Does the Failing Test Execute a Single or Multiple 
Faults? An Approach to Classifying Failing Tests 
Zhongxing Yu, Chenggang Bai, Kai-Yuan Cai 
Department of Automatic Control,  
Beihang University, Beijing, China,  
{yuzhongxing88@gmail.com}, {bcg, kycai}@buaa.edu.cn 
 
 
Abstract‚ÄîDebugging is an indispensable yet frustrating activity 
in software development and maintenance. Thus, numerous tech-
niques have been proposed to aid this task. Despite the demonstr-
ated effectiveness and future potential of these techniques, many 
of them have the unrealistic single-fault failure assumption. To 
alleviate this problem, we propose a technique that can be used to 
distinguish failing tests that executed a single fault from those 
that executed multiple faults in this paper. The technique suitably 
combines information from (i) a set of fault localization ranked 
lists, each produced for a certain failing test and ( ii) the distance 
between a failing test and the passing test that most resembles it 
to achieve this goal. An experiment on 5 real-life medium-sized 
programs with 18, 920 multiple-fault versions, which are shipped 
with number of faults ranging from 2 to 8, has been conducted to 
evaluate the technique. The results indicate that the performance 
of the technique in terms of evaluation measures precision, recall, 
and F-measure is promising. In addition, for the identified failing 
tests that executed a single fault, the technique can also properly 
cluster them. 
Index Terms‚Äîdistance calculation, fault localization, binary 
classification, debugging.  
I. INTRODUCTION  
Debugging is a notoriously difficult and time-consuming 
activity that can easily account for a significant part of costs in 
a typical software development and maintenance project [34].  
To tackle the ever-growing problem of high costs involved in it, 
researchers and practitioners have developed numerous techn-
iques as an aid for programmers during debugging. Represen-
tative examples include fault localization ( e.g., [1, 6, 12, 16, 21, 
38, 41, 42 , 45]), failure clustering ( e.g., [2, 5, 7, 22, 27]) and 
automated program repair ( e.g., [4, 18, 19, 25, 28, 37]), which 
in particular is gaining a growing interest in recent years. 
Collectively, these techniques have advanced the state of art in 
the area of program debugging.  
While these techniques have been shown to be effective, 
they have some questionably unrealistic assumptions that have 
restricted their application. The latter two of the above three 
techniques, in particular, explicitly or implicitly assume that 
every failure is caused by a single fault. Yet real-world prog-
rams will generally contain more than one fault, and some 
failures are caused by multiple faults. For these multiple-fault 
failures, the effectiveness of the proposed techniques is likely 
to degrade at best and to completely lose at worst. First, failure 
clustering techniques ideally aim to produce pure clusters (i.e., 
failures in each cluster are caused by exactly the same faults) 
and each generated cluster contains only failures due to a single fault. Obviously, the existence of multi-fault failures will make 
the ideal goal unrealistic . Second, program repair techniques 
typically use genetic programming to guide the generation of 
repairs that make passing test cases still pass and failing test 
cases also pass. The repair space that needs to be searched will 
increase significantly with the number of faults that a particular 
failure is involved with , which then will make the technique 
too expensive to be used in practice. 
To make these techniques more generally effective, in this 
paper, we propose a technique to alleviate this problem. Given 
a test suite in which each test has been labeled as passing or 
failing, our technique aims to distinguish failing tests that have 
executed a single fault (for simplicity, hereafter referred to as 
single-fault-execution failing tests ) from those that have exec-
uted multiple faults (referred to as multiple-fault-execution fail-
ing tests hereafter). Such a technique could potentially be serv-
ed as a preprocessing step for techniques that have the single-
fault failure assumption. Program repair techniques, for exam-
ple, can target only the identified single-fault-execution failures. 
By doing so, the repair success rate may be not too low, mak-
ing the technique practically feasible .  
Our technique builds on top of two observations. For each 
failing test tf in a given test suite, suppose we combine it with 
all the passing tests to get a specific test suite, and then use th is 
specific test suite and a certain fault localization algorithm to 
produce a ranked list of the possibly faulty program elements. 
This list actually embodies the opinion of tf on the likely fault 
location(s). The first observation is that the ranked list ùúãùëñ 
produced for a failing test ti can be contrasted with the other 
ranked lists produced for the other failing tests to provide with 
clues about the fault(s) that ti has executed . This paper presents 
an exploratory study of deriving and using the clues. Similarly, 
for each failing test tf in a test suite, according to a certain 
distance criterion, let us compute the distance between it and 
the passing test that most resembles it in the test suite. The 
second observation is that such a calculated distance for a 
multiple-fault-execution failing test (let s represent the set of 
faults executed) is likely to be larger than that for failing tests 
that have executed only part of s. Our proposed technique is 
built upon these two observations to infer fault indicators. Each 
fault indicator is made up of a set of statements that are able to 
indicate the execution of a certain fault once these statements 
have been executed, aiming at getting a classification of the 
failing tests into single-fault-execution ones and multiple-fault-
execution ones ultimately. Meanwhile, for the identified single- 
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.102924
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.102924
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.102924
ICSE 2015, Florence, Italy
  
 
 
 
 
 
  
 
 
   
 
 
Fig. 1.  The three faults of a three-fault version of program gzip.
fault-execution failing tests, our technique can also group faili-
ng tests due to the same fault together.  
We evaluated the technique using five real-life medium-
sized programs [9], including grep, sed, gzip, flex, and space. 
For each program, we generated 3,784 multiple-fault versions 
and the number of faults included in each version ranges from 
2 to 8. In total, the technique is evaluated on 18, 920 multiple-
fault versions and the number of test cases executed is over 10 
million. The experimental results indicate that the performance 
of the technique in terms of the commonly used evaluation me-
asures precision, recall, and F-measure in binary classification 
is promising. Also, the result of clustering the identified single-
fault-execution failing tests is sa tisfactory. 
In summary, the two main contributions of this paper are: 
ÔÅ¨ The proposal of a novel technique that can be used to 
classify failing tests into single-fault-execution ones and 
multiple-fault-execution ones, which in turn can potent-
ially be used to support techniques that have the single-
fault failure assumption. To the best of our knowledge, 
this paper is the first one to do this kind of classification. 
ÔÅ¨ An empirical evaluation of the technique on 5 real-life 
medium-sized C programs with 18, 920 multiple-fault 
versions demonstrates the effectiveness of the technique .  
The rest of this paper is structured as follows. Section II 
uses a motivating example to explain our technique and Section 
III describes the algorithm of the technique in detail. Section 
IV presents the experimental evaluation, followed by Section V 
which discusses the related work. Finally, Section VI concludes 
this paper and gives future perspectives. 
II. MOTIVATING EXAMPLE 
We use an example to explain our technique in this section. 
Figure 1 shows a three-fault version of program gzip. When th-
is program is executed with a test suite T, a set of passing test s 
TP and a set of failing test s TF will be produced. Among the 
tests in TF, some of them have executed only one fault, while 
the remaining will have executed more than one fault. 
A. Information from Fault Localization 
Fault localization techniques aim to identify the locations of 
faults when failure occurs and have been widely studied in the 
past two decades. D ue to its simplicity and effectiveness, one 
promising family of fault localization techniques is spectrum-
based fault localization (hereafter referred to as SBFL). SBFL 
techniques ( e.g., [16]) typically contrast the program spectra obtained from the execution of failing tests and that obtained 
from the execution of passing tests to highlight suspicious pro-
gram spectrum, and output a ranked list of them accordingly. 
Though SBFL techniques are generally applied using the 
entire failing test set and passing test set, they are not restricted 
in doing so. In fact, each failing test tf  in a given test suite can 
be combined with all the passing tests in this suite to get a spe-
cific test suite, and then a particular SBFL algorithm can use 
this specific test suite to do fault localization, that is , to produce 
a ranked list of the program spectrum. This ranked list then 
actually embodies tf‚Äôs opinion on the likely fault location(s).  
For the three-fault version of gzip, suppose we combine 
each failing test case in TF with TP to get a test suite and use an 
‚Äúideal‚Äù SBFL algorithm, which will always rank a faulty state-
ment at the top of the ranked list. Then, to identify multiple-
fault-execution failing tests, we can first establish a set S of sta-
tements such that each statement in S has at least ever appeared 
at the top of one rank ed list produced for a certain failing test, 
i.e., the statements in S are faulty, and then figure out which 
tests have executed more than one statement of the established 
set. Unfortunately, up to present, such an ‚Äúideal‚Äù SBFL algorit-
hm does not exist in practice. Table I lists part of the fault 
localization information produced by Naish2 [39] algorithm 
using seven failing tests of TF (for the space reason, the ranked 
lists are slightly modified). The stat column shows the state-
ment index (the index here counts only executable lines of co-
de), while the ran(sus) column represents the ranking and susp-
iciousness score of the corresponding statement. The numbers, 
shown in bold font, are the actual faulty statements. According 
to the formula of Naish2 (see Section III), for any failing tests 
that have executed a certain statement, the suspiciousness 
score of the statement will be larger than 0 and the same in the 
ranked lists produced for these tests. Note as the SBFL algor-
ithm may give the same suspicious ness score to different state-
ments, the sequential order of statements is used as the tie-
breaking scheme to rank such statements in this paper. Obvio-
usly, the faulty statements are likely not at the top of the ranked 
lists and can even be very low for some failing tests. 
Despite the absence of such an ‚Äúideal‚Äù SBFL algorithm, we 
can still get useful information from the produced rank ed lists. 
With regard to a particular faulty statement si that a failing test 
ti has executed, the statements ranked higher than it in the 
ranked list produced for ti can generally be classified into two 
categories. The first category consists of statements that are 
executed by every failing test that has executed si, while the se- 
local void fill_window()  {                                                              
‚Ä¶ 
 754      if (more == (unsigned)EOF)   { 
 755             more --; 
 756      } else if (strstart >= WSIZE+MAX_DIST) { 
                    ‚Ä¶ 
 771              for (n = 0; n < HASH_SIZE; n++) { 
 772                    m = head[n]; 
 773           head[n] = (Pos)(m >= WSIZE ? NIL : m-WSIZE); 
 //fault 1. correct: head[n] = (Pos)(m >= WSIZE ? m-WSIZE : NIL); 
 774             } 
                    ‚Ä¶ 
 787      }  /*else if (strstart >= WSIZE+MAX_DIST) */ 
             ‚Ä¶ 
 797  } /* fill_window */  
 
int zip(in, out)   
int in, out; { 
‚Ä¶ 
4848     if (save_orig_name) { 
4849     char *p = base_name(ifname);  
/* Don't save the directory part. */ 
4850     do { 
4851           put_char(*p ++); 
// fault 3. correct: put_char(*p);  
4852 } while (*p++); 
4853        } /* if (save_orig_name) */ 
        ‚Ä¶ 
4878     return OK; 
4879    } /* zip(in, out). */ 
 
off_t deflate  () { 
              ‚Ä¶ 
910     while (lookahead != 0) { 
         ‚Ä¶ 
921    if (hash_head != NIL && strstart - hash_head <= MAX_DIST) { 
//fault 2. correct: if (hash_head != NIL && prev_length <  
max_lazy_match && strstart - hash_head <= MAX_DIST) { 
                        ‚Ä¶ 
941         } 
        ‚Ä¶ 
  997      }  /* while */ 
998     if (match_available) ct_tally (0, window[strstart-1]); 
999     return FLUSH_BLOCK(1); /* eof */ 
1000    } /* deflate*/ 
 
925
925
925
ICSE 2015, Florence, ItalyTABLE I.  PART OF THE FAULT LOCALIZATION RANKED LISTS PRODUCED FOR SEVEN FAILING TESTS OF THE THREE -FAULT VERSION OF GZIP .  
tf1 tf2 tf3 tf4 tf5 tf6 tf7 
stat ran(sus)  stat ran(sus)  stat ran(sus)  stat ran(sus)  stat ran(sus)  stat ran(sus)  stat ran(sus) 
100 1(1.0) 1450 1(1.0) 25 1(0.63) 1450 1(1.0) 100 1(1.0) 25 1(0.63) 1362 1(0.98) 
101 2(1.0) 1569 2(1.0) 1314 2(0.61) 25 2(0.63) 101 2(1.0) 1452 2(0.61) 1373 2(0.98) 
102 3(1.0) 1588 3(0.99) 1315 3(0. 61) 70 3(0.52) 102 3(1.0) 70 3(0.52) 1374 3(0.98) 
103 4(1.0) 1362 4(0.98) 1317 4(0. 61) 150 4(0.51) 103 4(1.0) 154 4(0.52) 1375 4(0.98) 
104 5(1.0) 1373 5(0.98) 1318 5(0. 61) 151 5(0.51) 104 5(1.0) 147 5(0.51) 1666 5(0.98) 
1362 6(0.98) 1374 6(0.98) 1320 6(0. 61) 152 6(0.51) 25 6(0.63) 148 6(0.51) 1714 6(0.98) 
1373 7(0.98) 1375 7(0.98) 1323 7(0. 61) 153 7(0.51) 70 7(0.52) 149 7(0.51) 1719 7(0.98) 
1374 8(0.98) 1488 8(0.97) 1325 8(0. 61) 155 8(0.51) 154 8(0.52) 150 8(0.51) 1720 8(0.98) 
1375 9(0.98) ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ 156 9(0.51) 147 9(0.51) 151 9(0.51) 1722 9(0.98) 
1666 10(0.98) 1674 46(0.97) 1544 29(0.61) 157 10(0.51) 148 10(0.51) 152 10(0.51) 1253 10(0.97) 
‚Ä¶ ‚Ä¶ 25 47(0.63) 1545 30(0.61) 158 11(0.51) 149 11(0.51) 153 11(0.51) 1254 11(0.97) 
25 142(0.63)  70 48(0.52) 70 31(0.52) 159 12(0.51) 150 12(0.51) 155 12(0.51) 1255 12(0.97) 
70 143(0.52) 150 49(0.51) 150 32(0.51) 160 13(0.51) 151 13(0.51) 156 13(0.51) 1589 13(0.97) 
150 144(0.51) 151 50(0.51) 151 33(0.51) 161 14(0.51) 152 14(0.51) 157 14(0.51) 1590 14(0.97) 
151 145(0.51) 152 51(0.51) 152 34(0.51) 162 15(0.51) 153 15(0.51) 158 15(0.51) 1621 15(0.97) 
 
cond category consists of statements that are executed by some 
but not all failing tests that have executed si. The statements in 
the first category can be interpreted as a deputy for the real fau-
lty statement si. Note for the application of tie-breaking scheme, 
the suspiciousness scores of the statements included in the dep-
uty may be the same as that of si. Considering the seven failing 
tests in Table I, as the execution of the statement set { 100, 101, 
102, 103} is always accompanied with the execution of fault 1 
(statement 104), it can be regarded as a deputy for fault 1. Simi-
larly, the statement sets {25, 70} and {1362, 1373} can be inte-
rpreted as the deputy for fault 2 (statement 150) and fault 3 
(statement 1374), respectively. Differently, for instance, though 
the statement set {1450} is accompanied with the execution of 
fault 2 in tf2 and  tf4, it is not executed by tf1, tf3, tf5 and tf6 which 
also have executed fault 2, thus it belongs to the second categ-
ory and cannot be regarded as a deputy for fault 2.    
Meanwhile, after the application of the tie-breaking scheme , 
there may exist some other statements which are ranked lower 
than si yet have the same suspiciousness score with si, and for 
failing tests, the execution of si is always followed with the ex-
ecution of these statements. These statements can be interpret-
ted as a pursuer of si. Again, let us consider the seven tests in 
Table I, as the execution of the statement set {1375} is always 
followed with the execution of fault 3(statement 1374), then it 
can be deemed as a pursuer of fault 3, however the statement 
set {1666, 1714, 1719, 1720, 1722} is not a pursuer of fault 3 
as it is executed by tf7 yet not by tf2. 
Definition (complete fault indicator):  A complete fault in-
dicator for a particular fault  f whose faulty statement is s is 
made up of statements from the following three sets:  (i) a set 
that contains the deputy for s , (ii) a set that contains s , (iii) a 
set that contains  the pursuer  of s.  
For statement deleting fault , the statement preceding the 
missing one can be considered to be faulty following the conv-
ention adopted by [15] . Should we establish a mechanism to 
derive complete  fault indicators for possible faults in the progr-
am, it is similar to that we have an ‚Äúideal‚Äù SBFL algorithm. In 
practice, deriving a complete fault indicator for a certain fault 
can also be difficult. However, note any subset of the state-
ments included in the complete  fault indicator for a certain fault  
f can be used to indicate the execution of f for failing tests to a 
large extent. In particular, we can view a subset of them whose 
suspiciousness scores in a ranked list produced for a failing test 
that has executed f are equal and larger than that of any other 
statements included in the complete fault indicator for f as the 
‚Äúsuccinct‚Äù fault indicator for f. For instance , the complete fault 
indicator for fault 2 of the three-fault version of gzip is the stat-
ement set {25, 70, 150, 151, ‚Ä¶}, and the ‚Äúsuccinct‚Äù fault indic-
ator is the statement set {25} as statement 25 has the highest 
suspiciousness score 0.63. The fault indicator referr ed to in the 
remainder of this paper is the ‚Äúsuccinct‚Äù fault indicator and we 
will show how to derive it in detail in next section.  
B. Information from Distance Calculation 
Execution comparison has been widely used for debugging 
programs or explaining progra m behavior. Among the various 
techniques proposed, a simple yet effective technique is based 
on program spectra [ 32]. Specifically, in the nearest neighbor 
based model [ 30], for a given failing test ti in a test suite T, 
spectra have been used to select a passing test, which most res-
embles ti according to a certain distance criteri on, from T.   
Intuitively, the more faults a failing test executes, the furth-
er the behavior of it is likely to away from the correct behavior 
it is assumed to be.  Modern program development features the 
creation of a relatively large test suite, and thus a correct test 
that most resembles a failing test in a given test suite can parti-
ally represent what behavior the failing test is assumed to be. 
These two aspects explain the following observation that we 
have made during the experimental process: 
Observation (distance):  the distance between a multiple-
fault-execution failing test t m (let s represent the set of faults 
executed) and its nearest passing test is likely to be larger 
than the distance between a failing test t f, which has executed 
only part of s, and its nearest passing test.  
For the three-fault version of gzip, Table II shows the dista-
nce information calculated for 18 failing tests. The test(fault) 
column shows the index of the failing test and the set of faults 
the failing test has executed, while the distance column illustra-
tes the distance between the failing test and the passing test that 
most resembles it in TP using the ulam-distance  measure [ 30].  
926
926
926
ICSE 2015, Florence, ItalyTABLE II.  THE ULAM-DISTANCE OF THE FAILING TESTS TO THEIR 
NEAREST PASSING TESTS . 
test(fault)  distance test(fault)  distance test(fault)  distance 
1(2) 76 7(2) 6 13(1,2) 137(tf5) 
2(2,3) 76(tf2) 8(2) 3 14(2) 6 
3(2) 7 9(2) 7 15(2) 7(tf3) 
4(1,2) 180 10(2) 7 16(2) 3 
5(2) 7 11(2) 7(tf6) 17(2) 6(tf4) 
6(3) 134(tf7) 12(1,2,3) 239(tf1) 18(2) 14 
 
The underlined tests are the seven tests in Table I. The ulam-
distance is an edit distance between permutations. To count 
ulam-distance , each profile is first transformed into an ordered 
sequence of basic blocks of the program, sorted by their execu- 
tion frequencies. Then the distance between two profiles is the 
number of operations necessary to transform the sorted list of 
basic blocks from one into that from another. The merit of 
ulam-distance  over euclidean-distance  is that it makes use of 
the relevant execution counts, but not the counts themselves. 
Using directly the counts may lead to illogical results. As an 
example, if two runs just diff er greatly in the execution number 
of a loop (10 vs 10000) , using the execution counts to calculate 
the distance will lead to a great value while the two runs are in 
fact very close. We can see from the table that the 12 th failing 
test, which executes all the three faults, has the largest calcula-
ted distance measure, and the calculated distance measures of 
failing tests that have executed two faults are generally larger 
than that of failing tests that have executed a single fault. 
This observation can be used to help us more accurately 
derive the fault indicators for possible faults in the program , 
and we will describe this aspect in detail in next section.  
III. ALGORITHM  
The inputs to the algorithm include a program P and a test 
suite T consisting of a set of test cases TP which have passed 
and another set of test cases TF = {t1, t2, ..., tn} which have 
failed. The output of the algorithm is a subset Ts of TF such that 
each test in Ts has executed a single fault and another subset Tm 
of TF such that each test in Tm has executed multiple faults. 
Our algorithm operates in three main steps. The first step 
involves using a certain fault localization algorithm to produce 
a ranked list for each failing test in TF. In the second step, we 
contrast the ranked lists produced for different failing tests to 
derive each failing test‚Äôs opinion on the potential fault indicator. 
Finally, we synthesize the opinions on potential fault indicators 
from different failing tests to get real fault indicators and class-
ify the failing tests into Ts and Tm accordingly.  
A. Derivation of Opinions on Likely Fault Location(s) 
For each failing test ti 
ÔÉéTF, it can be combined with TP to 
form a new test suite Ti which can be used by a fault localiza-
tion algorithm FL to generate a ranked list
iÔÅ∞ of program spec-
trum, i.e., 
ùúãùëñ= FL ({ ti }, TP)  (ti 
ÔÉéTF , i = 1, 2, ..., n)         (1) 
As discussed in the previous section, 
iÔÅ∞ actually embodies 
ti‚Äôs opinion on the likely fault location(s). Any fault localization 
technique that works with one failing test and a set of passing 
tests, and outputs a ranked list of program spectrum can be us-ed (e.g., [16, 39]). In this paper, we use a particular SBFL algo-
rithm Naish2 because it is simple and has been demonstrated 
theoretically as an optimal SBFL formula for single-fault sce-
nario [39]. The formula for Naish2 can be represented as: 
suspiciousness (ùë†ùëñ)=  ùëéùëíùëìùëñ‚àíùëéùëíùëùùëñ
ùëéùëíùëùùëñ+ùëéùëõùëùùëñ+1                         (2) 
where ùëéùëíùëìùëñ and  ùëéùëíùëùùëñ represent the number of tests in a given test 
suite that execute statement ùë†ùëñ  and return the testing result of 
fail and pass, respectively, and ùëé ùëõùëùùëñ denotes the number of tests 
that do not execute   ùë†ùëñ and have the pass testing result.   
We expect other suitable fault localization algorithms to de-
liver similar results , and will study this question in future work. 
B. Derivation of Opinions on Potential Fault Indicators 
Though the individual ranked list ùúãùëñ may not always pinp-
oint the actual faulty statement(s) that failing test  ti has execut-
ed, we can explore it to get useful feedback. As discussed in the 
previous section, if we can establish a mechanism to derive 
fault indicators for possible faults in the program , then it is 
similar to that we have an ‚Äúideal‚Äù SBFL algorithm . To achieve 
this goal, we first derive each failing test‚Äôs opinion on the pote-
ntial fault indicator.  
Suppose a failing test ti has executed a certain fault f whose 
faulty statement is fs. As described in the previous section, the 
fault indicator for f we try to derive consists of a set A of 
statements with the same suspiciousness score ss in the ranked 
list ùúãùëñ produced for ti.  According to the characteristic of fault 
indicator and formula (2), the set A of statements should be 
executed by all failing tests that have executed f and the 
suspiciousness scores of them will also be  ss in the ranked lists 
produced for these failing tests . In general, there will be more 
than one failing test that has executed f in a given test suite. 
Meanwhile, the larger the suspiciousness score of a statement 
sta in ùúãùëñ, the more likely it is fs and as the value ss will always 
be larger than or equal to the suspiciousness score of  fs in ùúãùëñ, 
consequently the more likely  sta belongs to the set A. Thus, to 
get ti‚Äôs opinion on the potential fault indicator, we can search 
ùúãùëñ from top to down to find the first common subsequence 
between it and other individual ranked lists. Note the ‚Äúcomm-
on‚Äù here requires not only the same statement index, but also 
the same corresponding suspiciousness score. In case ti has ex-
ecuted multiple faults, the potential fault indicator established 
is likely for the fault(s) whose fault indicator‚Äôs suspiciousness 
score in ùúãùëñ is larger than that of other faults ti has executed. 
If a certain fault f has been executed by only one failing test 
ti and ti has not executed any other faults, we may not find a co-
mmon subsequence between the ranked list ùúãùëñ produced for ti 
and other ranked lists even at the suspiciousness score level l of 
the actual faulty statement of f in ùúãùëñ. Searching common subse-
quences below the level l will return spurious potential fault 
indicators. Thus we need to set a threshold value miniscore , 
which represents the estimated lowest suspiciousness score of 
an actual faulty statement in a ranked list and can be set accor-
ding to the recognition of how seriously coincidental correct-
ness [23, 35] occurs in TP. As the level of coincidental corre-
ctness has been shown to below 80% in most cases, especially 
for the studied programs in this paper whose sizes are relatively 
927
927
927
ICSE 2015, Florence, Italylarge [23], we use 0.2 as the miniscore  throughout the exper-
iment according to (2), i.e, 1 ‚àí0.8= 0.2. 
Formally, this step operates in two phases. Algorithm 1 
shows the two phases in detail. The algorithm takes as input a 
set of ranked lists { ùúãùëñ| i = 1, 2, ..., n } each produced for a 
failing test and a threshold value miniscore  on the minimum 
suspiciousness score, and outputs a set of potential fault indic-
ators { pfii | i = 1, 2, ..., n} each of which embodies a failing 
test‚Äôs opinion on the potential fault indicator and a set of suspi-
ciousness scores { ssi | i = 1, 2, ..., n} each of which represents 
the suspiciousness score of the corresponding potential fault 
indicator. 
Phase 1: Identifying the suspiciousness score for each 
failing test‚Äôs opinion on the potential fault indicator.  The 
aim of the first phase is to identify at what suspiciousness 
score level, the opinion of each failing test on the potential 
fault indicator can be found. To achieve this, for each failing 
test ti (i = 1, 2, ..., n) (line 1), we first use a boolean flag 
finished to illustrate whether the common subsequence search 
process has finished or not and set it to false initially (line 2), 
and then we search its corresponding ranked list  ùúãùëñ from top to 
down (line 3, where length (ùúãùëñ) returns the number of statem-
ents in the ranked list  ùúãùëñ) until we find a statement statdesire   
(line 4), which is the statement with the rank rank in ùúãùëñ and is 
returned using the function stat ( ùúãùëñ, rank), such that:  
a) statement  statdesire exists at least in one of  ùúãùëí (e = 1, 
2, ..., n, e!=i) with the same suspiciousness score as it in 
ùúãùëñ (lines 8-9, where function sus( ùúãùëñ, statdesire )  returns the 
suspiciousness score of statement  statdesire  in ùúãùëñ), 
b) the suspiciousness score sus(ùúãùëñ, statdesire ) of statem-
ent statdesire  is larger than or equal to miniscore . 
If statement statdesire  has been successfully found, we ass-
ign its corresponding suspiciousness score sus(ùúãùëñ, statdesire ) to 
ssi and finish the search process (lines 10-11). Otherwise, we 
set ssi to 0 and also finish the search process (lines 5-6), which 
implies that we believe the fault(s) that ti has executed has not 
been executed by other failing tests. 
      Phase 2: Deriving each failing test‚Äôs opinion on the 
potential fault indicator.  In this phase, we will establish each 
failing test‚Äôs opinion on the potential fault indicator based on 
the identified suspiciousness scores. For each failing test ti (line 
17), in case ssi equals to 0, pfii will be an empty set as we have 
no information to establish the fault indicator(s) for the fault(s) 
that ti has executed (lines 18-19). Otherwise, pfii consists of the 
set of statements with  suspiciousness score ssi in ùúãùëñ initially 
(line 21). On the one hand, the suspiciousness scores of differe-
nt fault indicators may be the same, thus pfii may contain multi-
ple potential fault indicators for different faults. On the other 
hand, even if pfii contains only a single potential fault indicator, 
pfii established this way may include redun dant statements that 
are executed by ti but not all failing test cases that have execu-
ted the corresponding fault of pfii. Note the statements included 
in a potential fault indicator should be executed by all failing 
tests that have executed the corresponding fault. Thus, to excl-
ude the redundant statements and get possible multiple poten-
tial fault indicators , we first search each ùúãùëò (k=1, 2, ...,n, k!=i) 
of the other ( n-1) failing tests   to get a test case set  testset i , wh- Algorithm 1: Derivation of Opinions on Potential Fault Indicators 
Input: a set of ranked lists { ùúãùëñ| i = 1, 2, ..., n} each produced for a failing test, a 
threshold value miniscore  on the minimum suspiciousness score  
Output: a set of potential fault indicators { pfii | i = 1, 2, ..., n} and a set of suspiciousness 
scores { ssi | i = 1, 2, ..., n} 
(1) for i =1 to n // phase 1 
(2)      Boolean finished=false  
(3)      for rank =1 to length ( ùúãùëñ) 
(4)         statdesire = stat (ùúãùëñ, rank) 
(5)         if (sus(ùúãùëñ, statdesire ) < miniscore )  then  
(6)           ssi= 0,  finished = true  
(7)         else  
(8)             for e =1 to n && e != i 
(9)                if (sus(ùúãùëí, statdesire ) == sus(ùúãùëñ, statdesire ))  then 
(10)                    ssi= sus(ùúãùëñ, statdesire ),  finished = true  
(11)                    break  
(12)             endfor  
(13)          if (finished==true) then  
(14)              break  
(15)       endfor  
(16)   endfor   //  phase 1  
(17)  for i =1 to n // phase 2 
(18)     if (ssi = 0)  then 
(19)         pfii = ‚àÖ  
(20)     else 
(21)        pfii  ={s|sus(œÄùëñ, s)= ssi } 
(22)        for k = 1 to n && k != i 
(23)            if (pfii ‚à© {s | sus(œÄùëò, s)= ssi }) !=  ‚àÖ  then 
(24)                testset i ‚à™ ={k} 
(25)         endfor 
(26)         ‚àÄ statset ‚äÜ pfii 
(27)             if each stata‚ààstatset  is executed by exactly the same test set A‚äÜ (testset i ‚à™
{i})&& ‚àÄstatb‚àà(pfi i Ôºçstatset), statb is not executed by every test of A then 
(28)                  indicator i ‚à™ = {statset} 
(29)          pfii  = indicator i  
(30) endfor  // phase 2 
(31) return {pfii | i = 1, 2, ..., n} and { ssi | i = 1, 2, ..., n} 
 
ich contains indexes of failing tests that have common subse-
quences with pfii (lines 22-25). We then search for any subset 
statset of pfii, such that (i) each statement stata of statset is ex-
ecuted by exactly the same test case set A, which is a subset of 
testset i ‚à™{i}, and (ii) for any statement statb that belongs to 
the subtraction  of set pfii and statset, statb is not executed by 
every test of A. If so, statset is regarded as a potential fault 
indicator and will be added to the set indicator i, which contains 
distinct potential fa ult indicators from the viewpoint of ti and 
will be finally assigned to pfii (lines 26-29).  
Example.  Take tf7 of the three-fault version of gzip as an 
example, as the statement 1362 exists in œÄ1 and œÄ2 with the sa-
me suspiciousness score 0.98 as it in œÄ7, phase 1 will assign 
0.98 to ss7. In phase 2, pfi7 initially equals to the set of statem-
ents with suspiciousness score 0.98 in œÄ7, that is {1362, 1373, 
1374, 1375, 1 666, 1714, 1719, 1720, 1722}. As œÄ1 contains a 
statement set {1362, 1373, 1374, 1375, 1666 } and œÄ2 contains 
a statement set {1362, 1373, 1374, 1375} that have common 
subsequences with initial pfi7, {1362, 1373, 1374, 1375} will be 
finally returned as tf7‚Äôs opinion on the potential fault indicator 
according to the search mechanism . Similarly, the statement 
sets {100, 101, 102, 103, 104}, {1450}, {25}, {1450}, {100, 
101, 102, 103, 104}, and { 25} will be returned as the opinions 
on the potential fault indicator for tf1 to tf6 respectively, and the 
corresponding suspiciousness score sets are {1.0}, {1.0}, 
{0.63}, {1.0}, {1.0}, and {0.63} respectively. In this example, 
the opinion of each failing test on the potential fault indicator 
contains only a single potential fault indicator. 
928
928
928
ICSE 2015, Florence, ItalyC. Synthesis of Potential Fault Indicators and Classification 
After deriving each failing test‚Äôs opinion on the potential 
fault indicator in the previous step, we need to synthesize them 
to get the real fault indicators . The synthesis and classification 
process also operates in two phases.  
Phase 1: Establishing ‚Äú child-parent ‚Äù relationship betwe-
en different potential fault indicators and initial classifica-
tion. Phase 1 involves some initial processing . First, the opini-
ons from different failing tests may directly contain the same 
potential fault indicator. The duplicate ones should be deleted . 
Second, a fault may be triggered in different ways. Imagine the 
situation where m tests fail due to a certain fault f.  Among the 
m tests, m1 (2
ÔÇ£ m1 < m) tests go through a branch b that has 
seldom been executed by passing tests. According to the previ-
ous steps described, the statements inside the branch b will get 
a high suspiciousness score in the ranked lists produced for the-
se m1 failing tests and are like ly to be returned as a potential 
fault indicator pfi from the viewpoint of these m1 failing tests. 
However, pfi is in fact not indicative enough of f. This kind of 
spurious fault indicator can be interpreted as a child of the real 
fault indicator of f. Note that a real fault indicator may have 
many children. Commonly, we can distinguish a real fault ind-
icator rfi from its child fault indicator cfi as failing tests which 
return a cfi as (or as part of) their opinions on the potential 
fault indicator will always also execute the r fi. Phase 1 will use 
this characteristic to identify real fault indicators and get an ini-
tial classification of the failing tests.   
Algorithm 2 shows the detailed process. The algorithm 
takes as input the set TF of failing tests , a set of potential fault 
indicators { pfii | i = 1, 2, ..., n} and their corresponding suspic-
iousness score set { ssi | i = 1, 2, ..., n} derived in the previous 
step, and outputs single-fault-execution failing test set Ts and 
multiple-fault-execution failing test set  Tm. 
Phase 1 begins with a set of initialization and preparation 
steps. In line 1, the function dupremove (I) removes the duplic-
ate potential fault indicators in the input set I and assigns the 
remaining unique elements to a set ini, the set R, which cont-
ains the real fault indicators , and the returned sets Ts and Tm are 
initialized to be empty set s. Then, the function descendingso-
rt(ini) sorts the potential fault indicators in ini by suspiciousne-
ss score in descending order and assigns the result to an array 
arr (line 2). Finally, for each potential fault indicator arr[i] in 
arr, a set arr[i].child, which contains all possible children of 
arr[i], is set up and initialized to be an empty set (lines 3-5). 
Then, we try to establish the possible ‚Äú child-parent‚Äù relati-
onship between different potential fault indicators. As a child 
fault indicator‚Äôs suspiciousness score is always larger than that 
of its parent real fault indicator, the process starts with the arr-
ay index 0. For each array index i, we first establish a set testset, 
which contains failing tests whose opinions on the potential fa-
ult indicator include arr[i] (line 7, where opinion(t) returns test 
t‚Äôs opinion on the potential fault indicator). Then , we check ea-
ch potential fault indicator with array index j larger than i (line 
8), such that (i) arr[j] is a non-empty set, (ii) the suspiciousness 
score of arr[i] (sus(arr[i])) is not equal to that of arr[j] (sus(-
arr[j])), and (iii) arr[j] is executed by every test of testset (lines 
9-10). If so, arr[i] is deemed as a child of arr[j] (line 11). Algorithm 2: Synthesis of Potential Fault Indicators and Classification 
Input: a set TF of failing tests, a set I of potential fault indicators { pfii | i = 1, 2, ..., n} and a 
set of suspiciousness scores { ssi | i = 1, 2, ..., n} 
Output: Ts and Tm 
(1) ini
ÔÇ¨ dupremove (I), R
ÔÇ¨ ‚àÖ , Tm 
ÔÇ¨ ‚àÖ , Ts 
ÔÇ¨ ‚àÖ // phase 1 
(2) arr
ÔÇ¨ descendingsort (ini )  
(3) for i=0 to arr.lengthÔºç1 
(4)      arr[i]. child =  ‚àÖ 
(5) endfor 
(6) for i=0 to arr.lengthÔºç1 
(7)      testset = { t | t
ÔÉéTF && arr[i]‚ààopinion (t)} 
(8)      for j=i+1 to arr.lengthÔºç1 
(9)        if arr[j] !=  ‚àÖ && sus(arr[i])!= sus(arr[j]) then 
(10)           if each test in  testset has executed arr[j] then 
(11)              arr[j]. child  ‚à™ ={arr[i]} 
(12)      endfor  
(13) endfor  
(14) for i=0 to arr.lengthÔºç1 
(15)      if ‚àÄ j, i<j< arr.length, arr [i]
ÔÉèarr[j]. child then  
(16)            R ‚à™ ={arr[i]} 
(17) endfor  
(18) foreach rfi 
ÔÉéR 
(19)      update rfi.child  
(20) endfor 
(21) foreach t 
ÔÉéTF 
(22)   if t executes more than one element from R  then 
(23)           Tm ‚à™ ={t} 
(24)   else   Ts ‚à™ ={t} 
(25) endfor // phase 1 
(26) foreach rfi
ÔÉé R // phase 2 
(27)     if  rfi.child !=  ‚àÖ then 
(28)        newtestset = tests1( rfi) 
(29)        Boolean repeat= true 
(30)        while(repeat) 
(31)           if ‚àÉ c
ÔÉérfi.child,  ‚àÄ t1
ÔÉé A=tests2( c), t2
ÔÉé newtestset ÔºçA, ulam (t1, np (t1)) > 
ulam (t2, np(t2))  then  
(32)            repeat = true, Tm ‚à™ =A, Ts Ôºç=A 
(33)            newtestsetÔºç=A , rfi.childÔºç= {c} 
(34)           else repeat = false 
(35)        endwhile  
(36) endfor  // phase 2 
(37) return Ts, Tm 
 
According to the possible ‚Äú child-parent‚Äù relationship estab-
lished, a child of a real fault indicator can possibly become the  
parent of another child of this real fault indicator. To get the 
real fault indicators, we identify a set of potential fault indicat-
ors such that each of them cannot be a child of any other pote-
ntial fault indicators, and add them to the real fault indicator set 
R (lines 14- 17). Moreover, it may happen that A is a child of B 
and B is a child of C, but A is not a child of C. To address this , 
for each real fault indicator rfi from R, we update its children 
information by iteratively viewing the children of its current 
children also as its children (lines 18-20). Finally, for each fail-
ing test t of TF, we check the number of fault indicators from R 
t has executed; if the number is larger than 1, t is added to Tm, 
otherwise it is added to Ts (lines 21-25). 
Phase 2: Refining the initial classification using ulam-
distance measures.  This phase refines the classification made 
in phase 1. While failing tests that return a child fault indicator 
as (or as part of) their opinions on the potential fault indicator 
will always execute the real fault indicator, the phenomenon 
that a set of failing tests, whose opinions on the potential fault 
indicator include A, all execute another potential fault indicator 
B, whose suspiciousness score is smaller than that of A, does 
not necessarily mean that A is a child of B. Consider a two-fault 
(faults f1 and f2) version program where failing tests execute f1 
929
929
929
ICSE 2015, Florence, Italyconstitute a set s1 and failing tests execute f2 constitute a set s2, 
and s1 happens to be a subset of s2. Since the execution of f1 is 
likely to be accompanied with the execution of f2, but not vice 
versa, that is, the number of passing tests execute f2 is likely to 
be larger than that execute f1, the suspiciousness score of the 
real fault indicator rfi1of f1 in a ranked list is likely to be larger 
than that of the real fault indicator rfi2 of f2 in a ranked list acc-
ording to (2) . Consequently , suppose the potential fault indicat-
ors returned by the tests in sets s1 and s2
ÔÄ≠s1 are real fault ind-
icators, then the potential fault indicators returned by the tests 
in set s1 are likely to be rfi1, and the potential fault indicators 
returned by the tests in set s2
ÔÄ≠s1 are likely to be rfi2. However, 
obviously, rfi2 will also be executed by tests in s1.  
To determine whether an established child of a real fault 
indicator rfi from the set R in phase 1 is really a child of rfi or 
is in fact indicative of a new fault, we use the observation made 
in the previous section about ulam-distance  between tests. The 
major idea is that if a child of a real fault indicator rfi is indicat-
ive of a new fault , then the ulam-distance  measures between te-
sts related with this child in Ts to their nearest passing tests are 
likely to be larger than the ulam-distance  measures between so-
me other tests related with the rfi in Ts to their nearest passing 
tests. Phase 2 of algorithm 2 gives the detailed mechanism. 
For each real fault indicator rfi that belongs to R and has at 
least one child (lines 26-27), the algorithm first creates a test 
case set newtestset (line 28) using the following function ,  
tests1(rfi)= {t | t
ÔÉéTs &&( rfi 
ÔÉé pfis(t) || (opinion(t) ‚à© rfi.child!= 
 ‚àÖ &&‚àÄ fi 
ÔÉéR, fi ‚àâ pfis(t) ) ) }                    (3) 
where function pfis(t) returns the set of potential fault indicat-
ors that test t has executed. In other words, newtestset  contains 
a subset of Ts such that each of them has executed rfi or has not 
executed any real fault indicators yet its opinion on the potent-
ial fault indicator includes a child of rfi. Then, it checks wheth-
er there exists a child fault indicator c of rfi, such that for any 
test t1 of the test set A, which contains tests of newtestset  that 
have executed c and is returned using the following function,  
            tests2(c)= {t | t
ÔÉénewtestset  && c 
ÔÉé pfis(t)}             (4) 
the ulam-distance between it and its nearest passing test ( ulam 
(t1, np (t1)) is larger than the ulam-distance between any test t2 
of the set ( newtestset
ÔÄ≠ A) and its nearest passing test ( ulam (t2, 
np (t2)) (line 31). If such a c exists, c is deemed to be indicative 
of a new fault, and tests of set A are removed from Ts and add-
ed to Tm (line 32). After removing tests of A from newtestset 
and c from the set that contains rfi‚Äôs children (line 33), the che-
ck process repeats to see whether there exists a child of the 
updated rfi which in fact is indicative of a new fault.  
      In this paper, for a child c of a real fault indicator rfi to be 
indicative of a new fault, we require that the ulam-distance me-
asures of all tests (i.e., 100%) related with c in Ts to their near-
est passing tests are larger than that of any other tests related 
with rfi in Ts to their nearest passing tests, we will study the 
results of using a high percentage (e.g., 90%) in future work. 
Example.  For the three-fault version of gzip, as tf2 and tf4 
which return {1450} as their opinions on the potential fault in-
dicator both execute the potential fault indicator {25} whose 
suspiciousness score is smaller than that of {1450} , the poten-tial fault indicator {1450} will be viewed as a child of the pote-
ntial fault indicator {25}.  Similarly, the potential fault indica-
tor {100, 101, 102, 103, 104} will be viewed as a child of the 
potential fault indicator {25}. As the potential fault indicators 
{25} and {1362, 1373, 1374, 1375} cannot be children of any 
other potential fault indicators, they will be added to the real 
fault indicator set R. Consequently , as tf1 and tf2 have executed 
more than one fault indicator of R, they will be classified as 
multiple-fault-execution failing tests and the other five are 
classified as single-fault-execution failing tests in phase 1. In 
phase 2, as we can see from Table II that tf5, which has execut-
ed a child {100, 101, 102, 103, 104} of the real fault indicator 
{25}, has the largest ulam-distance measure to the nearest pas-
sing test among the four tests tf3, tf4, tf5 and tf6 which have only 
executed the real fault indicator {25} and its possible children 
{1450} and {100, 101, 102, 103, 104}, then {100, 101, 102, 
103, 104} is deemed to be indicative of a new fault, and tf5 will 
be classified as a multiple-fault-execution failing test.  
Remark: After the establishment of single-fault-execution 
failing tests,  we can also group failing tests due to the same fa-
ult together. Test cases of Ts that have executed a common real 
fault indicator rfi from R or have not executed any real fault 
indicators yet their opinions on the potential fault indicators 
include a child of rfi could be clustered together, i.e., they are 
deemed to have executed the same fault . In case an identified 
single-fault-execution failing test t‚Äôs opinion on the potential 
fault indicator is an empty set, t forms a cluster that contains 
only t. In this example, the identified single-fault-execution fai-
ling tests tf3, tf4 and tf6 have executed the same real fault indicat-
or {25} from R, and thus form a cluster. The remaining identif-
ied single-fault-execution failing test tf7 forms another cluster.  
IV. EVALUATION  
A. Subject Programs 
We used five medium-sized C-language programs that have 
been popular for software engineering research in the experim-
ental evaluation ( e.g., [3, 10, 33 , 46]): the four UNIX utilities 
sed, flex, grep, and gzip, and the program space, developed at 
the Europe Space Agency. All of them were obtained from the 
Subject Infrastructure Repository (SIR) [9] along with their test 
cases. Programs sed, flex, grep, and gzip have the number of 
test cases 370, 567, 809, and 214, respectively. As the original 
test suite provided in SIR for space contains over 13, 000 test 
cases, to enable the experiment to scale, we used a subset of it. 
The subset was obtained by first randomly selecting five small 
test suites with a size of 160 from the original test suite and 
then uniting these five test suites. Finally, the number of test 
cases used for space is 722. For each program, our experiment 
required 12 faults. To achieve this, we used faults provided in 
SIR as well as faults generated by way of random mutation 
following the methods described in [26]. Overall, we attempt to 
cover typical faults programmers made in practice.  
B. Experimental Setup 
To enable the examination of our technique‚Äôs performance, 
we first created a set of multiple-fault versions for each prog-
ram. For each program, we created all the two-fault versions 
930
930
930
ICSE 2015, Florence, Italy(12 choose 2), then all the three-fault versions (12 choose 3), 
and on up to all the eight-fault versions (12 choose 8) that cou-
ld be derived from the program‚Äôs 12 faults. In other words, for 
each program, the number of multiple fault versions created is: 
  N=
ÔÄ® ÔÄ©8
212
iiÔÄΩÔÉ• = 66+220+495+792+924+792+495=3784   (5) 
Next, for each multiple fault version, we executed the entire 
test suite storing the output and profile for each test. Then, we 
compared the outputs against the outputs derived from a refer-
ence version of the program that contained none of the injected 
faults to determine the passing or failing results of the tests. 
In total, the technique is evaluated on 18, 920 multiple fault 
versions and the number of tests executed is over 10 million. 
The coverage of each test is captured using the gcov utility. All 
the experiments were carried out on an Ubuntu 10.04 machine 
with 2.13 GHz Intel quad-core CPU and 8 GB of memory.  
C. Evaluation Measures 
To evaluate our technique, we use the following measures 
that have been widely used to evaluate classification results [13 , 
20, 24, 31]: precision, recall, and F-measure. 
Precision : The ratio of the number of failing tests correctly 
classified as ones that have executed a single fault (ùëõ ùë†‚Üíùë†) to the 
total number of failing tests classified as ones that have execu -
ted a single fault  (ùëõùë†‚Üíùë†+ùëõùëö‚Üíùë†). 
          Single fault precision : P(s) = ùëõùë†‚Üíùë†
ùëõùë†‚Üíùë†+ùëõùëö‚Üíùë†                 (6) 
Recall: The ratio of the number of failing tests correctly 
classified as ones that have executed a single fault ( ùëõùë†‚Üíùë†) to the 
total number of failing tests that have actually executed a single 
fault (ùëõùë†‚Üíùë†+ùëõùë†‚Üíùëö). 
            Single fault recall: R(s) = ùëõùë†‚Üíùë†
ùëõùë†‚Üíùë†+ùëõùë†‚Üíùëö  
                   (7) 
F-measure : A composite measure of precision and recall. 
Specifically, we use the F-measure that weights recall and prec-
ision equally. 
         Single fault F-measure: F(s) = 2‚àóùëÉ (s)‚àóùëÖ(s)
ùëÉ(s)+ùëÖ(s)                (8)  
D. Results and Analysis 
1) Classification Performance: Table III shows the average 
performance of our technique. The fault number  column repre-
sents the number of faults seeded in the program. The column 
titled A displays the percentage of actual single-fault-execution 
failing tests in our experimental evaluation, whereas the P, R, F 
columns show the results for evaluation measures precision, 
recall, and F-measure, respectively . For each program and 
fault-seeded number n, the results depicted are the products of 
averaging the results across all n-fault versions .  
We can see from column A of this table that with the incre-
ase of the fault-seeded number n, the number of multiple-fault-
execution failing tests is also increasing significantly . Theref-
ore, the separation of the single-fault-execution and multiple-
fault-execution failing tests is of significant value. 
With regard to the precision of the technique, it can be seen 
from this table that for all program and fault-seeded number 
combinations, the precision achieved by our technique is much 
better than the precision obtained by assuming all failing tests are single-fault-execution ones, i.e., the values depicted in colu-
mn A. The larger the fault-seeded number, the more significant 
the difference. For the 8-fault version of space, for example, 
the distinction can be as large as 0.81 -0.50=0.31. Thus, the pre-
cision can be viewed as satisfactory. In certain cases, we not 
only want to achieve a high precision but also demand as many 
single-fault-execution failing tests as possible. Thus, the recall 
evaluation measure is also crucial. In terms of recall, our appro-
ach also achieves promising results. First, the recall values are 
relatively large for all program and fault-seeded number comb-
inations. For 4 of the 5 programs (besides space), the recall val-
ues are equal or above 0.8 for all fault-seeded number values. 
Second, the recall values generally differ not much with the 
variation of the fault-seeded number. The F-measure synthesi-
zes the precision and recall measures and has been widely used 
to evaluate the overall performance of a method. Even for the 
8-fault versions of the five programs, the average F-measure 
achieved by our technique is around or above 0.7, which is a 
reasonable value to certify the effectiveness of our technique. 
Collectively, the performance of our technique is promising. 
The detailed evaluation results are presented in Figs. 2 and 
3 as distributions. For each fault-seeded number n, the horizon-
tal axis presents the three evaluation measures (denoted as n(P), 
n(R), and n(F) respectively) and another precision measure wh-
ich is the precision obtained by assuming all failing tests are 
single-fault-execution tests and is denoted directly usi ng n, and 
the vertical axis depicts the calculated values. For each fault-
seeded number n, the data points contained in the four corresp-
onding box-plots equal to the number of all the n-fault versions, 
i.e., 12 choose n. The line inside each box marks the median 
value and the edges of the box mark the first and third quartiles. 
The whiskers extend from the quartiles and cover 90% of the 
distribution; outliers are shown as points beyond the whiskers. 
By comparing the distributions regarding the two precisio n 
measures, we can see that our technique can consistently offer 
great gain especially when the fault-seeded number n is large. 
For the three evaluation measures, the median value line is 
generally closer to the third quartile box edge than the first qua- 
rtile box edge. Also, the whisker line extended from the third 
quartile box edge is nearly always much shorter than the whi- 
 
 
Fig. 2.  Performance evaluation measure‚Äôs distribution for sed. 
931
931
931
ICSE 2015, Florence, ItalyTABLE III.  THE AVERAGE CLASSIFICATION PERFORMANCE OF OUR TECHNIQUE ON THE FIVE SUBJECT PROGRAMS .
fault 
number sed flex grep gzip space 
A P R F A P R F A P R F A P R F A P R F 
2 0.93 0.98 0.93 0.95 0.92 0.97 0.85 0.88 0.86 0.93 0.80 0.81 0.97 0.99 0.92 0.93 0.93 0.97 0.82 0.87 
3 0.87 0.98 0.90 0.93 0.87 0.96 0.82 0.86 0.73 0.86 0.81 0.80 0.95 0.98 0.90 0.92 0.86 0.93 0.78 0.83 
4 0.81 0.98 0.88 0.91 0.80 0.94 0.81 0.84 0.63 0.80 0.85 0.78 0.92 0.97 0.88 0.91 0.78 0.90 0.75 0.80 
5 0.76 0.97 0.86 0.90 0.74 0.92 0.80 0.84 0.55 0.74 0.87 0.77 0.89 0.95 0.86 0.88 0.71 0.87 0.72 0.77 
6 0.72 0.96 0.84 0.89 0.67 0.90 0.81 0.83 0.50 0.70 0.89 0.75 0.85 0.93 0.84 0.86 0.64 0.85 0.69 0.75 
7 0.68 0.96 0.83 0.88 0.60 0.89 0.83 0.84 0.46 0.67 0.90 0.74 0.81 0.91 0.83 0.84 0.57 0.83 0.66 0.72 
8 0.66 0.95 0.82 0.87 0.53 0.87 0.85 0.85 0.44 0.65 0.91 0.73 0.77 0.88 0.83 0.83 0.50 0.81 0.63 0.69 
 
              
Fig. 3.  Performance evaluation measure‚Äôs distribution for flex, grep, gzip and space.
sker line extended from the first quartile box edge. These two 
aspects suggest it is highly probable that our technique can off-
er a relatively accurate classification.  
2) Clustering Result: As described above, our technique can  
also cluster the identified single-fault-execution failing test 
cases. To evaluate the clustering result, we use two measures 
cluster purity  and quantity of clusters . For a cluster C, the 
purity of it is defined using the most represented fault f, that is, 
the number of actual single-fault-execution failing test cases  
that executed  f within C is larger than that executed any other 
faults involved with C. Formally, cluster purity  is defined as 
|ùë°‚ààùê∂&&ùë° actullay  has  executed  only  faul ùë° ùëì|
|ùë°‚ààùê∂|. Table IV presents the av -
erage results of these two measures across all n-fault versions  
for each fault -seeded number n and each program. The parent-hesized number is measure quantity of clusters . For measure 
cluster purity , we first average the results obtained for differ-
rent clusters to get a value for each particular program version. 
We can see from this table that the quanti ty of clusters  is gene-
rally small, which means the number of redundant clusters is 
generally small, and the cluster purity  is all above 0.7, which 
implies most failing tests within a certain cluster are actual 
single-fault-execution failing tests that have executed the same 
fault. Overall, the clustering result of the identified single -
fault-execution failing tests is satisfactory.  
3) Discussion: An important limitation of our current techn-
ique is that it suffers from the occurrence of ‚Äúaccompany-
execution‚Äù fault set. The ‚Äúaccompany-execution‚Äù fault set ref-
ers to a set F of faults where for any fault f
ÔÉéF, failing tests 
execute f will always also execute the other faults of F. Acco- 
932
932
932
ICSE 2015, Florence, ItalyTABLE IV.  CLUSTERING RESULTS OF THE IDENTIFIED SINGLE -FAULT-
EXECUTION FAILING TESTS FOR THE FIVE SUBJECT PROGRAMS . 
fault number  sed flex grep gzip space 
2 0.98(1.92) 0.98(2.47) 0.91(2.41) 0.97(1.88) 0.97(3.44) 
3 0.96(2.84) 0.96(3.44) 0.84(3.32) 0.96(2.75) 0.93(4.75) 
4 0.94(3.74) 0.94(4.29) 0.79(4.12) 0.95(3.33) 0.90(5.77) 
5 0.92(4.60) 0.92(5.07) 0.76(4.83) 0.94(3.82) 0.87(6.58) 
6 0.89(5.35) 0.89(5.80) 0.74(5.44) 0.93(4.19) 0.85(7.18) 
7 0.87(5.94) 0.87(6.51) 0.73(5.95) 0.92(4.46) 0.84(7.59) 
8 0.84(6.31) 0.85(7.22) 0.73(6.38) 0.90(4.68) 0.83(7.78) 
 
rding to the mechanism of our technique in deriving fault 
indicators for possible faults, it is very likely that a single fault 
indicator will be returned for all faults involved in an ‚Äúaccom-
pany-execution‚Äù fault set. Then if a set T of failing tests exec-
ute only faults involved in an ‚Äúaccompany-execution‚Äù fault set, 
these failing tests will be wrongly classified as single-fault-
execution tests . Deriving fault indicators for faults involved in 
an ‚Äúaccompany-execution‚Äù fault set is inherently difficult and 
we will explore this theme, as well as other threads of direct-
ions to improve the accuracy of inferring fault indicators for 
possible faults, in future work. 
4) Threats to Validity:  The primary threat to external vali-
dity of this paper arises because only five medium-sized C 
programs have been used in the experimental evaluation. Ho-
wever, all these programs are real-life programs and for each 
program, our technique is evaluated across 3,784 multiple 
fault versions. Hence, we expect the results from our study to 
be representative. Another concern with generalization is that 
we only evaluated the subjects from two to eight faults, and 
therefore we cannot make any claims about how the results 
will generalize above eight faults. However, the results in Tab-
le III and Figs. 2 and 3 display a consistent trend, i.e., the more 
faults a program contains, the larger number of multiple-fault-
execution failing tests it is likely to have and the more actual 
benefit our technique will have. A final issue is that some 
faults used are created using mutation-based fault injection, 
and thus are potentially not representative of real faults. To 
minimize this issue , we strictly followed the methods descri-
bed by Offutt et al. [26] during the mutant generation process. 
In addition, mutation-based fault injection has been shown to 
be an effective approach to simulating realistic faults that can 
be used in research to yield trustworthy results [8, 17]. 
The main threat to internal validity for our study is that 
there may be faults in our implementation of the algorithm. To 
reduce this threat, we carefully inspect all of the code prod-
uced and test it on known concrete examples. The major threat 
to construct validity for this study concerns the appropriate-
ness of the evaluation measures used. We used evaluation me-
asures precision, recall, and F-measure that have been widely 
used to evaluate a binary classification method [13 , 20, 24, 31] 
to reduce this threat. 
V. RELATED WORK 
Contrasting each failing test with all the passing tests in a 
given test suite to get an individual ranked list of program 
elements has already been used in some failure clustering 
techniques [14 , 22]. While our technique seeks to infer fault 
indicators for possible faults by searching the earliest common subsequence between each ranked list and the other ranked 
lists, failure clustering techniques typically use a distance fun-
ction to quantify the agreement between different ranked lists 
and then group failures that are likely due to the same fault(s) 
together based on the calculated distances. The distance mea-
sure used varies by the characteristic of the elements in the ran-
ked lists. The Kendall‚Äôs Tau distance is used in [ 22], while the 
Jaccard set similarity is employed to calculate distance in [14 ]. 
Besides, Hsu et al. [ 11] studied the longest common subseque-
nce among all the ranked lists to help understand the fault in 
single fault scenario.  
The calculation of the distance between a failing test and 
other passing tests has also been used in other debugging tasks. 
To isolate the location of the bug, Renieris and Reiss [ 30] 
propose the nearest neighbor  model, which first finds a passing 
test that has coverage that is most similar to the coverage of the 
failing test according to the ulam-distance  metric, and then 
removes the set of statements executed by the selected passing 
test from the set of statements executed by the failing test. The 
programmer should start his/her search for the fault from the 
resulting set of program statements. The nearest neighbor mod-
el has further been introduced as a means for selecting execu-
tion peers in [ 32]. In addition, several works regarding debug-
ing try to generate passing tests that are as close as to the fail-
ing tests to improve debugging effectiveness from different pe-
rspectives [1, 29, 36 , 40, 43, 44]. In this paper, our observation 
explores the distance between failing tests and passing tests 
from a totally new perspective. 
VI. CONCLUSIONS AND FUTURE WORK  
Being able to distinguish single-fault-execution failing tests 
from multiple-fault-execution failing tests supports debugging 
techniques that suffer from the single-fault failure assumption. 
We presented a technique, which suitably combines informat-
ion from (i) a set of fault localization ranked lists, each produ-
ced for a certain failing test and ( ii) the distance between a 
failing test and its nearest passing test to infer fault indicators 
for possible faults, to achieve this goal in this paper. Our evalu-
ation of the technique provides initial yet strong evidence that 
the performance of the technique in terms of evaluation measu-
res precision, recall, and F-measure is promising. In addition, 
for the identified single-fault-execution failing tests, the propo-
sed technique can also appropriately cluster them. For future 
work, we will apply the technique to larger programs and prog-
rams shipped with more than eight faults to verify the perform-
ance of the technique in general. We would also like to investi-
gate how to improve the accuracy of inferring fault indicators 
for possible faults of the program, especially for faults involved 
in an ‚Äúaccompany-execution‚Äù fault set. 
ACKNOWLEDGMENT  
We thank the anonymous reviewers very much for their 
invaluable comments and suggestions. We are also grateful to 
Professor Tsong Yueh Chen for his feedback on the earlier 
version of this paper. The work is supported in part by grants 
from National Natural Science Foundation of China (Nos. 
61402027 and 61272164). 
933
933
933
ICSE 2015, Florence, ItalyREFERENCES  
[1] S. Artzi, J. Dolby, F. Tip, and M. Pistoia, ‚ÄúPractical fau lt 
localization for dynamic web applications,‚Äù i n International 
Conference on Software Engineering (ICSE) , pp. 265-274, 20 10.  
[2] J.F. Bowring, J.M. Rehg, and M.J. Harrold, ‚ÄúActive learning for 
automatic classification of software behavior,‚Äù i n International 
Symposium on Software Testing and Analysis (ISSTA) , pp. 195-
205, 2004. 
[3] D. Cotroneo, R. Pietrantuono, S. Russo, ‚ÄúA learning-based 
method for combining testing techniques,‚Äù i n International 
Conference on Software Engineering  (ICSE), pp. 142-151, 2013. 
[4] V. Dallmeier, A. Zeller, and B. Meyer, ‚ÄúGenerating fixes from 
object behavior anomalies,‚Äù i n International Conference on 
Automated Software Engineering (ASE) , pp. 550-554, 2009. 
[5] W. Dickinson, D. Leon, and A. Podgurski, ‚ÄúFinding failures by 
cluster analysis of execution profiles,‚Äù i n International 
Conference on Software Engineering  (ICSE), pp. 339-348, 2001. 
[6] N. Digiuseppe and J.A. Jones, ‚ÄúOn the in fluence of multiple 
faults on coverage-based fault localization,‚Äù i n International 
Symposium on Software Testing and Analysis (ISSTA) , pp. 210-
220, 2011. 
[7] N. Digiuseppe and J.A. Jones, ‚ÄúSoftware Behavior and Failure 
Clustering: An Empirical Study of Fault Causality,‚Äù i n 
International Conference on Software Testing, Verification and 
Validation (ICST) , pp. 191-200, 2012. 
[8] H. Do and G. Rothermel, ‚ÄúOn the use of mutation faults in 
empirical assessments of test case prioritization techniques,‚Äù 
IEEE Transactions on Software Engineering (TSE) , 32 (9): 733-
752, 2006. 
[9] H. Do, S. Elbaum, and G. Rothermel, ‚ÄúSupporting Controlled 
Experimentation with Testing Techniques: An Infrastructure and 
its Potential Impact,‚Äù Empirical Software Engineering: An 
International Journal (ESE) , 10(4): 405-435, 2005. 
[10] S. Horwitz, B. Liblit, and M. Polishchuk , ‚ÄúBetter Debugging via 
Output Tracing and Callstack-Sensitive Slicing,‚Äù IEEE 
Transactions on Software Engineering (TSE) , 36 (1): 7-19, 2010.  
[11] H.-Y. Hsu, J. A. Jones, and A. Orso, ‚ÄúRapid: Identifying bug 
signatures to support debugging activities,‚Äù i n International 
Conference on Automated Software Engineering (ASE) , pp. 
439-442, 2008. 
[12] D. Jeffrey, N. Gupta, and R. Gupta, ‚ÄúFault localization using 
value replacement,‚Äù i n International Symposium on Software 
Testing and Analysis (ISSTA) , pp. 167-178, 20 08. 
[13] T. Jiang, L. Tan, and S. Kim, ‚ÄúPersonalized defect prediction,‚Äù 
in International Conference on Automated Software Engineering 
(ASE), pp. 279-289, 2013.  
[14] J.A. Jones, J.F. Bowring, and M.J. Harrold, ‚ÄúDebugging in 
parallel,‚Äù i n International Symposium on Software Testing and 
Analysis (ISSTA) , pp. 16-26, 2007. 
[15] J.A. Jones, M.J. Harrold, and J. Stasko, ‚ÄúFault Localization 
Using Visualization of Test Information,‚Äù i n International 
Conference on Software Engineering (ICSE) , pp. 467-477, 2002. 
[16] J.A. Jones and M.J. Harrold, ‚ÄúEmpirical evaluation of the 
Tarantula automatic fault localization technique,‚Äù in  
International Conference on Automated Software Engineering 
(ASE), pp. 273-282, 2005. 
[17] R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst, R. Holmes, and 
G. Fraser, ‚ÄúAre mutants a valid substitute for real faults in 
software testing?,‚Äù in International Symposium on the Foundations of Software Engineering (ESEC/FSE), pp. 654-665, 
2014. 
[18] S. Kaleeswaran , V. Tulsian , A. Kanade , and A. Orso, ‚ÄúMintHint: 
Automated synthesis of repair hints,‚Äù i n International 
Conference on Software Engineering (ICSE) , pp. 266-276, 2014. 
[19] D. Kim, J. Nam, J. Song, and S. Kim, ‚ÄúAutomatic patch 
generation learned from human-written patches,‚Äù in  
International Conference on Software Engineering (ICSE) , pp. 
802-811, 2013. 
[20] S. Kim, H. Zhang, R. Wu, and L. Gong, ‚ÄúDealing with noise in 
defect prediction,‚Äù i n International Conference on Software 
Engineering  (ICSE), pp. 484-490, 2011. 
[21] B. Liblit, M. Naik, A.X. Zheng, A. Aiken, and M.I. Jordan , 
‚ÄúScalable statistical bug isolation,‚Äù i n Programming Language 
Design and Implementation (PLDI) , pp. 15-26, 2005. 
[22] C. Liu and J. Han, ‚ÄúFailure proximity: A fault localization-based 
approach,‚Äù i n International Symposium on the Foundations of 
Software Engineering (ESEC/FSE), p p. 46-56, 2006. 
[23] W. Masri , RA. Assi, ‚ÄúPrevalence of coincidental correctness and 
mitigation of its impact on fault localization,‚Äù ACM 
Transactions on Software Engineering and Methodology 
(TOSEM) , 23(1): Article 8, 2014.  
[24] J. Nam, S. J. Pan, and S. Kim, ‚ÄúTransfer defect learning,‚Äù i n 
International Conference on Software Engineering  (ICSE), pp. 
382-391, 2013. 
[25] H.D.T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, 
‚ÄúSemFix: program repair via semantic analysis,‚Äù i n 
International Conference on Software Engineering (ICSE) , pp. 
772-781, 2013. 
[26] A.J. Offutt, A. Lee, G. Rothermel, R.H. Untch, and C. Zapf, ‚ÄúAn 
experimental determination of sufficient mutation operators,‚Äù . 
ACM Transactions on Software Engineering and Methodology 
(TOSEM) , 5(2): 99-118, 1996. 
[27] A. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch, J. Sun,  
and B. Wang, ‚ÄúAutomated support for classifying software 
failure reports,‚Äù i n International Conference on Software 
Engineering  (ICSE), pp. 465-475, 2003. 
[28] Y. Qi, X. Mao, Y. Lei, Z. Dai, and C. Wang, ‚ÄúThe strength of 
random search on automated program repair,‚Äù i n International 
Conference on Software Engineering  (ICSE), pp. 254-265, 2014.  
[29] D. Qi, A. Roychoudhury, Z. Liang, and K. Vaswani, ‚ÄúDarwin: 
An approach to debugging evolving programs,‚Äù ACM 
Transactions on Software Engineering and Methodology 
(TOSEM) , 21(3): Article 19, 2012. 
[30] M. Renieris and S.P. Reiss, ‚ÄúFault localization with nearest 
neighbor queries,‚Äù in  International Conference on Automated 
Software Engineering (ASE) , pp. 30-39, 2003. 
[31] H. Seo and S. Kim, ‚ÄúPredicting recurring crash stacks,‚Äù in  
International Conference on Automated Software Engineering 
(ASE), pp. 180-189, 2012. 
[32] W.N. Sumner, T. Bao, and X. Zhang, ‚ÄúSelecting peers for 
execution comparison,‚Äù i n International Symposium on Software 
Testing and Analysis (ISSTA) , pp. 309-319, 20 11. 
[33] C. Sun and S.-C. Khoo, ‚ÄúMining succinct predicated bug 
signatures,‚Äù i n International Symposium on the Foundations of 
Software Engineering (ESEC/FSE), p p. 576-586, 2013.  
[34] I. Vessey, ‚ÄúExpertise in debugging computer programs: a 
process analysis,‚Äù International Journal of Man-Machine 
Studies, 23 (5): 459-494, 1985. 
934
934
934
ICSE 2015, Florence, Italy[35] X. Wang, S. Cheung, W. Chan and Z. Zhang, ‚ÄúTaming 
coincidental correctness: Coverage refinement with context 
patterns to improve fault localization,‚Äù i n International 
Conference on Software Engineering  (ICSE), pp. 45-55, 2009. 
[36] T. Wang and A. Roychoudhury, ‚ÄúAutomated path generation for 
software fault localization,‚Äù in  International Conference on 
Automated Software Engineering (ASE) , pp. 347-351, 2005. 
[37] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest , 
‚ÄúAutomatically finding patches using genetic programming,‚Äù in  
International Conference on Software Engineering (ICSE) , pp. 
364-374, 2009. 
[38] J. Xuan and M. Monperrus , ‚ÄúTest case purification for 
improving fault localization,‚Äù i n International Symposium on the 
Foundations of Software Engineering (ESEC/FSE) , pp. 52-63 , 
2014. 
[39] X. Xie, T. Y. Chen, F.-C. Kuo, and B. Xu, ‚ÄúA theoretical 
analysis of the risk evaluation formulas for spectrum-based fault 
localization,‚Äù ACM Transactions on Software Engineering and 
Methodology (TOSEM) , 22(4): Article 31, 2013. 
[40] Z. Yu, C. Bai, and K.-Y. Cai, ‚ÄúMutation-oriented test data 
augmentation for GUI software fault localization,‚Äù Information 
and Software Technology (IST) , 55(12): 2076-2098, 2013.  [41] Z. Yu, H. Hu, C. Bai, and K.-Y. Cai, ‚ÄúGUI software fault 
localization using N-gram analysis,‚Äù in International Symposium 
on High-Assurance Systems Engineering (HASE) , pp. 325-332, 
2011. 
[42] Y. Yu, J.A. Jones, and M.J. Harrold, ‚ÄúAn empirical study of the 
effects of test-suite reduction on fault localization,‚Äù i n 
International Conference on Software Engineering  (ICSE), pp. 
201-210, 2008.  
[43] A. Zeller, ‚ÄúYesterday, my program worked. Today, it does not. 
Why?,‚Äù i n International Symposium on the Foundations of 
Software Engineering (ESEC/FSE), p p. 253-267, 1999.  
[44] A. Zeller and R. Hildebrandt, ‚ÄúSimplifying and isolating failure-
inducing input,‚Äù IEEE Transactions on Software Engineering 
(TSE), 28 (2): 183-200, 2002. 
[45] X. Zhang, N. Gupta, and R. Gupta, ‚ÄúLocating Faults through 
Automated Predicate Switching,‚Äù i n International Conference 
on Software Engineering  (ICSE), pp. 272-281, 2006.  
[46] X. Zhang, S. Tallam, N. Gupta, and R. Gupta, ‚ÄúTowards 
locating execution omission errors,‚Äù i n Programming Language 
Design and Implementation (PLDI) , pp. 415-424, 2007.  
                                                                                                                                                            
935
935
935
ICSE 2015, Florence, Italy