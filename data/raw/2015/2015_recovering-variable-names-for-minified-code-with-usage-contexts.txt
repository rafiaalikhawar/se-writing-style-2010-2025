Recovering Variable Names for Miniﬁed Code
with Usage Contexts
Hieu Tran∗, Ngoc Tran∗, Son Nguyen∗, Hoan Nguyen†, and Tien N. Nguyen∗
∗Computer Science Department, The University of Texas at Dallas, USA,
Email: {trunghieu.tran,nmt140230,sonnguyen,tien.n.nguyen}@utdallas.edu
†Computer Science Department, Iowa State University, USA, Email: hoan@iastate.edu
Abstract —To avoid the exposure of original source code in
a Web application, the variable names in JS code deployed in
the wild are often replaced by short, meaningless names, thus
making the code extremely difﬁcult to manually understand and
analysis. This paper presents JSN EAT , an information retrieval
(IR)-based approach to recover the variable names in miniﬁed JS
code. JSN EAT follows a data-driven approach to recover names
by searching for them in a large corpus of open-source JS code.
We use three types of contexts to match a variable in given
miniﬁed code against the corpus including the context of the
properties and roles of the variable, the context of that variable
and relations with other variables under recovery, and the context
of the task of the function to which the variable contributes. We
performed several empirical experiments to evaluate JSN EAT
on the dataset of more than 322K JS ﬁles with 1M functions,
and 3.5M variables with 176K unique variable names. We found
that JSN EAT achieves a high accuracy of 69.1%, which is the
relative improvements of 66.1% and 43% over two state-of-the-
art approaches JSNice and JSNaughty, respectively. The time to
recover for a ﬁle or a variable with JSN EAT is twice as fast as
with JSNice and 4x as fast as with JNaughty, respectively.
Keywords -Miniﬁed JS Code, Variable Name Recovery, Natu-
ralness of Code, Usage Contexts.
I. I NTRODUCTION
Software developers have to spend a signiﬁcant portion of
their efforts in comprehending the code. An important aspect
of program understanding is the names of the identiﬁers used
in the source code [2]. Meaningful identiﬁers help developers
tremendously in quickly grasping the essence of the code.
Thus, naming conventions are strongly emphasized on pre-
scribing how to choose meaningful variable names in coding
standards [1]. These principles also apply to Web development.
Web technologies and programming languages require the
exposure of source code to Web browsers in the client side
to be executed there. To avoid such exposure, the source code
such as JavaScript (JS) ﬁles are often obfuscated in which
the variable names are miniﬁed, i.e., the variable names are
replaced with short, opaque, and meaningless names. The
intention has two folds. First, it makes the JS ﬁles smaller and
thus is quickly loaded for better performance. Second, miniﬁ-
cation diminishes code readability to hide business logics from
the readers, while maintaining the program semantics.
Due to those reasons, there is a natural need to automatically
recover the miniﬁed code with meaningful variable names.
When the original code is not available, with such recovery,
the miniﬁed JS code will be made accessible for code compre-hension as well as other maintenance activities such as code
review, reuse, analysis, and enhancement. Recognizing that
need, researchers have been introducing the automatically re-
covering tools for variable names in JS code. JSNice [17] is an
automatic variable name recovery approach that represents the
program properties and relations among program entities in a
JS code as dependence graphs. It leverages advanced machine
learning (ML) to recover missing variable names. Using also
ML, JSNaughty [21] formulates the variable name recovery
problem for JS code as a statistical machine translation from
miniﬁed code to the recovered code. Despite of their successes,
both approaches still suffer low accuracy and scalability issues
with the use of computationally expensive ML algorithms.
In this work, we present JSN EAT, a data-driven, information
retrieval (IR)-based approach to automatically recover vari-
able name for miniﬁed JS code. The recovered names for
variables must be natural in the context of the code and follow
naming conventions. Thus, we conform JSN EAT in a data-
driven direction, in which we aim to search for the name
recovered for a miniﬁed variable in a large corpus of open-
source JS code. We conjecture that meaningful, natural names
of miniﬁed variables could be seen before in such corpus. Our
key idea is to utilize the contexts for the variables in source
code to search for its name . For the miniﬁed variables in a
given a miniﬁed JS code, JSN EAT aims to match their contexts
against the contexts in the corpus. If two contexts of two
variables are similar, they should be named similarly, thus the
variable’s name in the corpus with the matched contexts should
be a candidate name for the corresponding miniﬁed variable.
For a given miniﬁed variable v, we model three types of
contexts. First, the name of a variable should be affected by
its own properties and roles in the source code . For properties,
ifvaccesses to a method mor a ﬁeld fin the code, then
the recovered name for vshould be naturally compatible with
the names of the method and the ﬁeld . For example, the
variable named dataTransfer is the receiver of the method
call getData() . However, a variable that calls the method
getData() cannot be randomly named in a regular program.
For a role, if vis used an argument of a method call m, the
data type of vmust be compatible with m, thus, their names
should also naturally be in conformance with one another. For
example, in JQuery.trigger(...) , the ﬁrst argument is either
an event or an event type, thus, the name of the ﬁrst argument
should be consistent with a direct object of the verb trigger .
11652019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 ©2019 IEEE
DOI 10.1109/ICSE.2019.00119
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. Second, the name of a variable should be affected by the
names of variables co-occurring in the same function . Several
variables are used together to contribute to the current task of
the function, thus, their names are naturally in concordance
with one another. Observing some variable names, JSN EAT
could predict the names for other co-occurring variables. It
takes into account the naming of multiple variables at once
considering such co-appearances. Finally, the third type of
context is the current task of the function to which the variable
belongs and contributes . The names of the variable should
be relevant to the common task/purpose of the function. For
example, the variables in a function getClipboardContent
should serve the task of getting the content from the clipboard
and have the names closely relevant to that task.
We combine the above contexts in the JSN EAT tool. We
built a database to store the information about the variable
names and the contexts extracted from a large corpus of open-
source JS code. To recover the names for a given miniﬁed JS
ﬁle, we use JSN EAT with three types of contexts to search
for and rank the candidate names. We performed several
experiments to evaluate JSN EAT on the dataset of 322K JS
ﬁles with almost 1M functions, and 3.5M variables with 176K
unique variable names. JSN EAT achieves high accuracy of
69.1% , which is the relative improvements of 66.1% and
43% over two state-of-the-art approaches JSNice [17] and
JSNaughty [21], respectively. A high percentage ( 29.4% )o f
variables is recovered only by JSN EAT, while 4.7% and 3.6%
of variables are recovered only by JSNaughty and JSNice,
respectively. We reported that the time to recover for a ﬁle or
for a variable with JSN EAT istwice as fast as with JSNice
and 4x as fast as with JNaughty , respectively. Importantly,
JSN EAT ’s training time is 4x faster than JSNice and 6x faster
than JSNaughty . This paper makes the following contributions:
1.JSN EAT: an IR-based, data-driven approach to recover
variable names for miniﬁed JS code using 3 types of contexts;
2.An extensive comparative evaluation and analysis on JS-
NEAT’s accuracy and running time to show that it outperforms
the state-of-the-art approaches (See results in a website [10]);
3.A novel formulation of variable name recovery problem
in miniﬁed JS code as an Information Retrieval problem.
II. M OTIV ATION AND APPROACH OVERVIEW
Figures 1 and 2 show the original and miniﬁed versions of
the JS function getClipboardContent in the vue-medium-ed-
itor project. The function is to retrieve the content of the clip-
board. In the miniﬁed code, all local variables are randomly re-
named with short and meaningless names, e.g., dataTransfer
becomes r,data becomes n, by a miniﬁcation tool, e.g., Ugli-
fyJS [20]. This makes developers difﬁcult to comprehend it.
Our goal is to assign meaningful names for the variables
in the miniﬁed code. The name chosen for a variable in the
code should be natural (unsurprising) in the context [21] and
follow naming conventions [1], so that the de-miniﬁed code
becomes easy to understand for developers.
To achieve this goal, we conjecture that the meaningful
names of miniﬁed variables could be observed in a large cor-1function getClipboardContent(event, win, doc) {
2var dataTransfer = event.clipboardData || win.clipboardData
|| doc.dataTransfer,
3 data = {};
4if(!dataTransfer) return data;
5if(dataTransfer.getData) {
6 var legacyText=dataTransfer.getData( ’Text’ );
7 if(legacyText && legacyText.length > 0) {
8 data[ ’text/plain’ ] = legacyText;
9 }
10 }
11 if(dataTransfer.types) {
12 for ( var i = 0; i < dataTransfer.types.length; i++) {
13 var contentType = dataTransfer.types[i];
14 data[contentType] = dataTransfer.getData(contentType);
15 }
16 }
17 return data;
18}
Figure 1: An Original Code from a Project in GitHub
1function getClipboardContent(t, a, e) {
2var r = t.clipboardData ||a.clipboardData ||e.dataTransfer,
3 n = {};
4if(!r) return n;
5if(r.getData) {
6 var i = r.getData( "Text" );
7 if(i && i.length > 0) {
8 n["text/plain" ]=i ;
9 }
10 }
11 if(r.types) {
12 for ( var p=0 ;p<r . t y p e s . l e n g t h ;p + + ){
13 var f = r.types[f];
14 n[f] = r.getData(f);
15 }
16 }
17 return n;
18}
Figure 2: The Miniﬁed Code for the Code in Figure 1
pus of existing source code. This motivates us to conform our
approach to a data-driven direction , where we learn the names
from original source code to recover the names for variables
in the miniﬁed code. Indeed, for the miniﬁed code in Figure 2,
all original names are found in our experimental dataset that
contains 322K JS ﬁles collected from 12K GitHub projects.
A. Observations
The name recovering process of variables in miniﬁed code
is affected by multiple factors. Let us illustrate these factors
through the following observations:
O1.Each individual variable has certain properties and
plays particular roles in the code. Thus, the name of a variable
is intuitively affected by its properties and roles . The properties
are the method calls or ﬁeld accesses to which a variable of
a certain type can access. If a method is called or a ﬁeld
is accessed by a variable, the name of the variable should be
compatible with the method’s or the ﬁeld’s name. For example,
in our experimental dataset, the number of candidates that can
1166
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. call method getData() (lines 6 and 14) is only 7 out of 31 vari-
ables names found in a function named getClipboardContent .
Such number is down to a single candidate if we additionally
consider that it can also access the ﬁelds getData (line 5) and
types (line 11). Thus, rcould be named as dataTransfer ,
which is the same name in the original code in Figure 1. For
the variable fthat is created and assigned as an element of
the array types[] at line 13, there are 4 candidates for such
variable that can be used as an argument of the method named
getData() (line 14). The number of candidates for i, which
is the returned result of the call to getData() (line 6) and also
has a ﬁeld with the name length (line 7), is only 7.
O2.In a function, a variable might collaborate with other
variables to implement the function. Consequently, the recov-
ering name for a variable might be inﬂuenced by the name
of others . Intuitively, since the variables are used together,
their names are often consistent with each other to achieve the
common task in the function. In the example, in 28 possible
pairs of candidates for i(7 candidates) and f(4 candidates),
there are only 2 pairs of candidates that are used to name two
variables in the same function in our dataset. One of them is
the correct pair, which is legacyText andcontentType .
O3. Within a function, e.g., getClipboardContent , a vari-
able name, e.g., contentType is affected by the speciﬁc task of
the function that is described by the function’s name [2] . This
is intuitive because the names of variables are often relevant to
the task that the variables are used in the code to achieve. Such
task is typically described with a succinct function name. In
Figure 1, the task of the function is to get the clipboard’s con-
tent, thus, it is named getClipboardContent . In our dataset,
there are 31 names being used to specify the variables in
function getClipboardContent ,e.g., data ,dataTransfer ,
contentType . Meanwhile, the variable names students or
salary have never been used in the function with that name.
Overall, these observations indicate that the names of the
variables in a particular function not only depend on the
task in which the variable is used to implement (called task-
speciﬁc context ), but their names are also affected by their
own properties and roles in the code (called single-variable
usage context ) and on the names of the other variables in the
same function (called multiple-variable usage context ).
B. Approach Overview
From the observations, we propose an IR-based, data-driven
approach to recover the variables’ names in a miniﬁed JS
code based on the contextual information including single-
variable usage context (SVC), multiple-variable usage context
(MVC), and task-speciﬁc context (TSC). We initially construct
a database to store the variables’ names and the corresponding
context information extracted from a large corpus of JS code.
To recover the names, given a miniﬁed JS code, we ﬁrst use
the SVC and TSC information to ﬁnd in our database the
candidate names for each variable. Then, these candidates for
each variable are ranked by the likelihood that they are used
along with the candidates of other variables, in order to name
the variables in the same function by using MVC .III. S INGLE -VARIABLE USAGE CONTEXT (SVC)
This section presents the single-variable context that we use
in the name recovery process. The intuition for this context is
that to recover the name of a variable, one could use its own
context based on its own properties and roles in the code.
By properties of a variable, we refer to the methods or ﬁelds
to which a variable of certain type can access. In the miniﬁed
code, the names of the called methods and accessed ﬁelds
are not miniﬁed. Thus, it could play the role of the pivots in
recovering the variables’ names. Importantly, due to nature of
naming, the name of the variable should be compatible with
the name of the method being called or the name of the ﬁeld
being accessed . Thus, they provide hints on the names of the
variables. For example, in Figure 2, the only candidate that
calls the method getData and accesses the ﬁeld types is
dataTransfer . Those names are compatible with each other.
To learn compatible names, we follow a data-driven approach
by learning from a large corpus of non-miniﬁed JS code.
By the role of a variable, we refer to its usage context with
the method calls or ﬁeld accesses that were not miniﬁed. For
example, a variable could be an argument of a method call,
or a variable could be assigned with the returned value from
a method call or ﬁeld access. The name of a variable used as
an argument is often compatible with its type/role and, thus,
coupled with the name of the method call itself. On line 14 of
Figure 1, the argument contentType is in conformance with
the method name getData . Thus, it helps recover the name of
the miniﬁed variable fon line 14 of Figure 2. Similarly, the
name of a variable receiving the returned value of a method
call or a ﬁeld access should conform with the name of the
method or ﬁeld. Such conformance can be learned from a large
corpus of non-miniﬁed code, and helps recover variable names.
A. Property and Role Relations
To realize the single-variable context for name recovery with
properties and roles of a variable, we deﬁne two key relations:
Property and Role . Those relations form the single-variable
usage context for name recovery.
Deﬁnition 1. [Property Relation (PropRel)] Property rela-
tion represents the relationships between a variable and its
ﬁelds or methods to which the variable can access or call.
A property relation between a variable vand its prop-
erty pis denoted by a triple (v,p,t ), where tis the
type of relation, which can be either fieldAccess or
methodCall . In Figure 2, a set of property relations for r
includes (r,types,fieldAccess ),(r,getData,fieldAccess ),
(r,getData() ,methodCall ).
Deﬁnition 2. [Role Relation (RoleRel)] Role relation rep-
resents the relationships between a variable and the method
calls or ﬁeld accesses in its usages.
Since the names of methods or ﬁelds are not miniﬁed, we
consider them as the pivots in the usage context for recovering
names of the miniﬁed variables. We focus on the roles of a
variable used as an argument in a method call orreceiving
1167
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. dataTransfer clipboardData
assignment
(RoleRel)assignment
(RoleRel)
getData()getData types
methodCall
(PropRel)fieldAccess
(PropRel)fieldAccess
(PropRel)
Figure 3: The Relation Graph of Variable rin Figure 2
the value returned by a method call or ﬁeld access .I fw eh a v e
o.m(...,v,... ),v=o.m(...),o rv=o.f, then there exist the
role relations between vandm, and between vandf. The
rationale is that the names of mand its argument are often
in conformance with each other, e.g., getData(contentType) .
Similar rationale is applied to the above assignments to v.
A role relation between a variable vand a ﬁeld/method pis
denoted by a triple (v,p,t ), where tis the type of role relation.
A role relation could be either argument orassignment .
B. Graph Representation of Single-V ariable Context
Deﬁnition 3. [Relation Graph] A relation graph (RG) for
a variable vis a directed graph in the shape of a star to
represent the single-variable usage context of vwith regard to
its property and role relations with the ﬁelds and methods in its
usage. The center vertex of the RG represents the variable. The
other vertices represent the methods/ﬁelds in method calls or
ﬁeld accesses, respectively, and are labeled with their names.
Edges represent relations and are labeled with relation types.
Figure 3 shows the relation graph of the variable r, which
includes a set of property relations: (r,types,fieldAccess ),
(r,getData,fieldAccess ),(r,getData() ,methodCall ), and
a set of role relations: (r,clipboardData ,assignment ),
(r,dataTransfer ,assignment )in our example.
C. Deriving Candidate Names Using Single-V ariable Context
Let us explain how we use the relation graphs to derive the
ranked list of candidate names for a miniﬁed variable.
Our idea is that if two variables have the same/similar
contexts, they are often named similarly. Given a miniﬁed
functionf, we ﬁrst parse fto produce a relation graph Gvfor
each variable v. We then search for the single-variable contexts
that are matched with the context of vwithin a dataset Gof
the relation graphs built from a large corpus of open-source
projects (We will explain how to build the dataset later).
Deﬁnition 4. [Single-Variable Context Matching] Two
single-variable usage contexts for a miniﬁed variable vand a
variablev/primein the dataset are considered to be matched if and
only if their corresponding relation graphs are matched.
Deﬁnition 5. [Relation Graph Matching] A relation graph
Gvof a miniﬁed variable vis considered as matched withrelation graph Gv/primeofv/primeinGif and only if their graph
matching score is equal or greater than a threshold ϕ.
Since RGs all have star shape, matching graphs can be done
by matching their sets of edges.
Deﬁnition 6. [Relation Graph Matching Score] The graph
matching score ρ(Gv,Gv/prime)betweenGvof a miniﬁed variable
vand a relation graph Gv/primeofv/primeis computed as the percentage
of the number of edges in Gvfound in Gv/prime.
A variable name might appear in multiple functions, so it
might have multiple relation graphs. Thus, we deﬁne a name
matching score considering all of those functions as follows.
Deﬁnition 7. [Single-Variable Score] Single-variable score
represents how well name vncan be used for a miniﬁed name
vand is computed based on graph matching score:
SC v,vn =m a x
Gv/prime∈G vnρ(Gv,Gv/prime) (1)
whereGvnis the set of relation graphs of name vn that
matchGv.Gvnrepresents multiple usages of the name vn.
If a match is found, the name vnof the variable v/primein the
matched relation graph in Gis considered as a candidate name
forv. There might exist many candidate names having similar
contexts with the context of a variable v. The candidate names
are ranked based on their name matching scores. The higher
the name matching score, the higher the conﬁdence of our
model in using the name vnofv/primeforv.
For example, when recovering the variable rin Figure 2, by
searching on the dataset, we found that the variables named
dataTransfer anddataObj have the relation graphs matching
with that of r. This implies that these variable names have been
used in the past and they have similar SVC contexts with r.
Therefore, rcould be recovered as dataTransfer ordataObj .
IV . M ULTIPLE -V ARIABLE USAGE CONTEXT (MVC)
Let us present how we deﬁne and use multiple-variable
usage context. To achieve a speciﬁc task, developers use one
or multiple variables in their code. Because the variables all
play their roles in the code, their names are often relevant and
consistent with one another in order to achieve the common
task in the function to which they belong. For example, in
Figure 1, the variables dataTransfer ,contentType , and data
serve their roles in the task to retrieve the content of a
clipboard, and their names are naturally consistent with one
another with regard to that task. In name recovery, we utilize
such co-occurrences of variable names to recover the name
for one variable while another one was recovered with the
co-occurring name if the contexts of two variables allow.
A. Multiple-V ariable Score
To formulate the co-occurrence of variable names, we deﬁne
the association score for a set of variable names, which rep-
resents how likely those names appear together in a function.
Assume that we have a set of nvariable names, and the name
1168
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. ofithvariable is vni. The association score for a set Sofn
names (vn1,vn 2,...,vn n)is computed as follows.
assoc (S)=Nvn1∩vn2∩...∩vnn
Nvn1∪vn2∪...∪vnn(2)
whereNvn1∩vnn 2∩...∩vnnis the number of functions that
contain all the names (vn1,vn 2,...,vn n)in the corpus.
Nvn1∪vn2∪...∪vnnis the number of functions that contain at
least one of the names (vn1,vn 2,...,vn n)in the corpus.
For a set of nrecovered names, we deﬁne a Multiple-
variable score (MC ) that represents the likelihood of those
variable names to be assigned to the variables based on MVC.
MC can be computed using the association score. However,
due to the fact that not all possible sets of nnames appear to-
gether in a corpus, we compute MC based on the associations
of all subsets of the size JwithJ≤n, as follows:
MC vn1,vn2,...,vn n=/summationtextnsubJ
i=1assoc (Si)
nsubJ(3)
whereSiis a subset of size Jof(vn1,vn 2,...,vn n);nsubJ is
the number of such subset; and assoc (Si)is the association
score of all variable names in set Si(computed by Formula 2).
B. Deriving Candidate Names Using Multi-V ariable Context
This section presents our algorithm to derive candidate
names using multiple-variable usage context. The algorithm
takes as input a set of miniﬁed variables in a JS code in which
each variable has a set of candidate names (derived using the
single-variable context as explained in Section III-C or using
task-speciﬁc context in Section V). The output is the ranked
list of the results with associated scores. Each result is a set
of the recovered names for all of the given variables.
1) Design Strategies: In developing our algorithm, we
face three key challenges. First, each variable might have a
large number of candidates, thus, there are an exponential
combination among variables’ names. How would we deal with
such complexity to make our algorithm scale? Second, given
a set of miniﬁed variables needed to be recovered, which one
should JSN EAT start ? This is important since if the algorithm
does not recover well the ﬁrst variable, this would affect much
to the accuracy of recovering the next variables. Finally, in
which recovery order for the variables in a function would it
be beneﬁcial from multiple-variable usage context? To address
those questions, we have the following design strategies.
S1. Pruning with Beam Search. To deal with the scalability
issue of the exponential combination among all possible names
of variables, we use the Beam Search strategy: at a step during
name recovery, our algorithm keeps only the best Ksets of
partially recovered results according to the association scores.
This would help to reduce signiﬁcantly the number of partially
recovered sets that need to be considered.
S2. Starting with Variable with Most Contextual Infor-
mation. A naive answer is to use the appearance order of the
variables in the code. However, the ﬁrst variable in the code
might not be the one that we have sufﬁcient information to
recover its name. Thus, we follow the idea of using context toAlgorithm 1 Multiple-Var Name Recovery Algorithm ( MVar )
1:function MV AR(candidates [],N,Context )
2:firstVar ←Pick the ﬁrst var using Context
3:CList←candidates [firstVar ]
4: whilenRecovered < N do
5: nextVar ←Decide the next variable using S3
6: cl←candidates [nextVar ]
7: CList←BeamSearch (CList,cl )
8: ReturnCList
9:function BEAM SEARCH (CList,cand )
10: allPossiblePartialRecoveredSets ←CList⊗cand
11: for partialRes inallPossiblePartialRecoveredSets do
12: MC =CalculateScore(partialRes) via Formula 3
13:SortAll (partialRes )byMC
14:TopRankedResult ←TopKHighestScores
15: ReturnTopRankedResult
decide the initial variable for name recovery. Our intuition is
that the more context information a variable has, the more
chance we have in correctly recovering its name. JSN EAT
starts with the variable having the most single-variable us-
age information. That is, the variable has the most relations
with method calls and ﬁeld accesses. The appearance order
is used to break the tie if multiple variables have the same
number of relations in their single-variable contexts.
S3. Selecting Next Variable with Greedy Strategy. After
one or multiple variable names are recovered, we need to
determine which variable to recover next. The appearing order
in the code might not give us the optimal one. The variable
with the most single-variable context might not work either
since it might not go together well with others. In JSN EAT,
we select the next variable vwith the list of candidate names
vns that gives us the best partially recovered result. That is,
together with the selected names for the previously recovered
variables, the best possible choice for vwould give the highest
score with respect to the likelihood of the co-occurrences of
the recovered variables including v. This is a greedy strategy
that favors the variable and its candidate name that has most
co-appearances with the previously recovered variable names.
It helps avoid considering all possible candidate names for all
the variables. We use the phrase “partially recovered result”
because only a subset of all variables including vis recovered
for their names, while other variables have not been processed.
2) Detailed Algorithm: Algorithm 1 shows the pseudo-code
for our algorithm, MVar , to derive names using the multiple-
variable contexts. Given a set of Nminiﬁed variables in
which each variable has a set of candidate names candidates
(provided by a Context ,e.g., Single-variable or Task-speciﬁc
contexts), MVar determines the ﬁrst variable to start. Using
the strategy S2, it chooses the ﬁrst miniﬁed variable with the
highest score according to the context (line 2). For example,
if the single-variable context is chosen, Formula 1 is used.
All the candidates for the ﬁrst variable are initially stored
in the current candidate list CList . Then, the algorithm
1169
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. iterates to recover the variable names until all the variables
are recovered (lines 4–7). At a step of the iteration, assume
that it has recovered nRecovered variables. Using the strategy
S3, the next variable nextVar is chosen such that together
with the selected names for nRecovered previously recovered
variables, the possible names for nextVar will give the
highestMC score (Formula 3) considering the co-occurrences
of currently recovered variables (lines 5–6).
After selecting the next variable, MVar performs beam search
by ﬁrst generating all possible names for the ( nRecovered +
1) variables with combining the name candidates for nextVar
andCList (line 10). Each of those sets of names represents
a partially recovered result for those ( nRecovered +1 )
variables. The scores of all of those sets are computed (lines
11–12) using Formula 3. Then, we keep only the best Ksets of
results with highest scores and then store them in CList (lines
13–14). MVar stops when all variables have been recovered.
CList is returned as the list of best Ksets of variable names
for all the variables in the input (line 8).
In our example, using the single-variable context and/or
the task-speciﬁc context , we have the set of name candidates
for each variable, e.g., r:(dataTransfer,dataContent,...) ,
f:(elementType,dataType,contentType,...) ,p:(i, j, ...),
n:(data, cacheData, dataContent,...) , etc. The variable
ris chosen ﬁrst since its single-variable context has most
relations. All the name candidates for rare stored in CList .
After that, the next variable is nbecause in all the can-
didate names of the non-yet-recovered variables, the name
data fornwhen appearing with the current candidate name
ofrgives the highest score. In this case, the candidate
name of risdataTransfer . Then, all the sets of variable
names for (r,n) include (dataTransfer, dataContent),
(dataTransfer, cacheData), (dataTransfer, data) , etc.
Ranking these partially recovered results with Formula 3 and
keeping only the top Kones,CList includes (dataTransfer,
data), (dataTransfer, dataContent) ,... The next variable
for recovery is f. The process continues until all variables are
recovered. CList results are returned as the output.
V. T ASK -SPECIFIC CONTEXT (TSC)
In a program, a function has its functionality and is written
to realize a speciﬁc task. Each variable used in that function
plays a certain role toward that task. Thus, the names of
variables are relevant to the task of the function and often
consistent with one another. The task of a function is typically
described by a succinct name of the function. To derive a
variable name using the task context, we use the association
relation to compute how likely a variable name appears within
a function with a particular name. Given a variable name vn
and a function name fn,Task Context score (TC) represents
the likelihood that the name vnappears within the body of a
function named fn. We utilize fuzzy set theory [12] as follows.
TC vn,fn =Nvn,fn
Nvn+Nfn−Nvn,fn(4)
whereNvn,fn is the number of functions in the corpus in
which vnand fnare observed together; Nvnis the numberof functions in which vnis used; and Nfnis the number of
functions named fn. As seen in Formula 4, the value of TC is
between [0,1]. The higher the value TC vn,fn , the higher the
likelihood that the variable name vnappears in the function fn.
In Figure 1, using our experimental dataset, we can compute
TC score between the function name getClipboardContent
and the variable name dataTransfer asTC =3
21+5−3=0.13.
A function name might contain multiple tokens, e.g., get,
Clipboard ,Content ingetClipboardContent . Each token
contributes to an aspect to emphasize the common task of the
function. A variable might be relevant to one speciﬁc aspect
of the task. Thus, if we tokenize the function names, we can
account for those cases. By tokenizing, a function name fn
could be represented by a set of key tokens S={t1,t2,t3...}
(stopwords are removed) and the TC score between a variable
name vnand a function name fnis computed as follows:
TC vn,fn =m a x
t∈SNvn,t
Nvn+Nt−Nvn,t(5)
whereSis the set of key tokens of fn;tis a token in S;Nvnis
the number of functions in which vnis used;Ntis the number
of functions containing token t; andNvn,t is the number of
functions in which vnand toccur together.
VI. V ARIABLE NAME RECOVERY WITH CONTEXTS
This section presents JSN EAT, our approach to recover the
variable names in miniﬁed code using the combination of those
above contexts. Given a miniﬁed JS ﬁle, whose variables have
been miniﬁed, JSN EAT produces a recovered JS ﬁle in which
all variables are recovered with meaning names.
Algorithm 2 shows the pseudo-code for JSN EAT. First,
JSN EAT builds the relation graph representing single-variable
context (SVC) for each variable in a function and derives the
candidate list. It then computes the task-speciﬁc context (TSC)
for each variable and derives the corresponding candidate list.
The two candidate lists produced by the contexts are combined
into a new list in which the likelihood that a candidate name
vnis assigned to a variable vis computed as follows.
STv,vn =α×SC v,vn+β×TC vn,fn (6)
wherefnis the function name; TC vn,fn is task context score
between vnandfn;SC v,vn is the name matching score
between vnandv, andαandβare weighting parameters,
representing the importance of the contexts.
After this step, for each variable, we have a candidate list
(STList ) in which each candidate name has a score. JSN EAT
then uses STList as the input for MVar (Algorithm 1) to
compute the top-ranked sets of names for all variables.
Note that the scoring function for a partially recovered result
inMVar algorithm (Formula 3) needs to be adjusted to account
for the above combined score ST as follows.
MC vn1,vn2,...,vn n=γ×/summationtextnsubJ
j=1assoc (Sj)
nsubJ
+θ×STv1,vn1+STv2,vn2+...+STvn,vn n
n(7)
whereγandθare weighting parameters.
1170
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. Algorithm 2 Context-based Name Recovery Algorithm
1:function JSN EAT(MiniﬁedFile f)
2:SVCs←Build Single-Var Contexts for variables in f
3:TSCs←Compute TSC contexts for all variables in f
4:STList←Combine cand lists from SVC and TSC
5:TopResults ←MVar (STList,N,SVCs )
6: ReturnTopResults
Table I: Data Collection
Category Test Corpus Training Corpus Total
Files 2K 320K 322K
Functions 6K 961K 967K
Variables 19K 3481K 3.5M
Unique variable names 5K 171K 176K
Variables per ﬁle 9.5 10.94 10.93
VII. E MPIRICAL METHODOLOGY
To evaluate JSN EAT, we answer the following questions:
RQ1 :Comparative Study . How accurate is JSN EAT in name
recovery for miniﬁed JS code and how is it compared with the
state-of-the-art approaches, JSNice [17] and JSNaughty [21]?
RQ2 :Context Analysis . How do different combinations of
contexts contribute to JSN EAT’s accuracy in different settings?
RQ3 :Sensitivity Analysis . How do various factors affect the
accuracy, e.g., data’s sizes, thresholds, parameters, etc.?
RQ4 :Time Complexity . What is JSN EAT’s running time?
A. Corpora
We collected a corpus of 12,000 open-source JS projects
from GitHub with highest ratings. For comparison, we fol-
lowed the same procedure in previous work [17], [21] to
collect and clean up data. We removed all duplicate ﬁles to
avoid overlapping when testing. We also removed the already-
miniﬁed ﬁles because they will not help in training. Table I
shows our dataset’s statistics. As seen, the number of unique
variable names is much smaller than that of variables. Thus,
such repetition in names would help our data-driven approach.
In our comparative study, we used the same experimental
setting as in JSNice [17] and JSNaughty [21] by randomly
splitting the dataset into training and test corpora. In particular,
to build Testing Corpus , we randomly sampled 2K JS ﬁles in
the dataset. The remaining 320K ﬁles were used as Training
Corpus . The level of sizes of testing and training data (Table I)
is comparable with that of the experimental studies in existing
tools [17], [21]. We miniﬁed the ﬁles using the minifying tool
UglifyJS [20], and used the original ﬁles as oracle.
To build the relation graphs, we used Rhino to parse the JS
ﬁles and extract the context information. Table II shows the
statistics of our dataset Gof relation graphs.
B. Evaluation Setup
1) Comparative Study: For a tool under study, we trained it
with the Training Corpus and tested it against the Testing
Corpus. For JSNice, we used the publicly available tools on
their website [11] with default parameters. For JSNaughty, weTable II: Database of Relation Graphs
Category Quantity
Total number of graphs 3.5M
Mean number of graphs per ﬁle 10.93
Mean number of graphs per function 3.62
Min/Mean/Max number of edges per graph 1, 2.2, 41
Figure 4: Accuracy Comparison
trained the translation and language models, and Nice2Predict
framework following the instructions. We did not run their
tools in 10-fold cross validation due to a long running time.
2) Context Analysis: We study the impact of different
contexts, we created different variants of JSN EAT with various
combinations of contexts and measured their accuracies. We
used the 10-fold cross-validation setting on the entire corpus:
90% of the ﬁles (9 folds) are used for training and 10% of the
ﬁles (one fold) for testing, and we repeated testing for each
of the 10 folds and training with the remaining folds. We also
performed 10-fold cross validation on the project basis.
3) Sensitivity Analysis: To study different factors that have
impact on JSN EAT’s accuracy, in our entire dataset, we ran-
domly chose one fold for testing and the remaining 9 folds for
training. We studied the following factors: relation graph size,
type of relation edges, thresholds, beam sizes, different degrees
of associations, different weight parameters, and data size.
C. Procedure and Metrics
To measure the accuracy of a tool, we used UglifyJS [20]
to minify the given JS ﬁles, and used the miniﬁed code as the
input for the tool under study. We then compared the resulting
names from the tool against the original names. Speciﬁcally,
the tool is considered to correctly recover the name of a
variablevif the recovered name vnis matched exactly with
its original name. For v, if matching, we count it as a hit,
otherwise, it is a miss. Accuracy is measured by the ratio
between the total number of hits over the total number of cases.
VIII. E MPIRICAL RESULTS
A. Accuracy Comparison (RQ1)
In this study, we evaluate JSN EAT’s accuracy and compare
it with JSNice [17] and JSNaughty [21]. As seen in Fig-
ure 4, for local variables, JSN EAT achieves high accuracy of
1171
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. Figure 5: Overlapping among Results from the Tools
69.1% : relative improvements of 66.1% and 43% over JSNice
and JSNaughty, respectively. The absolute improvements are
27.5% and 20.8%, respectively. For all variables (local and
global ones), JSN EAT achieves even higher accuracy: 75.8% ,
the relative improvements of 39% and 27% over JSNice
and JSNaughty, respectively. The absolute improvements are
21.3% and 16.2%, respectively. Note that global variables are
not miniﬁed, we computed the accuracy for all variables for
the completeness purpose. From now on, using the terms “vari-
ables”, we refer to the recovery accuracy for local variables.
We further analyze the overlapping between the results
from three tools. The Vein diagram in Figure 5 shows the
percentages of variable names that are correctly recovered. As
seen, a high percentage ( 29.4% ) of variables is recovered only
by JSN EAT, while 4.7% and 3.6% of variables are recovered
only by JSNaughty and JSNice, respectively. Meanwhile, there
are 14.5% of the variables that are correctly recovered only
by JSNice or JSNaughty, and not by JSN EAT.
We also analyzed which contexts in JSN EAT contribute
to those 29.4% correctly recovered names. To do that, we
deactivated each of the three contexts. When MVC is dis-
abled , the percentage of variables that are correctly recovered
only by JSN EAT decreases by 7.4% compared to the full
version. However, when TSC is disabled , the accuracy drops
dramatically from 29.4% to 7.6%. Taking a deeper look in the
cases of 29.4%, we found out that 19% of variables that were
recovered correctly when using all contexts become incorrectly
recovered ones when TSC is off. 83% out of that 19% come
from the function with a single variable. MVC certainly would
not help in those cases because it needs the contexts from
other variables. In fact, in such functions, the function names
are quite relevant to the variable names. While SVC does
not have enough information to rank the correct name on the
top, TSC provides useful information to help in those cases.
For example, function responseJson in project mf-geoadmin3
uses a variable named response , that was miniﬁed into x.
Using only SVC and MVC, the correct name response is
ranked at position 5, but adding TSC, response is ranked at
the top. In the next experiment, we deactivated the single-Table III: Impact of Contexts on Accuracy and Recovery Time
Combination of Contexts Acc (%) Time (ms)
1 Task (TSC) 5.2 1.3
2 SingleVar (SVC) 33.7 1.5
3 Task + SingleVar 47.3 2.1
4 Task + MultiVar (MVC) 9.3 2.3
5 SingleVar + MultiVar 37.8 2.6
6Task + SingleVar + MultiVar (= JSN EAT) 63.1 3.2
variable context (SVC) , and the percentage of variables that
are correctly recovered only by JSN EAT decreases to 0.2%.
This means that SVC with property and role relations with the
pivots in the code contributes most to that 29.4% of the cases
that were not recovered correctly by the other tools.
B. Context Analysis Evaluation Results (RQ2)
As seen in Table III, using only task-speciﬁc context (TSC)
(line 1), accuracy is low because all the variables in the same
function have the same chance to be recovered with a certain
name. In contrast, the single-variable context (SVC) achieves
much higher accuracy (33.7%). This is reasonable since SVC
provides more detailed context for individual variables such as
the relations to surrounding method calls and ﬁeld accesses.
Combining TSC and SVC provides an additional improve-
ment of 13.4% over the tool with only SVC (lines 2 and 3). We
found that several correct candidate names that were ranked
in the 2nd-4th positions become the top candidates with the
addition of TSC. In contrast, the combination of TSC and
MVC yields only slight improvement over TSC (5.2 to 9.3).
The reason is that MVC takes the lists of candidate names as
its input and such lists were not initially of high quality (only
5.2% accuracy), leading to low accuracy. For the combination
of SVC and MVC, the improvement is 4.1% over SVC (lines 2
and 5). We found that those 4.1% of cases, the co-occurrences
of variable names help rank them in the top positions. Com-
paring lines 3 and 5, adding TSC to SVC improves almost
10% more than adding MVC to SVC. Further analyzing, we
found that such improvement from TSC is for the cases in
which 1) the given JS function has only one variable (MVC
cannot help) and/or 2) the SVC has only one relation (SVC
did not perform well with little surrounding context).
Finally, combining three contexts, JSN EAT achieves the
highest accuracy. Compared to TSC+SVC (lines 3 and 6),
JSN EAT relatively improves 33.4% (15.8% absolute improve-
ment). This is reasonable since the two contexts TSC and SVC
alone achieve the highest accuracy among all the combinations
of two contexts. Thus, they give MVC algorithm the initial
candidate lists for variables with higher quality. Then, MVC
with the co-occurrence information among variables helps an
additional improvement of 15.8%. Moreover, comparing lines
5 and 6, TSC helps improve 25.3% since TSC helps in the
cases of single-variable functions or single-edge RGs.
To evaluate JSN EAT’s consistency in achieving high accu-
racy, we performed 10-fold cross validation. As seen in Table
IV , the accuracies for all the folds are stable (61.9–63.8%),
with the recovery time of 2.92 ms for a ﬁle. The results for 10-
fold cross validation on project basis are similar (not shown).
1172
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. Table IV: 10-fold Cross-Validation Evaluation on JSN EAT
Test Fold 0 1 2 3 4 5 6 7 8 9 Mean
Acc (%) 63.1 63.0 63.1 61.9 63.3 62.7 63.8 63.4 62.9 62.5 62.9
Time 2.9 2.92 2.86 2.83 2.95 2.99 2.92 2.95 3.01 2.85 2.92
Table V: Impact of Relation Graphs’ Sizes on Accuracy
Number of edges 1 2 3 4 5 >5 All
% of graphs 46.4 24.8 12.8 6.6 3.5 5.7 100
Accuracy (%) 58.5 65.7 67.4 68.4 68.7 69.8 63.1
C. Sensitivity Analysis Evaluation Results (RQ3)
1) Impact of Relation Graphs’ Sizes: To measure the im-
pact of the sizes of SVC, we selected in the corpus only certain
sizes of RGs (measured by the number of edges). As seen in
Table V , the more relations in SVC to be considered (more
edges a relation graph has), the more accurate a variable name
can be recovered. When having more than 4 edges, accuracy
becomes stable at a high level and gradually increases. This
also reafﬁrms our strategy S1 in selecting the ﬁrst variable
with most connecting edges in RGs.
2) Impact of Type of RG Edges: By omitting only certain
type of edges in RGs for SVC, we measured the impact of
each type of relations on accuracy. In Table VI, the lower
the accuracy, the higher the impact the corresponding relation
has. As seen, all the relation types contribute nearly equally
to JSN EAT. If one type of relation is not considered, accuracy
drops from 63.1% to around 45%. The argument relation has
a slightly higher contribution than the others.
3) Impact of Threshold for Graph Matching: We evaluated
the impact of the threshold ϕused to measure the similarity
between two RGs ( i.e., two SVCs). To do that, we used only
SVC to recover variable names with varied ϕ[0.5–1.0]. As
seen in Table VII, when ϕ=0.8, the accuracy is at the highest.
Withϕ< 0.8, the number of variables whose contexts are
matched with the miniﬁed variable is large, and the correct
name was not ranked at the top. When ϕ> 0.8, the condition
is too strict and the correct names were dropped because it is
not easy to ﬁnd a completely matched context.
4) Impact of Beam Size: In JSN EAT, beam size is used to
deal with the large combinations of possible names. Figure 6
shows the accuracy and running time per variable when we
varied the beam sizes. As seen, when the beam size is small,
the accuracy is very low. It is expected because pruning occurs
frequently, the number of results that were kept is smaller, and
the best candidates might be dropped out of the beam stack. As
the beam size is increased, accuracy increases and reaches the
highest point (around 63%) with the beam size of 30. Accuracy
becomes stable when the beam size is greater than 30. The
reason is that almost all the correct names are observed in the
top 30 results. Therefore, when we increase beam size over 30,
accuracy is not affected anymore. Regarding the running time,
the higher the beam size, the larger the number of candidate
results, and the higher the running time. Thus, we used 30 for
the beam size in other experiments.
5) Impact of Association Scores: In MVC, JSN EAT consid-
ers the co-occurrences of J= 2, 3, etc or nvariable names. TheTable VI: Impact of Relation Types in RGs on Accuracy
argument assignment ﬁeldAccess methodCall
Accuracy (%) 42.9% 44.5% 48.5% 45.7%
Table VII: Impact of Threshold ϕon Accuracy
Threshold ϕ 0.5 0.6 0.7 0.8 0.9 1.0
Accuracy (%) 27.1 27.4 30.6 33.7 31.2 30.8
Time 1.8 1.7 1.5 1.5 1.4 1.4
value of Jshows how many associations of variable names
that we need to have high accuracy. In Table VIII, using high-
degree association, accuracy decreases gradually since ﬁnding
co-occurrences of nvariable names has a lower probability
than ﬁnding the co-occurrences of n-1 variable names, i.e., the
co-occurrence condition is too strict. The decrease in accuracy
is not much since the high-degree association affects only a
smaller set of cases with higher numbers of variables in a
function. As expected, running time increases.
6) Impact of Parameters in Context Combinations: To
combine multiple contexts, we use parameters to put weights
on each of them, e.g.,αfor SVC, βfor TSC, and γfor MVC.
We varied their values to observe the impacts of the contexts.
In Table IX, when combining SVC and TSC, if the weight of
SVC is higher, accuracy is higher. In a function, task context
plays an equal role to all variables in that function, while
SVC provides directly related information to the variable.
Combining SVC, TSC, and MVC, the higher γ, the higher the
accuracy. This means that MVC contributes more important
information than SVC and TSC in the Formula 7.
7) Impact of Training Data Size: To measure impact of data
size, we used one fold for testing and increased the sizes of the
Training dataset by adding one fold at a time until 9 remaining
folds are added for training. We ran JSN EAT on each training
dataset with the best settings for SVC and MVC, and two
settings of TSC: tokenizing or using full function names. In
Table X, the accuracies in both settings increase linearly
and consistently with the training size. With a small size,
tokenizing function names gives better accuracy than using full
names. However, when the number of training folds is more
than 5 folds, using full function name is better. The reason is
that when the data’s size is large enough, the probability of co-
appearances between a function name and a variable name is
higher, the candidate names for a variable can be found better.
When data’s size is small, JSN EAT might not see a variable
name and a function name appearing together, then tokenizing
function names would give more useful context.
D. Time Complexity (RQ4)
All experiments were run on a Linux server with 20 Intel
Xeon 2.2GHz processors, 256GB RAM. In Table XI, the time
to recover for a ﬁle or for a variable with JSN EAT istwice
as fast as with JSNice and 4x as fast as with JNaughty . More
importantly, JSN EAT ’s training time is 4x faster than JSNice
and 6x faster than JSNaughty . This can be achieved due to
the nature of information retrieval in JSN EAT, in comparison
to machine learning in JSNice and JSNaughty.
1173
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. Figure 6: Impact of Beam Size on Accuracy and Running Time
Table VIII: Impact of Assoc Score Jon Accuracy and Time
J Accuracy (%) Time (ms) Perc. Found (%)
Pairwise (J =2) 63.1 2.88 68.5
Triple (J =3) 61.8 3.11 55.2
J=4 60.2 3.26 25.7
J=6 58.6 3.41 14.9
J=8 57.5 3.45 5.8
J=1 0 57.4 3.47 2.7
E. Limitations and Threats to V alidity
1) Limitations: First, as a data-driven approach, unseen
data affects our accuracy. For example, with 1-fold training
data, 33.4% of miniﬁed names have not been observed.Second, for the task context, JSN
EAT did not work well for
functions with general names, e.g., next ,find , etc. More
sophisticated solution could involve topic modeling [4] onthe function body. Third, for MVC, greedy strategy might notachieve the optimal result. Finally, if two variables in the samefunction are assigned with the same name (e.g., same SVC,
MVC, and TSC), we randomly pick different names. Programanalysis could be applied here to improve accuracy.
2) Threats to V alidity: Our corpus of JS code might not be
representative, however, we chose a large corpus with the sizecomparable with those in previous studies. We used only thetool Uglify to minify the code, which was also used in JSNiceand JSNaughty. We do not study the usefulness involvinghuman subjects. However, for comparison, we used the sameexperimental settings as in JSNice [17] and JSNaughty [21].
IX. R
ELATED WORK
JSN EAT is closely related JSNice [17] and JSNaughty [21].
JSNice [17] uses the graph representation of variables andsurrounding program entities via program dependencies. Itinfers the variable names as a problem of structured predictionwith conditional random ﬁelds (CRFs) [17]. In comparison,ﬁrst, while JSNice uses ML, JSN
EAT is IR-based in which it
searches for a list candidate names in a large code corpus. Sec-ond, JSN
EAT considers not only the impacts of surrounding
program entities in SVC, but also task and multiple-variablecontexts. Third, with CRF, JSNice is effective when variableshave more dependencies, and less effective with the functionsTable IX: Sensitivity Analysis on Combination Parameters
αβ Accuracy (%) θγ Accuracy (%)
1 0 45.4 1 0 57.9
0.25 0.75 45.9 0.75 0.25 58.5
0.5 0.5 46.5 0.5 0.5 60.2
0.75 0.25 47.3 0.25 0.75 61.7
0 1 46.7 0 1 63.1
Table X: Impact of Training Data’s Size on Accuracy
#Folds 1 2 3 4 5 6 7 8 9
Acc.Full 40.1 47.3 52.5 54.9 56.4 58.2 60.0 62.2 63.1
Acc.Token 42.3 48.5 53.6 55.1 56.5 57.3 57.9 58.5 59.1
Table XI: Running Time Comparison
Metric JSN EAT JSNice JSNaughty
Training 2h05m 8h35m 12h25m
Per-ﬁle Recovery 32ms 72ms 129ms
Per-variable Recovery 2.9ms 6.6ms 11.8ms
having one variable. Finally, JSN EAT is much faster and the
results are more accurate as shown in Section VIII.
JSNaughty [21] formulates name recovery as a statistical
machine translation from the miniﬁed code to the recoveredcode. First, due to the nature of ML, it faces the scalabilityissue in much higher time complexity. Second, JSNaughty usesa phrase-based translation model, which enforces a strict orderbetween the recovered variable names in a function. This is toostrict since a name of a variable might not need to occur beforeanother name of another variable. Third, JSNaughty does notconsider the task context of the variables. Finally, our train-ing/testing time is much faster. In contrast, other deobfuscationmethods use static/dynamic analyses [5], [13], [19].
Statistical NLP approaches have been used in SE. Natural-
ize [1] enforces a consistent naming style. Other applicationsof statistical NLP include code suggestion [9], [14], codeconvention [1], method name suggestion [2], API sugges-tions [18], code mining [3], type resolution [15], patternmining [6]. Statistical NLP was used to generate code fromtext, e.g., SWIM [16], DeepAPI [7], Anycode [8], etc.
X. C
ONCLUSION
This paper presents JSN EAT, an IR-based approach to
recover the variable names in miniﬁed JS code. We follow adata-driven approach by searching for names in a large corpusof open-source JS code. We use three types of contexts tomatch a variable in given miniﬁed code against the corpus.Our IR approach enables us to achieve high accuracy with lesstime complexity than the state-of-the-art approaches. JSN
EAT
achieves a high accuracy of 69.1%: the improvement of 66.1%and 43% over JSNice and JSNaughty, respectively. The time torecover for a ﬁle or for a variable with JSN
EAT is twice as fast
as with JSNice and 4x as fast as with JNaughty, respectively.
ACKNOWLEDGMENT
This work was supported in part by the US National Science
Foundation (NSF) grants CCF-1723215, CCF-1723432, TWC-1723198, CCF-1518897, and CNS-1513263.
1174
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton. Learning natural
coding conventions. In Proceedings of the International Symposium on
F oundations of Software Engineering , FSE 2014, pages 281–293. ACM,
2014.
[2] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton. Suggesting accurate
method and class names. In Proceedings of the 10th Joint Meeting on
F oundations of Software Engineering , ESEC/FSE 2015, pages 38–49.
ACM, 2015.
[3] M. Allamanis and C. Sutton. Mining source code repositories at massive
scale using language modeling. In Proceedings of the 10th IEEE
Working Conference on Mining Software Repositories (MSR’13) , pages
207–216. IEEE CS, 2013.
[4] D. M. Blei, A. Y . Ng, and M. I. Jordan. Latent dirichlet allocation. J.
Mach. Learn. Res. , 3:993–1022, Mar. 2003.
[5] M. Christodorescu and S. Jha. Static analysis of executables to detect
malicious patterns. In Proceedings of the 12th Conference on USENIX
Security Symposium - V olume 12 , SSYM’03, pages 12–12. USENIX
Association, 2003.
[6] J. M. Fowkes and C. A. Sutton. Parameter-free probabilistic API mining
at github scale. CoRR , abs/1512.05558, 2015.
[7] X. Gu, H. Zhang, D. Zhang, and S. Kim. Deep API learning. In
Proceedings of the 24th ACM SIGSOFT International Symposium on
F oundations of Software Engineering , FSE 2016, pages 631–642. ACM,
2016.
[8] T. Gvero and V . Kuncak. Synthesizing Java expressions from free-
form queries. In Proceedings of the 2015 ACM SIGPLAN International
Conference on Object-Oriented Programming, Systems, Languages, and
Applications , OOPSLA 2015, pages 416–432. ACM, 2015.
[9] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. On the
naturalness of software. In Proceedings of the 2012 International
Conference on Software Engineering , ICSE 2012, pages 837–847. IEEE
Press, 2012.
[10] JSNeat. https://mrstarrynight.github.io/JSNeat/.
[11] JSNice. https://ﬁles.sri.inf.ethz.ch/jsniceartifact/index.html.[12] G. J. Klir and B. Yuan. Fuzzy sets and fuzzy logic: Theory and
applicationss. Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1995.
[13] A. Moser, C. Kruegel, and E. Kirda. Exploring multiple execution paths
for malware analysis. In Proceedings of the 2007 IEEE Symposium on
Security and Privacy , SP ’07, pages 231–245. IEEE Computer Society,
2007.
[14] L. Mou, G. Li, Z. Jin, L. Zhang, and T. Wang. TBCNN: A tree-based
convolutional neural network for programming language processing.
CoRR , abs/1409.5718, 2014.
[15] H. Phan, H. A. Nguyen, N. M. Tran, L. H. Truong, A. T. Nguyen, and
T. N. Nguyen. Statistical learning of API fully qualiﬁed names in code
snippets of online forums. In Proceedings of the 40th International
Conference on Software Engineering , ICSE ’18, pages 632–642. ACM,
2018.
[16] M. Raghothaman, Y . Wei, and Y . Hamadi. SWIM: Synthesizing what i
mean: Code search and idiomatic snippet synthesis. In Proceedings of
the 38th International Conference on Software Engineering , ICSE ’16,
pages 357–367. ACM, 2016.
[17] V . Raychev, M. Vechev, and A. Krause. Predicting program properties
from "big code". In Proceedings of the 42nd Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages , POPL
’15, pages 111–124. ACM, 2015.
[18] V . Raychev, M. Vechev, and E. Yahav. Code completion with statistical
language models. In Proceedings of the 35th ACM SIGPLAN Conference
on Programming Language Design and Implementation , PLDI ’14,
pages 419–428. ACM, 2014.
[19] S. K. Udupa, S. K. Debray, and M. Madou. Deobfuscation: Reverse
engineering obfuscated code. In Proceedings of the 12th Working
Conference on Reverse Engineering (WCRE’05) , pages 45–54. IEEE
Computer Society, 2005.
[20] Uglify. https://github.com/mishoo/UglifyJS.
[21] B. Vasilescu, C. Casalnuovo, and P. Devanbu. Recovering clear, natural
identiﬁers from obfuscated js names. In Proceedings of the 11th Joint
Meeting on F oundations of Software Engineering , ESEC/FSE 2017,
pages 683–693. ACM, 2017.
1175
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:41:52 UTC from IEEE Xplore.  Restrictions apply. 