Context-Aware Conversational Developer Assistants
Nick C. Bradley
Department of Computer Science
University of British Columbia
V ancouver, Canada
ncbrad@cs.ubc.caThomas Fritz
Department of Informatics
University of Zurich
Zurich, Switzerland
fritz@iï¬.uzh.chReid Holmes
Department of Computer Science
University of British Columbia
V ancouver, Canada
rtholmes@cs.ubc.ca
ABSTRACT
Building and maintaining modern software systems requires devel-
opers to perform a variety of tasks that span various tools and infor-
mation sources. The crosscutting nature of these development tasks
requires developers to maintain complex mental models and forces
them (a) to manually split their high-level tasks into low-level com-
mands that are supported by the various tools, and (b) to (re)establish
their current context in each tool. In this paper we present Devy, a
Conversational Developer Assistant (CDA) that enables developers
to focus on their high-level development tasks. Devy reduces the
number of manual, often complex, low-level commands that devel-
opers need to perform, freeing them to focus on their high-leveltasks. Speciï¬cally, Devy infers high-level intent from developerâ€™s
voice commands and combines this with an automatically-generated
context model to determine appropriate workï¬‚ows for invoking low-
level tool actions; where needed, Devy can also prompt the developer
for additional information. Through a mixed methods evaluation
with 21 industrial developers, we found that Devy provided an in-
tuitive interface that was able to support many development tasks
while helping developers stay focused within their development envi-
ronment. While industrial developers were largely supportive of the
automation Devy enabled, they also provided insights into severalother tasks and workï¬‚ows CDAs could support to enable them to
better focus on the important parts of their development tasks.
CCS CONCEPTS
â€¢Software and its engineering â†’Integrated and visual develop-
ment environments;
KEYWORDS
Conversational Development Assistants, Natural User Interfaces
ACM Reference Format:
Nick C. Bradley, Thomas Fritz, and Reid Holmes. 2018. Context-Aware
Conversational Developer Assistants. In Proceedings of ICSE â€™18: 40th
International Conference on Software Engineering , Gothenburg, Sweden,
May 27-June 3, 2018 (ICSE â€™18), 11 pages.
https:// doi.org/ 10.1145/ 3180155.3180238
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner /author(s).
ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
Â©2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5638-1/18/05. . . $15.00
https:// doi.org/ 10.1145/ 3180155.31802381 INTRODUCTION
Software development is hard. Empirical studies have shown that
developers have to perform a broad variety of development tasks,frequently switching applications and contexts [
5,13,14]. To suc-
cessfully complete these higher level tasks, developers must performa series of low-level actionsâ€”workï¬‚owsâ€”and maintain mental mod-
els that combine data from a variety of disparate sources including
the source code of the system, version control, issue trackers, test
executions, and discussion threads [8, 9, 11].
While it would beneï¬t developers to automate these workï¬‚ows, it
is challenging for three reasons. First, it is hard to determine a prioriall the tasks and workï¬‚ows developers will need to complete. Second,
these workï¬‚ows consist of various low-level actions that often span
across tool boundaries and require a diverse set of parameters that
depend on the current context and developer intent. Third, even if
there are scripts conï¬gured for automating workï¬‚ows, the developerneeds to remember their existence and how to invoke them manually
in the current context.
In this paper, we explore the potential of conversational agents
to support and automate common development workï¬‚ows. We de-
signed a conversational developer assistant (CDA) that (a) provides
aconversational interface for developers to specify their high-level
tasks in natural language, (b) uses an intent service to automatically
map high-level tasks to low-level development actions, and (c) auto-
matically tracks developersâ€™ actions and relevant state in a context
model to automate the workï¬‚ows and speciï¬cation of parameters.
The CDA allows developers to express their intent conversationally,
eliminating the need for learning and remembering rigid syntax,
while promoting discoverability and task ï¬‚exibility. The automatic
mapping and execution of workï¬‚ows based on the developerâ€™s high-
level intent, augmented by the context model, reduces developersâ€™
cognitive eï¬€ort of breaking down high-level intents into low-level ac-
tions, switching context between disparate tools and parameterizing
complex workï¬‚ows.
In order to conduct a meaningful industrial evaluation of the
feasibility, usability, and potential use cases of CDAs in software
development, we implemented Devy, a prototype voice-controlled
CDA with a pre-deï¬ned set of automated Git and GitHub tasks.Devyâ€™s primary goal is to help developers maintain their focus on
their development tasks, enabling them to oï¬„oad low-level actions
to an automated assistant.
We performed a mixed methods studyâ€”a combination of an in-
terview and an experimentâ€”with 21 industrial software engineers
using Devy as a technology probe. Participants had the opportunity
to interact with our Devy prototype so they could o ï¬€er concrete
feedback about alternative applications of CDAs to their industrial
workï¬‚ows. Each engineer performed multiple experimental tasks
9932018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Nick C. Bradley, Thomas Fritz, and Reid Holmes
Table 1: Steps for the common â€˜share changesâ€™ workï¬‚ow.
(a) Manual steps.
(a) Open a web browser for the issue tracker and check the
issue number for the current work item.
(b) Open a terminal and run the tests against the changed code
to ensure they work (e.g., npm run tests).
(c) Open a terminal and commit the code, tagging it with
the current work item number (e.g., git commit -m â€˜See
issue #1223â€™).
(d) Pull any external changes from the remote repository (e.g.,
git pull).
(e) Push the local change to the remote repository (e.g., git
push).
(f) Open the commit in the version control system using the
GitHub web interface and open a pull request.
(g) Determine a set of reviewers and assign them to the pull
request with the GitHub web interface.(b) Literal CDA steps.
â€œCDA, Create a branch â€˜issue1223â€™ in the FrontEnd repo.â€
â†’ â€œBranch created.â€
â€œCDA, Run all the tests in the FrontEnd repo.â€
â†’ â€œTests executing.â€
â€œCDA, Commit with â€˜Fix #1223â€™ in the FrontEnd repo.â€
â†’ â€œCommit made.â€
â€œCDA, Pull the FrontEnd repo.â€
â†’ â€œPulled.â€
â€œCDA, Push the FrontEnd repo.â€
â†’ â€œPushed.â€
â€œCDA, Open GitHub for the FrontEnd repo and create a pull
request for branch issue1223.â€
â†’ â€œPull request created.â€
â€œCDA, Open GitHub for the FrontEnd repo and add alice79 as
a reviewer for the issue1223 pull request.â€
â†’ â€œReviewer added.â€
with Devy and answered a series of open-ended questions. The eval-
uation showed that engineers were able to successfully use Devyâ€™s
intent-based voice interface and that they saw promise in this type
of approach in practice.
This feedback provides evidence of the potential and broad appli-
cability of both Conversational Developer Assistants and developerâ€™s
interest in increased automation of their day-to-day workï¬‚ows.
The primary contributions of this paper are:
â€¢A context model and conversational agent to support auto-
mated development assistants.
â€¢Devy, a prototypical voice-activated CDA that infers devel-
oper intent and transforms it into complex workï¬‚ows.
â€¢A mixed methods study demonstrating the value of this ap-
proach to industrial developers and providing insight into how
CDAs can be used and extended in the future.
We describe a concrete scenario in Section 2, our approach in Sec-
tion 3 and our experiment in Sections 4 and 5. Related work, discus-
sion, and conclusions follow in Sections 6â€“8.
2 SCENARIO
Development projects often use complex processes that involve
integrating numerous tools and services. To perform their high-level
tasks, developers need to break down their intent into a list of atomic
actions that are performed as part of a workï¬‚ow. While the intentmay be compact and simple, workï¬‚ows often involve interacting
with a variety of diï¬€erent tools.
Consider a developer whose task is to submit their source code
changes for review, which requires using version control, issue track-
ing, and code review tools. At a low level, the developer needs to:commit their changes, push them to a remote repository, link thechange to the commit in the issue tracker, and assign reviewers inthe code review system. Our context model is able to track what
project the developer is working on, what issue is currently active,
and who common reviewers for the changed code are in order to
enable the developer to just say â€œDevy: Iâ€™m doneâ€ to complete thisfull workï¬‚ow without having to change context between diï¬€erent
tools.
To perform this task manually, the developer must follow a work-
ï¬‚ow similar to that shown in Table 1a (for simplicity, we illustrate
this workï¬‚ow using GitHub). In this scenario, developers use three
tools: GitHub (Table 1a-(a),(f),(g)), the test runner (Table 1a-(b)),
and git (Table 1a-(c),(d),(e)). They also performed four overlapping
subtasks: running the tests (Table 1a-(b)), linking the commit (Ta-
ble 1a-(a),(c)), managing version control (Table 1a-(c),(d),(e)), and
conï¬guring the code for review (Table 1a-(f),(g)). In addition, they
relied on several pieces of implicit contextual knowledge: (1) the
repository being used, (2) the current issue, (3) how to run the tests,
(4) the projectâ€™s commit linking protocol, and (5) the projectâ€™s code
review assignment protocol.
Providing a voice-activated CDA for this workï¬‚ow without any
additional abstraction (or context model) oï¬€ers little beneï¬t as shownby the transcript in Table 1b which has been aligned with the manual
steps in Table 1a. Grey rows are the developerâ€™s speech, white rows
are the CDA â€™s responses. This implementation has obvious short-
comings: it provides no meaningful beneï¬t over just accessing the
commands directly, as the developer must say all of the commands
with the right names, in the right order, with the right parameters,
and it would no doubt be faster if they performed the actions directly.
Automating this workï¬‚ow would require knowing the ï¬ve pieces
of the contextual information along with knowledge of how to use
the three tools employed by the developer. Fortunately, these are all
pieces of information that are tracked directly by the context model
of our conversational developer assistant Devy. The same workï¬‚ow
can be completed using Devyâ€™s verbal natural language interface:
Dev Devy, Iâ€™m done.
Devy Y ou have uncommitted changes. Should I commit them?
Dev OK.
Devy OK, Iâ€™m about to open a pull request, should I assign Alice?
Dev Y eah.
994
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Context-Aware Conversational Developer Assistants ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
During this interaction, the developer did not need to use any
other tools or switch their context away from their source code. The
context model automatically tracked the project being worked on,
the ï¬les being changed, and the current issue number. To show the
results of the tests (Table 1a-(b)), Devy appends the output of the
test execution as a comment to the pull request thread when it is
complete. To identify the list of appropriate reviewers (Table 1a-(g)),
Devy is able to query a simple service that examines past reviewers
for the code involved in the developerâ€™s change.
While the developerâ€™s intent, submitting changes, is simple, it
can only be realized through the indirect process listed above that
involves interacting directly with the version control system, issue
tracker, test execution environment, and code review system. Each
of these systems incurs their own mental and temporal costs, and
provides opportunities for a teamâ€™s workï¬‚ow to be ignored (e.g., if
new team members are not aware of all steps, or an experienced one
skips a step). Ultimately, this task involves four context switches
between the issue tracker, the version control system, the pull request
interface, and the code review system; Devy abstracts away this
minutiae so the developer can focus on their high level intent.
3 APPROACH
Devy, our conversational developer assistant (CDA), has three main
components: a conversational user interface, a context model, and
an intent service. Developers express their intent using natural lan-
guage. Our current prototype uses Amazon Echo devices and the
Amazon Alexa platform to provide the conversational interface; this
interface converts developer sentences into short commands. These
commands are passed to our intent service which runs on the devel-
operâ€™s computer. The context model actively and seamlessly updatesin the background on the developerâ€™s computer to gather information
about their activities. Using the context model and the short com-
mands from the conversational interface, the intent service infers
a rich representation of the developerâ€™s intent. This intent is then
converted to a series of workï¬‚ow actions that can be performed for
the developer. While the vast majority of input to the intent service
is derived from the context model, in some instances clariï¬cation is
sought (via the conversational interface) from the developer. Devyâ€™s
architecture is shown in Figure 1.
3.1 Conversational Interface (Devy Skill)
The conversional interface plays a crucial role by allowing develop-
ers to express their intents naturally without leaving their develop-
ment context. The Devy Skill has been implemented for the Amazon
Alexa platform (apps on this platform are called skills). To invoke
the Devy Skill, a developer must say:
â€œAlexa, ask Devy to ...â€
They can then complete their phrase with any intent they wish to
express to Devy. The Amazon microphones will only start recording
once they hear the â€˜Alexaâ€™ word, and the Devy skill will only be
invoked once â€˜Devyâ€™ has been spoken.
The Amazon natural language APIs translate the developerâ€™s con-
versation into a JSON object; to do this, the Devy skill tells the
Amazon Alexa platform what kinds of tokens we are interested
in. We have provided the platform with a variety of common ver-sion control and related development â€˜utterancesâ€™ we identiï¬ed in
Conversation
Layer
Developer's Computer
Natural
Language
Standard
Development
Behaviour
EchoAmazon
NLPDevy
Skill
Context
Model...
Workï¬‚ow
ActionsIntent 
Service
Figure 1: Devyâ€™s architecture. A developer expresses their inten-
tion in natural language via the conversational layer. The intent
service translates high-level language tokens into low-level con-crete workï¬‚ows which can then be automatically executed forthe developer. Dotted edges predominantly communicate in thedirection of the arrow, but can have back edges in case clariï¬ca-tion is needed from the user.
the literature and from discussions with professional developers;
many utterances also have synonyms (e.g., for â€˜pushâ€™, we also in-
clude â€˜submitâ€™, and â€˜sendâ€™). For a sentence like â€œAlexa tell Devy to
push.â€ the Amazon JSON object would contain one primary ï¬eld
intent.name with the value â€˜pushChangesâ€™.
While Alexa has been useful for constructing our prototype, it
imposes two restrictions that hinder our approach:
(1) The requirement to use two names, â€œAlexaâ€ and â€œDevyâ€ is
cumbersome.
(2) More technically, Alexa doesnâ€™t allow push notiï¬cations and
requires the client app to respond within ten seconds; both of
which cause issues for long running commands.
While our current approach uses the Amazon APIs for voice input,
using a text-based method (e.g., a ChatBot) would also be feasible
for scenarios where voice-based input is not appropriate.
3.2 Context Model
The context-aware development model represents the â€˜secret sauceâ€™
that enables advanced voice interactions with minimal explicit devel-
oper input. The diï¬€erences between manual CDA and context-aware
CDA (Devy) approaches are exempliï¬ed in Section 2. The model
acts as a knowledge base allowing the majority of the parameters re-
quired for performing low-level actions to be automatically inferred
without developer intervention. In cases where required information
is not present in the context model, it can be prompted from the
developer using the conversational interface.
The context model for our prototype system is described in Ta-
ble 2. The current model supports version control actions and online
code hosting actions. Our prototype tool includes concrete bindings
for Git and GitHub respectively for these two roles but also supports
other semantically similar systems such as Mercurial and BitBucket.
While other sets of information can be added to the model, these
were suï¬ƒcient for our current prototype.
TheActiveFile model parameter is the most frequently up-
dated aspect of the model. As the developer moves from ï¬le to ï¬le
995
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Nick C. Bradley, Thomas Fritz, and Reid Holmes
Table 2: Context Model elements.
Current Focus
ActiveFile
Each Local Repository
Path
Version Control TypeOriginURLUserNameCurrentBranchFileStatus
Each Remote Repository
OpenAssignedIssues[]Collaborators[]
Other Services
BlameServiceTestServiceReviewerAssignmentService
in any text editor or IDE, the full path of the active ï¬le is noted
and persisted in the ActiveFile ï¬eld. The context model is also
populated with information about all version control repositories it
ï¬nds on the developerâ€™s machine. From these local repositories, us-
ing theOriginURL , it is also able to populate information about the
developerâ€™s online code hosting repositories. The path component
ofActiveFile lets the model index into the list of version control
repositories to get additional details including the remote onlinerepository, if applicable. Our prototype also allows developers toexclude speciï¬c repositories and paths from being tracked by the
context model and only minimal information about the state of the
command is exchanged with Amazon to ensure the privacy of the
user.
We designed our context model to pull changes from a developerâ€™s
computer when they interact with Devy. Due to the limited size of ourcontext model, the pull-based architecture is suï¬ƒcient. However, for
more advanced models, a push-based architecture where the model
is initialized at startup and continuously updated by external change
events would be preferable to avoid delaying the conversational
interface.
Extending the model is only required if the developer wishes to
support workï¬‚ows that require new information. Model extensions
can be either in terms of pre-populated entries (push-based above),
or pointers to services that can be populated on demand (pull-based).
For example, the TestService , which takes a list of ï¬les and re-
turns a list of tests that can run, can be pointed to any service that
conforms to this API (to enable easy customization of test selection
algorithms). If developers wanted more ï¬ne-grained informationsuch as the current class, method, or ï¬eld being investigated, they
could add a relevant entry to the model and populate it using some
kind of navigation monitor for their current text editor or IDE.
3.3 Intent Service
The intent service does the heavy lifting of translating the limited
conversational tokens and combining it with the context model to
determine the developerâ€™s intent. This intent is then executed for theReadypull
stashcommitpush
commit?commit?stash?
pull
pull
pushbehind:
pull ï¬rstListingIssuesGettingOwner
CreatingPRListingUsers
owner
pull
request
get
issue
Branching branchlist
issuescreate
pull
requestcreate
pull
request
pushget
issue
CommittingPushingStashingPullingRunTests
testreport
on PRrun testsassign
reviewers
Figure 2: Devyâ€™s ï¬nite state machine for handling workï¬‚ows.Stack-push transitions are shown with solid lines while stack-
pop transitions are shown with dotted lines. For readability,
some arrows do not connect with their state. However, all linesare labelled with the action that causes the state transition andcorrespond to the next state. Edges between the FSM and theContext Model are elided for clarity.
developer in the form of workï¬‚ow actions. The context model is
updated as the actions execute since their outcomes may inï¬‚uence
subsequent steps of the workï¬‚ow.
The conversational layer provides the intent service with ex-
tremely simple input commands (e.g., a single verb or noun). The
intent service uses a stack-based ï¬nite state machine (FSM) to reason
about what the input command means in this context. While more
constrained than plan-based models, FSMs are simple to implement
and are suï¬ƒcient for the purposes of evaluating the potential of
CDAs. The complete FSM for our version control intent service is
shown in Figure 2. Within the FSM, transitions between states deï¬ne
workï¬‚ow steps while states contain the logic needed to prepare and
execute low-level actions. Each state is aware of other states that
may need to be executed before they can successfully complete (e.g.,a pull may be required before a push if the local repository is behindthe remote repository). We use a stack-based FSM because workï¬‚ow
actions frequently depend on each other. By using a stack, we are
able to just push commands on the stack and allow the execution to
return to the right place in an understandable way. These potential re-turn edges are denoted by the dotted arrows in Figure 2; for example,
Stashing can be accessed either directly by the developer from the
Ready state, or as a consequence of a Pulling precondition. The
states in the FSM make heavy use of the context model to provide
values for their parameters.
4 STUDY METHOD
The long-term objective of our research is to enable intent-based
workï¬‚ows without software developers having to continuously map
their intents to low-level commands. Therefore, we are investigating
996
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Context-Aware Conversational Developer Assistants ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
when and how software developers would use a conversational de-
veloper assistant that supports intent-based workï¬‚ows. Speciï¬cally,
we are examining the following research questions:
RQ1 How well can a conversational developer assistant approach
support basic development tasks related to version control?
RQ2 For which workï¬‚ows would a conversational developer assis-
tant be useful to developers and why?
To answer our research question, we developed the voice-enabled
CDA, Devy, as a prototype (see Section 3), piloted it with several
student developers, and then conducted a mixed methods study with
21 professional software developers. The study was a combination
of an experiment with Devy and semi-structured interviews.
4.1 Participants
We recruited 21 professional software developers (2 female, 19 male)
from 6 local software companies of varying size. Participants had
an average of 11 (Â±8) years of professional development experience
and an average of 15 ( Â±11) years1of programming experience.
Participants were classiï¬ed as either junior developers (8) or senior
developers (13) based on job title. All participants had experience
using version control systems and all but 1 had experience with Git.
Participants were recruited through personal contacts and recruit-
ing emails. To pique interest and foster participation, we included
a short video2introducing Devy and demonstrating it with the use
case of determining the owner of an open ï¬le. In addition, we in-
centivized participation with a lottery for two Amazon Echo Dots
amongst all participants. To participate in our study, subjects had to
be software developers and be familiar with some version control
system.
4.2 Procedure
The study consisted of three parts: (1) a brief semi-structured inter-
view to ask about developersâ€™ tasks and workï¬‚ows as well as about
the possible value of a conversational assistant to support these, (2)
an experiment with Devy comprised of two study tasks, and (3) afollow-up semi-structured interview on the experience and use ofa CDA. We piloted our study and adapted the procedure based onthe insights from our pilots. We chose this three step procedure to
stimulate developers to think about a broad range of workï¬‚ows and
how a CDA might or might not help, as well as to avoid priming the
participants too much and too early with the functionality that ourcurrent Devy prototype provided. The order and sample questionsof the parts of our study are illustrated in Table 3. The study was
conducted in quiet meeting rooms at the participantâ€™s industrial site.
Interview (Part One). To have participants reï¬‚ect upon their work
and workï¬‚ows, we started our ï¬rst semi-structured interview byasking general questions about participantsâ€™ work days and then
more speciï¬cally about the tasks they are working on as well as the
speciï¬c steps they perform for these tasks (see Table 3 for sample
questions). We then introduced Amazonâ€™s Alexa to participants. To
get participants more comfortable with interacting with Alexa, we
had them ask Alexa to tell them a joke. Next, we asked participants
1Missing data for two participants.
2http://soapbox.wistia.com/videos/HBzPb4ulqlQITable 3: Order, sample questions, and tasks from our mixed
methods study.
Interview - Part One
1.1 Walk me through typical development tasks you work on every day.
1.2 How do you choose a work item; what are the steps to complete it?
1.3 How do you debug a failed test?
2 To help you get familiar with Alexa, ask Alexa to tell us a joke.
3 Can you think of any tasks that you would like to have â€œmagicallyâ€
completed by either talking to Alexa or by typing into a natural
language command prompt?
Experiment - Interaction Task (T1)Complete the following tasks:
Launch Devy by saying â€œAlexa, launch Devyâ€ [..]
T1.1 Using Devy, try to get the name of the person whom you might
contact to get help with making changes to this â€˜readmeâ€™ ï¬le.
T1.2 Next, make sure you are on branch â€˜iss2â€™ and then make a change
to this â€˜readmeâ€™ ï¬le (and save those changes).
T1.3 Finally, make those changes available on GitHub.
Experiment - Demonstration Task (T2)Complete the following tasks:
T2.1 Say â€œAlexa, tell Devy to list my issues.â€ to list the ï¬rst open issue on
GitHub. List the second issue by saying â€œNextâ€, then stop by saying
â€œStopâ€. Notice that the listed issues are for the correct repository.
T2.2 Say â€œAlexa, tell Devy I want to work on issue 2.â€ to have Devy
prepare your workspace for you by checking out a new branch.
T2.3 Resolve the issue: comment out the debug console.log on line 8 of
log.ts by prepending it with //. Save the ï¬le.
T2.4 Say â€œAlexa, tell Devy Iâ€™m done.â€ to commit your work and open a
pull request. Devy will ask if you want to add the new ï¬le; say â€œY esâ€.
Next, Devy recommends up to 3 reviewers. Y ou choose any you
like. When completed, Devy will say it created the pull request and
open a browser tab showing the pull request. Notice the reviewers
you speciï¬ed have been added. Also, notice that tests covering the
changes were automatically run and the test results were included
in a comment made by Devy.
Interview - Part Two1
Imagine that Devy could help you with anything you would want,
what do you think it could help you with and where would it provide
most beneï¬t?
2 Are there any other tasks /goals/workï¬‚ows that you think Devy
could help with, maybe not just restricted to your development tasks,
but other tools you or your team or your colleagues use?
3 When you think about the interaction you just had with Devy, what
did you like and what do you think could be improved.
4 Did Devy do what you expected during your interaction? What
would you change?
5 Do you think that Devy adds value? Why or why not?
about the possible tasks and workï¬‚ows that a conversational assis-
tant such as Alexa could help them with in their workplaces.
Experiment. To give participants a more concrete idea of a CDA and
investigate how well it can support basic workï¬‚ows, we conducted
an experiment with Devy on two small tasks. For this experiment,
we provided participants a laptop that we conï¬gured for the study.
We connected the Amazon Echo Dot to the laptop for power and we
connected the laptop and the Echo Dot to the participantâ€™s corporate
wireless guest network.
997
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Nick C. Bradley, Thomas Fritz, and Reid Holmes
The tasks were designed to be familiar to participants and included
version control, testing, and working with issues. The objective for
the ï¬rst taskâ€”interaction taskâ€”was to examine how well developers
interact with Devy to complete a task that was described on a high-
level (intent-level) in natural language. This ï¬rst task focused on
ï¬nding out who the owner of a speciï¬c ï¬le is and making a change
to the ï¬le available in the repository on GitHub. The task description
(see also Table 3) was presented to the participants in a text editor
on the laptop. For the task we setup a repository in GitHub with
branches and the ï¬le to be changed for the task.
The objective for the second taskâ€”demonstration taskâ€”was to
have participants use Devy for a second case and demonstrate its
power and potential of mapping higher-level intents to lower-level ac-
tions, e.g. from telling Devy that one â€œis doneâ€ to Devy committing
and pushing the changes, running the relevant tests automatically
and opening the pull request in a web browser tab (see Table 3 for
the task description).
Interview (Part Two). After the experiment, we continued our in-
terview. Interacting with Devy and seeing its potential might have
stimulated participantsâ€™ thinking so we asked them further about
which scenarios an assistant such as Devy would be well-suited
and why. We also asked them about their experience with Devyduring the two experimental tasks (see Table 3 for the questions).
Finally, we concluded the study by asking participants demographic
questions and thanking them for their participation.
4.3 Data Collection and Analysis
The study, including the interviews and experiment, lasted an average
of 36 (Â±4) minutes. We audio recorded and transcribed the interviews
and the experiment and we took written notes during the study.
To analyze the data, we use Grounded Theory methods, in par-
ticular open coding to identify codes and emerging themes in thetranscripts [
17]. For the open coding, two authors coded ï¬ve ran-
domly selected transcripts independently and then discussed andmerged the identiï¬ed codes and themes. In a second step, we vali-
dated the codes and themes by independently coding two additional
randomly selected transcripts. For the coding of all transcripts, we
used the RQDA [ 6] R package. In this paper, we present some of
the most prominent themes, notably those that illustrate the most
common use cases, the beneï¬ts, and the shortcomings of CDAs.
From the experimental task T1, we derived a count based on the
number of times a participant had to speak to Devy before they were
able to complete a subtask. We adjusted this count by removing55 attempts (out of 175) that failed due to technical issues, i.e.
connectivity problems or unexpected application failures of Alexa,
due to a participant speaking too quietly, and due to participants
trying to invoke Devy without using the required utterance of â€œAlexa,
ask/tell Devy...â€.
5 RESULTS
In this section we present the results for our two research questions
that are based on the rich data gathered from the experimental study
tasks and the interview. First, we report on the participantsâ€™ inter-action and experience with our CDA Devy. Then we report on the
workï¬‚ows and tasks that a CDA might support as well as its beneï¬ts
and challenges.â—
â—1XPEHURI$WWHPSWV$GMXVWHG

7
)LOH2ZQHU7
%UDQFK1DPH7
6XEPLW&KDQJHV
Figure 3: Adjusted number of attempts required to completeeach task of T1 across 20 participants.
5.1 Completing Development Tasks with Devy
Overall, all participants were able to complete all subtasks of T1and T2 successfully with Devy. Many participants expressed that
Devy was â€œneat â€(P17) and â€œcool â€(P18) and some also stated that
Devy did more than they expected. For instance, P9 explicitly stated
â€œ[Devy] exceeded my expectationsâ€ while P8 â€œ [was] surprised at
how much it did [..] it actually did more than [..] expected â€.
For the ï¬rst experimental task T1, we examined if participants
were able to interact with Devy and complete speciï¬c subtasks that
were speciï¬ed on the intent level rather than on the level of speciï¬c
and executable (Git) commands. Figure 3 shows the number of Devy
interactions (attempts) that it took each participant to complete each
of the three subtasks of T1. The numbers in the ï¬gure are based on
20 participants (one participant completed T2 before T1 and wasexcluded due to learning eï¬€ects); the values were adjusted by re-
moving attempts that failed due to technical issues (see Section 4.3).
Across all three subtasks, participants used very few attempts to
complete the subtasks with an average of two attempts for T1.1
and T1.3 and a single attempt for T1.2.
Subtask T1.1 required getting the name of the person who made
the most changes to an open ï¬le. This task had the highest variance
with one participant taking 8 attempts since he had never used Git
before. Six participants required some guidance. This was largely
due to Devy only being trained with three utterances that all focusedon ï¬le ownership and none that focused on the number of changes. In
fact, seven participants used an utterance similar to â€œ who has made
changesâ€ (P1, P2, P3, P4, P14, P17, P19) on their ï¬rst attempt. This
shows that either developers require training to learn the accepted
utterances or, better yet, that Devy should support a broad set of
utterances . One participant compared it to the speciï¬c and rigid
syntax of command-oriented approaches:
â€œMultiple ways you can tell it to do the same thing [because] it might
be advantageous where [you] might forget the exact terminology.â€
(P10)
998
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Context-Aware Conversational Developer Assistants ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
In their ï¬rst interactions with Devy, most participants (16 out
of 20) did not take advantage of its automatic context tracking and
instead included the speciï¬c ï¬le name in their utterances. This was
due to participants thinking of Devy as a traditional tool, â€œmaking
the assumption [Devy] would work just like the command line client â€
(P19) and they â€œexpected [Devy] to be kind of simple and dumbâ€
(P7). As their sessions progressed, participants started to express
their intentions more naturally and more vaguely, for instance by
replacing the ï¬le name with â€œ thisâ€o râ€œ this ï¬leâ€, and participants
appreciated the automated context tracking:
â€œI was just thinking it knows the context about what Iâ€™m talking about.
Thatâ€™s kind of cool.â€ (P2)
Subtask T1.2 required getting the checked-out branch and only
took one attempt on average. All but two participants were ableto complete the subtask without guidance. Thereby,
participants
used various utterances to interact with Devy from ones that were
close to the Git commands â€œAlexa, tell Devy to checkout branch iss2â€
(P15) to less command-oriented phrases â€œAlexa, ask Devy to get onto
branch iss2 [..]â€ (P8). The two participants that did not complete
this task accidentally skipped it. The one participant that took 4
attempts paused between starting Devy and issuing the command.
Subtask T1.3 focused on submitting changes to GitHub. Partic-
ipants took an average of 2 attempts to complete this and had a
lower variance than for T1.1. While 14 participants followed the Git
command line interaction closely by ï¬rst committing the changes
and then pushing them, the other 6 participants took advantage of
some of Devyâ€™s automation and for example directly said â€œAlexa,
tell Devy to push to GitHubâ€ (P15) which resulted in Devy commit-
ting and pushing the changes. Also, for this subtask, most partici-pants took advantage of Devyâ€™s context model and omitted some
of the speciï¬cs from their phrases, such as what exactly should be
committed or pushed.
The second experimental task T2 was to demonstrate Devyâ€™s
potential. Since all utterances were speciï¬ed in the task description,
and no participants had problems following the steps, we will not
include any further analysis of this task.
Observation. Participants were able to use our conversational
developer assistant to successfully complete three common develop-
ment tasks without explicit training, with very few attempts, and by
taking advantage of the automatic context tracking.
5.2 CDA Beneï¬ts &Scenarios
Participants reported a myriad of scenarios and situations in which a
CDA could enhance their professional workï¬‚ow.
One common workï¬‚ow supports multi-step and cross-appli-
cation tasks . Several development tasks require a certain â€œprocessâ€
(P8) or sequence of steps to be completed that oftentimes require
developers to switch between di ï¬€erent applications. An example
referred to by several participants was the sharing of changes:
â€œOnce Iâ€™ve committed everything locally, Iâ€™ll push it to GitHub. Iâ€™ll
go to GitHub in my web browser , create a new pull request, writeout a description of the change [..] and how I tried to accomplish
that [..] Once the pull request is created, Iâ€™ll go to Slack and post a
message on our channel and say there is a new PRâ€ (P15)Participants indicated the cost and eï¬€ort of these multi-step and
cross-application tasks and how a conversational assistant would
reduce the necessary application/context switches and allow de-
velopers to not â€œlose [..] concentration on the thing Iâ€™m looking at â€
(P6) and stay more focused:
â€œIf you could do some [of these tasks] automatically, just by talking,
Iâ€™d be really happy because I usually have a ton of consoles and
switching over really confuses me when you have so many screens
open. Just alt-tabbing between them consistently, even if you do that
like 9 out of 10 times successfully, at the end of the day you startgetting sloppy and holding those trains of thought in mind would
probably be simpler if you werenâ€™t changing screens so oftenâ€ (P7)
â€œToday, I think I had like 20 emails all related to my pull request and
it was all just single comments and I have to link back [to the pull
request..] and then come back [to my email client] and then delete,
and then go back and [..]. So thereâ€™s a lot of back and forth there.
Those are the main things that I feel: â€˜oh these are taking time and
it shouldnâ€™t... â€™â€ (P3)
A CDA is also considered particularly beneï¬cial for the automatic
mapping of higher-level tasks to commands:
â€œAnything that helps you stay in the ï¬‚ow is good, so if I can do these
higher level tasks with a brief command to some place rather than
break them down into a sequence of git commands plus switching
to the browser plus doing yet another thing interspersed with some
manual reading, it would be a win.â€ (P19)
This automatic aggregation of multiple steps is seen as a â€œ simpliï¬ca-
tionâ€ (P7) by participants:
â€œ[If] we have a bot tell me the IP of my deployed AWS container
rather than a 10 step ssh-based process to get it that would be very
simple [..and] interacting with a voice assistant to get information
[..] out of the development ecosystem would be useful.â€ (P18)
By abstracting the speciï¬c low-level actions, the automatic mapping
reduces the need for memorization of the commands, which re-
duces errors and saves time:
â€œThere are too many command line steps that you can get wrongâ€
(P18)
â€œA lot of the time you know what it is in your head, but you still gotta
ï¬nd it. So thatâ€™s the stuï¬€ [..] this would be really helpful for .â€ (P8)
Participants mentioned that this can be valuable for infrequent but
recurringâ€”â€œonce in a whileâ€ (P11)â€”tasks, since they â€œdo [them] of-
ten enough to want a better interface but seldom enough that [they]
canâ€™t remember all the right buttonsâ€ (P10) or they canâ€™t remember
the â€œcrazy ï¬‚ags that youâ€™ve gotta remember every single time â€(P8).
By continuously switching between applications, developers have
to frequently re-establish their working context. For instance, after
committing and pushing a code change using a command line tool,
developers often â€œhave to go to the browser , open a new tab, go to
GitHub and ï¬nd the pull request â€(P15) which can be a â€œa pain in
the arseâ€ (P8). In general, when switching between applications,
participants need to do a lot of â€œadmin work â€(P14) just to ensure
that the applications are â€œin syncâ€ (P14). Therefore, a major beneï¬t
of a CDA that automatically tracks context in the background is that
itreduces the explicit speciï¬cation of context.
999
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Nick C. Bradley, Thomas Fritz, and Reid Holmes
By automatically aggregating multiple steps and keeping track of the
current context, participants also thought that a CDA can support
information retrieval , especially when â€œthere isnâ€™t really a good
interfaceâ€ (P1) for querying and it can speed up the process:
â€œSo, looking at the context of what Iâ€™m doing and then like highlight-
ing an error text and then copying it and pasting it into Google and
looking for it. And then looking down for some of the best matches.
Even just â€˜Devy look this up for meâ€™.â€ (P2)
â€œRight now, you need to do two or three commands and know what
all the change list numbers are [..to] look up all the information
[about who last touched a ï¬le].â€ (P20)
Instead of just automating and aggregating tasks, participants sug-
gested that a CDA that tracks a developerâ€™s steps and context could
help to enforce workï¬‚ows and make sure that nothing is forgotten:
â€œThere are certain ï¬‚ows that often go together . When I start a day,
I often need to check the same things [..and it is] not easy to check
oï¬€the top of my head, so I forget to do that sometimes [..] so that
type of thing would be great to do all in one shot. So where am I
[which branch], what is my status, do I need to rebase, and if I need
to rebase, do it [..]â€ (P3)
In case a developer does not follow the usual workï¬‚ow, the context
tracking can come in handy and allow her to go back in history:
â€œsometimes I just go too far down a path. And Iâ€™ve gone through
three or four of those branches in my mind and I know I need to
go back but because I [..] only want to go part way back, that just
becomes really di ï¬ƒcult. So if there was some simple way it could
recognize that Iâ€™m far enough down a path [..it] would be amazing
if I could then realize that I have screwed up, rewind 10 minutes,
commit and then come back to where I am now.â€ (P21)
Several participants noted that the additional communication chan-
nel oï¬€ered by Devy could be utilized to parallelize tasks that would
otherwise require a switch in context and focus. The archetypal case
of this was setting reminders:
â€œYeah, I think of them like if Iâ€™m coding and I have an idea of an-
other approach I could take but I want to ï¬nish the current one Iâ€™m
doing, Iâ€™ll throw myself in a little Slack reminder and then I get a
notiï¬cation in the time I speciï¬ed.â€ (P21)
However, this idea is more general and can be particularly useful in
cases where the secondary task may take some time to complete:
â€œWhere itâ€™d be useful is where Iâ€™m in the middle of one task and I
want another being done. If Iâ€™m working on one part of it, eitherdebugging or editing some code and I want something to go on in
the background... Like weâ€™ve got a build system so maybe I want to
start the build for another tool that I will be using soon and I donâ€™t
want to switch over to start that.â€ (P11)
â€œIf I have a pull request and Iâ€™m waiting for it to be approved and I
have nothing else to do in the meantime, Iâ€™m going to make lunch. I
could just be cooking and I could just be like: â€˜has it been approved
yet?â€™ and if it has then merge it before someone else gets their stuï¬€
in there. Oh, that would be great.â€ (P3)
Seven participants explicitly mentioned that a voice-activated assis-
tant provides an alternative to typing that allows tasks be performedwhen the developerâ€™s hands are â€œbusy â€(P11) or â€œinjured â€(P16,P20)
and that â€œas intuitive as typing is [..], talking is always going tobe more intuitiveâ€ (P12). Similarly, it provides an
alternative to
interacting with GUIs that â€œwaste a lot of time just by moving the
mouse and looking through menusâ€ (P7) or to navigate code with
context, for example by asking â€œwhereâ€™s this called fromâ€ (P10) or
â€œwhat classes are relevant to this concept â€(P13).
Observation. There are a large number of development tasks in
participantsâ€™ workï¬‚ows that are currently ineï¬ƒcient to perform due
to their multi-step and cross-application nature. A conversationaldeveloper assistant might be able to support these scenarios by
reducing application switches, the need for context speciï¬cation and
memorization, and by supporting parallelization of tasks.
5.3 CDA Challenges
Participants also raised several concerns about their interaction with
Devy and conversational developer assistants more generally. The
predominant concern mentioned by several participants was the dis-
ruptiveness of the voice interface in open oï¬ƒce environments:
â€œI work in a shared workspace so there would have to be a way for us
to have these dialogs that are minimally disruptive to other people.â€
(P19)
â€œI imagine a room full of people talking to their computers would be
a little chaotic.â€ (P2)
Further concerns of the voice interaction are its â€œaccuracyâ€ (P11)
and that the verbal interaction is slow:
â€œI didnâ€™t really enjoy the verbal interaction because it takes longer .â€
(P2)
â€œIt feels weird to interrupt [Devy]. Thatâ€™s probably more of a social
thing [..] itâ€™s a voice talking and you donâ€™t want to interrupt it and
then you have to end up waitingâ€ (P15)
While Devy was able to automate several steps, participants were
concerned about the lack of transparency and that it is important
to know which low-level actions Devy is executing:
â€œThe downside is I have to know exactly what itâ€™s doing behind the
scenes which is why I like the command line because it only does
exactly what I tell it to do.â€ (P8)
This can be mitigated by providing more feedback, possibly through
channels other than audio:
â€œI think for me, when [Devy] is changing branches or something,
Iâ€™d probably want to see that that has happened in there. Just some
indication visually that something has happened. I mean it told me
so Iâ€™d probably get used to that too.â€ (P6)
However, there is some disagreement on exactly how much feedback
is wanted:
â€œI liked that there was deï¬nitely clear feedback that something is
happening, even for things that take a bit of time like git pushes.â€
(P1) For a conversational developer assistant completeness â€”the
number of intents that the CDA is able to understandâ€”is important.
Participant P14 made the case that â€œthe breadth of commands needs
to be big enough to make it worthwhile.â€
1000
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Context-Aware Conversational Developer Assistants ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
This completeness is also related to challenges in understanding
the intent of all possible utterances a developer could use:
â€œItâ€™s frustrating to talk to something that doesnâ€™t understand you.
Regardless of how much more time it takes then another method, it
would still be more frustrating to argue with a thing that fundamen-
tally doesnâ€™t feel like it understands me.â€ (P12)
Finally, since developers use a diverse set of tools in a variety of
diï¬€erent ways and â€œeveryoneâ€™s got a little bit of a diï¬€erent workï¬‚owâ€
(P2), it is necessary for CDAs to support customization . For this,
one could either â€œcreate macrosâ€ (P2) or have some other means
for adapting to each developerâ€™s particular workï¬‚ow so that Devy
â€œcould learn how [people are] using it â€(P9). This aspect is related to
completeness but emphasizes altering existing functionality to suit
the individual or team.
Observation. Participants raised several concerns for conversa-
tional developer assistants related to disruptiveness of voice interac-
tions, the need for transparency, completeness, and customization.
5.4 Summary
Ultimately, industrial developers were able to successfully perform
basic software development tasks with our conversational developer
assistant, providing positive evidence for RQ1 . In terms of RQ2 ,
CDAs appear to be most useful for simplifying complex workï¬‚ows
that involve multiple applications and multiple steps because of
their unnecessary context switches which interfere with developer
concentration.
6 RELATED WORK
We build upon a diverse set of related work in this paper. To support
developers in their tasks, researchers have long tracked development
context in order to provide more e ï¬€ective analyses and to surface
relevant information. The emerging use of bots for software engineer-
ing also shows promise for automating some of the tasks, improving
developers eï¬€ectiveness and eï¬ƒciency. Finally, natural language
interfaces show increasing promise for reducing complexity and
performing speciï¬c development tasks.
6.1 Development Context
Our model of task context is fundamental to enabling Devy to pro-
vide a natural interface to complex workï¬‚ows. Previous work has
looked at diï¬€erent kinds of context models. Kersten and Murphy
provide a rich mechanism for collecting and ï¬ltering task contextdata, speciï¬cally about program elements being examined, as de-
velopers switch between diï¬€erent tasks [
7]. Our context model is
more restrictive in that we mainly track the current task context: past
contexts are not stored. Concurrently, our context model includes
much more detail about non-code aspects relevant to the developer,
their project, and their teammates.
Other systems have looked at providing richer contextual infor-
mation to help developers understand their systems. For example,
TeamTracks uses the navigation information generated by monitor-
ing how members of a team navigate through code resources to build
a common model of related elements [ 4]. MasterScope provides
additional context about code elements as they are selected in an
IDE [ 18]. The similarity between task contexts can also be used tohelp identify related tasks [ 10]. Each of these systems demonstrates
the utility context models can confer to development tasks. Our work
extends these prior eï¬€orts by providing a context model appropriate
for conversational development assistants.
6.2 Bots for SE
In their Visions paper, Storey and Zagalsky propose that bots act
as â€œconduits between users and services, typically through a conver-
sational UIâ€ [ 16]. Devy clearly sits within this context: the natural
language interface provides a means for developers to â€˜converseâ€™
with their development environment, while the provided workï¬‚ows
provide an eï¬€ective means for integrating multiple diï¬€erent prod-
ucts within a common interaction mechanism. Further to their botmetaphor, Devy is able to interpret the conversation to perform
much more complex sequences of actions based on relatively little
input, only checking with the developer if speciï¬c clariï¬cation is
required. As Storey and Zagalsky point out, there is a clear relation-
ship between bots and advanced scripts. We ï¬rmly believe that the
conversational interface, combined with the context model, moves
beyond mere scripting to enable new kinds of interactions and work-
ï¬‚ows that could not be directly programmed. One study participant
also pointed out that â€œitâ€™s nice to have the conversation when there
are things that are not speciï¬ed or you forgot to do; thatâ€™s whenyou want to get into a dialog. And when youâ€™re in the zone, then
you can just tell it what to doâ€ (P19), showing further beneï¬t of the
conversational UI beyond scripting itself.
Acharya et. al. also discuss the concept of Code Drones in
which all program artefacts have their own agent that acts semi-
autonomously to monitor and improve its client artefact [ 1]. One
key aspect of these drones is that they can be proactive instead of
reactive. While Devy is not proactive in that it requires a developer
to start a conversation, it can proactively perform many actions in
the background once a conversation has been started, if it determinesthat this appropriate for the given workï¬‚ow. Devy also takes a diï¬€er-ent direction than
Code Drones in that rather than attaching drones
to code artefacts, Devy primarily exists to improve the developerâ€™s
tasks and experience directly, rather than the code itself.
6.3 Natural Language Tools for SE
A number of tools have been built to provide natural language inter-
faces speciï¬cally for software engineering tasks.
The notion of programming with natural language is not new
(having ï¬rst been described by Sammet in 1966 [ 15]). Begel fur-
ther described the diverse ways in which spoken language can beused in software development [
2]. More recently, Wachtel et. al.
have investigated using natural language input to relieve the devel-
oper of repetitive aspects of coding [ 19]. Their system provides a
mechanism for users to specify algorithms for spreadsheet programs
using natural language. In contrast, Devy does not try to act as a
voice front-end for programming: it works more at a workï¬‚ow level
integrating diï¬€erent services.
Others though have looked at natural language interfaces as a
means for simplifying the complex tools used for software devel-opment. One early relevant example of this by Manaris et. al. in-
vestigated using natural language interfaces to improve the abilities
of novice users to access UNIX tools in a more natural way [ 12].
1001
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Nick C. Bradley, Thomas Fritz, and Reid Holmes
NLP2Code provides a natural language interface to a speciï¬c task:
ï¬nding a relevant code snippet for a task [ 3]. NLP2Code takes a
similar approach to Devy in that supports a speciï¬c development
task, but unlike Devy does not use a rich context model, nor does it
involve more complex development workï¬‚ows.
7 DISCUSSION
In this section we discuss threats to the validity of our study and
future work suggested by our study participants.
7.1 Threats to Validity
The goal of our study was to gain insight into the utility of conversa-
tional developer assistants in the software engineering domain. As
with any empirical study, our results are subject to threats to validity.
Internal validity. We elicited details about participantsâ€™ work-
ï¬‚ows before they interacted with our prototype to mitigate bias and
again after using Devy to capture more detail. Despite this, it is pos-
sible that participants did not describe all ways Devy could impact
their workï¬‚ows; given more time and a wider variety of sample
tasks, participants may have described more scenarios. While we
followed standard open coding processes, other coders may discern
alternative codes from our interview transcripts.
External validity. Though our 21 interviews with industrial de-
velopers yielded many insights, this was a limited sample pulled
from our local metropolitan region. While participants had diï¬€ering
years of experience and held various roles at six diï¬€erent organiza-
tions, each with a diï¬€erent set of workï¬‚ows, our ï¬ndings may not
generalize to the wider developer community.
7.2 Future Work
The feedback we received from industrial developers was broadly
positive for our prototype conversational developer assistant. Thank-
fully, our participants had many great suggestions of ways to extend
and improve Devy to make it even more eï¬€ective in the future.
The most pressing piece of future work is to implement alter-
native conversational layers for Devy, speciï¬cally a text-based
ChatBot-like interface. Participants mentioned this would be espe-
cially beneï¬cial in open-plan oï¬ƒces (which all participants used). It
also avoids requiring Devy-speciï¬c hardware.
Currently, Devy can be extended through the intent service by
wiring up new states in the FSM. This requires the same amount
of work as creating scripts, although enables better integration with
existing states than simple scripting. Based on participant feedback,
supporting a more parameterized view of how the states are con-nected to form custom workï¬‚ows
seems like a reasonable tradeoï¬€
between complete scripting and a fully autonomous agent. Partici-
pants were also forthcoming with suggestions for a diverse set offuture workï¬‚ows that could deï¬ne the out-of-box-workï¬‚ows for
version control, debugging, testing, collaboration, task management
and information retrieval.
A large step beyond this would be for the CDA to support generic
workï¬‚ows out-of-the-box that can self-adapt to better enable user-
speciï¬c custom workï¬‚ows without user intervention but based on
their own usage patterns.Several participants also wished for tighter source code integra-
tion . The intent of this integration was to perform more query-based
questions of the speciï¬c code elements they were looking at without
interrupting their current task. For example:
â€œthe thing people want the most...are abstract syntax trees. I think it
is something that would oï¬€er a lot of power if you also had assistive
technology layered on top.â€ (P8)
Using lightweight notes and reminders , CDAs might enable
semantic undos that could be further maintained using the context
model to rollback changes to meaningful prior states.
Enabling CDAs to proactively take action in terms of awareness
or in response to external events was widely requested:
â€œinï¬‚uence the output of what Iâ€™m working on...by [notifying] me
about getting oï¬€ my regular pattern, that would be the most valuable. â€
(P8)
This could also help by preventing mistakes before they happen:
â€œIf I tell it to commit and [there are an unusual number of changes],
it should conï¬rm.â€ (P15)
Next, extending support for industrial tools to those commonly
used by industrial teams will enable Devy to be deployed in a wider
variety of practical contexts.
Participants were also enthusiastic about the potential for support
forenhanced cross-application workï¬‚ows that otherwise cause
them to context switch or â€˜copy-and-pasteâ€™ between independent
systems. We will further investigate extending support for these
kinds of tasks that force developers to context switch.
Finally, we built our prototype using the Alexa service and our
intent service to handle the natural language discourse and map it to
workï¬‚ow actions. To support further workï¬‚ows and ease the natural
language discourse with developers, we will examine whether and
how to extend the underlying discourse representation structure.
8 CONCLUSION
In this paper, we have explored the potential of conversational agents
to support developer workï¬‚ows. In particular, we have described
Devy, a conversational development assistant that enables developers
to invoke complex workï¬‚ows with only minimal interaction using
a natural language conversational interface. Through its context-
aware model, Devy supports rich workï¬‚ows that can span multiple
independent tools; this frees the developer to oï¬„oad these low-level
actions and enables them to focus on their high-level tasks.
Using our Devy prototype as a technology probe, we evaluated
our approach in a mixed methods study with 21 industrial soft-
ware engineers. These engineers were able to use Devy successfully
and appreciated that they did not need to specify and memorize
multi-step workï¬‚ows and that it reduced context switches. They ad-
ditionally identiï¬ed a concrete set of challenges and future directions
that will improve the utility of future CDAs.
Ultimately, the Devy prototype demonstrates that developers can
successfully launch complex workï¬‚ows without interrupting their
current tasks while reducing developer eï¬€ort. We believe that that
future conversational developer assistants will have the ability to im-
prove developerâ€™s productivity and/or eï¬€ectiveness by allowing them
to focus on their core development tasks by oï¬„oading meaningful
portions of their workï¬‚ows to such automated agents.
1002
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Context-Aware Conversational Developer Assistants ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1] Mithun P . Acharya, Chris Parnin, Nicholas A. Kraft, Aldo Dagnino, and Xiao Qu.
2016. Code Drones. In Proceedings of the International Conference on Software
Engineering (ICSE). 785â€“788.
[2] Andrew Begel. 2006. Spoken Language Support for Software Development. Ph.D.
Dissertation. EECS Department, University of California, Berkeley.
[3] Brock Angus Campbell and Christoph Treude. 2017. NLP2Code: Code Snippet
Content Assist via Natural Language Tasks. In Proceedings of the International
Conference on Software Maintenance and Evolution (ICSME). 628â€“632.
[4] Robert DeLine, Mary Czerwinski, and George Robertson. 2005. Easing Pro-
gram Comprehension by Sharing Navigation Data. In Proceedings of the Visual
Languages and Human-Centric Computing (VLHCC). 241â€“248.
[5] Victor M. GonzÃ¡lez and Gloria Mark. 2004. Constant, Constant, Multi-tasking
Craziness: Managing Multiple Working Spheres. In Proceedings of the Conference
on Human Factors in Computing Systems (CHI). 113â€“120.
[6] Ronggui Huang. 2017. RQDA: R-based Qualitative Data Analysis.
[7] Mik Kersten and Gail C. Murphy. 2006. Using Task Context to Improve Program-
mer Productivity. In Proceedings of the International Symposium on F oundations
of Software Engineering (FSE). 1â€“11.
[8] Thomas D. LaToza, Gina V enolia, and Robert DeLine. 2006. Maintaining Mental
Models: A Study of Developer Work Habits. In Proceedings of the International
Conference on Software Engineering (ICSE). 492â€“501.
[9] David C. Littman, Jeannine Pinto, Stanley Letovsky, and Elliot Soloway. 1987.
Mental Models and Software Maintenance. Journal of Systems and Software (JSS)
7, 4 (1987), 341 â€“ 355.
[10] Walid Maalej, Mathias Ellmann, and Romain Robbes. 2017. Using Contexts Sim-
ilarity to Predict Relationships Between Tasks. Journal of Systems and Software(JSS) 128 (2017), 267 â€“ 284.
[11] Walid Maalej, Rebecca Tiarks, Tobias Roehm, and Rainer Koschke. 2014. On
the Comprehension of Program Comprehension. ACM Transactions on Software
Engineering and Methodology (TOSEM) 23, 4, Article 31 (2014), 37 pages.
[12] Bill Z. Manaris, Jason W . Pritchard, and Wayne D. Dominick. 1994. Developing
a Natural Language Interface for the UNIX Operating System. Proceedings of the
Conference on Human Factors in Computing Systems (CHI) 26, 2 (1994), 34â€“40.
[13] AndrÃ© N. Meyer, Laura E. Barton, Gail C. Murphy, Thomas Zimmermann, and
Thomas Fritz. 2017. The Work Life of Developers: Activities, Switches and
Perceived Productivity. IEEE Transactions on Software Engineering (TSE) 43, 12
(2017), 1178â€“1193.
[14] AndrÃ© N. Meyer, Thomas Fritz, Gail C. Murphy, and Thomas Zimmermann. 2014.
Software Developersâ€™ Perceptions of Productivity. In Proceedings of the 22Nd
ACM SIGSOFT International Symposium on F oundations of Software Engineering
(FSE). ACM, 19â€“29.
[15] Jean E. Sammet. 1966. Survey of Formula Manipulation. Communications of the
ACM (CACM) 9, 8 (1966), 555â€“569.
[16] Margaret-Anne Storey and Alexey Zagalsky. 2016. Disrupting Developer Pro-
ductivity One Bot at a Time. In Proceedings of the International Symposium on
F oundations of Software Engineering (FSE). 928â€“931.
[17] Anselm Strauss and Juliet M. Corbin. 1998. Basics of Qualitative Research:
Techniques and Procedures for Developing Grounded Theory. SAGE Publications.
[18] Warren Teitelman and Larry Masinter. 1981. The Interlisp Programming Environ-
ment. Computer 14, 4 (1981), 25â€“33.
[19] Alexander Wachtel, Jonas Klamroth, and Walter F. Tichy. 2017. Natural Language
User Interface For Software Engineering Tasks. In Proceedings of the International
Conference on Advances in Computer-Human Interactions (ACHI), V ol. 10. 34â€“
39.
1003
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. 