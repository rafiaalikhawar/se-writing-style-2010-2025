Floating-Point Precision T uning Using Blame Analysis
Cindy Rubio-Gonz´ alez1∗, Cuong Nguyen2∗, Benjamin Mehne2, Koushik Sen2, James Demmel2,
William Kahan2,C o s t i nI a n c u3, Wim Lavrijsen3, David H. Bailey3,a n dD a v i dH o u g h4
1University of California, Davis, crubio@ucdavis.edu
2University of California, Berkeley, {nacuong, bmehne, ksen, demmel, wkahan}@cs.berkeley.edu
3Lawrence Berkeley National Laboratory, {cciancu, wlavrijsen, dhbailey}@lbl.gov
4Oracle Corporation, david.hough@oracle.com
ABSTRACT
While tremendously useful, automated techniques for tun-
ing the precision of ﬂoating-point programs face importantscalability challenges. We present Blame Analysis , a novel
dynamic approach that speeds up precision tuning. Blame
Analysis performs ﬂoating-point instructions using diﬀer-
ent levels of accuracy for their operands. The analysis deter-mines the precision of all operands such that a given preci-sion is achieved in the ﬁnal result of the program. Our eval-
uation on ten scientiﬁc programs shows that Blame Anal-
ysisis successful in lowering operand precision. As it ex-
ecutes the program only once, the analysis is particularly
useful when targeting reductions in execution time. In such
case, the analysis needs to be combined with search-based
tools such as Precimonious. Our experiments show that
combining Blame Analysis with Precimonious leads to
obtaining better results with signiﬁcant reduction in anal-ysis time: the optimized programs execute faster (in three
cases, we observe as high as 39.9% program speedup) and
the combined analysis time is 9× faster on average, and up
to 38×faster than Precimonious alone.
CCS Concepts
•Software and its engineering →Dynamic analysis;
Software performance; Formalsoftwareveriﬁcation; Soft-
ware testing and debugging; •Mathematics of comput-
ing→Numerical analysis;
Keywords
ﬂoating point, mixed precision, program optimization
∗The ﬁrst two authors contributed equally to this paper.
ACM acknowledges that this contribution was authored or co-authored by an em-
ployee, or contractor of the national government. As such, the Government retains
a nonexclusive, royalty-free right to publish or reproduce this article, or to allow oth-
ers to do so, for Government purposes only. Permission to make digital or hard copiesfor personal or classroom use is granted. Copies must bear this notice and the full ci-tation on the ﬁrst page. Copyrights for components of this work owned by others thanACM must be honored. To copy otherwise, distribute, republish, or post, requires priorspeciﬁc permission and/or a fee. Request permissions from permissions@acm.org.
ICSE ’16, May 14-22, 2016, Austin, TX, USA
c/circlecopyrt2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.28848501. INTRODUCTION
Algorithmic [44, 2, 14] or automated program transforma-
tiontechniques[35, 25]totunetheprecisionofﬂoating-point
variables in scientiﬁc programs have been shown to signif-
icantly improve execution time. Developers prescribe the
accuracy required for the program result and the tools at-tempt to maximize the volume of data stored in the lowestnative precision. Generally, this results in improved memorylocality and faster arithmetic operations.
Since tuning ﬂoating-point precision is a black art that
requires both application speciﬁc and numerical analysis ex-pertise, automated program transformation tools are clearlydesirable and they have been shown to hold great promise.
State-of-the-art techniques employ dynamic analyses that
searchthroughtheprograminstructionspace[25]orthroughthe program variable/data space [35]. Due to the empiricalnature, a quadratic or worse (in instructions or variables)number of independent searches (program executions) with
diﬀerent precision constraints are required to ﬁnd a solution
that improves the program execution time for a given set ofinputs. While some search-based tools [35] attempt to onlyprovide solutions that lead to faster execution time, others
[25] provide solutions with no performance guarantees.
Inthispaperwepresentanovelmethodtoperformﬂoating-
point precision tuning that combines concrete and shadow
program execution, and it is able to ﬁnd a solution after onlyasingle execution . The main insight of Blame Analysis is
that given a target instruction and a precision requirement,one can build a blameset that contains all other program
instructions with their operands in minimum precision. Inother words, given an instruction and a precision require-ment, a blameset contains the precision requirements for
the instructions that deﬁne the values of its operands. Asthe execution proceeds, each instruction is executed withmultiple ﬂoating-point precisions for each operand and itsblameset is updated. The solution associated with a pro-
gram point is computed using a mergeoperation over all
blamesets. This can be used to infer the set of program
variables that can be declared as floatinstead of double
while satisfying the precision requirements for a providedtest input set. Note that, similar to [25], Blame Analysis
can only reduce precision with no performance guarantees.
We have implemented Blame Analysis using the LLVM
compiler infrastructure [27] and evaluated it on eight pro-grams from the GSLlibrary [17] and two programs from the
NASparallel benchmarks [37]. To provide more context,
we also evaluated it against the Precimonious [35] search-
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   1074
based tool. We have implemented both an oﬄineanalysis
that executes on program execution traces, as well as an
onlineanalysis that executes together with the program.
Blame Analysis was always successful in lowering the
precision of all test programs for the given test input sets: itidentiﬁed on average that 40% of the program variables canbe declared as float, 28% variables in median. The trans-
formed programs did not always exhibit improved executiontime. The oﬄineanalysis is able to lower the precision of a
larger number of variables than the onlineversion, but this
comes with decreased scalability. For the onlineversion, to
bound the overhead we had to restrict the analysis in some
casestoconsideronlythelaststagesofexecution. Evenwith
this restriction, the online analysis produced solutions whileimposing running time overhead as high as 50 ×, comparable
to other commercial dynamic analysis tools.
Ifreductioninexecutiontimeisdesired, Blame Analysis
can be combined with feedback-directed search tools suchasPrecimonious , which systematically searches for a type
assignment for ﬂoating-point variables so that the resultingprogram executes faster. When using Blame Analysis to
determine an initial solution for Precimonious ,w ea l w a y s
ﬁnd better type assignments. The total analysis time is 9 ×
faster on average, and up to 38× faster in comparison to
Precimonious alone. In all cases in which the resulting
type assignment diﬀers from Precimonious alone, the type
assignment produced by the combined analyses translatesinto a program that runs faster.
Ourresultsareveryencouragingandindicatethatﬂoating-
pointtuningofentireapplicationswillbecomefeasibleinthe
near future. As we now understand the more subtle behav-
ior of Blame Analysis, we believe we can improve both
analysis speed and the quality of the solution. It remainsto be seen if this approach to develop fast but conservativeanalyses can supplant the existing slow but powerful search-
based methods. Nevertheless, our work proves that using
a fast “imprecise” analysis to bootstrap another slow butprecise analysis can provide a practical solution to tuningﬂoating point in large code bases.
This work makes the following contributions:
•We present a single-pass dynamic program analysis fortuning ﬂoating-point precision, with overheads compa-rable to that of other commercial tools for dynamicprogram analysis.
•We provide an empirical comparison between single-passandsearch-based, dual-optimizationpurposetoolsfor ﬂoating-point precision tuning.
•We demonstrate powerful and fast precision tuning bycombining the two approaches.
The rest of this paper is organized as follows. Section 2
presents an overview of precision tuning and current chal-lenges. We describe Blame Analysis in Section 3, and
present its experimental evaluation in Section 4. We thendiscuss limitations and future work in Section 5. Relatedwork is discussed in Section 6. We conclude in Section 7.
2. TUNING FLOATING-POINT PRECISION
Programminglanguagesprovidesupportformultipleﬂoat-
ing point data types: float(single-precision 32-bit IEEE
754), double(double-precision 64-bit IEEE 754) and long
double(80-bit extended precision). Software packages suchas QD [24] provide support for even higher precision (datatypes double-double and quad-double). Because reason-
ing about ﬂoating-point programs is often diﬃcult given the
large variety of numerical errors that can occur, one com-
mon practice is to use conservatively the highest availableprecision. While more robust, this can signiﬁcantly degrade
program performance. Many eﬀorts [3, 4, 5, 13, 23, 28] have
shown that using mixed precision can sometimes computea result of the same accuracy faster than when using solelythe highest precision arithmetic. Unfortunately, determin-ing the appropriate precision combination requires domain-
speciﬁc knowledge combined with advanced numerical anal-ysis expertise.
Floating-point precision-tuning tools can help suggesting
ways in which programs can be transformed to eﬀectivelyuse mixed precision. These tools serve multiple purposes.
For a given test input set, one goal is to determine an opti-mal (minimal or maximal) set of program variables [35] orinstructions [25], whose precision can be changed such thatthe“answer”is within a given error threshold. If the goal is
to improve accuracy, expressions can be rewritten to reduce
rounding errors [31]. Another goal is to reduce memory stor-age by maximizing the number of variables whose precisioncan be lowered. Finally, improving program performance isanother important objective.
2.1 Design and Scalability Concerns
Our main interest is in tools that target scientiﬁc comput-
ing programming and use a dual objective by targeting bothaccuracy and performance. The state-of-the-art tools com-pute a solution by searching over global program state (vari-
ables or instructions). Thus the search maintains a“global”
solution and it requires multiple executions. Due to the em-pirical nature and the heuristics to bound the search spaceand state, the solutions do not capture a global optimum.
Fromourperspective, particularlyattractivearetoolsthat
operate on the program variable space, as they may suggestpermanent changes to the application. The state-of-the-art is reﬂected by Precimonious [35], which systematically
searches for a type assignment (also referred to as type con-
ﬁguration) for ﬂoating-point program variables. Its analysis
time is determined by the execution time of the programunder analysis, and by the number of variables in the pro-gram. The algorithm requires program re-compilation and
re-execution for diﬀerent type assignments. The search is
basedontheDelta-Debuggingalgorithm[45], whichexhibitsa worst-case complexity of O(n
2), where nis the number of
variables in the program. To our knowledge, Precimonious
and other automated ﬂoating point precision tuners [35, 25]
use empirical search and exhibit scalability problems with
program size or program runtime.
In practice, it is very diﬃcult for programmers to predict
how the type of a variable aﬀects the overall precision of
the program result and the Precimonious analysis has to
consider allthe variables within a program, both global and
local. This clearly poses a scalability challenge to the overall
approach. In our evaluation of Precimonious (Section 4),
we have observed cases in which the analysis takes hours
for programs that have fewer than 50 variables and native
runtime less than 5 seconds. Furthermore, as the analysisis empirical, determining a good solution requires repeatingit over multiple precision thresholds. A solution obtained
for a given precision (e.g., 10
−6) will always satisfy lower
thresholds (e.g., 10−4). Given a target precision, it is also
1075often the case that the solution determined independently
for a higher precision provides better performance than thesolution determined directly for the lower precision.
Inthiswork, wesettodevelopamethodthatalleviatesthe
scalability challenges of existing search-based ﬂoating-pointprecision tuning approaches by: (1) reducing the numberof required program analyses/transformations/executions,and (2) performing only local, ﬁne grained transformations,
without considering their impact on the global solution.
Our Blame Analysis is designed to quickly identify pro-
gram variables whose precision does not aﬀect the ﬁnal re-
sult, for anygiven target threshold. The analysis takes as
input one or more precision requirements and executes theprogram only oncewhile performing shadow execution. As
output, it produces a listing specifying the precision require-ments for diﬀerent instructions in the program, which thencan be used to infer which variables in the program can def-
initely be in single precision, without aﬀecting the required
accuracy for the ﬁnal result. When evaluating the approachwe are interested in several factors: (1) quality of solution,i.e., how much data is aﬀected, (2) scalability of the analysis,
and (3) impact on the performance of the tuned program.
Blame Analysis can be used to lower program precision
to a speciﬁed level. Note that, in general, lowering preci-
sion does not necessarily result in a faster program (e.g.,cast instructions might be introduced, which could make
the program slower than the higher-precision version). The
analysis focuses on the impact in accuracy, but does not con-sider the impact in the running time. Because of this, thesolutions produced are not guaranteed to improve program
performance and a triage by programmers is required.
Even when triaged, solutions do not necessarily improve
execution time. As performance is a main concern, we also
consider combining Blame Analysis with dual objective
search-based tools, as a pre-processing stage to reduce the
search space. In the case of Precimonious , this approach
can potentially shorten the analysis time while obtaining
a good solution. Figure 1 shows how removing variablesfrom the search space aﬀects the analysis time for the blas
program from the GSLlibrary [17], for the target precision
10
−10.T h e blasprogram performs matrix multiplication,
and it declares 17 ﬂoating-point variables. As shown at therightmost point in Figure 1, knowing a priori that 7 out of17 ﬂoating-point variables can be safely allocated as float
reduces Precimonious analysis time from 2.3 hours to only
35 minutes. This simple ﬁltering accounts for a 4 ×speedup
in analysis time.
In the rest of this paper, we present Blame Analysis and
evaluate its eﬃcacy in terms of analysis running time and
quality of solution in two settings: (1) applied by itself, and
(2) as a pre-processing stage for Precimonious .W er e f e rt o
quality of solution as whether the resulting type assignmentslead to programs with faster execution time.
3. BLAME ANALYSIS
Blame Analysis consists of two main components: a
shadow execution engine, and an integrated onlineanaly-
sis. The analysis is performed side-by-side with the pro-
gram execution through instrumentation. For each instruc-
tion, e.g., fadd(ﬂoating-point addition), Blame Analysis
executes the instruction multiple times, each time using dif-ferent precisions for the operands. Examples of precisioninclude float, double,a n d doubletruncated to 8 digits.
/g26/g1/g27/g26/g26/g26/g1/g28/g26/g26/g26/g1/g29/g26/g26/g26/g1/g30/g26/g26/g26/g1/g31/g26/g26/g26/g1/g32/g26/g26/g26/g1/g33/g26/g26/g26/g1/g34/g26/g26/g26/g1/g35/g26/g26/g26/g1/g27/g26/g26/g26/g26/g1
/g26/g1 /g27/g1 /g29/g1 /g31/g1 /g33/g1
# Removed Variables Time (secs) 
Figure 1: The eﬀect of reducing Precimonious search space
on analysis time for the blasbenchmark (error threshold
10−10). The horizontal axis shows the number of variables
removed from the search space. The vertical axis shows
analysis time in seconds. In this graph, the lower the curvethe faster the analysis.
1double mpow(double a, double factor, int n) {
2double res = factor;
3int i;
4for (i = 0; i < n; i++) {
5 res = res * a;
6}
7return res;
8}
9
10int main() {
11 d o u b l ea=1 . 8 4 0 8 9 6 4 2 ;
12 double res, t1, t2, t3, t4;
13 double r1, r2, r3;
14
15 t1 = 4*a;
16 t2 = mpow(a, 6, 2);
17 t3 = mpow(a, 4, 3);
18 t4 = mpow(a, 1, 4);
19
20 // res = a^4 - 4*a^3 + 6*a^2 - 4*a + 1
21 r1 = t4 - t3;
22 r2 = r1 + t2;
23 r3 = r2 - t1;
24 r e s=r 3+1 ;
25
26 printf("res = %.10f\n", res);
27 return 0;
28}
Figure 2: Sample Program
The analysis examines the results to determine which com-
binations of precisions for the operands satisfy given pre-cision requirements for the result. The satisfying precisioncombinationsarerecorded. The Blame Analysis algorithm
maintains and updates a blame set for each program instruc-
tion. The blame set associated with each instruction speci-ﬁes the precision requirements for all operands such that theresult of the instruction has the required precision. Blame
sets are later used to ﬁnd the set of variables that can be
declared in single precision.
3.1 Blame by Example
Consider the sample program shown in Figure 2, which
computes and saves the ﬁnal result in variable reson line
24. In this example, we consider three precisions: fl(ﬂoat),
db(double) and db8(accurate up to 8 digits compared to
the double precision value). More speciﬁcally, the value inprecision db
8represents a value that agrees with the value
obtained when double precision is used throughout the en-
1076Table 1: Statement r 3=r 2-t 1 isexecutedusingdiﬀerent
precisions for r2and t1.T h e c o l u m n Op Prec shows the
precisions used for the operands ( flcorresponds to ﬂoat,
dbto double, and db8to double accurate up to 8 digits).
Columns r2and t1show the values for the operands in the
corresponding precisions. Column r3shows the result for
the subtraction. Precision (db, db) produces a result that
satisﬁes the precision requirement db8.
Op Prec r2 t1 r3
(fl,fl) 6.8635854721 7.3635854721 -0.5000000000
(fl,db 8)6.8635854721 7.3635856000 -0.5000001279
(fl,db) 6.8635854721 7.3635856800 -0.5000002079
(db 8,fl)6.8635856000 7.3635854721 -0.4999998721
(db8,db8) 6.8635856000 7.3635856000 -0.5000000000
... ... ... ...
(db,db) 6.8635856913 7.3635856800 -0.49999998 87
tire program in 8 signiﬁcant digits. Formally, such a value
can be obtained from the following procedure. Let vbe the
value obtained when double precision is used throughoutthe entire program, and v
8is the value of vin precision db8.
According to the IEEE 754-2008 standard, the binary repre-sentation of vhas 52 explicitly stored bits in the signiﬁcand.
We ﬁrst ﬁnd the number of bits that corresponds to 8 signiﬁ-cant decimal digits. The number of bits can be computed as
lg(10
8)=2 6.57 bits. We therefore keep 27 bits, and set the
remaining bits in the signiﬁcand to 0 to obtain the value v8.1
Back to our example, when the ﬁnal result is computed in
doubleprecision, the result is r e s=0 . 50000001 13. When
the computation is performed solely in singleprecision (all
variables in the program are declared as floatinstead of
double), the result is res = 0.4999980927. Assuming that
we require the result to have precision db8, the result would
beres=0 .50000001 xy, where xandycan be any decimal
digits. Precision tuning is based on the precision require-ment set for the ﬁnal result(s) of the program.
For each instruction in the program, Blame Analysis de-
termines the precision that the corresponding operands arerequired to carry in order for its result to be accurate to a
given precision. For example, let us consider the statement
on line 23: r 3=r 2-t 1 , and precision db
8for its result.
Since the double value of r3is-0.49999998 87, this means
thatwerequire r3tobe -0.49999998 (i.e., thevaluematches
to 8 signiﬁcant digits). In order to determine the precision
requirement for the two operands (r2 and t1), we perform
the subtraction operation with operands in all considered
precisions. Table 1 shows some of the precision combina-tions we use for the operands. For example, ( fl,db
8)m e a n s
that r2hasflprecision, and t1hasdb8precision. For this
particular statement, all but one operand precision combi-nations fail. Only until we try ( db,db), then we obtain a
result that satisﬁes the precision requirement for the result(see last row of Table 1). Blame Analysis will record that
the precision requirement for the operands in the statementon line 23 is (db, db) when the result is required to have pre-
cision db
8. Similarly, operand precision requirements will
also be recorded when the result is required to have other
precisions under consideration (fl,a n d dbin this example).
Statements that occur inside loops are likely to be exe-
cuted more than once, such as line 5: r e s=r e s*a .F o r
1Similarly, if we are interested in 4, 6, or 10 signiﬁcant dec-
imal digits, we can keep 13, 19, or 33 signiﬁcant bits in the
signiﬁcand respectively, and set other bits to 0.prog::= (l:instr)∗
instr::=x=y aop z|x=yb o pz|
ifxgotoL|
x=nativefun( y)|x=c
aop::= +|−|∗| /
bop::= = | /negationslash=|<|≤
nativefun ::= sin|cos|fabs
l∈Labels x,y,z ∈Variables c ∈Constants
Figure 3: Kernel Language
the precision requirement db8for the result of this opera-
tion, the ﬁrst time this statement is executed the analysis
records the double values for the operands and the result
(6.0000000000, 1.8408964200, 11.0453785200). The algo-rithmtriesdiﬀerentprecisioncombinationsfortheoperands,and determines that precision (fl, db
8) suﬃces. The second
time the statement is executed, the analysis records new
doublevalues(11.0453785200, 1.8408964200, 20.3333977750).
After trying all precision combinations for the operands, it isdetermined that this time the precision required is ( db,db
8),
which is diﬀerent from the requirement set the ﬁrst time
the statement was examined. At this point, it is necessary
tomergeboth of these precision requirements to obtain a
uniﬁed requirement. In Blame Analysis, the merge oper-
ation over-approximates the precision requirements. In thisexample, merging ( fl,db
8) and ( db,db8) would result in the
precision requirement ( db,db8).
Finally, after computing the precision requirements for ev-
ery instruction in the program, the analysis performs a back-ward pass starting from the target statement on line 24,
and considering the precision requirement for the ﬁnal re-
sult. The pass ﬁnds the program dependencies and requiredprecisions, and collects all variables that are determined tobe in single precision. Concretely, if we require the ﬁnalresult computed on line 24 to be accurate to 8 digits db
8,
the backward pass ﬁnds that the statement on line 24 de-pends on statement on line 23, which depends on statementson lines 22 and 15, and so on, along with the correspond-ing precision requirements. The analysis then collects the
variables that can be allocated in single precision. In this
example, only variable factorin function mpowcan be allo-
cated in single precision (it always stores integer constantswhich do not require double precision).
In the rest of this section, we formally describe our Blame
Analysis algorithm and its implementation. Our imple-
mentation of Blame Analysis consists of two main com-
ponents: a shadow execution engine for performing singleand double precision computation side-by-side with the con-
crete execution (Section 3.2), and an online Blame Anal-
ysisalgorithm integrated inside the shadow execution run-
time (Section 3.3). Finally, we present analysis heuristicsand optimizations (Section 3.4).
3.2 Shadow Execution
Figure 3 introduces a kernel language used to formally de-
scribe our algorithm. The language includes standard arith-metic and boolean expressions. It also includes an assign-ment statement which assigns a constant value to a variable.Other instructions include if-goto and native function call
instructions such as sin, cos,a n dfabs.
In our shadow execution engine, each concrete ﬂoating-
point value in the program has an associated shadow value .
Each shadow value carries two values corresponding to the
1077Procedure FAddShadow
Inputs
/lscript:x=y+z: instruction
Outputs
Updates the shadow memory Mand the label map LM
Method
1{single: y single ,d o u b l e :y double }=M [ & y ]
2{single: z single ,d o u b l e :z double }=M [ & z ]
3M[&x] = {single: y single +zsingle ,d o u b l e :y double +zdouble }
4LM[&x] = /lscript
Figure 4: Shadow Execution of faddInstruction
concrete value when the program is computed entirely in
singleordoubleprecision. We will represent a shadow value
of a value vas{single:vsingle,double :vdouble}, where
vsingleandvdoublearethevaluescorrespondingto vwhenthe
program is computed entirely in singleanddoubleprecision,
respectively.
Inourimplementation, theshadowexecutionisperformed
side-by-side with the concrete execution. We instrumentcallbacks for all ﬂoating-point instructions in the program.
The shadow execution runtime interprets the callbacks fol-lowing the same semantics of the corresponding instructions,however, it computes shadow rather than concrete values.
LetAbe the set of all memory addresses used by the
program, Sbe the set of all shadow values associated with
the concrete values computed by the program, and Lbe
the set of labels of all instructions in the program. Shadowexecution maintains two data structures:
1. A shadow memory Mthat maps a memory address
to a shadow value, i.e., M:A→S.I fM(a)=sfor
some memory address a, then it denotes that the value
stored at address ahas the associated shadow value s.
2. A label map LMthat maps a memory address to an
instruction label, i.e., LM:A→L.I fLM(a)=/lscriptfor
some memory address a, then it denotes that the value
stored at address awas last updated by the instruction
labeled/lscript.
As an example, Figure 4 shows how MandLMare up-
dated when an faddinstruction /lscript:x=y+zis executed.
In this example, x,y,zare variables and /lscriptis an instruction
label. We also denote & x,&y,&zas the addresses of the
variables x,
 y,z, respectively, in that state. In this example,
theprocedure FAddShadow isthecallbackassociatedwiththe
faddinstruction. The procedure re-interprets the semantics
of the faddinstruction (see line 3), but it uses the shadow
values for the corresponding operands (retrieved on lines 1and 2), and creates/updates the shadow value associated
withx.T h el a b e lm a p LMis updated on line 4 to record
thatxhas been last updated at the instruction labeled /lscript.
3.3 Building the Blame Sets
In this section, we formally describe our analysis. Let A
be the set of all memory addresses used by the program, L
be the set of labels of all instructions in the program, Pbe
the set of all precisions, i.e., P={fl,db4,db6,db8,db10,db}.
Precisions fland dbstand for single and double precisions,
respectively. Precisions db4,db6,db8,db10denotevaluesthatFunction BlameSet
Inputs
/lscript:x=f(y1,..., yn) : instruction with label /lscript
p: precision requirement
Outputs
{(/lscript1,p1),···,(/lscriptn,pn)}: precision requirements of the
instructions that computed the operands
Method
1accurate res = trunc shadow(M [&x],p)
2(s1,...,sn)=(M[&y 1],...,M[&yn])
3ﬁnd minimal precisions p 1,..., pnsuch that the following holds:
4 (v1,...,vn)=( t r u n c shadow(s 1,p1),... ,trunc shadow(s n,pn))
5 trunc(f(v 1,...,vn),p)==accurate res
6return {(LM[&y 1],p1),...,(LM[&yn],pn)}
Figure 5: BlameSet Procedure
are accurate up to 4, 6, 8 and 10 digits in double precision,
respectively. We alsodeﬁne atotalorderonprecisionsas fol-lows: fl<db
4<db6<db8<db10<db.I n Blame Anal-
ysiswe also maintain a blame map Bthat maps a pair of
instruction label and precision to a set of pairs of instructionlabelsandprecisions, i.e., B:L×P→P(L×P), where P(X)
denotes the power set of X.I fB(/lscript,p)={(/lscript
1,p1),(/lscript2,p2)},
then it means that during an execution if instruction labeled
/lscriptproduces a value that is accurate up to precision p, then
instructions labeled /lscript1and/lscript2must produce values that are
accurate up to precision p1andp2, respectively.
The blame map Bis updated on the execution of every
instruction. We initialize Bto the empty map at the begin-
ning of an execution. We illustrate how we update Busing a
simple generic instruction of the form /lscript:x=f(y1,...,y n),
wherex,y1,...,y nare variables and fis an operator, which
could be +, −,∗,sin, log, etc. In a program run consider a
state where this instruction is executed. Let us assume that&x,&y
1,...,&y ndenote the addresses of the variables x,
y1,...,y n, respectively, in that state. When the instruction
/lscript:x=f(y1,...,y n) is executed during concrete execution,
we also perform a side-by-side shadow execution of the in-struction to update B(/lscript,p)f o re a c hp ∈Pas follows. We
use two functions, BlameSet and merge /unionsq,t ou p d a t e B(/lscript,p).
The function BlameSet receives an instruction and a pre-
cision requirement as input, and returns the precision re-
quirements for the instructions that deﬁne the values of
its operands. Figure 5 shows the pseudocode of the func-
tion BlameSet. The function ﬁrst computes the accurate
result by retrieving the shadow value corresponding to theinput instruction, and truncating the shadow value to pre-
cision p(line 1). Function trunc
shadow(s,p) returns the
ﬂoating-point value corresponding to the precision pgiven
the shadow value s. Speciﬁcally, if pis single precision, then
t h es i n g l ev a l u eo fs is returned, otherwise trunc shadowre-
turns the double value of struncated to p.T h e s h a d o w
values corresponding to all operand variables are retrieved
on line 2. Then, the procedure ﬁnds the minimal precisionsp
1,...,pnsuch that if we apply fons1,...,sntruncated to
precisions p1,...,pn, respectively, then the result truncated
to precision pis equal to the accurate result computed on
line 1. Function trunc(x,p) returns xtruncated to precision
p. We then pair each piwithLM[&yi], the last instruction
that computed the value yi, and return the resulting set of
pairs of instruction labels and precisions.
The merge function /unionsqis deﬁned as
/unionsq:P(L×P)×P(L×P)→P(L×P)
1078If (/lscript,p 1), (/lscript,p 2),...,(/lscript,pn) are all the pairs involving the
label/lscriptpresent in LP1orLP2,t h e n(/lscript,max(p1,p2,...,p n))
is the only pair involving /lscriptpresent in (LP 1/unionsqLP2).
Given the functions BlameSet and merge /unionsq, we compute
B(/lscript,p)/unionsqBlameSet( /lscript:x=f(y1,...,y n),p) and use the re-
sulting set to update B(/lscript,p).
At the end of an execution we get a non-empty map B.
Suppose we want to make sure that the result computed by
a given instruction labeled /lscriptoutis accurate up to precision
p. Then we want to know what should be the accuracy
of the results computed by the other instructions so thatthe accuracy of the result of the instruction labeled /lscript
outis
p. We compute this using the function Accuracy( /lscriptout,p,B)
which returns a set of pairs instruction labels and precisions,such that if ( /lscript
/prime,p/prime) is present in Accuracy( /lscriptout,p,B), then
the result of executing the instruction labeled /lscript/primemust have a
precisionofatleast p/prime.Accuracy( /lscript,p,B)canthenbedeﬁned
recursively as follows.
Accuracy( /lscript,p,B)={(/lscript,p)}/unionsq/unionsqdisplay
(/lscript/prime,p/prime)∈B(/lscript,p)Accuracy( /lscript/prime,p/prime,B)
Aftercomputing Accuracy( /lscriptout,p,B), weknowthatif( /lscript/prime,p/prime)
is present in Accuracy( /lscriptout,p,B), then the instruction la-
beled/lscript/primemust be executed with precision at least p/primefor the
result of executing instruction /lscriptoutto have a precision p.
3.4 Heuristics and Optimizations
To attain scalability for large or long running programs,
the implementation of Blame Analysis must address mem-
ory usage and running time. We have experimented with
bothonlineandoﬄineversions of our algorithm.
The oﬄine Blame Analysis ﬁrst collects the complete
execution trace, and then builds the blame set for each dy-
namicinstruction (i.e., if a static instruction is executed
more than once, a blame set will be computed for each timethe instruction was executed). As each instruction is exam-ined only once, merging operand precisions is not required.
Thus, whencomparedtoonline Blame Analysis, theoﬄine
approach often produces better (lower precision) solutions.
However, the size of the execution trace, and the blame setinformation explode for long running programs. For exam-ple, when running oﬄine Blame Analysis on the epNAS
[37] benchmark with input class
2S, the analysis terminates
with an out of memory error, exhausting 256 GB of RAM.
The online Blame Analysis is more memory eﬃcient be-
cause the size of the blame sets is bounded by the number of
staticinstructions in the program. As shown in Section 4.2,
the maximum analysis working set size is 81 MB for ep.O n
the other hand, the blame sets for each instruction have to
be merged across all its dynamic invocations, making theanalysis slower. In our implementation, we allow develop-ers to specify what part of the program they are interestedto analyze. For short running programs, such as functionswithin the GSL [17] library, examining all instructions isfeasible. Most long running scientiﬁc programs fortunatelyuse iterative solvers, rather than direct solvers. In this case,
analyzing the last few algorithmic iterations is likely to lead
to a good solution, given that precision requirements are in-creased towards the end of the execution. This is the casein the NAS benchmarks we have considered. If no options
are speciﬁed, Blame Analysis by default will be performed
2Class S is a small input, designed for serial execution.
LLVM Bitcode 
Instrumentation 
Instrumented 
LLVM Bitcode 
Online Blame 
Analysis 
Proposed Type 
Configuration 
Analysis Input 
Parameters 
I
L
O
ine Bl
de
Test Inputs 
posed T
Figure 6: Blame Analysis Architecture
throughout the entire program execution.
In summary, our results show that oﬄine Blame Anal-
ysisis fast (no merge operations) and produces better so-
lutions for small programs, but it is expensive in terms of
memory usage, which makes it impractical. In contrast, on-
line Blame Analysis is memory eﬃcient, produces good
solutions, and it is not too expensive in terms of running
time, thus it has the potential to perform better when ana-lyzing larger programs. For brevity, the results reported inthe rest of this paper are obtained using the online analysis.
4. EXPERIMENTAL EV ALUATION
The Blame Analysis architecture is described in Fig-
ure 6. We build the analysis on top of the LLVMcompiler
infrastructure [27]. The analysis takes as input: (1) LLVM
bitcode of the program under analysis, (2) a set of test in-puts, and (3) analysis parameters that include the targetinstruction and the desired error threshold(s). Because the
analysis is implemented using LLVM, it can be applied to
programs written in languages that have a LLVM compilerfrontend (e.g., C, C++, and Fortran). We use the origi-nalPrecimonious benchmarks (written in C), which have
been modiﬁed by experts to provide acceptability criteria forthe result precision. For Blame Analysis we select the ac-
ceptability code developed for Precimonious as the target
instruction set. Thus, the results provided by both analyses
always satisfy the programmer speciﬁed precision criteria.
The analysis result consists of the set of variables that
can be in single precision. In this section, we present theevaluation of Blame Analysis b yi t s e l f ,a sw e l la sw h e n
used as a pre-processing stage for Precimonious .W er e f e r
to the latter as Blame + Precimonious . We compare this
combined approach with using Precimonious alone, and
perform an evaluation in terms of the analysis running time,and the impact of the analysis results in improving programperformance. We validate all the results presented in this
section by manually modifying the programs according to
the type assignments suggested by the tools, and runningthem to verify that the corresponding ﬁnal results are asaccurate as required for all test inputs.
4.1 Experiment Setup
We present results for eight programs from the GSLli-
brary [17] and two programs from the NASparallel bench-
marks [37]. We use clangwith no optimizations3and a
3Optimizations sometimes remove ﬂoating-point variables,
which causes the set of variables at the LLVM bitcode level
to diﬀer from the variables at the source code level.
1079Table 2: Overhead of Blame Analysis
Program Execution (sec) Analysis (sec) Overhead
cg 3.52 185.45 52.55×
ep 34.70 1699.74 48.98 ×
Python wrapper [34] to build whole program (or whole li-
brary) LLVM bitcode. Note that we do apply optimizationlevel -O2when performing ﬁnal performance measurements
on the tuned programs. We run our experiments on an In-
tel(R) Xeon(R) CPU E5-4640 0 @ 2.40GHz 8-core machine
running Linux with 256 GB RAM.
We use the procedure described in [35] to select program
inputs. For the NASbenchmarks (programs epandcg), we
use the provided input Class A. For the rest, we generate1000 random ﬂoating-point inputs, which we classify intogroups based on code coverage. We then pick one inputfrom each group, i.e., we want to maximize code coverage
while minimizing the number of inputs to consider. We log
and read the inputs in hexadecimal format to ensure thatthe inputs generated and the inputs used match at the bitlevel. We are indeed using the same set of inputs used in
the original evaluation of Precimonious.
In our experiments, we use error thresholds 10
−4,1 0−6,
10−8,a n d1 0−10, which correspond to 4, 6, 8 and 10 digits of
accuracy, respectively. Additionally, for NASprograms ep
and cg, we conﬁgure Blame Analysis to consider only the
last 10% of the executed instructions. For the rest of the
programs, Blame Analysis considers all the instructions
executed.
4.2 Analysis Performance
This section compares the performance of Blame Analy-
sisand its combination with Precimonious.W ea l s oc o m -
pare the onlineandoﬄineversions of Blame Analysis in
terms of memory usage.
By itself, Blame Analysis introduces up to 50 ×slow-
down, whichiscomparabletotheruntimeoverheadreportedby widely-used instrumentation based tools such as Val-grind [30] and Jalangi [39]. Table 2 shows the overhead
for programs cgand ep. For the rest of our benchmarks,
the overhead is relatively negligible (less than one second).
Tomeasuretheanalysistimeofthecombinedanalyses, we
add the analysis time of Blame Analysis and the search
time of Precimonious for each error threshold. Figure 7
shows the analysis time of Blame + Precimonious (B+P)
andPrecimonious (P) for each of our benchmarks. We use
all error thresholds for all benchmarks, except for program
ep. The original version of this program uses error threshold
10
−8, thus we do not consider error threshold 10−10.
Overall, we ﬁnd that Blame + Precimonious is faster
than Precimonious in 31 out of 39 experiments (4 error
thresholds for 9 programs, and 3 error thresholds for 1 pro-
gram). In general, we would expect that as variables are
removed from the search space, the overall analysis time will
be reduced. However, this is not necessarily true, especiallywhen very few variables are removed. In some cases, re-moving variables from the search space can alter the search
path of Precimonious , which results in a slower analysis
time. For example, in the experiment with error thresh-
old 10
−4forgaussian, Blame Analysis removes only two
variables from the search space (see Table 4), a small reduc-tion that changes the search path and actually slows downTable 3: Average analysis time speedup of Blame + Prec-
imonious compared to Precimonious alone
Program Speedup Program Speedup
bessel 22.48 ×sum 1.85×
gaussian 1.45× ﬀt 1.54×
roots 18.32 ×blas 2.11×
polyroots 1.54× ep 1.23 ×
rootnewt 38.42 ×cg 0.99 ×
the analysis. For programs epand cg, the search space re-
duction results in analysis time speedup for Precimonious .
However, the overhead of Blame Analysis causes the com-
bined Blame + Precimonious running time to be slower
than Precimonious for programs ep(10−4and 10−6), and
cg(10−4). Figure 8 shows the analysis time breakdown.
Table 3 shows the average speedup per program for all er-
ror thresholds. We observe analysis time speedups for 9 outof 10 programs. The largest speedup observed is 38 .42×and
corresponds to the analysis of program rootnewt. When-
ever we observe a large speedup, Blame Analysis removes
a large number of variables from the search space of Prec-
imonious, at least for error thresholds 10
−4and 10−6(see
Table 4). This translates into signiﬁcantly shorter analy-sis time for Precimonious. The only experiment in which
Blame + Precimonious is slower than Precimonious,o n
average, is when analyzing the program cg, however the
slowdown observed is only 1%.
0 500 1000 1500 2000 2500 3000 3500 
B+P P B+P P B+P P B+P P B 
P 10-4                   10-6                     10-8                    10-10  
 seconds 
(a) ep
0 2000 4000 6000 8000 10000 12000 
B+P P B+P P B+P P B 
P 10-4                              10-6                                10-8 seconds 
(b) cg
Figure 8: Analysis time breakdown for Blame + Precimo-
nious(B+P) and Precimonious (P) for two NAS bench-
mark programs
In terms of memory usage, the onlineversion of Blame
Analysis uses up to 81 MB of memory in our experiments.
The most expensive benchmark in terms of analysis memory
usage is program ep. For this program, the oﬄineversion
of the analysis runs out memory (256 GB).
4.3 Analysis Results
Table 4 shows the type conﬁgurations found by Blame
Analysis (B), Blame + Precimonious (B+P) and Prec-
imonious (P), which consist of the numbers of variables
in double precision (D) and single precision (F). It alsoshows the initial type conﬁguration for the original pro-gram. Our evaluation shows that Blame Analysis is ef-
fective in lowering precision. In particular, in all 39 experi-ments, Blame Analysis successfully identiﬁes at least one
variable as float.I fw ec o n s i d e ra l l3 9e x p e r i m e n t s , Blame
Analysis removes from the search space 40% of the vari-
ables on average, with a median of 28%.
The type conﬁgurations proposed by Blame + Preci-
monious and Precimonious agree in 28 out of 39 exper-
iments, and diﬀer in 11 experiments. Table 5 shows the
10800 2000 4000 6000 8000 10000 12000 14000 16000 
10^-4 10^-6 10^-8 10^-10 P 
B+P Time (secs) 
Error threshold 
(a) bessel
Time (secs) 
Error threshold 
/g26/g1/g31/g26/g26/g26/g1/g27/g26/g26/g26/g26/g1/g27/g31/g26/g26/g26/g1/g28/g26/g26/g26/g26/g1/g28/g31/g26/g26/g26/g1/g29/g26/g26/g26/g26/g1/g29/g31/g26/g26/g26/g1/g30/g26/g26/g26/g26/g1
/g27/g26/g23/g22/g30/g1 /g27/g26/g23/g22/g32/g1 /g27/g26/g23/g22/g34/g1 /g27/g26/g23/g22/g27/g26/g1/g5/g1
/g2/g36/g5/g1
(b) gaussian
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 
10^-4 10^-6 10^-8 10^-10 P 
B+P Time (secs) 
Error threshold 
(c) roots
0 5000 10000 15000 20000 25000 30000 
10^-4 10^-6 10^-8 10^-10 P 
B+P Time (secs) 
Error threshold 
(d) polyroots
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 
10^-4 10^-6 10^-8 10^-10 P 
B+P 
Error threshold Time (secs) 
(e) rootnewt
0 5000 10000 15000 20000 25000 30000 35000 40000 
10^-4 10^-6 10^-8 10^-10 P 
B+P Time (secs) 
Error threshold 
(f) sum
0 2000 4000 6000 8000 10000 12000 
10^-4 10^-6 10^-8 10^-10 P 
B+P 
Error threshold Time (secs) 
(g) blas
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 
10^-4 10^-6 10^-8 10^-10 P 
B+P Time (secs) 
Error threshold 
(h) ﬀt
0 2000 4000 6000 8000 10000 12000 
10^-4 10^-6 10^-8 P 
B+P 
Error threshold Time (secs) 
(i) ep
Error threshold Time (secs) 
0 500 1000 1500 2000 2500 3000 3500 
10^-4 10^-6 10^-8 10^-10 P 
B+P 
(j) cg
Figure 7: Analysis time comparison between Precimonious (P) and Blame + Precimonious (B+P). The vertical axis
shows the analysis time in seconds. The horizontal axis shows the error thresholds used in each experiment. In these graphs,
a lower curve means the analysis is more eﬃcient.
speedup observed when we tune the programs according to
these type conﬁgurations. In all 11 cases in which the twoconﬁgurations diﬀer, the conﬁguration proposed by Blame
+P r e c i m o n i o u s produces the best performance improve-
ment. In particular, in three cases we observe 39.9% addi-
tional speedup.
In 31 out of 39 experiments, Blame + Precimonious
ﬁnds conﬁgurations that diﬀer from the conﬁgurations sug-
gested by Blame Analysis alone. Among those, 9 exper-
iments produce a conﬁguration that is diﬀerent from theoriginal program. This shows that our analysis is conserva-tive and Precimonious is still useful in further improving
conﬁgurations found by Blame Analysis alone.Note that for Blame Analysis , we have reported results
only for the onlineversion of the analysis. Our experi-
ments indicate that the oﬄineversion has memory scala-
bility problems and while its solutions sometimes are better
in terms of the number of variables that can be lowered to
single precision, it is not necessarily better at reducing anal-ysis running time, or the running time of the tuned program.
5. DISCUSSION
Blame Analysis has several limitations. First, similar
to other state-of-the-art tools for precision tuning, our anal-ysis cannot guarantee accurate outputs for all possible in-
1081Table 4: Conﬁgurations found by Blame Analysis (B), Blame + Precimonious (B+P), and Precimonious alone (P). The
column Initial gives the number of ﬂoating-point variables (double D, and ﬂoat F) declared in the programs. For each selected
error threshold, we show the type conﬁguration found by each of the three analyses B, B+P, and P (number of variables per
precision). ×denotes the cases where the tools select the original program as fastest.
Error Threshold 10−4Error Threshold 10−6
Initial B B+P P B B+P P
P r o g r a m D FD FD FD FD FD FD F
bessel 26 0 1 25 ×× ×× 12 5 ×× ××
gaussian 56 0 54 2 ×× ×× 54 2 ×× ××
roots 16 0 1 15 ×× ×× 11 5 ×× ××
polyroots 31 0 10 21 10 21 ×× 10 21 10 21 ××
rootnewt 14 0 1 13 ×× ×× 11 3 ×× ××
sum 34 0 24 10 11 23 11 23 24 10 11 23 ××
ﬀt 22 0 16 6 0 22 0 22 16 6 0 22 0 22
blas 17 0 1 16 0 17 0 17 1 16 0 17 0 17ep 45 0 42 3 42 3 ×× 42 3 42 3 ××
cg 32 0 26 6 2 30 2 30 28 4 13 19 13 19
Error Threshold 10−8Error Threshold 10−10
Initial B B+P P B B+P P
P r o g r a m D FD FD FD FD FD FD F
bessel 26 0 25 1 ×× ×× 25 1 ×× ××
gaussian 56 0 54 2 ×× ×× 54 2 ×× ××
roots 16 0 5 11 ×× ×× 51 1 ×× ××
polyroots 31 0 10 21 10 21 ×× 10 21 10 21 ××
rootnewt 14 0 5 9 ×× ×× 59 ×× ××
sum 34 0 24 10 11 23 ×× 24 10 24 10 ××
ﬀt 22 0 16 6 ×× ×× 16 6 ×× ××
blas 17 0 10 7 ×× ×× 10 7 ×× ××
ep 45 0 42 3 42 3 ×× -- -- --
cg 32 0 28 4 16 16 12 20 28 4 16 16 16 16
puts, thus there is the need for representative test inputs.
Although input generation has a signiﬁcant impact on thetype conﬁgurations recommended by our analysis, the prob-lem of generating ﬂoating-point inputs is orthogonal to theproblem addressed in this paper. In practice, we expect pro-
grammers will be able to provide meaningful inputs, or use
complementary input-generation tools [16]. Still, we believeour tool is a powerful resource for the programmer, who willultimately decide whether to apply the suggested conﬁgura-
tions fully or partially.
Another limitation is that Blame Analysis does not take
into account program performance; by itself, the suggested
conﬁgurations might not lead to program speedup. Notethat, in general, lowering precision does not necessarily re-
sult in a faster program. For example, consider the addition
v
1+v2. Assume v1has type floatandv2has type double.
The addition will be performed in doubleprecision, requir-
i n gt oc a s tv 1todouble. When a large number of such
casts is required, the tuned program might be slower thanthe original program. The analysis focuses on the impact inaccuracy, but does not consider the impact in running time.Because of this, the solutions produced are not guaranteedto improve performance.
Last, the program transformations suggested by Blame
Analysis arelimitedtochangingvariabledeclarationswhose
precision will remain the same throughout the execution ofthe program. We do not currently handle shift of preci-
sion during program execution, which could potentially con-
tribute to improving program performance. Also, the anal-ysis does not consider algorithmic changes that could alsopotentially improve running time. Note that both kinds oftransformations would require additional eﬀorts to express
program changes.
While very useful, automated tools for ﬂoating-point pre-
cision tuning have to overcome scalability concerns. As it
adds a constant overhead per instruction, the scalability ofour single-pass Blame Analysis is determined solely by the
programruntime. Thescalabilityof Precimonious isdeter-
mined by both program runtime and the number of variablesin the program. We believe that our approach uncovers very
exciting potential for the realization of tools able to handle
large codes. There are several directions to improve the ef-ﬁcacy of Blame Analysis as a standalone tool, as well as
a ﬁlter for Precimonious.
A future direction is to use Blame Analysis as an intra-
procedural analysis, rather than an interprocedural analy-sis as presented in this paper. Concretely, we can apply iton each procedure and use the conﬁgurations inferred foreach procedure to infer the conﬁguration for the entire pro-
gram. Doing so will enable the opportunity for parallelism
and might greatly improve the analysis time in modular pro-grams. Another future direction is to experiment with otherintermediate precisions. In this paper, we used four inter-
mediate precisions, db
4,db6,db8,a n d db10, to track preci-
sion requirements during the analysis. This proved a good
trade-oﬀ between the quality of the solution and runtimeoverhead. For some programs, increasing the granularity ofintermediate precisions may lead to more variables kept in
low precision, further pruning the search space of Precimo-
nious.
6. RELATED WORK
Precimonious [35] is a dynamic analysis tool for tun-
ing ﬂoating-point precision, already detailed. Lam et al.
[25] also propose a framework for ﬁnding mixed-precisionﬂoating-point computation. Lam’s approach uses a brute-force algorithm to ﬁnd double precision instructions that
can be replaced by single instructions. Their goal is to use
as many single instructions in place of double instructionsas possible, but not explicitly consider speedup as a goal.Blame Analysis diﬀers from Precimonious and Lam’s
framework in that it performs a white-box analysis on the
1082Table 5: Speedup observed after precision tuning using con-
ﬁgurations produced by Blame + Precimonious (B+P)
andPrecimonious alone (P)
Threshold 10−4Threshold 10−6
Program B+P P B+P P
bessel 0.0% 0.0% 0.0% 0.0%
gaussian 0.0% 0.0% 0.0% 0.0%
roots 0.0% 0.0% 0.0% 0.0%
polyroots 0.4% 0.0% 0.4% 0.0%rootnewt 0.0% 0.0% 0.0% 0.0%
sum 39.9% 39.9% 39.9% 0.0%
ﬀt 8.3% 8.3% 8.3% 8.3%blas 5.1% 5.1% 5.1% 5.1%
ep 0.6% 0.0% 0.6% 0.0%
cg 7.7% 7.7% 7.9% 7.9%
Threshold 10−8Threshold 10−10
Program B+P P B+P P
bessel 0.0% 0.0% 0.0% 0.0%
gaussian 0.0% 0.0% 0.0% 0.0%
roots 0.0% 0.0% 0.0% 0.0%polyroots 0.4% 0.0% 0.4% 0.0%
rootnewt 0.0% 0.0% 0.0% 0.0%
sum 39.9% 0.0% 0.0% 0.0%ﬀt 0.0% 0.0% 0.0% 0.0%
blas 0.0% 0.0% 0.0% 0.0%
ep 0.6% 0.0% - -
cg 7.9% 7.4% 7.9% 7.9%
set of instructions executed by the program under analysis,
rather than through searching. Thus, Blame Analysis is
not bounded by the exponential size of the variable or in-structionsearchspace. SimilartoLam’sframework, thegoalof our analysis is to minimize the use of double precision in
the program without considering performance.
Darulova et. al [19] develop a method for compiling a real-
valued implementation program into a ﬁnite-precision im-
plementation program, such that the ﬁnite-precision imple-
mentation program meets all desired precision with respect
to the real numbers, however the approach does not sup-port mixed precision. Schkufza et. al [38] develop a methodfor optimization of ﬂoating-point programs using stochasticsearch by randomly applying a variety of program transfor-
mations, which sacriﬁce bit-wise precision in favor of per-
formance. FloatWatch [11] is a dynamic execution proﬁling
tool for ﬂoating-point programs which is designed to identifyinstructions that can be computed in a lower precision by
computing the overall range of values for each instruction of
interest. As with other tools described in this paper, all theabove also face scalability challenges.
Darulova and Kuncak [18] also implemented a dynamic
range analysis feature for the Scala language that could beused for precision tuning purposes, by ﬁrst computing a dy-namic range for each instruction of interest and then tun-ing the precision based on the computed range, similar toFloatWatch. However, range analysis often incurs overesti-
mates too large to be useful for precision tuning analysis.
Gappa [20] is another tool that uses range analysis to verifyand prove formal properties of ﬂoating-point programs. Onecould use Gappa to verify ranges for certain program vari-ablesandexpressions, andthenchoosetheirappropriatepre-
cisions. Nevertheless, Gappa scales only to small programs
with simple structures and several hundreds of operations,and thus is used mostly for verifying elementary functions.
A large body of work exists on accuracy analysis [8, 6, 7,21, 26, 41, 46]. Benz et al. [8] present a dynamic approachthat consists on computing every ﬂoating-point instructions
side-by-side in higher precision, storing the higher precision
values in shadow variables. FPInst [1] is another tool thatcomputes ﬂoating-point errors to detect accuracy problems.It computes a shadow value side-by-side, but it stores anabsolute error in double precision instead. Herbie [31] esti-
mates and localizes rounding errors, and then rewrites nu-
merical expressions to improve accuracy. The above toolsaim to ﬁnd accuracy problems (and improve accuracy), notto ﬁnd opportunities to reduce ﬂoating-point precision.
Other large areas of research that focus on improving per-
formance are autotuning (e.g., [9, 22, 33, 42, 43]) and ap-proximate computing (e.g., [10, 15, 29, 36, 40]). However,no previous work has tried to tune ﬂoating-point precisionas discussed in this paper. Finally, our work on Blame
Analysis is related to other dynamic analysis tools that
employ shadow execution and instrumentation [39, 30, 32,12]. These tools, however, are designed as general dynamicanalysis frameworks rather than specializing in analyzingﬂoating-point programs like ours.
7. CONCLUSION
We introduce a novel dynamic analysis designed to tune
the precision of ﬂoating-point programs. Our implementa-
tion uses a shadow execution engine and when applied to a
set of ten programs it is able to compute a solution with atmost 50×runtime overhead. Our workload contains a com-
bination of small to medium size programs, some that arelong running. The code is open source and available online
4.
When used by itself, Blame Analysis is able to lower
the precision for all tests, but the results do not necessar-ily translate into execution time improvement. The largestimpact is observed when using the analysis as a ﬁlter to
prune the inputs to Precimonious, a ﬂoating-point tuning
tool that searches through the variable space. The com-
bined analysis time is 9 ×faster on average, and up to 38 ×
in comparison to Precimonious alone. The resulting type
conﬁgurations improve program execution time by as much
as 39.9%.
We believe that our results are very encouraging and in-
dicate that ﬂoating-point tuning of entire applications will
become feasible in the near future. As we now understand
the more subtle behavior of Blame Analysis,w eb e l i e v e
we can improve both analysis speed and the quality of the
solution. It remains to be seen if this approach to developfast but conservative analyses can supplant the existing slowbut powerful search-based methods. Nevertheless, our work
proves that using a fast “imprecise” analysis to bootstrap
another slow but precise analysis can provide a practical so-lution to tuning ﬂoating point in large code bases.
8. ACKNOWLEDGMENTS
Support for this work was provided through the X-Stack
program funded by the U.S. Department of Energy, Oﬃceof Science, Advanced Scientiﬁc Computing Research undercollaborative agreement numbers DE-SC0008699 and DE-
SC0010200. This work was also partially funded by DARPA
award number HR0011-12-2-0016, and ASPIRE Lab indus-trial sponsors and aﬃliates Intel, Google, Hewlett-Packard,Huawei, LGE, NVIDIA, Oracle, and Samsung.
4https://github.com/corvette-berkeley/shadow-execution
10839. REFERENCES
[1] D. An, R. Blue, M. Lam, S. Piper, and G. Stoker.
Fpinst: Floating point error analysis using dyninst,
2008.
[2] H. Anzt, D. Lukarski, S. Tomov, and J. Dongarra.
Self-adaptive multiprecision preconditioners onmulticore and manycore architectures. Proceedings of
11th International Meeting High PerformanceComputing for Computational Science, VECPAR ,
2014.
[3] M. Baboulin, A. Buttari, J. Dongarra, J. Kurzak,
J. Langou, J. Langou, P. Luszczek, and S. Tomov.
Accelerating scientiﬁc computations with mixed
precision algorithms. Computer Physics
Communications, 180(12):2526–2533, 2009.
[4] D. H. Bailey. High-precision ﬂoating-point arithmetic
in scientiﬁc computation. Computing in Science and
Engg., 7(3):54–61, May 2005.
[5] D. H. Bailey and J. M. Borwein. High-precision
arithmetic: Progress and challenges.
[6] T. Bao and X. Zhang. On-the-ﬂy detection of
instability problems in ﬂoating-point programexecution. In OOPSLA’13 , pages 817–832, 2013.
[7] E. T. Barr, T. Vo, V. Le, and Z. Su. Automatic
detection of ﬂoating-point exceptions. In R. Giacobazziand R. Cousot, editors, The 40th Annual ACM
SIGPLAN-SIGACT Symposium on Principles ofProgramming Languages, POPL ’13, Rome, Italy -January 23 - 25, 2013, pages 549–560. ACM, 2013.
[8] F. Benz, A. Hildebrandt, and S. Hack. A dynamic
program analysis to ﬁnd ﬂoating-point accuracyproblems. In J. Vitek, H. Lin, and F. Tip, editors,
ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI ’12,Beijing, China - June 11 - 16, 2012 , pages 453–462.
ACM, 2012.
[9] J. Bilmes, K. Asanovi´ c, C. Chin, and J. Demmel.
Optimizing matrix multiply using PHiPAC: aportable, high-performance, ANSI C codingmethodology. In Proceedings of the International
Conference on Supercomputing , Vienna, Austria, July
1997. ACM SIGARC. see
http://www.icsi.berkeley.edu/~bilmes/phipac.
[10] B. Boston, A. Sampson, D. Grossman, and L. Ceze.
Probability Type Inference for Flexible Approximate
Programming. To appear in OOPSLA, 2015.
[11] A. W. Brown, P. H. J. Kelly, and W. Luk. Proﬁling
ﬂoating point value ranges for reconﬁgurableimplementation. In Proceedings of the 1st HiPEAC
Workshop on Reconﬁgurable Computing , pages 6–16,
2007.
[12] D. Bruening, T. Garnett, and S. Amarasinghe. An
infrastructure for adaptive dynamic optimization. InProceedings of the International Symposium on Code
Generation and Optimization: Feedback-directed and
Runtime Optimization , 2003.
[13] A. Buttari, J. Dongarra, J. Kurzak, P. Luszczek, and
S. Tomov. Using mixed precision for sparse matrixcomputations to enhance the performance whileachieving 64-bit accuracy. ACM Trans. Math. Softw. ,
34(4):17:1–17:22, July 2008.
[14] A. Buttari, J. Dongarra, J. Langou, J. Langou,P. Luszczek, and J. Kurzak. Mixed precision iterativereﬁnement techniques for the solution of dense linearsystems. Int. J. High Perform. Comput. Appl. , 21(4),
Nov. 2007.
[15] M. Carbin, S. Misailovic, and M. C. Rinard. Verifying
quantitative reliability for programs that execute onunreliable hardware. In A. L. Hosking, P. T. Eugster,and C. V. Lopes, editors, Proceedings of the 2013
ACM SIGPLAN International Conference on Object
Oriented Programming Systems Languages &
Applications, OOPSLA 2013, part of SPLASH 2013,Indianapolis, IN, USA, October 26-31, 2013 , pages
33–52. ACM, 2013.
[16] W. Chiang, G. Gopalakrishnan, Z. Rakamaric, and
A. Solovyev. Eﬃcient search for inputs causing high
ﬂoating-point errors. In J. Moreira and J. R. Larus,
editors,ACM SIGPLAN Symposium on Principles
and Practice of Parallel Programming, PPoPP ’14,Orlando, FL, USA, February 15-19, 2014 ,p a g e s
43–52. ACM, 2014.
[17] G. P. Contributors. GSL - GNU scientiﬁc library -
GNU project - free software foundation (FSF).http://www.gnu.org/software/gsl/, 2010.
[18] E. Darulova and V. Kuncak. Trustworthy numerical
c o m p u t a t i o ni ns c a l a .I nC .V .L o p e sa n dK .F i s h e r ,editors,Proceedings of the 26th Annual ACM
SIGPLAN Conference on Object-OrientedProgramming, Systems, Languages, and Applications,OOPSLA 2011, part of SPLASH 2011, Portland, OR,USA, October 22 - 27, 2011 , pages 325–344. ACM,
2011.
[19] E. Darulova and V. Kuncak. Sound compilation of
reals. In S. Jagannathan and P. Sewell, editors, The
41st
Annual ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, POPL ’14, SanDiego, CA, USA, January 20-21, 2014,p a g e s
235–248. ACM, 2014.
[20] F. de Dinechin, C. Q. Lauter, and G. Melquiond.
Certifying the ﬂoating-point implementation of an
elementary function using gappa. pages 242–253, 2011.
[21] D. Delmas, E. Goubault, S. Putot, J. Souyris,
K. Tekkal, and F. V´ edrine. Towards an industrial use
of FLUCTUAT on safety-critical avionics software. In
M. Alpuente, B. Cook, and C. Joubert, editors,
Formal Methods for Industrial Critical Systems, 14thInternational Workshop, FMICS 2009, Eindhoven,The Netherlands, November 2-3, 2009. Proceedings ,
volume 5825 of Lecture Notes in Computer Science,
pages 53–69. Springer, 2009.
[22] M. Frigo. A Fast Fourier Transform compiler. In
Proceedings of the ACM SIGPLAN Conference onProgramming Language Design and Implementation,Atlanta, Georgia , May 1999.
[23] Y. He and C. H. Q. Ding. Using accurate arithmetics
to improve numerical reproducibility and stability inparallel applications. Journal of Supercomputing,
18:259–277, 2001.
[24] Y. Hida, X. S. Li, and D. H. Bailey. Algorithms for
quad-double precision ﬂoating point arithmetic. InARITH’01 , 2001.
[25] M. O. Lam, J. K. Hollingsworth, B. R. de Supinski,
and M. P. LeGendre. Automatically adapting
1084programs for mixed-precision ﬂoating-point
computation. In ICS’13, 2013.
[26] M. O. Lam, J. K. Hollingsworth, and G. W. Stewart.
Dynamic ﬂoating-point cancellation detection. Parallel
Computing , 39(3):146–155, 2013.
[27] C. Lattner and V. S. Adve. LLVM: A compilation
framework for lifelong program analysis &transformation. In CGO’04, pages 75–88, 2004.
[ 2 8 ]X .S .L i ,J .W .D e m m e l ,D .H .B a i l e y ,G .H e n r y ,
Y. Hida, J. Iskandar, W. Kahan, S. Y. Kang,
A. Kapur, M. C. Martin, B. J. Thompson, T. Tung,
and D. J. Yoo. Design, implementation and testing ofextended and mixed precision BLAS. ACM Trans.
Math. Softw. , 28(2):152–205, June 2002.
[29] S. Misailovic, M. Carbin, S. Achour, Z. Qi, and M. C.
Rinard. Chisel: reliability- and accuracy-aware
optimization of approximate computational kernels. In
A. P. Black and T. D. Millstein, editors, Proceedings
of the 2014 ACM International Conference on ObjectOriented Programming Systems Languages &Applications, OOPSLA 2014, part of SPLASH 2014,
Portland, OR, USA, October 20-24, 2014 ,p a g e s
309–328. ACM, 2014.
[30] N. Nethercote and J. Seward. Valgrind: A framework
for heavyweight dynamic binary instrumentation. In
Proceedings of the 2007 ACM SIGPLAN Conference
o nP r o g r a m m i n gL a n g u a g eD e s i g na n d
Implementation, PLDI ’07, 2007.
[31] P. Panchekha, A. Sanchez-Stern, J. R. Wilcox, and
Z. Tatlock. Automatically improving accuracy forﬂoating point expressions. In D. Grove andS. Blackburn, editors, Proceedings of the 36th ACM
SIGPLAN Conference on Programming LanguageDesign and Implementation, Portland, OR, USA,June 15-17, 2015, pages 1–11. ACM, 2015.
[32] H. Patil, C. Pereira, M. Stallcup, G. Lueck, and
J. Cownie. Pinplay: a framework for deterministicreplay and reproducible analysis of parallel programs.
InProceedings of the CGO 2010, The 8th
International Symposium on Code Generation and
Optimization, pages 2–11, 2010.
[33] M. P ¨uschel, J. M. F. Moura, J. Johnson, D. Padua,
M. Veloso, B. Singer, J. Xiong, F. Franchetti,
A. Gacic, Y. Voronenko, K. Chen, R. W. Johnson, and
N. Rizzolo. SPIRAL: Code generation for DSPtransforms. Proceedings of the IEEE, special issue on
“Program Generation, Optimization, and Adaptation” ,
93(2):232– 275, 2005.
[34] T. Ravitch. LLVM Whole-Program Wrapper.
https://github.com/travitch/whole-program-llvm,Mar. 2011.
[35] C. Rubio-Gonz´ alez, C. Nguyen, H. D. Nguyen,
J. Demmel, W. Kahan, K. Sen, D. H. Bailey, C. Iancu,and D. Hough. Precimonious: tuning assistant forﬂoating-point precision. In SC’13, page 27, 2013.
[36] A. Sampson, A. Baixo, B. Ransford, T. Moreau,J. Yip, L. Ceze, and M. Oskin. ACCEPT: AProgrammer-Guided Compiler Framework forPractical Approximate Computing. 2015.
[37] W. Saphir, R. V. D. Wijngaart, A. Woo, and
M. Yarrow. New implementations and results for the
nas parallel benchmarks 2. In In 8th SIAM Conference
on Parallel Processing for Scientiﬁc Computing, 1997.
[38] E. Schkufza, R. Sharma, and A. Aiken. Stochastic
optimization of ﬂoating-point programs with tunable
precision. In M. F. P. O’Boyle and K. Pingali, editors,ACM SIGPLAN Conference on ProgrammingLanguage Design and Implementation, PLDI ’14,
Edinburgh, United Kingdom - June 09 - 11, 2014 ,
page 9. ACM, 2014.
[39] K. Sen, S. Kalasapur, T. G. Brutch, and S. Gibbs.
Jalangi: a selective record-replay and dynamic
analysis framework for javascript. In ESEC/FSE’13,
pages 488–498, 2013.
[40] S. Sidiroglou-Douskos, S. Misailovic, H. Hoﬀmann,
and M. C. Rinard. Managing performance vs.accuracy trade-oﬀs with loop perforation. InT. Gyim´ othy and A. Zeller, editors,
SIGSOFT/FSE’11 19th ACM SIGSOFT Symposium
on the Foundations of Software Engineering (FSE-19)
and ESEC’11: 13rd European Software EngineeringConference (ESEC-13), Szeged, Hungary, September5-9, 2011 , pages 124–134. ACM, 2011.
[41] A. Solovyev, C. Jacobsen, Z. Rakamaric, and
G. Gopalakrishnan. Rigorous estimation of
ﬂoating-point round-oﬀ errors with symbolic taylor
expansions. In N. Bjørner and F. D. de Boer, editors,FM 2015: Formal Methods - 20th InternationalSymposium, Oslo, Norway, June 24-26, 2015,
Proceedings , volume 9109 of Lecture Notes in
Computer Science , pages 532–550. Springer, 2015.
[42] R. Vuduc, J. Demmel, and K. Yelick. OSKI: A library
of automatically tuned sparse matrix kernels. In Pr
 oc.
of SciDAC 2005, J. of Physics: Conference Series .
Institute of Physics Publishing, June 2005.
[43] C. Whaley. Automatically Tuned Linear Algebra
Software (ATLAS). math-atlas.sourceforge.net, 2012.
[44] I. Yamazaki, S. Tomov, T. Dong, and J. Dongarra.
Mixed-precision orthogonalization scheme and
adaptive step size for ca-gmres on gpus. Proceedings of
11th International Meeting High PerformanceComputing for Computational Science, VECPAR,
2014.
[45] A. Zeller and R. Hildebrandt. Simplifying and
isolating failure-inducing input. IEEE Trans. Software
Eng., 28(2):183–200, 2002.
[46] D. Zou, R. Wang, Y. Xiong, L. Zhang, Z. Su, and
H. Mei. A genetic algorithm for detecting signiﬁcant
ﬂoating-point inaccuracies. In 37th IEEE/ACM
International Conference on Software Engineering,ICSE 2015, Florence, Italy, May 16-24, 2015, Volume1, pages 529–539. IEEE, 2015.
1085