Search-Based Test Data Generation for SQL Queries
Jeroen Castelein1, Maurício Aniche1, Mozhan Soltani1, Annibale Panichella1,2, Arie van Deursen1
1Delft University of Technology,2Snt-University of Luxembourg
jeroencastelein11@gmail.com,{m.f.aniche,m.soltani,a.panichella,a.vandeursen}@tudelft.nl
ABSTRACT
Database-centric systems strongly rely on SQL queries to manage
andmanipulatetheirdata.TheseSQLcommandscanrangefrom
very simple selections to queries that involve several tables, sub-
queries,andgroupingoperations.And,aswithanyimportantpiece
of code, developers should properly test SQL queries. In order to
completely test a SQL query, developers need to create test datathat exercise all possible coverage targets in a query, e.g., JOINs
andWHEREpredicates.Andindeed,thistaskcanbechallenging
and time-consuming for complex queries. Previous studies havemodeled the problem of generating test data as a constraint sat-
isfactionproblemand,withthehelpofSATsolvers,generatethe
requireddata.However,suchapproacheshavestronglimitations,
such as partial support for queries with JOINs, subqueries, andstrings(whicharecommonlyusedinSQLqueries).Inthispaper,
we model test data generation for SQL queries as a search-based
problem. Then, we devise and evaluate three different approaches
basedonrandomsearch,biasedrandomsearch,andgeneticalgo-
rithms(GAs).TheGA,inparticular,usesafitnessfunctionbased
on information extracted from the physical query plan of a data-
baseengineassearchguidance.Wethenevaluateeachapproach
in 2,135 queries extracted from three open source software andoneindustrialsoftwaresystem.OurresultsshowthatGAisableto completely cover 98.6% of all queries in the dataset, requiring
onlyafewsecondsperquery.Moreover,itdoesnotsufferfromthe
limitations affecting state-of-the art techniques.
CCS CONCEPTS
•Software and its engineering →Software verification and
validation; Search-based software engineering;
KEYWORDS
search-basedsoftwareengineering,automatedtestdatageneration,
SQL, databases.
ACM Reference Format:
JeroenCastelein1,MaurícioAniche1,MozhanSoltani1,AnnibalePanichella1,2,
ArievanDeursen1.2018.Search-BasedTestDataGenerationforSQLQueries.
InProceedingsofICSE’18:40thInternationalConferenceonSoftwareEngi-
neering , Gothenburg, Sweden, May 27-June 3, 2018 (ICSE ’18), 11 pages.
https://doi.org/10.1145/3180155.3180202
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.31802021 INTRODUCTION
SQL queries form the heart of database-centric applications, which
can range from systems dealing with customer relations to applica-
tionsmanaging medicaldata forhospitals. Softwareengineers are
thenrequiredtotestsuchSQLqueriesasintensivelyastheytest
program code. However, the complexity of generating test data re-
quiredtofullytestaSQLquerygrowstogetherwiththecomplexity
of the query itself.
Consider a SQL query that joins two tables and contains two
predicates:
SELECT items.* FROMinvoice
JOINitems ONinvoice.id = items.invoiceid
WHEREamount > 1000 ORtaxFree = true
This SQL query returns all items of invoices that either have
amount greater than 1000 or that are tax free. To test this query
rigorously, the tester may want to follow some coverage criteria,
such as to exercise all “branches” that can be executed in this SQL
query.Thus,thetesterneedstotarget1)theJOINrelation,2)the
left predicate ( amount > 1000 ) to be evaluated to true, 3) the
right predicate ( taxFree = true ) to be evaluated to true. For
that to happen, the two tables should contain the right data thatsatisfies each of these targets. While in this illustrative example,generating test data can be done by a human, in more complex
cases,e.g., queries with multiple JOINs, predicates, and subqueries,
the number of targets grows, and the generation of data that test all
the branches of a SQL query becomes a difficult and time-consuming
task.
Researchers have proposed approaches to automatically gener-
atetestdatatosupportdeveloperstestingSQLqueries[ 4,11,21,34].
These approaches transform the test data generation problem into
a constraint satisfaction problem [ 36]. Subsequently, they use con-
straintsolvers,suchasAlloy[ 19]andChoco[ 20]togeneratetest
datasolvingtheconstraints.However,suchapproachessufferfrom
twoimportantproblemsthatmaypreventthemfrombeingused
inlargesoftwaresystems:First,duetolimitationsoftheexisting
constraintsolvertools,theseapproachescommonlydonotsupportstringsandanykindofstringmanipulation.Secondly,mappingtheentireSQLlanguagetoaconstraintsatisfactionproblemisahighly
complextask.Asaconsequence,theseapproachescommonlydo
not support JOIN expressions, subqueries, and specific database
functions, such as date/time functions.
At thesame time, SQLqueries often contain JOINs,subqueries,
strings, and database-specific functions. For example, 30% of the
queriesinSuiteCRM,alargeopensourcewebapplicationthatman-
agescustomerrelationships,containatleastasingleJOIN,and28%
of the queries in Alura, a closed-source e-learning web application,
containatleastonesubquery.Consequently,theaforementioned
limitationsofexistingsolutionsclearlyreducetheirapplicability
to real software systems.
12202018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Castelein et al.
Toovercometheidentifiedlimitations,wemodeltheproblemof
test data generation for SQL queries as a search-based problem.W e
optedforsearch-basedtechniquessincetheyhavebeensuccessfully
appliedinvarioussoftwaretestingscenarios(e.g.,white-boxunit
testing [24] and regression testing [ 38]), handling complex data
structures(e.g.,Javaobjects[ 13]),andforstringsearchproblems[ 1].
Given a SQL query, its respective database schema and a collec-
tionofcoveragerequirements,weimplementandevaluatethree
differentsearchapproaches,namelyrandomsearch,biasedrandom
search, and genetic algorithms, to populate tables with test data
meetingagiventestingcriterion.Therandomsearchexploresaset
of randomly generated data; the biased random search improves
the pure random search by seeding constants that can be extracted
from the SQL query under test. Finally, the genetic algorithm (GA)
is guided by a fitness function based on data collected from the
physical query plan generated by a fully-functioning instrumented
database engine.
Weprovideanimplementationofthethreeapproachesinatool,
namedEvoSQL.Toevaluatethethreeapproaches,weexecutethem
on 2,135 queries extracted from 4 software systems, one of them
being from an industry partner. Our results show that the GA isable to completely cover 2,106 queries (98.6%) of our dataset. On
average, the GA takes 2 seconds to cover queries up to 10 different
coverage targets,and 15 seconds to cover queries upto 20 different
coverage targets. Interestingly, we observe that the GA does not
get stuck in JOINs, subqueries, or string manipulation, and thus,
shows advantages over the two other search algorithms.
Our study leads to the following four contributions:
•AformulationofthetestdatagenerationproblemforSQL
queriesasasearchproblemtogetherwiththedefinitionof
three different search algorithms tailored to generate test
data for SQL queries (Section 3).
•AnopensourceJavaimplementationoftheapproach,namely
EvoSQL (Section 4).
•Anempiricalstudyontheeffectiveness,performance,and
difficultiesthatthethreeapproachesfaceon2,135queries
extracted from four software systems (Section 5), demon-
strating that the genetic algorithm reaches full coverage for
almostallcasesin2-15seconds,makingtheapproachusable
in a practical setting.
•Areplicationpackagecontainingthequeriesandschemas
usedinourevaluationthatcanhelpresearchersinreproduc-
ing and improving our results [7].
2 SQL TEST ADEQUACY
To enable the generation of test data for SQL queries, we must
first select a test adequacy criterion. SQL queries contain different
syntax and semantics that can be exercised, e.g., joining, grouping,
and aggregation. Consider the following SQL query:
SELECT *
FROMProduct
WHERECategory = 'Toy'
Itcontainsatleasttwodifferentscenariosthatcouldbetested:
1) when a row contains Category = ’Toy’ , and 2) when a row
containsCategory != ’Toy’.Tuya et al. [ 37] propose SQLFpc, a full predicate coverage cri-
terion for SQL queries which takes into account logical operators,
joins,grouping,aggregations,subqueries,caseexpressionsandnull
values.GivenaSQLquery,SQLFpcproducescoveragetargetsin
SQL formats. Such SQL targets are satisfied when the database
returns at least a single row after being populated with test data
and then executed.
Asanexample,SQLFpcwouldgeneratetwocoveragetargetsfor
the query above:
(1)SELECT * FROM Product WHERE (Category = ’Toy’) ,
which is, in this case, the same as the SQL under test, and
(2)SELECT * FROM Product WHERE NOT(Category = ’Toy’) ,
which represents the negative counterpart.
Adatabasethatcontainstworows(Row1={’Toy’},Row2={’Car’})
would achieve 100% of coverage for the SQL under test, as row 1
satisfies target 1, and row 2 satisfies target 2.
Inthispaper,weadoptthecoveragecriterionbyTuyaetal.[ 37].
Nevertheless, we devise our approach in a way that other coveragecriteriacanbeused.Ourapproachonlyrequiresacoveragecriterionthat1)producesasetofcoveragetargetsinSQLformat,and2)each
targetisconsideredsatisfiedwhenadatabasereturnsnon-empty
results after executing it against the generated data.
3 APPROACH
Given an SQL query under test, our goal is to generate test data
thatsatisfiesacoveragecriterion.Thetestdatagenerationproblem
can be formulated as follows:
Definition 3.1(Search Problem). LetR={r1,...,rm}bethe set
of coverage targets for a query Qaccording to a coverage criterion.
Find test data Sthat satisfies all coverage targets in R.
Toallowtheapplicationofsearch-basedtechniques,we(i)design
anencodingschemeforrepresentingsolutionsoftheproblemin
Definition3.1;(ii)defineafitnessfunctionmeasuringtheoptimality
of each solution; (iii) select three search algorithms to find optimal
solutionsandthussolvingourproblem.Theseaspectsaredescribed
in detail in the next subsections.
3.1 Solution Representation
Given a coverage target for a SQL query, the search space is repre-
sented by all possible combinations of database tables whose rows
satisfy the target. A database table has a specified (and previously
definedinthesupplieddatabaseschema)numberofcolumnsand
canstoreanynumberofrows.Thecolumnsdefinethedatatype
(e.g.,stringorinteger)thateachcellinarowcanstore.Asanexam-
ple, a table Product has three columns: namewhich stores strings,
pricewhich stores doubles, and sizewhich stores integers. Rows
in this table always contain three elements: the first cell stores a
string, the second stores a double, and the third stores an integer.
Our encoding scheme represents the set of database tables that
areusedintheSQLquery,whereeachofthemcontainsalistofnon-emptyrows.Morespecifically,acandidatesolutionisasetoftables
T={T1,...,Tn},whereeachtable Tiiscomposedofrows, i.e.,Ti=/braceleftbigR1,...,Rk/bracerightbig.Eachrowcontainscells, i.e.,Rj={V1,...,Vc},where
cis the number of columns in Ti.
1221
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Search-Based Test Data Generation for SQL Queries ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
6(/(&7
)520&DUV
:+(5(PRGHO 
)HUUDUL
6WHS
(a) Single step6(/(&7
)520&DUV
-2,17LUHV
21&DUVWLUHBLG 7LUHVLG
:+(5(PRGHO 
)HUUDUL
6WHS& W LL G7 L L G6WHS
(b) Two steps
Figure1:TwoexamplesofSQLquerieswiththecorrespond-
ing steps in a physical query plan.
Thetablesandcolumnsofacandidatesolutionaredefinedbythe
tables appearing in the target to solve. For example, let us consider
thetarget SELECT * FROM Product WHERE (price>100.00 AND
size>10.00) ; a candidate solution for this target contains a list of
rows for the previously defined table Product. As an example, a
candidate solution for the query is:
Product=/braceleftbigR1=/angbracketleftbig/primeTV/prime,500.0,60/angbracketrightbig/bracerightbig(1)
3.2 Fitness Function
Todeterminehowcloseacandidatesolution Tisfromcoveringa
querytargetweneedtodefineaproperfitnessfunction.Usually,the
fitnessfunctionisadistancemeasure f:T→[0,+∞ )thattakesas
inputacandidatesolution Tandreturnszeroifandonlyif Tsatisfies
the given target. In our case, the fitness function is computed by
analyzingtheexecutionofaSQLqueryinadatabaseengineandits
physical query plan, the plan that the database internally devises toprocessthequery.Therefore,beforedescribingourfitnessfunction,
we need to introduce and describe the physical query plan.
3.2.1 Physical query plan. For any SQL query to be executed,
databases first convert the query into a physical query plan [17],
which indicates the operations required to process the query as
wellastheorderbywhichtheyneedtobeperformed.Theresulting
physicalqueryplancanbeviewedasanorderedlistofrelational
algebraoperationsineachofitsnodes.Asanexample,nodescould
contain a JOINbetween two tables, or a predicate expressed in
aWHEREstatement. In the following, we refer to these relational
algebra operations in the physical query plan asstepsto satisfy.
Figure1depictstwoexamplesofSQLqueriesandhighlightsthe
states in the corresponding physical query plan. Figure 1a contains
aSQLquerywithasinglestep,whichcontainsasingleselectionop-eration(
model = ’Ferrari’ ).TheSQLqueryinFigure1bcontains
two steps: the physical query plan first commands the database to
work on the JOIN(step one), and then, after that step is done, to
workontheselection(steptwo).Inthesecondcase,ifthe JOINdoes
notproduceanoutput(i.e.,thepredicate Cars.tire_id=Tires.id
isnotTRUEforanyrowinboth CarsandTirestable),thedatabase
realizes that the query will return an empty result, and stops its
execution before proceeding to step two.
Theorderofthestepsinthe physicalqueryplan aswellasthe
orderofoperationsineachstepisautomaticallycomputedbythe
databaseenginethatperformsdifferentcostcalculationstocome
up with the best order to execute them [ 17]. The physical query
plan also takes into account the priorities that an operation may
have over another, e.g., a subquery may need to be executed beforeaJOIN. Nevertheless, to cover a given query target r, a solution
hastogothroughallthestepsinthequeryplan,whichwillonly
happen if all predicates are evaluated to TRUEfor at least once.
3.2.2 Fitness function definition. Executingacandidatesolution
against a given coverage target corresponds to executing all the
stepsintheplan.Ifasolution Tfullysatisfiesallsteps, i.e.,predicates
inallsteps,thenitsatisfiesthecoveragetargetundertest.Given
our coverage criterion, satisfying a coverage target means that the
query under test, when executed in a database that contains the
solutionT, yields a non-empty result. If Tdoes not cover the given
target, it implies that some steps are not satisfied and the database
engine terminates the execution before completing the last stepin the query plan. In this case, we can measure the distance of
T
to cover the target by (1) counting the number of yet unsatisfied
steps (step level ), and (2) measuring how far Tis to satisfy the step
where the database engine stopped its execution prematurely (step
distance).
Therefore, given a candidate solution Tand a coverage target r,
we design our fitness function as follows:
d(T,r)=step_level (T,r)+step_distance (T,L)(2)
In the equation above, step_level (T,r)measures the number
of steps in the physical query plan not executed by the database
enginewhenrunning Tagainstthetarget r.Thesecondfunction
clausestep_distance (T,L)measures the distance of Tin satisfying
thefirststep Lofthephysicalqueryplan thatisnotfullysatisfied
byT,i.e., thestep distance is computed for the step Lwhere the
database engine stopped its execution.
A stepLis satisfied by Tif all relational algebra operations in
Lare satisfied. Therefore, let OL=/braceleftbigop1,...,oph/bracerightbigbe the set of
operations in a given step L, the corresponding step distance for a
solutionTis defined as:
step_distance (T,L)=ϕ⎡⎢⎢⎢⎢⎢⎣h/summationdisplay
i=1ϕ[dist (T,opi)]⎤⎥⎥⎥⎥⎥⎦(3)
whereopiisthei-thoperationin L,dist (T,opi)denotesthedistance
ofTtosatisfytherelationaloperation opi,andϕisthenormalizing
function ϕ(x)=x/(x+1)widely used in search-based software
testing[24].AccordingtoEquation3,thestepdistanceresultsin
values in the range [0; +1)and it is obtained by summing up the
distances dist (T,opi)foralloperationsinthestepunderanalysis.
Alloperationscontributeinequalmannerto step_distance (T,L)as
theyarenormalizedbeforebeingaggregatedwiththesumoperator.
The actual distance dist (T,opi)for each operation opi∈OLis
defined depending on the type of the values being involved. In
particular, we identify the following operations and their corre-
sponding operation distances:
(i)Comparison operators. The existing comparison operations
in SQL (i.e., =,<>,>,>=,<, and<=) can involve either numbers,
booleans,strings,dates,orthespecialvalue NULL.Fornumbersand
boolean values, the distance dist (T,opi)corresponds to the stan-
dardbranchdistance definedbyKorel[ 22].Forexample,thedistance
for the equality price=10 isabs (price−10)[22]. For strings, we
use theenhanced edit distance defined by Alshraideh et al. [ 1] that
combines the standard edit distance with the character distance.
1222
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Castelein et al.
We selected this distance as it performs best when dealing with
stringvaluesintestdatageneration[ 1].Fordates,thedistanceis
computedasthesumofthedifferencesforeachnumericalcalendar
part: year, month, day, hour, minute, second, and millisecond.
(ii)Logicaloperators. Forlogicaloperations(e.g., AND)weusedthe
traditional branchdistance rules[ 22]that aggregate thedistances
of the logical expressions (predicates) involved in the operation.
For example, the branch distance for the operation price=100.00
AND size=10.00 isd=[abs (price−100.00)+abs (size−10.0)].
(iii)SQL operators. There arefive operators thatare specific for
SQL,whichare: BETWEEN,IS (NOT) NULL ,IN,LIKEandEXISTS.For
theseoperators,wedefinethecorrespondingoperationdistances
as described in the following.
TheBETWEEN operatorverifieswhetheragivenvalue viswithin
thetwogivenbounds,i.e.,if lb<=v<=ubwithlbandubbeingthe
lower and the upper bound respectively. In SQL, this is equivalent
tolb<=vANDv<=uband, therefore, we use the corresponding
branch distance rule for the ANDoperator.
TheIS NULL operator returns TRUEwhen the value vunder
inspectionis NULL,andFALSEotherwise.The IS NOT NULL operator
isequivalentto NOT(IS NULL).Therefore,itisequivalenttothe
boolean operator v/nequalNULLand it is treated as the not equal
operator when computing the operation distance.
TheINoperatorverifieswhetheragivenvalue lv(lefttermofthe
operation)iscontainedinthelistontherightpartoftheoperation.
Suchalistcanbeaconstantlistwritteninthequeryortheresultofasubquery.Thisoperatorreturns
TRUEifatleastoneofthevaluesin
thelistisequalto lv.Therefore,theoperationdistancecorresponds
totheminimumbranchdistancebetweeneachelement einthelist
andlv, i.e.,dist (T,opi)=min e∈listabs (lv−e).
TheLIKEoperatorperformspatternmatchingonastring.There-
fore,itsoperationdistancecorrespondstothebranchdistanceforpattern matching defined by Alshraideh et al. [1].
TheEXISTSoperator takes a subquery as a parameter. If the
subquery (once executed) returns a value, then the operation is
satisfiedanditsoperationdistanceiszero.Otherwise,theoperation
distance corresponds to the operation distance of the subquery.
TheJOINoperator merges two tables based on a list of join
conditions,e.g.,thecondition Cars.tire_id=Tires.id fortheSQL
query in Figure 1b. Join conditions are equality operators between
two columns, or between pairs of columns, to satisfy. Therefore,theoperationdistanceforajoinoperationiscomputedbasedon
thebranchdistanceforequalityoperators: dist (T,opi)isequalto
the sum of the branch distances applied to all join conditions inopi.
3.3 Search Algorithms
In this paper, we consider three different search algorithms: (1)
GeneticAlgorithms(GAs);(2)randomsearch;and(3)biasedrandom
search. The details of these algorithms are discussed below.
3.3.1 Genetic Algorithms. GAs are stochastic search algorithms
inspired by the Darwinian natural selection. These algorithmse v olv eapoolof
Nsolutions(calledpopulation)usingthefitness
function to measure the quality of each solution. Initially, the pop-
ulation consists of randomly generated solutions, that correspond
(inourcase)tosetofsolutionsasdescribedinSection3.1andfilledwithrandomlygeneratedvalues(e.g.,randomintegers).Thepop-
ulationisevolvedviaselectionandreproductionthroughvarious
iterations, called generations. In each generation, individuals in
the population are evaluated as follows: each individual is inserted
intoanin-memorydatabaseengineandexecutedagainstthetarget
under analysis. The in-memory database is instrumented to give
thecompleteexecutiontraceofthequery,step-by-step.Thisexecu-
tion trace is therefore used to computed the corresponding fitness
functionasdescribedinSection3.2.Afterevaluatingallindividu-
alsinthepopulations,the fittestindividuals(parents)areselected
using the tournament selection [18,33] and then recombined using
crossoverandmutation.Thesetwogeneticoperatorsgeneratenew
individuals(offsprings) thatinherit someproperties(tables inour
case)fromtheirparents.Then,parentsandoffspringscompetewith
each other and the Nfittest individuals (according to the fitness
functionforthetargetunderanalysis)areselectedtoformthepop-
ulation of the next generation (elitism ). The selection-reproduction
cycle ends if one individual in the current population achieves azero fitness value meaning that it covers the given coverage tar-
get; otherwise the search is terminated when the maximum search
budget allocated for the current target is reached.
Thedetailsofthegeneticoperatorsimplementedforourproblem
are discussed in the next paragraphs.
Crossover . As explained in Section 3.1, in our case an indi-
vidual (or solutions) is a set of tables T={T1,...,Tn}. There-
fore, the crossover operator has to generate two offspring by shuf-
fling the tables in the parent solutions. To this aim, we used theuniform crossover [
5]: given two parents T={T1,...,Tn}and
X={X1,...,Xn}, the uniform crossover uses a random binary
vector (crossover mask) to decide which offspring inherits each
table inTandX. The mask b={b1,...,bn}has the same length
as the parents (i.e., the number nof tables), one binary element for
each table in the solutions. If the binary element biis one, then the
first offspring inherits the table Ti(from the first parent) while the
secondoffspringinherits Xi(fromthesecondparent);otherwise,
thefirstoffspringinherits Xiwhilethesecondinherits Ti.Notice
that parents and offsprings always have the same number of tables.
Mutation .Whenanindividual T={T1,...,Tn}ismutated,there
is a 1/nprobability for each table Ti∈Tto be mutated, so on
average,onetableismutatedin T.Threetypesofmutationscanbe
appliedtoeach Ti∈T,whichareinorderoftheirapplication delete,
change, and insert. These operations have the same probability pm
ofbeingappliedandarenotmutuallyexclusive, i.e.,morethanone
mutationcanbeappliedtothesametable Ti.LetTi=/braceleftbigR1,...,Rk/bracerightbig
beonetablewith krowsinagivensolution Ttomutate.Thedelete
mutation consists in deleting one random row from Ti.
The change mutation modifies each row rin a table Tiwith a
probability pc. A row is modified by randomly changing one of its
values. Let rjbe the row to mutate; each cell in rjis modified with
a1/cprobability,where cisthenumberofcolumnsin Ti.Elements
are mutated depending on their types: floating-point numbers are
modified using the polynomial mutation [10], which is standard
forrealnumbers;integersaremutatedbyusinga deltamutation,
i.e., by adding or removing a deltavalue randomly generated from
the interval [−10;10]; a date is mutated by applying the same delta
mutation to all its calendar parts (e.g., days); strings are mutated
byadding,removingorreplacingcharacters[ 1];finally,booleans
1223
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Search-Based Test Data Generation for SQL Queries ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
are mutated by flipping their values (e.g., from true to false). If the
column to mutate is nullable , one of its value is set to NULLwith
a probability pnull.
Thelastmutationoperatoraddsonerowtoagiventable Ti.It
eitherduplicatesoneexistingrowin Toritaddsanewlyrandomly
generated row (i.e., a row containing randomly generated values).
Inagivenqueryundertest,notallcolumnsofthetableareexer-
cised.Therefore,welimititssearchspacebyignoringcolumnsthat
arenotusedbyanypredicate,andinstead,consideringtheother
columns to which we refer to as mutable columns. The approach
usesanaivetechniquetoidentifythemutablecolumns:ifacolumn
is used anywhere in the query’s important clauses ( FROM,WHERE,
GROUP BY, HAVING), it is added to the list of mutable columns.
Seedingstrategies .Seedingisthetechniqueofinsertingvalues
fromaseedingpoolintothepopulationwithsomeprobability.The
valuesintheseedingpoolareextractedusingtheknowledgetaken
from the query under test. Our GA uses a seeding pool containing
allconstants(e.g.,stringsandintegers)appearinginthequery.This
is because a query may contain comparisons with constant values.
Each constant in the query is extracted and added to the column
seedingpool,whichwillbeusedtobeseededintocolumnsofthe
generated individuals, when mutating a row.
We use a further seeding strategy specific for join operations. A
typical SQL predicate for joining two tables together is an equal-
ity between two columns. Each predicate of the form column1=
column2adds a logical link between the two columns. Therefore,
theseedingisappliedbycopyingsomevaluesfrom column1into
columncolumn2, and vice versa.
PostProcessing. Readabilityofthegenerateddataisimportant
when it comes to comprehending the SQLquery and writing test
casesforit.Toimprovethereadabilityofthegenerateddata,whenasolutionisfound,weapplyanaive rowminimization technique:for
each table Tiin the generated solution T, we remove all rows that
do not contribute to covering the coverage target under analysis.
3.3.2 Random Search. AsforGAs,randomsearchisexecuted
against each target to cover independently. It iteratively generates
randomsolutions,andthisprocessisrepeateduntilthecoverage
targetissatisfied(i.e.,thecoveragetarget,whenexecutedagainst
the generated data, yields a non-empty result) or the maximum
search budget allocated for the current target is reached.
Random search is a simple algorithm that does not evolve exist-
ing solutions. However, it is often used in the literature as baseline
the to test the complexity of the problem to solve and to assess
the need for more advanced algorithms (e.g., GAs). Moreover, ran-
domsearchhasbeenshowntooutperformothersearchalgorithms
when solving specific problems [3, 30].
3.3.3 Biased Random Search. In addition to the simple random
search algorithm, in this paper we used another variant of random
search that uses the same seeding strategy used in GAs. Therefore,
randomly generated solutions contain (with a given probability)
values seeded from the constants appearing in the query (i.e., from
thecolumnseeding )orobtainedbyapplyingtheseedingstrategy
specific for join operations.
3.3.4 Search budget allocation. All search algorithms described
above can optimize only one single coverage target at one time. Tosolve multiple coverage targets they have to be executed several
times, one run is executed independently for each coverage target.
Therefore, the total search budget SBgiven to test each query Qis
divided in equal manner among all coverage targets of the SQLFpc
coveragecriterion[ 37].Inotherwords,thelocalbudgetassigned
foreachcoveragetargetis SL=SB/m,wheremisthetotalnumber
oftargetsfor Q.Ifoneofthecoveragetargetsissatisfiedwithout
fully consuming the local budget SL, the saved search time is used
todynamicallyincrease thebudgetsfortheremaininguncovered
targets.
Thecoveragetargetsgeneratedforaquerydifferfromeachother
for only few spare operations and predicates (i.e., they are very
similar). Therefore, test data covering one target may be very close
to covering other similar targets (according to their fitness func-
tions). For this reason, some solutions generated when optimizing
previous coverage targets are used to seed the initial population in
the GA.
4 OUR IMPLEMENTATION: EVOSQL
We provide EvoSQL, a tool implementing the database engine in-
strumentation,thefitnessfunctionandthethreesearchalgorithms
described in the previous section. EvoSQL takes as input a query, a
database schema, and a time budget, and returns test data for each
SQLFpc coverage target. Our implementation is available open
source and can be found in our GitHub [ 6] as well as in our appen-
dix [7].
EvoSQL internally uses HSQLDB1, a Java relational database
engine that supports the latest SQL standards and is able to runin-memory. We modified the source code of the database engineto instrument the physical query plan generated during a query
execution, which is used to calculate our fitness function.
Toextractthecoveragetargets,ourtoolusesthewebservicethat
ismadeavailablebyTuya etal.[37].Thewebservicereceivesasan
input the SQL query and the database schema and returns a list of
coverage targets in SQL format.
WedisabledinternaloptimizationsofHSQLDBastheywould
reducetheamountofinformationwecouldcollect.Inparticular,
wedisabled(1) indexing,asitexcludesrowsthatdonotsatisfypred-
icates of indexed columns without individually evaluating them,
and(2)lazyAND/ORoptimizations, i.e.,short-circuitevaluation,
which makes the engine potentially not evaluate all predicates.
5 EMPIRICAL STUDY
Thegoalofthisstudyistoevaluatetheeffectivenessofthethreedif-
ferentsearchalgorithmswhengeneratingtestdataforSQLqueries.
More specifically, we investigate the following research questions:
•RQ1:Whatisthecoverageachievedbytheproposedsearch-
based algorithms?
•RQ2:Whatistheperformanceoftheproposedsearch-based
algorithms?
•RQ3:Whatcausesthedifferentapproachestonotachieve100%
coverage?
1HSQLDB - http://hsqldb.org/
1224
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Castelein et al.
ApplicationTotal
QueriesQueries w/o
bad syntaxUnique
QueriesFinal
Queries
Alura 554 494 258 249
EspoCRM 151 149 40 40
SuiteCRM 709 704 280 279
ERPNext 18,454 17,761 1,631 1,567
Total 19,868 19,108 2,209 2,135
Table 1: Queries collected per application
1/ 3/ 5/ 7/ 9/ 11/ 16/
P r o p e r t y 0 2468 1 0 1 5 2 0 2 1 +
Predicates 58 1389 495 100 33 11 27 16 6
Joins 1890 189 32 3 17 2 - 1 1
Subqueries 2052 78 3 1 - - 1 - -
Functions 1796 291 12 16 2 6 12 - -
Columns 60 1369 457 127 43 26 20 13 20
Targets - 656 382 408 346 114 107 51 71
Table2:Numberofqueriesaccordingtotheirdifferentprop-
erties
5.1 Context of the Study
We evaluate the different search algorithms on 2 ,135 queries taken
from four software systems:
•Alura2is a closed source Java e-learning platform that uses
Hibernate as layer between application and database. Hiber-
nategeneratesSQLqueriesbasedonincomingdatarequests.
•EspoCRM3is an open source web application to manage
customerrelationships(CRM).ItusesaRESTAPIbackend
writteninPHPwhichcommunicateswithaMySQLdatabase.
•SuiteCRM4isanotheropensourceCRM.Itisaforkofthe
SugarCRM CommunityEdition, andis written inPHP. The
databaseitusescanbeeitherMySQL,MariaDBorSQLServer.
For our evaluation, we used MySQL.
•ERPNext5isanend-to-endbusinesssolutionthatmanages
business information (ERP stands for Enterprise Resource
Planning).ItisbuiltontopofthePython&JavaScriptframe-
workFrappéand uses MariaDB.
Wechosethesesystemsbecausetheyaredatabase-centric, i.e.,they
make intensive use of databases and, thus, contain a large number
of SQL queries. In addition, these systems are written in three dif-
ferentprogramminglanguages(Java,PHP,andPython),givingusa
morediversesampleofprojectsandqueries,asthewayadeveloper
writesaSQLquerymightbeinfluencedbytheoverallecosystem
ofthechosenlanguage(e.g.,JavadeveloperstypicallyusedHiber-
nate to generate SQL queries). Apart from the industrial system,
all others can be found on GitHub, enabling other researchers to
reproduce our results.
2Alura- http://www.alura.com.br/.
3EspoCRM - https://www.espocrm.com.
4SuiteCRM - https://suitecrm.com.
5ERPNext - https://erpnext.com.Thequerieswerecollectedbyexecutingthetestsuitesofeach
system and mining the generated database logs. Table 1 shows the
total number of queries collected per system as well as the number
of queries selected in our empirical evaluation. For the query selec-
tion, we analyzed all extracted queries to filter out non-executable
andduplicatedqueries.Inparticular,wefilteredout HSQLDBand
SQLFpc non-compliant SQL , i.e., queries containing SQL con-
structsthatarenotsupportedbyeitherHSQLDBorSQLFpc.We
also removed duplicatedqueries , i.e., similar queries differing by
some constant values. For example, the queries SELECT * FROM t
WHEREa=1 andSELECT * FROM t WHEREa=2 are similar, as
theironlydifferenceisaconstant(1and2).Asthereisnodifference
in solving these queries, we exclude duplicated queries. Finally, we
ignoredqueries with no predicates or other constraints ,i.e.,,
queries with no conditional branches to be exercised, and thus,
without coverage targets.
In Table 2, we report the characteristics of the queries in our
dataset. As expected, the number of coverage targets of a query is
strongly correlated with the number of its properties (Spearman
correlation=0 .95, p-value < 0 .01). As queries with fewer properties
couldbeeasiertosolvethan(complex)querieswithmoreproperties,
we control our results by the number of coverage targets.
5.2 Configuration of the Search Parameters
Theperformanceofsearchalgorithmsisinfluencedbyalargenum-
ber of parameters. To identify the best configuration of parameters
fortheappliedGeneticAlgorithm(GA),weexecutedthealgorithm
inatrainingsetthatcontained100queries.Wecarefullydevised
thistrainingsettocontainallSQLconstructsthataresupportedby
HSQLDB, such as JOINs,WHEREs, subqueries, and string functions.
Thetrainingsetisavailableinouronlineappendix[ 7].Toevaluate
various possible configurations, we opted for a set of values and
thresholdsthatarecommonlyusedinapplyingevolutionarysearch
algorithms on similar software engineering problems [ 14,32]. The
exercised configurations for different probabilities (in a total of 108
different combinations) are as follows: (1) NULLmutation ( pnull)=
{0.01,0.10,0.50},(2)inserting,deletingorduplicatingarow( pm)=
{1/3,1/6},(3)rowchangemutation( pc)={1/n,1},wherenisthe
numberofrowsinthemutatedtable,(4)seeding= {0.01,0.10,0.50},
and (5) crossover = {0.0,0.6,0.75}.
Foreachcombinationofprobabilities,weexecutedtheapproach
10 times and averaged the execution time of the complete training
set. At the end, we selected the configuration with the smallest
executiontime.Thebestconfigurationweidentifiedisasthefol-
lowing:
•Population size = 50.
•Tournament size = 4.
•NULLmutation ( pnull)=0.1.
•Inserting, deleting, and duplicating ( pm)=1/3.
•Row Change Mutation ( pc)=1 .
•Seeding = 0 .5
•Crossover = 0 .75.
•Cloning from previous target population = 0 .6.
The time budget for the search process is 30 minutes. Finally, to
comparethebiasedrandomsearchwiththeGA,wesetthesame
probability for seeding individuals which is 0 .5.
1225
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Search-Based Test Data Generation for SQL Queries ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
5.3 Experimental Procedure
To answer RQ 1, we executed the three approaches on the entire
dataset.Giventherandomizednaturalofthealgorithms,weexecute
eachofthem10times.Weperform10runs,theminimumsuggestedinliterature[
2]andacommonchoiceforexpensiveexperiments[ 15,
28],whichisourcase.Foreachquery,weaveragetheircoverage
across the 10 executions. We consider a query to be successfully
coveredif its average coverage is 100%, meaning that all its targets
were covered in all 10 executions. If any target was not covered in
anyoftheexecutions,thatqueryisconsideredtobe notfullycovered.
Such definition gives us the “worst-case view” on the results.
Throughout the research, we have experienced that some of the
coverage targets that are produced by SQLFpc are infeasible. In
mostofthesecases,thisisduetosomecombinationofpredicates
that cannotbe satisfied, e.g.,A > 10 and A < 10. Suchinfeasible
targetsmayaffecttheanalysisofourresults,asaquerywouldthen
be marked as not succesfully covered. We manually analyzed each
targetproducedbySQLFpc,andremovedthemfromouranalysis.
Intheend,weremoved127outofthe12 ,991totalcoveragetargets.
ToanswerRQ 2,weanalyzetheaverageexecutiontimeperquery
in each approach. Once more, we control the queries by their cov-
eragetargetstoanalyzewhetherthishasaneffectontheexecution
time. We provide descriptive statistics of each approach.
Finally,toanswerRQ 3,wetrainandunderstandaJ48decision
tree [31] that classifies whether a coverage target is likely to be
coveredornotbasedontheresultsinRQ 1,usingR’s RWekapackage.
We use the following SQL constructs as features to the model:
numberoftablesused,numberofbasepredicates(doesnotinclude
AND,OR, andNOToperators), number of inner joins, number of
left joins, number of right joins, number of subqueries, numberof aggregate functions (
MIN, MAX, SUM, AVG, COUNT ), number
of non-aggregate functions (e.g., DATENOW, IFNULL ), number of
columns,numberof WHEREclauses,numberof GROUP BY clauses,
number of HAVINGclauses, number of string equality predicates,
number of date equality predicates, number of EXIST predicates,
numberofLIKE predicates,numberofIFNULL functions,andthe
SQL query’s total number of coverage targets.
Asclasses(failedandsuccessfulcoveragetargets)arenotevenly
balanced (there are more failing cases in the random search andmore successful cases in the biased and GA search), we applySMOTE (Synthetic Minority Over-sampling TEchnique) to gen-erate extra data points for the smaller class [
8]. This way, both
classes have equal size, which prevents the classifier from generat-
ingabiasedmodel.Finally,toenablethemodeltobeunderstoodby
ahuman,welimitthenumberofleavesinthetreeby10.Wealso
reporttheaccuracyofthegeneratedmodels, i.e.,,thepercentageof
instancesinourdatasetthatarecorrectlyclassifiedbythemodel.
The accuracy gives us an estimate of how much we can trust on
the model.
Replicationpackage. Weprovideanopensourcereplicationpack-
age[7]thatcontains(1)ourimplementationsofthethreesearch-
based algorithms, (2) the R scripts used to generate the analysis,
and (3) the queries and schemas from all systems but the closed-
source application Alura, which enables researchers to replicate
and further compare with other tools.6 RESULTS
6.1 RQ 1: What is the coverage achieved by the
proposed search-based algorithms?
InFigure2,weshowaboxplotofaveragecoveragethateachsearch-
based approach achieved. In addition, in Table 3 we present the
number of queries each approach was able to completely cover,
controlledbythenumberofcoveragetargets.Fromthisdata,we
observe that:
The random search proved to be highly inefficient. Among
the2,135queries,therandombaselinewasabletocompletelycover
only 140 (6.5%) of them. It also achieved a partial median coverage
of only 33.75% among the remaining queries, and completely failed
(i.e., achieved 0% coverage) in 605 queries (28.33%). Its ability to
findasolutionquicklydecreasesasthenumberofcoveragetargets
increases.Inpractice,randomsearchwasonlyabletocompletely
coverquerieswithlessthan8coveragetargets.Still,thenumber
oftimesitfailsisrelativelyhigheveninquerieswithlesstargets,
e.g., the random search was not able to solve any of the 21 queries
with1or2coveragetargetsinSuiteCRM,andonly34outof263
queries with 3 or 4 coverage targets in ERPNext.
Thebiasedsearchpresentsgoodefficacyinquerieswithless
than 10 coverage targets. The biased search was able to com-
pletely cover 1,923 (90%) queries, had a partial median coverage of
79.76%,andcompletelyfailedonlyin11queries(0.05%).Forqueries
withlessthan10coveragetargets,thebiasedsearchcouldonlynot
completelycover66outof1,906queries(3.46%).However,itseffec-tivenessalsodecreasesasthenumberofcoveragetargetsincreases.
In particular, it rapidly decreases after 10 targets: it solved only 83
out of the 229 queries (36.24%) with more than 10 targets, and only
5 out of the 71 queries with more than 20 coverage targets.
The GA is the most effective approach. It completely covered
2,106 queries (98.64%), had a partial median coverage of 86.66%
amongtheremainingones,anddidnotcompletelyfailinanyofthequeriesinthedataset.TheGAachievedhighefficacyinquerieswithlessthan10coveragetargets,asitdidnotcompletelycoveredonly3outof1,906queries.Ho wever,westill observethatitsperformance
decreases as the number of coverage targets increases. Still, thedecrease is much smaller than in the previous approaches. Even
in queries with more than 20 coverage targets, the GA was able to
solve 53 out of the 71 existing queries (74.64%).
6.2 RQ 2: What is the performance of the
proposed search-based algorithms?
In Table 4, we show descriptive statistics of the average runtime of
each approach. In addition, in Figure 3, we show the average query
coverage given a time budget for the biased and GA approaches,respectively. We do not present data for the random search as it
failsinmostcases,andthusisnotacompetitor;thefulldatacan
be found in our online appendix [7]. We observe that:
Thebiasedsearchisthefastestforqueriesupto5-6coverage
targets.BoththeGAandthebiasedsearchareabletosolvequeries
upto6coveragetargetsinlessthanonesecond.However,thebiased
search is even faster than the GA for such cases, e.g., for queries
1226
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Castelein et al.
# of targets1 - 2 (656 queries) 3 - 4 (382 queries) 5 - 6 (408 queries) 7 - 8 (346 queries)
Random Biased GA Random Biased GA Random Biased GA Random Biased GA
Alura 2/14 13/14 14/14 11/51 50/51 51/51 0/33 33/33 33/33 0/38 35/38 37/38
SuiteCRM 0/21 21/21 21/21 7/66 66/66 66/66 1/117 115/117 117/117 1/45 45/45 45/45
ERPNext 61/621 621/621 621/621 34/263 262/263 263/263 21/240 230/240 240/240 2/260 242/260 259/260
EspoCRM 0/0 0/0 0/0 0/2 2/2 2/2 0/18 13/18 18/18 0/3 3/3 3/3
# of targets9 - 10 (114 queries) 11 - 15 (107 queries) 16 - 20 (51 queries) 21+(71 queries)
Random Biased GA Random Biased GA Random Biased GA Random Biased GA
Alura 0/24 18/24 24/24 0/46 25/46 46/46 0/25 8/25 24/25 0/18 3/18 15/18
SuiteCRM 0/19 17/19 18/19 0/7 5/7 6/7 0/2 0/2 2/2 0/2 0/2 2/2
ERPNext 0/70 53/70 70/70 0/53 33/53 50/53 0/21 4/21 18/21 0/39 2/39 29/39
EspoCRM 0/1 1/1 1/1 0/1 1/1 1/1 0/3 2/3 3/3 0/12 0/12 7/12
Table 3: Number of completely successfully covered SQL queries vs their total number.
● ● ● ● ● ● ● ● ● ●
●
●●● ● ●
●●
●●
●
●●
●●
●●
●●●
●
●●●
●
●
●●●●
●●
●
●
●
●●
●●●●
●●
●●
●●●
●●
●●
●
●●●
●●
●●
●
●●●●
●●● ● ● ● ● ● ● ● ●
●
●●●
●●
●●●●
●
●
●●●
●
●●●
●
●
●●
●●●●
●●
●●
●
●●
●●
●●
●●
●
●●●●●
●
●●
●●●●
●●
●●
●●●
●●
●
●●
●●
●●
●
●●●
●
●●●●
●
●●
●●●
●●
●
●●
●●●
●
●●●
●
●●●●
●
●●●
●●
●●
●●
●●
●
●●●
●
●●●
●●●
● ●●
●
●●
●● ●
●●
●●
●
●●
●
●●
●
●●●
●●
●
●●●●
●●
●●
●
●●
●●●●
●●
●●●
●
●
●●●
●
●
●●●●
●
●●
●
●
●
●●
●●●●
●●
●
●●
●●● ●●
●●
●●
●
●●
●●
●●
●●
●
●●●●
●
●●●
●
●
●●
●●●●
●●
●●
●
●●
●●
●●
●●
●
●●●●●
●
●●
●●●●
●●
●●
●●●
●●
●
●●
●●
●●
●
●●●
●
●●●●
●
●●
●●●
●●
●
●●
●●●
●
●●●
●
●●●●
●
●●●
●●
●●
●●
●●
●
●●●
●
●●●
●●●
● ●●
●
●●
●●
●●
●●●
●
●●
●●
●●●
●●
●●
●●
●●●
●●
●●●●
●●●
●●●
●●
●●
●
●●
●
●●
●
●●●
●●
●
●●●●●
●
0.000.250.500.751.00
alura suitecrm erpnext espocrm all
systemcoveragealgorithm
random
biasedga
Figure 2: The average coverage for a SQL query achieved by
eachsearch-basedapproach,after10executions.Figurebet-ter visualized in colors.
with 1-2 coverage targets, the biased search has a median runtime
of 0.05 seconds, compared to 0.22 of the GA (a difference of 0.17s).
The GA becomes much faster than the biased search as the
numberofcoveragetargetsincrease. Assoonasthenumberof
coverage targets starts to increase, boththe biased search and the
GAbecomeslower.However,theruntimeofthebiasedsearchgrows
rapidly, while the GA’s runtime growth is less aggressive. As an
example, the median runtime for the biased search in queries with
9to10coveragetargetsis5.95secondswhileGApresentsamedian
of 1.48 seconds; for 11 to 15 coverage targets, the biased search
takes, on average, 74.04 seconds (a 12x increase when compared to
9-10 targets) while the GA takes 3.65 seconds (an increase of 2.4x).
A time budget of one minute is enough for the GA to com-
pletelycoversimplequeriesandtocoveratleast70%ofcom-plexqueries.
Atimebudgetofoneminuteis,onaverage,sufficient
fortheGAtocompletelycoverquerieswith6coveragetargetsor
less; for the biased search, the same occurs for queries with 4 or
lesscoveragetargets.However,forcomplexqueries,thegiven300.70.80.91.0
01 0 2 0 3 0
Biased0.70.80.91.0
01 0 2 0 3 0
GA
1−2 3−4 5−6 7−8 9−10 11−15 16−20 21+
Figure 3: The average coverage of a SQL query (Y axis) in
a given time budget (X axis, in minutes) controlled by thenumber of coverage targets. Figure better visualized in col-ors.
1st quantile Median 3rd quantile
Biased GA Biased GA Biased GA
1-2 0.03 0.15 0.05 0.22 0.07 0.37
3-4 0.09 0.2 0.15 0.28 0.36 0.465-6 0.17 0.3 0.34 0.41 0.56 0.587-8 1.23 0.59 2.25 0.88 7.64 1.459-10 1.26 0.9 5.95 1.48 100.80 2.4911-15 7.47 2.21 74.04 3.65 844.10 8.8016-20 83.1 7.33 844.00 15.69 1539.00 41.7921+ 1027 81.36 1688.00 155.90 1800.00 581.90
Table 4: Descriptive statistics of the biased and GA ap-
proaches’ average runtime (in seconds).
minutesbudgetdoesnotseemenoughforthebiasedsearch.Inpar-
ticular, even after 30 minutes, queries with more than 20 coverage
targetsachieveonly51%mediancoverage(notevenappearingin
the Figure, due to our scale). On the other hand, the GA needs a
single minute to cover at least 70% of queries with more than 20
targets,and90%inqueriesrangingfrom16to20targets.Toachieve90%ofcoverageinquerieswithmorethan20targets,theGAneeds,
on average, 4 minutes.
1227
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Search-Based Test Data Generation for SQL Queries ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
6.3 RQ 3: What causes the different approaches
to not achieve 100% coverage?
Thedecisiontreemodelsgeneratedaftertherandomsearch,biased
search,andGAresults,achievedanaccuracyof85.93%,90.27%,and
91.88%,respectively.Inthefollowingparagraphs,wepresentand
discuss them:
TherandomsearchcannotdealwithJOINsandstrings. The
model classifies any query with JOINs as a failing one. In addition,
even in queries with no JOINs, the existence of a single string
equality also makes the model to classify the target as failing.
Thebiasedsearchsuffersfromquerieswithmanypredicates.
The model classifies any query with more than 6 predicates as a
failing one. On the other hand, queries with 4 predicates or less
are classified as successful. When the query contains between 4and6predicates,themodelusesthenumberofjoinsasawayto
differentiatebetweenthem:themodelclassifiesthetargetswiththeexistence of a single JOIN as a failing one. Interestingly, the strings
feature does not appear in the model; we discusss more about this
in Section 7.
The size of the query impacts the GA. The classifier classifies
anyquerywithlessthan5predicatesorwithmorethan5predicates
and less than 60 columns as successful. No other feature seemedrelevant to the model, meaning that the number of columns in a
query is what impacts the performance of the algorithm.
7 DISCUSSION
7.1 Biased search vs Genetic Algorithm
Ourresultsindicatethatbiasedsearch,althoughhighlyefficientfor
simple queries, suffers in queries with many predicates. The num-
bersalsoshowthat,forsimplequeries,itsruntimeperformancecan
beevenbetterthanthatachievedbytheGA.Thiscanbeexplained
in two ways: First, the experimental dataset contains several string
comparisons and non-complex LIKE commands, both of which can
besolvedbyseeding.Ourdataset,however,didnotcontainmore
complex string manipulation functions, such as concatenations, left,
right,length,reverse,orcombinationsofthem.Althoughourdataset
suggests that these operations do not often appear in SQL queries,
developers can use such functions.
After further investigation with manually created cases that did
actuallycontainsuchoperations,weobservedthattheGAisthe
onlyapproachthatisabletofindsolutioninsuchcases.Forexample,
the queries SELECT * FROM product WHERE length(name) =
12 AND left(name, 5) = ’REFRI’ AND right(name, 7) =
’GERATOR andSELECT * FROM product WHERE reverse(name) =
’ROTAREGIRFER’ are not solved by the biased search. On the other
hand,giventheinstrumentationthatweperforminthedatabase
enginetoguidethefitnessfunction,theGAapproachsolvesthis
particular query in a few seconds. Although these queries are just
examples,theyillustratehowthebiasedsearchisunabletosolve
complex cases where guidance is needed.
Second,althoughtheGAimplementationhasaninitialization
stepthatissimilartowhathappensinthebiasedsearch,atevery
iterationoftheevolution,theGAspendstimecalculatingthefit-nesses, and applying the search operators. All these calculationsdo not happen in the biased search. Thus, for less complex queries,
these extra actions that are taken by the GA explain the small
difference in the runtime performance.
ItisalsoworthnoticingthattheinefficiencyofGAcanbedueto
the single-target search strategy: GA is re-executed multiple times,
once for each coverage target. H owever, such a search strategy
oftenleadstoaninefficientallocationofthesearchbudget[ 14]as
the order by which the targets are optimized (over independent
runs) byGA stronglyimpacts thesearch effectiveness[ 14]. Toad-
dress this limitation, various multi-target evolutionary algorithms
have been proposed literature in the context of white-box unit test-
ing [14,28,29]. Therefore, better results may be obtained when
using these multi-target strategies, whose evaluation is part of our
future agenda.
7.2 Implications
WedesignedandimplementedEvoSQLwiththeworkingdeveloper
in mind, and anticipate the following usage scenarios:
(i)Query unit testing : As we saw in our experimental dataset,
queries can be highly complex, containing many predicates and
subqueries.EvoSQLprovidesdeveloperswithtestdatathatcom-
pletely exercises the query, enabling them to verify whether the
behavior of the query is exactly as expected.
(ii)Query regression testing :Supportdevelopersinrefactor-
ing or evolving their queries, using data sets generated from an
earlier version as an oracle to ensure that the queries preserve the
desired behavior.
(iii)Integration testing : Support developers in writing inte-
gration tests, helping them to create the right data sets enabling
them to trigger and test interesting interactions between code and
queries.
TheuseofgeneticalgorithmsforgeneratingSQLtestdataopens
up interesting areas for future research. Our current approach,based on HSQLDB, only exercises queries expressed by the stan-
dard SQL92 Specification. Different databases (e.g., MySQL, Oracle,
Postgres)providetheirownfunctions,datatypes,andconstructs,
whichwerenotincludedinthisstudy.Oracle,forexample,provides
a data type to store XML. For each database-specific function ordata type, our proposed fitness function can be optimized withtailored branch distances, mutation operators, and random data
generation.
Fromanintegrationperspective,dependenciesonexternalinfras-
tructures,suchasdatabasesandfiles,currentlyhinderthepowerof
automated test generation tools [ 15]. Researchers have worked on
achieving high branch coverage of source code that interacts with
such infrastructure by either generating database test data from
scratch or using previously existing databases [ 9,12,23,27,35].
However, we discuss in Section 8, they suffer from different lim-
itations. Our GA approach has no such restrictions, suggesting
that the interplay of our technique and source code test generation
could increase test coverage substantially.
Finally, we did not consider local search or memetic algorithms
(thecombinationoflocalandglobalsearch)[ 16].Futureresearch
mayexploretheimpactofcombininglocalsearchwiththeglobal
one on the coverage rate and performance.
1228
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Castelein et al.
7.3 Threats to Validity
Withrespecttointernalvalidity,(1)wecollectedthe2,135fromthe
foursystemsafterexecutingtheirexistingtestsuitesandextract-
ing the SQL queries from the database logs. Thus, our dataset is
limitedbythequeriesthatareactuallyexercisedbytheirtestsuites.
However, as we see in Table 2, the final dataset has shown to be di-
verse,rangingfromsimplequerieswithfewconstructstolargeand
complexquerieswithmorethan20columnsandcoveragepaths.
(2) The analysis of the infeasible targets that were generated by
SQLFpc was performed manually. To reduce the risk of classifying
afeasiblequeryasinfeasible,theanalysiswereconductedbythe
first two authors of this paper. (3) We defined the internal probabil-
ities of the GA approach after performing experimenting different
combinations of probabilities in a set of 100 tests. As testing all
possibleprobabilitieswouldimplyinanexplosionofcombinations,
we experimented a set of well-known intervals in the field [ 14,32].
Nevertheless,theremightbebetterconfigurations,andfuturework
needs to be conducted on this matter.
Withrespecttoexternalvalidity,weanalyzed2,135fromfour
differentsystems,threeofthembeingopensourceandoneofthem
beinganindustrysystem.Althoughsystemsweredifferentintheir
programminglanguagesaswellasintheirnature,moreresearch
needstobeconductedtogeneralizeourresultstoSQLqueriesin
any software system.
8 RELATED WORK
Several approaches [ 4,11,21,34] have been introduced by re-
searchers with similar goals to EvoSQL: generating test data or
testdatabasesbasedononeormoreSQLqueries.Theseapproaches
are based on constraint solving, which transform SQL queries to
constraintsandapplyconstraintssolverstosatisfythem.Therefore,these approaches suffer from two main limitations, namely: 1) they
arenotabletodescribetheentireSQLsyntaxasconstraints,and
2) solvers may not be able to satisfy certain constraints, e.g., when
SQLqueriesinvolvecommonconstructionssuchassubqueriesand
String predicates (in our evaluation set, 84.1% of queries contained
such constructions).
EvoSQLtakesadifferentapproachandbenefitsfromusingan
existing, fully functioning SQL database engine. As a consequence,
allqueriesusingstandardSQLsyntaxaresupported.Unfortunately,
to the best of our knowledge, none of the tools discussed in thissection are available for download and re-implementating them
wouldbeahighlydemandingtask.Thispreventedusfromdoingan
empirical comparison between them and EvoSQL. In the following,
we discuss the existing approaches [ 4,11,21,34] based on what is
reported in their papers, and highlight the differences between our
approach and them.
QAGrow, presented by Suárez-Cabal et al. [ 34], generates test
databases for a set of queries, using SQLFpc [ 37] as coverage cri-
terion.Theapproachgeneratestestdatabasesbyformulatingthe
problem of generating data for a query as a constraint satisfac-tion problem, in which the current database state is also taken
intoaccount.TheythenuseaSATsolver, Choco[20],togenerate
thetestdata.Theyevaluatedtheirapproachon215queriestaken
fromaclosed-sourcesystem.Onthesequeries,theyachieved99.0%SQLFpccoverage,inabouttwominutestime.However,QAGrow
does not solve queries which contain strings and subqueries.
Emmietal.[ 11]describeanalgorithmtoautomaticallygenerate
test input for database applications. Their goal, however, is not
to test the SQL query itself, but maximizing branch coverage of
theprogramcodeundertest.Theapproachalsousesaconstraint
solver, where the constraints are a combination of the path con-
straints in the program and the database constraints. Although the
approachhandlesstringconstraints(equality,inequalityandLIKE),
it supports only FROMandWHEREclauses, but no JOINs.
Khaleketal.[ 21]presentADUSA,atoolthatgeneratestestdata
for SQL queries. The paper uses the generated data to find faults
in database systems, such as MySQL and HSQLDB. Thus, they
generate repeated test data for the same SQL query. The approach
modelsSQLqueriesinto Alloy[19]specifications.ADUSAsupports
FROM,WHERE,GROUP BY andHAVINGclauses.However,ADUSAonly
supportsnaturaljoins(animplicitjoinbetweentwotables,using
commoncolumnnames)andcrossjoins(whichhavenopredicates).
In addition, the Alloy solver is unable to solve string constraints.
TheQAGentool,presentedbyBinnigetal.[ 4],alsogenerates
test databases with the goal of testing a DBMS. The approach uses
aconstraintsolvertogeneratetestdata.Themodelisbuiltusing
symbolic query processing, their extension of symbolic execution.
QAGen does not support subqueries, and solely supports JOINs
that use foreign key constraints in the JOINpredicate.
Finally, McMinn et al.[25,26] have worked on generating tests
fortheintegrityconstraints(e.g.,“mustnotbeNULL”constraint)
thatmayexistinadatabaseschemausingasearch-basedapproach.
Theirworkcomplementsoursas,inpractice,developersmustcom-
pletelytesttheirdatabases:thisincludesboththeintegrityofthe
schemas as well as executed SQL queries.
9 CONCLUSION
Thegoalofthispaperistounderstandhowtoautomaticallygen-
erate test data systematically covering realistic SQL queries. Toachieve this goal, we model test data generation for SQL queries
as a search-based problem. We devise and evaluate three different
approaches,basedonrandomsearch,biasedrandomsearch,and
geneticalgorithms.Wedefineafitnessfunctionthatcanbeusedto
steer such algorithms towards an optimal solution containing the
appropriate data to reach a given coverage target.
We offer an implementation of our approach in the open source
tool EvoSQL, which we use to evaluate the applicability of ourapproach in practice. Using the 2,135 queries collected from reallifesystems,wedemonstratethat(1)EvoSQLcanhandlethefull
SQL standard, including subqueries, JOINs, and string handling; (2)
achieves 100% coverage for 98% of our queries; and (3) manages to
do so in 2-15 seconds in most of the cases.
Our work paves the way for systematic and automated unit,
integration, and regression testing of SQL queries. Furthermore,
weanticipateinterestingfutureresearchintermsoffurtheropti-
mizing the performance and effectiveness of the genetic algorithm,
takingEvoSQLbeyondstandardSQL,makinguseofallintegrity
constraintsoftheschemato speedupthesear ch,andlettingthege-
neticalgorithmtakeinformationobtainedfromthesystem’ssource
code into account as well.
1229
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Search-Based Test Data Generation for SQL Queries ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]Mohammad Alshraideh and Leonardo Bottaci. 2006. Search-based software test
datagenerationforstringdatausingprogram-specificsearchoperators. Software
Testing, Verification and Reliability 16, 3 (2006), 175–203.
[2]AndreaArcuriandLionelBriand.2014. Ahitchhiker’sguidetostatisticaltests
forassessingrandomizedalgorithmsinsoftwareengineering. SoftwareTesting,
Verification and Reliability 24, 3 (2014), 219–250.
[3]JamesBergstraandYoshuaBengio.2012. RandomSearchforHyper-parameter
Optimization. J. Mach. Learn. Res. 13 (Feb. 2012), 281–305.
[4]CarstenBinnig,DonaldKossmann,EricLo,andMTamerÖzsu.2007. QAGen:
generating query-aware test databases. In Proceedings of the 2007 ACM SIGMOD
international conference on Management of data. ACM, 341–352.
[5] Edmund K Burke, Graham Kendall, et al. 2005. Search methodologies. Springer.
[6]JeroenCastelein,MaurícioAniche,MozhanSoltani,AnnibalePanichella,andArie
van Deursen. 2018. EvoSQL. https://github.com/SERG-Delft/evosql. (2018). The
preciseversionusedinthispapercanbefoundathttps://github.com/SERG-Delft/
evosql/releases/tag/icse.
[7]Jeroen Castelein, Maurício Aniche, Mozhan Soltani, Annibale Panichella, and
Arie van Deursen. 2018. Search-Based Test Data Generation for SQL Queries:
Appendix. https://www.zenodo.org/record/1166023.(2018). https://doi.org/10.
5281/zenodo.1166023
[8]Nitesh V Chawla. 2003. C4. 5 and imbalanced data sets: investigating the ef-
fectofsamplingmethod,probabilisticestimate,anddecisiontreestructure.In
Proceedings of the ICML, Vol. 3.
[9]David Chays, John Shahid, and Phyllis G Frankl. 2008. Query-based test genera-
tion for database applications. In Proceedings of the 1st international workshop on
Testing database systems. ACM, 6.
[10]Kalyanmoy Deb and Debayan Deb. 2014. Analysing mutation schemes for real-
parameter genetic algorithms. International Journal of Artificial Intelligence and
Soft Computing 4, 1 (2014), 1–28.
[11]Michael Emmi, Rupak Majumdar, and Koushik Sen. 2007. Dynamic test input
generation for database applications. In Proceedings of the 2007 international
symposium on Software testing and analysis. ACM, 151–162.
[12]Michael Emmi, Rupak Majumdar, and Koushik Sen. 2007. Dynamic test input
generation for database applications. In Proceedings of the 2007 international
symposium on Software testing and analysis. ACM, 151–162.
[13]GordonFraserandAndreaArcuri.2011. Evosuite:automatictestsuitegenerationfor object-oriented software. In Proceedings of the19th ACM SIGSOFT symposium
andthe13thEuropeanconferenceonFoundationsofsoftwareengineering.ACM,
416–419.
[14]Gordon Fraser and Andrea Arcuri. 2013. Whole test suite generation. IEEE
Transactions on Software Engineering 39, 2 (2013), 276–291.
[15]Gordon Fraser and Andrea Arcuri. 2014. A large-scale evaluation of automated
unittestgenerationusingevosuite. ACMTransactionsonSoftwareEngineering
and Methodology (TOSEM) 24, 2 (2014), 8.
[16]Gordon Fraser, Andrea Arcuri, and Phil McMinn. 2015. A memetic algorithm for
wholetestsuitegeneration. JournalofSystemsandSoftware 103(2015),311–327.
[17]Hector Garcia-Molina, Jeffrey D Ullman, and Jennifer Widom. 2000. Database
system implementation. Vol. 654. Prentice Hall Upper Saddle River, NJ:.
[18]DavidEGoldbergandKalyanmoyDeb.1991. Acomparativeanalysisofselection
schemesusedingeneticalgorithms. Foundationsofgeneticalgorithms 1(1991),
69–93.
[19]Daniel Jackson. 2002. Alloy: a lightweight object modelling notation. ACM
Transactions on Software Engineering and Methodology (TOSEM) 11, 2 (2002),
256–290.[20]NarendraJussien,GuillaumeRochart,andXavierLorca.2008. Choco:anopen
sourcejavaconstraintprogramminglibrary.In CPAIOR’08WorkshoponOpen-
Source Software for Integer and Contraint Programming (OSSICP’08). 1–10.
[21]Shadi Abdul Khalek, Bassem Elkarablieh, Yai O Laleye, and Sarfraz Khurshid.
2008. Query-awaretestgenerationusingarelationalconstraintsolver.In Proceed-
ingsofthe200823rdIEEE/ACMinternationalconferenceonautomatedsoftware
engineering. IEEE Computer Society, 238–247.
[22]Bogdan Korel. 1990. Automated software test data generation. IEEE Transactions
on software engineering 16, 8 (1990), 870–879.
[23]ChengkaiLiandChristophCsallner.2010. Dynamicsymbolicdatabaseapplica-
tion testing.. In DBTest.
[24]PhilMcMinn.2004.Search-basedsoftwaretestdatageneration:asurvey. Software
testing, Verification and reliability 14, 2 (2004), 105–156.
[25]Phil Mcminn, Chris J Wright, and Gregory M Kapfhammer. 2015. The effective-
nessoftestcoveragecriteriaforrelationaldatabaseschemaintegrityconstraints.ACMTransactionsonSoftwareEngineeringandMethodology(TOSEM) 25,1(2015),
8.
[26]PhilMcMinn,ChrisJWright,CodyKinneer,ColtonJMcCurdy,MichaelCamara,
and Gregory M Kapfhammer. 2016. SchemaAnalyst: Search-based test data
generationforrelationaldatabaseschemas.In SoftwareMaintenanceandEvolution
(ICSME), 2016 IEEE International Conference on. IEEE, 586–590.
[27]KaiPan,XintaoWu,andTaoXie.2011. Databasestategenerationviadynamic
symbolicexecutionforcoveragecriteria.In ProceedingsoftheFourthInternational
Workshop on Testing Database Systems. ACM, 4.
[28]A.Panichella,F.Kifetew,andP.Tonella.2017. AutomatedTestCaseGenerationas
a Many-Objective Optimisation Problem with Dynamic Selection of the Targets.
IEEE Transactions on Software Engineering PP, 99 (2017), 1–1.
[29]Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2015. Refor-
mulatingbranchcoverageasamany-objectiveoptimizationproblem.In Software
Testing, Verification and Validation (ICST), 2015 IEEE 8th International Conference
on. IEEE, 1–10.
[30]YuhuaQi,XiaoguangMao,YanLei,ZiyingDai,andChengsongWang.2014. The
Strength of Random Search on Automated Program Repair. In Proceedings of the
36th International Conference on Software Engineering (ICSE 2014) . ACM, New
York, NY, USA, 254–265.
[31]Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann
Publishers, San Mateo, CA.
[32]JoséMiguelRojas,GordonFraser,andAndreaArcuri.2016. Seedingstrategiesin
search-basedunittestgeneration. SoftwareTesting,VerificationandReliability
26, 5 (2016), 366–401.
[33]S.N.SivanandamandS.N.Deepa.2007. Introductiontogeneticalgorithms.Springer
Science & Business Media.
[34]María José Suárez-Cabal, Claudio de la Riva, Javier Tuya, and Raquel Blanco.
2017. Incremental test data generation for database queries. Automated Software
Engineering (2017), 1–37.
[35]Kunal Taneja, Yi Zhang, and Tao Xie. 2010. MODA: Automated test genera-
tion for database applications via mock objects. In Proceedings of the IEEE/ACM
international conference on Automated software engineering. ACM, 289–292.
[36] Edward Tsang. 1993. Foundations of constraint satisfaction. Academic Press.
[37]JavierTuya,MaríaJoséSuárez-Cabal,andClaudioDeLaRiva.2010. Fullpredicate
coverage for testing SQL database queries. Software Testing, Verification and
Reliability 20, 3 (2010), 237–288.
[38]Shin Yoo and Mark Harman. 2012. Regression testing minimization, selectionand prioritization: a survey. Software Testing, Verification and Reliability 22, 2
(2012), 67–120.
1230
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. 