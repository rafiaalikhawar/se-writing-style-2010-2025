Effects ofExplicitFeatureTraceability on
Program Comprehension
JacobKrüger
Otto-von-GuerickeUniversity
Magdeburg,Germany
jkrueger@ovgu.deGülÇalıklı
Chalmers |Universityof Gothenburg
Gothenburg, Sweden
calikli@chalmers.seThorsten Berger
Chalmers |Universityof Gothenburg
Gothenburg, Sweden
bergert@chalmers.se
ThomasLeich
Harz University& METOP GmbH
Wernigerode & Magdeburg ,Germany
tleich@hs-harz.deGunterSaake
Otto-von-GuerickeUniversity
Magdeburg,Germany
saake@ovgu.de
ABSTRACT
Developers spend a substantial amount of their time with program
comprehension. To improve their comprehension and refresh their
memory,developersneedtocommunicatewithotherdevelopers,
readthedocumentation,andanalyzethesourcecode.Manystud-
ies show that developers focus primarily on the source code and
thatsmall improvementscan have a strong impact. Assuch, it is
crucial to bring the code itself into a more comprehensible form. A
particulartechniqueforthispurposeareexplicitfeaturetracesto
easily identify a program’s functionalities. To improve our empiri-
calunderstandingabouttheeffects offeaturetraces,wereportan
online experiment with 49 professional software developers. We
studied the impact of explicit feature traces, namely annotations
and decomposition, on program comprehension and compared
them to the same code without traces. Besides this experiment, we
also asked our participants about their opinions in order to com-
bine quantitative and qualitative data. Our results indicate that, as
opposedtopurelyobject-orientedcode:(1)annotationscanhave
positive effects on program comprehension; (2) decomposition can
have a negative impact on bug localization; and (3) our partici-
pants perceive both techniques as beneficial. Moreover, none of
the three code versions yields significant improvements on task
completiontime.Overall,ourresultsindicatethatlightweighttrace-
ability,suchasusingannotations,providesimmediatebenefitsto
developersduringsoftwaredevelopmentandmaintenancewithout
extensivetrainingortooling;andcanimprovecurrentindustrial
practices that rely on heavyweight traceability tools (e.g., DOORS)
and retroactive fulfillment of standards (e.g., ISO-26262, DO-178B).
CCS CONCEPTS
·Generalandreference →Empiricalstudies ;·Softwareandits
engineering →Softwaredesign tradeoffs ;Maintainingsoftware .
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrights forcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’19, August 26ś30,2019, Tallinn,Estonia
©2019 Copyright heldby the owner/author(s). Publicationrightslicensed to ACM.
ACM ISBN 978-1-4503-5572-8/19/08...$15.00
https://doi.org/10.1145/3338906.3338968KEYWORDS
Program comprehension, Feature traceability, Software mainte-
nance,Separationofconcerns
ACMReference Format:
JacobKrüger,GülÇalıklı,ThorstenBerger,ThomasLeich,andGunterSaake.
2019.EffectsofExplicitFeatureTraceabilityonProgramComprehension.In
Proceedingsofthe27thACMJointEuropeanSoftwareEngineeringConference
andSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE’19),
August 26ś30, 2019, Tallinn, Estonia. ACM, New York, NY, USA, 12pages.
https://doi.org/10.1145/3338906.3338968
1 INTRODUCTION
Developersoftenneedtounderstandthepurposeandthedetailsof
specific parts of a codebase, which is a time-consuming and cogni-
tively demanding activity during software engineering [ 33,60,61].
A developerperformsthisactivity, knownas program comprehen-
sion, when they are new to a program or forgot details that are
requiredfortheirtask[ 8,31].Consequently,togainimplicitknowl-
edgeaboutaprogram,developersneedtoreadandcomprehendthe
code,whichcanbefacilitatedbymentoringandbyexplanations
from other developers. However, communicating knowledge in
such a way requires considerable effort from other developers and
interrupts theirownactivities.
Totacklesuchproblems,severaltechniqueshavebeenproposed
to reverse-engineer information or to improve program compre-
hension,oftenuponempiricalstudies.Contemporarytechniques
comprise, for instance, creating on-demand documentation [ 47],
topic modeling [ 64], and visualizing execution traces [ 14]. Still,
developers are known to mainly focus on the source code itself,
rather thandocumentationandotherartifacts[ 6,33,51,57]. Con-
sequently, bringing the source code into a more understandable
form is crucial to support program comprehension and to improve
the software design. Several concepts and techniques have been
proposedforthispurpose,suchasprogrammingparadigms(e.g.,
object-orientation [ 3], feature-orientation [ 21]), code recommen-
dations (e.g., on identifier names [ 15,34], decomposition strate-
gies [27,59]), and other supportive techniques (e.g., source code
comments [ 39,67], documentation traceability [ 1,37]).
In this paper, we are concerned with a design decision that is
oftenarguedtopositivelyimpactsoftwaredevelopmentandmainte-
nance:Explicittraceabilityofsoftwarefeaturesinthesource
code.Explicit traceability refers to code styles that explicitly mark
338ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Jacob Krüger, Gül Çalıklı, ThorstenBerger,Thomas Leich,andGunter Saake
whatpartsofthecodebelongtowhatfeature.Suchexplicitloca-
tionshelpdeveloperstofasteridentifyrelevantcodeandunderstand
what the corresponding feature does. As we report in Section 2 ,
somestudiesindicateapositiveeffectofexplicittracesonprogram
comprehension.However,thesestudiesareusuallyconductedas
controlled experiments with a small number of students and in-
volve specialimplementationtechniques, suchas feature-oriented
programming [ 44]. In contrast, we (i) conducted an experiment in-
cluding 49 experienced, professional software developers; (ii) used
feature traces that are independent of implementation techniques;
and (iii) comparedboth types oftracesnot onlytoeachother,but
alsoto object-orientedcode withoutany traces.
Forthisexperiment,werandomlydistributedallinviteddevel-
opers into three groups, each of which had to perform six tasks
on Java code that comprised (1) no feature traces, (2) annotated
features or (3) decomposed features. We refer to annotating (i.e.,
features were commented) and decomposing (i.e., features were
implementedinseparateclasses)as separationoffeatures [27,53].
By using lightweight designs to incorporate feature traces, we did
notneedtoteachourparticipantsanewimplementationtechnique.
We did this to reduce learning efforts, which we argue to benefit
the usability andintroduction of explicit feature tracesin practice.
The results indicate that, compared to pure object-oriented code,
annotations can have a positive impact on understanding features,
while decomposition can potentially hamper bug localization. Still,
due to our sample size, we have to be careful with interpreting
these results, but qualitative responses also indicate a strong favor
ofmostparticipantstowardsexplicitfeaturetraces.Incombination
withfindingsofotherresearchersonmorespecializedimplementa-
tiontechniques[ 24,27,50,53],wearguethatexplicitfeaturetraces
andespeciallyannotationscanimproveprogramcomprehension
andsupportautomationwithoutnegativelyimpactingthetimethat
developers needto analyze code.
In summary,our contributionsare as follows:
•Wereportanddiscussquantitativedataonthecorrectness
and completion time of our participants for six program
comprehensiontasks.
•Wediscussqualitativeresponsestoshedfurtherlightinto
the benefitsandproblems that our participants faced.
•Weprovideareplicationpackagethatincludesourexperi-
mental design, the source code of our subject system, and
allanonymizedresponses inarepository.1
Our results provide empirical insights into the impact of explicit
featuretracesondevelopers’taskperformance.Especially,aswe
confront our participants with unfamiliar code, they have no previ-
ous knowledge about it and face the scenario of familiarizing with
newcode andthe assignedtasks.
2 RELATED WORK
Thenotionoffeatureshasbecomeafundamentalconcept,notonly
to implement variability in software product lines [ 2,7], but for
software engineering in generalÐused to communicate, document,
and structure systems [ 4,28]. In particular, an extensive body of
researchinvestigatesthetaskoflocatingfeaturesinthesourcecode
1https://doi.org/10.5281/zenodo.3264974ofasystem[ 5],automatically[ 49]aswellasmanually[ 25,63].Fea-
ture location is a time-consuming and costly task that is necessary
tomaintainorfixÐessentially,comprehendÐafeaturethatisnot
made explicit in the code. The benefits of explicit feature traces are
apparent,astheyfreethedeveloperfromlocatingfeaturesinthe
code,savingtimeandprovidingfocuspointsfordevelopers[ 19,23].
Research in the related area of requirements traceability is con-
cerned with tracing requirements throughout various artifacts
downtothesourcecodeofasystem.Tothisend,severaltechniques
have been proposed to recover traces to the source code [ 9,40].
Moreover,empiricalstudies[ 10,18,37,46]suggestthatsuchtraces
cansignificantlyfacilitatedevelopers’tasks.However,mostofsuch
techniques rely on external tools, and requirements are a different
abstractionthanfeatures.Bothcanbeinanyrelationtoeachother,
for example,afeature needsto fulfill multiple requirements.
Duetothevarietyoftechniquesthatcanbeusedtoenablefea-
ture traceability,an important question arises: What technique is
suitable in what situation to support developers understand source
code?Inthisregard,researchershavecompareddifferentfeature
characteristics and traceability techniques to gain insights. For in-
stance, Liebig et al .[36], Passos et al. [ 42,43], Melo et al .[38], and
we [26] investigated the characteristics of feature implementations
and how these impact maintainability, evolution, and the archi-
tectureofasystem.Furthermore,Feigenspanetal . [12]analyzed
whetherbackgroundcolorsinsteadoftextualannotationsfacilitate
program comprehension. In contrast, Parnas [41]discusses how to
decompose a system into modules or components, another widely
usedtechniqueto separate andtrace features.
Despitesuchtechniquesandstudies,itisstillanopenissuehow
toseparatefeaturesmosteffectively.Severalauthorsargueabout
potential advantages and disadvantages of annotating features in a
singlecodebaseversusseparatingthemintomodules[ 22,30,35].
Due to the complexity of comparing such implementation tech-
niquesandduetopsychologicalbiases[ 55],onlyfewresearchers
report empirical studies. Siegmund et al . [53]conducted a con-
trolled experiment in which they compare preprocessor annota-
tionsandfeature-orientedprogramming.However,thisexperiment
includes only eight students,limitingmore generalinterpretation.
In a follow-up experiment on bug fixing [ 50], 33 students have
beeninvolved,buttheresultsshownosignificantbenefitsofeither
technique. Our previous works on this topic include a survey with
34developers[ 27],apreliminaryanalysisofdevelopercommuni-
ties [24], and a case study [ 29]. During these works, we have been
concernedwithannotatinganddecomposingfeaturestoprovide
insightsinto the opinionsandexperiences of developers, but they
donotprovideexperimentalevidenceonprosandconsofeither
technique.Allthesestudiesfocusonspecializedimplementation
techniquesforvariability,addingcomplexityandeffortforpractical
usage. Moreover, none of these studies analyzes pros or cons of
using any ofthesetechniques comparedto not using it.
Overall, it is still not clear to what extent separating fea-
tures with one of the basic techniquesÐannotations or de-
compositionÐimpacts a developer’s ability to understand a
program. Ourgoalistoimprovetheempiricalevidenceconcerning
theimpactofsuchexplicitfeaturestraces.Incontrasttoprevious
works, we are not concerned with implementation techniques that
339Effects of Explicit Feature Traceability onProgram Comprehension ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
allowvariability,butrelyoncommentsandclassesthatdonotre-
quiredeveloperstolearnnewconcepts.Thisalsoexcludestheusage
of external tools,as these add further abstractions and developers
may be reluctantto use them[ 27]. Moreover, we are interestedin
understandingtheprosandconsoffeaturetracescomparedtocode
that does not comprise any. Nonetheless, our experimental design
ispartlyinspiredbyprevious studiesandguidelines [ 11,53,54].
3 EXPERIMENTALDESIGN
Inthissection,wedescribethe goal,subjectsystem ,implementation ,
distribution ofparticipants, and tasksofour experiment.
3.1 Goal& Research Questions
We aimed to empirically assess the impact of explicit feature traces
in source code. To this end, we have been concerned with two
establishedtechniques:annotationsanddecompositionintocom-
ponents (cf. Section 2). Arguably, both techniques facilitate feature
location, as features are separated and can be easily found through
searchingtheiridentifiersinannotationsorfilenames,respectively.
Inordertoinvestigatetheirimpactonprogramcomprehension,we
consideredthedifferentcodeversionsas independentvariables ,com-
prisingthethreelevelsobject-oriented,annotated,andcomponents.
Moreover,weaimedtocontroltheparticipants’programmingexpe-
rience, meaning that we considered the experience as independent
andnot as confoundingvariable.
Toaddressour goal, we definedthree researchquestions :
RQ1To what extent does feature traceability impact the ef-
fectiveness ofprogram comprehension?
Weinvestigatedwhetherannotationsordecompositionim-
proveourparticipants’abilityto correctlyunderstandcode
(i.e., effectiveness). To this end, we used the number of faults
as metric ( dependent variable ) and compared the ratios of
correctsolutions between allthree code versions.
RQ2To what extent does feature traceability impact the ef-
ficiency ofprogram comprehension?
We investigated whether annotations or decomposition facil-
itateourparticipants’abilitytounderstandcode faster(i.e.,
efficiency).Tothisend,wemeasuredtheircompletiontime
(dependentvariable ) for eachtask.
RQ3What is our participants’ perception of feature trace-
ability on theperformed tasks?
Besidesquantitative measures, we were concerned withour
participants’ perception of explicit feature traces. In partic-
ular, we wanted to understand what problems or benefits
they experienced while understanding the source code. Con-
sequently,weaddressedthisresearchquestionbasedonqual-
itativeresponsesandmappedthosetoourquantitativedata.
Based on existing studies [ 50,53], wehypothesized that an-
notations and decomposition perform comparable to each other.
In contrast, we assumed that the explicit traceability of features
would facilitate all tasks compared to pure object-oriented code,
while the correctness should remain similar. Overall, we defined
ournull-hypotheses thatweaimedtorefutewithourdataasfollows
for the corresponding researchquestions:
H1The correctness of our participants’ task solutions does not
differbetween groups.H2The efficiency of our participants to complete tasks does not
differbetween groups.
We tested each hypothesis by comparing two groups to each other
(pair-wise)forallofoursixtasks(cf. Section3.4 ),resultinginatotal
of18testsforeachhypothesis(e.g.,object-orientedcomparedto
annotations, annotations compared to composition). We corrected
ourtestresultstoaddressmultiplehypothesistesting(cf. Section4).
3.2 Subject System
As our subject system, we selected Mobile Media , which has been
developed by researchers of the software-product-line commu-
nity[68]andwaslaterextendedwithfeatureannotations(using
theCpreprocessor)thatweusedasbaseline[ 53].Duetoitscareful
designandusageofstandardcodingtechniques,itisanappropriate
subject system that has been used in several studies [ 50,52,53].
Moreover, it is implemented in Java, which is one of the most com-
mon programming languages.
The software provides a content management system for media
files on mobile devices. In our experiment, we used a single file,
namelyMediaControler.java , that implements ten features of the
software. Thesefeatures are related to storing and managing pho-
tos, music, and videos. To avoid biases, we removed all existing
commentsinthefile.Moreover,weremovedlibraryimports,which
contributetoapproximately10%ofthetotallinesofcode,andan
SMS feature, of which only a small part is implemented in this file.
We did this tolimit the code sizethatour participants had toread,
whichwasaround 400lines,inthe end.
Finally,we refactoredthe file intothree differentversions:
(1)Object-Oriented: Inthisversion,weonlyremovedtheexisting
preprocessorannotations(e.g., #ifdef)toprovidethesource
code without any feature traces. Thus, we obtained pure
object-orientedcode that we usedfor the controlgroup.
(2)Annotated: For annotation-based feature traces, we replaced
existing preprocessor annotations with traceability anno-
tations based on existing studies (i.e., //&begin [feature] ,
//&end [feature] )[19,26,28].Wedecidedtodothis, (i)as
CpreprocessorannotationsarerarelyusedinJavaprograms,
(ii)toavoidconfusionoverpotentialvariabilitywearenot
interestedin, and (iii)to not introduce newconcepts.
(3)Components: Toobtainthedecomposedversion,weextracted
each feature into a class and added static methods that com-
prise the feature’s code. For each class, we used the cor-
responding feature’s name as file name and removed all
existing annotations.
Due to these code designs, the participants did not need to learn
anynewconceptsforanyversion.Knowledgeaboutcommentsand
object-orientationaresufficienttounderstand suchfeature traces
after ashort introduction.
3.3 DistributionofParticipants
We personally invited 144 software developers from different coun-
triesandaskedthemtosharetheinvitationwithothers.Ourgoal
was to include developers with industrial experiences and increase
their motivation to participate. After accepting the invitation, each
340ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Jacob Krüger, Gül Çalıklı, ThorstenBerger,Thomas Leich,andGunter Saake
Table 1:Questions to quantifyprogramming experience.
ID Question|AnsweringOptions(A)
Q1Howdo youestimateyour programmingexperience?
A: 1(very inexperienced)ś10(very experienced)
Q2HowexperiencedareyouwiththeJavaprogramminglanguage?
A: 1(very inexperienced)ś10(very experienced)
Q3Forhowmanyyearshave youbeen programming?
A:◦<2;◦2-5;◦6-10;◦11+
Q4For how many years have you been programming for larger
software projects (e.g.,incompanies)?
A:◦<2;◦2-5;◦6-10;◦11+
Q5What is your highest degree of education that is related to
programming?
A: Multiplechoice (optional text)
developerhadtofillinasurveytoassesstheirprogrammingexperi-
ence. We provide an overview of the survey questions and possible
answers in Table1.
Thesequestionsarebasedonanempiricallyderivedproposal[ 54].
WebasedtheanswerclassificationsforQ 3andQ4onalargeuser
survey of Stack Overflow.2In this survey, approximately one quar-
ter of the participants has been in each of the classes we show
inTable 1. We mapped the classes to a scale from one to ten (i.e., 2,
4, 7, 9), aligning them to the first two questions. Considering the
degree, we only identified whether a developer received one (8)
ornot(3),asitishardlypossibletosaywhichonesmayindicate
łbetterž developers. For the experience value, we computed the av-
erageofallscalesandconsideredadeveloperasnoviceiftheresult
wasbeloworequalto5.5Ðorasexpert,otherwise.Werandomly
distributed our participants into three groups, one for each code
version(i.e.,object-oriented,annotated,components),withequal
ratiosofnovices andexperts,andsentthe actual experiment.
3.4 Tasks& Questions
For the first part of our experiment, we selected six tasks that
involve, but are not directly concerned with, feature location for
tworeasons:
•Participants of the annotated andcomponents groups can
quicklylocate features bysearching the names.
•We aimed to limit learning effects that may impact our sub-
jects’ performances incompleting theirtasks.
Incontrasttothestraight-forwardtaskoffeaturelocation,wewere
interestedintheimpactoffeaturetraceabilityontasksthatrequire
actualcomprehension.Therefore,wedesignedtwosectionswith
three tasks,each.
In the firstsection, wewere concernedwithcomprehending fea-
tures and their interactions, which does not only require to locate
the corresponding code, but to alsounderstand it. Feature interac-
tions represent different system functionalities that interact and
may influence each other. Thus, feature interactionsare an impor-
tantchallengethatcaneasilyresultinproblemsduringprogram
comprehension and bug fixing [ 2]. The tasks that we defined for
the firstsection were:
2https://insights.stackoverflow.com/survey/2016#developer-profile-experienceTable 2: Questions to evaluate the participants’ experiences
with thetasksandon feature traceability.
ID Question | Answering Options(A)
EQ1Did you have any problems in answering the survey, e.g., under-
standingthe questions or concepts?
A:◦yes;◦no
EQ2Whatwasyourstrategyforcomprehendingthecodeinordertodo
the tasks?
A:Freetext
EQ3Whathavebeenyourmainproblemsorchallengesduringthetasks?
A:Freetext
EQ4(Annotated) Do you thinkthattheannotations provided for each
featurehelped youunderstand the code?
(Components) Do you think that the separation of features into
classeshelped youunderstand the code?
(Object-Oriented) Doyouthinkthatadifferentcodedesignconcern-
ingthe features (e.g., annotating theirbegin and end, implement
theminseparateclasses)wouldhavefacilitatedyourprogramcom-
prehension?
A:Freetext
EQ5Did you face an interruption (more than 5 minutes) for anyof the
6 tasks?
A:Checkboxfor eachtask
EQ6Doyouhaveanycommentson the survey?
A:Freetext
(1) Outoffourfeature pairs, selectthosethat interact;
(2) Selectthe lineswhere twodescribedfeatures interact; and
(3)Out of four statements about this feature interaction, select
thosethat are correct.
In the second section, we asked our participants to locate bugs,
whichwe inserted:
(4) Intoafeature (cannotcapture photos);
(5) Intoafeature interaction (wrongcounter for videos); and
(6) Intothe basecode (cannotdelete photos).
These bugs resemble simple faults (i.e., copy-paste errors, incre-
ments),similartomutations[ 20].Eachtaskwasaboutadifferent
feature to mitigate learningbiases.
At the end of our experiment, we asked our participants to elab-
orate on their experiences and to describe whether they faced any
problems. We show the corresponding questions in Table 2. EQ1
wasasimplecheckquestiontoverifyiftherewereanymisunder-
standings, which could also be elaborated on in EQ 6. We used EQ 5
to verify whether a participant was interrupted during any task,
meaning that we considered the corresponding results differently.
With the remaining three questions, we were concerned with gath-
eringqualitativedatatoanswer RQ3.WeremarkthatEQ 4exists
inthree differentversions,one for eachcode version.
3.5 Implementation & Testing
Due to the tasks we defined (i.e., marking lines of code) and the
design of our experiment (i.e., accessible via internet), we were
notabletoreuseexistingsurveytoolswithoutconsiderablecosts
andadaptations.Forthesereasons,wedecidedtoimplementour
own solution that was based on a simple server-client architecture
and fulfilled the requirements of our experiment. We tested our
341Effects of Explicit Feature Traceability onProgram Comprehension ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
Table 3:Experiencevaluesofour participants.
VersionExperienceParticipantsMin Median Mean Max
Annotated 5.60 7.00 6.96 8.80 18
Components 4.00 7.20 6.88 9.20 15
Object-Oriented 6.20 7.60 7.61 9.20 16
Total 4.00 7.40 7.15 9.20 49
implementation extensively with own test runs. Code reviews and
additionaltestsofourimplementationwereperformedbythreecol-
leagues consisting of a software developer, a system administrator,
and a PhD student. Moreover, two of these colleagues tested the
actualsurveytoevaluateformulationsandthetasks’complexity.
None of the three colleagues participated in the actual experiment.
We decided to conduct our experiment via the internet to in-
crease our range, provide the opportunity to conduct the tasks
at any time, and have access to developers all around the world
without extensive traveling. Thus, this is not a fully controlled
experiment,butanunsupervisedonethatwasconductedinreal-
world settings in which developers may be distracted or switch
tasks.Withthisdesign,weaimedtoincreasetheexternalvalidity
ofour results.
4 RESULTS AND DISCUSSION
Inthissection,wereportdetailsabouttheparticipantsofourexperi-
mentandtheresults.Weseparatelyreportanddiscussobservations
for eachofour researchquestions.
4.1 Participants
Unsurprisingly,notalldevelopersthatweinvitedparticipatedin
our experiment. Overall, we received 49 responses from around
the world, mostly from Turkey (20), Germany (13), and the United
States (7). In Table 3, we show the distribution of our participants’
experience values based on our rating scale (cf. Table 1). Only two
participantsstatedthattheyhaveworkedforlessthentwoyears
onlarge-scaleprojects.Aswecansee,themedianandmeanvalues
areclosetoeachotherandamongthegroups.Whilethedistribu-
tionofparticipantsforeachprogramversionisnotidentical,the
differencesaresmall.Moreover,mostparticipantsareconsidered
experts according to our analysis with only three of them in the
componentsgroup not achievingthis rating.
Overall, we can see small differences between the groups of
participants. Nonetheless, we had at least 15 participants and 12
expertsforeachcodeversionofourexperiment.Consideringthis
information, the responses we analyzed represent a diverse and
experiencedsetofpractitioners.Thus,wearguethatnoneofthe
differences threatens the results of our study, but we have to be
cautious withour interpretations.
Validity of Responses. As aforementioned, we aimed to attract
experienced software developers and intended to focus on external
validity. Due to our study design, there have been several partic-
ipants who reported disruptions while they worked on a task or
problems in understanding some details (however, most elaboratedabout code issues, rather than the experiment itself). To address
this issue, we firstperformed a sanitycheck inthecontext of RQ1
for developers stating comprehension problems. Considering RQ2,
we removed all completion times for which interruptions were
reported, as these measures would not accurately represent the
requiredeffort.
Due to technical issues, single data points for some participants
aremissing.First,threeparticipantsreportedproblemswithtask
1, or were just missing the entry. We decided not to count these
responses, wherefore task 1 for the annotated group comprises
only 15 responses (cf. Figure 1). Second, one participant of the
components group did answer all questions except the elaboration
(cf.Table 2). We decided to include this response, but to put it into
thegroupwithcomprehensionproblemsforthesanitycheck(as-
suming that there have been misunderstandings). Except for these
four,we excludedallotherunfinishedorincompleteresponses.
4.2RQ1:Effectiveness
InFigure 1, we show how many of our participants were able to
correctly solve each task. We distinguish between three groups
according to our first independent variable, the version of the code
the participants investigated ( A:Annotated; C:Components; OO:
Object-Oriented). Moreover, we considered whether the partici-
pantsindicatedproblemsinunderstandinganypartoftheexper-
iment (CP) or not ( NCP). We applied hypothesis testing to test
whether our observations may be significant. In particular, we
tested observation 1 that represents our sanity check and based on
whichwe scopedour remaining observations,analyses,andtests.
Observation 1: Difficulties in understanding the survey had no
impactontheresults. Comparingthecorrectandincorrectanswers
oftheparticipants withandwithoutcomprehensionproblemsfor
eachtask,wecanseethatthedistributionsaresimilar.Moreover,in
some tasks the ratio of correctly solved tasks with comprehension
problems is identical compared to those without problems (e.g.,
for task 2 of the annotated group, both have eight correct and one
incorrectanswer).Thus,itseemsthatproblemsinunderstanding
ourexperimenthadonlylimitedimpactonourparticipants’ability
to correctly solve a task. This is reasonable, as the code was un-
knowntoourparticipants,meaningthattheyhadtounderstand
it anew anyway. As in daily life, they can still understand code,
even if facing a potentially vague assignment. In addition, most
participantsstatedthatthecodewastheproblemforunderstanding
(e.g.,toolong),ratherthanthetasksthemselves.However,thecode
anditsdesignwerethesubjectweaimedtounderstand,meaning
that the results should be comparable.
Hypothesistesting. Basedonourobservation,wehypothesized
thattherearenothreateningdifferencesbetweentheparticipantswho
did have and who did not have problems in understanding our exper-
iment.To test our hypothesis, we applied Fisher’s exact test [ 13],
asimplemented inthe Rstatistics software[ 45].Weused Fisher’s
exacttest,becauseitcanbeappliedonsmallsamplesizes,butwe
still have to be careful with interpreting the results. To account for
multiplehypothesistesting,wereliedonaBonferroni-Holmcorrec-
tion[16]withaglobalconfidenceintervalof0.95.Intheremaining
paper,wereportthep-valuesofallsignificantresultsandalsostate
342ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Jacob Krüger, Gül Çalıklı, ThorstenBerger,Thomas Leich,andGunter Saake
6621
3156
178
8811
7314
2266
1287
5334
178
8712
3255
871
8712
6324
853
5544
627
7414Task 1 Task 2 Task 3 Task 4 Task 5 Task 6
A C OO A C OO A C OO A C OO A C OO A C OO0.000.250.500.751.00
GroupRepsonses
Incorrect (CP) Incorrect (NCP) Correct (CP) Correct (NCP)Feature Comprehension Bug Localization
Figure1:Distributionofcorrectlyandincorrectlysolvedtasksforeachcodeversion(A:Annotated;C:Components;OO:Object-
Oriented)andtask.Moreover,wedisplaywhetherthecorrespondingparticipantsstatedcomprehensionproblemsornot(CP:
ComprehensionProblems; NCP: NoComprehensionProblems).
theapproximated,Bonferroni-Holmcorrectedthresholdthathadto
befulfilled.The null-hypothesis isthatcorrectandwrongresponses
are equally distributed.
In total, we tested 18hypotheses, one for eachpair-wise combi-
nationofgroupsforeachofthesixtasks.Noneofthetestsindicated
that the differences are significant, wherefore we cannot reject our
null-hypothesisSo,wecontinuedundertheassumptionwederived
from our observation, namely that participants who had problems
inunderstandingthesurveydidnotperformworsethanthosewho
didnot.Duetothisobservation,wefocusedonallparticipantsand
did not separate those that had comprehension problems for our
remaining analyses.
Observation 2: Explicit feature traces result in higher effectiveness
forcomprehending featureinteractions. Consideringthe firstthree
tasks, we can see in Figure 1that the pure object-oriented code
performs worse compared to annotations and components. For the
firstandthirdtask,onlyoneparticipantwhoworkedontheobject-
oriented version was able to correctly identify and understand
the feature interactions. Moreover, concerning task 2, only three
moreparticipantshavebeenabletocorrectlysolvethetask.This
result seems unsurprising, as explicit feature locations facilitate
understanding interactions considerably: Developers canfocus on
certain parts ofthecodeand do nothave toidentify thecodethat
implementsthefeaturefirst,which istimeconsumingandcanbe
faulty, as every developer has an own, potentially different, notion
ofwhat afeature comprises[ 4,5,25].However, it is surprising that participants who analyzed compo-
nentshadconsiderablymoreproblemswhileidentifyingfeatures
that interact (task 1). The data shows that participants selected
multiplewronginteractions.Incontrast,theyweremoreoftenable
to correctly explain how features interact (task 3). For this task,
the faulty responses usually show that it seems unclear for the
annotatedgroup,towhatextentfeaturesinteract:Whichfeature
doesmodifywhichfeatureinwhatway?Bothgroupsperformed
comparable for locating asinglefeature interaction (task 2).
Potentially, it is easier to identify that features interact at all
if their code is close to each other (annotated code), rather than
separatedintodifferentclassesÐresultinginthecodeloosingitssur-
rounding context and potentially leading to the anti-pattern action
at a distance [24,27]. In contrast, this loss of context may be better
to identify how features interact in the data-flow: Method calls can
already indicate whether a functionality is only used or whether
variables are changed. By inspecting the separated features, devel-
operscanmoreeasilyidentifygloballyaccessibleandpotentially
interacting variables. Annotated code may complicate this analysis
asallcontext,evenirrelevantone,isconnectedtothefeature.Iden-
tifying and understanding the actual data-flow interactions [ 48] of
featuresremainschallenging,evenwithexplicitfeaturetraces,as
wasexplicitly statedbysomeparticipants (cf. Section 4.4 ).
Observation3:Decompositionresultsinlesseffectivenessforbug
localization. Forthelastthreetasks,wecanseethatparticipants
who faced the decomposed code identified fewer bugs correctly
343Effects of Explicit Feature Traceability onProgram Comprehension ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
comparedtoothergroups.Surprisingly,theyevenperformedworse
for a faulty named label within a feature (task 4). As this bug is
connected to, and thus directly placed in, a feature, we expected
that the participants could easily identify the bug. The wrongly
selected answers show that most participants identified the correct
class, but selected a wrong line after the actual bug. As is also
highlighted by participants’ feedback (cf. Section 4.4 ), this problem
seemsconnectedtothefactthattheclassesrepresentedfeatures
andnotlogicalobjects,asintendedinobject-orientedprogramming.
Again, the same issueof lost context andunderstanding data-flow
we described for our previous observation seems to have impacted
our participants’ ability to locate bugs. However, to understand
theseeffectsinmoredetail,werequireadditionalstudiesonlocating
andfixing bugsindecomposedcode.
Observation4:Annotationsdonotresultinmoreeffectivebuglo-
calization. Forannotations,weobservethattheratioofcorrectly
localizedbugsissimilarcomparedtotheobject-orientedversion.
Thus,annotationsseemtohavenonegativeimpactonbuglocal-
ization. As the bugs are rather simple, the annotations may not
behelpfulinthisscenario,ortheanalysisofobject-orientedcode
mayhaveresultedinbetterknowledgeofourparticipants.More
extensivestudiesofthesefactorsareneededtobetterunderstand
howthey influencedevelopers’ program comprehension.
Hypothesistesting. Foreachtask,wecomparedallgroupsagainst
eachother(pair-wisetests).Toaccountforlearningeffectsofour
participants,weconductedall18testssimultaneouslyandcorrected
them togetherÐinstead of testing each task individually. Again, we
used Fisher’s exact test and the Bonferroni-Holm correction. To
this end, we always assumed as null-hypothesis that the ratio of
correct and incorrectanswersbetweentwogroupsis equal(cf. H1).
The test results include three significant outcomes for which we
can refute our nullhypothesis. For tasks1 and2, we foundsignifi-
cantdifferences betweentheannotated and object-oriented group
(p<.0001andp<.001with corrected thresholds of p=.0028and
p=.0029, respectively). This supports our second observation that
explicit feature traces support comprehending feature interactions.
However, this is solely limited to annotations and does not signifi-
cantlyapplytothedecomposedcodeversion.Inaddition,wefound
significant differences between components and object-oriented
code for task4, supporting ourthird observation ( p<.001and a
correctedthreshold of p=.0031).
To summarize RQ1, our results indicate that explicit feature
traceability can have positive, but also negative, effects on
program comprehension. Still, we have to be careful with
interpretationsandmustconductindustrialstudies,forwhich
annotations seem to be more promising.
O1Understanding problems donot biasthe results.
Notrejected.
O2Explicittraces improve interaction comprehension.
Accepted twice forannotated code.
O3Decomposition hampers buglocalization.
Accepted once.
O4Annotations have noeffectonbuglocalization.
Notrejected.4.3RQ2:Efficiency
InTable 4, we display statistics about the times our participants
in each group needed to complete a task, regardless of correctness.
Weonlyconsideredparticipantsthatdidnotstateinterruptionsfor
atask(undisturbed).Nonetheless,wefoundfewextremeoutliers
where some participants worked for several hours on a single task.
Theseoutliersindicatethatthecorrespondingparticipanthadbeen
interrupted,butdidnotstateso. Inordertoaddresssuchextreme
cases, we removed entries that were more than twice above the
thirdquartileofeachtaskandgroup.Thisledtotheexclusionof22
data points from our analysis, the differencebetween undisturbed
andincludedparticipants in Table4.
Observation 5: Explicit feature traceability does not influence effi-
ciency.Theresultsdonot varyheavilybetweendifferentversions
of the code. Moreover, only the first task required considerably
more time compared to the others. This is rarely surprising, as our
participants had to get familiar with the code and its structure. For
all other tasks, all groups needed between 1.19 and 3.2 minutes
tocompleteatask,onaverage.Likewise,theminimumandmaxi-
mum timesare similarthroughoutalltasks. Thus,explicitfeature
traceability seems to have neither a positive nor a negative impact
on the analysis time, especially compared to the time needed to
familiarizewiththe code.
Hypothesis testing. We compared the completion time distri-
butions of our groups within each task with the Kruskal-Wallis
test [32]. Thistest does not require normal distributions and can
comparemultiplegroupsagainsteachother.The null-hypothesis ,
whichwe aimed torefute,wasthattherearenosignificantdiffer-
encesbetweenthecompletiontimes(cf. H2).Asnoneofthetests
resultedinap-valuebelow0.3,wecannotrejectournull-hypothesis
andarguethat our observationisreasonable.
To summarize RQ2, the results show no impact of explicit
featuretraceabilityonthecompletiontimes.Consideringthat
annotationsseemtoimproveprogramcomprehension,this
indicates an overallpositive effectof these.
O5Explicitfeature traces do not influence efficiency.
Notrejected.
4.4RQ3:Participants’ Perception
InTable 5, we summarize the qualitative responses we received
from our participants, which are particularly important to prac-
titioners[ 62].Partly,thenumbers donotaccumulate tothe total
number of participants, as we allowed each participant not to elab-
orateindetail;buttheycouldalsoprovidemultipleinsightswith
their response. We read all their comments and summarized the
mentioned analysis strategies, challenges, and opinions on code
designforeachgroup.Oursummarizingstrategyfollowedtheidea
ofopen-cardsorting[ 58].
Analysis strategies. Concerning their analysis strategy, many
participantsineachgroupstatedthattheystartedwithageneral
exploration of the source code (25). They aimed to understand
the structure of the code and its behavior on an abstract level.
How these tasks have been performed is quite different among the
participants:Somesimplyskimmedthroughthecodetogetarough
344ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Jacob Krüger, Gül Çalıklı, ThorstenBerger,Thomas Leich,andGunter Saake
Table 4:Statistics on thecompletiontimes(in minutes)ofour participants.
Task 1 Task 2 Task 3 Task 4 Task 5 Task 6
A C OO A C OO A C OO A C OO A C OO A C OO
Und. Part. 10 10 9 13 12 15 16 14 15 18 13 16 18 13 15 16 10 16
Incl. Part. 10 8 9 12 11 13 14 14 13 16 11 15 16 12 14 15 10 14
Times (mins)
Min2.91 2.23 2.72 0.44 1.14 0.91 0.70 0.67 0.52 0.38 0.66 0.61 1.63 1.47 0.57 0.61 1.30 0.76
Mean13.07 5.51 12.27 1.72 3.26 3.30 2.73 2.26 1.84 1.19 2.40 1.58 3.03 2.90 2.91 3.23 2.59 1.49
Median 11.23 4.03 9.75 1.06 2.63 2.09 2.04 2.11 1.68 1.07 1.79 1.21 2.66 2.54 2.28 3.20 2.50 1.23
Max25.02 12.73 22.92 4.90 8.48 11.96 7.29 4.70 3.90 2.33 6.37 4.09 6.84 5.95 7.55 8.82 5.05 3.48
SD8.34 3.59 7.54 1.43 2.34 3.14 1.78 1.30 0.89 0.52 2.01 1.00 1.45 1.37 2.01 2.16 1.19 0.75
Part.: Participants;Und.: Undisturbed;Incl.:Included;SD: StandardDeviation
Table5:Summaryofourparticipants’qualitativeresponses
concerning analysis strategies, challenges, and code design
(łśž meansnotapplicable).
Response#Mentioned
Annotations Components Object-Oriented
Participants 18 15 16
Analysis strategy
Getpicture ofcode 7 6 12
Lookforkeywords 4 2 8
Use search function 0 1 3
Follow annotations 8 ś ś
Follow class names ś 7 ś
Challenges
Code quality 9 6 6
Code length 7 0 1
Missing IDE 4 4 3
Feature location 2 0 3
Missing knowledge 1 3 1
Code design
Positive 14 9 ś
Unsure 2 2 ś
Negative 2 3 ś
Components 1 ś 5
Comments ś 0 4
Explicitlocations ś ś 3
understanding, while others focused on specific code constructs,
such as labels andmethods.
Unsurprisingly, 15 participants relied on the explicit feature
traces to address their tasks, if these were available. In some cases,
the participants mentioned that they also focused on keywords
(14), mostly to understand details, and used their browser’s search
function(4).Keywordsandsearcheswerealsoexplicitlymentioned
andusedbyparticipantsthatworkedontheobject-orientedcode.
Thisbehavioralignswiththeresultsofpreviousstudiesonmanual
feature location [ 25,63].
Challenges. Consideringchallenges,21participantsmentioned
quality issues of the code. Mostconcerns were connectedto design
decisions of our experiment that they did not like, for example,the (long) code length (8), missing comments, or inappropriate
identifiers. We specifically removed comments to avoid biases and
reduced the code size, but the code had to be large enough for
feature traces to be useful.
Other general concerns were the intentionally missing IDE sup-
port (11), avoiding too many biases that would make any meaning-
fulassessmentimpossible.Fiveparticipantsalsomentionedtheir
missingknowledgeaboutthesystemasaproblemthathampered
theircomprehension.However,thiswasalsointendedtohaveequal
preconditionsforeveryparticipant.Interestingly,notonlytwopar-
ticipants of the object-oriented group, but also two participants
oftheannotated grouphad problems to identify feature locations.
For example, in the annotated group, one participant indicated the
needfor decomposing features to avoid cluttering:
ł[T]hebiggestchallengefor mewasthatallofthefeatures are
inasingleplace,justwritten one after another.ž
Opinions. Concerningthefeaturetraceabilitytechniquesontheir
own, most of our participants stated a positive perception after the
experiment. For example, 14 out of 18 participants in the annotated
group argue that the annotations helped, some stating that they
were elementary to locate and understand featuresÐconflicting
somescientificbeliefs aboutannotations:
łYes, they did. In fact, without the annotations (provided that
theyarecorrect),itwouldhavebeensignificantlymoredifficult
to understandwhichpart of the code does what.ž
The few critics of annotations were not focusing on the actual
annotations, but arguethat comments indicatepoorcode:
ł[N]o, adding comments in the code is a bad sign, it screams
that code isnot self explanatory enough.ž
Similarly, nine of 15 participants stated a positive effect of de-
composing the system into features. Most participants stated that it
helpedto fastertrace features:
łIthelps[to] logical[ly]aid to decidewhere to start.ž
The negative experiences were connected to identifying which
featuretolookat.Suchissuesmainlyarosebecauseourparticipants
hadnot been familiar withthe system:
345Effects of Explicit Feature Traceability onProgram Comprehension ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
łYes, I understood the intent [...] with this sorting, naming and
separation. It was still unfamiliar and took more time than it
wouldhave withfamiliar code.ž
This indicates that decomposition has to be used carefully: The
right separation strategy is important and especially to new devel-
opers we have to explain how it is used. For developers who are
familiarwiththestructureofthecodeanditsfeatures,thisprob-
lem will arguably diminish. One participantspecifically explained
their experienced pros and cons of decomposition and may best
summarize our overallresults (i.e., RQ1):
łOn the one hand, it made the classes small and locating pos-
siblyrelevantcodeeasy.Ontheotherhand,interactionswere
more difficult to spot because I had to switch between different
classes.ž
For theobject-oriented group, we did not ask about the antici-
pated impact of the code design, but whether annotations or de-
composition wouldhave been helpful:
łFeatures could have been implemented in a more organized
way.[W]eclearlyneedmore thanone class here.ž
More precisely, four participants were in favor of decomposing the
code and five were in favor of adding comments (i.e., annotations)
to indicatefeature locations:
łMore comments and better restructuring of the code should be
more helpful.ž
Overall,11outof16participantsmentionedthatanyexplicitfeature
traces inthe code wouldhave been helpful.
To summarize RQ3, the results show that most of our par-
ticipants have a positive perception of explicit feature traces.
Thus, introducing traces in practice may not be a problem
andespeciallyannotationsare simpletoadopt.Condensing
the qualitative responses, we can derive three observations:
O6Explicitfeatures extend generalanalysisstrategies.
O7Feature traces themselves are unproblematic to use.
O8Making features explicit has apositive perception.
5 THREATS TO VALIDITY
The goalof our study was toprovide empiricalinsights into a fun-
damentaldesign decision based on studying experienced software
developersintherealworld.Duetothetrade-offsbetweeninternal
and external validity [ 56], and the magnitude of interacting fac-
torsthatimpactprogramcomprehension,wecanhardlyaddress
allbiasesÐresultinginmoreinternalthreats.Inthefollowing,we
reportthreatstothevalidityofourstudybasedontheguidelines
ofWohlin etal.[ 66].
ConstructValidity. Concerningtheconstructvalidityofourstudy,
some participants indicated that they had problems understanding
the survey or the concepts of annotations and decomposition to
separate features. To mitigate this threat, we provided small exam-
ples and used check questions to identify whether any confusions
occurred.Moreover,weperformedasanitycheckonthecorrect-
ness oftasks andfound no differences for participants whostated
comprehension problems. So, we argue that this threat is properly
addressedinourdesign.In addition,mostparticipantsstatedthatthey had problems with the code and not the experimental design,
meaningthat the constructvalidity wouldnot be threatened.
Internal Validity. We aimed to reduce the impact of different
development environments by using a web-interface to display the
code. Still, we kept identifier names as well as syntax highlighting,
and did not control for tool usage (e.g., searches). While we cannot
ensure that our participants conducted the experiment with the
exact same set-ups (e.g., noise level, using additional tools, web
searches),wearguethatdevelopersinreal-worldsettingsalsohave
amulti-foldoftools,environments,anddifferentcomprehension
patterns.Thus,theset-upmaybiasourresults,butreflectspractice.
Our code examples comprise different techniques to trace fea-
tures, namely annotations and decomposition. We relied on the ex-
istingpreprocessordirectivesintheoriginal MobileMedia system
to add our own annotations. For the decomposition, we separated
the corresponding code into different classes. Both techniques are
inspiredbytheusageofpreprocessorsinopen-sourceandindus-
trialsystems,whicharesimilarlystructured[ 17].Togetherwiththe
additionalchangesthatweappliedtothecode(i.e.,removingone
feature, deleting imports and comments), the nature of our code
exampleschanged.Suchchangesmayhaveinfluencedtheresults.
We did all changes in order to keep our participants motivated
and to control biases. Still, we cannot fully avoid this threat to our
study and, for example, another decomposition may have resulted
inbetterresults for our participants inthe corresponding group.
Aconcerninginternalthreatarelearningeffectsofourpartici-
pants, meaning that they may got more familiar with the code. We
addressedthisthreatintwoways:First,whileweusedasinglecode
example, we asked about different features for each task. This way,
ourparticipantsmayhaveachievedbetterunderstandingaboutthe
overallcode,butnotthespecificfeature.Mostoftheparticipants
also indicated that they did neither focus on nor did achieve an un-
derstandingoftheoverallcode,besidesageneraloverview.Second,
wedecidedagainstarandomorderofthesurveytasks.So,foreach
task, the experience with the source code should be comparable
between our participants. Based on this, we argue that learning
effects are mostly impacted by the different traceability techniques
for features, whichisthe concernof our research questions.
ExternalValidity. Softwaredevelopershavevariousbackgrounds,
expertise with a programming language, and experiences with cer-
taintasks.Toaddressthesethreats,weinvitedagroupofexperi-
encedsoftwaredevelopersfromseveralcountriesandorganizations.
Besidesmostofthemworkingonlargerprojectsforalongtime,we
also evaluated theirprogramming experience,based onwhichwe
randomly sampled them into equally distributed groups. While the
responsesresultedinthreenovicesbeingpartofthesamegroup,
theywereclosetoexpertlevel.Overall,ourparticipantsarearather
homogeneousgroupconsidering their experiences,wherefore we
arguethat such threatsare diminished,but mayhave occurred.
Severalbackgroundfactors,suchasage,gender,ormotivation
may have an impact on the results. Moreover, program compre-
hensioncomprisescognitiveprocessesthathighlydependonthe
individual developer, as they learn and understand based on differ-
ent patterns and rates. We aimed to address such factors partly by
measuring them (i.e., programming experiences) and by personally
invitingparticipants(e.g.,increasing motivation). Still, wecannot
346ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Jacob Krüger, Gül Çalıklı, ThorstenBerger,Thomas Leich,andGunter Saake
controlallofthesefactorsperfectly.Consequently,theyremaina
threatto the external validity ofour study.
Severalstudiesused MobileMedia toprovidecodeexamplesfor
empirical studies. The code is also designed to reflect a real-world
system, and thus we argue that our examples can be considered as
realistic.Nonetheless, MobileMedia isanacademicsystem,which
is why our results may not be completely transferable to industrial
practice.Still,ourparticipants’ activitiesduringprogramcompre-
hension will most likely not have changed, since such systems are
similarlystructuresinindustrial,open-source,andacademiccon-
texts [17,53]. In addition, all participants faced the same system
(independent variable), meaning that our analysis of feature traces
(dependent variable) remainsvalid.
Conclusion Validity. We have to be careful with the conclusions
wederivedfromour observations.While theyare interesting,our
statistical testsrevealedonly fewsignificantcorrelations. However,
duetotheproblemsofsuchtests[ 65],weonlyusedthemassup-
portive means and focused more on our actual observations. To
this end, we carefully investigated different variables and analyzed
their impact on program comprehension. This way, we aimed to
mitigate threatsto the conclusion validity.
Despitethediscussedthreats,wearguethatourstudyisvalid
and provides reliable and interesting insights into an important
design decision. We used quantitative and qualitative methods,
combining measured data with subjective responses and tested our
observationsstatistically.Still,weencourage otherresearchersto
conductfurtherstudiesinthisdirectiontostrengthentheempirical
evidenceandgaininsightsintotheimpactofexplicitfeaturetraces.
In this regard,we arguethat our study can be replicated.
6 CONCLUSION
In this paper, we reported an online experiment with 49 experi-
enced software developers concerning explicit feature traceability,
whichweimplementedbasedonannotationsanddecomposition.
Webasedourdesignonexistingstudiesandrecommendations,with
a particular focus on increasing the external validity of our results.
To this end, we invited especially practitioners from various coun-
triesandorganizations.Wereliedonquantitativeandqualitative
analysesto find indications for the following fourconclusions:
(1)Annotationspositivelyimpacttheeffectivenessofdevelop-
ers when comprehending features and their interactions,
whilenot negativelyimpactingbuglocalization.
(2)Decompositionintocomponentshasnosignificantimpact
on the effectiveness of developers when comprehending fea-
tures, but resulted in less correct bug localization. However,
thisisarguablyconnectedtothestructure,size,andcohesion
ofthe decomposedfeatures.
(3)Explicit feature traces do not impact the efficiency of devel-
opers duringprogram comprehension.
(4)Explicit feature traces do not result in comprehension prob-
lems and practitioners have a positive perception of such
explicit traces.
Weremarkthatthereareseveralthreatstoourresultsandwehighly
encouragefurtherstudies.However,wearguethat,especiallyfor
annotations, our results indicate that explicit feature traces can be
ahelpful meanstosupport program comprehension.In particular,this may be the case if developers are facing unfamiliar code. As
theyare alsosimpletointroduce,annotationsmaybeagoodway
for organizations to implement and test feature traceability as well
as for researchersto conduct further studiesinthis direction.
In future work, we aim to extend our analysis and focus on
additional variables, particularly extending our investigations to
programmers’memory.Moreover,weplantodesigndifferentexper-
imentsandobservationalstudiesthatmaximizeinternalorexternal
validity,includingcollaborationswithindustrialpartners.Thisway,
we can consolidate the empirical knowledge about explicit feature
traces, provide more precise recommendations to practitioners,
andidentifyopenresearchproblems.Furthermore,wearguethat
differenttracingtechniquesshouldbecomparedtoidentifywhat
impact they may have. Similarly, our study was focused on pro-
gramcomprehensiontasks.Inthefuture,wealsoaimtoanalyze
the impact of explicit feature traces on other activities that we did
notconsider,forexample,onmaintainingandevolvingasystem
(e.g.,introducing newfeatures).
ACKNOWLEDGMENTS
JacobKrügerwouldliketothankACMSIGSOFTforsupportingthe
presentationofthispaperwithaCAPSaward.GülÇalıklı’swork
issupported by theSEFISprojectfunded by ChalmersArea ofAd-
vance(ICT-SEED-2018).ThorstenBerger’sworkissupportedbythe
ITEAprojectREVaMP2fundedbyVinnovaSweden(2016-02804),
and by the Swedish Research Council Vetenskapsrådet (257822902).
Thomas Leich’s (LE 3382/2-1, LE 3382/2-3) and Gunter Saake’s (SA
465/49-1, SA 465/49-3) work is supported by the German Research
Foundation(DFG) projectEXPLANT.
WethankChristianLausbergerfortestingandadministrating
theexperiment;WardahMahmoodandYüceerÇalıklıfortesting
our tasks; and Sebastian Krieter for helping us troubleshoot our
R code. Moreover, we thank the anonymous reviewers for their
valuablefeedback,andespeciallyforpointingoutabuginourdata
transformation.Finally,we wouldliketothankallparticipantsof
our experiment for theirvaluable help.
REFERENCES
[1]Giuliano Antoniol, Gerardo Canfora, Gerardo Casazza, Andrea De Lucia, and
Ettore Merlo. 2002. Recovering Traceability Links Between Code and Documen-
tation.IEEE Transactions onSoftwareEngineering 28,10(2002), 970ś983.
[2]Sven Apel, Don Batory, Christian Kästner, and Gunter Saake. 2013. Feature-
OrientedSoftwareProductLines: Concepts and Implementation . Springer.
[3]Deborah J. Armstrong. 2006. The Quarks of Object-Oriented Development.
Communications ofthe ACM 49,2 (2006), 123ś128.
[4]ThorstenBerger,DanielaLettner,JuliaRubin,PaulGrünbacher,AdelineSilva,
Martin Becker, Marsha Chechik, and Krzysztof Czarnecki. 2015. What is a
Feature? A Qualitative Study of Features in Industrial Software Product Lines. In
InternationalConference onSoftwareProductLine (SPLC) . ACM,16ś25.
[5]Ted J.Biggerstaff,BharatG.Mitbander,and DallasWebster. 1993. TheConcept
AssignmentProblem inProgramUnderstanding.In InternationalConference on
SoftwareEngineering (ICSE) . IEEE,482ś498.
[6]Mauro Cherubini, Gina Venolia, Rob DeLine, and Andrew J. Ko. 2007. Let’s
Go to the Whiteboard: How and Why Software Developers Use Drawings. In
Conference onHuman Factors inComputingSystems (CHI) . ACM,557ś566.
[7]PaulClementsandLindaNorthrop.2001. SoftwareProductLines:Practicesand
Patterns. Addison-Wesley.
[8]Joseph W. Davison, Dennis M. Manci, and William F. Opdyke. 2000. Understand-
ing and Addressing the Essential Costs of Evolving Systems. Bell Labs Technical
Journal(2000).
[9]AlexanderDelater andBarbaraPaech.2013. TracingRequirementsandSource
Code During Software Development: An Empirical Study. In International Sym-
posiumonEmpiricalSoftwareEngineeringandMeasurement (ESEM) .IEEE,25ś34.
347Effects of Explicit Feature Traceability onProgram Comprehension ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia
[10]AlexanderEgyed,FlorianGraf,andPaulGrünbacher.2010. EffortandQuality
ofRecoveringRequirements-to-CodeTraces:TwoExploratoryExperiments.In
InternationalRequirements Engineering Conference (RE) . IEEE,221ś230.
[11]JanetFeigenspan,ChristianKästner,SvenApel,andThomasLeich.2009. Howto
Compare Program Comprehension in FOSD Empirically: An Experience Report.
InInternational Workshop on Feature-Oriented Software Development (FOSD) .
ACM,55ś62.
[12]Janet Feigenspan, Christian Kästner, Sven Apel, Jörg Liebig, Michael Schulze,
Raimund Dachselt,Maria Papendieck, ThomasLeich, and Gunter Saake. 2013.
Do Background Colors Improve Program Comprehension in the #ifdef Hell?
EmpiricalSoftwareEngineering 18,4 (2013), 699ś745.
[13]Ronald A. Fisher. 1936. Statistical Methods For Research Workers . Oliver and
Boyd.
[14]FlorianFittkau,SantjeFinke, WilhelmHasselbring,andJanWaller.2015. Com-
paring Trace Visualizations for Program Comprehension through Controlled
Experiments.In InternationalConferenceonProgramComprehension(ICPC) .IEEE,
266ś276.
[15]Johannes C. Hofmeister, Janet Siegmund, and Daniel V. Holt. 2019. Shorter
IdentifierNamesTakeLongertoComprehend. EmpiricalSoftwareEngineering
24,1 (2019), 417ś443.
[16]Sture Holm. 1979. A Simple Sequentially Rejective Multiple Test Procedure.
Scandinavian Journal ofStatistics (1979), 65ś70.
[17]Claus Hunsen, Bo Zhang, Janet Siegmund, Christian Kästner, Olaf Leßenich,
Martin Becker, and Sven Apel. 2016. Preprocessor-Based Variability in Open-
Source and Industrial Software Systems: An Empirical Study. Empirical Software
Engineering 21,2 (2016), 449ś482.
[18]Khaled Jaber, Bonita Sharif, and Chang Liu. 2013. A Study on the Effect of
TraceabilityLinksin SoftwareMaintenance. IEEE Access 1 (2013), 726ś741.
[19]Wenbin Ji, Thorsten Berger, MichalAntkiewicz, and Krzysztof Czarnecki. 2015.
MaintainingFeatureTraceabilitywithEmbeddedAnnotations.In International
Conference onSoftwareProductLine (SPLC) . ACM,61ś70.
[20]Yue Jia and Mark Harman. 2011. An Analysis and Survey of the Development of
Mutation Testing. Transactions onSoftwareEngineering 37,5 (2011), 649ś678.
[21]KyoC.Kang,SajoongKim,JaejoonLee,KijooKim,EuiseobShin,andMoonhang
Huh. 1998. FORM: A Feature-Oriented Reuse Method with Domain-Specific
ReferenceArchitectures. AnnalsofSoftwareEngineering 5,1 (1998), 143.
[22] Christian Kästner,SvenApel,andKlausOstermann.2011. TheRoadtoFeature
Modularity?. In International Software Product Line Conference (SPLC) . ACM,
5:1ś5:8.
[23]Sebastian Krieter, Jacob Krüger, and Thomas Leich. 2018. Don’t Worry About
It: Managing Variability On-The-Fly. In International Workshop on Variability
ModellingofSoftware-IntensiveSystems (VaMoS) . ACM,19ś26.
[24]JacobKrüger.2018. Separationof Concerns:ExperiencesoftheCrowd.In Sym-
posiumonAppliedComputing (SAC) . ACM,2076ś2077.
[25]Jacob Krüger, Thorsten Berger, and Thomas Leich. 2019. Features and How
to Find Them: A Survey of Manual Feature Location. In Software Engineering
forVariabilityIntensiveSystems:FoundationsandApplications .LLC/CRCPress,
153ś172.
[26]Jacob Krüger, Wanzi Gu, Hui Shen, Mukelabai Mukelabai, Regina Hebig, and
Thorsten Berger. 2018. Towards a Better Understanding of Software Features
and Their Characteristics: A Case Study of Marlin. In International Workshop on
VariabilityModellingofSoftware-IntensiveSystems (VaMoS) . ACM.
[27]Jacob Krüger, Kai Ludwig, Bernhard Zimmermann, and Thomas Leich. 2018.
PhysicalSeparationof Features: A Survey with CPP Developers.In Symposium
onAppliedComputing (SAC) . ACM,2042ś2049.
[28]Jacob Krüger, Mukelabai Mukelabai, Wanzi Gu, Hui Shen, Regina Hebig, and
Thorsten Berger. 2019. Where is my Feature and What is it About? A Case
Study on Recovering Feature Facets. Journal of Systems and Software 152 (2019),
239ś253.
[29]Jacob Krüger, Marcus Pinnecke, Andy Kenner, Christopher Kruczek, Fabian
Benduhn, Thomas Leich, and Gunter Saake. 2018. Composing Annotations
Without Regret? Practical Experiences using FeatureC. Software: Practice and
Experience 48,3 (2018), 402ś427.
[30]Jacob Krüger, Ivonne Schröter, Andy Kenner, Christopher Kruczek, and Thomas
Leich.2016. FeatureCoPP:CompositionalAnnotations.In InternationalWorkshop
onFeature-OrientedSoftwareDevelopment (FOSD) . ACM,74ś84.
[31]Jacob Krüger, Jens Wiemann, Wolfram Fenske, Gunter Saake, and Thomas Leich.
2018. Do You Remember this Source Code?. In International Conference on
SoftwareEngineering (ICSE) . ACM,764ś775.
[32]William H. Kruskal and W. Allen Wallis. 1952. Use of Ranks in One-Criterion
VarianceAnalysis. JournaloftheAmericanStatisticalAssociation 47,260(1952),
583ś621.
[33]ThomasD.LaToza,GinaVenolia,andRobert DeLine.2006. MaintainingMen-
tal Models: A Study of Developer Work Habits. In International Conference on
SoftwareEngineering (ICSE) . ACM,492ś501.
[34]Dawn Lawrie, Christopher Morrell, Henry Feild, and David Binkley. 2007. Effec-
tiveIdentifierNamesforComprehensionandMemory. InnovationsinSystemsand SoftwareEngineering 3,4 (2007), 303ś318.
[35]DucLe,EricWalkingshaw,andMartinErwig.2011. #ifdefConfirmedHarmful:
PromotingUnderstandableSoftwareVariation.In SymposiumonVisualLanguages
and Human-Centric Computing (VL/HCC) . IEEE,143ś150.
[36]Jörg Liebig, Sven Apel, Christian Lengauer, Christian Kästner, and Michael
Schulze.2010. AnAnalysisoftheVariabilityinFortyPreprocessor-BasedSoft-
wareProductLines.In InternationalConferenceonSoftwareEngineering (ICSE) .
ACM,105ś114.
[37]Patrick Mäder and Alexander Egyed. 2015. Do Developers Benefit from Require-
mentsTraceabilitywhenEvolvingandMaintainingaSoftwareSystem? Empirical
SoftwareEngineering 20,2 (2015), 413ś441.
[38]Jean Melo, Claus Brabrand, and Andrzej Wąsowski. 2016. How Does the De-
greeofVariabilityAffectBugFinding?.In InternationalConferenceonSoftware
Engineering (ICSE) . ACM,679ś690.
[39]SebastianNielebock,DariuszKrolikowski,JacobKrüger,ThomasLeich,andFrank
Ortmeier. 2019. Commenting Source Code: Is it Worth it for Small Programming
Tasks?EmpiricalSoftwareEngineering 24,3 (2019), 1418ś1457.
[40]Nan Niu, Wentao Wang, and Arushi Gupta. 2016. Gray Links in the Use of
RequirementsTraceability.In InternationalSymposiumonFoundationsofSoftware
Engineering (FSE) . ACM,384ś395.
[41]David L. Parnas. 1972. On the Criteria to Be Used in Decomposing Systems into
Modules. Communications ofthe ACM 15,12(1972), 1053ś1058.
[42]Leonardo Passos, Jesús Padilla, Thorsten Berger, Sven Apel, Krzysztof Czarnecki,
andMarcoTulio Valente.2015. Feature Scattering in the Large:A Longitudinal
Study of Linux Kernel Device Drivers. In International Conference on Modularity
(MODULARITY) . ACM,81ś92.
[43]Leonardo Passos, Rodrigo Queiroz, Mukelabai Mukelabai, Thorsten Berger, Sven
Apel, Krzysztof Czarnecki, and Jesus Padilla. 2018. A Study of Feature Scattering
in the Linux Kernel. IEEE Transactions on Software Engineering (2018). Preprint.
[44]ChristianPrehofer.1997.Feature-OrientedProgramming:AFreshLookatObjects.
InEuropeanConferenceonObject-OrientedProgramming (ECOOP) .Springer,419ś
443.
[45]R Core Team. 2018. R: A Language and Environment for Statistical Computing . R
Foundationfor StatisticalComputing. https://www.R-project.org
[46]Patrick Rempel and Parick Mäder. 2016. Preventing Defects: The Impact of
Requirements Traceability Completeness on Software Quality. IEEE Transactions
onSoftwareEngineering 43,8 (2016), 777ś797.
[47]MartinP.Robillard, Andrian Marcus, ChristophTreude,GabrieleBavota, Oscar
Chaparro, Neil Ernst, Marco AurélioGerosa,Michael Godfrey, Michele Lanza,
Mario Linares-Vásquez, Gail C. Murphy, Laura Moreno, David Shepherd, and
EdmundWong.2017. On-DemandDeveloperDocumentation.In International
Conference onSoftwareMaintenance and Evolution (ICSME) . IEEE,479ś483.
[48]Iran Rodrigues, Márcio Ribeiro, Flávio Medeiros, Paulo Borba, Baldoino Fon-
seca, and Rohit Gheyi. 2016. Assessing Fine-Grained Feature Dependencies.
Informationand SoftwareTechnology 78(2016), 27ś52.
[49]JuliaRubinandMarshaChechik.2013. ASurveyofFeatureLocationTechniques.
InDomain Engineering . Springer.
[50]AlcemirRodriguesSantos,IvandoCarmoMachado,EduardoSantanadeAlmeida,
JanetSiegmund,andSvenApel.2019. ComparingtheInfluenceofUsingFeature-
OrientedProgrammingandConditionalCompilationonComprehendingFeature-
Oriented Software. EmpiricalSoftwareEngineering 24,3 (2019), 1226ś1258.
[51]IvonneSchröter,JacobKrüger,JanetSiegmund,andThomasLeich.2017. Com-
prehendingStudiesonProgramComprehension.In InternationalConferenceon
ProgramComprehension (ICPC) . IEEE,308ś311.
[52]Kanwarpreet Sethi, Yuanfang Cai, Sunny Wong, Alessandro Garcia, and Claudio
Sant’Anna.2009. FromRetrospecttoProspect:AssessingModularityandStability
fromSoftwareArchitecture.In ConferenceonSoftwareArchitecture&European
Conference onSoftwareArchitecture (WICSA/ECSA) . IEEE,269ś272.
[53]Janet Siegmund, Christian Kästner, Jörg Liebig, and Sven Apel. 2012. Comparing
Program Comprehension of Physically and Virtually Separated Concerns. In
International Workshop on Feature-Oriented Software Development (FOSD) . ACM,
17ś24.
[54]JanetSiegmund,ChristianKästner,JörgLiebig,SvenApel,andStefanHanenberg.
2014. Measuring and Modeling Programming Experience. Empirical Software
Engineering 19,5 (2014), 1299ś1334.
[55]JanetSiegmundandJanaSchumann.2015. ConfoundingParametersonProgram
Comprehension:ALiteratureSurvey. EmpiricalSoftwareEngineering 20,4(2015),
1159ś1192.
[56]Janet Siegmund, Norbert Siegmund, and Sven Apel. 2015. Views on Internal and
External Validity in Empirical Software Engineering. In International Conference
onSoftwareEngineering (ICSE) . IEEE,9ś19.
[57]JaniceSinger,TimothyLethbridge,NormanVinson,and NicolasAnquetil. 2010.
AnExaminationofSoftwareEngineeringWorkPractices.In CASCONFirstDecade
HighImpactPapers (CASCON) . IBM,174ś188.
[58]DonnaSpencer.2009. CardSorting:DesigningUsableCategories .RosenfeldMedia.
[59]PeriTarr,HaroldOssher,WilliamHarrison,andStanleyM.Sutton,Jr.1999. NDe-
grees of Separation: Multi-Dimensional Separation of Concerns. In International
Conference onSoftwareEngineering (ICSE) . ACM,107ś119.
348ESEC/FSE ’19, August 26ś30, 2019,Tallinn,Estonia Jacob Krüger, Gül Çalıklı, ThorstenBerger,Thomas Leich,andGunter Saake
[60]RebeccaTiarks. 2011. WhatMaintenance ProgrammersReallydo: An Observa-
tionalStudy. In Workshop onSoftwareReengineering (WSR) .
[61]AnneliesevonMayrhauser,A.MarieVans,andAdeleE.Howe.1997. Program
Understanding Behaviour during Enhancement of Large-Scale Software. Journal
ofSoftwareMaintenance:Research and Practice 9,5 (1997), 299ś327.
[62]Ivonne von Nostitz-Wallwitz, Jacob Krüger, Janet Siegmund, and Thomas Leich.
2018. Knowledge Transfer from Research to Industry: A Survey on Program
Comprehension.In InternationalConferenceonSoftwareEngineering(ICSE) .ACM,
300ś301.
[63]Jinshui Wang, Xin Peng, Zhenchang Xing, and Wenyun Zhao. 2013. How Devel-
opersPerformFeatureLocationTasks:AHuman-CentricandProcess-Oriented
Exploratory Study. Journal of Software: Evolution and Process 25, 11 (2013), 1193ś
1224.[64]TianxiaWangandYanLiu.2017. Jsea:AProgramComprehensionToolAdopting
LDA-Based Topic Modeling. International Journal of Advanced Computer Science
and Applications 2,3 (2017).
[65]RonaldL.Wasserstein,AllenL.Schirm,andNicoleA.Lazar.2019. Movingtoa
WorldBeyond łp <0.05ž. The AmericanStatistician 73(2019), 1ś19.
[66]Claes Wohlin, Per Runeson, Martin Höst, Magnus C Ohlsson, Björn Regnell, and
AndersWesslén.2012. Experimentation inSoftwareEngineering . Springer.
[67]ScottN.Woodfield,HubertE.Dunsmore,andVincentYunShen.1981. TheEffect
of Modularization and Comments on Program Comprehension. In International
Conference onSoftwareEngineering (ICSE) . IEEE,215ś223.
[68]Trevor J.Young.2005. UsingAspectJtoBuildaSoftwareProductLineforMobile
Devices. Master’s thesis. Universityof BritishColumbia.
349