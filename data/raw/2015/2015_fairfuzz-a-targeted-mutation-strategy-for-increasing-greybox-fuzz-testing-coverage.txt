FairFuzz: A Targeted Mutation Strategy for Increasing Greybox
Fuzz Testing Coverage
Caroline Lemieux
University of California, Berkeley, USA
clemieux@cs.berkeley.eduKoushik Sen
University of California, Berkeley, USA
ksen@cs.berkeley.edu
ABSTRACT
In recent years, fuzz testing has proven itself to be one of the most
effective techniques for finding correctness bugs and security vul-
nerabilities in practice. One particular fuzz testing tool, American
Fuzzy Lop (AFL), has become popular thanks to its ease-of-use and
bug-finding power. However, AFL remains limited in the bugs it
can find since it simply does not cover large regions of code. If it
does not cover parts of the code, it will not find bugs there .W ep r o -
pose a two-pronged approach to increase the coverage achieved by
AFL.First,theapproachautomaticallyidentifiesbranchesexercised
by few AFL-produced inputs (rare branches), which often guard
code that is empirically hard to cover by naïvely mutating inputs.
The second part of the approach is a novel mutation mask creation
algorithm, which allows mutations to be biased towards produc-
ing inputs hitting a given rare branch. This mask is dynamically
computed during fuzz testing and can be adapted to other testing
targets. We implement this approach on top of AFL in a tool named
FairFuzz. We conduct evaluation on real-world programs against
state-of-the-art versions of AFL. We find that on these programs
FairFuzz achieves high branch coverage at a faster rate that state-
of-the-art versions of AFL. In addition, on programs with nested
conditional structure, it achieves sustained increases in branch cov-
erageafter24hours(average10.6%increase).Inqualitativeanalysis,
we find that FairFuzz has an increased capacity to automatically
discover keywords.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
fuzz testing, coverage-guided greybox fuzzing, rare branches
ACM Reference Format:
Caroline Lemieux and Koushik Sen. 2018. FairFuzz: A Targeted Mutation
Strategy for Increasing Greybox Fuzz Testing Coverage. In Proceedingsof
the 2018 33rd ACM/IEEE International Conference on Automated Software
Engineering(ASE’18),September3–7,2018,Montpellier,France. ACM, New
York, NY, USA, 11pages.https://doi.org/10.1145/3238147.3238176
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.32381761 INTRODUCTION
Fuzz testing has emerged as one of the most effective testing tech-
niques for finding correctness bugs and security vulnerabilities in
real-worldsoftwaresystems.Ithasbeenusedsuccessfullybymajor
software companies such as Microsoft [ 21] and Google [ 6,18,40]
for security testing and quality assurance. The success of coverage-
guidedgreyboxfuzzing (CGF)inparticularhasgainedattentionboth
in practice and in the research community [ 10,11,37,42,46]. One
of the leading CGF tools, American Fuzzy Lop (or simply AFL) [ 51],
has found vulnerabilities in a broad array of programs, including
Web browsers (e.g. Firefox, Internet Explorer), network tools (e.g.,
tcpdump, wireshark), image processors (e.g., ImageMagick, libtiff),
various system libraries (e.g., OpenSSH, PCRE), C compilers (e.g.,
GCC, LLVM), and interpreters (for Perl, PHP, JavaScript).
Coverage-guided greybox fuzzing is based on the observation
thatincreasingprogramcoverageoftenleadstobettercrashdetection.
The actual fuzzing process starts with a set of user-provided seed
inputs. It then mutates the seed inputs with byte-level operations.
It runs the program under test on the mutated inputs and collects
program coverage information. Finally, it saves the mutated inputs
which are interesting according to the coverage information—the
ones that discover new coverage. It continually repeats the process,
but starting with these interesting mutated inputs instead of the
user-provided inputs.
While many of the individual test inputs it generates may be
garbage,duetoitslowcomputationaloverhead,CGFgeneratestest
inputs much faster than more sophisticated methods such as sym-
bolic execution [ 17,34] and dynamic symbolic execution (a.k.a.
concolic testing) techniques [ 5,7,13,16,21,22,36,45,47]. In
practice, this trade-off has paid off, and CGF has found numer-
ous correctness bugs and security vulnerabilities in widely used
software [3, 10,11,37,46,51].
AlthoughthegoalofAFLandotherCGFtoolsistofindassertion
violations and crashes as quickly as possible, their core search
strategies are based on coverage feedback —AFL tries to maximize
the coverage it achieves. This is because there is no way to find
bugsorcrashesataparticularprogramlocationunlessthatlocationis covered by a test input. However, while experimenting with AFL
and its extensions, we observed that AFL often fails to cover key
program functionalities. For example, AFL did not cover colorspaceconversioncodein
djpeg,attributelistprocessingcodein xmllint ,
and a large variety of packet structures in tcpdump . Therefore,
AFL cannot be expected to find bugs in these functionalities. Put
succinctly, if AFL does not cover some program regions, it will not
find bugs in those regions.
We propose a lightweight technique, called FairFuzz, which
helps AFL achieve better coverage. This technique requires noex-
trainstrumentation beyond AFL’s regular instrumentation, unlike
475
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Caroline Lemieux and Koushik Sen
some other recently proposed fuzzing techniques [ 15,37,42,46].
Thus, the technique preserves AFL’s ease-of-use. While we focus
on branch coverage, our proposed technique could easily be modi-
fied for other kinds of coverage and testing objectives. FairFuzz is
based on a novel mutation strategy that increases the probability
of hitting the code locations that have been hit by few of AFL’s
previously-generated inputs.
FairFuzz works in two main steps. First, it identifies the pro-
gram branches that are rarely hit by previously-generated inputs.
We call such branches rare branches. These rare branches guard
under-explored functionalities of the program. By generating more
random inputs hitting these rare branches, FairFuzz greatly in-
creases the coverage of the parts of the code guarded by them.
Second,FairFuzzusesanovellightweightmutationtechniqueto
increasetheprobabilityofhittingtheserarebranches.Themutation
strategy is based on the observation that certain parts of an input
already hitting a rare branch are crucial to satisfy the conditions
necessary to hit that branch. Therefore, to generate more inputshitting the rare branch via mutation, the parts of the input that
are crucial for hitting the branch should not be mutated. FairFuzz
identifies these crucial parts of the input by performing a number
of small mutation experiments. Later, in test input generation, itavoids mutating these crucial parts of the input. This mutationstrategy is orthogonal to approaches that try to increase crashes
found by AFL by helping AFL pass magic byte checks [ 37,46]o rb y
customizing power schedules [ 10,11] and can be combined with
them to simultaneously increase code coverage and the number of
bugs or crashes discovered.
We implemented our technique, FairFuzz, on top of AFL. We
evaluated FairFuzz against three popular versions of AFL. Wecompare to AFLFast [
11], an extension of AFL which prioritizes
inputsthathitrarepathsthroughtheprogram,butdoesnotchangethemutationtechniquesofAFL.Weconductourevaluationonnine
real-world benchmarks, including those used to evaluate AFLFast.
We repeat our experiments and provide measures of variability
whencomparingtechniquesinourevaluation.Wefindthatonthese
benchmarksFairFuzzachieveshighbranchcoverageatafasterrate
that state-of-the-art versions of AFL (average 6.0% increase after 1
hour,and3.5%increaseafter5hours).Priorworkdemonstratesthatafteracertainamountofcoverageisachieved,evensmallincreasesinbranchcoveragecanyieldmorebugfindingpower[
7].FairFuzz
has a stronger advantage on programs with nested conditional
structure,obtainingsustainedincreasesinbranchcoverage(average
10.6%increaseafter24hours),and,accordingtoqualitativecoverage
analysis, better discovering complex keywords.
In summary, we make the following contributions:
•We propose a novel lightweight mutation masking strategy
to increase the chance of hitting the program regions thatare missed by previously generated inputs. We describe amethod to implement mutation masking which smoothly
integrates with the usual fuzz testing mutation procedure.
•We develop an open-source1implementation of mutation
masking targeted to rare branches on top of AFL, named
FairFuzz.
1https://github.com/carolemieux/afl-rbAlgorithm 1 AFL algorithm.
1:procedure FuzzTest(Prog, Seeds)
2:Queue←Seeds
3:whiletruedo ⊿begin a queue cycle
4:forinputinQueuedo
5: if¬isWorthFuzzing(input )then
6: continue
7: score←performanceScore(Prog,input )
8: for0≤i<|input|do
9: formutation indeterministicMutationTypes do
10: newinput←mutate(input, mutation, i )
11: runAndSave(Prog, newinput, Queue )
12: for0≤i<scoredo
13: newinput←mutateHavoc(input )
14: runAndSave(Prog, newinput, Queue )
15:procedure mutateHavoc(Prog, input )
16:numMutations←randomBetween(1,256)
17:newinput←input
18:for0≤i<numMutations do
19:mutation←randomMutationType
20:position←randomBetween(0 ,|newinput|)
21:newinput←mutate(newinput, mutation, position)
22:returnnewinput
23:procedure runAndSave(Prog, input, Queue )
24:runResults←run(Prog, input )
25:ifnewCoverage(runResults )then
26:addToQueue(input, Queue )
•We perform evaluation of FairFuzz against different state-
of-the-art versions of AFL on real-world benchmarks.
We detail the general method and implementation of FairFuzz
inSection 3andtheperformanceresultsinSection 4.Wewillbegin
in Section 2with a more detailed overview of AFL, its current
limitations, and how to overcome these with our method.
2 OVERVIEW
Our proposed technique, FairFuzz, is built on top of AmericanFuzzy Lop (AFL) [
51]. AFL is a popular greybox mutation-based
fuzz tester. Greybox fuzz testers [ 3,51] are designated as such since,
unlikewhitebox fuzz testers [ 13,21,22,44], they do not do any
source code analysis, but, unlike pure blackbox fuzz testers [ 30],
theyuselimitedfeedbackfromtheprogramundertesttoguidetheir
fuzzing strategy. Next we give a brief description of AFL, illustrate
one of its limitations, and motivate the need for FairFuzz.
2.1 AFL Overview
To fuzz test programs, AFL generates random inputs. However,
instead of generating these inputs from scratch, it selects a set of
previouslygeneratedinputsandmutatesthemtoderivenewinputs.
The overall AFL fuzzing algorithm is given in Algorithm 1.
The fuzzing routine takes as input a program and a set of user-
provided seed inputs . The seed inputs are used to initialize a queue
(Line2) of inputs. AFL goes through this queue (Line 4), selects an
input to mutate (Line 5), mutates the input (Lines 10,13), runs the
476
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing CoverageASE ’18, September 3–7, 2018, Montpellier, France
program on and, simultaneously, collects the coverage information
for the mutated inputs (Line 24), and finally adds these mutated
inputs to the queue if they achieve new coverage (Line 26). An
entire pass through the queue is called a cycle. Cycles are repeated
(Line3) until the fuzz testing procedure is stopped by the user.
AFL’s mutation strategies assume the input to the program un-
der test is a sequence of bytes, and can be treated as such during
mutation. AFL mutates inputs in two main stages: the deterministic
(Algorithm 1, Lines8-11) stages and the havoc(Lines12-14) stage.
All the deterministic mutation stages operate by traversing the
input under mutation and applying a mutation at each position
in this input. These mutations (Line 9) include bit flipping, byte
flipping, arithmetic increment and decrement of integer values, re-
placing of bytes with “interesting” integer values (0, MAX_INT ), etc.
The number of mutated inputs produced in each of these stages is
governed by the length of the input being mutated (Line 8). On the
other hand, the havoc stage works by applying a sequence of ran-
dom mutations—setting random bytes to random values, deleting
orcloningsubsequencesoftheinput—totheinputbeingmutatedto
produce a new input. Several mutations are applied to the original
input(Line 21)beforerunningitthroughtheprogram(Line 14).The
numberoftotalhavoc-mutatedinputstobeproducedisdetermined
by a performance score, score(Line7).
2.2 AFL Coverage Calculation
Above, we mentioned the role that coverage information plays in
the AFL procedure. The use of this information is one of AFL’skey innovations. Specifically, AFL uses this information to selectinputs for mutation and save new inputs, saving only those that
have achieved new program coverage. In order to collect this cov-
erage information efficiently, AFL inserts instrumentation into the
program under test. To track coverage, it first associates each basic
block with a random number via instrumentation. The randomnumber is treated as the unique ID of the basic block. The basic
block IDs are then used to generate unique IDs for the transitions
between pairs of basic blocks. In particular, for a transition from
basic block AtoB, AFL uses the IDs of each basic block— ID(A)and
ID(B), respectively—to define the ID of the transition, ID(A→B),
as follows:
ID(A→B)def= (ID(A)/greatermuch1)⊕ID(B).
Right-shifting (/greatermuch) the basic block ID of the transition start block
(A) ensures that the transition from AtoBhas a different ID from
the transition from BtoA. We associate the notion of basic block
transitionwiththatofa branchintheprogram’scontrolflowgraph,
and throughout the paper we will use the term branch to refer to
this AFL-defined basic block transition unless stated otherwise.
The coverage of the program under test on a given input is
collected as a set of pairs of the form ( branchID, branchhits ). If a
(branchID, branchhits )pairispresentinthecoverageset,itdenotes
that during the execution of the program on the input, the branch
with IDbranch ID was exercised branch hits number of times. The
hits are bucketized to small powers of two. AFL refers this set of
pairs as the pathof an input. AFL says that an input achieves new
coverage if it discovers a new (branch ID, branch hits ) pair.2.3 Limitations of AFL
While AFL’s search strategy is guided by coverage, we observed
in our experiments that often AFL fails to cover some important
functionalities of the program under test. Note that achieving good
coverage is precursor to finding bugs and crashes—if a program
region is not covered, there is no way AFL can find bugs or crashes
in that region.
ConsiderthecodefragmentshowninFigure 1.Itisadaptedfrom
theparser.c file used in libxml2 ’sxmllint utility. AFL found
manysecurityvulnerabilitiesinthislibraryinthepast[ 51].Weran
AFL on this benchmark for 24 hours, repeating this experiment 20
times (see Section 4for more experimental details). Only in one of
these 24-hour runs did AFL produce an input passing Line 1.E v e n
then, AFL failed to explore the contents of any of the if statements
in Lines6-30. As such, it failed to explore the large quantity of code
after Line 31(mostly omitted in Figure 1). Since this code is not
evencovered, then AFL simply cannot find any bugs in it.
The key reason AFL is unable to produce inputs covering any of
this code—even after discovering an input containing <!ATTLIST —
is that AFL mutates bytes paying no attention to which byte values
are required to cover particular parts of the program. For exam-
ple, after having produced the input <!ATTLIST BD , AFL will not
prioritize mutation of the bytes after <!ATTLIST . Instead, it is as
likely to produce the mutants <!CATLIST BD ,<!!ATTLIST BD ,o r
???!ATTLIST BD as it is to produce <!ATTLIST ID .H o w e v e r ,t o
explore the code in Figure 1, once AFL discovers <!ATTLIST BD ,i t
should not mutate the <!ATTLIST part of this input. To see why,
suppose that the production of an input like <!ATTLIST ID —with
the token “ ID”—is required to pass the processing code omitted
in Line 3 of Figure 1. Preventing the modification of <!ATTLIST
increasesAFL’sprobabilityofgenerating <!ATTLIST ID byatleast
6×. Figure2illustrates how restricting mutation to only the last
two characters of the input yields to a smaller space of mutants to
explore, and thus, a higher probability of discovering an input that
will get deeper into the program.
2.4 Overview of FairFuzz
We propose a two-pronged approach that addresses this concern
but can be smoothly integrated into AFL or other mutation-based
fuzzers. It works as follows.
The first part of our approach is the identification of statements
like the if statement in Line 1 of Figure 1, which potentially guard
large unvisited regions of code. For this, we utilize the observation
that such statements are usually hit by very few of AFL’s gener-
ated inputs (i.e. they are rare), and can thus be easily identified by
keeping the track of the number of inputs which hit each branch.
Intuitively,thecodeguardedbyabranchhitbyfewinputsismuch
lesslikelytohavebeenthoroughlyexploredthanthecodeguardedby
a branch hit by a huge percentage of generated inputs.
Having identified these rare branches for targeted fuzzing, we
modify the input mutation strategy in order to keep the condition
of the rare branch satisfied. Specifically, we use a deterministic
mutation phase to approximately determine the parts of the input
that cannot be mutated for mutants to hit the rare branch. The
subsequent mutation stages are then not allowed to mutate these
crucial parts of the input. As a result, we significantly increase the
477
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Caroline Lemieux and Koushik Sen
1if(CMP9(ptr,'<','!','A','T','T','L','I','S','T')) {
2ptr+= 9;
3/* some processing code omitted */
4 while((ptr!='>')& &(ptr!=EOF)){
5 inttype=0 ;
6 if(CMP5(ptr,'C','D','A','T','A')){
7 ptr+= 5;
8 type=XML_ATTRIBUTE_CDATA ;
9}else if (CMP6(ptr,'I','D','R','E','F','S')){
10 ptr+= 6;
11 type=XML_ATTRIBUTE_IDREFS \;
12}else if (CMP5(ptr,'I','D','R','E','F')){
13 ptr+= 5;
14 type=XML_ATTRIBUTE_IDREF ;
15}else if ((ptr=='I')& &( ( ptr+1)=='D')){
16 ptr+= 2;
17 type=XML_ATTRIBUTE_ID ;
18}else if (CMP6(ptr,'E','N','T','I','T','Y')){
19 ptr+= 6;
20 type=XML_ATTRIBUTE_ENTITY ;
21}else if (CMP8(ptr,'E','N','T','I','T','I','E','S')){
22 ptr+= 8;
23 type=XML_ATTRIBUTE_ENTITIES ;
24}else if (CMP8(ptr,'N','M','T','O','K','E','N','S')){
25 ptr+= 8;
26 type=XML_ATTRIBUTE_NMTOKENS ;
27}else if (CMP7(ptr,'N','M','T','O','K','E','N')){
28 ptr+= 7;
29 type=XML_ATTRIBUTE_NMTOKEN ;
30}
31 if(type== 0) { ptr++; break;}
32
33/* more omitted code */
3435
if(CMP9(ptr,'#','R','E','Q','U','I','R','E','D')) {
36 ptr+= 9;
37 default_decl =XML_ATTRIBUTE_REQUIRED ;
38}
39 if(CMP8(ptr,'#','I','M','P','L','I','E','D')) {
40 ptr+= 8;
41 default_decl =XML_ATTRIBUTE_IMPLIED ;
42}
43 if(CMP6(ptr,'#','F','I','X','E','D')) {
44 ptr+= 6;
45 default_decl =XML_ATTRIBUTE_FIXED ;
46 if(!IS_BLANK_CH (ptr)) {
47 xmlFatalErrorMsg ("Space required after '#FIXED'" );
48 }
49}
50 ptr++;
51}
52}
Figure 1: Code fragment based off the libxmlfileparser.c
showingmanynestedifstatementsthatmustbesatisfiedto
explore erroneous behavior.
probability of generating new inputs that hit the rare branch. This
opens up the possibility of better exploring the part of the code
that is guarded by the branch. While we apply it to targeting rare
branches, this mutation modification strategy is general and can be
applied to other testing targets.
Weimplement thisapproachontopofAFLinFairFuzz.Wefind
thisapproachleadstofastercoverage,aswellasincreasedcoverage
compared to the maximum coverage achieved, over stock AFL and
other modified versions of AFL on several real-world benchmarks.
The details of the evaluation are presented in Section 4. We present
the details of our approach in the next section.<!ATTLIST BD <!ATTLIST BD
 1-character mutants  1-character mutantsmutable region 
without maskmutable region 
with mask
Figure 2: Preventing AFL from mutating the <!ATTLIST
part of this input increases the probability of generating
<!ATTLIST ID by at least 6×.
3 FAIRFUZZ ALGORITHM
In FairFuzz, we modify the AFL algorithm in two key ways toincrease the coverage achieved. First, we modify the selection of
inputs to mutate from the queue, and second, we modify the way
mutations are performed on these inputs. Algorithm 2outlines the
FairFuzz algorithm and how it differs from the AFL algorithm. We
beginwithanabstracttreatmentofthemutationmaskingtechnique,
and then dive into the FairFuzz particulars.
3.1 Mutation Masking
In this section we introduce the mutation mask for a given input, x,
and a given testing target, T. We saysatisfies (x,T)is true if input
xsatisfies T.
Definition1. Amutation is a tuple (c,m), where mis the number
of bytes impacted by the mutation and cis one of the following
mutation categories:
O:overwrites mbytes starting at position kwith some values,
I:insertssome sequence of mbytes at position k,
D:deletes mbytes starting at position k.
To fully specify mutations with c∈{O,I}, the values that are
inserted or written over existing bytes must be specified. Given an
input x, a mutation μ=(c,m), and a position i∈[0,|x|−m), let
mutate (x,μ,i)denote the input produced by applying mutation μ
onxat position i.
Definition2. Themutationmask foraninput xandatestingtarget
Tis a function maskx,T:N→P ({O,I,D})which takes a position
iin the input xand returns a subset of {O,I,D}. We say that a
mutation category c∈maskx,T(i)ifsatisfies (mutate (x,(c,1),i),T)
is true. That is, if cis in the set maskx,T(i), then after applying a
mutation of category cat position ionx, the resulting input will
satisfy the target T.
Intuitively, the mutation mask specifies whether the input pro-
duced from mutating xat position iwill (likely) reach the testing
target. With this mask, given a mutation μ=(c,m)at position k,
we can compute
okToMutate (maskx,T,μ,k)=k+m−1/logicalanddisplay
i=kc∈maskx,T(i).
We describe the algorithm to compute maskx,T(i)in Sec-
tion3.2.2.
478
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing CoverageASE ’18, September 3–7, 2018, Montpellier, France
Algorithm2 FairFuzz algorithm. Differences from the AFL algo-
rithm are highlighted in gray.
1:procedure FairFuzz(Prog,Seeds )
2:Queue←Seeds
3:whiletruedo ⊿begin a queue cycle
4:forinputinQueuedo
5: rarestBranch←rarestHitBy (Prog, input,numHits )
6: ifnumHits[rarestBranch] >rarity_cutoff then
7: continue
8: score←performanceScore(Prog, input )
9: mask←computeMask(Prog, input, rarestBranch )
10: for0≤i<|input|do
11: formutation indeterministicMutationTypes do
12: if¬okToMutate (mask, mutation, i )then
13: continue
14: newinput←mutate(input, mutation, i )
15: runAndSave(Prog,newinput, Queue )
16: for0≤i<scoredo
17: newinput←mutateHavoc(input )
18: runAndSave(Prog, newinput, Queue )
19:procedure mutateHavoc(Prog, input )
20:numMutations←randomBetween(1,256)
21:newinput←input
22:for0≤i<numMutations do
23:mutation←randomMutationType
24:position←randomOkToMutate(mask, mutation )
25:newinput←mutate(newinput, mutation, position)
26:returnnewinput
27:procedure runAndSave(Prog, input, Queue)
28:runResults←run(Prog, input)
29:forbranchinrunResults do
30:numHits[branch] +=1
31:ifnewCoverage(runResults) then
32: addToQueue(input, Queue)
3.1.1 Biasing Mutation with the Mutation Mask. FairFuzz uses
okToMutate (maskx,T,μ,k)to bias mutations towards the testing
target as follows.
In the deterministic mutation stages (Algorithm 2, Line10-15),
okToMutateisusedtofilteroutmutantsthatcouldviolatethetest-
ingtarget,asillustratedinLine 12ofAlgorithm 2.Inparticular,fora
givenmutationtype μandposition i,ifokToMutate (maskx,T,μ,i)
is true, the mutant x/prime=mutate (x,μ,i)is generated and passed to
runAndSave. Otherwise FairFuzz skips the mutant.
Recallthatduringthehavocstage,mutantsarecreatedbychoos-
ing a random mutation and random position at which to applyit. FairFuzz selects the random mutation
μ=(c,m)as AFL does
(Algorithm 2, Line23). However, instead of selecting the position
at random between 0 and |newinput|−m−1, as in Algorithm 1,
Line20, FairFuzz chooses the position randomly from the subset
of ok-to-mutate positions (Algorithm 2, Line24). Precisely, the call
torandomOkToMutate (maskx,T,μ)in Line24of Algorithm 2is:
sampleUniform ({i∈[0,|x|−m−1] : okToMutate (maskx,T,μ,i)}).If the set of ok-to-mutate positions is empty, FairFuzz skips themutation in Line 25and chooses a new
μ= (c,m)at the next
iteration of the havoc mutation inner loop (Line 22).
3.2 Targeting Rare Branches
So far we have kept the testing target abstract. In this section, we
concretize it by elaborating the definition of rare branches and
giving the concrete algorithm which FairFuzz uses to compute the
mutation mask for rare branches.
3.2.1 Selecting Inputs to Mutate. To bias input generation towards
rare branches, FairFuzz selects only inputs that hit rare branches
for mutation. First, we formalize the concept of a rare branch.
Definition 3. We say that an input xhitsa branch b, denoted
hits (x,b), if the execution of the program on xexercises the branch
bat least once.
Thehit count of a branch is the number of produced inputs i
which have exercised the branch. More formally,
Definition 4. LetIbe the set of all inputs produced by fuzzing
so far. The hit count of branch bis
numHits[ b]=|{x∈I:hits (x,b)}|.
Concretely, numHits is kept as a map of branches to hit count,
updated every time a mutant is run (Line 30of Algorithm 2). To
establish numHits , FairFuzz runs one round of mutation on the
seed input with no masking.
A natural idea is to designate the nbranches hit by the fewest
inputsasrare,orthebrancheshitbylessthan ppercentofinputsto
be rare. After some initial experiments, we rejected these methods
as (a) they can fail to capture what it means to be rare (e.g. if n=5
and the two rarest branches are hit by 20 and 15,000 inputs, both
would be “rare”), and (b) these thresholds need to be modified for
differentbenchmarks.Instead,wedefineararebranchasonewhose
hit count is smaller than a dynamic rarity cutoff as follows. Let B
be the set of all branches in the program.Definition 5.
LetBv={b∈B:numHits[b]>0}.Arare branch
is a branch bsuch that
numHits[ b]≤rarity_cutoff
where
rarity_cutoff =2isuch that 2i−1<min
b/prime∈Bv(numHits[ b/prime])≤2i.
For example, if the branch hit by the fewest inputs has been hit
by 17 inputs, any branch hit by ≤25inputs is rare.
To determine whether an inputs hits a rare branch, FairFuzz
computes the rarest branch hit by the input:
Definition 6. Letbranches (x)={b∈B:hits (x,b)}. Then the
rarest branch hit by input xis the branch b∗such that
b∗=argmin
b∈branches (x)numHits[ b].
Then, FairFuzz selects only inputs whose rarest branch is a rare
branch for mutation (Line 6of Algorithm 2).
Although FairFuzz only selects inputs using the above strategy,
it could run some of the cycles using AFL’s default strategy. This
would ensure that it does not skip the default strategies of AFL,
which might be better for creating crash-prone inputs.
479
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Caroline Lemieux and Koushik Sen
Algorithm 3 Computing the mutation mask in FairFuzz.
1:procedure computeMask(Prog, input, branch )
2: mask←initWithEmptySet (|input|)
3:for0≤i<|input|do
4: inputO←mutate(input, flipByte, i )
5: ifbranch∈branchesHitBy(Prog, inputO )then
6: mask[i]←mask[i]∪{O}
7: inputI←mutate(input, addRandomByte, i )
8: ifbranch∈branchesHitBy(Prog, inputI )then
9: mask[i]←mask[i]∪{I}
10: inputD←mutate(input, deleteByte, i )
11: ifbranch∈branchesHitBy(Prog, inputD )then
12: mask[i]←mask[i]∪{D}
return mask
3.2.2 Computation of the Mutation Mask. Algorithm 3outlines
how FairFuzz computes maskx,bfor a given input xand rare
branch b. The algorithm works as follows.
For each position iin the x, FairFuzz produces the mutants xO
by flipping the byte at position i(Line4of Algorithm 3),xIby
adding a random byte at position i(Line7), and xDby deleting the
byte at position i(Line10). Then, for each xc, FairFuzz determines
whetherhits (xc,b)by running xcthrough the program (captured
in branchesHitBy on Lines 5,8,11). Finally, if xchitsb, FairFuzz
notes the position ias overwritable (O), insertable (I), or deletable
(D), respectively (Lines 6,9,12). While the calculation is illustrated
as separate from the deterministic mutation stages in Algorithm 2,
the two are integrated in the implementation. Since the mask com-
putation adds only two new deterministic mutation types to AFL
(byte-flipping is a default mutation type), the computation adds
negligible overhead to stock AFL.
Of course, this computation of O∈maskx,b(i)and I∈
maskx,b(i)is approximate—FairFuzz doesn’t check whether every
value overwritten or inserted results in bbeing hit. Unfortunately,
trying all possible values to insert or write is too expensive and
produces too many redundant inputs. Empirically we find this ap-
proximation produces an effective mutation mask (see Section 4.2).
Finally, note that this algorithm could be easily adapted to other
testing targets by replacing hits (xc,b)withsatisfies (xc,T).
3.3 Trimming Inputs for Testing Targets
AFL’s efficiency depends on large part on its ability to quickly
produce and modify inputs [ 54]. Thus, it is important to make sure
the deterministic mutation stage—and in FairFuzz, mutation mask
computation—is efficient. Since the runtime of the computation is
linear in the length of the selected input, FairFuzz needs to keep
the length of the inputs in the queue short. AFL has two techniques
forkeepinginputsshort:(1)prioritizingshortinputswhenselecting
inputs for mutation and (2) trimming (an efficient approximation
of delta-debugging [ 55]) the parent input before mutating it. This
trimming is omitted from Algorithms 1and2for clarity. Trimming
attempts to minimize the input to mutate with the constraint that
the minimized input hits the same path (set of ( branchID, branch
hits))astheun-minimizedone.Ho wever,this constraintisnotgood
enough for reducing the length of inputs when very long inputsare chosen. FairFuzz may do this since it selects inputs based only
on whether they hit a rare branch. We found that we can make
inputs shorter in spite of this if we relax the trimming constraint.
In particular, we relax the constraint to require that the minimized
inputhitsonlythetargetbranchoftheoriginalinput,insteadofthe
same path as the original input. Similar relaxation could be done
for other testing targets. We refer to FairFuzz with this relaxed
constraint as FairFuzz with trimming.
4 IMPLEMENTATION AND EVALUATION
We implemented FairFuzz as an open source tool built on top of
AFL. The implementation adds around 600 lines of C code to the
file containing AFL’s core implementation.
We evaluated FairFuzz on 9 different real-world benchmarks.
We selected these from those favored for evaluation by the AFL
creator (djpegfrom libjpeg-turbo-1.5.1, and readpng from libpng-
1.6.29), those used in AFLFast’s evaluation ( tcpdump -nr from
tcpdump-4.9.0; and nm,objdump -d ,readelf -a , andc++filt
from GNU binutils-2.28) and a few benchmarks with more complex
input grammars in which AFL has previously found vulnerabilities
(mutool draw from mupdf-1.9, and xmllint from libxml2-2.94).
Since some of these input formats had AFL dictionaries and some
did not, we ran all the evaluation without dictionaries to level out
the playing field. In each case we seeded the fuzzing run with the
inputs in the corresponding AFL testcases directories (except
c++filt , which was seeded with the input “ _Z1fv\n ”); for PNG
we used only not_kitty.png.
4.1 Coverage Compared to Prior Techniques
In this section of evaluation we compare three popular versions of
AFL against FairFuzz, all based off of AFL version 2.40b.
(1) “AFL” is the vanilla AFL available from AFL’s website.(2) “FidgetyAFL” [52] is AFL run without deterministic mutations.(3)
“AFLFast.new” [ 9] is AFLFast run without deterministic stage
and with the cut-off-exponential exploration strategy.
Configurations (2) and (3) are the fastest-performing versions of
AFL and AFLFast, respectively. We compare to 1) for baseline refer-
ence. We ran FairFuzz with input trimming for the testing target
and omitting all deterministic stages except those necessary to
compute the mutation mask.
We ran each technique for 24 hours (on a single core) on each
benchmark.Werepeatedeach24hourexperiment20timesforeach
benchmark. We ran our experiments for 24 hours as the fuzzing
process does not have a defined end-time and this is a runtime
used in prior work [ 11,37,42,46]. We repeated our experiments
20 times because fuzz testing is an inherently non-deterministic
process, and so is its performance. This enabled us to report results
that are statistically significant in Section 4.1.1.
4.1.1 OverallBranchCoverageAchieved. We begin by analyzing
coverage achieved by different techniques through time. The main
metricwereportisbasicblocktransitionscovered,whichiscloseto
the notion of branch coverage used in real-world software testing.
Why branch coverage? Other than basic block transitions (i.e.,
branches)covered,theonlyothercommonlyusedmetrictoevaluate
AFL coverage is AFL path coverage. As mentioned in Section 2.2,
480
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing CoverageASE ’18, September 3–7, 2018, Montpellier, France
(a) tcpdump
 (b) readelf
 (c) nm
(d) objdump
 (e) c++filt
 (f) xmllint
(g) mutool draw
 (h) djpeg
 (i) readpng
Figure3:Numberofbasicblocktransitions(AFLbranches)coveredbydifferentAFLtechniquesaveragedover20runs(bands
represent 95% C.I.s).
an AFL path is set of (branch ID, branch hits ) But due to AFL’s
implementation, only the branches covered metric is robust to the
orderinwhichinputsarediscovered.Hereisasimpleillustrationof
whytheAFLpathcoverageisorder-dependent.Consideraprogramwith two branches,
b1andb2. Suppose input Ahitsb1once, input B
hitsb2once, and input Chits both b1andb2. Their respective paths
arepA={(b1,1)},pB={(b2,1)}, and pC={(b1,1),(b2,1)}. If AFL
discovers these inputs in the order A,B,Cit will save both AandB
and count 2 paths, and not save Csince it does not exercise a new
(branchID, branchhits ) pair. On the other hand, if AFL discovers
the inputs in the order C,A,B, it will save Cand count 1 path, and
save neither AnorB. Thus it appears on the second run that AFL
has found half the paths it did on the first run. On the other hand,
regardless of the order in which inputs A,B,Care discovered, the
numberofbranchescoveredwillbe2.Reportingrealpathcoverage
is only possible if all inputs produced by AFL were saved, but this
was not tractable in our 24-hour experiments given the volume of
inputsproduced(tensofmillions).Thus,wereportbranchcoverage.
We also noted that the creator of AFL also favors branch coverage
(which he refers to as “tuple resolution”) as a performance metric,statinghehasfounditisthebestpredictorofhowAFLwillperform
“in the wild” [53].
Results.Figure3plots, for each benchmark and technique, the
average number of branches covered over all 20 runs at each time
point (dark central line) and 95% confidence intervals in branches
covered at each time point (shaded region around line) over the 20
runs for each benchmark. For the confidence intervals we assume
Student’s t distribution (taking 2 .0860 times the standard error).
From Figure 3, we see that that on all benchmarks except
c++filt , FairFuzz achieves the upper bound in branch coverage,
generally showing the most rapid increase in coverage at the be-
ginning of execution.
Note that while FairFuzz keeps a sizeable lead on the xmllint
benchmark (Figure 3f), it does so with wide variability. Closer anal-
ysis reveals that one run of FairFuzz on xmllint was buggy, and
no inputs were selected for mutation—this run covered no more
than 6160 branches. However, FairFuzz had two runs on xmllint
covering an exceptional 7969 and 10990 branches, respectively.
Figure4, shows, at every hour, for how many benchmarks each
technique has the leadin coverage. By leadwe mean its average
481
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Caroline Lemieux and Koushik Sen
Higher is better
Figure 4: Number of benchmarks on which each technique
has the lead in coverage at each hour. A benchmark iscounted for multiple techniques if two techniques are tiedfor the lead.
coverage is above the confidence intervals of the other techniques,
and no other technique’s average lies within its confidence interval.
We say two techniques are tied if one’s average lies within the
confidence interval of the other. If techniques tie for the lead, the
benchmark is counted for both techniques in Figure 4, which is
why the number of benchmarks at each hour may add up to more
than 9. This figure shows that FairFuzz quicklyachieves alead in
coverage on nearly all benchmarks and is not surpassed in coverage
by the other techniques in our time limits.
4.1.2 Detailed Analysis of Coverage Differences. Figure3shows
there are three benchmarks ( c++filt ,tcpdump , andxmllint )o n
which one technique achieves a statistically significant lead in
AFL’s branch coverage after 24 hours (with AFLFast.new leading
onc++filt and FairFuzz on the other two). We were curious as
to what these branch coverage increases corresponded to in terms
of source code coverage differences.
Since AFL saves all inputs that achieve new program coverage
(i.e. that are placed in the queue) to disk, we can replicate what pro-
gram coverage was achieved in each run by replaying these queue
elements through the programs under test. Since each benchmark
wasrun20times,wetaketheunion(overeachtechnique)ofinputs
in the queue for all 20 runs. We ran the union of the inputs foreach technique through their corresponding programs and thenran
lcovon the results to reveal coverage differences. Using the
union is a generous approach and can only reveal which regions
are uncoverable by the different techniques over all the 20 runs.
xmllint.Thebulkofthecoveragegainson xmllint wereinthe
mainparser.c file. The key trend in increased coverage appears
to be FairFuzz’s increased ability to discover keywords.
For example, both AFL and FairFuzz have higher source code
coverage than FidgetyAFL and AFLFast.new as they discovered the
patterns<!DOCTYPE and<!ATTLIST in at least one run. However,
FairFuzz also produced inputs satisfying all the other conditionals
illustrated in Figure 1, which meant discovering all the keywords
used in the comparisons. The produced inputs included:
<!DOCTYPET@[ <!ATTLIST?D T NMTOKENS
<!DOCTYPE?[ <!ATTLIST D T ENTITY
<!DOCTYPE\[ <!ATTLISTíD T ID #REQUIREDˆ@ˆP
We believe the mutation masking technique is directly respon-
sible for the discovery of these. To see this, let us focus on theTable 1: Number of runs, for each technique, producing an
input with the given sequence in 24 hours.
sequence AFL FidgetyAFL AFLFast.new FairFuzz
<!A 71 5 1 8 1 7
<!AT 12 3 1 1
<!ATT 10 0 1
<!ATTLIST block covered by the inputs above, whose code is out-
linedininFigure 1.WhilebothAFLandFairFuzzhadarundiscov-
ering the sequence <!ATTLIST , of all the saved inputs for AFL in
that run, only 0.4% of them (18) visited Line 2 of Figure 1, resulting
in 18 hits of the line. In contrast, we found that 12.3% (1169) of the
saved inputs produced by FairFuzz in the run where it discovered
<!ATTLIST visited Line 2 of Figure 1, resulting 2124hits of the line.
With two orders of magnitude more hits of this line, it is obvious
that FairFuzz was better able to explore the code in Figure 1.W e
believe the orders of magnitude difference can be attributed to the
mutation mask.
To confirm the effect was not just luck, we also look at the num-
ber of runs which produced subsequences of <!ATTLIST . This is
illustrated in Table 1. The decrease in the number of runs discov-
ering<!ATfrom the number of runs discovering <!Ain this table
shows the mutation mask in action, with 11 of FairFuzz’ runs dis-
covering<!AT,comparedto1,2,and3forAFLFast.new,FidgetyAFL,
and AFLFast.new, respectively.
Finally, as is obvious from the example inputs above, although
FairFuzz discovered more keywords, the inputs it produced were
not necessarily more well-formed. Nonetheless, these inputs al-
lowed the FairFuzz to explore more of the program’s faults. This
is reflected in the coverage of a large case statement differentiating
57 error messages in parser.c . These messages do not result in
AFL “crashes” (i.e. segmentation faults), simply in non-zero exit
codes. Both FidgetyAFL and AFLFast.new cover only 22 of these
cases, AFL covers 33, and FairFuzz covers 39.
tcpdump.Likexmllint ,tcpdump hasextensivenestedstructure,
with the presence of various sequences in the input leading to
differentprintingfunctions.Weobservedthatcoveragefor tcpdump
differs a bit for all four techniques over a variety of different files,
but see the biggest gains in three files printing certain packet types
(print-forces.c, print-llc.c, and print-snmp.c).
The coverage gains in these files suggest FairFuzz is better
able to automatically detect sequences in the inputs necessary to
increase program coverage. For example, unlike the other three
techniques,FairFuzzwasabletocreatefilesthathavelegalForCES
(RFC 5810) packet length. FairFuzz was also able to create IEEE
802.2 Logical Link Control (LLC) packets with the organization-ally unique identifier (OUI) corresponding to RFC 2684, and sub-
sequently explore the many subtypes of this OUI. Finally, in the
Simple Network Management Protocol parser, FairFuzz was able
tocreateinputscorrespondingtoTrapPDUs,aswellassomeinputs
with a correct SNMPv3 user-based security message header.
We note these gains in coverage seem less impressive than those
of FairFuzz on xmllint , even though the performance in Figure 3
looks similar. This appears to be because FairFuzz gets consis-
tently higher coverage of tcpdump instead of covering parts of
482
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing CoverageASE ’18, September 3–7, 2018, Montpellier, France
the program wholly uncoverable by the other techniques. We can
see this by looking at the number of branches covered byatleast
one ofthe 20 runs (the union over the runs) and the number of
branches covered atleastonceinall the 20 runs (the intersection
over the runs). For tcpdump , FairFuzz has a consistent increase in
the intersection of coverage (FairFuzz’s contains 11,293 branches
compared to AFLFast.new’s 10,724), but a smaller gain in the union
(FairFuzz’s is 16,129, while AFLFast.new’s is 15,929). On the other
hand,theintersectionofcoveragefor xmllint isvirtuallythesame
for all techniques except stock AFL (5,876 for FidgetyAFL, 5,778
for AFLFast.new and 5,884 for FairFuzz, maybe because of thebuggy run mentioned in Section 4.1.1), but FairFuzz’s union of
coverage (11,681) contains over 4,000 more branches than that of
AFLFast.new (7,222).
c++filt.The differences in terms of source code coverage be-
tween techniques were much more minimal for c++filt than
fortcpdump orxmllint . For example, FairFuzz covers 3 lines
incp-demangle.c that AFLFast.new does not, related to deman-
gling binary components when the operator has a certain op-code. On the other hand, AFLFast.new covers a branch where
xmalloc_failed(INT_MAX) is called when a length bound com-
parison fails, while FairFuzz fails to produce an input long enough
to violate the length bound. FairFuzz also fails to cover a branch
incxxfilt.c taken when the length of input read into c++filt
surpasses the length of the input buffer allocated to store it.
FairFuzz’s inability to produce very long inputs may be related
to the second round of trimming FairFuzz does. Or, it could be
becausec++filt has highly recursive structure, so full branch
coverage is not as good a exploration heuristic for this program. A
testing target other than hitting rare branches may be better suited
for programs like c++filt.
The pattern we see from this analysis is that FairFuzz is better
able to automatically discover input constraints and keywords—
special sequences, packet lengths, organization codes—and target
exploration to inputs which satisfy these constraints than the other
techniques. We suspect the gains in coverage speedon benchmarks
such asobjdump ,readpng , andreadelf are due to similar factors.
We conjecture the targeting of rare branches shines the most in the
tcpdump andxmllint benchmarks since these programs are struc-
tured with many nested constraints, which the other techniques
are unable to properly explore over the time budget (and perhaps
even longer) without extreme luck.
4.2 Can Masking Effectively Target Branches?
Finally, we were curious as to whether the mutation mask strategy
effectively biased mutation towards our testing target. In FairFuzz,the target was hitting the same rare branch as the parent input. Weconductedthefollowingexperimentonasubsetofourbenchmarks
to evaluate the effect of the mask.
We added a shadowmode to FairFuzz. When running in shadow
mode, every time an input is selected for mutation, FairFuzz first
performs all mutations without the influence of the mutation mask
(theshadowrun). Then, for the same input, FairFuzz performs all
mutations again, using the mutation mask filtering and bias.
This shadow run allows us to compute the difference between
the percentage of generated inputs hitting the target with andTable 2: Average % of mutated inputs hitting target branch
for one queueing cycle.
(a) Cycle without trimming.
det. mask det. plain hav. mask hav. plain
xmllint 92.8% 46.5% 31.8% 6.6%
tcpdump 99.0% 74.0% 34.2% 9.3%
c++filt 97.6% 64.1% 41.4% 14.4%
readelf 99.7% 82.7% 57.7% 14.9%
readpng 99.1% 34.6% 24.3% 2.4%
objdump 99.2% 70.2% 42.4% 9.0%
(b) Cycle with trimming.
det. mask det. plain hav. mask hav. plain
xmllint 90.3% 22.9% 32.8% 2.9%
tcpdump 98.7% 72.8% 36.1% 9.0%
c++filt 96.6% 14.8% 34.4% 1.1%
readelf 99.7% 78.2% 55.5% 11.4%
readpng 97.8% 39.0% 24.0% 2.4%
objdump 99.2% 66.7% 46.2% 7.6%
without the mutation mask foreachparentinput. Since some target
branches may be easier to hit than others, this gives us a betteridea of how effective the masking technique is in general. In our
experiments, we ran FairFuzz with the shadow run on a subset of
our benchmarks. For each benchmark we ran a cycle with target
branch trimming and one without.
Our results are presented in Table 2, which shows the target
branchhitpercentagesforthedeterministicandhavocstages.These
percentages are the averages—over all inputs selected for mutation
in the first queueing cycle—of the percentage of children inputs
hitting the target.
Overall, Table 2shows that the mutation mask largely increases
the percentage of mutated inputs hitting the target branch. The hit
percentages for the deterministic stage are strikingly high. This
is not unexpected because in the deterministic stage the mutation
mask simply prevents mutations at locations likely to violate the
target branch. Thus, the gain percentage of inputs hitting the tar-
get branch in the havoc stage is most impressive. In spite of theuse of the mutation mask in the havoc stage being heuristic, weconsistently see the use of the mutation mask causing a 3x-10x
increase in the percentage of inputs hitting the target branch. As
for trimming, it appears that extra trimming reduces the number of
inputshittingthetargetbranchwhenthemutationmaskisdisabled
but has minimal effect when the mutation mask is enabled.
Again, we note that the mutation masking technique is indepen-
dent of the testing target. In particular, the fact that the branches
being targeted are “rare”. This suggests that this strategy could be
used in a more general context. For example, we could target only
the branches within a function that needs to be tested, or, if some
area of the code was recently modified or bug-prone, we could
target the branches in that area with the mutation mask. Recent
work on targeted AFL [ 10] shows promise in such an application of
AFL, and we believe the mutation masking technique could be used
cooperatively with the power schedules presented in that work.
483
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Caroline Lemieux and Koushik Sen
1if(strcmp(str,"BAD!")) {
2// do bad things
3}1if(str[0] == 'B'){
2 if(str[1] == 'A'){
3 if(str[2] == 'D'){
4 if(str[3] == '!'){
5// do bad things
6}}}}
Figure 5: Multi-byte comparison (left) unrolled to byte-by-
byte comparison (right).
5 DISCUSSION
While we chose benchmarks with a variety of input formats ac-
complishing different tasks, the results of our evaluation may not
generalize to other programs.
Theforemostlimitationofusingrarebranchesasatestingtarget
in FairFuzz is the fact that branches that are never hit by any AFL
inputcannotbetargetedbythismethod.So,itconferslittlebenefits
to discovering a single long magic number when progress towards
matching the magic number does not result in new coverage—e.g.,
the comparison on the left of Figure 5. We believe the FairFuzz
mutation masking algorithm could be used in conjunction withmethods targeting the magic number issue [
37,42,46] to build a
more effective fuzzer.
Recall FairFuzz was effective at finding keyword sequences
in thexmllint benchmark. This may be because the long string
comparisons in parser.c useCMPnmacros (see Figure 1), which
are structured as byte-by-byte comparisons. So, AFL’s instrumen-
tation reports new coverage when progress was made on these
comparisons.Thecreatorsof laf-intel [1]proposeseveralLLVM
“deoptimization” passes to improve AFL’s performance, including a
pass that automatically turns multi-byte comparisons into byte-by-
byte comparisons. Figure 5shows an example of this comparison
unrolling. The integration of these LLVM passes into AFL’s in-
strumentation is straightforward, requiring only a patch to AFL’s
LLVM-based instrumenter [ 2]. Due to FairFuzz’s performance on
xmllint , we believe FairFuzz could show similar coverage gains
on other programs if they were compiled with this laf-intel pass.
We did not evaluate this as the evaluation of the laf-intel pass
wasdoneinAFL’s“parallel”fuzzingmode.Wedidnotdoanyexper-
imentswiththisparallelfuzzingasourimplementationdidnothave
a distributed version of the rare branch computation algorithm.
6 OTHER RELATED WORK
We have discussed AFLFast [ 11] in the previous sections. Unlike
our proposed approach, it targets rare paths, not branches, and it
does not change the mutation strategies of AFL. Other prior work
on AFL has focused on producing a single input passing a difficult
tohitbranch[ 37,46],liketheoneontheleftofFigure 5.Driller[ 46]
uses symbolic execution to pass branches when AFL gets stuck.
Steelix [37], whose source code was unavailable at the time of sub-
mission, adds a static analysis stage, extra instrumentation, andmutations to AFL to better produce inputs satisfying multi-byte
comparisons. These techniques require more instrumentation but
find magic numbers more accurately than FairFuzz. Ho wever, nei-
ther of these techniques will be able to prevent a discovered magic
sequence from being mutated to encourage further exploration,
while FairFuzz does.
Unlike FairFuzz and other greybox fuzzers [ 3] which use cov-
erage information as a heuristic for which inputs may yield newcoverage under mutation, symbolic execution tools [ 13,22,44,45]
methodically explore the program under test by capturing path
constraints and directly producing inputs which fit yet-unexploredpath constraints. The cost of this precision is that it can lead to the
path explosion problem, which causes scalability issues.
Traditional blackbox fuzzers such as zzuf [ 30] mutate user-
provided seed inputs according to a mutation ratio, which may
need to be adjusted to the program under test. BFF [ 33] and Sym-
Fuzz [14] adapt this parameter automatically, by measuring crash
density and doing input bit dependence, respectively. These opti-
mizations are not relevant to AFL-type fuzzers which do not use
this mutation ratio parameter.
There exist several fuzzers highly optimized for certain input
file structures, including network protocols [ 4,12], and source
code[31,43,49].FairFuzzisofmuchlowerspecificitysowillnotbe
as effective as these tools on these specific input formats. However,
its method is fully automatic, requiring neither user inputs [ 4]o r
extensive tuning [43, 49].
While FairFuzz uses its mutation mask to try and fix important
parts of program inputs, recent work has more explicitly tried to
automatically learn input formats. Learn&Fuzz [ 23] uses sequence-
based learning methods to learn the structure of PDF objects, Au-
togram [ 32] proposes a taint analysis-based approach to learning
input grammars, while Glade [ 8] uses an iterative approach and
repeated calls to an oracle to learn a context-free grammar for a
set of inputs. FairFuzz does not assume a corpus of valid inputs of
anysizefromwhichvaliditycouldbeautomaticallylearned,noran
oracle for the grammar, but consequently does not learn as precise
a grammar of the input.
Another approach to smarter fuzzing is to find locations in seed
inputs related to likely crash locations in the program and focusmutation there [
20,26,48] and TaintScope [ 48]. These directed
methods are not directly comparable to FairFuzz since they do not
have the same goal of increasing program coverage. VUzzer [ 42]
uses both static and dynamic analysis to get immediate values and
input positions used in comparisons. It uses a Markov Chain modeltodecidewhichpartsoftheprogramshouldbetargeted,asopposed
to our empirical approach.
Randoop [ 41] automatically generates test cases for object
oriented-programs through feedback-directed random test gen-eration. Evosuite [
19] uses seed inputs and genetic algorithms to
achieve high code coverage. Both these techniques focus on gener-
atingsequencesofmethodcallstotestprograms,notbyte-sequence
inputs for a given program like FairFuzz.
Search-based software testing (SBST) [ 24,25,27–29,35,38,39,
50] uses optimization techniques such as hill climbing and genetic
algorithmstogenerateinputsthatoptimizesomeobservablefitness
function. These techniques work well when the fitness curve is
smooth with respect to changes in the input, which is not the case
in coverage-based greybox fuzzing.
ACKNOWLEDGMENTS
This research is supported in part by NSF grants CCF-1409872
and CCF-1423645. Thanks to Rohan Padhye, Kevin Laeufer, and the
anonymous reviewers for their extensive feedback on this paper.
484
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing CoverageASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1] 2016. laf-intel. https://lafintel.wordpress.com/. Accessed August 23rd, 2017.
[2]2016. laf-tintel source. https://gitlab.com/laf-intel/laf-llvm-pass. Accessed
August 24th, 2017.
[3]2016. libFuzzer. http://llvm.org/docs/LibFuzzer.html. Accessed August 25th,
2017.
[4]Pedram Amini and Aaron Portnoy. 2012. Sulley.
https://github.com/OpenRCE/sulley. Accessed August 22nd, 2017.
[5]Saswat Anand, Corina S. Păsăreanu, and Willem Visser. 2007. JPF-SE: a symbolic
execution extension to Java PathFinder. In ProceedingsofToolsandAlgorithms
for the Construction and Analysis of Systems (TACAS).
[6]Abhishek Arya and Cris Neckar. 2012. Fuzzing for Security.
https://blog.chromium.org/2012/04/fuzzing-for-security.html.
[7]Thanassis Avgerinos, Alexandre Rebert, Sang Kil Cha, and David Brumley. 2014.
Enhancing Symbolic Execution with Veritesting. In Proceedings of the 36th Inter-
national Conference on Software Engineering (ICSE 2014). ACM, New York, NY,
USA, 1083–1094.
[8]Osbert Bastani, Rahul Sharma, Alex Aiken, and Percy Liang. 2017. Synthesizing
Program Input Grammars. In Proceedingsofthe 38thACMSIGPLAN Conference
on Programming Language Design and Implementation (PLDI 2017).
[9] Marcel Böhme. 2016. AFLFast.new. https://groups.google.com/d/msg/afl-users/
1PmKJC-EKZ0/lbzRb8AuAAAJ. Accessed August 23rd, 2017.
[10]Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoud-
hury. 2017. Directed Greybox Fuzzing. In Proceedingsofthe2017ACMSIGSAC
Conference on Computer and Communications Security (CCS ’17).
[11]Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-
basedGreyboxFuzzingAsMarkovChain.In Proceedingsofthe2016ACMSIGSAC
Conference on Computer and Communications Security (CCS ’16).
[12]Sergey Bratus, Axel Hansen, and Anna Shubina. 2008. LZfuzz:afastcompression-
basedfuzzerforpoorlydocumentedprotocols. Technical Report. Department of
Computer Science, Darmouth College.
[13]Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: Unassisted and
Automatic Generation of High-coverage Tests for Complex Systems Programs.
InProceedings of the 8th USENIX Conference on Operating Systems Design and
Implementation (OSDI’08).
[14]Sang Kil Cha, Maverick Woo, and David Brumley. 2015. Program-Adaptive
Mutational Fuzzing. In Proceedingsofthe2015IEEESymposiumonSecurityand
Privacy (SP ’15).
[15]Peng Chen and Hao Chen. 2018. Angora: Efficient Fuzzing by Principled Search.
InProceedings of the 39th IEEE Symposium on Security and Privacy.
[16]VitalyChipounov,VolodymyrKuznetsov,andGeorgeCandea.2012. TheS2EPlat-form: Design, Implementation, and Applications. ACMTransactionsonComputer
Systems. 30, 1 (2012), 2.
[17]Lori A. Clarke. 1976. A program testing system. In Proc. of the 1976 annual
conference. 488–491.
[18]Chris Evans, Matt Moore, and Tavis Ormandy. 2011. Fuzzing at Scale. https:
//security.googleblog.com/2011/08/fuzzing-at-scale.html. Accessed August 24th,
2017.
[19]Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: Automatic Test Suite Gen-eration for Object-oriented Software. In Proceedings of the 19th ACM SIGSOFT
Symposiumandthe13thEuropeanConferenceonFoundationsofSoftwareEngi-
neering (ESEC/FSE ’11).
[20]VijayGanesh,TimLeek,andMartinRinard.2009. Taint-basedDirectedWhitebox
Fuzzing.In Proceedingsofthe31stInternationalConferenceonSoftwareEngineering
(ICSE ’09).
[21]Patrice Godefroid, Adam Kiezun, and Michael Y. Levin. 2008. Grammar-based
Whitebox Fuzzing. In Proceedingsofthe29thACMSIGPLANConferenceonPro-
gramming Language Design and Implementation (PLDI ’08).
[22]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI ’05).
[23]Patrice Godefroid, Hila Peleg, and Rishabh Singh. 2017. Learn&Fuzz: Machine
Learning for Input Fuzzing. CoRR(2017).http://arxiv.org/abs/1701.07232
[24]Mark Grechanik, Chen Fu, and Qing Xie. 2012. Automatically finding perfor-
mance problems with feedback-directed learning software testing. In 201234th
International Conference on Software Engineering (ICSE). IEEE, 156–166.
[25]Mark Grechanik, Qing Xie, and Chen Fu. 2009. Maintaining and evolving GUI-
directed test scripts. In 2009 IEEE 31st International Conference on Software Engi-
neering. IEEE, 408–418.
[26]IstvanHaller,AsiaSlowinska,MatthiasNeugschwandtner,andHerbertBos.2013.
Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations. In
Proceedings of the 22Nd USENIX Conference on Security (SEC’13).
[27]Mark Harman. 2007. The current state and future of search based software
engineering. In 2007 Future of Software Engineering. IEEE Computer Society,
342–357.[28]MarkHarmanandJohnClark.2004. Metricsarefitnessfunctionstoo.In Software
Metrics, 2004. Proceedings. 10th International Symposium on. IEEE, 58–69.
[29]Mark Harman and Bryan F Jones. 2001. Search-based software engineering.
Information and software Technology 43, 14 (2001), 833–839.
[30]Sam Hocevar. 2007. zzuf. http://caca.zoy.org/wiki/zzuf/. Accessed August 22nd,
2017.
[31]Christian Holler, Kim Herzig, and Andreas Zeller. 2012. Fuzzing with Code
Fragments. In Presented as part of the 21st USENIX Security Symposium (USENIX
Security 12).
[32]Matthias Höschele and Andreas Zeller. 2016. Mining Input Grammars from
Dynamic Taints. In Proceedings of the 31st IEEE/ACM International Conference on
Automated Software Engineering (ASE 2016).
[33]Allen D. Householder and Jonathan M. Foote. 2012. Probability-Based Parameter
SelectionforBlack-BoxFuzzTesting. TechnicalReport.CarnegieMellonUniversity
Software Engineering Institute.
[34]James C. King. 1976. Symbolic execution and program testing. Commun.ACM
19 (July 1976), 385–394. Issue 7.
[35]Bogdan Korel. 1990. Automated software test data generation. IEEE Transactions
on software engineering 16, 8 (1990), 870–879.
[36]Guodong Li, Indradeep Ghosh, and Sreeranga P. Rajan. 2011. KLOVER: A Sym-
bolic Execution and Automatic Test Generation Tool for C++ Programs. In CAV.
609–615.
[37]Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu,
andAlwenTiu.2017. Steelix:Program-stateBasedBinaryFuzzing.In Proceedings
of the 2017 11th Joint Meeting on Foundations of Software Engineering (ESEC/FSE
2017).
[38]Phil McMinn. 2011. Search-Based Software Testing: Past, Present and Future.
InProceedingsof the2011 IEEEFourthInternational Conferenceon SoftwareTest-
ing,VerificationandValidationWorkshops (ICSTW’11). IEEE Computer Society,
Washington, DC, USA, 153–163. https://doi.org/10.1109/ICSTW.2011.100
[39]Webb Miller and David L. Spooner. 1976. Automatic generation of floating-point
test data. IEEE Transactions on Software Engineering 2, 3 (1976), 223.
[40]Max Moroz and Kostya Serebryany. 2016. Guided in-process fuzzing of Chrome
components. https://security.googleblog.com/2016/08/guided-in-process-fuzzing-
of-chrome.html.
[41]CarlosPachecoandMichaelD.Ernst.2007. Randoop:Feedback-directedRandom
Testing for Java. In Companion to the 22nd ACM SIGPLAN Conference on Object-
oriented Programming Systems and Applications Companion (OOPSLA ’07).
[42]Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuffrida,and Herbert Bos. 2017. VUzzer: Application-aware Evolutionary Fuzzing. In
Proceedingsofthe2017NetworkandDistributedSystemSecuritySymposium(NDSS
’17).
[43]Jesse Ruderman. 2015. jsfunfuzz. https://github.com/MozillaSecurity/funfuzz/
tree/master/js/jsfunfuzz.
[44]Koushik Sen and Gul Agha. 2006. CUTE and jCUTE: Concolic Unit Testing
and Explicit Path Model-checking Tools. In Proceedings of the 18th International
Conference on Computer Aided Verification (CAV’06).
[45]KoushikSen,DarkoMarinov,andGulAgha.2005. CUTE:AConcolicUnitTestingEngine for C. In Proceedingsofthe10thEuropeanSoftwareEngineeringConference
Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of
Software Engineering (ESEC/FSE-13).
[46]Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In
Proceedingsofthe2016NetworkandDistributedSystemSecuritySymposium(NDSS
’16).
[47]NikolaiTillmannandJonathandeHalleux.2008. Pex-WhiteBoxTestGeneration
for .NET. In Proceedings of Tests and Proofs.
[48]Tielei Wang, Tao Wei, Guofei Gu, and Wei Zou. 2010. TaintScope: A Checksum-
Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection. In
Proceedings of the 2010 IEEE Symposium on Security and Privacy (SP ’10).
[49]Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and Un-derstanding Bugs in C Compilers. In Proceedings of the 32nd ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI ’11).
[50]Shin Yoo and Mark Harman. 2007. Pareto efficient multi-objective test case
selection. In Proceedings of the 2007 international symposium on Software testing
and analysis. ACM, 140–150.
[51]Michał Zalewski. 2014. American Fuzzy Lop. http://lcamtuf.coredump.cx/afl.
Accessed August 18th, 2017.
[52]Michał Zalewski. 2016. FidgetyAFL. https://groups.google.com/d/msg/afl-users/
fOPeb62FZUg/CES5lhznDgAJ. Accessed August 23rd, 2017.
[53]Michał Zalewski. 2016. Unique crashes as a metric. https://groups.google.com/d/
msg/afl-users/fOPeb62FZUg/LYxgPYheDwAJ. Accessed August 24th, 2017.
[54]Michał Zalewski. 2017. American Fuzzy Lop Technical Details. http://lcamtuf.
coredump.cx/afl/technical_details.txt. Accessed August 18th, 2017.
[55]Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and Isolating Failure-
Inducing Input. IEEE Transactions on Software Engineering 28, 2 (2002), 183–200.
485
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. 