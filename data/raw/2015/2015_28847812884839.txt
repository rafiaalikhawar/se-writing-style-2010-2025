Cross-project Defect Prediction Using a
Connectivity-based Unsupervised Classiï¬er
Feng Zhang1, Quan Zheng1, Ying Zou2, and Ahmed E. Hassan1
1School of Computing, Queenâ€™s University , Canada
2Department of Electrical and Computer Engineering, Queenâ€™s University , Canada
1{feng, quan, ahmed}@cs.queensu.ca,2ying.zou@queensu.ca
ABSTRACT
Defect prediction on projects with limited historical data
has attracted great interest from both researchers and prac-titioners. Cross-project defect prediction has been the main
area of progress by reusing classiï¬ers from other projects.
However, existing approaches require some degree of ho-mogeneity (e.g., a similar distribution of metric values) be-tween the training projects and the target project. Satisfy-ing the homogeneity requirement often requires signiï¬canteï¬€ort (currently a very active area of research).
An unsupervised classiï¬er does not require any training
data, therefore the heterogeneity challenge is no longer anissue. In this paper, we examine two types of unsupervised
classiï¬ers: a) distance-based classiï¬ers (e.g. ,k-means); and
b) connectivity-based classiï¬ers. While distance-based un-supervised classiï¬ers have been previously used in the de-
fect prediction literature with disappointing performance,connectivity-based classiï¬ers have never been explored be-
fore in our community.
We compare the performance of unsupervised classiï¬ers
versussupervisedclassiï¬ersusingdatafrom26projectsfrom
three publicly available datasets (i.e., AEEEM, NASA, and
PROMISE). In the cross-project setting, our proposed con-nectivity-based classiï¬er (via spectral clustering) ranks asone of the top classiï¬ers among ï¬ve widely-used supervisedclassiï¬ers (i.e., random forest, naive Bayes, logistic regres-sion, decision tree, and logistic model tree) and ï¬ve unsu-
pervised classiï¬ers ( i.e.,k-means, partition around medoids,
fuzzy C-means, neural-gas, and spectral clustering). In the
within-project setting (i.e., models are built and applied on
the same project), our spectral classiï¬er ranks in the second
tier, while only random forest ranks in the ï¬rst tier. Hence,
connectivity-based unsupervised classiï¬ers oï¬€er a viable so-lution for cross and within project defect predictions.
CCS Concepts
â€¢Software and its engineering â†’Software veriï¬cation
and validation; Software defect analysis;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citationon the ï¬rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciï¬c permissionand/or a fee. Request permissions from permissions@acm.org.
ICSE â€™16, May 14 - 22, 2016, Austin, TX, USA
c/circlecopyrt 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.2884839Target
project!Heterogeneity
Supervised
classiï¬erBuildTraining
projects
Defect
proneness
Unsupervised
classiï¬er
Figure 1: Illustration of the heterogeneity challenge.
Keywords
defectprediction, heterogeneity, cross-project, unsupervised,
spectral clustering, graph mining
1. INTRODUCTION
A defect is an error in a software system that causes
a system to behave improperly or produce unexpected re-sults. Fixing defects typically consumes about 80% of thetotal budget of a software project [55]. Such cost can be
signiï¬cantly reduced if defects are ï¬xed in an early stage
[3, 11, 18, 37, 38, 46, 54]. Hence, defect prediction modelsare often used to prioritize quality improvement and defectavoidance eï¬€orts.
However, defect prediction is not widely adopted in indus-
try [45, 49, 56]. The barriers include the cost of collectingup-to-date training data ( e.g., defect data) [45, 49, 56, 57],
the low generalizability of prediction models [49], and thelack of automated tooling for the prediction process [10, 49,56]. Moreover, many companies lack the needed resources
and technical expertise to prepare data for building defect
prediction models [45]. A typical solution ( i.e., cross-project
prediction) is to apply defect prediction models that are
built using data from other training projects using super-
vised classiï¬ers [56, 62].
As illustrated in Figure 1, the major challenge in cross-
projectprediction comes from the heterogeneitybetween thetraining projects and the target project [13, 41]. The hetero-
geneity may be caused by diverse development settings (e.g.,
varyinguserrequirementsanddeveloperexperiences)[8,32].
It is common that the distribution of metric values of soft-
ware entities ( e.g., ï¬les or classes) exhibits signiï¬cant dif-
ferences across projects with varied contexts ( e.g.,s i z ea n d
programming language) [64]. Another heterogeneity chal-
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   309
lenge in cross-project prediction, as pointed out recently by
Nam and Kim [40], is that diï¬€erent projects may have dif-ferent sets of metrics all together.
To mitigate such challenges, an unsupervised classiï¬er
could be used. As shown in Figure 1, such classiï¬ers donot require any training data, and are therefore by naturefree of the challenges that are due to heterogeneity of the
training and target projects. However, distance-based unsu-
pervised classiï¬ers (e.g. ,k-means) have shown disappointing
performance for within-project defect prediction (e.g., [65]).
In this study, we propose to apply a connectivity-based
unsupervised classiï¬er that is based on spectral clustering
[43, 59]. Unlike distance-based unsupervised classiï¬ers thatpartition the data based on Euclidean distance, spectral
clustering considers the connectivity among all entities andtherefore has many advantages [33]. In defect prediction,
the connectivity among software entities can be determined
by their similarity in metric values. Our key intuition for ex-ploring spectral clustering is that defective entities tend tocluster around the same neighbourhoods ( i.e.,c l u s t e r s ) ,a s
observed as well by by Menzies et al.[35] and Bettenburg et
al.[4] in their work on local prediction models.
To evaluate the feasibility of using unsupervised classi-
ï¬ers for cross-project prediction, we perform an experimentusing three publicly available datasets ( i.e., AEEEM [14],
NASA [42], and PROMISE [29]) that include 26 projects intotal. Our major ï¬ndings are presented as follows:
â€¢Unsupervised classiï¬ers underperform supervised clas-siï¬ers in general. However, a connectivity-based un-supervised classiï¬er ( i.e., via spectral clustering) can
compete with supervised classiï¬ers.
â€¢Inthecross-projectsetting, ourproposedspectralclus-
tering based classiï¬er achieves a median AUC value of
0.71, and ranks as one of the top classiï¬ers.
â€¢In the within-project setting, our spectral clusteringbased classiï¬er ranks in the second tier, the same as
three commonly used supervised classiï¬ers ( i.e., logis-
tic regression, logistic model tree, and naive Bayes).
The random forest classiï¬er appears in the ï¬rst rank.
â€¢A deeper investigation conï¬rms our intuition that de-fective entities have signiï¬cantly stronger connections
with other defective entities than with clean entities.
As a summary, we propose to tackle cross-project predic-
tions from a diï¬€erent perspective, i.e., using a connectivity-
based unsupervised classiï¬er. Our spectral classiï¬er is rel-
atively simple (the implementation with 17 lines of R codeis provided in Appendix A). Moreover, our spectral classi-
ï¬er is unsupervised, therefore it can be applied on a project
without training data.
Paper organization. Section 2 presents the background
and related work. In Section 3, we describe details of our
spectral classiï¬er. Experimental setup and case study re-
sults are presented in Sections 4 and 5, respectively. Sec-
tion 6 closely examines the defect data in order to better
understand the strong performance of our spectral classi-
ï¬er. The threats to validity of our work are discussed in
Section 7. We conclude the paper and provide insights for
future work in Section 8.Software
metricsUnsupervised
clusteringCluster
1
Cluster
2Cluster
labelerDefective
CleanUnsupervised classiï¬er
Figure 2: A typical process to do defect predictionusing an unsupervised classiï¬er.
2. BACKGROUND AND RELATED WORK
In this section, we ï¬rst present the related work on cross-
project defect prediction and unsupervised defect predic-
tion. We then describe essential backgrounds on unsuper-
vised classiï¬ers.
2.1 Cross-project Defect Prediction
Prior attempts for cross-project defect prediction often re-
sulted in disappointing performance ( e.g., [23, 48, 57, 66]).
The major challenge is the heterogeneous distribution ofmetric values between the training projects and the target
project [13, 41].
To reduce the heterogeneity in cross-project defect predic-
tion, there are two major approaches:
(1) Using a model derived from training entities that are
most similar to the entities in the target project ( e.g.,
[26, 27, 32, 35, 57]); For instance, He et al.[26, 27] pro-
pose to ï¬lter the training set based on distributional
characteristics (e.g., mean and standard deviation) of
both the training and the target sets. Turhan et al.
[57] propose to perform nearest neighbour ï¬ltering (NN-
ï¬ltering).
(2) Transforming the metrics of both the training projects
and the target project to increase their similarity ( e.g.,
[12,34,41,58]). Forinstance, Ma et al.[34]applyTrans-
fer Naive Bayes (TNB), Nam et al.[41] use Transfer
component analysis (TCA), Chen et al.[12] use double
transfer boosting (DTB) model. Our previous work [62]
proposes a context-aware rank transformation approachthat transforms software metrics based on project con-
texts(e.g. , programminglanguageandprojectsize), and
builds a universal defect prediction model that achievescomparable performance as within-project models.
Anotherchallengeincross-projectdefectpredictionisthat
thesetofmetricsisoftendiï¬€erentamongprojects. NamandKim [40] propose an approach to deal with heterogeneous
sets of metrics between the training projects and the target
project.
2.2 Unsupervised Defect Prediction
Unsupervised defect prediction predicts defect proneness
without requiring access to training data. As illustratedin Figure 2, a typical process for predicting defects usingan unsupervised classiï¬er has two steps: 1) clustering soft-ware entities into kclusters (usually two clusters); and 2)
labelling each cluster as a defective or clean cluster. How-
ever, thereexistsalimitednumberofstudiesintheliterature
310on unsupervised defect prediction. One reason is that un-
supervised classiï¬ers usually underperform supervised ones(e.g., random forest and logistic regression) in terms of their
predictive power.
An initial attempt to use unsupervised defect classiï¬ers
is by Zhong et al.[65] who apply k-means and neural-gas
clustering in defect prediction. Zhong et al.[65] observe
that a neural-gas classiï¬er outperforms k-means in terms of
predictive power, but runs slower. However, their approach
requires one to specify the expected number of clusters, andinvolves experts to determine which cluster contains defec-tive entities (i.e., label the cluster). Catal et al.[9] pro-
pose to use metric values to label the clusters. Bishnu and
Bhattacherjee [5] propose to apply quad trees to initialize
the cluster centres of k-means clustering. In addition to k-
means clustering based classiï¬ers, Abaei et al.[1] propose
to use self-organizing maps (SOM) and Yang et al.[60] pro-
pose to apply the aï¬ƒnity propagation clustering algorithm.Recently, Nam and Kim [39] proposed to label the clusters
using thresholds on selected metrics.
2.3 Background on Unsupervised Classiï¬ers
Unsupervised classiï¬ers make use of clustering methods.
Clustering is a common way to explore groups of similar
entities. Frequently applied clustering methods include hi-
erarchical clustering and k-means. Hierarchical clustering
produces clusters based on the structure of a similarity or
dissimilarity matrix. K-means clustering is used to cluster
high-dimensional data that are linearly separable [17].
In recent years, spectral clustering has become one of
the most eï¬€ective techniques for clustering [43, 59]. Unlikedistance-based classiï¬ers ( e.g.,k-means clustering) that di-
vide a data set based on Euclidean distance, spectral cluster-ing partitions a data set based on the connectivity between
its entities. Spectral clustering is performed on a graph con-
sisting of nodes and edges. In the context of defect pre-
diction, each node represents a software entity ( e.g.,ï¬ l eo r
class). Each edge represents the connection between soft-ware entities, and its weight is measured by the similarity of
metric values between its two ends.
Similarity deï¬nition. A widely used similarity is the
dot product between vectors of two nodes iandj[ 2 ,6 ,1 6 ] ,
as shown in Equation (1).
w
ij=xiÂ·xj=m/summationdisplay
k=1aikakj (1)
wherexiandxjdenote the metric values of software entities
iandj, respectively; aikis the value of the kthmetric on the
ithsoftware entity, and mis the total number of metrics.
From the geometric perspective, the similarity wijcan be
interpreted as xiÂ·xj=|xi||xj|cosÎ¸ ij, where|xi|and|xj|are
the norms, and Î¸ijis the angle between two vectors. It is
the length of the projection of one vector onto the other unit
vector.
From a correlation perspective, the similarity wijis ba-
sically the unnormalized Pearson correlation coeï¬ƒcient [7]between nodes iandj. Each element in vector x
irepresents
a metric value. It is unnormalized, since it makes little sense
to normalize the values across metrics belonging to the same
software entity. The similarity wijcan be positive, negative
or zero. A positive value indicates a positive correlation be-
tween two software entities, and a negative value indicatesa negative correlation. A value of zero indicates that there
is no linear correlation. It is meaningless to study the self-Algorithm 1: Spectral clustering based classiï¬er for de-
fect prediction
Input:A matrix with rows as software entities and
columns as metrics.
Output: A vector of defect proneness of all software
entities.
1: Normalize software metrics using z-score.
2: Construct a weighted adjacency matrix W.
3: Calculate the Laplacian matrix Lsym.
4: Perform the eigendecomposition on Lsym.
5: Select the second smallest eigenvector v1.
6: Perform the bipartition on v1using zero.
7: Label each cluster as defective or clean.
circle of a software entity, therefore we set the self-similarity
(i.e.,a l lw ii)t oz e r o .
Spectral clustering steps. A popular algorithm for
spectral clustering is to minimize the normalized cut [53].The normalized cut is a disassociation measure to describe
the cost of cutting two partitions in a graph [53]. This al-
gorithm partitions a graph into two subgraphs to gain high
similarity within each subgraph while achieving low similar-
ity across the two subgraphs.
The input for spectral clustering is a weighted adjacency
matrix that stores the similarity between each pair of nodes
in the graph. There are three major steps in the algorithm:
(1) Computing the Laplacian matrix from the weighted ad-
jacency matrix, where the Laplacian matrix is a widely
used matrix representation of a graph in graph theory;
(2) Performing an eigendecomposition on the Laplacian ma-
trix;
(3) Selecting a threshold on the second smallest eigenvector
to obtain the bipartitions of the graph.
3. OUR SPECTRAL CLASSIFIER
In this section, we describe details on our spectral cluster-
ing based classiï¬er (see Algorithm 1). The Rimplementa-
tion of our spectral classiï¬er consists of 17 lines of code (see
Appendix A).
3.1 Preprocessing Software Metrics
Software metrics have varied scales. Hence, software met-
rics are often normalized before further processing [24, 41,
44]. For instance, Nam et al.[41] ï¬nd that applying z-score
to normalize software metrics can signiï¬cantly improve the
predictive power of defect prediction models. The advantageofz-score is that a normalized software metric has a mean
value of zero and a variance of one.
Our spectral classiï¬er uses the z-score for the normaliza-
tion of each metric. We use y
jto denote a vector of values
of thejthmetric in a project. Then yj={a1j,...,a nj}T,
wherenis the number of entities in the project, and aijis
the value of the jthmetric on the ithsoftware entity. The
vectoryjis normalized as Ë†yj=yjâˆ’Â¯yj
sj, where Â¯yjis the aver-
age value of yjandsjis the standard deviation of yj. This
step corresponds to Line 1 in Algorithm 1.
3113.2 Spectral Clustering
We now describe the three steps for spectral clustering.
Step 1. The ï¬rst step is to calculate the Laplacian matrix
Lsym. The symmetric Laplacian matrix Lsymis derived
from the adjacency matrix Wthat stores the similarity be-
tween each pair of software entities. The adjacency matrix
Wis computed directly from the normalized software met-
rics (i.e., Line 2 in Algorithm 1). In spectral clustering,there is usually an assumption that all values of the similar-ity are non-negative [36]. Hence, we set all negative w
ijto
zero.
The symmetric Laplacian matrix Lsymis calculated us-
ingLsym=Iâˆ’Dâˆ’1
2WDâˆ’1
2(i.e., Line 3 in Algorithm 1),
where the matrix Iis the unit matrix with size n, the ma-
trixDis a diagonal matrix of row sums of W,a n dDâˆ’1
2=
Diag(dâˆ’1
2
1,...,dâˆ’1
2n), where dâˆ’1
2
i=(/summationtextn
j=1wij)âˆ’1
2.
Step 2. The second step is to perform the eigendecomposi-
tion on the symmetric Laplacian matrix Lsym(i.e.,L i n e4
in Algorithm 1). Eigenvalues will always be ordered increas-
ingly [33, 53]. We follow the normalized cut algorithm byShi and Malik [53] and use the second smallest eigenvectorfor clustering (i.e., Line 5 in Algorithm 1). We use v
1to
denote the second smallest eigenvector of Lsym.
Step 3. The third step is to separate all entities into two
clusters. Shi and Malik [53] propose to apply a particularthreshold, such as zero or median, on the second smallesteigenvector v
1. If the median is used, then 50% of entities
are predicted as defective. Inspecting 50% of entities re-quires signiï¬cant eï¬€ort. Hence, we adopt zero as the thresh-old value of v
1(i.e., Line 6 in Algorithm 1) to create two
non-overlapped clusters. We use v1ito denote the ithvalue
ofv1, where iâˆˆ{1,...,n},a n dnis the total number of
software entities in the given project. The value v1icor-
responds to the eigenvalue of the ithsoftware entity. All
entities with v1i>0 create a cluster called Cpos, and all
entities with v1i<0 create the other cluster called Cneg.
In the following subsection, we describe how to determinewhether cluster C
poscontains defective entities, or cluster
Cnegdoes.
3.3 Labelling Defective Cluster
The last step ( i.e., Line 7 in Algorithm 1) of applying the
spectral clustering based classiï¬er in defect prediction is to
label the defective cluster.
We useCdefective to denote the cluster that contains de-
fective entities only, and use Ccleanto represent the cluster
that contains clean entities only.
To determine whether CposorCnegis the defective cluster
Cdefective, we use the following heuristic: For most metrics,
software entities containing defects generally have larger val-
ues than software entities without defects. This heuristic is
based on our ï¬eldâ€™s extensive empirical observations on the
relationship between software metrics and defect proneness.
For instance, Gaï¬€ney [19] ï¬nd that larger ï¬les have a higherlikelihood to experience defects than smaller ï¬les. Kitchen-hamet al.[30] report that more complex ï¬les are more likely
to experience defects than ï¬les with lower complexity. Sim-
ilar ï¬ndings are also observed in many other studies ( e.g.,
[15, 25, 39]).
With this heuristic in mind, we use the average row sums
of the normalized metrics of each cluster to determine whichcluster is defective. The row sum is the sum of all metricvalues of the same entity. We compute the average row sum
of all entities within each cluster (i.e., either C
posorCneg).
The cluster with larger average row sum is considered asthe cluster containing defective entities. We label all entitieswithin this cluster as defective ( i.e.,C
defective), and all the
remaining entities as clean ( i.e.,Cclean).
However, the aforementioned heuristic does not necessar-
ily work for all kinds of metrics. For instance, in the case
where smaller values indicate less chance of defects, the
aforementioned heuristic should be reversed. We suggestpractitioners to derive the appropriate heuristic based ontheir set of metrics.
4. EXPERIMENT SETUP
In this section, we present the experimental setup to eval-
uate the performance of our spectral classiï¬er.
4.1 Corpora
We examined data from three commonly studied datasets:
AEEEM [14], NASA [42], and PROMISE [29]. The threedatasets are publicly available and have been used exten-sively in defect prediction studies (e.g., [20, 22, 35, 41]). Abrief description on each dataset and our selected metricsare presented as follows.
D1.The AEEEM dataset was prepared by Dâ€™Ambros et
al.[14] to compare the performance of diï¬€erent sets of
metrics. Accordingly, theAEEEMdatasetcontainsthe
most number of metrics. In particular, it has 61 met-rics, including product, process, previous-defect met-rics, and entropy-based metrics.
All projects in the AEEEM dataset have 61 identical
software metrics. We use all 61 metrics in our study.
D2.The NASA dataset was collected by the NASA Met-rics Data Program. Shepperd et al.[51] observe that
the original NASA dataset contains many repeated andinconsistent data points, and they clean up the NASAdataset. Inthisstudy, weusethecleanedNASAdatasetthat is available in the PROMISE repository.
In the NASA dataset, projects do not share the same
setofmetrics. Forinstance, projectKC3has39metrics
while project JM1 has 21 metrics. Since supervisedclassiï¬ers require exact the same sets of metrics, weonly select the 20 metrics that are common across all
of the 11 studied NASA projects.
D3.The PROMISE dataset was prepared by Jureczko and
Madeyski [29]. It contains open source Java projectsand has object-oriented metrics.
In the PROMISE dataset, projects do not have the
same set of metrics. Hence, we select the 20 metricsthatarecommonacrossallofthe10studiedPROMISEprojects.
In general, the selected projects have diverse size ( i.e.,
having 125 to 7,782 instances) and varied percentage of de-
fective entities ( i.e., ranging from 2.1% to 63.6%). The sum-
mary of all selected projects is presented in Table 1. More
details about these metrics can be found on the correspond-ing website of each dataset.
312Table 1: An overview of the studied projects.
Dataset Project #o fE n t i t i e sDefective
(#) (%)
AEEEMEclipse JDT Core 997 206 20.7%
Equinox 324 129 39.8%
Apache Lucene 691 64 9.3%
Mylyn 1,862 245 13.2%
Eclipse PDE UI 1,497 209 14.0%
NASACM1 327 42 12.8%
JM1 7,782 1,672 21.5%
KC3 194 36 18.6%
MC1 1,988 46 2.3%
MC2 125 44 35.2%
MW1 253 27 10.7%
PC1 705 61 8.7%
PC2 745 16 2.1%
PC3 1,077 134 12.4%
PC4 1,287 177 13.8%
PC5 1,711 471 27.5%
PROMISEAnt v1.7 745 166 22.3%
Camel v1.6 965 188 19.5%
Ivy v1.4 241 16 6.6%
Jedit v4.0 306 75 24.5%
Log4j v1.0 135 34 25.2%
Lucene v2.4 340 203 59.7%
POI v3.0 442 281 63.6%
Tomcat v6.0 858 77 9.0%
Xalan v2.6 885 411 46.4%
Xerces v1.3 453 69 15.2%
Average 1,036 196 18.9%
4.2 Performance Measure
There are many performance measures, such as precision,
recall, accuracy, F-measure and the Area Under the receiver
operating characteristic Curve (AUC). However, a cut-oï¬€
value on the predicted probability of defect proneness is re-
quired when computing precision, recall, accuracy, and F-measure. The default cut-oï¬€ is 0.5 which may not be thebest cut-oï¬€ value in practice [63]. On the other hand, theAUC value is independent of a cut-oï¬€ value and is not im-
pacted by the skewness of defect data. Lessmann et al.[31]
and Ghotra et al.[20] suggest to use the AUC value for bet-
ter cross-dataset comparability. Hence, we select the AUC
measure as our performance measure.
When computing the AUC measure, a curve of the false
positive rate is plotted against the true positive rate. Ac-
cordingly, the AUC value measures the probability that a
randomly chosen defective entity ranks higher than a ran-domly chosen clean entity. An AUC value of 0.5 implies
that a classiï¬er is no better than random guessing. A larger
AUC value indicates a better performance. In particular,Gorunescu [21] advises the following guideline to interpretthe AUC value: 0.90 to 1.00 as excellent prediction, 0.80 to
0.90 as a good prediction, 0.70 to 0.80 as a fair prediction,
0.60 to 0.70 as a poor prediction, and 0.50 to 0.60 as a failedprediction.
4.3 Classiï¬ers for Comparison
To ï¬nd if our spectral classiï¬er is applicable for defect
prediction in a cross-project setting, we compare its perfor-mance with nine oï¬€-the-shelf classiï¬ers. We not only select
supervised classiï¬ers, but also choose distance-based unsu-pervised classiï¬ers.
For supervised classiï¬ers, we select ï¬ve classiï¬ers that
have been commonly applied to build defect prediction mod-
els. The ï¬ve classiï¬ers are random forest (RF), naive Bayes
(NB), logistic regression (LR), decision tree (J48), and lo-
gistic model tree (LMT).Fordistance-basedunsupervisedclassiï¬ers, wechoosefour
classiï¬ers that have been previously used in the defect pre-dictionliterature[9,65]. Thefourclassiï¬ersinclude k-means
clustering (KM), partition around medoids (PAM), fuzzy C-
means (FCM), and neural-gas (NG). These classiï¬ers arebased on Euclidean distance, therefore employ a diï¬€erentclustering mechanism than spectral clustering (SC).
4.4 Scott-Knott Test
To compare the performance across the large number of
datasets, we apply the Scott-Knott test [28] using the 95%conï¬dence level (i.e., Î±=0.05). The Scott-Knott test can
overcome the issue of overlapping multiple comparisons that
are obtained from other tests, such as the Mann-Whitney U
test [52]. The Scott-Knott test has been used in defect pre-
diction studies to compare the performance across diï¬€erentclassiï¬ers [20].
The Scott-Knott test recursively ranks the evaluated clas-
siï¬ers through hierarchical clustering analysis. In each itera-
tion, the Scott-Knott test separates the evaluated classiï¬ers
into two groups based on the performance measure (i.e.,t h e
AUC value). If the two groups have statistically signiï¬cantdiï¬€erence in the AUC value, the Scott-Knott test executesagain within each group. If no statistically distinct groupscan be created, the Scott-Knott test terminates [20].
5. CASE STUDY RESULTS
In this section, we present our research questions, along
with our motivation, approach, and ï¬ndings.
RQ1. How does our spectral classiï¬er perform
in cross-project defect prediction?
Motivation. Unlike supervised classiï¬ers, unsupervised
classiï¬ers do not have to deal with the challenge of hetero-
geneity between the training projects and the target project.
While distance-based classiï¬ers ( e.g.,k-means clustering)
underperform supervised classiï¬ers, connectivity-based un-
supervised classiï¬ers have not been explored in our com-munity. Hence, it is of signiï¬cant interest to investigate ifconnectivity-based classiï¬ers (particularly via spectral clus-tering) can provide comparable performance as supervisedclassiï¬ers in the context of cross-project defect prediction.Approach. To address this question, we need to get the
performance of all studied classiï¬ers for each project. For
each classiï¬er, all entities of the target project are used to
obtain its performance.
Supervised classiï¬ers require a training project. All su-
pervised classiï¬ers under study require the exact same setof metrics between the training and the target projects.As the three studied datasets ( i.e., AEEEM, NASA, and
PROMISE) have diï¬€erent sets of metrics, we make cross-
project defect prediction within the same dataset. For each
target project, we select all other projects from the same
dataset for training. For instance, if the target project is
â€œEclipse JDT Coreâ€, then each supervised classiï¬er is used
to build four models using each of the remaining projects
within the same dataset ( i.e.,â€œEquinoxâ€,â€œApache Luceneâ€,
â€œMylynâ€, andâ€œEclipse PDE UIâ€), respectively. We compute
the average AUC values of these four models to measure
the performance of the corresponding classiï¬er on the tar-
get project, since it is unknown which model performs the
best on the target project prior to the prediction.
313SC
RF
FCM
LMT
NB
LR
NG
KM
PAM
DT0.500.550.600.650.700.750.800.85AEEEMAUC
SC
RF
NB
LMT
FCM
PAM
LR
NG
KM
DT0.500.550.600.650.700.750.800.85NASAAUCSC
NB
RF
LMT
LR
KM
NG
FCM
PAM
DT0.500.550.600.650.700.750.800.85PROMISEAUC
SC
RF
NB
LMT
LR
FCM
PAM
KM
NG
DT0.500.550.600.650.700.750.800.85All projectsAUC
Figure 3: The boxplots of AUC values of all stud-
ied supervised (blue labels) and unsupervised (red
labels) classiï¬ers (for the abbreviations, see Sec-tion 4.3). Diï¬€erent colors represents diï¬€erent ranks
(red>yellow>green>blue).
Unsupervised classiï¬ers do not require training projects.
We directly apply the studied unsupervised classiï¬ers on thetarget project. When do clustering, we create kclusters.
We setk= 2 for clustering, since this setting yields the
best performance in defect prediction (e.g., [20]). In the
resulting two clusters, one cluster is labelled as defective,
and the other cluster is labelled as clean, using the heuristic
that is described in Section 3.3.
To compare the predictive power among all classiï¬ers, we
apply the Scott-Knott test with the 95% conï¬dence level torank all classiï¬ers across projects within the same dataset.
We examine the Scott-Knott ranks per dataset. Further-
more, we perform one large Scott-Knott run where we inputall the AUC values for all the classiï¬ers across all datasets.
Findings. Our spectral classiï¬er achieves good re-
sults for defect prediction in the cross-project set-
ting.In general, our spectral classiï¬er signiï¬cantly outper-
forms all other unsupervised classiï¬ers, and it has slightlybetter performance than the best supervised classiï¬er under
study (i.e.,r a n d o mf o r e s t ) .
Our spectral classiï¬er ranks the ï¬rst in all the three stud-
ied datasets. The colors in Figure 3 illustrate the ranks of all
classiï¬ers. The boxplots show the distribution of the AUCvalues of each classiï¬er under study. Classiï¬ers with box-
plots of the same color are ranked at the same tier. The per-formances of classiï¬ers in the same tier are not statistically
distinct. Among all supervised and unsupervised classiï¬ers,only two supervised classiï¬ers (i.e., random forest and lo-
gistic model tree) are in the same ranking tier across all thethree datasets as our spectral classiï¬er.
T h ee x a c tA U Cv a l u e so ft h et o pf o u rc l a s s i ï¬ e r s( i.e.,o u r
spectral classiï¬er, random forest, naive Bayes, and logisticmodel tree) on each project are presented in Table 2. In
particular, the median AUC values of the top four classiï¬ersTable 2: The AUC values of the top four classiï¬ers in
cross-project defect prediction (Bold font highlightsthe best performance).
Dataset Project SC RF NB LMT
AEEEMEclipse JDT Core 0.83 0.81 0.68 0.75
Equinox 0.81 0.70 0.66 0.71
Apache Lucene 0.79 0.76 0.72 0.70
Mylyn 0.63 0.62 0.53 0.57
Eclipse PDE UI 0.72 0.71 0.65 0.67
NASACM1 0.67 0.66 0.66 0.62
JM1 0.66 0.62 0.64 0.60
KC3 0.64 0.65 0.62 0.63
MC1 0.69 0.71 0.66 0.67
MC2 0.68 0.62 0.64 0.59
MW1 0.70 0.67 0.70 0.67
PC1 0.71 0.73 0.70 0.70
PC2 0.78 0.76 0.73 0.79
PC3 0.72 0.70 0.70 0.68
PC4 0.65 0.67 0.63 0.67
PC5 0.71 0.66 0.66 0.63
PROMISEAnt v1.7 0.79 0.75 0.77 0.75
Camel v1.6 0.62 0.60 0.60 0.61
Ivy v1.4 0.70 0.71 0.68 0.70
Jedit v4.0 0.79 0.74 0.75 0.73
Log4j v1.0 0.82 0.76 0.81 0.74
Lucene v2.4 0.67 0.68 0.69 0.66
POI v3.0 0.82 0.71 0.78 0.69
Tomcat v6.0 0.80 0.78 0.80 0.77
Xalan v2.6 0.54 0.66 0.60 0.62
Xerces v1.3 0.77 0.69 0.70 0.71
Median 0.71 0.70 0.68 0.68
across all projects under study are: 0.71, 0.70, 0.68 and 0.68,respectively.
We observe that distance-based unsupervised clas-
siï¬ers (e.g., k-means) do not perform as well as su-
pervised classiï¬ers. The poor performance of these dis-
tance-based classiï¬ers may explain why unsupervised classi-ï¬ers are not widely applied in defect prediction.
In summary, our results clearly show that applying con-
nectivity-based unsupervised classiï¬cation is a promising di-rection to tackle the heterogeneity challenge in cross-projectdefectprediction. Ourconnectivity-basedunsupervisedclas-siï¬er is based on spectral clustering. We suspect that the
success of spectral clustering is because defective entities
are more similar to other defective entities than other cleanentities in terms of the values of their various software met-rics. Such intuition is supported through recent work by
Menzies et al.[35] and Bettenburg et al.[4] on local defect
prediction models.


Our spectral classiï¬er performs the best among all
studied classiï¬ers that include ï¬ve supervised and
ï¬ve unsupervised classiï¬ers. Therefore, applying
the connectivity-based unsupervised classiï¬cation is apromising direction to tackle the challenge of heteroge-neous data in cross-project defect prediction.
RQ2. Does our spectral classiï¬er perform well
in within-project defect prediction?
Motivation. In comparison to a cross-project setting, the
chance of experiencing heterogeneous training and target
data is much lower in a within-project setting. As unsu-
pervised classiï¬ers can save signiï¬cant eï¬€ort in defect data
collection, we are interested to ï¬nd if our connectivity-based
314Table 4: The average AUC values of the top ï¬ve classiï¬ers in both cross-project (CP) and within-project
settings (WP). The column â€œdiï¬€ â€ shows the diï¬€erence between cross-project models and within-project
models.
Dataset ProjectRF LR SC LMT NB
CP WP diï¬€ CP WP diï¬€ CP WP diï¬€ CP WP diï¬€ CP WP diï¬€
AEEEMEclipse JDT Core 0.81 0.870.06 0.75 0.790.04 0.83 0.83 0 0.75 0.820.07 0.68 0.740.06
Equinox 0.70 0.840.14 0.61 0.640.03 0.81 0.80-0.01 0.71 0.79 0.08 0.66 0.720.06
Apache Lucene 0.76 0.810.05 0.66 0.63-0.03 0.79 0.79 0 0.70 0.78 0.08 0.72 0.740.02
Mylyn 0.62 0.820.20 0.56 0.790.23 0.63 0.63 0 0.57 0.78 0.21 0.53 0.650.12
Eclipse PDE UI 0.71 0.780.07 0.66 0.730.07 0.72 0.72 0 0.67 0.75 0.08 0.65 0.670.02
NASACM1 0.66 0.680.02 0.61 0.740.13 0.67 0.67 0 0.62 0.65 0.03 0.66 0.670.01
JM1 0.62 0.670.05 0.55 0.690.14 0.66 0.66 0 0.60 0.680.08 0.64 0.650.01
KC3 0.65 0.710.06 0.59 0.640.05 0.64 0.64 0 0.63 0.63 0 0.62 0.650.03
MC1 0.71 0.810.10 0.64 0.740.10 0.69 0.69 0 0.67 0.58-0.09 0.66 0.680.02
MC2 0.62 0.650.03 0.54 0.660.12 0.68 0.67-0.01 0.59 0.670.08 0.64 0.660.02
MW1 0.67 0.720.05 0.59 0.640.05 0.70 0.70 0 0.67 0.63-0.04 0.70 0.710.01
PC1 0.73 0.830.10 0.68 0.820.14 0.71 0.71 0 0.70 0.750.05 0.70 0.68-0.02
PC2 0.76 0.74-0.02 0.65 0.660.01 0.78 0.78 0 0.79 0.53-0.26 0.73 0.71-0.02
PC3 0.70 0.780.08 0.65 0.810.16 0.72 0.72 0 0.68 0.710.03 0.70 0.730.03
PC4 0.67 0.910.24 0.63 0.880.25 0.65 0.65 0 0.67 0.880.21 0.63 0.740.11
PC5 0.66 0.760.10 0.60 0.730.13 0.71 0.71 0 0.63 0.720.09 0.66 0.680.02
PROMISEAnt v1.7 0.75 0.820.07 0.74 0.800.06 0.79 0.79 0 0.75 0.81 0.06 0.77 0.780.01
Camel v1.6 0.60 0.710.11 0.61 0.730.12 0.62 0.62 0 0.61 0.690.08 0.60 0.670.07
Ivy v1.4 0.71 0.67-0.04 0.69 0.55-0.14 0.70 0.70 0 0.70 0.57-0.13 0.68 0.64-0.04
Jedit v4.0 0.74 0.800.06 0.72 0.770.05 0.79 0.78-0.01 0.73 0.780.05 0.75 0.75 0
Log4j v1.0 0.76 0.800.04 0.74 0.69-0.05 0.82 0.78-0.04 0.74 0.810.07 0.81 0.81 0
Lucene v2.4 0.68 0.770.09 0.65 0.750.10 0.67 0.66-0.01 0.66 0.750.09 0.69 0.730.04
POI v3.0 0.71 0.880.17 0.70 0.830.13 0.82 0.81-0.01 0.69 0.830.14 0.78 0.820.04
Tomcat v6.0 0.78 0.810.03 0.75 0.820.07 0.80 0.80 0 0.77 0.810.04 0.80 0.80 0
Xalan v2.6 0.66 0.850.19 0.60 0.810.21 0.54 0.54 0 0.62 0.810.19 0.60 0.760.16
Xerces v1.3 0.69 0.830.14 0.72 0.770.05 0.77 0.77 0 0.71 0.740.03 0.70 0.790.09
Median 0.70 0.80 0.07 0.65 0.74 0.09 0.71 0.71 0 0.68 0.75 0.07 0.68 0.72 0.02
Table 3: Ranks of all studied classiï¬ers for within-
project defect prediction based on 1,000 evaluations.
Overall Classiï¬er Median Average Standard
ranks rank rank deviation
1 RF 1 1.42 0.64
2LR 2 3.19 2.15
SC 3 3.35 1.67
LMT 3 3.42 1.94
NB 3.5 3.54 1.27
3 FCM 6 5.96 1.08
4PAM 6.5 6.73 1.69
NG 7 6.85 1.67
DT 7 6.89 1.56
KM 7.5 7.35 1.55
unsupervised classiï¬er (i.e., the proposed spectral classiï¬er)
can still compete with supervised classiï¬ers in a within-
project setting.Approach. To evaluate the performance of supervised clas-
siï¬ers in a within-project setting, the essential step is to sep-arate all entities of a project into two sets. One set is fortraining a model and the other one is the target set to applythe model. Both supervised and unsupervised classiï¬ers are
applied on the same target set of entities. The only diï¬€er-
ence is that supervised classiï¬ers require an additional stepto build a model from the training set of entities.
To create the training and target sets, we apply a two-fold
cross validation (i.e., a 50:50 random split) that has been
previously applied in the defect prediction literature [39, 47].
For a 50:50 random split, each classiï¬er is evaluated twice:
1) the ï¬rst half is used as the training data while the other
half is used as the target data; and 2) the second half is used
as the training data while the ï¬rst half is used as the target
data. To deal with the randomness of sampling, we repeatthe random splits for 500 times (i.e., 500 times of two-foldcross validation). In total, 1,000 evaluations are performed
for each classiï¬er on each project. To get the performance
of each classiï¬er on each project, we compute the average
AUC value of the total 1,000 evaluations.
To ï¬nd statistically distinct ranks of all classiï¬ers, we fol-
low the approach of Ghotra et al.[20] and perform a double
Scott-Knott test. The double Scott-Knott test ensures arobust ranking of all classiï¬ers across projects, regardless
o ft h e i re x a c tA U Cv a l u e s . T h eï¬ r s tS c o t t - K n o t tt e s ti sperformed on each individual project to rank all classiï¬ersbased on their AUC values for that particular project. Theobtained ranks are used in the second run of the Scott-Knott
test to yield a global ranking of all classiï¬ers across all stud-
ied projects.
Findings. Generally speaking, in a within-project
setting, supervised classiï¬ers outperform unsuper-
vised classiï¬ers . There is only one unsupervised classiï¬er
(i.e., our spectral classiï¬er) among the top ï¬ve classiï¬ers.
The detailed rankings are presented in Table 3, including
the global ranks of all classiï¬ers across all projects, and the
statistics (i.e., median, average, and standard deviation) ofthe ranks of each classiï¬er as obtained in the ï¬rst Scott-
Knott test on the results of 1,000 evaluations. In particular,
our spectral classiï¬er has a median rank of 3, and is ranked
in the same tier as three widely used classiï¬ers, i.e., logistic
regression, logistic model tree, and naive Bayes.
The actual AUC values of the top ï¬ve classiï¬ers ( i.e.,r a n -
dom forest, logistic regression, our spectral classiï¬er, logis-tic model tree, and naive Bayes) on each project are pre-
sented in Table 4. The AUC values in both cross-project andwithin-project settings are shown, as well as their diï¬€erence
(i.e., the AUC value in a within-project setting minus the
AUC value in a cross-project setting).
3150.0
0.10.2
0.3
0.4
0.5
0.6âˆ’0.3âˆ’0.2âˆ’0.10.00.10.20.3
Defect RatioDifferenceRF
LR
OursLMT
NB
Figure 4: The regression lines of the performance
diï¬€erence of the top ï¬ve classiï¬ers between a within-
project setting and a cross-project setting over the
ratio of defects of each project. (The dotted line isthe horizontal base line.)
Our spectral classiï¬er achieves almost the same
predictive power between cross-project and within-project settings across all projects under study, ass h o w ni nT a b l e4 . The size of the target project in a
within-project setting is only half of that in a cross-projectsetting, highlighting that our spectral classiï¬er tends to berobust when the size of the target project changes.
A within-project model can sometimes signiï¬cant-
ly underperform a cross-project model, although a
within-project model generally outperforms a cross-
project model. For example, looking at Table 4, and for
projectâ€œIvy v1.4â€, the top four supervised classiï¬ers experi-enceadowngradedperformancewhenchangingfromacross-
project setting to a within-project setting. In particular, the
random forest classiï¬er achieves an AUC value of 0.71 in across-project setting, but yields a lower AUC value of 0.67in a within-project setting. We conjecture that the decreasein performance when changing to a within-project setting
is caused by the low ratio of defects (i.e., the low percent-
age of defective entities) in the target project. For instance,project â€œIvy v1.4â€ has a ratio of defects of 6.6% with only16 defective entities. Similar observations are noted in other
projects, such asâ€œApache Luceneâ€andâ€œPC2â€.
Supervised classiï¬ers tend to experience a performance
decrease, if the ratio of defects becomes lower. To illustrate
the relationships between the performance of each classiï¬erand the ratio of defects, we plot regression lines of the per-
formance diï¬€erence of the top ï¬ve classiï¬ers over the ratio of
defects in Figure 4. In comparison to supervised classiï¬ers,our spectral clustering based classiï¬er is more robust acrossa varying ratio of defects. One possible reason is that su-
pervised classiï¬ers experience a signiï¬cant class-imbalance
problem on these projects, while our spectral classiï¬er isan unsupervised approach, therefore has no issue of class-imbalance. We conjecture that, for projects with a low ratioof defects, our spectral clustering based classiï¬er may be
more suitable than the supervised classiï¬ers.



	In a within-project setting, our spectral classiï¬er ranks
in the second tier with only random forest ranking in
the ï¬rst tier. However, our spectral classiï¬er may be
more suitable for projects with heavily imbalanced ( i.e.,
very low percentage of defective entities) defect data.6. WHY DOES IT WORK?
In this section, we present an in-depth analysis to under-
stand why our spectral classiï¬er, which is a connectivity-based classiï¬er, achieves good results in defect prediction.As aforementioned, spectral clustering separates all entitiesin a project based on the connections among entities. Weconjecture that software entities may reside within twoâ€œso-cial networkâ€-like communities: 1) one community is formu-latedbydefectiveentities; and2)theotheroneisestablishedby clean entities.
6.1 Essential Deï¬nitions
Community deï¬nition. Wedeï¬neacommunityasaset
of members (i.e., software entities) that have much strongerconnections with each other than with members from other
communities. A connection is basically an edge in a graph,
as mentioned in Section 2.3. We deï¬ne an edge betweenentitiesiandjusing Equation (2).
e
ij=1(wij)( 2)
where 1(wij)=1ifwij>0,a n d1(wij)=0otherwise.
As described in Section 2.3, wijrepresents the similarity
or the correlation between entities iandj.H e n c e , eijequals
to 1, if there is a positive correlation between entities iand
j.W ed e n o t et h es e to fa l le d g e sa sE , thenE={eij}.
We construct the community as follows. For each project,
we partition the entities into two sets based on their defect
proneness. We use Vdto denote the set of actual defective
entities, and Vcto denote the set of actual clean entities.
A software entity can be either defective or clean. Hence,
there is no overlap between VdandVc, and the union of Vd
andVccontains all entities within the same project.
Connectivity measurement. We deï¬ne degdd, the to-
tal degree of all defective entities, using Equation (3). We
deï¬nedegdd, the total degree of all clean entities, using
Equation (4). Similarly, we deï¬ne degcd, the total number
of edges between each pair of defective and clean entities,using Equation (5).
deg
dd=/summationdisplay
iâˆˆVd/summationdisplay
jâˆˆVdeij,j/negationslash=i (3)
degcc=/summationdisplay
iâˆˆV c/summationdisplay
jâˆˆVceij,j/negationslash=i (4)
degcd=/summationdisplay
iâˆˆVc/summationdisplay
jâˆˆVdeij (5)
To measure the connectivity among entities within Vdor
Vc, or between VdandVc, we further deï¬ne the ratio of edges
(i.e., connections) as follows.
Ï†dd=degdd
|Vd|(|Vd|âˆ’1)(6)
Ï†cc=degcc
|Vc|(|Vc|âˆ’1)(7)
Ï†cd=degcd
|Vc||Vd|(8)
To illustrate the computation, we present an example in
Figure 5. There are three defective and four clean entities.Each defective entity has connections to all other two defec-
tive entities. Hence, deg
dd=2+2+2=6and Ï†dd=6
3Ã—2=
1.000. Similarly, we can get degcc=2+2+2+2=8and
316Clean entities Defective entities
Ï†cc=8
4Ã—3
Ï†dd=6
3Ã—2
Ï†cd=2
4Ã—3
Figure 5: Illustrating example of computing the ra-
tio of edges (i.e. ,Ï†dd,Ï†cc,Ï†cd).
Ï†cc=8
4Ã—3=0.667, and degcd=2a n d Ï†cd=2
4Ã—3=0.167.
6.2 Hypotheses
For each project, we compute the ratios Ï†dd,Ï†cc,a n d
Ï†cdbased on the actual defect proneness. To compare the
connectivity among entities across all projects under study,
we test the following hypotheses:H0
1: there is no diï¬€erence in the ratios of connections from
defective entities to other defective entities ( Ï†dd) and clean
entitiesÏ†cd.
H02: there is no diï¬€erence in the ratios of connections from
clean entities to other clean entities (Ï†cc) and defective en-
titiesÏ†cd.
Hypotheses H01andH02are two sided and paired, since
each project has three unique values: Ï†dd,Ï†cc,a n dÏ†cd.T o
test the hypotheses, we apply paired Mann-Whitney U testusing the 95% conï¬dence level (i.e., Î±<0.05). We further
compute the Cliï¬€â€™s Î´[50] as the eï¬€ect size to quantify the
diï¬€erence. Both the Mann-Whitney U test and the Cliï¬€â€™s Î´
are non-parametric statistical methods, and do not requirea particular distribution of assessed variables. An eï¬€ect sizeis large, if Cliï¬€â€™s |Î´|â‰¥0.474 [50].
6.3 Empirical Findings
We observe that in general the connections between
defective and clean entities are weaker than the con-nection among defective entities and the connections
among clean entities. Table 5 presents the detailed val-
ues of our three measures ( i.e.,Ï†
cc,Ï†cd,a n dÏ†dd)f o re a c h
project. For instance, in project â€œEclipse JDT Coreâ€, theratio of connections among defective entities Ï†
dd=0.564.
The ratio of connections among clean entities Ï†cc=0.614.
These two ratios are signiï¬cantly greater than the ratio ofconnections between clean and defective entities which isÏ†
cd=0.365.
Defective entities have signiï¬cantly stronger connections
with other defective entities than with clean entities. The p-
value of the Mann-Whitney U test is is 4.20e-05, when com-
paring the ratios Ï†ddandÏ†cdacross all projects. The diï¬€er-
ence is large, as the corresponding Cliï¬€â€™s |Î´|is 0.654>0.474.
Similarly, clean entitieshave signiï¬cantly stronger connec-
tions with other clean entities than with defective entities
(i.e.,t h e p-value of the Mann-Whitney U test is 8.55e-06).
The diï¬€erence is also large, as Cliï¬€â€™s |Î´|is 0.769>0.474.
As a summary, our observation indicates that either defec-
tiveorcleanentitiesaresimilarintermsofmetricvalues, butTable 5: The values of Ï†cc,Ï†cd,a n dÏ†ddfor each
project. (Bold font highlights the minimum valueper row).
Dataset Project Ï†ccÏ†cdÏ†dd
AEEEMEclipse JDT Core 0.614 0.365 0.564
Equinox 0.694 0.443 0.470
Apache Lucene 0.554 0.374 0.556
Mylyn 0.575 0.442 0.489
Eclipse PDE UI 0.576 0.426 0.512
NASACM1 0.616 0.497 0.502
JM1 0.628 0.515 0.519
KC3 0.585 0.498 0.477
MC1 0.572 0.437 0.540
MC2 0.646 0.495 0.496
MW1 0.551 0.439 0.546
PC1 0.594 0.470 0.556
PC2 0.594 0.442 0.602
PC3 0.586 0.450 0.593
PC4 0.583 0.489 0.577
PC5 0.714 0.574 0.588
PROMISEAnt v1.7 0.522 0.398 0.606
Camel v1.6 0.487 0.455 0.481
Ivy v1.4 0.482 0.417 0.508
Jedit v4.0 0.504 0.402 0.536
Log4j v1.0 0.538 0.368 0.535
Lucene v2.4 0.542 0.438 0.459
POI v3.0 0.605 0.390 0.537
Tomcat v6.0 0.485 0.380 0.630
Xalan v2.6 0.540 0.439 0.438
Xerces v1.3 0.488 0.394 0.504
Median 0.576 0.439 0.536
defective and clean entities are less likely to experience sim-ilar metric values. In other words, there roughly exist twocommunities based on defect proneness. Entities within thesame community have stronger connections than cross com-munities. This may be the reason as to why the proposedconnectivity-based unsupervised classiï¬er (i.e.,o u rs p e c t r a l
classiï¬er) achieves empirically good results in defect predic-
tion.



	There roughly exist two communities of entities: a
defective community and a clean community of enti-
ties. Within-community connections are signiï¬cantlystronger than cross-community connections.
7. THREATS TO V ALIDITY
In this section, we describe the threats to validity of our
study under common guidelines by Yin [61].
Threats to conclusion validity concern the relation be-
tween the treatment and the outcome. The major threat isthat we only compare our approach with oï¬€-the-shelf clas-siï¬ers. Future work should explore state-of-the-art cross
project defect classiï¬ers. Unfortunately the implementation
of such specialized classiï¬ers are rarely available and oftenrequire a considerable amount of setup â€“ making them hardfor practitioners to easily adopt. Hence we chose to compareagainst commonly used and readily available classiï¬ers.
Threats to internal validity concern our selection of
subject systems and analysis methods. We select 26 projectsthat have been commonly used in the defect prediction lit-erature. These projects are from diï¬€erent domains, include
both open source and industrial projects, and have diï¬€er-
ent sets of metrics. However, evaluating our approach on a
317large scale of projects is always desirable. Nevertheless our
ï¬ndings raise a very poignant point about the importance ofexploring connectivity-based unsupervised classiï¬ers in fu-ture defection prediction research. Moreover, the simplicityof our approach makes exploring it in future studies as avery lightweight and simple step to perform.
Threats to external validity concern the possibility to
generalize our results. Our approach only requires softwaremetrics that can be computed in a standard way by publiclyavailable tools. However, only metrics that are collected in
the three data sets are applied in our experiments. Replica-tion studies using diï¬€erent sets of metrics may prove fruitful.
Threats to reliability validity concern the possibility of
replicating this study. All the three studied data sets are
publicly available. Moreover, the Rimplementation of our
approach is provided in Appendix A.
8. CONCLUSION
As new or small projects do not have suï¬ƒcient training
data, cross-project defect prediction has attracted great in-terest from both researchers and practitioners (e.g., [26, 27,
32, 34, 35, 41, 57, 58]). The major challenge in cross-projectdefect prediction is the heterogeneity between the trainingprojects and the target project ( e.g., diï¬€erent distributions
of metric values [13, 41] and diï¬€erent sets of metrics [40]).
This study brings a new insight to tackle this challenge
using connectivity-based unsupervised classiï¬ers. Unsuper-
vised classiï¬ers do not require any training data, and there-
fore have no issue of heterogeneity. Apart from distance-based unsupervised classiï¬ers ( e.g.,k-means clustering), the
connectivity-based unsupervised classiï¬ers assume that de-fective entities tend to cluster around the same area, a sim-
ilar intuition as the recent work on local prediction models
by Menzies et al.[35] and Bettenburg et al.[4].
Toevaluatetheperformanceof our proposedspectral clas-
siï¬er, we perform experiments using 26 projects from three
publicly available datasets ( i.e., AEEEM [14], NASA [42],
and PROMISE [29]). The results show that the proposed
connectivity-based unsupervised classiï¬er (i.e.,o u rs p e c t r a l
classiï¬er) achieves impressive performance in a cross-project
setting. Speciï¬cally, our spectral classiï¬er ranks as one of
the top classiï¬ers among ï¬ve supervised classiï¬ers ( e.g.,r a n -
dom forest) and ï¬ve unsupervised classiï¬ers ( e.g.,k-means).
In a within-project setting, our spectral classiï¬er ranks in
the second tier, the same as three widely used supervised
classiï¬ers ( e.g., logistic regression, logistic model tree, and
naive Bayes) with random forest as the only classiï¬er in the
ï¬rst tier.
As a summary, our contributions are as follows:
â€¢Demonstrating that connectivity-based unsu-pervised classiï¬cation (particularly via spectral
clustering) performs well in a cross-project set-
ting.Our experiments show that our connectivity-
based unsupervised classiï¬er (via spectral clustering)can achieve similar or better performance than severalcommonly used supervised and unsupervised classi-
ï¬ers. We believe that unsupervised classiï¬cation holds
greatpromiseindefectprediction, especiallyinacross-
project setting and for highly skewed within-project
settings.
â€¢Demonstrating the existence of two (defectiveand clean) separated communities of softwareentities based on the connectivity between theentities in each community. We believe that this
observation highlights the importance for the softwareengineering research community to explore more ad-vanced techniques for unsupervised defect predictioninstead of current strong reliance on supervised classi-ï¬ers.
APPENDIX
A. R IMPLEMENTATION OF OUR SPEC-
TRAL CLASSIFIER
In Listing 1, we present the Rimplementation of our spec-
tral classiï¬er.
Listing 1: Rimplementation of our approach.
1spectral _clustering _based _classifier <- function (A) {
2 # Normalize software metrics.
3 normA = apply (A, 2, function (x){(x- mean (x))/sd (x)})
4 # Construct the weighted adjacency matrix W.
5 W = normA % *% t(normA)
6 # Set all negative values to zero.
7 W[W<0] = 0
8 # Set the self-similarity to zero.
9 W=W- diag (diag (W))
10 # Construct the symmetric Laplacian matrix Lsym.
11 Dnsqrt = diag (1/sqrt(rowSums(W)))
12 I=diag (rep(1, nrow (W)))
13 Lsym = I- Dnsqrt % *%W%*%Dnsqrt
14 # Perform the eigendecomposition.
15 ret_egn = eigen (Lsym, symmetric=TRUE)
16 # Pick up the second smallest eigenvector.
17 v1 = Dnsqrt % *%ret_egn$vectors[, nrow (W)-1]
18 v1 = v1 / sqrt (sum(v1^2))
19 # Divide the data set into two clusters.
20 defect _proneness = (v1>0)
21 # Label the defective and clean clusters.
22 rs = rowSums(normA)
23 if(mean (rs[v1>0])< mean (rs[v1<0]))
24 defect _proneness = (v1<0)
25 # Return the defect proneness.
26 defect _proneness
27}
References
[1] G. Abaei, Z. Rezaei, and A. Selamat. Fault prediction by uti-
lizing self-organizing Map and Threshold. In 2013 IEEE In-
ternational Conference on Control System, Computing and
Engineering , pages 465â€“470. IEEE, Nov. 2013.
[2] C. C. Aggarwal, editor. Data Classiï¬cation: Algorithms and
Applications . CRC Press, 2014.
[3] O. F. Arar and K. Ayan. Software defect prediction us-
ing cost-sensitive neural network. Applied Soft Computing ,
33:263â€“277, Aug. 2015.
[4] N. Bettenburg, M. Nagappan, and A. E. Hassan. Think lo-
cally, act globally: Improving defect and eï¬€ort predictionmodels. In Proceedings of the 9th IEEE Working Confer-
ence on Mining Software Repositories, MSR â€™12, pages 60â€“
69, June 2012.
[5] P. Bishnu and V. Bhattacherjee. Software fault predic-
tion using quad tree-based k-means clustering algorithm.IEEE Transactions on Knowledge and Data Engineering ,
24(6):1146â€“1150, June 2012.
[6] P. Blanchard and D. Volchenkov. Mathematical Analysis of
Urban Spatial Networks. Springer Berlin Heidelberg, Heidel-
berg, Germany, 2009.
318[7] S. P. Borgatti and M. G. Everett. Models of core/periphery
structures. Social Networks, 21(4):375 â€“ 395, 2000.
[8] L. C. Briand, W. L. Melo, and J. W Â¨ust. Assessing the appli-
cabilityoffault-pronenessmodelsacrossobject-orientedsoft-
ware projects. IEEE Transactions on Software Engineering ,
28(7):706â€“720, July 2002.
[9] C. Catal, U. Sevim, and B. Diri. Metrics-driven software
quality prediction without prior fault data. In S.-I. Ao andL. Gelman, editors, Electronic Engineering and Computing
Technology , volume 60 of Lecture Notes in Electrical Engi-
neering, pages 189â€“199. Springer Netherlands, 2010.
[10] C. Catal, U. Sevim, and B. Diri. Practical development
of an Eclipse-based software fault prediction tool using
Naive Bayes algorithm. Expert Systems with Applications,
38(3):2347â€“2353, Mar. 2011.
[11] E. Ceylan, F. Kutlubay, and A. Bener. Software De-
fect Identiï¬cation Using Machine Learning Techniques. In
32nd EUROMICRO Conference on Software Engineering
and Advanced Applications (EUROMICROâ€™06), pages 240â€“
247. IEEE, 2006.
[12] L. Chen, B. Fang, Z. Shang, and Y. Tang. Negative sam-
ples reduction in cross-company software defects prediction.
Information and Software Technology, 62:67â€“77, June 2015.
[13] A.CruzandK.Ochimizu. Towardslogisticregressionmodels
for predicting fault-prone code across software projects. In
Proceedings of the 3rd International Symposium on Empiri-
cal Software Engineering and Measurement, pages 460â€“463,Oct. 2009.
[14] M. Dâ€™Ambros, M. Lanza, and R. Robbes. An extensive com-
parison of bug prediction approaches. In Proceedings of the
7th IEEE Working Conference on Mining Software Reposi-tories, pages 31â€“41. IEEE CS Press, May 2010.
[15] M. Dâ€™Ambros, M. Lanza, and R. Robbes. Evaluating defect
prediction approaches: a benchmark and an extensive com-parison. Empirical Software Engineering, 17(4-5):531â€“577,
Aug. 2012.
[16] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer,
and R. Harshman. Indexing by latent semantic analysis.Journal of the American Society for Information Science ,
41(6):391â€“407, 1990.
[17] I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means: Spec-
tral clustering and normalized cuts. In Proceedings of the
Tenth ACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 551â€“556. ACM,
2004.
[18] M. Fagan. Design and code inspections to reduce errors in
program development. IBM Systems Journal , 38(2.3):258â€“
287, 1999.
[19] J. E. Gaï¬€ney. Estimating the number of faults in code.
IEEE Transactions on Software Engineering , SE-10(4):459â€“
464, July 1984.
[20] B.Ghotra, S.McIntosh, andA.E.Hassan. Revisitingtheim-
pact of classiï¬cation techniques on the performance of defectprediction models. In Proceedings of the 37th IEEE Interna-
tional Conference on Software Engineering,v o l u m e1 ,p a g e s
789â€“800, May 2015.
[21] F. Gorunescu. Data mining concepts, models and techniques .
Springer, Berlin, 2011.
[22] D. Gray, D. Bowes, N. Davey, Y. Sun, and B. Christianson.
The misuse of the nasa metrics data program data sets for
automated software defect prediction. In Proceedings of the
15th Annual Conference on Evaluation Assessment in Soft-
ware Engineering (EASE 2011) , pages 96â€“103, April 2011.[23] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell. A
systematic literature review on fault prediction performancein software engineering. IEEE Transactions on Software En-
gineering, 38(6):1276â€“1304, Nov. 2012.
[24] J. Han, M. Kamber, and J. Pei. Data Mining: concepts and
techniques. Morgan Kaufmann, Boston, 3 edition, 2012.
[25] A. E. Hassan. Predicting faults using the complexity of code
changes. In Proceedings of the 31st IEEE International Con-
ference on Software Engineering, pages 78 â€“88, 2009.
[26] Z. He, F. Peters, T. Menzies, and Y. Yang. Learning from
op
en-source projects: An empirical study on defect predic-
tion. In ACM / IEEE International Symposium on Empir-
ical Software Engineering and Measurement, pages 45â€“54,
Oct. 2013.
[27] Z. He, F. Shu, Y. Yang, M. Li, and Q. Wang. An inves-
tigation on the feasibility of cross-project defect prediction.
Automated Software Engineering , 19(2):167â€“199, June 2012.
[28] E. G. Jelihovschi, J. C. Faria, and I. B. Allaman. Scot-
tknott: A package for performing the scott-knott clusteringalgorithm in r. Trends in Applied and Computational Math-
ematics, 15(1):3â€“17, 2014.
[29] M. Jureczko and L. Madeyski. Towards identifying software
project clusters with regard to defect prediction. In Pro-
ceedings of the 6th International Conference on PredictiveModels in Software Engineering , pages 9:1â€“9:10, 2010.
[30] B. Kitchenham, L. Pickard, and S. Linkman. An evalua-
tion of some design metrics. Software Engineering Journal ,
5(1):50â€“58, Jan 1990.
[31] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch. Bench-
marking classiï¬cation models for software defect prediction:
A proposed framework and novel ï¬ndings. IEEE Transac-
tions on Software Engineering (TSE) , 34(4):485â€“496, 2008.
[32] M. Li, H. Zhang, R. Wu, and Z.-H. Zhou. Sample-based
software defect prediction with active and semi-supervised
learning. Automated Software Engineering , 19(2):201â€“230,
June 2012.
[33] U. Luxburg. A tutorial on spectral clustering. Statistics and
Computing , 17(4):395â€“416, Dec. 2007.
[34] Y. Ma, G. Luo, X. Zeng, and A. Chen. Transfer learning for
cross-company software defect prediction. Information and
Software Technology , 54(3):248â€“256, Mar. 2012.
[35] T. Menzies, A. Butcher, A. Marcus, T. Zimmermann, and
D. Cok. Local vs. global models for eï¬€ort estimation and de-fect prediction. In Proceedings of the 2011 26th IEEE/ACM
International Conference on Automated Software Engineer-
ing, ASE â€™11, pages 343â€“351. IEEE Computer Society, 2011.
[36] B. Mohar. The laplacian spectrum of graphs. In Graph
Theory, Combinatorics, and Applications , pages 871â€“898.
Wiley, 1991.
[37] R. Moser, W. Pedrycz, and G. Succi. A comparative analysis
of the eï¬ƒciency of change metrics and static code attributes
fordefectprediction. In Proceedings of the 30th International
Conference on Software Engineering, pages 181â€“190. ACM,May 2008.
[38] R. Mullen and S. Gokhale. Software Defect Rediscoveries: A
Discrete Lognormal Model. In Proceedings of the 16th IEEE
International Symposium on Software Reliability Engineer-
ing, pages 203â€“212. IEEE, 2005.
319[39] J. Nam and S. Kim. Clami: Defect prediction on unlabeled
datasets. In Proceedings of the 30th IEEE/ACM Interna-
tional Conference on Automated Software Engineering,A S E
â€™15, 2015.
[40] J. Nam and S. Kim. Heterogeneous defect prediction. In Pro-
ceedings of the European Software Engineering Conferenceand the ACM SIGSOFT Symposium on the Foundations ofSoftware Engineering, ESEC/FSE â€™15, 2015.
[41] J. Nam, S. J. Pan, and S. Kim. Transfer defect learning. In
Proceedings of the 2013 International Conference on Soft-ware Engineering , pages 382â€“391. IEEE Press, 2013.
[42] NASA. Metrics Data Program. http://openscience.us/repo/
defect/mccabehalsted, 2015. [Online; accessed 25-August-
2015].
[43] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clus-
tering: Analysis and an algorithm. In Advances in Neural
Information Processing Systems, pages 849â€“856. MIT Press,
2001.
[44] M. Ohlsson and P. Runeson. Experience from replicating
empirical studies on prediction models. Proceedings of the
8th IEEE Symposium on Software Metrics , pages 217â€“226,
2002.
[45] T. Ostrand and E. Weyuker. On the automation of software
fault prediction. In Testing: Academic and Industrial Con-
ference - Practice And Research Techniques, 2006. TAICPART 2006. Proceedings, pages 41â€“48, Aug. 2006.
[46] L. Pelayo and S. Dick. Applying novel resampling strate-
gies to software defect prediction. In Annual Conference of
the North American Fuzzy Information processing Society,
NAFIPS â€™07, pages 69â€“72, June 2007.
[47] M. Pinzger, N. Nagappan, and B. Murphy. Can developer-
module networks predict failures? In Proceedings of the 16th
ACM SIGSOFT International Symposium on Foundationsof Software Engineering, SIGSOFT â€™08/FSE-16, pages 2â€“12,
N e wY o r k ,N Y ,U S A ,2 0 0 8 .A C M .
[48] R. Premraj and K. Herzig. Network versus code metrics
to predict defects: A replication study. In 2011 Interna-
tional Symposium on Empirical Software Engineering and
Measurement (ESEM) , pages 215â€“224, 2011.
[49] R. Rana, M. Staron, C. Berger, J. Hansson, M. Nilsson, and
W. Meding. The Adoption of Machine Learning Techniquesfor Software Defect Prediction: An Initial Industrial Val-idation. Knowledge-based Software Engineering, JCKBSE
2014, 466:270â€“285, 2014.
[50] J. Romano, J. D. Kromrey, J. Coraggio, and J. Skowronek.
Appropriatestatisticsforordinalleveldata: Shouldwereally
be using t-test and cohenâ€™s d for evaluating group diï¬€erences
on the nsse and other surveys? In Annual Meeting of the
Florida Association of Institutional Research , pages 1â€“33,
Feb. 2006.
[51] M. Shepperd, Q. Song, Z. Sun, and C. Mair. Data qual-
ity: Some comments on the nasa software defect datasets.
IEEE Transactions on Software Engineering , 39(9):1208â€“
1215, Sept 2013.
[52] D. J. Sheskin. Handbook of Parametric and Nonparamet-
ric Statistical Procedures, Fourth Edition . Chapman & Hal-
l/CRC, Jan. 2007.
[53] J. Shi and J. Malik. Normalized cuts and image segmenta-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 22(8):888â€“905, Aug. 2000.[54] F. Shull, V. Basili, B. Boehm, A. Brown, P. Costa, M. Lind-
vall, D. Port, I. Rus, R. Tesoriero, and M. Zelkowitz. What
we have learned about ï¬ghting defects. In Proceedings of the
8th IEEE Symposium on Software Metrics , pages 249â€“258,
2002.
[55] G. Tassey. The economic impacts of inadequate infrastruc-
ture for software testing. Technical Report Planning Report
02-3, National Institute of Standards and Technology, May
2002.
[56] A. Tosun, A. Bener, B. Turhan, and T. Menzies. Practi-
cal considerations in deploying statistical methods for de-
fect prediction: A case study within the Turkish telecommu-nications industry. Information and Software Technology,
52(11):1242â€“1257, Nov. 2010.
[57] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano.
On the relative value of cross-company and within-companydata for defect prediction. Empirical Software Engineering ,
14(5):540â€“578, Oct. 2009.
[58]
S. Watanabe, H. Kaiya, and K. Kaijiri. Adapting a fault
predictionmodeltoallowinterlanguagereuse. In Proceedings
of the 4th International Workshop on Predictor Models inSoftware Engineering, PROMISE â€™08, pages 19â€“24. ACM,2008.
[59] A. R. Webb and K. D. Copsey. Statistical Pattern Recogni-
tion, Third Edition . John Wiley & Sons, Inc., 2011.
[60] B. Yang, Q. Yin, S. Xu, and P. Guo. Software quality pre-
diction using aï¬ƒnity propagation algorithm. In Proceedings
of the 2008 IEEE International Joint Conference on Neural
Networks. IJCNN 2008 , pages 1891â€“1896, June 2008.
[61] R. K. Yin. Case Study Research: Design and Methods -
Third Edition. SAGE Publications, 3 edition, 2002.
[62] F. Zhang, A. Mockus, I. Keivanloo, and Y. Zou. Towards
building a universal defect prediction model. In Proceedings
of the 11th Working Conference on Mining Software Repos-
itories, MSR â€™14, pages 41â€“50, Piscataway, NJ, USA, 2014.
IEEE Press.
[63] F. Zhang, A. Mockus, I. Keivanloo, and Y. Zou. Towards
building a universal defect prediction model with rank trans-formed predictors. Empirical Software Engineering ,p a g e s
1â€“39, 2015.
[64] F. Zhang, A. Mockus, Y. Zou, F. Khomh, and A. E. Hassan.
How does context aï¬€ect the distribution of software main-
tainability metrics? In Proceedings of the 29th IEEE Inter-
national Conference on Software Maintainability, ICSM â€™13,pages 350 â€“ 359, 2013.
[65] S. Zhong, T. Khoshgoftaar, and N. Seliya. Unsupervised
learning for expert-based software quality estimation. InProceedings of the 8th IEEE International Symposium on
High Assurance Systems Engineering , pages 149â€“155, Mar.
2004.
[66] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy. Cross-project defect prediction: a large scale
experiment on data vs. domain vs. process. In Proceedings
of the the 7th joint meeting of the European software engi-neering conference and the ACM SIGSOFT symposium onThe foundations of software engineering, ESEC/FSE â€™09,
pages 91â€“100, New York, NY, USA, 2009. ACM.
320