Replay without Recording of Production Bugs for Service
Oriented Applications
Nipun Arora
Dropbox
New York, NY, USA
nipun@dropbox.comJonathan Bell
George Mason University
Fairfax, VA, USA
bellj@gmu.eduFranjo Ivančić
Google
New York, NY, USA
ivancic@google.com
Gail Kaiser
Columbia University
New York, NY, USA
kaiser@cs.columbia.eduBaishakhi Ray
Columbia University
New York, NY, USA
rayb@cs.columbia.edu
ABSTRACT
Short time-to-localize and time-to-fix for production bugs is ex-
tremely important for any 24x7 service-oriented application (SOA).
Debuggingbuggybehaviorindeployedapplicationsishard,asitre-
quirescarefulreproductionofasimilarenvironmentandworkload.PriorapproachesforautomaticallyreproducingproductionfailuresdonotscaletolargeSOAsystems.Ourkeyinsightisthatformany
failures in SOA systems (e.g., many semantic and performance
bugs), a failure can automatically be reproduced solely by relaying
network packets to replicas of suspect services, an insight that we
validatedthroughamanualstudyof16realbugsacrossfivedifferent
systems. This paper presents Parikshan , an application monitoring
framework that leverages user-space virtualization and network
proxytechnologiestoprovideasandbox“debug”environment.In
this “debug” environment, developers are free to attach debuggers
andanalysistoolswithoutimpactingperformanceorcorrectnessofthe production environment. In comparison to existing monitoring
solutionsthatcanslowdownproductionapplications, Parikshan
allows application monitoring at significantly lower overhead.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
Fault reproduction, live debugging
ACM Reference Format:
Nipun Arora, Jonathan Bell, Franjo Ivančić, Gail Kaiser, and Baishakhi
Ray. 2018. Replay without Recording of Production Bugs for Service Ori-
ented Applications. In Proceedings of the 2018 33rd ACM/IEEE International
Conference on Automated Software Engineering (ASE ’18), September 3–7, 2018, Montpellier, France. ACM, New York, NY, USA, 12pages.https:
//doi.org/10.1145/3238147.3238186
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.32381861 INTRODUCTION
Modern-daydevicesrelyoninteractiveandresponsiveapplications
that provide a rich interface to end-users. Behind the scenes of
theseapplicationsareoftenseveralservice-orientedapplications
working in concert to provide the final service. Such services in-
clude storage, compute, queuing, synchronization, and application-
layerfunctionality.Applicationsfollowingsuchservice-oriented
architectures(SOA)requiretheorchestrationofavarietyofcompo-
nents.Rapidresolutionofincident(error/alert)management[ 58]in
SOA[17,20,55,69]isextremelyimportant,asfailureofoneservice
canleadtocascadingfailureofthewholesystem.Thelargescale
ofsuchsystemsmeansthatanydowntimehassignificantimpact
on the “user experience, a product’s image, and a company’s brand
and, potentially, revenue” [22].
DebuggingproductionbugsinSOAisnotoriouslychallengingbe-
cause(1)itrequirescarefulreproductionofasimilarlyorchestratedenvironmentandworkloadsothatdeveloperscanidentifytheroot
cause,and(2)bugsneedtoberesolvedASAPtoensureminimum
downtime. Worse still, a single observed failure in one component
might in fact be due to several latent bugs in other components.
Thus, localizing a single bug might require understanding complex
interactionsacrossmultiplecomponentsrunningondifferenthosts.
Debuggingbecomesevenmorefrustratingfor non-crashing bugs,
suchasperformancebugs,semanticbugs,andresourceleaks,which
tendtoariseduetoaccumulatedstate,makingthemparticularly
complicated to reproduce in testing environments.
Most debugging techniques are not suitable for on-the-fly de-
bugging of SOA production bugs. For instance, approaches likerecord-and-replay (R&R) typically collect execution trace infor-mation from the production environment and use that trace to
reproduce the bug in a testing environment where developers can
usetraditionaldebuggingtools.WhileR&Rcanbeveryeffectivein
reproducing a bug, if sufficient execution traces can be captured to
allowtheentireapplicationexecutiontobefaithfullyreproduced
inadebuggingenvironment[ 5,27,53],thiscancausesignificant
performanceoverhead.Despitemuchworktowardsoptimizingthetrace data captured, overheads imposed by such tracing can still be
unacceptable for SOA production use: the overhead can balloon up
to 2-10x overhead [71, 86].
In contrast, some monitoring systems capture only very min-
imal, high level information, for instance, collecting existing log
452
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Nipun Arora, Jonathan Bell, Franjo Ivančić, Gail Kaiser, and Baishakhi Ray
information and from it building a model of the system and its
irregularities [ 10,29,32,48]. While these systems impose almost
no overhead on the production system being debugged (since they
simplycollectloginformationalreadybeingcollected,orhavelight-
weight monitoring), they can not automatically reproduce all bugs,
and hence may be limited in their utility.
Thus, for on-the-fly debugging of SOA production bugs, we
require a solution which allows developers to observe, instrument,
and debug the system components in parallel with the production.
Inthispaper,weproposealivedebuggingenvironmentforSOA,
whichallowsdebuggersafreereigntodebug,withoutimpacting
theuser-facingapplication.Bymanuallystudying220realworld
SOA production bugs, we observe that it is possible to successfully
replay a SOA bug solely by capturing network transmissions. For
these bugs, conventional R&R capture of very low-level sources of
non-determinism(e.g.,threadscheduling,generalsystemcalls)is
unnecessary to automatically reproduce the buggy execution.
Guided by this insight, we have developed Parikshan1, a “live
debugging” architecture that supports online debugging of produc-
tion SOA applications withoutdegrading access to the app during
debugging. Our approachleverages technologies commonly used
inSOAsystems,suchaslightweightcontainers,toautomatically
createsandboxeddebuggingenvironmentsthatmirrortheirproduc-
tion environments. Each replica is kept isolated so developers can
modifyitwithoutfearofimpactingtheproductionsystem. Parik-
shanreplicates all network inputs flowing to the corresponding
productioncontainer,bufferingandfeedingthem(withoutblock-
ing) to the debugging container. Within the debug environment,
developersarefreetouseheavyweightinstrumentation,thatwould
not be suitable in a production environment, to diagnose the fault.
This approach could be used offline, recording rather than replicat-
ingtrafficandstoringtheclonedreplicasfor later,but Parikshan
focuses on helping developers debug faults online— as they occur
in production systems. The key benefits of Parikshan are:
Verylowoverheadinproduction: Parikshandoes imposeashort
pausewhendebugenvironmentsarelaunched,butthendevelopersare free to use very high overhead debugging tools (e.g. gdb) in the
debugenvironment, yetthe productionenvironment continuesto
service requests at near-native speed.Captures large-scale context:
Parikshan captures the context of
largescale,longrunningproductionsystemsbycloningservices
in situ, creating sandbox environments. Capturing such state isextremely difficult in conventional testing environments as they
wouldneedlong-runningtestinputsequencesandlargetest-clusters.
Weevaluate Parikshan bysuccessfullyreplaying16real-world
bugs, finding that Parikshan imposed very low overhead. Manually
reproducing real-world bugs is a very time-intensive process, and
hence, to lend additional validity to this evaluation of 16 bugs, we
categorized 220 additional bugs from three applications, finding
that most were similar in nature to the 16 that we reproduced.
2 MOTIVATION
To better understand what kinds of bugs occur in production SOA
systemsandhowtheycanbestbedebugged,westudied220real-
worldproductionbugsfromthreeSOAapplications:Apache,MySQL
1Parikshan is theSanskritword for testing.Table 1: Survey and classification of bugs
Category Apache MySQL HDFS Total
Performance 3 10 6 19Semantic 37 73 63 173Concurrency 3 7 6 16
Resource Leak 5 6 1 12
Total 48 96 76 220
and HDFS. We searched issue trackers for each project, ignoring
bugsinnon-productioncomponents.Wealsofilteredbugreports
that were feature requests or did not include a triggering test—our
goalwastofocusonlyonbugsthataroseduringproductionsce-
narios. To understand the nature of these bugs, we classified them
(e.g.basedondescriptionandfix)intothefollowingcategories:Per-
formance, Semantic, Concurrency, and Resource Leak, as shown in
Table1.Completedetailsonhowweselectedandcategorizedthese
bugs(alongwithalistingofthebugsthemselves)areavailablein
the full version of this work [7].
One of the key insights from this study is that most of the bugs
we examined (93%) are deterministic in nature (everything but
concurrencybugs),andinfact,mostaresemanticbugs(80%).For
manyofthem,theapplicationbehaviorisincorrect(e.g.itprovides
the wrong output to the user), but there is no error or warning
generated in the system log(s). To trigger these bugs, we only need
tocapture thestateofthe systemandthe inputthatresultsin the
bug,and notallnon-deterministicevents(e.g.threadscheduling).
Wecapturethestateofthesystemthroughlivecloning,replicating
the entire state of each production container that is relevant to the
bug. To capture the inputs to the system that result in the bug, we
replicate all network inputs that enter the production containers.
2.1 Sample Scenario
Consider the complex multi-tier service-oriented system shown
inFigure 1,whichcontainsseveralinteractingserviceseachrun-
ning in its own container (segregating components in separatecontainers isgenerally considereda best practice[
4]). Operators
might observe unusual memory usage in the Glassfish application
server, causing error logs to be generated in the Nginx web server.
Operatorssurmisethereisapotentialmemoryleak/allocationprob-
lem. However, with monitoring restricted to avoid performance
Cloned test contain ers &
network duplicationDebug 
outputTier 1 Tier 2 Tier 3 Tier 4 Tier 5 Tiers n
User observes 
error & 
creates 
sandbox
Production 
System
Online Debug 
System
Figure1:Workflowof Parikshan inamulti-tiersystemwith
interacting services. When the administrator observes er-
rors in two of the tiers, she can create a sandboxed debugenvironment.
453
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Replay without Recording of Production Bugs for Service Oriented Applications ASE ’18, September 3–7, 2018, Montpellier, France
Asynchronous
ForwarderDownstream
Components
Buffer ManagerProduction 
Container
Debug 
ContainerPass-through
Forwarder
Upstream 
ComponentsPass-through
Forwarder
Asynchronous
ForwarderNetwork Duplicator Network Aggregator
Buffer Manager
Dummy Reader Dummy ReaderLive 
ClonedClients/User 
Requests etc.Storage/Datab ase Services etc.
Process P1Process P2
Process P3Process P4Process P1
Process P2
Process P3Process P4
Legend: DuplicatorLegend: AggregatorClone Manager
Figure2:Highlevelarchitectureof Parikshan ,showingthemaincomponents:NetworkDuplicator,NetworkAggregator,and
CloningManager.Thereplica(debugcontainer)iskeptinsyncwiththemaster(productioncontainer)throughnetwork-levelrecord and replay. In our evaluation, we found that this light-weight procedure was sufficient to reproduce many real bugs.
penalties on production, they can only go so far. Extensive trace
collectionintheproductionenvironmentforreliableR&Rdebug-
ging is also not feasible as that will hurt the system’s performance.
Thus, trouble tickets are typically generated for such problems, to
be debugged offline. However, repr oducing similar scenario offline
involving so many SOA applications is challenging.
Weobservethatitispossibletoreplaythebugfaithfully(without
hurting the performance of the system), by simply cloning the
potentiallybuggycontainersandthensendingthesamenetwork
inputsastheproductioncontainerstothesereplicas.Wedesigna
faultreproductionframework, Parikshan ,basedonthisobservation.
Based on the erroneous behavior, the administrators can choose
the Nginx and Glassfish containers for cloning and debugging, ask-
ingParikshan tocreatethenew Nginx-debug andGlassfish-debug
containers. Parikshan ’s network duplication mechanism ensures
thatthedebugreplicasreceivethesameinputsastheproduction
containers and that the production containers continue service
withoutfurtherinterruption.Oncethedebugenvironmentiscre-
ated,Parikshan canbeusedwithanyexistingautomatedormanual
debugging tools that developers may wish to use. This separation
ofproductionanddebuggingenvironmentallowsthedevelopers
touseheavierdynamicinstrumentationfordeeperdiagnosisinthedebug containers without fearof disrupting production. Since eachreplicaisclonedfromitsoriginal“buggy” productioncontainer ,itex-
hibitsthesamepersistentmemoryleaksand/orlogicalerrors.Note
thatweprimarilyenvision Parikshan beingappliedtoreproduce
application bugs, and not to reproduce security attacks.
Debugcontainerscanbecreatedandrecreatedatanytime:either
at the start of execution or at any point during execution, allowing
post-facto analysis of the bugs. Within debug replicas, analysistools that slow down the buggy execution may be used without
impacting production performance. Hence, developers can use any
oftheirpreferreddebuggingapproachesinthesereplicasinorder
to determine the cause of the failure.
3 DESIGN
SinceParikshan isbuiltforon-the-flydebuggingofSOAproduction
bugs, its design is guided by the following principles.
(1)Real-Time Insights: Observing application behavior as a bug
presents itself will allow for quick insights and shorter timetodebug.Developersshouldbeabletomonitorsystemstatus
as they debug.
(2)SanityandCorrectness :Ifdebuggingistobedoneinarun-
ningapplicationwithrealusers,itshouldbedonewithout
impactingtheoutcomeoftheprogram.Theframeworkmust
ensurethatanychangestotheapplication’sstateorto the
environment does not impact the user-facing production
application.
(3)Language/Application Agnostic : The mechanisms presented
should be applicable to any language, and any service ori-
entedapplication(ourscopeislimitedtoSOAarchitectures).
(4)PerformanceImpact :Theenduserofasystemthatisbeing
debugged should not observe any noticeable performance
degradation.Debuggingmustbeunobtrusivetotheenduser,bothintermsoffunctionalityandanyconfigurationorsetup,
in addition to performance.
(5)Service Interruption : Since we are focusing our efforts on
service oriented systems, any solution should ensure that
thereisnoimpactontheservice,andtheuserfacingservice
should not be interrupted.
Figure2shows the architecture of Parikshan when applied to a
single mid-tier application server. Parikshan consists of 3 modules:
(1)Clone Manager : manages live cloning between the production
containersandthedebugreplicas.Livecloningallowsdevelopers
todecidetocreatenewdebugenvironmentsatanytimewhilean
applicationisrunning.(2) NetworkDuplicator :managesnetwork
trafficduplicationfromdownstreamserverstoboththeproductionanddebugcontainers.(3)
NetworkAggregator :managesnetwork
communicationfromtheproductionanddebugcontainerstoup-
streamservers.Thenetworkduplicatoralsoperformstheimportant
task of ensuring that the production and debug container execu-
tions do not diverge. The duplicator and aggregator can be used to
target multiple connected tiers of a system by duplicating traffic at
thebeginningandendofaworkflow. Parikshan candynamically
detectwhichportsanapplicationusesandpromptthedeveloperto
choose if traffic on each port should be aggregated or duplicated.
3.1 Clone Manager
Parikshan uses live cloning (a variant of live migration [ 23,37,62])
tospawndebugcontainersthatexactlymirrorthecorresponding
production services without disconnecting any clients or stopping
454
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Nipun Arora, Jonathan Bell, Franjo Ivančić, Gail Kaiser, and Baishakhi Ray
any processes, incurring a negligible suspend time. The challenge
here is to manage two containers with the same identities in the
networkandapplicationdomain.Thisisimportantastheoperating
systemandtheapplicationprocessesrunninginitmaybeconfig-
ured with IP addresses that cannot be changed on the fly. Hence,
the same network identifier should map to two separate addresses,
and enable communication with no problems or slowdowns.
Parikshan supports two high-level modes of live cloning:
Internal Cloning :Inthismode,weallocatetheproductionand
debugcontainerstothesamephysicalhost.Thismodetakesless
time to perform the initial clone (since the container does not need
tobetransferredoverthenetwork),andmaybemorecost-effective
since it does not require additional machines. However, co-hosting
the debug and production containers could potentially decrease
performanceoftheproductioncontainerduetoresourcecontention.Networkidentitiesinthismodearemanagedbyencapsulatingeach
container in separate network namespaces [ 2]. This allows both
containers to have the same IP address with different interfaces.
Theduplicatoristhenabletocommunicatetoboththesecontainers
with no networking conflict.
External Cloning :Inthismode,weprovisionanexternalserver
as the host of our debug container (this server can host more than
one debug container). While this mechanism can have a higher
overheadintermsofsuspendtimeandrequiresprovisioninganad-
ditional host, the advantage of this mechanism is that once cloned,
the debug container is totally separate and will not impact theperformance of the production container. Network identities inexternal mode are managed using NAT (network address trans-lation) in both host machines. Hence both containers can have
thesameaddresswithoutanyconflict.Currently,weassumethat
each production container has at most a single debug replica, inthefuturewewillconsidersupportingmultiplereplicasforeach
production container, which might make it easier for developers to
apply multiple debugging techniques simultaneously.
The suspend time of cloning depends on the operations happen-
ingbetweenstep2andstep4(thefirstandthesecondrsync):moreoperationswillresultinmoremodifiedpagesofmemory,impacting
the amount of memory that needs to be later copied. This suspend
time can be viewed as an amortized cost in lieu of instrumentation
overhead. We evaluate the performance of live cloning in §4.1.
3.2 Network Duplicator and Aggregator
Oncethedebugcontainerisprovisioned, Parikshan keepsthedebug
container in sync with the production container by duplicating
networktrafficintothecontainer.Thenetworkproxyduplicator
andaggregatorarecomposedofthefollowinginternalcomponents:
•SynchronousPassthrough :Thesynchronouspassthroughtakes
input from a source port and forwards it to a destination port.
Thepassthroughisusedforcommunicationfromtheproduction
container out to other components (which are not duplicated).
•AsynchronousForwarder :Theasynchronousforwardertakes
input from a source port and forwards it to both a destination
portandtoaninternalbuffer.Forwardingtothebufferisdonein
a non-blocking manner, so as to not delay network forwarding.
•BufferManager :ManagesaFIFOqueuefordatakeptinternally
in the proxy for the debug container. It records the incoming
data, and forwards it to a destination port.•Dummy Reader : This is a standalone daemon that reads and
drops packets from a source port.
Proxy Network Duplicator: All requests inbound to the produc-
tioncontainerareduplicatedandforwardedtothedebugcontainer.
Asimplenetworkproxyorportmirrorwouldduplicatealltraffic
from the production container to the debug container but would
not be able tocope with the different execution speeds of the two
containers, and would not be able to correctly filter responses from
thedebugcontainerbacktotheclient(whichshouldonlyreceive
responses from the production container).
Our solution is a customized TCP level proxy. This proxy du-
plicatesnetworktraffictothedebugcontainerwhilemaintaining
theTCPsessionandstatewiththeproductioncontainer.Sinceit
works at the TCP/IP layer, applications are completely oblivious to
it. Figure 2shows how our proxy works: each incoming connec-
tion isforwarded toboth theproduction container andthe debug
container. This is a multi-process job involving 4 parallel processes
(P1-P4): In P1, the asynchronous forwarder sends data from client
totheproductionservice,whilesimultaneouslysendingittothe
buffermanagerinanon-blockingsend.Thisensuresthatthereisnodelayintheflowtotheproductioncontainerbecauseofslow-down
in the debug container. In P2, the pass-through forwarder readsdata from the production and sends it to the client (downstreamcomponent). Process P3 then sends data from Buffer Manager to
thedebugcontainer,andProcessP4usesadummyreadertoread
from the production container and drops all the packets.
The above strategyallows for non-blocking packetforwarding
and enables a key feature of Parikshan , whereby it avoids slow-
downsinthedebugcontainertoimpacttheproductioncontainer
using an in-memory buffer (discussed further in §3.3).
ProxyNetworkAggregator: Whilethenetworkduplicatordupli-
catesincomingrequests,thenetworkaggregatormanagesincoming
“responses” for requests sent from the debug container. In addition
to dropping duplicate responses to clients, the network aggregatormust also drop duplicate requests to backend servers. For instance,
processing a request in a mid-tier server might require insertingor deleting data from a backend database: since both the produc-
tionanddebugcontainerswillprocessthisrequest,therewillbe
duplicate requests sent to these backend services, leading to an in-
consistent state. The “proxy aggregator” module stubs the requests
fromaduplicatedebugcontainerbyreplayingtheresponsessent
totheproductioncontainertothedebugcontaineranddropping
all packets sent from it to upstream clients.
AsshowninFigure 2,whenanincomingrequestcomestothe
aggregator, it first checks if the connection is from the production
containerordebugcontainer.InprocessP1,theaggregatorforwards
the packets to the upstream component using the pass-through
forwarder. In P2, the asynchronous forwarder sends the responses
from the upstream component to the production container, and
sends theresponse in anon-blockingmanner to theinternal queue
in the buffer manager. Once again this ensures no slow-down in
theresponsessenttotheproductioncontainer.Thebuffermanager
thenforwardstheresponsestothedebugcontainer(ProcessP3).
Finally, in process P4 a dummy reader reads all the responses from
the debug container and discards them.
455
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Replay without Recording of Production Bugs for Service Oriented Applications ASE ’18, September 3–7, 2018, Montpellier, France
Weassumethattheproductionandthedebugcontainerarein
the same state, and are sending the same requests. Hence, sending
the corresponding responses from the FIFO queue instead of the
backend ensures: (a) all communications to and from the debug
container are isolated from the rest of the network, (b) the de-
bug container gets a logical response for all it’s outgoing requests,
making forward progress possible, and (c). similar to the proxy
duplicator,thecommunicationsfromtheproxytointernalbufferis
non-blocking to ensure no overhead on the production container.
3.3 Debug Window
Parikshan ’s asynchronous forwarder uses an internal buffer to en-
sure that incoming requests proceed directly to the production
container without any latency, regardless of the speed at whichthe debug replica processes requests. The incoming request ratetothebufferisdependentontheuser,andislimitedbyhowfast
the production container manages the requests (i.e. the production
containeristherate-limiter).Theoutgoingratefromthebufferis
dependent on how fast the debug container processes the requests.
As instrumentation overhead increases, the incoming rate of re-
quests may eventually exceed the transaction processing rate in
the debug container, leading to a buffer overflow. We call the time
perioduntilbufferoverflowhappensthe debugwindow.Oncethe
bufferhasoverflowed,thedebugcontainermaybeoutofsyncwithproduction,andafreshdebugcontainerwouldneedtobelaunched.
The debug window size depends on the size of the buffer, the
incoming request rate, the overhead of any debugging activitiesand the application behavior, in particular how it launches TCP
connections. Parikshan generatesapipebufferforeachTCPconnect
call, and the number of pipes are limited to the maximum number
of connections allowed in the application. Hence, buffer overflows
happen only if the requests being sent in the same connectionoverflow the queue. For webservers and application servers, the
debugging window size is generally not a problem, as each request
isanew“connection.”Thisenables Parikshan totoleratesignificant
instrumentation overhead without a buffer overflow. On the other
hand, database and other session based services usually have small
requestsizes,butmultiplerequestscanbesentinonesessionwhich
isinitiatedbyauser.Insuchcases,foraserverreceivingaheavy
workload, the number of calls in a single session may eventually
have a cumulative effect and cause overflows.
To further increase the debug window, Parikshan could load bal-
ancedebugginginstrumentationoverheadacrossmultipledebug
containers, each of which can get a duplicate copy of the incoming
data. For instance, debug container 1 could have 50% of the instru-
mentation, andthe restcould occurin debugcontainer 2.Such a
strategy would significantly reduce the chance of a buffer overflow
in cases where heavy instrumentation is needed.
3.4 Divergence Checking
Itispossiblethatnon-deterministicbehavior(discussedin§6)in
the containers or instrumentation could cause the production and
debugcontainertodivergeovertime.Tounderstandandcapture
thisdivergence,wecomparethecorrespondingnetworkoutputs
received by the proxy, providing a black-box mechanism to check
thefidelityofthereplicabasedonitscommunicationwithexternal
components. We use a hash of each data packet, which is collectedandstoredinmemoryforthedurationthateachpacket’sconnection
is active. The degree of acceptable divergence is dependent on the
application behavior, and the operator’s wishes. For example, an
application that includes timestamps in each of its messages (i.e. is
expectedtohavesomenon-determinism)couldperhapsbeexpected
to have a much higher degree of acceptable divergence than an
applicationthatshouldnormallybereturningdeterministicresults.
3.5 Implementation
Parikshan ispubliclyavailableundertheMITopensourcelicense
onGitHub[ 6].Theclonemanagerandthelivecloningutilityare
built on top of the user-space container virtualization software
OpenVZ [ 51].Parikshan extendsVZCTL4.8 [35] live migration
facility [62], to provide support for online cloning. The network
isolationfortheproductioncontainerwasdoneusingLinuxnet-
work namespaces [ 2] and NAT. While Parikshan is based on light-
weightcontainers,webelieveitcanalsobeappliedtotraditional
virtualization software where live migration has been further opti-
mized [26, 81].
Thenetworkproxyduplicatorandthenetworkaggregatorare
implemented in C/C++. The forwarding in the proxy is done by
forkingoffmultipleprocesseseachhandlingonesend/orreceive
a connection in a loop from a source port to a destination port.
Data from processes handling communication with the production
container, is transferred to those handling communication with
the debug containers using Linux Pipes [1]. Pipe buffer size is a
configurable input based on user specifications.
4 EVALUATION
To evaluated Parikshan through the following research questions:
RQ1:Howlongdoesittaketocreatealivecloneofaproduction
container and what is its impact on the performance of the produc-
tion container?RQ2:
What is the impact of Parikshan on the throughput and la-
tency of the production application?RQ3:
Whatisthesizeofthedebuggingwindow,andhowdoesit
depend on resource constraints?RQ4:CanParikshan successfully reproduce real bugs?
Wecompared Parikshan ’stwocloningmodes(internalandexter-
nal). Our internal cloning mode was evaluated using two identical
VM’swithanInteli7CPU,with4Cores,and16GBRAMeachinthe
samephysicalhost(oneeachforproductionanddebugcontainers).
Weevaluatedtheexternalcloningmodeontwoidenticalhostnodes
withIntelCore2DuoProcessor,8GBofRAM.Allevaluationswere
performed on CentOS 6.5. Apart from cloning performance eval-
uationinRQ1,otherevaluationsuseexternalmode(i.e.different
identical machines for debug and production containers).
4.1 RQ1: Live Cloning Performance
As explained in §3, a short suspend time during live cloning is
necessary to ensure that both containers are in the exact same
system state. We measure this overhead on both real and synthetic
workloads,andseparatethesuspendtimeintoitsfourcomponents:
(1) Suspend & Dump: time taken to pause and dump the container,
(2)Pcopyaftersuspend:timerequiredtocompletersyncoperation,(3) Copy Dump File: time taken to copy an initial dump file, and (4)
456
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Nipun Arora, Jonathan Bell, Franjo Ivančić, Gail Kaiser, and Baishakhi Ray
BasicApache Thttpd
TradeBeansTradeSoapPetStore0510Time(seconds)Suspend & Dump Pcopy after suspend
Copy Dump File Undump & Resume
Figure3:Suspendtimeforlivecloning,whenrunningarep-
resentativebenchmark.Here‘basic’indicatesbaselinewith-out any services running in the container.
Undump & Resume: time taken to resume the containers. We used
both micro and macro benchmarks to evaluate live cloning.
Real-worldapplicationsandworkloads: First,weevaluated
Parikshan ’ssuspendtimeusingfivewell-knownapplicationswork-
loads. We ran the httperf [ 63] benchmark on Apache and thttpdto
compute max throughput of the web-servers, by sending a large
number of concurrent requests. Tradebeansand Tradesoap are re-
alistic workloads running a multi-tier stock trading application
andarepartoftheDaCapo[ 16]benchmark“DayTrader”applica-
tion. PetStore [ 3] is also a well known JEE reference application.
We deployed PetStore in a 3-tier system with JBoss, MySQL and
Apacheservers,andclonedtheapp-server.Theinputworkloadwas
a random set of transactions which were repeated for the duration
of the cloning process.
As shown in Figure 3, for Apache and Thttpd the container sus-
pend time ranged between 2-3 seconds. However, in more memory
intensiveapplicationserverssuchasPetStoreandDayTrader,the
total suspend time was higher (6-12 seconds). Nevertheless, wedid not experience any timeouts or errors for the requests in the
workload
2. We felt that these relatively fast temporary app sus-
pensions were a reasonable price to pay to launch an otherwise
overhead-free debug replica.Microbenchmark:
Themainfactorthatimpactssuspendtimeis
thenumberof“dirtypages”(recentlymodified)inthesuspendphase
that have not been copied over in the pre-copy rsync operation
(see§3.1).Hence,tofurthercharacterizethesuspendtimeimposed
bythecloningphaseof Parikshan ,wecreatedamicrobenchmark
thatcontrolsthisvariable.Weusedthe fioutility[9]togradually
increasethenumberofI/Ooperationswhiledoinglivecloning.We
ranfiotoreadandwritesofrandomvalueswithacontrolledI/O
2Incaseofpacketdrops,requestsareresentbothattheTCPlayer,andtheapplication
layer. This slows down the requests for the user, but does not drop them10310451015
I/O ops(Kbps)Time (secs)read-internalMode read-externalMode
write-internalMode write-externalMode
Figure 4: Live Cloning suspend time with increasing
amounts of I/O operations
bandwidth. We ensured that the I/O workload being processed by
fiowas long enough to last through the cloning process.
AsshowninFigure 4,readoperationshaveamuchsmallerim-
pact on suspend time of live cloning compared to write operations.
This can be attributed to the increase of dirty pages in write opera-
tions,whereasforread,thediskimageremainslargelythesame.
Theinternalmodeismuchfasterthantheexternalmode,asboth
theproductionanddebugcontainerarehostedinthesamephysical
device. For higher I/O operations, with a large amount of dirty
pages, network bandwidth becomes a bottleneck: leading to longer
suspendtimes.Overallinourexperiments,theinternalmodeisabletomanagewriteoperationupto10Mbps,withatotalsuspend-time
of approx 5 seconds, whereas, the external mode is only able to
manage up to 5-6 Mbps, fora5s ecsuspend time.
To answer RQ1, live cloning introduces a short suspend
timeintheproductioncontainerdependentonthework-
load.Writeintensiveworkloadswillleadtolongersuspend
times, while read intensive workloads will take much less.
Suspendtimesinrealworkloadonreal-worldsystemsvary
from2-3secondsforwebserverworkloadsto10-11seconds
forapplication/databaseserverworkloads.Comparedto
external mode,internal mode hada shorter suspend time.
A production-quality implementation could reduce sus-
pend time further by rate-limiting incoming requests in
theproxy,orusing copy-on-writemechanismsandfaster
shared file system/storage devices already available in sev-
eral existing live migration solutions.
4.2 RQ2: Impact on Production Performance
We measured the impact of Parikshan on a running production
application (after the debug environment was created) in terms of
457
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Replay without Recording of Production Bugs for Service Oriented Applications ASE ’18, September 3–7, 2018, Montpellier, France
Table 2: First four latencies of GET/POST requests from
wikipedia traces. The third and fourth column show over-
head of proxy compared to native, and overhead of duplica-
tion compared to proxy mode
Native Proxy DuplicationProxy
OverheadDuplication
Overhead
0.29702 0.30696 0.30623 3.347 -0.24050.06117 0.06154 0.06250 6.08 1.550.05342 0.05676 0.05564 6.25 -1.978250.05424 0.05438 0.05437 0.261 -0.0168
throughputandlatency.Tounderstandtheimpactofduplication
on network throughput, we ran a microbenchmark using iperf[82]
inthreedifferentmodes—native(justtheclientandserver),proxy
(clientcommunicatestoserverthroughaproxy)andduplication
(clientcommunicatestooriginalserver,andaclonevia Parikshan ’s
duplicator). We observed that while the native and proxy com-munication had no discernible difference with both transferring
at941Mb/s,theduplicationmodewasonanaverage0.5%slower
thantheothertwo.Webelievethisdifferenceisnegligibleformost
practicalapplicationsandwillnotimpactapplicationend-to-end
performance.
To measure the impact on latency, we created a scaled down
versionofWikipedia(MediaWiki)[ 12]wherewepopulateddata
from data dumps available through wikibench [ 84]. We used a
sampleworkloadofrequestsfrom2008,andcomparedthelatenciesofabout500HTTPrequestsinthesamethreedeployments(native,
proxy and duplicate). Table 2shows a snapshot of 4 such requests
andtheirlatenciesandoverheadsindifferentmodes.Wefoundthat
theproxywasgenerallyslowerthanthenativeconnection,with
the slowdown ranging from 1-8%. More importantly we found that
whencomparingthelatenciesintheduplicationmodetoourproxy
mode, the overhead was negligible ( ±2%due to caching). These
experiments are described in greater detail in the full version of
this work [7].
To answer RQ2, we found that duplication of traffic has
minimalimpact onthroughput(0.5%), andno discernible
impact on network latency.
4.3 RQ3: Debug Window Size
The network-level proxies are responsible for buffering commu-
nicationto/fromthedebugcontainer(s),allowingtheproduction
application to operate without slowing down to account for any
overheadsinthedebugapplication.Hence,thesizeofthisbufferdi-
rectlyimpactshowfarthedebugenvironmentisabletofallbehind
production. We refer to this time window (where debugging can
call behind production) as the debug window, and evaluated how
differentsizebuffersimpacttheabilityofdeveloperstodebugin
both real-world experiments, and also in controlled simulations.Experimental Results:
To evaluate the approximate size of the
debugwindow,wesentrequeststobothaproductionanddebug
MySQLcontainerviaournetworkduplicator.Eachworkloadran
for about 7 minutes (10,000 “select * from table” queries), with
varying request workloads. We also profiled the server, and foundTable 3: Approximate debug window sizes for a MySQL re-
quest workload
Input Rate Debug Window Pipe Size Slowdown
530 bps, 27 rq/s
84096 1.8x
530 bps, 27 rq/s 8 sec 4096 3x
530 bps, 27 rq/s 72 sec 16384 3x
Pois., λ= 17 rq/s 16 sec 4096 8x
Pois., λ= 17 rq/s 18 sec 4096 5x
Pois., λ=1 7r q / s
865536 3.2x
Pois., λ= 17 rq/s 376 sec 16384 3.2x
thatisabletoprocessamaxof27req/s3inasingleuserconnect
session.Foreachofourexperiments,wevarythebuffersizestoget
anideaofdebugwindow.Wegeneratedaslowdownbymodeling
the time taken by MySQL to process requests (27 req/s or 17req/s),
and putting an approximate sleep in the request handler.
Initially, we created a connection and sent requests at the maxi-
mumrequestratetheserverwasabletohandle(27req/s).Wefound
that for overheads up-to 1.8x (approx) we experienced no buffer
overflows. For higher overheads the debug window decreased, pri-
marily dependent on buffer size, request size, and slowdown.
Next, we mimic user behavior, to generate a realistic workload.
WesendpacketsusingaPoissonprocesswithanaveragerequest
rateof17requestspersecondtoourproxy.Thisvariestheinter-
requestarrivaltime,andletsthedebugcontainercatchupwiththe
productioncontainerduringidleperiodsbetweenrequestbursts.
We observed that compared to earlier experiments there was more
slack in the system, allowing it to tolerate a much higher overhead
(3.2x) with no buffer overflows.
Simulation Results: In our next set of experiments, we simulate
packet arrival and service processing for a buffered queue in SOA
applications. We use a discrete event simulation based on an MM1
queue,whichisaclassicqueuingmodelbasedonKendall’snota-
tion[49],andisoftenusedtomodelSOAapplicationswithasingle
buffer basedqueue. Essentially, weare sendingand processingre-
quests based on a Poisson distribution with a finite buffer capacity.
In our simulations (see Figure 5), we kept a constant buffer size of
64GB,anditerativelyincreasedtheoverheadofinstrumentation,
thereby decreasing the service processing time. Each series (set of
experiments), starts with an arrival rate approximately 5 times less
thantheserviceprocessingtime.Thismeansthatat400%overhead,
the system would be running at full capacity (for stable systems
SOA applications generally operate at much less than system ca-
pacity). Each simulation instance was run for 1,000,000 seconds
(277.7hours).Wegraduallyincreasedtheinstrumentationby10%
each time, and observed the hitting time of the buffer (time it takes
for the buffer to overflow for the first time). As shown, there is no
bufferoverflowinanyofthesimulationsuntiltheoverheadreaches
around420-470%,beyondthisthedebugwindowdecreasesexpo-
nentially.Sincebeyond400%overhead,thesystemisover-capacity,
the queue will start filling up fairly quickly. This clarifies the be-
haviorweobservedinourexperiments,whereforloweroverheads
3Not the same as bandwidth, 27 req/s is the maximum rate of sequential requests
MySQL server is able to handle for a user session
458
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Nipun Arora, Jonathan Bell, Franjo Ivančić, Gail Kaiser, and Baishakhi Ray
(1.8-3.2x) we did not observe any overflow, but beyond a certain
point, we observed that the buffer would overflow fairly quickly.
Alsoasshowninthesystem,sincethebuffersizeissignificantly
larger than the packet arrival rate, it takes some time for the buffer
to overflow (several hours). We believe that while most systems
willrun significantlyunder capacity, largebuffer sizescan ensure
that our debug container may be able to handle short bursts in the
workload. However, a system running continuously at capacity is
unlikely to tolerate significant instrumentation overhead.
To answer RQ3, we found that the debug container can
stay in a stable state without any buffer overflows as long
as the instrumentation does not cause the service times to
becomemorethantherequestarrivalrate.Furthermore,
a large buffer will allow handling of short bursts in the
workload until the system returns back to a stable state.
Thedebugwindowcanallowfor asignificantslowdown,
which means that many existing dynamic analysis tech-
niques[33,68],aswellasmostfine-grainedtracing[ 32,48]
can be applied on the debug container without leading to
an incorrect state.
4.4 RQ4: Reproducing Real Bugs
One of our core insights is that for most SOA systems, production
bugscanhencebetriggeredbynetworkreplayalone.Tovalidate
thisinsight,weselectedsixteenreal-worldbugs,applied Parikshan ,
reproduced them in a production container, and observed whether
theywerealsosimultaneouslyreproducedinthereplica.Foreachofthe sixteen bugs that we triggered in the production environments,
Parikshan faithfully reproduced them in the replica.
Weselectedourbugsfromthoseexaminedinpreviousstudies
[59,89], focusing on bugs that involved performance, resource-
leaks, semantics, concurrency, and configuration. We have further
categorized these bugs whether they lead to a crash or not, andif they can be deterministically reproduced. Table 4presents an
400 500 600 7000100200300
Overhead(in percentage)Debug Window(in hours)arrival 4MB/s
arrival 2MB/s
arrival 1MB/s
Figure 5: Simulation results for debug window size
(buffer=64GB). Each series has a constant arrival rate.overview of the bugs that we studied. A thorough description of
each bug, the steps that we took to reproduce it with Parikshan ,
and description of the debugging experience is available in the full
version of this work [7] in section 4.3.
Semantic Bugs : We recreated 4 semantic bugs from Redis [ 20]
queuing system, and Cassandra [ 55] (a NoSQL database). For in-
stance,Redis#761isanintegeroverflowerror.Thiserroristriggered,
when the client tries to insert and store a very large number. This
leads to an unmanaged exception, which crashes the production
system.OtherssuchasRedis#487resultedinexpiredkeysstillbeing
retainedin Redis,because ofanunchecked edgecondition. While
this error does not lead to any exception or any error report inapplication logs, it gives the user a wrong output. In the case ofsuch logical errors, the application keeps processing, but the in-ternal state can stay incorrect. In our experiments, we were able
to clone the input of the production in the debug containers and
easily replayed both these errors.
Performance Bugs :Wereplayed3MySQLproductionbugs.For
example, iMySQL#15811 reported that some of the user requests
which were dealing with complex scripts (Chinese, Japanese), were
runningsignificantlyslowerthanothers.Toevaluate Parikshan ,we
re-created a two-tier client-server setup with the server (container)
running a buggy MySQL server and sent queries to the produc-
tion container with complex scripts (Chinese). These queries were
asynchronously replicated,in thedebug container.To furtherin-
vestigatethebug-diagnosisprocess,wealsoturnedonexecution
tracing in the debug container using SystemTap [ 30]. This gives us
the added advantage, of being able to profile and identify the func-
tions responsible for the slow-down, without the tracing having
any impact on production.Resource Leaks
:Parikshan successfully reproduced 2 resource
leak bugs in Redis. Let us take Redis#417 for instance, here we
had a redis master and slave set up for both production and debug
container. We then triggered the bug by running concurrent re-
quests through the client which can trigger the memory leak. The
memoryleakwaseasilyreplayedinthedebugcontainerbyturning
on debug tracing, which showed a growing memory usage.Concurrency Bugs
: One of the most subtle bugs in production
systemsiscausedduetoconcurrencyerrors.Thesebugsarehard
toreproduce,astheyarenon-deterministic,andmayormaynot
happeninagivenexecution.Although Parikshan cannotguarantee
thereplayofconcurrencybugs,inourexperimentwecouldsuccess-fullyreproducealltheconcurrencybugsshowinginTable 4.Given
thatthedebugcontainerisalive-cloneoftheproductioncontainer,
and that it replicates the state of the production container entirely,
we believethat the chancesof replaying thenon-deterministic con-
currencybuginthedebugcontainerarequitehigh,asevidentby
our experiments. Additionally, the debug container is a useful trac-
ingutilitytotrackthreadlockandunlocksequences,togetanidea
of the concurrency bug.Configuration Bugs
:Configuration errorsare usuallycausedby
wrongly configured parameters, i.e., they are not bugs in the appli-
cation,butbugsintheinput(configuration).Thesebugsusuallygettriggered at scale or for certain edge cases, making them extremelydifficulttocatch.AsimpleexampleofsuchabugisRedis#957,here
theslaveisunabletosyncwiththemaster.Theconnectionwith
theslavetimesoutandit’sunabletosyncbecauseofthelargedata.
459
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Replay without Recording of Production Bugs for Service Oriented Applications ASE ’18, September 3–7, 2018, Montpellier, France
Table 4: List of real-world production bugs studied with Parikshan
Bug Type Bug ID Application Symptom/CauseDeterm-
inisticCrash Steps to Reproduce
SemanticRedis #487 redis-2.6.14 Keys* command duplicate or omits keys Yes No Setkeystoexpire,executespecific
reqs
Cassandra #5225 cassandra-1.5.2 Missing columns from wide row Yes No Fetch columns from cassandra
Cassandra #1837 cassandra-0.7.0 Deleted columns become available after flush Yes No Insert, delete, and flush columns
Redis #761 redis-2.6.0 Crash with large integer input Yes Yes Query for input of large integer
PerformanceMySQL #15811 mysql-5.0.15 Bug caused due to multiple calls in a loop Yes No Repeated insert into tableMySQL #26527 mysql-5.1.14 Load data is slow in a partitioned table Yes No
Createtablewithpartitionandload
data
MySQL #49491 mysql-5.1.38 calculation of hash values inefficient Yes No MySql client select requests
ConcurrencyApache #25520 httpd-2.0.4 Per-child buffer management not thread safe No No Continuous concurrent requests
Apache #21287httpd-2.0.48,
php-4.4.1Dangling pointer due to atomicity violation No Yes Continuous concurrent request
MySQL #644 mysql-4.1 data-race leading to crash No Yes Concurrent select queries
MySQL #169 mysql-3.23 Race condition leading to out-of-order logging No No Delete and insert requests
MySQL #791 mysql-4.0 Race - visible in logging No No Concurrentflushlogandinsertre-
quests
Resource LeakRedis #614 redis-2.6.0 Master + slave, not replicated correctly Yes No Setup replication, push and pop
some elements
Redis #417 redis-2.4.9 Memory leak in master Yes No Concurrent key set requests
ConfigurationRedis #957 redis-2.6.11 Slave cannot sync with master Yes No Load a very large DB
HDFS #1904 hdfs-0.23.0 Create a directory in wrong location Yes No Create new directory
While the bug is partially a semantic bug, as it could potentially
have checks and balances in the code. The root cause itself is a
lower output buffer limit. Once again, it was easily replayed in our
debugcontainersthattheslavewasnotsynced,andinvestigated
further by the debugger.
To answer RQ4, we found that Parikshan ’s approach of
capturing the network traffic and replaying it in an offline
environmentisefficienttoreproducerealproductionbugs.
5 APPLICATIONS OF LIVE DEBUGGING
StatisticalTesting: Onewell-knowntechniquefordebuggingpro-
ductionapplicationsisstatisticaltesting.Thisisachievedbyhaving
predicate profiles from both successful and failing runs of a pro-
gramandapplyingstatisticaltechniquestopinpointthecauseof
the failure. The core advantage of statistical testing is that the sam-
pling frequency of the instrumentation can be decreased to reduce
theinstrumentationoverhead. However, theinstrumentationfre-
quency for such testing to be successful needs to be statistically
significant. Unfortunately, overhead concerns in the production
environment limit the frequency of instrumentation. In Parikshan ,
the buffer utilization can be used to control the frequency of such
statisticalinstrumentationinthedebugcontainer.Thiswouldallow
the user to utilize the slack available in the debug container for
instrumentation toit’s maximum,without leading toan overflow.
Thereby improving the efficiency of statistical testing.
Record and Replay: Record and Replay techniques have been
proposed to replay production site bugs. However, they are not yet
used in practice as they can impose unacceptable overheads in theserviceprocessingtime. Parikshan replicascanbeusedtodorecord-
ingatamuchfinergranularity(higheroverhead),allowingforeasy
and fast replays offline. Similar to existing mechanisms, the system
can be replayed can then be used for offline debugging, without
imposing any recording overhead to the production container.
PatchTesting: Bug fixes and patches to resolve errors, often need
to undergo testing in the offline environment and are not guar-anteed to perform correctly. Patches can be made to the replica
instead.Thefixcanbetracedandobservedifitiscorrectlywork-
ing, before moving it to the production container. This is similar in
naturetoAB-Tesing,whichisappliedtofindifanewfixisuseful
or works [31].
6 LIMITATIONS AND THREATS TO VALIDITY
There may be several threats to the validity of our findings. For
instance,the bugsthat weselectedto studymay notbetruly repre-
sentativeofabroadrangeofdifferentfaults.Perhaps Parikshan ’s
low-overhead network replay approach is less suitable to some
classes of bugs. To alleviate this concern, we selected from several
establishedbugcategories,andfurther,evaluated Parikshan with
bugsthathadalreadybeenstudiedinotherliterature,toalleviatea
risk of selection bias. We further strengthened this by categorizing
220 bug reports from three real-world applications, finding that
mostweresemanticinnature,andveryfewwerenon-deterministic,
withsimilarcharacteristicstothe16thatwedemonstrated Parik-
shancan reproduce.
Therearealsoseveralunderlyinglimitationsandassumptions
regarding Parikshan ’s applicability:
Non-determinism: Non-determinism can be attributed to three
mainsources(1)systemconfiguration,(2)applicationinput,and(3)
460
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Nipun Arora, Jonathan Bell, Franjo Ivančić, Gail Kaiser, and Baishakhi Ray
orderinginconcurrentthreads.Livecloningoftheapplicationstate
of a service container ensures that both the production and debug
services are in the same “system-state” and have the same con-
figurationparametersforitselfandalldependencies. Parikshan ’s
networkproxyensuresthatallinputsreceivedintheproduction
container are also forwarded to the debug container. However, any
non-determinism from other sources (e.g., thread interleaving, ran-
dom numbers, reliance on timing) may limit Parikshan ’s ability to
faithfullyreproduceanexecution.Whileourcurrentprototypedoes
nothandle these,webelieve thereare severalexistingtechniques
that can be applied to tackle this problem in the context of live
debugging, such as deterministic scheduling [ 88].Parikshan allows
significanttracingofsynchronizationpoints,oftenrequiredforcon-
straint solvers [ 33,36] to go explore all synchronization orderings
tofindconcurrencyerrors.Wehavealsotriedtoalleviatethisprob-
lemusingourdivergencechecker(Section 3.4).And,aswasseenin
our case-studies, even in the face of limited non-determinism bugs
will often still be triggered in the replica.
Distributed Services: Large-scale distributed systems are often
comprised of several interacting services such as storage, NTP,
backup services, controllers and resource managers. Parikshan can
be used for multiple communicating services, where any given ser-
vicemaybecloned,turnedofforcontinueasis,dependingonits
nature.Forexample,storageservicessupportingareplicashould
be cloned or turned off (depending on debugging environment)
astheycouldpropagatechangesfromthedebugcontainertothe
production containers. Services like NTP can be allowed to con-
tinuewithoutcloningastheirpublishsubscribebroadcastcannotbeimpactedbycloningofotherservicesanyway.Furthermore,instru-
mentationinsertedinareplicawillnotnecessarilyslowdownall
itsservices,e.g.,addinginstrumentationtoaMySQLqueryhandler
will not slow down file-sharing or NTP services running in the
same container.DataPrivacy:
Thisisalimitationthat Parikshan shareswithmost
existing record-replay systems [ 19,25].Parikshan clones incoming
traffictherebyanydebuggerhavingaccesstothedebugmachine
can potentially look at the user data (depending on the kind of
instrumentationtheyuse).Currentlywedonotproposeanywayto
address this issue inour system, and leave it to thedebugger (and
thedataaccesscontrolpoliciesoftheirproductiondeployments)
as to how this can be addressed.
7 RELATED WORK
Record and Replay Systems: Record and Replay [ 8,11,13,15,
18,24,28,34,38–41,43–47,52,54,56,57,60,61,65,66,70,71,73–
79,83,85–87,90]hasbeenanactiveareaofresearchintheacademic
community for many years. These systems offer highly faithful
re-execution but incur performance overhead on the production
application — for instance, ODR [ 5] reports 1.6x slowdown and
rr [64] 1.2x; Scribe [ 53] reduces to 2.5% for server applications and
15% for desktop applications. Parikshan avoids recording overhead
entirely,butitscloningsuspendtimemaybeviewedasanamortized
cost in comparison to the overhead in record-replay systems.
Amongrecordandreplaysystems,theworkweknowofmost
closelyrelatedtooursisAftersight[ 21].Aftersightrecordsapro-
ductionsystemandreplaysitconcurrentlyalongsideinanotherVM.While Aftersight intends, like Parikshan , to enable nearly real-time
diagnosisfacility,Aftersightsuffersfromrecordingoverheadinthe
production VM. The average slow-down in Aftersight is 5% and
canballoonupto2.6xinworst-casescenarios.VARAN[ 42]isanN-
versionexecutionmonitorthatmanagessimultaneousexecutions
ofaproductionapplication,checkingamongthemfordivergence.
VARANeffectivelyreplicatesapplications atthesystemcalllevel,
butParikshan ’s lower overhead mechanism does not impact the
performance of the master (production) application. Parikshan also
toleratesgreaterdivergencefromtheproductionexecution,i.e.,a
debugreplicacontinuestorunevenifitsexecutionpathismodifiedby the analysis instrumentation. VARAN has recently been appliedto run multiple incompatible dynamic analyses in parallel [
72]—i t
would be interesting to use Parikshan for this application as well.
Real-Time Diagnosis Techniques: Chaos Monkey [ 14] injects
faultsintoproductionsystemstoconductfault-tolerancetesting,
randomly introducing time-outs, resource hogs, etc. This allows
Netflixtotesttherobustnessoftheirsystematscale,andavoidlarge-scalesystemcrashes.ABTesting[
31]probabilisticallytestsupdates
or beta releases on some percentage of users, while letting the
majorityoftheuserscontinueworkingwiththeoriginalapplication.
AB Testing allows the developer to understand user-response to
anynewadditionstothesoftware,whichcouldbeusedtodetect
bugs as well as feature problems. Unlike Parikshan , this kind of
approach directly impacts (some) users.
Live Migration & Cloning Live migration of virtual machines
facilitates fault management, load balancing, and low-level system
maintenance for the administrator. Most existing approaches use a
pre-copyapproach that copies the memory state over several itera-
tions, and then copies the process state. This includes hypervisors
such as VMWare [ 67], Xen [23], and KVM [ 50]. VM Cloning, on
the other hand, is usually done offline by taking a snapshot of a
suspended/shutdown VM and restarting it on another machine.Cloning is helpful for scaling out applications using multiple in-stances of the same server. Live cloning such as Sun et al. [
80]
uses copy-on-write mechanisms, to create a duplicate of the tar-
getVMwithoutshutting downtheoriginal.Otherwork[ 37]uses
live-cloning to do cluster-expansion.
8 CONCLUSION & FUTURE WORK
Parikshan isanovelframeworkforlivedebuggingofproduction
SOA applications. We show that in combination with existing bug
diagnosistechniques, Parikshan successfullylocalizesseveralreal-
world production bugs that would be hard to find otherwise. Com-
paredtoexistingmonitoringsolutionsthatfocusonreducinginstru-
mentationoverhead,ourapproachenablesminimalperformance
slowdown while at the same time allowing heavyweight debug-
ginginstrumentation.The Parikshan prototypeispubliclyavailable
under the MIT open source license on GitHub [6].
ACKNOWLEDGEMENTS
We would like to thank the anonymous reviewers for their helpful
feedback.ThisworkwassupportedinpartbyNSFCNS-1563555,
CCF-1619123, and CNS-1618771.
461
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Replay without Recording of Production Bugs for Service Oriented Applications ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1] [n. d.]. Linux IPC pipes. http://man7.org/linux/man-pages/man7/pipe.7.html.
[2] [n. d.]. Network Namespaces. https://lwn.net/Articles/580893/.[3]
[n. d.]. PetStore asample Java Platform, EnterpriseEdition referenceapplication.
http://www.oracle.com/technetwork/java/petstore1-1-2-136742.html.
[4]2017. Microservices Architecture. https://docs.microsoft.com/en-us/azure/
architecture/guide/architecture-styles/microservices.
[5]Gautam Altekar and Ion Stoica. 2009. ODR: output-deterministic replay formulticore debugging. In Proceedings of the ACM SIGOPS 22nd symposium on
Operating systems principles. ACM, 193–206.
[6]Nipun Arora. [n. d.]. https://github.com/Programming-Systems-Lab/Parikshan
[7]Nipun Arora. 2018. Sandboxed, Online Debugging of Production Bugs for SOA
Systems. Ph.D.Dissertation.ColumbiaUniversity,ColumbiaUniversityAcademic
Commons. http://www.nipunarora.net/pdf/sandbox_thesis.pdf .
[8]ShayArtzi,SunghunKim,andMichaelD.Ernst.2008. ReCrash:MakingSoftware
Failures Reproducible by Preserving Object States. In ECOOP.
[9] Jens Axboe. 2008. Fio-flexible io tester. http://freecode.com/projects/fio.
[10]Paul Barham, Austin Donnelly, Rebecca Isaacs, and Richard Mortier. 2004. Using
Magpie for Request Extraction and Workload Modelling.. In OSDI.
[11] Earl T. Barr and Mark Marron. 2014. Tardis: Affordable Time-travel Debugging
inManagedRuntimes.In ACMInternationalConferenceonObjectOrientedPro-
grammingSystemsLanguagesandApplications (OOPSLA’14).ACM,NewYork,
NY, USA, 67–82.
[12] Daniel J Barrett. 2008. MediaWiki. " O’Reilly Media, Inc.".
[13]Jonathan Bell, Nikhil Sarda, and Gail Kaiser. 2013. Chronicler: Lightweight
RecordingtoReproduceFieldFailures.In InternationalConferenceonSoftware
Engineering (ICSE ’13). IEEE Press, Piscataway, NJ, USA, 362–371. http://dl.acm.
org/citation.cfm?id=2486788.2486836
[14]C Bennett and A Tseitlin. 2012. Netflix: Chaos Monkey released into the wild.
Netflix Tech Blog.
[15]SanjayBhansali,Wen-KeChen,StuartdeJong,AndrewEdwards,RonMurray,
Milenko Drinić, Darek Mihočka, and Joe Chau. 2006. Framework for instruction-
level tracing and analysis of program executions. In Proceedings of the 2nd Inter-
national Conference on Virtual Execution Environments (VEE ’06). 154–163.
[16]StephenMBlackburn,RobinGarner,ChrisHoffmann,AsjadMKhang,KathrynS
McKinley, Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel Frampton,
Samuel Z Guyer, et al .2006. The DaCapo benchmarks: Java benchmarking
development and analysis. In ACM Sigplan Notices, Vol. 41. ACM, 169–190.
[17]Dhruba Borthakur. 2008. HDFS architecture guide. HADOOP APACHE PROJECT
http://hadoop. apache. org/common/docs/current/hdfs design. pdf (2008), 39.
[18]BrianBurg,RichardBailey,AndrewJ.Ko,andMichaelD.Ernst.2013. Interactive
Record/Replay for Web Application Debugging. In 26th ACM Symposium on User
Interface Software and Technology (UIST ’13) . ACM, New York, NY, USA, 473–484.
https://doi.org/10.1145/2501988.2502050
[19]Yu Cao, Hongyu Zhang, and Sun Ding. 2014. SymCrash: Selective Recording for
Reproducing Crashes. In 29th ACM/IEEE International Conference on Automated
Software Engineering (ASE ’14). ACM, New York, NY, USA, 791–802. https:
//doi.org/10.1145/2642937.2642993
[20] Josiah L Carlson. 2013. Redis in Action. Manning Publications Co.
[21]Jim Chow, Tal Garfinkel, and Peter M Chen. 2008. Decoupling dynamic programanalysisfromexecutioninvirtualenvironments.In USENIX2008AnnualTechnical
Conference on Annual Technical Conference. 1–14.
[22]Ben Christensen. 2013. Application Resilience in a Service-
oriented Architecture. http://radar.oreilly.com/2013/06/
application-resilience-in-a-service-oriented-architecture.html
[23]ChristopherClark,KeirFraser,StevenHand,JacobGormHansen,EricJul,Chris-
tian Limpach, Ian Pratt, and Andrew Warfield. 2005. Live migration of virtual
machines.In Proceedingsofthe2ndconferenceonSymposiumonNetworkedSys-
tems Design & Implementation-Volume 2. USENIX Association, 273–286.
[24]James Clause and Alessandro Orso. 2007. A Technique for Enabling and Sup-
porting Debugging of Field Failures. In 29th International Conference on Software
Engineering (ICSE ’07). IEEE Computer Society, Washington, DC, USA, 261–270.
https://doi.org/10.1109/ICSE.2007.10
[25]JamesClauseandAlessandroOrso.2011.Camouflage:AutomatedAnonymization
of Field Data. In 33rd International Conference on Software Engineering (ICSE ’11).
ACM, New York, NY, USA, 21–30. https://doi.org/10.1145/1985793.1985797
[26]Umesh Deshpande and Kate Keahey. 2016. Traffic-sensitive live migration of
virtual machines. Future Generation Computer Systems (2016).
[27]George W Dunlap, Samuel T King, Sukru Cinar, Murtaza A Basrai, and Peter M
Chen.2002. ReVirt:Enablingintrusionanalysisthroughvirtual-machinelogging
and replay. ACM SIGOPS Operating Systems Review 36, SI (2002), 211–224.
[28]George W. Dunlap, Samuel T. King, Sukru Cinar, Murtaza A. Basrai, and Pe-
ter M. Chen. 2002. ReVirt: Enabling Intrusion Analysis through Virtual-Machine
Logging andReplay. In 5thSymposium onOperating SystemsDesign andImple-
mentation (OSDI ’02) . ACM, 211–224. https://doi.org/10.1145/1060289.1060309
[29]Frank Ch Eigler and Red Hat. 2006. Problem solving with systemtap. In Proc. of
the Ottawa Linux Symposium. Citeseer, 261–268.[30]FrankCEigler,VaraPrasad,WillCohen,HienNguyen,MartinHunt,JimKeniston,
andBradChen.2005. Architectureofsystemtap:aLinuxtrace/probetool. (2005).
[31]Bryan Eisenberg and John Quarto-vonTivadar. 2009. Always be testing: The
complete guide to Google website optimizer. John Wiley & Sons.
[32]Úlfar Erlingsson, Marcus Peinado, Simon Peter, Mihai Budiu, and Gloria Mainar-
Ruiz.2012. Fay:ExtensibleDistributedTracingfromKernelstoClusters. ACM
Trans. Comput. Syst. 30, 4, Article 13 (Nov. 2012), 35 pages.
[33]Cormac Flanagan and Patrice Godefroid. 2005. Dynamic Partial-order Reductionfor Model Checking Software. In Proceedings of the 32Nd ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (POPL ’05). ACM, New York,
NY, USA, 110–121. https://doi.org/10.1145/1040305.1040315
[34]Free Software Foundation. [n. d.]. GDB and Reverse Debugging. http://www.
gnu.org/software/gdb/news/reversible.html.
[35] Mark Furman. 2014. OpenVZ Essentials. Packt Publishing Ltd.
[36]MalayK.Ganai,NipunArora,ChaoWang,AartiGupta,andGogulBalakrishnan.
2011. BEST: A Symbolic Testing Tool for Predicting Multi-threaded Program
Failures. In ASE.
[37]A.GebhartandE.Bozak.2009. Dynamicclusterexpansionthroughvirtualization-
based live cloning. https://www.google.com/patents/US20090228883 US Patent
App. 12/044,888.
[38]Dennis Geels, Gautam Altekar, Petros Maniatis, Timothy Roscoe, and Ion Stoica.
2007. Friday: Global Comprehension for Distributed Replay.. In NSDI, Vol. 7.
285–298.
[39]DennisGeels,GautamAltekar,ScottShenker,andIonStoica.2006. ReplayDebug-
gingforDistributedApplications. In 2006USENIXAnnual TechnicalConference.
289–300.
[40]Zhenyu Guo, XiWang, Jian Tang, Xuezheng Liu, Zhilei Xu,Ming Wu, M.Frans
Kaashoek,andZhengZhang.2008. R2:anapplication-levelkernelforrecordand
replay. In OSDI. Berkeley, CA, USA.
[41]ZhenyuGuo, XiWang,Jian Tang,XuezhengLiu, ZhileiXu,MingWu, MFrans
Kaashoek,andZhengZhang.2008. R2:Anapplication-levelkernelforrecordand
replay.In Proceedingsofthe8thUSENIXconferenceonOperatingsystemsdesign
and implementation. USENIX Association, 193–208.
[42]Petr Hosek and Cristian Cadar. 2015. VARAN the Unbelievable: An Efficient
N-version Execution Framework. In Proceedings of the Twentieth International
ConferenceonArchitecturalSupportforProgrammingLanguagesandOperating
Systems (ASPLOS’15).ACM,NewYork,NY,USA,339–353. https://doi.org/10.
1145/2694344.2694390
[43]JeffHuang,PengLiu,andCharlesZhang.2010. LEAP:LightweightDeterministic
Multi-processor Replay of Concurrent Java Programs. In FSE.
[44]Jeff Huang and Charles Zhang. 2012. LEAN: Simplifying Concurrency Bug
Reproduction via Replay-supported Execution Reduction. In ACM International
Conference on Object Oriented Programming Systems Languages and Applications
(OOPSLA ’12). ACM, New York, NY, USA, 451–466. https://doi.org/10.1145/
2384616.2384649
[45]YanyanJiang,TianxiaoGu,ChangXu,XiaoxingMa,andJianLu.2014. CARE:
Cache Guided Deterministic Replay for Concurrent Java Programs. In 36th Inter-
national Conference on Software Engineering (ICSE 2014). ACM, New York, NY,
USA, 457–467. https://doi.org/10.1145/2568225.2568236
[46]Wei Jin and Alessandro Orso. 2012. BugRedux: reproducing field failures forin-house debugging. In 2012 International Conference on Software Engineering
(ICSE2012).IEEEPress,Piscataway,NJ,USA,474–484. http://dl.acm.org/citation.
cfm?id=2337223.2337279
[47]Shrinivas Joshi and Alessandro Orso. 2007. SCARPE: ATechniqueand Tool for
SelectiveCaptureandReplayofProgramExecutions.In ICSM.https://doi.org/
10.1109/ICSM.2007.4362636
[48]BarisKasikci,BenjaminSchubert,CristianoPereira,GillesPokam,andGeorge
Candea.2015. FailureSketching:A Techniquefor AutomatedRoot CauseDiag-
nosisofIn-productionFailures (SOSP’15).ACM,NewYork,NY,USA,344–360.
https://doi.org/10.1145/2815400.2815412
[49]David G. Kendall. 1953. Stochastic Processes Occurring in the Theory of Queues
andtheirAnalysisbytheMethodoftheImbeddedMarkovChain. Ann.Math.
Statist.24, 3 (09 1953), 338–354. https://doi.org/10.1214/aoms/1177728975
[50]Avi Kivity, Yaniv Kamay, Dor Laor, Uri Lublin, and Anthony Liguori. 2007. kvm:
theLinuxvirtualmachinemonitor.In ProceedingsoftheLinuxSymposium,Vol.1.
225–230.
[51]KirillKolyshkin.2006. Virtualizationinlinux. Whitepaper,OpenVZ 3(2006),39.
[52]Ravi Konuru, Harini Srinivasan, and Jong-Deok Choi. 2000. Deterministic replay
ofdistributedJavaapplications.In Proceedingsofthe14thInternationalSymposium
on Parallel and Distributed Processing (IPDPS ’00). 219–228.
[53]OrenLaadan,NicolasViennot,andJasonNieh.2010. Transparent,lightweight
applicationexecutionreplayoncommoditymultiprocessoroperatingsystems.
InACM SIGMETRICS Performance Evaluation Review, Vol. 38. ACM, 155–166.
[54]Oren Laadan, Nicolas Viennot, Chia-Che Tsai, Chris Blinn, Junfeng Yang, and
Jason Nieh. 2011. Pervasive Detection of Process Races in Deployed Systems.
In23rdACMSymposiumonOperatingSystemsPrinciples (SOSP’11) .ACM,New
York, NY, USA, 353–367. https://doi.org/10.1145/2043556.2043589
462
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Nipun Arora, Jonathan Bell, Franjo Ivančić, Gail Kaiser, and Baishakhi Ray
[55]AvinashLakshmanandPrashantMalik.2010. Cassandra:adecentralizedstruc-
turedstoragesystem. ACMSIGOPSOperatingSystemsReview 44,2(2010),35–40.
[56]T.J.LeBlancandJ.M.Mellor-Crummey.1987. DebuggingParallelProgramswith
Instant Replay. IEEE Trans. Comput. 36, 4 (1987), 471–482.
[57]Hongyu Liu, Sam Silvestro, Wei Wang, Chen Tian, and Tongping Liu. 2018. iRe-
player: In-situ and Identical Record-and-Replay for Multithreaded Applications.
arXiv preprint arXiv:1804.01226 (2018).
[58]Jian-Guang Lou, Qingwei Lin, Rui Ding, Qiang Fu, Dongmei Zhang, and Tao
Xie. 2013. Software analytics for incident management of online services: An
experiencereport.In AutomatedSoftwareEngineering(ASE),2013IEEE/ACM28th
International Conference on. IEEE, 475–485.
[59]ShanLu,ZhenminLi,FengQin,LinTan,PinZhou,andYuanyuanZhou.2005.
Bugbench:Benchmarksforevaluatingbugdetectiontools.In Workshoponthe
evaluation of software defect detection tools, Vol. 5.
[60]Toshinori Matsumura, Takashi Ishio, Yu Kashima, and Katsuro Inoue. 2014.
Repeatedly-executed-method Viewer for Efficient Visualization of Execution
Paths and States in Java (ICPC).
[61]James Mickens, Jeremy Elson, and Jon Howell. 2010. Mugshot: Deterministic
Capture and Replay for Javascript Applications. In NSDI.
[62]AndreyMirkin,AlexeyKuznetsov,andKirKolyshkin.2008. Containerscheck-
pointing and live migration. In Proceedings of the Linux Symposium.
[63]David Mosberger and Tai Jin. 1998. httperf—a tool for measuring web server
performance. ACM SIGMETRICS Performance Evaluation Review 26, 3 (1998),
31–37.
[64] mozilla. [n. d.]. what rr does. http://rr-project.org/.
[65]Satish Narayanasamy, Gilles Pokam, and Brad Calder. 2005. BugNet: Contin-uously Recording Program Execution for Deterministic Replay Debugging. In
32Nd Annual International Symposium on Computer Architecture (ISCA ’05) . IEEE
Computer Society, Washington, DC, USA, 284–295. https://doi.org/10.1109/
ISCA.2005.16
[66]Satish Narayanasamy, Zhenghao Wang, Jordan Tigani, Andrew Edwards, and
BradCalder.2007. AutomaticallyClassifyingBenignandHarmfulDataRacesus-
ingReplayAnalysis.In ACMSIGPLAN2007ConferenceonProgrammingLanguage
Design and Implementation (PLDI ’07).
[67]MichaelNelson,Beng-HongLim,GregHutchins,etal .2005. FastTransparent
MigrationforVirtualMachines..In USENIXAnnualTechnicalConference,General
Track. 391–394.
[68]Nicholas Nethercote and Julian Seward. 2007. Valgrind: a framework for heavy-
weight dynamic binary instrumentation. In PLDI ’07.
[69] Sam Newman. 2015. Building Microservices. " O’Reilly Media, Inc.".
[70]Soyeon Park, Yuanyuan Zhou, Weiwei Xiong, Zuoning Yin, Rini Kaushik, Kyu H.
Lee, and Shan Lu. 2009. PRES: probabilistic replay with execution sketching on
multiprocessors. In 22nd ACM Symposium on Operating Systems Principles (SOSP
’09). 177–192.
[71]HarishPatil,CristianoPereira,MackStallcup,GregoryLueck,andJamesCownie.
2010. PinPlay: a framework for deterministic replay and reproducible analysis of
parallel programs (CGO ’10). ACM.
[72]Luis Pina, Anastasios Andronidis, and Cristian Cadar. 2018. FreeDA: Deploying
IncompatibleStockDynamicAnalysesinProductionviaMulti-VersionExecution.
InACM International Conference on Computing Frontiers (CF 2018).
[73]Tobias Roehm and Bernd Bruegge. 2014. Reproducing Software Failures by
Exploiting the Action History of Undo Features. In Companion Proceedings of
the 36th International Conference on Software Engineering (ICSE Companion 2014).
ACM, New York, NY, USA, 496–499. https://doi.org/10.1145/2591062.2591101
[74]Tobias Roehm, Nigar Gurbanova, Bernd Bruegge, Christophe Joubert, and Walid
Maalej.2013. Monitoringuserinteractionsforsupportingfailurereproduction(ICPC).
[75]Rogue Wave Software. [n. d.]. Reverse debugging with ReplayEngine. http://
www.roguewave .com/products-services/totalvie w/features/reverse-debugging.
[76]YasushiSaito.2005.Jockey:AUser-spaceLibraryforRecord-replayDebugging.In
Sixth International Symposium on Automated Analysis-driven Debugging (AADE-
BUG’05). ACM, New York, NY, USA, 69–76. https://doi.org/10.1145/1085130.
1085139
[77]Sudarshan M. Srinivasan, Srikanth Kandula, Christopher R. Andrews, and
Yuanyuan Zhou. 2004. Flashback: A Lightweight Extension for Rollback and De-
terministic Replay for Software Debugging. In Proceedings of the USENIX Annual
Technical Conference (USENIX ’04).
[78]John Steven, Pravir Chandra, Bob Fleck, and Andy Podgurski. 2000. jRapture: A
Capture/ReplayToolforObservation-basedTesting.In ACMSIGSOFTInterna-
tional Symposium on Software Testing and Analysis (ISSTA ’00). ACM, New York,
NY, USA, 158–167. https://doi.org/10.1145/347324.348993
[79]DineshSubhravetiandJasonNieh.2011. RecordandTransplay:PartialCheck-
pointing for Replay Debugging Across Heterogeneous Systems. In ACM Interna-
tionalConferenceonMeasurementandModelingofComputerSystems(SIGMET-
RICS 2011). San Jose, CA.
[80]Yifeng Sun, Yingwei Luo, Xiaolin Wang, Zhenlin Wang, Binbin Zhang, Haogang
Chen, and Xiaoming Li. 2009. Fast live cloning of virtual machine based on xen
(HPCC).
[81]PetterSvärd,BenoitHudzia,SteveWalsh,JohanTordsson,andErikElmroth.2015.
Principles and performance characteristics of algorithms for live VM migration.
ACM SIGOPS Operating Systems Review 49, 1 (2015), 142–155.
[82]AjayTirumala,FengQin,JonDugan,JimFerguson,andKevinGibbs.2005. Iperf:
The TCP/UDP bandwidth measurement tool. htt p://dast. nlanr. net/Projects
(2005).
[83]Undo Software. [n. d.]. UndoDB reversible debugging tool for Linux. http:
//undo-software.com/undodb/.
[84]Erik-Jan van Baaren. 2009. Wikibench: A distributed, wikipedia based web
application benchmark. Master’s thesis, VU University Amsterdam (2009).
[85]KaushikVeeraraghavan,DongyoonLee,BenjaminWester,JessicaOuyang,Pe-
terM.Chen,JasonFlinn,andSatishNarayanasamy.2012. DoublePlay:Paralleliz-
ing Sequential Logging and Replay. ACM Trans. Comput. Syst. 30, 1, Article 3
(Feb. 2012), 24 pages. https://doi.org/10.1145/2110356.2110359
[86]Yan Wang, Harish Patil, Cristiano Pereira, Gregory Lueck, Rajiv Gupta, and
IulianNeamtiu.2014. Drdebug:Deterministicreplaybasedcyclicdebuggingwith
dynamic slicing. In Proceedings of annual IEEE/ACM international symposium on
code generation and optimization. ACM, 98.
[87]Min Xu, Rastislav Bodik, and Mark D. Hill. 2003. A "flight data recorder"
for enabling full-system multiprocessor deterministic replay. In 30th Annual
International Symposium on Computer Architecture (ISCA ’03). ACM, 122–135.
https://doi.org/10.1145/859618.859633
[88]JunfengYang,HemingCui,JingyueWu,YangTang,andGangHu.2014.Determin-ismIsNotEnough:MakingParallelProgramsReliablewithStableMultithreading.
Commun. ACM (2014).
[89]DingYuan, YuLuo, XinZhuang, GuilhermeRennaRodrigues, XuZhao, Yongle
Zhang, Pranay U Jain, and Michael Stumm. 2014. Simple testing can prevent
most critical failures: An analysis of production failures in distributed data-
intensive systems. In 11th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 14). 249–265.
[90]LongZheng,XiaofeiLiao,BingshengHe,SongWu,andHaiJin.2015. OnPerfor-
mance Debugging of Unnecessary Lock Contentions on Multicore Processors: A
Replay-based Approach. In (CGO ’15).
463
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. 