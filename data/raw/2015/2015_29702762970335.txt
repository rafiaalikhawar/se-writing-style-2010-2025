Greedy Combinatorial Test Case Generation using
UnsatisÔ¨Åable Cores
Akihisa Y amada
University of Innsbruck, Austria
akihisa.yamada@uibk.ac.atArmin Biere
Johannes Kepler University, Austria
biere@jku.at
Cyrille Artho, Takashi Kitamura, Eun-Hye Choi
National Institute of Advanced Industrial Science and Technology (AIST), Japan
{c.artho,t.kitamura,e.choi}@aist.go.jp
ABSTRACT
Combinatorial testing aims at covering the interactions of
parameters in a system under test, while some combinations
may be forbidden by given constraints (forbidden tuples).
In this paper, we illustrate that such forbidden tuples cor-
respond to unsatisable cores, a widely understood notion in
the SAT solving community. Based on this observation, we
propose a technique to detect forbidden tuples lazily during
a greedy test case generation, which signicantly reduces
the number of required SAT solving calls. We further re-
duce the amount of time spent in SAT solving by essentially
ignoring constraints while constructing each test case, but
then \amending" it to obtain a test case that satises the
constraints, again using unsatisable cores. Finally, to com-
plement a disturbance due to ignoring constraints, we im-
plement an ecient approximative SAT checking function
in the SAT solver Lingeling .
Through experiments we verify that our approach signi-
cantly improves the eciency of constraint handling in our
greedy combinatorial testing algorithm.
CCS Concepts
Software and its engineering !Software testing
and debugging;
Keywords
Combinatorial testing, test case generation, SAT solving
1. INTRODUCTION
Combinatorial testing (cf. [24]) aims at ensuring the qual-
ity of software testing by focusing on the interactions of pa-
rameters in a system under test (SUT), while at the same
time reducing the number of test cases that has to be ex-
ecuted. It has been shown empirically [23] that a signif-
icant number of defects can be detected by t-way testing ,which tests all t-way combinations of parameters at least
once, where tis a relatively small number.
Constraint handling , mentioned already by Tatsumi [34]
in the late '80s, remains as a challenging research topic in
combinatorial testing [25]. To illustrate the concept, we take
a simple web application example which is expected to work
in various environments listed as follows:
Parameter Values
CPU Intel, AMD
OS Windows, Linux, Mac
Browser IE, Firefox, Safari
Combinatorial testing aims at covering all combinations of
values, but not all of them are necessarily executable; e. g.,
we have the following constraints:
1. IE is available only for Windows.
2. Safari is available only for Mac.
3. Mac does not support AMD CPUs.
Thus one must take care of such combinations which cannot
be executed, called forbidden tuples . In the above example,
there are six forbidden tuples: fLinux;IEg,fLinux;Safarig,
fAMD;Macg, etc.
There is substantial work on combinatorial testing tak-
ing constraints and forbidden tuples into account, includ-
ingmeta-heuristic approaches [10, 17, 20, 27], SAT-based
approaches [29, 36], and greedy approaches, which is fur-
ther categorized into one-test-at-a-time (OTAT) approaches
[9, 11, 10] and in-parameter-order generalized (IPOG) ap-
proaches [37, 38].
Meta-heuristic approaches and SAT-based approaches of-
ten generate test suites that are smaller than the greedy ap-
proaches, although they usually require more computation
time (cf. [20, 36, 27]). Thus, these approaches are preferred
in case the cost of test execution is high.
On the other hand, there are practical needs for quickly
generating a test suite of reasonable size, while the size is
not the primary concern. For instance, if test execution is
automated, one might better start executing the test cases,
instead of waiting for a sophisticated algorithm to nd a
smaller test suite. Also in the phase of test modeling, one
might want to check how test cases look like for an unnished
test model, not expecting a highly optimized test suite that
would be benecial if it were to be executed.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970335
614
In the IPOG-based approach, Yu et al. [37] proposed an
ecient constraint handling mechanism which made the ap-
proach practical in the presence of complex constraints. In
more recent work, Yu et al. [38] signicantly improved the
eciency of the approach, by developing a dedicated analy-
sis of minimal forbidden tuples .
In the OTAT approach, on the other hand, such a signif-
icant progress towards ecient constraint handling has not
yet been made [8]. There is both theoretical and practical in-
terest in this approach: An ideal greedy OTAT algorithm|
ignoring constraints|is shown to deliver a test suite whose
size is logarithmic to the number of parameters in the input
SUT model [9]. A similar result is known [7] for the more
feasible density algorithm of this category. In addition, the
nature of generating \one test at a time" can be benecial
since one can start test execution before the entire test suite
has been generated.
In this paper, we introduce an ecient constraint handling
technique for the OTAT algorithms.
The rst challenge for ecient test suite generation is how
to eciently detect forbidden tuples. To this end, we exploit
the information of unsatisable cores , a notion widely under-
stood in the SAT community [5]. In essence, we point out
that every forbidden tuple corresponds to an unsatisable
core|more precisely, the failed assumptions in it. Using
failed assumptions, which IPASIR1-compliant SAT solvers
can provide, we propose a technique to lazily detect forbid-
den tuples during greedy test case construction.
The second challenge is that, due to the nature of OTAT
algorithms, still a larger number of SAT solving calls is
needed. We show that most of these SAT solving calls
are not required to guarantee the termination of the algo-
rithm, but are needed to ensure that the given constraints
are satised by the generated test cases. We introduce a
new technique to \amend" a test case|turn a test case that
possibly violates the constraints into one that satises the
constraints|again using failed assumptions. Then we show
that we can omit most of the SAT solving calls without af-
fecting the correctness of the overall algorithm.
Finally, this omission of SAT solving makes the greedy test
case generation heuristic to be approximative, i. e., it can
make a wrong choice that will later be amended. Hence, we
propose reducing the chance of making such wrong choices
by exploiting the internal reasoning of SAT solvers. We
added a new API function in the SAT solver Lingeling [2] that
instantly checks satisability but allows for the third answer
\unknown". We experimentally show that this technique
pays o in terms of the sizes of generated test suites, with a
mild computational overhead.
In principle, the proposed constraint handling method is
applicable to any algorithms that comply the OTAT frame-
work of Bryce et al. [8]. We implement the method in our
base OTAT algorithm that is inspired by PICT [11] and
AETG [9], and experimentally show that the proposed con-
straint handling method delivers a signicant improvement
to the eciency of the algorithm.
This paper is organized as follows: Section 2 denes com-
binatorial testing formally, and Section 3 describes the us-
age of embedded SAT solvers. Section 4 shows the greedy
(base) variant of our algorithm, where we observe that for-
bidden tuples correspond to unsatisable cores, as described
1IPASIR is the new incremental SAT solver interface used
in the SAT Race/Competition 2015/2016.in Section 5. Section 6 uses unsatisable cores to amend test
cases, and Section 7 uses an extension of our SAT solver
Lingeling [2] to optimize this algorithm. Section 8 gives on
overview of related work. The results of our experiments are
shown in Section 9, and Section 10 concludes.
2. COMBINATORIAL TESTING
We dene several notions for combinatorial testing. First,
we dene a model of a system under test (SUT).
Denition 1. AnSUT model is a triplehP;V;iof
a nite set Pofparameters ,
a familyV=fVpgp2Pthat assigns each p2Pa nite
setVpofvalues , and
a boolean formula called the SUT constraint , whose
atoms are pairshp;viofp2Pandv2Vp.
Hereafter, instead of hp;viwe writep:vor evenvif no
confusion arises.
A valid test case is a choice of values for parameters that
satisfy the SUT constraint.
Denition 2 (test cases) .Given SUThP;V;i, atest case
is a mapping :P!SVthat satises the domain con-
straint :(p)2Vpfor everyp2P. A test case is called
valid if it satises ; more precisely, the following assign-
mentbsatises.
b(p:v) :=(
True if(p) =v
False otherwise
We call a set of valid test cases a test suite .
Example 1. Consider the web-application mentioned in the
introduction. The SUT model hP;V;ifor this example con-
sists of the following parameters and values:
P=fCPU;OS;Browserg
VCPU=fIntel;AMDg
VOS=fWindows;Linux;Macg
VBrowser =fIE;Firefox;Safarig
Following convention, the size of this model is denoted as
2132, meaning that there is one parameter with two values
and two with three values. The constraint in the introduction
are expressed by the following SUT constraint:
:= (IE)Windows)^(Safari)Mac)^(Mac):AMD)
The following table shows a test suite for the SUT model
consisting of seven valid test cases.
No. CPU OS Browser
1AMD Windows IE
2Intel Windows Firefox
3Intel Linux Firefox
4Intel Windows IE
5Intel Mac Safari
6AMD Linux Firefox
7Intel Mac Firefox
The observation supporting combinatorial testing is that
faults are caused by the interaction of values of a few pa-
rameters. Such interactions are formalized as follows.
615Denition 3 (tuples) .Given SUThP;V;i, aparameter
tuple is a subset Pof parameters, and a (value) tuple
overis a mapping :!SVthat satises the domain
constraint. Here, is denoted by Dom().
We identify a tuple with the following set:
=fp:vj(p) =vis denedg
A test case is also a tuple s. t. Dom() =P.
Denition 4 (covering tests) .We say that a test case 
covers a tuplei, i. e., value choices in meet.
A tuple is possible i a valid test case covers it; otherwise,
it is forbidden . Given a set Tof tuples, we say that a test
suite  isT-covering i every2Tis either forbidden or
covered by some 2 .
The covering test problem is to nd aT-covering test suite.
The terms t-way ort-wise testing and covering arrays (cf.
[30]) refer to a subclass of the covering test problems, where
Tis the set of all value tuples of size t. The number tis
called the strength of combinatorial testing.
Example 2. The SUT model of Example 1 has 21 tuples of
size two, where six out of them are forbidden. The test suite
in Example 1 covers all the 15 possible tuples and thus is a
2-way covering test suite for the SUT model.
3. SAT SOLVING
Satisability (SAT) solvers [5] are tools that, given a
boolean formula in conjunctive normal form (CNF) , decide
whether it is possible to instantiate the variables in the for-
mula such that the formula evaluates to true.
More formally, consider a boolean formula over a set
Xof variables. An assignment is a mapping :X !
fTrue;Falseg. It satises a formulaievaluates to
True after replacing every variable xinby(x). A for-
mula is satisable if it can be satised by some assignment,
and is unsatisable otherwise.
When SAT solvers conclude unsatisability, they are typ-
ically able to output a (minimal) unsatisable core, which
is dened as follows. Here, we consider a CNF also as a set
of clauses.
Denition 5. Anunsatisable core of a CNFis a sub-
set ofwhich is unsatisable. An unsatisable core is
minimal if any proper subset of is satisable.
3.1 CDCL
The DPLL algorithm [12] with conict-driven clause learn-
ing (CDCL) [28] is a de facto standard architecture of SAT
solvers. The CDCL approach constitutes a backtrack-based
search algorithm, which eciently scans the search space of
possible assignments for the variables of a given formula. Its
basic procedure is to repeat (1) choosing a variable and as-
signing a truth value for it ( decision ) and (2) simplifying the
formula based on decisions using unit propagation . During
this procedure, it may detect a conict of some combinations
of decisions and propagated assignments. Then a cause of
the conict|a set of decisions that derives it|is analyzed,
and a clause is learned to avoid the same conict later on.
After backtracking the learned clause forces the assignment
of one of its variables to be ipped, which might trigger
further propagation. During this procedure the algorithmalso checks whether all variables have been assigned and no
more propagations are pending. In this case it terminates
indicating that the formula is satisable.
3.2 Incremental SAT Solving and Failed As-
sumptions
Incremental SAT solving facilitates checking satisability
for a series of closely related formulas. It is particularly im-
portant [33, 13] in the context of bounded model checking [4].
State-of-the-art SAT solvers like Lingeling [3] implement the
assumption-based incremental algorithm, as pioneered by
the highly inuential SAT solver MiniSAT [14].
Incremental SAT solvers remember the current state and
do not just exit after checking the satisability of one input
formula. Besides asserting clauses that will be valid in the
later satisability checks, incremental SAT solvers accept
assumption literals [14], which are used as forced decision
and are only valid during the next incremental satisability
check, thus abandoned in later checks.
When an incremental SAT solver derives unsatisability,
it is particularly important to know which assumption liter-
als are a cause of unsatisability|in other words, constitute
an unsatisable core. Such literals are called failed assump-
tions .
The interface of an incremental SAT solver is expressed
in an object-oriented notation as follows, which is also com-
patible with the IPASIR interface.
class solverf
literal newVar ();
void assert (clauseC);
void assume (literall);
bool check ();
assignment model ; // refers to a solution if exists
listhliteralifailed assumptions ;
g;
4. BASE GREEDY ALGORITHM
Before introducing constraint handling, we introduce our
base OTAT algorithm without constraint handling, which is
shown as Algorithm 1.
The algorithm works as follows: To generate one test case,
the rst step picks up a parameter tuple that has most un-
covered tuples, and xes those parameters to cover one of
the uncovered tuples (lines 2{3). The second step greed-
ily chooses a parameter and a value so that the number of
newly covered tuples is maximized (lines 4{6). It may hap-
pen that xing any single parameter/value will not increase
the number of covered tuples, but xing more than two will.
In such a case, we apply the rst step again to x multiple
parameters (lines 7{9).
The rst step mimics that of PICT , while the second step
is similar to AETG . The main dierence from AETG is that
we search all unxed parameters for a value that maximizes
the number of newly covered tuples, while AETG randomly
chooses a parameter to be xed and searches only for the
best value for the parameter. Here, we do some clever com-
putation in order to eciently search all parameters. Due to
this dierence, our algorithm is deterministic (for tie break-
ing we use the deterministic random number generator of
the standard C library) and produces a fairly small test suite
without requiring multiple test case generation runs.
616Algorithm 1: Basic Greedy Test Case Generation
Input: An SUT modelhP;V;iand a setTof tuples
Output: AT-covering test suite  
1whileT6=;do
2 choose2Ts. t.Dom() contains most uncovered
tuples (as in PICT );
3 ;// cover at least this tuple
4 while there are an unxed parameter pand a value
vs. t.[fp:vgcovers some uncovered tuples do
5 choose such pandvthat maximize the number
of covered tuples;
6 [fp:vg;
7 ifthere is an uncovered tuple that may be covered
by xing more than two parameters in then
8 choose such as in PICT ; [;
9 go to line 4;
10 Fix unxed parameters in to arbitrary values;
11    [fg;// add the new test case
12 Remove from Tthe tuples covered by ;
4.1 Naive Constraint Handling
Now we present a variant of Algorithm 1 with naive con-
straint handling using incremental SAT solving, as already
proposed by Cohen et al. [10].
Consider an SUT model hP;V;i. To represent a test case
as a SAT formula, we introduce a boolean variable p:vfor
eachp2Pandv2Vp, denoting(p) =v. Since(p) must
be uniquely dened, we impose the following constraint:2
Unique :=^
p2P
1 =X
v2Vpp:v
In the following algorithms, tool is assumed to be a SAT
solver instance on which Unique^is asserted.
Algorithm 2 shows our rst algorithm called\naive", which
however utilizes incremental SAT solving. It works as fol-
lows: Before generating test cases, it rst removes all for-
bidden tuples in the set Tof target tuples that has to be
covered (lines 4{5). Then, whenever it chooses a tuple or
value, it checks if the choice does not violate the constraint
with already xed values in (lines 9 and 11).
The use of incremental SAT solving reduces the total cost
of SAT solving [10]. Nevertheless, as we will see in the exper-
imental section, Algorithm 2 is not ecient for large-scale
test models due to the large number of required SAT solving
calls. Hence in the next section, we try to improve eciency
by reducing the number of SAT solving calls.
5. FORBIDDEN TUPLES AS CORES
The most time-consuming part of Algorithm 2 consists of
lines 4{5, where all forbidden tuples are removed a priori .
Note that for t-way testing of an SUT model of size gk, the
number of tuples in T, that is, the number of SAT solving
calls needed in this phase, sums up to O(gtkt).
Hence, as the rst improvement to this naive algorithm,
we propose to remove forbidden tuples lazily . The key ob-
servation is that a forbidden tuple corresponds to the set of
failed assumptions in an unsatisable core.
2In order to encode the above formula into a CNF, we use
theladder encoding [18] for at-most-one constraints.Algorithm 2: Naive Treatment of Constraints
1function Possible ()
2 foreachv2dotool:assume (v);
3 return tool:check ();
4foreach2Tdo// remove forbidden tuples
5 if:Possible ()thenT Tnfg;
6whileT6=;do// main loop
7 choose2Tas in PICT ;
8 ;// cover at least this tuple
9 while there existpandv2Vps. t.[fp:vgcovers
new tuples and Possible ([fp:vg)do
10 Choose such best pandv; [fp:vg;
11 ifthere is2Ts. t.Possible ([)then
12 choose such as in PICT ; [;
13 go to line 9;
14 At this point, Possible () =True is ensured. Fix
unxed parameters in according to tool:model ;
15    [fg;// add the new test case
16 Remove from Tthe tuples covered by ;
Example 3. Consider the SUT model of Example 1, and
suppose that in a test case generation procedure, Browser has
been xed to Safari andOShas been xed to Mac. Note that
no conict will arise at this point. Next, consider xing CPU
toAMD . This choice raises unsatisability in the Possible
call in line 9 of Algorithm 2. The corresponding minimum
unsatisable core is either of the following:
Mac^AMD^(Mac):AMD) (1)
Safari^AMD^(Safari)Mac)^(Mac):AMD) (2)
The set of failed assumptions in (1)isfMac;AMDg; this
indicates that one cannot x OStoMac andCPU toAMD
in a test case, i. e., fMac;AMDgis a forbidden tuple. Sim-
ilarly, core (2)indicates thatfSafari;AMDgis a forbidden
tuple. In either case, we detect a forbidden tuple.
Now we introduce Algorithm 3, called \lazy", which omits
to remove the forbidden tuples a priori , but lazily removes
them when SAT checks show unsatisability.
Algorithm 3: Lazy Removal of Forbidden Tuples
1function Possible' ()
2 foreachv2dotool:assume (v);
3 iftool:check ()then return True ;
4 else
5T f02Tjtool:failed assumptions ()*0g;
6 return False ;
7while there is2Ts. t.Possible' ()do
8 choose2Tas in PICT ; ;
9 Do lines 9{13 of Algorithm 2, where Possible is
replaced by Possible' ;
Function Possible' () checks if the tuple (or equivalently,
partial test case) is possible (line 3). If it is not the case,
then the SAT solver provides a set of failed assumptions,
such asfMac;AMDgin Example 3. We now know that tu-
ples containing the failed assumptions are forbidden; hence
we remove such tuples from T(line 5).
617The correctness of Algorithm 3, i. e., that it terminates
and generates a T-covering test suite, is easily proven.
Proposition 1. Algorithm 3 is correct.
Proof. In every iteration of the main loop, the rst chosen
in line 8 is either removed or covered by the newly added
test case. Hence, the algorithm terminates as jTjstrictly
decreases in each iteration. Clearly, all tuples in Tare either
forbidden or covered by the output test suite.
In many examples, the lazy removal of forbidden tuples
signicantly improves the runtime of the algorithm. In cer-
tain cases, however, this approach still suers from an ex-
cessive large number of SAT solving calls (see also the ex-
perimental section). This phenomenon is caused by the sec-
ond occurrence of PICT-like value choices (line 11 in Algo-
rithm 2); if the constraint is so strict that the current values
inare not compatible with any other remaining tuples in
T, then at line 11 we have to perform SAT checks for all
the remaining tuples in T. Hence in the next section, we
consider to even omit these SAT checks.
6. CORES FOR AMENDING TEST CASES
The termination argument of Proposition 1 shows that
only the rst SAT check in every iteration is crucial to
achieve the termination of the algorithm. The remaining
SAT checks are only necessary to ensure that intermediate
value choices in never contradict the SUT constraint .
However,need not always respect ; it suces if the nal
test cases added to the output in line 16 satisfy .
Hence, in all iterations we can omit the SAT checks except
for the rst one. Unfortunately, then the resulting test case
is not guaranteed to be valid in line 14 anymore. To solve
this problem, we propose a technique to \amend" such an
invalid test case and turn it into a valid one, again using the
information of failed assumptions.
In general, if an incremental SAT solver detects unsatisa-
bility, then one of the failed assumptions must be removed in
order to satisfy the formula. In our application, this means
that some of the value assignments must be abandoned. By
repeatedly removing failed assumptions until the formula
becomes satisable, we can derive a test case that satises
the SUT constraint.
Example 4. Consider again the SUT model of Example 1,
and the following invalid test case:
=fSafari;Mac;AMDg
Possible' ()will return False with a set of failed assump-
tions, e. g.,fMac;AMDg. This indicates that at least either
Mac orAMD must be removed from the test case. Thus
consider removing, e. g., AMD :
0=fSafari;Macg
Possible' (0)returns True , with a satisfying assignment
00=fSafari;Mac;Intelg
which is a valid test case.
It is important to properly choose which value assignment
to remove. Note that even the termination of the algorithm
cannot be ensured if one removes the rst assumptions that
are made to cover the rst tuple in line 8.For this choice, we propose to remove the failed assump-
tion that corresponds to the most recently chosen value
assignment. The underlying observation for this choice is
that later value assignments are decided depending on ear-
lier choices; the earlier the value assignment is chosen, the
more inuential it is to the coverage of the test case.
This algorithm, which we call \amend", is shown in Algo-
rithm 4.
Algorithm 4: Amending Test Cases
1function Amend ()// makea valid test case
2 while:Possible' ()do
3 Identifyp:v2tool:failed assumptions that is the
most recently xed;
4 nfp:vg;
5 Fix unxed parameters in according to tool:model ;
6 return;
7while there is2Ts. t.Possible' ()do
8 choose2Tas in PICT ; ;
9 Do lines 4{9 of Algorithm 1 (ignoring constraints);
10 amend ();
11    [fg;// add the new test case
12 Remove from Tthe tuples covered by ;
Proposition 2. Algorithm 4 is correct.
Proof. The crucial point is that the assumptions made in
line 8 to cover the rst tuple will not be removed. Since
the satisability of is ensured, any unsatisable core that
is found later must contain a failed assumption that is added
by later greedy choice. In the worst case, all the choices but
may be removed, but still Tstrictly decreases by each
iteration.
As we see in the above proof, Algorithm 4 is correct;
however, it may happen that many value choices in a test
case, which are chosen to cover as many tuples as possi-
ble, are eventually abandoned. This may in some particular
cases result in preferable randomness that eventually yields a
smaller test suite, but in general disturbs the greedy heuris-
tic and results in a larger test suite, especially when the con-
sidered SUT constraint is so strict that most value choices
violate the constraint. Note also that the notion of strict-
ness does not directly correspond to the size or complexity
of the constraint.
7. A VOIDING WRONG CHOICES
Finally, we reduce the chance of making wrong choices
by extending the SAT solver Lingeling with the following
method:
bool imply (literall);
Method imply (l) addslas an assumption literal just like
assume (l), but it additionally performs the unit propaga-
tion. If the unit propagation causes a conict, then the
method returns False and the SAT solver goes into the
state where it derives unsatisability. Otherwise, lis added
as an assumption literal.
Using this method we implement a function Maybe , which
is an approximate variant of Possible' .Maybe () tests if
618is possible by dispatching the imply method on the back-
end SAT solver, but will not perform the actual satisability
check. That is, this function may fail to detect a conict,
even ifis actually impossible. However, if it detects a
conict, then is indeed impossible and the lazy removal of
forbidden tuples (Section 5) is performed.
The overall algorithm \imply", which leverages the new
imply method, is presented in Algorithm 5.
Algorithm 5: Avoiding Wrong Choices
1function Maybe ()
2 foreachv2do
3 if:tool:imply (v)then
4 T fjtool:failed assumptions ()*g;
5 return False ;
6 return True ;
7while there is2Ts. t.Possible' ()do
8 choose2Tas in PICT ; ;
9 Do lines 9{13 of Algorithm 2, where Possible is
replaced by Maybe ;
10 amend ();
11    [fg;// add the new test case
12 Remove from Tthe tuples covered by ;
Since Maybe is only approximate, it is not ensured that
the test case is valid after all values are xed. Thus, the
application of the Amend function from the previous section
is a crucial step (line 10).
Proposition 3. Algorithm 5 is correct.
Proof. The reasoning is the same as Proposition 2.
The estimation quality of Maybe depends on how many
clauses have been learned by the SAT solver. At the be-
ginning of test suite generation, it may often fail to detect
conicts, but at this stage disturbance by Amend should
be small, since most tuples are yet to be covered and any
test case will cover some of them. As more test cases are
generated, Maybe becomes more precise.
8. RELATED WORK
Here we recall previous work towards ecient constraint
handling in combinatorial testing, and remark a few other
uses of SAT solving for test case generation.
8.1 PICT
PICT [11] is a well-known combinatorial testing tool based
on the OTAT approach. For constraint handling, it precom-
putes allforbidden tuples|regardless of t|and uses this in-
formation when greedily constructing a test case [37]. This
approach is quite fast if the SUT constraint is weak, i. e., only
few tuples are forbidden. However, it becomes intractable
if the constraint is so strict that a great number of tuples
are forbidden. Note again that the strictness of constraint
does not correspond to the complexity or the size of the con-
straint. On the other hand, the eciency of our approach is
stable from the strictness of the SUT constraint.8.2 AETG
Cohen et al. [10] pioneered the use of incremental SAT
solving for constraint handling in their OTAT algorithm
AETG [9]. All their algorithms however remove forbidden
tuples before the actual test case generation phase, as in
our \naive" algorithm. Hence, their AETG variants would
also benet from our ideas. We also did not follow their
\may" and \must" analysis, which was introduced to prevent
the back-end SAT solver from encountering unsatisability.
In our usage, deriving unsatisability is the key to detecting
forbidden tuples, and we have no reason to prevent it.
8.3 ACTS
ACTS [6] is another successful combinatorial testing tool,
which is based on the IPOG algorithm [26]. Yu et al. [38]
improved the constraint handling of the IPOG algorithm
using the notion of minimum forbidden tuples (MFTs) , i. e.,
the forbidden tuples whose proper subsets are not forbidden.
Their rst algorithm precomputes all MFTs, and uses this
information during the test case generation. Moreover, to
relax the cost of computing all MFTs, which is signicant
if the constraint is complex [38], they further introduced
an algorithm using necessary forbidden tuples (NFTs) , that
computes forbidden tuples when it becomes necessary.
Their work largely inspired us, although we did not choose
IPOG as our base greedy algorithm. The notion of MFTs is
somewhat related to minimal unsatisable cores, i. e., unsat-
isable cores whose proper subsets are satisable. The idea
of the NFT algorithm is also visible in our \lazy" algorithm.
However, while minimal unsatisable cores are naturally ob-
tained by CDCL SAT solving, computing MFTs is a hard
optimization problem since one has to further minimize un-
satisable cores in terms of the failed assumptions.
8.4 ICPL
In the context of software product lines (SPLs) , Johansen
et al. [21] introduced the ICPL algorithm for t-way test suite
generation for SPLs. ICPL also incorporates SAT solvers,
and take a dierent approach to reduce the cost of precom-
puting forbidden tuples. Their algorithm generates t-way
test suite by generating t0-way test suites for t0= 1;2;:::;t .
Using the information of t0-way forbidden tuples, they pro-
posed an algorithm to eciently compute ( t0+ 1)-way for-
bidden tuples.
Compared to their work, our approach does not require a
particular phase for computing forbidden tuples, since they
are provided for free by incremental SAT solvers as failed
assumptions. We leave it for future work to experimentally
compare our tool with ICPL; these tools assume dierent
input formats.
8.5 Constraints in Other Approaches
Of course, there are eorts towards constraint handling in
non-greedy algorithms. The SAT-based approach encodes
entire test suite generation as a SAT formula [19], and hence
constraints can be naturally encoded [29]. Also the use of
SAT solving has been proposed for the simulated-annealing-
based tool CASA [16]. Lin et al. [27] recently introduced the
two-mode meta-heuristic approach in combinatorial testing,
and their tool TCA produces notably small test suites.
The primary concern of these approaches are not on ex-
ecution time, but on the size of test suites. Indeed, CASA ,
TCA, and the SAT-based test suite optimization function in
619x1x10x100
Naive Lazy Amend Imply2-way
x1x10x100x1000
Naive Lazy Amend Imply3-wayFigure 1: Comparison of the runtime of Algorithms 2{5. The ratio of the runtime over the best among all
algorithms for each benchmark are plotted.
x1x1.05x1.1x1.15x1.2x1.25
Naive Lazy Amend Imply2-way
x1x1.05x1.1x1.15x1.2x1.25
Naive Lazy Amend Imply3-way
Figure 2: Comparison of Algorithms 2{5 in terms of the sizes of the generated test suites.
Calot [36] do not output a test suite even when they inter-
nally have one; they try to optimize it as far as possible,
until certain termination conditions are met.
Nevertheless, we observe some cases where our work may
provide a benet to these approaches; e. g., Calot previously
employed ACTS for constructing an initial test suite for op-
timization, which is now done eciently inside Calot .TCA
also employs a greedy algorithm for the initial test suite, and
one of its \two modes" is a \greedy" mode, where we expect
our technique can improve its eciency.
Farchi et al. [15] proposed using unsatisable cores in the
test modeling phase of combinatorial testing. Their test
modeling tool FOCUS tests if tuples are possible or forbid-
den as expected. If a tuple is unexpectedly forbidden, then
the tool analyzes the unsatisable core and indicates which
clauses of the SUT constraint forbid the tuple. Thus users
can eectively model an intended the SUT constraint. Our
use of unsatisable cores makes a good contrast: Farchi et al.
[15] consider assumed value choices are correct and xes the
clauses in an unsatisable core, while we consider the clauses
are correct and x the value choices.
8.6 SAT Solvers in Model-Based Testing
The model-based testing (MBT) (cf. [32]) considers more
elaborated SUT models compared to combinatorial testing;
namely, states of SUTs are considered. MBT tools aim at
generating sequences of test cases which ensure a certain
path-coverage criterion.
The use of incremental SAT solving in MBT is also pro-
posed [1]. We expect that it is also interesting to use unsat-
isable cores to improve such SAT-based MBT tools; e. g.,
it might be able to eciently detect \forbidden paths". We
leave it for future work to explore to this direction.9. EXPERIMENTS
We implemented Algorithms 2{5 in our tool Calot , and
conducted experiments to investigate the following research
questions.
RQ1 How ecient is the \lazy" algorithm compared to the
\naive" one? How does it aect the sizes of test suites?
RQ2 How ecient is the \amend" algorithm compared to
\lazy"? How does it aect the sizes of test suites?
RQ3 How much does the \imply" algorithm improve the
sizes of test suites compared to \amend"? How does it
aect the eciency of the algorithm?
RQ4 How does the \imply" algorithm compare with other
greedy test case generation tools?
As the benchmark set, we collected the following:
the 35 benchmarks from Cohen et al. [10],
the 20 industry applications from Segall et al. [31],
the two industry applications from Yu et al. [38],
the 18 industry applications from Kitamura et al. [22],
and two applications from our industry collaborators.3
The experiments were run on a laptop with a 2.59GHz
Intel Core i5-4310U processor and 4GB of RAM running
Windows 10.
Figure 1 compares our four algorithms in terms of runtime
for generating both 2-way and 3-way test suites, and Figure 2
compares the sizes of the generated test suites.
3One of our examples is available at https://sta.aist.go.jp/
t.kitamura/dl/.
620x1x10x100x1000
Wins
TimeoutsPICT
35
3ACTS
0
1Imply
48
02-way
x1x10x100x1000
Wins
TimeoutsPICT
5
4ACTS
34
1Imply
38
03-wayFigure 3: Comparing relative execution times with other tools.
x1x1.1x1.2x1.3x1.4x1.5
Wins
TimeoutsPICT
12
3ACTS
30
1Imply
59
02-way
x1x1.1x1.2x1.3x1.4x1.5
Wins
TimeoutsPICT
12
4ACTS
14
1Imply
63
03-way
Figure 4: Comparing test suite sizes with other tools.
The box-plots show the distribution of the data: The
box encompasses the rst and third quartile of the distri-
bution, with the middle line denoting the median value of
the data. The \whiskers" are drawn at 1.5 times the in-
terquartile range, to the data point closest to 1.5 times the
distance between the median and the lower/upper quartile.
Points outside that range are considered outliers and shown
as a dot. To measure the validity of our claims, we also
report thep-values in the Wilcoxon signed-rank test [35].
RQ1: Naive vs. Lazy
From Figure 1 we observe that \lazy" signicantly improves
the average execution time over \naive". The signicance of
the dierence is p<0:0001 for both 2- and 3-way cases. As
explained in Section 5, however, there are a few examples in
3-way case where the runtime does not improve.
In terms of the sizes of test suites, from Figure 2 we ob-
serve only minor dierence in 3-way case where \lazy" can
be slightly worse than \naive", with p0:1711.
RQ2: Lazy vs. Amend
Here we measure the improvement due to amending invalid
choices (see Section 6). In Figure 1, we observe further im-
provement in the eciency of the \amend" algorithm over
\lazy". The signicance is p<0:0001 for both 2- and 3-way
cases. On the other hand, the sizes of generated test suites
get noticeably worse (Figure 2), with p<0:0003.RQ3: Amend vs. Imply
Now we measure the improvement due to the new \imply"
method (see Section 7). Our nal algorithm \imply" im-
proves the sizes of test suites over the previous \amend"
algorithm with signicance p <0:0002, and apparently re-
covers to a similar result to \lazy". On the other hand, the
overhead in runtime over\amend"is noticeable ( p<0:0002).
RQ4: Comparison with Other Greedy Tools
Finally, we compare our algorithms with the OTAT-based
greedy tool PICT (version 3.3) and the IPOG-based greedy
toolACTS (version 2.93).
The results are shown in Figures 3{5, and also summarised
in Table 1. The scatter-plots in Figure 5 compare individ-
ual data points between two settings. A data point above
(below) the diagonal implies a higher (lower) value for the
baseline algorithm|the naive algorithm.
For a few benchmarks PICT and ACTS do not respond
within a reasonable time, so we set 3600 seconds as timeout.
In case of timeout, we assumed 3600 seconds as the runtime.
We do not know the size of the output test suite in these
cases; hence Figure 4 excludes the case of timeouts in the
box-plots. Instead, the number of timeouts are reported.
The gures also report the number of \wins", i. e., how of-
ten the tool achieved the best result among others. When
counting wins, ties are counted as a win for all tied tools.
When comparing the size of test suites, from Figure 4 we
clearly observe that our algorithm outperforms others. The
621Table 1: Detailed comparison with other tools. For each categories the average (avr.), geometric mean (g.m.),
and the number of wins are reported. Fields with `{' cannot be computed due to timeouts.
2-way 3-way
imply PICT ACTS imply PICT ACTS
Source # size time size time size time size time size time size time
Cohen et al. [10] 35avr. 34.9 0.3 43.9 0.2 36.4 4.6 209.0 26.5 252.0 83.2 219.7 5.6
g.m. 32.5 0.2 40.4 0.2 33.8 3.9 174.6 8.4 206.5 16.4 183.6 3.2
wins 32 19 1 18 9 0 33 3 1 3 3 29
Segall et al. [31] 20avr. 74.5 0.1 77.0 0.2 73.9 1.0 668.0 1.2 684.5 1.9 675.0 1.1
g.m. 37.1 0.1 39.7 0.1 37.1 0.8 163.9 0.3 174.0 0.5 171.2 0.9
wins 11 13 4 10 12 0 15 14 5 2 5 4
Yu et al. [38] 2avr. 467.5 1.5 { 1800.7 461.0 369.5 7112.5 225.9 { 1805.4 8054.5 360.6
g.m. 348.6 0.9 { 71.7 344.8 146.7 3716.1 64.4 { 196.4 4117.5 149.0
wins 0 2 0 0 2 0 2 2 0 0 0 0
Kitamura et al. [22] 18avr. 74.7 0.1 75.8 217.7 76.3 15.3 559.8 3.5 { 220.7 567.6 14.9
g.m. 22.7 0.1 22.1 0.3 23.8 1.8 71.6 0.2 { 1.1 74.3 1.8
wins 14 12 7 7 7 0 11 17 6 0 6 1
Company A 1 42.0 0.1 44.0 3221.5 43.0 120.7 126.0 0.8 { 3600.0 128.0 97.7
Company B 1 81.0 0.2 { 3600.0 { 3600.0 369.0 0.8 { 3600.0 { 3600.0
0.1s1s10s100s1000s
0.1s 1s 10s 100s2-way
 naive
PICT
ACTS
imply
0.1s1s10s100s1000s
0.1s 1s 10s 100s 1000s 10000s3-way
 naive
PICT
ACTS
imply
Figure 5: Comparing runtime with other tools. The vertical axis presents the runtime of each algorithm,
while the horizontal axis presents the runtime the \naive" algorithm took for the same benchmark.
signicance is p < 0:0001 for both 2- and 3-way, and for
both PICT andACTS .
When comparing runtime, from Figure 3 one might think
that our \imply" is faster than the other tools. However,
looking more detail in Figure 5 and Table 1 we see that
the other tools perform quite well for many benchmarks. In
particular, PICT is fast for some 2-way examples and ACTS
is remarkably fast for many 3-way examples from Cohen
et al. [10]. We conjecture that the eciency of these tools
depends on the weakness of the constraints.
10. CONCLUSION
In this paper, we have developed constraint handling tech-
niques for one-test-at-a-time (OTAT) combinatorial test case
generation algorithms, using the information of unsatisable
cores. We implemented the proposed constraint handling
methods in the OTAT algorithm of our test case generation
tool Calot . Through experiments we veried that our tech-
niques signicantly improve the eciency of test case gen-eration without sacricing the size of generated test suites,
compared to constraint handling utilizing incremental SAT
solvers naively. We also compared our tool with the greedy
test case generation tools PICT and ACTS , and observed
that our tool perform well in terms of both eciency and
the size of generated test suite, which are usually a trade-o
with each other.
Limitation and Future Work.
Although Calot performed well in our experiments, if one
ignores constraints, the IPOG algorithm of ACTS is remark-
ably faster than our base algorithm for large SUT models
and higher-strength cases. This is explained by the fact
that our base algorithm always has to compute the best pa-
rameters and values to x, although this eort often results
in a smaller test suite. Hence, we could only observe in some
industry examples that the minimum forbidden tuple com-
putation of Yu et al. [38] may become a signicant overhead.
We leave it for future work to implement our constraint han-
dling approach in the IPOG algorithm to fairly compare our
622constraint handling and the minimum forbidden tuple ap-
proach.
In principle, our constraint handling method can be imme-
diately generalized to the OTAT framework by Bryce et al.
[8]. We leave it for future work to develop such a frame-
work where one can plug-in their own OTAT heuristics (such
as the AETG orPICT heuristics), without being concerned
about constraint handling.
11. ACKNOWLEDGMENTS
We would like to thank our two industry collaborators for
allowing us to present experimental results on their applica-
tions. We thank the anonymous reviewers for their construc-
tive and careful comments, which signicantly improved the
presentation of the paper. This work is in part supported
by JST A-STEP grant AS2524001H.
12. REFERENCES
[1] P. Abad, N. Aguirre, V. Bengolea, D. Ciolek, M. F.
Frias, J. Galeotti, T. Maibaum, M. Moscato, N. Rosner,
and I. Vissani. Improving test generation under rich
contracts by tight bounds and incremental sat solving.
InICST 2013 , pages 21{30, 2013.
[2] A. Biere. Lingeling, Plingeling and Treengeling entering
the SAT competition 2013. In SAT Competition 2013 ,
pages 51{52, 2013.
[3] A. Biere. Yet another local search solver and Lingeling
and friends entering the SAT Competition 2014. In SAT
Competition 2014 , volume B-2014-2 of Department of
Computer Science Series of Publications B , pages 39{
40. University of Helsinki, 2014.
[4] A. Biere, A. Cimatti, E. M. Clarke, and Y. Zhu. Sym-
bolic model checking without BDDs. In TACAS 1999 ,
volume 1579 of LNCS , pages 193{207, 1999.
[5] A. Biere, M. J. H. Heule, H. van Maaren, and T. Walsh,
editors. Handbook of Satisability , volume 185 of FAIA .
IOS Press, February 2009.
[6] M. N. Borazjany, L. Yu, Y. Lei, R. Kacker, and
R. Kuhn. Combinatorial testing of ACTS: A case study.
InICST 2012 , pages 591{600, 2012.
[7] R. C. Bryce and C. J. Colbourn. The density algorithm
for pairwise interaction testing. Softw. Test., Verif. Re-
liab., 17:159{182, 2007.
[8] R. C. Bryce, C. J. Colbourn, and M. B. Cohen. A
framework of greedy methods for constructing interac-
tion test suites. In ICSE 2005 , pages 146{155, 2005.
[9] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C.
Patton. The AETG system: An approach to testing
based on combinatorial design. IEEE Trans. Software
Eng., 23(7):437{444, 1997.
[10] M. B. Cohen, M. B. Dwyer, and J. Shi. Constructing
interaction test suites for highly-congurable systems in
the presence of constraints: A greedy approach. IEEE
Trans. Software Eng. , 34(5):633{650, 2008.
[11] J. Czerwonka. Pairwise testing in real world. In PNSQC
2006, pages 419{430, 2006.[12] M. Davis, G. Logemann, and D. Loveland. A machine
program for theorem-proving. Communications of the
ACM , 4:394{397, 1962.
[13] N. E en and N. S orensson. Temporal induction by in-
cremental SAT solving. Electr. Notes Theor. Comput.
Sci., 89(4):543{560, 2003.
[14] N. E en and N. S orensson. An extensible SAT-solver. In
SAT 2003 , volume 2919 of LNCS , pages 502{518, 2004.
[15] E. Farchi, I. Segall, and R. Tzoref-Brill. Using projec-
tions to debug large combinatorial models. In IWCT
2013, pages 311{320, 2013.
[16] B. J. Garvin, M. B. Cohen, and M. B. Dwyer. An im-
proved meta-heuristic search for constrained interaction
testing. In SSBSE 2009 , pages 13{22, 2009.
[17] B. J. Garvin, M. B. Cohen, and M. B. Dwyer. Evalu-
ating improvements to a meta-heuristic search for con-
strained interaction testing. Empir. Software Eng. , 16:
61{102, 2011.
[18] I. P. Gent and P. Nightingale. A new encoding of alld-
ierent into SAT. In International Workshop on Mod-
elling and Reformulating Constraint Satisfaction , pages
95{110, 2004.
[19] B. Hnich, S. D. Prestwich, E. Selensky, and B. M.
Smith. Constraint models for the covering test prob-
lem. Constraints , 11(2-3):199{219, 2006.
[20] Y. Jia, M. B. Cohen, M. Harman, and J. Petke. Learn-
ing combinatorial interaction test generation strategies
using hyperheuristic search. In ICSE 2015 , pages 540{
550, 2015.
[21] M. F. Johansen, . Haugen, and F. Fleurey. An algo-
rithm for generating t-wise covering arrays from large
feature models. In SPLC 2012, Volume 1 , pages 46{55,
2012.
[22] T. Kitamura, A. Yamada, G. Hatayama, C. Artho,
E. Choi, T. B. N. Do, Y. Oiwa, and S. Sakuragi. Com-
binatorial testing for tree-structured test models with
constraints. In QRS 2015 , pages 141{150, 2015.
[23] D. R. Kuhn, D. R. Wallace, and A. M. Gallo, Jr. Soft-
ware fault interactions and implications for software
testing. IEEE Trans. Software Eng. , 30(6):418{421,
2004.
[24] D. R. Kuhn, R. N. Kacker, and Y. Lei. Introduction to
Combinatorial Testing . CRC press, 2013.
[25] D. R. Kuhn, R. Bryce, F. Duan, L. S. Ghandehari,
Y. Lei, and R. N. Kacker. Chapter one { combinatorial
testing: Theory and practice. volume 99 of Advances
in Computers , pages 1{66. Elsevier, 2015.
[26] Y. Lei, R. Kacker, D. R. Kuhn, V. Okun, and
J. Lawrence. IPOG: A general strategy for t-way soft-
ware testing. In ECBS 2007 , pages 549{556, 2007.
[27] J. Lin, C. Luo, S. Cai, K. Su, D. Hao, and L. Zhang.
TCA: An ecient two-mode meta-heuristic algorithm
for combinatorial test generation. In ASE 2015 , pages
494{505, 2015.
623[28] J. Marques-Silva, I. Lynce, and S. Malik. Conict-
driven clause learning sat solvers. In A. Biere, M. J. H.
Heule, H. van Maaren, and T. Walsh, editors, Handbook
of Satisability , chapter 4, pages 131{153. IOS Press,
2009.
[29] T. Nanba, T. Tsuchiya, and T. Kikuno. Using sat-
isability solving for pairwise testing in the presence
of constraints. IEICE Trans. Fundamentals , E95-A(9),
2012.
[30] C. Nie and H. Leung. A survey of combinatorial testing.
ACM Computing Surveys , 43(2):11, 2011.
[31] I. Segall, R. Tzoref-Brill, and E. Farchi. Using binary
decision diagrams for combinatorial test design. In IS-
STA 2011 , pages 254{264, 2011.
[32] M. Shaque and Y. Labiche. A systematic review of
state-based test tools. Int. J. Softw. Tools Technol.
Transfer , 17:59{76, 2015.
[33] O. Shtrichman. Pruning techniques for the SAT-based
bounded model checking problem. In CHARME 2001 ,
volume 2144 of LNCS , pages 58{70, 2001.[34] K. Tatsumi. Test case design support system. In Proc.
International Conference on Quality Control (ICQC
1987) , pages 615{620, 1987.
[35] F. Wilcoxon. Individual comparisons by ranking meth-
ods. Biometrics Bulletin , 1(6):80{83, 1945. ISSN
00994987.
[36] A. Yamada, T. Kitamura, C. Artho, E. Choi, Y. Oiwa,
and A. Biere. Optimization of combinatorial testing by
incremental SAT solving. In ICST 2015 , pages 1{10.
IEEE, 2015.
[37] L. Yu, Y. Lei, M. Nourozborazjany, R. N. Kacker, and
D. R. Kuhn. An ecient algorithm for constraint han-
dling in combinatorial test generation. In ICST 2013 ,
pages 242{251. IEEE, 2013.
[38] L. Yu, F. Duan, Y. Lei, R. N. Kacker, and D. R. Kuhn.
Constraint handling in combinatorial test generation
using forbidden tuples. In IWCT 2015 , pages 1{9, 2015.
624