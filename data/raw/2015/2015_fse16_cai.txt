 
 A Deployable  Sampling Strategy for Data Race  Detection   
Yan Cai1, Jian Zhang1, Lingwei Cao1, and Jian Liu2 
1 State Key Lab oratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China  
2 Institute of Information Engineering , Chinese Academy of Sciences, Beijing, China  
ycai.mail@gmail.com , zj@ios.ac.cn , lingweicao@gmail.com , liujian6@iie.ac.cn  
 
ABSTRACT  
Dynamic data race detection incurs heavy  runtime overhead s. 
Recently, many sampling  techniques have been  proposed to detect 
data races. However, some sampling techniques (e.g., Pacer ) are 
based on traditional happens -before relation  and incur a large 
basic overhead. Others utilize hardware to reduce their sampling 
overhead (e.g., DataCollider ) and they, however, detect a race 
only when the race really occurs by delaying program executions. 
In this paper, we study the limitation s of existing techniques and 
propose  a new data race definition , named  as Clock Races , for low 
overhead sampling purpose . The innovation of clock races  is that  
the detection  of them  does not rely on concrete locks and also 
avoid s heavy basic overhead  from  tracking happens -before rela-
tion. We further propose  CRSampler  (Clock Race Samp ler) to 
detect clock races via hardware based sampling  without direct ly 
delaying program execution s, to further reduce runtime overhead. 
We evaluated CRSampler  on Dacapo benchmarks. The results 
show that CRSampler  incur red less than 5% overhead on average 
at 1% sampling rate. Whereas, Pacer  and DataCollider  incurred 
larger than 25% and 96% overhead , respectively . Besides, at the 
same sampling rate , CRSampler  detected significantly more  data 
races  than that by  Pacer  and DataCollider .  
CCS Concepts   
â€¢ Software and its engineering â Software testing and debug-
ging â€¢ Theory of computation âProgram verification .  
Keywords  
Data race, sampling, concurrency bugs , data breakpoints.  
1. INTRODUCTION  
A data race  (or race for short) occurs when two or more threads 
access the same memory location at the same time  and, at least 
one of these accesses is a write [19][45]. Race occurre nces may 
lead to occurrences of other concurrency bugs [39] and may result 
in real -world disasters [4][31][43].  
On race detection, s tatic techniques could  scale up to a whole 
program but may report many fa lse positives [26][37][41][51]. 
Dynamic techniques report fewer false positives. They are mainly 
based on either the lockset discipline  [45] or the happens -before 
relation  [19][29]. The l ockset discipline requires that all accesses to a shared memory location should be protec ted by a common set 
of locks. However, e ven violating such a discipline, no data race 
may occur [8][12][19]. The Happens -before relation  [29] is usual-
ly implemented via vector clocks  [19]. Each vector clock  contains 
n clock elements, where n is the number of threads.  They are used 
to track status es of th reads, lock s, and memory location s. Race 
detectors implementing vector clocks incur high er overhead than 
the lockset based ones . FastTrack  [19] further improves th e over-
head of the happens -before based race detectors to be the same 
level as that of lockset based ones by avoid ing most of O(n) oper-
ations  on memory accesses . However, FastTrack  still incurs from 
400% to 800%  overhead [12][19][54].  
To reduce runtime  overhead , sampling techniques [8][35][59] 
were  introduc ed to only monitor a small set of memory accesses . 
They could be deployed at the program user sites if they incur a n 
enough low overhead  (e.g., less than 5%) [8]. LiteRace  [35] targets 
to sample  memory accesses from  cold (i.e., not frequently called) 
functions. However, it fully monitors synchronization operations, 
even those in non -sampled functions , and maintains data struc-
tures for threads, locks, and memory locations ( needed by hap-
pens-before based race detect ors). As a result,  the overhead of  
LiteRace  varies from several percentages to ~140% [35]. Besides, 
LiteRace  needs to log various events for offline race detection, 
which may further prevent it from being deploy ed at user site s. 
Pacer  [8] introduces periodical sampling strategy. It only tracks 
memory accesses and synchronization operations in full during its 
sampl ing period s. In non -sampl ing period s, it only checks race 
occurr ences. However, Pacer  is based on dynamic sampling  (i.e., 
making sampling decision online)  and has to maintain basic data 
structures like LiteRace , incurring  certain  basic overhead. For 
example, with 0% and 3% sampling rates, Pacer  incurs 33% and 
86% overh ead, respectively [8]. Such  overhead makes Pacer  im-
practical to be deployed at user sites , as an acceptable overhead at 
user site s is usually 5% [6][27][33][60].  
The latest sampling strategy DataCollider  [17] completely dis-
cards both the monitoring on synchronization operations and the 
maintenance on data structures. It utilizes code and data break-
points of hardware archite ctures to support its sampling for  race 
detection . (A code breakpoint is s et to a program instruction and is 
fired if the instruction is executed. A data breakpoint is set to a 
memory address and is fired if a memory access to the address is 
executed .) DataCollider  firstly sets a code breakpoint on a random 
instruction. If this code breakpoint fires, it further sets a data 
breakpoint on the target address of this instruction and delays the 
execution of the instruction until the data breakpoint fires or a 
certain time limit is reached . If a data breakpoint fires, there must 
exist another access to the same  address. Then, a data race is re-
ported i f at least one of the two accesses is a write .  
DataCollider  could incur  a low runtime overhead by only focusing 
on memory accesses  via hardware supports . However , by discard-  
This is the author's version of the work. It is posted here for your personal use. Not 
for redistribution. The definitive version was published in the following publication:  
FSEâ€™16, November 13 â€“18, 2016, Seattle, WA, USA  
ACM. 978 -1-4503 -4218 -6/16/11...  
http://dx.doi.org/10.1145/2950290.2950310  
  
 ing the da ta structures of memory locations, it can only detect data 
races actually occurr ing in a run but may miss those races that do 
not occur but could be detected by happens -before based detectors 
like Pacer  and LiteRace . Besides, with a slight increas e in sam-
pling rate, its overhead increase s quickly  as it direct ly delay s pro-
gram execution . This increase is significantly  larger  than the in-
crease by Pacer . As a result, DataCollider  also become s ineffec-
tive, and could only work at user site s at an extremely low sam-
pling rate , which further makes it ineffective .  
In this paper, we firstly analyze  the limitations of existing sam-
pling approaches on race detection. We then propose a light but 
novel definition of data races , called Clock Races, based on 
thread -local clocks without the need of vector clock s and concrete 
locks . Clock races are provable to be also happens -before races 
(i.e., those detectable by happens -before based detectors, HB race  
for short). The benefit of clock races is that  the detection of them 
does not require heavy tracking on concrete locks . This result s in 
O(1) rather than  O(n) operations  on synchronization events  and 
hence further avoids memory maintenance overhead on the in-
volved locks . We then propose CRSampler , a novel  sampling ap-
proach for detection of clock races based on hardware support . 
Compared to Pacer , CRSampler  does not rely on heavy tracking of 
happens -before relation, avoid ing basic tracking overhead. Com-
pared to DataCollider , CRSampler  does not directly delay program 
execution to trap a second access, which not only reduce s runtime 
overhead  but also achieve s a bigger capability on race detection .  
Follow ing Pacer , we have implemented DataCollider  and 
CRSampler  within Jikes RVM [3] and compared CRSampler  with 
both DataCollider  and Pacer  on six benchmarks from Dacapo [9], 
including a large -scale eclipse . The experiment al results show 
that, at 1% sampling rate , CRSampler  only incurred less than  5% 
overhead  on average ; but Pacer  incurred more than 25 % overhead 
on average and DataCollider  incurred more than  96% overhead on 
average. CRSampler  also detected significantly more races  (i.e., 
404 races  in total ) than that by  Pacer  and DataCollider  (31 and 20 
races, respectively).  Besides , with the increas e in sampling rate  
from 0.1% to 1 .0% (with step 0.1%) , CRSampler  not only incurred 
the least overhead increase but also detected an obviously increas-
ing number of races. However, with the same increase on sam-
pling rate, both DataCollider  and Pacer  incurred a larger overhead 
increase than that by CRSampler ; they  almost detect ed a constant 
number of races on most of the benchmark s.  
In summary, the main contribution s of this paper are:  
ï‚· It presents a novel definition of data races, named  as clock 
races. Clock races are proved to be also happens -before rac-
es. Detection of clock races only requires O(1) operations in 
time on synchronization events , without the need of concrete 
synchronization objects .  
ï‚· It presents  CRSampler  framework to sample clock races 
based on hardware supports. Unlike existing techniques, 
CRSampler  does not directly delay program execution and 
hence incurs much lower overhead than existing ones.  
ï‚· We have implemented CRSampler  as a prototype tool (see 
http://lcs.ios.ac.cn/~ yancai/ cr/cr.html ). The experiments con-
firm that, compared to state-of-the-art happens -before based 
Pacer  and the hardware based DataCollider , CRSampler  is 
significantly much more effective and efficient . 
In the  rest of this paper , Section 2 gives  the background , followed 
by motivations in Section 3. Section 4 present s our clock races 
definition and  CRSampler . The evaluation is in Section 5. Section 
6 discusses related works and Section 7 concludes this paper . 2. BACKGROUND  
A multithreaded program  p consists of a set of threads ğ‘¡ïƒ T, a 
set of locks (or lock /synchronization  objects) ğ‘™ ïƒ L, and a set of 
memory locations ğ‘š ïƒ M. Each thread ğ‘¡ ïƒ T has a unique thread 
identifier  ğ‘¡ğ‘–ğ‘‘, denoted as ğ‘¡.ğ‘¡ğ‘–ğ‘‘.  
During an execution of a multithreaded program p, each thread ğ‘¡ 
performs a sequence of events , including :  
(1) Memory access  events : read or write  to a memory location 
ğ‘š, and  
(2) Synchronization events : acquire  or release  a lock ğ‘™. (Other 
synchronization e vents can be similarly defined [19].) 
We define a Non-Sync Block  (NSB  for short), during an execu-
tion, as a sequence of consecutive memory accesses between two 
synchronization events such that no other synchronization event 
exists between the two synchronization events.  
The Happens -before relation  (denoted as â†£, HBR for short ) is 
defined by the following three rules  [29]:  
(1) If two events  ï¡ and ï¢ are performed by the same thread, 
and ï¡ appear s before ï¢, then ï¡ â†£ ï¢.  
(2) If ï¡ is a lock release event and ï¢ is a lock acquire event on 
the same lock, and ï¡ appear s before ï¢, then ï¡ â†£ ï¢.  
(3) If ï¡ â†£ ï¢ and ï¢ â†£ ï§, then ï¡ â†£ ï§.  
HBR  is typically represented by vector clocks  [19]. A vector 
clock ğ¶ is an array of thread -local clocks. A clock is an integer, 
one for each thread ğ‘¡ (denoted as ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ ). During program exe-
cution,  one vector clock is allocated for each thread ğ‘¡, for each 
lock ğ‘™, and for each memory location ğ‘š, denoted as ğ¶ğ‘¡, ğ¶ğ‘™, and 
ğ¶ğ‘š, respectively. The i-th element in any vector clock ğ¶ (i.e., 
ğ¶[ğ‘–]) represent s the last known clock of the thread ğ‘¡ with ğ‘¡.ğ‘¡ğ‘–ğ‘‘=
ğ‘–. Specially, for a thread ğ‘¡, ğ¶ğ‘¡[ğ‘¡.ğ‘¡ğ‘–ğ‘‘] is equal to ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ . For a 
thread ğ‘¡, ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜  is only incremented right after  its value  is dis-
tributed to others (e.g., locks) on  synchronization events  [19]. 
Therefore, during an execution, we always have  [19] :  
ï‚· ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ =ğ¶ğ‘¡[ğ‘¡.ğ‘¡ğ‘–ğ‘‘], for any thread . 
ï‚· ğ¶ğ‘¡[ğ‘¡.ğ‘¡ğ‘–ğ‘‘]>ğ¶ğ‘ğ‘›ğ‘¦[ğ‘¡.ğ‘¡ğ‘–ğ‘‘], where ğ¶ğ‘ğ‘›ğ‘¦ is a vector clock of any 
thread  (but not thread ğ‘¡) or any lock , and  
ï‚· ğ¶ğ‘¡[ğ‘¡.ğ‘¡ğ‘–ğ‘‘]â‰¥ğ¶ğ‘š[ğ‘¡.ğ‘¡ğ‘–ğ‘‘], where ğ¶ğ‘š is a vector clock of any 
memory location  ğ‘š.  
3. MOTIVATIONS  
3.1 How to define and detect data races? 
A data race occurrence involves two accesses from different 
threads includ ing at least one write access [19]. However, there is 
no "gold standard" to define data races [17]. Existing technique s 
usually rely on either  the locking discipline [45] or the happens -
before relation [29]. The l ocking discipline defines  a data race  on 
a memory location, if all access es to the memory location is not 
protected by a common set of locks. Approaches based on l ocking 
discipline may report false positives as discussed  in Section 1.  
3.1.1 Happens -before based approaches  
The h appens -before relation (HBR) defines [19] a race on two  
memory accesses ğ‘’1 and ğ‘’2, with a write on the same memory 
location , if neither ğ‘’1 â†£ ğ‘’2 nor ğ‘’2 â†£ ğ‘’1. Such kind of races is 
known as HB Races [48].  
HBR requires ful l tracking of synchronizations from  all threads. 
Algorithm 1 shows a simplified basic HB race detector. The func-
tions onAcquire () and onRelea se() (lines 1 â€“11) track synchroniza-
tion events acquire () and release (), respectively, to maintain vec- 
 tor clocks for threads and locks. During tracking, the vector clocks 
of the involved threads and locks are firstly fetched ( i.e., ğ¶ğ‘¡ and ğ¶ğ‘™ 
lines 3 and 9, respectively ). Then an O(n) operation  (as noted)  is 
performed to update the vector clock of the thread ( i.e., ğ¶ğ‘¡â‰”ğ¶ğ‘™âŠ”
ğ¶ğ‘¡ at line 4) or the vector clock of the lock ( i.e., ğ¶ğ‘¡â‰”ğ¶ğ‘™âŠ”ğ¶ğ‘¡ at 
line 10). After that, the updated vector clock hold s the latest 
thread -local clocks from the two vector clocks. The functions 
onRead () and onWrite () check whether a data race occurs when a 
read or a write occurs. Still, the vector clocks of the thread and the 
memory location are firstly fetched (line 1 6). Then , the algorithm 
checks whether the last access to m (by thread lastThd) happens -
before the current access (line 1 8). If no happens -before relation 
exist s from the last access to the current one, a data race is report-
ed (if one of two accesses is a write , omitted in Algorithm 1). The 
check is also O(n) as noted .  
In Figure 1, we also show an example to illustrate the heavy track-
ing of HB based approache s. Figure 1(a) shows a Java program p. The program p contains a data race on x: accesses to x from two 
threads ğ‘¡1 and ğ‘¡2 (i.e., " x =â€¦" and " x +=â€¦") could occur concur-
rently. Figure 1(b) shows how each synchronization is instru-
mented to track HBR in Algorithm 1: for each synchronization, a 
call onAcquire (â€¦) or onRelease (â€¦) is inserted and the lock object  
(i.e., lock l or lock k) is taken as an argument. And on each read or 
write, an onRead () or a n onWrite () is also invoked.  
As a result , the tracking of HBR itself (i.e., without race detec-
tion) incurs high overhead. For example, Pacer  reports 15% over-
head [8], which already includes various optimizations. In our 
practice, we also experienced about 15% overhead on tra cking 
HBR. According to our experience, there are two factors contrib-
uting to the overhead: (1) the fetching of vector clocks of locks 
and threads (lines 3 and 9 in Algorithm 1), accounting for more 
than 5% overhead on average, and (2) operations on vector  clocks 
of locks and threads (lines 4 and 10 in Algorithm 1), contributing 
more than 8% overhead on average.   
Therefore, HBR via vector clocks is unlikely suitable for data race 
sampling as its tracking overhead  (without race detection)  is al-
ready larger than  5%, even with various optimizations [8]. Alt-
hough other existing works target  to track a subset of HBR [6][27] 
to reduce the tracking  overhead  to be low enough  (i.e., less than 
5%), their tracking  is only desig ned to check two known memory 
accesses in a production run  [6].  
3.1.2 Hardware based approaches  
Recently, DataCollider  defines a race on two accesses (with a 
write on the same memory location ), if they are executed at the 
same physical time. In this paper, w e call such races Collision 
Races, which are also HB race s [17].  
Algorit hm 2 shows the DataCollider  algorithm. Given a sampling 
rate r (and a time limit timeLimit  discussed in Section  4.2), 
DataCollider  randomly chooses a set of instructions to sample 
(lines 5 â€“9). For each sampled instruction ins, it delay s the execu-
tion of ins and, at the same time, waits for a second access  (line 
18). The trapping of a second access  is done by setting up a hard-
ware data breakpoint  on th e target address of ins (line 15 â€“16). If 
the data breakpoint fire s (lines 25â€“26), a data race occurs (lines 
21â€“22). The map â„³ at line 6  is used to check whether any data 
breakpoint fires on the address from the instruction of the delayed 
thread (lines 22 a nd 26).  
For example, Figure 1(c) shows how DataCollider  detects the race 
on x. Suppose that the write to x by thread ğ‘¡1 is sampled, then 
DataCollider  sets a data breakpoint on the address of x and then 
delays the write to x for a certain time. During the delayed period, 
if thread ğ‘¡2 reads  or writes to x, the data breakpoint fires. Then the Algorithm 1 : Simplified Basic HB Race Detector  
1.   
2.   
3.   
4.   
5.   
6.   
7.   
8.   
9.   
10.   
11.   
12.   
13.   
14.   
15.   
16.   
17.   
18.   
19.   
20.   
21.   onAcquire (Lock l ïƒ L) { 
   Let ğ‘¡ be the current thread;  
   Fetch vector clocks of ğ‘™ and ğ‘¡ as ğ¶ğ‘™ and ğ¶ğ‘¡, respectively.  
   ğ¶ğ‘¡â‰”ğ¶ğ‘™âŠ”ğ¶ğ‘¡;  //i.e., for each i, ğ¶ğ‘¡[ğ‘–]â‰”ğ‘šğ‘ğ‘¥  {ğ¶ğ‘™[ğ‘–],ğ¶ğ‘¡[ğ‘–]}       //O(n) 
} 
 
onRelease  (Lock l ïƒ L) { 
   Let ğ‘¡ be the current thread;  
   Fetch vector clocks of ğ‘™ and ğ‘¡ as ğ¶ğ‘™ and ğ¶ğ‘¡, respectively.  
   ğ¶ğ‘™â‰”ğ¶ğ‘™âŠ”ğ¶ğ‘¡;  ğ¶ğ‘¡[ğ‘¡.ğ‘¡ğ‘–ğ‘‘]âˆ¶=ğ¶ğ‘¡[ğ‘¡.ğ‘¡ğ‘–ğ‘‘]+1;       //O(n) 
} 
 
//simplified, do not distinguish read or write clock of m.  
onRead/onWrite(Memory location m ïƒ M) {  
   Let ğ‘¡ be the current thread;  
   Fetch vector clock of ğ‘¡ and ğ‘š as ğ¶ğ‘¡ and ğ¶ğ‘š, respectively . 
   ğ‘™ğ‘ğ‘ ğ‘¡ğ‘‡ â„ğ‘‘â‰” m.lastAccessedThread (); 
   if( not ğ¶ğ‘š[ğ‘™ğ‘ğ‘ ğ‘¡ğ‘‡ â„ğ‘‘.ğ‘¡ğ‘–ğ‘‘]â†£ğ¶ğ‘¡[ğ‘™ğ‘ğ‘ ğ‘¡ğ‘‡ â„ğ‘‘.ğ‘¡ğ‘–ğ‘‘])        //O(n) 
            //Note that some O(n) operations could be eliminated [19] 
      Report a race on m. 
} 
Algorithm 2: DataC ollider  
1.  
2.   
3.  
4.   
5.   
6.   
7.   
8.   
9.   
10.   
11.   
12.   
13.   
14.   
15.   
16.   
17.   
18.   
19.   
20.    
21.   
22.    
23.   
24.   
25.   
26.   Input : r â€“ a static sampling rate . 
Input : p â€“ a multithreaded program . 
Input : timeLimit  â€“ the max time on a data breakpoint.  
 
Let Î£ be a set of sampled instructions in p w.r.t r. 
Let â„³ be an empty  set. 
 
for each  ins ïƒ Î£          //static sampling  
   insertCall  (sample (ins)) ; 
 
sample  (ins)      //event e1 
{ 
   ïƒ¡addr , size, isWriteïƒ± âˆ¶= PARSE ( ins) ; 
 
   if ( isWrite )    setDataBreakpointRW  (addr , size) ; 
   else           setDataBreakpointW  (addr , size) ; 
 
   delay  (timeLimit );     //wait for an event e2 
   clearDataBreakpoint  (addr ) ; 
 
   if (ğ‘ğ‘‘ğ‘‘ğ‘Ÿ  âˆˆâ„³) reportDataRace  (ins) ; 
   â„³â‰”â„³\{addr }; 
} 
 
onDataBreakpointFired  (addr )     //event e2 
{  â„³â‰”â„³âˆª{addr };   } 
 
thread ğ‘¡1
acquire (l)
x= â€¦
â€¦
release (l)thread ğ‘¡2
acquire (k)
â€¦
release (k)
x+= â€¦thread ğ‘¡1
acquire (l)
onAcquire (l);
x= â€¦
onWrite (x);
â€¦
onRelease (l);
release (l)thread ğ‘¡2
acquire (k)
onAcquire (k);
â€¦
onRelease (k);
release (k)
tmp= xâ€¦
onRead (x);
x= tmp;
onWrite (x);(a)
(b)thread ğ‘¡1
acquire (l)
delay();
x= â€¦
â€¦
release (l)thread ğ‘¡2
acquire (k)
â€¦
release (k)
x+= â€¦
(c) 
Figure 1. (a): A program p with a race on x; (b): an instrumen-
tation on p (in bold ) by HB race detectors (e.g., Pacer ); (c): a 
delay inserted by DataCollider .   
 race on x is detected and DataCollider  resumes the execution o f 
thread ğ‘¡1.  
As the hardware breakpoint mechanism  (supported as debug utili-
zations on modern CPUs [17]) incurs almost no overhead, 
DataCollider  is able to limit a lmost all its overhead to be that 
caused by its delay . As the delay of DataCollider  directly contrib-
utes to its overhead, the  overhead  of DataCollider  could also be 
large . And with increasing sampling rate, it s overhead increases 
quickly (see our experiments in Section 5.3). Of course, DataCol-
lider could be configured to incur much lower overhead (e.g., 5%) 
by limiting i ts sampling rate to be much lower; but this also 
results in an extremely lower race detection rate and makes 
DataCollider  much more ineffective.   
Besides , the detection of  collision races  suffers the follow ing limi-
tations. First, if a collision race is detected, the race  has actually 
occurred . Therefore, once deployed at user sites, the occurrences 
of harmful collision races may cause unexpected results to users . 
Second , not all races could be easily detected in such a way 
[6][12], especially on large -scale programs (e.g., the e clipse pro-
gram in our experiment) . As a result , DataCollider  loses certain 
race detection ability compared to HBR based  race detectors .  
Theref ore, our first two insight s are, for a lightweight sampling 
technique:  
 
In Section 4, we present our definition o f data races with respect 
to the above two requirements.  
3.2 Dynamic Sampling vs. Static Sampling  
Dynamic sampling (as well as dynamic full race detection  [19]) 
requires fully instrumenting program instructions, even many 
instructions may not be  sampled during executions. For example, 
in Figure 1(b), every access to x incurs onRead(x) or onWrite(x) 
calls (of course, these calls could be inlined) ; but the  sampling 
decision is made  within these two functions. In other words, for 
dynamic sampling, to sample or not to sample an instruction is 
unknown until the instruction is about to be executed . And at th at 
time, the instrumented function calls have been executed . As a 
result, additional overhead is incurred due  to these per -instruction 
instrumentation s prior to the sampling decision making . That is 
part of reasons contributing to large overhead of Pacer  (i.e., 33% 
[8]) at 0% sampling rate.  
However , static sampling only requires to instrument exact in-
structions that are to be sampled, as adopted by DataCollider . That 
is, the sampling decision could  be made offline or during  program 
loading time ; and no additional overhead is introduced on those 
instruction s not to be sampled.  
Dynamic sampling incurs some basic overhead for all instructions 
(related to memory accesses  as well as potential HBR mainte-
nance ) while static sampling only incurs overhea d for those to -be-
sampled instructions. From this point, static sampling is more 
suitable for lightweight sampling techniques. Therefore, our third  
insight is that:  
 4. CRSAMPLER  
4.1 Clock Races  
As discussed in Section 3, the detection of HB race s requires in-
strumentation on synchronization oper ations. Such operations 
involve fetching vector clocks of locks and threads as well as O(n) 
operations on them . The resultant  overhead is already more than 
5% in previous experiment [8]. However , the definition of HB 
races is able to predict races that did not really occur  in the moni-
tored executions but may occur in other executions,  which is more 
powerful than DataCollider  that detects a race only when it really 
occurs.   
Therefore, we only consider a subset of HB R to define data races . 
Our definition of data races only relies  on thread -local clocks. 
Formally, we define our Clock Race s as follows:  
Definition 1 . Two memory ac cesses ğ‘’1 by thread ğ‘¡1 and ğ‘’2 by 
thread ğ‘¡2 form a Clock Race if: 
1) The two accesses ğ‘’1 and ğ‘’2 operate on the same memory lo-
cation and at least one of them is a write access, and  
2) At time that ğ‘’2 occurs, ğ‘¡1.ğ‘ğ‘™ğ‘œğ‘ğ‘˜  remains the same as that 
when ğ‘¡1 performs ğ‘’1. 
The first condition is the basic requirement of a race definition. 
The second one  is depicted in Figure 2. It requires that, between 
the occurrences of two accesses  ğ‘’1 and ğ‘’2 in physical time  (i.e., 
the period from time 1 to time 2 in Figure 2), the thread -local clock 
of ğ‘¡1 remains unchanged . That is, during the period, thread ğ‘¡1 
either (Case 1) does not execute any event  (including 
sleep ()/wait() event s that involve two incremen ts on a thread's 
local clock , resulting three non -sync blocks [19]) or (Case 2) only 
executes memory accesses in the same non-sync block. Therefore, 
DataCollider  only defines a subset of our clock races correspond-
ing to Case 1  where a thread ğ‘¡1 does not execute any event due to 
delay ing by DataCollider  (see Theorem 2  below ).  
We present two theorems to show that (1) c lock races are also HB 
races as the second condition is a subset of HBR  (i.e., the correct-
ness of clock races) , and (2) all races detected by  DataCollider  are 
also clock race s (corresponding to the above Case 1 ).  
Theorem 1. If two event s ğ‘’1 and ğ‘’2 form  a clock race, they  also 
form a n HB race.   
Proof Sketch.  (1) Firstly, it is impossible that ğ‘’2 â†£ ğ‘’1 as ğ‘’2 ap-
pears after ğ‘’1 occurs , which is implied by the second point of 
Definition 1 .  
(2) Suppose that ğ‘’1 â†£ ğ‘’2 is true . Let ğ¶ğ‘šğ‘’1 be the vector clock of 
m when ğ‘’1 occurs. When ğ‘’2 occurs , from ğ‘’1 â†£ ğ‘’2, we have:   
ğ¶ğ‘¡2[ğ‘¡1.ğ‘¡ğ‘–ğ‘‘]>ğ¶ğ‘šğ‘’1[ğ‘¡1.ğ‘¡ğ‘–ğ‘‘] EQ1 
(Otherwise, an HB race already occurs [8][19] and hence  ğ‘’1 â†£ 
ğ‘’2 does not hold). However, as ğ‘¡1.ğ‘ğ‘™ğ‘œğ‘ğ‘˜  is not changed since 
ğ‘’1 occurs , by definition of clock races  (in Section 2), we then 
have:  
ğ¶ğ‘šğ‘’1[ğ‘¡1.ğ‘¡ğ‘–ğ‘‘]=ğ¶ğ‘¡1[ğ‘¡1.ğ‘¡ğ‘–ğ‘‘]=ğ‘¡1.ğ‘ğ‘™ğ‘œğ‘ğ‘˜  EQ2 (1) It should not define a race according to  the full HBR  in-
volving vector clock  operations ;  
(2) It should not directly delay executing an instruction . 
(3) A  lightweight sampling technique should adopt static 
sampling strategy  to reduce its overhead  per execution . 
Thread ğ‘¡1
ğ‘’1Thread ğ‘¡2
ğ‘’2
ğ‘¡1.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ is not changed between time1and time2. time1
time2
Time elapse 
Figure 2. Illustration of clock races.   
 From  EQ1 and EQ2, we know that  thread  ğ‘¡1 does not own a 
large st clock of itself as ğ¶ğ‘¡2[ğ‘¡1.ğ‘¡ğ‘–ğ‘‘]>ğ‘¡1.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ , which con-
tradicts the fact that a thread always has the largest clock of it-
self than any other thread . Therefore, the assumption  ğ‘’1 â†£ ğ‘’2 
is not true. 
From (1) and (2), neither ğ‘’1 â†£ ğ‘’2 nor ğ‘’2 â†£ ğ‘’1 holds. As ğ‘’1 
and ğ‘’2 operate on the same memory locat ion and one of them 
is a write , by definition of the HB race, ğ‘’1 and ğ‘’2 form an HB 
race. Hence, a clock race is also a n HB race. ï€ ï¯ 
Theorem 2. If two event s ğ‘’1 and ğ‘’2 are detecte d by DataCollider  
as a collision race, they  also form a clock  race.  
Proof Sketch . Firstly, by definition of collision race, both events 
ğ‘’1 and ğ‘’2 operate on the same memory location and one of 
them is a write access. Secondly, s uppose that the event ğ‘’1 is 
the event delayed by DataCollider  (the other case can be 
proved similarly).  When the event ğ‘’2 occurs, the thread (say 
ğ‘¡1) performing ğ‘’1 does not execute any other event  as it is de-
layed;  therefore, the thread -local clock  of thread ğ‘¡1 is not 
changed . According to the definition of clock race, the two 
events ğ‘’1 and ğ‘’2 also form a clock race.  ï¯ 
Detection of clock races requires only thread -local clocks to iden-
tify NSB ( non-sync blocks ). Therefore, it is enough  to perform a 
light instrumentation . Figure 3 shows how the example  program is 
instrumented (i.e., " onSync ()"). Compared to instrumentation for 
HB races as shown in Figure 1(b), on synchronization event, (1) 
no concrete  lock is required  (i.e., " onSync ()" but not " onAc-
quire (l)" or " onRelease (l)"), and (2) hence  no vector clock of 
locks or threads is operated, avoiding O(n) operations . Therefore, 
a call " onSync ()" is enough  for each synchronization event , which 
only increment s thread -local  clock s of the involved thread s (see 
lines 4 1â€“45 in Algorithm 3 for details).  
Figure 3 also shows how a clock race on x is detected.  Let the 
initial clocks of two threads ğ‘¡1 and ğ‘¡2 be 10 and 8  (or any two 
other integers) , respectively. For both threads, on their acquisi-
tions of lock l and lock k, their clocks are incremented by 1 via 
calls onSync (). Then, t heir clocks become 11 and 9, respectively. 
Suppose that the write to x by thread ğ‘¡1 is sampled  (i.e., " sam-
ple(x)"). Next, suppose th at before thread ğ‘¡1 release s lock l, thread 
ğ‘¡2 releases lock k, followed by its read and writ e to x. At this time , 
the clock of thread ğ‘¡1 is still 11  which is the same as that when  its 
write to x is sampled. Therefore, a clock race on x is detected.   
Note that, if thread ğ‘¡1 first releases lock l before thread ğ‘¡2 reads 
from and write s to x, no clock race is then detected. It is because 
the clock of thread ğ‘¡1 is changed to 12 which  is different from that 
when the write to x by thread ğ‘¡1 is sampled. A lthough in both 
cases, full HBR based race detectors (e.g., FastTrack ) are able to 
detect the race on x, HBR based sampling detectors also suffer 
from the similar limitations. For example, Pacer  is able to detect 
the race on x only if  (1) the two accesses by two t hreads occur in 
the same sampling period or (2) one occurs in a sampling period 
and the second occurs in the right followed  non-sampling period. 
For other cases, Pacer  is unable to detect this race (e.g., both ac-
cesses occur in a non -sampling period or one acc ess occurs in a 
non-sampling period and the second one occurs in a sampling 
period). For DataCollider , it is able to detect the race by delaying 
the write to x by thread ğ‘¡1 until thread ğ‘¡2 reads  from and writes  to 
x (as shown in Figure 1(c)). However, as discussed in Section 3, 
such del ays directly increase its overhead. With increasing num-
ber of delays, its overhead increases significantly fast (see Section 
5.3.1 ).  Discussion on Definition of Clock Races.  In Definition 1, a clock 
race requires no change on ğ‘¡1.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ . Symmetrically, we could 
also define a clock race by extending Definition 1 (i.e., the second 
bullet): if at time when ğ‘’2 occurs, ğ‘¡2.ğ‘ğ‘™ğ‘œğ‘ğ‘˜  is not changed since 
the event ğ‘’1 occurs. This extension could increase the detection 
rate of clock races. How ever, it requires tracking of all clocks of 
any other threads by thread ğ‘¡1 when ğ‘’1 occurs. This tracking is 
O(n) in time and increases sampling overhead. In our preliminary 
experiment, this extension slightly increased race detection rates ; 
however, it d oubled sampling overhead , making detection of 
clock races inefficient . Therefore, we only follow Definition 1 to 
detect  races by sampling , avoiding incur ring any O(n) operations.   
4.2 Static and Hardware Based Sampling   
CRSampler  adopts static sampling strategy to sampl e each static 
instruction  based on our third  insight discussed in Section 3.2. 
When a sampled instruction is being executed, the access to the 
memo ry location (i.e., address) in this instruction is taken as a first 
event  ğ‘’1. CRSampler  further set s a data breakpoint on this 
memory location to trap an event ğ‘’2. If such an event ğ‘’2 occurs 
and the clock of the thread perform ing ğ‘’1 remains unchanged , a 
clock race is detected. Note that,  unlike DataCollider , during the 
trap of an event ğ‘’2, no thread is delayed .  
However, CRSampler  has to consider how much time is allowed 
to trap an event ğ‘’2. This is similar to DataCollider  that has to de-
termine how long it should delay the execution of a thread . It is 
because, a short time may not be enough for a race to occur. But a 
long time may make the usage of the data breakpoint s ineffective, 
as the number of data breakpoints is limit ed. For example, popular 
X86 CPU support s only four data breakpoint s and others may 
only support one [28].  
One strateg y is, like DataCollider , to set a constant time limit . 
Once such a time limit is reached, event ğ‘’1 is discarded. This 
strategy is simple  and does not incur additional overhead . The 
second one is to monitor the clock changes of all threads, on ce the 
thread  performing ğ‘’1 increments  its clock, event  ğ‘’1 is then dis-
carded  and the taken data breakpoint is cleared . The second strat-
egy seems more effective  than the first one.  However, it requires 
additional overhead  on synchronization events  (e.g., a check on 
whether an event ğ‘’1 is sampled from the exec ution of the current 
thread). Therefore, CRSampler  adopts the first strategy and sets a 
time limit ( which is the same as that by DataCollider ).  
4.3 CRSampler  Algorithm  
Algorithm 3 shows the CRSampler  algorithm. Given a sampling 
rate r and a program p, CRSampler  first selects a set of instruc-
tions Î£ randomly according to the given sampling rate r, and in-
struments these instructions (lines 8 and 9) to sample memory 
accesses at runtime. The input timeLimit  is the max time for a data 
breakpoint to be valid f or a given address.  CRSampler  maintains a 
thread -local clock for each thread ğ‘¡ as ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜  and a data structure 
â„³. (Note that, to simplify our presentation, we use the notation 
Thread ğ‘¡1
acquire (l) onSync ( );
x= â€¦      sample( x);
â€¦
onSync ( );
release (l)Thread ğ‘¡2
acquire (k)onSync ( );
â€¦ onSync ( );
release (k)
x += â€¦ğ‘¡1.ğ‘ğ‘™ğ‘œğ‘ğ‘˜
10
11
11
11
11
12ğ‘¡2.ğ‘ğ‘™ğ‘œğ‘ğ‘˜
8
9
9
10
10 
Figure 3. Instrumentation of CRSampler  (highlighted) and 
detection of a clock race on x with thread -local clocks.   
 â„³ğ‘¥ to denote the mapping  from ğ‘¥ to â„³(ğ‘¥).) The structure â„³ 
maps from one address to a pair of a thread and a clock, corre-
sponding to the thread ğ‘¡ performing event ğ‘’1 in definition of clock 
race and the clock of ğ‘¡ at that time.  Note that, (1) the map â„³ of 
Algorithm 3 is different from that in Algorithm 2; (2) as the 
number of data breakpoints is limited, operations over â„³ are 
actually O(1) operations in time (implemented via a global unique 
index for each data breakpoint).  
At runtime, once an instrumented instruction ins is being execut-
ed, the instrumented call sample (ins) fires . CRSampler  then sets a 
data breakpoint (lines 11 â€“20) to the memory address (i.e., addr ) 
that the instruction ins is about to access.  (We use a funct ion 
PARSE () to denote the extraction of the address  addr  and the size 
size in byte associated with the instruction ins, as well as isWrite  
indicating whether this instruction is a write one.)  There are two 
types of data breakpoints: read-write  data breakpo int and write  
data breakpoint. The former fires if either a read or a write to the 
target address occurs; the latter fires only if a write to the target 
address occurs. CRSampler  chooses either type of breakpoint ac-
cording to whether the sampled access is a write or a read (lines 
13â€“15). That is, a read access only forms a data race with a write access while a write access forms a data race with either a read or 
a write access. After setting the breakpoint, the thread id and the 
clock of the current thread is mapped in â„³ from the address addr  
(lines 17 and 18); and a time limit for this addr  is set (line 19).  
Once a data breakpoint on an address addr  fires (corresponding to 
event ğ‘’2 of the clock race definition), a clock race on addr  occurs 
if the current thread ğ‘¡ is different from the last thread lastThd and 
the current clock of thread lastThd remains the same as that 
mapped in â„³ (lines 27 â€“29). A data breakpoint is cleared if either 
a second event ğ‘’2 occurs (as stated in the last paragraph) (lines 
30â€“31) or a time limit is reached  (lines 3 5â€“39, i.e., the function 
onTimer (addr )).  
The function onSync  () is called whenever a synchronization event 
occurs to increment the thread -local clock of the corresponding 
thread (lines 4 1â€“45).  
Algorithm comparison . Compared to Pacer  that is based on HBR, 
CRSampler  does not maintain full HBR tracking but only a thread -
local clock on synchronization events (i.e., ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ âˆ¶=ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ +
1 instead of ğ¶ğ‘¡âˆ¶=ğ¶ğ‘™âŠ”ğ¶ğ‘¡ in Algorithm 1 ). Therefore, CRSampler  
invokes onSync () instrumentation call but not onAcquire (l) or 
onRelease (l). Thus, CRSampler  is able to avoid heavy tracking 
incurred by fetching lock objects and their vector clocks as well as 
the corresponding O(n) operations.   
Compared to DataCollider , CRSampler  does not rely on direct 
delay s to actually trigger  race occurrences . Instead, it checks races 
according to thread -local clocks if any data breakpoint fire s. Such  
race detection avoids direct delay -caused overhead. Besides, with 
a longer time limit , CRSamp ler does not incur a larger overhead; 
however, for DataCollider , its overhead is increased by the same 
fold of the increase in its time limit . This is also verified by our 
experiments (see Section 5.3.1 ). Of course, given the same sched-
uling and the same set of sampled memory accesses, CRSampler  
guarantees to detect all races detected by DataCollider , as each 
collision race is a clock race (see Theorem 2). In practic e, 
CRSampler  is able to detect more races as it does not delay any 
execution, resulting in more memory accesses sampled  (see our 
experiment in Section 5).   
4.4 Limitations  
Like existing sampling based techniques, CRSampler  also misses 
data races as expected. CRSampler  utilizes thread -local clocks to 
detect HB races. It then suffers limitations suffered by HB race 
detectors. One of such limitation s is the report of false positives : 
two accessed are reported as an HB race but they cannot occur at 
the same time. And data dependency is one factor. That is, two 
accesses form an HB race ; but the secon d access may depend on 
the value of the memory location of the first access. However, 
DataCollider  does not suffer from such a limitation as it only de-
tects those data races occurring at the same time.   
5. EXPERIMENTS  
This section presents the evaluation on CRSampler  (CR for short). 
We compared it with Pacer  and DataCollider  (DC for short) , and 
indirectly compare d it with LiteRace  [35] via Pacer  [8].  
5.1 Implementation and Benchmarks  
Implementation . We have implemented DC and CR in Jike s 
RVM [3][5], a widely used research JVM  [8][12]. Pacer  has also 
been  implemented in Jikes RVM and is available online  [2], we 
used its downloaded  implementation. The static sampling o f DC 
and CR is performed at Java class loading time .  Algorithm 3: CRSampler  
1.   
2.   
3.   
4.   
5.   
6.   
7.   
8.   
9.   
10.   
11.   
12.   
13.   
14.   
15.   
16.   
17.   
18.   
19.   
20.   
21.    
22.   
23.    
24.   
25.   
26.   
27.   
28.   
29.   
30.   
31.     
32.   
33.   
34.   
35.   
36.   
37.   
38.   
39.   
40.   
41.   
42.   
43.   
44.   
45.    Input : r â€“ a static sampling rate . 
Input : p â€“ a multithreaded program . 
Input : timeLimit  â€“ the max time on a data breakpoint.  
 
Let Î£ be a set of sampled instructions in p w.r.t r. 
Let â„³ be a map from an address to a pair ïƒ¡ğ‘¡, ğ‘ğ‘™ğ‘œğ‘ğ‘˜ïƒ±. 
 
for each  ins ïƒ Î£          //static sampling  
   insertCall  (sample (ins)) ; 
 
sample  (ins)      //event e1 
{ 
   ïƒ¡addr , size, isWriteïƒ± âˆ¶= PARSE ( ins) ; 
   if ( isWrite )  setDataBreakpointRW  (addr , size) ; 
   else              setDataBreakpointW  (addr , size) ; 
 
   Let ğ‘¡ be the current thread;  
   â„³â‰”â„³âˆª{ïƒ¡ğ‘ğ‘‘ğ‘‘ğ‘Ÿ ,ïƒ¡ğ‘¡,ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ïƒ±ïƒ±};                   
   setTimer  (addr , timeLimit ); //unlike DataCollider , no delay  
} 
 
onDataBreakpointFired  (addr )     //event e2 
{ 
   Let ğ‘¡ be the current thread;  
   ïƒ¡ ğ‘™ğ‘ğ‘ ğ‘¡ğ‘‡ â„ğ‘‘, ğ‘™ğ‘ğ‘ ğ‘¡ğ¶ğ‘™ğ‘œğ‘ğ‘˜  ïƒ± âˆ¶= â„³ğ‘ğ‘‘ğ‘‘ğ‘Ÿ ; 
   //check clock races  
   if (ğ‘¡â‰ ğ‘™ğ‘ğ‘ ğ‘¡ğ‘‡ â„ğ‘‘â‹€ğ‘™ğ‘ğ‘ ğ‘¡ğ¶ğ‘™ğ‘œğ‘ğ‘˜ =ğ‘™ğ‘ğ‘ ğ‘¡ğ‘‡ â„ğ‘‘.ğ‘ğ‘™ğ‘œğ‘ğ‘˜   ) 
   { 
       reportDataRace  (ins) ; 
       clearDataBreakpoint  (addr ) ; 
       â„³ğ‘ğ‘‘ğ‘‘ğ‘Ÿ âˆ¶= âˆ…; 
   } 
} 
 
onTimer (addr ) 
{ 
   clearDataBreakpoint  (addr ) ; 
   â„³ğ‘ğ‘‘ğ‘‘ğ‘Ÿ âˆ¶= âˆ…; 
} 
 
onSync  ()             //synchronization event s 
{ 
   Let ğ‘¡ be the current thread;  
   ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ âˆ¶=ğ‘¡.ğ‘ğ‘™ğ‘œğ‘ğ‘˜ +1; //unlike HBR, no O(n) operations  
} 
  
  
For DC and CR, the use of data breakpoint s is only allowed at  
Linux kernel . We implemented them for Java programs as depict-
ed in Figure 4: for each sampled access, the target address of the 
access is extracted within Jikes  RVM (i.e., Core of DC/C R). It is 
passed to a User -site Agent  (implemented in the interface exten-
sion of th e Jikes  RVM ) and is then sent to Linux kernel -site via 
Netlink communication (" Netlink Com. ") [1]. At Linux kernel -
site, a data breakpoint is set on the given address. Once a data 
breakpoint fires , a message is sent from the kernel -site to the user-
site agent, and is then sent to DC/CR. Both  DC and CR set at 
most four breakpoints at a time as our experiment environment 
supports four data breakpoints.   
Another challenge is that, u nlike C/C++, Java offers reference 
types (and primitive types) with automatic garbage collection  but 
not direct pointer types . Therefore, an add ress is not only accessed 
by application threads  (i.e., those created by Java applications) , 
but also accessed by Java VM threads (e.g., on objects moving 
and garbage collection). The latter kind of accesses does not incur 
data race s with accesses from application threads . However, it is 
impossible to identify and skip such accesses from VM threads at 
hardware  level . Therefore, once a data breakpoint fires, DC/CR 
extracts the native  thread id (i.e., Pthread on Linux) and checks whethe r such a thread is a Java application thread or a Java VM 
thread. It discards a ny access from Java VM threads.   
Benchmarks . We selected a set of six multithreaded benchmarks 
from Dacapo [9] benchmark suit e that could be run correctly by 
Jikes  RVM. These benchmarks include avrora 09, xalan 06, xa-
lan 09, sunflow 09, pmd 09, and eclipse 06, where the subscripts 
"06" and " 09" indicate their version number (i.e., "2006 " or the 
"2009 " version, respectively ). Table 1 shows  the statistics of these 
benchmarks, including the binary size , the number of threads , and 
the number of dynamically collected synchronizations for each 
benchmark . The last five columns show the number of data races 
detected by each technique. The last row also shows the total 
number of  data races detected by each technique.   
5.2 Experimental Setup  
Our experiment was performed on a workstation with an i7 -
4710MQ CPU (four cores), 16G memory, and 250G SSD. The 
workstation was installed with Ubuntu 14.04 x86 system.  
To evaluate CR, we run six benchmarks under Pacer , DC, and CR 
100 times for each samplin g rate  from 0.1 % to 1.0% with step  
0.1%. Note that , unlike DC and CR, Pacer  adopts dynamic sam-
pling strategy. We set three techniques to work at relatively low 
sampling rate s. This is because the three sampling techniques 
target to detect races at user sit es and their overhead should be 
low enough to be accepted by users. F or Pacer , its overhead at 0% 
sampling rate is already much higher  (i.e., 33% in [8]) than 5%.  
For DC, it incurred nearly 100% overhead at 1% sampling rates  in 
our experiment to be presented below .  
As DC and CR adopt time limit  mechanism, we selected  two time 
limit options : 15 ms (millisecond) and 30 ms. Note that the  first 
one is the default option of DC [17]. We refer to the two options  
of DC and CR as DC15 , DC30 , and CR15, CR30, respectively.  
5.3 Experimental Results  
We compare three techniques at their  runtime overhead s, total 
number s of races  detected, and per -race detection abilit ies.  
5.3.1 Overhead  
Figure 51 shows  the average overhead ( y-axis) of three techniques 
on all six benchmarks with i ncreasing sampling rate from 0. 1% to 
1.0% (x-axis) . Figure 6 (a) and (b)  shows the overhead of three  
techniques  on six benchmarks  in detail .  
                                                                 
1 Note, in both Figure 5 and Figure 6, we do not count and show the over-
head of Pacer  on sunflow 09 as on which, Pacer  incurred an overhead from 
600% to nearly 2,000% with sampling rate from 0.1% to 1.0%. No evalua-
tion of Pacer  on sunflow 09 was reported in [8]. 
JikesRVMUser -site 
AgentKernel
SiteCPU
Set breakpointsOn firing 
User space Kernel spaceNetlink
Com. Core of 
DC/C RExecution 
Figure 4. Architecture of CRSampler  and DataCollider .  
(a) Average overhead of three  techniques  (b) Average overhead of Pacer  and CRSampler  
Figure 5. Average runtime overhead.  
y = 9.0749x + 0.1474
RÂ²  = 0.9397y = 92.529x + 0.0588
RÂ²  = 0.9851y = 173.46x + 0.0997
RÂ²  = 0.9796
0%40%80%120%160%200%
0.1% 0.2% 0.3% 0.4% 0.5% 0.6% 0.7% 0.8% 0.9% 1.0%Avereage Overhead
Sampling rateAvg. Overhead of Three + TrendlinesPacer
DC15
DC30
CR15
CR30y = 9.0749x + 0.1474
RÂ²  = 0.9397
y = 2.6312x + 0.0252
RÂ²  = 0.9624
y = 2.0859x + 0.0269
RÂ²  = 0.9899 0%5%10%15%20%25%30%
0.1% 0.2% 0.3% 0.4% 0.5% 0.6% 0.7% 0.8% 0.9% 1.0%Avereage Overhead
Sampling rateAvg. Overhead of Pacer and CRSampler + TrendlinesPacer
CR15
CR30
Table 1. The statistics  of each benchmark and the number of 
total races detected by each technique.  
Bench - 
marks  Binary  
Size (KB)  # of  
threads  # of  
sync.  Pacer* 
DC15  
DC30  
CR15  
CR30  
avrora 09 2,086  7 3,312,801  3 3 3 5 3 
xalan 06 1,027  9 35,859,489  5 5 5 87 81 
xalan 09 4,827  9 12,599,144  0 2 2 84 91 
sunflow 09 1,017  17 1,590  0 0 2 46 45 
pmd 09 2,996  9 20,550  4 2 2 110 121 
eclipse 06 41,822  16 51,131,093  19 2 6 58 63 
Sum:  31 14 20 390 404 
*for Pacer  with 100% sampling rate, it was reported to have detected more 
races [8][18]. For example, on eclipse 06, Pacer  detected 39 races (out 
of 51 known races) [18] and 77 races [8]; and on xalan 06, Pacer  detected 
36 races (out of 41 known races) [18] and 73 races [8]. 
   
 Average overhead.  Figure 5(a) shows the average overhead of 
each technique . Figure 5(b) further shows the overhe ad compari-
son on CR and Pacer . The two figures also show the linear trend-
lines (with  both equations  ğ‘¦=ğ‘˜ğ‘¥+ğ‘ and goodness of fit R2) to 
indicate their trend s of overhead increase. Figure 5(b) also shows 
a dotted line indicating the  5% overhead.   
The trendline equations in Figure 5 (a) and (b) show that , statisti-
cally,  DC has the largest overhead increasing factors (i.e., 173.46  
of DC30  and 92.529  of DC15 ). Particularly, the increasing factors 
of DC30  is about two times of that of DC15 . This further validates 
the fact that, fo r DC, an increase  in its delay time also incurs the 
same fold increase in its overhead.  Although the increasing factor 
of Pacer  (i.e., 9.0749) is less than one tenth of DC15 , it is still sig-
nificantly more than 3 times larger than that of CR (i.e., 2.0859  
and 2.6312  for CR15 and CR30, respectively ). From the two fac-
tors of CR, we observe that, even with a longer time limit  (but at 
the same sampling rate), the overhead of CR does not increase but 
slightly dec rease s (see the second point of Section 5.4).  
Another insight from Figure 5 (a) and (b) is that , statistically , 
hardware based sa mpling has a s maller basic overhead (i.e., 
0.0588 , 0.0997 , 0.0252, and 0.0269 of DC15 , DC30 , CR15, and 
CR30) than that ( i.e., 0.1474  of Pacer ) by tracking HBR. And the 
basic overhead s of CR15 and CR30 are also the two least one s.  Overhead on each benchmark.  Figure 6(a) shows the detailed 
overhead of three techniques on each benchmark , where the leg-
end of the subfigure " avrora 09" applies to all other subfigures .  
From Figure 6(a), we observe that at 0.1% sampling rate, all three 
techniques incurred relatively small overhead. And most of their 
overheads are below 20%, where one exception is that DC15  and 
DC30  incurred more than 46.6% and 8 9.3% overhead on xalan 06. 
However, with i ncreasing sampling rate from 0.1% to 1.0 %, both 
DC15  and DC30  incurred significantly larger overhead. This is 
expected as the delayed time of DC is directly added into its 
overhead (as discussed in Section 3). Whereas , Pacer , CR15, and 
CR30 incurred  a slight overhead incre ase with increasing sampling 
rate except on eclipse 06 by Pacer . This slower increase shows 
the advantage of Pacer  and CR by defining data race based on 
synchronization  tracking (i.e., HB R tracking by Pacer  and thread -
local clock tracking by CR) instead of direct delaying .  
Although Pacer  has a linear  increase on its overhead with increas-
ing sampling rate, its basic overhead is much larger than  that of  
CR. Figure 6(b) (where the legend of " CR15" applies to all other 
subfigures ) further shows the comparison of Pacer , CR15, and 
CR30 on their overhead  with increasing sampling rate . From Fig-
ure 6(b), we observe that Pacer  usually incurred  from  15.5% to 
22.3%  overhead with 0. 1% sampling rate  except on avrora 09 
where the overhead was about 6.8%. However, the 15.5% to  
(a) Runtime overhead of three techniques  
 
(b) Runtime o verhead of Pacer  and CRSampler  
Figure 6. Runtime overhead of three techniques  on each benchmark . 
0%10%20%30%40%50%60%70%80%Overhead
Sampling rateavrora 09
Pacer
DC15
DC30
CR15
CR30
0%50%100%150%200%250%300%Overhead
Sampling ratexalan 06
0%50%100%150%200%250%300%Overhead
Sampling ratexalan 09
0%50%100%150%200%250%Overhead
Sampling ratesunflow 09
0%50%100%150%200%250%300%Overhead
Sampling ratepmd 09
0%5%10%15%20%25%30%35%40%45%50%Overhead
Sampling rateeclipse 06
0%5%10%Overhead
Sampling rateCR15
avrora 09 xalan 06
xalan 09 sunflow 09
pmd 09 eclipse 06
0%5%10%Overhead
Sampling rateCR30
0%10%20%30%40%50%Overhead
Sampling ratePacer 
 22.3% overhead of Pacer  is much larger than 5%.  With sampling 
rate increased fr om 0.1% to 1.0%, the overhead of Pacer  increased 
to more than 25% or even up to 41.4% (i.e., on eclipse 06 at 
1.0% sampling rate ). Compared to Pacer , with 0.1% sampling 
rate, both CR15 and CR30 incurred less than 5%  overhead (i.e., 
1.6%, 1.3%, 3.0%, 1. 4%, and 4.4% of CR15, 0.98%, 1.4%, 3.3%, 
2.5%, 4.7% and 4.9% by CR30) or slightly above 5% ( i.e., 5.12% 
on avrora 09 by CR15). With sampling rate increased from 0.1% 
to 0.9%, the largest overhead by CR15 and CR30 is 4.20% except 
on xalan 06 and avrora 09. On these two benchmarks, the largest 
overhead is 7.4%. At 1.0% sampling rate, the largest overhead is 
7.9% (i.e., on avrora 09 by CR15).  
In summary, from Figure 5 and Figure 6, we can see that CR in-
curred a much lower overhead than  that by  Pacer  and DC. At 
1.0%  sampling rate , CR incurred less than or slightly over 5% 
overhead on average, which makes CR practical to  be deployed at 
user sites.  
5.3.2 Total Races Detected  
Total races detected in 1 ,000 runs.  The last five  columns of Table 
1 show the total number of data r aces detected by Pacer , DC15 , 
DC30 , CR15, and CR30 in all their 1,000 runs (100 runs Ã— 10 sam-
pling rates). As shown in Table 1, CR detected significantly more 
races than that detected by both Pacer  and DC. And, DC detected 
more races on some benchmarks but fewer  races on other s than 
that by Pacer . Note that, on eclip se06 and xalan 06, Pacer  with 
100% sampling rate was reported  to have detect ed more races  [8] 
(see the note of Table 1). However, we focus on their race detec-
tion abilities and run time overhead at low sampling rate in this 
paper.   
Among all races detected by three techniques, both Pacer  and DC 
missed a larger number of races detected by CR as shown in Table 
1. However, CR only missed  four races detected by Pacer  and 
three races detected by DC. All these races were from eclipse 06. 
This is not a surprise as  eclipse 06 is a large -scale program (e.g., 
Table 1 shows that its binary size is nearly ten times larger than 
that of others ). Hence , it requires more runs to detect  more races 
from  eclipse 06.  
Distinct races per -100 runs with different sampling rates.  In our 
experiment, we run each technique 100 times under 10 sampling 
rates . Figure 7 shows t he number of distinct races detected in 
every 100 runs under  each of 10 sampling rates. The x-axis firstly shows the six benchmarks and then shows 10 sampling rates  from 
0.1% to 1.0%  for each benchmark. The y-axis shows the number 
of distinct races  detecte d. The legend of "Pacer " in Figure 7 also 
applies to other subfigures.   
From Figure 7, we observe that CR detected significantly more 
races than Pacer  and DC among most of 100 runs. This indicates 
that CR has a stronger  ability  to detect races in each run on aver-
age than Pacer  and DC.  
On eclipse 06 and xalan 06, Pacer  detected an increasing number 
of races with increasing sampling rate  (indicated by our manually 
added trend -like arrows for reference) ; and  on avrora 09, xalan 06, 
and pmd 09, one or both of DC15  and DC30  also detected an in-
creasing number of races. However, on other benchmarks, both 
Pacer  and DC detected almost the same number of races with 
increasing sampling rates. Whereas , CR detected an increasing 
number of races among all benchmarks except on avrora 09. On 
avrora 09, there are totally 5 races detected by CR, Pacer , and DC. 
On this benchmark, only CR15 detected an increasing number of 
races .   
5.4 More Discussion on DC and CR 
Both DC and CR adopt time limit  mechanism, although their 
usages are different. It is interesting that whether a longer time 
limit  is better. In our experiment, we set two time limit s: 15 ms 
and 30 ms. In this subsection, we analy ze how different time lim-
its affect (1) the ir race detection abilities and (2) their runtime 
overhead s.  
Race detection under  different time limit . In theory, w ith a longer 
time limit , there are two effects on both DC and CR: (1) the total 
sampled accesses were reduced due to limited number of data 
breakpoints, which may result in fewer  races to be detected; (2) on 
the other hand , with a longer time limit , DC and CR could detect 
those races needing a longer time to expose.  
As shown in Table 1, when  the time limit  was 30 ms, DC detected 
more races on sunflow 09 and eclipse 06 by 2 and 4 , respectively . 
On other benchmarks, DC detected the same number of races. 
However, for CR, there i s no consistent result . On avrora 09, xa-
lan 06, sunflow 09, CR with 30 ms detected less races by 2, 6, and 
1, respectively ; on xalan 09, pmd 09, and eclipse 06, CR with 30 
ms detected more races by 7, 11, and 5 , respectively . Therefore, it 
is difficult to say that a longer time limit  may result  in a better  
race detection , even for DC. Actually, previous work has pointed  
Figure 7. Comparisons on the number of distinct races detected in every 100 runs under different sampling rate s. 
05101520
avrora09 xalan06 xalan09 sunflow09 pmd09 eclipse06# of racesPacer
0.1% 0.2% 0.3% 0.4% 0.5%
0.6% 0.7% 0.8% 0.9% 1.0%
012345
avrora09 xalan06 xalan09 sunflow09 pmd09 eclipse06# of racesDC15
012345
avrora09 xalan06 xalan09 sunflow09 pmd09 eclipse06# of racesDC300102030405060
avrora09 xalan06 xalan09 sunflow09 pmd09 eclipse06# of racesCR15
0102030405060
avrora09 xalan06 xalan09 sunflow09 pmd09 eclipse06# of racesCR30 
 out that it is impossible to predict the time limit  of DC [17]; and, 
timing operations (e.g., instrumentation calls) may increase  the 
probability of a race occurrence but may also decrease  it [8].  
Overhead under  different time limit . Although there is no conclu-
sion on whether a longer time limit  may produce better race detec-
tion, it s effect on overhead is much clea rer. From Figure 5, it is 
obvious that, DC with a longer time limit  incurred a larger over-
head . In our experiment, the first time limit  is 15 ms and the sec-
ond one is 30 ms  (which is twofold of the first one ). From the 
equations of trendlines in Figure 5, we could observe that , for DC, 
the overhead increas ing factor (i.e., 173.46 ) at time limit  of 30 ms 
is nearly  two times of the former (i.e.,  92.529 ).  
However, from Figure 5 for CR, there is no o bvious difference 
between the two time limit s as the two trendlines almost overlap 
with increasing sampling rate. In detail, CR30 incurred a slightly 
less overhead. This is reasonable as with a longer time limit , (1) 
the increased time is not added into  the overhead of  CR; (2) how-
ever, the total number of sampled accesses could be reduced, re-
sulting in less maintenance overhead.  That is, an increasing time 
limit  has no bad effect on  the overhead of CR. This further pro-
vides flexibility for developers to set a time limit  according to 
their programs  without any worry on overhead increase .  
5.5 Threats to Validity  
Our benchmarks are Java pr ograms. The JVM contains other 
threads accessing application memory locations. Although we 
have carefully compared whether two accesses were both from 
application threads, some scenarios might be ignored in our im-
plementation. A more careful implementation  may produce more 
precise results. Besides, hardware breakpoints can only be ac-
cessed within (Linux) kernel space. So we adopt ed the Netlink 
communication approach between user space and kernel space. 
Other communication approaches may produce different p erfor-
mance results that may also affect the effectiveness of CRSampler  
and DataCollider  in the evaluation.    
6. RELATED WORK  
Concurrency bugs widely exist in multithreaded programs, includ-
ing data races [12][19], atomicity violations [32][49], and dead-
locks [15]. Both  static techniques [26][37][41][51] and dynamic 
techniques [19][40][45][48][54] aim to detect data races . Static 
ones [51][41] are able to analyze a whole program but are impre-
cise due to lack of runtime information . Dynamic ones detect data 
races from  execution traces. They either rely on  the strict locking 
discipline  (i.e., lockset)  [45][47][57] or the relatively precise hap-
pens-before relation [19][40] (including its improvement [7][44] 
[50][52]). However, dynamic detection usually incurs heavy 
overhead  [13][14][19]. Existing s ampling techniques aim to detect 
races at user site s by incurring much low er overhead, w hich is the 
focus of this paper.  
Systematic scheduling techniques such as model checking [53] 
[36], are in theory able to exhaustively execute every schedule  to 
achieve certain coverage [30]. However, due to the state explosion 
problem, enumerating  each schedule is not practical for real -world 
programs , even with reduction techniques [20]. Chess [36] sets a 
heuristic bound on the number of pre-emption s to explore the 
schedules. Also,  although systema tic approaches avoid executing  
previously explored schedules, they usually incur large overhead s 
and fail to scale up to handle long running programs. For exam-
ple, Maple  [55] is a coverage -driven [10][21] tool to mine thread 
interleaving so as to expose unknown concurrency bugs.  PCT  
[11][38] randomly schedules a program to expose concurrency bugs, which also requires large number of executions. However, it 
is diffi cult to apply these techniques to  large -scale prog rams (e.g., 
eclipse in our experiment).  
RVPredict  [24] achieve s a strictly higher coverage than HBR 
based  detectors . It firstly predicts a set of potential races and then 
relies on a number of production executions to check against each 
predicted race. Racageddon  [18] aims to s olve races that could be 
predicted in one execution but require different inputs. It still 
needs a larger number of execution s to ch eck against each pre-
dicted race  [42][46]. Both RVPredict  and Racageddon  have to 
solve scheduling constraints for each predicted race, which may 
fail. A recent work  DrFinder  [12] tries to expose races hidden by 
the happens -before relation. It dynamically predicts and tries to 
reverse happens -before relations from  observed executions. How-
ever, it s active scheduling is also heavy (e.g., about 400% [12]).  
RaceMob  [27] statically detect s data race warnings and distribut es 
them to a large number of user s to validate real races. In such a  
run, the schedules are guided by the set of data race warnings to 
trigger real data races. This kind of approach is able to confirm 
real races but cannot eliminate false positives.  Besides, it may 
miss real races if such races are not predicted in the ( static ) 
prediction phase.  CCI [25] proposes cross -thread sampling 
strategies to find causes of concurrency bugs  based on 
randomized sampling . Unlike race sampling techniques (e.g., 
CRSampler , DataCollider , Pacer , and LiteRace ), CCI focuses on 
failure diagnosis. However, CCI may cause heavy over head  (e.g., 
up to 900% [25]) although it targets on lightweight sampling . 
Carisma  [59] improves Pacer  by further sampling memory 
locations allocated at the same program locati on for Java program. 
Carisma  could be integrated into CRSampler  to improve  its 
effectiveness.   
ReCBuLC  [58] also adopts thread -local clocks (time stamps) to 
reproduce concurrency bugs. Unlike CRSampler , ReCBuLC  
requires concrete objects and may still incur large overhead if 
applied to race sampling .  
Recently, race detection ha s been extended to even -driven 
applications [34][23][22], concurrent librar y invocations  [16], and 
modified program codes  [56]. CRSampler  could also be adapted to 
detect these races. We leave it as future work.  
7. CONCLUSION  
Existing sampling techniques for race detection still incur high 
overhead, even with 0% sampling rates, and/or detect races only 
when they occur by delay ing program executions. We have pro-
posed a new data race definition (i.e., clock races) for race sam-
pling purpose . Detection of clock races avoid s O(n) operations 
and concrete synchronization objects, which hence incurs  a much 
lower overhead. We also propose d CRSampler  to sample  clock 
races via hardware support. The experiment on six benchmarks 
confirms that CRSampler  is both efficient and effective on race 
detection via sampling.  At 1% sampling rate, it only incurs nearly 
5% overhead, indicating that CRSample r is suitable to be deployed 
at user site  for race detections .  
8. ACKNOWLEDGEMENT  
We thank anonymous reviewers for their invaluable comments 
and suggestions on improving this work. This work is supported 
in part by National 973 program of China (2014CB340702 ), and 
National Natural Science Foundation of China (NSFC) (grant No. 
61502465, 91418206, and 61572481 ).   
 9. REFERENCE  
[1] Netlink communication between Linux user space and kernel 
space. http://man7.org/linux/man -pages/man7/netlink.7.html  
[2] Jikes Research Archive.  
http://www.jikesrvm.org/Resources/ResearchArchive  
[3] Jikes RVM 3.1.3. http://jikesrvm.org  
[4] J. Jackson. Nasdaq's Facebook glitch came from 'race condi-
tions', May 21 2012. 
http://www.computerworld.com/article/2504676/financial -it/nasdaq -s-
facebook -glitch -came -from --race-conditions -.html, last visited on March  
2016. 
[5] B. Alpern, C.R. Attanasio, A. Cocchi, D. Lieber, S. Smith, T. 
Ngo, J.J. Barton, S.F. Hummel, J.C. Sheperd, and M. Mer-
gen. Implementing jalapeÃ± o in Java. In Proc.  OOPSLA , 314 â€“
324, 1999.  
[6] S. Biswas, M. Zhang, and M.D. Bond. Lightweight data race 
detection for production runs. Ohio State CSE Technical Re-
port #OSU -CISRC -1/15-TR01 , January 2015. 23 pages, 
available at : http://web.cse.ohio -state.edu/~mikebond/litecollider -tr.pdf 
[7] E. Bodden and K. Havelund. Racer: effective race detection 
using Aspect J. In Proc . ISSTA , 155 â€“166, 2008.  
[8] M.D. Bond, K. E. Coons and K. S. Mckinley. PACER: Pro-
portional detection of data races. In Proc. PLDI , 255 â€“268, 
2010.  
[9] S.M. Blackburn, R. Garner, C. Hoffmann, A.M. Khang, K.S. 
McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, 
S.Z. Guyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, J. Eliot 
B. Moss, A. Phansalkar, D. StefanoviÄ‡, T. VanDrunen , D. 
von Dincklage, and B. Wiedermann. The Dacapo bench-
marks: Java benchmarking development and analysis. In 
Proc . OOPSLA , 169 â€“190, 2006.  
[10] A. Bron, E. Farchi, Y. Magid, Y. Nir, and S. Ur. Applica-
tions of synchronization coverage. In Proc . PPoPP , 206 â€“
212, 20 05. 
[11] S. Burckhardt, P. Kothari, M. Musuvathi, and S. Nagarakatte. 
A randomized scheduler with probabilistic guarantees of 
finding bugs. In Proc . ASPLOS , 167 â€“178, 2010.  
[12] Y. Cai and L. Cao. Effective and precise dynamic detection 
of hidden races for Java programs. In Proc. ESEC/FSE , 450â€“
461, 2015.   
[13] Y. Cai and W.K. Chan. LOFT: Redundant synchronization 
event removal for data race Detection. In Proc. ISSRE , 160 â€“
169, 2011 . 
[14] Y. Cai and W.K. Chan. Lock trace reduction for multithread-
ed programs. IEEE Transactions on Parallel and Distributed 
Systems  (TPDS ), 24(12): 2407âˆ’2417, 2013.  
[15] Y. Cai and W.K. Chan. Magiclock: scalable detection of 
potential deadlocks in large -scale multithreaded programs. 
IEEE Transactions on Software Engineering  (TSE), 40(3), 
266â€“281, 2014.  
[16] D. Di mitro, V. Raychev, M. Vechev, and E. Koskinen. 
Commutativity race detection. In Proc. PLDI, 305 â€“315, 
2014 . 
[17] J. Erickson, M. Musuvathi, S. Burckhardt and K. Olynyk. 
Effective data -race detection for the kernel. In Proc. OSDI , 
1â€“6, 2010.  [18] M. Eslamimehr and J. Palsberg. Race directed scheduling of 
concurrent programs. In Proc . PPoPP , 301 â€“314, 2014.  
[19] C. Flanagan and S. N. Freund. FastTrack: efficient and pre-
cise dynamic race detection. In Proc . PLDI , 121 â€“133, 2009.  
[20] C. Flanagan and P. Godefroid . Dynamic partial -order reduc-
tion for model checking software. In Proc. POPL , 110 â€“121, 
2005.  
[21] S. Hong, J. Ahn, S. Park, M. Kim, and M.J. Harrold. Testing 
concurrent programs to achieve high synchronization cover-
age. In Proc . ISSTA , 210 â€“220, 2012.  
[22] S. Hong, Y. Park, and M. Kim. Detecting concurrency errors 
in client -side java script web applications . In Proc . ICST , 61â€“
70, 2014.  
[23] C. Hsiao, Y. Yu, S. Narayanasamy, Z. Kong, C.L. Pereira, 
G.A. Pokam, P.M. Chen, and J. Flinn. Race detection for 
event -drive n mobile applications. In Proc. PLDI, 326 â€“336, 
2014.  
[24] J. Huang, P.O. Meredith, and G. Rosu. Maximal sound pre-
dictive race detection with control flow abstraction. In Proc . 
PLDI , 337 â€“348, 2014.   
[25] G. Jin, A. Thakur, B. Liblit and S. Lu. Instrumentation and 
sampling strategies for cooperative concurrency bug isola-
tion. In Proc . OOPSLA , 241 â€“225, 2010.  
[26] V. Kahlon, Y. Yang, S. Sankaranarayanan, and A. Gupta. 
Fast and accurate static data -race detection for concurrent 
programs. In Proc.  CAV , 226 â€“239, 2007.  
[27] B. Kasikci , C. Zamfir, and G. Candea. RaceMob: 
Crowdsourced data race detection. In Proc . SOSP , 406 â€“422, 
2013.  
[28] P. Krishnan. Hardware Breakpoint (or watchpoint) usage in 
Linux Kernel. IBM Linux Technology Center, Canada, July 
2009.  
[29] L. Lamport . Time, clocks, and the ordering of events. Com-
munications of the ACM , 21(7):558 â€“565, 1978.  
[30] Z. Letko, T. Vojnar, and B. KË‡rena. Coverage metrics for 
saturation -based and search -based testing of concurrent soft-
ware. In Proc. RV , 177 â€“192, 2011.  
[31] N.G. Leveson and C. S. Turner. An investigation of the 
Therac -25 accidents. Computer , 26(7), 18 â€“41, 1993.  
[32] S. Lu, S. Park, E. Seo, and Y.Y. Zhou, Learning from mis-
takes: A comprehensive study on real world concurrency bug 
characteristics. In Proc . ASPLOS , 329 â€“339, 2008.  
[33] B. Lucia and L. Ceze. Cooperative empirical failure avoid-
ance for multithreaded programs. In Proc . ASPLOS , 39â€“50. 
2013.  
[34] P. Maiya, a. Kanade, and R. Majumdar. Race detection for 
Android applications. In Proc . PLDI , 316 â€“325, 2014.  
[35] D. Marino, M. Musuvathi, a nd S. Narayanasamy. LiteRace: 
effective sampling for lightweight data -race detection. In 
Proc . PLDI , 134 â€“143, 2009.  
[36] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, 
and I. Neamtiu. Finding and reproducing heisenbugs in con-
current programs. In Proc. OSDI , 267 â€“280 2008.  
[37] M. Naik, A. Aiken, and J. Whaley. Effective static race de-
tection for Java. In Proc. PLDI , 308 â€“319, 2006.   
 [38] S. Nagarakatte, S. Burckhardt, M. M.K. Martin, and M. 
Musuvathi. Multicore acceleration of priority -based sched-
ulers for concur rency bug detection. In Proc . PLDI , 2012, 
543â€“554, 2012.  
[39] S. Narayanasamy, Z. Wang, J. Tigani, A. Edwards, and B. 
Calder. Automatically classifying benign and harmful data 
races using replay analysis. In Proc . PLDI , 22â€“31, 2007.  
[40] E. Pozniansky  and A. Schuster. Efficient on -the-fly data race 
detection in multithreaded C++ programs. In Proc . PPoPP , 
179â€“190, 2003.  
[41] P. Pratikakis, J.S. Foster, and M. Hicks. LOCKSMITH: con-
text-sensitive correlation analysis for race detection. In Proc. 
PLDI , 320 â€“331, 2006.   
[42] C.S. Park, K. Sen, P. Hargrove, and C. Iancu. Efficient data 
race detection for distributed memory parallel programs. In 
Proc. SC, 2011.  
[43] K. Poulsen. Software bug contributed to blackout. 
http://www.securityfocus.com/news/8016, Feb. 2004 . 
[44] A.K. Rajag opalan and J. Huang. RDIT: race detection from 
incomplete traces. In Proc . ESEC /FSE, 914 - 917, 2015.  
[45] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro and T. 
Anderson. Eraser: a dynamic data race detector for multi-
threaded programs. ACM TOCS , 15(4), 391 â€“411, 1997.  
[46] K. Sen. Race Directed Random Testing of Concurrent Pro-
grams. In Proc . PLDI , 11â€“21, 2008.  
[47] K. Serebryany and T. Iskhodzhanov. ThreadSanitizer: data 
race detection in practice. In Proc. WBIA , 62â€“71, 2009.   
[48] Y. Smaragdakis, J. Evans, C. Sadowski , J. Yi, and C. Flana-
gan. Sound predictive race detection in polynomial time. In 
Proc . POPL , 387 â€“400, 2012.  
[49] F. Sorrentino, A. Farzan, and P. Madhusudan. PENELOPE: 
weaving threads to expose atomicity violations. In Proc . 
FSE, 37â€“46, 2010.  [50] K. Vineet and C. Wang. Universal causality graphs: a precise 
happens -before model for detecting bugs in concurrent pro-
grams. In Proc . CAV , 434 â€“449, 2010.  
[51] J.W. Voung, R. Jhala, and S. Lerner. RELAY: static race 
detection on millions of lines of code. In Proc. FSE, 205 â€“
214, 2007.  
[52] C. Wang, K. Hoang. Precisely deciding control state reacha-
bility in concurrent traces with limited observabil ity. In Proc . 
VMCAI , 376 â€“394, 2014.  
[53] C. Wang, M. Said, and A. Gupta. Coverage guided systemat-
ic concurrency testing. In Proc.  ICSE , 221 â€“230, 2 011. 
[54] X.W. Xie and J.L. Xue. Acculock: Accurate and Efficient 
detection of data races. In Proc . CGO , 201 â€“212, 2011.  
[55] J. Yu, S. Narayanasamy, C. Pereira, and G. Pokam. Maple: a 
coverage -driven testing tool for multithreaded programs. In 
Proc . OOPSLA , 485 â€“502, 2012.  
[56] T. Yu, W. Srisa -an, and G. Rothermel. SimRT: An automat-
ed framework to support regression testing for data races. In 
Proc . ICSE , 48â€“59, 2014.  
[57] Y. Yu, T. Rodeheffer, and W. Chen. RaceTrack: efficient 
detection of data race conditions via adaptive trac king. In 
Proc . SOSP , 221 â€“234, 2005.  
[58] X. Yuan, C. Wu, Z. Wang, J. Li, P.C. Yew, J. Huang, X. 
Feng, Y. Lan, Y. Chen, and Y. Guan. ReCBuLC: reproduc-
ing concurrency bugs using local clocks. In Proc . ICSE , 824 â€“
834, 2015.  
[59] K. Zhai, B.N. Xu, W.K. Chan, and T.H. Tse . CARISMA: a 
context -sensitive  approach to race -condition sample -instance 
selection for multithreaded applications. In Proc . ISSTA , 
221â€“231, 2012.  
[60] W. Zhang, M. d. Kruijf, A. Li, S. Lu and K. Sankaralingam. 
ConAir : featherweight concurrency bug recovery via single -
threaded idempotent execution. In Proc . ASPLOS , 113 â€“126. 
2013.  
 
 
 