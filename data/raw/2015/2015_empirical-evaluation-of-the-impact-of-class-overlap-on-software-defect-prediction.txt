Empirical Evaluation of the Impact of Class
Overlap on Software Defect Prediction
Lina Gong
School of Computer Science and Technology,
China University of Mining and Technology,
Xuzhou 221116, China
Mine Digitization Engineering Research Center of
Ministry of Education, Xuzhou 221116, China
Department of Information Science and Engineering,
Zaozhuang University, Zaozhuang 277160, China
Email: linagong@cumt.edu.cnShujuan Jiang, Rongcun Wang and Li Jiang
School of Computer Science and Technology,
China University of Mining and Technology,
Xuzhou 221116, China
Mine Digitization Engineering Research Center of
Ministry of Education, Xuzhou 221116, China
Email: shjjiang@cumt.edu.cn
Email: rcwang@cumt.edu.cn
Email: lijiang@cumt.edu.cn
Abstract ‚ÄîSoftware defect prediction (SDP) utilizes the learn-
ing models to detect the defective modules in project, andtheir performance depends on the quality of training data. The
previous researches mainly focus on the quality problems of class
imbalance and feature redundancy. However, training data oftencontains some instances that belong to different class but have
similar values on features, and this leads to class overlap to
affect the quality of training data. Our goal is to investigatethe impact of class overlap on software defect prediction. At
the same time, we propose an improved K-Means clustering
cleaning approach (IKMCCA) to solve both the class overlap
and class imbalance problems. SpeciÔ¨Åcally, we check whether K-
Means clustering cleaning approach (KMCCA) or neighborhoodcleaning learning (NCL) or IKMCCA is feasible to improve defect
detection performance for two cases (i) within-project defect
prediction (WPDP) (ii) cross-project defect prediction (CPDP).
To have an objective estimate of class overlap, we carry out
our investigations on 28 open source projects, and compare the
performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS.
without cleaning data. The experimental results make clear that
learning models obtain signiÔ¨Åcantly better performance in termsof balance ,Recall and AUC for both WPDP and CPDP when
the overlapping instances are removed. Moreover, it is better to
consider both class overlap and class imbalance.
Index T erms ‚ÄîClass overlap, Software defect prediction, K-
Means clustering, Machine learning
I. I NTRODUCTION
Defect prediction technology is one of the research topics
among academic and industrial organizations [1], [2], [3], [4],
[5], [6], [7], [8], [9], [10]. With the development of machine
learning methods, more and more classiÔ¨Åcation models areapplied to software defect prediction (SDP) to detect as manydefective modules as possible with minimal cost. The qualityof training dataset from software projects seriously affects theprediction capabilities of learning models. Previous studies inthe data quality of SDP mainly focus on class imbalance andfeature redundancy, and have proposed lots of models to dealwith these quality problems.
However, in practical data collection, due to the objective
condition, some instances with different classes may haveFig. 1. The distribution of PC4 dataset in NASA. Figure 1 denotes the PC4 has
high overlapping instances, these overlapping areas are diffcult to classiÔ¨Åer.
similar values on some features, which results in overlap in
feature space. These instances are called overlapping instances,triggering the class overlap problem. Class overlap is one
of the bottlenecks in data mining and machine learning, and
SDP is no exception. Many instances from NASA [11], [12],
AEEEM [13], ReLink [14], SOFTLAB[11] and MORPH [15]
are overlapped (As shown in Ô¨Ågure 1, many instances arelocated in overlapping space, where learning models oftenfail to classify), which hinders the prediction performance oflearning models.
In the past few years, many studies have encountered class
overlap problem in practical Ô¨Åelds, such as credit card fraud
detection [16], text classiÔ¨Åcation [17]. They found the over-
lapping instances involve the performance of learning models,
which often becomes a thorny problem when combined with
other factors, such as class imbalance. In SDP, researchersmainly focus on data preprocessing including class imbalance
and noise cleaning, few studies have paid attention on classoverlap problem. Chen et al. [18] attempted to apply the neigh-borhood cleaning learning (NCL) rule to remove overlappinginstances for SDP. However, the NCL method only removed
the conÔ¨Çicting non-defective instances to eliminate the classoverlap, and the defective instances located in non-defective
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXB SF&OHJOFFSJOH	"4&
¬•*&&&
%0*"4&
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. overlapping space were not considered.
In this study, we are committed to identify and process
overlapping instances and propose an improved K-Meansclustering cleaning approach (IKMCCA) to remove the over-lapping instances. Our contribution is to investigate the impactof class overlap on software defect prediction, and excavate
how class overlap inÔ¨Çuence the prediction performance for
both WPDP and CPDP. So we have structured our workaccording to the following two research questions:
RQ1: How does class overlap inÔ¨Çuence the prediction
performance of within-project defect prediction approaches?
RQ2: How does class overlap inÔ¨Çuence the prediction
performance of cross-project defect prediction approaches?
We conduct experiments to investigate the impact of class
overlap for WPDP and CPDP using 28 overlapping data fromNASA, AEEEM, ReLink, SOFTLAB and MORPH. The restof this study is organized as follows: the next section IIprovides a review of related work on SDP. Section III provides
the methodology containing study questions, methods to build
models, data and model evaluation. Section IV lists the resultsof study. The discussions of our work are described in Section
V and the threats to validity of our work are analyzed in
Section VI. We conclude the paper in Section VII.
II. R
ELATED WORK
In this section, we mainly discuss the related researches on
the SDP and class overlap.
A. Software Defect Prediction
Software defect prediction technology has been one of
the most concerned research topics in software engineeringsince 1970s [6], [19], [20]. In recent years, with the rapid
development of machine learning, various machine learning
methods have been widely applied to improve the performanceof SDP.
According to whether the training and testing data come
from the same project, the SDP is divided into within-projectdefect prediction (WPDP) and cross-project defect prediction(CPDP). WPDP is to directly apply classiÔ¨Åers such as NaiveBayes (NB) [7], [10], Random forests (RF) [21], K-nearestneighbor (KNN) [22], Support vector machine (SVM) [23],
and Logistic regression (LR) [24] to detect the defective
modules.
CPDP is to Ô¨Ånd as many defective modules as possible
in one project based on the learning model trained by otherprojects. The most important problem of CPDP is the different
distribution of training and testing data. Many researchers have
presented a comparative research on CPDP methods [25], [26],[27]. Zhou et al. [27] found that the simple module-size model
method had comparable or better predictive performance thanmost cross-project defect prediction methods. In our work, we
focus our discussions on six empirical studies which could
achieve better performance. Detailed descriptions are shown
as follows.
Turhan et al. [28] chose similar instances as training set
from different projects, and using these similar instances totrain k-nearest neighbor (KNN) classiÔ¨Åer. Nam et al. [6]
used Transfer Component Analysis (TCA) method to maptraining and testing data into the common feature space.They conducted experiments on ReLink and AEEEM datasets,and proposed TCA+ method for automatic data selectionstandardization. Chen et al. [29] integrated two levels of
data transfer and proposed double transfer boosting (DTB)approach. First, they reshaped the whole distribution of cross-company (CC) dataset by data gravitation to Ô¨Åt WP data. Then,
they eliminated negative instances in CC dataset by transferboosting with NB based classiÔ¨Åer. Turhan et al. [30] foundmixed-project prediction was reasonable in early phases of de-velopment though studying the impact of mixed-project dataseton binary defect prediction. Ryu et al. [31] discussed the classimbalance for the mixed-project defect prediction (MPDP),
and put forward the value-cognitive boosting with support vec-
tor machine (VCB-SVM) approach. Their experimental resultsindicated that VCB-SVM could achieve better results when
training data only contained fewer label instances of testing
project. Xia et al. [32] proposed the HYDRA (HYbrid modelReconstruction Approach) including two phases of genetic and
ensemble learning. The phase of genetic algorithm was tobuild the optimal weight combination of multiple classiÔ¨Åers
by genetic algorithm. The phase of ensemble learning was
to produce a classiÔ¨Åer with good performance by AdaBoostmethod.
B. Class overlap
Class overlap is that some instances in training data are
close to or even overlap in the distribution space but havedifferent class. These instances often lead to poor class bound-
ary, and hinder to build a good learning model with good
performance [33]. In the Ô¨Åeld of SDP, researchers consideredclass overlap problem as the data quality or noise detection.
Tang et al. [34] proposed a clustering-based noise detection
method to remove noisy instances identiÔ¨Åed by the noise fac-tor. They conducted experiments on NASA with C4.5 learner,and the results indicated that removing noisy instances couldimprove the accuracy of classiÔ¨Åer. Kim et al. [35] evaluated
the effect of the dataset containing both false positive and false
negative noise, and proposed the closest list noise identiÔ¨Åcation(CLNI) method to identify noise instances. Based on CLNI,Chen et al. [18] proposed neighborhood cleaning learning
(NCL) to deal with both class overlap and class imbalance. Theexperimental results indicated that the new learning models
could get best values in terms of G-mean and AUC compared
with state-of-the-art methods.
However, they do not provide a comparison with CPDP
models. In our work, we evaluate the impact of class overlapon both WPDP and CPDP.
III. M
ETHODOLOGY
In this section, we Ô¨Årstly provide the rationale to our
studying questions. Then, we report the experimental data.At last, we introduce the methods under study and model

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. /g55/g85/g68/g76/g81/g76/g81/g74/g3
/g71/g68/g87/g68
/g55/g72/g86/g87/g76/g81/g74
/g71/g68/g87/g68/g53/g72/g80/g82/g89/g76/g81/g74/g3
/g82/g89/g72/g85/g79/g68/g83/g3
/g76/g81/g86/g87/g68/g81/g70/g72/g86
/g55/g85/g68/g76/g81/g76/g81/g74/g3
/g71/g68/g87/g68
/g55/g72/g86/g87/g76/g81/g74
/g71/g68/g87/g68/g54/g39/g51/g3/g80/g82/g71/g72/g79
/g11/g53/g52/g20/g12/g3/g58/g51/g39/g51/g3
/g80/g82/g71/g72/g79/g86/g11/g53/g52/g21/g12/g3/g38/g51/g39/g51/g3
/g80/g82/g71/g72/g79/g86
/g51/g85/g72/g71/g76/g70/g87 /g51/g85/g72/g71/g76/g70/g87
/g37/g68/g79/g68/g81/g70/g72/g36/g56/g38/g53/g72/g70/g68/g79/g79 /g37/g68/g79/g68/g81/g70/g72/g36/g56/g38/g53/g72/g70/g68/g79/g79
/g48/g82/g71/g72/g79/g3/g40/g89/g68/g79/g88/g68/g87/g76/g82/g81 /g48/g82/g71/g72/g79/g3/g40/g89/g68/g79/g88/g68/g87/g76/g82/g81 /g48/g82/g71/g72/g79/g3/g40/g89/g68/g79/g88/g68/g87/g76/g82/g81
/g53/g72/g86/g88/g79/g87/g86
Fig. 2. The overview of our model construction and evaluation approach.
evaluation. Figure 2 provides an overview of the steps in our
study.
A. Research questions
Our study is to evaluate the effect of overlapping instances
on SDP. To this end, our work checks the following two
questions.
RQ1: How does class overlap inÔ¨Çuence the prediction per-
formance of existing within-project defect prediction models?
RQ2: How does class overlap inÔ¨Çuence the prediction per-
formance of existing cross-project defect prediction models?
The aim of RQ1 and RQ2 are to compare the performance
of the existing state-of-the-art learning models by removing
overlapping instances against without removing under twoprediction cases (WPDP and CPDP). For RQ1, we study
NB [7], [10], RF [21], SVM [23], LR [24] and KNN [22]
classiÔ¨Åers for WPDP models. For RQ2, we study NN-Ô¨Ålter
[28], TCA+ [6], VCB-SVM [31], DTB [29], MNB [30], and
HYDRA [32] models for CPDP. Noted that all the above
learning models were conducted on Python 3.6 and Scikit-learn (0.19.2). Since our works are to investigate the impactof class overlap on SDP, the parameters of NB, RF, SVM, LR
and KNN classiÔ¨Åers are set as default parameters on Scikit-
learn (0.19.2), and the parameters of CPDP learning models
are set as the same as their studies.
If the learning models under removing overlapping instances
are much better than these without removing overlappinginstances, it would be a good option for practitioners to employ
removing overlapping instances before building the models.In addition, if the performance ranking of learning model ischanged, it would be indicated that class overlap problem hasa greater impact on this learning model.
B. Datasets
In order to build and validate the effect of class over-
lap on software defect prediction, we use 28 projects fromNASA, AEEEM, ReLink, SOFTLAB and MORPH groups.
These datasets collected from these 28 projects includes static
code metrics and process metrics, and the defect matchingwas relied on SZZ algorithm. The metrics along with label
information of these datasets are at different level (function orclass or Ô¨Åle granularity).
Projects from NASA systems are NASA aerospace projects
which were gathered as part of the metric data program
(MDP). These projects are all only one version and their metricfor SDP are function granularity. We only apply CM1, MW1,
PC1, PC3 and PC4 projects as they were developed by Clanguage and they have the 37 common metrics.
Projects from SOFTLAB systems are embedded controllers
for household appliances from Turkish software company.
These projects include AR1, AR3, AR4, AR5 and AR6developed by C language. The metrics for these projects arealso function granularity.
Projects from MORPH systems are collected by Jureczko
and Spinellis [15] including 46 releases of 14 open-sourceprojects and 11 student projects. The metrics of these projects
are class granularity and developed by Java language. In our
experiment, we choose one version for multi-version project,which were used in reference [36].
Projects from AEEEM and ReLink are developed by Java
language. AEEEM are collected by D‚ÄôAmbros et al. [13],and ReLink are collected by Wu et al. [14]. The metrics in
AEEEM are class granularity, and the metrics of ReLink areÔ¨Åle granularity.
Table I shows the testing projects used in our experiment.
From this table, we can observe that (i) the test projects are
from different Ô¨Åelds and are developed by different languages(C or Java); (ii) the number of instances in the test projectsvary greatly and the percentage of defective instances are verylow leading to class imbalance. These observations concludethat these test projects could be used to provide a fairevaluation of the impact of class overlap on software defect
prediction.
C. Methods to build the models
In the Ô¨Åeld of SDP, class overlap problem often is consid-
ered as the data quality or noise detection. In our experiment,
we use KMCCA [34], NCL [18] and our proposed IKMCCA
methods to remove the overlapping instances.
CLNI approach was presented by Kim [35] and used by
Chen simultaneously [18]. Chen et al. considered both classimbalance and class overlap in the same time. So in ourexperiment, we used the NCL proposed by Chen. They used

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. TABLE I
THE EXPERIMENTAL DATASETS .
Group Project Langeage granularity Number of metricsNumber of
total instances%o f
defective instances
NASACM1
C function37327 12.84
MW1 253 10.67
PC1 705 8.65
PC3 1077 12.44
PC4 1458 12.21
SOFTLABAR1
29121 7.44
AR3 63 12.70
AR4 107 18.69
AR5 36 22.22
AR6 101 14.85
AEEEMEquinox Framework (EQ)
JavaClass 61324 39.81
Eclipse JDT core (JDT) 997 20.66
Apache Lucence (LC) 691 9.26
Mylyn (ML) 1862 13.16
Eclipse PDE UI (PDE) 1497 13.96
ReLinkApache HTTP Server
File 26194 50.52
OpenIntents Safe 56 39.29
ZXing 399 29.57
MORPHant1.3
Class 20125 16
camel1.0 339 3.83
poi1.5 237 59.49
tomcat 858 8.97
velocity1.4 196 75
Xalan2.4 723 15.21
xerces1.2 440 16.14
arc 234 11.54
redktor 176 15.34
skarbonka 45 20
the neighborhood cleaning learn (NCL) rule to remove the
overlapping instances. It is noted that the non-defective in-stances only be removed to stress the class imbalance problem.
This is to say, they searched the nearest neighbors of each
defective instance and removed the nearest neighbors whoseclass were non-defective.
KMCCA approach was presented by Tang [34], and this
was a clustering-based noise detection approach based on K-
means. Firstly, the K-means algorithm was used to cluster the
data into Kclusters. Then, for each cluster, the noise factor
values of each instance are computed. Finally, the top p%o f
instances were removed.
In order to the same as the NCL, we improved the KMCCA
method called IKMCCA that considers the class imbalance at
the same time. In step of the removing overlapping instances,the rule is based on the percentage of defective instances. Ifthe percentage of defective instances in the i
thcluster is below
the p%, the defective instance in this cluster are removed.
On the contrary, the non-defective instances in this clusterare removed. Pseudo-code for IKMCCA
1is presented in
algorithm 1.
Noted that we employ the log-transformation before using
the datasets which was studied by previous researches [37].The pseudo-code for the simulation experiments are providedin algorithm 2 for evaluating the impact of class overlap toanswer RQ1 and RQ2.
1https://github.com/glnmzx888/class-overlapTABLE II
PSEUDO -CODE FOR IKMCCA.
Algorithm 1 Pseudo-code for IKMCCA
Inputs : Traning data D
the parameter m
1: n= the number of instances in D
2: d= the number of defective instances in D
3: p=d
n
4: k=/floorleftbign
m/floorrightbig
5: Using K-means algorithm to divide D into k clusters
6: fori=1‚àí>k do:
7: Compute the ratio r of defective instances to all instances
in cluster i
8: ifr>p :
9: delete the non-defective instances in cluster i
10: else:
11: delete the defective instances in cluster i
12: end if
13: end for
14: Combine the remaining instances in each cluster
D. Model Evaluation
In order to investigate the impact of class overlap on
learning models, we use three performance measures including
Balance [30], Recall and Area Under the receiver operating
characteristic Curve ( AUC ) [38].
Balance (bal) is the balance between pdand pf. The bigger
the balvalue is, the better the performance of learning model

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. TABLE III
PSEUDO -CODE FOR THE EXPERIMENTAL SETUP .
Algorithm 2 Pseudo-code for the experimental setup
Inputs : DATA1= {CM1, MW1, PC1, PC3, PC4 }
DATA2= {AR1, AR3, AR4, AR5, AR6 };
DATA3= {EQ, JDT, LC, ML, PDE };
DATA4= {Apache, Safe, ZXing };
DATA5= {ant1.3, camel1.0, poi1.5, tomcat, velocity1.4,
Xalan2.4, xerces1.2, arc, redktor, skarbonka };
learning model= {WPDP: {NB, SVM, KNN, RF, LR },
CPDP: {NN-Ô¨Ålter, TCA+, VCB-SVM,
DTB, MNB, HYDRA }}
D A T Ai sD A T A 1o rD A T A 2o rD A T A 3o rD A T A 4
or DATA5
methods=KMCCA, IKMCCA, NCL
1:fordata in DATA do:
2: fori=1‚àí>20do:
3: ifWPDP:
4: {WPTrain=Select 85% of data
5: Test=data-WPTrain }
6: end if
7: ifCPDP:
8: ifMixed-project:
9: {WP=Select 15% of a project,
10: Test=project-WP
11: CPTrain=data-project+WP }
12: else:
13: {Test=a project in data
14: CPTrain=data-project }
15: end if
16: end if
17: employ log-trainsform on WPTrain, CPTrain, Test
18: employ methods:KMCCA, IKMCCA, NCL on WPTrain
and CPTrain to remove overlapping instances
19: formodel in learning model:
20: Apply the learning model to train on WPTrain
or CPTrain
21: Apply the model to predict the class on Test and
report the perftomance: Balance ,Recall ,AUC
on Test for each learning model
22: end for
23: end for
24: end for
is. It is deÔ¨Åned as
bal=1‚àí/radicalbig
(1‚àípd)2+pf2
‚àö
2. (1)
Recall is the ratio of the number of correctly predicted
defective instances to the total number of defective instances.The bigger the Recall value is, the better the performance of
learning model is. It is deÔ¨Åned as
Recall =TP
TP+FN. (2)
AUC is the area under a Receiver Operating Characteristic
(ROC) curve. The x-axis of ROC is pfand y-axis of ROC ispd. The bigger the AUC value is, the better the performance
of learning model is.
In these measures, pd(probability of detection) is the ratio
of correctly predicted defective instances to total defective
instances. pfis the fraction of incorrectly predicted defective
instances to total non-defective instances.
In order to make a statistical evaluation of the detailed
prediction results, the non-parametric Friedman test with theNemenyi test is used to compare multiple runs over the 28
projects. The conÔ¨Ådence level is 95%. They are widely used in
software defect prediction (SDP) [39], [40]. Firstly, the Fried-man test is used to determine whether there are statistically
signiÔ¨Åcant differences among compared methods. If there are
signiÔ¨Åcant differences in statistical data, the differences willbe test by post-hoc Nemenyi test.
In addition, to evaluate the degree of difference among the
compared methods in terms of bal,Recall and AUC results,
we apply Cliff‚Äôs Delta to measure the effect size, which don‚Äôt
need the compared vectors meet normality assumption. This
is calculated as followed [41]:
Cliff
/primesD e l t a =#(x1>x 2)‚àí#(x1<x 2)
n1n2. (3)
Wherex1andx2are scores within group 1 (performance
values achieved by one compared method) and group 2 (per-
formance values achieved by another compared method), and
n1andn2are the sizes of the instances groups. The cardinality
symbol # indicates the number.
The effect size of Cliff‚Äôs Delta is divided into four levels:
|d|<0.147 (Negligible, N), 0.147‚â§|d|<0.333 (Small, S),
0.333‚â§|d|<0.474 (Medium, M) and |d|‚â•0.474 (Large,
L).
IV . E XPERMENTAL RESULTS
In this section, we report the experimental results for the
comparison of removing overlapping instances with without
removing overlapping instances to answer RQ1 and RQ2.
A. RQ1: How does class overlap inÔ¨Çuence the prediction per-
formance of existing within-project defect prediction models?
For answering the question, we replicate the experiments
on the 28 projects by NB, RF, SVM, KNN and LR classiÔ¨Åers
using IKMCCA VS. NCL VS. KMCCA VS. without removing
data, respectively. In IKMCCA method, the percentage p%i s
set as the percentage of defective instances in the training
data, and the parameter mis set as 20, that will be discussed
in section V . Parameters of KMCCA and NCL methods are setthe same as the references [34] and [18]. We randomly sample85% instances as training data, and the rest instances are astesting data. Noted that all compared methods are performedon Python3.6 and Scikit-learn (0.19.2) in our study. At thesame time, we repeat these experiments 20 times to avoid
randomness, and report the violin plots of the results for each
learning models in Ô¨Ågures 3, 4 and 5. The violin plot combines
the characteristics of box plot and density plot which could
display the distribution and probability density of results. At

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. the same time, the white dot on each violin represents the
medium value.
From these Ô¨Ågures, we can observe that using removing the
overlapping instances approaches including IKMCCA, NCLand KMCCA can get better medium values of bal,Recall
and AUC than these methods without removing overlapping
instances on NB, RF, SVM, KNN and LR classiÔ¨Åers. At the
same time, these WPDP learning models by using IKMCCAcan achieve best bal,Recall and AUC results than these
by using NCL and KMCCA for most projects. That is to
say, (i) compared with KMCCA, it is better to considerclass imbalance when solving class overlap; (ii) comparedwith NCL, it makes more sense to eliminate both defectiveinstances and non-defective instances.
/g54/g57/g48/g49/g37 /g53/g41 /g47/g53 /g46/g49/g49
/g58/g51/g39/g51/g3/g79/g72/g68/g85/g81/g76/g81/g74/g3/g80/g82/g71/g72/g79/g86/g19/g17/g19/g19/g17/g21/g19/g17/g23/g19/g17/g25/g19/g17/g27/g20/g17/g19/g37/g68/g79/g68/g81/g70/g72
/g44/g46/g48/g38/g38/g36
/g49/g38/g47
/g46/g48/g38/g38/g36/g3
/g58/g76/g87/g75/g82/g88/g87/g3/g85/g72/g80/g82/g89/g76/g81/g74
Fig. 3. The violin plot on balfor Within-project defect prediction (WPDP)
learning models. The white dot on each violin plot represents the medium ofeach model.
/g54/g57/g48/g49/g37 /g53/g41 /g47/g53 /g46/g49/g49
/g58/g51/g39/g51/g3/g79/g72/g68/g85/g81/g76/g81/g74/g3/g80/g82/g71/g72/g79/g86/g19/g17/g21/g19/g17/g19/g19/g17/g21/g19/g17/g23/g19/g17/g25/g19/g17/g27/g20/g17/g19/g20/g17/g21/g53/g72/g70/g68/g79/g79
/g44/g46/g48/g38/g38/g36
/g49/g38/g47/g46/g48/g38/g38/g36/g3
/g58/g76/g87/g75/g82/g88/g87/g3/g85/g72/g80/g82/g89/g76/g81/g74
Fig. 4. The violin plot on Recall for Within-project defect prediction (WPDP)
learning models. The white dot on each violin plot represents the mediumvalue of each model.
Furthermore, in order to statistically investigate the exper-
imental results, we rank the WPDP learning models by non-
parametric Fredman test with post-hoc Nemenyi test. Figures
6, 7, 8 show the rank results in terms of bal,Recall and
AUC . From these Ô¨Ågures we can Ô¨Ånd that (i) for each WPDP
classiÔ¨Åer, IKMCCA is always in the Ô¨Årst rank, NCL andKMCCA are in same rank, and without removing approach is
in the last rank. (ii) Regardless of the different class overlap
methods, the NB classiÔ¨Åer always ranks the Ô¨Årst, and RF,/g54/g57/g48/g49/g37 /g53/g41 /g47/g53 /g46/g49/g49
/g58/g51/g39/g51/g3/g79/g72/g68/g85/g81/g76/g81/g74/g3/g80/g82/g71/g72/g79/g86/g19/g17/g19/g19/g17/g21/g19/g17/g23/g19/g17/g25/g19/g17/g27/g20/g17/g19/g36/g56/g38
/g44/g46/g48/g38/g38/g36
/g49/g38/g47
/g46/g48/g38/g38/g36/g3
/g58/g76/g87/g75/g82/g88/g87/g3/g85/g72/g80/g82/g89/g76/g81/g74
Fig. 5. The violin plot on AUC for Within-project defect prediction (WPDP)
learning models. The white dot on each violin plot represents the medium of
each model.
KNN and LR rank the same. (iii) For SVM classiÔ¨Åer, using
IKMCCA method can achieve the second in these classiÔ¨Åers,
that is to say IKMCCA could greatly improve the predictingperformance for SVM.
In addition, in order to make a clearer comparison of
these models, we compare the effectiveness between without
removing method and each of class overlap methods in all 28projects based on Cliff‚Äôs Delta, and the results are shown intable IV.
For IKMCCA method, we can Ô¨Ånd the values of effect size
are large for compared WPDP learning models in terms of bal,
Recall and AUC . For NCL methods, the values of effect size
in terms of baland Recall are most small, and the values of
effect size in terms of AUC are most Medium. For KMCCA,
the values of effect size in terms of bal,Recall and AUC are
all small. That is to say, for improving the performance of
learning models, considering both class overlap and class im-
balance is more effective than only considering class overlap.
/g54/g57/g48/g11/g44/g46/g48/g12/g24/g17/g27/g20/g23
/g54/g57/g48/g11/g49/g38/g47/g12/g20/g23/g17/g27/g26
/g54/g57/g48/g11/g46/g48/g12/g20/g24/g17/g20/g27/g27
/g54/g57/g48/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g25/g17/g24/g24/g26/g49/g37/g11/g44/g46/g48/g12/g3/g23/g17/g24/g19/g27
/g49/g37/g11/g46/g48/g12/g26/g17/g26/g24/g21
/g49/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g28/g17/g21/g28/g20
/g46/g49/g49/g11/g49/g38/g47/g12/g20/g19/g17/g25/g21/g22/g46/g49/g49/g11/g46/g48/g12/g20/g20/g17/g22/g25/g20 /g49/g37/g11/g49/g38/g47/g12/g25/g17/g23/g26/g28
/g46/g49/g49/g11/g44/g46/g48/g12/g25/g17/g24/g24/g24/g3
/g53/g41/g11/g44/g46/g48/g12/g26/g17/g20/g23/g20
/g53/g41/g11/g49/g38/g47/g12/g28/g17/g25/g22/g28/g53/g41/g11/g46/g48/g12/g20/g19/g17/g27/g21
/g46/g49/g49/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g23/g17/g19/g21/g22/g3
/g53/g41/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g23/g17/g21/g24 /g47/g53/g11/g44/g46/g48/g12/g26/g17/g21/g19/g24/g47/g53/g11/g49/g38/g47/g12/g20/g19/g17/g28/g26/g22
/g47/g53/g11/g46/g48/g12/g20/g21/g17/g24/g25/g20
/g47/g53/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g23/g17/g22/g27/g28/g38/g85/g76/g87/g76/g70/g68/g79/g3/g39/g76/g73/g73/g72/g85/g72/g81/g70/g72/g3/g32/g3/g20/g17/g24
/g19/g20/g21/g22/g23/g24/g25/g26/g27/g28 /g20/g21/g20/g22/g20/g23/g20/g24/g20/g25/g20/g26/g20/g27/g20/g28/g21/g19/g21/g20 /g20/g19/g3/g20/g20/g3
Fig. 6. The ranks on bal for within-project defect prediction (WPDP)
methods with post-hoc Nemenyi test. Methods connected by gray lines are
not signiÔ¨Åcantly different.
B. RQ2: How does class overlap inÔ¨Çuence the prediction per-
formance of existing cross-project defect prediction models?
For answering this question, we replicate the experiments on
the 28 projects by NN-Ô¨Ålter, TCA+, VCB-SVM, DTB, MNB,
and HYDRA CPDP learning models using IKMCCA VS.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. /g49/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g26/g17/g26/g20/g27/g46/g49/g49/g11/g46/g48/g12/g20/g20/g17/g26/g22/g28/g49/g37/g11/g44/g46/g48/g12/g23/g17/g24/g28/g20/g3
/g49/g37/g11/g49/g38/g47/g12/g24/g17/g25/g26/g24/g3/g54/g57/g48/g11/g44/g46/g48/g12/g24/g17/g27/g23/g20/g3/g49/g37/g11/g46/g48/g12/g25/g17/g25/g27/g22/g3
/g46/g49/g49/g11/g44/g46/g48/g12/g25/g17/g28/g3
/g53/g41/g11/g44/g46/g48/g12/g26/g17/g21/g28/g24
/g53/g41/g11/g49/g38/g47/g12/g20/g19/g17/g22/g3
/g46/g49/g49/g11/g49/g38/g47/g12/g20/g20/g17/g19/g24/g21/g53/g41/g11/g46/g48/g12/g20/g20/g17/g23/g20/g25
/g47/g53/g11/g44/g46/g48/g12/g26/g17/g23/g22/g22/g47/g53/g11/g49/g38/g47/g12/g20/g20/g17/g23/g23/g24
/g47/g53/g11/g46/g48/g12/g20/g21/g17/g25/g27/g3
/g46/g49/g49/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g22/g17/g26/g24/g28
/g47/g53/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g23/g17/g22/g20/g20/g3
/g53/g41/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g23/g17/g25/g25/g22/g3
/g54/g57/g48/g11/g49/g38/g47/g12/g20/g23/g17/g27/g20/g22
/g54/g57/g48/g11/g46/g48/g12/g20/g24/g17/g21/g26/g24/g3
/g54/g57/g48/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g25/g17/g23/g20/g20/g38/g85/g76/g87/g76/g70/g68/g79/g3/g39/g76/g73/g73/g72/g85/g72/g81/g70/g72/g3/g32/g3/g20/g17/g24
/g19/g20/g21/g22/g23/g24/g25/g26/g27/g28 /g20/g21/g20/g22/g20/g23/g20/g24/g20/g25/g20/g26/g20/g27/g20/g28/g21/g19/g21/g20 /g20/g19/g3/g20/g20/g3
Fig. 7. The ranks on Recall for within-project defect prediction (WPDP)
methods with post-hoc Nemenyi test. Methods connected by gray lines are
not signiÔ¨Åcantly different.
/g54/g57/g48/g11/g44/g46/g48/g12/g24/g17/g28/g23/g25/g49/g37/g11/g44/g46/g48/g12/g23/g17/g28/g27/g23
/g49/g37/g11/g49/g38/g47/g12/g25/g17/g25/g26/g20
/g49/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g28/g17/g27/g24/g28/g46/g49/g49/g11/g49/g38/g47/g12/g20/g19/g17/g23/g27/g28
/g46/g49/g49/g11/g46/g48/g12/g20/g20/g17/g19/g24/g24
/g46/g49/g49/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g23/g17/g19/g21/g20/g46/g49/g49/g11/g44/g46/g48/g12/g26/g17/g19/g25/g20/g3
/g53/g41/g11/g44/g46/g48/g12/g26/g17/g24/g23/g25
/g49/g37/g11/g46/g48/g12/g27/g17/g20/g27/g28/g3
/g53/g41/g11/g49/g38/g47/g12/g28/g17/g21/g19/g23
/g53/g41/g11/g46/g48/g12/g20/g19/g17/g23/g26/g24/g47/g53/g11/g44/g46/g48/g12/g26/g17/g25/g22/g47/g53/g11/g49/g38/g47/g12/g20/g19/g17/g25/g21/g28
/g47/g53/g11/g46/g48/g12/g20/g21/g17/g20/g23/g20
/g53/g41/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g23/g17/g21/g24/g24/g3
/g47/g53/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g23/g17/g22/g21/g21
/g54/g57/g48/g11/g49/g38/g47/g12/g20/g23/g17/g24/g21/g20
/g54/g57/g48/g11/g46/g48/g12/g20/g23/g17/g27/g24/g20/g3
/g54/g57/g48/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g25/g17/g20/g22/g26/g38/g85/g76/g87/g76/g70/g68/g79/g3/g39/g76/g73/g73/g72/g85/g72/g81/g70/g72/g3/g32/g3/g20/g17/g24
/g19/g20/g21/g22/g23/g24/g25/g26/g27/g28 /g20/g21/g20/g22/g20/g23/g20/g24/g20/g25/g20/g26/g20/g27/g20/g28/g21/g19/g21/g20 /g20/g19/g3/g20/g20/g3
Fig. 8. The ranks on AUC for within-project defect prediction (WPDP)
methods with post-hoc Nemenyi test. Methods connected by gray lines are
not signiÔ¨Åcantly different.
NCL VS. KMCCA VS. without removing data, respectively.
These CPDP learning models are introduced in section II.A.
In IKMCCA method, the percentage pis set as the percentage
of defective instances in the training data, and the parameter m
is set as 20, that will be discussed in section V . Parameters of
KMCCA and NCL methods are set the same as the references
[34] and [18]. Note that the implementation of all comparedmethods is based on Python3.6 and Scikit-learn (0.19.2) in
our experiments. In order to avoid randomness, we repeat the
above experiments 20 times and report the violin plots of the
results for each learning models in Ô¨Ågures 9, 10, 11.
From these Ô¨Ågures, we can observe that the CPDP learning
models using IKMCCA, NCL and KMCCA can obtain the
better median values than these models without removingoverlapping instances in terms of bal,Recall and AUC .A t
the same time, the high density values getting by IKMCCA,NCL and KMCCA are better than that without removingoverlapping instances.
Furthermore, in order to statistically investigate the experi-
mental results, we apply the non-parametric Fredman test withpost-hoc Nemenyi test to rank the CPDP learning models.Figures 12, 13, 14 show the rank results in terms of bal,Recall
and AUC .
From these Ô¨Ågures, we can Ô¨Ånd that (i) for all CPDPTABLE IV
THE EFFECTIVENESS RESULTS OF WITHOUT REMOVING AGAINST EACH
COMPARED APPROACH FOR WPDP LEARNING MODELS IN TERMS OF bal,
Recall ,AND AUC .
Learning
modelremoving VS Without bal Recall AUC
SVMIKMCCA Vs. Without 0.797(L) 0.769(L) 0.761(L)
NCL Vs. Without 0.232(S) 0.192(S) 0.206(S)
KMCCA Vs. Without 0.205(S) 0.161(S) 0.172(S)
NBIKMCCA Vs. Without 0.523(L) 0.367(M) 0.479(L)
NCL Vs. Without 0.3956(M) 0.198(S) 0.338(M)
KMCCA Vs. Without 0.263(S) 0.168(S) 0.168(S)
KNNIKMCCA Vs. Without 0.645(L) 0.614(L) 0.718(L)
NCL Vs. Without 0.298(S) 0.258(S) 0.334(M)
KMCCA Vs. Without 0.269(S) 0.233(S) 0.299(S)
RFIKMCCA Vs. Without 0.617(L) 0.636(L) 0.745(L)
NCL Vs. Without 0.357(M) 0.336(M) 0.398(M)
KMCCA Vs. Without 0.257(S) 0.24(S) 0.292(S)
LRIKMCCA Vs. Without 0.651(L) 0.621(L) 0.741(L)
NCL Vs. Without 0.357(M) 0.236(S) 0.334(M)
KMCCA Vs. Without 0.205(S) 0.179(S) 0.234(S)
/g49/g49/g16/g73/g76/g79/g87/g72/g85 /g55/g38/g36/g14 /g48/g49/g37 /g43/g60/g39/g53/g36 /g39/g55/g37/g3 /g57/g38/g37/g3
/g38/g51/g39/g51/g3/g79/g72/g68/g85/g81/g76/g81/g74/g3/g80/g82/g71/g72/g79/g86/g19/g17/g21/g19/g17/g23/g19/g17/g25/g19/g17/g27/g20/g17/g19/g69/g68/g79/g44/g46/g48/g38/g38/g36
/g49/g38/g47
/g46/g48/g38/g38/g36/g3
/g58/g76/g87/g75/g82/g88/g87/g3/g85/g72/g80/g82/g89/g76/g81/g74
Fig. 9. The violin plot on bal for cross-project defect prediction (CPDP)
learning models. The white dots on each violin plot represents the medium
value of each model.
learning models, removing overlapping instances including
IKMCCA, NCL and KMCCA can get signiÔ¨Åcantly better thanthese without removing method, and IKMCCA ranks better
than NCL and KMCCA; (ii) for class overlapping methods,
HYDRA, DTB and VCB always rank better than NN, TCA+and MNB; (iii) the performance ranking of TCA+ learningmodel by removing overlapping instances becomes better thanwithout removing, it would be indicated that class overlap hasa greater impact on TCA+.
In addition, to give a clearer comparison of the methods, we
compare the effect size over all 28 projects between withoutremoving method and each of class overlap methods according
to Cliff‚Äôs Delta, and the results are shown in table V.
For IKMCCA method, we can Ô¨Ånd the values of effect size
are large or medium for compared CPDP learning models interms of bal,Recall and AUC . For NCL methods, the values
of effect size in terms of bal,Recall and AUC are most small
except for TCA+. For KMCCA, the values of effect size in
terms of bal,Recall and AUC for NN-Ô¨Ålter and TCA+ models
are all large, and the values of effect size for other models are

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. /g49/g49/g16/g73/g76/g79/g87/g72/g85 /g55/g38/g36/g14 /g48/g49/g37 /g43/g60/g39/g53/g36 /g39/g55/g37/g3 /g57/g38/g37/g3
/g38/g51/g39/g51/g3/g79/g72/g68/g85/g81/g76/g81/g74/g3/g80/g82/g71/g72/g79/g86/g19/g17/g21/g19/g17/g19/g19/g17/g21/g19/g17/g23/g19/g17/g25/g19/g17/g27/g20/g17/g19/g20/g17/g21/g53/g72/g70/g68/g79/g79/g44/g46/g48/g38/g38/g36
/g49/g38/g47/g46/g48/g38/g38/g36/g3
/g58/g76/g87/g75/g82/g88/g87/g3/g85/g72/g80/g82/g89/g76/g81/g74
Fig. 10. The violin plot on Recall for cross-project defect prediction (CPDP)
learning models. The white dots on each violin plot represents the medium
value of each model.
/g49/g49/g16/g73/g76/g79/g87/g72/g85 /g55/g38/g36/g14 /g48/g49/g37 /g43/g60/g39/g53/g36 /g39/g55/g37/g3 /g57/g38/g37/g3
/g38/g51/g39/g51/g3/g79/g72/g68/g85/g81/g76/g81/g74/g3/g80/g82/g71/g72/g79/g86/g19/g17/g19/g19/g17/g21/g19/g17/g23/g19/g17/g27/g20/g17/g19/g36/g56/g38/g44/g46/g48/g38/g38/g36
/g49/g38/g47/g46/g48/g38/g38/g36/g3
/g58/g76/g87/g75/g82/g88/g87/g3/g85/g72/g80/g82/g89/g76/g81/g74
Fig. 11. The violin plot on AUC for cross-project defect prediction (CPDP)
learning models. The white dots on each violin plot represents the medium
value of each model.
small.
C. Answers to research questions
‚Ä¢RQ1: How does class overlap inÔ¨Çuence the prediction
performance of existing within-project defect prediction
models?
This work is to investigate whether it is feasible to
removing overlapping instances to improve the defectpredicting performance of the WPDP learning models.From the experimental results, we observe a statistically
signiÔ¨Åcant improvement in favor of removing overlapping
instances including IKMCCA, NCL and KMCCA. It isuseful to consider removing the overlapping instances
before building the WPDP defect prediction models, inparticular, using SVM as the classiÔ¨Åer. IKMCCA and
NCL can achieve better results than KMCCA for WPDP
classiÔ¨Åers. Therefore, it is better to consider class imbal-ance when dealing with class overlap. Also, IKMCCA
can achieve better than NCL, it can not only consider the
overlapping non-defective instances when handling bothclass overlap and class imbalance for WPDP.
‚Ä¢RQ2: How does class overlap inÔ¨Çuence the predictionperformance of existing cross-project defect predictionmodels?/g49/g49/g11/g44/g46/g48/g12/g20/g19/g17/g23/g23/g25/g49/g49/g11/g46/g48/g12/g20/g20/g17/g25/g25/g25/g55/g38/g36/g14/g11/g44/g46/g48/g12/g26/g17/g28/g20/g23
/g55/g38/g36/g14/g11/g58/g76/g87/g75/g82/g88/g87/g12/g21/g20/g17/g28/g23/g25/g39/g55/g37/g11/g49/g38/g47/g12/g28/g17/g19/g21/g20/g39/g55/g37/g11/g44/g46/g48/g12/g25/g17/g26/g27/g23/g3
/g39/g55/g37/g11/g46/g48/g12/g26/g17/g23/g27/g28
/g39/g55/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g20/g17/g19/g22/g21
/g57/g38/g37/g11/g49/g38/g47/g12/g28/g17/g20/g25/g27/g57/g38/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g20/g17/g24/g20/g22/g57/g38/g37/g11/g46/g48/g12/g20/g19/g17/g26/g23/g20
/g55/g38/g36/g14/g11/g46/g48/g12/g20/g26/g17/g24/g22/g27/g3
/g48/g49/g37/g11/g46/g48/g12/g20/g28/g17/g20/g24/g23/g3
/g48/g49/g37/g11/g49/g38/g47/g12/g20/g28/g17/g22
/g49/g49/g11/g49/g38/g47/g12/g20/g28/g17/g26/g26/g24/g3
/g55/g38/g36/g14/g11/g49/g38/g47/g12/g20/g28/g17/g27/g21/g28
/g48/g49/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g28/g17/g27/g25/g25/g3/g3
/g49/g49/g11/g58/g76/g87/g75/g82/g88/g87/g12/g21/g20/g17/g20/g20/g43/g60/g39/g53/g36/g11/g44/g46/g48/g12/g23/g17/g25/g27/g27
/g43/g60/g39/g53/g36/g11/g49/g38/g47/g12/g27/g17/g21/g27/g24
/g43/g60/g39/g53/g36/g11/g58/g76/g87/g75/g82/g88/g87/g12/g27/g17/g23/g21/g26/g3
/g48/g49/g37/g11/g44/g46/g48/g12/g27/g17/g24/g24/g26/g57/g38/g37/g11/g44/g46/g48/g12/g26/g17/g27/g23/g20/g3
/g43/g60/g39/g53/g36/g11/g46/g48/g12/g26/g17/g28/g19/g27/g38/g85/g76/g87/g76/g70/g68/g79/g3/g39/g76/g73/g73/g72/g85/g72/g81/g70/g72/g3/g32/g3/g20/g17/g27/g22
/g19/g20/g21/g22/g23/g24/g25/g26/g27/g28/g20/g19/g20/g20/g20/g21/g20/g22/g20/g23/g20/g24/g20/g25/g20/g26/g20/g27/g20/g28/g21/g19/g21/g20/g21/g21/g21/g22/g21/g23/g21/g24
Fig. 12. The ranks on bal for within-project defect prediction (CPDP)
methods with post-hoc Nemenyi test. Methods connected by gray lines are
not signiÔ¨Åcantly different.
/g49/g49/g11/g44/g46/g48/g12/g26/g17/g20/g24/g21
/g49/g49/g11/g49/g38/g47/g12/g21/g19/g17/g19/g24/g21/g49/g49/g11/g46/g48/g12/g20/g20/g17/g22/g20/g23/g55/g38/g36/g14/g11/g44/g46/g48/g12/g25/g17/g28/g22/g21
/g55/g38/g36/g14/g11/g49/g38/g47/g12/g21/g19/g17/g20/g25/g25/g3
/g49/g49/g11/g58/g76/g87/g75/g82/g88/g87/g12/g21/g19/g17/g20/g26/g26/g55/g38/g36/g14/g11/g46/g48/g12/g20/g26/g17/g27/g20/g23
/g55/g38/g36/g14/g11/g58/g76/g87/g75/g82/g88/g87/g12/g21/g21/g17/g22/g19/g23/g39/g55/g37/g11/g49/g38/g47/g12/g20/g19/g17/g25/g25/g22
/g39/g55/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g20/g17/g28/g28/g25/g57/g38/g37/g11/g44/g46/g48/g12/g22/g17/g22/g19/g23
/g57/g38/g37/g11/g49/g38/g47/g12/g20/g19/g17/g23/g19/g24/g57/g38/g37/g11/g46/g48/g12/g20/g21/g17/g24/g21/g22
/g57/g38/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g28/g17/g26/g22/g27/g48/g49/g37/g11/g44/g46/g48/g12/g27/g17/g25/g28/g27 /g48/g49/g37/g11/g49/g38/g47/g12/g20/g28/g17/g24/g21/g48/g49/g37/g11/g46/g48/g12/g20/g28/g17/g22/g27/g25
/g48/g49/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g28/g17/g26/g24/g23/g43/g60/g39/g53/g36/g11/g44/g46/g48/g12/g24/g17/g26/g21/g20
/g43/g60/g39/g53/g36/g11/g58/g76/g87/g75/g82/g88/g87/g12/g28/g17/g22/g20/g25/g43/g60/g39/g53/g36/g11/g49/g38/g47/g12/g26/g17/g28/g22/g3
/g39/g55/g37/g11/g44/g46/g48/g12/g27/g17/g20/g26/g3/g43/g60/g39/g53/g36/g11/g46/g48/g12/g27/g17/g23/g25/g28/g3
/g39/g55/g37/g11/g46/g48/g12/g27/g17/g23/g28/g27/g38/g85/g76/g87/g76/g70/g68/g79/g3/g39/g76/g73/g73/g72/g85/g72/g81/g70/g72/g3/g32/g3/g20/g17/g27/g22
/g19/g20/g21/g22/g23/g24/g25/g26/g27/g28/g20/g19/g20/g20/g20/g21/g20/g22/g20/g23/g20/g24/g20/g25/g20/g26/g20/g27/g20/g28/g21/g19/g21/g20/g21/g21/g21/g22/g21/g23/g21/g24
Fig. 13. The ranks on recall for within-project defect prediction (CPDP)
methods with post-hoc Nemenyi test. Methods connected by gray lines arenot signiÔ¨Åcantly different.
This work is to investigate whether it is feasible to
removing overlapping instances to improve the defect
predicting performance of the CPDP learning models.In our experiments, we observe statistically signiÔ¨Åcance
results in terms of bal,Recall and AUC . However, we
observe the values of effect size of IKMCCA for allmodels are large, on the contrary, the values of effect
size of NCL and KMCCA for most CPDP models are
small. IKMCCA and NCL can achieve better resultsthan KMCCA for CPDP learning models. Therefore, it
is better to consider class imbalance when dealing withclass overlap and using IKMCCA to remove overlappinginstances before building the CPDP learning models.At the same time, IKMCCA can achieve better resultsthan NCL, it can not only consider the overlapping non-
defective instances when handling both class overlap and

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. /g49/g49/g11/g44/g46/g48/g12/g20/g20/g17/g22/g21/g28
/g49/g49/g11/g58/g76/g87/g75/g82/g88/g87/g12/g21/g19/g17/g19/g22/g55/g38/g36/g14/g11/g44/g46/g48/g12/g28/g17/g25/g28/g22
/g55/g38/g36/g14/g11/g58/g76/g87/g75/g82/g88/g87/g12/g21/g20/g17/g24/g19/g26/g39/g55/g37/g11/g49/g38/g47/g12/g27/g17/g26/g28/g20
/g39/g55/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g19/g17/g24/g25/g27/g57/g38/g37/g11/g44/g46/g48/g12/g27/g17/g26/g27
/g57/g38/g37/g11/g49/g38/g47/g12/g28/g17/g27/g22/g28/g57/g38/g37/g11/g46/g48/g12/g20/g20/g17/g28/g22
/g57/g38/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g21/g17/g24/g27/g25
/g49/g49/g11/g46/g48/g12/g20/g21/g17/g28/g28/g24/g3
/g55/g38/g36/g14/g11/g46/g48/g12/g20/g24/g17/g28/g22/g25/g3
/g48/g49/g37/g11/g49/g38/g47/g12/g20/g27/g17/g23/g20/g25/g3
/g48/g49/g37/g11/g46/g48/g12/g20/g27/g17/g24/g25/g20/g3
/g49/g49/g11/g49/g38/g47/g12/g20/g27/g17/g28/g24/g24
/g55/g38/g36/g14/g11/g49/g38/g47/g12/g20/g28/g17/g22/g24/g28/g3
/g48/g49/g37/g11/g58/g76/g87/g75/g82/g88/g87/g12/g20/g28/g17/g23/g26/g28/g43/g60/g39/g53/g36/g11/g58/g76/g87/g75/g82/g88/g87/g12/g27/g17/g23/g21/g28/g43/g60/g39/g53/g36/g11/g44/g46/g48/g12/g23/g17/g28/g27/g28/g3
/g39/g55/g37/g11/g44/g46/g48/g12/g25/g17/g22/g21/g22/g3
/g39/g55/g37/g11/g46/g48/g12/g26/g17/g24/g25/g27
/g43/g60/g39/g53/g36/g11/g49/g38/g47/g12/g26/g17/g26/g23/g20
/g43/g60/g39/g53/g36/g11/g46/g48/g12/g26/g17/g27/g23/g25/g3
/g48/g49/g37/g11/g44/g46/g48/g12/g27/g17/g22/g21/g38/g85/g76/g87/g76/g70/g68/g79/g3/g39/g76/g73/g73/g72/g85/g72/g81/g70/g72/g3/g32/g3/g20/g17/g27/g22
/g19/g20/g21/g22/g23/g24/g25/g26/g27/g28/g20/g19/g20/g20/g20/g21/g20/g22/g20/g23/g20/g24/g20/g25/g20/g26/g20/g27/g20/g28/g21/g19/g21/g20/g21/g21/g21/g22/g21/g23/g21/g24
Fig. 14. The ranks on AUC for within-project defect prediction (CPDP)
methods with post-hoc Nemenyi test. Methods connected by gray lines are
not signiÔ¨Åcantly different.
TABLE V
THE EFFECTIVENESS RESULTS OF WITHOUT REMOVING AGAINST EACH
COMPARED APPROACH FOR CPDP LEARNING MODELS IN TERMS OF bal,
Recall ,AND AUC .
Learning
modelremoving VS Without bal Recall AUC
NN-Ô¨ÅlterIKMCCA Vs. Without 0.88(L) 0.932(L) 0.747(L)
NCL Vs. Without 0.357(M) 0.146(N) 0.164(S)
KMCCA Vs. Without 0.753(L) 0.795(L) 0.503(L)
TCA+IKMCCA Vs. Without 1(L) 1(L) 0.857(L)
NCL Vs. Without 0.5(L) 0.535(L) 0.489(L)
KMCCA Vs. Without 0.642(L) 0.642(L) 0.535(L)
DTBIKMCCA Vs. Without 0.389(M) 0.387(M) 0.38(M)
NCL Vs. Without 0.174(S) 0.234(S) 0.234(S)
KMCCA Vs. Without 0.327(M) 0.333(M) 0.261(S)
VCBIKMCCA Vs. Without 0.327(S) 0.633(L) 0.296(S)
NCL Vs. Without 0.156(S) 0.011(N) 0.198(S)
KMCCA Vs. Without 0.009(N) 0.309(S) 0.0248(N)
MNBIKMCCA Vs. Without 0.897(L) 0.903(L) 0.867(L)
NCL Vs. Without 0.016(N) 0.093(N) 0.158(S)
KMCCA Vs. Without 0.046(N) 0.114(S) 0.138(S)
HYDRAIKMCCA Vs. Without 0.342(M) 0.333(M) 0.334(M)
NCL Vs. Without 0.083(N) 0.148(S) 0.023(N)
KMCCA Vs. Without 0.054(N) 0.061(N) 0.0262(N)
class imbalance for CPDP, and this is the same conclusion
as WPDP.
V. D ISCUSSION
In previous section, we observe that removing the over-
lapping instances including IKMCCA, NCL and KMCCA
can achieve statistically better than that without removingfor WPDP and CPDP learning models. As IKMCCA could
achieve better results than KMCCA and NCL. Therefore, we
will discuss the performance of IKMCCA in this section.A. Performance with different p of IKMCCA
In this experiment, we investigate the impact of the pa-
rameter pto IKMCCA, we change the value of it in the
range of {
1
2,1
3,¬∑¬∑¬∑,1
Œ±,1
Œ±+1,1
Œ±+2}, whereŒ±is the ratio of
all instances to defective instances in training data. To
avoid the randomness, we repeat 20 times and report the
average values of bal,Recall and AUC on PC4 project:
{CM1,MW 1,PC1,PC3}=>P C 4. Figures 15, 16 and
17 show the results for compared WPDP and CPDP learning
models on different p. Considering various factors, we can
observe that the parameter pbased on the Ô¨Åxed ratio of the
number of defective instances to all instances can obtain better
performance for most learning models.
/g20/g29/g21
/g20/g29/g22
/g20/g29/g23
/g20/g29/g24
/g20/g29/g25
/g20/g29/g26
/g20/g29/g27
/g20/g29/g28
/g20/g29/g20/g19
Fig. 15. balvalues on different pfor WPDP and CPDP learning models.
/g20/g29/g21
/g20/g29/g22
/g20/g29/g23
/g20/g29/g24
/g20/g29/g25
/g20/g29/g26
/g20/g29/g27
/g20/g29/g28
/g20/g29/g20/g19
Fig. 16. Recall values on different pfor WPDP and CPDP learning models.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. /s78/s66 /s83/s86/s77 /s82/s70 /s75/s78/s78 /s76/s82 /s78/s78 /s84/s67/s65/s43 /s68/s84/s66 /s86/s67/s66 /s77/s78/s66 /s72/s89/s68/s82/s65/s48/s46/s53/s48/s48/s46/s53/s53/s48/s46/s54/s48/s48/s46/s54/s53/s48/s46/s55/s48/s48/s46/s55/s53/s48/s46/s56/s48/s65/s85/s67
/s76/s101/s97/s114/s110/s105/s110/s103 /s109/s111/s100/s101/s108/s115/g20/g29/g21
/g20/g29/g22
/g20/g29/g23
/g20/g29/g24
/g20/g29/g25
/g20/g29/g26
/g20/g29/g27
/g20/g29/g28
/g20/g29/g20/g19
Fig. 17. AUC values on different pfor WPDP and CPDP learning models.
B. Effect of the parameter m for IKMCCA
In this experiment, we investigate the impact of the param-
eter mto IKMCCA, we change the value of it in the range
of{5,10,¬∑¬∑¬∑,35,40}. To avoid the randomness, we repeat 20
times and report the average values of bal,Recall and AUC
on PC4 project: {CM1,MW 1,PC1,PC3}=>PC 4. Noted
that performance pis Ô¨Åxed on the ratio of the number of
defective instances to all instances in this experiments. Figures
18, 19 and 20 show the results for compared WPDP and CPDPlearning models on different m. Considering various factors,
we can Ô¨Ånd that the parameter mÔ¨Åxed 20 could obtain better
results for most learning models.
/s78/s66 /s83/s86/s77 /s82/s70 /s75/s78/s78 /s76/s82 /s78/s78 /s84/s67/s65/s43 /s68/s84/s66 /s86/s67/s66 /s77/s78/s66 /s72/s89/s68/s82/s65/s48/s46/s51/s48/s46/s52/s48/s46/s53/s48/s46/s54/s48/s46/s55/s48/s46/s56EDO
/s76/s101/s97/s114/s110/s105/s110/s103 /s109/s111/s100/s101/s108/s115/g24
/g20/g19
/g20/g24
/g21/g19
/g21/g24
/g22/g19
/g22/g24
/g23/g19
Fig. 18. balvalues on different mfor WPDP and CPDP learning models./s78/s66 /s83/s86/s77 /s82/s70 /s75/s78/s78 /s76/s82 /s78/s78 /s84/s67/s65/s43 /s68/s84/s66 /s86/s67/s66 /s77/s78/s66 /s72/s89/s68/s82/s65/s48/s46/s48/s48/s46/s50/s48/s46/s52/s48/s46/s54/s48/s46/s565HFDOO
/s76/s101/s97/s114/s110/s105/s110/s103 /s109/s111/s100/s101/s108/s115/g24
/g20/g19
/g20/g24
/g21/g19
/g21/g24
/g22/g19
/g22/g24
/g23/g19
Fig. 19. Recall values on different mfor WPDP and CPDP learning models.
/s78/s66 /s83/s86/s77 /s82/s70 /s75/s78/s78 /s76/s82 /s78/s78 /s84/s67/s65/s43 /s68/s84/s66 /s86/s67/s66 /s77/s78/s66 /s72/s89/s68/s82/s65/s48/s46/s53/s48/s48/s46/s53/s53/s48/s46/s54/s48/s48/s46/s54/s53/s48/s46/s55/s48/s48/s46/s55/s53/s48/s46/s56/s48/s65/s85/s67
/s76/s101/s97/s114/s110/s105/s110/s103 /s109/s111/s100/s101/s108/s115/g24
/g20/g19
/g20/g24
/g21/g19
/g21/g24
/g22/g19
/g22/g24
/g23/g19
Fig. 20. AUC values on different mfor WPDP and CPDP learning models.
C. Why IKMCCA performs Best
The experimental results in above sections provide substan-
tial empirical support that removing overlapping instances can
improve the prediction performance. For all compared learningmodels, IKMCCA had a superior prediction performance
under WPDP and CPDP cases.
The performance of IKMCCA can be attributed to take
into account class imbalance factor in removing overlapping
instances. In IKMCCA, not only do we remove overlappinginstances in non-defective class, we also remove overlapping
instances in defective class. For example, if one cluster has
more than a certain proportion of non-defective instances,
the defective instances would be removed. To demonstratethis idea, we compare with NCL approach only considering

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. overlapping instances in non-defective instances (introduced
in section IV). Figures 3-14 show IKMCCA gain superiorperformance than NCL for learning models.
In addition, when IKMCCA removes overlapping instances,
the certain proportion pbased on the class imbalanced ratio
is taken into account. This is different from KMCCA only
removing top %poverlapping instances. Figures 3-14 indicate
considering the class imbalanced ratio could gain more adap-tation training datasets.
VI. T
HREATS TO V ALIDITY
A. Threats to construct validity
Our study in SDP is a binary classiÔ¨Åcation indicating
whether one module is defective. The datasets used in our
experiments are collected from open-source or proprietary
projects, and the defect matching is relied on SZZ algorithm
[40], the discovered defective modules may be incomplete,
which is a potential threat to construct validity. However, theseprojects are often used in SDP studies previously, we alsoassume that the defect matching criteria is the best extentpossible.
B. Threats to Internal validity
The WPDP and CPDP learning models that we select are
not all WPDP and CPDP studies. To minimize this threat, weselect the learning models based on the existing systematicliterature reviews that provided a comparative study to SDP
models. The selected WPDP and CPDP learning models are
the benchmark for SDP, therefore, this would minimize thisthreat in our work.
C. Threats to External validity
The 28 projects used in this study are NASA, ReLink,
SOFTLAB, AEEEM and MORPH which are from either open-source projects (ReLink, MORPH and AEEEM) or proprietaryprojects (NASA and SOFTLAB), and are different in metricset, scales and types. However, they may threaten the gener-alizable results of closed software projects. In future, we willinvestigate more commercial software projects to reduce thisthreat.
VII. C
ONCLUSION
Software defect prediction utilizes the learning models to
detect the defective modules in software project. Also, theperformance of these learning models depend on the quality
of training data. Previous studies in the data quality of SDPmainly focus on class imbalance and feature redundancy. How-ever, in practical data collection, some instances with differentlabel may have similar values on some metrics, which resultsin overlap in feature space. Inspired by this, we propose animproved K-Means clustering cleaning approach (IKMCCA)to remove the overlapping instances, and investigate whetherIKMCCA, KMCCA and NCL are feasible to improve defect
detection performance for WPDP and CPDP.
We conduct the experiments to compare the performance of
state-of-the-art learning models on 28 projects, and the resultsmake clear that using these removing overlapping instances
could obtain signiÔ¨Åcantly better performance in terms of bal,
Recall and AUC . That is to say, the overlapping instances affect
the defect detecting performance of WPDP and CPDP learning
models. The effect size of IKMCCA is large, and the effectsize of NCL and KMCCA are small or medium for WPDP and
CPDP models, so considering class imbalance when dealing
with class overlap may achieve better results, meanwhile it cannot only consider the overlapping non-defective instances.
In the further works, we will employ removing overlapping
methods to more software defect prediction models and some
commercial projects to evaluate the performance.
A
CKNOWLEDGMENT
The authors would like to thank the anonymous reviewers
and editors for suggesting improvements and for their veryhelpful comments. This work is supported by the National Nat-ural Science Foundation of China under grant No. 61673384,the Natural Science Foundation of Jiangsu Province undergrant No. K20181353, Postgraduate Research & PracticeInnovation Program of Jiangsu Province under grant No.KYCX19
2168 and Postgraduate Research & Practice Innova-
tion Program of China University of Mining and Technologyunder grant No.KYCX19
2168.
REFERENCES
[1] H. Zhimin, P. Fayola, M. T, and Y . Ye, ‚ÄúLearning from open-source
projects: An empirical study on defect prediction,‚Äù in Proc. IEEE
International Conference on Empirical Software Engineering and Mea-
suremen(ESEM‚Äô2013) , Baltimore, MD, USA, Oct. 2013.
[2] H. Zhimin, S. Fengdi, Y . Ye, L. Mingshu, and W. Qing, ‚ÄúAn investigation
on the feasibility of cross-project defect prediction,‚Äù Automated Software
Engineering , vol. 19, no. 2, pp. 167‚Äì199, 2012.
[3] L. Ming, Z. Hongyu, W. Rongxin, and Z. Zhihua, ‚ÄúSample-based
software defect prediction with active and semi-supervised learning,‚Äù
Automated Software Engineering , vol. 19, no. 2, pp. 201‚Äì230, 2012.
[4] M. Ying, L. Guangchun, Z. Xue, and C. Aiguo, ‚ÄúTransfer learning for
cross-company software defect prediction,‚Äù Information and Software
Technology , vol. 54, no. 3, pp. 248‚Äì256, 2012.
[5] T. Menzies, A. Butcher, A. Marcus, T. Zimmermann, and D. Cok, ‚ÄúLocal
vs. global models for effort estimation and defect prediction,‚Äù in Proc.
International Conference on Automated Software Engineering (ASE) ,
Lawrence KS, Nov. 2011.
[6] J. Nam, Sinno, J. Pan, and S. Kim, ‚ÄúTransfer defect learning,‚Äù ser. Proc.
International Conference on Software Engineering (ICSE 2013), San
Francisco, November 06‚Äì10, 2011.
[7] B. Turhan, T. Menzies, B. Ayse, and D. Justin, ‚ÄúOn the relative
value of cross-company and within-company data for defect prediction,‚Äù
Empirical Software Engineering , vol. 14, no. 5, pp. 540‚Äì578, 2009.
[8] S. Watanabe, H. Kaiya, and K. Kaijiri, ‚ÄúAdapting a fault prediction
model to allow inter language reuse,‚Äù in Proc. International Conference
on Software Engineering (ICSE 2008) , Leipzig, May 2008.
[9] X. Chen, Y . Zhao, Q. Wang, and Z. Yuan, ‚ÄúMulti: Multi-objective effort-
aware just-in-time software defect prediction,‚Äù Information and Software
Technology , vol. 93, no. 5, pp. 1‚Äì13, 2018.
[10] T. Menzies, A. Dekhtyar, and J. Distefano, ‚ÄúProblems with precision: a
response to comments on.‚Äù
[11] G. Blanchard and R. Loubere, ‚ÄúHigh-order conservative remapping
with a posteriori MOOD stabilization on polygonal meshes,‚Äù 2015,https://hal.archives-ouvertes.fr/hal-01207156, the HAL Open Archive,
hal-01207156. Accessed January 13, 2016.
[12] M. Shepperd, Q. Song, and Z. Sun, ‚ÄúData quality: some comments
on the nasa software defect datasets,‚Äù IEEE Transactions on Software
Engineering , vol. 39, no. 9, pp. 1208‚Äì1215, 2013.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. [13] D. Marco, L. Michele, and R. Romain, ‚ÄúEvaluating defect prediction ap-
proaches: a benchmark and an extensive comparison,‚Äù Commun Comput
Phys , vol. 17, no. 4‚Äì5, pp. 531‚Äì577, 2012.
[14] R. Wu, H. Zhang, S. Kim, and S. Cheung, ‚ÄúRelink: recovering links
between bugs and changes,‚Äù in Proc. International Conference on the
F oundations of Software Engineering and European Software Engineer-
ing (SIGSOFT/FSE 2011) , Szeged Hungary, Sep. 2011.
[15] J. Nam and S. Kim, ‚ÄúClami: Defect prediction on unlabeled datasets,‚Äù
inProc. IEEE International Conference on Automated Software Engi-
neering (ASE‚Äô2015) , Lincoln, NE, USA, Nov. 2015.
[16] M. Shepperd, D. Bowes, and T. Hall, ‚ÄúResearcher bias: the use of
machine learning in software defect prediction,‚Äù IEEE Transactions on
Software Engineering. , vol. 40, no. 6, pp. 603‚Äì616, 2014.
[17] Z. Zheng, X. Wu, and R. Srihari, ‚ÄúFeature selection for text catego-
rization on imbalanced data,‚Äù ACM SIGKDD Explorations Newsletter . ,
vol. 6, no. 1, pp. 80‚Äì89, 2004.
[18] L. Chen, B. Fang, Z. Shang, and Y . Tang, ‚ÄúTackling class overlap and
imbalance problems in software defect prediction,‚Äù Software Quality
Journal. , vol. 26, no. 01, pp. 97‚Äì125, 2018.
[19] X. Jing, S. Ying, S. Wu, and J. Liu, ‚ÄúDictionary learning based software
defect prediction,‚Äù in Proc. IEEE International Conference on Software
Engineering(ICSE‚Äô2014) , Hyderabad, India, May 2014, pp. 414‚Äì423.
[20] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y . Jiang, and A. Bener,
‚ÄúDefect prediction from static code features: current results, limitations,
new approaches,‚Äù Automated Software Engineering. , vol. 17, no. 04, pp.
375‚Äì407, 2010.
[21] D. Ibrahim, R. Ghnemat, and A. Hudaib, ‚ÄúSoftware defect prediction
using feature selection and random forest algorithm,‚Äù in International
Conference on New Trends in Computing Sciences (ICTCS) , Amman,
Jordan, October 2017, pp. 252‚Äì257.
[22] L. Hribar and D. Duka, ‚ÄúSoftware component quality prediction using
knn andfuzzy logic,‚Äù Information and Software Technology. , vol. 58,
no. 2, pp. 388‚Äì402, 2010.
[23] L. Miao, M. Liu, and D. ZHang, ‚ÄúCost-sensitive feature selection with
application in software defect prediction,‚Äù in Proc. IEEE International
Conference on Pattern Recognition(ICPR‚Äô2012) , ICPR, Nov. 2012, pp.
967‚Äì970.
[24] R. Malhotra, ‚ÄúA systematic review of machine learning techniques
for software fault prediction,‚Äù Information and Software Technology. ,
vol. 27, no. 0, pp. 504‚Äì518, 2015.
[25] A. Panichella, R. Oliveto, and L. AD, ‚ÄúCross-project defect pre-
diction models: L‚Äôunion fait la force,‚Äù in Proc. IEEE International
Conference on Software Maintenance, Reengineering, and Reverse
Engineering(CSMR-WCRE‚Äô2014) , Antwerp, Belgium, Feb. 2014, pp.
164‚Äì173.
[26] S. Herbold and J. Grabowski, ‚ÄúA comparative study to benchmark cross-
project defect prediction approaches,‚Äù IEEE Transactions on Software
Engineering. , vol. 44, no. 09, pp. 811‚Äì833, 2018.
[27] Y . Zhou, Y . Yang, H. Lu, L. Chen, Y . Li, and Y . Zhao, ‚ÄúHow far
we have progressed in the journey? an examination of cross-project
defect prediction,‚Äù ACM Transactions on Software Engineering and
Methodology. , vol. 27, no. 01, pp. 1‚Äì51, 2018.
[28] B. Turhan, A. Bener, and J. Stefano, ‚ÄúOn the relative value of cross-
company and within-company data for defect prediction,‚Äù Empirical
Software Engineering. , vol. 14, no. 05, pp. 540‚Äì578, 2009.
[29] L. Chen, B. Fang, Z. Shang, and Y . Tang, ‚ÄúNegative samples reduction in
cross-company software defects prediction,‚Äù Information and Software
Technology. , vol. 62, no. 01, pp. 67‚Äì77, 2015.
[30] B. Turhan, A. Misirli, and A. Bener, ‚ÄúEmpirical evaluation of the effects
of mixed project data on learning defect predictors,‚Äù Information and
Software Technology. , vol. 55, no. 06, pp. 1101‚Äì1118, 2013.
[31] D. Ryu and J. Baik, ‚ÄúValue-cognitive boosting with a support vector
machine for cross-project defect prediction,‚Äù Empirical Software Engi-
neering. , vol. 21, no. 01, pp. 43‚Äì71, 2016.
[32] X. Xia, D. Lo, S. Pan, and N. Nagappan, ‚ÄúHydra: Massively composi-
tional model for cross-project defect prediction,‚Äù IEEE Transactions on
Software Engineering. , vol. 42, no. 10, pp. 977‚Äì998, 2016.
[33] E. Kocaguneli, T. Menzies, A. Bener, and J. Keung, ‚ÄúExploiting the
essential assumptions of analogy-based effort estimation,‚Äù IEEE Trans-
actions on Software Engineering. , vol. 38, no. 2, pp. 425‚Äì438, 2012.
[34] T. Tang, W amd Khoshhgoftaar, ‚ÄúNoise identiÔ¨Åcation with the k-
means algorithm,‚Äù in Proc. IEEE International Conference on Tools with
ArtiÔ¨Åcial Intelligence(ICTAI ‚Äô2004) , Boca Raton, FL, USA, USA, Nov.
2004, pp. 373‚Äì378.[35] S. Kim, T. Zimmemann, and A. Zeller, ‚ÄúDealing with noise in defect
prediction,‚Äù in Proc. IEEE International Conference on Software Engi-
neering(ICSE‚Äô2011) , Honolulu, HI, USA, May 2011, pp. 481‚Äì490.
[36] F. Peters, T. Menzies, and A. Marcus, ‚ÄúBetter cross company defect
prediction,‚Äù in Proc. IEEE International Conference on Mining Software
Repositories(MSR‚Äô2013) , San Francisco, CA, USA, May 2013, pp. 409‚Äì
418.
[37] J. Sliwerski, T. Zimmermann, and A. Zeller, ‚ÄúWhen do changes induce
Ô¨Åxes?‚Äù in Proc. ACM International Conference on the 15th Annual Con-
ference on Mining software repositories(MSR‚Äô2005) , Louis, Missouri,
May 2005, pp. 1‚Äì5.
[38] J. A, Hanley, B. J, and McNeil, ‚ÄúThe meaning and use of the area
under a receiver operating characteristic (roc) curve,‚Äù Radiology. , vol.
143, no. 01, pp. 29‚Äì36, 1982.
[39] D. Marco, L. Michele, and R. Romain, ‚ÄúEvaluating defect prediction ap-
proaches: a benchmark and an extensive comparison,‚Äù Commun Comput
Phys , vol. 17, no. 4‚Äì5, pp. 531‚Äì577, 2012.
[40] H. Steffen, T. Alexander, and G. Jens, ‚ÄúA comparative study to bench-
mark cross-project defect prediction approaches,‚Äù IEEE Transactions on
Software Engineering , vol. 44, no. 9, pp. 811‚Äì833, 2018.
[41] G. Macbeth, E. Razumiejczyk, and R. Ledesma, ‚ÄúCliff‚Äôs delta calculator:
A non-parametric effect size program for two groups of observations,‚ÄùUniversitas Psychological , vol. 10, no. 2, pp. 545‚Äì555, 2011.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 