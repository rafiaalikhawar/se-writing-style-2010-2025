PRADA: Prioritizing Android Devices for Apps
by Mining Large-Scale Usage Data
Xuan Lu1Xuanzhe Liu1‡Huoran Li1T ao Xie2Qiaozhu Mei3Dan Hao1Gang Huang1Feng Feng4
1Key Laboratory of High Conﬁdence Software T echnologies (Peking University), Ministry of Education, PRC
2University of Illinois at Urbana-Champaign,3University of Michigan,4Wandoujia Lab, Beijing, China
{luxuan, xzl, lihuoran}@pku.edu.cn, taoxie@illinois.edu
qmei@umich.edu, {haodan, hg}@pku.edu.cn, jackfeng@wandoujia.com
ABSTRACT
Selecting and prioritizing major device models are critical
for mobile app developers to select testbeds and optimize
resourcessuchasmarketingandquality-assuranceresources.
TheheavilyfragmenteddistributionofAndroiddevicesmakesit challenging to select a few major device models out ofthousands of models available on the market. Currentlyapp developers usually rely on some reported or estimated
general market share of device models. However, these es-
timates can be quite inaccurate, and more problematically,
can be irrelevant to the particular app under consideration.
To address this issue, we propose PRADA, the ﬁrst ap-
proach to prioritizing Android device models for individ-
ualapps, based on mining large-scale usage data. PRADA
adapts the concept of operational proﬁling (popularly usedin software reliability engineering) for mobile apps – the us-age of an app on a speciﬁc device model reﬂects the impor-tance of that device model for the app. PRADA includes acollaborative ﬁltering technique to predict the usage of anapp on diﬀerent device models, even if the app is entirelynew (without its actual usage in the market yet), based onthe usage data of a large collection of apps. We empiricallydemonstrate the eﬀectiveness of PRADA over two popularapp categories, i.e., GameandMedia,c o v e r i n go v e r3 . 8 6
million users and 14,000 device models collected through aleading Android management app in China.
Categories and Subject Descriptors
D.2.9 [Management]: Software quality assurance (SQA)
Keywords
Mobile apps, Android fragmentation, prioritization, usagedata
‡Xuan Lu and Xuanzhe Liu are both ﬁrst authors, and con-
tributed equally to the work.
§Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others thanACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’16, May 14-22, 2016, Austin, TX, USA
c/circlecopyrt2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.28848281. INTRODUCTION
The wide adoption of smartphones and tablet comput-
ers has triggered a surge of developing mobile applications,
a.k.a, apps, in recent years. As of 2015, millions of apps havebeendevelopedandmadeavailableinappmarketplacessuchas the Apple Store and Google Play, which have received bil-lions of downloads. Numerous app developers have proﬁtedfrom the revenues generated by the downloads and usagesof their apps.
Compared to the iOS and Windows platforms, which have
a rather ﬁxed set of device models
1, the Android platform is
adopted by a diverse set of device manufacturers and mod-els. Indeed, the openness and ﬂexibility of the Android plat-form have greatly contributed to its popularity: the Androidplatform holds more than 80% of the smartphone market
share. Famous brands, such as Samsung, HTC, Motorola,
and Lenovo, have developed numerous device models usingAndroid. Meanwhile, most small and medium device man-
ufacturers also adopt Android as their platforms.
The heavily fragmented distribution of Android device
models is noticeable. It is reported by OpenSignal [6] that
there have been more than 20,000 Android device models on
the market up to the year 2014. Such a fragmentation brings
signiﬁcant challenges to software engineering practices formobile apps, such as the design, development, maintenance,quality assurance, and revenue generation [20, 31]. A recentstudy [1] (in 2013) shows that 94% of app developers who
avoid the Android platform cited fragmentation as the main
reason. Developers have to take into account device-speciﬁc
factorssuchasscreensizes, resolutionlevels, andotherhard-
ware speciﬁcations. An app (especially a game app) can runsmoothly on high-end device models, which have powerfulcomputation power and high resolution, but may run slug-gishly or improperly on lower-end devices. Developers needto conduct extensive testing and other quality assurance ac-tivities to validate the functionality and usability (such asthe GUI eﬀects) of their apps on multiple device models.The fragmentation also inﬂuences the revenue, of which in-app advertisement (or ads) is an important channel, espe-
cially for some types of apps (e.g., game and media apps).To more accurately target the audience, device-speciﬁc adsare often preferred. For example, Facebook customizes mo-bile ads according to device model types [4] since 2014. De-
velopers would like to know through which device models
they can gain more users and more ads-clicking opportuni-
ties, so that they can invest their eﬀort in customizing the
1A device model refers to the speciﬁc model of devices that
share the same hardware speciﬁcation.
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   3
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   3
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   3
ads on those models, e.g., designing banners of proper sizes
or placing videos at proper positions on the screen. As itis unrealistic to customize for every Android device model,selecting the major device models to focus on is quite impor-
tant to Android app development. A suboptimal selection
may cause to waste money and human eﬀort, miss potential
bugs, or even lose revenue, etc.
Most developers prioritize a small number of device mod-
els (normally less than ten) based on their market shares.These developers rely on the widely accessible market sharereportsorpredictions, suchasthoseprovidedbyAppBrain[2].
However, the market share of a device model is usually
calculated based on how many devices are sold instead of
how they are used, which is what the developers really careabout. More importantly, the market share of a devicemodel may not be relevant to particular apps. Indeed, itis not uncommon that an app is heavily installed on lesspopular device models and less preferred by those who use
a majority model. Even if an app is installed on a device,
it may be frequently or barely used [32]. To make the right
decision, the developers need an accurate estimate of how
their apps are actually used on diﬀerent device models.
To address this issue, we propose a novel approach, named
PRADA (Prioritizing Android Devices for Apps), to pri-
oritizing major device models of a given app. Rather than
counting the number of devices installing an app, PRADA
utilizes a diﬀerent signal – how the app is actually used onthedevices. ThekeyintuitionofPRADAisderivedfromtheconcept of operational proﬁle [28], a concept popularly used
in software reliability engineering, which is a quantitative
representation of how a system is used. PRADA assumes
that a device model is of a higher priority for an app if the
app is consumed more intensively by the users using that de-
vice model. PRADA then builds a data mining model thataccurately predicts the major device models for every app,even if it is newly launched on the market and the actual us-age data is unavailable. Based on the predictions, PRADArecommends a ranked list of device models that should beprioritized for an app.
We evaluate the eﬀectiveness of PRADA through a real
world data set collected by a leading Android app market-place in China, Wandoujia [9]. Wandoujia provides its na-tive management app to facilitate searching, downloading,and updating apps. In addition, the management app alsoprovides an interface to monitor the daily usage of the appsinstalled on a device. We choose the top 100 popular apps in
two app categories, i.e., GameandMedia. These apps (100
apps from each category) cover 3,861,444 users and 14,709
devicemodels. Weadopt browsing time asthetypicalmetric
of usage data, as it indicates the total time that users in-
teract with a speciﬁc app under network. We then evaluatethe signiﬁcance of PRADA-selected device models account-ing for the actual browsing time. Empirical results show
that, compared to the baseline of marketshare, PRADA se-
lects device models with top browsing time, i.e. device mod-
els that can maximize the coverage of browsing time,m o r e
accurately.
In summary, we make the following main contributions:
•We propose the ﬁrst approach to prioritizing Androiddevice models by mining app usage data collected fromAndroid devices.
•Wedevelopacollaborativeﬁlteringtechniquethatpre-dicts major device models for a newapp based on the
usage of other apps with similar functionalities.•We present the largest study to date on prioritizingdevice models using a real-world data set.
•We conduct experiments to evaluate the eﬀectivenessof PRADA and its related approaches.
The rest of this paper is organized as follows. Section 2
describes the related work of Android device prioritization.
Section 3 presents the overview of PRADA including its key
ideas and workﬂows. Section 4 illustrates PRADA throughan example case and Section 5 presents an evaluation. Sec-tion 6 presents the ﬁndings and implications for developers.Section 7 discusses threats to validity of our study. Section
8 concludes the paper with future work.
2. RELATED WORK
Wenextpresenta summaryof existing studiesonAndroid
device fragmentation, operational proﬁles, and analyses of
app usage data.
2.1 Android Fragmentation
Compared to software development for PC, one unique
challenge of developing Android apps is how to cope withthe heavy fragmentation of Android devices. Halpern et
al.[19] make a careful analysis of the market fragmenta-
tion caused by hardware speciﬁcations and OS versions, andpropose a capture-and-replay approach for testing. Park et
al. [31] propose two approaches to handle Android fragmen-tation at the code level and the API level. Han et al. [20]
analyze the bug reports related to the two popular mobile-
device vendors, HTCandMotorola, and propose an approach
for tracking fragmentation using feature analysis on project
repositories. Khalid et al. [22] leverage user reviews to fo-
cus on proper Android devices for app testing. They collect
about 100 thousand user reviews of 99 free game apps onGoogle Play. However, Google Play has recently shut downthe API of browsing user reviews for all device models, ex-
cept for the user reviews from speciﬁc device models asso-
ciated with a user’s Google account; such constraint mayintroduce a considerable bias when leveraging user reviews.Another concern is the potential subjectiveness and biases
in user reviews. For example, it is reported that users from
diﬀerent countries may behave very diﬀerently in writingreviews [25].
Some existing industrial cloud-based testing services, such
as Testin [8] and AppthWack [3], provide remote servicesand oﬀer a suﬃcient coverage of device models. However,the cost ranges from approximately one dollar per every 15-minute use of a device, leading to a very high expense if onewants to test many device models.
2.2 Operational Proﬁles
The concept of operational proﬁle is widely used in soft-
ware engineering, especially software reliability engineeringand software testing [11, 16]. Musa [28] deﬁned an opera-tional proﬁle as “a quantitative representation of howas y s t e mw i l lb eu s e d ”. It models how users execute a
system, speciﬁcally the occurrence probabilities of functioncalls and the distributions of parameter values. Such a de-scription of the user behavior can be used to generate testcases and to direct testing to the most used functions. For
example, with a software system, if operation A occurs in
60 percent of the time, B occurs in 30 percent, and C oc-curs in 10 percent, then the proﬁle is [A, 0.6...B, 0.3...C,0.1]. Descriptions of the user behavior as in an operational
4
4
4proﬁle can also be used for other purposes besides software
testing [10]. The performance and correctness of the systemcan be analyzed, and the system can be eﬀectively adaptedto speciﬁc user groups. For example, Mobahser et al.[27]
use operational proﬁles of Web-based systems for personal-
ization.
An operational proﬁle helps improve the communication
between customers and developers, and make the developers
think more deeply about the features of interest and theirimportance to the customers. In contrast, if developed early,an operational proﬁle may be used to prioritize operations
under development, so that more resources are invested on
more important operations.
2.3 App Usage Data Analysis
From the perspective of software engineering, usage data
of apps is a typical operational proﬁle. Understanding how
an app is used by real-world users is also important to im-prove the development of the app. One common approach istoconductaﬁeldstudywithtypicallyasmallgroupofusers,as proposed in LiveLab [39, 34, 33]. Many other studies have
been conducted in a similar fashion, for reporting the diver-sity of cellular usage from diﬀerent user groups, diﬀerentapp categories, etc. [21, 15, 12, 13, 17]. To collect usagedata, some researchers develop third-party apps and deploy
them as system-level services, such as AppInsight [35] and
AppJoy [41]. The app usage data can include detailed infor-mation such as the total launch time, the traﬃc volume, and
the session lengths to investigate user preferences and inter-
ests in depth. However, these apps are not widely adoptedand their users are usually at the scale of hundreds. Com-paredtotheseﬁeldstudies, ourapproachreliesonacommer-cial app, Wandoujia, which has been widely deployed amongover 250 million users. Although most Wandoujia users are
from China, our recent work [24] conducts a series of studies
of understanding user behaviors from multiple dimensions
such as app popularity, network usage, and price sensitiv-ity, and validates the general ﬁndings from previous eﬀortsmade over other popular app stores [32] and the traces attier-1 cellular network in US [40]. The results show that con-ducting studies using Wandoujia data can reduce the threatscaused by user selection bias. In addition, such a longitudi-
nal data set collected from about 4 million anonymized users
can make many more comprehensive analyses feasible.
3. THE PRADA APPROACH
In this section, we present the PRADA approach of prior-
itizing device models by using large-scale usage data. The
key idea of PRADA is that, a device model should be given
higher priority for an app, if the device model accounts for
more user activities. Naturally, when an app is already used
by many users on many devices, estimating the length oftime it is used or the number of user interactions is an easytask. We are motivated, however, to explore whether we can
make accurate estimates for a new app, which has not yet
reached a critical mass of users or even has not yet released.Market targeting and prioritized testing at this stage are
particularly important and challenging for the developers.
PRADA relies on the usage data of apps on speciﬁc devicemodels collected by Wandoujia.
3.1 Wandoujia
The app usage data are collected through a commercial
Android app management tool developed by a leading An-
Figure 1: Screenshots of advanced settings in the Chinese
version of the Wandoujia management app (the advanced
settings are not supported in the current English version).(a) is the homepage of the Wandoujia management app,w h e r eu s e r sc a nn a v i g a t et o“ settings”b yc l i c k i n go nt h e
text circled by red; (b) refers to the setting of background
management services, highlighted by the red rectangle; (c)
refers to the option of allowing Wandoujia to collect the dataof network activities or not.
droid marketplace in China, called the Wandoujia. Wan-
doujia was founded in 2009 and has grown into one of the
largest Android app marketplaces in the world, with over
250 million users and 1.5 million free Android apps
2as of
the year 2015. Each user is associated with at least one
Android device, either a smartphone or a tablet computer.
Wandoujia provides a native management app, through
which users can manage the apps on their devices, e.g.,downloading, searching, updating, and uninstalling apps.
Users can also rate/review apps. Beyond these basic fea-
tures, the Wandoujia management app is developed withsomeoptionalfeaturesthatcanmonitorandoptimizesystem-wideactivities. Thesefeaturesincludenetworkactivitystatis-
tics, permission monitoring, content recommendation, etc.
All features are developed upon Android system APIs anddo not require the“root”privilege. Users can opt in and out
these features. For example, as shown in Figure 1, tracking
the network statistics is an explicit option for end users, andtherefore our analysis is made on only those users who agreeto share and upload their usage data. However, it should benoted that these features are supported in only the Chinese
version of Wandoujia.
We obtain three months of app usage data collected from
July 1st, 2014 to September 30th, 2014. The data cover4,775,293 unique users, 16,602 device models, and 238,231apps. Every device has a unique IMEIidentiﬁer, and the
corresponding information of the device model is also cap-tured.•User Privacy Protection. We take a series of steps
to preserve the privacy of involved users in our data set.First, all raw data collected for this study are kept on theWandoujiadata-warehouseservers(whichlivebehindacom-pany ﬁrewall). Second, our data-collection logic and analy-
2Most apps released on Wandoujia can also be found on
other app stores such as Google Play.
5
5
5Figure 2: Overview of the PRADA Approach.
sis pipelines are governed by Wandoujia employees3to en-
sure compliance with the commitments of Wandoujia pri-
vacy stated in the Term-of-Use statements. Third and the
most signiﬁcantly, Wandoujia employees anonymize the useridentiﬁers before any data analysis. Only aggregated statis-tics are produced for the users covered in our study period.
Finally, we obtain the approval from the research ethnics
committee of School of Electronics Engineering and Com-
puter Science in Peking University to conduct this research.
3.2 PRADA: in a Nutshell
Based on the collected data, PRADA aims to recommend
the device models that should be prioritized for a new app,
which has not been used by many users and therefore its
usage data is either unavailable or untrustable. The basicidea is to predict the“expected”usage of this new app basedon the characteristics of the device models of existing apps
that are similarto the new app. We assume that the usage
data of this new app cannot be attained or is not informativeto use. We describe the key components and workﬂows of
PRADA, as illustrated in Figure 2.
•Usage-Data Collection. PRADA is general by lever-
aging various usage data from an operational proﬁle. Cur-
rently, PRADA employs the Wandoujia management app,
which provides the interface for collecting multiple types ofusage data of an app on a device, such as the lengths ofin-app network sessions, the volume of network traﬃc, the
energy drain, and the user reviews. Developers can choose
one or more types of usage statistics of their interest as fea-tures for prioritizing device models. For example, some de-velopers may want to know which device models accountfor longer time of network usage and design device-speciﬁc
ads to target that audience; some developers may distribute
moretestingeﬀortsondevicemodelsthatcontributetomore
negative user reviews, etc.
•Similar-App Selection. PRADA employs the idea of
collaborative ﬁltering : for an app whose major device mod-
els need to be predicted, PRADA relies on the usage datafrom a set of existing similar apps. In practice, PRADA
is compatible to diﬀerent types of app similarity measure-
ment [14], e.g., the name, textual description, code, library,
and category. In this paper, we adopt the category of Wan-
3The last author is the co-founder and CTO of Wandou-
jia. He supervises the process of data collection and de-
identiﬁcation.doujia’s classiﬁcation system4. For each new app, we choose
some existing apps with usage data from the same category
and perform the following steps. Although the category may
be conceptually coarse-grained, using the category can help
achieve suﬃcient eﬀectiveness as shown in our evaluation.
•Device-Model Clustering . Given the selected type(s)
of usage statistics, PRADA conducts an oﬄine analysis of
the device model distribution of each app from the selectedsimilar apps. In this step, PRADA summarizes all devicesthat have ever produced the selected usage data of the app,
andclustersthemaccordingtotheirdevicemodels(speciﬁed
in their identiﬁers). For example, assuming that there are
1,000Samsung Galaxy S3 smartphones using a speciﬁc app,
weaggregatetogethertheselectedusagedatageneratedover
them.•Device-Model Prioritization. As the core component
of PRADA, device-model prioritization produces a ranked
list of device models based on predicting the selected type
of usage statistics. To prioritize the device models, PRADAis designed based on the following two rationales.
•Pareto Distribution. It is well known that in many
situations, a larger portion of eﬀects comes from asmaller percentage of the causes (or roughly 80%-20%
rule), known as the Pareto distribution [30]. We hy-
pothesize that a similar distribution is still valid inthe distribution of usage data contributed by devicemodels. If such a hypothesis holds, we can remove alarge portion of device models to signiﬁcantly reducethe space of prioritization.
•Collective Intelligence. Thesecondrationaleisthat
similar apps may share similar distributions of devicemodels. Hence, we can leverage the knowledge de-
rived from a large number of existing similar apps to
predict the to-be-prioritized device models for a new
app. This rationale is shared by collaborative ﬁltering
[36], which is a major approach underlying most rec-ommender systems [37]. Various collaborative ﬁlteringtechniques adopt diﬀerent notions of“similarity”.
Basedontheprecedingtworationales, thegoalofPRADA
is to recommend a small set of device models that accountfor a desired coverage of usage data of a new app, given bythe distribution of usage data of existing similar apps.
3.3 Effectiveness Metrics
Before presenting the details of PRADA, we introduce
some metrics to evaluate the eﬀectiveness of any concretealgorithm that PRADA incorporates.
The ﬁrst metric is the Device Model Hit, which is the
number of recommended device models (by PRADA for anapp) that are“actually”among the top device models of theapp (observed after the app is deployed and used by users).•Deﬁnition 1: Given the number of device models (de-
noted as N) to be recommended and the number of the
existing similar apps (denoted as K-1), the Device Model
Hitis the size of the overlap between the recommended N
device models {D
/prime
1,D/prime
2,....,D/prime
N}and the actual top Ndevice
models{D1,D2,....,D N}. Formally, we can deﬁne
Device Model Hit (N,K)=|DNrecommended/intersectiondisplay
DNactual|
(1)
4Wandoujia’s classiﬁcation system is based on criteria in-
cluding the developer’s annotation, textual description, and
some code-level analytics. The details of the classiﬁcationsystem is out of the scope of this paper.
6
6
6Supposetheactualtop5devicemodelsare {D1,D2,D3,D4,D5}.
IfPRADArecommends {D2,D3,D1,D5,D6},th edevice model
hitis 4.
TheDevice Model Hit measures how many recommended
device models are valid, but does not distinguish among
these recommended items. In practice, the ranking of the
recommendations is usually important so that the develop-ers can prioritize on any number of device models based ontheir budget. We use the metric of Average Precision (ab-
breviated as APi nt h er e s to ft h i sp a p e r )t oe v a l u a t et h e
eﬀectiveness of the ranking of device models; such metric
h a sb e e nw i d e l yu s e di ne v a l u a t i n gs e a r c he n g i n e s[ 2 6 ] .
•Deﬁnition 2: Average Precision (AP) of the selected
Ndevice models {D
/prime
1,D/prime
2,....,D/prime
N}is the average precision
ofD/prime
i(1/lessorequalslanti/lessorequalslantN) against the actual top Ndevice models
selectedfortheirtoprankinginthecontributionoftheusage
data:
AP=N/summationdisplay
i=1Precision(D/prime
i)/N (2)
Precision (D/prime
i) denotes the precision at cut-oﬀ position i
where the device model D/prime
istays in the ranked device model
list.Precision (D/prime
i) is equal to 0 when D/prime
iis not found in
the actual top Ndevice model list.
We take a simple example to illustrate AP. Suppose that
the actual top 5 device models (the ground truth) with themost usage data are {D
1,D2,D3,D4,D5}, where each de-
vice model is ordered according to its contribution to us-age data. When the set of ground truth is ﬁxed, the or-
der of the device models is no longer concerned. Assume
that the top 5 device models recommended by PRADA are{D
1,D2,D3,D4,D6}, then the APis computed as ((1 /1+
2/2+3/3+4/4+0))/5=0.8.
Note that two sets of device models that have the same
Device Model Hit m a yv a r ym u c hi nt e r m so fAP. Consider
another list of device models {D6,D1,D2,D3,D4},w h o s e
Device Model Hit is still 4. However, its APis ((0+1 /2+
2/3+3/4+4/5))/5=0.54.
The third metric that we use is the Usage Data Coverage,
which measures how much the recommended device modelscover the entire set of usage by the actual top Ndevices
of the app. Such a metric can reﬂect how much the rec-
ommended device models can contribute to the usage data
of interest, and therefore has an indication of the potential
opportunity of revenue.•Deﬁnition 3: Usage Data Coverage is the percent-
age of aggregated usage statistics of recommended Ndevice
models over the aggregated usage statistics of actual top N
device models. Formally, we can deﬁne
UsageDataCoverage( N,K)=Usage Data( D
Nrecommended )
Usage Data( DNactual)
(3)
AsuccessfulinstantiationofPRADAisexpectedtoachieve
high scores of the three metrics. Next we illustrate how to
use PRADA to prioritize device models.
4. TIME-SHARE DRIVEN PRIORITIZATION
In this section, we illustrate PRADA through an example
case focusing on the in-app browsing time collected fromWandoujia while using the category to determine similar
apps.4.1 Browsing Time on an App
As shown in Figure 1, the Wandoujia management app
provides a system-wide service for recording daily network
activity statistics of each app, for both Wi-Fi and cellular(2G/3G/LTE). The Wandoujia management app does not
record the details of each interaction session. Instead, itrecords the total daily access time generated from both Wi-Fi and cellular network, by aggregating the time across TCPﬂowsgeneratedbyanapp. TheWandoujiamanagementapp
treats the network access time generated from foreground
and background, respectively.
Foreground access time is computed only when a user
browses an app, i.e., the app is currently active on screen.We can roughly measure how long a user is really “online”
when she interacts with the app , by aggregating the fore-
ground access time of Wi-Fi and cellular. We call such
aggregated time as “ browsing time ” (unless stated other-
wise, we exchangeably use“time”and“browsing time”in the
rest of this paper). Such a type of operational proﬁle could
be a useful indicator for app developers. For example, in-app advertisement is an important revenue for mobile appdevelopers [18]. Furthermore, as suggested by Facebook [4],it becomes popular to customize device-speciﬁc ads [29]. A
common rationale used by online advertisement is that the
longer a user stays on the site, the more probably she willclick through the ads. Although the browsing time cannotcapture the oﬄine usage of an app, such metric is valuable,
as apps with online app usage are increasingly widespread
and important.Deﬁnition 4: Formally, we deﬁne the browsing time for an
appAcontributed by device model Das follows:
Time(D→A)=/summationdisplay
Time(d
i→A)( 4 )
Here,Ddenotesaspeciﬁcdevicemodeland diisaparticular
device that belongs to device model D,i . e . ,d i∈D.
To reﬂect the importance of device models that account
for browsing time, we introduce the metric time share as
the indicator of ranking. The time share of a device model
is the percentage of browsing time consumed by the speciﬁc
model to that consumed by all the models that use the app.
Deﬁnition 5: Formally, we deﬁne the time share as follows:
Time Share( Dj→A)=Time(Dj→A)/summationtextTime(Dk→A)(5)
Here,Time(Dj) is the time spent on a speciﬁc device model
and/summationtextTime(Dk) is the total time spent on all device mod-
els, i.e., all users of the app. The higher time share a device
modelDjholds, the more time users use the app on Dj.
Suppose that we select Ndevice models, we can use the
metric of Time Share Coverage to concretize the Usage Data
Coverage deﬁned in equation 3 to measure the eﬀectiveness
of PRADA.
Time Share Coverage =Ti m e (DNrecommended)/summationtextTi m e (Dk→A)
Ti m e (DNautual)/summationtextTi m e (Dk→A)
=Time Share (DNrecommended )
Time Share( DNactual)(6)
4.2 Collaborative Filtering by Time Share
Recall that the PRADA approach accepts inputs as (1)
the target app, and (2) Na st h en u m b e ro fd e v i c em o d e l s
7
7
7that app developers would take into account. We predict the
rank of time share from device models for this target appbased on the“known”similar top K-1 apps in the same app
category. Then the collaborative ﬁltering technique outputs
the topNmajor device models.
More speciﬁcally, we derive DMas the set of device mod-
els that at least one app of the K-1 apps uses. Then, across
all apps in the K-1 apps, we compute the aggregated brows-
ing time for each device model in DM. Finally, we sort the
device models in DMby their aggregated browsing time,
and recommend the top Ndevice models as output for the
target app.
To evaluate the eﬀectiveness of the collaborative ﬁltering
technique, we then perform the process of Leave-One-Out
Cross-Validation ( LOOCV ) [23]. LOOCV uses one obser-
vation as the validation set and the remaining observationsas the training set, and then repeats as each observation hasserved as the validation set. Thus, for each of the Kapps,
we use the browsing time of the remaining K-1 apps to pre-
dict the top device models for the app. In particular, we use
LOOCV, rather than hold out cross validation, since most
apps have far less browsing time than popular apps.
For each app category, we select Kapps and run the Al-
gorithm 1: DHas a list of /angbracketleftapp, device model hit /angbracketright,TC
as a list of /angbracketleftapp,time share coverage/angbracketright andAPas a list of
/angbracketleftapp,AP/angbracketright. For the inputs, we take a list of Kapps (AL),
a total set (Ω) of /angbracketleftdevice model, time/angbracketright for all the Kapps.
In each iteration of the algorithm, the app Ais treated as
a “new” app (validation set) and the remaining K-1 apps
as the “existing” apps (training set). For each app A,w e
ﬁrst obtain the /angbracketleftdevice model,time/angbracketright set (ω
A) from Ω to
ﬁnd the actual top Ndevice models (TD ). Then we use
the other K-1 apps to recommend Napps (TD/prime) for app
A.U s i n g TDandTD/primeofA, we can calculate the inter-
section results as device model hit , and calculate APusing
equation 2. Finally, for each device model DinTD/prime,w e
update the timewith the timefromωAand calculate the
time share coverage using equation 6.
To better illustrate the algorithm, we present an example
ofGameapps. Suppose that Nis 10 and Kis 100 in our
example, respectively. We instantiate Aas app Modoomar-
ble5, a popular game from Tencent [7]. By aggregating
thebrowsing time of device models from the other popu-
lar 99Gameapps, we obtain top 10 device models as the
recommendations for Modoomarble. Comparing the device
models with the“actual”top 10 device models of Modoomar-
ble, we ﬁnd that the ﬁrst 8 device models are overlapped,and thus the APis 0.8. The“actual”top 10 device models
account for 76.3% time share of the total time of Modoomar-
ble, while our selected 10 device models account for 74.2%.More speciﬁcally, the two distinct device models selectedby our technique, Xiaomi 2s andXiaomi 3 ,d on o to c c u r
in the “actual” top 10 device models of Modoomarble,a n d
these two device models actually contribute 0.5% and 0.7%time share, respectively. In contrast, the two device modelsmissed by our technique (HTC One andNexus 5)a c c o u n t
for 2.3% and 1.1% time share, respectively. However, thetime share coverage of our technique (against the time sharefrom “actual” top 10 device models) can reach 97.2% (i.e.,
74.2%/76.3%).
5http://www.wandoujia.com/apps/com.tencent.modoomarbleAlgorithm 1: Device model hit ( DH), time share cov-
erage (TC ) and average precision ( AP) against top N
device models with Kapps in the same category
Input:AL,Ω,N,K
Output :DH,TC,AP
foreach app AinALdo
ωA=Ω (A);
//get/angbracketleftdevice model,time/angbracketright for all device models
//using app A
TD A=max N(ωA);
//get top Ndevice models most used by A
AL/prime=AL−{A};
//AL/primeis the list of apps except A
ω/prime=/uniontext
A/prime∈AL/primeΩ(A/prime);
//get/angbracketleftdevice model,time/angbracketright for all device models
//using apps in AL/prime
//timeof samedevice model is added up
TD/prime=max N(ω/prime);
//get top Ndevice models most used by AL/prime
//as the prediction for A
DH(A)=|TD A/intersectiontextTD/prime|;
//the intersection of predicted Ndevice models and
//the actual ones
AP(A)=/summationtextN
i=1|TD(A)top i/intersectiontextTD/prime|/i
N;
//get the APof the prediction A
//TD(A)top imeans the subset constituted of the
//top i items in set TD(A)
foreach device model DinTD/primedo
TD/prime(D)=ωA(D);
//get the time that D(inTD/prime)s p e n to nA
end
TC(A)=/summationtext
DTD/prime(D)//summationtext
DTD A(D);
//get the time share coverage of the recommended
//device models for A
end
5. EV ALUATION
In this section, we evaluate the PRADA approach over
two typical types of apps that care about browsing time,GameandMedia. More speciﬁcally, we intend to answer
two research questions:
•RQ1:How many device models account for the major-
ity of the browsing time? To demonstrate the Pareto
distribution underlying PRADA, we perform a char-
acteristic analysis of the time share distribution of alldevice models for apps from two popular categories.
•RQ2:How eﬀectively can PRADA identify major de-
vice models for a new app given that developers have noknowledge about this app’s actual usage? To demon-
strate how the collective intelligence c a nh e l pd e v i c e
model prioritization, we explore the utility of the timeshare to help app developers select Nmajor device
models of a new app whose time share usage is en-tirely unknown. Here, we apply our collaborative ﬁl-tering technique to predict the time share of the appbased on the browsing time of top K-1 apps from the
same app category.
5.1 Device Model Distribution
We ﬁrst address RQ1, i.e.,how many device models ac-
count for the majority of the browsing time? This question
8
8
8corresponds to the Pareto Principle of the time share distri-
bution of all device models for an app and can indicate the
number of major device models for an app.
In our three-month dataset, we have a total of 16,602 dis-
tinct Android device models. It would seem to be quite
challenging and time-consuming for app developers to work
acrossthewholesetofexistingAndroiddevicemodels. Hence,PRADA ﬁrst summarizes the distribution of device modelsof an app, according to the browsing time.
In our current approach, we take the apps from the same
category as “similar” apps. There are 14 categories deﬁned
by Wandoujia, such as Communication ,Tools,Media,a n d
Game. We aggregate the browsing time of all apps belong-
ing to the same category. This clustering process can better
organize a large number of apps and identify which cate-gories contribute more browsing time. Results show thatapps from some categories take up a lot of browsing time.We refer to such apps as networked apps .
Wetheninvestigatethetimesharedistributionatthelevel
of every individual app. We choose two networked app cat-
egoriesGameandMedia.
Table 1: Number of device models and users that use top
100 apps from each of the two categories.
Category # of Device Models #o fU n i q u eU s e r s
Game 11,538 2,159,238
Media 13,894 3,547,219
Table 1 shows the number of device models and unique
users that use top 100 apps from each of the two categories.
Each category has millions of recorded users. Such a scale
of data can promise a comprehensive analysis. For each
category, we choose the top 100 apps by their aggregated
browsing time from all device models using the app. Fig-ure 3 shows the maximum, median, and minimum numberof device models that can account for X% time share of apps
of each category, where Xvaries from 0 to 90. Although the
maximum and minimum varies a lot for a speciﬁc app, themedian number of device models that account for 80% timeshare is generally around 100.Findings. Theprecedingresultof RQ1validatesthePareto
Principle of the distribution of device models contributing tobrowsing time. Although the distribution does not strictlyfollow the 80-20 rules in the Pareto principle, it still showsthat a quite small percentage of device models can accountfor a quite large portion of browsing time. It indicates that
developers of apps in these app categories can signiﬁcantly
narrow their selection space. Meanwhile, developers can ap-
proximately estimate how many device models they should
use to reach a desired time share.
● ● ●●●●●●●●
01002003004005006007008009001000
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%Time ShareNumber of Device Models●MaximumMedianMinimum
(a) Game● ● ● ● ●●●●●●
01002003004005006007008009001000
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%Time ShareNumber of Device Models●MaximumMedianMinimum
(b) Media
Figure 3: The maximum, median and minimum numbers of
device models covering X% time share for each app in the
two categories.5.2 Predicting Top Device Models
Based on the distribution of device models derived from
RQ1, we can help reduce a large number of device models
from the fragmented Android markets. However, such a
ﬁnding is based on the assumption that the usage data is
already known. For a new app that is to be put on shelf andis not associated with informative knowledge of usage data,how can we leverage the ﬁndings of RQ1to predict the
device models to prioritize? Thus, we next address RQ2,
i.e.,how eﬀectively can PRADA identify major device models
given that developers have no usage knowledge about this
app?
Such an evaluation is done by applying the collaborative
ﬁltering algorithm 1. We still investigate the 200 apps of
the two categories, GameandMedia. For an app Ain one
category, we assume that its usage data is unknown. We
then apply the collaborative ﬁltering algorithm to ﬁnd the
set ofNdevice models with the most time share by leverag-
ing the browsing time from the remaining K-1 apps in the
same category. Here, we assign Nas 10 while Kas 100.
We then report the distribution of Device Model Hit ,Time
Share Coverage ,a n dAverage Precision against the ground
truth of A. We repeat the process for all 100 apps for each
category. In particular, to evaluate the overall precision ofall apps, we consider the widely used metric Mean Aver-
age Precision (MAP) ,w h i c hi su s e dt or a n kt h er e s u l t sf o r
a large number of queries [5]. Hence, we also compute MAP
for apps from each category as follows:
MAP(N)=
K/summationdisplay
j=1AP j/K (7)
5.2.1 Results
We report the results of Device Model Hit, Time Share
Coverage ,a n dAPoftop10devicemodelsthatarepredicted
by PRADA for 100 apps in each category, i.e., N=10 and
K=100.
It is observed that PRADA works quite well for both cat-
egories. For apps in the Gamecategory, the Device Model
Hitis with 8 as the median, 10 as the maximum, and 3 as
the minimum (shown in Figure 4(a)). Correspondingly, theboxplot of Figure 4(b) illustrates the Time Share Coverage
from the 10 selected device models against the 10 “actual”top device models, with 97.4% as the median, up to 98.9% asthe third quartile, and not less than 94.5% as the ﬁrst quar-
tile. The median of APcan reach 0.82 as median, and the
MAPforGameapps is 0.75, which is a satisfactory score.
Similar to the Gamecategory, the Device Model Hit re-
sults for Mediaare shown in Figure 5(a), with 8 as the
median. In terms of Time Share Coverage ,a ss h o w ni n
Figure 5(b), our technique can reach 96.7% as the median,
99.2% as the third quartile, and 90.2% as the ﬁrst quartile,
respectively. The median of APis 0.79, and the MAPfor
Mediaapps is 0.72.
Although the core of PRADA is to leverage usage data,
it is quite common that developers usually choose the de-
vice models from the most market share. However, such a
selection is too coarse-grained, as it relies on only the num-
ber of active device models that have been on the market.
More seriously, such selection may be inaccurate, with re-
spect to a speciﬁc metric of the developers’ interest. Wemake a simple comparison study between the top 10 devicemodels that are predicted by PRADA and that are directly
9
9
9Table 2: Top 10 device models with the most time share for two apps (Temple Run 2 and Xunlei Movie), and the selected
device models by AppBrain, Wandoujia, and PRADA.
Top 10 device models in market Top 10 device models for Temple Run 2 Top 10 device models for Xunlei Movie
AppBrain Wandoujia Ground Truth PRADA Ground Truth PRADA
Galaxy S3 Galaxy Note 3 Galaxy Note 3 Galaxy Note 3 Galaxy Note 3 Galaxy Note 3
Galaxy S4 Galaxy S4 Galaxy Note 2 Galaxy Note 2 Galaxy Note 2 Galaxy Note 2
Galaxy Note 3 Galaxy Note 2 Galaxy S4 Galaxy S4 Galaxy S4 Galaxy S4
Galaxy S5 Galaxy S3 MX 3 MX 3 MX 3 MX 3
Motorola Moto G Galaxy Win Galaxy S5 Galaxy S5 MX 2 Galaxy Mega 5.8
Galaxy S3 mini Xiaomi 2s Xiaomi 3 Galaxy Mega 5.8 Galaxy S5 Galaxy S5
Galaxy Tab 3.7 Xiaomi 3 Xiaomi 2s MX 2 Galaxy Mega 5.8 MX 2
Galaxy Note 2 MX 3 Galaxy Mega 5.8 Galaxy S3 HTC One Galaxy S3
Galaxy S Duos 2 Galaxy Mega 5.8 MX 2 Xiaomi 2s Galaxy S3 Xiaomi 2s
Galaxy S2 Galaxy S2 Galaxy S3 Xiaomi 3 LG Nexus 5 Galaxy S2
●
● ●●
●46810
AppBrain Wandoujia our approach
Top 10 device models from AppBrain, Wandoujia and our approachDevice Model Hit
(a) Device model hit●●●
●●●
●●
●●●
●●●
●●●
●●●
50%60%70%80%90%100%
AppBrain Wandoujia our approach
Top 10 device models from AppBrain, Wandoujia and our approachTime Share Coverage
(b) Time share coverage
 (c) Average precision
Figure 4: Comparison of Device Model Hit, Time Share Coverage ,a n dAPby using market share and PRADA to recommend
top 10 device models for Gameapps.
obtained from the market share, respectively. The goal of
such a comparison is two folds. First, we aim to demonstrate
that simply relyingon the marketshareis toocoarse-grained
and even inaccurate for prioritization, with respect to a spe-
ciﬁc metric of interest. Second, we aim to demonstrate thatPRADA can achieve a satisfactory accuracy to help selectdevice models.
To align with the time of our collected dataset, we choose
the market share reports of Android device models of the
3rdquarter2014, fromthewell-knownAppBrainwebsite[2].
AppBrain provides a global market share of device models.
Due to the lack of detailed local market share of device mod-els, we derive a local market share by aggregating the active
users of a speciﬁc device model from Wandoujia.
We also compute the three metrics, Device Model Hit ,
Time Share Coverage,a n d AP, by using the device mod-
els with the most market share of AppBrain (in short asusing AppBrain) and the ones with the most market shareof Wandoujia (in short as using Wandoujia), respectively.From Figure 4(a), we can observe that for the Gameapps,
the 10 device models selected by using AppBrain reach 5 asthe median of the Device Model Hit .T h es i t u a t i o no fu s i n g
Wandoujia is a bit better than using AppBrain, reaching 7
a st h em e d i a no ft h eDevice Model Hit. In other words, De-
vice Model Hit by market share is worse than using PRADA,
considering time share. In Figure 4(b), the median of time
share covered by the device models by PRADA is 97.4%. Incontrast, the value by using market share of Wandoujia is92.1%, while the median of using AppBrain is 76.4%, whichis the lowest. Additionally, the MAPv a l u ei s0 . 3 9b yu s i n g
AppBrain, 0.62 by using Wandoujia, which is far away from0.75 by using PRADA. Similar observations can be made forMediaapps.We show an example by two typical apps, which are also
popular on Google Play, the Temple Run 2
6and Xunlei
Movie7.W el i s tt h et h ed e v i c em o d e l s( 1 )w i t ht o p1 0m a r -
ket share from AppBrain and Wandoujia (descending orderof market share in Columns 1 and 2), respectively; (2) with
the most actual browsing time for the given apps (descend-
ing order of time share in Columns 3 and 5); (3) ranked byPRADA (Columns 4 and 6), in Table 2.
The top 10 device models with the most market share
by AppBrain can cover only 5 out of the top 10 devicemodels for Temple Run 2 and Xunlei Movie, compared to
the ground truth of browsing time. More speciﬁcally, App-Brainmisses5ofthetop10devicemodelsfor Temple Run 2.
The device models missed by AppBrain are MX 3, Xiaomi
3, Xiaomi 2s, Galaxy Mega 5.8,a n d MX 2. In contrast,
PRADA can hit all of the top 10 device models. For theXunlei Movie, AppBrain misses 5 device models, i.e., MX 3,
MX 2, Galaxy Mega 5.8, HTC One,a n d LG Nexus 5,w h i l e
PRADA misses only HTC One and LG Nexus 5. The device
models with the most market share from Wandoujia canhave higher device model hit (8 for Temple Run 2 and 6 for
Xunlei Movie), but the hit ratio is still lower than PRADA.
As fortime share coverage , it is 72.7% for Temple Run
2and 70.2% for Xunlei Movie by using the top 10 device
models from AppBrain. In contrast, PRADA can achieve100% and 97.5%. Using Wandoujia market share performsbetter than AppBrain but still worse than PRADA, whichachieves 95.1% and 86.1% for the two apps, respectively.
TheAPvalues by device models of AppBrain of the two
apps are both less than 0.5. The APis unsatisfactory (0.75
forTemple Run 2 and 0.53 for Xunlei Movie)b yt h et o pd e -
6https://play.google.com/store/apps/details?id=com.
imangi.templerun2
7https://play.google.com/store/apps/details?id=com.
xunlei.cloud
10
10
10● ● ● ● ● ● ● ● ● ●● ●
● 0246810
AppBrain Wandoujia our approach
Top 10 device models from AppBrain, Wandoujia and our approachDevice Model Hit
(a) Device model hit●
●●
●●●●●●
●●
●●
●●
●
●●
●●
●●
●●
●
●●
0%10%20%30%40%50%60%70%80%90%100%
AppBrain Wandoujia our approach
Top 10 device models from AppBrain, Wandoujia and our approachTime Share Coverage
(b) Time share coverage
 (c) Average precision
Figure 5: Comparison of Device Model Hit, Time Share Coverage ,a n dAPby using market share and PRADA to recommend
top 10 device models for Mediaapps.
vice models from Wandoujia’s market share. PRADA can
reach the APof 1.0 and 0.8, respectively. Such result illus-
trates that relying on only the market share is not accurate
to predict the top device models against the actual opera-tional proﬁle of an app.
5.2.2 Findings
When addressing RQ2, we have some ﬁndings. When
prioritizing device models, using the market share is not
always suﬃcient or even accurate, with respect to a speciﬁc
usage data, e.g., browsing time. In contrast, PRADA canmore accurately identify device models on which users spendmost browsing time.
Another ﬁnding is that the results derived from the Wan-
doujia market share are usually better than those from App-Brain. Such result indicates that the localization plays animportant role of device model prioritization. Even relyingon the market share, app developers targetingdiﬀerent areas
would need to treat device models diﬀerently.
In both categories, there are some apps not well supported
by our technique. For example, in Mediaapps, we ﬁnd 3
apps whose Time share Coverage is 0 or close to 0, e.g., HTC
Album(0%), UMI Media Player (0.6%), and Android Music
Player(1.6%). Indeed, all top devices of HTC Album are
d e v e l o p e db yH T C .M o s to ft h et o pd e v i c em o d e l so f UMI
Media Player and Android Music Player are local manu-
facturers from China. These two apps are also customized
for these manufacturers and preloaded on their devices.
In summary, PRADA can accurately predict top device
models, even if no informative usage data is given.
5.3 Discussion
The key idea of PRADA is to predict top device models of
an app based on the usage of similar apps, and the measure-
mentofsimilarity isquitegeneralinPRADA.Currently, the
selected “similar” apps are based on the app category. In-
deed, generally assuming that apps from the same categoryto be similar is a bit simplistic and coarse-grained, but the
classiﬁcation system of Wandoujia has some eﬀective criteria
to categorize the apps. Our evaluation has already shown
eﬀective results achieved by using the app category. Indeed,someexistingcomplexmetricsproposedintheliterature[14,38] can be integrated into PRADA.
Although we choose only two popular app categories as
illustrating examples, evidence actually supports that thetop device models can vary a lot among diﬀerent app cate-gories. There are 14 categories deﬁned by Wandoujia, i.e.,Business, Communication ,Finance, Game,Lifestyle,Media,
Mother
andBaby,News,Productivity, Reading andStudy,
Shopping, Social,Tools,a n dTravel. We perform the pair-wise comparison of top 100 device models between cate-
gories, and the overlap is quite low. For example, the device
modelXiaomi 2s is ranked as in the top 10 list of Game,
but does not appear in the top 10 lists of 11 other app cat-egories. Xiaomi 3 , which is ranked as in the top 10 list of
Game, isnotrankedasinthetop 10lists of13other appcat-
egories. Thus, it is important to conduct category-speciﬁcrecommendation of device models, as done in PRADA.
6. IMPLICATIONS
Based on a large-scale dataset collected from real-world
users on interacting with Android apps, we have evaluatedthat operational proﬁles such as browsing time can accu-
rately prioritize device models in the app categories of Game
andMedia. Although the speciﬁc results (e.g., which spe-
ciﬁc device models were top for an app or app category)
from this study are useful for developers, device models andtheir usage are constantly evolved, and more recent usage
data shall be used to re-apply PRADA in practice. Hence,
we should focus on our general ﬁndings and methodologies
beyond the speciﬁc results.
Relying on the market share is not always accurate, with
respect to a speciﬁc metric of interest. In other words, more
users do not always lead to more usage. Therefore, develop-ers need to carefully explore and avoid uninformed invest-mentonpopulardevicemodels, whichmaynotbesigniﬁcantwith respect to the metrics of their interest.
Theideaofleveragingthecollectiveknowledgefromlarge-
scale usage data is feasible. By using the time share frommajor device models of other apps in the same app category,developers of new apps can accurately select major devicemodels, even when the developers do not know which devicemodels would heavily use their apps in the future. In addi-tion, such idea can be extended to other types of usage data
of an app (such as reported bugs, user reviews, traﬃc, and
energy drain) over a speciﬁc device model.
Indeed, applying an approach such as PRADA requires a
suﬃcient usage dataset that is legallycollected from users.
We plan to release a sample anonymized dataset to otherresearchers for further study. In addition, we plan to makePRADA available as an analytic service for assisting An-
droid developers who publish or plan to publish their apps
on Wandoujia.
7. THREATS TO V ALIDITY
We have evaluated our approach in two main networked
app categories. This section discusses the threats to validityin our evaluation.
A threat to validity includes the localization: using only
Wandoujia (primarily in the China market). All users and
11
11
11usage data are from Wandoujia, because other marketplaces
such as Google Play do not release their usage data exter-nally. We cannot validate the time-share-based techniqueover users and apps that are not included in the Wandou-
jia dataset. Although the large scale of dataset could pro-
vide comprehensive results to developers in China, devel-
opers from other regions cannot directly use the results asreference to predict device models for their apps used intheir regions. However, the general idea of PRADA couldbe applicable, if developers have other published usage datasuch as AppJoy [41] and LiveLab [39, 34, 33]. However, the
data should be at scale to enable comprehensive analysis.
In future work, we plan to alleviate this threat by applying
PRADA on other usage data beyond the Wandoujia dataset.
We apply PRADA based on a speciﬁc usage data, i.e., the
browsing time that comes from foreground network accesstime in an app, indicating how long users interact with theapp. The metric of time share can measure the importance
of a device model for a speciﬁc app. Although most of these
apps need the network connection, some of them do not al-
ways produce foreground network activities. Instead, their
network activities are often performed in the background,such as downloading or updating. Some apps are mainlyused oﬄine, such as PDF readers. Therefore, our approachcould not be generalized to all kinds of apps, if only the
browsing time is used. However, the signiﬁcance of using
browsing time still remains because apps with online appusage are increasingly widespread and important. In addi-tion, as long as using online app usage allows to preservethe ranking of device models, it can still achieve eﬀective
results.
ThetimesensitivityalsoimpactstheeﬀectivenessofPRADA.
We limit only 3 months of data to evaluate our approach,
i.e., from July to September. One reason for such studysetup is that we want to compare the eﬀectiveness of thetime share and market share, where the latter is usuallymade quarterly. However, it is well known that the upgradeof smartphones is quite frequent, and users may buy newdevices. Hence, the number of users for a device model maykeepchanging, correspondinglyleadingtothechangeoftimeshare for an app. Therefore, for app developers, using our 3months of dataset could not well predict the currently ma-
jor device models. To alleviate this threat, possible solutionsinclude performing our approach online by using the latesttime share collected from the Wandoujia management app,or exploiting Wandoujia data covering a longer time span,
e.g., 1 year or longer, to learn how user behaviors impact
time share and improve the eﬀectiveness of our approach.
To this end, we need the server-side support of Wandoujia
such as exposing online data-retrieval APIs, and avoid po-tential side eﬀects and interferences to other online servicesof Wandoujia.
The release date of a device model can also impact the
eﬀectiveness of PRADA. PRADA relies on the real usagecollected from substantial users within a reasonable period.If a device model is recently released and no enough usagedata can be collected from this device model, the application
scope of PRADA may be limited. Such a limitation cannot
be completely overcome, as the device model is entirely new.However, the situation can be alleviated. One promising
solution is to reduce the latency between the release date of
the device model and the collection of usage data. To thisend, PRADA can be deployed over the Wandoujia server,being timely sensitive to the usage data collected from thisnew device model.
To predict device models, we use only the category of
apps deﬁned by Wandoujia, to obtain similar K-1 apps for
the collaborative ﬁltering in PRADA. Therefore, the perfor-
mance of PRADA currently relies on the accuracy of Wan-
doujia’s category taxonomy of apps. As found in our pre-vious work [24], a lot of apps from diﬀerent categories mayalso have very high similarity. Besides the category infor-mation, more proﬁles of apps (such as the vendor informa-
tion, user reviews, app requirements, and libraries) can be
further leveraged to collect those apps sharing similar fea-tures with the given app under consideration. Some existing
metrics [14, 38] are under consideration to be plugged into
PRADA.
8. CONCLUSION AND FUTURE WORK
To address the challenges caused by Android device frag-
mentation, inthispaper, wehavepresentedthenovelPRADAapproach by using real-world usage data collected from alarge number of users. PRADA includes a collaborative ﬁl-tering technique to accurately predict major device modelsfor a new app, given the usage data from existing apps withsimilar functionalities. We have evaluated PRADA by us-
ing the in-app browsing time, which indicates how much
users interact with an app on a speciﬁc device model. In
our study, we used 200 apps from two app categories (Gameand Media), spanning three months and covering 3.86 mil-lion users and 14.71 thousand device models. Implicationsderived from our ﬁndings provide useful guidelines for An-droid app developers.
Since most of the data collected from Wandoujia is from
China, a great deal of localization issues may account fordiﬀerences compared to the global market. We plan to in-vestigate the impact of localization on device model prior-
itization in future work. We also plan to further explore
how to cluster device models at diﬀerent granularities. An-other ongoing eﬀort is to measure the fragmentation impact
caused by the great diversity of Android OS versions when
applying PRADA.
Acknowledgment
This work was supported by the High-Tech Research and De-
velopment Program of China under Grant No.2015AA01A203,the Natural Science Foundation of China (Grant No. 61370020,61421091, 61222203, 61572051, 61528201). Tao Xie’s work wassupported in part by National Science Foundation under grantsno. CCF-1349666, CCF-1409423, CNS-1434582, CCF-1434596,andCNS-1513939, andaGoogleFacultyResearchAward. QiaozhuMei’s work was supported in part by the National Science Foun-
dation under grant No. IIS-1054199. The authors would like toappreciate Le Lian for valuable suggestions on the application
context of this work.
9. REFERENCES[1] Android fragementation problem. http://www.greyheller.
com/Blog/androids-fragmentation-problem.
[2] AppBrain. http://http://www.appbrain.com/.
[3] AppThwack. https://appthwack.com.
[4] Facebook: How do I run ads only on speciﬁc types of
phones? https:
//www.facebook.com/business/help/607254282620194.
[5] Mean average precision.
https://www.kaggle.com/wiki/MeanAveragePrecision.
[6] OpenSignal. http:
//opensignal.com/reports/2014/android-fragmentation/.
12
12
12[7] Tencent. http://www.tencent.com.
[8] Testin. http://www.testin.cn.
[9] Wandoujia. http://www.wandoujia.com.
[ 1 0 ]S .B a l s a m o ,A .D .M a r c o ,P .I n v e r a r d i ,a n dM .S i m e o n i .
Model-based performance prediction in software
development: A survey. IEEE Transactions on Software
Engineering , 30(5):295–310, 2004.
[11] P. G. Bishop. Rescaling reliability bounds for a new
operational proﬁle. In Proceedings of the ACM SIGSOFT
International Symposium on Software Testing andAnalysis, ISSTA 02 , pages 180–190, 2002.
[12] M. B ¨ohmer, B. Hecht, J. Sch ¨oning, A. Kr ¨uger, and
G. Bauer. Falling asleep with Angry Birds, Facebook and
Kindle: a large scale study on mobile application usage. InProceedings of the International Conference onHuman-computer Interaction with Mobile Devices and
Services, MobileHCI 11, pages 47–56, 2011.
[13] M. B ¨ohmer and A. Kr ¨uger. A study on icon arrangement
by smartphone users. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
CHI 13, pages 2137–2146, 2013.
[ 1 4 ]N .C h e n ,S .C .H .H o i ,S .L i ,a n dX .X i a o .S i m A p p :A
framework for detecting similar mobile applications byonline kernel learning. In Proceedings of the ACM
International Conference on Web Search and Data Mining,
WSDM 15 , pages 305–314, 2015.
[15] G. Chittaranjan, J. Blom, and D. Gatica-Perez. Mining
large-scale smartphone data for personality studies.Personal and Ubiquitous Computing, 17(3):433–450, 2013.
[16] B. Cukic and F. B. Bastani. On reducing the sensitivity of
software reliability to variations in the operational proﬁle.
InProceedings of the International Symposium on Software
Reliability Engineering, ISSRE 96 , pages 45–54, 1996.
[17] H. Falaki, D. Lymberopoulos, R. Mahajan, S. Kandula, and
D. Estrin. A ﬁrst look at traﬃc on smartphones. In
Proceedings of the ACM SIGCOMM Conference on
Internet Measurement, IMC 10, pages 281–287, 2010.
[18] J. Gui, S. Mcilroy, M. Nagappan, and W. G. J. Halfond.
Truth in advertising: The hidden cost of mobile ads forsoftware developers. In Proceedings of the IEEE/ACM
International Conference on Software Engineering, ICSE
15, pages 100–110, 2015.
[19] M. Halpern, Y. Zhu, R. Peri, and V. J. Reddi. Mosaic:
cross-platform user-interaction record and replay for thefragmented Android ecosystem. In Proceedings of the IEEE
International Symposium on Performance Analysis of
Systems and Software, ISPASS 15 , pages 215–224, 2015.
[20] D. Han, C. Zhang, X. Fan, A. Hindle, K. Wong, and
E. Stroulia. Understanding Android fragmentation withtopic analysis of vendor-speciﬁc bugs. In Proceedings of the
Working Conference on Reverse Engineering, WCRE 12 ,
pages 83–92, 2012.
[21] J. Jung, S. Han, and D. Wetherall. Short paper: enhancing
mobile application permissions with runtime feedback andconstraints. In Proceedings of the ACM Workshop on
Security and Privacy in Smartphones and Mobile Devices,SPSM 12 , pages 45–50, 2012.
[22] H. Khalid, M. Nagappan, E. Shihab, and A. E. Hassan.
Prioritizing the devices to test your app on: A case study
of Android game apps. In Proceedings of the SIGSOFT
International Symposium on the Foundations of Software
Engineering, FSE 14, pages 610–620, 2014.
[23] R. Kohavi et al. A study of cross-validation and bootstrap
for accuracy estimation and model selection. Ijcai,
14(2):1137–1145, 1995.
[24] H. Li, X. Lu, X. Liu, T. Xie, K. Bian, F. X. Lin, Q. Mei,
and F. Feng. Characterizing smartphone usage patterns
from millions of Android users. In Proceedings of the ACMSIGCOMM Conference on Internet Measurement, IMC 15 ,
pages 459–472, 2015.
[25] S. L. Lim, P. J. Bentley, N. Kanakam, F. Ishikawa, and
S. Honiden. Investigating country diﬀerences in mobile app
user behavior and challenges for software engineering. IEEE
Transactions on Software Engineering, 41(1):40–64, 2015.
[26] C. D. Manning, P. Raghavan, and H. Sch ¨utze. Introduction
to information retrieval . Cambridge University Press, 2008.
[27] B. Mobasher, H. Dai, T. Luo, and M. Nakagawa. Discovery
and evaluation of aggregate usage proﬁles for web
personalization. Data mining and knowledge discovery ,
6(1):61–82, 2002.
[28] J. D. Musa. Operational proﬁles in software-reliability
engineering. IEEE Software, 10(2):14–32, 1993.
[29] S. Nath. Madscope: Characterizing mobile in-app targeted
ads. In Proceedings of the International Conference on
Mobile Systems, Applications, and Services, MobiSys 15 ,
pages 59–73, 2015.
[30] M. E. Newman. Power laws, Pareto distributions and Zipf’s
law. Contemporary physics, 46(5):323–351, 2005.
[ 3 1 ]J .H .P a r k ,Y .B .P a r k ,a n dH .K .H a m .F r a g m e n t a t i o n
problem in Android. In Proceedings of the International
Conference on Information Science and Applications ,
pages 1–2, 2013.
[32] T. Petsas, A. Papadogiannakis, M. Polychronakis, E. P.
Markatos, and T. Karagiannis. Rise of the planet of the
apps: a systematic study of the mobile app ecosystem. InProceedings of the ACM SIGCOMM Conference on
Internet Measurement, IMC 13 , pages 277–290, 2013.
[33] A. Rahmati, C. Tossell, C. Shepard, P. Kortum, and
L. Zhong. Exploring iPhone usage: the inﬂuence of
socioeconomic diﬀerences on smartphone adoption, usage
and usability. In Proceedings of the International
Conference on Human-computer Interaction with Mobile
Devices and Services, MobileHCI 12, pages 11–20, 2012.
[34] A. Rahmati and L. Zhong. Studying smartphone usage:
Lessons from a four-month ﬁeld study. IEEE Transactions
on Mobile Computing , 12(7):1417–1427, 2013.
[35] L. Ravindranath, J. Padhye, S. Agarwal, R. Mahajan,
I. Obermiller, and S. Shayandeh. AppInsight: Mobile app
performance monitoring in the wild. In Proceedings of
USENIX Symposium on Operating Systems Design and
Implementation, OSDI 12, pages 107–120, 2012.
[36] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and
J. Riedl. GroupLens: an open architecture for collaborative
ﬁltering of netnews. In Proceedings of the ACM conference
on Computer supported cooperative work, CSCW 94 ,p a g e s
175–186, 1994.
[37] P. Resnick and H. R. Varian. Recommender systems.
Communications of the ACM , 40(3):56–58, 1997.
[38] Y. Tian, M. Nagappan, D. Lo, and A. E. Hassan. What are
the characteristics of high-rated apps? A case study on free
Android applications. In Proceedings of the IEEE
International Conference on Software Maintenance and
Evolution, ICSME 15, pages 301–310, 2015.
[39] C. Tossell, P. Kortum, A. Rahmati, C. Shepard, and
L. Zhong. Characterizing web use on smartphones. In
Proceedings of the SIGCHI Conference on Human Factorsin Computing Systems, CHI 12, pages 2769–2778, 2012.
[ 4 0 ]Q .X u ,J .E r m a n ,A .G e r b e r ,Z .M .M a o ,J .P a n g ,a n d
S. Venkataraman. Identifying diverse usage behaviors ofsmartphone apps. In Proceedings of the ACM SIGCOMM
Conference on Internet Measurement, IMC 11 ,p a g e s
329–344, 2011.
[41] B. Yan and G. Chen. AppJoy: personalized mobile
application discovery. In Proceedings of the International
Conference on Mobile Systems, Applications, and Services,
MobiSys 11, pages 113–126, 2011.
13
13
13