FastLane: Test Minimization for Rapidly Deployed
Large-scale Online Services
Adithya Abraham Philip, Ranjita Bhagwan, Rahul Kumar, Chandra Sekhar Maddila and Nachiappan Nagappan
Microsoft Research
Abstract —Today, we depend on numerous large-scale services
for basic operations such as email. These services, built on the
basis of Continuous Integration/Continuous Deployment (CI/CD)
processes, are extremely dynamic: developers continuously com-
mit code and introduce new features, functionality and ﬁxes.
Hundreds of commits may enter the code-base in a single day.
Therefore one of the most time-critical, yet resource-intensive
tasks towards ensuring code-quality is effectively testing such
large code-bases.
This paper presents FastLane, a system that performs data-
driven test minimization . FastLane uses light-weight machine-
learning models built upon a rich history of test and commit logs
to predict test outcomes. Tests for which we predict outcomes
need not be explicitly run, thereby saving us precious test-
time and resources. Our evaluation on a large-scale email and
collaboration platform service shows that our techniques can
save 18.04%, i.e., almost a ﬁfth of test-time while obtaining a
test outcome accuracy of 99.99%.
Index T erms —test prioritization, commit risk, machine learn-
ing
I. I NTRODUCTION
O365 is a large enterprise collaboration service that supports
several millions of users, runs across hundreds of thousands
of machines, and serves millions of requests per second.
Thousands of developers contribute code to it at the rate
of hundreds of commits per day. Dozens of new builds are
deployed every week.
Testing such a large and dynamic Continuous Integra-
tion/Continuous Deployment (CI/CD) system at scale poses
several challenges. First and foremost, a massive, fully dedi-
cated set of compute resources are required to run these tests
continuously. In spite of having such dedicated resources,
O365’s pipelines are often unable to keep up with testing
all commits thoroughly. Several enterprise testing platforms
suffer from this problem [1]. This is mainly because of a
growing code submission rate, and a constantly increasing test-
pool size. As a result, a large fraction of commits that could
potentially contain bugs go untested.
A key approach to addressing this problem is to minimize
the tests to run. One can use several approaches towards this.
However, with large CI/CD systems such as O365, we identify
a new opportunity: Big Data storage and analytics systems [2],
[3] are now ubiquitous. Modern-day services use these systems
to store and process detailed logs over extended time-periods,
thereby deriving rich insights about every aspect of the system
development life-cycle Most relevant to our problem are large-
scale commit logs and test-run logs. O365’s logs, for instance,reside on big-data stores, and we routinely analyze more than
one year of test and commit data in the FastLane pipeline.
We therefore ask the following question: Can we make high-
conﬁdence data-driven decisions towards test minimization?
In other words, can we learn models and rules from past data
that can guide more intelligent decisions on what subset of
tests we need to run? There has been a large body of prior work
in the test prioritization, selection and minimization areas [4]–
[8]. More closely related to our work is the work by Elbaum et
al. [4] which prioritizes tests based on the probability of fault
existence. Our work draws inspiration from prior published
work but differs in several key ways as detailed below. We
analyzed O365’s commit and test logs in detail, and made the
following observations:
1) Some commits are more likely to fail tests than others.
Not all commits are created equal. Some commits make major
changes, such as changing the functionality of a network
protocol. Other commits make tweaks to conﬁguration ﬁles
which may not impact core functionality at all. Some commits
introduce completely new features to UI components, while
others may just make cosmetic changes to the UI. This, by
itself, is not a new ﬁnding: prior work has found this to hold
for large open-source software repositories [9]. Nevertheless,
our study helped us determine that it is indeed feasible to learn
the complexity, and therefore, predict the risk associated with
every commit.
2) Several tests exercise the same or similar functionality.
Several developers add test-cases over time for the same code-
base. In many cases, these tests may be redundant, or at the
very least, related. That is, their code-coverage is largely the
same. Consequently, we observe inter-test correlations , i.e.,
the outcomes of various test-pairs are heavily correlated.
3) Several tests are combinations of smaller test-cases
whose outcomes are related. Such tests usually contain dif-
ferent code-paths that evaluate similar or same functions,
leading to intra-test correlations . This emerges as a correlation
between the test’s outcome and its run-time. For instance, a
speciﬁc test used the same API call multiple times with slightly
different parameters. When the test fails, it tends to fail at the
ﬁrst API call, and therefore has a very short run-time. This
emerges as a correlation between test failure and very short
run-times of the test.
Based on the above observations, we designed three differ-
ent approaches towards predicting test outcomes and therefore
saving test resources. This paper makes the following contri-
butions:
4082019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 ©2019 IEEE
DOI 10.1109/ICSE.2019.00054
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. 1) Commit Risk Prediction: Since commits vary in com-
plexity (Observation 1), we learn classiﬁcation models that
predict the complexity of a commit and, therefore, the need
to test it. Our model should predict that a commit that makes
major changes is “risky”, while a commit that makes a minor
tweak is “safe”. We can then use this label to test only risky
commits while fast-tracking safe commits to deployment. We
describe this in Section III-B.
2) Test Outcome-based Correlation: Since several tests have
well correlated outcomes (Observation 2), we learn association
rules that ﬁnd test-pairs that pass together and fail together.
For each test-pair, we then need to run only one test since we
can predict the outcome of the second with high conﬁdence.
We describe this in Section III-C.
3) Runtime-based Outcome Prediction: Since some tests are
amalgamations of related tests (Observation 3), we use tech-
niques derived from logistic regression to estimate a runtime
threshold for these tests. This threshold nicely separates passed
runs from failed runs for each test. We can then use this to
stop these tests after they have run for the threshold time, and
predict their outcome as pass (or fail). We describe this in
Section III-D.
4) Integrated Algorithm & System Design: We describe
an integrated algorithm that incorporates the above-mentioned
approaches into one control ﬂow (Figure 1). We have built
a system called FastLane which incorporates this algorithm
(Section IV). While providing lightweight techniques, Fast-
Lane also provides threshold-tuning knobs which can be used
to trade-off test-time saved with test prediction accuracy.
FastLane is implemented and integrated into O365’s pipelines
(Section V).
5) Evaluation: We provide a quantitative evaluation of the
above strategies on O365’s logs collected over a period of
14 months. We ﬁnd that, using such techniques, we can save
18.04% of test-time while maintaining an accuracy of test
outcomes at 99.99%. Finally, we provide ﬁndings from user-
studies to understand the reasons why our techniques work in
practice (Section VI).
To the best of our knowledge, this is the ﬁrst attempt to
leverage code risk and test interrelationships using a black-box
machine learning based approach for test minimization on such
large-scale services. Our techniques are simple, lightweight
and generic. They use well-known machine-learning and mod-
eling techniques and so can be extended to other services as
well.
II. O VERVIEW
In this section, we provide an overview of O365’s develop-
ment processes. We then describe our problem statement and
provide an overview of our approach.
A. O365
O365 uses Git as its version-control system. It sees about
130 to 140 code commits per-day, of which around 40 to 50
commits are tested (35%). A commit varies in complexity from
a single-line change in one ﬁle to major changes in hundreds,TABLE I
EXAMPLE OF THE COMPONENTS MODIFIED BY A COMMIT AND THE TEST
SUITES RUN .THE THREE COMPONENTS MODIFIED ARE FUNCTIONALLY
DIFFERENT .H ENCE ,THREE TEST -SUITES ARE RUN .
Component Modiﬁed Test Suite Run
/src/abc TestCreateMessageApi
/src/xyz TestParseFormInput
/src/klm V alidateLogEntry, WriteLogFile
even thousands of ﬁles. The developer who creates the commit
chooses one or more reviewers who review the commit. After
potentially multiple iterations with the reviewers, the developer
runs unit-tests on their commit. Once these tests pass, the
developer runs functional tests on the commit. Our work
concentrates on minimizing runs of functional tests since they
are most time and resource-intensive. Henceforth, we use the
term “test” and “functional test” interchangeably.
Over time, test engineers and developers have built a set of
rules based on code coverage and their knowledge of which
code ﬁles could affect a test. These determine what set of
functional tests to run on a given commit. Each rule maps a
modiﬁed component to a set of test-suites , where a component
is a subtree in the ﬁle hierarchy of the code-base and a
test-suite is a group of related tests. Thus, depending on the
components that the commit modiﬁes, the system determines
the tests-suites to run. Table II shows some example rules.
Note that, by design, functional tests are independent of each
other and therefore can be run in parallel.
The code-base has around 1000 test-suites. Each test-suite
consists of between 1 to 1000 tests, with an average of 60-70
tests per test-suite. Each test takes between less than 1 second
to more than 1000 seconds to run, with an average run-time of
6 seconds. 99.72% of all tests pass. On average, every commit
runs 3000 tests. Given this set-up, some commits can take
almost 30 hours to ﬁnish testing and be ready for deployment.
This extremely fast rate of development and large-scale
testing causes delays to commits from being deployed world-
wide. Test resources, i.e., machines on which we run tests, are
limited. The number of tests to run for a commit can be large
and therefore it may take a long time for all tests running on
a given commit to complete. This motivates our problem and
approach.
B. Problem Statement and Approach
Given the problem explained at the end of II-A, we ask
the question: Can we decrease the amount of testing required
while maintaining code quality? There are various approaches
to address this question. A large amount of previous work
targets the problem of test selection, i.e., how we can use
code-coverage metrics and static analysis techniques to prune
the set of tests to run for a given commit [4]–[8], [10]–[16].
Such techniques can help us with the problem of test selection.
However, the complexity and scale of O365 require us to use
additional light-weight techniques, on top of traditional test
409
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. selection techniques, to deal with the problem of minimizing
test resource usage.
Fortunately, O365 uses Big Data systems to maintain logs
that contain 14 months of test run-time data. These logs hold
detailed information on the set of tests run on every commit,
the time that each test took, and the outcome of the test. We
therefore use a black-box, data-driven approach to answer the
above question. We use machine-learning on test and commit
logs to learn models that predict the outcome of certain tests
so that we do not have to explicitly run them, thereby saving
test resources and decreasing time-to-deployment.
To determine if such an approach is feasible, we ﬁrst
analyzed a fraction of the test logs to ﬁnd predictable patterns.
We make three observations based on this:
1)Commit Risk Prediction ( CommRisk ):Not all commits
are equal. For instance, a commit that changes or adds
a feature to code is more likely to fail tests than a
commit that makes simple changes to a conﬁguration
ﬁle. We therefore use machine-learning to determine
what characteristics make a commit less risky, or rather,
“safe” so that we can deploy it without any testing.
2)Test Outcome-based Correlation ( Te s t Co rr ):Several
developers work on the same code-base and, over time,
may create tests that are redundant or test very similar
functionality. Consequently, several test-pairs either pass
together or fail together. Such correlations exist mostly
within a test-suite but can also exist across test-suites.
Therefore, for a test-pair whose outcomes have been
almost perfectly correlated in the past, we run only one
of the two tests and predict the outcome of the second
with high conﬁdence.
3)Runtime-based Outcome Prediction ( RunP red ):Devel-
opers often combine testing slight variations of the same
functionality into a single test. We found that for several
such “composite” tests, the amount of time they took
to run was very well correlated with actual outcome.
These tests either fail instantly with the ﬁrst variation of
the functionality it tests failing, or pass after a longer
duration after testing all variations. The difference in
duration between passes and fails is even more pro-
nounced when the functionality being tested is variations
of the same API call, as API calls tend to inherently
take longer. Some tests display the converse behavior,
with tests passing early or failing after a long time. We
observed that such tests typically involve network calls,
and timeouts in the services they make the calls to result
in their failing after a prolonged duration, as opposed to
a quicker run if all the calls go smoothly and they pass.
Hence, for such tests, we predict outcomes after the test
has run for a certain period of time that we call a runtime
threshold .
III. A LGORITHM
In this section, we ﬁrst describe the log record history that
we use in our analysis and predictions. We then provide detailson how we learn rules for each of the three strategies outlined
in Section II-B.
A. Data
Our analysis uses two data-sources:
Test logs: The O365 system stores test-logs for 14 months, and
we use the logs from this entire duration for our analysis. Each
log entry contains information about a < commit, test > pair:
it contains the start-time of the test, the end-time of the test,
the host machine and the commit the test ran on, and the test
outcome. The outcome can be either “Passed” or “Failed”.
Commit logs: O365 stores 3 years of commit logs. Commit
logs store information about each commit, namely, what time
each commit was made, the developer who made the commit,
the reviewers who reviewed the commit, and the ﬁles modiﬁed.
B. Commit Risk Prediction
The objective of commit risk prediction is to determine,
at commit creation time, whether it is risky or safe. If it is
safe, we do not test the commit, thereby saving test-time and
resources.
Our approach is to use the test logs and commit logs to learn
a classiﬁer that labels a commit as risky orsafe . To train the
classiﬁer, we label commits in the following way: A commit
is risky if it causes at least one test to fail. A commit is safe
if all tests run on it pass. We used a total of 133 features to
characterize commits. Table II provides a brief outline. We
chose our features based on inferences derived in previous
work and our own ﬁndings with O365, outlined below:
•The type of change determines how risky it is. We capture
several features related to the ﬁles that a commit modiﬁes
or adds. For instance, we capture the number of ﬁles
changed, and ﬁle-types changed. We choose such features
based on previous ﬁndings that certain types of ﬁle
changes are more likely to cause bugs than others [9].
•Some components are riskier than others. We use history-
based attribution to determine code hotspots. From past
test-runs, for each component, we measure a “risk”
associated with a component, i.e., the fraction of times
that a change within a component lead to a test failing.
•The more often a ﬁle changes, the more risky it is [1],
[17]. For every ﬁle, we capture frequency of change in
the last 1, 2 and 6 months, and since the creation of the
ﬁle.
•Number of contributors to a ﬁle affects risk [1], [18]. For
each ﬁle, we also leverage previous work on ownership
to record the developers who are major and minor owners
[18] of the ﬁle, and their percentage of ownership. The
features are calculated over the last 1, 2 and 6 months,
and since the creation of the ﬁle.
•Developer and reviewer history We use features to
capture developer experience and reviewer experience.
A developer who has recently joined the group may be
more likely to introduce a bug. Similarly, experienced
reviewers will tend to ﬁnd more bugs and weed them out
at review-time. Some developers or their teams may work
410
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. TABLE II
AS AMPLING OF THE FEA TURES USED IN CommRisk AND THEIR DEFINITION
Feature Sample Deﬁnition #Features
File type & counts Boolean variables per ﬁle-type capture the set of ﬁle types that a commit changes. 63
Change frequency Maximum value, over all ﬁles changed, of the number of times a ﬁle is changed in
the last 6 months.4
Ownership Maximum value over number of owners for each ﬁle changed. 27
Developer & Reviewer History The number of commits made by the commit’s developer so far. 37
Component Risk The fraction of times this component has been tested and has failed at least one test. 2
on inherently “risky” parts of code, and have a greater
chance of producing regressions, while some reviewers
review primarily “risky” code changes. These features are
calculated over the last 1, 2 and 6 months, and since the
creation of the ﬁle.
Most of the characteristics we capture, such as change
frequency, are at the level of a ﬁle. Therefore, to create
commit-level features, we aggregate the ﬁle-speciﬁc values
using operators such as Max ,Average , and Median .
To train our model, we used a total of 133 features and a
total of 16,958 commits spanning 1 year (2017). We test our
model using 2 months of data, namely 3504 commits across
the months of January and February 2018. We used several
classiﬁers to evaluate our model. Table V shows the precision
values for each classiﬁer that we built. We focus on precision
in the case of commit risk because it is important to not allow
any high-risk change to slip into the deployment phase. The
FastTree [19] algorithm, a fast implementation of gradient-
boosted decision trees [20], achieved good precision, F1-score
and AUC values, while remaining easily interpretable. We
therefore use it in our implementation.
We found that the most important features provided were the
ﬁle types. Changes to .cs ﬁles and .xml ﬁles were much more
likely to cause a test to fail than changes to a .csproj or a .ini
ﬁle. Our ﬁndings from the user study described in Section VI-F
explain this behavior. Apart from ﬁle types, code hotspots and
code ownership-based metrics also added signiﬁcantly to the
accuracy of our model.
C. Test Outcome-based Correlation
The second step in reducing test load is to correlate the
outcome of various test-runs over time. We ﬁrst describe
some example reasons for the correlation and then outline our
algorithm.
Our investigation revealed that there are numerous pairs
of tests that always pass or fail together. We discovered the
following reasons for these correlations:
The functionality covered by one test can be a ﬁner-
grained version of another broader test. One such ac-
tual pair we discovered is TestListSize100 and
TestListSize .TestListSize100 tests the same func-
tionality as TestListSize , except the former test runs for
list sizes greater than 100. The test run logs show that all
instances of these two tests either passed together or failed
together. In other words, if the functionality works for sizesgreater than 100, it also work with a smaller size. Thus,
TestListSize100 passing implies TestListSize must
also pass.
Two tests depend on the same underlying functional-
ity. One such example is TestForwardMessage and
TestSaveAfterSend , both of which require the basic
“send email” functionality. If that common functionality is
currently failing, both tests fail together.
Two tests may be redundant versions of each other . Since
multiple developers work on the same large code-base over
time, they may also create tests that are redundant, or test tiny
variations of the same functionality. TestEntityUpdate
andTestEntityRepair : Both tests performed the same
actions but written as part of two different components by
two different sets of developers.
Algorithm 1 describes the Te s t Co rr algorithm. Broadly,
Te s t Co rr has three steps. First, we ﬁlter the list of test-pairs
that we consider to contain only pairs that have run together
at least 50 times, failed together at least 20 times, and passed
together at least 20 times (Line 5). Second, for every test-pair
A and B, we count the number of times they both passed (or
failed) on the same commit. If the ratio of this count to the
count of all the times tests A and B ran together on a commit
and test A passed (or failed), is greater than a conﬁdence
threshold Ctoc, we learn the rule, “Test A passes (fails) =⇒
Test B passes (fails)” (Lines 8 - 13). FastLane sets Ctocto the
relatively high value of 0.99. Our approach towards ﬁnding
correlated tests is therefore deliberately conservative.
Within the O365 dataset, in total, we found 270,550 test-
pairs. This number seems surprisingly large at ﬁrst-glance, but
given the scale of the system, it seems quite natural to ﬁnd
tests that validate a subset of another test’s functionality or
depend on some common service.
An alternative approach would be to use rule-mining algo-
rithms such as Apriori [21] to discover not just test-pairs but
larger clusters of tests that pass and fail together. However,
these algorithms are slow and, in fact, any cluster of ntests
will be represented by/parenleftbign
2/parenrightbig
rules of size 2. Hence, we use the
lightweight, pairwise approach outlined above.
D. Runtime-based Outcome Prediction
In this section, we outline the RunP red algorithm, de-
scribed in Algorithm 2. First, for each test t, we use techniques
derived from logistic regression to identify the duration where
the log-likelihood of separation between failures and passes
411
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. Algorithm 1 Te s t Co rr rule generation algorithm.
Input: T: Tests run in training period, Ctoc: conﬁdence
threshold
Output: rules
Initialization :rules =[ ]
1:for every pair of tests ti,tjinTwhere i/negationslash=jdo
2: Letri,jbe #commits where both tiandtjwere run
3: Letpi,jbe #commits where both tiandtjpassed
4: Letfi,jbe #commits where tiandtjwere both run
andtipassed
5: if(ri,j<50orpi,j<20orfi,j<20)then
6: continue
7: end if
8: Letpass fraction i,j=pi,j/fi,j
9: if(pass fraction i,j≥Ctoc)then
10: Add ( ti,tj)t orules
11: end if
12:end for
13:return rules
is maximized (Line 2). We call this duration the runtime
threshold for the test t,o rΔt
r.
Second, we check if this threshold achieves a “good sepa-
ration” between failed and passed test runs for the test t. Our
deﬁnition of a good separation is that the precision on the
training data should be greater than or equal to a precision
threshold Crop (Lines 5- 12). If so, we learn a runtime
threshold for that test (Lines 8 and 11). If the precision is
lower than the threshold, we do not learn any rules for this
test.
Algorithm 2 RunP red Rule Generation Algorithm
Input: T: Tests run in training period, Crop: precision thresh-
old
Output: rules pass ,rules fail
Initialisation :rules pass = [], rules fail =[ ]
1:for every test tinTdo
2: LetΔt
r= duration where log-likelihood is maximized
3: Letpt= #runs where t’s run-time ≥Ct
randt passed
4: Letft= #runs where t’s run-time ≥Ct
randtf a i l e d
5: LetPP t=pt
pt+ft
6: LetFP t=ft
pt+ft
7: if(PP t≥Crop)then
8: Add ( t,Δt
r)t orules pass
9: end if
10: if(FP t≥Crop)then
11: Add ( t,Δt
r)t orules fail
12: end if
13:end for
14:return rules pass ,rules fail
IV . S YSTEM DESIGN
In this section, we describe how FastLane integrates all three
approaches into a single system design.Figure 1 shows a ﬂow-graph that captures the way we
integrate our models. The process can broadly be divided into
two stages. First, FastLane continuously learns and updates
models using the three approaches described in Section III
(box shown in blue). Second, as and when developers make
commits, FastLane applies these models on the commits so
that we can reduce the amount of testing required (boxes
shown in green).
When a developer makes a commit the system uses the
model that CommRisk learned to determine if this commit
is safe (Step 1). If not, the system determines the tests to be
run on the commit using the component-to-test-case mapping
described in Section II-A. Next, the system uses the rules that
Te s t Co rr learn to determine test-pairs for which only one test
needs to be run (Step 2). For each applicable rule, FastLane
picks the test that has the lower average run-time and runs
it. Based on its outcome, FastLane infers the outcome of the
second test. Finally, the system applies RunP red , i.e., the
system monitors the run-time of each test, and if this is above
the pre-determined threshold, it stops the test and predicts its
outcome (Step 3).
We now describe Step 4 in Figure 1. Sometimes, our models
may make incorrect predictions. Moreover, code functionality
and tests can change over time. Hence, we need to contin-
uously retrain and update our models. Therefore, the system
runs a fraction of all tests for which FastLane made predictions
in the background. The administrator determines the value
of this fraction. By running these tests in the background,
and off the critical path, we make more judicial use of
our test resources. By comparing the actual results of the
runs with the outcomes predicted by the models, FastLane
continuously validates and updates the models and their rules
to be consistent with the current state.
While CommRisk ,Te s t Co rr , and RunP red are gener-
ically applicable to other systems as well, we recognize that
some are easier to apply than others. Not running tests on
a safe commit is easy to incorporate into any test process.
Incorporating RunP red , on the other hand, may not be
easy because this requires constant monitoring of test run-
time and also the ability to kill a test after the run-time
threshold is reached. We therefore evaluate all three strategies
independently as well and leave it to the service administrators
to determine which combination of the strategies can be
practically deployed in their environment.
V. I MPLEMENTA TION
We have implemented FastLane with a combination of C#
(using .NET Framework v4.5), SQL and a custom query
language. We use the ML.Net library [22] to build our
machine-learning models and evaluate them. Currently, the
implementation is approximately 18,000 lines of code.
Since FastLane requires information about various different
entities – commits, developers, reviewers and tests – a sig-
niﬁcant part of our implementation are data loaders for these
different types of data. We implemented loaders for various
source-control systems such as Git and others internal to our
412
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. Fig. 1. The integrated algorithm and FastLane test prediction ﬂow. This includes all three types of predictions FastLane performs (green boxes): 1. commit risk
prediction, 2. test outcome-based correlation, and 3. runtime-based outcome prediction. The blue box captures the learning functionality used to continuouslytrain FastLane.
organization. These loaders ingest source-code, code-versions
and commit histories and store this data in a normalizedfashion in an SQL database. Test logs, on the other hand,are stored in our organization’s proprietary Big-Data storagesystem.
FastLane’s source-data loaders and test log analyses run
once a day. During the learning phase, the commit risk
prediction model runs on 16,958 commits, which including
ﬁle and code-review information, is approximately 36 MB ofdata. This takes around 10 minutes. Test logs, on the otherhand, are 1.24 TB in size. FastLane takes 4 hours to learnrules for test outcome-based correlation and 1 hour to learnrules for runtime-based outcome prediction.
VI. E
V ALUA TION
In this section, we ﬁrst explain the data being used in
the evaluation of FastLane. Next, we describe our evaluationof how effective each of the three techniques are. Finally,we show the effectiveness of putting all three techniquestogether. To measure how effective FastLane is, we answerthe following questions:
1) How well does each technique perform as compared to
a random model?
2) How much test-time does each technique save while
maintaining accuracy?
3) How much test-time does FastLane save when we put
all three techniques together?
A. Data Setup
Table III summarizes our data logs. Our data broadly
consists of two types of logs: commit-speciﬁc and test-speciﬁc.Our commit-speciﬁc data consists of 20,462 commits madefrom Jan-2017 to Feb-2018 to the O365 code-base.
We use test logs from test-runs made between Jan-2017
and Feb-2018. Our test log dataset contains around 1000test-suites. Each test-suite contains around 60 to 70 tests onaverage, and the total number of individual tests is 74,213.The total number of test-runs over the entire time-period is63,377,917. Of these, 51,102 test-runs failed and 59,619,979passed. These fail and pass counts exclude test runs whoseTABLE III
SUMMARIZING OUR TEST AND TRAIN DATA
Training Data
Log Type Duration Size #rows
Commit Jan-2017 to
Dec-201735.65 MB 152,383
Test Jan-2017 toDec-2017 1.24 TB 54,784,935
Test Data
Log Type Duration Size #rows
Commit Jan-2018 toFeb-2018 8.05 MB 36,924
Test Jan-2018 toFeb-2018 264 GB 8,592,982
outcomes are neither passes nor fails, such as “not run” and
“missing”, which occur due to errors or resource constraints inthe internal testing infrastructure. For each test-run, we recordthe start-time, end-time, and test outcome.
We use data between Jan-2017 and Dec-2017 to train our
models, and data from Jan-2018 and Feb-2018 to evaluatethem. We believe this approach is more suitable than standard
cross-validation [23] because system properties change with
time. As shown in Section IV, we continuously learn and up-date our models. Therefore the suitable approach to evaluationis to train our models using past data and test on more recentdata.
B. Commit Risk Prediction
To evaluate CommRisk , we calculate the metrics deﬁned
in Table IV.
The Safe Precision (P
safe ) tells us what fraction of commits
thatCommRisk classiﬁed as safe are actually safe, while the
Risky Precision (P risky ) tells us what fraction of commits
CommRisk classiﬁed as risky are actually risky.
We trained our model using various classiﬁers. Table V sum-
marizes their performance. Note that the F1-score and AUC for
all models are comparable. FastLane uses FastTree [19], as theinterpretability of the decision trees allows us to reason about
413
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. TABLE IV
METRICS USED IN OUR EV ALUA TION OF CommRisk .TEXT IN
PARENTHESES ,Rpass AND Rfail APPLY TO Te s t Co rr AND RunP red
Metric Deﬁnition
TP : True Positivessafe commits (passed tests) classiﬁed
as safe (passed)
TN : True Negativesrisky commits (failed tests) classiﬁedas risky (failed)
FP: False Positivesrisky commits (failed tests) classiﬁedas safe (passed)
FN : False Negativessafe commits (passed tests) classiﬁedas risky (failed)
Psafe (Ppass ): Safe
(Pass) PrecisionTP
TP+FP
Prisky (Pfail ): Risky
(Fail) PrecisionTN
TN+FN
Rpass : Pass RecallTP
TP+FN
Rfail : Fail RecallTN
TN+FP
fts: Time Saved Frac-
tionrun−time of predicted tests
total time to run all tests
Fig. 2. AUC for the CommRisk model.
the model’s predictions. We set the number of trees learned to
2.
We now probe a little deeper into the performance of Fast-
Tree. The Area-Under-the-ROC-Curve (AUC) seen in Figure 2is 0.8357, which is signiﬁcantly higher than the baseline thata random classiﬁer would obtain, which is 0.5. Also, a modelwith an AUC greater than 0.7 for a given classiﬁcation problemis considered suitable to use for that problem [27]. This showsthat our classiﬁer is very effective at predicting whether acommit will cause tests to fail (risky) or not (safe).
Ultimately though, we are interested in the amount of test
run-time our model saves. This is a function of the number ofcommits we label as safe: the more commits our model labelssafe, the more time we save since we do not run tests onthese commits. However, labeling a larger number of commitsas safe may also mean that we potentially miss testing riskycommits. To investigate this trade-off, we now translate theoutput of the model to fraction of time saved, or f
ts. For this,
we determine the time to run all tests on all commits in ourtest-set that we predicted are safe. We then calculate this as afraction of total time required to run all our tests in the test-set.We then plot this as a function of safe precision.
Figure 3 shows how f
tsvaries with safe precision. This
Fig. 3. Safe Precision-%Time Saved Graph for the CommRisk model.
curve shows that we can save 8.7% of test-time while achiev-ing 89.7% pass precision. Note that this is at the commit
level as opposed to the test-case level. The pass precision of
CommRisk at the test-case level is 99.91%, and its overall
accuracy on the system is 99.99% as seen in Table VIII.This overall accuracy takes into account not only the testsCommRisk predicts will pass and should not be run, but also
the tests CommRisk chooses to run for commits it deems risky,
thereby obtaining the correct outcomes for those tests.
C. Test Outcome-based Correlation
We now evaluate how Te s t Co rr performs. The metrics
used in this evaluation are the same as those used in Table IV,
except instead of commits being classiﬁed as safe orrisky,
it is now test outcomes that are classiﬁed as pass orfail
respectively. So safe and risky precision are now referred toas pass and fail precision. We also introduce two new metrics,pass recall (R
pass ) and fail recall (R fail).
As part of our sensitivity analysis, we varied the value of
Ctoc(conﬁdence threshold deﬁned in Section III-C) between
1 and 0.8 to evaluate different points in the design-space. Aswe reduce the value of C
toc, we expect to ﬁnd more correlated
test-pairs using Algorithm 1. This implies we have more rulesto apply and will therefore save more test-time. However, thismay also decrease our precision values, since rules learnedat lower C
tocvalues are inherently less conﬁdent than rules
learned at higher Ctocvalues.
Table VI shows this effect quantitatively. We also evaluated
a random straw-man for the same value of time saved, i.e.,f
TS, and present its precision and recall numbers. As we
decreased Ctocfrom 1 to 0.8, we noticed that pass precision
(Ppass ) decreases while ftsincreases. This is as expected.
However, the fail precision (P fail) initially increases asCtoc
is reduced, and reaches its peak when Ctocis at 0.9. This is
counter-intuitive as we expect less conﬁdent rules to be lessprecise. However, the number of test failures predicted are ofthe order 10
2, while the number of test passes predicted are of
the order 105, so fail precision is more prone to random effects
than pass precision. This imbalance in number of predictionsis a reﬂection of the low percentage of test runs which fail, asdiscussed in the overview of our data in Section V.
After discussions with product groups, we settled on 0.99 as
theC
tocvalue for our model which provides the desired trade-
414
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. TABLE V
PERFORMANCE OF DIFFERENT CLASSIFIERS FOR COMMIT RISK PREDICTION
Model Psafe Prisky F1-Score AUC
Logistic Regression 89.68% 67.96% 0.7915 0.8357
Linear SVM [24] 80.22% 70.55% 0.7955 0.8346
Binary Neural Network 90.05% 67.67% 0.7893 0.8377
Local-Deep SVM [25] 89.54% 68.37% 0.7949 0.8238
XGBoost [26] 88.22% 68.58% 0.7950 0.8445
FastTree (Boosted Trees) [19] 89.72% 68.49% 0.7962 0.8357
TABLE VI
Te s t Co rr RESULTS SHOW HOW THE FRACTION OF TIME SAV E D (fTS)V ARIES WITH PASS PRECISION (Ppass )FOR DIFFERENT VALUES OF
CONFIDENCE THRESHOLD (Ctoc)
Ctoc fTS #rules Ppass (FastLane) Ppass (random) Pfail (FastLane) Pfail (random) Rpass Rfail
1 6.55% 179,046 99.97% 99.72% 84.28% 0.28% 99.98% 78.82%
0.99 10.38 % 270,550 99.95 % 99.72 % 88.66 % 0.28 % 99.99% 68.04%
0.98 11.14% 356,504 99.93% 99.72% 88.85% 0.28% 99.99% 56.02%
0.95 12.26% 567,968 99.87% 99.72% 88.11% 0.28% 99.99% 41.91%
0.93 12.48% 625,684 99.86% 99.72% 88.34% 0.28% 99.99% 40.76%
0.90 12.90% 716,626 99.82% 99.72% 91.44% 0.28% 99.99% 29.12%
0.85 13.11% 779,108 99.80% 99.72% 90.12% 0.28% 99.99% 23.03%
0.80 13.20% 818,708 99.79% 99.72% 90.05% 0.28% 99.99% 21.27%
off between time saved (10.38%) and precision (99.95%).
Note, this value can be tuned for any service as desired by
the administrators. Overall accuracy on the system, as deﬁned
at the end of Section VI-B, and seen in Table VIII, is 99.99%.
D. Runtime-based Outcome Prediction
Finally, we evaluate the RunP red technique. We use the
same metrics used in Section VI-C in this evaluation. In this
case, we perform our sensitivity analysis by varying the value
of the precision threshold Crop, as deﬁned in Section III-D,
to determine how the precision and recall values are affected
as we make more aggressive predictions.
Table VII shows that we can save 1.57% of time while
maintaining 99.96% pass precision, when Crop is 1, which
is the threshold we chose based on discussions with product
groups. Again, depending on system requirements, FastLane
can tune the Crop value to either save more time or improve
pass precision. Overall accuracy on the system when Cropis 1,
as deﬁned at the end of Section VI-B, and seen in Table VIII,
is 99.99%.
E. Overall Gains
Finally, we show how the models perform when we ap-
ply them to our data in different combinations. As seen in
Table VIII, FastLane saves 18.04% of test-time when it com-
bines CommRisk ,Te s t Co rr andRunP red as described in
Section IV, while maintaining an overall accuracy of 99.99%.
The table also shows how much time FastLane saves when it
uses other combinations of the three techniques. For instance,
suppose an administrator decides to use only CommRisk .
She will still save 8.63% of test-time.The other point to note here is that the savings of
CommRisk and Te s t Co rr are fairly independent of each
other. Individually, they save 8.63% and 10.38% test-time
respectively. If the set of tests they predicted outcomes for
were completely disjoint, they would have saved 19.01% test
time. In reality, they save 17.84% together, which is only
1.17% lower.
Similarly, savings from CommRisk and RunP red are
independent too. The sum of their savings is 10.2% while
combined, they save 10.11%, very close to the sum.
On the other hand, Te s t Co rr andRunP red have a larger
common set of tests that they predict outcomes for. Combined,
they save 10.6%, only 0.22% more than Te s t Co rr alone.
This may suggest that for O365, Te s t Co rr largely subsumes
RunP red . However, this need not be true since Te s t Co rr and
RunP red are very different techniques. First, in a different
environment or as the system evolves with time, RunP red
may save on a disjoint set of tests. Second, testers in different
environments may ﬁnd that RunP red is a more acceptable
technique than Te s t Co rr since Te s t Co rr does not run the
test at all and is therefore more aggressive, whereas RunP red
makes predictions after starting the test.
F . User Study
We performed a user-study to help conﬁrm the validity
of our models, and understand why each of our approaches
works. We reached out to 100 developers, roughly 33 for
each technique, and asked them whether the predictions by
FastLane was correct or not — if yes, why, and if no, why
not? 70 of the 100 developers responded to the study. In this
section, we summarize their comments.
415
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. TABLE VII
RunP red RESULTS SHOW HOW THE FRACTION OF TIME SAV E D (fts)V ARIES WITH PASS PRECISION (Ppass )FOR DIFFERENT VALUES OF PRECISION
THRESHOLD (Crop)
Crop fTS #tests predicted Ppass (FastLane) Ppass (random) Pfail (FastLane) Pfail (random) Rpass Rfail
1 1.57% 162905 99.96% 99.72% 57.89% 0.28% 99.99% 23.91%
0.99 4.34% 272451 99.92% 99.72% 61.11% 0.28% 99.99% 9.24%
0.98 4.92% 280226 99.90% 99.72% 61.11% 0.28% 99.99% 7.17%
0.95 5.35% 286552 99.86% 99.72% 58.54% 0.28% 99.99% 5.73%
0.93 5.43% 289513 99.85% 99.72% 20.51% 0.28% 99.97% 5.30%
0.90 5.64% 293381 99.85% 99.72% 43.26% 0.28% 99.97% 14.72%
0.85 5.70% 294491 99.84% 99.72% 31.75% 0.28% 99.93% 17.48%
0.80 5.77% 295867 99.83% 99.72% 34.12% 0.28% 99.91% 20.34%
TABLE VIII
OVERALL GAINS OF USING DIFFERENT COMBINA TIONS OF CommRisk ,Te s t Co rr AND RunP red ,WHEN COMBINED AS DESCRIBED IN SECTION IV
Model fTS %Tests Predicted Accuracy(Predicted) Accuracy(Overall)
CommRisk 8.63% 6.73% 99.88% 99.99%
T estCorr 10.38% 7.26% 99.94% 99.99%
RunP red 1.57% 2.05% 99.95% 99.99%
CommRisk &T estCorr 17.84% 12.37% 99.90% 99.99%
CommRisk &RunP red 10.11% 8.42% 99.89% 99.99%
T estCorr &RunP red 10.60% 7.46% 99.94% 99.99%
CommRisk & TestCorr & RunPred 18.04% 12.57% 99.90% 99.99%
Commit Risk Prediction: Modiﬁcations to .csproj ﬁles were
often simple dependency updates, and errors in these changes
would be detected at build time without the need for testing.
Also, changes to .ini ﬁles tended to enable an older, existing
feature for different user groups. Since the feature was intro-
duced earlier, it had already been tested, and changes to the
.ini ﬁles could be checked in without testing. These reasons
were mostly captured by our ﬁle-speciﬁc features described in
Section III-B and Table II.
Test Outcome-based Correlation: Many tests test the same
functionality in very similar ways. Some tests fail together due
to outages in a service that both tests depend on, for instance,
if both tests used a speciﬁc database setup. Also, tests fail
together if they have similar setup processes: if the setup for
one test fails, the other one will fail too. These ﬁndings are
in addition to the reasons for correlation that we stated in
Section III-C.
Runtime-based Outcome Prediction: The difference in du-
rations of pass and fail runs were seen primarily in tests that
make network calls. Several such test-cases validate REST
APIs and start by fetching initial metadata about the REST
service, or connect and make requests to a database. These
requests tend to fail if the service is down, or go on to pass
the test after making several time-consuming calls. The test
environment being polluted by other tests is another reason
for early failure.
VII. T HREA TS TOVALIDITY
A. External V alidity: Generality of our Approach
While FastLane has shown the applicability of our tech-
niques on O365’s logs, we believe these techniques are univer-sally applicable to any large service that uses a CI/CD pipeline,
as long as we have rich logs of test and commit history. The
properties we observe and build upon - commit risk prediction,
inter-test correlation, and intra-test correlation - are inherent to
several large-scale software development processes [8], [28].
Moreover, all our approaches use generic machine-learning
and statistics techniques that do not make use of any speciﬁc
properties or characteristics of O365.
B. Internal V alidity
Prior work [29], [30] has investigated the problem of ﬂaky
tests and these could affect the validity of our models and
rules. O365 has several ﬂaky test detection mechanisms that
use similar techniques as explored in previous work. We
therefore apply these detection mechanisms prior to running
our prediction model to avoid losing accuracy.
We also recognize that a trade-off exists between maintain-
ing high precision and time-saved. To that end, we provide test
administrators with various knobs for each of our techniques
which can be tuned to obtain the time-saved versus precision
trade-off that suits their needs best. For instance, FastLane
uses Te s t Co rr with a correlation requirement of 99%. An
administrator can dial this up to 100% if they want to minimize
missing failures. In the case of O365, this decreases our time-
saved from 10.38% to 6.55% (Table VI). On the other hand,
they can turn this down too if they can accept a lower level
of pass precision.
VIII. R ELA TED WORK
Prior work has addressed problems related to Defect Pre-
diction, Test Selection/Minimization and Test Log Analysis.
416
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. In this section, we discuss these three aspects in the context
of FastLane.
A. Defect Prediction
The features that we use in our commit risk prediction
module have been derived from a vast body of previous work
in the ﬁeld of software defect prediction. Zimmermann et al.
have shown that defects can be attributed at component or
ﬁle-level [31], [32]. Our commit risk predictor therefore uses
ﬁle-level and directory-level “risk” as features. Past research
has also shown that process-level features are more effective at
predicting software defects than code-level features [28], [33].
We therefore use such process-level features in CommRisk .
Basili et al. investigated the ability of the classic CK Object-
oriented design metrics [34] to predict fault-proneness [35].
Additionally, Nagappan et al. [17] have shown how code churn
correlates with ﬁeld failures. Jaafar et al. [36] investigating
the effect of time found that classes having a speciﬁc lifetime
model are signiﬁcantly less fault-prone than other classes, and
faults ﬁxed by maintaining co-evolved classes, are signiﬁcantly
more frequent than others. We use similar approaches in our
analysis as well, investigating changes based on when they
happened and also learning from churn-based features. Bird et
al. [18] have shown that code ownership (or the lack thereof)
has a direct correlation with software defects. Our learner
therefore also uses ownership-based features.
Our work takes inspiration from this vast body of work to
develop suitable input features, and builds upon it with our
speciﬁc application in mind: test minimization and resource
savings. Therefore, we use commit risk prediction as a tool
towards test minimization. Moreover, FastLane does not derive
its deﬁnition of risk from ﬁeld-level bugs and reports, but
rather from test failures.
B. Test Minimization
A large body of work concentrates on the problem of test
selection and minimization [7], [8], [10]–[16]. In their seminal
work Rothermel et al. [5], [7] use several techniques for
using test execution information to prioritize test cases for
regression testing, by ordering test cases based on their total
coverage of code components; ordering based on the coverage
of components not previously covered; and ordering based on
their estimated ability to reveal faults. The techniques studied
improved the rate of fault detection of test suites. Elbaum et
al. [4] advance this further and show that ﬁne-granularity tech-
niques outperformed coarse-granularity techniques by a small
margin. Using fault-proneness techniques produced relatively
small improvements over other techniques in terms of rate
of fault detection and in general the effectiveness of various
techniques can vary signiﬁcantly across target programs. V a-
habzadeh et al. [8] have recently proposed a technique that
uses code instrumentation to analyze and remove ﬁne-grained
redundancy within tests.
These techniques mostly rely on code-coverage as the
main criterion. While effective, applying them continuously
as code and tests change can be a heavyweight process andnot easily applicable to large dynamic software code-bases
such as O365. FastLane, on the other hand, uses lightweight,
machine-learning based techniques to perform test minimiza-
tion. Moreover, FastLane, unlike previous work, gives us
the ability to tune thresholds at runtime: depending on our
accuracy requirements, we can turn FastLane’s thresholds to
achieve different time-accuracy trade-offs. Finally, previously
proposed techniques based on code-coverage can generate the
list of tests that are input to FastLane. Thus the two techniques
can be used in tandem to obtain better test minimization. It
is also important to note that FastLane differs from work on
code execution classiﬁcation [37], which tries to classify the
outcome of a program execution as a pass or fail after it
has completed, while FastLane predicts the outcome of a test
before it has completed.
From an industrial stand point, in recent work at Google [1],
the authors empirically leverage the relationship between
Google’s code, test cases, developers, programming languages,
and code-change frequencies, to improve Google’s CI and
development processes. They ﬁnd that very few tests fail and
those failures are generally “closer” to the code they test and
frequently modiﬁed code breaks more often. We use similar
churn and test execution metrics in our analysis. Work at Mi-
crosoft [38] describes Echelon, a test prioritization system that
works on binary code to scale to large binaries and to enable
the testing of large systems like Windows. Echelon utilizes a
binary matching system that can compute the differences at a
basic block granularity between two versions of the program
in binary form to prioritize tests. All such techniques can be
used in tandem with FastLane.
C. Test Log Analysis
Andrews [39] ﬁrst investigated the use of log ﬁles for
testing, presenting a framework for automatically analyzing
log ﬁles, and deﬁning a language for specifying analyzer
programs. The language permitted compositional, compact
speciﬁcations of software for unit and system testing. Recent
work has analyzed test logs and described the correlation
between test failures and the “closeness” of code changed and
frequency of code change [1]. However our concentration is
on analyzing tests logs and using such insights to effectively
save test-time.
IX. C ONCLUSION
FastLane is a system that saves test-time in large-scale
service development pipelines. It uses machine-learning on
large volumes of logs to develop accurate models that predict
test outcomes. With a combination of commit risk prediction,
test outcome correlation, and runtime-based outcome predic-
tion, we show that for a large-scale email and collaboration
service, FastLane saves 18.04% of test-time while ensuring a
test outcome accuracy of 99.99%.
X. A CKNOWLEDGEMENTS
We would like to thank Rob Land, Daniel Guo, B. Ashok,
Sumit Asthana, Chetan Bansal and Sonu Mehta for their valu-
able suggestions and help building and evaluating FastLane.
417
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] A. Memon, Z. Gao, B. Nguyen, S. Dhanda, E. Nickell, R. Siemborski,
and J. Micco, “Taming google-scale continuous testing,” in ICSE SEIP
Track , 2017.
[2] K. Shvachko, H. Kuang, S. Radia, and R. Chansler,
“The hadoop distributed ﬁle system,” in Proceedings of the
2010 IEEE 26th Symposium on Mass Storage Systems and
Technologies (MSST) , ser. MSST ’10. Washington, DC, USA:
IEEE Computer Society, 2010, pp. 1–10. [Online]. Available:
http://dx.doi.org/10.1109/MSST.2010.5496972
[3] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley,
M. J. Franklin, S. Shenker, and I. Stoica, “Resilient distributed
datasets: A fault-tolerant abstraction for in-memory cluster computing,”
in Proceedings of the 9th USENIX Conference on Networked
Systems Design and Implementation , ser. NSDI’12. Berkeley, CA,
USA: USENIX Association, 2012, pp. 2–2. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2228298.2228301
[4] S. Elbaum, M. J. Harrold, and G. Rothermel, “Test case prioritization:
a family of empirical studies,” in IEEE Transactions on Software
Engineering , vol. 28, no. 10, Feb. 2002, pp. 159–182.
[5] C. C. G. Rothermel, R. H. Untch and M. J. Harrold, “Prioritizing
test cases for regression testing,” in IEEE Transactions on Software
Engineering , vol. 27, no. 10, Oct. 2001, pp. 929–948.
[6] Q. Luo, K. Moran, and D. Poshyvanyk, “A large-scale empirical com-
parison of static and dynamic test case prioritization techniques,” in
Proceedings of the 2016 24th ACM SIGSOFT International Symposium
on F oundations of Software Engineering , ser. FSE ’16, Nov. 2016.
[7] G. Rothermel, M. J. Harrold, J. V . Ronne, and C. Hong, “Empirical stud-
ies of test-suite reduction. software testing, veriﬁcation and reliability,”
inSoftware Testing, V eriﬁcation and Reliability , 2002.
[8] A. V ahabzadeh, A. Stocco, and A. Mesbah, “Fine-grained test min-
imization,” in ACM/IEEE 40th International Conference on Software
Engineering , 2018.
[9] A. Hindle, D. M. German, and R. Holt, “What do alrge commits tell us?
a taxonomical study of large commits,” in Proceedings of MSR , 2008.
[10] L. Zhang, “Hybrid regression test selection,” in Proceedings of ICSE ,
2018.
[11] J. Black, E. Melachrinoudis, and D. Kaeli, “Bi-criteria models for all-
uses test suite reduction,” in Proceedings. 26th International Conference
on Software Engineering , May 2004, pp. 106–115.
[12] M. J. Harrold, R. Gupta, and M. L. Soffa, “A methodology for
controlling the size of a test suite,” ACM Trans. Softw. Eng.
Methodol. , vol. 2, no. 3, pp. 270–285, Jul. 1993. [Online]. Available:
http://doi.acm.org/10.1145/152388.152391
[13] J. A. Jones and M. J. Harrold, “Test-suite reduction and prioritization for
modiﬁed condition/decision coverage,” IEEE Transactions on Software
Engineering , vol. 29, no. 3, pp. 195–209, March 2003.
[14] T. Chen and M. Lau, “A new heuristic for test suite
reduction,” Information and Software Technology , vol. 40,
no. 5, pp. 347 – 354, 1998. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0950584998000500
[15] S. Y oo and M. Harman, “Regression testing minimization, selection and
prioritization: A survey,” Softw. Test. V erif. Reliab. , vol. 22, no. 2, pp. 67–
120, Mar. 2012. [Online]. Available: http://dx.doi.org/10.1002/stv.430
[16] D. Jeffrey and N. Gupta, “Improving fault detection capability by selec-
tively retaining test cases during test suite reduction,” IEEE Transactions
on Software Engineering , vol. 33, no. 2, pp. 108–123, Feb 2007.
[17] N. Nagappan and T. Ball, “Using software dependencies and churn
metrics to predict ﬁeld failures: An empirical case study,” in Proceedings
of ESEM , 2007.
[18] C. Bird, N. Nagappan, B. Murphy, H. Gall, and P . T. Devanbu, “Don’t
touch my code!: examining the effects of ownership on software quality,”
inProceedings of FSE , 2011.
[19] Microsoft. Fasttree (gradient boosted trees). [Online].
Available: https://docs.microsoft.com/en-us/machine-learning-server/r-
reference/microsoftml/rxfasttrees
[20] J. H. Friedman, “Greedy function approximation: A gradient boosting
machine,” Annals of Statistics ,vol. 29, pp. 1189–1232, 2000.
[21] R. Agrawal and R. Srikant, “Fast algorithms for mining association
rules in large databases,” in Proceedings of the 20th International
Conference on V ery Large Data Bases , ser. VLDB ’94. San Francisco,
CA, USA: Morgan Kaufmann Publishers Inc., 1994, pp. 487–499.
[Online]. Available: http://dl.acm.org/citation.cfm?id=645920.672836[22] Microsoft. Ml.net machine learning framework. [Online].
Available: https://www.microsoft.com/net/learn/apps/machine-learning-
and-ai/ml-dotnet
[23] C. M. Bishop, Pattern Recognition and Machine Learning (Information
Science and Statistics) . Berlin, Heidelberg: Springer-V erlag, 2006.
[24] S. Shalev-Shwartz, Y . Singer, N. Srebro, and A. Cotter, “Pegasos: primal
estimated sub-gradient solver for svm,” Mathematical Programming ,
vol. 127, no. 1, pp. 3–30, Mar 2011. [Online]. Available:
https://doi.org/10.1007/s10107-010-0420-4
[25] C. Jose, P . Goyal, P . Aggrwal, and M. V arma, “Local
deep kernel learning for efﬁcient non-linear svm prediction,”
in Proceedings of the 30th International Conference on
International Conference on Machine Learning - V olume 28 , ser.
ICML’13. JMLR.org, 2013, pp. III–486–III–494. [Online]. Available:
http://dl.acm.org/citation.cfm?id=3042817.3042991
[26] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”
inProceedings of the 22Nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining , ser. KDD ’16. New
Y ork, NY , USA: ACM, 2016, pp. 785–794. [Online]. Available:
http://doi.acm.org/10.1145/2939672.2939785
[27] C. Tantithamthavorn, A. E. Hassan, and K. Matsumoto, “The
impact of class rebalancing techniques on the performance and
interpretation of defect prediction models,” 2018. [Online]. Available:
http://arxiv.org/abs/1801.10269
[28] F. Rahman and P . Devanbu, “How, and why, process metrics are better,”
inProceedings of ICSE , 2013.
[29] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov, “An empirical analysis of
ﬂaky tests,” in Proceedings of the 22Nd ACM SIGSOFT International
Symposium on F oundations of Software Engineering , ser. FSE 2014.
New Y ork, NY , USA: ACM, 2014, pp. 643–653. [Online]. Available:
http://doi.acm.org/10.1145/2635868.2635920
[30] A. V ahabzadeh, A. M. Fard, and A. Mesbah, “An empirical study of
bugs in test code,” in 2015 IEEE International Conference on Software
Maintenance and Evolution (ICSME) , Sept 2015, pp. 101–110.
[31] T. Zimmermann, R. Premraj, and A. Zeller, “Predicting defects for
Eclipse,” in Proceedings of PROMISE , 2007.
[32] S. Kim, T. Zimmermann, E. J. W. Jr, and Z. Zeller, “Predicting faults
from cached history,” in Proceedings of ICSE , 2007.
[33] R. Moser, W. Pedrycz, and G. Succi, “A comparative analysis of
the efﬁciency of change metrics and statis code attributes for defect
prediction,” in Proceedings of ICSE , 2008.
[34] S. R. Chidamber and C. F. Kemerer, “A metrics suite for object oriented
design,” in IEEE Transactions on Software Engineering , vol. 20, no. 6,
Jun. 1994, pp. 476–493.
[35] V . R. Basili, L. C. Briand, and W. L. Melo, “A validation of object-
oriented design metrics as quality indicators,” in IEEE Transactions on
Software Engineering , vol. 22, no. 10, 1996, pp. 751–761.
[36] F. Jaafar, S. Hassaine, Y .-G. Gu ´eh´eneuc, S. Hamel, and B. Adams,
“On the relationship between program evolution and fault-proneness: An
empirical study,” in Software Maintenance and Reengineering (CSMR)
2013 17th European Conference on , 2013, pp. 15–24.
[37] D. Hao, X. Wu, and L. Zhang, “An empirical study of execution-data
classiﬁcation based on machine learning,” in SEKE , 2012.
[38] A. Srivastava and J. Thiagarajan, “Predicting defects using change
genealogies,” in Proceedings of the 2002 ACM SIGSOFT international
symposium on Software testing and analysis , ser. ISSTA ’02, 2002.
[39] J. Andrews, “Testing using log ﬁle analysis: Tools, methods, and issues,”
inProceedings of the 13th IEEE international conference on Automated
software engineering , ser. ASE’98, 1998.
418
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:00:11 UTC from IEEE Xplore.  Restrictions apply. 