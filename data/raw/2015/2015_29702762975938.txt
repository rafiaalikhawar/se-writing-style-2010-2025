Statistical Analysis of Large Sets of Models
Ã–nder Babur
Eindhoven University of Technology
5600 MB Eindhoven, The Netherlands
O.Babur@tue.nl
ABSTRACT
Many applications in Model-Driven Engineering involve pro-
cessing multiple models, e.g. for comparing and merging of
model variants into a common domain model. Despite many
sophisticated techniques for model comparison, little atten-
tion has been given to the initial data analysis and ltering
activities. These are hard to ignore especially in the case of a
large dataset, possibly with outliers and sub-groupings. We
would like to develop a generic approach for model com-
parison and analysis for large datasets; using techniques
from information retrieval, natural language processing and
machine learning. We are implementing our approach as
an open framework and have so far evaluated it on public
datasets involving domain analysis, repository management
and model searching scenarios.
CCS Concepts
Computing methodologies !Cluster analysis;
Software and its engineering !Model-driven soft-
ware engineering; Software reverse engineering;
Keywords
Model-driven engineering, model comparison, vector space
model, clustering
1. INTRODUCTION
Model-Driven Engineering (MDE) promotes the use of
models and metamodels as rst-class artefacts to tackle the
complexity of software systems [10]. As MDE is applied for
larger problems, the complexity, size and variety of models
increase. With respect to model size, the issue of scalability
for models has been pointed out by Kolovos et al. [10] as a
limiting factor. However, scalability with respect to model
variety and multiplicity (i.e. dealing with a large number of
dierent models) is also an important issue, and has been
diagnosed by Klint et al. [9] as an interesting aspect to ex-
plore. There are many approaches to fundamental opera-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full cita-
tion on the ï¬rst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciï¬c permission
and/or a fee. Request permissions from permissions@acm.org.
ASE â€™16 September 3â€“7, 2016, Singapore, Singapore
c2016 ACM. ISBN 978-1-4503-3845-5/16/09. . . $15.00
DOI:http://dx.doi.org/10.1145/2970276.2975938tions such as model comparison [16] applied to problems
such as model merging [6]; however those mainly focus on
pairwise and 'deep' comparison of models for a very small
number of models. Rubin et al. [15] further discuss the in-
adequacy of pairwise comparison for multiple models and
propose an N-way model merging algorithm. Indeed, many
problems in MDE involve processing a large number of mod-
els. Some examples are domain model recovery from several
candidate models [9] and family mining for Software Prod-
uct Lines (SPL) from model variants [8].
To make it concrete, consider the case where a common
model is reverse engineered out of several candidate model
variants. We argue that, as the number and variety of in-
put models get larger, the initial data analysis and ltering
step gets more relevant. This in turn calls for a need to in-
spect the dataset for an overview, identify potential relations
such as proximities, cluster formations and outliers. This
information can be used for ltering noisy data, for group-
ing models, or even for determining the order of processing
for a pairwise model merging or SPL generation algorithm
(see [15] for a discussion on how pairwise comparison order
aects the outcome of merging multiple models).
Having set the scene, we move to our main objectives. The
purpose of this study is to answer the following questions:
RQ1. How can we represent models for large-scale
comparative analysis?
RQ2. How can we analyse and compare a large set of
models as a whole, avoiding pairwise comparison?
2. RELATED WORK
Only a few comparison techniques consider multiple input
models without pairwise comparisons, such as N-way merg-
ing [15]. Concept mining [1] uses NLP to cluster concepts.
Another technique builds domain ontologies as the inter-
section of graphs of APIs [14], but does not focus on the
statistical dimension of the problem. A similar technique is
applied specically for business process models using process
footprints [7], and lacks the genericness of our approach.
Clustering is considered in the software engineering com-
munity mostly within a single body of code [11] or model [17].
A very recent approach, which we encountered after publish-
ing our early work, is presented by Basciani et al. [4]. They
share most of our objectives and some of the techniques we
use (i.e. vector representation and clustering), though fo-
cusing on repository management. Bislimovska et al. also
propose information retrieval techniques for indexing and
searching WebML models [5].
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
ASEâ€™16 , September 3â€“7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2975938
888
3. METHOD FOR MODEL CLUSTERING
In this section we outline our approach after a brief but
necessary summary of the underlying techniques.
3.1 Preliminaries
Information Retrieval [12] deals with eectively indexing,
analyzing and searching various forms of content including
natural language text documents. As a rst step for doc-
ument retrieval in general, documents are collected and in-
dexed via some unit of representation. Index construction
can be implemented using the vector space model (VSM)
with the following major components: (1) a vector repre-
sentation of occurrence of the vocabulary in a document,
named term frequency , (2) zones (e.g. 'author' or 'title'),
(3) weighting schemes such as inverse document frequency
(idf), and zone weights, (4) Natural Language Processing
(NLP) techniques for handling compound terms, detecting
synonyms and semantically related words.
The VSM allows transforming each document into an n-
dimensional vector, thus resulting in an mnmatrix for
mdocuments. Over the VSM, document similarity can be
dened as the distance (e.g. Euclidean or Cosine) between
vectors. These can be used for identifying similar groups of
documents in the vector space. This unsupervised machine
learning (ML) technique is called clustering. Among many
clustering methods [12], there is a major distinction between
at clustering, where a at cluster labelling is performed,
and hierarchical clustering, where a hierarchy of proximities
is produced. Specically, hierarchical agglomerative cluster-
ing (HAC) outputs a nested tree structure called dendro-
gram , suitable for visualization and manual inspection.
Finally, n-grams are used in computational linguistics to
build probabilistic models of natural language text [13], e.g.
for estimating the next word given a sequence, or comparing
text collections based on their n-gram proles. In essence,
n-grams represent a linear encoding of structural context.
3.2 Conceptual Overview
Set	 Â of	 Â models	 Â 
Metamodel	 Â 
Features	 Â 
NLP	 Â 
Tokeniza8on	 Â 
	 Â Matching	 Â scheme	 Â Weigh8ng	 Â scheme	 Â 
VSM	 Â 
Distance	 Â calcula8on	 Â 
Clustering	 Â 
Dendrogram	 Â 
Automated	 Â extrac8on	 Â Inferred	 Â clusters	 Â Extrac8on	 Â 	 Â scheme	 Â 
Filtering	 Â 
Synonym	 Â detec8on	 Â â€¦	 Â 
Data	 Â selec8on,	 Â ï¬ltering	 Â Clone	 Â detec8on	 Â 
â€¦	 Â 
Classiï¬ca8on	 Â 
Analysis	 Â 
â€¦	 Â 
Repository	 Â management	 Â Domain	 Â analysis	 Â 
â€¦	 Â N-Â­â€grams	 Â 
Metrics	 Â 
â€¦	 Â 
Manual	 Â inspec8on	 Â 
Figure 1: Overview of our approach.
In this section we describe our approach for the statistical
analysis of models. An overview of the conceptual frame-
work is given in Figure 1. The base framework is inspired
by IR-based and statistical techniques for comparing docu-
ments as summarized in Section 3.1. As input to our frame-
work, we obtain a set of models of the same type, e.g. Ecoremetamodels in our case studies. The major steps of the
workow are outlined in the following paragraphs.
Feature extraction.
The approach starts with the metamodel-based extraction
of features from the models. We use features as the general
term for any piece of information to be used statistical anal-
ysis. Currently the features supported by the framework are
are typed identiers of model elements (e.g. class, attribute
names), metrics (e.g. number of attributes for a class) and
an n-grams of those for capturing the structural relations
between model elements (e.g. a bigram for n= 2, encoding
two classes with an association in between). The extraction
can be a simple one based on the underlying graph, or a
domain-specic one exploiting the semantics of the model
(e.g. special handling of inheritance).
Feature comparison and NLP techniques.
The framework has several parameters on (1) using NLP
techniques for comparing model identiers, and (2) schemes
for comparing and weighting features. NLP techniques (to-
kenization, stemming, synonym checking -most notably us-
ing WordNet1- to name a few) are essential for handling real
datasets as model identiers will typically have compound
names, typos, acronyms, synonyms and so on. For compar-
ing features, the framework similarly oers a set of matching
schemes such as checking types of identiers (e.g. Class State
vs. Attribute State) or just ignoring them; choosing an n-
gram similarity method (e.g. simple vs. maximum similar
subsequence), etc. Other options are the weighting of dif-
ferent types of features (e.g. classes have higher weight than
attributes), idf weighting, and context weight for n-grams.
Computing the vector space model.
Here each model, consisting of a set of features, is com-
pared against the maximal set of features collected from all
models. The result is a matrix, similar to the term frequency
matrix in IR, where each model (or model fragment if aiming
for lower granularity of extraction e.g. for detecting clones)
is represented by a vector in a high dimensional vector space.
Consequently, we reduce the model similarity problem into
a distance measurement of the corresponding vectors.
Distance measurement and clustering.
The framework allows several parameters for (1) distance
measures among vectors such as Euclidean and Cosine dis-
tance, (2) dierent clustering algorithms such as k-means
and HAC, and (3) further clustering-specic parameters such
as linkage. The clusters in the dataset can be automatically
computed to be used directly e.g. for the purpose of data
selection and ltering. Also particularly for HAC, the re-
sulting dendrogram can be visualized and inspected manu-
ally for giving an insight into the dataset. The framework is
planned to be further extended for other types of statistical
analysis such as classication and multidimentional scaling.
Quantitative evaluation of the parameter space.
The framework natively supports quantitative evaluation
of the selected parameter space (e.g. dierent distance mea-
sures) and provides a range of external cluster validity mea-
sures (based on a reference clustering) such as F measure
and purity. Clustering is done for permutation of all the
values in the parameter space, and the resulting accuracies
are displayed in the form of box plots for inspection. See
Section 4.3 for an application in one of our case studies.
1https://wordnet.princeton.edu/
8894. PROGRESS AND EVALUATION SO FAR
We have implemented our approach in Java for the fea-
ture extraction, NLP and VSM construction, and R2for the
statistical algorithms. The following subsections outline the
use cases and evaluation from our three publications.
4.1 Proof of Concept on a Synthetic Dataset
In [3], we rst introduced our approach and evaluated it
on a synthetic dataset as a preliminary proof of concept. We
used a model mutation framework to synthetically generate
populations of models to be used for comparison: mutated
instances with small modications such as addition or re-
moval of a model element. The goal was to emulate a model
population evolved from a common model: starting with
a single model, recursively generating mutated osprings.
We eventually formed a tree with a branch for each mu-
tation and a depth level for each generation. For evalua-
tion, we got the youngest generation of models as the input
and performed hierarchical clustering, trying to reconstruct
a hierarchical similarity scheme resembling the original tree.
Indeed, we obtained a dendrogram of the models with an
obvious resemblance to the original tree, to be interpreted
as as an approximate reconstruction of the evolution.
4.2 Evaluation on Public Datasets
We extended our initial work in terms of NLP features
and case studies on public datasets in [2]. The NLP fea-
tures are already given in Section 3.2. For the evaluation
of the framework, we performed two case studies. In the
rst case study, we searched GitHub3for Ecore metamodels
for state machines and extracted the top 50 results accord-
ing to GitHub's Best Match criteria. The goal was to test
our approach on a dataset of a single domain, with possible
duplicates, outliers, and subdomains. We thus considered
domain analysis and model searching/exploration scenarios
(e.g. for reuse, in the sense of traversing search results and
nding the desired metamodels). Out of this search, we ob-
tained a somewhat heterogeneous dataset, and applied clus-
tering. Figure 2 shows the resulting dendrogram that we
qualitatively evaluated. Five clusters were manually iden-
tied (coloured and encircled) corresponding to basic nite
state machines (FSM), models with controllers, triggers, hi-
erarchical state machines and so on. Some items were con-
sidered outliers: e.g. a model with identiers in French and
one about train behaviour.
2"
1"
2"
3"
4"
5"0.00.20.40.60.81.02223217161412118101591347434240383735365048491454639412933312627302824251821192023443476534
Figure 2: Dendrogram of GitHub top search results.
2https://www.r-project.org/
3https://github.comFor the second case study, we used a subset of the Ecore
metamodels in AtlanMod Metamodel Zoo4. We selected a
subset of 107 metamodels from 16 dierent domains (mostly
as labelled in the repository). The aim was to evaluate our
approach on a large dataset with multiple domains. The do-
mains were chosen to be in a wide range; clustering was to
show the groups/subgroups in the dataset. We thus consid-
ered domain analysis and repository management scenarios.
The resulting dendrogram is omitted here due to space
limitations. Inspecting the dendrogram, we were able to
manually identify 16 clusters, roughly corresponding to sep-
arate domains with varying degree of accuracy. To name a
few, we identied small clusters for multi-agent and build
tool models, clusters of petri net and state machine models
in sibling branches; and clusters of word and excel models
in close sub-clusters. We employed an external measure of
validity and obtained an F0:5score of 0 :73 as the accuracy.
4.3 n-grams for Structural Comparison
In recent work, we have improved the clustering frame-
work by automating the cluster extraction, employing n-
grams to incorporate the structural information in the mod-
els, and nally introducing quantitative evaluation of clus-
tering with dierent parameters. We have developed a tech-
nique to encode model structure linearly as n-grams and
evaluated its accuracy quantitatively in two case studies.
In the rst one we have aimed to measure the accuracy of
n-grams for relatively small datasets using up to trigrams
(n= 3). We have extracted 50 random small subsets from
the dataset in [2], clustered each one using unigrams, bi-
grams and trigrams and reported the cumulative average
F0:5values. We have observed (1) n-grams do not univer-
sally improve accuracy over unigrams, (2) higher ndoes not
lead to monotonically higher accuracy, (3) yet n-grams with
n >1 on average perform better than with n= 1.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 regular cutF0.5
0.1 0.2 0.3 0.4 0.5 0.6 0.7F0.5
UNIGRAM BIGRAMdynamic cut
UNIGRAM BIGRAM
Figure 3: Unigram vs bigram F0:5measures for two
dierent cluster extraction methods.
With the rst case study giving us some insight into the
accuracy of n-grams, we have turned to cluster the whole
107-model dataset. We have restricted the upper bound
for n-grams to bigrams and found out that bigrams led to
an increase in clustering accuracy compared to unigrams.
The results are given in a boxplot of the F0:5measures with
dierent parameter permutations for unigrams and bigrams
in Figure 3. It is evident that bigrams considerably improve
the worst case, mean and median.
4http://web.emn.fr/x-info/atlanmod/index.php?title=
Ecore
8905. FUTURE WORK
We plan to extend the prototype and publish it as an open
framework, after a process of modularizing, documenting
the code and providing a simple GUI. Another important
improvement is support for parallel multicore execution and
possibly high performance computing, in order for our ap-
proach to be applicable for larger datasets.
Furthermore, we are looking for dierent datasets, both
from public and industrial domains. The current datasets we
use are obtained from model repositories (AtlanMod Zoo)
and crawled from GitHub. While looking at other repos-
itories (e.g. of UML models), we are considering another
option as well: reverse engineering models out of curated
code base (corpus) already used for source code analysis re-
search. More importantly, we are in the process of obtain-
ing datasets of real metamodels and models from a number
of industry partners that extensively use MDE or SPL ap-
proaches, and are interested in our framework.
While our research is inspired by the problems in domain
analysis and repository management, we intend to use our
approach for dierent scenarios. The most notable of these
is model clone detection, which we have already initiated
a study for feasibility. It seems to be a natural continua-
tion of our study: changing the granularity of the extrac-
tion (e.g. extract a vector per class) and perform a similar
analysis to obtain clusters of model fragments. This anal-
ysis would probably involve other types of features such as
metrics as well. Further scenarios include pattern detection,
model classication, querying and (co-)evolution analysis.
A nal path to mention as future work is the extension for
dierent types of models. These not only include structural
models such as UML class diagrams or feature models, but
also behavioural models such as statecharts and Simulink
models. Though the latter would involve model semantics
(e.g. comparing traces) and be more challenging.
6. CONCLUSION
In this paper, we have stated the problem of comparing
large sets of models for detecting relations such as groupings
and outliers among them. To solve this problem, we have
presented a generic approach using techniques from IR, NLP
and ML. Pointing to our previous work, we have evaluated
our approach using public datasets of models. The current
evaluation results elevate our condence in our approach,
and we outline many potential improvements and applica-
tion scenarios for it. We aim to further validate our approach
for real use cases in industrial context.
7. ACKNOWLEDGEMENTS
The research leading to these results has been funded
by EU programme FP7-NMP-2013-SMALL-7 under grant
agreement number 604279 (MMP).
8. REFERENCES
[1] S. L. Abebe and P. Tonella. Natural language parsing
of program element names for concept extraction. In
Program Comprehension (ICPC), 2010 IEEE 18th Int.
Conf. on , pages 156{159. IEEE, 2010.
[2]O. Babur, L. Cleophas, and M. van den Brand.
Hierarchical clustering of metamodels for comparative
analysis and visualization. In Proc. of the 12th
European Conf. on Modelling Foundations and
Applications, 2016 , pages 2{18, 2016.[3]O. Babur, L. Cleophas, T. Verhoe, and M. van den
Brand. Towards statistical comparison and analysis of
models. In Proceedings of the 4th International
Conference on Model-Driven Engineering and
Software Development , pages 361{367, 2016.
[4] F. Basciani, J. Di Rocco, D. Di Ruscio, L. Iovino, and
A. Pierantonio. Automated clustering of metamodel
repositories. In Int. Conf. on Advanced Information
Systems Engineering , pages 342{358, 2016.
[5] B. Bislimovska, A. Bozzon, M. Brambilla, and
P. Fraternali. Textual and content-based search in
repositories of web application models. ACM
Transactions on the Web (TWEB) , 8(2):11, 2014.
[6] G. Brunet, M. Chechik, S. Easterbrook, S. Nejati,
N. Niu, and M. Sabetzadeh. A manifesto for model
merging. In Proc. of the 2006 Int. Workshop on Global
Integrated Model Management , pages 5{12. ACM,
2006.
[7] R. Dijkman, M. Dumas, B. van Dongen, R. K aarik,
and J. Mendling. Similarity of business process
models: Metrics and evaluation. Inf. Systems ,
36(2):498{516, 2011.
[8] S. Holthusen, D. Wille, C. Legat, S. Beddig,
I. Schaefer, and B. Vogel-Heuser. Family model mining
for function block diagrams in automation software. In
Proc. of the 18th Int. Software Product Line Conf.:
Companion Volume for Workshops, Demonstrations
and Tools-Volume 2 , pages 36{43. ACM, 2014.
[9] P. Klint, D. Landman, and J. Vinju. Exploring the
limits of domain model recovery. In Software
Maintenance (ICSM), 2013 29th IEEE International
Conference on , pages 120{129. IEEE, 2013.
[10] D. S. Kolovos, L. M. Rose, N. Matragkas, R. F. Paige,
E. Guerra, J. S. Cuadrado, J. De Lara, I. R ath,
D. Varr o, M. Tisi, and J. Cabot. A research roadmap
towards achieving scalability in model driven
engineering. In Proceedings of the Workshop on
Scalability in Model Driven Engineering , BigMDE '13,
pages 2:1{2:10, New York, NY, USA, 2013. ACM.
[11] A. Kuhn, S. Ducasse, and T. G rba. Semantic
clustering: Identifying topics in source code. Inf. and
Software Technology , 49(3):230{243, 2007.
[12] C. D. Manning, P. Raghavan, H. Sch utze, et al.
Introduction to information retrieval , volume 1.
Cambridge university press Cambridge, 2008.
[13] C. D. Manning and H. Sch utze. Foundations of
statistical natural language processing , volume 999.
MIT Press, 1999.
[14] D. Ratiu, M. Feilkas, and J. J urjens. Extracting
domain ontologies from domain specic apis. In 12th
European Conf. on Software Maintenance and
Reengineering, 2008 , pages 203{212, 2008.
[15] J. Rubin and M. Chechik. N-way model merging. In
Proc. of the 2013 9th Joint Meeting on Foundations of
Software Engineering , pages 301{311. ACM, 2013.
[16] M. Stephan and J. R. Cordy. A survey of model
comparison approaches and applications. In
Modelsward , pages 265{277, 2013.
[17] D. Str uber, M. Selter, and G. Taentzer. Tool support
for clustering large meta-models. In Proc. of the
Workshop on Scalability in Model Driven Engineering ,
page 7. ACM, 2013.
891