On the Techniques We Create, the Tools We Build,
and Their Misalignments: a Study of KLEE
Eric F . Rizzi,
Grammatech Inc., Ithaca, NY , USA
{erizzi}@grammatech.comSebastian Elbaum, Matthew B. Dwyer
University of Nebraska - Lincoln, USA
{elbaum,dwyer}@cse.unl.edu
ABSTRACT
Our community constantly pushes the state-of-the-art by introduc-
ing ‚Äúnew‚Äù techniques. These techniques often build on top of, and
are compared against, existing systems that realize previously pub-
lished techniques. The underlying assumption is that existing sys-
tems correctly represent the techniques they implement. This pa-
per examines that assumption through a study of KLEE, a pop-
ular and well-cited tool in our community. We brieÔ¨Çy describe
six improvements we made to KLEE, none of which can be con-
sidered ‚Äúnew‚Äù techniques, that provide order-of-magnitude perfor-
mance gains. Given these improvements, we then investigate how
the results and conclusions of a sample of papers that cite KLEE are
affected. Our Ô¨Åndings indicate that the strong emphasis on intro-
ducing ‚Äúnew‚Äù techniques may lead to wasted effort, missed oppor-
tunities for progress, an accretion of artifact complexity, and ques-
tionable research conclusions (in our study, 27% of the papers that
depend on KLEE can be questioned). We conclude by revisiting
initiatives that may help to realign the incentives to better support
the foundations on which we build.
CCS Concepts
General and reference !Empirical studies; Software and
its engineering!Software libraries and repositories; Software
veriÔ¨Åcation and validation;
Keywords
Research incentives, research tools and infrastructure, replication
1. INTRODUCTION
The software engineering research community, like many in com-
puter science, relies on publication for both the dissemination of
ideas that advance the state-of-the-art and as a primary means of
career advancement [39]. It has been observed that this dual nature
of publication has skewed the review process, so that papers that
make claims of novelty are much more likely to be published [39,
45, 65, 74]. In turn, this has shaped the incentive structure for re-
searchers in ways that link discovery to career advancement, with
other forms of scholarly contribution relegated to a secondary role
at best [20]. In particular, the important work of improving and
maintaining the systems on which our Ô¨Åeld is so dependent, has
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14-22, 2016, Austin, TX, USA
c2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2884781.2884835been lost as a priority. We contend that the software engineering
research community is worse for this.
The focus on discovery leads much of the research published
in software engineering to make claims of the form ‚Äútechnique
A is the new state-of-the-art‚Äù. To support such claims it is very
common to manifest a technique in the implementation of a soft-
ware system. Every year there are papers in major conferences
and journals reporting evaluations using, for example, test genera-
tors, program analyzers, refactoring tools, program comprehension
systems, fault localizers, recommendation systems, and user inter-
faces. As a community, we rely on the Ô¨Ådelity and quality of these
implementations to support conclusions we draw about the tech-
niques that they realize, but it is notoriously difÔ¨Åcult to distinguish
discovery from mistaken or sub-optimal implementation [73].
Demonstrating the value of technique A may involve direct com-
parison with, or building on top of, technique B. In either case, the
implementations of A and B play a crucial role in the validity of the
conclusions that can be drawn about A. Inadequacies in those im-
plementations can lead to different kinds of problems. A faulty im-
plementation of B may lead to invalid conclusions about the value
of A. Researchers may waste effort in creating a new technique, A,
because of a perceived inadequacy in B, but that inadequacy may
simply be a fault in the implementation of B. Faults or limitations in
the implementation of B may discourage advances in building fur-
ther work along those lines. Finally, an accretion of artifact com-
plexity may result from layering implementations that work around
weaknesses in other implementations. This complexity can present
a barrier to further work in the Ô¨Åeld.
In this paper we explore these challenges through a mixed-methods
study involving the Symbolic Execution (SymExe) engine KLEE
[23, 9]. The study, and this paper, grew out from our own experi-
ences and missteps as we attempted to implement a new and highly
effective technique (or so we thought) into KLEE. We present, in
Section 3, six Ô¨Åxes and optimizations that we made to the KLEE
code base. These improvements cannot be considered ‚Äúnew‚Äù tech-
niques, but instead are best thought of as enhancements that might
naturally arise in the evolution of a high-quality software system.
The fact that these opportunities for improvement exist is by no
means an indictment of KLEE. In fact, as we discuss in Section 2,
KLEE is by all measures a well-developed and maintained research
tool ‚Äî among the best in our research community. Nevertheless,
the Ô¨Åxes and optimizations on such a recognized and popular tool
yielded performance gains ranging from 2X to 17X on different
conÔ¨Ågurations of KLEE, including 13X on the conÔ¨Åguration used
in the original KLEE paper [23].
The technical knowledge required to implement each of the im-
provements was available at the time KLEE was originally released.
This meant that many papers were published that either compared
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   132
against, or were built upon, a version of KLEE that did not repre-
sent the true capabilities of existing techniques. Given the magni-
tude of the performance gains we observed and how they affected
our judgments about own techniques, we wondered how claims
about published techniques would be strengthened or weakened if
they had been compared to KLEE with our improvements.
To assess this, we sampled and analyzed 100papers from inter-
national conferences and journals that cited KLEE. Of these 100
papers, 47referenced KLEE superÔ¨Åcially. Of the remaining 53pa-
pers, 27referenced KLEE but ended up using a SymExe implemen-
tation other than KLEE to evaluate their technique. The remaining
26papers used KLEE in some substantive way in their evaluations.
In an ideal world, we would have downloaded the experimental
bundles for all 26papers, applied our improvements to the KLEE
dependent parts of the bundle, and replicated the experiment to
measure the differences between the original and improved ver-
sions. Unfortunately, only two of the papers provided all of the
necessary details to repeat their experiments. Therefore, we ex-
amined each paper and, based on our knowledge of the parts of
KLEE that were affected by our improvements, identiÔ¨Åed 11papers
whose conclusions we felt could be impacted by our improvements
to KLEE. We then attempted to replicate these 11papers as faith-
fully as possible, searching the web for repositories and emailing
the authors with questions when necessary. Following this effort,
when it was possible to re-run the experiments, we did. In other
cases, we were forced, either due to non-responses or refusals, to
approximate the likely effects of our improvements. In three cases,
the author‚Äôs modiÔ¨Åcations to KLEE made their version different
enough from any other version of KLEE that we were able to eval-
uate that we had to withhold all judgements. Of the papers that
remained, we conclude that research Ô¨Åndings in 7of them may be
affected by our improvements to KLEE.
We believe that the results of this study provide insight into the
challenges faced by our community given its sharp focus on dis-
covery. We are spending signiÔ¨Åcant effort building ‚Äúnew‚Äù tech-
niques on top of, and comparing against, existing implementations
that may either lag behind or are incorrect implementations of the
known best techniques. This has the potential to lead to false claims
and redundant or misguided work. These problems are exacerbated
by the difÔ¨Åculties in replicating or even approximating the reported
studies on these techniques where small differences can have dra-
matic effects.
Our Ô¨Åndings also align with, and support, observations made in
other Ô¨Åelds that point to and attempt to address the under-emphasis
of the implementation and maintenance of software in research [47,
41, 10, 19, 18]. Recent efforts emphasizing artifact sharing and
evaluation to promote replication principles [4, 61, 67, 54], for ex-
ample, can help to better understand the implementations that in-
form future work. And although more can be done to support repli-
cation, distinct complementary efforts are needed to incentivize the
work of maintaining and improving the important systems we build
on [39, 64]. In Section 7, we describe other ways for the SE com-
munity to foster such activities. For example, we describe how
changes in the citation practices can help recognize researchers
working on tools [47, 53], how competitions could help to pro-
mote the introduction of known successful techniques into tools
and have known baselines [17, 19], and how specialized conference
tracks where tool improvements are judged based on their potential
impact to the community could provide a venue for researchers fo-
cusing their efforts on maintaining the sophisticated infrastructure
that we all build on. It is our hope that such steps could help to re-
duce the number of false research starts and mis-steps, and enable
more consistent progress by the community as a whole.2. BACKGROUND ON KLEE
Released in 2009, KLEE is designed to explore and automat-
ically generate tests for C programs. What made KLEE unique
from the SymExe tools that preceded it was the scale of programs
it could test. KLEE broke new ground, testing 89of Unix‚Äôs core-
utils programs, exposing new bugs and achieving higher coverage
in a few hours than test suites developed by experts over decades
[23]. Inspired in part by KLEE‚Äôs success, our research community
has developed a wide variety of SymExe tools, including ones for
binary code [38], Java [12, 7], C# [81], and C [2, 3, 52].
2.1 How KLEE is Maintained
In spite of the number of emerging SymExe tools, KLEE remains
among the most popular and our community keeps building on it.
It is being used to test a range of systems including: GPU ‚Äúker-
nels‚Äù [57], networks [70], and Ô¨Åle system checkers [24]. Research
groups have used KLEE for document synthesis [55], stress testing
[58], and aiding debugging [46, 44]. In addition, it is often used
as a baseline to show the efÔ¨Åcacy of a new technique [79, 59], the
implication being that if KLEE cannot do it, then it is unlikely any
other tool could.
Since it was Ô¨Årst released, KLEE has had 24different contribu-
tors to the main trunk. This effort translates into 989total commits
over the course of the project, including 177total commits in the
past year [9]. When compared with other SymExe projects that
share their code, KLEE‚Äôs research and development community is
among the largest. For example, CREST [3], jCute[7], CAUT [1],
and SPF [12] have equivalent or smaller development communities
[63]. Besides the code, KLEE has a website with a series of tutori-
als on how to get the tool running [8], a klee-dev email list, and a
trove of information that not only allows new users to replicate the
experiments presented in the original 2008 KLEE paper [23], but
also provides instructions on how to set up KLEE for development.
These are evidence that KLEE‚Äôs community is responsive, diverse,
active, and among the most mature in our research community.
2.2 How KLEE Works
KLEE (and SymExe tools in general) represents inputs, and val-
ues computed from them, symbolically during a program execu-
tion. This means that whenever a branch is encountered, the tool
forks, ostensibly exploring both sides of the branch. In order to fo-
cus towards generating meaningful inputs, SymExe tools use a sat-
isÔ¨Åability (SAT) solver to determine if a particular path is feasible.
A path is judged feasible if a logical formula encoding its branch
conditions is satisÔ¨Åable, which implies that there exists some input
that would cause the traversal of the path. Programs may have an
enormous number of feasible paths so, in practice, SymExe tools
rely on heuristic search methods to target certain paths sooner.
Invoking the solver is usually the most expensive part of SymExe
tools. To mitigate these costs, KLEE uses a Solver Chain made
of three Ô¨Ålters to reduce the size and number of constraint sets
that reach the solver. The Ô¨Årst Ô¨Ålter looks for constraint indepen-
dence, factoring the incoming constraint sets into smaller indepen-
dent subsets [82]. These subsets simplify computations in the latter
parts of the Solver Chain. The second Ô¨Ålter caches previously de-
termined solutions and checks whether the answer to an incoming
constraint set has already been computed. The third Ô¨Ålter uses a
UBTree to support the checking of subsumption relationships be-
tween constraint sets. Should the incoming constraint set be a sub-
set of a stored SAT constraint set, then the incoming constraint set
must also be SAT. If a stored UNSAT constraint is a subset of the
incoming constraint set, then the incoming constraint set must also
be UNSAT.
133Table 1: Our six improvements to KLEE. Improvement shows im-
provement‚Äôs name. Pull # shows improvement‚Äôs pull request num-
ber [9]. Size shows how many lines of code were changed in im-
provement. Status shows status of pull request as of 7/24/2015.
Improvement Pull # Size Status
Solution Re-computation Optimization 198 215 Accepted
Quick Cache Bug 206 85 Pending
Array Factory Bug 202 135 Accepted
Default Enabled no-prefer-cex Bug 241 24 Accepted
Unnecessary UBTree Superset Bug 250 15 Accepted
Inequality Concretization Optimization 229 231 Pending
3. IMPROVING KLEE
In this section, we survey six improvements that we made to
KLEE and evaluate their effect on its various conÔ¨Ågurations. The
key insight is that none of the improvements we made can be con-
sidered a ‚Äúnew‚Äù technique. Instead, four address performance bugs
in KLEE and two add simple optimizations that were introduced
almost ten years ago.
3.1 Fixes and Optimizations
Table 1 provides a brief description of these changes.1
3.1.1 Solution Re-computation Optimization
When KLEE traverses program paths, the Solver Chain‚Äôs Ô¨Årst Ô¨Ål-
ter minimizes the size of every collected constraint set by factoring
it [82]. When it attempts to generate concrete tests to follow these
paths, however, the original implementation of KLEE did not factor
incoming constraint sets. Therefore, instead of using cached solu-
tions, a constraint set would be passed to the solver. To Ô¨Åx this, we
changed KLEE so that the constraint sets associated with test case
generation would be fully factored. Once each subsets‚Äô solution
is retrieved, they are stitched together to form a single, complete
solution. This approach was introduced in 2004 [31].
3.1.2 Quick Cache Bug
This bug occurred when the second Ô¨Ålter was invoked to check
whether a particular branch could be traversed along both edges. If
the constraint set associated with the Ô¨Årst edge of the branch had
not been seen before, then the entire constraint set was forwarded
to the third Ô¨Ålter. This meant that the constraint set associated with
the other edge of the branch was never checked in the second Ô¨Ålter,
even if its solution was already cached. In practice, this meant that
the second Ô¨Ålter allowed cached constraint sets to reach the more
expensive third Ô¨Ålter. To Ô¨Åx this, we implemented a hashmap that
is checked for a cached solution before invoking the third Ô¨Ålter.
3.1.3 Array Factory Bug
Whenever KLEE encounters an array, a data structure that tracks
the reads and writes to that array is created. In the original imple-
mentation, when two separate paths encountered the same array,
two different instances of this data structure were created. This
meant that it was possible for equivalent constraint sets to not be
recognized as such, meaning that cached solutions would not be
retrieved. To Ô¨Åx this, we created a factory method that examined
every incoming request for an array. If the request was for an al-
ready created array, we simply returned a reference to that array.
3.1.4 Default Enabled no-prefer-cex Bug
No-prefer-cex is a command-line option that forces KLEE to try
to make the generated test cases more readable. In the original
implementation of KLEE, buried among 160other command-line
options, it was turned on by default. We consider this a bug because
1Commit 3bd3789 was our baseline. All experiments, data, and
builds in this paper are available at http://bit.ly/1PWVviz.the behavior it triggers is rarely desirable and very costly. Indeed,
none of the papers that we examined mentioned that they wanted
human readable output for their test cases. While it seems possible
that there is a place for this option, such as introducing new users
to KLEE, making it the default behavior reduces the performance
of KLEE for the average user.
3.1.5 Unnecessary UBTree Superset Bug
This bug was the result of what turns out to be a counter-productive
optimization added to KLEE‚Äôs third Ô¨Ålter [23]. In the original im-
plementation, the UBTree checked for both supersets and subsets
of an incoming constraint set. The subset query was very efÔ¨Åcient,
taking on average less than 1% of the overall computation.2The su-
perset query, however, accounted for 44% of the computation time.
The reason for this inefÔ¨Åciency is that, as constraints are placed into
the UBTree, the response time for superset queries increased at a
much faster rate than the subset query. These escalating costs even-
tually outweighed the costs of simply bypassing the superset check.
To Ô¨Åx this problem, we disabled the superset check by default.
3.1.6 Inequality Concretization Optimization
Concretization is an important optimization for SymExe tools.
If a symbolic variable can be proven to have a single value, then
it no longer needs to be symbolic. This logic is used to simplify
many of the constraint sets that need to be solved. KLEE‚Äôs original
implementation did not have a mechanism to handle inequalities,
which can constrain a variable to the point where it can only be a
single value. For example, the constraints (3< x)^(x < 5)can
be used to show that (x= 4). To implement this optimization, we
created a series of maps that hold the maximal and minimal values
that a variable can possibly take. Should the maximal and minimal
values ever become equal, then the variable would be concretized.
This optimization has been used since at least 2006 [27].
3.2 Assessing Fixes and Optimizations
In this section we assess the following question: what is the im-
pact of our Ô¨Åxes and optimizations on KLEE‚Äôs performance? To
answer that question, we evaluate two different instances of KLEE.
The Ô¨Årst instance is our treatment, that is, KLEE incorporating all
our improvements (denoted as all). The second one is our control,
that is, KLEE without any of the changes (denoted as none).3We
ran each of these instances on the same version of the 89 core-utils
programs utilized in the original KLEE paper [23]. For each arti-
fact, we used the same number and size of symbolic arguments as
suggested on the KLEE website.
Since KLEE can be conÔ¨Ågured to operate in many different ways,
and the performance may vary depending on which conÔ¨Åguration is
used, we structured our analysis across two popular conÔ¨Åguration
dimensions: targeted optimizations and testing goals. More specif-
ically, in terms of testing goals, we differentiated between generat-
ingRegression test suites, one for every path KLEE traversed, and
generating Coverage test suites, where only tests that increase cov-
erage are generated. In terms of targeted optimizations, the Ô¨Årst
one we cover, Generic, mimics a default conÔ¨Åguration, with no op-
timizing command-line options at all. The second one, Core-utils,
uses the command-line parameters suggested on the KLEE website
[8]. Combining these two dimensions leads to the four explored
conÔ¨Ågurations: Generic Regression ,Generic Coverage, Core-utils
2Based on data generated from all core-utils programs using the
none treatment of KLEE using the Generic Regression conÔ¨Ågura-
tion (to be discussed in Section 3.2).
3For details on the performance gains discriminated by individual
improvement we refer the reader to [68].
134Figure 1: Results for executing KLEE with allandnone on the
Generic Regression, Generic Coverage, Core-utils Regression, and
Core-utils Coverage conÔ¨Ågurations. The box-plots show how much
faster the allinstance is relative the none instance for each conÔ¨Ågu-
ration (a 1 indicates that allandnone have the same performance).
Regression, and Core-utils Coverage (we also refer to them as G.R,
G.C, C.R, and C.C later in the paper).
Finally, we designed a procedure to ensure a common baseline
for the comparisons across conÔ¨Ågurations and treatments. Since
KLEE does not provide a timeout for test generation, we resorted
to a proxy for it. Our procedure consisted of counting the num-
ber of forks the allinstance of KLEE could reach in one hour (for
each artifact and each conÔ¨Åguration). Using this initial target, we
then ran each instance of KLEE Ô¨Åve times, averaging the execu-
tion times across all runs. If a particular execution took longer than
24hours, we terminated it. This process resulted in almost 3560
observations (2 instances x 4 conÔ¨Ågs. x 89 artifacts x 5 runs).
Figure 1 summarizes the results, with the four KLEE conÔ¨Ågu-
rations on the x-axis, and the relative performance improvements
(ratio of none over all) on the y-axis (log-scale). The box-plots
show the minimal, Ô¨Årst quartile, median, third quartile, and maxi-
mal relative performance improvement across the set of all core-util
programs that could be run for a particular conÔ¨Åguration.
The median improvements were 17:4X for Core-utils Regres-
sion, 13X for Core-utils Coverage, 9:1X for Generic Regression,
and 2X for Generic Coverage.4The performance variation across
conÔ¨Ågurations was expected given their distinct behaviors. For ex-
ample, the Regression conÔ¨Ågurations usually produce many more
tests cases than the Coverage conÔ¨Ågurations, therefore the Solution
Re-computation Optimization and the Default Enabled no-prefer-
cex Bug Ô¨Åx lead to much greater improvement for the Generic Re-
gression andCore-utils Regression conÔ¨Ågurations. Similarly, the
Coverage conÔ¨Ågurations produce many more instances that can
be concretized through the Inequality Concretization Optimization
than with Generic conÔ¨Ågurations, leading to greater improvements
for the Core-utils-based conÔ¨Ågurations.
Independent of these variations, notice that these Ô¨Åxes and op-
timizations, while not ‚Äúnew‚Äù techniques, result in more than an
order of magnitude performance improvement on average across
conÔ¨Ågurations. This made up wonder how research that justiÔ¨Åes a
‚Äúnew‚Äù technique via the performance of an implementation would
perform in comparison.
4Under Core-utils Regression andCore-utils Coverage we could
not test three of the 89 programs (date, touch, and fmt) due to pre-
existing assertion violations that were repeatedly encountered by
both the alland the none instances. We omitted sortfrom the box-
plots since we consider it an outlier: it was over 1000X faster for
theGeneric Regression and the Generic Coverage conÔ¨Ågurations
and10X slower for the Core-utils Regression conÔ¨Åguration.4. ASSESSING WORK USING KLEE
The research questions we attempt to answer are:
RQ1 How is KLEE cited and used by the research community?
RQ2 How do our improvements to KLEE affect the results and
conclusions of papers that depend on KLEE?
4.1 Study Setup and Procedures
Paper selection process. We used Google Scholar [5], an aggre-
gator from many publications and professional venues, to build the
pool of papers that cited the original KLEE paper [23]. Our search
parameters allowed for all papers that were available on or before
3/6/2015 (the date of our search). We began our search by locating
the original KLEE paper in Google Scholar. We then clicked on the
Cited by button to access the papers that cited the original KLEE
paper [23]. We then scraped the contents of all the resulting pages
to create a local repository of papers. Next, we iteratively and ran-
domly selected a paper from the local repository, and evaluated it
based on our inclusion criteria until we had 100papers for analysis.
There were three criterion for inclusion. First, the article had to cite
the original KLEE paper. While our search process would seem to
imply this, there were a few papers returned by Google Scholar that
did not cite the original KLEE paper and were therefore removed
from the study. Second, the papers had to be full, peer-reviewed ar-
ticles published by international computer science conferences or
journals. Finally, we excluded papers that were duplicates of pa-
pers that had already been included in the analysis. The Ô¨Årst author
of this document was the original judge of all of the papers, with
the other two authors verifying samples of the results.5
Data Collection and Processing. For each paper in the study we
collected bibliographic data. We also classiÔ¨Åed each paper based on
how KLEE was used. If it relied on KLEE, we further analyzed the
degree to which its study depended on KLEE‚Äôs execution time and
whether the results may be affected in light of our improvements. If
it used SymExe tools other than KLEE, then we examined whether
the authors expressed a belief that their techniques could be imple-
mented in KLEE. The Ô¨Årst author of this document extracted and
classiÔ¨Åed the information. Particularly difÔ¨Åcult decisions were dis-
cussed among all authors until a consensus was reached.
To support such analysis we developed a series of rubrics. The
Usage Rubric helped us classify how a particular paper used KLEE
into one of three categories. The SuperÔ¨Åcial Citations category
contains papers that cite the original KLEE paper, but do not use
it or any other form of SymExe in their experiments or implemen-
tation. The SymExe Dependent category contains papers that cite
the original KLEE paper, but use some other SymExe tool in their
evaluation or implementation. Finally, the KLEE Dependent cat-
egory contains papers that both cite the original KLEE paper and
use KLEE in some part of their study.
TheTime Dependence Rubric helped us classify papers into three
categories representing an escalating dependence on execution time.
TheTime Reported category (T.R.) contains papers that only report
the time required to execute a technique. The Timeout Used cate-
gory ( T.O.) contains papers that use a timeout in their evaluation.
Finally the Time Compared category (T.C.) contains papers that use
time to directly compare two or more techniques.
TheEffects of Improvements Rubric helped us identify how stud-
ies might be affected by our improvements to KLEE. It also con-
5Instructions for accessing the papers that were evaluated against
the inclusion/exclusion criteria, along with a full justiÔ¨Åcation for
each of the decisions we made in our analysis, are available at http:
//bit.ly/1PWVviz.
135Figure 2: Replication and Approximation Protocol.
sists of three categories. The Unaffected category contains pa-
pers whose results we believed would not be affected by our im-
provements to KLEE. The Positively Affected category contains pa-
pers whose experimental results would likely improve. The Nega-
tively Affected category contains papers whose experimental results
would likely worsen due to the improvements we made to KLEE.
Approximation and Replication. The Effects of Improvements
Rubric provides a Ô¨Årst layer of analysis to determine the potential
impact of our KLEE improvements on a paper‚Äôs results, but it does
not investigate the magnitude of the improvements or how the im-
provements would affect the conclusions in a paper. To provide
such insights, we attempted to replicate the studies for papers that
we deemed candidates for having their conclusions affected. We
devised the protocol in Figure 2 to guide us through this process.
The process started by carefully analyzing each paper, searching
for setup and implementation details of each study including KLEE
version and parameters, relevant libraries, artifacts, and supporting
tools. If the paper provided enough information, then we attempted
to replicate it. Otherwise, we performed a web-search, attempting
to Ô¨Ånd a repository associated with the paper. The search targeted
the authors‚Äô web sites, the technique names, and common hosting
sites like GitHub and Sourceforge. If a repository was found, we
attempted to use information to replicate the study. Otherwise, we
sent an email to the authors requesting further information. In total
we reached out to 22authors of 10papers, and received a response
from the authors of eight papers. If viable information was pro-
vided by the authors, we attempted to replicate the study, and iter-
ated with the authors as needed. We continued this iterative process
for a period of three weeks.6
In the end, replication was not feasible for nine of 11papers (we
discuss the implications of this later in the paper). Since so many of
them could not make available critical information for replication,
we were forced to approximate their setup. We carefully examined
each paper and conservatively estimated the KLEE conÔ¨Åguration
they were most likely to have used. We adopted the strategy where
we assumed that a paper used KLEE in the most effective way pos-
sible. If it was clear that KLEE was being used to create a large
body of tests, we assumed its setup was most similar to the Generic
Regression conÔ¨Åguration discussed in Section 3.2. If KLEE was
used to achieve coverage, we assumed the Generic Coverage con-
Ô¨Åguration was used. Adopting this strategy also meant that we
were being conservative in terms of the effects of our improve-
ments, since we never mapped a paper to either of the Core-utils
conÔ¨Ågurations, the ones that our improvements affected the most.
Finally, if the way KLEE was employed did not map to these com-
mon conÔ¨Ågurations, we did not attempt to force an approximation
[44, 79]. Our Ô¨Ånal mapping of papers to conÔ¨Ågurations is given in
parentheses in column Ô¨Åve of Table 2.
6Instructions for accessing the complete communications with the
authors are available at http://bit.ly/1PWVviz.
Figure 3: Breaking down the 100analyzed papers. Attempt denotes
papers we attempted to replicate. Rep. denotes papers that provided
enough information to replicate their studies. Approx. denotes pa-
pers that we approximated due to lack of information. N/Adenotes
papers that we were unable to either replicate or approximate.
5. RESULTS
The Sankey diagram in Figure 3 provides an overview of the re-
sults for the 100analyzed papers. The Ô¨Årst breakdown (from left
to right) shows how the community cites and uses Klee: 27% are
SymExe Dep., 26% are KLEE Dependent, and 47% are SuperÔ¨Å-
cial Citations. The next breakdown shows, for the KLEE Depen-
dent papers, that 11may have their conclusions affected by our im-
provements so we attempted to replicate them (‚ÄúAttempt‚Äù). Among
those, the next breakdown shows we could replicate 2and approx-
imate 6. In the end, 7(26%) of the KLEE Dependent papers could
have their conclusions signiÔ¨Åcantly affected. The next sections ex-
amine in detail these results.
5.1 RQ1: How is KLEE cited and used?
To understand how our research community uses and cites KLEE,
we employed the KLEE Usage Rubric as a coarse Ô¨Ålter, and then
analyzed each category in an attempt to characterize them.
SuperÔ¨Åcial Citations. Papers in this category cover a wide range
of research areas including conÔ¨Åguration space testing [76], soft-
ware upgrades [14], and performance testing [50]. In some cases,
KLEE is used to provide context and motivation. For example, one
paper ‚Äúoffers a [technique for the] formalization of translation rules
for C language that allows a ... test generator to validate properties,
like KLEE‚Äù [66]. In other cases, KLEE is presented as an option
that would not quite work [33, 80, 83]. An example of this occurs in
a paper that addresses compile-time conÔ¨Ågurability and states that
‚Äúthe most powerful symbolic execution techniques (such as Klee
... ) would currently not scale to the size of‚Äù the problem being
studied [80]. In such cases, even when KLEE does not appear to be
well suited for a particular problem, it is both popular and powerful
enough to at least merit discussion.
SymExe Dependent. Papers in this category are mostly focused
on testing and debugging. Some papers propose new ways to make
the results of SymExe testing more meaningful for developers [35,
37]. Other papers are meant to support SymExe through changes
to the underlying theories [34], techniques [28, 22, 72], or tools
[82]. Finally, papers in this category often create their own hybrid
techniques to either target new languages [51], or to mitigate the
path explosion problem in SymExe [86, 78, 62].
These papers are interesting because they were written by SymExe
researchers who are aware of KLEE but chose to use a different
136Table 2: Analysis of papers in KLEE Dependent category. ID: pa-
per identiÔ¨Åer. Time: classiÔ¨Åcation with Time Dependence Rubric.
Eff.: classiÔ¨Åcation with Effects of Improvements Rubric with P,N,
andUmeaning Positively Affected, Negatively Affected, and Un-
affected. Rep.: attempt of replication. Nomeans we did not try
to replicate it, Approx. means we attempted to replicate, but could
only approximate, N/Ameans we could not even approximate, and
Yesmeans we were able to replicate. Con.: signiÔ¨Åcant impact on
paper‚Äôs conclusions. N/A shows that we were unable to reach a
conclusion based on the available information.
Paper Title ID Time Eff. Rep. Con.
KleeNet [70] KN-1 T.R. P No No
Modeling Ô¨Årmware as service functions and its
application to test generation [13]MF-2 T.R. P No No
A SOFT way for OpenFlow switch interoper-
ability testing [56]OFS-3 T.R. P No No
Symbolic software model validation [77] SMV-4 T.R. P No No
Automatic detection of Ô¨Çoating-point exceptions
[16]FPE-5 T.O. P No No
Control Ô¨Çow obfuscation using neural network
to Ô¨Åght concolic testing [59]CFO-6 T.O. N No No
Craxweb [42] CRX-7 T.O. P No No
Detecting problematic message sequences and
frequencies in distributed systems [58]PMS-8 T.O. P Approx. (G.R.) Yes
Docovery [55] DOC-9 T.O. P No No
MintHint [46] MH-10 T.O. P No No
Static analysis driven cache performance testing
[15]CPT-11 T.O. P No No
Automated software testing of memory perfor-
mance in embedded GPUs [25]TMG-12 T.C. N Approx. (G.C.) Yes
Automatic concolic test generation with virtual
prototypes for post-silicon validation [26]VPV-13 T.C. N Approx. (G.R.) Yes
Body armor for binaries: preventing buffer over-
Ô¨Çows without recompilation [75]BA-14 T.C. U No No
BugRedux [44] BR-15 T.C. N N/A N/A
Chaining test cases for reactive system testing
[71]RST-16 T.C. N Approx. (G.R.) Yes
Directed symbolic execution [60] DSE-17 T.C. N No No
GKLEE [57] GK-18 T.C. P No No
HAMPI [36] HMP-19 T.C. N Yes No
Industrial application of concolic testing ap-
proach [52]IA-20 T.C. N Yes Yes
Postconditioned symbolic execution [84] PSE-21 T.C. N Approx. (G.R.) Yes
Reproducing Ô¨Åeld failures for programs with
complex grammar-based input [49]RFF-22 T.C. N N/A N/A
Selecting peers for execution comparison [79] SPC-23 T.C. N N/A N/A
Scalable testing of Ô¨Åle system checkers [24] FSC-24 T.C. P Approx. (G.R.) Yes
Software dataplane veriÔ¨Åcation [30] SDV-25 T.C. N No No
A synergistic analysis method for explaining
failed regression tests [85]SA-26 T.C. P No No
tool. As we examined these papers it became clear that a major-
ity of them (66%) believe that their proposed techniques could be
mapped onto KLEE and achieve similar results to those presented
in the paper. In 18% of these papers this belief is explicit. For
example one paper states ‚Äúour algorithms are applicable in the con-
text of other languages for which symbolic execution tools exists
(e.g., Klee ... for C)‚Äù [37]. In 48% of the papers, the possibility of
extending the proposed technique to KLEE is implied. This usu-
ally occurs when a paper identiÔ¨Åes a problem with SymExe as a
whole and then Ô¨Åxes the problem on a particular tool. An example
of this occurs in a paper that asserts ‚Äú[o]ther concolic and symbolic
testing tools could integrate our algorithm to solve complex path
conditions without having to sacriÔ¨Åce any of their own capabilities,
leading to higher overall coverage‚Äù [28]. The reader is left with the
impression that all SymExe tools, including KLEE, would beneÔ¨Åt.
Given the questions we raise about papers that actually depend on
KLEE directly (next section), it seems that claims about the gener-
ality of a technique across tools merit at least further evaluation.
We found that it is rare for a technique to be implemented in more
than one tool. Indeed, only three of the 27papers in the SymExe
Dependent category went so far as to incorporate and test their tech-
nique on multiple tools. Following publication, there seems to be
even less chance that a technique will be implemented across the set
of SymExe tools: we could not Ô¨Ånd any of the techniques proposed
by the papers within this category in KLEE‚Äôs code-base. We specu-
late this disconnect leaves the community open to both duplication
and confounding techniques.Table 3: Time Dependence Rubric xEffects of Change Rubric. The
numbers represents the papers‚Äô whose studies results would be af-
fected by our improvements to KLEE across each time dimension.
The numbers in parenthesis correspond to instances where the con-
clusions of the work are likely to be affected.
Negatively Unaffected Positively Total
Affected Unaffected Affected
Time Reported 0 0 4 (0) 4
Timeout Used 1 (0) 0 6 (1) 7 (1)
Time Compared 11 (5) 1 (0) 3 (1) 15 (6)
Total 12 (5) 1 (0) 13 (2) 26 (7)
KLEE Dependent. When examining the papers in this category,
the one thing that was immediately obvious was the versatility of
KLEE. Several papers extend KLEE to test GPU programs [57],
Networks [70], and File Systems Checkers [24]. There are also
papers that used KLEE to help a developer Ô¨Åx a buggy line [46]
and recreate Ô¨Åeld failures in the lab [44]. Two other papers ar-
gue that KLEE has become such a powerful tool that it may be
necessary to rethink altogether how we address certain problems
[30, 59]. For example, one of these papers argues for the adoption
of a particular programming paradigm since it ‚Äúpreserves perfor-
mance‚Äù while also ‚Äúenabl[ing] veriÔ¨Åcation‚Äù through its amenability
to KLEE [30]. Finally, KLEE is also used as a baseline to assess
the efÔ¨Åcacy of new techniques [84, 36, 79], the implication being
that if KLEE cannot do it, then the proposed technique must be
powerful. We examine this category in more detail next.
5.2 RQ2: How do our KLEE improvements
affect KLEE dependent papers?
To get a better understanding of the different types of evaluations
being conducted among the 26KLEE dependent papers, we clas-
siÔ¨Åed them using the Time Dependence Rubric. We Ô¨Årst classiÔ¨Åed
four papers as Time Reported, seven as Timeout Used, and 15as
Time Compared. In addition, we examined how the papers in each
category would be affected by our improvements to KLEE using
theEffects of Improvements Rubric and, for the ones that we could
replicate and approximate, we re-assess their conclusions. The re-
sults of combining the Time and Effects rubrics can be seen in Ta-
ble 3. Note that in the subsequent analysis we use the IDs from the
second column of Table 2 to refer to the papers.
When examining Table 3, we see that 96% (25 out of 26) pa-
pers seem to be affected by our improvements to KLEE (the single
unaffected paper, SMV-4, uses a tool that builds off KLEE but its
technique and study focus on buffer-overÔ¨Çow detection). From this
initial breakdown it seems that papers that compare performance
are much more likely (11 out of 15) to have their results negatively
affected given the KLEE improvements we implemented.
Time Reported. There are four papers in the Time Reported
category. These papers use SymExe to solve new problems. For
example, the paper MF-2 examines how long it takes for KLEE
to explore a particular type of driver design. Overall, papers in
this category use time to show that applying KLEE to a particular
problem domain is possible. Effect. Since a faster KLEE only
strengthens what is possible, the results of papers in this category
are positively affected by our improvements, but the conclusions
are not altered.
Timeout Used. The Timeout Used category has seven papers
where authors determine, through the timeout, at what point the
cost/beneÔ¨Åt of a particular technique is no longer acceptable. For
example, FPE-5 builds a system on top of KLEE for Ô¨Ånding Ô¨Çaws
in Ô¨Çoating point programs. In their evaluation the authors set a
timeout and, because of this, several executions are not completed.
137Effect. Six papers in this Timeout category have results that could
be positively affected and one that could be negatively affected by
our changes to KLEE. In terms of the effects on the conclusions,
however, for six of the seven papers the likely importance of our
KLEE improvements is small. In the case of CPT-11, the authors
used exceptionally short timeouts, meaning the effects of the im-
provements would be limited. In other cases, the results were al-
ready so persuasive (e.g. in FPE-5 ‚Äúdiscovered inputs that gener-
ated 2091 Ô¨Çoating-point exceptions‚Äù... ‚Äùacross the 467functions‚Äù)
that improving them further would seem to be of little note. In oth-
ers, such as CFO-6 andMH-10, the results did not rely on KLEE in
such a way that our improvements would alter their results enough
to signiÔ¨Åcantly strengthen or weaken their conclusions. The one
paper that we believe could have its conclusions signiÔ¨Åcantly af-
fected by our KLEE improvements was PMS-8. For this reason,
we mark this paper for further analysis in the next section.
Time Compared. TheTime Compared category has 15papers
that somehow utilize KLEE‚Äôs performance to quantify a problem
or a technical solution. Effect. We deemed that 11papers would
have their results negatively affected, one not affected, and three
positively affected. Among the papers with affected results, we
deemed 10as likely to have their conclusions affected. We mark
those for further analysis in the next section. The reasons for papers
with affected results not being marked for further analysis varied.
Some, such as SDV-25, addressed an underlying theoretical prob-
lem of SymExe like path explosion. In other cases, such as BA-15
andDSE17, KLEE‚Äôs performance does not seem to affect the out-
come of the research questions being investigated. For example, in
DSE-17 the authors investigate different search strategies to reach
particular lines in test artifacts. They include KLEE‚Äôs search strat-
egy in their study, implementing it in their own SymExe tool. While
they test how well KLEE does on these test artifacts, they seem only
to include it as an implicit endorsement that their implementation
of the search was correct. Finally, in SA-26, the runtimes are short
enough that modiÔ¨Åcations in the results are unlikely to translate to
signiÔ¨Åcant improvement for the technique.
Overall, studies that used time to make value judgements seem
much more susceptible to performance improvements affecting their
results than those showing that an approach is possible. The chal-
lenge is that, as a community, we cannot just rely on studies that
focus just on what is possible. Studies that show what is better al-
low us to judge the relative merits of a technique, in turn enabling
experiments that make more things possible. Unfortunately, better
performance is contingent on a whole host of factors. From the
clock rate of the CPU all the way up to the conÔ¨Åguration details of
a tool, with many variables interacting in highly complicated ways.
How best to achieve progress in such Ô¨Çuid and complicated envi-
ronments is an exceptionally difÔ¨Åcult question.
Replication and Approximation
Of the 25 papers whose results could be affected by our KLEE
improvements, our analysis identiÔ¨Åed 11(one from the Timeout
category and 10from the Time Compared category) that required
a deeper examination because we deemed that their conclusions
could be signiÔ¨Åcantly affected. This deeper examination consisted
not just in analyzing the papers in more detail, but also attempting
to replicate studies. In spite of our efforts, we were able to repli-
cate the studies in only two of these papers. We were able to repli-
cate HMP-19 which contained a reference to an online-repository,
with all of the necessary code and data. The other paper we were
eventually able to replicate with the authors‚Äô assistance was IA-
20, although as we shall see even in this instance the result of the
replication did not quite match those in the paper. We did not re-
Figure 4: Reproduction of Figure 2 from paper PSE-21 comparing
the time required to run PSE (KLEE combined with the technique
proposed in the paper) against the time to run base KLEE (KLEE).
ceive a response for two papers, while for the remainder we were
informed that pending patents, work with industrial bodies, or un-
recoverable code and data prevented the authors from being able to
help us replicate their experiments. Overall, out of the 11papers
that we examined more deeply, 2had their conclusions strength-
ened by our Ô¨Åndings, 5had their conclusion negatively affected
and hence weakened by our Ô¨Åndings, 1paper was not affected, and
we were not able to approximate 3papers. We now discuss them
in detail with the caveats mentioned in terms of our ability to repli-
cate the studies and their contexts. The Ô¨Åndings were shared with
all the authors of the affected papers, and for three the discussion
was adjusted based on that feedback.
PMS-8: Approximated as G.R., Positively Affected. Even though
one of the co-authors is a co-author of this paper, the experimental
data and modiÔ¨Åed artifacts required to support this experiment were
not preserved after project completion. Hence, we approximated
the study using the Generic Regression conÔ¨Åguration which closely
matches what was originally used. Using this approximation, we
would expect a 10X speedup with our KLEE improvements. Con-
sidering that in the paper‚Äôs study KLEE ran for 24hours, the im-
provement would likely have strengthened the paper‚Äôs conclusions,
allowing it to be run more frequently, discovering more faulty se-
quences or scaling up the analysis to multiple nodes.
FSC-24: Approximated as G.R., Positively Affected. This pa-
per uses test suites generated by KLEE to Ô¨Ånd faults in Ô¨Åle sys-
tem checkers. In their evaluation, the authors compare their work
against a test suite that accompanied one of the checkers. We again
had to approximate as the authors replied that it was not available
to the public. However, the conÔ¨Åguration as described is similar
toGeneric Regression that resulted in a 10X speed up with our
improvements. In their discussion, the authors observe that ‚Äúapply-
ing [the technique] to the Ô¨Åle system checker led to a lower code
coverage than that obtained with the...test suite‚Äù. We believe, how-
ever, that given the magnitude of our improvements to KLEE, such
statements may no longer be true: either the coverage would dra-
matically increase or the time required to run the technique would
be dramatically reduced.
PSE-21: Approximated as G.R., Negatively Affected. This pa-
per uses state matching ‚Äúto identify and eliminate common path
sufÔ¨Åxes that are shared by multiple test runs‚Äù. An implementation
of the proposed technique was not available: one author replied
that they could not provide ‚Äúthe source code since it is currently
under contract with our project‚Äù. Therefore, we had to approxi-
mate, using the Generic Regression conÔ¨Åguration as the proposed
technique produces thousands of tests. The authors report a reduc-
tion in the number of generated test cases of 5X and state that the
proposed technique ‚Äúcan have a signiÔ¨Åcant speedup over state-of-
138Figure 5: Reproduction of Table 1 from the paper RST-16. The
table shows KLEE‚Äôs performance (KLEE) relative to other tech-
niques at creating test case chains.
the-art methods in KLEE‚Äù. Their technique, however, introduces
non-trivial overhead. They provide a graph, reproduced in Figure 4,
that compares the time to run KLEE versus KLEE with their tech-
nique added. As can be seen in the Ô¨Ågure, there is a tipping point
(around 500 seconds on the x-axis) where KLEE‚Äôs cost becomes
high enough to warrant the proposed technique. Upon applying
our improvements resulting in a 10X speedup for this likely con-
Ô¨Åguration, the tipping point would likely be delayed. This in turn
would call into question their conclusions about ‚Äúthe trade-offs be-
tween effective redundancy removal and the computational cost of
detecting and eliminating such redundancy‚Äù.
VPV-13: Approximated as G.R., Negatively Affected. This paper
proposes a test reduction technique by Ô¨Årst identifying important
‚Äústates under test from concrete executions ... and then symboli-
cally‚Äù executing them. An implementation of the proposed tech-
nique was not available as ‚Äú[c]urrently all the code is under that
patent and not open source‚Äù. The paper claims that the proposed
technique signiÔ¨Åcantly reduces the time required to generate a test
suite while only reducing test coverage by a small amount. For
example, in one of their experiments, their technique takes ‚Äú30
minutes which includes 3:5minutes for state selection and 26:5
minutes for test generation.‚Äù When examining a larger set of ‚Äú6000
states ... selected using the random strategy. It takes 1day [after
which] only two new [useful] test cases are generated‚Äù. Our im-
provements call into question the cost beneÔ¨Åt analysis presented in
the paper. By reducing the costs of ‚Äún√§ive‚Äù KLEE by 10X, the loss
in coverage that is associated with the technique that the authors
propose may no longer be acceptable.
RST-16: Approximated with G.R., Negatively Affected. This pa-
per attempts to ‚Äúdiscov[er] a test case chain - a single test case that
covers a set of multiple test goals and minimises overall test execu-
tion time‚Äù. While the tool itself was made available by the authors
upon request, the experiments involving KLEE, along with the test
harnesses required were not provided within the time speciÔ¨Åed in
the protocol (we did receive a response after the time limit estab-
lished by the protocol). The authors compare four techniques/tools
(ChainCover, FShell, Random, and KLEE) as seen in Figure 5 in
terms of the number of test cases in the chain (tcs), the length of the
test case chain (len), and the time it took to Ô¨Ånd the chain. When
the half hour timeout is reached, the maximum coverage achieved
so far is reported. They state that ‚ÄúKlee found test case chains on
a few of the benchmarks in very short time, [but it] did not achieve
full coverage within an hour on half of the benchmarks‚Äù. While
they were many complexities to how KLEE was used7, the paper
declares ‚Äúexhaustive exploration (i.e. Klee) is not suitable for our
problem‚Äù. We Ô¨Ånd it likely that a 10X faster KLEE would have
7Among the added complexity was that of test harness built on top
of KLEE, for which the cost is unclear.a greater chance of Ô¨Ånishing several of these benchmarks, and of
Ô¨Ånding the chains it did Ô¨Ånd faster. If this were the case, it might
call into question the conclusion reached in their paper that Chain-
Cover was the best choice.
IA-20: Replicated, Negatively Affected. The authors of this pa-
per create a new SymExe tool to generate more tests and achieve
greater coverage in a set amount of time. In their study, the authors
compare their tool‚Äôs performance to KLEE‚Äôs when using different
search algorithms. In the end, the authors observe that their tool is
‚Äú10to28times faster than KLEE in terms of test case generation
speed‚Äù. While the paper seems to provide all of the necessary de-
tails to replicate the study, after attempting to do so, we were unable
to achieve results comparable to those reported in the paper. The
authors were helpful in conÔ¨Årming that we were using the correct
settings, but even then our results turned out to be quite different
from those reported in the paper. In the process we realized that the
paper‚Äôs study, as described, neglected to include a command-line
option (max-sym-array-size) that would be recommended as it dra-
matically speeds up KLEE‚Äôs performance. Indeed, when compar-
ing the only two searches KLEE and CREST-BV have in common,
the improved KLEE outperforms the reported data of CREST-BV .
These increases in test case generation performance occur while si-
multaneously slightly improving coverage. These facts, combined
with our difÔ¨Åculties reproducing the results reported, mean that the
assertion that ‚Äú[t]he speed of test case generation by CREST-BV
was 28times faster than that of KLEE‚Äù is now questionable.
TMG-12: Approximated as G.C., Negatively Affected. This pa-
per presents a technique to ‚Äú[detect] the inefÔ¨Åciency of...software
developed for embedded GPUs‚Äù. To expose these problems, the
authors create a tool that combines several existing tools (includ-
ing GKLEE) with several new techniques (such as a static analyzer
and a thread selector). They then compare the performance of their
tool under several different conÔ¨Ågurations. Unfortunately, the pro-
posed tool was not available since ‚Äúthe related project currently
involves industrial bodies‚Äù. The complexity of their setup made it
difÔ¨Åcult to rely on approximations, so we conservatively used the
Generic Coverage conÔ¨Åguration to illustrate the potential impact
of our improvements to their tool chain. While the 2X improve-
ment of the Generic Coverage conÔ¨Åguration approximation would
likely increase the overall effectiveness of the tool that is tested
in the paper, it also calls into question many of the speciÔ¨Åc pro-
posed optimizations. SpeciÔ¨Åcally, the paper proposes a new static
analyzer that ‚ÄúÔ¨Årst produc[es] a summary of all GPU threads via
static analysis‚Äù. These summaries are used to ‚Äúguide the test gen-
eration to produce memory-performance stressing test case‚Äù. With
the improvements applied to GKLEE, this initial computation may
prove unnecessary since the cost of static analysis may end up be-
ing greater than simply allowing GKLEE to explore the states in a
n√§ive fashion.
HMP-19: Replicated, Not Affected. This paper presents ‚Äúan ef-
Ô¨Åcient and easy-to-use string solver‚Äù called HAMPI. In the KLEE-
based portion of the study, the authors test programs with ‚Äústruc-
tured input formats‚Äù, using HAMPI ‚Äúto generate constraints that
specify legal inputs to these programs‚Äù. By utilizing this initial Ô¨Ål-
ter, the authors argue that many of the redundant paths that KLEE
would explore are removed, allowing it to explore deeper into the
program. To see the effects that our improvements would have
on their proposed technique, we ran two versions of KLEE: one
with our improvements and one without, and compared the cover-
age achieved by each instantiation. On each of the three artifacts
investigated in the paper, our improvements did not result in any
increase in coverage. This shows that our improvements are not
enough to overcome the challenges that come with ‚Äústructured in-
139put formats‚Äù, and, thus, that the paper‚Äôs conclusions hold.
SPC:-23: Not Able to Approximate, Unknown. This paper com-
pared the ability of different techniques to generate a non-failing
trace that is similar to a faulty trace, allowing a ‚Äúdeveloper [to]
compare and contrast the control Ô¨Çow paths taken‚Äù. One of the
tools examined was KLEE. In the discussion of their implemen-
tation, it was clear that the authors had altered KLEE‚Äôs program
exploration. We did not receive a response from the authors to
our queries for details. Due to the nature of the changes to KLEE,
we did not feel comfortable approximating the likely effects of our
improvements, and refrained from making judgements as to the ef-
fects of our improvements.
BR-15: Not Able to Approximate, Unknown. This paper presents
a tool called BugRedux that attempts ‚Äúto synthesize ... executions
that mimic the observed Ô¨Åeld failures‚Äù. It uses traces from a native
execution to guide KLEE to bugs. We found a BugRedux repos-
itory after the online search, but had difÔ¨Åculty running the exper-
iments performed in the paper as provided by the authors despite
our email exchanges. As with the previous case, we refrained from
making a judgement.
RFF-22: Not Able to Approximate, Unknown. This paper used
the same BugRedux tool described above. In this paper, however,
the authors were examining programs that had ‚Äústructured input
formats‚Äù, similar to the ones examined by HMP-19. Unfortunately,
due to some confusion caused by the fact that this paper both used
the BugRedux tool and was authored by many of the same authors
as the BR-15 paper, we did not contact the authors of this paper
soon enough to have it qualify for our protocol. Therefore, we
excluded this paper from further analysis.
6. THREATS TO V ALIDITY
We have provided evidence that KLEE is a well-regarded and
popular tool in the software engineering community. Many of us
have built and continue to build on it, which is why we expect that
our Ô¨Åndings may resonate with many. Yet, a major threat to the ex-
ternal validity of this work is whether the Ô¨Åndings extend beyond
KLEE. Our personal experience with many other tools and frame-
works indicates that it does. Our research group has faced similar
challenges when building on powerful and complex tools within
the program analysis domain like JPF-SPF [12], Daikon [32], and
SOOT [11]. A new set of unknowns and a different set of incen-
tives is further introduced when commercial tools are considered,
but the key underlying issue of whether a tool embodies the state
of the art remains.
From an internal validity perspective, the studies we conducted
have several limitations including how we implemented and mea-
sured our KLEE improvements, how we sampled and analyzed
the papers, the possibility of different build processes and setups
yielding different artifacts, other contextual elements we may have
missed that were relevant to the authors or their tools, and how our
attempts at replication mostly ended in different levels of approx-
imation. We have mitigated these concerns through transparency
(all data and experiments‚Äô setups are shared), by reaching out to
the authors to improve our attempts at replication and also to get
feedback on the reported Ô¨Åndings, and by making what we con-
sider to be the most conservative estimates that favor the papers we
studied. From a construct perspective, our focus has been almost
exclusively on performance while keeping other measures constant
or assuming they were negligible (e.g., executing code other than
KLEE had no cost). Similarly, we recognize that gains in perfor-
mance alone, even when measured in orders of magnitude, may
not translate into practical gains (e.g., more coverage, more faults
found). These limitations, together with the available data, point forfurther opportunities to better understand the relationship between
the techniques we develop and the tools we create.
7. MOVING FORWARD
We now examine our Ô¨Åndings from the perspective of the larger
themes of replication and incentive structure.
7.1 Replication
In this work we have seen some of the extremes of replication.
On one end, the assessment of the improvements we made to KLEE
were enabled by the KLEE developers who provided code, arti-
facts, and conÔ¨Åguration documentation used in the original KLEE
paper [23]. On the other end, as discussed in Section 5.2, attempt-
ing to ‚Äúclosely follo[w] the baseline experiment‚Äù [45] for many of
the papers in the Klee Dependent category was difÔ¨Åcult. Only one
of the 11papers provided sufÔ¨Åcient information so that the origi-
nal study could be closely followed without interacting with the au-
thors. Most are missing critical pieces of information that forced us
to resort to approximation. We admit to the possibility of wrongly
approximating, even when doing it conservatively. But the key take
away is that the current situation makes us unable as a community
to distinguish implementation limitations versus technique limita-
tions, further undermining our conclusions.
Replication difÔ¨Åculties are not new. They Ô¨Åt into a long running
conversation about the importance of replication in our Ô¨Åeld [21,
61, 73]. Much of the work on this topic focuses on how the lack of
replication makes understanding contradictory studies difÔ¨Åcult. For
example, in the paper Reproducible Research‚ÄìWhat, Why and How
[61], the authors detail their efforts to do a meta-analysis of code-
reading techniques. After compiling the results from 18different
studies, the authors were unable to determine which technique was
the best. This revelation leads them to ‚Äústrongly advocate the adop-
tion of [replicable research] by software engineering and computer
science researchers and data analysts.‚Äù More recently, Proebsting et
al.compile a set of 601papers ‚Äúfrom ACM conferences and jour-
nals‚Äù [67] and attempt to build the code associated with each study.
After an extensive web-search and hundreds of emails, they deter-
mined that they could locate and build the code associated with
48:3% of these papers. While Proebsting et al. do not attempt
to verify the results in the papers, their work illustrates the extent
of the problem of replicability within our Ô¨Åeld. Our paper shows
that, even when we can build these tools, there is a large gap be-
tween techniques and their implementation. Still, in spite of the
slow adoption of replication, some corrective steps are being taken.
Continuously Evolve Repository of Artifacts. There has been an
emergence of repositories of artifacts to support experimentation
across tools (e.g., Dacapo [4], SIR [29]). Such repositories help to
make studies across tools more consistent by providing a common
set of evaluation artifacts. They have accelerated research progress
by lowering the cost of performing experimentation, hence making
it more common. Still, these repositories need to keep evolving in
order to avoid techniques over-Ô¨Åtting the body of artifacts available
and to better represent the universe of artifacts. The funding model
to support this evolution, however, is not yet established.
Entice Artifact Submission and Checking. Conferences like ES-
EC/FSE, OOPSLA, and ISSTA have put in place an artifact sub-
mission and evaluation process. Authors of accepted papers are en-
couraged to submit artifacts along with their papers. Such artifacts
are separately reviewed to assess how helpful they are to reproduce
the results in the paper and to be extended, and are given a badge
of approval if they are found to be of enough value. The percent-
age of accepted papers receiving the badge of approval seems to
140be slowly increasing (about a third of the accepted papers at IS-
STA 2015 received the badge). Still, artifact and paper evaluation
are completely decoupled, and papers can receive a badge without
sharing the artifacts with the community. It may be time to revisit
these conservative practices. Similarly, it would be beneÔ¨Åcial for
funding organizations to make it a requirement for researchers to
share their infrastructure and results with the community.
7.2 Aligning Incentives
Making studies replicable is necessary but not sufÔ¨Åcient for re-
search tools to keep maturing, much less for them to keep up with
the pace of discovery. We conjecture that the over-emphasis on cre-
ating ‚Äúnew‚Äù techniques, at the expense of working on the robust-
ness of existing infrastructure, is causing at least three problems.
First, unnecessary complexity ends up being added to the tools
we build. For example, in the context of our study, we observed that
four separate papers addressed KLEE‚Äôs test generation difÔ¨Åculties
through new techniques when debugging KLEE or incorporating
existing techniques might have led to similar or better outcomes.
This leads to wasted community effort, and has the potential to
generate layers of techniques that correct the mis-implementations
of the techniques below [48].
Second, there is little incentive to add a newly proposed tech-
nique to more than one tool. It is a common practice to assert that a
technique, once shown to be effective on one tool, will be effective
on all tools (Section 5.1). We ourselves are guilty of this [69]. De-
spite these assertions, and as we have seen with two of our improve-
ments to KLEE, a powerful technique can remain un-implemented
for years. This makes it difÔ¨Åcult to know which problems need to
be solved, and could lead to a situation where the same problem is
solved repeatedly by different research groups.
Finally, we are underestimating what it means for a novel techni-
cal contribution to be successful [43]. In our study we discovered,
for example, how techniques compared against a weaker version of
KLEE reached, at best, questionable conclusions. In some cases,
the conclusions derived lead to the rejection of a whole family of
techniques, overlooking much of the progress that has been made
in the Ô¨Åeld since 2008 .
Our study‚Äôs Ô¨Åndings highlighted these misalignments, but we are
cognizant that they are not just happening in software engineering,
but across the computer science Ô¨Åeld [47, 48, 40]. Yet, address-
ing them in software engineering is particularly pressing because
our relationship with software is more complicated than most other
disciplines: we often evaluate the artifacts we create. So, we now
examine several proposed solutions to mitigate these problems be-
lieving them to be worth further discussion in our community.
Cite Papers and Tools. Citations are used as a means of ‚Äúauthen-
tication and authority‚Äù and as a ‚Äúprovision of credit and acknowl-
edgment‚Äù [47]. Unfortunately, ‚Äúactivities that facilitate science ...
are not currently rewarded or recognized‚Äù [47] having ‚Äúdetrimental
effects on funding, future library development, and even scientiÔ¨Åc
careers‚Äù [53]. To combat this, several proposals have been made
to increase the visibility of work that maintains and improves soft-
ware. For example, one proposal involves a system of transitive
credit, where the authors of a tool reward their developers with a
percentage of the credit [47]. These proposals aim to create an
evolving list of contributors so new work is rewarded. Applying
proposals like this to tools like KLEE would require developers to
deÔ¨Åne how to quantify the credit different projects and developers
should receive. To be effective, however, these suggestions must
be understood and their implementation valued by relevant insti-
tutions through, for example, assessing them as part of hiring and
promotion [39, 64].Increase Institutional Support. Another suggestion is to create
a staff of dedicated developers to act as stewards of the tool [41].
While there are instances of this funding model [6, 10], they are
clearly not dominant and the focus on maintenance and improve-
ment is lacking. This would require the identiÔ¨Åcation of efforts
worthy of further investment, but also the recognition of and sup-
port for talented ‚Äútool-builders‚Äù in the academic world. In the con-
text of KLEE, it seems likely that the bugs we found could have
been discovered by a team of dedicated engineers. Integrating some
of the proposed sophisticated optimizations, however, would likely
require the participation of the researchers that created them, since
the information in papers is often not enough to successfully repli-
cate a technique [61]. Bringing together the people who understand
the tool with those who understand the technique would likely in-
crease the chances of success.
Promote Competition. A potential solution is to rely on com-
petitions. This model is being used by the veriÔ¨Åcation community,
‚Äúfacilitat[ing] the proliferation of different...approaches, algorithms
and implementations‚Äù [17], and the success of this model is prop-
agating to other venues [19]. Still, it seems that competitions have
limited traction in the community. We conjecture that, in part, this
could be corrected by deÔ¨Åning problems with the right scope (man-
ageable yet impactful) to entice more community participation.
Create a Tool Maintenance Track. Another potential solution
is adding a maintenance speciÔ¨Åc track to conferences. Such a track
would help to incentivize improvements to important tools and sys-
tems through the traditional and accepted reward mechanism of
publication, thereby making it easier to Ô¨Åt into hiring and tenure
practices [64]. Papers submitted to this track would be judged
based on new criteria that account for the potential impact to the
community based on the number of users or citations of the target
system, and the degree to which the tool is improved.
8. CONCLUSION
Our community constantly pushes the state-of-the-art by intro-
ducing ‚Äúnew‚Äù techniques. To do this, we often build on top of, and
compare against, existing implementations that realize previously
published work. Yet, we often ignore the fact that the veracity of
these ‚Äúnew‚Äù techniques depends on whether the underlying systems
really embody the techniques they implement. This omission can
lead to false claims, to multiple techniques that redundantly target
the same problem, to ‚Äúnew‚Äù techniques that simply work around
bugs in prior work, to ‚Äúnew‚Äù techniques that are undervalued be-
cause underlying faults mask their true potential, and to an accre-
tion of complexity as techniques unnecessarily layer upon one an-
other. From our study we know that this happens in the research
community that builds on KLEE. The fact it occurs with a tool as
highly cited and well-regarded as KLEE, along with our experience
with other analysis tools, leads us to hypothesize that it occurs with
many other tools as well. Further studies are needed to support this
hypothesis. Clearly, our community is focused on advancing the
state-of-the-art, but robust advances must be built on robust foun-
dations. The unknowns of SE research lay both ahead and behind
and only by addressing both we will make consistent progress.
Acknowledgements
We would like to thank the KLEE developers for their contribution
to our community, and for being extremely patient in answering our
questions. Thanks also to the authors that responded to our queries,
even when providing those answers was not always comfortable.
This work was supported in part by a Google Faculty Research
Award and NSF Awards #SHF-1218265, #CCF-1346769, #CCF-
1449626, and #CCF-1526652.
1419. REFERENCES
[1] CAUT project, accessed: 2015-7-3.
[2] CIVL project, accessed: 2015-7-3. vsl.cis.udel.edu/civl/.
[3] CREST project, accessed: 2015-7-3.
github.com/jburnim/crest/graphs/contributors.
[4] The dacapo benchmark suite, accessed: 2015-7-31.
dacapobench.org.
[5] Google scholar. scholar.google.com. Accessed: 2015-6-4.
[6] Google summer of code, accessed: 2015-7-3.
developers.google.com/open-source/gsoc.
[7] JCUTE project, accessed: 2015-7-3.
github.com/osl/jcute/graphs/contributors.
[8] KLEE LLVM execution engine website. klee.github.io/.
Accessed: 2015-5-21.
[9] KLEE project, accessed: 2015-7-3.
github.com/klee/klee/graphs/contributors.
[10] Software sustainability institute, accessed: 2015-7-3.
software.ac.uk.
[11] Soot project, accessed: 2015-8-25.
github.com/Sable/soot/graphs/contributors.
[12] SPF project, accessed: 2015-7-3.
babelÔ¨Åsh.arc.nasa.gov/hg/jpf/jpf-symbc.
[13] S. Ahn and S. Malik. Modeling Ô¨Årmware as service functions
and its application to test generation. In Hard. and Soft.:
VeriÔ¨Åcation and Testing, pages 61‚Äì77. Springer, 2013.
[14] R. Bachwani, O. Crameri, R. Bianchini, D. Kostic, and
W. Zwaenepoel. Sahara: Guiding the debugging of failed
software upgrades. In ICSM, pages 263‚Äì272. IEEE, 2011.
[15] A. Banerjee, S. Chattopadhyay, and A. Roychoudhury. Static
analysis driven cache performance testing. In RTSS, pages
319‚Äì329. IEEE, 2013.
[16] E. T. Barr, T. V o, V . Le, and Z. Su. Automatic detection of
Ô¨Çoating-point exceptions. In SIGPLAN Notices, volume 48,
pages 549‚Äì560. ACM, 2013.
[17] C. Barrett, M. Deters, A. Oliveras, and A. Stump. Design and
results of the satisÔ¨Åability modulo theories competition.
2008.
[18] S. Bauersfeld, T. E. V os, and K. Lakhotia. Unit testing tool
competitions‚Äìlessons learned. In Future Internet Testing,
pages 75‚Äì94. Springer, 2014.
[19] D. Beyer. Status report on software veriÔ¨Åcation. In Tools and
Algorithms for the Construction and Analysis of Systems,
pages 373‚Äì388. Springer, 2014.
[20] E. L. Boyer. Scholarship reconsidered: Priorities of the
professoriate. Carnegie Foundation for the Advancement of
Teaching, 1990.
[21] A. Brooks, M. Roper, M. Wood, J. Daly, and J. Miller.
Replication‚Äôs role in software engineering. In Guide to adv.
empirical soft. eng., pages 365‚Äì379. Springer, 2008.
[22] S. Bucur, J. Kinder, and G. Candea. Prototyping symbolic
execution engines for interpreted languages. In ASPLOS,
pages 239‚Äì254. ACM, 2014.
[23] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted
and automatic generation of high-coverage tests for complex
systems programs. In OSDI, volume 8, pages 209‚Äì224, 2008.
[24] J. C. M. Carreira, R. Rodrigues, G. Candea, and
R. Majumdar. Scalable testing of Ô¨Åle system checkers. In
EuroSys, pages 239‚Äì252. ACM, 2012.
[25] S. Chattopadhyay, P. Eles, and Z. Peng. Automated software
testing of memory performance in embedded GPUs. In
EMSOFT, page 17. ACM, 2014.[26] K. Cong, F. Xie, and L. Lei. Automatic concolic test
generation with virtual prototypes for post-silicon validation.
InICCAD, pages 303‚Äì310. IEEE, 2013.
[27] X. Deng, J. Lee, and Robby. Bogor/kiasan: A k-bounded
symbolic execution for checking strong heap properties of
open systems. In ASE, pages 157‚Äì166, 2006.
[28] P. Dinges and G. Agha. Solving complex path conditions
through heuristic search on induced polytopes. In FSE,
volume 14, 2014.
[29] H. Do, S. Elbaum, and G. Rothermel. Supporting controlled
experimentation with testing techniques: An infrastructure
and its potential impact. Empirical Software Engineering:
An International Journal, 10(4):405‚Äì435, 2005.
[30] M. Dobrescu and K. Argyraki. Software dataplane
veriÔ¨Åcation. In NSDI, pages 101‚Äì114, 2014.
[31] N. E√©n and N. S√∂rensson. An extensible SAT-solver. In
Theory and applications of satisÔ¨Åability testing, pages
502‚Äì518. Springer, 2004.
[32] M. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant,
C. Pacheco, M. S. Tschantz, and C. Xiao. The daikon system
for dynamic detection of likely invariants. Science of
Computer Programming, 69(1):35‚Äì45, 2007.
[33] N. S. Evans, A. Benameur, and M. C. Elder. Large-scale
evaluation of a vulnerability analysis framework. In CSET,
pages 3‚Äì3. USENIX Association, 2014.
[34] S. Falke, F. Merz, and C. Sinz. Extending the theory of
arrays: memset, memcpy, and beyond. In VeriÔ¨Åed Soft.:
Theories, Tools, Experiments, pages 108‚Äì128. Springer,
2014.
[35] A. Filieri, C. S. P ÀòasÀòareanu, and W. Visser. Reliability
analysis in symbolic pathÔ¨Ånder. In ICSE, pages 622‚Äì631.
IEEE Press, 2013.
[36] V . Ganesh, A. Kie Àôzun, S. Artzi, P. J. Guo, P. Hooimeijer, and
M. Ernst. HAMPI: A string solver for testing, analysis and
vulnerability detection. In CAV, pages 1‚Äì19. Springer, 2011.
[37] J. Geldenhuys, M. B. Dwyer, and W. Visser. Probabilistic
symbolic execution. In ISSTA, pages 166‚Äì176. ACM, 2012.
[38] P. Godefroid, M. Y . Levin, and D. Molnar. SAGE: whitebox
fuzzing for security testing. Queue, 10(1):20, 2012.
[39] L. Hafer and A. E. Kirkpatrick. Assessing open source
software as a scholarly contribution. CACM,
52(12):126‚Äì129, 2009.
[40] J. Howison and J. Bullard. How is software visible in the
scientiÔ¨Åc literature? Technical report, Univ. of Texas, 2015.
[41] J. Howison and J. D. Herbsleb. Incentives and integration in
scientiÔ¨Åc software production. In CSCW, pages 459‚Äì470.
ACM, 2013.
[42] S.-K. Huang, H.-L. Lu, W.-M. Leong, and H. Liu. Craxweb:
Automatic web application testing and attack generation. In
SERE, pages 208‚Äì217. IEEE, 2013.
[43] J. P. Ioannidis. Why most published research Ô¨Åndings are
false. Chance, 18(4):40‚Äì47, 2005.
[44] W. Jin and A. Orso. BugRedux: reproducing Ô¨Åeld failures for
in-house debugging. In ICSE, pages 474‚Äì484. IEEE, 2012.
[45] N. Juristo and O. S. G√≥mez. Replication of software
engineering experiments. In Empirical software engineering
and veriÔ¨Åcation, pages 60‚Äì88. Springer, 2012.
[46] S. Kaleeswaran, V . Tulsian, A. Kanade, and A. Orso.
MintHint: automated synthesis of repair hints. In ICSE,
pages 266‚Äì276. ACM, 2014.
[47] D. Katz. Citation and attribution of digital products: Social
142and technological concerns. WSSSPE at SC, 2013.
[48] D. Katz, S.-C. Choi, H. Lapp, K. Maheshwari, F. L√∂fÔ¨Çer,
M. Turk, M. Hanwell, N. Wilkins-Diehr, et al. Summary of
the Ô¨Årst workshop on sustainable software for science
(WSSSPE): Practice and experiences. arXiv, 2014.
[49] F. M. Kifetew, W. Jin, R. Tiella, A. Orso, and P. Tonella.
Reproducing Ô¨Åeld failures for programs with complex
grammar-based input. In ICST, pages 163‚Äì172. IEEE, 2014.
[50] C. Killian, K. Nagaraj, S. Pervez, R. Braud, J. W. Anderson,
and R. Jhala. Finding latent performance bugs in systems
implementations. In FSE, pages 17‚Äì26. ACM, 2010.
[51] M. Kim, Y . Kim, and G. Rothermel. A scalable distributed
concolic testing approach: An empirical evaluation. In ICST,
pages 340‚Äì349. IEEE, 2012.
[52] Y . Kim, M. Kim, Y . J. Kim, and Y . Jang. Industrial
application of concolic testing approach: A case study on
libexif by using CREST-BV and KLEE. In ICSE, pages
1143‚Äì1152. IEEE, 2012.
[53] M. Knepley, J. Brown, L. C. McInnes, and B. Smith.
Accurately citing software and algorithms used in
publications. Technical report, 785731, 2013.
[54] S. Krishnamurthi and J. Vitek. The real software crisis:
repeatability as a core value. CACM, 58(3):34‚Äì36, 2015.
[55] T. Kuchta, C. Cadar, M. Castro, and M. Costa. Docovery:
toward generic automatic document recovery. In ASE, pages
563‚Äì574. ACM, 2014.
[56] M. Kuzniar, P. Peresini, M. Canini, D. Venzano, and
D. Kostic. A soft way for openÔ¨Çow switch interoperability
testing. In CoNEXT, pages 265‚Äì276. ACM, 2012.
[57] G. Li, P. Li, G. Sawaya, G. Gopalakrishnan, I. Ghosh, and
S. P. Rajan. GKLEE: Concolic veriÔ¨Åcation and test
generation for gpus. In SIGPLAN Notices, volume 47, pages
215‚Äì224. ACM, 2012.
[58] C. Lucas, S. Elbaum, and D. S. Rosenblum. Detecting
problematic message sequences and frequencies in
distributed systems. ACM SIGPLAN Notices,
47(10):915‚Äì926, 2012.
[59] H. Ma, X. Ma, W. Liu, Z. Huang, D. Gao, and C. Jia. Control
Ô¨Çow obfuscation using neural network to Ô¨Åght concolic
testing. In SecureComm, pages 287‚Äì304, 2014.
[60] K.-K. Ma, K. Phang, J. Foster, and M. Hicks. Directed
symbolic execution. In Static Analysis, pages 95‚Äì111.
Springer, 2011.
[61] L. Madeyski and B. Kitchenham. Reproducible
research‚Äìwhat, why and how. Technical report, WrUT,
Report PRE W08/2015/P-02, 2015.
[62] L. Martignoni, S. McCamant, P. Poosankam, D. Song, and
P. Maniatis. Path-exploration lifting: Hi-Ô¨Å tests for lo-Ô¨Å
emulators. ACM SIGARCH, 40(1):337‚Äì348, 2012.
[63] N. McDonald and S. Goggins. Performance and participation
in open source software on GitHub. In CHI, pages 139‚Äì144.
ACM, 2013.
[64] D. Patterson, L. Snyder, and J. Ullman. Evaluating computer
scientists and engineers for promotion and tenure.
Computing Research Association, 1999.
[65] R. D. Peng. Reproducible research in computational science.
Science (New York, Ny), 334(6060):1226, 2011.
[66] G. Petiot, B. Botella, J. Julliand, N. Kosmatov, and
J. Signoles. Instrumentation of annotated c programs for test
generation. In SCAM, pages 105‚Äì114. IEEE, 2014.
[67] T. Proebsting and A. M. Warren. Repeatability andbenefaction in computer systems research. Technical report,
Univ. of Arizona TR 14-04, 2015.
[68] E. F. Rizzi. Discovery over application: A case study of
misaligned incentives in software engineering. Master‚Äôs
thesis, University of Nebraska, Lincoln, 2015.
[69] E. F. Rizzi, M. B. Dwyer, and S. Elbaum. Safely reducing the
cost of unit level symbolic execution through read/write
analysis. ACM SIGSOFT Soft. Eng. Notes, 39(1):1‚Äì5, 2014.
[70] R. Sasnauskas, O. Landsiedel, M. H. Alizai, C. Weise,
S. Kowalewski, and K. Wehrle. KleeNet: discovering
insidious interaction bugs in wireless sensor networks before
deployment. In IPSN, pages 186‚Äì196. ACM, 2010.
[71] P. Schrammel, T. Melham, and D. Kroening. Chaining test
cases for reactive system testing. In Testing Software and
Systems, pages 133‚Äì148. Springer, 2013.
[72] H. Seo and S. Kim. How we get there: a context-guided
search strategy in concolic testing. In FSE, pages 413‚Äì424.
ACM, 2014.
[73] M. Shepperd, D. Bowes, and T. Hall. Researcher bias: The
use of machine learning in software defect prediction. Soft.
Eng., IEEE Trans. on, 40(6):603‚Äì616, 2014.
[74] J. Siegmund, N. Siegmund, and S. Apel. Views on internal
and external validity in empirical software engineering. In
ICSE, 2015.
[75] A. Slowinska, T. Stancescu, and H. Bos. Body armor for
binaries: Preventing buffer overÔ¨Çows without recompilation.
InUSENIX ATC, pages 125‚Äì137, 2012.
[76] C. Song, A. Porter, and J. S. Foster. itree: efÔ¨Åciently
discovering high-coverage conÔ¨Ågurations using interaction
trees. Soft. Eng., IEEE Trans. on, 40(3):251‚Äì265, 2014.
[77] C. Sturton, R. Sinha, T. H. Dang, S. Jain, M. McCoyd, W. Y .
Tan, P. Maniatis, S. A. Seshia, and D. Wagner. Symbolic
software model validation. In MEMOCODE, pages 97‚Äì108.
IEEE, 2013.
[78] T. Su, Z. Fu, G. Pu, J. He, and Z. Su. Combining symbolic
execution and model checking for data Ô¨Çow testing. In ICSE,
volume 15, pages 654 ‚Äì 665, 2015.
[79] W. N. Sumner, T. Bao, and X. Zhang. Selecting peers for
execution comparison. In ISSTA, pages 309‚Äì319, 2011.
[80] R. Tartler, J. Sincero, C. Dietrich, W. Schr√∂der-Preikschat,
and D. Lohmann. Revealing and repairing conÔ¨Åguration
inconsistencies in large-scale system software. STTT,
14(5):531‚Äì551, 2012.
[81] N. Tillmann and J. De Halleux. Pex‚Äìwhite box test
generation for. net. In Tests and Proofs, pages 134‚Äì153.
Springer, 2008.
[82] W. Visser, J. Geldenhuys, and M. B. Dwyer. Green:
reducing, reusing and recycling constraints in program
analysis. In FSE, pages 1‚Äì11. ACM, 2012.
[83] X. Wang, D. Lazar, N. Zeldovich, A. Chlipala, and
Z. Tatlock. Jitk: a trustworthy in-kernel interpreter
infrastructure. In OSDI, pages 33‚Äì47. USENIX, 2014.
[84] Q. Yi, Z. Yang, S. Guo, C. Wang, J. Liu, and C. Zhao.
Postconditioned symbolic execution. In ICST, pages 1‚Äì10.
IEEE, 2015.
[85] Q. Yi, Z. Yang, J. Liu, C. Zhao, and C. Wang. A synergistic
analysis method for explaining failed regression tests. In
ICSE, pages 257‚Äì267, 2015.
[86] Y . Zhang, Z. Chen, J. Wang, W. Dong, and Z. Liu. Regular
property guided dynamic symbolic execution. In ICSE, pages
643‚Äì653, 2015.
143