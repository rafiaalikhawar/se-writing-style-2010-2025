Python Probabilistic Type Inference with Natural Language
Support
Zhaogui Xu*, Xiangyu Zhangyz, Lin Chen*, Kexin Peiy, and Baowen Xu*z
*State Key Laboratory of Novel Software TechnologyyDepartment of Computer Science
*Nanjing University, ChinayPurdue University, USA
zgxu@smail.nju.edu.cn {xyzhang, kpei}@cs.purdue.edu {lchen, bwxu}@nju.edu.cn
ABSTRACT
We propose a novel type inference technique for Python pro-
grams. Type inference is dicult for Python programs due
to their heavy dependence on external APIs and the dynamic
language features. We observe that Python source code of-
ten contains a lot of type hints such as attribute accesses and
variable names. However, such type hints are not reliable.
We hence propose to use probabilistic inference to allow the
beliefs of individual type hints to be propagated, aggregated,
and eventually converge on probabilities of variable types.
Our results show that our technique substantially outper-
forms a state-of-the-art Python type inference engine based
on abstract interpretation.
CCS Concepts
Software and its engineering !Automated static
analysis;Mathematics of computing !Max
marginal computation;
Keywords
Python; Dynamic Languages; Type Inference; Probabilistic
Inference
1. INTRODUCTION
Python is one of the most popular programming languages
nowadays. However, since Python is a dynamic language,
developers often suer from the lack of type information
during development. In fact, type errors are very commonly
encountered bugs in Python. However, Python type infer-
ence is highly challenging. Many Python projects heavily
utilize external API functions that are often not in Python.
Objects are created and mutated in those API functions.
They may also become correlated by these functions (e.g.,
through aliasing). Such creations, mutations, and correla-
tions are critical for correct type inference. However, these
zCorresponding authors.API functions are usually dicult to analyze due to the dif-
ferent programming languages used and the lack of source
code. The large number of API functions also makes manual
mocking prohibitively expensive. In addition, Python vari-
able types are path-sensitive. A variable may have various
types depending on the program paths. Types and attribute
sets of objects can be dynamically mutated, substantially
adding to the diculty of static typing.
Traditional unication based type inference is not appli-
cable to Python type inference due to path-sensitivity. Re-
searchers have proposed various solutions for Python type
inference [7, 8, 5, 9, 6]. Most of them work by leveraging
data ow between untyped variables and variables of known
types (e.g., variables assigned with constants). Their eec-
tiveness hinges on the manual mocking of the large num-
ber of external API functions. According to our experiment
(Section 6), a state-of-the-art system PySonar2 [1] that was
used by Google Inc. can only type 49.47% of variables in
real-world programs. There have been a lot of works on
type inference for dynamic languages in general [26, 28, 23,
24]. Many of them have the similar idea of leveraging vari-
ables with known types. Some of them are dynamic analysis,
requiring good test coverage.
We propose a novel Python type inference technique based
on probabilistic inference. We observe that Python pro-
grams have a lot of type hints such as attributes that are
accessed, variable names, and explicit type checks. How-
ever, many of these type hints are uncertain, meaning that
they are not fully reliable. For example, attribute sets may
be dynamically mutated so that the observed attribute ac-
cesses may not match the prototype of the type. Developers
may not respect the naming conventions. Our idea is to
correlate all these uncertain type hints through probabilis-
tic inference, which is a technique that allows beliefs, that
is, the initial condences of type hints, to be propagated
and aggregated through the correlations among program ar-
tifacts (e.g., data ow between variables). Eventually, the
computation converges on the probabilities of variable types.
Our contributions are summarized as follows.
We identify four kinds of type hints that can be used
to infer types in Python, including data ow between
variables, attribute accesses, variable names, and ex-
plicit type checks. Some of these hints are uncertain.
We propose the novel idea of using probabilistic in-
ference for Python type inference, which allows us to
naturally model the uncertainty of type hints and eas-
ily handle dynamic features.
We develop a machine learning based approach to ex-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
FSEâ€™16 , November 13â€“18, 2016, Seattle, WA, USA
c2016 ACM. 978-1-4503-4218-6/16/11...$15.00
http://dx.doi.org/10.1145/2950290.2950343
Artifact evaluated by FSEâœ“
607
1:def gzip(f, *args, **kwargs):
2: resp = f(*args, **kwargs)
3: url = resp.url
4: mthd = resp.method
5: data = compress(resp)
...
6: result = resp
7: return result
Figure 1: A motivating example.
tract variable type hints from their names.
We develop a prototype and evaluate it on 18 real-
world Python projects. Our results show that our
technique can type 79.09% of the variables that cannot
be typed by a state-of-the-art system with estimated
82.86% precision.
2. CHALLENGES
Python program type inference has the following promi-
nent challenges.
Incomplete Data Flow and Lack of Interface Def-
inition. Most existing static type inferencing techniques
work by observing data ow starting from values with known
types [1, 7, 23, 28]. Unfortunately, Python programs heavily
rely on external functions that may be implemented in other
languages and have undocumented interfaces/side-eects.
Moreover, many Python projects are indeed libraries that
provide services to other downstream projects that may not
be available during type analysis. All these lead to incom-
plete data ow and diculties in type inference.
Consider a Python program snippet shown in Fig. 1. It is
extracted and adopted from a popular Python library http-
bin. As we can see from the example, function f()is a
parameter of function gzip() . In the library, gzip() is a
top level function that is not invoked by any other func-
tions. As such, we cannot get any type information for the
parameters of gzip() from function invocations. Hence, we
do not know the interface of f(), including the type of re-
turn value. In addition, an external function compress() is
invoked at line 5, which does not have any Python source
code. Therefore, it is dicult to know if the external func-
tion has any side-eects on the object referenced by resp.
Most existing techniques will fail to infer the types of resp,
data andresult due to the incompleteness of data ow.
The overarching idea of our technique is to leverage the
incomplete/uncertain type hints in a program and conduct
probabilistic inference. For each variable, our tool will pro-
duce a list of types that is ordered by their likelihood. In
particular, we leverage two kinds of type hints. We call
them hints because they are uncertain/incomplete by na-
ture. The rst kind is extracted from program semantics,
describing how a variable is being used and what attributes
are being accessed. Note that such accesses are incomplete
in most cases (i.e., only a subset of attributes of an object are
accessed). The second kind is variable names. We assume
the developers for a project follow some naming conventions,
which can be extracted by data mining the names of vari-
ables that can be typed by existing type inference tools.
Note that these naming hints are uncertain. We can only
say\ this variable may likely be X type according to its name ".
In our example, according to lines 3 and 4, we know that
the object referenced by resp must contain attributes \ url"
and \ method ". However, from these two hints, we cannot de-
termine the type of resp because both the instances of type
Response and the instances of type Request have these two1:def deflate(f, *args, **kwargs):
2: r = f(*args, **kwargs)
3: if not args:
4: data = r.data
5: else :
6: req = r.request
...
7: return r
Figure 2: An example for path sensitivity.
attributes. In addition, we do not know if other attributes
ofresp are accessed in the function compress () . Inter-
estingly, from the naming convention, variable resp may
possibly be an instance of Response , as it is lexically simi-
lar to \Response". In other words, when we consider both
kinds of hints, the likelihood of resp being of Response type
becomes much higher.
Path Sensitivity and Dynamic Updates. As Python is
a dynamic language, a variable may have dierent types at
runtime, depending on the program paths. These types may
not even have sub-type relations. In such cases, static type
inference should not produce a specic type for xat location
l, but rather a type set. Path sensitivity makes probabilistic
inference particularly challenging as contradicting type hints
may be collected along dierent paths. If their probabilities
are simply aggregated, they may nullify each other, lead-
ing to type inference failures or incorrect types. Consider
an example in Fig. 2. Variable rhas dierent types along
the two branches. We observe two attribute accesses of r,
i.e., accessing data and request along the two respective
branches. We cannot simply assert that the type of rcon-
tain both attributes. Otherwise, the probabilistic inference
engine would fail to infer the type of ras there does not exist
any type (in the type domain) that have both attributes.
The set of attributes of an object can also be dynamically
modied at runtime so that the type hints collected along a
path may not match with the original prototype of an object.
Probabilistic inference needs to take this into consideration.
We discuss the probabilistic inference engine in Section 4.
We then further discuss how to handle path sensitivity and
dynamic updates in Section 5.2.
3. SYSTEM OVERVIEW
In this section, we give an overview of our system.
Framework. Fig. 3 presents the architecture of our system.
It consists of three components: the variable name classier,
the probabilistic constraint generator and the probabilistic
type inference engine. Basically, our technique takes the
whole project source code, and then outputs a ranked list of
types for each variable, each type annotated with its prob-
ability. The high level work-ow of our approach contains
the following steps. First, it extracts the naming conventions
of the project by performing machine-learning on the vari-
ables that can be typed with existing static type inference
techniques. Second, it generates a set of constraints from
the data ow, attribute accesses, type check predicates, and
variable names. Third, it transforms these generated con-
straints to a probabilistic inference network, called factor
graph . Fourth, it resolves the graph using belief propagation
to get the probabilities of individual types for each variable.
Finally, it ranks the computed type probabilities (for each
variable) and lters out the unlikely types according to some
given thresholds.
Variable Name Classier. The classier takes the source
code of a Python project and applies an existing static type
608Figure 3: System framework.
inference technique [1, 7, 23, 28]. The variables that can be
typed are fed to a machine learning engine that takes both
the variable names and the corresponding types, and out-
puts a trained type classication model based on the naming
conventions of the project.
Probabilistic Constraint Generator. We generate four
kinds of probabilistic constraints, namely, data ow con-
straints ,attribute constraints ,subtype constraints and nam-
ing constraints . Let us consider the motivating example.
Given the denitions presented in Fig. 4, assume our goal is
to probabilistically infer the types of variable result . Our
approach computes the probability of result having each
type in the type domain TypeDomain . As part of the pro-
cedure, given type Response2TypeDomain , our technique
aims to infer the probability of result being a variable of the
Response type. Fig. 5 shows the corresponding constraints.
First, it generates constraints from program data ow.
According to line 6, we can generate an equivalence con-
straint (a) between variables resp andresult with 1.0 prob-
ability. Intuitively, it means the types of resp and result
must be equivalent, according to the data ow.
Second, we generate probabilistic constraints from the
attribute accesses, called attribute constraints . The con-
straints shown in (b) represent the attribute constraints be-
tween resp and type Response . The rst constraint in (b)
denotes that if resp holds an instance of Response , the at-
tribute set of type Response must contain the attributes
\url" and \ method " (i.e. with a probability HIGH = 0:95).
The constraint is constructed from lines 3 and 4. It is a
standard in probabilistic inference to use a close to 1.0 value
to denote high probability when there is uncertainty [11]. In
this case, the uncertainty comes from the dynamic attribute
updates of an object. Hence, we can only say an instance of
Response has a high probability containing attributes \ url"
and\ method ". The second constraint in (b) represents if resp
contains the two attributes \ url" and \ method ", we have a
probability 0.8 that resp is an instance of Response . We
use probability because there are other types in the domain
that also have the two attributes (e.g., Request ). Note that
we do not use probability 0.5, which has the meaning of
\there is no evidence that resp is an instance of Response ".
In contrast, a probability less than 0.5 means that there is
evidence that the statement is not true.
Third, Python programs often use explicit runtime type
check functions such as isinstance() to test the dynamic
type of an object. A path sensitive analysis that can distin-
guish the type of an object according to the branch taken
can also provide strong type hints. We model such hints as
subtype constraints (See details in Section 5.3).
Fourth, we generate probabilistic constraints according to
the variable naming conventions, called naming constraints .
We assume that each variable in the program follows certainx2VariableSet  2TypeDomain
A() represents the attribute set of the original prototype of .
P(x;) represents a predicate denoting the type of variable
xcan be type .
N(x;) represents a predicate denoting the type of variable x
can be type from the naming conventions of type .
C:ap  !brepresents a probabilistic constraint denoting predicate
ahas a probability pimplying predicate b.
Figure 4: Basic denitions.
(a)P(resp;Response )1:0  !P (result;Response )
(b)P(resp;Response )0:95   ! (f\url";\method "gA (Response )
(f\url";\method "gA (Response ))0:8  !P (resp;Response )
(c)N(resp;Response )0:7  !P (resp;Response )
N(result;Response )0:7  !P (result;Response )
Figure 5: Generated constraints of the example.
naming conventions specic to the project. The constraints
in Fig 5 (c) state if resp/result is an instance of Response
according to the naming conventions, there is 0.7 probability
that resp/result is of type Response . The probability is to
model the uncertainty that the developers may not always
obey the naming conventions.
Probabilistic Inference Engine. We conjoin the four
kinds of constraints into a probabilistic graphical model,
called factor graph , and perform belief propagation using
thesum-product algorithm. Belief propagation is an iter-
ative procedure that will eventually produce a satisfying
marginal probability of P(result;Response ), which indi-
cates the probability of variable result having type Re-
sponse . We provide the details of the probabilistic model
in next section. Such probability is computed for each type
in the type domain. For example, the probability of re-
sult being of string type is very low as the probabilities of
constraints in Fig. 5 (b) become close to 0. Eventually, a
ranked list of types is generated for each variable at each
program point.
4. PROBABILISTIC INFERENCE
In this section, we illustrate how probabilistic inference is
performed via the motivating example in Fig. 1. Assume our
goal is to infer the probability of the type of variable result
being Response . Let boolean variables x1andx2denote
the predicatesP(result;Response ) andP(resp;Response )
in Fig. 5, respectively. We then can denote the original
constraint (a) as follows.
x11:0  !x2^x21:0  !x1 (1)
Let boolean variable x3represent the predicate
f\url";\method "gA (Response ) in Fig. 5 (b), we have
the following formula.
x20:95  !x3^x30:8  !x2 (2)
With the equations (1) and (2), given the observation x3=
1, we want to infer the probability for x1= 1. Intuitively, we
aim to compute the likelihood of result being an instance
ofResponse when we observe resp contains both \ url" and
\method " attributes.
In addition, let boolean variables x4andx5represent the
predicatesN(resp;Response ) andN(result;Response ) in
Fig. 5, respectively. We thus have the following formula.
x40:7  !x2^x50:7  !x1 (3)
Formally, we represent each of the aforementioned prob-
abilistic constraints Cias a probabilistic function Fias fol-
lows.
609C1:x11:0  !x2C2:x21:0  !x1C3:x20:95   !x3
C4:x30:8  !x2C5:x40:7  !x2C6:x50:7  !x1
C7:x3= 1(1:0)C8:x4= 1(0:8)C9:x5= 1(0:4)
F1(x1;x2) =(
1:0 if (x1!x2) = 1
0:0 otherwiseF6(x1;x5) =(
0:7 if (x5!x1) = 1
0:3 otherwise
F2(x1;x2) =(
1:0 if (x2!x1) = 1
0:0 otherwiseF7(x3) =(
1:0 ifx3= 1
0:0 otherwise
F3(x2;x3) =(
0:95 if (x2!x3) = 1
0:05 otherwiseF8(x4) =(
0:8 ifx4= 1
0:2 otherwise
F4(x2;x3) =(
0:8 if (x3!x2) = 1
0:2 otherwiseF9(x5) =(
0:4 ifx5= 1
0:6 otherwise
F5(x2;x4) =(
0:7 if (x4!x2) = 1
0:3 otherwise
Figure 6: Constraints and valuation functions.
Table 1: Boolean constraints with probabilities.
x2x3x4F3(x2;x3)F4(x2;x3)F5(x2;x4)F7(x3)F8(x4)
0 0 0 0.95 0.8 0.7 0.0 0.2
0 0 1 0.95 0.8 0.3 0.0 0.8
0 1 0 0.95 0.2 0.7 1.0 0.2
0 1 1 0.95 0.2 0.3 1.0 0.8
1 0 0 0.05 0.8 0.7 0.0 0.2
1 0 1 0.05 0.8 0.7 0.0 0.8
1 1 0 0.95 0.8 0.7 1.0 0.2
1 1 1 0.95 0.8 0.7 1.0 0.8
Fi(x1;:::;xm) =
p if the constraint is true
1 potherwise(4)
wherex1;x2;:::;xmrepresent the boolean variables associ-
ated with the constraint Ciandprepresents the probability
of the constraint being true.
For our motivating example, the probabilistic constraints
Ci(i= 1;:::;9) and the corresponding probabilistic functions
Fi(i= 1;:::;9) are listed in Fig. 6. Note that, according to
lines 3 and 4 of the motivating example, the attribute set
ofresp must contain \ url" and \ method ". Therefore, the
probability of x3= 1 (C7) is 1.0. The probabilities of C8and
C9are predicted by the variable name classier according
to the naming conventions. We will explain how they are
computed in Section 5.
In general, assume there are kprobabilistic constraints C1,
C2,...,Ckonnboolean variables x1,x2, ...,xn. Functions
F1,F2, ...,Fkcorrespond to these constraints. Since all the
constraints ought to be satised, the function representing
the conjunction of the constraints is hence the product of the
corresponding probabilistic functions. Therefore, we have
the following equation.
F(x1;x2;:::;xn) =F1F 2:::Fk (5)
The joint probability function is dened as follows [33],
which is essentially the normalized version of F(x1;:::;xn).
p(x1;x2;:::;xn) =F1F 2:::FkP
x1;:::;xn(F1F 2:::Fk)(6)
According to the above equation, we can further compute
the marginal probability pi(xi) as follows.
p(xi) =X
x1X
x2:::X
xi 1X
xi+1:::X
xnp(x1;x2;:::;xn) (7)
In other words, the marginal probability is the sum over
all the variables other than xi[33]. For a type i, variable
xiasserts that the type of a given program variable be i.
Hence, in order to discover the possible types of the vari-
able, we compute xi's probability for all the i's in the type
domain and order them by their marginal probabilities.
Consider the motivating example. For simplicity, we ig-
nore the program variable result and only consider variable
resp. The related boolean variables are thus x2,x3andx4,
Figure 7: Factor graph example.
.and the corresponding probabilistic functions are F3,F4,
F5,F7andF8. Table 1 presents the values of the proba-
bilistic functions. Assume we want to compute the marginal
probability p(x2= 1), that is, the probability of the type of
variable resp being Response . The computation procedure
is shown by the following equation.
p(x2= 1) =P
x3;x4F3(1;x3)F 4(1;x3)F 5(1;x4)F 7(x3)F 8(x4)
P
x2;x3;x4F3(x2;x3)F 4(x2;x3)F 5(x2;x4)F 7(x3)F 8(x4)
=0:050:80:70:00:2 +:::+ 0:950:80:71:00:8
0:950:80:70:00:2 +:::+ 0:950:80:71:00:8
=0:532
0:6042= 0:8805
(8)
Similarly, when program variable result is also consid-
ered, we can compute the marginal probability of p(x1=
1) = 0:9052, representing that the type of result has a
probability 0.9052 being Response . On the other hand, let
us consider the probability of the type of result being Re-
quest . Since the variable resp is less likely the Request
type following the naming conventions, the name classier
computes a low probability 0.3 for N(resp;Request ). With
other constraints, we can hence infer the probability of re-
sult being a Request type variable p0(x1= 1) = 0:8622,
which is lower than the probability of the Response type.
Factor Graph. The computation of marginal probabilities
via Equation 7 could be very expensive as it has to enumer-
ate all the possible combinations of the variable valuations.
Factor Graph [33] is a probabilistic graphical model allowing
ecient computation. We present a part of the factor graph
of our previous example in Fig. 7. A factor graph consists
of two kinds of nodes: factor nodes represented by squares
andvariable nodes represented by circles. A factor node rep-
resents a probabilistic function, e.g., Fiin Equation 7. A
variable node represents a variable in the function, e.g., xiin
Equation 7. Edges are directed from a factor node to each
variable of the function.
Sum-Product Algorithm. The sum-product algo-
rithm [33] is an ecient algorithm computing marginal prob-
abilities based on factor graph . In this algorithm, probabil-
ities are propagated only between adjacent nodes through
message passing. The probability of a node is updated by
integrating all the messages it receives. Then the node will
further propagate the probability to its receivers. The al-
gorithm is iterative and terminates when the probabilities
converge. Probabilistic inference has been successfully ap-
plied to many areas such as debugging and articial intelli-
gence [15, 11, 16, 12]. In this paper, our prototype tool is
built upon pgmpy [2], an open source probabilistic graphical
modeling library in Python.
5. CONSTRAINT GENERATION
In this section, we discuss how to generate the proba-
bilistic constraints used by the type inference engine. As
discussed in Section 3, the probabilistic constraints fall into
four categories: (1) data ow constraints generated from
data ow of the program; (2) attribute constraints extracted
according to attribute accesses; (3) subtype constraints that
610Code Constraint
1:y1=x1
2:ifa1> 0:
3:z1=y1
5:else :
6:z2=b1
7:z3=(z1;z2)
8:w1=z3
9:s1=w1P(x1;)1:0  !P (y1;)
P(y1;)1:0  !P (z1;)
P(b1;)1:0  !P (z2;)
P(z3;)1:0  !P (z1;)_P(z2;)
P(z3;)1:0  !P (w1;)
P(s1;)1:0  !P (w1;)
Figure 8: An illustrative example.
denote the explicit runtime type checking conditions; (4)
naming constraints derived from variable names and the
naming conventions of the project. Note that all these con-
straints are transformed into factor graphs , from which the
type probabilities are computed for each variable.
5.1 Data Flow Constraints
Data ow constraints denote the dene-use relations
across variables. It is particularly useful for inferring types
of a variable when its attributes are never or rarely accessed
but it has def-use relations with other variables whose at-
tribute accesses provide lots of type hints. In many cases,
data ow constraints also allow our engine to aggregate the
attribute access type hints across multiple correlated vari-
ables. In our motivating example Fig. 1, there is no direct
evidence that indicates the type(s) of the variable result .
However, we can still compute its type(s) due to its data
ow correlation with variable resp.
We analyze each module of the given Python project and
build the module dependence graph. According to the mod-
ule dependence graph, we compute a topological order for all
the involved modules. For each module, we rst transform
the source code into the Single Static Assignment (SSA)
form, in which each variable is dened exactly once. We
leveragefunctions to merge analysis results from dier-
ent branches. For each assignment y=op(x1;x2;:::) in the
program, we generate a data ow constraint as follows.
P(y;)1:0 !_
xiP(xi;) (9)
Sampleopincludes copy operations and operations. The
disjunction allows high condence from any right-hand-side
variable to be propagated to the left-hand-side variable and
vice-versa. In our implementation, we generate dierent
data-ow constraints depending on the dierent operand/re-
sult types (e.g., an addition of a oating point value and an
integer value yields a oating point value). We omit the
details as they are standard.
Our analysis is piggy-backed on a standard points-to anal-
ysis. To avoid bogus data ow, we only consider must-alias
relations. Note that missing data ow is less critical to our
analysis compared to others as it only means the correspond-
ing type hints of the disconnected variables cannot be linked
to acquire higher condence. In many cases, the discon-
nected variables already have strong enough local type hints
as suggested by our results in Section 6.
Example. We use an example in Fig. 8 to illustrate how
to construct data ow constraints. The left side of the g-
ure presents the program in SSA form, and the right side
presents the corresponding constraints. In the SSA form,
we use the superscripts to denote the dierent denitions of
a variable. For example, the three denitions of zare de-
noted byz1,z2andz3, respectively. Observe at line 7, the
type predicates for z1andz2are disjointed.During the analysis, we also collect all the observable
types involved in the project, which constitute the type do-
main . Later, for each type in the domain, our engine infers
the probabilities of each variable having the type.
5.2 Attribute Constraints
Attribute constraints are extracted from attribute ac-
cesses, which provide type hints for the objects. Intuitively,
the more (unique) attribute accesses we observe about an
object, the better chance we can infer its type. Generating
attribute constraints entails addressing the following chal-
lenges:
(1)Path Sensitivity. The type of a Python program vari-
able may be path sensitive, meaning that the variable may
have dierent types along dierent paths. As discussed in
Section 2, if we do not carefully handle path sensitivity, the
attribute constraints may become contradictory and lead to
inference failure. This suggests that our attribute constraint
construction shall be path-sensitive.
(2)Dynamic Update of Attribute Set. In Python, the at-
tribute set of an object can be dynamically updated. Con-
sequently, the relation between the type of an object and
its attribute set may become uncertain. Here, we consider
that dynamic updates do not change the type of an object,
which is determined upon the object creation. Fortunately,
probabilistic analysis allows us to naturally model such un-
certainty. More details will be provided later in this Section.
(3)Incompleteness in Attribute Accesses. Attribute accesses
provide a lot of type hints. However in most cases, we can
only observe a subset of the attributes of an object. In
other words, our observation is incomplete. As a result,
there is uncertainty in such type hints even though they
do provide useful information towards resolving the type.
In our motivating example (Fig. 1), both types Response
and Request have the attributes \ url" and \ method ". The
attribute accesses allow us to narrow down the set of possible
types. But there is still uncertainty in deciding the type.
Handling Path Sensitivity (Challenge 1). To address
the rst challenge, we collect the attribute accesses of each
variablexpath-sensitively. The collection procedure con-
sists of two steps: (1) Traverse the individual paths in the
control ow graph of the procedure in which xresides and
collect the attribute accesses of xfor each path. Cycles are
broken by unrolling each loop once. Note that the num-
ber of unrolling for a loop is irrelevant in our context. (2)
Merge paths with the same set of attribute accesses and re-
move those that have an empty set. Note that if multiple
paths have the same set of attribute accesses, they do not
provide additional condence of the inferred type as the con-
dence is derived from the uniqueness of the set of attribute
accesses (i.e., how many types contain these attributes) in-
stead of the number of paths along which these accesses are
observed. Consider the example in Fig. 2. Since there are
two paths in the program, we collect two sets of attribute
accesses,f\data"gandf\request "g, for variable r.
After collecting the attribute accesses of xalong each
path, we then construct the attribute constraints of x. For
each pathi, we introduce a predicate P(xi;) to assert that
the type of variable xin pathibe. These constraints often
have the following form.
P(xi;)p  !fattributes on igA () (10)
fattributes on igA () = 1(p0) (11)
fattributes on igA ()p0
 !P (xi;) (12)
611Intuitively, the rst constraint asserts that if xiis of type
, the attributes observed on iare a subset of the attributes
of typewith probability p=HIGH (i.e., 0.95). Note that
using a close to 1.0 value instead of 1.0 to denote a very high
probability is standard in probabilistic inference [12].
Handling Dynamic Attribute Updates (Challenge
2).Ideally, if the observed attributes are indeed a subset of
the attribute set of , denoted asA(), the boolean variable
denotingfattributes on igA () has the true value, false
otherwise. However in practice, if the attributes are not a
subset of's attributes (according to 's denition/original
prototype), we cannot simply set the boolean variable to
false as this may be caused by dynamic attribute updates.
For instance, assume a new attribute a0is added to xand
then accessed. The observed attributes are hence not a sub-
set of's attribute set although the type of xis still. To
model dynamic updates, ideally, we would identify all at-
tribute updates (especially attribute additions) and the vari-
ables that are aected by these updates, and then change
the attribute constraints according to the updates. In the
aforementioned example, a0should be precluded from the
observed attribute set of the constraint. However, achiev-
ing soundness in such analysis requires a full-edged path-
sensitive analysis. Our solution is hence to use probability to
model the uncertainty caused by dynamic updates. Speci-
cally, when the observed attribute accesses are not a subset
of's attribute set, we assign a probability p0to the pred-
icate (Equation 11), depending on the number of observed
attributes that are part of 's attribute set. The computa-
tion ofp0is represented by the following formula.
p0=LOW +jfattributes on ig\A ()j
jfattributes on igj(HIGH LOW )
(13)
Note that if all the observed attributes are part of 's at-
tribute set, p0=HIGH . If none of the observed attributes
are part of 's attribute set, p0=LOW (i.e., 0.05). An
example will be presented later in this section. 2
Handling Incompleteness in Attribute Accesses
(Challenge 3). Equation 12 asserts given that the at-
tributes observed on iare a subset of the attributes of type
,xiis of typewith probability p0. The probability p0
is determined by the uniqueness of the observed attributes,
that is, how many other types also contain these attributes.
The computation is represented by the following formula.
p0= 0:5 + 0:5(2HIGH 1)1
N(14)
Nrepresents the number of types in the observable do-
main which contain the attributes that we observed. When
Nis very large, p0is close to 0.5, meaning that we are com-
pletely uncertain if xiis of typeor not. Note that p0>0:5
means that we are positive about the assertion and p0<0:5
means that we are negative (i.e., there are evidence for xi
not having type ). WhenN= 1,p0=HIGH , meaning
that we are highly condent that xiis of typewhen the
observed attributes are unique.
Computing the Probability at the Denition Point.
Intuitively, if xhas typeat pathi, it also has type at its
denition point. Formally, if the attribute accesses of xcan
be collected from multiple paths, we use the maximal proba-
bility of all paths to represent the one at its denition point.
The computation is represented as the following formula.
p(x) =MAX (p(x1);p(x2);:::;p (xn)) (15)#Typehas attributes a1,a2,a3, and a4in its denition.
#Types1and2have attributes a1anda2in its denition.
#xis of type.
1:def foo(...):
2: x = ...
3: if...:
4: ... x.a1
5: else :
6: ... x.a2
7: gee(x)
8: if...:
9: ... x.a310: def gee(t):
11: if...:
12: ... t.a2
13: else :
14: ... t.a4
15 t.a5=... #attr. add
16: if...:
17: ... t.a5
Figure 9: Example code for attribute constraints.
path constraint
(1) TT in foo()(a)P(x1;)0:95   !fa1;a3gA ()
(b)fa1;a3gA () = 1(0:95)
(c)fa1;a3gA ()0:95   !P (x1;)
(2) TF in foo()(d)P(x2;)0:95   !fa1gA ()
(e)fa1gA () = 1(0:95)
(f)fa1gA ()0:65   !P (x2;)
(3) FT in foo() ...
(4) FF in foo() ...
(5) TT in gee()(g)P(t1;)0:95   !fa2;a5gA ()
(h)fa2;a5gA () = 1(0:5)
(i)fa2;a5gA ()0:65   !P (t1;)
... ...
Figure 10: Attribute constraints for individual paths
of the example in Fig. 9. \TT in foo() " means a
path in foo() in which both predicates take the true
branch.
p(x) represents the probability of xhaving type at its
denition point. Note that the probability p(xi) of each
pathican be computed by formulas stated in Section 4.
The Essence of Path Sensitivity in Our Technique. We can-
not aord full-edged path-sensitivity in general due to the
exponential number of possible paths. We only consider
path sensitivity of attribute accesses for the same variable
(within the same procedure). Specically, for each variable
xin a procedure, we collect its attribute accesses path-
sensitively within the procedure. Note that xmay be re-
lated to other variables. The attribute accesses for those
variables are collected independently and also in a path sen-
sitive fashion. The correlations between xand these vari-
ables are modeled by the data ow constraints discussed in
Section 5.1 that are path insensitive. From the results in
Section 6.2, such a design allows us to achieve a balance
between overhead and eectiveness.
Example. Consider the example in Fig. 9. Variable xis of
typethat has attributes a1,a2,a3, and a4. However this
is unknown and we want to infer x's type from the code.
Variablexis dened at line 2 and its attributes a1,a2, and
a3are accessed at lines 4, 6 and 9, respectively. It is also
passed to function gee() in which its attributes are accessed.
A new attribute a5is added to the object at line 15.
Ideally, we would explore the individual whole program
paths to construct attribute constraints. For example, a
path 2!3!4!7!11!14!15!16!
17!8!9 contains accesses to attributes a1,a4,a5,
and a3. Such whole-program path-sensitive analysis in-
curs prohibitive overhead. Instead, we collect the attributes
for variable xinfoo() along the intra-procedural path
2!3!4!7!8!9 and the attributes for tin
gee() along 11!14!15!16!17. The former has
attributes a1anda3whereas the latter has a4anda5. The
likelihoods of xhaving type andthaving type are rst in-
ferred independently based on the intra-procedural attribute
6121: s = f(*args, **kwargs)
2:if isinstance (s, basestring ):
3: y = s.lower()
4:else :
5: y = str(s)
Figure 11: An example for subtype constraints
sets. They are further aggregated through the data-ow con-
straint between xandt.
Fig. 10 shows the attribute constraints for individual
paths. The rst four rows denote the attribute constraints
for the four paths in foo() . In particular, constraint (a)
means that if xis of typein path (1) (both predicates
take the true branch), it is highly likely that the observed
attributes a1and a3are part of the attributes of . Con-
straint (b) means that a1and a3are highly likely to be in
the attribute set of . Constraint (c) asserts the reversion of
(a). Note that since the observed attributes a1and a3are
unique to, the probability on top of the arrow is HIGH .
Constraints (d)-(f) are similar except that the probability
in (f) is 0.65 due to the fact that the observed attribute a1
along path 2 is not unique to . Note that 1and2also
have a1. The probability 0.65 is computed by Equation 14.
Constraints (g)-(i) are for a path in gee() with both pred-
icates taking the true branch. Observe that constraint (h)
denotes that even though a2and a5are not part of the at-
tribute set of due to the dynamic attribute addition at
line 15, we do not set the predicate fa2;a5gA () to false.
Instead, it has a probability 0.5 of holding a true value, ac-
cording Equation 13.
According to equations in Section 4, we can infer the prob-
ability ofxhaving type at paths (1), (2) and (5) is 0.91,
0.63 and 0.41, respectively. With Equation 15, we further
infer the probabilities of xandthavingat their denition
points are both 0.91. Combining the data-ow constraints
P(x;)1:0 !P (t;), we have the nal likelihood of xbeing
ofis 0.99. Similarly, the likelihood of xhaving1is 0.74.
5.3 Subtype Constraints
Subtype constraints are extracted from the type check-
ing statement isinstance (x,), which implies the possible
types of a variable. We also generate the subtype constraints
path sensitively, which is similar to the generation of at-
tribute constraints. We use a simple example in Fig. 11 to
illustrate how we generate subtype constraints. In this ex-
ample, line 1 is an external function call, so we do not know
the type of its return value, and hence the type of variable
s. Line 2 represents a subtype checking of s. Intuitively,
we know that when the execution goes into the true branch
(line 3), the type of smust be a subtype of basestring . On
the other hand, when the false branch (line 5) is taken, it
must not be a subtype of basestring . Accordingly, assume
we are computing the likelihood of the type of variable s
being, we can generate the subtype constraints as follows.
(a)P(s1;)0:95  ! (4basestring )
(b)P(s2;)0:05  ! (4basestring )
Here,s1ands2represent variable sin the two branches,
and4represents a subtype relation. Intuitively, (a) denotes
that when observing is a subtype of basestring (e.g., str),
we know the type of variable sat line 3 is very likely to be
, and vice versa. Constraint (b) is similarly interpreted.
5.4 Naming ConstraintsOur technique is based on probabilistic inference, which
allows us to take into consideration uncertain type hints
even from the natural language perspective. The intuition
is that when programmers name a variable, they often fol-
low some conventions implying its type. In our motivating
example in Fig. 1 we observe that lots of variables are lexi-
cally similar to response (e.g. r,resp), and most of them
are indeed of the Response type. Besides, we can also infer
the type of a variable according to other natural language
features, such as part-of-speech and singular/plural form of
nouns. For instance, a variable with a name starting with a
verb (e.g., has_connected ) very likely represents a boolean
variable. When a variable's name is plural (e.g., connec-
tions), it probably represents a collection (e.g., list). Such
type hints are uncertain and hence cannot be leveraged by
most existing data-ow based type inference techniques. To
model uncertainty, we use probabilistic constraints to rep-
resent the implicit relations between variables and naming
conventions. In the following, we rst illustrate how we use
machine learning to mine the implicit naming conventions,
and then discuss how to generate the naming constraints.
Naming Convention Learning. The learning component
takes three inputs: (1) the variable set; (2) the observed
type domain; and (3) the (partial) mapping from variables
to their types, and nally produces a variable name classier
and a variable type classier. The process consists of three
main steps, First, it lexically clusters variable names using
k-means . Then, it extracts natural language features from
each variable. Finally, it trains the classication model for
each type in the domain.
Clustering Variable Names. The procedure of clustering
variable names consists of three steps: (1) normalize the
variables; (2) compute variable name similarities; (3) cluster
the variables using k-means. Normalization of a variable x
entails the following. First, we remove all the tail digits
ofx's name if any. Second, we identify all the terms of
x's name via some commonly used separators (e.g.," " and
capital letters). Third, we transform each term into its lower
case. Finally, we concatenate them again using separator
\". For instance, variable hasConnected2 is normalized to
\has connected". For the computation of lexical similarity
ofx1andx2, we use the following formula.
sim(x1;x2) =jlcs( ~x1;~x2)j+jlas( ~x1;~x2)j
2min(j~x1j;j~x2j)(16)
Here,jlcs( ~x1;~x2)jdenotes the length of the longest com-
mon substring between ~ x1and ~x2, which denote the nor-
malizedx1andx2, respectively.jlas( ~x1;~x2)jrepresents the
length of the longest common abbreviation between ~ x1and
~x2. We use the longest common abbreviation in addition
to the longest common substring because we observe that
abbreviations are widely used. For example, programmers
may use a abbreviation \ pgm" to represent the word \ pro-
gram". However, if we only use the longest common sub-
string, the similarity between the two is low. In contrast,
when the common abbreviation is considered, the similar-
ity becomes much higher. For clustering variable names, we
use k-means, which produces a classier that can identify
the variable name cluster of a given variable. The classier
is then used in the feature extraction for variables.
Extracting Natural Language Features. We extract
four kinds of features from each variable. Given a variable
x, we rst compute the cluster id of this variable using the
613aforementioned classier. Second, we extract the part-of-
speech features of x. We rst split the normalized name of
xinto terms and then count the frequencies of the verbs
and nouns as features. Third, we extract the singular/plural
form feature of x's name. We split the normalized xinto
terms. If the terms contain a plural word, we set the feature
to 1 , otherwise 0. Finally, we extract the lexical similarity
between the normalized xand each normalized name of type
in the domain. We extract this feature because we observe
that a lot of variables are lexically similar to its type name.
Training Classication Models. We train a model for
each type in the observed domain. It is a standard super-
vised machine learning procedure. Specically, given type
, we rst extract the aforementioned four kinds of features
for each variable xand set the target label as 1 when the
inferred types of xcontain(0 otherwise). We put all the
data into a SVM classier and train a model for , denoted
asM(). With the trained model M(), if we have a new
variablex0, we can predict the probability of the type of
x0being. The predicted probability will be used in our
naming constraints.
Constructing Naming Constraints. As we will show
in Section 6, due to the uncertainty of naming conventions,
the probabilities produced by the models alone cannot eec-
tively predict correct types. Instead, we represent them as
naming constraints that are used together with other con-
straints to infer types. To construct naming constraints, we
introduce a threshold to represent the weight of the nam-
ing conventions in type inference. Basically, for a given type
and a variable x, we have the following constraint:
N(x;)  !P (x;) (17)
N(x;) represents that the type of xcan beaccording
to the naming model M(). In our motivating example,
we set the threshold = 0:7 (seeC5andC6in Fig. 6).
Note that we set the initial probability of N(x;) to the
predicted probability M(). For example, we assume that
the initial probability as 0.8 in our motivating example (see
C8in Fig. 6). Due to the space limitations, we move the
details to our technical report [34].
6. EVALUATION
We have implemented a prototype in Python, which con-
sists of three components, namely, the naming convention
learning component, the constraint generator and the infer-
ence engine. The naming convention component relies on
a widely used type inference engine PySonar2 [1] to infer
variable types. The machine learning subcomponents are
implemented using sklearn [3]. The probabilistic inference
engine is built on pgmpy [2]. We make our tool along with
the evaluation environment publicly available at [34].
Our evaluation aims to address four research questions:
RQ1: How eective is our approach in inferring variable
types compared to existing static analysis?
RQ2: How ecient is our approach? Can it scale to real-
world Python programs?
RQ3: What is the impact of the thresholds?
RQ4: What is the impact of each kind of constraints?
6.1 Experiment Setup
We use PySonar2 , a static Python type inference engine,
as both the baseline for comparison and a building block for
the naming convention component. PySonar2 is based onabstract interpretation [1], an advanced static analysis tech-
nique. It was previously used by Google Inc. and its under-
lying analysis engine is currently used by SourceGraph [4], a
widely used analysis system for code repositories. We could
not compare with other research prototypes as they are not
publicly available or cannot be properly installed.
Preparation. We extract variables whose types cannot be
inferred by PySonar2 . To collect the ground truth for these
variables, we developed a dynamic analysis collecting the
runtime types of variables by executing all the test cases
of the benchmarks. The variables that are typed dynami-
cally but not statically are the targets of our experiment. In
other words, we want to observe how many of them can be
typed by our technique and how precise the inferred types
are compared to the observed types. Note that the dynamic
analysis is only for the experiment, our technique is static.
Benchmarks. We select a set of real-world Python
projects, as shown in Table 2 (columns 1-2). Most of them
are from GitHub and widely used in practice. When choos-
ing these projects, we also considered diversity. Observe that
some of them are large, with the largest over 54K lines of
Python code. These projects mainly fall into the following
domains:
Httpbin ,httpie ,requests andurllib3 - popular HTTP
libraries and applications.
Paramiko - a widely used SSHv2 library.
Fabric - a well known remote deployment and system
administration application.
Bs4,pyquery and simplejson - popular HTML/XML
parsing libraries.
Bottle ,cherrypy ,ï¬‚ask,tornado and web2py - widely
used web application and development frameworks.
Pyspider - a famous web crawling toolkit.
Click - a well known command line utility.
GitPython - a popular library used to interact with Git.
Computation of Recall and Precision. Before describ-
ing the results, we rst explain how we evaluate the eec-
tiveness. As described in Section 3, our system essentially
computes an ordered list of types with probabilities. When
the computed probability of a type is less than 0.5, we say
the variable unlikely has that type. For types whose prob-
abilities are over 0.5, we cluster them by a threshold GAP .
Intuitively, we traverse the ranked type list (of a variable)
and compute the gap gof probabilities between any two ad-
jacent types 1and2. Ifg > GAP , we introduce a new
cluster starting with 2. Eventually, we partition the list
to clusters with the gap larger than GAP . Our tool will
only report the rst cluster. In practice, the rst clusters
may also be large. To avoid overloading the users, we intro-
duce another threshold TOPn , which denes the maximum
number of types the system reports.
We leverage the dynamically collected types to compute
recall for variable x, denoted as R(x), in the following.
Rx=jD(x)^C1(x;TOPn )j
jD(x)j(18)
D(x) represents the dynamically collected type set of vari-
ablex.C1(x;TOPn ) represents the reported type cluster of
x, whose size is bounded by TOPn .
For precision P(x), we use the following.
Px=jFeasible (C1(x;TOPn ))j
jC1(x;TOPn )j(19)
614Table 2: Summary of the experiment results.
BenchmarksVar(#/%)Positives
(%)Recall(%) Precision(%) Type Set Size(Avg.)Time(s)Name SLOC Top 3 Top 5 Top 7 Top 3 Top 5 Top 7 Top 3 Top 5 Top 7
httpbin 744 150/36.82 97.33 85.62 90.41 90.41 90.05 88.86 88.35 1 2 2 0.1557
httpie 2243 424/47.71 95.99 77.11 80.88 81.12 85.34 83.88 82.48 2 2 3 0.4473
paramiko 8267 1517/38.65 96.11 86.08 87.58 88.20 84.60 82.05 80.77 2 3 3 0.2301
urllib3 3112 820/59.28 97.44 80.05 82.04 82.66 83.34 81.23 80.09 2 3 3 0.7456
requests 10595 856/57.80 97.78 72.98 77.94 83.40 83.19 80.90 79.38 2 3 3 0.8125
fabric 3061 832/44.39 97.84 77.64 80.53 82.41 85.30 83.80 82.46 2 2 3 0.2211
bs4 3341 1202/68.41 97.09 72.13 76.08 78.47 80.48 77.31 75.65 2 3 4 0.9185
simplejson 4104 299/55.12 95.32 75.73 79.29 78.62 88.58 87.18 86.08 1 2 2 0.375
pyquery 1133 412/53.18 96.60 70.14 70.39 72.03 83.94 80.94 79.65 2 3 3 0.5734
bottle 2569 643/49.56 97.05 77.08 79.79 80.29 85.36 83.55 82.31 2 2 3 0.1454
cherrypy 18795 1277/45.62 99.30 76.49 80.25 80.86 84.15 82.22 80.92 2 3 3 0.6275
ask 2658 893/61.39 96.98 71.29 73.83 74.24 85.00 82.59 81.39 2 2 3 0.4805
tornado 11134 3144/56.02 95.29 76.54 78.81 79.89 85.07 82.69 81.54 2 2 3 0.9221
web2py 54479 1931/43.89 92.18 69.62 72.72 74.65 82.78 80.70 79.18 2 3 3 1.0143
pyspider 7129 1612/49.20 97.08 72.49 75.62 76.19 86.81 84.67 83.64 2 2 3 1.4565
werkzeug 11009 3094/53.71 95.99 73.56 77.60 79.42 83.01 80.34 79.05 2 3 4 1.2348
click 3837 1069/52.34 99.35 78.12 82.03 83.22 85.83 83.82 82.81 2 2 3 0.5822
GitPython 5838 2179/54.44 98.16 74.00 77.75 79.56 86.46 84.81 83.56 2 2 3 0.8258
Average 8588/51.53 96.83 75.93 79.09 80.31 84.96 82.86 81.63 2 2 3 0.6538
Feasible (C1(x;TOPn )) represents the reported types that
are feasible at runtime. Since we do not have the ground
truth, for each possible cluster size ranging from 1 to TOPn ,
we randomly select 20 variables to estimate the overall av-
erage precision. For each variable, we use the following pro-
cedure to determine if a type in the reported type set is
feasible. (1) When the variable is a parameter (to an exter-
nal API function), we rst check the documentation of the
API to determine if is correct. If there is no documenta-
tion, we randomly create a value of and pass it as an input
to the function leveraging existing unit tests. If the execu-
tion does not crash, we consider it feasible. (2) If a variable
is assigned the return value of an API call, we rst check
documentation. If the documentation is not available, we
replace the function call with a random value of type, and
observe whether the execution will induce a type exception.
Eventually, we take the average over the precision of all the
sampled variables.
Our experiments were on an Intel i7-3770U machine with
16GB RAM, Ubuntu 14.04 and Python 2.7.6.
6.2 Experimental Results
Table 2 shows the summary of results. Column 3 ( Var#)
presents the number and percentage of the variables whose
types are observed during dynamic runs but cannot be com-
pletely inferred by PySonar2 . Observe that PySonar2 fails
to infer types for a large number of variables, ranging from
hundreds to thousands. Note that we do not consider vari-
ables whose types are not observed during execution as we
cannot acquire any ground truth for those variables.
Eectiveness Evaluation (RQ1). For the evaluation
of eectiveness, we use the naming convention threshold
= 0:7 (described in Section 5.4), and the probability gap
GAP = 0:1 (described in Section 6.1). Columns 4-13 present
the results. Column 4 ( Positives ) presents the percent-
age of variables having at least one type whose probability
is over 0.5. Observe that most variables have at least one
positive type. Columns 5-7 present the results of recall. We
choose three TOPn congurations, namely, 3, 5, and 7. Ob-
serve that we have a high recall with an average 75.93% at
top 3, 79.09% at top 5 and 80.31% at top 7. Also observe
that the recall becomes higher along with the increase of
TOPn . Columns 8-10 present the precision. Observe that
we also have a high precision with an average 84.96% at top
3, 82.86% at top 5 and 81.63% at top 7. The precision de-creases with the increase of TOPn . Columns 11-13 describe
the average size of the reported (rst) cluster. Observe that
the size is reasonably small.
Analysis Time (RQ2). Column 14 presents the average
time for inferring the type of a variable. Observe that most
variables can be inferred in one second, with an average
0.6538 second. This suggests our technique is suciently
fast to be used in IDE (for real-world Python programs).
Impact of Thresholds (RQ3). We study the impact of
three thresholds, namely, GAP ,HIGH and. We use the
thresholdTOPn = 5. Fig. 12(a) shows the impact of GAP .
We evaluate four settings, namely, 0.05, 0.10, 0.15 and 0.20.
Observe that the recall has a small increase with the increase
ofGAP , while the precision decreases a little bit. Fig. 12(b)
shows the impact of HIGH . Observe that the impact on
both recall and precision is negligible. Fig. 12(c) presents
the impact of . Observe that it has an impact similar to
GAP but in the other direction.
Impact of Constraints (RQ4). We evaluate the impact
of each kind of constraints by adding one kind at a time
with the order of attribute constraints ,subtype constraints ,
data-ow constraints andnaming constraints . For precision,
we only focus on variables having at least one positive type.
Fig. 12(d) shows the results. Observe that each kind has
positive contribution to the recall. Especially, the impact of
naming constraints is signicant. The precision overall has
a marginal decrease with the additions of constraints.
Threats to Validity. Our evaluation is performed on a
limited set of programs. It is possible that the performance
of our technique may vary for other programs. The evalu-
ation of precision is sample based and requires substantial
manual eorts. Sampling errors and human errors may skew
the results. Our technique depends on a set of parameters.
Although we have studied a number of parameter settings,
the study is incomplete due to the large parameter space.
7. RELATED WORK
Probabilistic Modeling. Recently, probabilistic graph
models were applied to type inference of JavaScript. Ray-
chev et al. [10] predicted JavaScript types by learning a prob-
abilistic model from a large repository of programs (\big
code"). Our technique is dierent from theirs. First, [10]
assumes a large number of programs from which variable
names and types can be learned. In contrast, our technique
615(a)
(b)
(c)
(d)
Figure 12: Impact of the thresholds (a-c) and each kind of constraints (d).
works on a single project. We do not assume a large code
repository. Note that many Python types are project specic
(30% by our experiment), only used in a project. Second,
the model in [10] is monolithic and encodes properties of
the entire program all together. A solution is global opti-
mal which may sacrice local optimality. In contrast, we
analyze types of individual variables one by one, using sepa-
rate models. Hence, our approach achieves local optimality,
potentially having better precision and recall for individ-
ual variables. Note that during development, programmers
likely query about types of individual variables instead of all
variables. Third, we leverage project specic naming con-
ventions, which have signicant contribution to the perfor-
mance according to our evaluation (Section 6). Probabilistic
inference has also been widely used in many other areas such
as software debugging [15], security [13], and specication
extraction [11, 16, 12, 14]. Although we rely on a simi-
lar underlying inference engine, we address a set of unique
challenges because of the dierent application domain.
Type Inference for Python. Several analyses have been
developed for type inference of (a subset of) Python. Slib
et al. [7] infer ow-insensitive types based on the Cartesian
Product Algorithm (CPA). Cannon et al. [8] analyze atomic
types from a local view of procedures. Aycock et al. [9] infer
types for a subset of Python aggressively according to type
consistency. Rigo et al. [5] and Gorbovitski et al. [6] opti-
mize Python programs using abstract-interpretation based
type inference. These approaches use forward analyses to
infer variable types, and hence when encountering external
function calls, they heavily rely on manual mocking. They
may also fail to infer lots of types due to the lack of test
drivers when analyzing libraries. Our approach is based on
collecting type hints according to how variables are used,
which requires much less mocking and test drivers. In addi-
tion, it leverages uncertain type hints from variable names
and the reasoning engine is probabilistic inference.
Type Analysis for Other Dynamic Languages. Manytype analyses have been proposed on other dynamic lan-
guages such as JavaScript [21, 22, 23, 24, 25], Ruby [26,
27, 28, 29] and PHP [30, 31]. TypeDevil [18] detects type
inconsistencies in JavaScript according to dynamic observa-
tions of types. Their method is dynamic analysis based and
depends on test drivers. Although their approach removes
false positives using belief analysis, their belief is not a no-
tion in probabilistic inference. DRuby [28] statically infers
types for a subset of Ruby. It also starts from variables
with known types. Rubydust [26] dynamically infers types
in Ruby, and hence heavily depends on test coverage. Zhao
et al. [31] developed a type inference technique for PHP op-
timizations. Ours is mainly for programming support and
maintenance.
8. CONCLUSION
We propose a probabilistic inference based Python type
inference technique. It allows us to leverage various type
hints such as those derived from data ow, attribute ac-
cesses, and variable names. Some of them are uncertain.
Our results show that our technique substantially outper-
forms a state-of-the-art type inference engine based on ab-
stract interpretation. Our technique can type 79.09% of the
variables that cannot be typed by abstract interpretation,
with estimated 82.86% precision.
9. ACKNOWLEDGMENTS
This research was partially supported by DARPA un-
der contract FA8650-15-C-7562, NSF under awards 1409668,
1320444, and 1320306, ONR under contract N000141410468,
Cisco Systems under an unrestricted gift, China Scholar-
ship Council (CSC) Scholarship, National Basic Research
Program of China (973 Program) No.2014CB340702, Na-
tional Natural Science Foundation of China No.91418202,
No.61472178 and No.91318301. Any opinions, ndings, and
conclusions in this paper are those of the authors only and
do not necessarily reect the views of our sponsors.
61610. ARTIFACT DESCRIPTION
We package our system in a VMWare image which can be
lauched on a host with VMWare Workstation (version 10)
installed. In this section, we give detailed instruction about
how to use the artifact.
Installation of VMWare Workstation. Our arti-
fact VM requires VMWare workstation 10, which can be
downloaded from [35]. Note that the Linux version of
VMWare Workstation is packaged in a \ *.bundle " exe-
cutable so that one can use the command\ sudo sh /path/-
to/<ï¬lename>.bundle " to install it.
Load the VM Image. First of all, go to the project web-
site https://sites.google.com/site/pyprobatyping/ to down-
load the VM image. After decompression, start VMware
Workstation and go to menu File>Open>Select the *.vmx
ï¬leto load the image.
Requirements for Starting the Image. The initial
memory allocated for our VM image is 7GB, and the
number of processors and cores is 2x2. If the host ma-
chine cannot support such a conguration, please go to
VM>Settings>Hardware Tab to reset the resources before
launching the VM. We have not tested our system with less
than 7GB allocated memory. In addition, the image requires
at least 15GB free disk space on the host. To get better per-
formance, we would suggest to allocate more resources (e.g.,
memory and processors).
Artifact Contents. After loading the image, the HOME
directory contains the following contents:
A README le on the desktop ( $HOME/Desktop ) of
the VM, including basic description of the working di-
rectory, execution requirements, instructions, evalua-
tion and so on.
The Python environment with our tool and the re-
quired libraries installed.
The working directory ( $HOME/Current/NamingPro-
ject), including benchmarks, data, test drivers and
running scripts. The structure is listed as follows:
{Benchmarks folder, including all the benchmark
sources.
{Data folder, including all the collected runtime
data (e.g., variables and types) by our tracing
tool.
{SData folder, containing all the static data gener-
ated by PySonar2, which includes the statically
inferred types of individual variables and data
ow between variables.
{MData folder, including all the merged data of dy-
namically and statically collected. This folder is
initially empty and will be dynamically lled by
our tool.
{tests folder, including all the running scripts,
congurations and test drivers.
We provide a number of case studies on our website https:
//sites.google.com/site/pyprobatyping/.
Running the Tool. First of all, open a terminal, and then
move to the working directory by command: cd $HOME-
/Current/NamingProject/tests . We provide two kinds
of evaluation. One is to execute all the benchmarks by
command: ./run.sh [-l=<N>] <HIGH> <ETA> . Here, [-
l=<N>] is an option to choose how many kinds of constraints
(N=1...4 ) are considered with the order corresponding toFig. 12(d). Note that all kinds of constraints will be in-
cluded if this option is omitted (i.e., N=4by default). <HIGH>
and <ETA> represents the high probability threshold HIGH
and the belief threshold of the naming convention, respec-
tively. For instance, to evaluate the results of Table 2, one
needs to input the command ./run.sh 0.95 0.7 . For de-
tails of these thresholds, please refer to Sections 3 and 5.4.
It will take several hours to complete all the benchmarks.
Another choice is to run each benchmark one by one via
command: python run.py test-XX-YY [-l=<N>] <HIGH>
<ETA> where XXand YYrepresent the category and project
name, respectively. One can nd them in the form of test-
XX-YY.py in the working test directory ( $HOME/Current/-
NamingProject/tests ). Take the benchmark httpbin as
an example, the corresponding command is python run.py
test-httptools-httpbin 0.95 0.7 .
Evaluating the Results. The evaluation goal is to repro-
duce results shown in Table 2 and Figure 12. These are the
main results of our experiment. To evaluate the results step
by step, please follow the instructions in Section [Evalua-
tion] of README. Our tool will output all the results in
the folder log/<project-name> . The logged les are listed
as below:
raw-diff-same-summaries.txt stores the overall fail-
ure percentage of PySonar2. Note that the results are
based on the traced variables.
analysis-summaries-<N>-<HIGH>-<ETA>.txt stores
partial (summarized) results corresponding to Table
2 and Figure 12.
analysis-results-<N>-<HIGH>-<ETA>.txt presents
the detailed results of the inferred types associated
with probabilities. Refer to README for the meaning
of each record.
11. REFERENCES
[1] PySonar2. https://github.com/yinwang0/pysonar2.
[2] pgmpy. http://pgmpy.org/.
[3] sklearn. http://scikit-learn.org/stable/.
[4] SourceGraph https://www.sourcegraph.com/
[5] A. Rigo and S. Pedroni. Pypy's approach to virtual
machine construction. In ACM SIGPLAN International
Conference on Object Oriented Programming Systems
Languages &Applications (OOPSLA) , 2006.
[6] M. Gorbovitski, Y. A. Liu, S. D. Stoller, T. Rothamel,
and K. T. Tekle. Alias analysis for optimization of
dynamic languages. In Dynamic Languages Symposium
(DLS) , 2010.
[7] M. Salib. Starkiller: A static type inferencer and
compiler for python. In Master's thesis, MIT , 2004.
[8] B. Cannon. Localized type inference of atomic types in
python. In Master's thesis, California Polytechnic State
University , 2005.
[9] J. Aycock. Aggressive type inference. In International
Python Conference , 2000.
[10] V. Raychev, M. Vechev and A. Krause. Predicting
Program Properties from \Big Code". In 42th ACM
SIGPLAN-SIGACT Symposium on Principles of
Programming Languages (POPL) , 2015
[11] N. E. Beckman and A. V. Nori. Probabilistic, modular
and scalable inference of typestate specications. In
32nd Annual Conference on Programming Language
617Design and Implementation (PLDI) , 2011.
[12] B. Livshits, A. V. Nori, S. K. Rajamani, and
A. Banerjee. Merlin: specication inference for explicit
information ow problems. In 30th Annual Conference
on Programming Language Design and Implementation
(PLDI) , 2009.
[13] Z. Lin, J. Rhee, C. Wu, X. Zhang, D. Xu DIMISUM:
Discovering Semantic Data of Interest from
Un-mappable Memory with Condence. In 19th Annual
Network &Distributed System Security Symposium
(NDSS) , 2012.
[14] A. Cozzie, F. Stratton, H. Xue, and S. T. King.
Digging for data structures. In 8th USENIX Symposium
on Operating Systems Design and Implementation
(OSDI) , 2008.
[15] L. Dietz, V. Dallmeier, A. Zeller, and T. Scheer.
Localizing bugs in program executions with graphical
models. In 23rd Annual Conference on Neural
Information Processing Systems (NIPS) , 2009.
[16] T. Kremenek, P. Twohey, G. Back, A. Ng, and
D. Engler. From uncertainty to belief: inferring the
specication within. In 6th USENIX Symposium on
Operating Systems Design and Implementation (OSDI) ,
2006.
[17] B. Moghaddam, T. Jebara, and A. Pentland. Bayesian
face recognition. In Pattern Recognition , 2000.
[18] M. Pradel, P. Schuh, and K. Sen. TypeDevil:
Dynamic type inconsistency analysis for JavaScript. In
37th International Conference on Software Engineering
(ICSE) , 2015.
[19] L. Gong, M. Pradel, M. Sridharan, and K. Sen. DLint:
dynamically checking bad coding practices in
JavaScript. In International Symposium on Software
Testing and Analysis (ISSTA) , 2015.
[20] K. Sen, S. Kalasapur, T. Brutch, et al. Jalangi: A
selective record-replay and dynamic analysis framework
for JavaScript In 9th Joint Meeting of the European
Software Engineering Conference and the ACM
SIGSOFT Symposium on the Foundations of Software
Engineering (ESEC/FSE) , 2013.
[21] W. Choi , S. Chandra, G. Necula, K. Sen SJS: A Type
System for JavaScript with Fixed Object Layout In
22nd International Static Analysis Symposium (SAS) ,
2015.
[22] P. Heidegger and P. Thiemann. Recency types for
analyzing scripting languages. In 24th European
Conference on Object-Oriented Programming
(ECOOP) , 2010.[23] S. H. Jensen, A. Mller, and P. Thiemann. Type
analysis for JavaScript. In 16th International Static
Analysis Symposium (SAS) , 2009.
[24] C. Anderson, P. Giannini, and S. Drossopoulou.
Towards type inference for javascript. In 19th European
Conference on Object-Oriented Programming
(ECOOP) , 2005.
[25] P. Thiemann. Towards a type system for analyzing
JavaScript programs. In European Symposium on
Programming (ESOP) , 2005.
[26] J. D. An, A. Chaudhuri, J. S. Foster, and M. Hicks.
Dynamic inference of static types for Ruby. In 38th
ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages (POPL) , 2011.
[27] M. Furr, J. D. An, and J. S. Foster. Prole-guided
static typing for dynamic scripting languages. In ACM
SIGPLAN International Conference on Object Oriented
Programming Systems Languages &Applications
(OOPSLA) , 2009.
[28] M. Furr, J. D. An, J. S. Foster, and M. W. Hicks.
Static type inference for ruby. In 24th Annual ACM
Symposium on Applied Computing (SAC) , 2009.
[29] J. D. An, A. Chaudhuri, and J. S. Foster. Static
Typing for Ruby on Rails. In 24th International
Conference on Automated Software Engineering , 2009.
[30] E. Kneuss, P. Suter, and V. Kuncak. Runtime
instrumentation for precise ow-sensitive type analysis.
In1st International Conference on Runtime
Verication , 2010.
[31] H. Zhao, I. Proctor, M. Yang, X. Qi, M. Williams,
Q. Gao, G. Ottoni, A. Paroski, S. MacVicar, et al. The
hiphop compiler for php. In ACM SIGPLAN
International Conference on Object Oriented
Programming Systems Languages &Applications
(OOPSLA) , 2012.
[32] O. Lhot ak , L. Hendren. Scaling Java points-to
analysis using SPARK, In 12th International
Conference on Compiler Construction , 2003.
[33] J. S. Yedidia, W. T. Freeman, and Y. Weiss.
Understanding belief propagation and its
generalizations, Exploring articial intelligence in the
new millennium 8 , 2003.
[34] PyProbaTyping.
https://sites.google.com/site/pyprobatyping/.
[35] VMWare Workstation
https://my.vmware.com/web/vmware/info?slug=
desktop enduser computing/vmware workstation/10 0
618