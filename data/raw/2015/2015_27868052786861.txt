Systematic Testing of Asynchronous Reactive Systems
Ankush Desai
University of California,
Berkeley, USA.Shaz Qadeer
Microsoft Research,
Redmond, USA.Sanjit Seshia
University of California,
Berkeley, USA.
ABSTRACT
We introduce the concept of a delaying explorer with the
goal of performing prioritized exploration of the behaviors
of an asynchronous reactive program. A delaying explorer
straties the search space using a custom strategy, and a de-
lay operation that allows deviation from that strategy. We
show that prioritized search with a delaying explorer per-
forms signicantly better than existing prioritization tech-
niques. We also demonstrate empirically the need for writ-
ing dierent delaying explorers for scalable systematic test-
ing and hence, present a exible delaying explorer interface.
We introduce two new techniques to improve the scalability
of search based on delaying explorers. First, we present an
algorithm for stratied exhaustive search and use ecient
state caching to avoid redundant exploration of schedules.
We provide soundness and termination guarantees for our
algorithm. Second, for the cases where the state of the sys-
tem cannot be captured or there are resource constraints,
we present an algorithm to randomly sample any execution
from the stratied search space. This algorithm guarantees
that any such execution that requires ddelay operations is
sampled with probability at least 1 =Ld, whereLis the max-
imum number of program steps. We have implemented our
algorithms and evaluated them on a collection of real-world
fault-tolerant distributed protocols.
Categories and Subject Descriptors
D.2.4 [ Software Engineering ]: Software/Program Veri-
cation; D.2.5 [ Software Engineering ]: Testing and De-
bugging
General Terms
Algorithms, Reliability, Verication
Keywords
Systematic testing, model checking, asynchronous programs,
distributed systems, random sampling1. INTRODUCTION
Asynchronous reactive systems are ubiquitous across do-
mains like distributed systems, device drivers, web appli-
cations, and operating systems. Processes in these systems
communicate by exchanging messages asynchronously and
react to environment input continuously. The system relia-
bility depends critically on correct handling of asynchrony
and reactivity using stateful protocols. Testing and debug-
ging of these systems is notoriously dicult due to the non-
deterministic nature of their computation; an error could re-
sult from a combination of some choice of inputs and some
interleaving of event handlers. This paper is concerned with
the problem of systematic testing of such such systems by
automatically enumerating all sources of nondeterminism,
both from environment input and from scheduling of con-
current processes.
The main challenge in scaling systematic testing to real-
world programs is the large number of behaviors that ex-
plode exponentially with the number of steps in the pro-
gram. Techniques such as state caching [11] and partial-
order reduction [9] have been developed to combat this ex-
plosion, yet their worst-case complexity remains exponen-
tial. In practice, the search often takes too long and has to be
terminated because of a time bound, thereby giving no infor-
mation to the programmer. Therefore, researchers have been
motivated to investigate prioritized search techniques, both
deterministic [12, 8] and randomized [3], to provide partial
coverage information. However, all of these techniques have
been developed for shared-memory multithreaded programs.
In asynchronous reactive programs, the primary mechanism
for communication among concurrent processes is message-
passing rather than shared-memory. We have discovered em-
pirically (Section 6) that prioritization techniques developed
for multithreaded programs are not eective when applied
to message-passing programs.
In this paper, we introduce a new technique for systematic
testing of asynchronous reactive programs. Our technique is
inspired by the notion of a delaying scheduler [8] for multi-
threaded programs. A delaying scheduler is a deterministic
thread scheduler equipped with a delay operation whose in-
vocation changes the default scheduling strategy. For asyn-
chronous reactive programs, we generalize this notion to a
delaying explorer of allnondeterministic choices (Section 2),
both from input and from the interleaving of event handlers.
The key observation that makes a delaying explorer suit-
able for systematic testing is that every execution can be
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ESEC/FSE‚Äô15 , August 30 ‚Äì September 4, 2015, Bergamo, Italy
c2015 ACM. 978-1-4503-3675-8/15/08...$15.00
http://dx.doi.org/10.1145/2786805.2786861
73
db = 1db = 2db = 3 db = 4db = 5
db = 1
db = 2
D1
 D2Figure 1: Stratication using delaying explorers
produced by introducing a nite number of delays in the
default deterministic execution prescribed by the explorer.
We show that appropriately designed delaying explorers are
signicantly better than existing prioritization techniques in
searching for errors in executions of asynchronous message-
passing systems.
A delaying explorer induces stratication in the search space
of all executions. A stratum is the set of executions that re-
quire the same number of delays. Figure 1 represents the
stratication pictorially; db= 1 is the set of executions
with one delay, db= 2 is the set of executions with two
delays, and so on. A delaying explorer species a prioritized
search that explores these strata in order. Since the num-
ber of possible executions increases exponentially with the
delay budget, exploration for high budget values becomes
prohibitively expensive. Therefore, a delaying explorer is ef-
fective only if bugs are uncovered at low values of the delay
budget. Figure 1 shows the stratication induced by two
dierent delaying explorers. The explorer D2is more eec-
tive thanD1at discovering a particular bug if that bug lies
in a lower stratum for D2than forD1.
The dierence in stratication induced by dierent delaying
explorers has practical consequences. We have observed em-
pirically that there is considerable variance in the speed of
detecting errors across dierent delaying explorers for dier-
ent test problems1. Motivated by this observation, we have
designed a general delaying explorer interface that helps pro-
grammers quickly write custom search strategies in a small
amount of code, typically less than 50 LOC. Delaying explor-
ers also provides developers and testers with a simple and
elegant mechanism to express domain-specic knowledge re-
garding parts of the search space to prioritize. We have writ-
ten several delaying explorers using our framework and used
them to nd bugs in implementations of distributed proto-
cols that could not be discovered using any other method.
We describe a particular case study in Section 6.
Given a delaying explorer, we need techniques for eectively
exploring the strata induced by the explorer. In this pa-
per, we also present two algorithms |Stratied Exhaustive
Search ( SES) and Stratied Sampling ( SS)| for solving this
problem. SES performs stratied search by iteratively in-
crementing the delay budget and exhaustively enumerating
all schedules that can be explored with a given delay budget.
1A test problem is the combination of a program and a spec-
ication.Inspired by model checking techniques, we incorporate state
caching to avoid redundant exploration of schedules. By
caching the states visited along an execution, we can prune
the search if an execution generated subsequently leads to a
state in the cache. Incorporating state caching in delaying
exploration is nontrivial because search is performed over
executions of the composition of the program and the delay-
ing explorer, both reading and updating their private state
in each step of the execution. The naive strategy of caching
the product of the program and the explorer state does not
work because the delaying explorer can be an arbitrary pro-
gram with a huge state space of its own. Instead, our al-
gorithm caches only the program state yet guarantees that
in the limit of increasing delay budgets, all executions of
the program are covered. Our evaluation shows that SES
nds bugs orders of magnitude faster than prior prioritiza-
tion techniques on our benchmarks (Section 6).
Even though state caching is an important optimization,
it is not a panacea to the explosion inherent in system-
atic testing. The complexity of the algorithm mentioned
in the previous paragraph still grows exponentially with the
number of allowed delays. Consequently, if a delaying ex-
plorer is unable to nd a bug quickly within a few delays,
the search must be stopped because of the external time
bound. To further scale search to large delay budgets, we
present the SSalgorithm which performs stratied sampling
of the search space with probabilistic guarantees. Our algo-
rithm guarantees that any execution that is visited with db
delays is sampled with probability at least 1 =Ldb, whereL
is the maximum number of program steps. SSis useful be-
cause it allows even distribution of the limited time resource
over the entire search space. Furthermore, since each sample
is generated independently of every other sample, random
exploration can be easily and eciently parallelized or dis-
tributed. Finally, for some systems state caching may not
be possible because of the diculty of taking a snapshot of
the entire system state. In this situation, search based on
random sampling could be very useful. We empirically show
(Section 6) that on our benchmarks, SScan nd bugs faster,
often by an order-of-magnitude, compared to the prior best
technique [3] for random sampling of executions of multi-
threaded programs.
We have implemented our framework and algorithms for sys-
tematic testing of applications written in P[6], a domain-
specic language for asynchronous event-driven program-
ming currently used for developing device drivers and dis-
tributed services. The Pcompiler generates both executable
Ccode and Zing code which can be explored with the Zing
model checker. The generated Ccode is used for executing
the application either locally on a single computing node
or distributed across a collection of nodes. We have imple-
mented in Pa fault-tolerant transaction management sys-
tem (TMS) that internally comprises many protocols such
as two-phase commit [2], multi-paxos [4], and chain replica-
tion [21]. Using our test framework, we found many bugs,
caused by protocol-level race conditions, in our implementa-
tion of TMS. The suite of protocols in TMS form the bench-
mark set for evaluating our algorithms.
We note that our techniques are not limited to the Plan-
guage. They generalize to any programming system with
74two properties: (1) ability to create executable models of
the execution environment of a program, and (2) control
over all sources of nondeterminism in program semantics.
We conclude this section by summarizing our contributions:
1. We introduce delaying explorers as a foundation for sys-
tematic testing of asynchronous reactive programs. We
empirically demonstrate that for the domain of message-
passing programs, delaying explorers are better, often by
an order-of-magnitude, than existing prioritization tech-
niques.
2. We observe that the ecacy of a delaying explorer de-
pends on the test problem. To enable programmers to
easily write custom explorers, we have created a exible
interface for specifying explorers. We have written four
delaying explorers, each in less than 50 LOC, using our
interface.
3. We present the SES algorithm that uses state-caching
for eciency while prioritizing search using a delaying ex-
plorer. The algorithm guarantees soundness even without
caching the state of the delaying explorer.
4. We present the SSalgorithm to eciently sample exe-
cutions with a xed number of delays. Our algorithm
guarantees that if a buggy execution exists with dbdelays
for a given delaying explorer, then each sample triggers
the bug with probability at least 1 =LdbwhereLis the
maximum number of steps in the program.
2. DELAYING EXPLORERS
In this section, we provide intuition for delaying explorers
and their use in systematic testing of asynchronous reactive
systems. We begin by formally stating our model of pro-
grams and explorers.
A programPis a tuple (S;Cid;T;s 0):
1.Sis the set of states of P.
2.Cidis a nite set of nondeterministic choices that Pcan
make during execution. This set includes both choices
due to scheduling of concurrent processes in Pand choices
due to nondeterministic input received by each process.
3.T2CidS * S is the transition function of P. If
s0=T(c;s), we say that ( s;s0) is a transition of P. We
dene Choices (s) =fcj9s0: T(c;s) =s0g.
4.s0is the initial state of P.
A sequence of states s0;s1;s2;:::;s nis an execution ofPif
(si;si+1) is a transition of Pfor alli2[0;n). A states2Sis
reachable if it is the nal state of some execution. An innite
sequence of states s0;s1;s2;:::is an innite execution ofPif
(si;si+1) is a transition of Pfor alli0. We assume that P
isterminating , i.e., it does not have any innite executions.
The formalization of the nondeterministic transition graph
of an asynchronous reactive program is standard in the lit-
erature; it is depicted pictorially in Figure 2. The explo-
ration algorithms popularized by model checking tools, e.g.
SPIN [11], view the transitions coming out of a state as un-
ordered; the order in which those transitions are explored
is considered an implementation-level detail. A delaying
ùëÜ0
ùëÜ2 ùëÜ1 ùëÜ3
ùëÜ5 ùëÜ4 ùëÜ6ùëÜ7 ùëÜ8 ùëÜ9 ùëÜ10ùëÜ11 ùëÜ12Figure 2: A concurent program
Next Delay
Next Delay Next DelayùëÜ1,ùê∑1,0ùëÜ0,ùê∑0,0
ùëÜ0,ùê∑2,1
ùëÜ4,ùê∑3,0 ùëÜ1,ùê∑4,1 ùëÜ2,ùê∑5,0ùëÜ0,ùê∑6,2
Next
ùëÜ3,ùê∑11,0Next Delay
ùëÜ5,ùê∑7,0 ùëÜ1,ùê∑8,2Next Delay
ùëÜ7,ùê∑9,0 ùëÜ2,ùê∑10,1
Figure 3: A concurrent program composed with a
delaying explorer
explorer, formalized below, instead considers the order of
transitions an important concern for ecient exploration. It
provides a general interface for specifying this order based
on the entire history of the program execution.
A delaying explorer Dis a tuple ( D;Next;Step;Delay;d0):
1.Dis the set of states of D. The state of the explorer
typically includes a data structure, e.g. stack or queue,
to maintain an ordering among the choices available to
the program.
2.Next2D!Cidis a total function. Given a explorer
stated, the choice Next (d) is prescribed by the explorer
to be taken next.
3.Step2SD!Dis a total function. Suppose we
have a program state sand a explorer state d, and we
execute the choice Next (d) ats. Then Step(s;d) yields
the explorer state corresponding to the program state
T(Next (d);s). The Step function enables building ex-
plorers which change their state in response to specic
events that occur during execution of the program, such
as sending or receiving of messages, creation of new pro-
cesses, etc.
4.Delay2D!Dis a total function. Given a explorer
stated, the application Delay (d) yields a new explorer
state. The Delay function provides a mechanism to change
the next choice to be explored.
5.d0is the initial state of D.
Consider a delaying explorer that attemts to order the out-
going transitions of each state left to right for the program
in Figure 2. The unfolding of the nondeterminism in this
program as controlled by such a delaying explorer is shown
in Figure 3. We formalize and explain the intuition behind
this gure below.
Let (P;D) denote the composition of a program Pand a de-
laying explorerD. A state of (P;D) is a triple ( s;d;n ), where
75sis the state ofP,dis the state ofD, andnis the number
of consecutive delay operations applied in state s. A nite
sequence (s0;d0;n0)x0 !(s1;d1;n1)x1 !(s2;d2;n2)x2 !
is an execution of ( P;D) if for alli0, either (1) xi=Next ,
ni+1= 0,T(Next (di);si) =si+1, and Step(si;di) =di+1,
or (2)xi=Delay ,ni+1=ni+ 1,ni+1<jChoices (s)j,
si=si+1, and Delay (di) =di+1. In this execution, a transi-
tionNext !is aNext -transition andDelay !is aDelay -transition.
In Figure 3, each state has exactly these two outgoing tran-
sitions. A triple ( s;d;n ) is a reachable state of ( P;D) if it
occurs on an execution. A db-delay execution of (P;D) is
one in which the number of Delay -transitions is db. Thus, a
delaying explorer Dinduces a stratication of the executions
of a programPsuch that the i-th stratum contains exactly
the set ofi-delay executions.
In order to ensure that all behaviors are covered, the delay-
ing explorer must ensure that all nondeterministic choices
from a state are generated by successive applications of Delay .
To formalize this requirement, we dene Delayk(fork0)
inductively as
Delay0(d) =d
Delayk+1(d) = Delay (Delayk(d))
and Nextk(fork0) inductively as
Next0(d) =fg
Nextk+1(d) = Nextk(d)[fNext (Delayk(d))g:
A delaying explorer Dissound with respect to a program P
ifChoices (s) = NextjChoices (s)j(d) for every reachable state
(s;d) of (P;D). This property states that all nondetermin-
istic choices in a state are covered through iterative appli-
cation of the Delay operation composed with Next . In Fig-
ure 3, all successors, S1throughS3, of stateS0are reachable
via at most two invocations of Delay . This property guar-
antees (Theorem 1) that reachability analysis on ( P;D) is
equivalent to reachability analysis on P.
Theorem 1.Consider a program Pand a delaying ex-
plorerDthat is sound with respect to P. A statesis reach-
able inPi(s;d)is reachable in (P;D)for somed.
Example: Let us consider a simple program in which the
only source of nondeterminism is the scheduling of concur-
rent processes. An example of a delaying explorer for this
program is a round-robin process scheduler. The state Dof
this scheduler is a queue of process ids initialized to contain
the id of the initial process. Next returns the process id
at the head of the queue. Step instruments the program's
execution so that the id of a new process is added to the
tail, the id of a terminated process is removed, and the id
of a blocked process is moved to the tail. Delay moves the
process id at the head to the tail. This explorer maintains
the invariant that the ids of all enabled processes are present
in the queue. By applying the Delay operation at most n
times, where nis the size of the queue, any enabled process
can be moved to the head and be returned by a subsequent
call to Next . Therefore, this explorer is sound with respect
to the program.
0 L-1
0 0 L-1 L-10
dbExecution with 0 delay
Executions with 1 delay
Executions with 2 delaysFigure 4: Stratied exhaustive search
3. STRATIFIED EXHAUSTIVE SEARCH
Figure 4 shows a pictorial representation of stratied exhaus-
tive search of a program with respect to a delaying explorer.
In this picture, Lis the maximum number of steps in the
program. In contrast to the graphs in Figures 2 and 3 where
a node represents the program state, each node in Figure 4
is a complete execution of the program. The root node is
the execution with no delays. This execution presents at
mostLpositions to insert a delay operation, each yielding
another complete execution with a single delay operation.
These executions are indicated by the nodes at the end of
the edges coming out of the root node. This process can
be continued until all executions have been generated. It is
clear that there can be at most Ldbexecutions with no more
than dbdelays. Thus, for small values of db, it is feasible to
enumerate all executions even for large values of L. This ob-
servation suggests our stratied exhaustive search algorithm
(SES) which generates executions level by level, exploring
all executions in a level before moving to the next level. A
delaying explorer induces a stratication of the executions
of a program; in general, dierent delaying explorers induce
dierent stratication for the same program. Thus, a delay-
ing explorer is a mechanism to bias the search performed by
ourSES algorithm to dierent parts of the execution space.
The algorithm in Figure 5 takes as input a program P, a
delaying explorer D, and a parameter  >0. It uses three
global variables. The integer db, initialized to 0 and itera-
tively incremented by , contains the current delay bound.
During the search, a frontier of pending executions, that go
beyond the current delay bound, is maintained in the dic-
tionary Frontier . For each state sin the frontier, Frontier
contains a pair ( d;i), wheredis the explorer state just prior
to the the execution of i-th transition from state s. The
mapping from sto (d;i) is put into the frontier because exe-
cution of the i-th transition would require more delays than
the current bound. Finally, we optimize the search by using
a cache of (hashes of) visited states maintained in the set
Cache .
The workhorse of our algorithm is DelayBoundedDFS , a
procedure with four parameters|program state s, explorer
stated, transition count i, and delay count n. The goal of
DelayBoundedDFS is to continue exploration from state s.
The transition count iis the number of transitions already
explored from s. The delay count nis the number of delays
required, starting from the initial state, to execute the next
transition out of s.DelayBoundedDFS iterates through the
transitions from sby repeatedly invoking the Next operation
of the delaying explorer to nd out which transition to exe-
cute and incrementing ito indicate the execution of another
transition. For each discovered state s0, ifs0is not present in
76vardb:N;
varFrontier :DictionaryhS;(DN)i;
varCache :SethSi;
DelayBoundedDFS (s:S;d:D;i:N;n:N)f
vars0:S;
while (i<jChoices (s)j)f
s0;i:=T(Next(d);s);i+ 1;
if(s062Cache )f
Cache:Add(s0);
DelayBoundedDFS (s0;Step(s;d);0;n);
g
if(n=db^i<jChoices (s)j)f
Frontier (s) := (d;i);
break ;
g
d;n:=Delay (s;d);n+ 1;
g
g
SES()f
vardb0:N;
varFrontier0:DictionaryhS;(DN)i;
db;Frontier;Cache := 0;;;;;
Cache:Add(s0);
DelayBoundedDFS (s0;d0;0;0);
while (Frontier6=;)f
Frontier0;Frontier :=Frontier;;;
db0;db:=db;db+;
foreach ((s;d;i )2Frontier0)
DelayBoundedDFS (s;Delay (s;d);i;db0+ 1);
g
g
Figure 5: SES algorithm: Stratied exhaustive
search
Cache then it is added to Cache and DelayBoundedDFS is
called recursively on s0. To move to the next transition, the
Delay operation of the delaying explorer needs to invoked.
If the current delay count nhas already reached the current
delay bound dband there is at least one more transition to
be executed, then exploration cannot continue from sand
work for the remainder of exploration from sis added to the
frontier. Otherwise, the Delay operation is used to update
dand the delay count nis incremented.
The top-level procedure of our algorithm is SES. This pro-
cedure initializes dbto 0 and Frontier and Cache to;. It
then executes two nested loops. The outer loop iterates over
the value of dbincrementing it by each time around. The
goal of each iteration of this loop is to restart each pend-
ing exploration in the current frontier. To do this task, a
copy of Frontier is made in Frontier0and Frontier is reset
to;. The inner loop then picks each work item in Frontier0
and invokes DelayBoundedDFS with it. The execution of
the inner loop rells Frontier which is again emptied in the
next iteration of the outer loop. Theorem 2 formalizes the
correctness of the SES algorithm.
Theorem 2.Consider a program Pand a delaying ex-
plorerDthat is sound with respect to P. The SES algorithm
(Figure 5) terminates and visits a state s0is0is reachable
froms0.
Neither the termination nor the safety argument for our al-
gorithm depends on Cache . The only role of Cache is to op-
timize the search by avoiding redundant executions. There-fore, there is considerable exibility in how much memory is
devoted to the storage for Cache . The two extreme cases are
when Cache is not used at all and when all visited states are
put into Cache . But, it is possible and our implementation
supports imposing a bound on the memory consumption for
Cache beyond which states are either not added to Cache
or added with replacement.
An important consideration in our use of Cache is that we
store only the program state in it and avoid storing the ex-
plorer state. This design has the advantage that we get the
maximum pruning out of the use of state caching. If a state
sis rst visited with explorer state dand later with explorer
stated0, the second visit is ignored even if it happened with
fewer delays compared to the rst visit. As a result, we can
avoid re-exploration for the second visit. However, it may
be possible that a state is discovered with a higher delay
than the minimum delay required to visit it. We believe
that this trade-o is good because the primary goal of a
delaying explorer is to bias the search rather than enforce
strict priority.
Finally, we note that it is enough to store only a hash of a
state in Cache . But it is important to store the full state
both when it is passed as a parameter to DelayBoundedDFS
or when it is stored in Frontier since the program needs to
be executed from it. For the latter uses, a state could either
be cloned or reconstructed by re-executing the program from
the beginning.
4. STRATIFIED SAMPLING
In the previous section, we described the SES algorithm
to perform stratied exhaustive search over the executions
of an asynchronous reactive program. In this section, we
describe a complementary algorithm that enables stratied
exploration via near-uniform random sampling of executions
from the strata induced by a delaying explorer; we call this
algorithm the stratied sampling algorithm ( SS).
To motivate why random sampling is benecial, we note
that the complexity of the SES algorithm grows exponen-
tially with the upper bound on the number of allowed de-
lays. Consequently, if a delaying explorer is unable to nd
a bug quickly within a few delays, the search often takes
more time than the programmer is willing to wait for. To
deal with this common problem, a time bound is usually
supplied in addition to the number of delays. When an ex-
ternal time bound could stop the search before the delay
limit has been reached, random sampling has certain ad-
vantages over exhaustive deterministic exploration. First,
unlike deterministic exploration, random sampling can sam-
pleevery execution with a non-zero probability, making it
possible to distribute the limited time resource over the en-
tire search space. Second, since each sample is generated
independently of every other sample, random exploration
can be easily and eciently parallelized, an important ad-
vantage in an era where parallelism is abundantly available
via multicore and cloud computing.
Figure 6 shows how our algorithm samples an execution
with two delay operations. First, the ExecutePath func-
tion (dened later in Figure 7) executes the program using
a custom strategy dened by the delaying scheduler with-
77ùëÜ0,0
ùëÜùëõ0,0
ùëÜùëõ0+1,0
ùëÜùêø0,0ùëÜùëõ1,1ùëÜ0,1
ùëÜùëõ1+1,1
ùëÜùêø1,1ùëÜùêø2,2ùëÜ0,2
ùëÜ1,2Next
NextNextNext
NextNextDelay
NextDelayFigure 6: A run of SSalgorithm
out introducing any delays. The ExecutePath function re-
turns the length of the execution L0from the start state to
the terminal state. Using choose (L0) we uniformly pick a
valuen0in the range [0 ;L0) to insert the rst delay. When
ExecutePath is invoked again, it introduces a delay at n0, de-
terministically executes the program upto termination, and
returnsL1, the length of the path since the last delay. Us-
ingchoose (L1) we uniformly pick a value n1in the range
[0;L1) to insert the second delay. Finally, the execution
S0;0!Sn0;0!S0;1!Sn1;1!S0;2!SL2;2represents
a random execution with two delays.
Given a program P, a delaying explorer D, and a delay
bound db, an invocation of DelayBoundedSample (Figure 7)
produces a terminating execution of Pwith no more than
dbdelays. The random exploration performed by our algo-
rithm is very dierent in spirit from the classical random
walk algorithm on a state-transition graph (Figure 2) which
starts from the initial state and executes the program by ran-
domly selecting a transition out of the current state. This
naive random walk, although it guarantees a non-zero prob-
ability for sampling any execution, suers from the problem
that the probability of sampling long executions decreases
exponentially with the execution length. Instead, our algo-
rithm performs a random walk, not on the state-transition
graph, but on a dierent graph (Figure 4) induced by the
delaying explorer D. In this graph, each node is a complete
terminating execution (as opposed to a state) and an edge
is a position in the execution for inserting a delay (as op-
posed to transition). We show later that the probability of
sampling any execution requiring dbdelays is at least1
Ldb.
Unlike the naive random walk, the probability of sampling
an execution is exponential in the number of required delays
rather than the number of steps. A long execution has just
as much chance to be produced as a short execution with the
same number of delays, thereby eliminating the bias towards
short executions.
The algorithm in Figure 7 uses a single global variable path,
a sequence of natural numbers. This sequence represents
a path as follows. For each istarting from 0 and up to
path:Length 1, executePforpath[i] steps followed by a
delay. Finally, execute Puntil it terminates. The procedure
ExecutePath performs the execution encoded by path and
returns the number of steps performed after the last delay.
The procedure DelayBoundedSample invokes the procedure
ExecutePath repeatedly to randomly sample an executionvarpath :SequencehNi
ExecutePath () :Nf
vari;j:N;
vars:S;
vard:D;
s;d;i :=s0;d0;0;
while (i<path:Length )f
j:= 0;
while (j <path[i])f
s;d;j :=T(Next(d);s);Step(s;d);j+ 1;
g
d;i:=Delay (s;d);i+ 1;
g
j:= 0;
while (0<jChoices (s)j)f
s;d;j :=T(Next(d);s);Step(s;d);j+ 1;
g
returnj;
g
DelayBoundedSample ()f
vari;l:N;
if(jChoices (s0)j= 0) return ;
path :=;;
l:=ExecutePath ();
i:= 0;
while (i<db)
invariant 0<l
f
path:Append (choose (l));
l:=ExecutePath ();
i:=i+ 1;
g
g
SS()f
vari:N;
db:= 1;
while (true)f
i:= 0;
while (i<NumSamples (db))f
DelayBoundedSample ();
i:=i+ 1;
g
db:=db+ 1;
g
g
Figure 7: SSalgorithm: Near-uniform random sam-
pling
with dbdelays. If the initial state s0does not have any
transitions, there is nothing to do. Otherwise, it sets path
to the empty sequence and calls ExecutePath which executes
Pwithout any delays. The algorithm chooses a step at ran-
dom from the number of steps returned by ExecutePath as
the position to execute a delay operation. It extends path
with it and invokes ExecutePath again to create a new ex-
ecution. It continues to do so iteratively until the number
of delays in the execution has reached db. A single invo-
cation of DelayBoundedSample samples a single execution
with dbdelays. To calculate this sample, it must re-execute
the program dbtimes and perform dbrandom choices.
Theorem 3.Consider a program Pand a delaying ex-
plorerDthat is sound with respect to P. LetLbe the max-
imum number of steps along any execution of P. For any
integer db0and any execution of(P;D)with db delays,
theSSalgorithm (Figure 7) generates with probability at
least1
Ldb.
78Figure 7 also shows a procedure SSthat repeatedly invokes
DelayBoundedSample to implement a stratied sampling al-
gorithm. This procedure has an (timeout-terminated and in-
nite) outer loop that repeatedly increases the delay bound
db. The inner loop samples NumSamples (db) executions
from the set of executions with exactly dbdelays by invok-
ingDelayBoundedSample repeatedly. Our algorithm is pa-
rameterized by a function NumSamples that species the
number of executions to be sampled for each delay bound.
As we have explained before, the number of executions in-
creases exponentially with the number of available delays.
Therefore, we believe that a practical NumSamples function
should also have an exponential dependency on the delay
bound. For our evaluation (Section 6), we chose c1+cdb
2
to be the shape for NumSamples (db); through trial and er-
ror, we found that c1= 100 and c2= 3 work well for the
benchmarks we studied in this paper.
5. IMPLEMENTATION
In this section, we provide an overview of our framework for
the evaluation of delaying explorers in systematic testing of
reactive asynchronous programs.
Pprograms : We wrote our programs in the Pprogramming
language [6], a domain-specic language for implementing
asynchronous event-driven systems. A Pprogram is a col-
lection of state machines, each with an input message queue,
communicating with each other by sending and receiving
messages. The Pcompiler generates from the input pro-
gram both Ccode and Zing code. The generated Ccode is
used for executing the application either locally on a single
computing node or distributed across a collection of nodes;
thePruntime supports both local and distributed execu-
tion. The generated Zing code is provided as input to the
Zing explorer [1] for systematic testing.
The contribution of this paper |exploiting delaying explor-
ers to search executions of asynchronous programs| de-
pends on two properties of the Pprogramming and testing
framework. First, Pallows the programmer to write con-
currency unit tests [14] by composing a program with an
executable model of its execution environment also written
inP. Environment models are erased during compilation to
Ccode and replaced with hand-written Ccode. Second, P
provides control over all sources of nondeterminism in the
program execution to enable systematic exploration of these
nondeterministic choices. The techniques described in this
paper are applicable to any programming system with these
two properties.
There are two sources of nondeterminism in the semantics of
Pprograms. First, Phas interleaving nondeterminism be-
cause the language provides a primitive for dynamic machine
creation. As a result, multiple machines can be executing
concurrently. In each step, one machine can be chosen non-
deterministically to execute and it can either compute on
local state or dequeue a message or send a message to an-
other machine. This nondeterminism implicitly creates non-
determinism in the order in which messages are delivered to
a machine. The code of a machine has to be programmed
robustly and tested so that it continues to perform safely
regardless of the reordering. Second, a Pprogram may also
make an explicit nondeterministic choice by using the spe-interface IZingDelayingScheduler
f
// Next is called to get the next process to be executed
intNext ();
// Delay is called to cycle through scheduling choices
void Delay ();
// Start is called when a new process is created
void Start ( intprocessId);
// Finish is called when a process is terminated
void Finish ( intprocessId);
// Step is called to communicate information about execution,
// e.g. change priority, blocked process, etc.
void Step (params object [] P);
g
Figure 8: Delaying explorer interface
cial expression $whose evaluation results in a nondetermin-
istic Boolean choice. This feature is extremely useful for
modeling the environment of reactive systems; like nonde-
terministic component failure or message loss. To nd bugs
quickly and debug them, it is essential to control both these
sources of non-determinism.
Implementing a delaying explorer : We have implemented
the algorithms in Sections 3 and 4 using the infrastructure
in the Zing model checker. The component of Zing most
pertinent to our implementation is state caching and the ex-
plorer that orchestrates the depth-rst search of the state-
transition graph of the input Zing program. We modied
the explorer to query an external object implementing the
IZingDelayingScheduler interface. The explorer invokes
the method Next to determine the process whose transition
it should explore and the method Delay to inform the sched-
uler of its decision to delay the next process.
The methods Start ,Finish , and Step together implement
the capability formalized by the Step function described
in Section 2; these methods inform the delaying scheduler
of important events occurring during the execution. The
method Start is invoked whenever a new process is cre-
ated and the method Finish whenever a process terminates.
The method Step is used to implement a general mechanism
for instrumenting the program's execution for updating the
scheduler state.
Controlling non-determinism : The general approach of
controlling schedules in systematic testing frameworks [3,
12, 10] is to instrument the program at every synchroniza-
tion points. In the context of asynchronous message passing
programs like P, the only synchronization points are at en-
queue of a message, blocking at dequeue and creation of
a new machine (more details in [6]). The Pcompiler au-
tomatically instruments the program at these three points
and passes the information to the delaying explorer using
theStep function. In addition to prioritizing interleaving
nondeterminism, a delaying explorer must also prioritize ex-
plicit nondeterministic choice. We simply adopt the con-
vention that false is ordered before true. For a language
that provides nondeterministic choice over types other than
Boolean , the choices may be controlled by expanding the
IZingDelayingScheduler interface.
796. EVALUATION
Our evaluation was directed towards the following goals:
XEvaluate the performance of SES andSSin compari-
son with the best known approaches, preemption bound-
ing [12] and probabilistic concurrency testing [3], respec-
tively (Section 6.1).
XEvaluate the performance of dierent delaying explorers
in nding bugs, and demonstrate the need for exible
delaying explorer interface (Section 6.2).
XDemonstrate the benet of writing custom explorer with
a case study of chain replication protocol(Section 6.3).
Experimental setup: All the experiments are performed
on Intel Xeon E5-2440, 2.40GHz, 12 cores (24 threads),
160GB machine running 64 bit Windows Server OS. The
Zing model checker can exploit multiple cores during explo-
ration as its iterative depth-rst search algorithm is paral-
lel [20]. We do not report the time taken to nd bugs as
it is dependent on the degree of parallelism and the parallel
explorer implementation, but instead we report the number
of distinct states explored (in the case of SES) and number
of schedules explored (in the case of SS) before nding the
bug. Time taken to nd the bug is directly proportional to
these parameters. The numbers reported for the evaluation
of stratied sampling algorithm in Table 1 are a median over
5 runs of the experiment.
Benchmarks: We have used P[6] to implement a fault tol-
erant Transaction Management System (TMS) and a Win-
dows driver communicating with an OSR device. We used P
because its compiler provides a translation both to Ccode
for execution on the Microsoft Azure cluster and to Zing
code for systematic testing. Our implementations are not
abstract models; they are detailed enough to be deployed as
a distributed service. TMS uses various protocols like Two-
Phase Commit protocol [2] for atomicity of transactions,
Chain Replication protocol [21] for fault-tolerant replication
of state machines, Multi-Paxos protocol [4] for consistent
log replication and consensus. The buggy programs used for
evaluation in this paper were collected during the develop-
ment of this protocol suite. Each row in Table 1 represents
a dierent bug. We only consider hard-to-nd bugs that led
to unhandled-event exceptions (system crash) and violation
of global safety specications (written as monitors).
6.1 Evaluation of SESand SS
Evaluating SES: We applied the iterative SES algorithm
with dierent delaying explorers to the set of buggy pro-
grams (incrementing the value of dbby 1 after each iter-
ation). For evaluating the performance of SES, we imple-
mented iterative preemption bounding [12] ( PB) with state-
caching in Zing . Table 1 shows the number of distinct states
explored before nding the bug by both the approaches. It
can be seen that PBfails to nd the bug in most of the
cases, and in cases where PBsucceeds, SES with some de-
laying explorer is able to nd the bug orders of magnitude
faster (except for TMS 1 and ChainRep 8). Also, there is
a lot of variance in the performance of SES when combined
with dierent delaying explorers, which motivates the need
for a exible interface to write custom delaying explorers.Evaluating SS: We implemented random scheduler (RS) [19]
as the baseline for comparison. Random scheduler fails to
nd most of the bugs, as the probability of nding a bug
decreases exponentially with length of buggy execution. We
found that iterative random scheduler (IRS) that combines
random scheduling with iterative depth bounding performs
better than simple random scheduling. Stratication in IRS
is obtained by iteratively incrementing the maximum depth
bound. We incremented the depth bound by 100 after each
iteration and sampled 100+3iexecutions from each stratum
(whereiis the iteration number).
We compared the iterative SSalgorithm described in Sec-
tion 4 with the PCT [3] algorithm, which is considered as
state of the art in probabilistic concurrency testing. PCT
provides probabilistic guarantees of nding a bug with bug-
depthd, by randomly inserting dpriority inversions. Most of
the concurrency bugs using PCT were found with bug depth
of less than 3 in [3, 15]. The PCT algorithm makes an as-
sumption about the maximum length of program execution
(k), which is hard to compute statically in the case of asyn-
chronous reactive programs. We use k= 5000 and d= 5
for our experiments. Table 1 shows that PCT fails to nd
most of the bugs, conrming that the bugs in asynchronous
programs generally have a larger bug-depth . In the cases
where PCT succeeds in nding the bug, SSwith some de-
laying explorer is orders of magnitude faster. Similar to the
behavior of SES, forSSalso we see variance in performance
of dierent delaying explorers across dierent problems.
Comparison between SESandSS: We have extensively
used both SES andSSfor nding bugs in our implementa-
tions. In our experience, the SES algorithm is able to nd
bugs faster than SSin most of the cases as it uses state-
caching to prune redundant explorations. Furthermore, SES
can nd low-probability bugs that occur at smaller values of
delay budget faster than SS. In the case of ChainRep 6 and
Paxos 3 there was a low probability bug at small delay bud-
get;SSfails to nd it whereas SES nds it.
As the delay bound increases, search space explodes expo-
nentially. If there is a bug that requires large delay budget
for a given stratication strategy, then SES may fail to nd
it due to running out of memory. We came across scenar-
ios (TMS 3 and TMS 4 in Table 1) where SES ran out of
memory but after running SSfor a long time we uncovered
a bug. SScan be kept running for a long time without any
memory constraints. Since it performs sampling with prob-
abilistic guarantees, it may nd a bug at larger delay budget
where SES fails.
We can fruitfully combine both approaches as follows. Per-
formSES rst to nd all shallow (few delays) bugs quickly
and get strong coverage guarantees. Once SES has uncov-
ered all shallow bugs and has almost consumed the memory
budget, perform SSfrom the frontier states and get proba-
bilistic guarantees. We leave the evaluation of this combi-
nation for future work.
6.2 Experience with Delaying Explorers
We have implemented three dierent delaying explorers. In
this section, we explain the construction of each explorer
and the reasons for the variance in their performance. The
80Table 1: Evaluation Results for SSandSESusing various delaying explorers
Stratied Sampling Stratied Exhaustive Search
ProgramsNo. of schedules explored before nding bug No. of states explored before nding bug
RS IRS PCTSS+ Delaying ExplorerPBSES+ Delaying Explorer
RR RTC PRR RR RTC PRR
2pc1 9842 1891 1983 781 331 816 793221 8851 6571 6512
2pc2 * * * 10943 6378 6300 * * 17690 9090
2pc3 * 2966 9835 1823 1018 4109 48321 1898 1123 2189
2pc4 * 7629 * * 3321 * * * 5101 77212
ChainRep 1 9655 652 9832 5607 9999 1985 * 92178 9913 936
ChainRep 2 * * * 34034 7829 28221 74231 32166 8821 88732
ChainRep 3 * * 13283 2032 1711 6093 * 19731 3452 8981
ChainRep 4 4213 313 4439 3452 4249 1238 59234 672 5441 11742
ChainRep 5 196 77 55 53 110 101 * 3973 521 6652
ChainRep 6 * * * * * * * 78443 44331 54981
ChainRep 7 * * * * * * * * 3538 *
ChainRep 8 * 4561 * 5513 * 2201 8342 9791 * 8218
ChainRep 9 * * * 66381 9425 16559 * 37222 7812 37213
ChainRep 10 782 159 74 129 331 888 4561 5431 1944 1781
MultiPaxos 1 * 5211 9934 7821 765 5819 * 82114 89341 88129
MultiPaxos 2 * * * 9872 8873 11239 * 15563 9983 1934
Multipaxos 3 * * * * 15023 9589 * 18831 8923 1198
Paxos 1 229 86 592 122 53 233 3320 2233 1098 4312
Paxos 2 * 2211 * 9563 831 1874 77834 4912 833 8831
Paxos 3 * * * * * * * * 14832 *
TMS 1 224 64 227 12 305 34 553 2220 660 8965
TMS 2 * * * * * * * * 44832 *
TMS 3 # # # # # 3009214 # # # #
TMS 4 # # # # 5530042 # # # # #
OSR 1 435 122 332 75 122 1009 5532 4421 683 55392
OSR 2 756 78 131 115 66 224 12864 12931 1634 3212
*!the search ran out of memory budget of 60GBor exceeded the time budget of 2 hours.
#!the search exceeded the time budget of 5 hours (running for longer duration).
source code for these explorers is available at the following
website: [ https://github.com/ZingModelChecker/Zing ].
Run-to-completion explorer ( RTC): The run to com-
pletion explorer was introduced in prior work [6] for testing
device drivers written in P. The default strategy in RTC
is to follow the causal sequence of events, giving priority to
the receiver of the most recently sent event. When a delay is
applied, the highest priority process is moved to the lowest
priority position. Even for small values of delay bound, this
explorer is able to explore long paths in the program since it
follows the chain of generated events. In our experience, this
explorer is able to nd bugs that are at large depth better
than any other explorer. For example, bugs in ChainRep 7
and TMS 2 were found were found by RTC at depth greater
than 1500 and delay budget less than 4 while other explorers
could not nd these bugs.
Round-robin explorer ( RR): The round-robin delaying
explorer, explained earlier in Section 2, cycles through the
processes in process creation order. It moves to the next
task in the list only on a delay or when the current task is
completed. Round-robin explorer has been used in the past
([8, 19]) to test multithreaded programs. In our experience,
in most of the cases (Table 1) other delaying explorers per-
form better than RR.RRcan be used for nding bugs that
manifest through a small number of preemptions or inter-
leaving between processes. Our evaluation shows that most
bugs in asynchronous programs do not fall in that category.
Probabilistic round-robin explorer ( PRR): A proba-
bilistic delaying explorer is one in which the Step operation
is allowed to make random choices. While a determinis-tic delaying explorer induces a xed stratication over the
executions of a program, a probabilistic delaying explorer
induces a probability space over stratication. We have ex-
perimented with a cannibalistic version of the round-robin
explorer ( PRR ). We believe that the culprit behind the poor
performance of the round-robin explorer is its default pro-
cess scheduling order which is based on the order of process
creation. The simplest way to change this default order is to
randomize it. Instead of inserting a freshly-created process
at the tail of the queue, insert it at a random position in the
queue; everything else carries over from the round-robin ex-
plorer. The probabilistic round-robin explorer is still sound
since the denitions of Next and Delay do not change. Ta-
ble 1 indicates that PRR typically performs better than RR.
6.3 Writing a Custom Delaying Explorer
After testing the chain replication protocol using the three
delaying explorers explained earlier, we tested it for more
specic scenarios. One such scenario is testing the system
against random node failures. We provide a brief description
of the chain replication protocol. Next, we show how we
wrote a custom explorer to test for the node failure scenario
and found a previously unknown bug in our implementation.
The chain replication protocol [21] is a distributed fault-
tolerant protocol for replicating state machines. Consider
an instance of a chain replication system with 6 machines|
4 instances of Server machine (S1;:::;S 4) connected in a
chain, 1 instance of Master machine (M), and 1 instance
ofFault machine (F).S1;:::;S 4communicate with each
other to implement replication. Mperiodically monitors
the health of S1;:::;S 4to detect if any of them has failed.
If it detects a fault in Si, it tells the neighbors of Sito re-
81congure. Fis a machine that models fault injection. It
maintains a set of numbers initialized to f1;:::; 4g.Fre-
peatedly and nondeterministically removes a number ifrom
this set and sends a failure message to Siuntil the size of the
set becomes 1. The chain replication protocol is expected to
behave correctly for Nservers as long as at most N 1 fail.
When a distributed system starts up, there is an initializa-
tion phase involving exchange of messages between nodes
for setting up the network topology and other system con-
guration. Bugs during the initialization phase are straight
forward, infrequent, and get discovered quickly. Subtle bugs
are generally encountered after the system is initialized and
has reached an interesting global state. Since we want to test
our system against a specic scenario of failure occurring
after the system has stabilized, the new delaying explorer
should not spend a lot of time injecting failures or moni-
toring the system during the initialization phase. We need
stratication that gives less priority to certain interleaving
in the the initial phase.
To capture this intuition with a delaying explorer, we wrote
a customized delaying explorer ( CustExplorer ). The ex-
plorer maintains an ordering of all dynamically-created ma-
chine and cycles through them based on the ordering. The
program can change the ordering by invoking ChangeOrder
callbacks (implemented using Step). Using ChangeOrder
callback in the initialization phase, the machines S1;:::;S 4
are ordered before machines MandF. After the initializa-
tion phase, the machines MandFare moved ahead in the
ordering as compared to machines S1;:::;S 4. Thus, Cust-
Explorer helps in stratifying the search by giving less priority
to interleaving the failure and monitor machines, until the
system has stabilized.
Using CustExplorer we were able to nd a previously un-
known bug in chain replication, which occurred when the
failure was injected simultaneously at two neighboring nodes
after the initialization phase. CustExplorer was able to nd
the bug with SES by exploring 220103 states and with SSby
exploring 193442 schedules. We applied the same strategy
to ChainRep 6 as it had similar bug related to node failure
and we were able to nd the bug in 10445 states which is
nearly 4 times faster than the next best.
7. RELATED WORK
Model checking [11, 22] is a classic technique applied to
prove temporal properties on programs whose semantics is
an arbitrary state-transition graph. Our use of state caching
to prune search is inspired by model checking. Partial-order
reduction [9] is another technique to prune search. Combin-
ing partial-order reduction with schedule prioritization tech-
niques is known to be a challenging problem [13]. Coons et
al. [5] have proposed a technique to combine preemption-
bounding with partial-order reduction. In future work, we
would like to investigate the feasibility of combining delayed
exploration with partial-order reduction.
There is prior work on random sampling of concurrent execu-
tions. Sen [16] provides an algorithm for sampling partially-
ordered multithreded executions. Similar to our work, the
PCT algorithm [3] also exploits prioritization techniques to
eectively sample multithreaded executions. The PCT al-gorithm characterizes a concurrency bug according to its
depth and guarantees that the probability of nding a bug
with depth din a program with Lsteps andnthreads is
at least 1=nLd 1. The mathematical techniques underlying
PCT and our sampling algorithm are dierent. PCT pro-
vides a custom algorithm for a particular notion of bug depth
whose denition has a deep connection with the proof for the
probability bound. On the other hand, our algorithm does
not depend on a characterization of bugs. Rather, it is pa-
rameterized by a delaying explorer, a mechanism used by the
programmer to stratify the search space. Consequently, the
proof for our probability bound is a straightforward combi-
natorial argument on a bounded tree in terms of its branch-
ing factor and depth.
Predictive testing [17, 18, 23, 24] follows the basic recipe
of executing the program, collecting information from the
execution, constructing a model of the program from the
collected information, and then re-executing the program
based on new predicted interleavings likely to reveal errors.
The various techniques dier in the information collected
and the targeted class of errors. The search performed by
predictive techniques is goal-driven but typically does not
provide coverage guarantees. On the other hand, our search
technique is not goal-driven but provides coverage guaran-
tees.
Concurrit [7] proposes a domain specic language for writ-
ing debugging scripts that help the tester specify thread
schedules for reproducing concurrency bugs. The search
is guided by the script without any prioritization. In con-
trast, our work is focused on nding rather than reproduc-
ing bugs. Instead of a debugging script, a tester writes a
domain-specic scheduler with appropriate uses of sealing;
iterative deepening with delays automatically prioritizes the
search with respect to the given scheduler.
8. CONCLUSION
We have demonstrated how delaying explorers help in sys-
tematic testing of asynchronous reactive programs. We also
showed that using delay bounding [8] with a single default
scheduler is not scalable for nding bugs. Dierent delaying
explorers induce dierent stratication, and hence, writing
custom delaying explorers as unit test strategies can make
testing complex asynchronous protocols scalable. We also
presented and evaluated two algorithms, (1) SES for exhaus-
tive search with strong coverage guarantees and showed how
state-caching can be used eciently for pruning, (2) SSfor
sampling executions with probabilistic guarantees. We eval-
uated both these algorithms on real implementation of dis-
tributed protocols and showed that our techniques perform
orders of magnitude better than state-of-art search prioriti-
zation techniques like preemption bounding and PCT.
9. ACKNOWLEDGMENTS
The rst and third authors were supported in part by the
TerraSwarm Research Center, one of six centers supported
by the STARnet phase of the Focus Center Research Pro-
gram (FCRP) a Semiconductor Research Corporation pro-
gram sponsored by MARCO and DARPA.
8210. REFERENCES
[1] T. Andrews, S. Qadeer, S. K. Rajamani, J. Rehof, and
Y. Xie. Zing: A model checker for concurrent software.
InProceedings of CAV . 2004.
[2] P. A. Bernstein, V. Hadzilacos, and N. Goodman.
Concurrency Control and Recovery in Database
Systems . Addison Wesley Publishing Company, 1987.
[3] S. Burckhardt, P. Kothari, M. Musuvathi, and
S. Nagarakatte. A randomized scheduler with
probabilistic guarantees of nding bugs. In Proceedings
of ASPLOS , 2010.
[4] T. D. Chandra, R. Griesemer, and J. Redstone. Paxos
made live: An engineering perspective. In Proceedings
of PODC 2007 .
[5] K. E. Coons, M. Musuvathi, and K. S. McKinley.
Bounded partial-order reduction. In Proceedings of
OOPSLA 2013 .
[6] A. Desai, V. Gupta, E. Jackson, S. Qadeer,
S. Rajamani, and D. Zuerey. P: Safe asynchronous
event-driven programming. In Proceedings of PLDI ,
2013.
[7] T. Elmas, J. Burnim, G. Necula, and K. Sen.
CONCURRIT: A domain specic language for
reproducing concurrency bugs. In Proceedings of
PLDI , 2013.
[8] M. Emmi, S. Qadeer, and Z. Rakamari c.
Delay-bounded scheduling. In Proceedings of POPL ,
2011.
[9] P. Godefroid. Partial-Order Methods for the
Verication of Concurrent Systems: An Approach to
the State-Explosion Problem . Springer-Verlag, 1996.
[10] P. Godefroid. Model checking for programming
languages using Verisoft. In Proceedings of POPL ,
pages 174{186, 1997.
[11] G. Holzmann. The model checker SPIN. IEEE
Transactions on Software Engineering , 1997.
[12] M. Musuvathi and S. Qadeer. Iterative contextbounding for systematic testing of multithreaded
programs. In Proceedings of PLDI , 2007.
[13] M. Musuvathi and S. Qadeer. Partial-order reduction
for context-bounded state exploration. Technical
Report MSR-TR-2007-12, Microsoft Research, 2012.
[14] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A.
Nainar, and I. Neamtiu. Finding and reproducing
heisenbugs in concurrent programs. In Proceedings of
OSDI , 2008.
[15] S. Nagarakatte, S. Burckhardt, M. M. Martin, and
M. Musuvathi. Multicore acceleration of priority-based
schedulers for concurrency bug detection. In
Proceedings of PLDI 2012 .
[16] K. Sen. Eective random testing of concurrent
programs. In Proceedings of ASE, 2007 .
[17] K. Sen. Race directed random testing of concurrent
programs. In Proceedings of PLDI , pages 11{21, 2008.
[18] F. Sorrentino, A. Farzan, and P. Madhusudan.
Penelope: Weaving threads to expose atomicity
violations. In Proceedings of FSE , 2010.
[19] P. Thomson, A. F. Donaldson, and A. Betts.
Concurrency testing using schedule bounding: An
empirical study.
[20] A. Udupa, A. Desai, and S. Rajamani. Depth bounded
explicit-state model checking. In Proceedings of SPIN ,
2011.
[21] R. van Renesse and F. B. Schneider. Chain replication
for supporting high throughput and availability. In
OSDI 2004 .
[22] W. Visser and P. C. Mehlitz. Model checking programs
with Java Pathnder. In Proceedings of SPIN , 2005.
[23] C. Wang, S. Kundu, M. Ganai, and A. Gupta.
Symbolic predictive analysis for concurrent programs.
InProceedings of FM 2009 .
[24] C. Wang, M. Said, and A. Gupta. Coverage guided
systematic concurrency testing. In Proceedings of
ICSE 2011 .
83