Cross-Supervised Synthesis of Web-Crawlers
Adi Omari
Technion
omari@cs.technion.ac.ilSharon Shoham
Academic College of Tel Aviv
Ya f fo
sharon.shoham@gmail.comEran Y ahav
Technion
yahave@cs.technion.ac.il
ABSTRACT
A web-crawler is a program that automatically and systematically
tracks the links of a website and extracts information from its pages.
Due to the different formats of websites, the crawling scheme fordifferent sites can differ dramatically. Manually customizing a crawler
for each speciï¬c site is time consuming and error-prone. Further-
more, because sites periodically change their format and presenta-tion, crawling schemes have to be manually updated and adjusted.
In this paper, we present a technique for automatic synthesis of
web-crawlers from examples. The main idea is to use hand-crafted(possibly partial) crawlers for some websites as the basis for crawl-ing other sites that contain the same kind of information. Techni-
cally, we use the data on one site to identify data on another site.
We then use the identiï¬ed data to learn the website structure andsynthesize an appropriate extraction scheme. We iterate this pro-
cess, as synthesized extraction schemes result in additional data to
be used for re-learning the website structure. We implemented ourapproach and automatically synthesized 30crawlers for websites
from nine different categories: books, TVs, conferences, universi-ties, cameras, phones, movies, songs, and hotels.
Categories and Subject Descriptors D.1.2 [Programming Tech-
niques]: Automatic Programming; I.2.2 [Artiï¬cial Intelligence]:
Program Synthesis
1 Introduction
A web-crawler is a program that automatically and systematically
tracks the links of a website and extracts information from its pages.
One of the challenges of modern crawlers is to extract complex
structured information from different websites, where the informa-
tion on each site may be represented and rendered in a differentmanner and where each data item may have multiple attributes.
For example, price comparison sites use custom crawlers for
gathering information about products and their prices across theweb. These crawlers have to extract the structured information de-scribing products and their prices from sites with different formats
and representations. The differences between sites often force a
programmer to create a customized crawler for each site, a taskthat is time consuming and error-prone. Furthermore, websites
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full cita-
tion on the ï¬rst page. Copyrights for components of this work owned by others thanACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciï¬c permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™16, May 14-22, 2016, Austin, TX, USA
câƒ2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2884781.2884842may eventually change their format and presentation, therefore the
crawling schemes have to be manually maintained and adjusted.
Goal The goal of this work is to automatically synthesize web-
crawlers for a family of websites that contain the same kind of
information but may signiï¬cantly differ on layout and formatting.
We assume that the programmer provides one or more hand-craftedweb-crawlers for some of the sites in the family, and would like to
automatically generate crawlers for other sites in the family. For ex-
ample, given a family of four websites of online book stores (eachcontaining tens of thousands of books), and a hand-crafted crawlerfor one of them, we automatically generate crawlers for the other
three. Note that our goal is not only to extract data from web-sites,
but to synthesize the programs that extract the data.
Existing Techniques Our work is related to wrapper induction [24].
The goal of wrapper induction is to automatically generate extrac-
tion rules for a website based on the regularity of pages inside
the site. Our main idea is to try and leverage a similar regularity
across multiple sites. However, because different sites may signiï¬-cantly differ on their layout, we have to capture this regularity at a
more abstract level. Towards that end, we deï¬ne an abstract logical
representation of a website that allows us to identify commonality
even in the face of different formatting details.
In contrast to supervised techniques [24, 25, 29, 5], which re-
quire labeled examples, and unsupervised techniques [2, 3, 8, 32,
31, 34, 40] that frequently require manual annotation of the ex-
tracted data, our approach uses cross supervision, where the learned
extraction rules of one site are used to produce labeled examples for
learning the extraction rules in another site.
Our technique uses XPath [7], a widely used web documents
query language along with regular expressions. This makes ourresulting extraction schemes human readable and easy to modify
when needed. There has been some work on the problem of XPath
robustness to site changes [9, 26, 28], trying to pick the most ro-
bust XPath query for extracting a particular piece of information.
While robustness is a desirable property, our ability to efï¬ciently
synthesize a crawler circumvents this challenge as a crawler can beregenerated whenever a site changes.
Our Approach: Cross-Supervised Learning of Crawling Schemes
We present a technique for automatically synthesizing data-extractingcrawlers. Our technique is based on two observations: (i) sites with
similar content have data overlaps, and (ii) in a given site, infor-
mation with similar semantics is usually located in nodes with asimilar location in the document tree.
Using these observations, we synthesize data-extracting crawlers
for a group of sites sharing the same type of information. Starting
from one or more hand-crafted crawlers which provide a relatively
small initial set of crawled data, we use an iterative approach todiscover data instances in new websites and extrapolate data ex-
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   368
traction schemes which are in turn used to extract new data. We
refer to this process as cross-supervised learning , as data from one
web-site is repeatedly used to guide synthesis in other sites.
Our crawlers extract data describing different attributes of items.
We introduce the notion of a container to maintain relationships
between different attributes that refer to the same item. We use con-
tainers, which are automatically selected without any prior knowl-
edge of the structure of the website, to handle pages with multipleitems, and to ï¬lter out irrelevant data. This allows us to synthesizeextraction schemes from positive examples only.
Our approach is scalable and practical: we used cross-supervision
to synthesize crawlers for several product review websites, e.g.,tvexp.com, weppir.com, camexp.com and phonesum.com.
Main Contributions The contributions of this paper are:
âˆ™A framework for automatic synthesis of web-crawlers. The
main idea is to use hand-crafted crawlers for a number of
websites as the basis for crawling other sites that contain thesame kind of information.
âˆ™A new cross-supervised crawler synthesis algorithm that ex-trapolates crawling schemes from one web-site to another.The algorithm handles pages with multiple items and syn-
thesizes crawlers using only positive examples.
âˆ™An implementation and evaluation of our approach, automat-
ically synthesizing 30crawlers for websites from nine differ-
ent categories: books, TVs, conferences, universities, cam-eras, phones, movies, songs and hotels. The crawlers thatwe synthesize are real crawlers that were used to crawl morethan 12,000webpages over all categories.
2O v e r v i e w
2.1 Motivating Example
Consider a price comparison service for books, which crawls book
seller websites and provides a list of sellers and correspondingprices for each book. Examples of such book seller sites in-
clude
barnesandnoble.com (B&N ),blackwell.co.uk (BLACKWELL )a n d
abebooks.com (ABE). Each of these sites lists a wide collection of
books, typically presented in template generated webpages feeding
from a database. Since these pages are template generated, they
present structured information for each book in a format that is re-
peated across books. By recognizing this repetitive structure for a
given site, one can synthesize a data extraction query and use it to
automatically extract the entire book collection.
While the format within a single site is typically stable, the for-
mats between sites differ considerably. Fig. 1 shows a small and
simpliï¬ed fragment of the page structure on B&N and BLACKWELL
in HTML. Fig. 3 shows part of the tree representation of the cor-
responding sites (as well as of ABE), whereğ·1,...,ğ· 4denote
different pages. Due to the differences in structure, the data ex-traction query can differ dramatically. For example, in
BLACKWELL
and ABE, each of the pages (ğ· 2,ğ·3,ğ·4) presents a single book,
whereas in B&N the pageğ·1shows a list of several books.
The goal of this work is to automatically synthesize crawlers for
new sites based on some existing hand-crafted crawler(s). For ex-
ample, given a crawler for the BLACKWELL site, our technique syn-
thesizes a crawler for B&N website. The synthesized crawler is
depicted in Fig. 2. We show that this can be done despite the signif-icant differences between the sites
BLACKWELL ,a n d B&N ,i nt e r m s
of HTML structure. We note that the examples that we present in
this section are abbreviated and simpliï¬ed. For example, the real
DOM tree for the B&N page we show here contains around 1,000
nodes. The structure of the full trees, and the XPaths required for
processing them are more involved than what is shown here.barnesandnoble.com
<ol class ="result-set box">
<li class ="result box">..
<div class ="details below-axis" >
<a href ="..." data-bntrack="Title_9781628718980"
class ="title" >
THROUGH THE LOOKING GLASS</ a>
<a href =".."
data-bntrack="Contributor_9781628718980"
class ="contributor" >
David Winston Busch</ a>
...
<div class="price-format">
<a href="..." data -bntrack="Paperback_Format">
<span class ="format">Paperback</span >
<span class ="price">$9.91</ span>
</a>
</div></div>
...</li >
<li class ="result box">..
<div class ="details below-axis" >
<a href ="..." data-bntrack="Title_9780071633604"
class ="title">
Aliceâ€™s Adventures in Wonderland</ a>
<a href =".." data-bntrack="Contributor_9780071633604"
class ="contributor" >Lewis Carroll</ a>
...
<div class="price-format">
<a href="..." data -bntrack="Paperback_Format">
<span class ="format">Paperback</span>
<span class ="price">$6.49</ span>
</a>
</div>
</div>
...</li >
</ol>
blackwell.com
<div id="product-biblio">
<h1>Through the looking glass</ h1>
<a class ="link_type1" href="/jsp/a/Lewis_Carroll">
David Winston Busch
</a>
<div class ="price-info" align="center">
<span class="price">
Â£8.99</ span>
</div>
</div>
Figure 1: Fragments of webpages with the similar attribute val-
ues for a book on two different book shopping sites.
2.2 Cross-Supervised Learning of Crawling Schemes
Our main observation is that despite the signiï¬cant differences in
the concrete layout of websites, the pages of websites that exhibitthe same product category often share the same logical structure ;
they present similar attributes for each product. For example, eachof the pages of Fig. 1 presents the same important attributes aboutthe book, including its title, author name andprice . Moreover,
there is a large number of shared products between these websites.
The book â€œThrough the looking glassâ€ is one such example for
B&N and BLACKWELL .
Our technique exploits data overlaps across sites in order to learn
the concrete structure of a new website ğ‘ based on other websites.
Speciï¬cally, we identify in ğ‘ concrete data extracted from other
sites and as such learn the structure in which this data is representedinğ‘ . We then use multiple examples of the structure in which the
data appears in ğ‘ in order to generalize and get an extraction query
forğ‘ . This enables our algorithm to extrapolate a crawler for ğ‘ .
We do not require a precise match of data across sites, as our
technique also handles noisy data. (For example, prices do not have
3691class MySpider(CrawlSpider):
2 name = "barnesandnoble"
3 allowed_domains = ["www.barnesandnoble.com"]
4 start_urls = [
5 ("http://www.barnesandnoble.com/s/java-programming
?store=allproducts&keyword=java+programming" )
6 ]
7
8 rules = (
9 Rule(LinkExtractor(
10 allow=("/s/. *"),callback="parse_item", follow=True
11 ),
12 )
13
14 def parse_item(self, response):
15 sel = Selector(response)
16 rows = sel.XPath( â€™//body/div/div/section/div/ol["result-set
box"]/li[@class="result box"]/div/div[@class="detailsbelow-axis"]â€™ )
17 for r in rows:
18 item = BooksItem()
19 item[â€™titleâ€™] = r.XPath(
20 â€™//a[@class="title"]â€™
21 ).extract()
22 item[â€™authorâ€™] = r.XPath(
23 â€™//a[@class="contributor"]â€™
24 ).extract()
25 item[â€™priceâ€™] = r.XPath(
26 â€™//div[@class="price-format"]/a/span[@class="price"]â€™
27 ).extract()
28 yield item
Figure 2: Crawler for java books from Barnes&Noble.
to be identical; any number can be a match.)
Crawling schemes A crawler, such as the one of Fig. 2, contains
some boilerplate code deï¬ning the crawler class and its operations.
However, the essence of the crawler is its crawling scheme.F o r
example, in Fig. 2 the crawling scheme is highlighted in boldface.
A crawling scheme is deï¬ned with respect to a set of semantic
groups, called attributes , which deï¬ne the types of data to be ex-
tracted. In the books example, the attributes are: book title ,author
andprice.
Given a set of attributes, a crawling scheme consists of the fol-
lowing two components: (i) A data extraction query that deï¬neshow to obtain values of the attributes for each item listed on thesite. (ii) A starting point URL and a URL ï¬ltering pattern which let
the crawler locate â€œrelevantâ€ pages and ï¬lter out irrelevant pages
without downloading and processing them.
Our crawlers use XPath as a query language for data extrac-
tion. XPath is a query language for selecting nodes from an XMLdocument which is based on the tree representation of the XMLdocument, and provides the ability to navigate around the tree, se-lecting nodes by describing their path from the document tree root
node. For example, Fig. 4 and Fig. 5 show the crawling schemes
for crawling books from
BLACKWELL and B&N respectively, where
the data extraction query is expressed using XPaths.
Two-level data extraction schemes We assume that the data ex-
traction query has two levels: The ï¬rst level query is an XPath
describing an item container. Intuitively, a container is a sub-tree
that contains all the attribute values we would like to extract (de-ï¬ned more formally in Sec. 5.) For example, in Fig. 4, the XPath
//body/div[@class="content__maincore-shop"]... de-
scribes a container of book attributes on BLACKWELL pages.
The second level queries contain an extraction XPath for values
of each individual attribute. These XPaths are relative to the rootof the container. For example,
//div/h1/ i nF i g .4i su s e dt op i c k
the node that has type h1(heading 1), containing the book title.Iterative synthesis of crawling schemes Our approach considers a
set of websites, and a set of attributes to extract. To bootstrap thesynthesis process, the user is required to provide the set of websitesfor which crawler synthesis is desired, as well as a crawling scheme
for at least one of these sites. Alternatively, the user can provide
multiple partial crawling schemes for different sites, that togethercover all the different item attributes.
The synthesis process starts by invoking the provided extraction
scheme(s) on the corresponding sites to obtain an initial set of val-ues for each one of the attributes. These values are then used to
locate nodes that contain attribute values in the document trees of
webpages of new sites. The nodes that contain attribute values re-veal the structure of pages of the corresponding websites. In par-ticular, smallest subtrees that exhibit all the attributes amount to
containers. This allows for synthesis of data extraction schemes
for new websites. The newly learned extraction schemes are usedto extract more values and add them to the set of values of each
attribute, possibly allowing for additional websites to be handled.
This process is repeated until complete extraction schemes are ob-tained for all websites, or until no additional values are extracted.
In our example, the algorithm starts with the data extraction scheme
for
BLACKWELL (see Fig. 4), provided by a user. It extracts from
ğ·2author-x ,title-x ,a n d price as values of the book title,
author, and price attributes, respectively (see Fig. 3). These values
are identiï¬ed in ğ·1(B&N ) within the subtree of the left most node
represented by
//body/.../ol["result-set box"]
/li[@class="result box"]/...
/div[@class="details below-axis"] ,
which then points to the latter node as a possible container. Addi-
tional values taken from ğ·3and other pages in BLACKWELL identify
additional nodes in the B&N tree as attribute and container nodes.
Note that author-x is also found in another subtree in ğ·1.H o w -
ever, there are no instances of the remaining attributes in that sub-
tree; Therefore, the subtree is not considered a container and thecorresponding node is treated as noise.
By identifying the commonality between the identiï¬ed contain-
ers and between nodes of the same attribute, a data extraction schemefor
B&N is synthesized (see below). In the next iteration, the new
data scheme is used to extract from B&N the values author-z ,
title-z andprice as additional values for book title, author, and
price respectively (that did not exist in BLACKWELL ). The new val-
ues are located in ABE(seeğ·4in Fig. 3), allowing to learn an
extraction scheme for ABEas well.
XPath synthesis for two-level queries Our approach synthesizes a
two level extraction scheme for each website from a set of attributenodes and candidate containers identiï¬ed in its webpages. The two-
level query structure is reï¬‚ected also in the synthesis process of the
extraction scheme. Technically, we use a two-phase approach tosynthesize the extraction scheme. In each site, we ï¬rst generate an
XPath query for the containers. We then ï¬lter the attribute nodes
keeping only those reachable from containers that agree with thecontainer XPath, and generate XPaths for their extraction relatively
to the container nodes.
To generate an XPath query for a set of nodes (e.g., for the set of
containers), we consider the concrete XPath of each nodeâ€”this is
an XPath that extracts exactly this node. We unify these concrete
XPaths by a greedy algorithm that aims to ï¬nd the most concrete
(most strict) XPath query that agrees with a majority of the concreteXPaths. Keeping the uniï¬ed XPath as concrete as possible prevents
the addition of noise to the extraction scheme.
The generated XPaths for
B&N are depicted in Fig. 5. In this
example, uniï¬cation is trivial since the XPaths are identical. How-
370Figure 3: Example DOM trees
Container: //body/div[@class="content__maincore-shop"]
/table[@class="main-page"]/tr/
td[@class="two-col-right"]/table/tr/td
Title: //div/h1/
Author: //div/a[@class="link_type1"]
Price: //div[@id="buy-options"]/div/span
URL Pattern: .*jsp/id/. *
Figure 4: Crawling scheme for BLACKWELL .
Container: //body/div/div/section/div/ol["result-set box"]
/li[@class="result box"]/div/div[@class="details below-axis"]
Title: //a[@class="title"]
Author: //a[@class="contributor"]
Price: //div[@class="price-format"]/a/span[@class="price"]
URL Pattern: /s/. *
Figure 5: Crawling scheme for B&N .
ever, if for example each of the container nodes labeled div inğ·1
had different idâ€™s, the idfeature would have been removed during
uniï¬cation. Note that even if the subtree that contains the noisy
instance of author-x inğ·1had been identiï¬ed as a candidate
container (e.g., if it had contained values of the other attributes), itwould have been discarded during the uniï¬cation.
URL pattern synthesis In order to synthesize a URL pattern for the
crawling scheme of a new site, we extend the iterative technique
used for synthesis of data extraction schemes; In each iteration of
the algorithm, for each website we identify a set of pages of interest
as pages that contain attribute data. We ï¬lter these pages in accor-dance with the ï¬ltering of container and attribute nodes. We thenunify the URLs of remaining pages similarly to XPath uniï¬cation.
Fig. 5 depicts the URL pattern generated by our approach for
B&N . This pattern identiï¬es webpages in B&N that present a list of
booksâ€”these are the pages whose structure conforms with the syn-thesized extraction scheme. Note that
B&N also presents the same
books in a separate page each, but such pages require a different
crawling scheme.
3 Preliminaries
In this section we deï¬ne some terms that will later be used to de-
scribe our approach.3.1 Logical Structure of Webpages
Each webpage implements some logical structure . Following [19],
we use relations as a logical description of data which is indepen-
dent of its concrete representation. A relational speciï¬cation is a
set of relations, where each relation is deï¬ned by a set of column
names and a set of values for the columns. A tupleğ‘¡=âŸ¨ğ‘1:
ğ‘‘1,ğ‘2:ğ‘‘2,...âŸ©maps a set of columns {ğ‘1,ğ‘2,...}to values. A
relationğ‘Ÿis a set of tuples {ğ‘¡1,ğ‘¡2,...}such that the columns of
everyğ‘¡,ğ‘¡â€²âˆˆğ‘Ÿare the same.
For example, B&N ,BLACKWELL and ABEdescribed in Section 2
implement a relational description of a list of books, where each
book has a title, an author and a price. Then â€œbook titleâ€, â€œauthorâ€
and â€œpriceâ€ are columns, and the set of books is modeled as a rela-
tion with these columns, where each tuple is a book item.
Data items, attributes and instances We refer to each tuple of a
relationğ‘Ÿas adata item. The columns of a relation ğ‘Ÿare called at-
tributes , denoted Att. Each attribute deï¬nes a class of data sharing
semantic similarities, such as meaning and/or extraction scheme.
The value of attribute ğ‘âˆˆAttin some tuple of ğ‘Ÿis also called
aninstance ofğ‘. The set of all values of all attributes is denoted
ğ‘‰. Each attribute ğ‘is associated with an equivalence relation â‰¡ğ‘
that determines if two values are equivalent or not as instances of
ğ‘. (The notion of â€œequivalenceâ€ may differ between different at-tributes.) By default (if not speciï¬ed by the user) we use the bag
of words representation of each value ğ‘‘, denotedğ‘Š(ğ‘‘), and use
Jaccard similarity function [21], ğ½(ğ‘‘
1,ğ‘‘2), with a threshold of 0.5
as an equivalence indicator between values ğ‘‘1andğ‘‘2:
ğ‘‘1â‰¡ğ‘ğ‘‘2iffğ½(ğ‘‘1,ğ‘‘2)>0.5whereğ½(ğ‘‘1,ğ‘‘2)=âˆ£ğ‘Š(ğ‘‘1)âˆ©ğ‘Š(ğ‘‘2)âˆ£
âˆ£ğ‘Š(ğ‘‘1)âˆªğ‘Š(ğ‘‘2)âˆ£.
3.2 Concrete Layout of Webpages
Technically, webpages are documents with structured data, such as
XML or HTML documents. The concrete layout of the webpageimplements its logical structure, where attribute instances are pre-
sented as nodes in the DOM tree.
XML documents as DOM trees A well formed XML document,
describing a webpage of some website, can be represented by a
DOM tree. A DOM tree is a labeled ordered tree with a set ofnodesğ‘and a labeling function that labels each node with a set
of node features (not to be confused with data attributes), wheresome of the features might be unspeciï¬ed. Common node featuresinclude
tag,class andid.
371For example, Fig. 3 depicts part of the tree representation of
pages of B&N ,BLACKWELL and ABE. A node labeled by a,
class=title is a node whose tag isa,class istitle ,a n d id
is unspeciï¬ed.
Node descriptors Anode descriptor is an expression ğ‘¥in some
language deï¬ning a set of nodes in the DOM tree. We use Expr to
denote the set of node descriptors. For a node descriptor ğ‘¥âˆˆExpr
and a webpage ğ‘,w ed e ï¬ n e /llbracketğ‘¥/rrbracketğ‘to be the set of nodes described
byğ‘¥fromğ‘.W h e nğ‘is clear from the context, we omit it from the
notation. A node descriptor is concrete if it represents exactly one
node. We sometimes also refer to node descriptors as extraction
schemes. In this work, we use XPath as a speciï¬cation language
for node descriptors.
3.3 XPath as a Data Extraction Language
XPath [7] is a query language for traversing XML documents. XPath
expressions (XPaths in short) are used to select nodes from the
DOM tree representation of an XML document. An XPath expres-
sion is a sequence of instructions, ğ‘¥=ğ‘¥1...ğ‘¥ ğ‘˜. Each instruc-
tionğ‘¥ğ‘–deï¬nes how to obtain the next set of nodes given the set
of nodes selected by the preï¬x ğ‘¥1...ğ‘¥ ğ‘–âˆ’1, where the empty se-
quence selects the root node only. Roughly speaking, each instruc-tionğ‘¥
ğ‘–consists of (i) axis deï¬ning where to look relatively to the
current nodes: at children (â€œ /â€), descendants (â€œ //â€), parent, sib-
lings, (ii) node ï¬lters describing which tag to look for (these can
be â€œallâ€, â€œtextâ€, â€œcommentâ€, etc.), and (iii) predicates that can re-
strict the selected nodes further, for example by referring to values
of additional node features (e.g. class ) that should be matched.
For example, the XPath //div/ */a[@class="link_type1"]
selects all nodes that follow a sequence of nodes that can start any-
where in the DOM tree, and has to consist of a node with tag=div
followed by some node whose features are unspeciï¬ed and is fol-lowed by a node with
tag=a andclass=link_type1 .
4 The Crawler Synthesis Problem
In this section we formulate the crawler synthesis problem. A
crawler for a website can be divided into two parts: a page crawler ,
and a data extractor . The page crawler is responsible for grabbing
the pages of the site that contain relevant information. The data ex-
tractor is responsible for extracting data of interest from each page.
Logical structure of interest Our work considers websites whose
data-containing webpages share the following logical structure: each
webpage describes one main relation, denoted data. As such, data
items are tuples of the data relation. Further, the set Attof attributes
consists of the columns of the data relation.
Note that different concrete layouts can implement this simple
logical structure. For example, if we consider a webpage that ex-hibits a list of books, then the concrete layout can ï¬rst group books
by author, and for each author list the books, or it can avoid the par-
tition based on authors. Further, some websites will present eachbook in a separate webpage, whereas others will list several books
in the same page. Even for websites that are structured similarly by
the former parameters, the mapping of attribute instances to nodesin the DOM tree can vary signiï¬cantly.
Page crawlers A page crawler for a website ğ‘ is given by a URL
pattern, denoted ğ‘ˆ(ğ‘ ), which identiï¬es the set of webpages of in-
terest. These are the webpages of the website that contain data of
the relevant kind. We denote by ğ‘ƒ(ğ‘ )the set of webpages whose
URL matchesğ‘ˆ(ğ‘ ).
Data Extractors Recall that we consider webpages where instances
of different attributes are grouped into tuples of some relation, de-noted data. We are guided by the observation that data in suchwebpages is typically stored in subtrees, where each subtree con-tains instances of all attributes for some data item (i.e., tuple of thedata relation). We refer to the roots of such subtrees as containers:
Containers: A node in the DOM tree whose subtree contains all the
entries of a single data item (i.e., a single tuple of data) is called a
container. Note that any ancestor of a container is also a container.
We therefore also deï¬ne the notion of a best container to be a con-
tainer such that none of its predecessors is a container. Depending
on the concrete layout of the webpage, a best container might cor-
respond to an element in a list or in another data structure. It might
also be the root of a webpage, if each webpage presents only onedata item.
For example, in the tree ğ·
1depicted in Fig. 3, both of the nodes
selected by //body/.../div[@class="details below-axis"]
are containers, and as such so are their ancestors, including the root.However, the latter are not best containers since they include strictsubtrees that are also containers.
Attribute nodes: A node in the DOM tree that holds an instance
of an attribute ğ‘âˆˆAttis called anğ‘-attribute node, or simply an
attribute node whenğ‘is clear from the context or does not matter.
Data extractors: A data extractor for the relation data over columns
Attin some website ğ‘ can be described by a pair (ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿ,ğ‘“ )
whereğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿ âˆˆExpr is a node descriptor representing con-
tainers, andğ‘“:Att/arrowhookleftâ†’Expr is a possibly partial function that maps
each attribute name to a node descriptor, with the meaning that this
descriptor represents the attribute nodes relatively to the container
node, i.e., the attribute descriptor considers the container node asthe root. The data extractor is partial ifğ‘“is partial. Ifğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿ
is empty, it is interpreted as a node descriptor that extracts the rootof the page. If ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿ is empty andğ‘“is undeï¬ned for every
attribute, we say that the data extractor is empty .
Examples of data extractors appear in Fig. 5 and Fig. 4.
Crawler synthesis The crawler synthesis problem w.r.t. a set Att
of attributes is deï¬ned as follows. Its input is a set ğ‘†of websites,
where each website ğ‘ âˆˆğ‘†is associated with a data extractor, de-
notedğ¸(ğ‘ ), over Att.ğ¸(ğ‘ )might be partial or even empty. The
desired output is a page crawler, along with a complete data extrac-tor for everyğ‘ âˆˆğ‘†.
5 Data Extractor Synthesis
In this section we focus on synthesizing data extractors, as a ï¬rst
step towards synthesizing crawlers. We temporarily assume that
the page crawler is given, i.e., for each website we have the setof webpages of interest, and present our approach for synthesizingdata extractors. We will remove this assumption later, and also
address synthesis of the page crawler, using similar techniques.
The input to the data extractor synthesis is therefore a set ğ‘†of
websites, where each website ğ‘ âˆˆğ‘†is associated with a set of
webpages, denoted ğ‘ƒ(ğ‘ ), and with a data extractor, denoted ğ¸(ğ‘ ),
which might be partial or even empty. The goal is to synthesize a
complete data extractor for every ğ‘ âˆˆğ‘†. The main challenge in
synthesizing a data extractor is identifying the mapping between
the logical structure of a webpage, and its concrete layout as a
DOM tree. The key to understanding this mapping amounts toidentifying the container nodes in the DOM tree that contain all the
attributes of a single data item (tuple). Once this mapping is learnt,
the next step is to capture it by synthesizing extraction schemes inthe form of XPaths.
The data extractor synthesis algorithm is ï¬rst described using the
generic notion of node descriptors. In Section 5.2 we then instanti-ate it for the case where node descriptors are provided by XPaths.
Before we describe our algorithm, we review its main ingredi-
372ents. In the following, we use ğ‘(ğ‘)to denote the set of nodes in
the DOM tree of a webpage ğ‘âˆˆğ‘ƒ(ğ‘ ).
Knowledge base of data across websites Our synthesizer maintains
a knowledge base ğ‘‚:Attâ†’2ğ‘‰which consists of a set of observed
instances for each attribute ğ‘âˆˆAtt. These are instances collected
across different websites from ğ‘†. They enable the synthesizer to
locate potential ğ‘-attribute nodes in webpages for which the data
extractor ofğ‘is unspeciï¬ed.
Data to node mapping per website In addition to the global knowl-
edge base, for each website ğ‘ âˆˆğ‘†our synthesizer maintains: (i) a
setğ‘cont(ğ‘)âŠ†ğ‘(ğ‘)of (candidate) container nodes for each web-
pageğ‘âˆˆğ‘ƒ(ğ‘ ), and (ii) a set ğ‘ğ‘(ğ‘)âŠ†ğ‘(ğ‘)of (candidate) at-
tribute nodes for each webpage ğ‘âˆˆğ‘ƒ(ğ‘ )and attributeğ‘âˆˆAtt.
Deriving extraction schemes per website The synthesis algorithm
iteratively updates the container and attribute node sets for each
webpage inğ‘ƒ(ğ‘ ), and attempts to generate a data extractor ğ¸(ğ‘ ):
ExprÃ—(Att/arrowhookleftâ†’Expr )forğ‘ by generating node descriptors for
the set of containers, and for each of the attributes. The extractionscheme is shared by all webpages of the website. The updates of
the sets and the attempts to generate node descriptors from the setsare interleaved, as one can affect the other; on the one hand node
descriptors are generated in an attempt to represent the sets; on the
other hand, once descriptors are generated, elements of the sets thatdo not conform to them are removed.
While attribute instances are used to identify attribute nodes across
different websites, the synthesis of node descriptors is performedfor each website separately and independently of others (while con-sidering all of the webpages associated with the website).
5.1 Algorithm
Algorithm 1 presents our data extractor synthesis algorithm. The
algorithm is iterative, where each iteration consists of two phases:
Phase 1: Data extraction for knowledge base extension. Initially,
the setsğ‘‚(ğ‘)of instances of all attributes ğ‘âˆˆAttare empty. In
each iteration, we use yet un-crawled extraction schemes to extract
attribute nodes in all webpages of all websites and extend the setsğ‘‚(ğ‘)for every attribute ğ‘based on the content of the extracted
nodes. At the ï¬rst iteration, input extraction schemes are used. Inlater iterations, we use newly learnt extraction schemes, generatedin phase 2 of the previous iteration.
Phase 2: Synthesis of data extractors. For every website ğ‘ âˆˆğ‘†for
which the extraction scheme is not yet satisfactory, we attempt to
generate an extraction scheme by performing the following steps:
(1)Locating attribute nodes per page: We traverse all webpages
ğ‘âˆˆğ‘ƒ(ğ‘ )and for each attribute ğ‘we use the instances ğ‘‚(ğ‘)col-
lected in phase 1 (from this iteration and previous ones) to identify
potentialğ‘-attribute nodes in ğ‘. Technically, for every ğ‘âˆˆğ‘ƒ(ğ‘ )
we iterate on all ğ‘›âˆˆğ‘(ğ‘)and use the (default or user-speciï¬ed)
equivalence relation â‰¡
ğ‘to decide whether ğ‘›contains data that
matches the attribute instances in ğ‘‚(ğ‘).I fs o ,ğ‘›is added toğ‘ğ‘(ğ‘).
(2)Locating container nodes per page: In every webpage ğ‘âˆˆ
ğ‘ƒ(ğ‘ )we locate potential container nodes, and collect them in ğ‘cont(ğ‘).
A container is expected to contain instances of all attributes Att.
However, since our knowledge of the attribute instances is incom-
plete, we need to also consider subsets of Att. In each webpage,
we deï¬ne the â€œbestâ€ set of attributes to be the set of all attributes
whose instances appear in it. Potential containers are nodes whose
subtree contains attribute nodes of the â€œbestâ€ set of attributes, andno strict subtree contains nodes of the same set of attributes. The
latter ensures that the container is best. Technically, for every node
ğ‘›âˆˆğ‘(ğ‘)we compute the set of reachable attributes ğ‘âˆˆAtt
such that anğ‘-attribute node in ğ‘
ğ‘(ğ‘)is reachable from ğ‘›. Nodesğ‘›whose set is best and no other node reachable from ğ‘›has the
same set of reachable attributes are collected in ğ‘cont(ğ‘). For each
container node ğ‘›ğ‘âˆˆğ‘cont(ğ‘)we also maintain its support -t h e
number of attribute nodes reachable from it.
(3)Generating container descriptor: We consider the concrete
node descriptor of every container node ğ‘›ğ‘âˆˆğ‘cont(ğ‘)in every
webpageğ‘âˆˆğ‘ƒ(ğ‘ ). We unify the concrete node descriptors across
all webpages into a single node descriptor, and use it to updateğ¸(ğ‘ ), relying on the observation that containers are typically ele-
ments of some data structure and are therefore accessed similarly.
(4)Filtering attribute nodes based on container descriptor: We
ï¬lter the setsğ‘
cont(ğ‘)of containers in all webpages to keep only
containers that match the uniï¬ed node descriptor, and accordingly
ï¬lter the setsğ‘ğ‘(ğ‘)of attribute nodes in all webpages to con-
tain only nodes that are reachable from the ï¬ltered sets of con-
tainers. This step enables us to automatically distinguish the nodeswe are interested in from others that accidentally contain attribute
instances, without any a-priori knowledge.
(5)Generating attribute descriptors: For each attribute ğ‘âˆˆAtt,
we consider the concrete node descriptors of all the nodes in theï¬ltered setsğ‘
ğ‘(ğ‘)of all webpages ğ‘âˆˆğ‘ƒ(ğ‘ ), where the concrete
node descriptor of ğ‘›is computed relatively to the container node
whose subtree contains ğ‘›. For each attribute ğ‘, we ï¬nd a uniï¬ed
node descriptor for these concrete node descriptors, and use it to
updateğ¸(ğ‘ ). Again, we use the observation that containers are
structured similarly and therefore attribute data within them is ac-
cessed similarly.
Remark. For a successful application of our algorithm, at leastone extraction scheme should be provided for every attribute. Ourapproach is also applicable if a user provides a set of annotated
webpages instead of a set of initial extraction expressions.
Section 2 describes a running example of our algorithm.
Node descriptor uniï¬cation Node descriptors for the container and
attributes are generated by unifying concrete node descriptors of
the nodes inğ‘
cont(ğ‘)andğ‘ğ‘(ğ‘)respectively. Roughly speaking,
the purpose of the uniï¬cation is to derive a node descriptor that
is general enough to describe as many of the concrete node de-
scriptors as possible, but also as concrete as possible in order tointroduce as little noise as possible. â€œConcretenessâ€ of a node de-
scriptorğ‘¥is measured by an abstraction score, denoted abs (ğ‘¥).
The node descriptor uniï¬cation algorithm is parametric in the ab-
straction score. In Section 5.2, we provide a deï¬nition of this score
when the node descriptors are given by XPaths.
D
EFINITION 5.1. For a setğ‘‹of concrete node descriptors and
a weight function support that associates each ğ‘¥âˆˆğ‘‹with its sup-
port, the uniï¬cation problem aims to ï¬nd a node descriptor ğ‘¥ğ‘”, s.t.:
1.support ({ğ‘¥âˆˆğ‘‹âˆ£/llbracketğ‘¥/rrbracketâŠ†/llbracketğ‘¥ğ‘”/rrbracket})
support (ğ‘‹)>ğ›¿, i.e.,ğ‘¥ğ‘”captures at least ğ›¿of the
total support of the node descriptors in ğ‘‹.
2. abs(ğ‘¥ ğ‘”)is minimal.
In container descriptor uniï¬cation (step 3), the given node de-
scriptors represent container nodes. The support of each descriptor
represents the number of attribute nodes reachable from the con-
tainer. In attribute descriptors uniï¬cation (step 5), the given de-
scriptors represent attribute nodes for some attribute, all of whichare reachable from a set of containers of interest. The attribute node
descriptors are relative to the container nodes.
5.2 Implementation using sequential XPaths
In order to complete the description of our data extractor synthe-
sizer, we describe how the ingredients of Algorithm 1 are imple-
mented when node descriptors are given by XPaths. Speciï¬cally,our approach uses sequential XPaths:
373Algorithm 1: Data Extractor Synthesizer
Input: set of attributes Att
Input: set of websites ğ‘†
Input: am a p ğ¸:ğ‘†â†’(Expr Ã—(Att/arrowhookleftâ†’Expr)) mapping a website ğ‘ 
to a data extractor ğ¸(ğ‘ )which consists of a (possibly empty)
container descriptor as well as a (possibly partial) mapping of
attributes to node descriptors
ğ‘‚=[ ]
while there is change in ğ‘‚orğ¸do
/*Data extraction phase */
foreach ğ‘ âˆˆğ‘†s.t.ğ¸(ğ‘ )is uncrawled do
ğ‘‚=ğ‘‚âˆªExtractInstances (Att,ğ‘ƒ (ğ‘ ),ğ¸ (ğ‘ ),ğ‘‚ )
/*Synthesis phase */
foreach ğ‘ âˆˆğ‘†s.t.ğ¸(ğ‘ )is incomplete do
/*Locate attribute nodes */
foreach ğ‘âˆˆğ‘ƒ(ğ‘ )do
foreach ğ‘âˆˆAttdo
ğ‘ğ‘(ğ‘)=FindAttNodes (ğ‘(ğ‘),ğ‘ ,ğ‘‚ (ğ‘))
/*Locate container nodes */
foreach ğ‘âˆˆğ‘ƒ(ğ‘ )do
ğ‘ğ‘’ğ‘ ğ‘¡ğ´ğ‘¡ğ‘¡ğ‘†ğ‘’ğ‘¡ ={ğ‘âˆˆAttâˆ£ğ‘ğ‘(ğ‘)âˆ•=âˆ…}
foreach ğ‘›âˆˆğ‘(ğ‘)do
ğ‘Ÿğ‘’ğ‘ğ‘â„ğ´ğ‘¡ğ‘¡ [ğ‘][ğ‘›]={ğ‘âˆˆAttâˆ£âˆƒğ‘›â€²âˆˆğ‘Ÿğ‘’ğ‘ğ‘â„ (ğ‘›):
ğ‘›â€²âˆˆğ‘ğ‘(ğ‘)}
ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡[ğ‘ ][ğ‘›]=# {ğ‘›â€²âˆˆğ‘Ÿğ‘’ğ‘ğ‘â„ (ğ‘›)âˆ£âˆƒğ‘âˆˆAtt:
ğ‘›â€²âˆˆğ‘ğ‘(ğ‘)}
ğ‘cont(ğ‘)=ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  ={ğ‘›âˆˆğ‘(ğ‘)âˆ£
ğ‘Ÿğ‘’ğ‘ğ‘â„ğ´ğ‘¡ğ‘¡ [ğ‘›]=ğ‘ğ‘’ğ‘ ğ‘¡ğ´ğ‘¡ğ‘¡ğ‘†ğ‘’ğ‘¡}
foreach ğ‘›âˆˆğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  do
foreach ğ‘›â€²âˆˆğ‘â„ğ‘–ğ‘™ğ‘‘ğ‘Ÿğ‘’ğ‘›(ğ‘› )do
ifğ‘›â€²âˆˆğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘  then
ğ‘cont(ğ‘)=ğ‘cont(ğ‘)âˆ–{ğ‘›}
break
/*Generate container descriptor */
ğ¸ğ‘¥ğ‘ğ‘Ÿğ‘  =
{(ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ¸ğ‘¥ğ‘ğ‘Ÿ(ğ‘, ğ‘’ğ‘šğ‘ğ‘¡ğ‘¦ğ¸ğ‘¥ğ‘ğ‘Ÿ, ğ‘› ), ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡[ğ‘ ][ğ‘›])âˆ£
ğ‘âˆˆğ‘ƒ(ğ‘ ),ğ‘› âˆˆğ‘cont(ğ‘)}
ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿ =UnifyExpr (ğ¸ğ‘¥ğ‘ğ‘Ÿğ‘  )
FilterAttributeNodes()
/*Generate attribute descriptors */
foreach ğ‘âˆˆAttdo
ğ¸ğ‘¥ğ‘ğ‘Ÿğ‘  =
{(ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ¸ğ‘¥ğ‘ğ‘Ÿ(ğ‘, ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿ, ğ‘›), 1)âˆ£ğ‘âˆˆ
ğ‘ƒ(ğ‘ ),ğ‘› âˆˆğ‘ğ‘(ğ‘)}
ğ‘ğ‘¡ğ‘¡ğ¸ğ‘¥ğ‘ğ‘Ÿ [ğ‘]=UnifyExpr (ğ¸ğ‘¥ğ‘ğ‘Ÿğ‘  )
ğ¸(ğ‘ )=( ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿ, ğ‘ğ‘¡ğ‘¡ğ¸ğ‘¥ğ‘ğ‘Ÿ )
return ğ¸
Sequential XPaths Apathğœ‹in the DOM tree is a sequence of
nodesğ‘›1,...,ğ‘› ğ‘˜, where for every 1â‰¤ğ‘–<ğ‘˜ , there is an edge
fromğ‘›ğ‘–toğ‘›ğ‘–+1. Such a path can naturally be encoded using an
XPath XS(ğœ‹)=ğ‘¥1...ğ‘¥ ğ‘˜where eachğ‘¥ğ‘–starts with â€œ /â€.ğ‘¥1may
start with â€œ //â€ rather than â€œ /â€i fğœ‹does not necessarily start at the
root of the tree. Further, each ğ‘¥ğ‘–uses node ï¬lters and predicates
to describe the features of ğ‘›ğ‘–. Therefore,ğ‘¥ğ‘–can be described via
equalitiesğ‘“1=ğ‘£1,...,ğ‘“ ğ‘š=ğ‘£ğ‘š, such thatğ‘“ğ‘—âˆˆğ¹,w h e r eğ¹is
the set of node features used. We consider ğ¹={tag,class,id}
for simplicity, but our approach is not limited to these features. A
feature might be unspeciï¬ed for ğ‘›ğ‘–, in which case no corresponding
equality will be included in ğ‘¥ğ‘–.
For example, let ğœ‹be the left most path in ğ·2(Fig. 3). Then
XS(ğœ‹)=//body/.../td/div/h1 .XS(ğœ‹)can also be described
as a sequence âŸ¨tag=body âŸ©...âŸ¨tag=td âŸ©âŸ¨tag=div âŸ©âŸ¨tag=h1 âŸ©.
We refer to XPaths of the above form as sequential . The XPaths
that our approach generates as node descriptors are all sequential.
Concrete XPaths Each nodeğ‘›in the DOM tree can be uniquely
described by the unique path, denoted ğœ‹ğ‘›, leading from the root toğ‘›. The XPath XS(ğœ‹ğ‘›)is a sequential XPath such that /llbracketXS(ğœ‹ğ‘›)/rrbracketâŠ‡
{ğ‘›},a n d /llbracketXS(ğœ‹ğ‘›)/rrbracketis minimal (i.e., every other sequential XPath
that also describes ğ‘›, describes a superset of /llbracketXS(ğœ‹ğ‘›)/rrbracket). We there-
fore refer to XS(ğœ‹ğ‘›)as the concrete XPath ofğ‘›, denoted XS(ğ‘›)
with abuse of notation. (If we include in ğ¹the position of a node
among its siblings as an additional node feature, and encode it by
an XPath instruction using sibling predicates then we will have
/llbracketXS(ğœ‹ğ‘›)/rrbracket={ğ‘›}).
Agreement of sequential XPaths We observe that for sequential
XPaths, checking if a node ğ‘›matches a node descriptor ğ‘¥ğ‘”(i.e.
ğ‘›âˆˆ/llbracketğ‘¥ğ‘”/rrbracket) can be done by checking if the concrete XPath XS(ğ‘›)
agrees with the XPath ğ‘¥ğ‘”, where agreement is deï¬ned as follows.
DEFINITION 5.2. Letğ‘¥=ğ‘¥1...ğ‘¥ ğ‘˜andğ‘¥ğ‘”=ğ‘¥ğ‘”
1...ğ‘¥ğ‘”
ğ‘šbe
sequential XPaths. The instruction ğ‘¥ğ‘–agrees with instruction ğ‘¥ğ‘”
ğ‘–
if whenever some feature is speciï¬ed in ğ‘¥ğ‘–, it either has the same
value inğ‘¥ğ‘”
ğ‘–or it is unspeciï¬ed in ğ‘¥ğ‘”ğ‘–. The XPathğ‘¥agrees with the
XPathğ‘¥ğ‘”ifğ‘šâ‰¤ğ‘˜, and for every ğ‘–â‰¤ğ‘š,ğ‘¥ğ‘–agrees withğ‘¥ğ‘”
ğ‘–.
For example, //body/.../td/div[id=name1]/h1 agrees with
both //body/.../td/div/h1 ,a n d //body/.../td/div .
Node descriptor uniï¬cation via XPath uniï¬cation We now de-
scribe our solution to the node descriptor uniï¬cation problem in the
setting of sequential XPaths. We ï¬rst deï¬ne the abstraction score:
Abstraction score For a sequential XPath instruction ğ‘¥ğ‘–we deï¬ne
spec(ğ‘¥ğ‘–)to be the subset of features whose value is speciï¬ed in ğ‘¥ğ‘–,
andunspec(ğ‘¥ ğ‘–)=ğ¹âˆ–spec(ğ‘¥ ğ‘–)is the set of unspeciï¬ed features
inğ‘¥ğ‘–. We deï¬ne the abstraction score of ğ‘¥ğ‘–to be the number of
features in unspec (ğ‘¥ğ‘–),t h a ti s ,a b s (ğ‘¥ğ‘–)=âˆ£unspec(ğ‘¥ ğ‘–)âˆ£.
For a sequential XPath ğ‘¥=ğ‘¥1...ğ‘¥ ğ‘˜, we deï¬ne abs(ğ‘¥) to be
the sum of abs(ğ‘¥ ğ‘–).
Greedy algorithm for uniï¬cation Algorithm 2 presents our uniï¬ca-
tion algorithm. We use the observation that for sequential XPaths,
the condition /llbracketğ‘¥/rrbracketâŠ†/llbracketğ‘¥ğ‘”/rrbracketthat appears in item 1 of the uniï¬ca-
tion problem (see Deï¬nition 5.1) can be reduced to checking if the
XPathğ‘¥agrees with the XPath ğ‘¥ğ‘”.
Letğ‘‹be a weighted set of sequential XPaths, with a weight
function support that associates each XPath in ğ‘‹with its support.
LetTS=support(ğ‘‹ )denote the total support of XPaths in ğ‘‹.
The uniï¬cation algorithm selects ğ‘˜to be the length of the longest
XPath inğ‘‹. It then constructs a uniï¬ed XPath ğ‘¥ğ‘”=ğ‘¥ğ‘”
1,...,ğ‘¥ğ‘”
ğ‘š
top down, from ğ‘–=1 toğ‘˜(possibly stopping at ğ‘–=ğ‘š<ğ‘˜ ).
Intuitively, in each step the algorithm tries to select the most â€œcon-
creteâ€ instruction whose support is high enough. Note that there
is a tradeoff between the high-support requirement and the high-
concreteness requirement. We use the threshold as a way to balancethese measures.
At iterationğ‘–of the algorithm, ğ‘‹
ğ‘–âˆ’1is the restriction of ğ‘‹to
the XPaths whose preï¬x agrees with the preï¬x ğ‘¥ğ‘”
1,...,ğ‘¥ğ‘”
ğ‘–âˆ’1ofğ‘¥ğ‘”
computed so far (Initially, ğ‘‹0=ğ‘‹). We inspect the ğ‘–â€™th instruc-
tions of all XPaths in ğ‘‹ğ‘–âˆ’1. The corresponding set of instructions
is denoted byğ¼ğ‘–={ğ‘¥ğ‘–âˆ£ğ‘¥âˆˆğ‘‹ğ‘–âˆ’1}. The support of an instruction
ğ‘¥ğµw.r.t.ğ¼ğ‘–issupport({ğ‘¥âˆˆğ‘‹ğ‘–âˆ’1âˆ£ğ‘¥ğ‘–agrees withğ‘¥ğµ}).
To select the most â€œconcreteâ€ instruction whose support is high
enough, we consider a predeï¬ned order on sets of feature-value
pairs, where sets that are considered more â€œconcreteâ€ (i.e., moreâ€œspeciï¬edâ€) precede sets considered more â€œabstractâ€. Technically,
we consider only feature-value sets where each feature has a unique
value. The order on such sets used in the algorithm is deï¬ned suchthat if âˆ£ğµ
1âˆ£>âˆ£ğµ2âˆ£thenğµ1precedesğµ2. In particular, we make
sure that sets where all features are speciï¬ed are ï¬rst in that order.
For every setğµof feature-value pairs, ordered by the predeï¬ned
order, we consider the instruction ğ‘¥ğµthat is speciï¬ed exactly on
374the features inğµ,a sd e ï¬ n e db y ğµ. If its support exceeds ğ›¿,w es e t
ğ‘¥ğ‘”
ğ‘–toğ‘¥ğµandğ‘‹ğ‘–to{ğ‘¥âˆˆğ‘‹ğ‘–âˆ’1âˆ£ğ‘¥ğ‘–agrees withğ‘¥ğµ}. Otherwise,
ğ‘¥ğµis not yet satisfactory and the search continues with the next ğµ.
There is always a ğµfor which the support of the ğ‘¥ğµexceeds the
threshold, for instance, the last set ğµis always the empty set with
ğ‘¥ğµ=/*, which agrees with all the concrete XPaths in ğ‘‹ğ‘–âˆ’1.
If at some iteration ğ¼ğ‘–=âˆ…, i.e. the XPaths in ğ‘‹ğ‘–âˆ’1are all of
length<ğ‘–and therefore there is no â€œnextâ€ instruction to discover,
the algorithm terminates. Otherwise, it terminates when ğ‘–=ğ‘˜.
EXAMPLE 1.Given the following concrete XPaths as an input:
ğ‘ğ‘¥1=/div[class=â€œtitleâ€]/span/a[id=â€œt1â€]
ğ‘ğ‘¥2=/div[class=â€œtitleâ€]/span/a[id=â€œt2â€]
ğ‘ğ‘¥3=/div[class=â€œnoteâ€]/span/a[id=â€œn1â€]
The uniï¬cation starts with ğ‘‹0={ğ‘ğ‘¥1,ğ‘ğ‘¥2,ğ‘ğ‘¥3}, andğ‘–=1.T o
selectğ‘¥ğ‘”
1, recall that the algorithm ï¬rst considers the most speciï¬c
feature-value sets (in order to ï¬nd the most speciï¬c instruction).
In our example it starts from ğµ1={tag=div,class=note }
for whichğ‘¥ğµ1=/div[class=â€œnoteâ€] . However,ğ‘ğ‘¥3is the
only XPath inğ‘‹0which agrees with ğ‘¥ğµ1. Therefore it has sup-
port of 1/3. We use a threshold of ğ›¿=1/2. Thus, the sup-
port ofğ‘¥ğµ1is insufï¬cient. The algorithm skips to the next op-
tion, obtaining ğ‘¥ğµ2=/div[class=â€œtitleâ€] . This instruc-
tion is as speciï¬c as ğ‘¥ğµ1and has a sufï¬cient support of 2/3 (it
agrees withğ‘ğ‘¥1andğ‘ğ‘¥2). Therefore, for ğ‘–=1, the algorithm
selectsğ‘¥ğ‘”
1=ğ‘¥ğµ2andğ‘‹1={ğ‘ğ‘¥1,ğ‘ğ‘¥2}.F o rğ‘–=2, the algo-
rithm selectsğ‘¥ğ‘”
2=/span as the most speciï¬c instruction, which
also has support of 2/2 (both ğ‘ğ‘¥1andğ‘ğ‘¥3fromğ‘‹1agree with
it). Forğ‘–=3 , the algorithm selects ğ‘¥ğ‘”
3=/aas none of the
more speciï¬c instructions ( /a[id=â€œt1â€] or/a[id=â€œt2â€] ) has
a support greater than ğ›¿=1/2. The resulting uniï¬ed XPath is
ğ‘¥=/div[class="title"]/span/a .
Algorithm 2: Top-Down XPath Uniï¬cation
Input: setğ‘‹of sequential XPaths
Input: support function support :ğ‘‹â†’N
Input: threshold ğ›¿
TS=support(ğ‘‹ )
ğ‘˜=m a x ğ‘¥âˆˆğ‘‹âˆ£ğ‘¥âˆ£
ğ‘‹0=ğ‘‹
foreach ğ‘–=1,...,ğ‘˜ do
ğ¼ğ‘–={ğ‘¥ğ‘–âˆ£ğ‘¥âˆˆğ‘‹ğ‘–âˆ’1}
ifğ¼ğ‘–=âˆ…then
ğ‘–=ğ‘–âˆ’1
break
foreach ğµâŠ†ğ¹in decreasing order of âˆ£ğµâˆ£do
ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ ğµ=FindSupport (ğ‘¥ğµ,ğ‘‹ğ‘–âˆ’1,ğ‘– ,support)
ifğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ ğµ>ğ›¿â‹…TSthen
ğ‘¥ğ‘”
ğ‘–=ğ‘¥ğµ
ğ‘‹ğ‘–={ğ‘¥âˆˆğ‘‹ğ‘–âˆ’1âˆ£ğ‘¥ğ‘–agrees with ğ‘¥ğµ}
break
return ğ‘¥ğ‘”
1,...,ğ‘¥ğ‘”
ğ‘–
6 Crawler Synthesis
In this section we complete the description of our crawler synthe-
sizer. To do so, we describe the synthesis of a page crawler for eachwebsiteğ‘ . Recall that a page crawler corresponds to a URL pattern
ğ‘ˆ(ğ‘ )which deï¬nes the webpages of interest. The synthesis of a
page crawler is intertwined with the data extractor synthesis, anduses similar uniï¬cation techniques to generate the URL pattern.Initialization We assume that each website ğ‘ âˆˆğ‘†is given by a
â€œmainâ€ webpage ğ‘
ğ‘šğ‘ğ‘–ğ‘› (ğ‘ ). Initially, the set ğ‘ƒ(ğ‘ )of webpages of ğ‘ 
is the set of all webpages obtained by following links in ğ‘ğ‘šğ‘ğ‘–ğ‘› (ğ‘ )
and recursively following links in the resulting pages, where the
traversed links are selected based on some heuristic function which
determines which links are more likely to lead to relevant pages.
Iterations We apply the data extractor synthesis algorithm of Sec-
tion 5 using the sets ğ‘ƒ(ğ‘ ). At the end of phase 2 of each iteration,
we updateğ‘ˆ(ğ‘ )using the steps described below. At the beginning
of phase 1 of the subsequent iteration we then update ğ‘ƒ(ğ‘ )to the
set of webpages whose URLs conform with ğ‘ˆ(ğ‘ ).
(6)Filtering webpage sets: Based on the observation that rel-
evant webpages of a website ğ‘ have a similar structure, we keep
inğ‘ƒ(ğ‘ )only webpages that contain container and attribute nodes
that match the generated ğ¸(ğ‘ )and are reachable from ğ‘ğ‘šğ‘ğ‘–ğ‘› (ğ‘ )
via such webpages.
(7)Generating URL patterns: For each webpage ğ‘âˆˆğ‘ƒ(ğ‘ )we
consider its URL. We unify the URLs into ğ‘ˆ(ğ‘ )by a variation of
Algorithm 2 which views a URL as a sequence of instructions, sim-
ilarly to a sequential XPath.
7 Evaluation
In this section we evaluate the effectiveness of our approach. Weused it to synthesize data extracting web-crawlers for real-world
websites containing structured data of different categories. Our
experiments focus on two different aspects: (i) the ability to suc-cessfully synthesize web-crawlers, and (ii) the performance of the
resulting web crawlers.
7.1 Experimental Settings
We have implemented our tool in C#. All experiments ran on a ma-
chine with a quad core CPU and 32GB memory. Our experiments
were run on 30 different websites, related to nine different cat-
egories: books, TVs, conferences, universities, cameras, phones,movies, songs and hotels. For each category we selected a group
of 3-4 known sites, which appear in the ï¬rst page of Google search
results.
The sites in each category have a different structure, but they
share at least some of their instances, which makes our approach
applicable. The complexity of the data extracted from different cat-
egories is also different. For instance a movie has four attributes:title, genre ,director and list of actors. For a book, the set of at-
tributes consists of title, author andprice, while the attribute set
of a camera consists of the name andprice only. In each category
we used one manually written crawler and automatically synthe-sized the others (for the books category we also experimented with
3partial extraction schemes, one for each attribute). To synthesize
the web crawlers, our tool processed over 12,000 webpages from
the30different sites.
To evaluate the effectiveness of our tool we consider 4aspects of
synthesized crawlers: (i) Crawling scheme completeness, (ii) URL
ï¬ltering, (iii) Container extraction, and (iv) Attributes extraction.
7.2 Experiments and Results
Crawling Scheme Completeness A complete crawling scheme de-
ï¬nes extraction queries for all of the data attributes. The complete-
ness of the synthesized crawling schemes is an indicator for the
success of our approach in synthesizing crawlers. To measure com-
pleteness, we calculated for each category the average number ofattributes covered by the schemes, divided by the number of at-
tributes of the category. The results are reported in Fig. 6 (left).
The results show that the resulting extraction schemes are mostlycomplete, with a few missing attribute extraction queries.
375Figure 6: Results: Crawling scheme completeness (left), URL ï¬ltering (middle) and Attribute extraction (right) for each category.
Figure 7: Attribute extraction precision and recall, and crawl-
ing scheme completeness, as a function of the threshold of Jac-card similarity used to deï¬ne equivalence between instances.
URL Filtering The ability to locate pages containing data is an
important aspect of a crawlerâ€™s performance. To evaluate the URL
ï¬ltering performance of the synthesized crawlers, we measure therecall andprecision of the synthesized URL pattern for each site:
ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ =âˆ£ğ‘…ğ‘’ğ‘™âˆ©ğ‘†ğ‘œğ‘™âˆ£
âˆ£ğ‘…ğ‘’ğ‘™âˆ£ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =âˆ£ğ‘…ğ‘’ğ‘™âˆ©ğ‘†ğ‘œğ‘™âˆ£
âˆ£ğ‘†ğ‘œğ‘™âˆ£(1)
To do so, we have manually generated two sets of URLs for eachsite: one containing URLs for pages that contain relevant data,
comprising the ğ‘…ğ‘’ğ‘™ set (ground truth), and another, denoted ğ¼ğ‘Ÿğ‘Ÿ,
contains a mixture of irrelevant URLs from the same site. ğ‘†ğ‘œğ‘™con-
tains the URLs from ğ‘…ğ‘’ğ‘™âˆªğ¼ğ‘Ÿğ‘Ÿ that match the synthesized URL
pattern for the site (i.e., the URLs accepted by the synthesised URL
pattern). A good performing URL ï¬ltering pattern should match allthe URLs from ğ‘…ğ‘’ğ‘™ and should not match any from ğ¼ğ‘Ÿğ‘Ÿ.T h e a v -
erage recall and precision scores of the sites of each category arecalculated and reported in Fig. 6 (middle).
Container Extraction To check the correctness of the synthesized
container extraction query, we have manually reviewed the result-
ing container XPaths against the HTML sources of the relevant
webpages for each site, to verify that each extracted container con-tains exactly one data item. We found that the containers always
contained no more than one item. However, in a few cameras and
songs websites, the container query was too speciï¬c and did notextract some of the containers (this happened in tables containingclass=â€œoddâ€ in some rows and class=â€œevenâ€ in others), which af-
fected the recall scores of attribute extraction.
Attributes Extraction We calculate the recall and precision (see
equation (1)) of the extraction query for each attribute. Techni-cally, for each category of sites, we have manually written extrac-
tion queries for each attribute in every one of the category relatedsites. For each attribute ğ‘, we used these extraction queries to ex-
tract the instances of ğ‘from a set of sample pages from each site.
The extracted instances are collected in ğ‘…ğ‘’ğ‘™. We have also applied
the synthesized extraction queries (as a concatenation of the con-tainer XPath and attribute XPath) to extract instances of ğ‘from the
same pages into ğ‘†ğ‘œğ‘™. For each site, the precision and recall are cal-
culated according to equation (1). The average (over sites of thesame category) recall and precision scores of all attributes of each
category are reported in Fig. 6 (right).
Equivalence Relation To evaluate the effect of the threshold used
in the equivalence relation, â‰¡
ğ‘, on the synthesized crawlers, we
have measured the average completeness, as well as the average
recall and precision scores of attribute extraction as a function of
the threshold. The results appear in Fig. 7.
Remark. The reported attribute extraction recalls in Fig. 6 and
Fig. 7 are computed based on queries for which synthesis suc-
ceeded (missing queries affect only completeness, and not recall).
7.3 Discussion
The completeness of the synthesized extraction schemes is highly
dependent upon the ability to identify instances in pages of some
site by comparison to instances gathered from other sites. For mostcategories, completeness is high. For the conferences category,
however, completeness is low. This is due to the use of acronyms in
conference names (e.g., ICSE) in some sites vs. full names (e.g., In-ternational Conference on Software Engineering) in others, whichmakes it hard for our syntax-based equivalence relation to identify
matches. This could be improved by using semantic equivalence
relations (such as ESA [12] or W2V [33]).
As for the quality of the resulting extraction schemes and URL
ï¬ltering patterns, most of the categories have perfect recall (Fig. 6).
However, some have a slightly lower recall due to our attempt to
keep the synthesized XPaths (or regular expressions, for URL ï¬l-tering) as concrete as possible while having majority agreement.
This design choice makes our method tolerant to small noises in
the identiï¬ed data instances, and prevents such noises from caus-ing drifting, without negative examples. Yet, in some cases, the
resulting XPaths are too speciï¬c and result in a sub-optimal recall.
For precision, most categories have good scores, while a few
have lower scores. Loss of precision can be attributed to the majority-
based uniï¬cation and the lack of negative examples. For the books
category, for instance, the synthesized extraction XPath of price for
some sites is too general, since they list multiple price instances(original price, discount amount, and new price). All are listed in
376the same â€œparent containerâ€ with the author and book title, and are
therefore not ï¬ltered by the container, hence affecting XPath uniï¬-cation. This could be improved with user guidance.
The results in Fig. 7 reï¬‚ect the tradeoff between precision and
crawling scheme completeness. A more strict equivalence relation
(with higher threshold) leads to a better precision but has negative
effect on the scheme completeness, whereas the use of a forgiv-ing equivalence relation (with lower threshold) severely affects theprecision. We use a threshold of 0.5as a balanced threshold value.
According to our ï¬ndings, the attribute queries suffer from a lowrecall for both low and high threshold values. In low threshold,it is due to wrong queries, that extract wrong nodes (e.g., menu
nodes), without including attribute nodes. For higher threshold val-
ues, the tool identiï¬ed less instances of attribute nodes (sometimesonly one), leading to a lower quality generalization.
Real-World Use Case We used our crawler synthesis process as
a basis for data extraction for several product reviews websites.
For instance, tvexp.com, weppir.com, camexp.com and phonesum.com extract product names and speciï¬cations (specs) using our ap-proach. We manually added another layer of specs scoring, and
created comparison sites for product specs. These websites have a
continually updated database with over 20,000 products.
8 Related Work
In this section, we brieï¬‚y survey closely-related work. While therehas been a lot of past work on various aspects of mining and dataextraction, our technique has the following unique combination of
features: (i) works across multiple websites, (ii) synthesizes both
the extraction XPath queries, and the URL pattern, (iii) is auto-matic and does not require user interaction, (iv) works with only
positive examples, (v) does not require an external database, and
(vi) synthesizes a working crawler.
Data Mining and Wrapper Induction Our work is related to data
mining and wrapper induction. In contrast to supervised techniques
(e.g., [24, 25, 29, 5, 16]), our approach only requires an initial
crawler (or partial crawling scheme) and requires no tagged exam-
ples. FlashExtract [25] allows end-users to give examples via aninteraction model to extract various ï¬elds and to link them using
special constructs. It then applies an inductive synthesis algorithm
to synthesize the intended data extraction program from the givenexamples. In contrast, our starting point is a crawler for one (ormore) sites, which we then extrapolate from. Further, our tech-
nique only requires positive examples (obtained by bootstrapping
our knowledge base by crawling other sites).
Unsupervised extraction techniques [2, 3, 8, 32, 31, 34, 40, 36]
have been proposed. Several works [3, 31, 2, 10, 38, 27, 37] pro-
pose methods that use repeated pattern mining to discover data
records, while [34, 40] use tree-edit distance as the basis for recordrecognition and extraction in a single given page. These methodsrequire manual annotation of the extracted data or rely on knowl-
edge bases [14, 20]. Roadrunner [8] uses similarities and differ-
ences between webpages to discover data extraction pattern. Sim-ilarities are used to cluster similar pages together and dissimilari-
ties between pages in the same cluster are used to identify relevant
structures. Other information extraction techniques rely on textual,or use visual features of the document [41, 30] for data extraction.
ClustVX [15] renders the webpage in contemporary web browser,
for processing all visual styling information. Visual and structuralfeatures are then used as similarity metric to cluster webpage ele-
ments. Tag paths of the clustered webpages are then used to de-
rive extraction rules. In contrast, our approach does not use visualstyling, but relies on similar content between the different sites.HAO et al. [18] present a method for data extraction from a group
of sites. Their method is based on a classiï¬er that is trained on aseed site using a set of predeï¬ned feature types. The classiï¬er isthen used as a base for identiï¬cation and extraction of attribute in-
stances in unseen sites. In contrast, our goal is to synthesize XPaths
that are human-readable, editable, and efï¬cient. Further, with thelack of an attribute grouping mechanism (such as our notion of con-
tainer), the method cannot handle pages with multiple data items.
Program Synthesis Several works on automatic synthesis of pro-
grams [22, 13, 25, 17] were recently proposed, aiming for automat-
ing repetitive programming tasks. Programming by example, for
instance, is a technique used in [22, 25] for synthesizing a programby asking the user to demonstrate actions on concrete examples. In-
spired by these works, our approach automatically synthesizes data
extracting web crawlers. However, we require no user interaction.
Semantic Annotation Many works in this area attempt to automat-
ically annotate webpages with semantic meta-data.
Seeker [11] is a platform for large-scale text analysis, and an
application written on the platform called SemTag that performs
automated semantic tagging of large corpora. Ciravegna et al. [6]
propose a methodology based on adaptive information extractionand implement it in a tool called Armadillo [4]. The learning pro-
cess is seeded by a user deï¬ned lexicon or an external data source.
In contrast to these works, our approach does not require externalknowledge base and works by bootstrapping its knowledge base.
Other Aspects of Web Crawling There are a lot of works dealing
with different aspects of web crawlers. Jiang et al. [23] and Jung
et al. [1] deal with deep-web related issues, like the problem of
discovering webpages that cannot be reached by traditional web
crawlers mostly because they are results of a query submitted toa dynamic form and they are not reachable via direct links fromother pages. Some other works like [35, 39] address the problem of
efï¬cient navigation of website pages to reach pages of speciï¬c type
by training a decision model and using it do decide which links tofollow in each step. Our paper focuses on the different problem of
data extraction, and is complementary to these techniques.
9 Conclusion
We presented an automatic synthesis of data extracting web crawlersby extrapolating existing crawlers for the same category of data
from other websites. Our technique relies only on data overlapsbetween the websites and not on their concrete representation. As
such we manage to handle signiï¬cantly different websites. Techni-
cally, we automatically label data in one site based on others andsynthesize a crawler from the labeled data. Unlike techniques that
synthesize crawlers from user provided annotated data, we cannot
assume that all annotations are correct (hence some of the examplesmight be false positives), and we cannot assume that unannotated
data is noise (hence we have no negative examples). We overcome
these difï¬culties by a notion of containers that ï¬lters the labeling.
We have implemented our approach and used it to automatically
synthesize 30crawlers for websites in nine different product cat-
egories. We used the synthesized crawlers to crawl more than12,000 webpages over all categories. In addition, we used our
method to build crawlers for real product reviews websites.
Acknowledgements
The research leading to these results has received funding from the
European Unionâ€™s - Seventh Framework Programme (FP7) under
grant agreement no. 615688, ERC-COG-PRIME.
37710 References
[1] A N,Y .J . ,G ELLER ,J . ,W U,Y . - T . , AND CHUN ,S .
Semantic deep web: automatic attribute extraction from the
deep web data sources. In Proceedings of the 2007 ACM
symposium on Applied computing (2007), ACM,
pp. 1667â€“1672.
[2] A RASU ,A . , AND GARCIA -MOLINA , H. Extracting
structured data from web pages. In Proceedings of the 2003
ACM SIGMOD international conference on Management of
data (2003), ACM, pp. 337â€“348.
[3] C HANG ,C . - H . , AND LUI, S.-C. IEPAD: Information
extraction based on pattern discovery. In Proceedings of the
10th International Conference on World Wide Web (2001),
WWW â€™01, pp. 681â€“688.
[4] C HAPMAN ,S . ,D INGLI ,A . , AND CIRA VEGNA ,F .
Armadillo: harvesting information for the semantic web. In
Proceedings of the 27th annual international ACM SIGIR
conference on Research and development in informationretrieval (2004), ACM, pp. 598â€“598.
[5] C
HUANG ,S . - L . , AND HSU,J . - J. Tree-structured template
generation for web pages. In Web Intelligence, 2004. WI
2004. Proceedings. IEEE/WIC/ACM InternationalConference on (2004), IEEE, pp. 327â€“333.
[6] C
IRA VEGNA ,F . ,C HAPMAN ,S . ,D INGLI ,A . , AND WILKS ,
Y. Learning to harvest information for the semantic web. In
The Semantic Web: Research and Applications . Springer,
2004, pp. 312â€“326.
[7] C LARK ,J . ,D EROSE,S . , ET AL . Xml path language
(xpath). W3C recommendation 16 (1999).
[8] C RESCENZI ,V . ,M ECCA ,G . , AND MERIALDO ,P .
Roadrunner: Towards automatic data extraction from largeweb sites. In Proceedings of the 27th International
Conference on Very Large Data Bases (2001), VLDB â€™01,
pp. 109â€“118.
[9] D
ALVI ,N . ,B OHANNON ,P . , AND SHA, F. Robust web
extraction: An approach based on a probabilistic tree-editmodel. In Proceedings of the 2009 ACM SIGMOD
International Conference on Management of Data (New
York, NY , USA, 2009), SIGMOD â€™09, ACM, pp. 335â€“348.
[10] D
ALVI ,N . ,K UMAR ,R . , AND SOLIMAN , M. Automatic
wrappers for large scale web extraction. Proceedings of the
VLDB Endowment 4, 4 (2011), 219â€“230.
[11] D ILL,S . ,E IRON ,N . ,G IBSON ,D . ,G RUHL ,D . ,G UHA ,R . ,
JHINGRAN ,A . ,K ANUNGO ,T . ,R AJAGOPALAN ,S . ,
TOMKINS ,A . ,T OMLIN ,J .A . , ET AL . Semtag and seeker:
Bootstrapping the semantic web via automated semantic
annotation. In Proceedings of the 12th international
conference on World Wide Web (2003), ACM, pp. 178â€“186.
[12] G ABRILOVICH ,E . , AND MARKOVITCH , S. Computing
semantic relatedness using wikipedia-based explicit semanticanalysis. In IJCAI (2007), vol. 7, pp. 1606â€“1611.
[13] G
ALENSON ,J . ,R EAMES ,P . ,B ODIK ,R . ,H ARTMANN ,B . ,
AND SEN, K. Codehint: Dynamic and interactive synthesis
of code snippets. In Proceedings of the 36th International
Conference on Software Engineering (2014), ACM,
pp. 653â€“663.
[14] G ENTILE ,A .L . ,Z HANG ,Z . ,A UGENSTEIN ,I . , AND
CIRA VEGNA , F. Unsupervised wrapper induction using
linked data. In Proceedings of the Seventh International
Conference on Knowledge Capture (New York, NY , USA,
2013), K-CAP â€™13, ACM, pp. 41â€“48.[15] G RIGALIS , T. Towards web-scale structured web data
extraction. In Proceedings of the sixth ACM international
conference on Web search and data mining (2013), ACM,
pp. 753â€“758.
[16] G ULHANE ,P . ,M ADAAN ,A . ,M EHTA ,R . ,
RAMAMIRTHAM ,J . ,R ASTOGI ,R . ,S ATPAL ,S . ,
SENGAMEDU ,S .H . ,T ENGLI ,A . , AND TIW ARI ,C .
Web-scale information extraction with vertex. In Data
Engineering (ICDE), 2011 IEEE 27th International
Conference on (2011), IEEE, pp. 1209â€“1220.
[17] G ULW ANI ,S . ,J HA,S . ,T IW ARI ,A . , AND VENKATESAN ,
R. Synthesis of loop-free programs. In ACM SIGPLAN
Notices (2011), vol. 46, ACM, pp. 62â€“73.
[18] H AO,Q . ,C AI,R . ,P ANG ,Y . , AND ZHANG , L. From one
tree to a forest: a uniï¬ed solution for structured web dataextraction. In Proceedings of the 34th international ACM
SIGIR conference on Research and development inInformation Retrieval (2011), ACM, pp. 775â€“784.
[19] H
AWKINS ,P . ,A IKEN ,A . ,F ISHER ,K . ,R INARD ,M .C . ,
AND SAGIV , M. Data structure fusion. In Programming
Languages and Systems - 8th Asian Symposium, APLAS
2010 (2010), pp. 204â€“221.
[20] H ONG , J. L. Data extraction for deep web using wordnet.
Systems, Man, and Cybernetics, Part C: Applications and
Reviews, IEEE Transactions on 41 , 6 (2011), 854â€“868.
[21] J ACCARD , P. The distribution of the ï¬‚ora in the alpine zone.
New Phytologist 11 , 37â€“50.
[22] J HA,S . ,G ULW ANI ,S . ,S ESHIA ,S . ,T IW ARI ,A . , ET AL .
Oracle-guided component-based program synthesis. InSoftware Engineering, 2010 ACM/IEEE 32nd International
Conference on (2010), vol. 1, IEEE, pp. 215â€“224.
[23] J
IANG ,L . ,W U,Z . ,F ENG,Q . ,L IU,J . , AND ZHENG ,Q .
Efï¬cient deep web crawling using reinforcement learning. In
Advances in Knowledge Discovery and Data Mining .
Springer, 2010, pp. 428â€“439.
[24] K USHMERICK ,N . ,W ELD,D .S . , AND DOORENBOS ,R .B .
Wrapper induction for information extraction. InProceedings of the Fifteenth International Joint Conference
on Artiï¬cial Intelligence, IJCAI 97, Nagoya, Japan, August
23-29, 1997, 2 Volumes (1997), pp. 729â€“737.
[25] L
E,V . , AND GULW ANI , S. Flashextract: a framework for
data extraction by examples. In Proceedings of the 35th ACM
SIGPLAN Conference on Programming Language Design
and Implementation (2014), ACM, p. 55.
[26] L EOTTA ,M . ,S TOCCO ,A . ,R ICCA ,F . , AND TONELLA ,P .
Reducing web test cases aging by means of robust xpath
locators. In Proceedings of 25th International Symposium on
Software Reliability Engineering Workshops (ISSREW 2014)(2014), pp. 449â€“454.
[27] L
IU,B . ,G ROSSMAN ,R . , AND ZHAI, Y. Mining data
records in web pages. In Proceedings of the ninth ACM
SIGKDD international conference on Knowledge discoveryand data mining (2003), ACM, pp. 601â€“606.
[28] L
IU,D . ,W ANG ,X . ,Y AN,Z . , AND LI, Q. Robust web data
extraction: a novel approach based on minimum cost script
edit model. In Web Information Systems and Mining .
Springer, 2012, pp. 497â€“509.
[29] L IU,L . ,P U,C . , AND HAN, W. Xwrap: An xml-enabled
wrapper construction system for web information sources. In
Data Engineering, 2000. Proceedings. 16th International
Conference on (2000), IEEE, pp. 611â€“621.
378[30] L IU,W . ,M ENG,X . , AND MENG, W. Vide: A vision-based
approach for deep web data extraction. Knowledge and Data
Engineering, IEEE Transactions on 22, 3 (2010), 447â€“460.
[31] M IAO,G . ,T ATEMURA ,J . ,H SIUNG ,W . - P . ,S AWIRES ,A . ,
AND MOSER , L. E. Extracting data records from the web
using tag path clustering. In Proceedings of the 18th
International Conference on World Wide Web (New York,
NY , USA, 2009), WWW â€™09, ACM, pp. 981â€“990.
[32] M ICHELSON ,M . , AND KNOBLOCK , C. A. Unsupervised
information extraction from unstructured, ungrammatical
data sources on the world wide web. International Journal of
Document Analysis and Recognition (IJDAR) 10 , 3-4 (2007),
211â€“226.
[33] M IKOLOV ,T . ,C HEN,K . ,C ORRADO ,G . , AND DEAN,J .
Efï¬cient estimation of word representations in vector space.arXiv preprint arXiv:1301.3781 (2013).
[34] R
EIS,D . D.C . ,G OLGHER ,P .B . ,S ILV A ,A .S . , AND
LAENDER , A. Automatic web news extraction using tree
edit distance. In Proceedings of the 13th international
conference on World Wide Web (2004), ACM, pp. 502â€“511.
[35] R ENNIE ,J . ,M CCALLUM ,A . , ET AL . Using reinforcement
learning to spider the web efï¬ciently. In ICML (1999),
vol. 99, pp. 335â€“343.
[36] S LEIMAN ,H .A . , AND CORCHUELO , R. Tex: An efï¬cient
and effective unsupervised web information extractor.Knowledge-Based Systems 39 (2013), 109â€“123.
[37] T
HAMVISET ,W . , AND WONGTHANA V ASU , S. Information
extraction for deep web using repetitive subject pattern.World Wide Web (2013), 1â€“31.
[38] T
HAMVISET ,W . , AND WONGTHANA V ASU , S. Information
extraction for deep web using repetitive subject pattern.
World Wide Web 17 , 5 (2014), 1109â€“1139.
[39] V YDISW ARAN ,V .V . , AND SARAW AGI , S. Learning to
extract information from large websites using sequential
models. In COMAD (2005), pp. 3â€“14.
[40] Z HAI,Y . , AND LIU, B. Web data extraction based on partial
tree alignment. In Proceedings of the 14th international
conference on World Wide Web (2005), ACM, pp. 76â€“85.
[41] Z HU,J . ,N IE,Z . ,W EN,J . - R . ,Z HANG ,B . , AND MA,
W.-Y. Simultaneous record detection and attribute labeling
in web data extraction. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge Discoveryand Data Mining (New York, NY , USA, 2006), KDD â€™06,
ACM, pp. 494â€“503.
379