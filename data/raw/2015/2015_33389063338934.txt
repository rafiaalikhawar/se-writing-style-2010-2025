Target-Driven Compositional Concolic Testing with Function
Summary Refinement for Effective Bug Detection
Yunho Kim
KAIST
Daejeon, South Korea
yunho.kim03@gmail.comShin Hong
Handong Global University
Pohang, South Korea
hongshin@handong.eduMoonzoo Kim
KAIST
Daejeon, South Korea
moonzoo.kim@gmail.com
ABSTRACT
Concolic testing is popular in unit testing because it can detect
bugs quickly in a relatively small search space. But, in system-level
testing, it suffers from the symbolic path explosion and often misses
bugs. To resolve this problem, we have developed a focused composi-
tional concolic testing technique, FOCAL, for effective bug detection.
Focusing on a target unit failure v(a crash or an assert violation)
detected by concolic unit testing, FOCAL generates a system-level
test input that validates v. This test input is obtained by building
and solving symbolic path formulas that represent system-level
executions raising v. FOCAL builds such formulas by combining
function summaries one by one backward from a function that
raisedvtomain . If a function summary ϕaof function aconflicts
with the summaries of the other functions, FOCAL refines ϕatoϕ′a
by applying a refining constraint learned from the conflict. FOCAL
showed high system-level bug detection ability by detecting 71
out of the 100 real-world target bugs in the SIR benchmark, while
other relevant cutting edge techniques (i.e., AFL-fast, KATCH, Mix-
CCBSE) detected at most 40 bugs. Also, FOCAL detected 13 new
crash bugs in popular file parsing programs.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging .
KEYWORDS
Automated test generation, target-driven compositional concolic
testing, function summary refinement, Craig interpolant, dynamic
symbolic execution
ACM Reference Format:
Yunho Kim, Shin Hong, and Moonzoo Kim. 2019. Target-Driven Composi-
tional Concolic Testing with Function Summary Refinement for Effective
Bug Detection. In Proceedings of the 27th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software En-
gineering (ESEC/FSE ’19), August 26–30, 2019, Tallinn, Estonia. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3338906.3338934
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
©2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5572-8/19/08. . . $15.00
https://doi.org/10.1145/3338906.33389341 INTRODUCTION
Concolic testing and software model checking have been popular in
unit-level [ 8,15,21–23,26,27,35,39,40] (i.e., exploring a function
fwith symbolic unit driver/stubs/environments after separating
ffrom an entire target program P) because they can detect bugs
quickly in a relatively small search space [ 26]. However, concolic
unit testing has a weakness, in that most of the detected failures
infare false alarms raised by infeasible unit executions (i.e., unit
executions that are infeasible at system-level). This is because con-
colic unit testing uses approximate symbolic driver/stubs which
do not accurately represent the real context(s) of finP. Note that
false alarms are serious obstacles for unit testing [ 13,17,37]. In
contrast, system-level concolic testing does not suffer from false
alarms because it generates concrete system-level test inputs that
execute Pfrommain . However, system-level concolic testing often
fails to detect bugs due to a huge symbolic path space.
To resolve these weaknesses of unit testing and system testing,
we have developed FOcused CompositionAL concolic testing (FOCAL).
Instead of exploring a huge symbolic search space from scratch
to detect failures, FOCAL identifies target failures quickly by using
concolic unit testing and to focus on generating system-level test
inputs that validate the failures. Thus, it can detect many bugs in a
limited time without false alarms. FOCAL operates as follows:
1.Identifying target failures v:
FOCAL applies concolic unit testing to every function in Pand
checks whether a failure occurs (i.e., a crash or an assert viola-
tion). This unit-level concolic testing can identify many more
failures (although many of them are false ones) than system-level
concolic testing in a limited testing time.
2.Generating a system-level test input that validates the identified
target failures v:
1)For each function a, FOCAL builds a function summary (FS)
ϕawhich is a disjunction of the explored symbolic paths (i.e.,
a under-approximate FS).
2)FOCAL tries to construct a validating system-level symbolic
path formula Φvwhose solution is a system-level test input
that validates v. Suppose that a static function call-graph of
Phas a call-chain from main toa1where a1calls fvwhich
raisedvin concolic unit testing (i.e., ⟨main ,an, ...,a2,a1⟩such
thatmain calls an,ancalls an−1, and so on).
•FOCAL obtains Φvby combining the summaries of the func-
tions in⟨main ,an, ...,a2,a1⟩and the unit executions of fv
that raisev(calling itψv) one by one backward (i.e., com-
biningψvwithϕa1first, and then with ϕa2, and so on).
•If an intermediate symbolic path formula (SPF) generated
by combining ψvwith the summaries of functions in the
call-chain⟨ak−1, ...,a1⟩is satisfiable, but that of a grown
16
ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Yunho Kim, Shin Hong, and Moonzoo Kim
call-chain
ak,ak−1, ...,a1withψvis unsatisfiable, ϕak
conflicts with the combined summaries of the functions in
⟨ak−1, ...,a1⟩andψv. Then, FOCAL refinesϕaktoϕ′akto
resolve the conflict by applying a refining constraint which
is learned from the conflict by using an SMT solver (i.e.,
Craig interpolants in Sect. 3.5.4). Note that this target-driven
refinement of under-approximate FSes using the Craig in-
terpolants is a new technique and crucial in generating a
system-level test input for v(Sect. 3.5.4 and Sect. 6.1).
We performed experiments on the SIR benchmark programs and
case studies to detect new crash bugs in real-world file parsing
programs. The experiments showed that FOCAL achieved high
system-level bug detection ability: it detected 71 out of the 100 real-
world target bugs while relevant cutting-edge testing techniques
(i.e., AFL-fast, KATCH, Mix-CCBSE) detected only 40, 34, and 25
bugs, respectively (Sect. 5.1). Also, FOCAL successfully detected 13
new crash bugs in popular file parsing programs (Sect. 5.4).
The contributions of this paper are as follows:
•FOCAL is the first technique that detects bugs effectively in a
limited testing time by combining the advantages of concolic
unit testing (i.e., quick target failure identification) and system-
level concolic testing (i.e., validating target failures without false
alarms). Without exploring a huge symbolic search space from
scratch, it focuses on generating system-level test inputs that
validate the target failures identified by concolic unit testing.
•We have developed the following techniques to construct Φv
effectively and efficiently by composing FSes:
•Construction of a realistic FS of a function aibased on ai’s
extended unit , which provides realistic contexts to ai(Sect. 3.3).
Extended units can reduce false target failures as well as non-
validating SPFs (i.e., satisfiable SPFs whose solutions do not val-
idate the target failures) and increase validating SPFs (Sect. 6.2).
•Target-driven refinement of under-approximate FSes using
Craig interpolants to guide concolic testing to construct more
validating SPFs (Sect. 3.5.4 and Sect. 6.1).
•We performed systematic empirical evaluations of the bug de-
tection ability of FOCAL and relevant cutting edge testing tech-
niques (i.e., AFL-fast, KATCH, Mix-CCBSE and several variants
of FOCAL) on the SIR C programs. In the experiments, FOCAL
detected 71 out of the 100 target bugs without any false alarms,
while the other techniques detected at most 40 (Sect. 4– 5).
•FOCAL detected 13 new crash bugs in 12 popular file parsing pro-
grams. These were reported with the crashing system test inputs
generated by FOCAL to the original developers and confirmed
by the developers (Sect. 5.4).
•We made 100 real-world bug data for the SIR benchmark pro-
grams publicly available (https://sites.google.com/view/focal-
fse19). These data were collected and organized after examining
the bug reports of the last 12–24 years, so that researchers can
use them for various testing research purposes (Sect. 4.2.1).
The paper is organized as follows. Sect. 2 shows an illustrating
example. Sect. 3 describes details of FOCAL. Sect. 4 explains the
experiment setup used to evaluate FOCAL for comparison with
other techniques. Sect. 5 reports the experimental results. Sect. 6 dis-
cusses observations from the experiments. Sect. 7 discusses related
work. Finally, Sect. 8 concludes the paper with future work.
Figure 1: Example target program
2 ILLUSTRATING EXAMPLE
We explain how FOCAL generates a system-level input that fails
through an example (three functions main ,f, andg, in Fig. 1).
Step 1. Identifying a target failure line ving:
In Fig. 1, FOCAL identifies Line 22 in gas a failure line by concolic
unit testing (i.e., vis Line 22 and fv=g). The SPF of the unit
execution of fvthat fails at v,ψv, is (*s=‘C’)∧(*(s+1) ,‘\0’ ).
Step 2. Generating summaries of functions in P:
FOCAL generates a FS of every function in Pusing concolic unit
testing. Suppose that, during the concolic unit testing of fin a
limited testing time, salways starts with ‘ B’ (i.e.,∗s=‘B’at Line
12). Then, the FS of f,ϕf, will be as follows:
ϕf=((∗s=‘B′)∧(∗(s+1)=‘\0′))
∨ ((∗s=‘B′)∧(∗(s+1),‘\0′)∧ . . .∧(∗(s+2)=‘\0′))
∨ . . .
Step 3. Constructing a system-level symbolic path formula
that validates v:
To construct a system-level symbolic path formula (SPF) from a
target function that raises a failure at v(i.e.,g) tomain , FOCAL
selects one of the g’s callers and combines ψvwith the FS of the
selected caller. Among multiple callers, FOCAL first chooses a caller
having the highest function relevance with g(the function relevance
is given as a label between function nodes in Fig. 1). Among the
two callers of g(i.e.,fandmain ), FOCAL first chooses fbecauseg
has a higher function relevance with f(i.e., 0.7) than main (0.5).
Oncefis chosen, FOCAL conjoins ψv(Step 1) and ϕf(Step 2)
and finds that ϕf∧ψvis unsatisfiable because ( *s=‘B’) inϕf
conflicts with ψv=(*s=‘C’)∧(*(s+1) ,‘\0’ ).
To refineϕf, first FOCAL obtains a Craig interpolant Iofψvand
ϕf(i.e.,I:=(*s=‘C’)) using Z3.1Then, it inserts assume(I)at
the beginning of f(i.e., at the end of Line 11) as a refining constraint,
1I:= (*s=‘C’) is a Craig interpolant of ψvandϕf(see Corollary 1) because
• |=ψv→I (i.e.,|=((*s=‘C’)∧(*(s+1) ,‘\0’ ))→(*s=‘C’), and
• I∧ϕfis unsatisfiable (i.e., ( *s=‘C’)∧(((*s=‘B’)∧(*(s+1) =‘\0’ ))∨... ))
17Target-Driven Compositional Concolic Testing with Function Summary Refinement for Effective Bug Detection ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
Figure 2: An extended unit of f5and two failure contexts
⟨main ,f1,f3⟩and⟨main ,f2,f3⟩in a static call-graph of P. We
assume that f5has a target failure line v(i.e., fv=f5).
whereassume( exp)immediately terminates a current execution if
expis false. By re-running concolic unit testing of fwithassume(*s
==‘C’) at Line 11, FOCAL obtains a refined FS ϕ′
f:= (*s=‘C’).
Now,ϕ′
f∧ψv= (*s=‘C’)∧((*s=‘C’)∧(*(s+1) ,‘\0’ ))
becomes satisfiable. Then, FOCAL continues to construct a system-
level SPF for the failure by conjoining ϕmain withϕ′
f∧ψv. Finally, if
the final SPF Φv(=ϕmain∧ϕ′
f∧ψv) is satisfiable, FOCAL generates
a system-level input (i.e., a solution of Φv) that crashes the program
at Line 22.
IfΦvis not satisfiable, FOCAL tries to refine ϕmain intoϕ′
main
then it checks if Φ′v(=ϕ′
main∧ϕ′
f∧ψv) is satisfiable. If Φ′vis
not satisfiable, FOCAL tries to build a different validating SPF by
backtracking to combine ψvandϕmain (instead ofϕf) and checks if
ϕmain∧ψvis unsatisfiable (it refines ϕmain intoϕ′′
mainif necessary).
Ifϕmain∧ψv(andϕ′′
main∧ψv) is still unsatisfiable, FOCAL tried
all possible call-chains from main tog, but failed to generate a test
input to validate the target failure vat Line 22.
3 FOCUSED COMPOSITIONAL CONCOLIC
TESTING TECHNIQUE (FOCAL)
This section explains how FOCAL operates using a target program
Pin Fig. 2 as an example. Fig. 2 shows a static call-graph of Pwhich
consists of 11 functions (i.e., main ,f1, ..., and f10) including the
program entry function main .
3.1 Overview
Fig. 3 shows the overall process of FOCAL. FOCAL takes as inputs
a target program Pand a set of system-level seed test inputs Tand
generates system-level test inputs that make Pfail. FOCAL operates
in the following four phases (see Fig. 3):
1. Measuring function relevance (Sect. 3.2):
First, FOCAL generates system-level test inputs by fuzzing the
given system-level seed test inputs. Then, from the function
call profile obtained by executing the fuzzed system tests, it
measures relevance between every pair of the functions in P.
This information is used for target failure line identification, FS
generation, and system-level SPF construction.
2. Identification of a target failure line v(Sect. 3.3):To identify a target failure line v(i.e., a line of program code
where a crash or an assert violation occurs), FOCAL applies
concolic unit testing to every function in P. We call a function
that hasvasfv.
For example, FOCAL applies concolic unit testing to each of main ,
f1, ..., and f10in Fig. 2 separately. Suppose that f5is written in
Lines 30–50 and f5crashes at Line 40 during concolic unit testing
off5. Then, we set Line 40 as a target failure line vandfv=f5.
3. Construction of function summaries (Sect. 3.4):
For each function ainP, FOCAL builds a FS ϕawhich is a dis-
junction of SPFs explored by concolic unit testing (i.e., an under-
approximate FS).
4.Construction of system-level SPFs to validate a failure at v(Sect. 3.5):
To validate a failure at vin system-level (i.e., generating a test
input that makes Pfail atv), FOCAL builds SPFs by combining
unit failure executions ψv(i.e., a set of unit executions of fvthat
fail atv) and the summaries of the functions in v’sfailure-context
(e.g.,⟨ak, ...,a1⟩which is a call-chain to a1that calls fv).
For example of Fig. 2, suppose that concolic unit testing of f5
crashes at Line 40 in f5(i.e.,fv=f5). Then, FOCAL builds SPFs by
combining ψvand the summaries of the functions in the failure-
context of v(e.g.,⟨f1⟩,⟨main ,f1⟩,⟨f3⟩,⟨f1,f3⟩,⟨main ,f1,f3⟩,
⟨f2,f3⟩, and⟨main ,f2,f3⟩) until it constructs a validating SPF
whose solution makes Pfail atv.
3.2 Function Relevance Metric
FOCAL computes function relevance metric using the conditional
probability based on the function call profiles observed from sys-
tem test executions. To obtain accurate function relevance, FOCAL
generates a large number of system test inputs by fuzzing a given
set of system-level seed test inputs T. FOCAL considers that fand
дare highly relevant if it frequently observes that fcallsд(immedi-
ately or transitively) (denoted by f→д) orдcalls fin system test
executions. Intuitively speaking, if caller-callee functions execute
together frequently, they closely interact with each other, which
means that they are highly relevant to each other.
We measure the relevance between fandд(denoted by r(f,д))
as(x+y)
2such that
•xisp((f→дorд→f)|f)which is calculated byw
zwhere
•wis the number of the system test executions where f→дor
д→foccurs
•zis the number of the system test executions where foccurs
•yisp((f→дorд→f)|д)
Ex. Suppose that we have the following test executions in Fig. 2:
•t1={main→f1,f1→f5,f5→f7}
•t2={main→f1,f1→f5,f5→f8,f8→f9}
•t3={main→f1,f1→f5,f5→f7,f5→f8,f8→f9,f8→f10}
•t4={main→f1,f1→f3,f3→f6,f6→f10}
In this example, r(f5,f7)=0.83 (=(2
3+1)/2because p(f5→f7|f5)=2
3
andp(f5→f7|f7)=1) and r(f5,f10)= 0.42 (= (1
3+1
2)/2because
p(f5→f10|f5)=1
3andp(f5→f10|f10)=1
2).
3.3 Identification of a Target Failure Line v
FOCAL applies concolic unit testing to each function ainPand
identifies a target line vinaifafails. To reduce false target lines
18ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Yunho Kim, Shin Hong, and Moonzoo Kim
Figure 3: Focused Compositional Concolic Testing (FOCAL)
caused by infeasible unit executions, FOCAL applies concolic unit
testing to an extended unit ofa(denoted by E(a)) [23].E(a)consists
ofa,a’sclosely relevant callee functions b1...bn(i.e., functions reach-
able from ain a static call-graph), and symbolic stubs to replace
callee functions of athat are not closely relevant to a(symbolic
stubs return unconstrained symbolic-values).
Note that concolic unit testing E(a)(instead of aalone) can
reduce false alarms by removing infeasible unit executions of a
because a’s closely related functions can provide realistic environ-
ment to a. The relevance between functions aandb(denoted by
r(a,b)) is measured based on how frequently acalls bor vice versa,
among system test executions (Sect. 3.2). Note that E(a)should only
contain functions b1...bnwhich are closely relevant to a(i.e.,r(a,b)
is among the top 30% of the relevancies between all function pairs
inP) since including more functions will enlarge symbolic path
space and degrade unit testing effectiveness and efficiency.
For example of Fig. 2, suppose that E(f5)is {f5,f7,f8,f9} because
r(f5,f7) is high (i.e., among the top 30% of the relevancies between
all function pairs in P). Similarly, suppose that r(f5,f8)andr(f5,f9)
are also high, but not r(f5,f10). When FOCAL applies concolic unit
testing to f5, it explores E(f5)consisting of{f5,f7,f8,f9}.
3.4 Function Summary Construction
FOCAL builds a FS of every function aofPas a disjunction of all
explored SPFs σis onE(a)(i.e.,ϕadef=Ôσi) during the target failure
line identification process (Sect. 3.3). This approach of building
FSes is applicable for complex real-world programs with nested
loops, external binary libraries, complex pointer arithmetic, etc.
The syntax and semantics of FS follow QF_BV in SMTLIB2.
To focus on exploring diverse behaviors of ain a given time bud-
get, FOCAL applies concolic unit testing to E(a)using a weighted
random negation search strategy. This search strategy randomly
negates a branch in a current symbolic path while giving four times
higher chance to the branches in athan the branches in the other
functions in E(a).
3.5 Construction of System-level SPFs to
Validate a Failure at v
3.5.1 Preliminaries.
•fvis the function which has a target failure line v.
•Cvis a set of functions that directly call fv. For example of Fig. 2,
fv=f5andCv={f1,f3}.•ψvis a set of failure executions in fv(i.e., a disjunction of the
SPFs of E(fv)(generated by concolic unit testing) that fail at v).
•ϕadenotes a FS of a function a.ϕais a disjunction of the SPFs of
E(a)(generated by concolic unit testing) (i.e., ϕa=Ôσiwhere
σiis a SPF of E(a)).
•Afailure-context Sv=⟨ak, ...,a2,a1⟩of a target failure line v
is a call-chain/path in a static call-graph of Psuch that akcalls
ak−1,ak−1calls ak−2and so on and a1∈Cv. For example, the
failure-contexts of vin Fig. 2 are⟨f1⟩,⟨main ,f1⟩,⟨f3⟩,⟨f1,f3⟩,
⟨main ,f1,f3⟩,⟨f2,f3⟩, and⟨main ,f2,f3⟩.
•Slice(ϕai+1,ai)is a sliced formula of ϕai+1with regard to the
invocation of a function ai(i.e., forϕai+1=Ôσj,Slice(ϕai+1,ai)
=Ôσ′
jwhereσ′
jis a prefix of σjonly up to an invocation of ai).
•Φv(⟨a⟩)fora∈Cvdenotes a combined SPF of ψvand the FS of
athat directly calls fv(i.e.,Φv(⟨a⟩)=Slice(ϕa,fv)∧ψv).
•For a failure-context Skv=⟨ak, ...,a1⟩,Φv(Skv)denotes a com-
bined SPF of ψvandψv’s symbolic calling context formula which
is the combined sliced summaries of the functions in Svin a back-
ward order (i.e., Φv(Skv)=Slice(ϕak,ak−1)∧Φv(⟨ak−1, ...,a1⟩)).
Combined FSes capture effects on visible variables, parameters,
return-values in SSA form (i.e., all variables in FSes are expressed
as expressions over the symbolic input-variables).
For example of Fig. 2,
Φv(⟨main ,f1,f3⟩)=Slice(ϕmain ,f1)∧Φv(⟨f1,f3⟩)
=Slice(ϕmain ,f1)∧Slice(ϕf1,f3)∧Φv(⟨f3⟩)
=Slice(ϕmain ,f1)∧Slice(ϕf1,f3)∧Slice(ϕf3,f5)∧ψv
3.5.2 Strategies for Function Summary Composition. To generate
SPFs that validate a failure at vquickly, FOCAL uses function rel-
evance metric to select a FS to combine as follows (i.e., giving a
high priority to a function which has high relevance with a most re-
cently combined function). Suppose that FOCAL has built Φv(Sk−1v)
where the failure-context Sk−1vis⟨ak−1, ...,a1⟩andak−1is called
byb1,...,bm. FOCAL selects biwhose relevance with ak−1is the
highest and combines ϕbiwithΦv(Sk−1v). If FOCAL fails to gener-
ateΦvafter selecting ϕbito combine with Φv(Sk−1v), it backtracks
to select and combine bi′which has the second highest relevance
with ak−1and so on.
This function relevance-based FS composition can be effective
because, if bis more relevant with ak−1than b′, it will be easier to
refineϕbto be compatible with ak−1thanϕb′because bandak−1
share more common contexts than b′andak−1(i.e.,ϕbmay need
less refinement steps to become compatible with ak−1thanϕb′).
19Target-Driven Compositional Concolic Testing with Function Summary Refinement for Effective Bug Detection ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
Figure 4: FSR using the Craig interpolants
3.5.3 Generation of Symbolic Path Formulas by Combining Func-
tion Summaries. To generate a SPF Φvto validate a failure at v, FO-
CAL combines the summaries of the functions in a failure-context
⟨main , ...,a1⟩ofvandψvin a backward order. If the combined SPF
is satisfiable, FOCAL uses a solution of the formula obtained by an
SMT solver as a system-level test input to validate a failure at v.
For Fig. 2 where fv=f5, FOCAL generates SPFs as follows:
1.Suppose that r(f3,f5)>r(f1,f5). FOCAL generates Φv(⟨f3⟩)=
Slice(ϕf3,f5)∧ψv. IfΦv(⟨f3⟩)is satisfiable and r(f1,f3)>r(f2,f3),
FOCAL increases a failure-context of vto⟨f1,f3⟩
2.It generates Φv(⟨f1,f3⟩)=Slice(ϕf1,f3)∧Φv(⟨f3⟩)and checks if
Φv(⟨f1,f3⟩)is satisfiable. If yes, FOCAL increase a failure-context
ofvto⟨main ,f1,f3⟩.
3.Finally, if Φv(⟨main ,f1,f3⟩)=Slice(ϕmain,f1)∧Φv(⟨f1,f3⟩)is
satisfiable, FOCAL obtains a solution of Φv(⟨main ,f1,f3⟩)by
using a SMT solver and uses the solution as a system-level test
input to validate a failure at v.
Meanwhile, a combined SPF may be unsatisfiable if a FS conflicts
with the other FSes. For example, suppose that akcalls ak−1in
a failure-context of v(i.e., Sk−1v=⟨ak−1, ...,a1⟩) andϕakdoes
not contain a symbolic path that provides a context necessary for
Φv(Sk−1v)to invoke a failure at v. Then, the combined formula
Slice(ϕak,ak−1)∧Φv(Sk−1v)will be unsatisfiable. In such cases,
FOCAL refines ϕakas shown in Sect. 3.5.4.
3.5.4 Function Summary Refinement (FSR). Fig. 4 shows how FO-
CAL refines a FS. Suppose that FOCAL combined ϕakandΦv(Sk−1v)
where a failure-context Sk−1vis⟨ak−1, ...,a1⟩,akcalls ak−1and
Φv(Sk−1v)is satisfiable. If the combined formula is unsatisfiable due
to the conflict between ϕakandΦv(Sk−1v), to continue construction
of SPFs to validate a failure at v, FOCAL refines ϕakintoϕ′ak. It
buildsϕ′akusing concolic testing2onE(ak)with a Craig inter-
polant of Φv(Sk−1v)andSlice(ϕak,ak−1)as a refining constraint.
Craig interpolation theorem is given as follows:
2To build a refined FS quickly, FOCAL extends CFG search heuristic [ 7] to guide the
search to reach the lines where akcallsak−1quickly.Theorem 1 (Craig, 1957 [ 9]).Suppose A→Cis a valid impli-
cation in first-order logic (i.e., |=A→C). Then, there is a Craig
interpolantIsuch that|=A→I and|=I→ C.
Corollary 1. Suppose that A∧Bis unsatisfiable in first-order
logic (i.e.,|=A→¬ B). Then, by Thm. 1, there is a Craig interpolant
Isuch that|=A→I andI∧Bis unsatisfiable.3
Suppose that AisΦv(Sk−1v),BisSlice(ϕak,ak−1), andΦv(Sk−1v)∧
Slice(ϕak,ak−1)is unsatisfiable. Then, by Corollary 1, there exists
a Craig interpolant IofΦv(Sk−1v)andSlice(ϕak,ak−1)such that
I∧Slice(ϕak,ak−1)is unsatisfiable. Note that Slice(ϕak,ak−1)rep-
resents the already explored paths in ak. Thus, Craig interpolant I
can work as a guide in concolic unit testing of akto avoid revisiting
already explored paths (i.e., I→¬ Slice(ϕak,ak−1)). And at the
same time,Ican lead the concolic unit testing to explore paths
compatible with Φv(Sk−1v)(i.e.,Φv(Sk−1v)→I ).
Now we propose the following heuristic to build ϕ′aksuch that
Φv(⟨ak, ...,a1⟩)is satisfiable.
•When FOCAL generates ϕ′akusing concolic testing, FOCAL en-
forces a Craig interpolant IofΦv(Sk−1v)andSlice(ϕak,ak−1)as
a FS refining constraint so that ϕ′akcan be different from ϕak
(and, thus, ϕ′akmay not conflict with Φv(Sk−1v)).
This strategy constructs a new FS ϕ′akthat can be compatible with
Φv(Sk−1v). This is becauseIguides concolic testing to make ϕ′ak
contain symbolic paths different from the ones in ϕakby pruning
Slice(ϕak,ak−1)(becauseI∧Slice(ϕak,ak−1)is unsatisfiable).
FOCAL implements this strategy by inserting assume(I)at the
beginning of the body of ak,which guides concolic execution to
explore paths that satisfy Iby terminating an execution of ak
immediately ifIis violated.
Suppose that ϕ′akdoes not resolve the conflict in the first function
summary refinement step. We call the Craig interpolant used in this
first function summary refinement step as I1and the first refined
FS asϕ1ak(=ϕ′ak). Then FOCAL obtains the second interpolant
I2=I(Φv(Sk−1v),Slice(ϕ1ak,ak−1)). Then, it builds a new refined
FSϕ2akusingI2∧I1as a new refining constraint and checks
whetherϕ2akresolves the conflict. If not, this step repeats until a
newly refined FS does not increase branch coverage of akthree
times in a row (i.e., until this step does not explore new search
space of akmuch) or the conflict is resolved.
3.5.5 Example of Constructing Symbolic Path Formulas to Validate
a failure at vin Fig. 2. Suppose that fv=f5andr(f5,f3)=0.9,
r(f5,f1)=0.8,r(f3,f1)=0.7, and r(f3,f2)=0.6.
First, FOCAL selects f3to build Φv(⟨f3⟩)by combining ϕf3and
ψvbecause r(f5,f3)>r(f5,f1). Suppose that Φv(⟨f3⟩)is satisfiable.
Then, FOCAL continues to select f1(because r(f3,f1)>r(f3,f2))
to combine ϕf1andΦv(⟨f3⟩)and obtains Φv(⟨f1,f3⟩). Suppose that
Φv(⟨f1,f3⟩)is unsatisfiable. Then, FOCAL refines ϕf1intoϕ′
f1by
using a Craig interpolant I(Φv(⟨f3⟩),Slice(ϕf1,f3)). Suppose that
Φ′v(⟨f1,f3⟩)=Slice(ϕ′
f1,f3)∧Φv(⟨f3⟩)is satisfiable. Then, FOCAL
3Corollary 1 is just another form of Theorem 1. |=A→Cin Thm 1 is equivalent
to that A∧Bis unsatisfiable in Cor 1, because if we replace Cin Thm 1 with¬Bin
Cor 1, then|=A→¬B≡|=¬A∨¬B≡|=¬(A∧B). Also,|=I→ Cin Thm 1 is
equivalent to|=I→¬ Bin Cor 1 which indicates that I∧Bis unsatisfiable because
|=I→¬ B≡|=¬I∨¬ B≡|=¬(I∧ B).
20ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Yunho Kim, Shin Hong, and Moonzoo Kim
Table 1: Programs of the known crash bug benchmark
Target Lines # of # of seed Branch Func # of target
programs func. sys. TCs cov.(%) cov. (%) bugs
Bash-2.0 32714 1214 1100 46.2 89.0 39
Flex-2.4.3 7471 147 567 45.7 93.9 5
Grep-2.0 5956 132 809 50.3 94.7 6
Gzip-1.0.7 3054 82 214 55.8 87.8 5
Make-3.75 28715 555 1043 64.5 87.9 10
Sed-1.17 4085 73 360 47.3 87.7 3
Vim-5.0 66209 1749 975 35.8 91.0 32
Sum 148204 3952 5068 N/A N/A 100
Average 21172.0 564.6 724.0 49.4 90.3 14.3
finally combines ϕmain andΦ′v(⟨f1,f3⟩)to build Φv(⟨main ,f1,f3⟩)
(see a dotted circle on a failure-context ⟨main ,f1,f3⟩in the middle
of Fig.2). However, suppose that Φv(⟨main ,f1,f3⟩)is unsatisfiable
and FOCAL fails to refine ϕmain to be compatible with Φ′v(⟨f1,f3⟩).
Then, FOCAL backtracks to f3and combines f2(instead of f1)
withΦv(⟨f3⟩)to build Φv(⟨f2,f3⟩). Suppose that Φv(⟨f2,f3⟩)is
satisfiable. Then, FOCAL continues to build Φv(⟨main ,f2,f3⟩)(see
a dotted circle on a failure-context ⟨main ,f2,f3⟩in the right part
of Fig.2). If Φv(⟨main ,f2,f3⟩)is satisfiable, FOCAL generates a so-
lution to the formula and uses the solution as a system-level test
input to validate a failure at v.
4 EXPERIMENT SETUP
4.1 Research Questions
RQ1. Bug detection ability : How many target bugs does FOCAL
detect, compared to fuzzing (AFL-fast [ 5]) and the guided concolic
testing techniques (KATCH [31] and Mix-CCBSE [30])?
RQ2. Effect of the Craig interpolants in FSR : How much does
the Craig interpolants in function summary refinement (FSR) affect
the number of the detected bugs and the execution time to build
SPFs?
RQ3. Effect of the extended units for bug detection and exe-
cution time : How much do the extended units affect FOCAL’s
number of the bugs detected and execution time, compared to
FOCAL without the extended units (FOCAL−E) and FOCAL with
randomly built extended units (FOCALR)?
RQ4. New crash bug detection : How many new bugs does FO-
CAL detect, compared to AFL-fast, KATCH, and Mix-CCBSE?
4.2 Target Bugs
To measure bug detection ability of the techniques, we target crash
bugs (although FOCAL can detect non-crash bugs if test oracles are
provided as assertions) because they (e.g., null-pointer dereference,
divide-by-zero, buffer overflow) cause serious reliability and secu-
rity problems and can be detected with automatically generated
assertions.
FOCAL automatically inserts assertions (e.g., assert(ptr!=NULL) )
to detect crash bugs in the target programs.
We use two sets of target programs: known crash bug benchmark
for RQ1-3 and new crash bug benchmark for RQ4.4.2.1 Known Crash Bug Benchmark. We collected 100 real-world
crash bugs of the seven SIR C programs (shown in Table 1) that are
larger than 1 KLoC and were fixed by the original developers from
Dec 1996 to July 2018. Each program is the same version of the
programs in SIR [ 12] because they are widely used for the software
testing research.
From the revision histories, we collected bugs such that (1) the
bug report shows that the bug crashes the program, (2) the original
developers confirmed the bug report and released a patch to fix the
bug, and (3) the bug exists at the version chosen for the benchmark
(i.e., the same version in SIR). We collected total 19,108 bug-fix
commits of the target programs. Then, we extracted 587 crash bug-
fix commits by searching keywords like “overflow”, “segfault”, etc.
We manually analyzed the changed code and commit logs of the
587 crash bug-fix commits and identified 100 crash bugs.
We consider a program line lbas a faulty line of a bug biflbis
included in the patch (i.e., the bug-fix commits for b). We did not
use any artificially inserted bugs in SIR.
4.2.2 New Crash Bug Benchmark. To evaluate the effectiveness of
FOCAL for discovering new crash bugs, we target the popular C
programs that parse regular expression, XML and JSON [ 6]. We
choose the latest versions of the target text parsing C programs
as of July 2018. The programs consist of 7243.8 LoC and 272.3
functions on average (details of the new crash bug target programs
are available at https://sites.google.com/view/focal-fse19). The text
parsing libraries are widely used in various software including
server applications and smartphone apps and the crash bugs in
these libraries can cause severe reliability and security problems.
4.3 FOCAL Setup
4.3.1 Fuzzing . To compute function relevance from diverse sys-
tem behaviors, FOCAL applied the AFL-fast fuzzer [ 5] to generate
various system test inputs. Using all system tests provided in a
target program as seed test inputs, FOCAL ran AFL-fast for 1 hour
(no target bug detected).
For known crash bug target programs, it generated 24,300 system
test inputs that executed previously unexplored execution paths on
average per program (achieving 79.3% branch coverage).4For new
crash bug target programs, it generated 33,300 system test inputs
that executed previously unexplored execution paths on average
per program (achieving 90.3% branch coverage).
4.3.2 Construction of Extended Units. For each function a, FOCAL
constructs an extended unit of a(i.e., E(a)) based on the function
relevance between aanda’s (immediate or transitive) callee func-
tionb(i.e.,r(a,b)). Ifr(a,b)is in the top 30% of the relevancies of all
pairs of functions in P(i.e.,bis closely relevant to a),bis included
inE(a); if not, bis not included in E(a)and replaced by a symbolic
stub function.
4.3.3 Timeout of Concolic Unit Testing. For concolic unit testing for
target failure line identification (Sect. 3.3), FS construction (Sect. 3.4),
4The quality of the seed test inputs does not affect bug detection ability of FOCAL
much because it uses diverse test inputs generated by fuzzing the seed test inputs.
For example, when we randomly selected and used only 10% of the seed test inputs
(achieving branch coverage 28.3% on average) per program, FOCAL still detected 69
out of 100 target bugs.
21Target-Driven Compositional Concolic Testing with Function Summary Refinement for Effective Bug Detection ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
and FSR (Sect. 3.5.4), we set ten minutes timeout for each function
inP.
4.3.4 Implementation. We have implemented FOCAL and its vari-
ants in 7,800 lines of C++ code using Clang/LLVM-4.0 [ 29]. FOCAL
uses AFL-fast [ 5] for fuzzing, CROWN [ 25] for concolic testing and
Z3 [10] for solving SMT constraints and computing Craig inter-
polants.
4.4 Automated Test Generation Techniques to
Compare
We have evaluated FOCAL and the following testing techniques:
•Fuzzing technique (AFL-fast [ 5]): AFL-fast guides the search-based
fuzzing to cover rarely explored code locations. We used all
system test inputs generated to compute function relevance as
seed test inputs for AFL-fast. We set the timeout of AFL-fast as
the same amount of the total execution time of FOCAL.
•Directed concolic testing techniques (KATCH and Mix-CCBSE) :
KATCH [ 31] takes a program, a patch, and a set of regression tests
to generate test inputs to cover the code locations changed by the
patch. To guide KATCH to execute the target failure line viden-
tified by concolic unit testing of FOCAL, we make a patch that
adds a crash assertion at vto a target program. Mix-CCBSE [ 30]
takes a program and a target line as inputs and performs concolic
testing to cover the target code lines. We give each of the target
failure lines videntified by concolic unit testing of FOCAL to
Mix-CCBSE as the target line. We implemented our own pro-
totype of Mix-CCBSE on KLEE 1.4 (in 600 lines of C++ code)
since we could not use the Mix-CCBSE implementation due to
technical problems (the implementation has not been maintained
since 2013).
We set the timeout of KATCH and Mix-CCBSE for each target
failure line vas the same amount of the execution time spent
for the most time-consuming target failure line of Pby FOCAL.
For example, if FOCAL spends one hour to validate the most
time-consuming target failure line in P, we give one hour to
KATCH and Mix-CCBSE for each target failure line in P.
•FOCAL−I:it is a variant of FOCAL that performs FSR without the
Craig interpolants. For fair comparison with FOCAL, FOCAL−I
builds a refined FS in 90 minutes, which is more than the largest
amount of time (87 minutes) spent by repeated FSRs using the
Craig interpolants (Sect. 3.5.4).
•FOCAL−E: it is a variant of FOCAL that does not use extended
units (i.e., concolic unit testing performs on a single function a
with symbolic stubs that replace all callee functions of a).
•FOCALR: it is a variant of FOCAL that uses randomly constructed
extended units (i.e., E(a)contains aandrandomly selected callee
functions of a(with a probability 0.5), and symbolic stubs that
replace the other callee functions of a). For example of Fig. 2,
suppose that FOCALRrandomly adds f8toE(f5)but not f7. Then,
it continues to randomly add f10(a callee of f8) toE(f5)but not
f9. As a result, FOCALRconstructs E(f5)as{f5,f8,f10}.
4.5 Measurement
To reduce the random variance on the experiment, we repeated the
experiments ten times and report the average numbers.4.5.1 Bug Detection. For a known crash bug b, we report that b
isdetected if a technique generates a system-level test input that
makes Preach lb(one of the faulty code lines of b) and then crash at
a target failure line. If one system execution has covered the faulty
lines of multiple target bugs, we manually analyzed the system
execution to identify which bug causes the failure at a target failure
line.
For a new crash bug, we report the number of the target failure
lines where crashes are validated by the generated system test
inputs as the number of detected bugs. This is because we do not
know which bug covers which failure line(s).
4.5.2 Execution Time. We report the execution time of a technique
on a single machine for a fair comparison of FOCAL with other
testing techniques. The execution time of FOCAL and its variants
consists of:
•Fuzzing and function relevance measurement (FZ): one hour
spent to fuzz the seed test inputs (and negligible amount of time
to calculate the function relevance using the fuzzed test inputs)
•Target failure line identification (FLI): time spent by concolic unit
testing each E(a)to identify target failure lines
•Satisfiability check (SC): time spent for checking satisfiability of
constructed SPFs (Sect. 3.5.3)
•Craig interpolant calculation (CC): time spent for computing the
Craig interpolants for FSR
•Function summary refinement (FSR): time spent for running
concolic unit testing to obtain a refined FS
In RQ1 and RQ4, we report the sum of FZ, FLI, SC, CC, and FSR
time as the execution time of FOCAL to compare with AFL-fast,
KATCH, and Mix-CCBSE. In RQ2 to RQ3, we report the sum of SC,
CC, and FSR time because FOCAL and its variants share the same
target failure lines and have the same amount of FZ and FLI time.
4.6 Testbed Setting
Since the experiment scale is large, the experiments were performed
on 30 machines equipped with Intel quad-core i5 4670K (3.4 Ghz)
and 8GB RAM, running Ubuntu 16.04 64 bit version. Each machine
runs four instances of testing processes.
4.7 Threats to Validity
A threat to external validity is the representativeness of our target
programs. We expect that this threat is limited since the target
programs are widely used real-world ones and tested by many
other researchers. Also, the set of target bugs might not be com-
plete because we might fail to extract one from the bug reports
or a target program has an unknown (i.e., not reported) bug. We
expect that this threat is also limited because we did our best to
thoroughly review the bug reports and the target programs are
actively maintained. A threat to internal validity is possible bugs
in the implementations of FOCAL and the other concolic testing
techniques we studied. We extensively tested our implementations
to address this threat.
5 EXPERIMENT RESULTS
This section presents experiment results to answer the research
questions. All detailed data are available at https://sites.google.com/
view/focal-icse19.
22ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Yunho Kim, Shin Hong, and Moonzoo Kim
Table 2: # of the target bugs detected by and the execution
time (hours) on a single machine of AFL-fast, KATCH, Mix-
CCBSE, and FOCAL
AFL-fast KATCH Mix-CCBSE FOCAL
Targets #det. Time(h) #det. Time(h) #det. Time(h) #det. Time(h)
bugs bugs bugs bugs
Bash 11 399.1 10 522.7 8 669.2 25 399.1
Flex 2 125.2 2 164.0 1 176.0 4 125.2
Grep 4 250.8 3 357.8 2 400.1 5 250.8
Gzip 4 112.5 3 181.1 2 227.2 4 112.5
Make 6 294.7 4 347.4 3 479.4 9 294.7
Sed 3 134.6 2 205.4 2 225.3 3 134.6
Vim 10 521.5 10 648.9 7 746.9 21 521.5
Sum 40 1838.3 34 2427.3 25 2924.0 71 1838.3
Avg. 5.7 262.6 4.9 346.8 3.6 417.7 10.1 262.6
5.1 RQ1: Bug Detection Ability
FOCAL showed high bug detection ability. Table 2 shows the num-
bers of the target bugs detected by and the execution time of AFL-
fast, KATCH, Mix-CCBSE, and FOCAL. It shows the total execution
time (in hours) spent on a single machine (for fair comparison be-
tween the techniques). The wall-clock execution time is roughly
1/100 of the reported time (e.g., FOCAL spent 18 hours to perform
all experiment) because the experiment was run on 30 quad core
machines in parallel.
In each run, FOCAL always detected the 71 bugs in 1838.3 hours
on average (262.6 hours on average per program), which consist of
•Fuzzing and function relevance measurement (FZ): 1 hour
•Target failure line identification (FLI): 553.4 hours
•Satisfiability check (SC): 77.8 hours
•Craig interpolant calculation (CC): 155.8 hours
•FS refinement (FSR): 1044.2 hours
In contrast, AFL-fast detected only 40 bugs with the same amount
of time as that of FOCAL. KATCH and Mix-CCBSE detected only
34 and 25 bugs after spending 1.3 and 1.6 times larger amount
of the execution time than FOCAL (i.e., total 2427.3 and 2924.0
hours), respectively. Since KATCH and Mix-CCBSE do not perform
concolic testing in a compositional way, they need to explore large
execution space to guide concolic testing to raise a failure at v. All
bugs detected by these techniques were also detected by FOCAL.
5.2 RQ2: Effect of the Craig Interpolants in FSR
Table 3 shows that the Craig interpolants in the FSR improved bug
detection ability. FOCAL detected 4.4 times more bugs (=71/16)
than FOCAL−I.5Also, the table shows that the Craig interpolants-
based FSR refines FSes effectively in terms of branch coverage (i.e.,
ϕ′acovers a largely different set of branches than ϕa).C(ϕa)is a
branch coverage of aachieved by a set of execution paths in ϕaand
C(ϕa∪ϕ′a)is a branch coverage of aachieved by a set of execution
paths inϕaorϕ′a.6Table 3 shows that FOCAL increases the branch
coverage of each function by 17.3%p ( =C(ϕa∪ϕ′a)−C(ϕa)) by using
FSR with the Craig interpolants (it generates 946.9 interpolants on
5FOCAL−Ialways detected the 19 bugs in each of the 10 runs. The bugs detected by
FOCAL−Iare also detected by FOCAL.
6For FOCAL which repeats the FSR step, ϕ′
ais the final refined FS (Sect. 3.5.4). For
FOCAL−I,ϕ′
ais a FS refined for the same amount of the total refinement time spent
by FOCAL.Table 3: # of the detected target bugs, the time (hours) to
build SPFs, and the effect of FSR of FOCAL−Iand FOCAL
FOCAL−IFOCAL
Tar- # det. Time Branch Cov.(%) #det. Time #I Branch Cov. (%)
gets bugs (h) C(ϕa) C(ϕabugs (h) C(ϕa) C(ϕa
∪ϕ′a) ∪ϕ′a)
Bash 4 145.6 54.1 59.1 25 231.2 1170 54.1 66.8
Flex 1 69.5 55.3 62.0 4 103.7 531 55.3 73.8
Grep 2 129.8 59.1 67.9 5 231.7 1141 59.1 77.9
Gzip 2 61.6 56.5 61.0 4 99.4 571 56.5 72.0
Make 1 134.9 59.8 65.3 9 217.5 1198 59.8 78.7
Sed 1 75.1 52.9 60.1 3 123.0 570 52.9 71.8
Vim 5 195.3 48.2 53.3 21 271.2 1447 48.2 66.0
Sum 16 811.7 N/A N/A 71 1277.8 6628 N/A N/A
Avg. 2.3 116.0 55.1 61.3 10.1 182.5 946.9 55.1 72.4
Table 4: # of the detected bugs and the execution time to
build SPFs of FOCAL−E, FOCALR, and FOCAL.
FOCAL−EFOCALRFOCAL
Targets #det. Time #det. Time #det. Time
bugs (h) bugs (h) bugs (h)
Bash 10 902.9 7.4 238.8 25 231.2
Flex 2 173.4 1.2 97.6 4 103.7
Grep 2 617.4 2.6 314.9 5 231.7
Gzip 2 376.8 2.0 98.9 4 99.4
Make 3 712.9 2.8 331.6 9 217.5
Sed 1 468.3 1.4 203.1 3 123.0
Vim 12 1010.6 10.8 494.5 21 271.2
Sum 32 4262.3 28.2 1779.4 71 1277.8
Avg. 4.6 608.9 4.0 254.2 10.1 182.5
average per program). In contrast, FOCAL−Iincreases the branch
coverage of each function by only 6.2%p.
The execution time of FOCAL−Ito build symbolic path formulas
is shorter than that of FOCAL (i.e., 811.7 vs. 1277.8 which correspond
to SC+CC+FSR) because FSR without Craig interpolants was not
effective in resolving the conflicts and FOCAL−Igenerates much
fewer SPF than FOCAL (Sect. 6.1).
5.3 RQ3: Effect of the Extended Units on Bug
Detection and Execution Time
The experiment results show that utilizing extended units con-
tribute to high bug detection ability because FOCAL detected more
than twice the number of bugs (71 bugs) than FOCAL−E(32 bugs)7
and FOCALR(28.2 bugs). Table 4 shows the numbers of the detected
target bugs and the execution time to build SPFs of these techniques.
Also, FOCAL spent only 1/3 of the time spent by FOCAL−E(i.e.,
1277.8 vs. 4262.3 hours) because FOCAL−Eidentified 4.8 times more
target failure lines than FOCAL (497 and 2402 target failure lines,
respectively). FOCALRidentified 1.7 times more target failure lines
(i.e., 849.4 vs. 497) and spent 1.4 times larger amount of time (i.e.,
1779.4 vs. 1277.8 hours) than FOCAL.
7FOCAL−Ealways detected the 32 bugs in each of the 10 runs. The bugs detected by
FOCAL−Eare also detected by FOCAL.
23Target-Driven Compositional Concolic Testing with Function Summary Refinement for Effective Bug Detection ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
5.4 RQ4: New Crash Bug Detection
FOCAL detected 13 new crash bugs in the 12 target C programs
in 782.5 hours on average8. We have reported the new bugs with
crashing system-level test inputs to the developers of the target
programs. Eight of them were confirmed by the developers and we
have not received a response for the remaining five (we uploaded
all responses in https://sites.google.com/view/focal-fse19).
For example of libxml2-2.9.8 , FOCAL identified 87 target fail-
ure lines and detected two bugs in 124 hours: one buffer overflow
bug (crashing at HTMLparser.c:5408 ) and one null pointer derefer-
ence bug (crashing at xmlregexp.c:4349 ). For example, to validate
the buffer overflow bug of libxml2-2.9.8 , FOCAL generates a
pair of command line options ( –html and–push ) and an input file
(a 765 bytes long xml file) as a test input.
6 DISCUSSION
6.1 Effectiveness of the Craig Interpolants
Guided FSR for Bug Detection Ability
Since the amount of the execution time of FOCAL is proportional
to a number of SPFs generated, reducing non-validating SPFs (i.e.,
satisfiable SPFs that correspond to the executions from main tov
but those whose solutions do not validate the target failures) is
important in detecting bugs effectively in a limited testing time.
FOCAL uses under-approximate FSes to reduce non-validating SPFs
because over-approximate FSes may lead compositional concolic
testing to generate many non-validating SPFs.
FSR is crucial for FOCAL in detecting bug effectively because
under-approximate FSes might not provide necessary execution
contexts for ψv. With the help of the Craig interpolants as FS re-
fining constraints, FOCAL generates 11.6 validating SPFs (each of
which consists of 5.2 FSes on average) that reach main from the
target failure lines and whose solutions validate the target failures
per program on average, which is 4.5 times (=11.6/2.6) more than
the validating SPFs generated by FOCAL−I. Consequently, FO-
CAL detects 4.4 times (=71/16) more bugs than FOCAL−I. Thus,
we can conclude that FSR using the Craig interpolants as refining
constraints significantly improves bug detection ability of FOCAL.
6.2 Effectiveness of Function Relevance-Based
Extended Units for Execution Time and
Bug Detection Ability
Realistic FSes are important for compositional concolic testing tech-
niques to detect bugs effectively in a limited testing time. FOCAL
uses function relevance-based extended units (Sect. 3.2 and Sect. 3.3)
to obtain realistic FSes. Sect. 5.3 demonstrates that the function rel-
evance based extended units contribute in reducing the execution
time and improving bug detection ability of FOCAL.
First, since the amount of the execution time of FOCAL is pro-
portional to a number of target failure lines, the extended units
saved the execution time in a large degree by reducing (false) tar-
get failure lines. FOCAL and FOCAL−Eidentified 67.1 and 313.9
target failure lines and spent 182.5 and 608.9 hours on average per
program, respectively.
8AFl-fast, KATCH, and Mix-CCBSE detected 8, 7, and 5 crash bugs in 782.5, 1210.3,
and 1610.9 hours, respectively. All bugs detected by them were detected by FOCAL.Second, the extended units help FOCAL to reduce non-validating
SPF generation. For example, FOCAL−Egenerated 21.3 satisfiable
SPFs that reach main from the target failure lines on average per
program. But, only 5.4 test inputs obtained by solving these SPFs
validate target failures (=25.4%=5.4/21.3). FOCAL generated 14.8
satisfiable SPFs that reach main from the target failure lines and
11.6 test inputs that validate the target failures (=78.4%=11.6/14.8)
on average per program.9Thus, we can conclude that the extended
units contribute to build SPFs that closely represent realistic system-
level behaviors of a target program.
Third, the extended units also improve bug detection ability (i.e.,
71 vs. 32 bugs detected by FOCAL and FOCAL−E). This is because
the FSes based on extended units (i.e., FOCAL) are more realistic
and more compatible to combine to build SPFs than the ones based
on single function (i.e., FOCAL−E). For example, FOCAL generates
0.22 (= 14.8/67.1) satisfiable SPFs that reach main per target failure
line while FOCAL−Egenerates only 0.07 (=21.3/313.9) satisfiable
SPFs that reach main per target failure line.
6.3 Comparison of the Directed Compositional
Concolic Testing Techniques
We compare FOCAL with SMASH [ 16] and Alter [38] which are
the most closely related work. Since the implementations of these
techniques are not publicly available, we compare them in an ana-
lytic way. FOCAL uses an under-approximate summary of a func-
tion based on its extended unit, and then repeatedly refines the
summary to cover program behaviors that are compatible with the
target failures by using the Craig interpolants.
SMASH [ 16] generates an over-approximate summary (i.e., may-
summary by predicate abstraction) and an under-approximate sum-
mary (i.e., must-summary by dynamic symbolic execution) of a
function. It uses both summaries to prune the execution space that
do not lead to a target failure. Unlike FOCAL, SMASH does not
refine a must-summary and may fail to detect bugs. The experiment
results with 69 device drivers [ 16] showed that SMASH is three
times faster than a non-compositional may-must analysis technique
DASH [4], but detects no more bugs than DASH.
Alter [38] explores symbolic space of a program in a goal-driven
way with selectively composing over-approximate FSes. Alter uses
Craig interpolants to check if the current search scope cannot have a
solution towards a target failure. But FOCAL uses Craig interpolants
to refine FSes to build satisfiable SPFs targeting v. The Alter paper
does not show a system-level bug detection ability because Alter
generates only test inputs to a public method nearest to a target
failure (a method call distance from a nearest public method to a
target failure is usually short), not to a program entry function (e.g.,
main in C programs). In contrast, FOCAL generates a test input
that runs Pfrommain to validate a target failure at vand, thus,
fully demonstrates its bug detection ability as a system-level bug
detection technique.
9A system-level test input obtained by solving a satisfiable SPF still may not vali-
date/reproduce the target failure because ϕamay not represent real behaviors of a
due to a’s symbolic stubs that may return infeasible values.
24ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Yunho Kim, Shin Hong, and Moonzoo Kim
7 RELATED WORK
7.1 Compositional Symbolic Analysis
SMART [ 14] generates a FS as a disjunction of function-wise sym-
bolic path formulas (i.e., conjunctions of constraints over inputs and
outputs of a function). However, since SMART does not refine FSes,
the bug detection ability may be low. Godefroid et al. [ 14] reported
only a case study on message parsing modules in oSIP without re-
porting bug detection ability. Anand et al. [ 2] extends SMART [ 14]
by generating FSes as first-order formulas with uninterpreted func-
tions. It refines a FS by refining uninterpreted functions on demand
(not using Craig interpolants). However, unlike FOCAL that fo-
cuses on generating test inputs to validate target failures that were
quickly identified by concolic unit testing, Anand et al. [ 2] may
not detect bugs effectively in given limited time as it targets all
uncovered code locations. Anand et al. [ 2] reported three small case
studies on C# programs where the proposed technique detects only
one more bug than Pex [ 39]. Qiu et al. [ 34] proposed a composi-
tional symbolic execution for heap manipulating programs. They
reported the execution time performed by the proposed approach to
explore all feasible program paths, but did not report bug detection
ability of the proposed approach.
For bounded model checking, FunFrog [36] and HiFrog [1] con-
struct over-approximate FSes using Craig interpolants in a proposi-
tional logic and a quantifier-free linear real arithmetics with UF the-
ory, respectively. Asadi et al. [ 3] proposed an on-demand FSR using
different theories for bounded model checking. Unlike these tech-
niques which use Craig interpolants to generate over-approximate
FSes, FOCAL constructs and refines under-approximate FSes using
Craig interpolants as refining constraints.
7.2 Directed Symbolic Analysis
Mix-CCBSE [ 30] combines backward call-chain exploration and
forward shortest-distance guided symbolic execution to reach a
given target code line, but without using FSes. Cilocnoc [ 11] com-
bines symbolic backward execution and search-based concrete for-
ward execution such that forward execution handles complex code
which symbolic backward execution cannot analyze (e.g., external
function calls or complex loops). Since Cilocnoc does not adopt
compositional approach, it may suffer from search space explosion
problem. Cilocnoc’s bug detection ability is evaluated on seven toy
programs by comparing time to reach the given goal line between
Cilocnoc, jCUTE and Symbolic PathFinder.
BugRedux [ 20] generates a system-level test input that repro-
duces a failure from the system-level failed execution information
such as a call stack dump or a call sequence obtained from the
failed system-level execution. Similarly to BugRedux, Hercules [ 33]
generates a system-level test input that reproduces a crash in real-
world binary programs from a crash report (e.g., call stack dump,
program state). First, Hercules identifies a crash condition from the
crash report. Then, it performs symbolic execution and computes a
minimal unsatisfiability core if a symbolic path formula σconflicts
with the crash condition. Hercules guides symbolic execution to
resolve the conflict by negating every clause in σthat appears in
the minimal unsatisfiability core. In contrast, FOCAL uses a Craig
interpolant for FSR to resolve a conflict between ϕakand summariesof a current failure-context.10In addition, FOCAL can detect un-
known failures where as BugRedux and Hercules cannot detect
unknown failures, but reproduce failures only if information on a
corresponding system-level execution is available.
Jaffar et al. [ 19] use a Craig interpolant-based search strategy to
prune symbolic paths. Unlike FOCAL that utilizes compositional
concolic testing, Jaffar et al.’s work analyzes a whole program by
performing function inlining (i.e., limited scalability.) Jaffar et al.
reported the execution time spent by the proposed technique to
explore all feasible execution paths in relatively small target bench-
mark programs (i.e., SV-COMP12), but did not report bug detection
ability. KATCH [ 31] uses a directed forward search strategy to cover
changed portion of source code (i.e., a patch) effectively by using
regression tests as initial tests of symbolic executions.
8 CONCLUSION
We present FOCAL which detects many bugs in programs without
false alarms. A core idea of FOCAL is to effectively and quickly
identify the target failures using concolic unit testing and focus to
generate system-level tests that validate the target failures using
compositional concolic testing with the Craig interpolants-based
function summary refinement. The evaluation with the real-world
C programs shows that FOCAL outperforms fuzzing (AFL-fast) and
directed concolic testing (KLEE and Mix-CCBSE) techniques.
As future work, we will improve the FS composition strategy
to prune the failure-contexts from which validating SPFs may not
be generated. Also, we will improve the accuracy of function rele-
vance metric by using machine learning techniques with various
static and dynamic code features. Furthermore, we will expand
the target domain of the compositional approach to invasive soft-
ware testing [ 24] to reduce computational cost and mutation-based
fault localization (MBFL) [ 18,28,32] to improve fault localization
precision by generating more failing test inputs.
ACKNOWLEDGMENTS
This research has been supported by Next-Generation Informa-
tion Computing Development Program through NRF funded by
MSIT (NRF-2017M3C4A7068177 and NRF-2017M3C4A7068179), Ba-
sic Science Research Program through NRF funded by MSIT (NRF-
2017R1C1B1008159 and NRF-2019R1A2B5B01069865), and Basic
Science Research Program through NRF funded by MOE (NRF-
2017R1D1A1B03035851).
REFERENCES
[1]Leonardo Alt, Sepideh Asadi, Hana Chockler, Karine Even Mendoza, Grigory
Fedyukovich, Antti E. J. Hyvärinen, and Natasha Sharygina. 2017. HiFrog: SMT-
based Function Summarization for Software Verification. In Tools and Algorithms
for the Construction and Analysis of Systems , Axel Legay and Tiziana Margaria
(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 207–213.
[2]Saswat Anand, Patrice Godefroid, and Nikolai Tillmann. 2008. Demand-Driven
Compositional Symbolic Execution. In Tools and Algorithms for the Construction
and Analysis of Systems , C. R. Ramakrishnan and Jakob Rehof (Eds.). Springer
Berlin Heidelberg, Berlin, Heidelberg, 367–381.
10Using Craig interpolants as path guiding constraints (FOCAL) may guide concolic
testing to a target path more effectively than the minimal unsatisfiability core based
constraints (Hercules). This is because the Craig interpolant is a goal-driven over-
approximation of the failure conditions constructed so far (i.e., Φv(Sk−1
v))and, thus,
can serve as hints to guide concolic execution toward the target failures. In contrast,
the minimal unsatisfiability core based constraints just prevent concolic execution
from exploring the execution paths that cannot raise the target failures.
25Target-Driven Compositional Concolic Testing with Function Summary Refinement for Effective Bug Detection ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
[3]Sepideh Asadi, Martin Blicha, Grigory Fedyukovich, Antti Hyv\"arinen, Karine
Even-Mendoza, Natasha Sharygina, and Hana Chockler. 2018. Function Sum-
marization Modulo Theories. In LPAR-22. 22nd International Conference on Logic
for Programming, Artificial Intelligence and Reasoning (EPiC Series in Computing) ,
Gilles Barthe, Geoff Sutcliffe, and Margus Veanes (Eds.), Vol. 57. EasyChair, 56–75.
https://doi.org/10.29007/d3bt
[4]Nels E. Beckman, Aditya V. Nori, Sriram K. Rajamani, and Robert J. Simmons.
2008. Proofs from Tests. In Proceedings of the 2008 International Symposium
on Software Testing and Analysis (ISSTA ’08) . ACM, New York, NY, USA, 3–14.
https://doi.org/10.1145/1390630.1390634
[5]Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-
based Greybox Fuzzing As Markov Chain. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’16) . ACM, New York,
NY, USA, 1032–1043. https://doi.org/10.1145/2976749.2978428
[6]Tim Bray. 2017. The JavaScript Object Notation (JSON) Data Interchange Format.
RFC 8259. https://doi.org/10.17487/RFC8259
[7]Jacob Burnim and Koushik Sen. 2008. Heuristics for Scalable Dynamic Test
Generation. In Proceedings of the 2008 23rd IEEE/ACM International Conference on
Automated Software Engineering (ASE ’08) . IEEE Computer Society, Washington,
DC, USA, 443–446. https://doi.org/10.1109/ASE.2008.69
[8]Arindam Chakrabarti and Patrice Godefroid. 2006. Software Partitioning for Ef-
fective Automated Unit Testing. In Proceedings of the 6th International Conference
on Embedded Software (EMSOFT ’06) . ACM, New York, NY, USA, 262–271.
[9]William Craig. 1957. Three Uses of the Herbrand-Gentzen Theorem in Relating
Model Theory and Proof Theory. The Journal of Symbolic Logic 22, 3 (1957),
269–285. http://www.jstor.org/stable/2963594
[10] Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An Efficient SMT Solver.
InProceedings of the Theory and Practice of Software, 14th International Con-
ference on Tools and Algorithms for the Construction and Analysis of Systems
(TACAS’08/ETAPS’08) . Springer-Verlag, Berlin, Heidelberg, 337–340.
[11] Peter Dinges and Gul Agha. 2014. Targeted Test Input Generation Using Symbolic-
concrete Backward Execution. In Proceedings of the 29th ACM/IEEE International
Conference on Automated Software Engineering (ASE ’14) . ACM, New York, NY,
USA, 31–36. https://doi.org/10.1145/2642937.2642951
[12] Hyunsook Do, Sebastian Elbaum, and Gregg Rothermel. 2005. Supporting Con-
trolled Experimentation with Testing Techniques: An Infrastructure and Its
Potential Impact. Empirical Software Engineering 10, 4 (Oct. 2005), 405–435.
[13] Gordon Fraser and Andrea Arcuri. 2013. 1600 Faults in 100 Projects: Automatically
Finding Faults While Achieving High Coverage with EvoSuite. Empirical Software
Engineering 20, 3 (2013), 611–639.
[14] Patrice Godefroid. 2007. Compositional Dynamic Test Generation. In Proceed-
ings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages (POPL ’07) . ACM, New York, NY, USA, 47–54. https:
//doi.org/10.1145/1190216.1190226
[15] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI ’05) . ACM, New York,
NY, USA, 213–223.
[16] Patrice Godefroid, Aditya V. Nori, Sriram K. Rajamani, and Sai Deep Tetali.
2010. Compositional May-must Program Analysis: Unleashing the Power of
Alternation. In Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages (POPL ’10) . ACM, New York, NY, USA,
43–56. https://doi.org/10.1145/1706299.1706307
[17] Florian Gross, Gordon Fraser, and Andreas Zeller. 2012. Search-Based System
Testing: High Coverage, No False Alarms. In Proceedings of the 2012 International
Symposium on Software Testing and Analysis (ISSTA ’12) . ACM, New York, NY,
USA, 67–77.
[18] Shin Hong, Taehoon Kwak, Byeongcheol Lee, Yiru Jeon, Bongseok Ko, Yunho
Kim, and Moonzoo Kim. 2017. MUSEUM: Debugging real-world multilingual
programs using mutation analysis. Information and Software Technology 82 (2017),
80 – 95. https://doi.org/10.1016/j.infsof.2016.10.002
[19] Joxan Jaffar, Vijayaraghavan Murali, and Jorge A. Navas. 2013. Boosting Con-
colic Testing via Interpolation. In Proceedings of the 2013 9th Joint Meeting on
Foundations of Software Engineering (ESEC/FSE 2013) . ACM, New York, NY, USA,
48–58. https://doi.org/10.1145/2491411.2491425
[20] Wei Jin and Alessandro Orso. 2012. BugRedux: Reproducing Field Failures for In-
house Debugging. In Proceedings of the 34th International Conference on Software
Engineering (ICSE ’12) . IEEE Press, Piscataway, NJ, USA, 474–484. http://dl.acm.
org/citation.cfm?id=2337223.2337279
[21] Moonzoo Kim, Yunho Kim, and Yunja Choi. 2012. Concolic testing of the multi-
sector read operation for flash storage platform software. Formal Aspects of
Computing 24, 3 (01 May 2012), 355–374. https://doi.org/10.1007/s00165-011-
0200-9
[22] M. Kim, Y. Kim, and H. Kim. 2011. Comparative Study on Software Model
Checkers as Unit Testing Tools: An Industrial Case Study. IEEE Transactions onSoftware Engineering (TSE) 37, 2 (March 2011), 146–160.
[23] Yunho Kim, Yunja Choi, and Moonzoo Kim. 2018. Precise Concolic Unit Testing
of C Programs Using Extended Units and Symbolic Alarm Filtering. In Proceedings
of the 40th International Conference on Software Engineering (ICSE ’18) . ACM, New
York, NY, USA, 315–326. https://doi.org/10.1145/3180155.3180253
[24] Yunho Kim, Shin Hong, Bongseok Ko, Duy Loc Phan, and Moonzoo Kim. 2018. In-
vasive Software Testing: Mutating Target Programs to Diversify Test Exploration
for High Test Coverage. In 2018 IEEE 11th International Conference on Software
Testing, Verification and Validation .
[25] Yunho Kim and Moonzoo Kim. [n.d.]. CROWN: Concolic testing for Real-wOrld
softWare aNalysis. http://github.com/swtv-kaist/CROWN Accessed: 2019-06-29.
[26] Yunho Kim, Youil Kim, Taeksu Kim, Gunwoo Lee, Yoonkyu Jang, and Moonzoo
Kim. 2013. Automated Unit Testing of Large Industrial Embedded Software Using
Concolic Testing. In Proceedings of the 28th IEEE/ACM International Conference
on Automated Software Engineering (ASE’13) . IEEE Press, Piscataway, NJ, USA,
519–528. https://doi.org/10.1109/ASE.2013.6693109
[27] Yunho Kim, Dongju Lee, Junki Baek, and Moonzoo Kim. 2019. Concolic Testing
for High Test Coverage and Reduced Human Effort in Automotive Industry. In
International Conference on Software Engineering (ICSE) Software Engineering In
Practice (SEIP) track .
[28] Yunho Kim, Seokhyeon Mun, Shin Yoo, and Moonzoo Kim. To appear. Precise
Learn-to-Rank Fault Localization using Dynamic and Static Features of Target
Programs. ACM Transactions on Software Engineering and Methodology (To
appear).
[29] Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for
Lifelong Program Analysis & Transformation. In Proceedings of the International
Symposium on Code Generation and Optimization: Feedback-directed and Runtime
Optimization (CGO ’04) . IEEE Computer Society, Washington, DC, USA, 75–.
[30] Kin-Keung Ma, Khoo Yit Phang, Jeffrey S. Foster, and Michael Hicks. 2011.
Directed Symbolic Execution. In Proceedings of the 18th International Confer-
ence on Static Analysis (SAS’11) . Springer-Verlag, Berlin, Heidelberg, 95–111.
http://dl.acm.org/citation.cfm?id=2041552.2041563
[31] Paul Dan Marinescu and Cristian Cadar. 2013. KATCH: High-coverage Testing
of Software Patches. In Proceedings of the 2013 9th Joint Meeting on Foundations
of Software Engineering (ESEC/FSE 2013) . ACM, New York, NY, USA, 235–245.
https://doi.org/10.1145/2491411.2491438
[32] Seokhyeon Moon, Yunho Kim, Moonzoo Kim, and Shin Yoo. 2014. Ask the
Mutants: Mutating Faulty Programs for Fault Localization. In Proceedings of the
2014 IEEE International Conference on Software Testing, Verification, and Validation
(ICST ’14) . IEEE Computer Society, Washington, DC, USA, 153–162. https:
//doi.org/10.1109/ICST.2014.28
[33] Van-Thuan Pham, Wei Boon Ng, Konstantin Rubinov, and Abhik Roychoudhury.
2015. Hercules: Reproducing Crashes in Real-world Application Binaries. In
Proceedings of the 37th International Conference on Software Engineering - Volume
1 (ICSE ’15) . IEEE Press, Piscataway, NJ, USA, 891–901. http://dl.acm.org/citation.
cfm?id=2818754.2818862
[34] Rui Qiu, Guowei Yang, Corina S. Păsăreanu, and Sarfraz Khurshid. 2015. Composi-
tional Symbolic Execution with Memoized Replay. In Proceedings of the 37th Inter-
national Conference on Software Engineering - Volume 1 (ICSE ’15) . IEEE Press, Pis-
cataway, NJ, USA, 632–642. http://dl.acm.org/citation.cfm?id=2818754.2818832
[35] Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit Testing
Engine for C. In Proceedings of the 10th European Software Engineering Conference
Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of
Software Engineering (ESEC/FSE-13) . ACM, New York, NY, USA, 263–272.
[36] Ondrej Sery, Grigory Fedyukovich, and Natasha Sharygina. 2011. Interpolation-
Based Function Summaries in Bounded Model Checking. In Hardware and Soft-
ware: Verification and Testing , Kerstin Eder, João Lourenço, and Onn Shehory
(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 160–175.
[37] Sina Shamshiri, Rene Just, Jose Miguel Rojas, Gordon Fraser, Phil McMinn, and
Andrea Arcuri. 2015. Do Automatically Generated Unit Tests Find Real Faults?
An Empirical Study of Effectiveness and Challenges (T). In Proceedings of the
2015 30th IEEE/ACM International Conference on Automated Software Engineering
(ASE) (ASE ’15) . IEEE Computer Society, Washington, DC, USA, 201–211. https:
//doi.org/10.1109/ASE.2015.86
[38] Nishant Sinha, Nimit Singhania, Satish Chandra, and Manu Sridharan. 2012.
Alternate and Learn: Finding Witnesses without Looking All over. In Computer
Aided Verification , P. Madhusudan and Sanjit A. Seshia (Eds.). Springer Berlin
Heidelberg, Berlin, Heidelberg, 599–615.
[39] Nikolai Tillmann and Jonathan De Halleux. 2008. Pex: White Box Test Generation
for .NET. In Proceedings of the 2Nd International Conference on Tests and Proofs
(TAP’08) . Springer-Verlag, Berlin, Heidelberg, 134–153.
[40] Aaron Tomb, Guillaume Brat, and Willem Visser. 2007. Variably Interprocedural
Program Analysis for Runtime Error Detection. In Proceedings of the 2007 Interna-
tional Symposium on Software Testing and Analysis (ISSTA ’07) . ACM, New York,
NY, USA, 97–107. https://doi.org/10.1145/1273463.1273478
26