API Method Recommendation without Worrying about the
Task-API Knowledge Gap
Qiao Huang
Zhejiang University
China
tkdsheep@zju.edu.cnXin Xia
Monash University
Australia
xin.xia@monash.eduZhenchang Xing
Australian National University
Australia
zhenchang.xing@anu.edu.au
David Lo
Singapore Management University
Singapore
davidlo@smu.edu.sgXinyu Wang
Zhejiang University
China
wangxinyu@zju.edu.cn
ABSTRACT
Developers often need to search for appropriate APIs for their
programming tasks. Although most libraries have API reference
documentation, it is not easy to find appropriate APIs due to thelexicalgapandknowledgegapbetweenthenaturallanguagede-
scription of the programming task and the API description in API
documentation.Here,thelexicalgapreferstothefactthatthesame
semantic meaning can be expressed by different words, and theknowledgegapreferstothefactthatAPIdocumentationmainly
describesAPIfunctionalityandstructurebutlacksothertypesof
information like concepts and purposes, which are usually the key
informationinthetaskdescription.Inthispaper,weproposeanAPI
recommendationapproachnamedBIKER( Bi-Informationsource
basedKnowledgERecommendation)totacklethesetwogaps.To
bridgethelexicalgap,BIKERuseswordembeddingtechniqueto
calculate the similarity score between two text descriptions. In-spired by our survey findings that developers incorporate Stack
OverflowpostsandAPIdocumentationforbridgingtheknowledge
gap, BIKER leverages Stack Overflow posts to extract candidate
APIs for a program task, and ranks candidate APIs by considering
the query’s similarity with both Stack Overflow posts and API doc-
umentation.Italsosummarizessupplementaryinformation(e.g.,
APIdescription,codeexamplesinStackOverflowposts)foreach
API to help developers select the APIs that are most relevant to
theirtasks.Ourevaluationwith413API-relatedquestionsconfirmstheeffectivenessofBIKERforbothclass-andmethod-levelAPIrec-
ommendation,comparedwithstate-of-the-artbaselines.Ouruser
studywith28Javadevelopersfurtherdemonstratesthepracticality
of BIKER for API search.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238191CCS CONCEPTS
•Software and its engineering →Software development tech-
niques;
KEYWORDS
API Recommendation, API Documentation, Stack Overflow, Word
Embedding
ACM Reference Format:
QiaoHuang,XinXia,ZhenchangXing,DavidLo,andXinyuWang.2018.API
MethodRecommendationwithoutWorryingabouttheTask-APIKnowledge
Gap.InProceedingsofthe201833rdACM/IEEEInternationalConferenceon
Automated Software Engineering (ASE ’18), September 3–7, 2018, Montpellier,
France.ACM,NewYork,NY,USA, 12pages.https://doi.org/10.1145/3238147.
3238191
1 INTRODUCTION
Application Programming Interfaces (APIs) in software libraries
(e.g., Java SDK) play an important role in modern software de-
velopment.WiththehelpofAPIs,developerscancompletetheir
programmingtasksmoreefficiently.However,itisnoteasytobe
familiarwithallAPIsinalargelibrary.Thus,developersoftenneed
to check the API documentation to learn how to use an unfamiliar
API for a programming task, and the prerequisite is that they al-
ready know which API to use but are just unfamiliar with the API.
This situation can be referred to as “known unknowns”.
However, a more practical scenario is that developers only have
the requirement of a programming task, while they do not even
know which API is worth learning (i.e., “unknown unknowns”). A
possiblesolutionistousethenaturallanguagedescriptionofthe
programming task as a query, and use Information Retrieval (IR)
approachestoobtainsomecandidateAPIswhosedocumentationis
similartothequery.However,thissolutionmaynotworkwelldue
tothelexicalgapbetweenthequeryandtheAPIdocumentation.
For example, given the query “How to initialize all values in anarraytofalse? ”,thedescriptionofthemostappropriateJavaAPI
methodArrays.fill is “Assigns the specified boolean value to each
element of the specified array of booleans.”, which does not contain
any important keywords like initialize orfalsein the query.
Recently, a neural network-based approach called word embed-
ding [27] has been proposed to capture the semantic meaning of
different words. It represents each word by a low dimensional vec-
tor, and semantically similar words (e.g., initialize andassign,false
293
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, and Xinyu Wang
andboolean) would be close in the vector space. Ye et al. [ 49] lever-
aged word embedding to bridge the lexical gap between the query
of programming task and Java API documentation. However, by
replicatingtheirstudy,weobservetwomajorproblems,aslisted
below.
The first problem is that they investigated API recommendation
at class-level only. Given the above query example, their approach
recommendsonlythe Arraysclassanddevelopersstillhavetocheck
about 50 methods to locate Arrays.fill if they check the methods
onebyoneinthedefaultorderinthe Arraysdocumentation.While
theirapproachcanbeappliedformethod-levelrecommendation,
its effectiveness is unknown.
The second problem is that even if their approach could bridge
the lexical gap, it is still difficult to find the relevant API whose
description does not share semantically similar words with the
query. Forexample, given thequery“How tocheck whether a class
exists?”, the most relevant Java API method recommended by Ye
et al.’s approach is org.omg.CORBA.Object._is_a, whose descrip-
tion is “Checks whether this object is an instance of a class that
implementsthegiveninterface.”,andthesimilarityscorebetween
this description and the query is 0.669, since the two sentences
havesemanticallysimilarwords(e.g., classandobject)orexactly
thesamewords.However,thetrulyrelevantAPIforthequeryis
java.lang.Class.forName, whose description is “Returns the Class
object associated with the class with the given string name.”; its simi-
larity score with the query is only 0.377, since its description does
not contain words similar to ‘check ’, ‘whether’o r‘exists’. However,
forNamecanbeusedto“checkwhetheraclassexists”.Wecallsuch
mismatchesbetweenataskdescriptionandtheAPIdocumentation
astask-APIknowledgegap,andourobservationisalsoconsistent
withpreviousstudies[ 23,28,31,39],whichpointedoutthatAPI
documentationmainlydescribesAPIfunctionalityandstructure,
but lacks other types of information (e.g., concepts or purposes).
Tobridgethistask-APIknowledgegap,weconductasurveywith
developers from two IT companies to understand how developers
search for APIs to resolve programming tasks and the developers’
expectations on automatic API recommendation techniques. From
47responses,wefindthatwhendeveloperssearchAPIs,atypical
informationseekingprocessistobrowseanumberofStackOver-
flow (SO) questions and pick out the APIs that seem to be useful
accordingtothediscussions.Thus,SOisoftenexploitedasabridge
between the programming task and theneeded API(s). This is fea-
sible becauseSO discussions aretask centric andcan complement
APIdocumentationwiththemissingconceptsandpurposes[ 39].
However,thedecisiononwhichAPI(s)touseisoftennotpurely
basedontheSOdiscussions,anddevelopersmayfurthercheckAPI
documentation to confirm the relevance of API(s). Furthermore, in
theknownunknownssetting,informationlikeAPIdescriptionand
code examples is crucial for determining which API(s) to use.
Inspired by this information seeking process, we propose an
automatic approach named BIKER ( Bi-Information source based
KnowledgERecommendation) whichleverages both SOposts and
API documentation to recommend APIs for a programming task.
To bridge the knowledge gap, BIKER retrieves the top-k questions
fromSOthataresemanticallysimilarwiththequery.Sincethese
questionsandthequerysharesimilarpurposes,theAPIsmentioned
in the questions are also likely to resolve the programming taskinthequery.Inthisway,wecangreatlynarrowdownthesearch
spaceofcandidateAPIs.ToranktherelevanceofacandidateAPItothequery,weconsiderthequery’ssimilaritywithboththeSOposts
inwhichthecandidateAPIismentionedandthecandidateAPI’s
officialdescription.Inthisway,wecanbalancetheAPIinformation
from both the API designer and user perspectives. To bridge the
lexicalgapbetweensemanticallysimilartextsthatareexpressed
by different words, we follow Ye et al. [ 49] to use word embedding
techniquestocalculatethesimilarityscore.Inadditiontorecom-
mendingAPIs,BIKERalsosummarizessupplementaryinformation
likeofficialAPIdescriptionandcodesnippetsinSOpoststohelp
developers better understand why these APIs are recommended so
that they can select the right API(s) more easily.
ToevaluateBIKER,wemanuallyselected413questionsfromSO
that are seeking APIs to resolve programming tasks and labelled
the ground-truth APIs for these questions based on their accepted
answers. For class-level recommendation, we enrich our dataset
with the dataset published by RACK [ 34] which contains 150 ques-
tions and corresponding class-level APIs. Note that RACK onlysupports class-level recommendation. For class-level recommen-dation, BIKER achieves a mean reciprocal rank (MRR) and mean
averageprecision(MAP)of0.692and0.659respectively,andthis
outperformsYeetal.’sapproachandthetwostate-of-the-artAPI
recommendation approaches RACK [ 34] and DeepAPI [ 21]b ya t
least 42% in MRR and 57% in MAP. For method-level recommenda-
tion, BIKER achieves an MRR and MAP of 0.573 and 0.521, and thisoutperformsYeetal.’sapproachandDeepAPI[
21]by205%inMRR
and 241% in MAP. Our evaluation also confirms the importanceof SO information in API recommendation and the usefulness of
incorporating SO information and API documentation. Finally, we
conductauserstudyinwhich28Javadevelopersaredividedinto
four groups using different tools to answer 10 API-method-related
questionsrandomlysampledfromthe413questions.Onaverage,
compared with the other three groups (i.e., web search only, using
DeepAPIand usingBIKERwith onlyAPIrecommendation butno
supplementary information), the group using the full version ofBIKER can improve answer correctness by at least 11% and save
answering time by at least 28%.
The main contributions of this paper are:
(1)We conduct a survey of developers’ API search behavior and
expectations, which suggests the necessity of incorporating SO
posts and API documentation for effective API search.
(2)Inspired by our survey results, we propose BIKER to recom-
mendAPImethodsbyexploitingSOpoststobridgetask-API
knowledgegap,andbyincorporatingtheinformationfromboth
SO posts and APIdocumentation for measuring API relevance
and assisting developers in selecting recommended APIs.
(3)Both our quantitative evaluation and user study show that
BIKERcanhelpdevelopersfindthecorrectAPIsforJavapro-
grammingtasksmoreefficientlyandaccurately,comparedwith
state-of-the-art baselines.
(4)We release the source code of BIKER and the dataset of our
evaluationanduserstudy1tohelpotherresearchersreplicate
and extend our study.
1The replication package can be downloaded at: https://github.com/tkdsheep/BIKER-
ASE2018
294
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. API Method Recommendation without Task-API Knowledge Gap ASE ’18, September 3–7, 2018, Montpellier, France
PaperOrganization. Theremainder ofthe paperis organizedas
follows.Wepresentthesurveytoinvestigatehowdeveloperssearch
forAPIsandtheirexpectationsofaneffectiveAPIrecommendation
tool in Section 2. We describe the technical details of BIKER in
Section3.WepresentourexperimentalsetupandresultsinSection 4
andSection 5,respectively.Wepresenttheresultsofouruserstudy
in Section 6. We discuss threats to validity in Section 7. We present
related work in Section 8. We conclude the paper and mention
future work in Section 9.
2 DEVELOPERS’ EXPECTATIONS ON API
RECOMMENDATION
To gain insights into how developers search for APIs to resolve
programming tasks and the developers’ expectations on automatic
API recommendation techniques, weconducteda survey with 130
JavadevelopersfromtwoITcompanies(bothareoutsourcingcom-
panies with more than 2,000 employees) and received 47 replies.
Our survey includes the following questions: 1) Do you often need
to search for appropriate APIs for your programming tasks? 2)
What tools and/or resources do you usually use to search APIs?
And why do you prefer these tools and/or resources? 3) Do you
feel searching APIs on the Internet is a time-consuming task? 4)
Which granularity of API recommendation(class or method or no
preference)doyouprefer?5)Whatfeature(s)doyouexpectanAPI
recommendation tool to support?
According to the responses, we have the following findings:
•87%oftherespondentsagreedorstronglyagreedthattheyoftenneed to search for appropriate APIs to resolve different program-
ming tasks during development.
•94% of the respondents chose search engines (e.g. Google) to
perform general search, because search engines can return infor-
mation from various sources like SO, Java API documentation
andtechnicalblogs.74%oftherespondentschosetofocussearch
on Q&A website (e.g., SO), because they can find similar ques-
tionswhoseanswersoftencontainrelevantAPIstouse.45%of
therespondents choseto directlyread JavaAPI documentation,
when they have some candidate API classes in mind and theywant to further check the documentation to decide which API
method to use.
•76% of the respondents agreed or strongly agreed that it is time-
consuming to find appropriate APIs by searching and browsing
resources on the Internet.
•63%oftherespondentspreferredthatthetoolshouldrecommend
APIsatmethod-level.19%preferredclass-leveland18%hadno
preference.
•85% of the respondents expect the tool to directly recommend
relevantAPIsforaprogrammingtaskdescribedinnaturallan-
guage.90% ofthese respondentssuggested thatthe tool should
provide additional information to explain why it recommends
certain APIs and how to use them.
ThesurveyresponsessuggestthatapartfromAPIdocumenta-
tion,SOisalsoanimportantresourcefordeveloperstosearchAPIs.
By interviewing with several respondents, we find that a typical
APIsearchprocesstheyadoptistofirstbrowseseveralrelevantSO
questionsandpickouttheAPIsthatseemtobeusefulinthedis-
cussions.TheintervieweddeveloperssuggestthatSOdiscussionsare usually centered on some programming tasks, which makes
iteasierforthemtonarrowdownsomecandidateAPIsthatmay
supporttheirtasks.Theyalsosuggestthatiftheystillcannotde-
cidewhichAPIistherightchoice,theywillfurtherchecktheAPIs’
documentation or code examples.
ThisAPIsearchprocessinspiresustodesignBIKERthatexploits
SOpoststobridgetask-APIknowledgegapandincorporatesthe
informationfrombothSOquestionsandAPIdocumentationtomea-
suretherelevance ofanAPIto theprogrammingtaskdescription.
As suggested by developers, BIKER also summarizes supplemen-
taryAPIinformationforeachrecommendedAPItohelpdevelopers
betterunderstandwhatanAPIcandoandselecttherightAPI(s)
for their tasks more easily.
3 APPROACH
Fig.1shows the overall framework of BIKER, which consists of
threemaincomponents:buildingdomain-specificlanguagemodels
for similaritycalculation (Section 3.1), searchingfor relevant APIs
basedonSOpostsandAPIdocumentation(Section 3.2),andsum-
marizingAPIsupplementaryinformation(Section 3.3).SinceBIKER
recommends APIs at method level by default, we also introduce
how to adapt BIKER for class-level recommendation in Section 3.4.
3.1 Building Language Models for Similarity
Calculation
To measure a query’s similarity to a SO post or an API descrip-
tion,weneedtobuilddomain-specificlanguagemodels.Wefirst
build a text corpus by extracting the text content from SO posts in
HTMLpages.WeremovelongcodesnippetsenclosedinHTMLtag
/angbracketleftpre/angbracketright, but keep short code fragments in /angbracketleftcode/angbracketrightin natural language
sentences. We use NLTK package [ 10] to tokenize the sentence.
Notethatifoneisinterestedinaparticularlanguageorlibrary’s
APIs, he may use a subset of SO post tagged with that library (e.g.,
Java).UsingtheSOcorpus,wetrainawordembeddingmodelusing
word2vec[ 27].Wordembeddingmodelprovidesthebasicmodelto
measure word similarity. Then we build the word IDF (inverse doc-
ument frequency) vocabulary. A word’s IDF represents the inverse
of thenumber ofSO poststhat containthe word.Wereduce each
wordinthecorpustoitsrootform(aka.stemming)usingtheNLTK
package[ 10].Thus,thewordswiththesamerootformwillhavethe
same IDF value. The more posts in which a word appears, the less
likelythewordcarriesimportantsemanticinformation,andthus
its IDF is lower. We use IDF as a weight on top of word embedding
similarity. Finally, the words in API documentation would directly
use this word embedding model and IDF vocabulary, since the text
volume of SO posts is much larger than API documentation.
3.2 Searching for Relevant APIs
OurAPIsearchcomponenthasthreesteps:retrievingsimilarSO
questionsto thequery, detectingAPIentities inthe SOposts,and
calculatingthequery’ssimilaritywithSOpostsandAPIdescrip-
tions for ranking the relevance of candidate APIs to the query.
3.2.1 Retrieving Similar Questions. Given a query describing a
programming task, the first step is to retrieve the top-k similar
questions from SO. BIKER first transforms the text of a question’s
title and the query into two bags of words, denoted as TandQ,
respectively. Then an asymmetric similarity score from T to Q
295
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, and Xinyu Wang
Text Corpus
API-Related 
QuestionsWord 
Embedding 
ModelWord IDF
Vocabulary
Official API 
DocumentationSimilar 
Questions 
RetrievalTop-k 
QuestionsAPI Entities 
DetectionCandidate 
APIs
Ranked 
List of APIsSimilarity 
Score 
CalculationQuery
Offline 
Processing
Summarizing API 
Supplementation Information
Recommended 
Results
Title of Similar Questions
Code Snippets from SO PostsOfficial API descriptionSearching 
Relevant APIsBuilding 
Language Model1
2
3
Figure 1: Overall framework of BIKER
is computed as a normalized, IDF-weighted sum of similarities
between words in Tand all words in Q:
sim(T→Q)=/summationtext.1
w∈Tsim(w,Q)∗idf(w)/summationtext.1
w∈Tidf(w)(1)
wheresim(w,Q)isthemaximumvalueof sim(w,w/prime)foreachword
w/prime∈Q, andsim(w,w/prime)is the cosine similarity of the word em-
bedding vectors of wandw/prime. The asymmetric similarity score
sim(Q→T)is computed analogously, by swapping TandQin
Equation 1. Intuitively, a word with lower IDF value would con-
tributelesstothesimilarityscore.Finally,thesimilarityscorebe-
tweenTandQiscomputedasthe harmonicmean ofthetwoasym-
metric scores:
sim(T,Q)=2∗sim(T→Q)∗sim(Q→T)
sim(T→Q)+sim(Q→T)(2)
Theretrievedtop-ksimilarquestionswillbeusedtodetectcandi-
date APIs for recommendation. In this paper, BIKER only retrieves
thetop-50similarquestions,sinceretrievingtoomanyquestions
may introduce noise to the recommendation process.
3.2.2 DetectingAPIEntities. Afterretrievingthetop-ksimilarques-
tions,BIKERusesseveralheuristicrulestoextractAPIentitiesfrom
eachquestion’sanswers.TheseAPIsareconsideredascandidate
APIs for recommendation. If an API is not mentioned in any of the
top-ksimilarquestions,itislesslikelytobetherightAPIforthe
query.Thus,wedonotconsiderallAPIsofalanguageorlibrary
for recommendation. In this way, a lot of irrelevant APIs would be
filtered out.
TodetectAPIentities,wefirstmanuallycheckedalargenum-
ber of API-related questions. We observe that an important API
mentionedbydevelopers isoftenhighlightedwith theHTMLtag
/angbracketleftcode/angbracketrightor referenced by a hyperlink to the API’s corresponding
documentation page. Thus, BIKER detects API entities using the
following two heuristics:
•BIKERcheckseveryhyperlinkineachanswerandusesregular
expressions to identify the hyperlink to a library’s official API
documentationsite,forexample, https://docs.oracle.com forJava
APIdocumentation.Thenitusesregularexpressionstodetectthe
fullnameofthecorrespondingAPImethodfromthehyperlink
andmarkthismethodasacandidateAPI.Forexample,giventhe
hyperlink https://docs.oracle.com/javase/8/docs/api/java/lang/C
lass.html#forName(java.lang.String), it extracts the API method
java.lang.Class.forName.•BIKERfirstbuildsadictionarythatstoresthenamesofallAPIsof
a language or library crawled from the language or library’s offi-
cialdocumentationsite.Thenitcheckstheplaintextcontainedin
every HTML tag /angbracketleftcode/angbracketrightin each answer. If the text fully matches
any API method in the dictionary, it is marked as a candidate
API.Note thatin mostcases, developers wouldomit thepackage
name of an API. For example, java.lang.Class.forName is usually
written as Class.forName. Thus, our dictionary only stores the
partially-qualified name of an API for string matching.
3.2.3 Calculating Similarity Score for Ranking Candidate APIs. Af-
terobtainingalistofcandidateAPIsfromthetop-ksimilarques-
tions,BIKERcalculatesthesimilarityscorebetweeneachcandidate
API and the query. Given an API and a query Q, their similarity
scoreisacombinationoftwoscores,namely SimSOandSimDoc.
Specifically, SimSOmeasures the similarity between the query and
the question title Tof a top-k similar question in which the API is
mentioned,and SimDocmeasuresthesimilaritybetweenthequery
and the API’s description in official API documentation.
Supposethatamongallthetop-ksimilarquestions,theAPIis
mentioned in nquestions, then SimSOis computed as:
SimSO(API,Q)=min(1,/summationtext.1n
i=1sim(Ti,Q)
n×log2n)(3)
wheresim(Ti,Q)represents the similarity score between the query
andthetitleofthe i-thquestionthatmentionstheAPI,and sim(Ti,Q)
is calculated based on Equation 2.SimSOconsiders two aspects.
First, the score should be related to the similarity between each
question and the query. Thus, it calculates the average of the simi-
larity score between each question’s title and the query. Second, if
theAPIismentionedinmultiplequestions,itismorelikelytobe
therightAPIforthequery.Thus,thescoreisfurtherboostedbased
on the number of questions. We add a logarithm transformation
log2ntocontrolthescaleofboosting.Forexample,thescorewould
be boosted by 20% if the API is detected in 4 questions. We also
restrict that the boosted score should not exceed 1.
TheSimDocis also calculated based on Equation 2given the
queryQand the API description D. Finally, the similarity score
between the query and the API is the harmonic mean of the corre-
sponding SimSOandSimDoc.
3.3 Summarizing API Supplementary
Information
AfterobtainingtherankedlistofcandidateAPIs,BIKERsumma-
rizes supplementary information for each API in the list. We do
296
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. API Method Recommendation without Task-API Knowledge Gap ASE ’18, September 3–7, 2018, Montpellier, France
Table 1: An example of API summary
Query:run linux commands in java code
API1:java.lang.Runtime.exec
JavaDoc: Executes the specified string command in a separate process
Similar Questions
1. Run cmd commands through java
2. use cmd commands in java program
3. Unable to execute Unix command through Java codeCode Snippets
/**********code snippet 1 **********/
Process p = Runtime.getRuntime().exec(command);
/**********code snippet 2 **********/
Runtime.exec( -whatever cmd command you need to execute- )
/**********code snippet 3 **********/
String command1 = “mv $FileName /bgw/feeds/ibs/incoming/”;Runtime.getRuntime().exec(command1);
this because our survey responses and interviews with developers
suggest that developers usually need to check more information
about API description and API usage examples to decide which
APIshouldbechosenfortheirtasks.Thus,thesupplementaryin-
formation summarized by BIKER considers three aspects, as listed
below:
•Official API description: It presents the API designer’s official
description of an API so that API users can quickly check the
API’s functionality.
•Title of similar questions: Based on the top-k similar ques-
tions, it extracts the title of all the questions whose answers
mention the API. Then it ranks these questions by their titles’
similarity scores with the query in descending order and presentthesequestionstitles(withhyperlinkstothecorrespondingweb-
page). To reduce information overloading, it only presents thetop-3 questions. Thus, developers can compare question titles
with their tasks.
•Code Snippets: Based on the top-k similar questions, it checks
each question’s answersand extracts the code snippets contain-
ing the API. Specifically, given an API (e.g., Math.round ) ,ac od e
snippetisextractedifitsatisfiesboththefollowingconditions:
1) The number of lines of code is no more than five; 2) The API’s
class name (i.e., Math) and method name (i.e., round) are both
containedinthecodesnippet.Theextractedcodesnippetsare
ranked by their corresponding questions’ similarity scores with
thequeryindescendingorder.Toreduceinformationoverload-
ing, it presents only the top-3 code snippets. Thus, developers
can check these code snippets to understand how to use the API.
Tobetter illustratetheoutcomeof thissummarizationstep,Ta-
ble1presents an example of the summary results for the top-1
recommendedAPI“java.lang.Runtime.exec ”,giventhequery“run
linux commands in java code ”.
3.4 Adapting BIKER for Class-Level
Recommendation
By default, BIKER recommends APIs at method-level. However,it can be easily adapted to support class-level recommendation.First, we need to revise the heuristic rules for detecting API en-tities. Specifically, we change the regular expressions so that itonly extracts the API’s class name (with full path of its package)
from the hyperlink to an API documentation page. We also change
the dictionary to store all APIs’ class names for string matching.Second, we need to change the way of calculating SimDocin the
stepofsimilarityscorecalculation.AlthoughanAPIclasshasits
own description like an API method, we do not use it since weobserve that the description of an API class is rather long in most
cases and it usually does not contain much useful information for
specific task requirements. Thus, BIKER calculates the similarity
score between the query and the description of each method in the
class,andchoosesthemaximumscoreastheresultof SimDocfor
this API class.
4 EXPERIMENTAL SETUP
Inthissection,wedescribetheexperimentalsetupthatwefollowtoevaluateBIKER.Theexperimentalenvironmentisalaptopequipped
withIntel(R)Core(TM)i7-6700HQCPUand16GBRAM,running
Ubuntu 16.04 LTS (64-bit).
4.1 Data Collection and Tool Implementation
4.1.1 SO TextCorpus. We downloadedthe officialdata dump [ 2]
of SO (published in: Dec 9th, 2017). As our current tool focuses on
Java API, we extracted 1,347,908 questions that are tagged with
“java”.Based onthese questionsand theiranswers, webuilt atext
corpususingtheplaintextineachposttotrainthewordembed-
ding model and build the IDF vocabulary. We used Gensim[35]
(apythonpackagewhichimplementsword2vec[ 27])totrainthe
word embedding model.
4.1.2 SO Question Base. To create the knowledge base of API-
relatedquestionsforsimilarquestionsretrieval,weselectedonly
the questions satisfying the following criteria: 1) the question
should have positive score; and 2) at least 1 answer to the question
contains API entities and the answer’s score should be positive.
Note that the API entities mentioned in a post were automatically
detectedbytheheuristicsdescribedinSection 3.2.2.Inthisway,we
collected125,847questionsastheknowledgebaseofAPI-related
SO questions.
4.1.3 ExperimentalQueriesandGround-TruthAPIs. Tocreateex-
perimental queriesfor the evaluation ofBIKER, we followedYe et
al.[49]toselectasmallnumberofAPI-relatedquestionssatisfying
thefollowingcriteria: 1)thescoreofthequestion itselfshouldbe
atleast5.Yeetal.setthisthresholdto20butthisleavesonly604
candidate questions which is too few; 2) the question’s accepted
answershould containAPIentities andtheanswer’s scoreshould
be positive.
Inthisway,wecollected3,395questionsintotal.Amongthese
questions, we randomly selected 1,000 questions. We manually
checked each selected question’s title to remove the questions that
do not aim to search APIs for programming tasks. We examine
only the question titles because we assume that developers would
use BIKER like a search engine, and thus BIKER is not likely to
receive a query with too many words. The first author and another
PhDstudentindependentlylabelledthequestionstoberemoved.
Typical examples of questions being removed are shown below:
•The question seeks for comparison of multiple APIs (e.g., Differ-
ence between HashSet and HashMap? ).
•The question seeks for the theories or algorithms behind an API
(e.g.,why HashMap Values are not cast in List? )
•The question’s title contains the word like ‘this’, ‘that’ or ’it’,
which makes its purpose unclear (e.g., how to parse this string in
java?).
•The question describes an error or a bug (e.g., IP Address not
obtained in java ).
297
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, and Xinyu Wang
We use Fleiss Kappa [ 18] to measure the agreement between
thetwolabelers.TheKappavalueis0.85,whichindicatesalmost
perfect agreement.After completingthe manual labeling process,
the two labelers and another post-doc discussed together their
disagreementstoreachacommondecision.Inthisway,wecollected
469 questions for further inspection.
By default, for each question, all the API entities in the accepted
answer are considered as relevant APIs to resolve the question.
However,someoftheAPIentitiesmaynotbetrulyhelpfulandsome
trulyhelpfulAPIsmaynotbedetectedbyourheuristicrules.Thus,
the first author and the same PhD student manually checked each
question’s title, body and its accepted answer to fix this issue. The
overallkappavalueis0.78,whichindicatesasubstantialagreement,
and the two labelers also discussed their disagreements with the
same postdoc to reach a common decision.
Specifically, a small number of questions were removed since
they cannot be easily resolved by Java APIs. For example, in thequestion “How can I set the System Time in Java? ”, the accepted
answer clearly stated that Java does not have an API to do this.
For most questions,wemainly relied on each question’saccepted
answertodecidethegroundtruthAPIs.However,sinceboththe
two labelers have at least 3 years of Java development experience,
if the question is asking a common programming task, we also
checkedtheotheranswerstoaddotherAPIsthatarealsohelpful
but not mentioned in the accepted answer. For example, for the
question“HowtoroundanumbertondecimalplacesinJava ”,the
accepted answer only mentioned DecimalFormat.setRoundingMode,
buttheothertwoAPIs(i.e., Math.round andBigDecimal.setScale )
mentioned in other answers are also helpful.
Afterthismanuallabelingprocess,wegot413questionsalong
withtheirgroundtruthAPIsasthetestingdatasetfortheevaluation
ofBIKER.Weusethetitleofthese413questionsasthequeryforAPI
search. Note that these 413 questions and their duplicate questions
were excluded from the SO question base.
4.1.4 JavaAPIDictionaryandAPIDescription. Wedownloadedthe
JavaSE8APIdocumentation[ 1]andparsedthehtmlfileofeach
API class to extract all API methods, along with their descriptions.
Forsimplicity,JavainterfaceswerealsotreatedasJavaclasses.In
total,weextracted4,216classesand31,736methodsandbuiltaJava
API dictionary with the name of these API classes and methods.
4.2 Baseline Approaches
WecomparetheperformanceofBIKERwithtwobaselinemethods,
as listed below:
Baseline1(RACK): Rahmanetal.[ 34]proposedRACK,which
constructsakeyword-APImappingdatabasewherethekeywords
are extracted from SO questions and the mapped APIs are col-lected from corresponding accepted answers. Based on this data-base, RACK recommends a ranked list of API classes for a givennatural language query. Note that we only compare BIKER with
RACKatclass-level,sinceRACKdoesnotsupportrecommendation
atmethod-level.AlthoughRACKalsoleveragesSOtobridgethe
knowledge gap, it does not consider API documentation and its
technique is different from BIKER.
Baseline2(DeepAPI): Guetal.[ 21]proposedDeepAPI,which
adaptsaRecurrentNeuralNetwork(RNN)Encoder-Decodermodel.
DeepAPI encodes a word sequence (user query) into a fixed-lengthcontext vector, and generates an API-method sequence based onthe context vector. For example, given the query “open a url ”, its
firstrecommendedresultis“URL.new →URL.openConnection ”.Deep-
API’s technique is different from BIKER and their knowledge base
isa largecorpus ofannotatedAPI sequencesextractedfromcode
repositories.
Note that we do not choose Ye et al.’s approach [ 49]a so u r
baseline, since it can be considered as part of BIKER. If BIKER uses
onlyJavaAPIdocumentation,thenBIKERisreducedtobethesame
as Ye et al.’s approach. We also have a research question (RQ2 in
Section5.2) to discuss the effectiveness of BIKER when using Java
API documentation only.
4.3 Evaluation Metrics
WeevaluateBIKERandotherbaselinesusingMRRandMAP,which
are classical evaluation metrics for information retrieval [ 25]. MRR
measureshowfarweneedtocheckintherecommendedlisttofindthefirstcorrectanswer,whileMAPconsiderstheranksofallcorrect
answers. MRR and MAP are also widely used in previous software
engineeringstudies[ 24,34,37,40,45–48,50].Inaddition,werun
theWilcoxonsigned-ranktest[ 41]withBonferronicorrection[ 6]to
checkifthedifferencesbetweentheperformanceofBIKERandthebaselinesarestatisticallysignificant.Weconsiderthatoneapproach
performssignificantly betterthanthe otheroneat theconfidence
level of 95% if the corresponding Wilcoxon signed-rank test result
(i.e., p-value) is less than 0.05. We also use the Cliff’s delta ( δ)[15]
to quantify the amount of difference between two approaches. The
amountofdifferenceisconsiderednegligible( |δ|<0.147),small
(0.147≤|δ|<0.33), moderate (0 .33≤|δ|<0.474), or large
(|δ|≥0.474), respectively.
5 EXPERIMENT RESULTS
5.1 RQ1: How effective is BIKER? How much
improvement can it achieve over the
baseline methods?
Motivation. BIKERaimstoautomaticallyrecommendappropriate
APIs for programming tasks described in natural language queries.
Thus, for the approach to be useful, we need to see how accurate it
is in API recommendation and how it compares with existing API
recommendation methods.
Approach. Toanswerthisresearchquestion,wecompareBIKER
with the two baselines (i.e., RACK and DeepAPI) using our testing
datasetincluding413queriesandground-truthAPIs.SinceRACK’s
authorshavepublishedanexecutabletool[ 4]forreplication,we
directly use this tool to compare with BIKER. For DeepAPI, the
authorshavedeployedanonlinedemotool[ 3],whichreceivesa
userqueryandpresentstherecommendationresultsontheweb-
page. Thus, to compare with DeepAPI, we wrote a web-crawler to
automaticallysendallqueriesinthetestingdatasetonebyoneand
retrievetherecommendationresultsthroughHTTPrequests.We
also carefullychecked theJavaScript codebehind thewebpage to
make sure that we did the same text preprocessing for each query.
Since DeepAPI recommends API sequence, we consider an APIsequence is correct if any one of the APIs in the sequence is the
groundtruthAPI.ThismakesthefaircomparisonwithDeepAPI.
Finally, RACK’s authors also published their testing dataset, which
298
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. API Method Recommendation without Task-API Knowledge Gap ASE ’18, September 3–7, 2018, Montpellier, France
Table2:PerformanceofBIKERandthebaselinemethodsfor
class-level recommendation
AppraochClass-Level Recommendation
Our Dataset RACK’s Dataset
MRR MAP MRR MAP
BIKER 0.692 0.659 0.428 0.271
RACK 0.296 0.266 0.302 0.171
DeepAPI 0.462 0.420 0.276 0.149
Improve. RACK134% 148% 42% 58%
p<0.001 p<0.001 p<0.001 p<0.001
|δ|=0.57|δ|=0.59|δ|=0.12|δ|=0.17
Improve. DeepAPI50% 57% 55% 82%
p<0.001 p<0.001 p<0.001 p<0.001
|δ|=0.33|δ|=0.35|δ|=0.28|δ|=0.30
Table 3: Performance of BIKER and DeepAPI for method-
level recommendation
AppraochMethod-Level Recommendation (Our Dataset)
MRR MAP
BIKER 0.573 0.521
DeepAPI 0.188 0.153
Improve. 205% (p <0.001,|δ|=0.57) 241% (p <0.001,|δ|=0.59)
contains 150 code search queries randomly chosen from several
Java tutorial sites. Thus, we also evaluate all approaches using this
dataset, which only supports class-level evaluation.
Results. Table2presentstheperformanceofBIKERandthetwo
baselines for class-level recommendation. The results show that
BIKERsignificantlyoutperformsRACKandDeepAPIintermsof
MRR andMAPfor bothdatasets, withan improvementof atleast
42%inMRRandatleast57%inMAP.WealsonotethattheMRRand
MAP achieved by BIKER for RACK’s dataset are relatively lower
thanthoseachievedforourdataset.BymanuallycheckingRACK’sdataset,wefindthatabout19%ofitsquestionsincludeground-truth
APIsfromthird-partypackages(e.g.,MongoDB,ApacheCommons,
etc.)orJavaEE,whichisbeyondtheknowledgebasedofourcurrent
tool (i.e., we only consider APIs from Java SE). Except for the MRR
and MAP comparison with RACK on our dataset and for the MAP
comparisonwithDeepAPIonourdataset,theamountofdifference
between the compared methods for other comparisons is either
small or negligible.
Table3presents the performance of BIKER and DeepAPI for
method-levelrecommendationusingourdataset.RACKandRACK’s
datasetarenotusedsinceRACKonlysupportsclass-levelrecom-
mendation. The MRR and MAP achieved by BIKER is 0.573 and0.521, respectively, which significantly outperforms DeepAPI by
205% in MRR and 241% in MAP. The amount of difference between
the two approaches are large for both MRR and MAP.
To sum up, BIKER significantly outperforms the two state-of-
the-art baseline methods for both class- and method-level API rec-
ommendation.TheadvantageofBIKERismoreevidentformethod-
level API recommendation.
5.2 RQ2: How effective is BIKER when using
the two different information sources
individually?
Motivation. BIKER leverages both SO posts and Java API docu-
mentation to calculate the similarity score between an API and the
query.However,BIKERcanstillworkifweonlyuseoneofthetwo
informationsourcesindividually.Thus,wewouldliketoinvestigate
whether the combination of the two information sources results in
better or poorer performance.Table 4: Performance of BIKER for our dataset when using
one or both information sources
Info SourceClass-Level Method-Level
MRR MAP MRR MAP
Stack Overflow 0.559 0.529 0.524 0.476
Java Documentation 0.287 0.265 0.097 0.079
Both 0.692 0.659 0.573 0.521
Improve. SO 24% 25% 9% 9%
Improve. JavaDoc 141% 149% 491% 559%
Approach. To answer this research question, we evaluate the per-
formanceofBIKERwhenusingeitherSOpostsorJavaAPIdocu-
mentation for calculating the query-API similarity score, and com-
parethatperformancewith theperformanceofBIKERusingboth
information sources. When using only SO, the candidate APIs are
extracted from top-k similar questions, and the similarity score of
each candidate API with the query is calculated based on only SO
questions (i.e., SimSO). Whenusing onlyJava APIdocumentation,
thelistofcandidateAPIsisthelistofallAPImethods(orclasses)in
JavaAPIdocumentation,andthesimilarityscoreofeachcandidate
API with the query is calculated based on Java API documentation
(i.e.,SimDoc).Notethattheonly-Java-API-documentationsetting
is essentially Ye et al.’s approach [49].
Results. Table4presentstheperformanceofBIKERwhenusing
each information source individually. In general, when combining
both information sources together, BIKER performs better thanusing each information source individually. Comparing the im-provement ratio over SO or Java documentation, we can see the
importanceofSOinformationinBIKER.UsingonlySOinformation,
the performance is only 24% worse in MRR and 25% worse in MAP
thanusingbothinformationsourcesforclass-levelrecommenda-
tion,andonly9%worseinbothMRRandMAPformethod-level
recommendation.Ho wever,usingonlyJavadocumentation,theper-
formance becomes significantly worse than using two information
sources.ButusingJavadocumentationasanadditionalinformationsourcecanfurtherimprovetherecommendationperformancethan
using only SO information.
5.3 RQ3: How efficient is BIKER for practical
use?
Motivation. Duringthemodelbuildingprocess,BIKERneedsto
trainwordembeddingmodelandbuildIDFvocabularyusingthe
corpus extracted from more than one million SO questions. This
would require substantial computational time, especially for theword embedding model. Another time-consuming process is to
transformthetitleofallthe125,847questionsinquestionknowl-
edge base and the description of all the 31,736 API methods into
matrix representation based on each word’s embedding vector and
IDFvalue,sothatwecancomputethesimilarityscorebetweenthequeryandthedocumentsefficiently.Duringtherecommendingpro-
cess,givenaquery,BIKERneedstocalculatethesimilaritybetween
thisqueryandeachquestioninthequestionbase,whichcouldalsobetime-consuming.IfBIKERcannotrunwithareasonableruntime
performance, developers may not be willing to use it in practice.
Approach. To answer this research question, we record model
training time and query processing time of BIKER and the two
baselinesusingourtestingdatasetforclass-levelAPIrecommenda-
tion.ThetimecostforBIKERandDeepAPIdonotchangeunder
method-level recommendation.
299
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, and Xinyu Wang
Table 5: Time cost for model training and query processing
of BIKER and the baseline methods
Approach Model Training Time Query Processing Time
BIKER 36 minutes 2.8s / query
DeepAPI 240 hours 2.6s / query
RACK unknown 12.8s / query
Results. Table5presents the model training time and the average
query processing time of BIKER and the two baseline methods.
As reported by DeepAPI’s authors [ 21], their approach takes 240
hours of model training, since their approach is based on RNN
(i.e.,adeepneuralnetwork),whichiscomputationallyexpensive
duringtraining[ 20].ThetrainingtimecostofRACKisunknown
sinceit isnotreported bytheauthorsand itisnot easytoreplicate
the training process without RACK’s source code. BIKER takes
36 minutes to train, which is also relatively slow, and almost the
whole time cost is due to training word embedding model. The
wordembeddingmodelonlyneedstobetrainedonceanditdoes
notneedtobeupdatedfrequentlysincethetextcorpusisalready
very large (i.e., extracted from 1.3 million questions). If we use pre-
trained word embedding model, we just need about 10 seconds to
transform text into matrix representation.
For the average query processing time, RACK is slowest (12.8
seconds) to process each query, while DeepAPI is the fastest (2.6
seconds). BIKER (2.8 seconds) is slightly slower than DeepAPI. The
major computation cost of BIKER for query processing is due to
thestepofsimilarquestionsretrieval,whereweneedtocompare
thequerywiththetitlesofabout120thousandquestions.Toim-
prove the time efficiency, we can reduce the size of questions to be
compared with some heuristic rules (e.g., only comparing with the
questionwhosescoreislargerthan k)oracceleratesimilarityscore
computation by GPU [17].
6 USER STUDY
Inthissection,weconductauserstudytoinvestigatehowdevel-
opers interact with BIKER and whether it can help developers find
correct APIs more efficiently and accurately.
6.1 Study Design
6.1.1 Experimental Queries and Ground-Truth APIs. To conduct
ouruserstudy,werandomlyselected10questionsfromourtesting
dataset, as shown in Table 6. The last column shows the ground-
truthanswers,whichrefertotheAPIsextractedfromeachques-
tion’sacceptedanswer.Threequestions(i.e.,Q1,Q3andQ10)re-
quiremultipleAPIs(i.e.,anAPIsequence)tocompletetheprogram-
ming task.
6.1.2 Participants. We recruited 28 participants from both uni-
versity and IT companies. 16 of them (2 postdocs, 9 PhDs and 5
graduate students) are from the first author’s university, and 12 of
themarefromtwoITcompanies.AllofthemhaveJavadeveloping
experienceineithercommercialoropensourceprojects,andthe
years of their developing experience vary from 1 year to 5 years,
with an average of 2.9 years.
6.1.3 Experimental Groups. Next, we divided the participants uni-
formly based on years of development experience into four groups,
withthefollowingsettings:1) WSO:FindappropriateAPImethods
by searching and browsing resources on the Internet (i.e., Web
SearchOnly);2)DeepAPI: UseDeepAPI’sonlinetool;3) BIKER-
Simple:UseasimplifiedversionofBIKER,whichonlyrecommendsthenameofAPIs;4) BIKER-Full: Usethefully-featuredversion
of BIKER.
RACKisnotevaluatedsinceitdoesnotsupportmethod-levelrec-
ommendationanditrunsmuchslowerthanDeepAPIandBIKER.
The DeepAPI, BIKER-Simple and BIKER-Full groups are also al-
lowed to search any resources on the Internet if the participants
deemtheinformationprovidedbythetoolisnotenoughtoanswerthequestions.Sincethe10questionswereextractedfromSO,tobe
fairacrossdifferenttechniques,weinstructedtheparticipantsto
ignore the 10 questions on SO when searching the Web.
6.1.4 Procedure. We deployed a simple website with 10 pages,
eachcorrespondingtoonequestion.Whenaparticipantclickedthe
webpageofaquestion,atimerinthebackgroundwouldcollecthow
muchtimehe/shespentuntilsubmittingtheanswer.Participants
wereencouraged tocompleteeach questionwithoutinterruption
and they would explicitly inform us if there was interruption.
6.2 Results Analysis
Weanalyzetwometricswiththeuserstudyresults,asshownbelow:
•Correctness: This metric evaluates whether a participant can
findthecorrectAPIs.ForthequestionthatonlyneedsoneAPI
method, correctness is 1 if the participant submitted the cor-rect API, otherwise 0. For the question that needs an API se-
quence, correctness is the proportion of the correct APIs submit-
tedbytheparticipantamongallAPIsinthecorrectAPIsequence.
SomequestionscanalsoberesolvedusingotherAPIsdifferent
from the ground-truth APIs. For example, BigDecimal.setScale
orMath.round are also the correct answers for Q10. Thus, we
manually check each participants’ answers to make sure the
correctnessisalso1iftheysubmittedthecorrectbutnotground-
truth APIs.
•Completion time: This metric evaluates how fast a participant
can answer the question. One problem is that in some cases, the
recordedcompletiontimemaynotreflectthetrueeffortneededto
answer the question. For example, for the DeepAPI group, while
6participantsneededatleast30secondstoanswerQ9,thereis1
participantwhoonlyspent12seconds.Byconsultingwiththis
participant,wefoundthatheisaseniorJavadeveloperandhecan
directly answer this question without any tool’s support. On the
other hand, we recorded more than 20 minutes completion time
for a single question for a few participants. They explained that
they were interrupted by urgent tasks or bad network condition.
Toavoidtheeffectofoutliers,foreachquestion,wereportthe
median value of the time spent for each group.
Table7presentstheresultsofuserstudy.Ingeneral,participants
inBIKER-Full groupperformedaswellasorbetterthantheother
threegroupsforeveryquestionintermsofcorrectness,andthey
werethefastesttosolvesixoutofthetenquestions.Onaverage,the
fullversionofBIKERcanimprovecorrectnessbyatleast11%and
savethetimecostbyatleast28%.Wealsonotethatthecorrectness
of different groups vary a lot for several questions (e.g., Q3 and
Q7). By manually checking the participants’ answers, we have the
following two findings:
First,althoughBIKERdoesnotrecommendAPIsequences,partic-
ipantscanfindthenecessarysequencebythemselveswiththehelpof code snippets provided by BIKER. For example, in Q3, all partici-pantsinBIKER-Simple grouponlychosethefirstrecommendedAPI
300
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. API Method Recommendation without Task-API Knowledge Gap ASE ’18, September 3–7, 2018, Montpellier, France
Table 6: Ten questions and their standard answers for user study
PIDStackOverflow ID Query Answers
Q115788453 Resolving ip-address of a hostname? InetAddress.getByName →InetAddress.getHostAddress
Q229259201 How to make a list thread-safe for serialization? Collections.synchronizedList
Q311284938 Remove trailing zeros from double? BigDecimal.stripTrailingZeroes →BigDecimal.toPlainString
Q433773708 How to check whether a class exists? Class.forName
Q510383688 Is there any way to find os name using java? System.getProperty
Q619486077 Java Fastest way to read through text file with 2 million lines? BufferedReader.readLine
Q74584541 Check if a class is subclass of another class in Java? Class.isAssignableFrom
Q85505927 How to generate a random permutation in Java? Collections.shuffle
Q910078867 How to initialize all the elements of an array to any specific value in java? Arrays.fill
Q10153724 How to round a number to n decimal places in Java? DecimalFormat.setRoundingMode →DecimalFormat.format
Table 7: Results of user study
Metrics Group Q1Q2Q3Q4Q5Q6Q7Q8Q9Q10Average
CorrectnessWSO 0.790.790.861.00.710.710.570.790.711.0 0.79
DeepAPI 0.790.860.640.860.861.01.00.860.861.0 0.87
BIKER-Simple 0.640.860.501.00.711.01.00.861.01.0 0.86
BIKER-Full 0.791.00.931.01.01.01.01.01.01.0 0.97
Completion TimeWSO 132s74s91s76s57s97s146s33s53s82s 84s
DeepAPI 104s93s72s87s49s44s41s73s68s21s 65s
BIKER-Simple 113s52s43s72s53s86s61s59s45s19s 60s
BIKER-Full 81s28s65s42s44s51s32s35s29s26s 43s
“BigDecimal.stripTrailingZeroes ” as their answers, possibly because
the API’s name seems to be the right choice and the key phrase in
its documentation (i.e., with any trailing zeros removed ) also seems
tomeetthetaskrequirement.However,asshowninSOpost,this
APIwouldtransformanumberlike600.0intoscientificnotation.
Tofixthisissue,developersneedtocall BigDecimal.toPlainString
afterstrippingthetrailingzeros.In BIKER-Full group,sixoutofthe
seven participants chose both the two APIs as their answers, since
the code snippets with stripTrailingZeroes has clearly showed that
toPlainString shouldbecalledbeforeprinting.Suchphenomenon
also appeared in the answers for Q1.
Second,insomecases,participantscanfindthecorrectAPIsmore
easilyor withmoreconfidence iftheyhavetool support.For exam-
ple,bothDeepAPIandBIKERrecommended Class.isAssignableFrom
asthetop-1ortop-2answerforQ7,whichmayhelpparticipants
narrow down the search space. On the other hand, three out of
the seven participants in WSOgroup submitted Class.isInstance or
instanceof (notanAPIbutJavaoperator),whicharebothincorrect.
Actually, many developers are confused about the difference [ 5]
betweenClass.isInstance andClass.isAssignableFrom. Thus, it is not
surprising that these participants submitted Class.isInstance, which
is also “relevant” to the question, but cannot directly solve the task.
To sum up, BIKER can help developers find appropriate APIs
moreefficientlyandaccurately.Thiscanbeattributedtoitscapa-
bility of effectively narrowing down candidate APIs and providing
supplementaryinformationforunderstandingandselectingrecom-
mended APIs.
6.3 Participants’ Comments
We encouraged the participants in BIKER-Full group to write their
comments and suggestions for BIKER after the experiment. Forthe participants in the other three groups, we also showed them
the results recommended by the full version of BIKER after they
finished their tasks, and invited them to provide comments and
feedbackstoo.Amongallthe28participants,13participantspro-
vided some comments and suggestions. Based on these comments,
we summarized several major aspects of BIKER that are liked or
disliked by participants, as shown below:•Positive Opinions
–“GiventheJavadocandcodesnippets, IcaneasilydecidewhetherthisAPIisuseful .Idon’t
need to Google for more information in most cases, this saves me a lot of time.”
–“Since the tool recommended 5 APIs, there must be some APIs not helpful to solve the question.
However, I especially appreciate the fact that some of these unrelated APIs also inspired me
a lot. For example, in Q2, it also recommended the API for unmodifiable list and map, whichwould be useful if the scenario or requirement is broadened.”
–“Thecodesnippets isveryuseful.Itgivesme moreconfidence tomakethefinalchoiceand
shows me how to use the API.”
•Negative Opinions
–“Although I can easily judge which API is correct with the information (like Javadoc) provided,sometimesIstilldon’tknowhowtouseit.Yes,yourtoolcanprovidecodesnippetsformostAPIs,butsomeAPIsarenotandsometimestheyarejusttheexactAPIsIwanttofurthercheck!Is this a bug? For example, in Q6, you recommended BufferedReader.readLine as the first result,butno code snippet provided and thejavadoc is also too simple...”
–“The layout is not ideal. Sometimes it just looks like a mess, especially when every recom-mended API has multiple code snippets with many lines.”
–“SometimestheAPInameisalreadyenoughformetojudge.Whydon’tyou makeadditional
information folded up and let me to decide read it or not by myself?”
Fromthesecomments,wecanseethat participantscanbenefit
from the supplementary information provided for each API. How-
ever,sometimesBIKERmayfailtoextractcodesnippetsforsome
APIs, because we only scanned the top-k similar questions. We
could improve this component by building a mapping database
which stores the API and its code snippets extracted from more
questions. Finally, as pointed out by the participants, we need to
carefullydesignthelayout orthewaywepresentthesupplemen-
taryinformationtomaketheusefulinformationmoreusable,which
is an important aspect of user experience to be improved.
7 THREATS TO VALIDITY
Threats to internal validity relates to the errors in the imple-
mentation of BIKER and the baseline methods. We have double
checkedourcodetomakesurethatthequestionsintestingdataset
arenotincludedinthequestionbase.Forthebaselinemethods,wedirectlyusedtheir publishedtools.Thus,thereis littlethreattothe
approach implementation. The degree of participants’ carefulness
andeffortspentinouruserstudymayalsoaffectsthevalidityofour
user studyresults. To reducethis threat, werecruited participants
whoexpressinterestsinourresearchandmadetheaverageyears
of development experience in each group as uniform as possible.
Threats to external validity relates to the quality of our dataset
and generalizability of our results. To ensure the quality of ourdataset, we had two labelers to label the data and we relied on
the accepted answer to label the ground-truth APIs. Although our
301
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, and Xinyu Wang
datasetcontainsonly413questions,mostofthesequestionshavea
largenumberofviewcount.Amongthese413questions,about70%ofthequestionsinourdatasethavetheirviewcountrankedwithin
top-5%and45%ofthequestionsarerankedwithintop-1%among
the1.3millionjava-taggedquestionsonSO.Thisindicatesthatif
BIKERcansolvethesequestions,itcanbenefitalargenumberof
developers.WealsousedthedatasetpublishedbyRACKtodemon-
strate the effectiveness of BIKER. Another threat is that BIKER
only supports Java API recommendation. But this is an implemen-
tation limitation, rather than a methodological threat. It would not
be difficult to adapt BIKER to support API recommendation for
other programming languages, as long as we can obtain related SO
questions and API documentation.
Threatstoconstructvalidity relatestothesuitabilityofoureval-
uation measures. We use MRR and MAP, which are classical evalu-
ationmeasuresforinformationretrieval[ 25]andarealsowidely
usedinpreviousstudiesinsoftwareengineering[ 24,34,37,40,50].
8 RELATED WORK
APIRecommendation: InadditiontoRACKandDeepAPI,there
areotherapproachesforAPIrecommendation.McMillanetal.[ 26]
proposed Portfolio to find relevant functions for a code search
queryfromalargearchiveofC/C++sourcecode.Chanetal.[ 13]
furtherimprovedPortfoliobyemployinggraphsearchapproach.
Raghothamanetal.[ 33]proposed SWIM,atoolthatlearnscommon
API usage patterns from open-source code repositories and syn-
thesize idiomatic code describing the use of these APIs. In general,
these methods do not leverage information from Q&A websiteslike SO or do not incorporate information from SO and API doc-
umentation.Wedonotchoosethemasbaselinessincetheyhave
beenreportedaslessoptimalthanRACKorDeepAPI.Ontheother
hand, a number of previous studies (e.g., [ 7,12,14,22,29,51] have
proposed different approaches to recommend code snippets fora programming task described in natural language. We did notcompare BIKER with these approaches since we focus more on
therecommendationofaspecificAPI,whichisdifferentfromthe
granularity of code snippet recommendation.EmpiricalStudiesonDevelopers’Behaviors:
Inthispaper,we
conducted a survey to investigate developers’ API search behav-
iorsandexpectations.Anumberofpreviousstudiesalsofocused
on developers’ behaviors and some of their findings are relevantto ours [
8,11,16,36,43,44]. For example, in a study involving
twentydevelopers,Duala-EkokoandRobillard[ 16]identifieddif-
ferenttypesofquestionsthatarecommonlyaskedbydevelopers
whenworkingwithunfamiliarAPIsandtheyanalyzedthecause
of the difficulties when answering questions about the use of APIs.
Sadowski et al.[ 36] investigated howdeveloperssearch for code
through a case study at Google. They found that developers search
forcodeveryfrequentlyandgenerallyseekanswerstoquestions
abouthowtouseanAPI.Brandtetal.[ 11]observedthatdevelopers
mostlyleverageonlineresourcesforjust-in-timelearningofnew
skills,andtoclarifyorremindthemselvesofexistingknowledge.
Oursurveyservesasacomplementtothesestudies,sincewefocusondevelopers’APIsearchbehaviorsandwerevealtheinformation
seeking process when developers perform API search.MiningAPIUsages:
ManystudiesfocusedonminingAPIusages
to help developers learn how to use an API. Moreno et al. [ 28]proposed MUSEfor mining and ranking actual code examples that
show howto use aspecific method. MUSEcombines staticslicing
withclonedetection,andusesheuristicstoselectandrankthecode
examples in terms of reusability, understandability, and popularity.
Petrosyan et al. [ 31] proposed an approach to discover tutorial
sectionsthatexplainagivenAPItype.Treudeetal.[ 39]proposedan
approachtoautomaticallyaugmentAPIdocumentationwithusage
insights extracted from SO. Jiang et al. [ 23] proposed FRAPT,a n
unsupervised approach for discovering relevant tutorial fragments
for APIs. Nguyen et al. [ 30] proposed API2VEC which uses word
embedding to infer the semantic relations between APIs. Our workisacomplementtothesestudies,sincetheyassumethatdevelopers
already knowthe nameof an APIfor furtherinvestigation. These
approaches could be integrated in BIKER to improve the quality of
the supplementary information for the recommended APIs.Mining Developer Forums:
Researchers leveraged the rich re-
sourcesindeveloperforumstobuildtoolsforsoftwareengineering.
Barua et al. [ 9] used topic model to discover main topics discussed
inSO,aswellastheirrelationshipsandtrendsovertime.Treude
etal.’sstudy[ 38]onhowprogrammersaskandanswerquestions
onthewebfoundthatQ&Awebsitesareparticularlyeffectiveat
codereviewsandconceptualquestions.Gaoetal.[ 19]pr oposedan
approach to automatically fix recurring crash bugs by retrieving a
listofQ&Apagestogenerateeditscripts.Wongetal.[ 42]proposed
anapproachtoautomaticallygeneratecodecommentsbymining
commentsextractedfromQ&Asites.Ponzanellietal.[ 32]proposed
Prompter to automatically generate queries based on code context,
andretrievepertinentdiscussionsfromSO.Ourworkalsoleverages
developer discussionsin SO,but wefocus onrecommending APIs
for programming tasks.
9 CONCLUSION AND FUTURE WORK
In this paper, we propose BIKER to automatically recommend rele-
vantAPIs fora programming taskdescribedin naturallanguage.
Inspiredbytheinformationseekingprocessofdevelopers,welever-
age both Stack Overflow posts and API documentation to improve
the effectiveness of BIKER, and summarize supplementary infor-
mationforeachrecommendedAPItohelpdevelopersbetterunder-standtheAPIusageanddeterminetheirrelevancetothequerytask.
The evaluation with both our dataset and RACK’s dataset confirms
theeffectivenessofBIKER.OuruserstudydemonstratesthatBIKER
can help developers find the appropriate APIs more efficiently and
accurately in practice. In the future, wewill develop an automatic
tool(e.g.,aplugininawebbrowserorIDE)toenabledevelopersto
use BIKER to search APIs for programming tasks. We will further
improvetheperformanceofBIKERandtheinteractiondesignof
our tool as suggested by the participants in user study. Finally, we
will extend BIKER to support more programming languages.
ACKNOWLEDGMENTS
WewouldliketothankRahmanetal.andGuetal.forsharingtheir
toolsanddataset.Wealsoappreciatethereviewersfortheirinsight-
ful comments to help us improve this paper. Xin Xia and Xinyu
Wangarethecorrespondingauthors.Thisresearchwaspartially
supportedbytheNationalKeyResearchandDevelopmentProgram
of China (2018YFB1003904) and NSFC Program (No. 61602403).
302
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. API Method Recommendation without Task-API Knowledge Gap ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1] 2017. JavaSE8APIdocumentationdownloadingsite. http://www.oracle.com/t
echnetwork/java/javase/documentation/jdk8-doc-downloads-2133158.html.
[2]2017. Stack Overflow Data Dump. https://archive.org/download/stackexchange.
[3]2018. DeepAPI’s online demo. http://www.cse.ust.hk/~xguaa/deepapi/tooldemo.
html.
[4]2018. RACK’s dataset and tool demo. http://homepage.usask.ca/~masud.rahman/
rack/.
[5]2018. Stack Overflow question: Class.isInstance vs Class.isAssignableFrom.
https://stackoverflow.com/questions/3949260/java-class-isinstance-vs-class-i
sassignablefrom.
[6]HervéAbdi.2007. BonferroniandŠidákcorrectionsformultiplecomparisons.
Encyclopedia of measurement and statistics 3 (2007), 103–107.
[7]Miltos Allamanis, Daniel Tarlow, Andrew Gordon, and Yi Wei. 2015. Bimodal
modellingofsourcecodeandnaturallanguage.In InternationalConferenceon
Machine Learning. 2123–2132.
[8]LingfengBao,ZhenchangXing,XinXia,DavidLo,andAhmedEHassan.2018.
Inference of development activities from interaction with uninstrumented appli-
cations.Empirical Software Engineering 23, 3 (2018), 1313–1351.
[9] Anton Barua, Stephen W Thomas, and Ahmed E Hassan. 2014. What are devel-
operstalkingabout?ananalysisoftopicsandtrendsinstackoverflow. Empirical
Software Engineering 19, 3 (2014), 619–654.
[10]Steven Bird and Edward Loper. 2004. NLTK: the natural language toolkit. InProceedings of the ACL 2004 on Interactive poster and demonstration sessions.
Association for Computational Linguistics, 31.
[11]JoelBrandt,PhilipJGuo,JoelLewenstein,MiraDontcheva,andScottRKlemmer.
2009. Two studies of opportunistic programming: interleaving web foraging,
learning,andwritingcode.In ProceedingsoftheSIGCHIConferenceonHuman
Factors in Computing Systems. ACM, 1589–1598.
[12]BrockAngusCampbellandChristophTreude.2017. NLP2Code:Codesnippet
content assist via natural language tasks. In Software Maintenance and Evolution
(ICSME), 2017 IEEE International Conference on. IEEE, 628–632.
[13]Wing-Kwan Chan,Hong Cheng, andDavid Lo.2012. Searchingconnected API
subgraph via text phrases. In Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering. ACM, 10.
[14]Shaunak Chatterjee, Sudeep Juvekar, and Koushik Sen. 2009. Sniff: A search en-
gineforjavausingfree-formqueries.In InternationalConferenceonFundamental
Approaches to Software Engineering. Springer, 385–400.
[15]Norman Cliff. 2014. Ordinal methods for behavioral data analysis . Psychology
Press.
[16]EkwaDuala-EkokoandMartinPRobillard.2012.AskingandansweringquestionsaboutunfamiliarAPIs:Anexploratorystudy.In SoftwareEngineering(ICSE),2012
34th International Conference on. IEEE, 266–276.
[17]KayvonFatahalian,JeremySugerman,andPatHanrahan.2004. Understanding
theefficiencyofGPUalgorithmsformatrix-matrixmultiplication.In Proceedings
of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware . ACM,
133–137.
[18]JosephLFleiss.1971. Measuringnominalscaleagreementamongmanyraters.
Psychological bulletin 76, 5 (1971), 378.
[19]Qing Gao, Hansheng Zhang, Jie Wang, Yingfei Xiong, Lu Zhang, and Hong Mei.
2015. Fixing recurring crash bugs via analyzing q&a sites (T). In Automated
Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on.
IEEE, 307–318.
[20]Ian Goodfellow, Yoshua Bengio, AaronCourville,and Yoshua Bengio.2016. Deep
learning. Vol. 1. MIT press Cambridge.
[21]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2016. Deep
API learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo-
sium on Foundations of Software Engineering. ACM, 631–642.
[22]TihomirGveroandViktorKuncak.2015. Interactivesynthesisusingfree-form
queries.In SoftwareEngineering(ICSE),2015IEEE/ACM37thIEEEInternational
Conference on, Vol. 2. IEEE, 689–692.
[23]HeJiang,JingxuanZhang,ZhileiRen,andTaoZhang.2017. Anunsupervised
approachfordiscoveringrelevanttutorialfragmentsforAPIs.In Proceedingsof
the 39th International Conference on Software Engineering. IEEE Press, 38–48.
[24]An Ngoc Lam, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen. 2015.
Combiningdeeplearningwithinformationretrievaltolocalizebuggyfilesfor
bugreports(n).In AutomatedSoftwareEngineering(ASE),201530thIEEE/ACM
International Conference on. IEEE, 476–481.
[25]Christopher D Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Intro-
duction to Information Retrieval. Cambridge University Press.
[26]CollinMcMillan,MarkGrechanik,DenysPoshyvanyk,QingXie,andChenFu.
2011. Portfolio:finding relevantfunctions andtheir usage.In Proceedingsof the
33rd International Conference on Software Engineering. ACM, 111–120.
[27]TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111–3119.
[28]Laura Moreno, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, and
Andrian Marcus. 2015. How can I use this method?. In Software Engineering(ICSE),2015IEEE/ACM37thIEEEInternationalConferenceon,Vol.1.IEEE,880–
890.
[29]AnhTuanNguyen,PeterCRigby,ThanhVanNguyen,MarkKaranfil,andTienN
Nguyen. 2017. Statistical translation of English texts to API code templates.
InSoftware Engineering Companion (ICSE-C), 2017 IEEE/ACM 39th International
Conference on. IEEE, 331–333.
[30]TrongDucNguyen,AnhTuanNguyen,HungDangPhan,andTienNNguyen.
2017. Exploring API embedding for API usages and applications. In Software
Engineering (ICSE), 2017 IEEE/ACM 39th International Conference on. IEEE, 438–
449.
[31]GayanePetrosyan,MartinPRobillard,andRenatoDeMori.2015. Discovering
informationexplainingAPItypesusingtextclassification.In SoftwareEngineering
(ICSE),2015IEEE/ACM37thIEEEInternationalConferenceon,Vol.1.IEEE,869–
879.
[32]Luca Ponzanelli, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, and
Michele Lanza. 2014. Mining StackOverflow to turn the IDE into a self-confident
programming prompter. In Proceedings of the 11th Working Conference on Mining
Software Repositories. ACM, 102–111.
[33]Mukund Raghothaman, Yi Wei, and Youssef Hamadi. 2016. SWIM: Synthesizing
WhatIMean-CodeSearchandIdiomaticSnippetSynthesis.In SoftwareEngineer-
ing (ICSE), 2016 IEEE/ACM 38th International Conference on. IEEE, 357–367.
[34]Mohammad Masudur Rahman, Chanchal K Roy, and David Lo. 2016. Rack:
Automatic api recommendation using crowdsourced knowledge. In Software
Analysis, Evolution, and Reengineering (SANER), 2016 IEEE 23rd International
Conference on, Vol. 1. IEEE, 349–359.
[35]Radim Řehůřek and Petr Sojka. 2010. Software Framework for Topic Modelling
withLargeCorpora.In ProceedingsoftheLREC2010WorkshoponNewChallenges
for NLP Frameworks. ELRA, Valletta, Malta, 45–50. http://is.muni.cz/publication
/884893/en.
[36]CaitlinSadowski,KathrynTStolee,andSebastianElbaum.2015. Howdevelopers
search for code: a case study. In Proceedings of the 2015 10th Joint Meeting on
Foundations of Software Engineering. ACM, 191–201.
[37]Ripon K Saha, Matthew Lease, Sarfraz Khurshid, and Dewayne E Perry. 2013.
Improving bug localization using structured information retrieval. In Automated
SoftwareEngineering(ASE),2013IEEE/ACM28thInternationalConferenceon.IEEE,
345–355.
[38]Christoph Treude, Ohad Barzilay, and Margaret-Anne Storey. 2011. How doprogrammers ask and answer questions on the web?: Nier track. In Software
Engineering (ICSE), 2011 33rd International Conference on. IEEE, 804–807.
[39]Christoph Treude and Martin P Robillard. 2016. Augmenting api documentation
withinsightsfromstackoverflow.In SoftwareEngineering(ICSE),2016IEEE/ACM
38th International Conference on. IEEE, 392–403.
[40]MingWen,RongxinWu,andShing-ChiCheung.2016. Locus:Locatingbugsfrom
software changes. In Automated Software Engineering (ASE), 2016 31st IEEE/ACM
International Conference on. IEEE, 262–273.
[41]FrankWilcoxon.1945. Individualcomparisonsby rankingmethods. Biometrics
bulletin1, 6 (1945), 80–83.
[42]Edmund Wong, Jinqiu Yang, and Lin Tan. 2013. Autocomment: Mining question
and answer sites for automatic comment generation. In Automated Software
Engineering(ASE),2013IEEE/ACM28thInternationalConferenceon.IEEE,562–
567.
[43]Xin Xia, Lingfeng Bao, David Lo, Pavneet Singh Kochhar, Ahmed E Hassan, and
ZhenchangXing.2017. Whatdodeveloperssearchforontheweb? Empirical
Software Engineering 22, 6 (2017), 3149–3185.
[44]Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, and Shan-
ping Li. 2017. Measuring program comprehension: A large-scale field study with
professionals. IEEE Transactions on Software Engineering (2017).
[45]Xin Xia and David Lo. 2017. An effective change recommendation approach for
supplementary bug fixes. Automated Software Engineering 24, 2 (2017), 455–498.
[46]BowenXu,ZhenchangXing,XinXia,andDavidLo.2017. AnswerBot:automatedgenerationofanswersummarytodevelopersźtechnicalquestions.In Proceedings
ofthe32ndIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering .
IEEE Press, 706–716.
[47]Bowen Xu, Zhenchang Xing, Xin Xia, David Lo, Qingye Wang, and Shanping Li.
2016. Domain-specific cross-language relevant question retrieval. In Proceedings
ofthe13thInternationalConferenceonMiningSoftwareRepositories .ACM,413–
424.
[48]Xinli Yang, David Lo, Xin Xia, Lingfeng Bao, and Jianling Sun. 2016. Combining
wordembeddingwithinformationretrievaltorecommendsimilarbugreports.
In2016 IEEE 27th International Symposium on Software Reliability Engineering
(ISSRE). IEEE, 127–137.
[49]XinYe,HuiShen,XiaoMa,RazvanBunescu,andChangLiu.2016. Fromword
embeddings to document similarities for improved information retrieval in soft-
ware engineering. In Proceedings of the 38th international conference on software
engineering. ACM, 404–415.
[50]Motahareh Bahrami Zanjani, Huzefa Kagdi, and Christian Bird. 2016. Automati-
callyrecommendingpeerreviewersinmoderncodereview. IEEETransactions
on Software Engineering 42, 6 (2016), 530–543.
303
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, and Xinyu Wang
[51]Hongyu Zhang, Anuj Jain, Gaurav Khandelwal, Chandrashekhar Kaushik, Scott
Ge, and Wenxiang Hu. 2016. Bing developer assistant: improving developer
productivity byrecommending samplecode. In Proceedings ofthe 201624th ACMSIGSOFT International Symposium on Foundations of Software Engineering. ACM,
956–961.
304
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. 