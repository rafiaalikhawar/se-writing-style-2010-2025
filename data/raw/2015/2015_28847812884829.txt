Performance Issues and Optimizations in JavaScript:
An Empirical Study
Marija Selakovic
Department of Computer Science
TU Darmstadt, Germany
m.selakovic89@gmail.comMichael Pradel
Department of Computer Science
TU Darmstadt, Germany
michael@binaervarianz.de
ABSTRACT
As JavaScript is becoming increasingly popular, the per-
formance of JavaScript programs is crucial to ensure the
responsiveness and energy-eciency of thousands of pro-
grams. Yet, little is known about performance issues that
developers face in practice and they address these issues.
This paper presents an empirical study of 98 xed perfor-
mance issues from 16 popular client-side and server-side
JavaScript projects. We identify eight root causes of is-
sues and show that inecient usage of APIs is the most
prevalent root cause. Furthermore, we nd that most is-
sues are addressed by optimizations that modify only a few
lines of code, without signicantly aecting the complexity
of the source code. By studying the performance impact
of optimizations on several versions of the SpiderMonkey
and V8 engines, we nd that only 42.68% of all optimiza-
tions improve performance consistently across all versions
of both engines. Finally, we observe that many optimiza-
tions are instances of patterns applicable across projects, as
evidenced by 139 previously unknown optimization oppor-
tunities that we nd based on the patterns identied during
the study. The results of the study help application de-
velopers to avoid common mistakes, researchers to develop
performance-related techniques that address relevant prob-
lems, and engine developers to address prevalent bottleneck
patterns.
1. INTRODUCTION
JavaScript has become one of the most popular program-
ming languages. It is widely used not only for client-side web
applications, but also for server-side applications, mobile ap-
plications, and even desktop applications. The performance
of JavaScript code is crucial to ensure that applications re-
spond quickly to requests without consuming unnecessarily
high amounts of CPU-time and energy. For example, for
server-side code that may respond to thousands of requests
every second, even a relatively small performance improve-
ment can have a signicant impact on the overall through-
put and energy consumption. Likewise, client-side code that
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee. Request permissions from Permissions@acm.org.
ICSE ‚Äô16, May 14 - 22, 2016, Austin, TX, USA
Copyright is held by the owner/author(s). Publication rights licensed to
ACM.
ACM 978-1-4503-3900-1/16/05 ...$15.00.
DOI: http://dx.doi.org/10.1145/2884781.2884829.performs poorly can cause users to perceive an application
as unresponsive [34], which may encourage them to instead
use a competitor's web site.
The use of the language has evolved from simple client-
side scripts to complex programs, such as email clients, word
processors, and interactive games. This development has
been enabled by signicant improvements of JavaScript's
performance. The main reason for this evolutionary step
are the tremendous improvements of JavaScript engines in
recent years, e.g., due to highly optimized just-in-time (JIT)
compilers [8, 20, 13, 5, 1]. Despite the eectiveness of JIT
compilation, developers still apply optimizations to address
performance issues in their code, and future improvements
of JavaScript engines are unlikely to completely erase the
need for manual performance optimizations.1
For example, a common optimization is to replace a for-
inloop that iterates over the properties of an object oby
code that rst computes these properties using the built-in
Object.keys() function, and then iterates through them with
a traditional forloop. This optimization often improves
performance on the V8 engine because the JIT compiler may
refuse to optimize a function that contains a for-in loop if
the object ois internally represented as a hash map. In
contrast, the JIT compiler successfully optimizes the call of
Object.keys() and the traditional forloop.
Despite the importance of JavaScript's performance, little
is currently known about performance issues and optimiza-
tions in real-world JavaScript projects. This paper addresses
this problem and asks the following research questions:
RQ 1: What are the main root causes of performance
issues in JavaScript?
RQ 2: How complex are the changes that developers ap-
ply to optimize their programs?
RQ 3: What is the performance impact of optimizations?
RQ 4: Are optimizations valid across JavaScript engines,
and how does the performance impact of optimizations
evolve over time?
RQ 5: Are there recurring optimization patterns, and can
they be applied automatically?
Answering these questions helps improve JavaScript's per-
formance by providing at least three kinds of insights. First,
application developers benet by learning from mistakes done
1We refer to a source code change that a developer applies
to improve performance as an optimizations , and we refer
to an automatically applied transformation, e.g., in a JIT
compiler, as a compiler optimization .
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   61
by others. Second, developers of performance-related pro-
gram analyses and proling tools benet from better under-
standing what kinds of problems exist in practice and how
developers address them. Third, developers of JavaScript
engines benet from learning about recurring bottlenecks
that an engine may want to address and by better under-
standing how performance issues evolve over time.
To address these questions, we present an empirical study
of performance issues and optimizations in real-world Java-
Script projects. The study involves 98 xed issues that de-
velopers have documented in bug tracking systems. The is-
sues come from 16 JavaScript projects, including both client-
side and server-side code, popular libraries, and widely used
application frameworks.
Our main ndings are the following:
The most prevalent root cause of performance issues (52%)
is that JavaScript provides APIs that are functionally
equivalent but provide dierent performance, and that de-
velopers often use these APIs in a suboptimal way. This
nding suggests that developers need guidance in choos-
ing between such APIs, and that future language and API
designs may want to reduce the amount of redundancy of
APIs.
Many optimizations aect a small number of source code
lines: 28% and 73% of all optimizations aect less than 5
and 20 lines, respectively.
Many optimizations do not signicantly aect the com-
plexity of the source code: 37.11% of all optimizations
do not change the number of statements and 47.42% of
all optimizations do not change the cyclomatic complex-
ity of the program. This nding challenges the common
belief that improving the performance of a program often
implies reducing its understandability and maintainabil-
ity [17, 6].
Only 42.68% of all\optimizations"provide consistent per-
formance improvements across all studied JavaScript en-
gines. A non-negligible part (15.85%) of changes even
degrades performance on some engines. These ndings
reveal a need for techniques to reliably measure perfor-
mance and to monitor the performance eect of changes
across multiple execution environments.
Many optimizations are instances of recurring patterns
that can be re-applied within the same project and even
across projects: 29 of the 98 studied issues are instances of
patterns that reoccur within the study. Furthermore, we
nd 139 previously unreported instances of optimization
patterns in the studied projects.
Most optimizations cannot be easily applied in a fully
automatic way, mostly due to the dynamism of Java-
Script. We identify ve kinds preconditions for safely ap-
plying recurring optimization patterns. Statically check-
ing whether these preconditions are met is a challenge.
Our results suggest a need for tools that help developers
applying recurring optimizations.
In summary, this paper contributes the following:
The rst in-depth study of JavaScript performance issues
and optimizations that developers apply to x them.
A documented set of 98 reproduced issues that may serve
as a reference point for work on nding and xing per-formance bottlenecks. All data and code gathered during
the study is available for download.2
Evidence that developers need tools and techniques to
improve performance, e.g., to choose between redundant
APIs, to apply recurring optimization patterns, and to
reliably measure performance improvements.
2. METHODOLOGY
This section summarizes the subject projects we use in
the empirical study, our criteria for selecting performance
issues, and our methodology for evaluating the performance
impact of the optimizations applied to address these issues.
2.1 Subject Projects
We study performance issues from widely used JavaScript
projects that match the following criteria:
Project type. We consider both node.js projects and client-
side frameworks and libraries.
Open source. We consider only open source projects to
enable us and others to study the source code involved
in the performance issues.
Popularity . For node.js projects, we select modules that
are the most depended-on modules in the npm reposi-
tory.3For client-side projects, we select from the most
popular JavaScript projects on GitHub.
Number of reported bugs . We focus on projects with a
high number of pull requests ( 100) to increase the
chance to nd performance-related issues.
Table 1 lists the studied projects, their target platforms,
and the number of lines of JavaScript code. Overall, we
consider 16 projects with a total of 63,951 lines of code.
2.2 Selection of Performance Issues
We select performance issues from bug trackers as follows:
1.Keyword-based search or explicit labels . One of the stud-
ied projects, Angular.js, explicitly labels performance is-
sues, so we focus on them. For all other projects, we
search the title, description, and comments of issues for
performance-related keywords, such as\performance",\op-
timization", \responsive", \fast", and \slow".
2.Random selection or inspection of all issues . For the
project with explicit performance labels, we inspect all
such issues. For all other projects, we randomly sample
at least 15 issues that match the keyword-based search,
or we inspect all issues if there are less than 15 matching
issues.
3.Conrmed and accepted optimizations . We consider an
optimization only if it has been accepted by the develop-
ers of the project and if it has been integrated into the
code repository.
4.Reproducibility . We study a performance issue only if we
succeed in executing a test case that exercises the code lo-
cation lreported to suer from the performance problem.
We use of the following kinds of tests:
A test provided in the issue report that reproduces the
performance problem.
2https://github.com/marijaselakovic/
JavaScriptIssuesStudy
3https://www.npmjs.com/browse/depended
62Table 1: Projects used for the study.
Project Description Kind of platform Total LoC Number of issues
Angular.js MVC framework Client 7,608 27
jQuery Client-side library Client 6,348 9
Ember.js MVC framework Client 21,108 11
React Library for building reactive user interfaces Client 10,552 5
Underscore Utility library Client and server 1,110 12
Underscore.string String manipulation Client and server 901 3
Backbone MVC framework Client and server 1,131 5
EJS Embedded templates Client and server 354 3
Moment Date manipulation library Client and server 2,359 3
NodeLruCache Caching support library Client and server 221 1
Q Library for asynchronous promises Client and server 1,223 1
Cheerio jQuery implementation for server-side Server 1,268 9
Chalk Terminal string styling library Server 78 3
Mocha Testing framework Server 7,843 2
Request HTTP request client Server 1,144 2
Socket.io Real-time application framework Server 703 2
Total 63,951 98
A unit test published in the project's repository that
exercises l.
A newly created unit test that calls an API function
that triggers l.
A microbenchmark that contains the code at l, possibly
prexed by setup code required to exercise the location.
5.Split changes into individual optimizations . Some issues,
such as complaints about the ineciency of a particular
function, are xed by applying multiple independent opti-
mizations. Because our study is about individual perfor-
mance optimizations, we consider such issues as multiple
issues, one for each independent optimization.
6.Statistically signicant improvement . We apply the test
that triggers the performance-critical code location to the
versions of the project before and after applying the op-
timization. We measure the execution times and keep
only issues where the optimization leads to a statistically
signicant performance improvement.
The rationale for focusing on unit tests and microbench-
marks in step 4 is twofold. First, JavaScript developers ex-
tensively use microbenchmarks when deciding between dif-
ferent ways to implement some functionality.4Second, most
projects we study are libraries or frameworks, and any mea-
surement of application-level performance would be strongly
inuenced by our choice of the application that uses the li-
brary or framework. Instead, focusing on unit tests and mi-
crobenchmarks allows us to assess the performance impact
of the changed code while minimizing other confounding fac-
tors.
In total, we select and study 98 performance issues, as
listed in the last column of Table 1.
2.3 Performance Measurement
Reliably measuring the performance of JavaScript code
is a challenge, e.g., due to the inuence of JIT compilation,
garbage collection, and the underlying operating system. To
evaluate to what extent an applied optimization aects the
program's performance we adopt a methodology that was
previously proposed for Java programs [10]. In essence, we
repeatedly execute each test in NVMnewly launched VM
4For example, jsperf.com is a popular microbenchmarking
web site.instances. At rst, we perform NwarmUp test executions in
each VM instance to warm up the JIT compiler. Then, we
repeat the test Nmeasure more times and measure its exe-
cution times. To determine whether there is a statistically
signicant dierence in execution time between the original
and the optimized program we compare the sets of mea-
surements Mbefore andMafterfrom before and after the op-
timization. If and only if the condence intervals of Mbefore
andMafterdo not overlap, we consider the dierence to be
statistically signicant. Based on preliminary experiments
we use NwarmUp = 5,Nmeasure = 10, and NVM= 5, because
these parameters repeat measurements suciently often to
provide stable performance results. We set the condence
level to 95%. Because very short execution times cannot be
measured accurately, we wrap each test in a loop so that it
executes for at least 5ms. All experiments are performed on
an Intel Core i7-4600U CPU (2.10GHz) machine with 16GB
of memory running Ubuntu 14.04 (64-bit).
2.4 JavaScript Engines
JavaScript engines evolve quickly, e.g., by adding novel
JIT optimizations [8, 20, 13, 5, 1] or by adapting to trends
in JavaScript development5. To understand how the per-
formance impact of an optimization evolves over time, we
measure the performance of tests on multiple engines and
versions of engines. Table 2 lists the engines we consider.
We focus on the two most popular engines: V8, which is
used, e.g., in the Chrome browser and the node.js platform,
and SpiderMonkey, which is used, e.g., in the Firefox browser
and the GNOME desktop environment. For each engine, we
use at least three dierent versions, taking into account only
versions that are published after introducing JIT compila-
tion, and including the most recent published version. All
considered versions are published in dierent years and we
take engines for which their version number indicates that
the engine potentially introduces signicant changes com-
pared to the previous selected version. Table 2 lists for each
engine which types of projects it supports. We execute the
tests of a project on all engines that match the kind of plat-
form (client or server), as listed in column three of Table 1.
5http://asmjs.org
63Table 2: JavaScript engines used to the test performance
impact of optimizations (SM = SpiderMonkey).
Engine ver-
sionPlatform Project type
24 Firefox Client
31 Firefox ClientSM39 Firefox Client
3.14 Node.js Server
3.19 Chrome ClientV83.6 Chrome and node.js Client and server
4.2 Chrome and io.js Client and server
(a) Most prevalent root causes.
 0 10 20 30 40 50 60
Inefficient API usage Inefficient iteration Repeated execution Inefficient or unnecessary copying Special cases API reimplementation Repeated checks Too generic API OtherPercentage of issues     
(b) APIs that are used ineciently.
 0 5 10 15 20 25 30 35
Reflection Strings DOM Arrays Project-internal Other built-in Other third-partyPercentage of issues          
 within "Inefficient API usage"          
Figure 1: Root causes of performance issues.
3. RQ 1: ROOT CAUSES OF
PERFORMANCE ISSUES
This section addresses the question which root causes real-
world performance issues have. To address this question, we
identify eight root causes that are common among the 98
studied issues, and we assign each issue to one or more root
cause. Figure 1 summarizes our ndings, which we detail in
the following.
3.1 API-related Root Causes
The root cause of 65 performance issues is related to how
the program uses or does not use an API. We identify three
specic root causes related to API usage.
InefÔ¨Åcient API Usage.
The most common root cause (52% of all issues), is that
an API provides multiple functionally equivalent ways to
achieve the same goal, but the API client does not use the
most ecient way to achieve its goal. For example, thefollowing code aims at replacing all quotes in a string with
escaped quotes, by rst splitting the string into an array
of substrings and then joining the array elements with the
quote character:6
.. = str . split ("'"). join ("\\'");
The optimization is to use a more ecient API. For the
example, the developers modify the code as follows:
.. = str . replace (/ '/g, "\\ '");
Inecient API usage is the most prevalent root cause,
with a total of 50 issues. Figure 1b further classies these
issues by the API that is used ineciently. The most com-
monly misused APIs are reection APIs, such as runtime
type checks, invocations of function objects, and checks whe-
ther an object has a particular property. The second most
common root cause is inecient use of string operations,
such as the above example.
InefÔ¨Åcient Reimplementation.
The root cause of 8% of all issues is that the program
implements some functionality that is already implemented
in a more ecient way, e.g., as part of the built-in API.
The optimization applied to avoid such performance issues
is to use the existing, more ecient implementation. For
example, Angular.js had implemented a map() function that
applies a given function to each element of an array. Later,
the developers optimize the code by using the built-in Ar-
ray.prototype.map() function, which implements the same
functionality.7
Generic API is InefÔ¨Åcient.
Another recurring root cause (7% of all issues) is to use
an existing API that provides a more generic, and therefore
less ecient, functionality than required by the program.
For example, given a negative number n, the following code
accesses thejnjth-to-last element of the array arr:8
arr . slice (n) [0]
The code is correct but inecient because slice() copies
parts of the array into another array, of which only the rst
element is used. The optimization applied to avoid such
performance issues is to implement the required function-
ality without using the existing API. For the example, the
developers improve performance by directly accessing the
required element:
arr [ arr . length + n]
3.2 Other Root Causes
Besides API-related problems, we identify six other com-
mon causes of performance issues.
InefÔ¨Åcient Iteration.
JavaScript provides various ways to iterate over data col-
lections, such as traditional forloops, for-in loops, and the
Array.prototype.forEach() method. A common root cause
of poor performance (18% of all issues) is that a program
iterates over some data in an inecient way. The optimiza-
tion applied to avoid such performance issues is to iterate
in a more ecient way. For example, the following code
iterates through all properties of argusing a for-in loop:9
6Issue 39 of Underscore.js.
7Issue 9067 of Angular.js.
8Issue 102 of jQuery.
9Issue 11338 of Ember.js.
64for ( var prop in arg ) {
if ( arg . hasOwnProperty ( prop )) {
// use prop
}
}
This iteration is inecient because it requires to check whe-
ther the property is indeed dened in argand not inherited
from arg's prototype. To avoid checking each property, the
developers optimize the code by using Object.keys(), which
excludes inherited properties:
var updates = Object . keys (arg );
for ( var i = 0, l = updates . length ; i < l; i++) {
var prop = updates [i];
// use prop
}
Repeated Execution of the Same Operations.
13% of all issues are caused by a program that repeat-
edly performs the same operations, e.g., during dierent calls
of the same function. For example, the following code re-
peatedly creates a regular expression and uses it to split a
string:10
function on(events , ...) {
events = events . split (/\ s +/);
...
}
The code is inecient because creating the regular expres-
sion is an expensive operation that is repeatedly executed.
The optimization applied to avoid such performance issues
is to store the results of the computation for later reuse,
e.g., through memoization [42]. For the above example, the
developers compute the regular expression once and store it
into a variable:
var eventSplitter = /\s +/;
function on(events , ...) {
events = events . split ( eventSplitter );
...
}
Unnecessary or InefÔ¨Åcient Copying of Data.
Another recurrent root cause (12% of all issues) is to copy
data from one data structure into another in an inecient
or redundant way. The optimization applied to avoid such
performance issues is to avoid the copying or to implement it
more eciently. For example, a function in Angular.js used
to copy an array by explicitly iterating through it and by ap-
pending each element to a new array.11The developers opti-
mize this code by using the built-in Array.prototype.slice()
method, which is a more ecient way to obtain a shallow
copy of an array.
A Computation Can Be SimpliÔ¨Åed or Avoided in Spe-
cial Cases.
10% of all issues are due to code that performs a com-
putation that is unnecessarily complex in some special case.
The optimization applied to avoid such performance issues
is to check for the special case and to avoid or to simplify
the computation. For example, the developers of Angular.js
used JSON.stringify(value) to obtain a string representation
of a value. However, the value often is a number and calling
stringify() is unnecessarily complex in this case.12The de-
velopers optimize the code by checking the runtime type of
10Issue 1097 of Backbone.
11Issue 9942 of Angular.js.
12Issue 7501 of Angular.js.the value and by using the much cheaper implicit conversion
into a string, ""+value , when the value is a number.
Repeated Checks of the Same Condition.
Several issues (8%) are because the program repeatedly
checks the same condition, even though some of the checks
could be avoided. For example, the following code repeat-
edly checks whether a given object is a function, which is
inecient because the object cannot change between the
checks.13
function invoke (obj , method ) {
_.map (obj , function ( value ) {
isFunc = _. isFunction ( method )
...
});
}
The optimization applied to avoid such performance issues
is to refactor the control ow in such a way that the check is
performed only once. For the above example, the developers
hoist the isFunction() check out of the map() call.
Our analysis shows that various performance issues can be
mapped to a relatively small number of recurring root causes.
Some but not all of these root causes have been addressed by
existing approaches on automatically nding performance
problems [11, 42, 43]. Our results suggest that there is a
need for additional techniques, in particular, to help devel-
opers choose among multiple functionally equivalent ways
to use an API.
4. RQ 2: COMPLEXITY OF
OPTIMIZATIONS
This section addresses the question how complex the source
code changes are that developers apply to optimize their pro-
grams. To address this question, we analyze the project's
code before and after each optimization. We study both the
complexity of the changes themselves (Section 4.1) and to
what degree applying these changes aects the complexity
of the program's source code (Section 4.2).
4.1 Complexity of Changes
To assess the complexity of changes applied as optimiza-
tions, we measure for each change the number of aected
lines of source code, i.e., the sum of the number of removed
lines and the number of added lines. To avoid biasing these
measurements towards particular code formatting styles, we
apply them on a normalized representation of the source
code. We obtain this representation by parsing the code
and pretty-printing it in a normalized format that does not
include comments.
We nd that optimizations aect between 2 and 145 lines
of JavaScript source code, with a median value of 10. Fig-
ure 2 shows the cumulative sum of the number of aected
lines per change, i.e., how many optimizations are achieved
with less than a particular number of aected lines. The
graphs shows that 73% of all optimizations aect less than
20 lines of code, and that 28% of all optimizations aect
even less than 5 lines of code. We conclude from these re-
sults that a signicant portion of optimizations are possible
with relatively simple changes, which empirically conrms
an assumption made by prior research on performance bug
detection [16, 30, 29].
13Issue 928 of Underscore.js.
65 0 20 40 60 80 100
 0  20  40  60  80  100  120  140  160Percentage of optimizations
Affected lines (cumulative sum)Figure 2: Number of source code lines that are aected by
optimizations.
(a) Eect on the number of
statements.
 0 5 10 15 20 25 30 35 40
-Infinity..-11-10..-4-3..-10..01..34..1010..InfinityPercentage of optimizations
Difference in number of statements(b) Eect on cyclomatic
complexity.
 0 10 20 30 40 50
-Infinity..-11-10..-4-3..-10..01..34..1010..InfinityPercentage of optimizations
Difference in cyclomatic complexity
Figure 3: Eect of applying an optimization on the cyclo-
matic complexity.
4.2 Change in Complexity of Program
To understand to what degree optimizations inuence the
complexity of the source code of the optimized program, we
measure the number of statements in the program and the
cyclomatic complexity [24] of the program before and after
each change. These metrics approximate the understand-
ability and maintainability of the code. For each change,
we obtain the metric before and after the change, nbefore
andnafter, and we summarize the eect of the change as
nafter nbefore . A positive number indicates that the change
increases the complexity of the program because the changed
program contains additional statements or increases the cy-
clomatic complexity, whereas a negative number indicates
that the program becomes less complex due to the change.
Figures 3a and 3b summarize our results. The graphs
show what percentage of optimizations aect the number
of statements and the cyclomatic complexity in a particu-
lar range. For example, Figure 3a shows that 24% of all
optimizations add between one and three statements to the
program. We nd that a large portion of all optimizations
do not aect the number of statements and the cyclomatic
complexity at all: 37.11% do not modify the number of state-
ments, and 47.42% do not modify the cyclomatic complexity.
A manual inspection of these optimizations shows that they
modify the code in minor ways, e.g., by moving a state-
ment out of a loop, by adding an additional subexpression,
or by replacing one function call with another. It is also
interesting to note that a non-negligible percentage of opti-
mizations decreases the number of statements (19.59%) and
-60-40-20 0 20 40 60 80 100
Inefficient API usage Inefficient reimplementation Generic API Inefficient iteration Repeated execution Inefficient copying Special cases Repeated checksPerformance impact (%)Performance improvements per root causeFigure 4: Performance improvements obtained by optimiza-
tions per root cause. (The bottom and the top of the box
indicate the rst 25% and 75% of the data, and the middle
line indicates the median value. Vertical lines extend the
bottom and the top of the box to indicate the minimum and
the maximum values.)
the cyclomatic complexity (14.43%). These results challenge
the common belief that optimizations come at the cost of
reduced code understandability and maintainability [17, 6].
We conclude from these results that many optimizations are
possible without increasing the complexity of the optimized
program.
5. RQ 3: PERFORMANCE IMPACT OF
OPTIMIZATIONS
The following addresses the question which performance
impact developers achieve by optimizing their programs. To
address this question, we execute the tests of all 98 opti-
mizations on all considered JavaScript engines where the
respective code can be executed (Section 2). In total, we
obtain 568 performance improvement results.
Figure 4 shows the performance results obtained by op-
timizations for each root cause. The gure illustrates that
optimizations lead to a wide range of improvements, with
the majority of optimizations saving between 25% and 70%
of the execution time. Perhaps surprisingly, the gure shows
that some optimizations cause a performance degradation.
We further analyze these cases in Section 6. In general, the
performance results depend on the tests we use, and we can
not generalize the results for a specic root cause. For exam-
ple, the optimizations tested with microbenchmarks usually
yield large improvements, because they run small snippets
of code.
Given these results and the results from Section 4, one
may wonder whether there is any correlation between the
\pain"and the\gain"of optimizations, i.e., between the num-
ber of lines aected by a change and the performance im-
provement that the change yields. To address this question,
Figure 5 shows the relation between these two metrics for all
issues. The gure does not show any correlation (Pearson's
correlation coecient: 5.85%).
We draw three conclusions from our results. First, devel-
opers apply some optimizations even though the achieved
performance impact is relatively small. This strategy seems
66 20 40 60 80 100
 0  20  40  60  80  100  120  140  160Max. performance improvement (%)
Affected linesFigure 5: Relation between the number of lines aected by
a change and the achieved performance improvement.
SpiderMonkey
+ +0 +- 0- -
V8+ 42.68 8.5 0 0 0
+0 13.4 19.5 1.2 3.7 0
+- 4.9 0 3.7 1.2 0
0- 1.2 0 0 { {
- 0 0 0 { {
Table 3: Percentage of optimizations that result in positive
(+), positive or no (+0), positive or negative (+-), no or
negative (0-), and negative (-) speedup in V8 and Spider-
Monkey.
reasonable, e.g., when the modied code is in a heavily used
library function, but may also be a sign for \premature op-
timizations" [17]. Second, developers apply some optimiza-
tions even though these optimizations cause a performance
degradation on some JavaScript engines, either consciously
or without knowing what impact a change has. Third, some
optimizations lead to signicant savings in execution time,
and future work on proling should pinpoint such optimiza-
tion opportunities to developers.
6. RQ 4: CONSISTENCY ACROSS
ENGINES AND VERSIONS
Since dierent JavaScript engines apply dierent optimiza-
tions, changing code to improve performance in one engine
risk to degrade it in another engine. Furthermore, since en-
gines evolve quickly, an optimization applied to speed up
the program in one version of an engine may have the op-
posite eect in another version of the same engine. To as-
sess to what degree developers struggle with these risks, this
section addresses the question how consistent performance
improvements are across dierent JavaScript engines and
across multiple versions of the same engine.
Similar to RQ 3, we address this question by measuring
the performance impact of the performance issues in all con-
sidered JavaScript engines. Since we want to compare per-
formance impacts across engines, we include only issues that
we can execute in both V8 and SpiderMonkey, i.e., we ex-
clude non-browser optimizations. In total, we consider 82
issues for this research question.
6.1 Consistency Across Engines
Table 3 compares the performance impact of changes in
the V8 and SpiderMonkey engines. For each engine, the
table distinguishes ve cases: + means that a change im-
-80-60-40-20 0 20 40 60 80
3.14 3.19 3.6 4.2Performance improvement (%)
Versions of V8 
-80-60-40-20 0 20 40 60 80
24 31 39Performance improvement (%)
Versions of SpiderMonkeyFigure 6: Performance improvement of changes in dierent
versions of engines. Each line represents one performance
optimization (same line style in both gures means the same
optimization).
proves performance in all versions of the engine, +0 means
that the change improves performance or does not aect per-
formance, +- means that the change improves performance
in some version but degrades it in another version, 0- means
no performance change or a performance degradation, and
nally, - means a negative performance impact in all ver-
sions. For each combination of these ve cases, the table
shows the percentage of changes that fall into the respective
category. Because the study includes only issues that pro-
vide an improvement in at least one engine (Section 2.2),
the four cases in the bottom right corner of the table cannot
occur.
We nd that only 42.68% of all changes speed up the pro-
gram in all versions of both engines, which is what devel-
opers hope for when applying an optimization. Even worse,
15.85% of all changes degrade the performance in at least
one engine, i.e., a change supposed to speed up the program
may have the opposite eect. Interestingly, a non-negligible
percentage of changes speed up the program in one engine
but cause slowdown in another. For example, 4.9% of all
changes that increase performance in all versions of Spider-
Monkey cause a slowdown in at least one version of V8.
Some changes even degrade performance in some versions of
both engines. For example, 3.7% of all changes may have a
positive eect in V8 but will either decrease or do not aect
performance in SpiderMonkey.
6.2 Consistency Across Versions of an Engine
To better understand how the performance impact of a
change evolves across dierent versions of a particular en-
gine, Figure 6 shows the speedups of individual changes in
the V8 (top) and SpiderMonkey (bottom) engines. For read-
ability, the gure includes only those 15.85% of all changes
that cause a slowdown in at least one version of some en-
67gine. The graphs show that performance can dier signi-
cantly across dierent versions. For example, a change that
provides an almost 80%-speedup in version 3.6 of the V8 en-
gine causes a non-negligible slowdown in version 4.2 of the
same engine. The graphs also show that the performance
impact of a change sometimes evolves in a non-monotonic
way. That is, a benecial optimization may turn into a
performance degradation and then again into a benecial
optimization.
In summary, our results show that performance is a moving
target. This nding motivates future work that supports
developers in achieving satisfactory performance despite the
heterogeneity of JavaScript engines, such as techniques to
decide when to apply an optimization, to reliably and au-
tomatically measure the performance eect of a change, to
track the eect of an optimization over time, to undo an
optimization that turns out to be counterproductive in a
new engine, and to specialize JavaScript code for particular
engines.
7. RQ 5: RECURRING OPTIMIZATION
PATTERNS
The following addresses the question whether there are
recurring optimization patterns and whether they can be
applied automatically. To address the rst part of the ques-
tion, we manually identify a set of optimizations that can
be re-applied across several projects and semi-automatically
search for instances of these optimization patterns beyond
the 98 studied issues (Section 7.1). To address the second
part of the question, we identify pre-conditions for apply-
ing the recurring optimization patterns in a fully automated
way (Section 7.2).
7.1 Prevalence of Recurring Optimization
Patterns
To identify performance optimizations that may apply in
more than a single situation we inspect all 98 issues. First,
we identify optimizations that occur repeatedly within the
study (Table 4, Patterns 1{5). Second, since the 98 studied
issues may not expose multiple instances of a pattern that
would occur repeatedly in a larger set of issues, we also iden-
tify patterns that may occur repeatedly (Table 4, Patterns
6{10). For each optimization pattern in Table 4, we provide
a short description, as well as an example of code before and
after the optimization.
In Table 5, the numbers before the vertical lines show
how often each optimization pattern occurs within the 98
issues considered in the study. For instance, there are two
instances of Pattern 1 (avoid for-in loops) in the Angular.js
project. The last column shows that the studied issues con-
tain eight optimizations that match Pattern 1. In total, 29
of the 98 studied optimizations match one of the 10 opti-
mization patterns.
To study whether there are occurrences of the optimiza-
tion patterns beyond the 98 studied optimizations, we de-
velop a simple, AST-based, static analysis for each pattern in
Table 4. Each such analysis performs an AST traversal of a
JavaScript program to nd matches of the patterns. Due to
the well-known diculties of statically analyzing JavaScript
in a sound way (Section 7.2), the analyses cannot guarantee
that the optimization patterns can indeed be applied at the
identied code locations without changing the program's se-Table 5: Instances of recurring optimization patterns (a jb,
a = number of pattern instances in the 98 studied issues, b
= number of previously unreported pattern instances found
with static analysis).
Id ProjectsEmber
React
Less
Mocha
Angular
Underscore.string
EJS
Socket.io
Moment
Cheerio
Underscore
Request
Chalk
11j20 0 j21 0 j12 1 j0 2 j2 0 j0 0 j0 0 j1 0 j0 0 j0 4 j0 0 j0 0 j08j56
20j0 0 j0 0 j1 0 j0 0 j1 0 j0 6 j1 0 j0 0 j0 0 j0 4 j0 0 j0 0 j06j3
30j6 0 j1 0 j3 0 j6 0 j2 2 j7 0 j7 0 j1 0 j0 0 j1 0 j0 0 j0 0 j12j35
40j0 0 j0 0 j0 0 j0 6 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j06j0
50j2 0 j0 0 j0 0 j0 2 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j02j2
60j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 1 j0 0 j0 0 j01j0
70j9 0 j0 0 j1 1 j4 0 j0 0 j0 0 j0 0 j0 0 j3 0 j2 0 j2 0 j0 0 j01j21
80j2 0 j0 0 j0 0 j0 1 j1 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j01j3
90j0 0 j0 0 j1 1 j0 0 j0 0 j0 0 j0 0 j0 0 j0 0 j2 0 j0 0 j2 0 j01j5
100j1 0 j0 0 j2 0 j0 0 j3 0 j1 0 j0 0 j5 0 j2 0 j0 0 j0 0 j0 1 j01j14
1j40 0 j22 0 j20 3 j10 11 j9 2 j8 6 j8 0 j7 0 j5 2 j5 5 j2 0 j2 1 j129j139
mantics. To enable us to manually check whether a match is
a valid optimization opportunity, the analyses also rewrite
the program by applying the respective optimization pat-
tern. We then manually inspect the rewritten program and
prune changes that would modify the program's semantics.
We apply the ten analyses to the current version of each
of project. In total, the analyses suggest 142 optimizations,
of which we manually prune 3 changes because they break
the semantics of the program. In Table 5, the numbers after
the vertical lines show how many previously unreported op-
timization opportunities the analyses nd for each project.
We omit four projects for which we do not nd any match.
In total, we nd 139 instances of recurring optimization
patterns. The most common patterns are Pattern 1 (avoid
for-in loop), Pattern 3 (use implicit string conversion), and
Pattern 7 (use instanceof ). For two patterns (4 and 6), the
analyses do not nd any previously unreported instances.
The results show that patterns are recurring within a project.
For example, Pattern 1 has been applied only once in the
Ember project by the developers, but we nd 21 additional
code locations that oer the same optimization opportu-
nity. Furthermore, the results show that recurring patterns
can be applied across projects. For example, the only in-
stance of Pattern 7 within the 98 studied issues is in the
Mocha project, but there are 17 additional instances in other
projects. We are reporting optimization opportunities to the
developers and some of them have already been applied in-
corporated into the projects.14
We conclude from these results that there is a need for
tools and techniques that help developers apply an already
performed optimization at other code locations, both within
the same project and across projects, possibly along the lines
of existing work [25, 26, 4].
7.2 Preconditions For Automatic
Transformation
The second part of the RQ 5 is whether recurring opti-
mization patterns can be applied automatically. Answering
14See https://github.com/marijaselakovic/
JavaScriptIssuesStudy for an updated list of reported
issues.
68Table 4: Recurring optimization patterns and pre-conditions for applying them (T = Type check, NF = Native function is
not overridden, P = Prototype is not overridden, TF = Function from third-party library is not overridden, V = Check on
value of an expression).
Id Description Example Preconditions
Before After T NF P TF V
1 Prefer Object.keys() over comput-
ing the properties of an object
with a for-in loop.for ( var key in obj ) {
if ( obj . hasOwnProperty (key ))
{ ... }
}var keys = Object . keys ( obj );
for ( var i=0, l= keys . length ;
i<l; i++) {
var key = keys [i];
...
}l l l m m
2 To extract a substring of length
one, access the character directly
instead of calling substr() .str . substr (i, 1) str [i]lmlm m
3 To convert a value into a string,
use implicit type conversion in-
stead of String() .starts = String ( starts ); starts = '' + starts ;mlmlm
4 Use jQuery's empty() instead of
html('') .body . html (''); body . empty ();lm m lm
5 Use two calls of charAt() instead of
substr() .key . substr (0, 2) !== '$$ ' key . charAt (0) !== '$' &&
key . charAt (1) !== '$'l l l m m
6 To replace parts of a string with
another string, use replace() in-
stead of split() and join().str . split ("'"). join ("\\'") str . replace (/ '/g, "\\ '")l l l m m
7 Instead of checking an object's
type with toString() , prefer the in-
stanceof operator.if ( toString . call (err )
=== "[ object Error ]") ...if ( err instanceof Error ||
toString . call ( err )
=== "[ object Error ]") ...ml l ml
8 For even/odd checks of a number
use &1instead of %2.index % 2 == 0 index & 1 == 0lm m m m
9 Prefer forloops over functional-
style processing of arrays.styles . reduce (
function (str , name ) {
return ...;
}, str );for ( var i =0; i< styles . length ;
i++) {
var name = styles [i];
str = ...; }
return str ;l l l m m
10 When joining an array of strings,
handle single-element arrays e-
ciently.[]. slice . call ( arguments )
. join (' ');arguments . length === 1 ?
arguments [0] + '' :
[]. slice . call ( arguments )
. join (' ');lmlm m
this question is an important rst step for developing tech-
niques that help developers nd optimization opportunities
based on recurring patterns, and for developers of JIT en-
gines who may want to address some of these patterns in the
engine. To address the question, we identify for each opti-
mization pattern the preconditions that must be satised to
safely apply the optimization in an automatic way. We nd
the following kinds of preconditions:
Type check . Check the type of an identier or expression.
For example, the objidentier in Pattern 1 must have
type Object.
Native function is not overridden . Check that a built-in
JavaScript function is not overridden. For example, in
Pattern 1, both hasOwnProperty() and keys() must be the
built-in JavaScript functions.
Prototype is not overridden . Check that particular prop-
erties of the prototype are not overridden. For exam-
ple, for Pattern 1, the hasOwnProperty property of Ob-
ject.prototype must not be overridden.
Function from third-party library is not overridden . Check
that a function from a third-party library is not overrid-den. For example, the html() and empty() functions in
Pattern 4 must be functions from the jQuery library.
Check the value of an expression . Check whether an ex-
pression has a particular value. For example, to apply
Pattern 7, the toString variable must refer to the Ob-
ject.prototype.toString() method.
The last columns of Table 4 show which preconditions
need to be satised to automatically apply the optimization
patterns. Due to the dynamic features of the JavaScript lan-
guage, it is challenging to statically analyze whether these
preconditions are met. Possible solutions include a more
sophisticated static analyses or dynamic checks that ensure
that the conditions are met at runtime. We conclude from
the fact that each pattern is subject to several kinds of pre-
conditions that applying optimizations in JavaScript in a
fully automatic way is not trivial, and that nding tech-
niques that address this challenge is subject to future work.
8. THREATS TO V ALIDITY
Subject Projects. Our study focuses on 16 open source
projects and the results may not be representative for closed
69source projects or other open source projects. Furthermore,
as we consider projects written in JavaScript, our conclu-
sions are valid for this language only.
Performance Tests. We measure the performance impact
of optimizations with unit tests and microbenchmarks; the
application-level impact of the optimizations may dier. We
believe that our measurements are worthwhile because Java-
Script developers heavily use unit tests and microbench-
marks to make performance-related decisions.
JavaScript Engines and Platforms. We measure perfor-
mance with several versions of two JavaScript engines that
implement JIT compilation. Our results may not generalize
to other JIT engines, such as Chakra, which is used in Inter-
net Explorer, or interpreter-based engines. Since the most
popular server-side platform, node.js, and popular browsers
build upon the V8 or SpiderMonkey engines, we expect that
our measurement are relevant for developers.
Underapproximation of Recurring Optimization Patterns.
Our methodology for ndings instances of recurring opti-
mization patterns may miss instances, e.g., because the static
analyses rely on naming heuristics. As a result, the number
of previously unreported instances of optimization pattern
is an underapproximation. Since our goal is not to precisely
quantify the prevalence of recurring patterns but to answer
the question whether such patterns exist at all, this limita-
tion does not invalidate our conclusions.
9. RELATED WORK
9.1 Studies of Performance Issues
Studies show that performance bugs occur frequently [16]
and that they account for a non-negligible amount of de-
veloper time [44]. Jin et al. report that many problems in
C and C++ can be xed by following eciency rules [16].
Liu et al. [19] study performance bugs in smartphone appli-
cations and propose specialized program analyses that de-
tect two common kinds of problems. Linares-Vasquez et
al. [18] study how API usages on Android inuence energy
consumption, which is closely related to performance. Our
work diers from the existing studies by studying the root
causes of issues, the complexity of optimizations, the per-
formance impact of the applied optimizations, and the evo-
lution of the performance impact over time. Furthermore,
we are the rst to study performance issues in JavaScript,
which diers from C, C++, and Java both on the language
and the language implementation level. We had presented
an earlier version of this work in a poster paper [40].
9.2 Studies of JavaScript
Ocariza et al. study the root causes and the impact of cor-
rectness bugs in JavaScript [31], as well as the characteristics
of failures [32]. A recent study by Gallaba et al. [9] focuses
on the usage of JavaScript callbacks. Other studies con-
sider the dynamic behavior of JavaScript applications [38],
the prevalence of the eval() function [37], JavaScript in-
clusions [28], and the harmfulness and prevalence of type
coercions [36]. In contrast to all these studies, this paper
focuses on performance issues.
9.3 EfÔ¨Åciency of JavaScript Engines
The need for eciently executing JavaScript code is ad-
dressed by optimizing just-in-time compilers that produce
type-specialized code [8, 20, 13], that eciently representobjects [1], and that specialize functions based on previ-
ously observed parameters [5]. Our work shows that engines
employ dierent optimization strategies and that some op-
timizations provide very dierent performance across the
engines. We envision that the ndings of our study help
engine developers to focus on performance bottlenecks that
they may currently not be aware of.
9.4 JavaScript Analysis
Program analyses for JavaScript include dynamic analy-
ses to nd type inconsistencies [35] and violations of code
quality rules [12], dynamic determinacy analysis [39], dy-
namic information ow analysis [14], change impact analy-
sis [2], combined dynamic-static analysis to check if an inter-
face description matches a library implementation [7], static
analysis of library clients [21], UI-level test generation [27,
3, 41, 34], and type analysis for JavaScript [15]. Our study
motivates future research on analyses that help developers
improve the performance of JavaScript code, and provides
evidence to steer such work toward relevant problems.
9.5 Detecting Performance Bottlenecks
There are various analyses and proling approaches to de-
tect performance bottlenecks, such as memoization opportu-
nities [42], JIT-unfriendly code [11], memory bloat [43], re-
peated operations in loops [30], repeated patterns of method
calls [23], inecient use of collections [22], and performance
regressions [33]. Our work highlights common performance
issues in JavaScript and provides insights to steer future ap-
proaches for detecting performance issues.
10. CONCLUSION
Optimizations of JavaScript code deserve attention be-
cause thousands of JavaScript applications rely on respon-
sive and energy-ecient computations. Despite the fact that
JavaScript engines keep getting faster, developers still deal
with performance issues by applying optimizations to their
code. This paper presents the rst systematic study of real-
world JavaScript performance issues. We collect, reproduce,
and make available 98 issues and optimizations collected
from 16 popular JavaScript projects. Our results provide in-
sights about the most prevalent root causes of performance
issues, the complexity of changes that developers apply to
optimize programs, the performance impact of optimizations
and its evolution over time. Furthermore, our work provides
evidence that many optimizations are instances of recurring
optimization patterns. By nding 139 previously unreported
optimization opportunities based on optimization patterns
applicable across projects, we show a great potential for
techniques to further optimize existing programs. Our re-
sults and observations motivate future work on developing
techniques and tools that help developers solve performance
issues, and that provide directions to steer such work toward
problems that are relevant in practice.
Acknowledgments
Thanks to the anonymous reviewers, Frolin Ocariza Jr. Cristian-
Alexandru Staicu, and Jibesh Patra for commenting on drafts of
this paper. This research is supported by the German Federal
Ministry of Education and Research (EC SPRIDE and CRISP)
and by the German Research Foundation within the Emmy Noether
Project \ConcSys"
7011. REFERENCES
[1] W. Ahn, J. Choi, T. Shull, M. J. Garzar an, and
J. Torrellas. Improving JavaScript performance by
deconstructing the type system. In Conference on
Programming Language Design and Implementation
(PLDI) , pages 496{507, 2014.
[2] S. Alimadadi, A. M. 0001, and K. Pattabiraman.
Hybrid dom-sensitive change impact analysis for
javascript. In ECOOP, pages 321{345, 2015.
[3] S. Artzi, J. Dolby, S. H. Jensen, A. Mller, and F. Tip.
A framework for automated testing of JavaScript web
applications. In ICSE , pages 571{580, 2011.
[4] M. Boshernitsan, S. L. Graham, and M. A. Hearst.
Aligning development tools with the way programmers
think about code changes. In CHI, pages 567{576,
2007.
[5] I. Costa, P. Alves, H. N. Santos, and F. M. Q. Pereira.
Just-in-time value specialization. In CGO, pages 1{11,
2013.
[6] R. R. Dumke, C. Rautenstrauch, A. Schmietendorf,
and A. Scholz, editors. Performance Engineering,
State of the Art and Current Trends , London, UK,
UK, 2001. Springer-Verlag.
[7] A. Feldthaus and A. Mller. Checking correctness of
typescript interfaces for javascript libraries. In
Conference on Object Oriented Programming Systems
Languages and Applications (OOPSLA) , pages 1{16.
ACM, 2014.
[8] A. Gal, B. Eich, M. Shaver, D. Anderson,
D. Mandelin, M. R. Haghighat, B. Kaplan, G. Hoare,
B. Zbarsky, J. Orendor, J. Ruderman, E. W. Smith,
R. Reitmaier, M. Bebenita, M. Chang, and M. Franz.
Trace-based just-in-time type specialization for
dynamic languages. In PLDI , pages 465{478, 2009.
[9] K. Gallaba, A. Mesbah, and I. Beschastnikh. Don't
call us, we'll call you: Characterizing callbacks in
JavaScript. In Proceedings of the ACM/IEEE
International Symposium on Empirical Software
Engineering and Measurement (ESEM) , page 10
pages. IEEE Computer Society, 2015.
[10] A. Georges, D. Buytaert, and L. Eeckhout.
Statistically rigorous Java performance evaluation. In
Conference on Object-Oriented Programming,
Systems, Languages, and Application (OOPSLA) ,
pages 57{76. ACM, 2007.
[11] L. Gong, M. Pradel, and K. Sen. JITProf: Pinpointing
JIT-unfriendly JavaScript code. In European Software
Engineering Conference and Symposium on the
Foundations of Software Engineering (ESEC/FSE) ,
2015.
[12] L. Gong, M. Pradel, M. Sridharan, and K. Sen. DLint:
Dynamically checking bad coding practices in
JavaScript. In International Symposium on Software
Testing and Analysis (ISSTA) , 2015.
[13] B. Hackett and S. Guo. Fast and precise hybrid type
inference for JavaScript. In Conference on
Programming Language Design and Implementation
(PLDI) , pages 239{250. ACM, 2012.
[14] D. Hedin, A. Birgisson, L. Bello, and A. Sabelfeld.
JSFlow: Tracking information ow in JavaScript and
its APIs. In SAC, pages 1663{1671, 2014.[15] S. H. Jensen, A. Mller, and P. Thiemann. Type
analysis for JavaScript. In Proc. 16th International
Static Analysis Symposium (SAS) , volume 5673 of
LNCS . Springer-Verlag, August 2009.
[16] G. Jin, L. Song, X. Shi, J. Scherpelz, and S. Lu.
Understanding and detecting real-world performance
bugs. In Conference on Programming Language Design
and Implementation (PLDI) , pages 77{88. ACM, 2012.
[17] D. E. Knuth. Computer programming as an art.
Commun. ACM, 17(12):667{673, Dec. 1974.
[18] M. Linares-V asquez, G. Bavota, C. Bernal-C ardenas,
R. Oliveto, M. Di Penta, and D. Poshyvanyk. Mining
energy-greedy api usage patterns in android apps: an
empirical study. In MSR , pages 2{11, 2014.
[19] Y. Liu, C. Xu, and S. Cheung. Characterizing and
detecting performance bugs for smartphone
applications. In ICSE, pages 1013{1024, 2014.
[20] F. Logozzo and H. Venter. RATA: Rapid atomic type
analysis by abstract interpretation|application to
JavaScript optimization. In CC, pages 66{83, 2010.
[21] M. Madsen, B. Livshits, and M. Fanning. Practical
static analysis of JavaScript applications in the
presence of frameworks and libraries. In
ESEC/SIGSOFT FSE , pages 499{509, 2013.
[22] I. Manotas, L. Pollock, and J. Clause. Seeds: a
software engineer's energy-optimization decision
support framework. In ICSE, pages 503{514, 2014.
[23] D. Maplesden, E. D. Tempero, J. G. Hosking, and
J. C. Grundy. Subsuming methods: Finding new
optimisation opportunities in object-oriented software.
InICPE, pages 175{186, 2015.
[24] T. J. McCabe. A complexity measure. IEEE
Transactions on Software Engineering , 2(4):308{320,
Dec. 1976.
[25] N. Meng, M. Kim, and K. S. McKinley. Systematic
editing: generating program transformations from an
example. In PLDI , pages 329{342, 2011.
[26] N. Meng, M. Kim, and K. S. McKinley. Lase: locating
and applying systematic edits by learning from
examples. In ICSE , pages 502{511, 2013.
[27] A. Mesbah, E. Bozdag, and A. van Deursen. Crawling
Ajax by inferring user interface state changes. In
International Conference on Web Engineering
(ICWE) , pages 122{134, 2008.
[28] N. Nikiforakis, L. Invernizzi, A. Kapravelos, S. V.
Acker, W. Joosen, C. Kruegel, F. Piessens, and
G. Vigna. You are what you include: large-scale
evaluation of remote JavaScript inclusions. In CCS,
pages 736{747, 2012.
[29] A. Nistor, P.-C. Chang, C. Radoi, and S. Lu. Caramel:
Detecting and xing performance problems that have
non-intrusive xes. In ICSE, 2015.
[30] A. Nistor, L. Song, D. Marinov, and S. Lu. Toddler:
Detecting performance problems via similar
memory-access patterns. In International Conference
on Software Engineering (ICSE) , pages 562{571, 2013.
[31] F. S. Ocariza Jr., K. Bajaj, K. Pattabiraman, and
A. Mesbah. An empirical study of client-side
JavaScript bugs. In Symposium on Empirical Software
Engineering and Measurement (ESEM) , pages 55{64,
2013.
71[32] F. S. Ocariza Jr., K. Pattabiraman, and B. G. Zorn.
JavaScript errors in the wild: An empirical study. In
International Symposium on Software Reliability
Engineering (ISSRE) , pages 100{109, 2011.
[33] M. Pradel, M. Huggler, and T. R. Gross. Performance
regression testing of concurrent classes. In
International Symposium on Software Testing and
Analysis (ISSTA) , pages 13{25, 2014.
[34] M. Pradel, P. Schuh, G. Necula, and K. Sen.
EventBreak: Analyzing the responsiveness of user
interfaces through performance-guided test generation.
InConference on Object-Oriented Programming,
Systems, Languages, and Applications (OOPSLA) ,
2014.
[35] M. Pradel, P. Schuh, and K. Sen. TypeDevil:
Dynamic type inconsistency analysis for JavaScript. In
International Conference on Software Engineering
(ICSE) , 2015.
[36] M. Pradel and K. Sen. The good, the bad, and the
ugly: An empirical study of implicit type conversions
in JavaScript. In European Conference on
Object-Oriented Programming (ECOOP) , 2015.
[37] G. Richards, C. Hammer, B. Burg, and J. Vitek. The
eval that men do - a large-scale study of the use of
eval in JavaScript applications. In European
Conference on Object-Oriented Programming
(ECOOP) , pages 52{78, 2011.
[38] G. Richards, S. Lebresne, B. Burg, and J. Vitek. An
analysis of the dynamic behavior of JavaScript
programs. In Conference on Programming Language
Design and Implementation, PLDI , pages 1{12, 2010.
[39] M. Sch afer, M. Sridharan, J. Dolby, and F. Tip.
Dynamic determinacy analysis. In PLDI , pages
165{174, 2013.
[40] M. Selakovic and M. Pradel. Automatically xing
real-world JavaScript performance bugs. In
International Conference on Software Engineering
(ICSE), Poster track , 2015.
[41] S. Thummalapenta, K. V. Lakshmi, S. Sinha,
N. Sinha, and S. Chandra. Guided test generation for
web applications. In International Conference on
Software Engineering (ICSE) , pages 162{171. IEEE,
2013.
[42] L. D. Toola, M. Pradel, and T. R. Gross.
Performance problems you can x: A dynamic
analysis of memoization opportunities. In Conference
on Object-Oriented Programming, Systems,
Languages, and Applications (OOPSLA) , 2015.
[43] G. H. Xu, N. Mitchell, M. Arnold, A. Rountev,
E. Schonberg, and G. Sevitsky. Finding low-utility
data structures. In Conference on Programming
Language Design and Implementation (PLDI) , pages
174{186, 2010.
[44] S. Zaman, B. Adams, and A. E. Hassan. A qualitative
study on performance bugs. In Working Conference on
Mining Software Repositories (MSR) , pages 199{208.
IEEE, 2012.
72