Automatically Generating
Test Templates from Test Names
Benwen Zhang
University of Delaware
Newark, DE, USA
benwen@udel.eduEmily Hill
Drew University
Madison, NJ, USA
emhill@drew.eduJames Clause
University of Delaware
Newark, DE, USA
clause@udel.edu
Abstract —Existing speciﬁcation-based testing techniques re-
quire speciﬁcations that either do not exist or are too difﬁcult
to create. As a result, they often fall short of their goal ofhelping developers test expected behaviors. In this paper wepresent a novel, natural language-based approach that exploitsthe descriptive nature of test names to generate test templates.Similar to how modern IDEs simplify development by providingtemplates for common constructs such as loops, test templatescan save time and lower the cognitive barrier for writing tests.The results of our evaluation show that the approach is feasible:despite the difﬁculty of the task, when test names contain asufﬁcient amount of information, the approach’s accuracy is over80% when parsing the relevant information from the test nameand generating the template.
I. I NTRODUCTION
Software testing is an expensive and laborious activity that
can account for the majority of the total cost of developing
software. In the future, such high costs are likely to persist oreven increase as the growing size and complexity of modernsoftware exacerbates existing challenges. Techniques that im-prove the efﬁciency and effectiveness of the testing processcan thus signiﬁcantly reduce the overall cost of softwaredevelopment as well as improve software quality.
Fortunately, the software engineering research community
has provided many techniques that can help developers reducethe costs of testing. One group of techniques attempts tocompletely remove the burden of testing by automating the testgeneration process (e.g., [13, 15]). While the tests generatedby such automated techniques can be successful at revealingsome types of unexpected behavior (i.e., sad paths), theyare much less successful at helping developers test expectedbehaviors (i.e., happy paths). In addition, the tests often lookvery different from manually written tests and are difﬁcult tomaintain and understand [15].
In contrast to techniques that attempt to completely au-
tomate the test generation process, speciﬁcation-based testgeneration techniques attempt to help developers test expectedbehaviors by creating tests from speciﬁcations (e.g., [5, 11,12, 16, 20]). Ideally such speciﬁcation-based approaches candecrease the costs of testing by eliminating the tedious anderror-prone work of manually translating speciﬁcations intoexecutable tests. However, they often fall short of this goalbecause the required speciﬁcations either do not exist or mustbe written in a format that takes as much, if not more, effortthan manually writing the tests.In this paper we present a new, speciﬁcation-based ap-
proach that can help reduce the costs of testing by eliminatingsome of the tedious and error-prone work of writing unit tests.At a high-level, the approach leverages test names, whichcapture a developer’s testing intent, to automatically generatetest templates. Similar to how modern IDEs provide templatesfor common constructs such loops, try-catch blocks, methodcalls, etc., our approach infers what and how the developerwants to test and generates the corresponding code. Notethat like the templates generated by IDEs, the test templatesgenerated by our approach will likely have “holes”—locationswhere developers will need to provide additional informationthat is not included in the test name. Developers can ﬁll in suchholes by either providing the necessary information directly(e.g., adding concrete inputs for method calls) or by improvingthe test’s name. Encouraging developers to write better testnames in this manner is an additional beneﬁt of the approach,potentially simplifying future maintenance tasks.
To assess the feasibility of the approach, we conducted a
preliminary evaluation by manually comparing the templatesgenerated by our approach to the original, developer writtentest bodies. Despite the difﬁculty of the task, when test namescontain a sufﬁcient amount of information, the approach’saccuracy at parsing the relevant information from the test nameand generating the corresponding template is over 80%.
II. M
OTIV A TION
Our choice to infer developer intent from test names is
based on a combination of two observations. The ﬁrst observa-tion is that every unit test must have a corresponding name.
While this may seem unremarkable, it does mean that develop-ers are accustomed to providing such information. Therefore,using it as the basis for our approach means that, unlike forother speciﬁcation-based techniques, there is no additionaloverhead for developers in either creating speciﬁcations ormodifying them to ﬁt a speciﬁc format. The second observationis that test names should describe and summarize important
parts of the test’s body. More speciﬁcally, they shouldindicate the scenario being tested and the expected outcome.As such, they can serve as a (partial) speciﬁcation for the test.
As examples of the descriptiveness of test names, con-
sider the tests shown in Figure 1. Despite coming fromdifferent projects and being written by different develop-ers, the name of each test summarizes its body: in test-
MaximumSize_negative, the test sets the maximum size
2015 30th IEEE/ACM International Conference on Automated Software Engineering
978-1-5090-0025-8/15 $31.00 © 2015 IEEE
DOI 10.1109/ASE.2015.68506
public void testMaximumSize_negative() {
  CacheBuilder<> builder = new CacheBuilder<Object, Object>();
/nobreakspace try {/nobreakspace   builder.maximumSize( -1);
/nobreakspace /nobreakspace fail();/nobreakspace } catch (IllegalArgumentException expected) {}}
(a) Guava test for the CacheBuilder class.
public void testEntrySetClearChangesMap() {  Map map = makeFullMap();  Set entrySet = map.entrySet();  assertTrue(map.size() > 0);
  assertTrue(entrySet.size() > 0);
  entrySet.clear();  assertTrue(map.size() == 0);
  assertTrue(entrySet.size() == 0);
}
(b) Apache Commons Collections test for the Map class.
public void testSettingHeightThatIsTooSmallLeavesHeightUnchanged() {  Barcode barcode = new BarcodeMock( “12345");
  int height = barcode.getHeight();  barcode.setBarHeight( 0);
  assertEquals(height, barcode.getHeight());}
(c) Barbecue test for the Barcode class.
Fig. 1. Unit tests illustrating the descriptive nature of test names.
testSetting Height Changes Default ()action predicate
action object predicate object
Fig. 2. Test name annotated with its parts of test.
of the cache builder to a negative number; in test-
EntrySetClearChangesMap, the test checks whether
clearing the map’s entry set changes the contents of themap; and in testSettingHeightThatIsTooSmall-
LeavesHeightUnchanged(), the test checks whether set-ting the barcode’s bar height to zero changes its height.
III. P
ARTS OF TEST
Beyond simply summarizing test bodies as shown in Sec-
tion II, we have found that test names frequently follow a setof well deﬁned grammatical rules. While in theory, developershave the full range of ﬂexibility allowed by the programminglanguage when naming tests, in practice, test names often
follow a relatively well deﬁned grammatical structure. Thisregular set of grammatical conventions makes it possible toextract relevant information from test names that relates tothe test’s body. More speciﬁcally, it allows for automaticallytagging the name with what we refer to as parts of test(POTs)—the testing equivalent to parts of speech.
At a high-level, test names typically consist of two verb
phrases: an action phrase and a predicate phrase. The action
phrase describes the scenario being tested, including actionswhich should be performed and the state of the environment,whereas the predicate phrase describes the expected outcomeof the test. For example, consider the test name shown inFigure 2. Here the action under test is “setting the height”, andthe result of that action is that “default” should be “changed”.
In some cases, the action phrase and predicate phrase can
be further broken down into individual components of a verb—an action or a predicate—and an object noun phrase—actionpublic void testGetDataReturnsData() {
  String data = ...;  Barcode barcode = new Barcode(data);
  String actual = barcode.getData();  assertEquals(data, actual);}
(a)
public void testGetDataReturnsData() {  String data = "12345";
  Barcode barcode = new BarcodeMock(data);
  assertEquals(data, barcode.getData());}
(b)
Fig. 3. Comparison between the the test template generated by our
approach (a) and the developer-written test (b).
object or predicate object. In Figure 2, the action is “setting”,
its object is “height” (what is being set), the predicate is“changes” and the predicate object is “default”.
Some very descriptive test names include additional
information beyond the action or predicate phrase. Forexample, in testSettingHeightThatIsTooSmall-
LeavesHeightUnchanged(), the phrase “that is toosmall” describes how the action object (height) shouldbe set. Similarly, in testCopy_byteArrayToWriter-
WithEncoding(), the phrases “to writer” and “with en-coding” further describe how the byte array should be copied.These phrases are not part of the action phrase or predicatephrase, but rather modify, or further describe them. In ourexperience, we have observed that these modiﬁers describe theaction phrase, and thus we refer to them as action modiﬁers.
In summary, test names typically contain one or more of
the following POTs: action, action object, action modiﬁers,predicate, and predicate object.
IV . A
UTOMA TICALLY GENERA TING TEST TEMPLA TES
Given a test name and a class under test, the approach
creates a corresponding test template using two main phases.The ﬁrst phase, parsing, involves parsing the test name to
identify the relevant information it contains (i.e., the actionphrase and the predicate phrase). Note that, because of po-tential ambiguities in how a test name can be interpreted,multiple parses may be produced by this step. The secondphase, generation, takes as input the set of parses produced
by the ﬁrst phase, and the class under test, to produce oneor more test templates for each parse. To produce the testtemplates, the generation phase statically analyzes the codeof the application under test to map the identiﬁed POTs tomethods, ﬁelds, parameters, assertions, etc. Again, multipletemplates may be generated for a parse because of potentialambiguities in how the POTs can be mapped to executablecode. For example, there may be multiple methods with thesame name or multiple ways to access a ﬁeld. In practice,developers can choose among the generated test templatesin the same manner they currently choose among possibleautocompletion options, with the IDE allowing them to quicklycycle though the options.
A. Concrete Example
As a concrete example of the output of each phase, assume
that a developer wants to create a test named testGetData-
507ReturnsData to test the Barbecue application’s Barcode
class. The ﬁrst phase analyzes the name and tags “get” as the
action, “data” as the action object, “returns” as the predicate,
and “data” as the predicate object. Given this parse, the second
phase generates the template shown in Figure 3(a). As a pointof comparison, the developer-written test body for this test isshown in Figure 3(b).
In the test template, an instance of Barcode, the class under
test, is instantiated. Note that the constructor requires a singleargument of type String. Because the test name does not
include any information about how the environment should besetup (i.e., any action modiﬁers), a hole is left by the approach,but the necessary type is clearly indicated. Note that in thiscase, the approach could easily provide a randomly selectedstring as the speciﬁc value is irrelevant. However, because thisfact is difﬁcult to infer in general, the approach leaves the holefor the developer to complete instead.
To generate the scenario under test part of the template,
the approach examines the code of Barbecue in order to mapthe action phrase, “get data”, to executable code. Here, theapproach ﬁnds a method of the Barcode class called getData
that matches the action phrase. To generate the expectedoutcome part of the test, the approach matches the predicate,“returns”, to a predeﬁned assertion method. In this case, themost appropriate assertion is assertEquals because the
intention is to compare the result of performing the action.Finally the predicate object, “data”, is added to the predicateand compared against the result of getting the barcode’s data.While it may be surprising that the approach can identify thatthe string argument provided to the constructor should alsoserve as the expected value in the assertion, it makes thisconnection using static analysis. Analyzing Barbecue revealsthat the constructor’s parameter is named “data”. With thisadditional information, it becomes possible to infer that thedata returned by getData should be the same data that was
used to create the barcode.
The fact that the test template generated by our approach
closely matches the developer-written test body demonstratesthe usefulness of our approach. Instead of having to type thename and then repetitively type the same information in thebody, the developer can rely on the approach to generate anappropriate template. Beyond the test name, the developer onlyneeded to provide a concrete value for the data used to createthe barcode. By eliminating the need to type test bodies, theapproach allows developers to focus on choosing relevant testdata or simply move on to other tasks more quickly.
B. Phase 1: Parsing Test Names
The goal of the parsing phase is to identify the action
phrase and predicate phrase contained in a test name, when
these phrases exist. Our approach for identifying such phrasesis the result of combining information about English grammar,knowledge of (and intuition about) the conventions developersfollow when writing tests, and a manual examination ofexisting test cases.
1) Extracting Grammatical Relations: The ﬁrst step in
identifying action phrases and predicate phrases is to use anoff-the-shelf parsing system for English to identify the gram-matical relations among the words in the test name (excluding
Fig. 4. Semantic graph for testCopy_byteArrayToWriterWith-
Encoding().
the leading “test”). The result of this step is a semantic graphthat shows the grammatical relationships among the words andtheir part of speech (POS). All of the semantic graphs extracteddirectly from the sentence fragment are passed onto the nextstep to identify the POTs.
Figure 4 shows one of the semantic graphs that re-
sults from parsing testCopy_byteArrayToWriter-
WithEncoding(). Each node in the graph represents anoccurrence of a word in the sentence fragment and its part of
speech based on the CLAWS7 Tagset (verb, noun, preposition,
etc.). Common word stems are indicated by “+”. For instance,“encoding” is an “-ing” verb participle (VVG), and it has thestem “-ing”. The edges in the graph represent grammaticalrelations such as subjects (XSUBJ, NCSUBJ), clausal compli-ments when overt subjects are missing (XCOMP , CCOMP),clausal modiﬁers (XMOD, NCMOD, CMOD), as well as directand indirect objects (DOBJ, IOBJ, OBJ2).
2) Identifying parts of test: The second step in identifying
action phrases and predicate phrases is to use the semanticgraph to tag the test name with its POTs. In identifying POTs,we are looking to identify the action and predicate phrasesas well as action modiﬁers. Recall that simple sentences inEnglish consist of a subject, verb, and then an object. (Notethe use of the word “subject” in this subsection is distinct fromthe test’s subject, or class under test, used elsewhere in thepaper.) If the semantic graph includes a word with the role ofsubject, then the approach identiﬁes that subject as the actionobject, its verb as the predicate, and the verb’s correspondingdirect or indirect object as the predicate object. Prepositionsother than “to” denote action modiﬁers. If the semantic graphdoes not contain a subject and the test name begins with averb, we assume there is no predicate in the name and attemptto extract only an action phrase and action modiﬁers.
One of the major challenges in identifying POTs is ro-
bustness. A test name may yield many possible semanticgraphs, some of which may be perfectly correct, some that areworkable, and some that are too incomplete to analyze. Thus,we have designed our approach to be as robust as possible inthe face of errors in the grammatical relation parsing and partof speech tagging. We take a best-effort approach that attemptsto match as many rules as possible and prioritizes the outputof our approach by the most frequently identiﬁed POTs.
508C. Phase 2: Generating Test Templates
The goal of the generation phase is to transform the POTs
provided by the parsing phase into test templates. At a high-
level, the process for transforming a parse into a test templateis done in two steps. The ﬁrst step is to convert the actionphrase into the scenario under test part of the template and thesecond step is to convert the predicate phrase into the expectedoutcome part of the template.
1) Generating the Scenario Under Test: Intuitively, action
phrases are converted into the scenario under test part of atemplate by searching the code of the application under test inorder to ﬁnd sequences of method calls and ﬁeld accesses thatcorrespond to the elements of the action phrase. Brieﬂy, thesearch process starts with the class under test and performs abreadth-ﬁrst traversal along the ﬁeld types and method returntypes of the encountered classes looking for entities that matchthe speciﬁed action. The path between the identiﬁed action andthe subject is then the sequence of method invocations and ﬁeldaccesses that will be performed by the test. In order to facilitatedifferent ways of matching, we use the strategy pattern to allowthe search process to transparently use different matchers.After the search is complete the matched ﬁelds and methodsare ranked according to the matcher that identiﬁed them.
2) Generating the Expected Outcome Checks: The process
of converting the predicate phrase into the expected outcomepart of the template is simpler than converting the action phraseinto the scenario under test. Because there are only a smallnumber of possible assertions, we do not need to use a fullsearch process. Rather we can use a set of manually createdrules that look for speciﬁc words in the predicate phrase aswell as the attitude of the test name to choose an assertion. Ifnegative words such as “not” or “cannot” occur anywhere inthe test name, we classify the attitude as negative, otherwisethe attitude is positive.
For example, if the predicate contains the words “true”
or “false”, we assume that the corresponding assertion shouldbeassertTrue orassertFalse, respectively. If the
predicate is a linking verb, the predicate object is a constant,and the attitude is positive, we choose “assertEquals”. If thepredicate contains the words “throw”, “fail” or “exception”,fail will be chosen. Currently, this manually generated rule
system is adequate, but we plan to improve it in future workby using machine learning techniques to generalize theserules from a large set of actual test names and source code.Such information will allow for a more accurate mapping ofpredicate phrases to assertions.
The ﬁnal step in the template generation process is to
combine the scenario under test part of the template and theexpected outcome part of the template. Unlike other test gen-eration approaches, the templates generated by our approachdo not need to be executable, which can simplify the process.Instead of needing to construct potentially complex data, wecan simply leave a hole that the developer can customize tosuit their particular testing goals.
We identiﬁed ﬁve common patterns for the test templates:
the do and affect pattern, the no effect pattern, the before
and after pattern, the throw exception pattern and the default
pattern. Of the ﬁve patterns, the default pattern is the mostcommon. It follows a simple sequential input and oraclestructure that consists of initializing the subject class, a methodsequence invocation, and asserting on the returned value. Thedo and affect pattern is similar, but slightly different in thatit invokes a method sequence in the input part of the test andasserts the returned value from another invocation of a methodsequence or ﬁeld access. This pattern is especially suitable fortesting paired getters and setters. The no effect pattern and thebefore and after pattern are similar in that the input and oracleparts of these two pattern are interlaced. Their code usuallyfollows an order of initializing the subject class, a methodsequence invocation, asserting a returned value or ﬁeld, anothermethod sequence invocation, and another assert on a returnedvalue or ﬁeld. The difference between these two patterns istheir different testing objectives. The no effect pattern checkswhether an action unexpectedly affects the status of the objectunder test. On the other hand, the before and after pattern testswhether the input changes the status of the object under testin expected way. The last pattern, the throw exception pattern,tests whether the input causes an exception to be thrown.
V. E
V ALUA TION
The goal of our evaluation is to determine whether our
approach can automatically create test templates that closelymatch tests written by developers. To perform a meaningfulevaluation with respect to this goal, we must therefore considertest names that are descriptive; if we considered test namesthat are void of meaning, it would be difﬁcult to assessthe beneﬁt that our approach can provide. Unfortunately, notevery test name is descriptive. In practice, because naming isdifﬁcult and there is no immediate downside to choosing poornames, developers often create generic test names (e.g., test1,test2, etc.) or names that contain very little information (e.g.,testAdd, testSubtract, etc.).
Given a descriptive test name, we determine if the gen-
erated test template bears any resemblance to the test bodywritten by the original developer. Since there may be manysemantically equivalent code sequences that test the same func-tionality, our goal is not to generate the exact same test caseas the original developer. As such, we determine feasibility ofour approach by investigating whether the basic components(object under test, action method call, assert structure) matchthe developer-written test.
A. Test Name Descriptiveness
To avoid the potential bias that could result from choosing
descriptive test names ourselves, we instead had Masters and
PhD students from the University of Delaware’s SoftwareTesting and Maintenance class label test names with theirPOTs. In the remainder of this section, we refer to thesegraduate students as labelers.
First, we used a weighted random selection strategy
to choose 100 test names from the test suites of Barbe-
cue, Commons-beanutils-1.8.3, Commons-cli-1.2, Commons-collections-3.2.1, Commons-io-2.4, DataStructures and Gson-2.2.4. Because labeling test names with their POT is timeconsuming—labelers took from 45min to 60min to label
the POTs in 15names—and we want to choose as many
descriptive test names as possible, we biased the selectionby the number of words in each test name; presumably test
509public void testIOExceptionStringThrowable() {
  Throwable cause = new IllegalArgumentException( "cause");
  IOExceptionWithCause exception =      new IOExceptionWithCause( "message", cause);
  assertEquals( "message" , exception.getMessage());
  assertEquals(cause, exception.getCause());  assertSame(cause, exception.getCause());}
Fig. 5. Name containing an action phrase that does not describe the test.
names that contain more words are more likely to contain more
information. To count the number of words contained in a testname, we stripped the leading “test” and used a purpose buildidentiﬁer splitter. The ﬁnal set contained 10test names with
1or2words, 30test names with 3words, and 60test names
with 4or more words.
After choosing the set of 100 test names, we randomly
assigned 15names to each labeler. Because we had 20labelers,
assigning 15names to each allowed us to keep the expected
length of the task below 1h and to have each test name labeled
three times. For each test name, the labeler was shown the testname as well as the name of the class under test and a briefdescription of the project from which the test name was taken.The labeler was then asked to identify which part of the testname should be tagged with each of the ﬁve POTs: action,action object, action modiﬁers, predicate, and predicate object.If a labeler felt that the test name did not contain a speciﬁcPOT, they were instructed to enter a special “N/A” value. Dueto labeler attrition, 10of the test names were not labeled a
sufﬁcient number of times and were removed from the setleaving us with 90labeled test names.
We then calculated how often a majority of labelers agree
on whether a test name contains each POT: for action, labellersagreed 73% of the time; for action object, the labellers agreed
68% of the time; for action modiﬁers, the labellers agreed
49% of the time; for predicate, the labellers agreed 80%o f
the time; and for predicate object, the labellers agreed 75%
of the time. As this data shows, tagging test names with theirPOT is a relatively difﬁcult task.
Using the POTs assigned by the labelers and the agreement
scores, we identiﬁed which test names contain a sufﬁcientamount of information as follows: test names where a majorityof labelers agree that it contains either (1) an explicit action(i.e., not “get” or “set”), or (2) an implicit action (i.e., “get” or“set”) and an action object are presumed to contain an actionphrase; and test names where a majority of labelers agree thatit contains both a predicate and a predicate object presumablycontain a predicate phrase. Under this classiﬁcation, 60%
of the test names (54 out of 90) have enough information
for the template to include the scenario under test (i.e., theycontain an action phrase) and 43%(39out of 90) have enough
information for the template to include the expected outcome(i.e., they contain a predicate phrase).
B. Comparing templates with developer-written tests
After identifying which test names contain a suitable
amount of information, we generated the corresponding test
templates for the 54test names that have an action phrase
and the 39 test names that have a predicate phrase. We
manually compared the templates against the developer-writtenTABLE I. R ESULTS OF COMPARING THE TEMPLA TES GENERA TED BY
OUR APPROACH TO THE DEVELOPER -WRITTEN TEST BODIES .
Outcome Action phrase Predicate phrase
Match 31 29
Incorrect parse 74
Incorrect template 64
test bodies to determine whether they match. We label a testtemplate as a match if it invokes the exact same method as themain test action (which requires instantiating a similar type ofobject), and when it uses the same assertion pattern (includingsimilarly typed actual and expected values). For templates thatdo not match the developer-written test body, we classiﬁed thecause of the mismatch as either an irrelevant name, an incorrectparse, or an incorrect template.
The ﬁrst mismatch cause, irrelevant name, was assigned
when the test name did not describe either the scenariounder test or the expected outcome. Although our labelersagreed that these names contained an action phrase or apredicate phrase, the information from the name does notsummarize the test body. Figure 5 shows an example ofan irrelevant name. In this case, the action phrase—getIOException—does not describe the scenario being tested,creating a Throwable IllegalArgumentException. The
second mismatch cause, incorrect parse, was assigned to when
the parser incorrectly identiﬁed the information contained inthe test name. The third mismatch cause, incorrect template,
was assigned when the parser correctly identiﬁed the informa-tion contained in the name but the template generator chosean incorrect method, ﬁeld sequence, assertion, etc.
Because cases where the name of the test is irrelevant
are equivalent to cases where the test name does not containsufﬁcient information, we removed them from further consid-eration. If the speciﬁcation is wrong, it is nearly impossiblefor the template to be correct. After removing the tests withirrelevant names, we were left with 44relevant test names
that contain an action phrase and 37relevant test names that
contain a predicate phrase.
Table I shows the results of comparing the test templates
for the relevant test names to the developer-written test bod-ies. The ﬁrst column, Outcome, shows whether the template
matched, was a parser mistake, or template generation mistake.The second and third columns, Action phrase and Predicate
phrase, show, for the test names that contain an action phraseor predicate phrase, respectively, the number of times eachoutcome occurred.
When considered individually, the accuracy of the compo-
nents of the approach are above 80%. More speciﬁcally, for
test names that contain an action phrase, the parser was ableto extract the correct information 82% of the time (
31
44−6) and
the generator was able to generate a template that matchedthe developer-written body 84% of the time (
31
44−7). For test
names that contain a predicate phrase, the parser was able toextract the correct information 88% of the time (
29
37−4) and
the generator was able to generate a template that matched thedeveloper-written body 88% of the time (
29
37−4). Based on this
data, we conclude that it is feasible to automatically generatetest templates based on the developer intent expressed in testnames.
510VI. R ELA TED WORK
Speciﬁcation-based Test Case Generation : At a high-
level, speciﬁcation-based approaches share our overall goal of
reducing the cost of writing tests by automatically generatingtests from some form of documentation. Speciﬁc types ofdocumentation that have been considered include: SystemDescription Languages (SDLs) [20], use cases [11], UML [12],and scenarios [16]. In addition to generating test cases,speciﬁcation-based techniques have also been used to generateapplication code [5]. In contrast to the English test methodnames used by our approach, the forms of documentationrequired by other speciﬁcation-based approaches are less likelyto exist and are more difﬁcult to create.
Code Completion: Code completion techniques attempt
to reduce the costs of coding, rather than testing, by reducinga developer’s searching and typing. V arious code completiontechniques have been developed based on Hidden Markovmodels (HMMs), ranking, statistical properties of source code,machine learning, data mining, as well as other matchingapproaches to recommend APIs according to users’ incompleteinputs (e.g., [4, 6]).
Code Snippet Search: Code snippet search techniques
attempt to locate and retrieve relevant source code in responseto a query. Related work in this ﬁeld include experiments toevaluate diverse approaches to code search [17]; techniquesto improve the searchability of source code (e.g., [9, 10]);techniques for extracting information from diverse sources(e.g., [14, 19]); and interactive techniques (e.g., [1, 21]).
Natural Language Program Analysis: Finally, in the area
of Natural Language Program Analysis (NLPA), researchershave investigated identiﬁer splitting techniques (e.g., [2, 7]);the technique for tagging words with their part of speech andidentifying large semantic structures [3]; the technique foridentifying programming-speciﬁc synonyms and antonyms [8];and the comment generation technique [18]. These techniquesare not alternatives to our approach but can be used to improveits accuracy and effectiveness.
VII. C
ONCLUSIONS
We presented a novel speciﬁcation-based approach for
automatically generating test templates based on the developerintent expressed in test names. The approach can help reducetesting costs by automating some of the tedious and error-pronework searching for code and manually writing tests. In addi-tion, it encourages developers to write more descriptive testnames, which can provide long-term beneﬁts by simplifyingtest comprehension and maintenance tasks. Our preliminaryevaluation provides evidence that the approach is feasibleand promising: despite the difﬁculty of the task, when testnames contain a sufﬁcient amount of information, its accuracyat parsing the relevant information from the test name andgenerating the corresponding template is over 80%.
VIII. A
CKNOWLEDGMENTS
This work is supported in part by National Science Foun-
dation Grant No. 1527093.REFERENCES
[1] J. Galenson, P . Reames, R. Bodik, B. Hartmann, and K. Sen. CodeHint:
Dynamic and interactive synthesis of code snippets. In Proceedings of
the 36th International Conference on Software Engineering, pages 653–
663, 2014.
[2] L. Guerrouj, M. Di Penta, G. Antoniol, and Y .-G. Gu ´eh´eneuc. Tidier: An
identiﬁer splitting approach using speech recognition techniques. Journal
of Software: Evolution and Process, 25:575–599, 2013.
[3] S. Gupta, S. Malik, L. Pollock, and K. Vijay-Shanker. Part-of-speech
tagging of program identiﬁers for improved text-based software engineer-ing tools. In Proceedings of the 21st IEEE International Conference on
Program Comprehension, pages 3–12, 2013.
[4] T. Gvero, V . Kuncak, I. Kuraj, and R. Piskac. Complete completion
using types and weights. In Proceedings of the 34th ACM SIGPLAN
Conference on Programming Language Design and Implementation,pages 27–38, 2013.
[5] M. Hamri and G. Zacharewicz. Automatic generation of object-oriented
code from DEVS graphical speciﬁcations. In Proceedings of the Winter
Simulation Conference, pages 409:1–409:12, 2012.
[6] S. Han, D. R. Wallace, and R. C. Miller. Code completion from
abbreviated input. In Proceedings of the 2009 IEEE/ACM International
Conference on Automated Software Engineering, pages 332–343, 2009.
[7] E. Hill, D. Binkley, D. Lawrie, L. Pollock, and K. Vijay-Shanker. An
empirical study of identiﬁer splitting techniques. Empirical Software
Engineering, 19(6):1754–1780, 2014.
[8] M. J. Howard, S. Gupta, L. Pollock, and K. Vijay-Shanker. Automatically
mining software-based, semantically-similar words from comment-codemappings. In Proceedings of the 10th Working Conference on Mining
Software Repositories, pages 377–386, 2013.
[9] I. Keivanloo, J. Rilling, and Y . Zou. Spotting working code examples.
InProceedings of the 36th International Conference on Software Engi-
neering, pages 664–675, 2014.
[10] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and C. Fu.
Portfolio: Finding relevant functions and their usage. In Proceedings
of the 33rd International Conference on Software Engineering , pages
111–120, 2011.
[11] C. Nebut, F. Fleurey, Y . Le Traon, and J. Jezequel. Requirements by
contracts allow automated system testing. In Proceedings of the 14th
International Symposium on Software Reliability Engineering, pages 85–96, 2003.
[12] J. Offutt and A. Abdurazik. Generating tests from uml speciﬁcations.
InProceedings of the 2nd International Conference on The Uniﬁed
Modeling Language: Beyond the Standard, pages 416–429, 1999.
[13] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball. Feedback-directed
random test generation. In Proceedings of the 29th International
Conference on Software Engineering, pages 75–84, 2007.
[14] P . C. Rigby and M. P . Robillard. Discovering essential code elements
in informal documentation. In Proceedings of the 2013 International
Conference on Software Engineering, pages 832–841, 2013.
[15] B. Robinson, M. D. Ernst, J. H. Perkins, V . Augustine, and N. Li. Scaling
up automated test generation: Automatically generating maintainableregression unit tests for programs. In Proceedings of the 26th IEEE/ACM
International Conference on Automated Software Engineering, pages 23–32, 2011.
[16] J. Ryser and M. Glinz. A practical approach to validating and testing
software systems using scenarios. In Proceedings of the Third Interna-
tional Software Quality Week Europe, 1999.
[17] S. E. Sim, M. Umarji, S. Ratanotayanon, and C. V . Lopes. How well do
search engines support code retrieval on the web? ACM Trans. Softw.
Eng. Methodol., 21(1):4:1–4:25, 2011.
[18] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker.
Towards automatically generating summary comments for java methods.InProceedings of the 25th IEEE International Conference on Automated
Software Engineering, 2010.
[19] S. Subramanian and R. Holmes. Making sense of online code snippets.
InProceedings of the 10th Working Conference on Mining Software
Repositories, pages 85–88, 2013.
[20] L. Tahat, B. V aysburg, B. Korel, and A. Bader. Requirement-based
automated black-box test generation. In Proceedings of the 25th Annual
International Computer Software and Applications Conference, pages489–495, 2001.
[21] S. Thummalapenta and T. Xie. Parseweb: A programmer assistant for
reusing open source code on the web. In Proceedings of the Twenty-
second IEEE/ACM International Conference on Automated SoftwareEngineering, pages 204–213, 2007.
511