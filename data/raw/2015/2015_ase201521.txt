Testing Cross-Platform Mobile App
Development Frameworks
Nader Boushehrinejadmoradi, Vinod Ganapathy, Santosh Nagarakatte, Liviu Iftode
Department of Computer Science, Rutgers University
{naderb,vinodg,santosh.nagarakatte,iftode }@cs.rutgers.edu
Abstract —Mobile app developers often wish to make their
apps available on a wide variety of platforms, e.g., Android, iOS,
and Windows devices. Each of these platforms uses a diﬀerent
programming environment, each with its own language and APIsfor app development. Small app development teams lack theresources and the expertise to build and maintain separate codebases of the app customized for each platform. As a result,we are beginning to see a number of cross-platform mobileapp development frameworks. These frameworks allow the appdevelopers to specify the business logic of the app once, using thelanguage and APIs of a home platform (e.g., Windows Phone),
and automatically produce versions of the app for multiple targetplatforms (e.g., iOS and Android).
In this paper, we focus on the problem of testing cross-
platform app development frameworks. Such frameworks arechallenging to develop because they must correctly translate thehome platform API to the (possibly disparate) target platformAPI while providing the same behavior . We develop a diﬀerentialtesting methodology to identify inconsistencies in the way thatthese frameworks handle the APIs of the home and target plat-forms. We have built a prototype testing tool, called X-Checker,and have applied it to test Xamarin, a popular framework thatallows Windows Phone apps to be cross-compiled into nativeAndroid (and iOS) apps. To date, X-Checker has found 47 bugsin Xamarin, corresponding to inconsistencies in the way thatXamarin translates between the semantics of the Windows Phoneand the Android APIs. We have reported these bugs to theXamarin developers, who have already committed patches fortwelve of them.
I. Introduction
Over the last several years, we have witnessed a number
of advances in mobile computing technology. Mobile devices
are now available in a variety of form factors, such as glasses,watches, smartphones, tablets, personal robots, and even cars.These devices come equipped with powerful processors, amplestorage, and a diverse array of sensors. Coupled with advancesin operating systems and middleware for mobile devices,programmers can now avail rich programming APIs to buildsoftware (“apps”) that leverage these advances in hardware.Modern app markets contain hundreds of thousands of apps,and the number and diversity of apps available to end-usershas further contributed to the popularity of mobile devices.These advances in hardware and software have made mobiledevices viable replacements for desktop computers.
At the same time, we are also witnessing a fundamental
shift in the practice of software development due largelyto the dynamics of mobile app development. Until a fewyears ago, the task of developing software (targeting mainlydesktop computers) was mostly conﬁned to teams of softwareengineers, either in the open-source community or at ITcompanies. In contrast, it is common today for small teamsor even individuals to build and distribute software via mobileapp markets. Such teams, or individuals, may lack the expertiseand experience of a large team of developers and oftenface economic and time constraints during app development.Nevertheless, mobile app development teams aim to maximizerevenue by making their apps available on a wide variety ofmobile devices, i.e., those running software stacks such as
Android, iOS, and Windows. Apps that are available for awide variety of mobile devices can reach a large user base,and can therefore generate more revenue either through apppurchases or via in-app advertisements.
One way to build apps for diﬀerent mobile platforms is to
create customized versions of apps for each platform, e.g., a
separate version of the app for Android, iOS and Windowsdevices. However, this approach leads to multiple versions ofthe app’s code-base, which are diﬃcult to maintain and evolveover time. Therefore, developers are increasingly adoptingcross-platform mobile app development frameworks. Theseframeworks allow developers to program the app’s logic oncein a high-level language, and provide tool-support to allow theapp to execute on a number of mobile platforms.
There are two broad classes of cross-platform frameworks
available today. The ﬁrst class, which we call W eb-based
frameworks, allows developers to build mobile apps usinglanguages popularly used to build Web applications, such asHTML5, JavaScript, and CSS. Examples of such frameworksinclude Adobe PhoneGap/Cordova [1], Sencha [7] and IBMMobileFirst [3]. Developers specify the app’s logic and userinterface using one or more of the Web-development lan-guages. However, these languages do not contain primitives toallow apps to access resources on the phone, e.g., peripherals
such as the camera and microphone, the address book, andphone settings. Thus, Web-based frameworks provide sup-porting runtime libraries that end-users must download andexecute on their mobile devices. Mobile apps interface withthese libraries to access resources on the mobile devices—such mobile apps are also popularly called hybrid mobile apps.Web-based frameworks allow developers to rapidly prototypemobile apps. However, these frameworks are ill-suited forhigh-performance apps, such as games or those that useanimation. The expressiveness of the resulting mobile apps isalso limited to the interface exported by the runtime librariesoﬀered by the frameworks.
The second class, which we call native frameworks, ad-
dresses the above challenges. Examples of such frameworksinclude Xamarin [8], Apportable [2], MD
2[6, 31], and the
recently-announced cross-platform bridges to be available onWindows 10 [30, 40]. These frameworks generally support ahome platform and one or more target platforms. Developers
build mobile apps as they normally would for the home plat-
2015 30th IEEE/ACM International Conference on Automated Software Engineering
978-1-5090-0025-8/15 $31.00 © 2015 IEEE
DOI 10.1109/ASE.2015.21441
Fig. 1. Overall operation of a cross-platform mobile app development framework, using Xamarin as a concrete example. Developers build apps as they would
for the Windows Phone, in C# using calls to the API of the Windows Phone SDK. This code can directly be compiled to Windows Phone apps using the VisualStudio toolchain. Xamarin allows developers to use the same source code to build native Android or iOS apps. Xamarin provides compatibility libraries thattranslate Windows SDK API calls in the code to the relevant API calls of the underlying Android and iOS SDKs.
form, and leverage the framework’s support to automatically
produce apps for the target platforms as well. For example, thehome platform for Xamarin is Windows Phone, and developersbuild apps using C# and the API of the Windows Phone SDK.The Xamarin framework allows developers to automaticallybuild Android and iOS apps using this code base. Likewise, thehome platform for Apportable is iOS. Developers build appsusing Objective-C and the iOS SDK, and leverage Apportableto produce Android apps from this code base. One of the mainhighlights of the Microsoft Build Developer Conference heldin April/May 2015 was the announcement that the upcomingrelease of Windows 10 will contain interoperability bridgesthat allow Android and iOS developers to easily port theirapps to the Windows 10 platform [30, 40]. These bridgesallow Android (or iOS) apps written in Java (or Objective-C)and programmed to use calls from the Android (or iOS) SDKto transparently execute atop Windows 10 devices. While thetechnical details of this platform are forthcoming as of thispaper’s publication, it is reasonable to assume that the bridgeswill incorporate a compatibility library to bridge the Android(or iOS) SDK and the Windows 10 SDK. In this paper, wewill focus on native frameworks for cross-platform mobileapp development. Figure 1 shows the typical workﬂow of appdevelopment using a native framework. We use Xamarin as aconcrete example, but the same general workﬂow applies toall such native frameworks.
When an app developer uses native frameworks, he implic-
itly expects the apps to behave consistently across the homeand target platforms. Realizing this expectation depends to alarge extent on the ﬁdelity with which the native frameworktranslates the API calls to SDK of the home platform to thecorresponding SDK of the target platform(s). Unfortunately,this translation is a complex task because the platform mustcorrectly encode the semantics of both the home platformand target platform SDK and the relationship between them.This complexity translates into bugs in the frameworks. Forexample, as of May 2015, Xamarin’s Bugzilla database showsa history of about 7,100 bugs that are related to cross-platformissues
1, about 2,900 of which are still unresolved (listed as
“open” or “new”). Although initial development of Xamarinonly started in 2011, its code is based on Mono, which startedin 2004 as an open-source implementation of .NET. The fact
that such a large number of bugs exist in a mature and heavily-used platform (over 500,000 users) such as Xamarin/Monopoint to the complexity of translating between platforms. Othernative frameworks are no exception, either. Apportable’s bugdatabase, for instance, shows a history of 820 bug reports, 449of which are still unresolved.
In this paper, we develop an approach to test native
frameworks. Speciﬁcally, we aim to discover cases wherethe behavior of the application on the home platform isinconsistent with the behavior of its counterpart on a targetplatform. Our approach is based on diﬀerential testing [33].
We generate random test cases (using methods described inprior work [36]), which in our case are mobile apps in thesource language of the home platform. We then use this codeto produce two versions of the app, one for the home platform,and one for the target platform using the native framework.We then execute the apps and examine the resulting statefor inconsistent behavior. When two versions of the app areproduced from the same source code, any diﬀerences in thebehavior across the versions are indicative of a problem eitherin the home platform’s SDK, the target platform’s SDK, or theway the native framework translates between the two SDKs.
To realize this approach, we must address two issues:
(1) Test Case Generation. The key research challenge in gen-
erating eﬀective test cases is that the space of valid programsthat we can generate as test cases is essentially unbounded.While we could sample from this space, the probability thatthese test cases will induce inconsistent behavior is low.
1There are a total of about 20,300 bugs in the database, related to various
related products oﬀered by Xamarin, e.g., the proﬁler, the IDE environment,
etc. We do not count those bugs because they are not directly related to cross-
platform issues.
442To address this challenge, we observe that the main dif-
ﬁculty in building cross-platform mobile app development
tools is translating between the semantics of the SDKs of the
home and target platforms. Our test-case generator thereforeproduces programs that contain random sequences of invoca-tions to the home platform’s SDK. We then observe whetherthe resulting apps on the home and target platforms behaveconsistently. By focusing on the SDK alone, our approachnarrows testing to the most error-prone components of thecross-platform frameworks.
(2) Test Oracle Design. Each of our test cases is compiled
into a full-ﬂedged app, one each for the home and targetplatforms. When we run the corresponding apps, the test oraclemust observe their behaviors to identify inconsistencies. Themain research challenge here is in deﬁning a suitable notionof “behavior” that can be incorporated into our test oracle.
We address this challenge by observing all data structures
that are reachable from the variables deﬁned in the test cases.We serialize these data structures into a standard format,and compare the serialized versions on the home and targetplatforms. Assuming that the state of the home and targetplatforms is the same before the test cases are executed, theﬁnal state in each platform after the test cases have beenexecuted must also be the same. If not, we consider thisinconsistent behavior and report an error.
We have prototyped this approach in a tool called X-
Checker, which we have applied to test the Xamarin frameworkusing Android as the target platform. Using X-Checker, wehave found 47 inconsistencies, which corresponded to bugseither in Xamarin or the Microsoft SDK (we have reportedthese to Xamarin or Microsoft). To date, 12 of these bugs havealso been ﬁxed in the development branch of Xamarin [9–19]and others are still open.
To summarize, our contributions are:
•We initiate the study of cross-platform mobile app devel-
opment frameworks, and present an analysis of the kinds ofbugs that may arise when these frameworks translate betweenthe semantics of the programming interfaces of two diﬀerentplatforms (Section III).
•We present the design of X-Checker, a testing tool for cross-
platform frameworks that uses random diﬀerential testing toexpose bugs in these frameworks (Section IV). We also presenta number of practical challenges that we had to overcome inthe implementation of X-Checker (Section V).
•We show the eﬀectiveness of X-Checker by applying it to
Xamarin. Speciﬁcally, X-Checker tests Xamarin’s ﬁdelity as ittranslates between the Windows Phone and Android platforms.To date, X-Checker has found 47 bugs, 12 of which have beenﬁxed after we reported them (Section VI).
II. B ackground on Native Frameworks
In this section, we provide background on native frame-
works using Xamarin as a concrete example. Xamarin allowsthe development of native mobile apps for multiple platformswhile aiming to maximize code-reuse across platforms. De-velopers using Xamarin target their apps to its home platform,Windows Phone, and can re-use much of the same code tobuild native apps for iOS, Android, and Mac. In this section,
Fig. 2. Structure of a cross-platform app written using Xamarin.
we discuss the structure of a cross-platform app written usingXamarin, and discuss the techniques that Xamarin uses toallow app logic and data storage code to be written once andreused across platforms.
A developer using Xamarin can build apps in C#, using
features such as generics, Linq and the parallel task library.The developer splits the app into two logical pieces (Figure 2):the application core, which encodes the business logic, andcontains code that is common across all platforms, and user
interface (UI), which is written for each platform and usesthe native UI features of that platform, e.g., buttons, widgets,
and the overall look and feel of the speciﬁc platform. Thedeveloper implements the UI layer in C# as well, using nativeUI design tools such as Android.Views, MonoTouch.UIKit
for iOS, and XAML, Silverlight and Metro APIs for WindowsPhone. The functionality and layout of the UI elements can becontrolled by the business logic in the application core, e.g., in
determining which button triggers what functionality in theapp. Xamarin is built atop the Mono .NET framework [5],
which provides the core cross-platform implementation ofMicrosoft’s .NET framework. C# source code can be compiled
with Xamarin’s compiler to produce a native iOS app, or anAndroid app with integrated .NET runtime support. In this
case, the C# code is compiled to an intermediate language,and packaged with MonoVM conﬁgured for just-in-time com-pilation on Android.
Xamarin aims to provide support to developers to mini-
mize the amount of platform-speciﬁc code that is needed toport an app across platforms. To achieve this goal, one ofthe main components of the core of a Xamarin-based appare cross-platform compatibility libraries, also called portable
class libraries, or PCLs in Xamarin, a technology originally
developed by Microsoft. On Visual Studio and other Microsoftenvironments, a PCL is a special type of a project that allowsdevelopers to write code and produce libraries that can beshared across multiple platforms, such as iOS, Android, andWindows Phone. To support this, PCLs export an interface ofmethods and properties that are portable across platforms, anddevelopers program to this interface. The app developer en-codes platform-independent business logic by programming tothis interface. The PCL provides forwarding stubs that ensuresthat calls to methods or property accesses are routed to thecorrect underlying platform libraries at runtime. The developer
443of the PCL typically identiﬁes the interface by choosing a set
of target platforms that the PCL will support. Because diﬀerentplatforms provide implementations of diﬀering subsets of thebase.NET class library, the PCL interface is typically restricted
to the common .NET functionality that is supported by all the
target platforms.
PCLs play a key role in Xamarin because they serve as
the compatibility layer between two diﬀerent platforms. Aspreviously mentioned, about Xamarin’s BugZilla database listsabout 7,100 that are related to PCL. Despite the functionalityprovided by the PCLs, some platform-dependent business logicmay be necessary in the application core. For example, PCLsare still in active development, and if the app developer wishesto use features that are not currently supported by the PCL, hehas to do so by writing platform-speciﬁc code, called shared
assets on Xamarin. It is possible to write this code once and
compile it for all desired target platforms using compiler orpre-processor directives (e.g., code speciﬁc to Android or iOS
would be guarded using a directive such as #ifdef ANDROID
or#ifdef iOS, respectively). Naturally, the goal of projects
such as Xamarin is to increase the coverage provided by theirPCLs, so as to minimize the amount of code that must bewritten as shared asset projects.
In addition to the application core, the app also includes UI
code. Currently, UI code is largely platform-speciﬁc becauseUI elements, e.g., the look and feel of buttons and widgets, are
customized to speciﬁc mobile platforms. Nevertheless, thereare ongoing eﬀorts such as Xamarin.Forms to even minimize
the amount of platform-speciﬁc UI code.
In this paper, we are primarily concerned with testing the
functionality of the PCLs on Xamarin that provide support forplatform-independent app code. Therefore, the test cases gener-ated by X-Checker only target the PCL interface. Our test casesdo not directly target the platform-speciﬁc UI code. However,note that many aspects of the layout and functionality of theUI are controlled by the business logic, which interacts withthe target platform’s SDK via the PCLs. Therefore, by testingthe functionality of the PCLs, we indirectly test the overallfunctionality of the app’s execution on the target platform(including its UI).
III. Inconsistent Behavior
In this section, we present a few motivating examples of
real inconsistencies that X-Checker found in Xamarin. For ourexamples, we use Android as the target platform; the defaulthome platform is Windows phone. We use these examples tomotivate some design features of X-Checker, and classify thetypes of inconsistencies that it can identify.
Figure 3 shows a test case generated by X-Checker. This
code is in C# and uses classes and methods from the WindowsPhone SDK. In this test case, the code ﬁrst creates two objects,base andexp, from the Systems.Numerics.Complex class,
and initializes them to 0+0i and 1+0i. On line (8), it uses the
Complex.Pow operation to raise base to the power of exp.
We used the Visual Studio toolkit and the Xamarin frame-
work to produce a Windows Phone app and an Androidapp, respectively, and ran the apps on the correspondingplatforms. Both apps execute and return success. However, in(1)using System.Numerics;
(2)Serializer serializer; // Serializer is a data structure serializer.
(3)public class TestCase {
(4)public static int TestMain (MyFileIO serialStream, MyFileIO logStream) {
(5) try{
(6) Complex base = new Complex(0,0);
(7) Complex exp = new Complex(1,0);
(8) Complex res = Complex.Pow(base,exp);
(9) serialStream.append(base.GetType().FullName, serializer.serialize(base));
(10) serialStream.append(exp.GetType().FullName, serializer.serialize(exp));
(11) serialStream.append(res.GetType().FullName, serializer.serialize(res));
(12) return success;
(13) }catch (System.Exception e) {
(14) logStream.append(e.GetType().FullName);
(15) return exception;
(16) }
(17)}
(18)}
Fig. 3. A test case that illustrates inconsistent handling of the semantics of
the Windows Phone SDK. The values of res are diﬀerent in the Windows
Phone and the Xamarin-produced Android versions of this code.
Type Platform 1 Platform 2 Consistency checks Example
(1) /check /check Check app state Figure 3
(2) × × Check exception code Figure 5
(3) /check × Always inconsistent Figure 6
Fig. 4. Diﬀerent ways in which a test case produced by X-Checker can exhibitinconsistent behavior when executed on two platforms. /checkdenotes that the test
case returns success, while ×denotes that the test case returns exception.
the Windows Phone app, the value of res is 0+0i, while on
the Android app, the value is NAN (not a number). This is
clearly an inconsistency in the way the two apps handled the
semantics of the Complex.Pow operation. Since we reported
this bug on Xamarin’s BugZilla forum, it has been ﬁxed in themaster branch for the next release [17].
In this example, eliciting the inconsistent behavior between
the Windows Phone and the Android versions of the apprequires the calls on lines (6)-(8), with the corresponding datadependencies. To systematically uncover more examples ofsuch inconsistencies, X-Checker must therefore generate manymore such test cases by systematically invoking methods fromthe API of the Windows Phone SDK with suitable arguments.
When the apps produced from these test cases are executed
on their corresponding platforms, inconsistent behavior maymanifest itself in one of three ways (Figure 4). The ﬁrst way,as illustrated in the example in Figure 3, is where the testcase returns success on both platforms, but the resulting state
is diﬀerent. Such inconsistencies are latent in the state of theapps, in this case, the values of the objects, and are not visibleunless this state is made explicit and compared across the twoversions.
X-Checker achieves this goal by serializing all objects that
are reachable from the variables that are in scope within thesource code of the app. Lines (9)-(11) in Figure 3 show theobjects being serialized and appended to a log. X-Checkercompares the logs produced by the Windows Phone and theAndroid versions of the apps to identify inconsistencies. Inthis example, serializing the Complex object simply prints
its value to the log. However, X-Checker’s serializer supportsarbitrary data types, and serializes them in a custom format.The serializer itself is written in C#, with calls to the WindowsPhone SDK, and is included as a library within the nativeapp. As with all our test cases, we use Xamarin to producethe Android version of the serialization library. Because X-Checker’s test cases include calls to the serializer in the sourcecode of the test case, we expect the serialized versions ofsimilar objects to also be similar on the Windows Phone andAndroid versions.
444(1)public class TestCase {
(2)public static int TestMain (MyFileIO serialStream, MyFileIO logStream) {
(3) try{
(4) string s = "test";
(5) Int32 index = -1;
(6) Double val = System.Globalization.CharUnicodeInfo.GetNumericValue(s, index);
(7) return success;
(8)}catch (System.Exception e) {
(9) logStream.append(e.GetType().FullName);
(10) return exception;
(11) }
(12)}
(13)}
Fig. 5. A test case that triggers an inconsistent exception behav-
ior. In the Windows Phone version of this code, line (6) throws aSystem.ArgumentOutOfRangeException, while on the Xamarin-producedAndroid version, it throws a System.IndexOutOfRangeException.F o r
brevity, we have omitted some code, such as calls to the serializer.
A second way for inconsistencies to manifest is when
a test case returns exception on both platforms, but the
exceptions thrown are diﬀerent on both platforms. Figure 5
illustrates a test case in which this scenario occurs. Thecall on line (6) throws an exception because the valueofindex is negative. However, the Windows Phone
version throws a System.ArgumentOutOfRangeException,
while the Android version throws aSystem.IndexOutOfRangeException. In this case, theWindows Phone and Android versions are inconsistent inthe way they handle the semantics of the GetNumericValue
method. X-Checker therefore logs the exception code, andcompare it across executions of the apps on the two platforms.This bug has also been ﬁxed in the master branch for the nextrelease [9].
In cases such as these, where the test cases on both
platforms throw exceptions, the logs only contain the exceptioncode. In particular, the logs do not contain the serialized datastructures because the calls to the serializer appear before thereturn success statement, and the exception was raised before
control reached the calls to the serializer. It may be possiblethat both the Windows Phone and the Android versions throwthe same exception code, but the state of the data structuresin the apps may have diverged before the code that raisedthe exception was executed, which is also an example ofinconsistent behavior. As will be clear when we discuss X-Checker’s approach to test case generation in Section IV, X-Checker would have also identiﬁed the divergence of state.In particular, X-Checker uses an iterative test case generationalgorithm that preserves the following property: any preﬁx of amethod sequence in a test case generated by X-Checker is alsoa test case that would have been generated by X-Checker in aprevious iteration. Therefore, if the state is inconsistent aftera call sequence preceding the exception-generating method,it would have been identiﬁed as an inconsistency when theshorter method sequence was used as a test case.
Note that in Figure 5, the test case executes the code
and catches a generic System.Exception. In practice, it
may be that a developer writing a Windows Phone app,familiar with the Windows Phone SDK, may write this codeto catch a System.ArgumentOutOfRangeException.I fh e
uses Xamarin to produce an Android app, it is possible for thethe inconsistent behavior to manifest itself in one of the othertwo forms shown in Figure 4.
The ﬁnal possibility for an inconsistency is when a test
case returns success on one platform, and exception on the
other. Figure 6 shows an example of such a test case. The(1) using System.Xml;
(2) public class TestCase {
(3) public static int TestMain (MyFileIO serialStream, MyFileIO logStream) {
(4) try{
(5) NameTable nt1 = new NameTable();
(6) NameTable nt2 = new NameTable();
(7) XmlNamespaceManager nsMgr = new XmlNamespaceManager(nt2); ...
(8) XmlParserContext xpctxt = new XmlParserContext(nt1, nsMgr , ...); ...
(9) return success;
(10) }catch (System.Exception e) {
(11) logStream.append(e.GetType().FullName);
(12) return exception;
(13) }
(14) }
(15) }
Fig. 6. A test case that triggers an exception in the Windows Phone version.
The constructor on line (8) throws an XmlException because nsMgr is
independent of nt1. This test case executes without throwing an exception
on the Xamarin-produced Android version.
XmlParserContext constructor in line (8) expects its sec-
ond argument (nsMgr) to be created from the ﬁrst argument
(nt1). However, in this case, nsMgr is created from another
object nt2. As a result, this constructor call must throw an
XmlException according to Microsoft’s documentation, and
it does on the Windows Phone version. However, on theAndroid version the constructor executes without throwing anexception. As with the previous two bugs, this one also hasbeen ﬁxed by Xamarin developers after we reported it [11].
IV . Design of X-Checker
X-Checker aims to ﬁnd bugs in Xamarin by generating
apps, executing these apps on Windows Phone and Android,and looking for inconsistencies in them. Thus, X-Checker’sdesign consists of two parts, the test case generator and theinconsistency checker.
Test Case Generation. X-Checker generates test cases that
exercise the programming API used by Windows Phone devel-
opers. As illustrated in Section III, each test case is a sequenceof method calls to this API. The arguments to these calls areeither values with primitive data types, or references to objectsconstructed and modiﬁed by method calls appearing earlier inthe sequence. The main challenge is to generate meaningfulmethod sequences that are also eﬀective, i.e., the test case
generator should be able to elicit error cases in Xamarinwithout generating too many test cases.
This problem has been investigated in the past in the con-
text of generating unit tests for object-oriented programs, andtools such as JCrasher [27] and Randoop [36] implement suchtest case generation. In particular, Randoop uses a feedback-
directed approach to random test generation and is the basis
for X-Checker’s test generator as well. We now brieﬂy de-scribe Randoop’s (and therefore X-Checker’s) approach to testgeneration.
The test generator accepts as input a list of classes to
be tested, a set of ﬁlters and contracts (which are sanitychecks to be performed), and a timeout. Intuitively, the testgeneration algorithm iteratively “grows” method sequencesfrom previously-generated shorter sequences. Suppose that thetest generator has already generated a set of method sequencesas valid test cases. During each iteration, the test generatorpicks a method m(T
1,...,T n)at random from the list of
classes provided to it as input, and “extends” the existingmethod sequences with a call to m(e.g., one way to “extend”
is to append mto the end of the sequence). If the parameters
ofmare all of primitive type, then the test generator randomly
selects the values of these parameters from a pool of acceptable
445values. If the parameter is a reference to an object, then the test
generator uses an object of suitable type created by a method inthe sequence that mjust extended (or passes a null reference).
X-Checker then wraps this method sequence with templatecode to serialize data structures and to catch exceptions, asshown in the examples from Section III, to produce the testcases.
The test generator then executes the newly-generated test
sequences looking for violations of ﬁlters and contracts. Theseare sanity checks that look for common error cases, suchas test cases that throw an exception, or those that violatecertain invariants (e.g., o.equals(o) not returning true). Test
sequences that violate these sanity checks are discarded, andthe remaining test cases are added to the set of valid test cases,to be extended in future iterations. This process continues tillthe speciﬁed timeout has expired. This iterative approach is keyto generating eﬀective test cases. It ensures that every preﬁxof a valid test sequence is also valid, and that test sequencesthat violate simple sanity conditions (e.g., those that throw an
exception) are never extended.
Serializing State and Checking Inconsistencies. For the test
cases generated using the approach above, X-Checker produces
a pair of apps for Windows Phone and Android. It executesthem atop these platforms to observe inconsistent behavior. Wenow discuss the design of the serializer, which helps identifyinconsistencies when both apps return success, i.e., the ﬁrst
case in Figure 4. The other two cases are straightforward andwe do not discuss them further.
The serializer recursively traverses object references to
create serialized representations. Intuitively, a serialized rep-resentation is a set of (
key,value ) pairs. The key is the name
of a public ﬁeld of the object. For ﬁelds of primitive type(e.g.,bool, int,String), the
value is simply the actual value
of the ﬁeld. For ﬁelds that are themselves object references,the value is a serialized representation of that object. Theexample below shows the serialized representation of a linkedlist with two entries. The data ﬁeld of the entries store 1and
2, respectively.
{(“data”, 1),(“next”, {(“data”, 2),(“next”, null)})}
X-Checker’s serializer uses the Json.NET [4] library,
which optionally supports the ability to serialize cyclic datastructures. It does so by keeping track of object referencesusing an additional identiﬁer. However, in our experience, therandom test cases that we generate do not produce cyclicheap data structures. We therefore did not enable support forserializing cyclic data structures in our prototype, and theserialized object representations are tree-structured as a result.Note that in the unlikely case that a test case does producea cyclic data structure, our serializer would inﬁnitely loop—asituation we have not encountered to date in our experiments.
X-Checker identiﬁes inconsistencies by comparing seri-
alized representations of objects on the home and targetplatforms. Comparison proceeds recursively in a bottom-upfashion. All the (
key,value ) pairs storing primitive types must
match, and the tree-structure of the serialized representationmust be the same, i.e., the same
keys on both platforms at each
level of the tree. Any mismatches indicate inconsistent state. Inmost of the bugs that we found, the mismatches were becausethe
value s diverged (e.g., the complex number example in
Figure 6). However, we also found cases where the ﬁelds inthe objects were diﬀerent on Windows Phone and Androidbecause a ﬁeld that was declared to be public on Windows
Phone was a private ﬁeld in Android, and therefore not listed
in the serialized representation.
As previously discussed, the feedback-directed approach
to test case generation does not extend any method sequencesthat violate its ﬁlters and contracts, e.g., sequences that throw
an exception when executed. While Randoop was originallydesigned for unit-testing object-oriented programs, X-Checkerextends it for cross-platform diﬀerential testing. For practicalreasons described in Section V, X-Checker ﬁrst executes thetest case generator on one platform, where it uses the iterativeapproach to output test cases. It then executes these test caseson the target platform (Android). Thus, X-Checker’s test casesalso preserve the property that only non-exception-generatingtest cases are extended in the iterative process.
However, because X-Checker generates all the test cases
on the home platform before executing them on the targetplatform, even those test cases that return success but pro-
duce inconsistent serialized state across the two platforms areextended during test generation. As a result, it is possible thatmultiple test cases produced by X-Checker may report thesame inconsistency.
Discussion. Diﬀerential testing oﬀers an attractive property. If
a test case is executed on two API implementations with the
same initial state, any inconsistency in the ﬁnal states indicatesa problem in at least one of the API implementations. That is,diﬀerential testing does not produce false positives.
However, in practice, it is possible that an inconsis-
tency does not always correspond to a problem. In ourexperiments, we found that such a situation could arisebecause of any one of a small number of reasons. First,some API methods, such as those from System.Random and
System.Time, invoke platform-speciﬁc features and returndiﬀerent values when invoked on diﬀerent platforms. Forexample, the System.Net.Cookie() constructor initializes
Cookie.TimeStamp with the current system time. Unless
the emulation environments that run the apps for both thehome and target platforms are synchronized, this call willreturn diﬀerent values. Second, for some methods, such asObject.GetHashCode, the documentation speciﬁes that thebehavior of the method varies across platforms. That is, theHashCode of an object can be diﬀerent on the home and
target platforms even if the serialized representations of theobject are the same on both platforms. A third source offalse positives was because the Mono runtime included in aXamarin-produced Android app uses Mono Assemblies as itslibraries. These libraries have diﬀerent metadata informationthan their Windows Phone counterparts, and any calls thataccess this metadata will result in inconsistent serialized state.
Fortunately, it is relatively easy to ﬁlter out test cases that
can potentially lead to such false positives. We simply integrateﬁlters that prevent the test generator from producing methodsequences that contain method calls or ﬁeld references thatcan potentially trigger such false positives. Thus, with just afew ﬁlters to eliminate the causes above (see Figure 7), wewere able to eliminate false positives, thereby ensuring that all
446Filtered classes: All methods/variables of this class ﬁltered.
System.Random — Members return random values
Filtered methods: Methods cannot appear in test cases.
System.Type GetType() — Return value may include information from
the underlying C# assembly, which is not
uniform across platforms
System.Int32 GetHashCode() — Documentation speciﬁes that hash code of
similar objects need not be similar acrossplatforms
Filtered constructors: Constructor cannot appear in test cases.
System.Xml.UniqueId() — Returns a unique GUID, which is not guaran-
teed to be consistent across platforms
Filtered ﬁelds/properties: Cannot be accessed in test cases.
System.Net.Cookie.TimeStamp — Value relies on system time at object creation
System.DateTimeOffset.Now — Value relies on system time
System.DateTimeOffset.UtcNow — Value relies on system time
System.DateTime.Now — Value relies on system time
System.DateTime.UtcNow — Value relies on system time
System.DateTime.Today — Value relies on system time
System.Exception.HResult — Value identiﬁes an exception, but documenta-
tion is not conclusive about whether value isconsistent across platforms
Fig. 7. Filters used by X-Checker to avoid generating test cases that produce
false positives.
inconsistencies reported by X-Checker indeed correspond to
real bugs. Note, however, that as with most other testing tools,our diﬀerential testing approach does have false negatives—i.e., it is not guaranteed to ﬁnd all possible inconsistencies.
V. P ractical Considerations
In this section, we discuss a few practical issues that we
had to address as we built X-Checker.
Where to Generate Test Cases? The ﬁrst practical considera-
tion that we addressed was the question of which platform to
use to execute our test generator. One possibility was to useWindows Phone, Xamarin’s home platform. This requires usto execute X-Checker directly on a device or emulator runningWindows Phone. However, we found that the environmenton such devices and emulators was somewhat awkward touse during active development of X-Checker, e.g., to debug
any issues that arose. We therefore decided to develop andexecute X-Checker on a desktop version of Windows (8.1). Ourhypothesis was that the desktop and phone versions would belargely similar because they use the same .NET code base, and
as a result, we could use the desktop to generate the test casesand execute them as apps on Windows Phone and Androiddevices.
This approach eased development of X-Checker, and for
the most part our hypothesis about the equivalence of thedesktop and phone version of Windows was correct. How-ever, we found (and reported to Microsoft) a case wherethe desktop and phone versions diverged in their semantics.In particular, the public property CurrencyDecimalDigits
of the class System.Globalization.NumberFormatInfo is
required to be a read-only ﬁeld according to MSDN docu-mentation. While the read-only property holds in the desktopversion, the property is mutable in the Windows Phone version.We also found, using diﬀerential testing against Android, acase where both the desktop and phone version of Win-dows incorrectly implement the documented semantics for agiven property, while Android’s implementation followed Mi-crosoft’s documentation. In particular, the property WebName of
System.Text.Encoding.BigEndianUnicode must have the
(a) Windows Phone version (b) Android version
Fig. 8. Screenshots showing the UIs of the test case apps on Windows Phone
and Android.
value UTF-FFFE according to Microsoft’s documentation, but
the desktop and phone version of Windows return UTF-16BE.
PCLs and Test Generation. A second issue that we had to
address was the integration of X-Checker’s test generator and
PCLs. As previously discussed, X-Checker’s test generatorproduces test cases for a given input set of classes. It usesreﬂection to identify public methods from those classes, thedata types of their arguments and the other properties ofthese classes, and uses this information in its test generationalgorithm.
However, PCLs pose a unique problem when used with
X-Checker’s test generator. Recall that PCLs deﬁne the in-terface against which developers can build their applicationswithout concerning themselves with cross-platform portabilityissues. PCLs enable this feature by transparently acting asforwarders—they route calls from the application layer to thecorresponding library in the platform on which the applicationis loaded. Thus, PCLs usually do not contain any of theexecutable code of the classes for which they act as aninterface, and merely contain forwarding stub code. As a resultof this feature, when X-Checker’s test generator is providedwith a set of PCL classes as input, it is unable to use reﬂectionto fetch the complete set of public methods, data types andproperties of the classes for which the PCL acts as a forwarder.
To address this issue, we had to extract the information
required by X-Checker’s test generator by loading PCLs ina non-executable reﬂection-only mode. In this mode, PCLs
are not executable, but can be queried using reﬂection andreturn metadata by accessing the corresponding classes on theplatform. We then re-load the PCLs in executable mode, anduse the metadata to iteratively generate and execute test cases.
How to Package Test Cases? Finally, we also had to address
the issue of how to package up the test cases for execution
on the Windows Phone and Android platforms. Each testcase is packaged as a separate class that can be instantiatedand executed. As discussed above, we run the iterative testgeneration algorithm on the desktop version of Windows. Asa result, we have all the test cases available for batch executionon the mobile platforms.
We package all the test cases into a single app each for
execution on the two mobile platforms. Both Windows Phone
447and Android require apps to deﬁne a UI. We wrote this
UI and the code to interface with the ﬁle system (to storethe logs generated when test cases are executed) within aplatform-speciﬁc presentation layer, and packaged up the testcases as platform independent code to be cross-compiled byVisual Studio and Xamarin. All the test cases generated byX-Checker can be invoked at the press of a single button onthe app. Figure 8 shows the UIs of the Windows Phone andAndroid versions of these apps. The UIs of these apps lookrather diﬀerent—each app uses buttons and icons unique tothe corresponding mobile platform. However, because our testcases focus only on the platform-independent PCL classes, thediﬀerences in the UI state do not manifest as divergent state(and therefore as inconsistencies) during the execution of thetest-cases.
VI. Experimental Results
Setup. For our experimental evaluation, we used Xam-
arin.Android version 4.16.0, business edition. We chose Win-dows 8.1 as the home platform, and Android 4.0.3 (APIlevel 15) as the target platform. As discussed in Section V,we generate test cases on a desktop version of Windows,and then run these cases on Windows Phone and Androidplatforms. Both the phone and desktop version of Windows use.NET version 4.5.51641. We use Visual Studio Ultimate 2013
version 12.0.30501 as the IDE to compile our test cases. Thisenvironment supports a package that integrates the tools forWindows Phone 8.1 into the controls of Visual Studio. We alsouse the same development environment to build the Androidversion using Xamarin. In particular, we use the Xamarin3.5.58.0 extension to enable development for Xamarin.Androidwithin Visual Studio.
We use emulators to mimic Windows Phone and Android
devices. Microsoft oﬀers a few pre-conﬁgured emulation envi-ronments for Windows Phone: our experiments use Emulator8.1/WVGA-4inch/512MB conﬁguration. We conﬁgured theAndroid emulator to match the hardware conﬁgurations of theWindows Phone emulator.
Examples of Inconsistent Behavior. Figure 9 presents the
results of our experiments. To date, we have used X-Checker
to generate 22,465 test cases, which invoke 4,758 methodsimplemented in 354 classes across 24 Xamarin DLLs. Inall, we observed 47 unique instances of inconsistent behavioracross Windows Phone and Android. The results also show adetailed breakdown of these inconsistencies by category, wherethe type of the inconsistency is as deﬁned in Figure 4.
In most cases, we were quickly able to quickly conﬁrm us-
ing MSDN and Xamarin documentation that the inconsistencywas indeed a bug in Xamarin. For each type of inconsistency inFigure 4, the test cases that induced them and the inconsistentresults they produced were largely similar to the examplesdescribed in Section III. We now discuss a few interestingexamples of inconsistencies that we found.
(1) Precision in math libraries. We observed two inconsisten-
cies that were related to precision with which math librariesused rounding and precision to represent numbers. In one testcase, a call to System.Math.IEEERemainder(double x,
double y) was invoked with x=1.49089193085384E-81
andy=2.22275874948508E-162. The Windows Phone ver-sion returns a result of 3.33639470813326E-163, while the
Android version produced by Xamarin returns 0.
The second test case was a method sequence with two
calls. The ﬁrst call, System.Math.Round(Decimal d, int
i, MidpointRounding mode) was invoked in the test case
asSystem.Math.Round(2, 3, ToEven). According to the
documentation, this call returns the value drounded with i
fractional digits in the given mode. The Windows Phone ver-
sion returns 2.000 while the Android version returns 2. While
these are equivalent if used in a mathematical calculation, thesecond call in the test case converted them to strings usingSystem.Convert.ToString, which resulted in inconsistentserialized state. These examples highlight inconsistent han-dling of ﬂoating point arithmetic across platforms.
(2) Ambiguous documentation of exception semantics.
We observed one test case where diﬀerent exceptionswere raised for the same failing method call because ofambiguity in the semantics of the exception to be raised.According to documentation, the NameTable.Add(Char[]
key, int start, int len) call can throw two types
of exceptions. It throws IndexOutOfRangeException
when any one of these three conditions is met: 0>start ,
start/greaterorequalslantkey.Length,o r len/greaterorequalslantkey.Length. It throws
ArgumentOutOfRangeException iflen<0.
In one of our test cases, the values of start and
len were such that 0>start andlen<0. For this test
case, both the Windows Phone and desktop versions threwIndexOutOfRangeException whereas Xamarin’s Android
code threw ArgumentOutOfRangeException. Although both
implementations are correct, the documentation must be clar-iﬁed to remove this ambiguity.
(3) Documented deviations of behavior . For some methods,
documentation speciﬁes that the behavior of the method willvary across platforms. Thus, the Xamarin and .NET imple-
mentations of these methods need not be similar. For exam-ple, consider the constructor for the UriBuilder class. The
documentation speciﬁes that if this class is implemented in aPCL, then if an invalid URI is provided as the string argumentto the constructor, it must throw a FormatException instead
of aUriFormatException.
We also observed examples where the deviations in be-
havior were not speciﬁed formally in the documentation, butwere known to the developers of the platform. One suchexample is the method ReadContentAsString from the class
XmlDictionaryReader. When included in a test case, thismethod showed inconsistent behavior across the WindowsPhone and Android versions. However, when we tried toidentify the cause of this bug by examining the source codeof the Mono platform (which Xamarin extends to provide across-platform implementation of .NET), we found that it was
marked with a MonoTODO attribute, indicating a known issue
with its implementation.
We were not aware of these documented deviations in
behavior when we tested the methods using X-Checker, andthe resulting diﬀerences were reported as inconsistencies.However, because the documentation (or code comments) didspecify that the inconsistencies were expected across plat-forms, we do not count these as bugs (and therefore they donot factor into the 47 inconsistencies reported in Figure 9).Nevertheless, we feel that for such methods, the deviations
448Library #Classes #Methods #Tests #Inconsistencies (by type)
Type 1 Type 2 Type 3
Microsoft.CSharp 6 56 1,848 0 0 0
Microsoft.VisualBasic 17 127 613 0 0 0
System.Collections.Concurrent 10 77 349 0 0 0
System.Collections 29 172 532 0 0 0
System.ComponentModel 5 4 1,578 0 0 0
System.Dynamic.Runtime 29 201 790 1 0 0
System.Globalization 14 288 567 3 3 0
System.Linq 5 172 591 0 0 0
System.Linq.Expressions 44 633 590 1 0 1
System.Net.Http 44 524 746 3 0 3
System.Net.NetworkInformation 1 1 1 0 0 0
System.Net.Primitives 13 105 956 0 1 1
System.Net.Requests 10 122 1,269 0 0 0
System.ObjectModel 16 52 1,573 0 0 0
System.Resources.ResourceManager 4 28 1,333 0 1 0
System.Runtime.Extensions 12 409 946 3 1 1
System.Runtime.Numerics 2 170 1,514 3 0 2
System.Runtime.Serialization.Json 4 37 1,642 1 0 0
System.Runtime.Serialization.Primitives 13 86 1,387 1 0 1
System.Runtime.Serialization.Xml 14 342 420 1 3 1
System.Text.Encoding 5 66 940 1 0 0
System.Text.RegularExpressions 10 103 848 0 0 0
System.Xml.ReaderWriter 24 346 820 2 3 3
System.Xml.XDocument 23 637 612 0 1 1
20 13 14
Total 354 4,758 22,465 47
Fig. 9. Summary of inconsistencies found by X-Checker in various Xamarin libraries. This table shows the number of classes in each library and the number
of methods in these classes. It also shows the number of test cases that X-Checker generated for those libraries, and the number of cases of inconsistent behavioracross platforms. These inconsistencies are sorted by type, as deﬁned in Figure 4.
of behavior should be encoded more explicitly (e.g., as pre-
conditions) rather than being latent in documentation or in
code comments.
Performance. Finally, we report the time taken to run test
cases on our experimental setup. We ran the Windows Phone
and Android emulators on a desktop system running Windows8.1 professional edition, and equipped with an Intel Core-i7-3770 running at 3.4GHz, 16GB of RAM. We created anapp that packaged 1000 randomly-generated tests and ran theWindows Phone and Android versions of this app on bothemulators. The Android emulator took 29.1 seconds to run theapp, while the Windows Phone emulator took 2.7 seconds. TheAndroid emulator is much slower because it emulates the ARMarchitecture atop our Intel platform. In contrast, the WindowsPhone “emulator” uses hyper-V and is implemented as a virtualmachine. The tests used to report the results in Figure 4 weregenerated by analyzing each library separately. We conﬁguredour test generator to emit test cases until a timeout of 300seconds was reached for each library being analyzed.
VII. Threats to Validity
Our results show the eﬀectiveness of using random dif-
ferential testing at ﬁnding bugs in native app developmentframeworks. We now summarize the threats to the internal andexternal validity of our results.
The main threat to internal validity is in determining
whether an inconsistency discovered by X-Checker is indeeda symptom of a bug in Xamarin. Although an inconsistencymanifests itself in one of the three diﬀerent ways outlinedin Figure 4, it may be the result of using a method witha documented diﬀerence in behavior across platforms. Weaddressed this threat in two ways. First, as discussed inSection IV, we created ﬁlters for methods with documenteddeviations of behavior, so false-positive-generating test casesare not produced. Second, we studied the results of X-Checkerto understand the cause of the inconsistency. In some cases,this study lead us to a sentence in the documentation or codecomments where the inconsistency was documented (as dis-cussed in Section VI). We did not include these inconsistenciesin our overall count shown in Figure 9, and reported the otherinconsistencies to the Xamarin BugZilla forum.
A second threat to internal validity is the “seriousness” of
the bugs found by X-Checker—i.e., does an inconsistency lead
to a serious error in the functioning of an app, or is it just aminor annoyance? Unfortunately, this aspect is much harderto evaluate. Our take on the issue is that an inconsistencyis indeed a bug that must be ﬁxed (or suitably documented).However, the fact that 12 (over 25%) of the inconsistencies thatwe found lead to bug-ﬁxes within days of our reports showsthat Xamarin developers did perceive the inconsistencies asbeing signiﬁcant.
The main threat to external validity is the ability of our
approach to generalize to other native frameworks, or evenother aspects of Xamarin (e.g., the compatibility libraries used
to translate between Windows Phone and iOS). We currentlydo not have experimental data to answer such questions. Nev-ertheless, our results with Xamarin PCLs for Android indicatethat inconsistencies arise because of the challenges involvedin translating the semantics of two diﬀerent mobile platforms.In particular, an analysis of our results does not indicatethat the kinds of inconsistencies we found are symptomaticof problems either in Windows Phone or Android alone.Therefore, we hypothesize that random di ﬀerential testing of
449other native frameworks is quite likely to ﬁnd similar bugs in
them as well.
VIII. Related Work
Testing Cross-platform Apps. To our knowledge, our work
is the ﬁrst on testing cross-platform mobile app developmentframeworks. However, there has been prior work on testingcross-platform apps themselves. The most relevant projects inthis area are X-PERT [25] and FMAP [26]. Both projects startwith the observation that an increasing number of Web applica-tions create customized views of Web pages, each optimizedfor diﬀerent platforms, e.g., form factors, mobile platforms,
and Web browsers. Yet, end-users expect Web applicationsto behave consistently across these platforms. The X-PERTproject aims to detect inconsistencies in the way that Web appsare displayed by these platforms. Dually, FMAP attempts toidentify similar elements on Web pages that may be rendereddiﬀerently on diﬀerent platforms.
Our work diﬀers from these projects in that it uses incon-
sistencies in apps to identify problems in the app developmentframeworks. While our work has primarily targeted APIs usedto support application logic, future work on testing mobileapps created using Web-based frameworks (e.g., [1, 3, 7])
can possibly use the techniques from X-PERT and FMAP toidentify inconsistencies in the way UI elements are displayedacross platforms.
Aside from testing techniques for cross-platform apps, a
number of recent projects [20–22, 32, 35, 37] have been devel-oping techniques to test mobile apps. The main goal of thesetechniques is to devise eﬀective input generation techniquesfor mobile apps, which is challenging because mobile appsare UI-based and event-driven. Although these projects are notdirectly related to our work, the input generation methods thatthey develop can potentially be used to identify inconsistenciesin the UIs and UI-handling code of cross-platform apps.
Zhong et al. [42] consider the related problem of test-
ing cross-language API mapping relations. Such a relation<f
S,fT>encodes that a method fSimplemented in a library
written in a source language implements the same features asthe method f
Twritten for an equivalent library in a target
language. Zhong et al. also use random diﬀerential testing
as the strategy to identify inconsistencies in these relations.Their ﬁndings are similar to ours, and showcase the diﬃcultyof correctly translating functionality across diﬀerent platformsand languages.
Random and Diﬀ erential Testing. There is a rich literature
on both random testing and diﬀerential testing, and both
methods have extensively been used for bug-ﬁnding. Fuzz-random testing, for example, feeds random inputs to ap-plications under test. Crashing applications are most likelybuggy because they do not handle ill-formed random inputsproperly. This method has been used to ﬁnd bugs in UNIXutilities [34] and GUI-based Windows applications [28]. Forobject-oriented code, JCrasher [27] generates random unittests, and uses crashes to identify buggy class implementations.Diﬀerential testing, originally introduced by McKeeman [33]has recently found a number of interesting applications insecurity (e.g., [23, 24, 39]).Random and diﬀerential testing can be usefully combined
as is evident from our results. This method has previously beenused successfully, for instance, to ﬁnd bugs in compilers [41].The authors of Randoop also used this method to test twoversions of the Java development kit, ﬁnding a number of bugsalong the way.
Implementing Cross-platform App Frameworks. As already
discussed in Section I, there is signiﬁcant recent interest
in techniques to develop cross-platform mobile apps. Forthis task, the dominant methods are the use of Web-basedframeworks, which support app development in Web-basedlanguages, and native frameworks, which create apps thatcan natively execute on the platform. These frameworks domuch of the leg-work necessary to translate API calls acrossplatforms. To our knowledge, these translations are createdmanually by domain experts. The software engineering re-search community has proposed methods to automaticallyharvest cross-platform API mappings by mining existing codebases (e.g., [29, 38, 43]). Such techniques could potentially be
used to improve the way that cross-platform app developmentframeworks are built.
IX. Summary and Future Work
Developers are eager to deploy their mobile apps on as
many platforms as possible, and cross-platform mobile appdevelopment frameworks are emerging as a popular vehicleto do so. However, the frameworks themselves are complexand diﬃcult to develop. Using X-Checker to test Xamarin,we showed that diﬀerential testing is an eﬀective methodto identify inconsistencies in the way that these frameworkshandle the APIs of the home and target platforms.
While X-Checker has been highly eﬀective, it suﬀers from
a number of limitations that we plan to remedy in future work.First, while X-Checker uses random method sequences as testcases, the arguments to these methods are drawn at randomfrom a ﬁxed, manually-deﬁned pool. We plan to investigatetechniques to invoke methods with random, yet meaningfularguments, which would further increase the coverage of theAPI during testing. Second, X-Checker has primarily focusedon testing the framework libraries that provide support for theplatform-independent part of cross-platform apps. However,when end-users interact with apps that have been cross-compiled, they also expect a similar end-user experience wheninteracting with the app’s UI. To ensure this property, wemust test that semantically-similar UI elements on diﬀerentplatforms elicit the same behavior within the apps on thecorresponding platforms. This will likely require an analysis ofthe elements of the UI itself, and recent work on cross-platformfeature matching [26] may help in this regard. Finally, weplan to extend X-Checker to work with other target platforms(e.g., Xamarin for iOS) and with other cross-platform app-
development tools.
X. A cknowledgments
We thank the reviewers for their comments and Thomas
Ball for encouraging us to consider Xamarin as our experi-mental platform. This research was funded in part by NSFgrants 0952128, 1408803 and 1441724.
450References
[1] Adobe PhoneGap. http:// phonegap.com.
[2] Apportable – Objective-C for Android. www.apportable.com.
[3] IBM MobileFirst platform foundation. http:// www-03.ibm.com/
software/ products/ en/mobileﬁrstfoundation.
[4] Json.NET – popular high-performance JSON framework for
.NET . http:// james.newtonking.com/ json.
[5] Mono – Cross-platform, open source .NET framework. http:
//www.mono-project.com.
[6] MyAppConverter – develop once, run anywhere. http:// www.
myappconverter.com.
[7] Sencha: HTML5 app development. http:// www.sencha.com.
[8] Xamarin – Mobile App Development and App Creation Soft-
ware. http:// xamarin.com.
[9] Xamarin Bug 25895. Wrong exception is thrown when calling
System .Globalization .CharUnicodeInfo .GetNumericValue with
invalid index. https:// bugzilla.xamarin.com/ show bug.cgi?id=
25895.
[10] Xamarin Bug 27901. XmlConvert .ToString returns wrong value.
https:// bugzilla.xamarin.com/ show bug.cgi?id=27901.
[11] Xamarin Bug 27910. XmlParserContext constructor not throw-
ingXmlException when it should. https:// bugzilla.xamarin.com/
show bug.cgi?id=27910.
[12] Xamarin Bug 27922. XmlConvert.ToUnit throwing
wrong/inconsistent exception. https:// bugzilla.xamarin.com/
show bug.cgi?id=27922.
[13] Xamarin Bug 27982. Inconsistent behavior in
DynamicAttribute.Equals . https:// bugzilla.xamarin.com/
show bug.cgi?id=27982.
[14] Xamarin Bug 28017. NameTable.Add throwing
wrong/inconsistent exception. https:// bugzilla.xamarin.com/
show bug.cgi?id=28017.
[15] Xamarin Bug 28123. Inconsistent behavior in
System .Xml.XmlReaderSettings .MaxCharactersInDocument .
https:// bugzilla.xamarin.com/ show bug.cgi?id=28123.
[16] Xamarin Bug 28134. System .Text .EncoderFallbackException
inconsistent initial state compared to .NET (two inconsistencies
assigned to this bug identiﬁer). https:// bugzilla.xamarin.com/
show bug.cgi?id=28134.
[17] Xamarin Bug 28562. Incorrect System .Numerics .Complex .Pow
result. https:// bugzilla.xamarin.com/ show bug.cgi?id=28562.
[18] Xamarin Bug 28571. Incorrect behavior in
System .Numerics .BigInteger . https:// bugzilla.xamarin.com/
show bug.cgi?id=28571.
[19] Xamarin Bug 28572. Incorrect/inconsistent behavior in
System .Numerics .Complex .Divide . https:// bugzilla.xamarin.com/
show bug.cgi?id=28572.
[20] S. Anand, M. Naik, H. Yang, and M. Harrold. Automated
concolic testing of smartphone apps. In ACM SIGSOFT Interna-
tional Symposium on the F oundations of Software Engineering
(FSE), 2012.
[21] T. Azim and I. Neamtiu. Targeted and depth-ﬁrst exploration
for systematic testing of Android apps. In ACM Symposium on
Object-Oriented Programming, Systems, Languages and Appli-cations (OOPSLA), 2013.
[22] P. Brooks and A. Memon. Automated GUI testing guided
by usage proﬁles. In IEEE/ACM International Conference on
Automated Software Engineering (ASE), 2007.
[23] C. Brubaker, S. Jana, B. Ray, S. Khurshid, and V . Shmatikov.
Using Frankencerts for automated adversarial testing of certiﬁ-cate validation in SSL/TLS implementations. In IEEE Sympo-
sium on Security and Privacy (Oakland), 2014.
[24] D. Brumley, J. Caballero, Z. Liang, J. Newsome, and D. Song.
Towards automatic discovery of deviations in binary imple-mentations with applications to error detection and ﬁngerprintgeneration. In USENIX Security Symposium, 2007.[25] S. Choudhary, M. Prasad, and A. Orso. X-PERT: Accurate
identiﬁcation of cross-browser issues in Web applications. In In-
ternational Conference on Software Engineering (ICSE), 2013.
[26] S. R. Choudhary, M. R. Prasad, and A. Orso. Cross-platform
feature matching for Web applications. In International Sympo-
sium on Software T esting and Analysis (ISSTA), 2014.
[27] C. Csallner and Y . Smaragdakis. JCrasher: An automatic
robustness tester for Java. Software–Practice and Experience,
34(11), 2004.
[28] J. E. Forrester and B. P. Miller. An empirical study of the
robustness of Windows NT applications using random testing.InUSENIX Windows Systems Symposium, 2000.
[29] A. Gokhale, V . Ganapathy, and Y . Padmanaban. Inferring
likely mappings between APIs. In International Conference on
Software Engineering (ICSE), 2013.
[30] S. Guthrie, T. Myerson, and S. Nadella. Day one keynote
presentation, Microsoft Build developer conference, April 2015.http:// channel9.msdn.com/ Events/ Build/ 2015/ KEY01.
[31] H. Heikotter, T. Majchrzak, and H. Kuchen. Cross-platform
model-driven development of mobile applications with MD
2.I n
ACM Symposium on Applied Computing (SAC), 2013.
[32] A. Machiry, R. Tahiliani, and M. Naik. Dynodroid: An input
generation system for Android apps. In ACM SIGSOFT Interna-
tional Symposium on the F oundations of Software Engineering(FSE), 2013.
[33] W. M. McKeeman. Diﬀerential testing for software. Digital
T echnical Journal, 10(1), December 1998.
[34] B. P. Miller, L. Fredriksen, and B. So. An empirical study of
the reliability of UNIX utilities. Communications of the ACM
(CACM), 33(12), December 1990.
[35] B. Nguyen, B. Robbins, I. Banerjee, and A. Memon. GUITAR:
An innovative tool for automated testing of GUI-driven soft-ware. Journal of Automated Software Engineering, 21(1), 2014.
[36] C. Pacheco, S. K. Lahiri, M. Ernst, and T. Ball. Feedback-
directed random test generation. In International Conference
on Software Engineering (ICSE), 2007.
[37] M. Pradel, P. Schuh, G. Necula, and K. Sen. Event-
break: Analyzing the responsiveness of user interfaces throughperformance-guided test generation. In ACM Symposium on
Object-Oriented Programming, Systems, Languages and Appli-cations (OOPSLA), 2014.
[38] M. P. Robillard, E. Bodden, D. Kawrykow, M. Mezini, and
T. Ratchford. Automated API property inference techniques.IEEE Transactions on Software Engineering (TSE), 39(5), May2013.
[39] V . Srivastava, M. D. Bond, K. S. McKinley, and V . Shmatikov. A
security policy oracle: Detecting security holes using multipleAPI implementation. In ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation (PLDI), 2011.
[40] C. Velazco. Microsoft invites Android and iOS apps to join
Windows 10, April 2015. http:// www.engadget.com/ 2015/ 04/
29/android-ios-apps-on-windows-10.
[41] X. Yang, Y . Chen, E. Eide, and J. Regehr. Finding and under-
standing bugs in C compilers. In ACM SIGPLAN Conference
on Programming Language Design and Implementation (PLDI),2011.
[42] H. Zhong, S. Thummalapenta, and T. Xie. Exposing behavioral
diﬀerences in cross-language API mapping relations. In Inter-
national Conference on Fundamental Approaches to SoftwareEngineering (F ASE), 2013.
[43] H. Zhong, S. Thummalapenta, T. Xie, L. Zhang, and Q. Wang.
Mining API mapping for language migration. In International
Conference on Software Engineering (ICSE), 2010.
451