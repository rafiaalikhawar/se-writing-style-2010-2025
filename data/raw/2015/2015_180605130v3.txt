Detecting Speech Act Types in Developer /Q_uestion/Answer
Conversations During Bug Repair
Andrew Wood
University of Notre Dame
Notre Dame, Indiana 46556
awood7@nd.eduPaige Rodeghero
Clemson University
Clemson, South Carolina 29634
prodegh@clemson.edu
Ameer Armaly
Google
Mountain View, California 94043
aarmaly@nd.eduCollin McMillan
University of Notre Dame
Notre Dame, Indiana 46556
cmc@nd.edu
ABSTRACT
/T_his paper targets the problem of speech act detection in conversa-
tions about bug repair. We conduct a “Wizard of Oz” experiment
with 30 professional programmers, in which the programmers /f_ix
bugs for two hours, and use a simulated virtual assistant for help.
/T_hen, we use an open coding manual annotation procedure to iden-
tify the speech act types in the conversations. Finally, we train and
evaluate a supervised learning algorithm to automatically detect
the speech act types in the conversations. In 30 two-hour conver-
sations, we made 2459 annotations and uncovered 26 speech act
types. Our automated detection achieved 69% precision and 50%
recall. /T_he key application of this work is to advance the state of
the art for virtual assistants in so/f_tware engineering. Virtual assis-
tant technology is growing rapidly, though applications in so/f_tware
engineering are behind those in other areas, largely due to a lack
of relevant data and experiments. /T_his paper targets this problem
in the area of developer Q/A conversations about bug repair.
CCS CONCEPTS
•So/f_tware and its engineering !Maintaining so/f_tware;
KEYWORDS
speech acts, virtual assistant, bug repair, classi/f_ication
ACM Reference format:
Andrew Wood, Paige Rodeghero, Ameer Armaly, and Collin McMillan. 2018.
Detecting Speech Act Types in Developer /Q_uestion/Answer Conversations
During Bug Repair. In Proceedings of 12th Joint Meeting of the European
So/f_tware Engineering Conference and the ACM SIGSOFT Symposium on the
Foundations of So/f_tware Engineering, Lake Buena Vista, Florida, USA, 4–9
Nov., 2018 (ESEC/FSE 2018), 12 pages.
DOI: 10.475/123 4
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro/f_it or commercial advantage and that copies bear this notice and the full citation
on the /f_irst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ESEC/FSE 2018, Lake Buena Vista, Florida, USA
©2018 Copyright held by the owner/author(s). 123-4567-24-567/08/06. . . $15.00
DOI: 10.475/123 41 INTRODUCTION
“Speech Acts” are spoken or wri/t_ten actions meant to accomplish
a task [ 8,77,85]. A classic example of a speech act is ‘I now pro-
nounce you husband and wife’ – the speech itself is an action with
consequences [ 78]. Naturally, most speech acts in life are less im-
pactful (‘let’s go to the movies’ or ‘please tell me how to /f_ind my
classroom’), though the principle is the same. Speech acts are key
components of conversations that guide the what the speakers do.
While research in sociology has studied speech acts for decades [ 8,
77], there has been an increase in interest due to the growth of vir-
tual assistants. Virtual assistants such as Cortana [ 51], Google
Now [ 32], Siri [ 5], etc., try to carry on a conversation with a hu-
man, to try to serve that person’s request – asking for a restaurant
recommendation, or the time of day. And while human conversa-
tion can seem eﬀortless at times, in fact there are several key steps
that we do without even being aware [ 26,38,56,86]: we detect
when speech acts occur, we comprehend the speech act as being
a particular type of act (e.g., an information request, a command,
a clari/f_ication), and cra/f_t an appropriate response. We understand
naturally that the type of act will depend on the context of the
conversation, and that a piece of dialog may be of more than one
type. Virtual assistants must be carefully designed to mimic this
process: the /f_irst step is to detect speech acts and classify them by
type.
Designing a virtual assistant to detect and classify speech acts
requires examples of conversations from which to learn what those
speech acts are. /T_hese conversations must be related to the task
for which the assistant is being designed. For example, a study by
Whi/t_taker et. al [90] targets dialog systems for restaurant recom-
mendations, and therefore collects 24 examples of conversations
in which a human asks for restaurant recommendations. Kerly et.
al[40] targets automated tutoring systems, and to do so collects 30
examples of tutoring sessions for a speci/f_ic subject area. /T_he data
collected for one application domain is generally not applicable to
other domains.
One key, accepted strategy for collecting examples of conversa-
tions is a user simulation in a “Wizard of Oz” experiment [ 22,67].
In a Wizard of Oz experiment, human participants interact with a
machine that the participants believe to be automated. In reality,
the machine is controlled by human experimenters. /T_he partici-
pants are asked to use the machine for a particular purpose (e.g.,arXiv:1806.05130v3  [cs.SE]  3 Jul 2018ESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA Andrew Wood, Paige Rodeghero, Ameer Armaly, and Collin McMillan
they ask for a restaurant recommendation). /T_he idea is that the ex-
perimenters can collect simulated conversations that closely re/f_lect
real-world use. Analysis of the conversations reveals what speech
acts the participants make, and clues as to how to detect them.
Today, virtual assistants are possible due to major eﬀorts in un-
derstanding human conversation, though these eﬀorts have largely
been con/f_ined to everyday tasks. While virtual assistants for so/f_t-
ware engineering have been envisioned for decades [ 8,77], progress
is limited, largely due to three problems that we target in this paper:
1) there are very few experiments with data released of so/f_tware
engineering conversations, 2) the speech act types that so/f_tware
engineers make are not described in the relevant literature, and 3)
there are no algorithms to automatically detect speech acts.
In this paper, we conduct a Wizard of Oz experiment in the
context of bug repair. We then manually annotate the data from
this experiment to /f_ind the speech act types and build and evaluate a
detector for these speech acts in conversations. Our target problem
domain is a virtual assistant to help programmers during bug repair.
We chose bug repair because it is a common so/f_tware engineering
task, and because, as previous studies have shown, bug repair is
a situation in which programmers are likely to ask questions [ 43,
82]. We recruited 30 professional programmers to /f_ix bugs for
two hours each, while providing an interface to a Wizard of Oz
simulated virtual assistant. /T_he programmers interacted with the
simulated virtual assistant for help on the debugging task. We then
manually annotated each conversation with speech act types in
an open coding procedure (see Section 5). Finally, we trained a
learning algorithm to detect speech acts in the user’s side of the
conversations, and evaluated its performance (Sections 7 - 9).
Across 30 two-hour conversations, we made 2459 annotations
and discovered 26 speech act types. Our automated speech act
detection algorithm achieved an average of 69% precision and 50%
recall. By releasing this corpus, we contribute one of very few
WoZ corpora, which are especially rare in the domain of So/f_tware
Engineering [ 79]. We release all data, including conversations,
annotations, and our detection algorithm source code via an online
appendix (Section 11), to promote reproducibility and assist future
research in so/f_tware engineering virtual agents.
2 PROBLEM, SIGNIFICANCE, SCOPE
/T_he problem we target in this paper is that models of developer
conversations are not described in the literature. Certainly, strong
eﬀorts in the area of program comprehension have made inroads
into our understanding of the types of information that program-
mers need and how programmers make sense of so/f_tware problems.
However, the “nuts and bolts” of actual conversations among pro-
grammers are still not well-understood.
A key component of those nuts and bolts are “speech acts” (as
de/f_ined in the previous section), and our goal is to automatically de-
tect these speech acts in conversations. But detection of speech acts
is useful beyond pure academic interest: advancements in program-
mer tool support depend on improved detection of programmer
intent. Numerous so/f_tware engineering tools depend on natural
language interfaces, such as code search engines, navigation tools,
traceability tools, and our target context of automated virtual as-
sistant technology. /T_he situation we envision is that a programmerasks an automated virtual assistant a question in lieu of a fellow
human programmer, and the virtual assistant is expected to provide
an answer to that question. A fundamental part of answering these
questions is to detect the types of statements, comments, etc., that
programmers make when asking and clarifying their questions.
/T_hroughout this paper, we refer to a 2011 book by Rieser and
Lemon [ 67] as both motivation for and rationale behind our work.
/T_he book provides an excellent summary of the design decisions
required for building dialog systems and re/f_lects the signi/f_icant
momentum in years of research on virtual agents – one key theme
is that using Wizard of Oz studies to inform data-driven dialog
system construction is a highly eﬀective strategy. /T_hey point out
that while it is possible to design a virtual assistant using manually-
cra/f_ted assumptions about user behavior, the existence of annotated,
simulated dialog (via a WoZ study) provides an immense boost to
the /f_lexibility and eﬀectiveness of virtual agent design. One bene/f_it
is from the increased knowledge scientists gain from studying the
dialog, while another bene/f_it is from the ability to use supervised
and reinforcement learning algorithms to “teach the computer”
correct behavior, even with relatively sparse data.
In this paper, we contribute the dataset, our manual annotation
of the dataset, and our analysis of those annotations to the commu-
nity as a foundation for building be/t_ter so/f_tware engineering virtual
agents. /T_his contribution alone is signi/f_icant, considering that a re-
cent survey by Serban et al. [79] found only four publicly-available
WoZ datasets (more are held privately) suitable for building dialog
systems – and none related to So/f_tware Engineering. However, we
take a further step towards a working virtual agent by building
a classi/f_ier to automatically label the dataset; in essence, this is a
detector for speech act type using supervised learning (as chapter
7 of [ 67] highlights, supervised learning is o/f_ten the /f_irst technique
tried for speech act type detection, prior to resorting to more com-
plex approaches).
Note that in our manual annotation process, we annotated the
entire conversation (both “Madeline’s” and the study participants’
side). However, during the speech act type detection, we only
predict the type of speech acts from the participants’ side of the
conversation. /T_his is because during the manual annotation pro-
cess, we study not only the participants, but the wizards’ actions
also: this is for the purpose of laying a groundwork for conver-
sation /f_low analysis in future work, in addition to the academic
interest presented in this paper. But, during speech act detection,
the realistic scenario is that a virtual assistant would never need
to classify its own conversation, since it would already know the
speech act types it generated itself. It would only need to detect
the speech act type of the human user.
3 BACKGROUND
/T_his section describes four key technologies related to and un-
derpinning our work in this paper: automated virtual assistants,
conversation analysis and modeling, studies of program compre-
hension, and text classi/f_ication.
3.1 Automated Virtual Assistants
Automated virtual assistants such as Siri, Cortana, and Google Now
are claiming an increasing role in computing for everyday tasks.
/T_hey simplify duties such as planning meals and /f_inding music,Detecting Speech Act Types During Developer Q/A ESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA
and are part of a broader trend towards automated productivity
services. Virtual assistants for so/f_tware engineering have been
envisioned for decades [ 13,69], with the dream being a system that
can mimic the answers that human teammates would give, such as
a system able to generate “On-Demand Developer Documentation”
as responses to source code queries [70].
We (the so/f_tware engineering research community) are still far
away from this dream. Nevertheless, advancements are being made
in that direction. Recently, Bradley et al. [14] built Devy, a virtual
agent to help automate programmer tasks. Devy diﬀers from our
work in that we seek to understand the structure of programmers’
conversations, to build a system to help programmers learn and
recall information, rather than automate tasks. Pruski et al. [61]
created TiQi, a technique that answers database query questions
in the form of unstructured dialog. Ko and Myers [ 44] created
Whyline, which answers questions about program output. Escobar-
Avila et al. [24] answered unstructured questions by connecting
questions to so/f_tware engineering video tutorials. A majority of
current eﬀorts focus on understanding unstructured so/f_tware en-
gineering data; for a more complete survey we direct readers to
Arnaoudova et al. [7]. But what holds back progress at the moment
is an incomplete understanding of how programmers communicate
– it is not possible to build a tool that participates in this communica-
tion without understanding the nature of that communication. /T_his
understanding can only be completed with conversation analysis
and modeling.
3.2 Conversation Analysis and Modeling
Conversation analysis and modeling is the task of extracting mean-
ing from human wri/t_ten or verbal communication. It usually in-
volves creating a representation of a type of conversation (e.g.,
restaurant recommendations, or technical support calls [ 67]), and
then using that representation to predict the /f_low of the conversa-
tion. A “/f_low” of a conversation is how people tend to put infor-
mation in conversations, for example one conversation participant
asking “does that make sense?” if the other participant is silent a/f_ter
receiving new information. Conversations are typically broken up
byturns [26,38,56,76,86]. A turn begins every time a speaker be-
gins speaking and can encompass multiple sentences. Conversation
analysis and modeling is what allows automated virtual assistants
to create human-like conversations.
Conversation modeling has its roots in sociology [ 26,38,56,76,
86] and psychology [ 33], where researchers studied the factors
behind conversation /f_low and form. /T_hese o/f_ten employ qualitative
analysis methods to isolate human factors such as social rank or
fatigue. A/f_ter a signi/f_icant investment in the 1990s, quantitative
analysis procedures have been developed to model and predict
the types of information that human conversations include, in
order to create interactive dialog systems. Work in this area has
/f_lourished, with representative work including: [ 23,25,34,53,54,
65,88]. For example, work by Lemon [ 48,67] models restaurant
recommendation conversations as a Markov Decision Process, in
which each turn is one of six possible states.
A typical strategy in conversation modeling for discovering
speech acts is user simulation , in which participants in a study
are told that they are interacting with a dialog system, which is ac-
tually a human acting like a dialog system via a chat program [ 2,75]./T_he simulation results in a transcript of a conversation between
a human participant and an idealized virtual assistant (simulated
by the researcher). /T_he transcript is an extremely valuable source
of information on how the human participant expects to inter-
act with a machine and how the machine should respond. While
rare in So/f_tware Engineering, these studies are not unheard of:
Goodrum et al. [31] perform a WoZ study to discover what re-
quirements knowledge programmers need, related conceptually to
requirements-gathering WoZ studies proposed earlier [89].
3.3 Studies of Program Comprehension
/T_his paper could be broadly classi/f_ied as a study in program compre-
hension – how programmers comprehend and communicate about
so/f_tware development and behavior. Typically questions asked by
program comprehension literature relate to the mental and physi-
cal processes that developers follow [ 36,46]. Examples of mental
processes include targeting how code is connected [ 45,52,81,83].
Physical processes include taking of notes [ 3] and pa/t_terns of move-
ments of the eyes [ 73,80]. Notably, Roehm et al. [74] point out that
programmers “try to avoid” program comprehension, and look for
short cuts whenever possible. /T_his /f_inding is in line with several
others that suggest that tool support for comprehension should
provide information incrementally and at as high a level as possible,
and avoid too many low-level details [ 27,49,84]. Our vision in this
paper is to build a foundation for so/f_tware engineering virtual assis-
tants, to provide information in the order and at the time requested
by programmers during a dialog.
3.4 Text Classi/f_ication
Text classi/f_ication is an intensely-studied area in machine learn-
ing, and text classi/f_ication techniques have seen extensive use in
so/f_tware engineering. A recent book by Aggarwal and Zhai [ 1] sur-
veys text classi/f_ication and mining techniques generally. So/f_tware
engineering applications are so prevalent that we cannot list them
all here, though representative examples include [ 4,41,50,71]. We
use text classi/f_ication as a component of our speech act detection.
4 USER SIMULATIONS
In this section, we describe our user simulation study. In general, a
user simulation is an imitation of a conversation between a human
and a machine – instead of a real machine, a researcher stands in for
the machine without the human being aware of it [ 22]. In this paper,
our user simulation is the interaction between our participants and
an imitated so/f_tware program. Participants believed the program
could automatically assist programmers with tasks. /T_hey were
informed their participation in this study was helping to improve a
virtual assistant program for programmers. However, there was no
actual virtual assistant producing answers to the questions asked
by the participants. We manually answered every question.
4.1 Methodology
We based our methodology on previous studies of bug repair in so/f_t-
ware engineering [ 30,39,42] and previous “Wizard of Oz” studies in
sociology [ 22]. We asked the programmers to remotely participate
in the study using a provided Ubuntu 64-bit virtual machine and the
Microso/f_t Skype application on their local machine. We instructed
the participants to /f_ix bugs from pre-installed open source JavaESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA Andrew Wood, Paige Rodeghero, Ameer Armaly, and Collin McMillan
projects contained within an Eclipse IDE [ 29] workspace on the
provided virtual machine. We instructed the participants to /f_ix as
many bugs as they could within a pre-de/f_ined two-hour time frame.
During that time, we gave the participants one bug at a time, one
bug per project. We asked the participants to avoid using the Inter-
net to search for solutions or answers to any questions that they
might have, and to instead direct their questions to a automated
virtual assistant named “Madeline” through the Skype platform.
Note that this involved two key design decisions informed by Rieser
and Lemon’s guide on WoZ studies for dialog systems (chapter 6
of [67]): First, we used the wri/t_ten text chat only, no voice, to limit
the scope of the study to developer Q/A conversations instead of
introducing the possibility of voice transcription errors (it is nec-
essary to deliberately add noise to WoZ studies involving voice to
simulate actual noise, and we felt this would add too many variables
considering the already complicated nature of debugging). Second,
we restricted access to internet resources. While this may seem to
create an unrealistic situation (since programmers frequently use
Stackover/f_low, etc.), it was necessary in order to learn how pro-
grammers might use a virtual agent, due to a bias in which people
might not try a new technology simply because it is unfamiliar, and
to avoid biases introduced by external tools. /T_hese restrictions are
o/f_ten a “necessary evil” in WoZ experiments – for example, 94%
of papers surveyed by Riek [ 66] placed substantive restrictions on
participant behavior and resources.
During each study, two to three of the authors collaborated at all
times to respond to the participants. At least one of the authors had
previously /f_ixed the bugs given to the participants. /T_his allowed
for quick and intelligent responses to the participants, giving the
illusion that Madeline produced responses automatically. /T_his de-
ception, typical of the “Wizard of Oz” strategy [ 21] was necessary
to ensure the authenticity of the responses. /T_he participants were
explicitly told that they were communicating with an automated
system supervised by humans (Madeline). /T_he participants were
told to interact with Madeline through Skype conversations, and
also to share their screens for quality assurance purposes. In reality,
screen sharing provided the means to prepare responses in real time
and was critical for imitating a fully autonomous system. Following
Rieser and Lemon’s WoZ process for dialog systems (again, chapter
6 of [ 67]), we did not restrict wizards to a script or set of prede/f_ined
speech act types, since a goal of our study was to understand what
the programmers needed rather than test a prede/f_ined script.
4.2 Participants
We recruited 30 professional programmers to participate in our
study. /T_hese programmers were recruited through email and an
online freelance website called Upwork [ 87]. /T_he programmers
work at various companies such as IBM, GolfNow, and Hyland
So/f_tware, while some work as freelancers full time. Note that the
programmers recruited are not students, but professionals working
in industry. Each programmer recruited had familiarity with Java
before participating in the study. Overall, the participants had an
average of 5.5 years of experience with Java. /T_he maximum number
of years of Java experience was 12 and the minimum was one.4.3 /T_hreats to Validity
As with most studies, this project has a few threats to validity.
First, since each experiment was two hours long (not including any
technical problems), it is possible that the participants experienced
fatigue. /T_his is compounded with any fatigue that they already
experienced from their normal work schedule. /T_his was mitigated
by using a large pool of participants. Another threat came from
technical problems with screen sharing. /T_he only issue with this,
however, was a possible reduction in response speed, but we saw
no noticeable reductions in any of the studies. Either through
technical problems or participants forge/t_ting to save them, a few
screen shares were unable to be stored. However, these stored
recording were not actually used in analysis. Finally, another threat
to validity was our lack of control over whether participants actually
refrained from searching for answers over the Internet rather than
asking our simulated virtual assistant. Participants could have used
another device to search the web. We did not notice any extended
lapses in questions or work time from any participants, though, so
we believe most participants followed our searching instructions
correctly.
Project Name: 2048
Bug Report: /T_he game board becomes unresponsive.
public GamePane(int size, BasePane basePane)
{
this.size = size;
this.basePane = basePane;
setScore(0);
this.tileSize = tileSizes[size];
this.moveTime = 100 * 4 / size;
setPrefSize(size * tileSize, size * tileSize);
setLayoutX(175 - (size * tileSize) / 2);
setLayoutY(175 - (size * tileSize) / 2);
setStyle(/quotedbl.Var-fx-background-color: #FFFFFF;/quotedbl.Var);
addTile();
... [Irrelevant code cut for paper space limitations]
Thread focusField = new Thread(new Runnable()
{
@Override
public void run()
{
while(!Thread.currentThread().isInterrupted()) {
if(!isFocused()) {
try { Thread.sleep(100); }
catch (InterruptedException e) {
e.printStackTrace();
}
requestFocus();
}}}});
focusField.setDaemon(true);
focusField.start();
}
Figure 1: A description of a bug in the “2048” project with source
code. Participants received full copies of the source code, however
parts have been omitted for space limitations in this /f_igure.Detecting Speech Act Types During Developer Q/A ESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA
4.4 Data Collection
We provided each participant with an Ubuntu 64-bit virtual ma-
chine. We asked the participants to download the virtual machine
ahead of the study. Inside the virtual machine, we provided Eclipse
with all the buggy projects. We also provided a screen recording
program called SimpleScreenRecorder [ 9]. We asked each partic-
ipant to start the screen recording at the beginning of the study
and leave the so/f_tware running until the study had completed. /T_he
participants then saved and sent the screen recording /f_ile to us.
We then collected the Skype transcript created by the study and
removed all identifying information from the conversations. Some
participants also sent back the project /f_iles of the /f_ixed bugs, but
these /f_iles were not used in our analysis.
4.5 Bugs
/T_he 20 Java bugs come from 17 diﬀerent open source projects. /T_he
project domains include a Pacman game, a calender application,
and a PDF /f_ile merger. We also selected bugs from commonly used
Java libraries such as OpenCSV [ 57] and Apache Commons IO [ 28].
We chose the bugs based on four criteria:
(1)/T_he bugs had to be simple enough that they could be solved
in a few hours, but complicated enough to take at least 20
minutes to solve.
(2)We had to be able to understand the bugs well enough to
give meaningful answers to questions during the simula-
tion.
(3) /T_he user had to be able to reproduce the bug easily.
(4)/T_he bugs had to be solvable without obscure domain knowl-
edge.
All of the bugs were previously solved, and we had the actual
solutions on hand throughout each study. However, we also dis-
cussed other solutions to the bugs before the user simulations. /T_his
is because some of the bugs could be /f_ixed in a variety of ways. /T_he
bugs were presented individually and randomized for each study.
An example of a bug given to the participants is as follows:
/T_he bug in the source code above occurs when a user tries to
make a move with the arrow keys. /T_he source of the bug is the
result of an incorrect fusion of a third party library used for graph-
ics (JavaFX) and the structural design of the project. /T_he project
contains multiple panes which house bu/t_tons performing diﬀerent
types of actions. For the sake of simplicity, consider there to be
only two panes; one that displays the board and is controlled by the
arrow keys (the “game pane”), and another that allows users to save
and load games (the “/f_ile pane”). Both of these panes are vying for
focus, but for the game to be played, the “game pane” must always
have focus. To ensure this, the project’s implementation spawns a
deamon thread that almost constantly requests focus for the “game
pane.” /T_he bug comes from the fact that JavaFX only allows for
one thread, called the “event thread,” to make changes to the UI.
When creating the deamon thread, the developer uses the “/T_hread”
type to request focus, which JavaFX interprets as modifying the UI.
/T_his causes an exception to be raised, and for the game to become
unresponsive to the arrow keys.
One solution to this bug is to use JavaFX safe data types to per-
form the action of the deamon thread. During studies, participants
were only provided with the buggy projects and the bug description.We (pretending to be Madeline), while aware of solutions, would in
no form “give” a solution to the participants, but would only react
to questions asked. Participants were incentivized to search the
source project for the /f_iles containing bugs, as questions designed to
tease solutions out of Madeline were met with vague and unhelpful
responses (i.e. “I am unsure”). A complete list of bugs can be found
at our online appendix (see Section 11).
4.6 Experiences & Lessons Learned
In this section, we discuss our experiences and lessons learned while
conducting the user simulation study. We do this to provide some
guidance to so/f_tware engineering researchers who might do studies
similar to ours in the future. One of the biggest lessons we learned
was to con/f_irm that the virtual machine we provided worked on the
participant’s machine before the study started. In roughly half of
the studies, we found ourselves /f_ixing problems on the participants’
machines and spending, on average, an extra 20 minutes /f_ixing the
issues. /T_his was problematic, as the studies took up more time than
originally anticipated, which threw oﬀ our original study schedule.
We also learned that additional information should be advertised
(beyond the scope of the study) to allow for smooth experiments,
such as experience with virtual machines or experience with the
Eclipse IDE.
Another lesson learned was how to eﬀectively schedule remote
studies. Many participants were unable to participate in the study
until they returned home from their jobs in the evening. Some had
families and wanted to participate in the study even later, once their
children were in bed. Many of our participants were in diﬀerent
time zones, there were days where we would schedule studies at 8
am, 1 pm, and 10 pm in our time zone. We learned, over time, to
hire participants overseas where their evening was our work day.
5 ANNOTATIONS
In this section, we describe our process for annotating the speech
acts from the data collected during the user simulation studies (see
Section 4.4). Essentially, our goal is to 1) determine what the speech
acts are and 2) to determine what parts of the conversations are
associated with those speech act types. We also discuss our research
questions, the rationale for asking them, and provide annotation
examples.
5.1 Research /Q_uestions
/T_he research objective of this section is to determine how program-
mers would use a virtual assistant to assist them in /f_ixing a source
code bug. We seek to see what types of questions programmers
would ask a virtual assistant and if those types of questions are
consistent across multiple programmers.
RQ1Do diﬀerent programmers ask the virtual assistant simi-
lar questions for the same bugs?
RQ2What types of questions did programmers ask during
bug repair?
RQ3What type of questions did programmers most frequently
ask?
/T_he rationale behind RQ1is that if programmers ask for help, and if
they ask similar questions for the same bug, it is possible to create aESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA Andrew Wood, Paige Rodeghero, Ameer Armaly, and Collin McMillan
Figure 2: /T_he annotation labels of all 30 transcripts and the occurrences for each label. Each turn can have multiple labels (speech act type).
speech acts classi/f_ication system given training data. We group the
questions to create labels for the training data in RQ2. Finally, we
investigate the most common uses of a potential virtual assistant
inRQ3to advise future virtual assistant implementations.
5.2 Methodology
We used a manual open coding qualitative analysis process [ 11]
adapted from the social sciences to create labels for the conver-
sations we collected. (/T_hough for the purposes of this paper, we
follow Rastkar et al. [63] in referring to “coding” as “annotating”
to prevent conceptual con/f_licts between sociological coding and
computer coding.) /Q_ualitative annotation is becoming more com-
mon in so/f_tware engineering literature [ 20,35,55,62,72], and it is
important to recognize that while standards and principles exist,
the nature of qualitative analysis is that each annotation procedure
is slightly diﬀerent based on the needs and circumstances of the
study. In this paper, we followed the recommendations of Rieser
and Lemon in a 2011 book on creating dialog systems from WoZ
data [67], with one exception noted below.
A metaphor for open coding is unsupervised learning, in that
the human annotators do not begin with a set of labels: our goal
is to discover those labels from the data, and then assign them to
the turns in our data. Practically speaking, we did this in three
rounds. /T_he /f_irst round of annotation consisted of “label creation”
where we created labels as we saw /f_it and did not have a pre-
determined list to choose from. /T_he second round consisted of “label
pruning” where we decided what labels could be safely removed
or merged. /T_he second round became necessary the more progress
was made in the /f_irst round, and was due to the complexity of
compressing sometimes vague and complex English text down
into its major concepts. /T_he result of label pruning was a set of
well de/f_ined and disjoint descriptions of English text describing
our examples. /T_he third and /f_inal stage of annotating involved re-
examining the annotations but instead searching for spelling errors
or other small mistakes. /T_his round had the eﬀect of ensuring
labels were consistent and resolving labels that represented the
same concept but used diﬀerent terminology (i.e. synonyms), or
were spelled incorrectly.
During any annotation process, and especially an open process
in which we do not begin with labels, the bias of the human an-
notator becomes a major concern. /T_he degree of bias is known asthe “reliability” of the data, and it is an extremely controversial
research topic. One possibility is to follow the lead of Carle/t_ta [ 15]
in calculating Kappa agreement from multiple annotators, and only
accepting agreement above a certain threshold; if agreement cannot
be achieved, the argument goes, then more annotators are necessary.
While this is a common procedure, it is by no means universally
accepted. As Craggs and McGee Wood point out, “one must decide
for oneself, based on the intended use of [an annotation] scheme,
whether the observed level of agreement is suﬃcient” [ 18]. Like-
wise, they “suggest that if a coding scheme is to be used to generate
data from which a system will learn to perform similar coding, then
we should be ‘unwilling to rely on imperfect data’.”
At the same time, it is not an option to merely add more and
more annotators until agreement is achieved. /T_here has long been
a recognized split between expert and naive annotators [ 15,58]. It
is not proper to allow naive annotators to have majority rule over
the experts. To be an expert annotator in our study, a person would
need to have 1) knowledge of the bugs solved in our study so they
can understand the conversations, and 2) not been a participant in
the study. Only the /f_irst and second authors were both quali/f_ied
and available (manual annotation is weeks of eﬀort).
Rieser and Lemon faced a similar situation, and solved it by
having a discussion between two annotators for all disagreements,
followed by independent decision-making and calculation of Kappa
(page 110 of [ 67]). We diﬀer from this procedure in that we con-
sider our situation to be more “unwilling to rely on imperfect data”
due to the fact that our research questions in Section 5.1 and our
prediction training in Section 7 could be aﬀected by errors. /T_here-
fore, for this paper, we had two experts annotate all data and solve
every disagreement through discussion as disagreements occurred,
followed by mutual decision-making, resulting in one set of anno-
tations. While this mutual process makes it impossible to calculate
a reliability metric, we felt it was more important to maximize
correctness of the annotations.
6 ANNOTATIONS RESULTS
In this section, we present the results from our annotation process.
We also provide annotation examples following the results. We note
that the programmers asked on average 12.8 questions throughout
the two hour user simulation study. A select few did not ask more
than three, however, these participants were outliers. /T_he highestDetecting Speech Act Types During Developer Q/A ESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA
number of questions asked during a user simulation study was 54
and the lowest number of questions asked during a study was 3.
6.1 RQ1: Programmers asking similar questions
We found that programmers asked similar questions to one an-
other. Of all the questions asked by the programmers, the ones
that were consistent across the majority of participants included
con/f_irmation/Q_uestions, clari/f_ication/Q_uestions, and api/Q_uestions.
Of these three types of questions, clari/f_ication/Q_uestion was asked
the most by all programmers. It was asked a total of 204 times,
which comprised 53.1% of all questions asked by programmers.
/T_here were various types of clari/f_ication questions asked. Some of
the clari/f_ication questions included questions about what the bug
report said, what questions Madeline could and could not answer,
and clarifying answers from Madeline. /T_he participants also asked
clari/f_ication questions to con/f_irm their understanding of the task
that they were to complete for the study.
6.2 RQ2: Types of questions being asked
We found that programmers asked a variety of questions that ranged
from system type questions to API and documentation types. An
example of an API question is:
/quotedbl.VarWhat methods are in eventyhandler(?)/quotedbl.Var
We also found many programmers asked implementation questions:
/quotedbl.VarWhat are valid values for the int direction
in PacPlayer.java?/quotedbl.Var
A/f_ter /f_inishing the annotation process, we were able to narrow
down the question annotation types into 10 categories. /T_he cat-
egories are: syntax, parameter, documentation, API, clari/f_ication,
implementation, bug report, con/f_irmation, clari/f_ication, and system
questions. Figure 2 lists the number of occurrences for each of
the speech act types. In Section 6.4 we go into detail with a short
example of an annotated conversation. We also provide all of the
annotations on our online appendix (see Section 11).
6.3 RQ3: Most frequent questions being asked
We found programmers asked a few questions signi/f_icantly more
than others. In Figure 2, the speech act type “statement” has the
most occurrences. We would like to point out that there was an-
other, more popular type of question; the “setup” speech act. Since
this speech act type is not relevant to the study itself, this speech
act type was removed from our corpus. “clari/f_ication/Q_uestion”
has the highest occurrence out of any question type. /T_his label
appeared 204 times throughout all 30 transcripts. Many of the
participants asked clari/f_ication questions on the bugs and on the
responses Madeline gave. Madeline asked clari/f_ication questions
as well when we needed more information from a participant to
answer a question. Sometimes the participants would ask questions
that needed more detail so that Madeline could answer the question.
/T_he second highest occurrence annotation label for a question type
was “APIquestion.” /T_his label occurred 94 times in the transcripts.
/T_his makes sense as programmers were not allowed to use the
internet during the bug repair task and were unfamiliar with the
given source code.6.4 Annotation Examples
We annotated over two thousand speech acts during the annotation
process. To further explain the previous sections, we provide an ex-
ample of one of the annotations. /T_hroughout the data, participants
asked API questions, documentation, and implementation questions.
Below is a section of a developer conversation. /T_his section of the
conversation includes implementation questions and clari/f_ication
questions. At the end of each speech act, there is the annotation la-
bel for that speech act. /T_he annotation is in bold text and is in brack-
ets. /T_he speech acts begin with “P” or “M” denoting the speaker
as a “participant” or “Madeline - Virtual Assistant” respectively.
P: So the bug is that the PacPlayer does not face right
when the key is released, but it is supposed to?
[clari/f_ication/Q_uestion]
M: Yes. He also disappears. [clari/f_icationAnswer]
P: Does he disappear because the alive bool is set
to false at the wrong time [implementation/Q_uestion]
M: I am unsure [unsureAnswer]
/T_hroughout the annotation process, we found similar results
to the previous example. However, we found programmers asked
varying amounts of questions throughout the bug repair task. /T_his
was evident once deep into the annotation process. It appeared that
the more senior a participant was, the less the participant asked for
help from the virtual assistant. /T_here are three interpretations we
derive from these observations. First, the programmers possibly
did not want to ask for help and instead wanted to solve the bug
without help. Second, it is possible that the programmers did not
feel comfortable asking questions. Finally, the programmers may
have assumed that there was no automated virtual assistant and,
therefore, did not ask questions.
We found that programmers o/f_ten made a statement before ask-
ing a question. It appeared the participants were explaining their
thought process before asking a question. /T_his occurred about 20%
of the time in the user simulation studies. An example of this is:
participant: /f_irst I tried “sudo apt-get install default-jre”
participant: it told me it depends on default-jre-
headless and openjdk-7-jre
participant: is it possible to set a command line
argument for start up of the program?
Here, the participant makes multiple statements before asking
Madeline a question. We did not ask participants to “think aloud”
during this study. However, we observed this phenomenon through-
out the user simulations and annotation process.
7 PREDICTING SPEECH ACT TYPE
Our approach for predicting the speech act type is, essentially, a
text classi/f_ier based on Logistic Regression. Recall the use case that
we envision in Section 2: a virtual assistant receives a message, and
needs to classify that message into one of several categories, so thatESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA Andrew Wood, Paige Rodeghero, Ameer Armaly, and Collin McMillan
it can respond appropriately. Our idea is to train a prediction model,
then use that prediction model to classify incoming messages.
7.1 Labeled Training Data
Supervised machine learning algorithms depend on labeled train-
ing data. We use the labels from Section 6.2. In that section, we
manually annotated every turn in every conversation as belonging
to one of the speech act types we identi/f_ied. In this section, we
use that data (however, only turns from the participants’ side of
the conversation, not “Madeline’s”, to match the use case of the
virtual agent classifying incoming messages) to train a classi/f_ier
to annotate the turns automatically. Note that this is a multi-label
classi/f_ication problem, because an “example” consists of a turn an-
notated with a list of all the speech act types to which that turn
belongs. Each speech act turn type is a label, so each turn may
belong to many labels.
7.2 Attributes
We use two types of a/t_tributes. First, we treat the problem as text
classi/f_ication, where each word is an a/t_tribute. We calculate the
a/t_tributes as a binary “bag of words”. For each example, the set of at-
tributes includes either a one or zero for each word, depending on if
that word occurs in the text of the turn or not. Recent industry-track
papers [ 6,19] came to the conclusion that to maximize potential
industrial impact, researchers should prioritize simplicity, and only
move to more complex approaches when absolutely necessary. We
stuck to binary bag of words for this reason. We also did not do stop
word removal or stemming. We defer word count normalization
(e.g. TF/IDF), NLP-based solutions, advanced preprocessing tech-
niques, etc., to our future work. As we will explain in Section 9.1,
the simple approach already achieves reasonable performance.
Second, we used three shallow features identi/f_ied by related lit-
erature [ 55,63,71]. /T_his related literature actually identi/f_ies over
twenty shallow features that complement or replace text classi/f_i-
cation, but many of these are not applicable in our context. For
example, many rely on computing entropy over a whole conver-
sation a/f_ter the fact. /T_hat is not possible in our context because
we can only know incoming message and previous messages, not
future messages. /T_he three features we used are: slen , the number
of words in the message normalized over all previous messages,
wc, the number of words not normalized, and ppau , the number of
seconds between the message and the previous message.
7.3 SMOTE
We use SMOTE [ 17] to overcome the problem of unbalanced data.
Some of the user speech acts we identi/f_ied only have a few examples
(e.g. we only found eight examples for the parameterQuestion
type). /T_hat presents a problem because the learning process will
inevitably classify no turns in that type, and still seem to achieve
very high accuracy. SMOTE works by synthesizing examples in
small classes from the known examples in those classes. /T_he result
is that the small classes are /f_illed with synthesized examples until
the data are balanced. SMOTE has been widely used to resolve
situations of unbalanced data generally as well as conversational
analysis [ 71]. In pilot studies, we compared SMOTE to duplicative
oversampling and observed slight performance improvements usingSMOTE. We used SMOTE only on the training data, to avoid biasing
the testing set.
7.4 Prediction Models
We trained a multi-label prediction model using the binary rele-
vance [64] procedure. /T_he procedure is to create one binary clas-
si/f_ier for every class. We used the Logistic Regression (LR) algo-
rithm [ 37] to create each classi/f_ier. We also tested Naive Bayes
and Support Vector Machines in pilot studies – LR had superior
performance to Naive Bayes, and the diﬀerence between LR and
SVM was so slight as to not be worth the much longer training
time for SVM (eight hours versus four minutes). Note that while
we built a multi-label prediction model, we calculated SMOTE us-
ing a multi-class structure. /T_hat is, we ran SMOTE once for each
category, then trained each classi/f_ier, then combined the classi/f_iers
with the binary relevance procedure. In theory it is possible to
run SMOTE in a multi-label con/f_iguration, by executing SMOTE
on every combination of labels. However, this would necessitate
nnruns of SMOTE (for ncategories), which would be far more
expensive.
We also performed parameter tuning for Logistic Regression
across twelve parameters and Naive Bayes across four parameters.
Parameter tuning has been recommended generally when working
with SE data [ 12]. Due to space requirements, we direct readers
to our online appendix and reproducibility package for complete
details (see Section 11).
7.5 Implementation Details
We used the toolkit scikit-learn [59,60] to implement our clas-
si/f_iers and SMOTE ( imblearn.over sampling.SMOTE ) [47].
We implemented the shallow a/t_tribute calculators ourselves, us-
ing related work as a guide [ 71]. /T_he hardware was an HP Z640
workstation with an E1630v3 CPU and 64GB of memory. For total
clarity, we make all implementation scripts and datasets available
via our online appendix (see Section 11).
8 EVALUATION OF PREDICTIONS
/T_his section describes our evaluation of the prediction models we
create. Essentially, we use a 5-fold cross validation procedure to
test the quality of the predictions, as well as explore where the
predictions are most accurate.
8.1 Research /Q_uestions
Our research objective is to determine what level of performance
we can expect from the prediction models, as well as to understand
which speech acts are “easiest” to detect.
RQ4What is the performance of our prediction models, over-
all in the multi-label con/f_iguration, according to the metrics
described in Section 8.3?
RQ5For which speech acts do the prediction models have the
highest performance?
RQ6Which a/t_tributes are the most informative?
/T_he rationale behind RQ 4lies in the application we intend in
Section 2: the performance of a virtual assistant will be limited by
its ability to detect what type of speech act to which an incoming
message belongs. While we do not expect perfect performance, weDetecting Speech Act Types During Developer Q/A ESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA
need to at least have an understanding of how much inaccuracy
may stem from the detection process. /T_he rationale behind RQ 5
is similar. Some speech acts are bound to be easier to detect than
others. It is helpful to know which speech acts about which we may
be con/f_ident, or others where we are less sure. In practice, it may be
necessary to return a message to the user indicating that the virtual
assistant is unsure what the user intends, and ask the user to clarify.
RQ6is useful because the presence of some a/t_tributes may indicate
high con/f_idence, while others may indicate low con/f_idence.
8.2 Methodology
In general, we follow a 5-fold cross validation study design. In a
standard n-fold design for evaluating classi/f_iers, 1 nexamples are
set aside as a testing set, while the remaining ¹n 1ºnexamples
are used for training. /T_he evaluation is conducted ntimes, once
for each nthselection of the examples as a testing set. /T_hen, the
evaluation metrics are calculated for each “fold” and averaged. We
chose 5 as a value for nbecause it ensured that our testing set would
not be too small (as it might have been with a 10-fold design), while
still maintaining multiple folds that could be averaged.
/T_he selection of a testing set is a non-trivial exercise in a multi-
label dataset, in contrast to a single-label one. In a single-label
dataset, it is usually suﬃcient to randomly selected 1 nof the ex-
amples for the testing set. But in our multi-label dataset, we need to
ensure that the testing set represents the same distribution of labels
as the overall dataset. With only /f_ive folds, it is conceivable that a
random selection would give too much weight to one label, and this
overweighted selection would not be “averaged out” over a large
number of folds. /T_herefore, we sampled each label in proportion
to the number of examples in that label, and con/f_irmed that the
distribution of the labels over the testing set was as close as possible
to the distribution of labels over the entire dataset.
A/f_ter we separated the testing and training data, we ran SMOTE
on the training data only. If we had executed SMOTE on the en-
tire dataset, then divided the data into testing/training groups, we
would have contaminated the testing set with information from the
training set. SMOTE synthesizes examples based on the examples it
is given. If we had run SMOTE on the entire dataset, we would have
created synthesized examples based on real examples that ended
up in testing set. /T_herefore, we only ran SMOTE on the training
set. /T_his did increase the execution cost of our experiment slightly,
since we needed to execute SMOTE /f_ive times (once for each fold,
a/f_ter we separated the testing set from the training set).
Note also that this methodology is conservative – it only uses
real examples for the testing set. We use the results from this
conservative approach to answer RQ 4and RQ 5, to avoid presenting
a biased result. We also use these results to calculate other metrics
(see the next section) to answer RQ 6.
8.3 Metrics
We report the metrics precision, recall, F-measure, and support
to answer RQ 4and RQ 5/T_hese are standard metrics for evaluating
classi/f_iers and have been covered extensively elsewhere [ 10,16]; for
space we do not discuss their details here. We calculate these metrics
for each speech act type (i.e., each label) for RQ 2, and combine the
results for each speech act type to answer RQ 4. We combine theprecision and recall values for each speech act type with a weighted
average, where the weights are based on the support for each speech
act type. /T_he reason is so that the combined values re/f_lect the size
of each label. A simple average, without the weights, would be
biased by labels that only have a few examples in the testing set.
For RQ 6, we calculate F-score [ 68] for the a/t_tributes. F-score
(distinguished from F-measure, the harmonic mean of precision
and recall) is typically used for feature selection, to indicate which
features are the most informative.
8.4 /T_hreats to Validity
Like all experiments, our study carries threats to validity. /T_he
main sources of threats to validity include: the participants in the
user simulations, the bugs we asked the users to repair, and the
in/f_luences of the technology used by the participants (e.g., the IDE)
on the questions they asked. Also, it is possible that there are errors
in our manual annotation process, or in our selection of categories.
While we try to mitigate these risks by following accepted data
collection and annotation procedures, and by including a relatively
large number of participants (30) and diﬀerent bugs, the threat
remains that changes in these variables could aﬀect the performance
of our classi/f_iers. As an additional guard against these risks, we
release all data via an online appendix for community scrutiny (see
Section 11).
9 PREDICTION EVAL. RESULTS
/T_his section discusses our answers to RQ 4-RQ 6, including our sup-
porting data and rationale.
Table 1: Performance metrics calculated for each speech act
type (some speech act types have been abbreviated). Recall
that the averages are a weighted average based on the sup-
port for each speech type, see Section 8.3.
precision recall f-measure support
apiAnswer 0.93 0.76 0.83 24.6
api/Q_uestion 0.81 0.66 0.71 17.2
clarifAnswer 0.13 0.07 0.09 6.0
clarif/Q_uestion 0.59 0.41 0.48 32.6
con/f_irmation 0.88 0.8 0.83 27.0
docAnswer 0.25 0.2 0.22 3.2
impl/Q_uestion 0.52 0.21 0.28 10.6
implStatement 0.0 0.0 0.0 3.0
introduction 0.76 0.6 0.63 4.0
stmnt 0.69 0.4 0.51 49.8
system/Q_uestion 0.37 0.22 0.27 4.8
avg / total 0.69 0.5 0.57 16.62
9.1 RQ 4: Overall Performance
/T_he weighted average precision of from our classi/f_iers was 69%,
while the weighted average recall was 50%, as reported in Table 1.
/T_hus for an arbitrary incoming message, we can expect this classi-
/f_ier to correctly identify the speech act type of that message 69% of
the time, while identifying 50% of the speech acts types to which
the message belongs. If the classi/f_ier claims that a message of a
particular type, we can estimate that that claim will be correctESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA Andrew Wood, Paige Rodeghero, Ameer Armaly, and Collin McMillan
Table 2: /T_he top 10 most-informative features for each speech act type, calculated by f-score. Most features are words, but
features with the suﬃx sfare shallow features (see Section 7.2). See Section 9.3 for a deeper discussion of this table.
10 9 8 7 6 5 4 3 2 1
apiAnswer node if on/f_inished keyframes constructor values time timeline keyvalue keyframe
api/Q_uestion size method have how pane class object an does what
clarifAnswer compilation con/f_igurations word trigger supply green appear box clicking bo/t_tom
clarif/Q_uestion need the or other wc sf you this /f_ix prime bug
con/f_irmation of yes is to thanks slen sf the thank wc sf ok
documentAnswer byte marks later reading bytes joptionpane external input audio stream
implement/Q_uestion face why mark eratosthenes occurs gets arraycopy reason bu/t_ton clicked
implementStatement signature widget funtion hidden drawing waitfor throwing paint timeout jcomponent
introduction supervised programmers today hello human am start hi study ready
statement seems what slen sf looks /f_ixed but works was think it
system/Q_uestion password there permi/t_ted lang running way programs kill eclipse how
roughly 2/3rds of the time. We acknowledge that we cannot evalu-
ate whether these improve over an earlier approach, given that we
are not aware of an earlier technique for identifying speech acts
on our data. Nevertheless, we /f_ind these results to be an encour-
aging starting point for building a virtual assistant, in light of the
somewhat bare bones text classi/f_ication strategy we used (binary
bag-of-words, see Section 7). A promising area of future work, in
our view, is to adapt more advanced classi/f_ication techniques.
9.2 RQ 5: Speech Act Type Variations
/T_he performance of our classi/f_iers varied considerably across diﬀer-
ent speech act types. At the high end, precision was over 90% and
recall over 75%. At the low end, precision and recall dipped below
around 10%. /T_his observation is important because it means that for
some speech act types, a virtual assistant can be highly con/f_ident
that the prediction is correct. As a practical ma/t_ter, a virtual assis-
tant may request the user to repeat a message in diﬀerent words,
or ask for other followup information, if the classi/f_ier is not able to
place the message into a speech act type with suﬃcient con/f_idence.
/T_his observation is also important from an academic viewpoint,
because it means that programmers use diﬀerent types of language
to make diﬀerent types of messages. In some cases, programmers
consistently use the same language (which is what the classi/f_ier
uses to make good predictions). In other cases, programmers use
much more diﬀerent language – it makes the prediction process
more challenging, but also raises academic questions about what is
diﬀerent about the language, which is an area of future work. We
begin to explore this in RQ 6.
9.3 RQ 6: Attribute Eﬀects
Table 2 shows the top-10 most informative features for each speech
act type. We make two observations from this data: First, the
shallow features are far more useful for some speech act types than
others. For example, con/f_irmation actions are likely to be short
messages, so the word count metric ( wcsf) is informative in this
case. /T_his observation is useful because shallow features are easy to
compute, so areas where they are informative can be predicted with
reasonable accuracy at low cost. Second, many of the words are
general enough that they are likely to be generalizable beyond the
set of bugs we chose, even though others are speci/f_ic to particular
domains. For example, the speech act implementationStatement
is informed by words like “function” and “signature”, which are
likely to be true across many programming conversations. But the
most informative feature for that action is “jcomponent”, which is aword speci/f_ic to Java and perhaps the domain of programs we study.
It is not likely to appear in every domain. /T_herefore, one possible
mediation is to use placeholder features that count the number of
e.g. domain-speci/f_ic programming words used in a message. Also,
we note again that we used the binary bag-of-words model, which
separates the words from their contexts. An area of future work is
in NLP-based recognition such as phrases or n-grams.
10 CONCLUSION
Our paper makes three contributions to so/f_tware engineering liter-
ature. First, we contribute 30 so/f_tware engineering conversations
with professional developers. Second, we created a system of classi-
/f_ication for developer speech acts. We manually detect and classify
relevant speech acts in order to contribute to the understanding
of developer question/answer conversations. We also provide this
annotation classi/f_ication system on our online appendix for fu-
ture researchers to use. /T_hird, we lay the foundation for a virtual
assistant by building an automatic speech act classi/f_ication system.
11 REPRODUCIBILITY
We have made our raw data, annotations, model, and source code
available via an online appendix (h/t_tps://tinyurl.com/yadfpojd) for
the research community to reproduce or use.
12 ACKNOWLEDGEMENTS
We thank and acknowledge the 30 professional developers who par-
ticipated in this research study. /T_his work is supported in part by
the NSF CCF-1452959, CCF-1717607, and CNS-1510329 grants. Any
opinions, /f_indings, and conclusions expressed herein are the authors’
and do not necessarily re/f_lect those of the sponsors. .
REFERENCES
[1]Charu C Aggarwal and ChengXiang Zhai. 2012. Mining text data . Springer
Science & Business Media.
[2] Hua Ai, Joel R Tetreault, and Diane J Litman. 2007. Comparing user simulation
models for dialog strategy learning. In Human Language Technologies 2007: /T_he
Conference of the North American Chapter of the Association for Computational
Linguistics; Companion Volume, Short Papers . Association for Computational
Linguistics, 1–4.
[3] Erik M. Altmann. 2001. Near-term memory in programming: a simulation-based
analysis. International Journal of Human-Computer Studies 54, 2 (2001), 189 –
210. h/t_tps://doi.org/10.1006/ijhc.2000.0407
[4] John Anvik, Lyndon Hiew, and Gail C. Murphy. 2006. Who should /f_ix this bug?. In
Proceedings of the 28th international conference on So/f_tware engineering (ICSE ’06) .
ACM, New York, NY, USA, 361–370. h/t_tps://doi.org/10.1145/1134285.1134336
[5] Apple. 2018. Siri. h/t_tps://www.apple.com/ios/siri/. (2018). Accessed: 2018-03-02.
[6]Ameer Armaly, John Klaczynski, and Collin McMillan. 2016. A Case Study of
Automated Feature Location Techniques for Industrial Cost Estimation. In 2016
IEEE International Conference on So/f_tware Maintenance and Evolution (ICSME) .
553–562. h/t_tps://doi.org/10.1109/ICSME.2016.76Detecting Speech Act Types During Developer Q/A ESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA
[7] Venera Arnaoudova, Sonia Haiduc, Andrian Marcus, and Giulio Antoniol. 2015.
/T_he Use of Text Retrieval and Natural Language Processing in So/f_tware Engineer-
ing. In 2015 IEEE/ACM 37th IEEE International Conference on So/f_tware Engineering ,
Vol. 2. 949–950. h/t_tps://doi.org/10.1109/ICSE.2015.301
[8] Kent Bach and Robert Harnish. 1979. Linguistic communication and speech acts.
(1979).
[9]Maarten Baert. 2018. SimpleScreenRecorder. h/t_tp://www.maartenbaert.be/
simplescreenrecorder/. (2018). Accessed: 2018-03-02.
[10] Gustavo EAPA Batista, Ronaldo C Prati, and Maria Carolina Monard. 2004. A
study of the behavior of several methods for balancing machine learning training
data. ACM Sigkdd Explorations Newsle/t_ter 6, 1 (2004), 20–29.
[11] Bruce L Berg. 2004. Methods for the social sciences . Pearson Education Inc, United
States of America.
[12] David Binkley, Daniel Heinz, Dawn Lawrie, and Justin Overfelt. 2014. Under-
standing LDA in Source Code Analysis. In Proceedings of the 22Nd International
Conference on Program Comprehension (ICPC 2014) . ACM, New York, NY, USA,
26–36. h/t_tps://doi.org/10.1145/2597008.2597150
[13] Barry Boehm. 2006. A view of 20th and 21st century so/f_tware engineering. In
Proceedings of the 28th international conference on So/f_tware engineering . ACM,
12–29.
[14] Nicholas Bradley, /T_homas Fritz, and Reid Holmes. 2018. Context-Aware Conver-
sational Developer Assistants. In International Conference on So/f_tware Engineering .
ACM, 12.
[15] Jean Carle/t_ta. 1996. Assessing Agreement on Classi/f_ication Tasks: /T_he Kappa
Statistic. Comput. Linguist. 22, 2 (June 1996), 249–254. h/t_tp://dl.acm.org/citation.
cfm?id=230386.230390
[16] Rich Caruana and Alexandru Niculescu-Mizil. 2006. An Empirical Comparison
of Supervised Learning Algorithms. In Proceedings of the 23rd International
Conference on Machine Learning (ICML ’06) . ACM, New York, NY, USA, 161–168.
h/t_tps://doi.org/10.1145/1143844.1143865
[17] Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer.
2002. SMOTE: synthetic minority over-sampling technique. Journal of arti/f_icial
intelligence research 16 (2002), 321–357.
[18] Richard Craggs and Mary McGee Wood. 2005. Evaluating Discourse and Dialogue
Coding Schemes. Comput. Linguist. 31, 3 (Sept. 2005), 289–296. h/t_tps://doi.org/
10.1162/089120105774321109
[19] B. Cruz, B. Jayaraman, A. Dwarakanath, and C. McMillan. 2017. Detecting Vague
Words & Phrases in Requirements Documents in a Multilingual Environment. In
Requirements Engineering Conference (RE), 2017 25th IEEE International .
[20] Laura Dabbish, Colleen Stuart, Jason Tsay, and Jim Herbsleb. 2012. Social coding
in GitHub: transparency and collaboration in an open so/f_tware repository. In
Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work .
ACM, 1277–1286.
[21] Nils Dahlb ¨ack, Arne J ¨onsson, and Lars Ahrenberg. 1993. Wizard of Oz stud-
ies/f_i/exclam_questionwhy and how. Knowledge-based systems 6, 4 (1993), 258–266.
[22] N. Dahlbck, A. Jnsson, and L. Ahrenberg. 1993. Wizard of Oz studies /f_i/exclam_question why
and how. Knowledge-Based Systems 6, 4 (1993), 258 – 266. h/t_tps://doi.org/10.
1016/0950-7051(93)90017-N Special Issue: Intelligent User Interfaces.
[23] Sidney D’mello and Art Graesser. 2013. AutoTutor and Aﬀective Autotutor:
Learning by Talking with Cognitively and Emotionally Intelligent Computers
/T_hat Talk Back. ACM Trans. Interact. Intell. Syst. 2, 4, Article 23 (Jan. 2013),
39 pages. h/t_tps://doi.org/10.1145/2395123.2395128
[24] Javier Escobar-Avila, Esteban Parra, and Sonia Haiduc. 2017. Text Retrieval-
based Tagging of So/f_tware Engineering Video Tutorials. In Proceedings of the 39th
International Conference on So/f_tware Engineering Companion (ICSE-C ’17) . IEEE
Press, Piscataway, NJ, USA, 341–343. h/t_tps://doi.org/10.1109/ICSE-C.2017.121
[25] Kate Forbes-Riley, Mihai Rotaru, and Diane J. Litman. 2008. /T_he Relative Impact
of Student Aﬀect on Performance Models in a Spoken Dialogue Tutoring System.
User Modeling and User-Adapted Interaction 18, 1-2 (Feb. 2008), 11–43. h/t_tps:
//doi.org/10.1007/s11257-007-9038-5
[26] Cecilia E Ford, Barbara A Fox, and Sandra A /T_hompson. 2002. /T_he language of
turn and sequence . Oxford University Press on Demand.
[27] Andrew Forward and Timothy C. Lethbridge. 2002. /T_he relevance of so/f_tware
documentation, tools and technologies: a survey. In Proceedings of the 2002 ACM
symposium on Document engineering (DocEng ’02) . ACM, New York, NY, USA,
26–33. h/t_tps://doi.org/10.1145/585058.585065
[28] Apache Foundation. 2018. Apache Commons IO. h/t_tps://commons.apache.org/
proper/commons-io/. (2018). Accessed: 2018-03-02.
[29] Eclipse Foundation. 2018. Eclipse. h/t_tps://eclipse.org/ide/. (2018). Accessed:
2018-03-02.
[30] Malcom Gethers, Bogdan Dit, Huzefa Kagdi, and Denys Poshyvanyk. 2012. Inte-
grated impact analysis for managing so/f_tware changes. In So/f_tware Engineering
(ICSE), 2012 34th International Conference on . IEEE, 430–440.
[31] Micayla Goodrum, Jane Cleland-Huang, Robyn Lutz, Jinghui Cheng, and Ronald
Metoyer. 2017. What Requirements Knowledge Do Developers Need to Manage
Change in Safety-Critical Systems?. In Requirements Engineering Conference (RE),
2017 IEEE 25th International . IEEE, 90–99.[32] Google. 2018. Google Now. h/t_tps://play.google.com/store/apps/details?id=com.
google.android.launcher&hl=en. (2018). Accessed: 2018-03-02.
[33] Stevan Harnad. 1990. /T_he symbol grounding problem. Physica D: Nonlinear
Phenomena 42, 1-3 (1990), 335–346.
[34] Lyne/t_te Hirschman. 1998. Evaluating Spoken Language Interaction: Experi-
ences from the DARPA Spoken Language Program 1988–1995. To appear. See
h/t_tp://www. research. a/t_t. com/˜ walker/eval/hirschman-survey. ps (1998).
[35] Rashina Hoda, James Noble, and Stuart Marshall. 2010. Using grounded theory to
study the human aspects of so/f_tware engineering. In Human Aspects of So/f_tware
Engineering . ACM, 5.
[36] Reid Holmes and Robert J. Walker. 2013. Systematizing pragmatic so/f_tware
reuse. ACM Trans. So/f_tw. Eng. Methodol. 21, 4, Article 20 (Feb. 2013), 44 pages.
h/t_tps://doi.org/10.1145/2377656.2377657
[37] David W Hosmer Jr and Stanley Lemeshow. 2004. Applied logistic regression .
John Wiley & Sons.
[38] Ian Hutchby and Robin Wooﬃ/t_t. 2008. Conversation analysis . Polity.
[39] Siyuan Jiang, Collin McMillan, and Raul Santelices. 2017. Do Programmers do
Change Impact Analysis in Debugging? Empirical So/f_tware Engineering 22, 2 (01
Apr 2017), 631–669. h/t_tps://doi.org/10.1007/s10664-016-9441-9
[40] Alice Kerly, Phil Hall, and Susan Bull. 2007. Bringing chatbots into education:
Towards natural language negotiation of open learner models. Knowledge-Based
Systems 20, 2 (2007), 177 – 185. h/t_tps://doi.org/10.1016/j.knosys.2006.11.014 AI
2006.
[41] Sunghun Kim, E James Whitehead Jr, and Yi Zhang. 2008. Classifying so/f_tware
changes: Clean or buggy? IEEE Transactions on So/f_tware Engineering 34, 2 (2008),
181–196.
[42] Andrew J Ko, Robert DeLine, and Gina Venolia. 2007. Information needs in
collocated so/f_tware development teams. In So/f_tware Engineering, 2007. ICSE 2007.
29th International Conference on . IEEE, 344–353.
[43] Andrew J. Ko and Brad A. Myers. 2004. Designing the Whyline: A Debugging
Interface for Asking /Q_uestions About Program Behavior. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems (CHI ’04) . ACM, New
York, NY, USA, 151–158. h/t_tps://doi.org/10.1145/985692.985712
[44] Andrew J. Ko and Brad A. Myers. 2010. Extracting and Answering Why and Why
Not /Q_uestions About Java Program Output. ACM Trans. So/f_tw. Eng. Methodol.
20, 2, Article 4 (Sept. 2010), 36 pages. h/t_tps://doi.org/10.1145/1824760.1824761
[45] Jan-Peter Kr ¨amer, Joachim Kurz, /T_horsten Karrer, and Jan Borchers. 2012. Blaze.
InProceedings of the 2012 International Conference on So/f_tware Engineering (ICSE
2012) . IEEE Press, Piscataway, NJ, USA, 1457–1458. h/t_tp://dl.acm.org/citation.
cfm?id=2337223.2337451
[46] /T_homas D. LaToza, Gina Venolia, and Robert DeLine. 2006. Maintaining mental
models: a study of developer work habits. In Proceedings of the 28th international
conference on So/f_tware engineering (ICSE ’06) . ACM, New York, NY, USA, 492–501.
h/t_tps://doi.org/10.1145/1134285.1134355
[47] Guillaume Lema ˆıtre, Fernando Nogueira, and Christos K. Aridas. 2017.
Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets
in Machine Learning. Journal of Machine Learning Research 18, 17 (2017), 1–5.
h/t_tp://jmlr.org/papers/v18/16-365
[48] Oliver Lemon. 2011. Learning what to say and how to say it: Joint optimisation
of spoken dialogue management and natural language generation. Computer
Speech & Language 25, 2 (2011), 210–221.
[49] T.C. Lethbridge, J. Singer, and A. Forward. 2003. How so/f_tware engineers use
documentation: the state of the practice. So/f_tware, IEEE 20, 6 (2003), 35–39.
h/t_tps://doi.org/10.1109/MS.2003.1241364
[50] Tim Menzies and Andrian Marcus. 2008. Automated severity assessment of so/f_t-
ware defect reports. In So/f_tware Maintenance, 2008. ICSM 2008. IEEE International
Conference on . IEEE, 346–355.
[51] Microso/f_t. 2018. Cortana. h/t_tps://www.microso/f_t.com/en-us/windows/cortana.
(2018). Accessed: 2018-03-02.
[52] Salman Mirghasemi, John J. Barton, and Claude Petitpierre. 2011. /Q_uerypoint:
moving backwards on wrong values in the buggy execution. In Proceedings of the
19th ACM SIGSOFT symposium and the 13th European conference on Foundations
of so/f_tware engineering (ESEC/FSE ’11) . ACM, New York, NY, USA, 436–439.
h/t_tps://doi.org/10.1145/2025113.2025184
[53] Vibhu O. Mi/t_tal and Johanna D. Moore. 1995. Dynamic Generation of Follow
Up /Q_uestion Menus: Facilitating Interactive Natural Language Dialogues. In
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems
(CHI ’95) . ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 90–97.
h/t_tps://doi.org/10.1145/223904.223916
[54] Johanna D. Moore. 1994. Participating in Explanatory Dialogues: Interpreting and
Responding to /Q_uestions in Context . MIT Press, Cambridge, MA, USA.
[55] Gabriel Murray and Giuseppe Carenini. 2008. Summarizing spoken and wri/t_ten
conversations. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing . Association for Computational Linguistics, 773–782.
[56] Toyoaki Nishida. 2007. Conversational Informatics: An Engineering Approach .
Wiley.
[57] OpenCSV. 2017. OpenCSV. h/t_tp://opencsv.sourceforge.net/. (2017). Accessed:
2017-08-20.ESEC/FSE 2018, 4–9 Nov., 2018, Lake Buena Vista, Florida, USA Andrew Wood, Paige Rodeghero, Ameer Armaly, and Collin McMillan
[58] Rebecca J Passonneau and Diane J Litman. 1993. Intention-based segmentation:
Human reliability and correlation with linguistic cues. In Proceedings of the 31st
annual meeting on Association for Computational Linguistics . Association for
Computational Linguistics, 148–155.
[59] Fabian Pedregosa, Ga ¨el Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand /T_hirion, Olivier Grisel, Mathieu Blondel, Peter Pre/t_tenhofer, Ron Weiss,
Vincent Dubourg, et al .2011. Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research 12, Oct (2011), 2825–2830.
[60] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. /T_hirion, O. Grisel, M.
Blondel, P. Pre/t_tenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[61] Piotr Pruski, Sugandha Lohar, William Goss, Alexander Rasin, and Jane Cleland-
Huang. 2015. TiQi: answering unstructured natural language trace queries.
Requirements Engineering 20, 3 (01 Sep 2015), 215–232. h/t_tps://doi.org/10.1007/
s00766-015-0224-4
[62] Sarah Rastkar, Gail C Murphy, and Gabriel Murray. 2010. Summarizing so/f_tware
artifacts: a case study of bug reports. In Proceedings of the 32nd ACM/IEEE
International Conference on So/f_tware Engineering-Volume 1 . ACM, 505–514.
[63] Sarah Rastkar, Gail C Murphy, and Gabriel Murray. 2014. Automatic summa-
rization of bug reports. IEEE Transactions on So/f_tware Engineering 40, 4 (2014),
366–380.
[64] Jesse Read, Bernhard Pfahringer, Geoﬀ Holmes, and Eibe Frank. 2011. Classi/f_ier
chains for multi-label classi/f_ication. Machine learning 85, 3 (2011), 333–359.
[65] Norbert Reithinger and Elisabeth Maier. 1995. Utilizing Statistical Dialogue Act
Processing in VERBMOBIL. In Proceedings of the 33rd Annual Meeting on Associa-
tion for Computational Linguistics (ACL ’95) . Association for Computational Lin-
guistics, Stroudsburg, PA, USA, 116–121. h/t_tps://doi.org/10.3115/981658.981674
[66] Laurel D Riek. 2012. Wizard of oz studies in hri: a systematic review and new
reporting guidelines. Journal of Human-Robot Interaction 1, 1 (2012).
[67] Verena Rieser and Oliver Lemon. 2011. Reinforcement learning for adaptive
dialogue systems: a data-driven methodology for dialogue management and natural
language generation . Springer Science & Business Media.
[68] C. J. Van Rijsbergen. 1979. Information Retrieval (2nd ed.). Bu/t_terworth-
Heinemann, Newton, MA, USA.
[69] Martin P Robillard, Walid Maalej, Robert J Walker, and /T_homas Zimmermann.
2014. Recommendation systems in so/f_tware engineering . Springer.
[70] Martin P Robillard, Andrian Marcus, Christoph Treude, Gabriele Bavota, Oscar
Chaparro, Neil Ernst, Marco Aur ´elio Gerosa, Michael Godfrey, Michele Lanza,
Mario Linares-V ´asquez, et al .2017. On-Demand Developer Documentation. In
So/f_tware Maintenance and Evolution (ICSME), 2017 IEEE International Conference
on. IEEE.
[71] Paige Rodeghero, Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. De-
tecting User Story Information in Developer-client Conversations to Gener-
ate Extractive Summaries. In Proceedings of the 39th International Conference
on So/f_tware Engineering (ICSE ’17) . IEEE Press, Piscataway, NJ, USA, 49–59.
h/t_tps://doi.org/10.1109/ICSE.2017.13
[72] Paige Rodeghero, Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. De-
tecting user story information in developer-client conversations to generate
extractive summaries. In Proceedings of the 39th International Conference on
So/f_tware Engineering . IEEE Press, 49–59.
[73] Paige Rodeghero, Collin McMillan, Paul W. McBurney, Nigel Bosch, and Sidney
D’Mello. 2014. Improving Automated Source Code Summarization via an Eye-
Tracking Study of Programmers. In Proceedings of the 36th international conference
on So/f_tware engineering (ICSE ’14) . 12. To appear.
[74] Tobias Roehm, Rebecca Tiarks, Rainer Koschke, and Walid Maalej. 2012. How do
professional developers comprehend so/f_tware?. In Proceedings of the 2012 Inter-
national Conference on So/f_tware Engineering (ICSE 2012) . IEEE Press, Piscataway,
NJ, USA, 255–265. h/t_tp://dl.acm.org/citation.cfm?id=2337223.2337254
[75] Jost Schatzmann, Blaise /T_homson, Karl Weilhammer, Hui Ye, and Steve Young.
2007. Agenda-based user simulation for bootstrapping a POMDP dialogue system.
InHuman Language Technologies 2007: /T_he Conference of the North American
Chapter of the Association for Computational Linguistics; Companion Volume,
Short Papers . Association for Computational Linguistics, 149–152.
[76] Emanuel A.. Schegloﬀ. 2007. Sequence Organization in Interaction: A Primer in
Conversation Analysis I . Cambridge University Press.
[77] John Searle. 1965. What is a speech act? na.
[78] John R Searle, Ferenc Kiefer, and Manfred Bierwisch. 1980. Speech act theory and
pragmatics . Vol. 10. Springer.
[79] Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin, and Joelle
Pineau. 2015. A survey of available corpora for building data-driven dialogue
systems. arXiv preprint arXiv:1512.05742 (2015).
[80] Bonita Sharif, Michael Falcone, and Jonathan I. Maletic. 2012. An eye-tracking
study on the role of scan time in /f_inding source code defects. In Proceedings of
the Symposium on Eye Tracking Research and Applications (ETRA ’12) . ACM, New
York, NY, USA, 381–384. h/t_tps://doi.org/10.1145/2168556.2168642
[81] Jonathan Sillito, Gail C. Murphy, and Kris De Volder. 2008. Asking and Answering
/Q_uestions during a Programming Change Task. IEEE Trans. So/f_tw. Eng. 34, 4(July 2008), 434–451. h/t_tps://doi.org/10.1109/TSE.2008.26
[82] J. Sillito, G. C. Murphy, and K. De Volder. 2008. Asking and Answering /Q_uestions
during a Programming Change Task. IEEE Transactions on So/f_tware Engineering
34, 4 (July 2008), 434–451. h/t_tps://doi.org/10.1109/TSE.2008.26
[83] S. E. Sim, C. L. A. Clarke, and R. C. Holt. 1998. Archetypal Source Code Searches:
A Survey of So/f_tware Developers and Maintainers. In Proceedings of the 6th
International Workshop on Program Comprehension (IWPC ’98) . IEEE Computer
Society, Washington, DC, USA, 180–. h/t_tp://dl.acm.org/citation.cfm?id=580914.
858229
[84] J. Starke, C. Luce, and J. Sillito. 2009. Searching and skimming: An exploratory
study. In So/f_tware Maintenance, 2009. ICSM 2009. IEEE International Conference
on. 157–166. h/t_tps://doi.org/10.1109/ICSM.2009.5306335
[85] Amanda Stent and Srinivas Bangalore. 2014. Natural language generation in
interactive systems . Cambridge University Press.
[86] Paul Ten Have. 2007. Doing conversation analysis . Sage.
[87] UpWork. 2018. UpWork. h/t_tps://www.upwork.com/. (2018). Accessed: 2018-03-
02.
[88] Marilyn A. Walker, Rebecca Passonneau, and Julie E. Boland. 2001. /Q_uantitative
and /Q_ualitative Evaluation of Darpa Communicator Spoken Dialogue Systems.
InProceedings of the 39th Annual Meeting on Association for Computational
Linguistics (ACL ’01) . Association for Computational Linguistics, Stroudsburg,
PA, USA, 515–522. h/t_tps://doi.org/10.3115/1073012.1073078
[89] Kevin F. White and Wayne G. Lu/t_ters. 2003. Behind the Curtain: Lessons Learned
from a Wizard of Oz Field Experiment. SIGGROUP Bull. 24, 3 (Dec. 2003), 129–135.
h/t_tps://doi.org/10.1145/1052829.1052854
[90] Steve Whi/t_taker, Marilyn A Walker, and Johanna D Moore. 2002. Fish or Fowl:
A Wizard of Oz Evaluation of Dialogue Strategies in the Restaurant Domain.. In
LREC .