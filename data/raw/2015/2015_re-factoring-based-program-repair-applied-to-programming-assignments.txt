Re-factoring based Program Repair applied to
Programming Assignments
Y ang Hu‚àó
The University of Texas at Austin
huyang@utexas.eduUmair Z. Ahmed‚Ä†
National University of Singapore
umair@comp.nus.edu.sgSergey Mechtaev
University College London
s.mechtaev@ucl.ac.uk
Ben Leong
National University of Singapore
bleong@comp.nus.edu.sgAbhik Roychoudhury
National University of Singapore
abhik@comp.nus.edu.sg
Abstract ‚ÄîAutomated program repair has been used to provide
feedback for incorrect student programming assignments, since
program repair captures the code modiÔ¨Åcation needed to makea given buggy program pass a given test-suite. Existing student
feedback generation techniques are limited because they either
require manual effort in the form of providing an error model,
or require a large number of correct student submissions to learn
from, or suffer from lack of scalability and accuracy.
In this work, we propose a fully automated approach for
generating student program repairs in real-time. This is achievedby Ô¨Årst re-factoring all available correct solutions to semantically
equivalent solutions. Given an incorrect program, we match the
program with the closest matching refactored program based on
its control Ô¨Çow structure. Subsequently, we infer the input-output
speciÔ¨Åcations of the incorrect program‚Äôs basic blocks from the
executions of the correct program‚Äôs aligned basic blocks. Finally,
these speciÔ¨Åcations are used to modify the blocks of the incorrect
program via search-based synthesis.
Our dataset consists of almost 1,800 real-life incorrect Python
program submissions from 361 students for an introductory pro-gramming course at a large public university. Our experimental
results suggest that our method is more effective and efÔ¨Åcient
than recently proposed feedback generation approaches. About
30% of the patches produced by our tool Refactory are smaller
than those produced by the state-of-art tool Clara, and can beproduced given fewer correct solutions (often a single correct
solution) and in a shorter time. We opine that our method
is applicable not only to programming assignments, and could
be seen as a general-purpose program repair method that can
achieve good results with just a single correct reference solution.
Index Terms ‚ÄîProgram Repair, Programming Education, Soft-
ware Refactoring
I. I NTRODUCTION
Program repair is an emerging technology that seeks to
rectify program errors automatically, thereby meeting a cor-rectness criterion, such as passing a test-suite. Besides improv-
ing programmer productivity, this technique can be applied to
programming education. Particularly, program repair has been
applied to automated grading [1], and providing hints about
program errors [2]. In this work, we propose a repair method
where an incorrect program can be repaired with the help of
‚àóThis work was done by the Ô¨Årst author at National University of Singapore.‚Ä†Corresponding author.one or more correct reference solutions. While our approach is
general-purpose, in our experiments, we focus on generating
repair-based feedback for incorrect programming assignments.
Program repair has previously been used to provide feed-
back on incorrect student submissions for programming as-
signments [1]‚Äì[7]. The programming assignments are usuallysegments of code, so the limited scalability of existing program
repair techniques is not a concern. However, it has been
observed from a corpus of programming assignments that
student submissions are often severely incorrect [1]. This is in
stark contrast to the ‚Äúcompetent programmer hypothesis‚Äù that
assumes code bases are largely correct. Since programming
assignments are written by novice programmers and can be
substantially erroneous, they are a testbed to validate the effec-
tiveness of program repair techniques. Since the submissions
for programming assignments are often incorrect, the search
space of edits to be navigated for program repair can be verylarge, even though the program might be small.
Existing systems that repair incorrect programming as-
signments have signiÔ¨Åcant drawbacks because of the manual
effort involved, underlying assumptions about the availability
of correct solutions, and scalability or accuracy concerns.
Approaches like Autograder [5] assume the availability of anerror model that has to be provided manually. Efforts like
sk
p[8] rely on neural networks to correct programs and
suffer from low precision; a recent work has extended neural
reasoning with symbolic analysis [6]. However, the accuracy
of repairs typically remains low in such efforts. Refazer [7]
learns program transformation schema from past submissions
and its performance critically depends on the quality and
quantity of corpus available. The recent works of Clara [3] and
Sarfgen [4] compare an incorrect assignment with an available
correct assignment. Such approaches assume the availability
of a large number and diversity of correct solutions. However,
this assumption often does not hold in practice, e.g. when a
newly crafted assignment is given by an instructor.
Technical Contribution: The main technical contribution
of this paper is a fully automated program repair method
for repairing incorrect student submissions for programming
assignments. While our technique can exploit the availability
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXB SF&OHJOFFSJOH	"4&
¬•*&&&
%0*"4&
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. of a large number of correct solutions to perform better, we
only assume and require one correct reference solution. Ourapproach is to use re-factoring rules to generate a correct
solution with the same control Ô¨Çow as the incorrect program.
Since the buggy program and the re-factored correct programpossess the same (or similar) control Ô¨Çow, we compare their
basic blocks and generate candidate variable mappings be-
tween the two programs based on dynamic observations overtest executions and static analysis. Given such a variable map-
ping, we formulate the program repair problem as judiciously
synthesizing expressions at selected basic blocks to meet the
given correctness criterion (such as passing a test-suite). This
synthesis problem is solved by efÔ¨Åcient search-based synthesis
where a large space of expressions is efÔ¨Åciently navigated
to construct minimal repairs. The expressions considered for
repairs of the basic blocks are obtained from expressiontemplates or by mutating existing expressions. Our Refactory
tool implementation of the above approach has been made
available at https://github.com/githubhuyang/refactory
Conceptual Contribution and Results: If we envision the
feedback generation problem through means of automated pro-gram repair as one of search space construction and traversal
(with the search space capturing the possible edits of the
buggy program), our solution enables a novel way to present
and understand this search space. This is the main conceptual
contribution of the work, and we believe this also leads to more
superior experimental results as evidenced by our repair tool
for actual Python programs from a real student submission data
set. By separating the control Ô¨Çow matching (obtained via re-
factoring) from data-Ô¨Çow matching (achieved via search-based
synthesis), we can construct small legible program repairs to
be used as feedback to the students. We evaluate our approachon a large data set of 1,783 buggy student programs, that was
curated from Ô¨Åve different Python assignments offered during
a Ô¨Årst-year university course credited by 361 students. Our tool
Refactory achieves a higher repair rate, smaller patch size and
less overÔ¨Åtting when compared to state-of-the-art tools such
asClara [3]. To verify the generality of our approach and
crafted refactoring rules, we randomly sample an additional
six assignments containing 7,290 buggy student programs and
observe similar results (Section VI). In addition to the practical
utility of our technique in feedback generation, we believe that
our viewpoint of cleanly partitioning the search-space of editsby separating control Ô¨Çow matching from expression synthesis
can be useful for automated program repair.
II. O
VERVIEW
Fig. 1 gives a high-level overview of our approach. Our
approach takes three inputs: a test-suite T, a buggy program Pb
and (one or more) correct programs C. Our approach includes
three phases, which are elaborated in the following.
Phase 1. Refactoring: Given a set of refactoring rules, we
conduct software refactoring on correct programs ( C)t o
generate additional correct programs with new control Ô¨Çow
structures. For example, Fig. 2a shows a correct program
for the programming assignment sequential search , which/g17/g437/g336/g336/g455/g3/g87/g396/g381/g336/g396/g258/g373/g18/g381/g396/g396/g286/g272/g410/g3/g87/g396/g381/g336/g396/g258/g373/g400
/g17/g367/g381/g272/g364/g3
/g68/g258/g393/g393/g349/g374/g336
/g87/g258/g410/g272/g346
/g17/g367/g381/g272/g364/g3/g90/g286/g393/g258/g349/g396/g100/g286/g400/g410/g882/g94/g437/g349/g410/g286/g94/g410/g396/g437/g272/g410/g437/g396/g286/g3
/g68/g258/g410/g272/g346/g349/g374/g336
/g94/g410/g396/g437/g272/g410/g437/g396/g286/g3/g4/g367/g349/g336/g374/g373/g286/g374/g410/g94/g410/g396/g437/g272/g410/g437/g396/g286
/g68/g437/g410/g258/g410/g349/g381/g374/g90/g286/g296/g258/g272/g410/g381/g396/g349/g374/g336
/g115/g258/g396/g349/g258/g271/g367/g286/g3
/g68/g258/g393/g393/g349/g374/g336
/g94/g393/g286/g272/g349/g296/g349/g272/g258/g410/g349/g381/g374/g3
/g47/g374/g296/g286/g396/g286/g374/g272/g286/g17/g367/g381/g272/g364/g3/g87/g258/g410/g272/g346/g3
/g94/g455/g374/g410/g346/g286/g400/g349/g400/g94/g437/g272/g272/g286/g400/g400/g38/g258/g349/g367
Fig. 1: Overview of our approach
outputs how many numbers in a sorted number sequence
seq are smaller than x. To generate a correct program with
new control Ô¨Çow, we mutate the control Ô¨Çow of the correctprogram by adding an empty
else branch to an ifbranch.
The refactored correct program is shown in Fig. 2b.
Phase 2. Structure Alignment: We perform structure match-
ing, for Ô¨Ånding refactored correct programs which have the
same control Ô¨Çow structure with the buggy program Pb.I f
we cannot Ô¨Ånd such programs, Pbmay have bugs in its
control Ô¨Çow. To Ô¨Åx such bugs, we conduct structure mutation ,
which edits the control Ô¨Çow structure of Pbto that of closest
refactored correct program in terms of tree edit distance.
Phase 3. Block Repair: Among all correct programs which
have the same control Ô¨Çow structure with the buggy program,
we search for the correct programs which are the top-k closestto the buggy program P
b(we set k=5in our experimental
evaluation). For any of these top-k closest programs, if we
can construct a patch passing the given test-suite T,w eh a v e
succeeded in repairing, and hence generating feedback.
Phase 3.1 Block Mapping: We build a mapping between basic
blocks in a correct program Pcand those of Pbbased on the
graph isomorphism of the control Ô¨Çow graph of Pcand Pb.
For example, consider the buggy program in Fig. 2c and the
refactored correct program in Fig. 2b, where line 2,3,4,6,7 are
different basic blocks (although line 7 in the buggy program isempty, we regard it as an empty basic block). Assume that B
c
i
is the basic block in line iin the refactored correct program,
and Bb
iis the basic block in line iin the buggy program. Based
on graph isomorphism, we can get {Bi
c/mapsto‚ÜíBi
b}i‚àà2,3,4,6,7.
Phase 3.2 V ariable Mapping: We build a variable mapping
between the correct program Pcand the buggy program Pb
using dynamic equivalence analysis (DEA) [3] and deÔ¨Åne/use
analysis (DUA). In DEA, we collect the trace of each variable
when running the correct and the buggy programs, and then
map two variables if they take the same values in the sameorder when running the same test. For variables that are not
mapped by DEA, we apply DUA, which maps two variables if
the blocks where the Ô¨Årst variable is deÔ¨Åned/used corresponds

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. 1def search (x, seq ) :
2 for iin range (len (
seq ) ) :
3 if x < = seq [ i ]:
4 return i
5
6
7 return len ( seq )
(a) A correct program1def search (x , seq ) :
2 for iin range (len (
seq ) ) :
3 if x < = seq [ i ]:
4 return i
5 else:
6 pass
7 return len ( seq )
(b) A refactored correct program1def search (e , lst ) :
2 for jin range (len (
lst )):
3 if e<l s t [ j ] :
4 return j
5 else :
6 return len(lst)
7
(c) A buggy program1def search (e , lst ) :
2 for jin range (len (
lst )):
3 if e< = lst [j ]:
4 return j
5 else :
6 pass
7 return len(lst)
(d) A Ô¨Åxed program
Fig. 2: Example programs of the sequential search programming assignment from our dataset.
to the blocks where the second variable is deÔ¨Åned/used. To
illustrate these approaches, consider building a variable map-
ping between the buggy program in Fig. 2c and the refactored
correct program in Fig. 2b using the tests search(2,[1,2,3])
and search(3,[4,5,6]) . Table Ia and Table Ib show all the
variable traces collected using DEA. Since the traces of eand
xare the same, and the traces of lst and seq are the same, we
get a variable mapping {e/mapsto‚Üíx,lst/mapsto‚Üíseq}. Note that jand
iare not mapped by DEA because their traces are different.
Then, we execute DUA that identiÔ¨Åes that jand iare deÔ¨Åned
in line 2 and used in line 3 and line 4, and the basic blocks in
line 2, 3, 4 in the buggy program correspond to the basic block
in line 2, 3, 4 in the correct program. Thus, we map jtoi, and
Ô¨Ånally obtain the variable mapping {e/mapsto‚Üíx,lst/mapsto‚Üíseq,j/mapsto‚Üíi}.
Phase 3.3 SpeciÔ¨Åcation Inference: We generate a speciÔ¨Åca-
tion for each basic block in the buggy program Pbby (1)
collecting inputs and outputs of each basic block in the correct
program and (2) using the variable mapping to translate them
into the inputs and expected outputs of each basic block in
the buggy program. For instance, consider the buggy basic
block e<lst[j] in the buggy program. We collect the inputs
and outputs of its corresponding basic block x<=seq[i] in the
refactored correct program (Table Ic). Then, we replace the
variables using the variable mapping {e/mapsto‚Üíx,lst/mapsto‚Üíseq,j/mapsto‚Üíi}
to generate the speciÔ¨Åcation of e<lst[j] .
Phase 3.4 Block Patch Synthesis: We use the input-output
speciÔ¨Åcation derived in the previous step to check the correct-ness of each basic block in the buggy program. In the buggy
program shown in Fig. 2c, the basic blocks in lines 3, 6 and 7
do not satisfy their input-output speciÔ¨Åcations, and hence we
deem them to be in need for repair. We attempt to generate a
patch for each incorrect basic block in the buggy program.
If the basic block in P
bis empty, we Ô¨Åx it based on the vari-
able mapping and its corresponding basic block in the correct
program Pc. For example, consider the buggy basic block in
line 7, which is an empty basic block. Its corresponding basic
block in the refactored correct program is return len(seq) .
Based on the variable mapping {e/mapsto‚Üíx,lst/mapsto‚Üí seq,j/mapsto‚Üíi},w e
replace the empty basic block in line 7 of the buggy programwith a Ô¨Åxed basic block
return len(lst) .
If the basic block in Pbis not empty, while its corresponding
basic block in Pcis empty, we Ô¨Åx it by making it empty.
For example, consider the incorrect basic block in line 6. Itscorresponding basic block in the refactored correct program is
pass , which is a key word to show it is an empty basic block.
We Ô¨Åx the basic block in line 6 of the buggy program to pass .
If the basic block in Pband its corresponding basic block
inPcare both non-empty, then we synthesize a patch for
the buggy basic block using its speciÔ¨Åcation. Given a set of
suspicious lines in a buggy basic block, we insert holes to
produce a partial program. Then, we perform enumerativesynthesis with test-equivalence analysis [9] to Ô¨Åll the holes
in the partial program. We use two heuristics to generate
expression candidates. First, we utilize expression templates
(i.e., syntax patterns [10] of expressions) in correct programs.
For example, given the expression
x<=seq[i] in the refac-
tored correct program, we can extract an expression template
v0<=v1[v2]where v0,v1,v2are free variables. Using this tem-
plate, we can generate a candidate e<=lst[j] . We also generate
expression candidates by mutating operators or variables of
the expressions in the buggy program. For example, given
the expression e<lst[j] in the buggy program, we generate
candidates e>lst[j] ,e<=lst[j] ,e>=lst[j] , and j<lst[j] .
Once the search space of candidate expressions is con-
structed, we traverse them efÔ¨Åciently using an approach based
on test-equivalence analysis [9]. In this approach, the candidate
expressions are grouped together if they behave identically on
the given input-output examples (these are the speciÔ¨Åcation we
derived earlier). Such an approach greatly contributes to the
scalability of our technique, since it helps to avoid traversing
and checking the candidate patches one by one.
After generating patches for each basic block, we combine
them into a global patch and validate its correctness via the
test-suite. Fig. 2d shows the Ô¨Åxed program.
III. R EFACTORING AND STRUCTURE MUTA TION
In this section, we introduce refactoring rules for mutating
the control Ô¨Çow structure of existing correct solutions to
generate new semantically equivalent correct solutions with
different control Ô¨Çow structures. This step is necessary since
the accuracy of repairing a given buggy program depends onÔ¨Ånding a correct program with similar control Ô¨Çow structure.
We designed generic rules based on the observa-
tion that the same algorithm can have syntactically dif-ferent implementations. For example, although the two
programs in Fig. 3 behave equivalently and contain
the same basic blocks, the control Ô¨Çow structure of

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. Test Inputs e lst j
search(2,[1,2,3]) 2 [1, 2, 3] 0
search(3,[4,5,6]) 3 [4, 5, 6] 0
(a) V ariable traces of the buggy programTest Case x seq i
search(2,[1,2,3]) 2 [1, 2, 3] 0/mapsto‚Üí1
search(3,[4,5,6]) 3 [4, 5, 6] 0
(b) V ariable traces of the correct programInputOutputx seq i
2 [1, 2, 3] 0 False
2 [1, 2, 3] 1 True3 [4, 5, 6] 0 True
(c) Inputs and outputs of x<=seq[i]
T ABLE I: Motivating example of our approach.
1def func (x) :
2 if x< =0 :
3 return ‚àí1
4
5 return 1
(a) Control Flow CFx1def func (x) :
2 if x< =0 :
3 return ‚àí1
4 else :
5 return 1
(b) Control Flow CFy
Fig. 3: Semantically equivalent programs with different control
Ô¨Çow structures.
Fig. 3a, given as {Func start ,I fstart ,I fend, Func end}, differs
from the control Ô¨Çow structure of Fig. 3b, given as
{Func start ,I fstart ,I fend, Else start , Else end, Func end}.
Since there is potential to generate inÔ¨Ånitely many correct
variants using our refactoring rules, we guide the search for
closest refactored program using the following heuristic. Thecorrect programs are ranked based on their edit distance from
the given buggy program. Rules are repeatedly applied on the
closest correct program to generate new correct programs untila correct program with a matching control Ô¨Çow is found, or a
predeÔ¨Åned maximum depth is reached, or a timeout occurs.
As observed in our dataset of 1783 incorrect programs
(Section V), 35% of them do not have a matching control
Ô¨Çow structure in the correct submissions. Thus, existing tech-
niques such as Sarfgen [4], which rely on exact control-Ô¨Çow
matches, cannot repair these programs. We deÔ¨Åne a total of
10 bi-directional refactoring rules divided into Ô¨Åve different
categories, which are listed in Fig. 4. These rules transform
control Ô¨Çow structures of our 2442 correct student programs,
after which only less than 20% of the incorrect programs lack
a matching correct program. For these remaining incorrect pro-
grams, structure mutation is used to borrow the missing control
Ô¨Çow structure from the closest matching correct program.
A. Existing conditional transformations
This category of rules transform existing conditional state-
ments. The rules are presented in the form of abstract syntax
tree (AST) transformations.
1) Successor statements to a conditional jump: The rule
R
A1in Fig 4 states that the block of statements Swhich
succeeds a conditional jump (such as Break /Continue /Return ),
can occur either as a successor node to the Ifblock (Fig. 4a),
or inside an Else branch to the Ifblock (Fig. 4b).
Consider our earlier example listed in Fig. 3. The program
in Fig. 3a (respectively Fig. 3b) can be mutated to the program
in Fig. 3b (resp. Fig. 3a) on application of refactoring rule RA1,since the control Ô¨Çow of program CFx(resp. CFy) matches
with the control Ô¨Çow of rule CFa(resp. CFb).
2) Conditional statement with conjunction: AnIfstatement
with the condition being a conjunction of C1and C2(Fig. 4c)
can be rewritten as a nested If structure, containing the
conditions C1and C2individually (Fig. 4d), using rule RA2.
B. New conditional transformations
These set of rules introduce additional guards; either around
arbitrary statements, or around existing conditionals.
1) Introduce new Ifconditionals: In this rule RB1,w e
introduce three types of Ifconditional blocks. Fig. 4f adds
a trivially True conditional guard around an arbitrary node S.
Fig. 4g introduces a trivially False conditional guard around
an arbitrary block B‚àó
1. Fig. 4h introduces an arbitrary condition
C‚àó
1around a pass (no-op) statement.
The arbitrary block B‚àó
1(respectively condition C‚àó
1) are place-
holders which can match and copy any corresponding block
(resp. condition) of incorrect program, during the block map-
ping phase of our approach described later in Section IV -A.
2) Introduce new Elif /Else branch: The rule RB2intro-
duces Elif and Else branching statements to an existing If
conditional statement. Fig. 4j adds a trivially False Elif branch
containing arbitrary block B‚àó
1. Fig. 4k introduces an arbitrary
C‚àó
1conditional Elif branch, around a pass (no-op) statement.
Fig. 4l adds an Else branch containing pass statement.
C. Loop guards
These set of rules deal with introducing additional guards
surrounding an existing loop structure.
1) Introduce guard around For loop: Programs containing
for statement which loops over an iterator (such as list) can be
mutated into a new program structure by introducing guards
around the loop, targeting the case when iterator is empty
(Fig 4m to Fig 4n), or non-empty (Fig 4m to Fig 4o).
2) Introduce guard around While loop: Similar to previous
rule, guards can be introduced in programs which loop over
an iterator using while loop, targeting the case when iterator
is empty (Fig 4p to Fig 4q), or non-empty (Fig 4p to Fig 4r).
D. While loop transformations
These set of rules replace while loop structure with an
equivalent conditional jump statement, or vice-versa.
1) Conditional break inside While loop: A program which
loops until a condition C1is satisÔ¨Åed (Fig. 4s) can be refac-
tored into another program which loops indeÔ¨Ånitely, with a C1
conditional break instruction inside the loop‚Äôs body (Fig. 4t).

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. Category A: Existing conditional transformations
Predecessor P
ifCondition C 1:
Basic Block B1
Jump Instruction J1
Successor S
(a)Predecessor P
ifCondition C 1:
Basic Block B1
Jump Instruction J1
else:
Successor S
(b)
Rule RA1: Successor statements to a conditional jump.Predecessor P
ifCondition C 1‚àßC2:
Block B1
Successor S
(c)Predecessor P
ifCondition C 1:
ifCondition C 2:
Block B1
Successor S
(d)
Rule RA2: Conditional expression with conjunction.
Category B: New conditional transformations
P
S
(e)P
ifTrue :
S
(f)P
ifFalse :
B‚àó
1S
(g)P
ifC‚àó
1:
pass
S
(h)
Rule RB1: Introduce new Ifconditionals.P
ifC1:
B1
S
(i)P
ifC1:
B1
elif False :
B‚àó
2S
(j)P
ifC1:
B1
elif C‚àó
2:
pass
S
(k)P
ifC1:
B1
else:
pass
S
(l)
Rule RB2: Introduce new Elif /Else branch.
Category C: Loop guards
P
for iinI1:
B1
S
(m)P
iflen( I1)>0:
for iinI1:
B1
S
(n)P
iflen( I1)==0:
pass
else:
for iinI1:
B1
S
(o)
Rule RC1: Introduce guard around For loop.P
while i<
len( I1):
B1
S
(p)P
iflen( I1)>0:
while i<
len( I1):
B1
S
(q)P
iflen( I1)==0:
pass
else:
while i<
len( I1):
B1
S
(r)
Rule RC2: Introduce guard around While loop.
Category D: While loop transformations
P
while C1:
B1
S
(s)P
while True :
ifNot C 1:
Break
B1
S
(t)
Rule RD1: Conditional break inside While .P
while C1:
B1
Return R1
S
(u)P
ifC1:
B1
Return R1
S
(v)
Rule RD2: Unconditional return inside While .P
while C1:
B1
Break
S
(w)P
ifC1:
B1
S
(x)
Rule RD3: Unconditional break inside While .
Category E: Loop unrolling
Predecessor P
for iinI1:
B1
Successor S
(y)Predecessor P
for iinSlice(I 1,0 , len( I1)/2):
B1
for iinSlice(I 1,len( I1)/2,len( I1)):
B1
Successor S
(z)
Rule RE1: Split a For loop by iterator slicing.
Fig. 4: List of refactoring rules

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. 2) Unconditional return inside While loop: AWhile loop
which contains an unconditional return jump (Fig. 4u) can be
replaced with an equivalent Ifconditional statement (Fig. 4v),
since the block of statements inside the loop will be executed
only once on successful satisfaction of the loop guard.
3) Unconditional break inside While loop: Similar to the
previous rule, a While loop which contains an unconditional
break jump (Fig. 4w) can be replaced with an equivalent If
conditional statement without the break statement (Fig. 4v).
E. Loop unrolling
1) Iterator slicing: Afor loop iterating over a sequence
(Fig. 4y), can be split into two loops that iterate over two
distinct sets of consecutive elements (Fig. 4z). The operator
Slice(I,i1,i2)(denoted as I[i1:i2]) returns a subsequence of
the elements starting of Iat index i1until index i2.
F . Structure Mutation
Given a buggy program Pb, we Ô¨Årst search for a program Pc
from the set of correct student submissions and their refactored
variants, such that Pchas the same control-Ô¨Çow structure as
Pb. If no such match is found, we attempt structure mutation
that modiÔ¨Åes the control-Ô¨Çow structure of the buggy program
Pb. First, it searches for the closest program P/prime
cwrt control-
Ô¨Çow structure from the set of correct programs and their
refactored variants. Then, it borrows a minimal number of
control-Ô¨Çow nodes (such as if-conditional or loop statements)
from P/prime
cinto Pb, in order to make their structure isomorphic.
Unlike refactoring rules, which mutates the control-Ô¨Çow
structure of correct programs while preserving semantic equiv-alence, structure mutation does not offer such guarantee.
IV . B
LOCK REPAIR
Given a correct program Pcthat has the same control Ô¨Çow
structure as the buggy program Pb, we execute block repair
algorithm to repair Pb. The algorithm consists of four stages.
First, we construct a block mapping based on the isomorphism
of the two control Ô¨Çow graphs (CFG). Second, we Ô¨Ånd a
mapping between the variables of Pband Pc. Third, we infer
a correct speciÔ¨Åcation for each basic block in Pbfrom Pc.
Finally, we synthesize a patch for each basic block of Pb, and
combine all block patches into a global patch.
A. Block Mapping
The goal of this stage is to Ô¨Ånd a mapping between the
basic blocks of Pband those of Pc. Since Pband Pchave the
same control-Ô¨Çow structure, their control Ô¨Çow graphs are iso-
morphic. Thus, a block mapping is effectively an isomorphism
between the two control Ô¨Çow graphs.
DeÔ¨Ånition 1. Block Mapping. LetG(Pc)be CFG of P c
with nodes {Bc
i}i‚àà1..nandG(Pb)be CFG of P bwith nodes
{Bb
i}i‚àà1..n. We deÔ¨Åne a block mapping B(Pc,Pb)as a CFG
isomorphism {Bc
1/mapsto‚ÜíBb
j1,..., Bc
n/mapsto‚ÜíBb
jn}between G(Pc)and
G(Pb), where{j1,..., jn}are different indexes from 1to n.B. V ariable Mapping
The purpose of variable mapping is to identify how variables
ofPccorrespond to the variables of Pb.
DeÔ¨Ånition 2. V ariable Mapping. Let x 1,..., xmbe variables of
the correct program P c, and y1,...,ynbe variables of the buggy
program P b. Then,{xi1/mapsto‚Üíyj1,..., xis/mapsto‚Üíyjs}is a mapping of
variables if i 1,..., is‚àà1..n are different indices and j 1,..., js‚àà
1..m are different indices.
Since there exist many possible mappings, we apply Dy-
namic Equivalence Analysis (DEA ) and DeÔ¨Åne/Use Analysis
(DUA ) to Ô¨Ålter out irrelevant mappings. In DEA, we collect the
variable traces collected on Pband Pc. The trace of a variable
in a test refers to the sequence of values that the variable takes
during the test execution.
The intuition behind DEA is that if a variable xinPctakes
the same values in the same order as a variable yinPbduring
each test execution, then they represent the same user intent.In this case, we say yis a variable candidate of x.
DeÔ¨Ånition 3. Mapped V ariable Candidates in DEA. Let x
be a variable in P
c,ybe a variable in P b, and T be a set
of tests.M DEA(x)represents a set of variable candidates in
Pbthat x can be mapped to. We deÔ¨Åne y‚ààM DEA(x)iff for
each test t ‚ààT, the sequence of values that ytakes during the
execution of P bwith t is the same as the sequence of values
that x takes during the execution of P cwith t.
In DUA, we assume that variables that are deÔ¨Åned and
used in the same manner are more likely to have the same
user intent. We get a set of variable candidates in Pbwhich a
variable in Pccan be mapped to as follows.
DeÔ¨Ånition 4. Mapped V ariable Candidates in DUA. Let
D(P,x)be the set of basic blocks in the program P where
the variable x is deÔ¨Åned, U(P,x)be the set of basic blocks in
the program P where the variable x is used, r be a variable
in P cand s be a variable in P b.M DUA(r)represents a set of
variable candidates in P bthat r can be mapped to in DUA.
We deÔ¨Åne s ‚ààM DUA(r)iff (1) there exists a one-one block
mapping from D(Pc,r)toD(Pb,s)and (2) there exists a one-
one block mapping from U(Pc,r)toU(Pb,s).
Finally, we rule out all invalid variable mappings that do
not map the variable rinPcto any variable candidates in Pb.
DeÔ¨Ånition 5. V alid V ariable Mapping. Let{ci1/mapsto‚Üí
bj1,..., cin/mapsto‚Üí bjn}be a variable mapping between P cand
Pb. We say that the variable mapping is invalid if and only if
‚àÉe‚àà1..n.(bje/nelementM DEA(cie)‚à™M DUA(cie))
If no variable mapping is valid, the block repair algorithm
will report a repair failure on Pb. Otherwise, we enumerate
all valid variable mappings one-by-one until the algorithm
successfully infers a speciÔ¨Åcation and synthesizes a patch.
C. SpeciÔ¨Åcation Inference
First, we analyze the correct program to extract a spec-
iÔ¨Åcation for each basic block. This is done by running Pc

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. on our test-suite Tto collect input-output state pairs of each
basic block in Pc. Here, input state refers to the values of all
variables before executing the basic block, and output state
refers to these values after executing the basic block.
DeÔ¨Ånition 6. SpeciÔ¨Åcation. Let B be a basic block in a
program P, and T be the test-suite. The speciÔ¨Åcation of B
is deÔ¨Åned as a set of input-output state pairs {/angbracketleftIj,Oj/angbracketright}j‚àà1,...,r,
where I jdenotes the input state and O jdenotes the expected
output state of B given I jas the input state.
Note that in the above deÔ¨Ånition we use a set of state pairs
because each basic block can be executed multiple timesduring a test execution. Our algorithm infers a speciÔ¨Åcation
of a basic block B
bbased on that of its corresponding basic
block Bcand the valid variable mapping M.
DeÔ¨Ånition 7. SpeciÔ¨Åcation Inference. Let B cbe a basic
block in the correct program P c, and{/angbracketleftIc
j,Oc
j/angbracketright}j‚àà1..rbe a
speciÔ¨Åcation of B c, and B bbe the corresponding basic block
in the buggy program P b, and M be a valid variable mapping
between P cand P b. A speciÔ¨Åcation of B bis inferred as a set
of input-output state pairs {/angbracketleftIb
j,Ob
j/angbracketright}j‚àà1..rsuch that
{y/mapsto‚Üíu}‚àà Ib
j‚áî‚àÉx.{x/mapsto‚Üíu}‚àà Ic
j‚àß{x/mapsto‚Üíy}‚àà M
{y/mapsto‚Üíu}‚àà Ob
j‚áî‚àÉx.{x/mapsto‚Üíu}‚àà Oc
j‚àß{x/mapsto‚Üíy}‚àà M
D. Patch Synthesis
Before repairing a basic block BbinPb, we verify the
correctness of Bbby collecting the inputs and their corre-
sponding outputs of Bband comparing them with the inputs
and expected outputs in its inferred speciÔ¨Åcation. Formally
speaking, we run Pbon the test-suite Tto collect a set of input-
output pairs {/angbracketleftÀÜIb
j,ÀÜOb
j/angbracketright}j‚àà1,...,zofBb.W es a y Bbis incorrect if
there exist /angbracketleftÀÜIb
u,ÀÜOb
u/angbracketrightand/angbracketleftIb
v,Ob
v/angbracketrightsuch that Ib
v‚äÇÀÜIb
u‚àßOb
v/npropersubsetÀÜOb
u.
For Bbis incorrect, we attempt to repair it. If either Bc
orBbis an empty basic block, we Ô¨Åx it either by generating
an empty block as a patch of Bbor using the valid variable
mapping to translate Bcto a patch of Bb. In other words,
we replace all variable names in Bcwith their corresponding
variable names according to the valid variable mapping.
IfBcand Bbare not empty, we use a program synthesis
technique to generate a patch for Bbbased on its speciÔ¨Åcation.
Given a set of suspicious lines, we produce a partial program
with holes inserted in buggy lines. We generate expression
candidates for each hole. Our goal is to Ô¨Åll holes with
expressions that enable the block to satisfy the speciÔ¨Åcation.
DeÔ¨Ånition 8. Block Patch Synthesis. Let B be an incorrect
basic block and L be a set of suspicious buggy lines in B. Let
P(B,L)be a partial block that holes are inserted into all lines
in L. Let S lbe a set of expressions candidates for the hole in
line l‚ààL. LetC:S1√ó...√óSl√óL‚ÜíRbe a cost function.
Our aim is to Ô¨Ånd a repair /angbracketlefts1,..., sl/angbracketright‚àà S1√ó...√óSlthat
(i) can Ô¨Åll in P(B,L)to pass the correct speciÔ¨Åcation and (ii)
C(s1,..., sl,L)be minimal among all such basic blocks.
Typically, program repair techniques identify suspicious
lines via statistical fault localization. Considering such tech-niques may not be accurate on students‚Äô submissions which
are usually severely incorrect, we enumerate the all subsets oflines in B
bas sets of suspicious buggy lines in the ascending
order of the number of lines until we Ô¨Ånd a patch.
A simple approach to generate a patch is to enumerate all
block candidates by Ô¨Ålling in holes in the partial block with
all combinations of expressions. However, the search space
might be huge, suffering from a combinatorial explosion as
the number of holes grows. To mitigate this issue, we performtest-equivalence analysis [9] when searching for a patch. In a
nutshell, we partition candidates into test-equivalence classes.
For each class, only one representative patch is executed and
veriÔ¨Åed, thereby reducing the number of test executions.
DeÔ¨Ånition 9. Test-Equivalence Relation. LetBbe a set of
block candidates, and Œ±be an input-output pair in the correct
speciÔ¨Åcation of B. An test-equivalence relation on Œ±is deÔ¨Åned
as an equivalence relation ‚Üî
Œ±‚äÇB√óBthat if B 1‚ÜîŒ±B2, then
B1and B 2both pass or failŒ±.
The search space of expression candidates for each hole is
constructed based on expression templates and operator/vari-able mutation. An expression template is a syntax pattern [10]
where variable names in the expression are abstracted away
(i.e., use a set of wildcards instead of the variable names).
Expression templates are extracted from expressions in correct
programs. Formally, let e=/angbracketlefte
1,..., en/angbracketrightbe an expression,
where eidenotes i-th token of e. Let Vbe a set of variable
names. An expression template of ecan be deÔ¨Åned as a
sequence of tokens /angbracketlefte/prime
1,..., e/prime
n/angbracketright, where e/prime
i=‚àóiffei‚ààV.G i v e n
a set of variable names from the buggy program, a space of
candidate expressions is generated by assigning each wildcard
with a unique variable name.
We also generate a space of candidate expressions by
mutating operators or variable names of the suspicious ex-pressions from the buggy program. Let e=/angbracketlefte
1,..., en/angbracketrightbe an
expression, where eidenotes i-th token of e. We construct a
space of candidate expressions by generating e/prime=/angbracketlefte/prime
1,..., e/prime
n/angbracketright,
where ej=e/prime
jwhen j‚àà1,..., k‚àí1,k+1,..., n, and e/prime
kis
such that ek/nequale/prime
kand if ekis a variable, then e/prime
kis another
variable, and if ekan operator, then e/prime
kis another operator.
V. D A TASET AND EXPERIMENTAL SETUP
We choose Clara [3], one of the most recent and related
feedback generation approach with publicly available imple-
mentation1, as the baseline to compare our approach against.
Clara [3] was evaluated on a similar dataset as AutoGrader [5],
which consists of student attempts from MITx MOOC [11].
However, this dataset is not publicly available.
Instead, we evaluate both Clara and our Refactory tool
on real student submissions, collected from an introductory
Python programming course offered at the author‚Äôs university
(National University of Singapore). This course was cred-ited by 361 students, who had to attempt a large number
of programming assignments throughout the entire semester.
1https://github.com/iradicek/CLARA

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. ID DescriptionAvg. #Lines #Correct #Incorrect % CFG MatchRepair RateAvg. Time Relative Patch
of Code Attempt Attempt W/O R W/R Taken (sec) Size (RPS)
1 Sequential search 10 768 575 80.00% 86.78% 98.96% (81.91%) 3.5 (12.1) 0.39 (0.51)
2 Unique dates/months 28 291 435 33.33% 68.28% 78.16% (42.07%) 4.8 (17.4) 0.56 (0.31)3 Duplicate elimination 7 546 308 87.34% 89.61% 97.40% (92.86%) 4.7 (8.5) 0.34 (0.58)
4 Sorting tuples 9 419 357 52.94% 81.23% 88.24% (64.43%) 8.7 (20.6) 0.31 (0.84)
5 Top-k elements 11 418 108 80.56% 83.33% 87.96% (93.52%) 13.1 (11.8) 0.32 (0.61)
1‚Äì5 overall 14 2442 1783 64.50% 81.44% 90.80% (71.28%) 5.5 (13.6) 0.40 (0.56)
T ABLE II: Results on Ô¨Åve programming assignments. ‚Äú% CFG Match‚Äù is the percentage of incorrect submissions for which
correct submissions with matching control-Ô¨Çow structure are found without refactoring (W/O R) and with refactoring (W/ R).
Repair rate, average time-taken and relative patch size per assignment are shown for Refactory (and for Clara in brackets).
Students were allowed to submit multiple attempts, only the
last of which was graded. On each attempt, students receivedthe test-suite evaluation results as feedback.
From these assignments, we Ô¨Ålter out those attempts that
contain syntax errors, or contain a single basic block (trivial
assignments), or utilize Python language features unsupported
by implementation of Refactory orClara (such as lambda
functions, exception handling, Object-Oriented Programming
concepts). After Ô¨Åltering, 19assignments remain, from which
we selected 5assignments for an initial evaluation and crafting
our refactoring rules. Totally. 2,442 correct submissions and
1,783 incorrect student attempts form our dataset, along with
the instructor designed test-suite and reference program. This
dataset is described in Table II. Our dataset and Refactory
repair tool are publicly released to aid further research
2.
To test the generality of our refactoring rules and block
repair algorithm, we report results on the remaining 14 as-
signments containing 6,448 correct submissions and 7,290
incorrect student attempts as well.
All experiments are conducted using IntelR/circlecopyrtCoreTMi7-4770
CPU, 8GB RAM and Ubuntu 18.10. Clara has an ofÔ¨Çine phase
for clustering the correct programs, for which we set a Ô¨Åve-
minute timeout per assignment. For the online phase, Clara
and Refactory are conÔ¨Ågured to run in a single-thread mode
with one-minute timeout to repair each incorrect submission.
VI. E V ALUA TION
To evaluate the effectiveness of Refactory , we aim at an-
swering the following research questions:
RQ1 Given a large number of correct submissions, how effec-
tively can Refactory repair incorrect submissions?
RQ2 Given a small number of correct submissions, how effec-
tively can Refactory repair incorrect submissions?
RQ1 and RQ2 investigate the applicability of our approach
to assignments with different number of correct submissions.
Existing data-driven approaches such as Clara and Sarfgen
are designed for assignments with a large number of correct
submissions. We use refactoring rules to generate new correct
submissions, which makes our approach applicable when only
a small number of correct submissions is available.
To answer RQ1, we evaluate Refactory and Clara on
the entire dataset of correct programs. To answer RQ2, we
2https://github.com/githubhuyang/refactoryevaluate them on downsampled dataset, where the number ofcorrect submissions provided as input to these tools is variedfrom 100% to 0% (only the reference program is used).
Explanation of Table II: Table II shows the results on
our 5 assignments selected for initial evaluation. Clara can
generate repairs for 71.28% of 1783 incorrect submissions
consuming 13.6 seconds on average per repair. In comparison,
Refactory can generate repairs for 90.8% of incorrect sub-
missions requiring 5.5 seconds on average per repair. Which
demonstrates that Refactory can repair a signiÔ¨Åcantly larger
% of incorrect submissions, while requiring lesser amount of
time.
This high repair rate of 90.8% by Refactory is made possible
by our refactoring and structure mutation phase. As seen
from Table II, only 64.5% of incorrect programs have a
matching correct submission with exactlt the same control
Ô¨Çow structure. By applying our refactoring rules, we generatenew correct programs, thereby increasing the % CFG match to
81.4%. The remaining incorrect programs which do not have a
CFG match with correct programs undergo structure mutation
during online phase, bringing our overall repair rate to 90.8%.
In comparison, almost a half of Clara ‚Äôs failures occur due to
exceeding the running timeout of 1 min. The remaining occur
when Clara is unable to Ô¨Ånd a matching correct submission
with the same looping structure as incorrect submission.
We also report on Relative Patch Size ( RPS ) metric, to
further evaluate generated patches. Patch size is deÔ¨Åned as the
Tree-Edit-Distance ( TED ) between the Abstract Syntax Tree
of given buggy program ( AST
b) and repaired program ( AST r)
generated by tool. Relative Patch Size ( RPS ), as deÔ¨Åned by
Clara [3], normalizes the patch size with the size of original
buggy program‚Äôs AST. RPS=TED(AST b,AST r)/Size(AST b)
As shown in Table II, repairs generated by Refactory have a
smaller average RPS compared to those generated by Clara
(for majority of incorrect attempts), which indicates that our
repairs are smaller and hence more likely to help students in
rectifying bugs in their incorrect attempts.
Explanation of Figure 5: Fig. 5a shows the average
repair rate achieved by both tools for various sampling rate of
correct submissions provided as input to the tools. The repair
rate of Refactory is relatively consistent when the sampling
rate is reduced while Clara ‚Äôs repair rate drops signiÔ¨Åcantly
with decrease in sampling rate. For example, when sampling

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. /g19 /g21/g24 /g24/g19 /g26/g24 /g20/g19/g19
/g54/g68/g80/g83/g79/g76/g81/g74/g3/g53/g68/g87/g72/g3/g8/g19/g21/g19/g23/g19/g25/g19/g27/g19/g20/g19/g19/g53/g72/g83/g68/g76/g85/g3/g53/g68/g87/g72/g3/g8 /g53/g72/g73/g68/g70/g87/g82/g85/g92
/g38/g79/g68/g85/g68
(a) Average repair rate across various
sampling rates of correct solutions./g19 /g21/g24 /g24/g19 /g26/g24 /g20/g19/g19
/g54/g68/g80/g83/g79/g76/g81/g74/g3/g53/g68/g87/g72/g3/g8/g19/g17/g19/g21/g17/g24/g24/g17/g19/g26/g17/g24/g20/g19/g17/g19/g20/g21/g17/g24/g55/g76/g80/g72/g3/g55/g68/g78/g72/g81/g3/g11/g86/g12 /g53/g72/g73/g68/g70/g87/g82/g85/g92
/g38/g79/g68/g85/g68
(b) Average time taken to repair incorrect
program, across various sampling rates./g19/g17/g19 /g19/g17/g24 /g20/g17/g19 /g20/g17/g24 /g21/g17/g19 /g21/g17/g24
/g53/g72/g79/g68/g87/g76/g89/g72/g3/g51/g68/g87/g70/g75/g3/g54/g76/g93/g72/g19/g17/g19/g19/g19/g17/g21/g24/g19/g17/g24/g19/g19/g17/g26/g24/g20/g17/g19/g19/g20/g17/g21/g24/g39/g72/g81/g86/g76/g87/g92/g53/g72/g73/g68/g70/g87/g82/g85/g92
/g38/g79/g68/g85/g68
(c) Density function of relative patch size.
Fig. 5: Performance comparison between Clara and Refactory .
rate is 0% (only the reference program is provided as input),
Clara achieves less than 40% repair accuracy due to its
reliance on diverse correct programs, compared to more than
90% achieved by Refactory due to its ability to generate new
correct programs using refactoring rules.
Fig. 5b shows the average time taken by both tools to repair
a single program across different sampling rates. Clara ‚Äôs time
cost is proportional to the sampling rate, which highlights its
scalability issue on dataset with a large number of correct
submissions, due to the increase in complexity of Integer
Linear Programming (ILP ). Since Refactory selects the top-
k closest refactored correct programs with same control Ô¨Çow
structure for repair generation, it scales better than Clara .
Fig 5c compares the relative patch size (RPS) of Refactory
against Clara [3], by plotting its density function. This is
obtained by deÔ¨Åning independent random variables which
represent the RPS of a patch generated by Refactory and Clara ,
for each assignment for each sampling rate. Then GaussianKernel Density Estimator [12] is used to generate their proba-
bility density functions based on individual observation values.
Formally, let (x
t
1,..., xt
n)denote the nobservation values of
RPS of a patch generated by a tool tacross all assignments
and sampling rates. We estimate the density function as
f(x)=1
nh/summationtext.1n
i=1K(x‚àíxt
i
h), where his a smoothing parameter
andKrepresents a Gaussian kernel function. From Fig. 5c,
the estimated density (y-axis) of patches for Refactory tool
is higher compared to Clara ‚Äôs when RPS (x-axis) is smaller
than 0.9. In other words, the patches generated by our tool areconcentrated towards small RPS.
Results on Full Data-set: To demonstrate that our
manually crafted refactoring rules do not over-Ô¨Åt our initially
selected Ô¨Åve assignments, reported in Table II, we report addi-
tional results on the 14held out assignments (refer Section V).
On these 14 new assignments, our Refactory tool achieves
repair rate of 71.65% on 7290 incorrect submissions within
6.4seconds on average, and the generated repair‚Äôs average
relative patch size is 0.44. In contrast Clara can repair 30.4%
of7290 incorrect submissions in 15seconds on average, with
a relative patch size of 0.82. Furthermore, our refactoring rules
can improve the overall CFG match rate from 55.47% (W/OR) to 67.08% (W/ R). Overall, our tool achieves high accuracy
with small patch size on the full set of 19 assignments.
VII. R
ELA TED WORK
In this section, we brieÔ¨Çy review existing state-of-the-art
approaches targeting introductory programming assignments,
and clarify the novelties provided by our approach.
A. Automated Program Repair
The Ô¨Åeld of automated program repair [13], where changes
are suggested to the program source code for Ô¨Åxing observable
errors and vulnerabilities, has witnessed an explosive growth
in recent years. GenProg [14] uses search-based techniques to
navigate the space of edits, so as to automatically Ô¨Ånd an edit
where the edited program passes a given test-suite. Learningor pattern-based approaches have been successfully applied in
program repair, e.g. Ô¨Ånding patterns of human patches and
using them in program repair [15], or using machine learning
techniques to rank patch candidates [16].
SemFix [17] and Angelix [18] extract speciÔ¨Åcation from
tests, and use it to synthesize a patch. Refactory also uses
speciÔ¨Åcation inference, however it differs in two key aspects.
First, the speciÔ¨Åcation is inferred from a correct solution
that might have a different implementation and use different
variable names. To tackle this, we apply automatic refactoringand variable matching. Second, Refactory is able to synthesize
patches not only for program expressions, but also for basic
blocks by transplanting fragments of code from the correct
solution. Symbolic execution based patch synthesis algorithm
of SemFix is orthogonal to our core contributions, and can be
potentially used to improve expression synthesis in Refactory .
SimFix [19] mines repairs from similar code and past
patches. In principle, it can be applied to correct student
assignments when a history of previous corrections and a
sufÔ¨Åcient number of similar solutions are available. In contrast,
our approach is designed to work when only a few correct
solutions are available, without relying on the history of
previous corrections. Instead, from a single correct solution we
can generate several correct solutions one of which can match
the control Ô¨Çow of the given buggy program, followed by
which we resort to basic block synthesis. Thus, our approach

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. 1def swap ( l s t , i , j ) :
2 tmp = l s t [ i]
3 lst[j] = lst[i]
4 lst [i] = t m p
(a) An incorrect program1def swap ( l s t , i , j ) :
2 tmp = l s t [ i ]
3 lst [i] = lst [j]
4 lst [j] = t m p
(b) A correct program
Fig. 6: Function to swap two elements in list .
is more applicable in pedagogical scenarios, e.g. when a newly
crafted assignment is given by an instructor.
Our work on using a reference correct solution may appear
superÔ¨Åcially similar to the recent paper [20]. However, [20]
employs simultaneous symbolic analysis of both buggy and
correct programs to produce provably correct repairs. Similar
to other recent works on repair in education [3], [4], we do
not give formal guarantees about the repairs generated by
our approach. Instead we use refactoring and synthesis toefÔ¨Åciently represent/navigate the space of patches.
B. Feedback Generation
Automated program repair tools, originally designed to
work on large codebases targeting experienced developers,
have been used to provide feedback to students on introductory
programming assignments with limited success [1]. Hence,
new tools have been proposed in literature, targeting the novice
programmers and their mistakes speciÔ¨Åcally. AutoGrader [5]
proposes a program synthesis based approach which takes
a reference solution and manually provided error model togenerate repairs on incorrect programs. Refazer [7] attempts to
learn simple syntactic program transformations from historical
edit examples, and applies AST rewrite rules on matching
incorrect programs to automatically repair them.
Clara [3] and Sarfgen [4] are two recent approaches related
to our work that generate complex patches on incorrect student
programs automatically. Given an incorrect program attempt,
Clara and Sarfgen rely on Ô¨Ånding a correct solution having
the same looping and control-Ô¨Çow structure, respectively. This
assumption presents a serious challenge when there is lack of
access to a diverse set of existing correct solutions, for example
when a newly crafted assignment is given by instructor. To
address this issue, our approach attempts to refactor one or
more correct solutions to generate new semantically equivalent
correct solutions, with different looping/control-Ô¨Çow struc-tures. In addition, as noted in our experiments on running
time, Clara suffers from a scalability problem due to the use
of Integer Linear Programming. We are unable to compare
our run-time/accuracy with Sarfgen since their implementation
has not been publicly released; moreover Sarfgen is targeted
towards C# while our tool works only for Python programs.
Consider the incorrect student attempt for
swap function in
Fig. 6a. Here, the student has made a mistake in swapping
two elements of a list, lst[i] and lst[j] , through use of an
intermediate tmp variable. Given the correct program shown in
Fig. 6b as input, our Refactory approach generates the minimal
repair of modifying a single line #2from tmp = lst[i] totmp= lst[j] , by replacing each line with holes and synthesizing
expression candidates. While Clara generates a sub-optimal
solution at the block level, by borrowing the two differing
lines ( 3and4) from the correct program.
VIII. T HREA TS TO V ALIDITY
Our choice of refactoring rules are by no means exhaustive,
and primarily targets conditionals, looping structures, and
their combinations; which constitutes majority of the control
Ô¨Çow mistakes made by students in introductory programmingclasses. While we do report experimental results on 6 addi-
tional assignments unseen during refactoring rule crafting, in
future we plan to rigorously test our tool on larger variety of
programs collated from other publicly available datasets.
Our implementation currently supports only structured pro-
gramming control Ô¨Çow structures. In future, we plan to extendour approach to handle Object-Oriented Programming con-
cepts. Additional complex features available in Python, such
as list comprehensions or lambda functions, are currently not
handled since novice students rarely utilize advanced concepts.
Correctness of repairs is veriÔ¨Åed against instructor-provided
test-suite, a manually designed incomplete speciÔ¨Åcation.
Finally we note that while our implementation is targeted
towards Python programs, our approach based on refactoring
and block repair is not restricted to Python programs.
IX. D
ISCUSSION
The recent past has witnessed an explosion of works on
automated feedback generation for introductory programming
assignments, through means of program repair [1]‚Äì[7]. At
a general level, most of the works search in the space of
program edits to either generate feedback for students or to
help automatically grade assignments. Due to a large variety
of coding errors in programming assignments written by
novice programmers, the search space of edits between a
given incorrect program and a correct program tends to be
huge [1]. Many past works have contributed immensely in
the navigation of this search space of edits which may enable
feedback generation for students. In our work, we have focused
Ô¨Årst on the search space representation, thereby prompting
our refactoring phase, and then attempted to systematize the
navigation of possible patches of a basic block by partitioningthe candidate patches using test-equivalence analysis. Such a
representation and navigation of the search space also allows
us to work in various set-ups including those where manycorrect solutions are not available.
Our efforts are embodied in the form of Refactory ,a
customized Python repair system. We have employed the
repair system extensively over a large data-set of more than athousand programming assignments collected from hundreds
of students enrolled in an introductory programming course.
In future work, we plan to conduct detailed user studies
where the feedback from our tool can be generated live during
tutorial or recitation sessions, so as to gauge the possible
improvement in meeting learning outcomes.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. ACKNOWLEDGMENTS
This work was supported in part by OfÔ¨Åce of Naval Re-
search grant ONRG-NICOP-N62909-18-1-2052. This workwas partially supported by the National Satellite of Excellence
in Trustworthy Software Systems, funded by NRF Singapore
under National Cybersecurity R&D (NCR) programme. Wewould like to thank Tegawend√© F. Bissyand√© and the anony-
mous reviewers of ASE for their valuable feedback.
R
EFERENCES
[1] J. Yi, U. Z. Ahmed, A. Karkare, S. H. Tan, and A. Roychoudhury,
‚ÄúA feasibility study of using automated program repair for introductory
programming assignments,‚Äù in ACM SIGSOFT International Symposium
on F oundations of Software Engineering (FSE) , 2017.
[2] S. Gulwani, ‚ÄúExample-based learning in computer-aided stem educa-
tion,‚Äù Communications of the ACM , vol. 57, 2014.
[3] S. Gulwani, I. Radicek, and F. Zuleger, ‚ÄúAutomated clustering and
program repair for introductory programming assignments,‚Äù in 39th
ACM SIGPLAN Conference on Programming Lnaguage Design and
Implementation (PLDI) , 2018.
[4] K. Wang, R. Singh, and Z. Su, ‚ÄúSearch, align, and repair: data-driven
feedback generation for introductory programming exercises,‚Äù in 39th
ACM SIGPLAN Conference on Programming Lnaguage Design and
Implementation (PLDI) , 2018.
[5] R. Singh, S. Gulwani, and A. Solar-Lezama, ‚ÄúAutomated feedback gen-
eration for introductory programming assignments,‚Äù in ACM SIGPLAN
Symposium on Programming Language Design and Implementation
(PLDI) , 2013.
[6] S. Bhatia, P . Kohli, and R. Singh, ‚ÄúNeuro-symbolic program repair
for correcting introductory programming assignments,‚Äù in ACM/IEEE
International Conference on Software Engineering (ICSE) , 2018.
[7] R. Rolim, G. Soares, L. D‚ÄôAntoni, O. Polozov, S. Gulwani, R. Suzki, and
B. Hartmann, ‚ÄúLearning syntactic program transformations from exam-
ples,‚Äù in ACM/IEEE International Conference on Software Engineering
(ICSE) , 2017.[8] Y . Pu, K. Narasimhan, A. Solar-Lezama, and R. Barzilay, ‚ÄúSkp: A
neural program corrector for moocs,‚Äù in ACM SIGPLAN International
Conference on Systems, Programming, Languages and Applications(SPLASH) , 2013.
[9] S. Mechtaev, X. Gao, S. H. Tan, and A. Roychoudhury, ‚ÄúTest-
equivalence analysis for automatic patch generation,‚Äù ACM Trans. Softw.
Eng. Methodol. , vol. 27, no. 4, pp. 15:1‚Äì15:37, Oct. 2018.
[10] J. Lee, D. Song, S. So, and H. Oh, ‚ÄúAutomatic diagnosis and correction
of logical errors for functional programming assignments,‚Äù Proceedings
of the ACM on Programming Languages , vol. 2, no. OOPSLA, p. 158,
2018.
[11] MITx MOOC. [Online]. Available: https://www.edx.org/school/mitx
[12] B. W. Silverman, Density estimation for statistics and data analysis .
Routledge, 2018.
[13] C. Le Goues, M. Pradel, and A. Roychoudhury, ‚ÄúAutomated program
repair,‚Äù Communications of the ACM , 2019.
[14] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, ‚ÄúGenprog: A
generic method for automatic software repair,‚Äù IEEE transactions on
software engineering , vol. 38, no. 1, pp. 54‚Äì72, 2012.
[15] D. Kim, J. Nam, J. Song, and S. Kim, ‚ÄúAutomatic patch generation
learned from human-written patches,‚Äù in Proceedings of International
Conference on Software Engineering (ICSE) , 2013.
[16] F. Long and M. Rinard, ‚ÄúProphet: Automatic patch generation via
learning from successful patches,‚Äù 2015.
[17] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, ‚ÄúSemÔ¨Åx:
Program repair via semantic analysis,‚Äù in Proceedings of the 2013
International Conference on Software Engineering (ICSE) , 2013, pp.
772‚Äì781.
[18] S. Mechtaev, J. Yi, and A. Roychoudhury, ‚ÄúAngelix: Scalable multiline
program patch synthesis via symbolic analysis,‚Äù in Proceedings of the
38th international conference on software engineering . ACM, 2016,
pp. 691‚Äì701.
[19] J. Jiang, Y . Xiong, H. Zhang, Q. Gao, and X. Chen, ‚ÄúShaping program
repair space with existing patches and similar code,‚Äù in Proceedings of
the 27th ACM SIGSOFT International Symposium on Software Testing
and Analysis . ACM, 2018, pp. 298‚Äì309.
[20] S. Mechtaev, M.-D. Nguyen, Y . Noller, L. Grunske, and A. Roychoud-
hury, ‚ÄúSemantic program repair using a reference implementation,‚Äù in
Proceedings of the 40th International Conference on Software Engineer-ing, 2018, pp. 129‚Äì139.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. 