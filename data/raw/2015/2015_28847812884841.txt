Nomen est Omen: Exploring and Exploiting Similarities
between Argument and Parameter Names
Hui Liu1, Qiurong Liu1, Cristian-Alexandru Staicu2, Michael Pradel2, Y ue Luo1
1School of Computer Science and Technology, Beijing Institute of Technology, China
2Department of Computer Science, TU Darmstadt, Germany
{liuhui08,liuqiurong}@bit.edu.cn, cris.staicu@gmail.com, michael@binaervarianz.de, 102286165@qq.com
ABSTRACT
Programmer-provided identiﬁer names convey information
about the semantics of a program. This information cancomplement traditional program analyses in various soft-ware engineering tasks, such as bug ﬁnding, code comple-tion, and documentation. Even though identiﬁer names ap-pear to be a rich source of information, little is known abouttheir properties and their potential usefulness. This paper
presents an empirical study of the lexical similarity between
arguments and parameters of methods, which is one promi-nent situation where names can provide otherwise missinginformation. The study involves 60 real-world Java pro-grams. We ﬁnd that, for most arguments, the similarity
is either very high or very low, and that short and generic
names often cause low similarities. Furthermore, we showthat inferring a set of low-similarity parameter names fromonesetofprogramsallowsforpruningsuchnamesinanother
set of programs. Finally, the study shows that many argu-
ments are more similar to the corresponding parameter thanany alternative argument available in the call site’s scope.As applications of our ﬁndings, we present an anomaly de-tection technique that identiﬁes 144 renaming opportunities
and incorrect arguments in 14 programs, and a code recom-
mendation system that suggests correct arguments with aprecision of 83%.
CCS Concepts
•Software and its engineering →Software develop-
ment techniques; Maintaining software;
Keywords
Empirical study, name-based program analysis, identiﬁernames, static analysis, method arguments
1. INTRODUCTION
Identiﬁer names chosen by developers convey information
about the semantics of a program [21], but many program
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permissionand/or a fee. Request permissions from permissions@acm.org.
ICSE ’16, May 14 - 22, 2016, Austin, TX, USA
c/circlecopyrt 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.2884841analyses ignore identiﬁer names. As a result, running an
analysis on a human-written program with meaningful iden-tiﬁer names and on an equivalent program where all iden-tiﬁers are consistently replaced with arbitrary strings gives
exactly the same result. However, ignoring names discards
a valuable source of information that may provide hintsthat are otherwise unavailable to an analysis. To exploitthis information, recent work uses identiﬁer names to inferAPI speciﬁcations [33, 25], to identify mismatches between
a method name and the method’s implementation [16], to
synthesize code completions [29], to predict syntactic andsemantic properties of programs [28], to suggest identiﬁernames [2, 3], and to detect incorrectly ordered method ar-
guments of the same type [26, 27].
Despite these recent approaches, little is known about
the properties of identiﬁer names in real-world software and
about how one could exploit these properties. Are identiﬁernames that refer to semantically related values similar? Is
it possible to predict from identiﬁer names which variable,
ﬁeld, or method a developer will use next? How prevalentare names that convey little or no semantic information,such as generated variable names or very short names, and
is there a way to identify them? Addressing these questions
is valuable because it may pave the road for name-basedanalyses that complement traditional program analysis. Forexample, one could exploit the similarity of names to com-plete pieces of code automatically, to warn developers about
anomalies that may correspond to code worth changing, or
to infer documentation from names.
This paper focuses on the lexical similarity of arguments
and parameters of methods, which is one prominent situ-
ation where identiﬁer names may provide otherwise miss-
ing semantic links. We say argument to values passed to a
method at a call site, and we say parameter to the formal
parameter in the method’s deﬁnition. Since an argumentand its corresponding parameter often refer to the same in-
stance, we hypothesize that their names are often similar.
1.1 Research Questions
Toevaluatethishypothesis, weconductanempiricalstudy
that addresses the following research questions:
RQ1: How similar are argument names to the names of
their corresponding parameters? Answering this questionwill help to decide whether exploring similarities between
argument names and parameter names is worthy.
RQ2: Howlongareargumentnamesandparameternames
in real-world Java programs, and how does the length relate
to their similarity? Answering this question may help es-
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   1063
timate how much conﬁdence one can have into similarities
between names of particular lengths.
RQ3: How often does an overriding method change the
parameter names compared to the names in the overriddenmethod? Answering this question helps estimate how oftencomparing arguments to the parameter names of the stati-cally resolved call target is suﬃcient, and how often consid-
ering the dynamic call target would yield diﬀerent results.
RQ4: Why are some arguments dissimilar to correspond-
ing parameters? Answering this question my help applica-
tionsthatexploitnamesimilaritiestoignoreparticularkinds
of dissimilarities.
RQ5: If parameters with speciﬁc names, such as arg0,
are frequently assigned with dissimilar arguments in sample
applications, are parameters with the same name frequentlyassigned with dissimilar arguments in other applications? If
yes, can we build a set of low-similarity parameters that are
likely to be assigned with dissimilar arguments? Answering
thisquestionmayhelpapproachesthatexploitthesimilarityof names, such as name-based code completion or anomaly
detection, to reduce false positives by ignoring arguments
assigned to low-similarity parameters.
RQ6: How often is the argument chosen by the devel-
oper more similar to the corresponding parameter than anyof its potential alternatives? Answering this question will
help to estimate the accuracy of approaches that exploit the
similarity of names, such as code completion.
1.2 Summary of Findings
To address these questions, we empirically study 609,489
named arguments in 60 popular open-source Java programs.
The main ﬁndings of the study are the following. For RQ1,
we ﬁnd that the distribution of the lexical similarity is a U-shape: the similarity is either extremely high or extremelylow. Many arguments (31%) exactly match their corre-
spondingparameter. Incontrast, themajorityofthoseargu-
mentsthatarenotsimilartotheircorrespondingparametersare very dissimilar (51% have a similarity of 0%).
ForRQ2, weﬁndthat84%ofallargumentnamesand70%
of all parameter names have at least four characters. Al-
most all argument names (91%) and most parameter names
(81%) are composed of atmost three terms. On average overall studied parameters, the similarity to their arguments in-creases with the length of parameter names and with the
number of terms used in the parameter name. However,
there is no strong correlation between the length of namesandthesimilaritybetweenindividualpairsofargumentsandparameters. These results suggest that one cannot infer thesimilarity between an argument and a parameter from the
length of their names.
For RQ3, we ﬁnd that most of all overriding methods
(92%) have exactly the same list of parameter names as
their overridden method. That is, comparing arguments to
the parameters of a statically computed call target is suﬃ-
cient in most cases, even though a call may be dispatchedto a diﬀerent target method at runtime.
For RQ4, we ﬁnd that the main reason for dissimilar argu-
ments and parameters are short parameter names. Among
the 310,814 named arguments whose similarity with their
correspondingparameteriszero, 23%areassignedtoparam-eters named with a single character, and 42% of them areassignedtoparametersnamedwithnomorethanthreechar-
acters. Another reason for dissimilar pairs of arguments andparameters are generic data collection operations. Among
the 310,814 named arguments whose lexical similarity withtheir corresponding parameters is zero, 14% are assigned toparameters named index, item, key,o r value. Such names
are common in methods manipulating data collections.
For RQ5, we ﬁnd that most dissimilar pairs of argument
names and parameters (75%) are due to a small set of pa-rameters that occur again and again across programs, such
asarg0. That is, extracting a set of parameter names to
ignore from sample programs helps ﬁnding parameters that
are likely to be associated with dissimilar arguments in otherprograms.
Finally, forRQ6, weﬁndthatmostargumentnames(55%)
that are not associated with low-similarity parameters aremore similar to their corresponding parameter name thanany other argument that a programmer could use in thecurrent scope. The ﬁgure is even up to 78% for those ar-
guments whose lexical similarity with their correspondingparameters is at least 0.67. That is, analyzing the similarityof argument names and parameter names can help in de-
ciding which argument to use and in detecting incorrect orotherwise suspicious arguments.
1.3 Applications
Our results suggest several research directions for exploit-
ing identiﬁer names to support and further automate soft-
ware development tasks, and we explore two such directions.
First, we present a static anomaly detection technique thatwarns about argument names that seem not to match theircorresponding parameter name. The basic idea is to report
a warning when the developer could use another argument
than the current one, and when this change would make theargument name and parameter name signiﬁcantly more sim-ilar to each other. We apply the analysis to 10 programs,where it ﬁnds 6 known incorrect arguments, 3 previously un-
known incorrect arguments, and 127 renaming opportunities
with a precision of 80%. Second, we present a name-basedrecommendation system that suggests an argument while adeveloper writes code that calls a method. The basic idea
is to recommend an argument from a set of potential ar-
guments so that the recommended argument is the mostsimilar to the corresponding parameter. The approach rec-ommends correct arguments with a precision of 83%.
In summary, this paper contributes the following:
•The ﬁrst extensive empirical study of argument names
and parameter names in real-world Java programs.
•Empiricalevidenceshowingthatmostargumentnames
and parameter names are either similar to each otheror can be easily ﬁltered, and that names help in decid-ing which argument to assign to a parameter.
•Two practical applications of our ﬁndings, anomalydetection and argument recommendation, along withexperimental results that show that the applicationsare eﬀective.
2. SETUP OF THE STUDY
2.1 Methodology
To study the lexical similarity of identiﬁer names involved
in method calls, we compare named method arguments and
1064method parameters with a string similarity metric. The
ﬁrst step of the study is to extract identiﬁer names from
the source code of the subject applications. To this end, an
AST-based, staticanalysisextractsfromeachformalparam-eter in a method deﬁnition the identiﬁer of the parameter,called parameter name . Furthermore, the analysis extracts
at each call site the names of particular kinds of arguments,
called argument names . Speciﬁcally, the analysis considers
the following expressions that may be passed as arguments:
•For a variable, the argument name is simply the vari-
able name.
•For a call expression, the argument name is the name
of the called method. That is, we ignore the receiver
object and any arguments of the call. For example, ifthereturnvalueofamethodcall student.firstName()
is passed as an argument, then the argument name is
firstName .
•For a ﬁeld access expression, the argument name is the
name of the ﬁeld, again ignoring the receiver object.F o re x a m p l e ,i ft h ea r g u m e n ti sstudent.id , then the
argument name is id.
•For the thiskeyword, the argument name is the name
of the class of which thisis an instance. For example, if
in a method invocation print(this) the argument this
refers to an instance of class Student then the argu-
ment name is Student.
All other arguments, such as complex expressions, are ig-
nored in the study.
To compare argument names with parameter names, the
analysis matches each call site with a method deﬁnition.Thismatchingisbasedonthestaticallyknowntargetmethodto which the call resolves. As a result, each parameter nameis associated with a set of argument names.
Based on the extracted argument and parameter names,
we compute the similarity of two names as follows. Tomeasure the lexical similarity between names, we decom-pose each identiﬁer name into a sequence of terms, noted asterms(arg). The decomposition is based on underscores and
capital letters, assuming that the name follows the popularcamel case or snake case naming convention. We measurethelexical similarity between argument argand parameter
paras follows:
lexSim(arg,par)=
|comterms (arg,par)|+|comterms (par,arg)|
|terms(arg)|+|terms(par)|(1)
comterms (n1,n2)isthelongestsubsequenceof terms(n1),
where each term in the subsequence appears in terms(n2).
For example, lexSim(“length ”,“inputLength ”) =1+1
1+2=
67% and lexSim(“fieldLength” ,“fieldLength”) =2+2
2+2=
100%. The measure may assign a similarity of 1 to non-equal names, such as “ fooBar” and “barFoo”, which may
appear unintuitive but is not a problem in practice.
2.2 Subject Applications
We search for the most popular open-source Java applica-
tions from SourceForge, and select the top 60 resulting ap-plications for investigation. Sizes of such applications varyϬйϭϬйϮϬйϯϬйϰϬйϱϬйϲϬй
΀Ϭ͘Ϭ͕
Ϭ͘ϭͿ΀Ϭ͘ϭ͕
Ϭ͘ϮͿ΀Ϭ͘Ϯ͕
Ϭ͘ϯͿ΀Ϭ͘ϯ͕
Ϭ͘ϰͿ΀Ϭ͘ϰ͕
Ϭ͘ϱͿ΀Ϭ͘ϱ͕
Ϭ͘ϲͿ΀Ϭ͘ϲ͕
Ϭ͘ϳͿ΀Ϭ͘ϳ͕
Ϭ͘ϴͿ΀Ϭ͘ϴ͕
Ϭ͘ϵͿ΀Ϭ͘ϵ͕
ϭ͘ϬͿϭ^ŝŵŝůĂƌŝƚǇ
Figure 1: Distribution of lexical similarity between
arguments and parameters
from 2,893 to 570,384 non-blank lines of source code. In to-
tal, the subject applications are composed of 5,841,635 lines
of code.
From these subject applications, we extract all named ar-
guments. In total, we get 609,489 named arguments from
these applications. For each such argument, we compute the
lexicalsimilaritybetweenitanditscorrespondingparameteraccording to Formula 1.
3. RESULTS OF THE STUDY
3.1 RQ1: Distribution of Lexical Similarity
To address the question how similar argument and pa-
rameter names are, we compute the similarity between all
argument names and the names of their corresponding pa-
rameters. Figure 1 shows the distribution of the similarity.The ﬁgure shows that the distribution has a U-shape: sim-ilarity is either very high or very low. In 31% of all cases,the similarity is equal to 1, whereas in 51% of all cases, the
similarity is 0 (i.e., the names share no common terms).
From the ﬁgure, we also observe that the number of argu-
ments whose similarity with corresponding parameters be-
longs to [0.1,0.2), [0.2,0.3), [0.3,0.4), [0.7,0.8), and [0.9,1.0)
is extremely small. One of the reasons is that the similar-
ity is computed based on terms, and thus the similarity isdiscrete. If two names contain no common terms, the sim-ilarity is zero. Otherwise, the numerator of Formula 1 isat least 2. That is, to obtain a similarity smaller than 0.2,
the denominator must be greater than 2 /0.2 = 10. How-
ever, it is not so common that the length of two identiﬁer
names is longer than 10 terms. Consequently, the numberof arguments whose similarity with corresponding param-
eters belongs to [0.1,0.2) is extremely small. The same is
true for other intervals, e.g., [0.2,0.3), [0.3,0.4), [0.7,0.8),and [0.9,1.0).
We conclude from these results that the similarity of ar-
gument and parameter names is worth exploring further,
because a signiﬁcant part of all arguments is very similar to
its corresponding parameter.
3.2 RQ2: Length of Names
To address the question how long argument names and
parameter names in real-world Java programs are, we mea-sure for each name the number of characters and the num-
ber of terms in the name. Figure 2 shows the results. The
1065ϬйϮйϰйϲйϴйϭϬйϭϮй
ϭ
Ϯ
ϯ
ϰ
ϱ
ϲ
ϳ
ϴ
ϵ
ϭϬ
ϭϭ
ϭϮ
ϭϯ
ϭϰ
ϭϱ
ϭϲ
ϭϳ
ϭϴ
ϭϵ
ϮϬ
ϮϬн
>ĞŶŐƚŚŽĨEĂŵƐ;ŚĂƌĂĐƚĞƌƐͿϬйϭϬйϮϬйϯϬйϰϬйϱϬй
ϭϮϯ ϰ н
>ĞŶŐƚŚŽĨEĂŵĞƐ;dĞƌŵƐͿ
ϬйϱйϭϬйϭϱй
ϭ
Ϯϯϰ
ϱ
ϲϳ
ϴ
ϵ
ϭϬϭϭϭϮ
ϭϯ
ϭϰ
ϭϱ
ϭϲ
ϭϳ
ϭϴ
ϭϵ
ϮϬ
ϮϬн >ĞŶŐƚŚŽĨEĂŵĞƐ;ŚĂƌĂĐƚĞƌƐͿϬйϮϬйϰϬйϲϬйϴϬй
ϭϮϯϰ н
>ĞŶŐƚŚŽĨEĂŵĞƐ;dĞƌŵƐͿƌŐƵŵĞŶƚƐ͗
WĂƌĂŵĞƚĞƌƐ͗
Figure 2: Length of argument names and parameter
names.
Ϭ͘ϬϬ͘ϭϬ͘ϮϬ͘ϯϬ͘ϰϬ͘ϱϬ͘ϲ
ϭ ϯ ϱ ϳ ϵ ϭϭ ϭϯ ϭϱ ϭϳ ϭϵ ϮϬнǀĞƌĂŐĞ^ŝŵŝůĂƌŝƚǇ
>ĞŶŐƚŚŽĨEĂŵĞƐ;ŚĂƌĂĐƚĞƌƐͿϬ͘ϬϬ͘ϭϬ͘ϮϬ͘ϯϬ͘ϰϬ͘ϱϬ͘ϲ
ϭϮϯ ϰ нǀĞƌĂŐĞ^ŝŵŝůĂƌŝƚǇ
>ĞŶŐƚŚŽĨEĂŵĞƐ;dĞƌŵƐͿ
Ϭ͘ϬϬ͘ϭϬ͘ϮϬ͘ϯϬ͘ϰϬ͘ϱϬ͘ϲϬ͘ϳϬ͘ϴ
ϭ ϯ ϱ ϳ ϵ ϭϭ ϭϯ ϭϱ ϭϳ ϭϵ ϮϬнǀĞƌĂŐĞ^ŝŵŝůĂƌŝƚǇ
>ĞŶŐƚŚŽĨEĂŵĞƐ;ĐŚĂƌĂĐƚĞƌƐͿϬ͘ϬϬ͘ϭϬ͘ϮϬ͘ϯϬ͘ϰϬ͘ϱϬ͘ϲϬ͘ϳϬ͘ϴ
ϭϮϯϰ нǀĞƌĂŐĞ^ŝŵŝůĂƌŝƚǇ
>ĞŶŐƚŚŽĨEĂŵĞƐ;dĞƌŵƐͿƌŐƵŵĞŶƚƐ͗
WĂƌĂŵĞƚĞƌƐ͗
Figure 3: Correlation between length of names and
average similarity (with polynomial trendlines)
left-hand side of the ﬁgure shows that most names are com-
posed of no more than 10 characters. For example, 63%of the argument names contain 3 to 10 characters. 80% of
the parameter names contains no more than 8 characters.
The right-hand side of the ﬁgure shows that most names arecomposed of a small number of terms. 99% of the parame-ter names and 96% of the argument names are composed ofat most three terms. Single-term names account for 45% of
argument names and 73% of parameter names.
Theaveragelengthofargumentnames(1.7terms)ishigher
than that of parameter names (1.3 terms). One of the rea-
sons why argument names contain more terms are method
invocations whose return value is passed as an argument.
For a single-term parameter, e.g., rectangle, an argument
could be a method invocation, e.g., createRectangle().
Wealsoinvestigatewhetherthelengthofparameternames
or argument names inﬂuences the similarity between argu-
ments and parameters. The results are presented in Fig-
ure 3. The ﬁgure shows that the average similarity increaseswhen the length of parameter names increase. In contrast,the similarity decreases while the length of argument names
increases. Even though, on average over all studied argu-
ments and parameters, the length of names inﬂuences thesimilarity, the length of parameter names and argumentnames is only weakly correlated to similarity for individualpairs of arguments and parameters. The correlation coeﬃ-
cientis0.15(parameternames)and-0.18(argumentnames),
respectively. One of the reasons for this seemingly contra-dictory result is that even for the argument (or parameter)names of the same length, the similarity between arguments
and parameters may vary dramatically.
We conclude from these results that the length of parame-
ter names does inﬂuence the similarity between their names.
This ﬁnding suggests that developers should aim for expres-sive parameter names with multiple terms (if appropriate).
3.3 RQ3: Parameter Names of Overridden
Methods
For this study, we resolve invoked methods based on the
statically known type of the receiver object. However, staticanalysis may resolve method invocations incorrectly becausethe dynamic receiver type may be a subtype of the staticreceiver type that overrides the called method. To investi-
gate to what extent such mis-resolution may inﬂuence the
measured similarity between arguments and parameters, wemeasure how often overriding methods use diﬀerent param-eter names than the overridden method.
In total over all applications, we ﬁnd that most of the
overriding methods (92%) have exactly the same parameterlist (including the same parameter names) as the methodsthey override. In other words, in most cases the fact thatstatic method resolution may be incorrect does not inﬂuence
the measured similarity between arguments and parameters.
3.4 RQ4: Reasons for Dissimilarity
To address the question why some arguments are dissimi-
lar to their corresponding parameter, we further analyze all
pairs of argument name and parameter name that have asimilarity of 0. In total, there are 310,814 such pairs. We
randomly sample 200 of them and manually inspect them.
Moreover, we validate the results of the manual inspectionon the whole population of 310,814 pairs of names.
The manual analysis leads to two ﬁndings: First, very
short parameter names are the major reason for the dis-similarity between arguments an parameters. 21% of thearguments are assigned to parameters named with a singlecharacter, e.g., sand i, and 40.5% of the arguments are as-
signed to parameters named with at most three characters.
Such short parameter names often convey little semantics
and therefore cause dissimilarities with the correspondingargument names.
Second, generic parameter names of methods of collection
classesareanotherreasonforthedissimilaritybetweenargu-ments and parameters. 14% of the arguments are assignedto parameters named index, item, key,o r value. Such pa-
rameter names are popular in methods that manipulate datacollections. Although such parameter names are meaning-
ful, their corresponding arguments are usually dissimilar to
them because their arguments are concrete value or indexes.F o re x a m p l e ,a ni n v o c a t i o no fm e t h o dList.add(int in-dex, Object value) may be add(i, newElement).
Toinvestigatewhethertheanalysisresultsonthe200sam-
ple arguments can be generalized, we validate the results onthe entire population of argument names that have no simi-larity with the corresponding parameter name. The valida-tion results are as follows:
1066•71,499 (23%) of the 310,814 named arguments were
assigned to parameters named with a single character.
130,708 (42%) out of the 310,814 named arguments
were assigned to parameters named with no more than3 characters. These data are similar to the analysisresults on the sample.
•36,886 (12%) of the 310,814 named arguments wereassignedto index, item, key,o r value, whichissimilar
to the analysis results on the sample.
We conclude from the results that short parameter names
and generic names are main reasons for dissimilarities be-tween argument names and parameter names. This ﬁnd-ing can beneﬁt applications of name similarities, which may,
e.g., ignore such parameters.
3.5 RQ5: Filtering Parameters with Low
Similarity
The following addresses the question whether one can
build a set of low-similarity parameters from a corpus of
sample applications. We present a technique for computingsuch a set and assess the eﬀect of ﬁltering pairs of argument
names and low-similarity parameter names.
Given a corpus of sample applications, our approach for
identifying low-similarity parameters has three steps. First,
we cluster all argument names in the sample applications
by their corresponding parameter names. If the parameters
associated with two arguments have the same name, thenwe assign both arguments into the same cluster. That is,each cluster is associated with a unique parameter name.Second, for each cluster, we calculate the average similarity
sbetween arguments in this cluster and their correspond-
ing parameters. Finally, if the average similarity sof a
cluster is smaller than 0.5, we add the parameter name as-
sociated with this cluster to the set of low-similarity pa-
rameters. For example, for two calls m(a,x)and m(a,y)
of the method m(A a, B b), we extract two clusters C
a=
[a,a]a n dC b=[x,y]. The average similarities for Caand
Cbareavg(lexSim(“a”,“a”),lexSim (“a”,“a”)) = 1 and
avg(lexSim(“x”,“b”),lexSim (“y”,“b”)) = 0, respectively.
Hence, we add bto the set of low-similarity parameters.
To assess to what extent extracting low-similarity param-
eters from sample programs can help ﬁnd parameters that
are likely to be associated with dissimilar arguments in other
programs, we carry out a k-fold cross-validation on the 60
subject applications (k=6). The applications are randomlypartitioned into 6 groups, notated as Sag
i(i=1...6),
where each group is composed of 10 subject applications.For the ith cross-validation, we consider all subject appli-
cations except for those in Sag
ias the corpus of training
applications, and we consider the applications in Sag 1as
the validation applications.
After identifying low-similarity parameters from a corpus
of sample applications, we compute all argument names inthe remaining applications whose corresponding parametersare low-similarity parameters. We call such arguments ﬁl-
tered out arguments . Figure 4 shows the distribution of
the similarity between ﬁltered out arguments and their cor-
responding parameters. The ﬁgure shows that for most
(73% on average) of the ﬁltered out arguments the simi-larity between them and their corresponding parameters iszero. Note that this percentage is signiﬁcantly higher than
when considering all arguments (Figure 1). The observationTable 1: Inﬂuence of ignoring arguments associated
with low-similarity parameters.
Similarity Arguments ( n1)Filtered out
arguments (n 2)n2/n1
[0.0, 0.1) 310,814 233,311 75%
[0.1, 0.2) 4 0 0%
[0.2, 0.3) 1,059 300 28%
[0.3, 0.4) 2,481 527 21%
[0.4, 0.5) 8,799 1,919 22%
[0.5, 0.6) 30,707 9,821 32%
[0.6, 0.7) 53,023 15,947 30%
[0.7, 0.8) 344 4 1%
[0.8, 0.9) 10,827 730 7%
[0.9, 1.0) 55 0 0%
1 191,376 56,849 30%
Total 609,489 319,408 52%
ϬйϭϬйϮϬйϯϬйϰϬйϱϬйϲϬй
΀Ϭ͘Ϭ͕
Ϭ͘ϭͿ΀Ϭ͘ϭ͕
Ϭ͘ϮͿ΀Ϭ͘Ϯ͕
Ϭ͘ϯͿ΀Ϭ͘ϯ͕
Ϭ͘ϰͿ΀Ϭ͘ϰ͕
Ϭ͘ϱͿ΀Ϭ͘ϱ͕
Ϭ͘ϲͿ΀Ϭ͘ϲ͕
Ϭ͘ϳͿ΀Ϭ͘ϳ͕
Ϭ͘ϴͿ΀Ϭ͘ϴ͕
Ϭ͘ϵͿ΀Ϭ͘ϵ͕
ϭ͘ϬͿϭ
^ŝŵŝůĂƌŝƚǇ
Figure 5: Distribution of similarity parameters
and arguments that are not associated with low-
similarity parameters.
holds in all of the six rounds of cross-validations regardless
of the changes on both training data and validation data.
The above results show that low-similarity parameters are
an eﬀective means for ﬁltering arguments across applica-
tions. In the remainder if the paper, we use low-similarity
parameters computed from all subject applications. Table 1illustrates how ﬁltering out arguments associated with low-similarity parameters changes the distribution of similarity
between arguments and parameters. From the table, we ob-
servethatargumentsthatarelesssimilartotheirparametersare more likely to be ﬁltered out. For example, the ﬁlteringremoves75%ofallargumentswhosesimilaritywiththeirpa-rameters is zero, whereas the ﬁltering removes only 29% of
all arguments whose similarity with parameters is not zero.
Figure 5 shows the distribution of similarity between pa-
rameters and arguments that are not ﬁltered out arguments.
By comparing Figure 5 with Figure 1, we observe that the
distribution of lexical similarity between arguments and pa-
rameters has been reshaped dramatically by ﬁltering basedon low-similarity parameters: The ratio of arguments thatare dissimilar to their parameters decreases and the ratio ofarguments that are similar to their parameters increases.
Weconcludefromtheresultsthatcomputinglow-similarity
parameters from a corpus of sample applications is an eﬀec-tive means to predict whether a parameter will be dissimilarto its arguments. This ﬁnding enables approaches that ex-
ploit name similarities to improve their precision by ignoring
low-similarity parameters.
1067ϬйϮϬйϰϬйϲϬйϴϬй
΀Ϭ͘Ϭ͕
Ϭ͘ϭͿ΀Ϭ͘ϭ͕
Ϭ͘ϮͿ΀Ϭ͘Ϯ͕
Ϭ͘ϯͿ΀Ϭ͘ϯ͕
Ϭ͘ϰͿ΀Ϭ͘ϰ͕
Ϭ͘ϱͿ΀Ϭ͘ϱ͕
Ϭ͘ϲͿ΀Ϭ͘ϲ͕
Ϭ͘ϳͿ΀Ϭ͘ϳ͕
Ϭ͘ϴͿ΀Ϭ͘ϴ͕
Ϭ͘ϵͿ΀Ϭ͘ϵ͕
ϭ͘ϬͿϭϭƐƚƌŽƐƐͲsĂůŝĚĂƚŝŽŶ
ϬйϮϬйϰϬйϲϬйϴϬй
΀Ϭ͘Ϭ͕
Ϭ͘ϭͿ΀Ϭ͘ϭ͕
Ϭ͘ϮͿ΀Ϭ͘Ϯ͕
Ϭ͘ϯͿ΀Ϭ͘ϯ͕
Ϭ͘ϰͿ΀Ϭ͘ϰ͕
Ϭ͘ϱͿ΀Ϭ͘ϱ͕
Ϭ͘ϲͿ΀Ϭ͘ϲ͕
Ϭ͘ϳͿ΀Ϭ͘ϳ͕
Ϭ͘ϴͿ΀Ϭ͘ϴ͕
Ϭ͘ϵͿ΀Ϭ͘ϵ͕
ϭ͘ϬͿϭϮŶĚƌŽƐƐͲsĂůŝĚĂƚŝŽŶ
ϬйϮϬйϰϬйϲϬйϴϬйϭϬϬй
΀Ϭ͘Ϭ͕
Ϭ͘ϭͿ΀Ϭ͘ϭ͕
Ϭ͘ϮͿ΀Ϭ͘Ϯ͕
Ϭ͘ϯͿ΀Ϭ͘ϯ͕
Ϭ͘ϰͿ΀Ϭ͘ϰ͕
Ϭ͘ϱͿ΀Ϭ͘ϱ͕
Ϭ͘ϲͿ΀Ϭ͘ϲ͕
Ϭ͘ϳͿ΀Ϭ͘ϳ͕
Ϭ͘ϴͿ΀Ϭ͘ϴ͕
Ϭ͘ϵͿ΀Ϭ͘ϵ͕
ϭ͘ϬͿϭϯƌĚƌŽƐƐͲsĂůŝĚĂƚŝŽŶ
ϬйϮϬйϰϬйϲϬйϴϬй
΀Ϭ͘Ϭ͕
Ϭ͘ϭͿ΀Ϭ͘ϭ͕
Ϭ͘ϮͿ΀Ϭ͘Ϯ͕
Ϭ͘ϯͿ΀Ϭ͘ϯ͕
Ϭ͘ϰͿ΀Ϭ͘ϰ͕
Ϭ͘ϱͿ΀Ϭ͘ϱ͕
Ϭ͘ϲͿ΀Ϭ͘ϲ͕
Ϭ͘ϳͿ΀Ϭ͘ϳ͕
Ϭ͘ϴͿ΀Ϭ͘ϴ͕
Ϭ͘ϵͿ΀Ϭ͘ϵ͕
ϭ͘ϬͿϭϰƚŚƌŽƐƐͲsĂůŝĚĂƚŝŽŶ
ϬйϮϬйϰϬйϲϬйϴϬй
΀Ϭ͘Ϭ͕
Ϭ͘ϭͿ΀Ϭ͘ϭ͕
Ϭ͘ϮͿ΀Ϭ͘Ϯ͕
Ϭ͘ϯͿ΀Ϭ͘ϯ͕
Ϭ͘ϰͿ΀Ϭ͘ϰ͕
Ϭ͘ϱͿ΀Ϭ͘ϱ͕
Ϭ͘ϲͿ΀Ϭ͘ϲ͕
Ϭ͘ϳͿ΀Ϭ͘ϳ͕
Ϭ͘ϴͿ΀Ϭ͘ϴ͕
Ϭ͘ϵͿ΀Ϭ͘ϵ͕
ϭ͘ϬͿϭϱƚŚƌŽƐƐͲsĂůŝĚĂƚŝŽŶ
ϬйϮϬйϰϬйϲϬйϴϬй
΀Ϭ͘Ϭ͕
Ϭ͘ϭͿ΀Ϭ͘ϭ͕
Ϭ͘ϮͿ΀Ϭ͘Ϯ͕
Ϭ͘ϯͿ΀Ϭ͘ϯ͕
Ϭ͘ϰͿ΀Ϭ͘ϰ͕
Ϭ͘ϱͿ΀Ϭ͘ϱ͕
Ϭ͘ϲͿ΀Ϭ͘ϲ͕
Ϭ͘ϳͿ΀Ϭ͘ϳ͕
Ϭ͘ϴͿ΀Ϭ͘ϴ͕
Ϭ͘ϵͿ΀Ϭ͘ϵ͕
ϭ͘ϬͿϭϲƚŚƌŽƐƐͲsĂůŝĚĂƚŝŽŶ
Figure 4: Distribution of similarity between parameters and arguments associated with low-similarity pa-
rameters.
3.6 RQ6: Picking Among Alternative
Arguments
To address the question how similar the argument chosen
bythedeveloperistootheravailablearguments, wecompareeach argument against its potential alternatives. To thisend, wedeﬁnewhichargumentscouldbeusedbyadeveloper
and then compute which of these possible arguments is the
most similar to the parameter.
Deﬁnition 1. Potential alternatives of argument argare:
•Ifargis a local variable or a ﬁeld of the enclosing class,
all ﬁelds of the enclosing class and local variables thatare available at the location are in the set of potentialalternatives.
•Ifargis a ﬁeld access or a method invocation without
arguments, then all ﬁeld accesses and method invoca-tions without arguments that have the same receiverobject are potential alternatives.
•Ifargis a method invocation with arguments, method
invocationswiththesamereceiverobjectandthesame
arguments are in the set of potential alternatives.
For example, for an argument a.b.foo, the following are po-
tential alternatives: a.b.bar and a.b.getBar().L i k e w i s e ,
for a method invocation x.getSize(y), the method invoca-
tion x.length(y) is a potential alternative.
Replacing an argument with a potential alternative may
introduce syntactical errors or type errors. For example,
some alternatives may not be available in the current scope
(e.g., privateﬁelds), ortheirtypesmaybeincompatiblewith
the parameter’s type. We exclude such invalid potential al-ternatives from the comparison, and deﬁne alternative argu-
mentsas follows:
Deﬁnition 2. Analternative argument of argument argis
a potential alternative that does not introduce new syntac-tical or type errors while replacing arg.
Among the alternative arguments alt
args, we call argu-
ments that have the greatest lexical similarity with the cor-responding parameter paramost similar argument :
Deﬁnition 3. An alternative argument m
alt∈altargs
is amost similar argument if for any alternative argument
anyalt∈altargs, the following inequation holds:
lexSim(malt,par)≥lexSim(anyalt,par)To investigate how often the argument chosen by the devel-
oper is more similar to the corresponding parameter than
any of its potential alternatives, we compare each argumentname in the applications to the names of its alternative ar-guments (especially the most similar argument).
50% of the 609,489 studied arguments do not have any
alternative arguments, i.e., they are the only argument thatis available in the scope of the method call and that istype-compatible with the parameter. Among the 304,387arguments with alternatives, only a small number (13.9%=
42,339/304,387) have an alternative that is more similar to
the parameter than the current argument. In other words,86.1%ofallargumentsareamongthemostsimilarofallpos-sible arguments. For 27%=82,158/304,387 of the arguments
with an alternative, the current argument is even strictly
more similar to the parameter than any of its alternatives.This number increases to 78% for arguments whose similar-ity with the corresponding parameter is greater than 0.667.
We also analyze the impact of ﬁltering arguments asso-
ciated with low-similarity parameters (Section 3.6). Afterthe ﬁltering, we keep 253,928 arguments, of which 100,696(40%) have at least one alternative. Among these argu-ments, 55%= 55,885/100,696 are more similar to the corre-
sponding parameters than any of their alternatives. Com-
pared to the ratio without ﬁltering arguments (27%), theratio has increased by 104%=(55%-27%)/27%. One of thereasons for the increase is that most of the arguments whosesimilarity to their corresponding parameters is zero have
been ﬁltered out based on low-similarity parameters.
We conclude from these results that approaches that try
toinferthemostappropriateofallavailablearguments, such
ascodecompletionoranomalydetection, haveahighchance
tomakeaccuratesuggestions, inparticularwhenﬁlteringar-
guments based on low-similarity parameters and thresholdsin minimal similarity.
4. APPLICATIONS
The results from Section 3 suggest several applications
that exploit the similarities between arguments and param-eters. In this section, we explore two such applications.
10684.1 Anomaly Detection
We present a static analysis that detects anomalies. The
main idea is to report arguments and parameters where the
current argument is signiﬁcantly less similar to the param-eter than an alternative. The analysis helps developers in
two ways. First, it reveals call sites that accidentally pass
incorrect arguments . Based on the analysis, the developer
can ﬁx such bugs, possibly using the alternative argumentsuggested by the analysis. Second, the analysis reveals argu-mentsand parameters that are correctbut notappropriately
named, making the code unnecessarily hard to understand
and maintain. Developers should address such renaming
opportunities by choosing identiﬁer names that convey the
semantics of the value that the identiﬁer points to.
4.1.1 Approach and Implementation
Our approach for detecting incorrect arguments and re-
naming opportunities works as follows. For a given argu-mentcurArgthe analysis at ﬁrst checks the corresponding
parameter paragainst the set of low-similarity parameters
(Section 3.6). If paris in this set, which suggests that it is
oftenassociatedwithdissimilararguments, thentheanalysis
ignores the current argument and does not report any warn-
ing for it. Otherwise, the analysis computes the most similarpotential argument m
alt(Deﬁnition 3). If maltis diﬀer-
ent from curArgand if the diﬀerence is above a threshold,
i.e.,lexSim(malt,par)−lexSim(curArg,par )≥β,t h e n
the analysis reports a warning that suggests to replace thecurrent argument with m
alt, or to rename the argument or
the parameter.
We implement the approach as an Eclipse plug-in that can
be used in two ways. First, to check arguments incremen-tally and instantaneously, that is, whenever an argument isintroduced or modiﬁed. In this scenario, the plug-in identi-ﬁes and reports suspicious arguments immediately when the
developer introduces them and suggests to the developer an
alternative argument as a quick-ﬁx. Second, to check all ar-guments in a project at once. In this scenario, the plug-inchecks the whole application and reports all suspicious argu-
ments, along with the source code location of each problem
and suggestions for alternative arguments.
4.1.2 Calibration
The approach depends on a threshold βthat decides when
to present warnings to the developer. In the following, wepresent how we calibrate this threshold using three open-sourceprogramsthatarenotamongthesubjectapplications
of the study: Domination (version 1.1.1.5), Openbravo POS
(version 2.30.2), and Dom4j(version 1.6.1). The applica-
tions cover diﬀerent domains and are developed by diﬀerent
developers.
To choose a reasonable threshold, we conservatively set
the threshold to β=0.4, and apply the anomaly detection
to the three applications. We manually check every reportedwarning and classify as a true positive if it points to a valid
renamingopportunityortoanincorrectargument. Basedon
this classiﬁcation, we compute the precision of the anomaly
detection as follows: Precision =
Number of true positives
Number of reported warnings.
For the 41 reported warnings, the similarity between ar-
guments and parameters is discrete, and it is either 0.4, 0.5,
0.6, 0.667, or 1. We observe that the precision increases
whileβincreases from 0.4 to 0.667, and it decreases slightly1) Example from LWJGL (commit 029fa0e)
•Signature of called method:
void writeVersionFile(File file, float version)
•Incorrect method call:
writeVersionFile(dir, latestVersion);
•Fix applied in commit 3656b80:
writeVersionFile(versionFile, latestVersion);
2) Example from Mondrian (commit b583845)
•Signature of called method:
void putChildren(RolapMember member,
ArrayList children);
•Incorrect method call:
cache.putChildren(member, list);
•Fix applied in commit c26d9f2:
cache.putChildren(member, children);
Figure 6: Examples of incorrect arguments detectedby the anomaly detection.
after this point. Based on these results, we use β=0.667 in
the remaining experiments.
4.1.3 Evaluation
We evaluate the eﬀectiveness of the anomaly detection,
by manually identifying known problems related to incorrect
arguments in the history of the subject applications and bychecking whether the analysis detects these problems. Toidentify known problems, we use ChangeDistiller [15] to ex-
tract source code changes that aﬀect a single argument and
then manually ﬁlter those that replace an incorrect argu-ment with a correct argument. Our methodology ensuresthat each considered change is indeed a bug ﬁx. We man-
ually inspect the commit messages and the changed code,
and we keep only those changes that deﬁnitely ﬁx a bugcaused by using an incorrect argument. Most of the commitmessages of the selected changes are very explicit, e.g.,“codecleanup: wrong parameter was used”, “ﬁxed bug: upload-
rate is protocol+data”, or “Fix for bug #44277 - correctly
reference the crosstab id”. We consider all applications thathaveapubliclyaccessibleversioncontrolsystem(GIT,SVN,or CVS), which yields 51 of the 60 applications. In total, we
identify 14 incorrect arguments in 11 of these applications.
Figure 6 lists two example bugs. We then apply the anal-ysis to the buggy versions of the 11 applications, manuallyinspect all reported anomalies, and classify each of them asincorrect argument, renaming opportunity, or false positive.
We apply the anomaly detection to the 11 subject ap-
plications with known incorrect arguments (Table 2). Theapproach successfully detects 6 of the 14 known incorrect ar-guments. Besides such 6 incorrect arguments, the approach
also identiﬁes 3 incorrect arguments that have been missed
bythemanualidentiﬁcationbasedonChangeDistiller, show-ing that incorrect arguments are more frequent than ourChangeDistiller-based search suggests.
In addition to the 9 incorrect arguments, the analysis re-
ports127renamingopportunitiesand33falsepositives. Theaverage precision of the analysis, i.e., the sum of the numberof incorrect arguments and renaming opportunities dividedby the total number of reported anomalies, is 80%.
The detected renaming opportunities fall into four cate-
gories:
•Abbreviations (42/127=33%). For example, the ap-
proach warns about an argument cwhose correspond-
1069Table 2: Results of anomaly detection.
Application Size (LOC)Known
Incorrect
ArgumentsReported
WarningsIdentiﬁed
Incorrect
ArgumentsIdentiﬁed
Renaming
OpportunitiesFalse
PositivesPrecision
LWJGL 32,137 1 6 4 1 1 100%
Mondrian 50,003 1 4 1 2 1 75%
DavMail 5,088 1 1 1 0 0 100%
VASSAL Engine 98,024 1 16 1 10 5 69%
Vuze-Azureus (version 1.8) 112,619 1 17 0 14 3 82%
Vuze-Azureus (version 1.17) 108,043 1 14 1 10 3 73%
Vuze-Azureus (version 1.126) 187,607 1 12 0 10 2 83%
iText 86,952 1 54 0 43 11 80%
JabRef 59,273 1 5 0 5 0 100%
PyDeva1,281 1 1 1 0 0 100%
PyDevb29,300 1 3 0 2 1 67%
Subsonic 30,507 1 2 0 1 1 50%
JasperReports Library 249,185 1 33 0 29 4 88%
Sweet Home 3D 28,824 1 1 0 0 1 0%
Total 1,078,843 14 169 9 127 33 80%
aVersion e9325bcef1f1a911642b3a76cf5563753f512eaa .
bVersion 8fef8ba12fe8b5ﬀ0cd37c2bb6f5c4750c18a54c .
ing parameter is country, and about a name dsthat
stands for“data source”.
•Incomplete descriptions that are missing a noun (30/127
=24%). Identiﬁer names often consist of a noun com-
bined with some adjunct. The approach warns aboutseveral argument names where the noun is missing,such as missed, which should be renamed into missed-
Members (because the corresponding parameter is par-
entMembers), and to_connect, which should be re-
named into to_connect_address.
•Meaningless names (17/127=13%). The approach re-
ports argument names that reveal little or nothingabout the value that the identiﬁer refers to, such aslistand object.
•Inconsistent names (38/127=30%). The approach re-
ports argument names where multiple terms are usedto describe a single concept, such as fileandmodule,
orthickness and width.
Most of the renaming opportunities (94%) are associated
with arguments, i.e., leading to renamings of arguments,suggesting that the quality of parameter names is gener-ally higher than that of argument names. It is reasonablein that developers usually pay more attention to parameternames because methods are expected to be called later and
may be used by other developers. However, arguments are
often encapsulated and hidden within methods, and thus de-velopers rarely expect them to be read or modiﬁed by otherdevelopers.
Previousworkshowsthatmeaningfulidentiﬁernamescon-
tribute to code understandability [21], and we believe thatfollowingtherenamingsuggestionsoftheanalysiscangreatlyimprove the readability of the code.
There are two main reasons for false positives reported by
the approach. First, the analysis is unable to distinguishintended from unintended anomalies. For example, for thestatement bounds = new Rectangle(bounds.y, bounds.x,
bounds.height, bounds.width), theanalysissuggestsswap-
ping the last two arguments because their corresponding pa-
rameters are widthand height, respectively. However, thedeveloper intends to rotate the rectangle, i.e., the anomalyis intended. Second, the analysis currently fails to identifysimilarities that are obvious for a human but not for ourdeﬁnition of similarity, e.g., because the analysis does nottokenize names correctly or because it is unaware of irregu-
lar English plural forms. We believe that the second reason
for false positives can be addressed by more sophisticatedprocessing of names, such as Butler et al.’s method for tok-enizing identiﬁer names [7] or techniques borrowed from the
natural language processing community.
4.2 Recommendation of Arguments
As the second application of the ﬁndings presented in Sec-
tion 3, we present a name-based recommendation system
that suggests arguments to a developer. Such a system can,e.g., be used as part of the code completion algorithm ofan IDE, where it recommends an argument just when thedeveloper types a method call. The key idea is to pick from
the set of potential arguments the argument whose similar-
ity with the corresponding parameter is signiﬁcantly higherthan any of the alternatives.
4.2.1 Approach
Our approach recommends arguments as follows. First,
for a given argument slot, i.e., where an argument should beinserted, the approach retrieves its corresponding parame-
ter, noted as par. If the name of this parameter is one of the
low-similarity parameters, then the approach makes no rec-
ommendation for this argument. Otherwise, the approachcollects all potential arguments (noted as S
pot): local vari-
ables, parameters of the enclosing method, invocation on
methods of the enclosing class, and ﬁelds of the enclosing
class. It does not consider complex expressions or literals,likethis.getAuthor().getName() and 99, because consid-
ering such complex expressions or literals would make the
searchspaceforpotentialargumentsextremelylarge. Third,
it excludes elements from S
potthat are not type-compatible
with the parameter or are not available in the slot. Ex-cluding such elements guarantees that the recommendedargument will not introduce syntactic errors. Fourth, it
computes the similarity between the parameter name and
1070Table 3: Results of argument recommendation.
ApplicationSize
(LOC)Recommended
ArgumentsPrecision
Neuroph 11,377 326 80%
WURFL 10,252 343 87%
Json-lib 8,055 122 92%
Joda-Time 27,779 797 81%
Total 57,463 1,588 83%
the names of the collected potential arguments. Finally, if
there is an argument in Spotwhose similarity is signiﬁcantly
greater than others (i.e., the distance is no less than α), the
approach recommends this argument.
4.2.2 Calibration
To calibrate the threshold αand to evaluate the approach,
weruntherecommendationsystemforeachargumentslotina program, i.e., for each argument position of all call sites in
the program. For each slot, we compare the recommended
argument against the current one. If both arguments areidentical, the recommendation is considered to be correct.
Otherwise, the recommendation is considered to be incor-
rect. Based on these data, we compute the precision of the
approach as the number of correct recommendations dividedby the total number of recommendations.
We calibrate the threshold αon three open source appli-
cations: HtmlUnit, CKEditor ,a n d c3p0.W e o b s e r v e t h a t
whileαis smaller than 0.5, the precision increases quickly
with the increase of α. However, after that the precision in-
creases insigniﬁcantly with the increase of α. Consequently,
we useα=0.5 in the remaining experiments.
4.2.3 Evaluation
The approach is evaluated on four open source applica-
tions: Neuroph,WURFL ,Joda-Time ,a n d Json-lib.T h e
evaluation results are presented in Table 3. From the table,
we observe that the approach recommends 1,588 arguments
with a precision of 83%.
72% of the incorrect recommendations are associated with
arguments that are either complex expressions or literals,
which the approach cannot recommend. Consequently, if
the current argument is a complex expression or a literal,the approach fails to recommend the correct argument. 22%of the arguments in the subject applications are complexexpressions or literals. Another reason for incorrect recom-
mendations (7%) are typecasts, such as (Map) value.S i n c e
the approach recommends type-compatible arguments only,
it cannot succeed if the actual argument is a cast expres-sion. Excluding complex expressions, literals, and cast ex-
pressions, the precision of the recommendation is up to 96%.
Among the 1,588 recommended arguments, 1,135 (71%)
are associated with inter-class invocations, i.e., the invoca-
tions (and the arguments) are out of the documents wherethe invoked methods are declared. The average precision for
such inter-class recommendation is 84%. It is even slightly
greater than that for inner-class recommendation (averageprecision 80%). A possible reason is that many of the inter-class invocations are associated with APIs whose parameters
are often named more carefully.5. THREATS TO V ALIDITY
A threat to external validity is that conclusions drawn
from a set of Java applications might not hold for other ap-
plications. To reduce the threat, we select the most popu-lar open source applications from SourceForge, which yields60 applications from various application domains. Further-
more, for RQ5, we address this threat via k-fold cross-vali-
dation.
For RQ4, another threat to external validity is that we
manually analyze only 200 samples to investigate why some
arguments are dissimilar to their corresponding parameters.
To reduce this threat, we randomly select these 200 sample
arguments from the population and validate the analysisresults on the entire population.
The study results depend on the similarity measure that
compares argument names and parameter names. We haveexperimentswithseveralalternativesimilaritymeasures, butwe have not found major changes in the overall results ofthe study. For example, when replacing Formula 1 with thestring similarity-based metric from [27], the resulting accu-
racy in argument recommendation (82%) is almost identical
to the current one (83%). In future work, it would be inter-esting to try additional alternative measures of similarity.
Our evaluation of the eﬀectiveness of a name-based ano-
maly detection is subject to two threats to validity. First,our approach to manually identify known argument-relatedchanges in the history of applications may not yield a repre-sentative set of argument-related bugs. We carefully inspecteach of the changes that we consider as known bugs to en-
sure that they are indeed bugs, but we cannot ensure that
we consider all such bugs in the history of these applica-tions. Second, the classiﬁcation of anomalies into incorrectarguments, renaming opportunities, and false positives is, to
some degree, subjective. To reduce any potential bias, three
engineers inspect each warning and must reach a consensusabout its classiﬁcation.
6. RELATED WORK
Theimportanceofidentiﬁernameshasbeenvalidatedand
well recognized [6, 9, 10]. As suggested by Lawrie et al.[21],there are two main sources of domain information: identi-
ﬁer names and comments. Because many developers do not
write comments, identiﬁer names are critical for programcomprehension. A method in which names assist in reverseengineering is presented in [9]; it consist of extracting con-
ceptlatticesbyanalyzingtheidentiﬁersnamesinprograms.
1
Consistent naming is important and a number of approaches
havebeenproposedtokeepnamesconsistent[4, 17, 18]. Thebasic idea is that a single concept within the same applica-tion should be referred to by the same name [21, 12].
A number of approaches have been proposed to calculate
name similarity for identiﬁers. Cohen et al.[11] compare dif-ferent string metrics for matching names and records. Anumber of approaches to tokenize identiﬁers have been pro-
posed [13, 14]. Enslen et al. [13] propose an approach to
split identiﬁers into sequences of words by mining word fre-quencies in source code. Butler et al.[7] propose an approachto decompose identiﬁer names into meaningful words evenif these names do not follow the camel case naming conven-
tion. Taneja et al. [30] suggest to use a synonym database
to improve the accuracy of name comparison. These ap-
1Their paper has inspired the preﬁx of our title.
1071proaches might be used to split identiﬁers, and to facilitate
computationofhybridsimilarity[24]betweenidentiﬁers. In-
corporating such sophisticated approaches of computing the
similarity between arguments and parameters may increasethe similarities measured in our study.
Zhang et al. [32] propose an approach to recommend ar-
guments for API usage. They mine existing code bases and
build an argument usage database. For each API usage, the
approach retrieves similar usage instances from the databaseand recommends arguments by concretizing such instances.A diﬀerence between their work and this one is that their
approach depends on a large number of usage instances of
the APIs. For non-API method invocation or less popularAPIs, it is challenging to collect such instances.
An approach by Pradel and Gross [27, 26] relates to one
of the applications of our ﬁndings, anomaly detection. Theypresentanapproachtoidentifyproblemsrelatedtotheorderof equally typed arguments. For each call site, they reorderequally typed arguments. If the reordered arguments matchthe names used at other call sites signiﬁcantly better, they
report a warning. Our anomaly detection diﬀers from theirs
in the following two aspects. First, they identify problemsrelated to the order of equally typed arguments, while thiswork addresses arbitrary arguments. Second, they comparearguments (at a call site) with other arguments (at other
call sites of the same method). In contrast, we compare
arguments with corresponding parameters.
There are several approaches that identify renaming op-
portunities. The ﬁrst category of such approaches is to
identifyrenamingopportunitiesbycheckingidentiﬁernames
against predeﬁned rules. All names breaking such rules arepresented as potential renaming opportunities. Abebe etal. [1] introduce lexicon bad smells that indicate potentiallexicon construction problems. Caprile and Tonella [10] pro-
pose an approach to standardize program identiﬁer names.
First, it standardizes the lexicon (terms in identiﬁer names).Second, it standardizes the arrangement of terms within anidentiﬁer name. Other approaches to standardize identiﬁer
names have been conducted by Lawrie et al. [19, 20] and
Butler et al. [8]. Our work diﬀers from these approaches inthat it can identify renaming opportunities without requir-ing a set of predeﬁned naming conventions.
The second category of work on renaming opportunities
searchesforinconsistentnaming. DeissenboeckandPizka[12]propose a model-based approach for identifying inconsistentnaming. Based on maps between concepts and names, theirapproach can identify two categories of basic warnings. The
ﬁrst category of such warnings is given when two identiﬁers
have identical names but diﬀerent types. The second cate-gory of such warnings is given when an identiﬁer is declaredbut never referenced. The ﬁrst category of such warningsmight suggest renaming opportunities. Our approach dif-
fers from their work in that it focuses on arguments and pa-
rameters only (arguments and parameters often refer to thesame concept) and thus does not require the maps betweenconcepts and names. It is a great advantage because it is
diﬃcult to build the maps accurately, and it usually requires
domainexperts. ThiesandRoth[31]proposeanotherwaytoidentify inconsistent naming. They analyze variable assign-ments and identify variables that refer to the same objectand are used in the same way. They suggest such variables
to share the same name. Our approach diﬀers by focusing
on arguments and parameters instead of assignments.The third category of work on renaming opportunities
builds relationships between special terms and infers a setof rules, e.g., that a method that matches contain* should
return a boolean [16]. Methods that break such rules are
reported as renaming opportunities. Our approach diﬀers
from their work in that our approach identiﬁes renaming op-portunities on arguments and parameters whereas their ap-proach identiﬁes renaming opportunities on method names.
The fourth category of work on renaming opportunities
generalize renamings conducted in the rest of the program.Once a rename refactoring is conducted manually or withtool support, Liu et al. [22] recommend to rename closelyrelated software entities whose names are similar to that of
the renamed entity. The approach proposed in this paper
diﬀers from that in [22] in that it does not depend on con-ducted renamings.
Arnaoudovaetal.[5]analyzeandclassifyidentiﬁerrenam-
ings, e.g., by comparing the original and the new name using
an ontological database [23]. Future work may improve oursimilarity measure using such ontological databases. Sta-
tistical language models extracted from a corpus of codehave been used to automatically suggest identiﬁer names for
variable names [2], as well as method and class names [3].
JSNice [28] predict names of local variables in JavaScriptapplications.
7. CONCLUSIONS AND FUTURE WORK
This paper presents the ﬁrst in-depth empirical study of
similarities between the names of arguments and parame-ters of methods. Our results show that identiﬁer names ofarguments and parameters are often similar to each other,that dissimilar names can be ﬁltered based on low-similarityparameters inferred from a set of sample programs, and that
many arguments are the most similar to the corresponding
parameter of all possible arguments that are available in thescope of the method call. As applications of our ﬁndings, wepresent an anomaly detection technique that identiﬁes re-
naming opportunities and potentially incorrect arguments,
aswellasarecommendationsystemthatsuggestsargumentst oad e v e l o p e rw h oi st y p i n gam e t h o dc a l l .
The broader impact of our work is to show that identi-
ﬁer names are a rich source of information that can provide
otherwise missing information to program analyses. We ex-
pect our results to encourage future research on name-basedprogram analyses, which will complement existing programanalyses for several software engineering tasks. For example,
names may improve code completion algorithms, support
the generation of documentation, and support fault local-ization.
Acknowledgments
This research is supported by the National Natural ScienceFoundation of China (61272169, 61472034), the Program forNew Century Excellent Talents in University (NCET-13-
0041), the Beijing Higher Education Young Elite Teacher
Project (YETP1183), the German Federal Ministry of Ed-ucation and Research (EC SPRIDE and CRISP), and bythe German Research Foundation within the Emmy NoetherProject“ConcSys”
10728. REFERENCES
[1] S. Abebe, S. Haiduc, P. Tonella, and A. Marcus.
Lexicon bad smells in software. In WCRE,p a g e s
95–99, Oct 2009.
[2] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton.
Learning natural coding conventions. In FSE,p a g e s
281–293, 2014.
[3] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton.
Suggesting accurate method and class names. In
ESEC/FSE , pages 38–49, 2015.
[4] N. Anquetil and T. Lethbridge. Assessing the
relevance of identiﬁer names in a legacy softwaresystem. In CASCON , page 4, 1998.
[ 5 ]V .A r n a o u d o v a ,L .E s h k e v a r i ,M .P e n t a ,R .O l i v e t o ,
G. Antoniol, and Y.-G. Gueheneuc. Repent: Analyzingthe nature of identiﬁer renamings. IEEE Transactions
on Software Engineering, 40(5):502–532, May 2014.
[6] S. Butler, M. Wermelinger, Y. Yu, and H. Sharp.
Exploring the inﬂuence of identiﬁer names on codequality: An empirical study. In CSMR, pages 156–165,
2010.
[7] S. Butler, M. Wermelinger, Y. Yu, and H. Sharp.
Improving the tokenisation of identiﬁer names. InECOOP, pages 130–154. 2011.
[8] S. Butler, M. Wermelinger, Y. Yu, and H. Sharp.
Mining Java class naming conventions. In ICSM,
pages 93–102, Sept 2011.
[9] B. Caprile and P. Tonella. Nomen est omen:
Analyzing the language of function identiﬁers. InWCRE, pages 112–122, 1999.
[10] B. Caprile and P. Tonella. Restructuring program
identiﬁer names. In ICSM, pages 97–107, 2000.
[11] W. W. Cohen, P. D. Ravikumar, and S. E. Fienberg.
A comparison of string distance metrics forname-matching tasks. In Workshop on Information
Integration on the Web , pages 73–78, 2003.
[12] F. Deissenboeck and M. Pizka. Concise and consistent
naming. Software Quality Journal , 14(3):261–282,
2006.
[13] E. Enslen, E. Hill, L. Pollock, and K. Vijay-Shanker.
Mining source code to automatically split identiﬁersfor software analysis. In MSR, pages 71–80, 2009.
[14] H. Feild, D. Binkley, and D. Lawrie. An empirical
comparison of techniques for extracting conceptabbreviations from identiﬁers. In International
Conference on Software Engineering and Applications(SEA), pages 156–165, 2006.
[15] B. Fluri, M. W ¨ursch, M. Pinzger, and H. C. Gall.
Change distilling: Tree diﬀerencing for ﬁne-grainedsource code change extraction. IEEE Transactions on
Software Engineering , pages 725–743, 2007.
[16] E. W. Høst and B. M. Østvold. Debugging method
names. In ECOOP, pages 294–317, 2009.
[17] P. Jablonski and D. Hou. CReN: A tool for tracking
copy-and-paste code clones and renaming identiﬁersconsistently in the IDE. In Workshop on Eclipse
Technology eXchange , pages 16–20, 2007.
[18] P. Jablonski and D. Hou. Renaming parts of identiﬁers
consistently within code clones. In ICPC, pages 38–39,
2010.[19] D. Lawrie and D. Binkley. Expanding identiﬁers to
normalize source code vocabulary. In ICSM,p a g e s
113–122, 2011.
[20] D. Lawrie, D. Binkley, and C. Morrell. Normalizing
source code vocabulary. In WCRE, pages 3–12, 2010.
[21] D. Lawrie, C. Morrell, H. Feild, and D. Binkley.
What’s in a name? a study of identiﬁers. In ICPC,
pages 3–12, 2006.
[22] H. Liu, Q. Liu, Y. Liu, and Z. Wang. Identifying
renaming opportunities by expanding conductedrename refactorings. IEEE Transactions on Software
Engineering, (99):1–1, 2015.
[23] G. A. Miller. Wordnet: a lexical database for english.
Communications of the ACM , 38(11):39–41, 1995.
[24] A. Monge and C. Elkan. An eﬃcient
domain-independent algorithm for detectingapproximately duplicate database records. InWorkshop on data mining and knowledge discovery ,
pages 267–270, 1997.
[25] R. Pandita, X. Xiao, H. Zhong, T. Xie, S. Oney, and
A. Paradkar. Inferring method speciﬁcations fromnatural language API descriptions. In ICSE, pages
815–825, 2012.
[26] M. Pradel and T. R. Gross. Detecting anomalies in
the order of equally-typed method arguments. InISSTA, pages 232–242, 2011.
[27] M. Pradel and T. R. Gross. Name-based analysis of
equally typed method arguments. IEEE Transactions
on Software Engineering, 39(8):1127–1143, 2013.
[28] V. Raychev, M. Vechev, and A. Krause. Predicting
program properties from“big code”. In POPL,p a g e s
111–124, 2015.
[29] V. Raychev, M. Vechev, and E. Yahav. Code
completion with statistical language models. In PLDI,
pages 419–428, 2014.
[30] K. Taneja, D. Dig, and T. Xie. Automated detection
of API refactorings in libraries. In ASE,p a g e s
377–380, 2007.
[31] A. Thies and C. Roth. Recommending rename
refactorings. In Workshop on Recommendation
Systems for Software Engineering, pages 1–5, 2010.
[32] C. Zhang, J. Yang, Y. Zhang, J. Fan, X. Zhang,
J. Zhao, and P. Ou. Automatic parameterrecommendation for practical API usage. In ICSE,
pages 826–836, 2012.
[33] H. Zhong, L. Zhang, T. Xie, and H. Mei. Inferring
resource speciﬁcations from natural language API
documentation. In ASE, pages 307–318, 2009.
1073