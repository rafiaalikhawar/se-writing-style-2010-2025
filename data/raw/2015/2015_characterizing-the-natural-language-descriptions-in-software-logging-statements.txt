Characterizing the Natural Language Descriptions in Software
Logging Statements
Pinjia He
The Chinese University of Hong Kong
Hong Kong
Shenzhen Research Institute
The Chinese University of Hong Kong
Shenzhen, China
pinjiahe@gmail.comZhuangbin Chen
The Chinese University of Hong Kong
Hong Kong
Shenzhen Research Institute
The Chinese University of Hong Kong
Shenzhen, China
zbchen@cse.cuhk.edu.hk
Shilin He
The Chinese University of Hong Kong
Hong Kong
Shenzhen Research Institute
The Chinese University of Hong Kong
Shenzhen, China
slhe@cse.cuhk.edu.hkMichael R. Lyu
The Chinese University of Hong Kong
Hong Kong
Shenzhen Research Institute
The Chinese University of Hong Kong
Shenzhen, China
lyu@cse.cuhk.edu.hk
ABSTRACT
Loggingisacommonprogrammingpracticeofgreatimportance
in modern software development, because software logs have been
widely used in various software maintenance tasks. To provide
high-quality logs, developers need to design the description text in
loggingstatementscarefully.Inappropriatedescriptionswillslow
downorevenmisleadthemaintenanceprocess,suchaspostmortem
analysis.However,thereiscurrentlyalackofrigorousguideand
specifications on developer logging behaviors, which makes the
constructionofdescriptiontextinloggingstatementsachallenging
problem. To fill this significant gap, in this paper, we systemati-
cally study what developers log, with focus on the usage of natural
language descriptions in logging statements. We obtain 6 valuable
findingsbyconductingsourcecodeanalysison10Javaprojectsand
7C#projects,whichcontain28,532,975LOCand115,159logging
statementsintotal.Furthermore,ourstudydemonstratesthepoten-
tialofautomateddescriptiontextgenerationforloggingstatements
by obtaining up to 49.04 BLEU-4 score and 62.1 ROUGE-L scoreusing a simple information retrieval method. To facilitate future
research in this field, the datasets have been publicly released.
CCS CONCEPTS
•Softwareanditsengineering →Softwaredevelopmentmeth-
ods;•Computing methodologies →Natural language process-
ing;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238193KEYWORDS
Logging, natural language processing, empirical study
ACM Reference Format:
PinjiaHe,ZhuangbinChen,ShilinHe, andMichaelR.Lyu.2018.Character-
izingthe NaturalLanguage DescriptionsinSoftware LoggingStatements.
InProceedings of the 2018 33rd ACM/IEEE International Conference on Au-
tomated Software Engineering (ASE ’18), September 3–7, 2018, Montpellier,
France.ACM,Montpellier,France, 12pages.https://doi.org/10.1145/3238147.
3238193
1 INTRODUCTION
Loggingisacommonprogrammingpracticeofpracticalimportance
formodernsoftwaredevelopers.Developersmainlyconductlog-
gingbywritingloggingstatements(e.g., printf(), logging.info() )and
insertingtheminto thecodesnippets.Asin-house debuggingtools
(e.g.,debugger),alltoooften,areinapplicableinproductionsettings
[62],softwarelogshavebecometheprincipalsourceofinformation
when diagnosing a problem. Specifically, software logs have been
usedinvariousreliabilityenhancementtasks,includinganomaly
detection [ 18,57], fault diagnosis [ 55,63], program verification
[17,51], performance monitoring [ 27,43], etc. The performance of
thesetaskshighlydependsonthequalityofthecollectedlogs.Thus,
formodernsoftwaredevelopmentandmaintenance,appropriate
logging statements are of great importance.
Typically,aloggingstatementcontainsdescriptiontextandvari-
ables. Real-world examples of logging statements can be found
inFig.2.Descriptiontextdescribesthespecificsystemoperation
inruntime,whichisthemainfocusofthispaper,whilevariables
record necessary system status (e.g., IP address). Elaborate descrip-
tion text can accelerate reliability enhancement tasks by providing
better understanding of the system runtime information. On the
contrary,immaturedescriptiontext(e.g.,outdatedtext)slowsdown
the log analysis process or may even mislead the developers [13].
However,loggingisnoteasybecauseofthefollowingreasons.
First,there iscurrentlyalack ofrigorousspecificationon logging
178
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Pinjia He, Zhuangbin Chen, Shilin He, and Michael R. Lyu
practice. For example, Fu et al. [ 19] find that even in a leading soft-
ware company like Microsoft, it is difficult to find rigorous (i.e.,
thoroughandcomplete)specificationsfordeveloperstoguidetheir
logging behaviors. Moreover, with the popularity of source code
sharingplatforms(e.g.,Github),modernsoftwaremayconsistof
componentswrittenbymultipledevelopersallovertheworld.This
further increases the difficulty for a developer to align with the
project logging style. This problem is compounded by the fact that
logging statements update frequently in modern software devel-
opment (e.g., hundreds of new logging statements every month
[56]).Althoughtherearesomeexistingloggingframeworks(e.g.,
Microsoft’sULS[ 6]andApache’slog4net[ 1]),developersstillneed
tomaketheirownloggingdecisions[ 62].Thus,wheretologand
what to log have become crucial but challenging problems.
Recently,Cinqueetal.[ 15]summarizeempiricalrulesregarding
logging placement. Zhu et al. [ 62] propose LogAdvisor, which rec-
ommends whether developers should write a logging statement in
a code snippet or not. They focus on the "where to log" problem,
while do not consider "what to log". Yuan et al. [ 60] develop Lo-
gEnhancer,atoolthatcanenhanceloggingstatementsbyadding
informativevariables.However,theirmethodfocusesonexisting
statements and does not consider the description text.
Tofillinthissignificantgapof"whattolog",weconductthefirst
empiricalstudyonthecontextofloggingstatements,withfocus
ontheirnaturallanguagedescriptions.Specifically,wecollect10
ApacheJavaprojectsand7C#projects,whichcontain28,532,975
LOC and 115,159 logging statements in total. We first study the
purpose of logging by manually inspecting 383 logging statements
and their surrounding code snippets. Then, we study the repeti-
tiveusageofcertainn-grams(i.e.,asequenceof ntokens),which
wecalln-grampatterns,inthenaturallanguagedescriptions.For
simplicity, in the following, we use logging descriptions to repre-
sent the natural language description text in logging statements.
In particular, could we generate the logging descriptions based on
historicallogsusingn-gramlanguagemodel[ 5]?Areloggingde-
scriptions locally repetitive (e.g., in a source file)? Moreover, based
onthemanualinspectionexperienceandquantitativeevaluation,
wefurtherstudythepossibilityofautomatedloggingdescription
generation. In particular, is it potentially feasible to implement alogging description suggestion tool to assist developers in deter-mining what to log? By answering the above questions throughsystematic analysis, this investigation helps to characterize thecurrent usage of logging descriptions and serves as the first step
towards automated logging description generation.
The results of our study show that there are generally three cat-
egories of logging descriptions, including description for program
operation (37.34%), description for error condition (39.16%), and de-
scription for high-level code semantics (23.5%) (Finding 1). Besides,
compared with common English, the repetitiveness in loggingde-
scriptions can be better captured by statistical language models.
(Finding2).However,then-grampatternsindifferentprojectsvary
alot(Finding 3),whichiscausedbythelocalness[ 53]oflogging
descriptions. In particular, the n-gram patterns in logging descrip-
tions are endemic to one source file or frequently used in a fewsourcefiles(Findings 4
∼5).Inaddition,weevaluatethepotential
feasibility to automatically generate logging descriptions based on
historical logs. The high BLEU score [ 45] and ROUGE score [ 39]implythatautomatedloggingdescriptiongenerationisfeasibleand
deserves more future exploration (Finding 6).
In summary, our paper makes the following contributions:
•Thispaperconductsthefirstempiricalstudyontheusage
ofnatural languagein loggingpractice byan evaluationof
10 Java projects and 7 C# projects.
•It summarizes three categories of logging descriptions inlogging statements, including description for program op-
eration,descriptionforerrorcondition,anddescriptionfor
high-level code semantics, covering all the scenarios ob-
served in our study.
•Wedemonstratetherepetitivenessinloggingdescriptions
globally (i.e., in a project) and locally (i.e., in a source file),and further present the potential feasibility of automated
logging description generation.
•The datasets studiedhave been publicly released [ 3], allow-
ingeasyusebypractitionersandresearchersforfuturestudy.
Theremainderofthispaperisorganizedasfollows.Section 2ex-
plainsthemethodologyinourstudy.Section 3summarizeslogging
descriptionsintodifferentcategories.Section 4characterizesthe
natural language descriptions by quantitative analysis. Based on
theevaluationresults,thepotentialfeasibilityofautomatedloggingdescriptiongenerationisassessedinSection 5.Wediscusspotential
directions to improve logging in Section 6. Section 7introduces
related work. Finally, we conclude this paper in Section 8.
2 STUDY METHODOLOGY
Inthispaper,westudytheloggingstatementsof10Javaprojects
and7C#projects,whichcontain28,532,975LOCand115,159log-
gingstatementsintotal.Inparticular,theJavaprojectsarecollected
from Apache Project List [ 2] with more than 15 committers, while
theC#projectsarewidely-usedsoftwarewithmorethan1,000stars
on Github.
Table1presents the details of these open-source projects. These
projects are of great variety, ranging from distributed system, data-
base,enterpriseservicebus,SDK,toIDE. Description presentsabrief
introduction of the project. Versionshows the date of the last com-
mittothemasterbranchintherepositorywhenwedownloadedtheproject source code. LOCindicates thecorresponding project’sline
ofcode. # of Logging Statements isthenumberofloggingstatements
in the project. # of Logging Descriptions is the number of logging
statements that have natural language descriptions. Tokens in Logs
counts the total tokens extracted from the logging descriptions;
while Distinct Tokens counts the distinct tokens.
Wecanobservethatamajorityoftheloggingstatementscontain
logging descriptions. In particular, 81 .9% of the logging statements
in ActiveMQ have logging descriptions; while the average per-centage of all projects is 69
.8%. Thus, we can find that logging
descriptions are important and widely adopted by developers.
The lengths of the logging descriptions in all of the projects are
illustratedinFig. 1.Inthispaper,bylength,wemeanthenumber
oftokensinaloggingdescription.Thelengthsofmorethan90%of
theloggingdescriptionsareinrange[1,10].Differentfromother
textinsoftwareengineering,suchascodeordocumentation,the
lengthofloggingdescriptionisshorter.Thus,weexpectdifferent
characteristicsofthenaturallanguageusedinloggingdescriptions.
179
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Characterizing the Natural Language Descriptions in Software Logging Statements ASE ’18, September 3–7, 2018, Montpellier, France
Table 1: Java and C# project details.
# of Logging # of Logging
Description Version Statements Descriptions Total Distinct
ActiveMQ Message Broker 20180212 689.9K 6.8K 5.6K (81.9%) 23,659 2,200
Ambari Hadoop Monitor 20180220 798K 5K 3.7K (75.2%) 22,590 2,104
Brooklyn Distributed System Manager 20170901 483.5K 4.5K 3.6K (79.7%) 24,255 2,458
Camel Integration Framework 20180126 1.8M 11.7K 8.8K (75.5%) 45,288 3,521
CloudStack Cloud Computing Software 20180131 838.8K 12K 9.8K (81.6%) 67,753 3,579
Hadoop Distributed Computing Platform 20171214 1.9M 13.9K 11.0K (79.4%) 59,825 4,669
HBase Distributed Database 20170804 957K 8.3K 6.6K (79.8%) 36,782 2,969
Hive Data Warehouse 20170812 1.5M 7.3K 5.9K (81.2%) 32,756 3,151
Ignite Distributed Database 20171031 1.6M 4.6K 3.5K (77.3%) 18,476 1,737
Synapse Enterprise Service Bus 20171204 586.3K 8.1K 5.1K (64.0%) 31,489 1,436
# of Logging # of Logging
Description Version Statements Descriptions Total Distinct
Azure SDK Azure Tools for Visual Studio 20170301 2.1M 786 529 (67.3%) 1,804 474
CoreRT .NET Core Runtime 20180208 537.6K 677 413 (61.0%) 1,364 419
CoreFX .NET Core Foundational Lib. 20180208 3.6M 7.5K 4.7K (63.2%) 21,219 2,009
Mono .NET Framework 20180116 7.5M 14.1K 7.9K (56.4%) 26,457 3,681
Monodevelop Cross Platform IDE 20180103 2.2M 6.6K 3.0K (46.3) 13,540 2,067
Orleans Distributed Virtual Actor Model 20180228 242.6K 761 484 (66.2%) 2,342 529
Sharpdevelop Cross Platform IDE 20171221 701.5K 2.1K 1.1K (51.8%) 4,168 1,190Tokens in Logs
C# Project LOCTokens in LogsJava Project LOC
0 2 4 6 8 10 12 14 16 18 >=20
Number of Tokens0.00.20.40.60.81.01.2Number of Logging Descriptions (1e4)Distribution of Logging Statements
# of Logging Statements
0.20.40.60.81.0
Cumulative Distribution of Logging Descriptions(10.0, 0.916)
Figure1:Distributionsofthelengthofloggingdescriptions.
Theseprojectsaresupposedtohavegoodloggingpractice,es-
pecially appropriate logging descriptions, for the following two
reasons. First, these projects are maintained by developers fromlarge organizations or companies, which provide important soft-
waresolutionspoweringmodernITindustry.Forexample,Hadoop
is used by a large amount of companies to process large-scale data.
Second,theseprojectshavebeenservingusersgloballyforalong
time, and thus their logging statements have fulfilled the use of
daily development and maintenance.
We characterize the logging descriptions by both manual in-
spection and these evaluation metrics. To study the purpose of
logging descriptions, we first randomly sample a subset of logging
descriptionsandtheircorrespondingcodesnippets.Aftermanually
exploring these samples in detail, we categorize logging descrip-tions into 3 groups. Then, to further study the characteristics ofthem, we evaluate the global and local repetitiveness in loggingTable 2: Categories from 383 sampled logging descriptions.
Samples
Completed 59/383 15.40%
Current 18/383 4.70%
Next 66/383 17.23%
Exception 96/383 25.07%
Value-Check 54/383 14.10%
Variable 40/383 10.44%
Function 15/383 3.92%
Branch 35/383 9.14%Categories
Program 
Opeartion% of Samples
Error 
Message
Semantic 
Description37.34%
39.16%
23.50%
descriptions by the evaluation metrics proposed in previous empir-
ical studies on source code [ 28,53]. In all the experiments using
evaluationmetrics,weregardtheloggingdescriptionsasplaintext
and study them from the natural language perspective.
3 CATEGORIES OF LOGGING DESCRIPTIONS
Tocharacterizetheusageofnaturallanguageinloggingdescrip-
tions,thefirststepistounderstandthepurposeofthesedescrip-
tions. Thus, in this section, we manually inspect the logging de-
scriptions and summarize them into different categories.
Since the total number of logging descriptions is large, it is pro-
hibitivetomanuallyinspectalltheloggingdescriptionsandanalyze
the corresponding code snippets. Thus, we randomly sample a sub-
setofloggingdescriptionstogetherwiththecorrespondingcode
snippets from all the projects in Table 1. Similar to existing studies
[20,21],wecalculatethenumberofsamplesbystandardtechniques
[35].Specifically,thenumberofsamplesisdeterminedbyadesired
margin of error, confidence level, and the data size. There are to-
tally 82,476 logging descriptions in our study. Thus, we determine
180
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Pinjia He, Zhuangbin Chen, Shilin He, and Michael R. Lyu
/*  Example 7: Function Description */
public void forget(Xid xid) throws XAException  {
        LOG.debug("Forget: {}" , xid);
... 
}/*  Example 1: Completed Operation */
final lbmonitor monitorObj  = lbmonitor .get(_netscalerService,  
nsMonitorName);
monitorObj.set_respcode( null);
lbmonitor .delete(_netscalerService, monitorObj);
s_logger .info("Successfully deleted monitor : "  + nsMonitorName);
/*  Example 3: Next Operation */
LOG.info("Close consumer on A" );
clientA.close();
/*  Example 4: Exception */
try {
      count = c.readAndProcess();
    } catch (InterruptedException ieo) {
       LOG.info(Thread.currentThread().getName() + ": readAndProcess 
caught InterruptedException" , ieo);
       throw ieo;
}
/*  Example 5: Value-Check */
if (jobId == null) {
   s_logger .error("Unable to get a jobId" );
   return null ;
}
/*  Example 6: Variable Description */
String messageID = ( String) element.get(" JMSMessageID" );
LOG.debug("MessageID: {}" , messageID);/*  Example 2: Current Operation */
while (nm.getServiceState() != STATE.STOPPED  && waitCount++ != 20) {
      LOG.info("Waiting for NM to stop.." );
      Thread.sleep(1000);
}
/*  Example  8: Branch Description */
if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof 
CloudPageBlobWrapper ) {
   LOG.debug("Found blob as a directory- using this file under it to     
infer its properties {}" , blobItem.getUri());
...  
}
Figure 2: Real-world examples of logging statements.
the sample size as 383 after setting ±5% margin of error and 95%
confidence.
To figure out the purpose of logging descriptions, we assign
eachloggingstatementanditssurroundingcodesnippettothree
researcherswith6years’programmingexperienceinaverage.Each
researcher labels the category of the logging description after man-
ual analysis. Then we compare the labels returned by different
researchers. If all the labels for a logging description are the same,
weregarditasthefinallabel.Otherwise,theresearchersre-visit
thecasetogetherandproducethefinallabelafterdiscussion.Table
2shows the details of the categories and the number of samples
in each category. In particular, we summarize 3 main categories,
underwhichtherearetotally8mutuallyexclusivesubcategories.
Inaddition,weprovideaselectedexamplefromthestudiedproject
foreachsubcategoryinFigure 2.Thedetailsofthesecategoriesare
introducedasfollows.Inthisstudy,westartwith7subcategories
based on previous experience and preliminary inspection on the
logging statements. Afterexploring all the 383samples, we find a
newsubcategory(i.e.,branchdescription)andfinallysummarize
with 8 subcategories in this paper.
Category1:ProgramOperation. Loggingdescriptionsinthis
categorysummarizethedetailedactionsorintentionsofthesur-
rounding program. Based on the position of the described programstatements, this category can be further divided into three subcate-
gories, including completed operation, current operation, and next
operation.Inparticular,a completed operation loggingdescription
concludes the behavior of program statements preceding a logging
statement. This kind of logging statements are often placed at the
end of a function/block scope. Example 1 shows a completed op-
eration that deletes a monitor successfully. In current operation,
developerslogthecurrentstatusofaprogramtotracetheprogress
of an action. Typical usage of such description is in a whileloop or
aforloop,asillustratedinExample2.Comparedwithaforemen-
tioned two types, next operation is more widely utilized to forecast
the following behaviors of a program, which often indicates thestart of some operations. In Example 3, developers log the next
operationofclosingtheconsumeronclientA.Inourstudy,more
than 37% of samples belong to the program operation category.
Category 2:Error Message. It is a common practice for devel-
opers to log the error message for maintenance. In this category,
loggingdescriptions mainlypresent theoccurrenceor thebehind-
reasonsofanerror/exception.Therearetwotypesofloggingde-
scriptioninthiscategory: exception andvalue-check. Exception uses
thetry-catch blockandtheloggingdescriptionisoftenwrittenin
thecatchclause. Some error-related keywords (e.g., failed, error,
exception)arefrequentlyemployedandoftenindicateafailedex-
ecution in the try block. Example 4 is a representative example.
Value-check logging descriptions explain errors without explicitly
employingthe try-catch block.Instead,an if-statement isusually
applied tocheck thevalue of avariable incurrentprogram orthe
returnvalueof aspecificfunction.For instance,Example5shows
the value checking of a variable against null. Other values such as
false, emptyarealsowidelycheckedandthecorrespondingerror
messages are then logged. Most samples (around 40%) in our study
are categorized into the error message category.
Category3:SemanticDescription. Inthiscategory,thereare
three subcategories of logging descriptions according to the ob-
ject they describe, i.e., variable description, function description and
branch description . In particular, variable description records the
value of a pivotal variable during execution. As shown in Example
6,thedetailedvalueofmessageIDislogged. Function description is
widelyutilizedtodepictthefunctionalityandusage ofafunction
anditsarguments.Generally, function description isplacedatthe
beginningofafunctionbody,asillustratedinExample7. Branch
description describesthesemanticmeaningofabranch/path.Differ-
ent from program operation and error message, branch description
is mainly used in if-elseblocks to indicate the execution path in
software runtime. Example 8 presents a logging description that
capturesthesemanticmeaningofthisbranch.23.50%ofourstudied
samples are in the semantic description category.
Finding 1: There are three main categories of logging
descriptions,i.e.,descriptionforprogramoperation,description
for error condition, and description for high-level codesemantics.
4 LANGUAGE PATTERNS OF LOGGING
DESCRIPTIONS
In thissection, we try toanswer the followingresearch questions:
•RQ1: Is there any repetitiveness in logging descriptions?
181
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Characterizing the Natural Language Descriptions in Software Logging Statements ASE ’18, September 3–7, 2018, Montpellier, France
•RQ2: Can the repetitiveness be captured by cross-project
n-gram models?
•RQ3: Are there any n-grams in logging descriptions appear-
ing in only one source file?
•RQ4: Are n-grams in logging descriptions locally specific to
a few source files?
By answering these research questions with quantitative analy-
sis of the open-source projects, we aim to facilitate better under-
standing of the current logging description usage by developers. In
addition, the answers to these questions demonstrate the potential
of automated logging description generation.
4.1 RQ1: Is There Any Repetitiveness in
Logging Descriptions?
Therepetitivenessofn-gramsincommonEnglishhasalreadybeen
successfully used in various tasks, such asspeechr ecognition, ma-
chine translation, and code completion. Thus, it would be of great
interest to explore whether logging descriptions are also repetitive
andpredictable forpotential applications(e.g., automateddescrip-
tion generation). To answer this question, in this section, we try
tousen-gramlanguagemodelstopredictthenexttokeninalog-
ging description. Intuitively, if there is observable repetitiveness
in logging descriptions, the models should have decent prediction
performance.Inthefollowing,wefirstintroducethen-gramlan-
guage model and the evaluation metric. After that, we analyze the
experimental results and summarize with a finding.
N-gramlanguagemodel. Languagemodelsarestatisticalmod-
elsthatpredicttheprobabilityofsequencesofwords.N-grammod-
elsassumea Markov property,i.e.,theprobabilityofatokenonly
dependsonthepreceding n−1tokens.Forexample,for4-grammod-
els,theprobability pofatokeniscalculatedbasedonthefrequency
counting of the previous 3 tokens, as the following:
p(a4|a1a2a3)=count (a1a2a3a4)
count (a1a2a3∗). (1)
Ifthereisobservablerepetitivenessinloggingdescriptions,ann-
grammodelMshouldbeabletolearntheprobabilitydistributions
of n-grams from a corpus. In practice, the n-gram model often
encounters some unseen n-grams during prediction. This makesthe probability
pM(ai|a1...ai−1)=0. Smoothing is a technique
that can handle such cases, and assign reasonable probability tothe unseen n-grams. In this paper, we use Modified Kneser-Ney
Smoothing [ 34], which is a standard smoothing technique and can
give good results for software corpora [28].
Evaluationmetric:cross-entropy. Weuse10-foldcrossvali-
dation, where 90% of the logging descriptions are the training data,
and the remaining 10% are the testing data. To evaluate the perfor-
manceofthen-grammodels,weuse cross-entropy,whichisdefined
as follows:
HM(s)=−1
nn/summationdisplay
i=1logpM(ai|a1...ai−1) (2)
HMis the cross-entropy of the n-gram model M;sis a logging
description of length nands=a1...an; andpMis the probabil-
ity that the next token is aigiven the preceding token sequenceTable 3: English corpora.
Version Total Distinct
Brown 20171201 56,832 1,023,161 53,090
Gutenberg 20171201 98,326 2,136,001 53,253Tokens
English Corpus Lines
a1...ai−1. A good model has low cross-entropy for logging descrip-
tions.Forexample,ifamodelcancorrectlypredictallthetokensinaloggingdescription,theprobability
pM(ai|a1...ai−1)willbe1for
alltokens ai,andhencethecross-entropywillbe0.Wecalculatethe
cross-entropy for projects in Table 1. Specifically, for each project,
we calculate the cross-entropy for every logging description using
Equ.2and use their average as the cross-entropy for the project.
Results. Thecross-entropyresultsofJavaandC#projectsare
demonstrated by the boxplots in Fig. 3. In this experiment, we
evaluatetheprojectswithmorethan1,000loggingdescriptions.To
figureoutthepotentialdifferencesbetweenloggingdescriptions
andcommonEnglish,wealsocalculatethecross-entropyof twocommon English corpora, which is demonstrated by the singlelines in Fig. 3. The details of these two corpora are illustrated in
Table3.Bydoingso,wecanhaveanintuitiveunderstandingofthe
repetitivenessofnaturallanguageinloggingdescriptionsandincommon English. We analyze each project separately (boxplots),
while we analyze the two English corpora as a whole (a line).
We can observe that both the single lines and boxplots have
similartrends.Thesinglelinesstartat11for1-grammodelsand
traildowntoabout8.5for8-grammodels.Thismeansthatforboth
loggingdescriptionsandcommonEnglish,usingmorepreceding
tokens (i.e., larger n) can lead to more accurate results. Besides, ac-
cordingtothefigures,cross-entropysaturatesaround3-or4-grams.
Thus, 3- or 4-gram models are the best choice for the investigated
projectsconsideringthetrade-offbetweencross-entropyandmodel
complexity.
In addition, compared with common English, the cross-entropy
oftheloggingdescriptionsisgenerallysmaller,whichmeansthe
tokens in logging descriptions are easier to predict. This phenome-
nonismoreobviousforloggingdescriptionsinJavaprojects.We
think this is mainly because all the Java projects are collected from
ApacheProjectList,whichsharesimilarloggingstyles.Theobserv-
able repetitiveness in logging descriptions is encouraging, and it is
promisingtoutilizetherepetitivenessforautomatedlogginginthe
future.
Finding 2: Compared with common English, the repetitiveness
of logging descriptions can be better captured by statisticallanguage models.
4.2 RQ2: Can the Repetitiveness be Captured
by Cross-project N-gram Models?
Inthissection,wefurtherstudytherepetitiven-grampatternsin
loggingdescriptions.Specifically,doesrepetitivenessexistacross
different projects, or does it lie in individual projects locally? Toanswer this question, we conduct the cross-project experiments.For each Java project, we train a 3-gram model based on 90% of
the project logging descriptions. Then, we use the model to predict
182
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Pinjia He, Zhuangbin Chen, Shilin He, and Michael R. Lyu
1 2 3 4 5 6 7 8
Order of N-grams3456789101112Cross Entropy (with 10-Fold Cross Validation)English Cross-entropy
Java Projects Cross-entropy
(a) Java projects
1 2 3 4 5 6 7 8
Order of N-grams3456789101112Cross Entropy (with 10-Fold Cross Validation)English Cross-entropy
C Sharp Projects Cross-entropy
(b) C# projects
Figure 3: Cross-entropy of natural language descriptions in
logging statements.
tokens in (1) the remaining 10% of the logging descriptions and (2)
the logging descriptions of all other 9 Java projects. We chose 3-
gram models because they do not require too much memory while
achievingdecentperformance.WemainlyfocusonJavaprojects,
becausetheyareallApacheJavaprojectswhichmaysharemore
cross-project similarities than C# projects.
TheresultsareshowninFig. 4.Thex-axislistsalltheprojects
that are used to train the n-gram models. The single line illustrates
the cross-entropy of the models on the remaining 10% logging
descriptions(in-project),whiletheboxplotshowsthecross-entropy
of the models on all other projects (cross-project). We can observe
that the cross-project cross-entropy is clearly larger than the in-project cross-entropy. This indicates that the repetitive n-grampatterns in different projects are quite different, and thus the n-
gram patterns can hardly be captured by cross-project models. The
in-projectcross-entropyofprojectSynapseisverylowbecauseit
contains many identical logging descriptions.
Finding 3: N-gram models trained on other projects cannot
capture repetitive n-gram patterns well, indicating that then-gram patterns in different projects vary a lot.hadoopcloudstackcamel hbase synapse activemq ambari brooklynignite hive
Corpus Projects3456789101112Cross Entropy (with 10-Fold Cross Validation)
In-Project
Cross-Projects
Figure4:Cross-projectcross-entropyversusin-projectcross-
entropy of the 10 Java projects.
Table 4: Percentage of the endemic n-grams.
Lang. Freq. 1-gram 2-gram 3-gram 4-gram
>=1 2.83% 28.86% 57.62% 68.89%
Java Log >=2 1.32% 10.01% 16.86% 17.37%
>=1 2.84% 15.82% 31.54% 40.61%
Java Code >=2 2.54% 7.25% 12.05% 14.03%
>=1 8.85% 47.91% 67.23% 74.81%
C# Log >=2 4.87% 21.02% 25.28% 25.28%
>=1 2.71% 15.48% 31.52% 42.24%
C# Code >=2 2.43% 8.17% 13.87% 16.97%
4.3 RQ3: Are There Any N-grams in Logging
DescriptionsAppearinginOnlyOneSource
File?
Resultsintheprevioussectionindicatethatn-grampatternstendto
appearlocallyinsidetheproject.Tofurtherstudytherepetitiveness
inlocalcontext,inthefollowing,weexplorewhethersomen-grams
in logging descriptions can be found in only one source file. These
n-grams are called endemic n-grams. Table 4demonstrates the
percentageofendemicn-gramsinthestudiedprojects.Forexample,
if the logging descriptions of a project have 100 2-grams and 20 of
themcanonlybefoundinonesourcefile,thepercentageofendemic2-gramsis20%.Wecanobservethat28.86%2-gramsinJavalogging
descriptions and 47.91% 2-grams in C# logging descriptions are
endemic. The percentage rapidly increases for settings with longer
n-grams,becauseitismoredifficulttospotidenticallongern-grams
indifferentsourcefiles.Inaddition,amongtheendemicn-grams,
16.86% endemic 3-grams and 17.37% endemic 4-grams are found
morethanonceinJavaprojects.ForC#projects,thepercentagesare
evenlarger.Theseendemic,butlocallyrepeatingn-gramsfurther
demonstrate the local repetitiveness in source file level.
Besidesloggingdescriptions,wealsocalculatethepercentageof
n-grams from all code statements that only appears in one source
183
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Characterizing the Natural Language Descriptions in Software Logging Statements ASE ’18, September 3–7, 2018, Montpellier, France
file. As illustrated in Table 4, the percentages of the endemic n-
grams in logging descriptions are generally larger than that in
source code, which indicates that logging descriptions are more
locally endemic than source code. As reported by a previous paper
[53],localnesscannotbefoundincommonEnglish,soweregardit
asaspecialfeature ofthenaturallanguageinloggingstatements.
We think this feature can be utilized to improve the automated
logging method. For example, we can improve the performance of
language models with a cache mechanism similar to that proposed
in [53].
Finding 4: Logging descriptions are locally endemic. A number
of N-grams are repetitively used in the logging descriptions inonly one source file.
4.4 RQ4: Are N-grams in Logging Descriptions
Locally Specific to a Few Source Files?
We have studied the endemic n-grams that only appear in one
source file. In this section, we further explore whether the non-
endemic n-grams in logging descriptions also favor a specific local-
ity.Bydefinition,eachnon-endemicn-gramcanbefoundinasetof
filesF, and thus, there is a discrete probability distribution pforF.
Forexample,ifann-gramisuniformlydistributed,theneachfilein
Fcontains the same number of this n-grams. We hypothesize that,
if the non-endemic n-grams favor specific locality, the distribution
pwill be skewed. For example, an n-gram, which is found in 100
source files, appears 20 times in one source file and once in the
remaining 99 source files. Inspired by [ 53], we use locality entropy
HL, which is defined as follows, to measure the skewness of the
distribution of an n-gram σinF.
HL(σ)=−/summationdisplay
f∈Fp(fσ)log2p(fσ), (3)
wherep(fσ)is defined as follows:
p(fσ)=count(n-gram) σinf
count(n-gram) σinproject(4)
Notefis a source file that contains the non-endemic n-gram σ;
projectisthecollectionofallthesourcefilesinaproject;and count
calculates the number of n-grams. Intuitively, the more skewed
thedistribution,thelowertheentropy.Forexample,ifann-gram
is found only in the logging descriptions of one source file, the
entropy will be the lowest (i.e., 0). On the contrary, if an n-gram is
uniformlydistributedintheloggingdescriptionsofsourcefiles F,
the entropy will be the highest. Thus, the lower the entropy, the
more the logging descriptions are locally specific.
The entropy of both Java and C# projects is shown in Fig. 5.
The x-axis is the number of files that contain the non-endemic
n-gram.Wecanobservethatalmostallnon-endemicn-gramshavelowerentropythanuniformdistribution,whichdemonstratestheir
locallyspecificproperty.Besides,then-gramwithvaryingorders,
which are marked by different colors, share similar trends. We can
also observe that some 1-grams have much lower entropy than the
others.This isbecausesomecommon tokensareintensively used
intheloggingdescriptionsofafile.Forexample,inprojectHive,
token"writing"isfoundintheloggingdescriptionsin169source
files (i.e.,|F|=169), which is non-endemic. While token "writing"4 8 16 32 64 128 256 512 1024
Number of Files (log-scale)12345678910Java Projects Locality EntropyUniform
1-gram
2-gram
3-gram
4-gram
(a) Java projects
4 8 16 32 64 128 256 512
Number of Files (log-scale)123456789C# Projects Locality EntropyUniform
1-gram
2-gram
3-gram
4-gram
(b) C# projects
Figure5:Entropyofthefiledistributionsfornon-endemicn-grams. "Uniform" denotes that the n-grams are distributeduniformly in the files.
onlyappearsonceinmostofthesesourcefiles,itappears366times
inonesourcefile,becauseloggingdescription"Exception writing
tointernalframebuffer"isrepetitivelyusedfor366timeslocallyin
that file.
Finding 5 : Logging descriptions are locally specific. The
non-endemicn-gramsarerepetitivelyusedinafewsourcefiles.
5 AUTOMATED LOGGING DESCRIPTION
GENERATION
BasedontheexperimentalresultsinSection 3and4,itisvaluableto
explorewhetheritispossibletoautomaticallygeneratethelogging
description for a logging statement. If shown possible, such an
automated toolwill beof greathelp fordevelopers, becauseit can
greatlyacceleratethedevelopmentprocessandpotentiallyimprovethequalityoftheirloggingdescriptions.Inthissection,wepropose
a simple but effective automated logging description generation
method, in order to demonstrate the potential feasibility of logging
automation.
184
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Pinjia He, Zhuangbin Chen, Shilin He, and Michael R. Lyu
5.1 Methodology
Our method is based on an assumption: similar code snippets tend
to contain similar logging descriptions. This assumption is trig-
gered by the results and findings in previous sections. In particular,
as explained in Section 3, all categories of logging descriptions are
used to describe certain code statements, thus the logging descrip-
tions should be closely related to the corresponding code snippets.
Additionally, in Section 4, the locality experiments demonstrate
that similar n-gram patterns are repetitively used in a local con-
text(sourcefile-level),wherecodeprovidingsimilarfunctionalities
gathers.Basedonthisassumption,weproposeasimpleinforma-
tion retrieval-basedmethod. Inparticular, to generatethe logging
descriptionforaloggingstatement,weextractitscorresponding
codesnippet,andsearchforthemostsimilarcodesnippetinthe
corpus (i.e., existing code). Then, we use the logging description
in the searched code snippet as the description for current logging
statement. The similarity is measured by Levenshtein distance [ 4],
which regards a code snippet as a string and calculate the distance
usingcharacter-basededitdistance.Forexample,theLevenshtein
distance between "public boolean createFile();" and "public booleannewFile();"is5,becauseweneed5substitutions(i.e.,change"cr"to
"n" and "ate" to "w") to make them identical.
In this experiment, for each project, we use 10-fold cross vali-
dation. Specifically, we first extract the <code,loд>pairs from
the training data. Considering the code snippet, we study two code
ranges: Pre-10andSur-20, where Pre-10indicates the 10 lines of
codeprecedingtheloggingstatementand Sur-20indicatesthe10
lines of code preceding and succeeding the logging statement. We
generatetheloggingdescriptionforeachcodesnippetinthetestingdatabysearchingthemostsimilarcodesnippetinthetrainingdata.
5.2 Evaluation Metrics
5.2.1 BLEU. To measure the accuracy of automated logging
description generation, we use BLEU [ 45], a popular evaluation
metricusedintextsummarizationandmachinetranslationtasks
[9,41,49,50].WeuseBLEUbecauseit canmeasurethesimilarity
betweenthecandidateandthereference.Inourexperiments,the
logging description generated by our method is regarded as the
candidate, while the original logging description written by the de-
veloperisregardedasthereference.Specifically,BLEUiscalculated
as follows:
BLEU =BP·exp (N/summationdisplay
n=1wnlogpn), (5)
whereBPisabrevitypenaltythatpenalizesoverlyshortcandidates;
Nisthemaximumnumberofgramsusedintheexperiments; pn
is themodified n-gram precision; and wnis theweight of each pn.
BLEU-1meanstheBLEUscoreconsideringonlythe1-gramsinthe
calculation,where w1=1andw2=w3=w4=0.Specifically, BP
is calculated as follows:
BP=⎧⎪⎨⎪⎩1i f c>r
e(1−r/c)ifc≤r(6)
whereristhelengthofthereference; cisthelengthofthecandidate.
The modified n-gram precision is defined as follows:pn=#n-grams appear in the reference
#n-grams in the candidate(7)
FromthedefinitionofBLEU,weknowthatthehighertheBLEU,
thebettertheloggingstatementgenerationperformance.Therange
of BLEU is [0 ,1], which is often presented as a percentage value
(i.e., [0,100]). Thus, if none of the n-grams in the candidate appear
inthereference,BLEUscoreis0.Inthecontrary,ifthecandidate
is exactly the same as the reference, BLEU score is 100.
5.2.2 ROUGE. BLEU measures how many n-grams in the gen-
erated logging statement appear in the reference, which enjoys
similarsense as"precision". Comparedwith BLEU,ROUGE [ 39]is
like"recall",whichmeasureshowmanyn-gramsinthereference
appear in the generated logging statement.
Specifically, ROUGE is defined as follows:
ROUGE−N=/summationtext
S∈Ref/summationtext
дram n∈Scountmatch (дramn)
/summationtext
S∈Ref/summationtext
дram n∈Scount (дramn),(8)
wherenrepresentsthelengthofthen-gram, дramn;Sisareference;
Refisthesetofallreferences; countmatch (дramn)isthemaximum
numberofn-gramsco-occurringinthecandidateandthereference;
andcount (дramn)isthenumberofn-gramsinthereference.Inour
experiments,wecalculateROUGE-1toROUGE-3,andROUGE-L.
ROUGE-L does not require a predefined n-gram length. Instead,it measures the longest matching sequence of words using LCS
(Longest Common Subsequence).
Similar to BLEU, the range of ROUGE is [0 ,1], which is often
presented as a percentage value (i.e., [0 ,100]).
5.3 Results
The BLEU scores and the ROUGE scores are shown in Table 5.W e
run the experiments on 5 Java projects and 3 projects, which are
selectedbasedonthenumberofloggingdescriptions.Wecanob-
servethattheBLEU-1scoresonalltheevaluatedprojectsarelarger
than 35, and the BLEU-1 score on CoreFx is 68.76, which means
that 68.76% of the tokens in the generated logging descriptions can
be found in the ground truth. The BLEU scores gradually decrease
asthen-gramsbecomelonger.Forexample,theBLEU-1scoreon
HadoopPre-10is36.59,whilethecorrespondingBLEU-4scoreis
16.96.ThisisreasonablebecauseBLEU-4scoreconsidersthematch
of consecutive 4 tokens. Besides, the BLEU scores and ROUGEscores for Java projects (Hadoop, Cloudstack, Camel, Hbase, and
Hive)andC#(Mono,CoreFx,Monodevelop)aresimilar,whichshow
that the effectiveness of our approach is robust against different
programming languages.
Inaddition,previouslyweexpecttoobtainlargerBLEUscores
andROUGEscoreswithSur-20thanwithPre-10,sinceweconsider
more code statements in the information retrieval process. We are
surprised to observe that the BLEU scores and ROUGE scores with
Pre-10 are better in most cases. After manual inspection of thecorresponding code snippets, we think it is caused by two mainreasons. First, considering the succeeding 10 lines of code may
bringinsomenoises,whichmisleadthemethod.Recallthemanual
categorizationinTable 2,wecanobservethatonly17.23%ofthe
loggingdescriptionsareusedtoexplainthesucceedingprogram
185
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Characterizing the Natural Language Descriptions in Software Logging Statements ASE ’18, September 3–7, 2018, Montpellier, France
Table 5: Log generation results.
Code
Scope 1 2 3 4 avg L 1 2 3
Pre-10 36.59 25.57 20.98 16.96 24.02 36.24 36.88 24.26 19.99
Sur-20 35.30 23.06 18.02 13.93 21.26 35.31 36.14 22.40 17.52Pre-10 47.60 36.35 31.17 27.57 34.92 46.05 47.11 34.64 28.97Sur-20 45.33 33.41 28.09 24.29 31.88 43.94 45.19 32.07 26.43Pre-10 51.98 41.98 36.41 30.74 39.53 49.62 50.23 38.57 33.53Sur-20 50.45 39.25 33.21 27.51 36.67 48.46 49.37 36.46 30.79Pre-10 37.69 27.05 22.40 18.28 25.42 37.71 38.47 26.76 22.76Sur-20 37.36 25.88 20.93 16.69 24.11 37.55 38.37 25.29 20.69Pre-10 40.78 31.26 26.97 23.04 29.83 40.08 40.58 29.83 25.41Sur-20 41.42 31.07 26.28 22.37 29.49 40.91 41.55 29.93 25.12Pre-10 40.54 31.99 26.27 18.65 28.23 35.95 36.10 27.28 24.69Sur-20 38.08 28.54 22.60 15.49 24.83 34.15 34.32 24.63 21.66Pre-10 68.76 60.69 55.26 49.04 57.99 62.10 62.30 53.12 49.17Sur-20 65.28 55.87 50.05 44.00 53.23 57.74 57.96 46.15 41.86Pre-10 40.74 31.40 25.68 20.07 28.48 37.86 38.14 29.88 26.95Sur-20 41.43 31.75 26.05 21.12 29.14 36.57 36.92 28.25 25.05ROUGEDataset
Hadoop
Cloudstack
Camel
Hive
Mono
CoreFx
MonodevelopBLEU
Hbase
operation.Second,inthissection,weproposeasimpleinformation
retrieval method based on Levenshtein distance, which gives equal
weightstotheeditdistancesofallthetokens.However,inpractice,
sometokensorstatementsaremoreimportantinthiscontext.Thus,
the performance of our model is affected.
This paper presents the first step towards automated logging de-
scription generation, and thus there is no existing baseline method
tocomparewith.However,insoftwareengineering,somesimilar
tasksalsogeneratenaturallanguagetextusingcorrespondingcode
snippets,suchascodesummarization,whichaimstogeneratealine
oftexttosummarizeacodesnippet.TheBLEU-4scoresreported
in the state-of-the-art code summarization papers [ 31,40] range
from 6.4% to 34.3%. Meanwhile, the BLEU-4 scores of our simpleinformationretrieval-basedmethodrangefrom16.96%to49.04%
withthe"Pre-10"setting,whichisencouraging.Notethatwedonot
intendtodirectlycomparetheperformanceofmethodsfordifferent
tasks. However, we want to provide an intuitiveunderstanding of
theBLEUscoresandROUGEscoresweachieve.Furthermore,we
visionplentyofspaceforimprovementbyadoptingmoremature
models or specialized feature engineering. However, this is not the
focus of this paper, so we leave it as our future work.
Finding 6:A simple information retrieval-based method, which
generates logging descriptions by finding similar code snippets,
can achieve decent performance in terms of BLEU score and
ROUGE score.
6 FUTURE DIRECTIONS
This paper aims to study the usage of natural language in logging
practiceandfurthertriggerfollow-upresearchworkinthisfield.
In thissection, we presentsome potential directionsbased on our
study of software logs.
Improved Information Retrieval Models. To this end, we
use a simple character similarity-based method to find suitable nat-
urallanguagedescriptionsforloggingstatements,whichdemon-
strates decent performance. However, it has obvious limitations.
For example, as a reviewer mentioned, a method being called at
differentprogramlocationscanhavedifferentloggingdescriptions,whichcannotbeaddressedbythismodel.Thereareseveralavenues
for extension. First, as explained in [ 28], very large bodies of code
can be readily parsed, typed, scoped, and even subject to simple
semanticanalysis.Thus,allthesedatacouldbeusedtodevelopa
more sophisticated approach to search similar code snippets. Be-
sides,codeclonedetection[ 16,33,54]isaclassicaltopicthathas
been widely studied in software engineering area. Based on the re-
sults of our automated logging generation experiments, we believe
it is promising to adapt code clone detection methods to further
improve the generation performance.
LoggingStatementGenerationfromCode. Althoughthepo-
tentialofinformationretrieval-basedloggingdescriptiongenera-
tion has been validated in the experiments, the model has some
limitations. In particular, it assumes thatthe current code snippet
can be described by an existing logging description. However, new
projectsoftencontainonlyafewloggingstatements,whichmay
maketheproposedmodelineffective.Thisisalsoatypicalproblem
knownas"coldstart"inthefieldofinformationretrieval.Thus,it
will be of great help if we can generate the logging descriptions
based on the corresponding code snippets. We vision it is feasi-ble because logging descriptions are mainly used to explain the
surroundingcodes.Toachievethisgoal,existingworkoncodesum-
marization [ 31,40], code comment generation [ 52], and commit
message generation [32] are good starting points.
Data Augmentation. Compared with most NLP (natural lan-
guage processing) applications in software engineering, such ascode completion [
11,38], the data volume of "what to log" is not
large. As illustrated in Table 1, the largest Java project in our ex-
periments(i.e.,Hadoop)contains1.9MLOCbutonly13.9Klogging
statements. The relatively small data volume could stay in the way
oftheapplicationofdeeplearning-basedalgorithms[ 14,24],which
dominates many difficult research problems in recent years. We
think "what to log" is such a difficult problem, because researchers
need to understand the semantic meaning of the corresponding
codesnippets.Thus,effectivedataaugmentationtechniques,which
aim at the generation of more training data, is in high demand.
186
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Pinjia He, Zhuangbin Chen, Shilin He, and Michael R. Lyu
The basic idea of data augmentation for "what to log" is to gen-
erate more <code,loд>pairs using existing data. For example,
a simple data augmentation method is to change the identifier
names in a code snippet and keep the original log, which leads to a
new<code,loд>pair. Towards this end, researchers could start
withdata augmentationmethods inimage processingfield[ 29,30],
which have been widely studied.
7 RELATED WORK
7.1 Log Analysis
Softwarelogscontainawealthofruntimeinformation,andthus
have been widely used in various software reliability enhancement
tasks,includinganomalydetection[ 18,57],faultdiagnosis[ 55,63],
program verification [ 17,51], performance monitoring [ 27,43,61],
etc. Most of these tasks use data mining models to extract criti-
calruntimeinformationfromalargevolumeofsoftwarelogs.To
facilitate log analysis, researchers also focus on related log data
preprocessing topics, including log collection [ 12,17] and log pars-
ing [25,26,42]. These existing papers study how to utilize logs
printed by existing logging statements in software. Instead, in this
paper, wefocus on the designof logging statements, andthus can
potentially benefit these log analysis tasks.
7.2 Logging Practice
Currentresearchhasmostlyfocusedontheusageoflogsprinted
byexistingloggingstatements,butlittleonloggingitself.Recently,
some empirical studies [ 10,13,19,36,46,58,59] have been con-
ducted to characterize logging practice. Specifically, Yuan et al.[
58,59] study the logging practice of open-source software and
furtherproposeproactiveloggingstrategy.[ 13,36]characterizethe
loggingpracticeofJavaprojects.Consideringloggingpracticein
industry,Fuetal.[ 19]conductanempiricalstudyonthelogging
practice of software used in Microsoft. Pecchia et al. [ 46] study
the logging practice in a critical software development process. All
thesestudiesprovideinsightfulfindingsonloggingpractice,which
shedlightsintoourstudyofthenaturallanguagedescriptionsin
software logs.
7.3 Improving Logging
Towards improving logging practice, there are two categories of
work:"wheretolog"[ 15,62]and"whattolog"[ 60]."Wheretolog"
studies focus on strategic logging, which recommends developers
the suitable logging places. Specifically, Cinque et al. [ 15] propose
a logging method based on a set of rules about logging placement,
which makes the logs able to detect more software failures. Zhu et
al. [62] design a tool LogAdvisorthat informs developers whether
they should place a logging statement in a code snippet or not.Different from these methods, we focus on the contents of thelogging statements, which is the goal of "what to log" research
work, instead of logging placement. Yuan et al. [
60] propose a
tool LogEnhancer that can enhance existing logging statements by
augmentingimportantvariables.Lietal.[ 37]designaregression
model to recommend the log level in a logging statement. Our
paperalsotargetsonimprovingthe"whattolog"partoflogging
practice. Different from [ 37,60], we focus on the natural language
descriptionsinloggingstatements.Besides,wepresentasimplebuteffective description generation tool. Thus, we believe our study
can complement existing logging improving work.
7.4 NLP in Software Engineering
Naturallanguagewidelyexistsinsoftwareartifacts,suchasdesign
documents, user manuals, bug reports, source code comments, and
identifiernames[ 23].Inrecentyears,varioustechniqueshavebeen
proposedbyresearcherstoanalyzenaturallanguagetextforthe
improvementofmodernsoftwareengineering.GabelandSu[ 20]
study the syntactic redundancy of source code, which reveals a
general lack of uniqueness in software. Hindle et al. [ 28] study the
naturalness of software, showing that repetitive and predictableregularities of source code can be captured by a simple n-gramlanguage model. Tu et al. [
53] further explore the localness char-
acteristicsofsoftware.Thesethreestudiesregardsourcecodeas
natural language text and study the related characteristics of them.
Inspiredbythesestudies,inthispaper,westudythecharacteristics
of natural language in logging statements. Additionally, NLP meth-
ods have been adapted to many software engineering scenarios,including defect prediction [
48], code completion [ 44], program
synthesis [ 47], API recommendation [ 22], identifier/method name
suggestion [ 7,8], etc. Different from these papers, we study "what
tolog"withthefocusonthenaturallanguagedescriptionsinlog-
gingstatements.Webelievethisstudypavesthepathforthedesign
of NLP techniques for the "what to log" problem.
8 CONCLUSION
To facilitate software development and maintenance, developers
are expected to provide informative and appropriate logging de-
scriptions. However, there is currently a lack of investigations and
specificationsonstudyingsuchdescriptionsinloggingpractice.Tofillthissignificantgap,thispaperpresentsanempiricalstudyontheloggingstatementsin10Javaprojectsand7C#projects,withfocus
onwhat to log. We summarize with 6 valuable findings, ranging
fromtheloggingdescription’scategories,thegloballyandlocally
repetitiveusageofn-grampatterns,toanencouragingindication
towards automated logging description generation. In addition,some valuable directions for improving current logging practice
are discussed. In summary, this paper systematically characterizes
thenatural languagedescriptions usedinlogging practice,which
serves as the first work towards automated logging description
generation. With our datasets released, we hope to trigger relatedresearch projects and push this field forward.
ACKNOWLEDGMENT
The authors thank the anonymous reviewers for their feedback
which helped improve this paper. The work described in this paper
was fully supported by the National Natural Science Foundation of
China(ProjectNos.61332010and61472338),theResearchGrants
Council ofthe Hong Kong Special Administrative Region, China
(No.CUHK14234416oftheGeneralResearchFund),theNational
Basic Research Program of China (973 Project No. 2014CB347701),
and Microsoft Research Asia (2018 Microsoft Research Asia Collab-
orative Research Award).
187
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. Characterizing the Natural Language Descriptions in Software Logging Statements ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1] [n. d.]. Apache log4net. https://logging.apache.org/log4net/
[2] [n. d.]. Apache Project List. https://projects.apache.org/projects.html
[3] [n. d.]. Datasets. https://github.com/logpai/LoggingDescriptions
[4][n. d.]. Levenshtein Distance. https://en.wikipedia.org/wiki/Levenshtein_
distance
[5] [n. d.]. N-gram. https://en.wikipedia.org/wiki/N-gram
[6][n. d.]. Overview of Unified Logging System (ULS). http://msdn.microsoft.com/
en-us/library/office/ff512738(v=office.14).aspx
[7]M.Allamanis,E.T.Barr,C.Bird,andC.Sutton.2014. Learningnaturalcoding
conventions. In FSE’14: Proc. of the 22nd ACM SIGSOFT International Symposium
on Foundations of Software Engineering.
[8]M.Allamanis,E.T.Barr,C.Bird,andC.Sutton.2015. Suggestingaccuratemethod
andclassnames.In FSE’15: Proc. of the ACM SIGSOFT International Symposium
on Foundations of Software Engineering.
[9]D.Bahdanau,K.Cho,andY.Bengio.2015. NeuralMachineTranslationbyJointly
Learning to Align and Translate. In ICLR’15: Proc. of the International Conference
on Learning Representations.
[10]T.Barik,R.Deline,S.Drucker,andD.Fisher.2016. TheBonesoftheSystem:A
CaseStudyofLoggingandTelemetryatMicrosoft.In ICSE’16: Proc. of the 38th
International Conference on Software Engineering.
[11]A.Bhoopchand,T.Rocktaschel,E.Barr,andS.Riedel.2016.LearningPythonCode
Suggestion with a Sparse Pointer Network. In arXiv preprint arXiv:1611.08307.
[12]N.BusanyandS.Maoz.2016. BehavioralLogAnalysiswithStatisticalGuarantees.
InICSE’16: Proc. of the 38th International Conference on Software Engineering
(Companion Volume).
[13]B. Chen and Z. Jiang. 2017. Characterizing logging practices in Java-based open
sourcesoftwareprojects-areplicationstudyinApacheSoftwareFoundation.
Empirical Software Engineering 22 (2017), 330–374. Issue 1.
[14]K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
and Y. Bengio. 2014. Learning Phrase Representations using RNN Encoder-
Decoder for Statistical Machine Translation. In EMNLP’14: Proc. of the Empirical
Methods in Natural Language Processing.
[15]M. Cinque, D. Cotroneo, and A. Pecchia. 2013. Event Logs for the Analysis
of Software Failures: A Rule-Based Approach. IEEE Transactions on Software
Engineering (TSE) 39, 6 (2013), 806–821.
[16]Y. Dang, D. Zhang, S. Ge, R. Huang, C. Chu, and T. Xie. 2017. Transferring Code-
CloneDetectionandAnalysistoPractice.In ICSE’17: Proc. of the 40th International
Conference on Software Engineering (SEIP-track).
[17]R.Ding,H.Zhou,J.G.Lou,H.Zhang,Q.Lin,Q.Fu,D.Zhang,andT.Xie.2015.
Log2: A Cost-Aware Logging Mechanism for Performance Diagnosis. In ATC’15:
Proc. of the USENIX Annual Technical Conference.
[18]Q.Fu,J.Lou,Y.Wang,andJ.Li.2009.ExecutionAnomalyDetectioninDistributed
SystemsthroughUnstructuredLogAnalysis.In ICDM’09: Proc. of International
Conference on Data Mining.
[19]Q.Fu,J.Zhu,W.Hu,J.Lou,R.Ding,Q.Lin,D.Zhang,andT.Xie.2014. WhereDo
DevelopersLog?AnEmpiricalStudyonLoggingPracticesinIndustry.In ICSE’14:
Companion Proc. of the 36th International Conference on Software Engineering.
24–33.
[20]M.GabelandZ.Su.2010. Astudyoftheuniquenessofsourcecode.In FSE’10: Proc.
of the 18th ACM SIGSOFT International Symposium on Foundations of Software
Engineering.
[21]Z. Gao, C. Bird, and E. T. Barr. 2017. To Type or Not to Type: Quantifying
DetectableBugsinJavascript.In ICSE’17: Proc. of the 39th International Conference
on Software Engineering.
[22]X. Gu, H. Zhang, D. Zhang, and S. Kim. 2016. Deep API learning. In FSE’16: Proc.
of the 24th ACM SIGSOFT International Symposium on Foundations of Software
Engineering.
[23]S. Haiduc, V. Arnaoudova, A. Marcus, and G. Antoniol. 2016. The Use of Text
Retrieval and Natural Language Processing in Software Engineering. In ICSE’16:
Proc. of the 38th International Conference on Software Engineering (Companion
Volume).
[24]K. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep Residual Learning for Image
Recognition. In CVPR’16: Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition.
[25]P. He, J. Zhu, S. He, J. Li, and M. R. Lyu. 2016. An Evaluation Study on Log
Parsing and Its Use in Log Mining. In DSN’16: Proc. of the 46th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks.
[26]P. He, J. Zhu, S. He, J. Li, and M. R. Lyu. 2017. Towards Automated Log Parsing
forLarge-ScaleLog DataAnalysis. IEEE Transactions on Dependable and Secure
Computing (TDSC) (2017).https://doi.org/10.1109/TDSC.2017.2762673
[27]P. He, J. Zhu, Z. Zheng, and M. R. Lyu. 2014. Location-based Hierarchical Matrix
Factorization for Web Service Recommendation. In ICWS’14: Proc. of the 21st
International Conference on Web Services.
[28]A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. 2012. On the naturalness
of software. In ICSE’12: Proc. of the 34th International Conference on Software
Engineering.[29]H. Hosseini, B. Xiao, M. Jaiswal, and R. Poovendran. 2017. On the Limitationof Convolutional Neural Networks in Recognizing Negative Images. In arXiv
preprint arXiv:1703.06857.
[30]H.Inoue.2018. DataAugmentationByPairingSamplesforImagesClassification.
InarXiv preprint arXiv:1801.02929.
[31]S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer. 2016. Summarizing Source
CodeusingaNeuralAttentionModel.In ACL’16: Proc. of the 54th Annual Meeting
of the Association for Computational Linguistics.
[32]S.Jiang,A.Armaly,andC.McMillan.2017. AutomaticallyGeneratingCommit
MessagesfromDiffsusingNeuralMachineTranslation.In ASE’17: Proc. of the
32nd IEEE/ACM International Conference on Automated Software Engineering.
[33]H. Kim, Y. Jung, S. Kim, and K. Yi. 2011. MeCC: memory comparison-based
clonedetector. In ICSE’11: Proc. of the 33rd International Conference on Software
Engineering.
[34] P. Koehn. 2010. Statistical Machine Translation. Cambridge University Press.
[35]Robert V Krejcie and Daryle W Morgan. 1970. Determining sample size for
research activities. Educational and psychological measurement 30, 3 (1970),
607–610.
[36]H. Li, Shang W. Chen, T., and A. E. Hassan. 2017. Studying Software Logging
Using Topic Models. Empirical Software Engineering (2017).
[37]H. Li, W. Shang, and A. E. Hassan. 2017. Which log level should developers
chooseforanewloggingstatement? Empirical Software Engineering 22(2017),
1684–1716. Issue 4.
[38]J. Li, Y. Wang, I. King, and M. R. Lyu. 2017. Code Completion with Neural
Attention and Pointer Networks. In arXiv preprint arXiv:1711.09573.
[39]C.Y.Lin.2004. ROUGE:APackageforAutomaticEvaluationofSummaries.In
WAS’04: Proc. of the Workshop on Text Summarization Branches Out.
[40]P. Loyola, E. Marrese-Taylor, and Y. Matsuo. 2017. A Neural Architecture for
GeneratingNaturalLanguageDescriptionsfromSourceCodeChanges.In ACL’17:
Proc. of the 55th Annual Meeting of the Association for Computational Linguistics.
[41]M.Luong,H.Pham,andC.D.Manning.2015. EffectiveApproachestoAttention-
based Neural Machine Translation. In EMNLP’15: Proc. of the Empirical Methods
in Natural Language Processing.
[42]A.Makanju,A.N.Zincir-Heywood,andE.E.Milios.2012. ALightweightAlgo-
rithmforMessageTypeExtractioninSystemApplicationLogs. TKDE’12: IEEE
Transactions on Knowledge and Data Engineering (2012).
[43]K. Nagaraj, C. Killian, and J. Neville. 2012. structured comparative analysis of
systems logs to diagnose performance problems. In NSDI’12: Proc. of the 9th
USENIX conference on Networked Systems Design and Implementation.
[44]A. T. Nguyen, T. V. Nguyen, H. D. Phan, and T. N. Nguyen. 2018. A Deep Neural
Network Language Model with Contexts for Source Code. In SANER’18: Proc.
of the 25th IEEE International Conference on Software Analysis, Evolution and
Reengineering.
[45]K.Papineni,S.Roukos,T.Ward,andW.Zhu.2002. BLEU:amethodforautomatic
evaluation of machine translation. In ACL’02: Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics.
[46]A.Pecchia,M.Cinque,G.Carrozza,andD.Cotroneo.2015. Industrypracticesand
event logging: assessment of a critical software development process. In ICSE’15:
Proc. of the 37th International Conference on Software Engineering. 169–178.
[47]M. Raghothaman, Y. Wei, and Y. Hamadi. 2016. SWIM: Synthesizing What I
Mean: Code Search and Idiomatic Snippet Synthesis. In ICSE’16: Proc. of the 38th
International Conference on Software Engineering.
[48]B. Ray, V. Hellendoorn, S. Godhane, Z. Tu, A. Bacchelli, and P. Devanbu. 2016.
Onthe"Naturalness"ofBuggyCode.In ICSE’16: Proc. of the 38th International
Conference on Software Engineering.
[49]A. M. Rush, S. Chopra, and J. Weston. 2015. A Neural Attention Model for
AbstractiveSentenceSummarization.In EMNLP’15: Proc. of the Empirical Methods
in Natural Language Processing.
[50]A. See, P. Liu, and C. Manning. 2017. Get To The Point: Summarization with
Pointer-GeneratorNetworks.In ACL’17: Proc. of the 55th Annual Meeting of the
Association for Computational Linguistics.
[51]W. Shang, Z. Jiang, H. Hemmati, B. Adams, A.E. Hassan, and P. Martin. 2013.
Assistingdevelopersofbigdataanalyticsapplicationswhendeployingonhadoop
clouds.In ICSE’13: Proc. of the 35th International Conference on Software Engineer-
ing. 402–411.
[52]G.Sridhara,E.Hill,D.Muppaneni,L.Pollock,andK.Vijay-Shanker.2010. To-
wardsautomaticallygeneratingsummarycommentsforJavamethods.In ASE’10:
Proc. of the 25th IEEE/ACM International Conference on Automated Software Engi-
neering.
[53]Z. Tu, Z. Su, and P. Devanbu. 2014. On the localness of software. In FSE’14: Proc.
of the 22nd ACM SIGSOFT International Symposium on Foundations of Software
Engineering.
[54]M. White, M. Tufano, C. Vendome, and D. Poshyvanyk. 2016. Deep learning
codefragmentsforcodeclonedetection.In ASE’16: Proc. of the 31st IEEE/ACM
International Conference on Automated Software Engineering.
[55]W. E. Wong, V. Debroy, R. Golden, X. Xu, and B. Thuraisingham. 2012. Effective
softwarefaultlocalizationusinganRBFneuralnetwork. TR’12: IEEE Transactions
on Reliability (2012).
188
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Pinjia He, Zhuangbin Chen, Shilin He, and Michael R. Lyu
[56]W.Xu.2010. System Problem Detection by Mining Console Logs. Ph.D.Dissertation.
University of California, Berkeley.
[57]W.Xu,L.Huang,A.Fox,D.Patterson,andM.I.Jordon.2009. DetectingLarge-
Scale System Problems by Mining Console Logs. In SOSP’09: Proc. of the ACM
Symposium on Operating Systems Principles.
[58]D.Yuan,S.Park,P.Huang,Y.Liu,M.Lee,X.Tang,Y.Zhou,andSSavage.2012. Be
conservative:enhancingfailurediagnosiswithproactivelogging.In OSDI’12: Proc.
of the 10th USENIX Conference on Operating Systems Design and Implementation.
293–306.
[59]D. Yuan, S. Park, and Y. Zhou. 2012. Characterizing logging practices in open-
source software. In ICSE’12: Proc. of the 34th International Conference on Software
Engineering. 102–112.
[60]D. Yuan, J. Zheng, S. Park, Y. Zhou, and S. Savage. 2011. Improving software
diagnosability via log enhancement. In ASPLOS’11: Proc. of the 16th InternationalConference on Architectural Support for Programming Languages and Operating
Systems. 3–14.
[61]A. Zhou, S. Wang, Z. Zheng, C. Hsu, M. R. Lyu, and F. Yang. 2016. On cloud
servicereliabilityenhancementwithoptimalresourceusage. IEEE Transactions
on Cloud Computing (TCC) 4 (2016), 452–466.
[62]J.Zhu,P.He,Q.Fu,H.Zhang,M.R.Lyu,andD.Zhang.2015. LearningtoLog:
HelpingDevelopersMakeInformedLoggingDecisions.In ICSE’15: Proc. of the
37th International Conference on Software Engineering.
[63]D. Q. Zou,H. Qin, and H. Jin.2016. UiLog: Improving Log-BasedFaultDiagnosis
by Log Analysis. Journal of Computer Science and Technology 31, 5 (2016), 1038–
1052.
189
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. 