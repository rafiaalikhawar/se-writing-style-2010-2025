A Comparative Study to Benchmark Cross-project Defect
Prediction Approaches
Steffen Herbold, Alexander Trautsch, Jens Grabowski
University of Goettingen, Insititute of Computer Science
Göttingen, Germany
{herbold,alexander.trautsch,grabowski}@cs.uni-goettingen.de
EXTENDED ABSTRACT
Cross-ProjectDefectPrediction( CPDP)asameanstofocusqual-
ityassuranceofsoftwareprojectswasunderheavyinvestigation
inrecentyears.However,withinthecurrentstate-of-the-artitis
unclearwhichofthemanyproposalsperformsbestduetoalack
ofreplicationofresults anddiverseexperimentsetupsthatutilize
different performance metrics and are based on different under-
lyingdata.Withinthisarticle[ 2,3],weprovideabenchmarkfor
CPDP. Our benchmark replicates 24 CPDPapproaches proposed
by researchers between 2008 and 2015. Through our benchmark,
we answer the following research questions:
•RQ1:WhichCPDPapproaches perform best in terms of
F-measure, G-measure, AUC, and MCC?
•RQ2:Doesany CPDPapproachconsistentlyfulfilltheper-
formance criteria for successful predictions postulated byZimmermann et al.[
4], i.e., have at least 0.75 recall, 0.75
precision, and 0.75 accuracy?
•RQ3:What is the impact of using only larger products ( >
100instances)withacertainbalance(atleast5%defective
instances and at least 5% non-defective instances) on the
benchmark results?
•RQ4:Whatistheimpactofusingarelativelysmallsubset
of a larger data set on the benchmark results?
Weidentified5publicdatasets,whichcontaindefectdataabout
86softwareproductsthatweusedtoanswertheseresearchques-
tion.Theadvantageofusingmultipledatasetswasthatwecould
increasethenumberofsoftwareproductsand,thereby,increasethe
external validity of our results. Moreover, we wanted to use multi-
pleperformancecriteriafortheevaluationofthe CPDPapproaches.
Therefore,RQ1ranksapproachesnotjustusingasinglecriterion,
butusingthefourperformancemetrics AUC, F-measure, G-measure,
andMCC.Existingapproachesfortherankingofstatisticallydiffer-
entapproachesneitheraccountforsoftwareproductsfromdifferent
data sets, nor multiple performance metrics. Therefore, we defined
anewapproachforthecombinationofseparaterankingsforthe
performance criteria and data sets, into one common ranking.
Figure1depictstheresultsforRQ1.Theresultsshowthatanap-
proachproposedbyCamargoCruzandOchimizu[ 1]performsbest
and even outperforms cross-validation. Moreover, our results show
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.
For all other uses, contact the owner/author(s).
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5638-1/18/05.
https://doi.org/10.1145/3180155.31825420.7870.860.917
0.170.903
0.802
0.7370.742
0.5210.82
0.551
0.3470.6820.689
0.682
0.680.833
0.762
0.3090.3150.59
0.1580.801
0.4260.7310.815
0.715
0.666
Trivial−FIXCanfora13−MODEPRandom−RANDOMRyu14−VCBSVMNam13−NBUchigaki12−LELiu10−GPMenzies11−NBRyu15−NBZimmermann09−NBPeters13−NBNam15−LRPeters12−NBPanichella14−CODEP−BNZHe13−RFWatanabe08−NBKawata15−NBKoshgoftaar08−NBPHe15−NBALL−NBTurhan09−NBHerbold13−NETYZhang15−MAXVOTEMa12−NBPeters15−NBAmasaki15−NBCV−RFCamargoCruz09−NB
0.00 0.25 0.50 0.75 1.00
Mean rankscoreApproachRanking of approaches using AUC, F−measure, G−Measure, and MCC
Figure 1: Mean rank score over all data sets for the metrics
AUC, F-measure, G-measure,and MCC.Incasemultipleclas-
sifiers were used, we list only the result achieved with thebest classifier.
thatonly6ofthe24approachesoutperformoneofourbaselines,
i.e., using all data for training without any transfer learning. Re-
gardingRQ2,wedeterminedthatpredictionsonlyseldomlyachieve
a high performance of 0.75 recall, precision, and accuracy. The best
CPDPapproaches only fulfill the criterion for 4 of the 86 products,
i.e.,4.6%ofthetime.Thus, CPDPstillhasnotreachedapointwhere
the performance of the results is sufficient for the application in
practice.
RQ3andRQ4wereusedtoseeifresultsareaffectedbysubsetting
data,asisoftendonefordefectpredictionexperiments.ForRQ3,i.e.,usingalargesubset,wedeterminednodifferencebetweenusingall
dataandusingthesubset.ForRQ4,i.e.,usingasmallsubsetofof
data, we found that there are statistically signifcant differences inreported performances of up to 5%. Thus, the use of small subsets
should be avoided.
REFERENCES
[1]A. E. Camargo Cruz and K. Ochimizu. 2009. Towards logistic regression models
forpredictingfault-pronecodeacrosssoftwareprojects.In Proc. 3rd Int. Symp.
on Empirical Softw. Eng. and Measurement (ESEM).IEEEComputerSociety. https:
//doi.org/10.1109/ESEM.2009.5316002
[2]S. Herbold, A. Trautsch, and J. Grabowski. 2017. A Comparative Study to Bench-
markCross-projectDefectPredictionApproaches. IEEE Trans. Softw. Eng. Online
First (2017). https://doi.org/10.1109/TSE.2017.2724538
[3]S.Herbold,A.Trautsch,andJ.Grabowski.2017. Correctionof"AComparative
Study to Benchmark Cross-project Defect Prediction". CoRRabs/1707.09281
(2017). https://arxiv.org/abs/1707.09281
[4]T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy. 2009. Cross-
projectdefectprediction:alargescaleexperimentondatavs.domainvs.process.
InProc. the 7th Joint Meet. Eur. Softw. Eng. Conf. (ESEC) and the ACM SIGSOFT
Symp. Found. Softw. Eng. (FSE). ACM. https://doi.org/10.1145/1595696.1595713
10632018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:24 UTC from IEEE Xplore.  Restrictions apply. 