PaMpeR: Proof Method Recommendation System for
Isabelle/HOL
Yutaka Nagashima
CIIRC, Czech Technical University in Prague
Prague, Czech Republic
Department of Computer Science, University of Innsbruck
Innsbruck, Tyrol, Austria
yutaka.nagashima@uibk.ac.atYilun He
School of Information Technologies, University of Sydney
Sydney, New South Wales, Australia
yihe8397@uni.sydney.edu.au
ABSTRACT
Deciding which sub-tool to use for a given proof state requires ex-
pertisespecifictoeachinteractivethe oremprover(ITP).Tomitigate
this problem, we present PaMpeR,aproofmethodrecommendation
system for Isabelle/HOL. Given a proof state, PaMpeRrecommends
proof methods to discharge the proof goal and provides qualitative
explanationsastowhyitsuggeststhesemethods. PaMpeRgenerates
these recommendations based on existing hand-written proof cor-
pora,thustransferringexperiencedusers’expertisetonewusers.
Ourevaluationshowsthat PaMpeRcorrectlypredictsexperienced
users’proofmethodsinvocationespeciallywhenitcomestospecial
purpose proof methods.
CCS CONCEPTS
•Informationsystems →Recommendersystems ;Information
extraction ;•Security and privacy →Logic and verification ;•
Human-centeredcomputing →Userinterfacetoolkits ;•Theory
of computation →Higher order logic ;
KEYWORDS
Isabelle/HOL,recommendationsystem,datamining,proofmethod,
interactive theorem prover
ACM Reference Format:
Yutaka Nagashima and Yilun He. 2018. PaMpeR: Proof Method Recommen-
dationSystemforIsabelle/HOL.In Proceedingsofthe201833rdACM/IEEE
International Conference on Automated Software Engineering (ASE ’18), Sep-
tember3–7,2018,Montpellier,France. ACM,NewYork,NY,USA, 11pages.
https://doi.org/10.1145/3238147.3238210
1 INTRODUCTION
Doyouknowwhentousetheproofmethod1calledintro_classes
inIsabelle?Whatabout uint_arith ?Canyoutellwhen fastforce
tends to be more powerful than auto? If you are an Isabelle expert,
1ProofmethodsaretoolsusedtodischargeproofgoalsinIsabelle.Theyaresimilarto
tactics in other LCF-style provers.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238210youransweris“Sure.”ButifyouarenewtoIsabelle,youranswer
might be “No. Do I have to know these Isabelle specific details? ”
Interactivetheoremprovers(ITPs)areformingthebasisofre-
liable software engineering. Klein et al.proved the correctness
of the seL4 micro-kernel in Isabelle/HOL [ 11]. Leroy developed
a certifying C compiler, CompCert, using Coq [ 15]. Kumar et al.
built a verified compiler for a functional programming language,
CakeML,inHOL4[ 14].Inmathematics,mathematiciansarereplac-
ing their pen-and-paper proofs with mechanised proofs to avoid
human-errors in their proofs: Hales et al.mechanically proved the
Kepler conjecture using HOL-light and Isabelle/HOL [ 6], whereas
Gonthier etal.finishedtheformalproofsofthefourcolourtheorem
inCoq[5].Intheoreticalcomputerscience,PaulsonprovedGödel’s
incompleteness theorems using Nominal Isabelle [22].
To facilitate efficient proof developments in such large scale ver-
ification projects, modern ITPs are equipped with many sub-tools,
suchasproofmethodsandtactics.Forexample,Isabelle/HOLcomes
with160proofmethodsdefinedinitsstandardlibrary.Thesesub-
toolsprovideusefulautomationforinteractivetheoremproving;
however, it still requires ITP specific expertise to pick up the right
proof method to discharge a given proof goal.
This paper presents our novel approach to proof method recom-
mendation and its implementation, PaMpeR. The implementation is
available at GitHub [1]. Our research hypothesis is that:
itispossibletoadvisewhichproofmethodsareuse-
ful to a given proof state, based only on the meta-
information about the state and information in the
standard library. Furthermore,we can extract advice
by applying machine learning algorithms to existing
large proof corpora.
The paper is organized as follows: Section 2explains the basics
of Isabelle/HOL and provides the overview of PaMpeR. Section 3
expounds how PaMpeRtransforms the complex data structures rep-
resenting proof states to simple data structures that are easier to
handle for machinelearning algorithms. Section 4shows how our
machine learning algorithm constructs regression trees from these
simple data structures. Section 5demonstrates how users can elicit
recommendationsfrom PaMpeR.Section6presentsourextensive
evaluation of PaMpeRto assess the accuracy of PaMpeR’s recom-
mendations.Section 7discussesthestrengthsandlimitationsofthe
currentimplementationandthefutureworkthatmightimprove
PaMpeR’sperformancefurtherorprovideevenmoredetailedevalu-
ation of the current implementation. Section 8compares our work
with other attempts of applying machine learning and data mining
to interactive theorem proving.
362
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yutaka Nagashima and Yilun He
2 BACKGROUND AND OVERVIEW OF PAMPER
2.1 Background
Isabelle/HOL is an interactive theorem prover, mostly written in
StandardML.TheconsistencyofIsabelle/HOLiscarefullyprotected
by isolating its logical kernel using the module system of Standard
ML.Isabelle/Isar [24](Isarfor short) is a proof language used in
Isabelle/HOL.Isarprovidesahuman-friendlyinterfacetospecify
anddischargeproofgoals.Isabelleusersdischargeproofgoalsby
applying proofmethods,whicharetheIsarsyntacticlayerofLCF-
style tactics.
Each proof goal in Isabelle/HOL is stored within a proof state ,
which also contains locally bound theorems for proof methods
(chained facts ) andthe background proof context of theproof goal,
whichincludeslocalassumptions,auxiliarydefinitions,andlemmas
proved prior to the the current step. Proof methods are in general
sensitive not only to proof goals but also to their chained facts
andbackgroundproofcontexts:theybehavedifferentlybasedon
information stored in proof state. Therefore, when users decide
which proof method to apply to a proof goal, they often have to
take other information in the proof state into consideration.
IsabellecomeswithmanyIsarkeywordstodefinenewtypesand
constants, such as datatype ,codatatype ,primrec,primcorec ,
inductive , anddefinition . For example, the funcommand is
used for general recursive definitions.
Thesekeywordsnotonlyletusersdefinenewtypesorconstants,
but they also automaticallyderive auxiliary lemmasrelevantto the
defined objects behind the user-interfaceand register them in the
backgroundproofcontextwhereeachkeywordisused.Forexample,Nipkowet al.defined a function,
sep, using the funkeyword in an
old Isabelle tutorial [21] as follows:
fun sep::"'a => 'a list => 'a list" where
" s e pa[] =[] "|"sep a [x] = [x]" |"sep a (x#y#zs)=x#a#s e pa(y#zs)"
Intuitively, this function inserts the first argument between
any two elements in the second argument. Following this defini-
tion, Isabelle automatically derives the following auxiliary lemma,
sep.induct , and registers it in the background proof context as
well as other four automatically derived lemmas:
sep.induct: (!!a. ?P a [])
==> (!!a x. ?P a [x])==> (!!a x y zs. ?P a (y # zs)
= = >? Pa( x#y#zs))
==> ?P ?a0.0 ?a1.0
where variables prefixed with ?, such as?a0.0, are schematic vari-
ables,!!isthemeta-logicuniversalquantifier, ==>isthemeta-logic
implication2. Isabelle also attaches unique names to these auto-
matically derived lemmas following certain naming conventions
hard-coded in Isabelle’s source code. In this example, the full name
of this lemma is fun0.sep.induct , which is a concatenation of
the theory name ( fun0), the delimiter ( .), the name of the constant
2Isabelle/HOL is a specialization of Isabelle for Higher-Order Logic (HOL) formalized
in Isabelle’s meta-logic. Therefore, it has two versions of universal quantifier and
implication: one in the meta-logic and the other one in HOLdefined(sep),followedbyahard-codedpostfix( .induct),which
represents the kind of this derived lemma.
When users want to prove conjectures about sep, they can
specify their conjectures using Isar keywords such as lemmaand
theorem.TheIsarcommands, applyandby,allowuserstoapply
proof methods to these proof goals. In the above example, Nipkow
et al.proved the following lemma about mapandsepusing the au-
tomatically derived auxiliary lemma, sep.induct , as an argument
to the proof method induct_tac as following:
lemma "map f (sep x xs) = sep (f x) (map f xs)"
apply(induct_tac x xs rule: sep.induct)apply simp_all done
wheresimp_all isaproofmethodthatexecutessimplificationto
all sub-goals and doneis another Isar command used to conclude a
proof attempt.
Isabelle provides a plethora of proof methods, which serve as
ammunitions when used by experienced Isabelle users; however,
new Isabelle users sometimes spend hours or days trying to prove
goals using proof methods sub-optimal to their problems without
knowingIsabellehasalreadyspecializedmethodsthatareoptimized
for their goals.
2.2 Overview of PaMpeR
Figure1illustrates the overview of PaMpeR. The system consists of
twophases:theupperhalfofthefigureshows PaMpeR’spreparation
phase, and the lower half shows its recommendation phase.
Inthepreparationphase, PaMpeR’sfeatureextractorconvertsthe
proofstatesinexistingproofcorporasuchastheArchiveofFormal
Proofs (AFP) [ 12] into a database. This database describes which
proofmethodshavebeenappliedtowhatkindofproofstate,whileabstractingproofstatesasarraysofbooleanvalues.Thisabstractionisamany-to-onemapping:itmaymapmultipledistinctproofstates
into to the same array of boolean values. Therefore, each array
represents a group of proof states sharing certain properties.
PaMpeRfirstpreprocessesthisdatabaseandgeneratesadatabase
foreachproofmethod.Then, PaMpeRappliesaregressionalgorithm
toeachdatabaseandcreatesaregressiontreeforeachproofmethod.
This regression algorithm attempts to discover combinations of
featuresusefultorecommendwhichproofmethodtoapply.Each
treecorrespondstoacertainproofmethod,andeachnodeinatreecorrespondstoagroupofproofstates,andthevaluetaggedtoeachleafnodeshowshowlikelyitisthatthemethodrepresentedbythe
tree is applied to these proof states according to the proof corpora
used as training sample.
For the recommendation phase, PaMpeRoffers three commands,
which_method ,why_method ,andrank_method .Thewhich_method
commandfirstabstractsthestateintoavectorofbooleanvaluesusing
PaMpeR’s feature extractor. Then, PaMpeRlooks up the re-
gression trees and presents its recommendations in Isabelle/jEdit’s
outputpanel.Ifyouwonderwhy PaMpeRrecommendscertainmeth-
ods, for example auto, to your proof state, type why_method auto .
Then,PaMpeRtells you why it recommended autoto the proof
stateinjEdit’soutputpanel.Ifyouarecurioushow PaMpeRranks
a certain method, let us say intro_classes , typerank_method
intro_classes .Thiscommandshows intro_classes ’srankgiven
363
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. PaMpeR: Proof Method Recommendation System for Isabelle/HOL ASE ’18, September 3–7, 2018, Montpellier, France
full feature extractor
preprocess
decision tree construction
fast feature extractor
?feature vector
lookupdatabase
large proof corpora
proof method 
recommendationpreparation phase
recommendation phase
proof 
state
proof 
engineer
Figure 1: Proof attempt with PaMpeR.
byPaMpeRin comparison to other proof methods. In the following,
we describe these steps in detail.
3 PROCESSING LARGE PROOF CORPORA
Thekeycomponentof PaMpeRisitsfeatureextractor:theextractor
converts proof goals, chained facts, and proof contexts into arrays
of boolean values by applying assertions to them.
3.1 Representing a Proof State as an Array of
Boolean Values
Currently we employ 108 assertions manually written in Isabelle’s
implementation language, Standard ML, based on our expertise in
Isabelle/HOL.Table 1showsselectedassertionsweusedin PaMpeR.
Mostoftheseassertionsfallintotwocategories:assertionsabout
proof goals themselves, and assertions about the relation between
proof goals and information stored in the corresponding proof
context.
Notethat PaMpeR’sassertionsdonotdirectlyrelyonanyuser-
defined constants because PaMpeR’s developers cannot access con-
cretedefinitionsofuser-definedconstantswhendeveloping PaMpeR.
For example, we can check if the first proof goal has a constant
definedinthe Set.thy fileinIsabelle/HOL,butwecannotcheckif
thatsub-goalhasaconstantdefinedintheproofscriptthatsome
user developed after we released PaMpeR.However,byinvestigatinghowIsabelle/HOLworks,weimple-
mented assertions that can check the meta-information of proof
goalevenwithoutknowingtheirconcretespecificationswhende-
veloping PaMpeR. For example, the lemma presented in Section
2.1hasafunction, sep,whichwasdefinedwiththe funkeyword.
PaMpeR’sfeatureextractorchecksiftheunderlyingproofcontext
contains a lemma of name sep.elims . If the context has such a
lemma,PaMpeRinfers that a user defined sepusing either the fun
keyword or the function keyword, rather than other keywords
such asprimcorec ordefinition.
We wrote some assertions to reflect our own expertise in Is-
abelle/HOL.Oneexampleistheassertionthatchecksiftheproof
goalorchainedfactsinvolvetheconstant, Filter.eventually ,de-
fined in Isabelle’s standard library. We developed such an assertion
because we knew that the proof method called eventually_elim
can handlemany proof goals involvingthis constant. Butin some
cases we were not sure which assertion can be useful to decide
whichmethodtouse.Forexample,wehaveassertionstocheckifa
proofgoalhasconstantsdefinedin Set.thy,Int.thy,orList.thy
as these theory files define commonly used concepts in theorem
proving.Buttheireffectstoproofmethodselectionwereunclear
until we conducted an extensive evaluation described in Section 6.
More importantly, we did not know numerical estimates on
which assertion is more useful than others when developing these
assertions. For instance, we guessed that the assertion to check
364
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yutaka Nagashima and Yilun He
Table 1: Selected Assertions.
•Assertions about proof goals themselves.
–constants defined in Isabelle’s standard library
∗check if the first goal has the BNF_Def.rel_fun constant or the Fun.map_fun constant.
∗check if the first goal has Orderings.ord_class.less_eq, Orderings.ord_class.less,o rGroups.plus_class.plus.
∗check if the first goal and its chained facts have Filter.eventually
–constants defined in Isabelle’s standard library at certain locations in the first proof goal
∗check if the outermost constant of the first goal is the meta-logic universal quantifier
∗check if the first goal has the HOL existential quantifier but not as the outermost constant
–terms of certain types defined in Isabelle’s standard library
∗check if the first goal has a term of type Word.word
∗check if the first goal has a schematic variable
–existence of constants defined in certain theory files
∗check if the first goal has a constant defined in the Nattheory
∗check if the first goal has a constant defined in the Realtheory
∗check if the first goal has a constant defined in the Settheory
•Assertions about the relation between proof goals and proof contexts.
–types defined with a certain Isar keyword
∗check if the goal has a term of a type defined with the datatype keyword
∗check if the goal has a term of a type defined with the codatatype keyword
∗check if the goal has a term of a type defined with the recordkeyword
–constants defined with a certain Isar keyword
∗check if the goal has a constant defined with the lift_definition keyword
∗check if the goal has a constant defined with the primcorec keyword
∗check if the goal has a constant defined with the inductive keyword or inductive_set keyword.
the use of the constant Filter.eventually to be useful to rec-
ommend the use of the eventually_elim method, but we did not
have means of comparing the accuracy of this guess with other
hints prior to this project. To obtain numerical assessments for
proofmethodprediction,weappliedthemulti-outputregression
algorithm described in Section 4.
The evaluation in Section 6corroborates that it is possible to
derivemeaningfuladviceaboutproofmethods.Thisimpliesthat
somepartsoftheexpertisenecessarytoselectappropriateproof
methodsarebasedonthemeta-informationaboutproofstatesor
theinformationavailablewithinIsabelle’sstandardlibrary,andour
assertion-based feature extractor preserves some essenceof proof
states while converting them into simpler format.
3.2 Database Extraction from Large Proof
Corpora
Thefirststepofthepreparationphaseistobuildadatabasefrom
existing proof corpora. We modified the proof method application
commands, applyandby,inIsabelleandimplementedalogging
mechanism to build the database. The modified applyandbytake
the following steps to generate the database:
(1) apply assertions to the current proof state,
(2) represent the proof state as an array of boolean values,
(3) record which method is used to that array,(4)
apply the method as the standard applyorbycommand,
accordingly.This step requires a slight modification to the Isabelle source code
toallowustooverwritethedefinitionofthesecommand.Thisway,
we build its database by running the target proof scripts.
The current version of PaMpeRavailable at our website [ 1]i s
based on the database extracted from Isabelle’s standard library
and the AFP, but the database extraction mechanism is not specific
tothislibrary.Incaseusersprefertooptimise PaMpeR’srecommen-
dationfortheirownproofscripts,theycantakethesameapproach
following the instructions at our website [ 1], even though this
process tends to require significant computational resources.
This overwriting of applyandbyis the only modification we
made to Isabelle’s source code, and we did so only to build the
database for our machine learning algorithm. As long as users
choose to use the off-the-shelf default learning results, they can
usePaMpeRwithoutevermodifyingIsabelle’ssourcecode.Inthat
case,theyonlyhavetoincludethetheoryfile PaMpeR/PaMpeR.thy
into their own theory file using the Isar keyword importjust as a
normal theory file to use PaMpeR.
Notethatloggingmechanismignoresthe applycommandsthat
contain composite proof methods to avoid data pollution. Whenmultiple proof methods are combined within a single command,
the naive logging approach would record proof steps that are back-
tracked to produce the final result. One exemplary data point in an
extracted database would look as the following:
induct, [1,0,0,1,0,0,0,0,1,0,0,1,0,...]
whereinductisthenameofmethodappliedtothisproofstateand
thenthelementin thelistshowsthe resultofthe nthassertionof
the feature extractor when applied to the proof state.
365
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. PaMpeR: Proof Method Recommendation System for Isabelle/HOL ASE ’18, September 3–7, 2018, Montpellier, France
The default database construction from Isabelle standard library
andtheAFPtookabout6,021hours43minutesofCPUtime,pro-
ducing a database consisting of 425,334 unique data points. We
used three multi-core server machines3to reduce the clock time
necessary to obtain this dataset. Unfortunately, this database is
heavilyimbalanced:someproofmethodsareusedfarmoreoften
than others. We discuss how this imbalance influenced the quality
ofPaMpeR’s recommendation in Section 6.
4 MACHINE LEARNING DATABASES
In this section, we explain the multi-output regression tree con-struction algorithm we implemented in Standard ML for
PaMpeR.
We chose a multi-output algorithm because there are in general
multiplevalidproofmethodsforeachproofgoal,andwechosea
regressionalgorithm ratherthanclassificationalgorithm because
wewouldliketoprovidenumericalestimatesabouthowlikelyeach
methodwouldbeusefultoagivenproofgoal.Wechosearegres-
sion tree construction algorithm [ 3] because this simple algorithm
allows us to produce qualitative explanations as to why PaMpeR
recommendscertainmethodsanditworkswell forsmalldatasets
for rarely used methods as shown in Section 6.The comparisons of
various machine learning algorithms remain as our future work.
4.1 Preprocess the Database
We first preprocess the database generated in Section 3.2. This
processproduces aseparatedatabase foreachproof methodfrom
therawdatabase,whichdescribestheuseofmostproofmethods
appearing in the target proof corpora.
Among the class of problem transformation methods for multi-
outputregressionproblems,thisstraightforwardapproachiscalled
single-target method: it first transforms a single multi-output prob-
lemintoseveralsingle-targetproblems,thenappliesaregression
algorithm to each of them separately, then combines the results of
eachregressionalgorithmtobuildasinglepredictorfortheoriginal
multi-output problem.
Forexample,ifourpreprocessorfindstheexamplelinediscussed
inSection 3.2,itconsidersthatanidealuserrepresentedbytheproof
corpora decided to use the inductmethod but not other methods,
suchasautoorcoinduction ,andproducesthefollowinglinein
the database for induct:
used, [1,0,0,1,0,0,0,0,1,0,0,1,0,...]
Andthepreprocessoraddsthefollowinglineinthedatabasesfor
other proof methods appearing in the proof corpora:
not, [1,0,0,1,0,0,0,0,1,0,0,1,0,...]
Notethattheresultingdatabasesdonotalwaysrepresentaprov-
ablycorrectchoiceofproofmethodsbutconservativeestimates.In
principle,therecouldbemultipleequallyvalidproofmethodsforasingleproofstate,butexistingproofcorporadescribeonlyoneway
of attacking it. For example, Nipkow et al.applied the induct_tac
method tothe lemmain Section 2.1,but wecan prove thislemma
withanothermethodformathematicalinduction( induction )as
follows:
3Oneofthemhas2Intel(R)Xeon(R)CPUsE5-2698v3@2.30GHzwith16coresfor
eachandwithhyperthreading,theothertwohave2Intel(R)Xeon(R)CPUsE5-2690
v4 @ 2.60GHz with 14 cores for each with hyperthreading.lemma "map f (sep x xs) = sep (f x) (map f xs)"
apply(induction x xs rule: sep.induct)apply simp_all done
For this reason, this preprocessing may misjudge some methods
tobeinappropriatetoaproofstaterepresentedbyafeaturevectorin
some cases. Unfortunately, exploring all the possible combinations
of proof methods for each case is computationally infeasible: some
proof methods work well only when they are followed by otherproof methods or they are applied with certain arguments, and
thecombinationoftheseproofmethodsandargumentsexplodes
quickly.
On the other hand, we can reasonably expect that the proof
method appearing in our training sample is the right choice tothe proof state represented by the feature vector, since Isabellemechanicallycheckstheproofscripts.Furthermore,webuiltthe
default recommendation using Isabelle’s standard library, which
was developed by experienced Isabelle developers, and the AFP,
whichacceptsnewproofsonlyafterpeerreviewsbyIsabelleexperts.
This allowed us to avoid low quality proof scripts that Isabelle
canmerelyprocessbutareinappropriate.Therefore,weconsider
the approximation PaMpeR’s preprocessor makes to be a realistic
pointofcompromiseandshowtheeffectivenessofthisapproach
in Section 6.
4.2 Regression Tree Construction
After preprocessing, we apply our regression tree construction
algorithm to each created database separately. We implemented
our treeconstruction algorithm from scratchin Standard MLfor
better flexibility and tool integration.
Ingeneral,thegoaloftheregressiontreeconstructionistoparti-
tion the feature space described in each database into partitions of
sub-spacesthatleadtotheminimalResidualSumofSquares(RSS)4
whileavoidingover-fitting.Intuitively,RSSdenotesthediscrepancy
between the data and estimation based on a model. The RSS in our
problem is defined as follows:
RSS=J/summationdisplay.1
j=1/summationdisplay.1
i∈Rj(usedi−/hatwidestusedRj)2(1)
where Rjstands for the jth sub-space, to which certain data points
(represented as lines in database) belong. The value of usediis 1.0
if the data point represented by the subscript isays the method
was applied to the feature vector, and it is 0 .0 if the data point
representedbythesubscript isaysotherwise. /hatwidestusedRjistheaverage
valueof usedamongthedatapointspertainingtothesub-space Rj.
Computing the RSS for every possible partition of the data-
base under consideration is computational infeasible. Therefore,
PaMpeR’s tree construction takes a top-down, greedy approach,
calledrecursive binary splitting [10].
Inrecursivebinarysplitting,westartconstructingtheregression
tree from the root node, which corresponds to the entire dataset
for a given method. First, we select a feature in such a way we can
achieve the greatest reduction in RSS at this particular step. WefindsuchfeaturebycomputingthereductionoftheRSSbyeach
4RSS is also known as the sum of squared residuals (SSR).
366
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yutaka Nagashima and Yilun He
feature by one level. For each feature, we split the database into
two sub-spaces, Rused(j)andRnot(j)as follows:
Rused(j)={used|usedj=1.0}and
Rnot(j)={used|usedj=0.0}(2)
where jstands for the number representing each feature. Then, for
each feature represented by j, we compute the following value:
/summationdisplay.1
i:xi∈Rused(j)(usedi−/hatwidestusedRused(j))2+
/summationdisplay.1
i:xi∈Rnot(j)(usedi−/hatwidestusedRnot(j))2(3)
and choose the feature jthat minimizes this value.
Second, we repeat this partition procedure to each emerging
sub-node of the regression tree under construction until the depth
of tree hits our pre-defined upper limit, three.
After reaching the maximum depth, we compute the average
valueof used(j)inthecorrespondingsub-space Rforeachleafnode.
We consider this value as the expectation that the method is useful
toproofstatesabstractedtothecombinationoffeaturevaluesto
that leaf node.
PaMpeRrecords these regression trees in a text file, so that users
canavoidthecomputationallyintensivedataextractionandregres-
siontree constructionprocesses unlessthey wanttooptimize the
learning results based on their own proof corpora.
Notethatifweaddmoreassertionstoourfeatureextractorin
future, the complexity of this algorithm increases linearly with
the number of assertions given a fixed depth of regression tree,
since the partition only takes the best step at each level instead of
exploring all the combinations of partitions.
5 RECOMMENDATION PHASE
Once finishing building regression trees for each proof methodappeared in the given proof corpora, one can extract recommen-
dations from PaMpeR. When imported to users’ theory file, PaMpeR
automaticallyreadsthesetreesusingthe read_regression_trees
command in PaMpeR/PaMpeR.thy.
PaMpeRprovidesthreenewcommandstoprovidetwokindsofin-
formation:the which_method commandtellswhichproofmethods
are likely to be useful for a given proof state; the why_method com-
mandtakesanameofproofmethodandtellswhy PaMpeRwould
recommendtheproofmethodfortheproofstate;the rank_method
commandshowstherankofagivenmethodtotheproofstatein
comparisontootherproofmethods.Inthefollowing,weexplain
how these three commands produce recommendations from the
regression trees produced in the preparation phase.
5.1 Faster Feature Extractor
Before applying the machine learning algorithm, we were not sure
which assertion produces valuable features, but after applying the
machine learning algorithm, we can judge which assertions arenot useful, by checking which features are used to branch each
regressiontree.The build_fast_feature_extractor command
inPaMpeR/ PaMpeR.thy constructs a fasterfeature extractor fromthe regression trees built in the preparation phase and the fullfeatureextractortoreducethewaitingtimeof
PaMpeR’susers.It
buildsthefasterfeatureextractorbyremovingassertionsthatdo
not result in a branch in the regression trees.
5.2 The which_method Command
Whenusersinvokethe which_method command, PaMpeRapplies
the faster feature extractor to convert the ongoing proof state into
afeaturevector,whichconsistsofthosefeaturesthataredeemedto
beimportanttomakeare commendation.Thespeedof thisfaster
feature vector depends on both the regression trees and what each
proofstatecontains.Asaruleofthumb,iftheproofgoalhasless
terms, it tends to spend less time.
Then,PaMpeRlooksupthecorrespondingnodeineachregres-
siontreeanddecidestheexpectationthatthemethodistheright
choicefortheproofstaterepresentedbythefeaturevector. PaMpeR
computesthisvalueforeachproofmethoditencounteredinthe
trainingproofcorpora,bylookingupanodeineachregressiontree.Finally,
PaMpeRcomparestheseexpectationsandshowsthe15most
promising proofmethodswith their expectations in Isabelle/jEdit’s
output panel. In the on-going example from Section 2.1, a user can
knowwhichmethodtousebytypingthe which_method command
as follows:
lemma "map f (sep x xs) = sep (f x) (map f xs)"
which_method
Then,PaMpeRshows the following message in the output panel for
the top 15 methods5:
Promising methods for this proof goal are:
simp with expectation of 0.4119
auto with expectation of 0.1593
rule with expectation of 0.0874
induction with expectation of 0.06137metis with expectation of 0.05260 ...
Attentivereadersmighthavenoticedthat PaMpeR’srecommenda-
tions are not identical to the model answer provided by Nipkow et
al.This,however,doesnotimmediatelymean PaMpeR’srecommen-
dationisnotvaluable:infact, PaMpeRrecommendedthe induction
methodatthefourthplaceoutof239proofmethods,and induction
isalsoavalidmethodforthisproofgoalasdiscussedinSection 4.1.
5.3 The why_method Command
Our rather straightforward machine learning algorithm makes
PaMpeR’s recommendation explainable. If you wonder why PaMpeR
recommendsacertainmethod,forexample case_tac ,toyourproof
goal,type why_method case_tac intheproofscript. PaMpeRfirst
checksfeaturesusedtoevaluatetheexpectationforthemethodandtheirfeaturevalues.Second,
PaMpeRshowsqualitativeexplanations
taggedtoboththesefeaturesandtheirvaluesinjEdit’soutput.If
you wonder why PaMpeRrecommended induction in the above
example, type the following:
lemma "map f (sep x xs) = sep (f x) (map f xs)"
why_method induction
Then, you will see this message in jEdit’s output panel:
5Note that we truncated the message due to the space restriction here.
367
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. PaMpeR: Proof Method Recommendation System for Isabelle/HOL ASE ’18, September 3–7, 2018, Montpellier, France
Because it is not true that the context has locally
defined assumptions.Because the underlying proof context has a recursivesimplification rule related to a constant appearing in
the first subgoal.
The first reason corresponds to the first branching at the root node
in the regression tree forthe induction method, andthe second
reason corresponds to the second branching in the tree. In this
case,PaMpeRfound thatthe proof goalinvolves the constant, sep,
and the underlying proof context contains a simplification rule,
sep.simps(3) , which involves a recursive call of sepas following:
sep.simp(3):
sep ?a (?x # ?y # ?zs) = ?x # ?a # sep ?a (?y # ?zs)
5.4 The rank_method Command
Sometimes users already have a guess as to which proof method
would be useful to their proof state, but they want to know how
PaMpeRrankstheproofmethodinmind.Continuingwiththeabove
example,ifyouwanttoknowhow PaMpeRranksconduction for
this proof state, type the following:
lemma "map f (sep x xs) = sep (f x) (map f xs)"
rank_method coinduction
Then,PaMpeRwarns you:
coinduction 123 out of 239
indicating that PaMpeRdoes not consider coinduction to be the
right choice for this proof goal, before you waste your time on
emerging sub-goals appearing after applying coinduction.
6 EVALUATION
Weconductedacross-validationtoassesstheaccuracyof PaMpeR’s
which_method command. For this evaluation, we used Isabelle’s
standard library and the AFP as follows: First, we extracted a data-
base from these proof corpora. This database consists of 425,334
data points. Second, we randomly chose 10% of data points in this
databasetocreatetheevaluationdataset.Third,webuiltregression
treesfromtheremaining90%.Thereisnooverlapbetweentheeval-
uation dataset and training dataset. Then, we applied regression
trees to each data point in the evaluation dataset and counted how
oftenPaMpeR’s recommendation coincides with the proof methods
chosen by human proof authors.
Since there are often multiple equallyvalid proof methods for
eachproofstate,itisonlyreasonabletoexpectthat which_method
should be able to recommend the proof method used in the evalua-
tiondatasetasoneofthemostimportantmethodsforeachproof
methodinvocation.Therefore,foreachproofmethod,wemeasuredhowofteneachproofmethodusedintheevaluationdatasetappears
among the top nmethods in PaMpeR’s recommendations.
Table2showstheresultsforthe15proofmethodsthataremost
frequently used in the training data in the descending order.
Forexample,thetoprowfor simpshouldbeinterpretedasfol-
lowing:The simpmethodwasused102,441timesinthetraining
data. This amounts to 26.8% of all proof method invocations inthe training data that are recorded by
PaMpeR. In the evaluation
dataset,simpwas used 11,385 times, which amounts to 26.8% of
proofmethodinvocationsintheevaluationdatasetthatarerecordedbyPaMpeR. For 58% out of 11,385 simpinvocations in the evalu-
ation dataset, PaMpeRpredicted that simpis the most promising
methodforthecorrespondingproofstates.For98%outof11,385
simpinvocationsintheevaluationdataset, PaMpeRrecommended
thatsimpis either the most promising method or the second most
promising method for the corresponding proof states.
Notethatthenumberspresentedinthistablearenotthesuccess
ratesofPaMpeR’srecommendationbutitsconservativeestimates.
AssumePaMpeRrecommends simpas the most promising method
andautoas the second most promising method to a proof goal,
saypg, in the evaluation dataset, but the human proof author of
pgchose to apply autoto this proof goal. This does not imme-
diately mean that PaMpeRfailed to recommend autoin the first
place,becauseboth simpandautomightbeequallysuitablefor pg.
Therefore,the58%for simpmentionedaboveshouldbeinterpreted
as follows: PaMpeR’s recommendation coincides with the choice of
experienced Isabelle user for 58% of times where human engineers
appliedsimpwhenPaMpeRisallowedtorecommendonlyoneproof
method, but the real success rate of PaMpeR’s recommendation can
be higher than 58% for these cases. To avoid the confusion with
successrate,weintroducetheterm, coincidencerate,forthismea-
sure. Appendix of our technical report [ 19] presents three tables to
provide the complete list of the evaluation results.
The overall results of this evaluation are as follows: PaMpeR
learnt 239 methods from Isabelle’s standard library and the AFP:
160 of them are defined within Isabelle’s standard library, and the
others are user-defined proof methods specified in the AFP entries.
Out of the 239 proof methods PaMpeRlearnt from the training
dataset, 171 proof methods appeared in the evaluation dataset. Out
ofthese171proofmethodswithintheevaluationdataset,133meth-
ods are defined in Isabelle’s standard library, and 38 methods were
defined by the AFP authors.
Thedistributionofproofmethodusageisheavilyimbalanced.
The three most frequently used proof methods ( simp,auto, and
rule) account for 59.1% of all data points in the training dataset,
and the ten most frequently used methods account for 79.2% in the
trainingdataset.Similarlyintheevaluationdataset,thetopthree
methods account for 58.9%, and the top ten methods for 79.1%.
Fig. 2 illustrates this imbalance, in which the horizontal axis
represents the rank of method usage for a proof method and the
vertical axis stands for the number of methods invocations for
thatproofmethod.Forinstance,thesquarelocatedatthetop-left
cornerdenotesthat themostfrequentlyused proofmethodinthe
trainingdataset( simp)isused102,441times.Andthecirclelocated
at(6,1093)denotes that the sixth most frequently used method
in the evaluation dataset ( fastforce ) is used 1,093 times in the
evaluationdataset.Withtheuseoflogarithmicscaleonthevertical
axis, this figure presents the serious imbalance of proof method
invocations occurring in Isabelle’s standard library and the AFP.
Fig. 3 summarises the overall performance of PaMpeR. In this
figure the horizontal axis represents the number of proof methods
PaMpeRisallowedtorecommend(15bydefault),whereastheverti-
calaxisrepresentsthenumberofproofmethods,forwhich PaMpeR
achieves certain coincidence rates.
Forexample,thesquareat (3,23)meansthat PaMpeRcanachieve
50% of coincidence rate for 23 methods if PaMpeRis allowed to rec-
ommendthreemostpromisingmethods.Similarly, PaMpeRachieves
368
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yutaka Nagashima and Yilun He
Table 2: Evaluation of PaMpeRon 15 most frequently used proof methods.
proof method training % evaluation % 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
simp 102441 26.8 11385 26.8 58 98 99 99 100 100 100 100 100 100 100 100 100 100 100
auto 85097 22.2 9527 22.4 60 94 98 99 100 100 100 100 100 100 100 100 100 100 100
rule 38856 10.2 4150 9.8 3 15 86 99 100 100 100 100 100 100 100 100 100 100 100
blast 23814 6.2 2590 6.1 0 26 26 35 84 95 99 100 100 100 100 100 100 100 100
metis 19771 5.2 2149 5.1 0 0 13 72 84 89 93 96 98 99 100 100 100 100 100
fastforce 9477 2.5 1093 2.6 0 0 0 0 5 54 70 81 89 93 96 96 97 98 98
force 6232 1.6 708 1.7 0 0 0 0 1 9 22 32 40 51 66 77 84 89 94
clarsimp 5984 1.6 628 1.5 0 12 14 14 20 29 39 49 54 57 62 64 66 67 73
cases 5842 1.5 689 1.6 0 0 1 16 16 20 34 54 70 80 86 91 93 95 96
erule 5732 1.5 707 1.7 0 0 15 38 44 53 64 70 76 82 85 87 91 92 93
subst 5655 1.5 619 1.5 0 0 19 19 19 20 22 28 45 58 69 77 82 86 90
rule_tac 5342 1.4 631 1.5 0 14 32 34 44 45 46 47 50 51 52 52 53 57 63
intro 4988 1.3 619 1.5 0 0 5 18 24 39 46 47 48 48 49 57 69 77 84
simp_all 4982 1.3 568 1.3 0 0 0 1 3 6 15 21 26 33 45 60 70 78 83
induct 4884 1.3 568 1.3 0 0 0 1 27 45 49 50 50 51 56 62 71 77 79
50% of coincidence rate for 58 methods when recommending 10
methods and for 72 methods when recommending 15 methods.
Thenumberofmethods PaMpeRthatachievedthefourcoincident
rates(25%,50%,75%,and90%)reachedaplateau when PaMpeRis
allowed to recommend about 60 proof methods.
Overall,PaMpeR’s recommendations tend to coincide with hu-
manengineers’choicewhenIsabellehasonlyonemethodthatis
suitablefortheproofgoalathand,whereas PaMpeR’srecommen-
dations tend to differ from human engineers’ choice when there
are multiple equally valid proof methods for the same goal. For ex-
ample,PaMpeR’s coincidence rates are low for less commonly used
general-purpose methods, such as safe,clarimp,best,bestsimp
because multiple general purpose proof methods can often handle
the same proof goal equally well.
A careful observation at the raw evaluation results provided
in the Appendix of the technical report reveals that PaMpeRpro-
vides valuable recommendations when proof states are best han-
dledbyspecialpurposeproofmethods,suchas unfold_locales ,
transfer, eventually_elim, standard, and so on.
PaMpeR’s regression tree construction does not severely suffer
from the imbalance among proof method invocation, even though
class imbalances often cause problems in other domains such as
frauddetectionandmedicaldiagnosis[ 7].Thecompleteevaluation
resultsinAppendixofthereportshowthat PaMpeRachieved50%
of coincidence rate for 34 proof methods that appear less than 0.1%
of times in the training dataset.
The reason the imbalance did not cause serious problems to
PaMpeRisthatsomeoftheserarelyusedmethodsarespecialised
proof methods, for which we can write assertions that can ab-stract the essence of the problem very well. Another reason is
thefactthatcommonlyusedproofmethodstendtoholdupeach
other’s share, since they address similar problems, lowering expec-
tationsforcommonlyusedgeneralpurposemethodswhereboth
specialisedmethodsandgeneralpurposemethodscandischarge
proof goals.On the other hand, PaMpeRdid not produce valuable recommen-
dations to some special purpose proof methods, such as vector
andnormalization ,forwhichwedidnotmanagetodevelopas-
sertions that capture the properties shared by the proof goals that
thesemethodscanhandlewell.Writingsuitableassertionsforthese
remain as our future work.
Some of the proof methods appearing in our evaluation dataset
areclearlyoutsidethescopeof PaMpeR.Forexample, cartouche ,
tactic,ml_tactic ,rotate_tac donothavemuchsemanticmean-
ing:tacticissimplyaninterfacebetweenIsabelle’ssourcecode
language,StandardML,andIsabelle’sprooflanguage,Isar,whereas
rotate_tac simplyrotatestheorderofpremiseswhenaproofgoal
hasmultiplepremises.Anothergoodexampleofproofmethodsout-side the scope of
PaMpeRis themy_simp method. This method was
definedinthestandardlibrarytotestthedomainspecificlanguage,
Eisbach,forwritingnewproofmethods: my_simpissimplyasyn-
onymofsimpandnobodyisexpectedtouse my_simp.Predicting
such methods is not a very meaningful task for PaMpeR.
To our surprise, Table V in Appendix of our technical report
[19] shows that PaMpeR’s recommendation achieved 50% of coin-
cidenceratefor 12methodsoutof38 user-defined proofmethods
defined outside Isabelle’s standard library appearing in the evalua-
tion dataset when PaMpeRis allowed to provide 15 most promising
proofmethods,eventhough PaMpeR’sdevelopersdidnotknowany-
thing about these proof methods at the time of development. This
suggeststhatonedoesnotneedtoknowtheproblemspecificinfor-
mationaboutproofgoalstopredicttheuseofsomeuser-defined
proof methods. For example, PaMpeRachieves 100% of coincidence
rate forseprefwhen allowed to recommend only four methods,
by checking if the first sub-goal has a schematic variable and if the
first sub-goal has variables of type record.
7 DISCUSSION AND FUTURE WORK
PriortoPaMpeR,Isabellehadthe print_methods command,which
merely lists the proof methods defined in the corresponding proof
contextinalphabeticalorderignoringthepropertiesoftheproof
369
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. PaMpeR: Proof Method Recommendation System for Isabelle/HOL ASE ’18, September 3–7, 2018, Montpellier, France
20 40 60 80 100 120 140 1601101001,00010,000100,000
Nth most commonly used proof method.Number of method invocation.in training dataset
in evaluation dataset
Figure 2: Fig.2: Method usage in large proof corpora.
11 02 03 04 05 06 0120406080100120140160
Number of methods PaMpeRrecommends.Methods above four coincidence rates.more than 25%
more than 50%
more than 75%
more than 90%
Figure 3: Fig. 3: Coincidence rate for PaMpeR.
goalathand.Therefore,newIsabelle/HOLusershavetogothrough
various documentations and the archive of mailing lists to learn
how to prove lemmas in Isabelle/HOL independently.
Choosing the right methods was a difficult task for new ITP
users especially when they should choose special-purpose proof
methods,sincenewuserstendnottoknoweventheexistenceof
those rarely used proof methods. Some proof methods are strongly
related to certain definitional mechanisms in Isabelle. Therefore,
whenIsabelleexpertsusesuchdefinitionalmechanisms,theycan
often guess which proof methods they should use later. But this
is not an easy task for new users. And this problem is becoming
severernowadays,sincelargescaletheoremprovingprojectsare
slowlybecomingpopularandnewITPusersoftenhavetotakeover
proofscripts developedbyothers andtheyalso havetodischargeproof goals specified by others. PaMpeRaddressed this problem by
systematicallytransferringexperiencedusers’knowledgetoless
experienced users. We plan to keep improving PaMpeRby incorpo-
rating other Isabelle users intuitions as assertions.
Our manually written feature extractor may seem to be naive
compared to the recent success in machine learning research: in
someproblemdomains,suchasimagerecognitionandthegameof
Go, deep neural networks extract features of the subject matters
via expensive training. Indeed, others have applied deep neural
networks to theorem proving, but without much success [9, 16].
The two major problems of automatic feature extraction for the-
oremprovingisthelackofenormousdatabaseneededtotraindeep
neuralnetworksandtheexpressivenatureoftheunderlyinglan-
guage,i.e.logic.Thesecondproblem,theexpressivenatureoflogic,
contributes to the first problem: self-respecting proof engineers
tend to replace multiple similar propositions with one proposition
fromwhichonecaneasilyconcludesimilarpropositions,aimingat
a succinct presentation of the underlying concept.
What is worse, when working with modern ITPs, it is often
notenoughtoreasonaboutaproofgoal,butonealsohastotake
itsproofcontextintoconsideration.Aproofcontextusuallycon-
tains numerous auxiliary lemmas and nested definitions, and each
of them is a syntax tree, making the effective automatic feature
extraction harder.
Furthermore,wheneveraproofauthordefinesanewconstant
or prove a new lemma, Isabelle/HOL changes the underlying proof
context,whichaffectshowoneshouldattackproofgoalsdefined
withinthisproofcontext.Andproofauthorsdoaddnewdefinitions
because they use ITPs as specification tools as well as tools for
theoremproving.Someofthesechangesareminormodificationstoproof states that do not severely affect how to attack proof goals in
the following proof scripts, but in general changing proof contexts
results in, sometimes unexpected, problems.
Forthisreason,eventhoughtheITPcommunityhaslargeproof
corpora, we essentially deal with different problems in each line of
proofcorpus.Forexample,eventheAFPhas396articlesconsistingofmorethan100,000lemmas,only4articlesareusedbymorethan
10 articles in the AFP, indicating that many authors work on their
ownspecifications,creatingnewproblems.Thisresultsinanim-
portant difference between theorem proving in an expressive logic
and other machine learning domains, such as image recognition
where one can collect numerous instances of similar objects.
Weaddressedthisproblemwithhuman-machinecooperation,
thephilosophythatunderpinsITPs.Eventhoughitishardtoex-
tract features automatically, experienced ITP users know that they
can discharge many proof goals with shallow reasoning. We en-
codedexperiencedIsabelleusers’expertiseasassertionstosimulate
their shallow reasoning. Since these assertions are carefully hand-
written in Isabelle/ML, they can extract features of proof states
(including proof goal, chained facts, and its context) despite the
above mentioned problems.
Currently PaMpeRrecommends only which methods to use and
shows why it suggests that method. This is enough for special pur-
posemethodsthatdonottakeparameters.Forothermethods,such
asinduct, it is often indispensable to pass the correct parameters
to guide methods. If you prefer to know which arguments to pass
to the proof method PaMpeRrecommends, we would invite you to
370
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yutaka Nagashima and Yilun He
usePSL[20],theproofstrategylanguageforIsabelle/HOL,which
attemptstofindtherightcombinationofargumentsviaaniterative
deepening depth first search based on rough ideas about which
methodto use.If youwant tohavethose roughideas, use PaMpeR.
PaMpeRconstructs regression trees of a fixed height. We set the
heighttoasmallnumber,three,toavoidover-fitting.Itmightbe
possible that advanced pruning methods can improve the accuracy
ofPaMpeR’s recommendation. Furthermore, since we developed
108 assertions based on our limited expertise, it is likely that we
havemissedoutinformationvaluabletorecommendproofmethods
when abstracting proof states using assertions.
Our cross-validation showed that some simple assertions, such
ascheckingtheexistenceofcertainconstantsinproofgoals,turned
out to be useful. Therefore, it might be possible to find more useful
assertionsbysystematicallyenumeratingmoreassertionsofthis
kindtochecktheexistenceofotherconstantsappearinginproof
corpora. Unfortunately, the database construction based on 108
assertions already consumed serious computational resources, and
databaseconstructionbasedongeneratedassertionsremainsasour
futureworkduetothelimitationofresourcescurrentlyavailable
toPaMpeR’s developers.
Whenconductingthecross-evaluation,wefocusedonthecoinci-
dencerateforeachmethod.Itwouldbeworthwhiletocomparethe
results of PaMpeR’s overall coincidence rate for all methods with
the corresponding overall coincidence rate that would be producedbyanaivesystemthatrecommendsproofmethodsinorderoftheir
frequency in training data without constructing decision trees.
Finally, our choice of machine learning algorithm is not final.
Wecurrentlyuseregressiontreeconstructionalgorithmbasedona
problem transformation method because the straightforward algo-
rithmletsusproducequalitativeexplanationsof PaMpeR’srecom-
mendation;however,othermachinelearningalgorithmsmightlead
to higher coincidence rates. The comparison of various machine
learning algorithms on the dataset remains as our future work.
8 CONCLUSION AND RELATED WORK
We presented the design and implementation of PaMpeR. In the
preparation phase, PaMpeRlearns which method to use from ex-
istingproofcorporausingregressiontreeconstructionalgorithm.
In the recommendation phase, PaMpeRrecommends which proof
methods to use to a given proof goal and explains why it suggests
that method. Our evaluation showed that PaMpeRtends to provide
valuablerecommendationsespeciallyforspecialisedproofmeth-
ods, which new Isabelle users tend not to be aware of. We also
identifiedproblemsthatarisewhenapplyingmachinelearningto
proof method recommendation and proposed our solution to them.
Related Work. ML4PG [8] extends a proof editor, Proof General,
tocollectproofstatisticsaboutshapesofgoals,sequenceofapplied
tactics, and proof tree structures. It also clusters the gathered data
using machine learning algorithms in MATLAB and Weka and pro-
vides proof hints during proof developments. Based on learning,
ML4PGlistssimilarproofgoalsprovedsofar,fromwhichuserscan
inferhowtoattacktheproofgoalathand,while PaMpeRdirectly
works on proof methods. Compared to ML4PG, PaMpeR’s featureextractor is implemented within Isabelle/ML, which made it pos-
sibletoinvestigatenotonlyproofgoalsthemselvesbutalsotheir
surrounding proof context.
Gauthier et al.developed TacticToe for HOL4 [ 4]. It selects
proved lemmas similar to the current proof goal using premise
selectionandappliestacticsusedtothesesimilargoalstodischarge
the current proof goal. Compared to TacticToe, the abstraction via
assertions allows PaMpeRto provide valuable recommendations
even when similar goals do not exist in the problem domain.
Several people applied machine learning techniques to improve
the so-called Hammer-style tools. For Isabelle/HOL, both MePo[
18] and MaSh [ 13] decreased the quantity of facts passed to the
automaticproverswhile increasingtheirqualityto improveSledge-
hammer’sperformance.Theirapproachesattempttochoosefacts
that are likely to be useful to the given proof goal, while PaMpeR
suggests proof methods that are likely to be useful to the goal.
MePo judgesthe relevance offacts by checkingthe occurrence
ofsymbolsappearinginproofgoalsandavailablefacts,whileMaSh
computes the relevance using sparse naive Bayes and k Nearest
Neighbours.Theydetectsimilaritiesbetweenproofgoalsandavail-
ablefactsbycheckingmostlyformalization-specificinformationand only two piece of meta information, while
PaMpeRdiscards
mostofproblemspecificinformationandfocusonmetainforma-
tion of proof goals: the choice of relevant fact is a problem specific
question, while the choice of proof method largely depends on
which Isabelle’s subsystem is used to specify a proof goal.
The original version of MaSh was using machine learning li-
brariesinPython,andBlanchette etal.portedthemfromPython
to Standard ML for better efficiency and reliability. Similarly, anearly version of
PaMpeRwas also using a Python library [ 23] un-
tilweimplementedtheregressiontreeconstructionalgorithmin
Standard ML forbetter toolintegration andflexibility. BothMaSh
andPaMpeRrecord learning results in persistent states outside the
main memory, so that users can preserve the learning results even
after shutting down Isabelle.
Blanchette etal.analysedtheAFP,lookingatsizesanddepen-
dencies for theory files [ 2]. Matichuk et al.investigated the seL4
proofs and two articles in the AFP to find the relationship between
the size of statement and the size of proof [ 17]. None of them anal-
ysed the occurrence of proof methods in their target proof corpora
nor developed a recommendation system based on their results.
Moreover, PaMpeR’sdatabaseconstructionismoreactivecompared
to their work: it applies 108 hand-written assertions to analyse the
properties of not only each proof goal but also the relationship
between each goal and its background context and chained facts.
ACKNOWLEDGMENTS
The authors would like to the anonymous referees at ITP2016,
ITP2017,andASE2018fortheirvaluablecommentsandhelpfulsug-gestions.ThisworkwassupportedbytheEuropeanRegionalDevel-
opment Fund under the project AI & Reasoning (reg. no.CZ.02.1.01
/0.0/ 0.0/ 15_003/ 0000466)
REFERENCES
[1]2018. PSL with PGT: CICM2018 for Isabelle2017. (2018). https://github.
com/data61/PSL/releases/tag/v0.1.1 To usePaMpeR, one first needs to install
Isabelle/HOL, which is distributed at https://isabelle.in.tum.de/.
371
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. PaMpeR: Proof Method Recommendation System for Isabelle/HOL ASE ’18, September 3–7, 2018, Montpellier, France
[2]JasminChristianBlanchette,MaximilianP.L.Haslbeck,DanielMatichuk,and
TobiasNipkow.2015.MiningtheArchiveofFormalProofs.In IntelligentComputer
Mathematics-InternationalConference,CICM2015,Washington,DC,USA,July
13-17, 2015, Proceedings (Lecture Notes in Computer Science), Manfred Kerber,
JacquesCarette,CezaryKaliszyk,FlorianRabe,andVolkerSorge(Eds.),Vol.9150.
Springer, 3–17. https://doi.org/10.1007/978-3-319-20615-8_1
[3]LeoBreiman,J.H.Friedman,R.A.Olshen,andC.J.Stone.1984. Classification
and Regression Trees. Wadsworth.
[4]Thibault Gauthier, Cezary Kaliszyk, and Josef Urban. 2017. TacticToe: Learning
to Reason with HOL4 Tactics. In LPAR-21, 21st International Conference on Logic
forProgramming,ArtificialIntelligenceandReasoning,Maun,Botswana,May7-12,
2017 (EPiCSeriesinComputing),ThomasEiterandDavidSands(Eds.),Vol.46.
EasyChair, 125–143. http://www.easychair.org/publications/paper/340355
[5]Georges Gonthier. 2007. The Four Colour Theorem: Engineering of a Formal
Proof.In ComputerMathematics,8thAsianSymposium,ASCM2007,Singapore,
December 15-17, 2007. Revised and Invited Papers (Lecture Notes in Computer
Science),DeepakKapur(Ed.),Vol.5081.Springer,333. https://doi.org/10.1007/
978-3-540-87827-8_28
[6]Thomas C. Hales, Mark Adams, Gertrud Bauer, Dat Tat Dang, John Harrison,
TruongLeHoang,CezaryKaliszyk,VictorMagron,SeanMcLaughlin,ThangTat
Nguyen, Truong Quang Nguyen, Tobias Nipkow, Steven Obua, Joseph Pleso,
Jason M. Rute, Alexey Solovyev, An Hoai Thi Ta, Trung Nam Tran, Diep Thi
Trieu, Josef Urban, Ky Khac Vu, and Roland Zumkeller. 2015. A formal proof
of the Kepler conjecture. CoRRabs/1501.02155 (2015). arXiv:1501.02155 http:
//arxiv.org/abs/1501.02155
[7]HaiboHeandEdwardoA.Garcia.2009. LearningfromImbalancedData. IEEE
Trans.Knowl.DataEng. 21,9(2009),1263–1284. https://doi.org/10.1109/TKDE.
2008.239
[8]Jónathan Heras and Ekaterina Komendantskaya. 2013. ML4PG: proof-mining in
Coq.CoRRabs/1302.6421 (2013). arXiv:1302.6421 http://arxiv.org/abs/1302.6421
[9]Geoffrey Irving, Christian Szegedy, Alexander A. Alemi, Niklas Eén, François
Chollet, and Josef Urban. 2016. DeepMath - Deep Sequence Models for Premise
Selection. In Advances in Neural Information Processing Systems 29: Annual Con-
ference on Neural Information Processing Systems 2016, December 5-10, 2016,Barcelona, Spain, Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Is-
abelleGuyon,andRomanGarnett(Eds.).2235–2243. http://papers.nips.cc/paper/
6280-deepmath-deep-sequence-models-for-premise-selection
[10]Gareth James,Daniela Witten, TrevorHastie, and RobertTibshirani. [n. d.]. An
Introduction to Statistical Learning. https://doi.org/10.1007/978-1-4614-7138-7
[11]GerwinKlein,JuneAndronick,KevinElphinstone,GernotHeiser,DavidCock,
Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael
Norrish, Thomas Sewell, Harvey Tuch, and Simon Winwood. 2010. seL4: formal
verification of an operating-system kernel. Commun. ACM 53, 6 (2010), 107–115.
https://doi.org/10.1145/1743546.1743574
[12]Gerwin Klein, Tobias Nipkow, Larry Paulson, and Rene Thiemann. [n. d.]. .
https://www.isa-afp.org/
[13]Daniel Kühlwein, Jasmin Christian Blanchette, Cezary Kaliszyk, and Josef Urban.
2013.MaSh:MachineLearningforSledgehammer.In InteractiveTheoremProving-
4thInternationalConference,ITP2013,Rennes,France,July22-26,2013.Proceedings
(LectureNotesinComputerScience),SandrineBlazy,ChristinePaulin-Mohring,andDavidPichardie(Eds.),Vol.7998.Springer,35–50. https://doi.org/10.1007/
978-3-642-39634-2_6
[14]Ramana Kumar, Magnus O. Myreen, Michael Norrish, and Scott Owens. 2014.
CakeML:averifiedimplementationofML.In The41stAnnualACMSIGPLAN-
SIGACTSymposiumonPrinciplesofProgrammingLanguages,POPL’14,SanDiego,
CA, USA, January 20-21, 2014, Suresh Jagannathan and Peter Sewell (Eds.). ACM,
179–192. https://doi.org/10.1145/2535838.2535841
[15]XavierLeroy.2009. Formalverificationofarealisticcompiler. Commun.ACM
52, 7 (2009), 107–115. https://doi.org/10.1145/1538788.1538814
[16]Sarah M. Loos, Geoffrey Irving, Christian Szegedy, and Cezary Kaliszyk. 2017.DeepNetworkGuidedProofSearch.In LPAR-21,21stInternational Conference
on Logic for Programming, Artificial Intelligence and Reasoning, Maun, Botswana,
May 7-12, 2017 (EPiC Series in Computing), Thomas Eiter and David Sands (Eds.),
Vol.46.EasyChair,85–105. http://www.easychair.org/publications/paper/340345
[17]DanielMatichuk,TobyC.Murray,JuneAndronick,D.RossJeffery,GerwinKlein,
andMarkStaples.2015. EmpiricalStudyTowardsaLeadingIndicatorforCost
ofFormalSoftwareVerification.In 37thIEEE/ACMInternationalConferenceon
SoftwareEngineering,ICSE2015,Florence,Italy,May16-24,2015,Volume1,Antonia
Bertolino, Gerardo Canfora, and Sebastian G. Elbaum (Eds.). IEEE Computer
Society, 722–732. https://doi.org/10.1109/ICSE.2015.85
[18]Jia Meng and Lawrence C. Paulson. 2009. Lightweight relevance filtering for
machine-generated resolution problems. J. Applied Logic 7, 1 (2009), 41–57.
https://doi.org/10.1016/j.jal.2007.07.004
[19]YutakaNagashimaandYilunHe.2018. PaMpeR:ProofMethodRecommendation
System for Isabelle/HOL. CoRRabs/1806.07239 (2018). arXiv:1806.07239 http:
//arxiv.org/abs/1806.07239
[20]Yutaka Nagashima and Ramana Kumar. 2017. A Proof Strategy Languageand Proof Script Generation for Isabelle/HOL. In Automated Deduction -
CADE 26 - 26th International Conference on Automated Deduction, Gothenburg,
Sweden, August 6-11, 2017, Proceedings (Lecture Notes in Computer Science),
Leonardo de Moura (Ed.), Vol. 10395. Springer, 528–545. https://doi.org/10.
1007/978-3-319-63046-5_32
[21]TobiasNipkow,LawrenceC.Paulson,andMarkusWenzel.2002. Isabelle/HOL
- A Proof Assistant for Higher-Order Logic . Lecture Notes in Computer Science,
Vol. 2283. Springer. https://doi.org/10.1007/3-540-45949-9
[22]Lawrence C. Paulson. 2015. A Mechanised Proof of Gödel’s Incompleteness
Theorems Using Nominal Isabelle. J. Autom. Reasoning 55, 1 (2015), 1–37. https:
//doi.org/10.1007/s10817-015-9322-8
[23]Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn:
Machine Learning in Python. Journal of Machine Learning Research 12 (2011),
2825–2830. http://dl.acm.org/citation.cfm?id=2078195
[24]Markus Wenzel. 1999. Isar - A Generic Interpretative Approach to Readable
FormalProofDocuments.In TheoremProvinginHigherOrderLogics,12thInterna-
tional Conference, TPHOLs’99, Nice, France, September, 1999, Proceedings (Lecture
NotesinComputerScience),YvesBertot,GillesDowek,AndréHirschowitz,Chris-
tine Paulin-Mohring, and Laurent Théry (Eds.), Vol. 1690. Springer, 167–184.
https://doi.org/10.1007/3-540-48256-3_12
372
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. 