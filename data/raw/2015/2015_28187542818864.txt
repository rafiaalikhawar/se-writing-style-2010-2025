An Empirical Study on Real Bug Fixes
Hao ZhongyZhendong Suy
Department of Computer Science and Engineering, Shanghai Jiao Tong University, China
yUniversity of California, Davis, USA
zhonghao@sjtu.edu.cn, su@ucdavis.edu
Abstract ‚ÄîSoftware bugs can cause signiÔ¨Åcant Ô¨Ånancial loss
and even the loss of human lives. To reduce such loss, devel-
opers devote substantial efforts to Ô¨Åxing bugs, which generally
requires much expertise and experience. Various approaches
have been proposed to aid debugging. An interesting recent
research direction is automatic program repair , which achieves
promising results, and attracts much academic and industrial
attention. However, people also cast doubt on the effectiveness
and promise of this direction. A key criticism is to what extent
such approaches can Ô¨Åx real bugs. As only research prototypes
for these approaches are available, it is infeasible to address the
criticism by evaluating them directly on real bugs. Instead, in
this paper, we design and develop B UGSTAT, a tool that extracts
and analyzes bug Ô¨Åxes. With B UGSTAT‚Äôs support, we conduct an
empirical study on more than 9,000 real-world bug Ô¨Åxes from
six popular Java projects. Comparing the nature of manual Ô¨Åxes
with automatic program repair, we distill 15 Ô¨Åndings, which are
further summarized into four insights on the two key ingredients
of automatic program repair: fault localization and faulty code
Ô¨Åx. In addition, we provide indirect evidence on the size of the
search space to Ô¨Åx real bugs and Ô¨Ånd that bugs may also reside in
non-source Ô¨Åles. Our results provide useful guidance and insights
for improving the state-of-the-art of automatic program repair.
I. I NTRODUCTION
Over the past decades, software has permeated into almost
every economic activity, and is boosting economic growth
from many perspectives. At the same time, like any other man-
made artifacts, software suffers from various bugs which lead
to incorrect results, deadlocks, or even crashes of the entire
system. When this happens in a critical application, it can
cause great loss of money or even human lives. For example,
Zhivich and Cunningham [40] report that the software of
a hospital miscalculated the proper dosage of radiation for
patients. In this accident, at least eight patients died. To
improve the quality of software, it is desirable to Ô¨Åx as many
bugs as possible. The long battle with software bugs began
ever since software existed. It requires much effort to Ô¨Åx bugs,
e.g.Kim and Whitehead [14] report that the median time for
Ô¨Åxing a single bug is about 200 days.
It has been actively studied to reduce the effort of Ô¨Åxing
bugs. A recent research direction is to investigate automatic
approaches to Ô¨Åxing bugs (see Section V for a detailed
discussion). A typical approach in this direction Ô¨Årst locates
faults of a program, and then mutates the located faulty code
with predeÔ¨Åned operators until the program passes all the
test cases. In this paper, we refer to this line of research
asautomatic program repair , which complements traditional
approaches (e.g. [18]), since it has the potential to deal with a
variety of different bugs. Research in this direction has alreadyproduced promising results. For example, Le Goues et al. [17]
reported that their approach was able to automatically Ô¨Åx 55
out of 105 bugs. However, many people question the positive
results. Existing approaches (e.g. [17], [12]) seem to be able to
Ô¨Åx only simple bugs, due to various limitations. For example,
Table 2 of Kim et al. [12] lists their ten templates to Ô¨Åx bugs,
which are all quite simple. As existing empirical studies ( e.g.
[14]) show that it requires much expertise and time to Ô¨Åx
bugs, many researchers doubt the reported positive results. For
example, at ICSE 2014, Monperrus [22] criticized Kim et al. ‚Äôs
recent work [12] and discussed a number of issues concerning
this research direction.
The preceding criticism shows that the research community
has limited knowledge on the nature of bug Ô¨Åxes. Although
empirical studies exist to understand bug Ô¨Åxes (see Section V
for details), only one [21] analyzed the links between the
nature of bug Ô¨Åxes and automatic program repair. Furthermore,
the empirical study focuses on only one aspect of automatic
program repair, namely the search space of Ô¨Åxing bugs;
most questions raised by Monperrus [22] are still open. For
example, how many bugs could be Ô¨Åxed by automatic program
repair? Which parts should be focused upon to further improve
the state-of-the-art? It is important to carefully design an
empirical study to answer these questions, and its results will
beneÔ¨Åt future research in this direction:
BeneÔ¨Åt 1. The results will provide insights on the potential of
automatic program repair. For example, the results will reveal
how many bugs can be Ô¨Åxed by simple changes. As pointed
out by Monperrus [22], existing approaches are effective in
Ô¨Åxing this type of bugs. As another example, the results will
reveal the essential operators to Ô¨Åx bugs. If we compare these
operators with an existing approach, we may estimate the
potential of the approach for Ô¨Åxing bugs.
BeneÔ¨Åt 2. The results will provide insights on how to
improve existing approaches. For example, the results will
reveal the distribution of bug Ô¨Åxes, and the required knowledge
to Ô¨Åx each type of bugs. Future research may leverage such
knowledge to Ô¨Åx more bugs. As another example, the results
will reveal the nature of bug Ô¨Åxes, and future work may be able
to tune existing approaches to achieve their best performance.
BeneÔ¨Åt 3. The results will provide insights on new research
directions of Ô¨Åxing bugs. For example, the results will reveal
how many bug Ô¨Åxes require modiÔ¨Åcations on only non-source
Ô¨Åles. Follow-up work may explore how to locate bugs in non-
source Ô¨Åles and how to Ô¨Åx them with advanced techniques.
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 European Union
DOI 10.1109/ICSE.2015.101913
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 European Union
DOI 10.1109/ICSE.2015.101913
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 European Union
DOI 10.1109/ICSE.2015.101913
ICSE 2015, Florence, Italy
Despite the preceding beneÔ¨Åts, it is difÔ¨Åcult to conduct such
an empirical study, due to the following challenges:
Challenge 1. It is difÔ¨Åcult to collect the bug Ô¨Åxes for the
empirical study. First, it needs a large number of bug Ô¨Åxes to
ensure the representativeness of the results, but many projects
do not provide adequate data for analysis. For example, many
projects on SourceForge1do not have a long active period, so
they provide limited bug Ô¨Åxes for analysis. Second, Kim et
al.[15] point out that it needs high quality bug Ô¨Åxes to reduce
superÔ¨Åcial conclusions, but many bug Ô¨Åxes are polluted. For
example, although Linux has a long active period, its bug
Ô¨Åxes are intertwined with other types of commits ( e.g.new
features). Although Tian et al. [31] propose an approach to
identifying bug Ô¨Åxes for Linux, its precision is relatively low,
due to the difÔ¨Åculty in accurately identifying bug Ô¨Åxes.
Challenge 2. It is challenging to implement the support tool.
It is infeasible to analyze a large amount of bug Ô¨Åxes manually,
so it is desirable to implement a support tool for analysis. To
save space, a bug Ô¨Åx typically consists of only buggy Ô¨Åles and
modiÔ¨Åed Ô¨Åles, instead of the whole project. As a result, the
tool should be able to compare and analyze partial code.
To address the two preceding challenges, we collect high-
quality bug Ô¨Åxes, and implement a tool, called B UGSTAT, that
automatically extracts, compares and classiÔ¨Åes bug Ô¨Åxes. With
the support of the tool, we conduct the Ô¨Årst empirical study to
investigate the aforementioned research questions. This paper
identiÔ¨Åes the following key insights:
Fault localization. Our results show that it is reason-
able to assume that it needs to Ô¨Åx only several Ô¨Åles to
Ô¨Åx a bug (Findings 2 and 14). Current fault localization
approaches can deal with 30% of source Ô¨Åles at the most
(Finding 3). To deal with more source Ô¨Åles, researchers
should consider multiple faulty lines (Findings 4 and 5)
and the data dependence among faulty lines (Finding 6).
Faulty code Ô¨Åx. Our results show that it is reasonable
for automatic program repair to focus on modiÔ¨Åed source
Ô¨Åles (Findings 12 and 15). The existing approaches can
potentially Ô¨Åx 30% of source Ô¨Åles (Finding 3), but their
effectiveness in practice may be further reduced since
it is difÔ¨Åcult to decide the faulty lines of a bug, and
the interference among multiple bugs is serious (Finding
4). To Ô¨Åx more bugs, researchers can focus on muta-
tion operators of several most common modiÔ¨Åed code
elements (Finding 8), API knowledge (Finding 11), the
frequency of repair actions (Finding 9), multiple faulty
lines (Finding 5), their data dependence (Finding 6), and
multi-language programming (Finding 14).
The search space. Combining the results of Martinez
and Monperrus [21] with our results in Figure 2, we Ô¨Ånd
that automatic program repair can potentially Ô¨Åx only half
of the buggy Ô¨Åles, due to the huge space of searching
correct repair shapes (Finding 4). The potential is further
reduced, since even after such a shape is found, the effort
to Ô¨Ånd its concrete edits is nontrivial (Finding 11).
1http://sourceforge.net/Non-source bugs. Our results show that about 10%
of bugs reside in non-source Ô¨Åles (Finding 1), and the
bug prediction models should consider non-source Ô¨Åles
(Findings 1 and 2). Most of these Ô¨Åles are conÔ¨Åguration
Ô¨Åles and natural language documents (Finding 13). Even
in source Ô¨Åles, there are many bug Ô¨Åxes on non-code
elements such as comments (Finding 7).
II. M ETHODOLOGY
This section describes the dataset used in our empirical
study and our research questions. To answer these research
questions, we should analyze thousands of bug Ô¨Åxes. To
reduce the effort of manual inspection, we have developed
the B UGSTATtool to identify and classify bug Ô¨Åxes.
A. Dataset
Table I lists the subject projects used in our study. Aries2
is a set of Java components that enable an enterprise OSGi
application programming model. Cassandra3is a distributed
database management system that handles large amounts of
data across commodity servers. Derby4is a relational database.
Lucene5is an information retrieval library, and Solr6is an
enterprise search platform that is built on Lucene. As Lucene
and Solr share the same source code repository, it is difÔ¨Åcult to
determine whether a commit belongs to Lucene or Solr. We put
the results of the two projects into a single row. Mahout7is a
machine learning library. All the projects are from the Apache
software foundation8, and most Apache projects carefully
maintain the links between bug reports and bug Ô¨Åxes. Wu et
al.[35] reported that even simple heuristics achieved almost
the same precision and recall with their proposed sophisticated
technique, when they identiÔ¨Åed bug Ô¨Åxes for the Apache
projects. We select the Apache projects, so that we can focus
on the study, rather than the techniques to identify bug Ô¨Åxes.
Column ‚ÄúLOC‚Äù lists lines of code. To ensure the reliability
of our results, we selected both median and large projects.
The total lines of code add up to more than one million. We
collected the dataset in February 2014. Cassandra changed its
source code repository from SVN9to Git10in December 2011.
As B UGSTATretrieves commits from only SVN repositories,
for Cassandra it retrieved the data before the repository was
changed.
All the projects in Table I have source code repositories. For
each project in Table I, B UGSTATretrieves all the commits
from its source code repository. Each commit has a message.
We inspected the messages, and found two types of bugs: (1)
bugs reported through issue trackers, which we call reported
bugs, and (2) those not reported to issue trackers, which we
2https://aries.apache.org
3http://cassandra.apache.org
4http://db.apache.org/derby
5https://lucene.apache.org
6https://lucene.apache.org/solr
7https://mahout.apache.org
8http://www.apache.org
9https://svn.apache.org/repos/asf/cassandra/
10https://git-wip-us.apache.org/repos/asf?p=cassandra.git
914
914
914
ICSE 2015, Florence, ItalyTABLE I
DATASET
Name LOCBug Commit
F I % T N K %
Aries 142,110 497 490 98.6% 5,318 839 226 20.0%
Cassandra 121,170 1,374 1,236 90.0% 6,825 1,708 281 29.1%
Derby 659,426 2,433 2,022 83.1% 10,285 4,624 346 48.3%
Lucene/Solr 677,873 3,145 2,226 70.8% 26,890 4,234 2,019 23.3%
Mahout 121,084 457 407 89.1% 3,632 553 289 23.2%
Total 1,721,663 7,906 6,381 80.7% 52,950 11,958 3,161 28.6%F: Ô¨Åxed bugs in bug reports. I: bugs whose
Ô¨Åxes are identiÔ¨Åed by issue number; T: total
commits; N: bug-Ô¨Åx commits that are iden-
tiÔ¨Åed by issue number; K: bug-Ô¨Åx commits
that are identiÔ¨Åed by keywords.
callon-demand bugs. To identify Ô¨Åxes of the two types, we
deÔ¨Åne the following two criteria:
1. Issue number. All the projects in Table I have their issue
trackers to track various issues (e.g. bugs, improvements, new
features, tasks, and sub-tasks). Each reported issue has an
associated issue number. If a change of an issue is committed,
programmers often write its issue number to the message of
the commit. For example, in Cassandra, a commit‚Äôs message
says ‚Äúimplement multiple index expressions. patch by jbellis;
reviewed by Nate McCall for CASSANDRA-1157‚Äù. The ben-
eÔ¨Åt of the practice is that it is easy to track the issue. In the
issue tracker, the page of the issue11lists useful information
(e.g., its description, the discussions among programmers, and
the relations to other issues). In the Apache projects, when
writing issue number to messages, programmers typically use
the ‚Äúname-number‚Äù pattern, where name denotes the name of
a project, and number denotes the issue number. B UGSTAT
uses the pattern to extract issue number, and checks the
issue tracker to determine whether a commit is a bug Ô¨Åx.
In the above example, as CASSANDRA-1157 is a sub-task,
BUGSTATdetermines that the commit is not a bug Ô¨Åx.
In Table I, column ‚ÄúBug‚Äù lists the results with the issue
number criterion. Initially, the resolution of a reported issue is
unresolved, and is changed to Ô¨Åxed after programmers Ô¨Åx the
issue. Programmers may not Ô¨Åx some issues, since they resolve
these reports as invalid andduplicate. Programmers may have
different opinions on some reported issues, and change them to
other categories. For example, a reported bug can be changed
as a new feature request. For the ‚ÄúBug‚Äù column, subcolumn
‚ÄúF‚Äù lists the number of bug reports that are resolved as Ô¨Åxed;
subcolumn ‚ÄúI‚Äù lists the number of bug reports whose bug
Ô¨Åxes are identiÔ¨Åed by issue number; and subcolumn ‚Äú%‚Äù
is calculated asD
F. The result shows that the issue number
criterion identiÔ¨Åes Ô¨Åxes for 80% of the reported bugs, so
our study reÔ¨Çects the nature of the majority. B UGSTAT is
similar to existing approaches ( e.g.[35]). The difference is that
BUGSTATchecks the issue tracker to determine whether an
issue number indicates a bug, while other approaches assume
that every issue number indicates a bug.
2. Keyword. Issue trackers do not store all the bug Ô¨Åxes.
In some cases, programmers may bypass issue trackers, e-
specially when they believe that a change is trivial. When
they commit a change, programmers may write a message to
describe the Ô¨Åx. For example, in Aries, the message of a com-
11https://issues.apache.org/jira/browse/CASSANDRA-1157mit says ‚ÄúFix broken service registration listener‚Äù. B UGSTAT
determines a commit as a bug Ô¨Åx, if its message contains words
such as ‚Äúbug‚Äù or ‚ÄúÔ¨Åx‚Äù. The preceding commit was identiÔ¨Åed
as a bug Ô¨Åx, since its message contains the keyword ‚ÄúÔ¨Åx‚Äù. The
heuristic is simple, and a number of previous studies (e.g. [16])
used the same technique to extract bug Ô¨Åxes.
In Table I, column ‚ÄúCommit‚Äù lists the results using the two
criteria. For this column, subcolumn ‚ÄúT‚Äù lists the number of
retrieved commits; subcolumn ‚ÄúN‚Äù lists the number of bug
Ô¨Åxes that are identiÔ¨Åed by issue number; subcolumn ‚ÄúK‚Äù lists
the number of bug Ô¨Åxes that are identiÔ¨Åed by keywords; and
subcolumn ‚Äú%‚Äù is calculated asN+KT. Our result shows that
about 30% of commits are bug Ô¨Åxes. In addition, as shown in
subcolumns ‚ÄúI‚Äù and ‚ÄúN‚Äù, a reported bug has two commits on
average, and an on-demand bug has one commit by deÔ¨Ånition.
If it is necessary, we treat the two types of bugs differently,
when we investigate our research questions.
B. Research Questions
RQ1. To what extent are bugs localized (Section III-A)?
As the Ô¨Årst step, automatic program repair leverages fault
localization techniques to locate the faulty line. Wong and
Debroy [34] show that most fault localization approaches
assume that each buggy source Ô¨Åle has exactly one line of
faulty code. However, if a bug has multiple lines of faulty
code, the impacts of these lines may depend on each other. In
this study, we analyze the fault distribution of real bugs. The
results provide insights on locating faults.
RQ2. How complicated is it to Ô¨Åx bugs (Section III-B)?
After a faulty line is located, automatic program repair mu-
tates the faulty line to generate candidates, and uses genetic
algorithm (e.g. [17]) or random search (e.g. [26]) to select
candidates, until a candidate passes all the test cases. These
approaches are effective in Ô¨Åxing a faulty line, but may be
ineffective in Ô¨Åxing multiple faulty lines, especially when such
lines are relevant. In this study, we analyze data dependence
among faulty lines to investigate the complexity of Ô¨Åxing bugs;
the results provide insights on Ô¨Åxing bugs.
RQ3. What operators are essential (Section III-C)?
When Ô¨Åxing bugs, automatic program repair uses predeÔ¨Åned
operators to mutate faulty code. The operators are quite
important, since they decide how many and which bugs can
be Ô¨Åxed. Current automatic program repair uses quite limited
operators. For example, although Kim et al. [12] achieved
better results than previous approaches, their approach relies
only ten templates to mutate code. Although it is widely
915
915
915
ICSE 2015, Florence, Italy0 1 2 3 4 5 6 7 8 900.10.20.30.4
number of modified source filespercent of corresponding bugs
  
Aries
Cassandra
Derby
Lucene/Solr
Mahout
Totalreported bugs
0 1 2 3 4 5 6 7 8 900.10.20.30.40.50.6
number of modified source filespercent of corresponding bugs
  
Aries
Cassandra
Derby
Lucene/Solr
Mahout
Total on-demand bugs
Fig. 1. The fault distribution at the bug level.
known that existing approaches use incomplete operators, it is
challenging to make improvements. In this study, we analyze
the operations to Ô¨Åx real bugs. The results provide insights on
designing more comprehensive operators.
RQ4. What is the importance of APIs (Section III-D)?
API knowledge is useful in various programming tasks ( e.g.,
coding [43] and migrating code [42]). Our previous work [44]
shows that careless programmers may introduce API-related
bugs. However, current automatic program repair does not
leverage API knowledge, and thus is ineffective in Ô¨Åxing API-
related bugs. In this study, we analyze how many bug Ô¨Åxes are
related to APIs. The results provide insights on the importance
of API knowledge for Ô¨Åxing bugs.
RQ5. What kinds of Ô¨Åles are modiÔ¨Åed (Section III-E)?
We notice that many bugs are not related to source Ô¨Åles, and
such bugs could not be Ô¨Åxed by current automatic program
repair techniques. It is desirable to understand where such
bugs reside, so we could investigate their nature and explore
corresponding repair approaches. In this study, we analyze the
types and the distribution of modiÔ¨Åed Ô¨Åles when programmers
Ô¨Åx real bugs. The results provide insights on Ô¨Åxing bugs that
do not reside in source Ô¨Åles.
RQ6. How many Ô¨Åles are added or deleted (Section III-F)?
Automatic program repair only modiÔ¨Åes source Ô¨Åles, but
programmers may also add or delete Ô¨Åles to Ô¨Åx real bugs. Thus
existing approach may be insufÔ¨Åcient in Ô¨Åxing certain bugs.
In this study, we analyze the distribution of added and deleted
Ô¨Åles when programmers Ô¨Åx real bugs. The results provide
insights on Ô¨Åxing the corresponding bugs.
We implement B UGSTATto reduce the manual effort, and it
uses ChangeDistiller [7] to compare source Ô¨Åles and PPA [4]
to parse partial code.
III. E MPIRICAL RESULTS
A. RQ1: Fault Distribution
We calculate the number of modiÔ¨Åed Ô¨Åles for each bug Ô¨Åx,
and Figure 1 shows the distribution. Its horizontal axes show
the number of modiÔ¨Åed source Ô¨Åle, and its vertical axes show
the percentage of the corresponding bugs. The results lead to
the following Ô¨Åndings:
Finding 1. In total, programmers did not modify any source
Ô¨Åles to Ô¨Åx about 10% of reported bugs and about 20% of
on-demand bugs. Yin et al. [38] show that both open-source
and commercial projects contain many errors in conÔ¨Åguration
Ô¨Åles, and Xiong et al. [36] propose an approach to Ô¨Åx such
errors. In this study, we Ô¨Ånd bug Ô¨Åxes in conÔ¨Åguration Ô¨Åles.For example, a bug report of Solr12says that the released code
did not compile, since the compiler‚Äôs conÔ¨Åguration Ô¨Åle did not
set the paths correctly. The modiÔ¨Åed lines are as follows:
+ <tarfileset dir="../lucene"
+ prefix="lucene"
+ excludes=" **/build/" /> ...
Our previous work [41] detects errors in documents. In this
study, we Ô¨Ånd bug Ô¨Åxes in documents. For example, a bug
report of Lucene13says that the online user guide did not
display correctly, and the modiÔ¨Åed line is as follows:
-<!DOCTYPE...".../xhtml1/DTD/xhtml1-transitional.dtd">
+<!DOCTYPE...".../html4/loose.dtd">
Researchers [45], [13], [37] have proposed models to predict
buggy source Ô¨Åles for a bug report. As source Ô¨Åles are quite
different from non-source Ô¨Åles, it needs nontrivial extension
to predict buggy non-source Ô¨Åles. In addition, as non-source
Ô¨Åles are typically not executable, it needs nontrivial extension
for fault localization to locate faulty lines for non-source
Ô¨Åles. Section III-E shows that some modiÔ¨Åed Ô¨Åles are in
programming languages other than Java. As B UGSTATparses
Java code, it considers only Java code as source Ô¨Åles.
Finding 2. In total, programmers modiÔ¨Åed one or more
source Ô¨Åles to Ô¨Åx about 90% of reported bugs and more than
70% of on-demand bugs. The number of buggy Ô¨Åles for a bug
is an important parameter for fault prediction models ( e.g.[45],
[13]). Our results show that the percentage of bugs decreases
rapidly with the increasing number of modiÔ¨Åed source Ô¨Åles,
and the percentage of on-demand bugs decreases even faster.
It is reasonable to set the value of the parameter to be four
or less, since more than 80% of bugs have fewer than four
modiÔ¨Åed source Ô¨Åles.
We further analyzed the modiÔ¨Åed locations of each modiÔ¨Åed
source Ô¨Åle. The underlying tool, ChangeDistiller [7], produces
a set of repair actions from two compared source Ô¨Åles. A
repair action is a pair ‚ü®a; e‚ü©, where ais an action such as add,
delete, update, and move, and eis a code element. For actions,
BUGSTAT considers add as additions, delete as deletions,
andupdate andmove as modiÔ¨Åcations. To understand bug
Ô¨Åxes, we often need statement-level changes. For example, it
requires different knowledge to Ô¨Åx ifstatements and return
statements, although modiÔ¨Åed internal code elements are the
same ( e.g.variables). For each repair action inside a statement,
BUGSTATreplaces its code element with the statement, and
12https://issues.apache.org/jira/browse/SOLR-1989
13https://issues.apache.org/jira/browse/LUCENE-4302
916
916
916
ICSE 2015, Florence, Italy1 5 10 15 20 25 3000.050.10.150.20.250.3
number of repair actions percents of 
corresponding modified files
  
Aries
Cassandra
Derby
Lucene/Solr
Mahout
TotalFig. 2. The fault distribution at the Ô¨Åle level.
it ignores repair actions on the same statement. A code
element may not have a parent statement ( e.g.,Modifier
andJavadoc). B UGSTAT does not change repairs on these
code elements. In addition, when programmers Ô¨Åx a bug,
they often modify the test code to reproduce the bug. As
repair actions on test code are more like implementing new
features, B UGSTATignores these repair actions. For each of
the remaining modiÔ¨Åed Ô¨Åles, we calculate its number of repair
actions, and Figure 2 shows the results. Its horizontal axis
shows the number of repair actions, and its vertical axis shows
the percentage of the corresponding modiÔ¨Åed Ô¨Åles. We have
the following Ô¨Ånding:
Finding 3. In total, programmers made a repair action to Ô¨Åx
less than 30% of source Ô¨Åles. Wong and Debroy [34] claim
that most fault localization approaches assume that each buggy
Ô¨Åle has exactly a faulty line. As these source Ô¨Åles Ô¨Åt the
assumption, existing approaches have the potential to locate
their faulty lines. However, their effectiveness in practice may
be limited for two reasons. First, a faulty line may not appear
in traces, so fault localization approaches cannot locate it.
For example, code comments are not executable and will not
appear in traces, although they may contain errors. As another
example, Ô¨Åxing bugs sometimes requires changing modiÔ¨Åers.
In particular, a bug report of Solr14says that the visibility of
a method should be changed. The original code is as follows:
46: public abstract Query parse()...
The Ô¨Åxed code is as follows:
46: protected abstract Query parse()...
As many tools instrument only method bodies, Line 46 will
not appear in their traces. Second, an added or deleted code
element may have multiple lines (see Section III-C for details).
For example, adding a method is considered a single repair
action. Current automatic program repair cannot add a method
effectively, since a method may contain many lines.
Finding 4. In total, programmers made at least two repair
actions to Ô¨Åx more than 70% of source Ô¨Åles.
DiGiuseppe and Jones [5] conducted an empirical study
to explore the inÔ¨Çuence of multiple faults on fault local-
ization approaches. Their results show that fault localization
approaches are effective in locating at least one faulty line, but
the ranks of the remaining faulty lines are low. They suggest
that programmers may use fault localization to locate and Ô¨Åx
faulty lines one bye one. The suggestion works for humans, but
may not apply to automatic program repair for two reasons.
First, automatic program repair needs to rerun test cases to
14https://issues.apache.org/jira/browse/SOLR-601
Aries
Cassandra
Derby
Lucene/Solr
Mahout
Total
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  
C2
C3
C4Fig. 3. The category for fault complexity.
guide the bug Ô¨Åx process. DiGiuseppe and Jones [5] show
that the interference among multiple faults is serious, and
such interference may negatively impact the search algorithms.
Second, it may be insufÔ¨Åcient to mutate a single line to
Ô¨Åx bugs. For example, a bug report of Solr15says that an
exception was thrown due to the misuse of Java reÔ¨Çection. In
particular, three methods use Java reÔ¨Çection to call the methods
ofclob, and one method has the faulty lines as the following:
110: Method m = clob.getClass().
getDeclaredMethod("getCharacterStream");
111: if (Modifier.isPublic(m.getModifiers())) f
...
116: return (Reader) m.invoke(clob);
117: g
If the getCharacterStream method is deÔ¨Åned in the super-
class of clob, Line 110 will throw an exception, since clob
does not deÔ¨Åne the method. The Ô¨Åxed code is as follows:
110: return clob.getCharacterStream();
It is reasonable for programmers to Ô¨Åx the bug from a faulty
line, but it can be difÔ¨Åcult for automatic program repair to Ô¨Åx
the bug, since fault localization approaches typically do not
report the line range of a bug. Qi et al. [27] use different fault
localization approaches to locate the faulty lines for automatic
program repair. Their results are consistent with our results,
since they Ô¨Ånd that the best technique for humans is not
the best technique for automatic program repair. Still, more
research is needed to fully understand such inÔ¨Çuence, since
their study [27] does not analyze low-level details (e.g. the
inference of multiple faults). Research exists ( e.g.[19]) to
recover the links between test cases and multiple bugs, but
additional research is needed to locate multiple faults and fault
ranges, especially for automatic program repair.
Automatic program repair uses machine learning algorithms
such as genetic algorithm [17] and random search [26] to
guide the bug Ô¨Åx process. Martinez and Monperrus [21] mine
two repair models (CT and CTET) from bug Ô¨Åxes. The two
mined repair models are the probability distributions of repair
actions (e.g. , inserting a statement) at different granularity.
Their results show that the space of searching repair shapes
explodes when the length of repair actions is more than four
or eight for the two models, respectively. Here, a repair shape
is a set of repair actions. The two repair models are built
on ChangeDistiller, and the granularity of ChangeDistiller is
between CT and CTET. If we adjust the repair models to the
granularity of ChangeDistiller, the maximum number may be
between four and eight. Figure 2 shows that about half of the
buggy Ô¨Åles have more repair actions. In addition, even after a
15https://issues.apache.org/jira/browse/SOLR-1794
917
917
917
ICSE 2015, Florence, ItalyJavadoc
ExpressionStatement 
ReturnStatement 
Modifier 
MethodDeclaration 
BreakStatement 
ThrowStatement 
VariableDeclarationStatement 
IfStatement 
FieldDeclaration 
SwitchCase 
SingleVariableDeclaration 
Block 
0 50 100 150 200 250 300 350 400  
Modification
Addition
DeletionAries
ExpressionStatement 
MethodDeclaration 
ReturnStatement 
ThrowStatement 
VariableDeclarationStatement 
Modifier 
Javadoc 
BreakStatement 
IfStatement 
SingleVariableDeclaration 
SwitchCase 
FieldDeclaration 
ContinueStatement 
0 50 100 150 200 250 300  
Modificatin
Addition
Deletion Cassandra
ExpressionStatement
Javadoc
MethodDeclaration
ReturnStatement
FieldDeclaration
BreakStatement
IfStatement
Modifier
ContinueStatement
SwitchCase
ThrowStatement
VariableDeclarationStatement
0 500 1000 1500 2000 2500  
Modification
Addition
Deletion
Derby
Javadoc
ExpressionStatement
ReturnStatement
MethodDeclaration
Modifier
BreakStatemen
SwitchCase
ThrowStatement
VariableDeclarationStatement
AssertStatement
Block
FieldDeclaration
IfStatement
SingleVariableDeclaration
0 500 1000 1500 2000 2500 3000 3500 4000  
Modification
Addition
Deletion Lucene/Solr
Javadoc
ExpressionStatement
ReturnStatement
VariableDeclarationStatement
MethodDeclaration
Modifier
ThrowStatement
FieldDeclaration
IfStatement
SingleVariableDeclaration
0 100 200 300 400 500 600  
Modification
Addition
Deletion
Mahout
Javadoc
ExpressionStatement
ReturnStatement
MethodDeclaration
Modifier
BreakStatement
SwitchCase
ThrowStatement
FieldDeclaration
VariableDeclarationStatement
IfStatement
Block
SingleVariableDeclaration
ContinueStatement
0 1000 2000 3000 4000 5000 6000 7000  
Modificatoin
Addition
Deletion Total
Fig. 4. The distribution of repair actions.
correct repair shape is found, it is still nontrivial to synthesize
the concrete Ô¨Åx. As a result, we estimate that current automatic
program repair cannot Ô¨Åx more than half of the buggy Ô¨Åles
due the large search space to synthesize Ô¨Åxes.
B. RQ2: Fault Complexity
Based on the fault complexity, we classify modiÔ¨Åed source
Ô¨Åles into four categories: a single repair action (C1 ), non-
data dependent repair actions (C2), data dependent repair
actions (C3), and mixture repair actions (C4). B UGSTATputs
a repair action into a set of data dependent repair actions, if
its code element contains a variable, and the variable has data
dependence on variables in that set.
As shown in Figure 2, programmers made a single repair
action to Ô¨Åx about 30% of source Ô¨Åles. These source Ô¨Åles
fall into the C1category. Current automatic program repair
is effective in Ô¨Åxing bugs in this category. We calculated the
number for the other three categories, and Figure 3 shows
the results. Its vertical axis shows the subject projects, and its
horizontal axis shows the percentages of the three categories.
The results lead to the following Ô¨Åndings:
Finding 5. As shown in Figures 2 and 3, programmers made
multiple non-data dependent repair actions to Ô¨Åx about 40% of
source Ô¨Åles (the C2category). As these repair actions are not
data dependent, it may be feasible to apply the repair actions
one by one when Ô¨Åxing bugs. For example, a bug report ofLucene16says that an exception was thrown if a resource was
closed, and a submitted patch is as follows:
@@ -152,7 +152,15 @@
- swapSearcher(newSearcher);
+ boolean success = false;
+ try f
+ swapSearcher(newSearcher);
+ success = true;
+ gfinally f
...
+ g
@@ -204,7 +212,12 @@
- public void close() throws IOException f
- swapSearcher(null);
+ public synchronized void close() throws IOException f
+ if (currentSearcher != null) f
...
+ swapSearcher(null);
+ g
In the Ô¨Årst diff hunk of the patch, programmers placed a
statement inside a try statement. In the second diff hunk of
the patch, programmers put a statement into an ifstatement.
As the two faulty lines have no data dependency, it may be
feasible for existing approaches to Ô¨Åx them one by one.
Finding 6. In total, as shown in Figures 2 and 3, program-
mers made data dependent repair actions to Ô¨Åx more than 40%
of source Ô¨Åles (the C3andC4categories). For example, a bug
report of Mahout17says that a function got broken over time,
and the faulty lines were as follows:
130: if (r == 0) f
131: if (index < o.index) f
132: return -1;
16https://issues.apache.org/jira/browse/LUCENE-3476
17https://issues.apache.org/jira/browse/MAHOUT-591
918
918
918
ICSE 2015, Florence, Italy133: gelse if (index > o.index) f
134: return 1;
135: g
136: return 0;
137: gelse f
138: return r;
139: g
When ris zero, the preceding code returns a wrong value. To
calculate the correct value, programmers should understand the
relation among r,o.index, and index. The modiÔ¨Åed lines
are as follows:
130: if (r != 0) f
131: return r;
132: gelse f
133: return o.index - index;
134: g
It should be more difÔ¨Åcult for programmers to Ô¨Åx source Ô¨Åles
in this category than the C1andC2categories, since they have
to consider the nodes that depend on each other. However,
the added complexity may be an opportunity for automatic
approaches. For example, a fault localization approach may
use the data dependencies to locate relevant faulty lines of a
located faulty line, and automatic program repair could use
the data dependencies to prune its search space.
C. RQ3: Mutation Operator
As the underlying tool, ChangeDistiller [7], is built on
the Eclipse‚Äôs Java model18, the code elements of extracted
repair actions follow the APIs of Eclipse19. We Ô¨Ånd that the
underlying tool has special strategies to extract repair actions
onBlock andMethodDeclaration . For example, it counts
adding or deleting a method as an addition or a deletion
onMethodDeclaration , but it counts only modifying a
method name as a modiÔ¨Åcation on MethodDeclaration .
It does not count modiÔ¨Åcations inside a method (e.g., the
modiÔ¨Åer, the parameters, and the statements in the method
body) as a modiÔ¨Åcation on MethodDeclaration , but counts
them at Ô¨Åner levels. Here, as a method may have complicated
structures, current program repair may not add a method
effectively, although it is counted as a repair action.
As discussed in Section III-B, non-data dependent repair
actions follow quite different patterns from data dependent
repair actions, and current automatic program repair is ef-
fective to apply only non-data dependent repair actions. To
eliminate the interference between the two types of repair
actions and to provide valuable Ô¨Åndings for the state-of-the-art,
this study focuses only on non-data dependent repair actions.
By deÔ¨Ånition, Javadoc, Modifier ,BreakStatement , and
ContinueStatement have only non-data dependent repair
actions. All the other code elements have both data dependent
and non-data dependent repair actions. The nature of code
elements affects their ranks. For example, Pan et al. [25]
show that a large portion of bug-Ô¨Åx patterns are related to
ifstatements. In our study, we Ô¨Ånd that most repair actions
onifstatements are data dependent. As a result, its rank is
low. The results show that automatic program repair should
18http://www.vogella.com/tutorials/EclipseJDT/article.html
19http://help.eclipse.org/indigo/topic/org.eclipse.jdt.doc.isv/reference/api/
overview-summary.html
0 5 10 15 20 25 3000.10.20.30.40.5
number of API repair actionspercent of corresponding 
modified source files
  
Aries
Cassandra
Derby
Lucene/Solr
Mahout
TotalFig. 5. The distribution of API repair actions.
leverage data-Ô¨Çow analysis to effectively apply repair actions
on this code element.
We calculated the number of repair actions on each code
element, and Figure 4 shows the results. Its vertical axis
shows the names of code elements. To save space, we do not
present code elements whose repair actions are less than 1%.
Its horizontal axis shows the number of repair actions with
the categories such as additions, deletions, and modiÔ¨Åcations.
The results lead to the following Ô¨Åndings:
Finding 7. In total, repair actions on Javadoc rank the Ô¨Årst.
In the Eclipse‚Äôs Java model, Javadoc denotes the comments
between code. Programmers modiÔ¨Åed these code comments,
since they contain errors or become inconsistent with the
implementation. There exists research to address this problem.
For example, our previous work [41] detects documentation
errors ( e.g.outdated code names in comments). As another
example, Tan et al. [29] and Zhong et al. [44] infer formal
rules from code comments and check the rules against the im-
plementation to detect inconsistencies. Although the problem
has been explored, there may still be space for improvement,
since there are many modiÔ¨Åcations on Javadoc.
Finding 8. The repair actions on a code element typically
increase with its complexity. For example, in Eclipse‚Äôs Java
model, Expression is complicated, since it has a rich set of
subclasses20.MethodInvocation is one of its subclasses, and
it may invoke complicated API methods. By deÔ¨Ånition, each
ExpressionStatement has at least an Expression node,
and each ReturnStatement can have an Expression node.
As the inside nodes are complicated, the repair actions on the
two types of statements rank the second and the third, in total.
In contrast, BreakStatement andContinueStatement are
relatively simple, so the repair actions on the two types of
statements are much fewer.
Finding 9. The actions on code elements follow two pat-
terns. First, the modiÔ¨Åcations on a code element increase
with its complexity. For example, ExpressionStatement
is more complicated than BreakStatement as discussed
before. Thus as shown in Figure 4, the former statement has
more modiÔ¨Åcations than the latter statement. In fact, all the
modiÔ¨Åcations of the latter statement are to move from one
line to another, since it has no internal structures. Second,
additions on a code element are more than deletions. When
bugs are introduced, a careless programmer may forget some
statements, and may also add unwanted statements. Our results
20http://help.eclipse.org/indigo/topic/org.eclipse.jdt.doc.isv/reference/api/
org/eclipse/jdt/core/dom/Expression.html
919
919
919
ICSE 2015, Florence, ItalyTABLE II
THE DISTRIBUTION OF MODIFIED FILES
Aries Cassandra Derby Lucene/Solr Mahout Total
java java java java java java
xml txt out txt xml txt
mdtext py properties xml CHANGELOG xml
xml sql html out
xml properties
sql
show that the former case is more common than the latter case.
Automatic program repair could use the two patterns to prune
its search space, when they mutate the faulty code.
D. RQ4: API Knowledge
The underlying tool, PPA [4], parses and resolves full names
of code elements. B UGSTAT considers a repair action as
an API repair action, if the full name of its code element
indicates that the code element is declared by third-party API
libraries. If PPA fails to resolve the full name for a code
element, B UGSTATconservatively classiÔ¨Åes the corresponding
repair action into the non-API category. Figure 5 shows the
distribution. Its horizontal axis shows the number of API
repair actions, and its vertical axis shows the percentage of
corresponding modiÔ¨Åed source Ô¨Åles. Based on the results, we
have the following Ô¨Åndings:
Finding 10. In total, programmers did not make any API
repair actions to Ô¨Åx half of the source Ô¨Åles. The results
explain why current automatic program repair is able to Ô¨Åx
many bugs without API knowledge. By deÔ¨Ånition, some code
elements ( e.g.,Javadoc, Modifier ,BreakStatement , and
ContinueStatment ) do not contain API elements, so repair
actions on these code elements are not related to APIs. The
repair actions on some other code elements have limited rela-
tion to APIs. For example, in Figure 4, SwithCase requires
constants. If a constant in API libraries passes the type check,
it may be used to Ô¨Åx bugs in SwithCase.
Finding 11. In total, programmers made at least one
API repair action to Ô¨Åx the other half of source Ô¨Åles.
We Ô¨Ånd that complicated code elements tend to have
more API repair actions. For example, as discussed before,
ExpressionStatement is complicated, and we Ô¨Ånd many
API repair actions on this code element. In particular, a bug
report of Cassandra21says that it fails to read saved Ô¨Åles, and
the faulty line is as follows:
249: in = new ObjectInputStream(...);
Line 249 is an ExpressionStatement . This statement calls
ObjectInputStream to read Ô¨Åles, but the Ô¨Åles are saved
with incompatible APIs. To Ô¨Åx the bug, programmers choose
another API, and the modiÔ¨Åed line was as follows:
249: in = new DataInputStream(...);
Robillard et al. [28] show that various approaches have been
proposed to mine speciÔ¨Åcations for API libraries. As mined
speciÔ¨Åcations describe legal API usage, they may be useful
to Ô¨Åx API-related bugs. An interesting point to note is that
21https://issues.apache.org/jira/browse/CASSANDRA-2174most mined speciÔ¨Åcations describe the usage of only several
API elements, according to our previous work (e.g. [43]).
In Figure 5, a Ô¨Åle has fewer than Ô¨Åve API repair actions
on average. The results indicate that an API usage typically
involves only several API elements.
E. RQ5: File Type for ModiÔ¨Åcations
We calculated the Ô¨Åle types of all the modiÔ¨Åed Ô¨Åles, and
Table II shows the distribution. In Table II, we use the sufÔ¨Åces
of Ô¨Åle names to denote Ô¨Åle types. To save space, we do not
present Ô¨Åle types that are less than 1%. Based on the results,
we have the following Ô¨Åndings.
Finding 12. The most common modiÔ¨Åed Ô¨Åles are Java
source Ô¨Åles, and the other Ô¨Åles are much fewer. The result
is not surprising, since all the studied projects are in Java.
However, we have noticed something interesting when we
compare the results with results in Figure 1. Figure 1 shows
that programmers did not modify Java source Ô¨Åles to Ô¨Åx at
least 10% of the bugs. As a result, on average, programmers
modiÔ¨Åed fewer Ô¨Åles when a bug did not involve Java source
Ô¨Åles than when it did. The result indicates that the dependency
among Java source Ô¨Åles may be higher than that among other
Ô¨Åles ( e.g.conÔ¨Åguration Ô¨Åles).
Finding 13. The two most common modiÔ¨Åed non-source
Ô¨Åles are conÔ¨Åguration Ô¨Åles and natural language documents.
The names of the most found conÔ¨Åguration Ô¨Åles end with
‚Äúxml‚Äù or ‚Äúproperties‚Äù, and we Ô¨Ånd three typical types of
such Ô¨Åles. The Ô¨Årst type deÔ¨Ånes the parameters of build tools
(e.g., Ant22), and one such example is presented in Finding 1.
The second type deÔ¨Ånes runtime parameters. For example, a
bug report of Cassandra23says that committing log can cause
write pauses. To Ô¨Åx the bug, programmers modiÔ¨Åed code and
enlarged the following old parameter:
312:<CommitLogSyncPeriodInMS>1000</CommitLogSyncPeriodInMS>
The modiÔ¨Åed line was as follows:
312:<CommitLogSyncPeriodInMS>10000</CommitLogSyncPeriodInMS>
The third type deÔ¨Ånes parameters of third-party tools. For
example, many programmers use Findbugs24to detect bugs,
and the tool has a conÔ¨Åguration Ô¨Åle to enable speciÔ¨Åc checks.
We Ô¨Ånd that the programmers of Mahout added several lines
to the Findbugs‚Äô conÔ¨Åguration Ô¨Åle to enable a check:
40:<Match>
41: <Bug pattern="SE_NO_SUITABLE_CONSTRUCTOR"/>
42:</Match>
22http://ant.apache.org
23https://issues.apache.org/jira/browse/CASSANDRA-668
24http://Ô¨Åndbugs.sourceforge.net
920
920
920
ICSE 2015, Florence, Italy0 1 2 3 4 5 6 7 8 900.20.40.60.81
number of added source files percent of 
corresponding bugs
  
Aries
Cassandra
Derby
Lucene/Solr
Mahout
Totaladditions for reported bugs
0 1 2 3 4 5 6 7 8 900.20.40.60.81
number of deleted source filespercent of 
corresponding bugs
  
Aries
Cassandra
Derby
Lucene/Solr
Mahout
Total deletions for reported bugs
0 1 2 3 4 5 6 7 8 900.20.40.60.81
number of added source filespercent of 
corresponding bugs
  
Aries
Cassandra
Derby
Lucene/Solr
Mahout
Total
additions for on-demand bugs
0 1 2 3 4 5 6 7 8 900.20.40.60.81
number of deleted source filespercent of 
corresponding bugs
  
Aries
Cassandra
Derby
Lucene/Solr
Mahout
Total deletions for on-demand bugs
Fig. 6. Additions and deletions.
The names of most found natural language documents end
with ‚Äúhtml‚Äù or ‚Äútxt‚Äù. These documents are manuals, tutorials
and change logs. The results highlight the importance of Ô¨Åxing
bugs in conÔ¨Åguration Ô¨Åles and natural language documents.
Finding 14. Some modiÔ¨Åed source Ô¨Åles are in programming
languages other than Java for two reasons. First, a project
may be implemented in multiple programming languages.
For example, Cassandra is a database, and its programmers
implement a Python driver. A bug report25says that the driver
did not parse queries correctly. To Ô¨Åx the bug, programmers
modiÔ¨Åed one faulty line of the cursor.py Ô¨Åle:
39:_cfamily_re = re.compile("...", re.I | re.M)
The modiÔ¨Åed line was as follows:
39:_cfamily_re = re.compile("...",
re.IGNORECASE | re.MULTILINE | re.DOTALL)
Second, a project may implement an interface for a program-
ming language. For example, Derby is a database that supports
queries in SQL, and its programmers use SQL queries as
test cases. A bug report of Derby26says that when an error
code was returned when a query casted DATE toTIMESTAMP.
To reproduce the error, a programmer added a line to the
cast.sql Ô¨Åle:
476: select cast(t as timestamp) from tab1;
On this line, tis a column of table tab1, and the type of t
istime, and the cas.out Ô¨Åle recorded the output:
ij> select cast(t as timestamp) from tab1;
ERROR 42846: Cannot convert types ‚ÄôTIME‚Äô to ‚ÄôTIMESTAMP‚Äô.
The two situations highlight the importance of Ô¨Åxing bugs in
multiple programming languages. As automatic program repair
has been evaluated on only a limited number of programming
languages, such as C and Java, it may require signiÔ¨Åcant
improvement to Ô¨Åx bugs in other programming languages. For
example, it is challenging to instrument SQL queries to collect
their executed traces. Without such traces, it is infeasible to
use fault localization to locate faulty lines in such queries.
25https://issues.apache.org/jira/browse/CASSANDRA-2993
26https://issues.apache.org/jira/browse/DERBY-896F . RQ6: Additions and Deletions
We calculated the number of added and deleted Ô¨Åles for
each bug Ô¨Åx, and Figure 6 shows the distribution. Its horizontal
axes show the number of added or deleted source Ô¨Åles, and
its vertical axes show the corresponding percentage. Based on
the results, we have the following Ô¨Ånding:
Finding 15. In total, programmers did not add any Ô¨Åles to
Ô¨Åx more than 80% of the bugs, and they did not delete any
Ô¨Åles to Ô¨Åx more than 90% of the bugs. Hattori and Lanza [9]
analyze the nature of commits, and they Ô¨Ånd that the size of a
commit is associated with the type of the commit. In particular,
tiny commits are more related to bug Ô¨Åxes, and large commits
are more related to new features. Our result reÔ¨Çects another
nature of commits, namely at the Ô¨Åle level, modiÔ¨Åcations may
be more related to bug Ô¨Åxes. As most bug Ô¨Åxes require only
modifying Ô¨Åles, it is reasonable for automatic program repair
to focus on mutating Ô¨Åles to Ô¨Åx bugs.
G. Threats to Validity
The threat to internal validity includes the defects in the
underlying tools that we use. ChangeDistiller may produce
infeasible edit scripts, and PPA may wrongly resolve code
elements. To reduce this threat, we reported our found defects,
and if they are not Ô¨Åxed, we tried to Ô¨Åx them by ourselves. The
threat could be further reduced by developing more advanced
tools. The threat to external validity includes our selected
subjects. Although we analyzed thousands of bug Ô¨Åxes in
total, the selected projects may still be limited and are all
for Java. It is likely that most our Ô¨Åndings still hold in other
programming languages, but the speciÔ¨Åc numbers may be
different. For example, although the repair actions on a code
element increase with the complexity of its usage (Finding 8)
in other programming languages, the ranks of code elements
will be different, since other programming languages may
deÔ¨Åne quite different code elements. To reduce this validity,
our study should be replicated in future work by using subjects
in other programming languages.
921
921
921
ICSE 2015, Florence, ItalyIV. D ISCUSSIONS AND FUTURE WORK
Bug deÔ¨Ånitions. Practitioners and researchers can have d-
ifferent deÔ¨Ånitions of bugs. Herzig et al. [10] claimed that
at least 30% of reported bugs are not bugs, but features.
Even researchers may have different deÔ¨Ånitions of bugs. For
example, in our previous work [41], we reported our detected
documentation errors as bugs, and programmers accepted and
Ô¨Åxed them as bugs. However, by the deÔ¨Ånition of Herzig et al.,
our detected errors are documentation requests, but not bugs.
The different deÔ¨Ånitions lead to quite different results. In our
study, we follow the pragmatic deÔ¨Ånition of practitioners, i.e.,
those issues that are reported and Ô¨Åxed as bugs. The beneÔ¨Åt is
that the results reÔ¨Çect the reality of practice, and practitioners
do not need to read all the deÔ¨Ånitions to understand our results.
More factors of automatic program repair. We have focused
on the major factors of current automatic program repair, and
due to space limit, we certainly miss some factors that are also
relevant. For example, the domain knowledge of compilers,
tool chains, programming models, multi-programming and
concurrent, and the low-level knowledge of operating systems
and hardware are essential to Ô¨Åx some bugs. Murphy-Hill et
al.[23] present various factors, when programmers manually
Ô¨Åx bugs. In addition, Luo et al. [20] show that bugs can not
be easily reproducible or veriÔ¨Åable, which introduces extra
barriers to automatic program repair. In future work, we plan to
conduct studies to investigate the importance of these factors.
Manual Ô¨Åxes vs. automatic Ô¨Åxes. Neither manual Ô¨Åxes
nor automatic Ô¨Åxes are perfect. Although Yin et al. [39]
show that manual Ô¨Åxes can be incorrect, our results are still
reliable, since most Ô¨Åxes are correct. At the same time, a
program Ô¨Åx may pass test cases, but does not Ô¨Åx the real
problem. Bird et al. [3] show that even manual Ô¨Åxes should
be carefully examined, so it is likely that the correctness
of program Ô¨Åxes also needs to be veriÔ¨Åed. Although we
agree with Monperrus [22] that understandability of patches
is inessential in certain situations, in most cases, generated
patches should be readable to humans or other programs for
veriÔ¨Åcation. In addition, although it is a practical way to mimic
humans and it is a good way to understand the challenges
by analyzing manual Ô¨Åxes, we agree that there could be
alternative ways to solve the problem. For example, computers
can already beat the best human players in playing chess, and
the algorithm is quite different from humans, since computers
have much larger (and more reliable) memories and much
stronger computational capabilities than most humans [1].
With advanced techniques, it is conceivable that a computer
algorithm may produce better patches than we humans do.
V. R ELATED WORK
Automatic program repair. Weimer et al. [33] proposed
GenPro, a seminal work on automatic program repair. Le
Goues et al. [17] extended GenPro, and proposed new muta-
tion and crossover operators. Kim et al. [12] manually inspect-
ed thousands of bug Ô¨Åxes and summarized ten templates as
new mutation operators. Wei et al. [32] used mined invariants
to Ô¨Åx bugs. Jin et al. [11] proposed new mutation operatorsand selectors that are designed to Ô¨Åx concurrent bugs. All the
preceding approaches deÔ¨Åne a limited number of simple repair
shapes, and rely on genetic algorithm to generate complicated
repair shapes to Ô¨Åx complicated bugs. Qi et al. [26] show
that random search is more effective than genetic algorithm to
guide the bug Ô¨Åx process. Martinez and Monperrus [21] mine
repair models from manual Ô¨Åxes, and the mined repair models
improve random search. Our study provides Ô¨Åndings and
insights to better understand and improve these approaches.
Fault localization. Fault localization has been an extensive s-
tudied topic. Wong and Debroy [34] provided a comprehensive
survey on this line of research. Most of the approaches ( e.g.
[8]) assume that each program has exactly one faulty location,
and a few recent approaches ( e.g.[19]) start to explore the
links between traces and multiple faulty locations. Our study
shows that future research should further improve existing
approaches to locate bugs in multiple locations.
Empirical study on bug Ô¨Åxes. Various empirical studies
exist to understand the nature of bug Ô¨Åxes. Yin et al. [39]
show that bug Ô¨Åxes could introduce new bugs. Nguyen et
al.[24] show that repetitiveness is common in small size
bug Ô¨Åxes. Eyolfson et al. [6] show that the bugginess of
a commit is correlated with the time to make the commit.
Bird et al. [2] show that many projects did not carefully
maintained the links between bug reports and bug Ô¨Åxes. Our
empirical study has analyzed bug Ô¨Åxes to gain insights on
automatic program repair, providing a different research angle
than previous studies. Thung et al. [30] manually examined
bug Ô¨Åxes; their results show that faults are not localized. Our
results are largely consistent with their Ô¨Åndings. However, our
results are built on much larger samples, since we analyzed
bug Ô¨Åxes automatically. In addition, different from theirs, our
study is not limited to fault localization.
VI. C ONCLUSION
Automatic program repair shows promising results, but its
effectiveness on real-world bugs has also been questioned and
criticized. Directly evaluating tools from these efforts on real
bugs may produce many trivial and negative results, since
they are still immature research prototypes. Instead, we have
conducted a large-scale empirical study on thousands of real-
world bug Ô¨Åxes from Ô¨Åve popular projects. We have compared
how programmers Ô¨Åx bugs and how bug-Ô¨Åx approaches are
designed, and distilled 15 Ô¨Åndings and four insights that we
believe are useful to guide future research in this direction.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their constructive
comments. This research was sponsored in part by the National
Basic Research Program of China (973) No. 2015CB352203,
the National Science Foundation of China No. 61100071,
61272102, and 91118004, and by United States NSF Grants
1117603 and 1319187. The information presented here does
not necessarily reÔ¨Çect the position or the policy of the Gov-
ernment and no ofÔ¨Åcial endorsement should be inferred.
922
922
922
ICSE 2015, Florence, ItalyREFERENCES
[1] H. J. Berliner. Backgammon computer program beats world champion.
ArtiÔ¨Åcial Intelligence, 14(2):205‚Äì220, 1980.
[2] C. Bird, A. Bachmann, E. Aune, J. Duffy, A. Bernstein, V. Filkov, and
P. Devanbu. Fair and balanced?: bias in bug-Ô¨Åx datasets. In Proc.
ESEC/FSE, pages 121‚Äì130, 2009.
[3] C. Bird, N. Nagappan, B. Murphy, H. Gall, and P. Devanbu. Don‚Äôt touch
my code!: examining the effects of ownership on software quality. In
Proc. 19th FSE , pages 4‚Äì14, 2011.
[4] B. Dagenais and L. J. Hendren. Enabling static analysis for partial Java
programs. In Proc. 23rd OOPSLA , pages 313‚Äì328, 2008.
[5] N. DiGiuseppe and J. A. Jones. On the inÔ¨Çuence of multiple faults on
coverage-based fault localization. In Proc. ISSTA, pages 210‚Äì220, 2011.
[6] J. Eyolfson, L. Tan, and P. Lam. Correlations between bugginess and
time-based commit characteristics. Empirical Software Engineering ,
19(4):1009‚Äì1039, 2014.
[7] B. Fluri, M. Wursch, M. PInzger, and H. C. Gall. Change distilling:
Tree differencing for Ô¨Åne-grained source code change extraction. IEEE
Transactions on Software Engineering , 33(11):725‚Äì743, 2007.
[8] D. Hao, L. Zhang, Y. Pan, H. Mei, and J. Sun. On similarity-awareness
in testing-based fault localization. Automated Software Engineering ,
15(2):207‚Äì249, 2008.
[9] L. P. Hattori and M. Lanza. On the nature of commits. In Proc. 23rd
ASE-Workshops, pages 63‚Äì71, 2008.
[10] K. Herzig, S. Just, and A. Zeller. It‚Äôs not a bug, it‚Äôs a feature: How
misclassiÔ¨Åcation impacts bug prediction. In Proc. 35th ICSE, pages
392‚Äì401, 2013.
[11] G. Jin, W. Zhang, D. Deng, B. Liblit, and S. Lu. Automated concurrency-
bug Ô¨Åxing. In Proc. 10th OSDI , pages 221‚Äì236, 2012.
[12] D. Kim, J. Nam, J. Song, and S. Kim. Automatic patch generation
learned from human-written patches. In Proc. 35th ICSE , pages 802‚Äì
811, 2013.
[13] D. Kim, Y. Tao, S. Kim, and A. Zeller. Where should we Ô¨Åx this bug?
a two-phase recommendation model. IEEE Transactions on Software
Engineering, 39(11):1597‚Äì1610, 2013.
[14] S. Kim and E. J. Whitehead Jr. How long did it take to Ô¨Åx bugs? In
Proc. 3rd MSR , pages 173‚Äì174, 2006.
[15] S. Kim, H. Zhang, R. Wu, and L. Gong. Dealing with noise in defect
prediction. In Proc. 33rd ICSE , pages 481‚Äì490, 2011.
[16] S. Kim, T. Zimmermann, E. J. Whitehead Jr, and A. Zeller. Predicting
faults from cached history. In Proc. 29th ICSE , pages 489‚Äì498, 2007.
[17] C. Le Goues, M. Dewey-Vogt, S. Forrest, and W. Weimer. A systematic
study of automated program repair: Fixing 55 out of 105 bugs for $8
each. In Proc. 34th ICSE , pages 3‚Äì13, 2012.
[18] G. Liang, Q. Wang, T. Xie, and H. Mei. Inferring project-speciÔ¨Åc bug
patterns for detecting sibling bugs. In Proc. ESEC/FSE , pages 565‚Äì575,
2013.
[19] C. Liu and J. Han. Failure proximity: a fault localization-based approach.
InProc. 14th FSE , pages 46‚Äì56, 2006.
[20] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov. An empirical analysis of
Ô¨Çaky tests. In Proc. 22nd FSE , pages 643‚Äì653, 2014.
[21] M. Martinez and M. Monperrus. Mining software repair models for
reasoning on the search space of automated program Ô¨Åxing. Empirical
Software Engineering , pages 1‚Äì30, 2013.
[22] M. Monperrus. A critical review of ‚Äúautomatic patch generation learned
from human-written patches‚Äù: Essay on the problem statement and theevaluation of automatic software repair. In Proc. 36th ICSE, pages 234‚Äì
242, 2014.
[23] E. Murphy-Hill, T. Zimmermann, C. Bird, and N. Nagappan. The design
of bug Ô¨Åxes. In Proc. 35th ICSE, pages 332‚Äì341, 2013.
[24] H. A. Nguyen, A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, and H. Rajan.
A study of repetitiveness of code changes in software evolution. In Proc.
28th ASE , pages 180‚Äì190, 2013.
[25] K. Pan, S. Kim, and E. J. Whitehead Jr. Toward an understanding of
bug Ô¨Åx patterns. Empirical Software Engineering , 14(3):286‚Äì315, 2009.
[26] Y. Qi, X. Mao, Y. Lei, Z. Dai, and C. Wang. The strength of random
search on automated program repair. In ICSE , pages 254‚Äì265, 2014.
[27] Y. Qi, X. Mao, Y. Lei, and C. Wang. Using automated program repair
for evaluating the effectiveness of fault localization techniques. In Proc.
ISSTA , pages 191‚Äì201, 2013.
[28] M. P. Robillard, E. Bodden, D. Kawrykow, M. Mezini, and T. Ratchford.
Automated API property inference techniques. IEEE Transactions on
Software Engineering, 39(5):613‚Äì637, 2013.
[29] L. Tan, D. Yuan, G. Krishna, and Y. Zhou. /* iComment: Bugs or Bad
Comments?*/. In Proc. 21st SOSP, pages 145‚Äì158, 2007.
[30] F. Thung, D. Lo, and L. Jiang. Are faults localizable? In Proc. 9th
MSR, pages 74‚Äì77, 2012.
[31] Y. Tian, J. Lawall, and D. Lo. Identifying linux bug Ô¨Åxing patches. In
Proc. 34th ICSE , pages 386‚Äì396, 2012.
[32] Y. Wei, Y. Pei, C. A. Furia, L. S. Silva, S. Buchholz, B. Meyer, and
A. Zeller. Automated Ô¨Åxing of programs with contracts. In Proc. 19th
ISSTA , pages 61‚Äì72, 2010.
[33] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest. Automatically
Ô¨Ånding patches using genetic programming. In Proc. 31st ICSE, pages
364‚Äì374, 2009.
[34] W. E. Wong and V. Debroy. A survey of software fault localization.
Technical report, University of Texas at Dallas, 2009.
[35] R. Wu, H. Zhang, S. Kim, and S.-C. Cheung. Relink: recovering links
between bugs and changes. In Proc. ESEC/FSE, pages 15‚Äì25, 2011.
[36] Y. Xiong, A. Hubaux, S. She, and K. Czarnecki. Generating range Ô¨Åxes
for software conÔ¨Åguration. In Proc. 34th ICSE , pages 58‚Äì68, 2012.
[37] X. Ye, R. Bunescu, and C. Liu. Learning to rank relevant Ô¨Åles for bug
reports using domain knowledge. In Proc. FSE , pages 66‚Äì76, 2014.
[38] Z. Yin, X. Ma, J. Zheng, Y. Zhou, L. N. Bairavasundaram, and
S. Pasupathy. An empirical study on conÔ¨Åguration errors in commercial
and open source systems. In Proc. 23rd SOSP , pages 159‚Äì172, 2011.
[39] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and L. Bairavasundaram. How
do Ô¨Åxes become bugs? In Proc. ESEC/FSE, pages 26‚Äì36, 2011.
[40] M. Zhivich and R. K. Cunningham. The real cost of software errors.
IEEE Security & Privacy, 7(2):87‚Äì90, 2009.
[41] H. Zhong and Z. Su. Detecting API documentation errors. In Proc.
SPASH/OOPSLA , pages 803‚Äì816, 2013.
[42] H. Zhong, S. Thummalapenta, T. Xie, L. Zhang, and Q. Wang. Mining
API mapping for language migration. In Proc. 32nd ICSE, pages 195‚Äì
204, 2010.
[43] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei. MAPO: Mining and
recommending API usage patterns. In Proc. 23rd ECOOP , pages 318‚Äì
343, 2009.
[44] H. Zhong, L. Zhang, T. Xie, and H. Mei. Inferring resource speciÔ¨Å-
cations from natural language API documentation. In Proc. 24th ASE ,
pages 307‚Äì318, 2009.
[45] J. Zhou, H. Zhang, and D. Lo. Where should the bugs be Ô¨Åxed?
more accurate information retrieval-based bug localization based on bug
reports. In Proc. 34th ICSE , pages 14‚Äì24, 2012.
923
923
923
ICSE 2015, Florence, Italy