Efﬁcient Transaction-Based Deterministic Replay
for Multi-threaded Programs†
Ernest Pobee
Department of Computer Science
City University of Hong Kong
Kowloon Tong, Hong Kong
ernestpob@gmail.comXiupei Mei
Department of Computer Science
City University of Hong Kong
Kowloon Tong, Hong Kong
xpmei2-c@my.cityu.edu.hkW .K. Chan‡
Department of Computer Science
City University of Hong Kong
Kowloon Tong, Hong Kong
wkchan@cityu.edu.hk
Abstract —Existing deterministic replay techniques propose
strategies which attempt to reduce record log sizes and achieve
successful replay. However, these techniques still generate large
logs and achieve replay only under certain conditions. We propose
a solution based on the division of the sequence of events
of each thread into sequential blocks called transactions. Our
insight is that there are usually few to no atomicity violations
among transactions reported during a program execution. We
present TPLAY, a novel deterministic replay technique which
records thread access interleavings on shared memory locations
at the transactional level. TPLAY also generates an artiﬁcial
pair of interleavings when an atomicity violation is reported
on a transaction. We present an experiment using the Splash2x
extension of the PARSEC benchmark suite. Experimental results
indicate that TPLAY experiences a 13-fold improvement in record
log sizes and achieves a higher replay probability in comparison
to existing work.
Keywords -Concurrency, Deterministic Replay, Transactions,
Multi-threading.
I. I NTRODUCTION
Deterministic replay of a program involves recording data
from the program execution and subsequently scheduling the
program to achieve some desired state or output [8], [11],
[16], [23], [29]. Deterministic replay techniques are generally
categorized into hardware-based [2], [20], [21], [26], [29] and
software-based [11], [12], [16] techniques.
Software-based techniques often consist of a record phase
and a replay phase. Some of them [12], [31] further include
an ofﬂine phase to compute/construct feasible schedules for
replay. Due to the non-ubiquitous nature of specialized hard-
ware required for most hardware-based techniques, we present
our technique in relation to software-based replay techniques.
In the record phase, replay techniques typically record three
types of thread access orders known as interleavings (/squiggleright)
namely, read-write ,write-read and write-write interleavings
[17], [22], [30] on shared memory locations. Interleavings on
synchronization primitives are also recorded. Some techniques
[1] also record other types of data such as read values to ensure
value determinism in the replay phase. Chen et al. [6] refer
†This research is supported in part by the GRF of HKSAR Research Grants
Council (project nos. 11214116 and 11200015), the HKSAR ITF (project
no. ITS/378/18), the CityU MF_EXT (project no. 9678180), the CityU SRG
(project nos. 7004882 and 7005216) and the CityU SGS Conference Grant.‡Correspondence Authorto the proportion of interleavings recorded as the “degree of
record ﬁdelity” and indicate a positive correlation between the
degrees of recording ﬁdelity and replay ﬁdelity.
Towards the goal of achieving high record ﬁdelity with
minimal overhead, Huang et al. [11] propose LEAP which
records data at a local level for shared memory locations.
This strategy reduces the record runtime overhead by 10x
compared to the prior local-order and global-order based
techniques. Zhou et al. [31] propose Stride, a relaxed recording
technique which reduces the need of synchronization whilst
recording interleaving orders and achieves a 2.5x improvement
in runtime overhead as well as a 3.8x reduction in record log
sizes compared to LEAP . Liu et al. [17] also propose Light
which achieves a 10x reduction in record log sizes compared
to Stride. However due to its recording strategy, Light may fail
to reproduce correct output for serialized write events to output
devices involving multiple shared memory locations. Also, its
replay phase enforces a global order on an execution which
increases replay runtime overhead and reduces concurrency.
AggrePlay [24] compresses some interleavings during the
record phase using a read vector. It then assigns thread-
local scheduling constraints during the replay phase improving
replay concurrency. However, it suffers higher record overhead
compared to Light.
A desirable attribute of a deterministic replay technique is
to minimize the impact of high record ﬁdelity whilst ensuring
deterministic replay.
We propose our technique based on the following insight:
During a program execution, several interleavings may be
observed between events executed by different threads on
shared memory locations. However, there are usually few to
no atomicity violations ( which means a thread accesses a
shared memory location in an atomic region at the same
time as another thread ) reported during a program execution.
Therefore, modeling the sequence of events of each thread as
a sequence of transactions enables the application of transac-
tional attributes and conditions to the events executed in the
program.
In this paper, we present TPLA Y , a novel deterministic
replay technique which segregates the sequence of events of
each thread into a sequence of transactions. During the record
phase, TPLA Y creates a new transaction on each thread’s ﬁrst
7602019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
978-1-7281-2508-4/19/$31.00 ©2019 IEEE
DOI 10.1109/ASE.2019.00076
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. write event or on any write event preceded by a read event.
Consider a scenario where two transactions tr1and tr2are
created by threads t1and t2respectively. Suppose that, all
interleavings between tr1and tr2are in the form e/squigglerighte’,
where e∈tr1and e’∈tr2, may be reduced to a single
interleaving recorded as tr1/squigglerighttr2. This reduces the record log
whilst preserving record ﬁdelity. During replay the two trans-
actions tr1and tr2can be executed sequentially, preserving all
interleavings between the two threads.
However to ensure deterministic interleaving reproduction
at a transactional level, the atomicity of each transaction must
be ascertained. We apply an existing transactional atomicity
checker during the record phase. For transactions whose
atomicity is violated, we propose a solution which we illustrate
as follows:
Suppose /angbracketlefte1,e4/angbracketright∈ tr1and/angbracketlefte2,e3/angbracketright∈ tr2and the trace
σis/angbracketlefte1,e2,e3,e4/angbracketright. TPLA Y records tr1/squigglerighttr2based on the
interleaving e1/squigglerighte2. Then on the interleaving e3/squigglerighte4,a n
atomicity violation is reported on tr1. TPLA Y removes the
transactional interleaving tr1/squigglerighttr2and records the interleaving
tr2/squigglerighte4.H o w e v e r ,a sa te v e n t e4, no information about the
interleaving e1/squigglerighte2is known. To preserve any previous
interleavings, the immediate-preceding event of e4in the same
transaction i.e., e1, is ordered before tr2, creating the artiﬁcial
interleaving e1/squigglerighttr2which is recorded by TPLA Y . This
preserves all interleaving orders in the presence of an atomicity
violation on any transaction.
During the replay phase, TPLA Y replays a program using
transactional level interleavings if no atomicity violation was
reported during the record phase. Otherwise the generated
interleavings are used in preserving the observed atomicity
violations. For the trace σin our example, TPLA Y enforces
the replay constraints tr2/squigglerighte4and e1/squigglerighttr2during replay
due to the atomicity violation.
To evaluate the performance of TPLA Y , we answer the
following questions using our experimental results:
RQ1: Can TPLA Y achieve smaller overheads compared to
an existing technique in the record phase?
TPLAY’s recording strategy results in an average of
25.44MB in record log size compared to an average of
333.20MB for Light [17].
RQ2: Does TPLA Y achieve a small runtime overhead in
the replay phase?
TPLAY’s replay strategy does not enforce a global ordering
of events on all threads. This results in a signiﬁcant decrease in
replay runtime overhead especially. Our results indicate that
TPLAY’s replay phase executes at 76% of the record phase
runtime.
RQ3: Does TPLA Y exhibit high replay ﬁdelity?
TPLAY reproduces the program state of benchmarks with a
probability of 96.6%.
Our main contributions are as follows
•We present a novel deterministic replay technique which
models the event sequence of each thread as a sequence
of transactions, then records interleavings between the
transactions to reduce record log sizes.•We present an algorithm which preserves the original
event interleavings during a transactional atomicty vio-
lation by generating an artiﬁcial pair of interleavings.
•We show the feasibility of TPLA Y by implementing it as
a tool and evaluate TPLA Y through an experiment.
The rest of the paper is organized as follows. Section
II outlines preliminary information. Section III describes a
motivating example used in presenting our technique. Section
IV details our algorithm, while Section V contains evaluation
and experimental results. Section VI discusses the related
work. Finally, Section VII concludes the paper.
II. P RELIMINARIES
This section details the preliminary information used in this
paper.
T ABLE I
PRELIMINARY INFORMA TION
Operation op:=w(x) | r(x) | acq(m) | rel(m) | fork(u) |
join(u)
x∈Memory Location; m∈Lock; u∈Thread;
Event e:=/angbracketleftt,op/angbracketright,t∈Thread; op∈Operation
Execution trace σ:=/angbracketlefte1,e2,e3,...,en/angbracketright,ei∈Event
A. Execution Trace
An execution trace σ=/angbracketlefte1,e2,...,en/angbracketrightis a sequence of
events observed from the execution of a program. An event e
represents one of the following:
•t.r(x) : A read instruction executed by thread ton memory
location x.
•t.w(x) : A write instruction executed by thread ton mem-
ory location x.
•t.acq(m) : A lock acquisition instruction executed by
thread ton lock m.
•t.rel(m) : A lock release instruction by thread ton lock m.
•t.fork(u) : Thread tforks another thread u.
•t.join(u) : Thread tjoins another thread u.
Other synchronization primitives such as wait ,signal , and
barrier are also considered by our algorithm and follow
procedures similar to the synchronization primitives above, as
show by Table I. We omit them for brevity.
B. Interleaving
An interleaving /squiggleright is deﬁned as the order by which dif-
ferent threads access shared memory objects. Three types of
interleavings may be observed in an execution:
1)write-write : two threads perform write events consecu-
tively on some shared object.
2)write-read : a thread performs a write event followed by
a read event by some other thread.
3)read-write : a thread performs a read event followed by
a write event by some other thread.
761
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Fig. 1. Running example: An execution trace σ1of a multi-threaded program
with three threads t1t2,and t3. (Dashed arrows represent the global trace σ1)
C. Transactions
RegionTrack [28] deﬁnes transactions as “a sequence of
events executed by a thread tin between of a matching pair of
events begin(t, l) and end(t, l) as a (regular) transaction tx=
/angbracketleftbegin(t,l ),...,ex,...,end(t,l)/angbracketright”, where begin(t, l) and end(t,
l)denote the beginning and the ending of an atomic region l
and ex∈tx.
We adapt their idea to formulate our notion of a transaction
and transactional atomicity violation (or atomicity violation
for short) as follows.
Deﬁnition 1. (Transaction). A transaction is a sequence
of events executed by a thread trepresented by tr =
/angbracketleftbegin(t,l ),...,ex,...,end(t,l)/angbracketrightwhere begin(t, l) refers to an
event eiwhich satisﬁes the following three conditions:
1)eiis a write event.
2) There is either an immediate preceding read event e i-1
or no event ei-1exists.
3)ei,ei-1∈σt.(thread t’s execution trace).
Also, end(t, l) is a either read event ejinσtwhich imme-
diately precedes a write event ej+1 inσtor the last event in
σt.
Deﬁnition 2. Atransactional interleaving (TI) /squigglerighttris such
that if ei/squigglerightejand ei∈tr1and ej∈tr2, then tr1/squigglerighttrtr2.
Deﬁnition 3. (Happens-Before Relation) [4], [10]: The
happens-before (HB) relation < αfor a trace αis the smallest
transitively-closed relation over the events in αsuch that the
relation a<αbholds whenever aoccurs before bin and one
of the following holds:
•Program order: The two operations are performed by
the same thread.
•Locking: The two events acquire or release the same
lock.
•Fork-join: One events is t.fork(u) ort.join(u) and the
other events is by thread u.
Fig. 2. Illustration of TPLA Y record phase on σ1. Dashed arrows represent
transaction-level interleavings and double compound arrows represents event
level interleavings.
For events a,b,c∈α,I fa <αband b<αc, then a<αc.
D. Transactional Atomicity Violation
Each thread maintains a vector clock (VC) [14] under
RegionTrack and assigns a VC to each transaction created by
the thread. RegionTrack captures HB-relations across threadsby performing a join operation on the VCs of threads during
an HB event. When an HB relation is captured, the VC of the
current thread is updated via a join operation.
Given two events e
iand ejwhere ei/arrowtailrightejand ej∈tx,
RegionTrack will report an atomicity violation on txif the
VC of tx.begin≤the VC of ei.
E. Read Count V ector Clocks
A read count (RC) vector clock [24] (a variation of Lam-
port’s vector clock [14]) is a tuple of values where each value
which tracks the number of read events of the correspondingthread in an execution trace. An RC vector clock maintains a
count of a thread’s read events to a shared memory location
in the form of RC
t[t], where trepresents the current thread.
III. M OTIV A TING EXAMPLE
We present a running example to motivate our work. Fig. 1
shows an execution trace σ1of a multi-threaded program with
three threads t1,t2and t3, 14 write and read events labeled as
e1toe14, and two shared memory locations xand y. In Fig.
1,t1executes a write then two reads e2and e3on location x.
Thread t2then executes a write e4onythen two reads e5and
e6on location x. Thread t3then executes a write e7onx, then
a write e8ony, followed by two reads e9and e10toyand
xrespectively. Thread t2executes two reads e11and e12toy
and xrespectively. Thread t1then executes a write on ythen
executes e14, which is a read on y.
An existing technique Light [17] records inter-thread ﬂow
dependencies (write-read interleavings). For σ1, Light records
the set{e1/squigglerighte5,e8/squigglerighte11,e7/squigglerighte12}and passes them as
constraints to a constraint solver to generate a feasible trace.
762
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. To enforce intra-thread access orders, Light also encodes the
constraints e1/squigglerighte2,e2/squigglerighte3,e3/squigglerighte13,e13/squigglerighte14,e4/squigglerighte5,
e5/squigglerighte6,e6/squigglerighte11,e11/squigglerighte12,e7/squigglerighte8,e8/squigglerighte9,
e9/squigglerighte10as constraints in the constraint solver. As such, Light
records a total 3 inter-thread interleavings and 11 intra-thread
interleavings for σ1.
One main drawback of Light is that it may fail to reproduce
a serialized sequence of writes to an external device involving
multiple shared memory locations. As an example, the inter-
leaving e4/squigglerighte8may be e8/squigglerighte4in some trace σ/prime
1.Light is
also limited by the capacity of constraint solvers to generate
traces.
TPLA Y reproduces the program state by enforcing write-
write ,read-write , and write-read interleavings in σ1at a
transactional level. We discuss TPLA Y in detail in the next
section.
IV . TPLA Y A LGORITHM
In this section, we present TPLA Y . The algorithms are
implemented as callback functions which are triggered on the
execution of speciﬁc events. The following notations are used
in algorithms 1 and 2:
•⊥: represents a null instruction.
•{}: represents an empty set.
•dSet t: The set of interleavings for thread t.
•rwSet t: The set of read-write interleavings for thread t.
•aSet t: The set of artiﬁcial interleavings for thread t.
•bSet t: The set of event interleavings for thread t.
•trID t: Current transaction ID for thread t.
•lwx: Last write operation to a shared memory location x.
•Ψt: Last instruction type for thread t.
•Lm: Last lock operation on lock m.
•Tid(e) : Executing thread of operation e.
•Lock(e) : Lock object acquired in operation e.
•var(e) : Shared object being accessed by operation e.
•RC t: The RC vector clock for thread t.
•execute(e) : The operation for event eis executed.
•t.yield() : The executing thread twaits for other threads
to advance without blocking.
•pop() : removes data from the ﬁrst index position of a data
structure.
•ﬁrst() : represents data in the ﬁrst index position of a data
structure.
•sinkThread : the thread which performs the second event
in an interleaving.
•sourceThread : the thread which performs the ﬁrst event
in an interleaving.
A. Record Phase
During the record phase, a modiﬁed version of RegionTrack
(RT m) is used to create transactions and track HB relations
across threads. When an interleaving is detected, TPLA Y re-
trieves the transactional information of the interleaving events
from RT mand records a transactional interleaving (TI) in dSet .
When an atomicity violation over two events is reported by
RT m, TPLA Y removes any recorded TI associated with thetwo events, then generates two event access orders which are
stored in two different sets aSet and bSet .
The TPLA Y record phase is presented in Algorithm 1. Lines
1-2 initialize the RC vector clock of thread tto 0, rwSet t,aSet t,
bSet tand dSet tto empty, trID tto 0 andΨto⊥for each thread.
During a write access event, the transaction count of thread t
is incremented by 1 if no previous instruction exists or there is
a read for t(line 4). The write operation is executed at line 5.
If the last write to the shared memory location was performed
by some other thread t’, the last write (made up of t’and the
trID t’) is ordered before the current write event and appended
todSet t(lines 6 -7). This also eliminates thread-local orderings
which are inherently deterministic. Then the last write for the
shared memory location is updated using tand trID tat line
8. Line 9 records any read-write interleavings in which the
write event is involved and updates the last instruction type
for thread t.
For each read access event, the index for the current thread
in its RC vector clock is incremented by 1 and the read
operation is executed at line 12. If the last write to the shared
memory location was performed by some other thread t’, the
last write is ordered before the current read event and appended
todSet t(lines 13 -14). Line 15 updates the last instruction type
for thread t.
On lock acquisition, the lock operation is executed at line
18. If the last lock access to some lock object mwas performed
by some other thread t’,Lmis ordered before the current
synchronization event and appended to dSet t(lines 19 -20).
The last lock access to the shared object is updated at line 21.
The function recordRW (lines 23-26) is invoked at line 9
in the onWrite function. This function retrieves the read count
values for every other thread in the set of Threads (line 24).
The updated RC vector clock for thread tand the current write
event are appended as a triple to rwSet t(line 25).
Lines 27-33 detail the action taken when an atomicity
violation is detected by RT m. If the executing thread’s id and
current transaction id exist in the dSet of the last thread tto
access the shared object refrenced by e, the record is removed
(lines 29-30). Then at line 31, /angbracketleftTid(e),e/prime,t,tr/angbracketrightis recorded in
aSet twith e’being the preceding event of einσTid(e). Finally/angbracketleftbig
t,tr,Tid(e),trID Tid(e)/angbracketrightbig
is appended to bSet Tid(e) . Algorithm
1 produces a record log which consists of aSet ,rwSet ,bSet
and dSet.
Fig. 2 illustrates the TPLA Y record scheme for σ1.O ne v e n t
e1, a transaction tr1is created by t1.lwxis also updated with
the pair/angbracketleftt1,tr1/angbracketright. Events e2and e3each increments RC t1[t1]b y
1. On e4, a transaction tr2is created by t2.lwyis also updated
with the pair /angbracketleftt2,tr2/angbracketright.R C t2is updated using RC t1and RC t3
and/angbracketleftRCt2,t2,tr2/angbracketrightis appended to rwSet t2.Events e5and e6
each increments RC t2[t2]b y1 .F o r e5,lwxis retrieved and
the interleaving /angbracketleftt1,tr1,t2,tr2/angbracketrightis appended to dSet t2.O n e6,
/angbracketleftt1,tr1,t2,tr2/angbracketrightis not recorded since it already exists in dSet t2.
On event e7, a transaction tr3is created by t3.lwxis also
updated with the pair /angbracketleftt3,tr3/angbracketright.R C t3is updated using RC t1
and RC t2. The triple /angbracketleftRCt3,t3,tr3/angbracketrightis recorded in rwSet t3.
On e8,lwyis retrieved and the interleaving /angbracketleftt2,tr2,t3,tr3/angbracketright
763
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. is appended to dSet t3.l w yis then updated with the pair
/angbracketleftt3,tr3/angbracketright. However, /angbracketleftRCt3,t3,tr3/angbracketrightis not recorded on e8since
the values in RC t3have not changed. Events e9and e10each
increments RC t3[t3] by 1. An atomicity violation is reported
one11fortr2. Since an interleaving with the pair /angbracketleftt2,tr2/angbracketright
exists in dSet t3, the interleaving is removed and an artiﬁcial
interleaving /angbracketleftt2,e6,t3,tr3/angbracketrightis then appended to aSet t3.This
ensures that tr2will be paused until e6has been executed to
reproduce the atomicity violation on tr2. Then the interleaving
/angbracketleftt3,tr3,t2,e11/angbracketrightis appended to bSet t2. On event e13,t1creates
transaction tr4.lwxis also updated with the pair /angbracketleftt1,tr4/angbracketright.R C t1
is updated using RC t2and RC t3. The triple /angbracketleftRCt1,t1,tr4/angbracketrightis
recorded in rwSet t1. Event e14ﬁnally increments RC t1[t1]b y
1.
TPLA Y produces dSetσ1={/angbracketleftt1,tr1,t2,tr2/angbracketright},bSetσ1=
{/angbracketleftt3,tr3,t2,e11/angbracketright},aSetσ1={/angbracketleftt2,e6,t3,tr3/angbracketright},rwSetσ1=
{/angbracketleft[2,0,0]t2,t2,tr2/angbracketright,/angbracketleft[2,2,0]t3,t3,tr3/angbracketright,/angbracketleft[2,4,2]t1,t1,tr4/angbracketright}
as the record log for trace σ1.
B. Replay Phase
During the replay phase, RT mis used to create transactions
only. TPLA Y divides the record log into constraint sets based
on the sinkThread in each interleaving. Each thread is assigned
a set in the form { aSet t,rwSet t,bSet tand dSet t}. The constraint
set is treated as a stack where only the ﬁrst record in each
data structure is used in evaluating the current event. When a
record is successfully used in evaluating an event, the record
is popped out of the stack.
The TPLA Y replay phase is shown in Algorithm 2. The
record log ( aSet, rwSet ,bSet and dSet ) is used as input to
replay callback functions. Lines 1-3 initialize RC vector clock
to 0, trID tto 0 and Ψto⊥for each thread respectively.
For each write event, the checkEvent and checkAv functions
are called on line 3 to ensure that the current event is not
involved in some atomicity violation. The current operation is
aborted if read-write conditions are not satisﬁed (line 4).
If the thread’s dSet Tid(e) is not empty, and the current
transaction id of the thread is lower than the sink transaction id
of the topmost record in dSet Tid(e) ,the write event is executed
(lines 5 - 7). This means each sinkThread can execute up
until its ﬁrst transaction involved in an interleaving. Also, If
the thread’s dSet Tid(e) is not empty, and the current transaction
id of the sourceThread is greater than or equal to the source
transaction id of the topmost record in dSet Tid(e) ,this means the
interleavings has been fulﬁlled. The topmost record is popped
out of dSet Tid(e) and the write event is executed. (lines 8-9).
When neither of the two previous conditions are met at lines
7 and 8, the thread waits without blocking other threads at
line 10. At Lines 11 and 12 the transaction id for the thread
is incremented if the last event for the thread is either a read
event or a null event. The write event is executed and the last
instruction for the thread is updated at line 13.
On handling read events, on line 16, the two functions
checkEvent and checkAv are called to ensure that the current
event is not involved in some atomicity violation. Lines 17-
18 update the RC vector clock of Tid(e) , execute the readAlgorithm 1 TPLAY record algorithm
1)∀t∈Thread do RC t=0;Ψt=⊥; trID t=0; dSet t={};
2) rwSet t={}; aSet t={}; bSet t={};
3)onWrite (Event e )do
4) if (ΨTid(e)∈{⊥,read} ){trID t++;}
5) execute(e);
6) if(Tid(lw var(e))/negationslash=Tid( e)){
7) dSet t^=/angbracketleftbig
Tid(lwe),lwe,Tid(e),trID Tid(e)/angbracketrightbig
;}
8) lw e=/angbracketleftbig
Tid(e),trID Tid(e)/angbracketrightbig
;
9) recordRW( Tid( e),e);ΨTid(e) =write;
10) onWrite
11) onRead (Event e )do
12) RC Tid(e) [t] ++; execute( e);
13) if (Tid(lw e)/negationslash=Tid( e)){
14) dSet t^=/angbracketleftbig
Tid(lwe),lwe,Tid(e),trID Tid(e)/angbracketrightbig
;}
15)ΨTid(e) =write;
16) end onRead
17) onLockAcquire( Event e ):
18) execute( e) ;m=v a r ( e);
19) if(Tid(L m)/negationslash=Tid( e)){
20) dSet t^=/angbracketleftbig
Tid(Lm),Lm,Tid(e),trID Tid(e)/angbracketrightbig
;}
21) L m^=/angbracketleftbig
Tid(e),trID Tid(e)/angbracketrightbig
;
22) End onLockAcquire
23) recordRW (Thread t,Event e )do
24)∀t’∈Thread do RC t[t’]=RC t’[t’];
25) rwSet t^=/angbracketleftbig
RCt,Tid(e),trID Tid(e)/angbracketrightbig
;
26) end recordRW
27) onAtomicityViolation( Event e ):
28)∀θ∈(Lvar(e) ,lwvar(e) ){t=Tid(θ);tr=θ.trID; }
29) if(/angbracketleftbig
Tid(e),trID Tid(e)/angbracketrightbig
∈dSet t){
30) dSet t=dSet t//angbracketleftbig
Tid(e),trID Tid(e),t,tr/angbracketrightbig
);
31) aSet t^=/angbracketleftTid(e),(e/prime),t,tr/angbracketright;}
32) bSet Tid( e)^=/angbracketleftbig
t,tr,Tid(e),trID Tid(e)/angbracketrightbig
;
33) end onAtomicityViolation
event and update the last instruction of the thread with a read
instruction.
Lock access events follow a similar replay strategy to read
and write events. If the current transaction id for Tid(e) is less
than the topmost transaction id in dSet Tid(e) , the lock operation
is executed (lines 21-23). Otherwise if the sourceThread in-
volved in the dependency has a trID t’greater than or equal to
the transaction id from dSet Tid(e) , the lock operation is executed
(line 24-25) and the topmost record in dSet Tid(e) is removed
since the interleaving constraint has been satisﬁed at line 24.
The thread waits without blocking other threads at line 26 if
the conditions on lines 22 and 24 are not satisﬁed.
The checkRW (lines 28-33) function is invoked at line 4 in
the onWrite function. This function retrieves the read count
values for every other thread in the thread set Thread and
updates the RC vector clock of t(line 29). Line 31 iterates
764
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Algorithm 2 The TPLAY replay algorithm
1)∀t∈Thread do RC t=0;Ψt=⊥; trID t=0;
2)onWrite (Event e )do
3) checkEvent(); checkAv();
4) if (!checkRW( Tid( e),e)){ Tid(e).yield(); }
5) if (dSet Tid(e) contains records ){
6) rec =dSet Tid(e) .ﬁrst();
7) if (trID Tid(e) < rec. trID Tid(e) ){skip to line 11 }
8) else if ( trID t’>= rec.trID t’){
9) dSet Tid(e) .pop(); skip to line 11 }
10) else { Tid(e).yield(); }}
11) if (ΨTid(e)∈{⊥, read} ){
12) trID Tid(e) ++;}
13) execute(e); ΨTid(e) =write;
14) end onWrite
15) onRead (Event e )do
16) checkEvent(); checkAv();
17) RC Tid(e) [Tid(e)] ++; execute(e);
18) ΨTid(e) =read;
19) end onRead
20) onLockAcquire( Event e ):
21) rec =dSet Tid(e) .ﬁrst();
22) if (trID Tid(e) < rec. trID Tid(e) ){
23) execute(e); }
24) else if ( trID t’>= rec.trID t’){
25) execute(e); dSet Tid(e) .pop(); }
26) else { Tid(e).yield(); }
27) End onLockAcquire
28) checkRW (Thread t,Event e )do
29)∀t’∈Thread do RC t[t’]=RC t’[t’];
30)∀t’∈Thread do
31) if (RC t[t’] < rwSet t[0].RC t[t’]){return false; }
32) rwSet t.pop() ; return true;
33) end checkRW
34) checkEvent (Event e )do
35) rec =bSet Tid(e) .ﬁrst();
36) if (eTid(e) < rec. e){
37) execute(e); }
38) else if ( trID t’>rec.trID t’){
39) execute(e); bSet Tid(e) .pop(); }
40) else { Tid(e).yield(); }
41) end checkEvent
42) checkAv (Event e )do
43) rec =bSet Tid(e) .ﬁrst();
44) if (trID Tid(e) < rec. trID ){
45) execute(e); }
46) else if ( et’>rec.e){
47) execute(e); aSet Tid(e) .pop(); }
48) else { Tid(e).yield(); }
49) end checkEventover RC tand returns false if each thread’s current read count
value is less than the recorded value in rwSet t. Or else the
interleaving is satisﬁed and removed from rwSet tat line 32 .
The checkEvent function ensures that the sinkThread blocks
prior to an event which is involved in an atomicity violation.
Lines 36-37 ensure thread execution until the the event in the
topmost record in bSet Tid(e) has been executed.
Alternatively line 38 checks if the sourceThread’s transac-
tion id is greater than the recorded transaction id in bSet Tid(e) .
If the condition is satisﬁed, the event is executed and the
interleaving is removed from bSet Tid(e) at line 39. When both
conditions are not met, the thread identifying by Tid(e) waits
without blocking other threads at line 40.
The checkAv function ensures the artiﬁcial interleaving is
enforced by the source thread. Lines 44-45 ensure thread
execution until the event in the topmost record in aSet Tid(e) .
Otherwise line 46 checks if the source thread’s current event
is greater than the recorded event. If the condition on line
46 is satisﬁed, the event is executed and the interleaving is
removed from bSet Tid(e) at line 47. Otherwise Tid(e) waits
without blocking other threads at line 48.
The trace σ1is replayed as follows: During the replay
phase, t1creates tr1with event e1in the abscence of any
interleaving constraint involving tr1. Events e2and e3are
executed and each event increments RC t1[t1] by 1. Then t3
attempts to create tr3but fails since there are two constraints
ontr3({/angbracketleftt2,e6,t3,tr3/angbracketright} and{/angbracketleft[2,2,0]t3,t3,tr3/angbracketright} not yet
satisﬁed. t1then attempts to create tr4and t1fails due to
the interleaving {/angbracketleft[2,4,2]t1,t1,tr4/angbracketright}not yet satisﬁed. t2then
proceeds to create tr2since the constraints {/angbracketleftt1,tr1,t2,tr2/angbracketright}
and{/angbracketleft[2,0,0]t2,t2,tr2/angbracketright}are satisﬁed. t2executes e5and e6
and updates RC t2[t2].t2pauses at e11due to the interleaving
{/angbracketleftt2,e6,t3,tr3/angbracketright}not yet satisﬁed. t3proceeds to create tr3and
executes events e7and e8. Events e9and e10each increments
RC t3[t3]b y1 . t2then executes e11and e12, reproducing the
atomicity violation on tr2. Finally, t1creates tr4and executes
e13and e14.
C. Thread abstraction & matching
Algorithms 3 and 4 present the thread abstraction process
in the record phase and subsequent matching process in the
replay phase using the following notations:
•osTid t: System-assigned value for thread t.
•opsTid t: System-assigned value for parent thread of
thread t.
•pTid t: The ID of the parent thread of thread t.
•Ω: Data Structure mapping opsTid ttoosTid t.
•tLog : Set of thread abstractions.
•Γ: Data Structure mapping each replay thread to its
current number of children threads.
•Φ: Data Structure mapping replay threads to threads from
tLog.
At line 1 of algorithm 3, Ωand tLog are initialized as empty
structures. On thread creation, a value pair made up of the
system-assigned value and the thread id is appended to Ω(line
3). For all threads with the exception of the main thread, a
765
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. value pair made up of the thread id and its parent thread id is
appended to the set of thread abstractions tLog at line 4.
Algorithm 3 TPLAY thread abstraction algorithm
1) tLog={}; Ω={};
2)onThreadCreate (Thread t )do
3)Ω^=/angbracketleftosTidt,Tidt/angbracketright;
4) if(Tid t/negationslash=0){tLog ^=/angbracketleftTidt,Ω[opsTid t]/angbracketright;}
5)end onThreadCreate
For Fig. 1, the resulting thread abstraction set is
{/angbracketleft1,0/angbracketright,/angbracketleft2,0/angbracketright,/angbracketleft3,0/angbracketright}.
Algorithm 4 TPLAY thread matching algorithm
1)Ω={};Γ={};Φ={}; tLog;
2)onThreadCreate (Thread t )do
3)Ω^=/angbracketleftosTidt,t/angbracketright;
4) if (Tid t== 0 ){Φ^=/angbracketleft0,0/angbracketright;}
5) else {Γ[Ω[opsTid t]]++;
6) freq = 1; parent = Φ[Ω[opsTid t]];
7) childFreq = Γ[Ω[opsTid t]];
8) ∀/angbracketlefta,b/angbracketright∈ tLog {
9) if (b == parent && freq == childFreq ){
10) Φ^=/angbracketleftt,a/angbracketright; break; }
11) elseif ( b == parent && freq /negationslash=replayFreq ){
12) freq++ ; }}}
13) end onThreadCreate
Algorithm 4 presents the thread matching algorithm in the
replay phase. tLog is used as an input to the thread matching
algorithm. Data structures Ω,ΓandΦare all initialized as
empty sets at line 1. On thread creation, the system-assigned
value and the thread id pair is appended to Ωat line 3. If the
thread is the main thread, the pair /angbracketleft0,0/angbracketrightis appended to Φ(line
4).The main thread in each execution is always assigned with
id 0.
Otherwise, the number of children threads for the current
thread’s parent is incremented by 1 at line 5. Then we iterate
over each thread abstraction in tLog (lines 8-12). If the second
value ( b) of a thread abstraction matches a parent thread and
the parent thread’s freq value matches the value of childFreq ,
the current thread id and the ﬁrst value (a) of the matching
abstraction is appended to Φ(lines 9-10). Otherwise, the
freq value is incremented by 1 at line 12 when the thread
abstraction is parent thread is matched but the child frequency
is not.
V. E V ALUA TION
A. Execution Environment
Our hardware setup consisted of a Dell PowerEdge R930
running the Dell Customized Image ESXi 6.0.0 Update 2
A01. Our experiments were conducted on a 64 bit virtual
machine running the guest OS Ubuntu 18.04 Linux with 8T ABLE II
EXECUTION MET ADA T A FOR BENCHMARKS USED IN OUR EXPERIMENT .
Benchmarks# of Events
Read Write Lock
cholesky 13,539,277 3,576,547 1,529
fft 23,738,757 14,934,083 34
lu_cb 95,376,483 45,684,566 274
lu_ncb 94,188,973 45,662,755 274
ocean_cp 95,195,697 18,453,146 4,434
ocean_ncp 95,164,212 18,453,231 4,427
raytrace 319,947,352 69,805,540 239,444
radiosity 3,403,130 2,061,336 76
radix 237,630,987 78,661,575 213,960
volrend 35,082,358 10,642,543 7,133
water_nsquared 117,250,198 46,340,899 6,294
water_spatial 104,690,984 41,625,532 159
barnes 2,784,956,319 1,680,710,156 275,331
mysql 1,560,872 542,635 6
Intel Xeon(R) CPU E7- 4850 v3 @ 2.20Ghz processors, and
16GB of RAM.
We have implemented TPLA Y and Light using Intel PIN
version 3.0-76991 [19]. To be speciﬁc, for each tool, we
implemented two separate pintools, for the record and replay
phases respectively. In the case of Light, we followed the
implementation in the paper using a solver for the Integer
Difference logic theory in z3[9]. The record implementation
of Light is publicly available without the replay phase and
constraint solver and can only handle Java applications. Our
benchmarks were run on C/C++ programs with the pthread
standard. This made direct comparison difﬁcult. A precaution
taken was to test the correctness of our implementation of
Light on a small benchmark we developed prior to our exper-
iments. We also used code inspection on our implementations.
B. Benchmarks
We evaluated our implementation using benchmarks from
the Splash2x extension of the P ARSEC 3.1 benchmark suite
[3], speciﬁcally barnes, ocean_cp, radiosity, raytrace, vol-
rend, water_spatial, water_nsquared ,water_spatial as well
as kernel applications cholesky, fft, lu_cb, lu_ncb, radix , and
mysql . We selected the Splash2x extension of P ARSEC for its
focus on concurrent computation on parallel machines. Table
II details the number of read, write and lock acquisition events
in each program under the experiment conﬁguration.
C. Methodology
We ran each benchmark in the native conﬁguration to
establish native execution runtime. We reported this as the
base time in our experiment results.
Each Splash2x benchmark program was conﬁgured with
4 worker threads with the gcc-pthreads conﬁguration option
and the test-input workload. This conﬁguration provided each
program adequate concurrency and input. The input for mysql
was MySQL (Bug #85413) .
1) Record and Replay setup: For our record phase, each
thread kept a local instruction counter and incremented it by
1 for each instruction executed. Each thread also kept a vector
766
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. T ABLE III
EXPERIMENT AL RESUL TS ON RECORD PHASE OF TPLA Y AND LIGHT .
Benchmark Application DomainLog Size (MB)Base time(s)Normalized Slowdown
TPLAY Light Light/TPLAY
cholesky HPC 10.5 39.90 0.02 0.19
fft Signal Processing 12.20 32.70 0.05 0.19
lu_cb HPC 8.30 503 0.09 0.64
lu_ncb HPC 8.30 502.50 0.125 0.72
ocean_cp HPC 17.50 44 0.188 0.47
ocean_ncp HPC 27.70 43.90 0.19 0.77
raytrace Graphics 50.90 765 0.0574 0.03
radiosity Graphics 7.50 13.10 0.092 0.05
radix General 59.30 720 0.554 0.01
volrend Graphics 12 152.30 0.086 0.06
water_nsquared HPC 10.10 561.10 0.17 0.13
water_spatial HPC 5.20 396.90 0.16 0.11
barnes HPC 126 890 1.044 1.97
mysql Database application 0.60 0.40 0.8548 0.75
Mean 25.44 333.20 0.26 0.44
T ABLE IV
RECORD PHASE DATA F O R TPLA Y AND LIGHT . (TI REFERS TO TRANSACTIONAL INTERLEA VINGS .A V REFERS TO ATOMICITY VIOLA TIONS .T HE
V ALUES MARKED WITH *REPRESENT THE INTERLEA VING SET SIZES NOT SOL VED BY Z 3).
Benchmark # of Transactions #o fT I #A V # of Light W-R interleavings
cholesky 9,632 761 - 2,009,143*
fft 11,355,241 639,294 - 1,672,548*
lu_cb 46,392,503 399,169 - 23,785,125*
lu_ncb 46,117,695 397,970 - 23,757,388*
ocean_cp 18,536,819 239,971 - 2,360,277*
ocean_ncp 17,545,516 287,698 - 2,351,007*
raytrace 17,108,800 1,203,113 2 73,440,436*
radiosity 2,057,238 413,560 - 777,345*
radix 21,165,176 2,177,458 - 77,013,873*
volrend 5,559,482 421,344 98 7,711,556*
water_nsquared 42,878,818 254,304 - 28,316,969*
water_spatial 39,523,924 217,046 - 20,725,403*
barnes 69,510,023 8,510,541 - 411,164,199*
mysql 49,821 9,581 - 352
Mean 24,129,334 1,083,700 50 48,220,401
clock to track reads by other threads as well as a transaction
counter which was incremented for every transaction created
by the thread. For all shared memory locations, we maintained
a data structure to store data on the last access to each
shared memory location. We also kept a global map of each
memory address with its associated data structure which is
write protected by a single lock during initial creation and
storage of the data structure. However, for subsequent access
to each memory address index in the map, we maintained a set
of 210locks which were acquired via a hash function (similar
to Light).
For read events, we conﬁgured each thread to keep track of
its read accesses to shared memory locations and made this
data structure accessible to other threads. We protected access
to this data structure by assigning each thread its own lock.
Throughout the record phase, TPLA Y was conﬁgured to keep
all recorded data in memory until the program exited or was
terminated.
Apart from read-write interleavings, we stored each in-
terleaving (regardless of interleaving type) in the format
/angbracketlefta,b,c,d/angbracketrightwhere a,b,cand drepresented the sourceThread
id, the transaction/event id executed by the sourcThread, sink-Thread id and transaction/event id executed by the sinkThread
respectively. A thread id was typically a 32-bit integer whereas
an event/transaction id was a 64-bit unsigned integer. Read-
write interleavings were stored in the format /angbracketleftRC,a,b/angbracketrightwhere
RC represented a vector of read aggregate values from all
threads and aand brepresent the sink thread id and sink
transaction/event id respectively.
We recorded time spent (total processor clock ticks for each
run) using the clock function1. The time spent for each tool
is calculated by recording the value returned by the clock
function at the beginning and at the end of each run. The
normalized slowdown difference between TPLA Y and Light
was calculated using the following formula ( time spent for
Light / time spent for TPLAY ) and is shown in column 6 of
Table III.
During the replay phase, each thread created during the
replay phase was matched to a record phase thread using
our thread matching algorithm i.e. Algorithm 4, and each
thread only kept a record of interleavings within which it
served as a sinkThread. During replay, we maintained an
1man7.org/linux/man-pages/man3/clock.3.html
767
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. instruction counter, transaction counter for each thread. To
ensure optimal concurrency, each thread was conﬁgured to
evaluate the interleaving constraint without synchronization.
This meant for every interleaving /angbracketlefta,b,c,d/angbracketright, the sinkThread c
only had to check if the sourceThread a’s current transaction
id was greater than the recorded transaction id b. This ensured
that the sourceThread was not blocked until the interleaving
was conﬁrmed by the sinkThread. The artiﬁcial interleavings
created on atomicity violations were evaluated in the same
manner.
Each interleaving was removed from the data structures
once it was satisﬁed. TPLA Y relinquished control of the pro-
gram execution to the system scheduler when all interleavings
were satisﬁed.
We recorded each interleaving for Light in the format
/angbracketlefta,b,c,d/angbracketrightwhere a,b,cand drepresent the source thread id,
the source event id, sink thread id and sink event id respec-
tively. Our implementation of TPLA Y in the Pin framework
is available online2.
D. Constraint solving Approach
To benchmark the Light replay technique, the Z3 constraint
solver was used with write-read interleavings from the Light
record phase as inputs. We did this in accordance with the
steps stated in [17]. A constraint solver accepts as input a
set of statements which are encoded as constraints. In our
experiments, we implemented a constraint solver using the
Z3Py library (version 4.8.4) of Z3 [9] theorem solver for
python programming. We present an example of the constraint
solving approach as follows:
Fig. 3. Running example: An execution trace of a multi-threaded program
with threads t1t2,t3. (Dashed arrows represent the global trace σ2)
The trace σ2=/angbracketlefte1,e2,e3,e4,e5,e6,e7,e8,e9/angbracketrightrepresents
the execution shown in Fig. 3. The interleavings produced
fromσ2are e1/squigglerighte3,e3/squigglerighte4,e2/squigglerighte5,e6/squigglerighte7,e6/squigglerighte8,e8/squigglerighte9
and e7/squigglerighte9.
Light records the inter-thread interleavings e3/squigglerighte4,
e6/squigglerighte7ande6/squigglerighte8. We encode each interleaving for the
constraint solver in the form {a−b−c−d}where a,b,cand
2github.com/testrepo007/TPLAYdrepresent the thread id from the write event, the event id for
the interleaving write event, the thread id for the read event
and the event id for the interleaving read event respectively.
The set of write-read interleavings for σ2is encoded
as{{3−3−2−4},{2−6−3−7},{2−6−2−8}}. The
constraint solver also accepts the intra-thread interleaving con-
straints{1−1−1−9},{2−2−2−4},{2−4−2−6},
{2−6−2−8},{3−3−3−5},{3−5−3−7}.
The execution schedules {e1,e2,e3,e4,e5,e6,e7,e8,e9}and
{e2,e1,e3,e4,e6,e5,e7,e8,e9}may be generated by the con-
straint solver.
Our Z3Py code for the constraint solver is available online3.
E. Record Phase results
Table III details our experimental results for the record
phase. The ﬁrst two columns detail the benchmarks and their
respective application domains. The next two columns show
the record log sizes for TPLA Y and Light in megabytes.
Base Time represents the native runtime for each benchmark
in seconds. Column 6 of Table III shows the difference in
time spent between TPLA Y and Light. On average, Light
achieves 44% of runtime overhead of TPLA Y because TPLA Y
incurs runtime cost in creating and maintaining transaction
data structures as well as the transaction atomicity checker
RT m.
Our experimental results indicate an average of 25.44MB,
which is a 13-fold improvement when compared to Light’s
average of 333.20MB. Speciﬁcally, TPLA Y records a smaller
log size for 13 out of 14 programs. Light experiences a slight
gain for mysql, which does not produce enough transactions
to highlight the efﬁciency of TPLA Y .
Table IV shows the results on trace reduction for TPLA Y
and Light. Column 2 shows the number of transactions created
by TPLA Y for each program execution. Column 3 shows the
number of transactional interleavings whilst column 4 shows
the number of atomicity violations reported for each program
execution. Finally, Column 5 shows the interleavings recorded
by Light.
We also compare the number of transactional interleavings
with the number of write-read interleavings recorded by Light.
Table IV shows a 44-fold improvement in trace reduction by
TPLA Y over Light.
Atomicity violations were reported for two benchmarks
volrend (98) and radiosity (2).For each atomicity violation
reported, TPLA Y created a pair of interleavings, one inter-
leaving was stored for the sourceThread in aSet and another
was saved for the sinkThread in bSet . No atomicity violations
are reported for 12 out of 14 benchmarks, thereby conﬁrming
our insight. This also means that these benchmarks could be
replayed exclusively at the transactional level.
F . Replay Phase results
Table V presents the experimental results on replay phases
for TPLA Y . A successful replay run for TPLA Y in our
3github.com/testrepo007/Constraint-solver
768
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. T ABLE V
EXPERIMENT AL RESUL TS FOR REPLA Y PHASE .
BenchmarkNormalized Slowdown# Successful Executions % Replay ProbabilityTPLAY Light
cholesky 0.86 - 45 90
fft 0.83 - 50 100
lu_cb 0.8 - 50 100
lu_ncb 0.77 - 50 100
ocean_cp 0.77 - 50 100
ocean_ncp 0.71 - 41 82
raytrace 0.72 - 50 100
radiosity 0.8 - 50 100
radix 0.77 - 50 100
volrend 0.79 - 50 100
water_nsquared 0.85 - 50 100
water_spatial 0.64 - 50 100
barnes 0.7 - 40 80
mysql 0.57 0.77 50 100
Mean 0.76 0.77 48.3 96.6
experiment was the reproduction of interleavings observed in
the record phase and the program output. A successful replay
for Light was the reproduction of write-read interleavings for
a program. Each subject program was run 50 times.
Recall that Light employs a constraint solver in generating
possible schedules for replay. However, in our experiment
we found the constraint solver strategy to be limited in con-
structing feasible schedules due to the high number of write-
read interleavings generated for our benchmarks. Column 5
of Table IV shows the number of inter-thread write-read
interleavings recorded by Light. With the exception of mysql
(26 possible traces), the constraint solver did not return any
output even after several hours of constraint solving. For
mysql , Light succeeded in reproducing correct interleavings
80% of the time.
The time spent for each tool in the replay phase was
calculated in a manner similar to the record phase (See
paragraph 6 of subscetion C of Section V). The normalized
runtime for each tool in the replay phase is calculated as the
(time spent for tool in replay phase / time spent for tool in
record phase) . The results are presented in columns 2 and 3 of
of Table V . TPLA Y’s replay phase is able to execute at 76%
of its record phase on average.
On average, TPLA Y replays programs with a 96.6% prob-
ability, with a mean of 48.3 successful executions out of
50 executions. For cholesky , and ocean _ncp , TPLA Y fails to
achieve 100% probability due to the program outputs being
different from that observed in the record phase.
G. Threats to V alidity
The ability of the TPlay tool in the experiment to report
atomicity violations was dependent on the algorithm in [28].
Our implementation of Light (and TPLA Y) was also subject
to inherent binary instrumentation limitations of Pin. Also the
constraint solver was developed using the python library of
z3 (Z3Py)4. Another way of encoding constraints or other
constraint solvers may make Light able to produce thread
4github.com/Z3Prover/z3schedules for its replay phase. TPLA Y may also suffer over-
heads in the record phase mainly during the creation and
maintainance of data structures for transactions. Threads not
previously encountered in the record phase but observed in the
replay phase may not be handled by TPLA Y .
VI. R ELA TED WORK
There are many existing deterministic replay techniques
such as Samsara [26] and Odr [1] that are implemented for
C/C++ applications based on the pthread execution model.
TPLA Y follows a similar implementation style to these tech-
niques.
Some recent techniques such as DoubleTake [18] conduct
record and replay in one sinlge phase. In these techniques, an
execution is divided into epochs based on either irrevocable
system calls or user-deﬁned conditions. The programs state
is logged just before each epoch creation and execution, and
the epoch is analyzed afterwards. If any error is found in an
epoch, the program state prior to the epoch is restored and the
epoch is replayed to reproduce the error. Otherwise, the next
epoch is created and executed. DoubleTake requires special
hardware support whereas TPLA Y does not. Unlike MobiPlay
[25], TPLA Y does not modify the underlying framework to
facilitate replay. AggrePlay [24] tracks the number of read
accesses by each thread prior to some write event, then
aggregates these numbers of read accesses by all threads in
a read vector. The read vector is then associated with the
write event. AggrePlay however, incurs higher overheads in the
record phase compared to TPLA Y by recording all interleaving
sets.
Checkpointing is often used in replay techniques. iReplayer
[16] stores the system state in memory and enables user-
customizable checkpointing rules. iReplayer acheives small
log sizes by not monitoring data races during record. It may
report them in the replay phase by iteratively replaying an
epoch if there is a divergence from the thread interleavings
observed during replay and the recorded thread interleavings.
TPLA Y does not use checkpointing and replays data races
without iteration.
769
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Processor-oblivious record and replay [27] focuses on
recording the synchronization order for programs which em-
ploy task-parallelism. Such programs do not have any notion
of threads or data and are inherently data-race free, e.g.,
Cilk programs. However, current mainstream software sup-
ports multi-threaded parallelism. TPLA Y is applicable to most
mainstream software.
Recording write-read interleavings has been explored by
existing work. Light [17] records inter-thread and intra-
thread write-read interleavings and uses a constraint solver
to generate feasible execution schedules with the interleav-
ings as constraints. TPLA Y records write-read interleavings
at a transactional level and does not require any constraint
solver. Minimizing record impact on runtime overhead via
thread-local data storage has been explored by existing work
CLAP [12], which relies on constraint solving to generate
execution schedules. CARE [13] maintains thread-local caches
for shared memory locations, records cache-missed write-read
interleavings but does not record write-read interleavings if the
interleaving results in a cache hit. CARE records exact read-
write interleavings unlike TPLA Y . Some existing techniques
focus on trace reduction strategies. Netzer [22] has proposed
an adaptive tracing strategy which records the minimal amount
of data required to replay a speciﬁc race condition. This is
achieved by optimizing transitively-implied inter-thread de-
pendencies similar to Light. TPLA Y by default does not record
interleavings at the event level.
Xu et al. [30] proposes a regulated transitive reduction
algorithm which improves Netzer’s trace reduction technique
by generating artiﬁcial dependencies on inter-thread depen-
dencies. TPLA Y generates artiﬁcial dependencies but only
when an atomicity violation is reported on an event within a
transaction. OCTET [5] avoids creating thread interleavings by
associating each shared object with a thread-local state variable
and a mechanism based on the concurrent read, exclusive write
strategy. TPLA Y reduces thread interleavings by executing
threads’ instructions as atomic code regions and recording
interleavings on these atomic regions.
Existing replay techniques like Stride [31] include a search
process and attempt to re-construct the missing interleavings
before generating a trace with the targeting output in its replay
phase. Bbr [7] uses a predeﬁned set of location checkpoints
to minimize record log sizes as well as a symbolic-execution
based search process to generate feasible traces involving the
checkpoints. TPLA Y does not rely on a search process, sym-
bolic execution nor any ofﬂine phase.Similar to Stride, ODR
aims to reproduce a speciﬁc program output by extracting
some execution trace data from a core dump and generating a
trace through a search process.
Deterministic replay is applied in other ﬁelds of discipline.
DETER [15] is a deterministic replay tool used in recording
transmission control protocol (TCP) packets to reproduce
communication network states. TPLA Y does not focus on
replay of communication networks.VII. C ONCLUSION
Existing deterministic replay techniques proposed strategies
which attempt to reduce record log sizes and achieve suc-
cessful replay. However, these techniques still generate large
logs and and achieve replay only under certain conditions.
We have proposed a solution based on the division of the
sequence of events of each thread into sequential blocks called
transactions. Our insight is that there are usually few to
no atomicity violations among transactions reported during
a program execution. We have presented TPLA Y , a novel
deterministic replay technique which records thread access
interleavings on shared memory locations at the transactional
level. TPLA Y also generates an artiﬁcial pair of interleavings
when an atomicity violation is reported on a transaction. We
present an experiment using the Splash2x extension of the
P ARSEC benchmark suite. Experimental results have indicated
that on average, TPLA Y experienced a 13-fold improvement
in record log sizes and achieved a high replay probability.
Future work includes solving the problem of high runtime
overhead in the record phase for TPLA Y .
REFERENCES
[1] G. Altekar and I. Stoica, “Odr: Output-deterministic replay for
multicore debugging,” in Proceedings of the ACM SIGOPS 22Nd
Symposium on Operating Systems Principles , ser. SOSP ’09. New
Y ork, NY , USA: ACM, 2009, pp. 193–206. [Online]. Available:
http://doi.acm.org/10.1145/1629575.1629594
[2] D. F. Bacon and S. C. Goldstein, Hardware-assisted replay of multipro-
cessor programs . ACM, 1991, vol. 26, no. 12.
[3] C. Bienia, S. Kumar, J. P . Singh, and K. Li, “The parsec benchmark suite:
Characterization and architectural implications,” in Proceedings of the
17th international conference on Parallel architectures and compilation
techniques . ACM, 2008, pp. 72–81.
[4] S. Biswas, J. Huang, A. Sengupta, and M. D. Bond, “Doublechecker:
Efﬁcient sound and precise atomicity checking,” SIGPLAN Not. ,
vol. 49, no. 6, pp. 28–39, Jun. 2014. [Online]. Available: http:
//doi.acm.org/10.1145/2666356.2594323
[5] M. D. Bond, M. Kulkarni, M. Cao, M. Zhang, M. Fathi Salmi,
S. Biswas, A. Sengupta, and J. Huang, “Octet: Capturing and
controlling cross-thread dependences efﬁciently,” in Proceedings of the
2013 ACM SIGPLAN International Conference on Object Oriented
Programming Systems Languages; Applications , ser. OOPSLA ’13.
New Y ork, NY , USA: ACM, 2013, pp. 693–712. [Online]. Available:
http://doi.acm.org/10.1145/2509136.2509519
[6] Y . Chen, S. Zhang, Q. Guo, L. Li, R. Wu, and T. Chen, “Deterministic
replay: A survey,” ACM Comput. Surv. , vol. 48, no. 2, pp. 17:1–17:47,
Sep. 2015. [Online]. Available: http://doi.acm.org/10.1145/2790077
[7] A. Cheung, A. Solar-Lezama, and S. Madden, “Partial replay of
long-running applications,” in Proceedings of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on Foundations of
Software Engineering , ser. ESEC/FSE ’11. New Y ork, NY , USA:
ACM, 2011, pp. 135–145. [Online]. Available: http://doi.acm.org/10.
1145/2025113.2025135
[8] J.-D. Choi and H. Srinivasan, “Deterministic replay of java multithreaded
applications,” in Proceedings of the SIGMETRICS Symposium on
Parallel and Distributed Tools , ser. SPDT ’98. New Y ork,
NY , USA: ACM, 1998, pp. 48–59. [Online]. Available: http:
//doi.acm.org/10.1145/281035.281041
[9] L. De Moura and N. Bjørner, “Z3: An efﬁcient smt solver,” in
Proceedings of the Theory and Practice of Software, 14th International
Conference on Tools and Algorithms for the Construction and
Analysis of Systems , ser. T ACAS’08/ET APS’08. Berlin, Heidelberg:
Springer-V erlag, 2008, pp. 337–340. [Online]. Available: http://dl.acm.
org/citation.cfm?id=1792734.1792766
770
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. [10] C. Flanagan, S. N. Freund, and J. Yi, “V elodrome: A sound and
complete dynamic atomicity checker for multithreaded programs,”
SIGPLAN Not. , vol. 43, no. 6, pp. 293–303, Jun. 2008. [Online].
Available: http://doi.acm.org/10.1145/1379022.1375618
[11] J. Huang, P . Liu, and C. Zhang, “Leap: Lightweight deterministic
multi-processor replay of concurrent java programs,” in Proceedings
of the Eighteenth ACM SIGSOFT International Symposium on
Foundations of Software Engineering , ser. FSE ’10. New Y ork,
NY , USA: ACM, 2010, pp. 207–216. [Online]. Available: http:
//doi.acm.org/10.1145/1882291.1882323
[12] J. Huang, C. Zhang, and J. Dolby, “Clap: Recording local
executions to reproduce concurrency failures,” SIGPLAN Not. ,
vol. 48, no. 6, pp. 141–152, Jun. 2013. [Online]. Available:
http://doi.acm.org/10.1145/2499370.2462167
[13] Y . Jiang, T. Gu, C. Xu, X. Ma, and J. Lu, “Care: Cache guided
deterministic replay for concurrent java programs,” in Proceedings of
the 36th International Conference on Software Engineering , ser. ICSE
2014. New Y ork, NY , USA: ACM, 2014, pp. 457–467. [Online].
Available: http://doi.acm.org/10.1145/2568225.2568236
[14] L. Lamport, “Time, clocks, and the ordering of events in a distributed
system,” Communications of the ACM , vol. 21, no. 7, pp. 558–565, 1978.
[15] Y . Li, R. Miao, M. Alizadeh, and M. Y u, “DETER: Deterministic TCP
replay for performance diagnosis,” in 16th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 19) . Boston,
MA: USENIX Association, 2019, pp. 437–452. [Online]. Available:
https://www.usenix.org/conference/nsdi19/presentation/li-yuliang
[16] H. Liu, S. Silvestro, W . Wang, C. Tian, and T. Liu, “ireplayer: In-situ
and identical record-and-replay for multithreaded applications,” in
Proceedings of the 39th ACM SIGPLAN Conference on Programming
Language Design and Implementation , ser. PLDI 2018. New
Y ork, NY , USA: ACM, 2018, pp. 344–358. [Online]. Available:
http://doi.acm.org/10.1145/3192366.3192380
[17] P . Liu, X. Zhang, O. Tripp, and Y . Zheng, “Light: Replay via tightly
bounded recording,” SIGPLAN Not. , vol. 50, no. 6, pp. 55–64, Jun.
2015. [Online]. Available: http://doi.acm.org/10.1145/2813885.2738001
[18] T. Liu, C. Curtsinger, and E. D. Berger, “Doubletake: Fast and precise
error detection via evidence-based dynamic analysis,” in Proceedings
of the 38th International Conference on Software Engineering , ser.
ICSE ’16. New Y ork, NY , USA: ACM, 2016, pp. 911–922. [Online].
Available: http://doi.acm.org/10.1145/2884781.2884784
[19] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney,
S. Wallace, V . J. Reddi, and K. Hazelwood, “Pin: Building customized
program analysis tools with dynamic instrumentation,” SIGPLAN
Not. , vol. 40, no. 6, pp. 190–200, Jun. 2005. [Online]. Available:
http://doi.acm.org/10.1145/1064978.1065034
[20] P . Montesinos, L. Ceze, and J. Torrellas, “Delorean: Recording and
deterministically replaying shared-memory multiprocessor execution ef-
[25] Z. Qin, Y . Tang, E. Novak, and Q. Li, “Mobiplay: A remote execution
based record-and-replay tool for mobile applications,” in Proceedings
of the 38th International Conference on Software Engineering . ACM,
2016, pp. 571–582.ﬁciently,” in Computer Architecture, 2008. ISCA’08. 35th International
Symposium on . IEEE, 2008, pp. 289–300.
[21] S. Narayanasamy, G. Pokam, and B. Calder, “Bugnet: Continuously
recording program execution for deterministic replay debugging,”
SIGARCH Comput. Archit. News , vol. 33, no. 2, pp. 284–295, May
2005. [Online]. Available: http://doi.acm.org/10.1145/1080695.1069994
[22] R. H. B. Netzer, “Optimal tracing and replay for debugging shared-
memory parallel programs,” in Proceedings of the 1993 ACM/ONR
Workshop on Parallel and Distributed Debugging , ser. P ADD ’93.
New Y ork, NY , USA: ACM, 1993, pp. 1–11. [Online]. Available:
http://doi.acm.org/10.1145/174266.174268
[23] S. Park, Y . Zhou, W . Xiong, Z. Yin, R. Kaushik, K. H. Lee,
and S. Lu, “Pres: Probabilistic replay with execution sketching
on multiprocessors,” in Proceedings of the ACM SIGOPS 22Nd
Symposium on Operating Systems Principles , ser. SOSP ’09. New
Y ork, NY , USA: ACM, 2009, pp. 177–192. [Online]. Available:
http://doi.acm.org/10.1145/1629575.1629593
[24] E. Pobee and W . K. Chan, “Aggreplay: Efﬁcient record and replay of
multi-threaded programs,” in Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering , ser. ESEC/FSE 2019.
New Y ork, NY , USA: ACM, 2019, pp. 567–577. [Online]. Available:
http://doi.acm.org/10.1145/3338906.3338959
[26] S. Ren, L. Tan, C. Li, Z. Xiao, and W . Song, “Leveraging hardware-
assisted virtualization for deterministic replay on commodity multi-core
processors,” IEEE Transactions on Computers , vol. 67, no. 1, pp. 45–58,
Jan 2018.
[27] R. Utterback, K. Agrawal, I.-T. A. Lee, and M. Kulkarni, “Processor-
oblivious record and replay,” in Proceedings of the 22Nd ACM SIGPLAN
Symposium on Principles and Practice of Parallel Programming , ser.
PPoPP ’17. New Y ork, NY , USA: ACM, 2017, pp. 145–161. [Online].
Available: http://doi.acm.org/10.1145/3018743.3018764
[28] S. Wu, “Efﬁcient and sound dynamic atomicity violation
checking,” Ph.D. dissertation, City University of Hong Kong,
2016. [Online]. Available: https://scholars.cityu.edu.hk/en/theses/
theses(b64a7371-a086-434c-9ff4-a042a3c8effa).html
[29] M. Xu, R. Bodik, and M. D. Hill, “A" ﬂight data recorder" for
enabling full-system multiprocessor deterministic replay,” in Computer
Architecture, 2003. Proceedings. 30th Annual International Symposium
on. IEEE, 2003, pp. 122–133.
[30] M. Xu, M. D. Hill, and R. Bodik, “A regulated transitive reduction
(rtr) for longer memory race recording,” SIGARCH Comput. Archit.
News , vol. 34, no. 5, pp. 49–60, Oct. 2006. [Online]. Available:
http://doi.acm.org/10.1145/1168919.1168865
[31] J. Zhou, X. Xiao, and C. Zhang, “Stride: Search-based deterministic
replay in polynomial time via bounded linkage,” in Proceedings of
the 34th International Conference on Software Engineering , ser. ICSE
’12. Piscataway, NJ, USA: IEEE Press, 2012, pp. 892–902. [Online].
Available: http://dl.acm.org/citation.cfm?id=2337223.2337328
771
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 