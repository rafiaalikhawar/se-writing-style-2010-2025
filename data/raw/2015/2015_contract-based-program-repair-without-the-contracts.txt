Contract-Based Program Repair
without the Contracts
Liushan Chen∗·Y u Pei∗·Carlo A. Furia†
∗Department of Computing, The Hong Kong Polytechnic University, China {cslschen, csypei}@comp.polyu.edu.hk
†Department of Computer Science and Engineering, Chalmers University of Technology, Sweden bugcounting.net
Abstract —Automated program repair (APR) is a promising
approach to automatically ﬁxing software bugs. Most APR
techniques use tests to drive the repair process; this makes themreadily applicable to realistic code bases, but also brings therisk of generating spurious repairs that overﬁt the availabletests. Some techniques addressed the overﬁtting problem bytargeting code using contracts (such as pre- and postconditions),which provide additional information helpful to characterizethe states of correct and faulty computations; unfortunately,mainstream programming languages do not normally includecontract annotations, which severely limits the applicability ofsuch contract-based techniques.
This paper presents
JAID, a novel APR technique for
Java programs, which is capable of constructing detailedstate abstractions—similar to those employed by contract-basedtechniques—that are derived from regular Java code without anyspecial annotations. Grounding the repair generation and valida-tion processes on rich state abstractions mitigates the overﬁttingproblem, and helps extend APR’s applicability: in experimentswith the
DEFECTS 4Jbenchmark, a prototype implementation of
JAID produced genuinely correct repairs, equivalent to those
written by programmers, for 25 bugs—improving over the stateof the art of comparable Java APR techniques in the numberand kinds of correct ﬁxes.
I. I NTRODUCTION
Every general software analysis technique based on a ﬁnite
collection of tests is prone to overﬁtting them. Automated
program repair (APR) is no exception; in particular, overﬁtting
is likely to cripple the performance of APR tools followingthe generate-then-validate paradigm that was pioneered by
GenProg [33], where each heuristically generated candidate
repair—a source code patch—undergoes testing, and only thecandidates that pass all available tests for the method beingrepaired are classiﬁed as valid and returned as ﬁx suggestions.
Since validation is against a ﬁnite—often small—number oftests, there is no guarantee that a valid repair is genuinelycorrect against a complete, and implicit, speciﬁcation of the
method. Indeed, experiments have repeatedly conﬁrmed [19],[28], [29] that automated program repair techniques are proneto producing a signiﬁcant fraction of valid but incorrectrepairs, which merely happen to pass all available tests butare clearly inadequate from a programmer’s perspective.
The AutoFix technique for APR [31] mitigated the overﬁt-
ting problem by using contracts, made of assertions such as
pre- and postconditions, as additional information to improvethe precision of repair generation and validation. Even if thecontracts used by AutoFix are far from being detailed, letalone complete, method speciﬁcations, they signiﬁcantly helpincrease the fraction of correct ﬁxes that can be generated [25].Unfortunately, even such simple contracts are hardly everavailable in the most widely used programming languages.
1
Can we still generalize some of the techniques used forcontract-based program repair to work effectively without user-
written contracts?
In this paper we describe
JAID : a technique and tool
for automated program repair of Java programs that is
based on detailed, state-based dynamic program analyses—
akin those employed by contract-based techniques such asAutoFix, but working on regular Java code (without anycontracts). State abstractions drive both the generation and thevalidation stages of
JAID , and help construct high-quality ﬁxes:
in experiments targeting bugs from the DEFECTS 4J curated
collection, JAID produced repairs passing all available tests
for 31 of the bugs, and correct repairs—equivalent to those
written by programmers—for 25 of the bugs. These resultsare close to, or outperform, other comparable tools for theautomated program repair of Java programs in terms of totalnumber of correct repairs and precision, and include theﬁrst automatically produced correct repairs for 14 bugs of
DEFECTS 4J that were previously outside the capabilities of
APR. JAID is also the ﬁrst APR technique that achieves high
levels of precision without relying on additional input other
than tests and faulty code; in contrast, other recent high-precision APR techniques [13], [35] analyze a large numberof project repositories to collect additional information thatguides ﬁxing.
This paper’s key contributions, which bolster
JAID ’s perfor-
mance, include techniques to build a rich abstraction of object
state. In turn, the state abstraction relies on a purity analysis
of functions—only functions that are pure, that is withoutside effects, can be safely used to characterize state. Whereastechniques, such as AutoFix, that use programmer-writtencontracts can easily rely on the functions used as predicatesin the contracts,
JAID has to extract similar information from
regular code without annotations. To curb the number ofcandidate ﬁxes that are generated and validated,
JAID relies on
fault localization and ranking heuristics , which help identify
program states that are likely to be implicated with faultybehavior; both fault localization and ranking are cruciallyinformed by
JAID ’s detailed state-based abstractions. Thanks
1AutoFix targets the Eiffel programming language, where contracts are
embedded in the program code and routinely written by programmers.
978-1-5386-2684-9/17/$31.00 c2017 IEEEASE 2017, Urbana-Champaign, IL, USA
Technical Research637
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. to these techniques, JAID can generate correct ﬁxes that are
based on a more “semantic” analysis of how to modify the
object state to avoid a failure—beyond just working aroundthe existing implementation by syntactically modifying it, asmost other APR tools do.
Terminology. In this paper we use the nouns “defect”,
“bug”, “fault”, and “error” as synonyms to indicate errors ina program’s source code; and the nouns “ﬁx”, “patch”, and“repair” as synonyms to indicate source-code modiﬁcationsthat ought to correct errors. For simplicity,
JAID denotes both
the APR technique and the tool implementing it.
A vailability. JAID and all the material of the experiments
described in this paper is available as open source at:
https://bitbucket.org/maxpei/jaid
II. A NEXAMPLE OF JAID INACTION
Apache Commons is a widely used Java library that extends
Java’s standard API with a rich collection of utilities. Class
WordUtil of package org.apache.commons.lang includes a
method abbreviate to simplify strings with spaces: given
a string str , lower and upper indexes lower and upper ,
and another string appendToEnd , the method returns a string
obtained by truncating str at the ﬁrst index between lower
and upper where a space occurs, and replacing (or abbre-
viating) the truncated sufﬁx with appendToEnd . For exam-
ple,abbreviate("Apache Commons library", 9, 18, "+")
returns the string "Apache Commons+" .
Lst. 1 shows the implementation of abbreviate at commit
#cfff06bead of the library, which is also part of DEFECTS 4J’s
curated collection of defects (bug Lang-45). The implemen-tation begins by handling a number of special cases but,unfortunately, it misses the case when
lower is greater than
str ’s length: the index of the ﬁrst occurrence of a space from
lower will then be -1(corresponding to a failing search for
such a space in the call at line 16), and upper will be greater
than or equal to lower (possibly after being adjusted at lines
12–13), and thus also greater than str.length() ; in these
conditions, the call to method substring at line 19 throws an
IndexOutOfBoundsException .
The maintainers of Apache Commons ﬁxed this fault in a
later version of the library by resetting lower tostr.length()
to ensure that the case lower > str.length() never occurs,
as shown in the patch of Lst. 2, to be inserted right beforeline 9 in Lst. 1.
DEFECTS 4Jincludes a test that triggers this
fault in abbreviate , to avoid reintroducing the same mistake
in future revisions of the code.
After running for about 70 minutes, JAID produces a number
of ﬁx suggestions for the fault of abbreviate , including
the ﬁx in Lst. 3; this ﬁx is equivalent (nearly identical) tothe programmer-written ﬁx, and thus completely removes thesource of error by handling the special case correctly. Togenerate ﬁxes for
abbreviate ,JAID only needs the source
code of the faulty implementation, as well as the programmer-written tests that exercise the method.
DEFECTS 4J actually
includes only one test—the test triggering the fault—for this1public static String abbreviate
2 (String str, int lower, int upper, String appendToEnd) {
3 if(str == null){
4 return null;
5 }
6 if(str.length() == 0) {
7 return StringUtils.EMPTY;
8 }
9 if(upper == -1 || upper > str.length()) {
10 upper = str.length();
11 }
12 if(upper < lower) {
13 upper = lower;
14 }
15 StringBuffer result = new StringBuffer();
16 int index = StringUtils.indexOf(str, " ", lower);
17 if(index == -1) {
18 // throws IndexOutOfBoundsException if lower > str.length()
19 result.append(str.substring(0, upper));
20 if(upper != str.length()) {
21 result.append(StringUtils.defaultString(appendToEnd));
22 }
23 }else if (index > upper) {
24 result.append(str.substring(0, upper));
25 result.append(StringUtils.defaultString(appendToEnd));
26 }else {
27 result.append(str.substring(0, index));
28 result.append(StringUtils.defaultString(appendToEnd));
29 }
30 return result.toString();
31 }
Listing 1. Faulty method abbreviate from class StringUtils in
package org.apache.commons.lang .
8a10,12
>if(lower > str.length()) {
> lower = str.length();>}
Listing 2. Programmer-written ﬁx
to the fault in abbreviate .8a10,12
>if(lower >= str.length()) {
> lower = str.length();>}
Listing 3. JAID’s correct ﬁx
to the fault in abbreviate .
bug; JAID can produce a correct ﬁx even with such limited
information.
To our knowledge, JAID is the ﬁrst APR tool that can
correctly repair the fault of abbreviate ; no other existing
tools even provided so-called test-suite adequate ﬁxes, which
spuriously pass all available tests avoiding the failure, butdo not correctly ﬁx the behavior in the same way thatthe developers did. Key to
JAID ’s success is its capability
of constructing rich state-based abstractions of a program’sbehavior, which improves the accuracy of fault localizationand guides the creation of state-modifying ﬁxes in responseto failing conditions.
III. H
OW JAID WORKS
JAID follows the popular “generate-then-validate” approach,
which ﬁrst generates a number of candidate ﬁxes, and thenvalidates them using the available test cases; Fig. 1 givesan overview of the overall process. Inputs to
JAID are a
Java program, consisting of a collection of classes, and testcases that exercise the program and expose some failures.One key feature of
JAID is how it abstracts and monitors
program state in terms of program expressions; all stages of
JAID ’s workﬂow rely on the abstraction derived as described
in Sec. III-A. Fault localization (Sec. III-B) identiﬁes statesand locations (snapshots) that are suspect of being implicated
638
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. in the failure under repair. Fix generation (Sec. III-C and
Sec. III-D) builds code snippets that avoid reaching suchsuspicious states and locations by modifying the program state,the control ﬂow, or by other simple heuristics. Generated ﬁxesare validated against the available tests (Sec. III-E); the ﬁxesthat pass validation are presented to the user, heuristicallyranked according to how likely they are correct (Sec. III-F).
The rest of this section describes how
JAID repairs a generic
method fixMe of class FC, with tests Tthat exercise fixMe in
a way that at least one test in Tis failing.
Java program Test cases
Fault localization
Fix generation
Fix validation
Fix ranking
Ranked valid ﬁxesSuspicious snapshots
Candidate ﬁxes
Valid ﬁxes
JAID
Fig. 1. An overview of how JAID works. Given a Java program and a set
of test cases, including at least one failing test, JAID identiﬁes a number of
suspicious snapshots, each indicating a location and an abstraction of the
program state at that location that may be implicated in the failure; based onthe snapshot information,
JAID generates a number of candidate ﬁxes, which
undergo validation against all available tests for the method under repair; ﬁxesthat pass all available tests are considered valid;
JAID ﬁnally heuristically ranks
the valid ﬁxes, and presents the valid ﬁxes to the user in ranking order.
A. Program State Abstraction
JAID bases its program analysis and ﬁx generation processes
on a detailed state-based abstraction of the behavior of method
fixMe . For every location /lscriptinfixMe , uniquely identifying a
statement in the source code, the JAID records the values of a
setM/lscriptof expressions during each test execution: 1) the exact
value of expressions of numeric and Boolean types; 2) the
object identiﬁer (or null ) of expressions of reference types,
so that it can detect when a reference is aliased, or is null .
JAID selects the expressions in M/lscriptas follows.
Expressions. A type is monitorable if it is a reference
type or a primitive type (numeric types such as int , and
boolean ).E/lscriptdenotes the set of all basic expressions of
monitorable types at /lscript, namely 1) local variables (including
fixMe ’s arguments) declared inside fixMe that are visible at
/lscript; 2) attributes of class FCthat are visible at /lscript; 3) expressions
anywhere inside fixMe that can be evaluated at /lscript(that is,
they only involve items visible at /lscript), and that don’t obviouslyhave side effects (namely, we exclude assignments used asexpressions, self increment and decrement expressions, andcreation expressions using
new ).X/lscriptdenotes the set of all
extended expressions of monitorable types at /lscript: for each basic
expression of reference type r∈E/lscript,X/lscriptincludes: 1) r.f()
for every argumentless function fof the class corresponding
tor’s type that returns a monitorable type and is callable at
/lscript; 2) only if risthis ,r.a, for every attribute aof the class
corresponding to r’s type that is readable at /lscript.
For example, the extended expressions X9at line 9 in
method abbreviate of Lst. 1 include lower (an argument
ofabbreviate ),str.length() (a call of function length()
onabbreviate ’s argument str ), and upper < lower and
str == null (both appearing in abbreviate ).
Purity analysis. One lesson that we can draw from the
experience of contract-based APR [25] is that constructing arich set of expressions that abstract the program state can helpsupport more accurate fault localization and ﬁx generation, andultimately the construction of higher-quality “semantic” ﬁxesthat are less prone to overﬁtting. However, monitoring a richset of expressions extracted from the program text does notwork as well in languages such as Java as it does in languagesthat support contracts. In the latter, programmers speciﬁcallyequip classes with public query methods that are pure—they
are functions that return a value without changing the state oftheir target objects—and can be used in the contracts to char-acterize the program state in response to method calls; thesemethods are thus easily identiﬁable and natural candidatesto construct state abstractions reliably. In Java, in contrast,programmers need not follow such a discipline of separatingpure functions from state-changing procedures, and methodsthat return a value but have side effects are indeed common.Clearly, a function that is not pure is unsuitable for abstractingand monitoring an object’s state.
To identify which expressions can reliably be used for
state monitoring,
JAID performs a dynamic purity analysis
on all expressions that include method invocations. Givenan expression rof reference type, the set W
rofr’swatch
expressions consists of: 1) all subexpressions Srofrthat
do not include method invocations; 2) for each subexpressions∈S
r,s.afor every attribute aof the class corresponding to
s’s type. Note that watch expressions are constructed so that
they are syntactically free from side effects.
An expression rof reference type is then considered pure if
evaluating it does not alter the value of its watch expressions.Precisely, at every location /lscriptin the method
fixMe under
repair, 1) ﬁrst, JAID records the value σ=/angbracketleftσ1,...,σ m/angbracketrightof
all watch expressions, where σkis the value of wk∈Wr, for
1≤k≤m, before evaluating r; 2) then, JAID evaluates r;
3) ﬁnally, JAID records again the value σ/prime=/angbracketleftσ/prime
1,...,σ/prime
m/angbracketrightof
all watch expressions, where σ/prime
kis the value of wk∈Wr, for
1≤k≤m, after evaluating r.I fσ=σ/primeat every/lscriptin every
test exercising fixMe , we call rpure.
State monitoring. JAID collects in M/lscriptall extended expres-
sions inX/lscriptthat are pure according to this analysis.
639
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. B. Fault Localization
The goal of fault localization is to identify suspicious
snapshots indicating locations and states that are likely to be
implicated with a fault. A snapshot is a triple /angbracketleft/lscript,b, ?/angbracketright, where
/lscriptis a location in method fixMe under repair, bis a Boolean
expression, and ?is the value ( true orfalse )o fb at/lscript.
Boolean abstractions. The setB/lscriptincludes all Boolean
expressions that may appear in a snapshot at /lscript; it is constructed
by combining the monitored expressions M/lscriptto create Boolean
expressions as follows: 1) for each pair m1,m2∈M/lscriptof
expressions of the same type, B/lscriptincludes m1==m2and
m1!=m2; 2) for each pair k1,k2∈M/lscriptof expressions of
integer type, B/lscriptincludes k1⊿/triangleleft k 2, for⊿/triangleleft∈{<,<=,>=,>};
3) for each expression b∈M/lscriptof Boolean type, B/lscriptincludesb
and!b; 4) for each pair b1,b2∈M/lscriptof expressions of Boolean
type,B/lscriptincludesb1&&b2andb1||b2.
Continuing the example of method abbreviate in Lst. 1,
B9includes expressions such as lower >= str.length() and
!(str == null) .
Snapshot suspiciousness. JAID computes the suspiciousness
of every snapshot s=/angbracketleft/lscript,b, ?/angbracketrightbased on Wong at al.’s
fault localization techniques [34]. The basic idea is that the
suspiciousness of scombines two sources of information:
1) a syntactic analysis of expression dependence, which givesa higher value ed
stosthe more subexpressions bshares
with those used in the statements immediately before andimmediately after /lscript(this estimates how much sis relevant
to capture the state change at /lscript); 2) a dynamic analysis, which
gives a higher value dy
stosthe more often bevaluates to ?
at/lscriptin a failing test, and a lower value to sthe more often b
evaluates to ?at/lscriptin a passing test (this collects the evidence
that comes from monitoring the program during passing andfailing tests). The overall suspiciousness 2/(ed
−1
s+dy−1
s)is
the harmonic mean of these two sources, but the dynamicanalysis has the biggest impact—because ed
sis set up to be a
value between zero and one, whereas dysis at least one and
grows with the number of passing tests.
This approach is similar to AutoFix’s [25, Sec. 4.2]—which
is also based on [34]—but conspicuously excludes informationabout the distance between /lscriptand the location of failure on the
control ﬂow graph of the faulty method. AutoFix identiﬁesfailures as contract violations, which tend to be happen closerto where the program state becomes corrupted; by contrast, in
JAID ’s setting—using tests without contracts in Java—failures
normally happen when evaluating an assert statement insidea test method, and thus the distance to the location of failurewithin the faulty method is immaterial, and hardly a reliableindication of suspiciousness.
In the running example of method
abbreviate in Lst. 1,
the snapshot /angbracketleft9,lower >= str.length() ,true/angbracketrightreceives a
high suspiciousness score because low and str.length()
appear prominently in the statements around line 9, and, mostimportant,
lower >= str.length() holds in all failing and in
no passing tests.C. Fix Generation: Fix Actions
A snapshot s=/angbracketleft/lscript,b, ?/angbracketrightwith high suspiciousness indicates
that the program is prone to triggering a failure when theprogram state in some execution is such that bevaluates to
?
at/lscript; correspondingly, JAID builds a number of candidate ﬁxes
that try to steer away from the suspicious state in the hope
of avoiding the failure. To this effect, JAID enumerates four
kinds of ﬁx actions: 1) modify the state directly by assignment;
2) affect the state that is used in an expression; 3) mutate astatement; 4) redirect the control ﬂow. Each ﬁx action is a (pos-sibly compound) statement that can replace the statement at/lscript. Actions of kinds 1 and 2 are semantic—they directly target
the program state; actions of kind 3 are syntactic—they tinkerwith existing code expressions according to simple heuristics;actions of kind 4 are the simplest—they are independent ofthe snapshot’s information. We outline how
JAID builds ﬁx
actions in the following paragraphs, based on a deﬁnition ofderived expressions. Sec. IV discusses which ﬁx actions werethe most effective in the experimental evaluation.
Derived expression. Given an expression e,Δ
/lscript,e denotes
all derived expressions built from eas follows: 1) if ehas
integer type, Δ/lscript,e includes e,e+1 , ande-1 ;2 )i fe has
Boolean type, Δ/lscript,eincludeseand!e;3 )Δ/lscript,ealso includes t
andt.f(···), for every t∈M/lscriptof reference type, where
fis a function of the class tbelongs to—possibly called
with actual arguments chosen from the monitored expressionsM
/lscriptof suitable type. Given an expression e, its top-level
subexpressions Seare the expressions corresponding to the
nodes at depth 1 in e’s abstract syntax tree—namely, the
root’s immediate children. For example, the top-level subex-pressions of
(a + b) < c.d() area+b and c.d() . Then,
Δ/prime
/lscript,e=/uniontext
s∈S eΔ/lscript,e denotes all expressions derived from e’s
top-level subexpressions.
Modifying the state. For every top-level subexpression e
ofb,i fe is assignable to, JAID generates the ﬁx action e=δ
for eachδ∈Δ/prime/lscript,bwhose type is compatible with e’s.
In the running example of method abbreviate in Lst. 1,
JAID includes the assignment lower = str.length() among
the ﬁx actions that modify the state at line 9.
Modifying an expression. For every top-level subexpres-
sioneofbthat is not assignable to, but appears in the statement
Sat/lscript,JAID generates the ﬁx action tmp_e=δ;S[e/mapsto→tmp_e]
for each δ∈Δ/prime/lscript,bwhose type is compatible with e’s;tpm_e
is a fresh variable with the same type as e, andS[e/mapsto→tmp_e]
is the statement at /lscriptwith every occurrence of ereplaced by
tmp_e—which has just been assigned a modiﬁed value.
Mutating a statement. “Semantic” ﬁx actions—based on
the information captured by the state in suspicious snapshots—
are usefully complemented by a few “syntactic” ﬁx actions—based on simple mutation operators that capture commonsources of programming mistakes such as off-by-one errors.Following an approach adopted by other APR techniques [13],[36],
JAID generates mutations mainly targeting conditional
expressions. Precisely, if the statement Sat/lscriptis a conditional
or a loop, JAID generates ﬁx actions for every Boolean subex-
640
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. pression eofbthat appears in the conditional’s condition
or in the loop’s exit condition: 1) if eis a comparison
x1⊿/triangleleft x 2, for⊿/triangleleft∈{ <,<=,>=,>}, JAID generates the ﬁx
actionS[e/mapsto→(x1⊿/triangleleft/primex2)], for every comparison operator
⊿/triangleleft/prime/negationslash=⊿/triangleleft;2 ) JAID also generates the ﬁx actions S[e/mapsto→true]
andS[e/mapsto→false], whereeis replaced by a Boolean constant.
In addition to targeting Boolean expressions, if the statement
Sat/lscriptincludes a method call t.m(a 1,..., an),JAID generates
the ﬁx action S[m/mapsto→x], which calls any applicable method x
on the same target and with the same actual arguments as m
ins.
Modifying the control ﬂow. Even though ﬁx actions may
indirectly change the control ﬂow by modifying the state or abranching condition, a number of bugs require abruptly redi-recting the control ﬂow. To achieve this,
JAID also generates the
following ﬁx actions independent of the snapshot information:1) if method
fixMe is a procedure (its return type is void ),
JAID generates the ﬁx action return ; 2) if method fixMe is
a function, JAID generates the ﬁx action returne, for every
basic expression of suitable type available at /lscript;3 )i f/lscript is a
location inside a loop’s body, JAID generates the ﬁx action
continue .
D. Fix Generation: Candidate Fixes
Each ﬁx action—built by JAID as described in the previous
section—is a statement that modiﬁes the program behaviorat location /lscriptin a way that avoids the state implicated by
some suspicious snapshot s=/angbracketleft/lscript,b,
?/angbracketright. In most cases, a ﬁx
action should not be injected into the program under repairunconditionally, but only when state bis actually reached
during a computation. A conditional execution would leaveprogram behavior unchanged in most cases, and only addressthe failing behavior when it is about to happen.
To implement such conditional change of behavior,
JAID
uses the schemas in Fig. 2 to insert ﬁx actions into the method
fixMe under repair at location /lscript. First, JAID instantiates every
applicable schema with each ﬁx action action ; in addition to
the ﬁx action, schemas include the statement oldStatement at
location/lscriptin the faulty fixMe , and the condition suspicious ,
which isb==?as determined by the snapshot’s abstract state.
Then, JAID builds ﬁx candidates byreplacing the statement at
/lscriptinfixMe by each instantiated schema.2
Continuing the running example of method abbreviate in
Lst. 1, one of the ﬁx candidates consists of the ﬁx action
lower = str.length() instantiating schema B: the action is
executed only if lower >= str.length() (from the suspicious
snapshot), whereas the existing statement at line 9, as well asthe rest of method
fixMe , is unchanged by the ﬁx.
Two of the ﬁve schemas currently used by JAID to build
ﬁx candidates inject the ﬁx action unconditionally. On theother hand, different ﬁx actions may determine semanticallyequivalent ﬁxes when instantiated.
JAID performs a lightweight
redundancy elimination, based on simple syntactic rules such
2Since each ﬁx generated by JAID combines one ﬁx action and one schema,
it adds at most 5 new lines of codes to a patched method.action;
oldStatement;
Listing 4. Schema Aif(suspicious) {
action;
}oldStatement;
Listing 5. Schema Bif(!suspicious) {
oldStatement;
}
Listing 6. Schema C
if(suspicious) {
action;
}else {
oldStatement;
}
Listing 7. Schema D// oldStatementaction;
Listing 8. Schema E
Fig. 2. Schemas used by JAID to build candidate ﬁxes
// class being repaired
class FC {
U fixMe _(T1a1,T2a2,...)throws IllegalStateException {
switch (Session.getActiveFixId()) {
case 0:return fixMe(a 1,a2,...); // call faulty method
case 1:return fixMe _1(a1,a2,...); // call fix candidate 1
...
case n:return fixMe _n(a
1,a2,...); // call fix candidate n
default: throw new IllegalStateException();
}
}
}
Listing 9. How multiple ﬁx candidates are woven into a single class.
as that x= =y is equivalent to !(x != y) . In future work, we
plan to introduce a more aggressive redundancy elimination,
for example as done in related work [32].
E. Fix V alidation
Even if JAID builds candidate ﬁxes based on a semantic
analysis of the program state during passing vs. failing tests,
the candidate ﬁxes come with no guarantee of satisfyingthe tests. To ascertain which candidates are suitable, a ﬁxvalidation process, which follows ﬁx generation, runs all testsTthat exercise the faulty method
fixMe against each generated
candidate ﬁx. Candidate ﬁxes that pass all tests Tare classiﬁed
asvalid (also “test-suite adequate” [19]) and retained; other
candidates, which fail some tests, are discarded—as they donot ﬁx the fault, they introduce a regression, or both.
In the example of method
abbreviate in Lst. 1, the ﬁx can-
didates if(lower >= str.length) lower = str.length()
passes validation, since it ﬁxes the fault and introduces noregression error.
Since
JAID commonly generates a large number of candidate
ﬁxes for each fault, validation can take up a very large timespent compiling and executing tests, which may ultimatelyimpair the scalability of
JAID ’s APR. To curtail the time spent
compiling, JAID deploys a simple form of dependency injec-
tion. All candidate ﬁxes for a method fixMe become members
offixMe ’s enclosing class FC: candidate ﬁx number kbecomes
a method fixMe _kwith signature the same as fixMe ’s. Then,
as shown in Lst. 9, a method fixMe _—also with the same
signature—dispatches calls to any of the candidate ﬁxes basedon the value returned by static method
getActiveFixId() of
641
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. class Session , which supplies the dependency. This scheme
only requires one compilation per method under repair, thus
signiﬁcantly cutting down validation time.
F . Fix Ranking
Like most APR techniques, JAID ’s process is based on
heuristics and driven by a ﬁnite collection of tests, and thus is
ultimately best effort : a valid ﬁx may still be incorrect, passing
all available tests only because the tests are incomplete piecesof speciﬁcation.
JAID addresses this problem by ranking valid
ﬁxes using the same heuristics that underlies fault localization.Every ﬁx includes one ﬁx action, which was derived from asnapshot s; the higher the suspiciousness of s, the higher the
ﬁx is ranked; ﬁxes derived from the same snapshot are rankedin order of generation, which means that “semantic” ﬁxes(modifying state or expressions) appear before “syntactic”ﬁxes (mutating statements or modifying the control ﬂow),and ﬁxes of the same kind are enumerated starting from thesyntactically simpler ones.
When the ranking heuristics works, the user only inspects
few top-ranked ﬁxes to assess their correctness and whetherthey can be deployed into the codebase. The experimentalevaluation in Sec. IV comments on the effectiveness of
JAID ’s
ranking heuristics.
IV . E XPERIMENT AL EV ALUA TION
We evaluate the effectiveness of JAID on DEFECTS 4J[9], a
large curated collection of faults and programmer-written ﬁxesfrom real-world Java projects. This choice also enables us toquantitatively compare the results of
JAID ’s evaluation to most
state-of-the-art tools for APR of Java programs—which havealso targeted
DEFECTS 4Jin their experiments.
T ABLE I
The bugs in DEFECTS 4J: for each PROJECT inDEFECTS 4J, how many thousands
of lines of code ( KLOC ) and tests ( TESTS ) it includes, how many BUGS it
includes in total, and how many were SELECTED for our experiments.
PROJECT KLOC TESTS BUGS SELECTED
Chart JFreechart 96 2205 26 12
Closure Closure Compiler 90 7927 133 56
Lang Apache Commons-Lang 22 2245 65 22Math Apache Commons-Math 85 3602 106 42Time Joda-Time 27 4130 27 6
TOTAL 320 20109 357 138
A. Subjects
Our experiments target revision #a910322b of DEFECTS 4J,
which includes 357 bugs in 5 projects: Chart (26 bugs),
Closure (133 bugs), Lang (65 bugs), Math (106 bugs), andTime (27 bugs). Each bug in
DEFECTS 4Jhas a unique iden-
tiﬁer, corresponds to two versions—buggy and ﬁxed (by aprogrammer)—of the code (which may span multiple meth-ods or even multiple ﬁles), and is accompanied by someprogrammer-written unit tests that exercise the code—in par-ticular, at least one test triggers a failure on the buggy version.In the following, if kis the identiﬁer of a bug in
DEFECTS 4J,
βkdenotes the buggy version of the code corresponding to
bugk,φkdenotes the code of the programmer-ﬁxed version
ofβk, andTkdenotes the tests accompanying k.
The bugs included in DEFECTS 4Jare a representative sample
of real-world bugs, and as such they include several that admitsimple ﬁxes, as well as several others that require sweepingchanges to different parts of a project. In order to focusthe experiments on the bugs that have a chance of being in
JAID ’s purview, we selected a subset of all bugs kthat satisfy
the following criteria: 1) the programmer-written ﬁx φkonly
modiﬁes Java executable code—no other artifacts like conﬁg-uration ﬁles or compilation scripts; 2) the programmer-writtenﬁxφ
kmodiﬁes no more than 5 consecutive lines of code and
no more than 4 statements with respect to βk(as reported by
the ChangeDistiller tool [8]); 3) the bug is reproducible: atleast one test in T
kfails onβk, and all tests in Tkpass on
φk. A total of 138 bugs satisfy these criteria; these are the
subjects of our experiments with JAID . Tab. I shows the size
of and the number of bugs in each DEFECTS 4Jproject among
our experimental subjects.
B. Research Questions
Our evaluation addresses research questions in different
areas:
Effectiveness: How many bugs can JAID ﬁx?
Performance: How much time does JAID take?
Design: Which components of JAID ’s are the most important
for effectiveness?
Comparison: How does JAID compare to other APR tech-
niques for Java?
C. Setup
Since JAID ranks all generated snapshots according to their
suspiciousness (see Sec. III-B), and depends on the ranking to
guide the following stages, setting an arbitrary cutoff time mayprevent from generating a complete ranking. Instead, we limitthe search space in our experiments by conﬁguring
JAID so
that it uses at most 1500 snapshots in order of suspiciousness;3
then, the following stages (Fig. 1) all run to completion.
Each experiment targets one bug kinDEFECTS 4J, and runs
JAID on buggy code βkusing the tests Tk; the output is a
ranked list of valid ﬁxes for the bug. We manually inspect thetop 50 ﬁxes in order of ranking to determine which are correct ;
if all 50 ﬁxes are incorrect, we continue the manual inspectionof the other ﬁxes and stop when we ﬁnd a correct one, or nomore valid ﬁxes are available. We classify a ﬁx as correct onlyif it is semantically equivalent to the programmer-written ﬁx
φ
kinDEFECTS 4J. This is a high bar for correctness, which
provides strong conﬁdence that a ﬁx is high-quality enough tobe deployable.
All the experiments ran on a cloud infrastructure, with each
run of
JAID using exclusively one virtual machine instance,
conﬁgured to use one core of an Intel Xeon Processor E5-2630 v2, 8 GB of RAM, Ubuntu 14.04, and Oracle’s Java
3The number 1500 was chosen heuristically.
642
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. JDK 1.8. In this section, averages are measured using the
median by default, with exceptions explicitly pointed out.
Other tools for Java APR. We quantitatively compare JAID
to all other available tools for APR of Java programs thathave also used
DEFECTS 4Jin their evaluations:41) jGenProg
is the implementation of GenProg [14], [33]—which workson C—for Java programs; we refer to jGenProg’s evaluationin [19]; 2) jKali is the implementation of Kali [28]—whichworks on C—for Java programs; we refer to jKali’s evalu-ation in [19]; 3) Nopol focuses on ﬁxing Java conditionalexpression; we refer to Nopol’s evaluation in [19]; 4) xP ARis a reimplementation of P AR [12]—which is not publiclyavailable—discussed in [13] and [35]; 5)
HDA implements
the “history-driven” technique of [13]; 6) ACS implements the
“precise condition synthesis” of [35].
Experiments with APR tools often target a different subset
ofDEFECTS 4Jthat is amenable to the technique being evalu-
ated. Comparing the number and kinds of ﬁxed bugs amongtools remains meaningful, because bugs that are excluded apriori from an evaluation can normally be considered beyonda tool’s current capabilities.
D. Results
Effectiveness.
JAID was able to produce valid ﬁxes for the
31 bugs of DEFECTS 4J listed in Tab. II. More signiﬁcant,
it produced correct ﬁxes—equivalent to those written by
programmers—for 25 of these bugs (integer RANK in Tab. II).
This indicates that JAID is applicable to realistic code and,
when it runs successfully, it often produces ﬁxes of high
quality. As we discuss in more detail below, the number ofcorrectly ﬁxed bugs is on par with, or above, the state of theart of Java APR techniques.
Unsurprisingly,
JAID produces ﬁxes that tend to be small
in size: 1.7 lines of code changes per valid ﬁx on average.This is a result of its ﬁx generation process, which is basedon state-based information, targets simple ﬁx actions, and thensharpens their precision by injecting them into the code usingconditional schemas.
When it is successful,
JAID often produces a signiﬁcant
number of valid ﬁxes—39 per ﬁxed bug on average in ourexperiments—and a much smaller number of correct ﬁxes—1per ﬁxed bug on average in our experiments—which all aresemantically equivalent. In
JAID ’s output for the 25 bugs that
were correctly ﬁxed, the median position of the ﬁrst correctﬁx in the list of all valid ﬁxes was 6 (from the top); for 15bugs that
JAID correctly ﬁxed, a correct ﬁx appears among the
top-10 valid ones in order of ranking; for 9 bugs it appearsat the top position. These results indicate the importance ofranking to ensure that the correct ﬁxes are easier to spot amongseveral valid but incorrect ones. For 10 bugs, the correct ﬁxappears further down in the output list; in 6 of these cases, thecorrect ﬁx turns out to be a “syntactic” one, but several valid,incorrect “semantic” ﬁxes are generated and ranked higher.
4We ascertained that the version of DEFECTS 4Jused in our experiments
does not differ substantially from those used in the other tools’ experiments;
in particular, all bugs analyzed by JAID were also available to the other tools.This suggests that improving the precision of ranking may
beneﬁt from mining additional information about commonfeatures of programmer-written ﬁxes, as done by
HDA and
ACS .
JAID can correctly ﬁx even 4 bugs that include only one
failing test (and no passing tests), ranking the correct ﬁx ﬁrst intwo cases. These results showcase how
JAID can be successful
at mitigating the baneful problem of overﬁtting.
Performance. As shown in Tab. II, JAID runs in 119.5
minutes per bug on average (median, whereas the meanrunning time is 355.1 minutes).
JAID is unsurprisingly sig-
niﬁcantly slower than tools based on constraint solving andother symbolic techniques; for example, Nopol takes around22 minutes per bug on average—on what, we assume, iscomparable hardware. They are, however, in line with otherAPR techniques mainly based on dynamic analysis, such asjGenProg which takes about one hour per bug.
Looking more closely into how much time each stage of
JAID takes, it is clear that validation is by far the most time
consuming: fault localization takes 2.7% of the median timeper bug, ﬁx generation takes 0.5%, and ﬁx validation takes92.8%. V alidation time tends to be proportional to the numberof available tests, and to the number of ﬁx candidates—which,in turn, is proportional to the number of snapshots that areactually analyzed. The approach outlined in Sec. III-E stillhelps save a signiﬁcant amount of compilation time; as futurework, we plan to further improve the performance of validationby running multiple concurrent instances on the same JVM.
Design. Which of the ﬁx actions and schemas are the
most useful to build correct ﬁxes? Regarding ﬁx actions,we distinguish three kinds: 1) sfor actions modifying the
state or an expression—“semantic” actions that are built on
JAID ’s rich state-based abstractions; 2) mfor actions mutating
a statement—“syntactic” actions that correct common errorsand are commonly used in APR systems; 3) cfor actions
modifying the control-ﬂow—“terminating” actions that canstill have a signiﬁcant impact on program behavior.
JAID uses
all three kinds of actions, with similar frequencies, in correctﬁxes, which indicates that they are largely complementary andall contribute to
JAID ’s effectiveness.
The ﬁve ﬁx schemas of Fig. 2, which JAID uses to inject a ﬁx
action into the method under repair, also all feature in correctﬁxes. This suggests that both conditional and unconditionalapplications are required to target a wide choice of bugs.
Comparison: correct ﬁxes. Tab. III compares
JAID to
JAID HDA
ACS14 13
158
21 0
Fig. 3. Number of bugs correctly ﬁxed by each of the main APR tool.
643
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. T ABLE II
Summary of the experimental results. For every bug ( BUG ID k) that JAID ﬁxed with a valid ﬁx: the lines of code ( LOC ) of the method under repair; how many
tests ( TESTS ),Passing and Failing, exercise the method under repair; the number of suspicious SNAP shots that are analyzed (capped at 1500); the number of
CAND idate ﬁxes that undergo validation; the mean SIZE , in lines of code changes, of a valid ﬁx; the total number of V ALID ﬁxes; the number of CORRECT
ﬁxes among the top 50 valid ﬁxes; the position of the ﬁrst correct ﬁx in ranking order ( RANK ); the TOTAL running time in minutes, and the breakdown into
time for fault Localization, ﬁx Generation, and ﬁx Validation; the kinds of ACTIONS (sfor state- and expression-modifying actions, mfor mutation actions,
andcfor control-ﬂow actions, see Sec. III-C), and the kinds of SCHEMAS (see Fig. 2) used in correct ﬁxes.
TESTS GENERA TION FIXES TIME
BUG ID k LOC PF SNAP CAND SIZE V ALID CORRECT RANK TOTAL L G V ACTIONS SCHEMAS
Chart 1 32 37 1 791 3762 1.5 536 0 84 54.1 1.0 1.5 51.6 m E
Chart 24 6 0 1 813 2476 1.5 2 2 1 16.8 0.3 0.2 16.3 s A,B
Chart 26 108 23 22 1500 2018 1.7 82 3 1 53.6 10.3 4.7 38.6 c,s B,E
Chart 9 38 1 1 1500 5991 1.8 52 2 43 72.2 2.6 0.8 68.8 s E
Closure 125 15 538 1 154 517 1.9 98 0 – 131.3 9.7 0.0 121.6 – –
Closure 126 95 71 2 1500 4583 1.9 425 0 93 601.4 6.0 0.5 594.9 m E
Closure 18 122 2096 1 1000 9215 1.0 9 1 1 1367.1 449.5 5.8 911.8 m E
Closure 31 122 2037 1 1500 14464 1.0 9 1 8 1440.1 448.4 6.9 984.7 m E
Closure 33 27 259 1 1500 4484 1.5 2720 2 1 258.0 6.5 0.1 251.4 c B
Closure 40 46 305 2 871 5243 1.0 4 4 1 119.5 8.1 0.6 110.9 m E
Closure 5 98 56 1 1500 25816 1.5 2 0 – 975.9 6.0 3.7 966.2 – –Closure 62 45 45 2 1500 7138 1.0 87 1 31 126.7 6.8 1.1 118.8 m E
Closure 63 45 45 2 1500 7138 1.0 87 1 31 127.1 6.7 1.1 119.3 m E
Closure 70 19 2337 5 393 2359 1.0 5 5 1 70.4 31.1 0.2 39.1 m E
Closure 73 70 482 1 1500 11472 1.0 1 1 1 473.4 165.1 2.5 305.8 m E
Lang 24 102 0 1 1500 51872 1.0 2 0 – 2228.6 1.2 7.2 2220.1 – –Lang 33 11 0 1 150 792 2.1 7 7 1 11.0 3.3 0.1 7.7 c,s C,D,B
Lang 38 6 33 1 328 1363 1.7 28 4 4 10.7 1.0 0.1 9.6 s A,B
Lang 39 126 1 1 1500 11702 2.1 39 0 – 408.2 10.7 3.7 393.9 – –Lang 45 37 0 1 1500 7173 1.9 68 2 34 105.1 0.7 0.6 103.9 s B
Lang 51 51 0 1 959 8514 1.7 424 1 46 188.4 0.3 0.8 187.3 c B
Lang 55 6 4 1 21 170 2.0 15 1 6 3.6 0.4 0.0 3.2 s C
Lang 61 27 7 2 1500 18289 1.0 4 0 – 327.0 0.5 1.7 324.7 – –
Math 105 2 8 1 64 1478 2.0 139 0 – 9.2 0.2 0.1 9.0 – –Math 32 52 6 1 1500 2997 1.0 5 1 4 37.5 1.7 0.4 35.4 s E
Math 5 22 5 1 162 1426 2.0 61 3 1 11.3 0.5 0.1 10.8 c B
Math 50 125 3 1 1500 37848 1.5 1101 3 28 1502.6 23.4 7.6 1471.5 s,c,m E,C,D
Math 53 5 19 1 246 2010 2.0 10 2 6 19.0 3.2 0.1 15.7 c B
Math 80 15 16 1 1500 9526 1.9 3877 0 1366 156.7 0.5 0.9 155.2 s B,A
Math 82 15 13 1 436 1707 1.9 13 1 9 33.1 3.0 0.1 30.0 m E
Math 85 43 12 1 1500 2922 1.7 709 0 247 68.3 1.2 0.2 66.9 m E
MEDIAN 38 16 1 1500 4583 1.7 39 1 6 119.5 3.2 0.6 110.9
T ABLE III
A comparison of APR techniques on bugs in DEFECTS 4J. This paper’s JAID is compared to ACS ,HDA , xP AR, jGenProg, and jKali. For each tool, the table
reports the number of bugs that were ﬁxed with a V ALID ﬁx; the number of bugs that were ﬁxed with a CORRECT ﬁx (among ANY of those outputed by the
tool, only among the TOP -10 POSITIONS in the output, and only in the FIRST POSITION in the output); and the resulting PRECISION (CORRECT /VA L I D ) and
RECALL (CORRECT /357, where 357 is the total number of bugs in DEFECTS 4J). Question marks represent data not available for a tool.
ANY POSITION TOP -10 POSITIONS FIRST POSITION
TOOL V ALID CORRECT PRECISION RECALL CORRECT PRECISION RECALL CORRECT PRECISION RECALL
JAID 31 25 80.6 7.0 15 48.4 4.2 9 29.0 2.5
ACS 23 18 78.3 5.0 18 78.3 5.0 18 78.3 5.0
HDA ? 23 ? 6.4 23 ? 6.4 13 ? 3.6
Nopol 35 5 14.3 1.4 5 14.3 1.4 5 14.3 1.4
xPAR ? 4 ? 1.1 4 ? 1.1 ? ? ?
jGenProg 27 5 18.5 1.4 5 18.5 1.4 5 18.5 1.4jKali 22 1 4.6 0.3 1 4.6 0.3 1 4.6 0.3
six other APR tools for Java. In terms of number of bugs
ﬁxed with a correct ﬁx, JAID outperforms all other tools.
Note that both runners-up, HDA and ACS , crucially rely
on mining additional information from other sources: HDA
mines frequency information about 3000 bug ﬁxes from 800popular GitHub projects, whereas
ACS searches for predicates
in “all open-source projects in GitHub” [35, Sec.III-E]. Theimplementation of
HDA additionally requires fault localization
information as part of the input. Thus, both tools use aricher input than just a buggy program and its accompanyingunit tests, which indicates that
JAID ’s performance is highly
competitive, and arguably improving the state-of-the-art in itsown league.
JAID fares very well also in terms of precision (fraction
of bugs with a valid ﬁx that have a correct ﬁx) and recall
(percentage of all bugs in DEFECTS 4J that have a correct
ﬁx). Since different approaches, and different experimentalevaluations, deal differently with bugs that admit multiple
644
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. valid ﬁxes, we measure three variants of precision and recall:
1) relative to the number of bugs that were correctly ﬁxedby any of the valid ﬁxes, regardless of the correct ﬁx’s rank;2) relative to the number of bugs that were correctly ﬁxed bya ﬁx ranked among the top 10 in a tool’s output; 3) relative tothe number of bugs that were correctly ﬁxed by a ﬁx rankedﬁrst in a tool’s output.
JAID achieve the best precision5and
recall if we disregard ranking; and the second-best precisionand third-best recall in the other two cases. Again, note thatthe only tools that outperform
JAID rely on additional input
information to sharpen their precision and recall.
Comparison: kinds of ﬁxes. Fig. 3 zooms in on the bugs
that are correctly ﬁxed by JAID ,HDA , and ACS , and shows
how many bugs each tool can ﬁx that the others cannot. Thetools are mainly complementary in the speciﬁc bugs they are
successful on:
JAID ﬁxes 14 bugs that no other tool can ﬁx;
HDA ﬁxes 13; and ACS ﬁxes 15.
Among the tools not in Fig. 3: Nopol ﬁxes 2 bugs that no
other tool can ﬁx (plus 2 bug also ﬁxed by JAID , and 1 of
which also ﬁxed by HDA ); jGenProg ﬁxes 1 bug that no other
tool can ﬁx (plus 4 bugs also ﬁxed by HDA , 3 of which JAID
can also ﬁx); jKali ﬁxes 1 bug that jGenProg, Nopol, HDA and
JAID can also ﬁx. These numbers indicate that each technique
is successful in its own domain. The complementarity alsosuggests that combining techniques based on mining (such as
HDA and ACS ) with JAID ’s techniques is likely to yield further
improvements in terms of precision and effectiveness.
Comparison: other tools. We refrain from quantita-
tively comparing APR tools that target other programminglanguages—and thus were evaluated on different bench-marks [15]. Nevertheless, just to give an idea, Angelix [22]and Prophet [17]
6achieve a precision of 35.7% and 42.9%,
and a recall of 9.5% and 17%, on 105 bugs in the C GenProgbenchmark [14]; AutoFix [25]
7achieves a precision of 59.3%
and a recall of 25% on 204 bugs from various Eiffel projectswith contracts.
E. Threats to V alidity
Construct validity indicates whether the measures used in
the experiments are suitable. We classify a ﬁx as correct if it is
semantically equivalent to a programmer-written ﬁx. Since we
assess semantic equivalence manually, different programmersmay provide different assessments; to mitigate this threat, wewere conservative in evaluating equivalence—if a ﬁx does notclearly produce the same behavior as the ﬁx in
DEFECTS 4J
for the same bug, we classify it as incorrect. This approachis consistent with what done by other researchers. A moredetailed analysis of patch correctness belongs to future work.
We measured, and compared, precision and recall relative
toall bugs in
DEFECTS 4J, even if most APR techniques—
including JAID —only run experiments on a subset of the
bugs whose features have a chance of being ﬁxable. Usingthe largest possible denominator ensures that measures are
5The precision of HDA is not reported in [13].
6V alid ﬁxes are called “plausible” in [17].
7Correct ﬁxes are called “proper” in [25], [26].comparable between different tools, and is consistent with theultimate ambition of developing APR techniques that are aswidely applicable as possible.
Tools, and their experimental evaluations, often differ in
how they deal with multiple valid ﬁxes for the same bugs. Inthe tool comparison, we counted all correct ﬁxes generatedby each tool that were reported in the experiments, and wereported separate measures of precision according to howmany valid ﬁxes are inspected. This gives a nuanced pictureof the results, which must however be taken—as usual—witha grain of salt: different tools may focus on achieving a betterranking vs. correctly ﬁxing more bugs, and we do not implythat there is one universal measure of effectiveness. Anyway,our evaluation is widely applicable—including to papers thatmay not detail this aspect—and is in line with what done inother evaluations [14], [17], [25], [28], [29].
Internal validity indicates whether the experimental results
soundly support the ﬁndings. Comparing the performance—running time, in particular—of different APR techniques is aparticularly delicate matter because of a number of confound-ing factors. First of all, the experiments should all run on thesame hardware and runtime environment, using comparableconﬁgurations (e.g., in terms of timeouts). Techniques usingrandomization, such as jGenProg, require several repeatedruns to get to quantitative results that are representative ofa typical run [1]. Some techniques, such as
ACS and HDA ,
rely on a time-consuming preprocessing stage that mines coderepositories (and is crucial for effectiveness), and hence it isunclear how to appropriately compare them to techniques, suchas
JAID , that do not depend on this auxiliary information. Fault
localization is also an input to HDA ’s main algorithm. In all,
we used standard, clearly speciﬁed settings for the experimentswith
JAID , and we relied on the overall results—in terms of
correct ﬁxes—reported in other tools’ experiments. In contrast,we refrained from qualitatively compare tools in measuresof performance, which depend more sensitively on having acontrolled experimental setup, and which we therefore leaveto future work.
External validity indicates whether the experimental ﬁnd-
ings generalize. The
DEFECTS 4Jdataset is a varied collection
of bugs, carefully designed and maintained to support realisticand sound comparisons of the effectiveness of all sorts ofanalyses based on testing and test-case generation; it has alsobecome a de facto standard to evaluate APR techniques forJava. These characteristics mitigate the risk that our exper-iments overﬁt the subjects. As future work, we plan to run
JAID on other open-source Java projects; we see no intrinsic
limitations that would prevent JAID from working reliably on
different projects as well.
V. R ELA TED WORK
Automated program repair has become a bustling research
area in the course of just a few years. The ﬁrst APR tech-niques [2], [33] used genetic algorithms to search the spaceof possible ﬁxes for a valid one. GenProg [33] pioneeredthe “generate-and-validate” approach, where many plausible
645
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. ﬁxes are generated based on heuristics, and then are validated
against the available tests. More recently, others [5], [21],[22], [24], [36] have pursued the “constraint-based” approach,where ﬁxes are constructed to satisfy suitable constraintsthat correspond to their validity. The two approaches arenot sharply distinct, in that ﬁxes generated by constraint-based techniques may still require validation if the constraintsthey satisfy by construction are not sufﬁciently precise toensure that they are correct—as it often happens when dealingwith incomplete speciﬁcations. Nevertheless, the categoriza-tion remains useful; we devote more attention to generate-and-validate techniques, since
JAID belongs to this category, and
thus is more directly comparable to them. For a broader list ofAPR techniques, see Monperrus’s annotated bibliography [23].
Generate-and-validate. GenProg [33] is based on a genetic
algorithm that mutates the code of a faulty C function by delet-ing, adding, or replacing code taken from other portions of thecodebase—following the intuition [20] that existing code isalso applicable to patch incorrect functionality. GenProg’s al-gorithm and implementation were substantially extended [14]to scale to code bases of realistic code sizes—producing validﬁxes for 52% of 105 bugs.
Encouraged by GenProg’s promising results, various ap-
proaches tried to make the mutation of candidate ﬁxes moreeffective, or the search in the space of possible ﬁxes moredirected and thus more efﬁcient. For example, MutRepair [4]only modiﬁes operators appearing in expressions (such ascomparison operators and Boolean connectives), since thesetend to be a common source of programming mistakes.P AR [12] bases the generation of ﬁxes on ten patterns, selectedbased on a manual analysis of programmer-written ﬁxes,which helps generate ﬁxes that are more readable, and possiblyeasier to understand. A complementary approach [30] suggeststo use anti-patterns, trying to capture ﬁxes that are likely to
be incorrect but still pass validation.
The overﬁtting problem. A more detailed analysis [28]
of the ﬁxes produced by GenProg and similar techniques hasshown that only a small fraction of them is genuinely correct;for example, less than 2% of the bugs of [14] are correctlyﬁxed. [28]’s analysis has pushed the research in APR toaddressing this manifestation of the overﬁtting problem [29].
Most techniques for APR are based on tests, which are
necessarily incomplete characterization of correct behavior.By also relying on contracts (speciﬁcations embedded in theprogram text) AutoFix [25], [26], [31] was the ﬁrst general-purpose APR technique to substantially increase the numberof correct ﬁxes—for 25% of 204 bugs in [25].
JAID generalizes
AutoFix’s state-based analysis to work on Java code withoutcontracts, so as to improve the quality of the generated ﬁxeswithout sacriﬁcing applicability.
Code mining. SearchRepair [11] is one of few other
approaches based on semantic analysis—as opposed to themore commonly used syntactic analysis. SearchRepair relieson preprocessing a large dataset of programmer-written codesnippets, and encoding their behavior as input/output relationalconstraints; it then generates ﬁxes by searching the datasetfor snippets that capture the desired input/output behavior.HDA [13] also leverages a model of programmer-written codebuilt by mining software repositories, but combines it witha mutation-based syntax-driven analysis similar to GenProg:mutants that are “more similar” to what the learned modelprescribes are preferred in the search for a repair. The idea ofmining programmer-written code is applicable to other APRapproaches, including
JAID , as a way to provide additional
information that reduces the chance of overﬁtting.
Condition synthesis. Constraint-based approaches often
target the synthesis of conditions in if statements or loops,
since changing those conditions often affects the control ﬂowin decisive ways. SemFix [24] is one of the early examples;it relies on symbolic execution to summarize tests and onlocation-based fault localization, and it synthesizes expressionsin conditionals and in assignments that try to avoid triggeringfailures. DirectFix [21] expresses the repair problem as aMaxSMT constraint, and supports generating multi-line ﬁxes.Both SemFix and DirectFix, however, have limited scalability.Angelix [22] addresses this problem by introducing an efﬁcientrepresentation of constraints, and by combining it with asymbolic execution analysis similar to SemFix’s.
Nopol [36] only targets conditional expressions, and uses
a form of angelic debugging [3], [37] to reconstruct theexpected value of a condition in passing vs. failing runs;based on it, it synthesizes a new conditional expression usingan SMT solver. SPR [16] also combines condition synthesiswith a dynamic analysis of the value each abstract conditionalexpression should take to make all tests pass, which helpsaggressively prune the search space when no plausible repairexists. Prophet [17] improves SPR with a probabilistic modellearned by mining programmer-written ﬁxes. MintHint [10]also builds a statistical model to generate repair suggestions
consisting of expressions that may be useful in a completeﬁx. ACS [35] is a recent technique that signiﬁcantly improvesthe precision of condition synthesis based on a combinationof data- and control-dependency analysis, and mining APIdocumentation and Boolean predicates in existing projects.
JAID also relies on data- and control-dependency analysis, and
can guess modiﬁcations to conditional expressions, but it doesnot need any additional source of information other than theproject being ﬁxed.
Runtime patching. Runtime patching [6], [7], [18] denotes
approaches that operate at runtime as fallback measures inresponse to triggered failures—in contrast to APR techniquesthat modify source code. Under the hood, runtime patchingoften uses program analysis techniques similar to those ofAPR systems; ClearView [27], for instance, dynamically inferstate invariants like
JAID does, but does so at runtime on
instrumented binaries with the goal of preventing problemssuch as buffer overﬂows.
A
CKNOWLEDGMENTS
This work was partially supported by Hong Kong RGC
General Research Fund (GRF) PolyU 152703/16E and theHong Kong Polytechnic University internal fund 1-ZVJ1.
646
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] A. Arcuri and L. C. Briand. A hitchhiker’s guide to statistical tests for
assessing randomized algorithms in software engineering. Softw. Test.,
V erif. Reliab., 24(3):219–250, 2014.
[2] A. Arcuri and X. Y ao. A novel co-evolutionary approach to automatic
software bug ﬁxing. In Proceedings of the IEEE Congress on Evolu-
tionary Computation (CEC), pages 162–168. IEEE, 2008.
[3] S. Chandra, E. Torlak, S. Barman, and R. Bodík. Angelic debugging.
InProceedings of the 33rd International Conference on Software Engi-
neering (ICSE), pages 121–130. ACM, 2011.
[4] V . Debroy and W . E. Wong. Using mutation to automatically suggest
ﬁxes for faulty programs. In 3rd International Conference on Software
Testing, V eriﬁcation and V alidation (ICST), pages 65–74. IEEE, 2010.
[5] F. DeMarco, J. Xuan, D. Le Berre, and M. Monperrus. Automatic
repair of buggy if conditions and missing preconditions with SMT.
InProceedings of the 6th International Workshop on Constraints in
Software Testing, V eriﬁcation, and Analysis, pages 30–39. ACM, 2014.
[6] B. Demsky and M. Rinard. Automatic detection and repair of errors in
data structures. ACM SIGPLAN Notices, 38(11):78–95, 2003.
[7] B. Elkarablieh and S. Khurshid. Juzi: a tool for repairing complex
data structures. In Proceedings of the 30th International Conference on
Software Engineering, pages 855–858. ACM, 2008.
[8] B. Fluri, M. Wuersch, M. PInzger, and H. Gall. Change distilling:
Tree differencing for ﬁne-grained source code change extraction. IEEE
Transactions on Software Engineering, 33(11):725–743, Nov. 2007.
[9] R. Just, D. Jalali, and M. D. Ernst. Defects4J: A database of existing
faults to enable controlled testing studies for Java programs. InProceedings of the 2014 International Symposium on Software Testingand Analysis, pages 437–440. ACM, 2014. http://defects4j.org.
[10] S. Kaleeswaran, V . Tulsian, A. Kanade, and A. Orso. MintHint:
automated synthesis of repair hints. In 36th International Conference
on Software Engineering (ICSE), pages 266–276. ACM, 2014.
[11] Y . Ke, K. T. Stolee, C. Le Goues, and Y . Brun. Repairing programs with
semantic code search. In 30th IEEE/ACM International Conference on
Automated Software Engineering, (ASE), pages 295–306. IEEE, 2015.
[12] D. Kim, J. Nam, J. Song, and S. Kim. Automatic patch generation
learned from human-written patches. In Proceedings of the International
Conference on Software Engineering (ICSE), pages 802–811. IEEE,2013.
[13] X. B. D. Le, D. Lo, and C. Le Goues. History driven program repair.
In2016 IEEE 23rd International Conference on Software Analysis,
Evolution, and Reengineering (SANER) , volume 1, pages 213–224.
IEEE, 2016.
[14] C. Le Goues, M. Dewey-V ogt, S. Forrest, and W . Weimer. A systematic
study of automated program repair: Fixing 55 out of 105 bugs for $8each. In 34th International Conference on Software Engineering (ICSE) ,
pages 3–13. IEEE, 2012.
[15] C. Le Goues, N. Holtschulte, E. K. Smith, Y . Brun, P . T. Devanbu,
S. Forrest, and W . Weimer. The ManyBugs and IntroClass benchmarksfor automated repair of C programs. IEEE Trans. Software Eng.,
41(12):1236–1256, 2015.
[16] F. Long and M. Rinard. Staged program repair with condition synthesis.
InProceedings of the 10th Joint Meeting on F oundations of Software
Engineering (ESEC/FSE), pages 166–178. ACM, 2015.
[17] F. Long and M. Rinard. Automatic patch generation by learning correct
code. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (POPL), pages298–312. ACM, 2016.
[18] M. Z. Malik and K. Ghori. A case for automated debugging using
data structure repair. In Proceedings of the IEEE/ACM International
Conference on Automated Software Engineering, pages 620–624. IEEE,2009.
[19] M. Martinez, T. Durieux, R. Sommerard, J. Xuan, and M. Monperrus.
Automatic repair of real bugs in Java: A large-scale experiment on theDefects4J dataset. Empirical Software Engineering, 2016.
[21] S. Mechtaev, J. Yi, and A. Roychoudhury. DirectFix: Looking for
simple program repairs. In37th
International Conference on Software
Engineering (ICSE), volume 1, pages 448–458. IEEE, 2015.[20] M. Martinez, W . Weimer, and M. Monperrus. Do the ﬁx ingredients
already exist? An empirical inquiry into the redundancy assumptionsof program repair approaches. In 36th International Conference on
Software Engineering (ICSE), pages 492–495. ACM, 2014.
[22] S. Mechtaev, J. Yi, and A. Roychoudhury. Angelix: Scalable multiline
program patch synthesis via symbolic analysis. In Proceedings of the
38th International Conference on Software Engineering , pages 691–701.
ACM, 2016.
[23] M. Monperrus. Automatic software repair: a bibliography. Technical
Report hal-01206501, University of Lille, 2015.
[24] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra. SemFix:
program repair via semantic analysis. In Proceedings of the International
Conference on Software Engineering (ICSE), pages 772–781. IEEE,2013.
[25] Y . Pei, C. A. Furia, M. Nordio, Y . Wei, B. Meyer, and A. Zeller.
Automated ﬁxing of programs with contracts. IEEE Transactions on
Software Engineering, 40(5):427–449, 2014.
[26] Y . Pei, Y . Wei, C. A. Furia, M. Nordio, and B. Meyer. Code-based
automated program ﬁxing. In 26th IEEE/ACM International Conference
on Automated Software Engineering (ASE), pages 392–395. IEEE, 2011.
[27] J. H. Perkins, G. Sullivan, W . Wong, Y . Zibin, M. D. Ernst, M. Rinard,
S. Kim, S. Larsen, S. Amarasinghe, J. Bachrach, M. Carbin, C. Pacheco,F. Sherwood, and S. Sidiroglou. Automatically patching errors indeployed software. In Proceedings of the ACM SIGOPS Symposium
on Operating Systems Principles, pages 87–102, 2009.
[28] Z. Qi, F. Long, S. Achour, and M. Rinard. An analysis of patch
plausibility and correctness for generate-and-validate patch generationsystems. In Proceedings of the 2015 International Symposium on
Software Testing and Analysis (ISSTA), pages 24–36. ACM, 2015.
[29] E. K. Smith, E. T. Barr, C. Le Goues, and Y . Brun. Is the cure worse than
the disease? Overﬁtting in automated program repair. In Proceedings
of the 10th Joint Meeting on F oundations of Software Engineering(ESEC/FSE), pages 532–543. ACM, 2015.
[30] S. H. Tan, H. Y oshida, M. R. Prasad, and A. Roychoudhury. Anti-
patterns in search-based program repair. In Proceedings of the 24th
ACM SIGSOFT International Symposium on F oundations of SoftwareEngineering (FSE), pages 727–738. ACM, 2016.
[31] Y . Wei, Y . Pei, C. A. Furia, L. S. Silva, S. Buchholz, B. Meyer, and
A. Zeller. Automated ﬁxing of programs with contracts. In Proceedings
of the 19th International Symposium on Software Testing and Analysis(ISSTA), pages 61–72. ACM, 2010.
[32] W . Weimer, Z. P . Fry, and S. Forrest. Leveraging program equivalence
for adaptive program repair: Models and ﬁrst results. In 28th IEEE/ACM
International Conference on Automated Software Engineering (ASE) ,
pages 356–366. IEEE, 2013.
[33] W . Weimer, T. Nguyen, C. Le Goues, and S. Forrest. Automatically
ﬁnding patches using genetic programming. In 31st International
Conference on Software Engineering (ICSE), pages 364–374. IEEE,2009.
[34] W . E. Wong, V . Debroy, and B. Choi. A family of code coverage-
based heuristics for effective fault localization. Journal of Systems and
Software, 83(2):188–208, 2010.
[35] Y . Xiong, J. Wang, R. Y an, J. Zhang, S. Han, G. Huang, and L. Zhang.
Precise condition synthesis for program repair. In Proceedings of the
39th International Conference on Software Engineering (ICSE) . ACM,
Aug. 2017.
[36] J. Xuan, M. Martinez, F. Demarco, M. Clement, S. R. L. Marcote,
T. Durieux, D. L. Berre, and M. Monperrus. Nopol: Automatic repairof conditional statement bugs in Java programs. IEEE Transactions on
Software Engineering, 43(1):34–55, 2017.
[37] X. Zhang, N. Gupta, and R. Gupta. Locating faults through automated
predicate switching. In Proceedings of the 28th International Conference
on Software Engineering (ICSE), pages 272–281. ACM, 2006.
647
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:43 UTC from IEEE Xplore.  Restrictions apply. 