Time to Clean Your Test Objectives
Michaël Marcozzi∗
Imperial College London
Department of Computing
London, United Kingdom
m.marcozzi@imperial.ac.ukSébastien Bardin†
CEA, List
Software Safety and Security Lab
Gif-sur-Yvette, France
sebastien.bardin@cea.frNikolai Kosmatov
CEA, List
Software Safety and Security Lab
Gif-sur-Yvette, France
nikolai.kosmatov@cea.fr
Mike Papadakis
SnT, University of Luxembourg
Luxembourg
michail.papadakis@uni.luVirgile Prevosto‡
CEA, List
Software Safety and Security Lab
Gif-sur-Yvette, France
virgile.prevosto@cea.frLoïc Correnson
CEA, List
Software Safety and Security Lab
Gif-sur-Yvette, France
loic.correnson@cea.fr
ABSTRACT
Testingistheprimaryapproachfordetectingsoftwaredefects.A
major challenge faced by testers lies in crafting efficient test suites,
able to detect a maximum number of bugs with manageable effort.
To do so, they rely on coverage criteria, which define some precise
testobjectivestobecovered.However,manycommoncriteriaspec-
ify a significant number of objectives that occur to be infeasible
orredundantinpractice,likecoveringdeadcodeorsemantically
equalmutants.Suchobjectivesarewell-knowntobeharmfultothe
design of test suites, impacting both the efficiency and precision of
the tester’s effort. This work introduces a sound and scalable tech-
niquetopruneoutasignificantpartoftheinfeasibleandredundant
objectives produced by a panel of white-box criteria. In a nutshell,
wereducethistasktoprovingthevalidityoflogicalassertionsin
thecodeundertest.Thetechniqueisimplementedinatoolthatre-
liesonweakest-preconditioncalculus andSMTsolvingforproving
the assertions. The tool is built on top of the Frama-C verification
platform, which we carefully tune for our specific scalability needs.
Theexperimentsrevealthatthepruningcapabilitiesofthetoolcan
reducethenumberoftargetedtestobjectivesinaprogrambyup
to 27% and scale to real programs of 200K lines, making it possible
to automate a painstaking part of their current testing process.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;•Theory of computation →Program analysis ;
KEYWORDS
Coverage Criteria, Infeasible Objectives, Redundant Objectives
∗A major part of this work has been performed as a CEA, List employee.
†This work has been partially funded by the French ANR (grant ANR-12-INSE-0002).
‡This work has been partially funded by the EU H2020 programme (grant 731453).
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.
For all other uses, contact the owner/author(s).
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5638-1/18/05.
https://doi.org/10.1145/3180155.3180191ACM Reference Format:
Michaël Marcozzi, Sébastien Bardin, Nikolai Kosmatov, Mike Papadakis,
Virgile Prevosto, and Loïc Correnson. 2018. Time to Clean Your Test Objec-
tives.InProceedingsofICSE’18:40thInternationalConferenceonSoftware
Engineering , Gothenburg, Sweden, May 27-June 3, 2018 (ICSE ’18), 12 pages.
https://doi.org/10.1145/3180155.3180191
1 INTRODUCTION
Context. Heretofore,softwaretestingistheprimarymethodfor
detecting software defects [ 2,42,44,66]. It is performed by exe-
cuting the programs under analysis with some inputs, and aims
at finding some unintended (defective) behaviors. In practice, asthe number of possible test inputs is typically enormous, testersdo limit their tests to a manageable but carefully crafted set of
inputs,calleda testsuite.Tobuildsuchsuites,theyrelyonso-called
coverage criteria, also known as adequacy or test criteria, which
definetheobjectivesoftesting[ 2,66].Inparticular,many white-box
criteria have been proposed so far, where the test objectives are
syntacticelementsof thecodethatshouldbecoveredbyrunning
thetestsuite.Forexample,the conditioncoverage criterionimposes
to cover all possible outcomes of the boolean conditions appearing
in program decisions, while the mutant coverage criterion requires
to differentiate the program from a set of its syntactic variants.
Testers need then to design their suite of inputs to cover the cor-
respondingtestobjectives,suchas—forthetwoaforementioned
cases — condition outcomes or mutants to kill.
Problem. White-box testing criteria are purely syntactic and thus
totallyblind tothesemanticsof theprogramunderanalysis. Asa
consequence, many of the test objectives that they define may turn
out to be either
(a)infeasible : no input can satisfy them, such as dead code or
equivalent mutants [2], or
(b)duplicate versionsofanotherobjective:satisfiedbyexactlythe
same inputs, such as semantically equal mutants [53], or
(c)subsumed by another objective: satisfied by every input cover-
ingtheotherobjective[ 1,37,52],suchasvalidityofacondition
logically implied by another one in condition coverage.
We refer to these three situations as polluting test objectives, which
arewell-knowntobeharmfultothetestingtask[ 52,53,62,63,65]
for two main reasons:
4562018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden M. Marcozzi, S. Bardin, N. Kosmatov, M. Papadakis et al.
•While (early) software testing theory [ 66] requires all the cri-
terion objectives to be covered, this seldom reflects the actual
practice, which usually relies on test suites covering onlya part
ofthem[23].Thisisduetothedifficultyofgeneratingtheappro-
priate test inputs, but also to infeasible test objectives. Indeed,
testers often cannot know whether they fail to cover them be-cause their test suites are weak or because they are infeasible,
possibly wasting a significant amount of their test budget trying
to satisfy them.
•As full objective coverage is rarely reached in practice, testersrelyontheratioofcoveredobjectivestomeasurethestrengthof their test suites. However, the working assumption of this
practice is that all objectives are of equal value. Testing research
demonstratedthatthisisnottrue[ 1,13,17,52],asduplicationand
subsumption can make a large number of feasible test objectives
redundant. Such coverable redundant objectives may artificially
deflateorinflatethecoverageratio.Thisskewsthemeasurement,
which may misestimate test thoroughness and fail to evaluate
correctly the remaining cost to full coverage.
GoalandChallenges. Whiledetectingallpollutingtestobjectives
is undecidable [ 1,52], our goal is to provide a technique capable to
identifyasignificantpartofthem.Thisisachallengingtaskasit
requires one to perform complex program analyses over large sets
ofobjectivesproducedbyvariouscriteria.Moreover,duplication
andsubsumptionshouldbecheckedforeachpairofobjectives,a
priori putting a quadratic penalty over the necessary analyses.
Althoughmanystudieshavedemonstratedtheharmfuleffectsof
pollutingobjectives,todatethereisnoscalabletechniquetodiscard
them.Mostrelatedresearchworks(seeTables1,2andSection8)
focusontheequivalentmutantproblem,i.e.theparticularinstanceofinfeasibletestobjectivesforthemutantcoveragecriterion.These
operateeitherin dynamicmode,i.e.mutant classification[ 57,58],
or in static mode, i.e. Trivial Compiler Equivalence (TCE) [ 53].
Unfortunately, the dynamic methods are unsound and producemany false positives [
51,58], while the static one does not deal
with all forms of mutation and cannot detect subsumed mutants
(whereasithandlesduplicatesinadditiontoinfeasibleones).The
LUncov technique [ 8] combines two static analyses to prune out
infeasible objectives from a panel of white-box criteria in a generic
way, but faces scalability issues.
Sound ScaleKind of Pollution Criterion
Inf. Dupl. Subs. Genericity
Mutant class. [58]× /check/check×× ×
TCE[53] /check/check /check /check ××
LUncov[8] /check×/check×× /check
LClean(this work) /check/check /check /check/check /check
Table 1: Comparison with closest research techniques
Analyses Scope Acuteness
TCE[53]built-in compilerinterprocedural +optimizations
LUncov[8]value analysis andinterprocedural ++weakest-precondition
LClean(this work) weakest-precondition local function +
Table 2: Static analyses available in closest techniques
Proposal. Ourintentistoprovidea unified,soundandscalablesolu-
tiontopruneoutasignificantpartof pollutingobjectives,includinginfeasible butalsoduplicate andsubsumed ones,whilehandlinga
largepanelofwhite-boxcriteriaina genericmanner.Toachievethis,
we propose reducing the problem of finding polluting objectives
forawiderangeofcriteriatotheproblemofprovingthevalidity
of logical assertions inside the code under test. These assertions
can then be verified using known verification techniques.
Ourapproach,called LClean,isthefirstonethatscalestopro-
gramscomposedof 200KlinesofC code,whilehandlingall types
ofpollutingtestrequirements.Itisalsogeneric,inthesensethat
it covers most of the common code-based test criteria (described
in software testing textbooks [ 2]) and it is capable of using almost
any state-of-the-art verification technique. In this study, we use
weakest-preconditioncalculus[ 21]withSMTsolving[ 18]andiden-
tify 25K polluting test objectives in fourteen C programs.
LClean introduces two acute code analyses that enable focusing
thedetectionofduplicateandsubsumedobjectivesoveralimited
amount of high-hit-rate pairs of objectives. This makes it possible
to detect a significant number of redundant objectives while avoid-
ing a quadratic penalty in computation time. The LClean tool is
implemented on top of the Frama-C/LTest platform [ 34,40], which
featuresstrongconceptual and technical foundations (Section 3).
WespecificallyextendtheFrama-Cmodulededicatedtoproving
code assertions to make the proposed solution scalable and robust.
Contributions. Tosum up, wemake the followingcontributions:
•TheLCleanapproach: a scalable, sound and unified formal tech-
nique (Sections 2 and 4) capable to detect the three kinds of
polluting test objectives (i.e. infeasible, duplicate and subsumed)
for a wide panel of white-box criteria, ranging from condition
coverage to variants of MCDC and weak mutation.
•Anopen-source prototype tool LClean (Section 5) enacting the
proposed approach. It relies on an industrial-proof formal verifi-cation platform, which we tune for the specific scalability needs
of LClean, yielding a robust multi-core assertion-proving kernel.
•A thorough evaluation (Sections 6 and 7) assessing
(a)thescalabilityanddetectionpowerofLCleanforthreetypes
of polluting objectives and four test criteria – pruning out up
to 27% of the objectives in C files up to 200K lines,
(b)theimpactofusingamulti-corekernelandtailoredverificationlibrariesontherequiredcomputationtime(yieldingaspeedup
of approximately 45×), and
(c)that,comparedtotheexistingmethods,LCleanprunesoutfour
timesmoreobjectivesthanLUncov[ 8]inabouthalfasmuch
time,itcanbeoneorderofmagnitudefasterthan(unsound)
dynamicidentificationof(likely)pollutingobjectives,andit
detectshalfmoreduplicateobjectivesthanTCE,whilebeing
complementary to it.
Potential Impact. Infeasible test objectives have been recognized
as a main cost factor of the testing process [ 62,63,65]. By pruning
outasignificantnumberofthemwithLClean,testerscouldreinvestthesparedcostintargetingfullcoverageoftheremainingobjectives.
This would make testing more efficient, as most faults are found
within high levels of coverage [ 22]. Pruning out infeasible test
objectives could also make the most meticulous testing criteria
(e.g. mutation testing) less expensive and thus more acceptable
inindustry[ 53].Furthermore, gettingridofredundantobjectives
should provide testers with more accurate quality evaluations of
457
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Time to Clean Your Test Objectives ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
1// given three sides x,y,z of a valid triangle, computes
2// its type as: 0 scalene, 1 isosceles, 2 equilateral
3int type = 0;
4// l1: x == y && y == z; (DC) l2: x != y || y != z; (DC)
5// l3: x == y; (CC) l4: x != y; (CC)
6if(x= =y& &y= =z) {
7 type = type + 1;
8}
9// l5: x==y || y==z || x==z; (DC) l6: x!=y && y!=z && x!=z; (DC)
10// l7: x == y; (CC) l8: x != y; (CC)
11// l9: x!=y && y==z && x==z; (MCC) l10: x==y && y!=z && x==z;(MCC)
12if(x= =y| |y= =z| |x= =z) {
13// l11: type + 1 != type + 2; (WM) l12: type+1! =type; (WM)
14// l13: type + 1 != -type + 1; (WM) l14: type+1! =1 ;( W M )
15 type = type + 1;
16}
Figure1:ExampleofasmallCprogramwithtestobjectives
their test suites and also result in sounder comparisons of test
generation techniques [52].
2 MOTIVATING EXAMPLE
Figure1showsasmallCprograminspiredbytheclassictriangle
example [ 43]. Given three integers x,y,zsupposed to be the sides
of a valid triangle, it sets variable typeaccording to the type of
thetriangle:equilateral( type=2),isosceles( type=1)orscalene
(type=0). Figure 1 also illustrates fourteen test objectives from
common test criteria labelled from l1tol14.l1andl2require the
test suite to cover both possible decisions (or branches) of the
conditional at line 6. For example, covering l2means to find test
datasuchthat,duringtestexecution,thelocationof l2isreached
and the condition x! =y| |y! =z is true at this location,
whichensurestoexecutetheelsebranch.Similarly, l5andl6require
theteststocoverbothdecisionsatline12.Thesefourobjectivesarespecified by the Decision Coverage (DC) criterion for this program.
l3andl4(resp.,l7andl8)requiretheteststocoverbothtruthvalues
ofthefirstconditioninthecompoundconditiononline6(resp.,line
12).TheyareimposedbyConditionCoverage(CC)–thesimilartest
objectivesimposedbyCCfortheotherconditionsarenotshown
to improve readability. l9andl10provide examples of objectives
fromMultipleConditionCoverage(MCC)forconditionalatline12.
MCC requires the tests to cover all combinations of truth values of
conditions.Finally,objectives l11tol14encodesomeWeakMutants
(WM)oftheassignmentonline15(seeBardinetal.[ 9,Theorem2]
for more detail).
We can easily notice that l9andl10put unsatisfiable constraints
overx,yandz. They are thus infeasible objectives and trying to
coverthemwouldbeawasteoftime.Otherobjectivesare duplicates,
denotedby⇔:theyare alwayscovered(i.e.reached andsatisfied)
simultaneously. We obviously have l3⇔l7andl4⇔l8since the
valuesofxandydonotchangein-between.Althoughsyntactically
different, l13andl14are also duplicates, as they are always reached
together (we call them co-reached objectives) and satisfied if and
only iftype/nequal0. Finally, we refer to objectives like l11andl12as
beingtrivialduplicates:theyareco-reached,andalwayssatisfied
as soon as reached. While we do not have l1⇔l5, covering l1
necessarily implies covering l5, that is, l1subsumes l5, denoted
l1⇒l5. Other examples of subsumed objectives can be found,
likel6⇒l2. Duplicate and subsumed objectives are redundant
objectives that can skew the measurement of test suite strength,as it should be provided by the test coverage ratio. For example,considering the objectives from the DC criterion, the test suite
composedofthesingletest (x=1,y=2,z=1)coversl2andl5but
notl1andl6,whichimpliesamediumcoverageratioof50%.The
tester may be interested to know the achieved level of coverage
without counting duplicate or subsumed objectives. Here, l2andl5
are actually subsumed by l1andl6. If the subsumed objectives are
removed,thecoverageratiofallsdownto0%.Discardingredundant
objectives provides a better measurement of how far testers are
from building an efficient test suite, only with the necessary inputs
for covering the non-redundant objectives ( l1andl6in this case).
Themainpurposeofthispaperistoprovidealightweightyet
powerfultechniqueforpruningoutinfeasible,duplicateandsub-
sumed test objectives. To do so, our approach first focuses on in-feasible objectives. In Figure 1, one can notice, for instance, thatthe problem of proving
l9to be infeasible can be reduced to the
problem of proving that a code assertion !(x!=y && y==z
&& x==z) at line 11 will never be violated. Our approach then
delegatesthisproofforeachobjectivetoadedicatedverification
tool.Whileinfeasibilityshouldbecheckedonceperobjective, dupli-
cationandsubsumptionrequireonetoanalyseallthepossiblepairs .
To avoid quadratic complexity, we focus on detecting duplicate
andsubsumedpairsonly amongtheobjectivesthatbelongtothe
same sequential block of code, with no possible interruption of the
control flow (with goto, break, ...) in-between. By construction,
theobjectivesinthesegroupsarealwaysco-reached.InFigure1,
l1–l10andl11–l14are two examples of such groups. Examples of
duplicate and subsumed objectives within these groups include
l3⇔l7,l4⇔l8,l11⇔l12,l13⇔l14,l1⇒l5andl6⇒l2. We call
themblock-duplicate andblock-subsumed objectives.Ontheother
hand,l1andl13are duplicate (at line 14, typeis nonzero if and
onlyif x,y,and zareequal),butthiswillnotbedetectedbyour
approach since those labels are not in the same block.
3 BACKGROUND
3.1 Test Objective Specification with Labels
Given a program Pover a vector Vofminput variables taking
values in a domain D/definesD1×···×Dm,atest datum tforPis a
valuation of V, i.e.t∈D.Atest suite TS⊆Dis a finite set of test
data.A(finite)executionof Poversome t,denoted P(t),isa(finite)
runσ/defines/angbracketleft(loc0,s0),..., (locn,sn)/angbracketrightwherethe locidenotesuccessive
(control-)locationsof P(≈statementsoftheprogramminglanguage
inwhich Piswritten), loc0referstotheinitialprogramstateand
thesidenotethesuccessiveinternalstatesof P(≈valuationofall
globalandlocal variablesandofallmemory-allocatedstructures)
after the execution of each loci.
A testdatum treachesa location locat stepkwith internal state
s, denoted t/leadstok
P(loc,s),i fP(t)has the form σ·(loc,s)·ρwhere
σis a partial run of length k. When focusing on reachability, we
omitkand write t/leadstoP(loc,s).
Givenatestobjective c,wewrite t/leadstoPciftestdatum tcoversc.
Weextendthenotationforatestsuite TSandasetoftestobjectives
C,writing TS/leadstoPCwhenforany c∈C,thereexists t∈TSsuch
thatt/leadstoPc.A(source-codebased)coveragecriterion Cisdefinedas
asystematicwayofderivingasetoftestobjectives C/definesC(P)for
anyprogramundertest P.Atestsuite TSsatisfies(orachieves)a
coverage criterion CifTScovers C(P).
458
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden M. Marcozzi, S. Bardin, N. Kosmatov, M. Papadakis et al.
Labels.Labelshave been introduced in [ 9] as a code annotation
languagetoencodeconcretetestobjectives.Severalcommoncover-
agecriteriacanbesimulatedbylabelcoverage,inthesensethatfor
a given program Pand a criterion C, every concrete test objective
from C/definesC(P)can always be encoded using a corresponding label.
Given a program P,alabel/lscript∈LabsPis a pair (loc,φ)whereloc
isalocationofPand φisapredicateovertheinternalstateat loc.
There can be several labels defined at asingle location, which can
possiblysharethesamepredicate.Moreconcretely,thenotionof
labels can be compared to labels in the C language, decorated with
a pure (i.e. side-effect-free) boolean C expression.
Wesaythatatestdatum tcoversalabel/lscript/defines(loc,φ)inP,denoted
tL/leadstoP/lscript, if there is a state ssuch that treaches (loc,s)(i.e.t/leadstoP
(loc,s)) andssatisfiesφ.A nannotated program is a pair/angbracketleftP,L/angbracketright
wherePis a program and L⊆LabsPis a set of labels for P. Given
an annotated program /angbracketleftP,L/angbracketright, we say that a test suite TSsatisfies
thelabelcoveragecriterion (LC)for/angbracketleftP,L/angbracketright,denoted TSL/leadsto/angbracketleftP,L/angbracketrightLC,
ifTScovers every label of L(i.e.∀/lscript∈L:∃t∈TS:tL/leadstoP/lscript).
CriterionEncoding. Labelcoverage simulatesacoveragecriterion
Cif any program Pcan beautomatically annotated with a set of
corresponding labelsLin such a way that any test suite TSsatisfies
LCfor/angbracketleftP,L/angbracketrightifandonlyif TScoversalltheconcretetestobjectives
instantiated from CforP. The main benefit of labels is to unify
the treatment of test requirements belonging to different classes
ofcoveragecriteriainatransparentway,thankstothe automatic
insertionof labels in the program under test. Indeed, it is shown in
[9]thatlabelcoveragecannotablysimulatebasic-blockcoverage
(BBC), branch coverage ( BC), decision coverage ( DC), function
coverage ( FC), condition coverage ( CC), decision condition cov-
erage (DCC), multiple condition coverage ( MCC) as well as the
side-effect-free fragment of weak mutations ( WM’). The encoding
ofGACCcomes from [50]. Some examples are given in Figure 1.
Co-reached Labels. We say that location locisalways preceded
bylocationloc/primeif for any test datum t, whenever the execution
P(t)/defines/angbracketleft(loc0,s0),..., (locn,sn)/angbracketrightpasses through location locat
stepk(i.e.loc=lock) thenP(t)also passes through loc/primeat some
earlier step k/prime≤k(i.e.loc/prime=lock/prime) without passing through locor
loc/primein-between (i.e. at some intermediate step iwithk/prime<i<k).
Similarly, loc/primeis said to be always followed by locationlocif for
anyt,whenevertheexecution P(t)passesthrough loc/primeatstepk/prime
thenP(t)also passes through locat some later step k≥k/primewithout
passingthrough locorloc/primein-between.Twolocationsare co-reached
if one of them is always preceded by the other, while the second
oneisalwaysfollowedbythefirstone.Notethatweexcludethe
case when one of locations is traversed several times (e.g. due to a
loop)beforebeingfinallyfollowedbytheotherone.Inasequential
block of code, with no possible interruption of the control flow in-
between(nogoto,break,...),alllocationsareco-reached.Wefinally
say that two labels are co-reached if their locations are co-reached.
3.2 Polluting Labels
Intheremainderofthepaper,testobjectiveswilloftenbeexpressed
in terms of labels. This work addresses three kinds of polluting
labels:infeasible,duplicateandsubsumed.Alabel /lscriptinPiscalled
infeasible if there is no test datum tsuch that tL/leadstoP/lscript. In other
words,itisnotpossibletoreachitslocationandsatisfyitspredicate.Wesaythatalabel /lscriptsubsumes anotherlabel/lscript/prime(or/lscript/primeissubsumed
by/lscript)inP,denoted/lscript⇒/lscript/prime,ifforanytestdatum t,iftL/leadstoP/lscriptthen
tL/leadstoP/lscript/primeas well. Finally, two labels /lscriptand/lscript/primeare called duplicate1,
denoted/lscript⇔/lscript/prime, if each of them subsumes the other one. For the
specificcasewherebothlabels /lscriptand/lscript/primebelongtothesamegroup
ofco-reachedlabelsinablock,wecalladuplicate(resp.,subsumed)
labelblock-duplicate (resp.,block-subsumed ).
Noticethatifalabel /lscriptisinfeasible,itsubsumesbydefinitionany
other label/lscript/prime. We call this phenomenon degenerate subsumption.I f
/lscript/primeisfeasible,itshouldbekeptandcovered.Inthiscase,thetruly
polluting objective is /lscriptrather than/lscript/prime. That is the reason why it is
necessary to eliminate as many infeasible labels as possible before
pruning out subsumed labels.
3.3 The Frama-C/LTest Platform
Frama-C[ 34]isanopen-sourceindustrial-strengthframeworkded-
icatedtotheformalanalysisofCprograms.Ithasbeensuccessfully
used in several safety and security critical contexts. The tool is
writteninOCaml,andrepresentsaverysignificantdevelopment
(around 150K lines for the kernel and the main plug-ins alone).
Frama-C is based on a small kernel that takes care of providing
anabstractrepresentationoftheprogramunderanalysisandmain-tainingthesetofpropertiesthatareknownabouttheprogramstate
ateachpossibleexecutionstep.Thesepropertiesareexpressedas
ACSL [11] annotations. On top of the kernel, many plug-ins can
perform various kinds of analysis, and can interact with the kernel
eitherby indicatingthata property ϕholds,or byaskingwhether
someotherproperty ψistrue(inthehopethatanotherplug-inwill
be able to validate ϕlater on).
In the context of this paper, we are mainly interested in the
four following (open-source) plug-ins. LAnnotate, LUncovandLRe-
playare part of Frama-C/LTest [ 7,40]. LAnnotate annotates the
program with labels according to the selected criterion. LUncov
combines weakest-precondition and value analysis to detect infea-
sible test objectives. LReplay executes a test suite and computes its
coverageratio. WPisaplug-inimplementingweakest-precondition
calculus [10, 28] in order to prove that an ACSL assertion holds.
4 THE LCLEAN APPROACH
TheLCleantechniqueinvolvesthreemainsteps(cf.Figure2)pre-
ceded by a preprocessing phase. The first step aims at detecting
infeasible label-encoded objectives. The second step targets trivial
block-duplicate labels, while the third step focuses more generally
on block-subsumed and block-duplicate labels.
Given a program Pand a coverage criterion Cthat can be simu-
lated by labels, the preprocessing consists in generating the corre-
spondinglabels L.ForCprograms,thisisdonebytheLAnnotate
plug-in of Frama-C. The LClean approach itself operates on the
annotatedprogram /angbracketleftP,L/angbracketrightandmarkspollutinglabelssothatthey
can be pruned out.
4.1 Step 1: Infeasible Labels
LClean systematicallyexplores /angbracketleftP,L/angbracketrightand replacesevery label /lscript/defines
(loc,φ)byanassertion assert(! φ),whosepredicateisthenega-
tion of the label condition. The resulting assertion-laden code is
1The term equivalent label is not used here to avoid any confusion with the notion of
equivalent mutant, which in mutation testing means infeasible objective.
459
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Time to Clean Your Test Objectives ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Figure 2: Process view of the LClean approach with main steps and substeps
senttoadeductiveverificationtooldesignedforprovingthatthe
received program is correct w.r.t. the defined assertions, i.e. that
none of them can be violated during a possible run of the program.
In practice, the verification tool returns the list of the assertions
thatitwasabletoprovecorrect.Sinceeachassertionisbyconstruc-
tion the negation of a label condition, the corresponding labels are
formallyproventobeinfeasible,andaremarkedasso.Thesemarks
willbebothusedasafinalresultoftheapproachandasinternal
informationtransmittedtothenexttwostepsofLClean.Regarding
Figure 1, LClean indeed detects that l9andl10are infeasible.
4.2 Detection of Co-reached Labels
1void calledOnce () {
2 // l1:φ1
3 code1;
4}
5int main ( int i){
6// l2:φ2
7if(i>0) {
8 // l3:φ3
9 if(i==5) i++;
10 // l4:φ4
11 calledOnce ();
12 if(i==7) exit(0);
13 // l5:φ5
14 i++;
15 // l6:φ6
16}else {
17 // l7:φ7
18 code2;
19}
20return i;
21}
Figure 3: Co-
reached locationsPriortoSteps2and3,LCleanperforms
the detection of blocks of co-reached
locations.Weillustrateitusingthesam-
ple program of Figure 3. First, a basic
syntactic analysis detects six blocks in
the program: the global block of each
of the two functions, the two branches
oftheouterconditional(line7),andthe
then branches of the two nested con-
ditionals. Second, a call-graph analysis
discovers that the first function is only
called once in the whole program, so
that its outer block can be seen as exe-
cuted as a part of the block containing
the function call. The two blocks can
thenbemerged.Finally,aconservative
control-flow interruption analysis de-
tects that the exit(0); statement at
line 9 may interrupt the control-flow
withinthethenbranchoftheouterconditional.Thecorrespond-
ingblockisthussplitintotwoblocks,gatheringrespectivelythe
statements before and after the exit(0); statement. The identi-
fied blocks enabling us to conclude that there are four groups of
mutually co-reached labels: {l2},{l3,l4,l1},{l5,l6}and{l7}.
4.3 Step 2: Trivial Block-Duplicate Labels
As in Step 1, LClean systematically explores /angbracketleftP,L/angbracketrightand replaces la-
belsbyassertions.ExceptforthelabelsmarkedasinfeasibleinStep
1, which are simply dropped out, each label /lscript/defines(loc,φ)is replaced
byanassertion assert( φ).Thistime,thepredicateisdirectlythe
labelcondition.Theresultingassertion-ladencodeissenttothever-
ification tool. The proven assertions correspond to labels that will
be always satisfied as soon as their location is reached. Afterwards,
LCleanidentifiesamongthesealways-satisfied-when-reachedthe
groups of co-reached labels (cf. Section 4.2). The labels within each
ofthegroupsaretrivialblock-duplicates,andtheyaremarkedasbe-
ingclonesofasinglelabelchosenamongthem.Again,thesemarkswill be both final results and internal information transmitted to
thenextstep.FortheexampleofFigure1,LCleanwillidentifythat
l11andl12aretrivial block-duplicate labels.Similarly, ifwe assume
that all predicates φiare always satisified for the code of Figure 3,
Step2detectsthat l3,l4andl1aretrivialduplicates,and l5andl6
areaswell.Asasubtleoptimization,LCleancandetectthatlabel
l2is always executedsimultaneouslywith the outer conditional,so
thatl2will be covered if and only if at least one of the labels l3and
l6is covered. l2can thus be seen as duplicate with the pair ( l3,l6)
and is marked as so.
4.4 Step 3: Block-Subsumed Labels
Within each group of co-reached labels, the labels previously de-
tected as infeasible by Step 1 are removed and those detected astrivial block-duplicates by Step 2 are merged into a single label.
Afterwards, every label
/lscripti= (loci,φi)remaining in the group
is replaced by a new statement int vli=φi;, which assigns
thevalueofthelabelconditiontoafreshvariable vli.Then,for
each pair (/lscripti,/lscriptj)i/nequaljof co-reached labels in the group, the assertion
assert(vl i=⇒vlj);isinsertedattheendofthecorrespond-
ing block of co-reached locations. If this assertion is proven by
theverificationtool,thenlabel /lscriptisubsumeslabel/lscriptj.Indeed,their
locationsareco-reached,andtheprovenassertionshowsthatevery
input satisfying φiwill also satisfy φj. As a consequence, every
input that covers/lscriptialso covers/lscriptj.
The graph of subsumption relations detected in a group of co-
reachedlabelsisthensearchedforcycles.Alllabelsinacycleare
actuallyduplicatesandcanbemarkedasmergeableintoasingle
label.Amongthelabelsthatsurvivesuchamergingphase,those
thatarepointedtobyatleastonesubsumptionrelationaremarkedassubsumedlabels.FortheexampleofFigure1,LCleanwillidentify,
for instance, l1⇒l5,l6⇒l2,l3⇔l7andl13⇔l14.
4.5 Discussion of LClean Design
Once the third and final step finished, LClean returns a list of
polluting labels composed of the infeasible ones returned by Step 1
and of the duplicate and subsumed ones returned by Steps 2 and3. It should be noted that the approach is incremental and thateach of the three main steps can even be run independently of
the others. However, removing infeasible objectives before Steps 2
and3 isimportant, asitreduces therisk ofreturning degeneratesubsumption relations. Similarly, Step 2 detects duplicate labels
that would be identified by Step 3 anyway, but Step 2 finds them atmuchlowercost.Indeed,thenumberofproofsrequiredbyStep2is
linear in the number of labels as it does not have to consider pairs
oflabels.Theincrementalnatureoftheapproach,coupledwiththe
fact that assertion proving has become reasonably fast (c.f. Section
6)andthatitcanbeparallelised,aswellasperformedindependently
over stand-alone code units (e.g. C functions), makes a continuous
computationofpollutingobjectivesconceivableduringsoftware
460
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden M. Marcozzi, S. Bardin, N. Kosmatov, M. Papadakis et al.
Criterion Labels Block Pairs Function Pairs Program Pairs
CC 27,638 94,042 3,013,940 428,075,244
MCC 30,162 314,274 3,961,004 503,856,852
GACC 27,638 94,042 3,013,940 428,075,244
WM 136,927 2,910 908 80,162,503 8,995,885,473
TOTAL222,365 3,413,266 90,151,387 10,355,892,813
(×1/15)(×1 )(×26 )(×3034 )
Figure 4: Number of pairs of labels in 14 C programs
development. This could be used for continuous integration to
enforce test suites of specific coverage levels.
TheLCleanapproachmightbeextendedtodetectduplicateor
subsumedlabelsthatarenotinthesamebasicblock,bygenerating
more complex assertions that would be flow-sensitive. However,
limiting the analysis to block-duplicate and block-subsumed labels
turns out to be a sweet spot between detection power and com-
putation time. Indeed, Figure 4 details the total number of pairsof labels for four common criteria in the 14 C programs used in
the evaluation in Section 6 (cf. Figure 6). Figure 4 also presents the
totalnumber ofpairsof labelstakeninsidethe sameblock,inside
the same function or over the whole program. We can see that
focusing the analysis on block pairs enables reducing the number
of necessary proofs by one to four orders of magnitude. At the
sametime,itseemsreasonabletothinkthatasignificantpartofthe
duplicate or subsumed labels reside within the same basic block,
as those labels are always executed together and typically describe
test objectives related to closely interconnected syntactic elements
of the program.
5 IMPLEMENTATION
ThethreestepsoftheLCleanapproachareimplementedin three
independent open-source Frama-C plug-ins2(≈5,000 locs in OCaml).
Theseplug-insshareacommonarchitecturedepictedinFigure5.
ItreliesontheFrama-Ckernel(inblack)andfeaturesfourmodules
(in color) performing the different substeps of an LClean step. It
receives as input an annotated program /angbracketleftP,L/angbracketright, in which labels
havealreadybeengeneratedwithplug-inLAnnotate[ 7]inor der
tosimulatethecoveragecriterionofinterest.Asastartingpoint,
the program is parsed by the Frama-C kernel, which makes its
abstractsyntaxtree(AST)availableforallthecomponentsofthe
architecture. We now present the four modules performing the
analysis.
AssertionGenerator. TheAssertionGeneratorreplacesthelabels
inthecodebyassertionsaccordingtothecorrespondingstep(cf.
Section 4). Frama-C primitives are used to explore the AST, locate
thenodescorrespondingtolabelsandreplacethembytherequired
assertions, written in ACSL.
2Available from http://icse18.marcozzi.net.
Figure 5: Frama-C plug-in implementing one LClean stepRobustMulticoreAssertionProver. TheAssertionProverdeals
withprovingtheassertionsintroducedintheASTbytheAssertion
Generator and relies on the WP plug-in. It is not a simple wrap-
perforWP:theAssertionProverintroduces crucialoptimizations
ensuring its scalability and robustness:
•First, it embeds a version of WP that we carefully optimized
for our specific needs, making it capable to prove severaldifferent assertions independently in a single run of the
tool. This version factors out a common part of the analysis
(related to the program semantics) that would have to be
repeated uselessly if WP was called once per assertion.
•Second, its multi-core implementation ensures a significant
speedup. The assertions to be proved are shared among sev-
eral parallel WP instances running on different cores.
•Third,the AssertionProveralsoguarantees robustnessand
adaptabilityoftheprocess.Indeed,theWPtoolcanconsume
a high amount of memory and computation time when ana-
lyzingalargeandcomplexCfunction.TheAssertionProver
can smoothly interrupt a WP session when a threshold w.r.t.
the used memory or elapsed time has been reached.
Alltheseimprovements toFrama-C/WPhavebeenprovencru-
cial for large-scale experiments (cf. Section 6). A technical descrip-
tionofhowtheywereactuallyimplemented,comparingtheopti-
mised and non-optimised source code of the tool, can be found on
the companion website2of this paper.
Label StatusManager. TheLabel StatusManager maintainsand
givesaccesstoasetoffilesstoringastatusforeachlabel.EachlabelisidentifiedbyauniqueintegerIDusedbothintheASTandinthe
status files. The status of a label can be a) infeasible, b) duplicateto another ID (or a pair of IDs), c) subsumed by other IDs, or d)
unknown.Thestatusfilesareupdatedbytheplug-inswhenthey
detect that some labels can be marked as polluting. The plug-ins
for Steps 2 and 3 also check the files in order to drop out the labels
marked as polluting during the previous steps.
BlockDetector. Thedetectorofblocksofco-reachedlabelsisonly
used before Steps 2 and 3. It relies on the Frama-C primitives to
exploretheASTandperformtheanalysesdetailedinSection4.2.
Foreachblockfound,itreturnsthelabelIDsofco-reachedlabels
belonging to the block.
6 EXPERIMENTAL EVALUATION
ToevaluateexperimentallyLClean,weconsiderthefollowingthree
research questions:
ResearchQuestion1(RQ1) :Istheapproacheffectiveanduse-
ful?Especially,(a)Doesitidentifyasignificantnumberofobjectives
from common criteria, all being real polluting objectives? b) Can it
scaletoreal-worldapplications,involvingmanylinesofcodeand
complex language constructs?
Research Question 2 (RQ2) : Do the optimizations (Section
5) improve the time performance in a significant way, impacting
LClean acceptability in practice?
ResearchQuestion3(RQ3) :Howdoesourapproachcompare
with the closest approaches like LUncov, mutant classification and
TCE, especially in terms of pruning power and time performance?
Theexperimentalartefactsusedtoanswerthesequestionsand
the fully detailed results that we obtained are available on the
461
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Time to Clean Your Test Objectives ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
companion website2of the paper. The tool and artefacts have also
been installed in a Linux virtual machine provided on the website
and enabling an easy reproduction of the experiments described
in thenext subsections. Allthese experiments were performedon
a Debian Linux 8 workstation equipped with two Intel Xeon E5-
2660v3 processors, for a total of 20 cores running at 2.6Ghz and
taking advantage of 25MB cache per processor and 264GB RAM.
6.1 RQ1: Effectiveness and Scalability
WeconsiderfourteenCprogramsofvarioustypesandsizes(min:
153locs,mean:16,166locs,max:196,888locs)extractedfromfive
projects:thesevenSiemensprogramsfrom[ 29],fourlibrariestaken
from the cryptographic OpenSSL toolkit [ 49], the full GNU Zip
compression program [ 27], the complete Sjeng chess playing IA
application [ 59] and the entire SQLite relational database manage-
mentsystem[ 60].Everyprogramisannotatedsuccessivelywiththe
labels encoding the test objectives of four common coverage crite-
ria:ConditionCoverage(CC),Multiple-ConditionCoverage(MCC),GeneralActiveClauseCoverage(GACC)andWeakMutations(WM,
with sufficient mutation operators [ 46]). The LClean tool is then
run to detect polluting objectives for each (program,criterion) pair.
For each step of the LClean process, the number of marked
objectives and the computation time are reported in Figure 6. 11%
of the 222,365 labels were marked as polluting in total (min: 4% for
CC/MCC with SQLite, max: 27% for WM in Siemens/printokens.c).
Theglobalratioofmarkedpollutingobjectivesis5%forCC,5%for
MCC, 6% for GACC and 15% for WM. In total, 13% of the detected
polluting objectives were infeasible, 46% were duplicate (about one
halfweremarkedduringStep2andtheotherduringStep3)and41%
weresubsumed.Thecomputationtimerangesfrom10sforMCCin
Siemens/schedule.c (410 locs and 58 objectives) to ∼69h for WM in
SQLite (197K locs and 90K objectives). Globally, computation time
is split into 10% for Step 1, 8% for Step 2 and 82% for Step 3. While
the computation time is acceptable for a very large majority of
theexperiments,Step 3becomesparticularlycostlywhen applied
on thelargest programswith themost meticulous criteria.This is
ofcourseduetothefactthatthisstepisquadraticinthenumber
of labels. While we limit our analysis to block pairs, the number
ofresultingproofattemptsstillgetslargeforbiggerapplications,
reaching1.8MproofsforSQLiteandWM(whichremainstractable).
Yet, limiting LClean to Steps 1 & 2 still marked many labels and is
muchmoretractable:onSQLite,itdetects4566pollutingobjectives
in only 9h (13692 objectives in 69h for full LClean). Moreover, this
shouldbecomparedtothefactthatrunningtheSQLiteTH3test
suite3andcomputingthemutationscoretakesmanydaysandthat
identifyingpollutingobjectivesisatime-consumingmanualtask
(authors of [ 58] report 15 minutes per instance). As the SQLlite
developers report3that they work hard to obtain test suites with a
100% coverage score for different criteria, they should immediately
benefit from our tool.
Conclusion:TheseresultsindicatethatLCleanisausefulapproach
able to detect that a significant proportion of the test objectives from
variouscommoncriteriaarepollutingones,evenforlargeandcom-
plexreal-wordapplications. Inpractice,forverylarge programsand
demandingcriteria,LCleancanbelimitedtoSteps1&2,keepinga
significant detection power at a much lower expense.
3https://www.sqlite.org/testing.html6.2 RQ2: Impact of Optimizations
Werepeattheexperimentsperformedin RQ1fortheWMcriterion
over the seven Siemens programs, but we deactivate the optimiza-
tions that we implemented in the Assertion Prover of our tool,
namelytailoredWPtoolandmulti-coreimplementation(Section
5). Figure 7 details the obtained computation times (in logarithmic
scale)forthethreestepsoftheLCleanprocess,consideringthree
levelsofoptimizations.Atlevel0(oblique-linedblue),theAssertion
Prover uses a single instance of the classical Frama-C/WP running
on a single core. At level 1 (horizontal-lined red), the Assertion
Prover uses 20 instances of the classical version WP running on20 cores. Level 2 (plain beige) corresponds to the actual version
ofthetoolusedin RQ1,whenalltheoptimizationsareactivated:
the Assertion Prover uses 20 instances of our tailored version WP
running on 20 cores.
Weobservethatthetotalcomputationtimeisreducedbya factor
of 2.4when switching from level 1 to level 2, and that it is reduced
byafactorof45 whenswitchingfromlevel0tolevel2.Thesefactors
areverysimilarforallthestepsoftheLCleanprocess.Theanalysis
results remained unchanged across the optimization levels.
Conclusion: These results show that our optimizations have a very
significant impact over the time performance of our tool, makingthe experiments on large programs intractable without them. The
measuredspeedupof45xhasasensibleinfluenceovertheperceived
speedoftheto ol, improving its acceptability in practice.
6.3 RQ3: LClean vs. Closest Related Works
6.3.1 LUncov. We apply both LUncov [ 8] and LClean on the
same benchmarks [ 8]. The measured computation time and detec-
tion power for LUncov and LClean are compared in Figure 8. As
LUncov is limited to infeasibility, we also provide results for Step 1
ofLClean.ItappearsthatLCleandetects4.2 ×morepollutinglabels
than LUncov in 1.8 ×less time. When LClean is limited to Step 1, it
detects 1.6×less polluting labels than LUncov, but in 10 ×less time.
Conclusion:LCleanprovidesamoreextensivedetectionofpolluting
objectivesthanLUncov(especiallyasitgoesbeyondinfeasibility)at
cheaper cost, thanks to modularity and optimized implementation.
6.3.2 Mutant Classification. Thecoreprincipleofmutantclassi-
fication [57,58] is to rely on dynamic coverage data to identify (in
anapproximatedway)pollutingmutants.AsacomparisonbetweenLCleanandsuchadynamicpruningprinciple,Figure9revealsthat
the time necessary torun a high-coverage test suite (Siemens test
suite),savecoverage dataandfindlikely-pollutingobjectivescan
be one order of magnitude higher than running LClean over the
sametestobjectives.Inthesametime,itappearedthatmanyoftheobjectivesdetectedinthiswaywerefalsepositives,leadingtoa89%
rateoflabelstobeconsideredaslikelypolluting(mainlybecause
ofduplicationandsubsumption).Actually,whiletheSiemenstest
suiteachieveshighcoverageofstandardmetrics,itisnotbuiltto
reveal different coverage behaviours between feasible test objec-
tives. Crafting new test cases to do so would reduce the number of
false positives but even more penalize the computation time.
Conclusion: By relying on lightweight static analyses, LClean pro-
vides a sound and quick detection of a significant number of both
infeasibleandredundanttestobjectives,whiledynamicdetectionis
expensiveandunsound,yieldingmanyfalsepositivesevenbasedon
high-quality test suites.
462
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden M. Marcozzi, S. Bardin, N. Kosmatov, M. Papadakis et al.
Benchmark LabelsSTEP 1 STEP 2 STEP 3 TOTAL
Criterion marked astimemarked astimemarked as marked astimemarked as pollutingtimeinfeasible duplicate duplicate subsumed ratio %
siemens 654 0 35s 0 38s 2 41 83s 43/654 7% 156s CC
(agg. 7 programs) 666 20 36s 0 40s 0 16 78s 36/666 5% 154s MCC
3210 locs 654 1 37s 0 39s 18 17 77s 36/654 6% 153s GACC
3543 37 114s 123 126s 134 336 723s 630/3543 18% 963s WM
openssl 1022 28 67s 3 67s 4 57 391s 92/1022 9% 525s CC
(agg. 4 programs) 1166 134 77s 0 83s 2 24 294s 160/1166 14% 454s MCC
4596 locs 1022 29 70s 0 81s 30 24 324s 83/1022 8% 475s GACC
4978 252 356s 270 372s 200 326 4214s 1048/4978 21% 5122s WM
gzip 1670 23 149s 5 152s 19 54 578s 101/1670 6% 879s CC
7569 locs 1726 44 170s 5 171s 11 34 628s 94/1726 5% 969s MCC1670 31 154s 5 156s 43 34 555s 113/1670 7% 865s GACC
12270 267 1038s 942 1210s 542 895 10029s 2646/12270 22% 12277s WM
sjeng 4090 34 351s 15 354s 82 215 798s 346/4090 8% 1503s CC
14070 locs 4746 358 417s 9 436s 34 26 1912s 427/4746 9% 2765s MCC4090 35 349s 15 353s 82 210 751s 342/4090 8% 1453s GACC
25722 353 5950s 483 4791s 640 706 19586s 2182/25722 8% 31478s WM
sqlite 20202 120 1907s 3 1416s 130 456 4646s 709/20202 4% 7969 CC
196888 locs 21852 394 2295s 0 1902s 178 255 11958s 827/21852 4% 16155 MCC
20202 129 2065s 0 1613s 803 223 4773s 1155/20202 6% 8451 GACC
90240 878 18104s 3688 13571s 2962 6164 216140s 13692/90240 15% 247815s WM
TOTAL 27638 205 2509s 26 2027s 237 823 6496s 1291/27638 5% 3h3m52 CC
226333 locs 30156 950 2995s 14 2632s 225 355 14870s 1544/30156 5% 5h41m37 MCC
27638 225 2675s 20 2242s 976 508 6480s 1729/27638 6% 3h9m57 GACC
136753 1787 25562s 5506 20070s 4478 8427 250692s 20198/136753 15% 82h18m44 WM
222185 3167 9h22m21 5566 7h29m31 5916 10113 77h22m18 24762/222185 11% 94h14m10 TOTAL
Figure 6: Pruning power and computation time of LClean over 14 various "real-world" C programs
STEP 1 STEP 2 STEP 3 TOTAL101103105computation time (s) in log. scale1 core + classic WP
20 cores + classic WP
20 cores + tailored WP
Figure 7: Tool optimization impact (Siemens, WM)
CriterionLUncov LClean (step 1) LClean(all steps)
marked time marked time marked time
CC 4/162 97s 4/162 12s 51/162 46s
MCC 30/203 125s 30/203 15s 51/203 53s
WM 84/905 801s 41/905 75s 385/905 463s
TOTAL9% 17m3s 6% 1m42s 38% 9m22s
(×1 ) (×1 ) (÷1.6) (÷10 ) (×4.2) (÷1.8)
Figure 8: LUncov [8] vs LClean (benchmarks from [8])
6.3.3 Trivial Compiler Equivalence (TCE). A direct comparison
with TCE [ 53] is not possible, as TCE aims at identifying strong
mutantequivalences,whicharefundamentallydifferentfromthe
structuraloneswe handle.Killingstrongmutantsrequiresindeed
the propagation of the mutated program states to the program out-
puts, which is more complex to formalize [ 20]. Thus, the only way
to compare the two approaches is to assume that weakly polluting
mutantsarealsostronglypollutingones.Thisassumptionistrue
for the case of equivalent mutants, but not entirely true for the
caseoftheduplicatedmutants.Weaklyduplicatesmutantsmight
not be strongly duplicates due to failed mutated state propagation.
However, this is usually quite rare, as most weakly killed mutants
propagate to the program outputs [ 47]. Nevertheless, we reporttheseresultsfordemonstratingthecapabilitiesoftheapproaches
and not for suggesting a way to detect redundant strong mutants.
To perform the comparison, we generated some strong mutants
as well as our corresponding weak ones for the replace program.
We selected only the replace program as our purpose here is to
demonstratetherelativedifferencesoftheapproaches:replaceis
one of the largest program from the Siemens suite, for which TCE
performsbestwithrespecttoequivalentmutantdetection[ 31].Our
results show that among the 1,579 mutants involved, our approach
detected 103 (7%) as infeasible, while TCE detected 96 (6%). Among
these, 91 are shared, which means that 12 of the infeasible mutants
were only found by our approach and 5 only by TCE. Regarding
duplicated mutants, our approach detected 555 (35%) duplicates,and TCE detected 352 (22%). 214 were shared, which means that
both techniques together identify 693 (44%) duplicated mutants.
Conclusion: Overall, the results show that our approach outper-
forms TCE in terms of detection power and form a relatively good
complementofit.Moreover,LCleanisabletodetectsubsumption.Yet,
TCE is much more efficient, relying on compiler optimizations.
7 DISCUSSION
7.1 Threats to Validity
Common to all studies relying on empirical data, this one may
beoflimitedgeneralizability.Todiminishthisthreatweused,in
additiontotheSiemensbenchmarkprograms,fourlargereal-world
applications composed of more than 200 kloc (in total), like SQLite,
and showed that our approach is capable of dealing with many
types of polluting objectives, which no other approach can handle.
Ourresultsmightalsohavebeenaffectedbythechoiceofthe
chosentestcriteriaandinparticularthespecificmutationoperators
we employ. To reduce this threat, we used popular test criteria
(CC, MCC, GACC and WM) included in software testing standards
[55,56],andemployedcommonlyusedmutationoperatorsincluded
in recent work [1, 16].
463
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Time to Clean Your Test Objectives ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
CriterionDynamic Detection LClean
possibly possibly possibly total ratio fortimemarked as marked as marked as total ratio fortimeinfeasible duplicate subsumed possibly polluting infeasible duplicate subsumed marked as polluting
CC 37/654 243/654 230/654 80% (510/654) 3132s 0/654 2/654 41/654 7% (43/654) 156s
MCC 76/666 221/666 215/666 77% (512/666) 3142s 20/666 0/666 16/666 5% (36/666) 154s
GACC 46/654 249/654 212/654 78% (507/654) 3134s 1/654 18/654 17/654 6% (36/654) 153s
WM 386/3543 2327/3543 641/3543 95% (3354/3543) 8399s 37/3543 257/3543 336/3543 18% (630/3543) 963s
TOTAL 545/5517 3040/5517 1298/5517 89%(4883/5517)4h56m4758/5517 277/5517 410/5517 14%(745/5517)23m46s
(×12 )( ×1 )
Figure 9: Dynamic detection of (likely) polluting objectives vs. LClean (Siemens)
Thevalidityofourexperimentalresultshavebeencrosschecked
in several ways. First, we compared our results on the Siemens
benchmarkwiththoseofothertools,namelyLUncovandTCE.We
knewbydesignthatinfeasibleobjectivesdetectedbyLCleanshould
bedetectedbyLUncovaswell,andwecheckedmanuallythestatusof each duplicate objective reported by LClean and not by TCE. Noissuewasreported.Second,weusedtheexistingtestssuitesfortheSiemensprogramsasaredundantsanitycheck,byverifyingthatev-ery objective reported as infeasible (resp. duplicated, subsumed) by
LCleanwasindeedseenasinfeasible(resp.duplicated,subsumed)
whenrunningthetestsuite.Thesetestsuitesareextremelythor-
ough [30,51] andare thuslikely todetect errorsin LClean.Third,
forlargerprograms,wepickedarandomselectionofahundredtestobjectivesreportedasinfeasible,duplicatedorsubsumedbyLClean
andmanuallycheckedthem–thiswasoftenstraightforwarddue
to the local reasoning of LClean. All these sanity checks succeeded.
Anotherclassofthreatsmayarisebecauseofthetoolsthatwe
used,asitislikelythatFrama-Corourimplementationaredefective.
However, Frama-C is a mature tool with industrial applications in
highlydemandingfields(e.g.,aeronautics)andthus,itisunlikely
tocauseimportantproblems.Moreover,oursanitycheckswould
have likely spotted such issues.
Finally,otherthreatsmaybeduetothepollutingnatureofthe
objectives that we target. However, infeasible objectives are a well-
knownissue,usuallyacknowledgedintheliteratureasoneofthe
mosttimeconsumingtasksofthesoftwaretestingprocess[ 2,38,53,
58], and redundant objectives have been stated as a major problem
in both past and recent literature [37, 38, 52].
7.2 Limitations
Labels cannot address all white-box criteria. For example, dataflow
criteria or full MCDC require additional expressive power [ 41].
Currently, parts of the infeasibility results from LClean could be
lifted to these classes of objectives. On the other hand, it is unclear
how it could be done for redundancy. Extending the present work
to these criteria is an interesting future work direction.
From a more technical point of view, the detection of subsump-
tionislimitedmoreorlesstobasicblocks.Whileitalreadyenables
catching many cases, it might be possible to slightly extend the
searchwhile retainingscalability.Inthe samevein,theproofs are
performedinLCleanona perfunctionbasis.Thisisaproblemas
it is often the case that a given function is always called within
thesamecontext,reducingitspossiblebehaviors.Allowingalim-
ited degree of contextual analysis (e.g., inlining function callers
and/or callees) should allow to detect more polluting objectives
while retaining scalability.
Finally, as we arefacing an undecidable problem, our approach
issound,butnotcomplete:SMTsolversmightanswer unknown.In
that case, we may miss polluting objectives.8 RELATED WORK
8.1 Infeasible Structural Objectives
Early research studies set the basis for identifying infeasible test
objectives using constraint-based techniques [ 24,48]. Offutt and
Pan[48]suggestedtransformingtheprogramsundertestasaset
of constraints that encode the test objectives. Then, by solving
theseconstraints,itispossibletoidentifyinfeasibleobjectives(con-straintswithnosolution)andtestinputs.Otherattemptsusemodel
checking [ 14,15] to prove that specific structural test objectives
(givenasproperties)areinfeasible.Unfortunately,constraint-based
techniques, as they require a complete program analysis, have the
usual problems of the large (possibly infinite) numbers of involved
paths, imprecise handling of program aliases [ 35] and the handling
of non-linear constraints [ 3]. Model-checking faces precision prob-
lems because of the system modelling and scalability issues due
tothelargestatespaceinvolved.Ontheotherhand,werelyona
modular, hence not too expensive, form of weakest precondition
calculus to ensure scalability.
Perhaps the closest works to ours are the ones by Beckman et
al.[12],Baluda etal.[4–6]andBardin etal.[8]thatrelyonweakest
precondition. Beckman et al.proves infeasible program statements,
Baludaetal.infeasibleprogrambranchesandBardin etal.infeasible
structural test objectives. Apart from the side differences (Beck-manet al.targets formal verification, Baluda et al.applies model
refinementincombinationtoweakestpreconditionandBardin et
al.combines weakest precondition with abstract interpretation)
with these works, our main objective here is to identify all types of
polluting test objectives (not only infeasible ones) for real-worldprograms in a generic way, i.e. for most of the test criteria, in-cluding advanced ones such as multiple condition coverage and
weak mutation. Another concern regards the scalability of the pre-
viousmethods,whichremainsunknownunderthecombinatorial
explosion of test objectives that mutation criteria introduce.
Othertechniquesattempttocombineinfeasibletestobjectives
detectiontechniquesasameanstospeed-uptestgenerationand
refine the coverage metric. Su et al.[61] combines symbolic execu-
tion with model checking to generate data flow test inputs. Baluda
et al.[6] combines backward (using weakest precondition) and for-
ward symbolic analysis to support branch testing and Bardin et al.
[8,9]combinesweakestpreconditionwithdynamicsymbolicexecu-
tion to support the coverage of structural test objectives. Although
integrating such approaches with ours may result in additional
benefits,ourmainobjectivehereistodemonstratethatlightweightsymbolicanalysistechniques,suchasweakestprecondition,canbeusedtoeffectivelytacklethegeneralproblemofpollutingobjectives
for almost all structural test criteria in real-world settings.
Anotherlineofresearchattemptsdiminishingtheundesirable
effects of infeasible paths in order to speed-up test generation.
464
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden M. Marcozzi, S. Bardin, N. Kosmatov, M. Papadakis et al.
Woodward etal.[64]suggested usingsomestatic rulescalledalle-
gationstoidentifyinfeasiblepaths.PapadakisandMalevris[ 54]and
Lapierreetal.[39]usedaheuristicbasedonthek-shortestpaths
in order to select likely feasible paths. Ngo and Tan [ 45] proposed
someexecutiontracepatterns thatwitnesslikelyinfeasiblepaths.
Delahaye et al.[19] showed that infeasibility is caused by the same
reason for many paths and thus, devised a technique that given an
infeasiblepathcanidentifyother,potentiallyunexploredpaths.All
thesemethodsindirectlysupporttestgenerationandcontraryto
ours do not detect polluting test objectives.
8.2 Equivalent Mutants
Automaticallydeterminingmutantequivalenceisaninstanceofthe
infeasibility problem and is undecidable [ 48]. There are numerous
propositionsonhowtohandlethis problem,ho wevermost ofthem
have only been evaluated on example programs and thus, their ap-
plicability and effectiveness remains unexplored [ 31]. Due to space
constraints we discuss the most recent and relevant approaches.
Detailsregardingtheolderstudiescanbefoundintherecentpaper
by Kintis et al.[31], which extensively covers the topic.
One of the most recent methods is the Trivial Compiler Op-
timization (TCE) [ 31,53]. The method assumes that equivalent
mutantinstancescanbeidentifiedbycomparingtheobjectcode
of the mutants. The approach works well (it can identify 30% of
the equivalent mutants) as the compiler optimisations turn mutant
equivalenciesintothesameobjectcode.Incontrastourapproach
usesstate-of-the-artverificationtechnologies(insteadofcompilers)
and targets all types of polluting objectives.
Alternative to static heuristics are the dynamic ones. Grun et al.
[26] and Schuler et al.[57] suggested measuring the impact of mu-
tants on the program execution and program invariants in order to
identify likely killable mutants. Schuler and Zeller [ 58] investigate
alargenumberofcandidateimpactmeasuresandfoundthatcover-
agewasthemostappropriate.AlongthesamelinesKintis etal.[33]
foundthathigherordermutantsprovidemoreaccuratepredictions
thancoverage.Overall,theseapproachesareunsound(theyprovidemanyfalsepositives)andtheydependontheunderlyingtestsuites.
In contrast our approach is sound and static.
8.3 Duplicate and Subsumed Test Objectives
The problems caused by subsumed objectives have been identified
a long time ago. Chusho introduced essential branches [ 17], or
non-dominated branches [ 13], as a way to prevent the inflation of
the branch coverage score causedby redundant branches. He also
introduced a technique devising graph dominator analysis in order
to identify the essential branches. Bertolino and Marré [ 13] also
usedgraph dominatoranalysis toreduce thenumber oftest cases
needed to cover test objectives and to help estimate the remaining
testingcost.Althoughtheseapproachesidentifytheharmfuleffects
of redundant objectives, they rely on graph analysis, which results
in a large number of false positives. Additionally, they cannot deal
with infeasible objectives.
In the context of mutation testing, Kintis et al.[32] identified
the problem and showed that mutant cost reduction techniquesperform well when using all mutants but not when using non-
redundantones.Amman etal.[1]introducedminimalmutantsanddynamic mutant subsumption and showed that mutation testing
tools generate a large number of subsumed mutants.
Althoughmutantredundancieswereknownfromtheearlydays
ofmutationtesting[ 37],theirharmfuleffectswereonlyrecently
realised. Papadakis et al.[52] performed a large-scale study and
demonstrated that subsumed mutants inflate the mutation score
measurement. Overall, Papadakis et al.[52] showed that arbitrary
experiments can result in different conclusions when they account
forthecofoundingeffectsofsubsumedmutants.Similarly,Kurtz
et al.[37,38] compared selective mutation testing strategies and
found that they perform poorly when the mutation score is free of
redundant mutants.
Overall, most of the studies identify the problem but fail to deal
with it. One attempt to reduce mutant redundancies uses TCE [ 31,
53] to removeduplicate mutants. Other attemptsare due to Kurtz
etal.[36]whodeviseddifferentialsymbolicexecutiontoidentify
subsumedmutants.Gong etal.[25]useddominatoranalysis(inthe
contextofweakmutation)inordertoreducethenumberofmutants.
Unfortunately,bothstudieshavelimitedscopeastheyhavebeen
evaluated only on example programs and their applicability andscalability remain unknown. Conversely, TCE is applicable andscalable, but it only targets specific kinds of subsumed mutants
(duplicatedones)andcannotbeappliedonstructuraltestobjectives.
9 CONCLUSION
Softwaretesting istheprimary methodfordetecting softwarede-
fects.Inthatcontext,pollutingtestobjectivesarewell-knownto
beharmfultothetestingprocess,potentiallywastingthetester’s
effortsandmisleadingthemonthequalityoftheirtestsuites.We
have presented LClean, the only approach to date that handles in a
unifiedwaythedetectionof(thethreekindsof)pollutingobjectives
for a large set of common criteria, together with a dedicated (open-
source) tool able to prune out such polluting objectives. LClean
reducestheproblemofdetectingpollutingobjectivestotheproblemofprovingassertionsinthetestedcode.Thetoolreliesonweakest-precondition calculusand SMT solving toprove theseassertions. It
is built on top of the industry-proof Frama-C verification platform,
specificallytuned toour scalabilityneeds. Experimentsshowthat
LClean provides a useful, sound, scalable and adaptable means for
helping testers to target high levels of coverage (where most faults
are detected) and to evaluate more accurately the strength of their
testsuites(aswellasofthetoolspossiblyusedtogeneratethem).
It could immediately benefit to all application developers that aim
atspecifictestsuitecoveragelevelsintheircurrenttestingprocess,
likeforexampleinthewell-knownSQLitedatabasemanagement
system.Apromisingdirectionforfutureworkistheextensionof
LCleanto thefewremainingunsupportedclasses oftestobjectives,
like data-flow criteria.
ACKNOWLEDGMENTS
WethanktheanonymousreviewersofICSE’2018fortheirvaluable
comments and remarks.
465
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Time to Clean Your Test Objectives ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]Paul Ammann, Márcio Eduardo Delamaro, and Jeff Offutt. 2014. Establishing
TheoreticalMinimalSetsofMutants.In SeventhIEEEInternationalConference
on Software Testing, Verification and Validation, ICST 2014, March 31 2014-April 4,
2014, Cleveland, Ohio, USA. 21–30.
[2]Paul Ammann and Jeff Offutt. 2008. Introduction to Software Testing (1 ed.).
Cambridge University Press.
[3]Saswat Anand, Edmund K. Burke, Tsong Yueh Chen, John A. Clark, Myra B. Co-
hen, Wolfgang Grieskamp, Mark Harman, Mary Jean Harrold, and Phil McMinn.
2013. An orchestrated survey of methodologies for automated software test case
generation. Journal of Systems and Software 86, 8 (2013), 1978–2001.
[4] Mauro Baluda, Pietro Braione, Giovanni Denaro, and Mauro Pezzè. 2010. Struc-
turalcoverageoffeasiblecode.In The5thWorkshoponAutomationofSoftware
Test, AST 2010, May 3-4, 2010, Cape Town, South Africa. 59–66.
[5]Mauro Baluda, Pietro Braione, Giovanni Denaro, and Mauro Pezzè. 2011. En-
hancingstructuralsoftwarecoveragebyincrementallycomputingbranchexe-
cutability. Software Quality Journal 19, 4 (2011), 725–751.
[6]Mauro Baluda, Giovanni Denaro, and Mauro Pezzè. 2016. Bidirectional Symbolic
Analysis for Effective Branch Testing. IEEE Trans. Software Eng. 42, 5 (2016),
403–426.
[7]SébastienBardin,OmarChebaro,MickaëlDelahaye,andNikolaiKosmatov.2014.
An All-in-One Toolkit for Automated White-Box Testing. In TAP. Springer.
[8]SébastienBardin,MickaëlDelahaye,RobinDavid,NikolaiKosmatov,MikePa-
padakis, Yves Le Traon, and Jean-Yves Marion. 2015. Sound and Quasi-Complete
Detection of Infeasible Test Requirements. In ICST.
[9]SébastienBardin,NikolaiKosmatov,andFrançoisCheynier.2014. EfficientLever-
aging of Symbolic Execution to Advanced Coverage Criteria. In Software Testing,
Verification and Validation (ICST), 2014 IEEE Seventh International Conference on
(pp. 173-182). IEEE.
[10]Mike Barnett and Rustan Leino. 2005. Weakest-Precondition of Unstructured
Programs. In ACM SIGPLAN-SIGSOFT workshop on Program analysis for software
tools and engineering (PASTE). 82–87.
[11]Patrick Baudin, Pascal Cuoq, Jean C. Filliâtre, Claude Marché, Benjamin Monate,
Yannick Moy, and Virgile Prevosto. [n. d.]. ACSL: ANSI/ISO C Specification
Language. http://frama-c.com/acsl.html
[12]NelsE.Beckman,AdityaV.Nori,SriramK.Rajamani,RobertJ.Simmons,SaiDeep
Tetali, and Aditya V. Thakur. 2010. Proofs from Tests. IEEE Trans. Software Eng.
36, 4 (2010), 495–508.
[13]Antonia Bertolino and Martina Marré. 1994. Automatic Generation of Path
Covers Based on the Control Flow Analysis of Computer Programs. IEEE Trans.
Software Eng. 20, 12 (1994), 885–899.
[14]DirkBeyer,AdamChlipala,ThomasA.Henzinger,RanjitJhala,andRupakMa-
jumdar. 2004. Generating Tests from Counterexamples. In 26th International
ConferenceonSoftwareEngineering(ICSE2004),23-28May2004,Edinburgh,United
Kingdom. 326–335.
[15]Dirk Beyer, Thomas A. Henzinger, Ranjit Jhala, and Rupak Majumdar. 2007. The
software model checker Blast. STTT9, 5-6 (2007), 505–525.
[16]Thierry Titcheu Chekam, Mike Papadakis, Yves Le Traon, and Mark Harman.
2017. An empirical study on mutation, statement and branch coverage fault
revelationthatavoidstheunreliablecleanprogramassumption.In Proceedings
ofthe39thInternationalConferenceonSoftwareEngineering,ICSE2017,Buenos
Aires, Argentina, May 20-28, 2017. 597–608.
[17]Takeshi Chusho. 1987. Test Data Selection and Quality Estimation Based on the
Concept of Esssential Branches for Path Testing. IEEE Trans. Software Eng. 13, 5
(1987), 509–517.
[18]LeonardoDeMouraandNikolajBjørner.2011. SatisfiabilityModuloTheories:
Introduction and Applications. Commun. ACM 54, 9 (Sept. 2011), 69–77.
[19]Mickaël Delahaye, Bernard Botella, and Arnaud Gotlieb. 2015. Infeasible path
generalizationindynamicsymbolicexecution. Information&SoftwareTechnology
58 (2015), 403–418.
[20]RichardA.DeMilloandA.JeffersonOffutt.1991. Constraint-BasedAutomatic
Test Data Generation. IEEE Trans. Software Eng. 17, 9 (1991), 900–910.
[21] E. W. Dijkstra. 1976. A Discipline of Programming. Prentice Hall.
[22]PhyllisG.FranklandOlegIakounenko.1998. FurtherEmpiricalStudiesofTest
Effectiveness.In Proceedingsofthe6thACMSIGSOFTInternationalSymposium
onFoundationsofSoftwareEngineering(SIGSOFT’98/FSE-6).ACM,NewYork,NY,
USA, 153–162.
[23]MilosGligoric,AlexGroce,ChaoqiangZhang,RohanSharma,MohammadAmin
Alipour, and Darko Marinov. 2015. Guidelines for Coverage-Based Comparisons
of Non-Adequate Test Suites. ACM Trans. Softw. Eng. Methodol. 24, 4 (2015),
22:1–22:33.
[24]Allen Goldberg, Tie-Cheng Wang, and David Zimmerman. 1994. Applications of
FeasiblePathAnalysistoProgramTesting.In Proceedingsofthe1994International
SymposiumonSoftwareTestingandAnalysis,ISSTA1994,Seattle,WA,USA,August
17-19, 1994. 80–94.
[25]DunweiGong,GongjieZhang, XiangjuanYao,andFanlinMeng. 2017. Mutant
reductionbasedondominancerelationforweakmutationtesting. Information&Software Technology 81 (2017), 82–96.
[26]Bernhard J. M. Grün, David Schuler, and Andreas Zeller. 2009. The Impact
of Equivalent Mutants. In Second International Conference on Software Testing
Verification and Validation, ICST 2009, Denver, Colorado, USA, April 1-4, 2009,
Workshops Proceedings. 192–199.
[27]GZip(SPEC)2018. https://www.spec.org/cpu2000/CINT2000/164.gzip/docs/164.
gzip.html. (2018).
[28]C. A. R.Hoare. 1969. Anaxiomatic basis for computerprogramming. Commun.
ACM12, 10 (October 1969), 576–580 and 583.
[29]MonicaHutchins,HerbFoster,TarakGoradia,andThomasOstrand.1994. Exper-
iments of the Effectiveness of Dataflow- and Controlflow-based Test Adequacy
Criteria. In Proceedings of the 16th International Conference on Software Engineer-
ing (ICSE ’94). IEEE Computer Society Press, Los Alamitos, CA, USA, 191–200.
[30]MonicaHutchins,HerbertFoster,TarakGoradia,andThomasJ.Ostrand.1994.
ExperimentsoftheEffectivenessofDataflow-andControlflow-BasedTestAd-
equacyCriteria.In Proceedingsofthe16thInternationalConferenceonSoftware
Engineering. 191–200.
[31]M.Kintis,M.Papadakis,Y.Jia,N.Malevris,Y.LeTraon,andM.Harman.2017.
Detecting Trivial Mutant Equivalences via Compiler Optimisations. IEEE Trans-
actions on Software Engineering PP, 99 (2017), 1–1.
[32]Marinos Kintis, Mike Papadakis, and Nicos Malevris. 2010. Evaluating Mutation
TestingAlternatives:ACollateralExperiment.In 17thAsiaPacificSoftwareEn-
gineeringConference, APSEC2010,Sydney,Australia,November 30-December 3,
2010. 300–309.
[33]MarinosKintis,MikePapadakis,andNicosMalevris.2015. Employingsecond-
order mutation for isolating first-order equivalent mutants. Softw. Test., Verif.
Reliab.25, 5-7 (2015), 508–535.
[34]Florent Kirchner, Nikolai Kosmatov, Virgile Prevosto, Julien Signoles, and Boris
Yakobowski. 2015. Frama-C: A Program Analysis Perspective. Formal Aspects of
Computing Journal (2015).
[35]NikolaiKosmatov.2008. All-PathsTestGenerationforProgramswithInternal
Aliases.In 19thInternationalSymposiumonSoftwareReliabilityEngineering(ISSRE
2008), 11-14 November 2008, Seattle/Redmond, WA, USA. 147–156.
[36]Bob Kurtz, Paul Ammann, and Jeff Offutt. 2015. Static analysis of mutant sub-
sumption.In EighthIEEEInternationalConferenceonSoftwareTesting,Verification
and Validation, ICST 2015 Workshops, Graz, Austria, April 13-17, 2015. 1–10.
[37]BobKurtz,PaulAmmann,JeffOffutt,MárcioEduardoDelamaro,MarietKurtz,
andNidaGökçe.2016. Analyzingthevalidityofselectivemutationwithdomina-tormutants.In Proceedingsofthe24thACMSIGSOFTInternationalSymposiumon
Foundations of Software Engineering, FSE 2016, Seattle, WA, USA, November 13-18,
2016. 571–582.
[38]BobKurtz,PaulAmmann,JeffOffutt,andMarietKurtz.2016. AreWeThereYet?
HowRedundantandEquivalentMutantsAffectDeterminationofTestComplete-
ness. InNinth IEEE International Conference on Software Testing, Verification and
ValidationWorkshops,ICSTWorkshops2016,Chicago,IL,USA,April11-15,2016.
142–151.
[39]SébastienLapierre,EttoreMerlo,GillesSavard,GiulianoAntoniol,RobertoFi-
utem, and Paolo Tonella. 1999. Automatic Unit Test Data Generation Using
Mixed-IntegerLinearProgrammingandExecutionTrees.In 1999International
Conference on Software Maintenance, ICSM 1999, Oxford, England, UK, August 30 -
September 3, 1999. 189–198.
[40]MichaëlMarcozzi,SébastienBardin,MickaëlDelahaye,NikolaiKosmatov,and
VirgilePrevosto. 2017. TamingCoverageCriteriaHeterogeneitywithLTest.In
2017IEEEInternationalConferenceonSoftwareTesting,VerificationandValidation,
ICST 2017, Tokyo, Japan, March 13-17, 2017. 500–507.
[41]MichaëlMarcozzi,MickaëlDelahaye,SébastienBardin,NikolaiKosmatov,and
Virgile Prevosto. 2017. Generic and Effective Specification of Structural Test
Objectives. In 2017 IEEE International Conference on Software Testing, Verification
and Validation, ICST 2017, Tokyo, Japan, March 13-17, 2017. 436–441.
[42] Aditya P. Mathur. 2008. Foundations of Software Testing. Addison-Wesley Prof.
[43]Glenford J. Myers and Corey Sandler. 2004. The Art of Software Testing. John
Wiley & Sons.
[44]Glenford J. Myers, Corey Sandler, and Tom Badgett. 2011. The Art of Software
Testing(3 ed.). Wiley.
[45]Minh Ngoc Ngo and Hee Beng Kuan Tan. 2008. Heuristics-based infeasible path
detectionfordynamictestdatageneration. Information&SoftwareTechnology
50, 7-8 (2008), 641–655.
[46]A.JeffersonOffutt,AmmeiLee,GreggRothermel,RolandH.Untch,andChristian
Zapf.1996. AnExperimentalDeterminationofSufficientMutantOperators. ACM
Trans. Softw. Eng. Methodol. 5, 2 (April 1996), 99–118.
[47]A.JeffersonOffuttandStephenD.Lee.1994. AnEmpiricalEvaluationofWeak
Mutation. IEEE Trans. Software Eng. 20, 5 (1994).
[48]A.JeffersonOffuttandJiePan.1997. AutomaticallyDetectingEquivalentMutants
and Infeasible Paths. Softw. Test., Verif. Reliab. 7, 3 (1997), 165–192.
[49] OpenSSL 2018. https://www.openssl.org. (2018).
[50]RahulPandita,TaoXie,NikolaiTillmann,andJonathandeHalleux.2010. Guided
Test Generation for Coverage Criteria. In ICSM.
466
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden M. Marcozzi, S. Bardin, N. Kosmatov, M. Papadakis et al.
[51]MikePapadakis,MárcioEduardoDelamaro,andYvesLeTraon.2014. Mitigat-
ingtheeffectsofequivalentmutantswithmutantclassificationstrategies. Sci.
Comput. Program. 95 (2014), 298–319.
[52]Mike Papadakis, Christopher Henard, Mark Harman, Yue Jia, and Yves Le Traon.
2016. Threats to the Validity of Mutation-based Test Assessment. In Proceedings
ofthe25thInternationalSymposiumonSoftwareTestingandAnalysis(ISSTA2016).
ACM, New York, NY, USA, 354–365.
[53]MikePapadakis,YueJia,MarkHarman,andYvesLeTraon.2015. TrivialCom-
piler Equivalence: A Large Scale Empirical Study of a Simple, Fast and Effective
Equivalent Mutant DetectionTechnique. In Proceedings of the37th International
Conference on Software Engineering - Volume 1 (ICSE ’15). IEEE Press, Piscataway,
NJ, USA, 936–946.
[54]MikePapadakis andNicos Malevris.2012. Mutationbasedtest casegeneration
via a path selection strategy. Information & Software Technology 54, 9 (2012),
915–932.
[55]RadioTechnicalCommissionforAeronautics.1992. RTCADO178-BSoftware
Considerations in Airborne Systems and Equipment Certification. (1992).
[56]StuartC.Reid.1995. TheSoftware TestingStandard—Howyoucanuseit.In
3rdEuropean Conference on Software Testing, Analysis and Review (EuroSTAR
’95). London.
[57]David Schuler, Valentin Dallmeier, and Andreas Zeller. 2009. Efficient mutation
testing by checking invariant violations. In Proceedings of the Eighteenth Interna-
tional Symposium on Software Testing and Analysis, ISSTA 2009, Chicago, IL, USA,
July 19-23, 2009. 69–80.
[58]DavidSchulerandAndreasZeller.2013. CoveringandUncoveringEquivalent
Mutants. Softw. Test., Verif. Reliab. 23, 5 (2013), 353–374.
[59]SJeng (SPEC) 2018. https://www.spec.org/cpu2006/Docs/458.sjeng.html. (2018).
[60] SQLite 2018. https://www.sqlite.org. (2018).
[61]TingSu,ZhoulaiFu,GeguangPu,JifengHe,andZhendongSu.2015. CombiningSymbolicExecutionandModelCheckingforDataFlowTesting.In 37thIEEE/ACM
International Conference on Software Engineering, ICSE 2015, Florence, Italy, May
16-24, 2015, Volume 1. 654–665.
[62]E. J. Weyuker. 1993. More Experience with Data Flow Testing. IEEE Trans. Softw.
Eng.19, 9 (Sept. 1993), 912–919.
[63]M. R. Woodward, D. Hedley, and M. A. Hennell. 1980. Experience with Path
Analysis and Testing of Programs. IEEE Trans. Softw. Eng. 6, 3 (May 1980), 278–
286.
[64]Martin R. Woodward, David Hedley, and Michael A. Hennell. 1980. Experience
withPathAnalysisandTestingofPrograms. IEEETrans.SoftwareEng. 6,3(1980),
278–286.
[65]D.YatesandN.Malevris.1989. ReducingtheEffectsofInfeasiblePathsinBranch
Testing. In Proceedings of the ACM SIGSOFT ’89 Third Symposium on Software
Testing, Analysis, and Verification (TAV3). ACM, New York, NY, USA, 48–54.
[66]Hong Zhu, Patrick A. V. Hall, and John H. R. May. 1997. Software Unit Test
Coverage and Adequacy. ACM Comput. Surv. 29, 4 (1997).
467
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. 