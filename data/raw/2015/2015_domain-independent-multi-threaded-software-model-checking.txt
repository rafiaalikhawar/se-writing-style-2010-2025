Domain-Independent
Multi-threaded Software Model Checking
Dirk Beyer
LMU Munich
GermanyKarlheinz Friedberger
LMU Munich
Germany
ABSTRACT
Recent developmentof softwareaims atmassively parallelexecu-
tion, because of the trend to increase the number of processing
units per CPU socket. But many approaches for program analy-
sis are not designed to benefit from a multi-threaded execution
andlacksupport toutilizemulti-corecomputers.Rewriting exist-
ingalgorithmsisdifficultanderror-prone,andthedesignofnew
parallel algorithms also has limitations. An orthogonal problem is
thegranularity:computingeachsuccessorstateinparallelseems
too fine-grained, so the open question is to find the right struc-
turallevelforparallelexecution.Weproposeanelegantsolutionto
theseproblems:Blocksummariesshouldbecomputedinparallel.
Many successful approaches to software verification are based on
summaries of control-flow blocks, large blocks, or function bodies.
Block-abstractionmemoizationisasuccessfuldomain-independent
approach for summary-based program analysis. We redesigned the
verification approach of block-abstraction memoization starting
fromitsoriginalrecursivedefinition,suchthatitcanruninaparal-
lelmannerforutilizingtheavailablecomputationresourceswithout
losing its advantages of being independent from a certain abstract
domain. We present an implementation of our new approach for
multi-core shared-memory machines. The experimental evaluation
showsthatoursummary-basedapproachhasnosignificantover-
head compared to the existingsequential approach and that it has
a significant speedup when using multi-threading.
CCS CONCEPTS
•Software and its engineering →Formal software verifica-
tion;•Theory of computation →Parallel algorithms ;
KEYWORDS
Program Analysis, Software Verification, Parallel Algorithm, Multi-
threading, Block-Abstraction Memoization
ACM Reference Format:
Dirk Beyer and Karlheinz Friedberger. 2018. Domain-Independent Multi-
threadedSoftwareModelChecking.In Proceedingsofthe201833rdACM/IEEE
International Conference on Automated Software Engineering (ASE ’18), Sep-
tember3–7,2018,Montpellier,France. ACM,NewYork,NY,USA, 11pages.
https://doi.org/10.1145/3238147.3238195
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.32381951 INTRODUCTION
Program verification hasbeen appliedsuccessfully to find errors
inapplicationsortoprovetheircorrectness.Recenthardwarede-
velopmentaimstowardsparallelexecutionofprogramseitheron
multi-coremachinesorsharedacrossseveralmachinesinacom-
puting cluster. For large-scale program verification, we do not only
need efficient algorithms, but also make use of available hardware
resourcesuptotheirlimits.Therearesomeapproachestoleverage
such systems, but most recent algorithms for program verification
and model checking are not designed to work in parallel man-
nerandutilizeonlyasmallpartofavailableresources.Thereare
several reasons for this: Either the verification algorithms have
dependencies between intermediate results, such that only a se-
quential execution is useful, or the amount of parallelism is bound
by a small number, e.g., only two analyses are executed in paral-lel and communicate information, effectively using only a small
numberofCPUcores.Themainquestioniswhetherandhowwe
can (re-)design existing verification techniques such that they can
be executed on parallel computer architectures.
We contributetheidea touse summariesasthe objectsto com-
pute in parallel, instead of inventing new parallel state-space itera-
tionalgorithms.Block-abstractionmemoization(BAM)[ 31]isapar-
ticularly nice method to summarize blocks of program statements,
because it is independent from a particular analysis — it wrapsan existing analysis without much interference and stores block
summaries in a cache. We use this concept to develop a domain-
independent analysis that distributes a verification problem across
multipleprocessingunitswithoutchangestotheanalysistechnique.
Our analysis is based on a standard state-space exploration using a
control-flowautomatonthatrepresentstheprogram.Theapproach
is orthogonal to other data-flow-based analyses, and thus, it can be
combinedwithanalysesbasedondifferentabstractdomainslike
BDDs, explicit values, intervals, or predicates.
Thevalueofourapproachisitslevelof separationofconcerns :it
separatestheconcernofmakingananalysismulti-threadedfrom
the concern of designing and implementing an abstract domain
and its operators.Webase our approach onBAM and use most of
itsdatastructures,suchthatmostpartsofthe(wrapped)analysis
(and its implementation) remain unchanged. We redesigned the
algorithm such that we can efficiently execute it across severalprocessing units. The parallelism of the analysis is only bound
bythe structureof theprogram tobe analyzedandthe amountof
workfoundduringtheanalysis.Ourworkincludesatransformation
of the existing algorithm of BAM from a sequential, recursively
definedalgorithmintoaparallelapproach.Additionally,webenefit
fromtheexistinginfrastructureofBAM,i.e.,wealsouseacache
forblockabstractionsandapplytheoperators reduceandexpand
to increase the cache hit rate. The analysis is sound, implemented
634
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Dirk Beyer and Karlheinz Friedberger
in the open-source verification framework CPAchecker , and can be
combinedwithexistingcomponentsoftheframework,including
CEGAR [ 20] or witness export [ 5,6].
Contributions. We make the following contributions:
•We introduce a new technique for parallelization of verifica-
tion algorithms that is independent from particular abstract
domainsbecauseitisbasedonaflexibleandconfigurableblock
summarization.
•We implemented the technique in the open-source verification
framework CPAchecker .Ourimplementationandallexperimen-
tal data are available to other researchers and practitioners for
replication via our artifact [9] and supplementary website.1
•We evaluated our new technique on a large set of benchmarks
andshow(1)thattheparallelversionofBAM(ifusingonlyone
CPU core) behaves similar to the sequential version (i.e., there
isnosignificantoverheadforparallelization)and(2)thatthe
parallel version of BAM significantly improves the responsetime of the verification process for programs that are large
enough to benefit from multi-threading.
Related Work. The idea to use parallel algorithms in software
verification is not new. There exist several approaches reaching
from plain parallel execution of different algorithms (until the first
analysis succeeds) via one-way communication between (some)
analyses (one analysis provides additional information for another
one) to fully parallel analyses (dividing the state space into par-titions that are explored separately).Portfolio Approaches. A simple, but effective approach is to run
a portfolio analysis [
24], i.e., a fixed number of predefined anal-
yses in parallel to leverage the available CPU cores on a single
machine,suchthattheverifierterminateswiththefirstsucceeding
analysis (e.g. [ 22,26]). This strategy is applied either to separately
explore the state space, e.g., with different domains, or in a way
thatoneanalysisprovidesinformationforanotherone,forexam-
ple to enrich it with additional invariants [ 7]. Such approaches
for parallel software verification are not scalable due to its fixednumber of different analyses, and they suffer from the problem
that each single analysis only uses a small fraction of the available
resources. If all but one analysis fail to determine a verificationresult (because of unsupported features in the task, or impreci-
sionoftheanalysis),theremainingworkissometimeslimitedto
a single analysis and thus a single core.
Multi-ThreadingApproaches. SPIN[23]andDivine[3,29]arebased
on pure explicit model checking and use a central hash table to
check for existing (already analyzed) states. LTSmin[18] either per-
forms explicit state-space search in a parallel manner or uses a
BDD-based approach using the BDD-library Sylvan[30] that inter-
nallyparallelizesitsoperations.Otherapproachesdivideagiven
problemintosmallercomponentsthatareverifiedseparately,be-
fore joining the results to get a proof for a whole program [ 21,25].
Anexampleimplementationforsuchatechniqueisthetool Soft-
Verthat uses BDDs and predicates.
Structurally-definedconditionalanalysis[ 28]isanapproachthat
splits a program according to conditions as in conditional modelchecking [
11], that is, given a program Pand a condition ψ,t w o
1https://www.sosy-lab.org/research/bam-parallel/analysis instances can be created, one conditional analysis of P
andψand one conditional analysis of Pand¬ψ. The two anal-
ysis instances are completely independent and can be executedin parallel. The approach can scale up to an arbitrary number of
splits. The elegance of this approach is that it does not depend
on a specific implementation but can be built on top of existing,off-the-shelf tool components.Multi-Machine Approaches. State-space exploration can be dis-
tributed across several machines by partitioning the possible state-
space. Tools like SPIN[23],CSeq-Swarm [27], or the SPARK Analysis
Tools[19] divide the verification problem after a short pre-analysis
of the program, and split the potential state space and the verifi-
cation condition according to given time and memory limitations,
available processing units, or other criteria. This approach is po-
tentiallyproblematicduetotheunknownnatureoftheprogram
to be analyzed, e.g., it might not match the pre-defined schedul-ing. For degenerated state spaces, the parallel analysis might be
imbalancedbetweendifferentthreads/processes.Othertoolslike
LTSmin[18]o rDivine[3,29] circumvent such imbalances by a dy-
namic scheduling approach. The approach of structurally-defined
conditional analysis [ 28] can also be extended to benefit from
multi-machine environments.
Our contribution is a more general parallel technique for pro-
gram analysis and can be applied to an arbitrary domain and even
combinations of several domains. Thus, explicit-value analysis,
BDD-based analysis, as well as predicate analysis can benefit from
our approach. The parallelism of the approach presented in this
paperisbasedontheinternalstructureoftheprogram,i.e.,anauto-
matic partitioning of the control flow, and tries to use all available
processingunits,onlydependingonthedynamicbehaviorofthe
programanalysis,i.e.,theunfoldingoftheabstractstatespace.
2 BACKGROUND
Thefollowingsectionprovidesanoverviewofbasicconceptsand
definitions that our approach is based on. We describe the pro-
gram representation, configurable program analysis, the details of
block-abstractionmemoization,andhowweadvancedittowards
anefficientparallelalgorithmforprogramanalysis(formorede-tail see the original articles [
12,31]).
2.1 Program Representation
Werestrictthepresentationtoasimpleimperativeprogramming
language, where all operations are either assignment or assume
operations.Aprogramisrepresentedbya control-flowautomaton
(CFA)A=(L,l0,G), which is a directed graph consisting of a set
Lof program locations (modeling the program counter), a set G⊆
L×Ops×Lofcontrol-flowedges(modelingthecomputationsteps
fromonelocationtothenext:assignmentorassumeoperations),
and an initial program location l0(entry point of the program).
2.2 CPA and CPA Algorithm
Aconfigurable program analysis (CPA) [12] is specified by an ab-
stractdomainforaprogramanalysisandoperatorstomodelthe
behavior of the program analysis: A CPA D=(D,/leadsto,merge ,stop )
consists of
635
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. Domain-Independent Multi-threaded Software Model Checking ASE ’18, September 3–7, 2018, Montpellier, France
(1)an abstract domain D=(C,E,[[·]])that consists of a set Cof
concrete states, a lattice E=(E,/subsetsqequal)over a set Eof abstract-
domainelements(i.e.,abstractstates)andapartialorder /subsetsqequal,and
a concretizationfunction [[ ·]] that mapseach abstract-domain
element to the represented set of concrete states.
(2)atransferrelation /leadsto⊆E×Ethatyieldssuccessorsofanabstract
state.
(3)amergeoperator merge⊆E×E→Ethatdetermineshowto
merge two abstract states when control flow meets).
(4)a termination check stop⊆E×2E→Bthat specifies whether
an abstract state is covered by a set of abstract states.
Algorithm 1CPAalgperforms a state-space exploration. It com-
putesanoverapproximationofthereachablestatesbyconstructing
abstract states for the program based on a given CPA and an initialabstractstate.Thealgorithmisafixed-pointiterationandmaintains
a setwaitlistof abstract states that still have to be explored, and
asetreachedofalreadyexploredabstractstates.Ineachiteration,
thealgorithmtakesanabstractstatefrom waitlist(line2)andcom-
putes itssuccessors (line3). Thealgorithm checkswhether a new
statecanbemergedwithanexistingstate,andupdatesthework
setsaccordingly(lines5–8).Theoperator stopensuresthatthenew
abstract state is only added to the work sets if the abstract state is
notalreadycoveredbyanyoftheexistingstatesin reached(lines9–
11).Thealgorithmterminatesifeithertheset waitlistisemptyor
thereisanotherreasontoabortearly,e.g.,apropertyviolation.
We use a simplified version of algorithm CPAalg[8] in order
to shorten the presentation. The precision and precision adjust-ment, which determine the granularity of the analysis within a
CEGARloop,areneglectedinthisdescription,butfullyavailable
and supported in our implementation.
DifferentaspectsofaprogramareanalyzedbydifferentCPAs,
and compositions of CPAs allow more advanced analyses. CPAs
Algorithm 1 CPAalg(D, reached,waitlist), taken from [8]
Input:aC P AD =(D,/leadsto,merge ,stop ),
whereEdenotes the set of elements of the lattice of D,
a setreached⊆Eof abstract states,
a setwaitlist⊆reachedof frontier abstract states,
a function abort:E→Bthat defines whether the algorithm
should abort early
Output: the updated sets reachedandwaitlist
1:whilewaitlist /nequal∅do
2:pop(e)f r o mwaitlist
3:foreache/primewithe/leadstoe/primedo
4:foralle/prime/prime∈reacheddo
5: enew:=merge (e/prime,e/prime/prime)
6: ifenew/nequale/prime/primethen
7: reached:=reached∪{enew}\{e/prime/prime}
8: waitlist:=waitlist∪{enew}\{e/prime/prime}
9:if¬stop(e/prime,reached) then
10: reached:=reached∪{e/prime}
11: waitlist:=waitlist∪{e/prime}
12: ifabort (e/prime)then
13: return(reached, waitlist)
14:return(reached, waitlist)have been defined formany abstract domains, such as BDD-based
analysis[ 17],(explicitorsymbolic)valueanalysis[ 14,15],predicate
analysis[ 8,10,13],orcombinationthereof[ 2].Alsothetracking
of the program counter and of the call stack for procedures are
defined as CPAs. We will not go into detail for all their definitions
and descriptions here, because our approach works on an abstract
level and is independent from a specific domain. For our evalua-tion later, we use a value analysis that tracks variables and their
values explicitly, e.g., an abstract state is a (partial) function that
maps program variables to values.
2.3 BAM
Block-abstraction memoization (BAM) [ 31] is a modular approach
forreachabilityanalysisofabstractstategraphs(suchasabstract
modelsofprograms).Therefore,ittreatsalargeprogramasaset
ofblocks,and analyzestheblocksseparately. Theresultof ablock
analysis (the block abstraction ) of a nested block is embedded in
thesurroundingblock’sanalysis.Blockabstractionsarealsostored
inacacheforlaterreuseinordertoavoidrepeatedcomputationof the same block abstraction, to speed up the analysis. BAM de-
fines the two operators
reduceandexpandthat aim at a higher
cache hit rate. For simplicity we will neglect both operators inthe further description. They are orthogonal to the approach ofparallel analysis that we present here.
ThecomponentsofBAMaredefinedindetailinthefollowing:
2.3.1 Blocks. The basic components of BAM are blocks, which
are formally defined as parts of a program: A block B=(L/prime,G/prime)
o faC F A A= (L,l0,G)consists of a set L/prime⊆Lof connected
program locations and a set G/prime={(l1,op,l2)∈G|l1,l2∈L/prime}
of control-flow edges. Two different blocks B1= (L/prime
1,G/prime
1)and
B2= (L/prime
2,G/prime
2)are either disjoint (L/prime
1∩L/prime
2=∅) or one block
is completely nested in the other block ( L/prime
1⊂L/prime
2). Each block
B= (L/prime,G/prime)hasentryandexit locations, which are defined as
In(B)=/braceleftbigl∈L/prime|(∃(l/prime,op,l)∈G∧l/prime/nelementL/prime)∨∄(l/prime,op,l)∈G/bracerightbigand
Out (B)=/braceleftbigl∈L/prime|(∃(l,op,l/prime)∈G∧l/prime/nelementL/prime)∨∄(l,op,l/prime)∈G/bracerightbig,r e -
spectively.In general,theblock sizecanbe freelychosenin BAM.
Inmostcases,functionsandloopsareusedasblocksize,because
theyrepresentthelogicalstructureofaprogramandleadtonat-
ural block abstractions.
Figure1showsaschematicexampleofaCFAandhowitcouldbe
dividedintoblocks.Itdoesnotshowanyoperations;weomitdetailsfor ease of presentation. The largest block (denoted as
BA) consists
of all locations and represents the whole CFA of the program. The
other blocks (denoted as BBtoBF) are smaller and consists of
fewer locations. Block BFis nested in block BE, which in turn is
nestedinblock BA.Location3istheentrylocationofblock BB,i.e.,
In(BB)={3}, and location 4 is its exit location, i.e., Out (BB)={4}.
2.3.2 BAM-CPA. The basis of CPAchecker is the idea of config-
urableprogramanalysis.Thus,BAMisformalizedasaCPA BAM =
(DBAM ,/leadstoBAM ,mergeBAM ,stopBAM ).BAMworksonanabstract,
domain-independent level and uses an abstract-domain-dependent
wrappedanalysis(liketheBDD-based,explicitvalue,interval,or
predicateanalysis)totrackvariablesandvalues.Thiswrappedanal-
ysis is also given as CPA W=(DW,/leadstoW,mergeW,stopW), based
on which we now formalize BAM:
636
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Dirk Beyer and Karlheinz Friedberger
1
2
5
8
11
12
13
17
18
19
20
21
223
4
146
7
159
10
16BA
BB
BC
BD
BE
BF
Figure 1: Schematic control-flow automaton with blocks
(1) The domain DBAMwraps the domain DW.
(2)The transfer relation /leadstoBAMfor a block Bhas a transfer
s/leadstoBAMs/primefor two abstract states sands/primeif
s/prime∈⎧⎪⎪⎨⎪⎪⎩{s/prime/prime|sBsub/leadstoBAMs/prime/prime}ifl∈In(Bsub )// apply BAM to Bsub
{s/prime/prime|s/leadstoWs/prime/prime}ifl/nelementOut (B)// delegate to W
wherelis the program location of s.
Dependingonthecurrentlyanalyzedprogramlocation l,the
transfer relation chooses between two possible steps: For an
entrylocationofablock Bsub,theoperationBsub/leadstoBAMrepresents
the block abstraction for the block Bsuband the block-entry
abstract state s. The block abstraction is computed by a call
CPAalg (DBAM ,{s},{s}).Forexitlocationsofblocks,thereisno
succeedingabstractstate(intheanalysisofthecurrentblock
B). For other program locations, the wrapped transfer relation
/leadstoWis applied.
(3)Themergeoperator mergeBAM=mergeWandthetermination
checkstopBAM=stopWcorrespond to the wrapped analysis.
The performance of BAM can easily be increased by a cache
cache⊆(Blocks×E)→ (2E×2E),whichmapsablockandanentry
abstract state of the block to the set of reached abstract states and
thesetoffrontierstates.Thecacheisoptionalfortheapplication
ofBAM,butthememoizationofblockabstractionsimprovesthe
performance.Additionally,theoperators reduceandexpandcanbe
appliedforahighercachehitrate.Weignorethemforsimplicity.
2.4 Towards Parallel BAM
A simple state-space exploration that enumerates all reach-
able abstract states and only checks whether they were al-
ready part of the set reachedcan be done with the operators
mergesepandstopsep(defined as mergesep (e,e/prime):=eand
stopsep (e,R):=∃e/prime∈R:e/subsetsqequale/prime, or even with a simpler formstopsep (e,R):=∃e/prime∈R:e=e/prime). Well-known techniques for
explicit-statemodelchecking[ 3,23]usesuchanapproachtoan-
alyze the state space. This approach can be parallelized easily by
synchronizing the access to the existingabstract states in the sets
reachedandwaitlistandapplyingtheoperators /leadsto,mergesep,and
stopsepconcurrently. With lock-free implementations of the set
data structures for reachedandwaitlistthere is only minimal syn-
chronization necessary for an efficient analysis. However, whenusing more general (and possibly more expensive) operator in-stances, the complete sets
reachedandwaitlist(and also larger
parts of the CPA algorithm) would need to be locked to ensuresingle-thread access, which prevents an efficient parallel appli-cation of the algorithm.
Tocircumventthisproblem,ournewapproachdoesnotintro-
duce parallelismwithin the CPAalgalgorithm, butapplies several
independent CPAalginstances in parallel. Each CPAalginvoca-
tion is executed in a separate thread on its own part of the state
space, i.e., with its own sets reachedandwaitlistof abstract states,
such that there is only minimal communication between the algo-
rithminstances.Thenecessaryinfrastructureforsuchanapproach
is based on BAM. The previously given basic definition of BAM
leaves room for several implementation details, such that both (the
sequential and the parallel) implementation match the given spec-
ification. The computation and application of block abstractionscan be done in sequential or parallel manner.
3 PARALLEL BAM
Our contribution is a scalable parallelization of the sequential algo-
rithm of BAM. Block abstractions are independent from each otherandalsofromthesurroundingcontext.Thus,theycanbecomputedinparallel,assoonastheinitialabstractstateofablockabstraction
is known. The sequential version of BAM, which was defined by
WonischandWehrheim[ 31],recursivelycallsanotherCPAalgo-
rithmforeachnewlyenteredblock,waitsforitsterminationand
directly uses the result as a block abstraction of the entered block.
In contrast to that, our parallel version schedules the computation
of a nested block abstraction in another thread and continues with
the analysis of further abstract states from the set waitlist.
Each block abstraction is computed by a separate instance of
theCPAalgalgorithm(inownthread),withowninstancesofthe
setsreachedandwaitlist,andathread-safeinstanceofthetransfer
relation /leadstoand the operators mergeandstop. The operators are
stateless, and thus can be used in parallel from several threads.
Thereisnoneedtolockthedatastructuresofa CPAalginstance.
In parallel algorithms, a critical point is the number of synchro-
nizations.Blockabstractionsare largeenough toavoidexpensive
synchronization for single steps during the computation. Synchro-
nizationisonlyneededwhenenteringorleavingablock,i.e.,whenstartingorterminatingablock’sanalysisinstance.Additionally,the
communication only happens between dependent block abstrac-tions, such that no global locking is required in the algorithm.
3.1 Jobs as Components with Dependencies
Our technique is based on the parallel execution of componentsnamedjobs. A job
job=(D,reached ,waitlist ,B)consists of
637
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. Domain-Independent Multi-threaded Software Model Checking ASE ’18, September 3–7, 2018, Montpellier, France
•aC P AD=(D,/leadsto,merge ,stop )that determines the analysis
(in our case we always set D=BAM),
•asetreachedandasetwaitlistofabstractstatestobeanalyzed,
and
•ablockB=(L/prime,G/prime)representingthepartitionoftheprogram’s
CFA to be analyzed.
A job is executed by applying Alg. 1CPAalgwith the given
CPADon the sets reachedandwaitlist. Note that there can be
severaljobsforthesameblock B,buteachset reachedandeachset
waitlistareassignedtoexactlyonejob.Therearenoshareddata
basedonabstractstatesfordifferentjobs.Thisallowsustoexecute
jobsinparallel,becausethejobexecutionsareindependentfrom
each other. If a block has nested blocks, the corresponding block
abstractiondependsontheblockabstractionsofthosenestedblocks.
InthesequentialimplementationofBAM,thedependenciesofblock
abstractions on nested-block abstractionsareimplicitly solved by
callingalgorithm CPAalgrecursively,i.e.,theanalysisofanouter
blockwaitsuntilanestedblockabstractioniscomputedcompletely,andthencontinues.Intheparallelapproachweexplicitlymaintain
such dependencies between (analyses of) block abstractions. A
relationdeps∈jobs×E×jobstracksatwhichabstractstateablock
abstractionneedstobecomputedandapplied.Thisrelationneedsto
be globally visible, shared across all threads, and modifications are
appliedatomically.Asdependenciesareonlymodifiedwhenajobisstartedorterminated,theoverheadforsynchronizationisnegligible.Ourimplementationdoescurrentlynotsupportrecursivetasksand
thustherearenocyclicdependenciesbetweenblockabstractions.
3.2 Scheduling and Job Execution
The parallel execution of analyses needs a scheduling algorithm
that distributes the parallel running analyses onto the availableprocessing units. In our case we chose a simple task queue fromthe Java Concurrency API, where we insert our jobs, and let the
frameworkdothescheduling.Wecansetthenumberofrunning
threads to the available hardware by using the default Java thread
pool. For simplicity of Alg. 3, the actual scheduling is hidden in
thecallschedule that(asynchronously)executesthegivenjobwith
the given data.2This solution has only small overhead (for run
timeandfordevelopers)andisperformantenoughfortheanalysis,
even when applied to a larger scale of computing resources. We
have nearly linear speedup when using multiple cores (see Sect. 4),
thus we assume that the build-in scheduling is efficient enoughfor our currently available hardware.
The basic idea of a parallel implementation is given in Algs. 2
and3. The function abortof Alg.1CPAalgterminates the analysis
as soon as a nested block abstraction needs to be computed. In
this case, we determine the necessary data to compute the block
abstraction in our scheduling algorithm and schedule a new analy-
sistocomputethenested-blockabstractionasynchronously.The
abstractstatebeforeenteringtheblockisremovedfromthecurrent
setwaitlistand stored as a part of the dependency relation deps.
Afterthecomputationofthenested-blockabstractionisfinished,
thedependencyisremovedfrom depsandthestateisre-addedinto
2The pseudo code omits some scheduling-related code, as this would be too much
detail for this description and can be looked up in our reference implementation.Algorithm 2 ParallelBAM( D,reached,waitlist): Initial step for
parallel BAM
Input:aC P AD =(D,/leadsto,merge ,stop ),
whereEdenotes the set of elements of the lattice of D,
a setreached⊆Eof abstract states,
a setwaitlist⊆reachedof frontier abstract states,
a global relation deps⊆jobs×E×jobsto track computations
of block abstractions
Output: a set of reachable abstract states,
a subset of frontier abstract states
1:deps:=∅
2:mainJob:= (D,reached,waitlist, mainBlock)
3:JobExecutor(mainJob, deps,∅)
4:return(mainJob .reached,mainJob .waitlist)
Algorithm 3 JobExecutor( job,deps,statesToAdd ): Job execution
for parallel BAM
Input:ajob=(D,reached ,waitlist ,B),
a global relation deps⊆jobs×E×jobsto track computations
of block abstractions,
a setstatesToAdd⊆Eof abstract states to be added before
starting the analysis
1:job.waitlist:=job.waitlist∪statesToAdd
2:deps:=deps\/braceleftBig
(job,e,·)∈deps|e∈statesToAdd/bracerightBig
3:job.reached,job.waitlist:=
CPAalg(D, job.reached,job.waitlist)
4:missinдBAs :={e∈reached|hasMissinдBA (e)}
5:ifmissinдBAs /nequal∅then// nested BA needed
6:fore∈missinдBAs do
7:job.waitlist:=job.waitlist\{e}
8:childJob:= (C,{e},{e}, getBlock( e))
9:deps:=deps∪{ (job,e,childJob )}
10:schedule(childJob, deps,∅)
11:schedule(job, deps,∅)
12:else
13:finished :=job.waitlist =∅∧{ (job,·,·)∈deps}=∅
14:shouldAbort :=∃e∈job.reached:abort (e)
15:iffinished∨shouldAbort then
16:registerBA(job .reached,shouldAbort )
17:parents:={(·,·,job )∈deps}
18:for(parentJob, updateState ,·)∈parentsdo
19: schedule(parentJob, deps,{updateState})
20:deps:=deps\parents
thesetwaitlist.Thefunction schedule executesthegivenjobasyn-
chronouslywithalgorithmAlg. 3.Theasynchronousexecutionofa
jobcanbedelayedduetolimitedresourcesorbecausethesamejob
isscheduledtwice,i.e.,withdifferentarguments.Weuseathread
pool for scheduled jobs based on a job queue with a FIFO ordering
strategy. The function scheduleAndWait does the same, but awaits
theterminationofthejob.Themethod registerBA isexecutedwhen-
everablockanalysisterminates.Itextractstheblockabstraction
from the analyzed set reachedand updates the cache of BAM.
638
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Dirk Beyer and Karlheinz Friedberger
timeblock
BABBBCBDBEBF
A1A2A3A4A5A6A7 A8B1C1D1E1F1
E2
Figure2:Schematictimelineofapossibleexecutionofjobs
with parallel BAM for the example in Fig. 1
3.3 Example Application of Parallel BAM
The CFA in Fig. 1consists of two characteristic parts: the upper
parthasheavybranchingandseveralcontrol-flowpaths,thepart
belowlocation17consistsofasimplechainoflocations.Parallel
BAMimplicitlyrecognizesthisstructureandtheschedulingwill
apply a parallel analysis for the upper part. Figure 2shows a possi-
ble time line for the execution of the new algorithm for the CFA
given in Fig. 1. The heavy branching part of the program results
inindependentblocks BB,BC,andBD,whichcanbeanalyzedin
parallel. Each box in Fig. 2represents a job, consisting of a CPA
W, a setreached,as e twaitlist, and a block B∈{BA, ...,BF}. For
each block (more concretely: for each set reached), there can be
several jobs that are applied in sequential order.
Fortheexample,letusassumeadepth-firstsearchasiteration
order and an expensive computation in the blocks BB,BC, and
BD. In general, the iteration order for the program analysis can
be configured by the user, and the effort to analyze blocks de-pends of course on the given task.
Initially,Alg 2createsjob
A1(Alg.2,line2)fortheanalysisof
the block BA. Figure2shows the execution of job A1 with Alg. 3
as a box along the time axis. Internally, Alg. 1CPAalganalyzes
the first abstract states of the given task (Alg. 3, line 3), until the
entrylocationofblock BBisreached.Algorithm CPAalgterminates
for the job A1 and two further (independent) jobs A2 andB1a r e
scheduled (Alg. 2, line 10 and 11) and executed in parallel. The
jobB1 analyses the block BBand is not interrupted by another
block-entry location. The job A2 is scheduled because there is a
branching at location 2 in the CFA, such that the set waitlistof
the terminated CPAalgin jobA1 was not empty.
For the example, we assume that the job A7 analyzes the pro-
gram location with CFA location 17. For the part below location 17
however, inter-block dependencies prevent a parallel execution of
jobs and we need to explicitly wait for nested-block abstractionsto be computed. In Fig. 2this is visible for jobs
E1,F1,E2, and
A8, which do not have any parallel execution. Overall, our par-
allel version of BAM uses a dynamic scheduling, such that suchimbalances are prevented in most cases.
3.4 Soundness of the Parallel Approach
We take a short look at the soundness of the parallel algorithm
basedonitssequentialinstance.Themaindifferencebetweenthe
sequentialandtheparallelversionofBAMisthecomputationorderofblockabstractions.Insteadofcomputingoneblockabstraction
after another, they are computed in parallel whenever possible. As
thecomputationsofblockabstractionsthemselvesareindependent
and do not share any relevant data, the theoretical basis for sound-
nessdoesnotchange.Thus,theparallelapproachisassoundasthe
sequential algorithm that was proven to be sound in [ 31], i.e., only
the iteration strategy for the state space differs and the soundness
relies on the underlying analysis Wof BAM. In other words: If
thereexistsanabstractpathintheanalyzedsourcefilethatreaches
a property violation, then the same path is also explored by theparallelalgorithm,consistingofthesameblockabstractionsandabstract states as computed by a sequential analysis.
3.5 Requirements for Parallel Execution
Our parallel approach has some additional requirements on the
usedcomponents:EachusedCPAhastoallowmulti-threadedac-
cesstoitsmaincomponents,theoperatorsmustbethread-safeand
usableinparallel.Thiscaneitherbeimplemented(a)bystateless
operators (which is the intended behavior of operators anyway)or (b) by separate instances of the operators for each accessing
thread (including independent data structures). (a)An ideal frame-
workwouldonlyhavestatelessoperators(justastheirtheoretically
definedmathematicalpendant) andthus,theywould easilybeus-
able in multi-threaded context without locking or synchronization.
(b)While the operators are stateless in theory, a large software
system (such as the framework CPAchecker ), where the developers
integrateseveraldifferenttheoreticalapproaches,requiresanim-
plementationthatpartiallydeviatesfromtheconceptofstateless
operators. We noticed that the transfer relation /leadstoand also the
operators mergeandstopfor several CPAs were already designed
and implemented in a stateless manner, such that they can eas-ily be used for our parallel BAM implementation. Depending ontheCPA,mostofthecode(andalsomostofthetheoreticalback-
ground)isplacedinthetransferrelation,andthustheconceptual
difficulty was to rewrite those parts that are critical and might
needtobesynchronized.Toavoidheavysynchronization,wehave
convertedsome(non-critical)partslikestatisticsandtimemeasure-
ment into a thread-safe implementation or provide independent
instances of operators for special cases.
We have not only added the new algorithms (Alg. 2and3) for
parallel BAM into the framework, but also modified some other
componentssuchthattheycanbecombinedandusedwiththenew
algorithm.ThefollowinglistcontainsafewcornercasesofCPAs
that were touched or are usable with our approach:
•LocationCPA: The program location for the current analysis is
trackedwiththeLocationCPA.Astheprogramlocationofeachstatementisconstantafterparsingtheprogramandwrittenintothe CFA location, the ’state’ of the operators is the (immutable)
CFA itself. Thus no changes had to be made.
•CallstackCPA:Thecallstackforthecurrentanalysisisdeter-
mined by the CallstackCPA. As the corresponding operators
arestateless(i.e.,onlydependingontheabstractcall-stackstate
given as parameter), no changes were required.
•ValueCPA:TheValueCPAperformsanexplicit-valueanalysis
and tracks numerical values for variables. The analysis itself
does not need to be changed for synchronization.
639
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. Domain-Independent Multi-threaded Software Model Checking ASE ’18, September 3–7, 2018, Montpellier, France
4 EVALUATION
This section compares our new parallel approach with the existing
sequentialimplementationandshowsthatthenewapproach can
reducetheresponsetimeconsiderablywhenexecutedonseveral
cores.First,wecomparetheoldsequentialimplementationwiththe
newimplementation(executedwithonlyonethread),inorderto
show that no regression appears and that both analyses behave as
similar as possible. Second, we explore the speedup of the analysis
dependingonthenumberofthreads(asfarasourhardwareallows).
4.1 Evaluation Goals
Itisclearfromtheorythatnotallverificationtaskswillbenefitfrom
ourparallelizedverificationapproach:(a)Therearemanyprograms
wheremostpathshavesequentialdependenciesbetweenblocksand
therefore,thereisnotmuchroomforperformanceimprovements
from parallelization, and (b) there are many small programs, for
whichparallelizationdoesnotmakeadifference.Weclaimthatour
approachiseffectiveinbothr egards:itparallelizesand speedsup
verificationprocess(responsetime) ifthestructureoftheprogram
containssufficientbranchingandthesizeoftheprogramislarge
enoughanddoesnotnegativelyinfluencetheperformanceforthose
verificationtasksthataresmallorhavesequentialdependencies.
Claim 1. TheBAM-basedapproachtoparallelizationdoesnot
negatively impact the performance of verification tasks overall.Evaluation Plan: We take a large benchmark set of verification
tasksandverifythemwithandwithoutparallelization,restricted
to one processing unit. If the run time is not worse for the par-allel version, then the claim is valid.
Claim 2. The BAM-based approach to parallelization reduces
the response time of verification tasks by leveraging several pro-
cessing units. Evaluation Plan: We take a large set of verification
tasks that can potentially benefit from parallelization and com-
paretheresponsetimeoftheverificationwithdifferentnumbers
of processing units.
If this experiment is positive, the question raises where the ben-
efitcomesfrom:IsittheBAM-basedapproachtoparallelization,or
arethereothertechnicalcomponentsoftheverifierthatcontribute
to the speed up? What are the configurable parts of the verifier
that can benefit from parallelization? Can they be controlled in an
experiment (switched on and off separately)?
Claim3. TheparallelizationoftheprogramanalysisusingBAM
contributes considerably to the speedup. Evaluation Plan: After
identifying variables to control, we run experiments to investigate
the influence of the identified components.
4.2 Benchmark Environment and Limitations
Benchmark Sets. For our evaluation we use a large subset of the
SV-Benchmarks repository [ 4] containing over 5400verification
tasks3,sortedintodifferentcategoriesaccordingtheirspecification,
internal structure, or behavior. For the comparison of the existing
sequential implementation with the new parallel approach (limited
tooneCPUcore),weuseallverificationtaskswithareachability
property, in order to evaluate on a diverse set that the approach
has no negative effect (Claim 1). To demonstrate the positive effect
of parallelization of the new approach, we chose those verification
3https://github.com/sosy-lab/sv-benchmarkstasks from the category ReachSafety-ECA that consists of rather
large problems with a highly branching control flow (Claim 2).
Setup.Werantheexperimentsonaclusterof168identicalma-
chineswithahardwarespecificationthatroughlymatchesavailable
resourcesonmachinesofsoftwaredevelopers.Thisway,replica-
tion of our experiments does not require specific hardware. Foreachsingle verificationrunwe limittheCPU timeto15min andthememoryto15GB,andweuseanIntelXeonE3-1230v5CPU
with3.40GHzwith8processingunits(4physicalcoreswithhyper-
threading).The limitof CPUtimeenables usto evencompare the
effectiveness of parallelization (response time vs. CPU time) for
thoseverificationtasksforwhichtheverifierrunsintoatimeout.
Weevaluatedourimplementationin CPAchecker4,revision r28809,
from the official project repository5.
Because we use Intel processors with hyper-threading, where
two neighboring (virtual) processing units share some hardware
components and influence each other, we pair the (virtual) process-ingunitsanduseastepwidthof2forourexperimentswithvarying
numberofprocessingunits,i.e.,weuse2,4,6,8processingunits
and omit the odd numbers of processing units. The benchmarking
framework BenchExec [16]takescareofcorrectlyassigningthetwo
processing units of the same physical core together to the verifi-
cationprocesses.Wereportalltimesinsecondsandusetheterm
CPU time for the accumulated usage of processing units of a CPU,
andtheterms responsetime orwalltime forthetimethatelapses
between the start and the termination of the verification run.
Analysis Configuration. We configure BAM to use function and
loop bodies as blocks. BAM can be combined with several analy-ses; for our evaluation, we choose a combination where the per-formance influence from additional components is small: BAM
with an explicit-value analysis (VA) without CEGAR. This way,we configure a simple state-space exploration based on an ex-
plicit tracking of variables and their values. Both the sequentialand the parallel configurations apply a depth-first-search as ex-ploration strategy, i.e., the set
waitlistof the CPA algorithm is a
FIFO queue for each configuration.
Unfortunately,wecannotcomparetoothermulti-threadingver-
ifiers for reachability properties of sequential C programs, because
thereexistsnoequivalentapproachtothebestofourknowledge
(cf.relatedworkintheintroduction;thereareportfolioverifiers).
4.3 Claim I: Sequential vs. Parallel Algorithm
Configuration. In our first experiment we compare the existing
sequential algorithm with the new parallel algorithm. Therefore,
werunallexperimentsononlyoneprocessingunit.TheFIFOor-
dering of the job queue (see Sect. 3.2) in the parallel algorithm
guarantees that block abstractions are computed in the same order
as their blocks are reached, i.e., it behaves as similar as possibleto the sequential algorithm.
Results.Figures3aand3bshow the response time of the con-
figurationsforthebenchmarksetcontainingallverificationtasks
withareachabilityproperty.Aquantileplotcontainsgraphsthat
indicate the quantile of solved problem instances (x-axis) each
4https://cpachecker.sosy-lab.org
5https://gitlab.com/sosy-lab/software/cpachecker
640
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Dirk Beyer and Karlheinz Friedberger
0 250 500 750 1000 1250 1500 17501101001000
n-th fastest resultresponse time (s)VA-BAM
VA-parallelBAM (1 thread)
(a) Verification tasks with correctness proof
0 250 500 750 1000 12501101001000
n-th fastest resultresponse time (s)VA-BAM
VA-parallelBAM (1 thread)
(b) Verification tasks with property violation
Figure3:QuantileplotsforresultsofBAMwithvalueanaly-
sis,sequentialcomparedtoparallelversionwithonethread
withinacertainresponsetime(y-axis).6Itdoesnotshowadirect
comparison for individual verification tasks, but allows to com-
pare the overall behavior of an analysis configuration. We divided
the benchmarks into two groups: The plot in Fig. 3acontains re-
sults for all verification tasks for which a correctness proof wascomputed; the plot in Fig. 3bcontains results for all verification
tasks for which a property violation was found. The overall im-pression is that the (single-threaded) parallel technique does not
haveanynoticeableoverheadabovethesequentialapproach,i.e.,
the scheduler and the job executor from Alg. 3are efficient. The
new approach behaves almost identical when computing proofs,
andforfindingpropertyviolations,itisevenfasterandcansolve
more problems, which we discuss in the following.
Discussion. The difference in Fig. 3bbetween the verification
approaches results from the exploration order of the state space.
Afteranested-blockabstractionhasbeencomputed,thereisasmall
differenceinthesortingofabstractstatesinthesets waitlistofboth
approaches. The existing sequential analysis has (and keeps) theabstract states in the set
waitlist. The (single-threaded) parallel
approach removes abstract states when finding a missing block
abstraction (cf. Alg. 3, line 7) and re-adds those abstract states into
each setwaitlist(cf. Alg.3, line 1) after computing the necessary
block abstraction. There are small differences in the exploration
orderanddependingonthetask’sstructure,differentpathsmightbe
analyzed first. In those cases, the parallel approach does not apply
a pure depth-first exploration order, but partially prefers paths
6A detailed description of quantile plots can be found in the literature [16].thatdonottraversedeeplynestedblocks,whichseemsbeneficial
when it comes to finding property violations. For this reasoning,
weconcludethatforevaluatingClaimII,itwouldnotbevalidto
consider the verification tasks with property violations, because
the variable “exploration order” is not controlled.
We conclude that Claim 1 holds, because we did not observe
any negative impact of our new approach.
4.4 Claim II: Scalability of Parallel BAM
Configuration. We show the effectiveness of the parallelization
of our new approach by increasing the number of threads
(2,4,6,8threads)andobservetheimprovementoftheresponsetime.
Theupperlimitofthenumberofthreadsisdeterminedbythehard-
ware that we use. We set the number of processing units assigned
to the verification process to be equal to the number of threads.
We chose a subset of 154 tasks from the category ReachSafety-ECA,
suchthattheyneedareasonableamountoftime(atleast 3swith
onlyonethread)anddonotcontainapropertyviolation.Withatoo
small analysis time, the default overhead of the CPAchecker frame-
work itself (like JVM startup time or parsing time) hides the effect
of the parallel approach and blurs the picture. Additionally, finding
a path to a property violation with a parallel verification approach
easilyleadstonon-deterministicresultsifthereareseveralproperty
violationsinaverificationtaskorapropertycanbereachedviadif-
ferentprogrampaths7.Thus,weselectfromthebenchmarksetonly
thoseverificationtaskswithoutpropertyviolation,inordertomake
sure to compare the response time that is necessary to analyze the
wholestatespace.Theusedbenchmarksetconsistsofthreegroups:
47simpletasks,36mediumtasks,and71difficulttasks.Thediffi-
culty is roughly given by the size of the state space to be explored.
Results.Figure4ashows the response time of the configurations
for the benchmark set. Each function graph in the quantile plot
referstoadifferentnumberofthreadsusedintheanalysis.Asmaller
response time of the analysis corresponds to a smaller state space
and relates to a simpler task. The different groups of verification
tasks(simple,medium,anddifficult)areclearlyrecognizablebythe
levelofresponsetime,i.e.,theplotcontainslargersteps.Overall,
additional threads improve the performance of the analysis.
Figure4bshowsthespeedupofourparallelapproachoverthe
single-threaded application in the evaluation using box plots. Each
entryintheplotshowsthemedianasthehorizontallinewithinthebox,togetherwithitstwosurroundingquartilesbetweentheupper
andlowerlineofthebox,aswellastheminimumandmaximum
as whiskers. The speedup becomes larger the more threads we use.
Theevaluationwith2threadsoutperformsthesingle-threadedexe-
cutionbyabout20%(median).Theparallelapproachwith8threads
is about three times as fast as with 2 threads.
Discussion. The results look impressive: Only by parallelizing
independent BAM explorations in a way that is not tailored in
any specific way towards the framework or to a particular abstract
domain,weobserveasignificantimprovementoftheresponsetime.
Obviously,somepartsoftheverificationprocesscannotbeexecuted
inparallel.Thisdeniesa‘perfect’parallelizationandisknownas
Amdahl’slaw[ 1].Thesequentialpartsincludethestartupprocessof
7The supplementary artifact [ 9] and website include additional data about the evalua-
tion of our approach on a benchmark set of tasks containing a property violation.
641
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. Domain-Independent Multi-threaded Software Model Checking ASE ’18, September 3–7, 2018, Montpellier, France
0 20 40 60 80 100 120 140204060
source fileresponse time (s)12468 (#threads)
(a) Quantile plot for verification tasks without property violation
124681234
#threadsspeedup
(b) Box plot comparing 1 thread to N threads; without property violation
Figure 4: Comparison of response time for different num-
bers of threads, based on restricted benchmark set
CPAchecker aswellastheinitialoverheadoftheanalysistocompute
blocksforBAMandanalyzepartsofthemostouterblockuntila
nested block isreached, which inturn can be analyzed ina parallel
manner. Some parts of the implementation cause an additionalsynchronization overhead, like multi-threaded statistics for the
concurrent access to shared resources like the cache of BAM. The
rathermodestimprovementfrom1threadto2threadsismostlikely
due to hyperthreading of the processor, where the two processing
unitsofonephysicalcoreshareimportanthardwareresources.8
We conclude that Claim 2 holds, because the experiments show
thatforthoseprogramsthathavepotentialforspeedupbyparal-
lelization, we actually observe a significant speedup.
4.5 Claim III: Control Influencing Variables
Thepreviousexperimentsshowthatseveralprocessingunitsare
effectively used by the verification tool, but it is unclear where the
benefit comes from. Therefore, we need to investigate which parts
oftheverifierareparallelizedandmakesurethatournewapproach
contributed to the benefit. Since our implementation is based on
Java, we have also enabled the JVM to use multi-threaded garbage
collection (GC), because if we create abstract states in a parallelmanner, we should also deallocate them in parallel. The default
strategyforGCinOpenJDK1.8.0isacombinationof PSMarkSweep
andPS Scavenge. The mark-sweep collector applies a full mark-
sweepgarbagecollectionalgorithmforold-generationobjects.The
parallel scavenge collector cleans up young-generation objects.
8In our experiments we assigned successive processing units to the verification runs;
the experiment with 2 threads could be improved by using two processing units of
different physical cores.12468
112468
212468
412468
612468
81234
#analysis threads (major) with #GC threads (minor)speedup
(a)Boxplotcomparingtheresponsetimeof1threadto8threads,evaluated
on as many processing units as #analysis threads
12468
112468
212468
412468
612468
81234
#analysis threads (major) with #GC threads (minor)speedup
(b) Box plot comparing the response time of 1 thread to 8 threads, evalu-
ated on 8 processing units
Figure 5: Comparison of different numbers of analysis
threads and different numbers of GC threads
Configuration. Weusethe154tasksfromthepreviousexperiment
andre-evaluatethem.Wedivideourevaluationintotwocases:First,
the number of available processing units is equal to the numberof analysis threads. Second, the number of available processingunits is set to 8, which is the upper limit the available hardware.
Forbothcases,weevaluatedallcombinationsofanalysisthreads
(using1,2,4,6,8threads;major,largenumbersinfigure)andGC
threads (using 1, 2, 4, 6, 8 threads; minor, small numbers in figure).
Results.Wepresentthespeedupstatisticsforcomparingthere-
sponse time of a single-threaded analysis with a single-threaded
GConasingleprocessingunittoanexecutionwithagivennumber
of analysis threads with a given number of threads for GC on agivennumberofprocessingunits.InFig. 5athenumberofavail-
able processing units is equal to the number of analysis threads.In Fig.5ball 8 processing units of the machine are available to
the verifier. In both figures we configure the number of analysis
threadsandGCthreads.Ineachplot,thehorizontalaxiscontains
5 major groups (representing the number of analysis threads) of
each 5 minor entries (number of threads for GC). For example, the
five first (most left) entries in each figure show the speedup of the
approachifusingonethreadfortheanalysisandavaryingnumber
ofthreadsforGC.Unsurprisingly,theoverallresultisthatusingmultiple threads for both the analysis and additionally the GC is
beneficial.Nearlyalltasksaresolvedfasterifmultipleprocessing
units are assigned to the verification process.
642
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Dirk Beyer and Karlheinz Friedberger
Discussion. In Fig.5a, the first entry of each group shows the
speedupoftheanalysiswhenusingonlyonethreadforGC.Thisiso-
lates the the benefit of multi-threading caused by our new analysis
approach.Similarly,Fig. 5bshows(withineachofthe5groups)that
keeping the number of analysis threads constant and incrementing
thenumberofthreadsforGCalso speedsupthe verificationpro-
cess.Thereforeboth,analysisandGC,benefitfrommulti-threading.
Figure5bshows that if the analysis is bound to one thread, the
benefitfrommulti-threadingis ratherlimited,whilethespeedupis
improvedifweusemorethreadsfortheanalysis.Themostinterest-ingindicatorsarethemedianvalue(middlelineinsidethebox)and
the minimal speedup values (lower whisker). The overall variance
fortheresponsetimeandspeedupisquitelargeifthereareseveral
processing units available. This might indicate a non-deterministic
scheduling of workload across free resources, in contrast to the
narrowboxesinFig. 5ainthelefttwogroups(wherethenumber
of processing units is bound to one and two, respectively).
We conclude that Claim 3 holds, because we were able to isolate
and control the only other cause for a significant speedup, and the
experiments confirmed that our new approach is reponsible forthe improved performance of the analysis, while the parallel GCalgorithms of the JVM take care of parallelized deallocation.
4.6 Threats to Validity
External Validity: Our benchmark suite consists of a large set of C
source files. We use the largest publicly available benchmark suite
inordertooptimizethediversityinsizeandtypeofprograms.ThisisparticularlyimportantforevaluatingClaim1.ForClaims2and3,
werestricted thebenchmarkset toverificationtasks thathavepo-
tentialtobenefitfromparallelization.Ourevaluationisrestricted
tothelanguageC,andwhileitseemsclearthattheconceptsand
results can be transferred to other imperative languages, such a
claim is notbacked up by our experiments. Thechosen time limit
of15minandmemorylimitof15GBforverifyingagiventaskis
inspired by the research community on software verification (cf.
one of the reports on the International Competition on Software
Verification[ 4]).Ofcourse,theevaluationofourapproachdepends
on the tool in which it is implemented. There is currently no other
tool implementing the same approach, and a comparison with a
completely different approach for parallel analysis might be mis-leading.
9With the assumption that the default configuration is
optimized for most use cases, we did not change the configuration
of the JVM except the increment of maximal heap memory and the
adjustment of the garbage-collection strategy, such that the effect
of the number of threads can be measured. The available hardware
mightalsoinfluencetheresults.Forparallelexecution,theinternal
structure of the CPU is a critical element, i.e., low-level cachingand the hierarchy of processing units have a large effect on therun time of tasks. We used a modern Intel Xeon E3-1230 v5 thatis available on the market for a reasonable price, in order to ob-
tain results that have a higher externally validity than experiments
on special high-performance clusters.
9Thesupplementaryartifact[ 9]andwebsiteincludeanadditionalcomparisonwith
some non-BAM analysis approaches, in order to show that using the BAM technology
does not negatively effect an analysis’ performance (known result [31]).InternalValidity: BesidesgarbagecollectionoftheJVM,thereare
otherfactors thatinfluence thespeedupof theparallel approach.
Some components of CPAchecker , e.g., counters and measurements
for statistics, are not yet fully optimized for parallel execution. Ad-
ditionally, it depends on the task’s structure how many blocks can
be analyzed in parallel. Controlling this variable (number of paral-
lelization blocks) is not possible or very difficult, thus, we prefer to
increasetheinternalvaliditybythelargenumberofexperiments
ondifferenttasks.Anothercontrolvariableistheblocksize.Larger
blocksarebeneficialforaconcurrentanalysis,duetothesmaller
synchronizationfootprint.ForClaims2 and3,thebenchmarkset
was already chosen such that it contains only programs where the
block size is very large. Thus, we did not further analyze differ-ent block sizes. We also need to consider that the explicit-value
analysis computes a large number of abstract states, while other
abstractdomainsmightlead tomorecompactrepresentationsof
the state space, and the fewer abstract states areexploredthe less
might be parallelized. Our time measurement includes the mem-ory allocation for the JVM, parsing time, and internal statistics,
which adds processing workload that cannot be parallelized cur-
rently.Wemitigatethiseffectbyusingonlythoseverificationtasks
that need more than 3swhen using one thread, i.e., we consider
verificationtasksforwhichtheanalysisitselfconsumesaportion
of the run time that is not negligible.
5 CONCLUSION
Wepresentedanewapproachformulti-threadedsoftwareverifi-
cation that is based on program-block summaries. Our emphasis
isonprovidingasolutionthatfollowstheprincipleofseparation
of concerns: the problem of making the analysis benefit from mul-
tiple processing units is treated completely orthogonal from theproblem of designing and implementing an abstract domain and
theoperatorsforaprogramanalysis.Weformallydefinethenew
algorithm in the framework, provide a working implementation,
anddemonstrateitsapplicabilityonalargesetofbenchmarks.The
experiments show that our approach (a) does not add noticeable
overhead for verification tasks that do not benefit from paralleliza-
tion,(b)canconsiderablyspeeduptheverificationprocessinmany
cases (given the verification task has a certain minimal size and
some independent branches to explore), and (c) contributes largely
to the performance improvements, i.e ., the speedup is not only due
to multi-threading features that the JVM provides.
The presented algorithm is implemented as a shared-memory
approach, which allows efficient interaction of all components. As
thenumberofCPUcorespermachineandalsotheamountofmem-oryperhostislimited,weplantoextendouralgorithmtoleverage
severalprocessesthatmightbedistributedoverseveralmachinesin
acluster.Anadditionalbenefitwouldbeasimplerusageofabstract
domains that rely on libraries that are not thread-safe, because
there is no problem with interleaved usage of libraries in separate
processes.AdditionallyweplantooffloadthecacheofBAMtoa
disk-based storage, in order to lower the memory usage for veryresource intensive tasks. The combination of both, a distributed,
multi-process verification algorithm and a disk-based cache, seems
tobeverypromisingfortheverificationofverylargeprograms.
643
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. Domain-Independent Multi-threaded Software Model Checking ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]G. M. Amdahl. 1967. Validity of the Single Processor Approach to Achieving
Large Scale Computing Capabilities. In Proc. AFIPS. ACM, 483–485. https://doi.
org/10.1145/1465482.1465560
[2]P.Andrianov,K.Friedberger,M.U.Mandrykin,V.S.Mutilin,andA.Volkov.2017.
CPA-BAM-BnB: Block-Abstraction Memoization and Region-Based Memory
ModelsforPredicateAbstractions(CompetitionContribution).In Proc.TACAS
(LNCS 10206). Springer, 355–359. https://doi.org/10.1007/978-3-662-54580-5_22
[3]J. Barnat, J. Havlícek, and P. Rockai. 2013. Distributed LTL Model Checking with
HashCompaction. ENTCS296(2013),79–93. https://doi.org/10.1016/j.entcs.2013.
07.006
[4]D.Beyer.2017. SoftwareVerificationwithValidationofResults(ReportonSV-
COMP2017).In Proc.TACAS (LNCS10206).Springer,331–349. https://doi.org/
10.1007/978-3-662-54580-5_20
[5]D. Beyer, M. Dangl, D. Dietsch, and M. Heizmann. 2016. Correctness Witnesses:
ExchangingVerificationResultsBetweenVerifiers.In Proc.FSE.ACM,326–337.
https://doi.org/10.1145/2950290.2950351
[6]D. Beyer,M. Dangl, D.Dietsch, M. Heizmann,and A. Stahlbauer.2015. Witness
ValidationandStepwiseTestificationacrossSoftwareVerifiers.In Proc.FSE.ACM,
721–733. https://doi.org/10.1145/2786805.2786867
[7]D. Beyer, M. Dangl, and P. Wendler. 2015. Boosting k-Induction with
Continuously-RefinedInvariants.In Proc.CAV (LNCS9206).Springer,622–640.
https://doi.org/10.1007/978-3-319-21690-4_42
[8]D. Beyer, M. Dangl, and P. Wendler. 2018. A Unifying View on SMT-Based
SoftwareVerification. J.Autom.Reasoning 60,3(2018),299–335. https://doi.org/
10.1007/s10817-017-9432-6
[9]D. Beyer and K. Friedberger. 2018. Replication Package for Article “Domain-
Independent Multi-threaded Software Model Checking” in Proc. ASE’18. https:
//doi.org/10.5281/zenodo.1322090
[10]D. Beyer, T. A. Henzinger, R. Jhala, and R. Majumdar. 2007. The Software Model
Checker Blast. Int. J. Softw. Tools Technol. Transfer 9, 5-6 (2007), 505–525.
https://doi.org/10.1007/s10009-007-0044-z
[11]D.Beyer,T.A.Henzinger,M.E.Keremoglu,andP.Wendler.2012. Conditional
Model Checking: A Technique to Pass Information between Verifiers. In Proc.
FSE. ACM, Article 57, 11 pages. https://doi.org/10.1145/2393596.2393664
[12]D. Beyer, T. A. Henzinger, and G. Théoduloz. 2007. Configurable Software
Verification: Concretizing the Convergence of Model Checking and Program
Analysis. In Proc. CAV (LNCS 4590) . Springer, 504–518. https://doi.org/10.1007/
978-3-540-73368-3_51
[13]D. Beyer, M. E. Keremoglu, and P. Wendler. 2010. Predicate Ab-
straction with Adjustable-Block Encoding. In Proc. FMCAD. FMCAD,
189–197. https://www.sosy-lab.org/research/pub/2010-FMCAD.Predicate_
Abstraction_with_Adjustable-Block_Encoding.pdf
[14]D.BeyerandT.Lemberger.2016. SymbolicExecutionwithCEGAR.In Proc.ISoLA
(LNCS 9952). Springer, 195–211. https://doi.org/10.1007/978-3-319-47166-2_14
[15]D. Beyer and S. Löwe. 2013. Explicit-State Software Model Checking
Based on CEGAR and Interpolation. In Proc. FASE (LNCS 7793). Springer,146–162. https://www.sosy-lab.org/research/pub/2013-FASE.Explicit-State_
Software_Model_Checking_Based_on_CEGAR_and_Interpolation.pdf
[16]D. Beyer,S. Löwe,and P.Wendler. 2017. Reliable Benchmarking:Requirements
and Solutions. Int. J. Softw. Tools Technol. Transfer (2017).https://doi.org/10.
1007/s10009-017-0469-y
[17]D. Beyer and A. Stahlbauer. 2014. BDD-based software verification: Applications
to event-condition-action systems. STTT16, 5 (2014), 507–518. https://doi.org/
10.1007/s10009-014-0334-1
[18]S.Blom,J.vandePol,andM.Weber.2010. LTSmin:DistributedandSymbolic
Reachability. In Proc. CAV (LNCS 6174). Springer, 354–359.
[19]M. Brain and F. Schanda. 2012. A Lightweight Technique for Distributed and
IncrementalProgramVerification.In Proc.VSTTE (LNCS7152).Springer,114–129.
https://doi.org/10.1007/978-3-642-27705-4_10
[20]E. M. Clarke, O. Grumberg, S. Jha, Y. Lu, and H. Veith. 2003. Counterexample-
guided abstraction refinement for symbolic model checking. J. ACM50, 5 (2003),
752–794. https://doi.org/10.1145/876638.876643
[21]H.Guo,M.Wu,L.Zhou,G.Hu,J.Yang,andL.Zhang.2011. Practicalsoftware
modelcheckingviadynamicinterfacereduction.In Proc.SOSP.ACM,265–278.
https://doi.org/10.1145/2043556.2043582
[22]A. Gurfinkel, A. Albarghouthi, S. Chaki, Y. Li, and M. Chechik. 2013. Ufo:
Verification with Interpolants and Abstract Interpretation (Competition Contri-
bution). In Proc. TACAS (LNCS 7795). Springer, 637–640. https://doi.org/10.1007/
978-3-642-36742-7_52
[23]G. J. Holzmann. 2003. The Spin Model Checker: Primer and Reference Manual .
Addison-Wesley.
[24]B.A.Huberman,R.M.Lukose,andT.Hogg.1997. AnEconomicsApproachto
Hard Computational Problems. Science275, 7 (1997), 51–54. http://www.hpl.hp.
com/research/idl/papers/EconomicsApproach/EconomicsApproach.pdf
[25]K. Laster and O. Grumberg. 1998. Modular Model Checking of Software. In Proc.
TACAS (LNCS 1384). Springer, 20–35. https://doi.org/10.1007/BFb0054162
[26]P. Müller, P. Peringer, and T. Vojnar. 2015. Predator Hunting Party (Competition
Contribution). In Proc. TACAS (LNCS 9035). Springer, 443–446.
[27]T. L. Nguyen, P. Schrammel, B. Fischer, S. La Torre, and G. Parlato. 2017. Parallel
bug-finding in concurrent programs via reduced interleaving instances. In Proc.
ASE.IEEEComputerSociety,753–764. https://doi.org/10.1109/ASE.2017.8115686
[28]E. Sherman and M. B. Dwyer. 2018. Structurally Defined Conditional Data-
Flow Static Analysis. In Proc. TACAS, Part II (LNCS 10806). Springer, 249–265.
https://doi.org/10.1007/978-3-319-89963-3_15
[29]V.Still,P.Rockai,andJ.Barnat.2016. DIVINE:Explicit-StateLTLModelChecker
(Competition Contribution). In Proc. TACAS (LNCS 9636) . Springer, 920–922.
[30]T. vanDijk. 2016. Sylvan: multi-coredecision diagrams. Ph.D. Dissertation. Uni-
versityofTwente,Enschede,Netherlands. http://purl.utwente.nl/publications/
100676
[31]D. Wonisch and H. Wehrheim. 2012. Predicate Analysis with Block-Abstraction
Memoization. In Proc. ICFEM (LNCS 7635). Springer, 332–347. https://doi.org/10.
1007/978-3-642-34281-3_24
644
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:50:20 UTC from IEEE Xplore.  Restrictions apply. 