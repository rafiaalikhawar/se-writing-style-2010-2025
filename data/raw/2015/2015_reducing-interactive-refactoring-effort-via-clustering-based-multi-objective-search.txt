Reducing Interactive Refactoring Effort via Clustering-Based
Multi-objective Search
Vahid Alizadeh
CIS department, University of Michigan
Dearborn, Michigan, USA
alizadeh@umich.eduMarouane Kessentini
CIS department, University of Michigan
Dearborn, Michigan, USA
marouane@umich.edu
ABSTRACT
Refactoringisnowadayswidelyadoptedintheindustrybecause
bad design decisions can be very costly and extremely risky. Onthe one hand, automated refactoring does not always lead to thedesired design. On the other hand, manual refactoring is error-
prone, time-consuming and not practical for radical changes. Thus,
recentresearchtrendsinthefieldfocusedonintegratingdevelopers
feedback into automated refactoring recommendations becausedevelopers understand the problem domain intuitively and may
haveacleartargetdesigninmind.However,thisinteractiveprocess
can be repetitive, expensive, and tedious since developers must
evaluaterecommendedrefactorings,andadaptthemtothetargeted
design especially in large systems where the number of possible
strategies can grow exponentially.
In this paper, we propose an interactive approach combining
the use of multi-objective and unsupervised learning to reduce the
developer’s interaction effort when refactoring systems. We gener-
ate,first,usingmulti-objectivesearchdifferentpossiblerefactoring
strategiesbyfindingatrade-offbetweenseveralconflictingqual-
ityattributes.Then,anunsupervisedlearningalgorithmclusters
the different trade-off solutions, called the Pareto front, to guide
the developers in selecting their region of interests and reduce the
numberofrefactoringoptionstoexplore.Thefeedbackfromthe
developer, both at the cluster and solution levels, are used to au-
tomatically generateconstraints toreduce thesearch spacein the
nextiterationsandfocusontheregionofdeveloperpreferences.Weselected14activedeveloperstomanuallyevaluatetheeffectiveness
our tool on 5 open source projects and one industrial system. The
results show that the participants found their desired refactorings
faster and more accurate than the current state of the art.
CCS CONCEPTS
•Software and its engineering →Search-based software en-
gineering;
KEYWORDS
Search based software engineering, refactoring, multi-objective
search, clustering
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238217ACM Reference Format:
Vahid Alizadeh and Marouane Kessentini. 2018. Reducing Interactive Refac-
toring Effort via Clustering-Based Multi-objective Search. In Proceedings
of the 2018 33rd ACM/IEEE International Conference on Automated Software
Engineering(ASE’18),September3–7,2018,Montpellier,France. ACM,New
York, NY, USA, 11pages.https://doi.org/10.1145/3238147.3238217
1 INTRODUCTION
As projects evolve, developers frequently postpone necessary sys-
temrestructuring,knownasrefactoring[ 17],intherushtodelivera
newreleaseuntilacrisishappens.Whenthatoccursitoftenresults
insubstantiallydegradedsystemperformance,perhapsaninability
to support new features, or even in terminally broken system ar-chitecture.Thus, refactoringreceivedmuchattention duringthe
lasttwodecadestoproposesolutionsthatcanmanagethegrowing
complexityofsoftwaresystemsnowadays.Mostexistingstudies
focusoneithermanualorfullyautomatedcode-levelrefactoring.
The manual support, integrated into modern IDEs such as Eclipse,
NetBeans, and Visual Studio [ 5,14,15,21–23,26,27,30,32,33],
consists of helping developers to apply refactorings based on au-
tomatedroutinesthatcancheckalistofpre-andpost-conditions
but they have to specify manually which types of refactoring to
beapplied,suchasextractclassormovemethod,andwhere.The
fullyautomatedtechniquestrytoidentifyrefactoringopportunities
and which refactorings to apply using static and dynamic analy-
sis, and the history of changes. However, design restructuring is a
human activity that cannot be fully automated because developers
understand the problem domain intuitively and they have targeted
designgoalsinmind.Thus,severalempiricalstudiesshowthatfullyautomated refactoring does not always lead to the desired architec-
ture [8,10,23,24]. Furthermore, manual refactoring is error-prone,
time consuming and not practical for radical changes. For instance,
Batory et al. [ 22] presented several case studies where refactoring
involved more than750 refactoring steps onone project and took
more than 3 weeks to execute.
Recently, few approaches have been proposed to interactively
evaluaterefactoringrecommendationsusingsearch-basedsoftware
engineering [ 7,24,28]. The developers can provide a feedback
abouttherefactoredcodeandintroducemanualchangestosome
of the recommendations. However, this interactive process can be
repetitive,expensive,andtedioussincedevelopersmustevaluate
recommended refactorings, and adapt them to the targeted design
especiallyinlargesystemswherethenumberofpossiblestrategies
can grow exponentially. Thus, we seek, in this work, to answer
the fundamental scientiïňĄc question: "What is the minimalguid-
ance that leads automated search to useful and realistic refactoring
recommendations?"
464
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Vahid Alizadeh and Marouane Kessentini
Inthispaper,weproposeaninteractiveapproachcombiningthe
use of multi-objective search, based on NSGA-II [ 12] and unsuper-
vised learning to reduce the developer’s interaction effort when
refactoringsystems.Wegenerate,first,usingmulti-objectivesearch
different possible refactoring strategies by finding a trade-off be-
tween several conflicting quality attributes. Then, an unsupervised
learning algorithm clusters the different trade-off solutions, called
theParetofront,toguidethedevelopersinselectingtheirregionof
interestsandreducethenumberofrefactoringoptionstoexplore.
Thefeedbackfromthedeveloper,bothattheclusterandsolution
levels, are used to automatically generate constraints to reduce
thesearchspaceinthenextiterationsandfocusontheregionof
developer preferences. For instance, the developer can select the
mostrelevantclusterofsolutions,calledregionofinterest,based
onhispreferencesandthemulti-objectivesearchwillreducethe
space of possible solutions, in the next iterations, by generating
constraintsfromtheinteractiondatasuchaseliminatingpartofthe
code(e.gclassesormethods)thatarenotrelevantforrefactoring
to the programmer.
We selected 14 active developers to manually evaluate the ef-
fectivenessourtoolon5opensourceprojectsandoneindustrial
system. The results show that the participants found their desired
refactorings faster and more accurate than the current state of the
art.
2 PROBLEM STATEMENT
Toinvestigatethechallengesassociatedwithcurrentrefactoring
tools, a survey was conducted, as part of an NSF I-Corps project,
with127professionaldevelopersat38mediumandlargecompanies
including eBay, Amazon, Google, IBM, and others. 112 of these
interviews were conducted face-to-face. As an outcome of these
interviews, the following challenges were identified:
-Challenge 1: The refactorings effort required by exist-
ing approaches and tools. 83% of the interviewed developers
confirmedthattheywerereluctanttouseexistingautomatedrefac-toringtoolsbecausethosedetect,ingeneral,hundredsofcodelevel
quality issues such as anti-patterns but without specifying from
wheretostartorhowtheyaredependentoneachothers,norare
thereanyclearbenefitssuchasanimpactonthesystem’squality.
Duringtheinterviews,86%ofdevelopersconfirmedthattheywantbetterrefactoringtoolstogivethembetterunderstandingofdesign
preferences rather than asking developers to manually inspect a
large list of recommendations covering the whole system. A devel-
oper said "We need better solutions of refactoring tasks that can
reducethecurrenttime-consumingmanualworkofevaluatinga
large number of refactorings. Automated tools provide refactoring
solutions that are hard and costly to repair because they did not
consider our design needs and hard to assess their impact." This
argument is consistent with empirical studies performed by Kim et
al. [22].
-Challenge 2: Lack of visualization support to estimate
the impact of recommended refactorings. 69 out of the 112
participantshighlightedintheinterviewsthatitishardtounder-
stand the impact of suggested refactorings on the system and they
have to look manually at the code before and after refactoring.
Determiningwhichanti-patternshouldberefactoredandhowisneverapuretechnicalprobleminpractice.Instead,high-levelrefac-
toring decisions have to take into account trade-offs between code
quality,availableresourcesandexpectedeffort.Furthermore,53par-
ticipantsmentionedthatseveralrefactoring"paths"arediscussed
between architects to determine the best solution to restructure
thecurrentarchitectureorcode.However,mostofexistingrefac-
toring tools and approaches just recommend only one sequence of
refactorings to apply.
-Challenge3:Itisdifficultfordeveloperstoexpresstheir
preferencesupfront. Basedonourextensiveexperienceworking
onlicensingrefactoringresearchprototypestoindustry,developers
always have a concern on expressing their preferences upfront as
an input for a tool to guide refactoring suggestions. They prefer
to get insights from some generated refactoring solutions then
decide which quality attributes they want to improve. However,
severalofexistingrefactoringtoolsfailtoconsiderthedeveloper
perspective,asthedeveloperhasnoopportunitytoprovidefeed-
backontherefactoringsolutionasitisbeingcreated.Furthermore,
asdevelopmentmusthaltwhiletherefactoringprocessexecutes,
fully-automated refactoring methodsare not useful forfloss refac-
toring where the goal is to maintain good design quality while
modifying existing functionality. The developers have to accept
the entire refactoring solution even though they prefer, in general,
step-wise approaches where the process is interactive and they
have control of the refactorings being applied.
-Challenge4:Lackofrefactoringtoolsthatcanlearnfrom
developers interaction. High-levelrefactoringsareusuallysys-
tematicandrepetitiveindifferentcontexts,involvingsimilarchanges
to numerous locations [ 9]. If these repetitive high-level changes
can be learned, abstracted, and automated, a large amount of main-
tenance effort could be saved.
3 CLUSTERING-BASED INTERACTIVE
MULTI-OBJECTIVE SOFTWARE
REFACTORING
The general structure of our approach is sketched in Fig. 1.I n
the following, we decribe the different main components of our
approach.
3.1 Phase 1: Multi-Objective Refactoring
Discovering a refactoring solution can be a challenging task since
a large searchspace needs to beexplored. This large searchspace
istheresultofthenumberofrefactoringoperationsandtheimpor-
tance of their order and combination. To explore this search space,
we propose an adaptation of the non-dominated sorting genetic
algorithm(NSGA-II)[ 12]tointeractivelyfindatrade-offbetween
multiple quality attributes.
A multi-objective optimization problem can be formulated in
the following form:
Minimize F (x)=(f1(x),F2(x), ...,fM(x)),
Subject to x ∈S,
S={x∈Rm:h(x)=0,д(x)≥0};
whereSis the set of inequality and equality constraints and the
functions fiareobjective orfitnessfunctions. In multi-objective
465
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. Reducing Interactive Refactoring Effort via Clustering-Based Multi-objective Search ASE ’18, September 3–7, 2018, Montpellier, France
Figure 1: Overview of our proposed IC-NSGA-II approach.
optimization, the quality of a solution is recognized by dominance.
Thesetoffeasiblesolutionsthatarenotdominatedbyanyother
solution is called Pareto-optimal orNon-dominated solution set.
NSGA-IIisamulti-objectiveevolutionaryalgorithmoperating
onapopulationofcandidatesolutionswhichareevolvedtoward
the Pareto-optimal solution set. NSGA-II uses an explicit diversity-
preserving strategy together with an elite-preservation strategy
[12]. As described in Algorithm 1, the first iteration of the process
begins with a complete execution of adapted NSGA-II to our refac-
toringrecommendationproblembasedonthefitnessfunctionsthat
willbediscussedlater.Atthebeginning,arandompopulationof
encoded refactoring solutions, P0, is generated as the initial parent
population. Then, the children population, Q0, is created from the
initial population using crossover and mutation. Parent and chil-
drenpopulationsarecombinedtogethertoform R0.Finally,asubset
ofsolutionsisselectedfrom R0basedonthecrowding distanceand
dominationrules.Thisselectionisbasedonelitismwhichmeans
keeping the best solutions from the parent and child population.
Elitism does not allow an alreadydiscovered non-dominated solu-
tion to be removed. This process is continued until the stopping
criteria is satisfied.
The results of the first execution of search algorithm are a set of
non-dominatedsolutionsthatwillbeclusteredandthenupdatedby
the users. After this interactions phase, the multi-objective search
algorithmwillcontinuetorunusingthenewconstraintsgenerated
at the cluster and solution levels.
3.1.1 Refactoring Solution Representation. Arefactoringsolution
isrepresentedasavectorconsistsofanorderedsequenceofmul-
tiple refactoring operations. Each refactoring operation includes
a refactoring action and its specific controlling parameters. The
refactoringtypesconsideredinourexperimentsare:MoveMethod,
Move Field, Extract Class, Encapsulate Field, Pull Up Field, Pull Up
Method,PushDownField,PushDownMethod,ExtractSubClass,
Extract SuperClass. Refactoring operations are created or modified
randomlyduringthepopulationinitializationormutation.Also,thesizeofasolutionvectorwhichisthenumberofincludedrefactoringoperation is randomly selected between lower and upper bound
values.Therefore, itisimportant toinvestigatethe feasibilityofa
solutionanditsoperationsusingrelatedpre-andpost-conditions
[31].Theseconditionsensurethattheprogramwillnotbreakwhile
the behavior is preserved by the refactoring.
3.1.2 Fitness Functions. Thefitnessorobjectivefunctionevaluates
a candidate solution and calculates its goodness degree to the con-
sidered problem. In order to measure the influence of a refactoring
solution on the software project, we utilized Quality Model forObject-Oriented Design (QMOOD) [
4]. This model is developed
basedoninternationalstandard forsoftwareproductqualitymea-
surement. QMOOD is a comprehensive way to assess the software
qualityandincludesfourlevels.Weemployedthefirsttwolevels
known as "Design Quality Attributes" and "Object-oriented Design
Properties"tocalculateourfitnessfunctions(Reusability,Flexibil-
ity, Understandability, Functionality, Extendibility, Effectiveness,
Complexity,Cohesion,Coupling).Therelativechangeofthequality
metric afterapplying therefactoring solutionis consideredas the
fitness function and can be expressed as:
FitnessFunction i=QMaf ter
i−QMbef ore
i
QMbef ore
i(1)
whereQMbef ore
iandQMaf ter
iarethevalueofthequalitymet-
ricibefore and after applying a refactoring solution, respectively.
3.2 Phase 2: Clustering the Pareto Front of
Refactoring Solutions
The goal of this phase is to reduce the effort to investigate the
solutions in Pareto-optimal front. We try to group the solutions
basedontheirfitnessfunctionvalueswithoutfilteringorremoving
any of them. In this way, the solutions can be categorized based
thesimilarityamongthemintheobjectivesspace.Then,arepre-
sentativesolutionisidentifiedfromeachpartitiontorecommend
tothedecisionmaker(centerofthecluster).Forthispurposewe
used clustering analysis technique. Clustering is one of the most
466
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Vahid Alizadeh and Marouane Kessentini
Algorithm 1: Interactive Clustering-based NSGA-II (IC-NSGA-II)
Input :Population Size ( N), Source Code
Output:Recommended Pareto-optimal Solutions
1UserPreferences ←∅; /* Initiate Preference Parameters */
2while¬The user is satisfied do
phase1 beginMulti-objective Refactoring
4 P1←InitializePopulation( N,UserPreferences); /* User preferred random population */
5 EvaluateObjectives( P1,UserPreferences);
6 FastNonDominatedSort( P1);
7 Q1←SelectCrossoverMutate( P1,UserPreferences);
8 while¬StoppingCondition() do
9 EvaluateObjectives( Q1,UserPreferences); /* User preferred evaluation */
10 Rt←P1∪Q1;
11 Fronts=FastNonDominatedSort( Rt);
12 Pt+1←∅;
13 i←1;
14 while|Pt+1|+|Front i|≤Ndo
15 CrowdingDistanceAssign(Front i);
16 Pt+1←Pt+1∪Front i;
17 i←i+1;
18 SortByRankAndDistance(Front i);
19 Pt+1←Pt+1∪Front i[1:(N−|Pt+1|)];
20 Qt+1←SelectCrossoverMutate( Pt+1,UserPreferences) ; /* Customized GA Operator */
21 t=t+1;
22 RecommendedSolutions ←Qt+1;
phase2 beginPareto Front Clustering
24 GMMClustering (RecommendedSolutions); /* Described in Algorithm 2 */
25 ClustersCenter ();
phase3 beginInteraction and User Preference
27 GetUserFeedBack (Clusters,Centers) ; /* Described in Algorithm 3 */
28 UserPreferences ←ExtractPreferences ();
29ReturnRecommendedSolutions;
importantandpopularunsupervisedlearningproblemsinMachine
Learning. It helps to find a structure in a set of unlabelled data in a
way that the data in each cluster are similar together while they
are dissimilar to the data in other clusters.
One of the challenges in cluster analysis is to define the optimal
numberofclusters.Therefore,weneedclustervalidityindexasa
measureofclusteringperformance.Differentpartitionsiscomputed
and the ones that fits the data better are selected. The procedure of
Phase 2 is illustrated in Algorithm 2.
3.2.1 Calinski Harabasz (CH) Index. isaninternalclusteringvali-
dation measure based on two criteria: compactness and separation
[11]. CH evaluates the clustering results based on the average sum
of squares between and within clusters and it defines as follows:
CH=(N−K)
(K−1)ΣK
k=1|ck|dist(ck,S)
ΣK
k=1Σsi∈ckdist(si,ck)(2)wheredist(a,b)istheEuclideandistance, ckandSarethecluster
and global centroids, respectively.
The first step in Pareto-front clustering is to execute the clustering
process with different number of components and to compute CH
scoreforeach.Thebestnumberofclusters(K)isdefinedastheone
that achieves the highest CH score.
3.2.2 Gaussian Mixture Model (GMM). is a probabilistic model-
based clustering algorithm with which a mixture of kGaussian
distributions is fitted on the data. GMM is soft-clustering approach
in which each data point is assigned a degree that it belongs to
each of the clusters. The parameters that need to fit are Mean ( μk),
Co-variance ( Σk), and Mixing coefficient ( πk).
GMMclusteringbeginsbyrandominitiationofparametersforK
components. Then, Expectation-Maximization (EM) algorithm [ 35]
is employed forparameter estimation. EM is aniterative process to
train the parameters and has two steps. In the expectation step, an
467
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. Reducing Interactive Refactoring Effort via Clustering-Based Multi-objective Search ASE ’18, September 3–7, 2018, Montpellier, France
Algorithm 2: Pareto-front Clustering
Input :Pareto-front solutions (S)
Output:Labeled solutions (LS),
Clusters Representative Solution (CR)
1beginCalculate best number of clusters-K
2fori←2to10do
3 LS = GMMClustering (i, S);
4 Score i=CalinskiHarabaszIndex(LS);
5K←MaxScoreIdx();
6begin GMMClustering (K,S)
7μk,Σk,πk←Initialize-K-Gaussian();
/* Expectation-Maximization */
8while¬convergedo
9 γ(snk)←Expectation();
10 μk,Σk,πk←Maximization();
11 EvaluateLikelihood();
12foreachsn∈Sdo
/* assigning cluster labels */
13 Ln←MaxResponsibilityIdx( sn);
/* Find Clusters Representative */
14foreachClusterCkdo
15 CRk←MaxDensity( snk∈Ck)
16ReturnLS, CR;
assignment score to each Gaussian distribution, called "responsibil-
ity" or "membership weight", is determined for each solution point
as follow:
γ(znk)=πkN(sn|μk,Σk)
/summationtext.1K
i=1πiN(sn|μi,Σi)(3)
The responsibility coefficient will be used later for preference
extractionstep.Inthemaximizationstep,theparametersofeach
Gaussian are updated using the computed responsibility coeffi-
cients.
3.3 Phase 3: Developers Interaction and
Preferences Extraction
Our tool presents the results of clustering-based multi-objective
refactoring in a user-friendly way via interactive colored graphical
charts and tables as shown in Figure 2.
The developer has the ability to explore the recommended so-
lutions and clusters efficiently and discover the shared underlying
characteristicsofthesolutionsinaclusterataglance.Theusermay
only investigate the cluster’s center solution or search further and
examinethesolutionsinsideaclusterofinterest.Everyrefactoring
operation can be evaluated by the programmer. As described in
Algorithm 3,Wetranslateeachevaluationfeedbacktoacontinuous
score in the range of [-1,1].
The user can interact with the tool at the solution level by ac-
cepting / rejecting / modifying specific refactoring or the cluster
Figure 2: Interactive solution charts in our tool.
Algorithm 3: Interaction and User Preferences
Input :Labeled solutions (LS)
Output:Preferred Cluster (PC),Preference Parameters=[CWP(Classes Weighted Probability,RWP(Refactorings Weighted Probability),RS(Reference Solution)]
beginUser Interaction and Feedback
while¬interaction is done do
Feedback i←UserEvaluation( Refi);
Vi←Score(Feedback i);
/* Solutions and clusters score */
Score si←Average(Vi∈si);
Score ck←Average(Score si∈ck);
PC←cluster with Max score;
beginUser Preference Extraction
/* Representative solution as reference */
RS←CRPC;
foreach[refi,cli]∈PCdo
RWP p←AverageWeightedFreq( refp);
CWP q←AverageWeightedFreq( clq);
ReturnPC, Preference Parameters[];
level by specifying a specific cluster as the region of interest. Af-
tertheinteractionisdoneandtheuserdecidestocontinuetothe
next round, the score of each solution and cluster are computed.
Solution score ( Score si) is defined as the average of all refactoring
operations score exists in the solution vector. Similarly, Cluster
score (Score ck) is calculated as the average of all solutions score
assignedtothecluster.Then,theclusterachievedthehighestscore
468
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Vahid Alizadeh and Marouane Kessentini
amongallclustersisconsideredastheuserpreferredpartitionin
Pareto-frontspacefromwhichthepreferenceparameterswillbe
extracted.
The next step of phase 3 of our proposed approach is to extract
user preference parameters from the interaction step. We consider
the representative solution of the preferred cluster as the reference
point.Then,wecomputetheweightedprobabilityofrefactoring
operations ( RWP) and target classes of the source code ( CWP).
Notethatonlythenameofrefactoringactionwithoutitsassociated
controlling parameters is matched. Assuming the selected cluster’s
index isj, these parameters can be computed as follow:
RWP p=/summationtext.1
si∈cjγij×(|rp∈si|)
/summationtext.1
rm∈Ref/summationtext.1
si∈cjγij×(|rm∈si|)(4)
CWP q=/summationtext.1
si∈cjγij×(|clq∈si|)
/summationtext.1
clm∈Cls/summationtext.1
si∈cjγij×(|clm∈si|)(5)
wheresiis the solution vector, γijis the membership coefficient
ofsolutionitotheclusterj, risrefactoringaction, Refistheset
ofall refactoringoperations,and Clsisthe setofall classesinthe
source code.
3.4 Applying Preference Parameters
If the user decides to continue the search process, then the prefer-
enceparameterswillbeappliedduringtheexecutionofdifferent
components of multi-objective optimization as described in the
following:
•Preference-basedinitialpopulation :Thesolutionsfrompre-
ferred clusters will make up the initial population of next
iteration as a means of customized search starting point. In
thisway,weinitiatethesearchfromtheregionofinterest
ratherthanrandomly.Newsolutionsneedtobegenerated
tofillandachievethepre-definedpopulationsize.Insteadof
randomcreationoftherefactoringoperations(refactoring
actionandtargetclass)basedonaunifyprobabilitydistribu-
tion, we utilize RWPandCWPas a probability distribution.
•Preference-based mutation : For this operator, similarly, if a
solution is selected to mutate, we give a higher chance to
refactoring operations of interest to replace the chosen one
based on the probability distribution RWP.
•Preference-based selection : the selection operator tends to
filterthepopulationandassignhigherchancetothemore
valuableones based ontheir fitnessvalues. Inorder tocon-
sidertheuserpreferencesinthisprocess,weadjustedthis
operator to include closeness to the reference solution as an
added measure of being a valuable individual of the popula-
tion.Thatmeans thechanceofselectionis relatedtoboth
fitness values and distance to the region of interest as:
Chance(si)∝1
dist(si,CRj),Fitness(si) (6)
wheredist()indicates Euclidean distance and CRjis the
representative solution of cluster j.
The above-mentioned customized operators aid to keep the sto-
chasticnatureoftheoptimizationprocessandatthesametimetakeTable 1: Statistics of the studied systems.
System Release #Classes KLOC
ArgoUML v0.3 1358 114
JHotDraw v7.5.1 585 25
GanttProject v1.11.1 245 49UTest v7.9 357 74Apache Ant v1.8.2 1191 112Azureus v2.3.0.6 1449 117
the user preferred refactoring and target code locations (classes)
into account.
4 EVALUATION
In this section, we first present our research questions and val-
idation methodology followed by experimental setup. Then, wedescribe and discuss the obtained results. The data of our exper-iments including a tool demo and the complete statistical results
can be found in the following link [1].
4.1 Research Questions
Wedefinedthreemainresearchquestionstomeasurethecorrect-
ness, relevance and benefits of our interactive clustering-based
multi-objective refactoring tool comparing to existing approaches
thatarebasedoninteractivemutli-objectivesearch[ 29],fullyauto-
mated multi-objective search (Ouni et al.) [ 34] and fully automated
deterministic tool not based on heuristic search (JDeodorant) [ 16].
The research questions are as follows:
•RQ1: Refactorings relevance. To what extent can our ap-
proach make meaningful recommendations compared to
existing refactoring techniques?
•RQ2: Interactive clustering relevance. To what extent
can our clustering-based approach efficiently reduce the
interaction effort?
•RQ3: Impact. How do programmers evaluate the useful-
ness of our tool (questionnaire)?
4.2 Experimental Setup
Toaddressthedifferentresearchquestions,weusedthesixsystems
inTable1.Weselectedthesesixsystemsbecauseoftheirsize,have
been actively developed over the past 10 years and extensively
analyzedbythe competitivetoolsconsideredin thiswork.UTest1
is a project of our industrial partner used for identifying, reportingand fixing bugs. We selected that system for our experiments since
threeprogrammersofthatsystemagreedtoparticipateintheex-
perimentsandtheyareveryknowledgeableaboutrefactoringsincetheyarepartofthemaintenanceteam.Table 1providesinformation
aboutthesizeofthesubjectsystems(intermsofnumberofclasses
and KLOC).
ToanswerRQ1,weaskedagroupof14activeprogrammersto
identifyandmanuallyevaluatetherelevanceofthebestrefactor-
ings sequence that they found using four tools. These tools are
ourIC-NSGA-IIapproach,anexistinginteractivemulti-objective
refactoring tool [ 29] (without the clustering feature) and two fully-
automatedrefactoringtoolsbythemeansofOuni[ 34]andJDeodor-
ant [16]. Ouni [34] proposed a multi-objective refactoring formula-
tionbasedonNSGA-IIthatgeneratesasolutiontomaximizethedesign coherence and refactorings reuse from previous releases.
1Company anonymized for double-blind.
469
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. Reducing Interactive Refactoring Effort via Clustering-Based Multi-objective Search ASE ’18, September 3–7, 2018, Montpellier, France
Table 2: Selected programmers.
System #Subjects Avg. Prog. Exp. Avg. Refactoring Exp.
ArgoUML 4 10 High
JHotDraw 4 11.5 Very High
Azureus 4 9 Medium
GanttProject 4 10.5 High
UTest 7 13.5 Very High
Apache Ant 4 12 Very High
JDeodorant [ 16] is an Eclipse plugin to detect bad smells and apply
refactorings.AsJDeodorantsupportsalowernumberofrefactoring
typeswithrespecttotheonesconsideredbyourtool,werestrictour
comparisonwithittotheserefactorings.Mkaouer[ 29]proposed
a tool for interactive multi-objective refactoring but the interac-tions were limited to the refactorings (accept/reject) and there is
no clustering of the Pareto front or learning mechanisms from the
interaction data. We used these three competitive tools to eval-uate the benefits of the clustering feature in helping developers
identifying relevant refactorings.
We preferred not to use the antipatterns and internal quality
indicatorsasproxiesforestimatingtherefactoringsrelevancesince
wethedevelopersmanualevaluationalreadyincludesthereview
oftheimpactofsuggestedchangesonthequality.Furthermore,notall the refactorings that improve any quality attributes are relevant
to the developers, which is one of the main motivations of this
work.Theonlyrigorouswaytoevaluateourtherelevanceofour
tool is the manual evaluation of the results by active developers.
Participantswerefirstaskedtofilloutapre-studyquestionnaire
containingfivequestions.Thequestionnairehelpedtocollectback-
ground information such as their role within the company, their
programming experience, and their familiarity with software refac-
toring. In addition, all the participants attended one lecture of two
hoursonsoftwarerefactoringbytheorganizersoftheexperiments.
ThedetailsoftheselectedparticipantscanbefoundinTable 2in-
cludingtheirprogrammingexperience,familiaritywithrefactoring,etc.Eachparticipantwasaskedtoassessthemeaningfulnessofthe
refactorings recommended after using two out of the four tools on
two different systems to avoid the training threat. The participants
did not only evaluate the suggested refactorings but were asked to
configure, run and interact with the tools on the different systems.
The only exceptions are related to the participants from the indus-
trial partner where only two out of the three agreed to evaluatean additional system to UTest while the third only reviewed the
refactoring recommendations on the industrial software. Thus, the
totalnumberofevaluationsofthedifferenttoolsis27.Weassigned
thetaskstotheparticipantsaccordingtothestudiedsystems,the
techniques to be tested and developers’ experience. Each of the
fourtoolshasbeenevaluatedatleastonetimeoneveryofthesix
systems.
To answer RQ2, we measured the time (T ) that developers spent
toidentifythebestrefactoringstrategiesbasedontheirpreferencesandthenumberofrefactorings(NR ).Furthermore,wequalitatively
evaluatedtheimpactoftheinteractionswiththeusersonthePareto
fronttobetterconvergetowardsa"regionofinterests"reflecting
their preferences. For this research question, we decided to limitthe comparison to only the interactive multi-objective work of
Mkaoueretal.[
29]sinceitistheonlyonethatoffersinteraction
with the users and it will help us understand the real impact of
the clustering feature (not supported by [ 29]) on the refactoring
recommendations and interaction effort.ToanswerRQ3,weaskedtheparticipantstouseourtoolduring
aperiodoftwohoursonthedifferentsystemsandthenwecollected
theiropinionsbasedonapost-studyquestionnaire.Tobetterun-
derstandsubjects’opinionswithregardtousefulnessandusablility
of our approach in a real setting, the post-study questionnaire was
given to each participant after completing the refactoring tasks
usingourinteractiveapproachandallthetechniquesconsidered
inourexperiments.Thequestionnairescollectedtheopinionsof
the participants about their experience in using our tool compared
to existing manual, interactive and fully-automated refactoring
techniques.
4.3 Statistical Tests and Parameters Setting
We used one-way ANOVA statistical test with a 95% confidence
level (α= 5%) to find out whether our sample results of different
approaches are different significantly. Since one-way ANOVA is an
omnibus test, A statistically significant result determines whether
threeormoregroupmeansdifferinsomeundisclosedwayinthe
population.One-wayANOVAisconductedfortheresultsobtained
from each software project to investigate and compare each per-
formancemetric(dependentvariable)betweenvariousstudiedal-
gorithms(independentvariable).Wetestthenullhypothesis( H0)
that population means of each metric are equal for all methods
against the alternative ( H1) that they are not all equal and at least
one method population mean is different.
One-way ANOVA does not report the size of the difference.
Therefore,wecalculatedtheVargha-DelaneyAmeasure[ 36]which
isameasureoftheeffectsize(strengthofassociation)anditesti-
matesthedegreeofassociationbetweentheindependentfactorand
dependent variable forthe sample. Eta squared is the proportion
ofthetotalvariancethatisattributedtoafactor(the"refactoring
methods" in this study).
Adetaileddescriptionofthestatisticaltestsresultscanbefound
in this link [1].
Parameter setting influences significantly the performance of
a search algorithm on a particular problem [ 3]. For this reason,
for each algorithm and for each system, we perform a set of ex-
periments using several population sizes: 50, 100, 150, 200, 250and 30. The stopping criterion was set to 100,000 evaluations forall search algorithms in order to ensure fairness of comparison
(without counting the number of interactions since it is part of the
users decision to reach the best solution based on his preferences).
The other parameters’ values were fixed by trial and error and are
asfollows:crossoverprobability=0.6;mutationprobability=0.5
where theprobability of genemodification is 0.4. Inorder to have
significant results, for each couple (algorithm, system), we use the
trial and error method [ 20] in order to obtain a good parameter
configuration.
4.4 Results
ResultsforRQ1:Refactoringsrelevance. Wereporttheresults
ofourempiricalqualitativeevaluation(MC)inFigure 3basedon
themanualcheckingofthebestsolutionsidentifiedbyeachtool.
As reported in this figure, the majority of the refactoring solu-
tionsrecommendedbyourinteractiveclustering-basedapproach
470
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Vahid Alizadeh and Marouane Kessentini
Figure3:Themedianmanualevaluationscores,MC,onthe
six systems with 95% confidence level ( α= 5%) based on a
one-way ANOVA statistical test
were correct and validated by the participants on the different sys-
tems. On average, for all of our ten studied projects, 86% of theproposed refactoring operations are considered as semantically
feasible, improve the quality and are found to be useful by the soft-
ware developers of our experiments. The remaining approaches
have an average of 70%, 63% and 52% respectively for Mkoaueret al. (interactive multi-objective approach), Ouni et al. (fully au-
tomatedmulti-objectiveapproach)andJDeodorant(deterministic
non-search based approach).The highest MC score is 93% for theGantt project and the lowest score is 80% for JHotDraw. Thus, it
isclearthattheresultsare independent ofthesizeofthesystems
andthenumberofrecommendedrefactoringsasdetailedinRQ2
aswell.Bothoftheinteractivetoolsoutperformedfully-automated
ones which shows the importance of integrating the human in the
loopwhenrefactoringasystem.Furthermore,itisclearthatadding
the clustering feature to enable the developers to select a region of
interests based on which quality objectives they want to prioritize
and what refactoring solutions they partially liked.
A qualitative analysis of the results show that several interac-
tions with the developers helped to reduce the search space by
avoidingtherefactoringsthatwererejectedbythemandtheirlo-
cation. We found that the best final refactoring solutions identified
bythedevelopersafterseveralinteractionswithourtoolcannotbe
recommended by the remaining approaches. In fact, all these solu-
tionsareobtainedeitherafter1)eliminatingrefactoringsapplied
to specific code locations not relevant to the programmers’ context
(somethingthatcannotbelearnedwiththeinteractioncomponent)
or2)emphasizingspecificclusterthatprioritizessomeobjectives
andpenalizes others.For instance,the developersfrom theindus-
trialpartnerfoundseveraloftherefactoringsthatarerecommended
by Ouni et al. and JDeodorant as non relevant, while they could be
correct,becauseitmayrefactorastablecodeorclassesthatarenot
of their interest to be refactored.
All the results based on the MC metric on the different systems
werestatisticallysignificantwith95%ofconfidencelevel.Regarding
theeffectsize,wefoundthatourapproachisbetterthanalltheother
algorithms with an A effect size higher than 0.92 for ArgoUML,GanttProject,UTestandApacheAnt;andanAeffectsizehigher
than 0.83 for JHotDraw and Azureus.Table 3: Median time, in minutes, and number of refactor-
ings proposed by both interactive approaches on the differ-ent six systems
Techniques
Systems IC-NSGA-II (T,NR) Mkaouer et al. (T,NR)
ArgoUML 100 29 124 34
JHotDraw 25 27 67 52
Azureus 70 24 125 35
GanttProject 36 30 86 39
UTest 46 52 83 75
Apache Ant 51 26 147 35
Results for RQ2: Interactive clustering relevance. Table3
summarizesthetime,inminutes,andthenumberofrefactorings
in the most relevant solution found using our tool, IC-NSGA-II,
and the interactive approach of Mkaouer et al. [ 29]. All the partici-
pants spent less time to find the most relevant refactorings on the
different systems comparing to Mkaouer et al. [ 29]. For instance,
the average time is reduced by over 60% for the case of Apache
Ant from 147 minutes to just 51 minutes. The time includes the
execution of IC-NSGA-II and the different phases of interactionuntil that the developer is satisfied with a specific solution. It isclear as well that the time reduction is not correlated with the
numberofrecommendedrefactorings.Forinstance,thedeviation
betweenIC-NSGA-IIandMkaoueretal.forApacheAntintermsof
number of recommended refactorings is limited to 9 (26 vs 35) but
the time reduction is almost 100 minutes. However, it is clear that
ourapproachreducedaswellthenumberofrecommendedrefac-
toringscomparingtoMkaoueretal.whileincreasingthemanual
correctnessasdescribedinRQ1.Thehighestnumberofrefactorings
wasobservedontheindustrialsystemwith52refactoringsusing
IC-NSGA-II and 75 refactorings using Mkaouer et al. This couldbe explained by the fact that the original developers can betterunderstand the possible relevance of the recommended refactor-
ings comparing the remainingparticipants’ evaluation on the open
source systems.
Figure4showsa qualitative example extracted fromour exper-
iments using IC-NSGA-II on the Gantt project with a populationsize of 100 based on three phases of interactions. After the gen-
erationoftheParetofront,theclusteringfeatureidentifiedthree
main different clusters for the two objectives selected by the devel-
oper (extendibility and effectiveness). During the first phase, the
developerselectedtheclusterwithid0asthepreferredoneafter
exploring several refactoring solutions in that cluster includingthe center of the cluster. Thus, the next iterations of IC-NSGA-IIprioritized that "region of interest" so more refactoring options
were generated around the previously selected cluster. Then, since
theuserselectedagainaclustermaximizingthesetwoobjectives
(clusterwithid1)morerefactoringoptionsinthenextiterations
until that a good refactoring sequence is selected.
Results for RQ3: Impact. We summarize in the following the
feedback of the developers based on the post-study questionnaire.
12outthe14participantsmentionthatourinteractiveclustering-
based refactoring tool is faster and much easier to use than the
471
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. Reducing Interactive Refactoring Effort via Clustering-Based Multi-objective Search ASE ’18, September 3–7, 2018, Montpellier, France
Figure4:Illustrationoftherefactoringsolutionsconvergencetoaregionofinterestaftertworoundsofinteractionsextracted
from the experiments on the Gantt Project.
interactive multi-objective tool of Mkaouer et al. [ 29] to identify
quickly relevant refactorings based on their interests. For instance,
the comment of one participant is the following : "I believe the
addition of the clustering algorithm really helped identify a solution
quicker.Itwasdifficulttodecidebetweensimilarrefactoringsolutionsusingthenon-clusteringversionofthetool.Theclustercentershelped
focus the attention to just a few solutions, which were easy to choose
between." Asimilarobservationisvalidwhencomparingourtool
to the fully-automated multi-objective refactorings tool of Ouni et
al. [34] where 9 out of the 14 participants highlighted the difficulty
to select one relevant refactoring solution from a large set of non-
dominated solutions and without offering any flexibility to update
them.Oneexampleofreceivedcommentsis "Themainadvantageof
this tool is instead of looking so many refactoring solutions manually
this tool helps us to find the best solution based on objective selectingthe center of the different clusters which provide the good refactoring
recommendations."
All the developers mentioned the high usability of the tool and
the different options that are offered comparing to deterministic
tools like JDeodorant. In addition, they did not appreciate a lot the
long list of refactoring suggested by Ouni et al. and JDeodorantsince they want to take control of modifying and rejecting some
refactorings. In addition, the validation of this long list of refactor-
ingsistime-consuming.Thus,theyappreciatethatourtoolsuggests
refactoringonebyoneandupdatethelistbasedonthefeedback
ofdevelopers.13participantscommentedontheminimumeffort
required to understand the impact of the proposed refactorings on
the quality and to identify a relevant solution using the clusters
comparingallthethreeremainingtools: "Refactoringwithclustering
reduces the time of the analysis of the objectives. It keeps the similar
type of classes or patterns in the same cluster and dissimilar patternsinanothercluster." Alltheparticipantsfoundaswellourtoolhelpful
forbothflossrefactoring,tomaintainagoodqualitydesignandalso
forroot canal refactoring to fix some quality issues such as code
smells.
5 THREATS TO VALIDITY
Conclusion validity . The parameter tuning of the different op-
timization algorithms used in our experiments creates anotherinternal threat that we need to evaluate in our future work. The
parameters’ values used in our experiments are found by trial-and-
error[19].However,itwouldbeaninterestingperspectivetodesign
an adaptive parameter tuning strategy [ 2] for our approach so that
parametersareupdatedduringtheexecutioninordertoprovide
the best possible performance.
Internal validity. The variation of correctness and speed be-
tween the different groups when using our approach and other
toolssuchasJDeodorant.Infact,ourapproachmaynotbetheonly
reason for the superior performance because the participants have
different programming skills and familiarity with refactoring tools.
To counteract this, we assigned the developers to different groups
according to their programming experience so as to reduce thegapbetweenthedifferentgroupsandwealsoadaptedacounter-
balanceddesign.Regardingtheselectedparticipants,wehavetaken
precautionsto ensurethatour participantsrepresenta diverseset
ofsoftwaredeveloperswithexperienceinrefactoring,andalsothat
the groups formed had, in some sense, a similar average skill set in
the refactoring area.
Construct validity. The different developers involved in our
experiments may have divergent opinions about the recommended
refactorings in terms of relevance which may impact our results.
472
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Vahid Alizadeh and Marouane Kessentini
External validity. The first threat is the limited number of
participants and evaluated systems, which externally threatens the
generalizability of our results. In addition, our study was limited
totheuseofspecificrefactoringtypes.Futurereplicationsofthis
study are necessary to confirm our findings.
6 RELATED WORK
Hall et al. [ 18] treated software modularization as a constraint sat-
isfaction problem. The idea of this work is to provide a baseline
distribution of software elements using good design principles (e.g.
minimal coupling and maximal cohesion) that will be refined by
asetofcorrectionsintroducedinteractivelybythedesigner.The
approach,calledSUMO(SupervisedRe-modularization),consists
of incrementally feeding domain knowledge into the remodular-
ization process. The process is performed by the designer in terms
of constraints that can be introduced to refine the current modu-
larizations.Initially,thesystembeginswithgeneratingamodule
dependencygraphfromaninputsystem.Thisdependencyisbased
onthecorrelationbetweensoftwareelements(couplingbetween
methods, shared attributes etc.). Possible modularizations are then
generatedfromthegraphusingmultiplesimulatedauthoritative
decompositions.Then,usingaclusteringtechniquecalledBunch,
an initial set of clusters is generated that serves as an input to
SUMO. The SUMO algorithm provides a hypothesized modulariza-
tiontotheuser,whowillagreewithsomerelations,anddisagree
with others. The user’s corrections are then integrated into the
modularization process, to generate a better modularization.
Dig [13] proposes an interactive refactoring technique to im-
prove the parallelism of software systems. However, the proposed
approach did notconsider learning from thedevelopers’ feedback
and focused on making programs more parallel. Bavota et al. [ 6]
presented the adoption of single objective interactive genetic algo-
rithms in software re-modularization process. The main idea is to
incorporate the user in the evaluation of the generated remodular-
izations.InteractiveGeneticAlgorithms(IGAs)extendtheclassic
Genetic Algorithms (GAs) by partially or entirely involving the
user in the determination of the solution’s fitness function. The
basic idea of the Interactive GA (IGA) is to periodically add a con-
straint to the GA such that some specific components shall be put
inagivenclusteramongthosecreatedsofar.Afteranalyzingthe
current modularization solutions, the user provides feedback in
terms of constraints dictating for example, that a specific element
needs to be in the same cluster as another one. Although user feed-
backisimportantinguaranteeingconvergence,itisessentialnot
tooverloadtheuserbyaskingforadecisionaboutallthecurrent
relationships between elements, especially for a large system.
Arecentstudy[ 25]extendedapreviouswork[ 29]toproposean
interactivesearchbasedapproachforrefactoringrecommendations.
The developers have to specify a desired design at the architecture
level then the proposedapproach try to find therelevant refactor-
ings that can generate a similar design to the expected one. In our
work, we do not consider the use of a desired design, thus develop-
ersarenotrequiredtomanuallymodifythecurrentarchitectureof
thesystemtogetrefactoringrecommendations.Furthermore,de-
velopers maybe interested to change the architecture mainly whenthey want to introduce an extensive number of refactorings that
radically change the architecture to support new features.
Noneoftheaboveinteractivestudiesconsideredreducingthe
interaction effort with developers which is an important step to
improve the applicability of refactoring tools as highlighted in the
survey with developers.
7 CONCLUSIONS AND FUTURE WORK
Weproposed,inthispaper,aninteractiveclustering-basedrecom-
mendationtoolforsoftwarerefactoringthatreducestheeffortof
improving the quality of software systems. The exploration of the
non-dominated refactoring solutions is implicitly performed based
ontheinteractionwiththedevelopers.Thefeedbackreceivedfrom
the developers and the clustering of non-dominated refactoring
solutionsareusedtoreducethesearchspaceandconvergetobetter
solutions.Toevaluatetheeffectivenessofourtool,weconductedanevaluationwith14softwaredeveloperswhoevaluatedthetooland
compareditwiththestate-of-the-artrefactoringtechniques.Our
evaluationresultsprovidestrongevidencethatourtoolimproves
the applicability of software refactoring, and proposes a novel way
for software developers to refactor their systems interactively with
reasonable effort.
Futureworkinvolvesvalidatingourtechniquewithadditional
refactoring types, programming languages and programmers in or-
dertoconcludeaboutthegeneralapplicabilityofourmethodology.Furthermore,weonlyfocused,inthispaper,ontherecommendation
of refactorings. We plan to extend the interactive clustering-based
approach to others related software maintenance problems such
asregressiontestingandbugslocalization.Wewillalsoworkon
making the refactoring recommendations more personalized based
on the profile of programmers by learning their preferences.
REFERENCES
[1][n. d.]. REDUCING INTERACTIVE REFACTORING EFFORT VIA CLUSTERING-
BASED MULTI-OBJECTIVE SEARCH. https://sites.google.com/view/ase2018
[2]AndreaArcuriandLionelBriand.2011. Apracticalguideforusingstatisticaltests
toassessrandomizedalgorithmsinsoftwareengineering.In SoftwareEngineering
(ICSE), 2011 33rd International Conference on. IEEE, 1–10.
[3]Andrea Arcuri and Gordon Fraser. 2013. Parameter tuning or default values? An
empiricalinvestigationinsearch-basedsoftwareengineering. EmpiricalSoftware
Engineering 18, 3 (2013), 594–623.
[4]J. Bansiya and C.G. Davis. 2002. A hierarchical model for object-oriented designquality assessment. IEEE Transactions on Software Engineering 28, 1 (2002), 4–17.
https://doi.org/10.1109/32.979986 arXiv:arXiv:1011.1669v3
[5]DonBatory,JacobN.Sarvela,andAxelRauschmayer.2003. ScalingStep-Wise
Refinement. In Proc. 25th. 187–197.
[6] Gabriele Bavota, Filomena Carnevale, Andrea De Lucia, Massimiliano Di Penta,
andRoccoOliveto.2012. Puttingthedeveloperin-the-loop:aninteractiveGAfor
softwarere-modularization.In InternationalSymposiumonSearchBasedSoftware
Engineering. Springer, 75–89.
[7]Gabriele Bavota, Andrea De Lucia, Andrian Marcus, Rocco Oliveto, and Fabio
Palomba.2012. SupportingextractclassrefactoringinEclipse:theARIESproject.
In34thInternationalConferenceonSoftwareEngineering(ICSE).IEEEPress,1419–
1422.
[8]Yuanfang Cai, Rick Kazman, Ciera Jaspan, and Jonathan Aldrich. 2013. Intro-
ducing Tool-Supported Architecture Review into Software Design Education. In
Proc. 26th.
[9]Yuanfang Cai and Kevin Sullivan. 2012. A formal model for automated software
modularity and evolvability analysis. 21, 4 (2012), 21.
[10]Yuanfang Cai and Kevin J. Sullivan. 2006. Modularity Analysis of Logical Design
Models. In Proc. 21st. 91–102.
[11]TadeuszCalińskiandJerzyHarabasz.1974.Adendritemethodforclusteranalysis.
Communications in Statistics-theory and Methods 3, 1 (1974), 1–27.
[12]Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and T. Meyarivan. 2002. A
fast andelitist multiobjective genetic algorithm:NSGA-II. IEEE Transactions on
473
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. Reducing Interactive Refactoring Effort via Clustering-Based Multi-objective Search ASE ’18, September 3–7, 2018, Montpellier, France
Evolutionary Computation 6, 2 (2002), 182–197. https://doi.org/10.1109/4235.
996017
[13]Danny Dig. 2011. A refactoring approach to parallelism. IEEE software 28, 1
(2011), 17–22.
[14]DannyDigandRalphJohnson.2006. AutomatedDetectionofRefactoringsin
Evolving Components. In Proc.404–428.
[15]Bart Du Bois, Serge Demeyer, and Jan Verelst. 2004. Refactoring-improving
couplingandcohesionofexistingcode.In 11thWorkingConferenceonReverse
Engineering (WCRE). 144–151.
[16]MariosFokaefs,NikolaosTsantalis,EleniStroulia,andAlexanderChatzigeorgiou.
2011. JDeodorant: identificationand application of extract class refactorings. In
Proceedingsofthe33rdInternationalConferenceonSoftwareEngineering.ACM,
1037–1039.
[17]Martin Fowler and Kent Beck. 1999. Refactoring: improving the design of existing
code. Addison-Wesley Professional.
[18]Mathew Hall, Neil Walkinshaw, and Phil McMinn. 2012. Supervised software
modularisation. In Software Maintenance (ICSM), 2012 28th IEEE International
Conference on. IEEE, 472–481.
[19]Mark Harman, S Afshin Mansouri, and Yuanyuan Zhang. 2012. Search-based
software engineering: Trends, techniques and applications. ACM Computing
Surveys (CSUR) 45, 1 (2012), 11.
[20]RobertRJackson,ChrisMCarter,andMichaelSTarsitano.2001. Trial-and-error
solvingofaconfinementproblembyajumpingspider,Portiafimbriata. Behaviour
138, 10 (2001), 1215–1234.
[21]WaelKessentini,MarouaneKessentini,HouariSahraoui,SlimBechikh,andAli
Ouni. 2014. A cooperative parallel search-based software engineering approach
for code-smells detection. IEEE Transactions on Software Engineering 40, 9 (2014),
841–861.
[22]JongwookKim,DonBatory,DannyDig,andMaiderAzanza.2016. Improving
refactoringspeedby10x.In Proceedingsofthe38thInternationalConferenceon
Software Engineering. ACM, 1145–1156.
[23]MiryungKim,MatthewGee,AlexLoh,andNapolRachatasumrit.2009.Ref-finder:
a refactoring reconstruction tool based on logic query templates. In Proceedings
of the International Symposium on Foundations of Software Engineering (FSE).
371–372.
[24]Yun Lin, Xin Peng, Yuanfang Cai, Danny Dig, Diwen Zheng, and Wenyun Zhao.
[n. d.]. Interactive and Guided Architectural Refactoring with Search-Based
Recommendation. In Proc. 24th.
[25]Yun Lin, Xin Peng, Yuanfang Cai, Danny Dig, Diwen Zheng, and Wenyun Zhao.
2016. Interactive and guided architectural refactoring with search-based recom-
mendation.In Proceedingsofthe201624thACMSIGSOFTInternationalSymposiumon Foundations of Software Engineering. ACM, 535–546.
[26]Radu Marinescu. 2004. Detection strategies: metrics-based rules for detecting
designflaws.In 20thInternationalConferenceonSoftwareMaintenance(ICSM).
350–359.
[27]Mohamed Wiem Mkaouer, Marouane Kessentini, Slim Bechikh, Kalyanmoy Deb,
and Mel Ó Cinnéide. 2014. Recommendation system for software refactoring
usinginnovizationandinteractivedynamicoptimization.In Proceedingsofthe
29thACM/IEEEinternationalconferenceonAutomatedsoftwareengineering.ACM,
331–336.
[28]Mohamed Wiem Mkaouer, Marouane Kessentini, Slim Bechikh, Kalyanmoy Deb,
andMelÓCinnéide.2014. RecommendationSystemforSoftwareRefactoring
Using Innovization and Interactive Dynamic Optimization. In Proceedings of the
International Conference on Automated Software Engineering. 331–336.
[29]Mohamed Wiem Mkaouer, Marouane Kessentini, Slim Bechikh, Kalyanmoy Deb,
and Mel Ó Cinnéide. 2014. Recommendation system for software refactoring
using innovization and interactive dynamic optimization. Proceedings of the 29th
ACM/IEEE international conference on Automated software engineering - ASE ’14
(2014), 331–336. https://doi.org/10.1145/2642937.2642965
[30]Emerson Murphy-Hill, Chris Parnin, and Andrew P. Black. 2009. How We
Refactor, and How We Know It. In Proceedings of the International Conference on
Software Engineering. 287–297.
[31]William F Opdyke. 1992. Refactoring object-oriented frameworks . Ph.D. Disserta-
tion. University of Illinois at Urbana-Champaign.
[32]Ali Ouni, Marouane Kessentini, Houari Sahraoui, and Mounir Boukadoum. 2012.
Maintainability defects detection and correction: a multi-objective approach.
Automated Software Engineering 20, 1 (2012), 47–79.
[33]Ali Ouni, Marouane Kessentini, Houari Sahraoui, Katsuro Inoue, and Kalyanmoy
Deb. 2016. Multi-criteria code refactoring using search-based software engineer-
ing: An industrial case study. ACM Transactions on Software Engineering and
Methodology (TOSEM) 25, 3 (2016), 23.
[34]Ali Ouni, Marouane Kessentini, Houari Sahraoui, Katsuro Inoue, and Kalyanmoy
Deb. 2016. Multi-criteria code refactoring using search-based software engineer-
ing: An industrial case study. ACM Transactions on Software Engineering and
Methodology (TOSEM) 25, 3 (2016), 23.
[35]Richard A Redner and Homer F Walker. 1984. Mixture densities, maximum
likelihood and the EM algorithm. SIAM review 26, 2 (1984), 195–239.
[36]András Vargha and Harold D Delaney. 2000. A critique and improvement of
the CL common language effect size statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (2000), 101–132.
474
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:59 UTC from IEEE Xplore.  Restrictions apply. 