Supporting Analysts by Dynamic Extraction and ClassiÔ¨Åcation of
Requirements-Related Knowledge
Zahra Shakeri Hossein Abad‚àó, Vincenzo Gervasi‚Ä†, Didar Zowghi‚Ä°, Behrouz H. Far¬ß
‚àóDepartment of Computer Science, University of Calgary, Calgary, Canada zshakeri@ucalgary.ca
‚Ä†Department of Computer Science, University of Pisa, Italy, gervasi@di.unipi.it
‚Ä°School of Software, University of Technology Sydney, Australia, didar.zowghi@uts.edu.au
¬ßSchulich School of Engineering, University of Calgary, Calgary, Canada, far@ucalgary.ca
Abstract ‚ÄîIn many software development projects, analysts are
required to deal with systems‚Äô requirements from unfamiliar
domains. Familiarity with the domain is necessary in order
to get full leverage from interaction with stakeholders and
for extracting relevant information from the existing project
documents. Accurate and timely extraction and classiÔ¨Åcation
of requirements knowledge support analysts in this challenging
scenario. Our approach is to mine real-time interaction records
and project documents for the relevant phrasal units about the
requirements related topics being discussed during elicitation. We
propose to use both generative and discriminating methods. To
extract the relevant terms, we leverage the Ô¨Çexibility and power
of Weighted Finite State Transducers (WFSTs) in dynamic mod-
elling of natural language processing tasks. We used an extended
version of Support Vector Machines (SVMs) with variable-sized
feature vectors to efÔ¨Åciently and dynamically extract and classify
requirements-related knowledge from the existing documents.
To evaluate the performance of our approach intuitively and
quantitatively, we used edit distance and precision/recall metrics.
We show in three case studies that the snippets extracted by
our method are intuitively relevant and reasonably accurate.
Furthermore, we found that statistical and linguistic parameters
such as smoothing methods, and words contiguity and order
features can impact the performance of both extraction and
classiÔ¨Åcation tasks.
Index T erms ‚ÄîRequirements elicitation, Natural Language
Processing, Requirements classiÔ¨Åcation, Weighted Finite State
Transducers, Dynamic language models
I. I NTRODUCTION
In industrial software development, it is not uncommon for
an analyst to be assigned to work on the requirements for
a project whose domain is not totally familiar to them, be it
because the analyst does not have speciÔ¨Åc training or previous
experience with the project domain, or because the analysts
are assigned to an ongoing project. The analyst is faced with
the daunting task of becoming familiar with a possibly large
amount of documentation that has already been produced.
In each case, efÔ¨Åciently interacting with stakeholders (while
lacking familiarity with the domain or with pre-existing project
artifacts) might pose major challenges.
In this paper, we present a suite of innovative automated
techniques aimed at helping the analyst in this scenario. In
particular, we envision a system where real-time interaction
between an analyst and one or more stakeholders is processed
in real-time (we include spoken interaction, via a third-party
speech-transcription utility, and written in-context interaction,
e.g. in chat or quick-turn emails). The system is able to identifya sliding window of active topics in the conversation, and at
any stage, recall from a repository of pre-existing documents,
information that is relevant for eliciting requirements on the
topic at hand. In our scenario, the analyst is assisted in real-
time with extracted snippets of the existing documents, which
provide context and additional information on the topics under
discussion. Such documents might include domain description
documents, existing requirements, pending feature requests,
and so on ‚Äî in a word, the entire menagerie of Requirements
Engineering (RE) artifacts, as long as they are expressed in
Natural Language (NL).
We propose to use both generative models (based on
Weighted Finite State Transducers (WFSTs) [1], [2] and statisti-
cal Language Models (LMs) ) and discriminative models (based
on Kernel methods [3] and Support Vector Machines (SVMs)
[4]) to extract and classify (by F/NF and by type of NFRs)
requirements-relevant knowledge from the existing documents.
In contrast to other approaches, we integrate both techniques
so that information to present to the analyst is selected based
on both its content (i.e. the text detailing the sub-system that is
being discussed), and on its role (i.e. the text dealing with the
robustness requirements on that sub-system). This integration
allows us to improve, at times signiÔ¨Åcantly, over previous
approaches. The main contributions of this paper are:
1) We propose a simple novel and dynamic generative
model based on the concept of lexical association , which
is a quantitative measure of the strength of contextual
association between two or more words in a corpus. This
model leverages the Ô¨Çexibility and efÔ¨Åciency of WFSTs
for dynamic modelling and analysis of the incoming text.
2) By modelling requirements-related documents with WF-
STs, we propose to use an extended version of the SVM
classiÔ¨Åcation approach enriched with non-contiguous
string kernel in the context of requirements classiÔ¨Åcation.
We evaluate our solution to the problem of dynamic ex-
traction and classiÔ¨Åcation of requirements-related knowledge
by means of three experiments on real-world datasets and
compare the results of our classiÔ¨Åcation approach with others
published in the literature. The theoretical basis along with the
experimental results demonstrate that the proposed solution
advances the state of the art in requirements-related term
extraction and classiÔ¨Åcation. Moreover, this is the Ô¨Årst work
that aims to assist analysts by surfacing relevant information
4422019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 ¬©2019 IEEE
DOI 10.1109/ICSE.2019.00057
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. from documental sources during an interactive interview.
In the remainder of this paper, we Ô¨Årst provide some
background on the current state of the art and, in Section
III, formalize the problem we are addressing and establish
needed notations and concepts. Then, we present our technical
contribution in Section IV; this is followed, in Section V, by
an experimental evaluation and comparison. We conclude the
paper by discussing threats to validity, with possible extensions
and suggestions for future work in Section VII.
II. S TATE OF THE ART
To better set the stage for our contribution, in this section,
we review the salient aspects of existing work on extraction
and classiÔ¨Åcation of requirements-relevant terms.
1) Term Extraction: In an early work, Goldin and Berry
[5] proposed AbstFinder, a character-based approach to Ô¨Ånding
non-contiguous common substrings in a corpus, independently
of the order of these substrings. AbstFinder neatly addresses
the non-contingency of content-carrying terms by using circu-
lar shifts, however, it is highly sensitive to the position of terms
in a sentence and does not leverage the contextual information
implicitly contained in a corpus. Since this work, there has
been much active research into corpus-based language learning
and analysis in RE. In particular, Sawyer et al. [6] applied a
Berry-Rogghes z-score [7] to identify frequent n-grams within
a document. However, important and indicative n-grams might
not always occur frequently in a document and consequently
missed by z-scores and similar statistical methods. Gacitua et
al. [8] proposed a Relevance-driven Abstraction IdentiÔ¨Åcation
(RAI) technique which treats documents as a stream of words
and applies the corpora-based frequency proÔ¨Åling approach
[9] to rank a domain document words. To identify and rank
multi-word terms, they used syntactic patterns (e.g. adjec-
tive/nouns, adverb/verbs, and prepositions) and a heuristic
weighting approach in which component words in a multi-
word term are weighted in descending order from the last
word in the term. Likewise, Quirchmayr et al. [10] used lexical
and syntactical (i.e. part of speech tagging) characteristics to
semi-automatically extract Feature-Relevant (FR) information
from natural language user manuals. Following this technique,
analysts Ô¨Årst conduct a manual revision to ensure syntactical
correctness, and then deÔ¨Åne domain-speciÔ¨Åc terms which will
be used to automatically extract FR information (i.e. a clause
compromises at most one subject and one predicate). Lian
et al. [11] proposed an approach to explore and highlight
requirements knowledge from domain documents. They used a
pre-deÔ¨Åned set of search terms and measured their density and
diversity in Ô¨Åxed-length sequences of words (a.k.a windows).
2) Requirements ClassiÔ¨Åcation: There is a growing body of
research investigating the effect of using automatic methods
for requirements classiÔ¨Åcation [12]‚Äì[15]. An efÔ¨Åcient catego-
rization of requirements supports analysts to Ô¨Ålter relevant
information about their ongoing task and enables focused
communication and prioritization of requirements [16], [17].
Verma and Kass [18] proposed a Requirements Analysis
Tool (RAT) to automatically analyze requirements documents.They used a deterministic Ô¨Ånite automata-based approach
to parse and provided a set of user-deÔ¨Åned glossaries and
controlled syntaxes to formulate business rules to standard
(i.e./angbracketleftagent/angbracketright/angbracketleftmodal verb /angbracketright/angbracketleftaction/angbracketright/angbracketleftrest/angbracketright) and conditional (i.e.
/angbracketleftif/angbracketright/angbracketleftcond/angbracketright/angbracketleftthen/angbracketright/angbracketleftrest/angbracketright) requirements. They leveraged a pre-
deÔ¨Åned NFR classiÔ¨Åcation and the Web Ontology Language
(OWL) to model and query requirement‚Äôs relationships and se-
mantics. However, phrasal and semantic analysis steps of this
approach need a substantial manual preprocessing effort (e.g.
writing sentences in a formalized fashion), which makes them
highly dependent on the syntactical form of the requirements.
Cleland-Huang et al. [19], [20] proposed an algorithm that
uses the term and inverse document frequencies to measure
the weight score (w)of indicator terms for each NFR Qin
the training set. These terms will be used in an indication
function f(w)which deÔ¨Ånes a classiÔ¨Åcation threshold and
represents the likelihood that the new requirement belongs to
a certain NFR type. In this method, requirements must be
processed and reduced to a set of keywords. Also, context-
dependent terms such as products and clients‚Äô names might
appear in the list of indicator terms, which negatively im-
pact the performance of the classiÔ¨Åer in highly unbalanced
datasets. In the same vein, Rahimi et al. [21] augmented term
weight for indicator terms and phrases found in requirements
speciÔ¨Åcation to evaluate the mapping between requirements
and their associated goals. Also, the output of the indication
function in this approach (i.e. the probability score) depends on
the lexical content of requirements documents, as the authors
assume that these documents are more likely to contain only
indicator terms. Likewise, Casamayor et al. [22] used the term
frequency-inverse document frequency (TF-IDF) weighting
function to transform requirements speciÔ¨Åcation documents
into feature vectors. They used the expectation maximization
(EM) strategy with the Na ¬®ƒ±ve Bayes algorithm to propose
a semi-supervised approach for automatic identiÔ¨Åcation and
classiÔ¨Åcation of NFRs aiming at reducing the size of the
training dataset. The results of the empirical evaluations on the
same dataset used by Cleland-Huang et al. [19] showed that
the approach requires less human effort in labeling require-
ments and it outperformed the K-NN and EM/Na ¬®ƒ±ve Bayes
classiÔ¨Åcation methods.
Na¬®ƒ±ve Bayes and SVMs classiÔ¨Åers are used in several stud-
ies [17], [23]‚Äì[26] for automatic classiÔ¨Åcation of requirements.
As an example, Slankas and Williams [26] proposed a tool-
based approach to extract and classify NFRs in requirements
speciÔ¨Åcation documents presented in natural language. They
used words vectors to parse requirements strings and classify
them into 14 categories of NFRs. Among K-NN, Sequential
Minimum Optimizer (SMO), and Na ¬®ƒ±ve Bayes [27] classifying
techniques used in this work, SMO, a variation of the SVM
approach with heuristic training, had the highest effectiveness.
Abad et al. [17] used the Stanford NLP [28] and proposed a
set of context-based regular expressions to preprocess require-
ments documents and to increase the performance of various
classiÔ¨Åcation approaches (e.g. LDA [29], BTM [30], and Na ¬®ƒ±ve
Bayes [27], [31]). After applying their proposed contextual
443
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. rules, they showed that Binarized Na ¬®ƒ±ve Bayes (BNB), has the
highest performance for classifying NFRs compared to other
methods used in this study.
III. P ROBLEM FORMALIZATION AND PRELIMINARIES
We Ô¨Årst deÔ¨Åne a formalization of the problem we address in
this paper. We will then deÔ¨Åne basic notations of WFSTs and
will provide the technical description of WFST operations that
will be used during extraction and classiÔ¨Åcation processes.
A. DeÔ¨Åning the Problem
Given a document repository D={d1,...,d n}, with each
direpresenting a document, and a source stream (a real-time
transcription of an ongoing interview), S=/angbracketlefts1,s2,...sm/angbracketright
with each si=(ai,bi)representing an exchange (where
without loss of generality aiindicates text from the an-
alyst,biindicates text from the stakeholder), the problem
we address in this paper is to select inside each dithose
textual snippets that are most relevant for the most recent
part of the conversation happening in S, where ‚Äúrelevant‚Äù
is understood in intuitive terms, that is according to a (not
formally deÔ¨Åned) utility function linked to the effectiveness
of the elicitation process. Selection of requirements-relevant
snippets ( R={r1,r2,...rv}) will constitute selecting textual
spans which are most likely to be relevant given surrounding
context. The context of an occurrence is deÔ¨Åned by substrings
or subsequences (non-contiguous terms) surrounding it. Each
relevant snippet rjcontains a set of requirements-relevant
terms{t1,t2,...,tk}‚ààT .
Also, from a predeÔ¨Åned list of classes C={c1,c2,...,c p}
we assign a label/class to each selected snippet. Given D
andSas inputs, the output of our technique will be a
tuple/angbracketleftr,c,{t1,t2,...,tz}/angbracketright, wherezis deÔ¨Åned by analyst and
represents the maximum number of relevant terms that should
be highlighted in each extracted snippet.
B. Preliminaries
We brieÔ¨Çy describe some of the main theoretical and
algorithmic aspects of WFST machines.
a) Weighted Transducers and Automata: A Finite State
Transducer (FST) is a Ô¨Ånite automaton in which an acceptable
path through the initial state to a Ô¨Ånal state provides a mapping
from an input sequence to an output string [32]. Similarly, a
weighted transducer is an FST that, in addition to the input
and output strings, incorporates a weight into each transition.
This weight may present probability, priority, or any other
quantities assigned to alternative and uncertain transitions.
Figure 1 gives a simple, familiar example of a WFST to model
sample requirement ‚ÄúThe Disputes system shall support 350
concurrent users‚Äù. The bold circle represents the initial state
and double circle Ô¨Ånal state. The input and output labels xand
y, and weight wof a transition are presented on transition arcs
byx:y/w. We represent the weight associated with a pair of
input and output strings (x,y)modeled by transducer Tby
/llbracketT/rrbracket(x,y)(e.g. in Figure 1, /llbracketT/rrbracket(350,large)=0.2). Given the
alphabet Œ£, we refer to |w|as the length of a string w‚ààŒ£‚àó
and toŒµas the empty string (i.e. |Œµ|=0).	
		



	
	



Fig. 1. An example of using WFST to model natural language text. This
WFST replaces English and contextual stop-words with empty string Œµ.I t
also replaces numbers with a contextual term (contextual replacements are
out of the scope of this paper). If we deÔ¨Åne the weight as the cost of each
transition, the lower the weight, the higher the probability of the term in a
context. In this example, relevant terms large and concurrent are receiving
lower weights.
q1 q2x:y/w1
(a)T1q/prime
1 q/prime
2y:z/w2
(b)T2q1,q/prime
1 q2,q/prime
2x:z/w1‚äïw2
(c)T1‚ó¶T2
Fig. 2. The composition ( ‚ó¶) operation for detecting substrings (subsequences)
of interest with transition rule: (q1,x,y,w 1,q2)‚ó¶(q/prime
1,y,z,w 2,q/prime
2)‚áí
((q1,q/prime
1),x,z,w 1‚äïw2,(q2,q/prime
2))
b) Composition of WFSTs: WFSTs can be composed by
a general operation for tying two or more WFSTs together
to create a pipeline which can be used to represent statistical
models of both generative and discriminative models (e.g. LM
and SVMs). As illustrated in Figure 2(a-c), given two WFSTs
T1andT2such that the output alphabet of T1coincides with
the input alphabet of T2, composition feeds the output of T1
into the input of T2. The composition T1‚ó¶T2is identiÔ¨Åed
by‚äï-summing the weights of (x,z)paths, where the weight
of all these paths identiÔ¨Åed by ‚äó-multiplying the weights of
(x,y)and(y,z)paths. Substring ypresents the substring of
interest appearing in a weighted automata [1], [2].
c) Kernel Methods and Rational Kernels: Figure 3a illus-
trates a very simple example of distinguishing two different
classes (i.e. grey and red circles). One can simply choose a
hyperplane to separate the two populations correctly. Among
the inÔ¨Ånitely many choices available for this hyperplane, the
SVM approach determines the hyperplane that maximizes the
margin , which represents the distance between each population
and the hyperplane [4], [33]. However, in practice, the linear
separation of the training data is often not possible (Figure
3(b)). One way to address this problem is to use a nonlinear
mapping Œ¶:X‚ÜíYwhich transforms the problem space X
to a higher-dimensional space Ywith each of the dimensions
being combinations of the original features (Figure 3(c)). Even
though the space transformation may solve the problem of
nonlinearly separable data, taking a large number of dot prod-
ucts in a very high-dimensional space to deÔ¨Åne the hyperplane
may be very costly. Kernel Trick is a solution to this problem,
which deÔ¨Ånes a kernel as a similarity function (measure).
There are various types of kernels for text classiÔ¨Åcation such
as word [34], n-gram [35], mismatch [36], spectrum [37],
and string [38] kernels which use counts of the common
occurrence of subsequences. Kernel Kisrational when there
exists a weighted transducer Tsuch that k(x,y)= /llbracketT/rrbracket(x,y)
[32], for all sequences (i.e requirements expression) xandy,
where, /llbracketT/rrbracket(x,y)denotes the weight associated to a pair of
(x,y)[39]. This implies that the objects that are aimed to be
classiÔ¨Åed are weighted automata.
444
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. (a) (b)
 (c)
Fig. 3. The application of kernel trick to distinguish non linear separable data
points. (a): Linear separable, (b) Non-linear separable, (c) Mapped Space
IV . P ROPOSED EXTRACTION AND CLASSIFICATION
TECHNIQUE
In this section, we start by discussing the intuition behind
using lexical association and the rationale for its application in
extracting relevant terms in requirements-related documents.
Then, we provide the technical description of our proposed
approach for dynamic control of context-dependency as well
as the dynamic calculation of lexical association in existing
documents. We conclude this section by describing the process
of classifying snippets containing the extracted relevant terms.
More precisely, the proposed extraction and classiÔ¨Åcation
process is depicted in Figure 4. More application-minded
readers can consult Section IV-D Ô¨Årst to get a non-technical
summary of the extraction scenario and the state of the Ô¨Åeld,
and then read Sections IV-B to obtain more technical details.
A. Rationale
Recall from Section II, most existing work on the extraction
of relevant terms from NL text in the context of RE use
basic document features such as terms frequency and length,
document length and the existence of a term in a repository.
Using these features, relevant terms stay independent of other
content-carrying terms in the document which contributes to
overlooking the context surrounding terms when measuring
their relevance. This is a weakness shared by all bag of
words approaches. To address this problem, in this paper,
we use lexical association between documents terms, which
quantitatively determine the strength of association between
two or more words (or terms) based on their co(occurrence)
in a corpus [40] and will assign different weights to terms
depending on the context they occur in.
The intuition behind using lexical association is the basic
assumption that a context in which a word is used can often
inÔ¨Çuence its meaning [41]. Thus, in a document, the words
that are highly associated with each other and occur together
more often than expected by chance have a special function
and can be considered as relevant (content-carrying) terms. In
contrary, the irrelevant (background) terms will have a very
low association with the other terms in a corpus [42].
B. Extraction
To calculate lexical association (i.e. co-occurrence knowl-
edge) we use statistical language models (LMs), which assign
probabilities to sequences of words based on their prior history.
Using the chain rule of probability , we can decompose the
probability of any sequence wn
1=(w1w2...wn)to:
P(w1w2...wn)=n/productdisplay
i=1P(wi|wi‚àí1
1)Weighted-rewrite-
rules .fst
../d 1.fst
../d 2.fst
../d 3.fst
..
..
../d n-1.fst
../d n.fst
fst archiverequirements-
relevanent
terms/snippets
1
COMPILING EXTRACTION2 3 4
CLASSIFICATIONEXTRACTION LABELING
Input contextual
dataset
Domain repository D
symbols table (alphabet)
the most recent window of S-
-
-‰°Ω‚Üí
‰°Ω‰°Ω‚Üí
‚Üí‚Üí‚Üí‰°Ω
‚Üí
ih d i
LEARNING.00.0 .01.0 2/1s:Œµ/1
s:s/1 s:s/1s:Œµ/1 s:Œµ/Œª
Language
model
Krafted gappy
n-gram kernel
Most recent window
of S( s i)
Translating Dand the
most recent window of
Sto WFSTsLearning the language
model with various
configs ( using perplexity
metric )
- n-gram order
- discounting methodRequirements
extraction
a single-source
sourceshortest path
algorithm (n-shortest
paths)Requirements
classification/labeling
- SVMs + rational kernels
- labeled training set
‰°Ω‚Üí
‰°Ω‰°Ω‚Üí
‚Üí‚Üí‚Üí‰°Ω
‚Üí

‰°Ω‚Üí
‰°Ω‰°Ω‚Üí
‚Üí‚Üí‚Üí‰°Ω
Optimized
WFSTsLabeled
snippets
Fig. 4. The architecture of our proposed extraction/classiÔ¨Åcation process.
whereP(w|h)assigns a probability to term hw, considering
some history h, and word w[43]. A straightforward maximum
likelihood estimate of P(w|h)is given by relative frequency
count (hw)
count (h). Since the parameter space of P(w1w2...wn)is too
large (i.e. size of the language vocabulary) and there might
be some new sentences (or contexts) that have never occurred
before, we use n-gram language models [35], [44] which ap-
proximate hby just the last few words. An n-gram model is a
sequence of nwords that approximates the probability of each
word only to the last n‚àí1words: ‚Äúsoftware requirements‚Äù
and ‚Äúrequirements engineering‚Äù in ‚Äúsoftware requirements
engineering‚Äù , are bigrams (2-gram) and the whole expression
represents a trigram (3-gram). While the intuition behind n-
gram language models helps to manage the complexity of the
probability function, these models are highly dependent on
the corpus we use as the document repository (i.e. D) and it
underestimates the probability of all possible terms that might
occur. Given the fact that the contextual data available for
requirements speciÔ¨Åcations are often not big enough to give
us good estimates for the probability of all possible word
co-occurrences and considering the creativity of language,
there is always the possibility that stakeholders and clients
use requirements-relevant terms that have not occurred in the
training set (i.e. D) but do occur in the test set (i.e. S). For
instance, the following repository D:
D={The admin should be able to ensure the conÔ¨Ådentiality of
information and sensitive data of users‚Äù }
includes requirements-relevant bigrams IT={ensure con-
Ô¨Ådentiality, conÔ¨Ådentiality information, information sensitive,
sensitive data, data users }. However, given sj‚ààS with
aj={‚ÄúDo admins protect information conÔ¨Ådentiality?‚Äù }
bj={‚ÄúYes, all private information are conÔ¨Ådential.‚Äù }
the probability in Dof both terms ‚Äúinformation conÔ¨Ådential‚Äù
and ‚Äúprivate information‚Äù from sjare zero, and so is the
probability of the entire test set. This is because terms in aj
contain an unseen context despite the words appearing in the
vocabulary set of D, and terms in bjcontain unseen words (i.e.
445
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. Out-Of-V ocabulary (OOV)). We address these two scenarios
as follows:
Unseen Words [OOVs]- To increase the generalizability of
our proposed approach we exploit the Ô¨Çexibility of WFSTs by
dynamically adjusting the symbol table to map all words in
the source stream not found in the domain corpus to an OOV
symbol unk.
Unseen Context- To consider all possible variations of a
context and to keep our language model from assigning zero
to unseen contexts ( n-grams), we use n-gram hierarchy1.
In a nutshell this approach takes the view that, sometimes,
using less context is a good thing and helps generalizing the
context of the n-gram model. In other words, to calculate
P(wn|wn‚àí2wn‚àí1)of trigram wn‚àí2wn‚àí1wnwe can instead
estimate this probability by using less contexts (i.e bigram
probability P(wn|wn‚àí1)or unigram probability P(wn)). To
implement the n-gram hierarchy approach we use and evaluate
the following techniques:
1)Backoff: Following this technique, if the required n-gram
has zero instances, we approximate its probability by
backing off to (n-1)-gram. We iteratively back off to a
lower-order n-gram until we Ô¨Ånd a term with a non-zero
count. For instance, to compute the probability of trigram
wn‚àí2wn‚àí1wn(i.e.wn
n‚àí2)w eh a v e :
P(wn|wn‚àí1
n‚àí2):‚éß
‚é™‚é®
‚é™‚é©P(wn|wn‚àí1
n‚àí2)ifC(wn
n‚àí2)>0
P(wn|wn‚àí1)ifC(wn
n‚àí2)=0‚àßC(wn
n‚àí1)>0
P(wn) Otherwise
In this paper, we apply and evaluate two backoff methods
Katz [45] and Witten-bell [46] (Section V-C3).
2)Interpolation: No matter what the frequency of different
ordern-grams is, by applying this approach we mix the
probability of all the n-gram sequences. In other words,
we reduce the probability mass from some more frequent
terms and give it to the contexts that have never occurred
in the dataset [43]. Following presents the general idea of
the linear interpolation, combining uni/bi/trigrams, each
weighted by Œ∫, where/summationtext
iŒ∫i=1 [43], [47].
P(wn|wn‚àí1
n‚àí2)=Œ∫1P(wn|wn‚àí1
n‚àí2)+Œ∫2P(wn|wn‚àí1)+Œ∫3P(wn)
Going back to our example, using either of back-off or
interpolation techniques, our n-gram model assigns non-zero
probabilities to bigrams ‚Äúprivate information‚Äù and ‚Äúinfor-
mation conÔ¨Ådential‚Äù. Absolute discounting [48] subtracts a
constant discount dfrom each count and re-distributes the
probability mass. Kneser-ney discounting [47] augments ab-
solute discounting by considering the number of contexts each
word has happened in the domain dataset. By using this
approach, we hypothesise that words that happened in more
contexts in the past are most likely to appear in some new
contexts. For instance, to calculate the probability of term
1This approach is also called as smoothing or discounting in literature [43].00.0 .01.0 2/1s:Œµ/1
s:s/1 s:s/1s:Œµ/1
2-grams
(a)n-gram Kernel.00.0 .01.0 2/1s:Œµ/1
s:s/1 s:s/1s:Œµ/1 s:Œµ/Œª
(b) Gappy n-gram Kernel
Fig. 5. Weighted transducers computing the count of expecting (a) all bigrams;
(c) all gappy bigrams with a Ô¨Åxed penalty factor Œªand maximum gap 1. s
represents each symbol (word) ‚ààD.
‚Äúinformation conÔ¨Ådential‚Äù considering the following domain
corpus, our model assigns a higher probability to unigram
‚ÄúconÔ¨Ådential‚Äù than ‚Äúadmin‚Äù. While ‚Äúadmin‚Äù is more frequent
compared to ‚ÄúconÔ¨Ådential‚Äù, it is mainly frequent in the term
‚Äúsystem admin‚Äù, while ‚ÄúconÔ¨Ådential‚Äù has appeared in two
different contexts.
d1={The system administrator should be able to conÔ¨Ågure the
conÔ¨Ådentiality of sensitive information. }
d2={All sensitive conÔ¨Ådential data will be secured on the server
and only accessible by authorized system administrators. }
d3={Only system administrators can activate the account. }
‚Äì
To identify which language model parameters assign the
highest probability to the source stream data (i.e. more ac-
curately predicts the incoming source stream) we used the
perplexity score [43], [49]. By LM parameters we mean the
n-gram order (i.e. n) and the discounting method (i.e. Katz,
Witten bell, absolute, and Kneser-ney). The perplexity of a
LM on a test set2is deÔ¨Åned as the inverse n-gram probability
of the source stream data, normalized by the number of words.
The lower the perplexity, the higher the ability of the language
model in predicting the incoming text. More details about
the dynamic application of this parameter in our proposed
approach will be explained in Section V.
The model outlined above provides a context-sensitive
weight to each of the lexical components of the document.
The next step is applying these weighted components to the
source stream document to explore the relevant terms in this
document. Given source stream S, a weighted transducer Sis
built using the same symbol table we used for generating trans-
ducerDfromD. To efÔ¨Åciently capture these requirements-
relevant (i.e. domain-speciÔ¨Åc) terms in real-time, we rank
the extracted terms with more probable relevant phrases Ô¨Årst.
To do this, we use a single source shortest path algorithm
applied to the WFST resulted from composing SandD.I n
a simple word, we Ô¨Årst build a language model for D, then
recompute a language model for the most recent ‚Äúwindow‚Äù
ofSon every addition to S. The intersection3between the
two language models returns ‚Äúrelevant‚Äù terms (terms that are
2The test set in this context is the transcribed interview or a new speciÔ¨Å-
cation text.
3Note that intersection ( ‚à©) and composition( ‚ó¶) are two different operations
in automata theory and they return different results. We used this term to
imply the intuition behind our algorithm.
446
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. in the intersection). Finally, we fetch parts of dithat contain
the topnrelevant terms:
{t1,t2,...tn}‚ààShortest_path ([LMD‚ó¶LMS],n)
WherendeÔ¨Ånes the number of top relevant terms (i.e.
nshortest paths of the composed WFST). We illustrate this
process with a simple example later in Section IV-D.
C. ClassiÔ¨Åcation/Labeling
As pointed earlier, previous efforts to classify requirements
use methods that only work on Ô¨Åxed-size feature vectors,
which are difÔ¨Åcult to use in dynamic settings as well as in
large-scale datasets. Consequently, much effort in the area has
focused on feature engineering to produce a crafted list of
features [50]‚Äì[53]. Even more to the point, non-contiguous
language models (i.e gappyn-grams ), which positively impact
the performance of text classiÔ¨Åers [34], [38] have not yet been
applied in the context of requirements classiÔ¨Åcation.
In this paper, we propose to apply the rational kernels
[39] method for use with SVMs for classifying requirements.
This family of kernels is easy to design and implement and
lead to substantial improvements in the classiÔ¨Åcation accuracy
[39]. By using a gappy rational kernel (i.e. a WFST withŒµ
transitions), requirements speciÔ¨Åcations that diverge through
unrelated word sequences (e.g. products, clients, or end-users‚Äô
special names) are still likely to contain meaningful subse-
quences that almost or completely match. Also, adding weights
to speciÔ¨Åc sequences of words is one way of incorporating
prior knowledge into the classiÔ¨Åcation algorithm. Our ap-
proach is very simple. After extracting text snippets containing
requirements-relevant terms that have been identiÔ¨Åed in the
extraction phase, the rational kernel (a.k.a similarity function),
can be computed efÔ¨Åciently using a general composition
operation which is followed by calculating a single source
shortest distance from state 0 to Ô¨Ånd the sum of the weights
of all successful paths of the composed transducer. We deÔ¨Åne
two requirements are similar if they share many common
substrings or subsequences (non-contiguous/ gappy n-grams).
The similarity measure of two transducer E(i.e. extracted
from the source stream document) and T(i.e. transducer
associated with the training set4) can be computed as:
K(E,T )=/summationdisplay
x,y/llbracketE/rrbracket(x)./llbracketX/rrbracket(x, y)./llbracketT/rrbracket(y)=/summationdisplay
x,y/llbracketE‚ó¶X‚ó¶T/rrbracket(x, y)
Where /llbracketT/rrbracket(x,y)denotes the similarity measure between
two strings xandy. This measure can be deÔ¨Åned as the sum
of the expected counts in EandTof the matching substrings.
Figure 5a represents an example of transducer Xthat
is designed to compute the expected counts of all bigrams
in extracted transducer E. Here is how transducer Xdoes
so: State 0 reads a preÔ¨Åx of the bigram xand outputs the
empty string Œµ(i.e. ignoring unmatched preÔ¨Åxes). Then, an
4The training set for the classiÔ¨Åcation task includes a set of pre-existing
labeled functional and non-functional requirement and it is different from the
training set of the extraction taskQuestion: Imagine the ideal situation, can you describe the support process "as
it should be"?
[...] Either it be web based or Lotus Notes it must be easy to get to. Users would
then type in their problem and it would go out and search for that issue and
potentially give them a solution.
Another great solution would be to eliminate the ‚Äúholding queue‚Äù, whereas when
the ticket selects a category from the drop down list it would get assigned to a
particular individual. As of right now a ticket can be within the holding queue for
several hours until it gets assigned to a particular individual.
(a) Recent window of S(s)
Tickets can have an owner‚Äîthe user responsible for working on the ticket or for
coordinating the work. To assign a ticket to someone, go to the People form from
the ticket display page, and select the user from the Owner drop-down list. This
list contains the usernames of all the users allowed to own tickets in the ticket‚Äôs
current queue. You can assign only tickets that you own or that are unowned. If
you need to reassign a ticket that you do not own, you can steal the ticket and
then assign it to someone else.
(b) Extracted snippet from D
Fig. 6. Extracted requirements-relevant terms from a transcribed interview
question ( si‚ààS), using RT Essentials [54] as D.
occurrence of xis read and output identically. Finally, state 1
reads the rest of the sentence and outputs Œµ. The number of
ways that we can start from state 0 and follow this procedure
to reach state 1 (i.e. the accepting state) gives the counts of
the speciÔ¨Åc bigram x. Figure 5b represents a gappy bigram
kernel which allows for a gap between the occurrences of
two symbols (while keeping the order of terms). In this
case, we say two requirements descriptions are similar if
they share such subsequences (i.e. gappy bigrams). In some
contexts, ignoring the gap between content-carrying terms
might impact the context of their application. For example in
s:{The conÔ¨Ådentiality status does not affect the information
access}is dealing with information access (rather than the
conÔ¨Ådentiality (type) of the information). Thus, to penalize
the gap/distance between two words of the gappy bigram,
we use a Ô¨Åxed penalty factor 0‚â§Œª<1[38], [39]. The
self-loop on state 1, represents the way we consider gaps
between bigrams (or n-grams in general). In Section V we
investigate how this technique can impact the performance of
our labeling/classiÔ¨Åcation task.
Finally, we describe the whole process of proposed extrac-
tion/classiÔ¨Åcation approach that we name Relevance Extrac-
tion and Requirements ClassiÔ¨Åcation (RERC) in Algorithm 1.
D. Example
In this section, the extraction method described above is
applied to a real-world dataset containing a requirements
elicitation interview for a help desk ticketing system . As stake-
holders might use contextual technical terms when describing
a system‚Äôs features and requirements, we used a book [54]
(as domain repository D) which contains both technical and
general information about the ticketing system. This book is
large enough (224 pages) to be used as a substitution for
the volume of text that an analyst might need to review
to understand the domain material. We are looking for the
lexical association between terms in the two datasets D(the
book) and si(the most recent exchange of S, Figure 6a ‚Äî
in practice, a larger window would be used). Following the
central assumption of lexical association [41], we also assume
that lexical association can be used to semantically relate
447
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. ALGORITHM 1:Relevance Extraction and Requirements ClassiÔ¨Åcation (RERC) Algorithm
Input: (1) domain repository D={d1,d2,...d n}, (2) source stream documents
S=/angbracketlefts1,s2,...s i/angbracketright, Training set T//for the classification task
Extraction [Smoothed Language Model]
ST =/uniontextn
i=1ST[di]//ST represents the symbol table (i.e.
vocabulary) of the domain repository D
//n: the order of the n-gram LM
foreachn‚àà[1..5]do
foreachm‚àà[Katz, Witten-bell, Kneser-ney, Absolute ]do
P[m][n]‚Üêperplexity (n-gram,m,s );
//srepresents the most recent window of S
end
end
//Ifn/prime-gram and method m/primeyield the minimum perplexity
LM D‚Üêngrammake( n/prime-gramD,m/prime);
ET‚ÜêLMD‚ó¶LM s;//ET= the transducer of Extracted Terms
T‚Üê shortest path(ET,z );//T={t1,...,t z}: z-top ranked relevant terms
//Ifrihas the maximum overlap with T
r‚Üêri‚ààR //Ris a set of requirements-relevant snippets from D
Labeling/ClassiÔ¨Åcation [SVM enhanced with Rational Kernels]
//similarity function K
foreachn‚àà{2,3,4},j‚àà{0,1,2},andz‚àà{0,0.5}do
LM k‚Üêklngram order =nmax_gap =jŒª =z// generating
the transducer which measures lexical association (as the
similarity measure)
N=Tr‚ó¶LM k‚ó¶Tt//Tr: transducer of the snippet contains
relevant terms, Tt: transducer related to the documents in
training set
W[N]‚Üêshortest distance (N)//to calculate the shortest distance
from initial states to final state for all common
subsequences between the training and testing datasets
end
Note:W[N]will be used in combination with SVMs which is out of the scope of this paper
Result: ClassiÔ¨Åed snippet containing requirements-relevant terms: /angbracketleftr,c,{t1,t2,...,t z}/angbracketright
terms in both document Dands. Considering the innovative
nature of language and diversity of terms that can be used
to describe a speciÔ¨Åc feature, looking for the exact literal
match to measure the lexical association between the terms
ofDandsis very likely to overlook relevant terms that
occur in both documents. For example, after removing stop
words and stemming words to their roots, contiguous term
‚Äúsearch issue‚Äù (which appears in sand is a requirement-
relevant term) never occurs in D, but it occurs in a context
with the same meaning: ‚Äú[...] does a quick search for all of the
new or open Macintosh issues that are currently unowned‚Äù .
Also, the language model created for the book returns zero for
the probability of predicting the sentence of swhich contains
the term ‚ÄúLotus Notes‚Äù , as this term never occurs in the
book. However, this sentence contains a genuine requirement-
related information about the platform of the system which
should be web-based in this context (e.g. ‚Äú[...] this ensures
that anyone with a web browser can use the system [...]‚Äù
fromD). We handle out-of-vocabulary (OOV) words‚Äô problem
by simply replacing all words that do not appear in Dwith
unk when modeling s, and handle the problem of order and
contiguity by using a hierarchy of n-gram language models
when modeling both Dandsdocuments. If either OOVs or
non-contiguous words appear in a relevant context in s, our
approach would perfectly detect the same context in D. Thus,
the extraction would be treated as Ô¨Ånding all common terms
which appear in relevant contexts and our algorithm identiÔ¨Åes
the relevance of terms by computing the intersection between
the two hierarchical language models ( Dands). It then
extracts the snippet which has the highest lexical association
with the most recent window of S. For example, ‚Äúqueue‚Äùappears as a requirement-relevant term in both Dands. This
term occurred 128 times in 50 different contexts in D,b u t
our extraction algorithm returns the snippet in which ‚Äúqueue‚Äù
appears in the same context as s(Figure 6b). By looking at
this snippet, the analyst can come up with some follow up
questions about ticket assignment and its ownership methods
which have not been discussed in the interview so far, possibly
eliciting more information.
V. E XPERIMENTAL EV ALUATION
A. Method
Our main assumption is that lexical association ,a sa p -
proximated by our n-gram hierarchy and gappy n-gram ker-
nels (with decay factor), is a useful indicator of semantic
relatedness. To evaluate this assumption, we designed and
implemented three experiments as follows:
Experiment #1 (E 1):As discussed in Section III-A, the
successful extraction of snippets from domain repository D
means that the selected snippets have the highest degree of
overlap with the top-ranked extracted relevant terms. Thus,
we hypothesized that lexical association can be used as a
means to indicate the relevance of extracted snippets and,
following the central assumption of lexical association [41],
the snippet with the highest level of overlap with extracted
terms is the most relevant for the purpose of understanding and
analyzing the content of the recent window of the incoming
text (s‚ààS). In this experiment, we pose the null hypothesis:
[H0-lexical association has no impact on the relevance of
extracted snippets ] and use a publicly available industrial
dataset to test this hypothesis.
Experiment #2 (E 2):This experiment mimics a situation
in which an analyst is in a real-time conversation with a client.
In this experiment, the same hypothesis (as in Experiment
#1) was tested using a publicly available dataset containing
transcribed interviews for exploring the requirements of a Help
Desk Ticketing system.
Experiment #3 (E 3):In this experiment, we evaluate the
effectiveness of using gappy n-gram kernels as a similarity
measure (which is based on lexical association), in require-
ments classiÔ¨Åcation. Additionally, we are interested in measur-
ing how well our lexical-association based method performs
compared to the classiÔ¨Åcation methods used in [17], [19], [24].
B. Evaluation Metrics
1) Edit Distance: Given the identiÔ¨Åed snippets, to test the
null hypothesis H0, we need to compute the overlap (lexical
association) between the reference set of relevant terms and the
relevant terms included in the corresponding extracted snip-
pets. By a reference set, we mean a sequence of relevant terms
fromsrequired for understanding and analyzing its content.
Our reference for a correct set of relevant terms associated with
each snippet are the Ô¨Årst author of the paper and an external
research assistant, who are experienced analysts and familiar
with the problem. To remove any threat of biased decisions, we
use the kappa statistics [55], [56] to measure the magnitude of
agreement between them. The calculated kappa coefÔ¨Åcient for
448
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. this task is 0.92, which shows an almost perfect [56] agreement
for this task.
The most popular metric for measuring the similarity be-
tween sequences of words and more speciÔ¨Åcally between short
strings is string edit distance [57], [58]. This distance is
the minimum number of edit operations (e.g. substitutions,
insertions, and deletions) to transform the extracted sequence
to the reference sequence. For instance the distance between
extracted term ‚ÄúconÔ¨Ådential information day‚Äù and reference
term ‚Äúaccess conÔ¨Ådential information‚Äù is two because one
insertion (access) and one deletion (day) are required. To cal-
culate the edit distance, we use the Levenshtein edit distance
[57], [59], which, incidentally, we implemented as a further
transducer.
2) Precision/Recall: We evaluate the performance of our
proposed requirements classiÔ¨Åcation/labeling technique using
the standard metrics precision ( P), recall ( R), andFscore.
The recall is the percentage of the correct answers which are
retrieved, whereas precision is the percentage of the retrieved
instances that are correct [60], [61]. Fscore considers both P
andRand deÔ¨Ånes the overall effectiveness of the classiÔ¨Åcation
method. Formally, these measures are given by:
P=TP
TF+FP,R =TP
TP+FN,F =2P¬∑R
P+R
C. Experimental Design
1) Datasets: The experiments E 1‚àí3are conducted on the
following datasets:
Dataset 1[E1,3]:We used a published dataset by the U.S.
Department of Transportation (U.S. DOT), obtained from [62].
In addition to contextual data ( D), this dataset provides a
repository for both high-level and low-level requirements gov-
erning the design of Clarus, a system to monitor environmental
and road conditions. The full dataset consists of 700 (300 high
level + 400 low-level) requirements.
Dataset 2[E2]:This dataset was provided by ThyssenKrupp
Presta Steering Group USA. The Scomponent of this dataset
is made up of 18 transcribed requirements elicitation inter-
view questions obtained from two separate interviews (of 60
minutes each) to explore the requirements of a Ticketing
system5. The answer to each question can map to one
or more requirements (32 functional and 14 non-functional
requirements in total). We obtained Dfrom the full source
of a textbook on ticketing systems, RT Essentials [54]. RT is
a high-level open-source ticketing system. The book is large
enough to be used as the domain document and its subject is
representative of the technical domain that the analyst might
need to understand.
Dataset 3[E3]:This dataset is obtained from the Open-
Science tera-PROMISE repository6and consists of 625 labeled
natural language requirements (255 FRs and 370 NFRs).
2) Preprocessing and WFST Transformation: We utilized
the tm[63] and Snowball [64] packages in Rto remove
punctuations, English/predeÔ¨Åned stop words and to stem words
5This dataset is publicly available by ThyssenKrupp Presta Steering Group
6https://terapromise.csc.ncsu.edu/!/#repo/view/head/requirements/nfrTABLE I
DETAILS OF THE DATASET AND COMPARISON RESULTS FOR THE CLASSIFICATION TASK
NFR CQ 1 CQ2
#Te #Tr #Te #Tr
Us 20 56 15 52
Op 20 51 15 47
Pe 15 35 15 37
(a) ClassiÔ¨Åcation datasetsReference Us Op Pe
Cleland-Huang et al.[19] 0.25 0.20 0.38
Abad et al. [17] 0.86 0.84 0.93
Kurtanovi and Maalej [24] 0.74 0.71 0.82
Our approach 0.89 0.88 0.91
(b) Comparison results (E 3)
to their roots (e.g. conÔ¨Ådentiality, conÔ¨Ådential ‚ÜíconÔ¨Ådenti,
based on the popular Porter‚Äôs stemmer [65]).
3) Learning and Extraction: After transforming Dto a set
of WFSTs, we applied perplexity measure to Ô¨Ånd the most
probable LM for each pair of /angbracketleftsi,D/angbracketright. In particular, we built
20 static language models for Dand indexed each model with
/angbracketleftmj,n/angbracketright, wheremj‚àà{Katz, Witten-bell, Absolute, Kneser-ney }
and denotes the discounting method, and n‚àà{1,2,3,4,5}the
order of each LM. LM /angbracketleftmj,n/angbracketright, where discounting method mj
and order ngenerate the minimum perplexity, will be com-
posed with the most recent ‚Äúwindow‚Äù of Son every addition
toS. To explore and indentify relevant terms, we use the top
mranked paths of the WFST generated from LM /angbracketleftmj,n/angbracketright‚ó¶LMs.
This process involves a straightforward implementation of a
generalization of the Dijkstra algorithm [66] (i.e. the n-best
strings problem [67]).
To provide this Ô¨Çexibility to analysts to manage the number
of relevant terms (as it might need to be changed based on the
size of the s‚Äúwindow‚Äù), we used a parametric value which
can be changed during the application of the method.
4) ClassiÔ¨Åcation: This section illustrates and examines the
application of the rational kernels describes in Section IV to
requirements classiÔ¨Åcation. We did a series of experiments
on Datasets 1,3using rational kernels to answer the following
classiÔ¨Åcation questions (CQs) about the performance of our
algorithm in identifying various types of NFRs, in particular
usability, operability, and performance requirements7:
CQ1- Domain knowledge: Does combining the recently
discussed requirement with its corresponding extracted rele-
vant snippet impact the performance of the classiÔ¨Åer?
CQ2-N-gram kernel characteristics: How is the perfor-
mance of a classiÔ¨Åer affected by: (1) removing the contiguity
constraint; (2) the length of the n-gram; and (3) adding cost
(decay factor Œª) to gaps in non-contiguous n-gram kernels?
Table I(a) indicates the details of the datasets we used
for training and testing the classiÔ¨Åcation tasks. We used the
OpenFST library for the implementation of the rational kernels
used in our classiÔ¨Åcation task, and the LIBSVM [69] library
to combine these kernels with SVMs.
D. Results
In the following, the results of experiments E 1‚àí3on tasks
extraction and classiÔ¨Åcation are discussed and interpreted.
1) Extraction: In terms of the overall performance of our
proposed extraction technique, of 269 low-level requirements
descriptions in Dataset 1, experiment #1 matched 196 (73%)
7Due to the very few instances of other subclasses of NFRs in this dataset
and to be robust to within-class imbalance problem [68], in this experiment,
we focused only on usability, performance, and operability NFRs.
449
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. 

01020304050
Irrel RelWord Error Rate (WER)
(a) E101020304050
Irrel RelWord Error Rate (WER)
(b) E2
Fig. 7. 95% conÔ¨Ådence interval of sample means for WER and relevant and
irrelevant snippets: (a): E 1on 269 high-level requirements and (b): E 2on 31
transcribed interview answers.
descriptions to a relevant snippet (i.e. the corresponding high-
level requirement). Similarly, of 31 transcribed interview ques-
tions in Dataset 2, experiment #2 assigned 20 (64%) questions
to a relevant extracted snippet from D(i.e. RT Essentials [43]).
With regard to the accuracy of extracted relevant terms (i.e.
lexical association) in extracted snippets, the mean word error
rate for E 1is 16.9% (standard deviation ( œÉ): 9.1%, 95% CI
mean: 1.1%) and for experiment E 2is 19.3% ( œÉ: 10.1%, 95%
CI mean: 3.7%). The higher WER for E 2is probably due to
datasetDwe used to generate the training LM. Interestingly,
however, while this dataset is taken from a more general
domain, we achieved an accuracy of 81.7% (95% conÔ¨Ådence
interval¬±3.7%) for this task.
In terms of the impact of lexical association on the relevance
of extracted snippets from D, as we could not conÔ¨Årm the
normality of the distribution of our edit distance data (using
Q-Q plots [70]), we used the non-parametric Kruskal-Wallis
test to examine the null hypothesis posed in Section V-A.
With p-values at 95% signiÔ¨Åcance or greater ( ‚â§0.05)w ec a n
reject the null hypothesis that the lexical association makes no
signiÔ¨Åcant difference on the relevance of extracted snippets.
The results of our statistical tests reject this hypothesis for
Experiment #1 at p-value= 2.2e-10 and for Experiment #2
atp-value=0.02 . Moreover, by looking at Figure 7(a-b), we
can see that the 95% conÔ¨Ådence interval of sample means for
WER of extracted relevant snippets is consistently lower than
irrelevant snippets, which implies that lexical association can
be used as an indicator of relevant snippets.
Finding ¬ñ‚ÄúLexical association makes a signiÔ¨Åcant impact
on the relevance of extracted requirements-relevant snippets
and can be used as an indicator of relevance in these snippets.
2) ClassiÔ¨Åcation: To answer CQ1 , we used the results of
Experiment #1 and applied a simple n-gram kernel used with
SVMs to both low and high-level requirements. Figure 8 (a-
c) shows the results of using low-level requirements alone,
compared to using low-level and high-level in combination as
the input of the classiÔ¨Åer (for three requirements types: usabil-
ity, performance, and operability). These results show that the
classiÔ¨Åcation performance of applying n-gram SVMs to low-
level requirements combined with their corresponding high-
level description is consistently better than that applied to only
low-level requirements. From this, we conjecture that using
high-level requirements descriptions for the classiÔ¨Åcation task





0.650.700.750.800.85
12345
n‚àígram orderF‚àíScore
Low‚àílevel
Low+High Level
(a) Usability



0.70.80.9
12345
n‚àígram orderLow‚àílevel
Low+High Level
(b) Performance




0.60.70.8
12345
n‚àígram orderLow‚àílevel
Low+High Level
(c) Operability
Fig. 8. CQ1- The impact of using extracted requirements-relevant knowledge
on the performance of the classiÔ¨Åer.
may provide more contextual information for the similarity
measure function, which positively impacts its performance.
Finding ¬ó:‚ÄúApplying the classiÔ¨Åcation technique to the high-
level, instead of the low-level requirements (one-best snippet)
can improve the performance of the classiÔ¨Åer . This is inline
with the literature (e.g. [71], [72]) that show augmentation
in data-space can boost the performance of classiÔ¨Åers and
reduce overÔ¨Åtting‚Äù .
Table II presents the results of our experiments for the
classiÔ¨Åcation task associated with CQ2. It gives the perfor-
mance of the classiÔ¨Åcation approach as a function of the order
(length) and the contiguity of the n-gram kernel as well as the
decay factor assigned to each gap. The decay factor allows
controlling the number of gaps that are allowed in the kernel
function [34]. While for Œª=0, gaps have no impact on the
sequence similarity in our kernel function, for Œª=0.5, gaps
add more cost to corresponding paths (which will impact the
classiÔ¨Åcation decision when calculating the shortest distance
algorithm). From these results we conclude: Finding ¬ò‚ÄúThe
performance of the classiÔ¨Åer varies with respect to the con-
Ô¨Åguration of its similarity measure (kernel). In particular , we
found that: (3-4)-gram non-contiguous subsequences are more
effective than other conÔ¨Ågurations in capturing the similarities
between strings. ‚Äù
Regarding the number of gaps in non-contiguous subse-
quences, our results show that the F score peaks at max g=1
for all three types of NFRs; and at Œª>0(i.e. 0.5) for usability
and performance categories. Further increase in the number of
gaps ( max g) can degrade the power of the similarity function
(kernel) and substantially decreases the recall. We speculate
that this may be partly due the similar behaviour of unigrams
andn-grams with max g>1.
Although the kernels we used to perform the classiÔ¨Åcation
tasks do not incorporate any knowledge about the dataset being
used (except removing English and contextual stop words), it
still captures contextual information. It seems that the non-
contiguity of subsequences in the similarity function can better
detect the inherent polysemy characteristic of natural language.
Finally, Table I(b) compares our classiÔ¨Åcation results with
some previous work which used the same dataset to classify
NFRs. The results show that non-contiguous n-gram kernel
withŒª>0outperforms the techniques used in previous work,
except one NFR from [17]. This could be due to the pre-
processing step applied in [17] containing a set of manually-
450
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. TABLE II
CLASSIFICATION PERFORMANCE CONSIDERING V ARIOUS n-GRAM
KERNEL CHARACTERISTICS (CQ2) USING DATASET 3
Usability Operability Performance
max gg0 g1 g2 g0 g1 g2 g0 g1 g2
Œª0Œª0.5Œª0 Œª0Œª0.5Œª0Œª0Œª0Œª0.5Œª02-gramP 0.82 0.85 0.85 0.78 0.87 0.89 0.87 0.88 0.90 0.91 0.92 0.89
R 0.76 0.70 0.76 0.71 0.66 0.68 0.68 0.62 0.72 0.74 0.76 0.71
F 0.79 0.77 0.80 0.74 0.75 0.77 0.76 0.73 0.80 0.82 0.83 0.793-gramP 0.90 0.93 0.94 0.85 0.89 0.91 0.90 0.88 0.96 0.98 0.98 0.89
R 0.82 0.77 0.85 0.73 0.78 0.75 0.77 0.67 0.80 0.83 0.85 0.75
F 0.86 0.83 0.89 0.79 0.83 0.83 0.83 0.76 0.87 0.90 0.91 0.814-gramP 0.86 0.88 0.89 0.79 0.89 0.94 0.92 0.91 0.89 0.92 0.89 0.90
R 0.80 0.71 0.73 0.76 0.83 0.83 0.82 0.73 0.77 0.79 0.81 0.73
F 0.83 0.79 0.80 0.77 0.86 0.88 0.87 0.80 0.83 0.85 0.85 0.80
developed, ad-hoc transformation rules for each type of NFRs.
However, our approach obtains contextual semantics based on
the lexical association between terms and does not apply any
manually deÔ¨Åned contextual rules. Finding ¬ô:‚ÄúThe results
of our experiments indicate that the application of SVMs with
gappyn-gram kernels and with a non-zero decay factor can
provide an effective alternative to the existing methods used
in previous works for requirements classiÔ¨Åcation. ‚Äù
VI. T HREATS TO VALIDITY
Internal validity. While we used the kappa coefÔ¨Åcient to
assess the reliability of ce dataset for measuring the edit
distance, the estimated kappa coefÔ¨Åcient itself could be due
to chance [55], which poses a threat to the validity of our
results. To address this threat, we calculated the pvalue of
the resulted coefÔ¨Åcient. This value ( <0.05) shows that the
estimated agreement is not due to chance8. We can also
discount any history or maturation threat, as the technique
is entirely automated and the computations do not rely on
previous state.
External validity. Our experimental results only apply to
the three datasets we have used. We do not claim that those
datasets are representative of all situations. However, the main
contribution of this paper is the technique in itself, which due
to the intuitive nature of the ‚Äúrequirements-relevant‚Äù concept,
must to a certain extent rely on experimenting on each given
context before generalizing claims. More precise and extensive
case studies are needed to fully address this threat by better
scoping the applicability.
Construct validity. As we only used the Ô¨Ånal outcome of
the extraction approach, the method we used to evaluate this
approach might pose a threat to construct validity. One may
ask, other parameters such as the order of n-gram language
models or using a different discounting method (from the
one used to generate the Ô¨Ånal result) might impact the error
rate (or the edit distance) differently. However, theoretical
considerations [73] and a series of studies conducted by
Klakow and Peters [74] show that the word error rate (WER)
and perplexity are linearly correlated and are related by a
power law. As, during the learning process, we used the
perplexity measure to tune the LM of each incoming dataset
8Note that this pvalue only shows that the estimated kappa is not due to
chance and does not test the strength of the agreement level.and considered potential factors for this task, this threat is
mitigated in our evaluation method.
Researcher bias. In some experiment we relied on human
judgment. One of the judges was an author of the present
paper; however, a second judge was not involved in this
research, and the inter-subject agreement was quite strong, so
we trust our results were not unduly inÔ¨Çuenced.
Content validity. The metrics (Section V-B) we used to eval-
uate our approach pose a potential threat to content validity.
Future studies might be conducted with different metrics to
ensure that all interesting dimensions have been considered.
VII. C ONCLUSION AND FUTURE WORK
We have presented a technique to dynamically extract
requirements-relevant knowledge from existing documents,
in order to assist analysts by surfacing relevant information
from documental sources during an interactive interview. We
evaluated this technique by conducting experiments on three
different datasets and used the WER parameter to measure the
effectiveness of our approach. The results of our experiments
show that the proposed technique is an effective and feasible
approach for extracting requirements-relevant knowledge.
On the technical side, we also proposed to use non-
contiguous n-gram kernels in the context of requirements clas-
siÔ¨Åcation and applied rational kernels combined with SVMs
to implement this technique. Although these kernels do not
incorporate any contextual information (apart from the prepa-
ration process) they can still capture semantic information.
The results of our experiments show that non-contiguous
medium length n-grams with decay factor >0better capture
the similarity between strings that contiguous or short (bigram)
non-contiguous n-gram subsequences, which improves over
the previous state of the art at the NFR classiÔ¨Åcation task.
Our work advances current techniques by combining both
generative and discriminating models to support analysts in
their requirements elicitation tasks in real-time.
Although the paper includes a fairly varied evaluation of
the application of the proposed approach, there are still some
factors (e.g. human interface factors) that might impact the
application of this technique. Thus, one possible direction
for improvement is developing a tool that can perform the
extraction/classiÔ¨Åcation tasks for any industrial contexts, to be
able to gather users‚Äô feedback on real cases. To this end, we
have implemented a prototype tool that we intend to use for
a Ô¨Åeld study [75]‚Äì[77].
The techniques we have developed in this work can also
be applied to different problems. For example, a dynamically
updated language model like the one we have used for Ô¨Ånding
relevant snippets, could be used instead to identify unexplored
areas of a domain (i.e. issues in a change requests repository
which are not addressed by a requirements document), or to
match incoming bug reports or feature requests to commit
messages in open source projects. We intend to explore these
possibilities in future work. Moreover, the proposed approach
can assist analysts to manage the issues of memory recall [78]‚Äì
[80] after resuming an interrupted elicitation task.
451
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] M. Mohri, F. Pereira, and M. Riley, ‚ÄúWeighted Finite-State Transducers
in Speech Recognition,‚Äù Computer Speech & Language , vol. 16, no. 1,
pp. 69 ‚Äì88, 2002.
[2] M. Mohri, ‚ÄúWeighted Ô¨Ånite-state transducer algorithms. an overview,‚Äù
inFormal Languages and Applications , C. Mart ¬¥ƒ±n-Vide, V . Mitrana,
and G. P Àòaun, Eds. Springer Berlin Heidelberg, 2004, pp. 551‚Äì563.
[3] V . D. S ¬¥anchez A, ‚ÄúAdvanced Support Vector Machines and Kernel
Methods,‚Äù Neurocomputing , vol. 55, no. 1-2, pp. 5‚Äì20, 2003.
[4] T. Joachims, ‚ÄúText categorization with support vector machines: Learn-
ing with many relevant features,‚Äù in Machine Learning: ECML-98 ,
C. N ¬¥edellec and C. Rouveirol, Eds., Berlin, Heidelberg: Springer Berlin
Heidelberg, 1998, pp. 137‚Äì142.
[5] L. Goldin and D. M. Berry, ‚ÄúAbstÔ¨Ånder, a prototype natural language
text abstraction Ô¨Ånder for use in requirements elicitation,‚Äù Automated
Software Engineering , vol. 4, no. 4, pp. 375‚Äì412, 1997.
[6] P. Sawyer, P. Rayson, and K. Cosh, ‚ÄúShallow Knowledge as an Aid to
Deep Understanding in Early Phase Requirements Engineering,‚Äù IEEE
Transactions on Software Engineering , vol. 31, no. 11, pp. 969‚Äì981,
2005.
[7] G. Berry-Rogghe, ‚ÄúThe computation of collocations and their relevance
in lexical studies,‚Äù The computer and literary studies , pp. 103‚Äì112,
1973.
[8] R. Gacitua, P. Sawyer, and V . Gervasi, ‚ÄúOn the effectiveness of abstrac-
tion identiÔ¨Åcation in requirements engineering,‚Äù in 2010 18thIEEE
International Requirements Engineering Conference , 2010, pp. 5‚Äì14.
[9] P. Rayson and R. Garside, ‚ÄúComparing corpora using frequency
proÔ¨Åling,‚Äù in Proceedings of the Workshop on Comparing Corpora
- V olume 9 , ser. WCC ‚Äô00, Association for Computational Linguistics,
2000, pp. 1‚Äì6.
[10] T. Quirchmayr, B. Paech, R. Kohl, and H. Karey, ‚ÄúSemi-automatic
software feature-relevant information extraction from natural language
user manuals,‚Äù in Requirements Engineering: Foundation for Software
Quality ,P .G r ¬®unbacher and A. Perini, Eds., Springer International
Publishing, 2017, pp. 255‚Äì272.
[11] X. Lian, M. Rahimi, J. Cleland-Huang, L. Zhang, R. Ferrai, and M.
Smith, ‚ÄúMining requirements knowledge from collections of domain
documents,‚Äù in 2016 IEEE 24th International Requirements Engineer-
ing Conference (RE) , 2016, pp. 156‚Äì165.
[12] N. A. Ernst and J. Mylopoulos, ‚ÄúOn the Perception of Software
Quality Requirements during the Project Lifecycle,‚Äù in Requirements
Engineering: Foundation for Software Quality , R. Wieringa and A.
Persson, Eds., Berlin, Heidelberg: Springer Berlin Heidelberg, 2010,
pp. 143‚Äì157.
[13] N. Niu and S. Easterbrook, ‚ÄúExtracting and Modeling Product Line
Functional Requirements,‚Äù in 2008 16th IEEE International Require-
ments Engineering Conference , 2008, pp. 155‚Äì164.
[14] A. Mahmoud and G. Williams, ‚ÄúDetecting, Classifying, and Tracing
Non-functional Software Requirements,‚Äù Requirements Engineering ,
vol. 21, no. 3, pp. 357‚Äì381, 2016.
[15] E. Knauss and D. Ott, ‚ÄúAutomatic Requirement Categorization of
Large Natural Language SpeciÔ¨Åcations at Mercedes-benz for Review
Improvements,‚Äù in Proceedings of the 19th International Confer-
ence on Requirements Engineering: Foundation for Software Quality ,
ser. REFSQ‚Äô13, Essen, Germany: Springer-Verlag, 2013, pp. 50‚Äì64.
[16] C. Duan, P. Laurent, J. Cleland-Huang, and C. Kwiatkowski, ‚ÄúTowards
Automated Requirements Prioritization and Triage,‚Äù Requirements
Engineering , vol. 14, no. 2, pp. 73‚Äì89, 2009.
[17] Z. Shakeri Hossein Abad, O. Karras, P. Ghazi, M. Glinz, G. Ruhe, and
K. Schneider, ‚ÄúWhat Works Better? A Study of Classifying Require-
ments,‚Äù in 2017 IEEE 25th International Requirements Engineering
Conference (RE) , 2017, pp. 496‚Äì501.
[18] K. Verma and A. Kass, ‚ÄúRequirements analysis tool: A tool for
automatically analyzing software requirements documents,‚Äù in The Se-
mantic Web - ISWC 2008: 7th International Semantic Web Conference,
ISWC 2008, Karlsruhe, Germany, October 26-30, 2008. Proceedings .
Springer Berlin Heidelberg, 2008, pp. 751‚Äì763.
[19] J. Cleland-Huang, R. Settimi, X. Zou, and P. Solc, ‚ÄúAutomated clas-
siÔ¨Åcation of non-functional requirements,‚Äù Requirements Engineering ,
vol. 12, no. 2, pp. 103‚Äì120, 2007.
[20] J. Cleland-Huang, R. Settimi, X. Zou, and P. Solc, ‚ÄúThe Detection
and ClassiÔ¨Åcation of Non-Functional Requirements with Application toEarly Aspects,‚Äù in 14th IEEE International Requirements Engineering
Conference (RE‚Äô06) , 2006, pp. 39‚Äì48.
[21] M. Rahimi, M. Mirakhorli, and J. Cleland-Huang, ‚ÄúAutomated Extrac-
tion and Visualization of Quality Concerns from Requirements Speci-
Ô¨Åcations,‚Äù in 2014 IEEE 22nd International Requirements Engineering
Conference (RE) , 2014, pp. 253‚Äì262.
[22] A. Casamayor, D. Godoy, and M. Campo, ‚ÄúIdentiÔ¨Åcation of non-
functional requirements in textual speciÔ¨Åcations: A semi-supervised
learning approach,‚Äù Information and Software Technology , vol. 52,
no. 4, pp. 436 ‚Äì445, 2010.
[23] E. Knauss, D. Damian, G. Poo-Caamao, and J. Cleland-Huang, ‚ÄúDe-
tecting and Classifying Patterns of Requirements ClariÔ¨Åcations,‚Äù in
2012 20th IEEE International Requirements Engineering Conference
(RE) , 2012, pp. 251‚Äì260.
[24] Z. Kurtanovi ¬¥c and W. Maalej, ‚ÄúAutomatically classifying functional
and non-functional requirements using supervised machine learning,‚Äù
in2017 IEEE 25th International Requirements Engineering Confer-
ence (RE) , 2017, pp. 490‚Äì495.
[25] Y . Ko, S. Park, J. Seo, and S. Choi, ‚ÄúUsing ClassiÔ¨Åcation Techniques
for Informal Requirements in the Requirements Analysis-supporting
System,‚Äù Information and Software Technology , vol. 49, no. 11,
pp.1128 ‚Äì1140, 2007.
[26] J. Slankas and L. Williams, ‚ÄúAutomated Extraction of Non-functional
Requirements in Available Documentation,‚Äù in 2013 1st International
Workshop on Natural Language Analysis in Software Engineering
(NaturaLiSE) , 2013, pp. 9‚Äì16.
[27] E. Frank and R. Bouckaert, ‚ÄúNa ¬®ƒ±ve Bayes for Text ClassiÔ¨Åcation
with Unbalanced Classes,‚Äù Knowledge Discovery in Databases: PKDD
2006 , pp. 503‚Äì510, 2006.
[28] D. Klein and C. D. Manning, ‚ÄúAccurate Unlexicalized Parsing,‚Äù
inProceedings of the 41st annual meeting of the association for
computational linguistics , 2003.
[29] D. M. Blei, ‚ÄúProbabilistic Topic Models,‚Äù Commun. ACM , vol. 55,
no. 4, pp. 77‚Äì84, 2012.
[30] X. Yan, J. Guo, Y . Lan, and X. Cheng, ‚ÄúA Biterm Topic Model for
Short Texts,‚Äù in Proceedings of the 22Nd International Conference on
World Wide Web , ser. WWW ‚Äô13, ACM, 2013, pp. 1445‚Äì1456.
[31] D. D. Lewis, ‚ÄúNaive (Bayes) at Forty: The Independence Assumption
in Information Retrieval,‚Äù in Machine Learning: ECML-98: 10th
European Conference on Machine Learning Chemnitz, Germany, April
21‚Äì23, 1998 Proceedings ,C .N ¬¥edellec and C. Rouveirol, Eds. Springer
Berlin Heidelberg, 1998, pp. 4‚Äì15.
[32] M. Mohri, ‚ÄúFinite-state Transducers in Language and Speech Process-
ing,‚Äù Comput. Linguist. , vol. 23, no. 2, pp. 269‚Äì311, 1997.
[33] C. Cortes, P. Haffner, and M. Mohri, ‚ÄúA machine learning framework
for spoken-dialog classiÔ¨Åcation,‚Äù in Springer Handbook of Speech
Processing , J. Benesty, M. M. Sondhi, and Y . A. Huang, Eds. Springer
Berlin Heidelberg, 2008, pp. 585‚Äì596.
[34] N. Cancedda, E. Gaussier, C. Goutte, and J.-M. Renders, ‚ÄúWord-
sequence Kernels,‚Äù Journal of machine learning research , vol. 3,
no. Feb, pp. 1059‚Äì1082, 2003.
[35] M. Damashek, ‚ÄúGauging similarity with n-grams: Language-
independent categorization of text,‚Äù Science , vol. 267, no. 5199,
pp. 843‚Äì848, 1995.
[36] C. S. Leslie, E. Eskin, A. Cohen, J. Weston, and W. S. Noble,
‚ÄúMismatch string kernels for discriminative protein classiÔ¨Åcation,‚Äù
Bioinformatics , vol. 20, no. 4, pp. 467‚Äì476, 2004.
[37] A. Ben-Hur and W. S. Noble, ‚ÄúKernel methods for predicting protein-
protein interactions,‚Äù Bioinformatics , vol. 21, no. suppl 1, pp. i38‚Äìi46,
2005.
[38] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins,
‚ÄúText ClassiÔ¨Åcation Using String Kernels,‚Äù Journal of Machine Learn-
ing Research , vol. 2, pp. 419‚Äì444, 2002.
[39] C. C.P.H. M. Mohri, ‚ÄúRational Kernels,‚Äù International Journal of
Foundations of Computer Science , vol. 14, no. 6, pp. 957‚Äì982, 2003.
[40] P. Pecina, ‚ÄúLexical Association Measures and Collocation Extraction,‚Äù
Language Resources and Evaluation , vol. 44, no. 1, pp. 137‚Äì158, 2010.
[41] Z. S. Harris, ‚ÄúMathematical structures of language,‚Äù 1968.
[42] P. Goyal, L. Behera, and T. M. McGinnity, ‚ÄúA Context-Based Word
Indexing Model for Document Summarization,‚Äù IEEE Transactions on
Knowledge and Data Engineering , vol. 25, no. 8, pp. 1693‚Äì1705, 2013.
[43] D. Jurafsky and J. H. Martin, Speech and language processing . Pearson
London: 2014, vol. 3.
452
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. [44] C.-Y . Lin and E. Hovy, ‚ÄúAutomatic Evaluation of Summaries Using N-
gram Co-occurrence Statistics,‚Äù in Proceedings of the 2003 Conference
of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - V olume 1 , ser. NAACL
‚Äô03, Association for Computational Linguistics, 2003, pp. 71‚Äì78.
[45] S. Katz, ‚ÄúEstimation of probabilities from sparse data for the language
model component of a speech recognizer,‚Äù IEEE Transactions on
Acoustics, Speech, and Signal Processing , vol. 35, no. 3, pp. 400‚Äì401,
1987.
[46] I. H. Witten and T. C. Bell, ‚ÄúThe zero-frequency problem: Estimating
the probabilities of novel events in adaptive text compression,‚Äù IEEE
Transactions on Information Theory , vol. 37, no. 4, pp. 1085‚Äì1094,
1991.
[47] R. Kneser and H. Ney, ‚ÄúImproved Backing-off for M-gram Language
Modeling,‚Äù in 1995 International Conference on Acoustics, Speech,
and Signal Processing , vol. 1, 1995, 181‚Äì184 vol.1.
[48] H. Ney, U. Essen, and R. Kneser, ‚ÄúOn structuring probabilistic
dependences in stochastic language modelling,‚Äù 1, vol. 8, 1994, pp. 1
‚Äì38.
[49] A. K. Massey, J. Eisenstein, A. I. Antn, and P. P. Swire, ‚ÄúAutomated
Text Mining for Requirements Analysis of Policy Documents,‚Äù in 2013
21st IEEE International Requirements Engineering Conference (RE) ,
2013, pp. 4‚Äì13.
[50] J.-M. Davril, E. Delfosse, N. Hariri, M. Acher, J. Cleland-Huang,
and P. Heymans, ‚ÄúFeature Model Extraction from Large Collections
of Informal Product Descriptions,‚Äù in Proceedings of the 2013 9th
Joint Meeting on Foundations of Software Engineering , ser. ESEC/FSE
2013, ACM, 2013, pp. 290‚Äì300.
[51] E. Boutkova and F. Houdek, ‚ÄúSemi-automatic IdentiÔ¨Åcation of Features
in Requirement SpeciÔ¨Åcations,‚Äù in 2011 IEEE 19th International
Requirements Engineering Conference (RE) , 2011, pp. 313‚Äì318.
[52] E. Guzman and W. Maalej, ‚ÄúHow do users like this feature? a Ô¨Åne
grained sentiment analysis of app reviews,‚Äù in 2014 IEEE 22nd Inter-
national Requirements Engineering Conference (RE) , 2014, pp. 153‚Äì
162.
[53] T. Johann, C. Stanik, A. M. A. B., and W. Maalej, ‚ÄúSafe: A simple ap-
proach for feature extraction from app descriptions and app reviews,‚Äù in
2017 IEEE 25th International Requirements Engineering Conference
(RE) , 2017, pp. 21‚Äì30.
[54] J. Vincent, RT Essentials . O‚ÄôReilly Media, Inc., 2005.
[55] A. J. Viera, J. M. Garrett, et al. , ‚ÄúUnderstanding Interobserver Agree-
ment: The Kappa Statistic,‚Äù Fam Med , vol. 37, no. 5, pp. 360‚Äì363,
2005.
[56] J. R. Landis and G. G. Koch, ‚ÄúThe Measurement of Observer Agree-
ment for Categorical Data,‚Äù biometrics , pp. 159‚Äì174, 1977.
[57] V . I. Levenshtein, ‚ÄúBinary codes capable of correcting deletions,
insertions, and reversals,‚Äù in Soviet physics doklady , vol. 10, 1966,
pp. 707‚Äì710.
[58] U. Y . Nahm and R. J. Mooney, ‚ÄúText mining with information
extraction,‚Äù in Proceedings of the AAAI 2002 Spring Symposium on
Mining Answers from Texts and Knowledge Bases , 2002.
[59] E. S. Ristad and P. N. Yianilos, ‚ÄúLearning String-edit Distance,‚Äù IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. 20,
no. 5, pp. 522‚Äì532, 1998.
[60] D. M. Berry, ‚ÄúEvaluation of tools for hairy requirements and software
engineering tasks,‚Äù in 2017 IEEE 25th International Requirements
Engineering Conference Workshops (REW) , IEEE, 2017, pp. 284‚Äì291.
[61] T. Saracevic, ‚ÄúEvaluation of evaluation in information retrieval,‚Äù in
Proceedings of the 18th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval , ser. SIGIR
‚Äô95, ACM, 1995, pp. 138‚Äì146.
[62] A. Ferrari, G. O. Spagnolo, and S. Gnesi, ‚ÄúPURE: A Dataset of
Public Requirements Documents,‚Äù in 2017 IEEE 25th International
Requirements Engineering Conference (RE) , 2017, pp. 502‚Äì505.
[63] I. Feinerer and K. Hornik, ‚Äútm: Text Mining Package,‚Äù R package
version 0.5-7.1 , vol. 1, no. 8, 2012.
[64] D. Meyer, K. Hornik, and I. Feinerer, ‚ÄúText Mining Infrastructure in
R,‚Äù Journal of statistical software , vol. 25, no. 5, pp. 1‚Äì54, 2008.
[65] M. F. Porter, ‚ÄúAn Algorithm for SufÔ¨Åx Stripping,‚Äù Program , vol. 14,
no. 3, pp. 130‚Äì137, 1980.
[66] T. H. Cormen, Introduction to Algorithms . MIT press, 2009.
[67] M. Mohri and M. Riley, ‚ÄúAn EfÔ¨Åcient Algorithm for the N-Best-Strings
Problem,‚Äù in Seventh International Conference on Spoken Language
Processing , 2002.[68] J. Laurikkala, ‚ÄúImproving identiÔ¨Åcation of difÔ¨Åcult small classes by
balancing class distribution,‚Äù in ArtiÔ¨Åcial Intelligence in Medicine ,
S. Quaglini, P. Barahona, and S. Andreassen, Eds., Springer Berlin
Heidelberg, 2001, pp. 63‚Äì66.
[69] C.-C. Chang and C.-J. Lin, ‚ÄúLIBSVM: A Library for Support Vector
Machines,‚Äù ACM Trans. Intell. Syst. Technol. , vol. 2, no. 3, pp. 1‚Äì27,
2011, ISSN : 2157-6904.
[70] M. B. Wilk and R. Gnanadesikan, ‚ÄúProbability plotting methods for the
analysis for the analysis of data,‚Äù Biometrika , vol. 55, no. 1, pp. 1‚Äì17,
1968.
[71] Y . Elrakaiby, A. Ferrari, P. Spoletini, S. Gnesi, and B. Nuseibeh, ‚ÄúUsing
Argumentation to Explain Ambiguity in Requirements Elicitation In-
terviews,‚Äù in 2017 IEEE 25th International Requirements Engineering
Conference (RE) , 2017, pp. 51‚Äì60.
[72] S. C. Wong, A. Gatt, V . Stamatescu, and M. D. McDonnell, ‚ÄúUn-
derstanding data augmentation for classiÔ¨Åcation: When to warp?‚Äù In
Digital Image Computing: Techniques and Applications (DICTA), 2016
International Conference on , IEEE, 2016, pp. 1‚Äì6.
[73] S. F. Chen, D. Beeferman, and R. Rosenfeld, ‚ÄúEvaluation metrics for
language models,‚Äù 1998.
[74] K. Zechner and A. Waibel, ‚ÄúMinimizing Word Error Rate in Textual
Summaries of Spoken Language,‚Äù in Proceedings of the 1st North
American Chapter of the Association for Computational Linguistics
Conference , ser. NAACL 2000, Association for Computational Lin-
guistics, 2000, pp. 186‚Äì193.
[75] Z. Shakeri Hossein Abad, M. Rahman, A. Cheema, V . Gervasi, D.
Zowghi, and K. Barker, ‚ÄúDynamic visual analytics for elicitation
meetings with elica,‚Äù in 2018 IEEE 26th International Requirements
Engineering Conference (RE) , 2018, pp. 492‚Äì493.
[76] Z. Shakeri Hossein Abad, V . Gervasi, D. Zowghi, and K. Barker,
‚ÄúElica: An automated tool for dynamic extraction of requirements
relevant information,‚Äù in 2018 5th International Workshop on ArtiÔ¨Åcial
Intelligence for Requirements Engineering (AIRE) , 2018, pp. 8‚Äì14.
[77] Z. Shakeri Hossein Abad, ‚ÄúManaging multitasking in software devel-
opment tasks using visual analytics and machine learning,‚Äù 2018.
[78] Z. Shakeri Hossein Abad, G. Ruhe, and M. Bauer, ‚ÄúTask Interruptions
in Requirements Engineering: Reality Versus Perceptions!‚Äù In Require-
ments Engineering Conference (RE), 2017 IEEE 25th International ,
IEEE, 2017, pp. 342‚Äì351.
[79] Z. Shakeri Hossein Abad, O. Karras, K. Schneider, K. Barker, and
M. Bauer, ‚ÄúTask Interruption in Software Development Projects:
What Makes Some Interruptions More Disruptive Than Others?‚Äù In
Proceedings of the 22ndInternational Conference on Evaluation and
Assessment in Software Engineering 2018 , ACM, 2018, pp. 122‚Äì132.
[80] Z. Shakeri Hossein Abad, M. Noaeen, D. Zowghi, B. H. Far, and K.
Barker, ‚ÄúTwo sides of the same coin: Software developers‚Äô perceptions
of task switching and task interruption,‚Äù in Proceedings of the 22Nd
International Conference on Evaluation and Assessment in Software
Engineering 2018 , ser. EASE‚Äô18, ACM, 2018, pp. 175‚Äì180.
453
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. 