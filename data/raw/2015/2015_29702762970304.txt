EfÔ¨Åcient Detection of Inconsistencies in a Multi-developer
Engineering Environment
Andreas Demuth
Institute for Software Systems
Engineering
Johannes Kepler University
Linz, Austria
andreas.demuth@jku.atMarkus
Riedl-Ehrenleitner
Institute for Software Systems
Engineering
Johannes Kepler University
Linz, Austria
markus.riedl@jku.atAlexander Egyed
Institute for Software Systems
Engineering
Johannes Kepler University
Linz, Austria
alexander.egyed@jku.at
ABSTRACT
Software developers work concurrently on dierent kinds of
development artifacts such as requirements, architecture,
design, or source code. To keep these development arti-
facts consistent, developers have a wide range of consistency
checking approaches available. However, most existing con-
sistency checkers work best in context of single tools and
they are not well suited when development artifacts are dis-
tributed among dierent tools and are being modied con-
currently by many developers.
This paper presents a novel, cloud-based approach to con-
sistency checking in a multi-developer/-tool engineering en-
vironment. It allows instant consistency checking even if
developers and their tools are distributed and even if they
do not have access to all artifacts. It does this by system-
atically reusing consistency checking knowledge to keep the
memory/CPU cost of consistency checking to a small con-
stant overhead per developer. The feasibility and scalability
of our approach is demonstrated through an empirical vali-
dation with 22 partly industrial system models. A prototype
implementation implementation is available through the De-
signSpace Engineering Cloud.
CCS Concepts
Software and its engineering !Model-driven soft-
ware engineering;
Keywords
Incremental Consistency Checking; Multi-Developer Engi-
neering; Model-Driven Engineering
1. INTRODUCTION
Software engineering is an inherently collaborative disci-
pline with developers working concurrently on a wide rangeofdevelopment artifacts |requirements, use cases, design,
code, and more. To modify these development artifacts, an
equally diverse engineering tool landscape exists|each tool
usually specializing on specic kinds of development arti-
facts (e.g., Eclipse [1] for source code, IBM Rational Soft-
ware Architect [2] for UML models). Each tool thus only
presents a partial view of a software system [3]. After all,
developers often do not even need access to all engineering
artifacts [4, 5, 6, 7].
The software engineering landscape is characterized by the
distributed and concurrent modication of these develop-
ment artifacts by many developers. This follows a familiar
pattern: Typically, developers download development arti-
facts to their local workstations (i.e., checkout) and modify
them there in private before nally uploading the changes to
a shared repository for others to see (i.e., commit). In doing
so, developers create a local environment where they modify
development artifacts separately from the shared repository
and where they only have access to a subset of all devel-
opment artifacts|those that can be modied by the tools
they use. For example, the designer will use a modeling
tool to modify the models while the programmer will use a
programming tool to modify the source code.
Inconsistencies arise if development artifacts contradict.
It is easy to see that inconsistencies are particularly hard to
spot in situations where dierent developers modify develop-
ment artifacts privately within dierent tools. Here existing
work for consistency checking is lacking and ecient means
for managing consistency is needed [7]. While many consis-
tency checking approaches exist today (e.g., [3, 6, 8, 9, 10,
11, 12, 13, 14]) they do not focus on the multi-developer/-
tool problem. These approaches either require complete local
access to all development artifacts or they do not dierenti-
ate between private and public knowledge during consistency
checking, which is contradictory to the situation of develop-
ers working in private.
For example, it is hard for a designer working on a
checked-out design model to understand the consistency im-
plications of private changes with regard to source code that
is not locally available. We refer to the checked out devel-
opment artifacts as private working copies and there are
as many such copies as there are developers/tools working
concurrently|each having a partial view only and each re-
ecting changes that developers made in private. The base
problem is that today it is not possible that the designers
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970304
590
receive feedback about inconsistencies between model and
source code, if the latter is not available locally.
In this paper, we introduce a novel approach for consis-
tency checking in a multi-developer, multi-tool engineering
environment. Our approach relies on the DesignSpace Engi-
neering Cloud [13, 15] infrastructure to maintain i) a central,
public area that contains the entirety of development arti-
facts shared by all developers and ii) as many private work-
ing areas as there are developers to reect their individual,
not yet published modications. Changes that developers
perform locally in their tools are instantly propagated to
their respective private working areas in the cloud. These
changes are then instantly checkable against the public area,
therefore enabling a complete consistency checking.
The performance savings of our approach are the result of
systematic reuse of consistency checking knowledge. Our ap-
proach does not have any expensive initialization costs and
its benet increases with the number of developers working
concurrently. To validate our approach we analyzed its com-
putational and memory complexity. Furthermore, to show
its feasibility, we developed a prototype and conducted an
empirical evaluation to assess the scalability of our approach.
Finally, we discuss three partly industrial case studies in
which we applied our infrastructure.
2. PROBLEM ILLUSTRATION AND
BACKGROUND
This work builds on the Model/Analyzer [16, 10] which is
a single tool/single developer consistency checker. Like most
other consistency checking technologies [3, 11, 17, 18] it can-
not provide a holistic consistency check across all develop-
ment artifacts when a developer only has one development
artifact available (e.g., source code or model) To illustrate
this, we discuss a model of a video on demand (VOD) sys-
tem [19] that is concurrently modied by two developers:
Alice and Bob. Note that when referring to elements of
the example we will use the syntax [UML::<type>]<name> ,
where <type> is a placeholder for the specic UML type and
<name> for its name.
2.1 Initial State
Figure 1 shows the complete, public model of the VOD
system that both Bob and Alice are able to see. The UML
model consists of three diagrams: a class diagram showing
the entities Display andStreamer , a sequence diagram out-
lining the interaction among those two classes, and a state
chart depicting the state space of Streamer . We limit our il-
lustration to UML for simplicity. However, the public model
could also include requirements, use cases, source code, and
other development artifacts.
To express desired conditions that a model must satisfy
consistency rules are used. Below we show three exam-
ples of consistency rules, written in OCL [20], that apply
to UML models: CR1,CR2, and CR3in Listings 1, 2, and 3.
UML: : Message m
m. receiveEvent . covered  >f o r A l l ( L i f e l i n e l j
l . r e p r e s e n t s . type . ownedOperation  >e x i s t s (
Operation oj
o . name = m. name) )
Listing 1: (CR1) Message must be dened as an
operation in the receiver's class.
d:Display st:Streamer
stream 1.1
draw 1.2
wait 1.3(a) Sequence Diagram
stopped
playingstream wait
2.12.2 (b) State Space Diagram
Display
stop()
play()
draw()Streamer
stream()
pause()3.13.2
(c) Class Diagram
Figure 1: Current Public Version
UML: : Transition t
l e t c l a s s i f i e r : B e h a v i o r e d C l a s s i f i e r=
s e l f . owner . oclAsType ( Region ) . stateMachine .
context in
c l a s s i f i e r . oclIsTypeOf ( Class ) implies
c l a s s i f i e r . oclAsType ( Class ) . ownedOperation
 >e x i s t s ( o : Operation jo . name=s e l f . name)
Listing 2: (CR2) Action of statechart must be an
operation in the owner's class.
UML: : Class c
  a l l parents of UML Class c
l e t a l l P a r e n t s : Set(UML: : C l a s s i f i e r ) in
not allParents >i ncl u des ( c )
Listing 3: (CR3) No circular inheritance.
Each consistency rule is written for a specic context and
the rule must be evaluated for each instance of this context.
Since we focus on UML in this illustration, the context is a
specic UML element. CR3 with the context UML::Class
checks whether a given UML::Class is part of a circular
inheritance. For CR3 in the class diagram in Fig. 1 we
nd two UML::Class instances ( [UML::Class]Display and
[UML::Class]Streamer ) and thus two evaluations of CR3
are necessary|one for each class. We will refer to such
an evaluation as consistency rule instance (CRI) through-
out the rest of the paper. Similarly, CR1 and CR2 check
whether messages in sequence diagrams and transitions in
statecharts have names that correspond to operations in the
class diagrams (i.e., a message action must have an equally
named operation in a class).
Figure 1 depicts the entire UML model, and thus any con-
sistency checker applicable to UML models would be able to
determine its consistency with regard to the three consis-
tency rules. For example, the Model/Analyzer would rst
identify the CRIs needed by searching for all UML model in-
stances of the context elements and instantiating a CRI for
every instance found. For the VOD example, seven CRIs
are needed:
three instantiations of CR1 corresponding to
the three messages in the sequence diagram:
591CRI.1.1 for [UML::Message]stream , CRI.1.2
for [UML::Message]draw and CRI.1.3 for
[UML::Message]wait ;
two instantiations of CR2 corresponding to the two
transitions in the statechart diagram: CRI.2.1 for the
transition [UML::Transition]wait and CRI.2.2 for
[UML::Transition]stream ; and
two instantiations of CR3 corresponding to
the two classes in the class diagram: CRI.3.1
for [UML::Class]Display and CRI.3.2 for
[UML::Class]Streamer .
Except for CRI.2.1 and CRI.1.3 , all instances are consis-
tent. CRI.2.1 is inconsistent because there exists no oper-
ation in the class diagram for [UML::Transition]wait in
the statechart diagram. Similarly, CRI.1.3 is inconsistent
because there exists no operation for the \wait" message in
the sequence diagrams.
2.2 Multi-developer Consistency Checking
Consider now that Alice and Bob modify the UML model
separately. Each developer checks out the model to his or
her local workstation that runs a modeling tool and a con-
sistency checker. Normally, the public model would not only
have a UML model but also other kinds of development ar-
tifacts such as requirements, use cases. In analogy, let us
assume that there are two modeling tools involved and Al-
ice uses a modeling tool that can only modify class and state
chart diagrams whereas Bob uses a modeling tool that can
modify all three kinds of diagrams. Thus, Alice|and her
consistency checker|only has partial knowledge available.
Note that in the following, we append the starting letter of
the developer's name to the consistency rule instances to dis-
tinguish them (e.g., CRI.1.3.B is Bob's private consistency
rule instance of CRI.1.3 ).
2.2.1 Adaptations by Bob
Bob modies his private working copy of the UML model
by adding a feature: in order to stream movies a user must
rst connect to the [UML::Class]Streamer . Bob thus makes
the following changes consecutively:
an operation called \connect" is added to the
[UML::Class]Streamer in the class diagram,
a start state and transition is added in the state chart
diagram, and
a connect message is added to the sequence diagram.
Furthermore, Bob decides to resolve the two in-
consistencies CRI.1.3 and CRI.2.1 by renaming the
[UML::Operation]pause in the class diagram to\wait". The
new state of Bob's working copy is depicted in Fig. 2. These
changes alter the state of the UML model and thus have
implications on the CRIs. Incremental consistency checkers,
such as the Model/Analyzer, are able to react to changes
in a ne-grained manner without re-evaluating the entirety
of the model. The Model/Analyzer denes the concept of
ascope which is a set of elements that, if changed, should
trigger the re-evaluation of CRIs. This set is created at the
rst evaluation of the CRI and is simply the list of model
elements accessed during the evaluation of the CRI on the
UML model. Only changes to these accessed elements can
Display
stop()
play()
draw()Streamer
stream()
wait()
connect()3.1.B 3.2.B(a) Class Diagram
stopped
playingconnect
stream wait
2.1.B2.2.B2.3.B (b) State Chart Diagram
d:Display st:Streamer
connect 1.4.B
stream 1.1.B
draw 1.2.B
wait 1.3.B
(c) Sequence Diagram
Figure 2: Bob's Private Version
cause the CRI state to change. A CRI is thus re-evaluated
if an element in its scope changes|details are discussed by
Egyed [21]. Please note that most incremental consistency
checker have similar concepts (e.g., critical node [22]impact
matrix [17]).
The rst change triggers a re-evaluation of CRI.1.1.B
and CRI.1.3.B because a new operation was added to the
class [UML::Class]Streamer , which may aect their results.
However, the re-evaluation did not cause the CRIs to change
their state (i.e., change from consistent to inconsistent or
vice versa). The CRIs CRI.2.1.B and CRI.2.2.B need to
be re-evaluated as well. Furthermore, the second and third
change each require a new CRI, CRI.2.3.B and CRI.1.4.B
respectively because they introduce new UML model ele-
ments whose types match the context of a consistency rule
(CR2andCR1, respectively). This consistency check is indeed
necessary. Finally, the renaming of [UML::Operation]pause
to \wait" in the class diagram aects nearly all instances
of the interaction diagram (except 1.2.B ) and all instances
of the state chart, amounting to 6 CRI evaluations in to-
tal. This is because the changed name could aect any de-
ned transition in the state chart or any operation called on
[UML::Lifeline]Streamer in the interaction diagram. In-
deed, this change does resolve the inconsistency CRI.1.3.B .
In fact, all inconsistencies are resolved at this point and a
new feature (connect) is added.
2.2.2 Adaptations by Alice
Alice also wants to resolve the inconsistency between
the class diagram and the state chart. Unaware of Bob's
work she does so in a dierent way: by renaming the
[UML::Transition]wait to \pause". The state of her work-
ing copy is depicted in Fig. 3. This change resolves the
inconsistency and requires the re-evaluation of CRI.2.1.A .
However, recall that Alice's tool does not have available
the entire UML model. This demonstrates a situation
where a tool and its corresponding consistency checker
does not have access to all development artifacts|which
592is the norm [4, 7]. Her local consistency checker nds that
she indeed removed the inconsistency with the state chart.
However, her local consistency checker cannot know about
the inconsistency with the interaction diagram, which re-
mains (i.e., there is still no [UML::Operation]wait for the
[UML::Class]Streamer ).
3. PROBLEM STATEMENT
The Model/Analyzer, like other existing consistency
checking approaches, suers from incomplete knowledge in
private working copies. The following issues exist:
Incomplete information. Approaches such as the
Model/Analyzer [16, 21] or CLIME [3] are capable of
checking private working copies eciently and incre-
mentally. However, they require all development ar-
tifacts to be available locally, which is not often the
case in multi-developer, multi-tool development sce-
narios and should also not be the case just for the
sake of complete consistency checking and separation
of concerns.
No support for private working copies. Ap-
proaches that allow for incomplete local knowledge
(e.g., [23, 6, 8, 9, 24]) do not support the idea of pri-
vate adaptations (i.e., every change a developer per-
forms is immediately considered public). This limits
the applicability of those approaches because not ev-
ery developer would like changes to be publicly visible
immediately (i.e., trial and error).
4. GOAL
To address the problems described in Section 3, our goal is
to develop an infrastructure that is capable of checking the
impact of changes performed by many concurrently working
developers in private (i.e., many private work areas) with re-
spect to publicly available knowledge (i.e., one shared, public
area). The infrastructure's computational eort and mem-
ory consumption should be scalable and it should not repli-
cate all development artifacts to all private working areas.
5. APPROACH
Our approach to scalable consistency checking in a multi-
developer, multi-tool environment combines the advantages
of version control systems, private working areas, and in-
cremental consistency checkers. Our solution systematically
reuses computed consistency checking knowledge, and pro-
vides complete consistency checking for all developers at all
times.
stopped
playingstream pause
2.1.A2.2.A
(a) State Chart Dia-
gram
Display
stop()
play()
draw()Streamer
stream()
pause()3.1.A3.2.A(b) Class Diagram
Figure 3: Alice's Private Version
ToolTool-Adapter
observes commun-
icatesPWA M
‚àÜmodel
‚àÜCRIbaseVersion
.
.
.ToolTool-Adapter
observescommun-
icatesPWA 1
‚àÜmodel
‚àÜCRI
baseVersionVersion
Model
CRIPublic Area
NN-1 ...
.
.1Figure 4: Cloud
5.1 Overview
Figure 4 depicts an overview of our approach. The cloud-
based infrastructure is depicted on the right-hand side of
Fig. 4. The infrastructure maintains a version history
of all public development artifacts and their correspond-
ing consistency data ( public area ). The CRIs are thus
persisted alongside the development artifacts, which is a
key factor of our approach. The version history is ne-
granular, similar to the version control of Resource Descrip-
tion Framework(RDF) [25] or operation-based version con-
trol of EMF [26]. To represent arbitrary development arti-
facts, we use a unied representation, which we will refer to
simply as the model . In this unied representation individ-
ual elements of development artifacts and their properties
are subject to version control [27].
The left-hand side of Fig. 4 depicts development tools used
by developers (rectangles labeled as Tool).1The cloud main-
tains private work areas (pwa) (PWA1toPWAM) to capture the
dierences between the development artifacts in the tools
and the shared, public development artifacts in the cloud.
It is important to note that we do not expect development
tools to run within the cloud as developers typically use
them on their local workstations. However, PWAs do reside
in the cloud together with the public area. There are as
many PWAs as there are development tools used by devel-
opers. Each PWA reects what its developer has changed
in its respective tool as compared to the public area. This is
achieved by the means of tool-adapter s, depicted as grey
colored polygons that integrate the various tools with the
cloud-based infrastructure2. Tool-adapters support a typ-
ical SVN [30]-style workow (i.e., check-out, update, com-
mit) and, most importantly, they communicate development
artifact changes made by developers in tools to their respec-
tive PWAs in the cloud to ensure that the PWAs are always
up-to-date.
A tool is typically only concerned with the development
artifacts that it can edit (e.g., a modeling tool may not edit
source code). Yet, to perform a complete consistency check,
a consistency checker needs access to all development arti-
1As proof of concept, we already support a wide range
of such tools such as Eclipse for source code [1], IBM
RSA [2], Microsoft Excel and Visio, Eplan [28], Creo Ele-
ments Pro [29], generated ECore Editors.
2For more information on tool-adapters refer to Demuth et
al. [15].
593ModelElement
id:Integer
alive:Boolean
Property
name:StringDataType
StringBooleanFloatInteger
properties
*
value*Figure 5: Model Element
facts. While PWAs reect the ongoing works of their cor-
responding developers/tools. Placing a PWA in the cloud
provides it with access to all public, version controlled de-
velopment artifacts. Therefore, by extension a consistency
checker operating on the PWAs in the cloud has access to
all development artifacts|private and public. This satis-
es the basic requirement for complete consistency checking.
However, this alone does not guarantee scalability. Simply
executing the Model/Analyzer|or any other consistency
checker|in PWAs in the cloud would lead to unnecessary
CRI evaluations and unnecessary memory consumption be-
cause each PWA might replicate same/similar consistency
information. Consider the problem illustration again where
after obtaining the model, both private working areas of Al-
ice and Bob have available local copies of their CRIs to re-
ect their private state of consistency. For example, neither
Bob's nor Alice's changes aected the third consistency rule
(circular inheritance). Thus, CRI.3.1.A/B andCRI.3.2.A/B
were initially needlessly evaluated by both Alice's and Bob's
consistency checker.
To address this issue, each PWA not only maintains
the  of the model with respect to the base version (i.e.,
pwa.model ) but also its corresponding consistency  (i.e.,
the added/modied/removed CRIs stored in pwa.CRI).
Whenever a developer modies a development artifact, the
consistency checker re-evaluates the aected CRIs. When
CRIs change then they are stored as CRI deltas in the PWA,
which constitutes the dierence between the tool's consis-
tency state and public version's consistency state. The ver-
sion controlling of consistency data also avoids expensive,
initial batch consistency checking as the CRI can be checked-
out together with the artifacts.
5.2 UniÔ¨Åed Representation
Before we discuss the intrinsics of our approach, we need
to discuss how development artifacts are stored in the cloud
and how this allows us to integrate development tools with
our infrastructure. As previously mentioned, development
artifacts (e.g., models, source code) are translated to a uni-
ed representation, called model . A simplied metamodel
for this uniform representation is depicted in Fig. 5.
A model element has a unique id. Furthermore, the
boolean ag alive indicates whether the model element is
alive or not (i.e., whether it has been deleted or not). More-
over, model elements may have a variable number of prop-
erties , which are key-value pairs. Values of properties are
either of basic data types such as Boolean ,Integer ,Float ,
and String or references to other model elements (notice
that the inheritance of DataType extends to ModelElement ).Any tool that stores its development artifact in a graph-
like structure of nodes and edges can be mapped to such a
simple, unied representation (e.g.,ECore [31], ASTs). The
semantics do not change.
5.3 Check-Out
To follow the illustrative example, Alice at rst
needs to obtain (i.e., check-out) the class and state-
chart diagrams. Alice thus uses the check-out func-
tion in her modeling tool|provided by the tool-adapter|
where she species the model and version she wants to
check out. Listing 4 describes the check-out algorithm.
checkOut ( baseVersion , model )
pwa = create PrivateWorkingArea ( baseVersion )
return pwa , getModelElements ( model , baseVersion )
Listing 4: CheckOut
The algorithm rst creates a PWA for her tool, which ref-
erences a version of the public area as base version. The
PWA's  of the model is empty upon check-out because Al-
ice has not have made any changes yet ( pwa.model . Like-
wise, pwa.CRIis also empty. The algorithm returns a tuple
of the PWA and the checked-out model elements to the tool
adapter. The tool adapter then translates the model ele-
ments to her tool's internal language|which is tool-specic
and therefore not depicted.
5.4 Changes
After checking out a model, developers may modify it
within their development tool. Changes made by a devel-
oper are observed by the tool adapter and propagated to the
developer/tool's corresponding PWA as a change of model
elements. There the changes are stored as deltas with re-
gard to the base version. Less straightforward is comput-
ing the consistency implications of these changes. Aected
CRIs have to be re-evaluated and the new state needs also
be stored in the PWA. However, this process varies with re-
gard to the kind of changes: modification ,addition , and
deletion .
5.4.1 ModiÔ¨Åcation
Recall that Alice renames the transition from \wait" to
\pause". She does this in her tool and the tool adapter
will update Alice's pwa.model with the modied name.
Listing 5 describes the algorithm to handle modications
of model element properties in the PWA. The changed
model element property is added in line 2 as a model delta.
The syntax of this line is to be understood as follows:
pwa.model holds the  for all model elements. The 
for an individual model element with identier ican be ac-
cessed through pwa.model[ i]. Recall, that each model
element is identied by a unique id, which is utilized for
this access. Further, the  of individual properties of
model elements can be accessed by pwa.model[ i].[p],
pagain being the identier for the property In this
case, the name of a property is the unique identier
for the access of property s. An example of such an
access could be pwa.[[UML::Transition]wait].[name] .
594modify (pwa , modelElement , property , oldValue ,
newValue )
pwa . model [ modelElement ] . [ property ] = newValue
CRIs =fCRI2pa [ baseVersion ] . CRI j
CRI . a l i v e == true and CRI =2pwa . CRIg
CRIs[= pwa . CRI
f o r CRI2CRIs
i f( modelElement , property ) 2CRI . scope
pwa . CRI [ CRI ] = evaluate (CRI)
Listing 5: Modication of a Model Element
This model element/property does not exist in the PWA
before the modication and is set to the value \pause"|
thus overwriting the value \wait" from the same model ele-
ment/property found in the public model. Each model ele-
ment/property may have at most one entry in pwa.model .
Should Alice later overwrite this name again, then the newer
name would overwrite the previous change in the PWA.
For each modication, all CRIs aected by the modica-
tion must be re-evaluated and the result must be persisted
inpwa.CRI. Similar to model elements for tool-adapters,
a consistency checker has an internal language for CRIs.
Thus, CRIs are similarly translated into the unied represen-
tation and also uniquely identied and accessible as model
elements. Since these CRIs are as well subject to version
control, we must distinguish two cases: CRIs that already
are in pwa.CRIand CRIs that are in the public area. The
variable CRIs is the union of the private CRIs (line 5) and
the public CRIs (lines 3{4). Please note that in line 3 a new
syntax is introduced to access the public area (pa). There-
fore, pa[version] accesses a specic version of the public
area. Computing the private CRIs is straightforward be-
cause it is pwa.CRIitself. Computing the public CRIs is
more complex for two reasons: we must not include any
public CRIs that 1) have been deleted (i.e., certain changes
require CRIs to be deleted in pwa.CRI) and 2) have newer
counterparts in the PWA (i.e., changes may require CRIs
to be re-evaluate the new result of an evaluation must then
overwrite their public counterparts).
Once collected, we need to re-evaluate those CRIs whose
scopes contain the changed model element property (lines
6-8). The new evaluation result is then stored in pwa.CRI
(line 8)|which either overwrites a previous private result or,
if there was none, creating a new entry and therefore eec-
tively overwriting the public result. This explains the need
to lter the overwritten public CRIs as discussed above. In
case of Alice's change, pwa.CRIwas initially empty. Of the
seven CRIs that existed in the public area during check-out
(Fig. 1), only CRI.2.1 had a scope that included the tu-
ple([UML::Transition]wait, name) . Therefore, CRI.2.1
needs to be re-evaluated, resulting in the private CRI.2.1.A
stored in her PWA. This private CRI exists for Alice only
and overwrites the public CRI.2.1 . The public CRI.2.1 can-
not be replaced physically because it corresponds to the pub-
lic model element. For example, Bob's PWA still needs to
see the public CRI.2.1 as he does not see Alice's changes at
this point and has not made his changes either. The eval-
uation mechanism evaluate(CRI) is not discussed at this
point for brevity|please refer to Reder and Egyed [16].
5.4.2 Addition
In case of an addition, the new model element and all
its properties need to be inserted in pwa.model (line 2in Listing 6). The addition of a model element cannot
cause a re-evaluation because a new model element can-
not yet be part of the scope of any CRI (the addition of-
ten coincides with a modication in which case the mod-
ify algorithm takes care of the latter). However, if a
consistency rule exists whose context matches the type of
the model element then a CRI must be created and evalu-
ated (lines 5{8). For example, recall that Bob adds a new
[UML::Message]connect in his sequence diagram (Fig 2).
add (pwa , modelElement )
f o r property of modelElement
pwa . model [ modelElement ] . [ property ] =
modelElement . [ property ]
f o r CR2CRs
i fcr .context == modelElement . type then
CRI = create instance
pwa . CRI [ CRI ] = evaluate (CRI)
Listing 6: Addition of a Model Element
After Bob's tool adapter sends this newly added model
element, the add algorithm adds the new message to
pwa.model and adds a newly instantiated CRI.1.4.B to
pwa.CRI|recall that a CRI is instantiated for every in-
stance of a matching context element.
5.4.3 Deletion
The deletion of a model element (Listing 7) requires the re-
moval of all CRIs whose context elements match the deleted
element, as opposed to additions that cause the creation of
CRIs. Similar to addition, deletion often coincides with a
modication in which case the modify algorithm takes care
of the latter. A CRI residing in the private work area can
be deleted simply by removing it from pwa.CRI. However,
CRIs residing in the public area cannot be deleted because
they should continue to exist in the public base version.
These CRIs must be agged as\not alive"in further versions.
This is done by adding/modifying a CRI to pwa.CRIwith
the alive ag being false. Model elements are handled like-
wise. For example, should Bob delete [UML::Message]wait
then the model element and its CRI must be agged
deleted and added to the pwa.model and pwa.CRI.
d e l e t i o n (pwa , modelElement )
CRIs = . . . // as defined in change ( . . )
f o r CRI2CRIs
i fmodelElement 2CRI . scope then
i fCRI =2pa [ baseVersion ] . CRI then
remove CRI from pwa .  . CRI
else
CRI . a l i v e = f a l s e
i fmodelElement =2pa [ baseVersion ] . model then
f o r property of modelElement
remove pwa . model [ modelElement ] . [ property ]
else
pwa . model [ modelElement ] . a l i v e = f a l s e
Listing 7: Deletion of a Model Element
5.5 CRI Evaluations and Property Access
During the evaluation of a CRI the consistency checker
accesses model element properties (e.g., the name of
[UML::Class]Display ). Since a PWA possibly overwrites
any public version of a property, these accesses need
to be handled dierently from traditional approaches.
For example, when the consistency checker re-evaluates
595CRI.2.1.A then the transition's name should be \pause",
not \wait". Algorithm getProperty (Listing 8) shows
that we rst need to look for a model element prop-
erty in pwa.model and return its value, if found.
getProperty (pwa , modelElement , property )
i f:modelElement . a l i v e then throw Exception
i f( modelElement , property ) 2pwa . model then
return pwa . model [ modelElement ] . [ property ]
else return pa [ baseVersion ] . model [ modelElement
] . [ property ]
Listing 8: Property Accesses
However, if the model element is marked as\dead"an excep-
tion will be thrown. Otherwise, we return the value of the
property of the public model's base version. Therefore, the
consistency checker has access to all model elements and
properties (public and private) but the private ones have
precedence over the public ones. Each PWA thus reects its
own view of the complete model.
5.6 Commit
Once Alice and Bob are done with their changes, they will
want to commit them. Committing (Listing 9) requires de-
termining possible version conicts. Algorithm conflicts
in Listing 10 denes how to obtain conicts. A model
element conict ( conflictME ) exists for those model ele-
ments in pwa.model for which a newer version (higher
than the base version) exists. To determine the existing con-
icts, the deltas of a range of versions need to be accessed
(pa[lowerVersion, higherVersion] ). The same holds for
CRI conicts ( conflictCRI ) if a new version of a CRI exists
upon commit. If conicts are detected then the commit is
aborted and developers are expected to resolve them anal-
ogous to SVN and other repositories. Once no conicts are
detected then the PWA changes are moved to the public area
and given a new version number|a cheap operation since
they already reside in the cloud. Finally, pwa.model and
pwa.CRIare emptied because after a commit there is again
no dierence between the tool artifacts and the public area.
commit (pwa)
conflictsME , conflictsCRI = c o n f l i c t s (pwa ,
version )
i fconflictsME6=; _ conflictsCRI 6=;then
return
pa [ HeadRevision () +1]. model = pwa . model
pa [ HeadRevision () +1].CRI = pwa . CRI
pwa . model = ;
pwa . CRI =;
Listing 9: Commit
c o n f l i c t s (pwa , version )
conflictsME = fmodelElement 2pwa . modelj
modelElement 2pa [ pwa . baseVersion , version ] .
model
_9prop2modelElement . [ property ] :
prop2pa [ pwa . baseVersion , version ] . model g
conflictsCRI = fCRI2pwa . CRIj
CRI2pa [ pwa . baseVersion , version ] . CRI g
return ( conflictsME , conflictsCRI )
Listing 10: Conicts
5.7 Update
Developers may update the model they are currently mod-
ifying, if a newer version or conicts exists. Before updat-ing, version conicts (if any) must be determined and re-
solved manually. Updating a private working area (List-
ing 11) consists of the following steps: i) determine the
dierences between the PWA's base version and the de-
sired version, ii) determining the currently existing version
conicts, iii) updating the base version to the new higher
version, and nally returning the conicts and the dier-
ences to the tool. Finally, the dierences between base ver-
sion, new desired version result, and already existing deltas
in the PWA may require additional consistency checks.
update (pwa , version )
d i f f = pa [ pwa . baseVersion , version ] . model
conflictsME , conflictsCRI = c o n f l i c t s (pwa ,
version )
pwa . baseVersion = version
CRIs =fCRI2pa [ baseVersion ] . CRI j
CRI . a l i v e == true and CRI =2pwa . CRIg
CRIs[= pwa . CRI
f o r ( modelElement , property ) 2d i f f
f o r CRI2CRIs
i f( modelElement , property ) 2CRI . scope
pwa . CRI [ CRI ] = evaluate (CRI)
return conflictsME , conflictsCRI , d i f f
Listing 11: Update
6. V ALIDATION
In Section 5 we discussed how our approach integrates
PWAs with a public area in a cloud-based environment to
enable comprehensive, complete consistency checking in a
multi-developer environment. To validate our approach, we
i) demonstrate its feasibility by developing a prototype im-
plementation, ii) analyzed the computational complexity of
our approach, iii) performed an empirical evaluation to as-
sess the actual overhead imposed by our approach compared
to the Model/Analyzer applied to UML on a single tool, iv)
analyzed the memory consumption of storing the CRIs, v)
discuss the advantages of our approach in terms of memory
consumption for each PWA, and vi) conducted three case
studies to demonstrate the applicability of our approach.
6.1 Prototype Implementation
This section provides a short overview of the implementa-
tion details of the version control system and the incremental
consistency checker.
Version Control System - DesignSpace The version
control proof of concept implementation, called DesignSpace
(DS) [13, 15], is a cloud-based infrastructure that allows to
version control arbitrary development artifacts on a ne-
granular basis.
Incremental Consistency Checker - Model/-
Analyzer As consistency checker the Model/Analyzer
(M/A) [32] was adapted and employed. Previously exist-
ing implementations already worked with UML, EMF Core
(ECore ) [31], and also RDF. For this work, it was adapted
to work with the representation used by the VCS and its
change notications.
6.2 Computational Complexity
The computational complexity of consistency checking is
mostly a factor of the number of necessary CRI evalua-
tions. The changes that cause the re-evaluation of CRIs
areaddand modify |the deletion of model elements does
not cause re-evaluations. Previous validations of the Mod-
el/Analyzer [21] suggested that the average number of eval-
596Table 1: Regression Results for Total Processing
Time
Total Processing Time (ms)
Model Elements 0.0004581
(0.0000068)
Aected Instances (AI) -0.0017938
(0.0074605)
Evaluation Time per Instance (ET) -0.0001086
(0.0003659)
AI*ET 1.0003651
(0.0002575)
Observations 48708
R20.9996
Standard errors in parentheses
p <0:05,p <0:01,p <0:001
uations of CRIs per change is between 3 and 11 (depend-
ing on the model size). Following, we denote this value as
constant c. We dene the set of performed changes (which
trigger re-evaluations; i.e., add, modify) by an individual de-
veloper as change:am . Equation 1 denes the total number
of CRI evaluations CRI:E approach as the sum of evaluations
required for each individual developer.
CRI:E approach =jdeveloperjX
i=1(cjchange:am ij) (1)
This equation shows that the computational complex-
ity grows linearly with the average number of performed
changes per developer and the total number of developers.
Thus our approach scales.
To empirically assess the overhead imposed by our ap-
proach, we re-enacted a previous evaluation of the Model/-
Analyzer using our cloud (i.e., we used the same models
and performed random changes as described by Reder and
Egyed [16]). For this, we simulated changes for all model
elements in each of the used models. Specically, we cap-
tured the time required for re-evaluating all aected CRIs
and to persist the new result. Each change was performed
several times and the raw data for each change was recorded.
The experiment was performed on a Windows 7 Professional
PC with an Intel(R) Core(TM) i7-3770 CPU @ 3,4GHz and
16GB of RAM. Figure 6 depicts the evaluation times per
aected CRI on average depending on the project size. We
furthermore performed an OLS regression analysis on the
total processing time (i.e., the time it takes to nd aected
CRIs and to re-evaluate them). The obtained results are
summarized in Table 1, which shows that the model size
has a signicant, yet quite small eect on the evaluation
time. Increasing the model size by 1,000 elements leads to a
total processing time increase of 0.4ms on average. The re-
sults furthermore indicate that the number of aected CRIs
and evaluation times per CRI individually do not aect the
total processing time. However, in combination those fac-
tors are signicant determinants of the total processing time.
On average, an increase of the number of aected instances
by one increases the total processing time by 23ms. An in-
crease of the evaluation time per aected instance by 1ms in-
creases the total processing time by 1.5ms on average. Note
that more than 99.9% of sample variations are explained by
the analyzed factors. Comparing to previous evaluations of
the Model/Analyzer [16], we obtained similar result and our
framework did not slow down the evaluation times.
103219289417511762
11131272128012951404145116571706178319722211258126308076875998260.50.60.70.80.91.0
Model ElementsEvaluation Time per CRI [ms]Figure 6: Evaluation Time per aected Instance
6.3 Memory Consumption
The memory consumption of our approach correlates with
the number of CRIs that need to be maintained overall and
for each developer individually. Equation 2 denes the total
number of CRIs that have to be persisted in our approach
(CRI approach ). The rst factor describes the set of CRIs
needed for the public area, which is the number of CRIs for
a given model ( CRI(model )). Furthermore, since a PWA
stores the  with respect to the base version, we must con-
sider the eect of modications, additions, and deletions.
Each of these operations adds a new entry to pwa.CRI. Re-
call the average number of CRIs aected by a single change
c, each performed change adds cCRI entries. Note that the
number of CRIs needed for an entire model grows linearly
with the model size (factor CRI(model )). The CRIs' mem-
ory consumption in the public area is thus linear with the
model size. Furthermore the CRI's memory consumption
in the PWAs is again linearly dependent on the number of
changes per developer and the total number of developers.
This equation again shows the scalability of our approach.
CRI approach =CRI(model ) +jdeveloperjX
i=1(cjchange ij) (2)
6.4 Version Control Mechanism
In this section, we discuss the advantages of our approach
in terms of memory consumption for each individual PWA.
As previously stated, a PWA stores the  model and fur-
thermore  CRI. We provide a behavioral analysis of how a
PWA's memory footprint depends on model changes com-
pared to using the Model/Analyzer as a plugin.
In this discussion we use the function M(x) to denote
the memory consumption of a specic element x. In our
approach, for both  model and  CRIthere exists an up-
per bound in terms of memory consumption: i) M(model ),
the memory it takes to store the complete model after the
adaptations are nished by a developer (Equation 3), and ii)
M(CRI), the memory the consistency checker needs to store
the corresponding complete consistency information (Equa-
tion 4). Note that the plugin Model/Analyzer's footprint
597|‚àÜmodel |
|model |Memory
Model
Elements1=MSUM(model ) +M(CRI )
1
linear memory progression f(‚àÜ)
expected memory progression h(‚àÜ)c
mc.5
mm‚Äôh(m){h(m‚Äô)h(x1){h(x2)h(.5)
h(‚àÜ)
f(‚àÜ)
x1 x2Figure 7: Memory Consumption
always equals MSUas dened in Equation 5. MSUthe mem-
ory consumption of a complete consistency check where all
development artifacts are locally available.
M(model )M(model ) (3)
M(CRI)M(CRI) (4)
MSU=M(model ) +M(CRI) (5)
Intuitively, in the worst case (i.e., if the complete model was
changed) our approach equals the plugin Model/Analyzer
as in this case our approach stores the whole model again
in its entirety in the PWA and re-evaluates all CRIs. Sub-
sequently, as long as this is not the case our approach only
stores a fraction of the whole model. Thus, our approach
provides the advantage of ( M(model ) M(model )) +
(M(CRI) M(CRI)). Therefore, the amount of saved
memory depends on the size of  model and  CRI.
Equation 6 { Equation 7 state our assumptions about
both  model and  CRI. The size of  model is expressible
through a function of the model size (Equation 6) (e.g., a
developer may always change a xed number of elements per
commit). The size of  CRIis determined by the function
CRI (Equation 7), which takes as argument  model .
jmodelj=DeltaModel (jmodelj) (6)
jCRIj=CRI(model ) (7)
Memory consumption of PWAs is mostly inuenced by
model . In Fig. 7 a behavioral analysis for possible cases of
DeltaModel is presented, as described below.
6.4.1 Memory Consumption per Distinct Model Ele-
ments Changed
First, we discuss memory consumption per distinct
changed model elements, which is plotted on the pos-itive y-axis in Fig. 7. Values on the positive y-axis
are normalized as follows, ( M(model ) +M(CRI))=MSU
(i.e.,actual memory consumption
possible memory consumption). The x-axis shows that the
ratio of distinct model element changes to the model size (1
implies the entire model changed).
Two possible assumptions for distinct elements changed
are: 1) memory consumption may rise linearly with the per-
centage of changed elements (linear function f()) or more
pessimistically 2) memory consumption may rise faster in
the beginning than towards the end h(). The function
hrepresents a pessimistic case for our framework, as few
changes would already lead to high memory consumption.
Consider now x1and its corresponding memory consump-
tion of h(x1). If a developer changes more of the model (e.g.,
x2) then the memory consumption rises to h(x2). Hence
memory is saved with regard to MSU(even under a pes-
simistic assumptions).
6.4.2 Memory Consumption Depending on Model
Size
The negative y-axis indicates model sizes in total. One
can assume dierent functions of how much of a model is
changed by a developer. Consider now, that before each
commit regardless of model size a roughly constant number
of elements is changed (functionc
m), resulting in the mem-
ory consumption of h(m). As models increase in size then
the memory consumption relatively decreases. For example,
if the model size increases from m0tomthen memory con-
sumption changes from h(m0) toh(m). If we assume that the
commit sizes increase with larger models then our approach
is still benecial. For example, assume that each commit
changes about fty percent of the model. In this case re-
gardless of model size the memory savings of our approach
is|based on the assumed memory consumption|constant
(e.g., 1- h(:5)).
6.5 Case Studies
Our infrastructure was employed in three user studies,
which we briey discuss next. One of these case studies
was performed with the Austrian Center of Competence in
Mechatronics (ACCM) , another with the Flanders' Mecha-
tronics Technology Center (FMTC) , and a third study in-
volved students of the Johannes Kepler University.
ACCM Robot Arm: This case study involved the de-
sign, mechanical calculation and partial implementation of a
robot arm. The ACCM group provided the mechanical cal-
culation of the robot arm in several Excel sheets and CAD
drawings. UML models of the robot arm were drawn in IBM
RSA, Eclipse provided the source code. The mechatronic de-
sign followed a trial-and-error cycle where most changes were
instantly propagated in a synchronous collaboration style.
The scope of the project exceeds the discussion in this pa-
per. However, we provided instant consistency checking and
the ability to dene and check cross-tool traceability links.
FMTC: This FMTC case study involved EPLAN Electric
P8 [28] drawings that had to be kept consistent with source
code. The drawings and code were provided by a third-party
company. The goal was to enable instant consistency check-
ing across tool boundaries in an ecient manner.
Student Experiment: Finally, our approach was ap-
plied in a student survey, conducted as part of a research
project by a K2-Center of the COMET/K2 program. The
goal was to study the benets of dierent representations for
598expressing dependencies in mechatronical designs. In par-
ticular, whether graph visualization are inferior to matrix
representations. Participants of the experiment were 32 stu-
dents who had to perform a design refactoring based on a
requirement from a customer. The nature of the case study
required that all development artifacts resided in the same
private work area, which enabled a quick trial-and-error cy-
cle given the provided consistency information.
6.6 Threats to Validity
In terms of computational complexity, we demonstrated
that for each developer only a small set of CRIs needs to
be stored/evaluated|i.e., the CRIs that are aected by
changes made by the developer. Furthermore, the model of
the version control mechanism showed that it is optimal in
terms of memory consumption. The equations and models
dened in the validation are based on the presented algo-
rithms and observed behavior of the Model/Analyzer. Thus
their correctness can be inferred from the algorithms itself.
The empirical validation conrmed that our approach scales
well (based on a diverse and large set of models authored by
dierent developers). As to the validity of the empirical vali-
dation, we believe that the used models were representative.
They were diverse in size and domain, and were created by
dierent developers and companies. The case studies showed
that our infrastructure is applicable to real world problems.
The paper did not discuss performance threats the communi-
cation overhead might pose, as this is mostly an implemen-
tation detail. Further, incrementally propagating changes
from tools to the PWA leads to more communication than
the occasional batch processing of changes. We ignored this
in our models but believe that this does not pose a threat to
validity, as various cloud-based services already employ this
pattern (e.g., Google Docs).
7. RELATED WORK
Following, we discuss consistency checking and version
control research related to our approach.
Global Consistency Checking : These approaches ad-
dress the general problem of consistency checking across pos-
sibly distributed development artifacts of a system. Finkel-
stein et al. [8] introduced consistency checking in multi per-
spective specications. Each developer owns a perspective
(i.e., a viewpoint [5]) of the system according to his or her
knowledge, responsibilities or commitments. Multiple view-
points can describe the same design fragment, leading to
overlap and the possibility of inconsistencies. The issues in-
volved in inconsistency handling of multi perspective speci-
cations are outlined by Finkelstein et al. [33]. An important
insight for handling consistency is to allow models to be tem-
porarily inconsistent, rather than enforcing full consistency
at all times. Despite this advances, no implementation is
provided. Further, the approach does not dierentiate be-
tween a public state that is xed and private modications.
Nentwich et al. [12] present a framework ( xlinkit ) for consis-
tency checking distributed software engineering documents
encoded in XML. Sabetzadeh et al. [34] presented global
consistency checking by model merging. The approach fo-
cuses on handling inconsistencies between multiple models
expressed in a single language. Although both approaches
consider distributed models, at the time of the consistency
check there is no distinction between private adaptations
and public knowledge.Consistency Checking in General : Numerous ap-
proaches exist for consistency checking, specializing on spe-
cic artifact types or across artifacts [35]. Two approaches
need to be highlighted, as these could have been replace-
ments for the Model/Analyzer. Blanc et al. [36] look at the
sequence of operations used to produce the model rather
than at the model itself. Thus they can not only verify
structural consistency of models but also methodological
consistency. Reiss presented an approach (CLIME) to incre-
mental maintenance of software artifact [3]. This approach
covers a multitude of development artifacts (presented are
source code, models and test cases). Unied representation
information is extracted from the development artifact (e.g.,
symbol table for source code) and stored in a database.
Version control : Version control for text-based develop-
ment artifacts permeate software engineering and academia.
Extensive research was conducted on version control of mod-
els [37]. Research in the area of version control systems an-
alyze their version controlled development artifacts to nd
inconsistencies. An example of such an approach was pre-
sented by Taentzer et al. [38]. It considers the abstract syn-
tax of models as graphs and identies two kinds of con-
icts, operation-based and state-based as a result of merged
graph modications. State-based conicts are concerned
with the well-formedness, operation-based conicts are then
concerned on the parallel dependence of graph transforma-
tions and the extraction of critical pairs. Cicchetti et al. [39]
proposed a meta-model for representing conicts which can
be used for specifying both syntactic as well as semantic
conicts. Finally, there are GIT [40] and Apache Subversion
(SVN) [30]. Both inherently provide capabilities to create
a continuous integration, during which source code checks
are executed. However, we are not aware of any tools to
extent this idea to also check consistency across multiple ar-
tifacts. Applying our approach to the continuous integration
phase would allow that for a commit or push to a feature
branch the consistency checker would verify the impacts of
the performed changes with respect to all development arti-
facts. Another distributed revision control and source code
management system is GIT [40].With GIT, each obtained
working directory is a full-edged repository|a clone copies
the entire history of the repository to the working directory.
Therefore, a developer has the complete standard workow
available without being dependent on network access or a
central server. To publish changes, a push to a remote server
is necessary, which may at rst be pushed to a staging area
before being approved and becoming part of the main trunk.
8. CONCLUSION
This paper presented a novel approach to multi-developer
consistency checking. The approach eliminates consistency
checking redundancies in a multi-development environment
to reduce the CPU and memory footprint to a relative con-
stant per developer. We demonstrated that our approach
eliminates the model size as a scalability factor and is fast.
For future work, further development tools will be integrated
with our infrastructure (i.e., create tool-adapters).
9. ACKNOWLEDGMENTS
The research was funded by the Austrian Science Fund
(FWF): P25513-N15 and P25289-N15, and the Austrian
Center of Competence in Mechatronics (ACCM): C210101.
59910. REFERENCES
[1] \Eclipse http://www.eclipse.org/ ,2013.."
[2] \IBM Rational Software Architect
https://www.ibm.com/developerworks/
rational/products/rsa/ ,2013.."
[3] S. P. Reiss, \Incremental Maintenance of Software
Artifacts," IEEE Trans. Software Eng. , vol. 32, no. 9,
pp. 682{697, 2006.
[4] P. Kruchten, \The 4+1 View Model of Architecture,"
IEEE Software , vol. 12, no. 6, pp. 42{50, 1995.
[5] A. Finkelstein, J. Kramer, B. Nuseibeh, L. Finkelstein,
and M. Goedicke, \Viewpoints: A Framework for
Integrating Multiple Perspectives in System
Development," International Journal of Software
Engineering and Knowledge Engineering , vol. 2, no. 1,
pp. 31{57, 1992.
[6] P. Fradet, D. L. M etayer, and M. P erin, \Consistency
Checking for Multiple View Software Architectures,"
inESEC / SIGSOFT FSE , pp. 410{428, 1999.
[7] R. B. France and B. Rumpe, \Model-driven
Development of Complex Software: A Research
Roadmap," in FOSE , pp. 37{54, 2007.
[8] A. Finkelstein, D. M. Gabbay, A. Hunter, J. Kramer,
and B. Nuseibeh, \Inconsistency Handling in
Multi-Perspective Specications," in ESEC , pp. 84{99,
1993.
[9] M. Sabetzadeh, S. Nejati, S. M. Easterbrook, and
M. Chechik, \Global consistency checking of
distributed models with TReMer+," in ICSE ,
pp. 815{818, 2008.
[10] A. Egyed, \Automatically detecting and tracking
inconsistencies in software design models," IEEE
Trans. Software Eng. , vol. 37, no. 2, pp. 188{204, 2011.
[11] M. Vierhauser, P. Gr unbacher, A. Egyed, R. Rabiser,
and W. Heider, \Flexible and scalable consistency
checking on product line variability models," in ASE,
pp. 63{72, 2010.
[12] C. Nentwich, L. Capra, W. Emmerich, and
A. Finkelstein, \xlinkit: a Consistency Checking and
Smart Link Generation Service," ACM Trans. Internet
Techn. , vol. 2, no. 2, pp. 151{185, 2002.
[13] M. Riedl-Ehrenleitner, A. Demuth, and A. Egyed,
\Towards Model-and-Code Consistency Checking," in
COMPSAC , pp. 85{90, 2014.
[14] J. C. Grundy, J. G. Hosking, K. N. Li, N. M. Ali,
J. Huh, and R. L. Li, \Generating Domain-Specic
Visual Language Tools from Abstract Visual
Specications," IEEE Trans. Software Eng. , vol. 39,
no. 4, pp. 487{515, 2013.
[15] A. Demuth, M. Riedl-Ehrenleitner, A. N ohrer,
P. Hehenberger, K. Zeman, and A. Egyed,
\DesignSpace { An Infrastructure for
Multi-User/Multi-Tool Engineering," in SAC,
pp. 1486{1491, 2015.
[16] A. Reder and A. Egyed, \Incremental Consistency
Checking for Complex Design Rules and Larger Model
Changes," in MoDELS , pp. 202{218, 2012.
[17] X. Blanc, A. Mougenot, I. Mounier, and T. Mens,
\Incremental Detection of Model Inconsistencies Based
on Model Operations," in CAiSE , pp. 32{46, 2009.
[18] \Eclipse OCL
http://projects.eclipse.org/projects/modeling.mdt.ocl,2015.."
[19] A. Egyed, \Instant consistency checking for the UML,"
inICSE , pp. 381{390, 2006.
[20] OMG, ISO/IEC 19507 Information technology -
Object Management Group Object Constraint
Language (OCL) . ISO, 2012.
[21] A. Egyed, \Automatically detecting and tracking
inconsistencies in software design models," IEEE
Trans. Software Eng. , vol. 37, no. 2, pp. 188{204, 2011.
[22] C. Xu, S. Cheung, and W. K. Chan, \Incremental
consistency checking for pervasive context," in ICSE ,
pp. 292{301, 2006.
[23] C. Nentwich, W. Emmerich, A. Finkelstein, and
E. Ellmer, \Flexible consistency checking," ACM
Trans. Softw. Eng. Methodol. , vol. 12, no. 1,
pp. 28{63, 2003.
[24] D. Beyer, \Relational programming with CrocoPat,"
inICSE , pp. 807{810, 2006.
[25] \RDF/XML Syntax Specication (Revised)."
[26] M. Koegel and J. Helming, \EMFStore: a model
repository for EMF models," in ICSE , pp. 307{308,
2010.
[27] J. Estublier, T. Leveque, and G. Vega, \Evolution
control in MDE projects: Controlling model and code
co-evolution," in FSEN , pp. 431{438, 2009.
[28] \EPlan Electric P8
http://www.eplanusa.com/us/solutions/product-
overview/eplan-electric-p8/."
[29] \PTC, Inc. http://www.ptc.com/product/creo, 2015."
[30] B. Collins-Sussman, B. W. Fitzpatrick, and C. M.
Pilato, Version control with subversion - next
generation open source version control . O'Reilly, 2004.
[31] D. Steinberg, F. Budinsky, M. Paternostro, and
E. Merks, EMF: Eclipse Modeling Framework 2.0 .
Addison-Wesley Professional, 2nd ed., 2009.
[32] A. Reder and A. Egyed, \Model/analyzer: a tool for
detecting, visualizing and xing design errors in
UML," in ASE (C. Pecheur, J. Andrews, and E. D.
Nitto, eds.), pp. 347{348, ACM, 2010.
[33] A. Finkelstein, D. M. Gabbay, A. Hunter, J. Kramer,
and B. Nuseibeh, \Inconsistency handling in
multperspective specications," IEEE Trans. Software
Eng., vol. 20, no. 8, pp. 569{578, 1994.
[34] M. Sabetzadeh, S. Nejati, S. Liaskos, S. M.
Easterbrook, and M. Chechik, \Consistency checking
of conceptual models via model merging," in RE,
pp. 221{230, IEEE Computer Society, 2007.
[35] F. J. Lucas, F. Molina, and J. A. T. Alvarez, \A
systematic review of UML model consistency
management," Information & Software Technology ,
vol. 51, no. 12, pp. 1631{1645, 2009.
[36] X. Blanc, I. Mounier, A. Mougenot, and T. Mens,
\Detecting model inconsistency through
operation-based model construction," in ICSE ,
pp. 511{520, 2008.
[37] K. Altmanninger, M. Seidl, and M. Wimmer, \A
survey on model versioning approaches," IJWIS ,
vol. 5, no. 3, pp. 271{304, 2009.
[38] G. Taentzer, C. Ermel, P. Langer, and M. Wimmer,
\Conict Detection for Model Versioning Based on
Graph Modications," in ICGT , pp. 171{186, 2010.
600[39] A. Cicchetti, D. D. Ruscio, and A. Pierantonio,
\Managing Model Conicts in Distributed
Development," in MoDELS , pp. 311{325, 2008.[40] J. Loeliger, Version Control with Git - Powerful
techniques for centralized and distributed project
management . O'Reilly, 2009.
601