See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/290867469
Gra y Computing: An Analysis of Computing with Backgrou nd Ja vaScript Tasks
Conf erence Paper  · May 2015
DOI: 10.1109/IC SE.2015.38
CITATIONS
12READS
460
4 author s, including:
Yao P an
Vanderbilt Univ ersity
9 PUBLICA TIONS    321 CITATIONS    
SEE PROFILE
Yu Sun
Calif ornia St ate Polyt echnic Univ ersity
154 PUBLICA TIONS    2,381  CITATIONS    
SEE PROFILE
Jeff Gr ay
Univ ersity of Alab ama
435 PUBLICA TIONS    5,410  CITATIONS    
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Yao P an on 31 A ugust 2016.
The user has r equest ed enhanc ement of the do wnlo aded file.Gray Computing: An Analysis of Computing with
Background JavaScript Tasks
Yao Pan, Jules White
V anderbilt University
USA
{yao.pan, jules.white}@vanderbilt.eduY u Sun
California State Polytechnic University, Pomona
USA
yusun@cpp.eduJeff Gray
University of Alabama
USA
gray@cs.ua.edu
Abstract —Websites routinely distribute small amounts of work
to visitors’ browsers in order to validate forms, render anima-
tions, and perform other computations. This paper examines thefeasibility, cost effectiveness, and approaches for increasing theworkloads ofﬂoaded to web visitors’ browsers in order to turnthem into a large-scale distributed data processing engine, whichwe term gray computing. Past research has looked primarilyat either non-browser based volunteer computing or browser-based volunteer computing where the visitors keep their browsersopen to a single web page for a long period of time. This paperprovides a deep analysis of the architectural, cost effectiveness,user experience, performance, security, and other issues ofgray computing distributed data processing engines with highheterogeneity, non-uniform page view times, and high computingpool volatility.
I. I NTRODUCTION
Every website visitor does some computational work on the
website’s behalf, such as validating input values of a web form
before submission to the server, harvesting information relatedto the client’s browser, animating page elements, or displayingcomplex data analytics. Website visitors implicitly opt-intoperforming computational work for the website owner withoutknowing exactly what work their browsers will perform. Theline between what computational tasks should or should not beofﬂoaded to the visitor’s browser is not clear cut and createsa blurred boundary, which we term gray computing. The only
hard line deﬁning acceptable and unacceptable computingtasks is drawn by the security controls in the browser thatprotect the user from malicious logic.
The successful ofﬂoading of smaller tasks, such as form
validation, motivates the question of whether or not there is po-tential to perform more computational work in this gray com-puting area of website visitors’ browsers. Previously, browser-based JavaScript applications were single-threaded, preventingcomplex calculations from being performed without the user’sknowledge since they would directly impact user interactionwith the web page. The emergence of new standards, such as
Web Workers, which allow background JavaScript threadson web pages, offer the potential for much more signiﬁcantbackground usage of visitor’s computing resources withouttheir knowledge, which motivates the study of this new graycomputing power.
For example, big data processing [1], where organizations
process website visitor logs, user photos, social networkconnections, and other large datasets for meaningful infor-mation has become commonplace for large websites. Couldorganizations ofﬂoad these big data processing tasks to clientweb browsers in background JavaScript threads? How muchcomputational power could be harnessed in this type of model,both for computations directly beneﬁcial to the web visitor,such as product recommendations, as well as for attackers thatcompromise a website?
There are clearly signiﬁcant privacy and ethical questions
around this concept of gray computing, but before deeplyexploring them, it is important to ensure that there is actuallysigniﬁcant potential computational value in gray computing.For example, if the costs, such as added load to the webserveror reduced website responsiveness, reduce web trafﬁc, thenclearly gray computing will not be exploited. Further, if thecomputational power of browsers is insigniﬁcant and can notperform sufﬁcient work to outweigh the outgoing bandwidthcosts of the data transferred to clients for processing, then nouser incentives or other models to entice users into opting intocomputationally demanding tasks will be feasible.
Scientists have effectively applied volunteer computing [2],
which is a distributed computing paradigm in which computerowners donate their computing resources to research projects,to reduce costs and increase their computing power. V olunteercomputing differs from gray computing in that volunteerstypically directly opt-in by installing a client and have ful-l knowledge of the computational tasks that they will beperforming. Code obfuscation techniques, such as JavaScriptminimization, typically make web visitors unaware of thecomputational tasks that they are performing on behalf of awebsite. Moreover, few website visitors look at the sourcecode of webpages that they are visiting in order to determinethe computational work their browsers are performing.
How much gray computing power is currently untapped?
As we will show from empirical results presented in thispaper, there is signiﬁcant untapped gray computing power.For example, over 6 billion hours of video are watched eachmonth on Y ouTube [3]. Assume that each client computerhas an average processing ability of 50 GFLOPS, whichis equivalent to an Intel i3 2100 [4]. Further assume thateach client computer is only 25% utilized by gray big datacomputing tasks ofﬂoaded by Y ouTube, such as the client-side face detection task we present in Section IV-B. The
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.38167
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.38167
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.38167
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.38167
ICSE 2015, Florence, Italycombined processing power of the client computers would be
104 PFLOPS. Tianhe-2, the fastest super computer on recordtill June 2014, has a computing speed of 33.86 PFLOPS [5]. Inthis example, the combined processing power of the monthlyweb visitors to Y ouTube is ≈3X the largest supercomputer
in the world. Even if the true gray computing power is only1/100 of this estimate, the processing power would still makegray computing among the TOP50 fastest super computers inthe world [5].
Open Question ⇒ Is it feasible and cost-effective to
build a gray computing data processing infrastructureusing website visitors’ browsers? Although prior research on
volunteer computing has investigated browser-based volunteercomputing engines, past research has focused on scenarioswhere website visitors keep specialized web pages open forlong periods of time. The prior research has not considered thecost effectiveness of building browser-based distributed dataprocessing engines with gray computing or gray computing’simpact on the user experience of a normal website. If graycomputing can be done in a manner that is both cost-effectiveand does not impact user browsing experience, it warrantsboth signiﬁcant concern and study. A number of key researchchallenges exist toward adopting gray computing that stemfrom the volatility, security, user experience considerations,and cost of performing distributed data processing with graycomputing. This paper does not investigate the ethical andprivacy concerns of gray computing in depth, but instead aimsto determine if gray computing is an appealing enough targetfor both legitimate website operators and attackers to warrantfurther research into these issues.
This paper presents an architectural solution which ad-
dresses these research challenges and proves the feasibilityof performing distributed data processing tasks with graycomputing. There is a need for further study on the importanceof the security, ethical, and other considerations of this type ofcomputing resource utilization. To prove the feasibility of thegray computing concept, we built, empirically benchmarked,and analyzed a variety of browser-based distributed dataprocessing engines for different types of gray computing tasks,ranging from facial recognition to rainbow table generation.We analyzed the computational power that could be harnessedboth for valid user-oriented concerns, as well as by cyber-attackers. The same experiments were performed using Ama-zon’s cloud services for distributed data processing in order tocompare the performance and cost of gray computing to cloudcomputing.
Contributions: Although there are several past implementa-
tions [6] of browser-based distributed data processing engines,past work has not investigated the potential for more signiﬁ-cant usage of website visitor computing resources with back-ground JavaScript threads. This paper’s main contributions,which differentiate it from previous work, are as follows:
•A complete architecture for gray computing is providedthat improves cost-effectiveness by exploiting asymme-tries in cloud pricing models.•Experiments are presented that investigate the impact onuser experience of popular websites, such as Facebook.
•Past research treated the computing resources of browserclients as free. The additional cost/load on the networkand server when distributing data processing tasks tobrowser clients is considered in this work and shown tobe a critical component of determining which tasks arecost-effective to use for gray computing.
•This paper considers more realistic browsing behaviorversus past approaches that assumed the browser wasopen for long periods of time and did not considersituations where the user was only doing computationwhile a visitor was reading web pages on a site.
•A variety of real-world distributed data processing appli-cations, ranging from computer vision to machine learn-ing, are benchmarked and assessed for cost-effectivenesswith gray computing.
The remainder of this paper is organized as follows: Sec-
tion II presents the key research challenges investigated in thispaper; Section III presents our proposed solutions to addressthese challenges and the corresponding empirical veriﬁcations;Section IV examines some practical applications of distributeddata processing with gray computing and analyzes their suit-ability for this computing model; Section V describes relatedwork; Section VI presents concluding remarks and futurework.
II. R
ESEARCH QUESTIONS
In order to determine if it is feasible to tap into gray
computing, a number of key research questions need to beaddressed:
Question 1: Does JavaScript provide sufﬁcient perfor-
mance for computationally intensive tasks?
Question 2: Does gray computing impact website per-
formance in a user perceptible manner?
Question 3: What are the mechanisms for handling
malicious clients and how will they impact gray computingperformance?
Question 4: How cost-effective is gray computing versus
commodity public clouds?
Question 5: How do you effectively allocate tasks to
clients with unknown page view times?
The remainder of this paper presents the results of experi-
ments and analyses that we performed to answer each of thesekey questions. As will be shown, there are practical solutionsto the challenges of gray computing and it can be a cost-effective approach to perform a number of complex tasks, suchas image processing.
III. A
NSWERING THE RESEARCH QUESTIONS
To answer the key research questions, we built an imple-
mentation of a distributed data processing engine that cantap gray computing power, benchmarked it, and performedcost/performance analysis on a variety of different data pro-cessing tasks. We set out to build the most optimized graycomputing engine that we could design to ensure that our
168
168
168
168
ICSE 2015, Florence, Italycost/beneﬁt analyses were realistic. Initially, we used past
research on volunteer computing to guide our gray computingarchitecture [6], [7], [8], but found a number of architecturalassumptions in volunteer computing that did not hold in graycomputing hosted in cloud-based websites. In particular, wefound architectural issues discussed in prior work, such asmismatches between past architectures and cloud computingpricing, that we sought to address.
Our design focused on optimizing the gray computing
distributed processing engine architecture for websites servedout of cloud computing environments, such as Amazon EC2.Based on our analysis of the architectures used in past re-search [6], [7], [9], the key architectural limitations of pastresearch when applied to a cloud computing environment areas follows:
•Prior approaches assume a ﬁxed sunk cost for thecomputing time of the task distribution server. In a
cloud computing environment, the cost of the computingtime for the task distribution server is not ﬁxed andscales with load. Therefore, past architectures need to bereassessed to minimize the distribution server load anddecrease cost. Otherwise, gray computing is less cost-effective.
•Pricing asymmetry of cloud computing resources canbe exploited to reduce gray computing costs. For
example, Amazon S3 and Microsoft Azure only chargefor data transfer out of their storage services and notdata transfers into their storage services. Prior work didnot optimize the data distribution and results reportingarchitectures to take advantage of pricing asymmetry.
•Task distribution servers were reported as the bot-tleneck in some prior work [9]. The load on task
distribution servers can be substantially reduced by of-ﬂoading data distribution to content delivery networks,which provide better cost/performance ratios than servingthe same data out of a cloud-based server.
•All results were reported directly back to the taskdistribution server in prior work. Server load can
be reduced by allowing browser-based clients to bypassthe task distribution server and directly write results tocloud-based storage using a temporary URL authorizationmodel, such as the one provided by Amazon S3.
To address the architectural issues described above, we de-
veloped a novel architecture for browser-based data processingengines hosted out of cloud computing environments, suchas Amazon EC2. Our architecture was focused on providingwebsites with a MapReduce interface to gray computing,which is commonly used for data processing tasks [1].
To reduce the workload of the task distribution server,
we utilize the cloud provider’s storage service to serve datafor computing directly out of the storage service. The taskdistribution server is only responsible for determining whattasks should be given to a client. The task distribution serverhandles HTTP GET requests and responds with a JSON-basedstring containing the task ID and data location URL for eachtask. Each client calls the API once before working on anindividual task. Our empirical analysis shows a relatively smallworkload is added to the task distribution server and the costis negligible.
Besides reducing the workload of the task distribution
server, serving data directly out of the cloud-based storageserver also exploits the pricing asymmetry in cloud storageservices. In Amazon S3 and Microsoft Azure, only outbounddata transfer is charged. Data transferred into the storageservice is free. This means clients can report the results ofcomputations directly to the storage service for free. This setupallows the data processing engine to reduce bandwidth costs.
Since the clients are highly volatile and typically only
have short page view times, improving data transfer speedsis critical to the overall performance. It is essential to max-imize the time that clients spend executing computationaltasks and minimize the time spent waiting for input data.One optimization that has been applied in our architectureis the use of a Content Delivery Network (CDN). Insteadof serving the input data through a single storage server, aCDN works by serving the content with a large distributedsystem of servers deployed in multiple data centers in multiplegeographic locations. Requests for content are directed to thenodes that are closest and have the lowest latency connectionto the client. Take Amazon’s CDN service CloudFront as anexample. The original version of the data is stored in an originserver, such as S3. Amazon copies the data and producesa CDN domain name for the data. The clients request thedata using the CloudFront domain name and CloudFront willdetermine the best edge location to serve the contents. Anoverview of this proposed architecture is shown in Figure 1.
In order to manage and coordinate the computational tasks,
we built a cloud-based task distribution server using Node.jsand the Express framework. The task distribution server par-titions the data into small chunks and assigns them to clientsfor processing. We use Amazon S3 as our storage server tostore input data for clients and receive results from clients.An EC2 server is used to subdivide the tasks and maintaina task queue to distribute tasks to clients. In our proposedarchitecture, tasks are distributed to clients as follows:
1) The client requests an HTML webpage from the server.2) The server injects an additional JavaScript ﬁle into the
webpage that includes the data processing task.
3) A JavaScript Web Worker, which executes in a back-
ground processing thread after the page is fully loaded, isused to perform the heavy data processing computationwithout impacting the foreground JavaScript rendering.
4) The client sends AJAX HTTP requests to retrieve the
input data from the CDN. Once it receives the inputdata, it runs a map or/and reduce function on the data.
5) The client issues an HTTP PUT or POST of the re-
sults directly back to the cloud storage server. Aftersubmitting the results, the Web Worker messages themain thread to indicate completion. Upon receipt of thismessage, the main thread sends a new HTTP request tofetch another data processing task from the server.
169
169
169
169
ICSE 2015, Florence, Italy/g94/g410/g381/g396/g258/g336/g286/g3/g94/g286/g396/g448/g286/g396/g3
/g894/g4/g373/g258/g460/g381/g374/g3/g94/g1007/g895 /g3
/g18/g381/g381/g396/g282/g349/g374/g258/g410/g349/g374/g336/g3/g94/g286/g396/g448/g286/g396/g3
/g894/g4/g373/g258/g460/g381/g374/g3/g28/g18/g1006/g895 /g3/g18/g367/g349/g286/g374/g410/g855/g3
/g116/g286/g271/g3/g271/g396/g381/g449/g400/g286/g396 /g3
/g116/g286/g271/g3
/g449/g381/g396/g364/g286/g396/g3
/g24/g349/g400/g393/g367/g258/g455/g3/g272/g381/g374/g410/g286/g374/g410/g3
/g258/g374/g282/g3/g346/g258/g374/g282/g367/g286/g3
/g437/g400/g286/g396/g3/g349/g374/g410/g286/g396/g258/g272/g410/g349/g381/g374/g3
/g100/g258/g400/g364/g3
/g87/g396/g381/g272/g286/g400/g400/g349/g374/g336/g3
/g90/g286/g410/g437/g396/g374/g3/g90/g286/g400/g437/g367/g410/g400/g3
/g271/g455/g3/g44/g100/g100/g87/g3/g87/g104/g100/g3
/g100/g258/g400/g364/g3
/g38/g349/g374/g349/g400/g346/g286/g282/g3
/g39/g286/g374/g286/g396/g258/g410/g286/g3
/g410/g286/g373/g393/g381/g396/g258/g396/g455/g3
/g104/g90/g62/g3/g296/g381/g396/g3
/g349/g374/g393/g437/g410/g3/g282/g258/g410/g258/g856/g3
/g116/g286/g271/g3
/g100/g258/g400/g364
 /g367/g258/g455/g272
/g100/g258/g400/g364/g346/g410/g373/g367/g3/g58/g381/g271/g3/g89/g437/g286/g437/g286/g3
/g361/g381/g271/g1005/g3
/g361/g381/g271/g1005/g3
/g361
/g361/g381/g271/g1006/g3
/g361/g381/g271/g1006/g3
/g361
/g361/g381/g271/g1007/g3
/g361/g381/g271/g1007/g3
/g361
/g361/g381/g271/g1008/g3
/g361/g381/g271/g1008/g3
/g361
/g857/g3
/g18/g24/g69/g3/g94/g286/g396/g448/g286/g396/g3/g3
/g894/g4/g373/g258/g460/g381/g374/g3/g18/g367/g381/g437/g282/g38/g396/g381/g374/g410/g895 /g3
/g90/g286/g395/g437/g286/g400/g410/g3/g410/g258/g400/g364/g3
/g24/g349/g400/g410/g396/g349/g271/g437/g410/g286/g3/g410/g258/g400/g364/g3/g258/g374/g282/g3/g349/g410/g400/g3/g349/g374/g393/g437/g410/g3
/g282/g258/g410/g258/g3/g104/g90/g62/g3
/g87
/g90/g286/g410/g437/g396/g374/g90/g286/g400/g437/g367/g410/g400
Fig. 1: Architecture of the proposed distributed data processing engine for
gray computing.
A. Answering Question 1 ⇒Benchmarking JavaScript Per-
formance on Computationally Intensive Tasks
1) Current JavaScript performance: Popular browsers such
as Chrome, Firefox, and Safari have made great efforts tooptimize their JavaScript computing engines and have pro-duced signiﬁcant increases in JavaScript computational speed.Further, a variety of highly optimized tools have been devel-oped to automatically port C/C++ programs to JavaScript. Em-scripten [10] is an LLVM (Low Level Virtual Machine)-basedproject that compiles C/C++ code into highly-optimizableJavaScript in the asm.js format. Entire C/C++ code bases, suchas the Unity 3D gaming engine and the ffmpeg video process-ing library have been ported to JavaScript with Emscripten.Emscripten works by ﬁrst compiling C/C++ code throughClang [11] into an intermediary representation in LLVM. It isclaimed the optimized JavaScript code can achieve near nativeperformance to C++ code. We conducted some benchmarkexperiments to analyze the performance of the JavaScriptproduced by Emscripten. The results are quite startling: theported JavaScript codes signiﬁcantly outperformed the hand-written JavaScript code, and were within 2X performance ofthe original native C++ code.
TABLE I: E XECUTION TIME COMPARISON OF NA TIVE C++, NA TIVE
JS, AND PORTED JS ( FROM C++). J AVA SCRIPT IS RUNNING WITH V8
JAVA SCRIPT ENGINE 3.27.
Benchmark C++ native JS ported JS
Nbody 6.6s 16.9s 6.2s
fasta 1.3s 11.4s 2.1s
Table I presents the results from the comparison be-
tween native C++ code, hand-written JavaScript code, andported JavaScript code using Emscripten. The hand-writtenJavaScript code is written with the same algorithm as theC++ version. The ported JavaScript code is produced usingEmscripten 1.2.0 with the “-O3” optimization ﬂag. The C++code was compiled using LLVM gcc 5.0 with the “-O3”optimization parameter. The reason for the performance boostis that Emscripten uses asm.js, which is a highly optimizedlow-level subset of JavaScript. The types in JavaScript areimplicitly declared and thus take time for large applicationsto decide at runtime. Asm.js utilizes typed arrays for speedimprovements. It also eliminates JavaScript’s object-orientedconstructs and thus eliminates many hard-to-optimize codeconstructs.
B. Answering Question 2 ⇒Benchmarking Background Web
Worker Computational Task Impact on User Experience
One concern with deploying gray computing to websites
is whether the background computing tasks will affect the
website’s foreground user interaction tasks. Gray computingwill be of little interest to website owners and visitors if thecomputing tasks affect the user experience heavily.
Web Workers is a new feature introduced in HTML5, which
can create a separate JavaScript thread in the background.Using Web Workers, background computing tasks and fore-ground rendering / user interaction tasks can run in separatethreads, allowing the main JavaScript needed to render thepage and validate forms to run unimpeded without affectinguser experience.
To test whether doing computationally intensive tasks in
the background will affect the user experience, we conductedexperiments with Greasemonkey [12] to inject data processingtasks into popular websites. Greasemonkey is a browser exten-sion which allows users to install scripts that make on-the-ﬂychanges to web page content before or after the web page isloaded in the browser.
The ﬁrst concern regarding user experience is page load
time. However, Web Workers spawn threads in the onload()
function of a web page, which is executed only after theHTML page ﬁnishes loading. Background Web Worker threadscan be run without affecting page load time.
The second concern is the page responsiveness. Many met-
rics exist for page responsiveness. In this project, we choosethe search box item suggestion (or auto-completion), whichis a pervasive and time-critical feature of many websites, forthe user experience evaluation. Search box auto-completionis a time critical task since auto-completion results must becomputed faster than the user types in order to be helpful. Ouraim was to test if the generation of search suggestions wouldbe slowed down by a computationally intensive backgroundtask. Two scripts were written in Greasemonkey. Script 1 wassetup to inject a search term into the search bar and record howlong it took for the web page’s foreground client JavaScriptto generate the search result HTML elements and producea baseline time for comparison. Script 2 spawned a WebWorker to perform a computationally intensive task requiredby the applications we proposed in Section IV, such as facerecognition and image scaling, and then injected the searchterms. The time to generate auto-completion suggestions whilerunning the computationally intensive background tasks wasmeasured. We compared the time to generate the searchsuggestions with and without the Web Worker task to see ifthe background computation would impact the user experienceon a number of popular websites.
We used 50 different keywords as search inputs and for
each keyword we ran 100 trials and averaged the load times.As the results shown in Table II illustrate, there was nodiscernible difference in the search suggestion load time with
170
170
170
170
ICSE 2015, Florence, Italyand without a Web Worker computational task running. The
results show that background gray computing tasks would notbe easily discernible by the user – causing both alarm dueto the potential for misuse and potential for distributed dataprocessing.
TABLE II: A VERAGE SEARCH BOX HINT RESULTS GENERA TION TIME (T)
AND CPU UTILIZA TION .
Website With Web Worker Without Web Worker
T Avg. CPU util T Avg. CPU util
Twitter 0.58s 56% 0.59s 12%
Wikipedia 0.47s 58% 0.46s 15%
Gmail 0.75s 55% 0.73s 11%
C. Answering Question 3 ⇒Mechanisms for Handling Mali-
cious Clients and their Associated Overhead
Since clients in a gray computing distributed data processing
engine are not guaranteed to be trustworthy, it is possible thatthe results returned from them can be fraudulent. A simplebut effective approach to handle malicious clients it to use atask duplication and majority voting scheme. More complexstrategies, such as credibility-based approaches, exist but arenot necessarily a good ﬁt for the highly-volatile browser-based data processing environment. Many visitors may accessa website only once and the website may have no historystatistics to derive their visitor reputation scores. Suppose theoverall application consists of ndivisible task units. There
areCconcurrent clients and among them, Mare malicious
clients, which will send falsiﬁed results. f=C/N is the
fraction of malicious clients in total clients. Our duplicationapproach works by assigning identical task units to kdifferent
clients and verifying the results returned from different clients.If the computation results from different clients are the same,then the result will be accepted. If not, a majority votingstrategy is applied and the result from the majority of clientsis accepted. Notice that the server randomly distributes tasksto clients, while ensuring no client receives both a task andits duplicate and that clients have no control over which taskunits they will get. The duplication approach fails only whena majority of a task’s duplications are sent to a collaborativegroup of malicious clients.
Error Rate: The error rate /epsilon1is deﬁned as the fraction of
the ﬁnal accepted results that are malicious. As described bySarmenta et al. [13], the error rate can be given by:
/epsilon1=
2d−1/summationdisplay
j=d/parenleftbigg2d−1
j/parenrightbigg
fj(1−f)(2d−1−j)(1)
wheredis the minimum number of matching results needed
for the majority voting scheme. For example, for d=2 , the
server accepts a result when it gets two identical results. Whenthe server gets two different results, it keeps reassigning thetask until 2 identical results are received. Redundancy, R,i s
deﬁned as the ratio of the number of assigned task units to thenumber of total task units. It can be proved that R=d/(1−f).
Although simple, the duplication approach is robust even
with small d. For large websites with millions of visitors, it is
difﬁcult for attackers to gain control over a large portion of theclients. Suppose a hacker has a botnet of a thousand machines,f=0.001 for a website with a million total clients. As shown
in Figure 2, even with d=2 , we can ensure 99.9999% of
all the accepted results are correct. The choice of dis also
dependent on the type of application. Some applications aregenerally less sensitive and even no duplication is acceptablesince the falsiﬁed results do little harm.
/g1005/g856/g1004/g28/g882/g1004/g1012/g1005/g856/g1004/g28/g882/g1004/g1011/g1005/g856/g1004/g28/g882/g1004/g1010/g1005/g856/g1004/g28/g882/g1004/g1009/g1005/g856/g1004/g28/g882/g1004/g1008/g1005/g856/g1004/g28/g882/g1004/g1007/g1005/g856/g1004/g28/g882/g1004/g1006/g1005/g856/g1004/g28/g882/g1004/g1005/g1005/g856/g1004/g28/g1085/g1004/g1004
/g1005/g1006/g1007/g28/g396/g396/g381/g396/g3/g396/g258/g410/g286/g3
/g282/g3/g296/g1089/g1004/g856/g1004/g1005
/g296/g1089/g1004/g856/g1004/g1004/g1005
Fig. 2: Error rates for different parameter values of d and f.
D. Answering Question 4 ⇒A Cost Model for Gray Comput-
ing
Another challenge that must be dealt with is the cost-
effectiveness of gray computing. To assign tasks to a large
number of browser clients, a coordinating server is neededto send the input data to clients and collect the computingresults from them. The extra data transferred, in addition tothe original web page, will consume additional bandwidth andprocessing power of the server. It is possible that this extracost can be more than the value of the results computed bythe clients for some types of applications. Therefore, we needa cost model to help developers decide whether it is cost-effective to deploy an application through gray computingand estimate the cost savings. In our cost model, we choosecloud computing services as our benchmark. We chose tocompare against cloud computing because cloud computinghas a quantiﬁable computing cost that we can compare against.
For our gray computing distributed data processing engine,
the cost can be broken down as:
C
browser =Ctransfer +Crequest +Cdistribute
=Ctoclient +Ctoserver +Crequest +Cdistribute
≈Ctoclient +Ctoserver +Crequest (2)
Ctransfer is the data transfer cost calculated on volume of
data;Crequest is the HTTP request cost incurred based on the
number of GET or POST requests; Cdistribute is the virtual
compute units consumed by task distribution. We have shownin our empirical analysis that C
distribute is nearly zero for our
architecture because the data is served directly out of the cloudstorage service and the task distribution server only needs todo a small amount of work.
C
transfer can be further broken down into Ctoclient and
Ctoserver .
171
171
171
171
ICSE 2015, Florence, ItalyCtoclient =k×I×Ptransferout
=k×I×(POriginT oCDN +PCDNout )
Ctoserver =O×Ptransferin
=O×(PtoCDN +PtoOrigin ) (3)
Ctoclient is the cost to fetch input data for computing from
an S3 storage server; Iis the original size of input data to
be transferred to clients for processing; POriginT oCDN is the
unit price to transfer data from an origin server (S3 in our
case) to CDN servers (CloudFront in our case); PCDNout is
the unit price to transfer data from CDN servers to the clients.C
toserver is the cost to return computation results from clients
to the storage server; Ois the size of data to be returned;
PtoCDN is the unit price to transfer data from clients to CDN
servers and PtoOrigin is the unit price to transfer data from
CDN servers to the origin server.
The actual data transferred from server to client will be
more than the original size of input data I. One reason is as
described in Section III-C, the same task needs to be sent toddifferent clients to be robust to malicious clients. Another
reason is the clients may leave before the computation ﬁnishes.The data transferred to these clients is wasted. The actual vol-ume of data transferred out will be ktimes the data needed for
one copy of the task, where k=d+/summationtext
∞
n=1d(1−μ)n=d/μ.
dis the duplication factor. The variable μis the successful
task completion ratio of browser clients (i.e., the percentage
of distributed tasks that are successfully processed before abrowser client leaves the site).
In Section III-E, we discuss the estimation of value μand its
relationship with average page view duration, task granularity,and task distribution algorithms.
C
request =(k+d)×n×Prequest (4)
nis the number of tasks that need to be processed. Prequest is
the unit price of HTTP requests to the CDN server. For eachtask distribution session, the client needs one HTTP requestto fetch the data and one HTTP request to return the results.Since each task needs to be duplicated dtimes and not all
the tasks are completed successfully, more fetch requests areneeded than return requests. That is, knrequests to fetch input
data and dnrequests to return the results.
The cost to run the tasks in the cloud is given by C
cloud :
Ccloud =Tcloud×I×Punit (5)
wherePunit is the cost per hour of a virtual compute unit.
For example, Amazon provides an “ECU” virtual computingunit measure that is used to rate the processing power of eachvirtual machine type in Amazon EC2. Virtual compute unitsare an abstraction of computing ability. T
cloud is the computing
time to process 1 unit of data with one virtual compute unitin the cloud.
The proposed distributed data processing engine built on
gray computing is only cost effective when:
C
cloud>C browser (6)That is, the cost to distribute and process the data in thebrowsers must be cheaper than the cost to process the datain the cloud.
T
cloud×I×Punit>k×I×(POriginT oCDN +PCDNout )
+O×(PtoCDN +PtoOrigin )
+(k+d)×n×Prequest
(7)
Since prices are all constant for a given cloud service provider,the key parameters here are T
cloud , which can be computed
by a benchmarking process, and k, which can be computed
based on the voting scheme for accepting results and averagepage view time of clients.
The cost saving Uis deﬁned as:
U=C
cloud−Cbrowser (8)
A positive Uindicate an application is suitable for gray
computing.
E. Answering Question 5 ⇒An Adaptive Scheduling Algo-
rithm for Gray Computing
The clients in gray computing are website visitors’ browser-
s, which are not static and unreliable. The clients join and
leave the available computational resource pool frequently.The result is that a client may leave before its assignedcomputational task is ﬁnished, adding extra data transferand producing no computational value. We deﬁne μas the
successful task completion ratio. The value of μis important
and directly inﬂuences the cost of gray computing. The higherμis, the less data transferred is wasted, and thus more cost-
effective gray computing will be.
There are two factors affecting the successful task com-
pletion ratio μ: the page view duration of the client and the
computing time of the assigned task. The relationship betweenμ, average page view duration and task sizes is depicted in
Figure 3. Assume assigned task size is proportional to thecomputing time needed. For a ﬁxed task chunk size, the longerthe page view duration, the fewer chances the client will leavebefore the computation completed. The distribution of the pageview durations of a website’s clients is determined by thewebsite’s own characteristic. But the computing time of theassigned tasks is what we can change. Assume the wholeapplication can be divided into smaller chunks of arbitrarysize. Reducing the single task size assigned to the clients willincrease the task completion ratio, but result in more task units.More task units means more cost on the requests to fetch dataand return results. Therefore, there is a tradeoff between usingsmaller tasks and larger tasks.
Instead of treating all the clients as the same and assigning
them tasks of the same size, we developed an adaptivescheduling algorithm. We utilize the fact that website visitors’dwell time follows a Weibull distribution [14], [15]: the prob-ability a visitor leaves a web page decreases as time passes.This implies most visits are short. Our adaptive schedulingalgorithm works by assigning tasks of smaller size to clientsﬁrst and increasing the subsequent task sizes until a threshold
172
172
172
172
ICSE 2015, Florence, Italy40 80 120 160 200 2400.650.70.750.80.850.90.951Successful task completion ratio
Average dwell time (in second)  
Task size=5
Task size=10
Task size=15
Fig. 3: The relationship between average dwell time and successful task
completion ratio μfor different task sizes.
is reached. In this way, we achieve higher successful timecompletion ratio than ﬁxed task size while also reducing thenumber of task requests.
For comparison, we also implemented a Hadoop scheduler.
The Hadoop scheduler works as follows: The entire jobis partitioned into task units of the same size. The servermaintains a queue of tasks to assign. When a client connectsto the server, the server randomly selects a task from the queueand assigns it to the client. If a task fails, the failed task will bepushed onto the back of the queue and wait to be reassignedto other clients.
To derive μvalues for the schedulers, we ran simulations
in Matlab with varying average page view times and taskdurations. We assumed the page view duration follows aWeibull distribution with average page view duration equaltoT
apv. We used a duplication factor of d=2 for a voting
scheme that required 2 matching results. The results are shownin Table III. We can see that 1) μincreases as the average
page view time increases (i.e., the successful task completionrate goes up), and 2) The adaptive scheduler achieves higherμand faster task completion times compared to the Hadoop
scheduler.
TABLE III: C OMPARISON OF DIFFERENT SCHEDULERS IN TERMS OF TASK
COMPLETION SUCCESS RA TIO μ.
Average page view duration μ
Scheduler 30s 1min 2min 3min
Hadoop 0.80 0.85 0.88 0.90
Adaptive 0.84 0.88 0.92 0.93
IV . E XAMPLES OF GRA Y COMPUTING APPLICA TIONS
In this evaluation section, we examine some practical ap-
plications of distributed data processing with gray computing.We compute the cost saving Ufor each application to ﬁnd
out which applications are cost-effective in a gray computingmodel.
A. Experiment Environment
Browser client : We used a desktop computer for our exper-
iments with an Intel Core i5-3570 CPU clocked at 3.5GHz and
8GB of memory. The computational tasks were implementedin JavaScript and run on Chrome 39.0.Cloud instance: We use an Amazon EC2 m3.medium
instance (ECU=3) and Ubuntu 14.04 64bit operating systemfor benchmarking cloud data processing performance.
Price for cloud computing: See Table IV and V. P
unit=
$0.07/hour . We choose a duplication factor d=2 for most
use cases, because it guarantees a relatively high accuracy aswe have shown in Section III-C. We also choose a conservativesuccessful task completion ratio of μ=0.8, which is repre-
sentative of the Hadoop scheduler efﬁciency with an averagepage view time of roughly 30s.
We compute the cost saving: U=T
cloud×I×Punit−
Ctoclient−Ctoserver−Crequest for each task as shown in Ta-
ble VI. A data processing task is cost-effective for distributedprocessing with browsers if the Uis a positive number.
TABLE IV: A SUBSET OF AMAZON CLOUD SERVICE TYPE AND PRICE
(ASO F FEBRUARY 2015)
Service Subtype Price ECU
EC2 t2.micro $0.013 /Hr varied
m3.medium $0.07 /Hr 3
m3.large $0.14 /Hr 6.5
c3.large $0.105 /Hr 7
TABLE V: A SUBSET OF AMAZON CLOUD SERVICE TYPE AND PRICE
(ASO F FEBRUARY 2015)
Service Subtype Price
S3 Transfer IN To S3 Free
S3 to CloudFront Free
CloudFront To Origin $0.02/GB
To Internet (First 10TB) $0.085/GB
To Internet (Next 40TB) $0.08/GB
To Internet (Next 100TB) $0.06/GB
To Internet (Next 350TB) $0.04/GB
HTTP requests $0.0075/per 10000
B. Use Cases
Scenario 1: Face detection
Overview. For the ﬁrst gray computing case study, we chose
face detection, which is a data processing task that Facebook
runs at scale on all of its photo uploads to facilitate usertagging in photos. Face detection is a task that most Facebookusers would probably consider to be beneﬁcial to them. Facedetection also has an interesting characteristic in that it canbe run on the data that is already being sent to clients as partof a web page (e.g. , the photos being viewed on Facebook).
Notice in our cost model, a large portion of the cost comesfrom sending input data from the server to clients. This costbecomes prohibitively expensive for data intensive tasks. Datatransfer costs appear inevitable, since it is impossible for the
TABLE VI: C OMPARISON OF COST SA VING PER GB FOR DIFFERENT TASKS .
THE HIGHER THIS V ALUE ,THE MORE COMPUTA TIONALLY INTENSIVE THE
TASK ,AND THE MORE SUITABLE THE TASK FOR DISTRIBUTED DA TA PRO -
CESSING WITH BROWSERS .
Task Cost savings $ per GB
Rainbow table ∞
Face detection 0.09
Sentiment analysis 0.07
Image scaling 0.008
Word count -0.16
173
173
173
173
ICSE 2015, Florence, Italyclients to compute without input data. However, there are
some circumstances that the input data needs to be sent tothe client anyway, such as when the data is an integral partof the web page being served to the client. In these cases, noextra data transfer costs are incurred. Facebook has 350 million(as of 2013) photos uploaded every day and face detectionin photos is a relatively computationally intensive computervision task. Our hypothesis was that signiﬁcant cost savingscould be obtained through gray computing.
Experiment Setup. To test whether ofﬂoading face detec-
tion tasks to gray computing is cost-effective, we conductedexperiments for face detection tasks on both JavaScript in thebrowser and OpenCV , which is a highly optimized C/C++computer vision library, in the cloud. For the JavaScript facialdetection algorithm, we used an opensource implementation ofthe Viola-Jones algorithm [16]. For the Amazon cloud com-puting implementation, we use the same algorithm but a highperformance C implementation from OpenCV 2.4.8. There aremore advanced face detection algorithms that achieve betteraccuracy but need more computing time, which would favorbrowser-based data processing.
Empirical Results. The results of the experiment are shown
in Table VII. We can see that the computation time is approxi-mately linear to the total number of pixels or image size. Thisresult is expected because Viola-Jones face detection uses amulti-scale detector to go over the entire image, which makesthe search space proportional to the image dimensions.
Analysis of Results. Facebook has 350 million photos
uploaded by users each day. Suppose the average resolutionof the photos being uploaded is 2 million pixels, which isless than the average resolution of most smartphone cameras,such as the 8 million pixel (8 megapixel) iPhone 6 camera.It takes 1.7s×3.5×10
8/3600h = 165, 278h of computing
time for an EC2 m3.medium instance to process these photos.With our utility function U=T
cloud×I×Punit−1/μ×
d×I×Ptransferout −O×Ptransferin −Crequest , where
Ptransferout =0 because the photos already exist in clients so
there is no additional cost for transferring data to the clients.For each photo, the returned computing result should be anarray of coordinations of rectangles, which is rather small,soOcan be ignored. The client does not need to fetch the
photos only needs to issue one HTTP request to return theresults. We assume the client processes 5 photos at a time(which takes around 7s on a browser with average hardware).We choose a duplication factor d=1 since this application is
not very sensitive to security. Thus, C
request = 350×108/5×
0.0075/104= 5250, and U= 165278 ×0.07−5250 = $6319
could be saved each day by distributing this task to browserclients rather than running it in Amazon EC2. That is ayearly cost savings of roughly $2.3 million dollars. The actualalgorithm Facebook uses is likely to be more complex than thealgorithm we used and a larger T
cloud is expected. Therefore,
the cost savings could be even larger than we calculated.
Scenario 2: Image ScalingOverview. Another example of a useful gray computation
that can be applied to data already being delivered to clients
is image scaling. Websites often scale image resources, suchas product images, to a variety of sizes in order to adapt themto various devices. For instance, desktop browsers may loadthe image at its original size and quality, while mobile clientswith smaller screens may load the compressed image withlower quality. The Amazon Kindle Browser talks directly toEC2 rather than target websites and receives cached mobile-optimized versions of the site and images that are producedfrom large-scale image processing jobs.
We can ofﬂoad image compression tasks to gray computing.
Similar to Scenario 1, the photos are already being deliveredto the clients, so there is no added cost for transferring datafrom the server to clients. After loading the images, each clientwill do the scaling and compression work and then send thescaled images for mobile devices back to the server. Whenthe number of images to process becomes huge, the savedcomputation cost could be substantial.
Experiment Setup. There are many image scaling algo-
rithms and they achieve different compression qualities withdifferent time consumptions. We wanted to measure JavaScrip-t’s computation speed on this task in the real-world and did notwant to focus on the differences in efﬁciency of the algorithms.We chose bicubic interpolation for image scaling on both thebrowser and server. Bicubic interpolation is implemented inJavaScript and C++ for both cloud and browser platforms.
Empirical Results. The experiment results in terms of
computation speed are shown in Table VIII.
Analysis of Results. To get a quantitive understanding
of the cost savings by ofﬂoading the image scaling tasksto gray computing, we again use Facebook as an exampleand make several assumptions: suppose the average resolutionof the photos being uploaded is 2 million pixels and theaverage scaling ratio is 50%. 350 million uploaded photosdaily take 0.55s×3.5×10
8/3600h =5 3,472h for one EC2
m3.medium instance to scale and compress. With our utilityfunctionU=T
cloud×I×Punit−k×I×Ptransferout −
O×Ptransferin −Crequest . AgainPtransferout =0 because
the photos already exist in clients so there is no additionalcost for transferring data to the clients. We assume the clientprocesses 10 photos at a time (which takes around 10s ona browser with average hardware). We choose a duplicationfactord=1 since this application is not very sensitive
to security. O=3.5×10
8×80k/ 106=2.8×104GB .
Crequest = 350×108/10×0.0075/104= 2625. Therefore,
U= 53472 ×0.07−2.8×104×0.02−2625 = $558 is
saved each day by distributing this task to browsers, whichaggregates to $203,670 dollars a year.
There are many techniques that can be used to further
improve the quality of the resized images, such as sharpening,ﬁltering, etc. These techniques require additional computationtime (larger T
cloud ), so the amount of money saved could be
further increased if these techniques are applied to uploadedimages.
174
174
174
174
ICSE 2015, Florence, ItalyTABLE VII: C OMPUTING TIME COMPARISON FOR FACE DETECTION TASKS .
Image Dimension Number of pixels Size JS Computing Time(s) EC2 Computing Time(s)
960*720 0.69 million 82KB 0.67 0.56
1000*1500 1.5 million 156KB 1.05 1.30
1960*1300 2.55 million 277KB 1.70 2.15
TABLE VIII: C OMPUTING TIME COMPARISON FOR IMAGE RESIZE TASKS .
Image Dimension SizeJavaScript Computing Time(s) EC2 Computing time(s)
Scaling ratio 30% 50% 70% 30% 50% 70%
960*720 82KB 0.14 0.35 0.66 0.18 0.22 0.25
1000*1500 156KB 0.29 0.69 1.34 0.43 0.45 0.58
1960*1300 277KB 0.48 1.2 2.08 0.66 0.73 0.86
Scenario 3: Sentiment Analysis
Overview. Sentiment analysis [17] refers to using tech-
niques from natural language processing and text analysis to
identify the attitude of a writer with respect to some sourcematerials. For instance, Amazon allows customers to reviewthe products they purchased. It would be useful to automat-ically identify the positive/negative comments and rank theproducts based on the customers’ attitude. Since the commentcontext is sent to the websites’ visitors anyway, there will beno extra cost incurred due to the data transferred from serversto clients.
Experiment Setup. There are many machine learning algo-
rithms proposed for sentiment analysis in the literature [17].We implemented a Naive Bayes Classiﬁer, a simple but quiteeffective approach, for sentiment analysis. The classiﬁer takesas input a user review and predicts whether the review is posi-tive or negative. The classiﬁer is implemented with JavaScriptfor the browsers and Python for EC2. We trained our classiﬁerwith a movie review dataset [18] containing 1000 positive and1000 negative reviews. We collected movie reviews from theInternet as test data and partitioned them into 10 ﬁles each ofsize 200KB. Then we used the trained classiﬁer to predict theattitude of each review item and record the time needed.
Empirical Results. For an input size of 200KB, the predic-
tion time of browsers with JavaScript is 0.3s and for the sametask running on an EC2 m3.medium instance, the predictiontime is 0.7s.
Analysis of Results. For 1GB input data, the cost savings
is1000/0. 2×0.7/3600×0.07 = $0. 07. To get a quantitive
understanding of how much money can be saved by ofﬂoadingthe sentiment analysis tasks to gray computing, we use Face-book as an example and make several assumptions: supposeeach user uploaded photo has an average of 1Kb comments.Since Facebook has 350 million photos uploaded daily, thereis350×10
6×1/106= 350GB comments in total. If Facebook
wants to analyze the attitude of every comment of the photos,the cost is 350×0.07×365 = $8943 a year.
Scenario 4: Word count
Overview. Word counting is the classic use case that is
used to demonstrate big data processing with MapReduce.Word counting requires determining the total occurrence ofeach word in a group of web pages. It is a task that requiresa relatively large amount of textual input with a very smallamount of computational work.Experiment Setup. We compared the cost-effectiveness of
running a word count task in JavaScript versus the AmazonElastic Map Reduce (EMR) service (using a script written inPython). The Amazon EMR experiment was conﬁgured with1 master node (m1.medium) and 2 slave nodes (m1.medium).
TABLE IX: C OMPUTING TIME COMPARISON FOR WORDCOUNT .
Input size Browser (JavaScript) Amazon EMR
18MB 13.7s 94s
380MB 3min 6min
Empirical Results. The experiment results in terms of
computing speed are shown in Table IX. For 1GB of input,C
request is very small, the cost savings is Ccloud−Ctoclient =
1/0.38×6/60×0.109×2−0.085×2.5=$−0.16.
Analysis of Results. As can be seen, the cost saving is
a negative number which means the value of the computedresults is less than the cost of transferring the text data tothe clients. Word counting is not an ideal application fordistributed data processing with gray computing because itis a data intensive but computationally simple task. However,if the word count was being run on web pages delivered tothe clients, the data transfer cost would not be incurred and itwould be as cost effective as Scenarios 1 and 2.
Scenario 5: Rainbow Table Generation
Overview. The ﬁnal use case was designed to analyse the
gray computing power that could be wielded by an attacker
if they compromised a large number of websites and begandistributing malicious data processing tasks in their web pages.For the malicious use case, we focused on password crackingwith rainbow tables, which has become a common typeof work performed by cyber-attackers on data stolen fromwebsites. A rainbow table [19] is a precomputed table forreversing cryptographic hash functions, usually for crackingpassword hashes. These tables are used to recover the plaintextpasswords that are stored as hashes in databases.
To distribute the tasks, the central server only needs to send
a start string and end string of a range of the password to theclients. The size of input is extremely small and can almostbe ignored. Therefore, C
toclient =0 .
Experiment Setup. The browser environment is the same
as described in Section IV-A. For the cloud instance, weimplemented the rainbow table generation algorithm in C++and compiled with g++ 4.8.2.
175
175
175
175
ICSE 2015, Florence, ItalyEmpirical Results. The results of the experiment are shown
in Table X. Generating 900,000 hashes on a desktop’s browser
using ported JavaScript took 4s. Generating the same hasheson an Amazon EC2 m3.medium using C++ took 3s.
TABLE X: C OMPUTING TIME COMPARISON FOR GENERA TING A RAINBOW
TABLE .
Input size Browser(native JS) EC2(c++) Browser(ported JS)
9E5 90s 3s 4s
Analysis of Results. Suppose an attacker compromises
websites with a cumulative trafﬁc of 1 million visitors perday and an average browsing time of 15 minutes. The attackerwould be able to wield the equivalent of 10
6×0.25hour×
0.8/2×365day =3.65×107computing hours a year in
a browser. To accomplish the same task with an AmazonEC2 m3.medium instance for a year, the yearly cost wouldbe3.65×10
7/4×3×0.07$/hour =$ 1,916, 250.Crequest =
(2.5+2)× 106×900/10×365×0.0075/104= $110, 869. With
our utility function U=Ccloud−O×Ptransferin −Crequest ,
the yearly cost saving Uis around $1.8Million. The near-
native computing speed of JavaScript with browser and thelow data transferred per unit of computational work makesthis an effective processing task to distribute.
V. R
ELA TED WORK
The term “volunteer computing” was ﬁrst used by Luis
F. G. Sarmentan [20]. He developed a Java applet-basedvolunteer computing system called Bayanihan [20] in 1998.SETI@Home [21] was one of the earliest projects to prove thepracticality of the volunteer computing concept. SETI@Homewas designed to use the numerous personal computers of thepublic to process vast amounts of telescope signals to searchfor extraterrestrial intelligence. Some of SETI@Home’s suc-cessors include Folding@Home [22], which simulates proteinfolding for disease research, and BOINC [23], which is aplatform to hold various themes of research projects. Whatthese projects share in common is that they all focus on non-proﬁt scientiﬁc computing and they all require users to installa specialized client-side software in order to participate in theprojects. Whereas in our paper, we analyze the highly volatilebrowser-based data processing domain and extend beyondscientiﬁc computing to websites’ business operations.
A number of researchers have investigated volunteer com-
puting with browsers. The primary advantage of a browser-based approach is that the user only needs to open a web pageto take part in the computation. Krupa et al. [24] proposeda browser-based volunteer computing architecture for webcrawling. Finally, Konishi et al. [25] evaluated browser-basedcomputing with Ajax and the comparative performance ofJavaScript and legacy computer languages. In our work, weadd a comprehensive analysis of: 1) the architectural changesto optimize this paradigm for websites served from cloudenvironments; 2) the impact of page-view time on schedulerefﬁciency and data transfer; 3) a cost model for assessing thecost-effectiveness of distributing a given task to browser-basedclients as opposed to running the computation in the cloud; and4) present a number of practical examples of data processingtasks to support website operators, particularly social networksand e-commerce.
MapReduce [1] is a programming model for large par-
allel data processing proposed by Google, which has beenadapted to various distributed computing environments suchas volunteer computing [26]. There are some early prototypesimplementing MapReduce with JavaScript [6]. Lin et al. [26]observed that the traditional MapReduce is proposed for ho-mogeneous cluster environments and performs poorly on vol-unteer computing systems where computing nodes are volatileand with high rate of unavailability, as we also demonstratedwith our derivation of μin Section III-E. They propose MOON
(MapReduce On Opportunistic eNvironments) which extendsHadoop with adaptive task and data scheduling algorithmsand achieves a 3-fold performance improvement. However,MOON targets institutional intranet environments like studentlabs where computer nodes are connected with a local networkwith high bandwidth and low latency. We focus on cloud andInternet environments with highly heterogeneous computationability, widely varying client participation times, and non-ﬁxedcosts for data transfer and task distribution resources.
VI. C
ONCLUSION
Every day, millions of users opt into allowing websites to
use their browsers’ computing resources to perform computa-tional tasks, such as form validation. In this paper, we explorethe feasibility, cost-effectiveness, user experience impact, andarchitectural optimizations for leveraging the browsers ofwebsite visitors for more intensive distributed data processing,which we term gray computing. Although previous researchhas demonstrated it is possible to build distributed data pro-cessing systems with browsers when the web visitors explicitlyopt into the computational tasks that they perform, no detailedanalysis has been done regarding the computational power,user impact, and cost-effectiveness of these systems whenthey rely on casual website visitors. The empirical resultsfrom performing a variety of gray computing tasks, rangingfrom face detection to sentiment analysis, show that there issigniﬁcant computational power in gray computing and largeﬁnancial incentives to exploit it. Due to these incentives andthe vast potential for misuse, we believe that much moreresearch is needed into the security and ethical considerationsaround gray computing.
As part of the analysis in this paper, we derived a cost
model that can be used to assess the suitability of differentdata processing tasks for distributed data processing with graycomputing. This cost model can aid future discussions of waysof legitimately incentivizing users to opt-into gray computing.Further, we pinpointed the key factors that determine whethera task is suitable for gray computing and provide a processfor assessing the suitability of new data processing task types,which can help aid in guiding the design of gray computingsystems and user incentive programs. We also presented anumber of architectural solutions that can be employed to ex-ploit cost and performance asymmetries in cloud environmentsto improve the cost-effectiveness of gray computing.
176
176
176
176
ICSE 2015, Florence, ItalyREFERENCES
[1] J. Dean and S. Ghemawat, “Mapreduce: simpliﬁed data processing on
large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–113,
2008.
[2] L. F. Sarmenta, “V olunteer computing,” Ph.D. dissertation, Citeseer,
2001.
[3] “Y outube statistics,” https://www.youtube.com/yt/press/statistics.html.
[4] “Intel processors,” http://www.intel.com/support/processors/sb/
CS-017346.htm.
[5] “Top500 supercomputer,” http://www.top500.org/.[6] S. Ryza and T. Wall, “Mrjs: A javascript mapreduce framework for
web browsers,” URL http://www. cs. brown. edu/courses/csci2950-
u/f11/papers/mrjs. pdf, 2010.
[7] R. Cushing, G. H. H. Putra, S. Koulouzis, A. Belloum, M. Bubak,
and C. De Laat, “Distributed computing on an ensemble of browsers,”
Internet Computing, IEEE, vol. 17, no. 5, pp. 54–61, 2013.
[8] J.-J. Merelo, A. M. Garc ´ıa, J. L. J. Laredo, J. Lupi ´on, and F. Tricas,
“Browser-based distributed evolutionary computation: performance andscaling behavior,” in Proceedings of the 2007 GECCO conference
companion on Genetic and evolutionary computation. ACM, 2007,pp. 2851–2858.
[9] P . Langhans, C. Wieser, and F. Bry, “Crowdsourcing mapreduce:
Jsmapreduce,” in Proceedings of the 22nd international conference
on World Wide Web companion. International World Wide WebConferences Steering Committee, 2013, pp. 253–256.
[10] “Emscripten,” http://kripken.github.io/emscripten-site/index.html.
[11] “Clang,” http://clang.llvm.org/.[12] Greasemonkey, “http://www.greasespot.net/.”[13] L. F. Sarmenta, “Sabotage-tolerance mechanisms for volunteer comput-
ing systems,” Future Generation Computer Systems, vol. 18, no. 4, pp.
561–572, 2002.
[14] C. Liu, R. W. White, and S. Dumais, “Understanding web browsing
behaviors through weibull analysis of dwell time,” in Proceedings of the
33rd international ACM SIGIR conference on Research and development
in information retrieval. ACM, 2010, pp. 379–386.
[15] W. Weibull, “Wide applicability,” Journal of applied mechanics, 1951.[16] P . Viola and M. Jones, “Rapid object detection using a boosted cascade
of simple features,” in Computer Vision and Pattern Recognition, 2001.
CVPR 2001. Proceedings of the 2001 IEEE Computer Society Confer-ence on, vol. 1. IEEE, 2001, pp. I–511.
[17] B. Pang and L. Lee, “Opinion mining and sentiment analysis,” F oun-
dations and trends in information retrieval, vol. 2, no. 1-2, pp. 1–135,2008.
[18] B. Pang and L. Lee, “A sentimental education: Sentiment analysis using
subjectivity summarization based on minimum cuts,” in Proceedings of
the 42nd annual meeting on Association for Computational Linguistics.Association for Computational Linguistics, 2004, p. 271.
[19] P . Oechslin, “Making a faster cryptanalytic time-memory trade-off,” in
Advances in Cryptology-CRYPTO 2003. Springer, 2003, pp. 617–630.
[20] L. F. Sarmenta, “Bayanihan: Web-based volunteer computing us-
ing java,” in Worldwide Computing and Its Applications(WWCA’98).
Springer, 1998, pp. 444–461.
[21] D. P . Anderson, J. Cobb, E. Korpela, M. Lebofsky, and D. Werthimer,
“Seti@ home: an experiment in public-resource computing,” Communi-
cations of the ACM, vol. 45, no. 11, pp. 56–61, 2002.
[22] Folding@Home, “http://folding.stanford.edu/.”[23] D. P . Anderson, “Boinc: A system for public-resource computing and
storage,” in Proceedings of Fifth IEEE/ACM International Workshop on
Grid Computing, 2004. IEEE, 2004, pp. 4–10.
[24] T. Krupa, P . Majewski, B. Kowalczyk, and W. Turek, “On-demand web
search using browser-based volunteer computing,” in Sixth International
Conference on Complex, Intelligent and Software Intensive Systems(CISIS), 2012. IEEE, 2012, pp. 184–190.
[25] F. Konishi, S. Ohki, A. Konagaya, R. Umestu, and M. Ishii, “Rabc:
A conceptual design of pervasive infrastructure for browser computingbased on ajax technologies,” in Seventh IEEE International Symposium
on Cluster Computing and the Grid, 2007. CCGRID 2007. IEEE, 2007,
pp. 661–672.
[26] H. Lin, X. Ma, J. Archuleta, W.-c. Feng, M. Gardner, and Z. Zhang,
“Moon: Mapreduce on opportunistic environments,” in Proceedings of
the 19th ACM International Symposium on High Performance Distribut-ed Computing. ACM, 2010, pp. 95–106.
177
177
177
177
ICSE 2015, Florence, Italy
View publication stats