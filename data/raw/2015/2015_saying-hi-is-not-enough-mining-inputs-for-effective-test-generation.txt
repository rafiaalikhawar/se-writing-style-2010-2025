Saying ‚ÄòHi!‚Äô Is Not Enough:
Mining Inputs for Effective Test Generation
Luca Della Toffola
Department of Computer Science
ETH Zurich, SwitzerlandCristian-Alexandru Staicu
Department of Computer Science
TU Darmstadt, GermanyMichael Pradel
Department of Computer Science
TU Darmstadt, Germany
Abstract ‚ÄîAutomatically generating unit tests is a powerful
approach to exercise complex software. Unfortunately, current
techniques often fail to provide relevant input values, such as
strings that bypass domain-speciÔ¨Åc sanity checks. As a result,
state-of-the-art techniques are effective for generic classes, such
as collections, but less successful for domain-speciÔ¨Åc software.
This paper presents TestMiner, the Ô¨Årst technique for mining
a corpus of existing tests for input values to be used by test
generators for effectively testing software not in the corpus.
The main idea is to extract literals from thousands of tests
and to adapt information retrieval techniques to Ô¨Ånd values
suitable for a particular domain. Evaluating the approach with
40 Java classes from 18 different projects shows that TestMiner
improves test coverage by 21% over an existing test generator.
The approach can be integrated into various test generators in a
straightforward way, increasing their effectiveness on previously
difÔ¨Åcult-to-test classes.
I. I NTRODUCTION
Automated test generation is a powerful approach to create
inputs for exercising a software under test with minimal human
effort. Existing approaches use a wide range of techniques,
ranging from feedback-directed random test generation [5],
[37], [39], over search-based approaches [11], [17], to sym-
bolic reasoning-based test generators [8], [21], [45], [52].
Despite all successes, test generation still suffers from non-
trivial limitations. For example, a study reports that only
19.9% of bugs in a well known collection of existing faults are
revealed by the test suites generated by three state-of-the-art
test generators [48].
An important limitation is that test generators often fail to
cover a buggy statement because the inputs provided in the
test do not enable the code to bypass sanity checks that reject
invalid inputs. In particular, creating bug-revealing inputs often
requires suitable strings, but creating such strings requires do-
main knowledge about the software under test, which existing
test generators do not have. For example, consider testing a
class responsible for parsing SQL statements. Testing the class
with a randomly generated string is highly unlikely to reach
deeply into the code because the invalid input is discarded
quickly due to a parse error.
State-of-the-art test generators obtain input data, such as
strings, in various ways. First, most generators use randomly
This work has been supported by the DFG within ConcSys, and by
the BMBF and the HMWK within CRISP, and in part by SNF grant
206021 133835.generated values or values from a Ô¨Åxed pool, which are cheap
to obtain but unlikely to match domain-speciÔ¨Åc data formats.
For example, Randoop [37] often uses the value ‚Äúhi!‚Äù as a
string value. Second, some test generators extract constants,
e.g., stored in Ô¨Åelds of the class under test, and return values
of previously called methods and use these values as inputs.
This approach is effective if suitable constants and methods
are available, but fails otherwise.
Third, some test generators symbolically reason about ex-
pected values [22], e.g., based on a constraint solver able
to reason about strings [55]. While effective, this approach
often suffers from scalability issues and may not provide the
best cost-beneÔ¨Åt ratio, which is crucial for testing [6]. Finally,
some test generators rely on a grammar that describes expected
string values [20], [54]. However, including grammars for all,
or even most, domain-speciÔ¨Åc input formats into a general-
purpose test generators is impractical.
This paper present TestMiner, a novel technique to ad-
dress the problem of generating domain-speciÔ¨Åc input values
suitable for a given software under test. The key idea is to
exploit the wealth of information available in existing code
bases, in particular in existing tests, using an information
retrieval-inspired mining technique that predicts input values
suitable for testing a particular method. The approach consists
of two phases. At Ô¨Årst, the approach extracts literals from
the source code of existing tests and indexes them for quick
retrieval. Then, a test generator queries the mined data for
values suitable for a given method under test. These predicted
values are then used as test inputs during test generation. Our
idea can be incorporated into any automated test generator that
requires input values of primitive types or strings.
In summary, this paper makes the following contributions:
Information Retrieval for Test Inputs. We are the Ô¨Årst to
exploit the knowledge hidden in large amounts of existing
code to address the problem of Ô¨Ånding suitable input
values for testing.
Scalable and EfÔ¨Åcient Prediction of Domain-speciÔ¨Åc Val-
ues. We present a scalable and efÔ¨Åcient technique to
predict input values suitable for a given method under test
and show how to integrate the technique into an existing
test generator.
More Effective Test Generation. We show empirically
that the presented approach positively inÔ¨Çuences the
effectiveness of a state-of-the-art test generator.
978-1-5386-2684-9/17/$31.00 c2017 IEEEASE 2017, Urbana-Champaign, IL, USA
Technical Research - New Ideas44
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. II. A PPROACH
This section presents TestMiner, an approach for mining test
input values from existing projects. Figure 1 gives an overview
of the main steps.
A. Static Analysis of Test Suites
This Ô¨Årst step extracts input values from a corpus of tests
and associates to each literal value a context to be used later
for retrieving values during test generation.
DeÔ¨Ånition 1 (Context-value pair). A context-value pair (C; v)
consists of a context C:S !N, represented as a bag of
words, and a value v2V in some value domain. The set S
refers to the set of all strings.
The contextCmay be anything that can be represented as
a bag of words. There are various options for deÔ¨Åning the
context, such as the calling context for the method under test,
the type hierarchy of the program, or the signature of a method
that is tested. In this work, we compute the context of a value
from the fully qualiÔ¨Åed signature of the method that receives
the value as an argument. This notion of context works well
because fully qualiÔ¨Åed method signatures often contain rich
semantic information [24].
For each test in the corpus, the static analysis collects the
fully qualiÔ¨Åed method signature of all calls sites that pass a
literal argument, and it annotates each argument with its static
type. The analysis returns for each test a set of call site tuples.
DeÔ¨Ånition 2 (Call site tuple). A call site tuple Sc=
(Tc; mc;Vc)consists of a setTcof fully qualiÔ¨Åed type names
in which the method mcis deÔ¨Åned, the method name mc, and
a setVcof values passed as arguments at the call site.
For example, suppose a test case calls
sqlParser.parse("SELECT x FROM y"). Then the
static analysis extracts, the following call site tuple:
(forg :sql:SQLParserg;parse ;f"SELECT x FROM y "g).
The setTcmay, in principle, contain multiple type names
because a call may resolve to multiple methods. Our static
analysis considers only the statically declared type of the
base object of a call, i.e., jTcj= 1. Completely resolving all
possible call targets would require a whole-program analysis,
which does not scale to a large corpus. For the set Vcof values,
we focus on string values in this work. Applying the idea to
another value domain, e.g., integers, is straightforward. The
reason is that Ô¨Ånding suitable strings is a major obstacle for
state-of-the-art test generators [48].
Finally, the analysis transforms the tuples into context-
value pairs. For this purpose, the approach tokenizes the type
names inTcand the method name at dot-delimiters, splitting
strings based on the camelCase and snake case conventions,
and it normalizes the remaining tokens into lower case. The
resulting strings are then represented as a bag of words, which
represents the context C. For the above example, the analysis
yields this context-value pair:
(forg7!1; sql7!2; parser7!2g;"SELECT x FROM y ")In the second step the approach summarizes and indexes
context-value pairs for a later retrieval. The basic idea is to
associate each input value with one or more hash values that
summarize the context in which the input value occurs. The
resulting hash map then serves efÔ¨Åciently retrieving values
suitable for a particular context. These steps are summarezied
in Algorithm 1 and explained in detail here.
Algorithm 1 Summarize and index context-value pairs.
Input: SetPof context-value pairs
Output: Index-to-values map M
1:M empty map
2:for all (C; v)2P do
3:Cweighted normalize (tdf (C))
4:h simHash (Cweighted )
5: updateM(h) withv
6:end for
7:returnM
1) Assigning Weights to Context Words: While some words
in the context convey useful information about the domain of
the tested code, others may be redundant. For example the
word ‚Äúsql‚Äù is crucial to describe the context for a method that
expects an SQL query. In contrast, words such as ‚Äúorg‚Äù are
very frequent and occur across unrelated domains.
To enable TestMiner to focus on the most relevant words
in a context, we compute a weight for each word using
theterm frequency-inverse document frequency (tÔ¨Ådf). which
represents how important a word is to a document in a corpus
of documents. This measure is commonly used for information
retrieval. Formally, we compute tÔ¨Ådf as
tdf (t; d; D ) =ft;dlogjDj
jfd2D:t2dgj+ 1
(1)
where document dis the contextC, a term tis a context
word inC, the corpus Dis the set of all contexts, and where
ft;dis the frequency of term tin document d. For the above
example a low weight is assigned to ‚Äúorg‚Äù because this word
occurs frequently in the corpus, and it assigns a relatively
high weight to ‚Äúsql‚Äù because this word is relatively uncommon
but appears twice in the context. As a result, the approach
champions the informative parts of the signature and penalizes
the less informative ones.
2) Indexing with Locality-Sensitive Hashing: To index the
context words, we use the locality-sensitive hash function
Simhashing [10]. This class of hash functions is designed to
assign to similar values a similar hash value with high prob-
ability, preserving the value similarity also in the hash space.
In addition to efÔ¨Åcient retrieval through hashing, this choice
of hash function enables TestMiner to generalize beyond the
exact contexts extracted from the corpus. After assigning
weights to context words, TestMiner indexes the values for
efÔ¨Åcient retrieval (line 4) and stores values indexed by their
hash value (line 5). The Ô¨Ånal result of the indexing step of
TestMiner is a map that assigns hash values to test input
values:
45
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. Static
analysisIndexing RetrievalTest
generatorCorpus
of testsContext-
value pairsMap of
input values
QueriesValues
Fig. 1. Overview of the TestMiner approach. The blue components are part of the approach.
DeÔ¨Ånition 3 (Index-to-values map). The index-to-values map
M:Booleank!(V ! N)assigns a bit vector of length
kto a bag of values. The bag of values is itself a map that
assigns each value to its number of occurrences.
This representation summarizes the context-value pairs ex-
tracted from different projects by grouping together all values
with a similar context.
B. Retrieval of Values for Test Generation
TestMiner provides values to a test generator. When the test
generator retrieves values, e.g., to pass them to constructor or
method call, it is crucial to a query in a timely manner because
the time budget allocated for test generation is limited.
1) Integration into Test Generator: As a proof-of-concept,
we integrate TestMiner into the state-of-the-art feedback-
directed random test generator Randoop [37]. When Randoop
requires a string value, e.g., to pass it as an argument to a
method under test, we override its default behavior so that
it queries TestMiner for Vquery input values with a Pquery
probability.
2) Retrieval of Values: Algorithm 2 summarizes how Test-
Miner retrieves test input values for a given query with a
contextCq. At Ô¨Årst, it assigns weights to the query context
words inCqusing the following weighting function:
(0:5 + 0:5ft;q
max tft;q)logjDj
jfd2D:t2dgj+ 1
(2)
where ft;qis the frequency of a context word in the query and
all the other terms have the same meaning as in Equation 1.
These weights give the same importance to the word frequency
in the query and to its inverse document frequency in the
corpus, effectively prioritizing uncommon words. The weight-
ing function prevents bias towards longer signatures that may
contain multiple similar words. TestMiner matches the query
contextCqagainst contexts that have similar bit vectors using
the search algorithm presented in [33]. The search function
searchSimhash returns a mapR:String!Nof indexed
input values. This function selects values from hash indices
that differ at most in distbitsbits from the Simhashinged query
context. The threshold distbitscontrols the number of input
values returned to the test generator. A high value for distbits
matches against many contexts in the corpus, at the cost of
high query latency and possibly unrelated values. Finally,
the algorithm returns a map that represents a probability
distribution across suggested values where the probability of a
value is proportional to its frequency across all context-value
pairs.Algorithm 2 Retrieve values from index-to-values map.
Input: Query contextCqand index-to-values map M
Output: Probability distribution Vresult of values
1:Cq;weighted tdfWeightsForQuery (Cq)
2:R=searchSimhash (Cq;weighted ;M;dist bits)
3:return probDistribution (R;M)
III. E VALUATION
A. Implementation
We integrate TestMiner into Randoop 3.0.8, a state-of-the-
art feedback-directed, random test generator [37]. The retrieval
part of our approach is implemented as a server application,
making easy to integrate it into other test generators.
The static analyses part of TestMiner is implemented on
top of the Eclipse JDT framework [1]. Integrating TestMiner
into Randoop required only to change about 100 lines in the
ForwardGenerator class and to add about 70 lines of new
code to communicate with TestMiner.
To query TestMiner for values to be passed to a method m,
the modiÔ¨Åed Randoop performs multiple queries with different
contextsCq:
the package, the class and the method name, and
the class and the method name,
the method name.
Querying with multiple contexts allows the test generator to
retrieve more diverse values than with a single query because
different queries emphasize different domain speciÔ¨Åc terms.
We then combine all the returned values obtained from the
three different queries into one set. Our implementation is
publicly available at [3].
B. Experimental Setup
Data to Learn From: To learn about values suitable in a
particular context, we apply TestMiner to 3,601 Java projects
from the the Maven Central Repository [2]. We use all projects
with source code of tests, which yields 263,276 string values
used in 37,821 different contexts.
Classes Under Test: To evaluate the effectiveness of tests
generated with TestMiner, we use 40 classes from 18 open
source Java projects. 20 of these classes have been previously
used for evaluating test generators [19], [35], [46], [47]. We
further augment the existing benchmark classes with string
manipulation classes from Defects4J [27] and with parsers of
different text formats. Because the overall goal of TestMiner
is to suggest values for classes beyond the corpus that the
approach learns from, we remove from the corpus all the call
site tuples that contain a type name in the projects of the
classes under test.
46
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. Test Generation: For each class under test, we generate
tests using both the default version of Randoop and the
TestMiner-enhanced version. We use a time budget of 5
minutes per class and repeat each experiment 10 times to
account for the random-based nature of Randoop, where each
experiment uses a different random seed. Similar parameters
were used in previous work [48]. We run the tests using
JUnit and measure test coverage using the JaCoCo library. The
coverage is the ratio of executed branches and all branches in
the source code of a class. All experiments are performed on
a machine with 48 Intel Xeon E5-2650 CPU cores and 64GB
of memory.
C. Effectiveness of Generated Tests
To assess to what extent TestMiner increases the effective-
ness of generated tests, we measure the branch coverage in
each class under test. Figure 2 shows the average coverage
over 10 test suites generated for each class. Overall, TestMiner
improves the coverage for 30 classes and decreases it for 2
classes. On average over all classes, the relative improvement
is 21%, increasing coverage from 57% to 78%. A Wilcoxon
signed rank test shows this increase to be statistically sig-
niÔ¨Åcant (p < 2:710 6). Cohen‚Äôs deffect size, which is
a statistical measure of the strength of the increase, is 0:87,
which is considered large.
Increased Coverage: TestMiner increases coverage from
0% to more than 30% for the classes PathURN, ResourceURL,
and URN because Randoop is unable to instantiate these
classes. In contrast, by using domain-speciÔ¨Åc strings, Test-
Miner helps instantiate these classes, enabling the test
generator to explore their behavior. However, for the
UTF8StreamJsonParser class, TestMiner also fails to in-
stantiate the class. Manual inspection shows that the class
requires a complicated IOContext object that reads JSON
Ô¨Åles and assumes such Ô¨Åles to exist. Providing such Ô¨Åles is
out of reach for both Randoop and TestMiner but may be
addressed, e.g., by symbolic testing techniques [8].
Decreased Coverage: TestMiner decreases the branch
coverage for 2 classes. A manual inspection of the produced
test suites for StringEscapeUtils shows that the constant
retrieval fails due to an implementation error in our prototype
caused by a non-escaped unicode character that produces a
communication error with the server. For DateTime and
StrTokenizer, the decrease is on average, but achieves
higher coverage than Randoop for some test suites, as shown
by the error bars. Besides a few classes, most of the results
do not show a signiÔ¨Åcant difference in coverage across the 10
repetitions, i.e. the error bars are moderately small. The reason
is the large timeout, which allows the test generator to reach
saturation.
D. Query Result Examples
TestMiner provides to the test generator semantically rich
and diverse values. The following values are examples from
tests suites generated during our experiments:
1)IBAN: SCBL00000011234567022)SQL: ‚Äôabc‚Äô LIKE ‚Äô_‚Äô
3)Network address: fe80::226:8ff:fefa:d1e3
4)E-mail: test@example.org
However, there are also strings that do not help in testing
a speciÔ¨Åc method, such as ‚Äúfoo‚Äù or ‚Äúmetadata‚Äù. Fortunately,
due to the nature of feedback-directed test generation, these
values are likely to be ignored in later stages of the generation
process. For example, Randoop Ô¨Ålters already seen values that
did not trigger any errors during execution.
E. Performance
Analysis and Indexing: The static analysis takes several
hours to process the entire corpus but Ô¨Ånishes within a single
day. Indexing the context-value pairs takes about 20 seconds.
Retrieval: Retrieving values from TestMiner takes longer
than using Randoop‚Äôs hard-coded constants. To measure slows
down of test generation, we compare the size of the test
suites generated by Randoop and TestMiner. In the 5 minutes
time budget, Randoop generates 545,895 tests, whereas the
TestMiner-enhanced test generator creates only 243,494 tests,
i.e., a 55% reduction. During our evaluation, millions of string
values are requested by the test generator, but the number of
unique queries is only 481, allowing our implementation to
make extensive use of caching to keep the runtime overhead
low. Overall, TestMiner slows down the test generation, but
the increased runtime cost pays off because the tests generated
with TestMiner are signiÔ¨Åcantly more effective.
F . InÔ¨Çuence of Parameters
TestMiner has three meta-parameters, which we set experi-
mentally to maximize coverage increase:
distbits = 16. Running TestMiner with distbits =
4;8signiÔ¨Åcantly reduces branch coverage because fewer
strings are returned to Randoop, which often defaults
to its built-in strings. Setting distbits = 32 drastically
increases query time and reduces the number of generated
test in the time budget.
Pquery = 0:5. Running TestMiner with Pquery =
0:25; 0:5;0:75 provides a lower branch coverage.
Vquery = 10. Running TestMiner with Vquery = 5;15
provides no signiÔ¨Åcant difference in branch coverage.
IV. R ELATED WORK
Test Generation: There are various approach for auto-
matically generating test cases: symbolic [8], [28] and con-
colic [21], [45] execution [9], [52], [53], random-based test
generation [16], [37], and search-based testing [17]. Beyond
unit tests, automated testing has been applied, e.g., to concur-
rent software [13], [39] and to graphical user interfaces [12],
[14], [23], [40]. Our work is orthogonal and could be inte-
grated into many of these approaches.
Learning From Existing Code to Improve Test Genera-
tion: Liu et al. train a neural network to suggest textual inputs
for mobile apps [32]. Similar to TestMiner, they learn from ex-
isting tests how to create test inputs. Our work differs by using
information retrieval instead of a neural network, by learning
47
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply.  0 20 40 60 80 100
se.kth.cid.identity.MIMETypese.kth.cid.identity.PathURN
se.kth.cid.identity.ResourceURLse.kth.cid.identity.URIse.kth.cid.identity.URN
org.openscience.cdk.index.CASNumberorg.apache.commons.io.FilenameUtils
org.apache.commons.lang3.StringEscapeUtilsorg.apache.commons.lang3.StringUtilsorg.apache.commons.lang3.Validate
org.apache.commons.lang3.math.NumberUtilsorg.apache.commons.lang3.text.StrTokenizerorg.apache.commons.lang3.text.WordUtils
org.apache.commons.validator.routines.DateValidatororg.apache.commons.validator.routines.EmailValidatororg.apache.commons.validator.routines.IBANValidatororg.apache.commons.validator.routines.ISBNValidator
org.apache.commons.validator.routines.InetAddressValidatororg.apache.commons.validator.routines.UrlValidatornet.sf.dblp2db.dblpstat.db.fields.Isbnnet.sf.dblp2db.dblpstat.db.fields.Monthnet.sf.dblp2db.dblpstat.db.fields.Yearcom.google.gson.JsonParser
com.google.common.base.Strings
com.google.common.net.InetAddresses
com.fasterxml.jackson.core.json.UTF8StreamJsonParserorg.joda.time.DateTimeorg.jsoup.parser.Parserorg.jxpfw.util.CLocale
org.jxpfw.util.InternationalBankAccountNumbercom.efisto.util.Util
com.puzzlebazar.client.util.ValidateEmailstempeluhr.validation.TimeChecker
uk.gov.tameside.apps.validation.DateFormatValidatoruk.gov.tameside.apps.validation.NumericValidatoruk.gov.tameside.apps.validation.PostCodeValidatorcom.facebook.presto.sql.parser.SqlParsercom.prowidesoftware.swift.model.BICcom.prowidesoftware.swift.model.IBANwebwork.examples.userreg.ValidatorAverageCoverage (%)Randoop
TestMinerFig. 2. Coverage without and with TestMiner. The colored bars represent the average coverage value over 10 runs of the test generator. The size of the error
bars shows the difference between the Ô¨Årst and the third quartile.
from already existing tests written by developers instead of
writing tests speciÔ¨Åcally for learning, and by generating unit
tests instead of UI-level tests.
Testilizer [36] mines UI tests for web applications by
collecting input values for generating tests. In contrast, Test-
Miner statically collects input values from tests written for a
different application. The ‚Äúequivalence modulo input‚Äù (EMI)
approach [30] tests compilers by modifying existing tests, i.e.,
programs, into new test. Our work also learns how to create
new tests from existing tests but applies to various applications
domains beyond compilers. Several approaches improve test
generation by learning from existing code which methods to
call [25], [50], [51]. TestMiner differs from these techniques
by improving the selection of input values instead of the
selection of calls.
Domain Knowledge for Test Generation: Studies show
that providing domain knowledge to test generators improves
testing effectiveness [18], [44]. Several approaches obtain
domain-speciÔ¨Åc inputs, e.g., by querying web sites [35], [46],
web services [7], or semantic knowledge graphs [34]. All these
techniques require querying the internet for retrieving values.
To the best of our knowledge, TestMiner is the Ô¨Årst ofÔ¨Çine
technique to suggest domain-speciÔ¨Åc input values.
Learning from Existing Source Code: Existing work
exploits natural language information in source code, e.g., to
detect programming errors [31], [38] and suboptimal identiÔ¨Åer
names [4], [24], to cluster software systems [15], to inferspeciÔ¨Åcations [56], and to Ô¨Ånd inconsistencies between code
and comments [49]. Our work also exploits domain knowledge
encoded in natural language, speciÔ¨Åcally in identiÔ¨Åer names,
to improve testing. Other work on learning from existing
code includes learning a statistical language model for code
completion [43] and applying information retrieval to the
problem of bug localization [26], [29], [41], [42], [57].
V. C ONCLUSION
Test generation is a challenging problem, and Ô¨Ånding suit-
able input values is an important part of this challenge. Our
work presents TestMiner, a new approach that learns from a
large corpus of existing tests which input values to use in
newly generated tests based on the domain of the tested soft-
ware. The approach combines static analysis and information
retrieval to extract input values, to index them based on the
context in which they occur, and to provide values suitable for
a speciÔ¨Åc domain to a test generator. Our evaluation shows
that TestMiner improves test coverage from 57% to 78%, on
average over a set of 40 classes that challenge state-of-the-art
test generators. The approach scales to thousands of analyzed
projects, efÔ¨Åciently responds to queries for input values, and
generalizes beyond the software analyzed as part of the corpus.
The TestMiner approach provides a simple querying interface
that enables existing test generators to beneÔ¨Åt from domain-
speciÔ¨Åc input values with little effort.
48
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] Eclipse JDT. http://www.eclipse.org/jdt.
[2] Maven Central. https://search.maven.org.
[3] TestMiner - Repository. https://github.com/lucadt/testminer.
[4] M. Allamanis, E. T. Barr, C. Bird, and C. A. Sutton. Suggesting accurate
method and class names. In ESEC/FSE, pages 38‚Äì49, 2015.
[5] S. Artzi, J. Dolby, S. H. Jensen, A. M√∏ller, and F. Tip. A framework
for automated testing of JavaScript web applications. In ICSE, pages
571‚Äì580, 2011.
[6] M. B ¬®ohme and S. Paul. On the efÔ¨Åciency of automated testing. In FSE,
pages 632‚Äì642, 2014.
[7] M. Bozkurt and M. Harman. Automatically generating realistic test input
from web services. In SOSE, pages 13‚Äì24, 2011.
[8] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted and automatic
generation of high-coverage tests for complex systems programs. In
OSDI, pages 209‚Äì224. USENIX, 2008.
[9] C. Cadar and K. Sen. Symbolic execution for software testing: three
decades later. Commun. ACM, 56(2):82‚Äì90, Feb. 2013.
[10] M. Charikar. Similarity estimation techniques from rounding algorithms.
InSTOC, pages 380‚Äì388, 2002.
[11] W.-F. Chiang, G. Gopalakrishnan, Z. Rakamaric, and A. Solovyev.
EfÔ¨Åcient search for inputs causing high Ô¨Çoating-point errors. In PPOPP,
pages 43‚Äì52, 2014.
[12] W. Choi, G. Necula, and K. Sen. Guided GUI testing of Android apps
with minimal restart and approximate learning. In OOPSLA, pages 623‚Äì
640, 2013.
[13] A. Choudhary, S. Lu, and M. Pradel. EfÔ¨Åcient detection of thread safety
violations via coverage-guided generation of concurrent tests. In ICSE,
2017.
[14] S. R. Choudhary, A. Gorla, and A. Orso. Automated test input generation
for android: Are we there yet? (E). In ASE, pages 429‚Äì440, 2015.
[15] A. Corazza, S. D. Martino, V . Maggio, and G. Scanniello. Investigating
the use of lexical information for software system clustering. In CSMR,
pages 35‚Äì44, 2011.
[16] C. Csallner and Y . Smaragdakis. JCrasher: an automatic robustness tester
for Java. Software Practice and Experience, 34(11):1025‚Äì1050, 2004.
[17] G. Fraser and A. Arcuri. Evosuite: automatic test suite generation for
object-oriented software. In ESEC/FSE, pages 416‚Äì419, 2011.
[18] G. Fraser and A. Arcuri. The seed is strong: Seeding strategies in
search-based software testing. In ICST, pages 121‚Äì130, 2012.
[19] J. P. Galeotti, G. Fraser, and A. Arcuri. Improving search-based test
suite generation with dynamic symbolic execution. In ISSRE, pages
360‚Äì369, 2013.
[20] P. Godefroid, A. Kiezun, and M. Y . Levin. Grammar-based whitebox
fuzzing. In PLDI, pages 206‚Äì215, 2008.
[21] P. Godefroid, N. Klarlund, and K. Sen. DART: directed automated
random testing. In PLDI, pages 213‚Äì223. ACM, 2005.
[22] P. Godefroid, M. Y . Levin, and D. A. Molnar. Automated whitebox
fuzz testing. In Network and Distributed System Security Symposium
(NDSS), 2008.
[23] F. Gross, G. Fraser, and A. Zeller. Search-based system testing: High
coverage, no false alarms. In ISSTA, pages 67‚Äì77, 2012.
[24] E. W. H√∏st and B. M. √òstvold. Debugging method names. In ECOOP,
pages 294‚Äì317. Springer, 2009.
[25] H. Jaygarl, S. Kim, T. Xie, and C. K. Chang. OCAT: object capture-
based automated testing. In ISSTA, pages 159‚Äì170. ACM, 2010.
[26] H. Jiang, T. N. Nguyen, I.-X. Chen, H. Jaygarl, and C. K. Chang.
Incremental Latent Semantic Indexing for Automatic Traceability Link
Evolution Management. In ASE, 2008.
[27] R. Just, D. Jalali, and M. D. Ernst. Defects4j: a database of existing
faults to enable controlled testing studies for java programs. In ISSTA,
pages 437‚Äì440, 2014.
[28] J. C. King. Symbolic execution and program testing. Communications
of the ACM, 19(7):385‚Äì394, 1976.
[29] A. N. Lam, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen. Combining
Deep Learning with Information Retrieval to Localize Buggy Files for
Bug Reports. In ASE, 2015.[30] V . Le, M. Afshari, and Z. Su. Compiler validation via equivalence
modulo inputs. In PLDI, pages 216‚Äì226, 2014.
[31] H. Liu, Q. Liu, C.-A. Staicu, M. Pradel, and Y . Luo. Nomen est omen:
Exploring and exploiting similarities between argument and parameter
names. In ICSE, pages 1063‚Äì1073, 2016.
[32] P. Liu, X. Zhang, M. Pistoia, Y . Zheng, M. Marques, and L. Zeng.
Automatic text input generation for mobile testing. In ICSE, 2017.
[33] G. S. Manku, A. Jain, and A. Das Sarma. Detecting near-duplicates for
web crawling. In WWW, pages 141‚Äì150, 2007.
[34] L. Mariani, M. Pezz `e, O. Riganelli, and M. Santoro. Link: exploiting
the web of data to generate test inputs. In ISSTA, pages 373‚Äì384, 2014.
[35] P. McMinn, M. Shahbaz, and M. Stevenson. Search-based test input
generation for string data types using the results of web queries. In
ICST, pages 141‚Äì150, 2012.
[36] A. Milani Fard, M. Mirzaaghaei, and A. Mesbah. Leveraging Existing
Tests in Automated Test Generation for Web Applications. In ASE,
pages 67‚Äì78, 2014.
[37] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball. Feedback-directed
random test generation. In ICSE, pages 75‚Äì84. IEEE, 2007.
[38] M. Pradel and T. R. Gross. Detecting anomalies in the order of equally-
typed method arguments. In ISSTA, pages 232‚Äì242, 2011.
[39] M. Pradel and T. R. Gross. Fully automatic and precise detection of
thread safety violations. In PLDI, pages 521‚Äì530, 2012.
[40] M. Pradel, P. Schuh, G. Necula, and K. Sen. EventBreak: Analyzing
the responsiveness of user interfaces through performance-guided test
generation. In OOPSLA, pages 33‚Äì47, 2014.
[41] M. M. Rahman and C. K. Roy. QUICKAR: Automatic Query Reformu-
lation for Concept Location Using Crowdsourced Knowledge. In ASE,
pages 220‚Äì225, 2016.
[42] S. Rao and A. Kak. Retrieval from Software Libraries for Bug
Localization: A Comparative Study of Generic and Composite Text
Models. In MSR, pages 43‚Äì52, 2011.
[43] V . Raychev, M. T. Vechev, and E. Yahav. Code completion with
statistical language models. In PLDI, page 44, 2014.
[44] J. M. Rojas, G. Fraser, and A. Arcuri. Seeding strategies in search-based
unit test generation. Softw Test Verif Reliab, 26(5):366‚Äì401, 2016.
[45] K. Sen, D. Marinov, and G. Agha. CUTE: a concolic unit testing engine
for C. In ESEC/FSE, pages 263‚Äì272. ACM, 2005.
[46] M. Shahbaz, P. McMinn, and M. Stevenson. Automated discovery
of valid test strings from the web using dynamic regular expressions
collation and natural language processing. In QSIC, pages 79‚Äì88, 2012.
[47] A. Shahbazi and J. Miller. Black-box string test case generation through
a multi-objective optimization. 42(4):361‚Äì378, 2016.
[48] S. Shamshiri, R. Just, J. M. Rojas, G. Fraser, P. McMinn, and A. Arcuri.
Do automatically generated unit tests Ô¨Ånd real faults? an empirical study
of effectiveness and challenges (T). In ASE, pages 201‚Äì211, 2015.
[49] L. Tan, D. Yuan, G. Krishna, and Y . Zhou. /*icomment: bugs or bad
comments?*/. In SOSP, pages 145‚Äì158, 2007.
[50] S. Thummalapenta, J. de Halleux, N. Tillmann, and S. Wadsworth. Dy-
Gen: Automatic generation of high-coverage tests via mining gigabytes
of dynamic traces. In TAP, pages 77‚Äì93, 2010.
[51] S. Thummalapenta, T. Xie, N. Tillmann, J. de Halleux, and W. Schulte.
MSeqGen: Object-oriented unit-test generation via mining source code.
InESEC/FSE, pages 193‚Äì202, 2009.
[52] S. Thummalapenta, T. Xie, N. Tillmann, J. de Halleux, and Z. Su.
Synthesizing method sequences for high-coverage testing. In OOPSLA,
pages 189‚Äì206, 2011.
[53] T. Xie, D. Marinov, W. Schulte, and D. Notkin. Symstra: A framework
for generating object-oriented unit tests using symbolic execution. In
TACAS, pages 365‚Äì381, 2005.
[54] X. Yang, Y . Chen, E. Eide, and J. Regehr. Finding and understanding
bugs in c compilers. In PLDI, pages 283‚Äì294, 2011.
[55] Y . Zheng, X. Zhang, and V . Ganesh. Z3-str: a z3-based string solver for
web application analysis. In ESEC/FSE, pages 114‚Äì124, 2013.
[56] H. Zhong, L. Zhang, T. Xie, and H. Mei. Inferring resource speciÔ¨Åca-
tions from natural language API documentation. In ASE, pages 307‚Äì318,
2009.
[57] Y . Zou, T. Ye, Y . Lu, J. Mylopoulos, and L. Zhang. Learning to Rank
for Question-Oriented Software Text Retrieval. ASE.
49
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. 