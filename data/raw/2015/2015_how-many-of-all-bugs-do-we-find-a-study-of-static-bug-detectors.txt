How Many of All Bugs Do We Find?
A Study of Static Bug Detectors
Andrew Habib
andrew.a.habib@gmail.com
Department of Computer Science
TU Darmstadt
GermanyMichael Pradel
michael@binaervarianz.de
Department of Computer Science
TU Darmstadt
Germany
ABSTRACT
Static bug detectors are becoming increasingly popular and are
widely used by professional software developers. While most work
on bug detectors focuses on whether they find bugs at all, and
onhowmanyfalsepositivestheyreportinadditiontolegitimate
warnings, the inverse question is often neglected: How many of all
real-world bugs do static bug detectors find? This paper addresses
this question by studying the results of applying three widely used
static bug detectors to an extended version of the Defects4J dataset
that consists of 15 Java projects with 594 known bugs. To decide
whichof thesebugs thetools detect,we usea novelmethodology
thatcombinesanautomaticanalysisofwarningsandbugswitha
manual validation of each candidate of a detected bug. The results
ofthestudyshowthat:(i)staticbugdetectorsfindanon-negligible
amountofallbugs,(ii)differenttoolsaremostlycomplementaryto
eachother,and(iii)currentbugdetectorsmissthelargemajority
ofthestudiedbugs.Adetailedanalysisofbugsmissedbythestaticdetectors shows that some bugs could have been found by variantsoftheexistingdetectors,whileothersaredomain-specificproblems
that do not match any existing bug pattern. These findings help
potentialusersofsuchtoolstoassesstheirutility,motivateandout-
line directions for future work on static bug detection, and provide
a basis for future comparisons of static bug detection with other
bug finding techniques, such as manual and automated testing.
CCS CONCEPTS
â€¢Softwareanditsengineering â†’Automatedstaticanalysis ;
Software testing and debugging ;â€¢General and reference â†’
Empirical studies ;
KEYWORDS
static bug checkers, bug finding, static analysis, Defects4J
ACM Reference Format:
AndrewHabibandMichaelPradel.2018.HowManyofAllBugsDoWeFind?
AStudyofStaticBugDetectors.In Proceedingsofthe201833rdACM/IEEE
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE â€™18, September 3â€“7, 2018, Montpellier, France
Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238213International Conference on Automated Software Engineering (ASE â€™18), Sep-
tember3â€“7,2018,Montpellier,France. ACM,NewYork,NY,USA, 12pages.
https://doi.org/10.1145/3238147.3238213
1 INTRODUCTION
Findingsoftwarebugsisanimportantbutdifficulttask.Foraverage
industry code, the number of bugs per 1,000 lines of code has been
estimated to range between 0.5 and 25 [ 21]. Even after years of
deployment,softwarestillcontainsunnoticedbugs.Forexample,
studiesoftheLinuxkernelshowthattheaveragebugremainsin
thekernelforasurprisinglylongperiodof1.5to1.8years[ 8,24].
Unfortunately,asinglebugcancauseseriousharm,evenifithas
beensubsistingforalongtimewithoutdoingso,asevidencedby
examplesofsoftwarebugsthathavecausedhugeeconomicloses
and even killed people [17, 28,46].
Giventheimportance offindingsoftwarebugs,developers rely
on several approaches to reveal programming mistakes. One ap-
proach is to identify bugs during the development process, e.g.,through pair programming or code review. Another direction is
testing, ranging from purely manual testing over semi-automated
testing, e.g., via manually written but automatically executed unit
tests, to fully automated testing, e.g., with UI-level testing tools.
Oncethesoftwareisdeployed,runtimemonitoringcanrevealso
farmissedbugs, e.g.,collectinformationabout abnormalruntime
behavior,crashes,andviolationsofsafetyproperties,e.g.,expressedthroughassertions.Finally,developersusestaticbugdetectiontools,
which check the source code or parts of it for potential bugs.
Inthispaper,wefocusonstaticbugdetectorsbecausetheyhave
becomeincreasinglypopularinrecentyearsandarenowwidely
used by major software companies. Popular tools include Googleâ€™s
Error Prone [ 1], Facebookâ€™s Infer [ 7], or SpotBugs, the successor
to the widely used FindBugs tool [ 10]. These tools are typically
designed as an analysis framework based on some form of static
analysisthatscalestocomplexprograms,e.g.,AST-basedpattern
matching or data-flow analysis. Based on the framework, the tools
contain an extensible set of checkers that each addresses a specific
bug pattern, i.e., a class of bugs that occurs across different code
bases.Typically,abugdetectorshipswithdozensorevenhundreds
ofpatterns.Themainbenefitofstaticbugdetectorscomparedto
other bug finding approaches is that they find bugs early in the
development process, possibly right after a developer introduces a
bug.Furthermore,applyingstaticbugdetectorsdoesnotimpose
any special requirements, such as the availability of tests, and can
be fully automated.
The popularity of static bug detectors and the growing set of
bug patterns covered by them raise a question: How many of all
317
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Andrew Habib and Michael Pradel
real-worldbugsdothesebugdetectorsfind? Orinotherwords,what
is the recall of static bug detectors? Answering this question is
important for several reasons. First, it is an important part of as-
sessing the current state-of-the-art in automatic bug finding. Most
reportedevaluationsofbugfindingtechniquesfocusonshowing
that a technique detects bugs and how precise it is, i.e., how many
ofallreportedwarningscorrespondtoactualbugsratherthanfalse
positives. We do not consider these questions here. In contrast,
practicallynoevaluationconsiderstheaboverecallquestion.The
reasonforthisomissionisthatthesetofâ€œallbugsâ€isunknown(oth-erwise,thebugdetectionproblemwouldhavebeensolved),makingitpracticallyimpossibletocompletelyanswerthequestion.Second,
understanding the strengths and weaknesses of existing static bug
detectors will guide future work toward relevant challenges. For
example, better understanding of which bugs are currently missed
mayenable futuretechniques tocover previouslyignored classes
of bugs. Third, studying the above question for multiple bug detec-
torsallowsustocomparetheeffectivenessofexistingtoolswith
eachother:Areexistingtoolscomplementarytoeachotherordoes
one tool subsume another one? Fourth and finally, studying the
abovequestionwillprovideanestimateofhowclosethecurrent
state-of-the-artistotheultimate,butadmittedlyunrealistic,goal
of finding all bugs.
Toaddressthequestionofhowmanyofallbugsdostaticbug
detectors find, we perform an empirical study with 594 real-world
bugsfrom15softwareprojects,whichweanalyzewiththreewidelyusedstaticbugdetectors.Thebasicideaistoruneachbugdetectoronaversionofaprogramthatcontainsaspecificbug,andtocheck
whether the bug detector finds this bug. While being conceptually
simple,realizingthisideaisnon-trivialforreal-worldbugsandbugdetectors. The main challenge is to decide whether the set of warn-
ings reported by a bug detector captures the bug in question. To
address this challenge,we present a novel methodology thatcom-
bines automatic line-level matching, based on the lines involved in
thebugfix,andamanualanalysisofthematchedlines.Themanual
analysis is crucial because a bug detector may coincidentally flag a
line as buggy due to a reason different from the actual bug, such asanunrelatedproblemorafalsepositive.Sinceourstudyfocuseson
a finite set of bugs, we cannot really answer how many of allbugs
are found. Instead, we approximate the answer by considering a
large and diverse set of bugs from various real-world projects.
Ourstudyrelatestobutsignificantlydiffersfromapreviousstudy
by Thung et al. [ 39,40]. Their work also addresses the question
of how many of all real-world bugs are found by static checkers.
Our work differs in the methodology used to answer this question:
We manually validate whether the warnings reported by a tool
correspondtoaspecificbuginthecode,insteadofcheckingwhether
the lines flagged by a tool include the faulty lines. This manual
validation leads to significantly different results than the previous
study because many warnings coincidentally match a faulty line
but are actually unrelated to the specific bug. Another difference is
thatourstudyfocusesonamorerecentandimprovedgeneration
ofstaticbugdetectors.Thungetal.â€™sstudyconsiderswhatmightbecalledthefirstgenerationofstaticbugdetectorsforJava,e.g.,PMD
and CheckStyle.While thesetools contributed significantlyto the
state-of-the-artwhentheywereinitiallypresented,ithasalsobeen
shownthat theysuffer fromsevere limitations,in particular, largenumbersoffalsepositives.Hugeadvancesinstaticbugdetection
havebeenmadesincethen.Ourstudyfocusesonanovelgeneration
ofstaticbugdetectors,includingtoolsthathavebeenadoptedby
major industry players and that are in wide use.
The main findings of our study are the following:
â€¢Thethreebugdetectorstogetherreveal27ofthe594studied
bugs (4.5%). This non-negligible number is encouraging and
shows that static bug detectors can be beneficial.
â€¢The percentage of detected among allbugs ranges between
0.84% and 3%, depending on the bug detector. This result
pointsoutasignificantpotentialforimprovement,e.g.,by
consideringadditionalbugpatterns.Italsoshowsthatcheck-
ers are mostly complementary to each other.
â€¢Themajorityofmissedbugsaredomain-specificproblems
notcoveredbyanyexistingbugpattern.Atthesametime,
severalbugscouldhavebeenfoundbyminorvariantsofthe
existing bug detectors.
2 METHODOLOGY
This section presents our methodology for studying which bugs
are detected by static bugdetectors. At first, we describe the bugs
(Â§ 2.1) and bug detection tools (Â§ 2.2) that we study. Then, we
present ourexperimental procedure foridentifying and validating
matchesbetween thewarnings reportedbythe bugdetectors and
the real-world bugs (Â§ 2.3). Finally, we discuss threats to validity
inÂ§ 2.4.
2.1 Real-World Bugs
Ourstudybuilds onanextendedversionof theDefects4Jdataset,
a collection of bugs from popular Java projects. In total, the data
setconsistsof597bugsthataregatheredfromdifferentversions
of 15 projects. We use Defects4J for this study for three reasons.First, it provides a representative set of real-world bugs that has
beengatheredindependentlyofourwork.Thebugscoverawide
spectrumof applicationdomains andhavebeen sampledin away
thatdoesnotbiasthedatasetinanyrelevantway.Second,thedata
set is widely used for other bug-related studies, e.g., on test gen-
eration [38], mutationtesting [ 13], faultlocalization [ 26], andbug
repair[20],showingthatishasbeenacceptedasarepresentative
set of bugs. Third, Defects4J provides not only bugs but also the
corresponding bug fixes, as applied by the actual developers. Each
bug is associated with two versions of the project that containsthe bug: a buggy version, just before applying the bug fix, and a
fixed version, just after applying the bug fix. The bug fixes have
been isolated by removing any irrelevant code changes, such asnew features or refactorings. As a result, each bug is associated
with one or more Java classes, i.e., source code files that have been
modified to fix the bug. The availability of fixes is important not
onlytovalidatethatthedevelopersconsideredabugasrelevant,
but also to understand its root cause.
The current official version of Defects4J (version 1.1.0) consists
of395bugscollectedfrom6Javaprojects.Arecentadditiontothese
bugs extends the official release with 202 additional bugs from 9
additionalprojects.1Inourwork,weusetheextendedversionof
the dataset andrefer toit as â€œDefects4Jâ€. Table 1liststhe projects
1https://github.com/rjust/defects4j/pull/112
318
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. How Many of All Bugs Do We Find? ASE â€™18, September 3â€“7, 2018, Montpellier, France
Table 1: Projects and bugs of Defects4J.
ID Project Bugs
Original Defects4J:
Chart JFreeChart 26
Closure Google Closure 133
Lang Apache commons-lang 64 (65)Math Apache commons-math 106Mockito Mockito framwork 38Time Joda-Time 27
Total of 6 projects 394(395)
Extended Defects4J:
Codec Apache commons-codec 21 (22)Cli Apache commons-cli 24Csv Apache commons-csv 12JXPath Apache commons-JXPath 14Guava Guava library 9JCore Jackson core module 13
JDatabind Jackson data binding 39
JXml Jackson XML utilities 5Jsoup Jsoup HTML parser 63 (64)
Total of 9 projects 200(202)
Total of 15 projects 594(597)
and bugs in the data set.2We exclude three of the 597 bugs for
technicalreasons:Lang-48becauseErrorPronedoesnotsupport
Java 1.3, and Codec-5 and Jsoup-4 because they introduce a new
classinthebugfix,whichdoesnotmatchourmethodologythat
relies on analyzing changes to existing files.
2.2 Static Bug Detectors
We study three static bug detectors for Java: (i) Error Prone [ 1],
atooldevelopedbyGoogleandisintegratedintotheirTricorder
static analysis ecosystem [ 36]; (ii) Infer [ 7], a tool developed and
usedinternallybyFacebook;and(iii)SpotBugs,thesuccessorofthepioneeringFindBugs[
10]tool.Thesetoolsareusedbyprofessional
softwaredevelopers.Forexample,ErrorProneandInferareauto-maticallyappliedtocodechangestosupportmanualcodereview
atGoogleandFacebook,respectively.Allthreetoolsareavailable
as open-source. We use the tools with their default configuration.
2.3 Experimental Procedure
Givenasetofbugsandasetofstaticbugdetectors,theoverallgoal
of the methodology is to identify those bugs among the set Bof
provided bugs that are detected by the given tools. We representadetected bug as a tuple
(b,w), where bâˆˆBis a bug and wis
a warning that points to the buggy code. A single bug bmay be
detectedbymultiplewarnings,e.g., (b,w1)and(b,w2),andasingle
warning may point to multiple bugs, e.g., (b1,w)and(b2,w).
A naive approach to assess whether a tool finds a particular bug
would be to apply the tool to the buggy version of the code and to
2We refer to bugs using the notation ProjectID-N, where N is a unique number.  	


	
		
	

	
	 		 	
		
	

	

		



	
Figure 1: Overview of our methodology.
manually inspect each reported warning. Unfortunately, static bug
detectors may produce many warnings and manually inspecting
eachwarning foreach buggyversion ofa programdoes notscale
tothenumberofbugsweconsider.Anotherpossibleapproachistofullyautomaticallymatchwarningsandbugs,e.g.,byassumingthateverywarningatalineinvolvedinabugfixpointstotherespective
bug. While this approach solves the scalability problem, it risks to
overapproximatethenumberofdetectedbugs.Thereasonisthat
some warnings may coincidentally match a code location involved
in a bug, but nevertheless do not point to the actual bug.
Ourapproachtoidentifydetectedbugsisacombinationofau-
tomatic and manual analysis, which reduces the manual effortcompared to inspecting all warnings while avoiding the overap-proximation problem of a fully automatic matching. To identify
thedetectedbugs,weproceedintwomainsteps,assummarized
inFigure 1. The first step automatically identifies candidates for
detected bugs, i.e., pairs of bugs and warnings that are likely to
matcheachother.Weapplythreevariantsofthemethodologythat
differ in how to identify such candidates:
â€¢anapproachbasedondifferencesbetweenthecodebefore
and after fixing the bug,
â€¢anapproachbasedonwarningsreportedforthecodebefore
and after fixing the bug, and
â€¢the combination of the two previous approaches.
The second step is to manually inspect all candidates to decide
which bugs are indeed found by the bug detectors. This step is
importanttoavoidcountingcoincidentalmatchesasdetectedbugs.
2.3.1 Identifying Candidates for Detected Bugs.
CommonDefinitions. Weexplainsometermsandassumptions
used throughout this section. Given a bug b, we are interested
in the set Lbofchanged lines, i.e., lines that were changed when
fixing the bug. We assume that these lines are the locations where
developers expect a static bug detector to report a warning. In
principle,thisassumptionmaynotholdbecausethebuglocation
and the fix location may differ. We further discuss this potential
threat to validityin Â§ 2.4. We computethe changed lines based on
the differences, or short, the diff, between the code just before and
justafterapplyingthebugfix.Thediffmayinvolvemultiplesource
code files. We compute the changed lines as lines that are modified
319
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Andrew Habib and Michael Pradel
or deleted, as these are supposed to directly correspond to the bug.
Inaddition,weconsideraconfigurablewindowoflinesaroundthe
location of newly added lines. As a default value, we use a window
size of [-1,1].
Applyingabugdetectortoaprogramyieldsasetofwarnings.
We refer to the sets of warnings for the program just before and
just after fixing a bug basWbefore(b)andWafter(b), or simply
WbeforeandWafterifthebugisclearfromthecontext.Thebug
detectorsweusecananalyzeentireJavaprojects.Sincethepurpose
ofourstudyistodeterminewhetherspecificbugsarefound,we
apply the analyzers only to the files involved in the bug fix, i.e.,
files that contain at least one changed line lâˆˆLb. We also provide
eachbugdetectorthefullcompilepathalongwithallthird-party
dependencies of each buggy or fixed program so that inter-project
and third-party dependencies are resolved. The warnings reported
when applying a bug detector to a file are typically associated with
specific line numbers. We refer to the lines that are flagged by a
warning waslines(w).
Diff-basedMethodology. Oneapproachtocomputeasetofcandi-
datesfordetectedbugsistorelyonthediffbetweenthebuggyand
thefixedversionsoftheprogram.Theintuitionisthatarelevant
warning should pinpoint one of the lines changed to fix the bug.
Inthisapproach,weperformthefollowingforeachbugandbug
detector:
(1)Computethelinesthatareflaggedwithatleastonewarning
in the code just before the bug fix:
LwarninÐ´s =/uniondisplay.1
wâˆˆWbef orelines(w)
(2)Computethecandidatesofdetectedbugsasallpairsofabug
andawarningwherethechangedlinesofthebugoverlap
with the lines that have a warning:
Bdiff
cand={(b,w)|Lbâˆ©LwarninÐ´s /nequalâˆ…}
Forexample,thebugin Figure2a isacandidateforabugdetected
bySpotBugsbecausethetoolflaggedline55,whichisalsointhe
set of changed lines.
FixedWarnings-basedMethodology. As analternative approach
foridentifyingasetofcandidatesfordetectedbugs,wecompare
thewarningsreportedforthecodejustbeforeandjustafterfixingabug.Theintuitionisthatawarningcausedbyaspecificbugshould
disappearwhenfixingthebug.Inthisapproach,weperformthe
following for each bug and bug detector:
(1)Compute the set of fixed warnings, i.e., warnings that disap-
pear after applying the bug fix:
Wfixed =Wbefore\Wafter
(2)Compute the candidates for detected bugs as all pairs of a
bugandawarningwherethewarningbelongstothefixed
warnings set:
Bfixed
cand={(b,w)|wâˆˆWfixed}
Inthis step, wedo notmatch warningmessages based online
numbers because line numbers may not match across the buggy
and fixed files due to added and deleted code. Instead, we comparethemessagesbasedonthewarningtype,category,severity,rank,
and code entity, e.g., class, method, and field.
For example, Figure 2c shows a bug that the fixed warnings-
based approach finds as a candidate for a detected bug by Error
Pronebecausethewarningmessagereportedatline175disappears
in the fixed version. In contrast, the approach misses the candidate
buginFigure2a becausethedeveloperre-introducedthesamekind
of bug inline 62, andhence,the same warning isreported in the
fixed code.
CombinedMethodology. Thediff-basedapproachandthefixed
warnings-based approach may yield different candidates for de-
tected bugs. For instance, both approaches identify the bugs in Fig-
ure 2candFigure 2d as candidates, whereas only the diff-based
approach identifies the bugs in Figure 2a andFigure 2b. Therefore,
weconsiderasathirdvariantofourmethodologythecombination
of the fixed warnings and the diff-based approach:
Bcombine
cand=Bdiff
candâˆªBfixed
cand
Unless otherwise mentioned, the combined methodology is the
default in the remainder of the paper.
2.3.2 Manual Inspection and Classification of Candidates. The
automatically identified candidates for detected bugs may contain
coincidentalmatchesof abugandwarning. Forexample,suppose
that a bug detector warns about a potential null dreference at a
specificlineandthatthislinegetsmodifiedaspartofabugfix.If
the fixed bug is completely unrelated to dereferncing a null object,
thenthewarning wouldnothavehelpedadeveloperinspotting
the bug.
Toremovesuchcoincidentalmatches,wemanuallyinspectall
candidatesfordetectedbugsandcomparethewarningmessages
against the buggy and fixed versions of the code. We classify each
candidateintooneofthreecategories:(i)Ifthewarningmatches
thefixedbugandthefixmodifieslinesthataffecttheflaggedbug
only, then this is a full match. (ii) If the fix targets the warning but
also changes other lines of code not relevant tothe warning, then
it is apartial match. (iii) If the fix does not relate to the warning
message at all, then it is a mismatch.
Forexample,thebugin Figure2d isclassifiedasafullmatchsince
thebugfixexactlymatchesthewarningmessage:topreventa Null-
PointerException on the value returned by ownerDocument() ,a
checkfornullnesshasbeenaddedinthehelpermethod getOutput-
Settings() ,whichcreatesanempty Document("") objectwhen
ownerDocument() returns null.
As an example of a partial match, consider the bug in Figure 2a.
As we discussed earlier in Â§ 2.3.1, the developer attempted a fix by
applyingpropercheckandcastinlines58-63ofthefixedversion.
Weconsiderthiscandidatebugapartialmatchbecausethefixed
version also modifies line 60 in the buggy file by changing the
returnvalueofthemethod hashCode() .Thischangeisnotrelated
to the warning reported by SpotBugs. It is worth noting that the
factthatthedeveloperunfortunatelyre-introducedthesamebug
in line 62 of the fixed version does not contribute to the partial
matching decision.
Finally,thebugin Figure2b isanexampleofamismatchbecause
the warning reported by Error Prone is not related to the bug fix.
320
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. How Many of All Bugs Do We Find? ASE â€™18, September 3â€“7, 2018, Montpellier, France
Buggy code:
53@Override
54public boolean equals(Object o) {
55returnmethod.equals(o);
56}
57
58@Override
59public int hashCode() {
60return1;
61}Bug fixâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’Fixed code:
53@Override
54public boolean equals(Object o) {
55if(this== o) {
56return true;
57}
58if(oinstanceof DelegatingMethod) {
59DelegatingMethod that = (DelegatingMethod) o;
60returnmethod.equals(that.method);
61}else{
62returnmethod.equals(o);
63}
64}
6566
@Override
67public int hashCode() {
68returnmethod.hashCode();
69}
(a) Bug Mockito-11. Warning by SpotBugs at line 55: Equality check for operand not compatible with this.Lb={ 54, 55, 56, 57, 58, 59,
60, 61, 62, 63, 64, 67, 68, 69 }. Found by diff-based methodology. Classification: Partial match.
Buggy code:
1602publicDfp multiply(final int x) {
1603returnmultiplyFast(x);
1604}Bug fixâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’Fixed code:
1602publicDfp multiply(final int x) {
1603if(x >= 0 && x < RADIX) {
1604 returnmultiplyFast(x);
1605}else{
1606 returnmultiply(newInstance(x));
1607}
1608}
(b) Bug Math-17. Warning by Error Prone at line 1602: Missing @Override. Lb={ 1602, 1603, 1604, 1605, 1606, 1607, 1608 }. Found by
diff-based methodology. Classification: Mismatch.
Buggy code:
173publicWeek(Date time, TimeZone zone) {
174// defer argument checking...
175this(time, RegularTimePeriod.DEFAULT_TIME_ZONE,
Locale.getDefault());
176}Bug fixâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’Fixed code:
173publicWeek(Date time, TimeZone zone) {
174// defer argument checking...
175this(time, zone, Locale.getDefault());
176}
(c)BugChart-8.WarningbyErrorProneatline175:Chainingconstructorignoresparameter. Lb={175}.Foundby:Diff-basedmethod-
ology and fixed warnings-based methodology. Classification: Full match.
Buggy code:
214publicDocument ownerDocument() {
215if(this instanceof Document)
216return(Document) this;
217else if (parentNode == null)
218return null;
219else
220returnparentNode.ownerDocument();
221}
...
362protected void outerHtml(StringBuilder accum) {
363 newNodeTraversor(new
OuterHtmlVisitor(accum, ownerDocument()
.outputSettings()))
.traverse(this);
364}Bug fixâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’Fixed code:
362protected void outerHtml(StringBuilder accum) {
363newNodeTraversor(new OuterHtmlVisitor(accum,
getOutputSettings())).traverse(this);
364}
365
366// if this node has no document (or parent),
retrieve the default output settings
367private Document.OutputSettings
getOutputSettings() {
368returnownerDocument() != null?
ownerDocument().outputSettings() : (new
Document("")).outputSettings();
369}
(d)BugJsoup-59.WarningbyInferatline363: nulldereference. Lb={363,364,365,366,367,368,369}.Foundby:Diff-basedmethod-
ology and fixed warnings-based methodology. Classification: Full match.
Figure 2: Candidates for detected bugs and their manual classification.
321
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Andrew Habib and Michael Pradel
2.3.3 Error Rate. Beyond the question of how many of all bugs
are detected, we also consider the error rate of a bug detector. Intu-
itively, it indicates how many warnings the bug detector reports.
We compute the error rate by normalizing the number of reported
warnings to the number of analyzed lines of code:
ER=/summationtext.1
bâˆˆB|Wbefore(b)|
/summationtext.1
bâˆˆB/summationtext.1
fâˆˆfiles(b)LoC(f)
wherefiles(b)arethefilesinvolvedinfixingbug bandLoC(f)yields
the number of lines of code of a file.
2.4 Threats to Validity
Asforallempiricalstudies,therearesomethreatstothevalidity
of the conclusions drawn from our results. One limitation is the
selection of bugs and bug detectors, both of which may or may not
be representative for a larger population. To mitigate this threat,
we use a large set of real-world bugs from a diverse set of popular
open-source projects. Moreover, the bugs have been gathered inde-
pendently of our work and have been used in previous bug-related
studies[13,20,26,38].Forthebugdetectors,westudytoolsthatare
widely used in industry and which we believe to be representative
for the current state-of-the-art. Despite these efforts, we cannot
claim that our results generalize beyond the studied artifacts.
Anotherthreattovalidityisthatourmethodologyforidentify-
ingdetectedbugscould,inprinciple,bothmisssomedetectedbugsandmisclassifycoincidentalmatchesasdetectedbugs.Areasonfor
potentially missing detected bugs is our assumption that the lines
involved in a bug fix correspond to the lines where a developer
expects a warning to be placed. In principle, a warning reported at
someotherlinemighthelpadevelopertofindthebug,e.g.,because
the warning eventually leads the developer to the buggy code loca-
tion.Sincewecouldonlyspeculateaboutsuchcausaleffects,we
instead use the described methodology. The final decision whether
a warning corresponds to a bug is taken by a human and therefore
subjective. To address this threat, both authors discussed every
candidate for a detected bug where the decision is not obvious.
A final threat to validity results from the fact that static bug
detectors may have been used during the development processof the studied projects. If some of the developers of the studied
projectsusestaticbugdetectorsbeforecheckingintheircode,they
mayhavefoundsomebugsthatwemissinthisstudy.Asaresult,
our results should be understood as an assessment of how many
of those real-world bugs that are committed to the version control
systems can be detected by static bug detectors.
3 EXPERIMENTAL RESULTS
Thissectionpresentstheresultsofapplyingourmethodologyto
594 bugs and three bug detectors. We start by describing some
properties of the studied bugs (Â§ 3.1) and the warnings reported
by thebug detectors(Â§ 3.2).Next, we reporton thecandidates for
detected bugs (Â§ 3.3) and how many of them could be manually
validatedandtheirkinds(Â§3.4),followedbyacomparisonofthe
studied bug detectors (Â§ 3.5). To better understand the weaknesses
of current bug detectors, Â§ 3.6discusses why the detectors miss 0 50 100 150 200 250 300 350 400 450 500 550
1234567 1 1Number of bugs
Number of buggy files501
64
12 10 4 111
(a) Number of buggy files. 0 50100150200250300350400450500550
1-45-910-1415-1920-2425-4950-7475-99100-199200-1,999
Diff size between buggy and fixed versions (LoC)296
128
5429 2944
66 11
(b) Total size of diffs between
buggy and fixed files.
Figure 3: Properties of the studied bugs.
Table 2: Warnings generated by each tool. The minimum,
maximum, and average numbers of warnings are per bug
and consider all files involved in the bug fix.
Warnings
Per bug
Tool Min Max Avg Total Error rate
Error Prone 0 148 7.58 4,402 0.01225Infer 0 36 0.33 198 0.00055SpotBugs 0 47 1.1 647 0.0018
Total 5,247
manybugs.Finally, Â§3.7empiricallycomparesthethreevariants
of our methodology.
3.1 Properties of the Studied Bugs
To better understand the setup of our study, we measure several
propertiesofthe594studiedbugs. Figure3a showshowmanyfiles
are involved in fixing a bug. For around 85% of the bugs, the fixinvolves changing a single source code file. Figure 3b shows the
numberoflinesofcodeinthediffbetweenthebuggyandthefixed
versions. This measure gives an idea of how complex the bugs and
their fixes are. The results show that most bugs involve a small
numberoflines:For424bugs,thediffsizeisbetweenoneandnine
lines of code. Two bugs have been fixed by modifying, deleting, or
inserting more than 100 lines.
3.2 Warnings Reported by the Bug Detectors
Thefirststepinourmethodologyisrunningeachtoolonallfiles
involvedineachofthebugs. Table2showstheminimum,maximum,
and average number of warnings per bug, i.e., in the files involved
infixingthebug,thetotalnumberofwarningsreportedbyeach
tool, and the error rate as defined in Â§ 2.3.3. We find that Error
Pronereportsthehighestnumberofwarnings,withamaximumof
148 warnings and an average of 7.58 warnings per bug. This is also
reflected by an error rate of 0.01225.
The studied bug detectors label each warning with a description
of the potential bug. Table 3shows the top 5 kinds of warnings
reported by each tool. The most frequent kind of warning by Error
Prone is about missing the @Override annotation when a method
322
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. How Many of All Bugs Do We Find? ASE â€™18, September 3â€“7, 2018, Montpellier, France
Table 3: Top 5 warnings reported by each static checker.
Warning Count
Error Prone
Missing@Override 3211
Comparison using reference equality 398
Boxed primitive constructor 234Operator precedence 164
Type parameter unused in formals 64
Infer
nulldereference 90
Thread safety violation 43
Unsafe@GuardedBy access 30
Resource leak 29
Method with mutable return type re-
turns immutable collection1
SpotBugs
switchwithoutdefault 109
Inefficient Numberconstructor 79
Read of unwritten field 45
Method naming convention 37
Reference to mutable object 31
overrides amethodwith thesame signaturein itsparent class.In-
ferâ€™s most reported kind of warning complains about a potentialnull dereference. Finally, the most frequent kind of warning by
SpotBugs is related to missing the default case in aswitchstate-
ment.Thequestionhowmanyofthesewarningspointtoavalid
problem (i.e., true positives) is outside of the scope of this paper.
3.3 Candidates for Detected Bugs
Given the number of reported warnings, which totals to 5,247 (Ta-
ble2),itwouldbeverytime-consumingtomanuallyinspecteach
warning.Theautomatedfilteringofcandidatesfordetectedbugs
yields a total of 153 warnings and 89 candidates (Table 4), which
significantlyreducesthenumberofwarningsandbugstoinspect.
Compared to all reported warnings, the selection of candidates
reduces the number of warnings by 97%.
Thenumberofwarningsisgreaterthanthenumberofcandidates
because we count warnings and candidates obtained from all tools
together and each tool could produce multiple warnings per line(s).
3.4 Validated Detected Bugs
To validate the candidates for detected bugs, we inspect each of
themmanually.Basedontheinspection,weclassifyeachcandidateasafullmatch,apartialmatch,oramismatch,asdescribedin Â§2.3.2.
Overall, the three tools found 31 bugs, as detailed in the tableinFigure 4. After removing duplicates, i.e., bugs found by more
than one tool, there are 27 unique validated detected bugs.
We draw two conclusions from these results. First, the fact that
27 unique bugs are detected by the three studied bug detectors
showsthatthesetoolswouldhavehadanon-negligibleimpact,if
they would have been used during the development of the studiedTool Bugs
Error Prone 8Infer 5SpotBugs 18
Total: 31
Total of 27unique bugs
	


Figure 4: Total number of bugs found by all three staticcheckers and their overlap.
programs. This result is encouraging for future work on static bug
detectors and explains why several static bug detection tools have
beenadoptedinindustry.Second,evenwhencountingbothpartial
and full matches, the overall bug detection rate of all three bug de-
tectors together is only 4.5%. While reaching a detection anywherecloseto100%iscertainlyunrealistic,e.g.,becausesomebugsrequireadeepunderstandingofthespecificapplicationdomain,webelieve
that the current state-of-the-art leaves room for improvement.
Togetanideaofthekindsofbugsthecheckersfind,wedescribe
the most common patterns that contribute to finding bugs. Outof the eight bugs found by Error Prone, three are due to missingan
@Override annotation, and two bugs because the execution
may fall through a switchstatement. For the five bugs found by
Infer,fourbugsarepotential nulldeferences.Outofthe18bugs
detected by SpotBugs, three are discovered by pointing to dead
local stores (i.e., unnecessarily computed values), and two bugs are
potential nulldeferences.Finally,thetwobugsfoundbybothInfer
and SpotBugs are nulldeferences, whereas the two bugs found by
both Error Prone and SpotBugs are a string format error and an
execution that may fall through a switchstatement.
3.5 Comparison of Bug Detectors
The right-hand side of Figure 4 shows to what extent the bug
detectors complement each other. SpotBugs finds most of the bugs,
18ofall27,ofwhich14arefoundonlybySpotBugs.ErrorProne
finds 6 bugs that are not found by any other tool, and Infer finds 3
bugs missed by the other tools. We conclude that the studied tools
complementeachothertoalargeextent,suggestingthatdevelopers
may want to combine multiple tools and that researchers couldaddress the problem of how to reconcile warnings reported by
different tools.
3.6 Reasons for Missed Bugs
Tobetterunderstandwhythevastmajorityofbugsarenotdetected
bythe studiedbugdetectors, wemanuallyinspect andcategorize
some of the missed bugs. We inspect a random sample of 20 ofall bugs that are not detected by any bug detector. For each sam-pled bug, we try to understand the root cause of the problem by
inspectingthediffandbysearchingforanyissuereportsassociated
with the bug. Next, we carefully search the list of bug patterns
supported by the bug detectors to determine whether any of thedetectors could have matched the bug. If there is a bug detectorthat relates to the bug, e.g., by addressing a similar bug pattern,
then we experiment with variants of the buggy code to understand
323
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Andrew Habib and Michael Pradel
why the detector has not triggered an alarm. Based on this process,
we have the two interesting findings.
3.6.1 Domain-specific Bugs. First, the majority of the missed
bugs (14 out of 20) are domain-specific problems not related to any
of the patterns supported by the bug checkers. The root causes
of these bugs are mistakes in the implementation of application-
specificalgorithms,typicallybecausethedeveloperforgottohandle
a specific case. Moreover, these bugs manifest in ways that are dif-
ficulttoidentifyasunintendedwithoutdomainknowledge,e.g.,by
causing an incorrect string to be printed or an incorrect number
to be computed. For example, Math-67 is a bug in the implementa-
tion of a mathematical optimization algorithm that returns the last
computed candidate value instead of the best value found so far.
AnotherexampleisClosure-110,abuginaJavaScriptcompilerthat
failstoproperlyhandlesomekindsoffunctiondeclarations.Finally,
Time-14 is due to code that handles dates but forgot to consider
leap years and the consequences of February 29.
3.6.2 Near Misses. Second, some of the bugs (6 out of 20) could
be detected with a more powerful variant of an existing bug detec-
tor. We distinguish two subcategories of these bugs. On the one
hand,therootcausesofsomebugsareproblemstargetedbyatleast
one existing bug detector, but the current implementation of the
detectormissesthebug.Thesebugsmanifestthroughabehavior
that istypicallyconsidered unintended,such asinfinite recursion
orout-of-bounds arrayaccesses.For example,Commons-Csv-7is
causedbyaccessinganout-of-boundsindexofanarray,whichis
oneofthebugpatternssearchedforbySpotBugs.Unfortunately,
theSpotBugscheckerisintra-procedural,whiletheactualbugcom-
putes the array index in one method and then accesses the array
inanothermethod.AnotherexampleisLang-49,whichcausesan
infinite loop because multiple methods call each other recursively,
and the conditions for stopping the recursion miss a specific input.
Both Error Prone and SpotBugs have checkers for infinite loops
caused by missing conditions that would stop recursion. However,
these checkers target cases that are easier to identify than Lang-49,
which would require inter-procedural reasoning about integer val-
ues.AthirdexampleinthissubcategoryisChart-5,whichcausesan
IndexOutOfBoundsException when calling ArrayList.add . The
existingcheckerforout-of-boundsaccessestoarraysmighthave
caught this bug, but it does not consider ArrayLists.
Ontheotherhand,therootcausesofsomebugsareproblems
that are similar to but not the same as problems targeted by an ex-
isting checker. For example, Commons-Codec-8 is about forgetting
to override some methods of the JDK class FilterInputStream .
While SpotBugs and Error Prone have checkers related to streams,
including some that warn about missing overrides, none of the
existing checkers targets the methods relevant in this bug.
3.6.3 Implications for Future Work. We draw several conclu-
sions from our inspection of missed bugs. The first and perhaps
most important is that there is a huge need for bug detection tech-
niques that can detect domain-specific problems. Most of the ex-isting checkers focus on generic bug patterns that occur across
projectsandoftenevenacrossdomains.However,asmostofthe
missed bugsare domain-specific,future workshould complement
theexistingdetectorswithtechniquesbeyondcheckinggenericbugTable4:Candidatewarnings(W)andbugs(B)obtainedfrom
the automatic matching.
Approach
Diff-based Fixed warnings Combined
Tool WB WB WB
Error Prone 51 33 18 14 53 35
Infer 30 9 14 6 32 11
SpotBugs 51 32 29 22 68 43
Total: 132 74 61 42 153 89
patterns. One promising direction could be to consider informal
specifications, such as natural language information embedded in
code or available in addition to code.
Wealsoconcludethatfurtherworkonsophisticatedyetpractical
staticanalysisisrequired.Giventhatseveralcurrentlymissedbugscouldhavebeenfoundbyinter-proceduralvariantsofexistingintra-proceduralanalysessuggestsroomforimprovement.Thechallenge
here is to balance precision and recall: Because switching to inter-
proceduralanalysisneedstoapproximate,e.g.,callrelationships,
thisstepriskstocauseadditionalfalsepositives.Anotherpromising
direction suggested by our results is to generalize bug detectors
that havebeen developed fora specific kind ofproblem to related
problems, e.g., ArrayLists versus arrays.
Finally, our findings suggest that some bugs are probably easier
todetectwith techniquesotherthanstaticcheckers. Forexample,
themissedbugsthatmanifestthroughclearsignsofmisbehavior,suchasaninfiniteloop,aregoodcandidatesforfuzz-testingwith
automated test generators.
3.7 Assessment of Methodologies
We compare the three variants of our methodology and validate
thatthemanualinspectionofcandidatesofdetectedbugsiscrucial.
3.7.1 Candidates of Detected Bugs. Ourmethodology foriden-
tifying candidates for detected bugs has three variants (Â§ 2.3.1).
Table 4compares them by showing for each variant how many
warnings and bugs it identifies as candidates. The number of warn-
ingsislargerthanthenumberofbugsbecausethelinesinvolved
in a single bug may match multiple warnings. Overall, identify-ingcandidatesbasedondiffsyieldsmanymorewarnings,132in
total, than by considering which warnings are fixed by a bug fix,
which yields 61 warnings. Combining the two methodologies by
consideringtheunionofcandidatesgivesatotalof153warnings
correspondingto89bugs.Sincemorethanonestaticcheckercould
pointtothesamebug,thetotalnumberofuniquecandidatesfor
detected bugs by all tools together boils down to 79 bugs.
Figure5visualizeshowthevariantsofthemethodologycomple-
ment each other. For example, for Error Prone, the fixed warnings-
basedapproachfinds14candidates,2ofwhichareonlyfoundby
this approach. The diff-based technique finds 21 candidates not
found by the fixed warnings approach. Overall, the diff-based and
the fixed warnings-based approaches are at least partially comple-
mentary, making it worthwhile to study and compare both.
324
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. How Many of All Bugs Do We Find? ASE â€™18, September 3â€“7, 2018, Montpellier, France
	
   

Figure5:Candidatedetectedbugsusingthetwodifferentau-
tomatic matching techniques.
3.7.2 Validated Detected Bugs. Figure6showshowmanyofthe
candidates obtained with the diff-based and the fixed warnings-
based approach we could validate during the manual inspection.
Theleftchartof Figure6a showstheresultsofmanuallyinspecting
each warning matched by the diff-based approach. For instance,
out of the 51 matched warnings reported by Error Prone, 6 are full
matchesand2arepartialmatches,whereastheremaining43donot
correspond to any of the studied bugs. The right chart in Figure 6a
shows how many of the candidate bugs are actually detected by
the reported warnings. For example, out of 9 bugs that are possiblydetectedbyInfer,wehavevalidated3. Figure6b andFigure6c show
the same charts for the fixed warnings-based approach and the
combined approach.
Thecomparisonshowsthatthediff-basedapproachyieldsmany
more mismatches than the fixed warnings-based approach. Given
this result, one may wonder whether searching for candidates only
based on fixed warnings would yield all detected bugs. In Figure 7,
we see for each bug detector, how many unique bugs are found
by the two automaticmatching approaches. For both Error ProneandInfer,althoughthediff-basedapproachyieldsalargenumber
of candidates, the fixed warnings-based methodology is sufficient
toidentifyalldetectedbugs.ForSpotBugs,though,onedetected
bug would be missed when inspecting only the warnings that have
been fixed when fixing the bug. The reason is bug Mockito-11
inFigure 2a. The fixed warnings-based methodology misses this
bugbecausethebugfixaccidentallyre-introducesanotherwarning
of the same kind, at line 62 of the fixed code.
In summary, we find that the fixed warnings-based approach
requireslessmanualeffortwhilerevealingalmostalldetectedbugs.
This result suggests that future work could focus on the fixed
warnings-based methodology, allowing such work to manually
inspect even more warnings than we did.
3.7.3 Manual Inspection. Table4showsthatthecombinedap-
proach yields 153 candidate warnings corresponding to 89 (79
unique) bugs. However, the manual validation reveals that only 34
ofthosewarningsandacorrespondingnumberof31(27unique)
bugs correspond to actual bugs, whereas the remaining matches
are coincidental. Out of the 34 validated candidates, 22 are fullmatches and 12 are partial matches (Figure 6c). In other words,
78%ofthecandidatewarningsand66%ofthecandidatebugsare
spuriousmatches,i.e.,thewarningisaboutsomethingunrelatedto
the specific bug and only happens to be on the same line.
These results confirm that the manual step in our methodology
is important to remove coincidental matches. Omitting the manual
inspectionwouldskewtheresultsandmisleadthereadertobelievethatmorebugsaredetected.Thisskewingofresultswouldbeeven
stronger for bug detectors that report more warnings per line of
code, as evidenced in an earlier study [39].
To ease reproducibility and to enable others to build on our results,
full details of all results are available online.3
4 RELATED WORK
Studies of Static Bug Detectors. Most existing studies of static
bug detectors focus on precision, i.e., how many of all warnings
reported by a tool point to actual bugs [ 34,41,45]. In contrast, our
study asks the opposite question: What is the recall of static bug
detectors,i.e.,howmanyofall(known)bugsarefound?Another
differencetoexistingstudiesisourchoiceofstaticbugdetectors:
To the best of our knowledge, this is the first paper to study the
effectiveness of Error Prone, Infer, and SpotBugs.
Theperhapsmostrelatedexistingworkisastudy byThunget
al.[39,40]thatalsofocusesontherecallofstaticbugdetectors.Our
work differs in the methodology, because we manually validate
eachdetectedbug,andintheselectionofbugsandbugdetectors,
becausewefocusonmorerecent,industriallyusedtools.Asaresultofourimprovedmethodology,ourresultsdiffersignificantly:While
theyconcludethatbetween64%and99%ofallbugsarepartiallyor fully detected, we find that only 4.5% of all studied bugs are
found.Themainreasonforthisdifferenceisthatsome ofthebug
detectorsusedbyThung etal.reportalargenumber ofwarnings.
Forexample,asingletoolalonereportsover39,000warningsfor
the Lucene benchmark (265,821 LoC), causing many lines to beflagged with at least one warning with error rate 0.15 (Â§ 2.3.3).
Since their methodologyfully automaticallymatches sourcecode
linesandlineswithwarnings,mostbugsappeartobefound.Instead,
we manually check whether a warning indeed corresponds to a
particular bug to remove false matches.
To facilitate evaluating bug detection techniques, several bench-
marksofbugshavebeenproposed.BugBench[ 18]consistsof17
bugsinCprograms.Cifuentesetal.significantlyextendthisbench-
mark,resulting in181 bugsthat aresampledfrom fourcategories,
e.g.,bufferoverflows.Theyusethebenchmarktocompare4bug
detectorsusinganautomatic,line-basedmatchingtomeasurerecall.
Future work could apply our semi-manual methodology to theirbug collection to study whether our results generalize to C pro-
grams.Rahmanetal.comparethebenefitsofstaticbugdetectors
andstatisticalbugprediction[ 31].Toevaluatewhetheranapproach
wouldhavedetectedaparticularbug,theirstudycomparesthelines
flagged with warnings and the lines changed to fix a bug, which
roughly corresponds to the first step of our methodology and lacks
amanualvalidationwhetherawarningindeedpointstothebug.
Johnson et al. conducted interviews with developers to understand
whystaticbugdetectorsare(not)used[ 11].Thestudysuggeststhat
betterwaysofpresentingwarningstodevelopersandintegrating
bugdetectorsintothedevelopmentworkflowwouldincreasethe
usage of these tools.
Studies of Other Bug Detection Techniques. Other studies con-
sider bug detection techniques beyond static bug detectors, testgeneration [
2,38]. One of these studies [ 38] also considers bugs
3https://github.com/sola-da/StaticBugCheckers
325
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Andrew Habib and Michael Pradel
Error Prone Infer S potBugsNumber of warnings63 52 343
274351
3051
Error Prone Infer S potBugsNumber of bugs
5 3 51 327
62433
932
(a) Diff-based approach.Error Prone Infer S potBugsNumber of warnings 74113
17 8
911
18
1429
Error Prone Infer S potBugsNumber of bugs
6 4112166
15
14
622
(b) Fixed warnings-based approach.Error Prone Infer S potBugsNumber of warnings 74113
1843
274953
3268
Error Prone Infer S potBugsNumber of bugsFull match
Partial match
Mismatch
6 41121727
62535
1143
(c) Combined approach.
Figure 6: Manual inspection of candidate warnings and bugs from the two automatic matching approaches.
	
   

Figure 7: Actual bugs found using the two different auto-
matic matching techniques.
in Defects4J and finds that most test generators detect less than
20% of these bugs. Another study focuses on manual bug detection
and compares the effectiveness of code reading, functional testing,
andstructuraltesting[ 44].Finally,Legunsenetal.studytowhat
extent checking API specifications via runtime monitoring reveals
bugs. All these studies are complementary to ours, as we focus on
static bug detectors. Future work could study how well different
bug finding techniques complement each other.
StudiesofBugsandBugFixes. Animportantsteptowardimprov-
ing bug detection is to understand real-world bugs. To this end,
studieshaveconsideredseveralkindsofbugs,includingbugsinthe
Linuxkernel[ 8],concurrencybugs[ 19],andcorrectness[ 23]and
performance bugs [ 37] in JavaScript. Pan et al. study bug fixes and
identify recurring, syntactical patterns [ 25]. Work by Ray et al. re-
ports that statistical language models trained on source code show
buggycodetohavehigherentropythanfixedcode[ 32],whichcan
help static bug detectors to prioritize their warnings.
Static Bug Detectors and Real-World Deployments. The lint
tool [12], originally presented in 1978, is one of the pioneers on
staticbugdetection.Sincethen,staticbugdetectionhasreceived
significant attention by researchers, including work on finding
APImisuses[ 22,30,43],name-basedbugdetection[ 29],security
bugs [6], finding violations of inferred programmer beliefs [ 9] and
otherkindsofanomalydetection[ 16],bugdetectionbasedonstatis-
ticallanguagemodels[ 42],andondetectingperformancebugs[ 27].
Severalstaticbugdetectionapproacheshavebeenadoptedbyin-
dustry. Bessey et al. report their experiences from commercializing
staticbugdetectors[ 5].Ayewahetal.sharelessonslearnedfrom
applyingFindBugs,thepredecessoroftheSpotBugstoolconsidered
in our study, at Google [ 3,4]. A recent paper describes the success
of deploying a name-based static checker [ 33]. Since many bugdetectorssufferfromalargenumberofwarnings,someofwhich
are false positives, an important question is which warnings to
inspect first. Work on prioritizing analysis warnings addresses this
question based on the frequency of true and false positives [ 15],
the version history of a program [ 14], and statistical models based
on features of warnings and code [35].
5 CONCLUSION
This paper investigates how many of all bugs can be found by cur-
rentstaticbugdetectors.Toaddressthisquestion,westudyaset
of 594 real-world Java bugs and three widely used bug detection
tools.Thecoreofourstudyisanovelmethodologytoassessthe
recall of bug detectors, which identifies detected bugs through a
combination of automatic, line-based matching and manual valida-
tionofcandidatesfordetectedbugs.Themainfindingsofthestudy
include the following: (i) Static bug detectors find a non-negligible
number,27,ofreal-worldbugs,showingthatstaticbugdetectors
arecertainlyworthwhileanddevelopersshouldusethemduring
development.(ii)Differentbugdetectorscomplementeachother
inthesensethattheydetectdifferentsubsetsofthestudiedbugs.
Usersofstaticbugdetectorsmaywanttorunmorethanonetool.
(iii) The large majority (95.5%) of the studied bugs are not detected
by the studied tools, showing that there is ample of room for im-
proving thecurrent state-of-the-art.(iv) Many ofthe missedbugs
are due to domain-specific problems instead of generic bug pat-
terns, while some currently missed bugs could be found with more
powerful variants of existing checks.
Overall, we conclude that future work should focus not only
on reducing false positives, as highlighted by previous studies,butalsoon detecting alargerfractionof all real-worldbugs,e.g.,by considering a larger variety of bug patterns and by searching
domain-specific bugs. Beyond these findings, which are relevant to
usersandcreatorsofstaticbugdetectors,ourresultscanserveas
abasisforafuturestudyoncomparingstaticanalysiswithother
bug detection techniques, such as manual and automated testing.
ACKNOWLEDGMENTS
We thank the anonymous reviewers and Julia Lawall for their valuable
comments.ThisworkwassupportedinpartbytheGermanResearchFoun-
dation within theEmmy Noether project ConcSysand the Perf4JSproject,
by the German Federal Ministry of Education and Research and by the
Hessian Ministry of Science and the Arts within CRISP, and by the Hessian
LOEWE initiative within the Software-Factory 4.0 project.
326
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. How Many of All Bugs Do We Find? ASE â€™18, September 3â€“7, 2018, Montpellier, France
REFERENCES
[1]Edward Aftandilian, Raluca Sauciuc, Siddharth Priya, and Sundaresan Krishnan.
2012. Building Useful Program Analysis Tools Using an Extensible Java Com-
piler. In12th IEEE International Working Conference on Source Code Analysis and
Manipulation, SCAM 2012, Riva del Garda, Italy, September 23-24, 2012. 14â€“23.
[2]MohammadMoeinAlmasi,HadiHemmati,GordonFraser,AndreaArcuri,and
Janis Benefelds. 2017. An Industrial Evaluation of Unit Test Generation: Finding
RealFaultsinaFinancialApplication.In 39thIEEE/ACMInternationalConference
on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017,
Buenos Aires, Argentina, May 20-28, 2017. 263â€“272.
[3]NathanielAyewah,DavidHovemeyer,J.DavidMorgenthaler,JohnPenix,and
William Pugh. 2008. Using Static Analysis to Find Bugs. IEEE Software 25, 5
(2008), 22â€“29.
[4]Nathaniel Ayewah and William Pugh. 2010. The Google FindBugs fixit. In
ProceedingsoftheNineteenthInternationalSymposiumonSoftwareTestingand
Analysis, ISSTA 2010, Trento, Italy, July 12-16, 2010. 241â€“252.
[5]AlBessey,KenBlock,BenjaminChelf,AndyChou,BryanFulton,SethHallem,
CharlesHenri-Gros,AsyaKamsky,ScottMcPeak,andDawsonR.Engler.2010. A
few billion lines of code later: Using static analysis to find bugs in the real world.
Commun. ACM 53, 2 (2010), 66â€“75.
[6]Fraser Brown, Shravan Narayan, Riad S. Wahby, Dawson R. Engler, Ranjit Jhala,
and Deian Stefan. 2017. Finding and Preventing Bugs in JavaScript Bindings. In
2017IEEESymposiumonSecurityandPrivacy,SP2017,SanJose,CA,USA,May
22-26, 2017. 559â€“578.
[7]Cristiano Calcagno, Dino Distefano, JÃ©rÃ©my Dubreil, Dominik Gabi, Pieter
Hooimeijer, Martino Luca, Peter Oâ€™Hearn, Irene Papakonstantinou, Jim Pur-brick, and Dulma Rodriguez. 2015. Moving fast with software verification. In
NASA Formal Methods Symposium. Springer, 3â€“11.
[8]Andy Chou, Junfeng Yang, Benjamin Chelf, Seth Hallem, and Dawson R. Engler.
2001. AnEmpiricalStudyofOperatingSystemErrors.In SymposiumonOperating
Systems Principles (SOSP). 73â€“88. http://doi.acm.org/10.1145/502034.502042
[9]Dawson Engler, David Yu Chen, Seth Hallem, Andy Chou, and Benjamin Chelf.
2001. Bugs as Deviant Behavior: A General Approach to Inferring Errors in
SystemsCode.In SymposiumonOperatingSystemsPrinciples(SOSP).ACM,57â€“
72.
[10]David Hovemeyer and William Pugh. 2004. Finding bugs is easy. In Compan-
iontotheConferenceonObject-OrientedProgramming, Systems,Languages,and
Applications (OOPSLA). ACM, 132â€“136.
[11]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.
2013. Whydonâ€™tsoftwaredevelopersusestaticanalysistoolstofindbugs?.In
Proceedings of the 2013 International Conference on Software Engineering . IEEE
Press, 672â€“681.
[12] S. C. Johnson. 1978. Lint,aCP r ogram Checker.
[13]RenÃ©Just,DarioushJalali,LauraInozemtseva,MichaelDErnst,ReidHolmes,and
GordonFraser.2014. Aremutantsavalidsubstituteforrealfaultsinsoftware
testing?. In Proceedings of the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering. ACM, 654â€“665.
[14]Sunghun Kim and Michael D. Ernst. 2007. Which warnings should I fix first?.
InEuropeanSoftwareEngineeringConferenceandSymposiumonFoundationsof
Software Engineering (ESEC/FSE). ACM, 45â€“54.
[15]TedKremenekandDawsonR.Engler.2003. Z-Ranking:UsingStatisticalAnal-
ysistoCountertheImpactofStaticAnalysisApproximations.In International
Symposium on Static Analysis (SAS). Springer, 295â€“315.
[16]Bin Liang, Pan Bian, Yan Zhang, Wenchang Shi, Wei You, and Yan Cai. 2016.
AntMiner: Mining More Bugs by Reducing Noise Interference. In ICSE.
[17]J. L. Lions. 1996. ARIANE 5 Flight 501 Failure. Report by the Inquiry Board.
European Space Agency.
[18]ShanLu,ZhenminLi,FengQin,LinTan,PinZhou,andYuanyuanZhou.2005.
Bugbench:Benchmarksforevaluatingbugdetectiontools.In Workshoponthe
Evaluation of Software Defect Detection Tools.
[19]ShanLu,SoyeonPark,EunsooSeo,andYuanyuanZhou.2008. Learningfrom
mistakes: a comprehensive study on real world concurrency bug characteristics.
InConferenceonArchitecturalSupportforProgrammingLanguagesandOperating
Systems (ASPLOS). ACM, 329â€“339.
[20]Matias Martinez,Thomas Durieux, Romain Sommerard, Jifeng Xuan, andMartin
Monperrus.2017. Automaticrepairofrealbugsinjava:Alarge-scaleexperiment
on the defects4j dataset. Empirical Software Engineering 22, 4 (2017), 1936â€“1964.
[21]SteveMcConnell.2004. CodeComplete:APracticalHandbookofSoftwareCon-
struction, Second Edition. Microsoft Press.
[22]Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H. Pham, Jafar M. Al-Kofahi, and
Tien N. Nguyen. 2009. Graph-based mining of multiple object usage patterns. In
European Software Engineering Conference and Symposium on the Foundations of
Software Engineering (ESEC/FSE). ACM, 383â€“392.
[23]FrolinS.OcarizaJr.,KartikBajaj,KarthikPattabiraman,andAliMesbah.2013.
AnEmpiricalStudyofClient-SideJavaScriptBugs.In SymposiumonEmpirical
Software Engineering and Measurement (ESEM). 55â€“64.[24]NicolasPalix,GaÃ«lThomas0001,SumanSaha,ChristopheCalvÃ¨s,JuliaL.Lawall,
andGillesMuller.2011. Faultsinlinux:tenyearslater.In ConferenceonArchi-
tectural Support for Programming Languages and Operating Systems (ASPLOS) .
ACM, 305â€“318.
[25]Kai Pan, Sunghun Kim, and E. James Whitehead Jr. 2009. Toward an understand-
ing of bug fix patterns. Empirical Software Engineering 14, 3 (2009), 286â€“315.
[26]Spencer Pearson, JosÃ© Campos, RenÃ© Just, Gordon Fraser, Rui Abreu, Michael D
Ernst, Deric Pang, and Benjamin Keller. 2017. Evaluating and improving fault
localization. In Software Engineering (ICSE), 2017 IEEE/ACM 39th International
Conference on. IEEE, 609â€“620.
[27]JacquesA.PienaarandRobertHundt.2013. JSWhiz:StaticanalysisforJavaScript
memory leaks. In Proceedings of the 2013 IEEE/ACM International Symposium on
CodeGenerationandOptimization,CGO2013,Shenzhen,China,February23-27,
2013. 11:1â€“11:11.
[28] Kevin Poulsen. 2004. Software Bug Contributed to Blackout. SecurityFocus.[29]
Michael Pradel and Thomas R. Gross. 2011. Detecting anomalies in the order of
equally-typedmethodarguments.In InternationalSymposiumonSoftwareTesting
and Analysis (ISSTA). 232â€“242.
[30]Michael Pradel, Ciera Jaspan, Jonathan Aldrich, and Thomas R. Gross. 2012.
Statically Checking API Protocol Conformance with Mined Multi-Object Specifi-
cations. In International Conference on Software Engineering (ICSE). 925â€“935.
[31]Foyzur Rahman, Sameer Khatri, Earl T Barr, and Premkumar Devanbu. 2014.
Comparing static bug finders and statistical prediction. In Proceedings of the 36th
International Conference on Software Engineering. ACM, 424â€“434.
[32]Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto
Bacchelli,andPremkumarT.Devanbu.2016. Onthe"naturalness"ofbuggycode.
InProceedings of the 38th International Conference on Software Engineering, ICSE
2016, Austin, TX, USA, May 14-22, 2016. 428â€“439.
[33]Andrew Rice, Edward Aftandilian, Ciera Jaspan, Emily Johnston, Michael Pradel,
and Yulissa Arroyo-Paredes. 2017. Detecting Argument Selection Defects. In
OOPSLA.
[34]NickRutar,ChristianB.Almazan,andJeffreyS.Foster.2004. AComparisonof
BugFindingToolsforJava.In InternationalSymposiumonSoftwareReliability
Engineering (ISSRE). IEEE Computer Society, 245â€“256.
[35]Joseph R. Ruthruff, John Penix, J. David Morgenthaler, Sebastian Elbaum, and
Gregg Rothermel. 2008. Predicting accurate and actionable static analysis warn-
ings:anexperimentalapproach.In InternationalConferenceonSoftwareEngineer-
ing (ICSE). 341â€“350.
[36]CaitlinSadowski,Jeffreyvan Gogh,Ciera Jaspan,Emma SÃ¶derberg,andCollin
Winter. 2015. Tricorder: Building a Program Analysis Ecosystem. In Proceedings
of the 37th International Conference on Software Engineering - Volume 1 (ICSE â€™15).
IEEEPress,Piscataway,NJ,USA,598â€“608. http://dl.acm.org/citation.cfm?id=
2818754.2818828
[37]MarijaSelakovicandMichaelPradel.2016. PerformanceIssuesandOptimiza-
tionsin JavaScript:AnEmpiricalStudy.In InternationalConference onSoftware
Engineering (ICSE). 61â€“72.
[38]SinaShamshiri,RenÃ©Just,JosÃ©MiguelRojas,GordonFraser,PhilMcMinn,and
AndreaArcuri.2015. DoAutomaticallyGeneratedUnitTestsFindRealFaults?
An Empirical Study of Effectiveness and Challenges (T). In 30th IEEE/ACM Inter-
nationalConferenceonAutomatedSoftwareEngineering,ASE2015,Lincoln,NE,
USA, November 9-13, 2015. 201â€“211.
[39]FerdianThung,Lucia,DavidLo,LingxiaoJiang,FoyzurRahman,andPremku-
marT.Devanbu.2012. Towhatextentcouldwedetectfielddefects?anempirical
studyoffalsenegativesinstaticbugfindingtools.In ConferenceonAutomated
Software Engineering (ASE). ACM, 50â€“59.
[40]FerdianThung,Lucia,DavidLo,LingxiaoJiang,FoyzurRahman,andPremku-
marT.Devanbu.2015. Towhatextentcouldwedetectfielddefects?Anextended
empirical study of false negatives in staticbug-finding tools. Autom. Softw. Eng.
22, 4 (2015), 561â€“602.
[41]StefanWagner,JanJÃ¼rjens,ClaudiaKoller,andPeterTrischberger.2005. Com-
paring Bug Finding Tools with Reviews and Tests. In International Conference on
Testing of Communicating Systems (TestCom). Springer, 40â€“55.
[42]SongWang,DevinChollak,DanaMovshovitz-Attias,andLinTan.2016. Bugram:
bugdetectionwithn-gramlanguagemodels.In Proceedingsofthe31stIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering,ASE2016,Singapore,
September 3-7, 2016. 708â€“719.
[43]Andrzej Wasylkowski and Andreas Zeller. 2009. Mining Temporal SpecificationsfromObjectUsage.In InternationalConferenceonAutomatedSoftwareEngineering
(ASE). IEEE, 295â€“306.
[44]Murray Wood, Marc Roper, Andrew Brooks, and James Miller. 1997. Comparing
and Combining Software Defect Detection Techniques: A Replicated Empirical
Study. In Software Engineering - ESEC/FSE â€™97, 6th European Software Engineering
ConferenceHeldJointlywiththe5thACMSIGSOFTSymposiumonFoundations
ofSoftwareEngineering,Zurich,Switzerland,September22-25,1997,Proceedings.
262â€“277.
[45]Jiang Zheng, Laurie A. Williams, Nachiappan Nagappan, Will Snipes, John P.
Hudepohl, and Mladen A.Vouk. 2006. On the Value of Static Analysisfor Fault
Detection in Software. IEEE Trans. Software Eng. 32, 4 (2006), 240â€“253.
327
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Andrew Habib and Michael Pradel
[46]MichaelZhivichandRobertK.Cunningham.2009. TheRealCostofSoftware
Errors.IEEE Security & Privacy 7, 2 (2009), 87â€“90.
328
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. 