Test Analysis: Searching for Faults in Tests
Matias Waterloo, Suzette Person, Sebastian Elbaum
Computer Science and Engineering Department
University of Nebraska - Lincoln
Lincoln, NE, USA
{waterloo, sperson, elbaum}@cse.unl.edu
Abstract —Tests are increasingly speciﬁed as programs. Ex-
pressing tests as code is advantageous in that developers are
comfortable writing and running code, and tests can be au-tomated and reused as the software evolves. Tests expressedas code, however, can also contain faults. Some test faults aresimilar to those found in application code, while others are moresubtle, caused by incorrect implementation of testing conceptsand processes. These faults may cause a test to fail when it shouldnot, or allow program faults to go undetected. In this work weexplore whether lightweight static analyses can be cost-effective inpinpointing patterns associated with faults tests. Our explorationincludes a categorization and explanation of test patterns, andtheir application to 12 open source projects that include over 40Ktests. We found that several patterns, detectable through simpleand efﬁcient static analyses of just the test code, can detect faultswith a low false positive rate, while other patterns would requirea more sophisticated and extensive code analysis to be useful.
I. I NTRODUCTION
As with any software, test code may be written by devel-
opers who are inexperienced, rushed, or not fully aware of
what is to be tested or how to test it. As a result, test codemay contain faults. Some may be similar to those found inapplication code, such as null pointer dereferences. Other faultscan be quite distinct as they are caused by improper implemen-tation of testing concepts associated with how the applicationstate is set, manipulated, or checked by the test code, or byassumptions about the underlying testing infrastructure.
Consider for example, a test written to compare the results
computed by Software Under Test (SUT) during the test runwith the expected results using a String representation of twoobjects in an assert statement. In this case, the comparison
may include parts of the object that are updated during testexecution, but not relevant for determining if the test passesor fails. Hence, the test may fail because the test author wasunsure of which parts of the object to check in the assert
statement, rather than failing because a fault in the SUT causedit to update the SUT state incorrectly. To detect this Over
Checked fault in test code requires an analysis of the implicit
testing process of comparing the expected and actual results.Now consider test code that does not include any explicitbehavior checks. In this case, the test code may fail to reveala fault because the application behavior is Under Checked,
e.g., checked only through exceptions that may be thrownduring test execution. These two cases illustrate test faultsassociated with state checking, but similar test correctnessissues may emerge when test code does not set the applicationstate properly or incorrectly assumes an environmental setting.
Faults in test code may cause a test to report a failure
when it should not, resulting in wasted developer time toinvestigate a fault that does not exist. They may also allowapplication faults to go undetected. Sadly, detecting such testcode correctness issues mostly occurs when a test fails, andis typically performed manually. This challenge is aggravatedby the increasing numbers of tests that can be generatedand executed automatically and the continuous scheduling oftest executions [10], which also pose a tremendous scalingchallenge for approaches that attempt to check for test anoma-lies at run time (e.g., running the tests repeatedly to removeﬂakiness, dynamic analysis techniques [14], [21]). Techniquessuch as mutation testing [15] can help to assess the overalleffectiveness of a test suite, but its costs often makes itimpractical to apply at such large scales and it does not identifyfaults in individual test cases.
Automated static program analysis techniques have been
successfully used to identify coding patterns that may indicatea fault (e.g., [1], [2], [3], [6], [13]). These techniques, whenapplied to test code, tend to focus on misuses of the testframework API, e.g., a test class without test methods. Moresophisticated test code pattern detectors have been recentlyexplored by our community particularly to identify test smellsassociated with style and maintenance issues [7], [11], [12],[17], [18], [19], [20], and assessing the presence and impact oftest smells [8], [16], but have not focused on test correctness.
In this work we explore and assess a set of patterns
targeting correctness faults in test code. We encode patternsidentiﬁed by practitioners and researchers of XUnit tests thatare associated with tests that may either indicate a failure whenthe SUT does not actually fail, or tests that may miss a faultthey are intended to capture. Such faults may render tests oflimited value, or worse, waste testing resources. We focuson patterns that can be recognized with light-weight staticanalyses of the Abstract Syntax Tree associated exclusivelywith the test code (not the SUT) and perform at most, a basicdata ﬂow analysis. The contributions of our work include:
• Identiﬁcation of the problem space of automaticallydetecting faults in test code.
• A collection of test code patterns that may indicatefaults in XUnit test code.
• A prototype implementation of a light-weight staticanalysis to detect those patterns.
• A study of the prevalence of test code patterns inXUnit tests found in software projects, and a deter-mination of whether the patterns indicate real faults.
• An analysis of which patterns matter, which ones havepotential, what is needed to improve their effective-ness, and which ones seem to be misguided.
2015 30th IEEE/ACM International Conference on Automated Software Engineering
978-1-5090-0025-8/15 $31.00 © 2015 IEEE
DOI 10.1109/ASE.2015.37149
II. T EST CODE PA TTERNS
The goal of our approach is to quickly analyze large test
suites to detect faults in the test code. Inspired by existing
techniques with a similar goal but that operate on applicationcode [6], [13], our approach aims to detect test code patternsthrough a light-weight static analysis of only the test code,enabling our technique to work when the SUT is not availableor it is too expensive to analyze or execute. Light-weightenables us to analyze large test repositories with thousandsof tests in just a few seconds.
We scope the work to tests implemented in XUnit, and in
particular we study those in the popular JUnit framework [4]written in the Java programming language. Each test class
contains one or more test methods. Test classes are combined
to create a test suite. XUnit uses annotations or naming
conventions to mark test methods and to conﬁgure the testrun. Common test data can be speciﬁed as test ﬁxtures which
can be created and initialized using a setup method and thenreleased using a teardown method to prevent unintentionaldependencies between tests. The sequence of steps performedby an XUnit tests typically follow a pattern, beginning witha setup phase to establish a particular state, followed by asequence of calls to the SUT. Then, a check is performedcomparing the expected and actual SUT state, often usingmethods in the Assert class. Finally, the test performs the
clean-up phase.
Critical to the success of our approach is the selection of
coding patterns that are effective at pinpointing only trulyproblematic test code. In this work, we developed patternsbased on coding observations and experiences of our own orothers that we prototyped, tested, and reﬁned to strike a balancebetween recall and precision. For brevity, the following patterndescriptions focus only on the most interesting, surprising,or potentially valuable patterns we found. We discuss otherpatterns and lessons learned during this process in Section IV.
A. Categories of Patterns
In this work, we describe three categories of patterns for
identifying problematic test code.
Inter-Test Dependencies Patterns. This category includes
patterns that identify potential dependencies between tests,
where a test relies on the state set by a previously executed test.This type of dependence is problematic because it implies atest ordering that is rarely enforced by engineers, and that is notdeﬁned by default for JUnit. It is particularly bothersome forsoftware test engineers because it can manifest as intermittenttest failures. Common test maintenance tasks (e.g., move,remove, add a test), and parallelizing test code to reduce thetotal test suite execution time are two of the ways the implicittest ordering assumption can be broken. Test code that exhibitsthis pattern can fail when it should not because an implicitordering assumption was violated. Although less common, testcode with this pattern can also miss a failure that is intended tobe detected because a previously executed test helped to maskit. The recent study by Zhang et al. on the test independentassumption supports the value of this family of patterns [21].
External Dependencies Patterns. This category includes pat-
terns that are associated with unchecked dependencies on theunderlying test execution infrastructure. As tests target larger
pieces of software, test developers tend to leverage more ex-ternal resources and infrastructure, ranging from environmentvariables to a cloud of services. This reliance on externalresources comes with speciﬁc usage requirements that canchange without affecting the SUT but can impact the test, andintroduce implicit assumptions that may not always hold. Thistype of pattern can lead to tests that fail intermittently, wastingdeveloper’s time trying to reproduce and isolate the problem.The recent study by Luo et al. showed that such test faults canbe frequent and consequential [16].
Intra-Test Patterns. This category of test code patterns iden-
tiﬁes test code that may be inappropriately setting or checkingthe SUT state to determine whether it behaves as expected. Inthis work we focus on two variants: patterns that identify teststhat check too much of the state (potentially indicating a lackof understanding of the SUT or what the test is intended tocheck), and patterns that identify tests that check too little ofthe state. Test code that exhibits an intra-test pattern may missfaults because it is not checking enough state or the right partof the program state, or may detect differences in the programstate that are not relevant to what the test is validating.
Based on our experience and those of others working with
similar approaches [9], it seems more effective to miss someproblematic issues in order to increase speed and report fewerfalse positive, hereby increasing the chances of applicability inpractice. With this perspective in mind, we made three key de-cisions regarding the implementation of the pattern detectors.First, the patterns had to be detectable through a lightweightstatic analysis. All of the patterns shown in the next section canbe identiﬁed through the information provided in an AbstractSyntax Tree (AST), including the JUnit annotations. From theAST we make some inferences about control ﬂow, data ﬂow,and type, but, as shown in the performance analysis in SectionIII, the analyses remains very efﬁcient. Second, the analysiswas restricted to the scope of a single test class. This decisionsometimes limited the precision and recall of our analysis butprovided the performance necessary to analyze thousands oftests in seconds, and enabled us to explore how much we cando with the analysis when the SUT code is not available. Third,the analysis depth could vary across the patterns, going deepenough, within the bounds of the other two criteria, to eitherreduce the number of false positives or obtain more instancesof each pattern. This inconsistency reﬂects the tension betweentrying to only detect pattern instances that correspond to testfaults while doing it quickly.
B. Patterns
Table I provides a summary of the tests patterns that re-
sulted from our design-prototype-assess-reﬁne cycle, grouped
by their category. A common building block for all patternsis how we determine what methods to analyze. We recognizetest methods through their annotations, such as @Test in JUnit
4, or by the use of the Test sufﬁx on the method name (a
standard test coding convention), or by the extension of a classending with TestCase (JUnit 3). We ignore tests annotated
with@Ignore.
Inter-Test Dependencies Patterns. All patterns in this
category ignore test classes where the test ordering is en-forced through the @FixMethodOrder(...). Interestingly
150TABLE I: Correctness Patterns for Test Code.
Family Pattern Description
Inter-Test Test Calls Test Test reaches other tests and shared a static ﬁeld.
Shared Static Field A ﬁeld is written to and read from different tests without resetting it.
Shared Stream A stream is written to and read from different tests without resetting or checking it.
External Timing Dependency Test invokes hard-codes a wait period.
System Dependency Test uses and changes the underlying system state.
Network Dependency Test assumes a requested network resource is available.
Intra-Test Over Checked Test uses a serialized version of an entire object within assertion-like statements.
V ague Exception Test catches the most general exception instead of speciﬁc ones.
Under Checked Test cannot reach assertion-like statements.
enough, this annotation was used infrequently in all of the
projects we analyzed, enforcing the notion that the syntactictest ordering is typically assumed in practice. The ﬁrst twopatterns in this category are closely related, and focus on testdependencies that manifest through shared static ﬁelds. Sharingstatic ﬁelds is problematic as the order of test execution mayrender different results. The Test Calls Test pattern identiﬁes a
test, T
i, that invokes another test, Tj, either directly (T i→Tj)
or indirectly through other methods (T i→...→Tj), where at
least one of the tests writes to a static ﬁeld, potentially affectingthe behavior of the other test. As deﬁned, one can easilyimagine cases where this pattern would render false positives.However, we found that, even when considering transitivity,there were no instances of this pattern in our study. This wassurprising enough that we kept the pattern as a negative resultworth highlighting.
The second Inter-Test Dependencies pattern, Shared Static
Field, also involves two test methods, where one test, T
U,
updates a static ﬁeld and another test, TR, reads the static
ﬁeld, becoming dependent on the state set by TU. Although
dependencies of this type are possible among more thantwo tests, most faults found through this type of pattern areexpected to involve just two tests [21], whose execution ordermay generate different results. We ignore instances in whichthesetup() method or methods annotated with @Before
write to the ﬁeld as we assume the ﬁeld is being reset betweentests. We also ignore the pattern in test code if T
Ralso writes
to the ﬁeld before reading it.
The third pattern in this category, Shared Stream, is similar
to the Shared Static Field pattern, but differs in that it is asso-
ciated with dependencies on inputs and outputs. More speciﬁ-cally, we look for occurrences of instantiation of objects whoseclass is FileReader, FileInputStream, Scanner,
or RandomAccessFile in a test T
Rthat may indicate an
implicit dependence on a test, TU, that updates the shared
resource. We identify TUby looking for instantiation of objects
whose class is FileWriter, FileOutputStream,
PrintWriter or RandomAccessFile. After identify-ing the set of tests that either read or write (or both) to objectsof these types, we determine if there is a test T
Rthat may
perform a read operation on an object written by anothertest T
Uin the same test class. We ignore tests where the
input/output object may have been written prior to the readoperation (either in the setup or in a method invoked prior tothe read).
External Dependencies Patterns. The ﬁrst test code pat-
tern in this category is the Timing Dependency pattern. This
type of dependency has been identiﬁed as one of the mainsources of ﬂaky tests [16]. Our technique searches test codefor calls to Thread.sleep(...), a common instance of
this pattern (other instances include empty loops or calls toother libraries that provide timing features). We also performa check to ensure this coding pattern does not occur within apredicate that may trigger other actions and checks to handlethe dependency. Test code with this pattern is problematicbecause it can cause tests to fail, but not for the reasons thatwere intended if the speciﬁed time is not long enough. Tests ex-hibiting this pattern may also set the timer too conservatively,wasting cycles or causing other conﬂicts when, for example,the@Test(timeout=n) annotation is used.
The System Dependency pattern identiﬁes test code that
uses or sets parts of the system state that is not checkednor set by the test setup. This pattern identiﬁes invocationstoSystem. *Properties orruntime.exec(). Both
types of calls generate dependencies on external resources,either by setting particular properties or resources in theenvironment, or by reading environmental variables that mayaffect how the SUT state is set or checked. Furthermore, someof these invocations are platform dependent and may causefurther instability in the test results if the underlying executionenvironment changes.
The last pattern in this category is the Network Depen-
dency pattern. The current implementation of this pattern
looks for invocations to getContent, openStream or
openConnection onURL objects whose return value is
not included as part of an assertion-like statement, nor arethey checked by a predicate to determine the test ﬂow.
Intra-Test Patterns. The ﬁrst pattern in this category is the
Over Checked pattern which identiﬁes test code that checks
an entire object instead of checking only the relevant partsof the object. The problem with this code is that a test mayfail when it should not, e.g., when irrelevant parts of theobject are updated by the environment. The presence of thispattern in test code may indicate confusion on the part of thetester in terms of what is to be tested, or may indicate thetester was attempting a shortcut in the test. This pattern isdetected by identifying calls to toString() on an object
in an assertion statement. We ignore instances of this patternwhen the return value of the call to toString() is further
manipulated through calls to methods such as startsWith,
contains, endsWith andsubstring as we assume
the intent in these cases is to check a particular part of theobject. We also ignore cases where the object on which thetoString() is invoked is an instance of a standard li-
brary object, such as StringBuffer, StringBuilder,
StringWriter andByteArrayOutputStream or any
151wrapper of a primitive type, in which case it has different se-
mantics. We also ignore instances of calls to the toString()
method appearing in the message part of an assertion state-ment, or has multiple parameters that may render differentbehavior.
A variant of the Over Checked pattern is the V ague Excep-
tion pattern, which occurred frequently enough in our study to
merit its own pattern. This pattern identiﬁes test methods thatintend to throw a speciﬁc exception within a try block, but
capture a more general type of exception in the catch block.
Test code exhibiting this pattern is problematic in that it maygenerate false positives when the exception that causes the testto fail is forced by issues other than the one speciﬁed in thetry block. Test code that follows this pattern can also make it
more difﬁcult to diagnose the fault. We ignore this pattern intest code if there is a fail statement inside the catch blocks,
or if there is an assert statement that checks the variable
related to the exception, as this may indicate an alternativehandling of the exception.
The Under Checked pattern identiﬁes tests that may not
check enough of the expected SUT behavior, and as a resultmay fail to reveal a fault. More speciﬁcally, this patternidentiﬁes tests that lack any implicit checking of the actualresults with the expected results. Although tools like PMDidentify test methods without an assertion, our analysis is moreextensive. First, we identify various ﬂavors of assert statementsincluding those containing keywords that are often associatedwith checking the behavior of the SUT such as check, validate,
verify (this last one is used extensively by a common mocking
framework [5]). Second, since assertion-like statements canbe placed in shared helper methods, this pattern performs atransitive analysis, checking every method invoked by a testthat is within the same test class. Third, to reduce the numberof false positives, our approach ignores tests that call onlyconstructors since those tests often do not contain any type ofchecking.
III. S
TUDY
The goal of our study is three-fold: 1) explore the extent
to which the test fault patterns we have identiﬁed are presentin test code found in existing Java projects, 2) determine thecost of the analysis in terms of run time, and 3) learn whetherthe code detected with our patterns represents true faults. Toachieve these goals, we implemented our test code patterndetector using PMD [6], version 5.2.0. Our implementationand data are available at www.cse.unl.edu/
∼waterloo/ase2015.
A. Artifacts
The artifacts analyzed in our study are listed in Table II.
They correspond to Java projects with test suites implementedin JUnit. The ﬁrst two artifacts, Joda-Time and Jetty, are twopopular systems with many unit tests that we were familiarwith, and that helped us to evolve the patterns as we initiallydeveloped them. The rest of the artifacts were obtained fromthe list of projects analyzed by Luo et al. [16]. From theirlist, we picked the ﬁrst 10 implemented in Java. The artifactsanalyzed include 7730 test classes and 41977 test methods.For each artifact, Table II includes the version (+ for ApacheGit repository, * for Apache SVN repository, - for Github,x: for Eclipse Git repository), metrics about the size of theTABLE II: Test Code Artifacts. (TC: Test classes, TM: Testmethods, T+1A: Tests w/ at least one assert (not transitive),AM: Assert calls, SM: Setup methods, TDM: Tear downmethods, EE: Expected exceptions, SF: static ﬁelds.)
Artifact #TC #TM #T+1A #AM #SM #TDM #EE #SF
Joda -(986d905) 133 4131 3961 19161 117 109 1330 535
Jetty x(a71cc69) 624 3178 2625 11616 139 138 264 377
HBase +(9b7f36b) 637 2794 2140 10891 173 122 515 2652
Hadoop +(85aec75) 1668 8611 6328 31653 555 376 2467 3772
Derby +(314a19a) 534 3530 2280 15999 159 158 1273 1910
Lucene *(1651570) 1487 7509 5673 32568 289 193 1118 1244
Tomcat 8.0 +(023fd55) 322 2413 1434 4692 47 29 268 617
ServiceMix +(ead7e67) 3 4 4 10 0 0 0 2
ZooKeeper +(6ebd23b) 167 596 446 1849 61 47 188 171
CXF +(318501b) 1195 6706 5184 19062 328 171 1225 1590
Flume +(199684b) 164 929 674 2403 95 48 207 216
Maven +(32053c9)(b182c5b) 796 1576 1134 4669 66 49 109 53
test suites, number of statements that perform a check on theprogram state, and the number of methods that setup and teardown program states. The test code artifacts vary in the densityof assert statements per test, make different use of setup andtear down methods, and in general have different structuralattributes (e.g., Derby and HBase seem to use static ﬁelds twiceas often as other projects). All artifacts use JUnit 4.0, exceptfor Joda-time which uses JUnit 3.0.
B. Results
Extent of Pattern Instances. Table III provides a count of the
pattern instances detected by our tool across artifacts. Only one
of the ten artifacts, ServiceMix, which is the smallest artifact,presented no patterns. The rest of the artifacts have between67 and 833 instances of the patterns, somewhat proportionalto the number of test classes and methods, but with variabilityin terms of what patterns are most dominant across artifacts.
Inter-Test dependencies patterns do not occur as often in
practice as we and others had expected [21]. In fact, only twoprojects, HBase and Hadoop, had one and ﬁve instances ofthe Shared Static Field pattern respectively. We note that the
density of static ﬁelds per test class is the highest for HBase(4.2), but average for Hadoop (2.3), so this structural featurealone does not seem to indicate this pattern. The scarcity ofthis pattern is interesting as it provides further context on thevalue proposition of techniques for detecting and removing testdependencies. As noted previously, the Test Calls Test pattern
was not found in any of the artifacts evaluated.
External dependencies patterns were found much more
frequently. In particular, we note that simple patterns can beused to detect the Timing Dependency pattern identiﬁed in
recent work on test ﬂakiness [16]. This pattern was presentin 80% of projects, and on average in 14% of the test classeswe analyzed. Instances of the Network Dependency pattern
were less frequent, but were detected in seven of the projects.Instances of the System Dependency pattern were the most rare
in this category of patterns.
Instances of the Intra-Test patterns were detected most fre-
quently across the majority of the artifacts, occurring hundredsof times. Instances of the V ague Exception and Under Checked
patterns were found across all projects except Service Mix, andinstances of the Over Checked pattern were found in all but
Service Mix and Maven.
152TABLE III: Pattern instances found cross artifacts and results of selective analysis of these instances. In italics: (True
Positives/False Positives/Uncertain Cases). Artifacts in bold were selected for analysis due to their size. Artifact/pattern
combinations with * were selected for analysis because they were outliers.
Joda Jetty HBase Hadoop Derby Lucene Tomcat ServiceMix ZooKeeper CXF Flume Maven Total
Test Calls Test 0 0 0 0 0 0 0 0 0 0 0 0 0
Shared Static Field 0 0 1(0/0/1) 5(0/4/1) 0 0 0 0 0 0 0 0 6(0/4/2)
Shared Stream 0 0 0 0 0 0 0 0 0 0 0 0 0
Timing Dependency 0 144 (5/0/0)* 87(5/0/0) 280 (5/0/0) 19 36(5/0/0) 49 0 31 64(5/0/0) 96(5/0/0) * 1(1/0/0) 807 (31/0/0)
System Dependency 0 0 0 8(4/1/0) 2 0 0 0 0 0 0 0 10(4/1/0)
Network Dependency 0 2 1(0/1/0) 7(0/3/2) 0 2(1/1/0) 1 0 0 14(0/3/2) 1 0 28(1/8/4)
Over Checked 247 (0/0/5)* 4 0 42(3/1/1) 2 47(0/4/1) 45 0 0 105 (4/0/1) 1 0 493 (7/5/8)
V ague Exception 3 21 50(4/0/1) 206 (2/1/2) 10 67(3/2/0) 41 0 11 206 (1/1/3) 8 12(4/0/1) 635 (14/4/7)
Under Checked 3 24 53(4/1/0) 285 (5/0/0) 75(5/0/0)* 111 (5/0/0) 48 0 30 109 (1/4/0) 35 54(5/0/0) 827 (25/5/0)
Total 253 195 192 833 108 263 184 0 72 498 141 67 -
Cost of Analysis. When performed on a MacBookPro 10.9
laptop, with an Intel Core i5 2.6 GHz processor and 8GB ofmemory, the detection of each pattern on the largest artifact,Hadoop, takes approximately 90 seconds. The detection ofall patterns together on the same artifact only takes 5 to10 seconds more than any individual pattern, as most of theanalysis time is used to parse the code in the project and buildthe AST for those ﬁles containing test classes.
Extent of True Faults. We now discuss the results of our
analysis to determine if the patterns detected constitute real
test faults. We began this process by selecting the ﬁve artifactswith the largest number of test classes (Hadoop, Lucene, CXF,Maven and HBase). We then randomly chose at most ﬁveinstances of each detected pattern instance for each of theselected artifacts. Table III summarizes the ﬁndings. Cells withvalues in italics show three values enclosed by parentheses(True Positives/False Positives/Uncertain Cases) next to the
number of pattern instances found. The ﬁrst value, True
Positives, corresponds to pattern instances that we deemed tobe real faults. The second value, False Positives, identiﬁes the
number of pattern instance that do not represent a fault in thetest code. The third value, Uncertain Cases, is the number of
pattern instances that we could not classify either way with anycertainty, given our understanding of the test and SUT code.
The Shared Static Field pattern was found in only two
projects, HBase and Hadoop. Four out of six instances are falsepositives, caused by test code that invokes methods outsidethe scope of the test, and hence outside the scope of theanalysis. Extending the analysis to include the applicationcode could remove such false positives, but given the smallnumber of occurrences of this pattern and the cost of suchan analysis, this may not be cost-effective. The two uncertaincases appear to be faults as per the ﬁxture design, but wewere not conﬁdent enough to make this determination withouta better understanding of the application code.
For the Timing Dependency pattern, all projects reported
true faults. This conﬁrms the value of detecting this particularlypervasive pattern. Four of the System Dependency pattern
instances we analyzed for Hadoop were true positives, andone was a false positive caused by a helper method that reliedon an operating system call but was not reachable by any ofthe test methods. A deeper reachability analysis would be ableto remove this false positive. The Network Dependency pattern
appeared in all ﬁve projects but with low frequency. Onlyone of the instances is a true positive (the one in Lucene).When analyzed further, we discovered that the false positivesand uncertain cases are caused when there are indirect checkson the returned content. In some cases, the returned object istransformed into another object, i.e., a string, and checks areperformed over this object. A more sophisticated analysis overa scope that extends beyond the test code would be necessaryto remove such false positives.
Instances of the Over Checked pattern appeared in Hadoop,
Lucene, and CXF, with mixed results. A total of seven truepositives, ﬁve false positives, and three uncertain cases werereported. The false positives occur for two reasons. First, theSUT builds heavily on a string representation, so it is naturalfor tests to perform calls to the toString method to check
the object state. Second, the SUT offers no alternative methodsto check its state. Extending the analysis to the SUT could helpremove some of these false positives.
The V ague Exception pattern instances in HBase, Hadoop,
Lucene, CFX, and Maven correspond to 14 true positives, fourfalse positives, and seven uncertain cases. The false positivesare caused when the exception is analyzed in ways morecomplex than the pattern can recognize. Among the uncertaincases we found it common for catch blocks to print the stack
trace to standard output for the developer to later analyze. Thiswill not scale well, but it may work for smaller scopes so weleft it marked as uncertain. A simple additional analysis couldremove such doubtful cases.
The Under Checked pattern resulted in 20 true positives,
and 5 false positives (4 of which are in CXF). CXF wasparticularly challenging because it uses a test helper classthat is outside the scope of analysis to aggregate assertionmethods, but does not follow the standard naming conventionwe assumed in our pattern implementation.
In addition to analyzing the ﬁve largest artifacts, we also
inspected the results of the analysis for four cells in TableIII that reported more than 75 pattern instances for the otherprojects. These cells are marked with * in the table. Fifteenpattern instances were true positives, while the ﬁve in Joda-Time were uncertain. These uncertain patterns were causedby the state of DateTime objects, which are often checkedby invoking the toString method. Clearly, not all of the
DateTime object are changed by the invocations made so theassertions are over checking the program state. Yet, sinceDateTime objects rarely change, the risks of reporting a false
153failure seem small. In any event, a small adaptation of the
pattern to the project could remove this type of instance.
IV . L ESSONS LEARNED
During the course of our project, we considered several
patterns that intuitively seemed like good ideas, or that wereempirically supported by previous studies, but found theydid not perform as we expected on real and large test codebases. Some, like the Inter-Test Dependencies patterns, do not
occur in practice as often as we expected, based on our studyresults. In particular, the patterns relying on static ﬁelds sharedamong tests, which was found to be the source of most faultsassociated with the test independence assumption [21], wasquite rare even with very relaxed conditions. This negativeﬁnding was particularly surprising, which is why we retainedthis category of patterns in our study.
In general, the wide range of test implementations we
encountered was unexpected, despite our familiarity withJUnit, and frequently led to many pattern reﬁnements anddiscussions about their potential impact. Test code involvingtest class hierarchies or external test support libraries, andtest code meant to be executed across multiple conﬁgurationsseemed to be increasingly pervasive, resulting in increased testcomplexity and making test analysis more challenging.
An interesting case we discovered in our study was the
occurrence of patterns that sometimes appear in practice butthat were obfuscated by the behavior of the SUT. For example,within the Intra-Test pattern category, we investigated a pattern
that identiﬁes getter methods whose return value could notinﬂuence an assertion statement. The key to this pattern isidentifying getter methods. To do that we employed a simpleheuristic that reﬂects standard and recommended practices: weassumed that methods with a get preﬁx would be getters. In
practice we found that many SUT getter methods have sideeffects. This caused many false positives and illustrates thelimitations of analyzing only test code; however it also illus-trates how analyzing test code can help to identify anomaliesin the SUT. e.g., a mis-use of standard coding conventions.
V. C
ONCLUSION
We found that patterns associated with faults in test code
can be detectable using light-weight static analyses. The bigwinner is the Timing Dependency pattern, which was pervasive
across projects and was consistently associated with incorrecttests. The Under Checked pattern was the most pervasive but
with a 20% false positive rate (all from a single project). Itcould be improved with simple pattern tailoring to accommo-date the non-standard naming conventions used by this project.The V ague Exception pattern was the third most pervasive
pattern, has a false positive rate of slightly over 20%, but hadmany cases that we could not classify given the complexityof the analysis context. Still, its frequency and rate make itenticing to pinpoint tests that may fail when they should notbecause of an assertion that can be easily refactored. TheOver Checked pattern was also common, but with a false
positive rate of over 40% is not one to be adopted in practicewithout analyzing the SUT. This is interesting as it serves toillustrate the limits of what we can learn just within the testscope. The rest of the patterns, including all of the patterns intheInder-Test Dependencies category, may be reﬁnedthrough more sophisticated and extended-scope analyses, butthey do not occur often enough to justify such reﬁnement.The next steps in this exploration will include richer patternsto analyze tests generated by different techniques and tools,while relaxing the scope of analysis.
VI. A
CKNOWLEDGMENTS
This work was supported in part by a Google Faculty
Research Award and a National Science Foundation Award#SHF-1218265 and #CCF-1526652.
R
EFERENCES
[1] Coverity. http://www.coverity.com/.
[2] FxCop. http://msdn.microsoft.com/en-us/library/bb429476\%28v=vs.
80\%29.aspx.
[3] IntelliJ. http://www.jetbrains.com/idea/.[4] jUnit. http://www.junit.org/.[5] Mockito. https://code.google.com/p/mockito/.[6] PMD. http://pmd.sourceforge.net/.[7] G. Bavota, A. Qusef, R. Oliveto, A. De Lucia, and D. Binkley. An
empirical analysis of the distribution of unit test smells and their impact
on software maintenance. In ICSM, pages 56–65, Sept 2012.
[8] G. Bavota, A. Qusef, R. Oliveto, A. De Lucia, and D. Binkley. Are
test smells really harmful? an empirical study. Empirical Software
Engineering, pages 1–43, 2014.
[9] A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton, S. Hallem, C. Henri-
Gros, A. Kamsky, S. McPeak, and D. Engler. A few billion lines ofcode later: Using static analysis to ﬁnd bugs in the real world. Commun.
ACM , 53(2):66–75, Feb. 2010.
[10] S. G. Elbaum, G. Rothermel, and J. Penix. Techniques for improving
regression testing in continuous integration development environments.InFSE, pages 235–245, 2014.
[11] M. Greiler, A. van Deursen, and M.-A. Storey. Automated detection of
test ﬁxture strategies and smells. In ICST, pages 322–331, 2013.
[12] B. Hauptmann, M. Junker, S. Eder, L. Heinemann, R. V aas, and
P . Braun. Hunting for smells in natural language tests. In ICSE, pages
1217–1220, May 2013.
[13] D. Hovemeyer and W. Pugh. Finding bugs is easy. SIGPLAN Not.,
39(12):92–106, Dec. 2004.
[14] C. Huo and J. Clause. Improving oracle quality by detecting brittle
assertions and unused inputs in tests. In FSE, pages 621–631, 2014.
[15] Y . Jia and M. Harman. An analysis and survey of the development
of mutation testing. IEEE Transactions of Software Engineering,
37(5):649–678, 2011.
[16] Q. Luo, F. Hariri, L. Eloussi, and D. Marinov. An empirical analysis
of ﬂaky tests. In FSE, FSE 2014, pages 643–653, 2014.
[17] G. Meszaros. XUnit Test Patterns: Refactoring Test Code. Addison-
Wesley, 2007.
[18] H. Neukirchen and M. Bisanz. Utilising code smells to detect quality
problems in ttcn-3 test suites. In A. Petrenko, M. V eanes, J. Tretmans,and W. Grieskamp, editors, Testing of Software and Communicating
Systems, volume 4581 of Lecture Notes in Computer Science, pages
228–243. Springer Berlin Heidelberg, 2007.
[19] A. van Deursen, L. Moonen, A. van den Bergh, and G. Kok. Refactoring
test code. Technical report, Amsterdam, 2001.
[20] B. V an Rompaey, B. Du Bois, S. Demeyer, and M. Rieger. On the
detection of test smells: A metrics-based approach for general ﬁxtureand eager test. IEEE Trans. Softw. Eng., 33(12):800–817, Dec. 2007.
[21] S. Zhang, D. Jalali, J. Wuttke, K. Mus ¸lu, W. Lam, M. D. Ernst, and
D. Notkin. Empirically revisiting the test independence assumption. InISSTA, pages 385–396, 2014.
154