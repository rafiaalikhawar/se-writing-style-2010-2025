CacheOptimizer: Helping Developers ConÔ¨Ågure Caching
Frameworks for Hibernate-Based Database-Centric Web
Applications
Tse-Hsun Chen
Queen‚Äôs University
Ontario, Canada
tsehsun@cs.queensu.caWeiyi Shang
Concordia University
Quebec, Canada
shang@encs.concordia.caAhmed E. Hassan
Queen‚Äôs University
Ontario, Canada
ahmed@cs.queensu.ca
Mohamed Nasser
BlackBerry, CanadaParminder Flora
BlackBerry, Canada
ABSTRACT
To help improve the performance of database-centric cloud-
based web applications, developers usually use caching frame-
works to speed up database accesses. Such caching frame-
works require extensive knowledge of the application to op-
erate eectively. However, all too often developers have lim-
ited knowledge about the intricate details of their own ap-
plication. Hence, most developers nd conguring caching
frameworks a challenging and time-consuming task that re-
quires extensive and scattered code changes. Furthermore,
developers may also need to frequently change such cong-
urations to accommodate the ever changing workload.
In this paper, we propose CacheOptimizer , a lightweight
approach that helps developers optimize the conguration
of caching frameworks for web applications that are imple-
mented using Hibernate. CacheOptimizer leverages readily-
available web logs to create mappings between a workload
and database accesses. Given the mappings, CacheOpti-
mizer discovers the optimal cache conguration using coloured
Petri nets, and automatically adds the appropriate cache
congurations to the application. We evaluate CacheOpti-
mizer on three open-source web applications. We nd that
i)CacheOptimizer improves the throughput by 27{138%;
and ii) after considering both the memory cost and through-
put improvement, CacheOptimizer still brings statistically
signicant gains (with mostly large eect sizes) in compar-
ison to the application's default cache conguration and to
blindly enabling all possible caches.
1. INTRODUCTION
Web applications are widely used by millions of users
worldwide. Thus, any performance problems in such ap-
plications can often cost billions of dollars. For example, a
report published in 2012 shows that a one-second page load
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
FSE‚Äô16, November 13 - 19, 2016, Seattle, WA, USA
¬© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4218-6/16/11. . . $15.00
DOI:http://dx.doi.org/10.1145/2950290.2950303slowdown of the Amazon web applications can cost an aver-
age of 1.6 billion dollars in sales each year [25]. The complex-
ity and scale of modern database-centric web applications
complicate things further. As much as 88% of developers
nd their applications' performance is deeply impacted by
their reliance on databases [29]. Application-level caching
frameworks, such as Ehcache [54] and Memcached [38], are
commonly used nowadays to speed up database accesses in
large-scale web applications. Unlike traditional lower-level
caches (e.g., hardware or web proxies) [3, 7, 18, 34], these
application-level caching frameworks require developers to
instruct them about what to cache; otherwise these frame-
works are not able to provide any benet to the application.
Deciding what should be cached can be a very dicult and
time-consuming task for developers, which requires in-depth
knowledge of the applications and workload. For example,
to decide that the results of a query should be cached, devel-
opers must rst know that the query will be frequently exe-
cuted, and that the fetched data is rarely modied. Further-
more, since caching frameworks are highly integrated with
the application, these frameworks are congured in a very
granular fashion { with cache API calls that are scattered
throughout the code. Hence, developers must manually ex-
amine and decide on hundreds of caching decisions in their
application. Even worse, a recent study nds that most
database-related code is undocumented [37], which makes
manual conguration even harder.
Developers must continuously revisit their cache cong-
uration as the workload of their application changes [22].
Outdated cache congurations may not provide as much per-
formance improvement, and they might even lead to perfor-
mance degradation. However, identifying workload changes
is dicult in practice for large applications [53, 61]. Even
knowing the workload changes, developers still need to spend
great eort to understand the new workload and manually
re-congure the caching framework.
In this paper, we propose CacheOptimizer , a lightweight
approach that automatically helps developers decide what
should be cached (and also automatically places the cache
conguration code) in web applications that are implemented
using Hibernate in order to optimize the conguration of
caching frameworks. Using CacheOptimizer , developers can
better manage the cost of their database accesses { greatly
improving application performance [6,11,13{16,48].CacheOptimizer rst recovers the workload of a web appli-
cation by mining the web server access logs. Such logs are
typically readily-available even for large-scale applications
that are deployed in production environments. CacheOpti-
mizer further analyzes the source code statically to identify
the database accesses that are associated with the recov-
ered workloads. To identify detail information about the
recovered database accesses, such as the types of the ac-
cess and the accessed data, CacheOptimizer leverages static
taint analysis [30] to map the input variables of the web
requests to the exact database accesses. Combining the re-
covered workload and the corresponding database accesses,
CacheOptimizer models the workload, the database accesses,
and the possible cache congurations as a coloured Petri net.
By analyzing the Petri net, CacheOptimizer is able to deter-
mine an optimal cache conguration (i.e., given a workload,
which objects or queries should be cached by the caching
frameworks). Finally, CacheOptimizer automatically adds
the appropriate conguration calls to the caching framework
API into the source code of the application.
We have implemented our approach as a prototype tool
and evaluated it on three representative open-source database-
centric web applications (Pet Clinic [44], Cloud Store [20],
and OpenMRS [41]) that are based on Hibernate [21]. The
choice of Hibernate is due to it being one of the most used
Java platforms for database-centric applications in practice
today [58]. However, our general idea of automatically con-
guring a caching framework should be extensible to other
database abstraction technologies. We nd that after apply-
ingCacheOptimizer to congure the caching frameworks on
the three studied applications, we can improve the through-
put of the entire application by 27{138%.
The main contributions of this paper are:
1. We propose an approach, called CacheOptimizer , which
helps developers in automatically optimizing the con-
guration of caching frameworks for Hibernate-based
web applications. CacheOptimizer does not require
modication to existing applications for recovering the
workload, and does not introduce extra performance
overhead.
2. We nd that the default cache conguration may not
enable any cache or may lead to sub-optimal perfor-
mance, which shows that developers are often unaware
of the optimal cache conguration.
3. Compared to having no cache ( NoCache ), the default
cache congurations ( DefaultCache ), and enabling all
caches ( CacheAll ),CacheOptimizer provides a better
throughput improvement at a lower memory cost.
Paper Organization. The rest of the paper is organized
as follows. Section 2 rst discusses related work to our pa-
per. Section 3 introduces background knowledge for com-
mon caching frameworks. Section 4 describes the design
details of CacheOptimizer . Section 5 evaluates the benets
and costs of CacheOptimizer . Section 6 discusses threats to
validity of our study. Finally, Section 7 concludes the paper.
2. RELATED WORK AND BACKGROUND
In this section, we discuss related work to CacheOpti-
mizer . We focus on three closely related areas: software en-
gineering research on software conguration, optimizing the
performance of database-centric applications, and caching
frameworks.2.1 Software ConÔ¨Åguration
Improving Software Congurations. Software cong-
urations are essential for the proper and optimal operation
of software applications. Several prior software engineer-
ing studies have proposed approaches to analyze the con-
gurations of software applications. For example, Rabkin
et al:[47] use static analysis to extract the conguration
options of an application, and infer the types of these con-
gurations. Xu et al:[2] conduct an empirical study on the
conguration parameters in four open-source applications in
order to help developers design the appropriate amount of
congurability for their application. Liu et al:[57] focus on
conguring client-slide browser caches for mobile devices.
Detecting and Fixing Software Conguration Prob-
lems. Rabkin et al:[46] use data ow analysis to detect
conguration-related functional errors. Zhang et al:[59]
propose a tool to identify the root causes of conguration
errors. In another work, Zhang et al:[60] propose an ap-
proach that helps developers congure an application such
that the application's behaviour does not change as the ap-
plication evolves. Chen et al:[10] propose an analysis frame-
work to automatically tune congurations to reduce energy
consumption for web applications. Xiong et al:[56] au-
tomatically generate xes for conguration errors using a
constraint-based approach.
Prior research on software conguration illustrates that
optimizing congurations is a challenging task. In this pa-
per, we propose CacheOptimizer , which particularly focuses
on helping developers optimize the cache congurations to
improve the performance of large-scale web applications.
2.2 Improving Application Performance by Re-
ducing the Overhead of Database Accesses
Most studies in literature propose frameworks to reduce
the overhead of database accesses by batching [28], or re-
ordering [6] database accesses. Ramachandra et al:[48] pro-
pose a framework for pre-fetching data from the database
management system (DBMS) in batches in order to reduce
database access overheads. Similarly, Cheung et al:[16] pro-
pose a framework for delaying database accesses as late as
possible, and sending database access requests only when the
data is needed in the application. Chavan et al:[9] propose
a framework for sending queries asynchronously in order to
improve application performance.
Several proposed frameworks improve application perfor-
mance by analyzing the source code. In our prior work [12,
14], we propose a static analysis framework to detect and
rank database access performance anti-patterns. Develop-
ers can address these anti-patterns based on their priorities.
Cheung et al:[17] leverage static analysis and code synthe-
sis to automatically generate optimal SQL queries accord-
ing to post-conditions and loop invariants. Grechanik et
al:[31] propose a framework that combines both static and
dynamic analysis to prevent database deadlocks. Chaud-
huri et al:[8] use instrumented database access information
to nd database-related performance problems in the code.
Compared to prior studies, we not do propose a new frame-
work. Instead, CacheOptimizer helps developers optimize
the conguration of the frameworks (in particular caching
frameworks) that are already in use in practice today. In-
depth knowledge of a software application is needed for soft-
ware developers to optimally congure such frameworks.@Entity
@Cachable
@Table (name = ‚Äùuser‚Äù)
public class User {
 @Id 
 @Column
 (name=‚Äùuser_id‚Äù)  
 private long userId ; 
 @Column (
 name=‚Äùuser_name‚Äù) 
 private String userName;
     ... other instance 
variables
     ... setter and getter
 methods
}User.java
User user = 
findUserByID(1);Main.java
ORMCache
Layer
Database11 1 1
User(1)1 11
updateUserByID(1);2 2 2
User(1)2
User[] users = 
execute(‚Äúselect * 
from User‚Äù).cache()3 3 3
User(1)3 3 33
User, id = 1, 
name = Peter
User, id = 2, 
name = Jack
updateUserByID(2)4 44
User(1)4User(2)
User(2)Figure 1: An example of simplied Hibernate code,
Hibernate cache conguration code, and Hibernate
cache mechanism. The numbers and the arrows in-
dicate the ow for dierent workloads. The grey
Userobjects in the cache layer means the objects are
invalidated in the cache layer.
2.3 Caching Frameworks
There are many prior studies on cache algorithms and
frameworks. Many cache algorithms such as least recently
used (LRU) [34], and most recently used (MRU) [18] are
widely used in practice for scheduling lower-level caches. For
example, such algorithms are used to improve the perfor-
mance of web applications by caching web pages through
proxies [7, 24]. Most of these caching algorithms operate in
an unsupervised dumb fashion, i.e., these low-level caching
algorithms do not require any application-level knowledge
to operate.
Many modern applications generate dynamic content, which
may be highly variable and large in size, based on data in the
DBMS. Therefore, many low-level cache frameworks are be-
coming less eective. Many recent caching frameworks cache
database accesses at the application level [27,40]. When us-
ing these application-level caching frameworks, developers
have full control of what should be cached in an application.
However, to leverage these caching frameworks eectively,
they must be congured properly.
Unlike most prior studies, CacheOptimizer does not try
to manage cache scheduling. Instead, CacheOptimizer is
designed to help developers optimize the conguration of
application-level caching frameworks, which must be cong-
ured correctly for developers to fully leverage their benets.
3. HIBERNATE AND CACHING MECHA-
NISMS
3.1 Hibernate
CacheOptimizer automatically congure the caching frame-
work for Hibernate-based web applications [21]. Hibernate
is one of the most popular Java frameworks for abstract-
ing database operations. Hibernate abstracts database ac-
cesses as object calls in Java instead of using SQL or JDBC
directly. Hibernate is very popular among developers, be-
cause it helps reduce the amount of boilerplate code and
development time [36]. For instance, a recent survey shows
that among the 2,164 surveyed Java developers, 67.5% use
Hibernate [58] instead of other database abstraction frame-
works (including JDBC).
Figure 1 shows an example of using Hibernate to ab-
stract database accesses in Java. In this example, anno-
tations (e.g., @Entity ,@Table , and @Column ) are added to
User.java to specify the mapping between tables in a rela-tional database and objects in Java. Based on such anno-
tations, Hibernate automatically transforms user records to
user objects and vice versa, and automatically translates the
manipulation of the user object to the corresponding SQL
queries. Hibernate is often used along caching frameworks
like Ehcache [54]. These application-level caching frame-
works aim to improve the performance of database-centric
applications by reducing the number of database accesses.
3.2 Hibernate Caching Mechanism
Most caching frameworks act like an in-memory key-value
store. When using Hibernate, these caching frameworks
would store the database entity objects (objects that have
corresponding records in the DBMS) in memory and assign
each object a unique ID (i.e., the primary key). There are
two types of caches in Hibernate:
Object cache. As shown in workow 1 (Figure 1),
if the requested user object is not in the cache layer,
the object will be fetched from the DBMS. Then, the
user object will be stored in the cache layer and can be
accessed as a key-value pair using its id (e.g., fid: 1,
User objg). If the object is updated, the cached data
would be evicted to prevent a stale read (Workow 2).
To cache database entity objects, developers must add
an annotation @Cachable at the class declaration (as
shown in User.java in Figure 1). Then, alldatabase en-
tity object retrieved by ID (e.g., retrieved using nd-
UserById() ) would be cached. These annotations con-
gure the underlying caching frameworks.
Query cache. The cache mechanism for query cache
is slightly dierent from object cache. For example,
the cached data for a select all query on the user table
(Workow 3) would look as follows:

select * from User !fid: 1; id: 2g
fid: 1; id: 2g!f id: 1;User objg;fid: 2;User objg
The cache layer stores the idsof the objects (i.e., id 1
and 2) that are retrieved by the query, and uses the
ids to nd the cached objects (the corresponding User
obj). Thus, the object cache must be enabled to use
a query cache. When a user object is updated (work-
ow 4), the query cache needs to retrieve the updated
object from the DBMS to prevent a stale read. Thus,
if the queried entity objects are frequently modied,
using a query cache may not be benecial, and may
even hinder performance [26].
To cache query results, developers must call a method
like cache() before executing the query ( Main.java in
Figure 1). Such method is used to congure the un-
derlying caching frameworks.
Adding caches incorrectly can introduce overhead to the
application. Caching a frequently modied object or query
will cause the caching framework to constantly evict and
renew the cache, which not only causes cache renewal over-
head but may also result in executing extra SQL queries.
Therefore, blindly adding caches without understanding the
workload may lead to performance degradation [51].@RequestMapping(value=‚Äùuser/{id}‚Äù, 
method=GET)
public User getUserById(int id){
return findUserById(id);
}
127.0.0.1 [05/Aug/2015:10:38:38 -0400]  ‚ÄúGET /user/1 HTTP/1.1'' 200
127.0.0.1 [05/Aug/2015:10:38:40 -0400]  ‚ÄúGET /user/1 HTTP/1.1'' 200
127.0.0.1 [05/Aug/2015:10:38:42 -0400]  ‚ÄúGET /user/2 HTTP/1.1'' 200
127.0.0.1 [05/Aug/2015:10:38:45 -0400]  ‚ÄúGET /user/1 HTTP/1.1'' 200
127.0.0.1 [05/Aug/2015:10:38:47 -0400]  ‚ÄúGET /user/1 HTTP/1.1'' 200
127.0.0.1 [05/Aug/2015:10:38:50 -0400]  ‚ÄúGET /user/1 HTTP/1.1'' 200+@Cachable
@Entity
public class User{
...
}public User findUserById(int id){
Return entityManager.find(User.class, id);
}
Source code
Web access logsFigure 2: A working example of CacheOptimizer .
The +sign in front of the @Cachable line indicates
that the caching conguration is added by CacheOp-
timizer .
4. CACHE OPTIMIZER
CacheOptimizer optimizes the conguration of caches that
are associated with database accesses that occur for a given
workload. Hence, our approach needs to recover the work-
load of an application then to identify which database ac-
cess occurs within that particular workload. In the following
subsections, we explain each step of the inner workings of
CacheOptimizer in detail using a working example. The in-
put of the working example shown in Figure 2 consists of two
parts: 1) source code of the application and 2) web access
logs. Figure 3 shows an overview of CacheOptimizer .
4.1 Recovering Control and Data Flow Graphs
We rst need to understand the calling and data ow rela-
tionships among methods, and determine which application-
level methods are impacted by database caching (i.e., which
methods eventually lead to a database access). We therefore
extract the call and data ow graphs of the application by
parsing the source code of the application using the Eclipse
JDT. We opt to parse the source code instead of analyzing
the binary since we need to locate the Hibernate annotations
in the source code { such annotations are lost after compiling
the source code to Java byte code. We mark all Hibernate
methods that access the DBMS (e.g., query.execute() ) in the
call and data ow graphs. Such methods are easy to iden-
tify since they are implemented in the same class (i.e., in
theEntityManager and the Query class of Hibernate). Once
such methods are marked, we are able to uncover all the
application-level methods that are likely to be impacted by
optimizing the database cache. In our working example, af-
ter generating the call and data ow graphs, and identifying
the Hibernate database access methods, we would know that
the method getUserById contains one database access, and
the parameter is passed in through a web request.
4.2 Linking Logs to Application-Level Meth-
ods
We recover the workload of the application by mining its
web access logs. We leverage web access logs because of the
following reasons. First, web access logs are typically readily
available without needing additional instrumentation since
many database-centric applications rely on RESTFul web
service (based on HTTP web requests) [49] to accept re-
quests from users [5]. For example, large companies like
IBM, Oracle, Facebook and Twitter all provide RESTFulAPIs1. Second, unlike application logs, web access logs
have a universal structure (the format of all log lines are
the same) [55]. Hence, compared to application logs, web
access logs are easier to analyze and do not usually change
as an application evolves [50].
Web access logs may contain information such as the re-
questor's IP, timestamp, time taken to process the request,
requested method (e.g. GET), and status of the response.
An example web access log may look like:

 
127.0.0.1 [05/Aug/2015:10:38:38 -0400] 1202 \GET
/user/1 HTTP/1.1" 200
This web access log shows that a request is sent from the
local host at August 05, 2015 to get the information of the
user whose ID is 1. The status of the response is 200, and
the application took 1,202 milliseconds to respond to the
request.
In order to know which application-level methods will
be executed for each web request, we use static analysis
to match the web access logs to application-level methods.
CacheOptimizer parses the standard RESTful Web Services
(JAX-RS) specications in order to nd the handler method
for each web request [42]. An example of JAX-RS code is
shown below:
@RequestMapping ( value = "/ user /{ id}", method = GET )
public User getUserById ( int id) {
return findUserById (id );
}
In this example, based on the JAX-RS annotations, we know
that all GET requests with the URL of form \/user/ fidg"
will be handled by the getUserById method.
For every line of web access log, CacheOptimizer looks for
the corresponding method that handles that web request.
After analyzing all the lines of web access logs, CacheOp-
timizer generates a list of methods (and their frequencies)
that are executed during the run of the application.
In our working example, we map every line of web access
log to a corresponding web request handling method, i.e.,
getUserById method.
4.3 Database Access Workload Recovery
We want to determine which database accesses are exe-
cuted for the workload. Since application-level cache highly
depends on the details of the database accesses, we need to
recover the types of the database access (e.g., a query versus
a select/insert/update/delete of a database entity object by
id) and the data that is associated with the database access
(e.g., accessed tables and parameters). Such detailed in-
formation of database accesses helps us in determining the
optimal cache congurations. We rst link each web access
log to its request-handler method in the code (as described
in Section 4.2). Therefore, for each workload, we know the
list of request-handler-methods that are executed (i.e., entry
points into the application). Then, we conduct a call graph
and static ow-insensitive interprocedural taint analysis on
each web-request-handler method, using the generated call
and data ow graphs (as describe in Section 4.1).
Our algorithm for recovering the database access work-
load is shown in Algorithm 1. For each web-request-handler-
method, we identify all possible database accesses by travers-
1http://www.programmableweb.com/apis/directoryPossible caching
locations Source 
code
Web 
access
 logsFinding database 
access codeCall and data 
flow graphs1. Recovering control 
and data flow graphs3. DB 
access
workload 
recoveryDB access 
tables
DB access 
params
DB access 
types4. Identifying 
possible cache 
locations
Possible caching
locations
5. Evaluating 
potential cache 
benefit using
coloured petri net Miss ratio > threshold Add cacheYesFor each 
cache locationAnalysis
Data
Condition
in a loop6. Configuring the 
caching frameworks2. Linking logs to 
application-level
methodsRecovered
 request-handler 
methodPossible cache
locationsFigure 3: Overview of CacheOptimizer .
Algorithm 1: Our algorithm for recovering database ac-
cesses.
Input : CG, DG, Mthd /* call graph, data ow graph,
the request handler method */
Output : AccessInfo, Params /* accessed DB tables
and DB func type (query or key-value
lookup) and parameter of the request */
1AccessInfo ;;Params ;;
2/* Traverse the call graph from Mthd */
3foreach path2CG.ndAllPathFrom(Mthd) do
4 foreach call2pathdo
5 ifisDBCall(call) then
6 AccessInfo AccessInfo[
(getAccessedTable (call); getMthdType (call));
7 end
8 end
9end
10/* Track the usage of the input params */
11foreach param2Mthd.getParams() do
12 foreach path2DG.ndAllPathFrom(param) do
13 foreach node2pathdo
14 node pointToAnalysis (node)
15 ifusedInDBAccessCall(node) then
16 Params 
Params[(dbAccessCall; node )
17 end
18 end
19 end
20end
ing all paths in the call graph, and recording the type of
the database access. After recovering the database access,
we traverse the data ow graph of each web-request-handler
method to track the usage of the parameters that are passed
in through the web requests. We want to see if the pa-
rameters are used for retrieving/modifying the data in the
DBMS. Such information helps us better calculate the opti-
mized cache conguration. For example, we would be able
to count the number of times a database entity object is
retrieved (e.g., according to the id that is specied in the
web requests), or how many times a query is executed (e.g.,
according to the search term that is specied in the web re-
quest). For POST, PUT, and DELETE requests, we track
the URL (e.g., POST /newUser/1) to which the request is
sent, which usually species which object the request is up-
dating. If there is no parameter specied, then we assume
that the request may modify any of the objects to be con-
servative on our advice on enabling the cache.
In our working example, we recover a list of database ac-
cesses. All of the accesses read data from the User table.
In ve of the accesses, the parameter is 1 and in one of the
accesses, the parameter is 2.
4.4 Identifying Possible Caching LocationsAfter our static analysis step, we recover the location of all
the database access methods in the code, and the mapping
between Java classes and tables in the DBMS. Namely, we
obtain all potential locations for adding calls to the cache
conguration APIs. Thus, if a query needs to be cached,
we can easily nd the methods in the code that execute the
query. If we need to add object caches, we can easily nd
the class that maps to the object's corresponding table in
the DBMS. In our example, we identify that the class User
is a possible location to place an object cache. Our static
analysis step is very fast (23{177 seconds on a machine with
16G RAM and Intel i5 2.3GHz CPU) for our studied appli-
cations (see Table 1), and is only required when deploying a
new release. Thus, the execution time has minimal impact.
We use ow-insensitive static analysis approaches to iden-
tify possible caching locations, because it is extremely di-
cult to recover precise dynamic code execution paths with-
out introducing additional overhead to the application (e.g.,
using instrumentation). During our static analysis step, if
we choose to assign dierent probabilities to code branches,
we may under-count or over-count reads and writes to the
DBMS. Under-counting reads may result in failing to cache
frequently read objects, which has little or no negative per-
formance impact (i.e., the same as not adding a cache).
However, under-counting writes may result in caching fre-
quently modied objects and thus has signicant negative
eects on performance. In contrast, we choose a conser-
vative approach by considering all possible code execution
paths (over-counting) to avoid under-counting reads and
writes. We may over-count reads and writes to the DBMS,
but over-counting reads has minimal performance impact,
since in such cases we would only place cache conguring
APIs on objects that are rarely read from the DBMS; over-
counting writes means that we may miss some objects that
should have been cached, but will not aect the system
performance (the same as adding no cache). Hence, our
conservative choice by intentionally considering all possi-
ble code execution paths (over-counting) ensures that the
caching suggestions would not have negative performance
impact after placing the suggested caches. Note that there
may be some memory costs when turning on the cache (i.e.,
use more memory), and in RQ2 we evaluate the gain of our
approach when considering such costs.
4.5 Evaluating Potential Cache BeneÔ¨Åts Using
Coloured Petri Net
After linking the logs to handler methods and recovering
the database accesses, CacheOptimizer then calculates the
potential benets of placing a cache on each database ac-P1P2
P3P4
P5P1P2
P3P4
P5
P1P2
P3P4
P51) 2)
3)T1 T1
T1T2 T2
T2Figure 4: An example of modeling potential cache
benets using a coloured Petri net. A red token
represents a read to a specic database entity ob-
ject (e.g., ndUserById(1) ), and a blue token repre-
sents write to a specic database entity object ( up-
dateUserById(1) ).
cess call. We use Petri nets [45], a mathematical modeling
languages for distributed applications, to model the activ-
ity of caches such as cache renewal and invalidation. Petri
nets allow us to model the interdependencies, so the reached
caching decisions are global optimal, instead of focusing on
top cache accesses (greedy). Petri nets model the transition
of states in an application, and a net contains places ,tran-
sitions , and arcs. Places represent conditions in the model,
transitions represent events, and arcs represent the ow re-
lations among places. Formally, a Petri net Ncan be dened
as:
N= (P; T; A ) and A(PT)[(TP);
where Pis the set of places, Tis the set of transitions, and A
is the set of arcs. Places may contain tokens , which represent
the execution of the net. Any distributions of the tokens
in the places of a net represent a set of congurations . A
limitation of Petri nets is that there is no distinction between
tokens. However, to use Petri nets to evaluate potential
cache benets, we need to model dierent data types (e.g.,
a Hibernate query versus an entity lookup by id) and values
(e.g., query parameter). Thus, we use an extension of Petri
nets, called coloured Petri net (CPN) [33]. In a CPN, tokens
can have dierent values, and the values are represented
using colours. Formally, a CPN can be dened as:
CPN = (P; T; A; ; C; N; E; G; I );
where P,T, and Aare the same as in Petri nets.  repre-
sents the set of all possible colours (all possible tokens), C
maps Pto colours in  (e.g., specify the types of tokens that
can be in a place), and Nis a node function that maps A
into ( PT)[(TP).Eis the arc expression function, G
is the guard function that maps each transition into guard
expressions (e.g., boolean), and nally Irepresents an ini-
tialization function that maps each place to a multi-set of
token colours.
In our CPN (shown in Figure 4), we dene Pto be the
states of the data in the cache. P3 is a repository that
stores the total number of database accesses, P4 stores the
total number of cache hits, and P5 stores the number of
invalidated caches. P2 is an intermediate place for deter-
mining whether the data would be cached or invalidated.
We dene Tto be all database accesses that are recovered
from the logs. We dene  to distinguish the type of the
database access call (e.g., read/write using ids or queries),and the parameters used for the access (obtained using Algo-
rithm 1). Thus, our Cdenes that P4 can only have colours
of database access calls that are reads, and P1, P2, P3, and
P5 may contain all colours in . The transition function
on T1 always forwards the tokens in the initial place P1 to
P2 and P3. There are two guard functions on T2, where
one allows a token to be moved to P4 if there are two or
more tokens of the same colour in P2 (i.e., multiple reads to
the same data, so a cache hit), and another guard function
makes sure that if there is a write in P2, all the same write
tokens and the corresponding read tokens are moved to P5
(e.g., the cache is invalidated).
In our example (Figure 4), we let red tokens represent the
database access call ndUserById(1) , and blue tokens repre-
sent updateUserById(1) . In (1), there are two red tokens, and
T1 is triggered, so the two red tokens are stored in P2 and
P3. Since there are two red tokens in P2, T2 is triggered,
and moves one red token to P4 (a cache hit). The resulting
CPN is shown in (2). When a blue token appears in P1, T1
is triggered and moves the blue token to both P2 and P3.
Since there is a blue token in P2, T2 is triggered, and we
move both the red and blue token to P5 (cache invalidation).
The nal resulting Petri net is shown in (3). Note that T2
acts slightly dierent for tokens that represent query calls.
When an object is updated, the query cache needs to re-
trieve the updated object from the DBMS to prevent a stale
read. Thus, to model the behaviour, T2 would be triggered
to move the query token to P5 from P2 if we see any token
that represents a modication to the query table.
We use the recovered database accesses of the workload to
execute the CPN. For all tokens that represent the database
access to the same data (e.g., a read and write to user by id
1), we examine their total counts in P3 and P4 to calculate
the miss ratio (MR) of the cache. MR can be calculated as
one minus the total number of cache hits in P4 divided by
the total number of calls in P3. We choose MR because it
is used in many prior studies to evaluate the eectiveness
of caching (e.g., [24,43,62]). If MR is too high, caching the
data would not give any benet. For example, if a table is
constantly updated, then data in that table should not be
cached. Thus, we dene a threshold to decide whether a
database access call should be cached. In our CPN, if MR
is smaller than 35%, then we place the cache conguration
code for the corresponding query (query cache) or table (ob-
ject cache). Since object cache must be turned on to utilize
query cache, we enable query cache only if the MR of the
object cache is under the threshold. Such that, there would
not exist conicting decisions for object and query cache.
We choose 35% to be more conservative on enabling caches
so that we know the cached data would be invalidated less
frequently (lower cache renewal cost). We also vary MR to
45% and do not see any dierence in terms of the suggested
cache congurations. However, future work should further
investigate the impact of MR.
4.6 ConÔ¨Åguring the Caching Frameworks
CacheOptimizer automatically adds the appropriate calls
to the cache conguration API. Since the locations that
require adding cache conguration APIs may be scattered
across the code, CacheOptimizer helps developers reduce
manual eorts by automatically adding these APIs to the
appropriate locations. For example, if the query that is
executed by the request \/user/?query=Peter" should becached, CacheOptimizer would automatically call the caching
framework's API to cache the executed query in the corre-
sponding handler method searchUserByName . In our exam-
ple shown in Figure 2, the miss ratio of caching objects in
the User class is 0 :33, which is smaller than our threshold
0:35. CacheOptimizer automatically adds the @Cachable
annotation to the source code to enable cache for the User
class.
5. EV ALUATION
In this section, we present the evaluation of CacheOpti-
mizer . We rst discuss the applications that we use for our
evaluation. Then we focus on two research questions: 1)
what is the performance improvement after using CacheOp-
timizer ; and 2) what is the gain of CacheOptimizer when
considering the cost of such caches.
Experimental Setup. We evaluate CacheOptimizer on
three open-source web applications: Pet Clinic [44], Cloud
Store [20], and OpenMRS [41]. Table 1 shows the detailed
information of these three applications. All three applica-
tions use Hibernate as the underlying framework to access
database, and use MySQL as the DBMS. We use Tomcat as
our web server, and use Ehcache as our underlying caching
framework. Pet Clinic, which is developed by Spring [52],
aims to provide a simple yet realistic design of a web ap-
plication. Cloud Store is a web-based e-commerce appli-
cation, which is developed mainly for performance testing
and benchmarking. Cloud Store follows the TPC-W perfor-
mance benchmark standard [1]. Finally, OpenMRS is large-
scale open-source medical record application that is used
worldwide. OpenMRS supports both web-based interfaces
and RESTFul services.
We use one machine each for the DBMS (8G RAM, Xeon
2.67GHz CPU), web server (16G RAM, Intel i5 2.3GHz),
and JMeter load driver (12G RAM, Intel Quad 2.67GHz).
The three machines are all connected on the same network.
We use performance test suites to exercise these applications
when evaluating CacheOptimizer . Performance test suites
aim to mimic the real-life usage of the application and en-
sure that all of the common features are covered during the
test [4]. Thus, for our evaluation, performance test suites are
a more appropriate and logical choice over using functional
tests. We use developer written tests for Pet Clinic [23], and
work with BlackBerry developers on creating the test cases
for the other applications. For Cloud Store, we create test
cases to cover searching, browsing, adding items to shopping
carts, and checking out. For OpenMRS, we use its REST-
Ful APIs to create test cases that are composed of searching
(by patient, concept, encounter, and observation etc), and
editing/adding/retrieving patient information. We also add
randomness to our test cases to better simulate real-world
workloads. For example, we add randomness to ensure that
some customers may checkout, and some may not. We use,
for our performance tests, the MySQL backup les that are
provided by Cloud Store and OpenMRS developers. The
backup le for Cloud Store contains data for over 5K pa-
tients and 500K observations. The backup le for Cloud
Store contains about 300K customer data and 10K items.
RQ1: What is the performance improvement after
using CacheOptimizer?
Motivation. In this RQ, we want to examine how well
the performance of the studied database-centric web appli-Table 1: Statistics of the studied applications.
Total lines Number of
of code Java les
Pet Clinic 3.8K 51
Cloud Store 35K 193
OpenMRS 3.8M 1,890
Table 2: Performance improvement (throughput)
against NoCache after applying dierent cache con-
gurations.
Throughput
NoCache CacheOptimizer CacheAll DefaultCache
Pet Clinic 98.7 125.1 (+27%) 108.4 (+10%) |
Cloud Store 110.7 263.4 (+138%) 249.3 (+125%) 114.7 (+4%)
OpenMRS 21.3 30.8 (+45%) 25.5 (+20%) 27.7 (+30%)
cations can be improved when using CacheOptimizer to con-
gure the caching framework.
Approach. We run the three studied applications using the
performance test suites under four dierent sets of cache con-
gurations: 1) without any cache conguration ( NoCache ),
2) with default cache conguration ( DefaultCache , cache
congurations that are already in the code, which indicates
what developers think should be cached), 3) with enabling
all possible caches ( CacheAll ), and 4) with congurations
that are added by CacheOptimizer . We compare the perfor-
mance of the applications when congured using these four
dierent sets of cache congurations. We work with perfor-
mance testing experts from BlackBerry to ensure that our
evaluation steps are appropriate, accurate, and realistic. We
use throughput to measure the performance. The through-
put is measured by calculating the number of requests per
second throughout the performance test. A higher through-
put shows the eectiveness of the cache conguration, as
more requests can be processed within the same period of
time.
There may exist many possible locations to place the calls
to the cache conguration APIs. Hence, conguring the
caching framework may require extensive and scattered code
changes, which can be a challenging and time-consuming
task for developers. Therefore, to study the eectiveness of
CacheOptimizer and how it helps developers, we also com-
pare the number of cache congurations that are added by
CacheOptimizer relative to the total number of all possible
caching congurations that could be added, and the number
of cache congurations that exist in DefaultCache .
Results. CacheOptimizer outperforms DefaultCache
and CacheAll in terms of application performance
improvement. Table 2 shows the performance improve-
ment of the applications under four sets of congurations.
We use NoCache as a baseline, and calculate the throughput
improvement after applying CacheOptimizer ,CacheAll , and
DefaultCache . The default cache conguration of Pet Clinic
does not enable any cache. Therefore, we only show the
performance improvement of DefaultCache for Cloud Store
and OpenMRS. Using CacheOptimizer , we see a through-
put improvement of 27%, 138% and 45% for Pet Clinic,
Cloud Store and OpenMRS, respectively. The throughput
improvement of applying CacheOptimizer is always higher
than that of DefaultCache and CacheAll for all the stud-
ied applications. Figure 5 further shows the cumulative
throughput overtime. We can see that for the three stud-
ied applications, the throughput is about the same at the05010015005000100001500020000
Time in secNumber of handled requests (cumulative)CacheOptimizerCacheAllNo Cache(a) Pet Clinic.
050100150200250300350010000200003000040000
Time in secNumber of handled requests (cumulative)CacheOptimizerCacheAllDefault CacheNo Cache (b) Cloud Store.
02004006008000100002000030000
Time in secNumber of handled requests (cumulative)CacheOptimizerCacheAllDefault CacheNo Cache (c) OpenMRS.
Figure 5: Number of handled requests overtime (cumulative).
Table 3: Total number of possible places to add
cache in the code, and the number of location that
are enabled by CacheOptimizer and that exist in the
DefaultCache .
Object Cache Query Cache
Total CacheOptimizer DefaultCache Total CacheOptimizer DefaultCache
Pet Clinic 11 6 (55%) 0 4 3 (75%) 0
Cloud Store 33 2 (6%) 10 (30%) 24 9 (38%) 1 (4%)
OpenMRS 112 16 (14%) 7 (6%) 229 2 (0.9%) 0
beginning regardless of us adding cache or not. However,
as more requests are received, the benet of caching be-
comes more signicant. The reason may be that initially
when the test starts, the data is not present in the cache.
CacheOptimizer is able to discover the more temporal local-
ities (reuse of data) in the workload and help developers con-
gure the application-level cache more optimally. Therefore,
as more requests are processed, frequently accessed data is
then cached, which signicantly reduces the overhead of fu-
ture accesses. We see a trend that the longer the test runs,
the more benet we get from adding cache conguration
code using CacheOptimizer . We also observe that the per-
formance of Cloud Store with DefaultCache is close to the
performance with no cache. Such an observation shows in
some instances developers do not have a good knowledge of
optimizing cache conguration in their own application.
CacheOptimizer enables a small number of caches
to improve performance. CacheOptimizer can help de-
velopers change cache congurations quickly without man-
ually investigating a large number of possible cache loca-
tions. Table 3 shows the total number of possible locations
to place calls to object and query cache APIs in the studied
applications. We also show the number of CacheOptimizer
enabled caches, and the number of DefaultCache enabled
caches. CacheOptimizer suggests adding object cache con-
guration APIs to a fraction (6{55%) of the total number
of possible cache locations. In OpenMRS and Cloud Store,
where there are more Hibernate queries, CacheOptimizer is
able to improve performance by enabling 0.9% and 38% of
all the possible caches, respectively. For the object cache
of Cloud Store, CacheOptimizer even suggests enabling a
smaller number of caches than DefaultCache . For large ap-
plications like OpenMRS with 112 possible object caches
and 229 possible query caches, manually identifying the op-
timized cache conguration is time-consuming and may not
even be possible.Discussion. In our evaluation of CacheOptimizer , we ob-
serve a larger improvement in Cloud Store. After a manual
investigation, we nd that CacheOptimizer caches the query
results that contain large binary data, e.g., pictures. Since
the sizes of pictures are often larger, caching them signi-
cantly reduces the network transfer time, and thus results
in a large performance improvement. We see less improve-
ment when using DefaultCache , because most database ac-
cess calls are done through queries (like workow 3 in Fig-
ure 1), while the default cache congurations of Cloud Store
are mostly for object cache (Table 3). Thus, enabling only
object caches does not help improve performance. In Open-
MRS, both CacheOptimizer and DefaultCache cache some
database entity objects that are not often changed. How-
ever, CacheOptimizer is able to identify more object caches
and queries that should be cached to further improve per-
formance. We also see that the overhead of CacheAll causes
OpenMRS to run slower when compared to DefaultCache .
In Pet Clinic, we nd that caching the owner information sig-
nicantly improves the performance of searches. Moreover,
since the number of vets in the clinic is often unchanged,
caching the vet information also speeds up the application.

Adding cache conguration code, as suggested by
CacheOptimizer , improves throughput by 27{138%,
which is higher than using the default cache congura-
tion and enabling all possible caches. The sub-optimal
performance of DefaultCache shows that developers have
limited knowledge of adding cache conguration.
RQ2: What is the gain of CacheOptimizer when
considering the cost of such caches?
Motivation. In the previous RQ, we see that CacheOp-
timizer helps improve application throughput signicantly.
However, caching may also bring some memory overhead
to the application, since we need to store cached objects in
the memory. As a result, in this RQ, we want to evaluate
CacheOptimizer -suggested cache conguration when consid-
ering both the cost (increase in memory usage) and the ben-
et (improvement in throughput).
Approach. In order to evaluate CacheOptimizer when con-
sidering both benet and cost, we dene the gain of applying
a conguration as:
Gain (c) =Benefit (c) Cost (c); (1)where cis the cache conguration, Gain (c) is the gain of
applying c, while Benefit (c) and Cost (c) measure the ben-
et and the cost, respectively, of applying c. In our case
study, we measure the throughput improvement in order to
quantify the benet of caching, and we measure the memory
overhead in order to quantify the cost of caching. We use the
throughput and memory usage when no cache is added to
the application as a baseline. Thus, Benefit (c) and Cost (c)
are dened as follows:
Benefit (c) =TP(c) TP(no cache ); (2)
Cost (c) =MemUsage (c) MemUsage (no cache );(3)
where TP(c) is the average number of processed requests
per second with cache conguration c, and MemUsage (c) is
the average memory usage with cache conguration c.
Since the throughput improvement and the memory over-
head are not in the same scale, the calculated gain by Equa-
tion 1 may be biased. Therefore, we linearly transform both
Benefit (c) and Cost (c) into the same scale by applying min-
max normalization, which is dened as follows:
x0=(x xmin)
(xmax xmin); (4)
where xandx0are the values of the metric before and af-
ter normalization, respectively; while xmax andxminare
the maximum and the minimum values of the metric, re-
spectively. We note that if one wants to compare the gain
of applying multiple congurations, the maximum and the
minimum values of the metric are calculated by considering
all the values of the metrics across the dierent congura-
tions, including having no cache. For example, if one would
like to compare the gain of applying CacheOptimizer and
CacheAll ,throughput max is the maximum throughput of
applying CacheOptimizer ,CacheAll , and NoCache .
To evaluate CacheOptimizer , in this RQ, we compare the
gain of applying CacheOptimizer ,CacheAll , and Default-
Cache against NoCache . The larger the gain, the better the
cache conguration. If the gain is larger than 0, the cache
conguration is better than using NoCache . In order to un-
derstand the gain of leveraging cache conguration through-
out the performance tests, we split each performance test
into dierent periods. Since a performance test with dier-
ent cache congurations runs for a dierent length of time
(see Figure 5), we split each test by each thousand of com-
pleted requests. For each period, we calculate the gain of
applying CacheOptimizer ,CacheAll , and DefaultCache .
We study whether there is a statistically signicant dier-
ence in gain, between applying CacheOptimizer andCacheAll ,
and between applying CacheOptimizer and DefaultCache .
To do this we use the Mann-Whitney U test [39] on the
gains , as the gains may be highly skewed. Since the Mann-
Whitney U test is a non-parametric test, it does not have
any assumptions on the distribution. A p-value smaller than
0.05 indicates that the dierence is statistically signicant.
We also calculate the eect sizes in order to quantify the
dierences in gain between applying CacheOptimizer and
CacheAll , and between applying CacheOptimizer and De-
faultCache . Unlike the Mann-Whitney U test, which only
tells us whether the dierence between the two distributions
is statistically signicant, the eect size quanties the dier-
ence between the two distributions. Since reporting only the
statistical signicance may lead to erroneous results (i.e., ifTable 4: Comparing the gain of the application un-
der three dierent congurations: CacheOptimizer ,
CacheAll , and DefaultCache
gain(CacheOptimizer )> gain(CacheOptimizer )>
gain(CacheAll )? gain(DefaultCache )?
p-value Cli's d p-value Cli's d
Pet Clinic <<0.001 0.81 (large) | |
Cloud Store <0.01 0.32 (small) <<0.001 0.61 (large)
OpenMRS <<0.001 0.95 (large) <<0.001 0.95 (large)
the sample size is very large, the p-value are likely to be
small even if the dierence is trivial) [35], we use Cli's d
to quantify the eect size [19]. Cli's d is a non-parametric
eect size measure, which does not have any assumption of
the underlying distribution. Cli's d is dened as:
Cli's d =#(xi> x j) #(xi< x j)
mn; (5)
where # is dened the number of times, and the two distri-
butions are of the size mandnwith items xiandxj, re-
spectively. We use the following thresholds for Cli's d [19]:
eect size =8
>><
>>:trivial if Cli's d <0.147
small if 0.147 Cli's d <0.33
medium if 0.33Cli's d <0.474
large if 0.474 Cli's d
Results. CacheOptimizer outperforms DefaultCache
and CacheAll when considering the cost of cache.
Table 4 shows the result of our Mann-Whitney U test and
Cli's d value when comparing the gainof applying CacheOp-
timizer with that of CacheAll and DefaultCache . We nd
that in all three studied applications, the gain ofCacheOpti-
mizer is better than the gain ofCacheAll andDefaultCache
(statistically signicant). We also nd that the eect sizes of
comparing CacheOptimizer with CacheAll ongain are large
for Pet Clinic (0 :81) and OpenMRS (0 :95). The only excep-
tion is Cloud Store, where the Cli's d value indicates that
the eect of gain is small (0 :32) when comparing CacheOp-
timizer with CacheAll . On the other hand, when compared
toDefaultCache ,CacheOptimizer has a large eect size for
both Cloud Store and OpenMRS.
Discussion. We investigate the memory overhead of ap-
plying CacheOptimizer ,CacheAll , and DefaultCache . We
use the Mann-Whitney U test and measure eect sizes us-
ing Cli's d to compare the memory usage between applying
CacheOptimizer and the memory usage of having no cache,
CacheAll , and DefaultCache , respectively. The memory us-
age of applying CacheOptimizer and having no cache is
statistically indistinguishable for Pet Clinic and OpenMRS;
while for Cloud Store, applying CacheOptimizer has statisti-
cally signicantly more memory usage than having no cache
with a large eect size (0 :78). This may explain why we see
larger throughput improvement in Cloud Store. For Open-
MRS, the memory usage of applying CacheOptimizer and
DefaultCache is statistically indistinguishable. Finally, when
comparing CacheOptimizer with CacheAll , we nd that for
Pet Clinic and Cloud Store, the dierence in memory us-
age is statistically indistinguishable; while for OpenMRS,
CacheOptimizer uses statistically signicantly less memory
than CacheAll (p-value <0.01) with an eect size of 0.61
(large eect). Nevertheless, after considering both the im-
provement and cost, CacheOptimizer out-performs all other
cache congurations.


	When considering both the benet (throughput improve-
ment) and cost (memory overhead), the gain of applying
CacheOptimizer is statistically signicantly higher than
CacheAll and DefaultCache.
6. THREATS TO V ALIDITY
External Validity. We only evaluated CacheOptimizer on
three applications, so our ndings may not generalize to
other applications. We choose the studied applications with
various sizes across dierent domains to improve the gen-
eralizability. However, evaluating CacheOptimizer on other
applications would further show the generalizability of our
approach. We implement CacheOptimizer specically for
Hibernate-based web applications. However, the approach
inCacheOptimizer should be applicable to applications us-
ing dierent object-relational mapping frameworks or other
database abstraction technologies. For example, our ap-
proach for recovering the database accesses from logs may
also be used by non-Hibernate based applications. With mi-
nor modications (e.g., changes are needed to the denitions
of the tokens and transition functions in the coloured Petri
net), CacheOptimizer can be leveraged to improve cache
congurations of other applications.
Construct Validity. The performance benets of caching
highly depends on the workloads. Thus, we use performance
tests to evaluate CacheOptimizer . It is possible that the
workload from the performance tests may not be represen-
tative enough for eld workload. However, CacheOptimizer
does not depend on a particular workload, nor do we have
any assumption on the workload when conducting our ex-
periments. CacheOptimizer is able to analyze any given
workload and nd the optimal cache conguration for dif-
ferent workloads. If the workload changes greatly and the
cache conguration is no longer optimal, CacheOptimizer
can save developers' time and eort by automatically nd-
ing a new optimal cache conguration. For example, devel-
opers can feed their eld workloads on a weekly or monthly
basis, and CacheOptimizer would help developers optimize
the conguration of their caching frameworks. To maximize
the benet of caching, our approach aims to \overt" the
cache congurations to a particular workload. Thus, simi-
lar to other caching algorithms or techniques, our approach
will not work if the workload does not contain any repetitive
reads from the DBMS.
Our approach for recovering the database access.
Prior research leverages control ow graphs to recover the
executed code paths using logs [61]. We do not leverage con-
trol ow graphs to recover the database accesses from web
access logs for two reasons. First, as a basic design principal
of RESTFul web services, typically one web-request-handing
method maps to one or very few database accesses [32, 49].
Second, although leveraging control ows may give us richer
information about each request, it is impossible to know
which branch would be executed based on web access logs.
Heuristics may be used to calculate the possibility of tak-
ing dierent code paths. However, placing the cache incor-
rectly can even cause performance degradation. Thus, to
be conservative when enabling caching and to ensure that
CacheOptimizer would always help improve performance,we consider all possible database access calls. Our over-
estimation ensures that CacheOptimizer would not cache
data that has a high likelihood of being frequently modied,
so the CacheOptimizer added cache congurations should
not negatively impact on the performance. Future research
should consider the use of control ow information for opti-
mizing the cache congurations.
Cache concurrency level. There are dierent cache con-
currency levels, such as read-only and read/write. In this pa-
per we only consider the default level, which is read/write.
Read/write cache concurrency strategy is a safer choice if
the application needs to update cached data. However, con-
sidering other cache concurrency levels may further improve
performance. For example, read-only caches may perform
better than read/write cache if the cached data is never
changed. Future research should to add cache concurrency
level information to CacheOptimizer when trying to opti-
mize cache conguration.
Distributed cache environment. Cache scheduling is
a challenging problem in a distributed environment due to
cache concurrency management. Most caching frameworks
provide dierent algorithms or mechanisms to handle such
issues. Since the goal of CacheOptimizer is to instruct these
caching frameworks on what to cache, we rely on the un-
derlying caching frameworks for cache concurrency manage-
ment. However, the benet of using CacheOptimizer may
not be as pronounced in a distributed environment.
7. CONCLUSION
Modern large-scale database-centric web applications of-
ten leverage dierent application-level caching frameworks,
such as Ehcache and Memcached, to improve performance.
However, these caching frameworks are dierent from tra-
ditional lower-level caching frameworks, because developers
need to instruct these application-level caching frameworks
about what to cache. Otherwise these caching frameworks
are not able to provide any benet. In this paper, we propose
CacheOptimizer , an automated lightweight approach that
determines what should be cached in order to utilize such
application-level caching frameworks for Hibernate-based web
applications. CacheOptimizer combines static analysis of
source code and logs to recover the database accesses, and
uses a coloured Petri net to model the most eective caching
conguration for a workload. Finally, CacheOptimizer auto-
matically updates the code with the appropriate cache con-
guration code. We evaluate CacheOptimizer on three open
source applications (Pet Clinic, Cloud Store, and Open-
MRS). We nd that CacheOptimizer improves the through-
put of the entire application by 27{138% (compared to De-
faultCache andCacheAll ), and the increased memory usage
is smaller than the applications' default cache conguration
and turning on all caches. The sub-optimal performance of
the default cache congurations highlights the need for au-
tomated techniques to assist developers in optimizing the
cache conguration of database-centric applications.
8. REFERENCES
[1] Transactional web e-commerce benchmark.
http://www.tpc.org/tpcw/. Last accessed March 3
2016.
[2] T. , L. Jin, X. Fan, and Y. Zhou. Hey, you have given
me too many knobs! understanding and dealing withover-designed conguration in system software. In
Proceedings of the 10th Joint Meeting of the European
Software Engineering Conference and the ACM
SIGSOFT Symposium on the Foundations of Software
Engineering , ESEC/FSE '15, 2015.
[3] M. Altinel, C. Bornh ovd, S. Krishnamurthy,
C. Mohan, H. Pirahesh, and B. Reinwald. Cache
tables: Paving the way for an adaptive database
cache. In Proceedings of the 29th International
Conference on Very Large Data Bases , VLDB '03,
pages 718{729, 2003.
[4] R. Binder. Testing Object-oriented Systems: Models,
Patterns, and Tools . Addison-Wesley, 2000.
[5] J. Bloomberg. The Agile Architecture Revolution: How
Cloud Computing, REST-Based SOA, and Mobile
Computing Are Changing Enterprise IT . Wiley, 2013.
[6] I. T. Bowman and K. Salem. Optimization of query
streams using semantic prefetching. ACM Trans.
Database Syst. , 30(4):1056{1101, Dec. 2005.
[7] K. S. Candan, W.-S. Li, Q. Luo, W.-P. Hsiung, and
D. Agrawal. Enabling dynamic content caching for
database-driven web sites. In Proceedings of the 2001
ACM SIGMOD International Conference on
Management of Data , SIGMOD '01, pages 532{543,
2001.
[8] S. Chaudhuri, V. Narasayya, and M. Syamala.
Bridging the application and DBMS proling divide
for database application developers. In VLDB , 2007.
[9] M. Chavan, R. Guravannavar, K. Ramachandra, and
S. Sudarshan. Program transformations for
asynchronous query submission. In Proceedings of the
2011 IEEE 27th International Conference on Data
Engineering , ICDE '11, pages 375{386, 2011.
[10] F. Chen, J. Grundy, J.-G. Schneider, Y. Yang, and
Q. He. Stresscloud: A tool for analysing performance
and energy consumption of cloud applications. In
Proceedings of the IEEE/ACM 37th IEEE
International Conference on Software Engineering ,
ICSE '15, pages 721{724, May 2015.
[11] T.-H. Chen. Improving the quality of large-scale
database-centric software systems by analyzing
database access code. ICDE '15, pages 245{249, 2015.
[12] T.-H. Chen, S. Weiyi, A. E. Hassan, M. Nasser, and
P. Flora. Detecting problems in the database access
code of large scale systems - an industrial experience
report. ICSE '16, 2016.
[13] T.-H. Chen, S. Weiyi, h. M. Jiang, A. E. Hassan,
M. Nasser, and P. Flora. Finding and evaluating the
performance impact of redundant data access for
applications that are developed using object-relational
mapping frameworks. IEEE Transactions on Software
Engineering , 2016.
[14] T.-H. Chen, S. Weiyi, Z. M. Jiang, A. E. Hassan,
M. Nasser, and P. Flora. Detecting performance
anti-patterns for applications developed using
object-relational mapping. ICSE, 2014.
[15] T.-H. Chen, S. Weiyi, J. Yang, A. E. Hassan, M. W.
Nasser, Godfrey, Mohamed, and P. Flora. An
empirical study on the practice of maintaining
object-relational mapping code in Java systems. In
Proceedings of the 13th International Conference on
Mining Software Repositories , MSR '16, pages165{176, 2016.
[16] A. Cheung, S. Madden, and A. Solar-Lezama. Sloth:
Being lazy is a virtue (when issuing database queries).
InProceedings of the 2014 ACM SIGMOD
International Conference on Management of Data ,
SIGMOD '14, 2014.
[17] A. Cheung, A. Solar-Lezama, and S. Madden.
Optimizing database-backed applications with query
synthesis. PLDI '13, pages 3{14, 2013.
[18] H.-T. Chou and D. J. DeWitt. An evaluation of buer
management strategies for relational database
systems. VLDB '85, pages 127{141, 1985.
[19] N. Cli. Dominance statistics: Ordinal analyses to
answer ordinal questions. Psychological Bulletin ,
114(3):494{509, Nov. 1993.
[20] CloudScale. Cloud store.
http://www.cloudscale-project.eu/, 2016. Last
accessed July 25 2016.
[21] J. Community. Hibernate. http://www.hibernate.org/,
2016. Last accessed July 26 2016.
[22] S. Dar, M. J. Franklin, B. T. J onsson, D. Srivastava,
and M. Tan. Semantic data caching and replacement.
VLDB '96, pages 330{341, 1996.
[23] J. Dubois. Improving the performance of the
spring-petclinic sample application.
http://blog.ippon.fr/2013/03/14/
improving-the-performance-of-the-spring- n
petclinic-sample-application-part-4-of-5/, 2016. Last
accessed July 25 2016.
[24] L. Fan, P. Cao, J. Almeida, and A. Z. Broder.
Summary cache: A scalable wide-area web cache
sharing protocol. IEEE/ACM Trans. Netw. ,
8(3):281{293, June 2000.
[25] FastCompany. How one second could cost Amazon 16
billion sales. http://www.fastcompany.com/1825005/
how-one-second-could-cost-amazon-16-billion-sales,
2012. Last accessed March 3 2016.
[26] V. Ferreira. Pitfalls of the Hibernate second-level /
query caches. https:
//dzone.com/articles/pitfalls-hibernate-second-0. Last
accessed March 3 2016.
[27] B. Fitzpatrick. Distributed caching with memcached.
Linux J. , 2004(124), Aug. 2004.
[28] G. Giannikis, G. Alonso, and D. Kossmann. Shareddb:
Killing one thousand queries with one stone. Proc.
VLDB Endow. , 5(6):526{537, Feb. 2012.
[29] Gleanster. Application performance starts with
database performance analysis.
http://www.gleanster.com/report/application-/
/performance-starts-with-database-performance-/
/analysis, 2015. Last accessed June 20 2015.
[30] D. Gollmann. Computer Security . Wiley, 2011.
[31] M. Grechanik, B. M. M. Hossain, U. Buy, and
H. Wang. Preventing database deadlocks in
applications. ESEC/FSE 2013, pages 356{366, 2013.
[32] IBM. Restful web services: The basics. http:
//www.ibm.com/developerworks/library/ws-restful/,
2016. Last accessed July 25 2016.
[33] K. Jensen. Coloured Petri Nets: Basic Concepts,
Analysis Methods and Practical Use . Coloured Petri
Nets. Springer, 1997.
[34] T. Johnson and D. Shasha. 2q: A low overhead highperformance buer management replacement
algorithm. VLDB '94, pages 439{450, 1994.
[35] V. B. Kampenes, T. Dyb a, J. E. Hannay, and D. I. K.
Sjberg. Systematic review: A systematic review of
eect size in software engineering experiments. Inf.
Softw. Technol. , 49(11-12):1073{1086, Nov. 2007.
[36] N. Leavitt. Whatever happened to object-oriented
databases? Computer , 33(8):16{19, Aug. 2000.
[37] M. Linares-Vasquez, B. Li, C. Vendome, and
D. Poshyvanyk. How do developers document database
usages in source code? (n). In Proceedings of the 30th
IEEE/ACM International Conference on Automated
Software Engineering , ASE '15, pages 36{41, 2015.
[38] Memcached. Memcached. http://memcached.org/,
2015.
[39] D. Moore, G. MacCabe, and B. Craig. Introduction to
the Practice of Statistics . W.H. Freeman and
Company, 2009.
[40] R. Nishtala, H. Fugal, S. Grimm, M. Kwiatkowski,
H. Lee, H. C. Li, R. McElroy, M. Paleczny, D. Peek,
P. Saab, D. Staord, T. Tung, and V. Venkataramani.
Scaling Memcache at Facebook. In Proceedings of the
10th USENIX Conference on Networked Systems
Design and Implementation , NSDI'13, pages 385{398,
2013.
[41] OpenMRS. Openmrs. http://openmrs.org/, 2016. Last
accessed July 25 2016.
[42] Oracle. Java EE platform specication.
https://java.net/projects/javaee-spec/pages/Home,
2016. Last accessed July 25 2016.
[43] S. Paul and Z. Fei. Distributed caching with
centralized control. Comput. Commun. , 24(2):256{268,
Feb. 2001.
[44] S. PetClinic. Petclinic.
https://github.com/SpringSource/spring-petclinic/,
2016. Last accessed July 25 2016.
[45] J. L. Peterson. Petri Net Theory and the Modeling of
Systems . Prentice Hall PTR, 1981.
[46] A. Rabkin and R. Katz. Precomputing possible
conguration error diagnoses. ASE '11, pages 193{202,
2011.
[47] A. Rabkin and R. Katz. Static extraction of program
conguration options. ICSE '11, pages 131{140, 2011.
[48] K. Ramachandra and S. Sudarshan. Holistic
optimization by prefetching query results. SIGMOD
'12, pages 133{144, 2012.
[49] L. Richardson and S. Ruby. RESTful Web Services .
O'Reilly Media, 2008.[50] W. Shang, Z. M. Jiang, B. Adams, A. E. Hassan,
M. W. Godfrey, M. Nasser, and P. Flora. An
exploratory study of the evolution of communicated
information about the execution of large software
systems. In Proceedings of the 2011 18th Working
Conference on Reverse Engineering , WCRE '11, pages
335{344, 2011.
[51] A. J. Smith. Cache evaluation and the impact of
workload choice. In Proceedings of the 12th Annual
International Symposium on Computer Architecture ,
ISCA '85, pages 64{73, 1985.
[52] SpringSource. Spring framework.
www.springsource.org/, 2016. Last accessed July 25
2016.
[53] M. D. Syer, Z. M. Jiang, M. Nagappan, A. E. Hassan,
M. Nasser, and P. Flora. Continuous validation of load
test suites. In Proceedings of the 5th ACM/SPEC
International Conference on Performance
Engineering , ICPE '14, pages 259{270, 2014.
[54] Terracotta. Ehcache. http://ehcache.org/, 2015.
[55] A. Tomcat. Logging in tomcat. https:
//tomcat.apache.org/tomcat-8.0-doc/logging.html,
2015.
[56] Y. Xiong, A. Hubaux, S. She, and K. Czarnecki.
Generating range xes for software conguration.
ICSE '12, pages 58{68, 2012.
[57] X. z. Liu, Y. Ma, Y. Liu, T. Xie, and G. Huang.
Demystifying the imperfect client-side cache
performance of mobile web browsing. IEEE
Transactions on Mobile Computing , PP(99), 2015.
[58] ZeroturnAround. Java tools and technologies
landscape for 2015.
http://zeroturnaround.com/rebellabs/
java-tools-and-technologies-landscape-for-2014/, 2014.
Last accessed July 24 2016.
[59] S. Zhang and M. D. Ernst. Automated diagnosis of
software conguration errors. ICSE '13, pages
312{321, 2013.
[60] S. Zhang and M. D. Ernst. Which conguration option
should i change? ICSE 2014, pages 152{163, 2014.
[61] X. Zhao, Y. Zhang, D. Lion, M. F. Ullah, Y. Luo,
D. Yuan, and M. Stumm. Lprof: A non-intrusive
request ow proler for distributed systems. OSDI'14,
pages 629{644, 2014.
[62] Y. Zhou, J. Philbin, and K. Li. The multi-queue
replacement algorithm for second level buer caches.
InProceedings of the General Track: USENIX Annual
Technical Conference , ATC '01, pages 91{104, 2001.