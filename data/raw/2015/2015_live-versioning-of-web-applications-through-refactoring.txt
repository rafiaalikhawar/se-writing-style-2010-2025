Live Versioning of Web Applications through Refactoring 
Juli√°n Grigera 
LIFIA, Fac. de Inform√°tica, UNLP      
CIC 
Argentina  
juliang@li fia.info.unlp.edu.ar  Juan Cruz Gardey  
LIFIA, Fac. de Inform√°tica, UNLP  
Argentina  
jcgardey@li fia.info.unlp.edu.ar  Alejandra Garrido  
LIFIA, Fac. de Inform√°tica, UNLP 
CONICET  
Argentina 
garrido@li fia.info.unlp.edu.ar  
 Gustavo Rossi  
LIFIA, Fac. de Inform√°tica, UNLP 
CONICET 
Argentina  
gustavo@li fia.info.unlp.edu.ar   
ABSTRACT 
Client-Side Web Refactorings (CSWRs) allow improving 
interaction aspects of web applications by applying changes to the user interface without altering the code base, even in production settings. However, developers are not always willing, or allowed 
to apply external adaptations to their applications‚Äô user interface. Besides, CSWRs do not guarantee improvements in all contexts, so it may be unwise to install them in a production version. We propose a tool that allows creating private versions of a running web application almost automatically. With this tool, developers or usability experts can combine CSWRs to create alternative versions of web applications, without requiring a cloned sandbox environment for each one. This yields many uses, such as quickly 
setting up user tests, showing live alternatives to Product Owners, 
or performing A/B testing. The tool is built on top of Kobold, a 
service that allows applying CSWRs to fix usability smells. The 
tool is available at h ttps://bit.ly/2nhQ2MD. A screencast is 
available at h ttps://youtu.be/LVc3BOtVP3I. 
CCS CONCEPTS 
‚Ä¢ Software and its engineering ‚Üí Software evolution ‚Ä¢ Human-
centered computing ‚Üí HCI design and evaluation methods  
KEYWORDS 
Web Usability, Soft ware as a Service, Usability Refactoring 
ACM Reference format: 
Juli√°n Grigera, Juan Cruz Gardey, Alejandra Garrido, and Gustavo Rossi. 
2018. Live Versioning of Web Applications through Refactoring. In Proceedings of the 2018 33rd ACM/IEEE International Conference on 
Automated So ftware Engineering (ASE ‚Äô18), September 3‚Äì7, 2018, Montpellier, France. ACM, New York, NY, USA , 4 pages. 
https://doi.org/10.1145/3238147.3240483  
1 INTRODUCTION 
The web evolves quickly, and the user experience (UX) created by 
web interfaces is constantly under scrutiny. Many large companies today apply techniques such as A/B testing on their web applications, offering alternative user interfaces (UI) to evaluate them and select mainly the most profitable one [7]. However, A/B testing is expensive and usually unaffordable for small/medium sized companies. In this context, helping the developers to gradually improve the UX aspects is vital. One technique that allows improving the UX without losing functionality is usability refactoring. In a previous work we devised Kobold [6], a tool that is able to detect usability problems, captured as usability smells , and solve those smells by means of 
Client-Side Web Refactoring (CSWR). These refactorings perform changes to the Document Object Model (DOM) of web pages with the purpose of improving usability or accessibility issues [3]. Thus, Kobold allows developers, especially those with no UX experience, to discover the usability smells arising from the real use of their application, and solve those smells through client-side refactoring.  
After some experiences with Kobold in the industry, we found 
some concerns on its adoption: even when CSWRs do not modify the code base, they are applied on the production application, and if several CSWRs are possible for a given smell, it may not be easy to test them and discover which one would work best, if any.  
To assess our first impressions on the adoption of Kobold, and 
generalize it to the adoption of UI adaptation tools, we conducted a survey with 30 developers and product owners working in different types of organizations in Argentina. When questioned about how frequently a production UI is modified to improve UX in their team, 73.4% answered at least ‚Äúfrequently‚Äù (56.7% ‚Äúfrequently‚Äù and 16.7% ‚Äúvery frequently‚Äù). The majority uses functional prototypes and external services to evaluate a modified UI. However, when inquired whether they would allow external tools to apply changes on their UIs, 56.7% of them answered ‚Äúmaybe‚Äù while 30% straight ‚Äúno‚Äù (see the pie chart in Fig. 1).  
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full 
citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific 
permission and/or a fee. Request permissions from Permissions@acm.org. 
ASE '18, September 3‚Äì7, 2018, Montpellier, France  ¬© 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5937-5/18/09‚Ä¶$15.00  
https://doi.org/10.1145/3238147.3240483  
872
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:10:52 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France J. Grigera, J.C. Gardey, A. Garrido, G. Rossi 
 
 
  
Fig. 1: Survey Results 
The most popular reasons preventing the ‚Äúmaybe/no‚Äù group to 
allow changes were (Fig.1 left): needing to test before deployment; 
needing authorization from the client, and trust issues with code compatibility and stability. Amongst the developers who answered ‚Äúmaybe/yes‚Äù, the most popular reason to allow external changes was improving UX aspects (see Fig.1 right). 
In brief, while there is a need for frequent UI improvement, 
developers won't allow external services to apply these improvements unless they can test them before release, and most developers prefer functional prototypes to perform the test. Also, only 1 person indicated using A/B testing to evaluate UI changes - we think this is due to this technique being still too costly.  
The survey allowed us to confirm our initial suspicion and the 
claim of this paper: there is a need for a tool to create trial versions of an application where client-side adaptations may be easily applied and tested with users, even making A/B testing affordable. We have built a working prototype of such tool, allowing developers or UX experts to generate different versions of a running web application, by applying combinations of CSWRs over the original application in a semi-automatic way, and without the need of setting up different testing environments. 
2 RELATED WORK 
The idea of generating different versions of a website without altering the application‚Äôs codebase has existed for some years, and for diverse reasons. The field of web augmentation proposes making client-side alterations for specific viewpoints or contexts, even in 3rd party applications. For example, Ghiani et al. [4] propose the use of augmentation techniques in web applications to adapt the UI for different context of use (regarding technology, users or environment), exploiting multimodality, like turning to a 
voice-based interface while driving, to avoid visual distraction. 
There are also crowdsourced approaches for augmentation. 
Arellano et al. [1] propose a platform in which users may submit augmentations to adapt existing web applications, focusing on trust, which is a relevant issue in web crowdsourcing solutions, given that external client-side code could be used maliciously.  
Aiming at external personalization, Arellano et al. also propose 
the concept of ‚ÄúOpen Personalization‚Äù, where a web application defines extension points in which other stakeholders/partners can insert customized content [2]. In the industry, this practice could be compared to Google Adsense‚Äôs (h ttps://google.com/adsense): 
web owners offer an area of their UIs for ads providers to render their content, and collect payments from advertisers. These approaches require web owners to enable extension points in the application, confining the changes to the designated spaces. 
Contrarily to the aforementioned works, our proposal is 
focused on version generation for usability, only visible by web application managers. Also, the modifications are suggested as refactorings, i.e. working solutions to concrete usability problems, r a t h e r  t h a n  a  p l a t f o r m  w h e r e  d e v e l o p e r s  m u s t  c o d e  t h e  adaptations on their own. 
Moreover, there exist some commercial services to evaluate 
different versions of a UI, for instance to run A/B testing cycles [9]. These services overcome some limitations of the previously commented works: they provide WYSIWYG editors for generating versions, making it feasible even for non-developers. The solutions are client-side, except for a code snippet to be installed, as it is usual in such services. Even if there are ways of 
generating versions of a web application without altering the 
codebase, it is still up to the developers to craft the solutions manually, and there are no predefined solutions. In our proposal, CSWRs are automatically created by Kobold to fix specific smells, or semi-automatically if they need parameters.   
3 TOOL DETAILS 
The versioning tool allows users to generate alternative versions of a given web application without altering its code base. This is possible thanks to the CSWR technique [3], which consists in 
improving a web UI by altering the DOM on the client side, 
preserving content and functionality. The tool is built on top of Kobold, a previously developed service for web usability. 
3.1 Background on Kobold 
Kobold is a service for finding and correcting usability smells in deployed web applications [5]. To use it, developers get an account and paste a code snippet on their application‚Äôs header; this allows Kobold to capture user interaction events and use them to detect usability smells in real time. When account owners log in, they see a report of the smells captured thus far, and can also select one of the suggested CSWRs to solve them. Then, Kobold instantiates each CSWR (either with or without manual help to parameterize it) and applies them on the client side of the target application - in production, via the same code snippet. An example of a smell that Kobold may report is ‚ÄúFree Input for Limited Values‚Äù, which appears when a free text input is offered to fill up a value that is actually within a closed range (e.g. ‚ÄúCountry‚Äù)[5]. It may be solved by either ‚ÄúTurn Input into Radios‚Äù  (which Kobold 
may instantiate automatically), or ‚ÄúAdd Autocomplete‚Äù  (that 
allows parameterizing the li st of values to suggest). 
3.2 Versioning of Web Applications 
The versioning tool replaces the previous mechanism used by Kobold for applying the CSWRs in production. It allows the developers to create variants of the production version by applying different CSWRs on each one. As opposed to client-side adaptation browser tools, versioning scripts run from the code snippet, so no special software is required to view the versions. 

	
)'&%)",%))
 
)*"%!

+!"-"%!	 
)* &('$&.
)#$"%* "()*	(+)*"))+)	



873
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:10:52 UTC from IEEE Xplore.  Restrictions apply. Live Versioning of Web Applications through Refactoring ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France 
 
 Versioning a web application without changing the source 
code allows developers to effortlessly evaluate alternatives for the 
UI without compromising the production version, yet taking advantage of the same infrastructure. This is very important in a c o n t e x t  i n  w h i c h  u s e r ‚Äô s  r e q u i r e m e n t s  c h a n g e  c o n t i n u o u s l y  a n d  the UI needs to evolve accordingly. For instance, in order to run a usability test with volunteers in a lab setting (e.g. think aloud 
tests), or even remote user tests by recruiting online volunteers instead, there is always a need for setting up a working environment for users to perform their tasks. 
Given that a reported usability smell can be potentially solved 
by different refactorings, versioning becomes a powerful tool to compare solutions for a same problem. Developers could create two versions to solve a usability smell with different CSWRs and run user tests to evaluate each one. They could also combine 
various CSWRs in the same version to solve more than one smell, 
depending on how fine-grained the owner wants to test each change. In the case of semi-automated CSWRs, i.e. those that require parameters, they may even be applied to solve the same usability smell in different versions, but with different values. 
3.2.1 Using the Tool.
 Figure 2 shows the ‚ÄúVersions & 
Refactorings‚Äù  tab in Kobold with all application versions. Initially, 
only the default ‚Äúproduction‚Äù version exists, which cannot be 
removed. This version may have some installed CSWRs. To create 
a new version, developers must enter a version name and a tag to reference it; once the tag is entered, a unique URL for this version is generated, which can be used to access and browse the version. 
After creating the version, all usability smells are shown, and 
it is possible to apply CSWRs on them. Whenever a CSWR is 
applied within a version, the usability smell disappears from the report, and the applied CSWR shows up. Notice that, in other versions, the smell will still be listed (unless it is refactored too). 
Figure 3 shows a screenshot of a version named ‚ÄúVersion A‚Äù. 
At the top, below the version‚Äôs name, the generated URL for the version is shown. Below, the list of applied CSWRs for each smell allows for editing or removing each one. In particular, ‚ÄúAdd Date Picker‚Äù and ‚ÄúAdd Tooltip‚Äù refactorings are applied in this version.  
 
Figure 2: Version handling in Kobold. 
At the bo ttom of the page, the tool shows the list of usability 
smells still unsolved for the current version. Selecting one from the list will show its details and suggested refactorings. It‚Äôs also worth mentioning that the main list of usability smells (le ftmost 
tab on the top menu), also allows applying CSWRs, but since this is a general list, i.e. not in the scope of a speci fic version, the tool 
asks for the version/s on which the CSWR will be applied. 
3.2.2 Tool Architecture.  Internally, when a user enters the URL 
for a version, the client-side component sets the website‚Äôs version 
being browsed by using the browser‚Äôs local storage. This ensures 
that the browser will keep the same version running through all 
following navigations (or until another version is loaded). Once the version is set, on each request, the client-side component asks 
the server for any CSWRs installed on the current version. If there 
are any, the server generates the code and asynchronously sends it back to the client, which performs the evaluation at the time of page loading (i.e. ‚Äúready‚Äù event). Each CSWR immediately 
changes the DOM structure as necessary. This whole process 
happens instantly, even throughout navigations. During the tests 
we ran, the end users were not able to perceive the overhead.  
Given that the client-side component is aware of the current 
version, the versioning system is also able log the incoming usability events logged speci fically under the version they 
occurred. This is helpful to analyze behavior on di fferent versions 
(e.g. in A/B testing scenarios), or capture new potential usability smells introduced by the applied refactorings.  
 
Figure 3: Creating a web application version. On top, the version name and URL, next the applied refactorings (in this case 
‚ÄúAdd Date Picker‚Äù and ‚ÄúAdd Tooltip‚Äù). At the bottom, the rema ining Usability Smells still not refactored in this version.
874
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:10:52 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France J. Grigera, J.C. Gardey, A. Garrido, G. Rossi 
 
4 CASE STUDY 
We performed a preliminary validation of the versioning tool 
running on top of Kobold by using it on a real web application for a travel agency. For this validation, the tool was used by one team leader of the agency website (with some knowledge about Kobold), paired with one of the authors, and the goal was to perform a quick user test to fix some usability smells. 
We first created an account in Kobold for the application. After 
a week of capturing logs, Kobold had detected 7 usability smells, and 2 of them were selected to proceed: one was ‚ÄúUndescriptive Element‚Äù, which is captured when many users attempt to get a tooltip from an element, and the other ‚ÄúUnformatted Input‚Äù, which indicates that a text input is forcing users to enter formatted data (in this case a date). For each smell, Kobold suggested 2 CSWRs: for ‚ÄúUndescriptive Element‚Äù, the suggestions were Add Tooltip (A)  and Rename Element (B) ; for ‚ÄúUnformatted 
Input‚Äù, the suggestions were Add Date Picker (C)  and Date Input 
into Selects  (D), which converts a simple date input into 3 selects 
for day, month and year. The following step consisted in creating 2 versions of the web application using the versioning tool: one version with the CSWRs A and C, and the other with the CSWRs 
B and D. Having only 2 versions was enough, even if we were 
evaluating 4 different CSWRs, because each smell affected independent user tasks.  
For each of these versions, we ran user tests with 3 volunteers 
each (5 m. and 1 f., ages x =34 s=11.2), to establish which 
refactoring improved usability the most for each smell. The tasks involved in the user tests were: (1) searching for hotels in a specific date and city, which was the interaction affected by the smell ‚ÄúUndescriptive Element‚Äù  (a confusing link to ‚Äúadd room‚Äù to 
the search), and (2) make a reservation for a package, which was affected by ‚ÄúUnformatted Input‚Äù on the preferred date input. 
The user tests were run using the versions on the users‚Äô own 
devices, and for each one we paid special attention to the refactored elements (although more usability issues were detected), and the best refactorin gs were selected based on the 
observations: Rename Element and Date Input Into Selects . The 
team leader reported that the versioning tool made it very simple and fast to create application versions, and the subjects were able to perform all tasks without being impeded by the alterations that CSWR perform over the application‚Äôs interface.  
5 CONCLUSIONS AND FUTURE WORK 
Tool adoption is difficult in many respects. As academics in software engineering we construct tools with state-of-the-art solutions that developers don't find usable or appealing. This happened for instance, as empirical studies have shown [8], with code refactoring tools. In our case , we constructed a tool that can 
automatically find and fix usability smells of user interaction in web applications, which we believe can make a difference in UX improvement, many times neglected or abandoned. Still, a survey has shown that developers wouldn‚Äôt use this tool unless we cover a simple, real need to create alternative, trial versions of their adapted UIs, so they can test them, demo them, and get to trust the adaptations that our tool proposes. In this paper we have shown a tool that covers this need for 
creating alternative versions of a web application with little effort through the use of refactoring (CSWR). We have made a test run using the tool in a real environment, and the results were successful: the team leader found it valuable to be able to generate different versions of his running web application with little effort and use the outcome to run user tests for improving the application‚Äôs usability. 
A limitation we found in the tool was that it uses, as base, the 
production  version of the web application. Therefore, testing tasks 
with permanent effects (e.g. completing a checkout process) could not be done. However, in these cases, setting up a single sandbox environment still allows the versioning tool to create as many ‚Äútest‚Äù versions as desired within a single infrastructure. 
Future work includes extending our tools to support crafting 
the user tests, and promote A/B testing for usability by making it affordable. Another extension is to be able to apply CSWRs even when there is no usability smell reported, which would enable exp erts  t o  ex plor e  di ffe ren t  soluti o ns  to  pro ble ms  t he y  d i sc o ver  by heuristic inspection. Moreover, we plan to add features such as voting for a favorite version, which could come in handy during a review meeting with the Product Owner and stakeholders. 
We will also conduct new studies to evaluate the adoption of 
our new set of tools. In particular, we are planning a broader experiment to further validate the usage of live versioning. Since we claim that the strongest point in favor of the tool is in assisting developers to set up different versions, we intend to compare tool assistance against the way developers typically evaluate UI alternatives. We foresee two main challenges: the reliability of the CSWRs (i.e. not disrupting the original UI) and their coverage, i.e. the degree to which developers can use the available CSWRs for their solutions and not having to turn to manual coding. Even if the coverage of CSWRs is not strictly within the scope of the versioning tool itself, it is a decisive factor for its success. 
ACKNOWLEDGMENTS  
This research is supported by ANPCyT-Argentina grant PICT-2015-3000.  
REFERENCES 
[1] Arellano, C., D√≠az, O. and Iturrioz, J. 2010. Crowdsourced web augmentation: A 
security model. LNCS 6488, (2010), 294‚Äì307.  
[2] Arellano, C., D√≠az, O. and Iturrioz, J. 2012. Opening personalization to partners: 
An architecture of participation for websites. LNCS 7387, (2012), 91‚Äì105.  
[3] Garrido, A., Firmenich, S., Rossi, G., Grigera, J., Medina-Medina, N. and Harari, I. 
Personalized web accessibility using client-side refactoring. IEEE Internet 
Computing . 17, 4 (2013).  
[4] Ghiani, G., Manca, M., Patern√≤, F. and Porta, C.  Beyond Responsive Design: 
Context-Dependent Multimodal Augmentation of Web Applications. Proc. Int. 
Conf. Mobile Web Information Systems. (2014), 71-85.  
[5] Grigera, J., Garrido, A., Rivero, J.M. and Rossi, G. 2017. Automatic detection of 
usability smells in web applications. International Journal of Human Computer 
Studies . 97, (2017).  
[6] Grigera, J., Garrido, A. and Rossi, G. 2017. Kobold: Web Usability as a Service. 
Proc. Int. Conf. on Automated Software Engineering. (2017), 990-995. 
[7] Kohavi, R. and Longbotham, R. Online Controlled Experiments and A/B Testing. 
Encyclopedia of Machine Learning and Data Mining  (2017). 
[8] Murphy-Hill, E., Parnin, C. and Black, A.P. How We Refactor, and How We Know 
It. IEEE Trans. on Software Engineering 38  (1), (2012): 5-18. 
[9] Optimizely: The World‚Äôs Leading Experimentation Platform: 
https://www.optimizely.com/ . Accessed: 2018-05-30. 
 
875
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:10:52 UTC from IEEE Xplore.  Restrictions apply. 