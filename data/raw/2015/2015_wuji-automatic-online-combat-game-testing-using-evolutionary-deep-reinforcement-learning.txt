Wuji: Automatic Online Combat Game Testing
Using Evolutionary Deep Reinforcement Learning
Yan Zheng1,/dividemultiply, Xiaofei Xie2,/dividemultiply, Ting Su2, Lei Ma3, Jianye Hao1,
, Zhaopeng Meng1,
Yang Liu2, Ruimin Shen4, Yingfeng Chen4, Changjie Fan4
1College of Intelligence and Computing, Tianjin University, China.
2Nanyang Technological University, Singapore.
3Kyushu University, Japan.
4Fuxi AI Lab, Netease, Inc., Hangzhou, China.
Abstract â€”Game testing has been long recognized as a noto-
riously challenging task, which mainly relies on manual playing
and scripting based testing in game industry. Even until recently,
automated game testing still remains to be largely untouched
niche. A key challenge is that game testing often requires to
play the game as a sequential decision process. A bug may
only be triggered until completing certain difï¬cult intermediate
tasks, which requires a certain level of intelligence. The recent
success of deep reinforcement learning (DRL) sheds light on
advancing automated game testing, without human competitive
intelligent support. However, the existing DRLs mostly focus on
winning the game rather than game testing. To bridge the gap,
in this paper, we ï¬rst perform an in-depth analysis of 1349 real
bugs from four real-world commercial game products. Based
on this, we propose four oracles to support automated game
testing, and further propose Wuji , an on-the-ï¬‚y game testing
framework, which leverages evolutionary algorithms, DRL and
multi-objective optimization to perform automatic game testing.
Wuji balances between winning the game and exploring the space
of the game. Winning the game allows the agent to make progress
in the game, while space exploration increases the possibility
of discovering bugs. We conduct a large-scale evaluation on a
simple game and two popular commercial games. The results
demonstrate the effectiveness of Wuji in exploring space and
detecting bugs. Moreover, Wuji found 3 previously unknown
bugs1, which have been conï¬rmed by the developers, in the
commercial games.
Index Terms â€”Game Testing, Artiï¬cial Intelligence, Deep Rein-
forcement Learning, Evolutionary Multi-Objective Optimization.
I. I NTRODUCTION
Gaming has evolved into an all-around entertainment phe-
nomenon. The game market is growing rapidly. According to
Global Games Market Report [31] from NewZoo in 2018,
there are more than 2.3 billion active gamers in the world, of
which 46% spend money on games. The game market around
the world reaches around $138 billion and is expected to more
than $180 billion in 2021. With so many users, the quality of
games becomes especially important. A game bug may bring
bad gaming experience, even cause ï¬nancial losses for gamers
or game companies. Game companies not only put a lot of
efforts to detect and ï¬x the bugs but also suffer from losing
/dividemultiplyEqual contribution.
 Corresponding author.
1The video reproduction of the bugs can be found in our website [41].
Fig. 1: Block maze with bugs (red, green and yellow dots)
users. Hence, an early-stage testing is of great importance in
discovering bugs and guaranteeing the game quality.
Due to the complexity and heavy user interactions of games,
currently, game testing is mainly dependent on human testers.
Most game companies adopt some ad-hoc manual testing
without using systematic and automated testing solutions [3].
The ad-hoc manual testing is costly and is inefï¬cient in
discovering bugs for large games. As a result, many bugs are
still discovered long after the ofï¬cial release. Unfortunately,
most of these bugs are discovered by gamers. A study shows
that popular games receive a large number of reviews each
day, making it very time-consuming for developers to handle
them [24]. Semi-automatic script-based testing can improve
efï¬ciency. However, it still requires substantial manual efforts
to develop the scripts. In addition, the ad-hoc scripting is also
costly and usually focuses on main scenarios of the game
following a pre-deï¬ned strategy. From the internal report of
theNetEase, Inc. , there are 30 testers for testing even one
game (which was used as the benchmark in our evaluation).
The direct loss caused by the bugs in this game is about
$2 million each year. A systematic and automated testing
technique towards better quality assurance is urgent in the
game industry [25].
Automatic testing techniques have been widely stud-
ied, such as search-based testing [4], [16], coverage-guided
fuzzing [1] and symbolic execution [8]. However, such tech-
niques are challenging in testing games as the game playing
is a continuous interaction process with rich graphical user
interfaces (GUIs) between the gamer and the game. Some
techniques, such as random-based testing (Monkey [17]) and
7722019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
978-1-7281-2508-4/19/$31.00 Â©2019 IEEE
DOI 10.1109/ASE.2019.00077
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. model-based testing (Stoat [35]), have been proposed for
testing GUI applications. These techniques improve the code
coverage by covering the sequences of different events during
UI change. However, in a game, events in each UI are
usually ï¬xed (i.e., the skills), and it is very easy to cover
all events inside game UIs. These techniques are not effective
in game testing since they are not â€œsmartâ€ to accomplish the
complicated goal of the game. For example, Fig. 1 shows the
Block Maze game in which a robot hunts for gold. Bugs will
be triggered if the robot reaches one of the dots. The yellow
dots are easily reachable by Monkey and Stoat since they are
near the initial place. However, the red and green dots are
more difï¬cult to reach as the robot needs to ï¬nd a better path.
Recently, deep reinforcement learning (DRL) has achieved
great success in automatic game playing (e.g., Al-
phaGo [13] on the board game, OpenAI on Atari games [29],
and OpenAI-ï¬ve on the large-scale real-time strategy games
Dota2 [14]). This brings an opportunity for automated game
testing with the intelligent support of DRL. However, the
challenge is that the current DLR focuses on ï¬nding a better
solution to accomplish the mission (e.g., obtaining higher
rewards) rather than testing games [44]â€“[47]. For example,
in Fig. 1, DRL will learn a better strategy (the dotted arrow)
that will earn the gold quickly. Hence, the green dots can be
reached but other dots (e.g., the red dots) are hard to cover.
In addition, to capture various types of bugs (e.g., glitches,
gaming balance) during playing the game, the test oracle is
needed.
This paper performs a two-stage study towards addressing
the game testing challenges. In the ï¬rst stage, we conduct
an empirical study towards understanding the characteristics
of bugs in real-world games. We analyzed 1,349 real bugs
from four commercial online games and classify the bugs
into ï¬ve categories based on their manifestation, i.e., crash
bugs ,stuck bugs ,logical bugs ,game balance bugs anduser
experience bugs . Based on the manifestation of different types
of bugs, we proposed four oracles, that facilitate the detection
of such bugs. At the second stage, we develop an automated
game testing framework, named Wuji , to test the games and
discover different types of bugs. The objective of Wuji is to
detect game bugs by exploring states as much as possible.
To achieve the goal, Wuji not only considers accomplishing
the mission, but also considers searching different directions.
These two strategies complement each other. Consider Fig. 1,
the strategy of accomplishing mission helps reach the green
dots that are difï¬cult to cover by the general random strategy.
The strategy of exploration will help reach the red dots that
may not be located in the mainline of the game.
To test a game, Wuji ï¬rst trains a set of agents (i.e., deep
neuron networks) that can play the game automatically. The
agent policies will always be updating towards exploring the
states and accomplishing the mission. Based on proposed
oracles, Wuji performs an on-the-ï¬‚y testing while constantly
training the agent policies (rather than the usual AI solu-
tions that can be used only after training). We propose a
technique by a combination of evolutionary algorithms (EA),multi-objective optimization (MOO) and deep reinforcement
learning (DRL). EA and MOO mainly contribute to exploring
game space while DRL contributes to accomplishing the mis-
sion . Speciï¬cally, in each evolution iteration, the population
ï¬rst performs crossover and mutation, considering the two
objectives (i.e., exploring states and accomplishing mission).
Then, each policy in the population will be separately trained
by the DRL for enhancing the capability of accomplishing
the mission. Next, the MOO together with a non-dominated
sorting is used to select a better set of policies as the seed
population of the next evolution. By optimizing the population
iteratively, more states of the game could be explored and
tested.
We implemented Wuji and evaluated the usefulness by
applying it to one simple game and two commercial massively
multiplayer online games (MMOGs) including a mobile game
and a PC game. MMOG is one of the most popular games
nowadays, where thousands of players join together. The two
commercial games have more than 4 million and 880 thousand
peak daily players, respectively. The results demonstrated that
Wuji signiï¬cantly outperforms random testing, EA alone and
DRL alone in terms of bug detection and state coverage. More-
over, Wuji also successfully discovered previously unknown
bugs in the two active commercial games.
To the best of our knowledge, this is the ï¬rst automated
game testing work on real-world online combat games. The
main contributions of this paper are summarized as follows:
1) We perform an empirical study to characterize game bugs
by analyzing 1,349 real bugs from four industrial games.
We propose four test oracles for four types of bugs.
2) We propose an on-the-ï¬‚y automated testing framework
based on evolutionary deep reinforcement learning.
3) We implement Wuji and evaluate its effectiveness in
two real-world games. Moreover, we ï¬nd 3 previously
unknown bugs, which are conï¬rmed by the developers.
II. B ACKGROUND AND PROBLEM DEFINITION
A. Game Playing
Game playing is a sequential decision-making process
where a player needs to continuously make decisions and take
actions based on received observations before the game ends.
Such a problem can be modeled as a Markov Decision Process
(MDP) that is a classical formalization of sequential decision
making [38]. Fig. 2 depicts an MDP which shows a typical
overall interaction of playing the game. In general, the MDP
consists of 5 elements - Agent ,Environment ,State ,Action and
Reward . An environment is what an agent interacts with (e.g.,
the game). The agent utilizes a speciï¬c policy for interacting
with the environment. The state is the observation of the
environment which is usually different at different timestamps.
The action is a set of possible decision (e.g., moves, ï¬re) the
agent can make. The reward (e.g., scalar value) is the feedback
by which we measure the success or failure of an agents
actions with regard to achieving some goals (e.g., winning
the ï¬ght or achieving points).
773
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 








	

Fig. 2: The interaction of an agent and game environment.
For example, to be speciï¬c, given the state stat time
t, the agent selects an action atto interact with the game
environment and receives a reward rtfrom the environment.
The environment turns into a new state st+1, affecting the
agent selection of the next action. The environment and agent
interact continually until the game ends, and gives rise to a
trajectory as follows:
(s0,a0,r0,Â·Â·Â·,si,ai,ri,Â·Â·Â·) (1)
B. Problem Deï¬nition
A large online combat game (OCM) usually contains many
instance dungeons or scenarios. It is challenging to test the
whole game one time, the common way is to test each scenario
separately. Another overwhelming challenge is that the OCM
is usually played by multiple players. In this initial game
testing work, we mainly focus on game testing with one agent.
Based on the Equation 1, we deï¬ne a game as follows:
Deï¬nition 1 (Game) .A gameGis a function (S,R)=G(A),
whereAis a sequence of actions from an agent, Sis a
sequence of states and Ris a sequence of rewards. For
simplicity, we also use /bardblG(A)/bardblSand/bardblG(A)/bardblRto represent
the outputs of the game G(i.e.,SandR), respectively.
A test case of the game Gis a sequence of actions A, which
explore and drive the game into different states. Note that Gis
often a non-deterministic function because of the randomness
in the game (e.g., the randomness in the environment, the
randomness effect of an action).
Suppose/bardblG(A)/bardblR:= (r1,...,r n), the rewards of Aon the
gameG(denoted as RW(A,G)) is computed as follows:
RW(A,G)=/summationdisplay
tâˆˆ1...nÎ³trt (2)
whereÎ³âˆˆ[0,1]is a discount factor.
Deï¬nition 2 (Policy) .A policy Ï€is a function, a=Ï€(s), that
maps a state to an action, where sandarepresent a state and
an action, respectively.
The policy is the strategy that an agent adopts to decide the
next action based on the current state. For example, a policy
can be the strategy from the human or a trained deep neural
network that can make the decision.Deï¬nition 3 (Game State) .A statesof the game Gis a ï¬xed-
length vector /angbracketleftv0,v1,...,v n/angbracketright, where each value of the vector
represents a certain aspect of the state of the game.
The vector of a state represents the current status of the
game, and it differs for different games. For example, the
position of the player character, the health points, etc.
Deï¬nition 4 (Game Testing) .F o rag a m e G, a game tester T
can be deï¬ned as a function S,B=T(G,Î ), whereÎ is a
set of policies to interact with G,Sis a set of states explored
byTandBis a set of bugs by T.
A game tester employs different policies and tries to explore
different states of the game, where a bug might hide. The
goal of the game testing is to generate test cases (i.e., valid
actions) that can reach the diverse states, on which bugs will be
triggered. To check whether a speciï¬c state triggers a bug, the
test oracle is usually needed. However, due to the uniqueness
of game software, the test oracle also quite differs from other
kinds of software, which is yet not well-studied.
Thus, our ï¬rst goal is to perform an empirical study and
investigate the potential useful test oracles for checking game
bugs. Then, we propose effective testing policies (i.e., Î ) for
effective state exploration to cover those bugs might hide,
which is then captured by our proposed test oracle.
III. E MPIRICAL STUDY
Massively multi-player online game (MMOG) is one of the
most popular games nowadays, where thousands of players
join together. To achieve better user experience, game compa-
nies strive to detect and ï¬x as many bugs as possible before
each version release. Online combat is among the core and
most complicated elements of MMOGs, which is the key
challenge for MMOG testing. In this paper, we mainly focus
on such scenarios but our approach is general to other types
of games.
To better understand the characteristics of existing game
bugs, we ï¬rst perform an empirical study that analyzes
1,349 bugs from the bug lists of four popular commercial
OCMs that are actively used by over 10 million peak daily
players in total. All these games are developed and maintained
byNetease, Inc.2. Informed by the knowledge of game devel-
opers, we categorize them into 5 different types according to
their consequences. We order them by the ï¬xing priority from
highest to least as follows:
â€¢Crash bugs , which leads the entire game to crash and exit.
For example, the zero-dividing problem, memory leak issue
or recursive function calls will result in game crash. Crash
bugs are severe defects and should be ï¬xed ï¬rst.
â€¢Stuck bugs , which freezes the game. Game players are
unable to continue the interactions. Such bugs may be
caused when the player enters into an abnormal state, getting
stuck the whole game. Although there is no crash, the game
cannot response to any playerâ€™s actions.
2https://www.neteasegames.com/
774
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Bug distribution in four commercial games.
Games L10 NSH L12 LH01 Total
Crash 89 16 10 30 145( 10.75% )
Stuck 22 7 9 27 65(4.82% )
Logical 379 25 22 62 488( 36.17% )
Balance 7 2 2 1 12(0.89% )
Experience 476 88 20 55 639( 47.37% )
Total 973 138 63 175 1349 ( 100.00% )
â€¢Logical bugs , which often do not break the game like the
previous two types of bugs, but lead to unexpected results
(e.g., errors on score computation). This type of bugs are
usually caused by incorrect implementation of game logic.
â€¢Gaming balance bugs , which disturb the gaming balance
between human and computer-aided players (e.g., too strong
or too weak), and thus violating the designersâ€™ original
expectation. This type of bug is speciï¬c but critical to
combat games.
â€¢User experience bugs , which downgrade user experience,
e.g., missing video (e.g., ray tracing) and audio effects,
failing to give proper hints, text typos, etc.
Among these types of bugs, game developers give higher
priority for the ï¬rst four types of bugs, since they directly
affect the business logic, stability, and correctness of games.
These bugs are often strived to be ï¬xed before releasing games.
Table I shows the detailed classiï¬cation results of 1,349 bugs.
We can observe that these four critical types of bugs account
for half of all bugs in 4 studied games. Based on these, in this
paper, we deï¬ne four types of oracles, with the intention to
detect these four typical bugs respectively, which are urgent
to be ï¬xed with a higher priority:
â€¢Oracle 1 (Crash) , the crash bugs can be automatically cap-
tured by monitoring whether the game process is terminated.
â€¢Oracle 2 (Stuck) , the stuck bugs can be automatically
captured by: 1) monitoring the change of the game screen,
or 2) monitoring the states of the game, i.e., comparing
the difference between the current state and the previous
nconsecutive states. In this paper, we used the state-level
check since monitoring the game screen is too heavy and
not scalable. Speciï¬cally, during game playing, the moving
averaged state Â¯s=/summationtext
i=tâˆ’n+1,...,tsi/n is calculated by
averaging nprevious consecutive states. If the difference
between current observed state sandÂ¯sis less than a
threshold Ts, a stuck potential bug is discovered.
â€¢Oracle 3 (Logical) , the logical bugs are difï¬cult to be
discovered automatically. It requires speciï¬c assertions to
reveal logical bugs.
â€¢Oracle 4 (Gaming Balance) , the balance bugs can be cap-
tured by checking whether one player achieves unexpected
performance (e.g., too strong or weak) beyond a predeï¬ned
threshold. Gaming balance bugs are subjective and need the
conï¬rmation of game designers. We will not consider this
type of bugs in this paper.
IV . A UTOMA TED GAME TESTING
In this section, we ï¬rst describe the overview of our
approach. Then, we present our detailed techniques and al-gorithms for automated game testing.
A. Overview
The general idea of Wuji is to generate effective policies
that explore more states of the game where the bugs potentially
hide. In company with the proposed oracle, such bugs are more
likely to be detected. Fig. 3(a) shows the simpliï¬ed workï¬‚ow
ofWuji . Given a game, Wuji randomly initializes a population
of policies P(i.e., a speciï¬ed number of DNNs), after which
the main testing iteration starts on the basis of an evolutionary
process. Notably, all individuals of the ï¬rst generation are
randomly initialized (DNNs with random weights).
From the high level, in each iteration, Wuji adopts the
evolutionary multi-objective optimization (EMOO) and deep
reinforcement learning (DRL) to achieve efï¬cient game test-
ing. EMOO is used for improving the diverse state exploration
while DRL facilitates improving the capability of accomplish-
ing the mission of the game. The combination of EMOO and
DRL altogether makes Wuji potentially be able to complete
more missions and explore more states of the game.
The policies (i.e., DNNs) Pare randomly selected by the
binary tournament selection [27] to perform crossover and mu-
tation to evolve the population to P/prime, towards generating poli-
cies that are capable of exploring more states (Section IV-C).
The ï¬tness score of each policy is computed by playing and
interacting with the game. A challenge is that, if the policy
is not smart and is always unable to complete the mission of
the game, then the policy would not explore the states after
the mission. To mitigate the problem, Wuji leverages a DRL-
based technique which aims at improving the capability of
accomplishing the mission of the game (Section IV-D). Each
obtained DNN policy in P/primewill be further trained by DRL, and
an advanced population Qthat is good at completing missions
is obtained.
Afterwards, Wuji performs a multi-objective based selection
form the population P,P/prime, andQthat could survive in the
next iteration (Section IV-E). In particular, the non-dominated
sorting (Algorithm 2) and crowding distance sorting (Algo-
rithm 4) are used to sort the population PâˆªP/primeâˆªQ.
Wuji continually evolves its testing policy by EMOO and
DRL iteration by iteration. At the same time, the policies
obtained from both procedures are leveraged to perform on-
the-ï¬‚y game testing, until the given testing time or resource
budge exhausts. Wuji eventually outputs a set of failed traces
that facilitate reproducing the bugs and a set of obtained
competitive testing policies.
B. Multiple Objectives
To be speciï¬c, Wuji iteratively evolves the population to-
wards achieving two objectives: exploring more states and
winning the game (i.e., complete the mission). We capture
these two objectives by exploration score and the winning
score, respectively, which are measured as follows. For the
gameG, we use the policy Ï€to play the game for mtimes,
after which msequence of actions (i.e., A0,A1,...,A m) will
be generated. Each of these sequences represents the actions
775
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 



!
 
 
	!
"  
"



		!
  %




!

 
 

 
	

!


"



"






	

	!





 

 %
"" 


	
	!

  
	 
InitializationRandom
(a) The workï¬‚ow of Wuji	
	
(b) Visualization of MOO-based selection
Fig. 3: The overview and workï¬‚ow of Wuji and MOO-based selection.
taken by an agent and the corresponding game environment
state changes step by step. Then, the exploration score of Ï€
on the game Gis computed as follows:
ESÏ€
G:=1
mm/summationdisplay
i=1(size(/bardblG(Ai)/bardblS)) (3)
The wining score of Ï€on the game Gis calculated as:
RSÏ€
G:=1
mm/summationdisplay
i=1(RW(G,Ai)) (4)
Intuitively, the exploration score is computed as the average
of the total number of states in mruns of the game. The
winning score is computed as the average of the total rewards
inmruns of the game. The higher the exploration score, the
more states the policy will potentially explore. The higher
winning score, the policy has a higher chance to accomplish
more intermediate missions.
To better capture the capability relation of different policies,
we deï¬ne the policy domination relation as follows:
Deï¬nition 5 (Pareto domination) .A policy Ï€0dominates
another policy Ï€1(denoted as Ï€0/followsÏ€1)o ng a m e Gif (ESÏ€0
Gâ‰¥
ESÏ€1
Gâˆ§RSÏ€0
G>R SÏ€1
G)o r(ESÏ€0
G>ESÏ€1
Gâˆ§RSÏ€0
Gâ‰¥RSÏ€1
G).
C. Crossover and Mutation
Algorithm 1 shows the crossover and mutation on a popu-
lationP.|P|represents the total number of the policies in P.
Wuji adopts a binary tournament selection [27] to select two
policies (i.e., DNNs) Ï€fandÏ€mfor the crossover (Line 3-10).
Speciï¬cally, two candidates Ï€1andÏ€2are picked randomly. If
Ï€1dominates Ï€2,Ï€1will be selected (Line 5). If Ï€2dominates
Ï€2,Ï€2will be selected (Line 7). If they do not dominate each
other, we randomly select one of them (Line 9). For the two se-
lected DNNs, single-point crossover (SPX) [11] is performed
on the weights of DNN (Line 11). After that, random mutation
is further used by applying additive Gaussian noise (GN) [37]
to each weights of DNNs in P. By perturbation, diverse new
policies with different strategies are generated. Consequently,
more states may be explored by utilizing such policies.Algorithm 1: Cross Mutate
input : P: the population, G: the game
output : P/prime: a new population
1P/primeâ†âˆ… ;
2foriâˆˆ{1,...,|P|}do
3 Randomly select two policies Ï€1,Ï€2fromP;
4 ifÏ€1/followsÏ€2then
5 Ï€fâ†Ï€1;
6 else ifÏ€2/followsÏ€1then
7 Ï€fâ†Ï€2;
8 else
9 Ï€fâ†random(Ï€1,Ï€2);
10 Similarly, select another candidate Ï€m;
11Ï€câ†weights cross(Ï€f,Ï€m);âŠ¿(see SPX in [11])
12Ï€â†weights mutate(Ï€c);âŠ¿(see GN in [37])
13P/primeâ†P/primeâˆª{Ï€};
14returnP/prime;
D. DRL Training
Deep reinforcement learning (DRL) is an effective approach
that interacts with the real environment and learns the optimal
policy from the reward signal, enabling the agent to play
really complex games automatically and achieving a certain
level of human-competitive performance [33]. Wuji adopts a
standard DRL algorithm named advantage actor-critic (A2C)
[28], which tries to ï¬nd a policy to maximize the rewards in
terms of accomplishing the mission.
Speciï¬cally, all perturbed individuals in offspring P/primeare
further evolved by leveraging the gradient-based optimiza-
tion (A2C). The optimization continuous until either of the
following criteria is satisï¬ed: 1) reaching maximum train-
ing time, or 2) none performance improvement after certain
consecutive training rounds. These criteria are referred to as
the â€œearly stopâ€, which achieves a good trade-off between
improving performance and saving computational resources.
It is worth mentioning that, to be effective, Wuji adopts a
parallel approach for training all DNNs in the offspring,
asynchronously and simultaneously. The choices of hyper
776
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Algorithm 2: ND Sort
input : P: the population
output : P/prime: a sorted set of Pareto frontiers.
1P/primeâ†âˆ… ;
2while|P|>0do
3Fâ†{Ï€âˆˆP|âˆ„Ï€/primeâˆˆP,Ï€/prime/followsÏ€};
4P/primeâ†P/primeâˆª{F};
5Pâ†P\F;
6returnP/prime;
Algorithm 3: Pop Select
input :P: the population, n<|P|: the number of
agents to be selected
output:P/prime: a selected population
1P/primeâ†âˆ… ;
2{F1,...,F m}â† ND Sort (P);
3foreachFiin{F1,...,F m}do
4 if|P/prime|+|Fi|â‰¤nthen
5 P/primeâ†P/primeâˆªFi;
6 else
7 F/primeâ†CD Select(Fi,nâˆ’|P/prime|);
8 P/primeâ†P/primeâˆªF/prime;
9 break
10returnP/prime
parameters of DRL training and early stop will be detailed
in the later Section V-A4, respectively.
E. Population Selection
After the evolution, we obtain two new strategy populations
P/primeandQ. The policies in P/primeare evolved via crossover and
mutation, which pay more attention to the exploration. On this
basis, population Qare obtained via DRL training, which fo-
cuses on completing the mission. Given all candidate policies
(i.e.,PâˆªP/primeâˆªQ),Wuji utilizes the non-dominated sorting
and crowding distance sorting to dynamically select the better
policies in terms of the two objectives (see Section IV-B).
Algorithm 2 shows the non-dominated sorting algorithm.
Given a population P, it identiï¬es a number of prioritized
Pareto frontiers [12] (a subset of the policies) by a repeating
process. In each iteration, a policy, which cannot be dominated
by other policies, is added into the current selected Pareto
frontier (Line 3). Then, the Pareto frontier will be added to
the new population (Line 4) and be removed from the original
population (Line 5). In the next iteration, it continues to select
next Pareto frontier from the remaining agent policies.
The intuition of our selection strategy is that if a policy
is not dominated by others, it is at least no worse than any
other one in terms of the multi-objectives. Fig. 3(b) shows an
example of the non-dominated sorting that outputs a sequence
of Pareto frontiers {F1,F2,F3,...}. Each point is a policy
within a Pareto frontier. We could see that F1generally
exceedsF2becauseâˆ€Ï€âˆˆF1.(âˆ„Ï€/primeâˆˆF2.Ï€/prime/followsÏ€).Algorithm 4: CD Select
input :F={Ï€1,Ï€2,Â·Â·Â·,Ï€l}: a Pareto frontier, n<|F|:
the number of polices to be selected
output:P/prime: a set of selected policies
1âˆ€0<iâ‰¤l,dist(Ï€i)â†0;
2Eâ†Sorte
â†“(F);
3Wâ†Sortw
â†“(F);
4dist(E[0])â†âˆ ;
5dist(W[0])â†âˆ ;
6foreachÏ€âˆˆ(F\{E[0],W[0]}do
7iâ†IndexOf (E,Ï€);
8jâ†IndexOf (W,Ï€);
9dist(Ï€)â†
(ESE[iâˆ’1]âˆ’ESE[i+1])+(RSW[jâˆ’1]âˆ’RSW[j+1]);
10Zâ†Sortdist
â†“(F);
11return{Z[1],...,Z[n]};
Algorithm 3 presents the procedure that takes policies P
and a number nas the input, and outputs a better population
P/prime. We ï¬rst perform a non-dominated sorting to prioritize the
policies. Based on the prioritization, the Pareto frontiers will
be added to the new population in order (Line 5). If the total
number of the selected population P/primeand the current Pareto
frontierFiexceedsn(Line 4), we have to select part of the
polices (whose size is nâˆ’|P/prime|) fromFisuch that we can
exactly select npolicies. Crowding distance sorting strategy
is then adopted to select the remaining policies (Line 7).
Algorithm 4 shows the crowding distance sorting algorithm.
Crowding distance (CD) measures the density of the policies
andWuji tends to select the sparse points for constructing a
more diverse population. The CD of each policy is initialized
as zero (Line 1). Then we sort the policies Fin descending
order based on the exploration score and winning score,
respectively (Line 2-3). The CD of the policy, which has the
largest winning score or the exploration score, is set as the
max distance value (Line 4-5). The two policies are the best
ones in terms of exploring/winning and we will always select
them. For each of the other policies (Line 6), we compute the
CD by the distances from the surrounding polices. We get the
indexes in the ordered sequences EandW(Line 7-8). The CD
is computed based on the distance between two surrounding
policies in terms of exploration score and winning score (i.e.,
ESE[iâˆ’1]âˆ’ESE[i+1]andRSW[jâˆ’1]âˆ’RSW[j+1]). Afterward,
the polices are sorted in descending order based on the CD
and the ï¬rst npolicies are returned (Line 10-11).
Consider the two policies Ï€1andÏ€2in Fig 3(b), the CD of
Ï€1is computed as d1+d2+d3+d4, which is larger than the
CD ofÏ€2. Intuitively, the polices around Ï€2are very dense
while the polices around Ï€1are sparse.
V. E V ALUA TION
We have implemented Wuji (7,000+ lines of code) based on
PyTorch [32]. To demonstrate the effectiveness of Wuji ,w e
investigate the following research questions.
777
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Fig. 4: Examples of combat scenarios for L10 (left) and NSH.
RQ1 (Bug Detection): How effective is Wuji in detect-
ing game bugs? How do the key components of Wuji , i.e.,
EA,DRL andMOO , affect the performance when functioning
separately?
RQ2 (Coverage): How is the exploration capability of Wuji
in terms of code coverage and state coverage?
RQ3 (Bug Analysis): What are the root cause and character-
istics of those bugs found by Wuji ?
A. Experimental Design and Settings
1) Datasets: A simple game (Block Maze) and two com-
mercial online combat games (L103and NSH4) are selected
in the evaluation. Since L10 and NSH are quite complex
real-world commercial games. To make our study feasible,
for each game, we only selected one instance (i.e., a combat
scenario) for testing. Fig. 4 depicts the online combat scenarios
for the two commercial games. Table II summarizes the
detailed information about the games. Column #Code shows
the number of lines of source code for each game. In particular,
for L10 and NSH, the left number shows the code size of the
selected scenario while the right number is the code size of
the whole game. Column INST.Size represents the installation
size and Column #Players shows daily peak player count
(4,000,000+ for L10 and 880,000+ for NSH), from which
we could see the selected games are indeed large-scale and
practical.
Block Maze : the agent controls the robot to reach the goal
coin. The player has 4 actions to choose: {north, south,
east, west }. Every movement leads the agent to move into
a neighbor grid in the corresponding direction, except that a
collision on the block (dark green) results in no movement.
L10and NSH : the two games are online combat games, where
a game character ï¬ghts against an opponent in the game. To be
speciï¬c, L10 is 2-dimensional scenario running on the mobile
platform. NSH is a 3-dimensional game running on the PC
platform. In L10, the game character is equipped with 5 skills
while it has 8 skills in NSH. Also, the game character can
move in four directions. The goals of the games are to win
the ï¬ghting by discovering a combination of skills that damage
the opponent to the largest extent.
Note that all three games are tested on the server even if
L10 is released for mobile devices. GUIs of the games are
removed as we mainly focus on functional testing. In industry
game development and testing of Netease, Inc., this is a typical
method for maximizing the testing efï¬ciency.
3A Chinese Ghost Story (L10) https://xqn.163.com/
4Treacherous Water Online (NSH) http://n.163.com/TABLE II: Three games used in the evaluation
Name #Code (KLOC) INST.Size #Players
Block Maze 0.768 3 MB -
L10 45.99/1,472 1.54 GB 4 million+
NSH 292.07/23,457 61.38 GB 880 thousand+
2) Game Bugs: To the best of our knowledge, it still lacks
standard benchmarks for game testing at this moment. In our
study, to evaluate the usefulness of Wuji , the three games are
injected with history bugs for controlled study as follows. For
Block Maze , we injected 25 bugs that are randomly distributed
within the environment. The bugs will be triggered if the robot
has reached the speciï¬c location of the map (see Fig. 1).
ForL10 and NSH , the game developers of our collaborated
company help to inject 10 (2 crash bugs, 2 stuck bugs, and 6
logical bugs) and 18 (4 crash bugs, 3 stuck bugs and 11 logical
bugs) previously known bugs in the selected scenarios, based
on their experience. These bugs actually appeared in previous
different versions of the game, and are now injected in the
same game version for our evaluation purpose. Furthermore,
developers also insert the assertions for detecting logical bugs.
3) Baseline Approaches: To the best of our knowledge,
there still does not exist any automated testing technique on
large OCMs at this moment. As mentioned in Section V-A1,
our testing environment is deployed on the server and does
not contain the GUI part. Hence, the existing GUI testing
tools, such as Monkey [17], Sapienz [26], Stoat [35], are
not applicable. We tried our best to compare Wuji with the
ï¬ve strategies (including one baseline and four variants of
Wuji ) to demonstrate the effectiveness of Wuji and evaluate the
effectiveness of each strategy of Wuji in terms of bug detection
and coverage:
â€¢Random: during playing the games, it randomly selects
one of the available actions. We followed the idea of
Monkey [17] and implemented this random strategy as the
baseline.
â€¢EAS:the evolutionary algorithm based strategy whose
ï¬tness score only considers the winning score.
â€¢EAM:the evolutionary algorithm based strategy that uses a
MOO-based ï¬tness score (i.e., Algorithm 1).
â€¢DRL: test the games by only considering the winning score
with DRL.
â€¢EAS+DRL: the combination of EA and DRL but they only
consider the winning score. Compared with EAS+DRL ,
Wuji (EAM+DRL ) adopts a multi-objective optimization.
4) Conï¬gurations and Implementations: We select the
DNN model in the previous work A2C [28] as the seeding
model for all of the three games, with the input and output
dimensions adjusted for different games. Mean square error
(MSE) is adopted as the loss function in DRL training, which
is minimized by leveraging the Adam [21] optimizer. To be
speciï¬c, due to the stripping of GUIs, the inputs of the DNNs
models are scalar-based vectors. Consequently, we use three
fully-connected linear layers with 256,128,128 units as the
hidden layers, connected with the output layer. The output
778
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 0 ... 1m ... 30m ... 1h0510152025#BugsRandom
EA_SEA_M
DRLEA_S+DRL
Wuji
(a) Block Maze0 15m 30m 1h 3h 8h 10h0246810Random
EA_SEA_M
DRLEA_S+DRL
Wuji
(b) L100 ... 5h ... 10h048121620Random
EA_SEA_M
DRLEA_S+DRL
Wuji
(c) NSH
Fig. 5: Comparison of different strategies regarding the average number of bugs found in three games, respectively
Random EA_S EA_M DRL EA_S+DRL Wuji0510152025#Bugs
(a) Averaged bugs found in Block MazeRandom EA_S EA_M DRL EA_S+DRL Wuji0246810
(b) Averaged bugs found in L10Random EA_S EA_M DRL EA_S+DRL Wuji048121620
(c) Averaged bugs found in NSH
Fig. 6: The number of bugs discovered using different strategies after 1 hour for Block Maze and 10 hours for L10 and NSH.
layer consists of each valid actions for three games with the
corresponding size of 4, 7, 17. The output of each hidden layeris normalized using batch normalization [19], and the activa-
tion function ReLU [30] is adopted between two consecutive
layers. On the other hand, to ensure sufï¬cient exploration,we set the weight of the entropy item in the loss functionof the actor to 10
âˆ’4, and ensure at least 1% possibility of
each valid actions being selected for executing. The epoch forDRL training is set to 20 (BlockMaze), 1000 (L10) and 1000(NSH), respectively. The learning rate is set to 0.25âˆ—10
âˆ’3in
the Adam optimizer. In addition, the maximum training timeand threshold of the consecutive training period in â€œearly stopâ€are set to 10 minutes and 7 rounds, respectively. For the EAalgorithm, the size of the population and offspring are set to30. The crossover rate and mutation rate are set to 100% and
1%, respectively. Notably, a random noise sampled from a
Gaussian distribution N(0,0.005) is adopted for mutating the
DNNs. All the experiment were run on three powerful serverswith CPU(Intel(R) Xeon(R) Gold 6138 CPU @ 2.00GHz) with100 cores and 256GB RAM.
From a statistical perspective, in RQ1 and RQ2, each
strategy is used to test the games for a certain amount of timebased on the complexity. For the simple game Block Maze, weallocate one hour time budget. For L10 and NSH, we allocate10 hours for each. To counter the randomness effect duringtesting, we repeat each strategy 10 times to average the results.To mitigate the potential bias of bug injection, during eachtest run of Block Maze, the 25 bugs and the gold position are
randomly injected.
Note that, even though we only selected two scenarios in
the two commercial games, it still takes quite a lot of human
efforts: (1) the game developers spent about 20 man-days and$5,000+ manpower cost to inject these known bugs carefully,and (2) we ran 6 different strategies on three games with 10repetitions, and it overall took 1,260 machine-hours.
B. Evaluation on Bug Detection (RQ1)
Fig. 5 shows the average number of bugs discovered over
time. The statistical results of the number of bugs discovered
during the allocated testing time (see Section V-A4) are shownin Fig. 6. Overall, we have the following ï¬ndings.
First, compared with the baseline, we can see that Random
achieves better results in the smaller game (i.e., Block Maze).The reason is that small game has a relatively small searchspace. In the small space, Random can explore the states given
enough time budge. For example, in Block Maze Random can
ï¬nd more bugs than EA
SandDRL quickly. The reason is that
EASandDRL focus on how to accomplish the mission. In a
smaller game, they can ï¬nd a better policy quickly and stopexploring more states. This prevents them from discoveringmore bugs. In commercial games, the state space becomesquite larger. Due to lacking the intelligence, Random cannot
779
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. TABLE III: The average number of different types of bugs
L10 NSH
Wuji E S+DRL DRL E ME S Ran. Wuji E S+DRL DRL E ME S Ran.
Crash 1.4 1.2 0.2 0 0 0 2.4 2.2 0 0 0 0
Stuck 1.8 1.6 1.4 1.2 0.9 0 2.2 2.1 1.1 0 0 0
Logical 5.6 5.5 4.3 2.5 2.4 0.6 10.8 5 3 2.9 2.2 0.6
New 0.8 0 0 0 0 0 1.8 0.5 0 0 0 0
Total 9.6 8.3 5.9 3.7 3.3 0.6 17.2 9.8 4.1 2.9 2.2 0.6
accomplish the mission well. It only explores a small number
of states and has the worst performance. The results indicate
that random testing performs badly in the large games, which
is consistent with our intuition that traditional random testing
is less effective in testing large game testing because it cannot
effectively accomplish the game intermediate missions at the
ï¬rst hand. The AI-based approach (e.g., DRL) can mitigate
such a problem by learning a smart policy.
Second, the results also show the effect of EA,DRL and
MOO on training smart policies for bug detection. The combi-
nation of EASandDRL can achieve much better results than
using one of them separately. The reason is that EASmainly
considers the winning score, and it provides various searching
directions by leveraging the advantage of population, while the
DRL algorithm improves the searching speed in a particular
direction through gradient-based optimization. The results also
indicate that both gradient-based ( DRL ) and search-based
techniques ( EAS) contribute to discovering more bugs by
complementing each other.
Further, Wuji (i.e., EAM+DRL ) achieves better results than
EAS+DRL , and EAMachieves better results than EAS. This
suggests that the multi-objective optimization is helpful for
discovering more bugs efï¬ciently. Even though the population
can contribute to the exploration (e.g. EAS+DRL performs
well in all three games), the performance can be further
improved by involving the exploration score in the ï¬tness score
(i.e., EAM+DRL forWuji ). In a large game like NSH, the
improvement is even more obvious.
We also found that EAMis slightly better than EASwhile
EAS+DRL exceeds EASeven further. Intuitively, adding
DRL achieves larger improvement than adding exploration
score. We perform an in-depth analysis and found that: even
if we have added the winning score in EA, the performance
of accomplishing the mission is still not satisfying since EA
is gradient-free and it is not effective to improve the winning
score. As a result, if it cannot accomplish the mission, the
exploration score cannot be improved. The results further
demonstrate why the beneï¬ts of including DRL (gradient-
based) in Wuji , which facilities the mission accomplishment.
Table III shows the average number of different types of
bugs (including known injected bugs and newly discovered
bugs) in L10 and NSH. During the 10 rounds of testing, Wuji
detects 1 new bug in L10 and 2 new bugs in NSH. As for
the other strategies, only EAS+DRL ï¬nds one new bug in
NSH. These three new bugs have been conï¬rmed by the game
developers and will be ï¬xed in the next update. More details
about the bugs will be discussed in Section V-D. ConsideringTABLE IV: The results of line coverage with each strategy
BlockMaze L10 NSH
Coverage 98% 73% 66%
TABLE V: The results of state coverage with each strategy
Game Wuji EAS+DRL EAM EAS DRL Rand.
B.M 366.7 338.2 115.7 91.3 243.2 349.6
L10 85,764.8 59,726.7 50,177.7 46,604.7 51,750.6 12,298.8
NSH 807,344.6 651,608.6 305,662.2 265,662.2 395,726.8 84,744
these unknown bugs are detected from the stable release
version of these games, it further conï¬rms the usefulness of
Wuji for detection unknown bugs.
To summarize, Wuji enhances the exploration with the
MOO-based EAMand improves the capability of accomplish-
ing the mission with DRL , which complement each other and
achieve the best performance with combination.
Answer to RQ1: Wuji is much more effective in detecting
game bugs than random testing, which performs poorly on
large games. EA,DRL andMOO complement each other
in exploration of diverse states and accomplish the game
mission. Their combination (i.e., Wuji ) achieves better
performance.
C. Evaluation on Coverage Increase (RQ2)
We continue to analyze the coverage obtained by each strat-
egy on the game. Surprisingly, the line coverage of different
strategies is exactly the same for each game, despite the results
of bug detection are quite different (see Table IV).
The results indicate that these strategies achieve high line
coverage easily. This can be explained by the characteristics of
game, i.e., the code related to the function of the game is easily
covered by playing games. Hence, it seems to be ineffective
to detect game bugs by only improving code coverage.
Furthermore, we also analyze state coverage. The state
coverage is computed as follows: each state (see Deï¬nition 3)
contains a set of features. We adopt an interval approximation
to partition each feature into equal buckets. Based on the
discretization buckets of the features, we can count covered
buckets to estimate the state coverage. For example, if the
state has nfeatures and each feature is partitioned into m
intervals, the maximum number of states (i.e., buckets) is mn.
Table V shows the number of states that are covered by each
strategy on each game. Compared with the other strategies,
Wuji cover more states in each game (the bold number). For
large games such as NSH , the state coverage is relatively
low because large games might have more challenging (resp.
infeasible) states that are difï¬cult (reps. unable) to reach.
For example, if the player is not close to the boss, then the
boss cannot be attacked. We further found that the results of
state coverage are consistent with the results of bug detection
inRQ1 . In the small game, random testing can achieve
competitive results while it is the worst one in the larger
games, indicating that it is more useful in detecting bugs
780
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 
  
	

(a) Diversity of Policy
 (b) Trajectories
Fig. 7: (a) the ï¬nal distribution of policies in the population,
and (b) the trajectories of executing each policy.
by maximizing state coverage than code coverage for game
testing.
Answer to RQ2: The code of games is relatively easy
to cover. For state coverage, random testing is limited in
increasing state coverage on large games, while Wuji is
more effective than the other strategies by a combination
ofEA,DRL , and MOO .
D. Bug Analysis (RQ3)
In this section, we analyze some speciï¬c cases towards
better understanding the coverage and bugs of games. More
analysis and bug reproducing videos can be found on our
website [41].
Polices in Block Maze. Fig. 7 shows the ï¬nal distribution
of policies with EAS+DRL and Wuji in one run of Block
Maze, with the difference between single-objective and multi-
objective depicted. With MOO, the policies in the population
ofWuji are more diverse in terms of exploration score and
winning score while the polices in EAS+DRL only favors
in winning (i.e., closely distributed at the bottom right). The
right ï¬gure shows the trajectories of executing each policy.
Intuitively, the policies in Wuji can explore more directions
(yellow arrows) while the policies in EAS+DRL only move
towards the gold (green arrow).
Line/State coverage. The following example is a simpliï¬ed
code segment including a bug in L10. The bug will be
triggered once the variable distance is 0. Intuitively, the code
is very easy to cover as long as the skill (i.e., skill id) is used.
For state coverage, distance is a feature in the state, new states
will be covered when distance changes. The example shows
that line coverage is relatively easy to cover and it cannot
capture the state change.
if(player.buff_time[skill_id]>0){
distance = compute_distance(player, boss)
//will crash if distance == 0
attack = player.use_skill(skill_id, distance)
damage = boss.get_damage(attack, ...)...
}
Logical bug in L10 : The following code shows the cal-
culation of damage from the enemy to the player character.
Speciï¬cally, the damage from the boss is calculated based
on the current damage reduction buffs (e.g. Line 3,5,7) and
each buff has a time limit. A logical bug is that the damage(i.e., variable hurt ) can be negative after some operators, and
ï¬nally the damage from the boss becomes the treatment for
the player (Line 9). We investigate the bug and found that the
variablehurt becomes negative only when the initial damage
(Line 1) is within a certain range and three speciï¬c buffs (i.e.,
1, 3, 12) exist simultaneously. The state is difï¬cult to reach
butWuji discovered it by adopting an effective exploration.
1. hurt = boss.atk
2.if(boss.buf[1] == True)
3. hurt -= boss.buf[1].reduce_val
4.if(boss.buf[3] == True)
5. hurt -= boss.buf[3].reduce_val
6.if(boss.buf[12] == True)
7. hurt -= boss.buf[12].reduce_val
...
8.// fix: hurt = max(hurt, 0)
9. player.hp -= hurt
Crash bug in NSH : A crash bug discovered by Wuji
andMA S+DRL . Generally, the bug is triggered when some
properties of the game are reset with invalid values, which are
generated only when the boss is dead with multiple speciï¬c
buffs. This a very hard corner case to reach by manual testing
(e.g., manual playing or scripting). However, it could be
triggered during playing by a large number of game players.
Wuji found the bug by learning diverse policies in favoring to
explore more states of the game.
E. Threats to Validity
Randomness is a major factor during testing and the game
playing. We counter this issue by repeating 10 times for each
testing task and averaging the results. In addition, the oracle
we proposed may not be complete, and thus Wuji may miss
some other unknown bugs like logical bugs and other types
of bugs. The selection of the games could be biased. In this
paper, we try to counter this issue by using 3 games with
different sizes, where two of them are active commercial
games. The selection of the DNNs used for training policies
could be biased. We counter this problem by selecting models
from the state-of-the-art for game playing. The bias of bug
injection may be also a threat. We counter this issue by
randomly injecting bugs of Block Maze in each testing round,
and ask helps from game developers to inject bugs in the
commercial games.
On the other hand, false alarms reported by Wuji may be
a potential threat. Speciï¬cally, the thresholds in stuck and
balance oracles directly determinate the sensitivity of Wuji
reporting a bug. The choice of such hyper parameters are not
only highly dependent on the domain knowledge, but may
also be biased. To address this, for each discovered bugs, Wuji
will record every necessary information during game playing
(including every taken action, the opponentâ€™s HP value, attack,
and defensive value, etc.). Moreover, an additional replay suit
is also developed for replaying the game automatically. On
this basis, despite unable to achieve 100% detection accuracy,
Wuji , however, provides an effective way for testers to discern
real bugs efï¬ciently.
781
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. VI. R ELA TED WORK
This section surveys the related work in three aspects.
Game Testing . As games become increasingly popular and
complex, testing games is now an important challenge and
becomes the key vehicle to improving quality. However, exist-
ing testing practice and techniques still need more signiï¬cant
advances. As pointed by Lin et al. [24], even the most popular
games on the market show the lack of sufï¬cient testing.
Alemm et al. [3] conduct a quantitative survey to identify
key developers factors for a successful game development
process. They ï¬nd that manual testing and ad-hoc, exploratory
testing are still predominant than systematic and automated
testing. Iftikhar et al. [18] proposes a UML-based modeling
methodology to support automated system-level game testing
of platform games. They manually construct the model and
use the model to generate and execute test cases with oracles.
However, the manual model construction process still requires
a lot of efforts. As for mobile games, existing research is also
preliminary. Lovreto et al. [25] ï¬rst reports their experience
of using exploratory testing to test 16 mobile games. They
manually write test scripts for functional testing and ï¬nd
existing testing techniques still have a lot of limitations. To
prioritize testing efforts, Khalid et al. [20] mine the user
reviews from 99 free mobile game apps and ï¬nd that most
negative reviews come from a small subset of devices, which
game developers should emphasize during testing. Game apps
are GUI applications, which extensively interact with human
players. Although various research work exists for testing of
GUI applications [5], [10], these techniques only target at
general-purpose GUIs. They do not work for testing game
apps because game apps usually use a game engine to render
GUIs instead of using traditional GUI widgets. To our knowl-
edge, our technique, Wuji , is the ï¬rst to achieve systematic
and automated testing for real-world games and makes great
improvements over state-of-the-practice.
Reinforcement Learning for Testing .Wuji applies deep rein-
forcement learning (DRL) to facilitate automatic game testing.
V arious research work have also applied (deep) reinforcement
learning for their own purposes. For example, QBE [22] uses
Q-Learning, one type of reinforcement learning technique,
to automatically test mobile apps. Speciï¬cally, it explores
the GUIs of a set of apps to generate the corresponding
behavior models, and uses these models to train the transition
prioritization matrix with two optimization goals, i.e., activity
coverage and the number of crashes, respectively. QBE focuses
on improving code coverage and the number of detected
crashes for Android apps. Similarly, Adamo et al. [2] and
Vuong et al. [39] also use Q-learning to improve GUI testing
of Android apps. However, these work only focus on general
apps instead of game apps. B Â¨ottinger et al. [7] introduces the
ï¬rst program fuzzer that uses reinforcement learning to learn
high reward seed mutations for testing traditional software.
Speciï¬cally, by automatically rewarding runtime characteris-
tics of the target program, this technique obtains new inputs
that likely drive program execution towards a predeï¬ned goal,e.g., maximizing code coverage. Reinforcement learning is
also applied to facilitate test prioritization and selection in
regression testing [34]. Different from these work, Wuji com-
bines DRL with evolutionary algorithms to improve testing of
game apps, which have not been tackled before.
Evolutionary Testing .
Many evolutionary algorithms have been proposed and
widely used to solve the test generation problem in software
testing. Coverage-guided fuzzing is a popular testing technique
which has been proposed for detecting vulnerabilities in tra-
ditional software [6], [9], [23], [40]. Moreover, evolutionary
testing based techniques have also been applied to test deep
neural networks [15], [42], [43]. Compared with them, Wuji
is designed to explore more states in the game instead of
maximizing code/neural coverage.
In addition, there are some techniques using evolutionary
testing with multi-objective optimization. In the ï¬eld of GUI
testing, a number of research work exploit multi-objective
optimization to improve testing effectiveness. Sapienz [26]
uses multi-objective search-based testing to automatically ï¬nd-
ing bugs in Android apps. It optimizes test sequences by
minimizing length, while simultaneously maximizing cover-
age and fault revelation. Similarly, Stoat [36] use the Gibbs
sampling technique to optimize test generation for Android
apps by maximizing coverage at both model and code level,
while increasing test diversity. In our work, Wuji uses a
combined evolutionary algorithm and DLR for effective game
testing, towards covering diverse states and completing more
intermediate missions.
VII. C ONCLUSION
In this paper, we performed a study on the characteristics of
bugs in real-world commercial games. We classiï¬ed the bugs
into ï¬ve categories and proposed four test oracles. Based on
the oracles, we developed an automated testing framework for
games. By combing the evolutionary multi-object optimization
and DRL, Wuji can explore more states such that bugs can be
more likely to be detected. We demonstrated the effectiveness
ofWuji on one simple game and two large commercial games.
In the future, we plan to 1) extend Wuji to support multiplayer
games based on multi-agent reinforcement learning, and 2)
apply Wuji on more types of games.
ACKNOWLEDGMENT
The work is supported by the National key R&D program of
China (Grant Nos.: 2018YFB1701700), National Natural Sci-
ence Foundation of China (Grant Nos.: 61702362, U1836214),
the National Research Foundation, Prime Ministers Ofï¬ce
Singapore under its National Cybersecurity R&D Program
(Award No. NRF2018NCR-NCR005-0001), National Satellite
of Excellence in Trustworthy Software System (Award No.
NRF2018NCR-NSOE003-0001) administered by the National
Cybersecurity R&D Directorate, Singapore. We thank our
industrial research partner Netease, Inc., especially the Fuxi
AI Lab and Game Testing Department of Leihuo Business
Groups for their discussion and support with the experiments.
782
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] American Fuzzy Lop . http://lcamtuf.coredump.cx/aï¬‚/, 2018.
[2] David Adamo, Md Khorrom Khan, Sreedevi Koppula, and Ren Â´ee C.
Bryce. Reinforcement learning for android GUI testing. In Proceedings
of the 9th ACM SIGSOFT International Workshop on Automating TEST
Case Design, Selection, and Evaluation, A-TEST@ESEC/SIGSOFT FSE
2018, Lake Buena Vista, FL, USA, November 05, 2018 , pages 2â€“8, 2018.
[3] Saiqa Aleem, Luiz Fernando Capretz, and Faheem Ahmed. Critical
success factors to improve the game development process from a
developerâ€™s perspective. J. Comput. Sci. Technol. , 31(5):925â€“950, 2016.
[4] Nadia Alshahwan, Xinbo Gao, Mark Harman, Y ue Jia, Ke Mao, Alexan-
der Mols, Taijin Tei, and Ilya Zorin. Deploying search based software
engineering with sapienz at facebook. In International Symposium on
Search Based Software Engineering , pages 3â€“45. Springer, 2018.
[5] Ishan Banerjee, Bao N. Nguyen, V ahid Garousi, and Atif M. Memon.
Graphical user interface (GUI) testing: Systematic mapping and repos-
itory. Information & Software Technology , 55(10):1679â€“1694, 2013.
[6] Marcel B Â¨ohme, V an-Thuan Pham, and Abhik Roychoudhury. Coverage-
based greybox fuzzing as markov chain. IEEE Transactions on Software
Engineering , 45(5):489â€“506, 2017.
[7] Konstantin B Â¨ottinger, Patrice Godefroid, and Rishabh Singh. Deep
reinforcement fuzzing. In 2018 IEEE Security and Privacy Workshops,
SP Workshops 2018, San Francisco, CA, USA, May 24, 2018 , pages
116â€“122, 2018.
[8] Cristian Cadar, Daniel Dunbar, Dawson R Engler, et al. Klee: Unassisted
and automatic generation of high-coverage tests for complex systems
programs. In OSDI , volume 8, pages 209â€“224, 2008.
[9] Hongxu Chen, Yinxing Xue, Y uekang Li, Bihuan Chen, Xiaofei Xie,
Xiuheng Wu, and Yang Liu. Hawkeye: Towards a desired directed grey-
box fuzzer. In Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security , CCS â€™18, pages 2095â€“2108,
New Y ork, NY , USA, 2018. ACM.
[10] Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso.
Automated test input generation for android: Are we there yet? (E).
In30th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015 , pages
429â€“440, 2015.
[11] Kalyanmoy Deb and Ram Bhusan Agrawal. Simulated binary crossover
for continuous search space. Complex Systems , 9(2):115â€“148, 1994.
[12] Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and Tanaka Meyarivan.
A fast elitist non-dominated sorting genetic algorithm for multi-objective
optimization: Nsga-ii. In International conference on parallel problem
solving from nature , pages 849â€“858. Springer, 2000.
[13] DeepMind. Aplhago. https://deepmind.com/research/alphago/, 2019.
[14] DeepMind. Dota2. https://openai.com/ï¬ve/, 2019.
[15] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and Jianjun
Zhao. Deepstellar: model-based quantitative analysis of stateful deep
learning systems. In Proceedings of the 2019 27th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering , pages 477â€“487. ACM, 2019.
[16] Gordon Fraser and Andrea Arcuri. Evosuite: automatic test suite
generation for object-oriented software. In Proceedings of the 19th ACM
SIGSOFT symposium and the 13th European conference on Foundations
of software engineering , pages 416â€“419. ACM, 2011.
[17] Google. Ui/application exerciser monkey. https://developer.android.com/
studio/test/monkey, 2018.
[18] Sidra Iftikhar, Muhammad Zohaib Iqbal, Muhammad Uzair Khan, and
Wardah Mahmood. An automated model based testing approach for
platform games. In 18th ACM/IEEE International Conference on Model
Driven Engineering Languages and Systems, MoDELS 2015, Ottawa,
ON, Canada, September 30 - October 2, 2015 , pages 426â€“435, 2015.
[19] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating
Deep Network Training by Reducing Internal Covariate Shift. arXiv.org ,
February 2015.[20] Hammad Khalid, Meiyappan Nagappan, Emad Shihab, and Ahmed E.
Hassan. Prioritizing the devices to test your app on: a case study of
android game apps. In Proceedings of the 22nd ACM SIGSOFT Inter-
national Symposium on Foundations of Software Engineering, (FSE-22),
Hong Kong, China, November 16 - 22, 2014 , pages 610â€“620, 2014.
[21] Diederik P Kingma and Jimmy Ba. Adam - A Method for Stochastic
Optimization. ICLR , 2015.
[22] Yavuz K Â¨oroglu, Alper Sen, Ozlem Muslu, Y unus Mete, Ceyda Ulker,
Tolga Tanriverdi, and Y unus Donmez. QBE: qlearning-based exploration
of android applications. In 11th IEEE International Conference on
Software Testing, Veriï¬cation and Validation, ICST 2018, V Â¨aster Ëšas,
Sweden, April 9-13, 2018 , pages 105â€“115, 2018.
[23] Y uekang Li, Yinxing Xue, Hongxu Chen, Xiuheng Wu, Cen Zhang,
Xiaofei Xie, Haijun Wang, and Yang Liu. Cerebro: context-aware
adaptive fuzzing for effective vulnerability detection. In Proceedings of
the 2019 27th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering ,
pages 533â€“544. ACM, 2019.
[24] Dayi Lin, Cor-Paul Bezemer, and Ahmed E. Hassan. Studying the urgent
updates of popular games on the steam platform. Empirical Software
Engineering , 22(4):2095â€“2126, 2017.
[25] Gabriel Lovreto, Andr Â´e Takeshi Endo, Paulo Nardi, and Vinicius H. S.
Durelli. Automated tests for mobile games: An experience report. In
17th Brazilian Symposium on Computer Games and Digital Entertain-
ment, SBGames 2018, Foz do Iguac Â¸u, Brazil, October 29 - November
1, 2018 , pages 48â€“56, 2018.
[26] Ke Mao, Mark Harman, and Y ue Jia. Sapienz: multi-objective automated
testing for android applications. In Proceedings of the 25th International
Symposium on Software Testing and Analysis, ISSTA 2016, Saarbr Â¨ucken,
Germany, July 18-20, 2016 , pages 94â€“105, 2016.
[27] Brad L Miller, David E Goldberg, et al. Genetic algorithms, tournament
selection, and the effects of noise. Complex systems , 9(3):193â€“212,
1995.
[28] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray
Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
InInternational conference on machine learning , pages 1928â€“1937,
2016.
[29] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu,
Joel V eness, Marc G. Bellemare, Alex Graves, Martin Riedmiller,
Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie,
Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control
through deep reinforcement learning. Nature , 518(7540):529â€“533, 2015.
[30] Vinod Nair and Geoffrey E Hinton. Rectiï¬ed linear units improve
restricted boltzmann machines. In Proceedings of the 27th international
conference on machine learning (ICML-10) , pages 807â€“814, 2010.
[31] Newzoo. Global games market report. https://newzoo.com/solutions/
standard/market-forecasts/global-games-market-report, 2018.
[32] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward
Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga,
and Adam Lerer. Automatic differentiation in pytorch. 2017.
[33] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George V an Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
V eda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. nature , 529(7587):484, 2016.
[34] Helge Spieker, Arnaud Gotlieb, Dusica Marijan, and Morten Mossige.
Reinforcement learning for automatic test case prioritization and se-
lection in continuous integration. In Proceedings of the 26th ACM
SIGSOFT International Symposium on Software Testing and Analysis,
Santa Barbara, CA, USA, July 10 - 14, 2017 , pages 12â€“22, 2017.
[35] Ting Su, Guozhu Meng, Y uting Chen, Ke Wu, Weiming Yang, Yao Yao,
Geguang Pu, Yang Liu, and Zhendong Su. Guided, stochastic model-
based gui testing of android apps. In Proceedings of the 2017 11th
Joint Meeting on Foundations of Software Engineering , pages 245â€“256.
ACM, 2017.
[36] Ting Su, Guozhu Meng, Y uting Chen, Ke Wu, Weiming Yang, Yao Yao,
Geguang Pu, Yang Liu, and Zhendong Su. Guided, stochastic model-
based GUI testing of android apps. In Proceedings of the 2017 11th
783
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2017,
Paderborn, Germany, September 4-8, 2017 , pages 245â€“256, 2017.
[37] Felipe Petroski Such, V ashisht Madhavan, Edoardo Conti, Joel Lehman,
Kenneth O. Stanley, and Jeff Clune. Deep neuroevolution: Genetic
algorithms are a competitive alternative for training deep neural networks
for reinforcement learning. arXiv preprint arXiv:1712.06567 , 2017.
[38] Richard S Sutton and Andrew G Barto. Reinforcement learning: An
introduction . MIT press, 2018.
[39] Thi Anh Tuyet Vuong and Shingo Takada. A reinforcement learn-
ing based approach to automated testing of android applications.
InProceedings of the 9th ACM SIGSOFT International Workshop
on Automating TEST Case Design, Selection, and Evaluation, A-
TEST@ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, Novem-
ber 05, 2018 , pages 31â€“37, 2018.
[40] Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. Skyï¬re: Data-driven
seed generation for fuzzing. In 2017 IEEE Symposium on Security and
Privacy (SP) , pages 579â€“594. IEEE, 2017.
[41] Wuji. Wuji. https://sites.google.com/view/gametesting, 2019.
[42] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang
Liu, Jianjun Zhao, Bo Li, Jianxiong Yin, and Simon See. Deephunter:
a coverage-guided fuzz testing framework for deep neural networks. In
Proceedings of the 28th ACM SIGSOFT International Symposium on
Software Testing and Analysis , pages 146â€“157. ACM, 2019.
[43] Xiaofei Xie, Lei Ma, Haijun Wang, Y uekang Li, Yang Liu, and Xiaohong
Li. Diffchaser: Detecting disagreements for deep neural networks. In
IJCAI , pages 5772â€“5778, 2019.
[44] Tianpei Yang, Jianye Hao, Zhaopeng Meng, Chongjie Zhang, Yan
Zheng, and Ze Zheng. Towards Efï¬cient Detection and Optimal
Response against Sophisticated Opponents. IJCAI , 2019.
[45] Tianpei Yang, Jianye Hao, Zhaopeng Meng, Yan Zheng, Chongjie
Zhang, and Ze Zheng. Bayes-ToMoP - A Fast Detection and Best
Response Algorithm Towards Sophisticated Opponents. AAMAS , 2019.
[46] Yan Zheng, Zhaopeng Meng, Jianye Hao, and Zongzhang Zhang.
Weighted Double Deep Multiagent Reinforcement Learning in Stochas-
tic Cooperative Environments. PRICAI , 2018.
[47] Yan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei
Yang, and Changjie Fan. A Deep Bayesian Policy Reuse Approach
Against Non-Stationary Agents. NeurIPS , 2018.
784
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:39 UTC from IEEE Xplore.  Restrictions apply. 