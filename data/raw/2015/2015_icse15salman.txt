Are Students Representatives of Professionals in 
Software Engineering Experiments? 
 
Iflaah Salman 
1Department of Information 
Processing Science 
University of Oulu 
Oulu, Finland 
iflaah.salman@oulu.fi Ayse Tosun Misirli 
Faculty of Computer and Informatics 
Istanbul Technical University 
Istanbul, Turkey 
tosunmisirli@itu.edu.tr  Natalia Juristo1,2 
2Facultad de Informática 
Universidad Politécnica de Madrid 
Madrid, Spain 
natalia@fi.upm.es
 
 
Abstract —Background : Most of the experiments in software 
engineering (SE) employ students as subjects. This raises 
concerns about the realism of the results acquired through students and adaptability of the results to software industry.  
Aim: We compare students and professionals to understand how 
well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of 
two test-driven development experiments conducted with 
students in an academic setting and with professionals in a 
software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code 
quality metrics. Results : Except for minor differences, neither of 
the subject groups is better than the other. Professionals produce 
larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform 
similarly when they apply a new approach for the first time. 
Conclusion : Given a carefully scoped experiment on a 
development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject 
demographics and level of experience on the results of SE experiments.  
Index Terms— experimentation; empirical study; test-driven 
development; code quality  
I. INTRODUCTION  
The external validity of experiments conducted with 
students is very often criticized. The generalizability issue and 
concerns about the validity of experiments run with students [1, 2] raise doubts about how transferrable the results in academia are to the software industry [3]. The comparison of students and professionals as experimental subjects has been 
under debate for the past few years in response to questions 
concerning the realism of the experiments [1], [2].  
Experimental subjects are not easy to recruit, which is why 
the best option in software engineering (SE) experiments is considered to be using students. One important reason is the cost. Researchers use the students that they are teaching [3]. Conducting experiments with professionals in a real environment, on the other hand, is much more costly, and the research must be very well funded [2]. Höfer and Tichy [4] 
state that the fact that students participate in experimental studies more often than professionals is a reflection of the difficulties of conducting controlled experiments outside a 
laboratory environment. Given the constraints faced by researchers, students are an important mechanism for assessing technologies, conducting pilot studies and comparing alternative experimental designs [3] at least in early phases of the assessment. 
Despite the valid reasons for using students in experiments, 
there are major concerns about the generalizability of the results. According to Sjoberg et al. [2],  controlled experiments 
with students are unrealistic in terms of environment, tasks and 
subjects. Experiments with students often involve solving pen and paper tasks, which are easy to do in a classroom environment. This is then a major hurdle when results are to be transferred to industry [2]. Consequently, the results of student experiments are difficult to generalize [5], [1]. This also limits 
the understanding of industry processes [3]. When the 
experiments lack realism, the findings are only valid in a specified experimental situation, making the results less significant for both applied and theoretical research [2].  
In this paper, we present an empirical study, whose goal is 
to compare students and professionals in order to gain an understanding of how representative students might be of professionals in SE experiments. 
We address the research goal by executing two 
experiments: one with students in an academic setting and 
another with professionals within a software organization. The experiments are conducted to observe the effects of test-driven development (TDD) on code quality. We analyze the quality of the code produced by students and professionals during the execution of these two experiments and answer our research question, “How much does the code quality of a task implemented by a professional differ from that of a task implemented by a student?” We compare the code quality of the two subject groups using statistical tests, and we observe that professionals write higher quality code than students to implement tasks when subjects apply a development approach with which they are already familiar. However, students and professionals perform similarly when they apply a
 
development approach in which they are inexperienced. 
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.82666
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.82666
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.82666
ICSE 2015, Florence, ItalyThe paper is organized as follows. Section II presents 
relevant literature reporting similar comparisons between 
students and professionals as experimental subjects. Section III explains our study context. Section IV describes the research method used. Section V presents the details of the subjects. Section VI summarizes the analysis. Section VII reports the results. This is followed by a discussion in Section VIII. Section IX presents the threats to validity. Lastly, Section X concludes the study. 
II. R
ELATED WORK 
A. Comparing Students and Professionals 
The research methodologies that are mostly applied in 
empirical software engineering research are experiments, case 
studies, correlational studies and surveys [4]. Several studies report the rates of participation of students and professionals in SE experiments. Höfer and Tichy [4] report that 60% of experimental studies in SE used students, whereas 22% employed professionals,  and only 14% used both subject 
groups. A survey by Sjoberg et al. [3] reports similar rates 
regarding the recruitment of professionals and students as experimental subjects. 87% of 103 selected articles on controlled experiments conducted between 1993 and 2002 used students and only 9% employed professionals [3]. Moreover, undergraduates are used more often than graduates. 
Apart from the external validity issue, very few studies 
have been specifically carried out to compare the performance of students and professionals in their respective environments (e.g. [5], [6], [1]). 
 T a b l e  I  p r e s e n t s  s t u d i e s  t h a t  a i m e d  t o  c o m p a r e  t h e  
performance of students and professionals in the context of an SE experiment. In other words, we have only listed studies that were conducted to answer a similar research question to ours. 
Other empirical studies investigate both students and 
professionals, e.g. A tool usage was demonstrated from the perspective of both subject groups in [7]. However, these studies did not aim to compare the performance of the two groups, and, hence, they are not included in Table I. 
 W e  o b s e r v e  f r o m  T a b l e  I  t h a t  a l l  t h e  s t u d i e s  r e p o r t  
experiments, except one [9] which is conducted using a survey 
methodology. In some cases, instead of conducting the study with two subject groups, the researchers compared the data 
with the data from an earlier study. For example, two studies 
([1] and [9]) were conducted with students, and the researchers later compared their data with the data for professionals reported in their earlier experiments. 
 I n  m o s t  o f  t h e  s t u d i e s  l i s t e d  i n  T a b l e  I ,  t h e  s t u d e n t s  
recruited to perform a particular task outnumbered the professional participants. Two studies ([6] and [10]) reported significant differences between professionals and students, e.g., professionals performed better than students in [10] due to their experience. However, the other studies did not reveal major differences between the two subject groups.  
TABLE I.  
COMPARISON OF STUDENTS AND PROFESSIONALS IN PREVIOUS STUDIES  
Study Objective/Aim Context Type Subjects Results 
Porter & Votta [6] -Comparison of the 
performance of students and 
professionals 
-Extension of the external 
credibility of a previous study  Fault detection techniques for software requirements specification inspections Experiment 
with professionals 18 professionals48 students All hypothesis tests revealed the difference in the performances of students and professionals. 
 
Host et al. [5] Difference in the performance 
of students and professionals Factors affecting the  
project lead time Experiment 18 professionals 
26 students -Minor differences in the conception of students and 
professionals for the factors - No significant differences 
between students and 
professionals when compared with actual effect of factors 
Runeson [1] Feasibility of using students as 
subjects Improvements and performance in personal software process (PSP) Experiment with students 298 professionals 162 students Similar improvements between the levels of PSP by students and professionals 
Berander [8] -Cases of students usage in 
research studies -An exercise in release planning 
 Requirements prioritization (software 
development effort)  Experiment 20 students -Professionals in their 
industrial setting were better than students in classroom setting. 
-Students in projects setting 
were similar to professionals in industry. 
Svahnberg et al. [9] Students’ capability of imaging 
industry professionals Requirements engineering (requirements selection) Survey with students - Students may work well as subjects in this study context. 
McMeekin [10] -Measuring differences 
between students and professionals -Comparison of inspection 
techniques 
 Software inspection 
techniques for object-
oriented development Empirical 
Study 26 professionals 36 students -Professionals performed significantly better than 
student-developers. -Inspector’s experience has a 
significant effect on his/her 
ability for detection. 
667
667
667
ICSE 2015, Florence, Italy  The researchers of both of the studies that found a major 
difference between the two groups ([6] and [10]) address inspection techniques, e.g., for requirements specifications and object-oriented development. McMeekin [10] argues that experience has a significant effect on detecting faults during inspection, and the choice of subjects is judged to be a critical issue in such a context. The data used for analysis in most of the previous studies were collected directly from the participants, i.e., participants gave their subjective opinions about several factors of a study. In [5], for example, subjects were asked to prioritize the factors that affect project lead time, and the researchers compared the factor ranking based on student and professional ratings. Similarly, in [8], subjects were asked to prioritize requirements for measuring software development effort, and these requirements were compared between two subject groups.   
Our study is similar to the previous studies listed in Table I 
in that it investigates the differences between the two subject 
groups. We also employed the data from two experiments and compared the tasks implemented by the two subject groups to 
address our research goal. However, our study investigates this 
difference in the context of a technology-oriented experiment that observes the effects of the TDD approach on code quality. We recruited two subject groups (14 students and 21 professionals) in their respective environments and executed 
the same experiment using the same technological setup. 
Unlike previous comparisons, we collected the artifacts (source codes) produced during the experiment and measured code quality using a set of metrics rather than eliciting personal impressions about the dependent variable from the subjects. 
The usage of students as experimental subjects has been a 
concern in other fields, too. In the field of business studies, for 
example, Remus [11] conducted an experimental study with the aim of comparing students and industry managers. The study compared undergraduate students with experienced managers enrolled in an evening MBA program. The context was the usage of decision support systems for the problem of production scheduling. They found significant differences between the subject groups as students made more costly decisions. Moreover, students also used less effective heuristics for making decisions and were found to be more inconsistent than managers. Our study also differs from [11] in terms of the 
context and data collection. In [11], experience has an impact 
on the decision of production scheduling, as it does in [5] and [8]. In our study, we do not collect personal opinions from the two subject groups, but measure the artifacts that they produce and compare the code quality achieved by each group.  
B. Experimentation on Te st-Driven Development 
Test-driven development (TDD) is an important 
development practice within agile methodologies [9], [10]. 
TDD  requires developers to divide a requirement into smaller, implementable user stories, write the unit tests for each story, and later implement the actual code for that user story [9]. Some software organizations have been quick to adopt TDD as 
a development practice [10], while many others are still 
evaluating its benefits in terms of costs, quality and productivity [11]. Since 2010, systematic literature reviews [12], [13] and meta-analyses [10] have been published that summarize the types of empirical studies conducted on TDD, the level of rigor applied to the research, the set of measures used to address dependent variables and the findings.  
In this research, we focus on the effects of TDD on code 
quality in software projects implemented by students and professionals. Therefore, we identified previous studies 
investigating the effects of TDD on internal code quality rather 
than external code quality. Turhan and Layman [15] and Shull et al. [30] define internal  code quality as the design quality of a 
software system, and they list the set of metrics for quantifying internal code quality as object-oriented metrics, cyclomatic complexity metrics, and Halstead metrics. On the other hand, a 
system’s external code quality is usually measured in terms of 
the number of pre-release and post-release system defects [15], [30].  
Of the recent systematic reviews on TDD literature, Munir 
et al. [17] show that 15 out of 48 experimental studies (around 31%) investigated the effect of TDD on internal code quality. Four out of these 15 experimental studies used professionals as subjects, whereas 10 employed students. Furthermore, only two studies out of the 15 reported that TDD was beneficial in 
terms of internal code quality improvements. Out of these two, 
one study was conducted with 24 professionals and was a controlled experiment [18]. The other study was a pilot study with students and formed two groups of students where one group acted as a control group [19]. The other 13 studies 
reported no difference between TDD and the test-last approach 
in terms of code quality. 
Table II lists all metrics used for measuring internal code 
quality by the studies reviewed in [17]. Table II shows that 
researchers mostly choose object-oriented metrics such as nested block depth, coupling and cohesion, whereas some studies consider coverage metrics and mutation score as indicators of internal code quality. In our study, we also use metrics such as cyclomatic complexity  a n d  number of 
parameters  as code quality attributes. We did not include other 
metrics, e.g. coverage, mutation score, since these are known as quality attributes for test suites rather than for source code [16]. Furthermore, we added static code attributes that measure the code quality from the perspective of source lines of code, complexity and operator-operand counts, as they have been found to be good indicators of code defects in software systems [20]. The full list of metrics used in our study can be found in Section IV.
 
TABLE II.  METRICS USED FOR INTERNAL CODE QUALITY IN TDD  
EXPERIMENTS (MUNIR ET AL . 2014) 
 Metrics for Internal Code Quality 
Method / Statement/ Condition Coverage 
Branch Coverage 
Nested Block Depth 
Cyclomatic Complexity 
Number of Parameters 
Coupling between Objects 
Information Flow 
Weighted Class per Method 
Lack of Cohesion Metrics 
Mutation Score Indicator 
668
668
668
ICSE 2015, Florence, ItalyIII. STUDY CONTEXT  
This empirical study was conducted using the data from 
two experiments. We conducted these experiments in an 
academic setting with students and in an industrial setting with professionals. The academic site chosen for the study was 
Universidad Politécnica de Madrid (UPM), Spain. 17 graduate 
students of different nationalities, i.e., 15% of them were from South American countries and the others came from all around the world, enrolled for a five-day seminar course. The experiment with professionals was conducted at three different sites of the same software development organization: Helsinki 
(Finland), Oulu (Finland) and Kuala Lumpur (Malaysia). In 
total, 24 professionals participated in the training/experiment. The company has been operating as a multinational for over 25 years and has around 900 employees at 20 offices around the world. It has been supplying security services and products for protecting digital devices and consumer and business digital 
data for over two decades. 
The research goal of both experiments was to observe the 
effects of TDD on quality and productivity. The treatments (independent variables) were TDD and the incremental test-last 
development (TLD) approach, whereas the dependent variables were quality and productivity. Incremental test-last development differs from the traditional waterfall approach in that requirements are divided into smaller user stories in TLD, 
each of which is implemented and tested in small increments 
before moving on to the next user story. 
The experiment was replicated twice, once in academia and 
once in industry, using the same researchers, same trainer, 
training content and language (English), tasks, instrumentation and subject selection strategy. However, the replications differed in terms of the experimental design regarding assignment of tasks to treatments and the experiment protocol. In Table III, we summarize the similarities and differences pertaining to the experimental setup at both sites.
 
TABLE III.  COMPARISON OF EXPERIMENTAL SETUP  
 
1) Experimental Design : The experimental design 
followed is a one-factor two-level, within-subjects design 
[21], [22], [23]. According to the design, both industry and academia had the same sequence of treatments, as shown in Table IV. In Table IV, ES in dicates Experimental Session.
 During the industry experiment, one factor, i.e., 
development approach, was studied using two implementation styles applied twice (ABB): TLD, TDD on a toy example and TDD on a real-life application.  In the academic setting, TDD 
was applied three times (ABBB) and four tasks were randomly assigned to the treatments. 
  
TABLE IV.  SEQUENCE OF THE EXPERIMENTS . Training on unit testing ES
Training on TDD ES ES ES
TLD
MR in 
industry , 
 MR, BSK, 
MP and S 
randomly  
assigned tasks 
in academia  TDD1 
BSK  in 
industry , 
 MR, BSK, 
MP and S 
randomly  
assigned tasks 
in academia  TDD2
MP in 
industry ,  
MR, BSK, 
MP and S 
randomly  
assigned tasks 
in academia  TDD3
 
MR, BSK, 
MP and S 
randomly  
assigned 
tasks in 
academia  
 
2) Assignment of Tasks:  T h e  t a s k s  w e r e  r a n d o m l y  
assigned to treatments in the academic experiment. The 
student subjects were assigned to a different task each day, and they implemented all the four tasks during the experiment. In industry, on the other hand, three out of the four tasks, all of which were also used in the academic setting, were paired with the experimental treatments, i.e., all professionals implemented the same task for the same treatment. In both groups, tasks were implemented individually. More details about the four tasks are given in Section IV.B.  
3) Protocol: The protocol was slightly different in terms 
of the allocation of treatments to experimental sessions. For professionals, the first day was allocated to unit testing training and TLD implementation on a control task. The second day was reserved for TDD training, a practice session with TDD, and TDD implementation on the experimental task (TDD1). The third and fourth days were reserved for the practice sessions, i.e., professionals were asked to practice TDD at work, and no data were collected on these two days. On the fifth day, professionals implemented another task in TDD (TDD2). Hence, data were collected from the industry experiment on the first day (control task), second day (TDD1 task) and fifth day (TDD2 task). Students, on the other hand, received unit testing training on the first day, whereas they implemented the control task using TLD on the second day and received TDD training later on the same day. The third day was reserved for TDD implementation on several programming tasks (TDD1). On the fourth day, students implemented different tasks using TDD again (TDD2). The final day was reserved for another TDD implementation (TDD3). Therefore,
 data were collected from the academic 
setting on all but the first day. 
4) Data Collection: Using the same technological setup in 
both settings, the data (source and test codes) for the specified tasks were collected at the end of each day. In the case of students, we copied the workspaces from the workstation on which they worked, whereas for professionals, we copied  Industry 
Experiment Academic
Experiment 
Experimental Design ABB ABBB
Objects/Tasks MR, BSK, MP MR, BSK, MP, S
Trainer Same 
Time for the Tasks Same 
Protocol Five-day protoco l 
Sampling Convenience samplin g
Instrumentation Eclipse IDE with JUnit testing framework; Java 
programming language;  English as the training 
language 
669
669
669
ICSE 2015, Florence, Italyvirtual machine (VM) spaces from the computers on which 
they worked.  
IV. RESEARCH METHOD  
In this paper, we report an empirical study in which data 
from two experiments are compared to analyze how 
representative students are of professionals in SE experiments. According to the Goal/Question/Metric template, our research goal [24] is as follows: 
Analyze  students as experimental subjects 
For the purpose of  comparison with professionals 
With respect to  t h e  c o d e  q u a l i t y  o f  s o f t w a r e  t a s k s  
implemented by both subject groups 
From the point of view of the students and professionals 
In the context of  a  T D D  e x p e r i m e n t  c o n d u c t e d  i n  
academia with students and in industry with professionals. 
We chose a TDD experiment because we wanted a 
technology-oriented experiment. This should help us get a 
more objective evaluation of the two subject groups by eliminating subjective ratings by subjects. Furthermore, we have been conducting multiple experiments on TDD as part of 
an international project with several research and industry 
partners. These experiments are good basis for comparing two subject groups. Even though these experiments analyzed the effect of TDD on quality and productivity, we do not set out to report the respective results in this paper. Instead, we recount 
whether the results of using students and professionals as 
experimental subjects are similar. 
In order to address the goal of our study, we stated two 
research questions and their associated hypotheses:
 
RQ 1.1: How much does the code quality of a task1 produced 
by students using TDD differ from the code quality of a task 
produced completed by professionals using TDD? 
 
H0: The code quality of a TDD task implemented by a 
professional and of a TDD task implemented by a student is the 
same. 
 ܪ଴: ߤሺܳܥሻ ௉ି்஽஽ൌߤሺ ܳܥ ሻ ௌି்஽஽   
RQ 1.2: How much does the code quality of a task produced  by 
students  using TLD differ from the code quality of a task 
produced by professionals using TLD? 
H0: The code quality of a TLD task implemented by a 
professional and of a TLD task implemented by a student is the 
same. 
ܪ଴: ߤሺܳܥሻ ௉ି்௅஽ൌߤሺ ܳܥ ሻ ௌି்௅஽  
We divided the above two hypotheses into further null 
hypotheses based on the code quality metrics extracted from TDD and TLD implementations in the academic and industry 
 
1 In our RQs and hypotheses, “code quality of a TDD/TLD task” means the 
code quality measured on a software system that is implemented for a 
particular TDD/TLD level. experiments. For example, the null hypothesis for RQ 1.1 
regarding the cyclomatic density metric is defined as follows: 
H0CD: P_TDD = S_TDD: The cyclomatic density of a TDD 
task implemented by professionals and a TDD task 
implemented by students is the same.  
A. Variables 
The independent and dependent variables of our empirical 
study are Type of Subjects  and Code Quality , respectively. The 
independent variable is manipulated by choosing two subject 
groups, Professionals  a n d  Students , who worked in two 
experiments. On the other hand, the dependent variable, Code 
Quality, is associated with software tasks implemented by two 
types of subjects. It is measured in terms of static code attributes extracted from the source codes written by the subjects. 
B. Objects 
Four objects (programming tasks) are used in the 
experiments. Three of these programming tasks, namely 
Bowling Scorekeeper - BSK, Mars Rover - MR, and Sudoku - S, can be categorized as “toy” tasks. The toy tasks are all greenfield, i.e., there is no written code or existing constraint for the task. All of the toy tasks are of similar complexity in terms of the number of user stories, estimated time and difficulty level of their implementations. Two of these toy tasks (BSK, S) are well-known games, and the third one (MR) is a popular programming exercise within agile communities. The fourth task (MusicPhone – MP) can be categorized as a “real”, i.e., brownfield, task that has an existing architecture 
and code base. The subjects were supposed to implement the 
greenfield tasks from scratch, i.e., they were given a project skeleton only, whereas they had to add new or modify existing method(s) for the brownfield task. As already mentioned, tasks were assigned differently to treatments in the academic and industry experiments. In the industry experiment, two of the tasks, BSK and MP, were associated with TDD treatments and MR was associated with the TLD treatment. On the other hand, all tasks in the academic experiment are randomly assigned to treatments. The professionals worked on the three tasks (BSK, MR and MP) while students worked on four (BSK, MR, MP and S). Therefore, the number of tasks used in both experiments differs by one. The specification documents of these programming tasks can be found in [25].
 
C. Metrics 
 Several metrics have been used to measure internal code 
quality, some of which are presented in Table II in the context 
of TDD experiments. In this  study, we defined code  quality  in 
terms of 20 static code attributes/metrics [26] and extracted the attributes from the source codes implemented by subject groups. Whether the developed code was complete and correct is out of the scope of our study. Table V shows the descriptions of the full list of static code metrics; their calculations are also available in [26], [20]. 
D. Data Collection 
 W e  u s e  a  t o o l  c a l l e d  P r e s t  t o  e x t r a c t  t h e  s t a t i c  c o d e  
attributes. Prest is an open source metric extraction tool which 
670
670
670
ICSE 2015, Florence, Italysupports metric collection from multiple programming 
languages including Java [26]. 
TABLE V.  METRICS REPRESENTING CODE QUALITY IN THIS STUDY  
 
We extracted 20 code attributes (metrics) from the source 
codes of each subject at the method level. We chose to collect 
method-level metric data since subjects are supposed to implement new methods for or, in some programming tasks (e.g. MP), modify existing methods of a partially implemented task. Therefore, it is necessary to pinpoint even minor changes 
that each subject made during implementation. 
After the extraction of method-level metrics, they were 
transformed into subject-level metrics. We aggregated method-
level metric values to minimum, maximum, median, mean and standard deviation values at subject level and formed two datasets for two subject groups (professionals and students). The transformation from method to subject level was necessary, because all subjects implemented a different number of methods for the assigned tasks, and we needed to represent each subject as a single instance (five values of 20 metrics) in our dataset. 
V. S
UBJECTS  
The experiment at the university (academia) was performed 
as part of a seminar course that was offered to the students of an international graduate-level degree program. So, the sample population in the academic setting was nearly as diversified as in the industry setting in terms of subject nationality and language. The experiment involved a total of 17 students, who enrolled for the course in academia. The experiment at the software organization consisted of 24 professionals. Table VI presents the experience levels of the experimental subjects, students and professionals. The table shows that professionals are more experienced than students in programming skills. Eight students have more than five years of experience in programming and six have more than five years in Java programming, whereas only
 one has more than five years in 
unit testing. On the other hand, 20 of the professionals have medium to
 high experience in programming, whereas 17 have 
two or more years of unit testing experience. Furthermore, five 
professionals claim that they have experience (2 to 5 years) in 
TDD. We discussed with them and learned that although they received training on TDD, they never applied the technique in practice. So, both subject groups are similarly experienced with regard to the technology studied in the experiment (TDD). 
TABLE VI.  
EXPERIENCE LEVELS OF EXPERIMENTAL SUBJECTS  
VI. ANALYSIS  
In order to analyze the differences between students and 
professionals, we first performed normality tests on the metrics 
collected from the source codes that were produced by both subject groups. The metrics extracted from TLD and TDD tasks were tested against the normality assumption
 using the 
Shapiro-Wilk test. The test results reveal that none of the metrics were normally distributed (p-value <<
 0.05). Thus, we 
decided to use a non-parametric test, namely the  two-sample  
Kolmogorov-Smirnov test, which assesses the hypothesis that two samples (students and professionals) were drawn from different populations [27]. Unlike the parametric t-test or its non-parametric alternative, the Mann-Whitney U test, which 
checks the differences in population mean, the Kolmogorov-
Smirnov (KS) test evaluates the differences in the general shape of the distributions between two samples (differences in skewness, etc.) [27]. We chose the KS test since: 1) our aim is to check the general differences between two subject groups rather than differences in medians or variance, and 2) we 
would like to check if there are differences in terms of the 
minimum, maximum, median, mean and standard deviation of the metric data collected from the two subject groups. Furthermore, the KS test does not account for any pairing between the data from the two samples or place any limitation on sample size. 
 
VII. RESULTS  
This section presents the results of the comparative study. 
The results are presented as a comparison of the TLD, TDD1 
and TDD2 tasks for the two subject groups. We first describe the data reduction. We then report the descriptive statistics prepared from the data on professionals and students Cyclomatic Density (CD) Halstead Programming Time (HPT) 
Decision Density (DD) Maintenance Severity (MS) 
Essential Density (ED) Branch Count (BC) 
Cyclomatic Complexity (CC) Condition Count (Cnd.C) 
Essential Complexity (EC) Decision Count (DC) 
Halstead Difficulty (HD) Lines of Code (LOC) 
Halstead Length (H.Len) Total Operands (T.Oprnds) 
Halstead Volume (HV) Total Operators (T.Oprtr) 
Halstead Level (H.Ll) Unique Operands Count 
(U.Oprnd.C) 
Halstead Programming Effort 
(HPE) Unique Operators Count (U.Oprtr.C)  Programmi
ng Java 
Programmin g Unit 
Testin g JUnit TDD Students  >10 
years 0 0 0 0 0 
5-<=10 years 
8 6 1 1 0 
2-<=5 years 
5 7 2 2 0 
<2 years 
4 4 14 14 17 
Total 17 17 17 17 17 Professionals  >10 
years 10 3 2 1 0 
5-<=10 years 
10 5 3 3 0 
2-<=5 
years 4 9 14 8 5 
<2 years 
0 7 5 12 19 
Total 24 24 24 24 24 
671
671
671
ICSE 2015, Florence, Italyseparately. Finally, we present the results of the hypothesis 
testing using the statistical tests explained in the Section VI. 
A. Data Reduction 
Our analysis did not include the data on subjects who did 
not attend all the experimental sessions, since we needed the 
source codes for all control and experimental tasks for each subject. Thus, the final number of subjects was 21 (out of 24) professionals and 14 (out of 17) students. 
B. Descriptive Statistics 
In this section, we present the descriptive statistics of the 
code quality metrics collected for all three levels (TLD, TDD1, 
TDD2) implemented by students and professionals. These are 
prepared at project level from the method-level metrics data 
extracted using Prest. Table VII presents the median values of the metrics for the tasks using two treatments (TLD, TDD1 and TDD2) for students and professionals, respectively. The other metric values for minimum, maximum, mean and variance are 
reported in [28]. 
Table VII shows, for TLD, that there are clear differences 
between students and professionals for the LOC, total operands (T.Oprnds), total operators (T.Oprtr), unique operands and 
operators count metrics and for all the Halstead metrics. Applying the TLD approach, the professionals may appear to produce more lines of code and use more operands and operators in each method, although there is no increase in complexity. Comparing the median values for TDD1 metrics, 
we find that the differences between metric values are 
negligible (e.g. the Halstead volume is 0.52 for students, whereas it is 0.3 for professionals). Regarding the TLD level, we observe that professionals produce more LOC and use more operators and operands. The Halstead volume (HV) for professionals increases in TDD1 and TDD2, whereas it is not 
the case for students. 
Looking at other descriptive statistics [28], we find that the 
minimum values of maintenance severity (MS) for all tasks 
performed by students are lower than for professionals. Considering the mean values, we observe that the code produced by professionals implementing in TLD fashion is easier to maintain, whereas the source code that they implement in TDD fashion is harder to maintain. The mean values for the LOC metric indicate that students produced smaller methods than professionals in TLD task. In the TDD1 task, students produced bigger methods in terms of LOC than professionals, whereas the LOC values for the TDD2 tasks are comparable. The mean value for cyclomatic complexity (CC) in TDD1 is higher for students (as is the median in Table VII). The standard deviation values for Halstead Programming Effort (HPE) are higher for TLD and TDD1 but lower for TDD2 in student data. 
C. Hypothesis Testing 
In order to statistically analyze the differences in terms of 
code quality between students and professionals, we ran 
Kolmogorov-Smirnov (KS) tests on the minimum, maximum, mean,
 median and standard deviation values of  each metric. 
We checked the differences between students and professionals in terms of three tasks. Therefore, we ended up making 5 (values) x 3 (tasks) x 20 (metrics) = 300 comparisons between data for students and professionals.
  
Furthermore, we performed two comparisons. The first 
comparison was based on the experiment  t r e a t m e n t s  ( T L D ,  
TDD1 and TDD2), e.g., we compared the code quality for all the tasks implemented by both groups in TLD fashion.
 
TABLE VII.  TREATMENT -BASED DESCRIPTIVE STATISTICS (MEDIAN 
VALUES ) 
 
The second comparison was based on treatment/task combinations, e.g., we compared the code quality for the MR task implemented by both groups in TLD fashion only. Below, 
we report the results of the KS test for each of these 
comparisons separately. Table VIII and IX present the KS test results for the median  v a l u e s  o f  2 0  c o d e  m e t r i c s  o n l y .  T h e  
other hypothesis test results (for minimum, maximum, mean and standard deviation values) can be found in [28].  
1) Comparison in Terms of Experiment Treatments: Table 
VIII presents the results of the comparison based on experiment treatment (KS test p-values) between the two subject groups. The cells highlighted in Table VIII show the cases in which we found a significant difference between the two subject groups. 14 student entries are compared with 21 professionals for the TLD treatment. For TDD1 and TDD2 
treatments, the sample size for students is 14 students, and it is 
compared with 21 professionals using KS tests . 
 T a b l e  V I I I  s h o w s  t h a t ,  r e g a r d i n g  t h e  c o m p a r i s o n  o f  T L D  
between students and professionals, 15 out of 20 metric
 
hypotheses were rejected, i.e., 75% of the code quality metrics 
for the two subject groups follow different distributions. Of all the metric values reported in [28], we find that 55 out of 100 hypotheses were rejected for the TLD comparison.  Metric Students Professionals 
TLD TDD1 TDD2 TLD TDD1 TDD2 
CD 1 1 1 1.5 1 1 
DD 0 0 0 0 0 0 
ED 0 0 0 0 0 0 
BC 0 0 0 0.5 0 0 
Cnd.C 0 0 0 0 0 0 
CC 4 4 4 3 3 4 
DC 0 0 0 0.5 0 0 
EC 1 11  1  1 1
LOC 4 5 4 7 4 5
T.Oprnds 1 2 2 6 1 2
T.Oprt r 22 2  7  22
U.Oprnd.C 1 2 1 5 1 1
U.OprtrC 1 2 2 4 1 2
HD 0.5 1 1 2 0.5 1
H.Len 3 4 4 13.5 3 4
H.Ll 1 0.714 0.666 0.348 0.666 1
HPE 0.477 0.602 0.477 0.928 0.477 0.477
HPT 0.026 0.033 0.026 0.051 0.026 0.026
HV 0.477 0.518 0.477 0.301 0.465 0.602
MS 1 1 1 1 1 1 
672
672
672
ICSE 2015, Florence, Italy R e g a r d i n g  t h e  c o m p a r i s o n  o f  T D D 1 ,  1 2  o u t  o f  2 0  ( 6 0 % )  o f  
code quality metrics were found to be significantly different between the two subject groups, whereas the ratio is 49 to 100 
for all metric values.  
TABLE VIII.  
COMPARISON IN TERMS OF EXPERIMENT TREATMENTS  
 In the last comparison for TDD2, only seven out of 20 
(35%) of the code metric values were found to be significantly 
different between the two groups. Contrary to the previous two treatments (TLD and TDD1), we were unable to find a 
major difference in the median values of two (student and 
professional) samples for TDD2. We found that, taking into account the minimum, maximum, mean and standard deviation values, 53 out of 100 of all the hypotheses were rejected [28].  
 I n  s u m m a r y ,  t h e  t w o  s u b j e c t  g r o u p s  d i f f e r  i n  t e r m s  o f  t h e  
median values of code quality metrics for the TLD and TDD1 treatments, as more than half of the metrics for the two samples (75% for TLD and 60% for TDD1) follow different 
distributions. Overall, 55% and 49% of code quality metric 
values differ between the two subject groups for the TLD and for TDD1 treatments, respectively.  
2) Comparison in Terms of Treatment-Task Combination: 
This analysis was conducted by comparing a task implemented with a particular treatment by one group (professionals) with the same task implemented with the same treatment by the other group (students). Therefore, we consider three tasks linked by three treatments, MR—TLD, BSK—TDD1 and MP—TDD2. It is relevant to make this one-
to-one comparison, since the professionals worked on these 
three tasks on three treatment days only, whereas the tasks
 were randomly assigned to treatment days in the academic setting. Hence, the treatment-based comparison may indicate the effect of the task on our statistical test results. We checked 
the combination of the MR task and TLD treatment on student 
data and ended up with metric data for six students. We then compared these data with data collected from 21 professionals for the same task and treatment. Subsequently, data collected from seven students about the BSK task implemented in the TDD session were compared with data collected from 21 
professionals about BSK-TDD1. Finally, metric data collected 
from five students about the MP task implemented in the TDD session were compared with data for 21 professionals about MP-TDD2. Table IX presents the results of the hypothesis tests for the comparison based on the treatment-task combination for the median values of all code quality metrics.
  
TABLE IX.  COMPARISON IN TERMS OF TREATMENT -TASK 
COMBINATIONS  
 According to the results in Table IX, the treatment-task 
comparisons differ substantially from what we found for the comparisons based on treatment in Table VIII. For the MR-
TLD combination, only the median of the cyclomatic 
complexity metric denotes significant differences between the two subject groups, whereas the other median metric values do not show up any differences between students and professionals. From the results of MR-TLD for the other metric values in [28], we conclude that 90 out of 100 
hypotheses could not be rejected.  
 T h e  r e s u l t s  f o r  t h e  B S K - T D D  c a s e  s h o w  t h a t  t h e r e  i s  n o  
significant difference for any of the code quality metrics in terms of median values, whereas
 only one out of 100 
hypotheses in [28] (mean values of the Decision Density Metric TLD TDD1 TDD2 
CD 0.018 0.029 0.091 
DD 0.232 0.091 0.091 
ED 1 1 1 
BC 0.018 0.029 0.091 
Cnd.C 0.232 0.091 0.091 
CC 0.000 0.044 0.091 
DC 0.018 0.029 0.091 
EC 1 1 1 
LOC 0.007 0.008 0.029 
T.Oprnds 0.004 0.029 0.029 
T.Oprtr 0.000 0.019 0.029 
U.Oprnd.C 0.004 0.019 0.064 
U.Oprtr.C 0.002 0.029 0.091 
HD 0.000 0.008 0.029 
H.Len 0.001 0.029 0.029 
H.Ll 0.018 0.234 0.001 
HPE 0.001 0.127 0.091 
HPT 0.001 0.127 0.091 
HV 0.026 0.611 0.001 
MS 0.133 0.029 0.091 Metric MR-TLD BSK-TDD MP-TDD 
CD 0.954 1 1 
DD 1 1 1 
ED 1 1 1 
BC 0.954 1 1 
Cnd.C 0.999 1 1 
CC 0.031 1 1 
DC 0.999 1 1 
EC 1 1 1 
LOC 0.677 0.927 0.055 
T.Oprnds 0.591 0.999 0.061 
T.Oprtr 0.591 1 1 
U.Oprnd.C 0.055 0.431 1 
U.Oprtr.C 0.906 1 0.005 
HD 0.677 0.927 0.010 
H.Len 0.840 0.999 0.002 
H.Ll 0.677 0.927 1 
HPE 0.429 0.604 0.999 
HPT 0.429 0.604 0.999 
HV 0.906 0.604 0.319 
MS 0.995 1 1 
673
673
673
ICSE 2015, Florence, ItalyMetric) was rejected. Finally, for the MP-TDD case, Table IX 
shows that three out of 20 (15%) of the median values for code quality metrics point to significant differences between 
the two subject groups. This proportion
 climbs to 40% (40  out 
of 100 hypotheses were rejected) when all metric values are 
examined.  
VIII. DISCUSSION  
 The results of the hypothesis tests for the comparisons by 
experiment treatment and by treatment-task combination are 
different. The comparison of the two subject groups in terms 
of experiment treatments, namely TLD and TDD2, shows significant differences for more than 50% of code quality metrics.  
 I n  t h e  T L D  c a s e ,  w e  f o u n d  t h a t  p r o f e s s i o n a l s  p r o d u c e d  
more lines of code per method than students. Even though students produced smaller methods, the cyclomatic complexity of their methods was higher than for the methods produced by professionals. We observed a similar trend for four other metrics (total operands count, total operators count, 
unique operands count and unique operators count), i.e., 
professionals produce bigger methods with more operators and operands. A possible reason for such differences observed in the TLD case between students and professionals is experience. According to the experience levels shown in Table 
VI, professionals are more experienced in programming skills 
than students. Therefore, professionals produce higher code quality in terms of modularity (smaller methods) and complexity (less complex) in the context of this TDD experiment. However, the effect observed in this context could also be due to tasks, since professionals worked on only one 
task (MR) while students worked on four different tasks (one 
of them, MP, is considered as a difficult one). 
 I n  t h e  T D D 1  c a s e ,  s t u d e n t s  p r o d u c e d  m o r e  c o m p l e x  c o d e  
in terms of median cyclomatic complexity, lines of code and 
unique operands and operators count metrics. Professionals, on the other hand, produced fewer LOC with fewer unique operators and operands. Therefore, their code is less complex. A smaller cyclomatic complexity value in professionals’ data also leads to a lower value for Halstead metrics. Looking at all 
100 hypotheses, however, we failed to observe major 
differences between the two subject groups. Both students and professionals were using TDD in practice for the first time, i.e., neither had previous experience in the new approach. Therefore, they appear to act similarly during their first 
implementation. Another reason for this similarity might be 
the task assignment in the TDD1 case. The students worked on a randomly assigned task on the TDD1 day, whereas all professionals worked on the same BSK task. The complexity of a task, on the other hand, might influence subject performance in the TDD1 case. 
 C o n s i d e r i n g  t h e  T D D 2  c a s e ,  w e  f o u n d  t h a t  s t u d e n t s ’  a n d  
professionals’ performances are very similar in terms of median code quality metrics, except for the fact that students produced more LOC and used more operators and operands (as in the case of TDD1). Considering all the hypotheses, however, we observe differences between students and professionals in 53% of the cases. In the TDD2 case, professionals implemented a brownfield task, which is more complex to understand and modify than the other tasks. Thus, 
the differences between tasks are likely to be responsible for 
the differences observed between the performances of the two subject groups. Therefore, this
 result had to be confirmed 
through the second comparison that we made.  
 Surprisingly, most of the tests in the treatment-task 
comparison fail to reveal any differences between the two subject groups. This result strengthens our hypothesis that the tasks might have an impact on the performance of subjects. In other words differences among tasks seem to have a bigger 
impact on code internal quality than the subject type. 
 I t  i s  i m p o r t a n t  t o  n o t e  t h a t  t h e  s a m p l e  s i z e s  f o r  s t u d e n t  d a t a  
in this second comparison were quite small (five to seven 
students) compared to the sample size for professional data. 
So the results might have little statistical power. To analyze the power, we need to interpret significance level, sample size and effect size all together. We computed an effect size measure,  C o h e n ’ s  d , for the metrics that revealed significant 
differences between the two subject groups in order to 
interpret the size of the differences. We found that the effect 
sizes in all the comparisons are medium,  ( e . g . ,  0 . 5  f o r  D D  
metric in BSK-TDD1) to large, (e.g., 3 for CC in MR-TLD, 
and 1.8-2.8 for HD, H. Length in MP-TDD2). In other words, there is high probability (greater than approximately 71%  
[29]) that the code quality metrics extracted from the source 
codes are significantly different in the two groups. Based on the effect sizes, data sample size and significance levels, we could conclude that the few differences observed between subject groups on different tasks have a large statistical power. 
 
Taking into consideration our main research question 
“How much does the code quality of a task implemented by a professional differ from that of a task implemented by a student?”, we could argue that, for TLD and TDD applied on 
certain tasks, there are differences between the code quality of 
a task implemented by a student and a professional in terms of cyclomatic complexity, LOC, operator and operand counts. However, inconsistencies between the comparisons based on experiment treatment and the treatment-task
 show that: a) the 
selection of programming tasks for a particular development 
approach, and b) the experience of a subject in the 
development approach would significantly affect the findings. Subjects act differently depending on the size and complexity of the task at hand and produce software systems with different levels of complexity and size. They also act 
differently if the technology or development approach at hand 
has not been used before, e.g., professionals perform better in the TLD case but similarly to students in the TDD1 case, as it is the first time that they are applying the approach.
 
IX. THREATS TO VALIDITY  
We followed the checklist on possible threats to the validity 
of empirical studies provided by Wohlin et al. [22] and addressed the relevant ones to our study. 
674
674
674
ICSE 2015, Florence, ItalyA. Internal Validity 
1) Selection: The study is potentially exposed to this threat 
since all subjects were volunteers and might not, therefore, be 
representative of the general community. However, we had to choose convenience sampling in the case of TDD experiments within industry, as it was not possible to form a randomly generated sample in the software organization due to business strategies,
 workloads and group dynamics. We  followed the  
same selection technique in both environments for the sake of consistency. 
2) Diffusion or imitation of treatments:  In the academic 
setting, all students worked on all four tasks using a random assignment. As a result of this random assignment, students might have known the task descriptions in advance because students could discuss the tasks at the end of each day. To avoid this threat, students were told that it was important not to disclose activities. Furthermore, even though students may have discussed the tasks with each other, they were asked to follow different practices each day, i.e., slicing, coding and testing in TLD; slicing, testing and coding in TDD1; testing, coding and refactoring real code in TDD2. Therefore, it was unlikely for the development style to be transferred across consecutive days of the experiment. 
B. External Validity 
In this study, we selected samples from intended 
populations (students in an academic environment and 
professionals in an industrial setting) in order to study the differences. We took into account heterogeneity in the formation of subject groups and questioned subjects about their skill and experience levels in particular technologies and tools. Table VI confirms that both groups contain subjects with different levels of knowledge and experience in programming, unit testing and tools. Until replications of this study are run in other contexts, the scope of our findings would be limited to our study conditions: TLD and TDD as technology and subjects’ skills shown in Table VI.
 
C. Construct Validity 
1) Mono-operation bias: We addressed this threat by 
assigning multiple objects, the effect on which has been studied separately, and by choosing multiple subjects, each of which performed individually on the objects. 
2) Mono-method bias: We addressed this threat by 
quantifying code quality through 20 static code attributes that are well known in quality prediction research and utilized to detect potential faults in software systems (e.g., [15, 26, 20]). We used an open source metric extraction tool, Prest, to extract these metrics [26]. No subjective measures that might pose a threat to the reliability of study measures were applied.  
D. Conclusion Validity 
1) Violated assumptions of statistical tests : We addressed 
and avoided this threat by testing the data for normality and then choosing the relevant test for data analysis. We chose the Kolmogorov-Smirnov test as it can be used as a non-parametric test to observe differences between two samples. We further discussed our choice in Section VI.
 
2) Repeated hypothesis testing : There is a potential risk of 
accumulated type I error (a positive conclusion even though there is no significant difference) since we check multiple hypotheses  (20 metrics) for a single variable (code quality). We could have addressed this risk by applying the Bonferroni adjustment to the significance level of KS test and checking the test results with a significance level of 0.0025 (=0.05/20). Based on this significance level, we would reject fewer hypotheses in all treatments. However, we still observe that professionals and students perform differently while applying a TLD approach (7 out 20 hypotheses are rejected in Table VIII), whereas the difference is not so evident for TDD. But a reduction of the p-value might increase type II error, which is favorable to our conclusion. Thus we prefer to stick with a p-value of 0.05 in this study. 
X. C
ONCLUSION  
In this research, we investigated if students are 
representative of professionals as experimental subjects in the 
context of a test-driven development experiment. We observe 
that professionals and students differ in terms of code quality when they apply an incremental test-last approach (TLD), i.e., a development approach that subjects are used to. However, both subject groups perform similarly when they apply TDD for the first time. Further comparison when both treatment and 
task are fixed shows that differences in code quality between 
subject groups might be due to the tasks they are assigned to. However, we need larger samples to generalize our claims.  
Conclusively, our results support previous findings 
reported in [1, 5, 9] that neither of the subject groups performs better than the other when they apply a new technology during experimentation. The assessment of subject performance applying an already known technique, like TLD, confirms the results reported in [10] that experience plays a significant role 
in subject performance. 
We acknowledge
 that the research question addressed here 
cannot be answered by one experiment with a limited sample, case and process. Thus, results cannot be generalized to all SE 
experiments. However, studies like ours have the potential to incrementally build knowledge and contribute to the body of evidence. Note that a major differentiating factor affecting the results might be  subject’s experience levels  r a t h e r  t h a n  t h e  
experiment setting (classroom or industry). In an academic setting, we can find students who already possess industrial experience or in a field experiment, we can face novice professionals with regard to a particular technology. A key condition for subjects’ characterization might be the experience rather than which type of site they belong to.  
A
CKNOWLEDGMENT  
Our sincere thanks go to Hakan Erdogmus, Burak Turhan 
and Oscar Dieste for their tireless effort and valuable 
contribution to the planning and execution processes of the experiments in the context of the FiDiPro ESEIL project. This 
research is supported in part by TIN2011-23216.  
675
675
675
ICSE 2015, Florence, ItalyREFERENCES  
[1] P. Runeson, “Using students as experiment subjects–an analysis 
on graduate and freshmen student data,” Proc. 7th Int. Conf. 
Empir. Assess. Eval. Softw. Eng. , pp. 95–102, 2003. 
[2] D. I. K. Sjøberg, B. Anda, E. Arisholm, T. Dyba, M. Jorgensen, 
A. Karahasanovic, E. F. Koren, and M. Vokac, “Conducting 
realistic experiments in software engineering,” Proc. Int. Symp. 
Empir. Softw. Eng. , no. 1325, 2002. 
[3] D. I. K. Sjøberg, J. E. Hannay, O. Hansen, V. B. Kampenes, A. 
Karahasanovic, N.-K. Liborg, and a.C. Rekdal, “A survey of controlled experiments in software engineering,” IEEE Trans. 
Softw. Eng. , vol. 31, no. 9, pp. 733–753, 2005. 
[4] A. Höfer and W. F. Tichy, “Status of Empirical Research in 
Software Engineering,” no. January 1996, pp. 10–19, 2007. 
[5] M. Höst, B. Regnell, and C. Wohlin, “Using Students as 
Subjects — A Comparative Study of Students and Professionals in Lead-Time Impact Assessment,” Empir. Softw. Eng. , vol. 5, 
no. 3, pp. 201–214, 2000. 
[6] A. Porter, “Comparing Detection Methods For Software 
Requirements Inspections: A Replication Using Professional Subjects,” vol. 379, pp. 355–379, 1998. 
[7] M. Kamalrudin, S. Sidek, N. Yusop, J. Grundy, and J. Hosking, 
“MEReq: A Tool to Capture and Validate Multi - Lingual Requirements,” in Frontiers in Artificial Intelligent and 
Applications, IOS press in press ,
 2014. 
[8] P. Berander, "Using students as subjects in requirements 
prioritization," Proceedings. 2004 Int. Symp. Empir. Softw. Eng. 2004. ISESE ’04., pp. 167–176, 2004. 
[9]  M. Svahnberg, A. Aurum, and C. Wohlin, "Using students as 
subjects - an empirical evaluation," in Proceedings of the Second ACMIEEE international symposium on Empirical software engineering and measurement, 2008, pp. 288–290.  
[10]  D. A. McMeekin, B. R. Von Konsky, M. Robey, and D. J. A. 
Cooper, "The Significance of Participant Experience when 
Evaluating Software Inspection Techniques," 2009 Aust. Softw. 
Eng. Conf., pp. 200–209, 2009.  
[11]  W. Remus, "Using students as subjects in experiments on 
decision support systems," [1989] Proc. Twenty-Second Annu. Hawaii Int. Conf. Syst. Sci. Vol. III Decis. Support Knowl. 
Based Syst. Track, vol. 3, 1989. 
[12] A. Causevic, D. Sundmark, and S. Punnekkat, “Factors Limiting 
Industrial Adoption of Test Driven Development: A Systematic 
Review,” 2011 Fourth IEEE Int. Conf. Softw. Testing, Verif. 
Valid., pp. 337–346, Mar. 2011. 
[13] Y. Rafique and V. B. Misic, “The Effects of Test-Driven 
Development on External Quality and Productivity: A Meta-Analysis,” IEEE Trans. Softw. Eng. , vol. 39, no. 6, pp. 835–856, 
2013. 
[14] VersionOne, "8th Annual State of Agile Survey", VersionOne 
Inc., Tech. Rep., 2014. 
[15] B. Turhan and L. Layman, “How effective is test-driven 
development,” Mak. Softw. What Really Works, and Why we 
Believe it , no. 6, pp. 545–552, 2010. 
 [16] L. Madeyski, “The impact of Test-First programming on branch 
coverage and mutation score indicator of unit tests: An experiment,” Inf. Softw. Technol. , vol. 52, no. 2, pp. 169–184, 
Feb. 2010.  
[17] H. Munir, M. Moayyed, and K. Petersen, “Considering rigor and 
relevance when evaluating test driven development: A systematic review,” Inf. Softw. Technol. , vol. 56, no. 4, pp. 375–
394, Apr. 2014. 
[18] B. George and L. Williams, “A structured experiment of test-
driven development,” Inf. Softw. Technol. , vol. 46, no. 5 SPEC. 
ISS., pp. 337–342, Apr. 2004. 
[19] R. Kaufmann and D. Janzen, “Implications of test-driven 
development: a pilot study,” in Companion of the 18th annual 
ACM SIGPLAN conference on Objectoriented programming systems languages and applications , 2003, pp. 298–299. 
[20] T. Menzies, J. Greenwald, and A. Frank, “Data Mining Static 
Code Attributes
 to Learn Defect Predictors,” IEEE Trans. Soft w. 
Eng., vol. 33, no. 1, pp. 2–13, 2007. 
[21] W. R. Shadish and T. D. Cook, “Experimental and Quasi-
Experimental Designs for Generalized Causal Inference , Handb. 
Ind. Organ. Psychol., vol. 223, 2002, p. 623. 
[22] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, B. Regnell, 
and A. Wesslén, Experimentation in software engineering: an 
introduction , Kluwer Academic Publishers Norwell, MA, USA, 
vol. 15. 2000, p. 228. 
[23] N. Juristo and A. M. Moreno, Basics of Software Engineering 
Experimentation, Springer Publishing Company, vol. 5/6. 2001, p. 420. 
[24] B. A. Kitchenham, I. C. Society, S. L. Pfleeger, L. M. Pickard, 
P. W. Jones, D. C. Hoaglin, K. El Emam, and J. Rosenberg, “Preliminary Guidelines for Empirical Research in Software Engineering,” IEEE Trans. Soft w. Eng.,  vol. 28, no. 8, pp. 721–
734, 2002. 
[25] Task descriptions used in our experiment, [Online], Available: 
https://www.dropbox.com/sh/i7i9im898db4u0n/AAA06ZXgJ1DNEWUkagA7v_Hda?dl=0. 
[26] E. Kocagüneli, A. Tosun, A. Bener, B. Turhan, and B. 
Çağlayan, “Prest: An Intelligent Software Metrics Extraction, 
Analysis and Defect Prediction Tool", SEKE, pp. 637-642,  
2009. 
[27] F. J. J. Massey, “The Kolmogorov-Smirnov test of goodness of 
fit,” J. Am. Stat. Assoc. , vol. 46, pp. 68–78, 1951. 
[28] Hypotheses test results and descriptive statistics of our 
experiment
,[Online],Available:  
https://www.dropbox.com/sh/7y7xv2h19p4qnas/AAD2eXuhbd
Z7OF5iN1rUPmUaa?dl=0. 
[29] R. Coe, “It’s the Effect Size , Stupid What effect size is and why 
it is important,” in Paper presented at the Annual Conference of 
the British Educational Research Association, University of 
Exeter, England, 12-14 September 2002, 2002, pp. 12–14. 
[30] F. Shull, G. Melnik, B. Turhan, L. Layman, M. Diep and H. 
Erdogmus, “What Do We Know about Test-Driven Development?”, IEEE Software (Voice of Evidence),  V o l . 2 7 / 6 ,  
pp.16-19, 2010  
  
 
676
676
676
ICSE 2015, Florence, Italy