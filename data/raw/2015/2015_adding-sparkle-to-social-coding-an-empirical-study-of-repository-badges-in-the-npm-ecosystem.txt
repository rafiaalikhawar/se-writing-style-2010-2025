Adding Sparkle to Social Coding: An Empirical Study of
Repository Badges in the npm Ecosystem
Asher Trockman,†Shurui Zhou,‡Christian Kästner,‡Bogdan Vasilescu‡
†University of Evansville, USA‡Carnegie Mellon University, USA
ABSTRACT
Infast-paced,reuse-heavy,anddistributedsoftwaredevelopment,
thetransparencyprovidedbysocialcodingplatformslike GitHubis
essentialtodecisionmaking.Developersinferthequalityofprojects
usingvisiblecues,knownassignals,collectedfrompersonalprofile
andrepositorypages.Wereportonalarge-scale,mixed-methodsempiricalstudyof npmpackagesthatexplorestheemergingphe-
nomenon of repository badges, with which maintainers signal un-
derlying qualities about their projects to contributors and users.
We investigate which qualities maintainers intend to signal and
how well badges correlate with those qualities. After surveyingdevelopers, mining 294,941 repositories, and applying statistical
modeling and time-series analyses, we find that non-trivial badges,
which display the build status, test coverage, and up-to-dateness
of dependencies, are mostly reliable signals, correlating with more
tests, better pull requests, and fresher dependencies. Displayingsuch badges correlates with best practices, but the effects do not
always persist. In short,
 .
1 INTRODUCTION
Contemporary software development is characterized by increased
reuse and speed. Open-source software forges like GitHubhost
millions of repositories of libraries and tools, which developersreuse liberally [
25], creating complex, often fragile networks of
interdependencies [ 11]. Thishas earned GitHuba reputationas a
one-stop shop for software development [ 39] and as an influencer
of practices in both open-source and industry [ 33]. The DevOps
culture[31,46]alsocontributestothisacceleration,withitsempha-
sis on automation and rapid deployment. As a result, developers
areexpectedtomakemor edecisionsat higherspeed, e.g.,finding
which libraries to depend on and which projects to contribute to.
Akeyenablerofthisdecisionmakingprocessisthe transparency
provided by social coding platforms like GitHub[20,21]. The de-
velopment history of open-source GitHubprojects is archived and
publicly accessible in a standardized format, and user pages dis-play aggregate information about one’s contributions and social
standing in the community (e.g., through starsandwatchers). This
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180209transparency can enhance collaboration and coordination [ 21]. Us-
ing visible cues—known in the literature as signals—collected from
personalprofileandrepositorypages,developerscanbetterman-
age their projects and dependencies, communicate more efficiently,
becomeinformedaboutactionitemsrequiringtheirattention,learn,
socialize,andformimpressionsabouteachother’scodingability,
personal characteristics, and interpersonal skills [21, 38, 40, 57].
However, open-source ecosystems are also competitive. In order
to survive and thrive, projects must successfully attract and retain
contributors, and fend off competitors [ 16,36,42,45]. In a social
coding environment, the visible signals enabled by transparency
can,therefore,beseenasasurvivalmechanism,withhighprofile
signalers benefiting the most. For example, more popular and fa-
mous projects attract more contributors [ 62], coding “rock stars”
collect thousands of followers [ 20], and visible traces of developer
actionsandinteractionsareusedinrecruitmentandhiring[ 13,37].
Here we focus on repository badges, images such as
 ,
embeddedintoaproject’sREADME,oftengeneratedon-demand,
reflecting the current status of online services the project is using,
e.g., continuous integration and dependency management. From a
signaling theory [52] perspective (§2), badges can be seen as eas-
ily observable signals used by maintainers to convey underlying
qualitiesoftheirprojects, e.g.,codequalityandadherencetobest
practices. The resulting increased transparency (hard to observe
qualities become salient) may impact users’ and contributors’ deci-
sion making and the project’s chances of survival. Badges can also
beseenasa gamificationmechanism [23],i.e.,agame-likeincentive
designed to engage participants (§2); e.g., a badge with real-time
codecoverageinformationmayactasanincentiveforcontributorsto improve the project’s test suite. In summary, badges are a poten-
tially impactful feature in transparent, social coding environments.
However, the value and effects of badges are not well understood.
In this paper, we explore two main research questions regarding
badges.First,weexplorethephenomenonquantitativelyandquali-tatively,andask(
RQ 1)Whatarethemostcommonbadgesandwhat
does displaying them intend to signal? Second, we analyze whether
badgesindeedsignalwhatdevelopersexpect,andask( RQ 2)Towhat
degreedobadgescorrelatewithqualitiesthatdevelopersexpect? To
this end, we perform a large-scale mixed-methods empirical study
ofthebadgesin npm,alargeandvibrantopen-sourceecosystemfor
JavaScript with documented interdependency-related coordination
challenges[ 11],whereinmanybadgesoriginated.Weobservethe
frequency and historical adoption of badges among 294,941 npm
packages,wesurveymaintainersandcontributorsabouttheirin-
tentions and perceptions, and we build regression models to testhypotheses regarding developer perceptions (collected when ex-
ploringRQ 1),suchas,“coveragebadgessignaltheimportanceof
tests and therefore attract more pull requests with tests.”
5112018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Asher Trockman, Shurui Zhou, Christian Kästner, Bogdan Vasilescu
Ourinvestigationrevealsthatbadgesarepopularin npm,adopted
in46%ofpackages.Themostfrequentshowthebuildstatusorver-
sion of the latest release, but dependency managers, code coverage,anddownloadcountsarealsocommon.Maintainersintendtosignal
various qualities with badges, and, indeed, we found among others
that build-status and coverage badges correlate with larger test
suitesandencourageexternalcontributorstoincludemoretests,
popularity badges correlate with future gains in downloads, and
the introduction of dependency-manager badges correlates with a
lastingimprovementindependencyfreshness.Correlationsarepar-ticularlystrongfor assessmentsignals, i.e.,badgesthattestanunder-
lyingqualityratherthanjuststatingintentions.Ourresultsprovideguidanceforpackagemaintainerstomakemoreinformeddecisions
aboutbadgeadoptions,beingmoredeliberateaboutwhattheyin-
tend to signal and how that signal is supported. For users and con-
tributors,ourresultsindicatewhichbadgesprovidereliablesignals.
In summary, we contribute (1) a survey among npmdevelopers,
(2) a large scale analysis of 294,941 npmpackages and their history,
and (3) an in-depth analysis (using multiple regression models
andtime-seriesregressiondiscontinuitydesigns)of8hypotheses
regarding the effect of badges on various qualities, showing many
badges are indeed reasonably reliable signals. Furthermore, (4) we
frameourdiscussioninthecontextofsignalingtheoryandconfirm
that badges based on assessment signals are more reliable.
2 THEORETICAL FRAMEWORK
Wearguethatbadgeareintendedtosignalanunderlyingquality
abouttheprojecttopotentialusersandcontributors.Inaddition,
certainbadges(e.g.,codecoverage)mayencouragecertainkinds
ofpractices,orattemptstoimprovevisiblescores.Therefore,we
frameourstudyinthecontextofsignalingtheoryandgamification.
Signalingtheory. Signalingtheoryiswidelyappliedto selection
scenarios in a range of disciplines, from economics [ 51] to biol-
ogy[63].Inthesescenarios,the signalerbenefitsfromactionstaken
bythereceiver(e.g.,beingselectedoversomealternative),which
wouldnothaveoccurredintheabsenceofthe signal.Signalsare
observablepiecesofinformationthatindicateahiddenqualityof
the signaler. Receivercost to interpret the signaltends to outweigh
signal accuracy, with less reliable but more easily obtainable sig-
nals being preferred by receivers over more reliable signals that
are costlier to observe or assess [ 28]. The classical example in eco-
nomics is job market candidates signaling their quality through
education: holding a degree from a prestigious institution is easily
observable and communicates to potential employers the candi-
date’sability,whichisotherwiselessreadilyobservable[ 51].Selec-
tion scenarios occur routinely in open-source, e.g., choosing which
librariestodependon[ 11],repositoriestowatch[ 49],developersto
follow [10,35]o rh i r e[ 13,37], and projects to contribute to [ 9,14].
The phenomenon underlying signaling theory is information
asymmetry [52],whichoccursbetweenthosehavingalltheinfor-
mation and those who could potentially make better decisions if
they had all the information. To reduce information asymmetry,
actorsrelyonobservablesignals( e.g.,informationinCVs).Informa-
tion asymmetry also occurs in the open-source selection scenarios
above. Evenif activitytracesare typicallypublicly accessible,not
allinformationisequallyaccessible,withsomerequiringspecial-
ized mining, e.g., of git histories and issue trackers. For example,to avoid outdated, possibly vulnerable dependencies, developers
mayadoptadependencymanager, e.g.,Gemnasium, David[11,41],
to receive notifications when a dependency is updated; however,to potential users of the package, this practice is very difficult to
recognizeunlessitismadeobvious.Abadgereportingtheresultof
the same analysis, e.g.,David’s
 , indicates pub-
licly that the tool is not only enabled, but also used regularly. That
is, weargue that repositorybadges are signals : bymaking certain
information about the project’s code base or practices transparent,
badges contribute to reducing information asymmetry between
maintainers (insiders) and users and contributors (outsiders).
Researchhas confirmed thatintransparent, socialcodingenvi-
ronments,observablesignalsinonlineprofilesareusedasindica-
tionsofexpertiseandcommitment[ 20,38,48,55,56],e.g.,Stack
Overflow reputationscore, GitHubfollowersandlongestactivity
streak. We expect that repository badges may have a similar effect.
Assessmentsignals. Theliteraturedistinguishesbetween conven-
tional signals andassessment signals [24]. Both are used, but the
formerarenotinherentlyreliable;thequalitytheyindicateisestab-
lishedbyconventionandthesignalistypicallycheaptoproduce,
thereforeeasytofake.Thelatterareconsideredmorereliable,be-
cause“thequalitytheysignalis‘wasted’intheproductionofthe
signal, and the signal tends to be more expensive to produce for
anindividualwithlessofthequality”[ 48].Therefore,forasignal
to be effective, it must be both: (i) observable, i.e., readily notice-
ablebyoutsiders(otherwiseitmightgounnoticed), and(ii) costly
toproduce,suchthatonlyhigherqualitysignalerscanabsorbthe
costtoproduceit[ 17].Thereisa greatdiversityofbadgeswhich,
despite being equally observable on READMEs, vary widely in
productioncost.Some, e.g.,David’s
 ,indicate
relativelydeeptechnicalqualitiesthatareachievedwithspecific,ar-
guably costly, practices. Others indicate technical or non-technical
butrelativelyshallowqualitiesthat areeasytolookupelsewhere,
e.g.,
 ,
 ,and
 ,andothersstillaremere
statementsofintentionswithoutanyautomatedvalidationandthus
withoutanyassociatedcosts, e.g.,
 and
 .
Our goal is to evaluate the reliability of badges as signals, by as-
sessingsignalfit[17],i.e.,theextenttowhichsignalscorrespondto
the desirable unobservable quality of the signaler, described by the
strength of the statistical association between public information—
the signal—and private information—the unobservable quality.
Gamification. Using game-design elements in non-gaming con-
texts[23]ismainstreamin“socialprogramming”[ 6,54]environ-
ments,e.g.,onQ&Asiteslike StackOverflow.Thesegamification
elementsareknowntomotivateexistingusers[ 3,15,27]aswellas
attract new users, at the detriment of other platforms [53, 58].
Despite being voluntarily displayed on project READMEs by
maintainers, and not “earned” based on performance and automati-
cally displayed by the platform, we argue that repository badges
arealsogamificationmechanisms.Sincebadgestypicallyrepresent
best practices and tools (e.g., continuous integration, code cover-
age, dependency freshness), there is little incentive for maintain-
ers todisplay badges indicativeof “bad” practices(e.g.,
 ,
).Therefore,weexpectthatthemerepresenceof GitHub
badgescorrelateswithbestpractices( i.e.,theyareinasense“earned”).
Furthermore,aswewilldiscuss,badgessuchas
 may
512
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Repository Badges in the npm Ecosystem ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
incentivizecontributorstofollowbettertestingpractices.Hence,
GitHubbadges may have a similar effect to Stack Overflow
badges, steering user behavior towards specific practices [4].
Understanding practices in software ecosystems. Manystud-
ieshavelookedintospecificpracticesinsoftwareecosystems,in-
cluding communication [ 8,29,32,50], change planning [ 11,12,22,
44], dependency updates [ 5,19,30,34,41], static analysis [ 7,64],
testing and continuous integration [ 31,60,65], and many others.
Badgesandtheirunderlyingtoolsarealsoecosystem-levelpractices.
Incontrasttopriorwork,however,wespecificallytakeabroader
view on badges as signalsbeyond individual tools and practices.
3R Q 1: BADGES ON npm
We study the adoption and effects of badges in the npmecosystem.
npmis a package manager and corresponding repository launched
in 2010, currently hosting package releases for 500,000+ distinct
packages.Itwasoriginallydesignedfor Node.jsdevelopers,butis
usedmorebroadlybymanywebdeveloperstoday.Asinotherpack-
age managers and repositories, e.g.,MavenandRubyGems,a nnpm
package bundles files (typically JavaScript) with a README and
metadata (package.json ), which includes the unique release version
and dependencies toother packages. A clientinstalls and updates
dependenciesfromthecentral npmrepository.The npmcommu-
nity is generally considered innovation friendly, frequently adopts
external packages, values making it easy to contribute and pub-
lishpackages,andshowsahealthycompetitionbetweenmultiple
equivalent packages to solve any single problem [1, 11, 22, 61].
We chose npmbecause: (1) it provides API access to all package
releasesandmetadata,includingdownloadcounts,(2)most npm
packages point to a GitHubrepository, (3) the npmregistry and
GitHubbothprominentlyshowthepackage’sREADMEfile,pro-
viding a common place where badges are displayed, and (4) the
npmcommunity is innovation friendly and broadly experiments
with and adopts developer services [ 11,41], including cloud-based
continuous integration, dependency managers, and badges.
3.1 Research Methods
Toexplorewhichbadgesarecommonandwhattheyintendtosignal
(RQ 1), we conducted a survey and mined repositories at scale.
Survey design. To gauge perceptions and anticipated effects we
designedtwoonlinesurveystargeting npmpackagemaintainers
and corresponding GitHubcontributors; the former focused on
what maintainers intend to signal about their packages by display-
ing badges and what effects, if any, they expect badges would have
ontheirusersandcontributors,whilethelatterfocusedonwhat
inferences contributors make about a package given its badges (for
the specific questions see appendix). Both groups were asked to
namespecificbadgeswhenanswering.Weusedmostlyopen-ended
questions with free-text answers, and we piloted the survey first.
Weextractedcontactinformationandcommitcountsperperson
fromthegitlogsofthe10,000mostpopular npmpackagesbydown-
loads,andclassifieddevelopersasmaintainers( ≥33%ofproject
commits) or contributors ( <10%). We resolved multiple aliases us-
ing standard heuristics about common first name/last name/email
formats[ 59].Wethenrandomlysampledtwomutuallyexclusivesetsofcontributorsandmaintainers,300each,andsentpersonal-
izedinvitationemails(580succeeded,294tomaintainersand286
to contributors). We received 32 maintainer and 57 contributor re-
sponses, for a total response rate of 15.3%. Our respondents have a
median5yearsofexperiencewithopensourceandmanysurveyed
maintainers have contributed to dozens of packages. We analyzed
the textual responses using standard open-coding techniques.
Repositorymining. Wecollectedamultidimensionallongitudinal
data set of 294,941 npmpackages, as follows. We started mining all
npmpackagesonJuly11,2017(512,834),thenkeptonly346,369that:
(1)hadmetadataondownloads,releases,dependencies,dependents,
andmaintainersand(2)linkedtoa GitHubrepository.In11,316
cases when multiple packages linked to the same GitHubrepos-
itory,1we kept only the most downloaded one, which further re-
ducedthesizeofoursampleto322,734.Next,weattemptedtoclone
allGitHubrepositorieslocally,andsucceededfor294,941packages;
the others’ repositories were either private or missing.
Toidentifybadgesandtheirevolution,weusedthegithistoryof
eachrepository’sREADMEfile.2Thiswasiterative:Webeganby
matchingthemarkdownexpressiontypicallyusedtoinsertanSVGbadge,i.e.,
[![Badge Name](Image URL)](Service URL) ,butdis-
covered packages with badges added as plain HTML, markdownreference links, and PNG. Consequently, we converted the mark-
down to HTML and matched
imgtags. To reduce false positives
(not all images are badges), we curated a list of services frequently
associatedwithbadges, e.g.,TravisandCoveralls,anddevised
regular expressions for classification. We then split all images into
other-badge orother-images anditerativelyrefinedtheclassification
rulestodefinebadgeclassesforspecificservices,suchas Travis.
To support the iterative process, we generated web pages showing
allfoundbadgesperclasstoinspectthemmanuallyforaccuracy.
As needed, we refined our classification until we reached stability
(very few false positives in each class) and until the other-badge
category comprised only obscure badges and non-badge images.
Overall, we identified 88 kinds of badges (examples in Table 1). By
analyzing READMEs longitudinally (following only first parents in
the commit history so as not to detect temporary discrepancies be-
tweenthemainlineandotherbranches),wecouldidentifythedates
whenbadges wereintroducedorremoved. Notethatwe analyzed
badges on GitHub, not npm.GitHubprovides a finer temporal
granularity of commits, whereas npmtypically shows the same
README files but only updates them with each new release.
Threats to validity. Astypicalforasurvey,oursamplemaysuf-
ferfromselectionbias.Oneshouldbecarefulwhengeneralizing
the results beyond the studied corpus of npmpackages with cor-
responding GitHubaccounts. The npmcommunity has certain
characteristics and results may differ in other communities; e.g.,
PythonandJavadevelopersmayadoptinnovationsatadifferent
paceandmayusedifferentkindsoftools;notallpackagereposi-tories show READMEs with badges as prominently as npmdoes.
Resultsmayalsodifferoutsideofanopen-sourcecontext, e.g.,when
using badges to advertise practices among corporate teams.
1In most cases, code is developed together in one repository, but deployed as multiple
packages to reduce user download size when only parts of the project are needed.
2Weconsideredall GitHub-supportedfilenameextensionsformarkdownfiles(see
github.com/github/markup), most commonly README.md.
513
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Asher Trockman, Shurui Zhou, Christian Kästner, Bogdan Vasilescu
Table 1: Categories of badges present in our data.
Badge Name Description Adoption ST
Quality Assurance
Travis CI Build status 92789 (31.5%) A
Coveralls Test coverage 17603 (6.0%) A
CodeClimate Coverage & static analysis 6652 (2.3%) A
CodeCov Test coverage 4788 (1.6%) A
Circle CI Build status 3518 (1.2%) A
AppVeyor Build status 2629 (0.9%) A
BitHound Static analysis & dep. mgmt 1181 (0.4%) A
SauceLabs Cross-browser testing 751 (0.3%) A
Inch CI Documentation 437 (0.1%) A
Dependency Management
David DM Version tracking 23601 (8.0%) A
Gemnasium Version tracking 2851 (1.0%) A
Greenkeeper Version tracking 1599 (0.5%) A
Snyk Vulnerability tracking 883 (0.3%) A
VersionEye Version & vuln. tracking 240 (0.1%) A
Information
Version npm/GitHub version 64200 (21.8%) L
License License information 6250 (2.1%) S
JS Standard Coding style 5299 (1.8%) S
Semantic Rel. Release strategy 1001 (0.3%) S
Commitizen Commit msg. conventions 705 (0.2%) S
Heroku Installation help 463 (0.2%) S
PRs Welcome Static information 299 (0.1%) S
Popularity
Downloads npmdownload statistics 15552 (5.3%) L
cdnjs Host of popular libraries 1902 (0.6%) L
Twitter Twitter link and stats 810 (0.3%) S
GitHub Stars Github statistics 630 (0.2%) L
Support
Gitter Chat & collaboration 4786 (1.6%) S
GitHub Issues Issue statistics 1213 (0.4%) L
Slack Chat & collaboration 688 (0.2%) S
Other
Donation link PayPal, Patreon, ... 1632 (0.6%) S
Donation stats Gratipay, Gittip, ... 919 (0.3%) L
Ember Observer Reviews and scoring 474 (0.2%) A
ST (signal type): A—assessment signal based on nontrivial analysis or aggregation;
L—lookup of readily available information; S—static statement of information
3.2 Survey Insights and Hypotheses
Maintainer respondents to our survey generally see badges as im-
portant and avastmajority(88%)agreewiththestatement“Icon-
siderthepresenceofbadgesingeneraltobeanindicatorofproject
quality.” Among contributors, badges were se en as more controver-
sial: only 53% agreed with the same statement and 61% stated that
badgesdonotinfluencetheirdecisiontocontributetoapackage.
In their explanations some contributors indicated that badges pale
in comparison to other reasons for contribution, but many also say
that they consider them or expect to be influenced unconsciously.
Answers fromboth groupscovered a spectrumof badges,most
commonly build status, downloads, latest version on npm, and test
coverage, but many others were also mentioned. We group badges
in the following categories (see also Table 1): quality assurance, de-
pendencymanagement, information, popularity, support,and other.
Correspondingtothedistinctioninsignalingtheory(§2),inTable1
wedistinguishbadgeswithdifferentsignalingtypes[ 24]depend-
ingonwhethertheyshowresultsofdeeperanalyses(assessment
signal),summarizereadilyavailableinformation,ormerelystate
unvalidated information (both conventional signals).
Mostimportantly, thesurvey providesinsights intowhat main-
tainersintendtosignalandwhatconsequencestheyexpect,aswell
as insights into how contributors and users might interpret badges:Qualityassurance. Allsurveyedmaintainersandmostcontribu-
tors (89%) mentioned specific badges related to quality assurance.
Most maintainers (84%) stated explicitly that they intend to signal
codeanddevelopmentquality—from“havinganytestsandrunning
themregularly,”tosignalinggoodqualityassurancepracticesmore
generally, including striving for high coverage, standardizing code
layout, and using static analysis tools. Respondents often intended
tosignalqualitybroadly, e.g.,statingthattheirbadgesshowthat
theircodewas“builtwithlove”or“wellwritten”byan“experienced
developer” who pays “attention to quality.”
H1.Theadoptionofquality-assurancebadgescorrelateswithother
indicators of code quality (metric: test suite size).
H2.The adoption of quality-assurance badges correlates with in-
creased user confidence and attractiveness (metric: downloads).
Severalmaintainers(28%)alsoindicatedthatcontinuousinte-
gration,testcoverage,andstaticanalysisbadgessetexpectations
of contribution quality for new contributors. As one maintainer
phrasesit,withacoveragebadge,“PRswithnewfunctionalitytend
to include new tests, as not to decrease coverage.” This suggests
coveragebadgesmayhaveanadditionalgamificationeffectrelative
to other quality assurance badges.
H3.Theadoptionofaquality-assurancebadge,andevenmoresoofa
coverage badge, correlates with more external contributors including
tests (metric: percentage of PRs with tests).
Dependency management. Dependency-management badges
(mentioned in 26% of all responses) indicate whether direct and
indirectdependenciesrefertooutdatedversionsorevenversions
with known vulnerabilities. Respondents indicate that good depen-
dencymanagementpractices(signaledwiththesebadges)reduce
the chance of “conflicting versions of nested dependencies” and
indicate attention to updates and security patches.
H4.Theadoptionofdependency-managementbadgescorrelateswith
fresher dependencies (metric: freshness, see below).
Informationandnavigationlinks. Thoughmentionedbymany
(56%), usually as convenient shortcuts, we expect that link-related
badgesdonotprovidemuchsignalingimpactbeyondotherstate-
ments or links in the package description. Respondents suggested
the followingpotentialeffects:Showing the latest npmversion ofa
packageasabadgemightencouragemoreuserstomorequickly
updatetothelatestversion.Badgeslinkingto npm,Heroku,CDNJS,
or other external sites make it convenient for users to download
andexperimentwithapackageandmaythusattractmoreusers.
Badges indicatinghigh code quality (includingstyle conventions)
might encourage more users to attempt to read the code. Badges
indicatingexplicitlythatthepackageisopenforcontributionsmay
lower the bar for new contributors. Badges indicating licenses can
makeiteasierforuserstomakeadoptingdecisions.Nonetheless,
following signaling theory, we expect at most marginal effects.
H5.The adoption of a link-related badge does not correlate with
either popularity or code quality.
Popularity. Popularitybadgeswerementionedin25%ofresponses.
Five maintainers explicitly mentioned that they intend to signal
package popularity, to “instill confidence in new users,” and sev-
eralcontributorsindicatedthatpopularityisanimportantsignal
when deciding between similar packages, because the “wisdom of
514
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Repository Badges in the npm Ecosystem ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●Adoption ra te
2012 2013 2014 2015 2016 20170.00 0.10 0.20 0.30●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ● ●●
●Quality assurance
InformationDependency mgmt.
PopularitySupport
Figure 1: Badge adoption rate, by category; showing % of all
npmpackages in a given month with 1+ badge of that type.
the crowd” has deemed a package safe to use or of high quality.In addition, popularity is seen as a sign for likely sustainability.
Especially the npm-downloads badge was mentioned frequently as
an important addition to GitHubsignals like stars and number of
contributors. While downloads can be looked up on npm, a badge
was often mentioned as a more convenient and direct means.
H6.Theadoption ofpopularity-related badgesinpopular packages
correlateswithmorefuturedownloads(metric:monthlydownloads).
Support. Supportbadgeswerementionedinfrequently(9%),but
some interpret them to signal “dedication to offering support.”
H7.Theadoptionofasupport-relatedbadgescorrelateswithmore
responsive support (metric: issue closing time).
Too many badges. An interesting facet to explore that came up a
few times in the survey is that packages with too many badges can
be perceived as cluttered or “trying too hard” and may be taken
lessseriously:“Peopletendtooverwhelmvisitorswithtoomany
(useless)badges,thuscreatingacontraeffectandloosingtheinitial
purpose of having useful information.” Hence our final hypothesis:
H8.The number of badges correlates non-linearly with popularity.
3.3 Badge Popularity and Adoption
Ofour294,941npm packages,46%haveatleastonebadge.Adoption
statistics per badge (Table 1 for the most popular) reveal that only
fewbadgesarebroadlyadopted. Build status and version badges
are by far the most common, followed by dependency managers,
testcoverage,anddownloadstatistics.Alongitudinalanalysisof
badgeadoption(Fig.1)showsthatquality-assurancebadges(Travis
CIprimarily) wereadopted early andquickly, but seemedto have
reachedsaturation(roughlyeverythirdnewpackageaddsaquality-
assurancebadge).Otherkindsofbadgeshavebeenadoptedlater
and at lower rates; most seem to have reached saturation as well.
Badgestendtobeadoptedingroupsandarenotfrequently
changedafterward. Of 136,865 packages with badges, 66% adopted
multiple kinds, of which 82% did so within 24 hours. Combina-
tionsofbadgesfollowtheiroverallpopularityandtypicallyinvolve
quality-assurance,information,anddependency-managerbadges.
Whilethereareoftenmultiplebadge-relatedcommitswhenbadges
are first adopted (including temporary removal), only 13% of pack-
ages changed any badge more than 15 days after adopting their
first. Permanent badge removal is also rare (11% of packages).
4R Q 2: EFFECTS OF BADGES
After providing an overview of badge adoption in practice and
collecting hypotheses about what their effects might be, we cannowtestthesehypotheses.Inparticular,wewanttotesttowhat
degree the presence of badges correlates with certain expected
qualitiesofthepackage,whichweoperationalizewithmeasures
such as downloads or rate of external contributions with tests.
4.1 Data and Methods
Data analysis. To evaluate the badges’ signaling reliability we
proceed in three complementary steps per hypothesis.
Step1:Correlation. Welookforcorrelationsbetweenpresence
of badges and differences in the quality they are signaling. This
analysis takes the outsider’s perspective of somebody looking at a
repositorynow,andexploreswhetherbadgesarereliablesignalsfor
certainqualities,independentofcausalrelationships,confounds,
orhistorictrends.Inlinewithourhypotheses,wetypicallyanalyze
categoriesofbadgestogether,asbadgeswithinacategorycanbe
expectedtorepresentsimilarsignals.Weusethenon-parametric
WMW test to compare distributions and report Cliff’s delta.
Step2:Additionalinformation. Badgesmaycorrelatewithvarious
qualities,butstillberedundantorweakerpredictorsofthosequali-
ties,comparedtoothersignals.Hereweexplorewhetherbadges
addinformationtoexplainthequalitiesbeyondreadily-available
signals,e.g., stars and issues shown on GitHub, downloads and
dependentpackagesshownon npm.Toassesstheinformationgain
with badges, we model the variability in the underlying quality
usinghierarchicallinearregression.Specifically,wecomparethefit
ofabasemodel,whichincludesonlyreadilyavailablesignalsand
control variables, and a full model, which adds badge predictors.
The difference, i.e., the added explanatory power attributable to
badges, suggests their association with deeper-level qualities.
We use different types of linear models depending on the re-
sponse variable (details with each result below), but always follow
thesameprocedureformodelfitanddiagnostics.First,weconserva-tivelyremoveoutliersforpredictorswithexponentialdistributions,
i.e., those values exceeding k(1+2/n)median( x)+θ[43], where
θis theexponential parameter [ 47], and kis computed such that
notmorethan1%ofvaluesarelabeledasoutliers;amongtheseare
highleveragepointsthatdisproportionallyaffectregressionslopes,
affecting robustness. Second, we diagnose the models, checking
formulticollinearity(varianceinflationfactor,orVIF[ 2]belo w3,
except between the interaction terms and their comprising factors,
whichisexpected),andcheckingifmodelingassumptionshold(no
significantdeviationfromanormaldistributioninQQ-plots,ran-
domlydistributed residuals acrosstherange). Finally, weconsider
modelcoefficientsimportantiftheyarestatisticallysignificantat
0.05 level, and we estimate their effect sizes from ANOVA analyses.
Step3:Longitudinalanalysis. Previousstepslookatdifferencesbe-
tweenpackagesthathavebadgesnow.Alongitudinalanalysiscould
reveal whether packages with badges evolve differently than with-
out,andwhetherintroducingafirstbadge(theintervention)hasan
observableeffectonthepackage’squalityasthepackageevolves. Here
weuseapowerfultimeseriesanalysismethod—timeseriesregres-
siondiscontinuitydesign(RDD) [18]—toevaluatelongitudinaleffects
ofdisplayingthefirstbadge.WithRDD,weestimatethemagnitude
ofafunction’sdiscontinuitybetweenitsvaluesatpointsjustbeforeandjustafteranintervention.RDDisbasedontheassumptionthatintheabsenceofaneffect,thefunction’strendaftertheintervention
515
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Asher Trockman, Shurui Zhou, Christian Kästner, Bogdan Vasilescu
wouldbe continuousinthesame wayasprior totheintervention.
We consider the earliest display of a badge as the intervention, and
comparedataaboutthesignaledunderlyingqualitiesin18monthly
windows, 9 months on each side, centered around the adoption
month. Aligning the history on the intervention date, we can com-
pare9-monthtrendsbefore/afteraninterventionacrossmanypack-
ages,lookingforsuddenjumpsattheinterventionandlong-term
differences in trends. We use multiple regression to estimate the
trendintheresponsebeforethebadgeadoption(variable timeinour
models,e.g.,Table2),andthechangesinlevel(variable intervention )
andtrend(variable time_after_intervention )afterthebadgeadop-
tion,cf.[65].Bycontrollingforconfoundsinthemultipleregression
(includingpresenceofotherbadgeclasses),weevaluatewhetherthe
change could be attributed to other factors than the intervention.
To account for projects that adopt multiple badges (§3.3), we
align on the first badge adoption but add controls for the adop-
tion of other badges if they occur within 15 days of the first badge
adoption. As usual, the controls allow us to isolate the effects of
individual badges. The intuition is that, given the resolution of our
analysis,interventionswithin15dayscanbeconsideredassimul-
taneous.Lateradoptionsarenotconsidered(existingonlyin17,217
packages), but also do not systematically bias any specific month.
Repository mining and operationalization. Using the same
dataset (294,941 packages; §3.1), we collected, besides badge adop-
tion,datafromthreesources,bothforacurrentsnapshotforSteps1
and2(July2017)andlongitudinallyforStep3(monthly,January
2010—July 2017): (1) package metadata on npmusing the npmAPI
(e.g.,downloads, releases,dependencies),(2) GitHubprojectdata
using the GitHubAPI andGHTorrent [26](e.g., contributors,
issues,pullrequests),and(3)thepackage’sgitrepository,cloned
locally(e.g.,code size,badges, tests).Specifically, wecollected the
following data. Representing readily-available signals of an npm
package, we collected download statistics (npm), the number of
stars and issues (GitHub), commit counts (git), and the size of the
README file in bytes (git). As further controls, we collected the
package’s age (npm), the size of the code base in bytes (git), thenumber of dependencies per package (npm), and the number of
dependents, i.e., other packages depending on the package (npm).
Weoperationalizethequalitiesinourhypotheseswiththefollow-
ing metrics for which we collect both current and historic values:
•Indicator of code quality ( H1): We measure the (relative) size
of the test suite, by identifying files and directories that referto test code, reusing the detector maintained by the package
searchservice npms.io.Wemeasuresizeinbytes,asitisrobust
to different test frameworks and file formats.
•Indicator of contribution quality ( H3): We measure how many
pull requests contain test code (GitHub), using the same mecha-
nism to identify test-related files.
•Indicator of users and popularity ( H2,H6,H5,H8): We collect
download counts from npm.
•Indicatoroffreshdependencies( H4):Inspiredbyrecentwork[ 19],
we design a freshness score that performs a similar analysis to de-
pendency managers, based on how many dependencies declared
in a package have a newer version that existed on npmat the
time(git, npm).Foreachpackage,wecomputetheaveragefresh-
nessofalldirectdependencies, i.e.,theaveragedistancebetweenthe specified version and the most recent release. Assuming that
larger version changes require more effort to update, we assign
a distanceof 1for eachchange ofa patchversion, 5per minor
version change, and 20 per major version change. An up-to-date
dependency has distance 0. Details of the metric, including how
version ranges are handled, are described in the appendix.
•Indicatorofsupport( H7):Wecollecttheaveragetimebetween
whenanissueisfirstpostedandwhenitisclosed,forall GitHub
issues closed in July 2017, as the package’s average issue latency.
Threats tovalidity. Imperfect measures. Our operationalized mea-
sures can only capture some aspect of the underlying quality indi-
catedinthesurvey.Forexample,largetestsuitesareanindicator
ofgoodtesting practices,butneither theonly northe mostreliable
indicator. However, while individual packages may vary in their
practices,thelargesizeofthedatasetswestudyimpliesareduc-
tion to the mean in terms of individual behavior. Therefore, we
expect that by averaging over thousands of packages in our regres-
sionmodels,our(imperfect)measureswillmeaningfullyreflectthe
intensity and directionality of underlying relationships between
badges and project qualities.
To ensure internal validity, in our analysis Steps 2 and 3, we
removeoutliersandexploredifferentoperationalizations,whenever
practical, to improve robustness. We further account for manycovariates, especially otherreadilyavailable signals,but wemaymiss other confounds easily observable by humans but hard to
assess automatically, e.g., the quality of the documentation beyond
our proxy of README file size. Consequently, one must be careful
when generalizing our results beyond the studied measures.
Badgesvs.practices. Typically,wecannotdistinguisheffectsof
practiceadoptionfromeffectsofbadgeadoption;hence,ourresultscanonlybeinterpretedasexploringthereliabilityofthesignalthat
abadgeprovides.Ouranalysisalsodoesnotconsiderthespecific
valueshown onthebadge (e.g.,current coverage).Still,we expect
thatbadgesareusuallyadoptedtosignalgoodpractices,abadge
highlightingthatapracticeisnotfollowed(e.g.,lowcoverage)might
have a negative effect. We control for this indirectly in many mod-
els,e.g., by controlling for popularity in our analysis of downloads
(§4.3),butamoredetailedanalysisisoutsidethescopeofthispaper.
Beyondcorrelations. Noneofourthreeanalysisstepscanestablish
a causal relationship between badges and the studied qualities.Still, note how our three steps investigate each hypothesis from
complementaryperspectives:Step1checksforbasiccorrelations,
Step2explorestheroleofcovariatesandwhetherbadgesprovide
additionalinsights,andStep3looksatwhetheradoptingbadges
leads to observable differences over time. The careful multi-faceted
analysis,spearheadedbythesophisticatedtime-seriesregression
discontinuitydesign,canindicatethatcorrelationsarenotspurious
and associate with some underlying phenomenon.
4.2 Signals of Updated Dependencies (H 4,H 5)
We explore our hypotheses grouped by response variable and start
withadiscussionofdependency freshness,asitclearlyillustrates
our3-stepanalysis.Weexpectthatdependency-managerbadges
correlatewithmore up-to-dateandsecuredependencies( H4),op-
erationalized with our freshness metric (see Sec. 4.1), and expect at
most a marginal effect from information-related badges (H 5).
516
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Repository Badges in the npm Ecosystem ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 2: Dependency freshness models.
Basic Model Full Model RDD
response: freshness = 0 response: freshness = 0 response: log(freshness )
17.3% deviance explained 17.4% deviance explained R2
m=0.04,R2
c=0.35
Coeffs (Err.) LR Chisq Coeffs (Err.) LR Chisq Coeffs (Err.) Sum sq.
(Interc.) 3.54(0.03)∗∗∗3.50(0.03)∗∗∗1.45(0.09)∗∗∗
Dep.−1.78(0.01)∗∗∗32077 .8∗∗∗−1.79(0.01)∗∗∗32292 .8∗∗∗−0.04(0.02) 3.01
RDep. 0.22(0.01)∗∗∗610.3∗∗∗0.21(0.01)∗∗∗560.6∗∗∗−0.01(0.02) 0.11
Stars−0.08(0.00)∗∗∗301.4∗∗∗−0.09(0.00)∗∗∗311.2∗∗∗0.00(0.01) 0.00
Contr.−0.24(0.01)∗∗∗500.5∗∗∗−0.25(0.01)∗∗∗548.7∗∗∗−0.04(0.02)∗4.39∗
lastU−0.65(0.01)∗∗∗12080 .9∗∗∗−0.64(0.01)∗∗∗11537 .9∗∗∗0.01(0.02) 0.37
hasDM 0.24(0.03)∗∗∗116.1∗∗∗0.45(0.08)∗∗∗2.43
hasInf 0.11(0.02)∗∗∗48.3∗∗∗0.04(0.05) 0.45
hasDM:hasInf −0.05(0.04) 1.9−0.32(0.10)∗∗
hasOther 0.01(0.01)
time 0.03(0.00)∗∗∗82.99∗∗∗
intervention −0.93(0.03)∗∗∗1373 .22∗∗∗
time_after_intervention 0.11(0.00)∗∗∗455.56∗∗∗
time_after_intervention:hasDM −0.10(0.01)∗∗∗230.36∗∗∗
time_after_intervention:hasInf −0.00(0.01) 1.14
time_after_intervention:hasDM:hasInf 0.03(0.01)∗∗10.62∗∗
∗∗∗p<0.001,∗∗p<0.01,∗p<0.05;
Dep: dependencies; RDep: dependents; Contr.: contributors; lastU: time since last update;
hasDM: has dependency-manager badge; hasInf: has information badge; hasOther: adopts
additional badges within 15 days
Correlation. Amongtheanalyzedpackagesthathadanydepen-
dencies, 37% had all up-to-date dependencies (freshness = 0). Sup-
portingH4and, surprisingly, contradicting H5, Fig. 2a reveals a
small, but statistically significant difference: packageswithade-
pendency-managerbadgeoraninformationbadgetendtohave
overallfresherdependenciesthanpackageswithout.Wealsofind
that dependency-manager badges are overproportionally adopted
by packages with more dependencies.
Additional information. Totestifthe presenceofbadgescorre-
lateswithdeeper-levelfreshnessindicatorsbeyondotherreadily
available signals, we fit a hurdle regression: a logistic model of
the likelihood of freshness = 0 and a linear regression to model
levelsof freshnessfor packageswithoutdated dependencies.This
hybrid modeling approach is necessary due to the bimodality of
thedata(Fig.2a).Asdescribedin§4.1,the basemodel attemptsto
explain freshness given readily-available signals (stars, dependents,
dependencies, contributors) and a control for time since package
waslastupdated ;thefullmodel additionallymodelsthepresence
ofdependency-managerbadges andinformationbadgesandtheir
interaction, with controls for other badges adopted within 15 days.
We show the base and full logistic regression model (predicting
whether a package has any outdated dependencies) in Table 2. The
base model explains 17.3% of the deviance; the full model explains
17.4%.Thedifferenceissmallbutstatisticallysignificant(DeLong’s
testforcorrelatedROCcurves p<0.001).Thenumberofdependen-
ciesandthetimesincethelastupdateexplainthemajorityofthe
deviance, but dependency-managerbadgesaddexplanatorypower:
theoddsofhavingfresh dependencies increaseby27%( e0.24)for
packages with dependency-manager badges ( H4). Surprisingly, the
effectofinformationbadgesiscomparable: a 17% increase in odds
(H5). For the linear regression (predicting the severity of outdated
dependencies for packages with outdated dependencies), we see a
similarsmallbutsignificantdifferencebetweenbase(22.4%)and
full models (22.8%), and similar behavior of the badge predictors.
Longitudinal analysis. We collect a sample of 3,604 packages
thathaddependenciesandsatisfytheRDDrequirements(9monthsactivity before and after the adoption of their first dependency-
managerbadge),andkeep1,761thathadatleastonemonthwith
freshness /nequal0duringthe+/-9(toavoiddatabimodalityissues).A
trendisalreadyvisiblefromthelongitudinalfreshnessdataplot-
tedfor thosepackagesin Fig.3a,but acorrespondingRDD model
controlling for confounds (column RDD3in Table 2) confirms that:
Theadoptionof(any)badgescorrelatestoastrongimprovementin
freshness(seethe intervention terminthemodel),byaboutafactor
2.5onaverage,4afterwhichfreshnessslightlydecaysagainover
time(theinterpretationderivesfromthesumofthecoefficientsfor
timeandtimeafterintervention inthemodel,cf.RDD[ 65],which
expressestheslopeofthepost-interventiontrend).Ashypothesized,
theadoptionofadependency-managerbadgeisassociatedwitha
longer-lastingeffectonfreshnessthanotherbadges (see the inter-
actiontime after intervention *hasDMin the model; /similarequal80% slower
decay). The interaction effect of information badges is negligible.Discussion.
Resultsfromallthreestepsconfirm H4thatdependency-
manager badges signal practices that lead to fresher dependencies.
However,theeffectisnotexclusivetodependency-managerbadges;
we speculate that any maintenance task involving README up-
dateswithmorebadgesmightinvolveotherprojectcleanup.Still,
the effect of dependency-manager badges is both stronger and
longerlived,assignalingtheorywouldpredictgiventheassessment-
signal nature of dependency-manager badges. The results are simi-
larforasecurityscorethatcountsknownvulnerabilities,similar
to theSnykandnspservices (not shown due to space restrictions).
4.3 Signals of Popularity (H 2,H 5,H 6,H 8)
Weexpectthatadoptingquality-assuranceandpopularitybadges
correlates with increases in downloads ( H2,H6), and at most a
marginaleffectfrominformation-relatedbadges( H5).Wefollowthe
same three steps, analyzing monthly download counts as response.
Correlation. Supporting our hypotheses, comparing downloads
for packages with and without badges shown in Figure 2b, we
can see that for all categories of badges, those packageswitha
badgetendtoskewtowardmoredownloadsthanpackageswithout.
The differences are generally small, but statistically significant. As
hypothesized, the effect for information badges is smaller than for
quality assurance and popularity badges.
Additional information. We fit nested negative binomial regres-
sionmodelstoexplaintheinformationaddedbybadgesforexplain-
ing downloads over readily-available signals and controls. Since
already popularpackages mightbenefit frombadges ina different
way than less popular ones, we also model a dummy variable is-
Popular, that indicates whether the package was among the 10%
most downloaded packages in the prior month (June 2017). In the
modelwithbadges,wethenexploreinteractionswiththe isPopular
dummy. Both models (see appendix) explain downloads well: The
basic model explains 66% of the deviance (with expected behavior
of all main predictors), and the full model explains 86%. The differ-
enceisstatisticallysignificant;allbadgecategorieshavesignificanteffects.Modelinginteractionsrevealsthatquality-assurancebadges
3NotethatallpackagesmodeledintheRDDadoptedsomebadgeduringthealignment
month, hence the control hasOther is subsumed by experimental design.
4e0.93factor decrease in freshness score; note the log-transformed response, hence
the exponentiation here.
517
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Asher Trockman, Shurui Zhou, Christian Kästner, Bogdan VasilescuFreshness
Dep. Mgmt. Info100101102103
(−0.10) (−0.12)
(a)Dependencyfreshness
Downloads
QA Popularity Info100102104106
Badge: FALSE TRUE
(0.18) (0.25) (0.12)
(b) Popularity
Test Folder (Bytes)
QA Info100102104106
(0.55) (0.30)
(c) Test suite size
% PRs w/ tests
Coverage0. 2 . 4 . 6 . 81
(0.03)
(d)PRswithtests
Time (hours)
Support100101102103
(0.14)
(e) Issue latency
Figure2:Distributionsofresponsevariablesw/oandw/badges.Horizontallinesdepictmedians.Cliff’sdeltabeloweachplot.
−8−6−4−20 2 4 6 8 −8−6−4−20 2 4 6 8101102
Month index relative to badgeFreshness
(a)Monthlyfreshnessscores,rel.todependency-
manager (left) and information badges (right).101102103104
−8−6−4−2 02468
Month index relative to badgeDownloads
(b) Monthly downloads, rela-
tive to first badge.0.00.10.20.30.40.5
−8−6−4−2 02468
Month index relative to badgeTest suite size / Project size
(c) Ratio test suite size / pack-
age size, rel. to QA badge.0.00.10.20.30.40.5
−8−6−4−2 02468
Month index relative to badgeFraction PRs with tests
(d) Fraction PRs with tests,
relative to QA badge.
Figure 3: Trends in response variables before and after badge adoption.
haveastrongereffectinalreadypopularpackages:holdingother
variablesconstantattheirmeanvalues, popularpackageswitha
quality-assurancebadgetendtohaveabout2.2timesmoredown-
loadsthancomparablepackageswithout; the effect is marginal
forlesspopularpackages.Forpopularitybadgesandinformation
badges,weseesmallereffectsonpopularpackages( 1.2×more,and
0.78×fewer downloads respectively). Again, the models show that
badges explain additional aspects of popularity.
100000150000200000250000
02468 1 0 1 2
Number of distinct badgesDownloads
isPopular TrueBadge Overload Effects Separately, we also fit a negative
binomial regression to model the
effectofthenumberofbadgesdis-
played (and controls). The model
(see appendix) suggests a nonlinear
relationship in popular packages,
withapredictedinflectionpointat
5 badges, which supports H8:Pack-
ageswithmanybadgestendtohave
fewerdownloads.Theeffectforless
popular packages is negligible.
Longitudinal analysis. We compile a set of 1,762 packages that
satisfy the RDD requirements and have monthly download counts
after March 2015 (a limitation of the npmAPI). Specifically, we
align on the adoption month of their first badge for categories
pertaining to our hypotheses. Of these, 1,414 packages adopteda quality-assurance badge, 892 an information badge, and 366 apopularity badge. Prior to modeling, we inflation-adjusted each
package’smonthlydownloadcountstoaccountfor npm’snatural
growthovertime,basedontheaveragedownloadgrowthof10,000
randomly-sampled packages with at least 10 dependents each, that
existed the entire period.A visual inspection of trends around the first badge adoption
(Fig.3b)showspotentialinterventioneffects;anRDDmodelcon-
trolling for confounds (see appendix) suggests a small positive
trend prior to the intervention, a sizeable positive discontinuity in
downloadcountsatbadgeadoption(33%increaseonaverage),and,
surprisingly,a smallnegative slopeafterthe intervention.Thatis,
badgeadoptioncorrelateswithasuddenpopularityboost,butthe
accelerationisnotsustainedovertime.Thepost-interventionde-
cay for quality-assurance badges is slower than average (8.8%)—in
other words, the positive intervention effect lasts longer.
Discussion. Together,ourthreeanalysisstepspaintamixedpic-
tureforourhypotheses:Allbadgescorrelatewithmoredownloads
ingeneral,andalongitudinalanalysisshowsalsopositiveinterven-
tioneffectsforquality-assuranceandpopularitybadges( H2,H5,
H6), but the pace is not sustained over time. Still, not all packages
with badges show similar effects: Those with assessment-signal
badges (namely quality-assurance) tend to maintain the popularity
boostthat correlates withbadgeadoptionlonger, assignalingthe-
orywouldpredict.Atthesametime,toomanybadgesmayseem
counterproductive (H 8).
4.4 Signals of Test Suite Quality (H 1,H 5)
We expect that the adoption of quality-assurance badges correlates
with increases in test-suite quality ( H1), operationalized as the
size of the test suite, and again at most a marginal effect from
information badges (H 5). We follow our common 3-step analysis.
Correlation. The distribution of test suite sizes across packages
isbimodal(Figure2c),aswecannotdetectanytestcodein39.3%
of the packages. Packages with quality-assurance badges almostalways had tests (93.5%). Among the packages with tests, those
packageswithquality-assurancebadgestendtoskewtowardlarger
518
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Repository Badges in the npm Ecosystem ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
testsuites, with statistically significant differences, supporting our
hypothesis. Surprisingly, sodopackageswithinformationbadges.
Additional information. Given the bimodal distribution of test
suitesizes,asfordependencyfreshness(Sec.4.2),wefitahurdle
regression (see appendix), modeling separately the likelihood ofhaving any tests (logistic regression) and the test-folder size for
packageswithnon-emptytestsuites(negativebinomialregression).
In addition to the usual controls, we control for the package size
(larger packages are expected to have larger test suites).
The base logistic model is plausible for explaining whether a
package has any tests (4.4% deviance explained). The full model
with quality-assurance and information badges fits the data signif-
icantly better (22.6% deviance explained), most being attributed
to quality-assurance badges. The full negative binomial regression
model shows a small improvement for explaining test-suite size
(78.2% to 78.4%). In both cases (having any tests and size of test
suite),themodelsshowastrongpositiveeffectofquality-assurance
badges (H1): on average, theoddsofhavingtestsincreasebya
factor18forpackageswithquality-assurancebadges;amongpack-
ageswithtests,thosewithquality-assurancebadgesareexpected
tohave18.3%largertestsuites,othervariablesheldconstant.Infor-
mationbadgeshaveamarginaleffectontheirownbutinteractwith
quality-assurance badges, strengthening their effect slightly (H 5).
Longitudinal analysis. Toavoidtechnicalproblemswiththebi-
modal distribution, we study only how first badge adoption cor-relates with the growth of an existing test suite, not whether itcoincided with creating a test suite in the first place. To this end,
weassemblealongitudinalsampleof2,855packagesthatadopteda
quality-assurance badge and had at least 9 months of history with
non-emptytestsuites,bothbeforeandaftertheadoptionmonth.
An RDD model controlling for confounds (see appendix) reveals
whatisbarelyvisibleintheplotteddata(Fig.3c):thereisasmall
but statistically significant positive shift in test-suite size at the in-
tervention, but almost no change in slope after the badge adoption.
Stateddifferently, introducingquality-assurancebadgescoincides
withanimprovementinthetestsuite,butdoesnottriggeralasting
changetotestingpractices. Information badges are correlated with
a much smaller effect.
Discussion. As expected by the surveyed maintainers, all three
steps indicate that quality-assurance badges are a good signal for a
project having some tests, though they are a weaker signal for the
sizeofthetestsuite( H1).Sincetheadoptionofquality-assurance
badgesiscorrelatedwithaninterventioneffectontestsuitesize,we
concludethattheymayactasgamificationmechanisms.Again,we
see a marginal change at the intervention for information badges;
theformercanlikelybeexplainedthroughsomeoverallperception
of well-managed projects (H 5).
4.5 Signals of Better Contributions (H 3)
We expect that adopting quality assurance, and especially cover-
age badges, correlates with better pull requests ( H3), which we
operationalize as the likelihood of pull requests containing tests.
Correlation. A direct comparison (Fig. 2d) shows that, support-
ingourhypothesis, packageswithquality-assurancebadgestend
tohaveslightlyhigherfractionsofpullrequestswithtests. Notethat coverage badges are adopted almost exclusively together with
continuous integration badges (97%), but not vice versa (23%).
Additionalinformation. Wecompileasetof3,344packageshav-
ing more than 50 pull requests. Of these, 2,693 have at least one
badge(944coveragebadge,2289otherquality-assurancebadge)and
651havenobadges.Wethenfitanestedlogisticregression(with
our standard controls; 12.9% deviance explained in base model,
14.2%infull;seeappendix)tomodelthefractionofpullrequests
per package containing tests. We find that non-coveragequality-
assurancebadgesgenerallyhaveapositiveeffect, increasing the
chance of tests in a pull request by 24.1% (all other factors held
constant); coverageandotherqualityassurancebadgesinteract,
amplifyingeachother’seffectsbyanadditional16.8%ifacoverage
badge and a continuous-integration badge are adopted together.
Longitudinalanalysis. Weanalyzeasampleof324packageswith
9monthsofhistoryoneachsideofthecoveragebadgeadoption
monthusingourstandardRDDapproach.Visually(Fig.3d),thepre
andpost-badgeperiodsareclearlydistinguishable,withthepost-
badge period showing more pull requests with tests. The model
(see appendix) confirms an increaseinthemonthlyfractionofpull
requestscontainingtestsafteradoptingquality-assurancebadges
(onaverage23%),andaslowdecayafterwardthatisfurtherslowed
(5.7%) if coverage badges are adopted additionally.
Discussion. All three steps support H3: Packages with quality-
assurancebadgesreceivebetterpullrequestcontributions;coverage
badgestendtoamplifythiseffect,evenincreasingthedurationof
theinterventioneffect,whichsupportsourhypothesisthattheyare
gamification mechanisms, inspiring contributors to include tests in
order to not decrease the displayed coverage percentage.
4.6 Signals of Support (H 7)
Weexpectthattheadoptionofsupportbadgescorrelateswithbetter
support ( H7), operationalized as the average time to close issues
onGitHub.
Unfortunately,therearerelativelyfewpackagesthathaveadopted
supportbadges(seeTab.1).Welimitedouranalysisto826packages
withatleast100issuesoverall,andhavingatleastoneissuecreated
after July 1, 2017 and closed before Aug. 20; 397 of these packages
have support badges. In contrast to our hypothesis, we observethat projects with a support badge have, on average, 20% longerclosing times than those without; however, we also observe thatprojects with support badges receive twice the number of issues
onaverage.Whencontrollingforconfounds,includingthenumber
of issues (base model 21.2%, full model 21.3%), we find a similar
effect:Projectswithsupportbadgeshavea30.2%higherlatencyonaverage.RemovingpackagesthatdonotfulfilltheRDDconstraints
leavesonly76packages.Controllingforthesameconfounds,the
RDDmodelonlysuggeststhatissuelatencydecreaseswithtime,
but does not show an effect of support badges.
Overall,ourfindingsforhypothesis H7arenegativeandoppo-
sitetoourexpectations.TheresultsinSteps1and2arestatistically
significant, but all steps suffer from small data sets. We have no ex-
planation for this effect beyond conjecturing that external support
platforms (Gitter andSlack) handle support requests in a way
that is not captured by our operationalization with GitHubissues.
519
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Asher Trockman, Shurui Zhou, Christian Kästner, Bogdan Vasilescu
5 DISCUSSION AND CONCLUSION
We studied repository badges, a new phenomenon in social coding
environments like GitHub, with previously unknown effects.
Research questions. Weansweredtworesearch questions.First,
exploring the most common types of badges and their intended
signals (RQ 1), we found a diversity of badges and signals (Table 1):
some are merely static displays of (existing) information, others
aggregate information that is otherwise much harder to observe,e.g., reflecting build status, up-to-dateness of dependencies, andtest coverage. As predicted by signaling theory (§2), our survey
revealedthatpackagemaintainersdisplayingbadgeshaveclearsig-nalingintentionsdoingso,andmanycontributorsinterpretbadges
when evaluating packages. Second, exploring the fit of the signals
to underlying qualities hypothesized by our survey participants
(RQ 2), we found that packages with badges tend to skew towards
having more of the quality they signal, with stronger effects for
the non-trivial quality-assurance and dependency-manager badges.
Moreover, the presence of badges consistently adds explanatory
power,albeitlittle,toreadily-availablesignals.Time-seriesanalysis
further revealed thatthe introduction of quality-assurance badges
tendstocorrelatewithpositive intervention effects: Theunderly-
ing qualities they signal tend to improve immediately, especially
improved dependency freshness and more tests in pull requests.
Gamification effects. Our results also revealed gamification ef-
fects (§2): Clear examples are the quality assurance badges display-
ingtestcoveragepercentages(§4.5),whichwefoundtocorrelate
withdevelopersincreasingthesizeand,arguably,qualityoftheir
testsuites.Dependency-managementbadges(§4.2)couldalsobe
seenasagamificationmechanism:bymakingtheup-to-dateness
of dependencies noticeable, they create an incentive for developers
to make the most out of their dependency-management tools by
stayinguptodate.Onecanimagineotherbadgeswithgamification
value,e.g.,aroundbugfixing,beingusedinthefuturetoencourage
desirablepractices.However,weadv ocatecautioninimplement-
inggamificationandacknowledgetheriskofcreatingthewrong
incentives, e.g., writing tests only to maximize coverage.
The power of assessment signals. As discussed in §2, signal-
ing theory makes an interesting distinction between conventional
signalsandassessmentsignals,wherethelatterrequirethatthesig-
naleractuallypossessesthesignaledquality.Althoughafewsurveyparticipantssuggestedintendedsignalsforinformationbadgesand
supportbadges(§3.2),the theory predictsthat,withoutanassoci-
atedcostoranalysis,theyarelessreliablesignals.Ourresultsseem
toconfirmthatqualityassurance,dependencymanager,andpop-
ularity badges (mostly assessment signals) provide more reliable
signals than information badges (mostly conventional signals). For
example,theeffectonfreshnessisstrongerandlongerlastingfor
theassessmentsignal.Interestingly,informationbadgesoftendo
signalsomething,evenoutsidetheirdomainofcost(infact,indata
exploration we often also found other small effects of other badges
onvariousqualities).Wespeculatethatbadgesareoftenadopted
during a general maintenance phase in which also test suites ordependencies are improved, but we expect that only assessment
signals correlate with lasting change (§4.2).Theresultsencouragemorebadgestobedesignedasassessment
signals.Forexample,severalbadgesthatcurrentlyonlystateinten-tions,suchas
or
 ,couldberedesigned
toreportanalysisresultfortheunderlyingquality,suchaspastcon-
formancetocodingstandardsorresponsivenesstosupportrequests.
Such badges may encourage stronger conformance and even ac-
cruegamificationbenefits.Interestingly, Slackoffersmultiplekinds
ofbadges,includingoneinvitinguserstojoin(
 )andone
that shows the number of currently active and registered users
(
 ). In practice though, most package maintainers with
aSlackbadgeadopttheformer(conventionalsignal)ratherthan
thelatter(assessmentsignal).Ourresultsindicatethatmaintainers,
when they have the choice and are serious about signaling their
dedication, should adopt the assessment-signal badge.
Badges vs practices. Weemphasizethatinmostcaseseffectsas-
sociatedwithnon-trivialbadgesareinextricablylinkedtoeffects
associatedwiththebadges’correspondingtoolsorpractices.For
many kinds of badges, the badge is the only (easily) externally
observable indicator of the tool’s use (e.g.,
 ,
). While effects associated with adopting continuous
integration [ 31,60,65], static analysis [ 7,64], and dependency-
management tools [ 41] on software development practices have
beenstudiedbypriorwork,ourapproachisuniqueinthatwefocus
on thesignaling dimension added by repository badges to these
and other tools, previously overlooked in the software engineering
signaling literature (e.g.,[ 20,38,48,55,56]). Further delineating
effects associated with badges from effects associated with thetools themselves goes beyond the scope of this work, but some
preliminaryexperimentswiththecontinuousintegrationservice
Travis,whichisdetectableindependentlyofitsbadgethrougha
travis.yml configurationfileintherepository,suggestthatwhile
theobservedeffectsofadoptingthetoolaresimilartotheeffectsatthebadgeadoptiontime,badgesseemtohaveasmallamplifyingef-
fect.Pendingfurthervalidation,thisisencouragingforresearchers,
in that badges might be a reliable indicator for longitudinal studies
of practices that are hard to detect otherwise, because there are no
clear traces of the practice in the repository, beyond the badge.
Implicationsforpractitioners. Ourresultsprovideguidanceon
which qualities are usually signaled with badges and which signals
tend to be more reliable. Therefore, package maintainers can make
moredeliberatechoicesaboutbadges(e.g.,limitingconventional
signals),servicedevelopers candesignbadgesmorecarefully( e.g.,
providing an assessment signal based on some analysis of past
conformance),and packageusersandcontributors candecidewhich
badges to use as indicators of underlying practices and as starting
points to investigate deeper qualities. Overall:
 .
Acknowledgements. Many thanks to respondents to our survey!
Trockman was supported through Carnegie Mellon’s Research Ex-
periences for Undergraduates in Software Engineering. Kästner
andZhouhavebeensupportedinpartbytheNSF(awards1318808,
1552944, and1717022) and AFRLand DARPA(FA8750-16-2-0042).
Vasilescu has been supported in part by the NSF (award 1717415).
A ADDITIONAL MATERIAL
Plots, models, and exact survey questions are available online at
https://github.com/CMUSTRUDEL/npm-badges.
520
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Repository Badges in the npm Ecosystem ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]Rabe Abdalkareem, Olivier Nourry, Sultan Wehaibi, Suhaib Mujahid, and Emad
Shihab. 2017. Why Do Developers Use Trivial Packages? An Empirical Case
Study on npm.I nProc. Europ. Software Engineering Conf./Foundations of Software
Engineering (ESEC/FSE). ACM.
[2] Paul D Allison. 1999. Multiple regression: A primer. Pine Forge Press.
[3]BilalAmirandPaulRalph.2014. Proposingatheoryofgamificationeffectiveness.
InCompanion Proc. Int’l Conf. Software Engineering (ICSE). ACM, 626–627.
[4]Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg, and Jure Leskovec. 2013.
Steering user behavior with badges. In Proc. Int’l Conf. World Wide Web (WWW).
ACM, 95–106.
[5]GabrieleBavota,GerardoCanfora,MassimilianoDiPenta,RoccoOliveto,and
SebastianoPanichella.2015. HowtheApachecommunityupgradesdependencies:
An evolutionary study. Empirical Software Engineering 20, 5 (2015), 1275–1317.
[6]AndrewBegel,JanBosch,andMargaret-AnneStorey.2013. Socialnetworking
meetssoftwaredevelopment:PerspectivesfromGitHub,MSDN,StackExchange,
and TopCoder. IEEE Software 30, 1 (2013), 52–66.
[7]Moritz Beller, Radjino Bholanath, Shane McIntosh, and Andy Zaidman. 2016.
Analyzingthestateofstaticanalysis:Alarge-scaleevaluationinopensourcesoft-
ware. InProc. Int’l Conf. Software Analysis, Evolution and Reengineering (SANER),
Vol. 1. IEEE, 470–481.
[8]Christian Bird, Alex Gourley, Prem Devanbu, Michael Gertz, and Anand Swami-
nathan. 2006. Mining email social networks. In Proc. Working Conf. Mining
Software Repositories (MSR). ACM, 137–143.
[9]ChristianBird,AlexGourley,PremDevanbu,AnandSwaminathan,andGreta
Hsu. 2007. Open borders? Immigration in open source projects. In Proc. Working
Conf. Mining Software Repositories (MSR). IEEE, 6.
[10]Kelly Blincoe, Jyoti Sheoran, Sean Goggins, Eva Petakovic, and Daniela Damian.
2016. Understanding the popular users: Following, affiliation influence and
leadership on GitHub. Information and Software Technology 70 (2016), 30–39.
[11]ChristopherBogart,ChristianKästner,JamesHerbsleb,andFerdianThung.2016.
How to break an API: Cost negotiation and community values in three software
ecosystems. In Proc. Int’l Symp. Foundations of Software Engineering (FSE). ACM,
109–120.
[12]John Businge, Alexander Serebrenik, and Mark GJ van den Brand. 2015. Eclipse
API usage: the good and the bad. Software Quality Journal 23, 1 (2015), 107–141.
[13]AndreaCapiluppi,AlexanderSerebrenik,andLeifSinger.2013. Assessingtech-
nical candidates on the social web. IEEE Software 30, 1 (2013), 45–51.
[14]Casey Casalnuovo, Bogdan Vasilescu, Premkumar Devanbu, and Vladimir Filkov.
2015. DeveloperonboardinginGitHub:theroleofpriorsociallinksandlanguage
experience. In Proc. Europ. Software Engineering Conf./Foundations of Software
Engineering (ESEC/FSE). ACM, 817–828.
[15]Huseyin Cavusoglu, Zhuolun Li, and Ke-Wei Huang. 2015. Can gamification mo-
tivate voluntary contributions? The case of Stack Overflow Q&A community. In
CompanionProc.Conf.ComputerSupportedCooperativeWork&SocialComputing
(CSCW). ACM, 171–174.
[16]JailtonCoelhoandMarcoTulioValente.2017. WhyModernOpenSourceProjects
Fail. InProc. Europ. Software Engineering Conf./Foundations of Software Engineer-
ing (ESEC/FSE). ACM.
[17]BrianLConnelly,STrevisCerto,RDuaneIreland,andChristopherRReutzel.
2011. Signaling theory: A review and assessment. Journal of Management 37, 1
(2011), 39–67.
[18]Thomas D Cook, Donald Thomas Campbell, and Arles Day. 1979. Quasi-
experimentation: Design & analysis issues for field settings. Vol. 351. Houghton
Mifflin Boston.
[19]JoëlCox,EricBouwers,MarkovanEekelen,andJoostVisser.2015. MeasuringDe-
pendencyFreshnessinSoftwareSystems.In Proc.Int’lConf.SoftwareEngineering,
Volume 2. IEEE Press, 109–118.
[20]Laura Dabbish, Colleen Stuart, Jason Tsay, and Jim Herbsleb. 2012. Social coding
in GitHub: transparency and collaboration in an open software repository. In
Proc. Conf. Computer Supported Cooperative Work (CSCW). ACM, 1277–1286.
[21]LauraDabbish,ColleenStuart,JasonTsay,andJamesHerbsleb.2013. Leveraging
transparency. IEEE Software 30, 1 (2013), 37–43.
[22]AlexandreDecan,TomMens,andMaëlickClaes.2017. Anempiricalcomparison
of dependency issues in OSS packaging ecosystems. In Proc. Int’l Conf. Software
Analysis, Evolution and Reengineering (SANER). IEEE, 2–12.
[23]Sebastian Deterding, Miguel Sicart, Lennart Nacke, Kenton O’Hara, and Dan
Dixon. 2011. Gamification. using game-design elements in non-gaming contexts.
InProc. Conf. Human Factors in Computing Systems (CHI), Extended Abstracts.
ACM, 2425–2428.
[24] Judith Donath. 2007. Signals in social supernets. Journal of Computer-Mediated
Communication 13, 1 (2007), 231–251.
[25]Mohammad Gharehyazie, Baishakhi Ray, and Vladimir Filkov. 2017. Some from
here,somefromthere:cross-projectcodereuseinGitHub.In Proc.WorkingConf.
Mining Software Repositories (MSR). IEEE, 291–301.
[26]Georgios Gousios, Bogdan Vasilescu, Alexander Serebrenik, and Andy Zaidman.
2014. LeanGHTorrent:GitHubdataondemand.In Proc.WorkingConf.MiningSoftware Repositories (MSR). ACM, 384–387.
[27]ScottGrantandBuddyBetts.2013. Encouraginguserbehaviourwithachieve-ments:anempiricalstudy.In Proc.WorkingConf.MiningSoftwareRepositories
(MSR). IEEE, 65–68.
[28]TimGuilford andMarianStampDawkins. 1991. Receiverpsychologyandthe
evolution of animal signals. Animal Behaviour 42, 1 (1991), 1–14.
[29]Anja Guzzi, Alberto Bacchelli, Michele Lanza, Martin Pinzger, and Arie
VanDeursen.2013.Communicationinopensourcesoftwaredevelopmentmailing
lists.InProc.WorkingConf.Mining SoftwareRepositories(MSR).IEEE,277–286.
[30]Joseph Hejderup. 2015. In dependencies we trust: How vulnerable are dependencies
in software modules? Master’s thesis. TU Delft, The Netherlands.
[31]MichaelHilton,TimothyTunnell,KaiHuang,DarkoMarinov,andDannyDig.
2016. Usage, Costs, and Benefits of Continuous Integration in Open-source
Projects.In Proc.Int’lConf.AutomatedSoftwareEngineering(ASE).ACM,426–
437.
[32]Mitchell Joblin, Sven Apel, Claus Hunsen, and Wolfgang Mauerer. 2017. Clas-sifying developers into core and peripheral: An empirical study on count andnetwork metrics. In Proc. Int’l Conf. Software Engineering (ICSE). IEEE Press,
164–174.
[33]EiriniKalliamvakou,DanielaDamian,KellyBlincoe,LeifSinger,andDanielM
German.2015. Opensource-stylecollaborativedevelopmentpracticesincom-
mercial projects using GitHub. In Proc. Int’l Conf. Software Engineering (ICSE).
IEEE, 574–585.
[34]RaulaGaikovinaKula,DanielMGerman,AliOuni,TakashiIshio,andKatsuro
Inoue.2017. Dodevelopersupdatetheirlibrarydependencies? EmpiricalSoftware
Engineering (2017), 1–34.
[35]Michael J Lee, Bruce Ferwerda, Junghong Choi, Jungpil Hahn, Jae Yun Moon,
andJinwooKim.2013. GitHubdevelopersuserockstarstoovercomeoverflow
of news. In Proc. Conf. Human Factors in Computing Systems (CHI), Extended
Abstracts. ACM, 133–138.
[36]MLynneMarkusandBrookManvilleCaroleEAgres.2000. Whatmakesavirtual
organization work? MIT Sloan Management Review 42, 1 (2000), 13.
[37]JenniferMarlowandLauraDabbish.2013. Activitytracesandsignalsinsoftware
developerrecruitmentandhiring.In Proc.ACMConferenceonComputerSupported
Cooperative Work (CSCW). ACM, 145–156.
[38] Jennifer Marlow,Laura Dabbish,and JimHerbsleb.2013. Impression formation
inonlinepeerproduction:activitytracesandpersonalprofilesinGitHub.In Proc.
Conf. Computer Supported Cooperative Work (CSCW). ACM, 117–128.
[39]Cade Metz. 2015. How GitHub Conquered Google, Mi-
crosoft, and Everyone Else. https://www.wired.com/2015/03/
github-conquered-google-microsoft-everyone-else/. (2015).
[40]VishalMidhaandPrashantPalvia.2012. FactorsaffectingthesuccessofOpen
Source Software. Journal of Systems and Software 85, 4 (2012), 895–905.
[41]SamimMirhoseiniandChrisParnin.2017. CanAutomatedPullRequestsEncour-
ageSoftwareDeveloperstoUpgradeOut-of-DateDependencies?.In Proc.Int’l
Conf. Automated Software Engineering (ASE). to appear.
[42]AudrisMockus,RoyTFielding,andJamesDHerbsleb.2002. Twocasestudiesof
opensourcesoftwaredevelopment:ApacheandMozilla. ACMTransactionson
Software Engineering and Methodology (TOSEM) 11, 3 (2002), 309–346.
[43]Jagdish K Patel, CH Kapadia, and Donald Bruce Owen. 1976. Handbook of
statistical distributions. M. Dekker.
[44]Steven Raemaekers, Arie van Deursen, and Joost Visser. 2014. Semantic Version-
ing versus Breaking Changes: A Study of the Maven Repository. In Proc. Int’l
WorkingConf.SourceCodeAnalysisandManipulation(SCAM).IEEEComputer
Society, 215–224.
[45]EricSRaymond.2001. TheCathedral&theBazaar:Musingsonlinuxandopen
source by an accidental revolutionary. O’Reilly Media, Inc.
[46]RightScale. 2016. State of the Cloud Report: DevOps
Trends. http://www.rightscale.com/blog/cloud-industry-insights/
new-devops-trends-2016-state-cloud-survey. (2016).
[47]Peter J Rousseeuw and Christophe Croux. 1993. Alternatives to the median
absolute deviation. J. Amer. Statist. Assoc. 88, 424 (1993), 1273–1283.
[48]N Sadat Shami, Kate Ehrlich, Geri Gay, and Jeffrey T Hancock. 2009. Making
senseofstrangers’expertisefromsignalsindigitalartifacts.In Proc.Conf.Human
Factors in Computing Systems (CHI). ACM, 69–78.
[49]Jyoti Sheoran, Kelly Blincoe,EiriniKalliamvakou,DanielaDamian, andJordan
Ell. 2014. Understanding watchers on GitHub. In Proc. Working Conf. Mining
Software Repositories (MSR). ACM, 336–339.
[50]Leif Singer, Fernando Figueira Filho, and Margaret-Anne Storey. 2014. Software
engineering at the s peed of light: How d evelopers stay current using Twitter. In
Proc. Int’l Conf. Software Engineering (ICSE). ACM, 211–221.
[51]Michael Spence. 1973. Job market signaling. The Quarterly Journal of Economics
87, 3 (1973), 355–374.
[52]Michael Spence. 2002. Signaling in retrospect and the informational structure of
markets. The American Economic Review 92, 3 (2002), 434–459.
[53]MeganSquire.2015. “ShouldWeMovetoStackOverflow?”MeasuringtheUtility
ofSocialMediaforDeveloperSupport.In Proc.Int’lConf.SoftwareEngineering
(ICSE), Vol. 2. IEEE, 219–228.
521
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Asher Trockman, Shurui Zhou, Christian Kästner, Bogdan Vasilescu
[54]Margaret-AnneStorey,LeifSinger,BrendanCleary,FernandoFigueiraFilho,and
Alexey Zagalsky. 2014. The (r)evolution of social media in software engineering.
InProc. Workshop on the Future of Software Engineering. ACM, 100–116.
[55]JasonTsay,LauraDabbish,andJamesHerbsleb.2014. Influenceofsocialandtech-
nical factors forevaluating contribution inGitHub. In Proc. Int’lConf. Software
Engineering (ICSE). ACM, 356–366.
[56]JasonTsay,LauraDabbish,andJamesDHerbsleb.2013. Socialmediaintrans-
parentworkenvironments.In Proc.InternationalWorkshoponCooperativeand
Human Aspects of Software Engineering (CHASE). IEEE, 65–72.
[57]Bogdan Vasilescu, Vladimir Filkov, and Alexander Serebrenik. 2015. Perceptions
of diversity on GitHub: A user survey. In Proc. Int’l Workshop Cooperative and
Human Aspects of Software Engineering (CHASE). IEEE, 50–56.
[58]Bogdan Vasilescu, Alexander Serebrenik, Prem Devanbu, and Vladimir Filkov.
2014. How social Q&A sites are changing knowledge sharing in open sourcesoftware communities. In Proc. Conf. Computer Supported Cooperative Work &
Social Computing (CSCW). ACM, 342–354.
[59] Bogdan Vasilescu, Alexander Serebrenik, and Vladimir Filkov. 2015. A Data Set
for Social Diversity Studies of GitHub Teams. In Proc. Working Conf. Mining
Software Repositories (MSR). IEEE, 514–517.[60]BogdanVasilescu,YueYu,HuaiminWang,PremkumarDevanbu,andVladimir
Filkov.2015. QualityandProductivityOutcomesRelatingtoContinuousIntegra-
tioninGitHub.In Proc.Europ.SoftwareEngineeringConf./FoundationsofSoftware
Engineering (ESEC/FSE) (ESEC/FSE). IEEE, 805–816.
[61]Erik Wittern, Philippe Suter, and Shriram Rajagopalan. 2016. A look at the
dynamics of the JavaScript package ecosystem. In Proc. Working Conf. Mining
Software Repositories (MSR). IEEE, 351–361.
[62]Kazuhiro Yamashita, Yasutaka Kamei, Shane McIntosh, Ahmed E Hassan, and
NaoyasuUbayashi.2016. Magnetorsticky?Measuringprojectcharacteristics
fromtheperspectiveofdeveloperattractionandretention. JournalofInformation
Processing 24, 2 (2016), 339–348.
[63]Amotz Zahavi. 1975. Mate selection—a selection for a handicap. Journal of
Theoretical Biology 53, 1 (1975), 205–214.
[64]Fiorella Zampetti, Simone Scalabrino, Rocco Oliveto, Gerardo Canfora, and Mas-
similiano Di Penta. 2017. How open source projects use static code analysis
tools in continuous integration pipelines. In Proc. Working Conf. Mining Software
Repositories (MSR). IEEE, 334–344.
[65]Yangyang Zhao, Yuming Zhou, Alexander Serebrenik, Vladimir Filkov, and Bog-
danVasilescu.2016. TheImpactofContinuousIntegrationonOtherSoftware
Development Practices: A Large-Scale Empirical Study. In Proc. Int’l Conf. Auto-
mated Software Engineering (ASE). IEEE.
522
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. 