PerfLearner: Learning from Bug Reports to Understand and
Generate Performance Test Frames
Xue Han
Department of Computer Science
University of Kentucky
Lexington, KY, USA
xha225@g.uky.eduTingting Yu
Department of Computer Science
University of Kentucky
Lexington, KY, USA
tyu@cs.uky.eduDavid Lo
School of Information Systems
Singapore Management University
Singapore
davidlo@smu.edu.sg
ABSTRACT
Software performance is important for ensuring the quality of soft-
wareproducts.Performancebugs,definedasprogrammingerrors
that cause significant performance degradation, can lead to slow
systems and poor user experience. While there has been some
researchonautomatedperformancetestingsuchastestcasegen-
eration,themainideaistoselectworkloadvaluestoincreasethe
programexecutiontimes.Thesetechniquesoftenassumetheinitialtestcaseshavetherightcombinationofinputparametersandfocus
on evolving values of certain input parameters. However, such an
assumption may not hold for highly configurable real-word appli-
cations,inwhichthecombinationsofinputparameterscanbeverylarge.Inthispaper,wemanuallyanalyze300bugreportsfromthree
large open source projects - Apache HTTP Server, MySQL, and
MozillaFirefox.Wefoundthat 1)exposingperformance bugsoften
requires combinations of multiple input parameters, and 2) certain
input parameters are frequently involved in exposing performance
bugs.Guidedbythesefindings,wedesignedandevaluatedanauto-
matedapproach,PerfLearner,toextractexecutioncommandsand
inputparametersfromdescriptionsofperformancebugreportsand
use them to generate test frames for guiding actual performance
test case generation.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware testing and de-
bugging;
KEYWORDS
Software Testing, Performance Bugs, Software Mining
ACM Reference Format:
Xue Han, Tingting Yu, and David Lo. 2018. PerfLearner: Learning from
Bug Reports to Understand and Generate Performance Test Frames. In
Proceedingsofthe201833rdACM/IEEEInternationalConferenceonAutomated
Software Engineering (ASE ‚Äô18), September 3‚Äì7, 2018, Montpellier, France.
ACM,NewYork,NY,USA, 12pages.https://doi.org/10.1145/3238147.3238204
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
¬© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.32382041 INTRODUCTION
Softwareperformanceiscriticaltothequalityofadeployedsystem.
A performance bug can cause significant performance degrada-
tion [3], leading to problems such as poor user experience, long
response time, and low system throughput [ 6,21,28,33,48]. Com-
pared to functional bugs that typically cause system crashes or
incorrect results, performance bugs are substantially more difficult
tohandle[ 3,10]becausetheyoftenmanifestthemselvesbyspecial
inputs and in specific execution environments [ 33,36]. Over the
past decade, numerous research efforts have been made to analyze,
detect,andfixperformancebugs[ 7,22,28,29,34,36].Forexample,
manyprofilingtechniques[ 28]havebeenproposedtodynamically
determinewhatprogramentities(e.g.,methods)areresponsiblefor
theexcessiveexecutiontimeandresourceconsumptiongivenan
input.
Profiling methods depend on the chosen set of input values,
which is a known weakness [ 46] for successfully detecting per-
formancebugsinthesubjectundertest.Toaddressthisproblem,
several test case generation techniques have been proposed to
generate large workload test inputs for increasing the chance of
exposing performance bugs [ 7,41]. However, there are several lim-
itationsinexistingperformancetestgenerationtechniques‚Äìmanytechniquesfocusonevolvingthevaluesofcertaininputparameters
whilekeepingtheotherparametersasdefault.Forexample,Burnim
et al. [7] focus on increasing the workload values of data inputs
while keeping the values of configuration options as default. These
techniquesmaybeineffectiveatdetectingperformancebugsdue
to combinatorial effects of different input parameters. For example,
in Apachebug#52914, theperformance bug isexposed onlywhen
the configuration options KeepAlive andRequestReadTimeout
are specified. Otherwise, by using the default configuration, this
performancebugcannotbetriggeredeveniftheworkload(e.g.,the
number of requests) is increased.
While a full performance testing with all combinations of input
parameterscanaddresstheaboveproblem,itisinfeasibledueto
theenormouscombinationspace.Forexample,thelatestversion
ofApache HTTPServerhas 618inputparameters(610 configura-
tionoptionsand8typesofdatainputs).Itisimpracticaltotryall
combinationsofvaluesfortheseinputparameters.Toreducethe
costofperformancetesting,Shenetal.[ 46]useageneticalgorithm
(GA)asasearchheuristicforobtainingcombinationsofinputpa-
rameter values that maximize the execution time. However, thistechnique evolves all input parameters, which can be inefficientbecause many parameters may not provide contributions to the
application‚Äôs performance.
17
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Xue Han, Tingting Yu, and David Lo
Thegoalofourresearchistwofold.First,wewanttounderstand
towhatextentperformancebugsarerelatedtothecombinations
of input parameters. A study on performance bug reports from
bug tracking systems, such as Bugzilla, can help us understand
thecharacteristicsofinputparametersandtheircontributionsto
performancebugs.Second,weaimtodevelopaframeworktoauto-
maticallygeneratecombinationsofinputparameters,alsocalledtestframes (discussedinSection 2),forguidingthegenerationof
actualperformancetestcases1.Tothebestofourknowledge,no
existing research achieves the same goal.
Our main idea is to mine information from the application‚Äôs
bug reports to identify commands (i.e., commands for executing
the program) and input parameters (i.e., configuration options and
datainputs)thathavecausedperformancebugsandusethemto
generatetestframesfortestingnewerversionsoftheapplication.
PerfLearner is used during software maintenance and evolution,
wheretheprojects‚Äôissuetrackingsystemshavebeenestablished.
Specifically, we extract and rank commands and input parameters
fromeachbugreport.Wethengeneratetestframes(acombination
of the commands and input parameters) for each bug report and
prioritize the most frequently generated test frames among all bug
reports. Ourhypothesis includes: 1)bug reports containa specific
set of vocabulary relatedto commands and input parameters that
canmaketheautomatedtextextractionpossible;2)commandsand
input parameters appearing frequently in performance bug reports
maybemorelikelytotriggerperformancebugsthantheinfrequent
ones.PerfLearnerisapplicablesoftwareprojectswithestablished
issue tracking systems.
Inthisresearch,wemanuallyidentifiedandanalyzed300perfor-
mancebugreportsfromthreepopularopensourceprojects. We
discovered that it is possible to leverage information retrieval and
naturallanguageprocessingtechniques toextractcommandsand
inputparametersfrombugreports.Wefoundthatsomeinputpa-
rameters are more likely to cause performance bugs and should
be used with higher priority in performance testing. Based on our
findings, we develop PerfLearner, an approach that combines natu-
ral language processing and information retrieval to automatically
extract relevant commands and input parameters from bug reports
and use them to generate performance test frames for guiding per-
formance testing.
In summary, our paper makes the following contributions:
‚Ä¢We develop a tool, PerfLearner, that can automatically ex-
tract performance-related commands and input parameters,and generate performance test frames from the bug reports.
Tothebestofourknowledge,thisisthefirstworkthatau-
tomatically generatestest frames frombug reports written
in natural language.
‚Ä¢Weimplement PerfLearner andconduct anempirical study
to demonstrate its effectiveness and efficiency in generating
performance test frames and detecting real performance
bugs.
Weenvisiontheapproachtobeappliedtoatleasttwoscenar-
ios.First,givenaperformancebugreport,adeveloperwhowants
to know the commands and input parameters that have caused
1An actual test case is built from a test frame by specifying a concrete value for each
input parameter [37].thisbug,mayanalyzethebugreportwithPerfLearner.Second,a
testingengineercanusePerfLearnertogenerateandprioritizeper-
formance test frames from the historical performance bug reports.
The test frames can be converted into actual test cases by giving
input parameters with concrete values. Note that PerfLearner isorthogonal to existing performance testing tools. Existing tools
focus on increasing the values of certain workload-sensitive input
parameterswhileassumingthetestframes(i.e.,thecombination
ofinputparameters)exist.Therefore,PerfLearnercanbeusedto
enhancetheeffectivenessandefficiencyofexistingperformance
testing tools.
Toevaluatetheapproach,weapplyPerfLearnerto300bugre-
ports collected from Apache HTTP Server, MySQL, and Firefoxbug tracking systems. Our results show that PerfLearner is able
to extract commands and input parameters from performance bug
reports with a high accuracy. When using PerfLearner to generate
test frames, compared to a state-of-the-art combinatorial testing
(CT) technique, it generates significantly less (59.5%) test frames
on average to get the ground truth test frame. When combiningPerfLearner with an existing performance test input generationtool [
46] to detect 10 randomly selected performance bugs, Per-
fLearner detects 7 out of 10 bugs within a reasonable time whereas
when using the test input generation tool alone failed to detect all
10 bugs.
2 BACKGROUND
The concept of test frame was first introduced in the category-
partitionmethodwithtestspecificationlanguage(TSL)[ 37].TSL
wascreatedtodefinecombinationsofprograminputparameters
and environment factors. Each combination is a test frame that
canbeconvertedintoactualtestcases.Aperformancetestframe
consists of three input categories: command, configuration, and
datainput.Atestframecanhaveonecommandinthecommand
category,zeroormoreconfigurationoptionsintheconfiguration
category,andzeroormoredatainputsinthedatainputcategory.
Eachcommand,configurationoption,anddatainputinatestframe
is generally referred to as a test frame element or frame element.
Wedefinea command asanactiontoexecuteafunctionalunit[ 37]
of the program. For example, the MySQL server has several datamanipulationcommands,including
SELECT,UPDATE,and INSERT.
These commands correspond to three different functional units:
retrieve, modify, and add data records. We define input parameters
as explicit input points along with the command. An input param-
eter can be a configuration option or adata input. Configuration
options refer to a set of predefined options, e.g., command-line
optionsordirectivesinaconfigurationfile.Datainputsrefertothe
user-supplied data that is processed by the command. For example,
thedatainputassociatedwiththecommand UPDATEisthenameof
a table COLUMN.
Figure1shows a performance bug report snippet with the as-
sociated test frame and a test case. The test frame for manifesting
thisperformancebuginvolvesthreeframeelements:acommand
UPDATE,aconfigurationoption innodb_fill_factor ,andadata
input COLUMN.Aframeelementcanbe workload-sensitive.Inthisex-
ample, the UPDATEcommand is workload-sensitive because a large
numberof UPDATEqueriesisrequiredtotriggertheperformance
18
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. PerfLearner ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
Bug Description
Updatestoindexed columnmuchslowerin5.7.5.Repeatingthetest
doneforHeapengineonInnoDBshowsabigregressionfor updates
toanindexedcolumn.InnoDBismorethan2Xslowerthan5.6.21
and4Xslowerthan5.0.85.... innodb_fill_factor whosedefaultvalue
is 100 ...
Test Frame
<update >[workload] <innodb_fill_factor ><column >
Test Case
update fo oi=i+2where i = 100 innodb_fill_factor=100
mysqlslap ‚Äìnumber-of-queries=100
Figure 1: MySQL bug #74325
bug. In MySQL, a workload can be simulated by benchmark tools2
such as mysqlslap . Since many performance test generation tech-
niqueshavebeenfocusingonidentifyingtheworkload-sensitive
inputs [18,50], pinpointing the workload from a bug report may
speed up this pr ocess for performance test case generation tech-
niques. The actual test case is created by assigning concrete values
to frame elements.
3 PERFORMANCE BUG STUDY
Before designing our approach, we wish to understand to what
extentperformancebugsarerelatedtocertaincommandsandinput
parameters.
3.1 Data Collection
We chose three large open source software projects: Apache HTTP
Server,MySQLDatabaseServer,andMozillaFirefoxBrowser.With
publicly accessible sourcecode andwell-maintained bugtracking
systems, these projects have been widely used as subject programs
by existing bug characteristic studies [28, 54,55].
We collected performance bugs from bug tracking systems of
Apache,MySQL,andFirefox.Wesearchedthesesystemsusinga
set of commonly used general keywords and phrases to describe
the symptoms of performance bugs, such as ‚Äúslow‚Äù, ‚Äúlatency‚Äù, and
‚Äúlowthroughput‚Äù[ 23].Wealsosearchedtermsthatattributetoa
specific aspect of the performance problems such as‚ÄúCPU spikes‚Äù,
‚Äúcache hit‚Äù, and ‚Äúmemory leak‚Äù to identify performance bugs. Next,
we selected reports with the bug status field marked as either
‚ÄúRESOLVED", ‚ÄúVERIFIED", or ‚ÄúCLOSED" and the resolution field
marked as ‚ÄúFIXED".
The whole process yielded a total of 1383 bugs. With a large
amountofthereturnedbugreports,wecalculatetheneededsample
sizeis300,givenaconfidencelevelof95%andaconfidenceinterval
of5.Thissamplingstrategyhasbeencommonlyusedbyexisting
work [2,20].
We manually examined 300 bugs in a random order, and during
the manual inspection, we follow those reports that have sufficient
bug description details and discussions posted by commentators.Foreachbugreport,wetrytoidentifycommands,configuration
options,datainputs,andworkloadthatcausetheperformancebug.
To ensure the correctness of our results, the manual inspection
was performed independently by two inspectors ‚Äì graduate stu-
dentswhohave2-4yearsofindustrialwebdevelopmentexperience
2Abenchmarktoolisusedtomeasuretheperformanceoftheprogramundertestwith
synthesized workload.Table 1: Subjects and Their Characteristics
Application Searched Bugs Sampled Bugs #o fC M D #o fC O#o fD I
Apache 428 100 106108
MySQL 455 100 111240 5
Firefox 500 100 2456317
Total 1383 300 45241330
withApache,MySQL,andFirefox.Weholdtwotrainingsessions
of 30 minutes each to explain to inspectors the test frame elements
to be extracted from the bug report. Each inspector is given the
samesetofbugseachweektowritedownwhattheyconsiderto
be the command, configuration options, data inputs, and workload
thattriggerthebuginthereport.Inspectorsmettwiceaweekto
compare and consolidate their findings. A bug report is selectedonly when both inspectors agree on the outcome of the manualinspection. We refer to the consensus outcome as ground truthframe elements for the bug reports. This process terminates for
eachsubjectafter100bugreportshavebeenincludedinthesample
dataset.
Thenumberofbugssampledissimilartorecentworksonper-
formance bug study [ 10,23,28,56]. While a larger number of bug
reportsmayyieldabetterevaluation,thecostofthemanualpro-
cess is high ‚Äî our data collection process took a total of 320 to
400 hours spanning across more than 10 weeks. Columns 1-3 of
Table1listthe subject programs,the number ofbugs returned by
the keyword search, and the number of performance bugs sampledaftermanualinspection.Columns4-7listthenumberofcommands,configurationoptions,anddatainputsavailableinallthreesubjects.
The full lists of the three categories are saved in separate frame
elementdatabases,includingcommanddatabase,configurationdata-
base, and data input database. We collected such information by
studying all artifacts that are publicly available to users, including
documents(e.g.,usermanualsandonlinehelppages),configuration
files, and source code. Each database can be updated separately to
accommodate changes in different application versions.
3.2 Results Analysis
Aftermanuallyanalyzing300bugreports,wesummarizethefol-
lowing findings:
‚Ä¢A majority (89% to 92%) of studied performance bugs in-
volvesmorethanoneinputparameters(i.e.,configuration
options and data inputs): 91% in Apache, 92% in MySQL,
and89%inFirefox.Theseresultsimplythatcombinatorial
effectsamonginputparametersshouldbeconsideredinper-
formance testing.
‚Ä¢A significant number (41%) of performance bugs are related
toconfigurations:58% inApache,41% inMySQL,and25%
in Firefox. These results are consistent with a recent perfor-
mance bug study [23].
‚Ä¢Only23% of bugsrequire specificworkload valuesto mani-
fest:21%inApache,29%inMySQL,and19%inFirefox.These
results imply that workload is only part of the requirement
forexposingperformancebugs;otherfactors,suchasconfig-
urationoptions, shouldalso beconsidered forperformance
testing.
19
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Xue Han, Tingting Yu, and David Lo


!


 
	

"
"
"


 





	







Figure 2: Overview of the PerfLearner framework
‚Ä¢Among all 45 commands and 30 inputs for the three soft-
ware projects, 55% of them appear more than three times
in the studied bug reports. Among all 2413 configuration
options for the three software projects, only 5% of them are
relatedtothestudiedperformancebugreportsand57%of
them appear more than one time. These results suggest that
asmallsubsetofconfigurationoptionstendtoaffectappli-
cation‚Äôs performance, so performance testing might focus
mainly on such options to improve the efficiency of testing.
Inaddition,testframeelementsthatappearmultipletimesin
performance bug reports might be more likely to cause per-
formancebugsthantheothersandshouldbegivenhigher
priority in performance testing.
4 PERFLEARNER APPROACH
Guided by the findings in Section 3, we design and develop Per-
fLearner,anautomatedapproachforextractingperformancetest
frames from bug reports. Figure 2shows an overview of the Per-
fLearner framework. PerfLearner consists of three steps: frame
element extraction, test frame generation, and performance testcase generation. The shaded boxes indicate the information sup-
plied by users.
Frame Element Extraction. Given a performance bug report, Per-
fLearnerautomaticallyextractsframeelementsandtheirassociated
workloadfromthereport.PerfLearnerassumesthatabugreport
hasalreadybeenlabeledas‚Äúperformancebug",althoughexisting
techniquesonclassifyingbugreports[ 17,47,49]canbeadoptedto
automaticallyclassifyperformancebugs.Thelistofframeelementsare application and domain-specific, e.g., each application is associ-
ated with a list of different configuration options. The bug corpora
foreachapplicationisbuiltfromsourcesdescribedinSection 3.1.
The output of this step is a list of ranked frame elements and their
associated workload (if any) under each input category for each
bug report.
Performance Test Frame Generation. PerfLearner utilizes ranked
frame elements, a strength file, and a constraint file to generate
performancetestframes.Thestrengthfile,whichisusedtorestrict
the number of test frames, specifies the strength3of interaction
among elements within each input category. The constraint file
3Combinatorialtestingofstrength t(t‚â•2)requiresthateacht-wisetupleofvalues
of the different system input parameters is covered by at least one test case [8].specifies the constraints among frame elements to ensure their
combinationsarevalid.Bothfilesaredefinedoncebydevelopers
for each application, and generic to all bug reports in the sameapplication. Next, PerfLearner generates a set of test frames for
eachperformancebugreportbycombiningtheselectedcommands
andinputparameterswithrespecttothestrengthandconstraint
files. These test frames are closely related to the performance bug
describedinthereport. Finally,PerfLearnercounts thefrequency
of test frames generated from all bug reports and ranks them ina descending order. The top-ranked test frames are used first to
generate performance test cases.
Performance Test Case Generation. PerfLearner iteratively selects a
testframefromtherankedtestframesandconvertsitintoactual
performance test cases by assigning frame elements with concrete
values. PerfLearner can be combined with existing performance
testingtools,suchasprofilingandtestgenerationtools.Thecur-
rent version of PerfLearner is combined with a performance testinput generation tool [
46] that uses a search-based algorithm to
automatically generate input values to expose performance bugs.
4.1 Test Frame Element Extraction
For each bug report that is labeled as a performance bug report,
PerfLearnerextractscommands,configurationoptions,datainputs,
and the associated workload. A straightforward approach is tomatch frame element databases against each bug report using a
‚Äúgrep"-likemethod.Thematchedelementscanthenberankedby
countingtheiroccurrences‚Äìtheelementwiththehighestcount
is more likely to be the ground truth frame element for the perfor-
mancebug. However,in abugreportwritten innaturallanguage,
manywordscanbeambiguousintheirmeaning‚Äìthesameword
canrefertoacommandoraconfigurationoptiondependingonthe
context. For example, in the Apache bug #52914, the word token
timeoutcanbematchedaseitheracommandoraconfiguration
option. In addition, simply counting the occurrence of a token may
result in false positives. In the Apache bug #52914, both startand
requestappearinthebugreport,sobothtokenswouldbematched
ascommandsofthisbugreport.Incidentally,thecountof start
is actually higher than the count of request, although the ground
truth command is request.
PerfLearneremploystwostrategiestoaddresstheaboveprob-
lems. First, PerfLearner uses natural language processing and in-formation retrieval, together with user manuals to address the
20
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. PerfLearner ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
Table 2: Number of Patterns to Detect Frame Elements
Application# of Matched Patterns
Commands (8) Data Inputs (4) Workload (6)
Apache 104 77 168
MySQL 228 159 453
Firefox 203 146 270
Definitions: [CMD] ‚àà{drop, create, ...}, [SYMP] ‚àà{slow, long, ...}
Pattern:[CMD]+[SYMP]
Description : Command verbs appear in the same sentence that
symptoms exist.
Example: [DROP]CMDTABLE on very large tables can be very
[slow]SYMP.
Figure 3: A common pattern to identify command
Definitions : <ADP>‚àà{to, on, ...}, <NOUN> ‚àà{[DATA INPUT]},
[CMD]‚àà{update, insert, ...}
Pattern: [CMD]+<ADP>+<NOUN>Description
:Datainputisidentifiedasthesubjectofthecommand.
Example: [update]CMD[to]ADPindexed[column] NOUNmuch
slower in 5.7.5
Figure 4: A common pattern to identify data inputs
Definitions : [INPUT]‚àà{file, html, ...}, <VERB> ‚àà{contain, has, ...},
<ADJ>‚àà{long, large, ...}
Pattern: [INPUT]+<VERB>+<ADJ>+<NOUN>Description: Workload details the content of data inputs.Example:
atext[file] INPUT[containing] VERBavery[long] ADJ
[line]NOUN
Figure 5: A common pattern to determine a workload
mismatch problem between the frame elements (query) and bug
reports(documents). Second,wesummarize 18linguisticpatterns
that are commonly used to describe commands (eight patterns),
input parameters (four patterns), and workload in bug reports (six
patterns). While the frame elements are application-specific, the
linguisticpatternsaregenericandhencecanbereusedfordifferent
applications.
Toavoidoverfitting,thefirstauthorsummarizedthelinguistic
patterns from the 1083 bug reports (excluding the 300 sampled bug
reportsinthedataset).Intheexperiment,thesepatternsareapplied
tothe300bugreports.Wecanautomaticallydetectthepresence
ofthesepatternstolocatesentencesdescribingaparticularinput
category and identify the frame element under that category more
accurately.Table 2showsthenumberofpatternsweidentifiedin
all sentences from the 300 bug reports. While there has been some
research on using linguistic patterns in other software activities,
suchasanalyzingdeveloperintention[ 11,30]anddetectingmissing
information[ 9],littleworkisknownonusinglinguisticpatterns
to identify commands and input parameters.
4.1.1 Commands. Weobservethatacommandoftenappearswith
the bug symptom in one sentence. For example, the sentence de-
scribing the symptom of Apache bug #52914 is ‚ÄúI could reproduce
the100%CPUwithPOSTrequests‚Äù,wherethesymptomis‚Äú100%
CPU"andthecommandis request.Ifweidentifysentencescon-
tainingbugsymptoms,itnarrowsdownthesearchandimproves
the accuracy of finding the performance bug-triggering commands.We have defined six linguistic patterns using the part-of-speech
tag for detecting (one or more) sentences containing symptoms. If
such sentences are detected, PerfLearner matches the command
againstthesesentencesandcountstheiroccurrences.Theidentified
kcommands are ranked at the top kposition in a descending order
with respect to their occurrences.
Our patterns can precisely identify commands in 91% perfor-
mancebugreports(i.e.,rankedatthetop-1),comparedtothe78%
precision rate by the ‚Äúgrep"-like method. The most frequently used
pattern, as seen in Figure 3, illustrates a pattern that uses a verb
andaphrase,wheretheverbreferstothecommandelementand
the phrase refers to the predefined list of phrases indicating per-formance bug symptoms. If any of the symptoms appear in the
sentence, the verb is identified as a candidate of the bug-triggering
command.Ifnosymptomsentencesaredetected,PerfLearnerprior-
itizes sentences that appeared in the bug report title as well as the
first post, and uses the ‚Äògrep"-like method to count the occurrences
of commands. If no command sentences are detected, the same
approach is applied to the entire bug corpus.
4.1.2 Data Inputs. PerfLearner ranks data inputs in a similar way
ascommandsbecausesimplymatchingabugreportagainsttheele-mentsindatainputisimprecise.PerfLearnerdefinesfourlinguistic
patterns to detect sentences that contain data inputs and rank data
inputswithinthesesentences.Figure 4showsoneofthecommonly
usedpatterns.Thispatternindicatesthatdatainputscoexistwith
commandsinthesamesentence.Specifically,thesentencestarts
with a command (i.e. update), followed by a preposition (i.e. to, on)
and the data input (i.e. column).
4.1.3 Configuration Options. Unlikecommandsanddatainputs,
we observe that many configuration options cannot be directly
searched from bug reports. One solution is to leverage information
retrieval (IR) algorithms such as TF-IDF [ 40] and cosine similar-
ity [16] based on the vector space model (VSM) to rank config-
uration options in terms of their relevance to the bug report. A
straightforwardmethod istosplitthe configurationnameintoto-
kenstocalculateitscosinesimilaritytothebugreport.However,
weobservethatmanyconfigurationoptionssharewiththesame
tokens. Since a configuration option name is often short, this ap-
proachmayresultinmanyequallyrankedconfigurationoptions.
For example, in Figure 1,innodb_buffer_pool_instances and
innodb_buffer_pool_size wouldberankedequallyif‚Äúinnodb",
‚Äúbuffer", and ‚Äúpool" are the three word tokens appearing in the
report.
Toimprovetheaccuracyofranking,weleveragemanualsthat
describe configuration options to bridge the lexical gap between
configuration option names and bug reports. In the example of
Figure1,themanualdescriptionof innodb_fill_factor (Figure7)
contains words such as ‚Äúb-tree‚Äù, ‚Äúindex", and ‚Äúspace", which also
appear in the bug report, can be used to link the configuration
option to the bug report effectively.
To compute the similarity between a configuration option oand
a bug report br, we first concatenate owith its textual description,
where o=o‚à™o.desc.PerfLearnerthenprocesses obystandardNLP
pre-processing steps: word tokenization andstop word removal. The
tokenizationconvertsbugreportsintoa‚Äúbagofwords‚Äùusingwhite
spaces. We then remove punctuation, numbers, and standard stop
21
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Xue Han, Tingting Yu, and David Lo
words.Compoundwords,suchastheconfigurationoptionname
Browser.chrome.image_icons.max_size can be split by camel
case, dots, or underlines into tokens.
Next, all words are reduced to their base form using lemmatiza-
tion. Unlike stemming that simply chops off the ending of a word,
lemmatizationinvolvesacomplexwordanalysisandgenerallypro-
vides better results. Finally, we also remove repeated text sections,
asquotationsofthepreviouscommentatorinthebugreporthap-
penveryfrequentlyandtherepeatedtextwouldaffecttheaccuracy
oftexttokendistribution.Eachbugtrackingsystemmayhavetheir
ownmechanismtomarkquotations.Forexample,Bugzillabased
bugtrackingsystems,quotationstartswiththegreatersign(‚Äú >‚Äù)
symboloneachnewlineandthequotationblockhasaCSSclass
of ‚Äúquote‚Äù. Developers can design their own match patterns for
removing quotations and plug it into PerfLearner.
After processing o(the combinedconfiguration optionand its
description),let Vbethevocabularyofalltexttokensfromboththe
bugreport brando.Letr=[ wt,br|t‚ààV]ando=[ wt,o|t‚ààV]be
theVSMrepresentationsofthebugreport brandtheconfiguration
option o. The term weights wt,brandwt,oare computed using the
classical TF-IDF method described in existing literature [ 16]. After
thevectorspacerepresentationsarecomputed,thetextualsimilarity
score between oandbrcan be calculated using the standard cosine
similarity between their corresponding vectors:
sim (br,o)=cos (br,o)=r√óo
|r|√ó|o|
The score is computed by the inner product of the two vectors,
divided by their Euclidean distance. For MySQL bug #74325 (Fig-ure1), by utilizing the configuration API description (Figure 7),
PerfLearner ranks innodb_fill_factor at the top.
4.1.4 Identifying Workload. In performance testing, we need to
know which frame elements are workload-sensitive, so testingcan focus on generating workload values for these elements. We
havedefinedsixlinguisticpatternstoidentifysuchframeelements.
The most frequently used pattern is to locate sentences containing
benchmark tool names. Benchmark tools are often used to simu-late workload in performance bug reports. For instance, MySQLbug report #74325 uses benchmark tool
mysqlslap to generate
a large number of database updates. Therefore, by searching forthe benchmark name
mysqlslap , we can detect that updateis
workload-sensitive.Thispatternappliesto44.2%ofperformance
bug reports involving specific workload.
The second commonly used linguistic pattern detects sentences
describingworkloadinformationofdatainputs(Figure 5).Inthis
pattern, the data input (i.e. a text file) is followed by a verb (i.e.
containing)thatdetailsthecontentofinputdata(i.e.averylong
line). Once this pattern is detected, the corresponding data input is
considered to be workload-sensitive.
4.2 Performance Test Frame Generation
PerfLearner generates performance test frames from the ranked
frameelements,theworkloadspecification,astrengthfile,anda
constraint file. The strength file specifies top- Nframe elements
undereachinputcategorytobeusedfortestframegeneration.The
constraint fileis used toenforce constraints ofinteraction among
frame elements, which can limit the number of (invalid) frames
+" 0
+! 0


 	

	


	
)*
)	!*
)*
)	*
)*
)	!''	*	
((!
#( !(( $
	
	

((!
(( $
	
	

((!
#( !(( $)+! *
	

((!(( $
)+! *
	

((!
(! 
	


,% -% 
.% 

&
,% ((!-% #( !(( $
.% (( $
/% (! 
&
,% 	
-% 
.% 
&








	

Figure 6: An example of performance test frame generation
InnoDBperformsabulkloadwhencreatingorrebuildingindexes.
This method of index creation is known as a ‚Äúsorted index build".
innodb_fill_factor defines the percentage of space on each B-tree
pagethatisfilledduringasortedindexbuild,withtheremaining
space reserved for future index growth.
Figure 7: API description for innodb_fill_factor
to be generated. The constraints are manually derived from user
manuals. Both files are provided by users and generic to all bug
reports within the same application.
Figure6shows a partial constraint file of MySQL. The data def-
inition command DROPin the SQL works with DATABASE and
TABLE but not with COLUMN. We use ifto enforce conditions
on which frame elements can be combined. To enforce the rule
that UPDATEworks withTABLE but notDATABASE, condition [if
CMDUpdate]isaddedfordatainputsCOLUMNandTABLE.Condi-
tion [if CMDDrop] is added for data inputs DATABASE. Therefore,
when UPDATEis chosen, it can only be combined with COLUMN
and TABLE. Ourexperiment indicatesthat addingconstraints can
reduce 70% of test frames.
In the example of Figure 6, the strength file indicates that top-
2 commands ( nc), top-5% configuration options ( po), and top-2
datainputs( nd)areselectedtogeneratetestframes.Becausethe
numberofoptionsisoftenlarge,weuseapercentageofthetotal
numberofconfigurationoptionstoindicatetheselectednumber
ofconfiguration options.The threesymbols tc,to,and tdindicate
theinteractionstrengthsforcommands,configurationoptions,anddatainputsrespectively.InFigure 6,apair-wisecombination(
to=2)
is applied to the configuration options and no combinations are
used forthe command ( tc=1) anddata inputs ( td=1). Figure 6also
showsthedefaultstrengthfileusedbyPerfLearner.Thesestrength
valuesarechosenbasedonourempiricalevaluationastheyarethe
minimum requirements for generating test frames achieving up to
90%accuracy.Wealsoevaluatedthesensitivityofthesevaluesin
Section7.
22
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. PerfLearner ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
Algorithm 1 PerfLearner Test Frame Generation
Require: StrenF,ConsF,Bu–¥Reports ,LPtn
Ensure:TFprio
1:forbr‚ààBu–¥Reports do
2:RLcmd‚ÜêRankCmd (br,DBcmd,LPtn .cmd)
3:RLco‚ÜêRankConfig (br,DBco)
4:RLdi‚ÜêRankData (br,DBdi,DBdi,LPtn .di)
5:Lwl‚ÜêGetWorkload (br,DBdi,LPtn .wl)
6:RL‚ÜêSelectElements (RLcmd,RLco,RLdi,StrenF)
7:TFbr‚ÜêGenerateFrames (RL,Lwl,ConsF)
8:TF‚ÜêTFbr‚à™TF
9:end for
10:TFprio‚ÜêRankFreq (TF)
Algorithm 2 Combining PerfLearner with Testing Tools
Require: TFprio
Ensure:TestResults
1:fortf‚ààTFpriodo
2:fore‚ààtfdo
3:cate–¥ory‚ÜêGetInputCate–¥ory (e)
4: ifHasWorkload (e)then
5: e‚ÜêInte–¥rateWorkload (cate–¥ory ,e)
6: end if
7: tc.xml‚ÜêUpdateTestCase (e)
8:end for
9: TestResults‚ÜêRunPerfTestTool (tc.xml)
10:end for
Algorithm 1describes the process of generating performance
test frames. The algorithm takes as input a list of bug reports from
an application, a strength file, and a constraint file. For each bug
report, the algorithm obtains a ranked list for each input category
(Lines 2-4) and a list of workload (Line 5). It then selects frame
elements from the ranked lists with respect to the strengths. Next,
a list of candidate test frames is generated given the selected frame
elements and the constraints (Line 7). Finally, the algorithm ranks
test frames collected from all bug reports (Line 10) in terms of the
frequency of their appearance. Test frames ranked higher indicate
theymaybemorelikelytocauseperformancebugs.Thelastcolumn
of Figure 6shows an example of the five test frames generated.
4.3 Performance Test Case Generation
Algorithm 2outlines the process of generating performance test
casesfromtestframes.First,PerfLearneriterativelyselectsatest
frame from the prioritized list output by Algorithm 1. For each
frameelement,thealgorithmchecksforitsinputcategory.Ifthe
frame element is workload-sensitive, depending on the input cate-
gory, the algorithm applies workload in two ways (Line 5). For the
commandcategory, thebenchmark optionthat controlsworkload
isincludedinthetestcasegeneration.Forotherinputcategories,
theinputsizeisincludedinthetestcasegeneration.Thealgorithm
updates the test case as it gets more information from frame ele-
ments(Line7).Specifically,thetestframeisconvertedintoanXML
file(tc.xml)ofwhichstructureisknowntothetestcasegeneration
tools. Finally, the test input (tc.xml) is supplied to the performance
testingtool.Itisuptotheperformancetestingtooltodeterminehow to assign input values and execute the subject under test to
detect performance bugs.
5 IMPLEMENTATION
WeimplementedawebcrawlerusingthePythonBeautifulSoup
library [5] to collect raw bug reports and API documentations.
We then leveragedPython Natural Language Toolkit (NLTK) [ 35]to parse the description of the bug reports and match linguistic
patternsagainstthe newbugreportswithregular expressionson
part-of-speec htags.Fortheinformationretrievalcomponent,we
utilized the Python machine learning library scikit-learn [ 45]t o
gettheTF-IDFmatrixandcosinesimilarityscores.Lastly,weim-
plemented Python programs to handle the performance test frame
generation.
6 EVALUATION OF PERFLEARNER
We evaluated PerfLearner on three open source projects with char-
acteristicsdescribedinSection 3.1.Weaimtoanswerthefollowing
research questions:
RQ1:HowaccurateisPerfLearneratdetectingperformancebug-
triggering frame elements and workload?
RQ2:How effective and efficient is PerfLearner at generating per-
formance test frames?
RQ3:Can PerfLearner enhance existing performance testing tools
for detecting performance bugs?
6.1 Techniques and Metrics
RQ1:AccuracyofBugReportsAnalysis. ToanswerRQ1,weevalu-
atetheaccuracyofPerfLearnerinextractingframeelementsand
workload. The techniques for extracting commands, configuration
options, data inputs, and workload are denoted as CD, CO, DI, WL,
respectively.Eachtechniqueiscomparedtoabaselinemethodto
evaluatetheeffectsofusingadvancedtechniquessuchaslinguistic
patterns andinformation retrieval (TF-IDF, Cosine Similarityetc.).
Specifically, we compare CD, CO, DI to three baseline techniques ‚Äì
CDs,COs,andDIs.Thesebaselinetechniquesuseakeywordmatch
and count the occurrence of each frame element appearing in a
bug report. To evaluate the usefulness of configuration manuals
inextractingconfigurationoptions,wealsocompareCOtoCO a.
COauses only tokens in the configuration option name without
configurationmanualstomakethesimilaritycomparison.Sincethe
workload describes whether a frame element is workload-sensitive,
thekeywordcountingisnotapplicableinthiscase.Nevertheless,
toevaluatetheusefulnessoflinguisticpatternsinidentifyingthe
workload, the baseline technique WL rrandomly selects a frame
element and treats the element as workload-sensitive.
Weusetwometricstoevaluatetheeffectivenessofranking.The
firstmetricisthetop-Nsuccessrate,whichiscomputedbyranksofgroundtruthswithintopNitemsoverallbugreports.Forexample,
if20outof100performancebugreportsrankthegroundtruthof
configurationoptionsinthetop5%ofall600configurationoptions,
the top-N (N=5%) success rate is 20%. When there are multiple
elementsspecifiedasthegroundtruth,weonlyconsiderthefirst
one that PerfLearner can find. Since workload is directly identified
without ranking, we examine the percentage of bug reports in
which ground truth workload is found.
Forthesecondmetric,weuseMAP(MeanAveragePrecision).
MAP is a single-figure measure of ranked retrieval results indepen-
dentofthesizeofthetoplist[ 44].Itisdesignedforgeneralranked
retrieval problems, where a query can have multiple relevant docu-
ments. To compute MAP, it first calculates the average precision
23
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Xue Han, Tingting Yu, and David Lo
(AP) for each individual query Qi, and then calculates the mean of
APs on the set of queries Q:
MAP=1
|Q|¬∑/summationdisplay
Qi‚ààQAP (Qi)
ToillustratetheMAPcalculation,supposetherearetwoconfigu-
rationoptions o1and o2associatedwithabugreport.IfTechnique-I
ranksthetwooptionsatthe1stand2ndpositionsamongall500
options andTechnique-II ranksthe two optionsat the1st and 3rd
positions, then the MAP of Technique-I is (1/1 + 2/2)/2 = 1 and the
MAP of Technique-II is (1/1 + 2/3)/2 = 0.8.
RQ2: Effectiveness and Efficiency of Generating Performance Test
Frames.To answer RQ2, ideally, the comparison should be done
with existing approaches that generate performance test frames.
However, we cannot find an existing approach with this specific
goal.Intheabsenceofsuchapproaches,weinsteadcomparePer-
fLearner to a combinatorial testing (CT) strategy [ 19] that employs
the category-partition method [ 37], t-wise testing [ 39], and the
random testing approach. Specifically, CT generates test frames
by combining elements under each input category with respect to
theconstraints.ThefirstdifferencebetweenPerfLearnerandCT
is that CT does not analyze bug reports or rank frame elements in
terms of their relevance to the report; instead, CT ranks the frame
elements in a random order. The second difference is that in CT,
the workload is randomly assigned to a frame element. To make a
faircomparison,theinteractionstrengthofconfigurationoptions
and that of data inputs are the same as those used in PerfLearner.
To evaluate the cost-effectiveness of PerfLearner and CT in gen-
erating performance test frames, we wish to know whether frame
elements frequently appeared in historical bug reports can be used
togeneratetestframe fortestingfutureversionsoftheprograms.
For each bug report used for evaluation, we manually inspect and
derivethetestframethattriggerstheperformancebugdescribed
inthereport(Section 3.1).Werefertothistestframeastheground
truth test frame. Since test frames cannot be executed directly, we
consideranapproachdetectsthebugifthegroundtruthtestframe
is included in the generated test frames. To do this, we first list the
100 bug reports from each program in ascending order by the bug
creationdate. Wethen selectthe first90bug reports(training set)
andapplytechniques(PerfLearnerandCT)describedinSection 4.2
to generate testframes. Wecompare the test frames generated by
eachtechniqueagainsttheremaining10bugreports(testset)from
eachsubject.Specifically,weexamineatwhichiterationtheground
truth test frame of the test set bug report is generated by the tech-
nique.Toevaluatetheefficiencyofthetwotechniques,weevaluate
the time they take to generate the ground truth test frames.
RQ3:DetectingPerformanceBugs. BesidesevaluatingPerfLearneron
generatingperformancetestframes,wewouldliketoknowwhether
thegeneratedframesareusefulfordetectingactualperformance
bugs. PerfLearner is orthogonal to existing performance testingtools. It aims to improve the efficiency of testing by focusing on
selectingcommands andinput parametersthataremorelikely to
exposeperformancebugs.ToanswerRQ3,wecombinePerfLearner
withGA-Prof,aperformancetestinputgenerationtooltodetect
performancebugs[ 46].WechooseGA-Profbecauseitistheonly
toolthatcanevolvebothconfigurationoptionanddatainputvalues.
GA-Prof employs a genetic algorithm to explore the space of inputTable 3: RQ1: Test Frame Extraction Accuracy
App.MetricCommand Data Input Config. OptionMetricWorkload
CDCDsDIDIsCOCOsPLa WLWLr
Ap.Top-N91%78%83%67%71%71%67%Acc.78%60%MAP0.800.700.700.600.370.220.33
My.Top-N83%75%91%81%83%75%53%Acc.67%43%MAP0.600.500.800.700.240.230.21
Fi.Top-N82%80%90%90%85%83%60%Acc.80%42%MAP0.800.600.700.600.280.220.20
combinations among allinput parameters. We re-implemented the
genetic algorithm part of GA-Prof to handle C/C++ applications.
We compare two settingsof GA-Prof: 1) a default setting (denoted
byGA)inwhichthecombinationsareevolvedforallcommands
andinputparameters,and2)anenhancedtechnique,denotedby
GPPLwhereitutilizestestframesgeneratedfromPerfLearnerto
iteratively select and evolve input values to generate performance
test cases.
Toevaluatewhetherthetwotechniquesareabletodetectper-
formance bugs within a reasonable time limit, we select real per-
formancebugsthatwecanreproduce.Weiterativelyselectabug
reportfromthe1083performancebugreports(excludingthe300
sampledbugreportsinthedataset)andtrytoreproducethebug.Be-
cause reproducing performance bugs is challenging and expensive,
we stop this process after we have 10 bugs successfully reproduced
‚Äì this process took approximately 400 work hours.
Next, we apply the two techniques to the program versions
correspondingtothe10performancebugs.Weevaluatewhether
theperformancebugdescribedinthebugreportcanbedetectedand
recordthetimeittakes.Specifically,weconducttestexperimentson
High-Performance Computer(HPC) clusters. Thebasic HPC node
is equipped with a 6 core 2.66 GHz Intel Xeon X5650 Westmere, 36
GBmemory,and 256GB harddrive. Thisenvironment enablesus
to run multiple experiments simultaneously without interruption.
Each experiment is repeated10 times and we report the mean to
reduce the bias due to randomness. We default the time limit to
24 hours before terminating the experiment and set the maximum
number of GA iterations in each run to be 10.
6.2 Results and Analysis
RQ1: Accuracy of Bug Reports Analysis. Table3shows the effec-
tiveness of different techniques at ranking frame elements andextracting workload. The success rates are based on the default
valuesspecifiedinthestrengthfile.Theresultsindicatethatcom-
mands appear in the top-2 positions for 82-91% of bug reports; the
correctdatainputappearsinthetop-2positionsfor83-90%ofthe
bug reports; the correct configuration option appears in the top-5%
returnedresultsfor71-85%ofthereports.Additionally,thework-
loadisidentifiedwith56-80%accuracy.Comparedtothebaseline
approaches, the success rate is higher in each category over all
programs.
Where the MAP scores are concerned, PerfLearner is more ef-
fectivethanthebaselinetechniquesoverallthreetypesofframe
elementsacrossallsubjectprograms.Theimprovementsrangefrom
14%to40%.Theseresultssuggestthat heuristicsusedbyPerfLearner
is effective in boosting accuracy.
24
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. PerfLearner ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
Table 4: RQ2: Performance Test Frame Generation
Application # of Const.PerfLearner CT
SpaceCount Avg. SpaceCount Avg.
Apache 45 445K 2662 64M 10K
MySQL 12 1.4M 1831 2.9B 10K
Firefox 25 443K 7659 3.9B 10K
# of Const.=the number of constraints. Space=the number of total configurations w.r.t.
constraints and the default weight. Count Avg.=the average number of test frames
generated by the test method before reaching the ground truth.
Table 5: RQ3: Performance Testing with GA
Application Bug IDEffectiveness Efficiency
GAGAPLGACount GPPLCountPL
Apache54852 NOYES24H8297 5.2H 1714
52914 NOYES24H942910.1H 3052
37680 NONO24H8790 24H 9764
43081 NOYES24H882220.2H 6085
46749 NONO24H9037 8.7H 3125
MySQL21727 NOYES24H861414.8H 4097
44723 NOYES24H925911.7H 3015
74325 NOYES24H845811.3H 4055
15653 NOYES24H7446 7.3H 2910
26938 NONO24H9793 24H 9425
RQ2: Effectiveness and Efficiency of Performance Test Frame Genera-
tion.Table4showstheresultsofPerfLearnerandCTingenerating
performancetestframes.SinceCTdoesnotranktestframes,we
allowCTtogeneratetestframesamongrandomlysampledinput
spaceforeachinputcategory.Welimitthenumberoftestframes
to 10,000. The threshold number is based on practical considera-
tions as 10,000 tests may take considerable executing time. Withthe default CT method, all three subjects failed to generate theground truth test frame before the frame limit threshold. These
resultssuggestthat PerfLearnerismorecost-effectiveatgenerating
performance test frames than the traditional combinatorial testing
approach. Figure8shows the distribution of test frames generated
ineachsubjectforbothPerfLearner(PL)andCT.Firefoxhasthe
worst performance of all, this is largely due to Firefox bugs require
multiple steps to trigger. Firefox also has the largest number of
commandsandlowestcommandextractionaccuracy.Asaresult,
the ranking of test frames does not work as effectively as the other
two subjects.
Figure 8: Test frame generation
RQ3:EnhancingPerformanceBugDetection. Table5showsthere-
sults of GA and GP PL(GA enhanced with PerfLearner). GA failed
todetectall10performancebugs.Likeothertestcasegeneration
techniques, the genetic algorithm for generating input values is ap-pliedonlyafteratestframeissele cted.However,withoutknowing
which frame element is more likely to cause a performance bug,a random method is used to allow frame elements in each input
category to have an equal chance to be selected. As a result, many
low-quality test frames are generated. The ground truth test frame
often fails to be generated within the time limit (24 hours).
Our results show that the GP PLapproach can detect 7 out of
10 performance bugs within an average of 10.9 hours. These re-
sults suggestthat PerfLearner canpotentially enhance existingper-
formancetestingtools. ForthethreebugsGP PLfailedtodetect:1)
Apachebug#37680requirestwoentriesofthe‚ÄúListen‚Äùoption.When
selecting configuration options, we do not allow duplications of
configurationoptionsincemultipleappearancesofthesameoption
normally overwrites one another. 2) Apache bug#46749 executes a
testframe(servergracefulstop)thatcausesalongresponsetime.
Thistestframeisconsideredtotriggeraperformancebug,however,
thegroundtruthtestframeofthisbugisrelatedtocacheutilization.
Thisistheonlyfalsepositivecaseappearedinourexperiment.3)
ForMySQLbug#26938,the‚Äúprofile‚Äùcommandisrequiredtotrigger
this bug. However, none of the bug reports used to generate test
framesincludesthecommand‚Äúprofile‚Äù.Weconjecturefalsenega-
tive cases can be reduced as more bug reports are used for mining
test frames.
Figure 9: Weight sensitivity analysis
7 DISCUSSION
SensitivityofStrength. Bydefault,PerfLearnerusesstrengths{ nc
=2,no=5%, nd=2,nw=2,t=2}.Theselectedvaluesarebasedon
theempiricalstudythatachievesbesttestframeelementextracting
results. To understand the influence of selecting different sets of
strengths, we evaluate PerfLearner on two other sets of strengths:
w1={ nc=1,no=2%, nd=1,nw=1,and t=1}andw2={ nc=3,
no= 10%, nd=3 ,nw=3 ,a n d t= 3 }. Figure 9reports the results
of test frame generation using the three sets of strengths on the
test set (10 bug reports) for each of the three subjects. The verticalaxis indicates the number of frames generated before reaching the
groundtruth.Theresultsindicatethat,ingeneral,defaultstrengths
outperform the other two sets. In Apache, w1 outperforms the
default strengths in terms of the average frames generated, but w1
exhibitsalargerstandarddeviation.Theweightsensitivityanalysis
implies that the strengths should not be set too low or too high.
LowstrengthvaluesmaycausePerfLearnertomisscertainrelevant
frames, whereas high strength values may result in generating too
many performance test frames and thus reduce the efficiency of
PerfLearner.
ThreatstoValidity. Theprimarythreattotheexternalvalidityof
thisstudyinvolvestherepresentativeness ofoursubjectsandbug
reports.Wedoreducethisthreattosomeextentbyusingseveral
varieties of well studied open source projects and bug tracking
25
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Xue Han, Tingting Yu, and David Lo
systems for our study. Combining keyword search and manual
inspectionisaneffectivetechniquetoidentifybugsofaspecifictype
fromalargepoolofgenericbugsandhasbeenusedsuccessfully
inpriorstudies[ 28,33,54].Wecannotclaimthatourresultscan
begeneralizedtoallsystemsofalldomainsthough.Theprimary
threat to the internal validity involves the manual inspection to
identifythegroundtruthtestframefromabugreport.Tominimizetheriskofincorrectresultsgivenbymanualinspection,theanalysis
process was done independently by two trained inspectors.
Limitations. Thetextualqualityofabugreporthassubstantial
impact on the effectiveness of the proposed approach. For example,abugreportmaynotusethestandardnamesoftheframeelements.
This can be addressed by integrating advanced NLP techniques,
suchas Word2Vec [ 51].The incompletenessofbugreports isalso
amajorobstacleforPerfLearnertoworkwell,likeformanybug
report analysis techniques. One strategy is to filter out bug reports
containingmissinginformationusinganautomatedapproach[ 9]
and apply PerfLearner only to complete bug reports to improveaccuracy. Other classification techniques can be integrated with
PerfLearner as well, such as detecting reproducible [ 15] and dupli-
cate [26] bug reports.
PerfLearner takes only labeled performance bug reports. One
extensionpointistobuildapredictionmodelthatcanautomatically
predict whether a new bug report is related to performance or not.
There has been some research on using text mining to classify bug
reports [17,47,49], which can be easily tuned to handle perfor-
mance bug reports. In addition, when a performance bug requires
aspecificsystemstate(e.g.,networkingevents)tobetriggered,the
current approach cannot extract such information. For example,a state may be associated with the topology of the target system
(e.g., the firewall setup may negatively affect the performance of a
system).Nevertheless,webelievePerfLearnercanbeextendedto
handle system-level triggering events by defining additional frame
databases and linguistic patterns.
8 RELATED WORK
There has been a great deal of research on analyzing, detecting,
and fixing performance bugs [ 7,28,29,34,36]. Burnim et al. [ 7]
designedatechniquetogenerateworse-caseinputs(largerinput
sizes) to find performance bugs. As discussed in Section 1, these
techniques often rely on initial test cases and do not address the
challenges of finding the right combination of input parameters
tocreateeffectiveinitialtestcases.Asourempiricalstudyshows,
workload only helps to trigger some but not all performance bugs.
Although PerfLearner also takes workload into consideration, it
focuses more on the combination of elements to be used in the test
frame.Ourmethodisorthogonaltothetestcasegenerationtools,as
our experiment shows, PerfLearner can be integrated into existing
performancetestingtechniquestoimprovetheeffectivenessand
efficiency of bug detection.
A great body of work has been conducted on applying combina-
torial testing (CT) to address the problem of large input space in
complexandconfigurablesystems[ 13,32,52,57].CTsystematically
samplestheinputspaceandtestsonlytheselectedinputparame-
terscombinations.Zhangetal.[ 57]proposedamethodtooptimize
combinatorialtestingtogeneratetestcasestofindabalancedpointof coverage without pressuring on achieving the maximum cov-erage. Dumlu et al. [
13] proposed a feedback-driven approach to
detectandavoidmaskingeffectresultedfromCT.Thesetechniquesfocusonsamplingcombinationsfromtheentireinputspace.There-
fore, it is often inevitable to result in a large sampling space. Inthe contrast, PerfLearner detects and uses only the error-pronecommands and input parameters from the historical bug reports.
Empirical results show that our approach can significantly reduce
thesamplingspacewhengeneratingtestframesforperformance
bugs.
Therehasbeenconsiderableworkonusingnaturallanguageand
informationretrievaltechniquestoimprovecodedocumentation
and understanding [ 9,14,24,25] and to create code traceability
links[1,12,38].While ourworkappliessome ofthesesamebasic
techniques,suchastokenization,lemmatization,vectorspacemodel
with term frequency-inverse document frequency weighting [ 4],
the prior work has not applied these techniques to performance
bug reports and has not considered or extracted input parameters
to generate test frames.
Therehasbeenalargebodyofworkthatdemonstratestheneed
for configuration-aware testing techniques and proposes methods
to sample and prioritize the configuration space [ 27,31,42,43,53]
to reduce the cost of testing. For example, Jamshidi et al. [ 27] con-
ductanempiricalstudytoevaluatethefeasibilityofapplyingthe
transfer learning technique to reduce the dimensionality of the
configuration space when constructing performance models. Nair
et al. [31] use inexpensive and inaccurate models to find optimal
configurations with less cost compared to the state-of-the-art sam-
plingtechniques.Unliketheabovetechnique,ourapproachfocuses
on creating test frames to aim performance testing for finding per-
formance bugs instead of performance modeling.
9 CONCLUSIONS
Performancebugsaredifficulttoexposebecausetheyoftenman-
ifest under special input conditions and system configurations.In this paper, we studied 300 real-world performance bugs fromthree popular open source projects. Our findings indicate that
combinationsofinputparameters,especiallyconfigurations,can
playanimportantroleinexposingperformancebugs.Guidedby
thesefindings,wedesignedPerfLearner,anautomatedapproach
to extract test frame elements, and to generate test frames for
performance testing. We evaluated PerfLearner on 300 bug re-
ports and the results show that PerfLearner extracts test frame
elementswithhighaccuracy.PerfLearnerisalsoeffectiveingen-
erating performance-bug-triggering test frames. Our evaluation
oncombiningPerfLearnerwithGA-Proftodetectreal-worldper-
formance bugs indicates that PerfLearner can enhance existing
performancetestingtoolsforgeneratingtestcasesanddetecting
performance bugs. For reproducibility and further research, Per-
fLearnerandallthedatafromtheexperimentsarepubliclyavailable
at https://github.com/xha225/PerfLearner.
ACKNOWLEDGMENTS
This research is supported in part by the NSF grant CCF-1652149.
26
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. PerfLearner ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
REFERENCES
[1] N. Ali, W. Wu, G. Antoniol, M. Di Penta, Y. G. Gu√©h√©neuc, and J. H. Hayes.
Moms: Multi-objective miniaturization of software. In International Conference
on Software Maintenance, pages 153‚Äì162, 2011.
[2] Andrea Arcuri and Lionel Briand. A practical guide for using statistical tests to
assess randomized algorithms in software engineering. In Software Engineering
(ICSE), 2011 33rd International Conference on, pages 1‚Äì10. IEEE, 2011.
[3]Mona Attariyan, Michael Chow, and Jason Flinn. X-ray: Automating root-cause
diagnosis of performance anomalies in production software. In OSDI, pages
307‚Äì320, 2012.
[4]Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto. Modern Information Retrieval.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1999.
[5] Beautiful soup, 2017. https://www.crummy.com/software/BeautifulSoup/.
[6] Bugzilla keyword descriptions, 2016.
https://bugzilla.mozilla.org/describekeywords.cgi.
[7] Jacob Burnim, Sudeep Juvekar, and Koushik Sen. Wise: Automated test
generation for worst-case complexity. In Proceedings of the International
Conference on Software Engineering, pages 463‚Äì473, 2009.
[8] Andrea Calvagna and Angelo Gargantini. A formal logic approach to
constrained combinatorial testing. Journal of Automated Reasoning,
45(4):331‚Äì358, 2010.
[9] Oscar Chaparro, Jing Lu, Fiorella Zampetti, Laura Moreno, Massimiliano
Di Penta, Andrian Marcus, Gabriele Bavota, and Vincent Ng. Detecting missing
information in bug descriptions. In Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering, pages 396‚Äì407, 2017.
[10] Daniel J Dean, Hiep Nguyen, Xiaohui Gu, Hui Zhang, Junghwan Rhee, Nipun
Arora, and Geoff Jiang. Perfscope: Practical online server performance bug
inference in production cloud computing infrastructures. In Proceedings of the
ACM Symposium on Cloud Computing, pages 1‚Äì13. ACM, 2014.
[11] Andrea Di Sorbo, Sebastiano Panichella, Corrado A Visaggio, Massimiliano
Di Penta, Gerardo Canfora, and Harald C Gall. Development emails contentanalyzer: Intention mining in developer discussions (t). In Automated Software
Engineering (ASE), 2015 30th IEEE/ACM International Conference on, pages 12‚Äì23,
2015.
[12] Bogdan Dit, Latifa Guerrouj, Denys Poshyvanyk, and Giuliano Antoniol. Can
better identifier splitting techniques help feature location? In International
Conference on Program Comprehension, pages 11‚Äì20, 2011.
[13] Emine Dumlu, Cemal Yilmaz, Myra B. Cohen, and Adam Porter. Feedback
driven adaptive combinatorial testing. In Proceedings of the International
Symposium on Software Testing and Analysis, pages 243‚Äì253, 2011.
[14]Eric Enslen, Emily Hill, Lori Pollock, and K. Vijay-Shanker. Mining source code
to automatically split identifiers for software analysis. In International Working
Conference on Mining Software Repositories, pages 71‚Äì80, 2009.
[15] Mona Erfani Joorabchi, Mehdi Mirzaaghaei, and Ali Mesbah. Works for me!
characterizing non-reproducible bug reports. In Proceedings of the 11th Working
Conference on Mining Software Repositories, pages 62‚Äì71, 2014.
[16]Christos Faloutsos and Douglas W Oard. A survey of information retrieval and
filtering methods. Technical report, 1998.
[17] Michael Gegick, Pete Rotella, and Tao Xie. Identifying security bug reports via
text mining: An industrial case study. In Mining software repositories (MSR), 2010
7th IEEE working conference on, pages 11‚Äì20, 2010.
[18] Mark Grechanik, Chen Fu, and Qing Xie. Automatically finding performance
problems with feedback-directed learning software testing. In 2012 34th
International Conference on Software Engineering (ICSE), pages 156‚Äì166. IEEE,
2012.
[19] Mats Grindal, Jeff Offutt, and Sten F Andler. Combination testing strategies: a
survey.Software Testing, Verification and Reliability, 15(3):167‚Äì199, 2005.
[20] Robert J Grissom and John J Kim. Effect sizes for research: A broad practical
approach. Lawrence Erlbaum Associates Publishers, 2005.
[21]Shi Han, Yingnong Dang, Song Ge, Dongmei Zhang, and Tao Xie. Performance
debugging in the large via mining millions of stack traces. In Proceedings of the
International Conference on Software Engineering, pages 145‚Äì155, 2012.
[22]Shi Han, Yingnong Dang, Song Ge, Dongmei Zhang, and Tao Xie. Performance
debugging in the large via mining millions of stack traces. In ICSE, pages
145‚Äì155, 2012.
[23] Xue Han and Tingting Yu. An empirical study on performance bugs for highly
configurablesoftwaresystems. In Proceedingsofthe10thACM/IEEEInternational
Symposiumon EmpiricalSoftwareEngineeringand Measurement,page 23.ACM,
2016.
[24] Emily Hill, Zachary P. Fry, Haley Boyd, Giriprasad Sridhara, Yana Novikova,
Lori Pollock, and K. Vijay-Shanker. Amap: Automatically mining abbreviation
expansions in programs to enhance software maintenance tools. In International
Working Conference on Mining Software Repositories, pages 79‚Äì88, 2008.
[25] Matthew J. Howard, Samir Gupta, Lori Pollock, and K. Vijay-Shanker.
Automatically mining software-based, semantically-similar words from
comment-code mappings. In International Working Conference on Mining
Software Repositories, pages 377‚Äì386, 2013.[26] Nicholas Jalbert and Westley Weimer. Automated duplicate detection for bug
tracking systems. In IEEE International Conference on Dependable Systems and
Networks With FTCS and DCC , pages 52‚Äì61, 2008.
[27] Pooyan Jamshidi, Norbert Siegmund, Miguel Velez, Christian K√§stner, Akshay
Patel, and Yuvraj Agarwal. Transfer learning for performance modeling ofconfigurable systems: An exploratory analysis. In Proceedings of the 32nd
IEEE/ACM International Conference on Automated Software Engineering, pages
497‚Äì508. IEEE Press, 2017.
[28] Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and Shan Lu.
Understanding and detecting real-world performance bugs. In Proceedings of the
ACM SIGPLAN Conference on Programming Language Design and
Implementation, pages 77‚Äì88, 2012.
[29] Milan Jovic, Andrea Adamoli, and Matthias Hauswirth. Catch me if you can:
Performance bug detection in the wild. In Proceedings of the ACM SIGPLAN
International Conference on Object Oriented Programming Systems Languages and
Applications, pages 155‚Äì170, 2011.
[30]AndrewJKo,BradAMyers,andDuenHorngChau. Alinguisticanalysisofhow
people describe software problems. In Visual Languages and Human-Centric
Computing, 2006. VL/HCC 2006. IEEE Symposium on, pages 127‚Äì134, 2006.
[31]Vivek Nair, Tim Menzies, Norbert Siegmund, and Sven Apel. Using bad learners
to find good configurations. arXiv preprint arXiv:1702.05701, 2017.
[32] Changhai Nie, Huayao Wu, Xintao Niu, Fei-Ching Kuo, Hareton Leung, and
Charles J Colbourn. Combinatorial testing, random testing, and adaptive
random testing for detecting interaction triggered failures. Information and
Software Technology, 62:198‚Äì213, 2015.
[33] Adrian Nistor, Tian Jiang, and Lin Tan. Discovering, reporting, and fixing
performance bugs. In Proceedings of the International Conference on Mining
Software Repositories, pages 237‚Äì246, 2013.
[34] Adrian Nistor, Linhai Song, Darko Marinov, and Shan Lu. Toddler: Detecting
performance problems via similar memory-access patterns. In Proceedings of the
International Conference on Software Engineering, pages 562‚Äì571, 2013.
[35] Natural language toolkit, 2017. http://www.nltk.org/.
[36] Oswaldo Olivo, Isil Dillig, and Calvin Lin. Static detection of asymptotic
performance bugs in collection traversals. In Proceedings of the ACM SIGPLAN
Conference on Programming Language Design and Implementation, pages
369‚Äì378, 2015.
[37] Thomas J. Ostrand and Marc J. Balcer. The category-partition method for
specifying and generating fuctional tests. Communications of the ACM,
31(6):676‚Äì686, 1988.
[38] Annibale Panichella, Collin McMillan, Evan Moritz, Davide Palmieri, Rocco
Oliveto, Denys Poshyvanyk, and Andrea De Lucia. When and how using
structural information to improve ir-based traceability recovery. In European
Conference on Software Maintenance and Reengineering, pages 199‚Äì208, 2013.
[39] Gilles Perrouin, Sagar Sen, Jacques Klein, Benoit Baudry, and Yves Le Traon.
Automated and scalable t-wise test case generation strategies for software
product lines. In International Conference on Software Testing, Verification and
Validation (ICST), pages 459‚Äì468, 2010.
[40]Jay M Ponte and W Bruce Croft. A language modeling approach to information
retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on
Research and development in information retrieval, pages 275‚Äì281, 1998.
[41]Michael Pradel, Markus Huggler, and Thomas R. Gross. Performance regression
testing of concurrent classes. In Proceedings of the International Symposium on
Software Testing and Analysis, pages 13‚Äì25, 2014.
[42]Xiao Qu, Myra B. Cohen, and Gregg Rothermel. Configuration-aware regression
testing: An empirical study of sampling and prioritization. In ISSTA, pages
75‚Äì86, 2008.
[43] Atri Sarkar, Jianmei Guo, Norbert Siegmund, Sven Apel, and Krzysztof
Czarnecki. Cost-efficient sampling for performance prediction of configurablesystems (t). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM
International Conference on, pages 342‚Äì352. IEEE, 2015.
[44]
HinrichSch√ºtze,ChristopherDManning,andPrabhakarRaghavan. Introduction
to information retrieval, volume 39. Cambridge University Press, 2008.
[45] scikit-learn, 2017. http://scikit-learn.org/stable/.
[46] Du Shen, Qi Luo, Denys Poshyvanyk, and Mark Grechanik. Automating
performance bottleneck detection using search-based application profiling. In
Proceedings of the 2015 International Symposium on Software Testing and Analysis,
ISSTA 2015, pages 270‚Äì281, 2015.
[47] Wei Wen, Tingting Yu, and Jane Huffman Hayes. Colua: Automatically
predicting configuration bug reports and extracting configuration options. In
EEE 27th International Symposium on Software Reliability Engineering, pages
150‚Äì161, 2016.
[48] Alexander Wert, Jens Happe, and Lucia Happe. Supporting swift reaction:
Automatically uncovering performance problems by systematic experiments. In
Proceedings of the International Conference on Software Engineering, pages
552‚Äì561, 2013.
[49] X. Xia, D. Lo, W. Qiu, X. Wang, and B. Zhou. Automated configuration bug
report prediction using text mining. In Computer Software and Applications
Conference, pages 107‚Äì116, 2014.
27
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Xue Han, Tingting Yu, and David Lo
[50] Xusheng Xiao, Shi Han, Dongmei Zhang, and Tao Xie. Context-sensitive delta
inference for identifying workload-dependent performance bottlenecks. In
Proceedings of the International Symposium on Software Testing and Analysis,pages 90‚Äì100, 2013.
[51] Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. Normalized word embedding
and orthogonal transform for bilingual word translation. In Proceedings of the
2015 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 1006‚Äì1011,2015.
[52]
C. Yilmaz. Test case-aware combinatorial interaction testing. IEEE Transactions
on Software Engineering, 39:684‚Äì706, 2013.
[53] Cemal Yilmaz, Myra B. Cohen, and Adam Porter. Covering arrays for efficient
fault characterization in complex configuration spaces. TSE, 29(4), July 2004.[54] Zuoning Yin, Xiao Ma, Jing Zheng, Yuanyuan Zhou, Lakshmi N.
Bairavasundaram, and Shankar Pasupathy. An empirical study on configuration
errors in commercial and open source systems. In SOSP, pages 159‚Äì172, 2011.
[55] Shahed Zaman, Bram Adams, and Ahmed E. Hassan. A qualitative study on
performance bugs. In MSR, pages 199‚Äì208, 2012.
[56] Shahed Zaman, Bram Adams, and Ahmed E Hassan. A qualitative study on
performance bugs. In IEEE Working Conference on Mining Software Repositories
(MSR), pages 199‚Äì208, 2012.
[57]Zhiqiang Zhang, Jun Yan, Yong Zhao, and Jian Zhang. Generating combinatorial
test suite using combinatorial optimization. Journal of Systems and Software,
98:191‚Äì207, 2014.
28
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:53 UTC from IEEE Xplore.  Restrictions apply. 