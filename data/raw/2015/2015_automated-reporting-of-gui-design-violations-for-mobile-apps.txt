Automated Reporting of GUI Design Violations for Mobile Apps
Kevin Moran, Boyang Li, Carlos Bernal-Cárdenas, Dan Jelf, and Denys Poshyvanyk
College of William & Mary
Department of Computer Science
Williamsburg, VA, USA
{kpmoran,boyang,cebernal,dkjelf,denys}@cs.wm.edu
ABSTRACT
The inception of a mobile app often takes form of a mock-up of
the Graphical User Interface (GUI), represented as a static image
delineating the proper layout and style of GUI widgets that satisfy
requirements.Followingthisinitialmock-up,thedesignartifacts
arethenhandedofftodeveloperswhosegoalistoaccuratelyim-
plement these GUIs and the desired functionality in code. Given
thesizableabstractiongapbetweenmock-upsandcode,developers
often introduce mistakes related to the GUI that can negatively
impact an app’s success in highly competitive marketplaces. More-
over, such mistakes are common in the evolutionary context of
rapidly changing apps. This leads to the time-consuming and labo-
rioustaskofdesignteamsverifyingthateachscreenofanappwas
implemented according to intended design specifications.
Thispaperintroducesanovel,automatedapproachforverifying
whether the GUI of a mobile app was implemented according to its
intendeddesign.OurapproachresolvesGUI-relatedinformation
from both implemented apps and mock-ups and uses computer
visiontechniquestoidentifycommonerrorsintheimplementations
ofmobileGUIs.WeimplementedthisapproachforAndroidinatool
calledGvtand carriedout both a controlled empiricalevaluation
with open-source apps as well as an industrial evaluation with
designers and developers from Huawei. The results show that Gvt
solves an important, difficult, and highly practical problem with
remarkable efficiency and accuracy and is both useful and scalable
from the point of view of industrial designers and developers. The
tool is currently used by over one-thousand industrial designers &
developers at Huawei to improve the quality of their mobile apps.
CCS CONCEPTS
•Software and its engineering →Software design engineer-
ing;Requirements analysis ;
ACM Reference Format:
Kevin Moran, Boyang Li, Carlos Bernal-Cárdenas, Dan Jelf, and Denys
Poshyvanyk. 2018. Automated Reporting of GUI Design Violations for Mo-
bile Apps. In ICSE ’18:ICSE ’18: 40th InternationalConferenceon Software
Engineering , May 27-June 3, 2018, Gothenburg, Sweden. ACM, New York,
NY, USA, 11 pages. https://doi.org/10.1145/3180155.3180246
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s). Publication rights licensed to the
Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05 ...$15.00
https://doi.org/10.1145/3180155.31802461 INTRODUCTION
Intuitive, elegant graphical user interfaces (GUIs) embodying effec-
tive user experience (UX) and user interface (UI) design principles
are essential to the success of mobile apps. In fact, one may argue
thatthesedesignprinciplesarelargelyresponsibleforlaunching
themodernmobileplatformsthathavebecomesopopulartoday.
Apple Inc’s launch of the iPhone in 2007 revolutionized the mobile
handsetindustry(heavilyinfluencingderivativeplatformsinclud-
ing Android) and largely centered on an elegant, well-thought out
UXexperience,puttingmultitouchgesturesandanaturalGUIat
theforefrontoftheplatformexperience.Adecadelater,themost
successful mobile apps on today’s highly competitive app stores
(e.g.,GooglePlay[ 5]andApple’sAppStore[ 3])arethosethatem-
bracethisfocusoneaseofuse,andblendintuitiveuserexperiences
with beautiful interfaces. In fact, given the high number of apps in
today’smarketplacesthatperformremarkablysimilarfunctions[ 7],
the designand userexperience ofan appare oftendifferentiating
factors, leading to either success or failure [12].
Giventheimportanceofaproperuser interfaceanduserexpe-
rience for mobile apps, development usually begins with UI/UX
design experts creating highly detailed mock-ups of app screens
using one of several different prototyping techniques [ 25,44]. The
most popular of these techniques and the focus of this paper, isreferred to as mock-up driven development where a designer (or
group of designers) creates pixel perfect representations of app
UIsusing softwaresuch asSketch[ 10]or PhotoShop[ 1].Once the
designartifacts(or mock-ups )arecompleted,theyarehandedoff
todevelopmentteamswhoareresponsibleforimplementingthe
designsincodeforatargetplatform.Inorderforthedesignenvi-
sioned by the UI/UX experts (who carry domain knowledge that
front-end developers may lack) to be properly transferred to users,
an accurate translation of the mock-up to code is essential.
Yet,implementinganintuitiveandvisuallyappealingUIincode
iswell-knowntobeachallengingundertaking[ 37,39,46].Assuch,
manymobiledevelopmentplatformssuchasApple’sXcodeIDEand
Android Studio include powerful built-in GUI editors. Despite the
easeofusesuchtechnologiesareintendedtofacilitate,acontrolled
study has illustrated that such interface builders can be difficult to
operate,withuserspronetointroducingbugs[ 49].Becauseapps
under development are prone to errors in their GUIs, this typically
resultsinaniterativeworkflowwhereUI/UXteamswillfrequentlymanuallyaudit appimplementationsduringthedevelopmentcycle
andreportanyviolationstotheengineeringteamwhothenaims
tofixthem.Thisincrediblytimeconsumingback-and-forthprocess
is further complicated by several underlying challenges specific
tomobileappdevelopmentincluding:(i)continuouspressurefor
frequent releases [ 22,24], (ii) the need to address user reviews
1652018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Moran, B. Li, C. Bernal-Cárdenas, D. Jelf, and D. Poshyvanyk
quicklytoimproveappquality[ 19,20,40,41],(iii)frequentplatform
updates and API instability [ 15,27,28,33] including changes in
UI/UX design paradigms inducing the need for GUI re-designs
(e.g.,material design), and (iv) the need for custom components
and layouts to support complex design mock-ups. Thus, there is
a practical need for effective automated support to improve the
process of detecting and reporting design violations and providing
developers with more accurate and actionable information.
The difficulty that developers experience in creating effective
GUIs stems from the need to manually bridge a staggering abstrac-
tiongapthatinvolvesreasoningconciseandaccurateUIcodefrom
pixel-based graphical representations of GUIs. The GUI errors that
are introduced when attempting to bridge this gap are known in
literatureas presentationfailures .Presentationfailureshavebeen
definedinthecontextofwebapplicationsinpreviousworkas “a
discrepancy betweenthe actual appearanceof a webpage[or mobile
app screen] and its intended appearance" [32]. We take previous
innovative work that aims to detect presentation errors in web ap-
plications [ 18,31,32,42] as motivation to design equally effective
approaches in the domain of mobile apps. Presentation failures
are typically comprised of several visual symptoms or specific mis-
matches between visual facets ofthe intended GUI design and the
implementation of those GUI-components [ 32] in an app. These
visual symptoms can vary in type and frequency depending on the
domain(e.g.,webvs.mobile),andinthecontextofmock-updriven
development, we define them as design violations .
Inthispaper,wepresentanapproach,calledGvt( GuiVerification
sysTem), developed in close collaboration with Huawei. Our ap-
proach is capable of automated, precise reporting of the design
violations that induce presentation failures between an app mock-
upanditsimplementation.Ourtechniquedecodesthehierarchal
structure present in both mockups and dynamic representations
ofappGUIs,effectivelymatchingthecorrespondingcomponents.
Gvt then uses a combination of computer vision techniques to
accuratelydetectdesignviolations.Finally,Gvtconstructsareport
containing screenshots, links to static code information (if code is
provided),andprecisedescriptionsofdesignviolations. GVTwas
developedtobepracticalandscalable,wasbuiltinclosecollaboration
with the UI/UX teams at Huawei, and is currently in use by over
one-thousand designers and engineers at the company.
ToevaluatetheperformanceandusefulnessofGvtweconducted
threecomplementarystudies.First,weempiricallyvalidatedGvt’s
performance by measuring the precision and recall of detecting
synthetically injected design violations in popular open source
apps. Second, we conducted a user study to measure the usefulness
of our tool, comparing Gvt’s ability to detect and report design
violations to the ability of developers, while also measuring the
perceived utility of Gvt reports. Finally, to measure the applicabil-
ityof our approach in an industrial context, we present the results
of an industrial case study including: (i) findings from a survey
senttoindustrialdevelopersanddesignerswhouseGvtintheir
development workflow and (ii) semi-structured interviews with
bothdesignanddevelopmentteammanagersabouttheimpactof
thetool.Ourfindingsfromthis wide-ranging evaluationinclude
the following key points: (i) In our study using synthetic violations
Gvt is able to detect design violations with an overall precision
of 98% and recall of 96%; (ii) Gvt is able to outperform developerswith Android developmentexperience in identifyingdesign viola-
tions while taking less time; (iii) developers generally found Gvt’s
reports useful for quickly identifying different types of design vio-
lations; and (iv) Gvt had a meaningful impact on the design and
developmentofmobileappsforourindustrialpartner,contributing
to increased UI/UX quality.
Our paper contributions can be summarized as follows:
•Weformalizetheconceptsof presentationfailures anddesign
violations formock-updrivendevelopmentinthedomainof
mobileapps,andempiricallyderivecommontypesofdesign
violations in a study on an industrial dataset;
•We present a novel approach for detecting and reporting
theseviolationsembodiedinatoolcalledGvtthatuseshier-
archal representations of an app’s GUI and computer visiontechniques to detect and accurately report design violations;
•We conduct a wide-ranging evaluation of the Gvt studying
itsperformance ,usefulness , and industrial applicability ;
•Weincludeanonlineappendix[ 35]withexamplesofreports
generatedbyGvtandourevaluationdataset.Additionally,
we make the Gvt tool and code available upon request.
2 PROBLEM STATEMENT & ORIGIN
Inthissectionweformalizetheproblemofdetecting designviola-
tionsin GUIs of mobile apps and discuss the origin of the problem
rooted in industrial mobile app design & development.
2.1 Problem Statement
At a high level, our goal is to develop an automated approach
capableofdetecting,classifying,andaccuratelydescribing design
violations that exist for a single screen of a mobile app to help
developers resolve presentation failures more effectively. In this
section we formalize this scenario in order to allow for an accurate
descriptionandscopeofourproposedapproach.Whilethissection
focuses on concepts, Sec. 4 focuses on the implementation details.
2.1.1 GUI-Components & Screens. There are two main logi-
cal constructs that define the concept of the GUI of an app: GUI-
components (or GUI-widgets) and Screens.AGUI-component is a
discrete object with a set of attributes (such as size and locationamong others) associated with a particular Screenof an app. A
Screenis an invisible canvas of size corresponding to the physi-
calscreendimensionsofamobiledevice.Wedefinetwotypesof
screens,those createdbydesignersusing professional-gradetools
like Sketch, and those collected from implemented apps at runtime.
Each of these two types of Screens has an associated set of GUI-
components (or components ). Each set of components associated
with a screen is structured as a cumulative hierarchy comprising a
treestructure,startingwithasinglerootnode,wherethespatial
layout of parent always encompasses contained child components.
Definition 1:GUI-Component (GC) - Adiscreteobject GCwith
a corresponding set of attributes awhich can be represented as
a four-tuple in the form (<x-position,y-position>, <height,width>,
<text>, <image>) . Here the first four elements of the tuple describe
thelocationofthetopleftpointfortheboundingboxofthecom-
ponent,andtheheightandwidthattributesdescribethesizeofthe
boundingbox.Thetextattributecorrespondstotextdisplayedby
the component and the image attribute represents an image of the
component with bounds adhering to the first two attributes.
166
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. Automated Reporting of GUI Design Violations for Mobile Apps ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
A) App Mock-Up B) App Implementation
GUI-Component
Screen
Presentation Failure
with three DesignViolations (Color, Size, Location)
Presentation Failurewith one DesignViolation (Font)
Figure 1: Examples of Formal Definitions
Definition 2: Screen (S) - A canvas Swith a predefined height
andwidthcorrespondingtothephysicaldisplaydimensionsofa
smartphone or tablet. Each Screen contains a cumulative hierarchy
of components, which can be represented as a nested set such that:
S={GC1{GC2{GCi},GC3}} (1)
where each GChas a unique attribute tuple and the nested set can
beorderedineitherdepth-first(Exp.1)orinabreadth-firstmanner.
We are concerned with two specific types of screens: screens repre-
senting mock-ups of mobile apps Smand screens representing real
implementations of these apps, or Sr.
2.1.2 Design Violations & Presentation Failures. As described
earlier,design violations correspond to visual symptoms of pre-
sentationfailures ,ordifferencesbetweentheintendeddesignand
implementationofamobileappscreen.Presentationfailurescan
be made up of one or more design violations of different types.
Definition 3: Design Violation (DV) - As shown in Exp. 2, a
mismatchbetweentheattributetuplesoftwocorrespondingleaf-
level(i.e.,havingnodirectchildren)GUI-components GCm
iandGCr
j
oftwoscreens SmandSrimplyadesignviolation DVassociated
withthose components.Inthis definitionleafnodes correspond to
oneanotheriftheirlocationandsizeonascreen( i.e.,<x-position,y-
position>, <height,width> ) match within a given threshold. Equality
between leaf nodes is measured as a tighter matching threshold
acrossallattributes.Asweillustrateinthenextsection,inequalities
betweendifferentattributesintheassociatedtuplesofthe GCslead
to different types of design violations.
(GCm
i≈GCr
j)∧(GCm
i/nequalGCr
j)
=⇒DV∈{GCm
i,GCr
j}(2)
Definition 4: Presentation Failure (PF) - A set of one or more
DVsattributed to a set of corresponding GUI-components between
two screens SmandSr, as shown in Exp. 3. For instance, as shown
inFig.1,asinglesetofcorrespondingcomponentsmayhavedif-
ferences in both the <x,y>and<height,width> attributes leading to
two constituent design violations that induce a single presentation
failurePF.Thus,eachpresentationfailurebetweentwoScreens S
correspondstoatleastonemismatchbetweentheattributevectors
of two corresponding leaf node GUI-components GCimandGCir.
if{DV1,DV2, ...DVi}∈{GCm
i,GCr
j}
thenPF∈{Sm,Sr}(3)2.1.3 ProblemStatement. Giventhese definitions,theproblem
beingsolvedinthispaperisthefollowing:Giventwoscreens Sm
andSrcorresponding to the mock-up and implementation screens
of a mobile application, we aim to detect and describe the set of
presentation failures {PF1,PF2, ...PFi}∈{Sm,Sr}. Thus, we aim
to report all design violations on corresponding GCpairs:
{DV1,DV2, ...DVk}∈
{{GCm
i1,GCr
j1},{GCm
i2,GCr
j2}, ...{GCm
ix,GCr
jy}}(4)
2.2 Industrial Problem Origins
Atypicalindustrialmobiledevelopmentprocessincludesthefol-
lowing steps (as confirmed by our collaborators at Huawei): (i)
First a team of designers creates highly detailed mockups of anapp’s screens using the Sketch [
10] (or similar) prototyping soft-
ware. These mock-ups are typically “pixel-perfect" representations
of the app for a given screen dimension; (ii) The mock-ups are
then handed off to developers in the form of exported images with
designer added annotations stipulating spatial information and
constraints.Developersusethisinformationtoimplementrepre-
sentations of the GUIs for Android using a combination of Javaand
xml; (iii) Next, after the initial version of the app has been
implemented, compiled Android Application Package(s) ( i.e.,apks)
aresentbacktothedesignerswhotheninstalltheseappsontarget
devices, generate screenshots for the screens in question, and man-
ually search for discrepancies compared to the original mock-ups;
(iv) Once the set of violations are identified, these are communi-
catedbacktothedevelopersviatextualdescriptionsandannotated
screenshots at the cost of significant manual effort from the design
teams. Developers must then identify and resolve the DVsusing
thisinformation.Theprocessisoftenrepeatedinseveraliterations
causing substantial delays in the development process.
The goal of our work is to drastically improve this iterative
process by: (i) automating the identification of DVson the screens
of mobile apps - saving both the design and development teams
timeandeffort,and(ii)providinghighlyaccurateinformationto
thedevelopersregardingthese DVsintheformofdetailedreports-
in order to reduce their effort in resolving the problem.
3 DESIGN VIOLATIONS IN PRACTICE
Inordertogainabetterunderstandingofthetypesof DVsthatoccur
in mobile apps in practice, we conducted a study using a dataset
fromHuawei.Whiletheredoexistasmallcollectionoftaxonomies
related to visual GUI defects [ 23,26] and faults in mobile apps [ 21,
29],wechosetoconductacontextualizedstudywithourindustrial
partnerforthefollowingreasons:(i)existingtaxonomiesforvisual
GUI defects were not detailed enough, containing only general
faults(e.g.,“incorrectappearance”),(ii)existingfaulttaxonomies
for mobile apps either did not contain visual GUI faults or were
not complete, and (iii) we wanted to derive a contextualized DV
taxonomy for apps developed at Huawei. The findings from this
studyunderscoretheexistenceandimportanceoftheproblemthat
our approach aims to solve in this context. Due to an NDA, we are
notabletosharethedatasetorhighlightspecificexamples,inorder
to avoid revealing information about future products at Huawei.
However, we present aggregate results in this section.
167
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Moran, B. Li, C. Bernal-Cárdenas, D. Jelf, and D. Poshyvanyk
Layout  
Violations
39%Text 
Violations
23%Resource 
Violations
38%
Horizontal 
Translation
10%
Vertical 
Translation
16%H&V
Size
5%H 
Size
4%H&V
Trans
5%Font Style 
Violation
8%Font Color 
Violation
9%Incorrect Text 
Violation
6%Image Color 
Violation
10%Missing Component
12%
Extra 
Component
6%
Incorrect 
Image
7%
/g1Component Shape 1%/g2Component Type 1%!O
H = Horizontal 
V =  Vertical
Trans = Translation
Figure 2: Distribution of Different Types of Industrial DVs
3.1 Study Setting & Methodology
Thegoalof this study is to derive a taxonomy of the different
typesofDVsandexaminethedistributionofthesetypesinduced
during the mobile app development process. The contextof this
studyiscomprisedofasetof71representativemobileappmock-
up and implementation screen pairs from more than 12 different
internalapps,annotatedbydesignteamsfromourindustrialpartner
to highlight specific instances of resolved DVs . This set of screen
pairs was specifically selected by the industrial design team tobe representative both in terms of diversity and distribution of
violations that typically occur during the development process.
Inordertodevelopataxonomyanddistributionoftheviolations
presentinthisdataset,weimplementanopencodingmethodology
consistent with constructivist grounded theory [ 17]. Following the
advice of recent work within the SE community [ 45], we stipulate
our specific implementation of this type of grounded theory while
discussing our deviations from the methods in the literature. Wederived our implementation from the material discussed in [
17]
involvingthefollowingsteps:(i)establishingaresearchproblem
andquestions,(ii)data-collectionandinitialcoding,and(iii)focusedcoding.Weexcludedotherstepsdescribedin[
17],suchasmemoing
becausewewerebuildingataxonomyoflabels,andseekingnew
specificdataduetoourNDAlimitingthedatathatcouldbeshared.
The study addressed the following research question: What are the
differenttypesanddistributionsofGUIdesignviolationsthatoccur
during industrial mobile app development processes?
During the initial coding process, three of the authors were sent
the full set of 71 screen pairs and were asked to code four pieces of
informationforeachexample:(i)ageneralcategoryfortheviola-
tion,(ii)aspecificdescriptionoftheviolation,(iii)theseverityof
the violation (if applicable), and (iv) the Android GCtypes affected
(e.g.,button). Finally, we performed a second round of coding that
combined the concepts of focused and axial coding as described in
[17].Duringthisroundtwooftheauthorsmergedtheresponses
from all three types of coding information where at least two of
the three coders agreed. During this phase similar coding labelsweremerged( e.g.,“layoutviolation"vs.“spatialviolation"),conflicts
wereresolved,twoscreenpairswerediscardedduetoambiguity,
andcohesivecategoriesandsubcategorieswereformed.Theauthor
agreementforeachofthefourtypesoftagsisasfollows:(i)general
violationcategory(100%),(ii)specificviolationdescription(96%),
(iii) violation severity (100%), and (iv) affected GCtypes (84.5%).
3.2 Grounded Theory Study Results
Our study revealed three major categories of design violations,
each with several specific subtypes. We forgo detailed descriptions
and examples of violations due to space limitations, but provide
examples inour onlineappendix [ 35]. Thederived categoriesand
subcategoriesof DVs,andtheirdistributions,areillustratedinFig.2.
Overall 82 DVswere identified across the 71 unique screen pairs
consideredinourstudy.Themostprevalentcategoryof DVsinour
taxonomy are Layout Violations (≈40%), which concern either a
translationofacomponentinthe xorydirectionorachangeinthe
componentsize,withtranslationsbeingmorecommon.Thesecond
most prevalent category ( ≈36%) consists of Resource Violations ,
whichconcernmissingcomponents,extracomponents,colordif-
ferences, and image differences. Finally, about one-quarter ( ≈24%)
oftheseviolationsare TextViolations ,whichconcerndifferencesin
componentsthatdisplaytext.Weobservedthatviolationstypically
only surfaced for “leaf-level" components in the GUI hierarchy.
That is, violations typically only affected atomic components &
notcontainersorbackgrounds.Only5/82ofexaminedviolations
(≈6%) affected backgrounds or containers. Even in these few cases,
the violations also affected “leaf-level" components.
Thedifferenttypesofviolationscorrespondtodifferentinequali-
tiesbetweentheattributetuplesofcorrespondingGUI-components
defined in Sec. 2. This taxonomy shows that designers are charged
withidentifyingseveraldifferenttypesofdesignviolations,adaunt-
ing task, particularly for hundreds of screens across several apps.
4 THE GVTAPPROACH
4.1 Approach Overview
The workflow of Gvt (Fig. 3) proceeds in three stages: First in the
GUI-Collection Stage , GUI-related information from both mock-ups
andrunningappsiscollected;Next,inthe GUI-ComprehensionStage
leaf-level GCsareparsedfromthetreesandaKNN-basedalgorithm
isusedtomatchcorresponding GCsusingspatialinformation;Fi-
nally,inthe DesignViolationDetectionStage DVs aredetectedusing
acombinationofmethodsthatleveragespatial GCinformationand
computer vision techniques.
4.2 Stage 1: GUI Collection
4.2.1 Mock-UpGUICollection. SoftwareUI/UXdesignprofes-
sionals typically use professional-grade image editing software
(such as Photoshop[ 1] or Sketch[ 10]) to create their mock-ups. De-
signersemployedbyourindustrialpartnerutilizetheSketchdesign
software. Sketch is popular among mobile UI/UX designers due to
its simple but powerful features, ease of use, and large library of
extensions [ 11]. When using these tools designers often construct
graphical representations of smartphone applications by placing
objectsrepresenting GCs(whichwe refertoas mock-up GCs )ona
canvas(representingaScreen S)thatmatchesthetypicaldisplay
size of a target device. In order to capture information encoded
in these mock-ups we decided to leverage an export format that
168
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. Automated Reporting of GUI Design Violations for Mobile Apps ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
3   Design Violation Detection
2   GUI-Comprehension 1.1  Mock-Up GUI Collection
1.2  App Implementation GUI Collection
Resource Violation DetectorText Violation Detector
GVT GUI 
Collector
.apk
or
app 
src
Physical Device
or Emulator
Android 
UIAutomator
Android 
adb 
ScreencapAndroid
Application
Marketch 
Javascript 
File+
+
Export 
Screenshot
and 
Marketch 
FilesMock-Up GUI Hierarchy
Implementation GUI 
Hierarchy
UIX Parser
GUI 
Hierarchy
Construction
Detection of 
Corresponding 
Leaf Node 
PairsIdentiﬁed Corresponding 
Component Pairs
Difference Image
Layout Violation 
Detector
Sketch GUI-
Parser
Perceptual 
Image 
Differencing 
(PID)
UIX
xml FileChecks:
<image>
<text>
Color Quantization & Histogram Analysis
Checks:
<image>
+
Missing  & 
Extraneous 
Components
Color Quantization & 
Histogram Analysis
Javascript 
Parser
GUI
Hierarchy 
Construction
UIX 
Parser
Violation 
Manager
Checks:
<width>,<heigh>Checks:
<x>,<y>Design Violation Resolver
Difference 
Image 
Parser
Mismatched
ComponentPairs
≠
Spatial 
Comparator
Size 
Comparator
Component-Level
Perceptual Image Diff
Figure 3: Overview of GVT Workflow
wasalreadyinusebyourindustrialpartner,anopen-sourceSketch
extension called Marketch [ 6] that exports mock-ups as an html
page including a screenshot and JavaScript file.
Thus, as input from the mock-up, Gvt receives a screenshot (to
beusedlaterinthe DesignViolationDetectionPhase )andadirectory
containing the Marketch information. The JavaScript file contains
severalpiecesofinformationforeachmock-up GCincluding,(i)the
locationofthemock-up GConthecanvas,(ii)sizeofthebounding
box,and(iii)thetext/fontdisplayedbythemock-up GC(ifany).As
showninFigure3- 1.1,webuiltaparsertoreadthisinformation.
However,itshouldbenotedthatourapproachisnottightlycoupledto
SketchorMarketchfiles.1AftertheMarketchfileshavebeenparsed,
Gvt examines the extracted spatial information to build a GChier-
archy.Theresultcanbelogicallyrepresentedasarootedtreewhere
leaf nodes contain the atomic UI-elements with which a typical
user might interact. Non-leaf node components typically represent
containers, that form logical groupings of leaf node components
and other containers.In certain cases, ourapproximation of using
mock-up GCsto represent implementation GCsmay not hold. For
instance, an icon which should be represented as a single GCmay
consist of several mock-up GCsrepresenting parts of the icon. Gvt
handles such cases in the GUI-Comprehension stage.
4.2.2 Dynamic App GUI-Collection. In order to compare the
the mock-up of an app to its implementation Gvt must extract
GUI-related meta-data from a running Android app. Gvt is able to
useAndroid’s uiautomator framework[ 2]intendedforUItesting
to capture xmlfiles and screenshots for a target screen of an app
running on a physical device or emulator. Each uiautomator file
containsinformationrelatedtotheruntimeGUI-hierarchyofthe
target app, including the following attributes utilized by Gvt: (i)
The Android component type ( e.g., android.widget.ImageButton ),
(ii)thelocationonthescreen,(iii)thesizeoftheboundingbox,(iv)
textdisplayed,(v)adeveloperassigned id.Thehierarchalstructure
of components is encoded directly in the uiautomator file, and thus
webuiltaparsertoextractGUI-hierarchyusingthisinformation
directly (see Fig. 3- 1.2).
4.3 Stage 2: GUI Comprehension
In order for Gvt to find visual discrepancies between components
existing in the mock-up and implementation of an app, it must
1Similar information regarding mock-up GCscan be parsed from the htmlor Scalable
Vector Graphics ( .svg) format exported by other tools such as Photoshop[1].determine which components correspond to one another. Unfor-
tunately, the GUI-hierarchies parsed from both the Marketch, and
uiautomator filestend todifferdramatically dueto severalfactors,
making tree-based GCmatching difficult. First, since the hierarchy
constructedusingtheMarketchfilesisgeneratedusinginformation
from the Sketch mock-up of app, it is using information derived
from designers. While designers have tremendous expertise in con-
structing visual representations of apps, they typically do not take
thetimetoconstructprogrammatically-orientedgroupingsofcom-
ponents. Furthermore, designers are typically not aware of the
correct Android component types that should be attributed to dif-
ferentobjectsinamock-up.Second,the uiautomator representation
of theGUI-hierarchy containsthe runtimehierarchal structureof
GCsand correct GCtypes. This tree is typically far more com-
plex,containingseverallevelsofcontainersgrouping GCstogether,
which is required for the responsive layouts typical of mobile apps.
Toovercomethischallenge,Gvtinsteadformstwocollections
ofleaf-node components from both the mock-up and implementa-
tionGUI-hierarchies(Fig.3- 2),asthisinformationcanbeeasily
extracted. As we reported in Sec. 3, the vast majority of DVsaf-
fectsleaf-nodecomponents.Oncetheleafnodecomponentshave
been extracted from each hierarchy, GVT employs a K-Nearest-
Neighbors (KNN) algorithm utilizing a similarity function based
onthelocationandsizeofthe GCsinordertoperformmatching.
In this setting, an input leaf-node component from the mock-up
wouldbematchedagainstitclosest( e.g.,K=1)neighborfromthe
implementation based upon the following similarity function:
γ=(|xm−xr|+|ym−yr|+|wm−wr|+|hm−hr|)(5)
Whereγis a similarity score where smaller values represent closer
matches. The x,y,wandhvariables correspond to the x&ylo-
cation of the top and left-hand borders of the bounding box, and
theheightandwidthoftheboundingboxesforthemock-upand
implementation GCsrespectively. The result is a list of GCsthat
should logically correspond to one another ( corresponding GCs ).
It is possible that there exist instances of missing or extraneous
componentsbetweenthemock-upandimplementation.Toidentify
these cases, our KNN algorithm employs a GC-Matching Threshold
(MT). If the similarity score of the nearest neighbor match for a
giveninputmock-up GCexceedsthisthreshold,itisnotmatched
withanycomponent,andwillbereportedasa missingGC violation.
169
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Moran, B. Li, C. Bernal-Cárdenas, D. Jelf, and D. Poshyvanyk
Ifthereareunmatched GCsfromtheimplementation,theyarelater
reported as extraneous GC violations.
Also, there may be cases where a logical GCin the implemen-
tationisrepresentedassmallgroupofmock-up GCs.Gvtisable
to handle these cases using the similarity function outlined above.
Foreachmock-up GC,Gvtcheckswhethertheneighboring GCs
in the mockup are closer than the closest corresponding GCin
theimplementation.Ifthisisthecase,theyaremerged,withthe
process repeating until a logical GUI-component is represented.
4.4 Stage 3: Design Violation Detection
In theDesign Violation Detection stage of the Gvt workflow, the
approach uses a combination of computer vision techniques and
heuristiccheckinginordertoeffectivelydetectthedifferentcate-
gories of DVsderived in our taxonomy presented in Section 3.
4.4.1 Perceptual Image Differencing. In order to determine cor-
responding GCswith visual discrepancies Gvt uses a technique
called Perceptual Image Differencing (PID) [ 48] that operates upon
the mock-up and implementation screenshots. PID utilizes a model
of the human visual system to compare two images and detect
visual differences, and has been used to successfully identify visual
discrepancies in web applications in previous work [ 31,32]. We
use this algorithm in conjunction with the GCinformation derived
in the previous steps of Gvt to achieve accurate violation detec-
tion. For a full description of the algorithm, we refer readers to
[48]. The PID algorithm uses several adjustable parameters includ-
ing:Fwhichcorrespondstothevisualfieldofviewindegrees, L
which indicates the luminance or brightness of the image, and C
which adjusts sensitivity to color differences. The values used in
our implementation are stipulated in Section 4.5.
The output of the PID algorithm is a single difference image (Fig.
3-3) containing differencepixels ,which arepixels consideredto be
perceptuallydifferentbetweenthetwoimages.Afterprocessingthe
differenceimagegeneratedbyPID,Gvtextractstheimplementa-
tionboundingboxforeachcorrespondingpairof GCs,andoverlays
the box on top of the generated difference image. It then calculates
the number of difference pixels contained within the bounding box
where higher numbers of difference pixels indicate potential visual
discrepancies.Thus,Gvtcollectsall“suspicious" GCpairswitha%
of difference pixels higher than a Difference Threshold DT. This set
of suspicious components is then passed to the Violation Manager
(Fig. 3-3 ) so that specific instances of DVscan be detected.
4.4.2 Detecting Layout Violations. The first general category of
DVsthatGvtdetectsare LayoutViolations .Accordingthetaxonomy
derived in Sec. 3 there are six specific layout DVcategories that
relatetotwocomponentproperties:(i)screenlocation( i.e.,<x,y>
position)and(ii)size( i.e.,<h,w> oftheGCboundingbox).Gvtfirst
checksforthethreetypesoftranslation DVsutilizingaheuristicthat
measuresthedistancefromthetopandleft-handedgesofmatchedcomponents.Ifthedifferencebetweenthecomponentsineitherthe
xorydimensionisgreaterthana LayoutThreshold (LT),thenthese
componentsarereportedasa LayoutDV .Usingthe LTavoidstrivial
location discrepancieswithin design tolerances being reported as
violations, and can be set by a designer or developer using the tool.
Whendetectingthethreetypesofsize DVsinthederiveddesign
violationtaxonomy,Gvtutilizesaheuristicthatcomparesthewidth
and height of the bounding boxes of corresponding components. Ifthe width or height of the bounding boxes differ by more than the
LT, then a layout violation is reported.
4.4.3 Detecting Text Violations. The next general type of DV
thatGvtdetectsare TextViolations ,ofwhichtherearethreespe-
cific types: (i) Font Color, (ii) Font Style, and (iii) Incorrect Text
Content. These detection strategies are only applied to pairs of
text-based components as determined by uiautomator information.
To detect font color violations, Gvt extracts cropped images for
each pair of suspicious text components by cropping the mock-up
andimplementation screenshotsaccording tothe component’sre-
spective bounding boxes. Next, Color Quantization (CQ) is applied
to accumulate instances of all unique RGB values expressed in the
component-specificimages.Thisquantizationinformationisthen
usedtoconstructa ColorHistogram(CH) (Fig.3-3).Gvtcomputes
the normalized Euclidean distance between the extracted Color
Histograms for the corresponding GCpairs, and if the Histograms
donotmatchwithina ColorThreshold(CT) thenaFont-Color DV
isreportedandthetop-3colors(i.e,centroids)fromeachCHare
recordedintheGvtreport.Likewise,ifthecolorsdomatch,thenthe
PIDdiscrepancyidentifiedearlierisduetotheFont-Stylechanging
(provided no existing layout DVs), and thus a Font-Style Violation
is reported. Finally, to detect incorrect text content, Gvt utilizes
the textual information, preprocessed to remove whitespace andnormalize letter cases, and performs a string comparison. If the
stringsdonotmatch,thenan IncorrectTextContent DV isreported.
4.4.4 DetectingResourceViolations. Gvtisabletodetectthe
followingresource DVs:(i)missingcomponent,(ii)extraneouscom-
ponent, (iii)image color,(iv) incorrectimages, and(v) component
shape.Thedetectionanddistinctionbetween IncorrectImage DVs
andImage Color DVs requires an analysis that combines two differ-
ent computer vision techniques. To perform this analysis, cropped
images from the mock-up and implementation screenshots accord-
ing to corresponding GCsrespective bounding boxes are extracted.
Thegoalofthisanalysisistodeterminewhenthecontentofimage-
basedGCsdiffer, as opposed to only the colors of the GCsdiffering.
To accomplish this, Gvt leverages PID applied to extracted GC
images converted to a binary color space ( B-PID) in order to detect
differences in contentand CQ and CH analysis to determine differ-
encesincolor(Sec.4.4.3).ToperformtheB-PIDprocedure,cropped
GCimagesareconvertedtoabinarycolorspacebyextractingpixel
intensities, and then applying a binary transformation to the inten-
sity values ( e.g.,converting the images to intensity independent
black & white). Then PID is run on the color-neutral version of
these images. If the images differ by more than an Image Difference
Threshold (IDT),then an IncorrectImage DV (whichencompasses
theComponent Shape DV ) is reported. If the component passes the
binary PID check, then Gvt utilizes the same CQ and CH process-
ingtechniquedescribedabovetodetect imagecolor DVs .Missing
and extraneous components are detected as described in Sec. 4.3
4.4.5 GeneratingViolationReports. Inordertoprovidedevelop-
ersanddesignerswitheffectiveinformationregardingthedetected
DVs, Gvt generates an htmlreport that, for each detected viola-
tion contains the following: (i) a natural language description of
thedesign violation(s),(ii)anannotatedscreenshot oftheappim-
plementation,withtheaffectedGUI-componenthighlighted,(iii)
cropped screenshotsof the affected GCsfrom both the design and
170
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. Automated Reporting of GUI Design Violations for Mobile Apps ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
implementationscreenshots,(iv)linksto affected linesofapplica-
tionsourcecode,(v)colorinformationextractedfromtheCHfor
GCsidentified to have color mismatches, and (vi) the difference
image generated by PID. The source code links are generated by
matching the idsextracted fromthe uiautomator information back
to their declarationsin the layout xmlfiles in thesource code ( e.g.,
those located in the /res/directory of an app’s source code). We
provide examples of generated reports in our online appendix [ 35].
4.5 Implementation & Industrial Collaboration
OurimplementationofGvtwasdevelopedinJavawithaSwing
GUI. In addition to running the Gvt analysis the tool executable
allows for one-click capture of uiautomator files and screenshots
from a connected device or emulator. Several acceptance tests of
mock-up/implementation screen pairs with pre-existing violations
fromappsunderdevelopmentwithinourindustrialpartnerwere
usedtoguidethedevelopmentofthetool.12Periodicreleasesof
binariesforbothWindowsandMacweremadetodeploythetoolto
designers and developers within the company. The authors of this
paperheldregularbi-weeklymeetingswithmembersofthedesign
and development teams to plan features and collect feedback.
Using the acceptance tests and feedback from our collaborators
wetunedthevariousthresholdsandparametersofthetoolforbest
performance.ThePIDalgorithmsettingsweretunedforsensitivitytocapturesubtlevisualinconsistencieswhicharethenlaterfiltered
through additional CV techniques: Fwas set to 45◦,Lwas set to
100cdm2, andCwas set to 1. The GC-Matching Threshold (MC)
was set to 1 /8ththe screen width of a target device; the DTfor
determining suspicious GCswas set to 20%; The LTwas set to 5
pixels (based on designer preference); the CTwhich determines
thedegreetowhichcolorsmustmatchforcolor-based DVswasset
to85%;andfinally,the IDTwassetto20%.Gvtallowsforauser
tochangethesesettingsifdesired,additionallyusersarecapable
of defining areas of dynamic content ( e.g.,loaded from network
activity), which should be ignored by the Gvt analysis.
5 DESIGN OF THE EXPERIMENTS
To evaluate Gvt’s performance ,usefulness andapplicability ,w e
performthreecomplimentarystudiesansweringthefollowingRQs:
•RQ1:How well does Gvtperform in terms of detecting and
classifying design violations?
•RQ2:What utility can Gvtprovide from the viewpoint of
Android developers?
•RQ3:Whatistheindustrialapplicabilityof Gvtintermsof
improving the mobile application development workflow?
RQ1andRQ2focusonquantitativelymeasuringtheperformance
ofGvtandtheutilityitprovidestodevelopersthroughacontrolled
empirical and a user study respectively. RQ 3reports the results of
a survey and semi-structured interviews with our collaborators
aimed at investigating the industrial applicability of Gvt.
5.1 Study 1: GvtEffectiveness & Performance
ThegoalofthefirststudyistoquantitativelymeasureGvtinterms
of its precision and recall in both detecting and classifying DVs.
5.1.1 Study Context. To carry out a controlled quantitative
study, we manually reverse engineered Sketch mockups for ten
screensforeightofthemostpopularappsonGooglePlay.Tode-
rivethisset,wedownloadedthetop-10appsfromeachcategoryonthe Google-Play store removing the various categories correspond-
ing to games (as these have non-standard GUI-components that
Gvtdoesnotsupport).Wethenrandomlysampledoneappfrom
eachoftheremaining33 categories,eliminatingduplicates(since
apps can belong to more than one category). We then manuallycollected screenshots and
uiautomator files from two screens for
each application using a Nexus 5, attempting to capture the “main”
screenthatauserwouldtypicallyinteractwith,andonesecondary
screen.Usingthe uiautomator files,wegeneratedcroppedscreen-
shotsofalltheleafnodescomponentsforeachscreenoftheapp.
Fromthesewewereablegenerate10screensfrom8applications
that successfully ran through Gvt without any reported violations.
5.1.2 SyntheticDVInjection. Withasetofcorrectmock-upscor-
responding to implementation screens in an app, we needed a suit-
ablemethodtointroduce DVsintooursubjects.Tothisend,wecon-
structed a synthetic DV injection tool that modifies the uiautomator
xmlfilesandcorrespondingscreenshotsinordertointroducede-
sign violations from our taxonomy presented in Sec. 3. The tool
iscomposedof twocomponents:(i) an XMLParser thatreads and
extractscomponentsfromthescreen,then(ii)a ViolationGenerator
that randomly selects components and injects synthetic violations.
We implemented injection for the following types of DVs:
LocationViolation: Thecomponentismovedeitherhorizontally,
vertically,orinbothdirectionswithinthesamecontainer.However,
themaximumdistancefromtheoriginalpointislimitedbyaquarter
ofthewidthofthescreensize.Thiswasbasedontheseverityof
LayoutViolationsinourstudydescribedinSection3.Inorderto
generatetheimagewecroppedthecomponentandmovedittothenewlocationreplacingalltheoriginalpixelsbythemostprominent
color from the surroundings in the original location.SizeViolation:
The component size either increases or decreases
by20%oftheoriginalsize.Forinstanceswherethecomponentsize
decreases,wereplacedallthepixelsbythemostprominentcolor
from the surroundings of the original size.Missing Component Violation:
This violation removes a leaf
component from the screen, replacing the original pixels by the
most prominent color from the surrounding background.Image Violation:
We perturb 40% of the pixels in an image by
randomly generating an RGB value for the pixels affected.Image Color Violation:
This rule perturbs the color of an image
by shifting the hue of image colors by 30°.Component Color Violation:
Thisusesthesameprocessasfor
Image Color Violations but we change the color by 180°.
Font Violation: This violation randomly selects a font from the
set of:Arial,Comic Sans MS ,Courier,Roboto,o rTimes Roman and
applies it to a TextView component.
FontColorViolation: changes the text color of a TextView com-
ponent. We extracted the text color using CH analysis, then we
changedthecolorusingsamestrategyasfor ImageColorViolations .
5.1.3 StudyMethodology. Ininjectingthesyntheticfaults,we
took several measures to simulate the creation of realistic faults.
First, we delineated 200 different types of design violations accord-
ingtothedistributiondefinedinour DVtaxonomyinSec.3.We
then created a pool of 100 screens by creating random copies of
the both the uiautomator xml files and screenshots from our initial
set of 10 screens. We then used the synthetic DV injection tool to
seedfaultsintothepoolof100screensaccordingtothefollowing
171
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Moran, B. Li, C. Bernal-Cárdenas, D. Jelf, and D. Poshyvanyk
criteria: (i) No screen can contain more than 3 injected DVs, (ii)
eachGCshouldhaveamaximumof1 DVinjected,and(iii)Each
screenmusthaveatleast1injected DV.Afterthe DVswereseeded,
eachofthe100screensand200 DVsweremanuallyinspectedfor
correctness. Due to the random nature of the tool, a small num-
beroferroneous DVswereexcludedandregeneratedduringthis
process (e.g.,color perturbed to perceptually similar color.). The
breakdownofinjected DVsisshowninFigure4,andthefulldataset
with description is included in our online appendix [35].
Oncethefinalsetofscreenswithinjectedviolationswasderived,
we ran Gvt across these subjects and measured four metrics: (i)
detectionprecision( DP),(ii)classificationprecision( CP),(iii)recall
(R), and (iv) execution time per screen ( ET). We make a distinction
betweendetectionandclassificationinourdatasetbecauseitispos-
siblethatGvtiscapableofdetecting,butmisclassifyingaparticular
DV(e.g.,animagecolorDV misclassifiedasan incorrectimageDV ).
DP,CPandRweremeasuredaccordingtothefollowingformulas:
DP,CP=Tp
Tp+FpR=Tp
Tp+Fn(6)
where for DP,Tprepresent injected design violations that were
detected, and for CP,Tprepresents injected violations that were
bothdetectedandclassifiedcorrectly.Ineachcase Fpcorrespond
todetected DVsthatwereeithernotinjectedormisclassified.For
Recall,Tprepresentsinjectedviolationsthatwerecorrectlydetected
andFnrepresents injected violations that were not detected. To
collectthesemeasures,twoauthorsmanuallyexaminedthereports
from Gvtin order to collect the metrics.
5.2 Study 2: GvtUtility
Since the ultimate goal of an approach like Gvt is to improve
the workflow of developers, the goalof this second study is to
measure the utility ( i.e.,benefit) that Gvt provides to developers
by investigating two phenomena: (i) The accuracy and effort of
developers in detecting and classifying DVs, and (ii) the perceived
utility of Gvt reports in helping to identify and resolve DVs.
5.2.1 Study Context. We randomly derived two sets of screens
to investigate the two phenomena outlined above. First, we ran-
domlysampledtwomutuallyexclusivesetsof25,and20screens
respectively from the 100 used in Study 1, ensuring at least one
instanceofeachtypeof DVwasincludedintheset.Thisresultedin
bothsetsofscreenscontaining40designviolationsintotal.Thecor-rectmockupscreenshotcorrespondingtoeachscreensampledfrom
thestudywerealsoextracted,creatingpairsof“correct"mockup
and“incorrect"implementationscreenshots.10participantswith
atleast5yearsofAndroiddevelopmentexperiencewerecontacted
via email to participate in the survey.
5.2.2 Study Methodology. We created an online survey with
four sections. In the first section, participants were given back-
ground information regarding the definition of DVs, and the differ-
ent types of DVsderived in our taxonomy. In the second section,
participants were asked about demographic information such as
programming experience and education level. In the third section,
eachparticipantwasexposedto5mock-up/implementationscreen
pairs (displayed side by side on the survey web page) and asked to
identify any observed design violations. Descriptions of the DVs
were given at the top of this page for reference. For each screen
pair, participants were presented with a dropdown menu to select80859095100
Font Color(18) Font Style(18)Image Color(22)Image(16)
Missing GC(26)Size(18)
Text Content(14)Translation(68)
DV TypePercentageMetric
CPDPR
Figure 4: Stud y 1 - Detection Precision ( DP), Classification
Precision ( CP), and Recall ( R)
a type for an observed DV, and a text field to describe the error
in more detail. For each participant, one of the 5 mock-up screens
was a control, containing no injected violations. The 25 screens
wereassignedtoparticipantssuchthateachscreenwasobserved
by two participants and the order of the screens presented to each
participantwasrandomizedtoavoidbias.Tomeasuretheeffective-
nessofparticipantsindetectinganddescribing DVs,weleverage
theDP,CPandRmetrics introducedin Study 1. Inthe fourth sec-
tion,participantswerepresentedwithtwoscreenpairsfromthe
second set of 20 sampled from the user study, as well as the Gvt
reportsforthesescreens.Participantswerethenaskedtoanswer
5user-preferences (UP) and5user experience(UX) questionsabout
these reports which are presented in the following section. The UP
questions were developed according to the user experience hon-
eycomboriginallydevelopedbyMorville[ 36]andwereposedto
participantsasfreeformtextentryquestions.Weforgoadiscussion
of the free-form question responses due to space limitations, but
we offer full anonymized participant responses in our online ap-
pendix [35]. We derived the Likert scale-based UXquestions using
the SUS usability scale by Brooke [16].
5.3 Study 3: Industrial Applicability of Gvt
Thegoalof this final study is determine industrial applicability of
Gvt. To investigate this, we worked with Huawei to collect two
sources of information: (i) the results of a survey sent to designers
anddeveloperswhousedGvtintheirdailydevelopment/design
workflow, and (ii) semi-structured interviews with both design and
development managers whose teams have adopted the use of Gvt.
5.3.1 StudyContext&Methodology. Wecreatedasurveyposing
questionsrelatedtothe applicability ofGvttoindustrialdesigners
and developers. These questions are shown in Fig. 7. The semi-
structured interviews were conducted in Chinese, recorded, and
thenlatertranslated.Duringtheinterview,managerswereasked
torespondtofourquestionsrelatedtothe impactandperformance
ofthetoolinpractice.Weincludediscussionsoftheresponsesin
Section 6 and stipulate full questions in our appendix.
6 EMPIRICAL RESULTS
6.1 Study 1 Results: GVT Performance
TheresultsofStudy1,areshowninFigure4.Thisfigureshowsthe
averageDP,CP,andRforeachtypeofseededviolationoverthe200
seededfaultsandthenumberoffaultsseededintoeachcategory
(followingthedistributionsofourderivedtaxonomy)areshownon
thex-axis.Overall,theseresultsareextremelyencouraging,with
theoverall DPachieving99 .4%,theaverage CPbeing98 .4%,and
172
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. Automated Reporting of GUI Design Violations for Mobile Apps ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
CPDPR
0.00 0.25 0.50 0.75 1.00
Figure 5: Stud y2-D e v eloper CP, DP, and R
I found these reports unnecessarily complexI found these reports very cumbersome to read.I think that I would like to use these reports 
frequently for Identifying Presentation IssuesI thought these reports were very useful for 
accurately identifying Presentation ErrorsThese reports are easy to read/understand
SDD N ASA
Figure6:Study2-UXQuestionResponses.SD=StronglyDis-
agree, D=Disagree, N=Neutral, A=Agree, SA=Strongly Agree
theaverage Rreaching96 .5%.ThisillustratesthatGvtiscapable
ofdetectingseededfaultsdesignedtoemulateboththetypeand
distributionof DVsencounteredinindustrialsettings.WhileGvt
achievedatleast85%precisionforeachtypeof seeded DV,itper-
formed worse on some types of violations compared to others. For
instance,Gvtsawitslowestprecisionvaluesforthe Font-Style and
Font-Color violations, typically dueto the fact that the magnitude
ofperturbationfor thecolororfont typewasnotlargeenough to
surpasstheColororImageDifferenceThresholds( CT&IDT).Gvt
took 36.8 mins to process and generate reports for the set of 100
screenswithinjected DVs,or22 secperscreenpair.This execution
cost was generally acceptable by our industrial collaborators.
6.2 Study 2 Results: GVT Utility
TheDP,CPandRresults, representing the Android developers
ability to correctly detect and classify DVsis shown in Figure 5
asbox-plotsacrossall10participants.Herewefound CP=DP,as
when a user misclassified violations, they also did not detect them.
As this figure shows, the Android developers generally performed
muchworsecomparedtoGvtachievinganaverage CPofunder
≈60% and an average Rof≈50%. The sources of this performance
loss for the study participants compared to Gvt was fourfold: (i)
participants tended to report minor, acceptable differences in fonts
across the examples (despite the instructions clearly stating not
toreportsuchviolations);(ii)userstendedtoattributemorethan
oneDVtoasinglecomponent,specificallyfor fontstyle andfont
colorviolationsdespiteinstructionstoreportonlyone;(iii)users
tended to misclassify DVsbased on the provided categories ( e.g.,
classifying a layout DV f o raT e x t GCas anincorrecttext DV ), and
(iv)participantsmissedreportingmanyoftheinjected DVs,lead-
ing to the low recall numbers. These results indicate that, at the
very least, developers can struggle to both detect and classify DVs
betweenmock-upandimplementationscreenpairs,signalingthe
needforanautomatedsystemtocheckfor DVsbeforeimplemented
apps are sent to a UI/UX team for auditing. This result confirms
the notion that developers may not be as sensitive to small DVs
in the GUI as the designers who created the GUI specifications.
Furthermore,thisfindingisnotable,becauseaspartoftheiterative
processofresolvingdesignviolations,designersmustcommunicate
to developers DVsand developers must recognize and understand
theseDVsinordertoproperlyresolvethem.Thisprocessisoften
complicated due to ambiguous descriptions of DVsfrom designersThe GVT allowed for better transfer of the 
       design from mock−ups to the implementation of the appThe GVT has helped you to reduce the time 
       required for verifying design violations.The GVT is able to accurately report existing 
       design violations in production−quality applicationsThe GVT tool helped my team (design/implementation) 
       communicate with other teams (implementation/design) 
       regarding GUI design violationsUsing the GUI−Verification tool (GVT) 
       helped to improve the quality of mobile 
       applications produced by industrial partner
SDD N ASA
Figure7:Stud y3-ApplicabilityQuestions.SD=StronglyDis-
agree, D=Disagree, N=Neutral, A=Agree, SA=Strongly Agree
todevelopers,ordevelopersdisagreeingwithdesignersovertheex-istenceortypeofa
DV.Incontrasttothisfragmentedprocess,Gvt
provides clear, unambiguous reports that facilitate communication
between designers and developers.
Figure 6 illustrates the responses to the likert based UX ques-
tions,andtheresultsarequiteencouraging.Ingeneral,participants
found that the reports from Gvt were easy to read, useful for iden-
tifyingDVsandindicatedthattheywouldliketousethereportsfor
identifying DVs.Participantsalsoindicatedthatthereportswere
not unnecessarily complex or difficult to read. We asked the partic-
ipantsabouttheirpreferencesfortheGvtreportsaswell,asking
aboutthemostandleastusefulinformationinthereports. Every
singleparticipantindicatedthatthehighlightedannotationsonthe
screenshotsinthereportwerethemostusefulelement.Whereas
most users tended to dislike the PID output included at the bottom
of the report, citing this information as difficult to comprehend.
6.3 Study 3 Results: Industrial Applicability
The results for the applicability questions asked to 20 designers
and developers who use Gvt in their daily activities is shown in
Figure7.Apositiveoutcomeforeachofthesestatementscorrelates
to responses indicating that developers “agree” or “strongly agree”.
Theresultsofthisstudyindicateaweakagreementofdevelopers
forthesestatements,indicatingthatwhileGvtisgenerallyappli-
cable, there are some drawbacks that prevented developers anddesigners from giving the tool unequivocal support. We explore
these drawbacks by conducting semi-structured interviews.
In conducting the interviews, one of the authors asked the ques-
tionspresentedinFigure7to3managers(2fromUI/UXteamsand
1 from a Front-End development team). When asked whether Gvt
contributed to an increased quality of mobile applications at the
company,allthree managerstendedto agreethatthis wasthecase.Forinstance,oneofthedesignmanagersstated,
“Certainlyyes.The
tool is the industry’s first" and the other designer manager added,
“When the page is more complicated, the tool is more helpful" .
When asked about the overall performance and accuracy of the
tool in detecting DVs, the manager fromthe implementation team
admittedthatthecurrentdetectionperformanceofthetoolisgood,
but suggested that dynamic detection of some components may
improveit,stating, “[DVs]canbedetectedprettywell...[butthetool
is] not very flexible. For example, a switch component in the designis open, but the switch is off in the implementation"
. He suggested
that properly handling cases such as this would make the tool
more useful from a developers perspective. One of the design team
managers held a similar view stating that, “Currently, most errors
arelayouterrors,sotoolisaccurate.Staticcomponentsarebasicallydetected, [but] maybe the next extension should focus on dynamic
173
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden K. Moran, B. Li, C. Bernal-Cárdenas, D. Jelf, and D. Poshyvanyk
components." WhilethecurrentversionoftheGvtallowsforthe
exclusionofregionswithdynamiccomponents,itisclearthatboth
design and development teams would appreciate proper detection
ofDVsfordynamiccomponents.Additionally,twoofthemanagers
commented on the “ rigidity” of the Gvt’s current interface, and
explainedthatamorestreamlinedUIwouldhelpimproveitsutility.
WhenaskedaboutwhetherGvtimprovedcommunicationbe-
tweenthedesignanddevelopmentteams,thedevelopmentteam
manager felt that while the tool has not improved communica-
tion yet, it did have the potential to do so, “At present there is no
[improvement] but certainly there is the potential possibility." The
design managers generally stated that the tool has helped with
communication,particularlyinclarifyingsubtle DVsthatmayhave
caused arguments between teams in the past, “If you consider the
time savings on discussion and arguments between the two teams,
this tool saves us a lot of time" . Another designer indicated that the
toolishelpfulatdescribing DVstodeveloperswhomaynotbeable
torecognizethemwiththenakedeye “We found that the tool can
indeeddetectsomethingthatthenakedeyecannot" .Whilethereare
certainlyfurtherrefinementsthatcanbemadetoGvt,itisclear
thatthetoolhasbeguntohaveapositiveimpactofthedevelopmentof mobile apps, andas the tool evolves withinthe company, should
allow for continued improvements in quality and time saved.
7 LIMITATIONS & THREATS TO VALIDITY
Limitations :WhilewehaveillustratedthatGvtisapplicableinan
industrial setting, the tool is not without its limitations. Currently,
the tool imposes lightweight restrictions on designers creatingSketch mock-ups, chief among these being the requirement that
boundingboxesofcomponentsdonotoverlap.Currently,Gvtwill
trytoresolvesuchcasesduringthe GUI-Comprehensionstage using
an Intersection over union (IOU) metric.
Internal Validity : While deriving the taxonomy of DVs, mistakes
inclassificationarisingfromsubjectivenessmayhaveintroduced
unexpectedcoding.Tomitigatethisthreatwefollowedasetmethod-
ology, merged coding results, and performed conflict resolution.Construct Validity
: In our initial study (Sec. 3), a threat to con-
structvalidityarisesintheformofthemannerinwhichcoderswere
exposed to presentation failures. To mitigate this threat, designers
from our industrial partner manually annotated the screen pairs
in orderto clearlyillustrate theaffected GCsonthe screen. Inour
evaluationofGvtthreatsarisefromourmethodof DVinjection
usingthe syntheticfaultinjectiontool .However,wedesignedthis
tool to inject faults based upon both the type and distribution of
faults from our DVtaxonomy to mitigate this threat.
ExternalValidity :Inourinitialstudyrelatedtothe DVtaxonomy,
we utilized a dataset from a single (albeit large) company with
examplesacrossseveraldifferentapplicationsandscreens.Thereisthepotentialthatthismaynotgeneralizetootherindustrialmobileapplicationdevelopmentenvironmentsandplatformsormobileapp
developmentingeneral.Howevergiventherelativelyconsistent
design paradigms of mobile apps, we expect the categories andthe sub-categories within the taxonomy to hold, although it ispossible that the distribution across these categories may vary
acrossapplicationdevelopmentfordifferentdomains.InStudy3
we surveyed employees at a single (though large) company, and
findings may differ in similar studies at other companies.8 RELATED WORK
WebPresentationFailures : The work most closely related to our
approachareapproachesthataimatdetecting,classifyingandfixing
presentationfailuresinwebapplications[ 30–32,42].Incomparison
to these approaches, Gvt also performs detection and localization
of presentation failures, but is the first to do so for mobile apps.
In addition to the engineering challenges associated with building
anapproachtodetectpresentationfailuresinthemobiledomain
(e.g.,collection and processing of GUI-related data) Gvt is the first
approach to leverage metadata from software mock-up artifacts
(e.g.,Marketch) to perform GCmatching based upon the spatial
informationcollectedfrombothmock-upsanddynamicapplicationscreens, allowing for precisedetectionof the different types of
DVs
delineatedinourindustrial DVtaxonomy.Gvtisalsothefirstto
applytheprocessesofCQ,CHanalysis,andB-PIDtowarddetecting
differences in the content and color of icons and images displayed
in mobile apps.Gvt also explicitly identifies and reports different
faulty properties (such as location, ).Cross Browser Testing
: Approaches for XBT (or cross browser
testing) by Roy Choudhry et. al.[18,42,43] examine and automati-
callyreportdifferencesinwebpagesrenderedinmultiplebrowsers.
These approaches are currently not directly applicable to mock-up
driven development for mobile apps.VisualGUITesting
:AconceptknownasVisualGUITesting(VGT)
aimstotestcertainvisualaspectsofasoftwareapplication’sGUI
as well as the underlying functional properties. To accomplish this
visual GUI testing usually executes actions on a target applications
in order to exercise app functionality [ 13,14,23,38]. In contrast
to these approaches, Gvt is designed to apply to mobile-specific
DVs, is tailored for the mock-up driven development practice, and
is aimedonlyat verifying visual properties of a mobile app’s GUI.
Other Approaches : There are other approaches and techniques
that related to identifying problems or differences with GUIs of
mobileapps.Xie etal.introducedGUIDE[ 47],atoolforGUIdiffer-
encingbetweensuccessivereleasesofGUIsforanappbymatching
componentsbetweenGUI-hierarchies.Gvtutilizesamatchingpro-cedureforleafnodecomponentsasdirecttreecomparisonsarenot
possible in the context of mock-up driven development. There has
alsobeenbothcommercialandacademicworkrelatedtographi-
cal software built specifically for creating high-fidelity mobile app
mock-ups or mockups that encode information for automated cre-
ation of code for a target platform [ 4,8,9,34]. However, such tools
tend to either impose too many restrictions on designers or do not
allow for direct creation of code, thus DVsstill persist in practice.
9 CONCLUSION & FUTURE WORK
In this paper, we have formalized the problem of detecting design
violations in mobile apps, and derived a taxonomy of design viola-
tions based on a robust industrial dataset. We presented Gvt, an
approach for automatically detecting, classifying, and reporting de-
signviolationsinmobileapps,andconductedawiderangingstudy
thatmeasuredperformance,utility,andindustrialapplicabilityof
thistool.OurresultsindicatethatGvtiseffectiveinpractice,offers
utility for developers, and is applicable in industrial contexts.
ACKNOWLEDGMENTS
The authors would like to thank Kebing Xie, Roozbeh Farahbod,
and the developers and designers at Huawei for their support.
174
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. Automated Reporting of GUI Design Violations for Mobile Apps ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1] Adobe photoshop http://www.photoshop.com.
[2]Android uiautomator http://developer.android.com/tools/help/uiautomator/
index.html.
[3] Apple app store https://www.apple.com/ios/app-store/.[4] Fuild-ui https://www.fluidui.com.[5] Google play store https://play.google.com/store?hl=en.[6] The marketch plugin for sketch https://github.com/tudou527/marketch.[7]
Mobile apps: What consumers really need and want https://info.dynatrace.com/
rs/compuware/images/MobileAppSurveyReport.pdf.
[8] Mockup.io https://mockup.io/about/.[9] Proto.io https://proto.io.
[10] The sketch design tool https://www.sketchapp.com.[11] Sketch extensions https://www.sketchapp.com/extensions/.[12]
Why your app’s ux is more important than you think http://www.codemag.com/
Article/1401041.
[13]E. Alégroth and R. Feldt. On the long-term use of visual gui testing in industrial
practice:acasestudy. EmpiricalSoftwareEngineering ,22(6):2937–2971,Dec2017.
[14]E.Alegroth,Z.Gao,R.Oliveira,andA.Memon. Conceptualizationandevaluation
ofcomponent-basedtestingunifiedwithvisualguitesting:Anempiricalstudy.
In2015 IEEE 8th International Conference on Software Testing, Verification and
Validation (ICST) , pages 1–10, April 2015.
[15]G. Bavota, M. Linares-Vásquez, C. Bernal-Cárdenas, M. Di Penta, R. Oliveto, and
D.Poshyvanyk.Theimpactofapichange-andfault-pronenessontheuserratings
of android apps. Software Engineering, IEEE Transactions on , 41(4):384–407, April
2015.
[16]J.Brooke. SUS:Aquickanddirtyusabilityscale. InP.W.Jordan,B.Weerdmeester,
A.Thomas,andI.L.Mclelland,editors, Usabilityevaluationinindustry .Taylor
and Francis, London, 1996.
[17] K. Charmaz. Constructing Grounded Theory . SAGE Publications Inc., 2006.
[18]S.R.Choudhary,M.R.Prasad,andA.Orso. Crosscheck:Combiningcrawlingand
differencing to better detect cross-browser incompatibilities in web applications.
InProceedings of the 2012 IEEE Fifth International Conference on Software Testing,
Verification and Validation , ICST ’12, pages 171–180, Washington, DC, USA, 2012.
IEEE Computer Society.
[19]A. Ciurumelea, A. SchaufelbÃČÂĳhl, S. Panichella, and H. C. Gall. Analyzing
reviewsandcodeofmobileappsforbetterreleaseplanning. In 2017IEEE24thIn-
ternationalConferenceonSoftwareAnalysis,EvolutionandReengineering(SANER) ,
pages 91–102, Feb 2017.
[20]A. Di Sorbo, S. Panichella, C. V. Alexandru, J. Shimagaki, C. A. Visaggio, G. Can-
fora, and H. C. Gall. What would users change in my app? summarizing appreviews for recommending software changes. In Proceedings of the 2016 24th
ACMSIGSOFT InternationalSymposiumonFoundations ofSoftwareEngineering ,
FSE 2016, pages 499–510, New York, NY, USA, 2016. ACM.
[21]K. Holl and F. Elberzhager. A mobile-specific failure classification and its usage
to focus quality assurance. In 2014 40th EUROMICRO Conference on Software
Engineering and Advanced Applications , pages 385–388, Aug 2014.
[22]G. Hu, X. Yuan, Y. Tang, and J. Yang. Efficiently, effectively detecting mobileapp bugs with appdoctor. In Proceedings of the Ninth European Conference on
Computer Systems , EuroSys ’14, pages 18:1–18:15, New York, NY, USA, 2014.
ACM.
[23]A.Issa,J.Sillito,andV.Garousi. Visualtestingofgraphicaluserinterfaces:An
exploratorystudytowardssystematicdefinitionsandapproaches. In 201214th
IEEEInternationalSymposiumonWebSystemsEvolution(WSE) ,pages11–15,Sept
2012.
[24]N.Jones. Sevenbestpracticesforoptimizingmobiletestingefforts. Technical
Report G00248240, Gartner.
[25]K. Kuusinen and T. Mikkonen. Designing user experience for mobile apps: Long-
term product ownerperspective. In 2013 20thAsia-Pacific Software Engineering
Conference , volume 1 of APSEC’13 , pages 535–540, Dec 2013.
[26]V. Lelli, A. Blouin, and B. Baudry. Classifying and qualifying gui defects. In 2015
IEEE 8th International Conference on Software Testing, Verification and Validation
(ICST), pages 1–10, April 2015.
[27]M. Linares-Vásquez, G. Bavota, C. Bernal-Cárdenas, M. Di Penta, R. Oliveto, and
D. Poshyvanyk. Api change and fault proneness: A threat to the success ofandroid apps. In Proceedings of the 2013 9th Joint Meeting on Foundations of
Software Engineering , ESEC/FSE’13, pages 477–487, New York, NY, USA, 2013.
ACM.
[28]M.Linares-Vásquez,G.Bavota,M.D.Penta,R.Oliveto,andD.Poshyvanyk. How
do API changes trigger Stack Overflow discussions? a study on the android SDK.InProceedingsofthe22ndInternationalConferenceonProgramComprehension ,
ICPC’14, pages 83–94, 2014.
[29]M. Linares-Vásquez, G. Bavota, M. Tufano, K. Moran, M. Di Penta, C. Vendome,
C. Bernal-Cárdenas, and D. Poshyvanyk. Enabling mutation testing for android
apps. InProceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering , ESEC/FSE 2017, pages 233–244, New York, NY, USA, 2017. ACM.
[30]S. Mahajan, A. Alameer, P. McMinn, and W. G. Halfond. Automated repair of
layout cross browser issues using search-based techniques. In International
Conference on Software Testing and Analysis , ISSTA’17, 2017.
[31]S.MahajanandW.G.J.Halfond. Detectionandlocalizationofhtmlpresentation
failures usingcomputervision-based techniques. In Proceedingsof the8th IEEE
International Conference on Software Testing, Verification and Validation , ICST’15,
April 2015.
[32]S. Mahajan, B. Li, P. Behnamghader, and W. G. Halfond. Using visual symptoms
for debugging presentation failures in web applications. In Proceeding of the
9th IEEE International Conference on Software Testing, Verification, and Validation
(ICST), ICST’16, April 2016.
[33]T.McDonnell,B.Ray,andM.Kim.Anempiricalstudyofapistabilityandadoption
in the android ecosystem. In Proceedings of the 2013 International Conference on
Software Maintenance , ICSM’13, pages 70–79, 2013.
[34]J. Meskens, K. Luyten, and K. Coninx. Plug-and-design: Embracing mobile
devices as part of the design environment. In Proceedings of the 1st ACM SIGCHI
SymposiumonEngineeringInteractiveComputingSystems ,EICS’09,pages149–
154, New York, NY, USA, 2009. ACM.
[35]K. Moran, B. Li, C. Bernal-Cárdenas, D. Jelf, and D. Poshyvanyk. Gvt online
appendix http://www.android-dev-tools.com/gvt.
[36]P. Morville. User experience design. http://semanticstudios.com/user_
experience_design/.
[37]B. Myers. Challenges of hci design and implementation. interactions , 1(1):73–83,
Jan. 1994.
[38]B. N. Nguyen, B. Robbins, I. Banerjee, and A. Memon. Guitar: An innovative
tool for automated testing of gui-driven software. Automated Software Engg. ,
21(1):65–105, Mar. 2014.
[39]T. A. Nguyen and C. Csallner. Reverse engineering mobile application userinterfaces with REMAUI. In Proceedings of the 2015 30th IEEE/ACM Interna-
tional Conference on Automated Software Engineering , ASE’15, pages 248–259,
Washington, DC, USA, 2015. IEEE Computer Society.
[40]F.Palomba,M.Linares-Vásquez,G.Bavota,R.Oliveto,M.D.Penta,D.Poshyvanyk,and A. D. Lucia. User reviews matter! tracking crowdsourced reviews to support
evolutionofsuccessfulapps. In 2015IEEEInternationalConferenceonSoftware
Maintenance and Evolution (ICSME) , pages 291–300, Sept 2015.
[41]F. Palomba, P. Salza, A. Ciurumelea, S. Panichella, H. Gall, F. Ferrucci, and
A. De Lucia. Recommending and localizing change requests for mobile apps
based on user reviews. In Proceedings of the 39th International Conference on
Software Engineering , ICSE ’17, pages 106–117, Piscataway, NJ, USA, 2017. IEEE
Press.
[42]S. Roy Choudhary, M. R. Prasad, and A. Orso. X-pert: Accurate identification of
cross-browser issues in web applications. In Proceedings of the 2013 International
Conference on Software Engineering , ICSE ’13, pages 702–711, Piscataway, NJ,
USA, 2013. IEEE Press.
[43]S. Roy Choudhary, H. Versee, and A. Orso. Webdiff: Automated identification of
cross-browser issues in web applications. In Proceedings of the 2010 IEEE Interna-
tionalConferenceonSoftwareMaintenance ,ICSM’10,pages1–10,Washington,
DC, USA, 2010. IEEE Computer Society.
[44]T. Silva da Silva, A. Martin, F. Maurer, and M. Silveira. User-centered design and
agile methods: A systematic review. In Proceedings of the 2011 Agile Conference ,
AGILE ’11, pages 77–86, Washington, DC, USA, 2011. IEEE Computer Society.
[45]K.-J. Stol, P. Ralph, and B. Fitzgerald. Grounded theory in software engineering
research:Acriticalreviewandguidelines. In Proceedingsofthe38thInternational
ConferenceonSoftwareEngineering ,ICSE’16,pages120–131,NewYork,NY,USA,
2016. ACM.
[46]A. B.Tucker. Computer ScienceHandbook,SecondEdition . Chapman & Hall/CRC,
2004.
[47]Q.Xie,M. Grechanik,C.Fu,andC. Cumby. Guide: Aguidifferentiator. In 2009
IEEE International Conference on Software Maintenance , ICSM’09.
[48]H. Yee, S. Pattanaik, and D. P. Greenberg. Spatiotemporal sensitivity and visual
attention forefficient renderingof dynamic environments. ACM Trans. Graph. ,
20(1):39–65, Jan. 2001.
[49]C. Zeidler, C. Lutteroth, W. Stuerzlinger, and G. Weber. Evaluating Direct Manip-
ulationOperationsforConstraint-BasedLayout ,pages513–529. SpringerBerlin
Heidelberg, Berlin, Heidelberg, 2013.
175
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:51:40 UTC from IEEE Xplore.  Restrictions apply. 