Automatic Loop-Invariant Generation and
ReÔ¨Ånement through Selective Sampling
Jiaying Li1, Jun Sun1, Li Li1, Quang Loc Le2and Shang-Wei Lin3
1Singapore University of Technology and Design, Singapore
2School of Computing, Teesside University, United Kingdom
3School of Computer Science and Engineering, Nanyang Technological University, Singapore
Abstract ‚ÄîAutomatic loop-invariant generation is important in
program analysis and veriÔ¨Åcation. In this paper, we propose to
generate loop-invariants automatically through learning and veri-
Ô¨Åcation. Given a Hoare triple of a program containing a loop, we
start with randomly testing the program, collect program states
at run-time and categorize them based on whether they satisfy
the invariant to be discovered. Next, classiÔ¨Åcation techniques are
employed to generate a candidate loop-invariant automatically.
Afterwards, we reÔ¨Åne the candidate through selective sampling
so as to overcome the lack of sufÔ¨Åcient test cases. Only after a
candidate invariant cannot be improved further through selective
sampling, we verify whether it can be used to prove the Hoare
triple. If it cannot, the generated counterexamples are added as
new tests and we repeat the above process. Furthermore, we show
that by introducing a path-sensitive learning, i.e., partitioning
the program states according to program locations they visit
and classifying each partition separately, we are able to learn
disjunctive loop-invariants. In order to evaluate our idea, a
prototype tool has been developed and the experiment results
show that our approach complements existing approaches.
Index Terms‚ÄîLoop-invariant, program veriÔ¨Åcation, classiÔ¨Åca-
tion, active learning, selective sampling
I. I NTRODUCTION
Automatic loop-invariant generation is fundamental for
program analysis. A loop-invariant can be useful for software
veriÔ¨Åcation, compiler optimization, program understanding, etc.
In the following, we Ô¨Årst deÔ¨Åne the loop-invariant generation
problem, review existing approaches and then brieÔ¨Çy introduce
our proposal. Without loss of generality, we assume that we
are given a Hoare triple in the following form.
fPreg =?Assumption ?=
while (Cond)fBodyg=?Loop Body?=
fPostg =?Assertion?=
Assume that V=fx1;x2;;xngis a Ô¨Ånite set of program
variables which are relevant to the loop body. Pre,Cond and
Post are predicates constituted by variables in V.
Lets=fx17!v1;;xn7!vngbe a valuation of V. Let
be a predicate constituted by variables in V.is viewed as
the set of valuations of Vsuch thatevaluates to true given the
valuation. We thus write s2to denote that is evaluated to
true givens. Otherwise, we write s62.Body is an imperative
program that updates the valuation of V. For simplicity, we
assume that it is a deterministic function1on valuations of
1Our approach works as long as the non-determinism in Body orCond is
irrelevant to whether the postcondition is satisÔ¨Åed or not.variablesV, and write Body (s)to denote the valuation of
Vafter executing Body given the variable valuation s. For
convenience, Bodyi(s)wherei0is deÔ¨Åned as follows:
Body0(s) =sandBodyi+1(s) =Body (Bodyi(s)).
The goal is to either prove or disprove the Hoare triple. To
prove it, we would like to Ô¨Ånd a loop-invariant Inv which
satisÔ¨Åes the following three conditions.
PreInv (1)
8s: s2Inv^Cond =)Body (s)2Inv (2)
Inv^:CondPost (3)
To disprove it, we would like to Ô¨Ånd a valuation ssuch that
s2Pre and executing the loop until it terminates results in
a valuations0such thats062Post . For simplicity, we assume
that the loop always terminates and refer the readers to [ 2],
[10] for research on proving loop termination.
Loop-invariant generation is a long standing problem. Many
approaches have been proposed to solve this problem [ 13],
[36], [31], [28], [3], [11], [27], [34], [35], [25], [12], [24], [17].
These approaches all rely on some form of constraint solving
and often suffer from scalability issues. Recently, a number of
guess-and-check approaches [ 48], [47], [46], [44], [19], [18]
have been proposed. These approaches start with generating
a set of valuations of V(a.k.a. the samples) and categorize
them into different groups, e.g., one containing those satisfying
the loop-invariant and another containing those not. Learning
techniques are then applied to generalize the valuations in a
certain form to guess candidate loop-invariants. The candidates
are then check ed using program veriÔ¨Åcation techniques (like
symbolic execution [ 38]) to see whether they satisfy the three
conditions. If any of the conditions is violated, counterexamples
in the form of variable valuations can be obtained. For instance,
given a candidate loop-invariant , if condition (1) is violated,
a valuations2(Pre^:)is generated, which proves that 
is not an invariant. With this sample s, we can learn a new
candidate invariant. This guess-and-check process is repeated
until the Hoare triple is either proved or disproved.
Existing guess-and-check approaches vary in how samples
are generated and candidate invariants are guessed. We refer
the readers to Section V for a detailed discussion. A common
problem with these guess-and-check approaches is that their
effectiveness is often limited by the samples generated in their
Ô¨Årst phases. In order to guess the right invariant, often a large
978-1-5386-2684-9/17/$31.00 c2017 IEEEASE 2017, Urbana-Champaign, IL, USA
Technical Research782
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. number of samples are necessary. If classiÔ¨Åcation techniques
are employed, often those samples right by the boundary
between variable valuations which satisfy the actual invariant
and those which do not must be sampled so that classiÔ¨Åcation
techniques would identify the right invariant. Obtaining those
samples through random sampling is however often hard.
As a result, many iterations of guess-and-check are required.
Another problem is that the kinds of loop-invariants obtained
through existing guess-and-check approaches [ 48], [47], [46],
[44] are often limited, e.g., conjunctive linear inequalities [ 48]
or equalities [ 46]. Despite the approaches presented in [ 23],
[45], learning disjunctive loop-invariants remains a challenge.
Our Contribution In this work, we propose a technique to
improve the existing guess-and-check approaches [ 48], [47],
[46], [44] by making the following contributions. Firstly, we
propose an active learning technique, known as selective
sampling, to overcome the limitation of random sampling.
That is, selective sampling allows us to automatically generate
samples which are important in improving the quality of the
candidate invariants so that we can improve the candidates
prior to checking them using heavy program veriÔ¨Åcation
techniques. As a result, we can reduce the number of
guess-and-check iterations. Secondly, we propose to generate
disjunctive invariants through path-sensitive learning. That is,
we partition the samples according to the control locations
they visit, classify each partition separately and construct
a disjunction of the learned results for each partition as
the loop-invariant. Thirdly, our approach is designed to be
extensible so that we can learn different kinds of invariants.
For instance, we generate candidate invariants in the form of
polynomial inequalities or their conjunctions when different
classiÔ¨Åcation algorithms are adapted. Lastly, we implement
our framework as a tool called Z ILU (available at [ 1]) and
compare it with state-of-the-art tools like Interproc [ 30],
CPAChecker [ 7], InvGen [ 26] and BLAST [ 6]. Most test
subjects are gathered from previous collections as well as
the software veriÔ¨Åcation repository [ 5]. The results show that,
for those programs that are compatible with ZILU‚Äôs input
restrictions, Z ILUis able to prove the maximum number of
programs. Furthermore, it is shown that selective sampling is
able to reduce the need for checking, sometimes completely.
Organization The remainders of the paper are organized as
follows. Section II presents an overview of our approach using
simple illustrative examples. Section III shows how candidate
loop-invariants are generated through classiÔ¨Åcation and reÔ¨Åned
through selective sampling. Section IV evaluates our approach
using a set of benchmark programs. Section V reviews related
work and Section VI concludes.
II. T HEOVERALL APPROACH
Through this paper, loop-invariant generation using a guess-
and-check approach is an iterative process of data collection,
guessing (i.e., classiÔ¨Åcation in this work) and checking (i.e.,
veriÔ¨Åcation of the invariant candidate). In the following, wepresent how our approach works step-by-step and illustrate
each step with simple examples.
Example 1. Four Hoare triple examples are shown in Fig 1,
where an assume statement captures the precondition and an
assert statement captures the postcondition. The set Vfor
each program contains two integer variables: xandy. For
simplicity, we write (a;b) whereaandbare integer constants
to denote the evaluation fx7!a;y7!bg. Furthermore, we
interpret integers in the programs as mathematical integers (i.e.,
they do not overÔ¨Çow). One example invariant which can be
used to prove the Hoare triple is shown for each program. For
instance, the Hoare triple shown in Fig. 1(a) can be proven
using a loop-invariant: xy+ 16 , whereas conjunctive or
disjunctive invariants are necessary to prove the rest of the
Hoare triples. We remark that there might be different loop-
invariants which could be used to prove the Hoare triples. In
the following, we show how we generate loop-invariants for
proving these Hoare triples.
Our overall approach is shown in Algorithm 1. We start with
randomly generating a set of valuations of V, denoted as SP,
at line 1 (a.k.a. random sampling). Random sampling provides
us an initial set of samples to learn the very Ô¨Årst candidate for
the loop-invariant. In this work, we have two ways to generate
random samples. One is that we generate random values for
each variable in Vbased on its domain, assuming a uniform
probabilistic distribution over all values in its domain. The
other is that we apply an SMT solver [ 4], [15] to generate
valuations that satisfy Pre as well as those that fail Pre. These
two ways are complementary. On one hand, without using a
solver, we may not be able to generate valuations which satisfy
Pre ifPre is very restrictive (or fail Pre if the negation of
Pre is very restrictive). On the other hand, using a solver often
generates biased valuations.
Next, for any valuation sinSP, we execute the program
starting with initial variable valuation sand record the valuation
ofVafter each iteration of the loop. We write s)s0
to denote that there exists i0such thats0=Bodyi(s)
andBodyk(s)2Cond for allk2[0;i). That is, if we
start with valuation s, we obtain s0after some number of
iterations. At line 3 of Algorithm 1, we add all such valuations
s0intoSP. Next, we categorize SPinto four disjoint sets:
CE,Positive ,Negative andNP. Intuitively, CE contains
counterexamples which disprove the Hoare triple; Positive
contains those valuations of Vwhich we know must satisfy
any loop-invariant which proves the Hoare triple; Negative
contains those valuations of Vwhich we know must not satisfy
any loop-invariant which proves the Hoare triple; and NP
contains the rest. Formally,
CE(SP) =fs2SPj9s0;s0:
s02Pre^s0)s)s0
^s062Cond^s062Postg
A valuation sinCE(SP)starts from a valuation s0which
satisÔ¨ÅesPre and becomes a valuation s0which fails Post
783
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. 1assume(x<y);
2while(x<y)f
3 if(x<0) x:=x+7;
4 else x:=x+10;
5 if(y<0) y:=y 10;
6 else y:=y+3;
7g
8assert(yxy+16);
(a) Invariant: xy+ 161assume(x>0_y>0);
2while(x+y 2)f
3 if(x>0)f
4 x:=x +1;
5gelsef
6 y:=y +1;
7g
8g
9assert(x>0_y>0);
(b) Invariant: x > 0_y > 01assume(x =1^y=0);
2while()f
3 x:=x +y;
4 y:=y +1;
5g
6assert(xy);
(c) Invariant: y0^xy1assume(x <0);
2while(x <0)f
3 x=x+y;
4 y++;
5g
6assert(y >0);
(d) Invariant: x < 0_y > 0
Fig. 1: Example Programs
when the loop terminates. If CE(SP)is non-empty, the Hoare
triple is disproved.
Positive (SP) =fs2SPj9s0;s0:
s02Pre^s0)s)s0
^s062Cond^s02Postg
Positive (SP)contains a valuation sif there exists a valuation
s0inSPwhich satisÔ¨Åes Pre and becomes safter zero or
more iterations. Furthermore, ssubsequently becomes s0, which
satisÔ¨ÅesPost when the loop terminates. Let Inv be any loop-
invariant that proves the Hoare triple. Because s02Pre,
s02Inv sinceInv satisÔ¨Åes condition (1). Since Inv satisÔ¨Åes
condition (2) and Body (s0)2Inv ifBody (s0)2Cond . By
a simple induction, we prove s2Inv.
Negative (SP) =fs2SPj9s0;s0:
s062Pre^s0)s)s0
^s062Cond^s062Postg
Negative (SP)is a valuation swhich starts from a valuation s0
violatingPre and becomes a valuation s0which violates Post
when the loop terminates. We show that s62Inv for allInv
satisfying condition (1), (2) and (3). Assume that s2Inv, by
condition (2), s0must satisfy Inv through a simple induction.
By condition (3), s0must satisfy Post , which contradicts the
deÔ¨Ånition of Negative (SP).
NP(SP) =SP CE(SP) Positive (SP) Negative (SP)
NP(SP)contains the rest of the samples. We remark that a
valuationsinNP(SP)may or may not satisfy an invariant
Inv which satisÔ¨Åes condition (1), (2) and (3).
Example 2. Take the program shown in Fig. 1(a) as an example.
Assume that the following three valuations are randomly
generated: (1;2),(10;1)and(100; 0)at line 1. Three sequences
of valuations are generated after executing the program with
these three valuations: h(1;2);(11;5)i,h(10; 1)iandh(100; 0)i
respectively. Note that the loop is skipped entirely for the
latter two cases. After categorization, set CE(SP)is empty;
Positive (SP)isf(1;2);(11;5)g;Negative (SP)isf(100; 0)g;
andNP(SP)isf(10; 1)g.
After obtaining the samples and labeling them as discussed
above, method activeLearn (SP)at line 4 in Algorithm 1Algorithm 1: Algorithmverify ()
1letSPbe a set of randomly generated valuations of V;
2while not time out do
3 add all valuations s0such thats)s0for some
s2SPintoSP;
4 callactiveLearn (SP)to generate a candidate
invariant;
5 return ‚Äúproved‚Äù if the program is veriÔ¨Åed with 
otherwise add the counterexample into SP;
is invoked to generate a candidate invariant . We leave the
details on how candidate invariants are generated in Section III,
which is our main contribution in this work. Once a candidate
is identiÔ¨Åed, we move on to check whether satisÔ¨Åes condition
(1), (2) and (3) at line 5. In particular, we check whether any
of the following constraints is satisÔ¨Åable or not using an SMT
solver [4], [15].
Pre^: (4)
sp(^Cond;Body )^: (5)
^:Cond^:Post (6)
wheresp(^Cond;Body )is the strongest postcondition
obtained by symbolically executing program Body starting
from precondition ^Cond [16]. If all the three constraints
are unsatisÔ¨Åable, we successfully prove the Hoare triple with
the loop-invariant . If any of the constraints is satisÔ¨Åable, a
model in the form of a variable valuation is generated, which is
then added to SPas a new sample. Afterwards, we restart from
line 2, i.e., we execute the program with the counterexample
valuations, collect and add the variable valuations after each
iteration of the loop to the four categories accordingly, move
on to active learning and so on.
Example 3. For the example shown in Fig. 1(a), a candidate
invariant which is automatically learned is x y16. It is easy
to check that this candidate satisÔ¨Åes all the three conditions and
thus the Hoare triple shown in Fig. 1(a) is proved. For Fig. 1(c),
a candidate invariant returned by method activeLearn (SP)is
as follows.
490 + 16x 9y0^510 + 6x + 29y0^
56 y0^166 2x+ 5y0
784
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. Algorithm 2: AlgorithmactiveLearn (SP)
1while true do
2 if (CE (SP)is not empty) exit and report
‚Äúdisproved‚Äù;
3 letbe a set of candidates generated by
classify (SP);
4 if ( is the same as last iteration) return ;
5 addselectiveSampling ()intoSP;
6 add all valuations s0such thats)s0for some
s2SPintoSP;
A counterexample ( 28; 11) is generated when we check
the satisÔ¨Åability of (5), which is then used to generate a new
candidate. After multiple iterations of guess-and-check, the
following invariant is generated.
y0^x y0^x1
Different from the invariant in Fig. 1(d), this candiate still
succeeds in proving the given Hoare triple. Thus, the loop-
invariant is found.
III. O URAPPROACH : CLASSIFICATION , ACTIVE LEARNING
AND SELECTIVE SAMPLING
In this section, we present details on how candidate invariants
are generated. Algorithm 2 shows how activeLearn (SP)is
implemented in general, i.e., it iteratively generates a candidate
through classiÔ¨Åcation (at line 3) and improves it through
selective sampling (at line 5) until a Ô¨Åxed point is reached.
Note that once a counterexample is identiÔ¨Åed (at line 2), it
exits and reports that the Hoare triple is disproved.
The method call classify (SP)in Algorithm 2 generates
a candidate invariant based on classiÔ¨Åcation techniques. Intu-
itively, since we know that valuations in Positive (SP)must
satisfyInv and valuations in Negative (SP)must not satisfy
Inv, a predicate separating the two sets (a.k.a. a classiÔ¨Åer) may
be a candidate invariant. In the following, we Ô¨Åx two disjoint
sets of samples PandNand discuss how to automatically
generate classiÔ¨Åers separating PandN. For now,Pcan be
understood as Positive(SP )andNcan be understood as
Negative (SP). We discuss alternatives in Section III-D.
To automatically generate classiÔ¨Åers separating PandN,
we apply existing classiÔ¨Åcation techniques. There are many
classiÔ¨Åcation algorithms, e.g., [ 37], [40], [8]. In our approach,
the classiÔ¨Åcation algorithms must generate perfect classiÔ¨Åers.
Formally, a perfect classiÔ¨Åer forPandNis a predicate
such thats2for alls2Pands62for alls2N.
Furthermore, the classiÔ¨Åer must be human-interpretable or can
be handled by existing program veriÔ¨Åcation techniques. In the
following, we Ô¨Årst brieÔ¨Çy discuss how to generate conjunctive
invariants using the approach proposed in [ 48] and then propose
a path-sensitive approach to generate disjunctive invariants.
Afterwards, we show how to improve candidate invariants
systematically through selective sampling.A. Conjunctive Invariants
In the following, we show how to generate loop-invariants
in the form: 1^2^^kwhere eachiis a polynomial
inequality up to certain degree, constituted by variables in V.
Our approach is based on Support Vector Machines (SVM ).
SVM is a supervised machine learning algorithm for clas-
siÔ¨Åcation and regression analysis [ 8]. In general, the binary
classiÔ¨Åcation functionality of SVM works as follows. Given P
andN,SVM can generate a perfect classiÔ¨Åer to separate them
if there is any. We refer the readers to [ 39] for details on how
the classiÔ¨Åer is computed. In this work, we always choose the
optimal margin classiÔ¨Åer if possible. Intuitively, the optimal
margin classiÔ¨Åer could be seen as the strongest witness why
PandNare different. SVM by default learns classiÔ¨Åers in
the form of a linear inequality, i.e., a half space in the form
ofc1x1+c2x2+kwherexiare variables in Vandci
are constant coefÔ¨Åcients.
We can easily extend SVM to learn polynomial classiÔ¨Åers.
GivenPandNas well as a maximum degree dof the
polynomial classiÔ¨Åer, we can systematically map all the samples
inP(similarlyN) to a set of samples P0(similarlyN0) in a
high dimensional space by expanding each sample with terms
which have a degree up to d. For instance, assume that the
maximum degree is 2, the sample valuation fx7!2;y7!1g
inPis mapped tofx7!2;y7!1;x27!4;xy7!2;y27!1g.
SVM is then applied to learn a perfect linear classiÔ¨Åer for
P0andN0. Mathematically, a linear classiÔ¨Åer in the high
dimensional space is the same as a polynomial classiÔ¨Åer in the
original space [ 29]. Note that the size of each sample in P0
orN0grows rapidly with the increase of the degree and thus
the above method is limited to polynomial classiÔ¨Åers with a
relatively low degree.
A polynomial classiÔ¨Åer can represent some classiÔ¨Åers in
the form of disjunctive or conjunctive linear inequalities. For
instance, the classiÔ¨Åer (xd0^xd1)_(xd2)where
d0<d 1<d 2are constants can be represented equivalently as
the following polynomial inequality.
x3+ (d0d1+d0d2+d1d2)x2 (d0+d1+d2)x d0d1d20
However, this representation transformation is not always
possible, i.e., some conjunctive or disjunctive linear inequalities
cannot be expressed as a polynomial classiÔ¨Åer. One typical
example is: x0^y0.
To generate conjunctive classiÔ¨Åers, we adopt the algorithm
SVM-I proposed in [ 48]. The idea is to pick one sample s
fromNeach time and identify a classiÔ¨Åer iin the form
of a polynomial inequality to separate Pandfsg, remove
all samples from Nwhich can be correctly classiÔ¨Åed by i,
and then repeat the process until Nbecomes empty. The
conjunction of all the classiÔ¨Åers iis then a perfect classiÔ¨Åer
separatingPandN. We refer the readers to [ 48] for details
of the algorithm. We remark that if we switch PandN, the
negation of the learned classiÔ¨Åer using this algorithm is in the
form of a disjunction of polynomial inequalities.
785
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. B. Disjunctive Invariants
It is often challenging to automatically generate disjunctive
invariants [ 45], [23], whereas certain Hoare triples can only be
proved with disjunctive invariants. Two examples are shown in
Fig. 1(b) and Fig. 1(d). In the following, we show one way to
learn disjunctive invariants, i.e., invariants in the general form
of
1_2__m
where each i='i;1^'i;2^^'i;nis a conjunctive
polynomial inequality. Our observation is that disjunctive
invariants are often required to prove certain Hoare tripe
because the program contains branching commands (i.e., if
and while ). For instance, proving the Hoare triple shown in
Fig. 1(b) requires a disjunctive loop-invariant, which is largely
due to the branch at line 3. Based on this observation, we
propose to learn disjunctive invariants through path-sensitive
classiÔ¨Åcation.
Without loss of generality, we assume that the loop
bodyBody can be modeled as a transition system
(C;init;end;T;L ).Cis a Ô¨Ånite set of control locations.
init2Cis a unique entry point (i.e., the start of the program).
end2Cis a unique exit point (i.e., the end of the program,
which is assumed to be always reachable). T:C!Cis a
transition function2which captures the control Ô¨Çow. Lastly,
Lis a labeling function which labels each transition with a
pair(g;f)wheregis a guard condition and fis a function
updating variable valuation. Note that gis used to model
branching conditions whereas fis used to model program
statements like assignments. For instance, the loop body in
the Ô¨Årst program in Fig. 1(b) can be modeled as a transition
system with four control locations representing line 3, 4, 5
and 6; and the transition from the control location representing
line 3 to the one representing line 4 is labeled with a guard
conditionx >0and a function which does not change any
variable valuation.
Given a valuation sofVsatisfying the loop condition Cond ,
we can obtain a unique path through the program path(s) =
hc1;c2;;ckiwhereci2Cfor allisuch thatc1=init,
ck=end and every guard condition along the path is satisÔ¨Åed.
For instance, given the loop body in Fig. 1(b) and valuation
fx7!0;y7! 3g , the unique path is h3;5;6i. Ifsviolates the
loop condition Cond , we setpath(s) to be an empty sequence.
Intuitively,path(s) is the set of sequence of control locations
visited bysin one iteration of the loop.
Our path-sensitive classiÔ¨Åcation starts with partitioning P
into a set of disjoint partitions such that for each partition
Pi,path(s) =path(s0)for all valuation sands0inPi. For
eachPi, we can construct a unique path condition pci, i.e., a
formula over the symbolic variables in Vand the accumulated
constraints which the symbolic variables must satisfy in order
for an execution to follow the corresponding path. For instance,
given the program shown in Fig. 1(b), if Pis set to be
Positive(SP ), we have three partitions. The Ô¨Årst one contains
2It is a function as we assume Body is deterministic.all valuations swithpath(s) beingh3;4iwhose path condition
isx+y   2^x > 0; the second one contains all
valuationsswithpath(s) beingh3;5;6iwhose path condition
isx+y 2^x0and the last one contains all valuations
swithpath(s) beinghiwhose path condition is x+y> 2.
Next, we apply the approach presented in Section III-A
to learn a conjunctive classiÔ¨Åer for each partition Pi, i.e.,
we learn a classiÔ¨Åer ifor separating PifromNi. Then
the disjunctionW
i(i^pci)is a perfect classiÔ¨Åer separating
PfromN. Sinceiis a conjunctive predicate, we learn
candidate invariants in the form of disjunction of conjunction
of polynomial inequalities.
Example 4. Though the program shown in Fig. 1(d) contains
noifcommand, variable valuations in Positive(SP )can be
partitioned into two partitions according to our deÔ¨Ånition: one
containing those visit line 3 and 4, the other containing those
skipping the loop. In the following, we show how to learn a
disjunctive loop-invariant based on these two partitions. Note
that a valuation sis inNegative (SP)only ifs2(y0^x
0). If we have every valuation of Vfor these two partitions,
a classiÔ¨Åer we could learn for the former partition is x <0
(i.e., a valuation must satisfy the invariant if it enters the loop)
and the classiÔ¨Åer we learn for the latter partition is y > 0.
As a result, conjuncted with the path condition, we learn the
candidate invariant: (x< 0^x<0)_(y > 0^x0)
which can be simpliÔ¨Åed as x <0_y >0and proves the
Hoare triple.
We remark that in the above discussion, we assume that we
can obtain every variable valuation, which is often infeasible
in practice as there are too many of them. In the following
subsection, we aim to solve this problem.
C. Active Learning and Selective Sampling
One fundamental problem with applying machine learning
techniques to learn loop-invariants is that we often have
only a limited set of samples. That is, with the limited
samples inPositive(SP )andNegative (SP), it is unlikely
that we can obtain an ‚Äúaccurate‚Äù classiÔ¨Åer. For instance, as
shown in Example 2, Positive(SP )isf(1;2);(11;5)gand
Negative (SP)isf(100; 0)g. A linear classiÔ¨Åer identiÔ¨Åed
using SVM for this example is: 3x 10y152. Although
this classiÔ¨Åer perfectly separates the two sets, it is not useful
in proving the Hoare triple and is clearly the result of having
limited samples. One obvious way to overcome this problem
is to generate more samples. However, often a large number
of samples are necessary in order to learn the correct classiÔ¨Åer.
One particular reason is that we often need the samples right
on the classiÔ¨Åcation boundary in order to learn the correct
classiÔ¨Åer, which are often difÔ¨Åcult to obtain through random
sampling. In existing guess-and-check approaches [ 48], [47],
[46], [44], [19], [18], the problem is overcome by checking
whether the candidate invariant proves the Hoare triple through
program veriÔ¨Åcation. That is, new samples are obtained from
counterexamples generated by the program veriÔ¨Åcation engine,
which are then used to reÔ¨Åne the classiÔ¨Åer. The issue is that
786
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. often many iterations of guess-and-check are required before
the invariant would converge to the correct one.
Researchers in the machine learning community have studied
extensively on how to overcome the problem of limited samples.
One of the remedies is active learning [ 43]. Active learning
is proposed in contrast to passive learning. A passive learner
learns from a given set of samples that it has no control over,
whereas an active learner actively selects what samples to learn
from. It has been shown that an active learner can sometimes
achieve good performance using far fewer samples than would
otherwise be required by a passive learner [ 49], [50]. Active
learning can be applied for classiÔ¨Åcation or regression. In
this work, we apply it for improving the candidate invariants
generated by the above-discussed classiÔ¨Åcation algorithms.
A number of different active learning strategies on how to
select the samples have been proposed. For instance, version
space partitioning [ 41] tries to select samples on which there is
maximal disagreement between classiÔ¨Åers in the current version
space (e.g., the space of all classiÔ¨Åers which are consistent
with the given samples); uncertainty sampling [ 32] maintains
an explicit model of uncertainty and selects the sample that it
is least conÔ¨Ådent about. The effectiveness of these strategies
can be measured in terms of the labeling cost, i.e., the number
of labeled samples needed in order to learn a classiÔ¨Åer which
has a classiÔ¨Åcation error bounded by some threshold . For
some classiÔ¨Åcation algorithms, it has been shown that active
learning reduces the labeling cost from 
(1
)to the optimal
O(dlg1
)wheredis the dimension of the samples [ 21], [14].
That is, if passive learning requires a million samples, active
learning may require just lg 1000000 (20) to achieve the
same accuracy.
In this work, we adopt the active learning strategy for
SVM proposed in [ 42], called selective sampling, to improve
the invariant candidates. This strategy has been shown to be
effective in achieving a high accuracy with fewer examples
in different applications [ 49], [50]. In particular, at line 5 of
Algorithm 2, after obtaining a classiÔ¨Åer based on existing
samples in SP, we apply method selectiveSampling ()to
selectively generate new samples. It works by generating
multiple samples on the current classiÔ¨Åcation boundary .
Afterwards, the samples are added into SPat line 5 and 6 and
we repeat from line 2 until the classiÔ¨Åer converges.
The implementation of selectiveSampling depends on
the type of classiÔ¨Åers. For classiÔ¨Åers in the form of linear
inequalities, identifying samples on the classiÔ¨Åcation boundary
is straightforward, i.e., by solving an equation. In the above
example, given the current classiÔ¨Åer 3x 10y152, we
apply selective sampling and generate new valuations (7; 13)
and(14; 11) by solving the equation 3x 10y= 152 . For
classiÔ¨Åers in the form of polynomial inequalities, the problem
is more complicated since existing solvers for multi-variable
polynomial equations have limited scalability. We thus use a
simple approach to identify solutions of a polynomial equation,
which we illustrate through an example in the following.
Assume that we learn the classiÔ¨Åer:  4x2+ 2y 11 . The
following steps are applied for selective sampling.1) Choose a variable in the classiÔ¨Åer, e.g., x.
2)Generates random value for all other variables. For
example, we let ybe12.
3)Substitute the variables in the classiÔ¨Åers with the gen-
erated values and solve the univariable equation, e.g.,
 4x2+ 24 = 11 . If there is no solution, go back to
(1) and retry. In our example, x2:9580.
4)Roundoff the values of all the variables according to
their types in the program. In our example, we obtain
the valuation (3;12).
In the case that a conjunctive or disjunctive classiÔ¨Åer is
learned, we apply above selective sampling approach to every
clause in the classiÔ¨Åer to obtain new samples. With the help
of active learning and selective sampling, we can often reduce
the number of learn-and-check iterations. As the empirical
studies shown in Section IV, one iteration of guess-and-check
is sufÔ¨Åcient in some cases to prove the Hoare triple.
Advantages of Selective Sampling In the following, we brieÔ¨Çy
discuss why selective sampling is helpful from a high-level
point of view. In this work, we collect samples in three different
ways. Firstly, random sampling provides us an initial set of
samples. The cost of generating a random sample is often low.
However, we often need a huge number of random samples in
order to learn accurately. Secondly, selective sampling has a
slightly higher cost as it requires us to solve some equation
system. However, it has been shown that selective sampling is
often beneÔ¨Åcial compared to random sampling [ 49], [50]. The
last way of sampling is sampling through veriÔ¨Åcation. When
a candidate invariant fails any of the three conditions (1), (2)
and (3) in the candidate veriÔ¨Åcation stage, the veriÔ¨Åer provides
counter-examples, which are added as new samples. Sampling
through veriÔ¨Åcation provides useful new samples by paying
a high cost. Furthermore, for complex programs, sampling
through veriÔ¨Åcation may not be feasible due to the limited
capability of existing program veriÔ¨Åcation techniques. Thus,
in this work, our approach is to start with random sampling,
use selective sampling to improve the classiÔ¨Åer as much as
possible and apply sampling through veriÔ¨Åcation only as the
last resort.
Fig. 2 visualizes how different sampling methods work in a
2-D plane. We start with the Ô¨Ågure in the top-left corner, where
the dots are the samples obtained through random sampling.
The (green) area above the line represents the space covered
by the actual invariant. Based on these samples, a classiÔ¨Åer
(shown as the red line) is learnt to separate the random samples,
as shown in the top-right Ô¨Ågure. Selective sampling allows
us to identify those samples along the classiÔ¨Åcation boundary,
as shown in the bottom-left Ô¨Ågure. In comparison, sampling
through veriÔ¨Åcation would provide us a sample between the
two lines, as shown in the bottom-left Ô¨Ågure. The classiÔ¨Åer will
be improved by either selective sampling or sampling through
veriÔ¨Åcation, as shown in the bottom-right Ô¨Ågure. The beneÔ¨Åt of
always applying selective sampling before applying sampling
through veriÔ¨Åcation is that sampling through veriÔ¨Åcation is
often costly or even worse, not available due to the limitation
787
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. RandomSamplingActualInvariant
ActualInvariantLearnedInvariant
SelectiveSamplingActualInvariantLearnedInvariantSampling throughVeriÔ¨Åcation
ActualInvariantNewInvariant
Fig. 2: Sampling Approaches
of existing program veriÔ¨Åcation techniques. Thus we would
like to avoid it as much as possible.
D. Making Use of Undetermined Samples
So far we have focused on learning and reÔ¨Åning classiÔ¨Åers
between Positive (SP)andNegative (SP)as candidate invari-
ants. The question is then: how do we handle those valuations
inNP(SP)? If we simply ignore them, there may be a gap
between Positive (SP)andNegative (SP)and as a result, the
learnt classiÔ¨Åer may not converge to the invariant we want, even
with the help of active learning. This is illustrated in Fig. 3,
where the set of valuations in Positive (SP)(marked with +),
Negative (SP)(marked with ) and NP(SP)(marked with
?) for the example in Fig. 1(a) are visualized in a 2-D plane.
Many samples between the line x=yandx y= 16 may
be contained in NP(SP). As a result, without considering the
samples in NP(SP), a classiÔ¨Åer located in the NP(SP)region
(e.g.,x y10, orx y13) may be learned to perfectly
classify Positive (SP)andNegative (SP). Worse, identifying
more samples may not be helpful in improving the classiÔ¨Åer
if the new samples are in NP(SP).
To solve the problem, in addition to learn a classiÔ¨Åer sep-
arating Positive (SP)andNegative (SP), we learn candidate
invariants making use of NP(SP). In principle, we should
enumerate all the possible categorization of the samples in
NP(SP)and run classiÔ¨Åcation algorithm on each of them.
However at most time it is very time-consuming and instead
we only try two extreme case in our implementation, which
is far from perfect and will be reÔ¨Åned in the future. In our
current setting, we learn classiÔ¨Åers separating Positive (SP)
from Negative (SP)[NP(SP)(i.e., assuming valuations in
NP(SP)fail the actual invariant), and classiÔ¨Åers separating
Negative (SP)from Positive (SP)[NP(SP)(i.e., assuming
valuations in NP satisfy the actual invariant). For the example
in Fig. 1(a), if we focus on classiÔ¨Åers in the form of linear
inequalities, the classiÔ¨Åer separating Positive (SP)from the
+
+
+16
- 16x-y = 0
+ 
x-y = 16
-
--
--
---
-
----++
++
+
++
+++
+?
???
??
??
?xyFig. 3: Samples Visualization
rest converges to NULL (no such classiÔ¨Åer), whereas the
classiÔ¨Åer separating Negative (SP)from the rest converges
tox y16, which can be used to prove the Hoare triple.
Note that this is orthogonal to which classiÔ¨Åcation algorithm
is used and whether selective sampling is applied.
IV. I MPLEMENTATION AND EVALUATION
We have implemented our approach for loop-invariant
generation in a tool called Z ILU(available at [ 1]). For candidate-
invariant veriÔ¨Åcation, we modify the KLEE project [ 9] to
symbolically execute C programs prior to invoking Z3 [ 15] for
checking satisÔ¨Åability of condition (4), (5) and (6). We remark
that, as a concolic testing engine, KLEE may concretely execute
the programs and return under-approximated abstraction. This
may affect the soundness of our system. To overcome this
problem, we detect those path conditions produced from
concrete executions and return a sound abstraction (i.e., true ).
Our evaluation subjects include a set of C programs gath-
ered from multiple resources, such as previous publications
(e.g., [ 23], [18], [48], [22], [30], [17]) and the software
veriÔ¨Åcation competitions 2017 (SV-Comp [ 5]). We remark
that the loops in these benchmark programs often contain
788
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. non-deterministic choices, which are often used to model
I/O environment (e.g., an external function call). As non-
determinism is beyond the scope of this work in general,
we manually examine each program to check whether our
assumption is satisÔ¨Åed or not, i.e., whether the non-determinism
is relevant in satisfying the post-condition or not. Only
those programs which do not satisfy our assumption are
excluded from our experiments. For those which do satisfy our
assumption, we replace those non-determinism with random
free boolean variables. In total, out of the 323 benchmark
programs we gathered, 59 programs are excluded as they do
not satisfy their speciÔ¨Åcation; 140 programs are excluded as
they do not have non-trivial precondition or postcondition or
the loop body contains unsupported constructs like ‚Äòbreak‚Äô or
‚Äògoto‚Äô statement; 59 are excluded as they contain unsupported
operations such as array operation; 8 are excluded due to
multiple loops; and 15 are excluded due to non-determinism.
We also exclude programs which are trivial to prove and
copies of the same program. Furthermore, we construct 11
programs (benchmarks[43-53] in the table) due to lack of
programs requiring polynomial or disjunctive invariants in
these benchmarks. All 53 programs are available at [1].
The parameters in our experiments are set as follows. For
random sampling, we generate 10 random values for every
input variable of a program from their default ranges. During
selective sampling, we generate 2 values for every input variable
along the classiÔ¨Åcation boundary. The ratio between random
samples and selective samples is thus 5:1, which we consider
to be reasonable as selective sampling is slightly more costly.
When we invoke LibSVM for classiÔ¨Åcation, the parameter C
(which controls the trade-off between avoiding misclassifying
training examples and enlarging decision boundary) and the
inner iteration for SVM learning are set to their maximum
value so that it generates only perfect classiÔ¨Åers. During
candidate veriÔ¨Åcation, integer-type variables in programs are
encoded as integers in Z3 (not as bit vectors). Since we
have different ways of setting the samples for classiÔ¨Åcation,
e.g., by setting the two sets of samples PandNdifferently
as discussed in Section III-D , and different classiÔ¨Åcation
algorithms (linear vs. polynomial or conjunctive vs. disjunctive),
we simultaneously try all combinations and terminate as
soon as either the Hoare triple is proved or disproved. For
polynomial inequalities, the maximum degree is bounded by
4. In order to give priority to simpler invariants, we look for a
polynomial classiÔ¨Åer with degree donly if we cannot Ô¨Ånd any
polynomial classiÔ¨Åer with lower degree. All of the experiments
are conducted using x64 Ubuntu 14.04.1 (kernel 3.19.0-59-
generic) with 3.60 GHz Intel Core i7 and 32G DDR3. Each
experiment is executed Ô¨Åve times since there is randomness in
our approach and we report the median as the result.
In order to evaluate our approach in learning loop-invariants,
we investigate the following research questions:
RQ1 The Ô¨Årst research question which we would like to answer
is:does selective sampling help to reduce the number of guess-
and-check iterations? We compare Z ILU with and without
selective sampling. The results are summarized in Table I. TheÔ¨Årst column shows the number of the program; the second
column shows the type of loop-invariant needed to prove
the Hoare triple; and the next six columns show details on
verifying the Hoare triple with and without selective sampling.
We compare the total number of samples generated and the
number of guess-and-check iterations. To reduce randomness,
the same set of initial random samples are used in both settings.
Each experiment has a time limit of 10 minutes. The winner
of each measurement is highlighted boldly.
The results show that Z ILUsuccessfully veriÔ¨Åes all programs
with help of selective sampling, and fails to verify 9 programs
without selective sampling. For these 9 programs, Z ILUtime-
outs due to too many guess-and-check iterations. This clearly
evidences the usefulness of selective sampling. Furthermore,
in 36 cases, selective sampling helps to reduce the number of
guess-and-check iterations. Though it rarely happens, due to
the randomness in our approach, it may happen that the right
invariant is learned by luck with fewer samples. This happens
in 3 cases (i.e., 5%) where Z ILUwithout selective sampling
has fewer (by 1 or 2) iterations.
We would like to highlight that for 10 programs, ZILUis
able to learn the correct invariant within one guess-and-check
iteration with selective sampling. It is never the case without
selective sampling. Furthermore, it happens when the invariant
is a linear inequality. We remark that being able to learn the
correct invariant without program veriÔ¨Åcation is useful for
handling complex programs. That is, even if we are unable to
automatically verify the generated invariant due to the limitation
of existing program veriÔ¨Åcation techniques, Z ILU‚Äôs result is
still useful in these cases as the generated invariant can be
used to manually verify the program.
In addition, we observe that Z ILU often takes more
samples and guess-and-check iterations to learn conjunctive
or disjunctive invariants. On average, Z ILUtakes 1.7, 3, 5.9
and 4.5 guess-and-check iterations to learn linear, polynomial,
conjunctive and disjunctive loop-invariants. For conjunctive
invariants, more iterations are needed because the algorithm
adopted from [ 48] for learning conjunctive classiÔ¨Åers often
requires more samples before convergence. For disjunctive
invariants, this is because we need sufÔ¨Åcient samples in each
partition in order to learn the right invariant.
RQ2 The second research question which we would like
to answer is: does selective sampling incur signiÔ¨Åcant
overhead? This is a valid question as selective sampling
requires solving equation systems. In Table I, we show the
total execution time of both Z ILUwith and without selective
sampling. It can be observed that the overhead of selective
sampling is reasonable. All programs are veriÔ¨Åed with 2
minutes. A close look reveals that most of the time is spent
on classiÔ¨Åcation and selective sampling. For 29 programs,
the overall time is reduced with selective sampling, due
to a reduced number of guess-and-check iterations. Z ILU
often takes more time to learn conjunctive, polynomial or
disjunctive invariants. This is because in such a case, SVM
classiÔ¨Åcation is invoked many times in one guess-and-check
789
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. TABLE I: E XPERIMENTAL RESULTS
ZILUwith Selective Sampling ZILUwithout Selective Sampling
benchmark type ]sample ]iter time(s) ]sample ]iter time(s) Interproc CPAChecker BLAST InvGen
benchmark01 conjunctive 136 9 8.34 63 13 12.79 3 3.19 3 3
benchmark02 linear 29 1 3.06 12 2 2.68 3 3.73 3 3
benchmark03 linear 62 2 2.88 22 2 2.88 3 3.57 3 3
benchmark04 conjunctive 197 5 8.71 35 5 3.93 3 3.26 3 3
benchmark05 conjunctive 444 12 10.83 42 12 6.66 7 3.51 3 3
benchmark06 conjunctive 166 4 4.48 34 4 3.53 3 3.55 3 3
benchmark07 linear 110 2 3.85 to to to 7 7 7 7
benchmark08 linear 117 3 3.96 33 3 3.28 7 7 to 3
benchmark09 conjunctive 145 5 4.06 25 5 4.08 3 3.20 3 3
benchmark10 conjunctive 213 25 37.06 to to to 7 3.26 3 3
benchmark11 linear 81 1 5.6 22 2 55.95 3 3.32 3 3
benchmark12 linear 262 4 7.55 74 44 16.22 3 3.16 3 3
benchmark13 conjunctive 240 6 15.2 36 6 4.54 3 3.18 3 3
benchmark14 linear 31 1 2.78 12 2 2.95 3 3.40 3 3
benchmark15 conjunctive 331 8 77.21 to to to 3 3.76 3 3
benchmark16 conjunctive 87 3 3.96 23 3 3.63 3 3.42 3 3
benchmark17 conjunctive 154 4 4.2 34 4 3.5 3 3.63 3 3
benchmark18 conjunctive 406 10 65.65 39 9 21.31 3 3.67 3 3
benchmark19 conjunctive 345 9 10.34 40 10 8.09 3 3.68 3 3
benchmark20 conjunctive 148 4 4.79 34 4 8.44 7 7 3 3
benchmark21 disjunctive 72 3 91.92 to to to 7 3.39 3 7
benchmark22 conjunctive 158 6 34.46 93 13 44.78 7 3.71 7 7
benchmark23 conjunctive 170 6 18.83 to to to 3 47.82 to 3
benchmark24 conjunctive 357 9 55.42 38 8 26.97 7 3.64 3 3
benchmark25 linear 31 1 4.3 12 2 4.31 3 3.05 3 3
benchmark26 linear 57 1 83.57 22 2 60.2 3 3.63 3 3
benchmark27 linear 104 2 28.34 33 3 3.32 3 3.39 3 3
benchmark28 linear 66 2 5.32 22 2 5.07 7 7 7 7
benchmark29 linear 36 1 4.69 24 4 6.31 3 3.09 3 3
benchmark30 conjunctive 83 3 6.27 24 4 5.76 3 3.40 3 3
benchmark31 disjunctive 30 2 28.06 40 4 72.18 7 3.61 3 7
benchmark32 linear 33 1 11.6 12 2 15.68 3 3.41 3 3
benchmark33 linear 37 1 9.94 12 2 13.46 3 3.45 3 3
benchmark34 conjunctive 345 9 75.36 37 7 27.43 3 4.04 3 3
benchmark35 linear 39 1 10.45 12 2 13.14 3 3.61 3 3
benchmark36 conjunctive 83 3 6.64 23 3 6.2 3 3.28 3 3
benchmark37 conjunctive 104 4 7.84 24 4 19.87 3 3.40 3 3
benchmark38 conjunctive 108 4 26.06 24 4 25.72 3 3.51 3 3
benchmark39 conjunctive 112 4 19.89 24 4 18.83 7 3.65 3 3
benchmark30 polynomial 180 4 34.05 25 5 38.76 7 7 7 7
benchmark41 conjunctive 263 5 17.1 68 7 14.15 3 3.70 3 3
benchmark42 conjunctive 334 6 54.38 to to to 3 3.33 3 3
benchmark43 conjunctive 214 2 45.04 to to to 7 3.42 3 3
benchmark44 disjunctive 40 4 24.71 24 4 16.5 3 3.34 3 3
benchmark45 disjunctive 51 3 5.23 56 16 7.19 7 3.31 3 7
benchmark46 disjunctive 194 9 23.17 122 32 19.91 7 3.41 3 7
benchmark47 linear 61 1 63.55 to to to 3 3.61 3 3
benchmark48 linear 297 3 25.93 34 4 17.03 3 3.77 3 3
benchmark49 linear 80 2 37.52 34 4 16.77 3 3.36 3 3
benchmark50 linear 90 2 13.63 22 2 14.83 3 3.51 3 3
benchmark51 polynomial 48 2 12.2 42 4 22 7 3.25 3 3
benchmark52 polynomial 180 4 69.99 to to to 7 7 7 7
benchmark53 polynomial 176 3 61.71 105 5 63 7 7 7 7
iteration. Comparing the number of samples generated for
each program, it can be observed that Z ILU with selective
sampling often generates more samples. This is expected as we
generate multiple samples for each call of selectiveSampling ,
whereas only one sample is generated during the veriÔ¨Åcation
phase. This is because sampling through veriÔ¨Åcation has, in
general, a high cost and we aim to avoid it as much as possible.
RQ3 The third research question is: does ZILUoutperform
existing state-of-the-art program veriÔ¨Åcation tools on verifying
these programs? Ideally, we would like to compare with thosetools reported in [ 48], [47], [46], [44], [19], [18]. Unfortunately,
those tools are not maintained. We instead compare Z ILUwith
four state-of-the-art tools on loop-invariant generation and
program veriÔ¨Åcation. In particular, Interproc [ 30] is a program
veriÔ¨Åer which generates invariants based on abstract interpre-
tation. In the experiments, it is set to use its most expressive
abstract domain, i.e., the reduced product of polyhedra and
linear congruences abstraction. CPAChecker [ 7] is a state-of-
the-art program veriÔ¨Åer. The CPAChecker which we use in this
work is the version used for SV-COMP 2017 [ 5]. Note that
CPAChecker supports a variety of veriÔ¨Åcation methods and it
790
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. is conÔ¨Ågured in the exact same way as in SV-COMP 20173.
BLAST is a software model checker based on counterexample-
guided abstraction reÔ¨Ånement [ 6]. Lastly, InvGen [ 25] is a tool
which aims to generate linear arithmetic invariants, using a
combination of static and dynamic analysis techniques.
The results are shown in the last 4 columns of Table I where
3means that the Hoare triple is veriÔ¨Åed and 7means either
it outputs no conclusive results or false positives. We remark
that because the tools use approaches which are different from
each other, the comparison should be taken with a grain of
salt. Interproc and InvGen are very efÔ¨Åcient in handling the
programs, i.e., within 1 second for each program, and thus we
skip the veriÔ¨Åcation time. BLAST is similarly efÔ¨Åcient except
that it timeouts in two cases. We show the timed taken by
CPAChecker in case it successfully veriÔ¨Åes the program.
We have the following observation based on the experiment
results. First, for all 53 programs, Z ILU is able to Ô¨Ånd a
loop-invariant which proves the Hoare triple. In comparison,
Interproc failed in 18 cases; CPAChecker failed in 7 cases;
BLAST failed in 8 cases; and InvGen failed in 10 cases.
We note that this comparison might be not fair, as we have
not compared other tools on the excluded programs. More
precisely, for the programs that are compatible with input
restrictions of Z ILU, ZILUworks best compared with other
tools. Secondly, existing tools often complement each other.
For instance, BLAST successfully proves all programs which
require disjunctive loop-invariant, whereas it failed in several
cases where a polynomial loop-invariant is required. In contrast,
programs which require disjunctive loop-invariants are often
challenging for other tools (except Z ILU). Moreover, due to the
dynamic nature, Z ILUoften requires more time. Nonetheless,
we consider that Z ILUis relatively efÔ¨Åcient. For all 53 programs,
ZILUÔ¨Ånishes the proof within 92 seconds.
V. R ELATED WORK
The closest related work are those guess-and-check ap-
proaches on invariant generation. In [ 48], the authors proposed
to generate samples through constraint solving and learn loop-
invariants based on SVM classiÔ¨Åcation. In comparison, Z ILU
learns more expressive invariants in the form of polynomial
inequalities or their disjunctions and conjunctions. More
importantly, we apply active learning with selective sampling
so as to overcome the limitation of too few samples or too
many guess-and-check iterations. In [ 47], the authors proposed
to apply PAC learning techniques for invariant generation. It
has been demonstrated that their approach may learn invariants
in the form of arbitrary boolean combinations of a given set of
propositions (under certain assumptions). In [ 46], the authors
developed a guess-and-check algorithm to generate invariants
in the form of the algebraic equation. It learns invariants
of polynomial form by operating the null space operation
on matrix. In [ 44], the authors proposed a framework for
generating invariants based on randomized search. In particular,
their approach has two phases. In the search phase, it uses
3We thank the help from a researcher in the SV-COMP evaluation team.randomized search to discover candidate invariants and uses a
checker to either prove or refute the candidate in the validate
phase. In [ 18], Pranav Garg et. al proposed to synthesize
invariants by learning from implications along with positive
and negative samples. They further extend their approach by
modifying existing decision tree classiÔ¨Åcation algorithm with
heuristics adopted from [ 20]. In this way, they could cope
with implication better and, as a result, handle invariants of
combination of conjunctions and disjunctions in theory. One
limitation of their work is that the terms in the decision tree
(e.g., the propositions) must be pre-deÔ¨Åned.
Compared with the above-mentioned work, Z ILUimproves
loop-invariant generation through active learning with selective
sampling, so as to avoid applying the invariant checker as
much as possible. To the best of our knowledge, Z ILUis the
Ô¨Årst to combine selective sampling with invariant inference. In
particular, in the guessing phase, we additionally adopt a learn-
and-reÔ¨Åne iteration which improves the invariant candidates
through classiÔ¨Åcation and selective sampling. In comparison,
other guess-and-check approaches solely rely on the checkers to
improve invariant candidates. Furthermore, we show Z ILUcan
be extended easily to learn disjunctive loop-invariants through
data partitioning and classiÔ¨Åcation.
Lastly, our approach can be extended to learn other forms of
classiÔ¨Åers classiÔ¨Åcation algorithms. For instance, in principle,
any arbitrary mathematical classiÔ¨Åers can be learnt using
methods like SVM with kernel methods [ 29], but interpreting
the output models is challenging. Nonetheless, we focus on
invariants in the form of polynomial inequalities or conjunc-
tions/disjunctions of polynomial inequalities in our evaluation.
The experiment results show that our approach effectively
learns loop-invariant for proving a set of benchmark programs
and complements the existing approaches.
Besides the guess-and-check approaches, some alternative
approaches have been proposed for invariant generation. Ex-
amples include those based on abstraction interpretation [ 13],
[36], [31], those based on counterexample-guided abstraction
reÔ¨Ånement [ 28], [3], [11] or interpolation [ 27], [34], [35], [33],
and those based on constraint solving and logical inference [ 25],
[12], [24], [17]. These approaches depend on constraint solving
and thus suffer from scalability. For instance, the work in [ 36],
[31], [25] is restricted to generate invariants in abstract domains
for which constraint solving is manageable.
VI. C ONCLUSION
In this work, we propose a systematic approach to learn loop-
invariants based a combination of selective sampling and guess-
and-check. As for future work, we would explore methods for
learning more expressive loop-invariants as well as methods
for discovering new features for our classiÔ¨Åcation.
ACKNOWLEDGEMENTS
We greatly appreciate the anonymous reviewers for their
insightful feedback on the early draft of this paper. This research
is supported by MOE research grant ‚ÄúT2MOE1704‚Äù.
791
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] Z ILUrepository. https://github.com/lijiaying/zilu, 2017.
[2]D. Babic, B. Cook, A. J. Hu, and Z. Rakamaric. Proving termination of
nonlinear command sequences. Formal Asp. Comput., 25(3):389‚Äì403,
2013.
[3]T. Ball and S. K. Rajamani. The SLAM toolkit. In CAV, pages 260‚Äì264,
2001.
[4]C. W. Barrett, R. Sebastiani, S. A. Seshia, and C. Tinelli. SatisÔ¨Åability
modulo theories. Handbook of satisÔ¨Åability, 185:825‚Äì885, 2009.
[5]D. Beyer. Software veriÔ¨Åcation with validation of results. In International
Conference on Tools and Algorithms for the Construction and Analysis
of Systems, pages 331‚Äì349. Springer, 2017.
[6]D. Beyer, T. A. Henzinger, R. Jhala, and R. Majumdar. The software
model checker blast. STTT, 9(5-6):505‚Äì525, 2007.
[7]D. Beyer and M. E. Keremoglu. Cpachecker: A tool for conÔ¨Ågurable
software veriÔ¨Åcation. In CAV, pages 184‚Äì190, 2011.
[8]B. E. Boser, I. M. Guyon, and V . N. Vapnik. A training algorithm
for optimal margin classiÔ¨Åers. In workshop on Computational learning
theory, pages 144‚Äì152. ACM, 1992.
[9]C. Cadar, D. Dunbar, and D. R. Engler. Klee: Unassisted and automatic
generation of high-coverage tests for complex systems programs. In
OSDI, volume 8, pages 209‚Äì224, 2008.
[10] H. Chen, C. David, D. Kroening, P. Schrammel, and B. Wachter.
Synthesising interprocedural bit-precise termination proofs (T). In ASE,
pages 53‚Äì64, 2015.
[11] E. M. Clarke, O. Grumberg, S. Jha, Y . Lu, and H. Veith. Counterexample-
guided abstraction reÔ¨Ånement for symbolic model checking. J. ACM,
50(5):752‚Äì794, 2003.
[12] M. Col√≥n, S. Sankaranarayanan, and H. Sipma. Linear invariant
generation using non-linear constraint solving. In CAV, pages 420‚Äì432,
2003.
[13] P. Cousot and N. Halbwachs. Automatic discovery of linear restraints
among variables of a program. In POPL, pages 84‚Äì96. ACM, 1978.
[14] S. Dasgupta. Coarse sample complexity bounds for active learning. In
NIPS, pages 235‚Äì242, 2005.
[15] L. De Moura and N. Bj√∏rner. Z3: An efÔ¨Åcient smt solver. In Tools and
Algorithms for the Construction and Analysis of Systems, pages 337‚Äì340.
Springer, 2008.
[16] E. W. Dijkstra. Guarded commands, nondeterminacy and formal
derivation of programs. Commun. ACM, 18(8):453‚Äì457, 1975.
[17] I. Dillig, T. Dillig, B. Li, and K. L. McMillan. Inductive invariant
generation via abductive inference. In OOPSLA, pages 443‚Äì456, 2013.
[18] P. Garg, C. L√∂ding, P. Madhusudan, and D. Neider. ICE: A robust
framework for learning invariants. In Computer Aided VeriÔ¨Åcation - 26th
International Conference, CAV 2014, Held as Part of the Vienna Summer
of Logic, VSL 2014, Vienna, Austria, July 18-22, 2014. Proceedings,
pages 69‚Äì87, 2014.
[19] P. Garg, D. Neider, P. Madhusudan, and D. Roth. Learning invariants
using decision trees and implication counterexamples. In Proceedings of
the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, POPL 2016, St. Petersburg, FL, USA, January
20 - 22, 2016, pages 499‚Äì512, 2016.
[20] P. Garg, D. Neider, P. Madhusudan, and D. Roth. Learning invariants
using decision trees and implication counterexamples. In ACM SIGPLAN
Notices, volume 51, pages 499‚Äì512. ACM, 2016.
[21] R. Gilad-Bachrach, A. Navot, and N. Tishby. Query by committee made
real. In NIPS, pages 443‚Äì450, 2005.
[22] B. S. Gulavani, S. Chakraborty, A. V . Nori, and S. K. Rajamani.
Automatically reÔ¨Åning abstract interpretations. In TACAS, pages 443‚Äì458.
Springer, 2008.
[23] S. Gulwani, S. Srivastava, and R. Venkatesan. Program analysis as
constraint solving. In PLDI, pages 281‚Äì292, 2008.
[24] S. Gulwani, S. Srivastava, and R. Venkatesan. Constraint-based invariant
inference over predicate abstraction. In VMCAI, pages 120‚Äì135, 2009.[25] A. Gupta and A. Rybalchenko. Invgen: An efÔ¨Åcient invariant generator.
InProceedings of 21st International Conference on Computer Aided
VeriÔ¨Åcation, pages 634‚Äì640, 2009.
[26] A. Gupta and A. Rybalchenko. Invgen: An efÔ¨Åcient invariant generator.
InComputer Aided VeriÔ¨Åcation, 21st International Conference, CAV
2009, Grenoble, France, June 26 - July 2, 2009. Proceedings, pages
634‚Äì640, 2009.
[27] T. A. Henzinger, R. Jhala, R. Majumdar, and K. L. McMillan. Abstrac-
tions from proofs. In POPL, pages 232‚Äì244, 2004.
[28] T. A. Henzinger, R. Jhala, R. Majumdar, and G. Sutre. Software
veriÔ¨Åcation with blast. In Model Checking Software, pages 235‚Äì239.
Springer, 2003.
[29] T.-M. Huang, V . Kecman, and I. Kopriva. Kernel based algorithms for
mining huge data sets, volume 1. Springer, 2006.
[30] B. Jeannet. Interproc analyzer for recursive programs with numerical
variables. http://pop-art.inrialpes.fr/interproc/interprocweb.cgi, pages
06‚Äì11, 2010.
[31] V . Laviron and F. Logozzo. Subpolyhedra: A (more) scalable approach
to infer linear inequalities. In VMCAI, pages 229‚Äì244, 2009.
[32] D. D. Lewis and W. A. Gale. A sequential algorithm for training text
classiÔ¨Åers. In SIGIR Forum, pages 3‚Äì12, 1994.
[33] S.-W. Lin, J. Sun, T. K. Nguyen, Y . Liu, and J. S. Dong. Interpolation
guided compositional veriÔ¨Åcation (t). In Automated Software Engineering
(ASE), 2015 30th IEEE/ACM International Conference on, pages 65‚Äì74.
IEEE, 2015.
[34] K. L. McMillan. Interpolation and sat-based model checking. In Computer
Aided VeriÔ¨Åcation, pages 1‚Äì13, 2003.
[35] K. L. McMillan. Lazy abstraction with interpolants. In Computer Aided
VeriÔ¨Åcation, pages 123‚Äì136, 2006.
[36] A. Min√©. The octagon abstract domain. Higher-Order and Symbolic
Computation, 19(1):31‚Äì100, 2006.
[37] M. Minsky and S. Papert. Perceptrons: An Introduction to Computational
Geometry, 2nd edition. The MIT Press, 1972.
[38] C. S. P ÀòasÀòareanu and W. Visser. A survey of new trends in symbolic
execution for software testing and analysis. Int. J. Softw. Tools Technol.
Transf., 11(4):339‚Äì353, Oct. 2009.
[39] J. Platt et al. Sequential minimal optimization: A fast algorithm for
training support vector machines. 1998.
[40] J. R. Quinlan. Induction of decision trees. Machine learning, 1(1):81‚Äì106,
1986.
[41] R. A. Ruff and T. G. Dietterich. What good are experiments? In
Proceedings of the Sixth International Workshop on Machine Learning
(ML 1989), pages 109‚Äì112, 1989.
[42] G. Schohn and D. Cohn. Less is more: Active learning with support
vector machines. In ICML, pages 839‚Äì846, 2000.
[43] B. Settles. Active Learning. Synthesis Lectures on ArtiÔ¨Åcial Intelligence
and Machine Learning. Morgan & Claypool Publishers, 2012.
[44] R. Sharma and A. Aiken. From invariant checking to invariant inference
using randomized search. In Computer Aided VeriÔ¨Åcation , pages 88‚Äì105.
Springer, 2014.
[45] R. Sharma, I. Dillig, T. Dillig, and A. Aiken. Simplifying loop invariant
generation using splitter predicates. In CAV, pages 703‚Äì719, 2011.
[46] R. Sharma, S. Gupta, B. Hariharan, A. Aiken, P. Liang, and A. V . Nori.
A data driven approach for algebraic loop invariants. In ESOP, pages
574‚Äì592, 2013.
[47] R. Sharma, S. Gupta, B. Hariharan, A. Aiken, and A. V . Nori. VeriÔ¨Åcation
as learning geometric concepts. In Static Analysis Symposium, pages
388‚Äì411, 2013.
[48] R. Sharma, A. V . Nori, and A. Aiken. Interpolants as classiÔ¨Åers. In
Computer Aided VeriÔ¨Åcation, pages 71‚Äì87. Springer, 2012.
[49] S. Tong and E. Y . Chang. Support vector machine active learning for
image retrieval. In Proceedings of the 9th ACM International Conference
on Multimedia, pages 107‚Äì118, 2001.
[50] S. Tong and D. Koller. Support vector machine active learning with
applications to text classiÔ¨Åcation. Journal of Machine Learning Research,
2:45‚Äì66, 2001.
792
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:29:03 UTC from IEEE Xplore.  Restrictions apply. 