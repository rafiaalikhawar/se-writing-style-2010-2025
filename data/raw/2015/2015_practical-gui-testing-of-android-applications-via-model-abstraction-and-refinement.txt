Practical GUI Testing of Android Applications via
Model Abstraction and Reﬁnement
1Tianxiao Gu1Chengnian Sun2Xiaoxing Ma2Chun Cao
2Chang Xu2Y uan Y ao3Qirun Zhang2Jian Lu1,4Zhendong Su
1University of California, Davis, USA3Georgia Institute of Technology, USA4ETH Zurich, Switzerland
2State Key Laboratory for Novel Software Technology, Nanjing University, China
{txgu,cnsun }@ucdavis.edu, {xxm,caochun,changxu,y.yao,lj }@nju.edu.cn, qrzhang@gatech.edu, zhendong.su@inf.ethz.ch
Abstract —This paper introduces a new, fully automated model-
based approach for effective testing of Android apps. Different
from existing model-based approaches that guide testing with a
static GUI model ( i.e., the model does not evolve its abstraction
during testing, and is thus often imprecise), our approach dynam-
ically optimizes the model by leveraging the runtime information
during testing. This capability of model evolution signiﬁcantly
improves model precision, and thus dramatically enhances the
testing effectiveness compared to existing approaches, which our
evaluation conﬁrms. We have realized our technique in a practical
tool, A PE. On 15 large, widely-used apps from the Google Play
Store, A PEoutperforms the state-of-the-art Android GUI testing
tools in terms of both testing coverage and the number of detected
unique crashes. To further demonstrate A PE’s effectiveness and
usability, we conduct another evaluation of A PEon 1,316 popular
apps, where it found 537 unique crashes. Out of the 38 reported
crashes, 13 have been ﬁxed and 5 have been conﬁrmed.
Index T erms —GUI testing; mobile app testing; CEGAR;
I. I NTRODUCTION
Mobile application (app) testing heavily involves human
effort [1]. For example, a human tester writes code to simulate
GUI actions ( e.g. , clicking a button) to drive the execution of
Android apps [2]. This process is not only time-consuming
but also error-prone. Moreover, when the GUI changes, the
tester has to make nontrivial modiﬁcations to their existing
test scripts [1]. To mitigate these problems, many automated
GUI testing techniques have recently been proposed [3], [4].
Automated GUI Testing for Android Apps. Monkey [5],
a GUI fuzzing tool developed by Google, generates purely
random events [6] as test input without any guidance. Thus,
it does not guarantee steering test exploration to uniformly
traverse the GUIs ( i.e., low activity coverage [7]), and
cannot incorporate user-deﬁned rules such as inputting a
password [7] or prohibiting logging out. Additionally, the
generated events are low-level, with hard-coded coordinates,
and usually excessively long, which complicates reproduction
and debugging [3], [8]. To overcome Monkey’s limitations,
techniques such as evolutionary algorithms [9] and symbolic
execution [10] have been adapted to guide input generation.
However, such approaches are computationally expensive and
do not yet scale in practice [11].
An alternative approach for performing Android GUI testing
is model-based [4], [12], [13]. A model is usually a ﬁnite state
machine, where each state has a set of model actions, andeach transition between states is labeled with a model action
of the source state. In practice, almost no app comes with
a model. Existing testing tools thus build a GUI-based model
by abstracting/mapping GUI actions to model actions and GUI
views to states, respectively.
A model offers at least three types of beneﬁts to GUI testing.
First, a model can be used to guide the exploitation of an app.
A testing tool can traverse the model using speciﬁc guidance
to systematically generate action sequences and then replay the
action sequences to test the app [4]. Second, a model-based
testing tool generates input sequences composed of high-level
model actions rather than low-level events, which can facilitate
replaying [14]. Third, a proper abstraction can be applied to the
model, which in turn can help mitigate the explosion of GUI
actions. Through abstraction, many GUI actions with the same
behavior can be mapped to the same model action. Since these
GUI actions behave the same, the testing tool does not need to
exercise each of them and can instead select a representative
GUI action among them when executing the model action.
State Abstraction. Mapping each GUI action to a model
action is the most critical step in state abstraction. A state
is usually identiﬁed by the set of its model actions since
states with the same set of model actions can be merged [15].
All existing model-based approaches apply a static abstraction
based on certain heuristics throughout the testing of an app.
Designing a proper abstraction is challenging. First, if the
model is overly ﬁne-grained, the testing tool cannot systemat-
ically explore the model due to state explosion . Second, if
the model is overly coarse-grained, the testing tool cannot
gather sufﬁciently accurate knowledge on model actions to
realize effective guidance ( i.e., difﬁcult to replay model action
sequences on the tested app). In particular, an ineffective
abstraction may map multiple GUI actions with different
behaviors ( e.g. , leading to different target states) to the same
model action. Therefore, a model action cannot be replayed
as expected if the testing tool chooses a GUI action behaving
differently from the one chosen during model construction.
Accordingly, all subsequent actions in the sequence depending
on the model action can neither be successfully replayed.
Our Approach. This paper proposes A PE, a new, practical
model-based automated GUI testing technique for Android
apps via effective dynamic model abstraction. At the be-
ginning, a default abstraction is used to initiate the testing
2692019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 ©2019 IEEE
DOI 10.1109/ICSE.2019.00042
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. process. This initial abstraction may be ineffective. Based
on the runtime information observed during testing, A PE
gradually reﬁnes the model by searching for a more suitable
abstraction, an abstraction that effectively balances the size
and precision of the model. This dynamic nature distinguishes
our approach from all existing state-of-the-art techniques as
they rely solely on static abstractions. Instead of operating on a
ﬁxed abstraction granularity, our approach dynamically adjusts
the granularity as needed. Speciﬁcally, A PErepresents the dy-
namic abstraction with a decision tree , and tunes it on-the-ﬂy
with the feedback obtained during testing. This decision tree-
based representation of model abstractions greatly improves
testing effectiveness, as demonstrated in our comprehensive
evaluation (Section IV).
We compared A PE with the state-of-art testing tools ( i.e.,
Monkey [5], S APIENZ [3], and S TOA T [4]) on 15 large,
widely-used apps from the Google Play Store. A PE comple-
ments the state-of-art-tools (see Section IV -C). A PEachieved
higher code coverage and detected more unique crashes ( i.e.,
unique stack traces deﬁned in Section IV -C) than the other
tools on the 15 benchmark apps. Speciﬁcally, A PEconsistently
resulted in 26–78%, 17–22% and 14–26% relative improve-
ments over the three tools in terms of average activity cover-
age, method coverage, and instruction coverage, respectively.
Though these apps have been well tested, within one hour
APEmanaged to ﬁnd 62 unique crashes by testing their GUIs,
whereas Monkey found 44, S APIENZ 40, and S TOA T 31. To
further demonstrate the usability and effectiveness of A PE,w e
conducted another large-scale evaluation on 1,316 apps from
the Google Play Store. A PEfound 537 crashes from 281 apps
in total. We reported 38 crashes to developers with detailed
steps to reliably reproduce these crashes, where 13 crashes
have already been ﬁxed and 5 crashes have been conﬁrmed
(ﬁxes pending).
Contributions. This paper makes the following contributions:
•We propose a novel, fully automated, model-based tech-
nique for Android GUI testing. The major difference
of our approach from existing techniques is the ability
to dynamically evolve GUI models toward ones that
discard all irrelevant GUI details while faithfully reﬂect
the runtime states of the app under test.
•We realize dynamic model abstraction via a novel type of
decision trees, which can expressively represent a wide
range of abstractions and thus enables A PEto effectively,
dynamically reﬁne and coarsen abstractions to balance
model precision and size.
•Our extensive evaluation demonstrates that A PE out-
performs the state-of-the-art tools. It can automatically
explore many more GUIs ( i.e., activities) than the other
tools. Therefore, according to our evaluation results, it
not only increased code coverage but also found more
unique crashes.
•We implement the proposed technique into a practical
tool, A PE, and make it publicly available.1
1http://gutianxiao.com/apePaper Organization. The remainder of this paper is orga-
nized as follows. Section II introduces necessary background.
Section III presents the approach. Section IV details our
extensive evaluation. Section V surveys related work, and
Section VI concludes.
II. B ACKGROUND
This section introduces relevant background on model-based
Android GUI testing and its challenges.
A. GUI of Android Apps
In an Android app, an activity [16] is a composition of
widgets . These widgets are organized into a tree-like structure,
named GUI tree in this paper [4], [12], [17]. A widget can be
a button, a text box, or a container with a layout. It supports
various GUI actions such as clicking and swiping. A widget
has four categories of attributes describing its type (e.g. , class),
appearance (e.g. , text), functionalities (e.g. , clickable and
scrollable), and the designated order among sibling widgets
(i.e., index). Each attribute is a key-value pair. We use i,c,
andtto denote the key of an index, class, and text attribute,
respectively. For example, an index attribute whose value is 0
can be denoted by i=0.
(a)
 (b)
...
...
0,ListView,
0,TextView,XLSX
1,TextView,PPTX
2,TextView,DOCX
...Ti
wi
0
wi
1
wi
2
wi
3
(c)...
...
0,ListView,
0,TextView,DOCX
1,TextView,XLSX
2,TextView,PPTX
...Tj
wj
0
wj
1
wj
2
wj
3
(d)
Fig. 1: Figs. 1a and 1b are two GUI snapshots of Google
Drive. Figs. 1c and 1d are their corresponding GUI trees.
A GUI tree Tis a rooted, ordered tree where each node
wis a GUI widget that has a set of attributes, denoted by
attributes (w). The Android SDK has provided a tool [18]
to obtain the GUI tree of an activity. Figs. 1c and 1d show
270
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. the corresponding (simpliﬁed) GUI trees for Figs. 1a and 1b,
respectively. Since only bold parts are of interest in this
paper, we assumed that TiandTjare rooted at wi
0andwj
0,
respectively.
B. Attribute Path
A testing tool needs to properly identify widgets to accu-
mulate testing knowledge on them. The memory address of
the runtime object representing the widget cannot be used to
identify the widget because the same GUI may be created
and disposed many times. In a GUI tree T, given a widget
wn,anode path ω=/angbracketleftw1,w2,···,wn/angbracketrightis a sequence of tree
nodes ( i.e., widgets) that are on the traversal path from one
of its ancestors w1town. This node path is referred to as
anabsolute node path if the ﬁrst node w1is the root of the
tree, and otherwise a relative node path , which is similar to
the concept of absolute and relative ﬁle paths in ﬁle systems.
An absolute node path uniquely identiﬁes a widget in the tree.
For example, wi
1’s unique node path in Fig. 1c is /angbracketleftwi
0,wi
1/angbracketright.
Deﬁnition 1 ( Attribute Path): Given a widget wnand one
of its node paths ω=/angbracketleftw1,w2,···,wn/angbracketright, an attribute path
π=/angbracketlefta1,a2,···,an/angbracketrightis a projection of ω, such that,
∀n
i=1ai⊆attributes (wi).
That is, each aiis a subset of the attributes of the correspond-
ing widget wi.
Deﬁnition 2 ( Full Attribute Path): An attribute path π=
/angbracketlefta1,a2,···,an/angbracketrightis a full attribute path if
•itsω=/angbracketleftw1,w2,···,wn/angbracketrightis an absolute node path, and
•∀n
i=1ai=attributes (wi).
To distinguish from an arbitrary attribute path, we use σ
to denote a full attribute path. In this paper, a widget can
be uniquely identiﬁed (in the tree) by its full attribute path,
because the order of the widgets on the full attribute path
resembles the hierarchy of widgets in the tree, and the index
attribute of each widget determines the unique location of
the widget among its siblings. Therefore, a GUI tree Tcan
essentially be represented with, and is equivalent to, the set
of full attribute paths of all its widgets. Table I shows the full
attribute path for each widget in Fig. 1.
Deﬁnition 3 ( Attribute Path Reduction): An attribute path
reducer is a function Rthat takes as input an attribute path
π=/angbracketlefta1,a2,···,an/angbracketright, and returns a new attribute path π/prime=
/angbracketleftbm,···,bn/angbracketright, such that,
1≤m≤n∧∀n
i=mbi⊆ai.
In other words, π/primeis a sufﬁx of π, and each element of π/primeis
a subset of the corresponding element of π.
Widget Assumption. A GUI action is identiﬁed by its widget
σand action type τ,i.e.,/angbracketleftσ,τ/angbracketright. For a simpler presentation, we
assume that every widget supports only one GUI action and
omit action types and functionality attributes throughout the
paper. The relation between widgets and GUI actions becomes
bijective, and we use them ( i.e.,σand/angbracketleftσ,τ/angbracketright) interchangeably.
Note that this assumption is for illustration only — our
technique also supports widgets with multiple actions by using
functionality attributes (see Section III-B).GUI Model
AbstractionExploration Strategy
Model-Based
T esting T oolAppinject events
dump a GUI tree
Fig. 2: The typical workﬂow of model-based GUI testing.
C. Model-Based Android GUI Testing
Fig. 2 depicts the typical workﬂow of a model-based testing
tool [4], [12], [15], [17]. Such a tool interacts iteratively with
the app under test. Initially, the testing tool starts with an
empty state machine as the model. During each iteration, the
testing tool (1) obtains the current GUI tree of the app, (2)
identiﬁes an existing, corresponding state, or creates a new
state for this GUI tree, and (3) picks a model action and
determines a concrete GUI action to interact with the app.
A model-based testing tool aims at exploring the app to
discover new widgets and exploiting the app to exercise
interactions among the discovered widgets. In practice, modern
Android apps usually have a large number of widgets. To
mitigate this scalability challenge, a model-based testing tool
further attempts to identify equivalent GUI actions and abstract
them to a same model action to reduce the search space.
However, it is nontrivial to determine whether two widgets
are equivalent. A full attribute path usually contains irrelevant
information, which prevents two “semantically” equivalent
widgets from being discovered. For example, wi
1in Fig. 1c
andwj
2in Fig. 1d are essentially equivalent but have different
full attribute paths, as shown in Table I. Relying solely on
full attribute paths, a testing tool will view the two widgets
differently, and the model size is consequently increased.
State Abstraction. State abstraction refers to the procedure
that identiﬁes equivalent GUI trees and actions, and maps them
to the same state and the same model action, respectively. In
general, state abstraction is realized based on the similarity of
full attribute paths of GUI actions [4], [12], [15]. Speciﬁcally,
state abstraction determines two GUI actions as equivalent
if (1) they have the same action type and (2) their full
attribute paths can be reduced to the same attribute path
πby removing irrelevant attributes under certain reduction
rules. The corresponding model action is denoted as π(/angbracketleftπ,τ/angbracketright
when action type τis considered). Similarly, state abstraction
determines two GUI trees as equivalent if all of their GUI
actions can be reduced to the same set of attribute paths. The
corresponding model state is denoted as the set of all model
actions ( i.e., the set of attribute paths π) [15].
However, an automated testing tool has no prior knowledge
of the apps under test and can only deﬁne reduction rules
heuristically. Note that a testing tool can apply different
reductions to different widgets to achieve a proper abstrac-
tion granularity. For example, S TOA T [4] assumes that child
widgets of ListView behave equivalently and realizes this
heuristic in its state abstraction. Speciﬁcally, S TOA T removes
all attributes from child widgets of ListView , and removes
only types and texts from other widgets. For the examples in
271
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. T ABLE I: Model actions ( i.e., reduced attribute paths) by different abstractions.
Tree Widget Full Attribute Path STOA T AMOLA (C-Lv4) AMOLA (C-Lv5) Text-Only
Tiwi
0 /angbracketleft{i=0,c=LV,t=∅}/angbracketright /angbracketleft{ i=0}/angbracketright /angbracketleft{ i=0}/angbracketright /angbracketleft{i=0,t=∅}/angbracketright /angbracketleft{ i=0}/angbracketright
wi
1/angbracketleft{i=0,c=LV,t=∅},{i=0,c=TV,t=XLSX}/angbracketright /angbracketleft{ i=0},∅/angbracketright/angbracketleft { i=0},{i=0}/angbracketright /angbracketleft{ i=0,t=∅},{i=0,t=XLSX}/angbracketright /angbracketleft{ i=0},{t=XLSX}/angbracketright
wi
2/angbracketleft{i=0,c=LV,t=∅},{i=1,c=TV,t=PPTX}/angbracketright /angbracketleft{ i=0},∅/angbracketright/angbracketleft { i=0},{i=1}/angbracketright /angbracketleft{ i=0,t=∅},{i=1,t=PPTX}/angbracketright /angbracketleft{ i=0},{t=PPTX}/angbracketright
wi
3/angbracketleft{i=0,c=LV,t=∅},{i=2,c=TV,t=DOCX}/angbracketright /angbracketleft{ i=0},∅/angbracketright/angbracketleft { i=0},{i=2}/angbracketright /angbracketleft{ i=0,t=∅},{i=2,t=DOCX}/angbracketright /angbracketleft{ i=0},{t=DOCX}/angbracketright
Tjwj
0/angbracketleft{i=0,c=LV,t=∅}/angbracketright /angbracketleft{ i=0}/angbracketright /angbracketleft{ i=0}/angbracketright /angbracketleft{i=0,t=∅}/angbracketright /angbracketleft{ i=0}/angbracketright
wj
1/angbracketleft{i=0,c=LV,t=∅},{i=0,c=TV,t=DOCX}/angbracketright /angbracketleft{ i=0},∅/angbracketright/angbracketleft { i=0},{i=0}/angbracketright /angbracketleft{ i=0,t=∅},{i=0,t=DOCX}/angbracketright /angbracketleft{ i=0},{t=DOCX}/angbracketright
wj
2/angbracketleft{i=0,c=LV,t=∅},{i=1,c=TV,t=XLSX}/angbracketright /angbracketleft{ i=0},∅/angbracketright/angbracketleft { i=0},{i=1}/angbracketright /angbracketleft{ i=0,t=∅},{i=1,t=XLSX}/angbracketright /angbracketleft{ i=0},{t=XLSX}/angbracketright
wj
3/angbracketleft{i=0,c=LV,t=∅},{i=2,c=TV,t=PPTX}/angbracketright /angbracketleft{ i=0},∅/angbracketright/angbracketleft { i=0},{i=2}/angbracketright /angbracketleft{ i=0,t=∅},{i=2,t=PPTX}/angbracketright /angbracketleft{ i=0},{t=PPTX}/angbracketright
*LV andTV are abbreviations for ListView andTextView , respectively.
Fig. 1, S TOA T maps all child widgets of ListView to the
same model action ( i.e.,/angbracketleft{i=0},∅/angbracketright). As shown in Table I
and Fig. 3a, S TOA T assigns both Figs. 1a and 1b to the same
state ( i.e.,1) since they have the same set of model actions.
In contrast, AMOLA supports ﬁve static abstractions and its
C-Lv5 criterion [12] takes into account both the indices and
text content, and identiﬁes a model action for each widget.
As shown in Table I and Fig. 3c, AMOLA assigns Figs. 1a
and 1b to two states, i.e.,3and 4, respectively.
1WP
X/angbracketleft{i=0},∅/angbracketright
2WP
X/angbracketleft{i=0},{i=0}/angbracketright/angbracketleft{i=0},{i=2}/angbracketright
/angbracketleft{i=0},{i=1}/angbracketright
5WP
X /angbracketleft{i=0},{t=DOCX}/angbracketright/angbracketleft{i=0},{t=PPTX}/angbracketright
/angbracketleft{i=0},{t=XLSX}/angbracketright
3
X
PW/angbracketleft{i=0,t=∅},{i=0,t=XLSX}/angbracketright
/angbracketleft{i=0,t=∅},{i=1,t=PPTX}/angbracketright/angbracketleft{i=0,t=∅},{i=2,t=DOCX}/angbracketright
4/angbracketleft{i=0,t=∅},{i=0,t=DOCX}/angbracketright
/angbracketleft{i=0,t=∅},{i=1,t=XLSX}/angbracketright
/angbracketleft{i=0,t=∅},{i=2,t=PPTX}/angbracketright(a) S TOA T (b) AMOLA (C-Lv4)
(c) AMOLA (C-Lv5)(d) Text-Only
Fig. 3: Partial models. Each circle is a state and each edge is a
state transition labeled with a model action. States W,Xand
Pare w.r .t. GUI trees of ﬁle viewers for DOCX, XLSX and
PPTX, respectively. Other states are w.r .t. GUI trees in Fig. 1.
Model actions of ListView (i.e.,wi
0andwj
0) are omitted.
A good state abstraction should balance trade-offs between
the model size and model precision. On one hand, the state
abstraction should be coarse to avoid state explosion by
tolerating differences of GUIs that are irrelevant to testing.
For example, AMOLA builds very ﬁne-grained models, but
the models can quickly explode in size as they incorporate
much testing-irrelevant GUI information. In constrast, S TOA T
does not have this problem as it builds a coarse-grained model.
On the other hand, the state abstraction should precisely
model the runtime states of an app in order to interact with the
app. For example, AMOLA (C-Lv4) keeps indices only, and
thus can distinguish different ﬁle items in the same GUI but
not among different GUIs. S TOA T cannot distinguish different
ﬁle items even in the same GUI. As shown in Fig. 3a,
the model action /angbracketleft{i=0},∅/angbracketrightactually represents a set of
inequivalent widgets and leads to three different target states
(i.e.,W,Xand P). When the guidance in S TOA T prompts
it to open a DOCX ﬁle ( i.e., reach the state W), the actuallyopened ﬁle may be PPTX or XLSX ( i.e., the actual target states
may be Xor P). This divergent behavior will misguide the
testing tool.
A proper way to identify model actions here is to include
the ﬁle name (text) only. this state abstraction identiﬁes three
actions and a single state ( i.e.,5in Fig. 3d) for GUIs in
Figs. 1a and 1b and also avoids mapping inequivalent widgets
to the same model action. Unfortunately, none of the existing
techniques supports such a proper abstraction. To address
these difﬁculties, we propose a technique that can dynamically
optimize the abstraction during testing. Speciﬁcally, we start
the testing from a default abstraction. During the testing, we
systematically and efﬁciently improve the abstraction toward
the one that balances the size and precision of the model.
III. A PPROACH
A. Model
Deﬁnition 4 ( Model): A model M in A PE is a tuple
(S,A,T,L), where
•S, the set of states. S∈S is a set of attribute paths.
•A, the set of model actions. Each model action π∈A is
an attribute path, and A=/uniontext
S∈SS.
•T:S×A×S , the set of state transitions. Each transition
(S,π,S/prime)has a source state S, a target state S/prime, and a
transition label that is a model action π.
•L: the abstraction function, which reduces a full attribute
pathσto an attribute path π,i.e.,L(σ)=π.
We also record every GUI transition between iterations. Each
GUI transition is a tuple of (T,σ,T/prime), whereTandT/primeare
GUI trees and σis the chosen GUI action for simulation.
To facilitate presentation, we derive two variants from L,
LLandLL, to process a GUI tree and a GUI transition,
respectively. Given a GUI tree T(i.e., a set of full attribute
paths),LLcan be used to ﬁnd its corresponding state S,
S=LL(T)={π|π=L(σ)∧σ∈T}.
And given a GUI transition (T,σ,T/prime),LLcan be used to ﬁnd
its corresponding state transition (S,π,S/prime),
(S,π,S/prime)=LL((T,σ,T/prime)) = (LL(T),L(σ),LL(T/prime)).
Algorithm 1 sketches the workﬂow of model-based testing
shown in Fig. 2. Initially, the model M is empty. At the
beginning of each iteration, Sandπrefer to the state and
the chosen model action in the previous iteration. Next,
272
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. Algorithm 1: Model construction.
Input: Testing budget B, initial abstraction function L.
Output: The model M.
1(M,S,π )←((∅,∅,∅,L),∅,∅) ⊿Initialization.
2whileB> 0do
3B←B−1 ⊿Decrease the testing budget.
4T/prime←CaptureGUITree() ⊿Useuiautomator .
5M←UptAndOptModel( M,S,π,T/prime)⊿See Algorithm 2.
6S←LL(T/prime) ⊿Get the current state.
7π←SelectAndSimulateAction( S)⊿See Section III-D.
8returnM
function UptAndOptModel adds the state LL(T/prime)and the
state transition (S,π,LL(T/prime))to the model and also attempts
to optimize the model as needed. Finally, we update S
with the current state LL(T/prime)and determine a new model
actionπonLL(T/prime). The model action is determined with
the function SelectAndSimulateAction , which will be
informally described in Section III-D. When executing π,w e
ﬁrst determine a set of GUI actions that map to π(i.e.,
{σ|σ∈T∧L(σ)=π}) and then randomly choose one of
them to simulate.
B. Dynamic Abstraction Functions
As aforementioned in Section I, the key novelty of A PEis
its dynamic abstraction function that can dynamically adap-
t/change the model abstraction to balance model precision
and size based on runtime information. However, realizing
dynamic abstraction functions is nontrivial. First, a dynamic
abstraction function should be adaptable at runtime. All
previous approaches use a statically crafted abstraction func-
tion with a ﬁxed abstraction granularity. Hence, adapting the
abstraction needs to change the source code implementing
the static abstraction function. Second, a dynamic abstraction
function should be generalizable . For the examples in Fig. 1,
an abstraction function should apply to not only TiandTj
but also any GUI trees after reordering ﬁles. We cannot simply
use a hash map between full attribute paths and attribute paths
as the abstraction function, because new GUI trees and new
attribute paths are continuously being discovered when the
testing progresses and they are not in the hash map of such an
abstraction function. Third, a dynamic abstraction should also
behuman-interpretable so that users can incorporate critical
rules to further improve the dynamic abstraction.
Different from existing approaches, we represent an abstrac-
tion function as a decision tree that determines a reducer for a
givenσ. A decision tree is a rooted tree, where each node is a
reducerRand each edge (branch) is labeled with an attribute
pathπ. Here,πis called the selector of the branch.
Selector. A branch selects a full attribute path σifσcan be
reduced to the selector πof the branch. Given a GUI tree T,
πselects a set of full attribute paths in Tthat can be reduced
toπ,i.e.,{σ|σ∈T∧∃R:R(σ)=π}.
Decision Procedure. To determine the reducer for the given
σ, we start from the root node of the decision tree and check
whetherσcan be selected by any branch of the current node. If
yes, we move to the target node nof the branch and continueto recursively check the branches of n. Otherwise, the reducer
ofnis used as the output to reduce σ. Andnis referred as
output node in later discussions. As a function, an important
property of the decision tree is that it should determine one
and only one reducer for σ. To guarantee this property, we
enforce that any σmust be selected by at most one branch.
Reducer. A reducer is the aggregation of a set of primitive
reducers . We ﬁrst deﬁne two types of primitive reducers.
Deﬁnition 5 ( Local Reducer): LetAdenote a set of
attribute keys. Given a full attribute path σ=/angbracketlefta1,a2,...,a n/angbracketright,
a local reducer RAremoves a1,a2,...,a n−1and retains
attributes in Aforan.
RA(σ)=/angbracketleft{(k,v)|(k,v)∈an∧k∈A}/angbracketright
In this paper, we use four types of local reducers: Rcto select
the type attributes, Rtto select the appearance attributes, Ri
to select the index attribute, and R∅to select no attribute.
Deﬁnition 6 ( Parent Reducer): Given a full attribute path
σ=/angbracketlefta1,a2,...,a n/angbracketright, a parent reducer Rpreuses the output
of the parent full attribute path /angbracketlefta1,a2,...,a n−1/angbracketrightand retains
nothing for an.
Rp(σ)=L(/angbracketlefta1,a2,...,a n−1/angbracketright)⊕R∅(σ)
where⊕is the concatenation of sequences.
Deﬁnition 7 ( Reducer Aggregation): Given a full attribute
pathσ=/angbracketlefta1,a2,...,a n/angbracketrightand two reducers RandR/prime, suppose
thatR(σ)=/angbracketleftbm,bm+1,...,b n/angbracketright,R/prime(σ)=/angbracketleftck,ck+1,...,c n/angbracketright
andm≤k. The aggregation R/multicloseright /multicloseleftR/prime(σ)is the reverse element-
wise union of R(σ)andR/prime(σ):
R/multicloseright /multicloseleftR/prime=/angbracketleftbm,bm+1,...,b k∪ck,bk+1∪ck+1,...,b n∪cn/angbracketright.
We have deﬁned ﬁve primitive reducers in this paper and
obtain 24reducers in total (denoted by R) sinceR∅/multicloseright /multicloseleftR=R.
We also deﬁne a partial order over all reducers in R.W es a y
that a reducer R/primeis ﬁner than another reducer R(i.e.,R/prime⊏R)
ifR/primeconsists of more primitive reducers than R.
Multiple Action Types. Recall that we have assumed that
a widget supports only one action type in Section II-A. To
support multiple action types, we can represent a GUI action
as/angbracketleftσ,τ/angbracketrightand a model action as /angbracketleftπ,τ/angbracketright. A selector πselects all
/angbracketleftσ,τ/angbracketrightin which σcan be reduced to π. The decision tree can
still be used to realize L(/angbracketleftσ,τ/angbracketright)=/angbracketleftπ,τ/angbracketright.
Ri/multicloseright /multicloseleftRp Rp/angbracketleft{c=LV},∅/angbracketright
Fig. 4: The decision tree of S TOA T ’s abstraction function ( Ls).
Example. The decision tree in Fig. 4 implements S TOA T ’s
abstraction function (denoted by Ls), which we will use as the
starting abstraction function to illustrate dynamic optimization
of our approach in Section III-C. Take widgets wi
0andwi
1in
Fig. 1c as examples. LsassignsRi/multicloseright /multicloseleftRptowi
0becausewi
0
isnot a child of a ListView and cannot be selected by
/angbracketleft{c=LV},∅/angbracketright.LsassignsRptowi
1becausewi
1is a child
of aListView and can be selected by /angbracketleft{c=LV},∅/angbracketright. Since
wi
0is the root and has no parent, Rpselects nothing for it.
273
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. The attributes path of wi
0is actually created by Ri, which is
/angbracketleft{i=0}/angbracketright. Sincewi
1is a child of wi
0,Rpreuses/angbracketleft{i=0}/angbracketrightfor
wi
1. The ﬁnal attribute path of wi
1is/angbracketleft{i=0},∅/angbracketright.
C. Optimizing Abstraction Functions
APE starts with an initial abstraction function and contin-
uously reﬁnes and coarsens the abstraction function toward a
proper model precision. Speciﬁcally, reﬁnement either replaces
the reducer Rof certain leaf output node in the decision tree
with a not coarser reducer R/prime(i.e.,R/prime/negationslash⊐R) or inserts a new
branch with a ﬁner reducer R/prime(i.e.,R/prime⊏R) to certain output
node, while coarsening reverts the current abstraction function
Lto the previous one L/primethatLis reﬁned from. Hence,
the initial abstraction function represents the lower bound of
abstraction and should be coarse enough.
An extremely coarse abstraction function would map all
GUI actions to the same model action and all GUI trees to
the same state. Such a function builds a very simple model,
and this model does not have non-deterministic transitions,
because every GUI tree is mapped to the same state. To avoid
this trivial case, we ﬁrst reﬁne the abstraction function until
no model action abstracts more than αGUI actions. Second,
a state transition (S,π,S/prime)isnon-deterministic if there is an-
other transition (S,π,S/prime/prime), whereS/prime/negationslash=S/prime/prime. Such a pair of non-
deterministic transitions usually indicates that the abstraction
of the source state Sor the model action πis overly coarse and
needs to be reﬁned. Hence, we reﬁne the abstraction function
aiming at eliminating every non-deterministic transition. Third,
the previous reﬁnement monotonically increases the model
size, and may eventually lead to a state explosion. Thereby,
we revert the abstraction function Lto the previous one L/primeif
Lreﬁnes a state created by L/primeintoβnew states. Here, αand
βare two conﬁgurable threshold values.
Ri/multicloseright /multicloseleftRpLs
Rp/angbracketleft{c=LV},∅/angbracketrightRi/multicloseright /multicloseleftRpL1
Rp
Ri/multicloseright /multicloseleftRp/angbracketleft{c=LV},∅/angbracketright
/angbracketleft{i=0},∅/angbracketrightRi/multicloseright /multicloseleftRpL2
Rp
Ri/multicloseright /multicloseleftRt/multicloseright /multicloseleftRp/angbracketleft{c=LV},∅/angbracketright
/angbracketleft{i=0},∅/angbracketrightRi/multicloseright /multicloseleftRpL3
Rp
Rt/multicloseright /multicloseleftRp/angbracketleft{c=LV},∅/angbracketright
/angbracketleft{i=0},∅/angbracketright1 234
Fig. 5: Example of the optimization of abstraction functions.
Example. Fig. 5 depicts the evolution from Stoat’s abstraction
function Lsto the text-only abstraction function ( i.e.,L3)
discussed in Section II-C. As shown in Table I and Fig. 3a,
suppose that A PE visits the model action /angbracketleft{i=0},∅/angbracketrightonly
twice, on TiandTj, and simulates two GUI actions to click
widgetswi
2andwj
1, respectively. Since clicking wi
2andwj
1
leads to two different target states ( i.e.,Pand W), A PEcan
detect that /angbracketleft{i=0},∅/angbracketrightis non-deterministic. Suppose that A PE
prefers to use Ri/multicloseright /multicloseleftRpto reﬁne/angbracketleft{i=0},∅/angbracketrightand mapwi
2andwj
1
to/angbracketleft{i=0},{i=1}/angbracketrightand/angbracketleft{i=0},{i=0}/angbracketright, respectively. Since the
initial decision tree may incorporate user-deﬁned rules, A PE
does not modify its nodes ( i.e., rectangle nodes) and inserts a
new branch into Lsto createL1.L1only temporally eliminate
the non-deterministic transitions, because A PE cannot detectthat/angbracketleft{i=0},{i=1}/angbracketrightor/angbracketleft{i=0},{i=0}/angbracketrightis non-deterministic
when both /angbracketleft{i=0},{i=1}/angbracketrightand/angbracketleft{i=0},{i=0}/angbracketrighthave a single
transition each, i.e., by clicking wi
2andwj
1, respectively. A PE
will detect the non-determinism (see Fig. 3b) when there are
more transitions. Suppose that A PE further reﬁnes L1toL2
with a ﬁner reducer Ri/multicloseright /multicloseleftRt/multicloseright /multicloseleftRp. Similar to AMOLA C-
Lv5,L2cannot tolerate ﬁle reordering and leads to the state
explosion. Then, A PE revertsL2toL1and blacklists L2.A
later reﬁnement will reﬁne L1toL3, the text-only abstraction
discussed in section II-C.
Algorithm 2: Update and optimize the model.
1Function UptAndOptModel( M=(S,A,T,L),S,π,T/prime)
Input: The new GUI tree T/prime, the model M=(S,A,T,L), the
previous state S, and the previous action π.
Output: The new model.
2S/prime←LL(T/prime)
3M←(S∪{S/prime},A∪S/prime,T∪ {(S,π,S/prime)},L)
4 repeatM←ActionRefinement( M,T/prime)untilM is not updated
5M←StateCoarsening( M,T/prime)
6 return StateRefinement( M,(S,π,S/prime))
7Function ActionRefinement( M=(S,A,T,L),T/prime)
8 foreachπ/prime∈LL(T/prime)do
9 if|L(π/prime)∩T/prime|>α then
10 R←GetReducer( π/prime)
11 foreachR/prime∈{R/prime|R/prime∈R∧R/negationslash⊏R/prime}do
12 L/prime←L∪{ R→R/prime}|L∪{ (R,π/prime,R/prime)}
13 Π←{ L/prime(σ)|σ∈L(π/prime)∩T/prime}
14 if|Π|>1then
15 return RebuildModel( M,{LL(T/prime)},L/prime)
16 returnM
17Function StateCoarsening( M=(S,A,T,L),T/prime)
18L/prime←GetPrev( L)
19S←{LL(T)|T∈LL/prime(LL/prime(T/prime))}
20 if|S|>β then return RebuildModel( M,S,L/prime)else return M
21Function StateRefinement( M=(S,A,T,L),(S,π,S/prime))
22 foreach(S,π,S/prime/prime)∈{(S,π,S/prime/prime)|(S,π,S/prime/prime)∈T∧S/prime/prime/negationslash=S/prime}do
23 foreachπ/prime∈Sdo
24 R←GetReducer( π/prime)
25 foreachR/prime∈{R/prime|R/prime∈R∧R/negationslash⊏R/prime}do
26 L/prime←L∪{ R→R/prime}|L∪{ (R,π/prime,R/prime)}
27 S1←{LL/prime(T)|(T,σ,T/prime)∈LL/prime((S,π,S/prime))}
28 S2←{LL/prime(T)|(T,σ,T/prime/prime)∈LL/prime((S,π,S/prime/prime))}
29 ifS1∩S2=∅then
30 return RebuildModel( M,S,L/prime)
31 returnM
We use three requirements with threshold αandβto
measure the balance between model precision and model size.
1) No model action abstracts more than αGUI actions in
a GUI tree, meaning that we should reﬁne each model
action until it abstracts fewer than αGUI actions.
2) Non-deterministic transitions should be as few as possi-
ble, meaning that we should keep optimizing the abstrac-
tion function until no non-deterministic transition can be
eliminated.
3) No model state created by a previous Lis reﬁned into β
new states by the reﬁned version of L.
Algorithm 2 aims at balancing the model precision and size.
We ﬁrst introduce inverse mappings to facilitate explaining it.
•L(π)is the set of GUI actions that are abstracted to π.
274
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. •LL(S)is the set of GUI trees that are abstracted to S.
•LL((S,π,S/prime)) is the set of GUI transitions that are
abstracted to (S,π,S/prime).
Function UptAndOptModel ﬁrst adds the new state S/prime
and transition (S,π,S/prime)into the model. Then, it checks
whetherS/primeand (S,π,S/prime)violates any requirements and if
yes further attempts to optimize S/primeand(S,π,S/prime)to incremen-
tally optimize the model via function ActionRefinement ,
StateRefinement andStateCoarsening .A PEkeeps
using the model if it fails to optimize S/primeor(S,π,S/prime).
Action Reﬁnement. Function ActionRefinement reﬁnes
each model action that abstracts more than αGUI actions in
the GUI tree T/prime(i.e.,|L(π/prime)∩T/prime|>α ). For such a π/prime, the
function tries to ﬁnd a new abstraction function that reﬁnes π/prime
into multiple model actions. Function GetReducer returns
the reducer Rthat creates π/prime. Since any reducer R/primethat is not
coarser than R(i.e.,R/prime/negationslash⊐R) may reﬁne π/prime, a new abstraction
function is created by either replacing Rwith a not coarser R/prime
(denoted by L∪{R→R/prime}) or appending a new branch with a
ﬁnerR/primeto the output node of R(denoted by L∪{ (R,π/prime,R/prime)}).
IfL/primereﬁnesπ/primeinto multiple model actions, we replace L
byL/primeand rebuild the model accordingly. Since we only do
replace on leaf nodes and append at most one branch a time,
no node can be selected by multiple branches. The reﬁned
decision tree still guarantees to be a valid function. Function
RebuildModel takes the current model, the set of affected
states and the new abstraction function as input, removes all
affected model actions, states and transitions, and applies the
new abstraction function to re-add states and transitions for
all affected GUI trees and GUI transitions. Since the model
may be rebuilt, S/primeat line 8 may be stale. Hence, function
ActionRefinement repeats until M is not updated, i.e.,
no model action needs reﬁnement.
State Coarsening. Function StateCoarsening (1) ap-
plies a previous LL/primetoT/primeto obtain the old state LL/prime(T/prime), (2)
retrieves all GUI trees that can be abstracted to the old state,
i.e.,LL/prime(LL/prime(T/prime)), (3) obtains the current states Sof these
GUI trees, and (4) reverts LtoL/primeif|S|>β , which means L
is much ﬁner than L/prime.
State Reﬁnement. To eliminate non-deterministic transitions
(S,π,S/prime)and(S,π,S/prime/prime), function StateRefinement tries
to reﬁne each model action of Sfollowing the same steps
asActionRefinement , except that L/primemust reﬁne Sto
different states or reﬁne πto different model actions. We have
implemented the latter case but omit its details in Algorithm 2
since it is just a variant of ActionRefinement .
Parameter Selection. A model action will be reﬁned if it
leads to non-deterministic transitions no matter what αis. The
threshold αis set to 3, which is good in practice. If αis
too large, the exploration strategy should sample each model
action more times. If αis too small, the testing tool cannot
tolerate minor differences. The threshold βis calculated based
on the number of primitive reducers of the ﬁnest reducer in
the decision tree. The largest value of βis 8 by default.
Algorithm Decisions. When two reﬁnements can both elimi-
nate the non-deterministic transitions, we prefer the reﬁnementthat creates fewer states and then fewer model actions in
the new model at ﬁrst, e.g. , prefer L1toL2in Fig. 5.
Since reﬁnement may conﬂict with coarsening, the strictly
balanced model under αandβmay not even exist. When
the conﬂict arises, we prefer coarsening to suppress the state
explosion because the corresponding GUIs have been explored
sufﬁciently when the explosion is detected.
D. Exploration Strategy
We propose to combine random and greedy into the depth
ﬁrst search. First, we found that some non-determinism can
be tolerated by keeping the locality of the exploration. For
example, A PEcan tolerate non-determinism caused by access
time via keeping clicking the same ﬁle in Fig. 1. Hence,
we discover connected sub-graphs and try to explore all
model actions in a connected sub-graph before jumping to
other states. Next, we only greedily visit every newly added
unvisited model action, where model actions are matched by
their attribute paths. Third, we randomly visit other model
actions, and give a higher priority to model actions that are not
visited or abstract more GUI actions. When failing to replay a
transition, A PEdetaches the target states to avoid unnecessary
replaying attempts. When reaching the state again by random
actions, A PEattaches the state to the model again. Hence, A PE
evolves not only state abstraction but also state connectivity.
E. Implementation
APEis built on top of Monkey and runs on both emulators
and real devices. It is designed to be a drop-in replacement of
Monkey and requires no customization of both the app under
test and Android devices. We have tested A PE’s compatibility
on Android 6 and 7 devices because they dominate the market
share when we started to develop A PE [19]. GUI trees are
dumped with the accessibility API [20], the same one used
in [18]. The initial decision tree uses Rcfor every widget.
Initially, there is only one decision tree for all states. If a state
needs reﬁnement, we build a new decision tree for it in a copy-
on-write manner. Hence, we can apply different abstractions
to widgets with the same full attribute paths when necessary.
IV . E V ALUA TION
We evaluate A PEon 15 widely-used apps from the Google
Play Store and compare it with three state-of-art testing tools,
i.e., Monkey, S APIENZ [3] and S TOA T [4]. On average, A PE
achieved higher code coverage and detected more crashes than
all the others. Speciﬁcally, A PE consistently resulted in 26–
78%, 17–22% and 14–26% relative improvements over the
other three tools in terms of average activity coverage, method
coverage, and instruction coverage, respectively. Moreover,
APEdetected 61 crashes, while Monkey detected 44, S APIENZ
40, and S TOA T 31.
To further demonstrate A PE’s usability and effectiveness,
we applied A PE to test 1,316 apps in the Google Play Store
for 30 minutes each. A PE detected 537 unique crashes in 42
error types from 281 of 1,316 apps. We reported 38 crashes to
developers with steps to reproduce the crashes, where 13 have
been already ﬁxed, and 5 have been accepted (ﬁxes pending).
275
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. A. Experimental Setup
Setup. All tools can test apps directly without any modiﬁca-
tion to the apps and Android. However, the publicly available
version of S APIENZ only supports Android 4.4 emulators, and
therefore we ran all comparative experiments on emulators.
We ran S APIENZ and S TOA T on the same version of Android
(i.e., Android 4.4) as described in [3], [4], respectively, and
ran A PE and Monkey on Android 6. All emulators were run
on a MacBook Pro 2016 (Intel Core i7 3.3 GHz, 16GB, Mac
OS 10.13). We ran all tools with their default conﬁgurations.
For Monkey and S APIENZ , we additionally followed previous
work [11], [21] to set a 200 milliseconds delay. We reused
artifacts released by S TOA T and pushed them to emulators
before testing each app. We followed previous work [3], [11],
[12] to give all testing tools except S TOA T an hour to test
each app. We gave S TOA T an hour for model construction
and two additional hours for model-based fuzzing to assess
its full capability. Each experiment was repeated ﬁve times.
We report the average coverage and total crashes in ﬁve runs.
The method and instruction coverages were collected every
minute using our VM-based tracing tool.
Benchmark Collection. We selected 15 widely-used apps
that are compatible with x86 emulators, as shown in Table II.
Speciﬁcally, we ﬁrst randomly selected 11 apps from the edi-
tor’s choice collection of the Google Play Store,2where four
apps were also used by [21]. Second, we randomly selected
another four well-maintained ( i.e., updated since 2017) open-
source benchmark apps used by S APIENZ and S TOA T .
T ABLE II: Statistics of benchmark apps.
# Application Activity (#) Method (#) Instruction (#) Install (#)
1 Citymapper 55 70,182 1,126,836 5,000,000
2 Duolingo 57 63,303 1,150,165 100,000,000
3 Flowx 16 22,081 449,899 100,000
4 Google Drive 87 55,130 1,233,004 1,000,000,000
5 Google Translate 35 36,126 787,527 500,000,000
6 Nuzzel 42 32,587 533,618 100,000
7 30 Day Fitness 34 48,292 668,396 10,000,000
8 Zillow 85 109,338 1,554,545 10,000,000
9 Flipboard 69 49,535 985,299 500,000,000
10 VLC 22 28,264 372,488 100,000,000
11 Tunein Radio 52 90,865 1,480,064 100,000,000
12 Amaze 6 41,475 752,809 500,000
13 Book Catalogue 39 6,683 127,685 100,000
14 Any Memo 30 56,474 669,027 100,000
15 My Expenses 32 58,389 1,091,503 500,000
661 768,724 12,982,865
B. Coverage of Benchmark Apps
As shown in Table III, A PE is clearly the most effective
tool among the four in terms of coverage. Speciﬁcally, A PE
achieved the best per-app coverage on almost all apps and
consistently resulted in 26–78%, 17–22% and 14–26% relative
improvements over the other three tools in terms of average
activity coverage, method coverage, and instruction coverage
respectively. Moreover, the coverage of A PEalso grows much
faster than the other three tools, as shown in Fig. 6. For S TOA T ,
we reported data in both model construction and the entire
2https://play.google.com/store/apps/topic?id=editors choiceT ABLE III: Results on benchmark apps.
#Activity (%) Method (%) Instruction (%) Crashes (#)
Ape Mo Sa St1Ape Mo Sa St1St3Ape Mo Sa St1St3Ape Mo Sa St1St3
1 41 35 31 26 45 43 38 41 42 37 35 30 33 34 0 7 100
2 24 17 22 19 29 26 26 25 26 23 20 20 19 20 3 0 204
3 63 53 60 28 44 40 39 32 34 38 34 32 25 26 1 1 104
4 30 27 26 12 41 39 37 37 38 33 31 29 29 30 0 2 000
5 50 44 46 37 34 33 30 30 30 25 24 23 22 22 1 0 122
6 27 15 26 15 24 17 22 20 20 21 14 19 17 17 5 3 300
7 50 28 40 30 22 15 19 19 20 20 13 17 17 17 0 0 100
8 30 26 19 – 21 19 15 – – 19 17 13 – – 5 7 2–0
9 33 18 18 – 35 26 21 – – 28 20 15 – – 4 0 2–0
10 47 35 37 26 34 29 28 26 30 36 31 30 27 31 3 3 212
11 27 20 21 – 25 21 20 – – 24 20 16 – – 2 1 3–0
12 67 63 60 50 15 14 12 12 13 12 11 9 9 10 20 16 16 2 5
13 63 43 56 17 28 23 20 10 12 25 22 18 9 11 0 0 000
14 76 47 71 27 15 11 13 10 12 16 11 14 10 12 15 13 3 1 3
15 45 39 36 30 18 18 14 14 15 13 14 10 10 11 3 3 311
39 29 31 22 28 24 23 24 25 24 21 19 20 21 62 44 40 9 31
*Mo, Sa, and St are abbreviations for Monkey, S APIENZ , and S TOA T , respectively. St1
and St3represent data in the model construction and the entire testing, respectively.
testing. S TOA T did not report covered activities during model-
based fuzzing because S TOA T has an internal null intent fuzzer
which directly starts activities with empty intents. In practice,
STOA T can achieve 100% activity coverage on emulators. Due
to the intrusive null intent fuzzing, S TOA T always resulted
in not responding on three apps (#8, #9 and #11). Hence,
the average coverage of S TOA T counted 12 apps. All tools
obtained relatively low coverage on some apps, e.g. , app #6–8
and #11–15. The reason is that a proper environment is needed
to further improve the coverage. For example, #app 15 requires
premium user accounts to unlock certain functionalities.
10 20 30 40 50 6020%25%
10 20 30 40 50 6010%15%
10 20 30 40 50 6015%20%25%
Editor’s Choices (1–11) Open Source (12-15) AllAPE Monkey SAPIENZ STOA T
Fig. 6: Progressive instruction coverage. X axis is time in
minutes and Y axis is instruction coverage.
C. Detected Crashes of Benchmark Apps
We counted only fatal errors [22] that crashed app processes
during GUI exploration. This is different from the evaluation
method of S TOA T . First, S TOA T counted as crashes many
exceptions that do not terminate app processes, e.g. , window
leaked exceptions [4], [23]. Second, S TOA T also targeted to
detect crashes that were triggered by null intent fuzzing [24]
in addition to GUI testing. We believe that the class of
crashes found by GUI testing is more important than that
found by null intent fuzzing, because the former class can
usually be triggered with legitimate user interactions whereas
the latter class has been largely prohibited by certain security
mechanisms on real devices [25] and is mostly reproducible on
emulators only. Moreover, the results of our evaluation method
are also consistent with their latest study [26].
276
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. T ABLE IV: Crashes detected by GUI testing.
ToolJava Exceptions (#) Native Crashes (#) All (#)
BR UC BA BR UC BA BR UC BA
APE 161 51 9 67 11 5 228 62 11
Monkey 90 34 8 43 10 6 133 44 10
SAPIENZ 101 33 10 102 7 7 203 40 13
STOA T14 79 5 000 4 795
STOA T3315 31 7 0 0 0 315 31 7
*BR, UC, and BA are abbreviations for bug reports, unique crashes and buggy
apps, respectively.
We show statistics of crashes detected by GUI testing in
Table IV. A PEdetected the most unique crashes by GUI test-
ing. To identify unique crashes, we followed S TOA T to use the
full normalized stack traces [4], i.e., stack traces [22] without
irrelevant information such as messages of exceptions or pid
of processes [27]. Since benchmark apps were developed in
Java, native crashes [27] were mostly triggered by defects of
the framework rather then the apps. For example, almost a half
of native crashes detected by A PE, Monkey and S APIENZ were
caused by some defect of the WebView [28].
APE Monkey
9 53 35
(a) Android 6.0SAPIENZ STOAT
4 36 27
(b) Android 4.4
Fig. 7: Pairwise comparison of unique crashes (stack traces).
NullPointerExceptionNative
IndexOutOfBoundsExceptionIllegalStateException
NumberFormatExceptionIllegalArgumentExceptionSQLiteException
NotSerializableException010203033
11
55322 1Crashes
NullPointerExceptionNative
ClassCastException OutOfMemoryError
IndexOutOfBoundsException
ConcurrentModiﬁcationExceptionRuntimeException
NumberFormatException
IllegalArgumentExceptionSQLiteException051013
10
8
6
211111
NullPointerExceptionNative
ClassCastException
XmlPullParserException
ConcurrentModiﬁcationExceptionArrayIndexOutOfBoundsExceptionSecurityException
NoClassDefFoundError
IndexOutOfBoundsExceptionIllegalStateException05101517
76
2221111Crashes
NullPointerException
IllegalArgumentException NumberFormatExceptionrx.b.j
SecurityException
IllegalStateException
NotSerializableException0510152022
321111APE Monkey SAPIENZ STOA T
Fig. 8: Distribution of crashes types by GUI testing.
Fig. 7 depicts the pairwise comparison of unique normalized
stack traces for tools on the same version of Android. We do
not compare crashes detected by tools on different versions of
Android via normalized stack traces because different versions
of Android have different Android framework code. Particu-
larly, Android 4.4 employs the dalvik VM while Android 6.0
employs the ART runtime [29]. The two runtime environments
have different thread entry methods. Based on data in Figs. 7
and 8, each of the compared tools complements the others in
crash detection and has its own advantages.
APE.APEcan exhaustively exercise parts of the app locally
and also discover more activities than the other tools with adynamic GUI model. For app #14, A PEdetected more crashes
than others because to trigger these crashes it needs to navigate
in the ﬁle system thoroughly to search for some particular ﬁles
and then make some complicated interactions on the ﬁles. This
requires that the model should be ﬁne enough otherwise the
searching of the ﬁle may terminate prematurely. For app #10,
APE detected a crash on an activity that was not discovered
by all other tools in ﬁve runs.
STOAT .STOA T supports the most types of events and can ﬁnd
deep crashes via model-based fuzzing [4]. We plan to incorpo-
rate the powerful fuzzing strategy of S TOA T into A PEin future
work. Due to the limitation of its implementation, S TOA T did
not detect crashes triggered via out-of-order events [4] as the
other three tools.
Monkey. Monkey spent almost all testing budgets to generate
events, e.g. , no restart and no model construction. Hence,
Monkey detected some crashes that can only be triggered
under certain stress. As shown in Fig. 8, only Monkey has
detected six OutOfMemoryError .
SAPIENZ .SAPIENZ inherits from Monkey the same ability to
trigger some particular types of crashes (Fig. 8). For example,
Monkey and S APIENZ found many ClassCastException
andConcurrentModificationException , where all
of them were triggered via trackball and directional pad events.
However, these crashes are not important because trackballs
and directional pads are generally not available on modern An-
droid phones. Moreover, S APIENZ aims at minimizing event
sequences at the testing time and thereby missed deep crashes
that can only be detected via long meaningful interactions [4].
Actually, we can reduce sequences of interest afterwards [14]
without sacriﬁcing the ability to detect deep crashes.
D. Comparative Analysis of Results on Benchmark Apps
1) Model-based vs. Model-free: A model enables A PE to
effectively generate events ( e.g. , avoid clicking non-interactive
widgets) in terms of higher activity coverage with fewer events
in comparison with other tools. On average, A PE,S APIENZ
and Monkey generated 21,819, 33,376 and 71,125 events in
an hour, respectively. S TOA T is built on top of a high-level
testing framework and thus did not report such events.
A model enables A PE to tolerate non-determinism related
to coordinate changes, while model-free tools such as Mon-
key and S APIENZ rarely can. To reduce non-determinism,
SAPIENZ additionally cleared the local app data before re-
playing every test script. However, non-determinism caused
by data in ﬁle systems or from server is still a potential threat
to replaying. Besides, S APIENZ can be easily misguided due
to the lack of connectivity information between GUIs. For
example, app #8 has a welcome activity that requires users
to customize the app. Since local app data were cleared, this
activity always appeared during replaying every test script.
Events that skip or ﬁnish all customizations should always be
included in every test script, which is difﬁcult to guarantee
without connectivity information. In contrast, A PEcan easily
overcome this limitation by traversing the model.
277
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. T ABLE V: Statistics of models built by A PEand S TOA T .
APE STOA T
min median max min median max
State (#) 131 347 725 12 87 517
Action (#) 1,277 6,531 16,141 21 820 1,307
Transition (#) 836 2,282 3,240 21 834 1,340
2) Dynamic Model vs. Static Model: APE can ﬁrst try
ﬁne states and actions to explore the apps thoroughly and
coarsen states to avoid unnecessarily repeated exploration of
the same GUI, and produced ﬁner models than S TOA T ,a s
shown in Table V. Note that A PEcan detach model states to
mitigate state explosions caused by certain non-determinism
during systematic exploration (see Section III-D). Our results
are consistent with previous work [12]. That is, ﬁne models
are generally better than coarse models (see Table III).
E. Large-Scale Evaluation on Apps from Google Play Store
We additionally ran A PE on real Nexus 5 phones to test
1,316 apps from the Google Play Store for 30 minutes each.
The results are quite encouraging. A PE detected 537 unique
crashes in 42 error types from 281 of 1,316 apps. We reported
38 crashes to developers with steps to reproduce the crashes,
where 13 have already been ﬁxed, and 5 have been accepted
(ﬁxes pending). The results show that A PE is an industry-
strength practical tool in testing real-world apps.
T ABLE VI: Distribution of major crash types.
Error Type #
NullPointerException 226
IllegalStateException 58
IllegalArgumentException 47
NumberFormatException 45
Native Crash 33
RuntimeException 27
ArrayIndexOutOfBoundsException 13
IndexOutOfBoundsException 12
Table VI presents the major crash types, i.e., those with
more than 10 crashes each. Since we conducted the evaluation
on real Nexus phones, the results excluded trivial or spurious
crashes that can only be detected on emulators or by null intent
fuzzing, and also excluded crashes that can only be detected
on third-party Android builds ( e.g. , on Samsung phones [30]),
due to defects in third-party framework or library code.
V. R ELA TED WORK
We survey representative related work in this section. Ensur-
ing the quality of mobile apps is challenging and involves man-
ual work in practice [1]. Many pieces of existing work address
speciﬁc problems of Android apps, such as context [31], [32],
concurrent and asynchronous features [33], [34], fragmenta-
tion [30], [35], adverse conditions [36]–[38], and energy [39]–
[41]. Android apps generally comprise of various foreground
and background components, which require different testing
techniques. Hence, some testing tools focus on background
services [42], where the majority aims at testing the GUI [4],[7], [10], [12], [15], [17], [43]–[45] and many of them also
have limited support for both [4], [43].
Many tools attempt to improve back-box tools by ripping the
GUI [7], [43], [46]. Moreover, traditional model-based [12],
[17], [44], symbolic execution based [10], [47], [48], and
search based approaches [9] are more guided but face scal-
ability issues on large apps [11]. Recently, S APIENZ , a novel
search-based approach, has signiﬁcantly advanced the-state-of-
the-art-and-practice [3], [21], [49]. The success of S APIENZ
makes us plan to leverage search-based techniques to improve
APE’s exploration strategy.
Many efforts have been devoted to mitigating the scal-
ability issues, e.g. , hacking the app or framework to opti-
mize performance [13], [36], [50]. For model-based testing,
we can also apply a coarse-grained model [13], [15] or a
probabilistic model [4] but a ﬁner model is generally more
effective [12]. A PE applies a novel technique to adaptively
reﬁne and coarsen the model, which is inspired by counter-
example guided abstraction reﬁnement [51]. Currently, A PE
uses basic properties ( i.e., the number of model actions and
states) to check counterexamples. We plan to incorporate GUI
model checking [52], [53] in future work, e.g. , checking coun-
terexamples with temporal logic. With ﬁne models, A PEmay
identify overly many actions. This problem can be mitigated
by some action summary techniques [21], [54].
VI. C ONCLUSION
This paper presents A PE, a practical, fully automated model-
based tool for effective testing of Android apps. A PE has
a general, decision tree-based representation of abstraction
functions, in order to build a good testing model. It also
has a novel evolution mechanism to dynamically, continuously
update the model toward a good balance between the model
size and model precision.
Compared to the-state-of-the-art techniques that are either
modelless or based on static testing models, A PEconsistently
outperforms them on 15 widely-used benchmark apps in terms
of code coverage (14–78% relative improvements) and the
number of detected bugs (61 unique crashes v.s. 31–44 ones).
We have also applied A PEto test 1,316 apps from the Google
Play Store, resulting in 537 crashes. 38 crashes were reported
to developers with steps for reproducing them — 13 have been
ﬁxed and 5 have been conﬁrmed.
All our evaluations demonstrate that A PEis effective, prac-
tical and promising. Currently, A PEhas been adopted by our
industry partner, and integrated into their testing process.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their valuable feed-
back. This work was supported by the National Key R&D
Program of China (Grant No. 2018YFB1004805) and the
National Natural Science Foundation of China (Grant No.
61690204). Tianxiao Gu, Qirun Zhang, and Zhendong Su were
supported in part by United States NSF Grants 1528133 and
1618158, and Google and Mozilla Faculty Research A wards.
278
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] M. E. Joorabchi, A. Mesbah, and P . Kruchten, “Real Challenges in
Mobile App Development,” in Proceedings of 2013 ACM/IEEE Interna-
tional Symposium on Empirical Software Engineering and Measurement
(ESEM 2013) , 2013, pp. 15–24.
[2] Android, “Android Testing Support Library,” https://developer.android.
com/topic/libraries/testing-support-library/index.html, accessed: 2018-
04-01, 2018.
[3] K. Mao, M. Harman, and Y . Jia, “Sapienz: Multi-objective Automated
Testing for Android Applications,” in Proceedings of the 25th Interna-
tional Symposium on Software Testing and Analysis (ISSTA 2016) , 2016,
pp. 94–105.
[4] T. Su, G. Meng, Y . Chen, K. Wu, W . Y ang, Y . Y ao, G. Pu, Y . Liu,
and Z. Su, “Guided, Stochastic Model-Based GUI Testing of Android
Apps,” in Proceedings of the 2017 11th Joint Meeting on F oundations
of Software Engineering (ESEC/FSE 2017) , 2017, pp. 245–256.
[5] Android, “UI/Application Exerciser Monkey,” https://developer.android.
com/studio/test/monkey.html, accessed: 2018-04-01, 2018.
[6] “UI Events,” https://developer.android.com/guide/topics/ui/ui-events, ac-
cessed: 2018-04-01.
[7] X. Zeng, D. Li, W . Zheng, F. Xia, Y . Deng, W . Lam, W . Y ang, and T. Xie,
“Automated Test Input Generation for Android: Are We Really There Y et
in an Industrial Case?” in Proceedings of the 2016 24th ACM SIGSOFT
International Symposium on F oundations of Software Engineering (FSE
2016) , 2016, pp. 987–992.
[8] L. Clapp, O. Bastani, S. Anand, and A. Aiken, “Minimizing GUI Event
Traces,” in Proceedings of the 2016 24th ACM SIGSOFT International
Symposium on F oundations of Software Engineering (FSE 2016) , 2016,
pp. 422–434.
[9] R. Mahmood, N. Mirzaei, and S. Malek, “Evodroid: Segmented Evo-
lutionary Testing of Android Apps,” in Proceedings of the 22nd ACM
SIGSOFT International Symposium on F oundations of Software Engi-
neering (FSE 2014) , 2014, pp. 599–609.
[10] S. Anand, M. Naik, M. J. Harrold, and H. Y ang, “Automated Concolic
Testing of Smartphone Apps,” in Proceedings of the ACM SIGSOFT 20th
International Symposium on the F oundations of Software Engineering
(FSE 2012) , 2012, pp. 59:1–59:11.
[11] S. R. Choudhary, A. Gorla, and A. Orso, “Automated Test Input
Generation for Android: Are We There Y et?(e),” in Proceedings of
the 30th IEEE/ACM International Conference on Automated Software
Engineering (ASE 2015) , 2015, pp. 429–440.
[12] Y .-M. Baek and D.-H. Bae, “Automated Model-based Android GUI
Testing Using Multi-level GUI Comparison Criteria,” in Proceedings
of the 31st IEEE/ACM International Conference on Automated Software
Engineering (ASE 2016) , 2016, pp. 238–249.
[13] T. Gu, C. Cao, T. Liu, C. Sun, J. Deng, X. Ma, and J. L ¨u, “AimDroid:
Activity-Insulated Multi-level Automated Testing for Android Applica-
tions,” in Proceedings of the 2017 IEEE International Conference on
Software Maintenance and Evolution (ICSME 2017) , 2017, pp. 103–
114.
[14] W . Choi, K. Sen, G. Necula, and W . Wang, “DetReduce: Minimizing
Android GUI Test Suites for Regression Testing,” in Proceedings of the
40th International Conference on Software Engineering (ICSE 2018) ,
2018, pp. 445–455.
[15] W . Choi, G. Necula, and K. Sen, “Guided GUI Testing of Android Apps
with Minimal Restart and Approximate Learning,” in Proceedings of
the 2013 ACM SIGPLAN International Conference on Object Oriented
Programming Systems Languages and Applications (OOPSLA 2013) ,
2013, pp. 623–640.
[16] Android, “Activity,” https://developer.android.com/reference/android/
app/Activity.html, accessed: 2018-04-01, 2018.
[17] W . Y ang, M. R. Prasad, and T. Xie, “A Grey-box Approach for Auto-
mated GUI-model Generation of Mobile Applications,” in International
Conference on Fundamental Approaches to Software Engineering (F ASE
2013) . Springer, 2013, pp. 250–265.
[18] “UI Automator Viewer,” https://developer.android.com/topic/libraries/
testing-support-library/index.html \#uia-viewer, accessed: 2018-04-01.
[19] Android, “Android Dashboard,” https://developer.android.com/about/
dashboards/index.html, accessed: 2018-04-01, 2018.
[20] “Accessibility,” https://developer.android.com/guide/topics/ui/
accessibility/, accessed: 2018-04-01.[21] K. Mao, M. Harman, and Y . Jia, “Crowd Intelligence Enhances Auto-
mated Mobile Testing,” in Proceedings of the 2017 32nd IEEE/ACM
International Conference on Automated Software Engineering (ASE
2017) ,2017, pp. 16–26.
[22] “Android Crashes,” https://developer.android.com/topic/performance/
vitals/crash, accessed: 2018-04-01.
[23] “Window Leak,” https://stackoverﬂow.com/questions/2850573/
activity-has-leaked-window-that-was-originally-added, accessed:
2018-04-01.
[24] R. Sasnauskas and J. Regehr, “Intent Fuzzer: Crafting Intents of Death,”
inProceedings of the 2014 Joint International Workshop on Dynamic
Analysis (WODA) and Software and System Performance Testing, De-
bugging, and Analytics (PERTEA) , 2014, pp. 1–5.
[25] “Export Activity,” https://developer.android.com/guide/topics/manifest/
activity-element#exported, accessed: 2018-04-01.
[26] L. Fan, T. Su, S. Chen, G. Meng, Y . Liu, L. Xu, G. Pu, and Z. Su,
“Large-scale Analysis of Framework-speciﬁc Exceptions in Android
Apps,” in Proceedings of the 40th International Conference on Software
Engineering (ICSE 2018) , 2018, pp. 408–419.
[27] “Android Native Crashes,” https://source.android.com/devices/tech/
debug/native-crash, accessed: 2018-04-01.
[28] “Android WebView,” https://developer.android.com/guide/webapps/
webview, accessed: 2018-04-01.
[29] “ART and Dalvik,” https://source.android.com/devices/tech/dalvik/, ac-
cessed: 2018-04-01.
[30] L. Wei, Y . Liu, and S. Cheung, “Taming Android Fragmentation:
Characterizing and Detecting Compatibility Issues for Android Apps,”
inProceedings of the 31st IEEE/ACM International Conference on
Automated Software Engineering (ASE 2016) , 2016, pp. 226–237.
[31] C.-J. M. Liang, N. D. Lane, N. Brouwers, L. Zhang, B. F. Karlsson,
H. Liu, Y . Liu, J. Tang, X. Shan, R. Chandra, and F. Zhao, “Caiipa: Au-
tomated Large-scale Mobile App Testing Through Contextual Fuzzing,”
inProceedings of the 20th Annual International Conference on Mobile
Computing and Networking (MobiCom 2014) , 2014, pp. 519–530.
[32] K. Moran, M. Linares-V ´asquez, C. Bernal-C ´ardenas, C. V endome, and
D. Poshyvanyk, “CrashScope: A Practical Tool for Automated Testing
of Android Applications,” in Proceedings of the 39th International
Conference on Software Engineering Companion (ICSE-C 2017) , 2017,
pp. 15–18.
[33] L. Fan, T. Su, S. Chen, G. Meng, Y . Liu, L. Xu, and G. Pu, “Efﬁciently
Manifesting Asynchronous Programming Errors in Android Apps,”
inProceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering (ASE 2018) , 2018, pp. 486–497.
[34] J. Wang, Y . Jiang, C. Xu, Q. Li, T. Gu, J. Ma, X. Ma, and J. Lu,
“AA TT+: Effectively Manifesting Concurrency Bugs in Android Apps,”
Science of Computer Programming , vol. 163, pp. 1 – 18, 2018.
[35] H. Khalid, M. Nagappan, E. Shihab, and A. E. Hassan, “Prioritizing the
Devices to Test Y our App on: A Case Study of Android Game Apps,”
inProceedings of the 22nd ACM SIGSOFT International Symposium on
F oundations of Software Engineering (FSE 2014) , 2014, pp. 610–620.
[36] G. Hu, X. Y uan, Y . Tang, and J. Y ang, “Efﬁciently, Effectively Detecting
Mobile App Bugs with AppDoctor,” in Proceedings of the Ninth
European Conference on Computer Systems (EuroSys 2014) , 2014, pp.
18:1–18:15.
[37] C. Q. Adamsen, G. Mezzetti, and A. Møller, “Systematic Execution
of Android Test Suites in Adverse Conditions,” in Proceedings of the
2015 International Symposium on Software Testing and Analysis (ISSTA
2015) , 2015, pp. 83–93.
[38] Z. Shan, T. Azim, and I. Neamtiu, “Finding Resume and Restart Errors
in Android Applications,” in Proceedings of the 2016 ACM SIGPLAN
International Conference on Object-Oriented Programming, Systems,
Languages, and Applications (OOPSLA 2016) , 2016, pp. 864–880.
[39] Y . Liu, C. Xu, S. Cheung, and J. Lu, “GreenDroid: Automated Diagnosis
of Energy Inefﬁciency for Smartphone Applications,” IEEE Transactions
on Software Engineering , vol. 40, no. 9, pp. 911–940, Sept 2014.
[40] D. Li, Y . Lyu, J. Gui, and W . G. J. Halfond, “Automated Energy Op-
timization of HTTP Requests for Mobile Applications,” in Proceedings
of the 38th International Conference on Software Engineering (ICSE
2016) , 2016, pp. 249–260.
[41] M. Linares-V ´asquez, G. Bavota, C. E. B. C ´ardenas, R. Oliveto,
M. Di Penta, and D. Poshyvanyk, “Optimizing Energy Consumption of
GUIs in Android Apps: A Multi-objective Approach,” in Proceedings
of the 2015 10th Joint Meeting on F oundations of Software Engineering
(ESEC/FSE 2015) , 2015, pp. 143–154.
279
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. [42] L. L. Zhang, C.-J. M. Liang, Y . Liu, and E. Chen, “Systematically
Testing Background Services of Mobile Apps,” in Proceedings of the
2017 32nd IEEE/ACM International Conference on Automated Software
Engineering (ASE 2017) , 2017, pp. 4–15.
[43] A. Machiry, R. Tahiliani, and M. Naik, “Dynodroid: An Input Generation
System for Android Apps,” in Proceedings of the 2013 9th Joint Meeting
on F oundations of Software Engineering (ESEC/FSE 2013) , 2013, pp.
224–234.
[44] T. Azim and I. Neamtiu, “Targeted and Depth-First Exploration for
Systematic Testing of Android Apps,” in Proceedings of the 2013 ACM
SIGPLAN International Conference on Object Oriented Programming
Systems Languages and Applications (OOPSLA 2013) , 2013, pp. 641–
660.
[45] A. Sadeghi, R. Jabbarvand, and S. Malek, “P A TDroid: Permission-aware
GUI Testing of Android,” in Proceedings of the 2017 11th Joint Meeting
on F oundations of Software Engineering (ESEC/FSE 2017) , 2017, pp.
220–232.
[46] D. Amalﬁtano, A. R. Fasolino, P . Tramontana, S. De Carmine, and
A. M. Memon, “Using GUI Ripping for Automated Testing of Android
Applications,” in Proceedings of the 27th IEEE/ACM International
Conference on Automated Software Engineering (ASE 2012) , 2012, pp.
258–261.
[47] C. S. Jensen, M. R. Prasad, and A. Møller, “Automated Testing with
Targeted Event Sequence Generation,” in Proceedings of the 2013 In-
ternational Symposium on Software Testing and Analysis (ISSTA 2013) ,
2013, pp. 67–77.
[48] H. van der Merwe, B. van der Merwe, and W . Visser, “Execution andProperty Speciﬁcations for JPF-android,” SIGSOFT Software Engineer-
ing Notes , vol. 39, no. 1, pp. 1–5, Feb. 2014.
[49] N. Alshahwan, X. Gao, M. Harman, Y . Jia, K. Mao, A. Mols, T. Tei, and
I. Zorin, “Deploying Search Based Software Engineering with Sapienz
at Facebook,” in International Symposium on Search Based Software
Engineering (SSBSE 2018) , 2018, pp. 3–45.
[50] W . Song, X. Qian, and J. Huang, “EHBDroid: Beyond GUI Testing for
Android Applications,” in Proceedings of the 2017 32nd IEEE/ACM
International Conference on Automated Software Engineering (ASE
2017) , 2017, pp. 27–37.
[51] E. Clarke, O. Grumberg, S. Jha, Y . Lu, and H. V eith, “Counterexample-
Guided Abstraction Reﬁnement,” in Proceedings of the 12th Interna-
tional Conference Computer Aided V eriﬁcation (CA V 2000) , 2000, pp.
154–169.
[52] M. L. Bolton, E. J. Bass, and R. I. Siminiceanu, “Using Formal
V eriﬁcation to Evaluate Human-Automation Interaction: A Review,”
IEEE Transactions on Systems, Man, and Cybernetics: Systems , vol. 43,
no. 3, pp. 488–503, May 2013.
[53] M. B. Dwyer, V . Carr, and L. Hines, “Model Checking Graphical User
Interfaces Using Abstractions,” in Proceedings of the 6th European
Software Engineering Conference Held Jointly with the 5th ACM SIG-
SOFT International Symposium on F oundations of Software Engineering
(ESEC/FSE 1997) , 1997, pp. 244–261.
[54] M. Ermuth and M. Pradel, “Monkey See, Monkey Do: Effective Gener-
ation of GUI Tests with Inferred Macro Events,” in Proceedings of the
25th International Symposium on Software Testing and Analysis (ISSTA
2016) , 2016, pp. 82–93.
280
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:59:42 UTC from IEEE Xplore.  Restrictions apply. 