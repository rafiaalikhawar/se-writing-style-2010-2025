A Fast Causal Profiler for Task Parallel Programs
Adarsh Yoga
Rutgers University, USA
adarsh.yoga@cs.rutgers.eduSantosh Nagarakatte
Rutgers University, USA
santosh.nagarakatte@cs.rutgers.edu
ABSTRACT
This paper proposes TaskProf , a profiler that identifies parallelism
bottlenecks in task parallel programs. It leverages the structure of a
task parallel execution to perform fine-grained attribution of work
to various parts of the program. TaskProf ’s use of hardware perfor-
mance counters to perform fine-grained measurements minimizes
perturbation. TaskProf ’s profile execution runs in parallel using
multi-cores. TaskProf ’s causal profile enables users to estimate
improvements in parallelism when a region of code is optimized
even when concrete optimizations are not yet known. We have
used TaskProf to isolate parallelism bottlenecks in twenty three
applications that use the Intel Threading Building Blocks library.
We have designed parallelization techniques in five applications to
increase parallelism by an order of magnitude using TaskProf . Our
user study indicates that developers are able to isolate performance
bottlenecks with ease using TaskProf .
CCS CONCEPTS
•Computing methodologies →Parallel computing methodolo-
gies;•Software and its engineering →Software performance ;
KEYWORDS
TaskProf , Profilers, Task parallelism, Intel TBB, Causal profiles
ACM Reference Format:
Adarsh Yoga and Santosh Nagarakatte. 2017. A Fast Causal Profiler for Task
Parallel Programs. In Proceedings of 2017 11th Joint Meeting of the European
Software Engineering Conference and the ACM SIGSOFT Symposium on the
Foundations of Software Engineering, Paderborn, Germany, September 4–8,
2017 (ESEC/FSE’17), 12 pages.
https://doi.org/10.1145/3106237.3106254
1 INTRODUCTION
Task parallelism is an effective approach to write performance
portable code [ 20]. In this model, the programmer specifies fine-
grained tasks and the runtime maps these tasks to processors
while automatically balancing the workload using work stealing
algorithms. Many task parallelism frameworks have become main-
stream ( e.g., Intel Threading Building Blocks (TBB) [ 37], Cilk [ 16],
Microsoft Task Parallel Library [ 27], Habanero Java [ 5], X10 [ 6],
and Java Fork/Join tasks [26]).
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
©2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5105-8/17/09. . . $15.00
https://doi.org/10.1145/3106237.3106254A common metric used to quantify the performance of a task
parallel program is asymptotic parallelism, which measures the
potential speedup when the program is executed on a large number
of processors. It is constrained by the longest chain of tasks that
must be executed sequentially (also known as the span or the crit-
ical work). Hence, asymptotic parallelism is the ratio of the total
work and the critical work performed by the program for a given
input. A scalable program must have large asymptotic parallelism.
A task parallel program can have low asymptotic parallelism due to
multiple factors: coarse-grained tasks, limited work performed by
the program, and secondary effects of execution such as contention,
low locality, and false sharing.
Numerous techniques have been proposed to address various
bottlenecks in both multithreaded programs [ 7,11,13,29–31,42,45]
and task parallel programs [ 21,38]. These techniques range from
identifying critical paths [ 22,33,35], parallelism [ 21,38], synchro-
nization bottlenecks [ 7,11,13,42,45], and other performance
pathologies [ 29–31]. Tools for multithreaded programs identify
bottlenecks in a specific execution on a specific machine, which
does not necessarily provide information about the scalability of
the program. In contrast, tools that measure asymptotic parallelism
in task parallel programs run the program serially [ 21,38], which
is feasible only when the task parallel model provides serial seman-
tics ( e.g., Cilk) [ 16]. Although they identify parallelism bottlenecks,
they do not provide information on regions of code that matter in
improving asymptotic parallelism.
This paper proposes TaskProf , a fast and causal profiler that
measures asymptotic parallelism in task parallel programs for a
given input. TaskProf ’s causal profile allows users to estimate
improvements in parallelism when regions of code are optimized
even before concrete optimizations for them are known. TaskProf
has three main goals: (1) to minimize perturbation (also known as
interference [ 19]) while accurately computing asymptotic paral-
lelism and critical work for each spawn site (source code location
where a task is created), (2) to run the profiler in parallel, and (3)
to provide feedback on regions of code that matter in increasing
parallelism.
TaskProf computes an accurate parallelism profile by perform-
ing fine-grained attribution of work to various parts of the program
using the structure of a task parallel execution. The execution of
a task parallel program can be represented as a tree (specifically
Dynamic Program Structure Tree (DPST) [ 36]), which captures the
series-parallel relationships between tasks and can be constructed
in parallel. Given a task parallel program, TaskProf constructs the
DPST in parallel during program execution and attributes work
to the leaves of the DPST. To minimize perturbation, TaskProf
uses hardware performance counters to measure work performed
in regions without any task management constructs, which corre-
spond to the leaves in the DPST. TaskProf writes the DPST and
the work performed by the leaf nodes of the DPST to a profile data
15
ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Adarsh Yoga and Santosh Nagarakatte
Task parallel programParallelism ProﬁlerSufﬁcient parallelismProgram with sufﬁcient parallelism or no optimization opportunitiesInsufﬁcient parallelismAnnotate code regions to optimizeNo regions to optimizeCausal ProﬁlerTask parallel program with annotated regionsIncrease in parallelism?Design optimizations for the annotated regionsNoYesParallelism proﬁleCausal proﬁlestart
Figure 1: Identifying and diagnosing parallelism bottlenecks using TaskProf ’s parallelism and causal profiles.
file. The profile execution runs in parallel leveraging multi-cores
and the measurement of computation using performance counters
is thread-safe.
TaskProf ’s post-execution analysis tool uses the data file from
the profile run, reconstructs the DPST, and computes asymptotic
parallelism and critical work at each spawn site in the program
using the properties of the DPST (see Section 3.2). TaskProf maps
dynamic execution information to static spawn sites by maintaining
information about spawn sites in the DPST. TaskProf ’s profile for
the sample program in Figure 2 is shown in Figure 3(b).
The spawn sites that perform a large fraction of the critical work
in the profile are the parallelism bottlenecks in the program. How-
ever, optimizing regions that perform critical work may not increase
asymptotic parallelism when the program has multiple regions that
perform similar amount of critical work. Designing a paralleliza-
tion strategy that reduces critical work requires significant effort.
Hence, the programmer would like to know if optimizing a region
of code increases asymptotic parallelism even before the specific
optimization is designed.
TaskProf provides a causal profile that estimates the improve-
ment in asymptotic parallelism when a specific region of code in
the program is optimized even before concrete optimizations for
them are known. TaskProf ’s causal profile is inspired by Coz [10],
which quantifies the speedup when a selected program fragment
is optimized in multithreaded programs by slowing down all code
executing concurrently with the fragment. However, Coz cannot
be used with task parallel programs as it is not possible to slow
down all active tasks.
In contrast, TaskProf is able to generate a causal profile because
it builds an accurate performance model of a task parallel execution
by performing fine-grained attribution of work to the nodes of the
DPST. To quantify the impact of optimizing a region of code, the
programmer annotates the beginning and the end of the region in
the program and the anticipated speedup for the region. TaskProf
generates a causal profile that shows the increase in parallelism with
varying amounts of anticipated speedup for the annotated regions
(see Figure 3(c)). To generate a causal profile, TaskProf re-executes
the program, generates profile data, and identifies nodes in the DPST
that correspond to the annotated regions. Subsequently, TaskProf
recomputes the asymptotic parallelism in the program by reducing
the critical work of the annotated region of code by the anticipated
improvement. TaskProf ’s causal profiling enables the programmer
to identify improvements in asymptotic parallelism even before
the developer actually designs the optimization. Figure 1 illustratesTaskProf ’s usage to generate a parallelism profile and a causal
profile.
TaskProf prototype is open source and available online [ 43].
We have identified parallelism bottlenecks in twenty three Intel
TBB applications using the prototype. Using TaskProf ’s causal
profile, we also designed concrete parallelization techniques for five
applications to address the parallelism bottlenecks. Our concrete
optimizations increased parallelism in these five applications by an
order of magnitude. We conducted a user study involving thirteen
undergraduate and graduate students to evaluate the usability of
TaskProf . Our results show that the participants quickly diagnosed
parallelism bottlenecks using TaskProf .
2 BACKGROUND
This section provides a quick primer on the tree-based represen-
tation of a task parallel execution, which is used by TaskProf to
compute parallelism and causal profiles.
Task parallelism. Task parallelism is a structured parallel pro-
gramming model that simplifies the job of writing performance
portable code. In this model, parallel programs are expressed us-
ing a small set of expressive yet structured patterns. In contrast to
threads, task creation is inexpensive and a task is typically bound
to the same thread till completion [ 32]. The runtime uses work
stealing to map dynamic tasks to runtime threads and balances the
workload between threads [ 16]. Task programming models provide
specific constructs to create tasks ( e.g.,spawn keyword in Cilk and
spawn function in Intel TBB) and to wait for other tasks to com-
plete ( e.g.,sync keyword in Cilk and wait_for_all() function in
Intel TBB). A sample task parallel program is shown in Figure 2.
These models also provide patterns for recursive decomposition
of a program ( e.g.,parallel_for andparallel_reduce ) that are
built using the basic constructs. Task parallelism is expressive and
widely applicable for writing structured parallel programs.
Dynamic program structure tree. The execution of a task par-
allel program can be represented as a dynamic program structure
tree (DPST), which precisely captures the series-parallel relation-
ships between tasks [ 36]. Further, the DPST can be constructed in
parallel. Since our goal in this paper is to profile the program in
parallel, we use the DPST representation of a task parallel execution.
The DPST is a n-ary tree representation of a task parallel execu-
tion. There are three kinds of nodes in a DPST: (1) step , (2)async ,
and (3) finish nodes. The step node represents the sequence of
dynamic instructions without any task spawn or sync statements.
16A Fast Causal Profiler for Task Parallel Programs ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
1 void compute_tree_sum ( node * n, int* sum ) {
2 if(n-> num_nodes <= BASE ) {
3 // Compute sum serially
4 __CAUSAL_BEGIN__
5 * sum = serial_tree_sum (n);
6 __CAUSAL_END__
7 }else {
8 int left_sum , right_sum ;
9 if(n-> left ) {
10 spawn compute_tree_sum (n->left , & left_sum );
11 }
12 if(n-> right ) {
13 spawn compute_tree_sum (n->right , & right_sum );
14 }
15 sync ;
16 * sum = left_sum + right_sum ;
17 }
18 }
19 int main () {
20 __CAUSAL_BEGIN__
21 node * root = create_tree ();
22 __CAUSAL_END__
23 int sum ;
24 spawn compute_tree_sum (root , & sum );
25 sync ;
26 // print sum ;
27 return 0;
28 }
Figure 2: A program that computes the sum of the nodes in
a binary tree. It creates tasks and waits for tasks to complete
using spawn and sync keywords, respectively. Each node in
the tree holds an integer value, number of nodes in the
sub-tree rooted at the node, and pointers to the left and
right sub-tree. The create_tree function builds the tree. The
serial_tree_sum takes a node nas argument and computes
the sum in the sub-tree under n. BASE is a constant that de-
termines the amount of serial work. The user has used an-
notations ( __CAUSAL_BEGIN__ and __CAUSAL_END__ ) to specify
regions for causal profiling, which are not used in the regu-
lar profiling phase.
All computations occur in step nodes. The async node in the DPST
represents the creation of a child task by a parent task. The descen-
dants of the newly created task can execute in parallel with the
remainder of the parent task. A finish node is created in a DPST
when a task spawns a child task and waits for the child (and its
descendants) to complete. A finish node is the parent of all async,
finish, and step nodes directly executed by its children or their
descendants.
The DPST, by construction, ensures that two parallel tasks op-
erate on two disjoint sub-trees. DPST’s construction also ensures
that all internal nodes are either async or finish nodes. The siblings
of a particular node in a DPST are ordered left-to-right to reflect
the left-to-right sequencing of computation of their parent task.
A path from a node to the root and the left-to-right ordering of
siblings in a DPST do not change even when nodes are added to the
DPST during execution. The DPST was originally used for data race
detection because it allows a race detector to check if two accesses
can occur in parallel [ 36,44]. In a DPST, two step nodes S1andS2
(assuming S1is to the left of S2) can execute in parallel if the least
wcwew404040w75cw45ew5w5[<L13,40>,<L25,20>]ss_listew85cw145w175[<L13,40>,<L25,20>][<L13,40>]ss_list[<L13,40>]ss_listF0S0F1
A1A0
S5S4A2S6F2S2S3S1
Line numberParallelismWorkL20L25L10L131.211.511175903040Span145603040Optimization factorParallelismWork2X4X8X2.063.184.38175175175Span855540(a) DPST
(b) Parallelism proﬁle(c) Causal proﬁleCritical path percent58.6213.79027.58w90cwew0ss_list60
w5w5w80w10w40w30[<L13,40>,<L25,20>]w90cwew0ss_list60wcwew303030[<L10,30>]ss_listFigure 3: (a) The DPST for an execution of the program
in Figure 2. F0,F1, and F2are finish nodes. A0,A1, and
A2are async nodes. Step nodes are leaves in the DPST.
TaskProf maintains four quantities with each intermedi-
ate node in the DPST: work ( w), critical work ( cw), exclu-
sive work ( ew), and the list of spawn sites performing crit-
ical work ( ss_list ). Each entry in the spawn site list main-
tains the line number and the exclusive work done by the
spawn site ( e.g.,<L20,40>). Step nodes have work data
from the profile execution. TaskProf updates these quanti-
ties for the intermediate nodes by performing a bottom-up
traversal of the DPST. (b) The profile generated by TaskProf
reports the work, critical work, parallelism, and percentage
of critical work with each spawn site. Line with a “ ⋆” in the
profile corresponds to the main function and reports the par-
allelism for the entire program. (c) The causal profile reports
the parallelism for the whole program when the annotated
regions in Figure 2 are optimized by 2 ×, 4×, and 8×.
common ancestor of S1andS2in the DPST has an immediate child
that is an async node and is also an ancestor of S1. In Section 3.2,
we will highlight the properties of the DPST that we use to profile
programs.
Illustration of the DPST. Figure 3(a) shows the DPST for an
execution of the program in Figure 2. The program in Figure 2 will
execute the spawn call at line 10 and line 13 once when BASE=n/2,
where nis the number of nodes in the tree.
We construct the DPST during program execution as follows.
When the main function starts, we add a finish node F0as the root
of the DPST to represent the fact that main completes after all the
tasks spawned by it have completed. We add a step node S0as the
child of the root finish node to capture the initial computations
being performed in the main function. On a spawn call at line 24
in Figure 2, we create a finish node F1because it is the first spawn
performed by the task. We also add an async node A0as the child
ofF1to represent the spawning of a task. Any computation by the
17ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Adarsh Yoga and Santosh Nagarakatte
newly created task will be added as nodes in the sub-tree under the
async node A0. The operations performed in the continuation of
the main task will be added to the right of the async node A0under
the finish node F1. Hence, the continuation of the main task and
newly created task operate on distinct subtrees of the DPST and
can update the DPST in parallel.
3 PARALLELISM PROFILER
TaskProf computes the total work, part of the total work done
serially (critical work or span), and the asymptotic parallelism at
each spawn site in a task parallel program. The key contribution
ofTaskProf is in fine-grained attribution of work while ensuring
that the profile execution is fast, perturbation-free, and accurate.
TaskProf accomplishes the goal of fast profile execution by using
multi-cores. TaskProf ’s profile execution itself runs in parallel and
leverages the DPST representation to attribute work to various
parts of the program. TaskProf ensures that the profile execution
is perturbation-free by using hardware performance counters to
obtain information about the computation performed by the step
nodes in the DPST. TaskProf also maintains a very small fraction of
the DPST in memory during profile execution to further minimize
perturbation. TaskProf ensures that the parallelism profile is accu-
rate by capturing spawn sites through compiler instrumentation
and by precisely measuring work performed in each step node.
TaskProf computes the parallelism profile in three steps. First,
TaskProf provides a modified library for task parallelism that cap-
tures information about spawn sites. TaskProf ’s compiler instru-
mentation modifies the calls to the task parallel library in the pro-
gram to provide information about spawn sites. Second, TaskProf ’s
profile execution runs in parallel on the multi-core processors,
constructs the DPST representation of the execution, and collects
fine-grained information about the execution using hardware per-
formance counters. TaskProf writes the profile information to a
data file similar to the grof profiler for sequential programs [ 18].
Third, TaskProf ’s offline analysis tool analyzes the profile data and
aggregates the data for each static spawn site. Finally, it computes
asymptotic parallelism and critical work for each spawn site.
Static instrumentation and modified libraries. TaskProf
provides a modified task parallelism library that constructs the
DPST and reads hardware performance counters at the beginning
and end of each step node. TaskProf uses static instrumentation
to instrument the program with calls to the modified task parallel
runtime library. In the subsequent offline analysis phase, TaskProf
needs to map the dynamic execution information about asymptotic
parallelism and critical work to static spawn sites in the program.
Hence, TaskProf instruments the spawn sites to capture the line
number and the file name of the spawn site. TaskProf ’s static in-
strumentation is currently structured as a rewriter over the abstract
syntax tree of the program using the Clang compiler front-end. Our
instrumented libraries and compiler instrumentation enable the
programmer to use TaskProf without making any changes to the
source code.
3.1 Parallel Profile Execution
The goal of the profile execution is to collect fine-grained informa-
tion about the program to enable a subsequent offline computationof asymptotic parallelism. Typically, programs are profiled with
representative production inputs that have long execution times.
Hence, a fast profile execution is desirable. Our goal is to ensure
that the execution time of the program with and without profiling
is similar. Hence, TaskProf profiles in parallel leveraging multi-
core processors. To ensure a parallel profile execution, it needs to
construct the execution graph in parallel and collect information
about the program in a thread-safe manner.
The DPST representation for parallel profile execution.
We use the DPST representation to measure work performed by
various parts of the program because the DPST can be constructed
in parallel. TaskProf constructs the DPST as the program exe-
cutes the injected static instrumentation and measures the work
performed in each step node. The DPST, once constructed, allows
TaskProf to determine the dependencies between tasks. This fine-
grained attribution of work to the step nodes in the DPST enables
TaskProf to compute the parallelism in the program eventually
using an offline analysis.
The DPST of the complete task parallel execution has a large
number of nodes. Storing the entire DPST in memory during pro-
gram execution can cause memory overheads and perturb the execu-
tion. To address this issue, TaskProf does not maintain the entire
DPST in memory. In a library based task parallel programming
model, a task is always attached to the same thread. We leverage
this property to minimize the footprint of the DPST in memory.
TaskProf maintains a small fraction of the nodes that correspond
to the tasks currently executing on each thread in memory.
Once a step node of a task completes execution, the work per-
formed in the step node along with the information about its parent
node is written to the profile data file and the DPST node can be
deallocated. As async nodes do not perform any work, TaskProf
writes the information about its parent in the DPST and the spawn
site associated with the async node to the profile data file. In con-
trast to step and async nodes, only parent node information is
written to the profile data file for a finish node.
Measuring work with hardware performance counters. To
measure the work performed in each step node without perfor-
mance overhead, TaskProf uses hardware performance counters.
Performance counters are model specific registers available that
count various events performed by the hardware using precise
event-based sampling mechanisms [ 8]. These performance coun-
ters can be programmatically accessed. TaskProf can use both the
number of dynamic instructions and the number of execution cy-
cles to measure the work done in a step node. Measuring execution
cycles allows TaskProf to account for latencies due to secondary
effects such as locality, sharing, and long latency instructions. Fur-
ther, the operations on these counters are thread-safe. TaskProf
reads the value of the counter at the beginning and the end of the
step node using static instrumentation injected into the program.
It calculates the work performed in the step node by computing
difference between the two counter values. This fine-grained mea-
surement of work performed in each step node using hardware
performance counters along with the construction of the DPST
while executing in parallel allows TaskProf to compute a precise,
yet fast profile of the program.
18A Fast Causal Profiler for Task Parallel Programs ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
The profile data file generated at the end of parallel profile exe-
cution contains the work done in each step node. It also contains
the information about the parent for each node in the DPST and
the spawn site information for each async node. The left-to-right
sequencing of nodes is implicitly captured by the order of the nodes
in the profile data file.
3.2 Offline Analysis of the Profile Data
TaskProf ’s offline analysis reconstructs the DPST using the data
from the profile execution and computes the work and critical work
(span) for each spawn site in the program. The construction of the
DPST from the profile data is fairly straightforward as it contains
information about nodes, their parent nodes, and the left-to-right
ordering of the nodes. In this section, we describe the computation
of work and span for each intermediate node in the DPST given
the work performed in the step nodes. We also describe the process
of mapping this dynamic information to static spawn sites.
Computing work and critical work for each intermediate
node. In the DPST representation, all computation is performed in
the step nodes. The step nodes have fine-grained work information
from the profile execution. TaskProf needs to compute the total
work and the fraction of that work done serially (critical work)
for each intermediate node in the DPST. To provide meaningful
feedback to the programmer, TaskProf also computes the list of
spawn sites that perform critical work and the portion of the critical
work performed exclusively by each spawn site.
TaskProf computes the total work and the critical work at each
intermediate node by performing a bottom-up traversal of the DPST.
The total work performed in the sub-tree at each intermediate node
is sum of the work performed by all the step nodes in the sub-tree.
In contrast, critical work measures the amount of work that is
performed serially. Computing critical work and the set of tasks
performing the critical work requires us to leverage the properties
of the DPST. Specifically, we leverage the following properties of
the DPST to compute the critical work.
•The siblings of a node in a DPST are ordered left-to-right
reflecting the left-to-right sequencing in the parent task.
•Given an intermediate node, all the direct step children of
the node execute serially.
•All the left step or finish siblings of an async node execute
serially with the descendants of the async node.
•All the right siblings (and their descendants) of an async
node execute in parallel with the descendants of the async
node.
Using the above properties of the DPST, the critical work at an
intermediate node will be equal to either (1) the serial work done by
all the direct step children and the critical work performed by the
finish children or (2) the critical work performed by descendants of
an async child and the serial work performed by the left step and
finish siblings of the specific async child in consideration. Since any
intermediate node in the DPST can have multiple async children,
TaskProf needs to check if any of the async nodes can contribute
to the critical work. For example, consider the intermediate node
F2in Figure 3(a) that has two async nodes A1andA2. The critical
work will be the maximum of (1) the work done by the direct step
child S4or (2) the critical work by the async child A1(it does not1:function ComputeWorkSpan (T)
2: for each non-step node Nin bottom-up traversal of Tdo
3: CN←Children (N)
4: N.work←X
C∈CNC.work
5: SN←StepChildren (N)
6: FN←FinishChildren (N)
7: N.c_work←X
S∈SNS.work+X
F∈FNF.c_work
8: N.e_work←X
S∈SNS.work+X
F∈FNF.e_work
9: N.ss_list←[
F∈FNF.ss_list
10: for each A∈AsyncChildren (N)do
11: LSA←LeftStepSiblings (A)
12: LFA←LeftFinishSiblings (A)
13: llwA←X
LS∈LSALS.work+X
LF∈LFALF.c_work
14: ifllwA+A.c_work >N.c_work then
15: N.c_work←llwA+A.c_work
16: N.e_work←X
S∈LSAS.work+X
F∈LFAF.e_work
17: N.ss_list← ([
LF∈LFALF.ss_list )∪A.ss_list
18: end if
19: end for
20: ifNis aasync node then
21: N.ss_list←N.ss_list∪⟨N.s_site,N.e_work⟩
22: end if
23: end for
24:end function
Figure 4: Algorithm to compute the total work ( work), crit-
ical work ( c_work ), exclusive work ( e_work ), and the spawn
sites that perform the critical work ( ss_list ) for each inter-
mediate node in the DPST. The function Children returns
the set of children of the input node. Similarly, functions
StepChildren ,FinishChildren , and AsyncChildren re-
turn the set of step,finish , and async child nodes of the in-
put node, respectively. The function LeftStepSiblings re-
turns the set of step sibling nodes that occur to the left of
the input node in the DPST. Similarly, the LeftFinishSib-
lings returns the set of finish sibling nodes to the left of
the input node in the DPST.
have any left siblings), or (3) the sum of the critical work by the
async child A2and the work done by the step node S4, which is the
left step sibling of A2.
Each async node in the DPST corresponds to a spawn site in
the program because async nodes are created when a new task
is spawned. Hence, TaskProf computes the list of spawn sites
performing critical work by computing the list of async nodes that
contribute to the critical work in the sub-tree of the intermediate
node.
Algorithm to compute work and critical work. Figure 4 pro-
vides the algorithm used by TaskProf to compute the total work,
19ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Adarsh Yoga and Santosh Nagarakatte
the critical work, and the set of spawn sites contributing to the
critical work. The algorithm maintains four quantities with each
intermediate node in the DPST: (1) total work performed in the
sub-tree under the node ( work ), (2) the critical work performed
in the sub-tree ( c_work ), (3) the list of spawn sites that perform
the critical work ( ss_list ), and (4) the part of the critical work
that is performed exclusively by the direct children of the node
(e_work ). The exclusive work of a node is equal to sum total of
the work performed by the direct step children and the exclusive
work performed by the finish children. We consider the exclusive
work performed by a finish node because it is not yet associated
with any spawn site. The exclusive work of the current node will
eventually be associated with a spawn site. The algorithm does
not consider the exclusive work of the async children because it is
already associated with a spawn site.
The algorithm traverses each node in the DPST in a bottom-up
fashion. All step nodes have work information from the profile data.
For any intermediate node, the work performed under the sub-tree
is the sum of the work performed by all its children (lines 3-4 in
Figure 4). For a given intermediate node, TaskProf initially com-
putes the serial work performed in all the step and finish children
as the critical work (lines 5-7 in Figure 4). For each async child of
the current node, it checks if the serial work done by the async
node and its left siblings is greater than the critical work computed
until that point (lines 10-15 in Figure 4).
To compute the set of spawn sites performing critical work,
each intermediate node also maintains a list of spawn sites and
the exclusive work performed by them. The algorithm initially sets
the spawn site list for a node to be the union of spawn site lists of
its finish children (lines 8-9 in Figure 4). Whenever an async child
contributes to the critical work, the spawn site list of the current
node is the union of the spawn site list of the async child and the
spawn site lists of the finish children that are to the left of the async
child (line 17 in Figure 4). When an async child contributes to the
critical work, the exclusive work of the current node is equal to sum
of the work performed by the left step siblings and the exclusive
work performed by the left finish siblings of the async child (line 16
in Figure 4). The algorithm adds the spawn site and the exclusive
work performed by the current async node to the node’s spawn
site list (lines 20-22 in Figure 4).
After the algorithm completes traversing the entire DPST, the
root of the DPST will contain the list of all spawn sites that per-
form critical work and their individual contribution to the critical
work. The root node also contains information about the total work
performed by the program, the work that is computed serially by
the program, and the exclusive work performed under the entry
function of the program ( i.e.,main ).
Aggregating information about a spawn site. A single spawn
site may be executed multiple times in a dynamic execution. Hence,
TaskProf aggregates information from multiple invocations of the
same spawn site. TaskProf computes the aggregate information
for each spawn site by performing another bottom-up traversal of
the DPST at the end. When it encounters an async node, TaskProf
uses a hash table indexed by the spawn site associated with the
async node and adds the total work and critical work to the entry.
When aggregating this information, TaskProf has to ensure thatit does not double count work and critical work when recursive
calls are executed. In the presence of recursive calls, a descendant
of an async node will have the same spawn site information as the
async node. If we naively add the descendant’s work, it leads to
double counting as the work and critical work of the current async
node already considers the work/critical work of the descendant
async node. Hence, when TaskProf encounters an async node in a
bottom-up traversal of the DPST, it checks whether the descendants
of the async node have the same spawn site information. When
a descendant with the same spawn site exists, it subtracts such a
descendant’s work and critical work from the entry in the hash
table corresponding to the spawn site. Subsequently, TaskProf
adds the work and the critical work of the current async node to
the hash table.
Profile reported to the user. For each spawn site in the pro-
gram, TaskProf presents the work, the critical work, the asymp-
totic parallelism, and the percentage of critical work exclusively
done by the spawn site. The asymptotic parallelism of a spawn site
is the ratio of the total work and the critical work performed by
a spawn site. The spawn sites are ordered by the percentage of
critical work exclusively performed by the spawn site. Figure 3(b)
illustrates the parallelism profile for the program in Figure 2 that
has the DPST shown in Figure 3(a). If a spawn site has low paral-
lelism and performs a significant proportion of the critical work,
then optimizing the task spawned by the spawn site may increase
the parallelism in the program. This profile information provides a
succinct description of the parallelism bottlenecks in the program.
4 CAUSAL PROFILING
TaskProf reports the set of spawn sites performing critical work
to the user, which highlight the parallelism bottlenecks in the pro-
gram. A programmer can consider these spawn sites to be initial
candidates for optimization to reduce serial computation.
Reducing critical work and the impact on parallelism. De-
signing a new optimization or a parallelization strategy that reduces
the critical work typically requires effort and time. A program may
have multiple spawn sites that perform similar amount of critical
work. When a set of spawn sites are parallelized to reduce critical
work, the resultant execution may have new spawn sites whose
critical work is similar to the critical work before the optimization.
In such cases, an optimization to a spawn site performing critical
work may not improve the asymptotic parallelism in the program.
Hence, programmers would benefit from a causal profile of program
that identifies the improvement in asymptotic parallelism when
certain regions of the code are optimized.
Causal profile with TaskProf .A causal profile provides in-
formation on improvements in parallelism when certain parts of the
code are parallelized or optimized. TaskProf proposes a technique
to generate causal profiles for task parallel programs. The program-
mer can get an accurate estimate of the improvement in asymptotic
parallelism by reducing the serial work in a region of the program
using TaskProf ’s causal profile. TaskProf provides such an esti-
mate even before the programmer has designed a concrete strategy
to parallelize or reduce the serial work in the region of code under
consideration. In summary, a causal profile enables the programmer
20A Fast Causal Profiler for Task Parallel Programs ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
to identify parts of the program that really matter in increasing the
asymptotic parallelism. Figure 3(c) provides the causal profile for
the program in Figure 2 where the regions under consideration are
demarcated by __CAUSAL_BEGIN__ and__CAUSAL_END__ . Next, we
describe how TaskProf generates a causal profile leveraging the
accurate performance model of a task parallel execution created
with the fine-grained attribution of work and the DPST.
Static code annotations. To generate causal profiles, the pro-
grammer annotates a static region of code that is considered for
parallelization and the expected improvement to the critical work
from parallelization. The programmer can provide multiple regions
as candidates for optimization. TaskProf generates a causal pro-
file that estimates the improvement in parallelism when all anno-
tated regions are optimized. In addition, TaskProf also generates
a causal profile for optimizing each region in isolation. Figure 2
illustrates the regions of code annotated for causal profiling with
__CAUSAL_BEGIN__ and__CAUSAL_END__ annotations. If the pro-
grammer does not specify the amount of expected improvement
for the considered region, TaskProf assumes a default value. If the
annotations are nested, the outermost region of code is considered
for estimating the benefits.
Profile execution and attribution of work. TaskProf uses
these annotations, profiles the program, constructs the DPST to
attribute work to various regions, and provides the estimated im-
provement in asymptotic parallelism from optimizing the annotated
regions. During profile execution, TaskProf measures the work
performed in the annotated part of the step node and also in parts
of the step node that have not been annotated. Hence, each step
node can have multiple work measurements corresponding to static
regions with and without annotation. TaskProf accomplishes it by
reading the performance counter value at the beginning and the
end of the each dynamic region. TaskProf maintains a list of work
values for each step node and writes it to the profile data file.
Algorithm to generate causal profiles. The algorithm to com-
pute the causal profile is similar to the work and span algorithm
in Figure 4. It takes the DPST as input and a list of anticipated
improvements for the annotated regions. The algorithm outputs a
causal profile that computes the improvement in asymptotic paral-
lelism of the whole program for the specified improvements of the
annotated regions. The causal profile algorithm performs a bottom-
up traversal of the DPST similar to the work and span algorithm
in Figure 4. However, the causal profiling algorithm does not track
spawn sites and computes the whole program’s work and critical
work. The key difference with the causal profiling algorithm is
the manner in which it handles the work done by the step nodes,
which have regions corresponding to user annotations. Specifically,
TaskProf maintains a list of annotated and non-annotated regions
executed with each step node and the amount of work performed
in each region. To estimate the effect of optimizing/parallelizing the
annotated region, we reduce the critical work contribution of the
annotated region by the user-specified optimization factor while
keeping the total work performed by the regions unchanged. The
output of the causal profiling algorithm is a list that provides the
asymptotic parallelism for each anticipated improvement factor for
the regions under consideration.Illustration. After analyzing the parallelism profile in Fig-
ure 3(b) for the program in Figure 2, the programmer has identified
two regions of code (lines 4-6 and lines 20-22 in Figure 2) for opti-
mization. The regions are annotated with __CAUSAL_BEGIN__ and
__CAUSAL_END__ annotations to demarcate the beginning and the
end. During execution, the region at lines 20-22 is executed once
and is represented by step node S0in Figure 3(a). In contrast, the
region at lines 4-6 is executed twice and is represented by step
nodes S5andS6in Figure 3(a). In this example, the entire step
node corresponds to the annotated region. In general, a step node
may have multiple annotated and non-annotated regions. To gen-
erate a causal profile, the critical work performed by nodes S0,S5,
andS6are decreased by 2 ×, 4×, and 8×and its impact on whole
program parallelism is computed. Figure 3(c) provides the causal
profile with the annotated regions, which reports that the asymp-
totic parallelism in the program increases when those two regions
are optimized.
5 EXPERIMENTAL EVALUATION
This section describes our prototype, our experimental setup, and
an experimental evaluation to answer the following questions: (1) Is
TaskProf effective in identifying parallelism bottlenecks? (2) Is
TaskProf ’s parallel profile execution faster than serial profilers?
(3) Is TaskProf effective in minimizing perturbation in the profile
execution? (4) Is TaskProf usable by programmers?
Prototype. We have built a TaskProf prototype to profile task
parallel programs using the Intel Threading Building Blocks(TBB)
library [ 37]. The prototype provides a TBB library that has been
modified to construct the DPST, measure work done in step nodes
using hardware performance counters, and track file name and
line information at each spawn site. The prototype also handles
algorithms for geometric decomposition such as parallel_for and
parallel_reduce . The prototype also includes a Clang compiler
pass that automatically adds line number and file name information
to the TBB library calls, which enables the programmer to use
the modified library without making any source code changes.
Hence, the modified TBB library can be linked to any TBB program.
Our prototype adds approximately 2000 lines of code to the Intel
TBB library to perform various profiling operations. The TaskProf
prototype is open source [43].
Applications used for evaluation. We evaluated TaskProf
using a collection of twenty three TBB applications, which in-
clude fifteen applications from the problem based benchmark suite
(PBBS) [ 39], all five TBB applications from the PARSEC suite [ 4], and
three TBB applications from the structured parallel programming
book [ 32]. The PBBS applications are designed to compare differ-
ent parallel programming methodologies in terms of performance
and code. We conducted all experiments on a 2.1GHz 16-core Intel
x86-64 Xeon server with 64 GB of memory running 64-bit Ubuntu
14.04.3. We measured wall clock execution time by running each
application five times and use the mean of the five executions to
report performance. We use the perf events module in Linux to
programmatically access hardware performance counters.
RQ1: Is TaskProf effective in identifying parallelism bot-
tlenecks? We used TaskProf to identify parallelism bottlenecks in
21ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Adarsh Yoga and Santosh Nagarakatte
Table 1: Applications used to evaluate TaskProf . We pro-
vide a short description of the application, the speedup ob-
tained on a 16-core machine when compared to serial execu-
tion time, the asymptotic parallelism reported by TaskProf ,
the number of annotated regions that provides maximum
parallelism with causal profiling, and the asymptotic paral-
lelism when the critical work in the annotated regions is op-
timized by 100×, which we list as causal parallelism.
Application Description Speedup Parallel-
ism# of
regionsCausal
parallelism
blackscholes Stock option pricing 1.09 1.14 2 59.24
bodytrack Tracking of a human body 5.96 22.19 1 40.32
fluidanimate Simulate fluid dynamics 9.39 66.09 1 90.20
streamcluster Clustering algorithm 7.30 55.13 2 198.93
swaptions Price a portfolio 8.59 73.45 1 98.73
convexHull Convex hull 1.30 1.28 4 112.17
delRefine Delaunay Refinement 2.93 5.50 7 61.28
delTriang Delaunay triangulation 1.23 1.47 5 78.85
karatsuba Karatsuba multiplication 5.22 23.69 1 36.90
kmeans K-means clustering 2.54 4.18 6 69.60
nearestNeigh K-nearest neighbors 4.54 12.41 2 30.55
rayCast Triangle intersection 6.62 48.49 2 68.52
sort Parallel quicksort 3.91 6.33 2 45.04
compSort Generic sort 4.99 38.97 4 86.23
intSort Sort key-value pairs 4.71 48.68 2 75.02
removeDup Remove duplicate value 6.04 54.91 3 98.24
dictionary Batch dictionary opers 5.13 38.10 4 73.12
suffixArray Sequence of suffixes 3.75 5.50 1 28.53
bFirstSearch Breadth first search 6.60 22.45 5 60.55
maxIndSet Maximal Independent Set 5.48 16.46 5 52.23
maxMatching Maximal matching 6.73 46.04 0 46.04
minSpanForest Minimum spanning forest 3.47 7.99 2 49.78
spanForest Spanning tree or forest 7.46 44.04 1 58.91
all the twenty three applications. Table 1 provides details on appli-
cations used, their speedup on a 16-core machine compared to serial
execution, the asymptotic parallelism reported by TaskProf , the
number of regions that we identified using TaskProf to increase as-
ymptotic parallelism, and the resultant asymptotic parallelism from
causal profiling when the critical work in the identified regions is
decreased by 100×. Typically, asymptotic parallelism of a program
should be at least 10 ×or more than the anticipated speedup on a
machine to account for scheduling overheads [16, 37, 38].
TaskProf ’s profile shows that some applications in Table 1 have
reasonable asymptotic parallelism, which accounts for a reasonable
speedup on a 16-core machine. For example, fluidanimate applica-
tion has an asymptotic parallelism of 66.09 which is the maximum
possible speedup when the program is executed on a large number
of machines. The fluidanimate application exhibits a speedup
of 9.39×compared to a serial execution when the program was
executed on a 16-core machine.
Table 1 also shows that we were able to identify a small number of
code regions which when optimized provide a significant increase in
asymptotic parallelism. TaskProf ’s profile information on spawn
sites performing critical work and the causal profiling strategy
was instrumental in identifying the specific regions of code as
candidates for increasing asymptotic parallelism. The application
maxMatching already had a large amount of asymptotic parallelism
and we could not find any region that increases parallelism.
In summary, TaskProf enabled us to identify a set of code re-
gions that can increase asymptotic parallelism significantly in al-
most all our applications. Once we identified code regions that
Critical path percentreﬁneTime.C:59reﬁne.C:24955.34.8466.26hullTime.C:55Parallel-ism7.9951.7958.6942.11Optimization factorParallel-ism50X200X400X47.2851.1451.84Critical path percent85.972.641.523.68MSTTime.C:77spec_for.h:82IO.h:71graphIO.h:167100X49.78Parallel-ism33.3451.5452.2842.87Critical path percent33.739.458.4814.58File:LineMSTTime.C:77spec_for.h:82sampleSort.h:81graphIO.h:167I. Minimum spanning forest
1.282.471.21.3360.3196.82316.0680.665.752.6710.3hullTime.C:46hull.C:117hull.C:209112.1750.8441.9755.4652.7118.9317.0616.1525.82hullTime.C:55sequence.h:359IO.h:71geoIO.h:96
5.554.2549.8255.6864.5191.471.730.24reﬁneTime.C:59reﬁne.C:260topFromT.C:12661.2748.0858.4262.4856.8316.514.376.6425.22reﬁne.C:257sequence.h:365reﬁne.C:246File:Line
File:LineParallel-ismParallel-ism50X200X400X100XParallel-ismCritical path percentFile:Line
Parallel-ism(a) Original parallelism proﬁle(b) Causal proﬁle(c) Parallelism proﬁle after optimizationOptimization factor(a) Original parallelism proﬁle(b) Causal proﬁle(c) Parallelism proﬁle after optimizationII. Convex hull
File:LineParallel-ism50X200X400X100XOptimization factorParallel-ismCritical path percentFile:LineIV. Delaunay reﬁnement
(a) Original parallelism proﬁle(b) Causal proﬁle(c) Parallelism proﬁle after optimizationCritical path percent
Critical path percentdelTime.C:55geoIO.h:9658.280.09111.331.4756.5171.4163.5596.499.120.070.07delTime.C:55delaunay.C:385IO.h:16478.4553.1158.6551.7857.933.935.995.4247.46sequence.h:365IO.h:179delaunay.C:284Parallel-ismFile:LineParallel-ism50X200X400X100XOptimization factorParallel-ismCritical path percentFile:LineIII. Delaunay triangulation
(a) Original parallelism proﬁle(b) Causal proﬁle(c) Parallelism proﬁle after optimization
Critical path percentbscholes.c:464bscholes.c:27451.20.2896.521.1439.179.7899.72bscholes.c:32359.2440.03148.44121.7122.75946.52bscholes.c:212bscholes.c:415bscholes.c:145Parallel-ismFile:LineParallel-ism50X200X400X100XOptimization factorParallel-ismCritical path percentFile:LineV. Blackscholes
(a) Original parallelism proﬁle(b) Causal proﬁle(c) Parallelism proﬁle after optimizationFigure 5: The original parallelism profile, the causal pro-
file for the annotated regions, and final parallelism profile
generated by TaskProf after annotated regions were paral-
lelized for each of the five applications. We list the top four
spawn sites from TaskProf ’s parallelism profile. Line with
a “⋆” in the profile corresponds to the main function and
reports the parallelism for the entire program.
can increase asymptotic parallelism, we designed concrete paral-
lelization strategies to reduce the critical work for five applications,
which increased the asymptotic parallelism and the speedup of the
program. We describe them below.
Improving the speedup of the MinSpanningForest appli-
cation. This PBBS application computes the minimum spanning
forest of the input undirected graph. The program has a speedup
of 3.47×over serial execution on a 16-core machine. The paral-
lelism profile generated by TaskProf is shown in Figure 5(I)(a),
which reports that the parallelism in the program (main function
atMSTTime.C:77 ) is 7.99. The main function performs 85% of the
serial work in the program. We identified two regions of code using
annotations for causal profiling in the main function. Figure 5(I)(b)
presents the causal profile generated by TaskProf , which shows
22A Fast Causal Profiler for Task Parallel Programs ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
the increase in asymptotic parallelism in the program on poten-
tially optimizing these two regions. On further investigation of the
code regions, we realized that annotated regions were performing a
serial sort. We replaced them with a parallel sort function, which in-
creased the asymptotic parallelism to 33.34 from 7.99. Figure 5(I)(c)
reports the profile after our parallel sort optimization. The speedup
of the program increased from 3.49 ×to 6.37×.
Improving the speedup of the Convex Hull application.
This PBBS application computes the convex hull of a set of points
using a divide and conquer approach [ 3].TaskProf ’s profile shown
in Figure 5(II)(a) reveals that the program has an asymptotic paral-
lelism of 1.28 for the whole program. As expected, it did not exhibit
any speedup. Figure 5(II)(a) shows that 80% of the critical work
is performed by the spawn site at hullTime.C:55 . We annotated
two regions corresponding to that spawn site, which performed
sequential read and write operations of the input and output files re-
spectively. TaskProf ’s causal profile showed that it would increase
the parallelism to 6.85. Subsequently, we annotated two additional
regions of code corresponding to the spawn site performing the next
highest critical work ( hull.C:209 ) in Figure 5(II)(a). The causal
profile shown in Figure 5(II)(b) shows that asymptotic parallelism
increases significantly when all the four regions are optimized. We
parallelized a loop at spawn site hull.C:209 using parallel_for
and parallelized I/O at spawn site hullTime.C:55 . These optimiza-
tions increased the parallelism to 50.84 (see Figure 5(II)(c)) and the
speedup of the whole program increased from 1.3 ×to 8.14×.
Improving the speedup of Delaunay Triangulation. This
PBBS application produces a triangulation given a set of points
such that no point lies in the circumcircle of the triangle. The pro-
gram has an asymptotic parallelism of 1.47 (see Figure 5(III)(a)) for
the entire program and exhibits little speedup. The spawn site at
delTime.C:55 performs 99% of the critical work. When we looked
at the source code, we found that the program is structured as
a collection of parallel_for constructs interspersed by serial
code. We annotated five regions of code between the invocations
ofparallel_for . The causal profile in Figure 5(III)(b)) shows that
the asymptotic parallelism increases significantly by optimizing the
annotated regions. We parallelized the annotated regions, which
had serial for loops, using parallel_for while ensuring they op-
erate on independent data. The profile for the resultant program
is shown in Figure 5(III)(c). The parallelism increased to 53.11 and
the speedup increased from 1.23 ×to 5.82×.
Improving the speedup of Delaunay Refinement. This PBBS
application takes a set of triangles that form a delaunay triangula-
tion and produces a new triangulation such that no triangle has an
angle less than a threshold value. TaskProf ’s profile for this pro-
gram reports an asymptotic parallelism of 5.5 (see Figure 5(IV)(a))
and it had a speedup of 2.93 ×. Similar to delaunay triangulation,
this program also had a set of serial code fragments in-between
parallel_for calls. We identified seven regions of such serial
code and annotated them. TaskProf ’s causal profile shown in Fig-
ure 5(IV)(b) indicates that optimizing all these seven regions can
increase asymptotic parallelism. We parallelized the serial for loops
in these seven regions using parallel_for , which increased the
0X5X10X Speedup
blackscholesbodytrackfluidanimatestreamclusterswaptionsconvexHulldelRefinedelTriangkaratsubakmeans
nearestNeighrayCastsort
compSortintSort
removeDupdictionarysuffixArraybFirstSearchmaxIndSetmaxMatchingminSpanForestspanForestgeomeanFigure 6: Speedup of TaskProf ’s parallel profile execution
when compared to serial profile execution.
asymptotic parallelism to 48.08 (see Figure 5(IV)(c)) and the speedup
increased from 2.93 ×to 6.42×.
Improving the speedup of Blackscholes. This application
from the PARSEC suite [ 4] computes the price of a portfolio of
options using partial differential equations. It has low asymptotic
parallelism for the entire program (see Figure 5(V)(a)). This program
has a single parallel_for that has reasonable parallelism of 51.2.
However, the spawn site at bscholes.c:323 is performing 99% of
the program critical work. Our examination of the code revealed
that it was reading and writing serially. We split the input and out-
put into multiple files and parallelized the input/output operations
which increased the parallelism to 40.03 and the speedup increased
from 1.09×to 7.7×.
In summary, TaskProf enabled us to quantify asymptotic paral-
lelism in the program and its causal profiling strategy enabled us
to identify specific regions of code that can increase parallelism.
RQ2: Is TaskProf ’s parallel profile execution faster than
serial profile execution? TaskProf ’s profile execution executes
in parallel compared to prior profilers [ 38], which execute serially.
To quantify the benefits of parallel profile execution, we designed a
serial version of TaskProf by pinning the execution of the program
to a single core. This is an approximation of serial profiling as
TBB programs do not have serial semantics. Figure 6 reports the
speedup of a parallel TaskProf profile execution compared to a
serial profile execution. On average, TaskProf ’s parallel profile
execution is 4.32×faster than serial profile execution. The speedup
from a parallel profile execution is proportional to the amount of
parallelism in the application.
RQ3: Is TaskProf effective in minimizing perturbation
in the profile execution? TaskProf uses hardware performance
counters to perform fine-grain attribution of work and to minimize
perturbation. The average performance overhead of TaskProf ’s
profile execution compared to the parallel execution of the program
without any profiling instrumentation is 56%. A major fraction
of this performance overhead is attributed to system calls to read
hardware performance counters. TaskProf ’s profile execution is an
order of magnitude faster than instrumenting each dynamic instruc-
tion through compiler instrumentation, which exhibited overheads
of 20×-100×for the applications in Table 1. Hence, TaskProf mini-
mizes perturbation even with fine-grained attribution of work.
RQ4: Is TaskProf usable by programmers? We conducted
a user study to evaluate the usability of TaskProf . The user study
had thirteen participants: twelve graduate students and one senior
undergraduate student. Among them, two students had 4+ years
23ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Adarsh Yoga and Santosh Nagarakatte
of experience in parallel programming, five students had some
prior experience, four students had passing knowledge, and two
students had no prior experience with parallel programming. The
total duration of the user study was four hours. To ensure that every
student had some knowledge in parallel programming, we provided
a 2-hour tutorial on task parallelism, and on writing and debugging
task parallel programs using Intel TBB. We gave multiple examples
to demonstrate parallelism bottlenecks.
After the tutorial, the participants were given a total of four
applications and were asked to identify parallelism bottlenecks
without using TaskProf in a one hour time period. Among them,
three applications — minSpanForest, convexHull, and blackscholes
— were from Table 1 and a treesum application was similar to the
example in Figure 2. We chose these applications as they had vary-
ing levels of difficulty in diagnosing parallelism bottlenecks. We
asked the participants to identify the static region of code causing
the bottleneck and record the time they spent to analyze each pro-
gram. They were not required to design any optimization. Some
participants used gprof and others used fine-grained wall clock
based timing for assistance. At the end of the time period, twelve
of them did not correctly identify parallelism bottlenecks in any
of the four applications. One participant, who had 4+ years of ex-
perience in parallel programming, identified the bottleneck in one
(minSpanForest) application.
Subsequently after the first part, we gave a brief tutorial of
TaskProf on a simple example program. The participants were
then asked to identify bottlenecks in the four applications using
TaskProf within an hour. Using TaskProf , seven participants
found the parallelism bottleneck in all the four applications, one
participant found the bottleneck in three of them, four participant
found the bottleneck in two of them, and one participant did not
find the bottleneck in any application. Among the participants who
identified at least one bottleneck for any application, it took them
12 minutes on average per application to identify the bottleneck
using TaskProf . The participants indicated that once they became
familiar with the tool by identifying a bottleneck in one application,
subsequent tasks were repetitive. In summary, our user study sug-
gests that programmers can quickly identify parallelism bottlenecks
using TaskProf .
Threats to validity. Our user study uses a repeated-measures
experiment, which can introduce order effects. Students had an
opportunity to study the code and attempt to optimize it during
the first phase before they were given TaskProf .
6 RELATED WORK
There is a large body of work to identify parallelism bottlenecks.
These include techniques to address load imbalances [ 12,24,34,40],
scalability bottlenecks [ 31,38,41], visualizing bottlenecks [ 13–
15,25], synchronization bottlenecks [ 7,11,45], and data locality bot-
tlenecks [1, 28, 30]. Data locality and synchronization bottlenecks
increase serial work. Hence, TaskProf will report asymptotic par-
allelism in their presence. In contrast to prior proposals, TaskProf
also estimates the improvement in parallelism with causal profiling.
Next, we focus on the closest related work.
Profiling tools for task parallel programs. Profiling tools
such as HPCToolkit [ 2], and Intel VTune Amplifier [ 9] can analyzea program’s performance on various parameters using hardware
performance counters. HPCToolKit also has metrics to quantify
idleness and the scheduling overhead [ 41] in Cilk programs that is
specific to a machine. They do not compute the asymptotic paral-
lelism in the program. They also do not identify code that matters
with respect to asymptotic parallelism. CilkView [ 21] computes
the whole program asymptotic parallelism. CilkProf [ 38] computes
asymptotic parallelism per spawn site using an online algorithm.
However, these profilers execute the program serially, which is only
possible with Cilk programs with C-elision [ 16]. Many task paral-
lelism frameworks including Intel TBB do not have serial semantics,
which limits their use. Further, executing the profiler serially can
cause high overheads. Unlike TaskProf , they also cannot estimate
the benefits of optimizing specific regions of code.
Performance estimation tools. An early profiling technique
proposed Slack [ 22], which is a metric that estimates the improve-
ment in execution time through critical path optimizations for a
specific machine model. Kremlin [ 17] identifies regions of code
that can be parallelized in serial programs by tracking loops and
identifying dependencies between iterations. Kismet [ 23] builds on
Kremlin to estimate speedups for the specific machine on which the
serial program is executed. These techniques are tied to a specific
machine and cannot estimate asymptotic parallelism improvements.
Our work is inspired by Coz [10], a causal profiler for multi-
threaded programs that automatically identifies optimization op-
portunities and quantifies their impact on a metric of interest, such
as latency or throughput. It runs periodic experiments at runtime
that virtually speed up a single randomly selected program frag-
ment. Virtual speedups produce the same effect as real speedups
by uniformly slowing down code executing concurrently with the
fragment, causing the fragment to run relatively faster. In a task
parallel context, it is not possible to slow down all active tasks.
Further, slowing down threads does not measure the impact of
the region as work stealing dynamically balances the load. Fur-
ther, Coz’s virtual speedups are specific to a particular machine.
TaskProf , though similar in spirit, addresses the above challenges
and proposes a causal profiler that leverages the dynamic execution
structure and estimates improvements in asymptotic parallelism.
Hence, TaskProf ’s profile is not specific to a single machine and
enables the development of performance portable code.
7 CONCLUSION
TaskProf identifies parallelism bottlenecks by performing a low-
overhead, yet fine-grained attribution of work to various parts of
the program using the dynamic execution structure of a task parallel
execution. TaskProf reports asymptotic parallelism and serial work
performed at each spawn site. TaskProf ’s causal profile estimates
the improvements in parallelism when regions of code annotated
by the programmer are optimized. We have identified bottlenecks
and improved the speedup in numerous Intel TBB applications. Our
user study shows that developers can quickly identify parallelism
bottlenecks using TaskProf .
A ARTIFACT DESCRIPTION
TheTaskProf prototype is open source and is publicly available at
https://github.com/rutgers-apl/TaskProf. The artifact contains the
24A Fast Causal Profiler for Task Parallel Programs ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
following folders: (1) ptprof_lib contains the implementation of
the profiler for Intel TBB programs, (2) tprof-tbb-lib contains the
modified Intel TBB library, and (3) tests contains simple programs
used to illustrate TaskProf ’s usage. In addition, we also separately
provide applications that we used to evaluate TaskProf .
A.1 Setup
Requirements. TaskProf must be executed on a modern Linux
machine that supports hardware performance counters. The fol-
lowing command can be used to check if hardware performance
counters are supported on the machine.
$ dmesg | grep PMU
When the output of the command contains “Performance Events:
Unsupported...”, then the machine does not support performance
counters. TaskProf will not be functional on such a machine.
TaskProf uses the perf events module in Linux to read per-
formance counters. To check the support for perf events on the
machine, users can check the existence of perf_event_paranoid
file in the /proc/sys/kernel/ folder:
$ ls /proc/sys/kernel/perf_event_paranoid
Installation. The artifact contains a bash script to automate
the installation of TaskProf and the modified TBB library. The
script uses perf command line utility in Linux to check if hardware
performance counters are supported. To install perf on a Ubuntu
Linux machine, execute the following command:
$ sudo apt-get install linux-tools-common
linux-tools-generic linux-tools- ⁄asciigrave.Varuname -r ⁄asciigrave.Var
Let<TP_ROOT >refer to the base directory of the artifact. To
install TaskProf execute the following commands.
$ cd <TP_ROOT>
$ source build.sh
A successful build will create TaskProf ’s shared libraries and
also setup the appropriate environment variables.
A.2 Usage
We illustrate the usage of TaskProf using the tree_sum program in
thetests directory. To generate a profile for the program, execute:
$ cd <TP_ROOT>/tests/tree_sum
$ make
$ ./tree_sum
The profile execution will create data files, which can be analyzed
using TaskProf ’s profile analysis tool. To generate the parallelism
profile from the profile data file, execute:
$ $TP_GENPROF/gentprof
Here, TP_GENPROF is an environment variable that is already
setup by the build script. The parallelism profile will be generated
in the file ws_profile.csv . The first row specifies the parallelism
of the entire program and the percentage of critical work performed
by the main function. Other rows specify the parallelism and the
critical work percentage for each spawn site in the program. To
illustrate causal profiling, we have already annotated a region (in
fileTreeMaker.h ). The causal profile estimating the improvements
in parallelism for the entire program on optimizing the annotatedregion is generated in the file region_all.csv . It reports the in-
crease in parallelism in the program when the critical work in the
annotated region is reduced by 50 ×, 100×, 200×, and 400×.
A.3 Reproducing Results
Effectiveness. Using TaskProf , we were able to optimize five
applications. These applications and their optimized versions can be
downloaded from https://goo.gl/MzP4Tq. We provide the optimized
version of each application in a separate directory. Assuming the
applications have been extracted to the benchmarks folder within
TP_ROOT directory, each application can be compiled and profiled
with TaskProf as follows:
$ cd <TP_ROOT>/benchmarks/<benchmark>
$ make
$ sh run.sh
The optimized version of each application can be compiled and
executed similarly. To ease this process, we also provide a python
script to compile and execute all applications and their optimized
versions.
$ cd <TP_ROOT>/benchmarks
$ python run_bmarks_opt.py > report_opt.txt
The parallelism profile before the optimization and the causal
profile can in found in the ws_profile.csv andregion_all.csv
files in the folder corresponding to the application. The parallelism
profile after optimization can be found in the ws_profile.csv file
in the folder containing the optimized version of the application.
Performance. To reproduce the results comparing TaskProf ’s
parallel profile execution to serial profile execution, download all
benchmark applications from https://goo.gl/svCcZH and extract
them to the benchmarks folder within the TP_ROOT directory.
The script included in the artifact uses jgraph , a postscript graph-
ing tool, to generate graphs. To install jgraph on Ubuntu, execute
the following command:
$ sudo apt-get install jgraph
To convert the postscript graph generated by jgraph to a pdf,
we use epstopdf . To install epstopdf on Ubuntu, execute the fol-
lowing command:
$ sudo apt-get install texlive-font-utils
The artifact includes a python script that executes TaskProf
on all the benchmarks and generates the speedup graph as output.
Profile all applications using the script as shown below:
$ cd <TP_ROOT>/benchmarks
$ python run_bmarks_speedup.py > report.txt
Many applications take a reasonable amount of time to complete
execution. We suggest using the nohup command to run the script.
Our python script reproduces the speedup graph in a pdf file named
Speedup_graph.pdf .
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their feedback. This paper
is based on work supported in part by NSF CAREER Award CCF–
1453086, a sub-contract of NSF Award CNS–1116682, and a NSF
Award CNS–1441724.
25ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Adarsh Yoga and Santosh Nagarakatte
REFERENCES
[1]Umut A. Acar, Guy E. Blelloch, and Robert D. Blumofe. 2000. The Data Locality of
Work Stealing. In Proceedings of the Twelfth Annual ACM Symposium on Parallel
Algorithms and Architectures (SPAA) . 1–12.
[2]L. Adhianto, S. Banerjee, M. Fagan, M. Krentel, G. Marin, J. Mellor-Crummey, and
N. R. Tallent. 2010. HPCTOOLKIT: Tools for Performance Analysis of Optimized
Parallel Programs Http://Hpctoolkit.Org. Concurrency and Computation : Practice
& Experience - Scalable Tools for High-End Computing (2010), 685–701.
[3]C. Bradford Barber, David P. Dobkin, and Hannu Huhdanpaa. 1996. The Quickhull
Algorithm for Convex Hulls. ACM Transactions on Mathematical Software (TOMS)
22, 4 (Dec. 1996), 469–483.
[4]Christian Bienia, Sanjeev Kumar, Jaswinder Pal Singh, and Kai Li. 2008. The
PARSEC Benchmark Suite: Characterization and Architectural Implications. In
Proceedings of the 17th International Conference on Parallel Architectures and
Compilation Techniques (PACT) . 72–81.
[5]Vincent Cavé, Jisheng Zhao, Jun Shirako, and Vivek Sarkar. 2011. Habanero-Java:
The New Adventures of Old X10. In Proceedings of the 9th International Conference
on Principles and Practice of Programming in Java (PPPJ) . 51–61.
[6]Philippe Charles, Christian Grothoff, Vijay Saraswat, Christopher Donawa, Allan
Kielstra, Kemal Ebcioglu, Christoph von Praun, and Vivek Sarkar. 2005. X10: An
Object-oriented Approach to Non-uniform Cluster Computing. In Proceedings
of the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming,
Systems, Languages, and Applications (OOPSLA) . 519–538.
[7]Guancheng Chen and Per Stenstrom. 2012. Critical Lock Analysis: Diagnosing
Critical Section Bottlenecks in Multithreaded Applications. In Proceedings of the
International Conference on High Performance Computing, Networking, Storage
and Analysis (SC) . 71:1–71:11.
[8]Intel Corporation. 2016. Intel(R) 64 and IA-32 Architectures Software Developer’s
Manual, Combined Volumes: 1, 2A, 2B, 2C, 2D, 3A, 3B, 3C and 3D .
[9]Intel Corporation. 2017. Intel VTune Amplifier 2017. (2017). Retrieved July 1,
2017 from https://software.intel.com/en-us/intel-vtune-amplifier-xe
[10] Charlie Curtsinger and Emery D. Berger. 2015. Coz: Finding Code That Counts
with Causal Profiling. In Proceedings of the 25th Symposium on Operating Systems
Principles (SOSP) . 184–197.
[11] Florian David, Gael Thomas, Julia Lawall, and Gilles Muller. 2014. Continuously
Measuring Critical Section Pressure with the Free-lunch Profiler. In Proceedings
of the 2014 ACM International Conference on Object Oriented Programming Systems
Languages and Applications (OOPSLA) . 291–307.
[12] Luiz DeRose, Bill Homer, and Dean Johnson. 2007. Detecting Application Load
Imbalance on High End Massively Parallel Systems. In Proceedings of the 13th
International Euro-Par Conference on Parallel Processing (Euro-Par) . 150–159.
[13] Kristof Du Bois, Stijn Eyerman, Jennifer B. Sartor, and Lieven Eeckhout. 2013.
Criticality Stacks: Identifying Critical Threads in Parallel Programs Using Syn-
chronization Behavior. In Proceedings of the 40th Annual International Symposium
on Computer Architecture (ISCA) . 511–522.
[14] Kristof Du Bois, Jennifer B. Sartor, Stijn Eyerman, and Lieven Eeckhout. 2013.
Bottle Graphs: Visualizing Scalability Bottlenecks in Multi-threaded Applications.
InProceedings of the 2013 ACM SIGPLAN International Conference on Object
Oriented Programming Systems Languages and Applications (OOPSLA) . 355–372.
[15] Stijn Eyerman, Kristof Du Bois, and Lieven Eeckhout. 2012. Speedup Stacks:
Identifying Scaling Bottlenecks in Multi-threaded Applications. In Proceedings
of the 2012 IEEE International Symposium on Performance Analysis of Systems &
Software (ISPASS) . 145–155.
[16] Matteo Frigo, Charles E. Leiserson, and Keith H. Randall. 1998. The Implementa-
tion of the Cilk-5 Multithreaded Language. In Proceedings of the ACM SIGPLAN
1998 Conference on Programming Language Design and Implementation (PLDI) .
212–223.
[17] Saturnino Garcia, Donghwan Jeon, Christopher M. Louie, and Michael Bedford
Taylor. 2011. Kremlin: Rethinking and Rebooting Gprof for the Multicore Age.
InProceedings of the 32Nd ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI) . 458–469.
[18] Susan L. Graham, Peter B. Kessler, and Marshall K. Mckusick. 1982. Gprof: A
Call Graph Execution Profiler. In Proceedings of the 1982 SIGPLAN Symposium on
Compiler Construction (SIGPLAN) . 120–126.
[19] Susan L. Graham, Peter B. Kessler, and Marshall K. McKusick. 1983. An execution
profiler for modular programs. Software: Practice and Experience 13, 8 (1983),
671–685.
[20] Dan Grossman and Ruth E. Anderson. 2012. Introducing Parallelism and Con-
currency in the Data Structures Course. In Proceedings of the 43rd ACM Technical
Symposium on Computer Science Education .
[21] Yuxiong He, Charles E. Leiserson, and William M. Leiserson. 2010. The Cilkview
Scalability Analyzer. In Proceedings of the Twenty-second Annual ACM Symposium
on Parallelism in Algorithms and Architectures (SPAA) . 145–156.
[22] Jeffrey K. Hollingsworth and Barton P. Miller. 1994. Slack: A New Performance
Metric for Parallel Programs . Technical Report. University of Wisconsin-Madison.[23] Donghwan Jeon, Saturnino Garcia, Chris Louie, and Michael Bedford Taylor.
2011. Kismet: Parallel Speedup Estimates for Serial Programs. In Proceedings of
the 2011 ACM International Conference on Object Oriented Programming Systems
Languages and Applications (OOPSLA) . 519–536.
[24] Melanie Kambadur, Kui Tang, and Martha A. Kim. 2014. ParaShares: Finding the
Important Basic Blocks in Multithreaded Programs. In Proceedings of Euro-Par
2014 Parallel Processing: 20th International Conference (Euro-Par) . 75–86.
[25] Andreas Knüpfer, Holger Brunst, Jens Doleschal, Matthias Jurenz, Matthias Lieber,
Holger Mickler, Matthias S Müller, and Wolfgang E Nagel. 2008. The vampir
performance analysis tool-set. In Tools for High Performance Computing . 139–155.
[26] Doug Lea. 2000. A Java Fork/Join Framework. In Proceedings of the ACM 2000
Conference on Java Grande (JAVA) . 36–43.
[27] Daan Leijen, Wolfram Schulte, and Sebastian Burckhardt. 2009. The Design
of a Task Parallel Library. In Proceedings of the 24th ACM SIGPLAN Conference
on Object Oriented Programming Systems Languages and Applications (OOPSLA) .
227–242.
[28] Xu Liu and John Mellor-Crummey. 2011. Pinpointing Data Locality Problems
Using Data-centric Analysis. In Proceedings of the 9th Annual IEEE/ACM Interna-
tional Symposium on Code Generation and Optimization (CGO) . 171–180.
[29] Xu Liu and John Mellor-Crummey. 2013. A Data-centric Profiler for Parallel
Programs. In Proceedings of the International Conference on High Performance
Computing, Networking, Storage and Analysis (SC) . 28:1–28:12.
[30] Xu Liu and John Mellor-Crummey. 2013. Pinpointing data locality bottlenecks
with low overhead. In 2013 IEEE International Symposium on Performance Analysis
of Systems and Software (ISPASS) . 183–193.
[31] Xu Liu and Bo Wu. 2015. ScaAnalyzer: A Tool to Identify Memory Scalability
Bottlenecks in Parallel Programs. In Proceedings of the International Conference for
High Performance Computing, Networking, Storage and Analysis (SC) . 47:1–47:12.
[32] Michael McCool, Arch Robison, and James Reinders. 2012. Structured Parallel
Programming: Patterns for Efficient Computation . Morgan Kaufmann.
[33] B. P. Miller, M. Clark, J. Hollingsworth, S. Kierstead, S. S. Lim, and T. Torzewski.
1990. IPS-2: The Second Generation of a Parallel Program Measurement System.
IEEE Transactions on Parallel and Distributed Systems 1, 2 (April 1990), 206–217.
[34] Jungju Oh, Christopher J. Hughes, Guru Venkataramani, and Milos Prvulovic.
2011. LIME: A Framework for Debugging Load Imbalance in Multi-threaded Exe-
cution. In Proceedings of the 33rd International Conference on Software Engineering
(ICSE) . 201–210.
[35] Yoshihiro Oyama, Kenjiro Taura, and Akinori Yonezawa. 2000. Online Computa-
tion of Critical Paths for Multithreaded Languages. In Proceedings of the 15 IPDPS
2000 Workshops on Parallel and Distributed Processing (IPDPS) . 301–313.
[36] Raghavan Raman, Jisheng Zhao, Vivek Sarkar, Martin Vechev, and Eran Yahav.
2012. Scalable and Precise Dynamic Datarace Detection for Structured Parallelism.
InProceedings of the 33rd ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI) . 531–542.
[37] James Reinders. 2007. Intel Threading Building Blocks . O’Reilly & Associates, Inc.
[38] Tao B. Schardl, Bradley C. Kuszmaul, I-Ting Angelina Lee, William M. Leiserson,
and Charles E. Leiserson. 2015. The Cilkprof Scalability Profiler. In Proceedings of
the 27th ACM Symposium on Parallelism in Algorithms and Architectures (SPAA) .
89–100.
[39] Julian Shun, Guy E. Blelloch, Jeremy T. Fineman, Phillip B. Gibbons, Aapo Kyrola,
Harsha Vardhan Simhadri, and Kanat Tangwongsan. 2012. Brief Announcement:
The Problem Based Benchmark Suite. In Proceedings of the 24th Annual ACM
Symposium on Parallelism in Algorithms and Architectures (SPAA) . 68–70.
[40] Nathan R. Tallent, Laksono Adhianto, and John M. Mellor-Crummey. 2010. Scal-
able Identification of Load Imbalance in Parallel Executions Using Call Path
Profiles. In Proceedings of the 2010 ACM/IEEE International Conference for High
Performance Computing, Networking, Storage and Analysis (SC) . 1–11.
[41] Nathan R. Tallent and John M. Mellor-Crummey. 2009. Effective Performance
Measurement and Analysis of Multithreaded Applications. In Proceedings of the
14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
(PPoPP) . 229–240.
[42] Nathan R. Tallent, John M. Mellor-Crummey, and Allan Porterfield. 2010. Ana-
lyzing Lock Contention in Multithreaded Applications. In Proceedings of the 15th
ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
(PPoPP) . 269–280.
[43] Adarsh Yoga and Santosh Nagarakatte. 2017. TaskProf. (2017). Retrieved July 1,
2017 from https://github.com/rutgers-apl/TaskProf
[44] Adarsh Yoga, Santosh Nagarakatte, and Aarti Gupta. 2016. Parallel Data Race
Detection for Task Parallel Programs with Locks. In Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of Software Engineering
(FSE) . 833–845.
[45] Tingting Yu and Michael Pradel. 2016. SyncProf: Detecting, Localizing, and
Optimizing Synchronization Bottlenecks. In Proceedings of the 25th International
Symposium on Software Testing and Analysis (ISSTA) . 389–400.
26