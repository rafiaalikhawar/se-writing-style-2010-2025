How Does the Degree of Variability Affect Bug Finding?
Jean Melo, Claus Brabrand, Andrzej W Àõ asowski
IT University of Copenhagen, Denmark
{jeanmelo,brabrand,wasowski}@itu.dk
ABSTRACT
Software projects embrace variability to increase adaptabil-
ity and to lower cost; however, others blame variability for
increasing complexity and making reasoning about programs
more dicult. We carry out a controlled experiment to quan-
tify the impact of variability on debugging of preprocessor-
based programs. We measure speed and precision for bug
nding tasks dened at three dierent degrees of variability
on several subject programs derived from real systems.
The results show that the speed of bug nding decreases
linearly with the degree of variability, while eectiveness
of nding bugs is relatively independent of the degree of
variability. Still, identifying the set of congurations in
which the bug manifests itself is dicult already for a low
degree of variability. Surprisingly, identifying the exact set
of aected congurations appears to be harder than nding
the bug in the rst place. The diculty in reasoning about
several congurations is a likely reason why the variability
bugs are actually introduced in congurable programs.
We hope that the detailed ndings presented here will
inspire the creation of programmer support tools addressing
the challenges faced by developers when reasoning about
congurations, contributing to more eective debugging and,
ultimately, fewer bugs in highly-congurable systems.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging
Keywords
Variability, Preprocessors, Bug Finding
1. INTRODUCTION
A recent study reports that the global cost of debugging
software has risen to 312 billion dollars annually, and that
on average, software developers spend half of their program-
ming time nding and xing bugs [ 14]. This is particularly
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14-22, 2016, Austin, TX, USA
¬© 2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.2884831worrisome in the context of variability. Software projects em-
brace variability hoping to increase exibility at lower cost
(to better control system resources, to extend portability
across dierent hardware, to meet requirements of various
market segments). However, multiple research indicate that
variability might also amplify maintenance problems. Recent
literature on variability is riddled with claims to that end.
We list a few examples: \bug-nding is a time-consuming and
tedious task in the presence of variability" [30];\managing
variability can become complex" [34];\variability specications
and realizations tend to erode in the sense that they become
overly complex." [36];\understandability and maintainability
may be negatively aected" [12].
However reasonable, there is little to no hard evidence
for these claims. Specically, how are maintenance tasks
(bug nding in particular) aected by variability? How much
harder is it to debug a program as variability increases? Does
variability aect speed or also quality of debugging? In this
paper, we set o to understand such issues using a controlled
experiment designed to quantify the impact of the degree of
variability in program code on bug nding.
In the experiment we use simplications of real bugs ex-
tracted from Linux ,BusyBox , and BestLap . We dene
three degrees of variability: no, low, and high; corresponding
to zero, one, and three features, respectively. Given a pro-
gram and a degree of variability, we ask the participants to
debug the programs. In summary, we learn that:
The time needed for nding bugs increases only linearly
with the amount of features, with the time becoming
less predictable with more features. The dierences in
eectiveness of various programmers are amplied by
the increase of variability.
Most developers correctly identify bugs (partial correct-
ness), yet many fail to identify the set of aected cong-
urations (complete correctness). This is consistent with
earlier hypotheses that programmers introduce errors
because it is dicult to reason about all the execu-
tions involved via conguration choices. Interestingly,
this ability does not seem to improve with increasing
level of education, while general, non-variability related,
bug-nding skills do seem to improve.
Challenges with reasoning about congurations are
already measurable for low degrees of variability.
The intended audience of this work are designers of vari-
ability management tools, of bug-nding tools, and of other
supporting methods, such as variability-aware software archi-
tectures, aiming at improving eciency and correctness in
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   679
(a) No variability (zero features).
 (b) Low variability (one feature).
 (c) High(er) variability (three features).
Figure 1: A program with an uninitiliazed variable error with progressively increasing degrees of variability.
development of variability-intensive systems. We hope that
the insights provided will inuence new tools and methods
that can help avoiding bugs like the one presented in the
next section, or help debugging programs that contain them.
We also hope to inspire software architecture researchers
to (further) understand the trade-os between increasing
variability and development cost, by also including potential
debugging costs.
2. MOTIV ATING SCENARIO
Today, variability-intensive software systems include both
large industrial product lines [ 8,26,4,3] and open-source
systems of various sizes, up to the Linux kernel with more
than 13,000 congurable features [5 ]. A multitude of tech-
nologies can be used to implement congurable systems:
object-oriented patterns, aspects, domain-specic languages
and code generation, plugin mechanisms, and so on. Among
these, the C preprocessor ( cpp) is one of the oldest, the
simplest, and the most popular [9,19,17] mechanisms in use,
especially in the systems domain. For these reasons, we use
the preprocessor in our study. The results do not generalize
to other mechanisms, but provide a good indication, given
that the other solutions are more complex and require much
more additional code to handle variability.
Figure 1presents an example extracted from the Netpoll
module of Linux kernel, slightly adapted to Java syntax
using coloured lines instead of preprocessor [17 ]. Netpoll is
an API that provides a means to implement UDP clients and
servers in the kernel independently of the main networking
stack. These can be used in unusual situations, like failure
monitoring, crash recovery, or debugging of the kernel. The
original function, called netpoll_setup in C, is used to ini-
tialize the module. It is about 100 lines long and involves one
optional feature. Historic versions of the function, contained
anerror .1If the feature is disabled, the function returns the
value of an uninitialized variable ,err, intended to hold an
error value in case of unexpected situations.
We illustrate how the task of debugging becomes more
complex as the numbers of features increase. Figure 1a
shows a version of the bug as a conventional program without
variability. It is fairly easy to establish that the function
returns the value of an uninitialized variable in line 10. (In
line 6, the variable flag is assigned false which means that
1http://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/commit/
?id=e39363a9def53dd4086be107dc8b3ebca09f045dthe conditional statement in line 8 is not executed; hence,
the variable errwhich was declared and uninitialized in line
2, is never assigned a value. Now, since the value of the
variable ipv4 istrue (line 3), the conditional statement in
line 10 returns the value of the uninitialized variable err.)
Figure 1bcontains the same program, but now involving
one feature shown in light gray background color. A feature,
such as light gray in this example, can be congured either
asenabled ordisabled . Features are used in compile-time
conditional directives ( #ifdef s) to control whether to in-
clude orexclude code fragments in a program. We use the
conventions of Cide [17] which assign colors to features and
show conditional statements using background colors rather
than #ifdef s. We discuss implications of this dierence in
Section 5. A colored statement is to be included in a program
if and only if the corresponding feature (color) is enabled .
Obviously, a feature thus gives rise to twopossible cong-
urations: a program with the light gray statement, and a
program without it. In general, nfeatures will give rise to
2ncongurations; i.e., 2nprograms.
Now programming errors conditionally depend on congu-
rations. Indeed, the error in Figure 1boccurs only whenever
weenable the light gray feature. If the light gray statement
in line 6 is included, the variable erris not initialized in
line 8. If we disable the light gray feature, the error no longer
occurs; errwould then be initialized in line 8.
Ultimately, this leads to a notion of partial correctness
for debugging. A developer can correctly diagnose the error
(nd the bug), but incorrectly misdiagnose the exact set of
congurations (the combinations of features producing the
bug). In the case of a single feature, partial correctness
(misdiagnosis) means mixing up enabled and disabled.
The problems and diculties escalate when we consider
more features; i.e., higher degrees of variability. Figure 1c
shows the same programs as before, but now with three fea-
tures: light gray, gray, and dark gray. The three features yield
eight congurations. In debugging the program, the devel-
oper must somehow consider all congurations. Determining
the erroneous products becomes a non-trivial combinatorial
problem which, as we shall see in Section 4, is dicult. In-
deed, for our program in Figure 1c, the error now occurs in
exactly three (out of eight) congurations.
3. EXPERIMENT
In this section we explain our experimental design and setup.
680(a)P2.
 (b)P3.
Figure 2: Programs P2and P3with HIvariability degree; P1is a larger version of Fig. 1c.
Prg Origin Filename Bug type LOC #mth
P1 Linux netpoll.cUninitialized21 2variable
P2 BusyBox http.cNull-pointer29 3dereference
P3 BestLap GameScreen.javaAssertion31 4violationPrg Degree jFj Scattering Tangling VCC
P1 NO/ LO/HI 0/1/3 0/1/3 0/2/6 5/6/7
P2 NO/ LO/HI 0/1/3 0/3/4 0/4/8 7/8/9
P3 NO/ LO/HI 0/1/3 0/5/10 0/6/15 8/12/14
Figure 3: Characteristics of our three benchmark programs: P1,P2, and P3.
3.1 Objective
Our experiment aims to analyze the impact of variability
on bug nding. We want to understand exactly how much
harder does the debugging task become as the degree of
variability in a program increases? Specically, we aim to
answer the following research questions:
RQ1:How does the degree of variability aect
thetime of bug nding?
RQ2:How does the degree of variability aect
theaccuracy of bug nding?
To address these research questions, we perform a range of
classic \nd the bug" experiments [ 25] and measure the time
and accuracy of the bug-nding task. We expose develop-
ers to programs with dierent degrees of variability, while
controlling for noise factors such as learning, developer com-
petence and program complexity. We measure accuracy as
the number of correct vs incorrect identications of bugs.
We are particularly interested in the time and accuracy of
bug nding as functions of the degree of variability.
3.2 Treatments
In order to study debugging as a function of variability, we
expose each participant to programs with dierent degrees of
variability , so programs using dierent amounts of conditional
compilation blocks. We settled on using three distinct degrees
of variability. Let Fdenote the set of features (conditional
compilation symbols used in a program). First, to establisha baseline, we consider programs with no variability (degree
NO,F=;). Then we consider programs that use one feature
(degree LO,jFj= 1) and programs with three features (degree
HI,jFj= 3). The number of congurations grows from one
for degree NOprograms, two for LOprograms, to eight for
HI. This should make any performance dierences manifest
themselves clearly. Even though it would be interesting to
study higher degrees, the limitation to three features has one
important advantage: it leaves us with programs suciently
small to be used in a time-delimited controlled experiment.
We derive programs of lower degrees by taking an erroneous
program with three features (see below) and appropriately
x features as either enabled ordisabled retaining the original
error. We thus obtain three versions of each program: \ NO",
\LO", and \ HI" (much like in Fig. 1).
3.3 Subjects
We now turn to the subjects of the experiment which are
aected by the treatments; the participants and the programs .
Participants. We performed the experiment with N=69
participants: 31 M.Sc. students, 32 Ph.D. students, and 6
post-docs. The M.Sc. students came from two courses at
the IT University of Copenhagen: \Interactive Web Services
using Java and XML" and\System Architecture and Security" .
The Ph.D. students and post-docs came from three Danish
universities: IT University of Copenhagen (ITU), University
of Copenhagen (KU), and Technical University of Denmark
(DTU). We informed all participants that they could stop
681participating at any time.
All participants had programming experience, especially
inJava , and around half of the participants had industrial
experience ranging from a few months to several years.
Programs. For the robustness of our experiment|in order
to minimize risks of specic eects from particular programs
and bug types|we took three programs with dierent kinds
of errors. We based our programs on realvariability errors
from three highly-congurable systems: Linux [1],Busy-
Box2, and BestLap [27]. These are qualitatively dierent
systems in terms of size, architecture, purpose, variability,
and complexity. Linux is an operating system and is likely
the largest highly-congurable open-source system with more
than 12 MLOC and 13,000 features. BusyBox is an open-
source highly-congurable system with 204 KLOC and about
600 features, that provides several essential Unix tools in
a single executable le. BestLap is a commercial highly-
congurable race game with about 15 KLOC. The kinds of
errors we consider are also dierent: an uninitialized variable ,
anull-pointer dereference, and an assertion violation . We
simplied the error in each system down to an erroneous
program that would t on a screen without scrolling (25-35
lines) yet involve exactly three features.
Figure 2shows the programs P2andP3with HIvariability
degree ( P1is a larger version of Fig. 1c). The LOand NO
variability programs are obtained from the HIvariants by
selecting a feature and conguration that preserves the bug
and inuences the program size as little as possible, so that
it can still be comparable to the HIvariant. In the following,
we describe the bugs used in the experiment of HIvariability
degree, informing the type and erroneous congurations of
each bug.
Bug description of P1.P1has one only method which
contains conditional statements and an integer local variable
called err, as shown in Fig. 1c. The three features yield
eight possible congurations. In debugging the program,
the developer must somehow consider all congurations. To
accomplish the task, the developer needs to identify that the
variable errisuninitialized in exactly three (out of eight)
congurations. Indeed, for our program in Figure 1c, the
error occurs in the following congurations by enabling only:
(i) dark gray; (ii) light gray and dark gray; (iii) light gray,
gray, and dark gray. In these erroneous congurations, the
variable erris not initialized in line 8.
Bug description of P2.P2contains two methods to handle
incoming HTTP requests (see Figure 2a). It also has a
variable ( subject ) that may be null and is dereferenced for
certain congurations. So, to identify the bug, the developer
should realize that this happens in three congurations. In
fact, the error in Figure 2aoccurs only when we disable the
green feature. Thus, the erroneous congurations are (when
weenable only): (i) blue; (ii) yellow; (iii) blue and yellow.
If the sendHeaders method is called by either the yellow or
blue features, the variable subject will be null whenever the
green feature is disabled, and in line 15 there is an access to
subject which may cause null pointer exception.
Bug description of P3.P3has three methods responsible
for computing the score, as can be seen in Fig. 2b. According
to a user requirement, the game should also compute negative
scores. But, the method setScore contains a condition
2http://git.busybox.net/busybox/commit/?id=
5cd6461b6fb51e8cf297a49074fce825e1960774prohibiting negative scores. We encode the requirement using
assertions. To nd the bug, the developer should consider all
congurations and somehow see that the variable totalScore
is always equal to zero in the end of setScore computation,
when passing negative values to the method, which is revealed
through an assertion violation in the code (line 26). For our
program in Figure 2b, the presence condition of the error is:
blue^yellow . Thus, the assertion error occurs in exactly
two congurations: (i) blue and yellow; (ii) blue, yellow, and
green.
Figure 3lists various characteristics for each of our pro-
grams. The left-hand side gives the basic characteristics of
the programs; their origins (project and le name), bug type,
number of lines of code (excluding whitespace and comments),
and number of methods. The right-hand side lists variability
metrics for each of the degrees: feature scattering ,feature
tangling, and variational cyclometric complexity (vcc). We
show metrics for each of the degrees using a slash-separated
notation ( NO/LO/HI). For each feature, scattering counts the
number of conditional-compilation (colored) blocks (based
on the cdc metric [ 29]). We give accumulated numbers for
all features involved. The feature in P2-LO , for example, is
scattered over three locations in the source code. For each
feature, tangling, in turn, counts the number of switches be-
tween regular code and feature code through the control-ow
of the program (based on the cdloc metric [ 29]). Again,
the feature in P2-LO , for instance, requires a concern/scope
change between the code base and the feature code four
times through the program. For vcc, we use the cyclomatic
complexity metric [21 ] on the variability programs, treating
#ifdef s (colored lines) as ordinary ifs.
Notice that the programs are all quite dierent, yet in
terms of complexity they are hierarchically ordered from P1
(simplest) to P3(most complex).
3.4 Design
We present rst a fully randomized experiment design, point
out a problem, and address it using a well-known technique.
In terms of debugging tasks, we have nine, in total: three
programs, each at three degrees of variability. However, a
developer cannot be assigned to perform allnine debugging
tasks. Clearly, we can only have a developer nd bugs in a
given program once, otherwise there would be a learning eect
on subsequent attempts. For similarly obvious reasons, we
can only have a developer consider a variability degree once.
Abiding by these constraints, we can have each developer
debug three dierent programs, each at a dierent variability
degree. However, there might still be learning eects lurking
due to sequencing of assigned tasks. Presumably, debugging
LOafter HIis not the same as debugging HIafter LO, even for
dierent programs. Similarly, debugging an \easy" program
after a \hard" one is not the same as in the reverse order.
Fully Randomized Design. To counter these eects, we
need to randomize the order developers consider the tasks.
Aside from learning eect, we also need to control for other
subject-related noise factors (confounding factors) such as
dierences in developer competence and program complex-
ity. After all, a \competent" developer debugging an \easy"
program will obviously not produce the same result as an
\incompetent" developer debugging a \hard" program. Again,
randomization may be used to control for these eects. For
larger samples these eects will diminish.
Thus, one solution would be, for each developer, to assign
682programs
P1 P2 P3d
evelopersD1 NO LO HI
D2 LO HI NO
D3 HI NO LO
Figure 4: Latin Square (3 3).
programs and degrees completely at random without ever
reusing a program or a degree. This does control for con-
founding factors. However, statistically, we could get vastly
dierent number of data points for the nine debugging tasks.
In particular, we could get a low number of data points for
certain debugging tasks (e.g., P1-NO ). Obviously, this could
compromize the quality of subsequent statistical analysis.
Latin Square Design. However, there is a better solution.
AnnLatin square is an nnmatrix with ndistinct values
as entries with the property that no row or column contains
the same value twice.
Latin squares present a common solution to the above
statistical problem in many experimental setups [24 ,2,6].
Figure 4depicts a 33 Latin square applied to our context.
The columns are labelled with the three subject programs
(P1,P2,P3). The rows are labelled with names of three
subject developers ( D1,D2,D3). The nine squares in the
center contain the three treatments ( NO,LO,HI). Now with
this design, each developer receives all three treatments listed
in his row, for all three subject programs listed in the headers
of the corresponding columns.
We apply the Latin square design to ensure the same
number of data points for all debugging tasks, without com-
promising control over the confounding factors. There are
12 distinct 33 Latin squares, modulo swapping rows and
swapping columns. For each three developers, we randomly
pick one of these 12 Latin squares and randomize also the
assignment of developers and programs to rows and columns.
The result is the same number of data points for all debugging
tasks, without compromising control over the confounding
factors such as developer personal competence or program
complexity. Still, each combination of treatments and pro-
grams is equally probable for each developer (in any order
of programs and in any order of treatments). For N=69
participants, each performing three out of nine tasks, we get
exactly 23 data points for each of the nine debugging tasks.
Technically, our experiment is an instance of the so-called
within-group design in which all subjects are exposed to every
treatment. We apply our treatments (independent variable:
variability with three levels: NO,LO, and HI) to the subjects
(programs and developers). In addition, we distinguish the
tasks for each program since the programs are not equivalent.
We measure our dependent variables; time and number of
correct and incorrect answers for bug-nding tasks.
Data Analysis. We used Anova [6] to test signicance of
dierences between dierent treatments. Anova is heavily
used in controlled experiments and useful in comparing three
or more means for statistical signicance. Anova requires
a normal distribution, variance homogeneity, and model
additivity of the samples. We check these assumptions using
theBox Cox, Bartlett , and Tukey tests, respectively.We conventionally consider a factor as signicant when a
p-value <0.05.
3.5 Execution
Before the actual large-scale experiment on 69 subjects, we
executed a pilot study with a small group of local students to
assess our design and tasks. The results of the pilot are not
considered in our analysis and the number 69 is excluding
the pilot study. Based on the pilot feedback, we changed
mainly the presentation of the tasks.
Before the subject developers were confronted with their
tasks, we presented a simple tutorial on the basics of variabil-
ity; in particular, features, congurations, and compile-time
conditional statements. Also, we demonstrated how to solve
a small warm-up task to demonstrate the nature of the tasks
and what kind of answers are expected. The warm-up task
was inspired by Figure 2 in a paper by Liebig et al. [ 20].
We randomly generated 23 Latin squares as described
above. We then used the Latin squares to compile a task
description sheet for each participant with their three relevant
debugging tasks (e.g., rst P2-LO , second P1-HI , third P3-
NO). Every participant then performs the debugging task (i.e.,
nd the bug) for the three given programs, in sequence.
All task description sheets contained instructions and a
link to an online form for completing the task. We ensured
that each program ts in a single screen to avoid participants
scrolling up and down to see the entire code. We prepared
all tasks using Google forms to avoid heterogeneous environ-
ments and installing software on dierent machines. That is,
we provided to the participants only a static window, i.e., no
IDEs, no tools, no navigation. We recorded timestamps for
each of the participants when they start and nish, allowing
us to calculate the duration of their debugging. In addition
to time, after the experiment, we calculated the number of
correct, incorrect, and partially correct answers.
We eliminated participants who left the experiment early
without completing the task, and returned their Latin square
assignments to the pool of the rows available for further
random allocation. The eliminated (unmotivated) partici-
pants are not included in the 69 gure. No other deviations
happened during execution.
In addition to the quantitative results, we conducted semi-
structured interviews after the experiments. This was to get
qualitative feedback on how the participants approached the
debugging tasks, particularly the ones involving a HIdegree.
We asked two questions: (i) How did you go about nding
theHIbug? and (ii) What were the diculties?
4. RESULTS
We now present the results of our experiment and discuss
the implications. We make eight observations addressing the
research questions|the impact of variability on the time and
accuracy of bug nding. Before proceeding, we stress that
the observations should not be generalized far beyond the
degree of variability for which we ran the experiment; i.e.,
jFj 3. We will elaborate on external validity in Sect. 5.4.
All experiment materials are available online at http://itu.
dk/people/jeam/variability-experiment/ (including data, pro-
grams, task descriptions, and statistical processing scripts).
4.1 How does the degree of variability affect
the time of bug-Ô¨Ånding? (RQ1 )
683Figure 5: Mean bug-nding time (along the y-axis in minutes)
as a function of the degree of variability (x-axis).
We consider the rst research question now.
Observation 1 :Mean bug-nding time appears
to increase linearly with the degree of variability.
Figure 5plots the mean bug-nding times (in minutes, along
they-axis) for each of our three benchmark programs. Each
dot depicts the mean time to nd the bug, for a particular
program ( P1,P2andP3), for a particular degree of variability,
i.e.,NO(jFj= 0), LO(jFj= 1), and HI(jFj= 3). For instance,
the fastest mean bug-nding time is about 31
2minutes (for
program P1with NOvariability), whereas the slowest mean
bug-nding time is a bit less than 10 minutes (for program
P3with a HIvariability degree of jFj= 3). For each program,
we t a regression line to its respective points. The lines
suggests that the mean bug-nding time increases linearly
with the degree of variability. According to an ANOVA
test, the dierence between bug-nding times for distinct
degrees of variability is statistically signicant, with p-value
= 2:010 8. Also bug-nding time is a linear function of
programs and degrees, with p-value = 3: 610 9, by F-test
for regression.
Recall that the number of variant programs to be consid-
ered by a participant grows exponentially with the degree of
variability (i.e.,jKj= 2jFj, assuming all variants constitute
valid programs). Clearly, a developer has to somehow con-
sider each of the 2jFjvariants in order to make an accurate
diagnosis of the bug. Afterall, each of the variants may or
may not harbour a bug. One might then, in fact, suspect
that bug-nding time ought to increase exponentially with
the degree of variability.
The post-treatment interviews provide qualitative insights
into how the participants approached the problem and what
diculties they faced in understanding programs with a
HIvariability degree. The participants agreed that nding
bugs in the NOprograms, so without variability, required less
eort than in programs with HIdegree of variability. One
participant explains:
\I tried to keep all dierent paths in mind, but it
was especially dicult with multiple colors [ HI]."
Along the same lines, another participant says:
\With more variability [ HI] you need to build up
exponentially more traces in your head."
The participants analyze programs as one unit despite vari-
Figure 6: The distribution of bug-nding time.
ability. They do notsplit the task into analysis of exponen-
tially many independent programs, one variant at a time.
An unconscious use of brute force would yield a 2jFjfactor
slow down in overall bug-nding time.
Hick's Law [ 15] from psychology, based on so-called choice-
reaction-time experiments , explains that the amount of time
for a human response increases logarithmically with the num-
ber of possible choices. Compared to a baseline program with
NOvariability, programs with higher degrees of variability
involve exponentially more choices to be made. Obviously,
composing an exponential function with a logarithmic one
yields a linear function. We thus hypothesize that the seem-
ingly linear increase in bug-nding time, in spite of the
exponential blow up, can be attributed to Hick's Law.
Presumably, the more complex the variability of a program,
the more time it would take to nd bugs in that program.
Indeed, Fig. 5is consistent with this expectation: the slopes
of the lines are ordered according to the complexity of the
respective programs. Recall from Figure 1that programs
P1,P2, and P3were increasingly complex both in terms
of variability-unaware and variability-aware characteristics.
Also, we remark that, even if we exclude participants that
failed to correctly identify the bug, we see a picture similar
to that of Fig. 5.3
In summary, the rst observation indicates that an increase
in variability (e.g., by adding features) complicates bug nd-
ing, but not dramatically and not prohibitively so. This is
a very positive nding, that is consistent with existence of
software products with hundreds, even thousands, of features,
testifying that developers in the trenches areable to deal
with variability.
Observation 2 :The variance of bug-nding time
appears to be amplied by the degree of variability.
Figure 6, we plot the distribution of bug-nding times for
each program and variability degree. Each box encapsulates
the middle 50% data points. The lower and upper limit of the
box respectively represent the lower and upper quartiles (the
25% and 75% percentiles). The upper and lower whiskers
represent the data above and below the middle half of the
data. The horizontal line within the box draws up the median
3The diagram can be found in the accompanying materials.
684NO LO HI NO LO HI NO LO HI
P1 P2 P3Ph.D.
M.Sc.
Figure 7: Ranking of fastest (at the bottom) to slowest (top)
participants according to their educational level (i.e., M.Sc.
vs. Ph.D. students).
of the data points. Finally, the circles above the boxes
visualize outliers. For instance, for program P3(the three
rightmost boxes), the middle half of the participants spent
between 31
3and 5 minutes to nd the bug with NOvariability,
whereas, for HIvariability, the middle half spent from about
7 to 101
2minutes. Again, considering only participants that
found the bug yields a similar diagram, consistent with the
above.4
Amplication of variance is a predictable consequence of
our rst nding. For the variance of a stochastic variable, X,
multiplied by a constant factor, c(depending on the degree
of variability), we have that: Var( c X) =c2Var(X ).
In popular terms, this observation means that dierences
in bug-nding competences are amplied when working with
variability. Ultimately, this means that getting talented
developers on such projects is important.
Observation 3 :Ph.D. students appear to not
be faster at nding variability bugs than M.Sc.
students.
Figure 7shows the ranking of bug-nding from the fastest
participants (towards the bottom) to the slowest participants
(towards the top), abstracting away the actual time they
spent debugging. M.Sc. students are shown in light gray,
Ph.D. students in black. There appears to be no pattern of
one group of students being faster than the other, even for
higher degrees of variability.
In the late 1980es, Oman et al. [25 ] compared debugging
abilities of novice, intermediate, and skilled student program-
mers using two programs written in Pascal. Among other
things, they found that experienced programmers nd errors
faster than less experienced programmers. We, in turn, do
not notice any dierence in terms of bug-nding time be-
tween Ph.D. and M.Sc. students. This can be explained by
the level of subjects in that study. They considered only
undergraduate students, separating them according to the
amount of computer science courses taken, whereas we test
with graduate students, who would likely be considered as
skilled programmers in their setup. Furthermore, we study
a dierent phenomenon, which is variability, that might be
challenging independently of education level.
4.2 How does the degree of variability affect
the accuracy of bug-Ô¨Ånding? (RQ2 )
We now turn to our second research question ( RQ2) on the
accuracy of bug-nding:
Observation 4 :Most developers correctly iden-
tify bugs in programs regardless of the degree of
4See the accompanying website for more information.
Figure 8: Ratio of incorrectly vs.correctly identifying a bug.
variability.
Figure 8shows shows what percentage of developers were able
to nd the bugs correctly. The incorrect answers are black,
and the correct ones are gray. The data is presented for each
degree of variability separately. The frequency of incorrect
answers is consistently low, with around a fth being the
incorrect answers. For programs with NOvariability, 16% of
subjects (11 out of 69) did not nd the bug. Even for the HI
variability programs, only 22% of the subjects (15 out of 69)
answered incorrectly.
Generally, developers seem to be good at nding bugs in
programs|and in programs with variability (at least, up to
three features). Interestingly, more than half (38 out of 69)
of the participants correctly identied the bug in allthree
tasks. On average, if we disregard the variability degrees,
79% of the participants were able to correctly nd the bug.
All in all, we conclude that nding bugs in programs seems
to not be signicantly aected by the degree of variability
(at least forjFj 3).
Observation 5 :Many developers fail to exactly
identify the set of erroneous congurations, al-
ready for a low degree of variability.
We now look a little closer at accuracy and split the correct
answers in two sets. If the participant got the set of erroneous
congurations exactly right, we classify her answer as fully
correct. Similarly, we classify answers as partially correct ,
if the developer has correctly identied the bug, but failed
to correctly specify the set of congurations in which the
error occurred (missing some congurations or listing too
many). We ignore incorrectly identied bugs for this part
of the analysis, as it is hard to interpret the identication
of congurations for them. For instance, program P3with
HIvariability, contains an assertion error that occurs in two
(out of eight) congurations. For this task, some participants
found only one of the erroneous congurations and others
listed extra congurations for which the error does not occur.
Figure 9, presents the numbers of fully and partially correct
answers at dierent levels of variability.
Obviously, partial correctness does not make sense for
programs without variability (for NOwe have only one possible
conguration). Already for LOvariability (one feature), we
see that the number of partially correct answers quickly rises
to 17% (9 out of 52). For HIvariability, this number escalates
to almost 40% (20 out of 54).
Identifying the exact set of erroneous congurations seems
to become dicult already for jFj= 3 ( HIvariability). Do-
ing this requires understanding the combinations of features
that enable the incriminated execution paths|a form com-
binatorial reasoning, which apparently becomes dicult fast.
Such problems are notoriously hard for humans. For realis-
tic systems, where a feature model additionally shapes the
set of legal congurations, this task would presumably be
even harder (as one needs to reason about feature model
685Figure 9: Ratio of partially correctly versus fully correctly
identifying a bug.
constraints, in addition).
From a prior qualitative study [ 1], we know that program-
ming errors related to variability appear due to inability
of programmers to correctly reason about all variations of
the program that they are modifying. Those ndings are
consistent with the above: it is plausible that developers
mis-identify the sets of congurations during programming
tasks and during debugging tasks for the same reasons. To
the best of our knowledge, this study presents the rst quan-
titative conrmation that indeed reasoning about multiple
congurations is a challenge, even for relatively small sets.
Observation 6 :For higher degrees of variability,
it appears to be more dicult to correctly identify
the set of erroneous congurations than to nd
the bug in the rst place.
ForHIvariability, we saw that 22% (15 out of 69) did not nd
the bug (see Figure 8). Among the ones that did, a staggering
37% (20 out of 54) erred on set of erroneous congurations
(cf. Figure 9).
Although the participants were only asked to nd the
bugs, not (also) xthem, we nd that our results are con-
sistent with studies of creating and xing bugs. Yin and
coauthors report that in general bug xers \ may forget to
x all the buggy regions with the same root cause ."[35 ]. Our
earlier study [1 ] also reports that bugs are introduced because
the programmers do not realize the complexity of all the
congurations in which their code will run.
Observation 7 :Ph.D. students appear to be
more accurate at nding variability bugs than
M.Sc. students.
Figure 10acompares the ratio of correct-to-incorrect answers
according to educational level, separating M.Sc. students
and Ph.D. students. We see that the number of incorrect
answers for Ph.D. students are consistently low. In fact, even
forHIvariability, only 10% of the Ph.D. students answered
incorrectly. For M.Sc. students, the numbers are consistently
higher. For HIvariability, the number of incorrect answers
are more than three times higher at 35%. On average, the
Ph.D. students found bugs three times more accurately than
M.Sc. students for all degrees of variability. Presumably,
Ph.D. students, having more education, are more careful and
meticulous when debugging than M.Sc. students.
Interestingly, Ph.D. students prevail only as far as identi-
fying the actual bug is concerned, but they are not better in
identifying the relevant set of congurations. For both M.Sc.
and Ph.D. students the percentage of fully correct answers
seems to not signicantly be impacted by variability:
Observation 8 :Identifying the exact set of er-
roneous congurations is hard regardless of edu-
cation (both for M.Sc. and Ph.D. students).
Figure 10bshows the frequency of partially correct answers
for M.Sc. versus Ph.D. students. For HIvariability, forinstance, we see that 40% of M.Sc. students answered partially
incorrect versus 35% for that of Ph.D. students.
The numbers testify that the combinatorial task of identi-
fying the exact combination of features provoking an error is
dicult, regardless of educational level. We see the frequency
ofpartially correct answers double from LOtoHIdegree in
both groups of students.
5. THREATS TO V ALIDITY
5.1 Internal validity
Choice of variability degrees? We chose zero,one, and
three features for pragmatic reasons. If we instead had chosen,
for instance, two,four, and sixfeatures, the experiments
would have required much longer time, discouraging and
tiring participants. In fact, the mean time for program
P3with three features is almost ten minutes. Also, ve
participants spent more than twenty minutes to nd the bug
in programs involving three features.
Choice of language? In this experiment, we adopted
Java as our programming language. This is because we want
to run the experiment with as many students as possible,
andJava is well-known among students in Denmark. We
only admitted students who had experience with Java.
Use of background color? Researchers have shown that
background colors may improve program comprehension and
subjects favor background colors [ 11]. Additionally, the
benets of colors compared to text-based annotations is that
one can clearly distinguish background colors (feature code)
from base code and humans are able to recognize colors faster
than text [ 13]. Thus, we decide to use background colors
instead of preprocessor directives.
Choice of colors? Before the experiment, we check for
color blindness among the participants. We also take care of
choosing colors that are clearly distinguishable (blue, green,
and yellow). Aside from this, we do not believe that the
exact choice of colors matter so much,5for our experiment.
Selection bias? To minimize selection bias, we randomly
assign participants, degrees, and programs into the Latin
squares. So, we control our confounding factors via Latin
square design and randomization. Every participant takes
all treatments, including all the three programs. We did
have few (unmotivated) students who left the experiment
early without completing the tasks. As mentioned earlier,
we eliminated them from the study and returned their Latin
square assignments to the pool of the rows available for
further random allocation.
Participation incentives? We oered a chocolate bar
as a participation incentive. Aside from that, only pride
prohibits participants from deliberately performing poorly.
We found no indications of participants giving deliberately
silly answers.
5.2 Conclusion validity
Statistical tests? In this experiment, we use Anova (in-
cluding verication of its assumptions) to verify whether
or not the means of our test groups are equal by analyz-
ing variance. In fact, Anova is heavily used in controlled
experiments and useful in comparing three or more means
for statistical signicance. Besides that, we are comparing
5The fashion industry may disagree.
686(M.Sc.)
(Ph.D.)
(a) Ratios of incorrect vscorrect answers for M.Sc. students (above)
and Ph.D. students (below).(M.Sc.)
(Ph.D.)
(b) Ratios of partially correct vsfully correct answers for M.Sc.
students (above) and Ph.D. students (below).
Figure 10: Accuracy of bug-nding according to educational level (M.Sc. students vs Ph.D. students).
group means against each other, not specic subjects, which
decreases the impact of developer competence.
Time measured? The time measured to complete a test
involves both thinking as well as writing down the answer.
We asked the participants to write down the bug kind, line,
and a list of erroneous congurations (a conguration is
written as a list of enabled features). Obviously, this might
interfere in the measured time due to variations in writing
speed. However, we observed that the task of writing usually
took tens of seconds whereas thinking took on the order of
minutes. Hence, the speed of the task of writing is dwarfed
by the thinking. Note that we exclude the time of reading the
tasks, as we only start the timer once the participants have
read each task description (i.e., when they see the programs).
5.3 Construct validity
Do participants know what to do? Before exposure
to the programs with variability, we explained the basics
of variability (including features, compile-time conditional
statements, and congurations). In addition, we performed a
warm-up task with dierent degrees of variability with them
in order to demonstrate what they need to do and what
they need to answer (including the format of answers). In
summary, we essentially taught them how to accomplish the
tasks and ll in the forms.
Disregarding incorrect answers? When analyzing re-
sponse times, we decided not to exclude wrong answers
because we wanted to measure the time it takes to debug
a program, whether correct or not. Note, however, that we
do check that our observations still hold (are stable) when
disregarding incorrect answers.6Recall that the number of
incorrect answers were consistently low, regardless of the
degree of variability (cf. Observation 4, Figure 8and10a).
5.4 External validity
Beyond C IDEand preprocessors? Our experiment and
entire study is dedicated and tailored to a particular tech-
nique for dealing with variability: preprocessor. Our obser-
vations generalize to #ifdef s instead of background colors
because of their close relationship (after all, Cide is based on
#ifdef s) [16] and known results shows that a judicious use
of colours instead of #ifdefs can only simplify the task [ 11].
Generalization to other variability techniques is not intended.
Beyond university students ? The main question is
whether our study is also relevant to the industry? Our
N=69 participants were predominantly M.Sc. and Ph.D. stu-
dents from three dierent Danish universities. All had Java
6See the accompanying website for more information.programming experience and around half of them had indus-
trial experience (few months to several years). Note that we
had participants from Africa, Asia (incl., The Middle East),
Europe, North, and South America. In addition, previous
research has established that graduate students make good
proxies for industry developers [ 7]. All of this contributes to
representativity and generalization to \real-world" industrial
developers.
Beyond our buggy programs? We based our programs
(P1,P1, and P3) on real variability bugs from real highly-
congurable systems ( Linux [1],BusyBox ,7andBest-
Lap [28], respectively) precisely to minimize the risks of
introducing and studying articial problems. Also, the pro-
grams were qualitatively dierent (cf. Figure 3). Further, our
bug-nding tasks present three qualitatively dierent types of
bugs: uninitialized variable ,null-pointer dereference , and as-
sertion violation . In fact, these are common variability bugs
in bug reports [ 1]. For these reasons, we expect the results
should transfer to other (smaller) programs. Of course, there
may be additional eects, unaccounted for, when debugging
programs beyond 35 lines.
Beyond lab settings? More programs, larger programs,
higher degrees, realistic programs and tools, all extend the
task duration beyond one hour and make it signicantly
harder to attract anywhere near 69 participants. For these
reasons, we optimized for internal validity and quantitative
observations in lab conditions, recognizing that there is an
inherent tradeo between internal and external validity in
experiment design [32 ]. We ensured that each program ts
in a single screen to avoid participants scrolling up and down
to see the entire code. Additionally, we prepared all tasks
using Google forms to avoid heterogeneous environments
and installing software on dierent machines. That is, we
provided to the participants only a static window, i.e., no
IDEs, no tools, no navigation.
Beyond three features? It is entirely likely that the
linear relationship we observed (in Observation 1) breaks
down, at some point, for some higher degrees of variability.
Presumably at some point, developers will be unable to
simply cope with the exponentially many combinations of
features. However, this is beyond the scope of our study.
6. RELATED WORK
Variability Bugs. Previous studies have shown negative
aspects of preprocessor usage such as code pollution, no
separation of concerns, and error-proneness [33 ,18,10,9,17,
7http://git.busybox.net/busybox/commit/?id=
5cd6461b6fb51e8cf297a49074fce825e1960774
68719]. These studies are predominantly artifact-based (so based
on studying programs), not investigating human abilities to
work with the code.
Recently, Medeiros and co-authors interviewed 40 develop-
ers to study their perceptions of the C preprocessor [22 ]. The
developers assess that preprocessor-related bugs are easier to
introduce, harder to x, and more critical than other bugs.
Many admit that they check only a few congurations of
the source code in practice when testing their implementa-
tions. Our experiment conrms these qualitative insights
and complements them with quantitative data.
Medeiros et al. [23 ] investigated syntactic errors in pre-
processor-based systems. They noticed that developers intro-
duce syntax errors when changing existing code and adding
preprocessor directives, and that some of the relevant errors
survive in the life cycle all the way to the release stage. In
this paper, we work with human developers (not artifacts),
which allows us to quantify the eort of bug nding. Also, we
are concerned not with syntactic but with semantic errors.
Ribeiro et al. [ 27] conducted a controlled experiment to
evaluate whether emergent interfaces reduce eort and num-
ber of errors during code-change tasks involving feature code
dependencies. In general, they found a decrease in code-
change eort and number of errors when using their tool
support. Emergent interfaces are an example of tooling that
attempts to simplify reasoning about variability. Our experi-
ment conrms the need for more research on such tools.
Bug Finding. Oman et al. [ 25] compared debugging abili-
ties of novice, intermediate, and skilled student programmers
using two Pascal programs. Among other things, they found
that programmers' ability to nd errors increases with gen-
eral programming experience; they become faster and make
fewer mistakes. Comparing to our study, we did not design
the experiment to directly compare novices versus experts
even though we discussed some indications. Our observations
suggest that Ph.D. students are not faster at nding variabil-
ity bugs than M.Sc. students. But, the former appear to be
more careful and meticulous when debugging than the latter.
Furthermore, we studied a dierent phenomenon, which is
variability, trying to measure the impact of the degree of
variability on debugging.
Program comprehension. Feigenspan et al. [ 11] in a se-
ries of controlled experiments show that use of distinct back-
ground colors improves comprehension of #ifdefs . This is
one important reason why are we using colours in the experi-
ment, instead of preprocessor directives. In accordance with
their work, the bug nding with actual #ifdef directives
should likely be slower than with colours.
Another controlled experiment applied functional mag-
netic resonance imaging (fMRI) to measure program com-
prehension [ 31]. They found that ve dierent brain regions
associated with working memory, attention, and language
processing become activated for comprehending source code.
However, variability was not in their focus. We, in turn,
focused on quantifying the eect of the degree of variability
on debugging.
Schulze et al. [ 30] studied the inuence of the discipline of
preprocessor annotations on program comprehension. They
found that the discipline of annotations has no inuence
at all. Our observations agree in that nding bugs with
variability is time-consuming and dicult. We are however
able to quantify the increase of diculties when the degree
of variability grows.7. CONCLUSION
We have presented a controlled experiment quantifying the
impact of variability on the time andaccuracy of bug nding
in highly-congurable systems. We observe that bug-nding
time appears to increase linearly with the degree of vari-
ability. This conclusion is both positive and negative. An
increase in variability complicates bug nding (negative), but
not dramatically so (positive)|if developers reasoned about
each of the variants separately we would have observed an
exponential, not linear, growth. The practical implication
is that it is benecial to introduce variation points into pro-
grams from the debugging perspective: It is benecial to
pay a linear price for bug nding, if the alternative is to
maintain a super-linear set of variants (at least up to three
variations in a le). However, there might be benets in
selecting designs (architectures and algorithms) that require
less variability, if possible.
Somewhat expectedly, the variance in bug-nding time is
amplied by variability. In other words, dierences in bug-
nding competences of developers appear to be amplied
when working on software projects with variability. Getting
talented developers for such projects might be important.
We also nd that most participants correctly identify bugs
in programs with accuracy, that is independent of the de-
gree of variability. However, developers often fail to exactly
identify the set of erroneous congurations, and this happens
already for a rather low number of features, and gets worse
with the degree of variability increasing. Clearly, reasoning
about multiple congurations is a challenge. This is consis-
tent with earlier qualitative indications that variability bugs
appear, when developers unintentionally ignore an execution
that is enabled by an unexpected (for them) conguration of
features.
In fact, our study suggests that, for higher degrees of
variability, it is more dicult to correctly identify the set of
erroneous congurations than to nd the bug in the rst place.
This is rather unexpected, given that to understand the bug
one needs to reason about control ow, a temporal non-local
phenomenon that is not obviously simpler than combinatorics.
This means that it is benecial to work on support tools that
help developers to navigate the conguration space (on top
of ow-oriented bug nders).
The future follow up on this work, is expected to design
tools exploiting the results of the study, in particular indicat-
ing the sets of congurations impacted by a program change,
in order to simplify reasoning about all ows that a change
participates in (cf. Observations 5 and 8). Additionally,
further research like replicating this experiment |with more
programs, larger programs, more subjects, higher degrees,
realistic programs and tools| is required to confront our ob-
servations and to draw new ones. It would be also interesting
to replicate our study using fMRI or eye-tracking to better
explain the impact of variability on debugging. With this, it
would be possible to actually see how developers approach
programs with dierent degrees of variability.
Acknowledgements. We thank all the students for devoting
their time. Fritz Henglein, Christian Probst, Sren Debois
helped to attract the participants. We thank participants of
the FOSD'15 Meeting for valuable feedback. Melo is funded
by Brazilian Science without Borders Programme, CNPq
grant no. 249020/2013-0. W,asowski is funded by The Danish
Council for Independent Research, grant no. 0602-02327B.
6888. REFERENCES
[1] I. Abal, C. Brabrand, and A. Wasowski. 42 Variability
Bugs in the Linux Kernel: A Qualitative Analysis. In
Proceedings of the 29th ACM/IEEE International
Conference on Automated Software Engineering, ASE
'14, pages 421{432, New York, NY, USA, 2014. ACM.
[2] R. A. Bailey. Design of comparative experiment .
Cambridge University Press, 2008.
[3] T. Berger, D. Nair, R. Rublack, J. M. Atlee,
K. Czarnecki, and A. Wasowski. Three cases of
feature-based variability modeling in industry. In
ACM/IEEE 17th International Conference on Model
Driven Engineering Languages and Systems
(MODELS) , 2014.
[4]T. Berger, R. Rublack, D. Nair, J. M. Atlee, M. Becker,
K. Czarnecki, and A. W,asowski. A survey of variability
modeling in industrial practice. In Proceedings of the
Seventh International Workshop on Variability
Modelling of Software-intensive Systems , VaMoS '13,
pages 7:1{7:8, New York, NY, USA, 2013. ACM.
[5] T. Berger, S. She, R. Lotufo, A. Wasowski, and
K. Czarnecki. A study of variability models and
languages in the systems software domain. Software
Engineering, IEEE Transactions on , 39(12):1611{1640,
Dec 2013.
[6] G. E. P. Box, J. S. Hunter, and W. G. Hunter.
Statistics for Experimenters: design, innovation, and
discovery . Wiley-Interscience, 2005.
[7]R. P. Buse, C. Sadowski, and W. Weimer. Benets and
barriers of user evaluation in software engineering
research. ACM SIGPLAN Notices , 46(10):643{656,
October 2011.
[8] P. Clements and L. Northrop. Software Product Lines:
Practices and Patterns . Addison-Wesley, 2002.
[9]M. D. Ernst, G. J. Badros, and D. Notkin. An empirical
analysis of C preprocessor use. IEEE Transactions on
Software Engineering, 28:1146{1170, 2002.
[10]J. M. Favre. Understanding-in-the-large. In Proceedings
of the 5th International Workshop on Program
Comprehension (WPC) , pages 29{38. IEEE Computer
Society, 1997.
[11] J. Feigenspan, C. K astner, S. Apel, J. Liebig,
M. Schulze, R. Dachselt, M. Papendieck, T. Leich, and
G. Saake. Do background colors improve program
comprehension in the #ifdef hell? Empirical Softw.
Engg. , 18(4):699{745, Aug. 2013.
[12] M. Goedicke, K. Pohl, and U. Zdun. Domain-specic
runtime variability in product line architecture.
Proceedings of Object-Oriented Information Services
(OOIS'02).: Lecture Notes in Computer Science, 2002.
[13] E. Goldstein. Sensation and Perception . Cengage
Learning Services, 2002.
[14] P. Goodlie. Becoming a Better Programmer . O'Reilly
Media, Inc., 2014.
[15] W. E. Hick. On the rate of gain of information.
Quarterly Journal of Experimental Psychology ,
4(1):11{26, 1952.
[16] C. K astner. Virtual separation of concerns: Toward
preprocessors 2.0, 5 2010. Logos Verlag Berlin, isbn
978-3-8325-2527-9.
[17] C. K astner, S. Apel, and M. Kuhlemann. Granularity
in Software Product Lines. In Proceedings of the 30thInternational Conference on Software Engineering
(ICSE) , pages 311{320. ACM, 2008.
[18] M. Krone and G. Snelting. On the inference of
conguration structures from source code. In
Proceedings of the 16th International Conference on
Software Engineering (ICSE) , pages 49{57. IEEE
Computer Society Press, 1994.
[19] J. Liebig, S. Apel, C. Lengauer, C. K astner, and
M. Schulze. An analysis of the variability in forty
preprocessor-based software product lines. In
Proceedings of the 32nd ACM/IEEE International
Conference on Software Engineering (ICSE) , pages
105{114. ACM, 2010.
[20] J. Liebig, A. von Rhein, C. K astner, S. Apel, J. D orre,
and C. Lengauer. Scalable analysis of variable software.
InProceedings of the 2013 9th Joint Meeting on
Foundations of Software Engineering , ESEC/FSE 2013,
pages 81{91, New York, NY, USA, 2013. ACM.
[21] T. J. McCabe. A complexity measure. IEEE Trans.
Softw. Eng. , 2(4):308{320, July 1976.
[22] F. Medeiros, C. K astner, M. Ribeiro, S. Nadi, and
R. Gheyi. The love/hate relationship with the C
preprocessor: An interview study. In Proceedings of the
29th European Conference on Object-Oriented
Programming (ECOOP) , Lecture Notes in Computer
Science, Berlin/Heidelberg, 2015. Springer-Verlag.
[23] F. Medeiros, M. Ribeiro, and R. Gheyi. Investigating
preprocessor-based syntax errors. In Proceedings of the
12th International Conference on Generative
Programming: Concepts & Experiences , GPCE, pages
75{84. ACM, 2013.
[24] D. C. Montgomery. Design and Analysis of
Experiments . John Wiley & Sons, 2006.
[25] P. W. Oman, C. R. Cook, and M. Nanja. Eects of
programming experience in debugging semantic errors.
J. Syst. Softw. , 9(3):197{207, Mar. 1989.
[26]K. Pohl, G. Bockle, and F. J. van der Linden. Software
Product Line Engineering. Springer, 2005.
[27] M. Ribeiro, P. Borba, and C. K astner. Feature
maintenance with emergent interfaces. In Proceedings
of the 36th International Conference on Software
Engineering, ICSE 2014, pages 989{1000, New York,
NY, USA, 2014. ACM.
[28] M. Ribeiro, F. Queiroz, P. Borba, T. Tol^ edo,
C. Brabrand, and S. Soares. On the impact of feature
dependencies when maintaining preprocessor-based
software product lines. In Proceedings of the 10th ACM
International Conference on Generative Programming
and Component Engineering (GPCE) , pages 23{32.
ACM, 2011.
[29] C. Sant'anna, A. Garcia, C. Chavez, C. Lucena, and
A. v. von Staa. On the reuse and maintenance of
aspect-oriented software: An assessment framework. In
Proceedings XVII Brazilian Symposium on Software
Engineering, 2003.
[30] S. Schulze, J. Liebig, J. Siegmund, and S. Apel. Does
the discipline of preprocessor annotations matter?: A
controlled experiment. In Proceedings of the 12th
International Conference on Generative Programming:
Concepts & Experiences , GPCE '13, pages 65{74, New
York, NY, USA, 2013. ACM.
[31] J. Siegmund, C. K astner, S. Apel, C. Parnin,
689A. Bethmann, T. Leich, G. Saake, and A. Brechmann.
Understanding understanding source code with
functional magnetic resonance imaging. In Proceedings
of the 36th International Conference on Software
Engineering, ICSE 2014, pages 378{389, New York, NY,
USA, 2014. ACM.
[32] J. Siegmund, N. Siegmund, and S. Apel. Views on
internal and external validity in empirical software
engineering. In Software Engineering (ICSE), 2015
IEEE/ACM 37th IEEE International Conference on ,
volume 1, pages 9{19, May 2015.
[33] H. Spencer and G. Collyer. #ifdef considered harmful,
or portability experience with C news. In Proceedings
of the Usenix Summer Technical Conference, pages
185{198. Usenix Association, 1992.
[34] M. V olter. Handling variability. In Proceedings of the
14th annual European Conference on Pattern Languages
of Programming, EuroPLoP'09. Kelly & Weiss, 2009.
[35] Z. Yin, D. Yuan, Y. Zhou, S. Pasupathy, and
L. Bairavasundaram. How do xes become bugs? In
Proceedings of the 19th ACM SIGSOFT symposium
and the 13th European conference on Foundations of
software engineering (ESEC/FSE), pages 26{36. ACM,
2011.
[36] B. Zhang and M. Becker. Recovar: A solution
framework towards reverse engineering variability. In
Product Line Approaches in Software Engineering
(PLEASE), 2013 4th International Workshop on , pages
45{48, May 2013.
690