ACTGAN: Automatic Conﬁguration Tuning for
Software Systems with Generative Adversarial
Networks
Liang Bao∗1, Xin Liu†2, Fangzheng Wang∗and Baoyin Fang∗
∗School of Computer Science and Technology, XiDian University, Xi’an, ShaanXi, China
†Department of Computer Science, University of California, Davis, Davis, California, USA
1baoliang@mail.xidian.edu.cn,2xinliu@ucdavis.edu
Abstract —Complex software systems often provide a large
number of parameters so that users can conﬁgure them for
their speciﬁc application scenarios. However, conﬁguration tuning
requires a deep understanding of the software system, far beyond
the abilities of typical system users. To address this issue,
many existing approaches focus on exploring and learning good
performance estimation models. The accuracy of such models
often suffers when the number of available samples is small,
a thorny challenge under a given tuning-time constraint. By
contrast, we hypothesize that good conﬁgurations often share
certain hidden structures. Therefore, instead of trying to improve
the performance estimation of a given conﬁguration, we focus
on capturing the hidden structures of good conﬁgurations and
utilizing such learned structure to generate potentially better
conﬁgurations. We propose ACTGAN to achieve this goal. We
have implemented and evaluated ACTGAN using 17 workloads
with eight different software systems. Experimental results show
that ACTGAN outperforms default conﬁgurations by 76.22% on
average, and six state-of-the-art conﬁguration tuning algorithms
by 6.58%-64.56%. Furthermore, the ACTGAN-generated conﬁg-
urations are often better than those used in training and show
certain features consisting with domain knowledge, both of which
supports our hypothesis.
Index T erms —software system, automatic conﬁguration tuning,
generative adversarial networks
I. I NTRODUCTION
Many software systems, such as middleware, databases, and
Web servers, provide a large number of parameters for users
to conﬁgure in order to achieve desirable functional behaviors
and non-functional properties (e.g., performance and cost). For
example, both Spark [1] and HBase [2] have 200+ parameters
that a user can conﬁgure, among which 10-15 are considered
critical [3], [4], [5].
Performance (e.g., execution time, throughput, requests per
second) is one of the most important non-functional prop-
erties [6]. Previous studies have shown that conﬁgurations
have a signiﬁcant impact on performance [3], [7], [4], [8],
[5], [6], because software systems are complex with many
conﬁgurable options that control nearly all aspects of their
runtime behaviors. With this complexity, tuning a large number
of conﬁguration parameters requires a deep understanding of
the target system and has far surpassed the abilities of typical
system users [9]. As a result, users often (have to) accept
the default settings. Alternatively, organizations may choose
to hire human experts to conﬁgure their software systems.Unfortunately, manual conﬁguration is labor-intensive, time-
consuming, and often suboptimal. Therefore, there is a dire
need for automatic conﬁguration tuning on software systems.
One intuitive approach of automatic conﬁguration tuning
is to measure the performance of all conﬁgurations of a
system to identify the conﬁguration with the best performance.
Unfortunately, this is usually infeasible because of the com-
binatorial nature of conﬁguration parameters and the high-
dimensional conﬁguration space. Speciﬁcally, there are three
main challenges in conﬁguration tuning of complex software
systems:
Heterogeneity . Software systems that require conﬁguration
tuning are highly heterogeneous, including message system
like Kafka [10], data analytic systems like Spark [1] and
Hive [11], database systems like Redis [12], MySQL [13],
Cassandra [14] and HBase [2], or Web servers like Tomcat
[15]. Performance metrics also differ, including throughput,
execution time, transactions per minute, etc. Furthermore,
application workload can vary signiﬁcantly. For example,
Figure 1 illustrates the heterogeneity of performance surface
under only two parameters. We can see that Redis and Hive
have bumpy performance surfaces, while Kafka has a relatively
smooth performance surface.
Complexity . Given different performance metrics and dif-
ferent workloads, a deployed software system can have highly
complex performance surfaces for a given set of conﬁguration
parameters, as shown in Figure 1. It often takes an experienced
system developer months of efforts to reason about the un-
derlying interactions between parameters [4], let alone novice
users.
Costing evaluation . It is often costly to evaluate the per-
formance of an individual conﬁguration in a large software
system. Because no performance simulator exists for general
systems, performance evaluation often requires real experi-
ments on production systems. Such an evaluation is not only
time-consuming (e.g., executing a complex benchmark takes
minutes or hours) but also expensive (e.g., running a real
system for one minute may cost hundreds of dollars). Hence,
obtaining performance samples, i.e. the performance results
with different conﬁgurations, is costly.
Existing automatic conﬁguration tuning methods include
model-based, simulation-based, search-based, and learning-
4652019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
978-1-7281-2508-4/19/$31.00 ©2019 IEEE
DOI 10.1109/ASE.2019.00051
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. 4.55
4515.5
19104Requests per second (#)
401 17 351Redis
6
15
hz301 13
threshold-lower251 116.5
201 9151 7101 53 511 10.9
2751
250105Execution time (ms)
225 71.1Hive
mapjoin.bucket.cache.size6.25 2005.5
join.cache.size104 1751.2
4.75150 43.25 1252.5 1000
5720
5043 19Throughput(MB/s)
1740
36Kafka
15
batch.size(MB)29 13
num.network.threads1160
22 915 75 8 31 1
Figure 1. Performance surfaces of three different software systems
based, as discussed in Section II. Among them, learning-based
tuning has received much recent attention. The key idea of
learning-based tuning is to construct a performance prediction
model using training samples of different conﬁgurations, and
then explores better conﬁgurations using some searching al-
gorithms. Although previous studies on learning-based tuning
show promising results, one critical drawback is that it re-
quires a lot of samples to train an accurate prediction model.
For example, previous study [16] has shown that a typical
learning-based approach needs about 2000 samples to obtain
a good prediction model with 10 conﬁguration parameters.
Apparently, the assumption of the abundance of samples used
in many learning-based methods is invalid considering the
constrained time for tuning. Given the limited tuning time in
practice, only a small number of samples can be obtained.
It typically results in a less accurate prediction model that
jeopardizes the exploration for better conﬁgurations.
To overcome these challenges, we propose ACTGAN (Au-
tomatic Conﬁguration Tuning using Generative Adversarial
Network) that aims to automatically generate good conﬁg-
urations for different software systems under a given time
constraint. We approach the conﬁguration tuning for software
system (CTSS) problem in an unconventional manner – our
hypothesis is that good conﬁgurations share certain hid-
den structures . Therefore, instead of trying to improve the
performance estimation of a given conﬁguration, we focus on
capturing the hidden structures of good conﬁgurations and
using such hidden structures to generate potentially better
conﬁgurations .
We choose GAN (Generative Adversarial Network) for this
purpose because: 1) GAN’s inherent game-like structure forces
the neural networks to seek important features hidden in good
conﬁguration samples; 2) it circumvents the need to obtain
a highly accurate prediction model that is typically required
a signiﬁcant number of samples, and thus is more practical
under the limited tuning time; and 3) GAN allows us to
generate (better) conﬁgurations leveraging the learned hidden
structures.
In summary, our work makes the following contributions:
•We introduce and formulate the CTSS problem, and
model it as a combinatorial optimization problem.
•We propose ACTGAN, a novel approach to address
CTSS, from an unconventional direction. ACTGAN can
recommend promising conﬁgurations by directly learningand utilizing the hidden structures of existing good con-
ﬁgurations, without the need of learning a performance
prediction model.
•We evaluate the performance of ACTGAN through exten-
sive experiments using 17 workloads under eight different
software systems. We show that ACTGAN outperforms
default conﬁgurations by 76.22% on average, and six
state-of-the-art tuning algorithms by 6.58%-64.56%.
II. R ELATED WORK
Automatic conﬁguration tuning for software systems has
received much attention from both industry and academia.
We classify the previous studies on this problem into four
categories: model-based, measurement-based, search-based,
and learning-based conﬁguration tuning approaches, where the
latter two are most recent and relevant to our work.
Model-based conﬁguration tuning . The key to model-
based conﬁguration tuning is to construct an analytical per-
formance model for a software system on the early stage
of system development [17], see [18] and [19] for reviews.
Model-based approaches rely on domain speciﬁc knowledge,
mathematical theories or software architecture abstraction.
They can be labor-intensive, imprecise, and difﬁcult to evolve.
Measurement-based conﬁguration tuning . Measurement-
based conﬁguration tuning aims to derive performance mod-
els for software systems by statistical inferencing based on
benchmarked measurement data [20]. They collect program
proﬁles to identify performance bottlenecks, which often fail
to capture the overall program performance [21]. Also, most
of these approaches lack generality [22].
Search-based conﬁguration tuning . Search-based ap-
proaches regard conﬁguration problem as a black-box opti-
mization problem and use search algorithms to solve it, such
as in [23], [24], [25]. The search strategies include recursive
random search (RRS) heuristics [26], [16], evolutionary algo-
rithms [27], [28], hill-climbing algorithms [29], and recursive
bound & search [4].
Search-based conﬁguration is simpler and more general
compared to other approaches, because it takes the target
software system as a black-box function and does not need
detailed information about the internals. However, they fail to
exploit the knowledge of already-known good conﬁgurations
and need to search for the parameter space more times to
obtain good conﬁgurations [30].
466
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. System Under Tune
(SUT)Workload
(W)
Time Constraint
(TC)Performance
(P)Configuration
(C)Given
Figure 2. Overview of CTSS problem
Learning-based conﬁguration tuning . The most relevant
to our work is learning-based conﬁguration tuning. These ap-
proaches try to construct performance prediction models ﬁrst
by observing a collection of running results under different
conﬁgurations, and then apply some search algorithms to ﬁnd
the optimal conﬁguration based on these models. The methods
and models often used include regression [31], [32], [33], [34],
SVM/SVR [35], [36], CART [6], Fourier learning (FL) [37],
random-forest [38], [5], artiﬁcial intelligence (AI) planning
[39], spectral learning [40], Classiﬁcation and Regression
Training (Caret) [41], Bayesian optimization [42], [43], [44],
[30], reinforcement learning [45], comparison-based model
[46], and transfer learning [47], [48].
Learning-based approaches require considerable numbers of
samples to construct a good performance prediction model
for a software system [25]. The high demand of samples is
challenging here because only a limited set of samples can be
acquired under a given time constraint.
III. P ROBLEM STATEMENT
In this paper, we study Conﬁguration Tuning for Software
Systems ( CTSS ). As illustrated in Figure 2, for a given system
and a speciﬁc workload, the goal of CTSS is to search for an
optimal conﬁguration to achieve the best performance by ex-
ploring the high-dimensional parameter space within a limited
time period. We call this process of selecting conﬁguration
settings as conﬁguration tuning (or just tuning ).
More speciﬁcally, a CTSS problem can be deﬁned with the
following components.
System Under Tune ( S). Many software systems, such as
data analytic framework, databases, and Web servers, provide a
large number of parameters for users to conﬁgure. The subject
system that we want to tune is called the system under tune
(SUT) [4], and is represented as S.
Conﬁguration ( C). We represent a conﬁguration of an SUT
as a setC=(c1,c2,···,cn)ofnparameters, and the value
of each parameter cimust be within CB, the conﬁguration
bound predeﬁned by the SUT. Each conﬁguration Ciof an
SUT is represented as an n-tuple, assigning a valid value to
each parameter in C.
Workload ( W). A workload Wrepresents a speciﬁc task
running on an SUT. Example workloads include an application
on Spark, a set of transactions on MySQL, or a group of
requests on Tomcat. In the CTSS problem, we assume the
workload Wis given. Furthermore, we can measure theperformance of the SUT under a given conﬁguration Cby
simply executing the Won SUT under C, and record its
performance.
Performance ( P). Performance is an essential non-
functional property of an SUT that can directly affect user
perception and running cost. It is the optimization goal of
our CTSS problem. We treat the performance P(·)as a
blackbox function, and use the P(S,W,C )to denote the
performance value, given an SUT S, a workload W, and a
conﬁguration C. Different SUTs have different performance
goals for conﬁguration tuning: for a data-access workload on
MySQL, the performance goal is to increase the throughput ,
while for a data analytical job on Spark, it would be to reduce
the execution time . Without loss of generality, we assume
that we want to maximize the performance value over all
conﬁgurations.
Time Constraint ( TC ). In a practice, the time for con-
ﬁguration tuning is often restricted. We deﬁne this restricted
tuning time as time constraint , denoted as TC. Any solution
to the CTSS problem must complete within TC.
In summary, the CTSS problem can be deﬁned as follows:
max
C∈CBP(S,W,C ) (1)
s.t.tuning time ≤TC (2)
where (1) states that given an SUT Sand a workload W
onS, the goal of CTSS is to ﬁnd a conﬁguration Camong
all valid conﬁgurations of Sthat maximize the performance.
The constraint (2) is that any solution to the problem must
terminate after a TCamount of tuning time.
This deﬁnition shows that the goal of CTSS is to search for
an optimal conﬁguration of a set of parameters to maximize
the performance. According to a previous study [49], CTSS is
essentially an instance of classic combinatorial optimization
(CO) problems [50], which is known to be NP-complete. The
NP-completeness proof by restriction is established in [51].
Therefore, a naive exhaustive search solutions would not be
practical due to the high dimensionality of parameter space
and the combinatorial nature of brutal force search.
IV . ACTGAN FOR CTSS P ROBLEM
In this section, we introduce ACTGAN – a generative
adversarial network (GAN) model to solve the CTSS problem.
Its key idea is to capture the hidden structures of good conﬁg-
urations by learning how already-known good conﬁgurations
are distributed in a high-dimensional conﬁguration space. In
the following, we ﬁrst analyze existing tuning approaches and
their limitations. We then present ACTGAN, a novel approach,
which solves the CTSS problem in an unconventional manner.
Finally, we discuss the details of the ACTGAN algorithm.
A. Motivation
For a given SUT, ﬁnding a good conﬁguration is challenging
because of the high-dimensionality of the conﬁguration space,
the limited amount of tuning time, and the lack of a priori
467
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. knowledge about its performance surface. To address these
challenges, the following approaches have been considered.
Recently, learning-based approaches are the most popular
ones for addressing the CTSS problem. The key step is to
construct a performance prediction model for an SUT using
collected performance samples under different conﬁgurations.
However, given the time constraint for tuning, one can obtain
only a limited number of samples because it often takes
minutes or hours to evaluate the performance of an individual
conﬁguration on an SUT. Having insufﬁcient samples leads to
an inaccurate prediction model [25] and directly affects the
tuning performance [4].
Bayesian optimization (BO) based approaches are also
popular ways to optimize objective functions that are costly to
evaluate [30]. They build a surrogate for the objective, quan-
tify the uncertainty in that surrogate using Gaussian process
regression, and then use an acquisition function deﬁned from
this surrogate to decide where to sample [52]. Essentially, BO
uses all the information available from previous evaluations of
the objectives instead of relying simply on local gradient and
Hessian approximations. While BO methods often have good
convergence guarantees, their short-term performance under a
limited number of samples is not always desirable in practice
because they may spend too much time exploring uncertain
spaces.
Search-based approaches, such as random search, on the
other hand, take the SUT as a black-box function and do
not need the detailed information about its internals. How-
ever, because they fail to exploit the knowledge of already-
known good conﬁgurations, they have to explore the high-
dimensional conﬁguration space more times in order to obtain
the optimized conﬁguration [30]. Other heuristic algorithms,
such as genetic algorithm (GA), particle swarm optimization
(PSO), etc., are able to handle high-dimensional combinatorial
optimization problems [53]. Unfortunately, they often need
at least thousands of evaluations to ﬁnd good conﬁgurations
with 10–20 dimensions according to our experiments, which
is infeasible in our time-constrained CTSS scenario.
Most of the above approaches focus on improving the
performance estimation/modeling of a conﬁguration under a
limited number of samples. By contrast, our hypothesis is that
good conﬁgurations often share certain hidden structures .
Therefore, instead of trying to improve the performance esti-
mation of a given conﬁguration, we focus on capturing the
hidden structures of good conﬁgurations . In the literature,
GANs have been shown to be a powerful tool for ﬁnding
hidden structures, which is why we adopt them here to address
the CTSS problem.
Figure 3 illustrates the automatic conﬁguration tuning pro-
cess of ACTGAN. ACTGAN simultaneously trains two mod-
els: a generative model Gthat captures the hidden structures
of the already-known good conﬁgurations from the training
examples, and a discriminative model Dthat estimates the
probability that a sample comes from the training data rather
than from G. More speciﬁcally, we use the random sampling
method to generate a set of conﬁgurations, test them on
SUTConfiguration 
Bound (CB)
Random 
Samplingb Training 
ExamplesExecution
Generated 
SamplesGeneratorLatent 
Random 
VariablesDiscriminatorGood
BadLossTraining Phase
Recommendation Phase
GeneratorLatent 
Random 
VariablesNr Generated 
Configurations
SUTThe Best 
ConfigurationConfiguration 
Bound (CB)
Executionb Training 
ExamplesNs Samples
Configuration 
Selection
Figure 3. Overview of ACTGAN Approach
the SUT, and select a subset of conﬁgurations, i.e. training
examples , with the best performance. Using this training
set, we train a GAN with a discriminator and a generator
network. The GAN structure forces the networks to learn
the hidden structures of good conﬁgurations. To utilize the
hidden structures, we use the trained generator to generate a
set of potentially good conﬁgurations. Finally, we test these
conﬁgurations on the SUT and select the best one.
In summary, we propose an innovative way of utilizing
the GAN structure for CTSS that uncovers and utilizes the
hidden structures of good conﬁgurations .
B. ACTGAN Algorithm
Using the tuning process shown in Figure 3, we now discuss
the ACTGAN algorithm in Algorithm 1.
Estimating the sample size . Given a tuning time constraint
TC, an SUT Sconsisting of Cparameters, and a speciﬁc
workload Wrunning on S, lett(C)be the execution time
under the conﬁguration C. The execution time of two dif-
ferent conﬁgurations may differ signiﬁcantly. For example,
the running time of a pagerank workload on Spark using the
default conﬁguration is 440s, this time falls to 242s under an
optimized conﬁguration. Suppose the tuning time constraint is
TC and the number of recommended conﬁgurations from G
isNr(we need to run Swith these Nrconﬁgurations to ﬁnd
the best one, see line 18); then we can use the running time
under default conﬁguration to make a conservative estimate of
the sample size Ns(line 1–2).
Random sampling .G i v e nTC,S,C,W, and a set BC
representing the conﬁguration bound for every parameter,
ACTGAN ﬁrst generates a set of conﬁgurations Tfrom
Cwithin BC. This collection of conﬁgurations serves as
a training set for our GAN model. We use the random
sampling (RS) strategy, as it can effectively search for a larger
conﬁguration space, especially when conﬁguration parameters
are not equally important [23] (line 3).
468
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Algorithm 1 ACTGAN(S, W, C, TC, CB)
Input: S: the target SUT; W: workload; C: conﬁguration; TC: time
constraint; CB: conﬁguration bounds.
Variables: T: the set of initial samples, |T|=Ns;R: the set of
potentially good conﬁgurations recommended by G,|R|=Nr.
1:t(C0)←the time of running Susing the default conﬁguration
C0;
2:Ns=⌊TC
t(C0)⌋−Nr;
3:T←RS(Ns,S,W,C,CB );
4:B←a set ofbconﬁgurations with the best performance from
T;
5:fornumber of training iterations do
6: foranyksteps do
7: Sample minibatch of mnoise samples {z(1),z(2),···,
8:z(m)}from noise prior pg(z);
9: Sample minibatch of mconﬁgurations {x(1),x(2),···,
10:x(m)}fromB;
11: Update the discriminator by ascending its stochastic gra-
dient:
∇θd1
mm/summationdisplay
i=1[logD(x(i))+l o g ( 1 −D(G(z(i))))]
12: end for
13: Sample minibatch of mnoise samples {z(1),z(2),···,
14:z(m)}from noise prior pg(z);
15: Update the generator by descending its stochastic gradient:
∇θg1
mm/summationdisplay
i=1[log(1−D(G(z(i))))]
16: end for
17:R←Nrconﬁgurations recommended by G;
18:Run workload WonSwith each conﬁguration in R;
19:Select the conﬁguration C∗with the best performance in R/uniontextB;
20: returnC∗;
GAN architecture design . After obtaining T, a subset Bof
Tconsisting of the best bconﬁgurations are selected to serve
as the training set for the discriminator Dof the ACTGAN
(line 4). Speciﬁcally, we model both the generator Gand the
discriminator Dusing three-layer feedforward neural networks
(NNs) with one hidden layer. Simple neural networks work
here because the performance surfaces of SUTS are often
continuous (although not necessarily smooth) [4], so NNs can
capture the hidden structures of the performance surfaces. We
note that the original convolution neural network (CNN) struc-
tured GAN works well for image processing, because such
convolution operators generally capture intensity, gradient, and
edge characteristics of images. However, in our case, CNNs
fail to capture the hidden structures among the parameters for a
given SUT. The detailed architecture of ACTGAN is discussed
in the Section V-B.
Model training . At any step of the training process,
the generator Gtakes as input a Gaussian noisy source
z∼pg(z)and produces a set of synthetic conﬁgurations
{z(1),z(2),···,z(m)}that aims to follow a unknown target
data distribution for good conﬁgurations (line 7–8, 13–14).
Meanwhile, the discriminator Dtakes a subset of already-
known good conﬁgurations {x(1),x(2),···,x(m)}as input
and learns to maximize the probabilities that zis fake andxis real.
The min-max adversarial loss for training the generator
networkGand discriminator network Dis formulated as:
min
Gmax
DEx∼p(x)[logD(x(i))]+Ez∼pg(z)[log(1−D(G(z(i))))]
where we train Dto maximize the probability of identify-
ing the good conﬁgurations to both training examples and
samples from G, and simultaneously train Gto minimize
log(1−D(G(z))). The goal of this process is to learn the
hidden distribution of already-known good conﬁgurations p(x)
viaG, and generate realistic fake good conﬁgurations (line 11).
When we train GgivenD, we minimize the following loss
ofG(line 15):
Ez∼pg(z)min
G[log(1−D(G(z(i))))]
In our ACTGAN algorithm, we adopt ADAM [54], an opti-
mized mini-batch stochastic gradient descent (SGD) method,
to update the parameters in the networks in an efﬁcient and
stable way.
Conﬁguration recommendation . After ﬁnishing the train-
ing, we use the generator Gto construct a set of Nrrecom-
mended conﬁgurations Rand run them on the SUT. Finally,
we select the best conﬁguration in R/uniontextB, whereBis the best
conﬁgurations generated using random sampling method. (line
17–20).
V . ACTGAN I MPLEMENTATION
A. Data preparation
Categorical data processing . Categorical variables are
parameters that contain label values rather than numeric
values, and the number of possible values is often limited
to a ﬁxed set. Categorical variables are common in software
conﬁgurations. For example, there are two categorical
variables, i.e. spark.io.compression.codec=
{snappy, lz4, lzf }and spark.serializer= {JavaSerializer ,
KryoSerializer }, of Spark framework in our experiments.
Because our neural networks cannot operate on categorical
data directly, they require all input variables and output
variables to be numeric.
There are two encoding methods, i.e integer encoding and
one hot encoding, that can convert categorical data to numer-
ical data. In CTSS problem, categorical variables often have
no ordinal relationship, using integer encoding and allowing
the model to assume a natural ordering between categories
may result in poor performance or unexpected results. In this
case, we use one hot encoding to add binary variable for each
unique categorical value. In the “ spark.io.compression.codec ”
variable example, there are three categories and therefore three
binary variables are needed. A “1” value is placed in the binary
variable for the used compression options and “0” values for
the other options, as shown in Table I.
Data normalization . Software conﬁguration parameters are
often in highly different ranges. Take the Web server Tomcat
for example, the # of minimal processors parameter ranges
from 1–100 in our experiments, while the connection timeout
469
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Table I
ONE HOT ENCODING FOR spark.io.compression.codec
snappy lz4 lzf
1 0 0
0 1 0
0 0 1

ĂĂĂĂ
Input layer
nodesHidden layer
nodes
SigmoidOutput layer
nodes




Input data ( Ig)
m* random matrixOutput data ( Og)
m* matrixGenerator
ĂĂm
OgOTkgki
kgkikgki kgkh
kgkoIgIT
kgkom
kgko
Figure 4. The generator architecture of ACTGAN
ranges from 0–30,000 and higher. Further analysis shows that
the parameter connection timeout intrinsically inﬂuences the
result more due to its larger value. But this doesn’t necessarily
mean it is more important as a predictor.
To address problem, we use standard score to normalize
the dataset. More speciﬁcally, let xbe the original value of a
parameter, zbe the normalized value, we have:
z=x−μ
σ
whereμis the mean value of the parameter, and σis the
standard deviation of the parameter values.
B. GAN Architecture
Like any typical GAN architecture, ACTGAN consists of
two main components – a generator Gand a discriminator
D. The goal of the generator is to generate synthetic conﬁg-
urations that are plausible in the input (good) conﬁgurations.
At the same time, the discriminator learns to distinguish the
suboptimal conﬁgurations from the good ones that come from
the training set. Both GandDare trained end-to-end using
backpropagation .
At each step of the training process, ACTGAN uses Gto
generate a set of synthetic conﬁgurations, takes one conﬁg-
uration at a time to feed as D’s input. Doutputs a single
score that represents the probability of the conﬁguration being
real. An overview of ACTGAN’s detailed architecture of the
generator and the discriminator can be seen in Figure 4 and
5, respectively.
Generator . The generator Gdeﬁnes an implicit prob-
abilistic model for generating good conﬁgurations: C=
{c1,c2,···,cn}∼G. We model Gas a three-layer feed-
forward neural network. More speciﬁcally, Gconsists of anĂĂ
Input layer
   nodesHidden layer
  nodes
ReluOutput layer
   node
Sigmoid




Input data ( Id)
  m configurations
 ( m*    matrix)Output data ( Od)
m*1 vectorDiscriminator
ĂĂ
Od IdITm
kd ki kd kh kd ko m kg ko 
kg ko 
Figure 5. The discriminator architecture of ACTGAN
input layer of non-computational units (one for each input),
one hidden layer of computational units, and an output layer
of computational units. The key procedure of Gis to pass the
input signal forward and the error signal backward through
the network.
In the forward pass, a latent m×ki
gmatrixIg(represents m
noise samples) drawn from a multivariate Gaussian distribution
is presented to the input layer, and the input signal is ﬁltered
forward through the subsequent hidden layer (with kh
gnodes)
of the network to generate a set of outputs. The network
outputs, i.e. a set of msynthetic conﬁgurations (a m×ko
g
matrixOg), are fed into the discriminator Dand the possibility
value for each conﬁguration can lead to good performance is
generated an error signal.
In the backward pass, the synaptic weights are updated
using the learning constant according to the effect they have
in generating the incorrect outputs. Figure 4 demonstrates the
passing of noise prior Igthrough a neural network and the
structure of our three-layer network. The presentation of noise
samples to the network is repeated in each minibatch until the
network is stabilized and no more adjustments are required for
the synaptic weights, or the maximum number of epochs has
been reached.
As described in Table III, the number of input neurons for
Gis set to 5 ( ki
g=5); the number of hidden neurons for Gis
set to 128 ( kh
g=128); and the number of output neurons for G
is set ton(ko
g=n), i.e. the number of parameters for a given
SUT.
Discriminator . The discriminator Dis also a three-layer
feedforward neural network with a hidden layer consisting of
kh
dnodes. At every time step, a m×ko
gmatrixId, denoting
mconﬁgurations generated by G(i.e.ki
d=ko
g=n), is fed
as input. After processing these conﬁgurations, the discrimi-
nator outputs a vector of mscalar values that represents the
probability of each synthetic conﬁguration being good.
As shown in Table III, the number of initial training samples
fed intoDis set to 32 ( b=32); the number of input neurons
470
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. forDis set ton(ki
d=n), i.e. the number of parameters for
a given SUT; the number of hidden neurons for Dis set to
128 (kh
d=128); and the number of output neurons for Dis set
t o1(ko
d=1).
C. Model Training
To stabilize our model training procedure, we modify the
negative log likelihood objective by clipping the values be-
tween 0 and 1. This loss performs more stably during training
and generates higher quality results. We use the ADAM
optimizer [54] with a momentum value of 0.5. All networks
were trained from scratch with a learning rate of 0.0001 and
the total number of epochs are set to 150,000 for all 17
workloads. Finally, we apply instance normalization with batch
size of 16 ( m=16). We tried the conventional value 64 of batch
size at the ﬁrst place but observed the model oscillation of G
andDduring the training process, and ﬁnally found out that
the 16 is an appropriate value for the CTSS problem after many
attempts.
VI. E XPERIMENTS
We have implemented the ACTGAN and other algorithms,
and conducted extensive experiments under diverse workloads.
The source code and the data can be found in the online anony-
mous repository: https://github.com/anon4review/ACTGAN.
In this section, we ﬁrst describe our experiment setup, and
then present the experimental results to prove the efﬁciency
and effectiveness of the proposed approach.
A. Experimental Settings
Running environment . We conduct our experiments on a
local cluster of 5 physical servers. Each server is equipped with
two 8-core Intel Intel XeonE5-2650v2 2.6GHz processors,
256GiB RAM, 1.5TB disk, and running CentOS 6.0 and
Java 1.7.0 55. All of servers are connected via a high-speed
1.5Gbps LAN. To avoid interference and comply with the
actual deployment, we run the SUTs, the workload generators
and the ACTGAN algorithm on different physical servers at
each experiment.
SUTs, benchmarks, parameters, and performance met-
rics. We choose eight widely used software systems to eval-
uate ACTGAN, namely Kafka [10], Spark [1], Hive [11],
Redis [12], MySQL [13], Cassandra [14], HBase [2] and
Tomcat [15]: Kafka is a distributed message system (DMS)
for publishing and subscribing to streams of records; Spark
is a cluster computing engine for big data analytics; Hive
is a data warehouse software that manages large datasets
residing in distributed storage using SQL; Redis is an open-
source in-memory data structure store; MySQL is an open-
source relational database management system (RDBMS);
Cassandra is an open-source column-oriented NoSQL database
management system; HBase is a distributed and scalable big
data store; and Tomcat is an open source implementation of
the Java Web technologies.
We use six customized workloads for Kafka; HiBench
[55] for Spark; YCSB [56] for Hive, Cassandra and HBase;Redis-Bench1for Redis; TPCC2for MySQL; and JMeter
[57] for Tomcat. For each SUT, we use domain expertise
to identify 10–14 parameters that are considered critical to
the performance, as in [4], [5], [31]. Note that even with
only these parameters, the search space is still enormous, and
exhaustive search is infeasible. The reason we choose a small
subset of parameters that have great impact on performance
instead of all conﬁgurable parameters is because reducing the
number of tuning parameters can reduce the search space
exponentially, and most existing approaches [4], [5], [31]
also adopt this manual feature selection strategy. Table II
summarizes the SUTs along with the benchmarks, the numbers
of tuned parameters, and performance metrics, respectively.
Baseline Algorithms and Hyperparameters . To evaluate
the performance of ACTGAN, we compare it with six state-
of-the-art algorithms, namely random search [23], BestConﬁg
[4], RFHOC [5], SMAC [43], Hyperopt [44], and AutoConﬁg
[46]. We provide a brief description for each algorithm as
follows and report its hyperparameters (including ACTGAN)
in Table III.
Random search (Random) is a search-based tuning approach
that explores the conﬁguration space uniformly at random [23].
BestConﬁg3is a search-based tuning approach that uses
divide-and-diverge sampling and recursive bound-and-search
algorithm to ﬁnd the best conﬁguration.
RFHOC is a learning-based tuning approach that constructs
a prediction model using random forests, and a genetic algo-
rithm to automatically explore the conﬁguration space.
SMAC4is a learning-based tuning method using random
forests and an aggressive racing strategy.
Hyperopt5is a learning-based tuning approach based on
Bayesian optimization. It is widely used for hyperparameter
optimization.
AutoConﬁg6is a learning-based tuning approach using a
comparison-based prediction model and a weighted Latin
hypercube sampling method.
For each run in our experiments, every algorithm is executed
under the same time constraint and stops once the constraint is
met. To ensure consistency, we run each workload ﬁve times
and calculate the average of these ﬁve runs.
B. Experiment Results
Performance results . Given a ﬁxed time constraint (about
316 running times using default conﬁguration) for each SUT
and workload, we run seven different tuning algorithms
plus default conﬁguration independently. Table IV lists the
experiment results. As expected, the default conﬁguration
does not perform well. ACTGAN achieves an average of
76.22% improvement over the default conﬁgurations. Fur-
thermore, ACTGAN outperforms all other six algorithms:
1https://redis.io/topics/benchmarks
2https://github.com/Percona-Lab/tpcc-mysql
3https://github.com/zhuyuqing/bestconf
4http://www.cs.ubc.ca/labs/beta/Projects/SMAC
5http://jaberg.github.io/hyperopt/
6https://github.com/sselab/autoconﬁg
471
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Table II
SUT S,BENCHMARKS ,PARAMETERS ,AND PERFORMANCE METRICS
SUTs Categories Benchmarks# of tuned parameters/
# of all parametersPerformance Tuning Goal
Kafka DMS Customized 11/169 Throughput (TP) (MB/s) max↑
Spark Data analytics HiBench 13/227 Execution time (ET) (ms) min↓
Hive Data analytics YCSB 11/432 Execution time (ET) (ms) min↓
Redis In-memory DB Redis-Bench 10/95 Requests per second (RPS) max↑
MySQL RDBMS TPCC 13/46 Transactions per minute (TPM) max↑
Cassandra NoSQL DB YCSB 11/64 Operations per second (OPS) max↑
HBase NoSQL DB YCSB 10/202 Execution time (ET) (ms) min↓
Tomcat Web server JMeter 14/229 Requests per second (RPS) max↑
Table III
HYPERPARAMETERS FOR EACH ALGORITHM
Algorithm Hyperparameters
Random # of conﬁgurations: 316
BestConﬁgInitialSampleSetSize: 50; RRSMaxRounds: 5;
COMT2Iteration: 20; COMT2MultiIteration: 10;
TimeOutInSeconds: 50
RFHOCpopulation size: 100; epoch: 100000; crossover
parameters: 0.5; mutation probability: 0.015
SMACrunobj: quality; runcount limit: 316;
deterministic: true
Hyperoptsearch algorithms: hyperopt.rand.suggest,
hyperopt.anneal.suggest; max evals: 316
AutoConﬁg # of iterations: 100; α: 0.4;β: 0.6;h: 10;b:5
ACTGAN1Ns: 300;Nr: 16; epoch: 150000;
learning rate: 0.0001; b: 32;m: 16;ki
g:5 ;
kh
g: 128;ko
g:n;ki
d:n;kh
d: 128;ko
d:1
9.28%–53.99% improvement over Random, 7.21%–27.45%
improvement over BestConﬁg, 9.52%–38.44% improvement
over RFHOC, 10.41%–49.58% improvement over SMAC,
9.66%–64.56% improvement over Hyperopt, and 6.58%–
34.59% over AutoConﬁg.
Finally, we plot the overall performance improvement per-
centage of BestConﬁg, RFHOC, SMAC, Hyperopt, AutoCon-
ﬁg, and ACTGAN on Kafka, Spark, and other six software
systems in Figure 6, 7 and 8 respectively, using the random
algorithm as the baseline. In each ﬁgure, x-axis lists the
workloads and y-axis represents the performance improvement
over the random algorithm. We observe that compared with
the random algorithm, our approach achieves 9.28%–53.99%
improvement among all workloads. ACTGAN achieves an av-
erage of 20.04% improvement over Random, 17.50% improve-
ment over BestConﬁg, 31.48% improvement over RFHOC,
18.09% improvement over SMAC, 20.59% improvement over
Hyperopt, and 14.32% improvement over AutoConﬁg. We can
conclude from Figure 6, 7 and 8 that ACTGAN achieves stable
and signiﬁcant improvements compared with the other six
algorithms. Another interesting observation is that the random
search achieves surprisingly good results in our experiments.
This is consistent with the ﬁndings of Bergstra and Bengio in
[23].
In the above experiments, the number of the random sam-L100P1R1 L100P1R-1 L100P3R1 L1000P1R1 L100P1R-1 L1000P1R1
Kafka Workloads-30-20-100102030Imp(Random)BestConfig RFHOC SMAC Hyperopt AutoConfig ACTGAN
Figure 6. Performance comparison on six Kafka workloads with different
algorithms
Bayes KMeans Pagerank Sort Wordcount
Spark Workloads-40-30-201001020Imp(Random)BestConfig RFHOC SMAC Hyperopt AutoConfig ACTGAN
Figure 7. Performance comparison on ﬁve Spark workloads with different
algorithms
472
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Table IV
PERFORMANCE RESULTS FROM DIFFERENT ALGORITHMS UNDER FIXED TIME CONSTRAINTS
SUT/Perf./ ↑↓ Workload TC (h) Default Random BestConﬁg RFHOC SMAC Hyperopt AutoConﬁg ACTGAN
Kafka/TP (MB/s)/ ↑L100P1R1 15 16.41 20.81 20.62 18.89 21.07 20.99 20.21 23.40
L100P1R-1 17.5 5.50 13.09 13.32 11.40 13.39 13.39 13.21 14.79
L100P3R1 15 38.27 55.89 55.74 53.79 54.08 56.42 57.01 61.76
L1000P1R1 15 27.15 32.62 27.77 24.84 32.02 32.20 32.45 37.47
L1000P1R-1 17.5 8.37 15.68 16.01 15.86 15.76 16.53 16.93 19.77
L10000P1R1 15 23.18 34.97 35.20 33.40 35.15 38.16 37.93 42.26
Spark/ET (ms)/ ↓Bayes 15 150719 104626 104787 117983 104408 105776 103121 85402
KMeans 17.5 1124410 278854 266915 270065 644736 367715 261267 228236
Pagerank 36.7 440129 280736 303995 330637 289695 266026 259718 242334
Sort 20 249135 192932 189047 199555 178346 215654 188092 163168
Wordcount 12.5 479749 150827 145917 151290 150768 150084 142218 132858
Hive/ET (ms)/ ↓ YCSB 9.2 111256 94322 90900 111078 92375 92550 92910 83694
Redis/RPS/ ↑ Redis-Bench 1.5 82576 83633 97494 84947 100268 98032.2 100121 113145.6
MySQL/TPM/ ↑ TPCC 20 2735 2992 2950 2747 3518 2885 3106.5 4181
Cassandra/OPS/ ↑ YCSB 1.5 4088.31 12562.81 13550.14 8756.57 11750.88 11560.69 13148.52 14548.52
HBase/ET (ms)/ ↓ YCSB 1.5 14302 12233 15223 13282 12138 13508 12098 11098
Tomcat/RPS/ ↑ JMeter 4.6 6390 8243.57 11791.53 7381.80 8640.30 8111.18 9694.64 12694.44
Hive Redis MySQL Cassandra HBase Tomcat
Workloads of Six Software Systems-40-30-20-100102030405060Imp(Random)BestConfig RFHOC SMAC Hyperopt AutoConfig ACTGAN
Figure 8. Performance comparison on six SUTs with different algorithms
plesNsand the number of recommended conﬁgurations Nr
are set to 300 and 16, respectively. We believe that the optimal
values of these two hyperparameters are CTSS-speciﬁc, and
can be trained. In our experiment, we have evaluated different
values of NsandNrand observed that the results are not
sensitive to these hyperparameters. For the ease of reporting,
we choose to report the same set of values.
The quality of conﬁgurations . To evaluate the quality of
conﬁgurations recommended by ACTGAN, we plot the nor-
malized performance (the higher the better: i.e. “0” means the
worst performance and “1” means the best) histogram over
the initial training samples (blue bars) and the conﬁgurations
generated by ACTGAN (red bars) for eight different software
systems in Figure 9. We can see from Figure 9 that the
generated conﬁgurations can achieve the best performance in
all cases. The generated conﬁgurations in general outperform
the training samples, a clear demonstration of the effectiveness
of the ACTGAN approach.
Another interesting observation is that these ACTGAN-generated conﬁgurations incline to choose a ﬁxed value on
certain categorical parameters for a speciﬁc workload. For ex-
ample, the Spark parameter spark.serializer= {JavaSerializer ,
KryoSerializer }is all set to JavaSerializer for Bayes ,Sort
and Wordcount workloads, and for KMeans and Pagerank
workloads it is all set to KryoSerializer . Such choices are in
accord with domain knowledge and ﬁeld experience obtained
from long term software system operation and maintenance.
C. Threats to V alidity
Internal validity : To increase the internal validity, we
performed controlled experiments by executing each test case
ﬁve times and calculate the average of these ﬁve runs. Such
method can avoid misleading effects of speciﬁcally selected
test cases and ensures the stability of the result. Besides, we
set the same time constraint value suggested by users for
all algorithms (including ACTGAN) in each test case and
each algorithm is forced to stop when the time constraint
is met. Such results are considered to be fair and reliable.
In addition, we set the values of hyperparameters for each
compared algorithm as suggested by their authors (see Table
III). Finally, we have tried multiple values of hyperparameters
for ACTGAN in our experiments and observed that the good
values of these hyperparameters that can lead to better results
are almost the same from test case to test case.
External validity : We increase the external validity by
choosing eight different SUTs including one message sys-
tem (Kafka), two big data processing systems (Hive and
Spark), four database systems (Redis, MySQL, Cassandra, and
HBase), and one Web application server (Tomcat). Further-
more, we choose 17 different workloads on these SUTs and
among them six testing scenarios with various application-
level parameters are for Kafka, and ﬁve HiBench workloads
are for Spark. Finally, we are aware that because our ACTGAN
is a general black-box approach and is independent to the
SUTs, the results of our evaluations are transferable to other
software systems.
473
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Kafka
0.10.20.30.40.50.60.70.80.91.0
Normalized performance020406080# of configurationsTraining set Generated
Spark
0.10.20.30.40.50.60.70.80.91.0
Normalized performance0204060# of configurationsHive
0.10.20.30.40.50.60.70.80.91.0
Normalized performance051015# of configurationsRedis
0.10.20.30.40.50.60.70.80.91.0
Normalized performance0102030# of configurations
MySQL
0.10.20.30.40.50.60.70.80.91.0
Normalized performance010203040# of configurationsCassandra
0.10.20.30.40.50.60.70.80.91.0
Normalized performance05101520# of configurationsHBase
0.10.20.30.40.50.60.70.80.91.0
Normalized performance0102030# of configurationsTomcat
0.10.20.30.40.50.60.70.80.91.0
Normalized performance0102030# of configurations
Figure 9. Normalized performance distributions between the initial training set and the ACTGAN generated conﬁgurations for eight different software systems
D. Discussion
Our work builds upon the hypothesis that good conﬁgu-
rations share hidden structures. Although we could not prove
this hypothesis mathematically, our results clearly support this
hypothesis. First, our extensive experimental evaluations show
that the ACTGAN generates conﬁgurations are better than the
ones used for training, which suggests that ACTGAN suc-
cessfully uncovers hidden structures of good conﬁgurations.
Furthermore, the above observation of ACTGAN “choosing
a ﬁxed value on certain categorical parameters for a speciﬁc
workload” also supports our hypothesis because it is consistent
with domain knowledge.
The more interesting question is why ACTGAN could
improve upon training samples. An analogy is as follows:
suppose we feed real pictures of “good-looking” people into
a GAN and train it to generate artiﬁcial “good-looking” faces.
Psychological studies suggest that a face being symmetry is
important to “good-looking”, a hidden structure that GAN
needs to learn. After learning this structure, GAN can generate
perfectly symmetric faces. Given no real human faces are
perfectly symmetric, the generated pictures are “better”, in
terms of the hidden structure of symmetry. In the CTSS
scenario, our intuition is that ACTGAN can identify such
hidden structures in good conﬁgurations and then generate
conﬁgurations that inherit such hidden structures while
avoid (some of) the “imperfectness” in existing training
samples, which are generated and selected from random
sampling . Therefore, in the end, ACTGAN improves upon the
training samples.
We note that given the limited tuning time, noalgorithm
can explore the entire the high-dimensional parameter space.
Random sampling is a simple yet robust mechanism to ﬁnd
good conﬁgurations without prior knowledge, and thus chosen
here. Other effective learning schemes can be easily adopted
to replace random sampling to generate the initial set of good
training samples. Therefore, our ACTGAN can complementand augment existing learning algorithms.
We also note that because of the high dimensional con-
ﬁguration space and limited tuning time, it is possible that
obtained training samples do not contain all hidden structures
of an optimal conﬁguration, and thus ACTGAN could not
learn all them. This is an inherent limitation that all algorithms
are likely to suffer in a high-dimensional non-convex general
optimization problem.
VII. C ONCLUSION AND FUTURE WORK
In this paper, we propose ACTGAN, a novel approach to
address CTSS. ACTGAN can recommend promising conﬁgu-
rations by directly learning and utilizing the hidden structures
of existing good conﬁgurations. The performance superiority
of ACTGAN has been illustrated by extensive real-system
evaluations in comparison to six state-of-the-art algorithms.
ACTGAN is a ﬁrst attempt to leverage GAN structure to
solve CTSS and future work is multifold. First, it is well
known that GAN, although a powerful tool to identify hidden
structures, has its limitations, such as sensitivity to mode drop-
ping and mode collapsing. It is ongoing research to identify
and address these limitations. Our future work is to study
how these limitations affect ACTGAN and its performance
on CTSS. Furthermore, most existing work on addressing the
limitations of GAN focuses on image processing. How can
we adapt these studies to CTSS, e.g., how do we quantify and
identify mode dropping in CTSS? CTSS has unique features
such that existing solutions may or may not work effective,
and thus require further investigation.
The holy grail is to leverage ACTGAN to identify and
interpret the hidden structures of good conﬁgurations. Our
observation of ﬁxed values on certain categorical parameters
seems to be promising. However, this is a challenging task
that requires much domain knowledge and signiﬁcant future
work.
It is also our future work to reﬁne our ACTGAN approach
to adaptive adjust the hyperparameters ( NrandNs) based on
474
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. time constraint, SUT, and workload. We will also investigate
the performance dynamics of more software systems and
improve our GAN structure to obtain better conﬁgurations.
REFERENCES
[1] spark, “http://spark.apache.org,” Accessed on Decemeber 20th 2018.
[2] HBase, “https://hbase.apache.org/,” Accessed on July 31th 2018.
[3] H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, F. B. Cetin, and
S. Babu, “Starﬁsh: A self-tuning system for big data analytics.” in CIDR ,
vol. 11, 2011, pp. 261–272.
[4] Y . Zhu, J. Liu, M. Guo, Y . Bao, W. Ma, Z. Liu, K. Song, and Y . Yang,
“Bestconﬁg: tapping the performance potential of systems via automatic
conﬁguration tuning,” in Proceedings of the 2017 Symposium on Cloud
Computing . ACM, 2017, pp. 338–350.
[5] Z. Bei, Z. Yu, H. Zhang, W. Xiong, C. Xu, L. Eeckhout, and S. Feng,
“Rfhoc: A random-forest approach to auto-tuning hadoop’s conﬁgura-
tion,” IEEE Transactions on Parallel and Distributed Systems , vol. 27,
no. 5, pp. 1470–1483, 2016.
[6] J. Guo, D. Yang, N. Siegmund, S. Apel, A. Sarkar, P. Valov, K. Czar-
necki, A. Wasowski, and H. Yu, “Data-efﬁcient performance learning for
conﬁgurable systems,” Empirical Software Engineering , vol. 23, no. 3,
pp. 1826–1867, 2018.
[7] C. Liu, D. Zeng, H. Yao, C. Hu, X. Yan, and Y . Fan, “Mr-cof: a ge-
netic mapreduce conﬁguration optimization framework,” in International
Conference on Algorithms and Architectures for Parallel Processing .
Springer, 2015, pp. 344–357.
[8] K. Wang and M. M. H. Khan, “Performance prediction for apache
spark platform,” in High Performance Computing and Communications
(HPCC), 2015 IEEE 17th International Conference on . IEEE, 2015,
pp. 166–173.
[9] T. Xu, L. Jin, X. Fan, Y . Zhou, S. Pasupathy, and R. Talwadker,
“Hey, you have given me too many knobs!: understanding and dealing
with over-designed conﬁguration in system software,” in Proceedings of
the 2015 10th Joint Meeting on F oundations of Software Engineering .
ACM, 2015, pp. 307–319.
[10] Kafka, “https://kafka.apache.org/,” Accessed on July 31th 2018.
[11] Hive, “https://hive.apache.org/,” Accessed on July 31th 2018.
[12] Redis, “https://redis.io/,” Accessed on July 31th 2018.
[13] MySQL, “https://www.mysql.com/,” Accessed on July 31th 2018.
[14] Cassandra, “http://cassandra.apache.org/,” Accessed on July 31th 2018.
[15] Tomcat, “http://tomcat.apache.org/,” Accessed on July 31th 2018.
[16] T. Ye and S. Kalyanaraman, “A recursive random search algorithm
for large-scale network parameter conﬁguration,” ACM SIGMETRICS
Performance Evaluation Review , vol. 31, no. 1, pp. 196–205, 2003.
[17] M. Woodside, G. Franks, and D. C. Petriu, “The future of software
performance engineering,” in 2007 Future of Software Engineering .
IEEE Computer Society, 2007, pp. 171–187.
[18] S. Balsamo, A. Di Marco, P. Inverardi, and M. Simeoni, “Model-
based performance prediction in software development: A survey,” IEEE
Transactions on Software Engineering , vol. 30, no. 5, pp. 295–310, 2004.
[19] H. Koziolek, “Performance evaluation of component-based software
systems: A survey,” Performance Evaluation , vol. 67, no. 8, pp. 634–
658, 2010.
[20] D. Paluch, H. Kienegger, and H. Krcmar, “A workload-dependent
performance analysis of an in-memory database in a multi-tenant
conﬁguration,” in Companion of the 2018 ACM/SPEC International
Conference on Performance Engineering . ACM, 2018, pp. 131–134.
[21] B. Chen, Y . Liu, and W. Le, “Generating performance distributions via
probabilistic symbolic execution,” in Proceedings of the 38th Interna-
tional Conference on Software Engineering . ACM, 2016, pp. 49–60.
[22] A. Abdelaziz, W. Kadir, and A. Osman, “Comparative analysis of
software performance prediction approaches in context of component-
based system,” International Journal of Computer Applications , vol. 23,
no. 3, pp. 15–22, 2011.
[23] J. Bergstra and Y . Bengio, “Random search for hyper-parameter opti-
mization,” Journal of Machine Learning Research , vol. 13, no. Feb, pp.
281–305, 2012.
[24] T. Wang, M. Harman, Y . Jia, and J. Krinke, “Searching for better
conﬁgurations: a rigorous approach to clone evaluation,” in Proceedings
of the 9th Joint Meeting on F oundations of Software Engineering . ACM,
2013, pp. 455–465.[25] V . Nair, T. Menzies, N. Siegmund, and S. Apel, “Using bad learners to
ﬁnd good conﬁgurations,” in Proceedings of the 11th Joint Meeting on
F oundations of Software Engineering . ACM, 2017, pp. 257–267.
[26] J. Oh, D. Batory, M. Myers, and N. Siegmund, “Finding near-optimal
conﬁgurations in product lines by random sampling,” in Proceedings of
the 11th Joint Meeting on F oundations of Software Engineering . ACM,
2017, pp. 61–71.
[27] R. Olaechea, D. Rayside, J. Guo, and K. Czarnecki, “Comparison of
exact and approximate multi-objective optimization for software product
lines,” in Proceedings of the 18th International Software Product Line
Conference-V olume 1 . ACM, 2014, pp. 92–101.
[28] F. Wu, W. Weimer, M. Harman, Y . Jia, and J. Krinke, “Deep parameter
optimisation,” in Proceedings of the 2015 Annual Conference on Genetic
and Evolutionary Computation . ACM, 2015, pp. 1375–1382.
[29] B. Xi, Z. Liu, M. Raghavachari, C. H. Xia, and L. Zhang, “A smart hill-
climbing algorithm for application server conﬁguration,” in Proceedings
of the 13th international conference on World Wide Web . ACM, 2004,
pp. 287–296.
[30] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimiza-
tion of machine learning algorithms,” in Advances in neural information
processing systems , 2012, pp. 2951–2959.
[31] A. Sarkar, J. Guo, N. Siegmund, S. Apel, and K. Czarnecki, “Cost-
efﬁcient sampling for performance prediction of conﬁgurable systems,”
inAutomated Software Engineering (ASE), 2015 30th IEEE/ACM Inter-
national Conference on . IEEE, 2015, pp. 342–352.
[32] N. B. Rizvandi, J. Taheri, R. Moraveji, and A. Y . Zomaya, “On
modelling and prediction of total cpu usage for applications in mapre-
duce environments,” in International Conference on Algorithms and
Architectures for Parallel Processing . Springer, 2012, pp. 414–427.
[33] M. Niu, S. Rogers, M. Filippone, and D. Husmeier, “Fast inference in
nonlinear dynamical systems using gradient matching,” in Journal of
Machine Learning Research: Workshop and Conference Proceedings ,
vol. 48. Journal of Machine Learning Research, 2016, pp. 1699–1707.
[34] S. Wang, C. Li, H. Hoffmann, S. Lu, W. Sentosa, and A. I. Kistijan-
toro, “Understanding and auto-adjusting performance-sensitive conﬁgu-
rations,” in ACM SIGPLAN Notices , vol. 53, no. 2. ACM, 2018, pp.
154–168.
[35] P. Lama and X. Zhou, “Aroma: Automated resource allocation and
conﬁguration of mapreduce environment in the cloud,” in Proceedings
of the 9th international conference on Autonomic computing . ACM,
2012, pp. 63–72.
[36] N. Luo, Z. Yu, Z. Bei, C. Xu, C. Jiang, and L. Lin, “Performance
modeling for spark using svm,” in Cloud Computing and Big Data
(CCBD), 2016 7th International Conference on . IEEE, 2016, pp. 127–
131.
[37] Y . Zhang, J. Guo, E. Blais, and K. Czarnecki, “Performance prediction
of conﬁgurable software systems by fourier learning,” in Automated
Software Engineering (ASE), 2015 30th IEEE/ACM International Con-
ference on . IEEE, 2015, pp. 365–373.
[38] C. Tang, “System performance optimization via design and conﬁguration
space exploration,” in Proceedings of the 2017 11th Joint Meeting on
F oundations of Software Engineering . ACM, 2017, pp. 1046–1049.
[39] S. Soltani, M. Asadi, M. Hatala, D. Ga ˇsevi´c, and E. Bagheri, “Automated
planning for feature model conﬁguration based on stakeholders’ busi-
ness concerns,” in Automated Software Engineering (ASE), 2011 26th
IEEE/ACM International Conference on . IEEE, 2011, pp. 536–539.
[40] V . Nair, T. Menzies, N. Siegmund, and S. Apel, “Faster discovery of
faster system conﬁgurations with spectral learning,” Automated Software
Engineering , pp. 1–31, 2017.
[41] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and K. Matsumoto,
“Automated parameter optimization of classiﬁcation techniques for
defect prediction models,” in Software Engineering (ICSE), 2016
IEEE/ACM 38th International Conference on . IEEE, 2016, pp. 321–
332.
[42] S. Falkner, A. Klein, and F. Hutter, “BOHB: Robust and
efﬁcient hyperparameter optimization at scale,” in Proceedings
of the 35th International Conference on Machine Learning ,
ser. Proceedings of Machine Learning Research, J. Dy and
A. Krause, Eds., vol. 80. Stockholmsmssan, Stockholm Sweden:
PMLR, 10–15 Jul 2018, pp. 1436–1445. [Online]. Available:
http://proceedings.mlr .press/v80/falkner18a.html
[43] F. Hutter, H. Hoos, and K. L-Brown, “Sequential model based optimiza-
tion for general algorithm conﬁguration.” LION , vol. 5, pp. 507–523,
2011.
475
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. [44] J. Bergstra, D. Yamins, and D. D. Cox, “Hyperopt: A python library
for optimizing the hyperparameters of machine learning algorithms,” in
Proceedings of the 12th Python in Science Conference . Citeseer, 2013,
pp. 13–20.
[45] C. Peng, C. Zhang, C. Peng, and J. Man, “A reinforcement learning
approach to map reduce auto-conﬁguration under networked environ-
ment,” International Journal of Security and Networks , vol. 12, no. 3,
pp. 135–140, 2017.
[46] L. Bao, X. Liu, Z. Xu, and B. Fang, “Autoconﬁg: Automatic conﬁgu-
ration tuning for distributed message systems.” in Automated Software
Engineering (ASE), 2018 33rd IEEE/ACM International Conference on .
IEEE, 2018, pp. 29–40.
[47] H. Chen, W. Zhang, and G. Jiang, “Experience transfer for the conﬁgu-
ration tuning in large-scale computing systems,” IEEE Transactions on
Knowledge and Data Engineering , vol. 23, no. 3, pp. 388–401, 2011.
[48] P. Jamshidi, N. Siegmund, M. Velez, and C. K ¨astner, “Transfer learning
for performance modeling of conﬁgurable systems: An exploratory anal-
ysis,” in Proceedings of the 32nd IEEE/ACM International Conference
on Automated Software Engineering . IEEE Press, 2017, pp. 497–508.
[49] C. Blum and A. Roli, “Metaheuristics in combinatorial optimiza-
tion: Overview and conceptual comparison,” ACM computing surveys(CSUR) , vol. 35, no. 3, pp. 268–308, 2003.
[50] C. H. Papadimitriou and K. Steiglitz, “Combinatorial optimization:
algorithms and complexity,” 1982.
[51] M. R. Garey and D. S. Johnson, “Computers and intractability: a guide
to np-completeness,” 1979.
[52] P. I. Frazier, “A tutorial on bayesian optimization,” arXiv preprint
arXiv:1807.02811 , 2018.
[53] X. Li and X. Yao, “Cooperatively coevolving particle swarms for large
scale optimization,” IEEE Transactions on Evolutionary Computation ,
vol. 16, no. 2, pp. 210–224, 2012.
[54] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980 , 2014.
[55] S. Huang, J. Huang, J. Dai, T. Xie, and B. Huang, “The hibench bench-
mark suite: Characterization of the mapreduce-based data analysis,” in
Data Engineering Workshops (ICDEW), 2010 IEEE 26th International
Conference on . IEEE, 2010, pp. 41–51.
[56] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears,
“Benchmarking cloud serving systems with ycsb,” in Proceedings of the
1st ACM symposium on Cloud computing . ACM, 2010, pp. 143–154.
[57] JMeter, “https://jmeter.apache.org/,” Accessed on July 31th 2018.
476
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. 