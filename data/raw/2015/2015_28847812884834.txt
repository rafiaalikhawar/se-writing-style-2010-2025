Revisit of Automatic Debugging via Human Focus-tracking
Analysis
Xiaoyuan Xie
State Key Lab of Software
Engineering, Computer School
Wuhan University
xxie@whu.edu.cnZicong Liu
State Key Lab of Novel
Software Technology
Nanjing University
zicong.liu.nju@gmail.comShuo Song
State Key Lab of Novel
Software Technology
Nanjing University
songshuosz@gmail.com
Zhenyu Chen
State Key Lab of Novel
Software Technology
Nanjing University
zychen@nju.edu.cnJifeng Xuan
State Key Lab of Software
Engineering, Computer School
Wuhan University
jxuan@whu.edu.cnBaowen Xu
State Key Lab of Novel
Software Technology
Nanjing University
bwxu@nju.edu.cn
ABSTRACT
In many Ô¨Åelds of software engineering, studies on human behavior
have attracted a lot of attention; however, few such studies exist
in automated debugging. Parnin and Orso conducted a pioneer-
ing study comparing the performance of programmers in debug-
ging with and without a ranking-based fault localization technique,
namely Spectrum-Based Fault Localization (SBFL). In this paper,
we revisit the actual helpfulness of SBFL, by addressing some ma-
jor problems that were not resolved in Parnin and Orso‚Äôs study. Our
investigation involved 207 participants and 17 debugging tasks. A
user-friendly SBFL tool was adopted. It was found that SBFL
tended not to be helpful in improving the efÔ¨Åciency of debugging.
By tracking and analyzing programmers‚Äô focus of attention, we
characterized their source code navigation patterns and provided
in-depth explanations to the observations. Results indicated that (1)
a short ‚ÄúÔ¨Årst scan‚Äù on the source code tended to result in inefÔ¨Åcient
debugging; and (2) inspections on the pinpointed statements dur-
ing the ‚Äúfollow-up browsing‚Äù were normally just quick skimming.
Moreover, we found that the SBFL assistance may even slightly
weaken programmers‚Äô abilities in fault detection. Our observations
imply interference between the mechanism of automated fault lo-
calization and the actual assistance needed by programmers in de-
bugging. To resolve this interference, we provide several insights
and suggestions.
CCS Concepts
Software and its engineering !Software testing and debug-
ging;
Corresponding
author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14-22, 2016, Austin, TX, USA
c‚Éù2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2884781.2884834Keywords
Automated debugging, spectrum-based fault localization, user stud-
ies, attention tracking, navigation pattern, fault comprehension
1. INTRODUCTION
It is widely known that software engineering is a human-centric
discipline [29]. Studies on human behavior during the software life
cycle have attracted extensive attention in the community [6, 14,
21, 30]. Such studies usually aimed to reveal how people actually
behave and perform in various software engineering activities, in
order better facilitate these activities.
Software fault detection is one of the most human-intelligence-
intensive tasks in which analysis of human behavior is essential.
Uwano et al. [33] took the Ô¨Årst step, performing eye-tracking anal-
ysis in static code review, where a particular pattern, namely ‚Äúscan‚Äù
was identiÔ¨Åed. ‚ÄúScan‚Äù characterizes how a reviewer navigates the
entire source code before carefully inspecting each code line. On
the other hand, Bednarik [2] shifted the focus from static code re-
view to manual debugging1, speciÔ¨Åcally, how programmers dealt
with various information sources was investigated.
Apart from static code review and manual debugging, there is
another large family of fault detection techniques, namely auto-
mated debugging, which has been widely investigated over the past
two decades. In this Ô¨Åeld, researchers have been mainly focused
on proposing new techniques and investigating their performance
[1, 12, 20, 36‚Äì38] without paying much attention to human fac-
tors. However, the results provided by current automated debug-
ging techniques are only approximate, and their correctness is gen-
erally not guaranteed. As a consequence, these results still require
being understood and veriÔ¨Åed by the person who performs the re-
pair. Therefore, human behavior is critical and worth investigating
in these techniques.
Parnin and Orso realized this problem and conducted a study
which compared the actual performance of developers in debug-
ging with and without a lightweight ranking-based fault localiza-
tion technique [21], where they explicitly indicated the importance
of human factors in determining the helpfulness of this technique.
As a preliminary study, this work reveals several potential aspects
1W
e refer to ‚Äúdebugging‚Äù in this paper as the dynamic fault elim-
ination process using test execution information, in contrast to the
static code review.
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   808
that
should be further investigated.
First, a more comprehensive experiment is required to obtain a
statistically conclusive result. Secondly, the helpfulness of a better
automated debugging tool is worth studying. Thirdly, the impact
from the accuracy of fault localization needs to be re-examined
by considering the suggested measure of ‚Äúabsolute ranking‚Äù [21].
Fourthly, although it has been generally accepted that there is no
‚Äúperfect bug2detection‚Äù, we are interested to know whether auto-
mated debugging has any impact on human ability in comprehend-
ing the source code and identifying the bugs. Finally, we still do
not know what particular patterns programmers may follow in nav-
igating the source code during automated debugging and how these
patterns affect debugging efÔ¨Åciency.
Therefore, in this study, we revisit the helpfulness of automated
debugging by addressing the above points. We are most interested
in identifying the reasons behind the observed results via a quan-
titative focus-tracking analysis, which refers to tracking and ana-
lyzing the focus of attention of programmers. In this paper, we an-
swer three Research Questions (RQ) from an experiment with 207
participants and 17 debugging tasks which were categorized ac-
cording to their fault localization accuracies. Our experiment used
the same ranking-based fault localization technique that Parnin and
Orso used in [21], i.e. Spectrum-Based Fault Localization (SBFL).
Each participant was required to debug two programs with and
without SBFL, respectively. A platform, namely Mooctest [19],
with user-friendly SBFL assistance was implemented, which can
colorize the suspicious code lines and record all of the operations
performed by the participants during debugging, such as Ô¨Åle open-
ing, cursor movement and Ô¨Åle changing. These recorded operations
were used to analyze the focus of attention of the participants and
to reveal their code navigation patterns.
We found that an accurate localization result was helpful in com-
pleting the debugging tasks. However, surprisingly, we also ob-
served that SBFL was not helpful in improving the efÔ¨Åciency of
debugging and sometimes even made it worse. By analyzing the
operation logs recorded by Mooctest, we found some navigation
patterns. In debugging with SBFL, programmers typically started
with a ‚ÄúÔ¨Årst scan‚Äù, i.e. a skim of the entire or part of the code,
before inspecting the most suspicious code highlighted by the plat-
form, and then switched between the most suspicious code and oth-
ers. A quantitative analysis showed that (1) a short ‚ÄúÔ¨Årst scan‚Äù
tended to result in inefÔ¨Åcient debugging; and (2) inspections on
the pinpointed statements during the ‚Äúfollow-up browsing‚Äù on the
source code after the ‚ÄúÔ¨Årst scan‚Äù were normally just quick skim-
ming. Moreover, we found that the SBFL assistance in Mooctest,
especially for cases with inaccurate results, may even slightly weaken
programmers‚Äô abilities in comprehending the code and identifying
the faults.
Our analysis conÔ¨Årms the importance of code comprehension.
Also it implies that presenting code together with fault ranking in-
formation, which intends to help programmers focus on the sus-
picious code as quickly as possible without wasting time on other
parts of the program, may actually interfere with the real assistance
needed by programmers in debugging. This interference is most
likely the reason for the decrease in efÔ¨Åciency. We conjecture that
the interference is due to an inappropriate application of the tech-
nique. We further give some insights and suggestions on how to
properly perform automated debugging, in order to really beneÔ¨Åt
from it.
The contributions of this paper are summarized as follows.
We conducted a comparison of programmers‚Äô performance
2In
this paper, we will use ‚Äúbug‚Äù and ‚Äúfault‚Äù interchangeably.between debugging with and without SBFL, where 207 par-
ticipants and 17 debugging tasks were involved and a user-
friendly debugging platform was adopted. We also analyzed
the impact from different levels of SBFL accuracy by con-
sidering ‚Äúabsolute ranking‚Äù.
We performed analysis to track the focus of attention of our
participants, where two important code navigation patterns,
namely, ‚ÄúÔ¨Årst scan‚Äù and ‚Äúfollow-up browsing‚Äù, were observed.
And their relations with the efÔ¨Åciency of SBFL-assisted de-
bugging were revealed.
We reconÔ¨Årmed the importance of code comprehension with
supportive evidence. Our analysis also revealed that auto-
mated fault localization may interfere with programmers‚Äô code
comprehension and fault identiÔ¨Åcation. To help program-
mers really beneÔ¨Åt from automated debugging, we provided
some insights and suggestions.
2. RELATED WORK
2.1 Analyses of human behavior in software
engineering
Human behavior has been considered to have a major impact on
the entire software life cycle [9]. Understanding how people behave
and perform in various software engineering activities can help to
answer why a particular technique is or is not effective.
Crosby and Stelovsky [6] Ô¨Årst investigated the inÔ¨Çuence of pro-
gramming experience on viewing a short but complex algorithm
via an eye-tracking technique. Sharif and his colleagues conducted
eye-tracking studies on UML diagram comprehension [27]. They
also performed similar investigation about the impact of identiÔ¨Åer
style on code comprehension [3]. Rodeghero et al. [23] tracked
developers‚Äô focus of attention to provide evidence on how humans
chose the keywords of a program for source code summarization.
And Lewis et al. [18] evaluated how well a notable bug predication
algorithm, FixCache, helped developers.
In static code review, Uwano et al. [33] took the Ô¨Årst step in
adopting eye-tracking techniques to explain the performance of var-
ious reviewing strategies. Uwano et al. identiÔ¨Åed a ‚Äúscan‚Äù pattern
in static code review, i.e., people usually have a preliminary read-
ing of the entire code at the beginning of their review. They found
that the longer a reviewer scanned the code, the more efÔ¨Åciently he
could Ô¨Ånd the fault in code review. This Ô¨Ånding was later conÔ¨Årmed
by Sharif et al. [26] via more comprehensive empirical studies.
Concerning debugging, Stein and Brennan [30] conducted an in-
teresting analysis, which demonstrated that novice programmers
can debug more efÔ¨Åciently after viewing the eye gaze traces from
other professional programmers. Ko et al. [13] tracked developers
actions such as ‚Äúsearching‚Äù, ‚Äúnavigating‚Äù, and ‚Äúediting‚Äù to answer:
‚Äúhow do developers seek, relate, and collect relevant information‚Äù
in debugging tasks. There were also studies that aimed at predi-
cating programmers‚Äô code navigation behavior [16, 22]. However,
all of these works were about manual debugging; for automated
debugging, to the best of our knowledge, there are still very few
similar studies.
2.2 Analyses on automated debugging with hu-
man factors
It is widely accepted that debugging is extremely time-consuming.
In order to speed-up this process, many automated debugging tech-
niques have been proposed over the past two decades.
One type of techniques utilizes human intelligence to facilitate
debugging, for example one is called Whyline implemented by Ko
et al. [14]. Ko et al. invented a new debugging paradigm, namely,
809‚ÄúInterrog
ative Debugging‚Äù, which allowed programmers ‚Äúcommu-
nicate‚Äù with the software, by asking ‚Äúwhy did‚Äù questions. Answers
given by Whyline can guide programmers to Ô¨Ånd the cause of the
failure and Ô¨Åx the bug.
Another family of automated debugging techniques aims to re-
duce human involvement by providing possible fault locations. One
classic idea is to use program dynamic slicing, which can effec-
tively isolate statements involved in the calculation of the observed
failure. This method was Ô¨Årst proposed by Korel and Laski [15],
followed by a series improved versions [28, 40]. Another idea
is to model a fault localization problem into an information re-
trieval task, where bug reports are treated as queries and potential
faults are the most matchable code areas [24]. Besides, there is a
widely studied method, namely Spectrum-Based Fault Localization
(SBFL), which utilizes various program spectra and testing results
to evaluate the suspiciousness for each program entity. All entities
are then ranked in descending order and are provided to program-
mers as the suggested fault locations [1,12,37].
However, the fault locations suggested by this family of tech-
niques still require programmers‚Äô comprehension and veriÔ¨Åcation
to repair the software. In other words, although they reduce man-
ual efforts, these techniques cannot completely eliminate human
involvement. Hence, whether they are really helpful to program-
mers is well worth investigating. However, most of these studies
focused on how to improve the accuracy of fault localization; and
very few have taken human factors into account.
Weiser and Lyle [35] did the Ô¨Årst comparison of developers‚Äô
performance between debugging with and without program slic-
ing, where no signiÔ¨Åcant improvement was found by using slic-
ing. Francel and Spencer [7] also investigated the helpfulness of
program slicing to programmers in their code comprehension and
debugging abilities. In contrast, their experiments indicated that
slicing improved programmers‚Äô performance. Tao et al. [31] stud-
ied the effectiveness of human debugging via machine-generated
patches. A recent study by Wang et al. investigated how well infor-
mation retrieval techniques help developers to locate bugs [34].
In the Ô¨Åeld of SBFL that has been regarded as a light-weight
and practical fault localization technique, even fewer related works
have been reported. Parnin and Orso [21] did a pioneering study
to answer the question ‚Äúcan automated debugging help program-
mers‚Äù, where the actual performance of participants in debugging
with and without SBFL was compared. They showed that SBFL
can be helpful in some cases, especially for experienced program-
mers. They also reported some surprising observations, such as
‚Äúchanges in the ranking do not produce any signiÔ¨Åcant impact on
programmers‚Äô performance‚Äù, ‚Äúprogrammers may not follow the
provided ranking list‚Äù, and pointed out that the ‚Äúperfect bug de-
tection‚Äù is generally unrealistic. Besides, some suggestions were
given based on their observations to better improve the SBFL tools.
3. MOTIVATION AND RESEARCH QUES-
TIONS
The study by Parnin and Orso [21] provides a new perspective
from which to re-examine SBFL. Many aspects were revealed to
be in need of further investigation such as:
(1) Large-scale experiments in terms of the number of participants
and debugging tasks should be conducted. The experiment in
[21] only involved 34 programmers and two faulty programs.
Therefore, it is necessary to have larger-scale experiments for
a more statistically conclusive result.
(2) As suggested in [21], programmers‚Äô performance with a user-
friendly tool is worth studying. Parnin and Orso [21] provideda plug-in for SBFL which presented the localization results
simply in a ranking list. For example, two ‚Äúfor-loop‚Äù pred-
icates are ranked Ô¨Årst and second in the list. Then, a result
presented to a programmer looks like:
1 :‚Äúfor (int i= 0;i < N ;i++)‚Äù
2 :‚Äúfor (int i= 0;i < M ;i++)‚Äù
3 :: : :(other statements )
Clearly, such reordered statements are incomprehensible. Parnin
and Orso [21] alleviated this problem by providing navigation
from each statement in the list to its original location. They
have done this on purpose, because their goal was to investi-
gate the impact purely from the ranking list. However, it is
also interesting to know whether a more easy-to-follow result
can lead to similar or better performance. Actually, Gouveia et
al. have considered programmers‚Äô performance with their pro-
posed dynamic graphical localization reports [10]. However,
since the human factor was not the focus of that study, experi-
ments were still very preliminary.
In addition, facilities of test execution can be improved. In
[21], for each program, only one failed test case was provided
in a descriptive way, rather than in an executable format3. More
importantly, the failure information in one of their subject pro-
grams clearly indicated the lines throwing the exceptions, upon
which programmers were more likely to rely.
(3) Impact on debugging performance from SBFL accuracy with
respect to the ‚Äúabsolute ranking‚Äù, needs to be re-examined. As
conjectured by Parnin and Orso [21], programmers may not
follow the ranking after inspecting a few irrelevant statements,
which could be one possible explanation for their ‚Äúno impact‚Äù
observation, since neither of their changes on the list has ever
had the fault ranked top.
(4) Whether SBFL could affect fault detection, i.e. comprehend
and identify the fault, is still unknown. It has been generally
accepted that there is no ‚Äúperfect bug detection‚Äù. Simply in-
specting a fault is not sufÔ¨Åcient for immediately detecting it.
However, using the SBFL assistance and detecting the fault
may not be completely independent of one another. Therefore,
it is interesting to know whether the SBFL assistance has any
impact on fault detection.
(5) Last but most importantly, an in-depth analysis that tracks pro-
grammers‚Äô focus of attention to reveal their code navigation
pattern is needed. Parnin and Orso provided some preliminary
discussion on how developers navigate the program when de-
bugging. However it will be more convincing and informative
if some visualized and quantitative analysis can be provided.
In summary, there are still many questions to be answered, which
can help to reveal why SBFL is or is not helpful, and how to im-
prove it. Therefore, in our study, we revisit the performance of
SBFL by considering human factors to address the above points.
We recruited 207 participants to perform debugging on 17 faulty
programs. A platform, namely Mooctest, with better SBFL assis-
tance than the plug-in in [21] was implemented. It adopted Jones
et al.‚Äôs [12] idea to colorize the code according to their suspicious-
ness without changing their positions, such that the risk can be pin-
pointed but the integrity and readability of the program can still
be preserved. Each debugging task was associated with a set of
JUnit test cases that were executable, and the execution results of
these test cases were linked to the SBFL colorization. Our inves-
tigation was no longer solely on the helpfulness of the suggested
3The
plug-in hard-codes the SBFL ranking in a static XML Ô¨Åle,
which is not related to the given failed test case.
810ranking
list, but of the entire debugging tool. To learn the impact
from SBFL accuracy, we compared debugging with ‚Äúgood‚Äù (if the
faulty statement is ranked top) and ‚Äúbad‚Äù (otherwise) SBFL results.
More importantly, instead of simply reporting the performance of
debugging, we set our major focus as ‚Äúto identify the reasons and
give explanations to the observed phenomena‚Äù. Mooctest recorded
all of the operations of participants during their debugging process,
which were used for tracking and analyzing their focus of attention.
From the analysis, navigation patterns in debugging with SBFL and
the impact from SBFL on fault detection were investigated. We for-
mulated the following three research questions:
RQ1: Can Mooctest beneÔ¨Åt debugging? SpeciÔ¨Åcally, does
the accuracy of fault localization affect debugging efÔ¨Åciency?
RQ2: Is there any particular navigation pattern on source
code during debugging with SBFL? If yes, how does it affect
the efÔ¨Åciency and does it have any relation with the accuracy
of fault localization ? By addressing this question, we want
to Ô¨Ånd reasons for the observations in RQ1.
RQ3: Does the assistance from SBFL have any impact on
programmers‚Äô fault detection abilities?
4. EXPERIMENT
4.1 Mooctest - An on-line examination plat-
form with SBFL
We have developed Mooctest [19], an on-line examination plat-
form for a Coursera course on software testing, to provide all sup-
portive features. The subcomponent in Mooctest speciÔ¨Åcally for
SBFL is called MDebug, which consists of three major modules:
(1)Fault Localizer provides static source code analysis for con-
trol Ô¨Çow graph generation, dynamic coverage analysis for exe-
cution proÔ¨Åle construction, and risk analysis for suspiciousness
calculation. We borrowed Jones et al.‚Äôs [12] idea to colorize the
code according to their suspiciousness without changing their
positions, such that the risk can be pinpointed while the in-
tegrity and readability of the program can still be preserved.
Different from [12], we only colorized high-risk statements as
red, but distinguished their suspiciousness with different levels
of saturation: the higher the level, the greater the suspicious-
ness. As suggested by [21] that programmers are normally in-
terested in only several top-ranked statements, Fault Localizer
highlights the statements ranked as the top four or tied for the
top four. The formula used to calculate the suspiciousness can
be conÔ¨Ågured. In our experiment, Ochiai [1] was adopted.
(2)Operation Tracker records each operation during debugging,
e.g. Ô¨Åle opening, cursor movement, mouse clicking, Ô¨Åle modi-
Ô¨Åcation, and JUnit test execution, with a time stamp, such that
we can track a programmer‚Äôs operations to obtain more details
about his task completion.
(3)Log Analyzer receives the log Ô¨Åle and performs various anal-
yses to provide in-depth knowledge about the SBFL-assisted
debugging process.
The server of Mooctest assigned debugging tasks with an asso-
ciated JUnit test suite to the participants. It also speciÔ¨Åed whether
the SBFL feature was switched on. Participants were required to
login with their Mooctest accounts and run the MDebug client on
their local machines to perform the assigned tasks. During debug-
ging, Operation Tracker recorded all the necessary information in
the logs and sent them back to the server for marking and analyz-
ing. Figure 1 illustrates the Mooctest GUI for a debugging task
with the SBFL feature switched on, which is associated with the
testing results of the provided test suite.
Figure
1: The SBFL feature on Mooctest
4.2 Participants and program subjects
There were 207 participants from two classes involved in our ex-
periment where Class I and II had 97 and 110 participants, respec-
tively. They were 3rd-year college students in Computer Science
from our university, who were enrolled in our Coursera course on
software testing, a compulsory on-line subject. These students par-
ticipated in our experiment during an examination, which required
them to concentrate and deliver more reliable results than volun-
teers. Besides, they all had about two years of Java programming
experience. Their programming skills were diverse, which can help
to reduce the bias in our analysis.
In the experiment, we selected seven programs from classic Com-
puter Science algorithms, whose brief introduction is given in Ta-
ble 1. As compared with the two programs used by Parnin and
Orso [21], ours were smaller in size, but should have a more suit-
able level of difÔ¨Åculty for our student participants: comprehend-
ing these programs that have nested loops as well as complex data
structure and logic is non-trivial, but is still accomplishable. Be-
sides, these algorithms involved no domain expertise, which al-
lowed our analysis to focus on the impact from MDebug and code
comprehension.
Table 1: Program subjects
Program Description LOC T
ype
QuickSort Quick
sorting algorithm 35 S
HeapSort Heap
sorting algorithm 35 S
FindT
ripleFind
all triples with
sum=0in an integer array38 S
SubSequenceGet
the number of all
subsequences of a string17 S
T
icTacToe Noughts&crosses
game 500 M
Ele
vator An
elevator scheduler 238 M
BoatBombA
game of guessing the
‚Äúboat‚Äù in an nn grid164 M
ySmeans
single-Ô¨Åle program and Mrepresents multiple-Ô¨Åle program.
As shown in Table 1, there are four single-Ô¨Åle and three multiple-
Ô¨Åle programs, which partially indicate two levels of complexity in
debugging. For each program, we artiÔ¨Åcially seeded exactly one
fault on one code line to get a faulty version. We obtained 17 faulty
versions for all these program subjects. As compared with the two
faulty programs in [21] where each one was associated with only
811one
failed test case in descriptive way, each of our faulty programs
was associated with a set of executable JUnit test cases. Testing
results were different on these 17 versions and hence the SBFL
results varied as well. In contrast, the plug-in in [21] hard-coded
the SBFL ranking results in a static XML Ô¨Åle.
In order to provide a comprehensive analysis on the impact from
SBFL accuracy, we categorized these 17 versions according to their
SBFL results. First, we used ‚Äú hit‚Äù to indicate whether the faulty
statement sfis red with the highest saturation: hit= 1 if yes;
and 0 otherwise. For those cases where hit= 1, ‚Äú precision‚Äù
was then used to further distinguish the results with many false
positives from those with few. In our context, precision =1
1+N c,
where Ncis
the number of correct statements in red with the same
highest saturation as sf. The higher the precision, the better result
provided by SBFL. For the cases where hit= 0, i.e.sfdoes not
have red with the highest saturation, we considered the distance in
terms of the number of code lines, and data dependency between
sfand its nearest line in red with the highest saturation.
Finally, we categorized the 17 faulty versions into six groups,
as shown in Table 2. And the category of each faulty version is
shown in Table 3, which also gives the number of all test cases
(2ndcolumn) and the number of the failed test cases ( 3rdcolumn).
Table 2: Categorization strategy
hit=
1 hit=
0
p p
< dist
&&
dep=
1dist 
&&
dep=
0dist>
&&
dep=
1dist>
&&
dep=
0
A B C D E F
ypmeans
precision, dist represents code line distance, and depindicates
whether there is data dependency (1 means yes, and 0 means no). In our
experiment, =1=8 and=LOC 0:1.
Table 3: All faulty versions
F
aulty version T
est cases F
ailures Cate
gory
QuickSort_a 30 21 E
QuickSort_b 30 15 E
HeapSort_b 30 10 C
HeapSort_c 30 15 E
FindT
riple_a 12 6 E
FindT
riple_b 12 8 A
FindT
riple_c 12 3 A
SubSequence_a 39 22 A
SubSequence_b 39 20 A
T
icTacToe_a 38 24 F
T
icTacToe_b 38 21 D
T
icTacToe_c 38 29 D
Ele
vator_a 40 9 D
Ele
vator_b 40 20 E
BoatBomb_a 38 21 C
BoatBomb_b 38 13 B
BoatBomb_c 38 17 B
4.3
Experimental procedure
A tutorial on SBFL theory and the MDebug tool were Ô¨Årst given
to all participants to make sure that they fully understood the infor-
mation provided by the tool. Then, these two classes attended an
examination. Participants from each class were required to perform
two debugging tasks within 90 minutes, where one was on a single-
Ô¨Åle program and the other one was on a multiple-Ô¨Åle program. As a
reminder, participants were told that they would be scored accord-
ing to whether they Ô¨Åxed the bugs and passed all given test cases,
rather than their time consumption. The timeframe of 90 minutes
was imposed to avoid endless debugging. Class I was required to
debug the single-Ô¨Åle program by hand but to debug the multiple-Ô¨Åleprogram with SBFL. Class II had the reversed set-up. The tasks
were randomly assigned to each participant. Table 4 summarizes
the experiment design.
Table 4: Experiment design
Class
I Class
II
Single-Ô¨Åle
program without
SBFL with
SBFL
Multiple-Ô¨Åle
program with
SBFL without
SBFL
5.
RESULTS AND ANALYSIS
In this section, we will present our empirical results to address
the three research questions4.
5.1 RQ1: Can Mooctest beneÔ¨Åt debugging?
To address this question, we analyzed the task completion rate
and completion time to measure the effectiveness and the efÔ¨Åciency
of debugging, respectively. For each debugging task, i.e. to repair
one program, with or without the SBFL assistance, completion rate
Rcis deÔ¨Åned as Sc=Sall, where Scis the number of participants
who have successfully Ô¨Åxed the bug within the required timeframe;
andSallis the number of all participants involved in this task. For
each individual participant who has correctly Ô¨Åxed the bug within
the required timeframe, completion time Tcrepresents the time cost
for successfully repairing the program.
(1)Completion rate Rc. Intuitively, we were expecting that with
the SBFL assistance, Groups A and B, where the faulty state-
ment has been successfully pinpointed in red with the highest
saturation, should have increased Rc. Since Group A has even
better accuracy than Group B, the improvement of Group A
should be greater than that of Group B. For Groups C, D, E
and F, the improvement of Rc, if there is any, should be smaller
than those of Groups A and B.
Figure 2 demonstrates this comparison for each group, where
the results for Groups A and B comply with our expectation
that both of them have obvious improvement on Rcwhen SBFL
is switched on. Rcin Groups A and B increase by 14.8%
and 11.8%, respectively, which is also consistent with the in-
tuition. On the other hand, for Groups C, D, E and F which
do not correctly pinpoint the fault, the assistance from SBFL
hardly shows any advantage: Groups E and F demonstrate sim-
ilarRcof debugging with and without SBFL; and Groups C
and D even have the Rcdecrease with SBFL. Although we
cannot foresee this result before the experiment, the decreased
Rcis still understandable; it is very likely that the bad SBFL
results have side-tracked programmers, and hence brought in
noise during the process. Concerning the comparison among
Groups C, D, E and F, there is no obvious relationship ob-
served. Groups C and D do not show any advantage over
Groups E and F, which indicates that a small distance between
the fault and its nearest highlighted statement seems not help-
ful to mitigate the difÔ¨Åculties in debugging with inaccurate
fault localization results. By viewing the comparison results
of ‚ÄúGroup C v.s. Group D‚Äù and ‚ÄúGroup E v.s. Group F‚Äù, data
dependency between the faulty and the highlighted statements
tends to be not useful either.
In summary, it is reasonable to conclude that under certain con-
ditions, the SBFL assistance in Mooctest does help to improve
the completion rate. One dominant factor is the ‚Äú hit‚Äù value of
4All
the raw data and analysis are available on
http://mooctest.net/academic/mdebugICSE16.
812Comple/g415on Rate
Without SBFL With SBFL 100%
 75%
  50%
 25%
  0%Figure
2: Completion rate for each group of faulty programs
the SBFL result: only when the faulty statement is correctly
pinpointed (i.e. ranked or tied at the top of the list), does Rc
tend to increase with the SBFL assistance. The low precision
(i.e. many statements are tied at the top) may slightly weaken
this advantage; while the small line distance and data depen-
dency may not be able to compensate for the impact from the
inaccurate localization results. These observations verify the
explanation for ‚Äúno impact from the accuracy‚Äù and the feasi-
bility of the suggested ‚Äúabsolute ranking‚Äù in [21]. Therefore,
in the following discussion, the six groups of faulty versions
will be merged into two, namely Accurate Group Gaccwhere
hit= 1 (including Groups A and B), and Inaccurate Group
Ginwhere hit= 0(including Groups C, D, E and F).
(2)Completion time Tc. In terms of the completion time, we want
to investigate whether Mooctest can be helpful in decreasing
the time cost to successfully Ô¨Åx a bug.
Figure 3(a) demonstrates the comparison of average Tcval-
ues. The Ô¨Årst and second bars are for Gaccwithout and with
SBFL, respectively, where the average Tcwith SBFL (631.9
sec) is slightly longer than that without SBFL (578.2 sec). The
third and fourth bars are for Ginwithout and with the SBFL
feature, respectively, where the average Tcincreases by 48.5%
from the cases without SBFL (606.2 sec) to the cases with it
(900.3 sec). If we compare the helpfulness of the SBFL feature
between GaccandGin(shown in the second and the fourth
bars, respectively), it can be found that the average TcofGin
is about 42.3% higher than that of Gacc. However the average
time cost of these two groups is actually very similar (shown
in the Ô¨Årst and the third bars) when debugging by hand. These
results imply that SBFL prolongs the debugging process, espe-
cially in inaccurate cases.
Single-Ô¨Åle Programs
Accurate/
NoSBFLAccurate/
WithSBFLInaccurate/
NoSBFLInaccurate/
WithSBFLTime(s)1000
 750
  500
 250
     0
(a)
single-Ô¨Åle
Mul/g415ple-Ô¨Åle Programs
Accurate/
NoSBFLAccurate/
WithSBFLInaccurate/
NoSBFLInaccurate/
WithSBFLTime(s)1200
 900
  600
 300
     0 (b)
multiple-Ô¨Åle
Figure 3: Average completion time Tc
Similar results can be concluded from the comparison among
multiple-Ô¨Åle programs in Figure 3(b). For Gacc, the SBFL fea-ture helps to slightly reduce the average Tc(the Ô¨Årst and the
second bars). Regarding Gin, similar to the above results for
the single-Ô¨Åle programs, the average TcsigniÔ¨Åcantly increases
by 30.1% with the SBFL feature (the third and the fourth bars).
Our 2-sample T-tests (both 2-tailed and 1-tailed) shown in Ta-
bles 5 and 6 further verify the above comparison5.
Table 5: T-test on Tcfor single-Ô¨Åle programs
X Y H0 H1 p-v
alue Accept
Gacc
NoSBFLGacc
SBFLX=YXÃ∏=Y 0.63204 H0
Gin
NoSBFLGin
SBFLXYX
<Y 0.00543 H1
Gacc
SBFLGin
SBFLXYX
<Y 0.01423 H1
T
able 6: T-test on Tcfor multiple-Ô¨Åle programs
X Y H0 H1 p-v
alue Accept
Gacc
NoSBFLGacc
SBFLX=YXÃ∏=Y 0.86358 H0
Gin
NoSBFLGin
SBFLXYX
<Y 0.0362 H1
Gacc
SBFLGin
SBFLX=YXÃ∏=Y 0.81623 H0
In
summary, we can conclude that generally speaking, SBFL
may not be helpful in reducing the time cost of debugging,
even for those with accurate localization results. Concerning
the cases which give inaccurate results, SBFL may even lead to
signiÔ¨Åcant time increase. These observations are surprisingly
interesting and somehow counter to our intuition. At Ô¨Årst, we
conjectured that the longer average Tcmay be caused by the
increased Rcwhich implies that some uncompleted debugging
tasks become resolved with SBFL, and these tasks normally
require higher time cost. However, by viewing the detailed
data, we reckoned that the involved high time cost from these
difÔ¨Åcult tasks should not be the major contribution to the in-
crease of average Tcbecause (1) actually there were only two
groups of programs having increased Rc, while for the other
four groups whose Rcdecreased, we still observed, even more
clearly, increased Tc; (2) in all groups including A and B, not
just the top high Tcwith SBFL were higher than those without
it, the cases of low Tchad a similar tendency. Therefore, in
order to Ô¨Ånd out the explanations, we further investigated the
following research questions.
5.2 RQ2: Is there any navigation pattern in
SBFL-assisted debugging?
In order to gain insight into what was happening during the SBFL-
assisted debugging process, and Ô¨Ånd the reason for the above ob-
servations, we tracked and analyzed participants‚Äô focus of attention,
from which two navigation patterns were observed, as follows.
(1)First scan pattern. As introduced in Section 2.1, Uwano et
al. [33] identiÔ¨Åed a ‚Äúscan‚Äù pattern in static code review, where
they found that the duration of ‚Äúscan‚Äù presented a positive cor-
relation with the efÔ¨Åciency of review. This observation is rea-
sonable since a careful ‚Äúscan‚Äù shall give the reviewer a better
understanding about the code and hence will be helpful in iden-
tifying the fault more quickly.
Motivated by [33], we are curious as to whether there also ex-
ists a similar scan pattern in debugging activities with SBFL.
5All
hypothesis tests in this paper is set to be under the signiÔ¨Åcant
level of 0.05.
813Dif
ferent from [33] in which eye-movement was used to track
reviewers‚Äô focus of attention, we used participants‚Äô operations
on the screen such as cursor movement, mouse clicking, Ô¨Åle
opening, and code modiÔ¨Åcation to achieve this goal. In fact,
neither eye movements nor these operations can be 100% pre-
cise to reÔ¨Çect a programmer‚Äôs focus of attention. However, as
shown in both Psychology and Computer Human Interaction
studies [5, 8, 11], operations such as cursor and mouse move-
ments can be eligible surrogates with good practicality.
Figures 4 and 5 give two sample navigation patterns on the
source code without and with SBFL, respectively, where the
horizontal axis records the elapsed time and the vertical axis
indicates the code line. The red dot line (in Figure 5 only) and
the blue dashed lines (in both Figures 4 and 5) that run through
the whole debugging process represent the statements colored
as red with the highest saturation (presented to programmers
with the SBFL feature) and the actual faulty locations (hidden
to programmers during the process), respectively. The black
line segments record the duration of the programmer‚Äôs focus
on the corresponding code lines. And the orange points on
some black line segments indicate modiÔ¨Åcations on these lines
by the programmer.
Time(s)
0 100 200 300 400 500 600 700 800 900 1000Line numbers0
  5
  10
  15
  20
  25
  30
  35
Figure
4: Code navigation without SBFL
Time(s)
0 200 400 600 800 1000 1200 1400Line numbers0
  5
  10
  15
  20
  25
  30
  35
Figure
5: Code navigation with SBFL
Figure 4 illustrates that debugging without SBFL has a sim-
ilar pattern as static code review in which programmers nor-
mally have a Ô¨Årst scan followed by several rounds of navigation
along through all the code lines sequentially and Ô¨Ånally reach
a particular area and Ô¨Åx the bug. For debugging with SBFL,
shown in Figure 5, the code navigation presents a different pat-tern, i.e., programmers no longer iteratively follow the natu-
ral sequential order of the code lines. Instead, they frequently
switch their focus of attention between the red lines with the
highest saturation and the others. Interestingly, we have found
that the ‚ÄúÔ¨Årst scan‚Äù pattern seems to be preserved in debugging
with SBFL, although it becomes less obvious. In the theory
of SBFL, programmers are expected to inspect the code lines
along the given ranking list. However, we found that regardless
of the accuracy of the SBFL results, there was almost nobody
who directly jumped into the red line with the highest satura-
tion at their Ô¨Årst visit on the code. Instead, a ‚ÄúÔ¨Årst scan‚Äù was
performed (which may not be on the entire code), then was
followed by the inspection of the red line with the highest satu-
ration. However, the duration of this ‚ÄúÔ¨Årst scan‚Äù varied across
different participants: some were longer while others could be
very short such that only a few lines were skimmed within a
short period of time, as the one in Figure 5, circled in blue.
This observation is not surprising. However, we are more in-
terested in whether the duration of this ‚ÄúÔ¨Årst scan‚Äù (denoted as
T1) affects Ô¨Ånal debugging efÔ¨Åciency, in the similar way that
‚Äúscan‚Äù affects static code review [33].
Different from [33], we did not investigate the relationship be-
tween the absolute T1andTc(the completion time). Instead,
we deÔ¨Åned R1=T1=Tcto evaluate the portion of the effort that
a programmer put in his ‚ÄúÔ¨Årst scan‚Äù out of his total effort in Ô¨Åx-
ing the bug. We wanted to investigate the impact of R1onTc
for debugging with SBFL. We deÔ¨Åned, for SBFL-assisted de-
bugging, cases where R1<=10% as ‚Äúshort scan‚Äù while cases
where R1>10% as ‚Äúlong scan‚Äù. As a consequence, all par-
ticipants‚Äô debugging with SBFL can be split into two parts,
namely, cases with ‚Äúshort scan‚Äù and with ‚Äúlong scan‚Äù.
For single-Ô¨Åle programs belonging to the inaccurate group Gin,
T-tests comparing Tcamong debugging with ‚Äúlong scan‚Äù, ‚Äúshort
scan‚Äù and ‚Äúno SBFL assistance‚Äù, are presented in Table 7.
Table 7: T-test on Tcfor different R1(Gin)
X Y H0 H1 p-v
alue Accept
SBFL
LongSBFL
ShortXYX
<Y 0.01563 H1
NoSBFLSBFL
ShortXYX
<Y 0.00204 H1
SBFL
LongNoSBFL X=YXÃ∏=Y 0.61299 H0
T
able 7 shows that Tcwith ‚Äúshort scan‚Äù tends to be signiÔ¨Å-
cantly larger than those with ‚Äúlong scan‚Äù, as well as those with-
out SBFL; while ‚Äúlong scan‚Äù and debugging without SBFL
produce no signiÔ¨Åcant difference on their Tc. This implies
that among all the Tcof SBFL-assisted cases, smaller ones
(i.e. cases with higher debugging efÔ¨Åciency) tend to appear
with ‚Äúlong scan‚Äù; while higher ones (i.e. cases with lower ef-
Ô¨Åciency) usually belong to debugging with ‚Äúshort scan‚Äù. The
increase on Tcwith SBFL in Ginis very likely due to the ex-
istence of many ‚Äúshort scan‚Äù cases. On the other hand, ‚Äúlong
scan‚Äù could be helpful in reducing the gap between debugging
with and without SBFL.
Figure 6(a) depicts the consistent results on the average Tcin
a more vivid manner, where the leftmost bar is for debugging
without SBFL, and the second, third and fourth bars are for the
SBFL-assisted debugging in all cases, ‚Äúshort scan‚Äù and ‚Äúlong
scan‚Äù, respectively. The average Tcfor debugging with ‚Äúshort
scan‚Äù is about 59.9% and 76.9% higher than those for debug-
ging with ‚Äúlong scan‚Äù and without SBFL, respectively.
8141100
 825
  550
275
     0Inaccurate Group 
NoSBFL-allTime(s)
WithSBFL-all WithSBFL-short WithSBFL-long(a)Gin
660
 495
  330
 165
     0Accurate Group 
NoSBFL-allTime(s)
WithSBFL-all WithSBFL-short WithSBFL-long (b)Gacc
Figure
6: Average Tcwith different R1
And for Gacc, 2-tailed T-tests did not indicate any signiÔ¨Åcant
difference. The p-values for the same comparisons as Rows
1, 2, 3 in Table 7 become 0.84481, 0.6651, and 0.58687, re-
spectively. However the average Tccomparison shown in Fig-
ure 6(b) reveals some slight difference, which is similar to Gin
but less obvious. It can be found that for Gacc, the average
Tcfor ‚Äúshort scan‚Äù (the third bar) is about 10.0% and 13.9%
higher than those for the ‚Äúlong scan‚Äù (the fourth bar) and ‚Äúde-
bugging without SBFL‚Äù (the leftmost bar), respectively.
Similar results can also be found in multiple-Ô¨Åle programs.
In summary, quickly focusing on the highlighted lines with-
out sufÔ¨Åcient Ô¨Årst code scan could be harmful to the efÔ¨Åciency
of debugging. And the situation could be even worse when
the SBFL results are inaccurate. This observation is consistent
with the one in [33]. This is most likely because regardless
of SBFL-assisted debugging or manual static code review, the
success of fault detection shall always eventually rely on hu-
man intelligence. Reading and comprehending the source code
are inevitable to Ô¨Ånally detect and correct the faults. A more
careful scan on the code will generally lead to a better under-
standing of the program, and hence a more efÔ¨Åcient debugging.
This observation makes us re-examine the original motivation
of SBFL, one of whose goals is to help programmers quickly
enter the most suspicious code area when given a program to
debug. However, as discussed above, the debugging process
may not be accelerated in this way. This will be further dis-
cussed in Section 6.
(2)Follow-up browsing pattern. As shown in Figure 5, from the
Ô¨Årst visit on the red lines with the highest saturation to the com-
pletion of debugging, participants tend to switch their focus of
attention between the highlighted lines and the others. How-
ever, we also found that they actually spent most of the time
outside rather than on the red lines with the highest saturation.
Here, we use single-Ô¨Åle programs as illustrations.
LetT2denote the total time that a programmer spends on state-
ments outside the red lines with the highest saturation after his
Ô¨Årst scan, and R2denote T2=(Tc T1). Figure 7 gives the box-
plot diagrams for the distribution of R2inGaccandGin. In
Gin(the right box) about 98.0% of the participants have an
R2over 50%, and 85.7% of the participants even have an R2
higher than 70%. The average R2is as high as 84.8%. For
Gaccin the left box, the R2values are relatively lower. How-
ever, 75% of the participants still have an R2over 50%. The
average R2for the accurate group is 63.9%.
In other words, no matter whether or not the SBFL result is
accurate, programmers normally tend to spend most of their
rest debugging time (after the Ô¨Årst scan) outside the red lines
with the highest saturation. Inaccurate SBFL results normally
lead to more time cost outside the highlighted lines. This phe-
0.250.500.751.00
AccurateInaccurate
GroupRateFigure
7:R2in the follow-up browsing
nomenon implies that no matter how precisely we pinpoint the
buggy lines, ‚Äúwasting time‚Äù on a wider range of the source
code is generally indispensable, and even fundamental, since
it is helpful in comprehending the context. Obviously, simply
relying on the disjoint colored code lines cannot be helpful to
collect such information.
This raises doubts about another goal of SBFL that is to pin-
point the most suspicious statements and make programmers
concentrate on these risky lines without wasting time on other
‚Äúsafe‚Äù lines. Based on our observation, even if we assumed
that the lines outside the pinpointed ones were really safe, there
was still a large portion of time spent on these safe areas. Then,
does it mean that highlighting the risky code to save inspection
efforts on those ‚Äúsafe‚Äù statements is infeasible during practical
debugging activities? Again, we will discuss this in Section 6.
So far, it seems that we have found some possible explanations
to the decrease in debugging efÔ¨Åciency with SBFL. In summary,
no matter whether programmers claim that they are reluctant to
follow the SBFL results, the colorization will affect their debug-
ging behaviors, more or less. Some may trust the SBFL result very
much and would like to quickly start to check the pinpointed lines;
while others may not feel comfortable to check these lines until
they have a longer scan and better understanding on the code. Gen-
erally speaking, the former cases have shown an obvious harm-
fulness to the efÔ¨Åciency of debugging, especially when provided
with inaccurate SBFL results. After the Ô¨Årst visit on the pinpointed
code lines, programmers will switch their focus of attention be-
tween these lines and others. Most of their time has actually been
spent outside these lines, in order to comprehend the context and
hence to detect and correct the fault. As a consequence, the critical
factor that reduces the helpfulness of SBFL may not be the ones
that we normally suspected, such as the accuracy and reliability of
SBFL results, or people‚Äôs being reluctant to follow the localization
results. The critical factor is most likely residing in the interference
between the mechanism of automated fault localization and the ac-
tual assistance needed by programmers in debugging. Based on
these observations, we now delve further to investigate the impact
from SBFL on fault comprehension and identiÔ¨Åcation.
5.3 RQ3: Will SBFL affect fault detection?
It has been widely accepted by the SBFL research community
that there is no perfect bug detection. Simple inspection of a fault is
not sufÔ¨Åcient for immediately detecting it (i.e. understand and iden-
tify the fault). However, there is a lack of evidence about whether
SBFL has any impact on fault detection. Thus in this study we
analyzed participants‚Äô operation logs to address this question.
For both debugging with and without SBFL, we counted the fre-
quency of each participant‚Äôs visits on the real faulty line without
815detecting
it (denoted as Vf), and the average duration of all his vis-
its on the faulty line (denoted as Df).
Figure 8(a) demonstrates the average Vffor debugging on pro-
grams of GaccandGin, without and with SBFL, which conÔ¨Årms
that regardless of the debugging method, there is generally no ‚Äúper-
fect bug detection‚Äù. For the same debugging task, debugging with
SBFL tends to have a higher frequency: for programs in Gaccthe
average Vfwith SBFL (the second bar) is 13.2% higher than that
of debugging by hand (the Ô¨Årst bar). When the SBFL is inaccurate
(Gin), this difference is as high as 40.5% (the third and the fourth
bars for debugging without and with SBFL, respectively). These
imply that when using SBFL, programmers tend to revisit the bug
more times in order to detect it.
Average Visi/g415ng Frequency
Accurate/
NoSBFLAccurate/
WithSBFLInaccurate/
NoSBFLInaccurate/
WithSBFLFrequency15
 11.25
  7.5
 3.75
     0
(a)
Average Vf
Average Visi/g415ng Dura/g415on
Accurate/
NoSBFLAccurate/
WithSBFLInaccurate/
NoSBFLInaccurate/
WithSBFLTime(s)6
4.5
  3
1.5
     0 (b)
Average Df
Figure 8: Fault revisit
A similar tendency can be found on Df, shown in Figure 8(b).
ForGacc, the average Dftends to be slightly longer when using
SBFL (4.5 sec, in the second bar) than by hand (3.9 sec, in the Ô¨Årst
bar). Similarly, for Gin, the average Dfwith SBFL (the fourth bar)
is about 2 sec longer than that without it (3.2 sec in the third bar).
The above comparison results are consistent with the one in RQ1
that debugging with SBFL is slower than debugging by hand. With
these results, we now have more understanding about the average
ability that people detect a fault, and have learnt that the fault re-
visit frequency and the average duration on these visits may in-
crease when the SBFL feature is switched on. These phenomena
appear to be more obvious when the SBFL result is inaccurate. It is
very likely because SBFL may confuse programmers in their code
comprehension and slightly weaken their bug detection abilities.
5.4 Threats to validity
First, we have used programmers‚Äô operations such as cursor move-
ment, mouse clicking, Ô¨Åle opening and changing, to represent their
focus of attention on the code. However, this information may not
always be precisely reÔ¨Çecting the real situation. Another way to
achieve this goal is to use eye-movement. However, as shown in
previous studies [5,8,11], both eye-movement and operations con-
tain noise, and the latter one can be eligible surrogates with good
practicality. Actually, the operations may contain less noise than
eye-movement in our context. Based on our observation, partici-
pants may randomly shift their eye-attention to any place within or
even outside the screen, but not really for the purpose of inspecting
the code. For example, they may frequently check the time, espe-
cially when the 90 minutes were about to run out. On the other
hand, we observed that our participants always moved their cursor
along each code line that they were inspecting. Therefore, we feel
that in our experiment, tracking the operations may provide the data
more closely approximate to the fact.
Secondly, although we recruited many more participants in our
experiment than compared to prior work, these participants were
all students with very similar education background. There were
no professional programmers with industrial experience involved.However, our debugging tasks were neither from industry projects
nor involved any domain knowledge. As a consequence, these stu-
dents should be eligible representatives in our experiment [25]. Ac-
tually, students may even provide more reliable results than volun-
teers from the industry because students participated in our exper-
iment in the form of an examination, which required them to con-
centrate. Besides, these students from two classes have presented
diverse Java programming skills, which can help to reduce the bias
in our analysis.
The Ô¨Ånal threat to validity is in regards to the representative-
ness of the program subjects, which were small-size programs with
seeded faults. Actually, as discussed in Section 4.2, these pro-
grams should be suitable for our participants. Besides, we have
conÔ¨Årmed that the seeded faults did not include those ‚Äútoo obvious
or stupid‚Äù, such as a=a+bbeing mutated to a=a. More im-
portantly, these programs did not involve any domain knowledge
or advanced programming skills, thus could be proper to investi-
gate the impact from an SBFL tool and code comprehension. As
a reminder, though we adopted both single-Ô¨Åle and multiple-Ô¨Åle
programs to represent two levels of difÔ¨Åculty in debugging, we ac-
tually did not observe much difference between these two types of
programs. Thus, in our future studies, we will explore programs
with more levels of debugging difÔ¨Åculty, to investigate how differ-
ent levels of difÔ¨Åculty affect the results.
6. INSIGHTS AND SUGGESTIONS
6.1 Actual debugging process
In Section 5, RQ1 Ô¨Årst reveals a decrease in the efÔ¨Åciency of
debugging with SBFL. By investigating RQ2 and RQ3, we have
found some possible explanations. On one hand, by quickly enter-
ing the area of the suggested suspicious code, a programmer may
miss a good global understanding about the software. He may have
to waste more time to compensate for being unfamiliar with the
source code. Concerning the cases where the suggestion is actually
misleading, even more extra efforts may be required. In contrast,
a good ‚ÄúÔ¨Årst scan‚Äù, which has been regarded as an efÔ¨Åcient pattern
for programmers in static code review [33], turns to be also essen-
tial in SBFL-assisted debugging. On the other hand, continuously
and exclusively focusing on the pinpointed code lines is generally
unrealistic. Browsing the context is more necessary in detecting
the fault. Moreover, suggestions from SBFL may interfere with
programmers fault detection.
These observations conÔ¨Çict with the motivation of automated
fault localization, which helps localize the most suspicious code
and save the time that programmers waste on the irrelevant code.
It is generally believed that debugging involves fault localization
(denoted as JA), as well as code comprehension and fault correc-
tion (denoted as JB) [21]. The total cost of debugging is decided
by the costs of these two tasks. Automated fault localization aims
to reduce the time cost and human involvement in JAby providing
programmers with some suggested fault locations, and expects that
such saving can lead to the Ô¨Ånal improvement on the total debug-
ging efÔ¨Åciency. However, this expectation has an assumption that
JAandJBare independent and performed sequentially in debug-
ging, such that saving on JAby SBFL can lead to saving on the
total cost. But in practice, these two tasks are tightly coupled and
there is no clear boundary between them. Thus, it could be very
likely that saving on JAhas some impact on JB, such that Ô¨Ånal de-
bugging efÔ¨Åciency is not changed as expected. In the following two
subsections, we will discuss some possible solutions to the conÔ¨Çict
between the motivation of automated debugging and the real-life
situation.
8166.2Different fault localization techniques for
different levels of debugging
Although there are some opinions that statistical fault localiza-
tion (such as SBFL) may be helpful in small-scale programs, but
not suitable for large-scale systems where the small percentage of
the to-be-inspected code will result in a large number of statements,
we are holding a different view.
Based on the above analysis in Section 5, we conjecture that such
automated debugging techniques are not actually helpful to small-
scale programs, where the fault localization is always performed
on statement level. In contrast, these techniques may be useful in
large-scale systems, however, not for the purpose of locating the
faulty statement, but the faulty module . These suggestions are
not only because a coarser granularity can signiÔ¨Åcantly reduce the
absolute number of the to-be-inspected entities for large-scale sys-
tems, but are also supported by the following evidences.
(1) Fault may not be localizable in statement level. Thung et al.
[32] observed that many real-life faults span across multiple
code lines; some even involve multiple methods. This fact
makes locating a real-life fault very difÔ¨Åcult. On the other
hand, we observed that it is unlikely that one fault could span
across multiple modules. In other words, it is more feasible to
locate a faulty module than to locate a faulty statement.
(2) In locating the faulty statement for a unit program, due to the
nature of the control-dependency, it is impossible to always
rank the faulty statement at top. Distributions of ep(the num-
ber of passed test cases that cover the statement) and ef(the
number of failed test cases that cover the statement) could be
very different in the entry statement, a loop predicate, a branch
predicate, an assignment statement and the exit statement, and
such difference is not controllable by test cases but due to the
nature of their roles in the control Ô¨Çow graph. Any of these
statements could be faulty, thus a general method cannot guar-
antee to pinpoint the fault all the time. In contrast, the topo-
logical graph in high-level of a large system could be different,
especially those following the principles of high-cohesion and
low-coupling. There are fewer tight control-dependencies. The
distribution of epandefon these modules are relatively eas-
ier to control. Hence there is much higher chance to make an
accurate diagnose [4].
(3) Due to the high-cohesion, a module is usually self-descriptive.
For example, it is very easy to understand the major func-
tions of Class Postman in a PostOfÔ¨Åce system, without learning
too much about other classes, especially when there are well-
written JavaDoc. But given a statement like ‚Äúfor (int i = 0; i <
N; i++)‚Äù, nobody can comprehend it immediately. As a con-
sequence, programmers can quickly focus on the pinpointed
module to start identifying the fault. The time saved by the
fault localization technique has much higher chance to result
in a total saving on the efÔ¨Åciency.
Therefore, we propose to Ô¨Årst adopt statistical fault localization
on large-scale systems to isolate the suspicious module. Then, for
each suspicious module, a more sophisticated method like program
slicing is suggested, which is not only more accurate, but also in-
formative for code comprehension.
6.3 Improving the practicality
Based on the above discussion, we now summarize our sugges-
tions in improving the practicality of automated fault localization.
First, apply the right technique on the right scenario. As sug-
gested above, to achieve a better debugging performance, a smart
combination of various fault localization techniques is necessary.Secondly, accuracy still matters. It is always important not to
mislead programmers. This can be done by designing better al-
gorithms, performing program refactoring, or adjusting test suites
[39]. Some researchers also proposed to predicate the accuracy be-
fore really utilizing the localization results [17].
Thirdly, incorporate effective code comprehension assistance. In-
tegration of automated debugging and code comprehension assis-
tance shall be helpful to provide better practicality.
Finally, build a platform that can provide the features and infor-
mation really needed by programmers in different scenarios.
7. CONCLUSIONS AND FUTURE WORKS
Although not many studies in automated debugging have taken
human factors into account, it is undeniable that human involve-
ment plays a critical role in this activity. In this study, we revisited
the helpfulness of Spectrum-Based Fault Localization, by examin-
ing programmers‚Äô actual performance in debugging with its assis-
tance. Our experiment involving 207 participants and 17 debug-
ging tasks indicated that an accurate localization result was helpful
in terms of completing the debugging task. However, regardless
of the accuracy, SBFL was not helpful in terms of improving the
efÔ¨Åciency of debugging. And an inaccurate SBFL result may lead
to an even longer debugging process. To identify the reason, we
tracked and analyzed programmers‚Äô focus of attention, from which
source code navigation patterns (namely, ‚ÄúÔ¨Årst scan‚Äù and ‚Äúfollow-
up browsing‚Äù) and SBFL‚Äôs impact on bug detection were identiÔ¨Åed.
We found that (1) a slow and careful ‚ÄúÔ¨Årst scan‚Äù that provides a bet-
ter understanding about the code is favorable for a quicker debug-
ging process; (2) browsing on the context of the pinpointed code is
essential; and (3) a natural sequential code scan tends to be more
efÔ¨Åcient than an SBFL-guided one, in detecting a fault.
These observations reconÔ¨Årmed the importance of code compre-
hension in automated debugging. More importantly, they indicated
that the decrease in debugging efÔ¨Åciency was most likely due to
the interference between the mechanism of automated fault local-
ization and the actual assistance needed by programmers in debug-
ging. To resolve the interference, we proposed a conjecture that it
is not the problem of the technique itself. Instead, it should be due
to an inappropriate application of the technique. And we further
gave some suggestions on how to properly perform automated de-
bugging, in order to have an actual improvement on the efÔ¨Åciency.
In our future studies, we will focus on improving the practicality
of automated debugging from the suggested directions, such as the
applicability of a technique, an integration of automated debugging
and code comprehension assistance, a better debugging platform,
etc. And their actual helpfulness to programmers will be examined.
Acknowledgment
This work is partially supported by the National Key Basic Re-
search and Development Program of China (973 Program
2014CB340702) and the National Natural Science Foundation of
China (Grant No. 61572375, 61170067, 61373013, 91418202,
61472178).
8. REFERENCES
[1] R. Abreu, P. Zoeteweij, and A. J. C. van Gemund. An
evaluation of similarity coefÔ¨Åcients for software fault
localization. In Proceedings of the 12th PaciÔ¨Åc Rim
International Symposium on Dependable Computing , pages
39‚Äì46, Riverside, USA, 2006.
[2] R. Bednarik. Expertise-dependent visual attention strategies
develop over time during debugging with multiple code
817representations. International
Journal of Human-Computer
Studies, 70(2):143‚Äì155, 2012.
[3] D. Binkley, M. Davis, D. Lawrie, J. I. Maletic, C. Morrell,
and B. Sharif. The impact of identiÔ¨Åer style on effort and
comprehension. Empirical Software Engineering ,
18(2):219‚Äì276, 2013.
[4] C. Chen, H.-G. Gross, and A. Zaidman. Improving service
diagnosis through increased monitoring granularity. In
Proceedings of the 7th IEEE International Conference on
Software Security and Reliability , pages 129‚Äì138,
Gaithersburg, Maryland, USA, 2013.
[5] M. Chen and V. Lim. Eye gaze and mouse cursor relationship
in a debugging task. In Proceedings of HCI International
Posters‚Äô Extended Abstracts, pages 468‚Äì472. Springer, 2013.
[6] M. E. Crosby and J. Stelovsky. How do we read algorithms?
a case study. Computer , 23(1):25‚Äì35, 1990.
[7] M. A. Francel and S. Rugaber. The value of slicing while
debugging. Science of Computer Programming,
40(2):151‚Äì169, 2001.
[8] J. Freeman, R. Dale, and T. Farmer. Hand in motion reveals
mind in motion. Frontiers in Psychology , 2:59, 2011.
[9] J. D. Gannon. Human factors in software engineering.
Computer, 12(12):6‚Äì7, 1979.
[10] C. Gouveia, J. Campos, and R. Abreu. Using html5
visualizations in software fault localization. In Proceedings
of the First IEEE Working Conference on Software
Visualization, pages 1‚Äì10, Eindhoven, Netherlands, 2013.
[11] J. Huang, R. W. White, and S. Dumais. No clicks, no
problem: Using cursor movements to understand and
improve search. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems , pages 1225‚Äì1234,
Vancouver, BC, Canada, 2011.
[12] J. A. Jones, M. J. Harrold, and J. Stasko. Visualization of test
information to assist fault localization. In Proceedings of the
24th International Conference on Software Engineering ,
pages 467‚Äì477, Florida, USA, 2002.
[13] A. Ko, B. Myers, M. Coblenz, and H. Aung. An exploratory
study of how developers seek, relate, and collect relevant
information during software maintenance tasks. IEEE
Transactions on Software Engineering , 32(12):971‚Äì987,
2006.
[14] A. J. Ko and B. A. Myers. Designing the whyline: a
debugging interface for asking questions about program
behavior. In Proceedings of the SIGCHI conference on
Human factors in computing systems , pages 151‚Äì158,
Vienna, Austria, 2004.
[15] B. Korel and J. Laski. Dynamic program slicing. Information
Processing Letters, 29(3):155‚Äì163, 1988.
[16] J. Lawrance, C. Bogart, M. Burnett, R. Bellamy, K. Rector,
and S. Fleming. How programmers debug, revisited: An
information foraging theory perspective. IEEE Transactions
on Software Engineering , 39(2):197‚Äì215, 2013.
[17] T.-D. B. Le, D. LO, and F. Thung. Should i follow this fault
localization tool‚Äôs output? automated prediction of fault
localization effectiveness. Empirical Software Engineering,
pages 1‚Äì38, 2015.
[18] C. Lewis, Z. Lin, C. Sadowski, X. Zhu, R. Ou, and E. J.
Whitehead. Does bug prediction support human developers?
Ô¨Åndings from a google case study. In Proceedings of 35th
IEEE/ACM International Conference on Software
Engineering, pages 372‚Äì381, San Francisco, USA, 2013.[19] Mooctest. http://mooctest.net/.
[20] L. Naish, H. J. Lee, and K. Ramamohanarao. A model for
spectra-based software diagnosis. ACM Transactions on
Software Engineering and Methodology , 20(3):11:1‚Äì11:32,
2011.
[21] C. Parnin and A. Orso. Are automated debugging techniques
actually helping programmers? In Proceedings of the
International Symposium on Software Testing and Analysis ,
pages 199‚Äì209, Toronto, Canada, 2011.
[22] D. Piorkowski, S. Fleming, C. ScafÔ¨Ådi, L. John, C. Bogart,
B. John, M. Burnett, and R. Bellamy. Modeling programmer
navigation: A head-to-head empirical evaluation of
predictive models. In Proceedings of 2011 IEEE Symposium
on Visual Languages and Human-Centric Computing , pages
109‚Äì116, Pittsburgh, PA, USA, 2011.
[23] P. Rodeghero, C. McMillan, P. W. McBurney, N. Bosch, and
S. D‚ÄôMello. Improving automated source code
summarization via an eye-tracking study of programmers. In
Proceedings of the 36th International Conference on
Software Engineering , pages 390‚Äì401, Hyderabad, India,
2014.
[24] R. Saha, M. Lease, S. Khurshid, and D. Perry. Improving bug
localization using structured information retrieval. In
Proceedings of the 28th International Conference on
Automated Software Engineering , pages 345‚Äì355, Palo Alto,
California, USA, 2013.
[25] I. Salman, A. T. Misirli, and N. Juristo. Are students
representatives of professionals in software engineering
experiments? In Proceedings of 37th IEEE/ACM
International Conference on Software Engineering , pages
666‚Äì676, Florence, Italy, 2015.
[26] B. Sharif, M. Falcone, and J. I. Maletic. An eye-tracking
study on the role of scan time in Ô¨Ånding source code defects.
InProceedings of the Symposium on Eye Tracking Research
and Applications , pages 381‚Äì384, Santa Barbara, California,
2012.
[27] B. Sharif, J. Maletic, et al. The effects of layout on detecting
the role of design patterns. In Proceedings of the IEEE
Conference on Software Engineering Education and
Training, pages 41‚Äì48, Pittsburgh, USA, 2010.
[28] J. Silva and O. Chitil. Combining algorithmic debugging and
program slicing. In Proceedings of the 8th ACM SIGPLAN
international conference on Principles and practice of
declarative programming, pages 157‚Äì166, Venice, Italy,
2006.
[29] I. Sommerville. Software Engineering . Pearson, 9 edition,
2009.
[30] R. Stein and S. E. Brennan. Another person‚Äôs eye gaze as a
cue in solving programming problems. In Proceedings of the
6th international conference on Multimodal interfaces , pages
9‚Äì15, State College, USA, 2004.
[31] Y. Tao, J. Kim, S. Kim, and C. Xu. Automatically generated
patches as debugging aids: a human study. In Proceedings of
the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering , pages 64‚Äì74, Hong
Kong, China, 2014.
[32] F. Thung, D. Lo, L. Jiang, et al. Are faults localizable? In
Proceedings of the IEEE 9th Working Conference on Mining
Software Repositories , pages 74‚Äì77, Zurich, Switzerland,
2012.
[33] H. Uwano, M. Nakamura, A. Monden, and K.-i. Matsumoto.
Analyzing individual performance of source code review
818using
reviewers‚Äô eye movement. In Proceedings of the 2006
symposium on Eye tracking research & applications , pages
133‚Äì140, San Diego, USA, 2006.
[34] Q. Wang, C. Parnin, and A. Orso. Evaluating the usefulness
of ir-based fault localization techniques. In Proceedings of
the 2015 International Symposium on Software Testing and
Analysis, pages 1‚Äì11, Baltimore, Maryland, USA, 2015.
[35] M. Weiser and J. Lyle. Experiments on slicing-based
debugging aids. In Proceedings of the Workshop on
Empirical Studies of Programmers, pages 187‚Äì 197, USA,
1986.
[36] W. E. Wong, V. Debroy, and B. Choi. A family of code
coverage-based heuristics for effective fault localization.
Journal of Systems and Software, 83(2):188‚Äì208, 2010.
[37] X. Xie, T. Y. Chen, F.-C. Kuo, and B. Xu. A theoretical
analysis of the risk evaluation formulas for spectrum-based
fault localization. ACM Transactions on Software
Engineering and Methodology , 22(4):31:1‚Äì31:40, 2013.
[38] J. Xuan and M. Monperrus. Learning to combine multiple
ranking metrics for fault localization. In Proceedings of the
IEEE International Conference on Software Maintenance
and Evolution , pages 191‚Äì200, Victoria, Canada, 2014.
[39] J. Xuan and M. Monperrus. Test case puriÔ¨Åcation for
improving fault localization. In Proceedings of the 22nd
ACM SIGSOFT International Symposium on Foundations of
Software Engineering , pages 52‚Äì63, Hong Kong, China,
2014.
[40] X. Zhang, R. Gupta, and Y. Zhang. Precise dynamic slicing
algorithms. In Proceedings of the 25th International
Conference on Software Engineering , pages 319‚Äì329,
Portland, Oregon, 2003.
819