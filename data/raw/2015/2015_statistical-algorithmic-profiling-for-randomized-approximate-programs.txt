Statistical Algorithmic Proﬁling
for Randomized Approximate Programs
Keyur Joshi, Vimuth Fernando, Sasa Misailovic
University of Illinois at Urbana-Champaign
{kpjoshi2, wvf2, misailo}@illinois.edu
Abstract —Many modern applications require low-latency pro-
cessing of large data sets, often by using approximate algorithms
that trade accuracy of the results for faster execution or re-duced memory consumption. Although the algorithms provideprobabilistic accuracy and performance guarantees, a softwaredeveloper who implements these algorithms has little supportfrom existing tools. Standard proﬁlers do not consider accuracyof the computation and do not check whether the outputs ofthese programs satisfy their accuracy speciﬁcations.
We present A
XPROF , an algorithmic proﬁling framework for
analyzing randomized approximate programs. The developerprovides the accuracy speciﬁcation as a formula in a mathe-matical notation, using probability or expected value predicates.
A
XPROF automatically generates statistical reasoning code. It
ﬁrst constructs the empirical models of accuracy, time, andmemory consumption. It then selects and runs appropriatestatistical tests that can, with high conﬁdence, determine if theimplementation satisﬁes the speciﬁcation.
We used A
XPROF to proﬁle 15 approximate applications from
three domains – data analytics, numerical linear algebra, and ap-proximate computing. A
XPROF was effective in ﬁnding bugs and
identifying various performance optimizations. In particular, wediscovered ﬁve previously unknown bugs in the implementationsof the algorithms and created ﬁxes, guided by A
XPROF .
I. I NTRODUCTION
Modern applications, such as machine learning, data analyt-
ics, computer vision, ﬁnancial forecasting, and content search
require low-latency processing of massive data sets. To meetsuch demands, researchers have developed various approxi-mate algorithms, data structures, and systems software thattrade accuracy for performance and/or memory consumption.
Many emerging approximate algorithms come with analyt-
ically derived speciﬁcations of accuracy. The speciﬁcationsare typically probabilistic – e.g., an algorithm will return thedesired result with high probability. As an example, locality-sensitive hashing algorithm [1], [2] can ﬁnd nearest neighborsin a set of points, by using smart hashing to group similarpoints. It guarantees to return the most similar points with highprobability. Probabilistic speciﬁcations have been proposed forapplications in areas as diverse as theoretical computer sci-ence [3], [4], [5], [6], [7], [8], numerical computing [9], [10],[11], [12], databases [13], [14], [15], [16], [17], and compilersand hardware architectures [18], [19], [20], [21], [22], [23].
Despite many rigorous speciﬁcations, a software developer
who needs to implement, test, and tune these randomized pro-grams and systems has virtually no tool support for this effort.Standard proﬁlers only track and build models of run timeand memory consumption for individual inputs [24], [25],[26] or build performance models for multiple input sizes inthe case of algorithmic proﬁling [27], [28]. Researchers have
also given guidelines for how to rigorously apply statisticaltesting in software engineering, e.g., [29], but the process ismanual, and the developer may end up with inﬂexible andoverly conservative choices of test parameters.
There are numerous tasks that the developer needs to per-
form manually: infer the properties of the mathematical (prob-abilistic) speciﬁcation, write code to check this speciﬁcation,decide on the appropriate statistical test and its parameters(e.g., conﬁdence or power), provide appropriate inputs, andinterpret obtained statistical metrics. Frustrated by such man-ual effort, developers often resort to ad-hoc testing. Moreover,manually written test code can have various subtle errors thatprevent the discovery of errors in the implementation. A morepromising alternative is to automate these tasks with proﬁlingand testing frameworks.
Our Work. We present A
XPROF , an algorithmic proﬁl-
ing framework for analyzing accuracy, execution time, and
memory consumption of approximate programs. A XPROF
constructs statistical models of accuracy, time, and memory,checks if any of them deviate from the algorithm speciﬁca-tion, and if so, warns the developer. A
XPROF is available at
www.axprof.org.
The key novelty of A XPROF is the automatic generation
of the accuracy checking code from a high-level probabilisticspeciﬁcation (Section IV). The developer-written speciﬁcationhighly resembles the mathematical speciﬁcation that algorithmdesigners provide as a part of their theoretical study. A
XPROF
supports two general probabilistic predicates:
•Probability Predicate: It speciﬁes that the probability that
the output returned by the approximate program satisﬁes acondition is below, above, or equal to a certain threshold.
•Expectation Predicate: It speciﬁes that the output’s ex-
pected value is below, above, or equal to a certain threshold.
These two predicates can capture the key properties of many
representative randomized and approximate computations. Forinstance, they are expressive enough to capture the majorityof the accuracy speciﬁcations of the randomized algorithmsfrom [30]. They can also model common accuracy speciﬁca-tions from other domains, such as numerical linear algebraand approximate computing.
6082019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 ©2019 IEEE
DOI 10.1109/ICSE.2019.00071
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. An important concern of A XPROF ’s language design is
to present the speciﬁcations in an unambiguous manner. In
general, probabilistic speciﬁcations can be deﬁned over dif-
ferent sets of events (e.g., runs or inputs), which may require
different sampling procedures. In A XPROF , a developer ex-
plicitly writes if a probabilistic speciﬁcation is over inputs,
items within an input, or runs. For each speciﬁcation, A XPROF
automatically 1) selects a proper statistical test, 2) generates
checking code that aggregates the outputs and applies the
selected test, and 3) determines the number of samples to
achieve a desired level of statistical conﬁdence.
Testing randomized programs often requires a large number
of (concrete) inputs. To automatically produce representative
inputs, we provide several input generators for scalars, vectors,
and matrices, that allow for various input properties to be
modiﬁed: the difference in the frequency of values, order of
data, or various forms of correlations. We present a dynamic
analysis that infers which of these properties have a signiﬁcant
impact on the algorithm’s accuracy (Section V).
Results. We evaluated A XPROF on a set of 15 programs
that implement well-known randomized approximate compu-
tations from the domains of data analytics, numerical linear
algebra, and approximate computing. Each application has an
analytically derived speciﬁcation of accuracy, performance,
and memory consumption. We demonstrate that A XPROF can
help developers in two scenarios: 1) proﬁling to understand
program behavior and 2) identifying potential implementation
errors. Moreover, we demonstrate the effectiveness of the input
generation analysis, which discovers the parameters of the
input generator that affect output accuracy (Section VII).
AXPROF helped us identify and ﬁx previously-unknown
problems with ﬁve different implementations of these algo-
rithms. Our analysis shows that these problems could not
have been identiﬁed with standard proﬁlers that track only
memory or runtime. The problems were caused by incorrect
implementations of the algorithms or their key components
like hash functions. We prepared pull requests for each of these
problems. The implementation developers already accepted
three of our pull requests.
AXPROF also identiﬁed some implementations that make
additional optimizations in their resource consumption. These
optimizations may result in a different complexity of resource
consumption than speciﬁed by the algorithm. For instance,
the implementation developers may allocate resources dynam-
ically (only when needed) or they may create polyalgorithms
– compose multiple algorithms that work better for different
input sizes, and switch algorithms dynamically.
These results demonstrate that A XPROF ’s focus on accuracy
analysis opens a new dimension in algorithmic proﬁling. Pre-
vious approaches for algorithmic proﬁling [25], [28] focused
only on deriving models of performance. As such, they miss
to characterize important accuracy-related properties of the
emerging data-centric applications.Contributions. The paper makes the following contributions:
⋆Concept: We present algorithmic proﬁling for accuracy, per-
formance, and resource consumption of approximate com-
putations. We also present AxProf, a system that automates
many algorithmic proﬁling tasks and pinpoints potential
violations of algorithm speciﬁcations.
⋆Accuracy Analysis: We present an approach for automat-
ically generating statistical testing procedures from high-
level probabilistic speciﬁcations in mathematical notation,
as proposed by the algorithm authors. The generated proce-
dures can pinpoint if the algorithm implementations signif-
icantly deviate from their speciﬁcations.
⋆Evaluation: We present the evaluation of A XPROF on a
set of 15 approximate benchmarks from three application
domains (data analytics, numerical linear algebra, and ap-
proximate computing). Our results show that 1) A XPROF
can be effectively used to ﬁnd errors in randomized ap-
proximate programs and check for the correctness of the
ﬁxes and 2) A XPROF can identify polyalgorithms and
optimizations of resource usage.
II. E XAMPLE
Locality Sensitive Hashing (LSH) [1], [2] is an algorithm
for ﬁnding points that are near a given query point in multidi-
mensional space. Instead of directly computing the distance of
the query point to every other point in the set, LSH maintains a
compact representation of the points and their locations using
a set of hash maps. The keys of the maps are hash signatures
and the values are the list of points with that signature.
To obtain a hash signature of a point, LSH calculates
a “locality sensitive” hash of that point. Depending on the
desired similarity metric between points (such as /lscript1distance,
/lscript2distance, or cosine similarity), different “locality sensitive”
hash function families exist which hash similar points to the
same hash signatures with high probability.
When it receives a query, LSH calculates the query point’s
signature and returns all stored points with that signature. LSH
can increase the number of similar points found by increasing
the number of hash maps ( l). LSH can also concatenate
signatures from kdifferent hash functions as the keys in each
hash map to increase the probability that dissimilar points will
be mapped to different bins. Each of these hash functions must
be drawn uniformly at random from the same hash family.
AXPROF Speciﬁcation. We assign each point in the dataset
an index. One representation of LSH output is a list of pairs
of indices. The ﬁrst index is the index of the query point and
the second is the index of the detected neighbor. We use the
same set of points as both data points and query points.
Suppose a hash function chosen uniformly at random from
the desired hash function family puts a point din the dataset
and the query point qin the same map bin with probability
pd,q. This probability is calculated from the distance between
the dataset point and the query point. Then, for all dandq,
the probability that LSH will return din the output for the
query point qis1−(1−pk
d,q)l.
609
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. (a) Before
 (b) After Fix 1
 (c) After Fix 2
 (d) After Fix 3
 (e) After Fix 4
Fig. 1: TarsosLSH – Comparison of accuracy of the implementation before and after bug ﬁxes. Each point compares the
expected and observed probability for a query-datapoint pair. Ideally all points should lie on the diagonal line.
In A XPROF , we write the full speciﬁcation as:
1Input list of (list of real);
2Output list of (list of real);
3TIME k*l*datasize;
4SPACE l*datasize;
5ACC forall di in indices(Input), qi in indices(Input) :
6 Probability over runs [ [qi, di] in Output ] ==
7 L1HashEqProb(Input[di], Input[qi], 10, k, l)
Lines 1 and 2 indicate the data types of the input andoutput, respectively. Lines 3 and 4 give the time and memoryspeciﬁcation of LSH. As each point must be stored in eachhash table, the memory usage is O(ln), wherenis the number
of data points. Storing a point requires calculating lkhashes.
The total time required to construct the hash tables is O(lkn).
The last three lines give the accuracy speciﬁcation. Infor-
mally, it speciﬁes that, for all possible pairs of indices overthe input(di,qi), the probability that a particular run of
LSH has[di,qi] in its output is equal to the return value
ofL1HashEqProb, which is a user-deﬁned function that
calculates the expression 1−(1−p
k
d,q)l.
AXPROF uses this speciﬁcation to automatically generate
code to check that the property holds. The code aggregates
the outputs of the implementation over multiple runs. Then,for each pair of indices, it calculates the fraction of runs forwhich the pair is in the output. It compares this fraction againstthe return value of L1HashEqProb using the binomial test.
Finally, it combines the results of the binomial tests for eachpair of indices using Fisher’s method. For time and memory,
A
XPROF generates code to perform statistical regression. The
full details of code generation are in Section IV.
Testing the Implementation. We tested TarsosLSH [31],
an implementation of LSH in Java that has its own testing
framework and over 100 stars on GitHub. The algorithm canbe conﬁgured through two parameters kandl, for which the
developer speciﬁes a list of values of interest. The numberof points, datasize, can also be speciﬁed. We instruct
A
XPROF to uniformly generate random 2-dimensional points
with each coordinate in the range [−10,10].
Identifying and Fixing Bugs. While proﬁling TarsosLSH
for the/lscript1distance metric, A XPROF indicated errors for
several values of kandl. We used the visualization feature
of A XPROF and observed that many points were not being
considered similar at all, as shown in Figure 1a. Each pointrepresents a query-datapoint pair. The xandycoordinates
of the point denote the expected and observed probabilityrespectively. Ideally, all results should lie on the diagonal line.This prompted us to investigate the hash function used for
/lscript
1distance. We found that there were several inaccuracies in
the implementation and use of the hash function. We ﬁxed abug that occurred due to operator precedence, followed by abug caused by incorrect assumptions about the rounding ofﬂoating point values in Java. Fixing the ﬁrst bug led to theresult shown in Figure 1b and ﬁxing the second bug led tothe result in Figure 1c. While the result in Figure 1c seemedto conform with the diagonal line as expected, A
XPROF ’s
statistical tests reported that the number of outliers was stilltoo high, indicating the presence of more bugs.
On further investigation we found a bug in the method
by which the implementation chose a hash function from thehash function family, and a bug in the method by which theoutputs of the kdifferent hash functions for a hash table were
combined. Fixing the third bug led to the result shown inFigure 1d and ﬁxing the fourth led to the result in Figure1e. After ﬁxing the fourth bug, A
XPROF indicated that the
implementation conformed with the accuracy speciﬁcation.
An important point to note is that while the results in Figures
1c and 1d seem to be visually correct, A XPROF was able to
accurately conclude that there were still unﬁxed bugs in theimplementation via statistical testing.
The implementation included a test method which tested
the algorithm with various parameters. However, there wasan error in the tester code that miscounted the number offalse negatives. This led the tester to overestimate the recall ofthe implementation, i.e., the percentage of nearby points thatare correctly identiﬁed. A user that depended on the resultsof this test would mistakenly believe that the algorithm wasimplemented correctly. This further illustrates the need forautomated tools like A
XPROF .
III. A XPROF OVERVIEW
Figure 2 presents the overview of the system.
Inputs. AXPROF takes the following inputs:
•Implementation and Parameters: AXPROF takes an imple-
mentation of the algorithm to test and a set of algorithmconﬁguration parameter ranges to be tested.
•Property Speciﬁcation: The user provides an accuracy, time,
and memory speciﬁcation in a high-level language thatresembles the mathematical speciﬁcations usually providedby the algorithm authors. A
XPROF automatically generates
code to check these speciﬁcations.
610
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. Fig. 2: Overview of A XPROF
•Input Generator Properties: AXPROF provides several
input generators for scalar, vector, and matrix data. Alterna-
tively, the user can provide a custom input generator. The
user can allow A XPROF to infer interesting parameters that
affect the output accuracy of the algorithm, or ﬁx some or
all of the parameters to values that the user wants to test.
Components. AXPROF has multiple components:
•Checker Generator takes an accuracy, time, and memory
usage speciﬁcation from the user and generates code to
aggregate output data, along with time and memory usage
data, and check that the data conforms to the speciﬁcations.
•Input Generator generates inputs to test the program. It
can experiment with various input parameters to determine
which ones affect the accuracy of the output.
•Runner executes the program on an A XPROF -provided
input. It returns the generated output and resource consump-
tion statistics to A XPROF .
•Analyzer uses the code generated by the Checker Generator
to test whether the implementation conforms to the provided
speciﬁcations, and issues a warning otherwise. A XPROF ’s
analyses provide statistical guarantees that a warning may
signify a real discrepancy with high probability. The de-
veloper can set parameters that inﬂuence how sensitive the
statistical analyses are.
•Visualizer plots time, memory, and accuracy statistics for
visual inspection by the user.
We present the statistical techniques for constructing models
and checking whether they deviate from the ones provided
in the speciﬁcation for both accuracy and resource usage in
Section IV. These techniques guide the Checker Generator
(before proﬁling begins) and the Analyzer (as the proﬁler
runs). We describe input generators in Section V.
Speciﬁcation Language. The user writes speciﬁcations in a
high-level language. Figure 3 presents its grammar. Speciﬁ-
cations consist of type declarations, time expression, memory
expression, and accuracy expression. The time and memory
expressions are typical arithmetic expressions. Supported types
are real numbers, matrices of real numbers, or collections (lists
or maps). Knowing the types helps our generators to produce
the correct code for the checkers and connect them with the
rest of the framework.
The accuracy speciﬁcation encodes two common predicates:
1) probability comparison ( Probabilityover Qualif ) and
2) expectation comparison ( Expectation over Qualif ).
Both predicates explicitly deﬁne the probability space via
qualiﬁers . The qualiﬁer can be a list of items, a set of
executions ( runs ), or a set of inputs. If the qualiﬁcation isc∈Constants
x∈Va r s∪{Input, Output }
f∈Functions
aop∈{+,-,*,/,ˆ}
bop∈{&&,||}
rop∈{==,>,...}
Spec ::=TDclrTIME DExpr ;SPACE DExpr ;ACC ASpec
TDclr ::=Input Type ;Output Type ;(xType ;)∗(fType ;)∗
Type ::=real|matrix |list of Type|
map from Typeto Type
ASpec ::=Probability over Qualif [BExpr ]rop DExpr|
Expectation over Qualif [DExpr ]rop DExpr|
forall Range+:ASpec|
let x=DExprin ASpec
Qualif ::=runs|inputs |Range+
Range ::= xin DExpr|xin uniques (DExpr )|
xin indices (DExpr )
BExpr ::=BExpr bop BExpr|!BExpr|
DExprin DExpr|DExpr rop DExpr
DExpr ::=c|x|x[DExpr ]||DExpr||DExpr aop DExpr |
f(DExpr+)|[DExpr+]
Fig. 3: A XPROF Speciﬁcation Language
over items in the input, each item has equal weight, as used
in average-case analysis of algorithms [32]. The accuracy
speciﬁcation also allows quantiﬁcation over a list of items.
We interpret these quantiﬁers as the requirement that the
tests of the predicates inside the quantiﬁers should be correct
for each item in the list. These predicates are translated
to corresponding statistical tests using the code generation
process we describe in Section IV-B.
The speciﬁcation can contain the special variables Input
(the input data) and Output (the algorithm output). Addi-
tional variables may also be declared in range expressions and
let expressions.
Finally, the speciﬁcation can contain standard boolean
and arithmetic expressions. The boolean operator in checks
whether an element is in a collection. Elements within a
collection can be accessed using a key using typical array
access notation. The operator |·| calculates the size of a
collection. We allow a developer to call helper functions
written in Python. These functions may implement compli-
cated testing conditions or compute exact solutions through an
oracle. Individual parameters from the algorithm conﬁguration
can be accessed directly by their name. Multiple expressions
of the same type can be composed into a list.
IV . C HECKER CODE GENERA TION
We derive statistical hypotheses from A XPROF ’s accuracy
speciﬁcations, test them with common statistical tests, and
calculate the number of executions or inputs necessary. We
also generate code to check resource utilization speciﬁcations.
611
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. A. Background on Statistical Testing
A statistical hypothesis can be tested by observing samples
of one or more random variables. A tester forms two hypothe-
ses: a null hypothesis and an alternative hypothesis . Then they
use an appropriate statistical test to calculate a p-value: the
probability of obtaining a test statistic at least as extreme as
the one observed, assuming the null hypothesis is true. If the
p-value is too low, the null hypothesis can be rejected.
Several statistical tests are available for various use cases.
These tests are either parametric (they make some assumption
about the population from which the data is drawn) or non-
parametric (they make no such assumptions). Parametric tests
are generally more powerful at detecting statistical anomalies,
while nonparametric tests can handle more types of data and
small sample sizes. We use several statistical tests in A XPROF .
The binomial test [33] is an exact nonparametric test used
to compare the observed probability ( p1) of an event against
an expected or threshold probability ( p0). For example, to
test speciﬁcations that state p1≤p0we formulate a null
hypothesis H0:p1≤p0and test it against the alternative
hypothesis H1:p1>p 0using the binomial test. The greater
one-tailed ,lesser one-tailed , and two-tailed variants of this
test respectively check if the observed probability is too high,
too low, or different to the expected probability.
The one-sample t-test [34] is a parametric test used to com-
pare the mean of one set of samples ( μ) against an expected or
threshold mean ( μ0). For example, to test speciﬁcations that
stateμ=μ0we formulate the null hypothesis H0:μ=μ0
and test it against H1:μ/negationslash=μ0using the t-test. This test too
has one and two-tailed variants. Although the t-test requires
that the sample mean is normally distributed, when the sample
size is large enough, this requirement can be assumed to hold.
Another approach to perform hypothesis testing is to use
sequential testing, where hypothesis testing is performed
as samples are collected. Sequential Probability Ratio Test
(SPRT) [35] is often proposed as a technique to select between
two hypotheses with a minimum number of samples. SPRT
maintains a likelihood ratio, updated after each sample, that
rates two hypotheses based on the observed samples. Based
on this ratio one hypothesis is accepted, or more samples are
gathered until enough evidence is available to pick one.
Fisher’s method [36] is a technique for combining the results
of multiple statistical tests for the same null hypothesis. Each
individual test produces a p-value for the hypothesis. Fisher’s
method can then be used to calculate a single p-value for the
entire set of tests. If this combined p-value is too low, then the
null hypothesis can be rejected. Otherwise the null hypothesis
cannot be rejected even if some of the individual tests failed.
Fisher’s method assumes that the results of the individual tests
are independent, which is not always true. If the tests are
dependent, using Fisher’s method may result in a p-value lower
than the correct p-value.
B. Generating Accuracy Checker Code
Based on the accuracy speciﬁcation, A XPROF needs to
select what statistical test to use and how many samples areneeded for the statistical test based on the required level of
conﬁdence. In addition, A XPROF needs to decide how to
aggregate output data over multiple executions or inputs and
when to use the aggregated data to perform the statistical tests:
after every run, after multiple runs for the same input, or after
runs on multiple inputs.
AXPROF supports three main types of accuracy speciﬁca-
tions and selects the statistical test based on the type of ASpec
expression from the speciﬁcation language and the comparison
operator used in its predicate.
Probability Predicates. Speciﬁcations of the form
Probability over Qualif [BExpr] rop DExpr
require testing the probability of satisfying the inner boolean
expression ( BExpr ) against the probability DExpr .
Each element in the space deﬁned by Qualif can be
treated as one sample drawn from a Bernoulli distribution
which is 1 if BExpr is satisﬁed and 0 otherwise. This allows
us to use the binomial test to compare DExpr with the
probability of the Bernoulli variable. Based on the probability
space deﬁned by the user in Qualif , we need to gather
sample outputs of the program over multiple executions or
inputs to calculate the fraction of elements that satisfy BExpr :
•IfQualif isruns , the accuracy speciﬁcation deﬁnes a
probability over a set of executions of the program on
the same input. Therefore, A XPROF executes the program
multiple times and calculates the fraction of executions
that satisfy BExpr . At proﬁling time, A XPROF sets the
number of executions to the number of samples required for
the binomial test (Section IV-C). If the developer provides
multiple inputs, then the implementation is expected to pass
the test for each input separately.
•IfQualif isinputs , the accuracy speciﬁcation deﬁnes
a probability over the inputs of the program. In this case,
for each conﬁguration of algorithm parameters, A XPROF
executes the program on multiple inputs and calculates the
fraction of inputs that satisfy BExpr . At proﬁling time,
AXPROF sets the number of inputs to the number of samples
required for the binomial test (Section IV-C). A XPROF
generates inputs using its input generators (Section V) and
the implementation is executed once on each input.
•IfQualif is a list of items ( Range+), after each execution
of the program we calculate the fraction of items in the
list that satisfy BExpr . The test is performed separately
for each execution on each input. The speciﬁcation should
be valid for each run of the implementation. While we
cannot prove this property with full certainty, we can do
so with high conﬁdence. In particular, we formulate the
null hypothesis that the program succeeds with very high
probability (e.g. greater than 0.999) and use the SPRT to
estimate the number of executions sufﬁcient to check this
weaker property (Section IV-C).
In all cases, the fraction of samples (executions, inputs, or
items) that satisfy BExpr is compared against the value of
DExpr using the binomial test. If A XPROF observes enough
evidence to reject the null hypothesis, it issues a warning to the
612
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. user. Depending on the comparison operator used ( rop ), we
choose one of the three variants of the binomial test ( greater
one-tailed ,lesser one-tailed ,o r two-tailed ).
Expectation Predicates. Speciﬁcations of the form
Expectation over Qualif [DExpr1] rop DExpr2
require comparing the value in expression DExpr1 against
the expected value in expression DExpr2 . We gather samples
of the value of DExpr1 overQualif and calculate their
mean. For large enough sample sizes, this sample mean value
is an estimate of the real mean value and can be considered to
be drawn from a normal distribution centered around the real
mean value. This allows us to use of the t-test for comparing
the sample mean against the expected value. Similar to the
probability predicates, depending on the comparison operator
used (rop ), we choose one of the three variants of the t-test.
Based on the sample space deﬁned by the user in Qualif ,
we may have to gather sample outputs of the program over
multiple executions or inputs (as in the probability predicate
case). We calculate the value of DExpr1 for each sample
and take the mean. This mean is then compared against the
expected mean DExpr2 using the appropriate t-test. The
process of gathering samples is similar to the process used
for probability predicates, except that A XPROF uses the t-test
instead of binomial test for calculating the number of samples.
Universally Quantiﬁed Predicates. Speciﬁcations of the form
forall Range+ : ASpec
require that each element in Range+ satisfy the accuracy
predicateASpec (a probability or expectation speciﬁcation).
We perform the statistical test required for the probability or
expectation speciﬁcation ASpec as described in the previous
paragraphs for each individual element in Range+ . The null
hypothesis for each test is that the implementation satisﬁes
ASpec for all elements, the alternative being that it does not
satisfyASpec for that element. This results in multiple p-
values, one for each element in Range+ .
Next, we combine these p-values obtained from the individ-
ual tests into a single p-value, to test if the overall speciﬁcation
is satisﬁed. A XPROF uses Fisher’s method for this purpose.
C. Determining the Required Number of Samples
For statistical tests, the required number of samples depends
on the test being used, desired signiﬁcance level α, the
statistical power 1−β, and other test-speciﬁc parameters. The
user can specify these parameters to control false warnings,
risk of potentially missing a bug, and run time of A XPROF .
Binomial Test. The number of samples required also depends
on the size of the indifference region, δ, which determines
the minimum deviation from the expected probability that the
test will be able to detect [37]. The minimum number of
samples required to achieve the required statistical conﬁdence
is/parenleftBig/parenleftBig
zsig/radicalbig
p0(1−p0)+z1−β/radicalbig
pa(1−pa)/parenrightBig
/δ/parenrightBig2
, where for
any probability q,zqis the critical value of the normal
distribution for q. Here,zsig isz1−α/2for a two-tailed test
andz1−αfor a one-tailed test.One-sample t-test. The number of samples required also
depends on the effect size d=|μ0−μ1|/σ, the difference
in the means corresponding to the null hypothesis ( μ0) and
an alternative hypothesis ( μ1) divided by the standard devia-
tion (σ) of the sample being tested. The minimum number of
samples required is (tsig+tβ)2/d2, where for any probability
q,tqis the critical value of the t-distribution for q. Here,tsig
istα/2for a two-tailed test and tαfor a one-tailed test.
SPRT. To calculate the minimum number of runs n,w e
use SPRT with the minimum success probability Hand the
maximum failure probability L,a sn≥log(β)−log(α)
log(H)−log(L).W e
ensure that each individual execution passes the test.
D. Analyzing Resource Utilization
To analyze the time and memory consumption of a compu-
tation, A XPROF employs curve ﬁtting to build the most likely
regression model and checks the quality of the ﬁt. As the ﬁrst
step, A XPROF generalizes speciﬁcation expressions provided
by the developer to capture the hidden constants in asymptotic
notations. For example, if the speciﬁcation of time/memory is
the expression x+y∗z, then A XPROF will generate the general
expression: (p0∗x+p1)+(p2∗y+p3)∗(p4∗z+p5)+p6.
For this expression, A XPROF searches for the values of the
free variables p0...6that best ﬁt the data using statistical curve
ﬁtting [38]. The curve ﬁtting procedure computes the R2
metric, which quantiﬁes how well the model ﬁts the observed
data. Higher R2values indicate better ﬁtting models. A XPROF
triggers a warning if it cannot ﬁnd a model with high R2.
V. I NPUT GENERA TION
To generate random inputs to test the algorithms, we devel-
oped a set of ﬂexible input generators, each of which can
control different aspects of the generated data. Each input
generator outputs a required number of data elements. We
developed three input generators:
•Integer/Real Generator. This generator can be used to
generate a list of integers or reals. The data can either be
sampled uniformly from a range of valid values or can be
drawn from a Zipf distribution with a given skew, which
allows for the control of frequency of individual data items.
The generator also allows for the control of internal order of
data items, and the distance between individual data items.
•Matrix Generator. This generator can be used to generate
a matrix of given dimensions with random elements. In
addition to the controllable parameters from the previous
generator, the sparsity of the matrix can be controlled. The
sparsity can be between 0 (fully dense) to 1 (only zeros).
•Vector Generator. This generator can be used to generate a
list of vectors with given number of dimensions. The inter-
nal order of elements and the distance between individual
vectors (/lscript2distance) can be controlled.
Automatically Selecting Input Generator Features. Identi-
fying what input features affect the accuracy of a program is
important to selecting a input generator and deciding what
inputs to test. Each generator described above has input
features that can be used to control the generated inputs.
613
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Summary of the Algorithms. ( nis the size of the dataset in all unspeciﬁed cases)
Name Parameters (Informal) Accuracy Speciﬁcation Time Memory
Locality Sensitive Hashing k : hash functions per table P[Neighbor ]=1−(1−pk)lInsertion : O(kl) O(ln)
l : number of hash tables pdepends on similarity Query :O(kl+n)
Bloom Filter p: max. false pos. probability P[False positive ]≤p Insertion: O(log(1/p))O(nlog(1/p))
c: capacity If number of inserted elements <c Query:O(log(1/p))
Count-Min Sketch /epsilon1: error factor P[error>n∗/epsilon1]<1−δO (log(1/δ)) O((1//epsilon12)l o g ( 1/δ))
δ: error probability
HyperLogLog k : Number of hash values P[error≤1.04/sqrt(k)]>=0.65 O(1) for ﬁxed k O(k)
Reservoir Sampling s : reservoir size P[in sample ]=m i n ( s/n, 1) O(1) O(s)
Matrix Multiply c : sampling rate P[/bardblerror/bardblF<C ]>1−δ
A:mxnmatrix C=η/c∗| |A||F||B||F O(mcnp +c(m+p))O(mcnp +c(m+p))
B:nxpmatrix where η=1+/radicalbig
8∗log(1/δ)
Chisel/blackscholes r: reliability factor P[exact =approx ]>r O(1) O(1)
Chisel/sor r: reliability factor
mxn: matrix dimensions P[exact =approx ]>r O(imn ) O(mn)
i: iterations
Chisel/scale s : scale factor E[PSNR (d,d/prime)]≥− 10·log10(1−r)O(s2mn) O(s2mn)
r: reliability factor
TABLE II: Accuracy Speciﬁcations Provided to the Checker Generator
Algorithm Accuracy Speciﬁcation for A XPROF
LSHforall i in indices(Input), q in indices(Input) :
Probability over runs [ [q,i] in Output ] == L1HashEqProb(Input[i],Input[q],10,k,l)
Bloom Filter Probability over i in excluded(Config,Input )[ii n Output]<p
Count-Min SketchProbability over i in uniques(Input)
[ (count(i,Input) - Output[i]) > epsilon *|Input|]<1- delta
HyperLogLog Probability over inputs [ abs(datasize-Output) < (datasize *1.04)/sqrt(2ˆk) ] >= 0.65
Reservoir Sampling forall i in Input : Probability over run s[ii n Output ] == min(ressize/datasize,1)
Matrix MultiplyProbability over runs
[Output < (eta(delta)/samplingFactor) *(frobenius(Input[0]) *frobenius(Input[1]))] > delta
Chisel/blackscholes Probability over runs [ Output == oracle(Config,Input )]>r
Chisel/sor Probability over runs [ Output == oracle(Config,Input )]>r
Chisel/scale Expectation over runs [ PSNR(Input, Output) ] >= -10 *log10(1-r)
We adapt a technique from [39] to select important input
features that need to be explored. We use Maximal Information
Coefﬁcient (MIC) [40] to identify relationships between input
features and the accuracy of a program. MIC is commonly
used to identify associations between a given pair of variables.
For each input feature available in a input generator we
perform sample runs of the program and gather the accuracy
of the runs. We use this data to calculate a MIC value. If the
MIC value is beyond a threshold, we accept that input feature
as one that affects output.
VI. M ETHODOLOGY
Table I presents the summary of algorithms we analyze in
this paper. We chose these algorithms to represent common
randomized and approximate tasks. The table lists algorithmic
parameters that can be controlled (Column 2), and the accu-
racy, time, and memory speciﬁcations (Columns 3, 4, and 5).
Table II presents the accuracy speciﬁcations for each algorithm
speciﬁed using A XPROF ’s language.
A. Tested Algorithms
Locality Sensitive Hashing. We presented it in Section II.
Bloom Filter. Bloom Filter [8] checks if an item was present
in a data stream. It starts with kdifferent hash functions andan all-0 bit ﬁlter of length m. For each data item, it calculates
thekdifferent hash functions and sets the corresponding
bits to 1. To check if an item qwas in the stream, the
algorithm calculates all the hashes of qand checks that
each corresponding bit is 1. kandmare calculated from a
speciﬁed capacity cand a maximum false positive rate p.I n
the speciﬁcation in Table II, excluded calculates the set of
items in the input that were not inserted into the ﬁlter.
Count-Min Sketch. Count-Min Sketch [6] counts the fre-
quency of items in a dataset. The algorithm maintains a set
of uniform hash functions whose range is divided into a set
of bins. For every item in the dataset, the hash functions are
calculated and counters in the mapped bins are incremented.
The estimated frequency of an item is the minimum of all
the counters in the corresponding bins. The accuracy can be
improved by increasing the number of hash functions and bins.
In the speciﬁcation in Table II, uniques calculates the set of
unique items in the input, as some items appear multiple times.
HyperLogLog. HyperLogLog [3] is an algorithm for calculat-
ing the number of distinct elements in a large dataset. For each
element in a dataset, the algorithm calculates khash values.
The hash value with the maximum number of leading zeros
is then used to estimate the cardinality of the dataset. The
variance of the result can be reduced by using more hashes.
614
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. TABLE III: Controlled parameters
Algorithm Parameters Range
LSHHash functions per table 2, 4, 8
Number of hash tables 2, 4, 8, 16
Input size (performance) 1000 - 10000 step 1000
(accuracy) 100
Bloom FilterMax. false positive prob. 0.1, 0.01, 0.001
Load (fraction of capacity) 0.2 - 0.8 step 0.2
Capacity (performance) 20000 - 100000 step 20000
(accuracy) 200 - 1000 step 200
Count-Min/epsilon1: error factor 0.1, 0.01, 0.001
δ: error probability 0.2, 0.1, 0.05
Input size 10000 - 100000 step 10000
HyperLogLogNumber of hash function 28,210,212,214
Unique items in input 10000 - 100000 step 10000
Reservoir SamplingSize of reservoir 10000 - 100000 step 10000
Input: size 10000 - 100000 step 10000
Matrix MultiplySize of matrices 20×20-200×200
Sampling rate 0.2, 0.4, 0.8
blackscholes load error rate 0.000048, 0.00024, 0.24
sorSize of matrices 4×4-50×50
Iterations 1, 5, 10, 20
omega 0.1, 0.5, 0.75
scaleInput Image 5 Images
scale factor 1, 2, 4, 8
Reservoir Sampling. Reservoir Sampling [7] uniformly sam-
ples a data stream. For a reservoir of size s, the ﬁrst selements
are directly inserted into the reservoir. Afterwards, for the ith
element to be inserted, an integer pis chosen uniformly at
random from [1,i].I fp≤sthen the item currently in the pth
position in the reservoir is replaced with the new item.
Randomized Matrix Multiplication. Randomized matrix
multiplication [9] methods reduce computation time and re-
source consumption of matrix multiplication by randomly
sampling the matrices. Guarantees for accuracy are given as
an upper bound on the Frobenius norm of the errors. The error
can be controlled by changing the sampling rate.
Chisel Kernels. Chisel [20] is a reliability aware optimization
framework for programs that run on approximate hardware.
For a given reliability speciﬁcation, Chisel minimizes energy
consumption by utilizing approximate computations. Chisel
programs run on an approximate hardware simulator that
injects errors at runtime. We look at three kernels from Chisel:
1)scale scales an image by a speciﬁed scale factor; 2) sor
performs Jacobi SOR for a given matrix; 3) blackscholes
computes the price of a stock portfolio using the Black-Scholes
formula. For blackscholes and sor, Chisel provides bounds on
the probability that the output differs from the exact value.
For scale , the authors provided the expression for the expected
PSNR value between the exact and approximate results.
B. Algorithm Implementations
For most algorithms we selected two implementations from
GitHub. We preferred implementations in Java, Python, or
C/C++. We took several factors into account when selecting
implementations to proﬁle, such as the number of stars on
GitHub and how active the repository is. All the selected
implementations appear among the top-10 search results in
GitHub for the name of the algorithm. Finally, we chose three
implementations of kernels from the evaluation of Chisel [20].C. Experimental Setup
Parameters and Their Ranges. The parameter value choices
offer a trade-off between proﬁling time and the conﬁdence
in the algorithm’s correctness. We chose parameters across
their valid range. For each parameter in the time or memory
speciﬁcations, we used at least 3 values across the range (for
curve ﬁtting). Table III presents the ranges in our experiments.
Statistical Tests. For statistical tests, we used a signiﬁcance
levelα=0.05and a statistical power 1−β=0.8when
calculating the number of runs. To get the number of runs for
per-run checkers, we used SPRT with an minimum acceptable
probability of success of 0.999 and a maximum probability of
failure of 0.99. Based on these values, A XPROF calculated that
320 runs were sufﬁcient. For the speciﬁcations requiring the
binomial test, we chose a probability deviation δ=0.1.F o r
those that require the t-test, we chose an effect size d=0.2.
For both of these, A XPROF calculated (using the formulae
from Section IV-C) that slightly less than 200 runs were
sufﬁcient. For identifying resource model discrepancies, we
used anR2threshold of 0.9.
Environment. We ran experiments on a Xeon E5-1650 v4
CPU, 32 GB RAM, Ubuntu 16.04. For time proﬁling we used
a real-time timer around the relevant functions. For memory,
we used serialization in Java and Python, and the time Linux
utility for C/C++ programs. For ﬁtting the models we used the
scipy.optimize module of SciPy [52].
VII. E V ALUA TION
This section discusses the three main research questions:
•RQ1: How effective is A XPROF in proﬁling accuracy of
applications? (Section VII-A)
•RQ2: Can the bugs found by A XPROF be identiﬁed by
performance-only algorithmic proﬁling? (Section VII-D)
•RQ3: Is A XPROF effective in identifying input features that
affect program’s accuracy? (Section VII-E)
A. Effectiveness of AXPROF in Proﬁling Accuracy
Table IV presents a summary of our ﬁndings using
AXPROF . Column 1 presents the algorithm, and Column 2
the proﬁled implementation. Column 3 presents the results
for accuracy analysis for each benchmark implementation.
Columns 4 and 5 present the analysis for time and memory. In
each column, a “ /check” represents that A XPROF did not ﬁnd any
issues. W ARN(X/Y) represents cases where A XPROF issued a
warning in X out of Y conﬁgurations. A “*” indicates a false
warning. For accuracy analysis each conﬁguration was tested
independently. For time and memory analysis, data from all
conﬁgurations are part of a single regression model.
Out of the 15 implementations that we proﬁled, 4 im-
plementations (Matrix Multiplication in mscs and all Chisel
kernels) passed all tests for compliance with accuracy, time
and memory speciﬁcations. A XPROF detected conditions that
trigger warnings in the remaining 11 implementations. We
manually analyzed the implementations that caused warnings.
We found two causes for the 9 real warnings:
615
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. TABLE IV: Summary of the Results of Proﬁling
Algorithm Implementation Accuracy Time Memory
LSH TarsosLSH [31] W ARN (12/12) /check/check
LSH java-LSH [41] W ARN (4/4) /check N.A.
Bloom Filter libbf [42] /check Insertion: /check Query:W ARN /check
Bloom Filter BloomFilter [43] /check Insertion: /check Query:W ARN /check
Count-Min alabid [44] W ARN (90/90) /check/check
Count-Min awnystrom [45] W ARN (81/90) /check W ARN*
HyperLogLog yahoo [46] /check W ARN W ARN
HyperLogLog ekzhu [47] W ARN* (2/40) /check/check
Reservoir Sampling yahoo [46] /check/check W ARN
Reservoir Sampling sample [48] /check W ARN* /check
Matrix Multiplication RandMatrix [49] W ARN (30/243) /check/check
Matrix Multiplication mscs [50] /check/check /check
blackscholes Chisel [51] /check/check /check
sor Chisel [51] /check/check /check
scale Chisel [51] /check/check /check
•Errors in implementations: four implementations used hash
functions with errors that caused higher than expected accu-
racy loss. One implementation had a misconﬁgured random
number generator that affected sampling (Section VII-B).
•Performance optimizations that caused unexpected execu-
tion times or memory usage (Section VII-C).
We observed false warnings (W ARN*) in the time speciﬁca-
tion check for sample and the memory check for awnystrom
due to noise in measurements. In HyperLogLog, when the
input-set cardinality is very low and close to a predeﬁned
threshold, the estimate error is expected to be high. This led
to warnings for two conﬁgurations of ekzhu (yahoo avoided
the problem via a polyalgorithm [53]).
B. Errors in Implementations
LSH: T arsosLSH .We discuss this benchmark in Section II.
LSH: java-LSH .This is a MinHash-LSH implementation in
Java for Jaccard similarity metric [54]. We observed that sets
were being considered similar to the query set more often than
expected, as shown in Figure 4. Each point represents a query-
datapoint pair. The xandycoordinates of the point denote
the expected and observed probability, respectively. Results
before and after the bugﬁx are shown. The implementation
used the simple hash function h(x)=(a∗x+b)%m. This
hash function is usable only when mis a prime. However,
the implementation often chose a composite value for m.W e
ﬁxed the hash by setting mto a ﬁxed, large prime. After this
ﬁx, the observed results matched the expected values.
Count-Min Sketch: awnystrom ,alabid .For each data item,
Count-Min Sketch calculates multiple hash functions chosen
randomly from a family of hash functions. The family of
hash functions used in an implementation should be pairwise
independent . i.e. ifhis a function chosen uniformly at random
from the hash family, for two values xandy,h(x)andh(y)
are uniformly distributed and pairwise independent.
AXPROF detected that the observed error rate was higher
than expected for many conﬁgurations in both implemen-
tations. By manual inspection we identiﬁed that the buggy
implementation uses a hash function that does not satisfy
the pairwise independence property. Figure 5 presents theobserved errors in awnystrom . The X axis shows the various
input sizes, and the Y axis shows the fraction of runs that had
errors beyond the speciﬁed threshold. We observe that this
fraction dropped signiﬁcantly in both implementations after
we ﬁxed the bugs in the hash functions.
Matrix Multiplication: RandMatrix .The rows and columns
of the matrices to be multiplied are subsampled to reduce their
size. The algorithm [9] provides an optimum sampling method
that was not implemented correctly (wrong initialization of
std::discrete_distribution in C++) in the imple-
mentation leading to wrong results.
Developer-Provided Tests. In all cases, tests written by the
developers failed to catch the bugs identiﬁed through A XPROF .
We observed three main reasons: 1) unit tests only partially
check the algorithm functionality ( java-LSH ), 2) tests use
ﬁxed inputs that do not trigger bugs ( alabid ,awnystrom ,
RandMatrix ), 3) bugs in the test framework itself ( TarsosLSH ).
C. Performance Optimizations
We also observed situations where warnings were issued
in A XPROF due to performance optimizations in implementa-
tions that were causing unexpected behavior, which were not
necessarily errors.
Reservoir Sampling. The memory usage was unexpectedly
low due to the implementation incrementally allocating mem-
ory. Figure 6 plots the observed memory usage against the
number of inserted elements for various reservoir sizes.
HyperLogLog: yahoo .AXPROF was unable to model the
runtime of the algorithm against the input size due to the
polyalgorithm implementation [53]. Figure 7 shows the run-
time of the algorithm (Y axis) against the size of the dataset
(X axis) and the best linear model A XPROF found.
Bloom Filter. Instead of checking the entire ﬁlter to search
f o ra0v alue, the programs return when the ﬁrst 0 is found.
This property is not encoded in the speciﬁcation.
D. Effectiveness of Accuracy Proﬁling
We analyzed all of the accuracy bugs we identiﬁed and
conﬁrmed that they can not be detected through regular
616
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. Fig. 4: java-LSH Accuracy
 Fig. 5: awnystrom Accuracy
 Fig. 6: RS:yahoo Memory
 Fig. 7: HLL:yahoo runtime
TABLE V: Input feature impact on accuracy
Algorithm Order Frequency Distance Sparsity
Count-Min (x, x) (/check, /check) (x, x) -
HLL (x, x) (x, x) (/check,x ) -
BF (x, x) (/check, /check) (x, x) -
Matrix Mul (/check, /check)- ( /check,/check)( /check,/check)
LSH (x, x) (x, x) (/check, /check)-
proﬁling techniques that focus only on runtime and/or resource
consumption. As can be seen from Table IV, in all situationsaccuracy bugs could not be detected through unexpected run-time or memory consumption that could have been identiﬁedthrough usual algorithmic proﬁling methods. The only casewhere accuracy warnings coincide with other warnings wasdue to noise in performance measurements for the memoryspeciﬁcation check for awnystrom. These results show the
importance of including accuracy in algorithmic proﬁling.
E. Effectiveness of Input Feature Selection
We studied four input features from A
XPROF ’s input gen-
erators and their effect on accuracy of the program. Table V
shows the results of the analysis. We only looked at thecorrect implementations of the programs. We analyzed thebenchmarks manually to conﬁrm the results of the MIC -based
approach (Section V). Each column with the format “(auto-matic/manual)” has a check mark if A
XPROF ’s automatic and
our manual analyses, respectively, show that the input featureaffects accuracy. For Reservoir Sampling, we were unableto derive an accuracy measurement due to the nature of thespeciﬁcation. For the Chisel benchmarks, the accuracy dependsonly on the underlying hardware, therefore input features wechanged did not have any impact.
We observed one situation where the manual analysis differs
from the results of the MIC based approach. In HyperLogLog,
the distance between individual values is falsely identiﬁed as
affecting accuracy even though it should not. We attribute thisto randomized properties of the hash functions.
VIII. R
ELA TED WORK
Algorithmic Proﬁling. Recently, researchers explored tech-
niques for advanced algorithm-aware proﬁling and estimation.Several approaches have been proposed to model performanceof programs as a function of workload [26], [27], [28].Algorithmic proﬁling [28] is a framework that focuses onautomating such proﬁling tasks by detecting algorithms and
their inputs. COZ [55] is a causal proﬁler that estimates
the effect of potential optimization of subcomputations onthe performance of the whole program. Researchers proposedsimilar techniques for analyzing memory and recursive datastructures, e.g., [56], [57]. A
XPROF ’s accuracy analysis is
complementary to these existing approaches.
Statistical debugging and proﬁling tools, e.g., [58], [59],
[60] use statistical models of programs to predict and isolatebugs. This line of research is conceptually orthogonal to ours.
Analysis of Accuracy. Researchers have also looked at various
dynamic approaches [61], [62], [63], [64], [65] to empirically
analyze the impact on accuracy from transformations thatchange program semantics. In contrast, A
XPROF uses theoret-
ical speciﬁcations of approximate algorithms and checks fordiscrepancies in their implementations. MayHap [19] convertsprogram code to a Bayesian network and uses Chernoff boundsto check probabilistic assertions over a set of executions.In contrast, A
XPROF operates on a program as a black-box
system, supports a richer set of predicates including inputsand items, and automatically selects the appropriate test.
Statistical Model Checking. Statistical model checking [37],
[66], [67] is a general method to verify properties of black-
box stochastic systems using statistical hypothesis testing. Forinstance, the framework of Sen et al. [66] expresses propertiesin Continuous Stochastic Logic (CSL) and samples the outputs
from the tested system. CSL’s probability predicate, like our
“probability over runs” predicate, estimates the probabilitythat the system satisﬁes a speciﬁed logical property. However,expressing the predicates over inputs and items would besigniﬁcantly more complicated in CSL and it does not supportexpectation predicates or complex data structures, like lists ormatrices. In addition, A
XPROF automatically generates code
for collecting and aggregating data, thus giving a developeran intuitive tool to simultaneously explore various aspects ofprogram’s accuracy and resource consumption.
IX. C
ONCLUSION
We presented A XPROF , an algorithmic proﬁling framework
for analyzing execution time, memory consumption, and ac-curacy of randomized approximate programs. Our evaluationdemonstrated that A
XPROF helps developers to check that
implementations of such algorithms conform to the algorithmspeciﬁcations and can help in ﬁxing accuracy related bugs.With its ability to analyze accuracy speciﬁcations, A
XPROF
opens a new dimension in algorithmic proﬁling.
ACKNOWLEDGEMENTS
The research presented in this paper was funded in part by
NSF Grants CCF-1629431 and CCF-1703637.
617
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] P . Indyk and R. Motwani, “Approximate nearest neighbors: Towards
removing the curse of dimensionality,” in STOC , 1998.
[2] A. Andoni and P . Indyk, “Near-optimal hashing algorithms for approx-
imate nearest neighbor in high dimensions,” Communications of the
ACM , vol. 51, 2008.
[3] P . Flajolet, ´E. Fusy, O. Gandouet, and F. Meunier, “Hyperloglog: the
analysis of a near-optimal cardinality estimation algorithm,” in AofA:
Analysis of Algorithms , 2007.
[4] E. D. Demaine, A. L ´opez-Ortiz, and J. I. Munro, “Frequency estimation
of internet packet streams with limited space,” in European Symposium
on Algorithms , 2002.
[5] J. Misra and D. Gries, “Finding repeated elements,” Science of computer
programming , vol. 2, 1982.
[6] G. Cormode and S. Muthukrishnan, “An improved data stream summary:
the Count-Min sketch and its applications,” Journal of Algorithms ,
vol. 55, 2005.
[7] J. S. Vitter, “Random sampling with a reservoir,” ACM Trans. Math.
Softw. , vol. 11, 1985.
[8] B. H. Bloom, “Space/time trade-offs in hash coding with allowable
errors,” Communications of the ACM , vol. 13, 1970.
[9] P . Drineas, R. Kannan, and M. W. Mahoney, “Fast monte carlo al-
gorithms for matrices i: Approximating matrix multiplication,” SIAM
Journal on Computing , vol. 36, 2006.
[10] N. Halko, P .-G. Martinsson, and J. A. Tropp, “Finding structure with
randomness: Probabilistic algorithms for constructing approximate ma-
trix decompositions,” SIAM review , vol. 53, 2011.
[11] J. Tropp, “An introduction to matrix concentration inequalities,” F oun-
dations and Trends in Machine Learning , vol. 8, 2015.
[12] P . Drineas and M. W. Mahoney, “RandNLA: randomized numerical
linear algebra,” Communications of the ACM , vol. 59, no. 6.
[13] H. Shatkay and S. B. Zdonik, “Approximate queries and representations
for large data sequences,” in ICDE , 1996.
[14] S. Agarwal, B. Mozafari, A. Panda, H. Milner, S. Madden, and I. Stoica,
“Blinkdb: queries with bounded errors and bounded response times on
very large data,” in EuroSys , 2013.
[15] S. Agarwal, H. Milner, A. Kleiner, A. Talwalkar, M. Jordan, S. Madden,
B. Mozafari, and I. Stoica, “Knowing when you’re wrong: building fast
and reliable approximate query processing systems,” in SIGMOD , 2014.
[16] K. Zeng, S. Gao, J. Gu, B. Mozafari, and C. Zaniolo, “Abs: a system for
scalable approximate queries with accuracy guarantees,” in SIGMOD’14 .
[17] I. Goiri, R. Bianchini, S. Nagarakatte, and T. D. Nguyen, “Approx-
hadoop: Bringing approximations to mapreduce frameworks,” in ASP-
LOS , 2015.
[18] M. Carbin, S. Misailovic, and M. C. Rinard, “V erifying quantitative re-
liability for programs that execute on unreliable hardware,” in OOPSLA ,
2013.
[19] A. Sampson, P . Panchekha, T. Mytkowicz, K. S. McKinley, D. Gross-
man, and L. Ceze, “Expressing and verifying probabilistic assertions,”
PLDI , 2014.
[20] S. Misailovic, M. Carbin, S. Achour, Z. Qi, and M. C. Rinard, “Chisel:
Reliability-and accuracy-aware optimization of approximate computa-
tional kernels,” in OOPSLA , 2014.
[21] M. Samadi, D. A. Jamshidi, J. Lee, and S. Mahlke, “Paraprox: Pattern-
based approximation for data parallel applications,” in ASPLOS , 2014.
[22] A. Raha, S. V enkataramani, V . Raghunathan, and A. Raghunathan,
“Quality conﬁgurable reduce-and-rank for energy efﬁcient approximate
computing,” in DATE , 2015.
[23] B. Boston, A. Sampson, D. Grossman, and L. Ceze, “Probability type
inference for ﬂexible approximate programming,” in OOPSLA , 2015.
[24] S. Graham, P . Kessler, and M. Mckusick, “Gprof: A call graph execution
proﬁler,” in SCC , 1982.
[25] J. Huang, B. Mozafari, and T. F. Wenisch, “Statistical analysis of latency
through semantic proﬁling,” in EuroSys , 2017.
[26] S. F. Goldsmith, A. S. Aiken, and D. S. Wilkerson, “Measuring empirical
computational complexity,” in ESEC/FSE , 2007.
[27] E. Coppa, C. Demetrescu, and I. Finocchi, “Input-sensitive proﬁling,”
PLDI , 2012.
[28] D. Zaparanuks and M. Hauswirth, “Algorithmic proﬁling,” PLDI , 2012.
[29] A. Arcuri and L. Briand, “A practical guide for using statistical tests to
assess randomized algorithms in software engineering,” in ICSE , 2011.[30] R. Motwani and P . Raghavan, Randomized algorithms . Chapman &
Hall/CRC, 2010.
[31] “TarsosLSH github.com/JorenSix/TarsosLSH .”
[32] A. Bogdanov, L. Trevisan et al. , “Average-case complexity,” F oundations
and Trends in Theoretical Computer Science , vol. 2, 2006.
[33] M. M. WagnerMenghin, Binomial Test , 2014.
[34] H. Cram ´er,Mathematical methods of statistics . Princeton university
press, 2016.
[35] A. Wald, “Sequential tests of statistical hypotheses,” The annals of
mathematical statistics , vol. 16, 1945.
[36] R. A. Fisher, Statistical methods for research workers . Oliver and Boyd,
1925.
[37] H. L. Y ounes, M. Kwiatkowska, G. Norman, and D. Parker, “Numerical
vs. statistical probabilistic model checking,” International Journal on
Software Tools for Technology Transfer , vol. 8, 2006.
[38] C. John, W. Cleveland, B. Kleiner, and P . Tukey, “Graphical methods
for data analysis,” Wadsworth, Ohio , pp. 128–129, 1983.
[39] S. Mitra, G. Bronevetsky, S. Javagal, and S. Bagchi, “Dealing with the
unknown: Resilience to prediction errors,” in PACT , 2015.
[40] D. Reshef, Y . Reshef, H. Finucane, S. Grossman, G. McV ean, P . Turn-
baugh, E. Lander, M. Mitzenmacher, and P . Sabeti, “Detecting novel
associations in large data sets,” Science , vol. 334, 2011.
[41] “java-LSH github.com/tdebatty/java-LSH .”
[42] “libbf: Bloom ﬁlters for C++11 mavam.github.io/libbf .”
[43] “java-bloomﬁlter: github.com/MagnusS/Java-BloomFilter .”
[44] “Count-Min Sketch github.com/alabid/countminsketch .”
[45] “CountMinSketch github.com/AWNystrom/CountMinSketch .”
[46] “Sketches Library from Yahoo datasketches.github.io .”
[47] “Datasketch github.com/ekzhu/datasketch/ .”
[48] “sample github.com/alexpreynolds/sample .”
[49] “Randomized Matrix Product github.com/NumericalMax/
Randomized-Matrix-Product .”
[50] “mcschttps://github.com/gnu-user/mcsc-6030-project .”
[51] S. Misailovi ´c, “Accuracy-aware optimization of approximate programs,”
Ph.D. dissertation, Massachusetts Institute of Technology, 2015.
[52] E. Jones, T. Oliphant, P . Peterson et al. , “SciPy: Open source scientiﬁc
tools for Python,” 2001–. [Online]. Available: http://www.scipy.org/
[53] E. Cohen, “All-distances sketches, revisited: Hip estimators for massive
graphs analysis,” IEEE Transactions on Knowledge and Data Engineer-
ing, vol. 27, 2015.
[54] M. Levandowsky and D. Winter, “Distance between sets,” Nature , vol.
234, 1971.
[55] C. Curtsinger and E. D. Berger, “Coz: ﬁnding code that counts with
causal proﬁling,” in SOSP , 2015.
[56] G. Xu and A. Rountev, “Precise memory leak detection for java software
using container proﬁling,” in ICSE , 2008.
[57] E. Raman and D. I. August, “Recursive data structure proﬁling,” in
Workshop on Memory system performance , 2005.
[58] T. M. Chilimbi, B. Liblit, K. Mehra, A. V . Nori, and K. V aswani,
“Holmes: Effective statistical debugging via efﬁcient path proﬁling,” in
ICSE , 2009.
[59] L. Song and S. Lu, “Statistical debugging for real-world performance
problems,” in OOPSLA , 2014.
[60] A. Zheng, M. Jordan, B. Liblit, M. Naik, and A. Aiken, “Statistical
debugging: Simultaneous identiﬁcation of multiple bugs,” in ICML’06 .
[61] S. Misailovic, S. Sidiroglou, H. Hoffmann, and M. Rinard, “Quality of
service proﬁling,” in ICSE , 2010.
[62] M. Carbin and M. Rinard, “Automatically identifying critical input
regions and code in applications,” in ISSTA , 2010.
[63] S. Misailovic, D. Kim, and M. Rinard, “Parallelizing sequential pro-
grams with statistical accuracy tests,” ACM Transactions on Embedded
Computing Systems (TECS) , vol. 12, 2013.
[64] C. Rubio-Gonz ´alez, C. Nguyen, H. D. Nguyen, J. Demmel, W. Kahan,
K. Sen, D. H. Bailey, C. Iancu, and D. Hough, “Precimonious: Tuning
assistant for ﬂoating-point precision,” in SC, 2013.
[65] M. Ringenburg, A. Sampson, I. Ackerman, L. Ceze, and D. Grossman,
“Monitoring and debugging the quality of results in approximate pro-
grams,” in ASPLOS , 2015.
[66] K. Sen, M. Viswanathan, and G. Agha, “Statistical model checking of
black-box probabilistic systems,” in CA V , 2004.
[67] ——, “On statistical model checking of stochastic systems,” in CA V ,
2005.
618
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:37:17 UTC from IEEE Xplore.  Restrictions apply. 