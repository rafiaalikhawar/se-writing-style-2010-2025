Are Fix-Inducing Changes a Moving Target?
A Longitudinal Case Study of Just-In-Time Defect Prediction
Shane McIntosh
McGill University
Montréal, Canada
shane.mcintosh@mcgill.caYasutaka Kamei
Kyushu University
Fukuoka, Japan
kamei@ait.kyushu-u.ac.jp
ABSTRACT
Change-level defect prediction [ 5], a.k.a., Just-In-Time (JIT) defect
prediction[ 1],isanalternativetomodule-leveldefectprediction
thatoffersseveraladvantages.First,sincecodechangesareoften
smallerthanmodules(e.g.,classes),JITpredictionsaremadeata
finer granularity, which localizes the inspection process. Second,
while modules have a group of authors, changes have only one,
whichmakestriagingJITpredictionseasier.Finally,unlikemodule-
level prediction, JIT models can scan changes as they are being
produced, which means that problems can be investigated while
design decisions are still fresh in the developers’ minds.
DespitetheadvantagesofJITdefectprediction,likeallprediction
models,theyassumethatthepropertiesofpastevents(fix-inducing
changes) are similar to the properties of future ones. This assump-
tionmaynothold—thepropertiesoffix-inducingchangesinone
timeperiodmaybedifferentfromthoseofanotherperiod.Inour
paper [4], we set out to address the following central question:
Do the important properties of fix-inducing
changes remain consistent as systems evolve?
Toaddressourcentralquestion,wetrainJITmodelsusingsix
families of code change properties, which are primarily derived
from prior studies [ 1–3,5]. These properties measure: (a) the mag-
nitudeofthechange(Size);(b)thedispersionofthechangesacross
modules (Diffusion); (c) the defect proneness of prior changes tothe modified modules (History); (d) the experience of the author
(Auth. Exp.) and (e) code reviewer(s) (Rev. Exp.); and (f) the amount
of participation in the review of the code change (Review).
Through a longitudinal case study of 37,524 changes from the
rapidly evolving QtandOpenStack systems, we find that the
answer to our central question is no:
JIT models lose a large proportion of their discriminatorypower (AUC) and calibration (Brier) scores one year after
being trained.
The magnitude of the importance scores of code change
propertiesfluctuateassystemsevolve(e.g.,Figure1shows
fluctuations across six-month periods of OpenStack).
These fluctuations can lead to consistent overestimates (and
underestimates) of the future impact of the studied families
of code change properties.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.
For all other uses, contact the owner/author(s).
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5638-1/18/05.
https://doi.org/10.1145/3180155.3182514Short−period Long−period
0.16
**
0.06
0.19
***
0.02
0.11
*
0.27
***0.16
***
0.03
0.12
***
0
0.06
*
0.2
***0.14
***
0.01
0.06
*
0
0.09
**
0.38
***0.18
***
0.17
***
0.08
***
0.01
0.16
***
0.08
**0.25
***
0.11
***
0.02
0
0.23
***
0.19
***0.16
**
0.06
0.19
***
0.02
0.11
*
0.27
***0.16
***
0.05
**
0.14
***
0.01
0.06
**
0.32
***0.15
***
0.03
*
0.08
***
0.01
0.07
***
0.37
***0.17
***
0.07
***
0.06
***
0
0.09
***
0.25
***0.19
***
0.08
***
0.05
***
0
0.11
***
0.24
***
ReviewRev. Exp.Auth. Exp.HistoryDiffusionSize
12345 12345
Training Period
0.1 0.2 0.3Explanatory Power
Figure 1: The importance of code change properties over
time inOpenStack. Shade indicates magnitude while aster-
isks indicate that Wald χ2t e s tp<*0.05, ** 0.01, *** < 0.001.
To mitigate the impact on model performance, researchers and
practitioners should add recently accumulated data to the training
set and retrain JIT models to contain fresh data from within thelastthreemonths.Tobettercalibratequalityimprovementplans
(which are based on interpretation of the importance scores of
codechangeproperties),researchersandpractitionersshouldputagreateremphasisonlargercachesofdata,whichcontainatleastsix
monthsworthofdata,tosmooththeeffectofspikesandtroughs
in the importance of properties of fix-inducing changes.
ACM Reference Format:
Shane McIntosh and Yasutaka Kamei. 2018. Are Fix-Inducing Changes a
Moving Target?. In Proceedings of ICSE ’18: 40th International Conference on
SoftwareEngineering,Gothenburg,Sweden, May27-June3,2018(ICSE’18),
1 pages.
https://doi.org/10.1145/3180155.3182514
REFERENCES
[1]Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E. Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. 2013. A Large-Scale Empirical Study of
Just-in-Time Quality Assurance. IEEE Transactions on Software Engineering (TSE)
39, 6 (2013), 757–773.
[2]Sunghun Kim, E. James Whitehead, Jr., and Yi Zhang. 2008. Classifying Software
Changes: Clean or Buggy? IEEE Transactions on Software Engineering (TSE) 34, 2
(2008), 181–196.
[3]Oleksii Kononenko, Olga Baysal, Latifa Guerrouj, Yaxin Cao, and Michael W.
Godfrey. 2015. Investigating Code Review Quality: Do People and Participation
Matter?. In Proc. of the 31st Int’l Conf. on Software Maintenance and Evolution
(ICSME). 111–120.
[4]Shane McIntosh and Yasutaka Kamei. 2017. Are Fix-Inducing Changes a Moving
Target? A Longitudinal Case Study of Just-In-Time Defect Prediction. IEEE
TransactionsonSoftwareEngineering (2017),Toappear. https://doi.org/10.1109/
TSE.2017.2693980
[5]Audris Mockus and David M. Weiss. 2000. Predicting Risk of Software Changes.
Bell Labs Technical Journal 5, 2 (2000), 169–180.
5602018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. 