DeepRoad: GAN-Based Metamorphic Testing and Input
Validation Framework for Autonomous Driving Systems
Mengshi Zhang∗
University of Texas at Austin
USA
mengshi.zhang@utexas.eduYuqun Zhang†
Shenzhen Key Laboratory of
Computational Intelligence,
Department of Computer Science and
Engineering, Southern University of
Science and Technology
China
zhangyq@sustc.edu.cnLingming Zhang
University of Texas at Dallas
USA
lingming.zhang@utdallas.edu
Cong Liu
University of Texas at Dallas
USA
cong@utdallas.eduSarfraz Khurshid
University of Texas at Austin
USA
khurshid@utexas.edu
ABSTRACT
WhileDeepNeuralNetworks(DNNs)haveestablishedthefunda-
mentals of image-based autonomous driving systems, they may ex-
hibit erroneous behaviors and cause fatal accidents. To address the
safetyissuesinautonomousdrivingsystems,arecentsetoftesting
techniques have been designed to automatically generate artificial
drivingscenestoenrichtestsuite,e.g.,generatingnewinputimages
transformed from the original ones. Ho wever,these techniques are
insufficientduetotwolimitations:first,manysuchsyntheticimages
oftenlackdiversityofdrivingscenes,andhencecompromisethere-
sulting efficacy and reliability. Second, for machine-learning-basedsystems, a mismatch between training and application domain can
dramaticallydegrade systemaccuracy, suchthat itis necessaryto
validate inputs for improving system robustness.
In this paper, we propose DeepRoad, an unsupervised DNN-
basedframeworkforautomaticallytestingtheconsistencyofDNN-
based autonomous driving systems and online validation. First,
DeepRoad automatically synthesizes large amounts of diverse driv-
ing scenes without using image transformation rules (e.g. scale,
shearandrotation).Inparticular,DeepRoadisabletoproducedriv-
ing scenes with various weather conditions (including those with
ratherextremeconditions)byapplyingGenerativeAdversarialNet-
works (GANs) along with the corresponding real-world weather
scenes. Second, DeepRoad utilizes metamorphic testing techniques
∗This work was partially accomplished during visit to Southern University of Science
and Technology
†Yuqun Zhang is the corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238187tochecktheconsistencyofsuchsystemsusingsyntheticimages.
Third, DeepRoad validates input images for DNN-based systems
bymeasuringthedistanceoftheinputandtrainingimagesusing
their VGGNet features. We implement DeepRoad to test three well-
recognized DNN-based autonomous driving systems in Udacity
self-drivingcarchallenge.Theexperimentalresultsdemonstrate
that DeepRoad can detect thousands of inconsistent behaviors for
thesesystems,andeffectivelyvalidateinputimagestopotentially
enhance the system robustness as well.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
Software testing, Test generation, Input validation, Deep Neural
Networks
ACM Reference Format:
Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz
Khurshid. 2018. DeepRoad: GAN-Based Metamorphic Testing and Input
Validation Framework for Autonomous Driving Systems. In Proceedings of
the 2018 33rd ACM/IEEE International Conference on Automated Software
Engineering (ASE ’18), September 3–7, 2018, Montpellier, France. ACM,New
York, NY, USA, 11pages.https://doi.org/10.1145/3238147.3238187
1 INTRODUCTION
Thetraincameoutofthelongtunnelintothesnow
country.Theearthlaywhiteunderthenightsky.The
train pulled up at a signal stop.
—Yasunari Kawabata, Snow Country
Theabovequotationisfromthefirstparagraphoffiction“Snow
Country”, which describes the scene when the protagonist Shima-
mura enters the snow country. Back to that time, train was the
major vehicle for long-distance travels, while people have more
choices today. Now, suppose Shimamura takes a Tesla in Autopliot
mode [2], after coming out of the tunnel, there raises a question:
132
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid
can the autopilot system operate safely on the snow-covered road,
or the story just ends with a tragedy?
Autonomous driving is expected to transform auto industry.
Typically, autonomous driving refers to utilizing sensors (cameras,
Radar,Lidar,GPS,etc)[ 38]toautomaticallycontrolvehicleswithout
humanintervention.TherecentadvancesinDeepNeuralNetworks
(DNNs) enable autonomous driving systems to adapt their driving
behaviorsaccordingtodynamicenvironments[ 2,14].Inparticular,
an end-to-end supervised learning framework is made possible
totrainaDNNforpredictingdrivingbehaviors(e.g.,steeringan-
gles) by inputing driving images, using /angbracketleftdriving image, driving
behavior/angbracketrightpairs as training data. For instance, DAVE-2 [ 14], re-
leasedbyNVIDIAin2016,canaccuratelypredictsteeringangles
basedononlyimagescapturedbyasinglefront-centeredcamera
of autonomous cars.
Recenttestingtechniques[ 31,38]demonstratethattheautonomous
driving systems are error-prone to synthetic images of driving
scenes. DeepXplore [ 31] applies differential testing technique to
systematically generate images which disclose the inconsistent be-
haviors of multiple DNN systems. Specifically, it formulates the
image generation problem as a joint optimization problem, which
uses gradient-based search techniques to find images for maximiz-
ing neuron coverage and the number of inconsistent behaviors of
suchsystems. DeepTest [ 38]designs systematicwaysto automati-
callygeneratetestcases,seekingtomimicreal-worlddrivingscenes.
Its main methodology is to transform labeled images of driving
scenesbyapplyingsimpleaffinetransformationsandvariouseffectfilterssuchasblurring/fog/raintotheoriginalimages,andcheckif
theautonomousdrivingsystemsperformconsistentlyamongthe
original and transformed scenes. With large amounts of originaland transformed driving scenes, DeepTest can detect various er-
roneousdrivingbehaviorsforsomewell-performedopen-source
autonomous driving models, in a cheap and quick manner.
However, we observe that the methodologies applied in DeepX-
ploreandDeepTesttogeneratetestcasesmaynotaccuratelyreflect
the real-world driving scenes, which can rarely contain coloredpatch or black holes and sidelines; the blurring/fog/rain effectsmade by simple simulation also appear to be unrealistic, whichcompromises their efficacy and reliability. For instance, Figure 1
showsthesyntheticimageswhicharequotedfromthepapersof
DeepXplore and DeepTest. Note that the colored arrows are at-
tached to present the predicted steering angles. From Figure 1a,1b
and1c,itcanbeobservedthattheimagesofdrivingscenesinclude
several artifacts (patch, holes and sidelines), which significantlyhurt the image quality. Moreover, for Figure 1d, it appears to be
synthesizedbysimplydimmingtheoriginalimageandmixingit
with the scrambled “smoke" effect and it violates the facts that the
density of fog varies along depth. Similarly, in Figure 1e, DeepTest
simplysimulatesrainbyaddingagroupoflinesovertheoriginal
image. This rain effect transformation is even more distorted be-
cause usuallywhen itrains, the cameraor frontwindshield tends
to be wet and the image is highly possible to be blurred. These
factsshowthatitisdifficulttodeterminewhethertheerroneous
driving behaviorsare caused by theflaws of the DNN-basedmod-
els, or theinadequacy of the testing techniqueitself. Furthermore,
thesetransformations(e.g.translation,shearandrotation)canonlygenerate similar images, while they cannot sophisticatedly syn-
thesize images with different styles and thus limit the diversity of
testcases.Forinstance,thesnowyroadconditiondemandsdiffer-
entcomplicatedtransformationsforrenderingthetextureofroad
and roadside objects (such as trees), and it cannot be generated by
simple transformation rules.
Fortraditionalsoftware,inputvalidation(IV)isanimportantstep
before executing programs. For instance, in web applications, IV
checks and filters illegal or malicious inputs to prevent application-level attacks such as buffer overflow and code-injection attack [
25].
However,tothebestofourknowledge,currentDNN-basedsystems
lack to validate inputs (e.g., images of driving scenes) and thustends to cause system vulnerability. Specifically, invalid inputssuch as outlier images of driving scenes can highly degrade the
prediction accuracy and dramatically increase the risks of DNN-
based systems. For example, suppose a DNN-based autonomous
drivingsystemistrainedonadatasetwhichonlyincludesimagesof
sunnydrivingscenes. Forout-of-domaininputs(e.g. rainyimages
ofdrivingscenes)thatthesystemisnottrainedwith,itishighly
possible that the system outputs wrong control signals which lead
to danger for drivers and passengers.
Toaddressaboveissues,inthispaper,weproposeanunsuper-
vised learning framework, namely DeepRoad, to systematically
analyze DNN-based autonomous driving systems. DeepRoad is
composed of a metamorphic testing module, DeepRoad MTand an
input validation module, DeepRoad IV. DeepRoad MTemploys a
GenerativeAdversarialNetwork(GAN)-basedtechnique[ 18,27]to
synthesize driving scenes with various weather conditions, and de-
velops a metamorphic testing module for DNN-based autonomous
driving systems. Specifically, the metamorphic relations are de-
finedsuchthatnomatterhowthedrivingscenesaresynthesized
to cope with different weather conditions, the driving behaviors
are expected to be consistent with those under the corresponding
originaldrivingscenes.Atthispoint,DeepRoad MTenablesusto
testtheaccuracyandreliabilityofDNN-basedautonomousdriving
systems under different scenarios, including heavy snow and hard
rain,whichcangreatlycomplementtheexistingapproaches(e.g.,
DeepXplore, DeepTest). For instance, Figure 2presents the snowy
and rainy scenes generated by DeepRoad MT(from sunny scenes),
whichcan hardlybedistinguishedfromgenuineonesand cannot
be generated using simple transformation rules. DeepRoad IVis
designed to validate inputs for DNN-based autonomous driving
systemsbasedonimagesimilarity.Firstly,DeepRoad IVappliesa
pre-trained DNN model–VGGNet to extract high-level features (i.e.
contentsandstyles)ofbothtrainingandtestinputimages.Then,
thePrincipleComponentAnalysis(PCA)techniqueisappliedon
thesefeaturesfordimensionreduction.Finally,DeepRoad IVvali-
datesinputsbycomparingtheaveragedistancebetweentraining
and input images with a preset threshold.
ToevaluatetheeffectivenessofDeepRoad,wefirstsynthesize
drivingscenesunderheavysnowandhardrain.Inparticular,based
onGAN,wecollectimageswithtwoextremeweatherconditions
fromYouTubevideostotransformreal-worlddrivingscenes,and
deliver them with the corresponding weather conditions. Subse-
quently,thesesyntheticscenesareusedtotestthreeopen-source
DNN-based autonomous driving systems from Udacity commu-
nity [7]. The experimental results reveal that DeepRoad MTcan
133
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for ... ASE ’18, September 3–7, 2018, Montpellier, France
(a) Patch
 (b) Holes
 (c) Translation
 (d) Fog
 (e) Rain
Figure 1: Driving scenes synthesized by DeepXplore (a)(b) and DeepTest (c)(d)(e)
(a) Snow
 (b) Rain
Figure 2: Snowy and rainy scenes synthesized by DeepRoad
effectivelydetectthousandsofinconsistentbehaviorsofdifferent
levels for these systems. Furthermore, we use DeepRoad IVto vali-
datetheinputimagessampledfromdifferentdrivingscenes.The
results demonstrate that in the embedding space, the cluster of the
rainyandsnowyimagepointsareseparatelydistributedtotheclus-
ter of training images, however, the training cluster is mixed with
the majority of the sunny image points. It indicates that given a
properthreshold,DeepRoad IVcaneffectivelyvalidateinput,which
potentially improve the system robustness.
The key contributions of this paper are as follows.
•We propose the first GAN-based metamorphic testing ap-
proachtogeneratedrivingsceneswithvariousweathercon-
ditions for detecting inconsistent behaviors of autonomous
driving systems.
•We propose a novel approach to validate inputs for DNN-
basedautonomous drivingsystem. We presentthat thedis-
tancebetweenthehigh-levelfeaturesoftrainingandinput
images can be used for validating inputs.
•WeimplementtheproposedapproachesinDeepRoad,which
can generate images of diverse driving scenes (e.g. rain and
snow)andmeasurethesimilaritybetweenmultipleimage
sets in embedding space. We use DeepRoad to test well-
recognizedDNN-basedautonomousdrivingmodelsandsuc-
cessfully detect thousands of inconsistent driving behaviors.
Additionally,DeepRoadcanaccuratelydistinguishimages
with extreme weather conditions to the training images,
whichiseffectivetovalidateinputforautonomousdriving
systems.
2 BACKGROUND
Autonomous driving systems have been rapidly evolving in recent
years [14,32]. For example, many major auto manufacturers (in-
cluding Tesla, GM, Volvo, Ford, BMW, Honda, and Daimler) and IT
companies (includingWaymo/Google, Uber, andBaidu) arework-
ingonbuildingandtestingvariousautonomousdrivingsystems.Typically, autonomous driving systems capture data from environ-
mentviamultiplesensors(e.g.camera,Radar,Lidar,GPU,IMU,etc.)
as input, and use Deep Neural Networks (DNNs) to process data
and output control signals (e.g. steering and braking decisions). In
NVIDIA’s work [ 14], their autonomous driving system, DAVE-2
can fluently control cars only based on the images captured by a
singlefront camera.In thiswork, wemainlyfocus onDNN-based
autonomousdrivingsystemswithcamerainputsandsteeringangle
outputs.
2.1 DNN Architectures
To date, Convolutional Neural Network (CNN) [ 23] and Recurrent
Neural Network (RNN) [ 33] are the most widely used DNNs for
autonomousdrivingsystems.Typically,CNNsaregoodatanalyzing
visualimageryandRNNscaneffectivelyprocesssequentialdata.
In this work, the evaluated models are built on CNN and RNN
modules. We briefly introduce the basic concepts and components
ofeacharchitectureasfollows,wheremoredetailsaboutDNNsare
provided in [24].
2.1.1 Convolutional Neural Networks. ConvolutionalNeuralNet-
worksaresimilartoregularneuralnetworks,whichincludealarge
amount of neurons and pass information in a feed-forward way.
However, since the input data are images, several properties canbe applied to optimize the regular neural networks, where con-
volutional layer is a key component in CNNs. Instead of being
fully connected, a neuron in a layer only connects to some neu-
rons in the previous layer, and the computational process can be
presented as a convolution with kernels. Figure 3ashows an ex-
ampleofCNN-basedautonomousdrivingsystemthatconsistsof
an input layer (images) and an output layer (steering angles), as
well as multiple hidden layers. Convolution hidden layers allowweightsharingacrossmultipleconnectionsandcangreatlysave
the training efforts.
2.1.2 Recurrent Neural Networks. Regular neural networks and
CNNsaredesignedtoprocessindependentdata,suchasusingCNN
to classify images. However, for sequential data like videos, the
neuralnetworksshouldnotonlycaptureinformationofeachsingleframe,butarealsoexpectedtomodeltheconnectionsbetweenthem.
Unlike regular NNs and CNNs, RNN is a kind of neural network
withfeedbackconnections. Asshowninthe leftpartofFigure 3b,
RNNs use loops to forward the previous states to input, whichmodel the connection of input data. The right part of Figure 3b
showstheworkflowoftheunfoldedRNNforpredictingsteering
angles based on a sequence of images. At each step, RNN takes
134
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid
(a) CNN architecture
(b) RNN architecture
Figure 3: Autonomous driving systems built on CNN and
RNN
the current input image and previous hidden states as input and
predicts the steering angle.
DNN-based autonomous driving systems are essentially soft-
ware systems, which are error-prone and can lead to tragedies.
Forexample,onJanuary,2018,aTeslaModelSplowedintoafire
truckat65mphwhileusingAutopilot[ 9].AndonMar,2018,anau-
tonomous Uber failed to slow down and killed a pedestrian during
roadtestatnight[ 10].Toensurethequalityofsoftwaresystems,
many software testing techniques have been proposed in the lit-erature [
11,29], where typically, a set of specific test cases are
generated to test if the software programs perform as expected.
The process of determining whether the software program per-
formsasexpecteduponthegiventestinputsisknownasthe test
oracleproblem [ 11]. Despite the abundance of traditional software
testing techniques, they cannot be directly applied for DNN-based
systems since the logics of DNN-based softwares are learned from
datawithminimalhumaninterference(likeablackbox)whilethe
logics of traditional software programs are manually created.
3 APPROACH
3.1 Metamorphic Testing for DNN-Based
Autonomous Driving Systems
3.1.1 Metamorphic DNN Testing. Metamorphic Testing [ 35] (MT)
has been widely used to automatically generate tests to detect soft-
warefaults.ThestrengthofMTliesinitscapabilitytoautomatically
solve the test oracle problem via Metamorphic Relations (MRs). In
particular, let pbe a program mathematical representation that
mapsprograminputstoprogramoutputs(e.g., p/dblbracketlefti/dblbracketright=o).Assum-
ingfIandfOaretwospecificfunctionsfortransformingtheinput
and output domain respectively, and they satisfy the following MR
formulation:
∀i,p/dblbracketleftfI(i)/dblbracketright=fO(p/dblbracketlefti/dblbracketright) (1)
, whereidenotes the input of program p.
With such MRs, we can test a specific implementation ˆpofp
bycheckingwhether ˆp/dblbracketleftfI(i)/dblbracketright=fO(ˆp/dblbracketlefti/dblbracketright)forvariousinput i.A c-
cordingly, MT is defined as testing a program implementation viacross-checkinginputsandoutputswithMRs.Forinstance,given
a program implementing function sine, MT can be used to delin-
eate test oracles and create various new tests. For any existing
inputito test function sine, various facts can serve as MRs, e.g.,
sin(−i)=−sin(i)andsin(i+2π)=sin(i). Thesefacts can befor-
mulatedas1) fI(x)=fO(x)=−x,2)fI(x)=x+2πandfO(x)=x.
WithsuchMRs,wecantransformtheexistingtestinputsaccording
tofIto generate additional tests, and check the output based on
fO. For instance, suppose the default test case of function sineis
AssertTrue(sin(0.5 ·Pi), 1.0) . Based on above MRs, we can
generate two extra tests: AssertTrue(sin(-0.5 ·Pi), -1.0) and
AssertTrue(sin(2.5 ·Pi), 1.0).
Inthiswork,wefurtherapplyMTtotestDNN-basedautonomous
drivingsystems.Formally,denote DNNasaDNN-basedautonomous
driving system that continuously maps each image into predicted
steering angle signal (e.g., turning left for 15◦). One MR can be
definedasgiventheoriginalimagestream I,variousimagetrans-
formations Tcansimplychangetheroadscenes(detailedshown
in Section 3.1.2) without impacting the predictions for each image
i∈I(e.g.,thepredicteddirectionshouldbeapproximatelythesame
on the same road under different weather conditions). This MR to
test DNN with additional transformed inputs can be formalized as
follows:
∀i∈I∧∀τ∈T,DNN /dblbracketleftτ(i)/dblbracketright=DNN /dblbracketlefti/dblbracketright (2)
3.1.2 DNN-Based Road Scene Transformation. The recent work
DeepTest [ 38] also applied MT to test DNN-based autonomous
drivingsystems.However,itonlyperformsbasicimagetransforma-tions,suchasaddingsimpleblurring/fog/raineffectfilters,andthus
has the following limitations: (1) DeepTest may generate images
whichviolatecommonscenes(discussedinSection 1).(2)DeepTest
cannotsimulatecomplexroadscenetransformations(e.g.,snowy
scenes).
TocomplementDeepTestbyautomaticallygeneratingvarious
real-worldroadscenes,inthiswork,weleverageUNIT[ 27],arecent
published DNN-basedmethod toperform unsupervised image-to-
imagetransformationbasedonGenerativeAdversarialNetworks
(GANs)[18]andVariationalAutoencoders(VAEs)[ 22].Oneinsight
of UNIT is that suppose two images contain the same contents but
lieindifferentdomains,theyshouldhavethesamerepresentations
in a shared-latent space. Accordingly, given a new image from one
domain(e.g.,theoriginaldrivingscene),UNITcanautomatically
generate its corresponding version in the other domain (e.g., rainy
driving scene).
Figure4[27] presents the structure of UNIT, S1andS2denote
twodifferentdomains(e.g.,sunnyandrainydrivingscenes), E1and
E2denote two autoencoders which project the images from S1and
S2to a shared-latent space Z. Suppose x1andx2are paired images
which share the same content. Ideally, E1andE2would encode
themtothesamelatentvector z,anditcanbetranslatedbackto S1
andS2by two domain specific generators G1andG2, respectively.
D1andD2are two discriminators which detect whether the image
belongs to S1andS2respectively. Specifically, they are expected
to differentiate whether the input image is sampled from target
domain(e.g.realimage)orproducedbyawell-trainedgenerator
(e.g.syntheticimage).Basedontheautoencodersandgenerators,
135
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for ... ASE ’18, September 3–7, 2018, Montpellier, France
UNITcanbeusedtotransformimagesbetweentwodomains.For
instance, image x1can be transformed to S2byG2(E1(x1)).
	

 
 
 
Figure 4: Structure of UNIT
In UNIT, all Di,EiandGiare incarnated as neural networks,
and the learning objective of UNIT can be decomposed to optimize
the following costs:
•VAE loss: minimizingthelossofimagereconstructionfor
each/angbracketleftEi,Gi/angbracketright.
•GAN loss: achieving the equilibrium point in the minimax
game for each/angbracketleftGi,Di/angbracketright, whereDiaims at discriminating
the images to find out whether they are sampled from the
domainSior produced by Githat aims at fooling Di.
•Cycle-consistency loss: minimizing the loss of cycle- re-
constructionforeach /angbracketleftEi,Gj,Ej,Gi/angbracketright,wherex1isexpected
toequalto G1(E2(G2(E1(x1))))andx2isexpectedtoequal
toG2(E1(G1(E2(x2)))).
The total loss can be summarized as follows:
min
E1,E2,G1,G2max
D1,D2LCC1(E1,G2,E2,G1)
+LCC2(E2,G1,E1,G2)
+LVA E1(E1,G1)+LVA E2(E2,G2)
+LGAN1(D1,G1)+LGAN2(D2,G2)
, and this loss function can be optimized using Stochastic Gradient
Descent algorithm.
Figure 5: Framework of DeepRoad MT3.1.3 Framework of DeepRoad MT.Figure5showstheoverallde-
sign of our metamorphic testing framework for DNN-based au-
tonomousdrivingsystems–DeepRoad MT.InFigure 5,DeepRoad MT
first takes unpaired training images from two target domains (e.g.,
datasetsofthedrivingsceneundersunnyandsnowyweatherre-
spectively), and utilizes UNIT to project two domains to the same
latent space by optimizing the loss functions presented in Sec-
tion3.1.2.Whenthetraining processfinished,DeepRoad MTuses
the well-trained model to transform the whole dataset of sunnydriving scenes to snowy weather. Specifically, given any image
under sunny weather i, DeepRoad MTencodes it to vector zibyE1,
and synthesizes its corresponding version under snowy weather
τ(i)usingG2.DeepRoad MTfeedseachpairofrealandsynthetic
driving scene images to the autonomous driving systems under
test, i.e.,DNN, and compare their prediction results DNN /dblbracketleftτ(i)/dblbracketright
andDNN /dblbracketlefti/dblbracketrighttodetectanyinconsistentbehaviors.Normally,the
transformeddrivingscenesareexpectednottosignificantlyimpact
the predicted steering angles, and any inconsistency may indicate
correctness or robustness issues of the systems under test [ 31,38].
3.2 Input Validation for DNN-Based
Autonomous Driving Systems
Driving scenes synthesized by DeepTest and DeepXplore can be
usedastestcasestotestDNN-basedautonomousdrivingsystemsin
an offline manner. Though these test cases are useful to expose the
systemvulnerabilityandadvisedeveloperstocomplementtraining
data from real world to improve the system robustness, it is not
sufficientforonlinetesting.Forinstance,aDNN-basedautonomous
driving system can be well trained and perfectly function in sunny
environments, yet it might perform incorrectly at night or on a
snow-covered road, because the lane marks it detected for guiding
carsdisappearinsuchdrivingscenes.Thisexamplesuggeststhat
if the system can validate input images online, and actively advise
drivers to control the car when it cannot handle the invalid inputs,
theautonomousdrivingsystemscanbecomesaferandmorerobust.
In the following, we first define the criteria of input validation for
DNNs(especiallyimage-orientedmodels),andpresentourinput
validation framework for DNN-based autonomous driving systems.
3.2.1 Input validation of DNNs. Thegoalofinputvalidation(IV)is
toensurethatonlyproperlyformeddatacanbeacceptedbysystems,
andmalformeddatashouldberejectedbeforeexecution.Thereason
is that an invalid input may trigger malfunction of downstream
components,whichmakesthesysteminsecure.Generally,thevalidinputofaprogramcanbeexplicitlydefinedsuchastheinputstring
should not be null/empty or the value of a certain input variable
shouldbegreaterthan0.However,itisnottrivialtoproperlydefine
inputvalidityofaDNN-basedprogram.Forexample,wecandefine
an IV criteria as the input data should be any RGB image with size
640*480,oranyinputdatashouldexistinthetrainingdatasetto
guaranteethecorrectness.However,noneofthemarepropersince
the first criteria is too weak to improve system robustness, and the
second is so strong that makes the system lack generalisability.
WedefinetheIVcriteriaofDNN-basedprogrambasedonthe
ProbablyApproximatelyCorrect(PAC)Learningtheory.According
to the PAC Learning theory [ 13], a machine learning model Λis
expected to learn the distribution Dfrom the training dataset,
136
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid
and predict the correct label with high probability.1This can be
formulated as follows:
E(Λ;D)=Prx∼D(Λ(x)/nequaly) (3)
Pr(E≤ϵ)≥1−σ (4)
InFormula 3,Edenotesthattheprobabilityof Λmakesincorrect
prediction ( Λ(x)/nequaly) on input data xsampled from D, and in
Formula4,ϵandσare two parameters between 0 and 1, such that
it is highly possible (greater that 1 −σ)Eis small (less than ϵ),
whichmeans Λiseffectiveon D.Basedontheaboveequations,we
firstdefineanabstractIVcriteriaofDNN-basedsystemsisthatthe
inputdatashouldbesampledfrom D.Asdiscussedbefore,suppose
theinputdataisnotsampledfrom D,theIVcriteriaisviolatedand
thepredictionaccuracyisnotguaranteed.Therefore,itisnecessary
tovalidateinputstoimprovetherobustnessofDNN-basedsystems.
Intuitively, the IV criteria should be instantiated as:
Prx∼D(x=i)>θ (5)
, which means the probability of input ibeing sampled from D
shouldbegreaterthan thepredefinedthreshold θ.Otherwise,the
system refuses to predict on i. However, this definition is not
tractable for image data, because image data is highly dimensional
andtheirdistribution(e.g.GaussianMixturemodel)isdifficulttobe
explicitly represented. To address this issue, we project image data
to a low-dimensional space and use the distance between inputsand training data to replace
Prx∼D(x=i). In particular, accord-
ing to the Manifold Learning theory [ 13], the images generated
byDcan be embedded into a non-linear low-dimension manifold
MD. Suppose the input data iis sampled from D, its projection
ipshould be included by MD. Furthermore, we propose an extra
constraint for the non-linear embedding that suppose the input
data are generated by a different distribution D/prime, their projections
are expected to be included by another manifold MD/prime, which is
linearlyseparableto MD.Basedontheconstraint,wecancompute
theminimaldistanceof ipandtheprojectionsoftrainingdatato
validateif ipbelongsto MD.TheIVcriteriaisredefinedasfollows:
min
j/bardblh(i)−h(j)/bardbl2<θ/prime(6)
, where/bardbl·/bardbl2denotesL2norm,h(·)denotes the required non-liner
projectionand θ/primedenotespredefinedthresholdforinputvalidation.
IftheinputsatisfiesEquation 6,itwillbeprocessedbyDNNsfor
prediction, otherwise, it will be rejected.
3.2.2 Framework of DeepRoad IV.We propose DeepRoad IV, an in-
putvalidationframeworkforautonomousdrivingsystems.DeepRoad IV
separatetheprojection h(·)totwoparts:non-lineartransformation
and dimension reduction. For the first part, DeepRoad IVapplies
VGGNet [ 37], a widely used DNN [ 17,20] to extract high-level fea-
turesfromeachimage.Tobespecific,theinputimageisencoded
in each layer of VGGNet by kernels. Suppose layer iincludesNi
distinctkernels,itgenerates Nifeaturemapseachofsize (wi,hi),
wherewiandhiarethewidthandheightofthefeaturemapsre-
spectively. These feature maps can be stored as a feature matrix Fi
withsize (Ni,Mi),whereeachrowof Fiisthevectorflattenedfrom
the corresponding feature map and Miiswi·hi. DeepRoad IValso
1Forsimplicity,here weonlydiscussDNNsforclassification, thesameapproachcan
be applied to explain DNNs for other tasks such as regression.
Figure 6: Framework of DeepRoad IV
generates style information which are introduced in [ 16]. These
style information aims at capturing the texture of images and itis defined by feature correlation, which can be computed by the
Gram matrix
Gi=Fi·FT
i(7)
Suppose we choose layer iandjto extract the feature and style
matrixFiandGjrespectively,therepresentationvectorofthegiven
image is /vectorv=[/vectorvF,/vectorvG], where /vectorvF,/vectorvGare flattened vector of Fiand
Gjreceptively. Further, we apply Principle Component Analysis
(PCA) technique to reduce feature dimension of input and training
data as follows:
Y=X·P (8)
Xdenotes the input matrix with size (n,m), wherenis the total
number of input and training data and mis the length of feature
vector /vectorv.Pdenotes the projection matrix with size (m,k), wherek
isthetargetdimensionlessthan m,andPcanbecomputedusing
X[13].
Figure6shows the overall design of our input validation frame-
work,DeepRoad IV.DeepRoad IVfirsttakesthetrainingandonline
driving images as input, and uses VGGNet to extract their content
and style features. As shown in Figure 6, DeepRoad IVinputs a
snowyimagetoVGGNet,andchoosestheconvolutionallayer conv
4_2andconv 5_3 to extract content and style features respec-
tively.Tobespecific,thecoloredgrids F 4_2andF 5_3denotethe
content features extracted from VGGNet, and the style feature G
5_3iscomputedbyEquation 7.Notethatthesecoloredgridsare
just used to visualize results, and their dimensions do not match
therealoutputs.Then,matrix F 4_2andG 5_3areflattenedand
concatenated to feature vector v. DeepRoad IVprocesses all image
data using the same approach and the feature vectors compose
matrix X.Inthesecondstep,DeepRoad IVappliesPCAtoreduce
thefeaturedimension.InFigure 6,wesetthetargetdimensionto
2. The processed data Yare presented on a 2-D plane, where the
blueandrednodesdenotethetrainingandonlinedrivingimages
respectively.Finally,DeepRoad IVcomputestheminimaldistance
betweentrainingdataandeachonlineimage,andrefusestopredictfortheimageswhosedistancesaregreaterthanacertainthreshold.
137
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for ... ASE ’18, September 3–7, 2018, Montpellier, France
Table 1: Details of image sets
Dataset Frame Duration Weather Cond.
Udacity Training 33805 N.A. Sunshine
Udacity Test Ep1 15212 N.A. Sunshine
Udacity Test Ep2 5614 N.A. Sunshine
Youtube Ep1 1000 28:55 Heavy snow
Youtube Ep2 1000 1:09:03 Hard rain
4 EXPERIMENTS
4.1 Data
We useareal-world dataset releasedby Udacity[ 8]as abaselineto
check the inconsistency of autonomous driving systems. From the
dataset,weselecttwoepisodesofhigh-waydrivingvideowhere
obviouschangesoflightingandroadconditionscanbeobserved
amongframes.TotrainUNITmodel,wealsocollectimagesofex-
tremescenariosfromYouTube.Intheexperiments,weselectheavy
snow and hard rain, two extreme weather conditions to transform
real-world driving scenes. To make the variance of collected im-
ages relatively large, we only search for videos which is longer
than20mins.Inthescenarioofhardrain,thevideorecordswipers
swipingwindshield,whichwouldpotentiallydegradethequality
ofsyntheticimages.Hence,indatapreprocessingphase,weman-
ually check and filter those images. Note that all images used in
the experiments are cropped and resized to 320*240, and we have
performed down-sampling for YouTube videos to skip consecutive
frames with close contents. The detailed information is present in
Table1.
4.2 Models
We evaluate our metamorphic testing framework DeepRoad MTon
three DNN-based autonomous driving models which are released
byUdacity[ 8]:Autumn[4],Chauffeur [5],and Rwightman [6].We
choosethesethreemodelsastheirpre-trainedmodelsarepublicand
canbeevaluateddirectlyonthesyntheticdatasets.Tobespecific,
themodeldetailsof Rwightman arenotpubliclyavailable,however,
similar to black-box testing, our approach aims at detecting theinconsistencies of the model. Hence,
Rwightman is still used for
evaluations.
Autumn. Autumniscomposedofadatapreprocessingmoduleand
a CNN. Specifically, Autumnfirst computes the optical flow of raw
imagesandinputsthemtoaCNNtopredictsteeringangles.The
architectureof Autumnis:three5x5convlayerswithstride2pluses
two 3x3 conv layers and followed by five fully-connected layers
withdropout.ThemodelisimplementedbyOpenCV,Tensorflow
and Keras.
Chauffeur. Chauffeur consists of one CNN and one RNN with
LSTM module. The workflow is that CNN first extracts the fea-
tures of input images and then utilizes RNN to predict the steering
angle from previous 100 consecutive images. This model is also
implemented by Tensorflow and Keras.
4.3 Metric
Metric of model inconsistency. In this work, an autonomous
driving system is defined to act consistent if its steering angle
prediction falls within certain error bounds after modifying theweather condition of driving images. We define the number of
inconsistent behaviors of autonomous driving systems as follows:
IB(DNN,I)=/summationdisplay
i∈If(|DNN /dblbracketlefti/dblbracketright−DNN /dblbracketleftτ(i)/dblbracketright)|>ϵ)
,whereDNNdenotestheautonomousdrivingmodeland Iisthe
real-worlddrivingdataset. idenotesthe ithimagein I.τdenotes
the image generator/transformer which can change the weathercondition of the input image.
fis an indicator function which
outputs 1 or 0 if and only if the input is TrueorFalseandϵis the
error bound.Metric of input validation.
As introduced in Section 3.2.2, the
inputvalidityofDNN-basedautonomousdrivingsystemsisdefinedbytheminimaldistanceofinputandtrainingimagesintheembed-
ding space. This metric can reflect the similarity between the input
and training data, however, it has the following limitations: first,
generally,thetrainingdatasetislarge(e.g.10kimages).Supposewe
usetheabovemetrictovalidateasingleinputimage,thenumerous
trainingdatapointswilldominatePCAandtheresultsarebiased.
Second,usingtheminimaldistanceforinputvalidationisnotstable.
For example,suppose thedistance ofinput iand trainingdata jis
minimal and it is less than the threshold. However, jis far from
othertrainingdataandactually iisnotsimilartothemajorityof
the trainingdataset. Weaddress theselimitations inthe following
ways:first, tobalancetheinputdata andtrainingdata,we collect
Mimagesfromonlinedrivingscenesasinputdata,andrandomly
selectMimagesfromtrainingdatasetastrainingdata.Second,to
estimatethedistancemorestable,weaveragetheTop-Nminimal
distancesofeachimagetorepresenttheirsimilarities.Themetric
of input validity is defined as follow:
mIV(i,St)=f(1
N/summationdisplay
k∈{N}min
j∈Stk(/bardblh(i)−h(j)/bardbl2)<θ)
, whereNis a parameter less than M,idenotes the image of the
inputdatasetwithsize M.Stdenotesthesetof Mrandomlyselected
training images, mink(·)denotes the k-th minimal value among in-
putarray.Function fisanindicatorfunctionand θisthethreshold
of input validation.
4.4 Results
4.4.1 Results of DeepRoad MT.WefirstpresentseveralYouTube
screenshotsasgroundtruthinFigure 7tohelpreaderscheckthe
quality of synthetic images. In Figure 8, we list real and GAN-
generatedimagespairs,wherethetworowspresentthetransfor-
mation of Udacity dataset to snowy and rainy scenes, respectively,
andtheoddandevencolumnspresentoriginalandGAN-generated
images, respectively. Qualitatively, the GAN-generated images are
visuallysimilartotheimagescollectedfromYouTubevideosand
they also can keep the major semantic information (such as the
shape of tree and road) of the original images. Interestingly, in the
firstsnowyimageinFigure 8,theskyisrelativelydarkandGANcan
successfullyrenderthesnowtextureandthelightinfrontofthecar.
In the second column, the sharpness of rainy images are relatively
low and this is consistent to the real scene showed in Figure 7. Our
resultsareconsistentwiththeoriginalUNITwork[ 27],andfurther
demonstrate the effectiveness of UNIT for image transformation.
138
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid
(a) Heavy snow
 (b) Hard rain
Figure 7: Images collected from YouTube
Figure 8: Real and GAN-generated images
(a) Autumn
 (b) Chauffeur
 (c) Rwightman
Figure 9: Inconsistency of steering angle prediction on real and synthetic images
We further present examples for the detected inconsistent be-
haviors of autonomous driving models in Figure 9. In the figure,
each row shows the scenes of snow and rain, respectively. In each
sub-figure, the blue caption indicates the model name, while the
red and green captions indicate the predicted steering angles onthe real and synthetic images, respectively. The curves visualize
thepredictionswhichhelpcheckthedifferences.Fromthefigure
we can observe that model Autumn(the first two columns) has the
highestinconsistencynumberonbothscenes;incontrast,model
Rwightman (thelasttwocolumns)isthemoststablemodelunder
differentscenes.ThisfigureshowsthatDeepRoad MTisabletofind
inconsistentbehaviorsunderdifferentroadscenesforreal-world
autonomousdrivingmodels.Forexample,amodellike Autumnor
Chauffeur [3](theyarebothrankedhigherthan Rwightman inthe
Udacitychallenge)mayworkperfectlyinasunnydaybutcancrash
into thecurbside(oreven worse,the oncomingcars)in arainy or
snowy day (shown in Figure 9).
Table2presents the detailed number of detected inconsistent
behaviors under different weather conditions and error bounds foreachstudiedautonomousdrivingmodelontheUdacitydataset.For
example,whenusingtheerrorboundof 10°andtherainyscenes,
DeepRoad MTdetects 5279, 710, and 656 inconsistent behaviors for
Autumn,Chauffeur , and Rwightman , respectively. From the table
we can observe that the inconsistency number of Autumnis the
highest under both weather conditions. We think one potential
reason is that Autumnis purely based on CNN, and does not uti-
lizehistoryinformation(e.g.,viaRNN),andthusmaynotalways
perform well in all road scenes. On the other hand, Rwightman
performsthemostconsistentlythantheothertwomodelsunder
allerrorbounds.Thisresultpresentsaveryinterestingphenome-
non – DeepRoad MTcan not only detect thousands of inconsistent
behaviors of the studied autonomous driving systems, but can also
measuredifferentautonomoussystemsintermsoftheirrobustness.
Forexample,withtheoriginalUdacitydataset,itishardtofindthe
limitations of autonomous driving systems like Autumn.
4.4.2 Results of DeepRoad IV.We use sunny, rainy and snowy
drivingscenestotestDeepRoad IV.Theexpectationofthisexper-
iment is, in the embedding space, the sunny images are close to
139
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for ... ASE ’18, September 3–7, 2018, Montpellier, France
(a) Sunny
 (b) Rainy
 (c) Snowy
 (d) Distribution of Distances
Figure 10: Results of DeepRoad IV: Image embeddings and Distance distributions.
Table 2: Number of inconsistency behavior of three models
under different weather conditions
Scene ModelNum. of Inconsist. Behav.
10°20°30°40°
SnowyAutumn 11635116021138810239
Chauffeur 483921051093653
Rwightman 3341154514
RainyAutumn 5279527952795279
Chauffeur 7101759471
Rwightman 65692230
thetrainingimages,andtherainyandsnowyimagesarelinearly
separable to them. Specifically, sunny images are collected from
the original test dataset, and the rainy and snowy images are ex-
tracted from YouTube videos. Note that to ensure the authenticity
of input images, we only choose real-world instead of synthetic
images. Moreover, we choose convolutional layer conv 3_2 and
conv 4_1 ofVGGNettoextractthecontentandstylefeaturesfrom
theinputimages,andwesetthePCAdimensionto3forvisualizing
experimental results. To reduce the computational complexity, we
resize all the images to 120*90and set the sampling number M
of each dataset to 600. Furthermore, we use the average of Top-
100 minimal distances of each data point to reduce the variance of
similarity estimation for each input image. Figure 10visualizes the
resultsofDeepRoad IVonsunny,rainyandsnowydrivingscenes.
Tobespecific,thefirstthreefiguresofFigure 10presenttheresults
of sunny, rainy and snowy images, respectively. And the orangeand blue points present the sampled training and corresponding
input images. We first analyze the results of the image embedding.
From Figure 10a, we observe that the majority of the input images
are mixed with the training samples, and a few inputs are far from
the cluster. From Figure 10band10c, there are gaps between the
input and training points and the clusters are linearly separable.
These results indicate that the distributions of sunny and training
imagesareclosebuttherainyandsnowyimagesarenot.Onthe
other hand, the cluster of rainy and snowy images are relativelycompact but the sunny images are scattered. The reason may bethe texture of rainy and snowy images are unified and the con-tent is relatively poor, so that the distances between images are
small.However,thelightconditionandcontentofsunnyimages
are more diverse, hence the distances are large. Moreover, fromFigure10d, we find the distances of sunny images mainly lie be-
tween0and3,andalmostallofthedistancesofrainyandsnowy
images are larger than 2. Suppose the threshold of input validationis 2.5, DeepRoad IVcan detect 100% of rainy, 85% of snowy images
and 21% outliers among sunny images as invalid inputs, which
effectively improve the system robustness. Furthermore, we study
if the non-linear transformation of input images is necessary forinput validation. Figure 11visualizes the results of DeepRoad
IV
withoutfeatureextraction.FromFigure 11,weobservethatallblue
clusters are surrounded by the orange points, which show thatinput images are not linearly separable to the training images inthe embedding space. It implies in this case, the distance is not a
propermetricforinputvalidation,andnon-lineartransformation
(i.e. feature extraction using VGGNet) is indeed needed.
5 THREATS TO VALIDITY
Thereareseveralthreatstothevalidityoftheproposedapproach
and its result, which include the followings.
In this work, the main threat to internal validity is potential
defectsintheimplementationofourtechniques.Toreducethese
threats, in implementing DeepRoad MT, we used the original im-
plementation of UNIT to ensure DeepRoad MT’s performance. Fur-
thermore, in implementing of DeepRoad IV, we downloaded the
pre-trained VGGNet weights from PyTorch website2instead of
training it on ImageNet.
The threats to external validity mainly lie in image quality,
dataset and autonomous driving models. First, we lack a good stan-
dard to evaluate image quality (i.e. realisticity). In this paper, we
presentGAN-generatedimagestoletreaderschecktheirquality.
This approach is quite straightforward but less objective. Salimans
et.al[34]proposedInceptionScoretoevaluatethequalityofsyn-
thetic images. To be specific, Inception Score uses an Inception-v3
Network pre-trained on ImageNet to compute a statistic of thenetwork’s outputs as the quality of generated images. However,
Barratt et.al [ 12] demonstrate that Inception Score fails to provide
useful guidance when comparing generative models (e.g. GANs).
Furthermore,thegenerationprocessofGANsisnotcontrollable
thatsomesemanticcontent(e.g.treesorcars)maybemissingin
syntheticimages,andthismaythreatenthevalidityofMetamor-
phicTesting.Second,theUdacitydatasetisrelativesmallandthe
autonomousdrivingmodelsarequitesimple.Supposethedataset
issufficientlylarge,amorecomplicatedandrobustmodelisableto
betrained,andtheinconsistentbehaviorswouldbedramatically
reduced. Moreover, an autonomous driving system is complicated,
and its input and output are diverse. In this work, we only focus
2https://download.pytorch.org/models/vgg19-dcbb9e9d.pth
140
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid
(a) Sunny
 (b) Rainy
 (c) Snowy
Figure 11: Results of DeepRoad IV(without non-linear transformation): Image embeddings and Distance distributions.
on testing the accuracy of the steering angle instead of speeding
adjustments.
6 RELATED WORK
Metamorphic Testing. Metamorphic testing is a classical soft-
waretestingmethodthatidentifiessoftwarebugs[ 15,36,44].Its
key idea is to detect violations of domain-specific metamorphic
relations defined across outputs from multiple runs of the program
with different inputs. Metamorphic testing has been applied for
testingmachinelearningclassifiers[ 30,40,41].Inthispaper,Deep-
Road develops a specific GAN-based metamorphic testing module
for DNN-based autonomous driving systems, where the metamor-
phicrelationsaredefinedsuchthatregardlessofhowthedriving
scenesaresynthesizedtocopewithweatherconditions,thedriv-
ingbehaviorsareexpectedtobeconsistentwiththoseunderthe
corresponding original driving scenes.
Input Validation. Input Validation aims at ensuring that only
properlyformeddatacanbeacceptedbyaninformationsystem,andpreventingmalformeddataleadingsystemserrors.InputValidationhasbeappliedtoenhancetherobustnessofwebapplication[
1,25].
In this paper, DeepRoad develops a distance-based input validation
framework for DNN-based autonomous driving systems, wherethe key idea is that a valid input image is similar to a part of the
images in the training dataset, and the similarity can be measured
bythedistanceinanon-linearlow-dimensionspace.Toenhance
thesystems’security,theimageswillberejectediftheirdistanceis
greater than a given threshold.
TestingandVerificationofDNN-BasedAutonomousDriving
Systems. DifferentfromtraditionaltestingpracticesforDNNmod-
els [28,39], a recent set of approaches (such as DeepXplore [ 31]
andDeepTest[38])utilizedifferentialandmetamorphictestingal-
gorithms for identifying inputs that trigger inconsistencies among
differentDNNmodels,oramongtheoriginalandtransformeddriv-
ing scenes. Although such approaches have successfully found var-
ious autonomous driving system issues, there still lack approaches
that can test DNN-basedautonomous driving system with diverse
and realisticsynthesized drivingscenes. Moreover, DeepSafe[ 19]
focusesonautomaticallyidentifyingsaferegionsoftheinputspace,
within which the network is robust against adversarial perturba-
tions.GAN-Based Image Translation.
GAN-based domain adaption
has been recently shown to be effective in unsupervised image-to-
image translation [ 21,27,43,45]. CycleGan [ 45], DiscoGAN [ 21]
and DualGan [ 43] propose the similar idea that image-to-imagetranslation should satisfy the cycle consistency, where an image
fromDomainAshouldbeidenticalwhenitistranslatedtoDomain
BandtranslatedbacktoA.Theexperimentsshowthatthisextra
constraintcanmakethetranslatedimagesmorerealistic.UNIT[ 27]
furtherassumesthattherepresentationsoftwodomainsmaybeprojected to the same vector space (shared latent space), and is
constructed based on VAEs and GANs. Specifically, they also apply
cycle consistency to the GAN model to regularize the translation.
Moreover,GAN-baseddomainadaptionisalsoappliedforvirtual-
to-real and real-to-virtual driving scene adaption [ 26,42]. DU-
drive [42] proposes an unsupervised real to virtual domain uni-
fication framework for end-to-end driving. Their key insight is the
rawimagemaycontainnuisancedetailswhicharenotrelatedto
thepredictionofsteeringangles,andacorrespondingvirtualscene
can ignore these details and also address the domain shift problem.
Grad-GAN [ 26] is designed to automatically transfer the scene an-
notationinvirtual-worldtofacilitatereal-worldvisualtasks.Inthat
work, a semantic-aware discriminator is proposed for validating
the fidelity of rendered image w.r.t each semantic region.
7 CONCLUSION
In this paper, we propose DeepRoad, an unsupervised learningframework to synthesize realistic driving scenes to test inconsis-tent behaviors of DNN-based autonomous driving systems, and
validate online input images to improve the system robustness.
The experimental results on three real-world Udacity autonomous
drivingmodelsindicatethatDeepRoadcansuccessfullydetectthou-sandsofinconsistentbehaviors.Furthermore,ourresultsalsoshow
that DeepRoad can effectively validate input images to potentially
enhance the system robustness.
ACKNOWLEDGMENTS
This work was supported by the Ministry of Science and Tech-
nology ofChina (Grant No.2017YFC0804002 ), ShenzhenPeacock
Plan (GrantNo. KQTD201611 2514355531),and Scienceand Tech-
nology InnovationCommittee Foundation ofShenzhen (Grant No.
ZDSYS201703031748284andNo.JCYJ20170817110848086).Itwas
also supported by NSF grants CNS 1527727, CCF-1566589, CNS
CAREER1750263,andCCF-1704790.TheauthorsthankShiweiYan
for the support of evaluations, and thank Chenguang Liu, Meng Li,
Yibo Lin and anonymous reviewers for the valuable comments.
141
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for ... ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]2013. Open Web Application Security Project: Data Validation. https://www.
owasp.org/index.php/Data_Validation. Accessed: Jun. 2018.
[2]2014. TeslaAutopilotSystem. https://www.tesla.com/autopilot . Accessed:Jun.
2018.
[3]2016. Final leaderboard of Udacity Challenge 2. https://github.com/udacity/
self-driving-car/tree/master/challenges/challenge-2. Accessed: Jun. 2018.
[4]2016.Steeringanglemodel:Autumn. https://github.com/udacity/self-driving-car/
tree/master/steering-models/evaluation. Accessed: Jun. 2018.
[5]2016. Steering angle model: Chauffeur. https://github.com/udacity/
self-driving-car/tree/master/steering-models/community-models/chauffeur.A c -
cessed: Jun. 2018.
[6]2016. Steering angle model: Rwightman. https://github.com/udacity/
self-driving-car/tree/master/steering-models/evaluation. Accessed: Jun. 2018.
[7]2016. Udacitypre-trainedModels. https://github.com/udacity/self-driving-car/
tree/master/steering-models/evaluation. Accessed: Jun. 2018.
[8]2016. Udacityselfdriving car. https://github.com/udacity/self-driving-car.A c -
cessed: Jun. 2018.
[9]2018. Tesla Model S crash. https://www.wired.com/story/
tesla-autopilot-why-crash-radar. Accessed: Jun. 2018.
[10]2018. Uber’s Self-Driving Cars Were Struggling Before Arizona Crash. https:
//www.nytimes.com/2018/03/23/technology/uber-self-driving-cars-arizona.
html. Accessed: Jun. 2018.
[11] Paul Ammann and Jeff Offutt. 2016. Introduction to software testing. Cambridge
University Press.
[12]Shane Barratt and Rishi Sharma. 2018. A Note on the Inception Score. arXiv
preprint arXiv:1801.01973 (2018).
[13]ChristopherM.Bishop.2006. Pattern Recognition and Machine Learning. Springer.
http://research.microsoft.com/en-us/um/people/cmbishop/prml/
[14]Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp,PrasoonGoyal,LawrenceDJackel,MathewMonfort,UrsMuller,Jiakai
Zhang, et al .2016. End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 (2016).
[15]Tsong Y Chen, Shing C Cheung, and Siu Ming Yiu. 1998. Metamorphic testing: a
new approach for generating next test cases. TechnicalReport.Technical Report
HKUST-CS98-01, Department of Computer Science, Hong Kong University of
Science and Technology, Hong Kong.
[16]Leon Gatys, Alexander S Ecker, and Matthias Bethge. 2015. Texture synthesis us-
ing convolutional neural networks. In Advances in Neural Information Processing
Systems. 262–270.
[17]LeonAGatys,AlexanderSEcker,andMatthiasBethge.2016. Imagestyletransfer
usingconvolutionalneuralnetworks.In Computer Vision and Pattern Recognition
(CVPR), 2016 IEEE Conference on. IEEE, 2414–2423.
[18]IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672–2680.
[19]DivyaGopinath,GuyKatz,CorinaSPasareanu,andClarkBarrett.2017. Deepsafe:
A data-driven approach for checking adversarial robustness in neural networks.
arXiv preprint arXiv:1710.00486 (2017).
[20]Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-
time style transfer and super-resolution. In European Conference on Computer
Vision. Springer, 694–711.
[21]Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. 2017.
Learningtodiscovercross-domainrelationswithgenerativeadversarialnetworks.
arXiv preprint arXiv:1703.05192 (2017).
[22]Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114 (2013).
[23]AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.2012. Imagenetclassifica-tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.[24] YannLeCun,YoshuaBengio,andGeoffreyHinton.2015. Deeplearning. nature
521, 7553 (2015), 436.
[25]Nuo Li, Tao Xie, Maozhong Jin, and Chao Liu. 2010. Perturbation-based user-
input-validation testing of web applications. Journal of Systems and Software 83,
11 (2010), 2263–2274.
[26]Peilun Li, Xiaodan Liang, Daoyuan Jia, and Eric P Xing. 2018. Semantic-
aware Grad-GAN for Virtual-to-Real Urban Scene Adaption. arXiv preprint
arXiv:1801.01726 (2018).
[27]Ming-YuLiu,ThomasBreuel,andJanKautz.2017. Unsupervisedimage-to-image
translation networks. In Advances in Neural Information Processing Systems. 700–
708.
[28]AlexisCMadrigal.2017. Insidewaymo’ssecretworldfortrainingself-driving
cars. The Atlantic (2017).
[29]William M McKeeman. 1998. Differential testing for software. Digital Technical
Journal10, 1 (1998), 100–107.
[30]ChristianMurphy,GailEKaiser,LifengHu,andLeonWu.2008. Propertiesof
Machine Learning Applications for Use in Metamorphic Testing. In SEKE, Vol. 8.
867–872.
[31]Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-
tomated whitebox testing of deep learning systems. In Proceedings of the 26th
Symposium on Operating Systems Principles. ACM, 1–18.
[32] Dean A. Pomerleau. 1989. Advances in Neural Information Processing Systems
1. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, Chapter ALVINN:
An Autonomous Land Vehicle in a Neural Network, 305–313.
[33]HaşimSak,AndrewSenior,andFrançoiseBeaufays.2014. Longshort-termmem-
ory recurrent neural network architectures for large scale acoustic modeling. In
Fifteenth annual conference of the international speech communication association.
[34]Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
andXiChen.2016. Improvedte chniquesfortraininggans.In Advances in Neural
Information Processing Systems. 2234–2242.
[35]S. Segura, G. Fraser, A. B. Sanchez, and A. Ruiz-Cortes. 2016. A Survey onMetamorphic Testing. IEEE Transactions on Software Engineering 42, 9 (Sept
2016), 805–824.
[36]Sergio Segura, Gordon Fraser, Ana B Sanchez, and Antonio Ruiz-Cortés. 2016. A
survey onmetamorphic testing. IEEE Transactions on software engineering 42, 9
(2016), 805–824.
[37]KarenSimonyanandAndrewZisserman.2014.Verydeepconvolutionalnetworks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[38]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018.DeepTest:Automated
Testing of Deep-Neural-Network-driven Autonomous Cars. In Proceedings of the
40th International Conference on Software Engineering, Gothenburg, Sweden, May
27 - June 3, 2018 (ICSE 2018).
[39]IanHWitten,EibeFrank,MarkAHall,andChristopherJPal.2016. Data Mining:
Practical machine learning tools and techniques. Morgan Kaufmann.
[40]Xiaoyuan Xie, Joshua Ho, Christian Murphy, Gail Kaiser, Baowen Xu, and
Tsong Yueh Chen. 2009. Application of metamorphic testing to supervised
classifiers. In Quality Software, 2009. QSIC’09. 9th International Conference on.
IEEE, 135–144.
[41]XiaoyuanXie,JoshuaWKHo,ChristianMurphy,GailKaiser,BaowenXu,and
TsongYuehChen.2011. Testingandvalidatingmachinelearningclassifiersby
metamorphic testing. Journal of Systems and Software 84, 4 (2011), 544–558.
[42]Luona Yang, Xiaodan Liang, and Eric Xing. 2018. Unsupervised Real-to-
Virtual Domain Unification for End-to-End Highway Driving. arXiv preprint
arXiv:1801.03458 (2018).
[43]Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. 2017. Dualgan: Unsupervised
dual learning for image-to-image translation. arXiv preprint (2017).
[44]Zhi Quan Zhou, DH Huang, TH Tse, Zongyuan Yang, Haitao Huang, and TY
Chen.2004. Metamorphictestinganditsapplications.In Proceedings of the 8th
International Symposium on Future Software Technology (ISFST 2004). 346–351.
[45]Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. Unpaired
image-to-imagetranslationusingcycle-consistentadversarialnetworks. arXiv
preprint arXiv:1703.10593 (2017).
142
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. 