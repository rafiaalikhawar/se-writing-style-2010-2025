Performance-InÔ¨Çuence Models for
Highly ConÔ¨Ågurable Systems
Norbert Siegmundy, Alexander Grebhahny, Sven Apely, Christian K√§stnerz
yUniversity of Passau, GermanyzCarnegie Mellon University, USA
ABSTRACT
Almost every complex software system today is congurable.
While congurability has many benets, it challenges per-
formance prediction, optimization, and debugging. Often,
the inuences of individual conguration options on per-
formance are unknown. Worse, conguration options may
interact, giving rise to a conguration space of possibly ex-
ponential size. Addressing this challenge, we propose an
approach that derives a performance-inuence model for a
given congurable system, describing all relevant inuences
of conguration options and their interactions. Our ap-
proach combines machine-learning and sampling heuristics
in a novel way. It improves over standard techniques in that
it (1) represents inuences of options and their interactions
explicitly (which eases debugging), (2) smoothly integrates
binary and numeric conguration options for the rst time,
(3) incorporates domain knowledge, if available (which eases
learning and increases accuracy), (4) considers complex con-
straints among options, and (5) systematically reduces the
solution space to a tractable size. A series of experiments
demonstrates the feasibility of our approach in terms of the
accuracy of the models learned as well as the accuracy of
the performance predictions one can make with them.
Categories and Subject Descriptors: C.4 [ Perfor-
mance of Systems ]: Measurement techniques; D.2.13 [ Re-
usable Software ]: Domain engineering
Keywords: Performance-inuence models, sampling, ma-
chine learning
1. INTRODUCTION
End-users, developers, and administrators are often over-
whelmed with the possibilities to congure a software sys-
tem. In most systems today, including databases, Web ser-
vers, video encoders, and compilers, hundreds of congura-
tion options can be combined, each potentially with distinct
functionality and dierent eects on quality attributes. The
sheer size of the conguration space and complex constraints
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proÔ¨Åt or commercial advantage and that copies bear
this notice and the full citation on the Ô¨Årst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciÔ¨Åc permission and/or a fee. Request
permissions from Permissions@acm.org.
ESEC/FSE‚Äô15 , August 31 ‚Äì September 04, 2015, Bergamo, Italy
Copyright 2015 ACM 978-1-4503-3675-8/15/08 ...$15.00.
http://dx.doi.org/10.1145/2786805.2786845.among conguration options make it dicult to nd a con-
guration that performs as desired, with the consequence
that many users stick to default congurations or only try
changing an option here or there. This way, the signicant
optimization potential already built in many of our modern
software systems remains untapped. Even domain experts
and the developers themselves often do not (fully) under-
stand the performance inuences of all conguration options
and their combined inuence when they interact.
Our goal is to build performance-inuence models (and
models of other measurable quality attributes, such as en-
ergy consumption) that describe how conguration options
and their interactions inuence the performance of a sys-
tem (e.g., throughput or execution time of a benchmark).
Performance-inuence models are meant to ease understand-
ing, debugging, and optimization of highly congurable soft-
ware systems. For example, an end user may use an op-
timizer to identify the best performing conguration under
certain constraints (e.g., encryption needs to be enabled)
from the model; a database administrator may use it to
determine the inuence of certain conguration options and
how they interact; and a developer may compare an inferred
performance-inuence model with her own mental model to
check whether the system behaves as expected.
Our approach is to infer a performance-inuence model
for a given congurable system in a black-box manner from
a series of measurements of a set of sample congurations
using machine learning . That is, we benchmark a given
system multiple times in dierent congurations and learn
the inuence of individual conguration options and their
interactions from the dierences among the measurements.
Our approach addresses several challenges:
We are facing huge conguration spaces that explode with
the number of conguration options. At the same time,
we can sample and measure only a relatively small number
of congurations (several hundred or thousand measure-
ments), so we better select our sample purposefully.
Often, conguration spaces are highly constrained, such
that already random sampling is challenging, because most
random samples do not satisfy the constraints.
Binary and numeric options in congurable systems typi-
cally have dierent characteristics that require dedicated
sampling and learning strategies. Of course, binary and
numeric options can interact, too [1, 15].
If available, domain knowledge should be exploited for
sampling and learning. For example, if we know that per-
formance likely decreases quadratically with a certain nu-meric option, we would like to incorporate that knowledge
for creating better models with fewer measurements.
To address these challenges, we propose a novel approach
of sampling and learning that takes the characteristics of
binary and numeric conguration options and their interac-
tions into account. Specically, we propose a hybrid sam-
pling strategy that uses dierent experimental designs for
numeric options and sampling heuristics tailored for binary
options, grounded in their respective characteristics in prac-
tical systems. Furthermore, we use a learning mechanism
that can cope with both and that additionally incorporates
domain knowledge about an option's inuence, if available.
Our approach diers from prior work on performance opti-
mization in highly congurable systems in that (1) it sup-
ports both binary and numeric options, (2) it relies on sam-
pling heuristics that respect constraints among congura-
tion options, and (3) it learns models that can explain per-
formance behavior in terms of individual inuences of indi-
vidual options and their interactions, allowing applications
beyond mere local or global optimization.
Our approach is able to build reasonably accurate perfor-
mance models of conguration spaces of real-world systems,
including compilers, multi-grid solvers, and video encoders.
In a series of experiments with congurable systems with up
to 1031congurations, we demonstrate that few measure-
ments are sucient to build fairly accurate models (19 %
prediction error, on average). The performance-inuence
models learned by our approach can explain the performance
variation among congurations with a few dozen terms de-
scribing the inuence of individual options and another dozen
terms describing interactions. Finally, while accuracy is im-
portant, simple models are important, too. Views on a
performance-inuence model can be used to isolate inu-
ences of individual options and their interactions. This way,
we found two performance bugs in the Polly extension of
theLLVM compiler framework.1
In summary, our contributions are the following:
We introduce a novel kind of performance-inuence mod-
els describing the inuence of binary and numeric cong-
uration options and their interactions on performance.
We propose a learning algorithm based on stepwise feature
selection that learns performance models focusing on the
most important factors inuencing performance.
We develop a sampling approach that smoothly integrates
binary sampling using heuristics and numeric sampling
using experimental designs.
We demonstrate practicality and feasibility of our ap-
proach with several synthetic benchmarks as well as an
empirical evaluation on six real-world systems.
We make an implementation of our approach on top of the
toolSPL Conqueror as well as all measurement results
and conguration models available online, including data
of several months of performance measurements, support-
ing others in replicating our experiments and evaluating
related approaches: http://www.fosd.de/SPLConqueror/ .
2. PERFORMANCE-INFLUENCE MODELS
A performance-inuence model consists of several terms
that describe the performance of a conguration based on
the values of conguration options. Individual terms may re-
fer to a single option, describing the inuence of that option,
1http://polly.llvm.org/or to multiple options, describing an interaction. Before we
explain how inuence models are obtained by sampling and
learning in Section 3, let us outline the underlying concepts
and how we unify binary and numeric options.
AssumeOis the set of all conguration options, includ-
ing numeric and binary options, and Cthe set of all con-
gurations. We model a conguration c2Cas a function
c:O!Rassigning a (user-)selected value to every option.
For a binary option o,c(o) = 1 if the corresponding option
is selected and c(o) = 0 otherwise. For a numeric option
o,c(o) returns a number in the value range of that option.
Without loss of generality, we normalize the value range of
all numeric conguration options to the interval [0 ::1], as
common in machine learning.
Conceptually, a performance-inuence model is simply a
function from congurations to a performance measure  :
C!R, where performance can be any measurable property
that produces interval-scaled data. The model is described
as a sum of terms over conguration values. Individual terms
of the performance model can have dierent shapes, includ-
ingnc(X),nc(X)2, ornp
c(X)c(Y). For illustra-
tion, consider a congurable database management system
with the options encryption (E) ,compression (C) ,statistics
(S),page size (P) , and DB size (D) and a corresponding
performance-inuence model:
(c) = 50 +Ez}| {
20c(E) +Cz}| {
15c(C) +Sz}|{
5c(S) Pz}| {
0:5c(P) +Dz }| {
1:5c(D)2
 10c(E)c(C)| {z }
E;C+ 0:3c(E)c(P)| {z }
E;P+ 2:5c(E)c(C)c(D)| {z }
E;C;D
In general, we distinguish between terms that refer to a sin-
gle optiono, denoted as o, and terms that refer to multiple
optionsi::j, denoted as  i::j. The former describes the per-
formance inuence of an option (e.g., E= 20c(E) denotes
the inuence of encryption), the latter describes the inu-
ence of a performance interaction among multiple options
(e.g., E;C= 10c(E)c(C) denotes the inuence of the in-
teraction between encryption and compression).
All performance-inuence models are of the following form:
(c) =0+X
i2Oi(c(i)) +X
i::j2Oi::j(c(i)::c(j)) (1)
0represents a minimum, constant base performance shared
by all congurations, as determined during learning;P
i2Oi(c(i)) represents the sum of the inuences of all in-
dividual options;P
i::j2Oi::j(c(i)::c(j)) is the sum of the
inuences of all interactions among all options.
This structure allows us to easily see the inuence of an
individual option from the model. In our example, we can
see that encryption (E), if enabled, slows down our exam-
ple system by 20 (seconds) and that encryption interacts
with compression (i.e., their combined slowdown is less se-
vere than expected from their individual eects, since com-
pressed data are quicker to encrypt, which illustrates that
performance interactions are not necessarily bad).
Combining Binary and Numeric Options. Our approach
is exible enough to incorporate both binary and numeric
options, but still allows us to handle them separately. Al-
though we could discretize the values of a numeric option
into a number of mutually exclusive auxiliary binary options,
this way, we would loose the relation between the binary op-tions that make up the corresponding numeric option, such
that we cannot t a function and interpolate. By contrast,
converting a binary option into a numeric option, we would
interpret a categorical variable as an interval variable, as-
signing a possibly wrong meaning to the range between 0
and 1 and invalidating the mathematical methods we use in
our approach. Next, we explain how to obtain the and 
terms of Equation 1, by means of learning and sampling.
3. LEARNING INFLUENCE MODELS
Although performance-inuence models are simple in their
structure, it is challenging to actually determine the relevant
inuencing factors (i.e., terms) with a reasonable number of
measurements. The key problem is that we cannot learn
the whole inuence model in a single step, since it contains
a potentially exponential number of terms. As a solution, we
incrementally select only the strongest inuences regarding
prediction accuracy in each iteration of the learning process.
A key contribution of our work is thus not to develop a new
learning technique, but to select a suitable one and adapt it
to the requirements and specics of congurable systems.
3.1 Overview
We use stepwise linear regression to learn the function of
a performance-inuence model from a sample set of mea-
sured congurations. To reduce the dimensionality problem
of handling a very large number of options and interactions,
we use forward and backward feature selection to incremen-
tally learn the model.2
We use linear regression because it ts the requirements of
the domain of congurable software systems and the prac-
ticality requirements for end users. In contrast to many al-
ternative approaches, including classication and regression
trees, neuronal nets, and support vector machines, linear re-
gression allows us to learn a formula that can be understood
by humans. It also makes it easy to incorporate domain
knowledge on the inuence of certain options, if available,
which can be an eective means to avoid overtting and un-
dertting [5]. Finally, we do not need any internal informa-
tion about the system, but can apply the learning approach
in a black-box fashion onto sampled benchmark results.
Linear regression is a common approach to learn how a
dependent variable ydepends on a number of independent
variablesxiin the form
y=0+1x1+2x2+:::+nxn+
Given a learning set of observations for y;x1;:::;xn, linear
regression ts the regression coecients isuch that the
overall error is reduced, this way, learning a function that ex-
plains the observations. We use the measured performance
of a conguration as dependent variable yand the congu-
ration values c(i) as the independent variables, thus learning
performance models of the form 0+1c(o1)+:::+nc(on)
|a performance model of basic terms without any inter-
actions or nonlinear behavior.
Linear regression can also be used to learn linear coe-
cients for more complex terms (i.e., to learn non-linear func-
tions) by using computed values as independent variables.
For example, we can use c(p)c(q) as independent variable to
2Note that the term feature selection used here, is a
machine-learning term and must not be confused by the pro-
cess of selecting features in product-line engineering.Algorithm 1: Stepwise feature selection
Data : measurements, O
Result : model
1featureSet =;, error =1
2repeat
3 lastError = error
4 bestCandidate =?
5 candidates = generateCandidates(featureSet, O);
6 foreach feature incandidates do
7 model = learnFunction(featureSet [ffeatureg,
8 measurements)
9 modelError = computeError(model, measurements)
10 ifmodelError <error then
11 error = modelError, bestCandidate = candidate
12 end
13 end
14 ifbestCandidate6=?then
15 featureSet = featureSet [ffeatureg
16 end
17until (lastError error <margin )_(error <threshold );
18featureSet = backwardStep(featureSet, measurements)
19return learnFunction(featureSet, measurements);
learn interactions between pandq, orc(o)2to learn coe-
cient for a quadratic function. That is, if we know which
single-option terms, interaction terms, or function terms
might appear in the performance-inuence model, we can
learn linear coecients for all of them, yielding performance-
inuence functions as in our initial example in Section 2.
The key challenge of using linear regression is, hence, to
identify the relevant terms to be used as independent vari-
ables. Next tojOjbasic option terms ( x=c(o)), there
is an exponential number of basic interaction terms ( x=
c(o1):::c(on)) and an innite number of additional func-
tion terms over one or multiple options (e.g., polynomials).
The problem of too many possible independent variables is
known as the curse of dimensionality [5].
Given a set of measurements selected by a specic sam-
pling heuristics (see Section 4), our learning process pro-
ceeds iteratively in a process known as forward feature selec-
tion [4], learning one term at a time, until further improve-
ment (minimizing the error term ) becomes negligible. In
each iterative step, we try a number of candidate terms and
select the term that provides the biggest improvement for
the model. We select candidate terms based on heuristics
and domain knowledge, if available, as we will explain in
Section 3.3.
3.2 Incremental Learning Algorithm
In Algorithm 1, we sketch the learning algorithm that
computes the function representing the performance-influen-
ce model from a set of measurements (representing selected
congurations and corresponding measurement results). Con-
ceptually, we incrementally compute the relevant feature set,
in which each feature represents a term over one or multiple
conguration values c(o). Throughout the learning process,
the feature set holds the features that have been identied
to improve the error rate of the learned model with regard
to the measured learning set. For our example model in Sec-
tion 2, the feature set would eventually include c(E),c(D)2,
c(E)c(C), and so forth.
Starting with an empty feature set, the algorithm selects
one feature in each iteration until improvements of model
accuracy become marginal or a threshold for expected ac-
curacy is reached ( repeat loop, Line 2{17). The feature to
be added stems from a number of candidate features (theirselection is described in Section 3.3). Using linear regres-
sion to compute a model with every candidate (Line 12), we
select the candidate that reduces the error rate most.
The algorithm concludes with a backward learning step
(backward feature selection; not shown for brevity), in which
every feature in the feature set is tested for whether its re-
moval would decrease model accuracy. This can happen if
initially a single feature is selected because it best explains
the measurements, but it becomes obsolete by other features
(e.g., representing interactions) later in the learning process.
3.3 Selecting Candidate Features
A nal, critical step is to select candidate features that are
explored during the learning process. Much like it is infeasi-
ble to learn a model with all features at once, it is infeasible
to try all features in each iteration|it is typically not even
possible to enumerate all features due to their sheer number.
Hence, we have to apply heuristics to select these candidates
that are likely to inuence performance. A key innovation
of our approach is the procedure of determining which can-
didates we select for learning. We use separate heuristics
for the two main factors that result in the huge number of
candidate features: interaction and function terms.
Hierarchical Interactions. Conceptually, any combination
of options may cause a distinct performance interaction [25],
which would render any learning approach useless as there
is no common pattern. In practice, however, performance
behavior is usually more tractable in that only few inter-
actions contribute substantially to the overall performance.
In our previous work, we found that relevant interactions do
not emerge randomly among conguration options, but form
a hierarchy [25, 26]. That is, three-way interactions (i.e.,
interactions among three options) build on corresponding
two-way interactions among the same set of options. Fur-
thermore, we found, at most, ve-way interactions, and the
most common interactions were two-way. Thus, based on
our experience and following standard practices of machine
learning [2], we perform our learning hierarchically: We rst
start adding only the individual option inuences and then
add interactions as candidates containing options that have
been found already to contribute to performance.
Function Learning. For numeric options, we learn func-
tions over their data ranges. If we know the kind of the
function or the polynomial degree, then we generate the
corresponding candidates. For example, if we know that the
performance inuence of a numeric option ois logarithmi-
cally, we generate only the feature o(log(c(o))) rather than
a set of polynomial candidates. Without domain knowledge,
we use an array of standard functions (linear, quadratic, log-
arithmic) that can be activated by the user of our approach.
Again, our approach can incrementally increase the com-
plexity of these functions by building new functions from
combinations of already learned ones.
4. SAMPLING CONFIGURATION SPACES
Sampling heuristics must satisfy two requirements: First,
they have to produce a reasonable number of measurements;
heuristics requiring millions of measurements are certainly
infeasible. Second, a heuristic must select the congurations
that incorporate most of the relevant interactions. That is,we want to nd a sweet spot between prediction accuracy
and measurement eort. There are several invariants that
need to be considered when constructing the learning set:
Random sampling: Choosing congurations randomly
from the conguration space is still an open problem for
highly-congurable systems. The presence of constraints
prohibit o-the-shelf solutions, such as experimental de-
signs or random selection and ltering [17]. Using SAT
solving to nd valid congurations usually produces only
locally clustered solutions in the conguration space [23],
although recent attempts try to mitigate that problem [11].
Binary and numeric options: Binary and numeric op-
tions have substantially dierent value ranges resulting
in huge dierences in the number of measurement points
we should select for them. For instance, a numeric op-
tion might have a quadratic performance inuence when
varying its value, which certainly requires more measure-
ments than the constant inuence of a binary option when
switched on and o.
We address these problems by dividing the conguration
space along the two types of conguration options and by
applying dedicated sampling heuristics to them.
4.1 Binary-Option Sampling
For binary-option sampling, we use heuristics that we de-
veloped in previous work [25]. The goal of these heuristics
is to pick congurations to learn the basic inuence of each
individual binary option and, subsequently, to pick congu-
rations that exhibit two-way interactions.
Option-Wise Sampling (OW). Option-wise sampling se-
lects congurations such that it purposefully avoids interac-
tions. We use a constraint-satisfaction-problem (CSP) solver
to nd, for each option o, a conguration with as many op-
tions disabled as possible under the constraint c(o) = 1 and
the given constraints among the options. The process is re-
peated until all options have been included in the learning
set, which requires a number of measurements that is linear
in the number of binary options.
Negative Option-Wise Sampling (nOW). Instead of min-
imizing the number of options, negative option-wise sam-
pling aims at maximizing the number of options in a con-
guration to maximize the number of possible interactions.
That is, for each option o, we search a conguration with
as many options enabled as possible under the constraints
c(o) = 0 and the constraints between the options. Much like
OW, nOW requires a linear number of measurements.
Pair-Wise Sampling (PW). For pair-wise sampling, we con-
struct a learning set that includes a minimal set of cong-
urations, in which all two-way interactions are present and
not confounded with other interactions. That is, for each
pair of options qandp, we determine a conguration with
as many options disabled as possible under the constraints
c(q) = 1^c(p) = 1 and the constraints between the op-
tions. Without constraints, PW requires a number of mea-
surements that is quadratic in the number of binary options.
4.2 Numeric-Option Sampling
The science of choosing an appropriate learning set in the
presence of numeric options has a long history, and many
approaches have been proposed under the umbrella of theDesign of Experiments (orExperimental Designs ) [22]. The
goal of an experimental design is to generate an experimen-
tal plan by assigning values to independent variables (nu-
meric conguration options, in our case), such that a given
hypothesis can be properly tested.
As a preparation for this work, we surveyed eight stan-
dard experimental designs and sorted out those that do not
meet our requirements.3For example, the D-optimal design
requires to enumerate all possible combinations, or other
designs, such as the full factorial and the 2k 1design, re-
quire a number of measurements that is infeasible in prac-
tice. After this preselection, we selected the following exper-
imental designs for our empirical evaluation: Box-Behnken,
Plackett-Burman, Central Composite, and Randomization
with seed [18]. Due to space constraints, we report only
on the best performing designs (Plackett-Burman and Ran-
dom); the complete set of evaluation results are available on
our supplementary Web site.
Plackett-Burman Design. In 1946, Plackett and Burman
proposed an experimental design that aims at determining
the main eects of an experiment [20]. It minimizes the
variance of the estimates of the independent variables while
using a limited number of measurements. Wang and Wu
extended it to incorporate two-way interactions [28].
o1o2o3o4o5o6o7o8
c101120221
c210112022
c321011202
c422101120
c502210112
c620221011
c712022101
c811202210
c900000000
Figure 1: Plackett-
Burman design dening 9
experiments for 8 numeric
options using the mini-
mum (0), maximum (2),
and midpoint as levels (1).A Plackett-Burman de-
sign is a specic type of
fractional factorial design,
which are often used for
combinatorial testing [15].
The design species seeds
depending on the number
of experiments to be con-
ducted and the number of
levels of the input variables.
The level species the num-
ber of distinct values of an
independent variable, and
the seed denes the pattern
at which the dierent val-
ues are varied, which aects
also the number of measure-
ments. Since numeric con-
guration options can have
a large number of values, we sample the value range uni-
formly depending on the chosen level. For a three-level de-
sign, we take the minimum, maximum, and center point of
the value range. Next, we have to determine the number
of measurements to be performed. When we are interested
only in the main factors (i.e., only ), we need n+ 1 mea-
surements for nnumeric conguration options. If domain
knowledge is available and we know that there might be in-
teractions between conguration options, we chose the seed
such that more congurations are measured. In Table 1, we
show a Plackett-Burman design for a seed ( n= 9,l= 3)
dening 9 experiments for 8 independent variables with 3
levels. The rst row represents the seed, and each row be-
low is computed by a right shift of the row above, except for
the last row, which sets all numeric options to 0.
3Central Composite, 2k 1, D-optimal, Plackett-Burman,
Box-Behnken, one factor at a time, full factorial, hyper sam-
pling (form of gridding)Random Design. Unlike binary-option sampling, we can
easily determine random values from numeric options, be-
cause we know the minimum and maximum value as well as
the step size, as these must be given to obtain a meaning-
ful conguration. Furthermore, constraints among numeric
options appear rarely in congurable systems. It is very
unusual that numeric options have value ranges with unde-
ned or invalid holes or that setting a numeric option's value
prohibits certain value ranges of other options. To ensure
reproducibility of our experiments, we use random values
with seeds (available on the supplementary Web site).
4.3 Combining Binary and Numeric Sampling
So far, we have explained how we sample the congura-
tion space regarding binary and numeric options individu-
ally. The remaining task is to combine the two. In a rst
step, we determine congurations using binary-option sam-
pling, resulting in a set of partial congurations . In a second
step, for each partial conguration, we determine a set of
complete congurations based on numeric-option sampling.
The rationale behind this ordering is that the majority of
domain constraints are usually between binary options, so
this part of the conguration space is more challenging for
nding valid congurations. Furthermore, in real-world ap-
plications, such as database and web servers, compilers, and
enterprise applications, it is often the case that a binary op-
tion activates a piece of functionality and numeric options
adjust existing functionality (e.g., by setting input bounds,
specifying the workload, or controlling the output quality).
As a consequence, numeric options often depend on binary
options. Overall, this combined approach yields a learning
set comprising nrmeasurements, where nis the number of
binary partial congurations and ris the number of numeric
partial congurations.
5. EVALUATION
A key question is whether our approach can learn accurate
performance-inuence models for highly-congurable real-
world systems with a reasonably small number of measure-
ments. More specically, we aim at answering the following
three research questions:
RQ1 : What is the range of prediction errors per sampling
heuristics we get with our approach?
RQ2 : Which combinations of binary and numeric sam-
pling technique give rise to Pareto-optimal solutions with
respect to measurement eort and prediction accuracy?
RQ3 : Is our learning approach accurate in the sense that
we learn actually existing inuences and interactions?
For the purpose of our evaluation, we operationalize accu-
racy as follows: Given a sample set, we compute a perf-
ormance-inuence model, which we subsequently use to pre-
dict the performance of a large number of congurations
in an evaluation set. As said previously, the actual perfor-
mance metric may vary and depends on the subject sys-
tems. We then measure the congurations of the evaluation
set and calculate the error rate byjmeasured - predicted j
measured, av-
eraged over all congurations. Ideally, we would use the
whole conguration space of a system as evaluation set, but
the measurement eort would be prohibitively high for most
real-world systems. Therefore, we perform the evaluation in
two separate experiments with dierent goals.
In a rst experiment, we use synthetic performance mod-
els. We start with a known, realistic formula of a perfor-mance-inuence model as ground truth and derive the mea-
surements for both sample set and evaluation set from this
formula. Since there are no measurement costs and no mea-
surement bias, we can perform accurate and large-scale ex-
periments with high internal validity, using the entire cong-
uration space as evaluation set. We use this rst experiment
primarily as a sanity check to determine whether we can ac-
curately learn a performance-inuence model (RQ1 and 3)
and as means to explore the tradeos of dierent sampling
strategies (RQ2). In a second experiment, we assess the fea-
sibility of our approach by building performance-inuence
models for six real-world software systems from a number of
dierent domains (RQ1). That is, we execute their actual
benchmarks and measure execution time over a large num-
ber of congurations in both sample and evaluation sets.
5.1 Experiment #1: Correctness and
Accuracy
Standard learning approaches are typically sensitive to
even slight variations in the learning set. This is undesired,
because we cannot trust the resulting performance-inuence
model when analyzing the system, although it might give
reasonable performance estimates. Thus, we aim at checking
whether our approach learns the actually existing inuences,
answering RQ3. For this purpose, we create measurements
from a number of ground-truth models, from which we learn
performance-inuence models and compare them against the
ground truth to check whether the learned inuences and
interactions are similar to the given ones. A second goal of
this experiment is to lter out infeasible sampling heuristics
regarding prediction accuracy and measurement eort, be-
cause evaluating all combinations of sampling heuristics on
real-world systems is computationally infeasible.
Setup. Overall, we compare three sampling heuristics for bi-
nary options, option-wise (OW), negative option-wise (nOW),
and pair-wise (PW), their combinations, and several experi-
mental designs, from which we report here only the Plackett-
Burman and Random Design with 100 randomly selected
numeric congurations and ve seeds.
As ground-truth performance models, we use 7 formulas
() along the lines of our motivating example in Section 2.
But, instead of creating random models, we create mod-
els that represent performance variations in real software
systems by deriving them from actual performance mod-
els extracted in prior work [25] using a dierent approach
(see Section 5.3). Since the original models contained only
binary options, we enhance them with numeric options as
follows: (a) we add one to four numeric options with dif-
ferent value ranges (100 to 1000), (b) we vary the number
of numeric-option interactions (1 to 3) and their kinds (i.e.,
binary{numeric and numeric{numeric), and (c) we use dif-
ferent shapes of functions (linear, quadratic, linear combined
with quadratic). This way, we consider a wide range of dif-
ferent inuence functions and interactions. To get an im-
pression of how our given models look, we refer the reader
to Section 5.4.
To distinguish the ground-truth models, we name them
after the congurable systems from which we obtained the
binary options. Table 5.1 shows all ground-truth models in-
cluding their conguration spaces, the application domain
of the original performance models, and the number of con-
straints among options. We provide all models as well asTable 1: Overview of the ground-truth performance-
inuence models.
Model Domain # Bin #Num #Const jCj
AJStats Analysis tool 20 1 3 107
Apache Apache Web server 9 2 2 106
BDB C Berkeley DB C 7 2 0 105
BDB J Berkeley DB Java 26 4 31 1014
Clasp Answer set solver 20 4 64 1015
LLVM Compiler infrastructure 11 2 1 107
lrzip Compression library 19 4 107 1014
#Bin: Number of binary options; #Num: number of numeric
options; #Const: number of constraints; jCj: number of cong.
the intermediate models of the learning process on the sup-
plementary Web site.
Mean error rate 
Number of 
measurements
Terms of the 
influence model
Term with high 
divergenceUndetected 
influenceTerm equal to 
ground truth
To characterize the accuracy
of a learned model for a specic
sampling strategy, we use a com-
pact visual representation, as de-
picted to the right, covering sev-
eral aspects: mean error rate over
predictions of all congurations,
the standard deviation of the er-
ror when repeating sampling and learning (for random sam-
pling), the number of measurements required by the sam-
pling heuristic, and a compact representation of all terms
in the model (lower part: boxes denote individual terms
weighted in size by their eect strength (coecients); accu-
racy per term is color-coded, from green for the same value
to red for a relative dierence larger than 100%, and black
for terms that are missing in the learned model).
Results. Figure 2 summarizes our experimental results for
all binary-option samplings and Plackett-Burman and Ran-
dom Design. Again, we show only the best combinations
of sampling heuristics and refer the interested reader to our
supplementary Web site for all results. Overall, we observe
a good degree of similarity between the learned models and
the ground-truth models, indicated by the green rectangles.
Only when binary-option interactions exist and no pair-wise
sampling was used, we observe some undetected inuences
(black bars). There are also models, such as for the syn-
thetic Clasp model, for which we obtain a prediction error
of 1 %, on average, but we miss 3 to 5 inuence functions
(row PW PBD). This observation suggests that missing a
single term likely aects only a small fraction of the cong-
uration space, such that the overall prediction error remains
small. These results suggest that we indeed learn the actual
existing inuences in most cases, which is useful for perfor-
mance debugging and interactive conguration tools [19].
To analyze the tradeo among prediction accuracy and
measurement eort (RQ2), we plot the Pareto front (dashed
line) of the combination of binary-option and numeric-option
sampling heuristics in Figure 3. The Plackett-Burman de-
sign (n= 49,l= 7) outperforms all designs no matter
which binary sampling heuristics is used, which applies also
to combinations not shown. For binary-option sampling,
pair-wise sampling has the highest accuracy in combination
with Plackett-Burman, 1.6 %, on average, (even better in
combination with option-wise and negative option-wise sam-
pling, 0.3 %, on average). However, it comes at the cost of
an increased number of measurements compared to the pureOW 
PBD(49,7) AJStats
0%126
OW 
RD 0%
¬±0%900
nOW 
PBD(49,7) 0%126
nOW 
RD 0%
¬±0%900
PW 
PBD(49,7) 0%1k
PW 
RD 0%
¬±0%8k
OW PW 
PBD(49,7) 0%1k
OW PW 
RD 0%
¬±0%8k
nOW PW 
PBD(49,7) 0%1k
nOW PW 
RD 0%
¬±0%9kApache
0%441
41%
¬±59%450
0%735
30%
¬±35%750
0%2k
21%
¬±48%2k
0%2k
21%
¬±47%2k
0%2k
27%
¬±46%3kBDB C
61%392
61%
¬±1%400
100%392
100%
¬±2%400
6%1k
8%
¬±3%1k
6%1k
6%
¬±0%1k
19%2k
19%
¬±1%2kBDB J
9%490
9%
¬±0%500
0%2k
0%
¬±0%3k
0%2k
0%
¬±0%2k
0%2k
0%
¬±0%2k
0%4k
0%
¬±0%5kClasp
21%833
100%
¬±166%850
1%12k
22%
¬±37%12k
1%5k
23%
¬±28%6k
1%5k
27%
¬±53%6k
0%17k
61%
¬±109%18kLLVM
16%539
16%
¬±0%550
0%539
60%
¬±80%550
3%3k
17%
¬±8%3k
19%3k
16%
¬±8%3k
0%3k
1%
¬±1%3klrzip
12%882
31%
¬±25%900
1%9k
1%
¬±0%9k
1%5k
3%
¬±3%5k
2%5k
2%
¬±2%5k
6%14k
1%
¬±0%15kFigure 2: Comparison of learned and ground-truth
models in terms of mean prediction error, number
of measurements, and existence and similarity of
model terms.
option-wise approach (which in turn has a mean prediction
error of about 18 %). Random sampling is not competitive
at sample sizes comparable to the Plackett-Burman design,
and we observed a strong uctuation in the error rate when
repeating learning with a fresh random sample.
5.2 Experiment #2: Effort and Accuracy
In our second experiment, we evaluate whether our learn-
ing and sampling approach is feasible in practice. Although
partly parallelized, we invested more than two months (24/7)
for measuring congurations of six subject systems to obtain
a huge data basis (including learning and evaluation sets).
However, using our approach in practice would require much
less measurements, as most measurements were meant to
evaluate and analyze our approach.
Setup. With a focus on external validity, we selected six
highly congurable systems from dierent domains, written
in dierent programming languages, with varying numbers
of binary and numeric options. Some systems support con-
guration at compile time, others at load time. We pur-
posefully selected some systems for which we can perform a
whole population analysis ( Dune MGS ,HIPAcc,HSMGP )
being able to reliably quantify prediction accuracy as well
as systems that are highly congurable to evaluate the scal-
ability of our approach (see Table 2 for an overview).
Dune MGS is a geometric multi-grid solver based on the
Dune framework [3]. The framework provides algorithms
for smoothing and solving Poisson equations on structured
grids. Binary options include several smoother and solver
0 20 40 60 80 10002000 4000 6000 8000
Mean prediction error in %Mean number of measurements‚óè‚óè‚óè‚óè
‚óè
‚óèOW + PBD(49,7) 
OW + RD 
nOW + PBD(49,7) 
nOW + RD 
PW + PBD(49,7) 
PW + RD 
OW + nOW + PBD(49,7) 
OW + nOW + RD 
OW + PW + PBD(49,7) 
OW + PW + RD 
nOW + PW + PBD(49,7) 
nOW + PW + RD 
OW + nOW + PW + PBD(49,7) 
OW + nOW + PW + RD Figure 3: Pareto front (dashed line) of combinations
of sampling heuristics.
algorithms. Numeric options include dierent grid sizes
and pre- and post-smoothing steps. We measured the time
to solve Poisson's equation on a Dell OptiPlex-9020 with
an Intel i5-4570 Quad Code and 32 GB RAM (Ubuntu
13.4).
HIPAccis an image processing acceleration framework,
which generates ecient low-level code from a high-level
specication. Binary options are, among others, the kind
of memory to be used (e.g., texture vs. local). The number
of pixels calculated per thread is an example of a numeric
option. We measured the time needed for solving a test
set of partial dierential equations on an nVidia Tesla K20
card with 5GB RAM and 2496 cores (Ubuntu 14.04).
HSMGP is a highly scalable multi-grid solver for large-
scale data sets. Binary options include in-place conjugate
gradient and in-place algebraic multi-grid solvers. Nu-
meric options include the number of smoothing steps and
the number of nodes used for computing the solution. As
a benchmark, we performed a multi-grid iteration of solv-
ing Poisson's equation. We executed the benchmark runs
on JuQueen, a Blue Gene/Q system, located at the J ulich
Supercomputing Center, Germany.
JavaGC is the Java garbage collector (version 7) with
several options for adaptive garbage-collection boundary
and size policies. For measurement, we executed the Da-
Capo benchmark suite on a computing cluster consisting
of 16 nodes each equipped with an Intel Xeon E5-2690 Ivy
Bridge having 10 cores and 64 GB RAM (Ubuntu 14.04).
SaC is a variant of C for high-performance computing
based on stateless arrays. The SaC compiler implements
a large number of high-level and low-level optimizations to
tune high-level programs for ecient parallel executions.
The compiler is highly congurable, allowing users to se-
lect various optimizations and to customize the optimiza-
tion eort (e.g., optimization cycles and loop-unrolling
threshold). As benchmark, we compile and execute an
n-body simulation shipped with the compiler, measuring
the execution time of the simulation at dierent optimiza-
tion levels. We executed all benchmarks on an 8 core Intel
i7-2720QM machine with 8 GB RAM (Ubuntu 12.04).
x264 is a video encoder that encodes raw videos into the
H.264 compressed format. Conguration options cong-
ure output quality, encoder types, and encoding heuris-
tics. As benchmark, we measured the time needed to en-
code the Sintel trailer (734 MB) using on an Intel Core2
Q6600 with 4GB RAM (Ubuntu 14.04).Table 2: Overview of the real-world subject systems.
System Domain #Bin #Num #Const jCj
Dune MGS Multi-Grid Solver 8 3 20 2 304
HIPAccImage Processing 31 2 416 13 485
HSMGP Stencil-Grid Solver 11 3 45 3 456
JavaGC Runtime Env. 12 23 4 1031
SaC Compiler 53 7 10 1023
x264 Video Encoder 8 13 0 1027
#Bin: Number of binary options; #Num: number of numeric
options; #Const: number of constraints; jCj: number of cong.
We started our experiments by determining sample sets to
following the Plackett-Burman and Random Sampling (as
they performed best in our rst experiment; see Sec. 5.1) as
well as our binary-sampling heuristics (OW, PW). We do not
report on the results for nOW, because our rst experiment
(Sec. 5.1) showed that it increases measurement eort con-
siderably for only a limited gain in accuracy. As evaluation
set, we selected either the whole population ( Dune MGS ,
HSMGP ,HIPAcc) or a large random set of congurations
(more than 10 000 randomly selected congurations).
Results. In Table 3, we present the results for the six sub-
ject systems. As expected, the error rate is larger than for
the synthetic models, since we cannot fully control for con-
founding factors, such as measurement bias, which made up
to 10 % of deviations within multiple repetitions of measur-
ing the same conguration. Putting this into perspective,
we observe still a comparatively high prediction accuracy
for most of the systems. We yield for every subject system a
performance-inuence model whose error rate is below 19 %.
ForSaC, the PW heuristic requires more than 160 000 mea-
surements, which is infeasible.4
Surprisingly, the OW heuristic performs often equally well
as the PW heuristic (for Dune MGS ,HIPAcc,HSMGP ,
SaC,x264 ), although requiring a substantially lower num-
ber of measurements. For numeric-option sampling, we see
similar results as in our rst experiment: The Plackett-
Burman design is often superior to the Random Design. The
measurement eort for Plackett-Burman is higher with con-
strained conguration spaces (e.g., Dune MGS orHIPAcc).
For systems with few constraints and many options, Plackett-
Burman requires less measurements.
Learning a model required 1 to 5 hours, depending on
the size of the learning set and the size of the models. Re-
markably, the resulting models are compact with only few
terms explaining most of the performance variations; partic-
ularly, we observe usually a larger number of inuences from
individual options (i.e., i) and only a low number of inter-
actions (i.e.,  i::j), considering that there is potentially an
exponential number of interactions. Furthermore, we see an
increase in the number of interactions when PW sampling is
used compared to OW sampling, and also Plackett-Burman
results in larger numbers of identied interactions compared
to Random Design. In Section 5.4, we discuss these and
other results and put them into perspective.
4We use an active-learning approach, in which we rst eval-
uated the performance-inuence model produced with the
OW heuristic, to determine which binary options have an
inuence at all. Then, we applied the PW heuristic to these
options. The corresponding results are marked within
Table 3. We discuss this solution more in Section 5.4Table 3: Results for the six subject systems and
combinations of option-wise (OW) and pair-wise
(PW) sampling. We consider terms with an absolute
coecient of >0:01only.
OW PW
e/jCjiji::j e/jCjiji::j
Dune MGS
RD 20 :1%/49 5j0 22:1%/78 8j8
PBD(49,7) 10 :6%/240 6j24 11%/384 6 j18
PBD(125,5) 8 :8%/375 8j16 8:3%/600 8j20
HIPAcc
RD 14 :2%/261 16j13 13:9%/1281 11j16
PBD(49,7) 13 :9%/736 18j8 10:6%/3645 12j19
PBD(125,5) 13 :8%/528 18j9 10:7%/2631 12j19
HSMGP
RD 4 :5%/77 11j14 2:8%/173 9j13
PBD(49,7) 2 :2%/384 9j13 1:6%/864 10j13
PBD(125,5) 1 :7%/480 11j13 1:5%/1080 11j14
JavaGC
RD 31 :3%/534 4j0 24:6%/3032 5j7
PBD(49,7) 37 :4%/423 5j0 28:2%/2571 9j14
PBD(125,5) 21 :9%/855 3j0 18:8%/5312 5j21
SaC
RD 21 :1%/2060 14j5 30:7%/32617j9
PBD(49,7) 16%/2499 14 j11 25%/47048j13
PBD(125,5) 20 :3%/2295 14j5 27%/43206j19
x264
RD 14 :2%/304 4j3 13:5%/1000 3j7
PBD(49,7) 36 :7%/216 5j4 12:5%/636 6j9
PBD(125,5) 21 :2%/339 4j1 15%/1046 4 j6
PBD: Plackett-Burman Design; RD: Random Design;  e: mean
prediction error;jCj: size of sample set; i: number of terms
representing the inuence of individual options;  i::j: number
of terms representing the inuence of interactions
5.3 Threats to Validity
Internal Validity. To rule out conceptual and implementa-
tion errors of our approach, we conducted a high-internal
validity experiment, in which we used ground-truth mod-
els and aimed at learning these models using dierent sam-
pling heuristics. Note that the ground-truth models have
been learned in prior work with linear programming [25],
which is important, as it would be invalid to compare mod-
els that have been created with the same learning technique.
Furthermore, we do not only computationally compared the
models, but manually reviewed them to avoid errors in the
evaluation. This way, we mitigate the threat that we learn
models with similar prediction accuracy, but with dierent
inuences and detected interactions, which might be a re-
sult of overtting. In the second experiment, we repeated
all measurements several times and used averages to control
measurement bias. Although we parallelized the measure-
ment on equal machines, they might slightly dier in their
performance, but these eects are usually small and, due to
repeated measurements, likely cancel each other out.
External Validity. As highly congurable systems exist in
a broad range of application domains, with dierent cong-
uration spaces, we selected six real-world subject systems in
our second experiment. These systems have varying num-
bers of options and are from dierent application domains,which increases external validity. Naturally, we cannot guar-
antee that our approach works for all possibly congurable
software systems, but we invested several months of mea-
surement to gather and analyze a substantial evaluation set.
5.4 Discussion
Research Questions. Our results show that building per-
formance-inuence models is (a) computationally tractable
(a few hours of learning and measurement), (b) our approach
nds actual inuences and represents them directly, and (c)
we attain a reasonable prediction accuracy.
Regarding RQ1 (prediction accuracy), in the rst experi-
ment, we almost always learned performance-inuence mod-
els with nearly perfect prediction accuracy. For the real-
world systems, we observe average error rates of 10 % to
19 %, which are only a slightly higher than the measurement
bias. For some subject systems and sampling heuristics, we
observe larger prediction errors. A possible explanation is
that these subject systems contain three-way or more com-
plex interactions, which cannot be detected by OW and PW
sampling. Another possible reason is that many weak inu-
ences may sum up to larger prediction errors. We purpose-
fully do not search for weak inuences as they complicate
the model to an extent that is impracticable for performance
debugging or comprehension.
Regarding RQ2 (tradeo between measurement eort and
prediction accuracy), we found that more congurations do
not necessarily lead to more accurate predictions. It de-
pends on which congurations are selected and how they
are distributed in the conguration space, to cover the rel-
evant interactions. For both experiments, Plackett-Burman
designs yield the best tradeo. Although random sampling
is very eective in learning accurate models with compara-
ble sampling sizes, it incurs substantial uctuations, such
that an additional design should be applied that acts as a
base layer to cover all numeric options and their respec-
tive data ranges. For binary-option sampling, we observe a
mixed picture. We found that the PW heuristic is superior
to OW and nOW for the ground-truth models (in which the
binary-option part originates from real-world performance
models), whereas without OW sampling, we miss some in-
uences leading to higher error rates for some real-world
systems. Hence, also in this case, a combination of the OW
and PW heuristic might be a better solution.
RQ3 aimed at determining whether we learned the ac-
tual, relevant options and interactions. The results of the
rst experiment provide us the clear picture that we, in-
deed, nd most of the original terms of a given performance-
inuence model. Moreover, when considering the size of the
learned performance-inuence models of the real-world sys-
tems (number of terms), we conclude that we learn simple
models (at the cost of some prediction accuracy) with both
individual options and interactions. The ratio ofjij
jOjover
all subject systems for OW with Plackett-Burman sampling
is 0.31, indicating that one third of the options signicantly
contribute to the performance of a system. When consider-
ing interactions,ji::jj
jOjis only 0.34, such that the number of
determined interactions is similar the the number of relevant
inuences of individual options.Comprehension and Debugging. An issue not addressed
by our evaluation is to what extent performance-inuence
models help developers in their everyday development and
debugging tasks. While we strive for simple models that
contain only the most important factors for performance,
they still may get considerably large. Nevertheless, based on
such models, we are able to provide views such that isolated
inuences of conguration options or interactions become
immediately apparent. Let us consider again the excerpt of
the performance-inuence model of Berkeley DB C :
0:05PageSize 0:12CacheSize + 2:4Hash + 0:12Statistics
+ 29 :88CryptoHash 8:04HashVerifyStatistics
+ 0:59CryptoHashReplication 0:15CacheSizeCrypto
+: : :
If we are interested in understanding the inuence of cryp-
tography on the overall execution time, we can project out
all other inuences yielding the following simpler formula:
29:88CryptoHash + 0:59CryptoHashReplication
 0:15CacheSize
This view suggests that the cryptography feature should not
be used in combination with the hash search index, because
it degrades performance; when used in combination with
replication, the degradation is even worse, but, increasing
the cache size limits the negative inuence of cryptography
on the execution time.
Actually, using our approach, we found an unexpected
slowdown in the Polly extension of the LLVM compiler
framework ( trmm.c ). By incident, this was observed around
the same time by others.5Moreover, for the congura-
tion options ignore-aliasing andno-runtime-alias-checks , we
found diering performance inuences, for which a developer
ofPolly expected that both should have a similar eect.
We already propagated some performance-inuence mod-
els back to domain experts (e.g., the HPC domain) to guide
and improve the development and conguration, which goes
beyond performance tuning of highly-congurable software
systems. While these examples nicely illustrate the power of
(views on) performance-inuence models for program com-
prehension and debugging, it will be imperative to conduct
a comprehensive user study in further work.
Limitations. Our approach rests on several assumptions.
First, we use regression analysis to learn inuence functions.
If a conguration option has an unsteady performance be-
havior or has a very complex (nearly chaotic) behavior, we
cannot learn, but only approximate its performance inu-
ence. Furthermore, we need the congurable system to have
a deterministic performance behavior. If two equal runs of
the same program lead to largely dierent performances,
we cannot reliably learn inuences and hardly predict per-
formance. Finally, our approach has its limits regarding the
number of options and the size of the learning set. Although
we already tried to minimize the learning set (and there is
room for further improvement), it is still an infeasible prob-
lem to support systems with thousands of options (in terms
of constrainedness and performance variability). Still, our
evaluation demonstrated that our solution scales to prob-
lems with up to 1031congurations, making it feasible for a
sucient number of real-world systems.
5https://groups.google.com/forum/#!topic/isl-development/Dm0bJS7jsCYPerspectives. Beside various facets of performance, perfor-
mance-inuence models may be benecial to reason about
other non-functional properties and quality attributes, most
notably, energy consumption. Moreover, we can supply the
models we learned to other performance-modeling and opti-
mization tools, such as Clafer [19] and EPOAL [9].
Technically, our approach could be extended to support
active learning. That is, we could evaluate the performance-
inuence models in an intermediate step to decide whether
additional measurements should be applied. We found that,
forSaC, applying OW sampling with a Plackett-Burman
design resulted in a performance-inuence model, in which
only 10 of 53 binary options have a relevant inuence. Based
on this result, it is advisable to use the PW heuristic only
for the 10 binary options, which would reduce the required
measurements from 2809 to 100 (or from 280,900 to 10,000
when combined with 100 random numeric-option samples).
In general, our notion of performance-inuence models is
conceptually independent of the concrete learning technique.
That is, the concrete technique is hidden behind the is and
i::jterms of the model. Thus, our approach is complemen-
tary to existing approaches of performance modeling. We
made a number of decisions to support program compre-
hension and debugging performance of congurable systems.
However, when prediction accuracy or optimization is the
single most important aspect, then other techniques, such
as support vector machines, could be used.
6. RELATED WORK
Learning. Our approach aims at determining the individ-
ual inuences of conguration options and their interactions,
which has several use cases, such as performance-bug detec-
tion or conguration optimization. There are many suc-
cessful approaches that aim at nding optimal congura-
tions without pinpointing the inuence of conguration op-
tions explicitly [7, 12, 13]. More closely related to our work
are standard machine-learning techniques, such as support-
vector machines, Bayesian nets, and evolutionary algorithms.
These approaches trade simplicity and understandability of
the learned models for predictive power. Software congu-
ration, however, usually involves humans in the reasoning
process, since not a single, but a number of objectives need
to be satised. Hence, we need to understand how individual
options inuence performance and which interact.
There are a number of approaches that use proling data
to create performance models [16]. For instance, Jovic and
others analyze samplings of call stacks of deployed versions
of a program to nd performance bugs [14]. Grechanik and
others propose to learn rules for the generation of workloads
that reveal program paths with suboptimal performance [6].
However, these approaches concentrate on workload vari-
ability rather than software-system congurability.
Sampling. Although a proper sampling heuristic is a crit-
ical success factor for determining the inuence of cong-
uration options and nding optimal congurations, there is
only little work done so far. Important sampling approaches
have been developed in statistics, in which experimental de-
signs have been developed to ensure certain statistical prop-
erties. We used these designs, such as Central Composite,
Box Behnken, and Plackett Burman, to determine cong-urations of numeric options [18]. One simple approach is
Gridding , which computes a grid over the space of the in-
put parameters. It was used for sampling congurations of
Berkeley DB [27]. However, due to its exponential com-
plexity, Sullivan and others could consider only four options
in a reasonable amount of time [27].
For binary-option sampling, several approaches tackle the
problem of nding valid congurations [8]. Especially, evolu-
tionary algorithms have been proposed for this task [24, 23,
11]. Pohl and others found that determining a valid cong-
uration based on a variability model increases response time
exponentially with respect to the number of features [21].
The sampling heuristics and experimental designs we use
to identify interactions are related to the heuristics used in
combinatorial testing [10, 15]. The dierence is that we
do not focus on functional correctness, but on performance,
which allows us to learn performance-inuence models using
linear regression. This would be considerably harder when
applying it to defect prediction, as defects are much more
singular events in a program's execution than the observable
performance prole.
7. CONCLUSION
Today, most contemporary systems are congurable, which
makes performance prediction, optimization, and debugging
dicult. We address this challenge by proposing an ap-
proach that derives a performance-inuence model for a given
congurable system, describing all relevant inuences of in-
dividual conguration options and their interactions. To
this end, we select and adapt a suitable machine-learning
technique and combine it with sampling heuristics for bi-
nary and numeric conguration options in a novel way. Our
approach rests on an algorithm that iteratively learns a
performance-inuence model using a small set of candidate
features representing relevant performance inuences. To
derive learning sets of tractable sizes, we combine heuris-
tics for binary-option sampling with experimental designs
for numeric-option sampling.
By means of a rst experiment on (partially) synthetic,
ground-truth models, we could show that our hierarchical
learning strategy nds the actually relevant inuencing op-
tions and interactions and yields a mean prediction error of
1 %. In a second experiment, we applied our approach to
six real-world software systems, in which we measured per-
formance in terms of the execution time of a given bench-
mark. Our results conrm the rst experiment for both
accuracy and measurement eort. A major insight is that
the Plackett-Burman design is superior to all other numeric-
option sampling heuristics regarding the tradeo between
measurement eort and prediction accuracy, with an aver-
age prediction error below 19 %, which is only slightly above
the measurement bias. Furthermore, we found that our ap-
proach is feasible for nding performance bugs in real-world
systems, which is a promising avenue of further research.
8. ACKNOWLEDGMENTS
We thank Z. Kolter, Y. Agarwal, and D. Batory for com-
ments on earlier drafts of this paper, A. Simb urger for his
help with the measurements, and the J ulich Supercomputing
Center for providing access to the supercomputer JuQueen.
This work has been supported by the DFG grants AP 206/4,
AP 206/6, and AP 206/7 and by the NSF award 1318808.9. REFERENCES
[1] S. Apel, S. Kolesnikov, N. Siegmund, C. K astner, and
B. Garvin. Exploring feature interactions in the wild:
The new feature-interaction challenge. In Proceedings
of the International Workshop on Feature-Oriented
Software Development (FOSD) , pages 1{8. ACM,
2013.
[2] J. Bien, J. Taylor, and R. Tibshirani. A lasso for
hierarchical interactions. The Annals of Statistics ,
41(3):1111{1141, 2013.
[3] M. Blatt and P. Bastian. The iterative solver template
library. In Applied Parallel Computing. State of the
Art in Scientic Computing , pages 666{675. Springer,
2007.
[4] G. Chandrashekar and F. Sahin. A survey on feature
selection methods. Computers & Electrical
Engineering , 40(1):16{28, 2014.
[5] P. Domingos. A few useful things to know about
machine learning. Communications of the ACM ,
55(10):78{87, 2012.
[6] M. Grechanik, C. Fu, and Q. Xie. Automatically
nding performance problems with feedback-directed
learning software testing. In Proceedings of the
International Conference on Software Engineering
(ICSE) , pages 156{166. IEEE, 2012.
[7] J. Guo, K. Czarnecki, S. Apel, N. Siegmund, and
A. Wasowski. Variability-aware performance
prediction: A statistical learning approach. In
Proceedings of the International Conference on
Automated Software Engineering (ASE) , pages
301{311. IEEE, 2013.
[8] J. Guo, J. White, G. Wang, J. Li, and Y. Wang. A
genetic algorithm for optimized feature selection with
resource constraints in software product lines. J.
Systems and Software , 84(12):2208{2221, 2011.
[9] J. Guo, E. Zulkoski, R. Olaechea, D. Rayside,
K. Czarnecki, S. Apel, and J. Atlee. Scaling exact
multi-objective combinatorial optimization by
parallelization. In Proceedings of the International
Conference on Automated Software Engineering
(ASE) , pages 409{420. ACM, 2014.
[10] A. Hartman and L. Raskin. Problems and algorithms
for covering arrays. Discrete Mathematics ,
284(1-3):149{156, 2004.
[11] C. Henard, M. Papadakis, M. Harman, and Y. Traon.
Combining multi-objective search and constraint
solving for conguring large software product lines. In
Proceedings of the International Conference on
Software Engineering (ICSE) . ACM, 2015.
[12] F. Hutter, H. Hoos, K. Leyton-Brown, and T. St utzle.
Paramils: An automatic algorithm conguration
framework. J. Articial Intelligence Research ,
36(1):267{306, 2009.
[13] F. Hutter, L. Xu, H. Hoos, and K. Leyton-Brown.
Algorithm runtime prediction: Methods & evaluation.
Articial Intelligence , 206:79{111, 2014.
[14] M. Jovic, A. Adamoli, and M. Hauswirth. Catch me if
you can: Performance bug detection in the wild. In
Proceedings of the International Conference on
Object-Oriented Programming, Systems, Languages,
and Applications (OOPSLA) , pages 155{170. ACM,
2011.[15] R. Kuhn, R. Kacker, and Y. Lei. Introduction to
Combinatorial Testing . Chapman & Hall, 2013.
[16] Y. Kwon, S. Lee, H. Yi, D. Kwon, S. Yang, B.-G.
Chun, L. Huang, P. Maniatis, M. Naik, and Y. Paek.
Automatic generation of ecient performance
predictors for smartphone applications. In Proceedings
of the USENIX Annual Technical Conference , pages
297{308. Usenix Association, 2013.
[17] J. Liebig, A. von Rhein, C. K astner, S. Apel, J. D orre,
and C. Lengauer. Scalable analysis of variable
software. In Proceedings of the Joint Meeting of the
European Software Engineering Conference and the
ACM SIGSOFT Symposium on the Foundations of
Software Engineering (ESEC/FSE) , pages 81{91.
ACM, 2013.
[18] D. Montgomery. Design and Analysis of Experiments .
John Wiley & Sons, 2006.
[19] A. Murashkin, M. Antkiewicz, D. Rayside, and
K. Czarnecki. Visualization and exploration of optimal
variants in product line engineering. In Proceedings of
the International Software Product Line Conference
(SPLC) , pages 111{115. ACM, 2013.
[20] R. Plackett and J. Burman. The design of optimum
multifactorial experiments. Biometrika , 33(4):305{325,
1946.
[21] R. Pohl, V. Stricker, and K. Pohl. Measuring the
structural complexity of feature models. In
Proceedings of the International Conference on
Automated Software Engineering (ASE) , pages
454{464. IEEE, 2013.
[22] F. Pukelsheim. Optimal Design of Experiments .
Classics in Applied Mathematics. Society for
Industrial and Applied Mathematics, 2006.
[23] A. Sayyad, J. Ingram, T. Menzies, and H. Ammar.
Scalable product line conguration: A straw to break
the camel's back. In Proceedings of the International
Conference on Automated Software Engineering
(ASE) , pages 465{474. IEEE, 2013.
[24] A. Sayyad, T. Menzies, and H. Ammar. On the value
of user preferences in search-based software
engineering: A case study in software product lines. In
Proceedings of the International Conference on
Software Engineering (ICSE) , pages 492{501. IEEE,
2013.
[25] N. Siegmund, S. Kolesnikov, C. K astner, S. Apel,
D. Batory, M. Rosenm uller, and G. Saake. Predicting
performance via automated feature-interaction
detection. In Proceedings of the International
Conference on Software Engineering (ICSE) , pages
167{177. IEEE, 2012.
[26] N. Siegmund, A. von Rhein, and S. Apel.
Family-based performance measurement. In
Proceedings of the International Conference on
Generative Programming and Component Engineering
(GPCE) , pages 95{104. ACM, 2013.
[27] D. Sullivan, M. Seltzer, and A. Pfeer. Using
probabilistic reasoning to automate software tuning.
ACM SIGMETRICS Performance Evaluation Review ,
32(1):404{405, 2004.
[28] J. Wang and C. Wu. A hidden projection property of
Plackett-Burman and related designs. Statistica
Sinica , 5:235{250, 1995.