Understanding Feature Requests by Leveraging
Fuzzy Method and Linguistic Analysis
Lin Shiâˆ—â€¡, Celia ChenÂ¶Â§, Qing Wangâˆ—â€ â€¡1, Shoubin Liâˆ—â€¡, and Barry BoehmÂ¶
âˆ—Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China
â€ State Key Laboratory of Computer Sciences, Institute of Software Chinese Academy of Sciences, Beijing, China
â€¡University of Chinese Academy of Sciences, Beijing, China
{shilin,wq}@itechs.iscas.ac.cn, shoubin@iscas.ac.cn
Â§Department of Computer Science, Occidental College, Los Angeles, CA
Â¶Center for Systems and Software Engineering, University of Southern California, Los Angeles, USA
{qianqiac,boehm}@usc.edu
Abstract â€”In open software development environment, a large
number of feature requests with mixed quality are often posted
by stakeholders and usually managed in issue tracking systems.
Thoroughly understanding and analyzing the real intents thatfeature requests imply is a labor-intensive and challenging task.
In this paper, we introduce an approach to understand feature
requests automatically. We generate a set of fuzzy rules basedon natural language processing techniques that classify each
sentence in feature requests into a set of categories: Intent,
Explanation, Beneï¬t, Drawback, Example and Trivia. Conse-quently, the feature requests can be automatically structured
based on the classiï¬cation results. We conduct experiments on
2,112 sentences taken from 602 feature requests of nine popular
open source projects. The results show that our method can
reach a high performance on classifying sentences from featurerequests. Moreover, when applying fuzzy rules on machine
learning methods, the performance can be improved signiï¬cantly.
I. I NTRODUCTION
Online issue tracking systems such as Bugzilla [1], JIRA
[2], and Github [3] are widely used in the open software de-
velopment environment. These systems enable users to easilysubmit new feature requests in the format of issue reportsand greatly improve the efï¬ciency for organizations whengathering new ideas. In the software development process,these feature requests serve as critical inputs to software main-tenance and evolution. Developers rely on them to ï¬gure outthe high priority feature requests that should be implementedin the next release [4], to trace whether all requested featuresare implemented [5] [6], and to update documentation forimplemented features [7] [8].
However, identifying all the feature requests from issue
tracking systems is challenging. First, users tend to incor-
rectly differentiate feature requests from bug reports when
posting. Herzig et al. [9] reported that rather than referring
to feature requests, users are likely to inaccurately postedtheir desired features as bug reports in the issue trackingsystems. Moreover, the process of identifying feature requestsis workload-intensive and time consuming as it requires thesystem developers to understand and analyze a large amountof textual artifacts. For communities that have a large number
1The corresponding authorof users, the number of new incoming issues can be massive ona daily basis. The voluntary and loose management nature ofopen source community makes this task even more impossible[10] to accomplish.
Existing literatures point out that collecting the reports
in a structured manner could offset this burden [11], andseveral studies [12][13] have been proposed to structure thecontents of bug reports into code samples, stack traces, etc.
Unfortunately, they do not support structuring feature requests.
In this paper, we propose a method to automatically analyze
and structure the contents of feature requests based on fuzzyrules and natural language processing techniques. The contentsof feature requests are categorized, measured and classiï¬ed.The classiï¬cation results are used as the optimization objectiveto incrementally generate fuzzy rules. Our method is thenvalidated through experiments conducted on nine popular opensource projects to evaluate to what extent the generated rulescan classify the sentences from feature requests correctly.
The experiment results show that the classiï¬cation perfor-
mance of fuzzy rules is increasing as new rules are introduced.The generated fuzzy rules are able to classify feature requests
from the nine projects correctly. Additionally, after trans-
forming the fuzzy rules into boolean variables, we apply thefuzzy rules onto various machine learning methods. The resultsshow that these methods have signiï¬cant improvement in theirperformances when using fuzzy rules as model variables.
The major contributions of this paper are as follows.
1) We propose an automated solution to structure feature
requests that can facilitate deeper content mining on
feature requests.
2) We introduce an incremental approach that combines
fuzzy rules with natural language processing techniques.
3) We conduct an empirical evaluation of the generated
fuzzy rules.
4) We provide a publicly available tool and dataset
1to
replicate our experiments.
The remainder of the paper is organized as follows. Section
II elaborates the approach. Section III presents the experi-
1http://goo.gl/aWOvIX
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research440
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. mental setup. Section IV describes the results and analysis.
Section V discusses the implications and future work. SectionVI shows the threats to validity. Section VII introduces therelated work. Section VIII concludes our work.
II. A
PPROACH
In Section II-A and II-B, we introduce the deï¬nition of
categories and heuristic linguistic patterns that are used in
the proposed approach. Second, we illustrate the related def-initions of fuzzy rules in Section II-C. Then we provide theprocess of how we generate fuzzy rules by leveraging fuzzymethod and linguistic analysis in Section II-D.
A. Categories Deï¬nition of Sentences in Feature Requests
Categories for sentences in feature requests are initially
identiï¬ed from practice guidelines [14][15]. For example, we
identify beneï¬t and drawback from the practical guideline
â€œAvoid being vague about the beneï¬ts or possible drawbacksâ€.Then we manually inspect a sample of 488 sentences takenfrom the PMA and Hibernate projects. During this process,we notice that users are more likely to present ideas or actualneeds to explain the reason why they request the feature.They also include references and supporting materials via
hyper-links, and scenarios where the desired features will be
used. Some users even provide possible solutions to implementthe feature. Feature requests also contain trivial sentencessuch as â€œLooking forward to your replyâ€. As a result, wedeï¬ne a list of six categories for sentences in feature requests.The categories include intent, explanation, beneï¬t, drawback,
example, and trivia. The deï¬nition of each category is statedin Table I along with its importance. We deï¬ne the importanceof a category as the level of attention needed when analyzing
feature requests. We consider the category Intent as the most
important and the category Trivia as the least important. These
categories are deï¬ned to help structure feature requests and
highlight contents that deserve more attention.
TABLE I
DEFINITIONS OF SENTENCE CA TEGORIES
Cate gory Importance Deï¬nition
Intent 1Descriptions about ideas, needs, or expectations to
improve the system and its functionalities.
Beneï¬t 2Descriptions about good or helpful results or ef-fects that the proposed feature will deliver.
Drawback 3Descriptions of disadvantages or the negative partsof the current system behavior.
Example 4Descriptions of examples or references in supportof the proposed feature.
Explanation 5Detailed information about the current behavior,
scenarios, or solutions related to the proposed
feature.
Trivia 6Other information that are not related to the pro-
pose feature nor the system.
B. Deï¬nition of Heuristic Linguistic Patterns
Users are likely to use recurrent linguistic patterns in their
sentences when requesting new features[16]. Given a textual
sentence from a feature request, we can heuristically identifyits linguistic patterns from three levels:TABLE II
EXAMPLES OF HEURISTIC LINGUISTIC PA TTERNS AND 18 RULES FROM 81
GENERA TED FUZZY RULES
ID Antecedents (Heuristic Linguistic Patterns) Level C CF
1 action â€œproposeâ€ =1 LEX,
SYNintent 81
2 action â€œmeanâ€ =1 LEX,SYN explanation 80
3 start â€œpleaseâ€=1 LEX intent 79
4 start â€œunfortunatelyâ€,â€œactuallyâ€ =1 LEX explanation 76
5 contain â€œreallyâ€=1, question=1 LEX explanation 74
6{â€œhelloâ€, â€œthankâ€, â€œregardsâ€, â€œlook for-
wardâ€}LEX trivia 65
7{â€œw ould likeâ€, â€œwish forâ€, â€œI needâ€ } LEX intent 52
8 [SYS-N AME]+{â€œmayâ€,â€œneedâ€,â€œshouldâ€} LEX intent 51
9 ï¬rst sentence=1, start VB=1 LEX,SYN intent 41
10 vaild words = 0, positive good=0 LEX,SEM trivia 40
11 start â€œhoweverâ€=1, contain â€œonlyâ€=1 LEX drawback 39
12 start â€œhoweverâ€=1, negative=1 LEX,SEM drawback 38
13 start â€œfor exampleâ€ = 1 LEX example 36
14{â€œsa veâ€|â€œreduceâ€} +{â€œmemoryâ€ |â€œef-
fortâ€} =1LEX beneï¬t 31
15{â€œhelpfulâ€, â€œusefulâ€, â€œconvenientâ€, â€œawe-
someâ€}LEX beneï¬t 30
16 negative good =1 SEM drawback 27
17 positi vebad =1 SEM drawback 26
18 positi vegood=1 SEM beneï¬t 20
1) Lexical Patterns (LEX): We deï¬ne lexical patterns as
the words or phrases that frequently appear in the sentences
of a certain category. We construct ten glossaries to help usidentify lexical patterns: (1) six glossaries for the six categorieswe deï¬ned; (2) two glossaries for â€œGoodâ€ and â€œBadâ€ words;(3) and two glossaries for â€œstop wordsâ€ and â€œstop verbsâ€.
Table II lists some examples of the glossaries. Sentences thatcontain â€œwould likeâ€, â€œwishâ€, or â€œwantâ€ are more likely to beclassiï¬ed into class Intent (#7 in Table II). The valid
word=0
means that all the words in the sentence belong to the â€œstop
wordsâ€ glossary (#10 in Table II). The start â€œpleaseâ€ means
that the sentence starts with the word â€œpleaseâ€ (#3 in TableII). Sentences that contain the name of the system followed
by the word â€œmayâ€,â€œneedâ€, or â€œshouldâ€ are more likely to beclassiï¬ed into class Intent (#8 in Table II).
2) Syntax Patterns (SYN): We deï¬ne syntax patterns as
the speciï¬c sentence structures that frequently appear in a
certain category. After parsing sentences via Stanford NLPparser [17], we are able to obtain the sequence of Penn part-of-speech (POS) tags [18] and Stanford dependencies (SD) for
each sentence. The sequence of POS tags and Stanford depen-
dencies can be used to identify these patterns. For example,Rule #9 in Table II has a syntax pattern of â€œstart
VB=1â€,
which means that the ï¬rst word in the sentence is a verb. Wecan also identify the subject and action of this sentence byanalyzing â€œnsubjâ€ and â€œcopâ€ dependencies in SD. As shown
in Table II, action
â€œproposeâ€=1 means that the action of the
sentence is â€œproposeâ€ and the sentence is describing an intent.
3) Semantic Patterns (SEM): We deï¬ne semantic patterns
as the meaning of linguistic expressions that frequently appear
in the sentences of a certain category. More speciï¬cally, weare interested in the patterns of meaningfulness that we ï¬nd inwords. Taking the sentence â€œThe frameset of phpmyadmin isnot pretty.â€ as an example, we can obtain its dependenciesby using the Stanford NLP parser as shown in Figure 1.
441
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. The semantic pattern of this sentence is negative good, which
means the sentence describes something that is not good.
This semantic pattern is identiï¬ed by having the â€œ neg(pretty-
7,not-6)â€ dependency (7 and 6 represent the word positions).
This dependency indicates that the sentence has the negationmodiï¬er on the adjective word â€œprettyâ€ and the word â€œprettyâ€belongs to the â€œGoodâ€ glossary.
7KHIUDPHVHWRISKSP\DGPLQ LVQRWSUHWW\'711,1119 %=5%--
QHJ
FRS
QVXEMFDVH
QPRGGHW
æ¿¤æ¿¦æ¿™æ¿¨æ¿¨æ¿­Ä²ç€æ¿šæ¿¢æ¿¢æ¿—ç€æ¾³
æ¿¢æ¿™æ¿›æ¾»æ¿¤æ¿¦æ¿™æ¿¨æ¿¨æ¿­æ¿€æ¿Šæ¾¿æ¾³ç€ç€‚ç€‡æ¿€æ¿‰æ¾¼æ¾³
ç€æ¿¸æ¿ºæ¿´ç€‡æ¿¼ç€‰æ¿¸æ¿²æ¿ºç€‚ç€‚æ¿·æ¾¸æ¿¦æ¿•æ¿«æ¿–æ¿•æ¿—æ¿Ÿ
Fig. 1. An example of semantic pattern for drawback category.
C. Deï¬nition of Fuzzy Rules
1) Fuzzy Rules for Text Classiï¬cation: Fuzzy rules, which
are widely used in artiï¬cial intelligence and control systemsto solve pattern classiï¬cation problems, have also been usedfor text classiï¬cation problems [19].
Rule R
k:IF x 1is A k1AND ... AND x nis A kn,
THEN c la s s =Ckwith CF k,(1)
where Rkis the kthfuzzy rule, x1, ..., x nare the linguistic
variables, Akiare the values used to the represent linguistic
variables, Ckis a consequent class, and CF kis a rule weight.
For classifying an input text that satisï¬es multiple rules, therule that has the maximum CF
kis the winner rule. Taken rules
in Table II as an example, when Rule #1 and Rule #18 areboth satisï¬ed in one sentence, we classify the sentence to be
â€œIntentâ€ since Rule #1 has a higher CF value.
2) Conï¬dence of Fuzzy Rules: We rank the rules according
to the conï¬dence measurement, which is deï¬ned as follows:
conf (A
kâ‡’Ck)=|S(Ak)âˆ©S(Ck)|
|S(Ak)|(2)
where Sis the training dataset, S(Ak)is the subset of training
dataset compatible with Ak,S(Ck)is the subset of training
dataset that are compatible with Ck.
3) Performance Measurements of Fuzzy Rules: We deï¬ne
two measurements to evaluate the performances of fuzzy rulesin our cases, which are correctness and misclassiï¬cation, Since
one fuzzy rule can be viewed as a speciï¬c kind of association
rule [20] of the form A
kâ‡’Ck, we measure the correctness
of classiï¬cation results by leveraging the commonly usedmeasurements in machine learning methods: precision, recall,
and f
measure [21]. We use the average weighted f-measureof all categories to indicate the correctness of the fuzzy rules.
TheCorrectness is deï¬ned as follows.
Correctness( Rules )=
n/summationdisplay
i=1(max{P (i)}âˆ’P(i)+1/summationtextP(i)âˆ—fmeasure( i))
fmeasure( i)=2âˆ—P recision (i)âˆ—Recall (i)
P recision (i)+Recall (i)(3)
where P(i)is the set of importances of all categories.
We measure the misclassiï¬cation of classiï¬cation results for
fuzzy rules by leveraging the confusion matrix, also known as
error matrix. The confusion matrix is a speciï¬c table layout
that allows visualization of the mislabeling one as another[22].We use the average weighted error rate of all the categoriesto indicate the misclassiï¬cation. Given the confusion matrixC=(x
ij)nÃ—n, where nis the number of categories, xijis
the number of sentences that are classiï¬ed as jbut actually
belongs to i. By using the importance difference between
two category (| P(i)âˆ’P(j)|) as the weight, we deï¬ne the
misclassification as follows.
Misclassification (Rules )=
1
nn/summationdisplay
i=1(/summationtextn
j=1(|P(i)âˆ’P(j)|Ã—x ij)/summationtextnj=1(|P(i)âˆ’P(j)|Ã—x ij)+x ii(4)
D. The Process of Fuzzy Rules Generation
Figure 2 illustrates how we generate fuzzy rules. We divide
the process into two phases: initial phase and incremental
selection phase. In the initial phase, we aim to generate aset of initial fuzzy rules by heuristically identifying linguisticpatterns, and deï¬ne a reasonable set of CF
kfor those rules. In
the incremental selection phase, we aim to improve the initialfuzzy rules iteratively according to the incorrect classiï¬cationresults on dataset that includes new projects.
1) Initial Phase: We initiate the rule set by identifying the
heuristic linguistic patterns (identifyHLP in Figure 2) from
sentences taken from the ï¬rst project. For each sentence
in project P
1, we identify its lexical, syntactic, or semantic
patterns in a heuristic way as introduced in Section II-B.These linguistic patterns can work as the antecedent A
k1...A kn
for a rule r, and the Ckof rule ris the category that
the sentence belongs to. Since categories are not equallyimportant, we consider the rules with more important conse-quences to deserve higher weights. When rules have the sameconsequence, we consider the rules with higher conï¬dences tohave higher weights. Therefore, we rank the initial rules bytheir importances and conï¬dences, and set the weight CF
kof
each rule raccording to its position in the rank. The output
of the initial phase is a set of initial fuzzy rules R0.
2) Incremental Selection Phase: In this phase, we assemble
the dataset Tiby adding a new project at a time. Then
we classify Tiby applying fuzzy rules Riâˆ’1. By comparing
with the ground truth labels in Ti, we obtain the correct and
incorrect classiï¬cation results. We consider the fuzzy rules fail
to classify correctly for two reasons:
442
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. 3
3
 3QÍ™6HOHFW)5V
/DEHO6HQWHQFHVà¡¼×à¢™×Š à«šÕœ à¢™à¢‹à¢‰à¢”à¢‹à¢šà¢”à¢‹à¡¿
,GHQWLI\+/3V
5DQNE\ importance
& confidence
5XOH6HW R0
i=  1
à¢à¢€àµŒáˆ¼ à¡¼à«šà¡¼×«à«›à¡¼×«Ú®×« à­§à¬¾à¬µáˆ½
&ODVVLI\7LE\5L&ODVVLILFDWLRQ5HVXOWVRI 7L
,QFRUUHFW&RUUHFW
à¢à¢€×à¢™×Š Õœ à¢šà¢‰à¢‹à¢˜à¢˜à¢•à¢‰à¢”à¡µ
r ,GHQWLI\+/3V
à¢à¡¾×à¢˜à¬¿à«šÍ
5L 8SGDWHRi-1r 5L ,QVHUW Ri-1r< 15XOH6HW Ri
à¢à¡¾àµŒà¢à¡¾à¬¿à«š
25à¢àµŒà¢”àµ†à«š  Íi=  i + 1
<1

,QLWLDOSKDVH
,QFUHPHQWDOVHOHFWLRQSKDVH 
Fig. 2. The Process of Fuzzy Rules Generation.
â€¢Inappropriate CF kof existing rules. We deï¬ne such rules
as â€œFake winner rulesâ€.
â€¢Missing rules that are compatible with negative results.
Therefore, we analyze each sentence in the incorrect clas-
siï¬cation results to identify new rules or tune weights of the
existing rules. As shown in Figure 2, we identify heuristiclinguistic patterns and generate one fuzzy rule rfor each
sentence in the incorrect classiï¬cation. If rhas already existed
inR
iâˆ’1, then we update the existing fuzzy rule set by tuning
weights. If ris a new rule, then we insert it into Riâˆ’1.
We use the maximum correctness as the optimization objec-
tive to control the trade-off between inserting new rules and
tuning weights of existing rules. As shown in Algorithm 1,
for each negative result that is caused by inappropriate CF kof
fake winner rules, we update the rules by tuning their weights.We ï¬rst identify the real winner rule and the fake winner rule
respectively. There are two operations in the function update.
First we swap the weights between the real winner rule and
the fake winner rule. Then we deduct the weights of the restof the rules by 1 so that they have lower weights than the realwinner rule. We use the Correctness measurement deï¬ned in
section II-C as the optimization objective, which means that
theupdate action is accepted only when the correctness does
not decline. Similar to the update action, we insert new rules
to all the possible positions in the existing rule set, and ï¬nd
the maximum Correctness. If the new Correctness is larger
than the current maximum Correctness, then the insert action
is accepted.
We repeat this step iteratively until there is no more newAlgorithm 1 Insert or Update rules
1:procedure INSERT (Rule newRule, List ruleSet)
2: positions â†|ruleSet|+1
3: oldCorrectness â†Correctness( ruleSet )
4: maxCorrectness â†0
5: newRuleSet â†0
6: for each position p:positions do
7: ruleSet.add( p, newRule)
8: tmpCorrectness â†Correctness( ruleSet )
9: iftmpCorrectness > maxCorrectness then
10: maxCorrectness â†tmpCorrectness
11: newRuleSet â†ruleSet
12: if(newCorrectness > oldCorrectness )then
13: return newRuleSet
14: else
15: return ruleSet
1:procedure UPDA TE (List neg, List ruleSet Sentence text)
2: trueâ†{r|râˆˆneg, r.C =text.class}
3: realW inner â†{r|râˆˆtrue, r.CF =maxCF (true)}
4: fakeW inner â†{r|râˆˆneg, r.CF =maxCF (neg)}
5: oldCorrectness â†Correctness( ruleSet )
6: newCorrectness â†0
7: newRuleSet â†ruleSet
8: updates â†{ r|râˆˆnewRuleSet, realW inner.CF <
r.CFâ‰¤fakeW inner.CF }
9: for each Rule r: updates do
10: r.CF =r.CFâˆ’1
11: realW inner.CF =fakeW inner.CF
12: newCorrectness â†Correctness( newRuleSet)
13: if(newCorrectness > oldCorrectness )then
14: return newRuleSet
15: else
16: return ruleSet
rules identiï¬ed or weights tuned. As a result, we obtain a setof updated fuzzy rules that is more compatible with both thecorrect and incorrect classiï¬cation results.
E. Tool Support: Automated Structuring Feature Request
Based on the proposed approach, we implement an auto-
mated Feature Request Analyzer (FRA) online system that can
automated structure the contents of a given feature request intoa tree format, as shown in Fig. 3. We also provide a web API
for other applications to access FRA. The analyzed results arereturned in JSON format. More details can be found on our
project site: http://goo.gl/cL00vd
III. E
XPERIMENTAL SETUP
Three research questions are proposed in Section III-A. In
order to answer our research questions, we select and analyzenine open source projects in III-B and III-C. More speciï¬cally,we extract and tag sentences from feature requests that arefound in the nine projects. Then fuzzy rules are generated
and optimized incrementally until no rules can be added orchanged. We evaluate the performances of these generated
rules in Section IV.
A. Research Questions
The goal of the experiment is to evaluate the performance
of the generated fuzzy rules, with the purpose of investigating
443
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. Fig. 3. Tool Support for automatically structuring feature requests.
when the approach can generate a stable set of fuzzy rules, how
effective the fuzzy rules are when applied on datasets, and theperformance of various machine learning methods when usingthe fuzzy rules as model variables. Speciï¬cally, the experimentaims at addressing the following research questions:
RQ1: When can the generated fuzzy rules reach the state
of convergence? This research question aims at investigating
how many projects it will take for the approach to generatea stable set of fuzzy rules ( i.e. no new rules added and no
existing rules changed). We apply an incremental method toadd rules generated from one project at a time. Each incrementis considered as an iteration. Then we use the correctness ofeach iteration as the measurement to determine the state ofconvergence. Moreover, we use the misclassiï¬cation of eachiteration to determine the stable set of fuzzy rules.
RQ2: How effective are the fuzzy rules on classifying
feature requests gathered from nine open source projects?This research question aims at empirically evaluating theperformance of the generated stable fuzzy rules on the nineselected projects. We ï¬rst focus on classifying sentences from
feature requests from the projects we used to generate these
rules. Then we apply the rules on the remaining projects.
RQ3: How do machine learning methods perform when
using the fuzzy rules as their model variables? This research
questions aims at evaluating the performance of machinelearning methods that use the generated stable fuzzy rules astheir model variables. We use correctness and misclassiï¬cationas the performance measurements to compare these methodswith and without using the rules.
B. Subject Projects
We selected the subject projects from four representative
open source communities: Apache, Eclipse, Github, and Hib-
erante, starting from a list of popular projects. From these, weselected 9 projects that (i) had at least 100 feature requestsin their issue tracking systems, (ii) had active contributions inthe last one month, (iii) covered 9 different domains, and (iv)covered 4 open source communities. Requirements (i) and (ii)
aim to ensure that the data we collect are sufï¬cient and up-to-date. Requirements (iii) and (iv) aim to keep the diversity ofthe data so that the fuzzy rules we generated can be general.
The subject projects we selected and used in our experimentsare as follows. The characteristics of subject projects andselected feature requests are listed in Table III.
â€¢Phpmyadmin(PMA): A web-based MySQL management
tools written in PHP .
â€¢Mopidy: An extensible music server written in Python.
â€¢Activemq: An open source message broker that is writtenin Java.
â€¢AspectJ: An aspect-oriented programming (AOP) exten-
sion created at PARC for the Java programming language.
â€¢Hibernate ORM : An object-relational mapping tool for
the Java programming language, written in Java.
â€¢SWT : A UI toolkit used by the Eclipse Platform and most
other Eclipse projects written in Java.
â€¢Log4j: A Java-based logging utility.
â€¢HDFS : The primary storage system that is used by
Hadoop applications.
â€¢Archiva: An extensible repository management softwarewritten in Java.
We then randomly selected these projects into training and
testing datasets.
C. Data Preparation
Step 1: identify feature requests. The nine selected
projects come from four different communities. All these com-
munities manage feature requests via issue tracking systems.Feature requests are treated as one speciï¬c type of issues,typically tagged by â€œenhancementâ€ or â€œnew featureâ€ labels.We retrieved these feature requests by using the search enginein the issue tracking systems, and then exported the title andthe description of the retrieved issue as one feature request. Forprojects from Eclipse, Apache, and Hibernate communities,we were able to export feature requests easily, since the issuetracking system they use provide the export functionality.However, for projects from Github, we implemented a datacollection tool that can automatically collect feature requestsfrom search results.
Step 2: ï¬lter feature requests. For projects contain more
than 200 feature requests, we randomly selected 200 of themas the dataset for deeper analysis. As reported by Herzig et
al.[9], issue reports classiï¬cation is not one hundred percentreliable, and more than 40% of the issue reports are inaccu-
rately classiï¬ed. Therefore, we conducted a manual inspection
on feature requests to ï¬lter out the feature requests that are:(i) misclassiï¬ed; (ii) low in readability/clarity; and (iii) withincomplete descriptions. We also replaced any source codeand URL found in request descriptions with <C O D E> and
<L I N K> respectively to maintain the natural language
state of the requests. URLs were identiï¬ed through regular
expressions, while source code was ï¬ltered out automaticallyby using infoZilla found in [23].
Step 3: tag sentences. After obtaining a set of qualiï¬ed
feature requests as the subject data, we aimed at tagging
each sentence with an appropriate category. The tagged sen-tences were used as the ground-truth dataset for generating
444
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. TABLE III
DA TASET OF NINE SUBJECT PROJECTS
Project Community Domain #FRs#Selected
FRs#Sentences
PMA Githubdatabasetool1740 57 193
Mopidy Githubmusicserv er312 62 189
Acti vemq Apachemessagebrok er486 62 167
Log4j Apache logging 177 59 249
HDFS Apache storage 402 98 394
Archi va Apacherepository
tool137 42 112
AspectJ Eclipse programming 472 96 362
SWT Eclipse UItoolkit 1540 44 147
ORM Hibernatemappingtool778 82 299
fuzzy rules and evaluating the classiï¬cation performances. To
guarantee the correctness of the tagging results, we built aninspection team consisted with two senior researchers andfour Ph.D candidates. All of them either have done intensiveresearch work with issue tracking systems or have beenactively contributing to open source projects. We divided theteam into two groups. Each group consisted a leader (seniorresearcher) and two members (Ph.D candidates). The leadertrained members on how to tag and provided consultationduring the process. The two members tagged the same set ofsentences. We only accepted and included the sentence to thedataset when the sentence received the same category fromboth members. When a sentence received different taggingresults, we hosted a discussion with all six people and decidedthrough voting. Overall, 89% of the 2,112 sentences ob-tained agreements between labelers. Among the intent, beneï¬t,drawback, example, explanation, and trivia categories, theagreement rates were 90%, 87%, 82%, 85%, 92%, and 98%.
The size of tagged sentences and corresponding feature
requests are shown in Table III. â€œ#FRsâ€ lists the number offeature requests of the project we retrieved by labels â€œnewfeatureâ€ or â€œenhancementâ€. â€œ#Selected FRsâ€ represents thenumber of feature requests we selected after applying thedata preparation process. â€œ#Sentencesâ€ denotes the numberof sentences collected from the selected feature requests.In total, we acquired 602 feature requests and 2,112 sen-tences. All the data can be downloaded from our project site:http://goo.gl/aWOvIX.
IV . R
ESULTS AND ANALYSIS
This section reports the analysis of the results achieved
aiming at answering our three research questions.
A. Results of RQ1
As described in section II, fuzzy rules are generated by
incrementally introducing new projects until the rule set is
stable. In our experiments, we gradually added projects one at
a time to generate fuzzy rules until the rule set was stable. Weused the correctness of each iteration as the measurement to
30$30$
0RSLG\30$
0RSLG\
$FWLYH0430$
0RSLG\
$FWLYH04
$VSHFW-30$
0RSLG\
$FWLYH04
$VSHFW-
250
5XOHV     
&RUUHFWQHVV     
0LVFODVVLILFDWLRQ     
Fig. 4. Performance changes and growth of rules in each iteration
determine the convergence of the fuzzy rules. By observing the
growth trend of the rules in each iteration and the measurementof misclassiï¬cation, we assessed and proved the validity of the
ï¬nal fuzzy rule set.
Figure 4 shows the number of the fuzzy rules in the ï¬rst
ï¬ve iterations. We can see that the rules are growing from 30to 81 in the ï¬rst four iterations. The correctness also increasesfrom 75% to 90% respectively. However, when introducing a
new project during the ï¬fth iteration, the correctness remainsalmost the same, which means that inserting new rules orupdating existing rules cannot make any improvement on therule set. Therefore, we concluded that with four projects, the
fuzzy rules converged at a high correctness level of 90%.
Figure 4 also shows the changes in misclassiï¬cation be-
tween iterations. The misclassiï¬cation decreases from 37% to22% in the ï¬rst four iterations, and slightly increases to 24%at the ï¬fth iteration. The results can support the conclusion on
the state of convergence.
Furthermore, we veriï¬ed that such performance trend in
each iteration was found in individual projects. Figure 5 showsthe correctness and misclassiï¬cation for each project that havebeen used to generate fuzzy rules. Trend-lines end with â€œ
Câ€
represent correctness, and â€œ Mâ€ represent misclassiï¬cation.

   
30$B&
 0RSLG\B&
 $FWLYHPTB&
 $VSHFW-B&
30$B0
 0RSLG\B0
 $FWLYHPTB0
 $VSHFW-B0
Fig. 5. Performance changes on individual projects
445
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. As the size of the fuzzy rules increases, the performance
of these rules on each project is improving with higher
correctness and lower misclassiï¬cation.
Overall, these observations show that the fuzzy rule set can
reach the state of convergence at the end of the fourth iteration
with high performance. More speciï¬cally, our approach onlytook four projects to generate a stable set of 81 fuzzy rules.
B. Results of RQ2
As shown in Figure 4 and Figure 5, the stable set of fuzzy
rules can achieve high performance when applied on the four
projects where the rules were generated from. However, suchhigh performance may caused by the fact that the rules weregenerated from the sentences in those projects. Therefore, weevaluated the performance of these fuzzy rules on projects thatwere not used in generating the rules.
Figure 6 shows the correctness and misclassiï¬cation of the
81 rules in all projects. The left four data points represent theprojects that were used in rules generation, and the right ï¬vedata points represent the projects that were not used in rules
generation. We distinguished those two types of projects intoï¬tted dataset and unï¬tted dataset respectively. The correctness
slightly ï¬‚uctuates between 84% and 91% among the nine
projects, and the average correctness is 87% , while theï¬‚uctuation of the misclassiï¬cation is between 15% and 27%among all the projects and with an average of 22%. The results
show that the 81 fuzzy rules perform steadily well on both the
ï¬tted dataset and unï¬tted dataset.
We further looked into the classiï¬cation results on unï¬tted
dataset for each category in terms of precision, recall, and f-
measure. We used three boxplots to graphically illustrate thedetailed classiï¬cation results on the six categories (shown inFigure 7). The details of precision, recall and f-measure foreach category are shown in Figure 8. Categories Intent and
Explanation have the highest and most stable precisions of
nearly 90%, while other categories have less stable yet stillaccurate precisions. The average precision on unï¬tted datasetamong all categories is 85%. Each category has an average of88% in recall, while Intent has the highest recall among all.
F-measure considers both precision and recall and is used to
 

 


30$ 0RSLG\ $FWLYHPT $VSHFW- 250 6:7 /RJM +')6 $UFKLYD
&RUUHFWQHVV 0LVFODVVLILFDWLRQILWWHGGDWDVHW XQILWWHGGDWDVHW
Fig. 6. Correctness and misclassiï¬cation of the 81 rules in all projects.
3UHFLVLRQ 5HFDOO )0HDVXUH
Fig. 7. Classiï¬cation results on the six categories.
,QWHQW
([SODQDWLRQ%HQHILW 'UDZEDFN
([DPSOH 7ULYLD
Fig. 8. Detail classiï¬cation results on the six categories
show the predictive capability of the fuzzy rules. Categories
Intent and Explanation have the highest and most stable f-
measures while others are less stable but relatively good.
C. Results of RQ3
It is widely known that machine learning methods are
commonly used for resolving classiï¬cation tasks [24]. In this
study, the following methods are used: Random Forest (RF),Naive Bayes (NB), J48, Logistic Regression (LR), and Support
V ector Machine (SVM).
Choosing informative and discriminating features is impor-
tant to apply machine learning methods effectively. In order
to ensure that machine learning methods can use the featuresthat are as informative and discriminating as the fuzzy rules,we constructed a dataset for each project as follows.
First, we mapped the 81 rules into 81 boolean variables.
Then, for each sentence in the feature requests, we assigned 1to the variable if the corresponding rule is satisï¬ed. Otherwise,0. We used the boolean datasets that are transformed from theunï¬tted ï¬ve subject projects as the training datasets, and builtclassiï¬ers for the ï¬ve machine learning methods through Weka
[25]. Since we leveraged modal verbs and NLP parsing resultsfrom complete sentences in 81 rules, we did not stem non-stopwords from the feature requests for machine learning methods.We used the vector format and TF-IDF weights as features for
machine learning methods without fuzzy rules [26]. Instead of
using the four ï¬tted datasets (40%), we selected 10-fold (90%)
446
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. on the unï¬tted ï¬ve datasets when training machine learning
classiï¬ers to obtain higher performances. We evaluated theclassiï¬cation results of each machine learning model by pre-
cision, recall, f-measure, correctness and misclassiï¬cation.
Figure 9 shows that the average precision, recall, and f-
measure of the proposed fuzzy rules are the highest, following
by the four machine learning classiï¬ers with rules, and thenfour machine learning classiï¬ers without rules.
As shown in Figure 10 and Figure 11, the proposed fuzzy
rules can achieve the highest correctness and the lowest mis-classiï¬cation. Since the proposed approach considers correct-ness as the optimization objective while ML methods typically
  /51%-5)690-5/551%55)56905)X]]\53UHFLVLRQ
  5)/5-1%690-569055)5/551%5)X]]\55HFDOO
  5)/5-1%690/55-55)569051%5)X]]\5)0HDVXUH
Fig. 9. Average performances for the nine classiï¬ers.

250 6:7 /RJM +')6 $UFKLYD&RUUHFWQHVV
0/ZR5XOHV0/Z5XOHV)X]]\5XOHV
Fig. 10. Correctness for the nine classiï¬ers.

250 6:7 /RJM +')6 $UFKLYD0LVFODVVLILFDWLRQ
0/ZR5XOHV
0/Z5XOHV
)X]]\5XOHV
Fig. 11. Misclassiï¬cation for the nine classiï¬ers.
rely on probability, the proposed fuzzy rules outperformed the
machine learning classiï¬ers with rules. Most of the machinelearning methods without rules have under 50% in correctness,and all of them have above 60% in misclassiï¬cation. Thesemethods might be incapable to classify feature requests. Onemain reason for having low performance is because that thesemethods can predict well on categories that mainly containlexical patterns but not those with mainly syntax and semanticpatterns. For instance, all of the nine methods can achieve apretty good precision (69%) and recall (51%) in the category
Intent but not in the category Beneï¬t. However, when we
incorporated the fuzzy rules as the model inputs, there was
an over 50% improvement in performance for each machinelearning method. These results indicate that the fuzzy rulescan beneï¬t the machine learning methods by providing moresigniï¬cant model variables.
V. D
ISCUSSION AND FUTURE WORK
In this section, we discuss the implications of our results
and possible ideas of future work.
A. How to Use the Classiï¬cation Results?
(For practitioners) Feature requests are more likely to be
inaccurately submitted as bug reports [9], and buried under
the intensive textual communications such as mailing lists[27]. Practitioners who want to acquire a complete sense ofrequested features need to spend a lot of effort on analyzingthe massive unstructured textual artifacts. We offer a semanticparadigm of feature requests, including six categories that areextracted from conï¬rmed feature requests from open sourcecommunities. By automatically tagging each sentence into oneof the six categories, the classiï¬cation results can providepractitioners with an initial understanding of the unstructuredtextual artifacts. The tagged contents can help practitionerseasily locate the information they need (e.g., beneï¬t and draw-
backs) and ignore the unwanted parts (e.g., examples and trivial
information) from feature requests.
(For researchers) The tool and results can be used to con-
duct further empirical studies on feature requests. We noticedthat there are certain sequence patterns, which requesterslike to follow when propose feature requests. For example,some requesters like to start by describing the drawbacksof the current system, and then explain what they want,while some requesters ï¬rst describe what they want, and thenshow the corresponding beneï¬ts if the features they want areimplemented. It would be interesting if the following questionscan be answered: (1) How people describe software featurerequests? What kinds of â€œpatterns or mannersâ€ they follow? (2)What kinds of feature requests are easy/difï¬cult to understand?In addition, the researchers can also use the classiï¬cationresults for further content mining in feature requests or otherrelated textual artifacts, such as locating feature requests frommassive textual artifacts by using patterns, and analyzingquality concerns in feature requests or bug reports.
B. Optimizing the Generated Fuzzy Rules
In order to optimize the fuzzy rules to improve their
generalization in the open source communities, there is a need
to analyze more feature requests from the enormous projects
447
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. available in these communities. However, it is not feasible to
identify new linguistic patterns and optimize the fuzzy rulesby only human efforts with such big datasets. It becomesbeneï¬cial to automate the process to learn linguistic pat-terns through leveraging data mining techniques. As discussedearlier, there are three types of linguistic patterns: lexical,syntax, and semantics patterns. In most cases, lexical patternsare the phrases that commonly appear among sentences inthe same group. It is possible to extract lexical patternsby checking similarity in N-Gram phrases. Syntax patternsdescribe a higher level of knowledge of sentences in termsof POS tags. The sequences of POS tags for the sentences(e.g. ,{VB, DT, NN}) can be considered as the transaction
records for association rule mining. It is possible to extract
syntax patterns by using association rule mining to discoverinteresting relations between POS tags. The combination ofStanford dependencies and heuristic glossaries can indicatethe meaning of a sentence. Stanford dependencies can help usbuild the relationship between two words, and the heuristicglossaries can help us deï¬ne the meaningfulness similaritybetween two words. If any relationship is found between twowords, a graph can be built for a given sentence based onthe Stanford dependences between two words. The maximumcommon edge subgraph[28] of sentences in the same groupcan be used to calculate of graph similarity and as thesemantics patterns. We will explore this possibility in ourfuture work.
In addition, we plan to improve and extend the 10 glossaries
we build in our approach by leveraging automated extractionand clustering techniques [29].
C. Feature Requests/Bug Classiï¬cation on Software Qualities
As mentioned in previous Section V-A, the classiï¬cation
results can be used for further content mining. We plan to
use the results from further content mining of feature requestsand bug reports to examine the current quality status of thesoftware. We believe that user requirements and existing bugs
in the system can reï¬‚ect the quality status of the software.
Boehm et al.[30] proposed a System and Software Qualities
(SQs) ontology that shows the relationship between a set ofkey software qualities. He also identiï¬es the key roles ofmaintainability, not just for systems and software life cycle
costs, but also for systems and software changeability andavailability. We plan to use the ontology as a guideline to
conduct the study with a focus on software maintainability,changeability and availability.
VI. T
HREA TS TO VALIDITY
External Validity The results of our study may not gen-
eralize beyond the projects we evaluated. To mitigate thisthreat, we selected nine popular and well-known open sourceprojects from four different communities as well as ninedifferent domains. To test the generality of our results, we used
the fuzzy rules extracted from four projects, and tested theirperformance on the other ï¬ve projects. We observed that thefuzzy rules constructed from the four projects were useful andcould classify correctly in other ï¬ve projects as well. In our
future work, we plan to optimize the fuzzy rules by analyzingfeature requests in a large scale as discussed in Section V-B.
The subject projects that were used in this study are very
well established. The conclusions we drew may not reï¬‚ect infeature requests found in newly established projects. As futurework, we plan to take a deeper look at the characteristics offeature requests during aging, while comparing the differencesbetween feature requests found in mature projects with freshprojects. Moreover, compared to traditional projects, this kindof feature request management approach may not be needed in
agile and continuous delivery and development ops situation.
We plan to expand our projects to include projects that followagile process to examine whether the proposed approach isapplicable to those projects.
Internal Validity Threats to internal validity might come
from the process of manual inspection and tagging. We un-derstand that such a process is subject to mistakes. To reducethat threat, we build an inspection team to reach agreementson different options.
Construct Validity Threats to construct validity are on
how the performance and misclassiï¬cation measurement aredeï¬ned. Both correctness and misclassiï¬cation rely on theimportance of the categories to tune weights. We deï¬ne theimportance by using the natural number sequence from one tosix, which might not reï¬‚ect the actual importance in reality.However, our experiments show that the fuzzy rules extractionis effective on the nine subject projects. For projects that donot deï¬ne the importance in natural sequence, our existingrules cannot apply. By following the approach and steps ofour experiments, a new set of fuzzy rules can be extractedand applied.
Conclusion Validity When drawing the conclusion for
RQ3, there might exist a threat in the comparison between
the machine learning methods with and without applying the
generated fuzzy rules. We use a lot of linguistic variablesgenerated from syntax and semantic patterns, but informationretrieval based machine learning methods mainly use variablesat lexical level such as tf-idf weights and cosine similarity[31].We plan to extend the comparison to include more advanced
rule-based classiï¬cation methods to minimize the threat in the
near future.
VII. R
ELA TED WORK
Automated Support for Feature Request Management.
Cleland-Huang et al. [32] proposed an automated forum
management system for organizing discussion threads and
grouping feature requests so that users can easily navigate
through all the posts and ï¬nd the location to place newfeature requests. Rahimi and Cledland-Huang [33] proposedan partially automated approach to create personas from a setof feature requests in open forums by applying association
rule mining. Gill et al. [34] offered a framework that solves
the natural language ambiguity problem in the Open Source
Software Development by combining the positive attributesof automation-oriented domains with manual supports from
448
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. developers. Shi et al. [35] conducted an empirical study
on Hibernate Community with 11 project and 1898 feature
requests. They proposed a Feature Tree Model to automaticallyidentify redundant feature requests. Thung et al. [36] proposed
an automated recommendation framework that uses the textual
description of a feature request as input, then outputs a listof recommended methods from a set of library APIs that
can be used to implement the feature request. Many studies
have been conducted in the area of supporting feature requestmanagement automatically. Most of the existing studies adoptmachine learning techniques. Yet our work takes on a differentdirection and focuses on structuring and analyzing individualfeature request through generating fuzzy rules.
Detecting Feature Request. Di Sorbo et al.[37] proposed
a classiï¬cation method to classify development emails ac-cording to their purposes by using natural language parsingtechniques. The proposed approach also can be used for awider application domain, such as the preprocessing phaseof various summarization tasks. Merten et al.[38] evaluated
and compared multiple preprocessing techniques and machine
learning techniques to detect software feature requests inissue tracking systems. Panichella et al.[39] presented an
automated approach to analyze user reviews into categories
such as feature request, opinion asking, etc. through natural
language processing, text analysis and sentiment analysis.
Maalej and Nabil[40] proposed a classiï¬cation method thatclassiï¬es reviews from App stores into bug reports, featurerequests, user experience, and ratings by combining proba-bilistic techniques with text classiï¬cation, natural language
processing and sentiment analysis techniques. Other studies
have been found to also capture user needs from app reviews
automatically[41][27][42]. Kochhar, Thung, and Lo [43] pro-posed an approach that extracts various feature values from abug report. Based on the values, they predicted whether a bugreport needed to be reclassiï¬ed. Panichella et al.[44] presented
a tool that combines natural language parsing, text analysis andsentiment analysis to automatically classify useful feedbackfrom app reviews in order to perform software maintenanceand evolution tasks. Compared to the existing studies, ourapproach combines fuzzy theory with linguistic analysis anduses the maximum of correctness as the optimization conditionto generate fuzzy rules in an incremental way, so that wecan provide new solution to ï¬nd rules and facilitate featurerequests detection.
Structuring Issue reports. Panichella et al.[45] mined
external communication channels such as emails and bugreports to structure them into source code documentation. Theapproach used the textual similarity measurement betweeneach paragraph in those emails and bug reports and the methodcontent in the source code. Bettenburg et al.[12] built a tool to
detect structural information from bug reports. Their approachused patch information, stack trace frames, source code andenumeration as ï¬lters to extract structures from bug reports.
Although feature requests are typically managed as issues in
the issue tracking systems, methods to structure issue reportscan not support the structuring of feature requests. Our workprovides a stable set of fuzzy rules to structure feature requests.
VIII. C
ONCLUSION
In this paper, we have presented an approach based on
fuzzy method and natural language process to automatically
classify the content of feature requests according to the sixcategories: intent, beneï¬t, drawback, example, explanation,and trivia. Our approach builds on the Stanford NLP Parserand applies an incremental optimization algorithm. To evaluatethe our approach, we have conducted an empirical studyon 2,112 sentences taken from 602 feature requests of ninepopular open source projects. The results indicate that ourapproach can reach high performance with the correctness of87% and misclassiï¬cation of 22% on the classiï¬cation results.Moreover, when using fuzzy rules as model variables, themachine learning methods with fuzzy rules can also improveperformances over 50% compared to the methods withoutrules. Therefore, we conclude that the fuzzy rules we generatedare effective and useful in classifying feature requests. Addi-tionally, we have implemented a tool that uses the approachto automatically structure the contents of feature requests. Thetool is publicly available along with our experiment datasetson the project website. We believe that our contributions canmake the feature requests easier to understand and analyze,thus improves feature request management in existing issuetracking systems.
A
CKNOWLEDGMENTS
We would like to thank Mingzhe Xing, Muhammad Ilyas
Azeem and Kamonphop Srisopha for their help during de-velopment of the ideas presented in this paper. This materialis based upon work supported by National Natural ScienceFoundation of China No. 61432001, 61602450, the National
Science and Technology Major Project under Grant No.
2014ZX01029101-002, and the Y outh Innovation Promotion
Association CAS under Grant No. 2016105. It is also sup-ported in part by the U.S. Department of Defense through theSystems Engineering Research Center (SERC) under ContractH98230-08-D-0171. SERC is a federally funded UniversityAfï¬liated Research Center managed by Stevens Institute of
Technology. It is also supported by the National Science
Foundation grant CMMI-1408909, Developing a ConstructiveLogic-Based Theory of V alue-Based Systems Engineering.
R
EFERENCES
[1] Bugzilla, â€œBugzilla, a Bug-Tracking System.â€ [Online]. Available:
https://www.bugzilla.org
[2] JIRA, â€œJIRA: plan, track, and release software.â€ [Online]. Available:
https://www.atlassian.com/software/jira
[3] Github, â€œGithub, a Web-based Git or V ersion Control Repository and
Internet Hosting Service.â€ [Online]. Available: https://github.com/
[4] X. Franch and G. Ruhe, â€œSoftware release planning,â€ in Proceedings
of the 38th International Conference on Software Engineering, ICSE
2016, Austin, TX, USA, May 14-22, 2016 - Companion V olume, 2016,
pp. 894â€“895.
[5] Y . Shin, J. H. Hayes, and J. Cleland-Huang, â€œGuidelines for bench-
marking automated software traceability techniques,â€ in 8th IEEE/ACM
International Symposium on Software and Systems Traceability, SST
2015, Florence, Italy, May 17, 2015, 2015, pp. 61â€“67.
449
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. [6] P . M Â¨ader, R. Oliveto, and A. Marcus, â€œEmpirical studies in software and
systems traceability,â€ Empirical Software Engineering, vol. 22, no. 3, pp.
963â€“966, 2017.
[7] G. Uddin and M. P . Robillard, â€œHow API documentation fails,â€ IEEE
Software, vol. 32, no. 4, pp. 68â€“75, 2015.
[8] L. Shi, H. Zhong, T. Xie, and M. Li, â€œAn empirical study on evolution of
API documentation,â€ in Proc. International Conference on Fundamental
Approaches to Software Engineering (F ASE 2011), March-April 2011,
pp. 416â€“431.
[9] K. Herzig, S. Just, and A. Zeller, â€œItâ€™s not a Bug, itâ€™s a Feature: How
Misclassiï¬cation Impacts Bug Prediction,â€ in Proceedings of the 2013
International Conference on Software Engineering, 2013, pp. 392â€“401.
[10] A. N. Meyer, L. E. Barton, G. C. Murphy, T. Zimmermann, and
T. Fritz, â€œThe work life of developers: Activities, switches and perceived
productivity,â€ IEEE Transactions on Software Engineering, 2017.
[11] A. J. Ko, B. A. Myers, and D. H. Chau, â€œA Linguistic Analysis of How
People Describe Software Problems,â€ in Visual Languages and Human-
Centric Computing, 2006. VL/HCC 2006. IEEE Symposium on, 2006,
pp. 127â€“134.
[12] N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim, â€œExtracting
Structural Information from Bug Reports,â€ in Proceedings of the 2008
International Working Conference on Mining Software Repositories,2008, pp. 27â€“30.
[13] N. Bettenburg, S. Just, A. Schr Â¨oter, C. Weiss, R. Premraj, and T. Zimmer-
mann, â€œWhat Makes a Good Bug Report?â€ in Proceedings of the 16th
ACM SIGSOFT International Symposium on F oundations of SoftwareEngineering, 2008, pp. 308â€“318.
[14] G. Trinder, â€œHow to Write a Feature Request,â€ 2006 (accessed May 5,
2017). [Online]. Available: https://blogs.msdn.microsoft.com/tdragger/2006/01/24/how-to-write-a-feature-request/
[15] Werner, â€œHow do I Write a Good Feature Request?â€ 2015 (accessed
May 5, 2017). [Online]. Available: https://meta.stackexchange.com/questions/7656/how-do-i-write-a-good-answer-to-a-question
[16] T. Winograd and F. Flores, Understanding Computers and Cognition: A
New F oundation for Design. Intellect Books, 1986.
[17] M.-C. De Marneffe and C. D. Manning, â€œThe Stanford Typed Depen-
dencies Representation,â€ in Coling 2008: proceedings of the workshop
on cross-framework and cross-domain parser evaluation, 2008, pp. 1â€“8.
[18] M. P . Marcus, M. A. Marcinkiewicz, and B. Santorini, â€œBuilding a Large
Annotated Corpus of English: The Penn Treebank,â€ Computational
linguistics, vol. 19, no. 2, pp. 313â€“330, 1993.
[19] E. Hllermeier, â€œFuzzy Methods in Machine Learning and Data Mining:
Status and Prospects,â€ Fuzzy Sets and Systems, vol. 156, no. 3, pp. 387
â€“ 406, 2005.
[20] R. Agrawal, T. Imieli Â´nski, and A. Swami, â€œMining Association Rules
Between Sets of Items in Large Databases,â€ in Acm sigmod record,
vol. 22, no. 2, 1993, pp. 207â€“216.
[21] C. D. Manning, P . Raghavan, and H. Schtze, â€œAn Introduction to
Information Retrieval,â€ Journal of the American Society for Information
Science & Technology, vol. 43, no. 3, pp. 824â€“825, 2008.
[22] S. V . Stehman, â€œâ€Selecting and Interpreting Measures of Thematic
Classiï¬cation Accuracyâ€,â€ Remote Sensing of Environment, vol. 62,
no. 1, pp. 77 â€“ 89, 1997.
[23] N. Bettenburg, R. Premraj, T. Zimmermann, and S. Kim, â€œExtracting
structural information from bug reports,â€ in Proceedings of the 2008 in-
ternational working conference on Mining software repositories. ACM,2008, pp. 27â€“30.
[24] F. Sebastiani, â€œMachine Learning in Automated Text Categorization,â€
ACM computing surveys (CSUR), vol. 34, no. 1, pp. 1â€“47, 2002.
[25] I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal, Data Mining: Practical
Machine Learning Tools and Techniques . Morgan Kaufmann, 2016.
[26]
G. Salton and C. Buckley, â€œTerm-weighting Approaches in AutomaticText Retrieval,â€ Information processing & management, vol. 24, no. 5,
pp. 513â€“523, 1988.
[27] A. Di Sorbo, S. Panichella, C. V . Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, â€œWhat Would Users Change
in My App? Summarizing App Reviews for Recommending Software
Changes,â€ in Proceedings of the 2016 24th ACM SIGSOFT International
Symposium on F oundations of Software Engineering, 2016, pp. 499â€“510.
[28] J. W. Raymond, E. J. Gardiner, and P . Willett, â€œRascal: Calculation
of Graph Similarity using Maximum Common Edge Subgraphs,â€ The
Computer Journal, vol. 45, no. 6, pp. 631â€“644, 2002.[29] C. Arora, M. Sabetzadeh, L. Briand, and F. Zimmer, â€œAutomated
Extraction and Clustering of Requirements Glossary Terms,â€ IEEE
Transactions on Software Engineering, vol. PP , no. 99, pp. 1â€“1, 2016.
[30] B. Boehm, C. Chen, K. Srisopha, and L. Shi, â€œThe Key Roles of
Maintainability in an Ontology for System Qualities,â€ in INCOSE
International Symposium, vol. 26, no. 1, 2016, pp. 2026â€“2040.
[31] A. D. Lucia, M. D. Penta, R. Oliveto, A. Panichella, and S. Panichella,
â€œUsing IR methods for Labeling Source Code Artifacts: Is it Worth-while?â€ in 2012 20th IEEE International Conference on Program
Comprehension (ICPC), 2012, pp. 193â€“202.
[32] J. Cleland-Huang, H. Dumitru, C. Duan, and C. Castro-Herrera, â€œAu-
tomated Support for Managing Feature Requests in Open Forums,â€Communications of the ACM, vol. 52, no. 10, pp. 68â€“74, 2009.
[33] M. Rahimi and J. Cleland-Huang, â€œPersonas in the Middle: Automated
Support for Creating Personas as Focal Points in Feature GatheringForums,â€ in Proceedings of the 29th ACM/IEEE international conference
on Automated software engineering, 2014, pp. 479â€“484.
[34] K. D. Gill, A. Raza, A. M. Zaidi, and M. M. Kiani, â€œSemi-automation
for Ambiguity Resolution in Open Source Software Requirements,â€inElectrical and Computer Engineering (CCECE), 2014 IEEE 27th
Canadian Conference on, 2014, pp. 1â€“6.
[35] L. Shi, C. Chen, Q. Wang, and B. Boehm, â€œIs It a New Feature
or Simply Donâ€™t Know Yet? On Automated Redundant OSS FeatureRequests Identiï¬cation,â€ in Requirements Engineering Conference (RE),
2016 IEEE 24th International, 2016, pp. 377â€“382.
[36] F. Thung, S. Wang, D. Lo, and J. Lawall, â€œAutomatic Recommenda-
tion of API Methods from Feature Requests,â€ in Automated Software
Engineering (ASE), 2013 IEEE/ACM 28th International Conference on,2013, pp. 290â€“300.
[37] A. Di Sorbo, S. Panichella, C. A. Visaggio, M. Di Penta, G. Canfora, and
H. C. Gall, â€œDevelopment Emails Content Analyzer: Intention Mining in
Developer Discussions (T),â€ in Automated Software Engineering (ASE),
2015 30th IEEE/ACM International Conference on, 2015, pp. 12â€“23.
[38] T. Merten, M. Falis, P . H Â¨ubner, T. Quirchmayr, S. B Â¨ursner, and B. Paech,
â€œSoftware Feature Request Detection in Issue Tracking Systems,â€ in Re-
quirements Engineering Conference (RE), 2016 IEEE 24th International,
2016, pp. 166â€“175.
[39] S. Panichella, A. D. Sorbo, E. Guzman, C. A. Visaggio, G. Canfora, and
H. C. Gall, â€œHow Can I Improve my App? Classifying User Reviews
for Software Maintenance and Evolution,â€ in 2015 IEEE International
Conference on Software Maintenance and Evolution, ICSME 2015,
Bremen, Germany, September 29 - October 1, 2015 , 2015, pp. 281â€“
290.
[40] W. Maalej and H. Nabil, â€œBug report, Feature Request, or simply
Praise? on Automatically Classifying app Reviews,â€ in 2015 IEEE 23rd
international requirements engineering conference (RE) , 2015, pp. 116â€“
125.
[41] L. Villarroel, G. Bavota, B. Russo, R. Oliveto, and M. Di Penta, â€œRelease
Planning of Mobile Apps based on User Reviews,â€ in Proceedings of
the 38th International Conference on Software Engineering, 2016, pp.
14â€“24.
[42] F. Palomba, M. L. V Â´asquez, G. Bavota, R. Oliveto, M. D. Penta,
D. Poshyvanyk, and A. D. Lucia, â€œUser Reviews Matter! Tracking
Crowdsourced Reviews to Support Evolution of Successful Apps,â€ in
2015 IEEE International Conference on Software Maintenance and
Evolution, ICSME 2015, Bremen, Germany, September 29 - October
1, 2015,
2015, pp. 291â€“300.
[43] P . S. Kochhar, F. Thung, and D. Lo, â€œAutomatic ï¬ne-grained issue
report reclassiï¬cation,â€ in Engineering of Complex Computer Systems
(ICECCS), 2014 19th International Conference on. IEEE, 2014, pp.126â€“135.
[44] S. Panichella, A. Di Sorbo, E. Guzman, C. A. Visaggio, G. Canfora,
and H. C. Gall, â€œArdoc: App reviews development oriented classiï¬er,â€ inProceedings of the 2016 24th ACM SIGSOFT International Symposiumon F oundations of Software Engineering. ACM, 2016, pp. 1023â€“1027.
[45] S. Panichella, J. Aponte, M. Di Penta, A. Marcus, and G. Canfora,
â€œMining Source Code Descriptions from Developer Communications,â€inProgram Comprehension (ICPC), 2012 IEEE 20th International
Conference on, 2012, pp. 63â€“72.
450
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. 