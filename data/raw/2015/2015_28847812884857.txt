Automated Parameter Optimization of Classiﬁcation
T
echniques for Defect Prediction Models
Chakkrit Tantithamthavorn1, Shane McIntosh2, Ahmed E. Hassan3, Kenichi Matsumoto1
1Nara Institute of Science and
Technology, Japan
{chakkrit-t,matumoto}
@is.naist.jp2McGill University, Canada
shane.mcintosh@mcgill.ca3Queen’s University, Canada
ahmed@cs.queensu.ca
ABSTRACT
Defect prediction models are classiﬁers that are trained to
identify defect-prone software modules. Such classiﬁers have
conﬁgurableparametersthatcontroltheircharacteristics(e.g.,
the number of trees in a random forest classiﬁer). Recent
studies show that these classiﬁers may underperform due
to the use of suboptimal default parameter settings. How-
ever, it is impractical to assess all of the possible settings
in the parameter spaces. In this paper, we investigate the
performance of defect prediction models where Caret — an
automated parameter optimization technique — has been
applied. Through a case study of 18 datasets from systems
that span both proprietary and open source domains, we
ﬁnd that (1) Caret improves the AUC performance of de-
fect prediction models by as much as 40 percentage points;
(2) Caret-optimized classiﬁers are at least as stable as (with
35% of them being more stable than) classiﬁers that are
trained using the default settings; and (3) Caret increases
the likelihood of producing a top-performing classiﬁer by as
much as 83%. Hence, we conclude that parameter settings
can indeed have a large impact on the performance of de-
fect prediction models, suggesting that researchers should
experiment with the parameters of the classiﬁcation tech-
niques. Since automated parameter optimization techniques
like Caret yield substantially beneﬁts in terms of perfor-
mance improvement and stability, while incurring a manage-
able additional computational cost, they should be included
in future defect prediction studies.
CCS Concepts
•General and reference →Experimentation; •Software
and its engineering →Software defect analysis; Search-
based software engineering;
Keywords
Software defect prediction, experimental design, classiﬁca-
tion techniques, parameter optimization
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’16, May 14-22, 2016, Austin, TX, USA
c/circlecopyrt2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.28848571. INTRODUCTION
The limited Software Quality Assurance (SQA) resources
of software organizations must focus on software modules
(e.g., source code ﬁles) that are likely to be defective in the
future. To that end, defect prediction models are trained to
identify defect-prone software modules using statistical or
machine learning classiﬁcation techniques.
Such classiﬁcation techniques often have conﬁgurable pa-
rameters that control characteristics of the classiﬁers that
they produce. For example, the number of decision trees of
which a random forest classiﬁer is comprised can be conﬁg-
ured prior to training the forest. Furthermore, the number
of non-overlapping clusters of which a k-nearest neighbours
classiﬁer is comprised must be conﬁgured prior to using the
classiﬁcation technique.
Since the optimal settings for these parameters are not
known ahead of time, the settings are often left at default
values. Prior work suggests that defect prediction models
may underperform if they are trained using suboptimal pa-
rameter settings. For example, Jiang et al.[22] and Tosun et
al.[58] also point out that the default parameter settings of
randomforestandnaivebayesareoftensuboptimal. Koru et
al.[13] and Mende et al.[36, 37] show that selecting diﬀer-
ent parameter settings can impact the performance of de-
fect models. Hall et al.[15] show that unstable classiﬁcation
techniques may underperform due to the use of default pa-
rameter settings. Mittas et al.[41] and Menzies et al.[40]
argue that unstable classiﬁcation techniques can make repli-
cation of defect prediction studies more diﬃcult.
Indeed, we perform a literature analysis that reveals that
26 of the 30 most commonly used classiﬁcation techniques
(87%) require at least one parameter setting. Since such
parameter settings may impact the performance of defect
prediction models, the settings should be carefully selected.
However, itisimpracticaltoassessallofthepossiblesettings
in the parameter space of a single classiﬁcation technique [1,
16, 27]. For example, Kocaguneli et al.[27] point out that
there are at least 17,000 possible settings to explore when
training k-nearest neighbours classiﬁer.
In this paper, we investigate the performance of defect
prediction models where Caret [30] — an oﬀ-the-shelf au-
tomated parameter optimization technique — has been ap-
plied. Caretevaluatescandidateparametersettingsandsug-
gests the optimized setting that achieves the highest perfor-
mance. Through a case study of 18 datasets from systems
that span both proprietary and open source domains, we
record our observations with respect to two dimensions:
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   321
Table 1: Overview of studied parameters.
Family Family Description P arameter Name Parameter Description Classiﬁcation techniques that
apply with their default and can-
didate parameter values.
Naive
B
ayesNaive Bayes is a probability model that assumes that pre-
dictors are independent of each other.
Techniques: Naive Bayes (NB).Laplace Correction [N] Laplace correction (0 indicates no
correction).NB={0}
Distribution Type [L] TRUE indicates a kernel density es-
t
imation, while FALSE indicates a nor-
mal density estimation.NB={TRUE, FALSE}
Nearest
N
eigh-
bourNearest neighbour is an algorithm that stores all available
observations and classiﬁes new observations based on its
similarity to prior observations.
Techniques: k-Nearest Neighbour (KNN).#Clusters [N] The numbers of non-overlapping
clusters to produce.KNN={1, 5, 9, 13, 17}
RegressionL
ogistic regression is a technique for explaining binary de-
pendent variables. MARS is a non-linear regression mod-
elling technique.
Techniques: GLM and MARS.Degree Interaction [N] The maximum degree of interac-
tion (Friedman’s mi). The default is 1,
meaning build an additive model (i.e.,
no interaction terms).MARS= {1}
Partial
L
east
SquaresPartial Least Squares regression generalizes and combines
features from principal component analysis and multiple
regression.
Techniques: Generalized Partial Least Squares (GPLS).#Components [N] The number of PLS components. GPLS= {1, 2, 3, 4, 5}
Neural
N
etworkNeural network techniques are used to estimate or ap-
proximate functions that can depend on a large number
of inputs and are generally unknown.
Techniques: Standard (NNet), Model Averaged (AVN-
Net), Feature Extraction (PCANNet), Radial Basis Func-
tions (RBF), Multi-layer Perceptron (MLP), Voted-MLP
(MLPWeightDecay), and Penalized Multinomial Regres-
sion (PMR).Bagging [L] Should each repetition apply bag-
gin?AVNNet={TRUE, FALSE}
Weight Decay [N] A penalty factor to be applied to
t
he errors function.MLPWeightDecay, PMR, AVNNet,
NNet, PCANNet= {0, 0.0001, 0.001,
0.01, 0.1}, SVMLinear= {1}
#Hidden Units [N] Numbers of neurons in the hidden
l
ayers of the network that are used to
produce the prediction.MLP, MLPWeightDecay, AVNNet,
NNet, PCANNet= {1, 3, 5, 7, 9},
RBF={11, 13, 15, 17, 19}
Discrimina-
t
ion
AnalysisDiscriminant analysis applies diﬀerent kernel functions
(e.g., linear) to classify a set of observations into prede-
ﬁned classes based on a set of predictors.
Techniques: Linear Discriminant Analysis (LDA), Pe-
nalized Discriminant Analysis (PDA), and Flexible Dis-
criminant Analysis (FDA).Product Degree [N] The number of degrees of freedom
that are available for each term.FDA={1}
Shrinkage Penalty
C
oeﬃcient[N] A shrinkage parameter to be applied
to each tree in the expansion (a.k.a.,
learning rate or step-size reduction).PDA={1, 2, 3, 4, 5}
#Terms [N] The number of terms in the model. FDA= {1 0, 20, 30, 40, 50}
Rule-basedR
ule-based techniques transcribe decision trees using a set
of rules for classiﬁcation.
Techniques: Rule-based classiﬁer (Rule), and Ripper
classiﬁer (Ripper).#Optimizations [N] The number of optimization itera-
tions.Ripper= {1,2, 3, 4, 5}
Decision
T
rees-
BasedDecision trees use feature values to classify instances.
Techniques: C4.5-like trees (J48), Logistic Model Trees
(LMT), and Classiﬁcation And Regression Trees (CART).Complexity [N] A penalty factor to be applied to
the error rate of the terminal nodes of
the tree.CART= {0.0001, 0.001, 0.01, 0.1,
0.5}
Conﬁdence [N] The conﬁdence factor used for prun-
i
ng (smaller values incur more pruning).J48={0.25}
#Iterations [N] The numbers of iterations. LMT={1, 21, 41, 61, 81}
SVMS
upport Vector Machines (SVMs) use a hyperplane to sep-
arate two classes (i.e., defective or not).
Techniques: SVM with Linear kernel (SVMLinear), and
SVM with Radial basis function kernel (SVMRadial).Sigma [N] The width of Gaussian kernels. SVMRadial= {0.1, 0.3, 0.5, 0.7, 0.9}
Cost [N] A penalty factor to be applied to
t
he number of errors.SVMRadial= {0.25, 0.5, 1, 2, 4},
SVMLinear= {1}
BaggingB
agging methods combine diﬀerent base learners together
to solve one problem.
Technique: Random Forest (RF), Bagged CART
(BaggedCART)#Trees [N] The numbers of classiﬁcation trees. RF= {10, 20, 30, 40, 50}
BoostingB
oosting performs multiple iterations, each with diﬀerent
example weights, and makes predictions using voting of
classiﬁers.
Techniques: Gradient Boosting Machine (GBM), Adap-
tive Boosting (AdaBoost), Generalized linear and Addi-
tive Models Boosting (GAMBoost), Logistic Regression
Boosting (LogitBoost), eXtreme Gradient Boosting Tree
(xGBTree), and C5.0.#Boosting Itera-
tions[N] The numbers of iterations that are
used to construct models.C5.0={1, 10, 20, 30, 40}, GAM-
Boost={50, 100, 150, 200, 250},
LogitBoost= {11, 21, 31, 41, 51},
GBM,xGBTree= {50,100, 150, 200, 250}
#Trees [N] The numbers of classiﬁcation trees. AdaBoost= {5 0, 100, 150, 200, 250}
Shrinkage [N] A shrinkage factor to be applied to
e
ach tree in the expansion (a.k.a., learn-
ing rate or step-size reduction).GBM={0.1}, xGBTree= {0.3}
Max Tree Depth [N] The maximum depth per tree. AdaBoost, GBM, x GBTree= {1, 2, 3,
4, 5}
Min. Terminal
N
ode Size[N] The minimum terminal nodes in
trees.GBM={10}
Winnow [L] Should predictor winnowing (i.e fea-
t
ure selection) be applied?C5.0={FALSE, TRUE}
AIC Prune? [L] Should pruning using stepwise fea-
t
ure selection be applied?GAMBoost= {FALSE, TRUE}
Model Type [F] Either tree for the predicted class or
r
ules for model conﬁdence values.C5.0={rules, tree}
[N] denotes a numeric value; [L] denotes a logical value; [F] d enotes a factor value.
The default values are shown in bold typeface and correspond to the default values of the Caret R package.
(1) Performance improvement: C aretimprovestheAUC
performance of defect prediction models by up to 40
percentage points. Moreover, the performance improve-
ment provided by Caret is non-negligible for 16 of the
26 studied classiﬁcation techniques (62%).
(2) Performance stability: Caret-optimizedclassiﬁersare
at least as stable as classiﬁers that are trained using the
default settings. Moreover, the Caret-optimized clas-
siﬁers of 9 of the 26 studied classiﬁcation techniques
(35%) are more stable than classiﬁers that are trained
using the default values.
Sinceweﬁndthatparametersettingscanhavesuchanim-
pact on model performance, we revisit prior analyses thatrank classiﬁcation techniques by their ability to yield top-
performing defect prediction models. We ﬁnd that Caret
increases the likelihood of producing a top-performing clas-
siﬁer by as much as 83%, suggesting that automated pa-
rameter optimization can substantially shift the ranking of
classiﬁcation techniques.
Our results lead us to conclude that parameter settings
can indeed have a large impact on the performance of de-
fect prediction models, suggesting that researchers should
experiment with the parameters of the classiﬁcation tech-
niques. Since automated parameter optimization techniques
like Caret yield substantially beneﬁts in terms of perfor-
mance improvement and stability, while incurring a manage-
able additional computational cost, they should be included
in future defect prediction studies.
322Tothebestofourknowledge, thisistheﬁrstpapertostudy:
–
A large collection of 43 parameters that are derived
from 26 of the most frequently-used classiﬁcation tech-
niques in the context of defect prediction.
– The improvement and stability of the performance of
defect prediction models when automated parameter
optimization is applied.
– The ranking of classiﬁcation techniques for defect pre-
diction when automated parameter optimization is ap-
plied.
Paper Organization. The remainder of the paper is or-
ganized as follows. Section 2 illustrates the importance of
parameter settings of classiﬁcation techniques for defect pre-
diction models. Section 3 positions this paper with respect
to the related work. Section 4 presents the design and ap-
proach of our case study. Section 5 presents the results of
our case study with respect to our two research questions.
Section 6 revisits prior analyses that rank classiﬁcation tech-
niquesbytheirlikelihoodofproducingtop-performingdefect
prediction models. Section 7 discusses the broader implica-
tions of our ﬁndings. Section 8 discloses the threats to the
validity of our study. Finally, Section 9 draws conclusions.
2. THE RELEV ANCE OF PARAMETER
SETTINGS FOR DEFECT PREDICTION
MODELS
A variety of classiﬁcation techniques are used to train de-
fect prediction models. Since some classiﬁcation techniques
do not require parameter settings (e.g., logistic regression),
we ﬁrst assess whether the most commonly used classiﬁca-
tion techniques require parameter settings.
We ﬁrst start with the 6 families of classiﬁcation tech-
niques that are used by Lessmann et al.[32]. Based on a
recent literature review of Laradji et al.[31], we add 5 ad-
ditional families of classiﬁcation techniques that have been
recently used in defect prediction studies. In total, we study
30 classiﬁcation techniques that span 11 classiﬁer families.
Table 1 provides an overview of the 11 families of classiﬁca-
tion techniques.
Our literature analysis reveals that 26 of the 30 most com-
monly used classiﬁcation techniques (87%) require at least
one parameter setting. Table 1 provides an overview of the
25 unique parameters that apply to the studied classiﬁca-
tion techniques.
26 of the 30 most commonly used classiﬁcation techniques
(87%) require at least one parameter setting, indicating that
selecting an optimal parameter setting for defect prediction
models is an important experimental design choice.
3. RELATED WORK & RESEARCH QUES-
TIONS
R
ecent research has raised concerns about parameter set-
tings of classiﬁcation techniques when applied to defect pre-
diction models. For example, Koru et al.[13] and Mende et
al.[36, 37] point out that selecting diﬀerent parameter set-
tings can impact the performance of defect models. Jiang et
al.[22] and Tosun et al.[58] also point out that the de-
fault parameter settings of research toolkits (e.g., R [46],
Weka [14], Scikit-learn [44], MATLAB [35]) are suboptimal.Although prior work suggests that defect prediction mod-
els may underperform if they are trained using suboptimal
parameter settings, parameters are often left at their de-
fault values. For example, Mende et al.[38] use the default
number of decision trees to train a random forest classiﬁer
(provided by an R package). Weyuker et al.[60] also train
defect models using the default setting of C4.5 that is pro-
vided by Weka. Jiang et al.[21] and Bibi et al.[2] also use
the default value of the k-nearest neighbours classiﬁcation
technique (k = 1).
In addition, the implementations of classiﬁcation tech-
niques that are provided by diﬀerent research toolkits often
use diﬀerent default settings. For example, for the number
of decision trees of the random forest technique, the default
settings vary from 10 for the bigrfR package [34], 50 for
MATLAB [35], 100 for Weka [14], to 500 for the random-
ForestR package [33]. Moreover, for the number of hidden
layers of the neuron networks techniques, the default set-
tings vary from 1 for the neuralnet R package [11], 2 for the
nnetR package [48] and Weka [14], to 10 for MATLAB [35].
Such a variation of default settings that are provided by dif-
ferent research toolkits may inﬂuence conclusions of defect
prediction studies [53].
There are many empirical studies in the area of Search-
Based Software Engineering (SBSE) [16] that aim to opti-
mize software engineering tasks (e.g., software testing [20]).
However, little SBSE research has been applied to optimize
the parameters of classiﬁcation techniques for defect pre-
diction models. Although prior studies have explored the
impact of parameter settings, they have only explored a few
parameter settings. To more rigorously explore the param-
eter space of classiﬁcation techniques for defect prediction
models, we formulate the following research question:
(RQ1) How much does the performance of defect prediction
models improve when automated parameter optimization is
applied?
Recent research voices concerns about the stability of per-
fo
rmanceestimatesthatareobtainedfromclassiﬁcationtech-
niques when applied to defect prediction models. For ex-
ample, Menzies et al.[40] and Mittas et al.[41] argue that
unstable classiﬁcationtechniques canmake replicationof de-
fect prediction studies more diﬃcult. Shepperd et al.[49],
and Jorgensen et al.[23] also point out that the unstable
performance estimates that are produced by classiﬁcation
techniques may introduce bias, which can mislead diﬀerent
research groups to draw erroneous conclusions. Myrtveit et
al.[42] show that high variance in performance estimates
from classiﬁcation techniques is a critical problem in com-
parative studies of prediction models. Song et al.[51] also
show that applying diﬀerent settings to instable classiﬁca-
tion techniques will provide diﬀerent results.
Like any form of classiﬁer optimization, automated pa-
rameter optimization may increase the risk of overﬁtting ,
i.e., producing a classiﬁer that is too specialized for the data
from which it was trained to apply to other datasets. To in-
vestigate whether parameter optimization is impacting the
stability of defect prediction models, we formulate the fol-
lowing research questions:
(RQ2) How stable is the performance of defect prediction
models when automated parameter optimization is applied?
3234. CASE STUDY APPROACH
I
n this section, we discuss our selection criteria for the
studied systems and then describe the design of our case
study experiment that we perform in order to address our
research questions.
4.1 Studied Datasets
In selecting the studied datasets, we identiﬁed three im-
portant criteria that needed to be satisﬁed:
–Criterion 1 — Publicly-available defect datasets
from diﬀerent corpora: Our recent work shows that
researcherstendtoreuseexperimentalcomponents(e.g.,
datasets, metrics, and classiﬁers) [56]. Song et al.[52]
and Ghotra et al.[12] also show that the performance
of defect prediction models can be impacted by the
dataset from which they are trained. To combat po-
tential bias in our conclusions and to foster replica-
tion of our experiments, we choose to train our defect
prediction models using datasets from diﬀerent cor-
pora and domains that are hosted in publicly-available
data repositories.
–Criterion 2 — Dataset robustness: Mende et al.[36]
show that models that are trained using small datasets
may produce unstable performance estimates. An in-
ﬂuential characteristic in the performance of a classiﬁ-
cation technique is the number of Events Per Variable
(EPV) [45, 55], i.e., the ratio of the number of oc-
currences of the least frequently occurring class of the
dependent variable (i.e., the events) to the number of
independent variables that are used to train the model
(i.e., the variables). Our recent work shows that defect
prediction models that are trained using datasets with
a low EPV value are especially susceptible to unstable
results [55]. To mitigate this risk, we choose to study
datasets that have an EPV above 10, as suggested by
Peduzzi et al.[45].
–Criterion 3 — Sane defect data: Since it is un-
likely that more software modules have defects than
are free of defects, we choose to study datasets that
have a rate of defective modules below 50%.
Tosatisfycriterion1, webeganourstudyusing101publicly-
available defect datasets. 76 datasets are downloaded from
the Tera-PROMISE Repository,112 clean NASA datasets
are provided by Shepperd et al.[50], 5 are provided by
Kimet al.[26, 61], 5 are provided by D’Ambros et al.[6, 7],
and 3 are provided by Zimmermann et al.[64]. To satisfy
criterion 2, we exclude the 78 datasets that we found to
have EPV values below 10. To satisfy criterion 3, we ex-
clude an additional 5 datasets because they have a defective
rate above 50%.
Table2providesanoverviewofthe18datasetsthatsatisfy
our criteria for analysis. To strengthen the generalizability
of our results, the studied datasets include proprietary and
open source systems of varying size and domain.
Figure 1 provides an overview of the approach that we
apply to each studied system. We describe each step in the
approach below.
1http://openscience.us/repo/Table 2: An overview of the studied systems.
Domain System Defective #Files #Metrics EPV
R
ate
NASA JM112 1% 7,782 21 80
PC5128% 1,711 38 12
Proprietary Prop-121 5% 18,471 20 137
Prop-2211% 23,014 20 122
Prop-3211% 10,274 20 59
Prop-4210% 8,718 20 42
Prop-5215% 8,516 20 65
Apache Camel 1.223 6% 608 20 11
Xalan 2.5248% 803 20 19
Xalan 2.6246% 885 20 21
Eclipse Platform 2.031 4% 6,729 32 30
Platform 2.1311% 7,888 32 27
Platform 3.0315% 10,593 32 49
Debug 3.4425% 1,065 17 15
SWT 3.4444% 1,485 17 38
JDT521% 997 15 14
Mylyn513% 1,862 15 16
PDE514% 1,497 15 14
1Provided by Shepperd et al.[50].4Provided by Kim et al.[26, 61].
2Provided by Jureczko et al.[24].5Provided by Ambros et al.[6].
3Provided by Zimmermann et al.[62].
4.2 Generate Bootstrap Sample
In
ordertoensurethattheconclusionsthatwedrawabout
our models are robust, we use the out-of-sample bootstrap
validation technique [8, 55], which leverages aspects of sta-
tistical inference [9]. The out-of-sample bootstrap is made
up of two steps:
(Step 1) A bootstrap sample of size Nis randomly drawn
with replacement from an original dataset, which
is also of size N.
(Step 2) A model is trained using the bootstrap sample
and tested using the rows that do not appear
in the bootstrap sample. On average, 36.8% of
the rows will not appear in the bootstrap sample,
since it is drawn with replacement [8].
Theout-of-samplebootstrapprocessisrepeated100times,
and the average out-of-sample performance is reported as
the performance estimate.
Unlike the ordinary bootstrap, the out-of-sample boot-
strap technique ﬁts models using the bootstrap samples, but
rather than testing the model on the original sample, the
model is instead tested using the rows that do not appear
in the bootstrap sample [55]. Thus, the training and testing
corpus do not share overlapping observations.
Unlikek-foldcross-validation, theout-of-samplebootstrap
technique ﬁts models using a dataset that is of equal length
to the original dataset. Cross-validation splits the data into
kequal parts, using k- 1 parts for ﬁtting the model, setting
aside 1 fold for testing. The process is repeated ktimes, us-
ing a diﬀerent part for testing each time. However, Mende et
al.[36]pointoutthatthescarcityofdefectivemodulesinthe
smalltestingcorporaof10-foldcrossvalidationmayproduce
biased and unstable results. Prior studies have also shown
that 10-fold cross validation can produce unstable results
for small samples [3]. On the other hand, our recent re-
search demonstrates that the out-of-sample bootstrap tends
to produce the least biased and most stable performance es-
timates [55]. Moreover, the use of out-of-sample bootstrap
is recommended for high-skewed datasets [17], as is the case
in our defect prediction datasets.
324Caret- 
o
ptimized 
Setting
Default  
Se
tting
Construct  
d
efect 
models
Defect 
Mo
del
Defect 
Mo
del
Default 
Pe
rformanceCaret-optimized 
Performance
Defect  
D
ataset
Testing  
Corpus
Training 
Corpus
Generate 
b
ootstrap 
sample
Repeat 100 times
(Step 1) 
G
enerate 
candidate 
settings Caret Parameter Optimization
(Step 2)
Ev
aluate 
candidate 
settings 
(Step 3) 
I
dentify  
the Caret-
optimized 
setting
Calculate 
p
erform-
ance
Figure 1: An overview of our case study approach.
4.3
Caret Parameter Optimization
Since it is impractical to assess all of the possible param-
eter settings of the parameter spaces, we use the optimized
parameter settings suggested by the trainfunction of the
caretR package [30]. Caret suggests candidate settings for
each of the studied classiﬁcation techniques, which can be
checked using the getModelInfo function of the caretR
package [30]. The optimization process is made up of three
steps.
(Step 1) Generate candidate parameter settings: The
trainfunction will generate candidate parameter
settings based on a given budget threshold (i.e.,
tune length) for evaluation. The budget thresh-
old indicates the number of diﬀerent values to be
evaluated for each parameter. As suggested by
Kuhn [28], we use a budget threshold of 5. For
example, the number of boosting iterations of the
C5.0 classiﬁcation technique is initialized to 1 and
is increased by 10 until the number of candidate
settings reaches the budget threshold (e.g., 1, 10,
20, 30, 40). Table 1 shows the candidate param-
eter settings for each of the studied parameters.
The default settings are shown in bold typeface.
(Step 2) Evaluate candidate parameter settings: Caret
evaluates all of the potential combinations of the
candidate parameter settings. For example, if a
classiﬁcation technique accepts 2 parameters with
5 candidate parameter settings for each, Caret
will explore all 25 potential combinations of pa-
rameter settings (unless the budget is exceeded).
We use 100 repetitions of the out-of-sample boot-
strap to estimate the performance of classiﬁers
that are trained using each of the candidate pa-
rameter settings. For each candidate parameter
setting, a classiﬁer is ﬁt to a subsample of the
training corpus and we estimate the performance
of a model using those rows in the training cor-
pus that do not appear in the subsample that was
used to trained the classiﬁer.
(Step 3) Identify the Caret-optimized setting: Fi-
nally, the performance estimates are used to iden-
tify which parameter settings are the most opti-
mal. The Caret-optimized setting is the one that
achieves the highest performance estimate.4.4 Construct Defect Models
In order to measure the impact that automated parame-
ter optimization has on defect prediction models, we train
defect models using the Caret-optimized settings and the
default settings. To ensure that the training and testing
corpora have similar characteristics, we do not re-balance or
re-sample the training data, as suggested by Turhan [59].
Normality Adjustment. Analysis of the distributions of
ourindependentvariablesrevealsthattheyareright-skewed.
Assuggestedbypreviousresearch[22], wemitigatethisskew
by log-transforming each independent variable (ln( x+ 1))
prior to using them to train our models.
4.5 Calculate Performance
Prior studies have argued that threshold-dependent per-
formance metrics (i.e., precision and recall) are problematic
because they: (1) depend on an arbitrarily-selected thresh-
old [32, 47] and (2) are sensitive to imbalanced data [18].
Instead, we use the Area Under the receiver operator char-
acteristic Curve (AUC) tomeasurethediscriminationpower
of our models as suggested by recent research [32].
The AUC is a threshold-independent performance metric
that measures a classiﬁer’s ability to discriminate between
defective and clean modules (i.e., do the defective modules
tend to have higher predicted probabilities than clean mod-
ules?). AUC is computed by measuring the area under the
curve that plots the true positive rate against the false pos-
itive rate, while varying the threshold that is used to deter-
mine whether a ﬁle is classiﬁed as defective or not. Values
of AUC range between 0 (worst performance), 0.5 (random
guessing performance), and 1 (best performance).
5. CASE STUDY RESULTS
In this section, we present the results of our case study
with respect to our two research questions.
(RQ1) How much does the performance of de-
fect prediction models improve when automated
parameter optimization is applied?
Approach. To address RQ1, we start with the AUC per-
formance distribution of the 26 classiﬁcation techniques that
require at least one parameter setting (see Section 2). For
each classiﬁcation technique, we compute the diﬀerence in
the performance of classiﬁers that are trained using default
and Caret-optimized parameter settings. We then use box-
plots to present the distribution of the performance diﬀer-
ence for each of the 18 studied datasets. To quantify the
magnitude of the performance improvement, we use Co-
hen’sdeﬀect size [4], which is the diﬀerence between the
two means divided by the standard deviation of the data
(d=¯x1−¯x2
s.d.). The magnitude is assessed using the thresh-
olds provided by Cohen [5]:
eﬀect size =

negligible if Cohen’s d≤0.2
small if 0.2<Cohen’s d≤0.5
medium if 0.5<Cohen’s d≤0.8
large if 0.8<Cohen’s d
Furthermore, understanding the most inﬂuential parame-
ters would allow researchers to focus their optimization ef-
fort. To this end, we investigate the performance diﬀerence
325Large Medium Small Negligible
●●
●
●
●
● ●
●
●●
●
●0.00.10.20.30.4
C5.0
AdaBoostAVNNetCART
PCANNetNNetFDA
MLPWeightDecayMLPLMTGPLS
LogitBoostKNN
xGBTreeGBMNBRBF
SVMRadialGAMBoostRF
RipperPMRPDAMARS
SVMLinearJ48AUC Performance Improvement
Figure 2: The performance improvement and its Co-
h
en’sdeﬀect size for each of the studied classiﬁca-
tion techniques.
for each of the studied parameters. To quantify the indi-
vidual impact of each parameter, we train a classiﬁer with
all of the studied parameters set to their default settings,
except for the parameter whose impact we want to measure,
which is set to its Caret-optimized setting. We estimate the
impact of each parameter using the diﬀerence of its perfor-
mance with respect to a classiﬁer that is trained entirely
using default parameter settings.
Results. Caret improves the AUC performance by
up to 40 percentage points. Figure 2 shows the perfor-
mance improvement for each of the 18 studied datasets and
for each of the classiﬁcation techniques. The boxplots show
that Caret can improve the AUC performance by up to 40
percentage points. Moreover, the performance improvement
provided by applying Caret is non-negligible (i.e., d >0.2)
for 16 of the 26 studied classiﬁcation techniques (62%). This
indicates that parameter settings can substantially inﬂuence
the performance of defect prediction models.
C5.0 boosting yields the largest performance im-
provement when Caret is applied. According to Co-
hen’sd, the performance improvement provided by applying
Caret is large for 9 of the 26 studied classiﬁcation techniques
(35%). On average, Figure 2 shows that the C5.0 boost-
ing classiﬁcation technique beneﬁts most by applying Caret,
with a median performance improvement of 27 percentage
points. Indeed, the C5.0 boosting classiﬁcation technique
improves from 6 to 40 percentage points.
Moreover, Figure3showsthatthe #boosting iterations
parameter of the C5.0 classiﬁcation technique is the most
inﬂuential parameter, while the winnowandmodel type pa-
rameters tend to have less of an impact. Indeed, the default
#boosting iterations setting that is provided by the C5.0
R package [29] is 1, indicating that only one C5.0 tree model
is used for prediction. Moreover, we ﬁnd that, when large
datasets of more than 1,000 modules are analyzed, the per-●●● ●●●●
# Boosting Iterations of GAMBoostMax T
ree Depth of GBM# Boosting Iterations of LogitBoostMax Tree Depth of xGBTree#Hidden Units of NNet#Hidden Units of PCANNet#Hidden Units of AVNNet#Neighbors of KNN#Hidden Units of MLPWeightDecay#Components of GPLS# Iteratons of LMTWeight Decay of NNet#Hidden Units of MLPWeight Decay of MLPWeightDecay#Terms of FDAMax Tree Depth of AdaBoostComplexity Parameter of CARTWeight Decay of AVNNetWeight Decay of PCANNet# Boosting Iterations of C5.0
0.0 0.1 0.2 0.3 0.4
AUC  Performance Difference
Figure 3: The AUC performance diﬀerence of the
t
op-20 most sensitive parameters.
formance of C5.0 boosting with the default setting tends
to underperform. Nevertheless, we ﬁnd that the optimal
#boosting iterations parameter is 40, suggesting that the
default parameter settings of the research toolkits are sub-
optimal for defect prediction datasets, which agrees with the
suspicion of prior studies [15, 22, 51, 58].
In addition to C5.0 boosting, other classiﬁers also yield a
considerably large beneﬁt. Figure 2 shows that the perfor-
mance of the adaptive boosting (i.e., AdaBoost), advanced
neural networks (i.e., AVNNet, PCANNet, NNet, MLP, and
MLPWeightDecay), CART, and ﬂexible discriminant anal-
ysis (FDA) classiﬁcation techniques also have a large eﬀect
size with a median performance improvement from 13-24
percentage points. Indeed, Figure 3 shows that the ﬂuctua-
tionoftheperformanceoftheadvancedneuralnetworktech-
niques is largely caused by changing the weight decay , but
not the#hidden units orbagging parameters. Moreover,
thecomplexity parameter of CART and max tree depth of
adaptive boosting classiﬁcation techniques are also sensitive
to parameter optimization.
Caret improves the AUC performance of defect prediction
models by up to 40 percentage points. Moreover, the per-
formance improvement provided by Caret is non-negligible
for 16 of the 26 studied classiﬁcation techniques (62%).
(RQ2) How stable is the performance of defect
pr
ediction models when automated parameter
optimization is applied?
Approach. To address RQ2, we start with the AUC per-
formance distribution of the 26 studied classiﬁcation tech-
niques on each of the 18 studied datasets. The stability of
a classiﬁcation technique is measured in terms of the vari-
ability of the performance estimates that are produced by
the 100 iterations of the out-of-sample bootstrap. For each
classiﬁcation technique, we compute the standard deviation
326Large Medium Small Negligible
0.00.51.01.52.0
C5.0
AdaBoostAVNNetCART
PCANNetNNetFDA
MLPWeightDecayMLPLMTGPLS
LogitBoostKNN
xGBTreeGBMNBRBF
SVMRadialGAMBoostRF
RipperPMRPDAMARS
SVMLinearJ48Stability Ratio
Figure 4: The stability ratio of the classiﬁers that
a
re trained using Caret-optimized settings com-
pared to the classiﬁers that are trained using de-
fault settings for each of the studied classiﬁcation
techniques.
(S.D.) of the bootstrap performance estimates of the classi-
ﬁers where Caret-optimized settings have been used and the
S.D. of the bootstrapperformanceestimates of the classiﬁers
where the default settings have been used. To analyze the
diﬀerence of the stability between two classiﬁers, we present
distribution of the stability ratio (i.e., S.D. of the optimized
classiﬁer divided by the S.D. of the default classiﬁer) of the
two classiﬁers when apply to 18 studied datasets.
Similar to RQ1, we analyze the parameters that have the
largest impact on the stability of the performance estimates.
To this end, we investigate the stability ratio for each of the
studied parameters. To quantify the individual impact of
each parameter, we train a classiﬁer with all of the studied
parameters set to their default settings, except for the pa-
rameter whose impact we want to measure, which is set to
its Caret-optimized setting. We estimate the impact of each
parameter using the stability ratio of its S.D. of performance
estimates with respect to a classiﬁer that is trained entirely
using default settings.
Results. Caret-optimized classiﬁers are at least as
stable as classiﬁers that are trained using the de-
fault settings. Figure 4 shows that there is a median sta-
bility ratio of at least one for all of the studied classiﬁcation
techniques. Indeed, we ﬁnd that the median ratio of one
tends to appear for the classiﬁcation techniques that yield
negligible performance improvements in RQ1. These tight
stability ratio ranges that are centered at one indicate that
the stability of classiﬁers is not typically impacted by Caret-
optimized settings.
Moreover, the Caret-optimized classiﬁers of 9 of
the 26 studied classiﬁcation techniques (35%) are
more stable than classiﬁers that are trained using
the default values. Indeed, Figure 4 shows that there●● ●●● ●●● ●
# Boosting Iterations of GBMMax T
ree Depth of GBM#Randomly Selected Predictors of RF#Hidden Units of PCANNet#Trees of AdaBoost#Neighbors of KNNCost of SVMRadial#Hidden Units of AVNNet#Hidden Units of RBFMax Tree Depth of AdaBoost# Boosting Iterations of LogitBoost#Hidden Units of NNet#Hidden Units of MLPWeightDecay#Hidden Units of MLPWeight Decay of MLPWeightDecayWeight Decay of PCANNet# Boosting Iterations of C5.0Weight Decay of NNetWeight Decay of AVNNet#Terms of FDA
0.00 0.25 0.50 0.75 1.00
Stability Ratio
Figure 5: The stability ratio of the top-20 most sen-
s
itive parameters.
is a median stability ratio of 0.11 (NNet) to 0.61 (MLP)
among the 9 classiﬁcation techniques where the stability has
improved. Thisequatestoa39%-89%stabilityimprovement
for these Caret-optimized classiﬁers. Indeed, Figure 5 shows
that the stability of the performance of the advanced neural
network techniques is largely caused by changing the weight
decay, but not the #hidden units orbagging parameters,
which consistent with our ﬁndings in RQ1.
Caret-optimized classiﬁers are at least as stable as classi-
ﬁers that are trained using the default settings. Moreover,
the Caret-optimized classiﬁers of 9 of the 26 studied clas-
siﬁcation techniques (35%) are more stable than classiﬁers
that are trained using the default values.
6. REVISITING THE RANKING OF CLAS-
SIFICA
TION TECHNIQUES FOR DEFECT
PREDICTION MODELS
Prior studies have ranked classiﬁcation techniques accord-
ing to their performance on defect prediction datasets. For
example, Lessmann et al.[32] demonstrate that 17 of 22
studiedclassiﬁcationtechniquesarestatisticallyindistinguish-
able. On the other hand, Ghotra et al.[12] argue that clas-
siﬁcation techniques can have a large impact on the perfor-
mance of defect prediction models.
However, these studies have not taken parameter opti-
mization into account. Since we ﬁnd that parameter settings
can improve the performance of the classiﬁers that are pro-
duced (see RQ1), we set out to revisit the ﬁndings of prior
studies when Caret-optimized settings have been applied.
6.1 Approach
As Keung et al.[25] point out, dataset selection can be a
source of bias in an analysis of top-performing classiﬁcation
techniques. To combat the bias that may be introduced by
327Applying the 
Sc
ott-Knott 
ESD testRank Technique
1 T2
2 T1Dataset 1
100xTechnique 1
100xTechnique N
…
AUC Performance 
D
istributionAUC Performance 
D
istribution
Applying the 
Sc
ott-Knott 
ESD testRank Technique
1 T1, T2
2 T3Dataset M
100xTechnique 1
100xTechnique N
…
AUC Performance 
D
istributionAUC Performance 
D
istribution…
LineplotsDataset T1T2T3
1213
2123
3112
(Step 2-2) 
C
ompute 
likelihood
Repeat 100 timesTe
chnique  
ranking  
for studied  
datasets(Step 2) Bootstrap Analysis
(Step 2-1)
Resampling 
w
ith 
replacementT1 T2 T3
0.670.330Dataset T1T2T3
1213
2123
2123Likelihood(Ste
p 1) Ranking Generation
Figure 6: An overview of our statistical comparison over mult iple datasets.
●●
●●●
●●
●● ●
●●
● ●●● ●● ●● ● ● ● ● ● ● 0.00.20.40.60.81.0
C5.0
xGBTree AVNNetGBMRF
GPLSPDANNetPMR
GAMBoostPCANNetMARSFDA
AdaBoostSVMRadial
MLPWeightDecayMLPRBFNB
RipperLMTCART
SVMLinearJ48KNN
LogitBoostLikelihood
● Optimized Classifier Default Classifier
Figure 7: The likelihood of each technique appearing in the to p Scott-Knott ESD rank. Circle dots and
triangle dots indicate the median likelihood, while the error bars indicate the 95% conﬁdence interval of the
likelihood of the bootstrap analysis. A likelihood of 80% indicates that a classiﬁcation technique appears at
the top-rank for 80% of the studied datasets.
dataset selection, we use a bootstrap-based Ranking Likeli-
hood Estimation (RLE) experiment. Figure 6 provides an
overview of our RLE experiment. The experiment uses a
statistical comparison approach over multiple datasets that
leverages both eﬀect size diﬀerences and aspects of statisti-
cal inference [9]. The experiment is divided into two steps
that we describe below.
(Step 1) Ranking Generation. We ﬁrst start with the
AUC performance distribution of the 26 studied classiﬁca-
tion techniques with the Caret-optimized parameter settings
and the default settings. To ﬁnd statistically distinct ranks
of classiﬁcation techniques within each dataset, we provide
the AUC performance distribution of the 100 bootstrap iter-
ations of each classiﬁcation technique with both parameter
settings to a Scott-Knott Eﬀect Size Diﬀerence (ESD) test
(α= 0.05) [55]. The Scott-Knott ESD test is a variant of the
Scott-Knott test that is eﬀect size aware. The Scott-Knott
ESD test uses hierarchical cluster analysis to partition the
set of treatment means into statistically distinct groups.
Unlike the traditional Scott-Knott test [19], the Scott-
Knott ESD test will merge any two statistically distinctgroups that have a negligible Cohen’s deﬀect size [4] into
one group. The Scott-Knott ESD test also overcomes the
confounding issue of overlapping groups that are produced
by several other post-hoc tests [12, 41], such as Nemenyi’s
test [43], which were used in prior studies [32]. We imple-
menttheScott-KnottESDtestbasedontheimplementation
of the Scott-Knott test provided by the ScottKnott R pack-
age [19] and the implementation of Cohen’s dprovided by
theeffsize R package [57].
We use the Scott-Knott ESD test in order to control for
dataset-speciﬁcmodelperformance, sincesomedatasetsmay
have a tendency to produce over- or under-performing clas-
siﬁers. Finally, for each classiﬁcation technique, we have 18
diﬀerent Scott-Knott ranks (i.e., one from each dataset).
(Step 2) Bootstrap Analysis. We then perform a boot-
strap analysis to approximate the empirical distribution of
the likelihood that a technique will appear in the top Scott-
Knott ESD rank [8]. The key intuition is that the rela-
tionship between the likelihood that is derived from studied
datasets and the true likelihood that would be derived from
the population of defect datasets is asymptotically equiva-
328lenttotherelationshipbetweenthelikelihoodthatisderived
fro
m bootstrap samples and the likelihood that is derived
from studied datasets. We ﬁrst input the ranking of the
studied classiﬁcation techniques on 18 studied datasets to
the bootstrap analysis, which is comprised of two steps:
(Step 2-1) A bootstrap sample of 18 datasets is randomly
drawnwithreplacementfromtherankingtable,
which is also of comprised of size 18 studied
datasets.
(Step 2-2) For each classiﬁcation technique, we compute
the likelihood that a technique appears in the
topScott-KnottESDrankinthebootstrapsam-
ple.
The bootstrap analysis is repeated 100 times. We then
present the results with its 95% conﬁdence interval, which
is derived from the bootstrap analysis.
6.2 Results
C5.0 boosting tends to yield top-performing defect
prediction models more frequently than the other
studied classiﬁcation techniques. Figure 7 shows the
likelihood of each technique appearing in the top Scott-
Knott ESD rank. We ﬁnd that there is a 83% likelihood
of C5.0 appearing in the top Scott-Knott rank. Further-
more, the bootstrap-derived 95% conﬁdence interval ranges
from 67% to 94%. On the other hand, when default settings
are applied, C5.0 boosting has a 0% likelihood of appearing
in the top rank. This echoes the ﬁndings of RQ1, where
C5.0 boosting was found to be the classiﬁcation technique
that is most sensitive to parameter optimization.
Unlike prior work in the data mining domain, we ﬁnd
that random forest is not the most frequent top performer
in our defect prediction datasets. Indeed, we ﬁnd that there
is a 55% likelihood of random forest appearing in the top
Scott-Knott rank with a bootstrap-derived 95% conﬁdence
interval that ranges from 33% to 72%. A one-tailed boot-
strap t-test reveals that the likelihood of C5.0 producing a
topperformingclassiﬁerissigniﬁcantlylargerthanthelikeli-
hood of random forest producing a top-performing classiﬁer
(α= 0.05). This contradicts the conclusions of Fernandez-
Delgado et al.[10], who found that random forest tends to
yield top-performing classiﬁers the most frequently. The
contradictory conclusions indicate that the domain-speciﬁcs
play an important role.
Automated parameter optimization increases the
likelihood of appearing in the top Scott-Knott ESD
rank by as much as 83%. Figure 7 shows that automated
parameter optimization increases the likelihood of 11 of the
studied 26 classiﬁcation techniques by as much as 83% (i.e.,
C5.0 boosting). This suggests that automated parameter
optimization can substantially shift the ranking of classiﬁ-
cation techniques.
C5.0 boosting tends to yield top-performing defect predic-
tion models more frequently than the other studied classiﬁ-
cation techniques. This disagrees with prior studies in the
data mining domain, suggesting that domain-speciﬁcs play
a key role. Furthermore, automated parameter optimiza-
tion increases the likelihood of appearing in the top Scott-
Knott ESD rank by as much as 83%.7. DISCUSSION
7.1
Cross-Context Defect Prediction
The performance improvement of defect prediction mod-
els is estimated using a bootstrap resampling approach (see
RQ1). Whilethisbootstrapresamplingapproachiscommon
in other research areas [3, 8, 9], recent studies in software en-
gineering tend to estimate the performance of defect models
using data from diﬀerent contexts [63]. Hence, we perform
an additional analysis in order to investigate whether Caret
still improves AUC performance in a cross-context setting.
Weanalyzetheperformanceofdefectpredictionmodelsthat
aretrainedinonecontext, buttestedinanothercontext. We
then compute the performance improvement between the
models that are trained with Caret-optimal and default set-
tings.
Caret still improves the performance of cross-
context defect prediction models by up to 30 per-
centage points. Based on an analysis of 5 releases of pro-
prietary systems, 2 releases of Apache Xalan, and 3 releases
of Eclipse Platform, we ﬁnd that the performance of cross-
context classiﬁers that are trained using Caret outperform
classiﬁers that are trained using default settings. For exam-
ple, we ﬁnd that when neural network classiﬁers are trained
using Eclipse Platform 2.1 and tested using Eclipse Plat-
form 3.0, the AUC performance improves by 30 percentage
points when compared to classiﬁers that are trained using
default settings. This suggests that automated parameter
optimization also yields a large beneﬁt in terms of cross-
context defect prediction.
7.2 Computational Cost
Ourcasestudyapproachiscomputationally-intensive(i.e.,
450 parameter settings ×100 out-of-sample bootstrap rep-
etitions×18 systems = 810,000 results). However, the re-
sults can be computed in parallel. Hence, we design our
experiment using a High Performance Computing (HPC)
environment. Our experiments are performed on 43 high
performance computing machines with 2x Intel Xeon 5675
@3.1 GHz (24 hyper-threads) and 64 GB memory (i.e., in to-
tal, 24 hyper-threads ×43 machines = 1,032 hyper-threads).
Each machine connects to a 2 petabyte shared storage array
via a dual 10-gigabit ﬁbre-channel connection.
For each of the classiﬁcation techniques, we compute the
average amount of execution time that was consumed by
Caret when producing suggested parameter settings for each
of the studied datasets.
Caret adds less than 30 minutes of additional com-
putation time to 65% of the studied classiﬁcation
techniques. We ﬁnd that the optimization cost of 17 of
the 26 studied classiﬁcation techniques (65%) is less than 30
minutes. We ﬁnd that the C5.0 and extreme gradient boost-
ingclassiﬁcationtechniques, whichyieldtop-performingclas-
siﬁers more frequently than other classiﬁcation techniques,
fall into this category. This indicates that applying Caret
tends to improve the performance of defect models while
incurring a manageable additional computational cost.
On the other hand, 12% of the studied classiﬁcation tech-
niques require more than 3 additional hours of computation
time to apply Caret. Only AdaBoost, MLPWeightDecay,
and RBF incur this large overhead. Nonetheless, the com-
putation could still be completed if it was run overnight.
Since defect prediction models do not need to be built very
often in practice, this cost should still be manageable.
3298. THREATS TO V ALIDITY
W
e now discuss the threats to the validity of our study.
8.1 Construct Validity
Thedatasetsthatweanalyzearepartofseveralcollections
(e.g., NASA and PROMISE), which each provide diﬀerent
sets of metrics. Since the metrics vary, this is a point of
variation between the studied systems that could impact our
results. However, our within-family datasets analysis shows
that the number and type of predictors do not inﬂuence our
ﬁndings. Thus, we conclude that the variation of metrics
does not pose a threat to our study. On the other hand, the
variety of metrics also strengthens the generalization of our
results, i.e., our ﬁndings are not bound to one speciﬁc set
of metrics.
The Caret budget, which controls the number of settings
that we evaluate for each parameter, limits our exploration
of the parameter space. Although our budget setting is se-
lected based on the literature [30], selecting a diﬀerent bud-
get may yield diﬀerent results. However, the results of our
studyshowthatamodestexplorationoftheparameterspace
can already lead to a large change in the performance of de-
fect prediction models.
Our results from RQ1 show that Caret improves the per-
formance of defect prediction models. However, the perfor-
mance improvement may increase the complexity of defect
prediction models. Thus, we plan to investigate the rela-
tionship between model complexity and performance in fu-
ture work.
8.2 Internal Validity
We measure the performance of our classiﬁers using AUC.
Other performance measures may yield diﬀerent results. We
plan to expand the set of measures that we adopt in our
future work.
The generalizability of the bootstrap-based Ranking Like-
lihoodEstimation(RLE)isdependentonhowrepresentative
our sample is. To combat potential bias in our samples, we
analyzedatasetsofdiﬀerentsizesanddomains. Nonetheless,
a larger sample may yield more robust results.
Prior work has shown that dirty data may inﬂuence con-
clusion that are drawn from defect prediction studies [12,
53, 54]. Hence, noisy data may be inﬂuencing our conclu-
sions. However, we conduct a highly-controlled experiment
where known-to-be noisy NASA data [50] has been cleaned.
Nonetheless, dataset cleanliness should be inspected in fu-
ture work.
8.3 External Validity
Westudyalimitednumberofsystemsinthispaper. Thus,
our results may not generalize to all software systems. How-
ever, the goal of this paper is not to show a result that
generalizes to all datasets, but rather to show that there are
datasets where parameter optimization matters. Nonethe-
less, additional replication studies may prove fruitful.
9. CONCLUSIONS
Defect prediction models are classiﬁers that are trained to
identify defect-prone software modules. The characteristics
of the classiﬁers that are produced are controlled by conﬁg-
urable parameters. Recent studies point out that classiﬁers
may under-perform because they were trained using subop-
timal default parameter settings. However, it is impracticalto explore all of the possible settings in the parameter space
of a classiﬁcation technique.
In this paper, we investigate the performance of defect
prediction models where Caret [30] — an automated param-
eter optimization technique — has been applied. Through a
case study of 18 datasets from systems that span both pro-
prietary and open source domains, we make the following
observations:
– Caret improves the AUC performance of defect pre-
diction models by up to 40 percentage points. More-
over, the performance improvement provided by Caret
is non-negligible for 16 of the 26 studied classiﬁcation
techniques (62%).
– Caret-optimizedclassiﬁersareatleastasstableasclas-
siﬁersthataretrainedusingthedefaultsettings. More-
over, the Caret-optimizedclassiﬁersof 9ofthe 26stud-
iedclassiﬁcationtechniques(35%)aremorestablethan
classiﬁers that are trained using the default values.
– Caret increases the likelihood of producing a top-
performing classiﬁer by as much as 83%, suggesting
that automated parameter optimization can substan-
tially shift the ranking of classiﬁcation techniques.
Our results lead us to conclude that parameter settings
can indeed have a large impact on the performance of de-
fect prediction models, suggesting that researchers should
experiment with the parameters of the classiﬁcation tech-
niques. Since automated parameter optimization techniques
like Caret yield substantial beneﬁts in terms of performance
improvementandstability, whileincurringamanageablead-
ditional computational cost, they should be included in fu-
ture defect prediction studies.
Finally, we would like to emphasize that we do not seek to
claim the generalization of our results. Instead, the key mes-
sage of our study is that there are datasets where there are
statistically signiﬁcant diﬀerences between the performance
ofclassiﬁcationtechniquesthataretrainedusingdefaultand
Caret-optimized parameter settings. Hence, we recommend
that software engineering researchers experiment with auto-
mated parameter optimization (e.g., Caret) instead of rely-
ing on the default parameter setting of the research toolkits,
assuming that other parameter settings are not likely to lead
to signiﬁcant improvements. Given the availability of auto-
mated parameter optimization in commonly-used research
toolkits (e.g., Caret for R [30], MultiSearch for Weka [14],
GridSearch for Scikit-learn [44]), we believe that our recom-
mendation is a rather simple and low-cost recommendation
to adopt.
Acknowledgments. This study would not have been pos-
sible without the data shared in the Tera-PROMISE repos-
itory [39], Shepperd et al.[50], Zimmermann et al.[64],
Kimet al.[26, 61], and D’Ambros et al.[6, 7], as well as
High Performance Computing (HPC) systems provided by
the Compute Canada2and HPCVL.3This work was sup-
ported by the JSPS Program for Advancing Strategic Inter-
national Networks to Accelerate the Circulation of Talented
Researchers: Interdisciplinary Global Networks for Accel-
erating Theory and Practice in Software Ecosystem, and
the Natural Sciences and Engineering Research Council of
Canada (NSERC).
2https://www.computecanada.ca/
3http://www.hpcvl.org/
33010. REFERENCES
[
1] J. Bergstra and Y. Bengio. Random search for hyper-
parameter optimization. The Journal of Machine
Learning Research, 13(1):281–305, 2012.
[2] S. Bibi, G. Tsoumakas, I. Stamelos, and I. Vlahvas.
Software Defect Prediction Using Regression via Classi-
ﬁcation. In Proceedings of the International Conference
on Computer Systems and Applications, pages 330–336,
2006.
[3] U. M. Braga-Neto and E. R. Dougherty. Is cross-
validation valid for small-sample microarray classiﬁca-
tion?Bioinformatics, 20(3):374–380, 2004.
[4] J. Cohen. Statistical Power Analysis for the Behavioral
Sciences. 1988.
[5] J. Cohen. A power primer. Psychological bulletin,
112(1):155, 1992.
[6] M. D’Ambros, M. Lanza, and R. Robbes. An Exten-
sive Comparison of Bug Prediction Approaches. In Pro-
ceedings of the Working Conference on Mining Software
Repositories, pages 31–41, 2010.
[7] M. D’Ambros, M. Lanza, and R. Robbes. Evaluating
Defect Prediction Approaches: A Benchmark and an
Extensive Comparison. Empirical Software Engineer-
ing, 17(4-5):531–577, 2012.
[8] B. Efron. Estimating the Error Rate of a Prediction
Rule: Improvement on Cross-Validation. Journal of
the American Statistical Association, 78(382):316–331,
1983.
[9] B. Efron and R. J. Tibshirani. An Introduction to the
Bootstrap . CRC Press, 1993.
[10] M. Fern´ andez-Delgado, E. Cernadas, S. Barro, and
D. Amorim. Do we Need Hundreds of Classiﬁers to
Solve Real World Classiﬁcation Problems? Journal of
Machine Learning Research, 15(1):3133–3181, 2014.
[11] S. Fritsch and F. Guenther. neuralnet: Training of neu-
ral networks. http://CRAN.R-project.org/package=
neuralnet, 2015.
[12] B. Ghotra, S. McIntosh, and A. E. Hassan. Revisiting
the Impact of Classiﬁcation Techniques on the Perfor-
mance of Defect Prediction Models. In Proceedings of
the International Conference on Software Engineering,
pages 789–800, 2015.
[13] A. G ¨unes Koru and H. Liu. An investigation of the
eﬀect of module size on defect prediction using static
measures. In Proceedings of the International Workshop
on Predictor Models in Software Engineering, pages 1–
5, 2005.
[14] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. The WEKA data mining
software: an update. SIGKDD explorations newsletter,
11(1):10–18, 2009.
[15] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Coun-
sell. A Systematic Literature Review on Fault Pre-
diction Performance in Software Engineering. Trans-
actions on Software Engineering, 38(6):1276–1304, nov
2012.
[16] M. Harman, P. McMinn, J. De Souza, and S. Yoo.
Search based software engineering: Techniques, taxon-
omy, tutorial. pages 1–59, 2012.
[17] F. E. Harrell Jr. Regression Modeling Strategies.
Springer, 1st edition, 2002.[18] H. He and E. A. Garcia. Learning from Imbalanced
Data.Transactions on Knowledge and Data Engineer-
ing, 21(9):1263–1284, 2009.
[19] E. G. Jelihovschi, J. C. Faria, and I. B. Allaman. The
ScottKnott Clustering Algorithm. Universidade Estad-
ual de Santa Cruz - UESC, Ilheus, Bahia, Brasil, 2014.
[20] Y. Jia, M. Cohen, and M. Petke. Learning Combi-
natorial Interaction Test Generation Strategies using
Hyperheuristic Search. In Proceedings of the Interna-
tional Conference on Software Engineering , pages 540–
550, 2015.
[21] Y. Jiang, B. Cukic, and Y. Ma. Techniques for evalu-
ating fault prediction models. Empirical Software En-
gineering, 13(5):561–595, 2008.
[22] Y. Jiang, B. Cukic, and T. Menzies. Can Data Trans-
formation Help in the Detection of Fault-prone Mod-
ules? In Proceedings of the workshop on Defects in
Large Software Systems, pages 16–20, 2008.
[23] M. Jorgensen and M. Shepperd. A Systematic Re-
view of Software Development Cost Estimation Stud-
ies.Transactions on Software Engineering, 33(1):33–53,
2007.
[24] M. Jureczko and L. Madeyski. Towards identifying soft-
ware project clusters with regard to defect prediction.
InProceedings of the International Conference on Pre-
dictive Models in Software Engineering, pages 9:1–9:10,
2010.
[25] J. Keung, E. Kocaguneli, and T. Menzies. Finding con-
clusion stability for selecting the best eﬀort predictor
in software eﬀort estimation. Automated Software En-
gineering, 20(4):543–567, 2013.
[26] S. Kim, H. Zhang, R. Wu, and L. Gong. Dealing with
Noise in Defect Prediction. In Proceedings of the In-
ternational Conference on Software Engineering, pages
481–490, 2011.
[27] E. Kocaguneli, T. Menzies, A. B. Bener, and J. W. Ke-
ung. Exploiting the essential assumptions of analogy-
based eﬀort estimation. Transactions on Software En-
gineering, 38(2):425–438, 2012.
[28] M. Kuhn. Building Predictive Models in R Using caret
Package. Journal of Statistical Software, 28(5), 2008.
[29] M. Kuhn. C50: C5.0 decision trees and rule-based mod-
els. http://CRAN.R-project.org/package=C50, 2015.
[30] M. Kuhn. caret: Classiﬁcation and regression training.
http://CRAN.R-project.org/package=caret, 2015.
[31] I. H. Laradji, M. Alshayeb, and L. Ghouti. Software
defect prediction using ensemble learning on selected
features. Information and Software Technology, 58:388–
402, 2015.
[32] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch.
Benchmarking Classiﬁcation Models for Software De-
fect Prediction : A Proposed Framework and Novel
Findings. Transactions on Software Engineering,
34(4):485–496, 2008.
[33] A. Liaw and M. Wiener. randomforest: Breiman
and cutler’s random forests for classiﬁcation and
regression. http://CRAN.R-project.org/package=
randomForest, 2015.
[34] A. Lim, L. Breiman, and A. Cutler. bigrf: Big random
forests: Classiﬁcation and regression forests for large
data sets. http://CRAN.R-project.org/package=bigrf,
3312015.
[
35] MATLAB. version 8.5.0 (R2015a). The MathWorks
Inc., Natick, Massachusetts, 2015.
[36] T. Mende. Replication of Defect Prediction Studies:
Problems, Pitfalls and Recommendations. In Proceed-
ings of the International Conference on Predictive Mod-
els in Software Engineering, pages 1–10, 2010.
[37] T. Mende and R. Koschke. Revisiting the evaluation of
defect prediction models. In Proceedings of the Inter-
national Conference on Predictive Models in Software
Engineering, page 7, 2009.
[38] T. Mende, R. Koschke, and M. Leszak. Evaluating De-
fect Prediction Models for a Large Evolving Software
System. In Proceedings of the European Conference on
Software Maintenance and Reengineering, pages 247–
250, 2009.
[39] T. Menzies, C. Pape, R. Krishna, and M. Rees-Jones.
The Promise Repository of Empirical Software Engi-
neering Data. http://openscience.us/repo, 2015.
[40] T. Menzies and M. Shepperd. Special issue on repeat-
able results in software engineering prediction. Empir-
ical Software Engineering, 17(1-2):1–17, 2012.
[41] N. Mittas and L. Angelis. Ranking and Clustering Soft-
ware Cost Estimation Models through a Multiple Com-
parisons Algorithm. Transactions on Software Engi-
neering, 39(4):537–551, 2013.
[42] I. Myrtveit, E. Stensrud, and M. Shepperd. Reliability
and Validity in Comparative Studies of Software Pre-
diction Models. Transactions on Software Engineering,
31(5):380–391, 2005.
[43] P. Nemenyi. Distribution-free multiple comparisons.
PhD thesis, Princeton University, 1963.
[44] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, et al. Scikit-learn: Machine
learning in python. The Journal of Machine Learning
Research, 12:2825–2830, 2011.
[45] P. Peduzzi, J. Concato, E. Kemper, T. R. Holford, and
A. R. Feinstein. A Simulation Study of the Number
of Events per Variable in Logistic Regression Anal-
ysis.Journal of Clinical Epidemiology, 49(12):1373–
1379, 1996.
[46] R Core Team. R: A language and environment for sta-
tistical computing. http://www.R-project.org/, 2013.
[47] F. Rahman and P. Devanbu. How, and why, process
metrics are better. In Proceedings of the International
Conference on Software Engineering, pages 432–441,
2013.
[48] B. Ripley. nnet: Feed-forward neural networks
and multinomial log-linear models. http://CRAN.
R-project.org/package=nnet, 2015.
[49] M. Shepperd, D. Bowes, and T. Hall. Researcher Bias :
the Use of Machine Learning in Software Defect Predic-
tion.Transactions on Software Engineering, 40(6):603–
616, 2014.
[50] M. Shepperd, Q. Song, Z. Sun, and C. Mair. Data
quality: Some comments on the NASA software de-
fect datasets. Transactions on Software Engineering ,
39(9):1208–1215, 2013.
[51] L. Song, L. L. Minku, and X. Yao. The Impact of Pa-
rameter Tuning on Software Eﬀort Estimation UsingLearning Machines. In Proceedings of the International
Conference on Predictive Models in Software Engineer-
ing, pages 1–10, 2013.
[52] Q. Song, Z. Jia, M. Shepperd, S. Ying, and
J. Liu. A General Software Defect-Proneness Predic-
tion Framework. Transactions on Software Engineer-
ing, 37(3):356–370, 2011.
[53] C. Tantithamthavorn. Towards a Better Understand-
ing of the Impact of Experimental Components on De-
fect Prediction Modelling. In Proceedings of the Inter-
national Conference on Software Engineering, page To
appear, 2016.
[54] C. Tantithamthavorn, S. McIntosh, A. E. Hassan,
A. Ihara, and K. Matsumoto. The Impact of Misla-
belling on the Performance and Interpretation of De-
fect Prediction Models. In Proceedings of the Interna-
tional Conference on Software Engineering , pages 812–
823, 2015.
[55] C. Tantithamthavorn, S. McIntosh, A. E. Has-
san, and K. Matsumoto. An Empirical Com-
parison of Model Validation Techniques for Defect
Prediction Model. Under Review at a Software
Engineering Journal, http://sailhome.cs.queensu.ca/
replication/kla/model-validation.pdf, 2015.
[56] C. Tantithamthavorn, S. McIntosh, A. E. Hassan, and
K. Matsumoto. Comments on “Researcher Bias: the
Use of Machine Learning in Software Defect Predic-
tion”.PeerJ PrePrints, page 3:e1543, 2015.
[57] M. Torchiano. eﬀsize: Eﬃcient eﬀect size computation.
http://CRAN.R-project.org/package=eﬀsize, 2015.
[58] A. Tosun and A. Bener. Reducing false alarms in soft-
ware defect prediction by decision threshold optimiza-
tion. In Proceedings of the International Symposium
on Empirical Software Engineering and Measurement,
pages 477–480, 2009.
[59] B. Turhan. On the dataset shift problem in software
engineering prediction models. Empirical Software En-
gineering, 17(1-2):62–74, 2011.
[60] E. J. Weyuker, T. J. Ostrand, and R. M. Bell. Compar-
ing negative binomial and recursive partitioning models
for fault prediction. In Proceedings of the International
Workshop on Predictor Models in Software Engineer-
ing, pages 3–9, 2008.
[61] R. Wu, H. Zhang, S. Kim, and S. C. Cheung. ReLink:
Recovering Links between Bugs and Changes. In Pro-
ceedings of the joint meeting of the European Software
Engineering Conference and the Foundations of Soft-
ware Engineering, pages 15–25, 2011.
[62] T. Zimmermann and N. Nagappan. Predicting Sub-
system Failures using Dependency Graph Complexities.
InProceedings of the International Symposium on Soft-
ware Reliability Engineering, pages 227–236, 2007.
[63] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy. Cross-project defect prediction. In Proceed-
ings of the European Software Engineering Conference
and the symposium on the Foundations of Software En-
gineering, pages 91–100, 2009.
[64] T. Zimmermann, R. Premraj, and A. Zeller. Predicting
Defects for Eclipse. In Proceedings of the International
Workshop on Predictor Models in Software Engineer-
ing, pages 9–20, 2007.
332