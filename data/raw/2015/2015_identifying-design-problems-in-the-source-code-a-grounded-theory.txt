Identifying Design Problems in the Source Code
A Grounded Theory
Leonardo Sousa, Anderson
Oliveira
PUC-Rio, Rio de Janeiro - RJ
{lsousa,aoliveira}@inf.puc-rio.brWillian Oizumi, Simone
Barbosa, Alessandro Garcia
PUC-Rio, Rio de Janeiro - RJ
{woizumi,simone,afgarcia}@inf.puc-rio.brJaejoon Lee
Lancaster University
Lancaster, Lancashire
j.lee3@lancaster.ac.uk
Marcos Kalinowski, Rafael de
Mello
PUC-Rio, Rio de Janeiro - RJ
{kalinowski,rmaiani}@inf.puc-rio.brBaldoino Fonseca
UFAL, Maceio - AL
baldoino@ic.ufal.brRoberto Oliveira, Carlos
Lucena, Rodrigo Paes
PUC-Rio, Rio de Janeiro - RJ
{roliveira,lucena}@inf.puc-rio.br
rodrigo@ic.ufal.br
ABSTRACT
The prevalence of design problems may cause re-engineering or
even discontinuation of the system. Due to missing, informal or
outdateddesigndocumentation,developersoftenhavetorelyon
the source code to identify design problems. Therefore, developers
havetoanalyzedifferentsymptomsthatmanifestinseveralcode
elements,whichmayquicklyturnintoacomplextask.Although
researchers have been investigating techniques to help developers
in identifying design problems, there is little knowledge on how
developersactuallyproceedtoidentifydesignproblems.Inorder
to tackle this problem, we conducted a multi-trial industrial ex-
periment with professionals from 5 software companies to build a
grounded theory. The resulting theory offers explanations on how
developers identify design problems in practice. For instance, it
reveals the characteristics of symptoms that developers considerhelpful. Moreover, developers often combine different types of
symptoms to identify a single design problem. This knowledge
servesasabasistofurtherunderstandthephenomenaandadvance
towards more effective identification techniques.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware design engineer-
ing;
KEYWORDS
design problem, grounded theory, software design, symptoms
ACM Reference Format:
LeonardoSousa,AndersonOliveira,WillianOizumi,SimoneBarbosa,Alessan-
dro Garcia, Jaejoon Lee, Marcos Kalinowski, Rafael de Mello, Baldoino
Fonseca,andRobertoOliveira,CarlosLucena,RodrigoPaes.2018.Identi-
fying Design Problems in the Source Code: A Grounded Theory. In ICSE
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
¬© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180239‚Äô18: ICSE ‚Äô18: 40th International Conference on Software Engineering , May
27-June3,2018,Gothenburg,Sweden. ACM,NewYork,NY,USA,14pages.
https://doi.org/10.1145/3180155.3180239
1 INTRODUCTION
The development and maintenance of long-lived software systems
requirespecialattentiontonon-functionalrequirements,suchas
maintainability, extensibility, availability and performance. Each
non-functional requirement may be affected, either positively or
negatively,bydesigndecisions[ 35],suchas(mis-)prioritizingan
objected-orientedprincipleoveranotheror(mis)usingcertainde-
signpatterns[ 11].Adesignproblem [2,6,48]istheresultofoneor
more inappropriate design decisions, i.e., decisions that negatively
impactnon-functionalrequirements.Anexampleofdesignprob-
lem isFat Interface [25], which occurs when a developer decides to
aggregate multiple non-cohesive functionalities in a single system
interface;asaresult,thisinterfacebecomeshighlycoupledtoother
modules. The occurrence of Fat Interface negatively impacts the
maintainability and extensibility of the software system [25].
Design problems are often harmful in several software systems.
For instance, an industrial study [ 7] with 745 software systems,
from 160 different organizations, showed that technical debts ‚Äì
primarilyassociatedwithdesignproblems‚Äìweredirectlyrelated
with a significant increase in software project costs [ 7]. Another
study[39]showedthatdesignproblemsareoneofthemostcom-
mon categories of technical debt that leads to the rejection of
codecontributions.Thus,theprevalenceofdesignproblemsmay
cause the redesign or even the discontinuation of software sys-tems [
14,19,37,49]. Given the harmfulness of design problems,
developers should identify them as early as possible [12, 37, 54].
However, identifying a single design problem can itself quickly
turn into a very complex task [ 6,47]. One of the main reasons
is that design documentations are often unavailable or outdated[
18,48]. Then, developers are forced to analyze several elements
in source code to identify each design problem. Each single design
problem often manifests as multiple symptoms scattered in several
programelements.Forinstance,identifying FatInterface requires
thesearchforsymptomsaffectingnotonlysuspiciousinterfaces,
butalsoclassesthateitherimplementordependonthoseinterfaces.
Unfortunately, there is limited understanding about how develo-
pers identify design problems in practice, in particular when the
9212018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden L. Sousa et al.
source code is the only artifact available in a project. Existing stud-
ies tend to focus on proposing solutions for assisting developers
in identifying design problems [ 6,20‚Äì22,28,32,47,50,52,53].
However, such proposed solutions may be misaligned with how
developers identify design problems in practice. For instance, most
of these studies make oversimplified assumptions about the pro-
cess of identifying design problems. They consider that developers
would rely on a single type of symptom (e.g., either code smells
[20,28,32]ordesignprincipleviolations[ 52,53])toinfertheoc-
currence of a design problem. However, this assumption might
notholdinrealprojectsettings.Infact,weknowlittleabouthow
developers identify, in practice, design problems in source code.
Toprovidesuchnecessaryunderstanding,inthispaperwead-
dress the following research question: how do developers identify
design problems in source code? To do so, we conducted a multi-
trialindustrialexperimentwithprofessionalsoftwaredevelopers
from five different companies, where they had to identify design
problemsintheirsystemsunderdevelopment.Intheexperiment,
wecaptureddataontheirbehaviourbyfilmingtheenvironment,
recording audio and capturing their computer screens on video.
These data allowedus to conduct an in-depthqualitative analysis
basedonGroundedTheoryprocedures[ 44].Asaresult,wehave
built a theory of design problem identification.
According to Stol and Fitzgerald [ 42], nascent research areas
typically take the research-then-theory approach, whereas more
mature areas rely on (and refine) theories to further advance the
field. Aligned with this statement, the theory presented here offers
insightful propositions and explanations on how design problems
areidentified,whichcanserveasabasistoimprovethestate-of-art.
For example, while most studies address only one type of design
problem symptom, the theory reveals that, in practice, developers
relyonaheterogeneoussetofsymptoms.Thus,previousstudies
are misaligned not only for assuming that developers will use a
single, dominant type of symptom, but also for not consideringhow they use these symptoms. Based on the theory, researchers
canbuildsolutionsmostsuitabletohelpdevelopers.Forinstance,
we identified cases when developers consider a symptom useful to
identifydesignproblems.Thus,researcherscanusethisknowledge
to build tools that prioritize helpful symptoms for developers.
The remainder of this paper is organized as follows. Section 2
presentsbasicconceptsandanexampleofdesignproblemdiagnosis.
Section3presentsourresearchdesign.Section4summarizesthe
results in which our theory is grounded. Section 5 complements
the theory with additional propositions concerning the developer.
Section 6 presents how the theory can be used to drive research on
identifyingdesignproblems.Section7and8presentrelatedwork
and threats to validity, respectively. Section 9 concludes the paper.
2 CONCEPTS AND MOTIVATION
This section presents concepts about design problems (Section
2.1); an example of how developers may identifydesign problems
(Section 2.2); and concepts about Grounded Theory (Section 2.3).
2.1 Design Problems
Software design results from a series of decisions made during the
softwaredevelopment[ 45,46].Thosedecisionsdirectlyinfluencesoftware quality attributes, such as maintainability, robustness,
performance,andthelike.However,alongtheway,softwaredesign
maydecayduetotheintroductionofdesignproblems.A design
problem arises from one or more design decisions which, when
implemented in source code, negatively affect software quality
requirements[ 2,6,48].Althoughsomeofthemareself-admitted,
mostareintroducedbyunintendeddecisions,whichmakesthem
hardertoidentifyinsourcecode.Developersoftenhaveonlythesourcecodeasaresourcetoidentifydesignproblems,becauseof
missing, informal, or outdated design documentations [18, 48].
Adesignproblemmayaffectasingleormultipleelementsinthe
program. For instance, Delegating Abstraction [5] happens when
one class exists only for passing messages from one element toanother, while Fat Interface is a design problem that forces some
elements‚Äì i.e.,theinterfaceitselfandrelatedclasses‚Äìtodealwith
manyfunctionalities[ 25].Moreover,designproblemsalsoimpact
differentnon-functionalrequirements.Forinstance, MisplacedCon-
cerncan impact the understandability, since it happens when an
element implements functionality that is not the predominant one
in the element [ 12]. Conversely, Cyclic Dependency can impact the
systemperformanceandavailabilityduetodependencycyclesthat
can lead to deadlocks [34].
2.2 Design Problems Identification
Asdesignproblemshavenegativeconsequencesforsoftwaresys-
tems,theyareoftentargetsofsignificantmaintenanceeffort[ 12,
37,54], increasing the cost related to maintaining the software sys-
tems. Unfortunately, identifying design problems is challenging,
foranumberofreasons[ 6,47].Firstly,softwaresystemstendtobe
increasingly large in size and complexity, thereby expanding the
search space for problems. Secondly, each design problem usually
pervades the implementation of several elements [ 12,28]. Thus,
developers need to analyze several elements to identify a single
design problem [ 47]. Thirdly, design documentations are often un-
available or outdated, making the source code the only artifact
available for developers to identify design problems in most cases.
To illustrate the design problem identification, let us consider
the example in Figure 1, which uses a UML-like notation to show apartialviewofarealuniversitymanagementsystemweusedinour
experiments (Section 3.1). During design problem identification,
oneofthedevelopersstartedlookingfordesignproblem symptoms
intheclassesthatextendedthe AbstractService class.Asymptom
isapartialsignorindicationofthepresenceofadesignproblem.
The first one he noticed was the incidence of Feature Envy [ 10]i n
methods of the InstitutionalEnrollmentService class. A method
affected by Feature Envy is more interested in other classes ratherthan in its own class [
10]. Based on the analysis of Feature Envies,
the developer noticed that the class was implementing two distinct
responsibilities: query enrollment and manage enrollment. He also
noticed that the class had a high dependency with UserService
and IncidentService classes. He then concluded that the class had
a design problem because it presented the following symptoms:
(i) implementation of two unrelated responsibilities, (ii) strong
coupling with other classes, and (iii) overly complex methods.
In this example, the most helpful symptoms to identify a de-
sign problem were located in the InstitutionalEnrollmentService .
922
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. 923
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden L. Sousa et al.
Table 1: Characterization of the Developers
Team IDExperience
(years)System Company
T1D1 3S1 1D2 5
T2D3 13S1 1D4 14
T3D5 14S1 1D6 6
T4D7 7S2 2D8 2
T5D9 4S2 2D10 4
T6D11 10S3 3D12 8
T7D13 12S4 4D14 13
T8D15 4S5 4D16 8
T9D17 4S6 4D18 10
T10D19 7
S7 5 D20 7
D21 9
T11D22 12S8 5D23 9
Allteamsarecomposedoftwodevelopers,exceptforT10,whose
company asked us to involve three developers.
3.2 Experimental Tasks
The experiment comprises the following four activities:
Activity 1: Subjects characterization. We asked the develo-
perstofilloutaquestionnairetogathertheirinformation,including
educationallevel,professionalexperiencewithsoftwaredevelop-
ment, Java programming, and knowledge about design problems.
Activity 2: Training. We conducted a training session for all
thedevelopersaboutsoftwaredesignanddesignproblems,withex-
amplesofproblemspertainingtodifferentcategories.Thefollowing
design problems were included in the training session: Ambigu-
ousInterface, UnwantedDependency, ComponentOverload, Cyclic
Dependency, ScatteredConcern, FatInterface,and UnusedAbstrac-
tion.Weselectedthesedesignproblemstogetherwiththeproject
managers, who suspected that these represented common casesof design problems in the selected projects. However, we madeit clear to the developers that they were also allowed to identify
other types of design problems with which they were already fa-
miliar. The 40-minute training session was organized in two parts:
a Powerpoint-based presentation; and discussions and questions.
Activity 3: Problem identification. We asked developers to
identify design problems in their software systems. They had 90
minutes to analyze the source code to identify all the design prob-
lems they could find. At the beginning of this activity, we asked
them to explain aloud what they were doing while we recordedthetaskonvideo.Thus,wecouldtriangulatetheresultsfromthe
questionnaireandthevideorecordingtoimprovethedataanalysis.
In total, the problem identification sessions lasted over 13 hours.
Activity 4: Follow-up. Developers filled out a questionnaire
abouttheirperceptionofthetask.Wealsoaskedthemtoindicate
whether each symptom was useful to identify a design problem.
3.3 Provided Data
Most design problems manifest themselves in source code through
differentsymptoms,andmanydevelopersusetoolstoidentifythese
symptoms. Thus, to make Activity 3 more realistic, we provided
oursubjectswithasetofsymptomswehaddetectedinthecodeof
theanalyzedsystemsafterrunningandmanuallycombiningthe
outputofsometools[ 4,28,31];simulatingtheuseoftools,butnot
limiting the developers to the output of a specific one. For each
module, we provided the following types of symptoms:
1.ViolationofNon-functionalRequirements :Information
ofnon-functional requirements(e.g., readability, testability,
robustness, security), which were possibly being violated;
2.Code Smells :Weprovidedthelist ofcodesmellsbecause
previous studies suggest that code smells can be used asindicators of design problems [
20‚Äì22,32]. We used well-
known metrics-based strategies to identify 15 types of code
smells from Fowler‚Äôs Catalog [10];
3.Visual Representation of Modules :Avisualrepresenta-
tion of the module and relationships between modules;
4.DesignPatternViolation :Informationontheuseofarchi-
tectural and design patterns [ 11], to help identify misused
patterns;
5.QualityRequirements :Informationaboutqualityrequire-
ments (e.g. cohesion, coupling, complexity)
6.Violation of Object-Oriented Principles :Informationa-
boutobject-orientedprinciples[ 24]thatwerepossiblybeing
violated, which may indicate a problem. These principles
have been pointed out as guides to avoid design problems.
We summarized and presented these symptoms to developers
throughawebpagebasedonSonarQube[ 4].Weprovidedavisual-
izationsimilartoSonarQubebecauseitisawell-knownplatform
forinspectionofsoftwaresystems,andwhichwasfamiliartomost
subjects.Thus,wecouldreducethelearningcurveoraversionre-
latedtohowthesymptomsarepresented.Themaindifferenceof
ourmechanismisthatitpresentsallsymptomsinasinglepage.Itisnoteworthy that, while SonarQube provides several pieces of infor-mationunrelatedtodesignproblems,ourmechanismprovidesonly
symptoms that may help developers to identify design problems.
3.4 Data Collection and Analysis
We used different instruments to collect data. The developers had
toanswercharacterizationandfollow-upquestionnaires.Theyalso
hadtowriteanyobservationinaspecificfieldatthewebpagein
whichwepresentedthesymptoms.Theycouldwriteanythingin
the observation field, such as: the name of a design problem affect-
ingtheelements;whetherheagreedwiththesuggestedsymptoms;
or even comments about the code. They used either the Eclipse or
IntelliJ IDEs to analyze the source code, and they used the browser
toaccessthewebpagewiththesymptoms.Weusedthethink-aloud
924
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. Identifying Design Problems in the Source Code ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
method[8],askingthedeveloperstoverbalizetheirthoughtsdur-
ingtheexperiment.Alltheirprocedureswererecordedonaudio
and video. We used Techsmith‚Äôs Camtasia1to record audio and
screenshots of their computer. In addition, a video camera was
installed in the room to record the developers during the study.
Afterthedatacollection,weemployedGTprocedurestoanalyze
the data. We first transcribed all the video and audio recordings.
We then performed open coding to associate codes with quotations
of developers‚Äô utterances, as shown in the example below:
RawTranscript. ‚ÄúD6:Thereadabilityhereisawful,butthereisnowayto
escape from this (implementation). That is the standard (implementation).
(...) indeed, it (the class) is not easy to ready‚Äù
Code 1.developer mentions that the class readability is awful
Code 2.developer mentions that there is no way to escape from the
analyzed implementation
Code 3.developer mentions that the analyzed implementation is the
standard implementation
Code 4.developer accepts that the class is hard to read
Werelatedthecodesthrough axialcoding.Inthisprocedure,the
codes were merged and grouped into more abstract categories, and
thetypeofrelation[ 44]wasestablished.Forinstance,theprevious
codes were grouped into the following two categories:
Category 1. analysis of a non-functional requirement
Category 2. explanation for the existence of the symptom
Foreachtranscript,thecodes,memos,andnetworksshowing
therelationsinthecategoriesandcodeswerepeerreviewedand
changeduponagreementwithsomeofthepaper‚Äôsauthors.Then,
we usedselective coding to identify core categories that best ex-
plainhowdevelopers identifydesignproblems.Next, weusedthe
Sj√∏berg‚Äôs framework [ 40] to represent and describe the theory con-
structs, propositions, explanations and scope. A construct is a basic
particlethatcomposesatheory;thus,thecategoriesidentifiedinthe
axialandselectivecodingarecandidateconstructsforthetheory.
Aproposition is an interaction among constructs, which comprises
therelationsestablishedamongthecategories.An explanation com-
prises the factors behind the propositions. The explanations are
grounded in the categories, codes, relations, and in the transcripts.
Thescopeis the universe to which the theory is applicable.
4 A THEORY ON HOW DEVELOPERS
IDENTIFY DESIGN PROBLEMS
As aforementioned, we used Sj√∏berg‚Äôs framework [ 40] to describe
the theory, which is summarized in Section 2. According to his
framework,ourtheoryfitsinthe Explanationtype sinceitdescribes
andexplainshowtheidentificationofdesignproblemsisconducted
(Section 4.1),the symptomsand theircharacteristics (Section4.2),
and how the symptoms are used to diagnose design problems (Sec-
tion 4.3). In his framework, Sj√∏berg also describes criteria to evalu-
atetheories.Testabilityisonecriterion,whichindicates‚Äúthedegree
towhichatheoryisconstructedsuchthatempiricalrefutationis
possible.‚Äù Regarding such criterion, our theory has high testability
since empirical refutation of its propositions is possible by replicat-ing the study. In fact, such replication is feasible given that we firstconductedtheexperimentwiththreecompaniesandthenreplicated
it with two more companies to reach theoretical saturation.
1Camtasia is available at www.techsmith.com/camtasia.htmlTable 2: Helpfulness according to developers
SymptomApplied
timesNo. of
contributionsPercentage
of success
Design Pattern Violation 43 34 79.07%
Quality Requirements 43 31 72.09%
Violation ofNon-functional Requirements62 46 74.19%
Code Smells 37 17 45.95%
Violation ofObject-oriented Principles38 20 52.63%
When describing the theory, we introduce the constructs and
propositions,identifyingtheminthetextwith CandP,respectively.
We discuss the propositions and their constructs next. We also
presentexplanationsforpropositionsthatarealignedwithfindings
of previous studies and explanations that comprise findings thathave not been presented elsewhere. Complete description of the
constructs and propositions is available in the online material [ 26].
4.1 Identification of Design Problems
Adesignproblem (C1)arisesincodeelementsduetooneormore
design decisions (C2), made intentionally or accidentally. In fact, a
designproblemmayaffectoneormoreelementsinsuchawaythattheseelementsmanifestsymptomsofitspresence.A symptom (
C3)
is an indication of the presence of a design problem.
ThreeStepstoIdentifyDesignProblemsUsingSymptoms.
A code element may contain several design problem symptoms.
Thus,wedefinea syndrome (C4)asasetofsymptomsaffectingthe
same code element. In this context, we refer to diagnosis (C5)a s
the process of identifying a design problem through the analysis of
symptoms that manifest themselves in source code ( P1). From the
data collected during the subjects‚Äô diagnostic activities, we noticed
that the identification of design problems was often divided into
three steps: (i) locating code elements, (ii) analyzing theelements,
and(iii)confirmingorrejectingthepresenceofadesignproblem.In
all these three steps, developers rely on design problem symptoms
in source code (P2).
4.2 Design Problem Symptoms
SymptomHelpfulness. Wenoticedthatdevelopersdonotalways
consider all the symptoms of a syndrome when identifying design
problems. Instead, they only consider those symptom instances
that they judge helpful during the identification. We could identify
when developers judge a symptom helpful because we asked them
toevaluatethesymptombasedonhowhelpfulitwastoidentifytheproblem(Section3.2).Table2presentsthepercentageofhelpfulnessofeachtypeofsymptom.Thefirstcolumnindicatesthenameofthesymptom,whilethesecondcolumnshowsthenumberoftimesthat
the symptom was used by developers. The third column shows the
number of occasions that the developers mentioned the symptoms
were helpful to identify a design problem. Finally, the last column
indicates the percentage of helpfulness, i.e., the percentage that
developers used the symptom and evaluated it as helpful.
Symptom attributes that drive developers to select what
to analyze. Based on the helpfulness mentioned by developers,
we performed a qualitative analysis to investigate which symptom
925
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden L. Sousa et al.         	 
      
 


           
           
     
 !  !"# $ %  $ &  '( #  )%  *$   &  '( #  
+, , +,
-


 
 
.   
 

  
   ! 
  /  !0
 ' 1
 * ) (  $   & ' (#2
1
 * ) ( $     3 4 $  1
'5  1
+
4 6 ( $
        !
 5 1
*      
5 7  4   #8
( '   )(  $ 8
( 
"# $ %  $9
#% 
5    #&  4 ' #! 
   /   & 33    *) (  $   
  3 *   :    ;% 4     3 '* 1
% 4 6(  $<+,
+
=+
>
+
?
+
-+
@+A
+B+
C+, D+, A
+ ,B
+,
?
+,
>+,
=
+,
@
Figure 2: Theory representation of how developers identify design problems through the analysis of symptoms
attributes i.e., characteristics of the symptom (such as its accuracy
or type), developers take into consideration when they choose the
symptomsmostlikelytohelpthem.Weobservedthatthefollow-
ing symptom attributes are most helpful for developers to identify
designproblems:symptomtype,accuracy,density,relation(among
the symptoms), and diversity. Symptom type(C6) indicates a cate-
gory to which a set of symptoms with common characteristics
belongs(e.g.,codesmell). Accuracy (C7)isthedegreetowhicha
symptomiscorrectinindicatingadesignproblem,while density
(C8)isthenumberofsymptominstancesinasyndrome.Regard-
ing these attributes, we were already expecting that accuracy and
density would be attributes that developers take into account toconsider a symptom helpful. However, we had not expected that
they would take into consideration the relation among symptoms
and the diversity of symptoms in the syndrome.
Diversity of a syndrome .Relation(C9)i sh o wt w oo rm o r e
symptoms are connected. For instance, both Intensive Coupling
smellandviolationofthelayeredpattern[ 3]measurethedegree
to which elements are undesirably coupled with others. Since they
measuresimilar(albeitcomplementary)propertiesofanelement,
theyarerelatedtoeachother.Wenoticedthatdevelopersusethe
relation among the symptoms todiscover other types of symptoms
that can indicate a design problem (Section 4.3.1). We also noticed
thatdevelopersfrequentlylocatedelementsthatmanifestedseveraldifferenttypesofsymptoms(
P3).Infact,weobservedthat diversity
(C10) is another attribute that developers consider. Diversity is the
degreetowhichasyndromecontainsavarietyofsymptomtypes.
Uponanalysis,wefoundthatthemoredifferenttypesofsymptoms
a syndrome has, the greater the chance the developer will identify
at least one design problem in the element (Section 5.2).
Indeed, the diversity of a syndrome has a strong influence on
thediagnosis.Asthisfindinghadnotbeenobservedbefore,studies
that assume that developers rely on only one type of symptom
[27,32,52,53]maybemisalignedwithhowdiagnosisisconducted
inpractice,intwoways:(i)theymayassumethatdeveloperswill
use a predefined, dominant type of symptom; and (ii) they maynot consider the diversity of symptoms as another indicator foridentifyingdesignproblems.WediscussinSection5.2howdiversity
influences human aspects.
Considering the attributes that influence the developers.
Asmentionedbefore,developersdonotconsiderallsymptomin-
stancestoidentify adesignproblem.They takeintoaccountonly
thosesymptomsthattheyconsiderhelpfultoidentifyaproblem.
We showed the attributes developers consider to assume that a
symptomishelpful.Forinstance,ifasyndromehasseveraltypesof
symptoms, developers consider the density and diversityto select
thesymptom.Inotherwords,developersselectasymptomwhen
theyaresatisfiedwiththeseattributes.Conversely,whenattributes
donot satisfythe developers(e.g., thesyndrome does notcontain
diversetypesofsymptoms),thesymptomwouldbeignoredandnot
considered helpful, possibly leading to missing a design problem.
Knowingabouthowdevelopersconsiderasymptomhelpfulis
usefulforresearcherssincetheycanproposesolutionsthatempha-sizehelpfulsymptomsforthedevelopers.Forinstance,somestudies
proposesolutionstoprioritizesmellsthatcanhelpdevelopersto
identify design problems [ 1,32,50]. As code smells are a type of
symptom, they also present some of the attributes discussed above.
However, some studies on code smells may not consider attributes
as the density of smells or diversity. Therefore, developers may
neglect some code smells for not considering them helpful for the
identification.Thesestudiescouldusetheattributesthatdevelopers
take into account as a mechanism to prioritize smells (Section 6).
4.3 Design Problem Diagnosis
As aforementioned, diagnosis is the process of identifying a design
problemthroughtheanalysisofsymptoms.Wenoticedthatdevelo-
pers diagnosea design problem basedon two typesof analyses: a
symptom analysis (C11), and an epidemic analysis (C12).
4.3.1 Symptom Analysis. Insymptomanalysis,developerschoose
and analyze a set of symptoms affecting a single element, i.e., they
do not analyze multiple elements. This happens because they usu-
allyrelyontheaforementionedsymptomattributes:type,accuracy,
density, relation (among symptoms), and diversity. In this analysis,
926
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. Identifying Design Problems in the Source Code ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
Table 3: Combining Symptoms
Symptoms InstancesDesign
ProblemsTeams
1 16 11T7, T8, T9
2 13 10T1, T3, T7, T8, T9
3 14 11T3, T4, T5, T6, T7, T8, T9
4 10 6T2, T3, T5, T8, T9
5 3 1T4
developersverify,basedontheseattributes,whetherthesymptoms
affectingtheanalyzedelementindicateadesignproblem.Ifso,then
they do not proceed to analyze other elements.
Incorrectlyignoringsymptoms. Someonecanexpectthatthe
accuracy ( P4), the density ( P5), and the type ( P6) of the symptoms
influencetheproblemidentification.Forinstance,letusconsider
code smells. Palomba et al.[33] investigated to what extent code
smells are perceived as design problems. They noticed that develo-
pers consider the type of the code smell to decide whether it is a
problem. Surprisingly, developers tend to incorrectly associate the
type of symptom with its accuracy or density, and that does nothappenonlywithcodesmells.Thus,iftheyrelyontheaccuracy
or density to disagree that a symptom indicates a design problem,
theytendtonotconsiderthesametypeofsymptomintheother
elements,even whenthey actually indicatea designproblem.For
instance,ifadeveloperanalyzestheviolationofadesignpattern
suchasDataAccessObjectandconcludesthatthistypeofsymp-
tomisirrelevantforidentifyingadesignproblem,thenheisless
likely to consider a violation of a design pattern in the elements he
analyzes next. This happened, for instance, with the T3 developers.
Combiningmultiplerelatedsymptoms .Someonecanargue
thatanalyzing asingleelementis notenoughtoidentify adesign
problem.Nevertheless,wenoticedthattheycombinesymptomsinasingleelementinordertoconfirmthepresenceofadesignproblem.
Table 3 shows the frequency with which developers either used
onlyonesymptomorcombinedmultiplesymptoms.Itsfirstcolumn
indicates the number of symptoms that developers combined to
identifydesignproblems.Itssecondcolumnindicateshowmany
timesthesymptomorcombinationofsymptomshappened.Itsthird
columnindicatesthenumberofdesignproblemsfoundwhenthe
subject used a symptom or a combination of symptoms. Its last
columnindicatestheteamsthatusedorcombinedthesymptoms.
We obtained these data after applying the GT. The online material
[26]hasafullversionofthetablecontaining(i)whichsymptoms
were combined and (ii) which design problems the team found.
WecanseeinTable3thatmostdeveloperstendtocombinesymp-
toms to identify a design problem. Also, we noticed that develo-
pers identify more design problems when they combine symptoms.
Based on this result, we investigated how the combination takes
place.Wenoticedthatdevelopersusesymptomrelationstoidentify
thesymptomstocombine.Thereby,therelationhelpsdevelopers
toidentifyotherhelpfulsymptomsinthesyndrome.Therefore,themore related to others a symptom is, the greater the likelihood of a
developer selecting it for combination (P7).
Asanexampleofhowdevelopersusethesymptomrelationto
findother helpfulsymptoms,let usconsiderthe developersoftheT2 team. They were analyzing the code smells, and they noticed
that the class had the Dispersed Coupling smell. Due to the pre-
senceofthissmell,theyanalyzedthecouplingqualityrequirement.
When analyzing this type of symptom, they noticed it was indi-
cating a high coupling with other classes. This finding increased
theirconfidencethattheclasscontainedadesignproblem.These
developersalsonoticedthatthecouplingwasrelatedtoathirdtype
of symptom: violation of non-functional requirements. When they
analyzed this symptom, they noticed that the high coupling wasmaking the class harder to read. In this example, the developers
used the relation among the three symptoms (code smells, quality
requirements, and violation of non-functional requirements). Then
they combined these symptoms to identify that the element was
involved in the Concern Overload design problem [12].
Sousaet al.have shown that developers often combine mul-
tiple symptoms to identify design problems [ 41]. However, they
didnotobservehowthatcombinationtookplace.Inourcase,we
noticedthattherelationamongthesymptomsiswhatdrivesthe
combination,byhelpingtoidentifyotherrelatedsymptoms.The
combination of the symptoms is another evidence that previousstudies[
27,32,52,53]mayhaveproposedsolutionsfortheprob-
lem identification that do not fit the developers‚Äô needs. In other
words, developers consider multiple symptoms (Section 4.2) and
theyalsocombine thesesymptomstoincrease theirconfidencein
thepresenceofadesignproblem.Therefore,forcingthedevelopers
to use only a reduced set of symptoms is likely to go against the
way in which developers identify design problems in practice.
4.3.2 Epidemic Analysis. When developers analyze an element,
they do not consider only the symptoms affecting that element;
sometimes they also consider whether other elements are affected
bythesamesetofsymptoms. Wenamethisanepidemicanalysis.
Analogously to the way in which attributes influence the selection
ofsymptomsinasingleelement,thereareattributesthatdevelopers
consider before choosing elements for an epidemic analysis. In
additiontoconsideringthetypesofsymptoms( P8),developersalso
take into account the element role (C13) to choose the epidemic
elementsmostlikelytohelpthemtoidentifyadesignproblem( P9).
Element roleis the functionthat an elementplays in thesoftware
system,e.g., the role of Service.
Complementaryanalysis. Thereasonwhydevelopersusethe
elementroletoidentifyepidemicelementsisthateachdesignprob-
lemmaybescatteredoverseveralelements.Sincethoseelements
sharethesamesymptoms,developersassumethattheymayhelp
themtoidentifythedesignproblem,whichjustifiestheepidemic
analysis. Ho wever, asurprising finding is that developers analyzed
epidemic elements only when they had used the symptom analysis
buthad notsucceededin identifyingadesignproblem. Sincethey
arenotconfidentaboutthepresenceofadesignproblemduringthe
symptomanalysis,theyproceedtotheepidemicanalysisofother
elementsinordertodecidewhetherthereisadesignproblemin
the elements under analysis.
Prioritization of key elements. Developers tend to prioritize
epidemicelementsthatprovideacentralfunctionalityinthesys-
tem. This happens because they associate the role played by theelement with the probability of the element containing a design
problem( P10).Asanexample,theT2developerswereanalyzing
927
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden L. Sousa et al.
thesymptomsofanelement.Duringtheanalysis,theynoticedthat
the element was playing the Servicerole in the system. At this
point, the developers included other Serviceclasses in the anal-
ysis. When they focused the analysis on all the classes that play
a service role, this change of focus led them to identify a design
problem. They mentioned that all the service classes in the sys-tems are affected by the Scattered Concern design problem [
12].
Curiously, these developers had already analyzed other Service
classesbefore,withoutidentifyinganydesignproblem.Inthatcase,
theT2developerswerenotapplyingtheepidemicanalysis;thus,
theydidnot takeintoaccounttheelementrole toselectelements
with similar set of symptoms. Developers are more likely to accept
that elements playing an important role have a design problem.However, we found cases in which subjective factors influenced
their decision, as discussed next.
5 PROPOSITIONS CONCERNING THE
DEVELOPER
Inthissection,weprovidesomeadditionalpropositionsconcerning
thedeveloper,observedthroughthethink-aloudmethod[ 8]with
the support of video and audio recordings.
5.1 Confidence in the Presence of a Design
Problem
The confirmation or rejection of a design problem in a group of
elements is mainly influenced by the developers‚Äô confidence (C14),
whichisthedegreetowhichtheyareconvincedaboutthepresence
ofadesignproblem.Themostconfidentthedeveloperis,thegreater
the likelihood of confirming a design problem.
Attributesthatincreasedevelopers‚Äôconfidence. Theattribu-
tes that influence a design problem diagnosis also affect the develo-
pers‚Äô confidence( P11).According toour study,the attributesthat
influencethedevelopers‚Äôconfidencethemostare:accuracy,den-
sity,elementrole,anddiversity.Itisnotasurprisethatthemore
accurate ( P12) and denser ( P13) the developer believes that the
symptomis,the moreconfidenthewillbein thepresenceofade-
sign problem. Nevertheless, the element role plays an even greater
influence on the developers‚Äô confidence (P14).
Developers‚Äôdivergenceregardingelementrole. Atfirstglan-
ce, when most developers analyze an element that plays an im-portant role in the system, they tend to assume that the elementcontains a design problem. Examining further, we observed two
behaviors.Whendevelopersanalyzedelementroletogetherwith
otherattributes,theytendedtoconfirmthecorrespondingdesign
problem. Conversely,whey they onlyconsidered the elementrole
(ignoringotherattributes),theytendedtorejectthedesignprob-
lem, arguing it is acceptable to have design problem symptoms in
elements that play an important role in the system.
These two behaviors happened with T2 and T4, respectively. T2
developersconfirmedthedesignproblemintheelementbecause,
among other attributes, the element played an important role in
the system. Onthe other hand, T4 developers said that, due to the
elementrole,itisacceptablethattheelementcontainsthedesign
problemssymptoms.Accordingtothem,iftheelementwerenotan
importantclassforthesystem,itwouldnotbeacceptabletohave
a design problem or its symptoms in the class.Pondering about the number of symptoms. When a devel-
oper analyzes individual symptoms, the number of symptoms with
whichheagreesordisagreesinfluenceshisconfidenceinthedesign
problem identification. When analyzing each symptom, the devel-
operdecides whetheritindicates adesignproblem.In theend,he
counts the number of symptoms he judged as indicating a problem
and the number of symptoms he judged as irrelevant. If the former
isgreaterthanthelatter,thenheconfirmsthattheelementhasa
designproblem.T3developersusedthisstrategytoincreasetheir
confidence in the presence of a design problem in some elements.
5.2 Conscientiousness
Conscientiousness (C15) is a personality trait related to being care-
ful, responsible, and persevering [ 30]. The more conscientious the
developeris,thegreaterthelikelihoodofidentifyingadesignprob-
lem. Likewise, when developers diagnose more design problems,
theybecomemoreconscientious.Astheseattributeshaveacircular
effect between them ( P15), it would be interesting to find ways to
increase the developers‚Äô conscientiousness.
Diversityasanattributetoincreaseconscientiousness. The
diversity of symptoms is the attribute that most influences the con-
scientiousness of the developers ( P16). The higher the diversity of
a syndrome, the greater the chance the developer will identify a
design problem in the element. That happens because the diver-
sity not only increases the confidence of the developers, but it can
also help the developers to decide whether the element containsa design problem. In fact, the diversity had a great influence on
developersoftheT7,T9,T10andT11teams,becausetheytendedto
assume diversity as a strong indicator of a design problem (Section
4.2). Therefore, this finding is another evidence the studies with an
assumptionthatdevelopersrelyononlyonetypeofsymptommay
be misaligned with the developers‚Äô practice [ 27,32,52,53]. Even
worse, these studies are not taking advantage of the impact that
the diversity attribute has on developers‚Äô conscientiousness.
Side effect of only considering the diversity attribute. We
noticed a side effect when developers too much rely on the im-
portance of the diversity of a syndrome without further analyzing
other attributes. For instance, after the T4 developers had analyzed
asetofelementswithdiversesymptoms,theylaterjudgedanele-
ment as free of a design problem because it did not have the same
diversityofsymptomsastheonesanalyzedpreviously.Although
this behavior was not very frequent, it brings out another issue
thatstudiesthatrelyononlyonetypeofsymptomdonottakeinto
account.
5.3 Incapability of Providing an Alternative
Justifying the presence of a design problem with design de-
cisions.Sometimes the developers are convinced that an element
contains several symptoms that indicate a design problem, even
thoughtheydonotconfirmthepresenceofadesignproblem.Al-
thoughsuchbehaviorseemscontradictory,theyarguethattheydo
notconsidertheelementascontainingadesignproblembecause
they see no other way to implement the element. In these cases,
developersusetheconceptofdesigndecisiontojustifywhytheydonotconsiderthepresenceofadesignproblem(
P17).Consequently,
928
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. Identifying Design Problems in the Source Code ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
the design decision that developers use as an argument influences
their confidence in the presence of a design problem (P18).
Developers justified the presence of a design problem mostly
whentheycould notprovide analternative implementation. This
behavior is aligned with the theory discussed by March and Si-
mon[23],whotheorizedthatdeveloperstypicallydonotchoose
an optimal solution because such solution would require that all
alternativestoaproblembeperceived.However,theyarguethatinpracticeitisunlikelyfordeveloperstoknowallalternatives.Hence,
theknownalternativesrepresenttheboundariesthatdevelopers
facebeforemakingadecision.Therefore,developersstopsearching
for further solutions when one that satisfies their needs is found.
Justifyingthepresenceofadesignproblemwiththelack
ofanalternativeimplementation. MarchandSimon‚Äôs[ 23]the-
oryalsomanifestsinthecontextofidentifyingdesignproblems,as
weobservedinourstudy.Thedevelopersusedthelimitedknownal-
ternativestojustifywhyaspecificimplementationdoesnotpresent
adesignproblem.Inthesecases,theymentionedthattheycould
not find any alternative solution (optimal or not) to implement the
element. According to them, the element should not be considered
as an element involved in a design problem. In other words, the
knownalternativesarenotonlyboundariesthatdevelopersface,
butalsousedtojustifythepresenceorabsenceofadesignproblem.
6 TOWARDS IMPROVING DESIGN PROBLEM
DIAGNOSIS
Researcherscanusethediscussionspresentedinthispaperasan
underlyingmechanismtodrivesolutionsforsupportingdevelopers
during designproblem identification.For instance, inthis section,
we present some of these solutions that emerged from the theory.
6.1 Supporting Multiple Symptoms
Providing multiple design problem symptoms. Most studies
rely on a single, predefined, dominant type of symptom [ 27,32,
52,53], which may be limiting how developers identify design
problemsinpractice.Thus,thereisaneedforsolutionsthatprovidedeveloperswithmultiplesymptoms,andthenhelpthemtonavigate
amongthesesymptomsandtocombinethem.Infact,wenoticed
thatdeveloperswouldbenefitfrommechanismstoautomatically
provide symptoms for combination. For instance, a solution in this
sense is to provide other symptoms that are complementary to the
one being analyzed. Such tool, for instance, could have helped the
T2developerstoidentifyadesignproblem(Section4.3.1).Theyused
the Dispersed Coupling to choose the coupling attribute to analyze
next. Later, they chose the readability non-functional requirement
to complement their analysis. In this example, a tool could provide
thecouplingattributeandthereadabilityrequirementassoonasthe
developers indicate the Dispersed Coupling code smell as helpful.
Filtering relevant symptoms. Developers consider the diver-
sity ofsymptoms.However, ifan elementmanifests severalsymp-
toms, the developers could have a hard time to choose the most
helpfulone.Forinstance,D16(T8team)mentionedthedifficulty
that he had to choose helpful symptoms:
D16:‚ÄúSince I was not familiar with each type of symptom and design
problem,itwashardformetomatchthem.Evenwiththeprovidedsymptoms,
Icouldnotfigureoutwhichonewasactuallyrelatedtothedesignproblems.‚ÄùTo address this issue, an automatic tool could help them to filter
thosesymptomsthataremostlikelytoindicateadesignproblem.In
the same way that a tool could propose complementary symptoms
to the one being analyzed, it could hide symptoms that are least
similartotheoneunderanalysis.Suchtoolcouldmaketheanalysis
of multiple symptoms less cumbersome.
Visualizationsupport. Anothersolutiontohelpdevelopersto
deal with multiple symptoms is to provide visualization mecha-
nisms.Forinstance, ScatteredConcern [12]problemoccurswhen
multiple code elements implement a functionality that should have
been implemented by only a few elements. In this case, developers
have to analyze multiple elements that may have the scattered
functionality.Theseelementsarelikelytosharesomesymptoms.
Perhaps if developers could visualize how the multiple symptoms
interactinthesystem,theycouldidentifytheseelementsmoreeas-
ily.Infact,D14(T7team)mentionedinthefollow-upquestionnaire
that a visualization mechanism would help him to identify some
design problems:
D14:‚ÄúFor some design problems e.g. Cyclic Dependency, Scattered Concern,
it‚Äôs hard to find by looking at the source code manually, which is too low
level when we don‚Äôt have a higher level architecture view.‚Äù
6.2 Prioritization of Similar Elements
Prioritizing epidemic elements. Developers tend to prioritize
elements that play an important role in the system. In addition,if these elements have diverse symptoms, then they should bethe first elements to be analyzed by the developers. Researchers
could therefore use the attributes presented here to build tools that
prioritizeelements.Forinstance,developersoftheT2teamused
theelementroleduringtheepidemicanalysis(Section4.3.2).Intwo
casestheyreliedontheelementroletoselectepidemicelements.
However,inonecasetheycouldidentifyadesignproblem,whereasintheothercase,theycouldnot.Thedifferencebetweenthesetwo
cases wasrelated tothe number ofepidemic elementsplaying the
same role. While in the first case all the epidemic elements played
theServicerole, in the second case only few epidemic elements
playedthe Controller role.Thefollowingquotationsillustratethis.
D4:‚ÄúI think that all the service classes will have (the design problem)‚Äù
D3:‚ÄúIndeed, the service (classes)‚Äù
D4:‚ÄúIguessthat(they)aresimilartoeachother.Infact,Ibelievethatthe
next service (class) will be similar‚Äù
6.3 Additional Support for the Developer
Based on the propositions concerning the developers (Section 5)
we suggest providing the following additional support.
Providinganalternativeimplementation. Itisoftendifficult
for developers to provide an alternative implementation for anelement that may contain a design problem (Section 5.3). In thiscontext, a tool could indicate an alternative implementation that
couldremovethedesignproblemsymptoms.Hence,adeveloper
would not be able to use the lack of an alternative implementation
as justification for not confirming a design problem.
Personalizing the detection of symptoms. The accuracy of
the symptom is also influenced by the developers‚Äô subjectivity.
Developersmentionedthatacertaintypeofsymptomwasaccurate
in indicating a design problem in some elements, but not in others.
929
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden L. Sousa et al.
Thus,mostdevelopersmentionedthattheyneedtoolsthatallow
them to personalize the detection ofsymptoms according to their
software systems. Allowing developers to adjust thresholds and
detection rules would minimize how the (low) accuracy influences
theirconfidenceinthepresenceofadesignproblem.D11(T6team)
mentionedinthefollow-upquestionnairetheneedforsuchfeature:
D11:‚ÄúThesymptomssuggestapossibledesignproblem.However,noneof
them should be rigid rules. Often, it makes sense to have long methods,
message chains or many parameters (in the method). In some cases, we
could replace a long string of conditional (statements), but it would make it
difficulttounderstand.Amethodwasconsideredlong,butitsreadability
was very clear, which did not justify a refactoring.‚Äù
7 RELATED WORK
Alongthepaper,wepresentedsomestudiesabouttheidentification
of design problems. We have not found studies that present the
diagnosisofdesignproblemsasatheory.Instead,wefoundstudies
thatfocusonpresentingthephenomenonratherthanexplaining
it [6,20‚Äì22,27,28,32,47,50,53]. For instance, several researchers
proposed techniques to identify design problems [ 20‚Äì22,32,53].
Althoughthesestudieshadencouragingresults,theydidnotcon-
duct experiments with software developers or they have not taken
intoaccounttheattributesthataffectdesignproblemidentification.
Asfarasweareconcerned,Sousa etal.istheonlystudythathas
proposed to explain how developers identify design problems [ 41].
However,theyfellshortofframingtheirresultsasatheory.Similar
totheotherstudies,theyonlypresentthephenomenon,ratherthan
explain it. For instance, the authors only provide the symptoms
thatdeveloperstakeintoaccount,buttheycouldneitherexplain
how developers find these symptoms nor describe the attributes
that developers take into account during the diagnosis. Conversely,
we highlight that our goal was not to provide the most preeminent
symptomsnorthesymptomsthatleadtotheidentificationofde-
sign problems. Instead, we focused on revealing the attributes that
contribute to diagnosing a design problem the most, which allows
us to explainhow developersidentify designproblems inpractice.
8 THREATS TO VALIDITY
This section presents and discusses threats to validity.
Construct Validity. We provided some symptoms for develo-
pers to use during design problems diagnosis. These data could
have biased the experiments. However, we provided these dataconsidering the literature [
11,20‚Äì22,24,32] and considering the
companies‚Äômanagers.Theymentionedthatsomeofthedevelopersnotonlywerefamiliarwithsomesymptomsbutalsohadtheculture
of using them. Furthermore, our goal was to explain the diagnosis
of design problems, and not delve too deeply into the symptoms.
The time allocated for the tasks could be considered another threat
to validity. However, we conducted a pilot study to adjust the time
required to perform the tasks and thus reduce the threat.
InternalValidity. Thedifferencebetweenthedevelopers‚Äôback-
groundknowledgecanbeathreat.However,inthecontextofapply-ingananalysisthroughGT,wesawthisdiversityasanopportunity
to strengthen the evidence supporting the depicted propositions.
Moreover, we provided training to mitigate this threat.
External Validity. The number of subject represents a threat.
All of them worked for companies located in Brazil. However, it isimportanttonotethatthisisamulti-companystudyinvolvingfivedifferentworkingenvironmentsandeightdifferentsystems.Finally,the presented study covered only systems developed in Java. Using
otherprogramminglanguageswithdifferentcorecharacteristics
may influence developers in identifying design problems.
Conclusion Validity. Theparticipationof theauthorwhofol-
lowedtheGTproceduresposesanotherthreat.Hisbeliefsmight
havecausedsomedistortionswheninterpretingthedata.Tomiti-
gate this threat, the GT coding activities were shared with other
researchers. Moreover, the identification of the constructs and the
depicting of propositions were performed separately by the first
authorand otherresearchers.In fact,threeauthors conductedthe
GroundedTheoryproceduresindependently;thenwemergedtheir
results to shape the theory. Thus, the contents were compared and
discussed by the researchers until reaching a consensus.
9 CONCLUDING REMARKS
Adesignproblemistheresultofoneormoreinappropriatedeci-
sions that negatively impact non-functional requirements. Despite
their harmfulness, the identification of each design problem is not
trivial. One of the main reasons is that design documentation is
oftenunavailableoroutdated.Thus,developersoftenhavetorely
on the source code to identify design problems, which may quickly
turnintoacomplextask.Althoughresearchershaveinvestigated
techniques to help developers, there is little knowledge on how
developersactuallyproceedtoidentifydesignproblemsinpractice.
In order to address this limitation, we conducted a multi-trial
industrial experiment with developers from different companies,
where they had to identify design problems in their systems. As
aresult,wederivedatheorydescribingtheactivitiesandfactors
that influence on how developers identify design problems, which
canservetofurtherunderstandtheidentificationofdesignprob-
lems. For example, the theory reveals that developers rely on a
heterogeneoussetofsymptoms,andtheytendtocombinesymp-
toms.Thetheoryalsopresentsthecharacteristicsofsymptomsthatdevelopersconsiderhelpful.Then,wediscussedhowtheknowledge
revealed by our theory can be used to advance the state-of-art.
Futurestepsinthisworkinvolvetheexecutionofnewempiri-
calstudiestoassessinmoredepththetheory‚Äôspropositionsand
explanations.For instance,we intendto addresssome findingsde-
scribedatSection6andverifywhethertheyhavepositiveeffects
on design problem identification. The goal of these studies is touse the theory to implement a novel family of solutions that are
more effective than the current ones in helping developers identify
design problems.
10 ACKNOWLEDGMENT
The authors would like to thank the reviewers for their valuable
comments and suggestions. This work is funded by CAPES/Procad
(grant # 175956), CAPES/Procad (grant # 175956), CNPq (grants
#309884/2012-8,483425/2013-3and477943/2013-6),FAPERJ(E26-
102.166/2013), FAPERJ (grant # 102166/2013 and 22520 7/2016) and
FAPEAL (grant Institucional Links - V√≠rus Zika and PPGs 14/2016).
930
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. Identifying Design Problems in the Source Code ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]M Abbes, F Khomh, Y Gueheneuc, and G Antoniol. 2011. An Empirical Study
of the Impact of Two Antipatterns, Blob and Spaghetti Code, on Program Com-
prehension. In Proceedings of the 15th European Software Engineering Conference;
Oldenburg, Germany. 181‚Äì190.
[2]Holger B√§r and Oliver Ciupke. 1998. Exploiting Design Heuristics for Automatic
Problem Detection. In Workshop Ion on Object-Oriented Technology (ECOOP ‚Äô98).
Springer-Verlag, London, UK, UK, 73‚Äì74.
[3]FrankBuschmann,RegineMeunier,HansRohnert,PeterSommerlad,andMichael
Stal.1996. Pattern-OrientedSoftwareArchitecture-Volume1:ASystemofPatterns.
Wiley Publishing.
[4]GCampbellandPatroklosPPapapetrou.2013. SonarQubeinaction. Manning
Publications Co.
[5]Munkhnasan Choinzon and Yoshikazu Ueda. 2006. Detecting Defects in Object
Oriented Designs Using Design Metrics. In Proceedings of the 2006 Conference
onKnowledge-BasedSoftwareEngineering:ProceedingsoftheSeventhJointCon-
ference on Knowledge-Based Software Engineering. IOS Press, Amsterdam, The
Netherlands,TheNetherlands,61‚Äì72. http://dl.acm.org/citation.cfm?id=1565098.
1565107
[6]O. Ciupke. 1999. Automatic detection of design problems in object-oriented
reengineering. In Proceedings of Technology of Object-Oriented Languages and
Systems - TOOLS 30 (Cat. No.PR00278). 18‚Äì32.
[7]BillCurtis,JaySappidi,andAlexandraSzynkarski.2012. EstimatingtheSize,Cost,
and Types of Technical Debt. In Proceedings of the Third International Workshop
onManagingTechnicalDebt(MTD‚Äô12).IEEEPress,Piscataway,NJ,USA,49‚Äì53.
http://dl.acm.org/citation.cfm?id=2666036.2666045
[8]K. Anders Ericsson and Herbert A. Simon. 1993. Protocol Analysis: Verbal Reports
as Data(2 ed.). A Bradford Book.
[9]M.Fokaefs, N.Tsantalis,andA. Chatzigeorgiou.2007. JDeodorant:Identification
andRemovalofFeatureEnvyBadSmells.In 2007IEEEInternationalConference
on Software Maintenance. 519‚Äì520. https://doi.org/10.1109/ICSM.2007.4362679
[10]M Fowler. 1999. Refactoring: Improving the Design of Existing Code. Addison-
Wesley Professional, Boston.
[11]ErichGamma,RichardHelm,RalphJohnson,andJohnVlissides.1995. Design
Patterns:ElementsofReusableObject-orientedSoftware. Addison-WesleyLongman
Publishing Co., Inc., Boston, MA, USA.
[12] J Garcia, D Popescu, G Edwards, and N Medvidovic. 2009. Identifying Architec-
tural Bad Smells. In CSMR09; Kaiserslautern, Germany. IEEE.
[13]B.G.Glaser.1998. DoingGroundedTheory:IssuesandDiscussions. SociologyPress.
https://books.google.com.br/books?id=XStmQgAACAAJ
[14]M Godfrey and E Lee. 2000. Secrets from the Monster: Extracting Mozilla‚Äôs
Software Architecture. In CoSET-00; Limerick, Ireland. 15‚Äì23.
[15]J. E. Hannay, D. I. K. Sjoberg, and T. Dyba. 2007. A Systematic Review of The-ory Use in Software Engineering Experiments. IEEE Transactions on Software
Engineering 33, 2 (Feb 2007), 87‚Äì107. https://doi.org/10.1109/TSE.2007.12
[16]RossJeffery.2013. PathstoSoftwareEngineeringEvidence. SpringerBerlinHeidel-
berg, Berlin, Heidelberg, 133‚Äì144. https://doi.org/10.1007/978-3-642-37395-4_9
[17]P. Johnson, M. Ekstedt, and I. Jacobson. 2012. Where‚Äôs the Theory for Software
Engineering? IEEE Software 29, 5 (Sept 2012), 96‚Äì96. https://doi.org/10.1109/MS.
2012.127
[18]P. Kaminski. 2007. Reforming Software Design Documentation. In 14th Working
Conference on Reverse Engineering (WCRE 2007). 277‚Äì280.
[19]A MacCormack, J Rusnak, and C Baldwin. 2006. Exploring the Structure of
Complex Software Designs: An Empirical Study of Open Source and Proprietary
Code.Manage. Sci. 52, 7 (2006), 1015‚Äì1030.
[20]I.Macia,R.Arcoverde,E.Cirilo,A.Garcia,andA.vonStaa.2012. Supportingthe
identification of architecturally-relevant code anomalies. In ICSM12. 662‚Äì665.
[21]I. Macia, R. Arcoverde, A. Garcia, C. Chavez, and A. von Staa. 2012. On the Rele-
vanceofCodeAnomaliesforIdentifyingArchitectureDegradationSymptoms.
InCSMR12. 277‚Äì286.
[22]Isela Macia, Joshua Garcia, Daniel Popescu, Alessandro Garcia, Nenad Medvi-
dovic,andArndtvonStaa.2012. AreAutomatically-detectedCodeAnomalies
Relevant to Architectural Modularity?: An Exploratory Analysis of Evolving
Systems. In AOSD ‚Äô12. ACM, New York, NY, USA, 167‚Äì178.
[23]J.G.MarchandH.A.Simon.1958. Organizations. Wiley. https://books.google.
com.br/books?id=fx1HAAAAMAAJ
[24]RMartin.2002. AgilePrinciples,Patterns,andPractices. PrenticeHall,NewJersey.
[25]Robert C.Martin andMicah Martin.2006. AgilePrinciples, Patterns,and Practices
in C# (Robert C. Martin). Prentice Hall PTR, Upper Saddle River, NJ, USA.
[26] Complementar Material. 2017. https://ssousaleo.github.io/ICSE2018/. (2017).[27]
Ran Mo, Yuanfang Cai, R. Kazman, and Lu Xiao. 2015. Hotspot Patterns: The
FormalDefinition andAutomatic Detectionof ArchitectureSmells. In Software
Architecture (WICSA), 2015 12th Working IEEE/IFIP Conference on. 51‚Äì60.
[28]N Moha, Y Gueheneuc, L Duchien, and A Le Meur. 2010. DECOR: A Method for
the Specification and Detection of Code and Design Smells. IEEE Transaction on
Software Engineering 36 (2010), 20‚Äì36.[29]EmersonMurphy-Hill andAndrewPBlack. 2010. An interactiveambientvisu-
alization for code smells. In Proceedings of the 5th international symposium on
Software visualization; Salt Lake City, USA. ACM, 5‚Äì14.
[30]W. T. Norman. 1963. Toward an adequate taxonomy of personality attributes:replicatedfactorsstructureinpeernominationpersonalityratings. Journalof
abnormal and social psychology 66 (June 1963), 574‚Äì583.
[31]WOizumiandAGarcia.2015. Organic:APrototypeToolfortheSynthesisof
Code Anomalies. (2015). http://wnoizumi.github.io/organic/
[32]W Oizumi, A Garcia, L Sousa, B Cafeo, and Y Zhao. 2016. Code Anomalies
Flock Together: Exploring Code Anomaly Agglomerations for Locating Design
Problems. In The 38th International Conference on Software Engineering; USA.
[33]F.Palomba,G.Bavota,M.D.Penta,R.Oliveto,andA.D.Lucia.2014. DoThey
ReallySmellBad?AStudyonDevelopers‚ÄôPerceptionofBadCodeSmells.In 2014
IEEEInternationalConferenceonSoftwareMaintenanceandEvolution.101‚Äì110.
https://doi.org/10.1109/ICSME.2014.32
[34]DavidL.Parnas.1978. DesigningSoftwareforEaseofExtensionandContraction.
InProceedingsofthe3rdInternationalConferenceonSoftwareEngineering(ICSE
‚Äô78). IEEE Press, Piscataway, NJ, USA, 264‚Äì277.
[35]Dewayne E. Perry and Alexander L. Wolf. 1992. Foundations for the Study
of Software Architecture. SIGSOFT Softw. Eng. Notes 17, 4 (Oct. 1992), 40‚Äì52.
https://doi.org/10.1145/141874.141884
[36]PerRuneson,MartinHost,AustenRainer,andBjornRegnell.2012. CaseStudy
Research in Software Engineering: Guidelines and Examples. Wiley Publishing.
[37]SSchach,BJin,DWright,GHeller,andAOffutt.2002. Maintainabilityofthe
Linux kernel. Software, IEE Proceedings - 149, 1 (2002), 18‚Äì23.
[38]W. R. Shadish, T. D. Cook, and Donald T. Campbell. 2001. Experimental and
Quasi-ExperimentalDesignsforGeneralizedCausalInference (2ed.). Houghton
Mifflin.
[39]MarcelinoCampos OliveiraSilva,Marco TulioValente,andRicardo Terra.2016.
DoesTechnicalDebtLeadtotheRejectionofPullRequests?.In Proceedingsof
the 12th Brazilian Symposium on Information Systems (SBSI ‚Äô16). 248‚Äì254.
[40]DagI.K.Sj√∏berg,ToreDyb√•,BenteC.D.Anda,andJoE.Hannay.2008. Building
Theories in Software Engineering. Springer London, London, 312‚Äì336. https:
//doi.org/10.1007/978-1-84800-044-5_12
[41]LeonardoSousa,RobertoOliveira,AlessandroGarcia,JaejoonLee,TayanaConte,
WillianOizumi,RafaeldeMello,AdrianaLopes,NatashaValentim,EdsonOliveira,
and Carlos Lucena. 2017. How Do Software Developers Identify Design Prob-lems?: A Qualitative Analysis. In Proceedings of 31st Brazilian Symposium on
Software Engineering (SBES‚Äô17). 12.
[42]Klaas-Jan Stol and Brian Fitzgerald. 2015. Theory-oriented software engineering.
ScienceofComputerProgramming 101(2015),79‚Äì98. https://doi.org/10.1016/j.
scico.2014.11.010 Towards general theories of software engineering.
[43]Klaas-Jan Stol, Paul Ralph, and Brian Fitzgerald. 2016. Grounded Theory in
SoftwareEngineeringResearch:ACriticalReviewandGuidelines.In Proceedings
ofthe38thInternationalConferenceonSoftwareEngineering(ICSE‚Äô16).ACM,New
York, NY, USA, 120‚Äì131. https://doi.org/10.1145/2884781.2884833
[44]A.StraussandJ.M.Corbin.1998. BasicsofQualitativeResearch:Techniquesand
Procedures for Developing Grounded Theory. SAGE Publications.
[45]Antony Tang, Aldeida Aleti, Janet Burge, and Hans van Vliet. 2010. What makes
softwaredesigneffective? DesignStudies 31,6(2010),614‚Äì640. SpecialIssue
Studying Professional Software Design.
[46]RichardN.TaylorandAndrevanderHoek.2007. SoftwareDesignandArchi-tectureTheOnceandFutureFocusofSoftwareEngineering.In 2007Futureof
Software Engineering (FOSE ‚Äô07). IEEE Computer Society, Washington, DC, USA,
226‚Äì243. https://doi.org/10.1109/FOSE.2007.21
[47]A. Trifu and R. Marinescu. 2005. Diagnosing design problems in object oriented
systems. In WCRE‚Äô05.1 0p p .
[48]Adrian Trifu and Urs Reupke. 2007. Towards Automated Restructuring of Object
Oriented Systems. In CSMR ‚Äô07. IEEE, Washington, DC, USA, 39‚Äì48.
[49]JvanGurpandJBosch.2002. Designerosion:problemsandcauses. Journalof
Systems and Software 61, 2 (2002), 105 ‚Äì 119.
[50]S. Vidal, E. Guimaraes, W. Oizumi, A. Garcia, A. D. Pace, and C. Marcos. 2016.
Identifying Architectural Problems through Prioritization of Code Smells. In
SBCARS16. 41‚Äì50.
[51]SantiagoA.Vidal,Hern√°nCeferinoV√°zquez,JorgeAndr√©sD√≠azPace,Claudia
Marcos,AlessandroF.Garcia,andWillianNalepaOizumi.2015.JSpIRIT:aflexible
toolfortheanalysisofcodesmells.In 34thInternationalConferenceoftheChilean
Computer Science Society (SCCC). IEEE, Santiago, Chile, 1‚Äì6.
[52]S. Wong, Y. Cai, M. Kim, and M. Dalton. 2011. Detecting software modularity
violations. In Software Engineering (ICSE), 2011 33rd International Conference on.
411‚Äì420. https://doi.org/10.1145/1985793.1985850
[53]LuXiao,YuanfangCai,RickKazman,RanMo,andQiongFeng.2016. Identifying
and Quantifying Architectural Debt. In Proceedings of the 38th International
ConferenceonSoftwareEngineering(ICSE‚Äô16).ACM,NewYork,NY,USA,488‚Äì
498. https://doi.org/10.1145/2884781.2884822
[54]A Yamashita and L Moonen. 2012. Do code smells reflect important maintain-
ability aspects?. In ICSM12. 306‚Äì315.
931
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:38:44 UTC from IEEE Xplore.  Restrictions apply. 