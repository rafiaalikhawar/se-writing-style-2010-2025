Are Mutation Scores Correlated with Real Fault Detection?
A Large Scale Empirical study on the Relationship Between Mutants and Real Faults
Mike Papadakis
University of Luxembourg
michail.papadakis@uni.luDonghwan Shin
Korea Advanced Institute of Science and Technology
donghwan@se.kaist.ac.kr
Shin Yoo
Korea Advanced Institute of Science and Technology
shin.yoo@kaist.ac.krDoo-Hwan Bae
Korea Advanced Institute of Science and Technology
bae@se.kaist.ac.kr
ABSTRACT
Empiricalvalidationofsoftwaretestingstudiesisincreasinglyrely-
ingonmutants.Thispracticeismotivatedbythestrongcorrelation
between mutant scores and real fault detection that is reported in
theliterature.Incontrast,ourstudyshowsthatcorrelationsaretheresultsoftheconfoundingeffectsofthetestsuitesize.Inparticular,
we investigate the relation between two independent variables,
mutation score and test suite size, with one dependent variable the
detection of (real) faults. We use two data sets, CoreBench and De-
fects4J, with large C and Java programs and real faults and provide
evidence that all correlations between mutation scores and realfault detection are weak when controlling for test suite size. Wealso find that both independent variables significantly influence
thedependentone,withsignificantlybetterfits,butoverallwith
relative low prediction power. By measuring the fault detection
capabilityofthetopranked,accordingtomutationscore,testsuites
(opposed to randomly selected test suites of the same size), we find
thatachievinghighermutationscoresimprovessignificantlythe
fault detection. Taken together, our data suggest that mutants pro-
vide good guidance for improving the fault detection of test suites,
but their correlation with fault detection are weak.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
mutation testing, real faults, test suite effectiveness
ACM Reference format:
Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae. 2018. Are
Mutation Scores Correlated with Real Fault Detection?. In Proceedings of
ICSE’18:40thInternationalConferenceonSoftwareEngineering,Gothenburg,
Sweden, May 27-June 3, 2018 (ICSE ’18), 12 pages.
https://doi.org/10.1145/3180155.3180183
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05 ...$15.00
https://doi.org/10.1145/3180155.31801831 INTRODUCTION
What is the relation between mutants and real faults? To date, this
fundamental question remains open and, to large extent, unknown
ifnotcontroversial.Though,alargebody(approximately19%[ 34])
of the software testing studies rely on mutants.
Recent research investigated certain aspects of the fault and mu-
tant relation, such as the correlation between mutant kills with
real fault detection [ 3,24] and the fault detection capabilities of
mutation testing [ 8]. Justet al.[24] report that there is “a statis-
tically significant correlation between mutant detection and real
fault detection, independently of code coverage”, while Chekam
etal.[8]that“faultrevelationstartstoincreasesignificantlyonly
once relatively high levels of coverage are attained”.
Although thesestudies provideevidence supportingthe useof
mutantsinempiricalstudies,thisiscontradictorytothefindings
ofotherstudies,e.g.,studyofNaminandKakarla[ 28],andtosome
extentbetweenthemselves(astheydonotagreeonthestrengthand
nature of the investigated relations). Furthermore, there are many
aspects of the mutant-fault relation that still remain unknown.
For instance, the study of Just et al.[24] did not control for
thesizeofthe testsuites,whichisastrong confoundingfactorin
software testing experiments [ 21,27]. This is because a larger test
suiteismorelikelytodetectmorefaultsthanasmallerone,simply
because it contains more tests. At the same time, a larger test suite
kills more mutants than a smaller one. Therefore, as both mutation
scoreandtestsuitesizearefactorswithpotentialimpactonfault
detection, it is unclear what is the relation between mutation score
and real fault detection, independently of test suite size.
Additionally,thestudyofJust etal.[24]measuredthe correlation
betweenmutantkillsandfaultdetectiononJavaprogramswhile
thestudyofChekam etal.[8]measuredthe actualfaultdetection of
mutation-based test suites on C programs. Therefore, it is unclear
whetherthefindingsonJavaprogramsholdontheCones(andvice
versa)andmoregenerally,whetherthereisanypracticaldifference
betweenthetwoevaluationmeasurements,i.e.,correlationanalysis
between mutant kills and fault detection and actual fault detection
rate of mutation-based test suites.
The differencesbetween these two evaluationmetrics is impor-
tant as they are extensively used in empirical studies [ 36]. Yet, it
is unclear whether there are any practically significant differences
between them. In case the differences are significant, one could
drawdifferentconclusionsbyusingonemetricovertheother.Thus,investigatingthepotentialdifferencesbetweenthesemetricscanbe
useful to other studies that compare test criteria and test methods.
5372018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae
In our study, we use a large number of real faults from real-
worldCandJavaprograms.Toperformouranalysisinareliable
and as generic as possible way, we use the developer test suites,
augmentedbystate-of-the-arttestgenerationtools,KLEEforC[ 7],
Randoop [ 32] and EvoSuite [ 15] for Java. These tools helped us
composing a large, diverse and relatively strong test pool from
which we sample multiple test suites. To ensure the validity of our
analysis, we also repeat it with the developer and automatically
generated test suites and found insignificant differences.
One key resultis that the correlationbetween mutant kills and
real fault detection drops significantly and became weak when
controlling for test suite size. We also perform regression analysis
andshowthatbothsizeandmutationscoreindependentlyinfluence
the fault detection. Both of them, alone achieve a good level of
correlation, which becomes weak when the other is put under
experimental control. Overall, the combination of both size andmutationscoreachievesstatisticallysignificantlybetterfitsthan
the size and mutation score alone.
Interestingly,a deeperanalysis revealsthatmutants areindeed
capableofrepresentingthebehaviourofrealfaults.However,these
mutants are very few (less than 1% of the involved mutants). This
means that only a few mutants play the key role in representing
thebehaviourofrealfaults,andthus,mutationscoresaresubject
to ‘noiseeffects’ causedby the largenumbers ofmutants that are,
in some sense, “irrelevant” to the studied faults.
Finally,ourstudyinvestigatesthetesteffectivenessquestion,i.e.,
howeffectivemutantsareatfindingfaults.Correlationsmeasure
the extent of the variability of the independent variable, i.e., mu-
tationscore,isexplainedbythevariabilityofthedependentone,
i.e.,faultdetection.However,thisdoesnotnecessarilyimplythat
atestcriterionprovides(ornot)enoughguidanceonuncovering
faults when fulfilling its test requirements as the relation might be
non-linear[ 8,12].We,thus,demonstratethatcorrelationsdonot
reflect the fault detection capabilities of mutation testing, which
can be significant despite the low correlations.
2 MUTATION ANALYSIS
Mutationanalysisintroducesdefects,called mutants,whichform
the test objectives. A test case detects a mutant-defect, when it
makes its observable program output different from that of theoriginal program. A detected mutant is call killed, while a non-
detected one is called live.
Theratioofthekilledtothetotalnumberofmutantsiscalled
mutation score and represents the degree of adequacy achievement
of the test suites. Unfortunately, some mutants cannot be killed as
they are functionally equivalent to the original program [ 35,39].
These mutants are called equivalent and need to be removed from
the calculation of the mutation score. However, their identification
is done manually as it is an instance of an undecidable problem [ 2].
Mutation analysis has many applications [ 31,36], but the ma-
jority of the existing work is using it to support experimenta-
tion [34,36] and the testing process [ 2,16,36]. In the former case,
the key question is whether mutants provide results that are repre-
sentative of those that one could obtain by using real faults. In the
latter case, the key question is whether, by killing more mutants
one can significantly increase the capability to detect faults.2.1 Mutants and Real Faults
Studies investigating the relationship between mutants and realfaults are summarised in Table 1. Note that Table 1 is strictly re-
stricted to the findings related to the relationship between mutantsandrealfaults.Additionaldetailsrelatedtothesubjectcanbefound
in the recent mutation testing survey of Papadakis et al.[36]. As
canbeseenintheTable, sixofthestudiesconcludedthatthereis
a strong connection between mutation score and fault detection,
independentofthetestsuitesize.However,studiesconsideringthe
influenceofsizereportmixedresults.Amongfoursuchstudies,one
reports weak correlation, study of Namin and Kakarla [ 28], one re-
portssomeformofstrongcorrelationasitreportsminordifferences
betweenthemutantandfaultdetectionratios,studyofAndrews
etal.[3],andtheremainingtworeportthattheimprovementon
fault detection when reaching higher mutation score levels was
non-linear,studiesofFrankl etal.[14]andChekam etal.[8].There-
fore, the emerging questionis the one about the relationbetween
size, mutation score, and fault detection.
Furthermore, all these studies have been assessed by employ-
ing two main evaluation metrics. These are either some form ofcorrelation analysis between mutant kills and fault detection ra-
tios (studies with references [ 3,4,10,24,28]) or the fault detection
rateatthehighermutationscorelevels(studies[ 8,14,37,38,40]).
Therefore, it is unclear what the relationship between these two
evaluation metrics is, and what the relationship implies for the
actual application of mutation testing.
Perhaps the first study that investigated the use of mutants
as replacements of real faults was that of Daran and Thévenod-Fosse [
10]. This study considere daCp rogram of approximately
1,000linesofcodewith12faultsandshowedthatmutantsinfect
the internal program states in a way that is similar to the way that
realfaultscorruptprogramstates.Inparticular,thestudyreports
that 85% of the corrupted states, produced by mutants, were the
samewiththoseproducedbyrealfaults.Onlythe7%weredifferent
atthefaultintroductionstateand8%weredifferentduringerror
propagation. Overall, all the failures caused by real faults were
reproduced by the mutants.
Andrews etal.[3,4]usedaCprogram(named space)ofapprox-
imately 5,000 lines of code with 38 faults and demonstrated that
mutantkillsandfaultdetectionratioshavesimilartrends.Inalater
study,NaminandKakarla[ 28]usedthesameprogramandfaultset
andcametotheconclusionthatthereisaweakcorrelationbetween
mutants andfault detectionratios. Recently, Just et al.[24] useda
largenumberofrealfaultsfromfiveJavaprojectsanddemonstrated
that mutant detection rates have a strong positive correlation with
faultdetection rates.Since thestudyof Just etal.[24]did notcon-
sidertestsuitesizeanditsresultscontradicttheonesofNaminand
Kakarla[ 28],itremainsunclearwhethermutationscoreactually
correlates with fault detection when test suite size is controlled.
PapadakisandMalevris[ 37]usedthespaceprogram,Cprogram
of approximately 5,000 lines of code, with 38 faults and found
that mutants provide good guidance towards improving test suites
independent of test suite size. Shin et al.[40] and Ramler et al.[38]
came to similar conclusions (mutants can help improving the fault
detectionoftestsuites). However,both thesestudiesdidnotaccount
for the size effects of the test suites.
538
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. Are Mutation Scores Correlated with Real Fault Detection? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 1: Summary of studies investigating the relationship between mutants and real faults.
Author(s) [Reference] YearLargest
SubjectLanguageConsidered
Test SizeNo Faults Summary of Scientific Findings
Daran &
Thévenod-Fosse [10]96 1,000 C × 12Mutants result in failures and program data states that are similarto those produced by
real faults.
Franklet al.[14] ’97 78Fortran,
Pascal/check 9Fault detection probability is increasing at higher mutation score levels. The increase is
non-linear.
Andrews et al.[3] ’05 5,000 C /check 38 Mutants detection ratios are representative of fault detection ratios
Andrews et al.[4] ’06 5,000 C × 38 Mutants detection ratios are representative of fault detection ratios
Papadakis & Malevris [37] ’10 5,000 C × 381storder mutation has higher fault detection than 2ndorder and mutant sampling. There
are significantly less equivalent 2nd order mutants than 1storder ones.
Namin & Kakarla [28] ’11 5,000 C /check 38 Thereisaweak correlation betweenmutantdetectionratiosandrealfaultdetectionratios
Justet al.[24] ’14 96,000 Java × 357There is a strong correlation between mutant detection ratios and real fault detection
ratios
Shinet al.[40] ’17 96,000 Java × 352Distinguishing mutation adequacy criterion has higher fault detection probability than
strong mutation adequacy criterion
Ramleret al.[38] ’17 60,000 Java × 2Mutation testing helps improving the test suites of a safety-critical industrial software
system by increasing their fault detection potential.
Chekamet al.[8] ’17 83,100 C /check 61Mutation testing provides valuable guidance for improving test suites and revealing real
faults. There is a strong connection between mutation score increase and fault detection
at higher score levels.
This paper ’18 96,000 C & Java /check 420There is a weak correlation between mutation score and real fault detection. Despite the
weak correlations, fault detection is significantly improved at the highest score levels.
Finally,Frankl etal.[14],experimentedwithsomesmallmethod-
units (in Pascal and Fortran) and found that mutants provide good
guidancetowardsimproving testsuites,evenwhentestsuitesize
is controlled. Similarly, Chekam et al.[8] used real faults from
four real-world C projects and demonstrated that there is a strong
connection between mutation score attainment and fault detection
only at higher score levels.
Overall, despite the results found in the literature, our under-
standingoftherelationshipbetweenthetestsuitesize,mutation
score, and real fault detection remains limited as none of the stud-
iesinvestigatesthemwithalargesetofrealfaultsandreal-world
programs. Moreover, no previous study investigates whether there
arepracticaldifferencesbetweenthecorrelationanalysisandthe
faultdetectionrateoftestsuiteswhenusedasevaluationmetrics
of software testing experiments.
2.2 Mutants and Hand-Seeded Faults
Due to lack of real faults, many studies used hand-seeded faults to
simulate test effectiveness (i.e., fault detection capability).
WongandMathur[ 43]demonstratethatmutationtestinghas
higher fault detection potential than data-flow. Offutt et al.[30]
alsofoundthatmutationismoreeffectivethandata-flow(mutation
detected on average 16% more faults than the data flow) but at a
higher cost (measured as the number of test cases).
Liet al.[26] experimented with coverage criteria and compared
them with mutation testing, in terms of the number of faults de-
tected.Theirresultsshowedthatmutationtestingdetectedmore
faultsthanthecoveragecriteria.Interestingly,thesamestudyre-
portsthatmutationtestingrequiredlesstestcasesthanthecoverage
criteria.This issomehowcontradictoryto theresultsreported by
other studies, i.e., Wong and Mathur [ 43] and Offutt et al.[30],
which found that mutation requires more tests.All these studies compared criteria-adequate test suites (test
suitesfulfillingalltherequirementspossessedbythecriteria,i.e.,
killing all the mutants) and thus, their objectives were purely eval-
uating the effectiveness of the test criteria. Howev er, while such
anexperimentaldesignprovidessomeinsightsregardingthetest
criteria,itmakesthetesteffectivenessconclusionobscureasitis
unclearwhethertestsuitesarebetterduetotheinherentproperties
of the criteria or due to their sizes.
Othermutation-relatedstudiesthatmeasuretesteffectiveness
through hand-seeded faults are also some of those recorded on Ta-
ble1(studies[ 3,28,37]).Andrews etal.[3]reportthathand-seeded
faultsweremuchhardertodetectthanrealfaults,Papadakisand
Malevris report on the application cost and fault detection capa-
bilitiesofmutationtestingstrategies[ 37]andNaminandKakarla
reportontheinfluenceoftestsuitesize,andprogramminglanguage
on test effectiveness [28].
2.3 Mutants and Defect Prediction
Recentlyresearchersstartedusingmutationasanindicatoroferror-
proneness. Thus, they try to predict where (code components) the
defects of the tested system are. Bowes et al.[6] demonstrated
that using mutation score as a feature the accuracy of the defect
prediction methods is improving.
Tengeriet al.[41] investigated whether statement coverage, mu-
tationscore,andreducibility(theamountofredundanttestcases
in the test suites, w.r.t. coverage and mutation) are good predictors
of the expected number of remaining defects (confirmed defects,
normalized by the system size). Their results show that coverage is
notagoodindicator,andthatmutationandreducibilityimprove
thepredictions.Alongthesamelines,Ahmed etal.[1]measured
thecorrelationbetweencoverage,mutationscoreandsubsequent
bug-fix commits and found positive but weak correlations.
539
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae
2.4 Test Suite Size and Test Effectiveness
Early research on software testing showed that test suite size is
an important parameter influencing test effectiveness [ 12,13]. The
studiesofFrankl etal.[12–14]consistentlyreportthatapositive
relation between coverage attainment and fault detection exist,
whentestsuitesizeiscontrolled.Thisisreportedasnon-linearand
mainlyappearsatthehighestcoveragelevels.Similarly,Chekam et
al.[8] reports a positive relation but with insignificant, in practical
terms, improvements (for coverage when test suite size is fixed).
As these results investigated whether coverage provides enough
guidance for improving test effectiveness, these findings do not
concern the mutants and their representativeness (of real faults) in
software testing experiments.
Namin and Andrews [ 27] investigated the role and influence of
testsuitesizeandcoverageontesteffectiveness,usingmutantsand
real faults (13 faults for a single program) and concluded that both
coverage and test suite size independently influence test effective-
ness. Gligoric et al.[18] investigated the correlations between test
suitesizeandmutationscore andreportmixedresults,i.e.,strong
correlations for some programs, and weak for others.
Gopinath etal.[19]usedregressionanalysistomodeltherela-
tionbetweencoverageandmutationscoreandreportthattestsuite
sizedidnotimprovetheregression(indicatingthattestsuitesize
does notcontribute to explaining thevariability). On thecontrary,
InozemtsevaandHolmes[ 21]reportstrongcorrelationsbetween
coverage and mutation score when ignoring test suite size, but
weakcorrelationswhentestsuitesizeiscontrolled.Therefore,con-
cludingthattestsuitesizeisamongthemostimportantparameters
that influence mutationscore.Recently, Gay [ 17] investigated the
faultdetectioncapability(using353realfaults)ofdifferentconfigu-rations of a search-based test generation tool (EvoSuite) and report
that the test suite size had minor influence.
ThestudiesofAndrews etal.[3]andNaminandKakarla[ 28]also
controlfortestsuitesizebutfoundcontradictoryresults(related
to the underlying relation of mutation score and fault detection).
Additionally, these two studies considered a single C program and
a relatively small set of faults. In contrast, our study considers a
largenumberofrealfaultsfromlargereal-worldprograms,written
inbothCandJava,andcomparesresultsrelatedtothetwomain
experimental evaluation metrics that are used in the literature.
Overall, from the above discussion, it should be obvious that
thereismuchofcontroversyonthefindingofpreviousstudiesand
apoorunderstandingoftherelationbetweentestsuitesizeandreal
faultdetection.Weexpectalargeempiricalstudycanhelpclearing
up some of this controversy.
3 EXPERIMENTAL PROCEDURE
3.1 Test Subjects
In our study, we use two sets of subjects, CoREBench [ 5] and
Defects4J [ 23]. We choose these subjects as they form instances
of relatively large projects that are accompanied by mature testsuites as well as many real faults. CoREBench consists of four Cprograms named “Coreutils”, “Findutils”, “Grep” and “Make”. De-
fects4JconsistsoffiveJavaprogramsnamed“JFreeChart”,“Closure”,
“Commons-Lang”, “Commons-Math” and “Joda-Time”.Table 2: The subject programs used in the experiments. For
each of them, their size in KLOC, the number of developer
(DeveloperTC)andautomaticallygeneratedtestcases(Gen-
erated TC), and number of considered faults are presented.
Program Description Size Developer TC Generated TC Faults
CoreutilsUtilities manipulating
files and text83 4,772 13,947 22
FindutilsSearch directories
utilities18 1,054 3,877 15
GrepRegular expression
utilities9 1,582 4,317 15
MakeSource code build
system35 528 163 18
JFreeChart A chart library 96 3,691 111,279 19
Closure Closure compiler 90 271,904 543,947 92
Commons-Lang Java(library) - utilities 22 3,921 167,797 30
Commons-Math Mathematics library 85 8,081 465,361 74
Joda-TimeDate and Time
utilities28 38,223 86,957 16
DetailsaboutthetestsubjectsarerecordedonTable2.BothC
and Java programs are of non-trivial size and all are accompanied
by numerous developer test suites. All of the subjects are open
source. The C subjects, “Coreutils”, “Findutils”, “Grep” and “Make”
are tested by invoking them through command line, while the Java
ones, “JFreeChart”, “Closure”, “Commons-Lang”, “Commons-Math”
and “Joda-Time”, using the JUnit framework.
3.2 Fault Datasets (Defects4J and CoREBench)
Our study investigates the representativeness of mutants when
theyareusedastesteffectivenessevaluationmetrics.We,therefore,
need sets of typical instances of real faults that can be reliably
reproduced by test cases. As we make a controlled experiment,
weneedprojectswithlargeandmaturetestsuitessothatwecan
adequately control and simulate our hypothesis.
Most of the previous studies rely on the programs from the
SoftwareInfrastructureRepository(SIR)[ 11,20],typicallyusingthe
programscomposingtheSiemensSuite, spaceandUnixutilities.
Many of these programs includes artificially seeded faults and,consequently, are less relevant to this study, simply because we
investigatetherepresentativenessofmutants(whichareartificially
seeded faults themselves).
The spaceprogram in SIR is a notable exception as it comes
with real faults. Yet, it is a single C program. Therefore, the degree
towhichanygeneralisation(tootherprogramsandlanguages)is
possible remainslimited. Forall these reasons,we consider SIRas
less relevant for our study, and instead opt for benchmarks with
multiple programs and real faults.
We, therefore, use two benchmarks with real faults, CoREBench
and Defects4J, that have been systematically mined and isolated
from the project source code repositories. These benchmarks have
alsobeenused(amongmanyotherstudies)bythemostrelevantmutation-based studies, i.e., Just et al.[
24] and Chekam et al.[8].
Therefore,ourresultscomplementthefindingsofthesestudiesand
can be compared and contrasted directly (whenever relevant).
540
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. Are Mutation Scores Correlated with Real Fault Detection? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
CoREBench[ 5]isacollectionof70faultinstancesofCprograms,
identifiedbytestcasesthatreproducereportedfaults(bywriting
test cases that reproduce the behaviour described in bug reports).
Toidentifythefaults4,000commitswereanalysedbyexercising
them with the validating test cases in order to identify the fault-
introducing and fault-fixing commits (test cases pass before theintroductionofthefault,failafterandpassagainafterthefixing
commit). Additional details about CoREBench can be found in the
paper [5] and its accompanying website1.
Defects4J[ 23]isacollectionof357faultinstances,inJavapro-
grams, identified by the developer test cases. The corresponding
faultyandfixedversionswereminedandmanuallyverifiedbyexer-cisingthemwiththedevelopertestcases.Thedifferences(betweenthefaultyandfixedversion)werealsomanuallyrefined(minimized
sothattheyonlyincludethefaults)byremovingunrelatedchanges
suchasprogramrefactoring.AdditionaldetailsaboutDefects4Jcan
be found in the paper [23] and its GitHub webpage2.
Duringourstudy,weverifiedthefaultyandfixedversionsusing
developerandautomaticallygeneratedtestsuites.Allfaultshave
beenisolatedinsingleprograminstancesandthus,similartothe
previous studies [ 8,24] we treat them as separate subjects. We
excluded the CoREBench faults with identifiers 57 and 58 (from
the Make program) due to technical issues, these versions failed to
compileintheexperimentalenvironment.Similarly,weused231
faultsfromDefects4Jfaultsastheremaining126requiredinfeasible
amount of execution time for the experiment, i.e., more than an
hourpertestsuite.Asouranalysisinvolves21testsuitesperstudied
fault, we were forced to adopt this rule in order to complete the
experiment with reasonable resources.
3.3 Test Suites
CoREBench involves approximately 58,131 developer test cases.
Thesetestsuiteswerelateraugmented(manually)bythestudies
ofBöhmeandRoychoudhury[ 5](whichadded70testcases)and
Chekametal.[8](whichadded96testcases)inordertoexercise
and detect the studied faults (designed by reproducing the burgs
reported in bug reports) multiple times. Defects4J [ 23] includes
approximately 553,477 developer tests.
As our experiment involves a uniform test suite selection and
statistical analysis, we need a large, diverse and independentlygenerated test cases. Therefore, we use the test pools created byChekamet al.[
8] (for the case of CoREBench) that are large, di-
verse and include multiple tests that exercise the faults in different
ways. These include the developer test suites, the test cases gener-
ated with KLEE [ 33], in total 22,208 tests, and manually generated
ones, 166 tests. Additional details about the test suites we used (for
CoREBench) can be found in the work of Chekam et al.[8].
Defects4J[ 23]onlyincludesthedevelopertests.Therefore,we
augmentthesetestsusingtwostate-of-the-arttestgenerationtools,
Randoop [ 32] and EvoSuite [ 15]. Randoop was used for generating
large numbers of random tests, whereas EvoSuite for generating
large numbers of tests that maximise the branch coverage, weak
mutation and strong mutation.
1http://www.comp.nus.edu.sg/~release/corebench/
2https://github.com/rjust/defects4jWeappliedthesetoolsfiveindependenttimeswitharelatively
robust time budget per class (300 seconds), and limit the maximum
numberofteststo2,000,perrun.Overall,theprocessresultedin20
test suites, composed of 1,375,341 test cases. Following the typical
process for these experiments [ 23], all tests were automatically
verifiedtoensurethattheydonotcauseanycompileerrors,runtime
errorsandnon-deterministicbehaviour,usingtheavailableutilities
provided by Defects4J [23].
3.4 Mutation Testing Tools
We employed the same tools as those used by the studies of Just
et al.[24] and Chekam et al.[8]. We choose these tools as they are
publiclyavailable,robustandcanproduceresultsthatarecompara-
bletotheexistingwork.Weusedthesetoolswiththesamesettings
as used in the initial studies.
Bothtoolssupportasetofcommonlyusedmutantoperators[ 25,
29],i.e.,theAOR(ArithmeticOperatorReplacement),LOR(Logical
Operator Replacement), COR (Conditional Operator Replacement),
ROR(RelationalOperatorReplacement),ORU(OperatorReplace-
ment Unary), STD (STatement Deletion), and LVR (Literal Value
Replacement).Additionaldetailsaboutthemutationtestingtools
can be found in Just [22] as well as Chekam et al.[8].
3.5 Evaluation Metrics
We study the use of two evaluation metrics that are frequently
employed by empirical studies. Such studies employ the following
high level process:
(1)Create a large number of test suites, either by sampling test
cases from a large pool of existing tests or by using a test
generationalgorithmortool,untilreachingapredetermined
number of tests or criteria score level.
(2)Measure the criteria score such as coverage or mutationscore(if testsuites aresize controlled)ofthe testsuitesor
their size (if test suites are score controlled).
(3)Measurethefaultdetectioncapabilityofthetestsuites,by
measuring either number or ratio of detected faults. Em-
piricalstudies usuallyemployisolatedfaulty versions(real
faults) or hand-seeded faults or automatically seeded faults
(mutants).
(4)Determine the test effectiveness based on one of the two
following methods: 1) correlation analysis between fault de-
tection ratios and criteria scores such as coverage or mutant
detection ratios ( correlation method ), or 2) fault detection
ratios at predefined score points such as the highest crite-
ria score levels achieved by the available test suites ( fault
detection method ).
Thecorrelationmethodisbecomingincreasinglypopularand
is used to judge the effectiveness of the test methods. It is also
frequently used to perform the effectiveness comparison of the
testtechniques,e.g.,adoptedbystudieswithreferences[ 18,21,44]
perform such effectiveness comparisons.
The fault detection method simply compares the fault detection
capabilities of test suites at a score level of interest (usually at
100% or close to 100% score levels), e.g., adopted by studies with
references [12, 26, 43].
541
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae
The two evaluation metrics, i.e., correlation coefficients (used
in the correlation method) and fault detection ratios (used in thefault detection method), are often being confused as being thesame. However, in practice they capture different aspects of the
test criteria and should be investigated distinctly. In the present
study, we demonstrate that mutants can correlate weakly withfault detection, but they can provide statistically and practically
significantfaultdetectionimprovementsoverrandomlyselected
test suites (or test suites with lower scores) when reaching higher
mutation score levels.
3.6 Data Analysis
Initially, we form our test pools by merging the automatically gen-
eratedwiththedevelopertestcases.We,then,sample10,000test
suites of random sizes in the range (0-20% of the size of the test
pool).Then,wecategorisethesesuitesas‘Failing’,i.e.,theyinvolve
at least one test case fails, and ‘Passing’, i.e., all test cases pass, and
plot their mutation scores and test suite sizes respectively. This
visualisation gives us an initial indication on the examined rela-
tions. We then perform regression analysis (Logistic regression)on these data by modelling the relationships between test suitesize, mutation score, combination of test suite size and mutation
score,andfaultdetection.Byinspectingtheresulting p–valuesand
pseudoR2values,weinvestigatewhichoftheseparametersplay
the most important role and measure the fit of the models.
Tofurtheranalyseourdataand replicatepreviousfindings,we
apply both the correlation and the fault detection methods. For the
correlation method, we use the Kendall and Pearson coefficients to
compute the correlation between Mutation Score (MS) and Fault
Detection (FD) on the 10,000suites of random sizes. The resulting
coefficientsshowtheassociationbetweenMSandFDwhenthetest
suitesizeisnotcontrolled.Forsizecontrolledresults,wesample
test suites of the same size (without replacement) for different test
suite size groups (increments of 2.5% in the range 0-50% of the test
pool size). Thus, we sample 10,000 test suites per size group and
computethecorrelations betweenMSandFD. Theseresultsshow
the correlations independent of test suite size.
Forthefaultdetectionmethod,weorderthetestsuites,forevery
selected size, according to their mutation score and create threesets of test suites: those that are ranked within top 25%, top 10%and all of them. We then perform a Chi-squared test, which is a
non-parametricproportiontest,tocomparethefaultdetectionof
the pairs (top 25% and all test suites, top 10% and all test suites)
and compute the confidence intervals for 95 percent. These values
representthefaultdetectionprobabilitiesofthetoprankedsuites
compared with the whole set of test suites (baseline).
4 RESULTS
4.1 Visualisations
Figure 1 shows the test suite size and mutation score values of the
failing and passing test suites. As can be seen from the boxplots,
the trend is that failing test suites are of larger sizes and, at the
sametime,achievehighermutationscoresthanthepassingones.
This indicates that both variables may influence the fault detection
ability of the test suites.To further explore the interconnection between size and mu-
tation score, we investigate the relation between test suite size
and mutation score. Due to space constraints, we do not show this
plot,however,theresultsclearlyindicatealogarithmicrelation,i.e.,
y=axbwhere(s<b<1).Therefore,weexpectthattestsuitesize
andmutationscoreareinterconnectedinsuchawaythathigher
(or lower) mutation score levels require increased (or decreased)
number of test cases. This leads to the question of whether test
suite size or mutation score explain the test effectiveness.
4.2 Regression Models
Togetaninsightontherelationbetweentestsuitesizeandfault
detection, we apply regression analysis. As our data involve a con-
tinuousindependentvariable( MS:mutationscore),adiscretein-
dependentvariable( Size:testsuitesize),andabinarydependent
variable ( FD: fault detection) we apply a logistic regression. We
examinedvariousmodelsbetweenthesevariables,i.e., Size∗MS,
MS,Size,anddeterminedthe p–valuesindicatingtheirsignificance.
The output of the regression indicates that all the examined
variablesassociatetofaultdetection(testsuitesize,mutationscore,
and their combination).They aresignificantly associatedwith the
probability of detecting faults ( p–values <0.05). Therefore, the
resultssuggestthatallexaminedvariablesindependentlyinfluence
test effectiveness.
To evaluate the fit of the regression model we calculated the
pseudoR2values of the models. Figure 2 present the resulting
pseudoR2values. These data indicate that both the models of test
suite size and mutation scores (data with labels Size and MS) are
similar. Size models have a higher predictive power (and less vari-
ation) than the mutation score models in Defects4J while lower
in CoreBench. The combined model ( Size∗MS) outperforms both
Size and MS models, in terms of their predictive power. The differ-
ences are statistically significant (determined by using a Wilcoxon
signed-rank test, and significance level 0.001) suggesting that both
Size, MS and their combination contributes to test effectiveness.
However, the majority of the predictions is of moderate strengths.
One firstfinding is that thefault detection cannot bepredicted
well by mutation scores, size or their combination.
4.3 Correlation Analysis with Test Suite Size
Controlled/Uncontrolled
Toinvestigatetheextentofconfoundingeffectsoftestsuitesize,we
performacorrelationanalysisbycontrolling(holdingtestsuitesize
constant)andnotcontrolling(selectingtestsuitesofarbitrarysizes)
fortestsuitessize.Thisisatypicalprocess(alsofollowedbyexisting
work[8,21,28])thateliminatestheinfluenceoftheindependent
variable (size) on the observed effect and helps determine whether
mutation score is associated with fault detection (and the strength
of this association).
Our analysis is based on the Kendall and Pearson correlation
coefficients. We measure thecorrelations forvarious constant test
suite sizes, sizes in the range 2.5%-50%, of the test pools, incre-mented by 2.5% and for arbitrary selected test suites of the same
size: Figure 3 shows the results. The correlations on the left side of
the figure regard the uncontrolled size case while on the right side
of the figure the controlled one.
542
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. Are Mutation Scores Correlated with Real Fault Detection? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Chart
Closure
Lang
Math
Time
CoreUtils
FindUtils
Grep
MakeChart
Closure
Lang
Math
Time
CoreUtils
FindUtils
Grep
Make0.00.20.40.60.81.0Normalized Mutation ScorePassing Suites Failling Suites
(a) Mutation Score Vs Fault Detection.
Chart
Closure
Lang
Math
Time
CoreUtils
FindUtils
Grep
Make
Chart
Closure
Lang
Math
Time
CoreUtils
FindUtils
Grep
Make0.00.20.40.60.81.0Normalized Test Suite SizePassing Suites Failling Suites
(b) Test suite size Vs Fault Detection.
Figure 1: Mutation Score and Test suite size of the Passing and Failing Test Suites. Failing Test Suites have higher mutation
scores and suite sizes than the Passing ones.
MS Size Size*MS MS Size Size*MS0.0 0.4 0.8Defects4J CoreBench
Figure 2: Regression Models ( R2values) for Mutation Score
(MS), Test Suite Size (Size) and their Combination (Size*MS).
Allp–values are significant.
Theresultsoftheuncontrolledsizecaseshowmoderatetostrong
correlations (the majority of the values is within the range 0.35 to
0.75). These correlations become relatively weak (approximately
within the range 0.05 to 0.20) when the suite size is controlled (i.e.,
theinfluenceofsizedisappears).Theseresultssuggestthatamajor
part of the association between mutation score and fault detection
is simply an effect of size. Still a positive relation between them
exists, but it is relatively weak.
Interestingly, our results are fairly consistent across different
programsandthedifferentprogramminglanguages(JavaandC)
we used. This adds to the evidence that the results we observe are
valid. They alsosuggest that mutants havea consistent behaviour
among the subjects we used.
Finally, it is worth noting that our results (regarding the uncon-
trolledtestsizecase)areconsistentwiththosereportedbythework
ofJustetal.[24],which didnotconsiderthe testsizeeffects. This
factprovidesconfidenceonouranalysis. However, i ncontrastto
the conclusion of Just et al.[24], our results demonstrate the effect
of size and overall that mutants are not strongly correlated with
fault detection, when size is controlled.4.4 Fault Detection Probabilities
Mutationtestingisknownforitstesteffectiveness[ 8,16,31],while
our analysis shows that mutation score is not (strongly) correlated
with fault detection. Does this mean that mutation testing is signif-
icantlyoverrated?Whilethisisapossibility,anotherexplanation
could be that mutation testing is helpful at guiding testers to im-
provethefaultdetectioncapabilitiesoftestsuites,whileitisnot
thatgoodatrepresentingthebehaviouroftherealfaults.Itcould
be that the relation is non-linear and improves only at the highest
mutation score levels as suggested by some previous studies [12].
To investigate this, for every selected size of test suites, we
compute the fault detection probabilities (using the Chi-squaredtest) of the test suites with higher mutation scores and compare
them with the fault detection probabilities of all randomly selected
test suites of the same size. In this case, any differences we may
observe(onthefaultdetectionprobabilities)wouldbeattributed
to the mutation score differences and as we keep the test suite size
constant they are independent of the size effects.
SuchaprocesswasfollowedbytherecentstudyofFrankland
Iakounenko [ 12], and reports that test suites with higher coverage
scores reveal a significantly higher number of faults. As in our
case, correlations are weak, what happens to the fault detection at
highest mutation score levels? We hypothesise that fault detection
probability can be important given the results reported by the
studiesofChekam etal.[8]andFranklwithhercolleagues[ 12–14]
thatreportsasignificantfaultdetectiononlyatthehighestscore
levels (while not much of difference at the majority of the scores).
Figure4showstheimprovementonthefaultdetectionprobabili-
tiesbetweenthe10%oftestsuites(withthehighestmutationscores)andrandomlyselectedtestsuitesofthesamesize.Thepointsintheplotrepresentthestatisticallysignificantimprovements(at0.05sig-
nificance level) on the probabilities and their confidence intervals
(for95percentconfidence level)computedusingtheChi-squared
test.TheDefects4Jfaultsthathavestatisticallysignificantimprove-
mentsare138,whiletheCoreBenchare59.Thissuggeststhatwe
have no evidence to support the claim that there is a significant
improvement for the 40% (93 out of 231) and 13% (9 out of 68) of
the Defects4J and CoreBench faults.
543
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae
Chart
Closure
Lang
Math
Time
CoreUtils
FindUtils
Grep
Make
Chart
Closure
Lang
Math
Time
CoreUtils
FindUtils
Grep
Make0.00.20.40.60.81.0Pearson CorelationSize Uncontrolled Size Controlled
Chart
Closure
Lang
Math
Time
CoreUtils
FindUtils
Grep
MakeChart
Closure
Lang
Math
Time
CoreUtils
FindUtils
Grep
Make0.00.20.40.60.81.0Kendall CorelationSize Uncontrolled Size Controlled
Figure 3: Correlation between mutation score and fault detection. Correlations are relatively strong when test suite size is
uncontrolled but drop significantly when test suite size is controlled.
Interestingly,despitetheweakcorrelations,thefaultdetection
improvements (for the statistically significant cases) can be con-sidered of practical significance as the majority of the points areapproximately 10% for Defects4J and above 30% for CoreBench.
Specifically,theaverageimprovementsonfaultdetectionforthe
top ranked 25% and 10% of the test suites are 8% and 11% for De-
fects4J and 18% and 46% for CoreBench. These results indicate that
mutation is indeed strong and can successfully guide testers to-
wards improving their test suites. Unfortunately, as also indicated
by the related work, testers have to reach a relatively high score
level before they can be confident on their testing [8].
Our results reveal that the correlation method and the fault
detection method capture different aspects of test effectiveness.
This implies that future empirical studies that use these evaluation
methodsshouldinterpretthemaccordingly.Wediscussthisissue
in detail in Section 5.2.
5 DISCUSSION
Ourresultsshowthatthecorrelationsofmutationscoreandreal
faults are significant, but notably weaker than reported in the liter-
ature and assumed by empirical studies. This finding is in essence
negative as mutants seem to be relatively unreliable substitutesofrealfaults(asameanstosupportcontrolledexperiments).De-
spite this, we found significant improvements in fault detection (of
test suites at the highest score levels) suggesting that mutants can
provide valuable guidance and establish confidence.
Intheremainderofthissection,wetrytoshedsomelightonthe
mutant-fault relation, by investigating the behavioural similarities
ofmutantsandrealfaults.Wethendiscusspossibleimplications
ofthepracticaldifferencesofusingcorrelationanalysisandfault
detection methods in software testing experiments.
5.1 Behavioural Similarity between Mutants
and Real Faults
Are mutants a valid substitute of real faults in controlled experi-
ments? If we hypothesise that they are, then we should expect that
the majority ofthe mutants behaves similarly tothe real faults. In
a sense, we expect that when faults are detected, the majority of
the mutants are killed, and when not, the majority remain live.Toinvestigatethis,wemeasurethebehavioursimilarityofevery
singlemutantandtherespectivefaults.Todoso,weuseatypical
similarity coefficient, named Ochiai3, which we borrow from fault
localisation studies [ 42]). Ochiai takes values in the range [0 ,1]
indicatingthesimilaritylevel,with0indicatingcompletelydifferent
and 1 exactly the same.
Figure5recordsthemaximumsimilarityandthesimilarityof
all the mutants for every considered faults. As can be seen from
themaximumsimilarityvalues,themajorityofthefaultsissimu-
lated very well by at least one mutant, the one with the maximum
similarity (from the left box-plot we can observe that 50% of the
datahavesimilaritiesofatleast90%),whilethegreatmajorityof
the mutants behave differently. This implies that some mutantssimulate well the behaviour of the faults, while the majority ofthem do not (we measure approximately 1% of all mutants have
behaviour similarities above 0.5).
Overall,ourresultsrevealthatirrelevantmutantscausetheweak
correlations. We, therefore, believe, that future research should
focus on identifying mutants that are linked with the faults.
5.2 Correlations vs. Fault Detection
To investigate the practical differences between the evaluation
methods, i.e., the correlation and the actual fault detection, we
measuretheassociationbetweenthetwoevaluationmetrics,i.e.,
correlationcoefficientsandfaultdetectionimprovement.Informally,
wecheckwhetherhigh/lowcorrelationcoefficientsimplyhigh/low
faultdetectionimprovement.Astrongassociationwillindicatethat
one method implies the other, while a weak association that the
two methods capture different aspects of interest.
For every fault in our dataset, we measure a) the correlation
coefficients(betweenmutationscoreandfaultdetection)andb)the
improvementon faultdetection(approximatedbythe faultdetec-
tion rate differences of the top ranked test suites and all selected
test suites of the same size). Our raw data, when using the Pearson
correlationandfaultdetection improvementsatthe top10%,arede-
pictedonFigure6.Tofurtherinvestigatetheassociationbetweena)and b), we used Kendall and Pearson correlation on the data where
we had statistically significant fault detection improvements.
3TheOchiaicoefficientmeasuresthesimilaritybetweentwovariablesgivenasetof
observations, it is equivalent to the cosine similarity.
544
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. Are Mutation Scores Correlated with Real Fault Detection? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
/g3 /g7/g3 /g4/g3/g3 /g4/g7/g3 /g5/g3/g3/g239/g3/g2/g5 /g3/g2/g5 /g3/g2/g8 /g4/g2/g3
(a) Defects4J/g3/g4 /g3 /g5 /g3 /g6/g3/g7/g3/g8/g3/g9/g3/g10/g3/g239/g3/g2/g5 /g3/g2/g5 /g3/g2 /g9/g4/g2/g3
(b) CoreBench
Figure4:Improvementonthefaultdetectionprobabilities(intervals)withtestsuitesizecontrolled.Thevaluesrepresentthe
difference on the fault detection probabilities between the test suites with the highest mutation score and randomly selected
ones (top 10% of test suites Vs randomly selected). We observe that mutants lead to significant fault detection improvements.
Max All Max All0.0 0.4 0.8Defects4J CoreBench
Figure 5: Behaviour Similarity between Mutants and Real
Faults.Mostofthefaults’behaviourissimulatedwellbythe
(few)mutantswithmaximumsimilarity,butnotbythema-
jority of the mutants (similarity of all mutants).
Our results show a moderate to strong association between the
two metrics. In particular, the association between the correlation
coefficients, Kendall andPearson (usedto measurethe correlation
between mutation score and fault detection), with the fault de-tection improvement of the top 25% of test suites was 0.621 and
0.504whenusingtheKendall’srankcorrelationcoefficient τand
0.860 and 0.732 when using the Pearson correlation (for Defects4J
and CoreBench respectively). The Kendall’s rank correlations were
foundtobe0.471and0.500andthePearsoncorrelationswerefoundto be 0.673 and 0.652, respectively for Defects4J and CoreBench. In-
terestingly, by comparing the correlations of the test suites scoring
at the top 25% and 10%, we observe that the disagreement between
the two metrics is increasing when moving at higher score levels.
Overall, since the association is moderate, we can conclude that
correlationsaregoodatprovidingtrends,butatthesametimethey
do not capture the actual improvements at the specific points ofinterest (such as the fault detection improvement at the highest
score levels).6 THREATS TO VALIDITY
The most important question in empirical studies is the extent
towhichtheirfindingsgeneralise.Tocaterforthisissue,weper-
formedthelargestempiricalstudytodate,usingtwofaultsetsfrom
independentstudies,writteninCandJava,andfoundconsistent
results across all subjects. However, we acknowledge that sincethese programs are open source and the faults were mined from
source code repositories, our results might not be representative of
actualindustrialsystemsandother“pre-release”faults(faultsnot
recorded in the repositories).
Anotherthreatmaybeduetotheincompletenessofthefaultsin
the benchmarkswe study. Asthe faultsets are notexhaustive, we
canonlyassumethatsomecodepartsarefaulty.Thismeansthat
we cannot assume that the rest of the code is not faulty. Thus, our
resultsmightbesubjecttonoisecausedbymutantsresidingoncode
parts that are irrelevant, to the studied faults, but are relevant to
other ‘unknown’ faults. To diminish this threat, we performed our
analysis using only relevant, to the studied faults, test cases (tests
with dependence to the faulty component). To further check our
results, we also examined a sample of 10 faults from the CoreUtils
by considering only the mutants that are residing on the faulty
and directly dependent (control data dependencies) statements, to
thefaultystatements,andfoundverysimilar(weak)correlations.
Overall,weareconfidentthattheaboveissuesarenotofparticular
importance since we replicate (to some extent) previous studies
and find consistent trends among all the considered subjects.
Threats may arise from the automatically augmented test suites
weused.Whiletheaugmentedtestsuitesmaynotberepresentative
of testsgenerated entirelyby human engineers,the augmentation
wasnecessarysothatwecouldsamplemultiple,size-controlledtest
suites from a large, strong, and diverse test pool. To mitigate the
threatfromaugmentation,werepeatedourexperimentswithout
theaugmentation(i.e.,usingeitheronlythedevelopertestsuitesor
only the automatically generated test suites) and found exactly the
same trends (with slightly weaker correlations and fault detection
improvements).
545
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae
/g3/g2/g3 /g3/g2 /g4 /g3/g2/g5 /g3/g2/g6 /g3/g2/g7/g239/g3/g2/g5/g3/g2/g3 /g3/g2 /g5/g3/g2/g7
(a) Defects4J/g3/g2/g3 /g3/g2 /g4 /g3/g2/g5 /g3/g2/g6 /g3/g2/g7/g239/g3/g2/g5/g3/g2/g3 /g3/g2 /g5/g3/g2/g7
(b) CoreBench
Figure 6: Pearson correlation coefficients Vs fault detection improvement.
Similarly, due to technical difficulties, we did not analyse 126
faults from Defect4J. To reduce this threat, we measured the cor-relationswhenusing(only)thedevelopertestsuitesforallbut5
faults4of the benchmark and found similar results.
Otherthreatsmightbeattributedtothewaywehandledequiva-
lentmutants,whichwasbasedonthecomposedtestpools.Though,
weusedstateofthearttestgenerationtoolsandoverallachieveda
relatively high mutation score. Unfortunately, there are clear limitstoanypracticalidentificationofequivalentmutants,astheproblem
itself is undecidable. While we simply admit that this is a potential
threattoourstudy,wewouldarguethat,inpractice,thereisnot
much else that can be done (at least by today’s standards).
There may be threats due to the tools we used. Though, these
toolsweredevelopedbyindependentstudies[ 8,22].Tocaterfor
this threat, we also used Pit [ 9], a Java mutation testing tool that is
quite popular in mutation testing studies [ 25,36]. We repeated our
analysiswithPitusingtwotestsuitesofRandoopandEvoSuiteand
foundsimilarresults,i.e.,thecorrelationsandfaultdetectionim-
provements did not differ statistically. Therefore, we are confident
that our results generalise to Pit as well.
Followingthelinesofpreviouswork[ 4,22,28],weappliedall
ofouranalysisonthefixedprogramversionversion.Althoughthis
is a common practice, our results might not hold on the cases of
faultyprogramversions[ 8].Though,wewereforcedtodoso,in
order to replicate the previous studies. However, we believe thatthis threat is not of actual importance as we are concerned with
mutation testing, which according had a small variation on mutant
couplings of the fixed the faulty programs [8].
Finally, we used Kendall and Pearson correlation coefficients
to measure the association between the studied variables, whilethe work of Just et al.[
24] used the Biserial correlation. Unfortu-
nately, Biserial correlation assumes equal variance between the
instances of the dichotomous variable under analysis, which we
foundinapplicableinourcaseasthedifficultyofdetectingfaults
differs. Nevertheless, we also computed the Biserial correlations
andfoundthesameresults(thedifferencesweresmallerthan0.001).
We therefore do not consider this as a critical threat.
45 faults were not considered as they required exceedingly long time to complete
(more than an hour per test case).7 CONCLUSIONS
Our main conclusion is that both test suite size and mutation score
influence fault detection. Our results support the claim that rela-
tively strong correlations between mutation score and fault detec-
tion exist, as suggested by previous work [ 24]. However, these are
simplyaproductofthedependencebetweenmutationscoreand
testsuitesize.Ourdatashowthatwhencontrollingthetestsuite
size(decouplemutationscore fromtestsuitesize)allcorrelations
become weak or moderate in the best case.
In practice, our results suggest that using mutants as substi-
tutes of real faults (as performed in most of the software testing
experiments)canbeproblematic.Despitetheweakcorrelationswe
observe, our results also show that the fault detection improvessignificantly when test suites reach the highest mutation score
levels. Taken together, our findings suggest that mutation score is
actually helpful, to testers for improving test suites (by reachingrelative high levels of mutation scores), but it is not that good at
representing the actual test effectiveness (real fault detection).
A potential explanation of this point concerns the “nature” of
mutants and the real faults. Mutants are generated following asystematic procedure, while real faults are the specific instancesthat escape programmer’s attention. Thus, mutants represent a
large number and variety of possible faulty instances, while faults
are by nature few and irrelevant to the majority of the mutants.
Wealsofoundthatsomemutantsareindeedcapableofrepresent-
ing the behaviour of real faults for most of our subjects. However,
as the majority of the mutants involved have no representative
behaviour,themutationscoresareinflatedandthecorrelationsare
weak.Thisfindingsuggeststhatfutureresearchshouldfocuson
generating more representative sets of mutants.
ACKNOWLEDGEMENTS
This researchwas supported bythe Next-Generation Information
ComputingDevelopmentProgramoftheKoreanNationalResearch
Foundation (NRF), funded by the Ministry of Science, ICT (No.
2017M3C4A7068179), and the Institute for Information & commu-
nications Technology Promotion (IITP) grant, funded by the Korea
government (MSIP) (No.R0126-18-1101, (SW Star Lab) SoftwareR&D for Model-based Analysis and Verification of Higher-order
Large Complex System).
546
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. Are Mutation Scores Correlated with Real Fault Detection? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]IftekharAhmed,RahulGopinath,CaiusBrindescu,AlexGroce,andCarlosJensen.
2016. Cantestednessbeeffectivelymeasured?.In Proceedingsofthe24thACM
SIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering,FSE
2016,Seattle,WA,USA,November13-18,2016 .547–558. https://doi.org/10.1145/
2950290.2950324
[2] Paul Ammann and Jeff Offutt. 2008. Introduction to software testing . Cambridge
University Press.
[3]James H. Andrews, Lionel C. Briand, and Yvan Labiche. 2005. Is mutation an
appropriate tool for testing experiments?. In 27th International Conference on
SoftwareEngineering(ICSE2005),15-21May2005,St.Louis,Missouri,USA .402–411.
https://doi.org/10.1145/1062455.1062530
[4]James H. Andrews, Lionel C. Briand, Yvan Labiche, and Akbar Siami Namin.
2006. Using Mutation Analysis for Assessing and Comparing Testing Coverage
Criteria.IEEE Trans. Software Eng. 32, 8 (2006), 608–624. https://doi.org/10.1109/
TSE.2006.83
[5]MarcelBöhmeandAbhikRoychoudhury.2014. CoREBench:studyingcomplexityofregressionerrors.In InternationalSymposiumonSoftwareTestingandAnalysis,
ISSTA’14,SanJose,CA,USA-July21-26,2014 .105–115. https://doi.org/10.1145/
2610384.2628058
[6]David Bowes, Tracy Hall, Mark Harman, Yue Jia, Federica Sarro, and Fan Wu.2016. Mutation-awarefaultprediction.In Proceedingsofthe25thInternational
Symposium on Software Testing and Analysis, ISSTA 2016, Saarbrücken, Germany,
July 18-20, 2016 . 330–341. https://doi.org/10.1145/2931037.2931039
[7]CristianCadar,DanielDunbar,andDawsonR.Engler.2008.KLEE:Unassistedand
Automatic Generation of High-Coverage Tests for Complex Systems Programs.
In8th USENIX Symposium on Operating Systems Design and Implementation,
OSDI 2008, December 8-10, 2008, San Diego, California, USA, Proceedings . 209–224.
http://www.usenix.org/events/osdi08/tech/full_papers/cadar/cadar.pdf
[8]Thierry Titcheu Chekam, Mike Papadakis, Yves Le Traon, and Mark Harman.2017. An empirical study on mutation, statement and branch coverage fault
revelationthatavoidstheunreliablecleanprogramassumption.In Proceedings
ofthe39thInternationalConferenceonSoftwareEngineering,ICSE2017,Buenos
Aires, Argentina, May 20-28, 2017 . 597–608. https://doi.org/10.1109/ICSE.2017.61
[9]Henry Coles, Thomas Laurent, Christopher Henard, Mike Papadakis, and An-
thonyVentresque.2016. PIT:apracticalmutationtestingtoolforJava(demo).
InProceedings of the 25th International Symposium on Software Testing and
Analysis, ISSTA 2016, Saarbrücken, Germany, July 18-20, 2016 . 449–452. https:
//doi.org/10.1145/2931037.2948707
[10]Muriel Daran and Pascale Thévenod-Fosse. 1996. Software Error Analysis: A
Real Case Study Involving Real Faults and Mutations. In Proceedings of the 1996
International Symposium on Software Testing and Analysis, ISSTA 1996, San Diego,
CA, USA, January 8-10, 1996 . 158–171. https://doi.org/10.1145/229000.226313
[11]Hyunsook Do, Sebastian G. Elbaum, and Gregg Rothermel. 2005. Supporting
Controlled Experimentation with Testing Techniques: An Infrastructure and its
Potential Impact. Empirical Software Engineering 10, 4 (2005), 405–435. https:
//doi.org/10.1007/s10664-005-3861-2
[12]PhyllisG.FranklandOlegIakounenko.1998. FurtherEmpiricalStudiesofTest
Effectiveness. In SIGSOFT ’98, Proceedings of the ACM SIGSOFT International
Symposium on Foundations of Software Engineering, Lake Buena Vista, Florida,
USA, November 3-5, 1998 . 153–162. https://doi.org/10.1145/288195.288298
[13]PhyllisG.FranklandStewartN.Weiss.1993. AnExperimentalComparisonof
the Effectiveness of Branch Testing and Data Flow Testing. IEEE Trans. Software
Eng.19, 8 (1993), 774–787. https://doi.org/10.1109/32.238581
[14]Phyllis G. Frankl, Stewart N. Weiss, and Cang Hu. 1997. All-uses vs mutation
testing: An experimental comparison of effectiveness. Journal of Systems and
Software38, 3 (1997), 235–253. https://doi.org/10.1016/S0164-1212(96)00154-9
[15]GordonFraserandAndreaArcuri.2011. EvoSuite:automatictestsuitegeneration
for object-oriented software. In SIGSOFT/FSE’11 19th ACM SIGSOFT Symposium
on the Foundations of Software Engineering (FSE-19) and ESEC’11: 13th European
Software Engineering Conference (ESEC-13), Szeged, Hungary, September 5-9, 2011 .
416–419. https://doi.org/10.1145/2025113.2025179
[16]GordonFraserandAndreasZeller.2012. Mutation-DrivenGenerationofUnit
Tests and Oracles. IEEE Trans. Software Eng. 38, 2 (2012), 278–292. https://doi.
org/10.1109/TSE.2011.93
[17]GregoryGay.2017. TheFitnessFunctionfortheJob:Search-BasedGeneration
ofTestSuitesThatDetectRealFaults.In 2017IEEEInternationalConferenceon
SoftwareTesting,VerificationandValidation,ICST2017,Tokyo,Japan,March13-17,
2017. 345–355. https://doi.org/10.1109/ICST.2017.38
[18]MilosGligoric,AlexGroce,ChaoqiangZhang,RohanSharma,MohammadAmin
Alipour, and Darko Marinov. 2015. Guidelines for Coverage-Based Comparisons
of Non-Adequate Test Suites. ACM Trans. Softw. Eng. Methodol. 24, 4 (2015),
22:1–22:33. https://doi.org/10.1145/2660767
[19]RahulGopinath,CarlosJensen,andAlexGroce.2014. Codecoverageforsuite
evaluationbydevelopers.In 36thInternationalConferenceonSoftwareEngineering,
ICSE ’14, Hyderabad, India - May 31 - June 07, 2014 . 72–82. https://doi.org/10.
1145/2568225.2568278[20]MonicaHutchins,HerbertFoster,TarakGoradia,andThomasJ.Ostrand.1994.
ExperimentsoftheEffectivenessofDataflow-andControlflow-BasedTestAd-
equacyCriteria.In Proceedingsofthe16thInternationalConferenceonSoftware
Engineering . 191–200. http://portal.acm.org/citation.cfm?id=257734.257766
[21]Laura Inozemtseva and Reid Holmes. 2014. Coverage is not strongly corre-lated with test suite effectiveness. In 36th International Conference on Soft-
wareEngineering,ICSE’14,Hyderabad,India-May31-June07,2014 .435–445.
https://doi.org/10.1145/2568225.2568271
[22]René Just. 2014. The major mutation framework: efficient and scalable mutation
analysisforJava.In InternationalSymposiumonSoftwareTestingandAnalysis,
ISSTA’14,SanJose,CA,USA-July21-26,2014 .433–436. https://doi.org/10.1145/
2610384.2628053
[23]RenéJust,DarioushJalali,andMichaelD.Ernst.2014. Defects4J:adatabaseof
existing faults to enable controlled testing studies for Java programs. In Interna-
tionalSymposiumonSoftwareTestingandAnalysis,ISSTA’14,SanJose,CA,USA-
July 21 - 26, 2014 . 437–440. https://doi.org/10.1145/2610384.2628055
[24]René Just, Darioush Jalali, Laura Inozemtseva, Michael D. Ernst, Reid Holmes,
andGordonFraser.2014. Aremutantsavalidsubstituteforrealfaultsinsoftware
testing?. In Proceedings of the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering, (FSE-22),Hong Kong, China, November 16 -
22, 2014. 654–665. https://doi.org/10.1145/2635868.2635929
[25]Marinos Kintis, Mike Papadakis, Andreas Papadopoulos, Evangelos Valvis, Nicos
Malevris, and Yves Le Traon. 2017. How effective are mutation testing tools? An
empiricalanalysisofJavamutationtestingtoolswithmanualanalysisandreal
faults.Empirical Software Engineering (21 Dec 2017). https://doi.org/10.1007/
s10664-017-9582-5
[26]Nan Li, Upsorn Praphamontripong, and Jeff Offutt. 2009. An Experimental
Comparison of Four Unit Test Criteria: Mutation, Edge-Pair, All-Uses and Prime
Path Coverage. In Mutation 2009, Denver, Colorado, USA . 220–229. https://doi.
org/10.1109/ICSTW.2009.30
[27]Akbar Siami Namin and James H. Andrews. 2009. The influence of size and
coverageontestsuite effectiveness.In Proceedingsof theEighteenthInternational
Symposium on Software Testing and Analysis, ISSTA 2009, Chicago, IL, USA, July
19-23, 2009 . 57–68. https://doi.org/10.1145/1572272.1572280
[28]AkbarSiamiNaminandSahityaKakarla.2011. Theuseofmutationintesting
experiments and its sensitivity to external threats. In Proceedings of the 20th
InternationalSymposiumonSoftwareTestingandAnalysis,ISSTA2011,Toronto,
ON,Canada,July17-21,2011 .342–352. https://doi.org/10.1145/2001420.2001461
[29]A.JeffersonOffutt,AmmeiLee,GreggRothermel,RolandH.Untch,andChristianZapf.1996. AnExperimentalDeterminationofSufficientMutantOperators. ACM
Trans.Softw.Eng.Methodol. 5,2(1996),99–118. https://doi.org/10.1145/227607.
227610
[30]A. Jefferson Offutt, Jie Pan, Kanupriya Tewary, and Tong Zhang. 1996. An
Experimental Evaluation of Data Flow and Mutation Testing. Softw., Pract. Exper.
26, 2 (1996), 165–176. https://doi.org/10.1002/(SICI)1097-024X(199602)26:2<165::
AID-SPE5>3.0.CO;2-K
[31]Jeff Offutt. 2011. A mutation carol: Past, present and future. Information &
Software Technology 53, 10 (2011), 1098–1107. https://doi.org/10.1016/j.infsof.
2011.03.007
[32]Carlos Pacheco and Michael D. Ernst. 2007. Randoop: feedback-directed random
testing for Java. In Companion to the 22nd Annual ACM SIGPLAN Conference on
Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA
2007, October 21-25, 2007, Montreal, Quebec, Canada . 815–816. https://doi.org/10.
1145/1297846.1297902
[33]HristinaPalikareva,TomaszKuchta,andCristianCadar.2016. Shadowofadoubt:
testing for divergences between software versions. In Proceedings of the 38th
InternationalConferenceonSoftwareEngineering,ICSE2016,Austin,TX,USA,May
14-22, 2016 . 1181–1192. https://doi.org/10.1145/2884781.2884845
[34]Mike Papadakis, Christopher Henard, Mark Harman, Yue Jia, and Yves Le Traon.
2016. Threatstothevalidityofmutation-basedtestassessment.In Proceedingsof
the25thInternationalSymposiumonSoftwareTestingandAnalysis,ISSTA2016,
Saarbrücken,Germany,July18-20,2016 .354–365. https://doi.org/10.1145/2931037.
2931040
[35]MikePapadakis,YueJia,MarkHarman,andYvesLeTraon.2015. TrivialCom-
piler Equivalence: A Large Scale Empirical Study of a Simple, Fast and Effective
Equivalent Mutant Detection Technique. In 37th IEEE/ACM International Confer-
enceonSoftwareEngineering,ICSE2015,Florence,Italy,May16-24,2015,Volume1 .
936–946. https://doi.org/10.1109/ICSE.2015.103
[36]Mike Papadakis, Marinos Kintis, Jie Zhang, Yue Jia, Yves Le Traon, and Mark
Harman.2018. MutationTestingAdvances:AnAnalysisandSurvey. Advances
in Computers (2018).
[37]Mike Papadakisand NicosMalevris. 2010. An EmpiricalEvaluation of theFirst
and Second Order Mutation Testing Strategies. In Third International Conference
on Software Testing, Verification and Validation, ICST 2010, Paris, France, April 7-9,
2010, Workshops Proceedings . 90–99. https://doi.org/10.1109/ICSTW.2010.50
[38]RudolfRamler,ThomasWetzlmaier,andClausKlammer.2017.Anempiricalstudyontheapplicationofmutationtestingforasafety-criticalindustrialsoftwaresys-tem.InProceedingsoftheSymposiumonAppliedComputing,SAC2017,Marrakech,
547
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Mike Papadakis, Donghwan Shin, Shin Yoo, and Doo-Hwan Bae
Morocco, April 3-7, 2017 . 1401–1408. https://doi.org/10.1145/3019612.3019830
[39]DavidSchulerandAndreasZeller.2013. CoveringandUncoveringEquivalent
Mutants. Softw. Test., Verif. Reliab. 23, 5 (2013), 353–374. https://doi.org/10.1002/
stvr.1473
[40]DonghwanShin,ShinYoo,andDoo-HwanBae.2017.ATheoreticalandEmpirical
Study of Diversity-aware Mutation Adequacy Criterion. IEEE Trans. Software
Eng.(2017). https://doi.org/10.1109/TSE.2017.2732347
[41]Dávid Tengeri, László Vidács, Árpád Beszédes, Judit Jász, Gergo Balogh, Bela
Vancsics,andTiborGyimóthy.2016. RelatingCodeCoverage,MutationScoreand
Test Suite Reducibility to Defect Density. In Ninth IEEE International Conference
on Software Testing, Verification and Validation Workshops, ICST Workshops 2016,
Chicago, IL, USA, April 11-15, 2016 . 174–179. https://doi.org/10.1109/ICSTW.2016.25
[42]W. Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. ASurvey on Software Fault Localization. IEEE Trans. Software Eng. 42, 8 (2016),
707–740. https://doi.org/10.1109/TSE.2016.2521368
[43]W. Eric Wong and Aditya P. Mathur. 1995. Fault detection effectiveness ofmutation and data flow testing. Software Quality Journal 4, 1 (1995), 69–83.
https://doi.org/10.1007/BF00404650
[44]YuchengZhangandAliMesbah.2015.Assertionsarestronglycorrelatedwithtest
suiteeffectiveness.In Proceedingsofthe201510thJointMeetingonFoundations
ofSoftwareEngineering,ESEC/FSE2015,Bergamo,Italy,August30-September4,
2015. 214–224. https://doi.org/10.1145/2786805.2786858
548
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:54 UTC from IEEE Xplore.  Restrictions apply. 