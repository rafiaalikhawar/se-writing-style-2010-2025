AnswerBot: Automated Generation of Answer
Summary to Developers‚Äô Technical Questions
Bowen Xu‚àó¬ß, Zhenchang Xing‚Ä†, Xin Xia‚Ä°‚àó‚àö
, David Lo¬ß
‚àóCollege of Computer Science and Technology, Zhejiang University, China
‚Ä†School of Engineering and Computer Science, Australian National University, Australia
‚Ä°Department of Computer Science, University of British Columbia, Canada
¬ßSchool of Information Systems, Singapore Management University, Singapore
bowenxu.2017@phdis.smu.edu.sg, Zhenchang.Xing@anu.edu.au, xxia02@cs.ubc.ca, davidlo@smu.edu.sg
Abstract ‚ÄîThe prevalence of questions and answers on domain-
speciÔ¨Åc Q&A sites like Stack OverÔ¨Çow constitutes a core knowl-
edge asset for software engineering domain. Although searchengines can return a list of questions relevant to a user query
of some technical question, the abundance of relevant posts and
the sheer amount of information in them makes it difÔ¨Åcult fordevelopers to digest them and Ô¨Ånd the most needed answers to
their questions. In this work, we aim to help developers who
want to quickly capture the key points of several answer postsrelevant to a technical question before they read the details
of the posts. We formulate our task as a query-focused multi-
answer-posts summarization task for a given technical question.
Our proposed approach AnswerBot contains three main steps
: 1) relevant question retrieval, 2) useful answer paragraph
selection, 3) diverse answer summary generation. To evaluate our
approach, we build a repository of 228,817 Java questions and
their corresponding answers from Stack OverÔ¨Çow. We conductuser studies with 100 randomly selected Java questions (not in
the question repository) to evaluate the quality of the answer
summaries generated by our approach, and the effectiveness ofits relevant question retrieval and answer paragraph selection
components. The user study results demonstrate that answer
summaries generated by our approach are relevant, useful anddiverse; moreover, the two components are able to effectively
retrieve relevant questions and select salient answer paragraphs
for summarization.
Index T erms‚ÄîSummary generation, question retrieval
I. I NTRODUCTION
Answers on Stack OverÔ¨Çow have become an important
body of knowledge for solving developers‚Äô technical questions.
Typically, developers formulate their questions as a query tosome search engine, and the search engine returns a list ofrelevant posts that may contain answers. Then, developers
need to read the returned posts and digest the informationin them to Ô¨Ånd the answers to their questions. Information
seeking is rendered difÔ¨Åcult by the sheer amount of questionsand answers available on the Q&A site.
We survey 72 developers in two IT companies (i.e., Heng-
tian and Insigma Global Service) with two questions: (1)
whether you need a technique to provide direct answers when
you post a question/query online and why? and (2) what
is your expectation of the automatically generated answers,
e.g., must the answers be accurate? All the developers agree
‚àö
Corresponding author.that they need some automated technique to provide directanswers to a question/query posted online. The reasons theygive include (1) sometimes it is hard to describe the problemthey meet, so some hints would be useful, (2) there is toomuch noisy and redundant information online, (3) the answersin long posts are hard to Ô¨Ånd, and (4) even the answer theyfound may cover only one aspect of the problem. Developersexpect the answer generation tool to provide a succinct anddiverse summary of potential answers, which can help themunderstand the problem and reÔ¨Åne the queries/questions.
Our survey reveals a great need to provide improved tech-
niques for information retrieval and exploration. In this work,we aim to develop an automated technique for generatinganswer summary to developers‚Äô technical questions, insteadof merely returning answer ports containing answers. Many
developers‚Äô technical questions are non-factoid questions [1],
for example, what are differences between HashTable and
HashMap?, How do I write logs and display them realtime
in Java Swing? For such non-factoid technical questions,
multiple sparse and diverse sentences may make up the answersummary together.
We formulate our task as a query-focused multi-answer-
posts summarization task for a given input question. This
task is closely related to question answering task [2], [3],[4], which aims to Ô¨Ånd information from a huge text baseto answer a question. An answer summary from the text baseshould provide related information with respect to the queryquestion. However, the answer sentences and the query ques-tion are highly asymmetric on the information they convey.They may not share lexical units. Instead, they may only besemantically related (see examples in Table I and Table II).The inherent lexical gap between the answer sentences andthe questions imposes a major challenge for the non-factoidquestion answering task.
To tackle this challenge, we develop a three-stage frame-
work to achieve the goal of generating an answer summaryfor a non-factoid technical question. In the Ô¨Årst stage, weretrieve a set of relevant questions based on the question titles‚Äô
relevance to a query question. The question titles and the
query question are ‚Äúparallel text‚Äù whose relevance is easier todetermine. However, the question titles and the query questionoften still have lexical gaps. Inspired by the recent success of
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research706
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. word embeddings in handling document lexical gaps [5], we
learn word embeddings from a large corpus of Stack OverÔ¨Çowposts to encode the question titles and the query question andmeasure their relevance.
In the second stage, we collect all the answers of the rele-
vant questions retrieved in the Ô¨Årst stage. We extract answer
paragraphs from the collected answers. We develop a multi-criteria ranking method to select a small subset of relevant andsalient answer paragraphs for summarization, based on query-related, user-oriented, and paragraph content features. In thethird stage, given a set of answer paragraphs to be summarized,the generated answer summary needs to cover as much diverseinformation as possible. To generate a diverse summary, wemeasure novelty and diversity between the selected answerparagraphs by Maximal Marginal Relevance (MMR) [6].
We build a question repository of 228,817 Java questions
and their corresponding answers from Stack OverÔ¨Çow. Werandomly select another 100 Java questions as query questionsand use our approach to generate an answer summary foreach query question. Our user studies conÔ¨Årm the relevance,usefulness and diversity of the generated summary, and theeffectiveness of our approach‚Äôs relevant question retrieval andanswer paragraphs selection components. Our error analysisidentiÔ¨Åes four main challenges in generating high-quality an-swer summary: vague queries, lexical gap between query andquestion description, missing long code-snippet answers, and
erroneous answer paragraph splitting, which reveal the future
enhancements of our automated answer generation technique.
The main contributions of this paper are the following:
‚Ä¢We conduct a formative study to assess the necessity ofautomated question answering techniques to provide answersummary to developers‚Äô technical questions.
‚Ä¢We formulate the problem of automated question answeringas a query-focused multi-answer-posts summarization task
for an input technical question.
‚Ä¢We propose a three-stage framework to solve the task, i.e., 1)
relevant question retrieval, 2) answer paragraphs selection,3) answer summary generation.
‚Ä¢We conduct user studies to evaluate the effectiveness of ourapproach and its components, and identify several areas for
future improvements.
Paper Organization. Section II presents our formative study
for automated question answering. Section III describes our
approach. Section IV reports our experimental methods andresults. Section V analyzes the strengths and improvementsof our approach and discusses threats to validity of ourexperiments. Section VI reviews related work. Section VIIconcludes our work and presents future plan.
II. F
ORMATIVE STUDY OF ANSWER SUMMARY
We contacted 120 developers by emails in two IT com-
panies. We received 72 replies which help us understandthe developers‚Äô difÔ¨Åculties in the current information retrieval(IR) practice and assess the necessity of automated questionanswering techniques. Some comments we received are listedas follows:‚Ä¢‚ÄúGoogle will return a number of ‚Äúrelevant‚Äù links for a query,
and I have to click into these links, and read a number of
paragraphs ... It is really time-consuming ... Some links even
contain viruses. A tool which generates potential answers can
save my time wasted on reading a lot of irrelevant content.I fthe generated answer accurately solves my question, it is good.
But I think it would be difÔ¨Åcult. Anyway, I believe it is no harm
to use an answering tool, at least I can get some hints to solve
my problem.‚Äù
‚Ä¢‚ÄúSometimes I cannot accurately describe my questions, which
made it hard to Ô¨Ånd the answer . I have to browse a number ofposts online to learn how to reÔ¨Åne my query, and search again.Thus, I expect that the answer generation tool can help me
understand the information space better, so I can reÔ¨Åne myquery faster without browsing those noisy posts.‚Äù
‚Ä¢‚ÄúI notice even the best answers in Stack OverÔ¨Çow often answer
the questions only in one aspect. Sometimes I need to know a
diversity of aspects to understand the problem better , but they
cannot be found in a single best answer . Thus, I expect the tool
should provide a diversity of potential answers, even if some
answers are not accurate. ‚Äù
‚Ä¢‚Äú... Some questions received too many long answers, and many
of these answers have redundant content. I expect the answer
generation tool should return succinct answers which covers
many aspects of potential solutions. So I could have a high-
level understanding of the question I posted. ‚Äù
‚Ä¢‚ÄúEven if the accuracy of the tool is only 10%, I will still use
it.In the worst case, I will use your tool Ô¨Årst, and then search
on Google again to Ô¨Ånd the solutions. ‚Äù
We use a real-world example to illustrate the difÔ¨Åcul-
ties mentioned in the developers‚Äô replies and the desirable
properties of automated answer generation tool. Assume adeveloper was interested in the differences between HashMap
and HashTable in Java. He used Google to search for Stack
OverÔ¨Çow posts and Table I lists the top 5 ranked StackOverÔ¨Çow questions returned by Google. The question titlesare very relevant to the developer‚Äôs information need and heshould be able to Ô¨Ånd the needed information in the answersto these questions. However, information overload can be
detrimental to the developer. There are 51 answers which have
6,771 words in total. Reading all these answers may take 30minutes (based on the average readers‚Äô reading speed of 200words per minute [7]). Even just reading the best answers(i.e., the accepted answers) or top-voted answers may still takesome time.
It would be desirable to have a answer summary extracted
from the answers to the top 5 ranked questions, as the oneshown in Table II. This answer summary helps the developerquickly capture the key points of the answers relevant to histechnical question. These points reveal the salient and diversedifferences between HashMap and HashTable. They may help
the developer decides which API is more appropriate for histask, or provide information scents for guiding the developer
performing further search or learning [8].
However, manually generating this answer summary is not
an easy task. First, there is much low-quality and irrelevantinformation [9]. Table III shows two examples (eight answersin total). The Ô¨Årst example discusses HashMap and HashTable
in C. The second example discusses how to answer HashMap
and HashTable related interview questions. These answers are
valid in a particular questions context, but have nothing to dowith the developer‚Äôs technical question.
Another issue is information redundancy. As shown in
707
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. TABLE I
THE TOP 5RANKED STACK OVERFLOW QUESTIONS BY GOOGLE SEARCH ENGINE FOR THE QUERY ‚ÄúDIFFERENCES HASHMAPHASHTABLE JAVA‚Äù
No. Question Id Title #Answers #Words
1 40471 Differences between HashMap and Hashtable? 37 4135
2 8875680 Difference between Hashtable and Collections.synchronizedMap(HashMap) 5 847
3 36313817 What are the differences between hashtable and hashmap? (Not speciÔ¨Åc to Java) 3 747
4 32274953 Difference between HashMap and HashTable purely in Data Structures 3 565
5 30110252 What are the differences between Hashmap vs Hashtable in theory? 3 477
TABLE II
DESIRABLE ANSWER SUMMARY WITH RELEV ANT ,SALIENT AND DIVERSE INFORMATION
No. Answer Id Content Aspect
1 764418 HashMap is non synchronized whereas Hashtable is synchronized. Synchronization/Thread Safety
2 25526024 Hashmap can store one key as null. Hashtable can‚Äôt store null. Null Keys/ Null Values
3 22491742Second important difference between Hashtable and HashMap is performance,
since HashMap is not synchronized it perform better than Hashtable.Performance
4 16018266 HashTable is a legacy class in the jdk that shouldn‚Äôt be used anymore. Evolution History
5 20519518Maps allows you to iterate and retrieve keys, values, and both key-value
pairs as well, Where HashTable don‚Äôt have all this capability.Iteration
TABLE III
EXAMPLES OF IRRELEV ANT AND LOW-QUALITY ANSWER PARAGRAPHS
No. Type Answer Id Example
1 Irrelevant 42003464The explaination between hashmap and hashtable is quite correct as it also Ô¨Åts to the header of a string
hash map implementated in strmap. c where thestringmap is a hashtable for strings satisfying the properties
of a key,value-structure. Here it says : /...code.../
3 Low-quality 36325577The interviewer may have been looking for the insight that.
A hash table is a lower-level concept that doesn‚Äôt imply or necessarily support any distinction or separation of
keys and values...even if in some implementations they‚Äôre always stored side by side inmemory, e.g. members of the same structure / std::pair<>...
TABLE IV
REDUNDANT ANSWER PARAGRAPHS
Aspect Set of Redundant Answers‚Äô Id Example
40512,
30108941,40878,
42622789,764418,
10372667,39785829,
20519518,[764418] HashMap is non synchronized whereas Hashtable is synchronized.
Synchronization/
Thread Safety17815037,
34618895,28426488,
41454,27293997,
25348157,8876192,
22084149,[40512] Hashtable is synchronized, whereas HashMap isn‚Äôt. That makes
Hashtable slower than Hashmap.
8876289,
14452144,8876205,
25526024,42315504,
1188347322491742,[39785829] HashTable is internally synchronized. Whereas HashMap
is not internally synchronized.
40878, 7644118, 40548, 10372667, [25526024] Hashmap can store one key as null. Hashtable can‚Äôt store null.
Null Keys/
Null Values39785829,20519518,14452144,25526024,17815037,34618895,28426488,42622789,[40878] Hashtable does not allow null keys or values. HashMap allow sonenull key and any number of null values.
25348157, 30108941[10372667] HashTable can only contain non-null object as a key or as a value.HashMap can contain one null key and null values.
13797704, 40848, 30108941, 39785829,[40848] For threaded apps, you can often get away with ConcurrentHashMap-
depends on your performance requirements.
Performance 22491742, 34618895, 28426488, 24583680[22491742] Second important difference between Hashtable and HashMap is
performance, since HashMap is not synchronized it perform better than Hashtable.
[34618895] As HashMap is not synchronized it is faster as compared to Hashtable.
1041798, 22629569, 40522, 10372667, [16018266] HashTable is a legacy class in the jdk that shouldn‚Äôt be used anymore.
Evolution History 30108941, 39785829, 16018266, 14627155,[39785829] HashMap extends AbstractMap class where as HashTable extendsDictionary class which is the legacy class in java.
34618895, 42315504[42315504] Second difference is HashMap extends Map Interface and whetherHashSet Dictionary interface.
40878,
7344090,7644118,
41454,8832544,
10372667,40483,
30108941,[30108941] Iterating the values: Hashmap object values are iterated by using iterator.
HashTable is the only class other than vector which uses enumerator to iterate the
values of HashTable object.
Iteration 39785829, 20519518, 14452144, 34618895,[20519518] Maps allows you to iterate and retrieve keys, values, and both key-value
pairs as well, Where HashTable don‚Äôt have all this capability.
42622789 [42622789] Iterator in HashMap is fail-fast. Enumerator in Hashtable is not fail-fast.
Table IV, the same aspect may be mentioned in many answer
posts. The information redundancy and diversity creates adilemma for the developer. If he reads every post, he is likelyto come across the same information again and again, whichis a waste of time. If he skips some posts, he risks missingsome important aspects he has not seen. Reading only the bestanswers can address the information overload issue, but not theinformation redundancy and diversity. For example, the bestanswer
1to the 1st ranked question in Table I discusses only
three aspects (Synchronization or Thread Safety, Null Keys and
Null V alues, and Iteration) listed in Table II. To tackle the
1http://stackoverÔ¨Çow.com/a/40878above information relevance, redundancy and diversity issuesfor Ô¨Ånding answers to developers‚Äô technical questions, we needan effective technique to generate an answer summary withrelevant, salient and diverse information from unstructured textof answer posts.
III. P
ROPOSED APPROACH
As shown in Figure 1, our approach (called AnswerBot )
takes as input a software-engineering-related technical ques-tion as a query from the user, and produces as output an
answer summary for the question. Next, we describe the threecomponents of our approach, i.e., relevant question retrieval,
708
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. useful answer paragraphs selection, and diverse answer sum-
mary generation.
A. Relevant Question Retrieval
The relevant question retrieval component takes a technical
question as an input query qand ranks all questions Qin
a large question repository (e.g., questions from Q&A sites
like Stack OverÔ¨Çow). The questions that are ranked at the topare more likely to have answers that can answer the inputtechnical question. We combine word embedding techniqueand traditional IDF metric to measure the relevance betweenthe input query and the questions in the repository. Wordembedding has been shown to be robust in measuring textrelevance in the presence of lexical gap [10]. IDF metric helpsto measure the importance of a word in the corpus.
To train the word embedding model and compute the word
IDF metrics, we build a domain-speciÔ¨Åc text corpus using thequestion title and body of Stack OverÔ¨Çow questions. Eachquestion is considered as a document in the corpus. As the
text is from the website, we follow the text cleaning steps
commonly used for preprocessing web content [11]. We pre-serve textual content but remove HTML tags. We remove longcode snippets enclosed in HTML tag /angbracketleftpre/angbracketright, but not short code
fragments in /angbracketleftcode/angbracketright in natural language paragraphs. We use
software-speciÔ¨Åc tokenizer [12] to tokenize the sentence. Thistokenizer can preserve the integrity of code-like tokens and thesentence structure. We use Gensim (a Python implementationof the word2vec model [13]) to learn the word embeddingmodel on this domain-speciÔ¨Åc text corpus. To compute theword IDF metrics, we build a vocabulary from the text corpusby removing stop words based on the list of stop words forEnglish text
2and using a popular stemming tool [14] to reduce
each word to its root form. We then compute the IDF metricof each word in the vocabulary over the text corpus.
Given a query qand the title of a question Qin the
repository, our relevance calculation algorithm computes their
relevance based on an IDF-weighted word embedding similar-ity between the query and the question title. We use questiontitle in relevance calculation because query and question titleare ‚Äúparallel text‚Äù [15]. The query and the question title are
transformed into a bag of words, respectively, following thesame text preprocessing steps described above. Let W
qbe
the bag of words for the query qandWQbe the bag of
words for the title of the question Q. An asymmetric relevance
rel(Wq‚ÜíWQ)is computed as:
rel(Wq‚ÜíWQ)=/summationtext
wq‚ààWqrel(wq,WQ)‚àóid f(wq)
/summationtext
wq‚ààWqid f(wq)(1)
where id f(wq)is the IDF metric of the word wq,rel(wq,WQ)
ismax wQ‚ààWQrel(wq,wQ), and rel(wq,wQ)is the cosine
similarity of the two word embeddings wqandwQ. Intuitively,
the word embedding similarity of a more important word in thequery and the words in the question title carries more weighttowards the relevance measurement between the query and
2http://snowball.tartarus.org/algorithms/english/stop.txtthe question title. An asymmetric relevance rel(WQ‚ÜíWq)
is computed in the same way. Then, the symmetric relevancebetween the query qand the question Qis the average of
the two asymmetric relevance between W
qandWQ, i.e.,
rel(q,Q)=( rel(Wq‚ÜíWQ)+rel (WQ‚ÜíWq))/2.
Based on the symmetric relevance between the query and
each question in the repository, the questions in the repositoryare ranked and the top Nranked questions are returned as the
relevant questions for the query.
B. Useful Answer Paragraphs Selection
Given a ranked list of relevant questions, all the answer
paragraphs (split by HTML tag /angbracketleftp/angbracketright) in the answers to these
questions are collected. We decide to use the granularity of
answer paragraphs because they are the logical text units thatanswerers create when writing the posts. To select relevant andsalient answer paragraphs for summarization, our approachranks answer paragraphs based on three kinds of features,i.e., query related features, paragraph content features and useroriented features.
Query related features measure the relevance between an
answer paragraph and the query.
‚Ä¢Relevance to query. As the query and the answer paragraphs
usually have lexical gap between the information theyconvey, it is hard to directly measure their relevance. Inthis work, we set the relevance between a query and ananswer paragraph as the relevance between the query andthe question from which the answer paragraph is extracted.
3
The underlying intuition is that the more relevant thequestion is to the query, the more likely the answers tothe question contain relevant answer paragraphs.
‚Ä¢Entity overlap . If an answer paragraph contains software-
speciÔ¨Åc entities mentioned in the query, it is very likelythat the paragraph is relevant to the query. For exam-ple, all desirable answer paragraphs in Table II con-tain HashMap and/or HashTable mentioned in the query.
Software-speciÔ¨Åc entities can be programming languages,libraries/frameworks, APIs, data format, and domain-speciÔ¨Åc concepts [12]. In this work, we consider tags andtag synonyms on Stack OverÔ¨Çow as entities. We identifyentity mentions in a query or answer paragraph by matchingwords in the query or answer paragraph with tag namesand tag synonyms. Let E
qandEapbe the set of entities
mentioned in the query and the answer paragraph, respec-tively. The entity overlap between the query and the answerparagraph is computed as |E
q/intersectiontextEap|/|Eq|(|Eq| /negationslash=0 ). If
the query does not mention any entities (| Eq|=0), we set
entity overlap at 0.
Paragraph content features measure the salience of an
answer paragraph‚Äôs content.
‚Ä¢Information entropy. Salient answer paragraphs would con-tain high-entropy words. A word with higher IDF metricindicates that the word is less common in the corpus (i.e.,
3The relevance between the query and question is calculated during the
relevant question retrieval process ‚Äì see Section III-A.
709
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. &$%! $
&#*
)%!#"&$
&$%! 
+!+!#$ ' 
&%! 
!#%!#
 !#,'

!
 	$%!
' %
&$%! $
!%  $(#
##"$
 %
 $(#
##"$&#*%
%&#$
$## %
%&#$
##"! % %
%&#$%'#$
##"*

 	$%!
 % $(#
##"$ $(#
&#*)%
#"#!$$ !#
!&#*
&#*
+!+!#$
 	&
!&#*



	
	 
Fig. 1. The Framework of Our Approach AnswerBot
TABLE V
SEMANTIC PATTERNS FORSALIENT INFORMATION
No. Pattern No. Pattern
1 Please check XX 7 In short, XX
2 Pls check XX 8 The most important is XX
3 You should XX 9 I‚Äôd recommend XX
4 You can try XX 10 In summary, XX
5 You could try XX 11 Keep in mind that XX
6 Check out XX 12 I suggest that XX
higher entropy). Thus, we sum the IDF metrics of words
(after removing stop words and stemming) in a paragraph to
represent the paragraph‚Äôs entropy. Using this feature, manyparagraphs with low information entropy, e.g., ‚ÄúI cannotagree more.‚Äù, will be Ô¨Åltered out.
‚Ä¢Semantic patterns. We observe that there are certain sen-
tence patterns that often indicate recommendation or sum-marization of salient information in Stack OverÔ¨Çow discus-sions (see Table V for examples). For example, a question
on Stack OverÔ¨Çow asks ‚ÄúArray or List in Java. Which isfaster?‚Äù. The best answer to this question is ‚ÄúI suggest thatyou use a proÔ¨Åler to test which is faster . ‚Äù. In this work,
we summarize 12 sentence patterns based on our empiricalobservations of 300 randomly selected best answers onStack OverÔ¨Çow. If an answer paragraph contains at leastone of the sentence patterns, we set the paragraph‚Äôs patternvalue at 1, otherwise 0.
‚Ä¢Format patterns. We observe that HTML tags are often
used to emphasize salient information in the discussions.For example, /angbracketleftstrong/angbracketrighthighlights some text by bold font
and/angbracketleftstrike/angbracketright points out some incorrect information. If an
answer paragraph contains such HTML tags, we set its
format pattern score at 1, otherwise 0.
User oriented features select summary and high-quality
answer paragraphs based on user behavior patterns.
‚Ä¢Paragraph position. We observe that when answerers writeanswer posts, they usually start with some summary infor-mation and then go into details. For example, a questionasks ‚ÄúHow do I compare strings in Java?‚Äù, The Ô¨Årst threeparagraphs of the best answer of this question present ‚Äú==for reference equality‚Äù, ‚Äú.equals() for value equality‚Äù, and
‚ÄúObjects.equals() checks for nulls‚Äù. Therefore, we set aparagraph‚Äôs summary value to be inversely proportionalto the paragraph‚Äôs position in the post for the Ô¨Årst m
paragraphs, i.e., summary =1/pos (1‚â§pos‚â§m)
(m=3 in our current implementation). The summary
values of the subsequent (beyond the m
th) paragraphs are
set at 0.
‚Ä¢V ote on answer. Answers with higher vote indicate that the
community believes that they contain high-quality informa-tion to answer the corresponding question. In this work, we
set an answer paragraph‚Äôs vote as the vote on the answerpost from which the paragraph is extracted.
Based on the above seven features, an overall score is
computed for each answer paragraph by multiplying the nor-
malized value of each feature. To avoid the feature scoresbeing 0, all the feature scores are normalized to (0,1] byadding a smooth factor 0.0001 [16]. Answer paragraphs are
ranked by their overall scores and the top Mranked answer
paragraphs are selected as candidate answer paragraphs forsummarization.
C. Diverse Answer Summary Generation
As shown in Table IV, there are often many redundant
answer paragraphs from the answers to relevant questions.
The generated answer summary should avoid such redundantinformation. Given a list of candidate answer paragraphs, max-imal marginal relevance (MMR) algorithm is used to select a
subset of answer paragraphs in order to maximize novelty and
diversity between the selected answer paragraphs [6]. MMRÔ¨Årst builds a similarity matrix between each pair of candidateanswer paragraphs. The similarity is computed as the symmet-ric relevance between the two answer paragraphs as describedin Section III-A. It then iteratively selects Kcandidate answer
paragraphs with maximal marginal relevance. The selected
answer paragraphs form an answer summary to the user‚Äôstechnical question.
710
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. IV . E XPERIMENTS &R ESULTS
We conduct three user studies to answer the following three
research questions, respectively:
RQ1 How effective is our approach in generating answer
summaries with relevance, useful and diverse information
for developers‚Äô technical questions?
RQ2 How effective is our approach‚Äôs relevant question re-
trieval component?
RQ3 How effective is our approach‚Äôs answer paragraph se-
lection component?
In this section, we Ô¨Årst describe our repository of questions
and answers and tool implementation. We then describe our
experimental query questions, and how we select participantsand allocate tasks in our user studies. Finally, we elaboratethe motivation, approach and results for the three researchquestions.
A. Question Repository and Tool Implementation
We collect 228,817 Java questions (i.e., questions tagged
with Java) and their corresponding answers from Stack Over-
Ô¨Çow Data Dump of March 2016. These questions have atleast one answer. To ensure the quality of question repository,we require that at least one of the answers of the selectedquestions is the accepted answer or has vote >0. When
collecting questions, we avoid duplicate questions of thealready-selected questions, because duplicate questions discussthe same question in different ways and can be answeredby the same answer. We use these Java questions and theiranswers as a repository for answering Java-related technicalquestions. We build a text corpus using the title and body ofthese Java questions to train the word embedding model andbuild the word IDF vocabulary. Considering the conciseness ofthe generated answer summary and the fact that searchers tend
to browse only the top ranked search results [17], our currentimplementation returns top 5 relevant questions for a query and
selects top 10 candidate answer paragraphs for summarization.The generated answer summary contains 5 answer paragraphs.
B. Experimental Queries
We randomly select another 100 questions
4and use the
titles of these questions as query questions. We ensure that our
question repository does not contain these 100 query questionsand their duplicate questions. The randomly selected 100 query
questions cover a diversity of aspects of Java programming.For example, some of them are related to language features,
such as multi-threading (e.g., How does volatile actually
work?) and I/O (e.g., Can BufferedReader read bytes?), while
others are related to many third-party libraries, such as TEST-Assertions (e.g., Testing API which returns multiple values
with JUnit ) and REST (e.g., Is there an equivalent to ASP .NET
WEB API in JAVA world?). Some of the query questions areeasy to answer (e.g., How to convert String into DateFormat
in java? ), while others are difÔ¨Åcult (e.g., How does volatile
actually work?). The diversity of these 100 questions can
4See our replication package at http://bit.ly/2qBEUhiTABLE VI
TASK ALLOCATION TO PARTICIPANTS
RQs Group 1 (P1, D1, D2, D3) Group 2 (P2, D4, D5, D6)
RQ1 Q1-Q50 Q51-Q100
RQ2 Q51-Q100 Q1-Q50
RQ3 Q51-Q100 Q1-Q50
improve the generality of our study, and reduce the bias thatour approach might be only effective for a speciÔ¨Åc type ofquestions. We index these 100 questions as Q1 to Q100.
C. Participant Selection and Task Allocation
We recruited participants through our school‚Äôs mailing lists
and select 2 postdoctoral fellows (P1 and P2) and 6 PhD
students (D1 to D6) to join our user study. All the selectedparticipants have industrial experience on Java development,
and they have used Java to develop commercial projects intheir work before they went to graduate school. The years oftheir working experience on Java are vary from 2 to 8 years,
with an average 4.6 years. The diversity of these participants‚Äôworking experience on Java can improve the generality of
our results. In practice, our tool aims to help all levels ofdevelopers, from novice to senior developers. During our userstudy, no participants report being unable to understand thequery questions and answers.
We divided the eight participants into two groups, i.e.,
P1, D1, D2 and D3 in Group1 and P2, D4, D5 and D6in Group2. Furthermore, we divided the 100 questions intotwo tasks, i.e., Q1-Q50 in Task1 and Q51-Q100 in Task2.Table VI present the task allocation to the two participantgroups. For RQ1, Group1 worked on Task1 questions, whileGroup2 worked on Task2 questions. For RQ2 and RQ3,Group1 worked on Task2 questions, while Group2 worked onTask1 questions. All four participants in these two groups wererequired to review the answers of the assigned 50 questionsindependently. With this task allocation, we have all 100 queryquestions evaluated for the three research questions. As RQ1evaluates the overall performance of our approach, and RQ2and RQ3 evaluates its components, using the same set ofquery questions to evaluate RQ1 and RQ2/RQ3 may bias theparticipants‚Äô results. However, since RQ2 and RQ3 evaluatesthe two components independently and the two componentsdeal with completely different input/output, using the samequestions would have little impact on the participants‚Äô results.We asked the participants to complete the study in three 2-hoursessions; the Ô¨Årst session evaluates RQ1, while the second andthird evaluate RQ2 and RQ3 respectively.
D. Research Questions
RQ1: Effectiveness of the overall approach and the rele-
vance, usefulness and diversity of answer summary
Motivation. In the current IR practice, developers retrieve
relevant questions by entering their technical questions to a
search engine. Then they have to manually browse the answersof the returned relevant questions to Ô¨Ånd the needed infor-mation. In contrast, our approach can automatically generatean answer summary of the key points in the answers of thereturned relevant questions. We would like to investigate the
711
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. overall performance of our approach in terms of the relevance,
usefulness and diversity of the generated summary, comparedwith manual information seeking.Approach. We compare our approach against two baselines
built based on Google search engine and Stack OverÔ¨Çow
search engine, respectively. We add ‚Äúsite:stackoverÔ¨Çow.com‚Äùto the query of Google search engine so that it searches onlyposts on Stack OverÔ¨Çow. For a query question, we use the Ô¨Årstranked Stack OverÔ¨Çow question returned by a search engineas the most relevant question. We assume that a developerwould read the best answer (i.e., the accepted answer) orthe answer with the highest vote if there is no acceptedanswer of the relevant question. We refer to the collectedbest or highest-vote answer of the two baseline approaches asthe Baseline
Google answer summary and the Baseline SO
answer summary, respectively.
For each query question, we provide the participants the
answer summary generated by Baseline Google, Baseline SO
and our approach, respectively. The participants do not knowwhich answer summary is generated by which approach. Theyare asked to score the three answer summaries from threeaspects, i.e., relevance, usefulness and diversity. Relevance
refers to how relevant the generated summary is to the query.Usefulness refers to how useful the generated summary is for
guiding the developer‚Äôs further search or learning. Diversity
refers to whether the generated summary involves diverse
aspects of information. The score is a 5 point likert scale,with 1 being ‚ÄúHighly Irrelevant/Useless/Identical‚Äù and 5 being‚ÄúHighly Relevant/Useful/Diverse‚Äù.
Results. Table VII shows the mean of relevance, usefulness
and diversity scores of our approach and the two baselines. Theresult shows that our approach achieves the best performancein all three aspects, while the Baseline
SOachieves the
worst performance. Our approach and Baseline Google have
comparable relevance score, but our approach has higher scorein usefulness and diversity, especially diversity. The averagenumber of paragraphs in Baseline
Google and Baseline SO
answer summaries are 4.18 and 4.09, respectively.
We use Wilcoxon signed-rank test [18] to evaluate whether
the differences between our approach and the baseline ap-proaches are statistically signiÔ¨Åcant. The improvement of ourapproach over the Baseline
SOis statistically signiÔ¨Åcant on all
three aspects at the conÔ¨Ådence level of 99.9%. The improve-
ment of our approach over the Baseline Google on usefulness
and diversity is statistically signiÔ¨Åcant at the conÔ¨Ådence level
of 95%. We use the best answer of the most relevant questionreturned by Google as the Baseline
Google‚Äôs answer for the
query. Considering the Google‚Äôs capability, it is not surprisingthe difference in relevance is not statistically signiÔ¨Åcant.
However, our approach achieves statistically signiÔ¨Åcant betterperformance on usefulness and diversity (especially diversity).This indicates that the best or highest-vote answers may not
cover as diverse information as the developers need. Therefore,it is worth reading more answer posts to summarize more com-plete information. Our approach automates this summarization
process for a diversity of answers.TABLE VII
MEAN OF RELEV ANCE ,USEFULNESS AND DIVERSITY OF OURAPPROACH
AND THE BASELINE APPROACHES (RQ1)
Relevance Usefulness Diversity
Our Approach 3.450 3.720 3.830
Baseline Google 3.440 3.480* 2.930***
Baseline SO 2.576*** 2.712*** 2.305***
***p<0.001, **p<0.01, *p<0.05
RQ2: The Effectiveness of relevant question retrieval
Motivation. An answer summary is generated from the an-
swers of some questions relevant to a query question. If the
retrieved questions are not relevant to the query question, itis unlikely to generate a high-quality answer summary forthe query question. Our question retrieval approach combinesword embeddings with traditional IDF metrics. We wouldlike to investigate the effectiveness of our method, comparedwith the traditional TF-IDF based methods and other word- ordocument-embedding based methods.
Approach. We build three baseline approaches: TF-IDF based
IR [19], word-embedding based document retrieval [5], and
document-to-vector (Dov2Vec) based document retrieval [20].TF-IDF is a traditional IR metric that is often used to rank adocument‚Äôs relevance to a user query in software engineering
tasks, such as question retrieval [21] and code search [22].Yang et al. [5] average word embeddings of words in adocument to obtain a document vector which can be used tomeasure document relevance. Dov2Vec learns document em-
beddings together with the underlying word embeddings usinga neural network and is also applied to measure document
relevance [20].
For each query question, we collect the top-10 ranked
questions retrieved by our question retrieval approach or one
of the baseline approaches. We ask the participants to identifythe relevant questions in the top-10 ranked questions. Theparticipants do not know which approach generates which listof top-10 ranked questions. We use the following metrics in
comparison of different approaches.
Top-K Accuracy: Given a query question q, if at least one of
the top-k ranked questions is relevant, we consider the retrieval
to be successful, and set the value Success (q,top‚àík)to 1.
Otherwise, we consider the retrieval to be unsuccessful, andset the value Success (q,top‚àík)to 0. Given a set of queries,
denoted as qs, its top-k accuracy Top@k is computed as:
To p@k(qs)=/summationtext
q‚ààqsSuccess (q,top -k)/|qs|. The higher the
top-k accuracy score is, the better a relevant question retrievalapproach ranks the Ô¨Årst relevant question. In this paper, wes e tk=1 ,5a n d1 0 .
Mean Reciprocal Rank: Given a query question, its recip-
rocal rank is the multiplicative inverse of the rank of the Ô¨Årstrelevant question in a ranked list of questions. Mean Recipro-cal Rank (MRR) is the average of the reciprocal ranks of allqueries in a set of queries: MRR (qs)=
1
|qs|/summationtext
q‚ààqs1
Rank (q).
We denote qsas the set of queries. Rank(q) refers to the
position of the Ô¨Årst relevant question in the ranked list ofquestions returned by a relevant question retrieval approach fora query. The higher the MRR is, the higher the Ô¨Årst relevant
questions is ranked for a set of queries.
712
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. TABLE VIII
TOP@KACCURACY AND MRR OFOURAPPROACH AND THE BASELINE
APPROACHES IN RELEV ANT QUESTION RETRIEV AL (RQ2)
Top@1 Top@5 Top@10 MRR
Our Approach 0.460 0.610 0.660 0.520
Doc2Vec 0.180*** 0.580 0.650 0.335***
Word Embeddings 0.340* 0.520 0.550* 0.408*
TF-IDF 0.320* 0.490* 0.530** 0.388*
***p<0.001, **p<0.01, *p<0.05
Results. Table VIII shows the Top@k and MRR metrics of
our approach and the three baseline approaches for retrieving
relevant questions for the 100 query questions. We notice thatour approach achieves the best performance in all the evalu-ated metrics, especially for Top@1 and MRR. The Doc2V ec
baseline achieves comparable performance as our approachon Top@5 and Top@10, but it has the worst performanceon Top@1 and MRR. The word-embedding baseline performs
slightly better than the TF-IDF baseline in Top@k and MRR.
The differences between the three baseline approaches actuallyindicate that the traditional TF-IDF based method and the wordor document-embedding based methods can complement eachother. In fact, our approach that combines word embeddingsand IDF metrics achieves the best performance than eitherTF-IDF or word/document embedding alone.
We apply Wilcoxon signed-rank test to test the statistical
signiÔ¨Åcance of the differences between our approach and
the baseline approaches. The improvement of our approach
over the TF-IDF baseline is statistically signiÔ¨Åcant on all
the metrics at the conÔ¨Ådence level of 95%. Compared withthe word-embedding and Doc2V ec baselines, our approach
is statistically signiÔ¨Åcant better on Top@1 and MRR at the
conÔ¨Ådence level of 95%. Although the differences between
our approach and the word-embeddings and Doc2V ec baselines
on Top@5 and Top@10 are not statistically signiÔ¨Åcant, thesigniÔ¨Åcantly better MRR indicates that our approach can rankrelevant questions higher than the other two baselines.
RQ3: The Effectiveness of answer paragraph selection
Motivation. To select the most relevant and salient answer
paragraphs for summarization, our approach considers three
types of features of answer paragraphs, i.e., query-related,user-oriented and paragraph content features. We would like
to investigate the impact of different types of features on the
results of answer paragraphs selection.
Approach. We remove one type of features at a time from the
full feature set and reserve the other two types. Thus, three
baseline approaches are built, i.e., without (w/o) query-relatedfeatures, w/o user-oriented features, and w/o paragraph contentfeatures. We let each approach output a ranked list of 10answer paragraphs with the highest overall score. We take the
union of the 40 answer paragraphs selected by our approach(with all features) and the three baselines. Participants areasked to judge whether the selected paragraphs contain salientinformation relevant to the query question. They do not know
which answer paragraph is selected by which approach. We
use Top@k accuracy and MRR in the top 10 ranked candidateanswer paragraphs to evaluate the performance of answerparagraph selection with and without certain type of features.TABLE IX
TOP@KACCURACY AND MRR OFFEATURE ABLATION FOR ANSWER
PARAGRAPH SELECTION (RQ3)
Top@1 Top@5 Top@10 MRR
all features 0.660 0.990 1.000 0.803
w/o query related 0.610* 0.970 1.000 0.758**
w/o user oriented 0.500*** 0.980 1.000 0.699***
w/o paragraph content 0.570* 1.000 1.000 0.744**
***p<0.001, **p<0.01, *p<0.05
Results. Table IX presents Top@k and MRR metrics of using
all features or adopting certain type of features for answerparagraph selection. Using all features achieves the best per-
formance in all metrics compared with adopting certain typeof features. This suggests that all types of features are usefulfor answer paragraph selection. Using query-related featuresachieves better performance than adopting the other two types
of features, and using user-oriented features achieves the worst
performance. This suggests that user-oriented features play themost important role for answer paragraph selection, paragraphcontent features take a second place, and query-related featuresare relatively less important.
Wilcoxon signed-rank test shows that the improvement of
using all features over adopting certain type of features isstatistically signiÔ¨Åcant on Top@1 and MMR at the conÔ¨Ådencelevel of 95%. Top@5 and Top@10 results in Table IX showthat there is almost always at least one relevant answerparagraph in the top 5 or 10 ranked list of candidate answerparagraphs. This demonstrates the general effectiveness of ouranswer paragraph selection features. Therefore, the differencesbetween using all features and adopting certain type of featureson Top@5 and Top@10 are not statistically signiÔ¨Åcant.
V. D
ISCUSSION
In this section, we qualitatively compare our question an-
swering approach with community question answering prac-tice. We then discuss cases where our approach is ineffective,followed by some threats to validity.
A. Comparison with Community Question Answering
In community question answering, developers post their
questions on a Q&A site like Stack OverÔ¨Çow and wait for
answers from the community of developers. To understandthe differences between the community-provided answers to atechnical question and the answer summary that our approachgenerates from answers of relevant questions, we manuallycompare the best answers of the questions we use as queriesand the answer summary that our approach generates for thesequestions.
Table X presents two examples. The query question in
Ô¨Årst example is ‚Äúcalculating time difference‚Äù in which the
developer runs into some errors in using getTime ()to
calculate time difference. The best answer of this question
suggests to use long to store getTime ()‚Äôs return value,
rather than casting it to int. For this query question, our
generated answer summary consists of Ô¨Åve paragraphs fromthe four answers of two relevant questions. Except for
the Ô¨Åfth paragraph ‚ÄúYou can try this‚Äù (due to the limi-tation of paragraph splitting, see Section V-B), the other
713
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. four paragraphs provide two alternative solutions (using
System.nanoT ime() orcurrentT imeMillis ()) to calculate
time difference. They also describe the reason and cautions of
using the two APIs, such as System.nanoTime ()is more
precise, System.nanoT ime() must not be used as a wall
clock, currentT imeMillis() may not be a good method for
time due to method overhead.
The query question in the second example is about host-
name mismatch problem in HTTPCli e n t . The best answer
provides a solution and explains the advantage of the provided
solution. Our answer summary consists of Ô¨Åve paragraphs
from the Ô¨Åve answers of four relevant questions. Except forthe Ô¨Åfth paragraph from the fourth question (about a factof Amazon AWS service), the other four paragraphs providevaluable information which can complement the best answerof the query question. The third and fourth paragraphs provide
two different solutions for solving the problem, i.e., usingHttpClientBuilder.create ().build ()or using a Ô¨Åxed version
ofHTTPCli e n t instead of the buggy version. Although the
Ô¨Årst and second paragraphs do not provide direct solutions,
they point out some important aspects related to the hostnamemismatch problem, such as avoiding the action which allows
all hosts without any veriÔ¨Åcation, checking the DNS name ofthe certiÔ¨Åcate presented by the server.
As the above two examples show, community answers to a
technical question usually focus on a speciÔ¨Åc aspect technicalissue in the question. Our generated answer summary derivedfrom answers of several relevant questions can well comple-ment community answers to a technical question by providing
more alternative solutions and/or broader information usefulfor further search and learning.
B. Error Analysis
Through the analysis of the cases in which our approach
generates a low-quality even unrelated answer summary, we
identify four main challenges in generating high-quality an-swer summary: vague queries, lexical gap between query andquestion description, answers involving long code snippet, anderroneous answer paragraph splitting.
In our study, we randomly select 100 Stack OverÔ¨Çow
questions and use their titles as queries to search the questionrepository. However, question titles are short and some ofthem are vague, e.g., ‚ÄúWhy is this code working withoutvolatile?‚Äù, ‚ÄúFast processing of data‚Äù. It is hard to understand
such question titles without looking into the question bodies.Furthermore, the lexical gap between query question and titlesof questions in the repository makes it difÔ¨Åcult to measure theirsemantic relevance. Due to these two challenges, our relevant
question retrieval component fails to retrieve at least onerelevant question in the top 10 results for 34 of the 100 queryquestions. The low-quality question retrieval results directlyresult in the low quality of the generated answer summary. To
improve relevant question retrieval, we will consider questionbodies which contain richer information and adopt deep neural
network [15] which are more robust for handling lexical gapsin text.Our current approach keeps short code fragments (enclosed
in HTML tag /angbracketleftcode/angbracketright ) in natural language paragraphs but
removes long code snippets (enclosed in HTML tag /angbracketleftpre/angbracketright).
However, for some query questions, such as ‚ÄúHow to send an
Image from Web Service in Spring‚Äô ‚Äô, ‚ÄúHow to implement a db
listener in Java‚Äù and ‚ÄúXML to Json using Json-lib‚Äù, relevant
answers are code snippets, rather than natural language para-graphs. Thus, to generate high-quality answer summary forthis type of questions, we must take into account long codesnippets.
Our current approach splits the text in an answer post into
answer paragraphs by HTML tag /angbracketleftp/angbracketright. This simple strategy may
sometimes break the logic relationships between several con-
secutive physical paragraphs. For example, people often writesome introductory paragraph like ‚ÄúTry with the following: ‚Äù,
‚ÄúFrom here you can go to‚Äù, and ‚ÄúThere are many solutions
to this problem‚Äù, followed by a separate paragraph explainingthe details. To generate more sensible answer summary, anintroductory paragraph and the following detailed explanationshould be treated as a whole, using more robust paragraphsplitting method.
C. Threats to V alidity
Threats to internal validity are related to experimental bias of
participants in manual examination of relevant questions and
answer summaries. First, the participants‚Äô lack of knowledgeon Java may affect their judgements about question/answer‚Äôsrelevance and usefulness. This threat is limited by selectingonly participants who have at least 2 years industrial ex-perience on Java development. Still, there could be errorsbecause an experienced Java developer is not necessarilyfamiliar with all Java frameworks and APIs. On the otherhand, the participants‚Äô degree of carefulness and effort inmanual examination may affect the validity of judgements.We minimize this threat by choosing participants who expressinterests in our research, and giving the participants enoughtime to complete the evaluation tasks.
Threats to external validity are related to the generalizability
of our research and experiments. Stack OverÔ¨Çow questions are
related to many domains other than Java (e.g. Python, Eclipse,database), or combinations of multiple domains. Our approachis general, but considering the background knowledge ofavailable participants for the user studies, we use only Java
questions in this work. Furthermore, as user studies require
signiÔ¨Åcant human efforts, we only use 100 query questionsin this study. In the future, we will use more queries, largerquestion repository, and questions of more domains to reducethese threats.
Threats to construct validity are related to the suitability
of
our evaluation metrics. Relevance [23], usefulness [24]
and diversity [25] are widely used to evaluate summarization
tasks in software engineering, Both relevant question retrieval
and answer paragraphs selection are ranking problems. Thus,
we use Top@k accuracy (k=1, 5, 10) and MRR. These twometrics are the most widely used metrics for evaluating IRtechniques [26], [15], [27].
714
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. TABLE X
COMPARISON OF THE BESTCOMMUNITY -PROVIDED ANSWERS AND THE ANSWER SUMMARY GENERATED BY OURAPPROACH
#1 Query Question: Calculating time diffrence
Relevant QuestionsBest Answer of Query Question(Id: 18575058) Answer Summary Generated by Our ApproachId Title
[9707986] Try this
long start time = System.nanoTime();
resp = GeoLocationService.getLocationByIp(ipAddress);
long end time = System.nanoTime();
double difference = (end time - start time)/1e6;
Try changingint diff=(int)d2.getTime()-(int)d1.getTime();to long diff=d2.getTime()-d1.getTime();Explicit typecasting from long to int[9707972] No, it doesn‚Äôt mean it‚Äôs taking 0ms - it shows it‚Äôs takinga smaller amount of time than you can measure with currentTimeMillis().That may well be 10ms or 15ms. It‚Äôs not a good method to call for timing;it‚Äôs more appropriate for getting the current time.
9707938 Calculating time difference in Milliseconds
will cause precision loss and may result in
a negative value on subtraction.
long a = 90000000000000L;
long b = 10000001000000L;
a>b (int)a-(int)b=>negative value[9707972] To measure how long something takes, consider using
System.nanoTime instead. The important point here isn‚Äôt that the
precision is greater, but that the resolution will be greater...
but only when used to measure the time between two calls.
It must not be used as a ‚Äùwall clock‚Äù.
[9707982] Since Java 1.5, you can get a more precise time value with
System.nanoTime(), which obviously returns nanoseconds instead.
[24907491] You can try this : 24907002 Calculating Time Difference in Java
#2 Query Question: Android HttpClient - hostname in certiÔ¨Åcate didn‚Äôt match <example.com>!= <*.example.com>
Relevant QuestionsBest Answer of Query Question(Id: 3136980) Answer Summary Generated by Our ApproachId Title
[15497467] Important - if you are allowing all hosts (that is, disabling
host name veriÔ¨Åcation), then it is certainly NOT safe. You shouldn‚Äôt be
doing this in production.15497372Java HTTP post using HTTPS
Confusion - javax.net.ssl.SSLException:
hostname in certiÔ¨Åcate didn‚Äôt match
This is my (edited) solution:/...code.../
It has the advantage of not changing the[7257060] The certiÔ¨Åcate veriÔ¨Åcation process will always verify the
DNS name of the certiÔ¨Åcate presented by the server, with the hostname
of the server in the URL used by the client.7256955Java SSLException: hostname incertiÔ¨Åcate didn‚Äôt match
default behavior unless there is a wildcard
domain, and in that case it revalidates as
though the 2 part domain (e.g., someUrl.com)[24526126] This HttpClientBuilder.create().build() will return
org.apache.http.impl.client.InternalHttpClient. It can handle the this
hostname in certiÔ¨Åcate didn‚Äôt match issue.
were part of the certiÔ¨Åcate, otherwise theoriginal exception is rethrown. That meanstruly invalid certs will still fail.[34494091] This problem is described in Apache HttpClient resolvingdomain to IP address and not matching certiÔ¨Åcate. It appears to be abug in the version of HTTPClient you are using, where it compares the
target IP instead of the target hostname with the subject certiÔ¨Åcate.
Please use a Ô¨Åxed version of HTTPClient instead.34493872SSLException: hostname in certiÔ¨Åcatedidn‚Äôt match
<50.19.233.255>!=<*.heroku.com>
[12755039] Buckets whose name contains periods can now be correctlyaddressed again over HTTPS.12755038SSL problems with S3/AWS using the JavaAPI: hostname in certiÔ¨Åcate didn‚Äôt match
VI. R ELATED WORK
Many text summarization approaches have been applied
in different software engineering tasks, aiming to reduce the
developers‚Äô effort to read an immense quantity of information.Rastkar et al. propose an extractive approach for automatic bugreport summarization [28]. Their approach selects a subset ofbug report comments by training a binary classiÔ¨Åer to deter-mine whether a comment should be selected or not. Andreaet al. propose an approach SURF to produce a summary for
user reviews [24]. SURF classiÔ¨Åes each user review sentence
to one of the user-intention categories, and groups together
sentences covering the same topic. It then uses a sentence
selection and scoring mechanism to generate the user-reviewsummary. Different from the above studies, we focus onanswer summarization on online Q&A sites which requiresa different set of features. Additionally, we formulate ourproblem as a ranking problem instead of a classiÔ¨Åcation one.
A number of studies have also proposed methods to identify
relevant or high-quality posts in question and answering sites.Gottipati et al. propose a semantic search engine framework to
process posts in discussion threads to recover relevant answers
to a user query [29]. They classify posts in the discussionthreads into 7 categories (e.g., questions, answers, clarifyingquestion, junk, etc.) and use the category labels to Ô¨Ålter lessuseful information to improve the search experience. Yao
et al. propose an approach to detect high-quality posts incommunity question answering sites [30]. Different from the
above studies, we not only identify relevant posts but alsocreate summaries of these posts.
Popular search engines like Google can provide directanswers for some types of queries. For example, Googlecan extract a paragraph from the best answer of the StackOverÔ¨Çow question ‚Äúwhat are the differences between hashtableand hashmap?‚Äù as the answer for a query like ‚Äúdifferencesbetween hashtable and hashmap‚Äù. However, Google does thisonly for speciÔ¨Åc kinds of 5W+1H (who, what, where, when,
why, how) questions, and it does not generate direct answers
to all 5W+1H questions, e.g., ‚ÄúWhat are the differences be-tween quick sort and bubble sort?‚Äù. More importantly, Googleextracts only a paragraph from one answer of one question,while our approach is multi-answer-posts summarization.
VII. C
ONCLUSION
Our formative study indicates that developers need some
automated answer generation tools to extract a succinct anddiverse summary of potential answers to their technical ques-tions from the sheer amount of information in Q&A discus-sions. To meet this need, we propose a three-stage frameworkfor automated generation of answer summary. Our user studiesdemonstrate the relevance, usefulness and diversity of our
automated generated answer summaries. In the future, we will
investigate more robust relevant question retrieval algorithms,and explore more features for answer paragraph selection. Wewill develop our approach into practical tools (e.g., browserplugins) to provide developers with direct answers and ex-tended suggestions to help them Ô¨Ånd the needed information
more efÔ¨Åciently on the Internet.
A
CKNOWLEDGMENT
This work was partially supported by NSFC Program (No.
61602403 and 61572426).
715
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] H. Song, Z. Ren, S. Liang, P. Li, J. Ma, and M. de Rijke, ‚ÄúSummarizing
Answers in Non-Factoid Community Question-Answering.‚Äù
[2] C. Unger, L. B ¬®uhmann, J. Lehmann, A.-C. Ngonga Ngomo, D. Gerber,
and P. Cimiano, ‚ÄúTemplate-based question answering over RDF data,‚Äù
inProceedings of the 21st international conference on World Wide Web.
ACM, 2012, pp. 639‚Äì648.
[3] M. Iyyer, J. L. Boyd-Graber, L. M. B. Claudino, R. Socher, and
H. Daum ¬¥e III, ‚ÄúA Neural Network for Factoid Question Answering over
Paragraphs.‚Äù in EMNLP, 2014, pp. 633‚Äì644.
[4] M. Asaduzzaman, A. S. Mashiyat, C. K. Roy, and K. A. Schneider,
‚ÄúAnswering questions about unanswered questions of stack overÔ¨Çow,‚ÄùinMining Software Repositories (MSR), 2013 10th IEEE Working
Conference on. IEEE, 2013, pp. 97‚Äì100.
[5] X. Yang, D. Lo, X. Xia, L. Bao, and J. Sun, ‚ÄúCombining Word
Embedding with Information Retrieval to Recommend Similar BugReports,‚Äù in Software Reliability Engineering (ISSRE), 2016 IEEE 27th
International Symposium on. IEEE, 2016, pp. 127‚Äì137.
[6] J. Carbonell and J. Goldstein, ‚ÄúThe use of MMR, diversity-based rerank-
ing for reordering documents and producing summaries,‚Äù in Proceedings
of the 21st annual international ACM SIGIR conference on Research anddevelopment in information retrieval. ACM, 1998, pp. 335‚Äì336.
[7] M. A. Just and P. A. Carpenter, ‚ÄúSpeedreading,‚Äù 1987.[8] W.-C. Wu, ‚ÄúHow far will you go?: characterizing and predicting online
search stopping behavior using information scent and need for cogni-
tion,‚Äù in Proceedings of the 36th international ACM SIGIR conference
on Research and development in information retrieval. ACM, 2013,pp. 1149‚Äì1149.
[9] X. Xia, L. Bao, D. Lo, P. S. Kochhar, A. E. Hassan, and Z. Xing, ‚ÄúWhat
do developers search for on the web?‚Äù Empirical Software Engineering,
pp. 1‚Äì37, 2017.
[10] B. Xu, D. Ye, Z. Xing, X. Xia, G. Chen, and S. Li, ‚ÄúPredicting seman-
tically linkable knowledge in developer online forums via convolutionalneural network,‚Äù in Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering. ACM, 2016, pp. 51‚Äì
62.
[11] C. Chen, Z. Xing, and X. Wang, ‚ÄúUnsupervised Software-SpeciÔ¨Åc
Morphological Forms Inference from Informal Discussions.‚Äù
[12] D. Ye, Z. Xing, C. Y . Foo, Z. Q. Ang, J. Li, and N. Kapre, ‚ÄúSoftware-
speciÔ¨Åc named entity recognition in software engineering social con-tent,‚Äù in Software Analysis, Evolution, and Reengineering (SANER),
2016 IEEE 23rd International Conference on, vol. 1. IEEE, 2016,pp. 90‚Äì101.
[13] R. ÀáRehÀöuÀárek and P. Sojka, ‚ÄúSoftware Framework for Topic Modelling
with Large Corpora,‚Äù in Proceedings of the LREC 2010 Workshop on
New Challenges for NLP Frameworks. Valletta, Malta: ELRA, May
2010, pp. 45‚Äì50, http://is.muni.cz/publication/884893/en.
[14] S. Bird, ‚ÄúNltk: the natural language toolkit,‚Äù in Proceedings of the
COLING/ACL on Interactive presentation sessions. Association forComputational Linguistics, 2006, pp. 69‚Äì72.
[15] G. Chen, C. Chen, Z. Xing, and B. Xu, ‚ÄúLearning a dual-language vector
space for domain-speciÔ¨Åc cross-lingual question retrieval,‚Äù in Ieee/acm
International Conference on Automated Software Engineering, 2016, pp.
744‚Äì755.
[16] S. F. Chen and J. Goodman, ‚ÄúAn empirical study of smoothing tech-
niques for language modeling,‚Äù in Proceedings of the 34th annualmeeting on Association for Computational Linguistics. Association
for Computational Linguistics, 1996, pp. 310‚Äì318.
[17] ‚ÄúModels of the Information Seeking Process,‚Äù http:
//searchuserinterfaces.com/book/sui
ch3 models ofinformation
seeking.html.
[18] F. Wilcoxon, ‚ÄúIndividual comparisons by ranking methods,‚Äù Biometrics
bulletin, vol. 1, no. 6, pp. 80‚Äì83, 1945.
[19] S. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. De Lucia, and T. Men-
zies, ‚ÄúAutomatic query reformulations for text retrieval in software
engineering,‚Äù in Software Engineering (ICSE), 2013 35th International
Conference on. IEEE, 2013, pp. 842‚Äì851.
[20] Q. Le and T. Mikolov, ‚ÄúDistributed representations of sentences and
documents,‚Äù in Proceedings of the 31st International Conference on
Machine Learning (ICML-14), 2014, pp. 1188‚Äì1196.
[21] X. Cao, G. Cong, B. Cui, and C. S. Jensen, ‚ÄúA generalized framework
of exploring category information for question retrieval in community
question answer archives,‚Äù in Proceedings of the 19th international
conference on World wide web. ACM, 2010, pp. 201‚Äì210.
[22] D. Shepherd, K. Damevski, B. Ropski, and T. Fritz, ‚ÄúSando: an
extensible local code search framework,‚Äù in Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software
Engineering. ACM, 2012, p. 15.
[23] D. R. Radev and W. Fan, ‚ÄúAutomatic summarization of search engine
hit lists,‚Äù in Proceedings of the ACL-2000 workshop on Recent ad-
vances in natural language processing and information retrieval: held
in conjunction with the 38th Annual Meeting of the Association for
Computational Linguistics-V olume 11. Association for Computational
Linguistics, 2000, pp. 99‚Äì109.
[24] A. Di Sorbo, S. Panichella, C. V . Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, ‚ÄúWhat would users change in myapp? summarizing app reviews for recommending software changes,‚Äù inProceedings of the 2016 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering. ACM, 2016, pp. 499‚Äì510.
[25] S. Mani, R. Catherine, V . S. Sinha, and A. Dubey, ‚ÄúAusum: approach
for unsupervised bug report summarization,‚Äù in Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software
Engineering. ACM, 2012, p. 11.
[26] B. Xu, Z. Xing, X. Xia, D. Lo, Q. Wang, and S. Li, ‚ÄúDomain-speciÔ¨Åc
cross-language relevant question retrieval,‚Äù in Proceedings of the 13th
International Conference on Mining Software Repositories. ACM,
2016, pp. 413‚Äì424.
[27] B. Xu, Z. Xing, X. Xia, D. Lo, and X.-B. D. Le, ‚ÄúXsearch: a domain-
speciÔ¨Åc cross-language relevant question retrieval tool,‚Äù in Proceedings
of the 2017 11th Joint Meeting on Foundations of Software Engineering.
ACM, 2017, pp. 1009‚Äì1013.
[28] S. Rastkar, G. C. Murphy, and G. Murray, ‚ÄúAutomatic summarization
of bug reports,‚Äù IEEE Transactions on Software Engineering, vol. 40,
no. 4, pp. 366‚Äì380, 2014.
[29] S. Gottipati, D. Lo, and J. Jiang, ‚ÄúFinding relevant answers in software
forums,‚Äù in Proceedings of the 2011 26th IEEE/ACM International
Conference on Automated Software Engineering. IEEE Computer
Society, 2011, pp. 323‚Äì332.
[30] Y . Yao, H. Tong, T. Xie, L. Akoglu, F. Xu, and J. Lu, ‚ÄúDetecting
high-quality posts in community question answering sites,‚Äù Information
Sciences, vol. 302, pp. 70‚Äì82, 2015.
716
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:19:44 UTC from IEEE Xplore.  Restrictions apply. 