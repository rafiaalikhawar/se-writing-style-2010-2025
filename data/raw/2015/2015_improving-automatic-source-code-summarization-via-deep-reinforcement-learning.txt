Improving Automatic Source Code Summarization via Deep
Reinforcement Learning
Yao Wan, Zhou Zhao
College of Computer Science and
Technology, Zhejiang University,
Hangzhou, China
{wanyao,zhaozhou}@zju.edu.cnMin Yang
Shenzhen Institutes of Advanced
Technology, Chinese Academy of
Sciences, China
min.yang@siat.ac.cnGuandong Xu
Advanced Analytics Institute,
University of Technology Sydney,
Sydney, Australia
guandong.xu@uts.edu.au
Haochao Ying, Jian Wu
College of Computer Science and
Technology, Zhejiang University,
Hangzhou, China
wujian2000@zju.edu.cn
haochaoying@zju.edu.cnPhilip S. Yu
University of Illinois at Chicago,
Illinois, USA
Institute for Data Science, Tsinghua
University, Beijing, China
psyu@uic.edu
ABSTRACT
Code summarization provides a high level natural language de-
scriptionofthefunctionperformedbycode,asitcanbenefitthesoftware maintenance, code categorization and retrieval. To the
best of our knowledge, most state-of-the-art approaches follow an
encoder-decoder framework which encodes the code into a hidden
spaceandthendecodeitintonaturallanguagespace,sufferingfrom
two major drawbacks: a) Their encoders only consider the sequen-
tialcontentofcode,ignoringthetreestructurewhichisalsocritical
for the task of code summarization; b) Their decoders are typically
trainedtopredictthenextwordbymaximizingthelikelihoodofnext ground-truth word with previous ground-truth word given.
However,itisexpectedtogeneratetheentiresequencefromscratchattesttime.Thisdiscrepancycancausean exposure bias issue,mak-
ing the learnt decoder suboptimal. In this paper, we incorporatean abstract syntax tree structure as well as sequential content of
codesnippetsintoadeepreinforcementlearningframework(i.e.,
actor-criticnetwork).Theactornetworkprovidestheconfidence
ofpredictingthenextwordaccordingtocurrentstate.Ontheother
hand, the critic network evaluates the reward value of all possible
extensions of the current state and can provide global guidance for
explorations. We employ an advantage reward composed of BLEU
metric to train both networks. Comprehensive experiments on a
real-worlddatasetshowtheeffectivenessofourproposedmodel
when compared with some state-of-the-art methods.
CCS CONCEPTS
â€¢Softwareanditsengineering â†’Documentation ;â€¢Comput-
ing methodologies â†’Natural language generation;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE â€™18, September 3â€“7, 2018, Montpellier, France
Â© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238206KEYWORDS
Code summarization, comment generation, deep learning, rein-
forcement learning
ACM Reference Format:
Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu,
andPhilipS.Yu.2018.ImprovingAutomaticSourceCodeSummarization
viaDeepReinforcementLearning.In Proceedings of the 2018 33rd ACM/IEEE
International Conference on Automated Software Engineering (ASE â€™18), Sep-
tember 3â€“7, 2018, Montpellier, France. ACM,NewYork,NY,USA, 11pages.
https://doi.org/10.1145/3238147.3238206
1 INTRODUCTION
Inthelifecycleofsoftwaredevelopment(e.g.,implementation,test-
ing and maintenance), nearly 90% of effort is used for maintenance,
and much of this effort is spent on understanding the maintenance
task and related software source codes [ 22]. Thus, documentation
whichprovidesahighleveldescriptionofthetaskperformedby
code is always a must for software maintenance. Even though vari-
oustechniques havebeen developedto facilitatethe programmer
duringtheimplementationandtestingofsoftware,documenting
codewith commentsremainsalabour-intensive task,makingfew
real-worldsoftwareprojectsadequatelydocumentthecodetore-
ducefuturemaintenancecosts[ 9,16].Itâ€™snontrivialforanovice
programmer to write good comments for source codes. A good
commentshouldatleasthasthefollowingcharacteristics:a)Cor-
rectness. The comments should correctly clarify the intent of code.
b) Fluency. The comments should be fluent natural languages that
can beeasily read and understoodby maintainers. c) Consistency.
The comments should follow a standard style/format for better
code reading. Code summarization is a task that tries to compre-
hendcodeandautomaticallygeneratedescriptionsdirectlyfrom
the source code. The summarization of code can also be viewed as
a form of document expansion. Successful code summarization can
not only benefit the maintenance of source codes [ 15,30], but also
beusedtoimprovetheperformanceofcodesearchusingnatural
language queries [32, 51] and code categorization [31].
Motivation. Recent research has made inroads towards automatic
generationofnaturallanguagedescriptionsofsoftware.Asfaras
397
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Y. Wan et al.

 




 
	


 

	





(a) An example of abstractive syntax tree (AST).	

 
	



	



	

 
		

(b)Thelimitationofmaximumlikelihoodbasedtextgeneration.
Figure1:Anillustrationofthemotivationofourpaper.Traditionalmethodssufferfromthefollowingtwolimitations:a)On
representingthecode,thestructureinformationofcodeisalwaysignored.b)Traditionalmaximumlikelihoodbasedmethodssuffer from the exposure bias issue.
weknow,mostofexistingcodesummarizationmethodslearnthese-
manticrepresentationofsourcecodesbasedonstatisticallanguage
models[30,33],andthengeneratecommentsbasedontemplates
or rules [42]. With the development of deep learning, some neural
translation models [ 2,13,15] have also been introduced for code
summarization, which mainly follow an encoder-decoder frame-
work. They generally employ recurrent neural networks (RNN e.g.,
LSTM[14])toencodethecodesnippetsandutilizeanotherRNN
to decode that hidden state to coherent sentences. These models
aretypicallytrainedtomaximizethelikelihoodofthenextword
ontheassumptionthatpreviouswordsandground-trutharegiven.
These models are limited from two aspects: a) The code sequential
and structural information is not fully utilized on feature repre-sentation, which is critical for code understanding. For example,given two simple expressions â€œ
f=a+b" and â€œ f=c+d", although
they are quite different as two lexical sequences, they share the
same structure (e.g., abstractive syntax tree). b) These models, also
termed â€œteacher-forcing", suffer from the exposure bias since in
testing time the ground-truth is missing and previously generated
wordsfromthetrainedmodeldistributionareusedtopredictthe
next word [ 37]. Figure 1(b) presents a simple illustration of the
discrepancyamongtrainingandtestingprocessintheseclassical
encoder-decoder models. In the testing phase, this exposure bias
makes error accumulated and makes these models suboptimal, not
abletogeneratethosewordswhichareappropriatebutwithlow
probability to be drawn in the training phase.Contribution.
In this paper, we aim to address these two men-
tioned issues. To effectively capture the structural (or syntactic)
informationofcodesnippets,weemployabstractsyntaxtree(AST)[
7],adatastructurewidelyusedincompilers,torepresentthestruc-
ture of program code. Figure 1ashows an example of Python code
snippetanditscorrespondingAST.Therootnodeisacomposite
nodeoftype FunctionDef ,whiletheleafnodeswhicharetyped
asNamearetokensofcodesnippets.Itâ€™sworthmentioningthatthe
tokensfromASTparsingmaybedifferentfromthosefromword
segmentation. In our paper, we consider both of them. We parse
thecodesnippetsintoASTs,andthenproposeanAST-basedLSTM
model [46] to represent the structure of code. We also use anotherLSTMmodel[ 14]torepresentthesequentialinformationofcode.
Besides, we apply a hybrid attention layer to fuse the structure
representation and sequential representation of code on predicting
theword,consideringthe alignmentbetweenpredictedwordand
source word.
To overcome the exposure bias, we draw on the insights of deep
reinforcement learning, which integrates exploration and exploita-
tion into a whole framework. Instead of learning a sequential re-
currentmodeltogreedilylookforthenextcorrectword,weutilizeanactornetworkandacriticnetworktojointlydeterminethenextbestwordateachtimestep.Theactornetwork,whichprovidesthe
confidence of predicting the next word according to current state,
serves as a local guidance. The critic network, which evaluates the
rewardvalueofallpossibleextensionsofthecurrentstate,servesasaglobalguidance.Ourframeworkisabletoincludethegoodwords
that are with low probability to be drawn by using the actor net-
work alone. To learn these two networks more efficiently, we start
withpretraininganactornetworkusingstandardsupervisedlearn-
ingwithcrossentropyloss,andpretrainingacriticnetworkwith
meansquareloss.Then,weupdatetheactorandcriticnetworks
accordingtotheadvantagerewardcomposedofBLEUmetricvia
policy gradient. We summarize our main contributions as follows.
â€¢Weproposeamorecomprehensiverepresentationmethod
for source code, with one AST-based LSTM for the structure
of source code, and another LSTM for the sequential con-
tent of source code. Furthermore, a hybrid attention layer is
applied to fuse these two representations.
â€¢To the best of our knowledge, it is the first time that we pro-
poseanadvanceddeepreinforcementlearningframework,
namedactor-criticnetwork,tocope withtheexposurebias
issueexistinginmosttraditionalmaximumlikelihood-based
code summarization frameworks.
â€¢We validateourproposedmodelonareal-worlddatasetof
108,726 Python code snippets. Comprehensive experiments
show the effectiveness of the proposed model when com-
pared with some state-of-the-art methods.
398
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Improving Automatic Source Code Summarization via Deep Reinforcement Learning ASE â€™18, September 3â€“7, 2018, Montpellier, France
Organization. Theremainderofthispaperisorganizedasfollows.
Weprovidesomebackgroundknowledgeonneurallanguagemodel,
RNNencoder-decodermodelandreinforcementlearninginSection
2for a better understanding of our proposed model. We also for-
mally define the problem in Section 2. Section 3gives an overview
ofourproposedframework.Section 4presentsahybridembedding
approach for code representation. Section 5shows our proposed
deep reinforcement learning framework (i.e., actor-critic network).
Section6describes thedataset used inour experimentand shows
theexperimentalresultsandanalysis.Section 7showssomethreats
to validity and limitations existing in our model. Section 8high-
lightssomeworksrelatedtothispaper.Finally,weconcludethis
paper in Section 9.
2 BACKGROUND
As we declared before, the code summarization task can be seen
as a text generation task given the source code. In this section, we
firstpresentsomebackgroundknowledgeontextgenerationwhich
will be used in this paper, including language model, attentional
RNN encoder-decoder model and reinforcement learning for better
decoding. To start with, we introduce some basic notations and
terminologies.Let x=(x1,x2,...,x|x|)denoteasequenceofsource
code snippet, y=(y1,y2,...,y|y|)denote a sequence of generated
words, where |Â·|denotes the length of sequence. Let Tdenote the
maximum step of decoding in the encoder-decoder framework. We
will often use notation ym...lto refer to subsequences of the form
(ym,...,yl).D={(x1,y1),(x2,y2),...,(xN,yN)}is the training
dataset, where Nis the size of training set.
2.1 Language Model
Language model computes the probability of occurrence of a num-
berofwordsinaparticularsequence.Theprobabilityofasequence
ofTwords{y1,...,yT}isdenotedas p(y1,...,yT).Sincethenum-
ber of words coming before a word, yi, varies depending on its
locationintheinputdocument, p(y1,...,yT)isusuallyconditioned
on a window of nprevious words rather than all previous words:
p(y1:T)=i=T/productdisplay
i=1p(yi|y1:iâˆ’1)â‰ˆi=T/productdisplay
i=1p(yi|yiâˆ’(nâˆ’1):iâˆ’1).(1)
Thiskind ofn-grams approachsuffers apparentlimitations [ 27,
39].Forexample,then-grammodelprobabilitiescannotbederived
directly from the frequency counts, because models derived this
way have severe problems when confronted with some n-grams
that have not been explicitly seen before.
The neural language model is a language model based on neural
networks.Unlikethen-grammodelwhichpredictsawordbased
on a fixed number ofpredecessor words, a neural language model
can predict a word by predecessor words with longer distances.
Figure2(a)showsthebasicstructureofaRNN.Theneuralnetwork
includes three layers, that is, an input layer which maps each word
toavector,arecurrenthiddenlayerwhichrecurrentlycomputes
and updates a hidden state after reading each word, and an output
layerwhichestimatestheprobabilitiesofthefollowingwordgiven
the current hidden state. The RNN reads the words in the sentence
one byone, andpredicts thepossible following wordat eachtime
step. At step t, it estimates the probability of the following wordy1

 

	

y2y3y4y5o1o2o3o4o5
o1 o2o3 o4o5
y1 y2y3 y4y5
Figure 2: RNN and Tree-RNN (adapted from [46]).
p(yt+1|y1:t)by the following steps: First, the current word ytis
mapped to a vector by the input layer e. Then, it generates the
hidden state htat timetaccording to the previous hidden state
htâˆ’1and the current input yt:
ht=f(htâˆ’1,e(yt)). (2)
Here,twocommonoptionsfor farelongshort-termmemory
(LSTM)[14]andthegatedrecurrentunit(GRU)[ 21].Finally,the
p(yt+1|y1:t)is predicted according to the current hidden state ht:
p(yt+1|y1:t)=Ð´(ht), (3)
whereÐ´isastochasticoutputlayer(typicallyasoftmaxfordiscrete
outputs) that generates output tokens.
2.2 Attentional RNN Encoder-Decoder Model
RNN encoder-decoder has two recurrent neural networks. The
encodertransformsthecodesnippet xintoasequenceofhidden
states(h1,h2,..., h|x|)withaRNN,whilethedecoderusesanother
RNN to generate one word yt+1at a time in the target space.
2.2.1 Encoder. AsaRNN,theencoderhasahiddenstate,which
isafixed-lengthvector.Atthetimestep t,theencodercomputes
the hidden state htby:
ht=f(htâˆ’1,ctâˆ’1,e(xt))). (4)
Here,fis the hidden layer which has two main options, i.e.,
LSTMandGRU.Thelastsymbolof xshouldbeanend-of-sequence
(<eos>)symbolwhichnotifiestheencodertostopandoutputthe
final hidden state hT, which is used as a vector representation of x.
2.2.2 Decoder. Theoutputofthedecoderisthetargetsequence
y=(y1,Â·Â·Â·,yT).Oneinputofthedecoderisa <start>symbol
denoting the beginning of the target sequence. At the time step
t, the decoder computes the hidden state htand the conditional
distribution of the next symbol yt+1by:
p(yt+1|yt)=Ð´(ht,ct), (5)
399
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Y. Wan et al.
whereÐ´isastochasticoutputlayerand ctisthedistinctcontext
vector for yt, computed by:
ct=|x|/summationdisplay
j=1Î±t,jhj, (6)
whereÎ±t,jis the attention weight of ytonhj[4].
2.2.3 Training Goal. The encoder and decoder networks are
jointly trained to maximize the following objective:
max
Î¸L(Î¸)=max
Î¸E
(x,y)âˆ¼Dlogp(y|x;Î¸), (7)
whereÎ¸is the set of the model parameters. We can see that this
classical encoder-decoder framework targets on maximizing the
likelihood of ground-truth word conditioned on previously gener-
atedwords.Aswehavementionedabove,themaximumlikelihood
based encoder-decoder framework suffers the exposure bias issue.
Motivatedbythis,weintroducethereinforcementlearningtech-
nique for better decoding.
2.3 Reinforcement Learning for Better
Decoding
Thereinforcementlearningisanapproachthatinteractswiththe
real environment and learns the optimal policy from the reward
signal.Ittriestogeneratetextfromscratchwithoutgroundtruthin
the testing phase. Under this approach, the text generation process
canbeviewedasaMarkovDecisionProcess(MDP) {S,A,P,R,Î³}.
IntheMDPsetting,state stattimestep tconsistsofthesourcecode
snippets xand the words/actions predicted until t,y0,y1,...,yt.
Theactionspaceisthedictionary Ythatthewordsaredrawnfrom,
i.e.,ytâŠ‚Y. With the definition of the state, the state transition
function Pisst+1={st,yt+1}, where the action yt+1becomes a
partofthenextstate st+1andthereward rt+1isreceived. Î³âˆˆ[0,1]
isthediscountfactor.Theobjectiveofgenerationprocessistofind
apolicythatmaximizestheexpectedrewardofgenerationsentence
sampled from the modelâ€™s policy:
max
Î¸L(Î¸)=max
Î¸Exâˆ¼D
Ë†yâˆ¼PÎ¸(Â·|x)[R(Ë†y,x)], (8)
whereÎ¸is the parameter of policy needed to be learnt, Dis the
trainingset, Ë†yisthepredictedactions/words,and Risthereward
function. Our problem can be formulated as follows.
Givenacodesnippet x=(x1,x2,...,x|x|),ourgoalistofinda
policythatgeneratesasequenceofwords y=(y1,y2,...,y|y|)
fromdictionaryYwiththeobjectiveofmaximizingtheexpected
reward.
Tolearnthepolicy,manyapproacheshavebeenproposed,which
aremainlycategorizedintotwoclasses[ 44].a)Thepolicy-based
approaches (e.g., REINFORCE [ 50]) which optimizes the policy
directly via policy gradient. b) The value-based approaches (e.g.,
Q-learning [ 48]) which learns the Q-function, and in each time the
agentselectstheactionwithhighestQ-value.Ithasbeenverified
thatthepolicy-basedmethodsmaysufferfromavarianceissueand
the value-based methods may suffer from a bias issue [ 17]. Thus
in our paper, we adopt the actor-critic learning method which is
a more advanced technique that has the advantage of both policy-and value-based methods.
 
	

	
		
	
		 

Figure 3: An overall workflow of getting a trained model.
3 OVERVIEW OF PROPOSED FRAMEWORK
In this section, we firstly have a simple overview on the work-
flowofhowtogetatrained modelforcodesummarization.Then
we present an overview of the network architecture of our pro-
posed deep reinforcement learning based model. Figure 3shows
the overall workflow of how to get a trained model. It includesan offline training stage and an online summarization stage. Inthe training stage, we prepare a large-scale corpus of annotated
<code,comment >pairs.Theannotatedpairsarethenfedintoour
proposed deep reinforcement learning model for training. Aftertraining, we can get a trained actor network. Then, given a code
snippet,correspondingcommentcanbegeneratedbythe trained
actor network.
Figure4isanoverviewofthenetworkarchitectureofourpro-
posed deep reinforcement learning based model. The architecture
of our model follows the actor-critic framework [ 19], which has
beensuccessfullyadoptedinthedecision-makingscenariossuch
as AlphaGo [ 41]. We split the framework into four submodules.
(a)Hybridcoderepresentation(cf.Sec. 4).Thismoduleisusedto
represent the source code into a hidden space, which is also called
encoderintheencoder-decoderframework.(b)Hybridattention
(cf.Sec.5.1.1).Ondecodingtheencodedhiddenspaceintothecom-
mentspace, theattention layeris usedtoassign differentweights
tothecodesnippettokensforbettergeneration.(c)Textgeneration
(cf. Sec.5.1.2). This module is a RNN-based generative network,
whichis usedto generatethe nextword basedonprevious gener-
ated words. (d) Critic (cf. Sec. 5.2). This module is used to evaluate
whether the generated word is good or not.
Since the generated tokens on (d) can also been seen as actions,
wecanalsocalledtheprocess(a)-(b)-(c)asactornetwork.Comparedwiththearchitectureoftraditionalencoder-decoderframework,our
proposedmodelhasanadditionalcriticmoduleusedtoevaluatethe
valueofactiontakenundercurrentstate.Theprocess(a)-(b)-(c)-(d)
can alsobe calledas criticnetwork. We cansee that the actor and
critic networks share the modules (a)-(b)-(c), reducing the number
of learning parameters a lot. We will elaborate each component in
this framework in the following sections.
4 HYBRID REPRESENTATION OF CODE
Different from previous methods that just utilize sequential tokens
to represent code, we also consider the structure information of
source code. In this section, we present a hybrid embedding ap-proach for code representation. We apply an LSTM to represent
400
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Improving Automatic Source Code Summarization via Deep Reinforcement Learning ASE â€™18, September 3â€“7, 2018, Montpellier, France
.
.
..
.
.
Code 
Snippet
(a) Hybrid code representationhstr
1
hstr2
hstr
nhstrrootÎ±str
i
ASTdstr
j
/tildewidest
. . .
Hybrid. . .
Baseline Reward(b) Hybrid attention
htxt
1htxt2htxt
nÎ±txt
idtxt
j/tildewidestst+1/tildewidersT. . . ytyt+1yT
MLP
VÏ€ VÏ€
Ï†
/tildewidestt.../tildewidest/tildewidestst+1/tildewider/tildewidersT. . . ytyt+1yT
(c) Text generation
(d) CriticNotes:
Actor network: (a)-(b)-(c)
Critic network: (a)-(b)-(c)-(d)
Figure 4: An overview of our proposed deep reinforcement learning framework for code summarization.
thelexicallevelofcode,andanAST-based LSTMtorepresentthe
syntactic level of code.
4.1 Lexical Level
Thekeyinsightintolexicallevelrepresentationofsourcecodeis
that comments are always extracted from the lexical of code, such
asthefunctionname,variablenameandsoon.Itâ€™sapparentthatwe
apply an RNN (e.g., LSTM) to represent the sequential information
of source code. In our paper, the LSTM is adopted.
4.2 Syntactic Level
Inexecutingaprogram,acompilerdecomposesaprogramintocon-
stituentsandproducesintermediatecodeaccordingtothesyntaxof
the language. AST is one type of intermediate code that represents
the hierarchical syntactic structure of a program [ 1]. We represent
thesyntacticlevelofsourcecodefromtheaspectofASTembed-ding. Similar to a traditional LSTM unit, we propose AST-based
LSTM where the LSTM unit also contains an input gate, a memory
cellandanoutputgate.DifferentfromastandardLSTMunitwhich
only has one forget gate for its previous unit, an AST-based LSTM
unitcontainsmultipleforgetgates.GivenanAST,foranynode j,
let the hidden state and memory cell of its l-th child be hjlandcjl
respectively. Refer to [46], the hidden state is updated as follows:
ij=Ïƒ(W(i)xj+N/summationdisplay
l=1U(i)
lhjl+b(i)),
fjk=Ïƒ(W(f)xj+N/summationdisplay
l=1U(f)
klhjl+b(f)),
oj=Ïƒ(W(o)xj+N/summationdisplay
l=1U(o)
lhjl+b(o)),
uj=tanh(W(u)xj+N/summationdisplay
l=1U(u)
lhjl+b(u)),
cj=ijâŠ™uj+N/summationdisplay
l=1fjlâŠ™cjl,
hj=ojâŠ™tanh(cj), (9)wherek=1,2,Â·Â·Â·,N.Eachof ij,fjk,ojandujdenotesaninput
gate, a forget gate, an output gate, and a state for updating the
memorycell,respectively. W(Â·)andU(Â·)areweightmatrices, b(Â·)
isabiasvector,and xjisthewordembeddingofthe j-thnode.Ïƒ(Â·)
isthelogisticfunction,andtheoperator âŠ™denoteselement-wise
multiplicationbetweenvectors.Itâ€™sworthmentioningthatwhen
thetreeissimplyachain,namely N=1,theAST-basedLSTMunit
reducestothestandardLSTM.Figure 2showsthestructureofRNN
and Tree-RNN.
Notice that the number of children Nvaries for different nodes
of different ASTs, which may cause problem in parameter-sharing.
Forsimplification,wetransformthegeneratedASTstobinarytrees
bythefollowingtwostepswhichhavebeenadoptedin[ 49]:a)Split
nodeswithmorethan2children,generateanewrightchildtogether
withtheoldleftchildasitschildren,andthenputallchildrenexcept
theleftmostasthechildrenofthisnewnode.Repeatthisoperation
in a top-down way until only nodes with 0, 1, 2 children left; b)
Combine nodes with 1 child with its child.
5 DEEP REINFORCEMENT LEARNING FOR
CODE SUMMARIZATION
In this section, we introduce the advanced deep learning frame-
worknamedactor-criticnetwork,whichhasbeensuccessfullyused
in the AlphaGo [ 41]. We introduce the actor and critic network
respectively and then present how to train them simultaneously.
5.1 Actor Network
Afterobtainingtherepresentation ofcodesnippet,weneedtode-
codeitintocomment.Herewedescribehowwegeneratecomment
from the hidden space with a hybrid attention layer.
5.1.1 Hybrid Attention. Different parts of the code make dif-
ferent contributions to the final output of comment. We adopt
an attention mechanism [ 4] which has been successfully used in
neural machine translation. In the attention layer, we have two
attentionscores,one Î±str
t(j)forstructuralrepresentationandan-
otherÎ±txt
t(j)for sequential representation of code. At t-th step of
thedecodingprocess,theattentionscores Î±str
t(j)andÎ±txt
t(j)are
calculated as follows:
401
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Y. Wan et al.
Î±str
t(j)=exp(hstr
jÂ·st)
/summationtextn
k=1exp(hstrkÂ·st),Î±txt
t(j)=exp(htxt
jÂ·st)
/summationtextnk=1exp(htxt
kÂ·st),
(10)
wherenisthenumberofcodetokens; h(Â·)
jÂ·stistheinnerprojectof
h(Â·)
jandst, which is usedto directly calculate the similarity score
between h(Â·)
jandst. Thet-th context vector d(Â·)
tis calculated as
the summarization vector weighted by Î±(Â·)
t(j):
dstr
t=n/summationdisplay
t=1Î±str
t(j)hstr
j,dtxt
t=n/summationdisplay
t=1Î±txt
t(j)htxt
j.(11)
To integrate the structural context vector and the textual vector,
weconcatenatethem firstlyandthenfeedthem intoanone-layer
linear network:
dt=Wdt[dstr
t;dtxtt]+bdt), (12)
where [dstr
t;dtxtt] is the concatenation of dstr
tanddtxtt. The con-
textvectoristhenusedforthe (t+1)-thwordpredictionbyputting
an additional hidden layer /tildewidest:
/tildewidest=tanh(Wc[st;dt]+bd), (13)
where [s t;dt] is the concatenation of standdt.
5.1.2 Text Generation. The model predicts the t-th word by
usingasoftmax function.Let pÏ€denoteapolicy Ï€determinedby
the actor network, pÏ€(yt|st)denote the probability distribution of
generating t-th wordyt, we can get the following equation:
pÏ€(yt|st)=softmax(Ws/tildewidest+bs). (14)
5.2 Critic Network
Unliketraditionalencoder-decoderframeworkthatgeneratesse-
quencedirectlyviamaximizinglikelihoodofnextwordgiventhe
groundtruthword,wedirectlyoptimizetheevaluationmetricssuch
asBLEU[ 34]forcodesummarization.Weapplyacriticnetworkto
approximate the value of generated action at time step t. Different
fromtheactornetwork,thiscriticnetworkoutputsasinglevalue
instead of a probabilitydistribution on each decoding step. Before
introducing critic network, we introduce the value function.
Given the policy Ï€, sampled actions and reward function, the
value function VÏ€is defined as the prediction of total reward from
thestate statsteptunderpolicy Ï€,whichisformulatedasfollows:
VÏ€(st)=Est+1:T,yt:TâŽ¡âŽ¢âŽ¢âŽ¢âŽ¢âŽ¢âŽ£Tâˆ’t/summationdisplay
l=0rt+l|yt+1,Â·Â·Â·,yT,hâŽ¤âŽ¥âŽ¥âŽ¥âŽ¥âŽ¥âŽ¦,(15)
whereTisthemaxstepofdecoding; histherepresentationofcode
snippet.Forcodesummarization,wecanonlyobtainanevaluation
score (BLEU) when the sequence generation process (or episode) is
finished.Theepisodeterminateswhenstepexceedsthemax-step
Tor generating the end-of-sequence (EOS) token. Therefore, we
define the reward as follows:
rt=/braceleftBigg0 t<T
BLEU t =To rE O S. (16)Mathematically, the critic network tries to minimize the follow-
ing loss function, where mean square error is used.
L(Ï•)=1
2/bardblex/bardblex/bardblex/bardblexVÏ€(st)âˆ’VÏ€
Ï•(st)/bardblex/bardblex/bardblex/bardblex2
, (17)
whereVÏ€(st)is the target value, VÏ€
Ï•(st)is the value predicted by
critic network and Ï•is the parameter of critic network.
5.3 Model Training
We use the policy gradient method to optimize policy directly,
which is widely used in reinforcement learning. For actor network,
the goalof training isto minimizethe negative expectedreward,
which can be defined as L(Î¸)=âˆ’Ey1,... ,Tâˆ¼Ï€(/summationtextT
l=trt), whereÎ¸
is the parameter of actor network. Denote all the parameters as
Î˜={Î¸,Ï•},thetotallossofourmodelcanberepresentedas L(Î˜)=
L(Î¸)+L(Ï•).
Forpolicygradient,itistypicallybettertotrainanexpression
of the following form according to [40]:
âˆ‡Î¸L(Î˜)=E[Tâˆ’1/summationdisplay
t=0AÏ€(st,yt+1)âˆ‡Î¸logÏ€Î¸(yt+1|st)],(18)
whereAÏ€(st,yt+1)is advantage function. The reason why we
chooseadvantagefunctionisthatitachievessmallervariancewhen
compared with some other ones such as TD residual and reward
with baseline [40].
According tothe definitionofadvantage function,we canfor-
mulate the advantage function as follows. One can refer to [ 40] for
more details.
AÏ€(st,yt)=QÏ€(st,yt)âˆ’VÏ€(st), (19)
whereQÏ€(st,yt)isthestate-actionvaluefunctionwhichisdefined
asQÏ€(st,yt)=Est+1:T,yt+1:T/bracketleftBig/summationtextTâˆ’t
l=0rt+l/bracketrightBig
.Fromthisformulation,wecan
findthattheadvantagefunctionmeasureswhetherornottheaction
isbetterorworsethanthepolicyâ€™sdefaultbehavior.Therefore,a
step in the policy gradient direction can increase the probability of
better-than-average actions and decreasethe probability of worse-
than-average actions.
Ontheotherhand,thegradientofcriticnetworkiscalculated
as follows:
âˆ‡Ï•L(Î˜)=Tâˆ’1/summationdisplay
t=0[VÏ€(st)âˆ’VÏ€
Ï•(st)]âˆ‡Ï•VÏ€
Ï•(st).(20)
Weemploystochasticgradientdescendwiththediagonalvariant
of AdaGrad [ 10] to optimize the parameters of our framework.
Algorithm 1 summarizes our proposed model described above.
6 EXPERIMENTS AND ANALYSIS
To evaluate our proposed approach, in this section, we conduct
experiments to answer the following questions:
â€¢RQ1.Doesourproposedapproachimprovetheperformance
of code summarization when compared with some state-of-
the-art approaches?
â€¢RQ2.Whatâ€™s the effectiveness of each component for our
proposed model? For example, what about the performance
402
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Improving Automatic Source Code Summarization via Deep Reinforcement Learning ASE â€™18, September 3â€“7, 2018, Montpellier, France
Algorithm 1 Actor-Critic training for code summarization.
1:Initialize actor Ï€yt+1|stand critic VÏ€(st)with random weights
Î¸andÏ•;
2:Pre-train the actor to predict ground truth ytgiven
{y1,Â·Â·Â·,ytâˆ’1}by minimizing Eq. 7;
3:Pre-train the critic to estimate V(st)with fixed actor;
4:fort=1â†’Tdo
5:Receivearandomexample,andgeneratesequenceofactions
{y1,Â·Â·Â·,yT}according to current policy Ï€Î¸;
6:Calculate advantage estimate AÏ€according to Eq. 19;
7:Update critic weights Ï•using the gradient in Eq. 20;
8:Update actor weights Î¸using the gradient in Eq. 18.
of hybrid code representation and reinforcement learning
respectively?
â€¢RQ3.Whatâ€™stheperformanceofourproposedmodelonthe
datasets with different code or comment length?
We ask RQ1 to evaluate our deep reinforcement learning-based
model compared to some state-of-the-art baselines. We ask RQ2
in order to evaluate each component of our model. We ask RQ3 to
evaluateourmodelwhenvaryingthelengthofcodeorcomment.
In the following subsections, we first describe the dataset, some
evaluation metrics and the training details. Then, we introduce
some baselines for RQ1. Finally, we report our results and analysis
for the research questions.
6.1 Dataset Preparation
We evaluate the performance of our proposed method using the
datasetin[ 6],whichisobtainedfromapopularopensourceprojects
hosting platform, GitHub1. The dataset contains 108,726 code-
commentpairs.Thevocabularysizeofcodeandcommentis50,400and31,350,respectively.Forcross-validation,Weshufflethedatasetandusethefirst60%fortraining,20%forvalidationandtheremain-
ing for testing. To construct the tree-structure of code, we parse
Python code into abstract syntax trees via ast2lib. To convert code
intosequentialtext,wetokenizethecodeby{.,"â€™:;)(!( space)},
whichhasbeenusedin[ 31].Wetokenizethecommentby{(space)}.
Figure5shows the length distribution of code and comment on
testingdata.FromFigure 5a,wecanfindthatthelengthsofmost
code snippets are located between 20 to 60. This verifies the quote
in[26]â€œFunctionsshouldhardlyeverbe20lineslong".InPython
language, the limited length should be shorter. From Figure 5b,w e
cannoticethatthelengthofnearlyallthecommentsarebetween5to 15. Thisreveals thecomment sequence that we need to generate
will not be too long.
6.2 Evaluation Metrics
We evaluate the performance of our proposed model based on
four widely-used evaluation criteria in the area of neural machine
translation and image captioning, i.e., BLEU [ 34], METEOR [ 5],
ROUGE-L [ 23] and CIDER [ 47]. BLEU measures the average n-
gramprecisiononasetofreferencesentences,withapenaltyfor
short sentences. METEOR is recall-oriented and measures how
1https://github.com/
2https://docs.python.org/2/library/ast.html0 20 40 60 80 100
Code len gth025050075010001250150017502000Count
(a) Code length distribution.0 10 20 30 40 50
Code len gth010002000300040005000600070008000Count
(b)Commentlengthdistribution.
Figure 5: Length distribution of testing data.
0 25 50 75 100 125 150
iteration0200400600800perplexity
0 50 100 150 200 250 300 350
iteration010203040reward
x50 x50
Figure 6: Iteration of training perplexity and reward.
well our model captures content from the references in our output.
ROUGE-L takes into account sentence level structure similarity
naturallyandidentifieslongestco-occurringinsequencen-grams
automatically. CIDER is a consensus based evaluation protocol for
image captioning.
6.3 Training Details
The hidden size of the encoder and decoder LSTM networks are
both set to be 512, and the word embedding size is set to be 512.
The mini-batch size is set to be 64, while the learning rate is set to
be0.001.Wepretrainbothactornetworkandcriticnetworkwith
10epochseach,andtraintheactor-criticnetworksimultaneously
10epoches.Werecordtheperplexity3/rewardevery50iterations.
Figure6shows the perplexity and reward curves of our method.
AlltheexperimentsinthispaperareimplementedwithPython2.7,
andrunonacomputerwithan2.2GHzIntelCorei7CPU,64GB
1600 MHz DDR3 RAM, and a Titan X GPU with 12 GB memory,
running Ubuntu 16.04.
6.4 RQ1: Compared to Baselines
We compare our model with the following baselines:
â€¢Seq2Seq [ 43] is a classical encoder-decoder framework in
neuralmachinetranslation,whichencodesthesourcesen-
tences into a hidden space, and decodes it into target sen-
tences.Inourcomparison,theencoderanddecoderareboth
based on LSTM.
â€¢Seq2Seq+Attn[ 4]isaderivedversionofSeq2Seqmodelwith
an attentional layer for word alignment.
â€¢Tree2Seq[ 49]followsthesamearchitecturewithSeq2Seq
andappliesAST-basedLSTMasencoderforthetaskofcode
clone detection.
3Perplexityisafunctionofcrossentropyloss,whichhasbeenwidelyusedinevaluation
of many natural language processing tasks.
403
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Y. Wan et al.
Table 1: Comparison of the overall performance between our model and previous methods. (Best scores are in boldface.)
BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDER
Seq2Seq 0.1660 0.0251 0.0100 0.0056 0.0535 0.2838 0.1262
Seq2Seq+Attn 0.1897 0.0419 0.0200 0.0133 0.0649 0.3083 0.2594
Tree2Seq 0.1649 0.0236 0.0096 0.0053 0.0501 0.2794 0.1168
Tree2Seq+Attn 0.1887 0.0417 0.0197 0.0129 0.0644 0.3068 0.2331
Hybrid2Seq+Attn+DRL (Our) 0.2527 0.1033 0.0640 0.0441 0.0929 0.3913 0.7501
Table 2: Effectiveness of each component for our proposed model. (Best scores are in boldface.)
BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDER
Seq2Seq+Attn+DRL 0.2421 0.0919 0.0513 0.0325 0.0882 0.3935 0.6390
Tree2Seq+Attn+DRL 0.2309 0.0854 0.0499 0.0338 0.0843 0.3767 0.6060
Hybrid2Seq 0.1837 0.0379 0.0183 0.0122 0.0604 0.3020 0.2223
Hybrid2Seq+Attn 0.1965 0.0516 0.0280 0.0189 0.0693 0.3154 0.3475
Hybrid2Seq+Attn+DRL (Our) 0.2527 0.1033 0.0640 0.0441 0.0929 0.3913 0.7501
â€¢Tree2Seq+Attn [11] is a derived version of Tree2Seq model
withanattentionallayer,whichhasbeenappliedinneural
machine translation
â€¢Hybrid2Seq(+Attn+DRL) represents three versions of our
proposed model with/without Attn/DRL component.
Table1showstheexperimentalresultsofcomparisonbetween
ourproposedmodelandsomepreviousones.Fromthistable,wecan
findthatourproposedmodeloutperformsotherbaselinesinalmost
all of evaluation metrics. When comparing Seq2Seq/Tree2Seq with
its correspond attention-based version, we can see that attention is
reallyeffectiveinaligningthecodetokenswithcommenttokens.
We can also find that the performance of simply encoding the tree
structure ofcode is worse thanthat of simplyencodingthe code as
sequence. This can be illustrated by that the words of comments
are always drawn from the tokens of code. Thus, our model which
considers both the structure and sequential information of code
achieves the best performance in this comparison.
6.5 RQ2: Component Analysis
Table2showstheeffectivenessofsomemaincomponentsinour
proposedmodel.Fromthistable,comparingtheresultsofSeq2Seq+
Attn/Tree2Seq+Attnwithandwithout(Table 1)deepreinforcement
learning (DRL), we can see that the proposed DRL component can
really boost the performance of comment generation for sourcecode. We can also find the proposed approach of integrating theLSTM for content and AST-based LSTM for structure is effectiveon representing the code as compared with the correspondingnon-hybridonesinTable 1.Furthermore,italsoverifiesthatour
proposed hybrid attention mechanism works well in our model.
6.6 RQ3: Parameter Analysis
Wevarythelengthofcodeandcommentsincethecodelengthmay
haveaneffectontherepresentationofcodeandthecommentlength
may have an effect on the performance of text generation. Figure 7
and Figure 8show the performance of our proposed method when
comparedwithtwobaselinesondatasetsofvaryingcodelengths
and comment lengths, respectively.FromFigure 7,wecanseethatourmodelperformsbestwhen
compared with other baselines on four metrics with respect todifferent code lengths. Additionally, we can see that the our pro-
posedmodelhasastableperformanceeventhoughthecodelength
increases dramatically. We attribute this effect to the hybrid rep-
resentation we adopt in our model. For Figure 8, recall the com-
mentlengthdistributioninFigure 5b.Sincenearlyallthecomment
lengths of testing data are under 20, we ignore the performance
analysisoverthesampleswhosecommentlengtharelargerthan
20. From this figure, we can see the performances of our model
and baselines vary dramatically on four metrics with respect to
different comment lengths.
6.7 Qualitative Analysis and Visualization
We show two examples in Table 3. Itâ€™s clear that the generated
comments by our model are closest to the ground truth. Although
those models without DRL can generate some tokens which arealso in the ground truth, they canâ€™t predict those tokens which
arenotfrequentlyappearedinthetrainingdata.Onthecontrary,
ourdeepreinforcementlearningbasedmodelcangeneratesome
tokens which are closer to the ground truth, like git,symbolic .
This canbe illustrated by thefact that ourmodel has a morecom-
prehensive exploration on the word space and optimizes the BLEU
score directly.
InTable3,wealsovisualizetwoattentionsinourproposedmodel
for the target sentences. For example, for Case 1 with target sen-
tence check if git is installed . ,wecannoticethatthestr-attn(left
of figure) focuses more on tokens like OSError ,False,git,
version , which represent the structure of code. On the other
hand, the attention of txt-attn (right of figure) is comparatively dis-persed,andhaveafocusonsometokenslike
def,whichisoflittle
significance for code summarization. This verifies our assumption
that LSTM can capture the sequential content of code, and AST-
basedLSTMcancapturethestructureinformationofcode.Thus,
itâ€™s reasonable to fuse them together for a better representation.
404
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Improving Automatic Source Code Summarization via Deep Reinforcement Learning ASE â€™18, September 3â€“7, 2018, Montpellier, France
010 20 30 40 50 60 70 80 90
Code length0.150.200.250.300.35BLEUHybrid2Seq+Attn+DRL
Tree2Seq+Attn
Seq2Seq+Attn
(a) BLEU010 20 30 40 50 60 70 80 90
Code length0.060.080.100.120.14METEORHybrid2Seq+Attn+DRL
Tree2Seq+Attn
Seq2Seq+Attn
(b) METEOR010 20 30 40 50 60 70 80 90
Code length0.280.300.320.340.360.380.400.420.440.46ROUGE -LHybrid2Seq+Attn+DRL
Tree2Seq+Attn
Seq2Seq+Attn
(c) ROUGE-L010 20 30 40 50 60 70 80 90
Code length0.00.20.40.60.81.01.21.4CIDERHybrid2Seq+Attn+DRL
Tree2Seq+Attn
Seq2Seq+Attn
(d) CIDER
Figure 7: Experimental results of our proposed method and some baselines on different metrics w.r.t. varying code length.
0 5 10 15 20 25 30 35 40
Comment length0.050.100.150.200.250.300.35BLEUHybrid2Seq+Attn+DRL
Tree2Seq+Attn
Seq2Seq+Attn
(a) BLEU0 5 10 15 20 25 30 35 40
Comment length0.060.070.080.090.100.110.120.13METEORHybrid2Seq+Attn+DRL
Tree2Seq+Attn
Seq2Seq+Attn
(b) METEOR0 5 10 15 20 25 30 35 40
Comment length0.150.200.250.300.350.400.45ROUGE -LHybrid2Seq+Attn+DRL
Tree2Seq+Attn
Seq2Seq+Attn
(c) ROUGE-L0 5 10 15 20 25 30 35 40
Comment length0.00.20.40.60.81.0CIDERHybrid2Seq+Attn+DRL
Tree2Seq+Attn
Seq2Seq+Attn
(d) CIDER
Figure8:Experimentalresultsofourproposedmethodandsomebaselinesondifferentmetricsw.r.t.varyingcommentlength.
7 THREATS TO VALIDITY AND LIMITATIONS
One threat to validity is that our approach is experimented only
on Python code collected from GitHub, so they may not be rep-resentative of all the comments. However, Python is a popular
programminglanguageusedinalargenumberofprojects.Inthe
future, we will extend our approach to other programming lan-
guages.Anotherthreattovalidityisonthemetricswechoosefor
evaluation.Ithasalwaysbeenatoughchallengetoevaluatethesim-ilarity between two sentences for the tasks such as neural machinetranslation[
43],imagecaptioning[ 18].Inthispaper,weonlyadopt
fourpopularautomaticmetrics,itisnecessaryforustoevaluate
the performance of generated text from more perspectives, such as
humanevaluation.Furthermore,inthedeepreinforcementlearn-
ing perspective, we only set the BLEU score of generated sentence
as the reward. Itâ€™s well known that for a reinforcement learningmethod, one of the biggest challenge is how to design a reward
function to measure the value of action correctly, and it is still
anopenproblem.Inourfuturework,weplantodeviseareward
function that can reflect the value of each action more correctly.
8 RELATED WORK
8.1 Deep Code Representation
With the successful development of deep learning, it has also be-
come more and more prevalent for representing source code in
thedomainofsoftwareengineeringresearch.Guetal.[ 12]usea
sequence-to-sequencedeepneuralnetwork[ 43],originallyintro-
duced for statistical machine translation, to learn intermediate dis-
tributed vector representations ofnatural language queries which
they use to predict relevant API sequences. Mou et al. [ 29] learn
distributed vector representations using custom convolutional neu-
ral networks to represent features of snippets of code, then they
assume that student solutions to various coursework problemshave been intermixed and seek to recover the solution-to-problem
mappingviaclassification.Lietal.[ 21]learndistributedvectorrep-
resentationsforthenodesofamemoryheapandusethelearned
representations to synthesize candidate formal specifications for
thecodethatproducestheheap.Piechetal.[ 36]andParisottoetal.
[35]learndistributedrepresentationsofsourcecodeinput/output
pairs and use them to assess and review student assignments or to
guideprogramsynthesisfromexamples.Neuralcode-generative
models of code also use distributed representations to capture con-
text,whichisacommonpracticeinnaturallanguageprocessing.
For example, the work of Maddison and Tarlow [ 25] and other
neurallanguagemodels(e.g.LSTMsinDametal.[ 8])describecon-
textdistributedrepresentationswhilesequentiallygeneratingcode.
Lingetal.[ 24]andAllamanisetal.[ 3]combinethecode-context
distributedrepresentationwithdistributedrepresentationsofother
modalities (e.g. natural language) to synthesize code.
8.2 Source Code Summarization
Code summarization is a novel task in the area of software engi-
neeringandhasdrawngreatattentioninrecentyears.Theexisting
worksforcodesummarizationcanbemainlycategorizedasrule
basedapproaches[ 42],statisticallanguagemodelbasedapproaches
[30] and deep learning based approaches [ 2,13,15]. Sridhara et
al.[42]construct asoftwareword usagemodelfirst, andgenerate
commentaccordingtothetokenizedfunction/variablenamesvia
rules. Movshovitz-Attias et al. [ 30] predict comments from Java
source files using topic models and n-grams. In [ 2], the authors
introduce an attentional neural network that employs convolution
ontheinputtokenstodetectlocaltime-invariantandlong-range
topicalattentionfeaturestosummarizesource code snippetsinto
short, descriptive function name-like summaries. Iyer et al. [ 15]
proposetouseLSTMnetworkswithattentiontoproducesentences
405
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE â€™18, September 3â€“7, 2018, Montpellier, France Y. Wan et al.
Table 3: Examples of code summarization generated by each model and attention visualization of our model.
Case 1 Case 2
Code snippetdef _has_git():
try: subprocess.check_call(
[git, -version],
stdout=subprocess.DEVNULL,
stderr=subprocess.DEVNULL)
except(OSError, subprocess
.CalledProcessError):return False
else: return Truedef tensor3(name=None,dtype=None):
if(dtype is None):
dtype=config.floatX
type=CudaNdarrayType(
dtype=dtype,broadcastable=
(False, False, False))
return type(name)
Ground truth check if git is installed . return a symbolic 3-d variable .
Seq2Seqhelper function to create a new figuremanager instance .yaml
Seq2Seq+Attnreturn true if the user has access to thespecified resource . a decorator that returns a new class that will returna new class name .
Tree2Seq+Attntest that validate_folders throws afoldermissingerror .helper function for #4957 .
Hybrid2Seq+Attnreturns the number of git modules that arenot installed .return the path to the currently running server .
Hybrid2Seq+Attn+DRL returns trueifgitisinstalled. returnasymbolic graph .
Attention visualization
def
subprocess
subprocess
OSError
False
'git'
'--version'
subprocess
True
<EOS>
check
if
git
is
installed
.
<EOS>
def
<unk>
try
subprocess
check_call
['git'
'--version']
stdout
subprocess
DEVNULL
stderr
subprocess
DEVNULL
except
OSError
subprocess
CalledProcessError
return
<unk>
return
<EOS>
0.00.51.0
NoneNoneconfigFalseFalsetypenamedtypeFalseCudaNdarrayTypedtypetypedtypenamedtype<EOS>
returnasymbolic3-dvariable.<EOS>
def
<unk>
name
None
dtype
None
if
dtype
is
None
dtype
config
<unk>
CudaNdarrayType
dtype
dtype
broadcastable
False
False
False
return
type
name
0.00.51.0
thatdescribeC#codesnippetsandSQLqueries.InHaijeâ€™sthesis[
13], the code summarization problem is modeled as a machine
translation task, and some translation models such as Seq2Seq [ 43]
andSeq2Seqwithattention[ 4]areemployed.Unlikepreviousstud-
ies,wetakethetreestructureandsequentialcontentofsourcecode
into consideration for a better representation of code.
8.3 Deep Reinforcement Learning
Reinforcement learning [ 19,45,50], concerned with how software
agents ought to take actions in an environment so as to maxi-
mize the cumulative reward, is well suited for the task of decision-
making. Recently, professional-level computer Go program hasbeen designed by Silver et al. [
41] using deep neural networks
and Monte Carlo Tree Search. Human-level gaming control [ 28]
has been achieved through deep Q-learning. A visual navigationsystem [
53] has been proposed recently based on actor-critic re-
inforcement learning model. Text generation can also be formu-lated as a decision-making problem and there have been several
reinforcement learning-based works on this specific tasks, includ-
ingimagecaptioning[ 38],dialoguegeneration[ 20]andsentence
simplification[ 52].Renetal.[ 38]proposeanactor-criticdeepre-
inforcement learning model with an embedding reward for image
captioning. Li et al. [ 20] integrate a developer-defined reward with
REINFORCEalgorithmfordialoguegeneration.Inthispaper,we
followanactor-criticreinforcementlearningframework,whileourfocusisonencodingthestructuralandsequentialinformationof
code snippets simultaneously with an attention mechanism.
9 CONCLUSION
In this paper, we first point out two issues (i.e., code representa-
tion and exposure bias) existing in traditional code summarization
works. To handle these two issues, we first encode the structure
and sequential content of code via AST-based LSTM and LSTMrespectively. Then we add a hybrid attention layer to integratethem together. We then feed the code representation vector into
a deep reinforcement learning framework, named actor-critic net-
work. Comprehensive experiments on a real-world dataset show
that our proposed model outperforms other competitive baselines
and achieves state-of-the-art performance on several automatic
metrics, namely BLEU, METEOR, ROUGE-L and CIDER.
ACKNOWLEDGMENTS
This work is partially supported by the Ministry of Education of
ChinaundergrantofNo.2017PT18,theNaturalScienceFoundationofChinaundergrantofNo.61672453,61773361,61473273,61602405,
theWE-DOCTORcompanyundergrantofNo.124000-11110and
the Zhejiang University Education Foundation under grantof No.
K17-511120-017.ThisworkisalsosupportedbyCCF-TencentOpen
Research Fund, NSF through grants IIS-1526499, IIS-1763325, CNS-
1626432, and NSFC 61672313.
406
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Improving Automatic Source Code Summarization via Deep Reinforcement Learning ASE â€™18, September 3â€“7, 2018, Montpellier, France
REFERENCES
[1]A. V Aho, R. Sethi, and J. D Ullman. 1986. Compilers, Principles, Techniques.
Addison Wesley 7, 8 (1986), 9.
[2]M. Allamanis, H. Peng, and C. Sutton. 2016. A convolutional attention network
forextremesummarizationofsourcecode.In International Conference on Machine
Learning. 2091â€“2100.
[3]M. Allamanis, D. Tarlow, A. Gordon, and Y. Wei. 2015. Bimodal modelling
of source code and natural language. In International Conference on Machine
Learning. 2123â€“2132.
[4]D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473 (2014).
[5]S.BanerjeeandA.Lavie.2005. METEOR:AnautomaticmetricforMTevaluation
with improved correlation with human judgments. In Proceedings of the acl
workshop on intrinsic and extrinsic evaluation measures for machine translation
and/or summarization, Vol. 29. 65â€“72.
[6]A.V.M.BaroneandR.Sennrich.2017. AparallelcorpusofPythonfunctionsand
documentationstringsfor automatedcodedocumentationandcode generation.
arXiv preprint arXiv:1707.02275 (2017).
[7]I. D. Baxter, A. Yahin, L. Moura, M. Santâ€™Anna, and L. Bier. 1998. Clone de-
tectionusingabstract syntaxtrees.In Software Maintenance, 1998. Proceedings.,
International Conference on. IEEE, 368â€“377.
[8]H.K.Dam,T.Tran,andT.Pham.2016. Adeeplanguagemodelforsoftwarecode.
arXiv preprint arXiv:1608.02715 (2016).
[9]S. C. B. de Souza, N. Anquetil, and K. M. de Oliveira. 2005. A study of the
documentation essential to software maintenance. In Proceedings of the 23rd
annual international conference on Design of communication: documenting &
designing for pervasive information. ACM, 68â€“75.
[10]J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgradient methods for online
learningandstochasticoptimization. Journal of Machine Learning Research 12,
Jul (2011), 2121â€“2159.
[11]A. Eriguchi, K. Hashimoto, and Y. Tsuruoka. 2016. Tree-to-sequence attentional
neural machine translation. arXiv preprint arXiv:1603.06075 (2016).
[12]X.Gu,H.Zhang,D.Zhang,andS.Kim.2016. DeepAPIlearning.In Proceedings of
the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software
Engineering. ACM, 631â€“642.
[13]T.Haije,B.O.K.Intelligentie,E.Gavves,andH.Heuer.2016. AutomaticComment
Generation using a Neural Translation Model. (2016).
[14]S.HochreiterandJÃ¼rgenSchmidhuber.1997. Longshort-termmemory. Neural
computation 9, 8 (1997), 1735â€“1780.
[15]S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer. 2016. Summarizing Source
Code using a Neural Attention Model.. In ACL (1).
[16]MiraKajko-Mattsson.2005. Asurveyofdocumentationpracticewithincorrective
maintenance. Empirical Software Engineering 10, 1 (2005), 31â€“55.
[17]Y.Keneshloo,T.Shi,C.KReddy,andN.Ramakrishnan.2018. DeepReinforcement
Learning For Sequence to Sequence Models. arXiv preprint arXiv:1805.09461
(2018).
[18]M. Kilickaya, A. Erdem, N. Ikizler-Cinbis, and E. Erdem. 2016. Re-evaluating
automatic metrics for image captioning. arXiv preprint arXiv:1612.07600 (2016).
[19]V. R. Konda and J. N. Tsitsiklis. 2000. Actor-critic algorithms. In Advances in
neural information processing systems. 1008â€“1014.
[20]J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky. 2016. Deep re-
inforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541
(2016).
[21]Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. 2015. Gated graph sequence
neural networks. arXiv preprint arXiv:1511.05493 (2015).
[22]B.P.LientzandE.B.Swanson.1980. Softwaremaintenancemanagement. (1980).
[23]C.Y.Lin.2004. Rouge:Apackageforautomaticevaluationofsummaries. Text
Summarization Branches Out (2004).
[24]W.Ling,E.Grefenstette,K.M.Hermann,T.KoÄisk `y,A.Senior,F.Wang,andP.
Blunsom.2016. Latentpredictornetworksforcodegeneration. arXiv preprint
arXiv:1603.06744 (2016).
[25]C.MaddisonandD.Tarlow.2014. Structuredgenerativemodelsofnaturalsource
code. In International Conference on Machine Learning. 649â€“657.
[26]R.CMartin.2009. Clean code: a handbook of agile software craftsmanship. Pearson
Education.
[27]A. Mnih and Y. W. Teh. 2012. A fast and simple algorithm for training neural
probabilistic language models. arXiv preprint arXiv:1206.6426 (2012).
[28]V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A.
Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al .2015. Human-level
control through deep reinforcement learning. Nature518, 7540 (2015), 529â€“533.
[29]L.Mou,G.Li,L.Zhang,T.Wang,andZ.Jin.2016. ConvolutionalNeuralNetworks
over Tree Structures for Programming Language Processing.. In AAAI, Vol. 2. 4.[30]D. Movshovitz-Attias and W. W. Cohen. 2013. Natural language models for
predicting programming comments. (2013).
[31]A.T.NguyenandT.N.Nguyen.2017. Automaticcategorizationwithdeepneural
networkforopen-sourceJavaprojects.In Proceedings of the 39th International
Conference on Software Engineering Companion. IEEE Press, 164â€“166.
[32]L.Nie,H.Jiang,Z.Ren,Z.Sun,andX.Li.2016. Queryexpansionbasedoncrowd
knowledge for code search. IEEE Transactions on Services Computing 9, 5 (2016),
771â€“783.
[33]Y. Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and S. Nakamura. 2015.
Learning to generate pseudo-code from source code using statistical machine
translation (t). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM
International Conference on. IEEE, 574â€“584.
[34]K.Papineni,S.Roukos,T.Ward,andW.J.Zhu.2002. BLEU:amethodforauto-
maticevaluationofmachinetranslation.In Proceedings of the 40th annual meeting
on association for computational linguistics.AssociationforComputationalLin-
guistics, 311â€“318.
[35]E. Parisotto, A. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli. 2016. Neuro-
symbolic program synthesis. arXiv preprint arXiv:1611.01855 (2016).
[36]C. Piech, J. Huang, A. Nguyen, M. Phulsuksombati, M. Sahami, and L. Guibas.2015. Learning program embeddings to propagate feedback on student code.
arXiv preprint arXiv:1505.05969 (2015).
[37]M. Ranzato,S. Chopra,M. Auli,and W. Zaremba.2015. Sequenceleveltraining
with recurrent neural networks. arXiv preprint arXiv:1511.06732 (2015).
[38]Z.Ren,X.Wang,N.Zhang,X.Lv,andL.J.Li.2017. DeepReinforcementLearning-
BasedImageCaptioningwithEmbeddingReward.In Computer Vision and Pattern
Recognition (CVPR), 2017 IEEE Conference on. IEEE, 1151â€“1159.
[39]R. Rosenfeld. 2000. Two decades of statistical language modeling: Where do we
go from here? Proc. IEEE 88, 8 (2000), 1270â€“1278.
[40]J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. 2015. High-
dimensional continuous control using generalized advantage estimation. arXiv
preprint arXiv:1506.02438 (2015).
[41]D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J.
Schrittwieser,I.Antonoglou,V.Panneershelvam,M.Lanctot,andS.Dieleman.
2016. Mastering the game of Go with deep neural networks and tree search.
Nature529, 7587 (2016), 484â€“489.
[42]G.Sridhara,E.Hill,D.Muppaneni,L.Pollock,andK.Vijay-Shanker.2010. To-
wardsautomaticallygeneratingsummarycommentsforjavamethods.In Proceed-
ings of the IEEE/ACM international conference on Automated software engineering.
ACM, 43â€“52.
[43]I. Sutskever,O. Vinyals, and Q.V. Le. 2014. Sequence to sequence learningwith
neural networks. In Advances in neural information processing systems . 3104â€“
3112.
[44]R. S Sutton and A. G Barto. 1998. Introduction to reinforcement learning. Vol. 135.
MIT press Cambridge.
[45]R.SSutton,D.AMcAllester,S.PSingh,andY.Mansour.2000. Policygradient
methodsfor reinforcementlearningwith functionapproximation.In Advances
in neural information processing systems. 1057â€“1063.
[46]K. S. Tai, R. Socher, and C. D. Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory networks. arXiv preprint
arXiv:1503.00075 (2015).
[47]R. Vedantam, C. Lawrence Zitnick, and D. Parikh. 2015. Cider: Consensus-based
imagedescriptionevaluation.In Proceedings of the IEEE conference on computer
vision and pattern recognition. 4566â€“4575.
[48]C. J. Watkins and P. Dayan. 1992. Q-learning. Machine learning 8, 3-4 (1992),
279â€“292.
[49]H. H. Wei and M. Li. 2017. Supervised Deep Features for Software Functional
Clone Detection by Exploiting Lexical and Syntactical Information in Source
Code. (2017).
[50]R. J Williams.1992. Simple statistical gradient-following algorithmsfor connec-
tionist reinforcement learning. In Reinforcement Learning. Springer, 5â€“32.
[51]D. Yang, A. Hussain, and C. V. Lopes. 2016. From query to usable code: An
analysis of stack overflow code snippets. In Mining Software Repositories (MSR),
2016 IEEE/ACM 13th Working Conference on. IEEE, 391â€“401.
[52]X.ZhangandM.Lapata.2017. SentenceSimplificationwithDeepReinforcement
Learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing. 584â€“594.
[53]Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A Gupta, L. Fei-Fei, and A. Farhadi.
2017. Target-drivenvisualnavigationinindoorscenesusingdeepreinforcement
learning. In Robotics and Automation (ICRA), 2017 IEEE International Conference
on. IEEE, 3357â€“3364.
407
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. 