A Feasibility Study of Using Automated Program Repair for
Introductory Programming Assignments
Jooyong Yi‚àó
Innopolis University, Russia
j.yi@innopolis.ruUmair Z. Ahmed
Indian Institute of Technology
Kanpur, India
umair@cse.iitk.ac.inAmey Karkare
Indian Institute of Technology
Kanpur, India
karkare@cse.iitk.ac.in
Shin Hwei Tan
National University of Singapore,
Singapore
shinhwei@comp.nus.edu.sgAbhik Roychoudhury
National University of Singapore,
Singapore
abhik@comp.nus.edu.sg
ABSTRACT
Despite the fact an intelligent tutoring system for programming
(ITSP) education has long a/t_tracted interest, its widespread use has
been hindered by the diÔ¨Éculty of generating personalized feedback
automatically. Meanwhile, automated program repair (APR) is an
emerging new technology that automatically /f_ixes so/f_tware bugs,
and it has been shown that APR can /f_ix the bugs of large real-world
so/f_tware. In this paper, we study the feasibility of marrying intelli-
gent programming tutoring and APR. We perform our feasibility
study with four state-of-the-art APR tools (GenProg, AE, Angelix,
and Prophet), and 661 programs wri/t_ten by the students taking
an introductory programming course. We found that when APR
tools are used out of the box, only about 30% of the programs in
our dataset are repaired. /T_his low repair rate is largely due to the
student programs o/f_ten being signi/f_icantly incorrect ‚Äî in contrast,
professional so/f_tware for which APR was successfully applied typ-
ically fails only a small portion of tests. To bridge this gap, we
adopt in APR a new repair policy akin to the hint generation policy
employed in the existing ITSP. /T_his new repair policy admits par-
tial repairs that address part of failing tests, which results in 84%
improvement of repair rate. We also performed a user study with
263 novice students and 37 graders, and identi/f_ied an understudied
problem; while novice students do not seem to know how to eÔ¨Äec-
tively make use of generated repairs as hints, the graders do seem
to gain bene/f_its from repairs.
CCS CONCEPTS
‚Ä¢Applied computing ‚ÜíComputer-assisted instruction; ‚Ä¢So/f_t-
ware and its engineering ‚ÜíSo/f_tware testing and debugging;
KEYWORDS
Intelligent Tutoring System, Automated Program Repair
‚àó/T_he /f_irst author did part of this work at National University of Singapore.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro/f_it or commercial advantage and that copies bear this notice and the full citation
on the /f_irst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permi/t_ted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speci/f_ic permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE‚Äô17, Paderborn, Germany
¬©2017 ACM. 978-1-4503-5105-8/17/09. . . $15.00
DOI: 10.1145/3106237.31062621 INTRODUCTION
Developing and using intelligent tutoring system for novice pro-
grammers has gained renewed a/t_tention recently [ 8,10,12,13,15,
36,39,40]. /T_he typical goal of an intelligent tutoring system for
programming (ITSP) is to /f_ind bugs in student programs and pro-
vide proper feedback for the students to help them correct their
programs. An ITSP can also be used to help human tutors deal
with many diÔ¨Äerent student programs eÔ¨Éciently. While an ITSP
for novice programmers has already existed since at least the early
80s [ 42], it has not been widely adopted in the education /f_ield. /T_he
main diÔ¨Éculty of building an eÔ¨Äective ITSP is in the high degree
of variations of student programs, which makes it challenging to
automatically generate personalized feedback, without requiring
additional help from the instructor. Despite this diÔ¨Éculty, with the
advent of Massive Open Online Course (MOOC) and increasing
interest in end-user programming, the need for an eÔ¨Äective ITSP
has never been greater. With the technological advances made
during the last more than three decades since an early prototype
system Meno-II [ 42] was introduced, it may now be possible to
realize the widespread use of ITSP.
Automated program repair (APR) is an emerging new technology
that has recently been actively researched [ 9,11,17,21,22,24,25,
30,33,46,49]. An APR system /f_ixes so/f_tware bugs automatically,
only requiring a test suite that can drive the repair process. Failing
tests in the test suite become passing a/f_ter repair, which manifests
as a bug /f_ix. APR was originally developed to /f_ix professionally de-
veloped large so/f_tware, and an APR tool, Angelix, recently reported
and automated the /f_ix of the Heartbleed bug [ 25]. In this paper, we
seek to study the inter-play between APR and ITSP.
Given that student programs are much simpler than profes-
sionally developed so/f_tware, applying APR to student programs
may seem achievable. However, when we apply four state-of-the-
art APR tools, namely GenProg [ 20], AE [ 46], Angelix [ 25], and
Prophet [ 22] to 661 student programs (obtained from an introduc-
tory programming course oÔ¨Äered by the third author at Indian
Institute of Technology Kanpur), repairs are generated only for
31% of these programs. /T_he remaining about 70% of the student
programs in our dataset are not repaired by any of the four tools.
One of the main reasons for a low repair rate is that student
programs are o/f_ten severely incorrect, and fail the majority of the
tests. In our dataset, 60% of the programs fail more than half of
the available tests. /T_his is in contrast to the fact that professionalso/f_tware for which APR was successfully applied typically fails
only a small portion of tests. To rectify an incorrect program that
fails the majority of tests, it is o/f_ten necessary to make sizable
changes to the program. Indeed, about half of the programs in our
dataset require more than one hunk of changes to reach the correct
programs (our dataset contains a corresponding correct program
for each incorrect program). However, the current APR tools can
/f_ix only a small number of lines; most successful repairs reported in
the literature change a small number of lines, and some tools such
as SPR [ 21] and Prophet [ 22] even restrict the change to a single
line. Given these discrepancies, it seems infeasible to use APR tools
for the purpose of tutoring programming.
DiÔ¨Äerence between Bug Fixing and Program Tutoring. While
we report in this paper APR tools‚Äô weak capability to /f_ix novice
student programs, showing a correct program to a student is not
necessarily the best way to provide students with feedback. In fact,
experienced human tutors show an answer only selectively when
students make simple errors such as syntactic errors [ 28]. For more
complex errors such as semantic errors, human tutors, in general,
do not directly correct the error; instead, they give students hints.
/T_hat way, tutors can help students move toward a correct answer.
Partial Repairs as Hints. Considering this diÔ¨Äerence between
bug /f_ixing and program tutoring, we explore the possibility of
using APR tools for the purpose of generating hints, for the sake
of teaching programming to students. When student programs
fail multiple tests, we change the repair policy of APR tools as
follows. Given an incorrect student program P, a repair candidate
P0is returned as a repair if (1) all previously passing tests still
pass with P0, and (2) at least one of previously failing tests passes
with P0. We call such as repair a partial repair , distinguishing it
from the complete repair that passes all tests following the original
repair policy of APR. By comparing a generated partial repair with
the incorrect program, students can see when a particular test
fails or passes, which can help a student understand why his or
her program fails the test addressed by the partial repair. Since a
generated partial repair Ris specialized for the tests addressed by
R, the expected usage of partial repairs is to encourage students
to modify their own incorrect program by taking account of the
partial repair, rather than blindly accepting it.
We note that our partial repair is conceptually similar to the
‚Äúnext-step hint‚Äù advocated in the education /f_ield [ 2,31,34,35,37,
38]. By looking at a next-step hint, students can make forward
progress toward an answer. In contrast, recent automated feedback
generation techniques that appeared in the so/f_tware engineering
and programming languages /f_ields [ 15,39,40] are evaluated under
a restricted assumption that student programs are almost correct.
To facilitate the use of partial repairs as hints, our modi/f_ied re-
pair strategy generates one of the following two forms of repairs.
/T_he /f_irst kind of a partial repair is: if (E)fSg, where Sis a modi-
/f_ied/added/deleted statement and Eis the guard expression for S.
When such a form of a repair is generated, the student can obtain a
hint about a data-/f_low change by observing the modi/f_ication/addi-
tion/deletion of S, along with an additional hint about when that
data-/f_low change is necessary by observing the guard E. /T_he sec-
ond kind of a partial repair modi/f_ies only conditional expressions,
which gives students a hint about control-/f_low changes.Improved Feedback Rate. A/f_ter changing the repair policy (al-
lowing partial repairs) and the repair strategy, feedback rate (repair
generation rate) signi/f_icantly improves, showing 84% improvement.
In about 60% of the programs in our dataset, either complete or
partial repairs are generated. By analyzing the remaining cases
where repairs are not generated, we identify a few common reasons
for repair failure ‚Äî the two most common reasons being the need
for output string modi/f_ication and array modi/f_ication for which
the current APR tools are not specialized. It would be most cost
eÔ¨Äective to strengthen repair operators that can manipulate strings
and arrays in future APR tools.
User Study. A high feedback rate is only one necessary condition
for using APR for programming tutoring. To see whether auto-
matically generated repairs actually help students and graders, we
perform a user study with 263 students taking an introductory C
programming course and 37 teaching assistants (TAs) of the same
course, part of whose duty is to grade student assignments. In
our user study, students‚Äô problem solving time increases when
generated repairs are provided as hints, whereas TAs‚Äô grading per-
formance improves. /T_his diÔ¨Äerence seems to be due to that repairs
generated by APR tools over/f_it the provided test-suite, which is the
well-known problem in APR [ 41]. While TAs can, in general, spot
the problems of the incorrect student program based on suggested
repairs, novice students are likely to be distracted by the overly spe-
cialized suggestions. To transform automatically generated repairs
into feedback that can actually help students, post-processing of
generated repairs seems necessary, while answering the question
about which form of feedback is bene/f_icial for students remains a fu-
ture challenge. Note that even if the ideal correct repair for a given
student program is available, post-processing is still necessary to
give the student a hint, not a solution.
Our Contributions. In our feasibility study of using APR for
introductory programming assignments, we found that:
/T_he current state-of-the-art APR tools more o/f_ten than not fail
to generate a repair.
However, they can, more o/f_ten than not, generate partial repairs
that pass part of previously failing tests. Generating partial
repairs are analogous to that human tutors guide the students
gradually toward the answer by giving them hints.
Failure of APR is o/f_ten due to a few common reasons such as the
weak ability of APR tools to change the output string.
Automatically generated repairs seem to help TAs grade student
programs more eÔ¨Éciently.
However, novice students do not seem to know how to eÔ¨Äectively
make use of suggested repairs to correct their programs.
Overall, it seems feasible to use APR tools for the purpose of
tutoring introductory programming, given that repairs can be gen-
erated more o/f_ten than not a/f_ter tailoring APR tools, and further
improvement seems possible by addressing a few common reasons
for repair failure. To facilitate further research, we share our dataset
containing 661 real student programs, our toolchain implement-
ing the partial-repair policy/strategy, and our user-study materials
in the following URL: h/t_tps://github.com/jyi/ITSP. A summary
description is available in Section 11.Table 1: Characteristics of our dataset
Lab # Prog Topic
Lab 3 63 Simple Expressions, printf, scanf
Lab 4 117 Conditionals
Lab 5 82 Loops, Nested Loops
Lab 6 79 Integer Arrays
Lab 7 71 Character Arrays (Strings) and Functions
Lab 8 33 Multi-dimensional Arrays (Matrices)
Lab 9 48 Recursion
Lab 10 53 Pointers
Lab 11 55 Algorithms (sorting, permutations, puzzles)
Lab 12 60 Structures (User-De/f_ined data-types)
2 AUTOMATED PROGRAM REPAIR
We perform a feasibility study with the following four state-of-the-
art APR tools: GenProg [ 20], AE [ 46], Prophet [ 22], and Angelix [ 25].
/T_hese four tools, similar to the majority of APR tools, are test-
driven, meaning that a modi/f_ied program P0is considered repaired
ifP0passes all tests in the provided test suite. GenProg repeatedly
modi/f_ies the program using genetic programming [ 18] until it /f_inds
a repair or the time budget is exhausted. In contrast to GenProg
where the program is modi/f_ied in a stochastic fashion (the program
is modi/f_ied diÔ¨Äerently at each run of the tool), AE modi/f_ies the
program in a deterministic way by applying mutation operators to
the program. Prophet /f_irst searches for a transformation schema
that can be used to repair the program, and in the next step, it
instantiates the transformation schema to generate a repair. In
the second step of schema instantiation, Prophet uses a repair
model learned from successful human patches to prioritize the
instantiation similar to human patches. Angelix /f_irst searches for
a set of angelic values for potentially buggy expressions E; when
these angelic values substitute E, all tests are passed. In the next
step, Angelix synthesizes patch expressions that return the angelic
values found in the /f_irst step. /T_hese four APR tools, while sharing
the goal of generating repairs that pass all tests, internally use
diÔ¨Äerent repair algorithms and repair operators. We include these
diÔ¨Äerent APR tools in our study to gain holistic understanding of
the feasibility of using APR tools for programming tutoring.
3 DATASET
/T_he dataset on which we perform and report our analysis was
obtained from an Introductory C Programming (CS-101) course
oÔ¨Äered at Indian Institute of Technology Kanpur (IIT-K) by the
third author. /T_he programs were collected using Prutor [6], a sys-
tem that stores intermediate versions of programs in addition to
the /f_inal submissions. /T_his course was credited by 400+ /f_irst year
undergraduate students. One of the major grading component was
weekly programming assignments (termed Lab). /T_he assignments
were designed around a speci/f_ic topic every week, as described in
Table 1, so as to test the concepts learned so far. /T_he labs were
conducted in an environment where we recorded the sequence of
submissions made by students towards the goal of passing as many
pre-de/f_ined test-cases as possible. Multiple a/t_tempts were allowed,
with only the last submission being graded. For each of these labs,Table 2: /T_he result of our initial experiment in which the
existing APR tools are used out of the box. /T_he overall repair
rate is 31%.
Lab # Programs # Fixed Repair Rate Time
Lab 3 63 3 5 % 6 s
Lab 4 117 30 26 % 20 s
Lab 5 82 27 33 % 89 s
Lab 6 79 32 41 % 50 s
Lab 7 71 17 24 % 75 s
Lab 8 33 16 48 % 139 s
Lab 9 48 15 31 % 46 s
Lab 10 53 24 45 % 24 s
Lab 11 55 26 47 % 83 s
Lab 12 60 18 30 % 38 s
Total 661 208 31 % 59 s
we pick a random sample of ( Pb,Pc) program pairs as our dataset,
where Pbis a version of student program which fails on one or
more test-cases, and Pcis a later version of the a/t_tempt by the same
student which passes all the provided test-cases. We exclude from
our dataset the instances of Pbfailed to be compiled. /T_he second
column of Table 1 shows the number of programs for each lab we
include in our dataset.
4 INITIAL FEASIBILITY STUDY
How o/f_ten can the state-of-the-art APR tools /f_ix incorrect student
programs? A high repair rate of APR is a prerequisite to using
APR tools for feedback generation. As the /f_irst step of our feasibil-
ity study, we investigate how well four state-of-the-art APR tools
(i.e., GenProg, AE, Prophet, and Angelix) /f_ix the incorrect student
programs in our dataset. For each incorrect program, a repair is
considered found if one of the four APR tools successfully generates
a repair ‚Äî that is, a generated repair passes all provided tests of the
program. We run the four APR tools in parallel until either (a) one
of the APR tools successfully generates a repair or (b) all APR tools
fail to generate a repair within a time limit (15 minutes). We use the
default con/f_iguration of each APR tool with slight modi/f_ications for
Prophet to extend the search space of repair [ 23]. Our experiment
was performed on an Intel Xeon E5-2660 2.60 Ghz processor with
Ubuntu 14.04 64-bit operating system and 62 GB of memory.
4.1 Results of Initial Experiment
Table 2 shows the results of our initial experiment. Each column rep-
resents (from le/f_t to right) the lab for which the incorrect programs
were submi/t_ted (Lab), the number of incorrect programs submi/t_ted
to the lab (# Programs), the number of incorrect programs in the lab
that are /f_ixed by the APR tools we apply (# Fixed), repair rate, i.e., (#
Fixed)/(# Programs) in percentage (Repair Rate), and average time
taken to successfully generate repairs (Time), respectively. In our
experiments, repairs are generated only in 31% of the programs in
our benchmark, and repair rate is below 50% across each individual
lab. Meanwhile, the average time taken when repairs are found is
about 1 minute. Our initial experimental result suggests that a low
repair rate is a severe concern.0255075
Lab 3 Lab 4 Lab 5 Lab 6 Lab 7 Lab 8 Lab 9 Lab 10 Lab 11 Lab 12 TotalGroup High failure rate Low failure rateFigure 1: /T_his plot shows the repair rate of two diÔ¨Äerent
groups (Y axis) across each individual lab (X axis). /T_he ‚ÄúHigh
failure rate‚Äù group consists of the cases in which more than
half of the tests fail the given program, whereas the ‚ÄúLow
failure rate‚Äù group consists of the cases in which at least half
of tests pass the given program. Repair rate is signi/f_icantly
lower in the high failure rate group to which 60% of the pro-
grams in our dataset belong.
4.2 Reasons for Low Repair Rate
Despite the fact that student programs are simpler than programs
wri/t_ten by professional developers for which APR tools are devel-
oped, the state-of-the-art APR tools fail to generate repairs for the
majority of the incorrect program in our benchmark. Our result
suggests that /f_ixing short student programs is not easier than /f_ixing
developer programs. What makes automatically /f_ixing student pro-
grams diÔ¨Écult? Answering this question may help us adjust APR
to the new challenge posed by student programs. We observe in
our dataset that the following two properties of student programs
are likely to make automatically /f_ixing student programs diÔ¨Écult:
(1) student programs o/f_ten fail in a majority of the tests, and (2)
student programs o/f_ten require complex /f_ixes. We describe them in
more detail in the following sections.
4.2.1 High test failure rate. Student programs are o/f_ten signi/f_i-
cantly incorrect, and fail the majority of the tests. In our dataset,
60% of the programs fail more than half of the available tests. /T_his
is in contrast to the fact that professional so/f_tware for which APR
was successfully applied typically fails only a small portion of tests.
High test failure rate is likely to make automated program repair
diÔ¨Écult. Figure 1 compares the repair rate between the following
two groups of our benchmark programs: the high test failure group
in which more than half of the tests fail the given program and the
low test failure group where at least half of tests pass the given
program. While about half (48%) of the programs of the low failure
rate group are successfully repaired, the repair rate of the high
failure rate group is only 20%.
4.2.2 Complex fixes. /T_he majority of bugs reported to be suc-
cessfully repaired by APR tools are cosmetically simple, mostly
restricted to one-line changes of the given buggy program. Still,the promise of APR is that it can save developers from manual
search for a simple /f_ix in large so/f_tware. To investigate the dis-
tribution between simple /f_ixes (one-hunk changes) and complex
/f_ixes (multiple-chunk changes) in our dataset, we compare each
incorrect program in our dataset with its correct version. Recall
that our dataset contains both an incorrect program and its correct
version wri/t_ten by the same student. In our dataset, about half of
the incorrect programs (46%) are /f_ixed by adding more than 1 hunk
of changes. For these programs requiring complex /f_ixes, the repair
rate is shown to be 26%, lower than the repair rate for the rest of
the programs (36%).
5 TUTORING PROGRAMMING
Our initial experiment reveals that repair rate of the current APR
tools for novice student programs is prohibitively low. Does this
imply that it is infeasible to use APR for intelligent programming
tutoring (IPT)? Or, given that APR was originally not developed for
IPT, is it possible to tune up APR for the purpose of IPT?
One big diÔ¨Äerence between /f_ixing a bug and tutoring program-
ming is in the diÔ¨Äerent degrees of their interactivity with the users.
Tutoring is a highly interactive process between a tutor and a stu-
dent. To complete a program, a student takes multiple steps of
actions, and at each step, the tutor provides feedback. /T_he tutor
oÔ¨Äers a con/f_irmatory feedback if the student follows the right track
toward a correct solution. Meanwhile, if the student goes astray,
the tutor provides a hint for the student to get the student back
on track. In this highly interactive tutoring process, the tutor does
not simply show a correct program all at once. Instead, the tutor
provides for the student a series of feedback to help the student
stay on track toward a correct solution. /T_his behavior of human
programming tutors is recorded in detail in [ 28]. Intelligent tu-
toring systems expected to mimic human tutors should provide
interactive feedback for the students where each feedback should
help the students move to the next step toward a correct solution.
In contrast, the ideal of APR is to synthesize a correct bug /f_ix at
once, without involving a long feedback loop with the developer.
Given this diÔ¨Äerence between bug /f_ixing and programming tutor-
ing, we believe APR can be used for intelligent tutoring only a/f_ter
it is tailored to the new needs of programming tutoring.
6 FROM BUG FIXING TOWARD TUTORING
Given the diÔ¨Äerence between APR and IPT described in the previous
section, the problem of APR and the problem of IPT can be described
diÔ¨Äerently as follows.
De/f_inition 6.1 (Automated Program Repair (APR)). Given a pro-
gram Pand its speci/f_ication S, the following holds true initially,
re/f_lecting the fact that Pis buggy: P0S. /T_he problem of APR is to
generate an alternative program P0that satis/f_ies P0`S.
De/f_inition 6.2 (Intelligent Programming Tutoring (IPT)). Given a
program Pand its speci/f_ication Swhere P0S, the problem of IPT is
to generate a series of alternative programs, P0
1;P0
2; : : : ; P0
k;P0
k+1; : : : ;
P0n;P0
n+1that satis/f_ies the following, through an iterative interaction
with the student.
(1)For all odd numbers k,P0
kis an automatically generated pro-
gram by the tutoring system, and P0
k+1is a program constructed
by the student, using P0
kas a hint.(2)81kn:P0
kP0
k+1, where P0
kP0
k+1denotes that program
P0
k+1is closer to the speci/f_ication Sthan P0
k(one de/f_inition of
will be described later).
(3)81kn:P0
k0S
(4)P0
n+1`S
Notice that in IPT, the /f_inal correct version of the program ( P0
n+1)
is sought for through a series of feedback generation (represented by
P0
kfor all odd numbers k), interspersed with student programming
(represented by P0
k+1for all odd numbers k).
We describe in the following how we tailor APR to IPT. In par-
ticular, we tailor test-driven APR, given that the majority of APR
tools use a test-driven approach. In test-driven APR, a test suite is
used as the speci/f_ication of the program. /T_hat is, given a test suite
Tand a buggy program Pwhere Pdoes not pass all tests in T(i.e.,
P0T), test-driven APR generates a repaired program P0satisfying
P0`T, which denotes P0passes all tests in T.
6.1 Tailoring Repair Policy
We tailor ITP described in Def. 6.2 to test-driven APR as follows.
First, we replace in Def. 6.2 speci/f_ication Swith test suite T. /T_hus,
intermediate programs P0
kdo not pass all tests in T(P0
k0T), while
they are gradually approaching the /f_inal version that passes all tests.
Meanwhile, the partial relation used in Def. 6.2 can be naturally
de/f_ined as follows. We say P0
kP0
k+1if all tests passed in P0
kalso
pass in P0
k+1. Similar to test-driven development, the progression
of the student can be achieved by gradually passing more tests.
While the number of tests may not be a precise measure of student
progression, its practicality is high, given that tests are widely used
in evaluating student programs. Related but orthogonal issues are
how to construct an eÔ¨Äective test suite for the purpose of IPT, and
in which order each test should be satis/f_ied by the program; for
instance, given multiple failing tests Tfand an incomplete program,
which tests among Tfshould be addressed /f_irst? /T_hese orthogonal
issues are not addressed in this initial feasibility study.
To implement progressive program construction ( P0
1P0
2
: : :P0n), we modify the repair policy of APR as follows. Taking
as input a student program P0
k, we generate P0
k+1that satis/f_ies P0
k
P0
k+1. Note that it is not required for P0
k+1to pass all tests, unlike in
the original APR. /T_his diÔ¨Äerent repair policy can be implemented
in an APR tool in a straightforward way by generating a partial
repair de/f_ined as follows.
De/f_inition 6.3 (Partial Repair). Given npositive tests, where n0,
andmnegative tests, where m>0, a partial repair P0satis/f_ies the
following:
(1)P0passes all npositive tests, and
(2)P0passes at least one of mnegative tests.
In comparison, we de/f_ine a complete repair generated in the
original APR as follows.
De/f_inition 6.4 (Complete Repair). Given npositive tests, where
n0, and mnegative tests, where m>0, a complete repair P0
satis/f_ies the following:
(1)P0passes all npositive tests, and
(2)P0passes all mnegative tests.
/T_he expected usage of partial repairs is to encourage students
to modify their own incorrect program by taking into account thepartial repair as a hint. In fact, a partial repair is specialized for the
tests it addresses (the tests that turn from negative to positive a/f_ter
the partial repair), the student needs to generalize the partial solu-
tion shown to him or her. By comparing a generated partial repair
with the incorrect program, students can see when a particular test
fails or passes, which can help a student understand why his or her
program fails the test addressed by the partial repair.
6.2 Tailoring Repair Strategy
Partial repairs are generated as hints, not as solutions. Typical hints
partial repairs can provide are as follows.
(1)Control-/f_low hints. Students can see that a test can pass by
changing the control /f_low of the program ‚Äî which includes
changing the direction of an if-conditional, skipping over a
loop, and exiting a loop at a diÔ¨Äerent iteration than before.
(2)Data-/f_low hints. Students can see that a test can pass by
adding or deleting statements which aÔ¨Äects the data /f_low of
the program.
(3)Conditional data-/f_low hints. It is o/f_ten the case that the data-
/f_low of the program should be changed only under a certain
circumstance. In this case, statement addition/deletion can
be guarded with a condition. /T_he deleted/added statements
provide data-/f_low hints, while the guard conditions provide
control-/f_low hints. Note that a data-/f_low hint can be viewed as
a special case of a conditional data-/f_low hint where a statement
Sis guarded with either false (suggesting the deletion of S) or
true (suggesting the addition of S).
To facilitate the use of partial repairs as hints, we tailor the re-
pair strategy of APR, following Algorithm 1. Our repair strategy
searches for a control-/f_low hint and a conditional data-/f_low hint
in parallel (a data-/f_low hint is the special case of a conditional
data-/f_low hint). /T_his parallel use of tools is shown in Line 2 of the
algorithm: /c.sc/o.sc/n.sc/t.sc/r.sc/o.sc/l.scF/i.sc/x.sc( Pb,Tp,Tn)jj/c.sc/o.sc/n.sc/d.scD/a.sc/t.sc/a.scF/i.sc/x.sc( Pb,Tp,Tn), where
Pb,Tp, and Tnrepresent an input buggy program, positive tests
(passing tests), and negative tests (failing tests), respectively. Func-
tion /c.sc/o.sc/n.sc/t.sc/r.sc/o.sc/l.scF/i.sc/x.sc and /c.sc/o.sc/n.sc/d.scD/a.sc/t.sc/a.scF/i.sc/x.sc search for a partial repair that can
be used as a control-/f_low hint and a (conditional) data-/f_low hint,
respectively. Parallel search for a partial repair stops when either a
repair is found or the time budget is exhausted.
In function /c.sc/o.sc/n.sc/t.sc/r.sc/o.sc/l.scF/i.sc/x.sc by which a control-/f_low hint is searched
for, we invoke in parallel two APR tools, Angelix and Prophet, both
of which have repair operators that can modify the conditional ex-
pressions of the if/loop statements. We restrict the repair space only
to conditional expression changes when looking for a control-/f_low
hint. Meanwhile, in function /c.sc/o.sc/n.sc/d.scD/a.sc/t.sc/a.scF/i.sc/x.sc by which a conditional
data-/f_low hint is searched for, we use the following two-step repair
process. In the /f_irst step, we modify the data-/f_low of the program
by adding/deleting/modifying statements such that one of the neg-
ative tests becomes positive a/f_ter the modi/f_ication. At this step, we
do not preserve positive tests; that is, the modi/f_ied program Piin
line 9 may fail some/all of positive tests. However, in the second
step, we re/f_ine Pisuch that the re/f_ined program Pr(obtained in
either line 12 or 17) passes all positive tests. More speci/f_ically, our
re/f_inement process takes place as follows. Given a statement Sthat
is added or deleted in the /f_irst step, we transform Sinto ‚Äú if (true)Algorithm 1 Partial Repair Generation Using Our Repair Strategy
Input: buggy program Pb, test suite T
Output: partially repaired program Pr
.RunPbwith Tto /f_ind out positive tests Tpand negative tests Tn.
1: (Tp;Tn) /r.sc/u.sc/n.sc(Pb,T)
.Parallel call. Successful termination of one function (termination with
a non-NULL value) kills the remaining function.
2:Pr /c.sc/o.sc/n.sc/t.sc/r.sc/o.sc/l.scF/i.sc/x.sc (Pb,Tp;Tn)jj/c.sc/o.sc/n.sc/d.scD/a.sc/t.sc/a.scF/i.sc/x.sc (Pb,Tp;Tn)
.Function /c.sc/o.sc/n.sc/t.sc/r.sc/o.sc/l.scF/i.sc/x.sc searches for a partial repair changing the
control-/f_low of the program, using Angelix and Prophet. If a partial
repair is not found, NULL is returned.
3:function /c.sc/o.sc/n.sc/t.sc/r.sc/o.sc/l.scF/i.sc/x.sc (Pb,Tp,Tn)
.Set the repair con/f_iguration such that a partial repair changing
the control-/f_low of the program is searched for.
4: C fcontrol ;partialg
5: return /r.sc/u.sc/n.scA/n.sc/g.sc/e.sc/l.sc/i.sc/x.sc (C,Pb,Tp,Tn)jj/r.sc/u.sc/n.scP/r.sc/o.sc/p.sc/h.sc/e.sc/t.sc (C,Pb,Tp,Tn)
6:end function
.Function /c.sc/o.sc/n.sc/d.scD/a.sc/t.sc/a.scF/i.sc/x.sc searches for a partial repair changing the
data-/f_low and/or the control-/f_low of the program. If a partial repair is
not found, NULL is returned.
7:function /c.sc/o.sc/n.sc/d.scD/a.sc/t.sc/a.scF/i.sc/x.sc (Pb,Tp,Tn)
.Set the repair con/f_iguration such that a partial repair changing
the data-/f_low of the program is searched for.
8: C fdata ;partialg
.Search for a program Pithat makes at least one of the tests in
Tnpass, while ignoring Tp.Tirepresents a set of tests in Tnthat
pass with Pi.
9: (Pi;Ti) /r.sc/u.sc/n.scG/e.sc/n.scP/r.sc/o.sc/g.sc (C,Pb,Tn)jj/r.sc/u.sc/n.scAE (C,Pb,Tn)jj
/r.sc/u.sc/n.scA/n.sc/g.sc/e.sc/l.sc/i.sc/x.sc (C,Pb,Tn)jj/r.sc/u.sc/n.scP/r.sc/o.sc/p.sc/h.sc/e.sc/t.sc (C,Pb,Tn)
.IfPiis found (i.e., Pi!= NULL), re/f_ine Pisuch that not only Ti
but also all the tests in Tppass. /T_his is achieved by looking for a
complete (not partial) repair that passes all tests in Tp[Ti
10: ifPi!= NULL then
11: C fcontrol ;completeg
12: Pr R/u.sc/n.scA/n.sc/g.sc/e.sc/l.sc/i.sc/x.sc (C,Pi,Tp[Ti)
13: end if
.If re/f_inement with Angelix fails (i.e., Pr== NULL), try with
Prophet.
14: ifPi!= NULL && Pr== NULL then
15: C fcontrol ;completeg
16: Pr R/u.sc/n.scP/r.sc/o.sc/p.sc/h.sc/e.sc/t.sc (C,Pi,Tp[Ti)
17: end if
18: return Pr
19:end function
fSg‚Äù or ‚Äú if (false)fSg‚Äù, respectively. Similarly, if a statement Sis
modi/f_ied into another statement S0in the /f_irst step, we transform S
into ‚Äú if (true)fS‚ÄôgelsefSg‚Äù. /T_his transformation takes place inter-
nally inside the APR tools we modify for this purpose. /T_he re/f_ined
program Pris obtained by replacing the tautological conditions
(true or false) guarding the added/deleted/modi/f_ied statement with
diÔ¨Äerent expressions with which Prpasses all positive tests and
the negative tests addressed in the /f_irst step. We invoke multiple
APR tools in parallel in the two-step repair process of /f_inding a
conditional data-/f_low hint. In the /f_irst step, we invoke four tools,
that is, GenProg, AE, Prophet, and Angelix (for Prophet and An-
gelix, we turn oÔ¨Ä the options that allow conditional expression
changes). In the second step where guards are modi/f_ied, we invokeonly Prophet and Angelix, since GenProg and AE do not support
expression-level modi/f_ications.
6.3 Incremental Repair
Our overall repair algorithm optionally allows incremental repair,
that is, generating a series of partial repairs incrementally. More
speci/f_ically, a new partial repair Pi+1is generated based on the
previous partial repair Pigenerated at the i-th iteration. /T_he number
of passing tests grows as the iteration proceeds, and the tests passed
byPiare also passed by Pi+1. /T_he iteration proceeds until either
there is no remaining negative (failing) test or a partial repair is not
found. A repair obtained through the incremental repair approach
can be useful for graders to whom showing as many changes as
possible can provide hints about why the student program is wrong.
7 EVALUATION
We evaluate the feasibility of using our partial repair algorithm
for introductory programming assignments. /T_he following are our
research questions.
RQ1 How o/f_ten are repairs generated when our partial repair
algorithm is employed in addition to the complete repair algorithm
of the existing APR tools? A high repair rate is a prerequisite for
using APR for introductory programming assignments. /T_he current
state-of-the-art APR tools fail to generate repairs more o/f_ten than
not, as shown in Section 4. How signi/f_icantly does a new repair
strategy allowing both complete and partial repairs improve repair
rate?
RQ2 When are repairs not generated even a/f_ter employing our
partial repair algorithm? If there are common reasons for those
cases of repair failure, they should be addressed in future tools.
RQ3 Do tool-generated partial repairs help students in /f_inding a
solution more eÔ¨Éciently than when repairs are not shown?
RQ4 Similarly, do tool-generated repairs help graders in grad-
ing student programs more eÔ¨Éciently than when repairs are not
shown?
To investigate our research questions, we conduct a tool experi-
ment (to address RQ1 regarding repair rate), repair failure analysis
(to address RQ2), and user study (to address RQ3 and RQ4).
7.1 Tool Experiment
We developed a tool that implements our partial repair algorithm
on top of the same four existing APR tools as used in our initial
experiment. We apply our tool to the same dataset as used in
our initial experiment modulo the incorrect programs for which
complete repairs are already generated in the initial experiment.
Recall that the purpose of this tool experiment is to investigate
how signi/f_icantly a new repair strategy allowing both complete and
partial repairs improves repair rate. /T_he experiment was performed
on the same environment as used for the initial experiment.
Table 3 shows the results of our tool experiment. As compared
to our initial experiment that does not allow partial repairs, the
overall repair rate increases from 31% to 57%, showing about 84% of
improvement. Repair rate increases signi/f_icantly across all labs, as
shown in Figure 2. Meanwhile, the average successful repair time
stays as low as 58 seconds.Table 3: /T_he result of an experiment in which partial repairs
are sought for in case a complete repair is not found out. /T_he
overall repair rate is about 60%.
Lab # Programs # Fixed Repair Rate Time
Lab 3 63 14 22 % 3 s
Lab 4 117 61 52 % 27 s
Lab 5 82 52 63 % 85 s
Lab 6 79 49 62 % 69 s
Lab 7 71 44 62 % 51 s
Lab 8 33 28 85 % 99 s
Lab 9 48 26 54 % 70 s
Lab 10 53 36 68 % 35 s
Lab 11 55 33 60 % 77 s
Lab 12 60 35 58 % 52 s
Total 661 378 57 % 58 s
020406080
Lab 3 Lab 4 Lab 5 Lab 6 Lab 7 Lab 8 Lab 9 Lab 10 Lab 11 Lab 12 TotalPolicy Complete Partial+Complete
Figure 2: /T_his plot shows the repair rate in percentage
(Y axis) across each individual lab (X axis). /T_he ‚ÄúCom-
plete‚Äù represents the cases in which only complete repairs
are counted, whereas the ‚ÄúPartial+Complete‚Äù represents the
cases in which partial repairs are also allowed in case a com-
plete repair does not exist.
7.2 Repair Failure Analysis
Despite the increase of repair rate a/f_ter allowing partial repairs,
neither complete repair nor partial repair was generated in 43% of
our subject programs. We compare these 43% of programs with
their correct versions to look for common reasons for repair failure.
Speci/f_ically, for each defect represented by the buggy version Pb
and the correct version Pc, we obtain the AST diÔ¨Äerences between
Pband Pcusing Gumtree [ 7], an AST diÔ¨Äerencing tool. We /f_irst
perform manual inspection of the AST diÔ¨Äerences to derive a set of
common characteristics observed in the diÔ¨Äerences between Pband
Pc. /T_hen, we detect other such instances in our dataset, using our
extension of Gumtree where we encode the AST diÔ¨Äerence pa/t_terns
corresponding to the common characteristics we identi/f_ied. We
repeat this process until all programs for which repairs are not
generated are covered. Note that some programs are labeled with
multiple characteristics in this process.Table 4: /T_his table shows the distribution of the diÔ¨Äerence
characteristics of the two programs, a buggy program ( Pb)
and its correct version ( Pc), for which neither complete nor
partial repair is generated by the APR tools.
Pc Pb # Instances Portion
String 125 40 %
Array 44 14 %
Missing Function 38 12 %
Complex Control 35 11 %
Unsupported 30 10 %
Others 16 5 %
Empty Implementation 12 4 %
Wrong Parameters 6 2 %
Wrong Usage 6 2 %
Table 4 shows our analysis result. /T_he /f_irst column categorizes
the characteristics of the diÔ¨Äerences between the buggy program
(Pb) and its corrected version ( Pc). /T_he second and third column
show the number of instances and portion of each category, respec-
tively, by which the table is sorted. /T_he following describes each
category for Pc Pbwhich we represent as Œ¥:
String /T_his corresponds to the case where Œ¥involves changing
the string constants used in the program, such as adding a missing
space or a new line. It is observed that this category takes the most
number of instances of repair failure (40%).
Array /T_his corresponds to the case where Œ¥involves changes
in arrays that include array index changes, array size changes,
adding/deleting array access expressions, and using array lengths
in the program.
Missing Function /T_his corresponds to the case where Œ¥involves
adding a function call.
Complex Control /T_his corresponds to the case where Œ¥involves
complex control-/f_low changes that include control-/f_low changes
in a nested loop and control-/f_low changes in multiple conditionals.
While Angelix and Prophet can change conditional expressions,
they do not exhaustively consider all possible control-/f_low changes.
Unsupported /T_his corresponds to the case where Pcrequires
expressions that cannot be synthesized by the current APR tools
such as the expressions involving the modular operator and non-
linear expressions.
Empty Implementation /T_his corresponds to the case where the
main function of Pbis empty or contains only a return statement.
We do not label other characteristics for the programs belonging to
this category.
Wrong Parameters /T_his corresponds to the case where Œ¥involves
changing multiple parameters of a function call expression. While
Angelix can change multiple expressions, it does not exhaustively
consider all possible combinations.
Wrong Usage /T_his corresponds to the case where students use
language constructs in a semantically wrong way. /T_his includes mis-
takenly adding a semicolon before a for-loop body (i.e., using for(‚Ä¶);
f‚Ä¶ginstead of for(‚Ä¶)f‚Ä¶g), using scanf(‚Ä¶,x) instead of scanf(‚Ä¶,&x)
where xrepresents a variable, using ++xwhen x+1is required, using
‚Äòx‚Äôwhen xis required, and using *xwhen xis required.
Others /T_his covers the rest of the characteristics.(a) Programming Experience
 (b) Skill Level for C Programming
Figure 3: Background of Teaching Assistants
/T_he fact that the portions of the top two categories (String and
Array) take more than 50% suggests that it would be most cost
eÔ¨Äective to strengthen repair operators that can manipulate strings
and arrays in future APR tools.
7.3 User Study
We perform a user study with novice students and graders to see
(1)whether automatically generated feedback can help students
solve the problem on their own.
(2)whether automatically generated feedback can help teaching
assistants (TAs) grade submissions eÔ¨Éciently (faster grading)
and eÔ¨Äectively (only small variation in the marks for similar
submissions.)
For the student study, we selected 5 problems for which we had
buggy submissions and the partial repairs generated by our algo-
rithm. We divided the students into the experimental group for
whom the generated repairs are presented and the control group
for whom the repairs are not presented. Repairs are presented in
the form of a comment around the repaired lines of the buggy sub-
mission. We asked each student to /f_ix one randomly chosen buggy
submission. /T_he study was unannounced, that is, the task was
provided as a bonus question along with other regular assignment
problems. /T_he weight of the /f_ix-task was kept low so that it does
not impact the overall grade of the students in the course. /T_he par-
ticipation was voluntary, and in total 263 students submi/t_ted their
completed programs (140 students in the without-repair group, and
123 students in the with-repair group), out of the 400+ students
crediting the course.
Similarly, to estimate the impact of repairs on the grading task,
we did a study on TAs. 37 TAs volunteered for this task, out of
which 35 /f_illed in the pre-study survey and the post-study sur-
vey. Figure 3 summarizes the background of these TAs, which we
collected using a pre-study survey. For the study, we randomly
collected 43 buggy submissions from the subset of our dataset for
which our algorithm successfully generates either complete or par-
tial repairs. /T_hese 43 buggy submissions correspond to 8 diÔ¨Äerent
programming problems. We asked the TAs to grade these submis-
sions based on how close they are to a correct program by /f_iguring
out the bugs and their corresponding repairs. /T_he TAs were divided
into two groups. /T_he /f_irst group was given 22 tasks (set A) without
repair, and 21 tasks (set B) with repairs, while the second group was
conversely given set A with repairs, and set B without repairs. We
S-1 S-2 S-3 S-4 S-5
ProblemID050010001500200025003000Time T akenRepair Not Provided Repair ProvidedFigure 4: Time taken by students for bug /f_ix task
P-1 P-2 P-3 P-4 P-5 P-6 P-7 P-8
ProblemID0100200300400500600700800Time T akenRepair Not Provided Repair Provided
Figure 5: Time taken by TAs for grading task
Table 5: /T_he answer frequency from TAs for the question:
How do you categorize the errors of the program based on
the suggested repair? Multiple answers are allowed.
Category Frequency
Conditionals 29
Loops 19
Missing Character 8
String Modi/f_ications 5
Array Accesses 4
User de/f_ined Functions 4
Missing Values in the Output 4
Library Functions 3
Others 3
Missing Whitespace in the Output 2
Floating Point Operations 1
compared the time taken and marks assigned by the TAs for these
tasks. /T_he reference marks for these submissions were provided by
the instructor who did not participate in the study, and did not have
access to the repairs. With TAs, we also conducted a post-study
survey to understand the experience of TAs with repairs.
Figure 4 and Figure 5 respectively show the distribution of time
taken by the students for the solving task, and time taken by the
TAs for the grading task. Both the /f_igures are box-plots where
X-axis shows the problem IDs and Y-axis shows the time in seconds.
Each box (or rectangle) represents the /f_irst and third quartiles, withTable 6: /T_he answer frequency from TAs for the question:
What kind of modi/f_ications are necessary in the suggested
repair to obtain a correct solution? Multiple answers are al-
lowed.
Description Frequency
Fix condition for Conditionals or Loops 32
Fix Operators 15
Insert/Delete Character (e.g., ;,&) 15
Forma/t_ting the Output (whitespaces) 11
Fix Constants 10
Fix Array Indices 4
Others 4
Table 7: Analysis of TA Grading Time. ‚ÄúYes‚Äù TAs correspond
to those who answered in the post-study survey that repairs
were useful, while ‚ÄúNo‚Äù TAs answered conversely.
Grading All TAs ‚ÄúYes‚Äù TAs ‚ÄúNo‚Äù TAs
Time Without With Without With Without With
(sec) Repair Repair Repair Repair Repair Repair
Average 173.76 135.41 155.08 124.83 191.40 145.39
Median 150.95 133.68 120.70 126.90 166.87 144.85
Stdev 96.70 40.88 99.98 40.30 92.82 39.96
a horizontal line inside indicating the median value. /T_he ends of
the vertical lines (or whiskers) on either side of the box represent
the minimum and maximum time-taken. From these /f_igures, we
can infer that repairs aÔ¨Äect novice students and TAs diÔ¨Äerently.
While the problem solving time of the students tends to increase
in the group where repairs are shown, the grading time of the
TAs tends to decrease when repairs are shown. /T_hat is, when
repairs are shown, the students tend to solve the problems more
slowly, while the TAs tend to grade the problems more quickly. We
conjecture that these opposite trends between novice students and
TAs are due to their diÔ¨Äerent levels of expertise and the format
of feedback. In our post-study survey, we asked TAs (1) how do
you categorize the errors of the program based on the suggested
repair? and (2) what kind of modi/f_ications are necessary in the
suggested repair to obtain a correct solution? /T_he results are shown
in Table 5 and 6. In the /f_irst question, the majority of TAs identi/f_ied
the errors in loops/conditionals (see Table 5), while in the second
question, the most number of answers were given to the changes in
loops/conditionals (see Table 6). /T_hese results suggest that TAs are
capable of generalizing suggested repairs that are overly specialized
to the tests. It is likely that this generalization capability of TAs
helps them /f_inish the grading tasks more eÔ¨Éciently. However,
novice students do not seem to know how to eÔ¨Äectively make use
of suggested repairs, unlike TAs.
Table 7 shows a closer look at the grading performance of TAs.
/T_he /f_irst column (All TAs) shows the performance statistics for all
TAs (both without repair and with repair), and the second column
(‚ÄúYes‚Äù TAs) and the third column (‚ÄúNo‚Äù TAs) show the statistics for
those who said in the post-study survey that the suggested repairs
are useful and not useful, respectively. Half of the TAs answered the
repairs are useful (the Yes group) and the rest of the half answered
S-1S-2S-3S-4S-5S-6S-7S-8S-9S10S11S12S13S14S15
ProblemID05101520Marks AwardedInstructor's Grade Repair Not Provided Repair ProvidedFigure 6: Distribution of marks assigned by TAs
not useful (the No group). Given that the average grading time is
smaller in the Yes group, high performers tend to feel more strongly
that the suggested repairs are useful. In both groups, the average
grading time decreases when repairs are shown. Also notably, the
standard deviation decreases in both groups, indicating that the gap
between high performers and low performers becomes narrower
when repairs are shown.
Figure 6 shows the marks awarded by the TAs for 15 randomly
picked submissions out of 43 tasks. /T_he reference marks for these
submissions were provided by the instructor who did not participate
in the study, and also did not look at the generated repairs used in
the study. In the graph, the X-axis and Y-axis show, respectively,
the problem IDs and marks awarded (between 0 and 20). /T_he overall
trends are similar among the group for whom repairs are presented
(experimental group), the group for whom repairs are not presented
(control group), and the independent instructor. In a majority of
the cases the absolute diÔ¨Äerence between the experimental group
and the control group is not much: 1 for 22 =43 cases and2 for
30=43 cases.
8 THREATS TO VALIDITY
In our tool experiments, one of the APR tools, GenProg, uses a
random algorithm (genetic programming), which can produce dif-
ferent results for each run. To mitigate this threat, we applied the
same seed to GenProg in our initial experiment (Section 4) and the
second tool experiment (Section 7.1). Also, the fact that the rest
of the APR tools employed for our experiments (AE, Prophet, and
Angelix) use deterministic repair algorithm further mitigates this
threat. Our repair failure analysis (Section 7.2) may be restricted
by the diÔ¨Äerence categories, Pc Pb, to which our analysis tool
categorizes. To mitigate this threat, we manually inspected the
diÔ¨Äerences and added new categories when the previously used
categories were not suÔ¨Écient. Our dataset, while collected from the
actual students taking an introductory programming course, may
not be representative of all student programs. Similarly, in our user
study, participating students and TAs may not represent all novice
students and graders. In terms of the programming language, ourresults are con/f_ined to C programs for which APR has been devel-
oped most actively. However, our proposed partial-repair policy can
be applied to other programming languages. In our user study, the
experimental se/t_ting‚Äîwhere the participating students are given
buggy programs wri/t_ten by other students‚Äîis not identical with
the actual usage scenario where the students /f_ix the mistakes they
made. We leave further investigation as future work.
9 RELATED WORK
Many diÔ¨Äerent techniques have been applied to automated feed-
back generation, and each technique has diÔ¨Äerent advantages and
disadvantages. Program equivalence checking is used in [ 15] where
behavioral diÔ¨Äerence between a student program and its refer-
ence program (diÔ¨Äerences in input-output relations) is reported
to the student as feedback. Since program equivalence checking
is generally undecidable, [ 15] performs equivalence checking in
a constrained manner ‚Äî that is, when comparing a student pro-
gram Pswith its reference program Pr,Prshould be structurally
similar to Ps. Such Prcan be provided either manually (the instruc-
tor prepares Pr) or semi-automatically (the instructor selects Pr
from previously submi/t_ted correct student programs which can be
automatically clustered according to their structures). Since this
approach based on program equivalence uses a reference program
as a speci/f_ication, it can generate feedback even when there is no
failing test. Meanwhile, the fact that the instructor should validate
the correctness of the reference program poses not only a burden
to the instructor, but also a risk of generating false feedback in the
presence of a validation mistake.
A model-based approach is used in [ 40] where an instructor-
given error model describing possible student errors de/f_ines how
the given incorrect program is allowed to be modi/f_ied. To search for
a correct modi/f_ication eÔ¨Éciently, a program synthesis technique is
employed. While an error model can capture some common student
errors and hence can guide feedback generation, it also restricts
the search for feedback only to the common errors described in
the error model. /T_he fact that an error model should be prepared
beforehand by the instructor is another disadvantage.
Static analysis is used in [ 1,48] where dependence graphs ex-
tracted from a student program Psand the reference program Pr
are compared to each other, in order to identify a statement in Ps
that can potentially cause semantic diÔ¨Äerence from Pr. As usual in
conservative static analysis, these approaches can guarantee not to
miss a semantic error, while as a /f_lip side, an error can be falsely
reported.
A learning-based approach is used in Refazer [ 39] and Deep-
Fix [ 14]. Refazer learns programs transformation rules from the
past program changes, similar to [ 26,27] where systematic edits
(similar, but not identical, changes made in many program locations)
are learned from the past program changes. Meanwhile, DeepFix
applies deep learning to the correction of syntactic errors that cause
compilation failure. While learning-based approaches can comple-
ment the existing approaches when the previous submissions of
a programming assignment are available, their applicability and
eÔ¨Äectiveness are restricted by the availability and the quality of the
previous submissions.
A data-driven approach is used in [ 38] where, in order to gen-
erate feedback, not only the reference program, but also a chainof intermediate programs leading to the reference program are ex-
ploited. /T_he use of intermediate programs makes it more amenable
to generate a next-step hint, since the buggy student program is
likely to be closer to one of the intermediate programs than to the
reference program. However, the hint space, consisting of the refer-
ence programs and their intermediate programs, is more restricted
than the one of APR.
Automated program repair (APR) is fully automatic unlike some
approaches requiring additional input from the instructor, such
as an error model and multiple reference programs from multiple
clusters. In APR, it is suÔ¨Écient to provide a student program and a
test suite. Although generated repairs can be imperfect and overly
specialized to the provided test suite [ 41], this issue has been grad-
ually addressed in recent work of APR [ 5,19,22,24,43,44,47].
Meanwhile, fault localization can also be used to provide hints to
students, as suggested in [ 3]. In fact, APR also performs fault lo-
calization in the sense that APR performs fault localization before
synthesizing a /f_ix. Furthermore, students can also see how a pre-
viously failing test passes a/f_ter /f_ix, which provides an additional
hint.
/T_here have been several user studies in the area of program
debugging and repair [ 16,32,45]. Unlike these user studies con-
cerning the productivity of professional developers, our study is
conducted with diÔ¨Äerent target of users, that is, novice students
and graders. Overall, our study provides holistic information about
the feasibility of using APR for introductory programming assign-
ments, including how o/f_ten repairs are generated, why repairs are
failed to be generated, and how useful generated repairs are for
students and graders.
10 CONCLUSION
In this paper, we have explored the possibility of using APR as
a feedback generation engine of intelligent tutoring systems for
introductory programming. We have performed a feasibility study
with four state-of-the-art APR tools (GenProg, AE, Prophet, and
Angelix) and real student programs collected from a course on intro-
ductory programming. Although out-of-the-box application of APR
tools seems infeasible due to the low repair rate, we have shown
that repair rate can be boosted by tailoring the repair policy and
strategy of APR to the needs of intelligent tutoring. Most notably,
adopting a partial repair policy akin to the next-step hint gener-
ation advocated in the education /f_ield seems eÔ¨Äective in terms of
improving feedback generation rate. We have also shown through a
repair failure analysis that repair failures are o/f_ten caused by a few
common reasons. Further improvement of feedback generation rate
is expected by strengthening repair operators manipulating strings
and arrays in future APR tools. Lastly, we have shown our user
study results performed with novice students and graders (TAs).
In contrast to the TAs who use the suggested repairs as hints to
eÔ¨Éciently complete the grading tasks, the novice students do not
seem to know how to eÔ¨Äectively make eÔ¨Écient use of suggested
repairs to correct their programs. We leave as future work a study
of eÔ¨Äective post-processing of repairs to transform them to hints
more comprehensible to novice students.
ACKNOWLEDGMENTS
/T_he /f_irst author thanks Innopolis University for its support.11 ARTIFACT DESCRIPTION
To facilitate further research, we share the artifacts used in this
study in the following GitHub page:
h/t_tps://github.com/jyi/ITSP
A detailed description about the artifacts‚Äîincluding how to use
them‚Äîis provided in our GitHub page. In this section, we provide
summarized information about the shared artifacts.
11.1 Available Artifacts
/T_he following artifacts are available in our GitHub page:
(1)Dataset containing 661 student programs we used in our exper-
iments (see Section 3)
(2)A toolchain implementing our new repair policy (see Section 6.1)
and strategy (see Section 6.2)
(3)User study materials including the survey questionnaire we
used and survey responses (see Section 7.3)
Docker Image. /T_he provided toolchain runs on top of four APR
tools, namely GenProg [ 20], AE [ 46], Angelix [ 25], and Prophet [ 22].
We provide a docker image where all these four tools are already
installed. Our docker image can be downloaded from Docker Hub:
docker pull jayyi/itsp:0.0
Note that the size of the image is quite large ( >30 GB). /T_he
following more lightweight image (about 3 GB) is also available:
jayyi/itsp-no-angelix:0.0, which does not contain one of APR tools,
Angelix. In the lightweight image, the provided toolchain does not
use Angelix when generating a repair.
Tutorial. We provide in our GitHub page a tutorial about how to
use our toolchain and how to interpret the toolchain result.
11.2 Potential Users of the Artifacts
Our artifacts can be useful for:
(1) /T_hose who want to reproduce our experimental results.
(2)/T_hose who need a benchmark for an intelligent tutoring sys-
tem for programming (ITSP). Our shared dataset contains 661
incorrect student programs, their reference programs (correct
versions) and test suites.
(3)/T_hose who want to extend our partial repair policy/strategy.
/T_he provided toolchain (wri/t_ten in Bash) implements our partial
repair policy/strategy.
(4)/T_hose who want to conduct a user study on ITSP. We share the
survey questionnaire used in our user study.
A Note on Reproducibility. GenProg uses a random algorithm.
Also, parallel use of multiple repair tools introduces one more layer
of randomness (there is no guarantee that one repair tool always
/f_inds a repair faster than the other tools). /T_hus, diÔ¨Äerent results may
be produced at each experiment. We provide raw experimental data
we obtained in our GitHub page (located in the experiment/cache
directory) from which the summarized information we provide
in this paper (i.e., Table 2, Figure 1, Table 3 and Figure 2) can be
reproduced using the script (analysis.R) provided in our GitHub
page.REFERENCES
[1]Anne Adam and Jean-Pierre H. Laurent. 1980. LAURA, A System to Debug
Student Programs. Artif. Intell. 15, 1-2 (1980), 75‚Äì122.
[2] TiÔ¨Äany Barnes and John C. Stamper. 2008. Toward Automatic Hint Generation
for Logic Proof Tutoring Using Historical Student Data. In Intelligent Tutoring
Systems . 373‚Äì382.
[3]GeoÔ¨Ä Birch, Bernd Fischer, and Michael Poppleton. 2016. Using Fast Model-
Based Fault Localisation to Aid Students in Self-Guided Program Repair and to
Improve Assessment. In Proceedings of the 2016 ACM Conference on Innovation
and Technology in Computer Science Education, ITiCSE 2016 . 168‚Äì173.
[4] Cristian Cadar, Daniel Dunbar, and Dawson R. Engler. 2008. KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems Programs.
InOSDI . 209‚Äì224.
[5]Loris D‚ÄôAntoni, Roopsha Samanta, and Rishabh Singh. 2016. Qlose: Program
Repair with /Q_uantitative Objectives. In CAV. 383‚Äì401.
[6] Rajdeep Das, Umair Z. Ahmed, Amey Karkare, and Sumit Gulwani. 2016. Prutor:
A System for Tutoring CS1 and Collecting Student Programs for Analysis. CoRR
abs/1608.03828 (2016). h/t_tp://arxiv.org/abs/1608.03828
[7] Jean-R ¬¥emy Falleri, Flor ¬¥eal Morandat, Xavier Blanc, Matias Martinez, and Martin
Monperrus. 2014. Fine-grained and accurate source code diÔ¨Äerencing. In ASE.
313‚Äì324.
[8]Elena L. Glassman, Jeremy Sco/t_t, Rishabh Singh, Philip J. Guo, and Robert C.
Miller. 2015. OverCode: Visualizing Variation in Student Solutions to Pro-
gramming Problems at Scale. ACM Trans. Comput.-Hum. Interact. 22, 2 (2015),
7:1‚Äì7:35.
[9] Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, and Westley Weimer.
2012. A systematic study of automated program repair: Fixing 55 out of 105 bugs
for$8 each. In ICSE . 3‚Äì13.
[10] Sebastian Gross, Bassam Mokbel, Benjamin Paa√üen, Barbara Hammer, and Niels
Pinkwart. 2014. Example-based feedback provision using structured solution
spaces. IJLT 9, 3 (2014), 248‚Äì280.
[11] Zhongxian Gu, Earl T. Barr, David J. Hamilton, and Zhendong Su. 2010. Has
the Bug Really Been Fixed?. In Proceedings of the 32Nd ACM/IEEE International
Conference on So/f_tware Engineering - Volume 1 (ICSE ‚Äô10) . 55‚Äì64.
[12] Sumit Gulwani, Ivan Radicek, and Florian Zuleger. 2014. Feedback generation for
performance problems in introductory programming assignments. In Proceedings
of the 22nd ACM SIGSOFT International Symposium on Foundations of So/f_tware
Engineering, (FSE-22), Hong Kong, China, November 16 - 22, 2014 . 41‚Äì51.
[13] Philip J. Guo. 2015. Codeopticon: Real-Time, One-To-Many Human Tutoring for
Computer Programming. In Proceedings of the 28th Annual ACM Symposium on
User Interface So/f_tware & Technology, UIST 2015, Charlo/t_te, NC, USA, November
8-11, 2015 . 599‚Äì608.
[14] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. DeepFix: Fixing
Common C Language Errors by Deep Learning. In AAAI 2017 . To appear.
[15] Shalini Kaleeswaran, Anirudh Santhiar, Aditya Kanade, and Sumit Gulwani. 2016.
Semi-supervised veri/f_ied feedback generation. In FSE. 739‚Äì750.
[16] Shalini Kaleeswaran, Varun Tulsian, Aditya Kanade, and Alessandro Orso. 2014.
MintHint: automated synthesis of repair hints. In ICSE . 266‚Äì276.
[17] Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic
Patch Generation Learned from Human-wri/t_ten Patches. In Proceedings of the
2013 International Conference on So/f_tware Engineering (ICSE ‚Äô13) . 802‚Äì811.
[18] John R. Koza. 1993. Genetic programming - on the programming of computers by
means of natural selection . MIT Press.
[19] Xuan-Bach D. Le, David Lo, and Claire Le Goues. 2016. History Driven Program
Repair. In SANER . 213‚Äì224.
[20] C. Le Goues, /T_hanhVu Nguyen, S. Forrest, and W. Weimer. 2012. GenProg: A
Generic Method for Automatic So/f_tware Repair. IEEE Transactions on So/f_tware
Engineering 38, 1 (Jan 2012), 54‚Äì72.
[21] Fan Long and Martin Rinard. 2015. Staged program repair with condition syn-
thesis. In ESEC/FSE . 166‚Äì178.
[22] Fan Long and Martin Rinard. 2016. Automatic patch generation by learning
correct code. In POPL . 298‚Äì312.
[23] Fan Long and Martin C. Rinard. 2016. An analysis of the search spaces for
generate and validate patch generation systems. In ICSE . 702‚Äì713.
[24] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2015. DirectFix: Looking
for Simple Program Repairs. In ICSE . 448‚Äì458.
[25] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: scalable
multiline program patch synthesis via symbolic analysis. In ICSE . 691‚Äì701.
[26] Na Meng, Miryung Kim, and Kathryn S. McKinley. 2011. Systematic editing:
generating program transformations from an example. In PLDI . 329‚Äì342.
[27] Na Meng, Miryung Kim, and Kathryn S. McKinley. 2013. LASE: locating and
applying systematic edits by learning from examples. In ICSE . 502‚Äì511.
[28] Douglas C. Merrill, Brian J. Reiser, Shannon K. Merrill, and Shari Landes. 1995.
Tutoring: Guided Learning by Doing. Cognition and Instruction 13, 3 (1995),
315‚Äì372.
[29] Leonardo de Moura and Nikolaj Bj√∏rner. 2008. Z3: An eÔ¨Écient SMT solver. In
TACAS . 337‚Äì340.[30] Hoang Duong /T_hien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chan-
dra. 2013. SemFix: program repair via semantic analysis. In ICSE . 772‚Äì781.
[31] Luc Paque/t_te, Jean-Fran c ¬∏ois Lebeau, Gabriel Beaulieu, and Andr ¬¥e Mayers. 2012.
Automating Next-Step Hints Generation Using ASTUS. In Intelligent Tutoring
Systems . 201‚Äì211.
[32] Chris Parnin and Alessandro Orso. 2011. Are automated debugging techniques
actually helping programmers?. In ISSTA . 199‚Äì209.
[33] Yu Pei, C.A. Furia, M. Nordio, Yi Wei, B. Meyer, and A. Zeller. 2014. Automated
Fixing of Programs with Contracts. IEEE Transactions on So/f_tware Engineering
40, 5 (May 2014), 427‚Äì449.
[34] Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas Guibas. 2015. Au-
tonomously Generating Hints by Inferring Problem Solving Policies. In Proceed-
ings of the Second ACM Conference on Learning @ Scale . 195‚Äì204.
[35] Leena M. Razzaq, Neil T. HeÔ¨Äernan, and Robert W. Lindeman. 2007. What Level
of Tutor Interaction is Best?. In Arti/f_icial Intelligence in Education . 222‚Äì229.
[36] Kelly Rivers and Kenneth R. Koedinger. 2013. Automatic Generation of Program-
ming Feedback; A Data-Driven Approach. In Proceedings of the Workshops at the
16th International Conference on Arti/f_icial Intelligence in Education AIED 2013,
Memphis, USA, July 9-13, 2013 .
[37] Kelly Rivers and Kenneth R. Koedinger. 2014. Automating Hint Generation with
Solution Space Path Construction. In Intelligent Tutoring Systems . 329‚Äì339.
[38] Kelly Rivers and Kenneth R. Koedinger. 2017. Data-Driven Hint Generation in
Vast Solution Spaces: a Self-Improving Python Programming Tutor. International
Journal of Arti/f_icial Intelligence in Education 27, 1 (2017), 37‚Äì64.
[39] Reudismam Rolim, Gustavo Soares, Loris D‚ÄôAntoni, Oleksandr Polozov, Sumit
Gulwani, Rohit Gheyi, Ryo Suzuki, and Bjoern Hartmann. 2017. Learning Syn-
tactic Program Transformations from Examples. In ICSE . To appear.
[40] Rishabh Singh, Sumit Gulwani, and Armando Solar-Lezama. 2013. Automated
feedback generation for introductory programming assignments. In PLDI . 15‚Äì26.[41] Edward K. Smith, Earl T. Barr, Claire Le Goues, and Yuriy Brun. 2015. Is the cure
worse than the disease? over/f_i/t_ting in automated program repair. In ESEC/FSE .
532‚Äì543.
[42] Elliot Soloway, Beverly Park Woolf, Eric Rubin, and Paul Barth. 1981. Meno-II:
An Intelligent Tutoring System for Novice Programmers. In IJCAI . 975‚Äì977.
[43] Shin Hwei Tan and Abhik Roychoudhury. 2015. reli/f_ix: Automated repair of so/f_t-
ware regressions. In Proceedings of the 37th International Conference on So/f_tware
Engineering-Volume 1 . IEEE Press, 471‚Äì482.
[44] Shin Hwei Tan, Hiroaki Yoshida, Mukul R Prasad, and Abhik Roychoudhury. 2016.
Anti-pa/t_terns in search-based program repair. In Proceedings of the 2016 24th
ACM SIGSOFT International Symposium on Foundations of So/f_tware Engineering .
ACM, 727‚Äì738.
[45] Yida Tao, Jindae Kim, Sunghun Kim, and Chang Xu. 2014. Automatically gener-
ated patches as debugging aids: a human study. In FSE. 64‚Äì74.
[46] Westley Weimer, Zachary P. Fry, and Stephanie Forrest. 2013. Leveraging pro-
gram equivalence for adaptive program repair: Models and /f_irst results. In ASE.
356‚Äì366.
[47] Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and
Lu Zhang. 2017. Precise Condition Synthesis for Program Repair. In ICSE . To
appear.
[48] Songwen Xu and Yam San Chee. 2003. Transformation-Based Diagnosis of
Student Programs for Programming Tutoring Systems. IEEE Trans. So/f_tware Eng.
29, 4 (2003), 360‚Äì384.
[49] Jifeng Xuan, Matias Martinez, Favio Demarco, Maxime Clement, Sebastian
R. Lamelas Marcote, /T_homas Durieux, Daniel Le Berre, and Martin Monper-
rus. 2017. Nopol: Automatic Repair of Conditional Statement Bugs in Java
Programs. IEEE Trans. So/f_tware Eng. 43, 1 (2017), 34‚Äì55.