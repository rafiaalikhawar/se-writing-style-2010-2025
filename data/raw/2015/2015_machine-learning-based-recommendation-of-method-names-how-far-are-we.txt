Machine Learning Based Recommendation of
Method Names: How Far Are We
Lin Jiang
School of Computer Science and
Technology, Beijing Institute of
Technology
Beijing, China
jianglin17@bit.edu.cnHui Liu‚àó
School of Computer Science and
Technology, Beijing Institute of
Technology
Beijing, China
liuhui08@bit.edu.cnHe Jiang
School of Software Technology,
Dalian University of
Technology
Dalian, China
jianghe@dlut.edu.cn
Abstract ‚ÄîHigh quality method names are critical for the
readability and maintainability of programs. However, construct-
ing concise and consistent method names is often challenging,
especially for inexperienced developers. To this end, advancedmachine learning techniques have been recently leveraged torecommend method names automatically for given method bod-ies/implementation. Recent large-scale evaluations also suggestthat such approaches are accurate. However, little is known
about where and why such approaches work or don‚Äôt work.
To Ô¨Ågure out the state of the art as well as the rationale forthe success/failure, in this paper we conduct an empirical studyon the state-of-the-art approach code2vec . We assess code2vec
on a new dataset with more realistic settings. Our evaluationresults suggest that although switching to new dataset does not
signiÔ¨Åcantly inÔ¨Çuence the performance, more realistic settings
do signiÔ¨Åcantly reduce the performance of code2vec . Further
analysis on the successfully recommended method names alsoreveals the following Ô¨Åndings: 1) around half (48.3%) of theaccepted recommendations are made on getter/setter methods;2) a large portion (19.2%) of the successfully recommended
method names could be copied from the given bodies. To further
validate its usefulness, we ask developers to manually score thedifÔ¨Åculty in naming methods they developed. Code2vec is then
applied to such manually scored methods to evaluate how oftenit works in need. Our evaluation results suggest that code2vec
rarely works when it is really needed. Finally, to intuitively reveal
the state of the art and to investigate the possibility of designing
simple and straightforward alternative approaches, we proposea heuristics based approach to recommending method names.Evaluation results on large-scale dataset suggest that this simpleheuristics-based approach signiÔ¨Åcantly outperforms the state-of-the-art machine learning based approach, improving precision
and recall by 65.25% and 22.45%, respectively. The comparison
suggests that machine learning based recommendation of methodnames may still have a long way to go.
Index T erms ‚ÄîCode Recommendation, Machine Learning
I. I NTRODUCTION
IdentiÔ¨Åers are widely employed to identify unique software
entities. According to a recent study [1], identiÔ¨Åers accountfor approximately 70% of the source code in terms of charac-
ters. A well-constructed identiÔ¨Åer not only follows language-speciÔ¨Åc naming conventions, but also conveys intention/re-sponsibility of its associated software entity [2]. Consequently,
‚àóCorresponding authorhigh quality identiÔ¨Åers have signiÔ¨Åcant inÔ¨Çuence on the read-
ability of source code [3], [4].
Method names are a special kind of identiÔ¨Åers, employed
to identify methods. Methods are the smallest named unitsof aggregated behaviors, and serve as a cornerstone of ab-straction [5]. The readability of such methods is critical inunderstanding the functionality of programs and the interactionamong different units (methods). However, constructing high
quality method names is often challenging, especially forinexperienced developers [1].
To facilitate the construction of method names, a few auto-
matic approaches have been proposed recently to recommendmethod names according to given method bodies/implementa-tion [6]‚Äì[9]. All such approaches leverage advanced machine
learning techniques, and thus in this paper we call them
ML-based approaches. The rationale of such approaches isthat method names are associated with certain features ofmethods, and the association could be learned automaticallyby advanced machine learning techniques. Method features
frequently employed by such approaches include source code
metrics (e.g., cyclomatic complexity) [6], the sequence of iden-tiÔ¨Åers within the method body [7], and paths connecting nodesin the abstract syntax tree (AST) of method body [8], [9].Machine learning techniques that are frequently employed by
such approaches include log-bilinear neural network [6], con-volutional neural network [7], conditional random Ô¨Åelds [8],
and attentional neural network [9]. Existing approaches exhibithigh accuracy, and the state-of-the-art approach code2vec
achieves high precision of 63.1% and high recall of 54.4% [9].
Although such approaches are overall accurate in recom-
mending method names, little is known about where and why
they work or don‚Äôt work. We also know little about the useful-ness of such approaches. To this end, in this paper, we performa comprehensive assessment and in-depth analysis on the state-of-the-art approach (i.e., code2vec [9]) with more realistic
settings (i.e., cross-project validation and excluding overriding
methods) on a new dataset. Evaluation results suggest that
although switching datasets does not signiÔ¨Åcantly inÔ¨Çuencethe performance of code2vec , more realistic settings do lead
to signiÔ¨Åcant reduction in performance. Further analysis onthe cases where code2vec succeeds suggests that:
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXB SF&OHJOFFSJOH	"4&
¬•*&&&
%0*"4&
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. ‚Ä¢First, around half (48.3%) of the accepted method names
are made for getter/setter methods.
‚Ä¢Second, a large portion (19.2%) of successfully recom-mended method names could be copied from identiÔ¨Åerswithin given bodies. That is, the recommended methodname for the given body has been typed in the bodybefore recommendation is requested.
These Ô¨Åndings suggest that code2vec succeeds rarely when
it is strongly needed. To further validate its usefulness, weask developers to manually score the difÔ¨Åculty in constructingnames for 600 Java methods, and then request code2vec to
make recommendations for them. Results conÔ¨Årm the conclu-sion that code2vec rarely works when it is really needed.
To intuitively illustrate the state of the art, and to investigate
the possibility of designing simple but effective approaches,
we propose a He
uristic based approach to recommending
Method na mes (called HeMa ) for given method bodies. It is
essentially a sequence of simple heuristics, which makes iteasy to follow. Evaluation results suggest that it signiÔ¨Åcantlyoutperforms the state-of-the-art ML-based code2vec .
The paper makes the following contributions:
‚Ä¢First, a comprehensive assessment and in-depth analysis
of the state-of-the-art approach in ML-based methodname recommendation. The analysis not only reveals the
state of the art, but also discovers where and why the
approach works or doesn‚Äôt work.
‚Ä¢Second, a simple and straightforward approach in rec-ommending method names. It signiÔ¨Åcantly outperformsthe state-of-the-art ML-based complicated approach inthe evaluation, which suggests that ML-based approaches
may still have a long way to go.
The rest of this paper is structured as follows. Section II
presents related work. Section III introduces empirical setting,and Section IV presents results and analysis. Section V pro-
poses a heuristics based approach. Section VI discusses related
issues, and Section VII presents conclusions and future work.
II. R
ELATED WORK
A. Recommendation for Method Name
The quality of identiÔ¨Åers proved to have a signiÔ¨Åcant impact
on the readability and maintainability of software source
code [10], [11] Method names, as a special kind of identiÔ¨Åers,
are especially important because they serve as cornerstoneof abstraction for aggregated behaviors [5]. Consequently, aseries of approaches have been proposed to improve the qualityof method names.
H√∏st and √òstvold [5] develop a technique for automatically
inferring naming rules of methods based on the return type,control Ô¨Çow, and parameters. A rule violation indicates aconÔ¨Çict between the name and the associated implementationof a method. To resolve the conÔ¨Çict, they Ô¨Ålter relevant phrasesfor rule violations by sorting the candidate list according tothe rank of the semantic proÔ¨Åle in candidate phrase corpus aswell as the semantic distance from the inappropriate phrase tothe candidate phrase.Allamanis et al. [6] tackle the problem of method naming
by introducing a log-bilinear neural language model, whichincludes feature functions that capture long-distance contextin source code, and a subtoken model that can predict neolo-gisms, i.e. names that do not appear in the training set. Themodel embeds each token into a high dimensional continuousspace and suggest the name that is most similar in this spaceto those in the method body. Allamanis et al. [7] later takethe method naming as a problem of extreme summarization ofsource code where the method name is viewed as the summaryof a method body. To generate the summary, they introducean attentional neural network that employs convolution onthe input tokens in the body. The network can learn high-level patterns in source code that uses both the structure ofmethod body and the identiÔ¨Åers to detect and explain complexconstructs.
Alon et al. [8] propose a general path-based approach for
representation of methods. The main idea is to represent amethod using the bag of paths between leaves in its AST and
identiÔ¨Åers associated with corresponding leaves. This allowsmachine learning models to leverage the structured nature ofsource code rather than treating it as a Ô¨Çat sequence of tokens.
code2vec published by Alon et al. [9] further represents
a method body into a distributed vector by aggregating thebag of AST paths with attentional network [12]. The attention
mechanism computes a weighted attention value for each
of the AST paths in a method and aggregate them into asingle vector by weighted summation to represent the methodbody. The vectors of methods that share similar AST struc-tures are close to each other in the continuous distributionspace and thus capture semantic similarity between methods
as well as between method names. Through training with
abundant AST structures in large-scale open-source projects,code2vec can retrieve highly similar (i.e. close in the vector
space) method bodies with the given one and recommends toreuse names of such methods. Alon et al. apply code2vec to
method name recommendation, and evaluation results suggestcode2vec achieves the state of the art on this task.
Another deep learning based method name recommendation
approach is proposed by Liu et al. [13]. They embed method
names and method bodies into numerical vectors with para-graph vector and word2vec/CNNs, respectively. For a givenmethod m, they retrieve the set of method names (noted NS)
that are close to min the name vector space, and the set of
methods names (noted BS) whose bodies are close to min
the body vector space. If BS/intersectiontextNS =‚àÖ, method mshould
be renamed, and the proposed approach suggests alternativeconsistent names. We do not evaluate this approach becauseit is not published yet at the time of completing this paper. Infuture, we would like to empirically investigate it and compare
it again the proposed approach.
B. Recommendation for Other IdentiÔ¨Åers
There are also a lot of automated approaches proposed to
improve the quality of identiÔ¨Åers, e.g., class, Ô¨Åeld, variable andparameter names. Caprile and Tonella [14] Ô¨Årst propose an ap-

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. proach to standardize program identiÔ¨Åers. They Ô¨Årst generate a
dictionary that transforms composing words in identiÔ¨Åers intotheir associated standard terms. Second, they infer a standardsyntax from examples representative of the different formsthat can be associated to the main grammatical functions forarranging those standardized terms into a sequence.
Deissenboeck and Pizka [1] suggest that well-formed iden-
tiÔ¨Åer names should be both concise and consistent (i.e., asingle identiÔ¨Åer represents a single concept throughout theprogram). They propose a formal model which is based onbijective mappings between concepts of methods and names
to provide a solid foundation for the deÔ¨Ånition of concise
and consistent naming. In a follow up experiment, Lawrieet al. [15] surveyed the identiÔ¨Åers found in 48 MSLOCof C, C++, Fortran and Java source code to determine theextent of violations of concise and consistent naming. Thesyntactic methodology employed by Lawrie et al. identiÔ¨Åes
potential violations of concise and consistent naming which
include some conventional patterns of naming found in Javainheritance trees.
Thies and Roth [16] identify and rename inconsistent iden-
tiÔ¨Åers through static code analysis. The rationale of their
approach is that variables on different sides of the same
assignment had better be named consistently. They considertwo types of assignments: assignments between variables, andassignments between variables (on the left side) and methodinvocations (on the right side). They represent variables andmethod invocations involved in assignments as nodes, and
connect nodes (with edges) that are involved in the same
assignments. Based on the resulting graph, they identify in-consistent identiÔ¨Åers with heuristics.
Allamanis et al. [17] propose a framework called NATU-
RALIZE that learns the coding conventions used in a code
base and suggest natural identiÔ¨Åer names to improve stylistic
consistency. NATURALIZE works by identifying names orformatting choices that are surprising according to a probabil-ity distribution learned with n-gram model over source codetokens. When surprised, NATURALIZE determines whetherit is sufÔ¨Åciently conÔ¨Ådent to suggest a renaming. If yes, it
uniÔ¨Åes the surprising name with one that is preferred in similar
contexts elsewhere in its training set. LEAR, proposed by Linet al. [18], is a variation of NATURALIZE. It differs fromNATURALIZE in the following aspects. First, LEAR focuseson such tokens only that contain lexical information. Second,
LEAR checks the validity of possible renaming identiÔ¨Åerswhereas NATURALIZE does not.
Raychev et al. [19] build a scalable prediction engine called
JSNICE for predicting properties (including both type andname) of identiÔ¨Åers in the context of JavaScript. They Ô¨Årstconvert source code into a representation called dependency
network that captures relationships between program elements,
whose properties are to be predicted, with elements, whoseproperties are known. Once the network is obtained, theyperform structured prediction based on a learned conditionalrandom Ô¨Åeld model.
Pradel and Sen [20] formulate the problem of name-basedbug detection as a binary classiÔ¨Åcation problem. They de-
velop a framework to generate training data and to train abinary classiÔ¨Åer. They create training data by simple programtransformation that yields likely buggy programs. The binaryclassiÔ¨Åer is then trained on such automatically generated train-ing data (negative samples) as well as buggy free programs(positive samples).
C. Machine Learning based Code Recommendation
Recommender systems have a wide range of applications
in software engineering to improve productivity and reliabil-ity [21], [22]. Many of these systems employ machine learning
techniques to assist developers in writing or maintaining
software source code. The most popular recommender systemused in integrated development environments (IDEs) is coderecommendation system [23]. Hindle et al. [24] are the Ô¨Årst toemploy n-gram model in recommending the next code token
for developers by statistically learning the repetitiveness of
source code in token level. Following their work, a seriesof n-gram based approaches are proposed to recommend thenext token by exploiting large-scale dataset [25], augmentingwith semantic information [26], employing formal propertiesof code [27] and adding cache mechanism [28], [29]. Apart
from token-level models, graphic probability models are also
successfully employed to recommend the next API methodcall [30], [31] Such models statistically learn the probabilitydistribution of API usage graphs [32] extracted from sourcecode snippets, and then recommend the next API by computing
the appearance probability of each API against a given usagegraph. Another line of machine learning based code recom-
mendation approaches leverage extensively used deep neuralnetworks. Raychev et al. [27] Ô¨Årst employ RNN model [33] forcode recommendation and White et al. [34] also demonstrateits high effectiveness in recommending sequential source code.
Li et al. [35] augment RNN model with attention mecha-
nism [12] and pointer copy component [36] to recommendthe next AST node in both type and name of identiÔ¨Åers.
III. E
XPERIMENTAL SETUP
This section speciÔ¨Åes the setup of the experiment, i.e.,
approaches selected for the evaluation, research questionsexpected to answer, and metrics employed to quantitativelyassess the performance of the evaluated approaches.
A. Evaluated Approach
To evaluate the state of the art in ML-based recommendation
of method names, we select code2vec [9] for the evaluation.
code2vec is selected because of the following reasons. First,
it represents the state of the art in this Ô¨Åeld. As introducedin Section II, code2vec was proposed recently on POPL
2019, and proved signiÔ¨Åcantly more accurate than alternative
approaches [9]. Second, the source code of its implementa-
tion is publicly available, which signiÔ¨Åcantly facilitates theevaluation. It also facilitates other researchers to replicate theexperiment. We make the replication package of the evaluationpublicly available on GitHub [37] to facilitate third-party
replication and further investigation.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. B. Research Questions
The experiment investigates the following research ques-
tions:
‚Ä¢RQ1 : How well does code2vec work on datasets other
than the one employed by the original evaluation con-ducted by the authors of code2vec ?
‚Ä¢RQ2 : How well does code2vec work with more realistic
settings?
‚Ä¢RQ3 : Can code2vec generate method names correctly
when the given method bodies do not contain method
name tokens?
‚Ä¢RQ4 : Where and why does code2vec work?
‚Ä¢RQ5 : Where and why does code2vec fail?
‚Ä¢RQ6 :I s code2vec useful for developers? How often
does code2vec recommend correctly for methods that are
challenging to name manually?
Research question RQ1 validates whether code2vec keeps
highly accurate when evaluation data is replaced. code2vec
was originally evaluated on a big dataset (called original
dataset for short) [9]. Besides that, we employ another publicly
available dataset [38] (called new dataset for short) that
consists of 1,000 top-starred Java projects from GitHub. An-swering this question may reveal the generality of code2vec .
Research question RQ2 validates code2vec ‚Äôs sensibility to
the empirical settings. The original evaluation conducted in [9]
is Ô¨Åle based, i.e., Java Ô¨Åles from subject applications areshufÔ¨Çed, and randomly divided into three groups (training,validation and testing sets). As a result, while recommendingmethod names for a project, code2vec may exploit a large
number of Ô¨Åles from the same project. We call such validation
setting Ô¨Åle-based validation . Although Ô¨Åle-based validation is
reasonable, it is quite often that developers create new projectsfrom scratch, and thus they don‚Äôt have enough data fromthe current project to train the method name recommendationmodels. Project-based validation Ô¨Åts this scenario. It also
releases developers from on-site training, which requests deep
understanding of the underneath learning models. In thissetting ( project-based validation ), we pre-train code2vec off-
line with a corpus of open-source subject applications and nosource code from the testing project is exploited for training.The key advance of project-based validation is that the time
and resource consuming model training is conducted once and
for all before such models are actually exploited by develop-ers for method name recommendation. The third validationsetting is project-based non-overriding validation . The only
difference between project-based non-overriding validation
and project-based validation is that overriding methods are
excluded by the former pattern but included by the latter.Although the original evaluation in [9] excludes constructorsbecause it is unlikely that developers do not know how to nameconstructors, they do not exclude overriding methods that arequite similar to constructors. Overriding methods share the
same names with methods they override, and thus it is unlikely
that developers need help in naming such methods.Research question RQ3 investigates the performance of
code2vec when the given method bodies do not contain the
method name tokens. Answering research question RQ3 mayreveal to what extent code2vec can coin (instead of copy )
method names correctly.
Research questions RQ4 and RQ5 investigate the strengths
and weaknesses of code2vec . It is likely that code2vec works
well on one category of methods, but works badly on anothercategory. Answering these two research questions may revealwhere code2vec succeeds/fails and the rationale for the suc-
cess/failure.
Research question RQ6 concerns the usefulness of
code2vec . The usefulness depends on both accuracy of the rec-
ommendation and the difÔ¨Åculty of method name construction.
Ifcode2vec frequently fails when requested to recommend
difÔ¨Åcult method names (i.e. when really needed), it wouldbe useless for developers. To this end, we manually scorethe difÔ¨Åculty in method name construction, and evaluate theusefulness of the approach based on the scores. Answeringresearch question RQ6 helps to reveal how often code2vec
works when it is really needed.
C. Metrics
Commonly employed metrics for method name recommen-
dation include P recision @k,Recall @k
, and F1@k[6]‚Äì[8].
Precision @kpresents the precision of top krecommendation,
and it is computed as follows:
P recision @k=Naccepted @k
Nrecommended, (1)
where Naccepted @kis the number of cases where the evaluated
approach succeeds, and Nrecommended is the number of cases
the evaluated approach tries. Notably, the approach succeeds ifand only if one of items within the top krecommendation list
is accepted. In our evaluation, a recommended method name
is accepted if and only if it is identical to the one manually
constructed by developers. However, it is possible that therecommended name is acceptable although it is differentfrom the original one, especially when the original one is animproper name. Consequently, the performance we report isexactly the lower-bound, i.e., the actual performance could be
slightly higher from what we report in the paper.
Similarly, Recall @kandF1@kare computed as follows:
Recall @k=N
accepted @k
Ntested(2)
F1@k =2√óP recision @k√óRecall @k
P recision @k+Recall @k(3)
where Ntested is the number of tested methods, i.e. the size of
testing set. For approaches, e.g., code2vec , that always make
recommendation regardless of the input, Ntested is equal to
Nrecommended . Consequently, their precision and recall are
always equal, i.e., P recision @k=Recall @k.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. TABLE I
EV ALUATION RESULTS ON DIFFERENT DATASETS
Datasets Number of methodsPrecision / Recall
MRR
Rank 1 Rank 5 Rank 10
Original Dataset 13,376,807 49.89% 56.99% 58.41% 52.96%
New Dataset 3,826,986 43.87% 49.90% 51.33% 46.51%
In addition, we employ the Mean Reciprocal Rank to assess
the performance of method name recommendation approaches.It is computed as follows:
MRR =1
NtestedNtested/summationdisplay
i=11
rank i(4)
where rank iis the rank of correct name within the recom-
mendation list for the i-th testing method.
IV . R ESULTS AND ANALYSIS
A. RQ1: Comparable Performance on Different Datasets
To address research question RQ1, we evaluate code2vec
on a new dataset as well as its original dataset employed inpaper [9]. To be aligned to the evaluation in paper [9], werandomly select 50,000 Java Ô¨Åles from each of the datasets astesting set, and another 50,000 as validation set. Others areemployed as training set.
Evaluation results are presented in Table I. The Ô¨Årst column
presents datasets employed for the evaluation. The secondcolumn presents the size of involved datasets. The third to Ô¨Åfthcolumns show the performance (precision/recall) of code2vec
to quantitatively present how often correct method names
appear in the top- krecommendation list. The last column
presents the Mean Reciprocal Rank of code2vec . Notably, for
code2vec , its recall is always equal to the precision. The reason
has been presented in Section III-C.
From the table, we make the following observations:
‚Ä¢First, code2vec is accurate in recommending method names.
The top 1 recommendation is often (at a chance of 49.89%on original dataset and 43.87% on new dataset) correct. Ithas a great chance (58.41% on original dataset and 51.33%on new dataset) to present the correct method names on
its top 10 recommendation list. High MRR (52.96% and
46.51%) also suggests that the correct names are oftenranked on the top.
‚Ä¢Second, switching datasets does not result in signiÔ¨Åcantreduction in performance. Although the precision/recall isslightly reduced (e.g., from 49.89% to 43.87% on rank 1),the major reason for the reduction is the size of new dataset:the number of methods in new dataset is only 28.6% of that
inoriginal dataset .
We conclude from the preceding analysis that code2vec
overall is accurate, and switching to a new large-scale datasetdose not result in signiÔ¨Åcant reduction in performance ofcode2vec .
Fig. 1. Evaluation Results with Different Settings
B. RQ2: Realistic Settings Result in Reduced Performance
To address research question RQ2, we evaluate code2vec
on our new dataset with different settings, i.e., Ô¨Åle-based
validation ,project-based validation , and project-based non-
overriding validation .File-based validation partitions dataset
into training, validation and test sets at Ô¨Åle level whereas theother two validations work at project level. The only differ-ence between project-based validation and project-based non-
overriding validation is that overriding methods are excluded
by the latter but included by the former.
Evaluation results with different settings are presented in
Fig. 1. From the Ô¨Ågure, we make the following observations:
‚Ä¢First, switching evaluation setting from Ô¨Åle-based val-
idation toproject-based validation decreases the per-
formance signiÔ¨Åcantly. The precision/recall is signiÔ¨Å-cantly reduced by 46.84%=(43.87%-23.32%)/43.87% atrank 1, 41.98%=(49.90%-28.95%)/49.90% at rank 5, and40.42%=(51.33%-30.58%)/51.33% at rank 10.
‚Ä¢Second, excluding overriding methods further decreasesthe performance of code2vec . Notably, overriding meth-
ods are popular and account for 30%=1,147,980/3,826,986of methods in the dataset. Excluding such methods re-
duces precision/recall by 12.14%=(23.32%-20.49%)/23.32%
at rank 1, 13.96%=(28.95%-24.91%)/28.95% at rank 5, and14.09%=(30.58%-26.27%)/30.58% at rank 10.
‚Ä¢Third, overall, code2vec works well with different settings.
Its precision/recall keeps greater than 20% regardless of the
change in settings, suggesting that on more than one Ô¨Åfth
of cases code2vec succeeds in inferring the exact method
name. The high MRR (46.51%, 25.76%, and 22.43% inÔ¨Åle-based validation ,project-based validation , and project-
based non-overriding validation , respectively) also suggests
that code2vec is quite promising.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. TABLE II
EV ALUATION RESULTS ON UNSEEN /SEEN TOKENS
Token Types Total ( Nt) Correct ( Nc)Nc/Nt
Unseen Tokens 385,225 85,034 22.07%
Seen Tokens 581,237 152,826 26.29%
We have not yet fully understand why switching from
Ô¨Åle-based validation toproject-based validation results in so
much reduction (more than 40%) in performance. A possiblereason for the signiÔ¨Åcant reduction is that Ô¨Åle-based validation
leverages the nature of source code: local repetitiveness [28],
[29]. It proved that source code has the characteristic of local
repetitiveness within projects, and machine learning modelsmay learn special patterns that are speciÔ¨Åc to a given projectif they are trained with part of the project and then applied tothe other part of the project [28], [29]. File-based validation
exploits the major part of a subject application while another
small part of the same application is under test. As a result,
code2vec can take full advantage of local repetitiveness. How-
ever, project-based validation partitions data at the granularity
of projects, i.e., a single project as a whole is taken aseither testing data or training data (but never the both at the
same time). Consequently, it cannot take advantage of local
repetitiveness as Ô¨Åle-based validation does.
We conclude from the preceding analysis that the perfor-
mance of code2vec decreases signiÔ¨Åcantly on more realistic
settings. However, its overall performance is still promising.
C. RQ3: Coining Method Names
To address research question RQ3, we evaluate the perfor-
mance of code2vec in generating seen and unseen method
name tokens, respectively. For each of method names in new
dataset , we split it into tokens according to the Camel-Case
naming convention. A token tfrom method name mn is a
seen token if tappears (case insensitive) in the body named
bymn. Otherwise, it is an unseen token. We assess how often
code2vec can successfully recommend seen/unseen method
name tokens during project-based non-overriding validation
conducted in Section IV-B.
Evaluation results are presented in Table II. The Ô¨Årst column
presents the types of method name tokens: unseen tokens
and seen tokens. The second column presents total number
of seen/unseen method name tokens in the testing set. Thethird column presents the number of correctly recommendedseen/unseen method name tokens. A token tfrom method
name mnis correctly recommended if and only if the name
recommended for the method body of mn contains token
t. The forth column presents the chance that seen/unseen
method name tokens are recommended correctly. The chanceis computed by dividing the number of correctly recommendedseen/unseen method name tokens by the total number.
From the table, we make the following observations:
‚Ä¢First, a large percentage of method name tokens
(39.86%=385,225/(385,225+581,237)) are unseen, i.e., theydo not appear in the input (method bodies). Consequently,
ML-based approaches that strongly rely on copy mechanism
to select tokens from input have signiÔ¨Åcant limitation ontheir maximal potential.
‚Ä¢Second, code2vec works quite well in recommending unseen
method name tokens. It successfully generates 22.07% of
unseen tokens. Its performance in generating unseen method
name tokens is even comparable to that (26.29%) on seentokens.
code2vec can generate unseen method name tokens because
it does not select (copy) tokens from its input, i.e., method
body. In contrast, it extracts features of methods (i.e., pathsconnecting nodes in the AST of method bodies), and associatessuch features with method names. As a result, if the givenmethod body ( mb
1) shares the same or highly similar features
with another method body ( mb2) in the training set, code2vec
may recommend the method name ( Name mb2)o f mb2for
mb1even if the tokens of Name mb2do not appear in mb1.
A typical example is presented in Listing 1. code2vec
successfully coins the method name for the testing method onLine 1 though the token contains doesn‚Äôt appear in the body.
Through retrieving training methods named as contains (e.g.
the method on Line 9), we Ô¨Ånd that their syntax structuresexploited by code2vec are always similar. Thus code2vec can
successfully recognize the speciÔ¨Åed method body associatedwith contains and suggest to reuse the training method name.
1public static boolean contains ( String str ,
2 String [] array ) {
3 for( String s : array )
4 if( str . equals (s))
5 return true ;
6 return false ;
7}
8
9public static boolean contains ( int[] values ,
10 int candidate ) {
11 for(int i=0 ;i <values . l ength ; i++)
12 if( values [ i ] = = candidate )
13 return true ;
14 return false ;
15}
Listing 1. An Example of Coined Method Name
From the analysis in the preceding paragraphs, we conclude
that code2vec works well in generating unseen method name
tokens.
D. RQ4: Where and Why code2vec Works
To investigate where and why code2vec works, we manu-
ally analyze cases where it works during project-based non-
overriding validation . Notably, code2vec succeeds on a large
number (61,906) of methods, and thus it is challenging, if notimpossible, to manually analyze all of these methods. To this
end, we randomly sample one thousand of these methods for
manual analysis. Based on the manual analysis, we make thefollowing observations:
‚Ä¢First, a large portion (48.3%=483/1,000) of accepted namesare recommended for getter/setter methods.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. Fig. 2. Common AST Pattern of Getter Methods
‚Ä¢Second, it is quite often (at a chance of 19.2%) that the cor-
rect method name could be copied from the correspondingmethod body. In the rest of this paper, we call such methodsname-contained methods.
To investigate why code2vec works well on getter/setter
methods, we manually analyze features of such methods. Two
typical getter methods are presented in Listing 2. Their ASTs
are presented in Fig. 2. From this Ô¨Ågure, we observe thatthe two trees are highly similar, and thus their features (i.e.,paths that connect each pair of terminal nodes in the AST) arealso highly similar. According to our manual analysis, mostof getter methods have highly similar ASTs. Consequently,code2vec can distinguish getter/setter methods because of the
features, and generate method names according to the terminal
node on the bottom right corner of the AST.
1public int getPadding () {
2 return padding ;
3}
4
5public int getLength () {
6 return length ;
7}
Listing 2. Getter Methods
1public void reset () {
2 // delegation
3 reset ( operation ) ;
4}
5
6public void reset ( String operation ) {
7 // delegation and server
8 reset ( operation , true );
9}
10
11 public void reset ( String newOperation ,
12 boolean logOperationOnChange ) {
13 // server
14 if(t i m e ! = null )
15 if( operation . equals(new Operation ) ||
16 logOperationOnChange )
17 logOperation ( operation , ela psedTime () ) ;
18 time = new Date () . getTime () ;
19 if(! operation . equals ( newOperation ) ) {
20 operation = n ewOperation ;
21 log . info ( String . format(
22 ‚ÄùOperation ‚Äô%s ‚Äô started .‚Äù , newOperation ) ) ;
23}
24}
Listing 3. Delegations
Fig. 3. Common Path of Delegations
Name-contained methods are created for various reasons,
e.g. delegations. A delegation is such a method that does
nothing except passing messages between its invoker andanother method (called server). A typical example is presented
in Listing 3. Three overloading methods share the same name
reset , which is required by overloading mechanism. Overload-
ing method with fewer parameters (e.g., the one on Line 1)often serves as a delegation to more complex one (e.g., theone on Line 6) with default arguments. As a result, the name‚Äòreset ‚Äô of invoked method appears within the method body of
the delegation (on Line 3), and it happens that this name is
identical to that of the delegation. The same is true for another
delegation (on Line 6) whose server is the method deÔ¨Åned
on Line 11. According to our manual analysis, delegationsaccount for a large percentage (45.83%) of the name-containedmethods.
To investigate why code2vec works well on delegations, we
analyze the AST of delegations. ASTs of the delegations inListing 3 are presented in Fig. 3. From this Ô¨Ågure, we observethat such delegations share a common path (shown in dashedline). The path travels from the method name (terminal nodereset ) to the root of the tree ( MethodDeclaration ), passes the
root of method body ( BlockStatement ) and the only statement
(ExpressionStatement and MethodCallExpression ), and Ô¨Ånally
reaches the name of invoked method (another terminal nodereset ).code2vec can learn to decide whether a given method
body is a delegation by looking for such a path (ignoring thetwo terminals). If a delegation, code2vec suggests to reuse the
name of invoked method as the name of the delegation.
We conclude from the preceding analysis that code2vec
works well on getter/setter methods and delegations. The
rationale for the success is that such methods have common
structures in their ASTs.
E. RQ5: Where and Why code2vec Fails
To investigate where and why code2vec fails, we conduct
another manual analysis that is highly similar to that in

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. Section IV-D. The only difference is that here we analyze
one thousand failed cases during project-based non-overriding
validation whereas Section IV-D analyzes successful cases.
Based on the manual analysis, we Ô¨Ågure out the following
major reasons for the failure. First, out-of-vocabulary method
names are the primary for the failure. A method name is out-
of-vocabulary if and only if none of the methods in trainingset is named with it. Notably, code2vec builds a vocabulary
of method names by collecting all unique method names inthe training set, and selects a name from this vocabulary as
the recommendation for a given method body. Consequently,if the optimal method name is not included in the vocabulary,
code2vec has no chance to generate it. This kind of methods
are common, accounting for 52%=157,128/302,200 of meth-ods in the testing set. As a result, code2vec fails frequently
for this reason. Employing open vocabulary or coining methodname by composing tokens may help to further improve theperformance.
The second reason for the failure is that the exploited
method features are often insufÔ¨Åcient to infer method names.Listing 4 presents a typical example. Method bodies of deep-
Copy and build are almost identical except for the data type
(TIncrement vs. ReportAttributes ). Consequently, their AST
paths exploited by code2vec as method features are highly
similar as well. As a result, code2vec recommends the same
name for them. Another example is presented in Listing 5,where two methods ( errand rawError ) from different appli-
cations have the same method body. However, they are nameddifferently by their authors. All of these examples suggest thatthe exploited feature, sometimes even the whole input (methodbody), is often insufÔ¨Åcient to infer method names. To furtherimprove the performance, additional information, e.g., theirenclosing classes, should be exploited as well.
1public TIncrement deepCopy () {
2 return n e w TIncrement ( this );
3}
4
5public ReportAttributes build () {
6 return n e w ReportAttributes ( this );
7}
Listing 4. Similar Methods Named Differently
1public static void err ( String m s g) {
2 System. err . println (m s g);
3}
4
5public static void rawError( String m s g) {
6 System. err . println (m s g);
7}
Listing 5. Identical Methods Named Differently
The third reason for the failure is improper method names
in the testing set. During the evaluation, all recommendednames are compared against original names associated withtesting methods, i.e., the code2vec fails if the recommended
name is different from the original one. However, it is likely
that the recommended name could be even better than theoriginal one. In such cases, code2vec fails, and we call suchcases false negatives. For example, code2vec generates name
‚ÄòinitRecyclerView ‚Äô for the method on Line 1 in Listing 6. The
recommended name is better than the original one ‚Äò initial-
izeRecyclerview ‚Äô because the abbreviation ‚Äòinit‚Äô signiÔ¨Åcantly
shortens the identiÔ¨Åer whereas keeps the readability. Notably,‚Äòinit‚Äô has been widely employed to represent ‚Äòinitialize‚Äô (e.g.
in a large number of JDK methods) and thus it is not
difÔ¨Åcult for experienced developers to guess the meaning. Therecommended new name also successfully corrects the spellingof ‚ÄòRecyclerview‚Äô to make it consistent with Camel Casenaming convention. Another example is presented on Line 11,
for which code2vec generates name ‚Äò setPassword ‚Äô. It is better
than the original name ‚Äò password ‚Äô because the latter fails to
reveal the intent of method, and violates naming conventions:method names should be a verb or a verb-noun phrase [39],[40].
1private void initializeRecyclerview () {
2 RecyclerView . Lay outManager l ayoutManager =
3 new GridLayoutManager ( getActivity () , 2);
4 recyclerView . setLayoutManager ( layoutManager ) ;
5 recyclerView . setHasFixedSize ( true );
6 adapter = new RecyclerViewAdapter (
7 getActivity () . getApplicationContext () );
8 recyclerView . setAdapter ( adapter ) ;
9}
10
11 public KeycloakBuilder password (
12 String password) {
13 this . password = password ;
14 return this ;
15}
Listing 6. Improper Method Names in Testing Set
F . RQ6: Limited Usefulness for Developers
To answer research question RQ6, we investigate how often
code2vec works when it is strongly needed. The investigation
is conducted as follows:
‚Ä¢First, we invite six developers involved in a commercial
project for the evaluation. The commercial project has been
released recently by a giant of IT industry to conduct large-
scale software refactorings.
‚Ä¢Second, for each of the participants, we randomly select one
hundred methods developed by himself/herself.
‚Ä¢Third, we request all participants to score the difÔ¨Åculty in
naming sampled methods. Notably, participants do not scoremethods developed by other developers, and thus each ofthem score exactly one hundred methods. The scores rank
between one and Ô¨Åve (i.e. 5-point scale [41]‚Äì[44]) whereone represents least difÔ¨Åculty and Ô¨Åve represents highestdifÔ¨Åculty.
‚Ä¢Fourth, we apply code2vec ofproject-based non-overrding
validation to the scored methods, and validate the recom-
mendations against manually constructed names.
Evaluation results are presented in Table III. The Ô¨Årst
column presents the difÔ¨Åculty scores in naming methods. Thesecond column presents the number of methods with the spe-ciÔ¨Åc difÔ¨Åculty. The third column presents the precision/recallofcode2vec at rank 1 on the given methods.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. TABLE III
EV ALUATION RESULTS ON MANUALLY SCORED METHODS
Scores Number of methods Precision / Recall
1 (Very Easy) 287 34.84%
2 (Easy) 159 6.92%
3 (Normal) 91 5.49%
4 (DifÔ¨Åcult) 50 2.00%
5 (Extremely DifÔ¨Åcult) 13 0.00%
Total 600 19.50%
From the table, we make the following observations:
‚Ä¢First, a signiÔ¨Åcant percent of methods are difÔ¨Åcult to name
even for authors of such methods. We note that out of600 methods, 13 are extremely difÔ¨Åcult to name, and 50are difÔ¨Åcult to name. In total, 10.5% = (13+50)/600 of the
methods are difÔ¨Åcult to name for experienced developers.
‚Ä¢Second, the overall precision/recall (19.50%) of code2vec
is comparable to that on large-scale dataset employed inSection IV-B where its precision at rank 1 is 20.49%. Itsuggests that conclusions drawn on open-source applicationsmay hold on commercial closed-source applications as well.
‚Ä¢Third, code2vec works well on very-easy-to-name methods,
resulting in high precision of 34.84%.
‚Ä¢Finally, code2vec rarely succeeds on difÔ¨Åcult-to-name meth-
ods where recommendation on method names is stronglyneeded. It fails on all of 13 extremely-difÔ¨Åcult-to-namemethods, and 49 out of 50 difÔ¨Åcult-to-name methods. Over-
all, its precision on such methods is signiÔ¨Åcantly reduced
to 1.6%. We also notice that its precision decreases signiÔ¨Å-cantly with the increase of difÔ¨Åculty scores.
From the analysis in the preceding paragraph, we conclude
that code2vec rarely works when it is strongly needed.
G. Threats to V alidity
A threat to the external validity is that we employ only one
new dataset to validate the impact of switching datasets on
the performance of code2vec . It is likely that our conclusion
may not hold if other datasets are involved. To reduce thethreat, we perform our assessment and analysis on a large-scale dataset composed of 1,000 projects. It is challengingto collect additional comparable dataset for the evaluation.Another threat to external validity is that only 600 manuallyscored methods are involved in our experiment to assess theusefulness of code2vec . It is challenging to recruit more expert
developers from the industry for the evaluation. Another threatto validity concerning the manual scoring is that the scoringis rather subjective. Consequently, conclusions drawn on such
participants may not hold for other developers.
A threat to construct validity is that we assess the cor-
rectness of generated names based on their equivalence tothe manually constructed ones. However, as introduced in
Section IV-E, it is likely that the recommended method namescould be better than manually constructed ones, which makesthe assessment inaccurate. To reduce the threat, we employ
only top-starred projects where most of the method names arelikely well-constructed.
V. H
EURISTICS BASED ALTERNATIVE APPROACH
code2vec is a complicated ML-based approach that is difÔ¨Å-
cult to interpret. It also requires training on a large corpusof high quality source code that is both time consuming
and resource consuming. To investigate the possibility ofdesigning a simple and straightforward approach for methodname recommendation, in this section, we propose a heuris-
tics based approach HeMa that outperforms code2vec . The
implementation (source code) of HeMa is publicly available
on GitHub [37].
A. Overview
HeMa is essentially a sequence of heuristics. For a given
method body mb,HeMa works as follows:
1) First, based on a sequence of heuristics, HeMa decides
whether mb is a getter/setter method. If yes, HeMa
generates method name for it automatically based onanother sequence of heuristics.
2) Second, based on a sequence of heuristics, HeMa decides
whether mb is a delegation. If yes, HeMa generates
method name for it automatically based on another se-quence of heuristics.
3) Third, if the preceding heuristics fail, HeMa employs
a sequence of heuristics to retrieve methods in a large
corpus that share the same return type and parameters
(both parameter names and parameter types but regardlessof parameter orders) with mb. From the resulting set
of methods (notated as S
m),HeMa picks up the most
popular method name and suggests it to mb.I fSm=‚àÖ,
HeMa refuses to make any recommendation.
Details of the key steps are presented in the following sections.
B. Distinguishing Getter/Setter Methods
Getter and setter methods often follow common patterns.
One of the most well-known patterns for getter methods is‚Äúreturn ${Ô¨Åeld};‚Äù whereas ‚Äú$ {Ô¨Åeld}=${param};‚Äù is for setter
methods. Following these patterns, we deÔ¨Åne a sequence ofheuristics to distinguish getter and setter methods from others.
For a given method body mb,HeMa distinguishes whether
mbis a getter method as follows:
‚Ä¢First, if the given method body returns nothing, i.e. the return
type is void, the given method is not a getter;
‚Ä¢Second, if the given method body contains more than one
ReturnStatements ,HeMa will not recognize it as a getter
method;
‚Ä¢Third, if the value returned by the only ReturnStatement is
a Ô¨Åeld (notated as $ {Ô¨Åeld}) declared within the enclosing
class, it is a getter method.
For the potential getter method, HeMa composes a method
name as ‚Äúget$ {Ô¨Åeld}‚Äù.
HeMa distinguishes method body mbas a setter method if:
‚Ä¢The given method body has at least one parameter;

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. TABLE IV
COMPARISON BETWEEN HEMA AND CODE 2VEC
ScoresHeMa code2vec
Precision Recall F1 Precision
1 54.59% 41.46% 47.13% 34.84%
2 25.00% 10.69% 14.98% 6.92%
3 7.69% 3.30% 4.62% 5.49%
4 9.09% 4.00% 5.56% 2.00%
5 0.00% 0.00% 0.00% 0.00%
Total 39.94% 23.50% 29.59% 19.50%
‚Ä¢The body is composed of a single assignment;
‚Ä¢The left hand side of the assignment is a Ô¨Åeld (notated as
${Ô¨Åeld}) declared within the enclosing class;
‚Ä¢And, the right hand side of the assignment is a parameter
of the method (notated as $ {param}).
For the potential setter method, HeMa composes the method
name as ‚Äúset$ {Ô¨Åeld}‚Äù.
C. Distinguishing Delegations
For a given method body mb,HeMa distinguishes it as a
delegation if:
‚Ä¢The method body contains a single statement;
‚Ä¢And the statement is a ReturnStatement that returns an
invocation on another server method (notated as sMethod ).
For the potential delegation, HeMa recommends to reuse the
name of invoked method (i.e. sMethod ).
D. Frequency-based Name Recommendation
If the given body mbis neither geter/setter nor delegation,
HeMa retrieves a set of methods ( Sm) that share the same
return type and the same parameter list (including both param-eter types and names but regardless of the order of parameters).IfS
mis empty, HeMa refuses to make any recommendation.
Otherwise, it selects the method name with highest frequencyinS
m, and recommends to use this name for the given method
body.
E. Comparison Against code2vec
To compare HeMa against code2vec , we evaluate HeMa
on the new dataset employed in Section IV-B with project-
based non-overriding validation . Evaluation results suggest
that HeMa signiÔ¨Åcantly outperforms code2vec . Its precision,
recall, and F1 at rank 1 are 33.86%, 25.09%, and 28.82%,respectively. In contrast, the precision of code2vec at rank 1
is 20.49% (notably, its recall and F1 are equal to the precisionas explained in Section III-C). Compared to state-of-the-art
code2vec ,Hema successfully improves precision, recall, and
F1 by 65.25%=(33.86%-20.49%)/20.49%, 22.45%=(25.09%-20.49%)/20.49%, and 40.65%=(28.82%-20.49%)/20.49%, re-spectively.
We also evaluate HeMa with manually scored dataset used
in Section IV-F to compare the usefulness of HeMa againstcode2vec . Evaluation results (at rank 1) are presented in
Table IV. From the table, we observe that HeMa outper-
forms code2vec on difÔ¨Åcult-to-name methods, improving the
precision, recall and F1 from 2.00% to 9.09%, 4.00% and5.56%, respectively. One of the reasons for the improve-ment is that the frequency-based name recommendation em-
ployed by HeMa works for many complicated method bodies.
This heuristic alone successfully generates method names for8.99%=27,155/302,200 of the testing methods in new dataset .
From the analysis in the preceding paragraphs, we conclude
that the heuristics based HeMa signiÔ¨Åcantly outperforms the
state-of-the-art ML-based code2vec .
VI. D
ISCUSSION
A. Implications
The empirical study in Section IV and the alternative ap-
proach proposed in Section V have the following implications:
1) Empirical Settings Are Critical: We switch from casual
empirical setting to more realistic ones in Section IV-B, andresults suggest that the switching leads to signiÔ¨Åcant reduction
in performance. It implicates that empirical settings should be
carefully designed, and should be as close as possible to realscenarios in the industry. It is quite often that researchers, espe-cially those from Ô¨Åelds (e.g., AI and NLP) other than softwareengineering, pay little attention to the application scenarios
of proposed automatic software engineering tools/approaches.
As a result, the empirical settings are signiÔ¨Åcantly differentfrom real scenarios, and thus the evaluation results fail toreveal the reality of such approaches. Notably, recent empiricalstudy conducted by Hellendoorn et al. [45] results in similarimplications. They analyze empirical settings employed in the
evaluation of code completion tools, and suggest that such
settings are essentially different from real ones in practice.They empirically switch to more realistic settings, and resultssuggest that the switching leads to signiÔ¨Åcant reduction inperformance. Our Ô¨Åndings are highly consistent with theirs.
2) A Friend in Need Is A Friend Indeed: Evaluation
results in Section IV-F suggest that although the overall per-formance of code2vec is promising, it is not much useful. The
major reason is that it frequently works when it is not stronglyneeded but fails when it is in need. The evaluation may suggest
that overall performance alone could be insufÔ¨Åcient to measure
the usefulness of automatic software engineering tools. Weshould allocate priority to cases where the tools are urgentlyneeded, and assess how often the tools work in such cases.
3) Complex Approaches Are NOT Necessarily Better
than Simple and Straightforward Ones: To investigate
the possibility of designing a simple and straightforwardapproaches whose performance is comparable to code2vec ,w e
propose a heuristics based approach called HeMa . Evaluation
results in Section V-E suggest that this simple approach sig-
niÔ¨Åcantly outperforms the state-of-the-art ML-based code2vec .
The Ô¨Ånding is consistent with recent reports by Fu andMenzies [46] and by Liu et al. [44] in that complex approachesare not necessarily better than simple and straightforwardones. The empirical study conducted by Fu and Menzies [46]

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. suggests that simple Differential Evolution (DE) and SVM
are comparable to (if not better than) complex convolutionalneural network (CNN) in retrieving linkable questions fromStack OverÔ¨Çow. The empirical study conducted by Liu etal. [44] suggests that the classical k-Nearest Neighbor model(KNN) works even better than advanced state-of-the-art deeplearning models in generating commit messages. Our eval-uation, as well as existing reports [44], [46], suggest thatsimple and straightforward approaches are worth a shot beforecomplicated and time-consuming techniques are exploited forsoftware engineering tasks.
4) Coining Method Names with Tokens Outside Method
Body: As suggested by the evaluation results in Section IV-E,
52% of method names in testing set are not used as methodnames in the large-scale training set. Consequently, methodname recommendation approaches, e.g., code2vec , that encode
method names as a whole, fail to generate such names unseenin their bodies. A potential way to solve this issue is to encodetokens instead of whole method names, and to coin method
names at Ô¨Åne-grained token level. However, we also notice
from Table II that only 60.14%=581,237/(385,225+581,237)of the method name tokens could be copied from the input(associated method bodies). Consequently, coining methodnames with tokens from the method body only could beinsufÔ¨Åcient. Enlarging the search scope is potentially a good
solution: up to 81% of the method name tokens can be found
within the enclosing class, and up to 94% can be found withinthe enclosing package.
B. Limitations
The Ô¨Årst limitation of the empirical study is that only
one state-of-the-art approach (i.e., code2vec ) is involved in
the study. Notably, ML-based approaches, especially deeplearning based ones [6], [7], are often time and resourceconsuming. Consequently, involving more baselines in the
study could signiÔ¨Åcantly increase the cost in both computing
resource and human resource of manual analysis. For example,we evaluated convolutional attention network [7] with theemployed datasets, and it failed to generate complete results inseveral days. We only select code2vec for evaluation because
it represents the state of the art and proved signiÔ¨Åcantly better
than other approaches [9]. However, in future work, involving
more baseline approaches in the empirical study may increasethe generality of conclusions drawn on the empirical study.
The second limitation is the limited usefulness of alternative
approach proposed in Section V. We design this approach tointuitively reveal the state of the art as well as the possibility ofdesigning simple but effective approaches. Evaluation results
in Section V-E suggest that it has successfully accomplished
its mission. However, it should be noted that most of methodnames it recommends successfully are associated with simplemethods, like getter/setter and delegations. Developers rarelyneed help in naming such simple methods, which may suggestthat usefulness of the approach is limited.VII. C
ONCLUSIONS AND FUTURE WORK
Researches have recently achieved signiÔ¨Åcant advances in
machine learning techniques. As a result, many of the result-ing advanced machine learning techniques are exploited tosolve software engineering tasks, e.g., code completion andmethod name recommendation. Although machine learningbased approaches are proved accurate in generating methodnames, existing evaluations fail to reveal their real performancefor various reasons, e.g., arbitrary empirical settings. Suchevaluations fail to reveal where and why existing approacheswork or do not work. To this end, in this paper we conductan empirical study on the state-of-the-art method name recom-mendation approach code2vec with more realistic settings. Our
evaluation results suggest that code2vec deserves signiÔ¨Åcant
improvement. To intuitively reveal the state of the art and toinvestigate the possibility of designing simple and straight-forward alternative approaches, we also propose a heuristics
based approach to recommending method names according
to given method bodies. Our evaluation results suggest thatit outperforms code2vec signiÔ¨Åcantly, and improves precision
and recall by 65.25% and 22.45%, respectively.
Implication of the empirical study is severalfold. First,
empirical setting is critical for empirical study, and improper
setting can signiÔ¨Åcantly warp conclusions drawn on the em-pirical study. Second, the usefulness of automatic softwareengineering tools should not be assessed solely by theirperformance, e.g., precision and recall. Third, advanced andcomplex approaches are not necessarily better than simple and
straightforward approaches. Finally, machine learning based
recommendation of method names may not work as well asexpected. With more rigorous settings, the deep learning basedcode2vec fails frequently. However, in this paper we empir-
ically evaluate code2vec only, and other machine learning
based approaches [6]‚Äì[8], [13] may outperform code2vec in
our settings. In future, it should be interesting to investigatethis question.
Future work is needed to investigate the correlation between
difÔ¨Åculty in naming method names and source code metrics.In this paper, we measure the difÔ¨Åculty subjectively by askingdevelopers to give a number ranking from 1 to 5 to representthe difÔ¨Åculty. Such subjective ranking could be inaccurate. In
future, it would be interesting to investigate more accurate
and more objective ways to measure the difÔ¨Åculty in namingmethods. In future, it is also interesting to investigate whetherclone detection techniques could be exploited to retrievesimilar methods in corpus, and whether such techniques could
outperform the heuristics presented in this paper (Section V).
A
CKNOWLEDGMENT
The authors would like to say thanks to the anonymous
reviewers for their valuable suggestions.
The work is partially supported by the National Natural Sci-
ence Foundation of China (61690205, 61772071, 61722202)and the National Key Research and Development Program ofChina (2016YFB1000801).

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] F. Deissenboeck and M. Pizka, ‚ÄúConcise and consistent naming,‚Äù
Software Quality Journal , vol. 14, no. 3, pp. 261‚Äì282, Sep. 2006.
[Online]. Available: http://dx.doi.org/10.1007/s11219-006-9219-1
[2] D. Lawrie, C. Morrell, H. Feild, and D. Binkley, ‚ÄúWhat‚Äôs in a name? a
study of identiÔ¨Åers,‚Äù in 14th IEEE International Conference on Program
Comprehension (ICPC‚Äô06) , June 2006, pp. 3‚Äì12.
[3] V . Rajlich and N. Wilde, ‚ÄúThe role of concepts in program com-
prehension,‚Äù in Proceedings 10th International Workshop on Program
Comprehension , June 2002, pp. 271‚Äì278.
[4] E. Avidan and D. G. Feitelson, ‚ÄúEffects of variable names on
comprehension an empirical study,‚Äù in Proceedings of the 25th
International Conference on Program Comprehension , ser. ICPC ‚Äô17.
Piscataway, NJ, USA: IEEE Press, 2017, pp. 55‚Äì65. [Online]. Available:
https://doi.org/10.1109/ICPC.2017.27
[5] E. W. H√∏st and B. M. √òstvold, ‚ÄúDebugging method names,‚Äù in
Proceedings of the 23rd European Conference on ECOOP 2009‚Äî Object-Oriented Programming , ser. Genoa. Berlin, Heidelberg:
Springer-Verlag, 2009, pp. 294‚Äì317. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-03013-0
14
[6] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, ‚ÄúSuggesting accurate
method and class names,‚Äù in Proceedings of the 2015 10th Joint
Meeting on F oundations of Software Engineering , ser. ESEC/FSE 2015.
New York, NY , USA: ACM, 2015, pp. 38‚Äì49. [Online]. Available:http://doi.acm.org/10.1145/2786805.2786849
[7] M. Allamanis, H. Peng, and C. Sutton, ‚ÄúA convolutional attention
network for extreme summarization of source code,‚Äù in Proceedings
of The 33rd International Conference on Machine Learning , ser.
Proceedings of Machine Learning Research, M. F. Balcan andK. Q. Weinberger, Eds., vol. 48. New York, New York, USA:
PMLR, 20‚Äì22 Jun 2016, pp. 2091‚Äì2100. [Online]. Available:
http://proceedings.mlr.press/v48/allamanis16.html
[8] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‚ÄúA general
path-based representation for predicting program properties,‚Äù inProceedings of the 39th ACM SIGPLAN Conference on ProgrammingLanguage Design and Implementation , ser. PLDI 2018. New
York, NY , USA: ACM, 2018, pp. 404‚Äì419. [Online]. Available:http://doi.acm.org/10.1145/3192366.3192412
[9] ‚Äî‚Äî, ‚ÄúCode2vec: Learning distributed representations of code,‚Äù Proc.
ACM Program. Lang. , vol. 3, no. POPL, pp. 40:1‚Äì40:29, Jan. 2019.
[Online]. Available: http://doi.acm.org/10.1145/3290353
[10] J. Hofmeister, J. Siegmund, and D. V . Holt, ‚ÄúShorter identiÔ¨Åer names
take longer to comprehend,‚Äù in 2017 IEEE 24th International Conference
on Software Analysis, Evolution and Reengineering (SANER) , Feb 2017,
pp. 217‚Äì227.
[11] A. Schankin, A. Berger, D. V . Holt, J. C. Hofmeister, T. Riedel, and
M. Beigl, ‚ÄúDescriptive compound identiÔ¨Åer names improve source codecomprehension,‚Äù in Proceedings of the 26th Conference on Program
Comprehension , ser. ICPC ‚Äô18. New York, NY , USA: ACM, 2018,
pp. 31‚Äì40. [Online]. Available: http://doi.acm.org/10.1145/3196321.
3196332
[12] D. Bahdanau, K. Cho, and Y . Bengio, ‚ÄúNeural machine translation by
jointly learning to align and translate,‚Äù arXiv preprint arXiv:1409.0473 ,
2014.
[13] K. Liu, D. Kim, T. F. Bissyand ¬¥e, T. Kim, K. Kim, A. Koyuncu, S. Kim,
and Y . L. Traon, ‚ÄúLearning to spot and refactor inconsistent methodnames,‚Äù in Proceedings of the 41st International Conference on Software
Engineering , ser. ICSE ‚Äô19. Piscataway, NJ, USA: IEEE Press, 2019,
pp. 1‚Äì12. [Online]. Available: https://doi.org/10.1109/ICSE.2019.00019
[14] Caprile and Tonella, ‚ÄúRestructuring program identiÔ¨Åer names,‚Äù in Pro-
ceedings 2000 International Conference on Software Maintenance , Oct
2000, pp. 97‚Äì107.
[15] D. Lawrie, H. Feild, and D. Binkley, ‚ÄúAn empirical study of
rules for well-formed identiÔ¨Åers: Research articles,‚Äù J. Softw. Maint.
Evol. , vol. 19, no. 4, pp. 205‚Äì229, Jul. 2007. [Online]. Available:
http://dx.doi.org/10.1002/smr.v19:4
[16] A. Thies and C. Roth, ‚ÄúRecommending rename refactorings,‚Äù in
Proceedings of the 2Nd International Workshop on RecommendationSystems for Software Engineering , ser. RSSE ‚Äô10. New York,
NY , USA: ACM, 2010, pp. 1‚Äì5. [Online]. Available: http://doi.acm.org/10.1145/1808920.1808921
[17] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, ‚ÄúLearning natural
coding conventions,‚Äù in Proceedings of the 22Nd ACM SIGSOFTInternational Symposium on F oundations of Software Engineering , ser.
FSE 2014. New York, NY , USA: ACM, 2014, pp. 281‚Äì293. [Online].Available: http://doi.acm.org/10.1145/2635868.2635883
[18] B. Lin, S. Scalabrino, A. Mocci, R. Oliveto, G. Bavota, and M. Lanza,
‚ÄúInvestigating the use of code analysis and nlp to promote a consistentusage of identiÔ¨Åers,‚Äù in 2017 IEEE 17th International Working Confer-
ence on Source Code Analysis and Manipulation (SCAM) , Sep. 2017,
pp. 81‚Äì90.
[19] V . Raychev, M. Vechev, and A. Krause, ‚ÄúPredicting program properties
from ‚Äùbig code‚Äù,‚Äù in Proceedings of the 42Nd Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages , ser.
POPL ‚Äô15. New York, NY , USA: ACM, 2015, pp. 111‚Äì124. [Online].Available: http://doi.acm.org/10.1145/2676726.2677009
[20] M. Pradel and K. Sen, ‚ÄúDeepbugs: A learning approach to
name-based bug detection,‚Äù Proc. ACM Program. Lang. , vol. 2,
no. OOPSLA, pp. 147:1‚Äì147:25, Oct. 2018. [Online]. Available:
http://doi.acm.org/10.1145/3276517
[21] I. Avazpour, T. Pitakrat, L. Grunske, and J. Grundy, ‚ÄúRecommendation
systems in software engineering,‚Äù Dimensions and metrics for evaluating
recommendation systems , pp. 245‚Äì273, 2014.
[22] K. Mens and A. Lozano, ‚ÄúSource code-based recommendation systems,‚Äù
inRecommendation Systems in Software Engineering . Springer, 2014,
pp. 93‚Äì130.
[23] S. Amann, S. Proksch, S. Nadi, and M. Mezini, ‚ÄúA study of visual
studio usage in practice,‚Äù in 2016 IEEE 23rd International Conference
on Software Analysis, Evolution, and Reengineering (SANER) , vol. 1,
March 2016, pp. 124‚Äì134.
[24] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, ‚ÄúOn the
naturalness of software,‚Äù in 2012 34th International Conference on
Software Engineering (ICSE) , June 2012, pp. 837‚Äì847.
[25] M. Allamanis and C. Sutton, ‚ÄúMining source code repositories at mas-
sive scale using language modeling,‚Äù in 2013 10th Working Conference
on Mining Software Repositories (MSR) , May 2013, pp. 207‚Äì216.
[26] T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, ‚ÄúA
statistical semantic language model for source code,‚Äù in Proceedings of
the 2013 9th Joint Meeting on F oundations of Software Engineering ,
ser. ESEC/FSE 2013. New York, NY , USA: ACM, 2013, pp. 532‚Äì542.[Online]. Available: http://doi.acm.org/10.1145/2491411.2491458
[27] V . Raychev, M. Vechev, and E. Yahav, ‚ÄúCode completion with
statistical language models,‚Äù in Proceedings of the 35th ACM SIGPLAN
Conference on Programming Language Design and Implementation ,
ser. PLDI ‚Äô14. New York, NY , USA: ACM, 2014, pp. 419‚Äì428.[Online]. Available: http://doi.acm.org/10.1145/2594291.2594321
[28] Z. Tu, Z. Su, and P. Devanbu, ‚ÄúOn the localness of software,‚Äù in
Proceedings of the 22Nd ACM SIGSOFT International Symposiumon F oundations of Software Engineering , ser. FSE 2014. New
York, NY , USA: ACM, 2014, pp. 269‚Äì280. [Online]. Available:
http://doi.acm.org/10.1145/2635868.2635875
[29] V . J. Hellendoorn and P. Devanbu, ‚ÄúAre deep neural networks the best
choice for modeling source code?‚Äù in Proceedings of the 2017 11th
Joint Meeting on F oundations of Software Engineering , ser. ESEC/FSE
2017. New York, NY , USA: ACM, 2017, pp. 763‚Äì773. [Online].Available: http://doi.acm.org/10.1145/3106237.3106290
[30] A. T. Nguyen and T. N. Nguyen, ‚ÄúGraph-based statistical language
model for code,‚Äù in Proceedings of the 37th International Conference
on Software Engineering - V olume 1 , ser. ICSE ‚Äô15. Piscataway,
NJ, USA: IEEE Press, 2015, pp. 858‚Äì868. [Online]. Available:http://dl.acm.org/citation.cfm?id=2818754.2818858
[31] T. T. Nguyen, H. V . Pham, P. M. Vu, and T. T. Nguyen, ‚ÄúLearning api
usages from bytecode: A statistical approach,‚Äù in Proceedings of the
38th International Conference on Software Engineering , ser. ICSE ‚Äô16.
New York, NY , USA: ACM, 2016, pp. 416‚Äì427. [Online]. Available:http://doi.acm.org/10.1145/2884781.2884873
[32] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. M. Al-Kofahi, and T. N.
Nguyen, ‚ÄúGraph-based mining of multiple object usage patterns,‚Äù inProceedings of the the 7th Joint Meeting of the European SoftwareEngineering Conference and the ACM SIGSOFT Symposium on TheF oundations of Software Engineering , ser. ESEC/FSE ‚Äô09. New
York, NY , USA: ACM, 2009, pp. 383‚Äì392. [Online]. Available:http://doi.acm.org/10.1145/1595696.1595767
[33] T. Mikolov, M. KaraÔ¨Å ¬¥at, L. Burget, J. ÀáCernock `y, and S. Khudanpur,
‚ÄúRecurrent neural network based language model,‚Äù in Eleventh annual
conference of the international speech communication association , 2010.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. [34] M. White, C. Vendome, M. Linares-V ¬¥asquez, and D. Poshyvanyk,
‚ÄúToward deep learning software repositories,‚Äù in Proceedings of the
12th Working Conference on Mining Software Repositories , ser. MSR
‚Äô15. Piscataway, NJ, USA: IEEE Press, 2015, pp. 334‚Äì345. [Online].Available: http://dl.acm.org/citation.cfm?id=2820518.2820559
[35] J. Li, Y . Wang, M. R. Lyu, and I. King, ‚ÄúCode completion
with neural attention and pointer networks,‚Äù in Proceedings of the
27th International Joint Conference on ArtiÔ¨Åcial Intelligence , ser.
IJCAI‚Äô18. AAAI Press, 2018, pp. 4159‚Äì25. [Online]. Available:http://dl.acm.org/citation.cfm?id=3304222.3304348
[36] O. Vinyals, M. Fortunato, and N. Jaitly, ‚ÄúPointer networks,‚Äù
inProceedings of the 28th International Conference on Neural
Information Processing Systems - V olume 2 , ser. NIPS‚Äô15. Cambridge,
MA, USA: MIT Press, 2015, pp. 2692‚Äì2700. [Online]. Available:
http://dl
acm.gg363.site/citation.cfm?id=2969442.2969540
[37] ‚ÄúReplication package,‚Äù https://github.com/Method-Name-Recommend
ation/HeMa, 2019.
[38] ‚ÄúNew dataset,‚Äù https://s3.amazonaws.com/code2seq/datasets/java-med.t
ar.gz, accessed May 5th, 2019.
[39] S. L. Abebe, S. Haiduc, P. Tonella, and A. Marcus, ‚ÄúLexicon bad smells
in software,‚Äù in 2009 16th Working Conference on Reverse Engineering ,
Oct 2009, pp. 95‚Äì99.
[40] E. W. Host and B. M. Ostvold, ‚ÄúThe programmer‚Äôs lexicon, volume
i: The verbs,‚Äù in Seventh IEEE International Working Conference on
Source Code Analysis and Manipulation (SCAM 2007) , Sep. 2007, pp.
193‚Äì202.
[41] P. S. Kochhar, X. Xia, D. Lo, and S. Li, ‚ÄúPractitioners‚Äô expectations on
automated fault localization,‚Äù in Proceedings of the 25th International
Symposium on Software Testing and Analysis , ser. ISSTA 2016. New
York, NY , USA: ACM, 2016, pp. 165‚Äì176. [Online]. Available:http://doi.acm.org/10.1145/2931037.2931051
[42] J.-P. Kr ¬®amer, J. Brandt, and J. Borchers, ‚ÄúUsing runtime traces
to improve documentation and unit test authoring for dynamiclanguages,‚Äù in Proceedings of the 2016 CHI Conference on
Human Factors in Computing Systems , ser. CHI ‚Äô16. New
York, NY , USA: ACM, 2016, pp. 3232‚Äì3237. [Online]. Available:
http://doi.acm.org/10.1145/2858036.2858311
[43] E. Wong and and, ‚ÄúAutocomment: Mining question and answer sites for
automatic comment generation,‚Äù in 2013 28th IEEE/ACM International
Conference on Automated Software Engineering (ASE) , Nov 2013, pp.
562‚Äì567.
[44] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang,
‚ÄúNeural-machine-translation-based commit message generation: Howfar are we?‚Äù in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering , ser. ASE 2018. New
York, NY , USA: ACM, 2018, pp. 373‚Äì384. [Online]. Available:http://doi.acm.org/10.1145/3238147.3238190
[45] V . J. Hellendoorn, S. Proksch, H. C. Gall, and A. Bacchelli, ‚ÄúWhen
code completion fails: A case study on real-world completions,‚Äù in2019 IEEE/ACM 41st International Conference on Software Engineering(ICSE) , May 2019, pp. 960‚Äì970.
[46] W. Fu and T. Menzies, ‚ÄúEasy over hard: A case study on deep
learning,‚Äù in Proceedings of the 2017 11th Joint Meeting on
F oundations of Software Engineering , ser. ESEC/FSE 2017. New
York, NY , USA: ACM, 2017, pp. 49‚Äì60. [Online]. Available:http://doi.acm.org/10.1145/3106237.3106256

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. 