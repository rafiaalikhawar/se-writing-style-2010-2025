arXiv:1603.04584v1  [cs.SE]  15 Mar 2016Semi-Supervised VeriﬁedFeedback Generation
Shalini Kaleeswaran Anirudh Santhiar Aditya Kanade
Indian Institute of Science,Bangalore
{shalinik,anirudh_s,kanade}@csa.iisc.ernet.inSumit Gulwani
MicrosoftResearch,Redmond
sumitg@microsoft.com
ABSTRACT
Studentshaveenthusiasticallytakentoonlineprogrammin glessons
and contests. Unfortunately, they tend to struggle due to la ck of
personalized feedback when they make mistakes. The overwhe lm-
ing number of submissions precludes manual evaluation. The re is
an urgent need of program analysis and repair techniques cap able
of handling both the scale and variations in student submiss ions,
while ensuring qualityof feedback.
Towards this goal, we present a novel methodology called semi-
supervised veriﬁed feedback generation . We cluster submissions
by solution strategy and ask the instructor to identify or ad d a cor-
rect submission in each cluster. We then verify every submis sion
in a cluster against the instructor-validated submission i n the same
cluster. If faults are detected in the submission then feedb ack sug-
gestingﬁxestothemisgenerated. Clusteringreducesthebu rdenon
theinstructorandalsothevariationsthathavetobehandle dduring
feedbackgeneration. Theveriﬁedfeedbackgenerationensu resthat
only correct feedback is generated.
Wehaveappliedthismethodologytoiterativedynamicprogr am-
ming (DP) assignments. Our clustering technique uses featu res of
DP solutions. We have designed a novel counter-example guided
feedback generation algorithm capable of suggesting ﬁxes to all
faults in a submission. In an evaluation on 2226submissions to
4problems, we could generate veriﬁedfeedback for 1911(85%)
submissionsin 1.6seachonanaverage. Ourtechniquedoesagood
job of reducing the burden on the instructor. Only one submis sion
had tobe manually validatedor added for every 16submissions.
1. INTRODUCTION
Programming has become a much sought-after skill for superi or
employment intoday’stechnology-driven world[1]. Studen tshave
enthusiastically taken toonline programming lessons andc ontests,
in the hope of learning and improving programming skills. Un for-
tunately, theytendtostruggle due tolackofpersonalized f eedback
when they make mistakes. The overwhelming number of student
submissions precludes manual evaluation. There is an urgen t need
of automated program analysis and repair techniques capabl e of
handlingboththescaleandvariationsinstudentsubmissio ns,while
ensuring quality of feedback.
A promising direction is to cluster submissions, so that the in-
structor provides feedback for a representative from each c luster
which is then propagated automatically to other submission s in
the same cluster [20, 41]. This provides scalability while k eep-
ing the instructor efforts manageable. Many novel solution s have
been proposed in recent times to enable clustering of progra ms.
Theseincludesyntacticortest-basedsimilarity[15,44,2 0,13,12],
co-occurrence of code phrases [36] and vector representati ons ob-
tained by deep learning [40, 41, 33]. However, clustering ca n beStudent submissions
??
??
?
??
??Clustersof submissions
? ??? ?????
Clusterswith validatedsubmissions??? ?
??/check/check
/check
Submissions with feedback/check/check/check
/check/check /check✗
✗?Clustering by
solution strategy
Instructor-
validated
submissions
Veriﬁed feedback
generation
Figure 1: Semi-supervisedveriﬁed feedback generation: ✓is a
submission veriﬁed to be correct, ✗is a faulty submission for
which feedback is generated and ? is an unlabeled submission
for whichfeedbackis notgenerated.
correctonlyina probabilistic sense. Thus,thesetechniquescannot
guarantee that the feedback provided manually by the instru ctor,
by looking only at some submissions ina cluster, would indee d be
suitable to all the submissions in that cluster. As a result, some
submissions may receive incorrect feedback. Further, if su bmis-
sions that have similar mistakes end up in different cluster s, some
of them may not receive the suitable feedback. Instead of hel ping,
these drawbacks can cause confusion among the students.
Toovercome thesedrawbacks, wepropose anovelmethodology
in which clustering of submissions is followed by an automat ed
feedback generation phase grounded in formal veriﬁcation. Fig-
ure1showsourmethodology, called semi-supervisedveriﬁedfeed-
back generation . Given a set of unlabeled student submissions,
we ﬁrst cluster them by similarity of solution strategies an d ask
the instructor to identify1a correct submission in each cluster. If
none exists, the instructor adds a correct solution similar to the
submissions in the cluster; after which we do clustering aga in. In
the next phase, each submission in a cluster is veriﬁed against the
instructor-validated submission in the same cluster. If any faults
are detected in the submission then feedback suggesting ﬁxe s to
themisgenerated. Becauseprogramequivalencecheckingis anun-
decidable problem, it maynot be possible togenerate feedba ck for
every submission. We let the instructor evaluate such submi ssions
manually. This is better than propagating unveriﬁed or inco rrect
feedback indiscriminately.
Theproposed methodology has severaladvantages. First,it uses
unsupervised clusteringtoreduce theburdenonthe instruc tor. The
supervision from the instructor comes in the form of identif ying
a correct submission per cluster. Second, the veriﬁcation p hase
1To minimize instructor’s efforts further, we discuss strat egies to
suggest potentiallycorrect submissions tothe instructor .1void main() {
2 inti, j, n, max;
3scanf ("%d", &n); // Input
4 intm[n][n], dp[n][n]; // dp is the DP array
5 for(i = 0; i < n; i++)
6 for(j = 0; j <= i; j++)
7scanf ("%d", &m[i][j]); // Input
8 dp[0][0] = m[0][0]; // Initialization
9 for(i = 1; i < n; i++) {
10 for(j = 0; j <= i; j++) {
11 if(j == 0)
12 dp[i][j] = dp[i-1][j] + m[i][j]; // Update
13 else if (j == i)
14 dp[i][j] = dp[i-1][j-1] + m[i][j]; // Update
15 else if (dp[i-1][j] > dp[i-1][j-1])
16 dp[i][j] = dp[i-1][j] + m[i][j]; // Update
17 elsedp[i][j] = dp[i-1][j-1] + m[i][j]; // Update
18 }
19 }
20 max = dp[n-1][0];
21 for(i = 1; i < n; i++)
22 if(dp[n-1][i] > max) max = dp[n-1][i];
23printf ("%d", max); // Output
24}
Figure2: Acorrect submissionfor thematrix pathproblem.
complements clustering by providing certainty about correctness
of feedback. Clustering helps by reducing the variations re quired
to be handled during feedback generation. It would be difﬁcu lt to
check equivalence of very dissimilar submissions, e.g., an iterative
solution against a recursive solution. Since only submissi ons that
are similar ( i.e., belong to the same cluster) are compared, there is
a higher chance of making equivalence checking work in pract ice.
Third, feedback is generated for each submission separatel y so it
is personalized. This is superior topropagating a manually -written
feedback indiscriminatelytoallthe submissions ina clust er.
To demonstrate the effectiveness of this methodology, we ap ply
it to iterative dynamic programming assignments. Dynamic p ro-
gramming (DP) [8] is a standard technique taught in algorith ms
courses. Shortest-path and subset-sum are among the many pr ob-
lems that can be solved efﬁciently by DP. We design features t hat
characterizetheDPstrategyandextractthemfromstudents ubmis-
sions by static analysis and pattern matching. The features include
the types of the DP arrays used for memoizing solutions to sub -
problems, and how the sub-problems are solved and reused ite r-
atively. The set of features we need is small and all features are
discrete valued. Therefore, our clustering approach is ver y simple
and works directlyby checking equality of feature values.
We also propose a novel feedback generation algorithm calle d
counter-example guided feedback generation . The equivalence be-
tween two submissions is checked using syntactic simpliﬁca tions
and satisﬁability-modulo-theories (SMT) based constrain t solving.
Ifanequivalencecheckfails,ouralgorithmusesthecounte r-example
generated by the SMT solver to reﬁne the equivalence query. T his
process terminates when our algorithm proves the equivalen ce, or
is unable to reﬁne the query. A trace of reﬁnements leading to a
logically validequivalence queryconstitutes the feedbac k.
As an example, consider the matrix path problem2taken from
a popular programming contest site CodeChef. A lower triang ular
matrixof nrows is given. Startingata cell,apathcan be traversed
in the matrix by moving either directly below or diagonally b elow
totheright. Theobjectiveistoﬁndthemaximumweightamong the
pathsthatstartatthecellintheﬁrstrowandﬁrstcolumn,an dendin
any cell in the last row. The weight of a path is the sum of all ce lls
2http://www.codechef.com/problems/SUMTRIAN1intmax( inta,intb) {
2 return a > b ? a : b;
3}
4intmax_arr( intarr[]) {
5 inti, max;
6 max = arr[0];
7 for(i = 0; i < 100; i++)
8 if(arr[i] > max) max = arr[i];
9 return max;
10}
11intmain() {
12 intn, i, j, A[101][101], D[101][101]; // D is the DP array
13scanf ("%d", &n); // Input
14 for(i = 0; i < n; i++)
15 for(j = 0; j <= i; j++)
16 scanf ("%d", &A[i][j]); // Input
17 D[0][0] = A[0][0] ; // Initialization
18 for(i = 1; i < n; i++)
19 for(j = 0; j <= i; j++)
20 D[i][j] = A[i][j] + max(D[i-1][j], D[i-1][j-1]); // Update
21 intans = max_arr(D[n-1]);
22printf ("%d", ans); // Output
23 return 0;
24}
Figure 3: A faulty submission belonging to the same cluster a s
thecorrect submissioninFigure2.
Inthedeclaration: 1) Types of AandDshould be int[n][n]
Intheupdate:
2) Under guard j == 0,
compute D[i][j] = D[i-1][j] + A[i][j]
instead of D[i][j] = A[i][j] + (D[i-1][j]>D[i-1][j-1] ?
D[i-1][j] : D[i-1][j-1]) .
3) Under guard (j != 0 && j == i) ,
compute D[i][j] = D[i-1][j-1] + A[i][j]
instead of D[i][j] = A[i][j] + (D[i-1][j]>D[i-1][j-1] ?
D[i-1][j] : D[i-1][j-1]) .
Intheoutput: 4) Under guard true,compute maximum
overD[n-1][0],...,D[n-1][n-1] instead of D[n-1][0],...,D[n-1][99] .
Figure 4: The auto-generated feedback for the submission in
Figure3byverifyingitagainst the submissioninFigure2.
along that path. Figure 2 shows an example correct submissio n
to this problem and Figure 3 shows a faulty submission. The tw o
programs are syntactically and structurally quite differe nt but both
use 2D integer arrays for memoization and iterate over them i nthe
samemanner. Ourclusteringtechniquethereforeputsthemi ntothe
same cluster. This avoids unnecessarily creating many clus ters but
requires a more powerful feedback generation algorithm tha t can
handle stylisticvariations betweensubmissions, e.g.,Figure 3uses
multiple procedures whereas Figure 2uses only one.
Our algorithm automatically generates the feedback in Figu re 4
forthefaultysubmissionbyverifyingitagainstthecorrec tsubmis-
sion. The ﬁrst correction suggests that the submission shou ld use
array sizes as int[n][n]instead of hardcoded value of int[101][101] .
TheupdatetotheDParray Datline20inFigure3missessomecor-
ner cases for which our algorithm generates corrections #2 a nd #3
above. The computation of output at line 22 should use the cor rect
array bounds as indicated by correction #4. This is a compreh en-
sive listof changes tocorrect the faulty submission.
We have implemented our technique for C programs and evalu-
atediton 2226studentsubmissions to 4problems fromCodeChef.
On1911(85%)ofthem,wecouldgenerate feedbackbyeitherver-
ifying them to be correct, or identifying faults and ﬁxes for them.
In addition to faults in wrong answers, we also found faults i n
265submissions accepted by CodeChef as correct answers! Like
mostonlinecontestsites,CodeChefusestest-basedevalua tion. Ourstatic veriﬁcation technique has a qualitative advantage o ver the
test-based approach of online judges. The submissions come from
1860students from over 250different institutes and are therefore
representativeofdiversebackgroundsandcodingstyles. E venthen,
thenumber ofclustersrangedonlyfrom 2–80acrosstheproblems.
Our technique does a good job of reducing the burden on the in-
structor. On an average, using one manually validated or add ed
submission, we generated veriﬁed feedback on 16other submis-
sions. We had to add only 7correct solutions manually. While our
technique generated feedback automatically for 1911submissions,
the remaining 315(15%) submissions require manual evaluation.
Ourtechnique isfastandonanaverage,took 1.6stogeneratefeed-
back for eachsubmission.
Workonfeedback generation sofar has focused on introductory
programming assignments [46, 17, 47, 21]. In comparison, we ad-
dress the challenging class of algorithmic assignments, in particu-
lar, that of dynamic programming. The program repair approa ches
fordevelopers [27,37, 24,31,29]dealwithoneprogram atat ime.
Weworkwithallstudentsubmissionssimultaneously. Todos o,we
proposeamethodologyinspiredbybothmachinelearningand veri-
ﬁcation. Unlikethedevelopersetting,wehavetheluxuryof calling
upon the instructor to identify or add correct solutions. We exploit
this to give complete and correct feedback but then our techn ique
must solve the challenging (and in general, undecidable) pr oblem
of checking semantic equivalence of programs.
The salient contributions of this workare as follows:
•Wepresentanovelmethodologyofclusteringofsubmissions
followedbyprogramequivalence checkingwithineachclus-
ter. This methodology can pave way for practical feedback
generation tools capable of handling both the scale and vari -
ations in student submissions, while minimizing the instru c-
tor’s effortsand ensuring qualityof feedback.
•We demonstrate that this methodology is effective by apply-
ing it to the challenging class of iterative DP solutions. We
design a clustering technique and a counter-example guided
feedback generation algorithm for DPsolutions.
•We experimentally evaluate the technique on 2226submis-
sions to 4problems and generate veriﬁedfeedback for 85%
of them. We show that our technique does not require many
inputs from the instructor and runs efﬁciently.
2. DETAILED EXAMPLE
We now explain in details how our technique handles the moti-
vating example from the previous section.
2.1 Clustering Phase
The two submissions in Figure 2 and Figure 3 are syntacticall y
and structurally quite different. Our technique extracts f eatures of
the solution strategy in a submission. These features are mo re ab-
stract than low-level syntactic or structural features and put the su-
perﬁciallydissimilarsubmissions intothe same cluster.
The solution strategy of a DP program is characterized by the
DP recurrence being solved [8]. The DP recurrence for the cor rect
submission inFigure 2isas follows:
dp[i][j]=

m[0][0] ifi=0,j=0
dp[i-1][j]+m[i][j] ifi/ne}ationslash=0,j=0
dp[i-1][j-1]+m[i][j] ifj=i/ne}ationslash=0
max(dp[i-1][j],dp[i-1][j-1])+m[i][j] otherwise
where dpistheDParray, mistheinputmatrixof nrows,and igoes
from 0ton-1,jgoes from 0toiandmaxreturns the maximum ofthe twonumbers. The DPrecurrence of the submissioninFigur e3
is similarbut missesthe second andthirdcases above.
Comparingtherecurrencesdirectlywouldbeidealbutextra cting
them is not easy. Students can implement a recurrence formul a in
different imperative styles. They may use multiple procedu res (as
inFigure 3) and arbitrary temporary variables tohold inter mediate
results. Rather than attempting to extract the precise recu rrence,
we extract some features of the solution to ﬁnd submissions t hat
use similar solution strategies. For this, our analysis ide ntiﬁes and
labels the DParrays used ineach submission. Italso identiﬁ es and
labels key statements that 1) read inputs, 2) initialize the DP array,
3) update the DP array elements using previously computed ar ray
elements, and 4) generate the output. The comments in Figure 2
and Figure3identify the DParrays and keystatements.
Wecallaloopwhichisnotcontainedwithinanyotherstateme nt
as atop-level loop . Forexample, the loopat lines 9–19 inFigure 2
is a top-level loop but the loop at lines 10–18 is not. More gen er-
ally, a statement which is not contained withinany other sta tement
is atop-level statement . Thefeatures extracted by our technique
and theirvalues for the submission inFigure2are as follows :
1. Type andthe number ofdimensions of the DParray: /an}bracketle{tint, 2/an}bracketri}ht
2. Whether the input arrayis reusedas the DP array: No
3. The number of top-level loops which contain update state-
ments for the DParray: 1
4. Foreach top-level loop containing updates tothe DParray ,
(a) The loop nestingdepth: 2
(b) The direction of loop indices: /an}bracketle{t+,+/an}bracketri}ht(indicating that
the respective indices are incremented by one in each
iterationof the corresponding loops)
(c) The DParrayelement updated inside the loop: dp[i][j]
Extracting these features requires static analysis and syn tactic
pattern matching. The most challenging part is identifying which
array serves as a DP array. An array which is deﬁned in terms of
itself is identiﬁed as a DP array since in the DP recurrence, t he
DP array appears on both sides. However, a student may use som e
temporaryvariablestostoreintermediateresultsofDPcom putation
andpass valuesacrossprocedures. AsexplainedinSection3 .1,we
track data dependences inter-procedurally. In Figure 3, th e array
elements D[i-1][j]andD[i-1][j-1] are passed to the procedure max.
Through inter-procedural analysis, our technique infers t hat the re-
turn value of maxis indeed deﬁned in terms of these arguments
and hence, Dis deﬁned in terms of itself at line 20. Thereby, it
discovers that Dis a DParray.
The submission in Figure 3 yields similar feature values and is
clustered along with the submission in Figure 2. Note that ou r ob-
jective is not to use clustering to distinguish between corr ect and
incorrect submissions. We therefore do not encode the exact na-
ture of the initialization or update of the DP array in the fea tures.
Analyzing these is lefttothe veriﬁcationandfeedback phas e.
2.2 Veriﬁcation andFeedback Phase
Afterclustering,suppose theinstructor identiﬁesthesub mission
in Figure 2 as correct. Our objective is to verify the submiss ion in
Figure 3 against it and suggest ﬁxes if any faults are found. F or
brevity, wewillrefertothe submissioninFigure 2as the reference
and the submission inFigure 3as the candidate .
By analyzing the sequence in which inputs are read, our tech-
nique infers that the candidate uses two input variables: an integer
variable nand a 2D integer array A, where nis read ﬁrst (line 13)
andAsecond (line 16). Their types respectively match the types
of the input variables nandmof the reference except that the can-
didate uses a hardcoded arraysize for A. Bothsubmissions use 2DIsψ1
valid?
Isψ2
valid?
Isψ3
valid?
a) Successive equivalence queries and resultsCounter-example E1
Counter-example E2
Counter-example YesYes
YesUsing the counter-example E1, the algorithm dis-
covers that when j=0,ϕ2computes statement
s1(line 20 in Figure 3) but according to ϕ1, it
should compute s2:D[i][j] = D[i-1][j]+A[i][j] .
Letϕ′
2≡(j/ne}ationslash=0=⇒s1)∧(j=0=⇒s2).
Using the counter-example E2, the algorithm dis-
covers that when j=i∧j/ne}ationslash=0,ϕ′
2computes
s1but according to ϕ1, it should compute s3:
D[i][j] = D[i-1][j-1]+A[i][j] .
Letϕ′′
2≡(j/ne}ationslash=0∧j/ne}ationslash=i=⇒s1)∧(j/ne}ationslash=0∧j=
i=⇒s3)∧(j=0=⇒s2).Correction#2
Correction#3ψ1≡pre∧ϕ1∧ϕ2=⇒post
ψ2≡pre∧ϕ1∧ϕ′
2=⇒post
ψ3≡pre∧ϕ1∧ϕ′′
2=⇒post
b)Reﬁnementstepsandcorrections suggested
Figure5: Stepsof theveriﬁed feedbackgeneration algorith m for the DPupdateinthefaultysubmissioninFigure3.
integer DP arrays but the candidate hardcodes array size of t he DP
array D also. While the reference can handle arbitrarily lar ge in-
put matrices, the candidate can handle input matrices only u p to
size101×101. For smaller input matrices, the reference is more
space efﬁcient than the candidate. Our technique therefore emits
correction #1inFigure 4suggesting size declarations for AandD.
We check equivalence of matching code fragments of the two
submissions one-by-one. The matchingcode fragments are ea syto
identifygiventhestatementlabelscomputed duringfeatur e extrac-
tion. Forourexample,line20ofthecandidateisan“update” state-
ment and lines12, 14, 16 and17of the reference arealso“upda te”
statements. Therefore, thetop-level loop(say L2)atlines 18–20 of
the candidate matches the top-level loop (say L1) at lines 9–19 of
the reference. The question iswhether theyare equivalent.
We check equivalence of the loop headers ﬁrst. The input vari -
ables nin both submissions correspond to each other and are not
re-assigned before they are used in the respective loop head ers.
Therefore, the loop headers of L1andL2are equivalent. Thus,
the corresponding loop indices are equal ineachiteration.
To check equivalence of loop bodies, our algorithm formulat es
anequivalence query ψ1which asserts that inan ( i,j)thiteration, if
the two DP arrays are equal at the beginning then they are equa l at
the end of the iteration. The equivalence query is of the form :
ψ1≡pre∧ϕ1∧ϕ2=⇒post
where 1) preencodes the equality of DP arrays, loop indices and
input variables at the beginning of the iteration, and the lo wer and
upper bounds on the loop index variables in the reference, 2) post
encodes the equality of DP arrays at the end of the iteration ( we
syntacticallycheck thatinput variables arenot changed), 3)ϕ1isa
formula encoding the statements inthe loop body of the refer ence,
and 4) ϕ2is a formula encoding the statements inthe loopbody of
the candidate. Converting a loop-free sequence of statemen ts into
a formula is straightforward. For example, an if-statement such
asif(p) x = e is converted to a guarded equality constraint p:
x′=ewhere x′is a fresh variable. The predicates in an if-else
statementarepropagatedsoastomaketheguardsmutuallydi sjoint
and ﬁnally, the conjunction of all guarded equality constra ints is
taken. We defer other technical details toSection3.2.
As shown in Figure 5.a, the algorithm checks whether ψ1is
a logically valid formula. The SMT solver ﬁnds the followingcounter-example E1whichshows that the formula isnot valid:
j=0,dp[i][j]=1,D[i][j]=2,m[i][j]=A[i][j]=1,
dp[i-1][j] =D[i-1][j]=0,dp[i-1][j-1] =D[i-1][j-1] =1
Following the usual convention, we use =as equality in formulae
and use == as equality in code. Similarly, /ne}ationslash=and != denote dise-
qualitysymbols informulae versus code.
Our algorithm, called counter-example guided feedback genera-
tionalgorithm,uses E1tolocalizethe faultinthecandidate. Itﬁrst
identiﬁes which guards are satisﬁed by the counter-example in the
candidate and the reference, and whether they are equivalen t. The
guard j=0issatisﬁedin ϕ1andtheimplicitguard trueforline20
issatisﬁedin ϕ2. Sincetheyarenotequivalent,thealgorithminfers
thatthefaultysubmissionismissingacondition. Onthecon trary,if
the guards turnout tobe equivalent, the fault is localized t othe as-
signmentstatement. Itthenderivesaformula ϕ′
2giveninFigure5.b
whichletsthecandidate compute line20under theguard j!=0and
makes it compute D[i][j] = D[i-1][j]+A[i][j] under j==0. This
assignment statement is obtained from the assignment at lin e 12
under the guard j==0of the reference in Figure 2 by substituting
thevariablesfromthecandidate. Thealgorithmrecordsthi sreﬁne-
ment inthe form of correction#2 of Figure 4.
As shown in Figure 5.a, it checks validity of ψ2≡pre∧ϕ1∧
ϕ′
2=⇒ postobtained by replacing ϕ2(the encoding of candi-
date’s loop body) by ϕ′
2(deﬁned in Figure 5.b). This results in a
counter-example E2using which the algorithm discovers the miss-
ing case of j==iand generates correction #3 of Figure 4. For
brevity, we do not show the counter-example E2. A reﬁned equiv-
alence query ψ3shown in Figure 5.b is computed. As shown in
Figure5.a,thisformulaisvalidandestablishesthatthefa ultsinthe
candidate canbe ﬁxedusingthe synthesized feedback.
Theinputandinitializationpartsofthetwosubmissionsar efound
to be equivalent. In our experiments, we observed certain re peat-
ing iterative patterns. The computation of a maximum over an
array in lines 6–8 in Figure 3 is one such example. We encode
syntactic patterns to lift these to certain predeﬁned funct ions. We
deﬁne _maxwhich takes the ﬁrst and the last elements of a con-
tiguous arraysegmentasarguments andreturnsthemaximum o ver
the array segment. In Figure 3, the output expression in term s of
_maxis_max(D[n-1][0], D[n-1][99]) and in Figure 2, the output
is_max(dp[n-1][0], dp[n-1][n-1]) . Asyntacticcomparison between
the twoleads tocorrection # 4inFigure 4.3. TECHNICALDETAILS
Wenowexplainourapproachforclusteringsubmissions,and the
algorithm forveriﬁedfeedback generation.
3.1 Clustering by SolutionStrategy
The ﬁrst phase of our technique is to cluster submissions by t he
solution strategysothat eachcluster can be analyzed separ ately.
3.1.1 FeatureDesign
Section 2.1 has already introduced the features of a submiss ion
that we use. Typically, inmachine learning, a large number o f fea-
tures are obtained and then the learning algorithm ﬁnds the i mpor-
tant ones (called feature selection). In our case, since the domain
is well-understood, we design a small number of suitable fea tures
that provide enough informationabout the solutionstrateg y.
In particular, we cluster two submissions together if 1) the y use
the same type and dimensions for the DP arrays, 2) either both use
DP arrays distinct from the input arrays or not, and 3) there i s a
one-to-one correspondence between top-level loops which c ontain
DPupdate statements —the loops should have thesame depth, d i-
rection and the DP array element being updated. Twosubmissi ons
inthe same cluster can differinall other aspects.
The rationale behind these features is simple: Checking equ iv-
alence of two submissions which use the same types of DP array s
andsimilarDPupdateloopsiseasierthaniftheydonotshare these
properties. For example, the subset-sum problem can be solv ed by
using either a boolean DParrayor an integer DParray, but the two
implementations are hard to compare algorithmically. Reca ll the
matrix path problem stated in the Introduction. Consider a s ub-
missionwhichtraversesthematrixfromtop-to-bottomanda nother
which traverses it from bottom-to-top. Using one to validat e the
other isdifﬁcultandperhaps, evenundesirable. Thefeatur esofDP
update loops will prevent these submissions from being part of the
same cluster. Imposing further restrictions (by adding mor e fea-
tures)canmakeveriﬁcationsimplerbutwillincreasethebu rdenon
the instructor by creatingadditional clusters.
The feature 4.c described in Section 2.1 requires a bit more e x-
planation. We want to get the DP array element being updated
inside each loop containing a DP update statement. If two sub -
missions use different names for DP arrays and loop indices, we
cannot compare them. To compare them across submissions, ou r
technique uses canonical names for them: dpfor the DP array and
loop indices i,j,k, etc. from the outer to inner loops. If a submis-
sion uses multiple DParrays thenwe assign subscripts to dp.
3.1.2 FeatureExtraction
Identifying input statements and variables is simple. We lo ok
for the common C library functions like scanf. The case of out-
put statements is similar. A variable xis identiﬁed as a loop index
variable if 1) xis a scalar variable, 2) xis initialized before the
loop is entered, 3) xupdated inside the loop and 4) xis used in
the loop guard. Identifying DP arrays requires more subtle a naly-
sis discussed below. We call DP arrays, input variables and l oop
indices in a submission as DP variables . All other variables are
calledtemporary variables.
To eliminate the use of a temporary variable xat a control loca-
tionl,wecompute a set of guarded expressions
{g1:e1,. . .,gn:en}
where the guards and expressions are deﬁned only over DP vari -
ables, and the guards are mutually disjoint. We denote this s et by
Σ(l,x)and call Σthesubstitution store . Semantically, if gk:ek∈
Σ(l,x)then xandekevaluate to the same value at lwhenever gkevaluates to true at l. The substitution store Σis lifted in a nat-
ural manner to expressions and statements. For instance, fo r an
assignment statement s≡x = e,Σ(l,s) ={g1:x = e 1,. . .,gn:
x = e n}where{g1:e1,. . .,gn:en}=Σ(l,e).
Gulwani and Juvekar [16] developed an inter-procedural back-
ward symbolic execution algorithm to compute symbolic boun ds
onvaluesofexpressions. Whilewearenotinterestedintheb ounds,
the equality mode of their algorithm sufﬁces to compute subs titu-
tionstores. We referthe reader to[16] forthe details.
To determine whether a statement sat location lis an initial-
ization or an update statement, we perform pattern matching over
Σ(l,s). If the same array appears on both sides of an assignment
statementthenthearrayisidentiﬁedasaDParrayandthesta tement
is labeled as an update statement. A statement where the LHS i s a
DP array and RHS is an input variable or a constant is labeled a s
an initialization statement. In Σ(l,s), the temporary variables in s
arereplacedbytheguardedexpressionsfromthesubstituti onstore.
This makes the labeling part of our tool robust even in presen ce of
temporaries and procedure calls. For example, suppose we ha vet
= x[i-1]; x[i] = t; . Thesecondstatementcanbeidentiﬁedasanup-
datestatementthroughpatternmatchingonlyifwesubstitu tex[i-1]
in place of ton the RHS of the statement. In general, Σ(l,s)may
containmultipleguardedstatements. If Σ(l,s) ={s1, . . . , sn},we
require that all of s1, . . . , snsatisfy the same pattern and get the
same label. Extractingthe feature values is now straightfo rward.
3.1.3 ClusteringandIdentifyingCorrectSubmissions
Allourfeaturesarediscretevalued. Therefore,ourcluste ringal-
gorithm is very simple and works directly by checking equali ty of
featurevalues. Once theclusteringisdone,weasktheinstr uctorto
identify a correct submission from each cluster. To reduce i nstruc-
tor’sefforts,wecanemploysome heuristicstorankcandida tes ina
clusterandpresentthemone-by-one totheinstructor. Fore xample,
we can use a small set of tests or majority voting on some other
features of submissions like the loop bounds of update loops .
The instructor can accept a submission as correct or add a mod -
iﬁed version of an existing submission. If none of this is pos sible,
the instructor can write a correct solution similar to the so lutions
in the cluster. If a new submission is added, we perform clust ering
again. The instructor may have correct solutions from a prev ious
offeringofthecourseiftheassignment isrepeatedfromapr evious
offering. Theinstructorcanaddthemtothedatasetevenbef ore we
apply clusteringtothe submissions.
3.2 Veriﬁed Feedback Generation
Once thesubmissions areclusteredandtheinstructor has id enti-
ﬁed a valid submission for each cluster, we proceed to the ver iﬁed
feedback generation phase. We check semantic equivalence o f a
submissionfromacluster(calledthe candidate )withtheinstructor-
validated submission from the same cluster (calledthe reference).
3.2.1 VariableandControlCorrespondence
Program equivalence checking is an undecidable problem. In
practice, a major difﬁcultyis establishing correspondenc e between
variables and control locations of the two programs [34]. We ex-
ploit the analysis information computed during feature ext raction
tosolve this problem efﬁciently.
Letσbe a one-to-one function, called a variable map .σmaps
the input variables and DP arrays of the reference to the corr e-
sponding ones of the candidate. To obtain a variable map, the in-
put variables of the two submissions are matched by consider ing
the order in which they are read and their types. The DP arrays
are matched based on their types. If there are multiple DP arr ayswith the same type in both submissions then all type-compati ble
pairsareconsidered. Thisgeneratesasetofpotentialvari ablemaps
andequivalence checkingisperformedforeachvariablemap sepa-
rately. Theonewhichsucceedsandproducestheminimumnumb er
of corrections is used for communicating feedback to the stu dent.
In equivalence checking, we eliminate the occurrences of te mpo-
rary variables using the substitution store computed durin g feature
extraction. We therefore do not need to derive corresponden ce be-
tweentemporary variables –whichsimpliﬁesthe problem gre atly.
The feature extraction algorithm labels the input, initial ization,
update and output statements of a submission. We refer to the se
statements as labeled statements . The labeled statements give an
easy way to establish control correspondence between the su bmis-
sions. We now use the notion of top-level statements deﬁned i n
Section 2.1. Let ˆR= [s1
1, . . . , sk
1]be the list of all top-level state-
ments of the reference such that 1) each statement in ˆRcontains at
least one labeled statement and 2) the order of statements in ˆRis
consistent withtheirorder inthereferencesubmission. It iseasyto
seethatthetop-levelstatementsinasubmissionaretotall yordered.
Let ˆC= [s1
2, . . . , sn
2]be the similar list for the candidate submis-
sion. Withoutlossofgenerality,fromnowon,weassumethat there
is only one DP array in a submission and the top-level stateme nts
are (possibly nested) loops.
A (top-level) loop in ˆRorˆCmay contain multiple statements
which have different labels. For example, a loop may read the in-
put and also update the DP array. We call it a heterogeneous loop.
If a loop reads two different input variables then also we cal l it
a heterogeneous loop. Heterogeneous loops make it difﬁcult to es-
tablishcontrolcorrespondencebetweenthestatementlist sˆRand ˆC.
Fortunately, it is not difﬁcult tocanonicalize the stateme nt lists us-
ingsemantics-preserving loop transformations, well-known in the
compilers literature [3]. Our algorithm ﬁrst does loop spli tting to
split a heterogeneous loop into different homogeneous loop s. It
thendoesloopmergingtocoalescedifferentloopsoperatin gonthe
same variable. Speciﬁcally, it merges two loops reading the same
input array. It also merges loops performing initializatio n to the
same DP array. During merging, we ensure that there is no loop
in between the merged loops such that it reads from or writes t o
the same variable or array as the merged loops. In our experie nce,
in most cases, these transformations work because loops rea ding
inputs or performing initialization of DP arrays do nothave loop-
carrieddependences or ad-hoc dependences between loops.
In contrast, by deﬁnition, loops performing DP updates do ha ve
loop-carried dependences. We therefore do not attempt loop merg-
ing for such loops. The feature 3in Section 2.1 tracks the number
of loops containing DPupdates. Therefore, twosubmissions inthe
sameclusteralreadyhavethesamenumber ofloops containin gDP
updates. Thus, clustering helps in reducing the variants th at need
tobe considered duringfeedback generation.
LetRandCbetheresultingstatementlistsforthereferenceand
candidate submissions respectively. If they have the same l ength
and at each index i, the ith loops in the two lists 1) operate on the
variables related by a variable map σ, 2) the statements operating
on the variables carry the same labels and 3) the loops have th e
same nesting depth and directions then we get the control corre-
spondence π:R→C. If our algorithm fails to compute variable
or control correspondence for the candidate then it exits wi thout
generating feedback, implicitlydelegatingittothe instr uctor.
3.2.2 EquivalenceQueries
Lets′
1ands′
2be the top-level loops from the reference and the
candidate such that π(s′
1) = s′
2. We ﬁrst use the substitution map
computed during feature extraction to eliminate temporary vari-ables and procedure calls in s′
1ands′
2by equivalent guarded ex-
pressions over only DP arrays, loop indices and input variab les.
Lets1=Σ(l1,s′
1)ands2=Σ(l2,s′
2)where l1andl2are control
locations of s′
1ands′
2.
We formulate an equivalence query Φfor the iteration spaces
ofs1ands2. Let corrbe the correspondence between the input
variables, DP arrays, and loop indices of s1ands2at the matching
nesting depths. We deﬁne iter 1to be the range of the loop indices
ins1andguards 1to be the disjunction of all guards present inthe
loop body of s1. Similarly, we have iter 2andguards 2fors2. The
equivalence query Φis deﬁnedas follows:
Φ≡corr=⇒(iter 1∧guards 1⇐⇒ iter 2∧guards 2)
This query provides more ﬂexibility than using direct synta ctic
checking between the loop headers. For example, suppose s1is
for(i=1, i<=n, i++){ true: s}ands2isfor(i′=0, i′<=n, i′++){i′
>0: s′}.s1executes sfor1≤i≤nands2alsoexecutes s′for1≤
i′≤n. A syntactic check will end up concluding that s2executes
one additional iteration when i′is0. But our equivalence query
establishes equivalence between the iterationspaces as de sired.
Theformulationofthequery Ψtoestablishequivalence between
loopbodiesof s1ands2isasdiscussedinSection2.2. Eventhough
the submissions use arrays, we eliminate them from the queri es. A
loop body makes use of onlya ﬁnite number of symbolic arrayex -
pressions. Wesubstituteeachuniquearrayexpressioninaq ueryby
ascalarvariablewhileencodingcorrespondencebetweenth escalar
variables in accordance with the variable map σ. We overcome
some stylistic variations when the order of operands of a com mu-
tativeoperationdiffersbetweenthetwosubmissions. Fore xample,
says1uses x[i+j]ands2uses y[b+a]such that σ(x) =y,σ(i) =a
andσ(j) =b. Theexpressions i+jandb+aare notidenticalunder
renaming but are equivalent due to commutativity. To take ca re of
this, we force a ﬁxed ordering among variables in the two subm is-
sions for commutative operators. Sometimes, the instructo r may
include some constraints over input variables as part of the prob-
lem statement. In the equivalence queries, our algorithm ta kes in-
putconstraintsintoaccountandalsoaddsarrayboundschec ks. We
omit these details due tospace limit.
3.2.3 Counter-ExampleGuidedFeedbackGeneration
Algorithm 1is our counter-example guided feedback generat ion
algorithm. Its input is a list Qof equivalence queries where each
query(Φi,Ψi)correspondstothe ithstatementsinthetwosubmis-
sions. Φiencodes theequivalence ofiterationspaces and Ψiofthe
loop bodies. If the ith statements are not loops, ΦiistrueandΨi
just checks equivalence of the loop-free statements. The ou tput of
the algorithm is a listof corrections tothe candidate submi ssion.
Algorithm 1 iterates over the query list (line 1). For a query
(Φi,Ψi), it ﬁrst checks whether Φiis (logically) valid or not. If it
isnotthenthealgorithmsuggests acorrectiontomaketheit eration
spaces of the ith statements (loops) of the two submissions equal
(lines 2-4). Itthen enters a reﬁnement loop for Ψiat lines7-23.
During each iteration of the reﬁnement loop, it checks wheth er
Ψiis valid. If yes, it exits the loop (line 9). Otherwise, it get s
a counter-example αfrom the SMT solver and ﬁnds the guarded
statements that are satisﬁed by α. Let g1:s1∈ϕ1and g2:
s2∈ϕ2be those statements (line 11). The formulae ϕ1andϕ2
correspondtotheencodings oftheloopbodiesofthereferen ceand
the candidate respectively. Note that the conversion of sta tements
toguardedequalityconstraints(Section2.2)ensuresthat theguards
within ϕ1and within ϕ2are pairwise disjoint.
Letˆσbe the variable map which is same as the variable corre-
spondence σbutaugmentedwiththecorrespondence betweenloopAlgorithm1: Algorithm G ENFEEDBACK
Input: Alist Q= [(Φ1,Ψ1), . . . ,(Φk,Ψk)]of equivalence queries
Output: Alist of corrections to the candidate submission
1foreach(Φi,Ψi)∈Qdo
2if∃α/ne}ationslash|=Φithen
3Suggest corrections to make theiteration spaces of the ith
statements of the two submissions equal
4end
5LetΨi≡pre∧ϕ1∧ϕ2=⇒post
6 k←0
7repeat
8 k←k+1
9if|=Ψithen break else
10 Letα/ne}ationslash|=Ψibeacounter-example
11 Letg1:s1∈ϕ1andg2:s2∈ϕ2s.t.α|=g1andα|=g2
12 if|=pre=⇒(g1⇐⇒ g2)then
13 ϕ′
2←ϕ2[g2:s2/g2:ˆσ(s1)]
14 Ψi←Ψi[ϕ2/ϕ′
2]
15 Suggest computation of ˆσ(s1)instead of s2under g2
16 else
17 h2←g2∧ˆσ(g1);h′
2←g2∧ˆσ(¬g1)
18 ϕ′
2←ϕ2[g2:s2/h2:ˆσ(s1)∧h′
2:s2]
19 Ψi←Ψi[ϕ2/ϕ′
2]
20 Suggest computation of ˆσ(s1)instead of s2under h2
21 end
22end
23until k<δ
24ifk=δthenSuggest acorrection to replace ϕ2byˆσ(ϕ1)
25end
indices atthe samenestingdepths for the ithstatements. Thefunc-
tion ˆσis lifted in a straightforward manner to expressions and as-
signments. Thealgorithmchecks whethertheguards g1andg2are
equivalent (line 12). If they are then the fault must be in the as-
signmentstatement s2. Itthereforedeﬁnes ϕ′
2bysubstituting s2by
ˆσ(s1)inϕ2(line13)andreﬁnes Ψibyreplacing ϕ2byϕ′
2(line14).
It suggests an appropriate correction for the candidate sub mission
(line 15). The other case when the guards are not equivalent l eads
to the other branch (lines 16-21). The algorithm now splits t he
guarded assignment g2:s2to make it conform to the reference
under h2≡g2∧ˆσ(g1), whereas, for h′
2≡g2∧ˆσ(¬g1), the can-
didate can continue to perform s2(line 17). It computes ϕ′
2by
replacing g2:s2byh2:ˆσ(s1)andh′
2:s2(line 18). It then reﬁnes
Ψibyreplacing ϕ2byϕ′
2(line19)andsuggestsanappropriatecor-
rectionforthecandidatesubmission(line20). Thereﬁneme ntloop
terminates when no more counter-examples can be found (line 9)
and thus, progressively ﬁnds allsemantic differences between ith
statements of the twosubmissions.
Each iteration of the reﬁnement loop eliminates a semantic d if-
ference betweenapairofstatementsfromthetwosubmission s and
the loop terminates after a ﬁnite number of iterations. In pr actice,
givingalonglistofcorrectionsmightnotbeusefultothest udentif
there are toomanymistakes inthe submission. A betteralter native
mightbetostopgeneratingcorrectionsafterathresholdis reached.
We use a constant δto control how many reﬁnements should be
attempted (line 23). If this threshold is reached then the al gorithm
suggests a total substitution of ˆσ(ϕ1)in place of ϕ2(line 24). In
our experiments, we used δ=10.
Due to the explicit veriﬁcation of equivalence queries, our algo-
rithm only generates correct feedback. The feedback for the dec-
larations of the candidate are obtained by checking dimensi ons of
the corresponding variables according to σ.
4. IMPLEMENTATIONTable 1: Summaryof submissionsandclusteringresults.
Problem Total Clusters with Clusters with
subs. correct sub. manuallyadded
correct sub.
SUMTRIAN 1983 78 2
MGCRNK 144 23 3
MARCHA1 58 4 2
PPTEST 41 2 0
Total 2226 107 7
We consider C programs for experimental evaluation. We have
implementedthe source code analysis usingthe Clangfront- endof
the LLVM framework [26] and use Z3 [10] for SMT solving. We
presently donot support pointer arithmetic.
Inthepre-processingstep,ourtoolperformssomesyntacti ctrans-
formations. It rewrites compound assignments into regular assign-
ments. For example, x += yis rewritten to x = x + y . A code
snippet of the form: scanf("%d", &a[0]); for (i = 1; i < n; i++)
scanf("%d", &a[i]); , where the input array is read inmultiple state-
ments is transformed to use a single read statement. The abov e
snippet will be rewritten to for (i = 0; i < n; i++) scanf("%d",
&a[i]);. Sometimes, students read a scalar variable and then assign
ittoanarrayelement. Ourtooleliminatestheuseofthescal arvari-
able and rewrites the submission so that the input is read dir ectly
into the array element. Another common pattern is to read a se -
quence of input values into a scalar one-by-one and then use i t in
theDPcomputation. Forexample,consider thecode snippet: for (i
= 0; i < n; i++) for (j = 0; j < n; j++) { scanf("%d", &x); dp[i][j]
= dp[i-1][j] + x; } . It does not use an array to store the sequence
of input values. We declare an array and rewrite the snippet t o use
it. When feedback is generated for the submission, an explan atory
note about the input array is added. In each of the syntactic t rans-
formations, we ensure thatthe program semantics isnot alte red.
Many students, especially beginners, write programs with c on-
voluted conditional control ﬂow, and unnecessarily comple x ex-
pressions. Inaddition, thereﬁnementsteps ofourcounter- example
guidedfeedbackgenerationalgorithmmaygeneratecomplex guards.
Topresentclearandconcisefeedbackeveninthefaceofthes epos-
sibilities, in the post-processing step, our tool simpliﬁe s guards in
the feedback using the SMT solver. We use Z3’s tactics to remo ve
redundant clauses, evaluate sub-expressions to Boolean co nstants
and simplifysystems of inequalities.
5. EXPERIMENTAL EVALUATION
To assess the effectiveness of our technique, we collected s ub-
missions tothe following 4DPproblems3onCodeChef:
1.SUMTRIAN –Described inthe Introduction section.
2.MGCRNK – Find a path from (1,1) to (N,N) in an N ×N
matrix,sothattheaverageofallintegersincellsonthepat h,
excluding the end-points, is maximized. From each cell, the
pathcan extendtocells tothe rightor below.
3.MARCHA1 –The subset sum problem.
4.PPTEST –Theknapsack problem.
Weselected submissions tothese problems that implemented an
iterative DP strategy in the C language. A user can submit sol u-
tions any number of times. We picked the latest submissions f rom
individual users. These represent their best efforts and ca n beneﬁt
3http://www.codechef.com/problems/<problem-name>Table2: Results of feedbackgeneration.
Problem Veriﬁed as Corrections Average Unlabeled
correct (/check) suggested ( ✗) corrections ( ?)
SUMTRIAN 1049 659 3.3 275
MGCRNK 61 66 6.8 17
MARCHA1 9 35 10.3 14
PPTEST 3 29 12.7 9
Total 1122 789 4.3 315
from feedback. We do not consider submissions that either do not
compile or crash on CodeChef’s tests. To enable automated te st-
ing onCodeChef, the submissions had an outermost loop toite rate
over testcases –weidentiﬁedandremovedthisloopautomati cally
before further analysis.
Table 1 shows the number of submissions for each problem.
SUMTRIAN had the maximum number of submissions ( 1983) and
PPTEST had the minimum ( 41). There were a total of 2226sub-
missions from 1860students representing over 250institutions.
Thesesubmissionsemployawiderangeofcodingidiomsandma ny
possible solution approaches, both correct and incorrect. This is a
fairlylarge,diverse and challenging set ofsubmissions.
5.1 Effectiveness ofClustering
Our features were quite effective in clustering submission s by
their solution strategies. Since we do not include features repre-
sentinglow-levelsyntacticorstructuralaspectsofsubmi ssions, the
clusteringresultedinonlyafewclustersforeachproblem, without
compromising our ability to generate veriﬁed feedback. Tab le 1
gives the number of clusters. The number of clusters increas ed
gracefully from the smallest problem (by the number of submi s-
sions) to the largest one. The smallest problem PPTEST yielded
only 2clusters for 41submissions, whereas, the largest problem
SUMTRIAN yielded 80clustersfor 1983submissions. Ourmanual
evaluation revealed that ineachcluster, the solutions wer e actually
following the same DPstrategy.
The small number of clusters reduces the burden on the instru c-
torsigniﬁcantly. Insteadofevaluating 2226submissionsseparately,
the instructor is required to look at representatives from o nly114
clusters. CodeChef uses test suites to classify problems in to cor-
rect and incorrect. As a simple heuristic, we randomly picke d one
of the submissions marked as correct by CodeChef in each clus ter
and manually validated it. As shown in Table 1, this gave us co r-
rect representatives for 107/114 clusters across the problems. The
remaining 7clusters seemed tofollow some esoteric strategies and
we manually added a correct solutiontoeach of them.
Clustering also helps the instructor get a bird’s eye view of the
multitude of solution strategies. For example, it can be use d to
ﬁndthemost orleastpopular strategyusedinstudent submis sions.
InSUMTRIAN , the most popular strategy (with 677submissions)
was the one that traverses the matrixrows bottom up, travers es the
columns left toright and updates the element (i,j).
5.2 Effectiveness ofFeedback Generation
Our tool veriﬁes a submission from a cluster against the manu -
ally validated or added correct submission from the same clu ster.
Table 2 shows the number of 1) submissions veriﬁed as correct
(/check), 2) submissions for which faults were identiﬁed and correc -
tions suggested ( ✗) and 3) submissions which our algorithm could
not handle (?). Across the problems, 1122submissions amounting
to50% were veriﬁed to be correct, with the maximum at 53% for
SUMTRIAN and the minimum at 7% for PPTEST. For a total of
789submissions amounting to 35%, some corrections were sug-I only0.7%O only
2.9%Correct
7.9%I&U
7.9%I&U&O
7.9%U only
16.4%
U&O26.4%
Others30%
Figure 6: Distribution of submissions in a cluster of SUM-
TRIANbythetype of feedback.
gested by our tool. The maximum percentage of submissions wi th
corrections were for PPTEST at71% and the minimum was 33%
forSUMTRIAN . Many submissions had multiple faults. Table 2
shows the average number of corrections over faulty submiss ions
foreach problem. PPTEST requiredthemaximum number of cor-
rections of 12.7on average. In all, our tool succeeded in either
verifying or generating veriﬁedfeedback for 85% submissions.
Fortheremaining 315(15%)submissions,ourtoolcouldneither
generate feedback nor verify correctness. These submissio ns need
manual evaluation. MARCHA1 had the maximum percentage of
unlabeled submissions at 24% and MGCRNK had the minimum
at12%. These arise either because the SMT solver times out (we
kept the timeout of 3s for each equivalence query), or due to the
limitationsof the veriﬁcationalgorithm orthe implementa tion.
These results on the challenging set of DP submissions are en -
couraging and demonstrate effectiveness of our methodolog y and
technique. Even if we assume that all 315unhandled submissions
are faulty, wecould generate veriﬁedfeedback for 71% faultysub-
missions. Incomparison, onasetof introductory programming as-
signments, Singh et al. [46] report that 64% of faulty submissions
couldbeﬁxedusing manuallyprovidederrormodels . Ourcounter-
example guided feedback generation technique guarantees c orrect-
ness of the feedback. In addition, we would have liked to com-
municate the feedback to the students and assess their respo nses.
Unfortunately, theircontact details were not available to us.
DiversityofFeedbackandPersonalization.
The feedback propagation approaches [20, 41] suggest that t he
same feedback text written by the instructor can be propagat ed to
all submissions withina cluster. We found that this is not pr actical
and the submissions withinthe same cluster require heterog eneous
feedback. Figure 6shows the distributionof submissions in aclus-
terof SUMTRIAN bythetypeoffeedback. Weonlyhighlightfeed-
backoverthelogicalcomponentsofasubmission: initializ ation(I),
update (U) and output (O). Feedback related to type declarat ions
andinputstatements(possibly,inconjunctionwithfeedba ckonthe
logical components) issummarized under the category “Othe rs”.
While only 7.9% submissions were veriﬁed to be correct, 20%
submissions had faults in one of the logical components of th e DP
strategy: initialization ( 0.7%), update ( 16.4%) and output ( 2.9%).
Asshown inFigure 6,a largepercentage ofsubmissions hadfa ults
intwological components, and 7.9% had them inall three compo-
nents. 30%ofthesubmissionswereintheotherscategory. Clearly,
itwould be difﬁcult forthe instructor topredict faults ino ther sub-
missions in a cluster by looking only at some submissions in t he
cluster and write feedback applicable to all. We do admit tha t Fig-
ure6isbasedonourclusteringapproachandotherapproache smay
yield different clusters. Even then, the clusters would be c orrectTable 3: Submissionsbyfaultycomponents.
Faulty comp. SUMTRIAN MGCRNK MARCHA1 PPTEST
I only 36 15 0 0
U only 229 7 5 2
O only 31 1 6 0
I&U 29 18 2 8
I&O 10 0 1 0
U&O 97 1 2 0
I&U&O 30 0 11 0
Others 197 24 8 19
Total 659 66 35 29
0 5 10 15 20 25 30 35 40 450100200300400500600
SubmissionIDsFeedback SizeBefore Simpliﬁcation
AfterSimpliﬁcation
Figure7: Effectofsimpliﬁcationonfeedbacksizefor MGCRNK
onlyinaprobabilistic senseandtheveriﬁcationphase,wesuggest,
would add certainty about correctness of feedback.
Our technique generated personalized feedback depending o n
which components of a submission were faulty. Table 3 shows
the number of submissions by the faulty components. Across t he
problems, PPTEST had the maximum percentage 53.7% of sub-
missions requiring corrections to multiple logical compon ents and
SUMTRIAN had the minimum percentage 17.5%. The most com-
mon faultycomponents variedacross problems.
TypesofFaults FoundandCorrected.
Our tool found a wide range of faults and suggested appropria te
correctionsforthem. Thisismadepossiblebyavailability ofacor-
rect submission to verify against and the ability of our veri ﬁcation
algorithm to reﬁne the equivalence queries to ﬁnd all faults . The
faults found and corrected include: incorrect loop headers , initial-
ization mistakes including missing or spurious initializa tion, miss-
ing cases in the DP recurrence, errors in expressions and gua rds,
incorrect dimensions, etc.
ConcisenessofFeedback.
To reduce the size of formulae in the generated feedback, we
perform simpliﬁcations outlined in Section 4. We measure th e ef-
fectiveness of the simpliﬁcations by disabling them and usi ng the
sum of AST sizes (#nodes in the AST) of the guards in our feed-
back text as feedback size . Figure 7 shows the impact of the sim-
pliﬁcations on feedback size in the case of MGCRNK by plotting
submission IDs versus feedback size. The ﬁgure excludes cas es
where simpliﬁcation had no impact on feedback size. Simpliﬁ ca-
tions ensured that the feedback size was at most 150, and 42.1on
average. Without simpliﬁcations, the maximum feedback siz e was599. Simpliﬁcations, where applicable, reduced feedback size by
63.1%on anaverage across the problems.
5.3 ComparisonwithCodeChef
Our tool was able to verify 12submissions as correct that were
tagged by CodeChef as incorrect. This was surprising becaus e
CodeChefusestestswhichshouldnotproduce such falsepositives .
On investigation, we found that the program logic was indeed cor-
rect, as veriﬁed by our tool. The faults were localized to out put
formatting, or in custom input/output functions. Understa ndably,
black-box testing used by CodeChef cannot distinguish betw een
formatting and logical errors. However, being able to disti nguish
betweenthesetypesoffaultswouldsavetimeforthestudent s. Our
tool ﬁnds logical faults but not formattingerrors.
Due to the incompleteness of testing, CodeChef did not iden-
tify all faulty submissions ( false negatives ). This can hurt students
since they may not realize their mistakes. We checked the cas es
when CodeChef tagged a submission as correct but our tool is-
sued some corrections. For 64submissions, our tool identiﬁed that
the submissions were making spurious initializations to th e DP ar-
ray. For 112submissions, our tool identiﬁed that the DP udpate
wasperformedforadditionaliterationsthanrequiredandg enerated
feedback to ﬁx the bounds of loops containing update stateme nts.
Importantly, our tool detected out-of-bounds array accesses in99
submissions,andsuggestedappropriatecorrections. In 265distinct
submissions, ourtool was abletoidentifyone ormore of thef aults
described above, whereas CodeChef tagged themas correct! T hus,
our static technique has a qualitative advantage over the te st-based
approach of online judges.
5.4 Performance
WeranourexperimentsonanIntelXeonE5-16203.60GHzma-
chine with8 cores and 24GB RAM. Out tool runs only on a single
core. On an average, our tool generated feedback in 1.6s includ-
ing the time for clustering and excluding the time for identi fying
correct submissions manually.
5.5 Limitations andThreats to Validity
Our technique fails for submissions that have loop-carried de-
pendencies over scalar variables apart from the loop index v ari-
ables, submissions that use auxiliary arrays and submissio ns for
whichpatternmatchingfailstolabelstatements. Weinheri tthelim-
itations of SMT solvers in reasoning about non-linear const raints
and program expressions with undeﬁned semantics, such as di vi-
sionby0. Mostoftheunhandledcasesarisefromtheselimita tions.
Our approach cannot suggest feedback for errors in custom in -
put/output functions, output formatting, typecasting, et c. Our ap-
proach may provide spurious feedback enforcing stylistic c onfor-
mance with the instructor-validated submission. For examp le, if
a submission starts indexing into arrays from position 1but the
instructor-validated submission indexes from position 0, our tool
generates feedback requiring the submission to follow 0based in-
dexing. This may correct some misconception about array ind ex-
ing that the student may have. Nevertheless, these differen ces can
be either compiled away during pre-processing or through SM T
solving with additional annotations. We will investigate t hese in
future. Finally, our implementation currently handles onl y a fre-
quently used subset of Cconstructs and libraryfunctions.
There can be faults in our implementation that might have af-
fected our results. To address this threat, we manually chec ked the
feature values and feedback obtained, and did not encounter any
error. Threatstoexternal validityarisebecause our resul tsmaynot
generalize to other problems and submissions. We mitigated thisthreat bydrawing upon submissions from more than 1860students
on4differentproblems. Whileourtechniqueisabletohandlemo st
constructs that introductory DP coursework employs, furth er stud-
ies are required to validate our ﬁndings in the case of other p rob-
lems. In Section 5.3, we compared our tool with the classiﬁca tion
available on CodeChef. The tests usedby CodeChef are not pub lic
and hence, we cannot ascertaintheir quality. Byimprovingt he test
suites, some false negatives of CodeChef maydisappear butb lack-
box testing will not be able to distinguish between logical f aults
and formattingerrors (discussed inSection5.3).
6. RELATED WORK
ProgramRepresentationsandClustering.
In order to cluster submissions effectively, we need strate gies to
represent both the syntax and semantics of programs. Many cl us-
tering approaches use only edit distance between submissio ns [15,
44], while others use edit distance along with test-based si milar-
ity [20, 36, 12]. We use neither of these. Glassman et al. [13]
advocate a hierarchical technique where the submissions ar e ﬁrst
clustered using high-level (abstract) features and then us ing low-
level (concrete) features. An interesting recent directio n is to use
deep learning to compute and use vector representations of p ro-
grams [40, 41, 33]. Peng et al. [40] propose a pre-training te ch-
nique to automatically compute vector representations of d ifferent
ASTnodeswhichisthenfedtoatree-basedconvolutionneura lnet-
work[33]foraclassiﬁcationtask. Pieceetal.[41]propose arecur-
sive neural network to capture both the structure and functi onality
ofprograms. Thefunctionalityislearnedusinginput-outp ut exam-
ples. But the class of programs considered in [41] is very sim ple.
It onlyhandles programs whichdo not have anyvariables.
Since our experiments were focused on iterative DP solution s,
we designed features that capture the DP strategy. The above ap-
proaches are more general but unlike us, they maynot put the s ub-
missions in Figure 2 and 3 in the same cluster. Our algorithm e x-
tracts features in the presence of temporary variables and p roce-
dures, andmight be useful inother contexts as well.
FeedbackGenerationandPropagation.
Theideaofcomparinginstructorprovidedsolutionswithst udent
submissions appears in [2]. It uses graph representation an d trans-
formations for comparison of Fortranprograms. Xuand Chee [ 49]
userichergraphrepresentationsforobject-orientedprog rams. Rivers
andKoedinger [44]useeditdistance asametrictocompare gr aphs
and generate feedback. Gross et al. [15] cluster student sol utions
by structural similarity and perform syntactic comparison s with a
known correct solution to provide feedback. Feedback gener ated
by pattern matching may not always be correct. In contrast, w e
generateveriﬁedfeedback but for the restricteddomain of DP.
Alur et al. [4] develop a technique to automatically grade au -
tomata constructions using a pre-deﬁned set of corrections . Singh
et al. [46] apply sketching based synthesis to provide feedb ack for
introductory programming assignments. In addition to a ref erence
implementation, the tool takes as input an error model in the form
ofcorrectionrules. Theirerrormodelistoorestrictiveto beadapted
to our setting that requires more sophisticated repairs and that too
for a more challenging class of programs. Gulwani et al. [17] ad-
dress the orthogonal issue ofproviding feedback toaddress perfor-
mance issues,while SrikantandAggarwal [47] use machine le arn-
ing to assess coding quality of prospective employees and do not
provide feedback on incorrect solutions.
The idea of exploiting the common patterns in DP programshas been used by Pu et al. [43] but for synthesis of DP programs .
The clustering-based approaches [20, 41] propagate the ins tructor-
provided feedback to all submissions in the same cluster, wh ereas
we generate personalized and veriﬁed feedback for each subm is-
sioninaclusterseparately. OverCode[12]alsoperformscl ustering
of submissions and provides a visualization technique to as sist the
instructor inmanually evaluating the submissions.
ProgramRepairandEquivalenceChecking.
Genetic programming has been used to automatically generat e
program repairs [5, 11, 27]. These approaches are not direct ly ap-
plicable in our setting as the search space of mutants is very large.
Further, GenProg [27] relies on redundancy present in other parts
of the code for ﬁxing faults. This condition is not met in our s et-
ting. Software transplantation [18, 6] transfers function ality from
one program to another through genetic programming and slic ing.
Prophet [30] learns a probabilistic, application independ ent model
of correct code from existing patches, and uses it to rank rep air
candidates from a search space. These are generate-and-val idate
approaches which rely on a test suite to validate the changes . In
comparison, we derive corrections for a faulty submission b y pro-
gram equivalence checking witha correct submission.
Konighopher et. al. [25] present a repair technique using re f-
erence implementations. Their fault model is restrictive a nd only
considers faulty RHS. Many approaches rely on program speci ﬁ-
cations for repair, including contracts [39, 48], LTL [23], asser-
tions [45] and pre-post conditions [14, 28, 19]. Recent appr oaches
that use tests to infer speciﬁcations and propose repairs in clude
SemFix[37],MintHint[24],DirectFix[31]andAngelix[32] . These
approaches use synthesis [22], symbolic execution [9] and p artial
MaxSAT [10] respectively. BothDirectFixand Angelixuse pa rtial
MaxSAT but Angelix extracts more lightweight repair constr aints
toachieve scalability. SPR[29] uses parameterized transf ormation
schemas to search over the space of program repairs. In contr ast,
we use instructor-validated submissions and a combination of pat-
ternmatching, static analysis and SMTsolving.
Automated equivalence checking betweena program and itsop -
timized version has been studied in translation validation [42, 35,
7]. Partush and Yahav [38] design an abstract interpretatio n based
technique to check equivalence of a program and its patched v er-
sion. In comparison, our technique performs equivalence ch eck
between programs writtenbydifferent individuals indepen dently.
All these approaches are designed for developers and deal wi th
only one program at a time. Our tool targets iterative DP solu tions
written by students and works on a large number of submission s
simultaneously. It combines clustering and veriﬁcation to handle
both the scale andvariations instudent submissions.
7. CONCLUSIONSAND FUTUREWORK
We presented semi-supervised veriﬁed feedback generation to
deal with both scale and variations in student submissions, while
minimizing the instructor’s efforts and ensuring feedback quality.
We alsodesigned a novel counter-example guided feedback ge ner-
ationalgorithm. Wesuccessfullydemonstratedtheeffecti veness of
our technique on 2226submissions to 4DPproblems.
Our results are encouraging and suggest that the combinatio n
of clustering and veriﬁcation can pave way for practical fee dback
generation tools. There are many possible directions to imp rove
clustering and veriﬁcation by designing sophisticated alg orithms.
We plantoinvestigate these for more problem domains.
8. REFERENCES
[1] www.acm.org/public-policy/education-policy-commi ttee.[2] A.Adam andJ.-P.Laurent.LAURA,a system todebug
student programs. Artiﬁcial Intelligence , 15(1-2):75 –122,
1980.
[3] A.V. Aho, M. S.Lam,R.Sethi,and J.D. Ullman.
Compilers: Principles,Techniques, and Tools .Pearson, 2nd
edition, 2006.
[4] R.Alur, L.D’Antoni,S.Gulwani, D.Kini,and
M. Viswanathan. Automated grading of DFAconstructions.
InIJCAI,pages 1976–1982, 2013.
[5] A.Arcuri. Onthe Automation ofFixingSoftware Bugs. In
ICSE Companion , pages 1003–1006, 2008.
[6] E.T. Barr,M. Harman, Y.Jia, A.Marginean, andJ. Petke.
Automated software transplantation. In ISSTA, pages
257–269, 2015.
[7] C.Barrett,Y. Fang, B.Goldberg, Y. Hu,A. Pnueli,and
L.Zuck. TVOC:A translationvalidator for optimizing
compilers. In CAV,pages 291–295. Springer-Verlag, 2005.
[8] R.E.Bellman. Dynamic Programming . Dover Publications,
Incorporated, 2003.
[9] C.Cadar, D.Dunbar, andD. R.Engler.KLEE:Unassisted
and Automatic Generation ofHigh-Coverage Testsfor
Complex Systems Programs.In OSDI, pages 209–224, 2008.
[10] L.DeMoura andN.Bjørner.Z3: AnefﬁcientSMTsolver. In
TACAS,pages 337–340, 2008.
[11] V. Debroy andW.E.Wong. Using Mutationto
AutomaticallySuggest Fixesfor FaultyPrograms.In ICST,
pages 65–74, 2010.
[12] E.L.Glassman, J. Scott,R.Singh, P.J. Guo, andR.C.
Miller.OverCode: Visualizingvariationinstudent soluti ons
toprogramming problems at scale. ACMTrans.
Comput.-Hum. Interact. ,22(2):7:1–7:35, Mar. 2015.
[13] E.L.Glassman, R.Singh, and R.C.Miller.Feature
engineering forclustering student solutions. In Proceedings
of the FirstACMConference on Learning@ Scale
Conference , L@S’14, pages 171–172. ACM,2014.
[14] D.Gopinath, Z.M. Malik, andS.Khurshid.
Speciﬁcation-based program repair usingSAT.In TACAS,
pages 173–188, 2011.
[15] S.Gross, X.Zhu, B.Hammer, andN. Pinkwart.Cluster
based feedback provision strategies inintelligenttutori ng
systems. In Intelligent TutoringSystems , pages 699–700.
Springer, 2012.
[16] S.Gulwani andS.Juvekar. Bound analysis using backwar d
symbolic execution. Technical report,October 2009.
[17] S.Gulwani, I. Radi ˇcek, andF.Zuleger. Feedback generation
for performance problems inintroductory programming
assignments. In FSE,pages 41–51, 2014.
[18] M. Harman, W.B.Langdon, and W.Weimer.Genetic
programming forreverse engineering. In WCRE,pages 1–10,
2013.
[19] H.He and N.Gupta. Automated debugging usingpath-base d
weakest preconditions. In FASE,pages 267–280, 2004.
[20] J. Huang, C.Piech, A.Nguyen, and L.J.Guibas. Syntacti c
and functional variabilityof a millioncode submissions in a
machine learning MOOC.In AIED,2013.
[21] P.Ihantola, T.Ahoniemi, V.Karavirta,and O.Seppälä.
Review of recent systems for automatic assessment of
programming assignments. In Proceedings of the 10thKoli
CallingInternational Conference on Computing Education
Research, Koli Calling’10, pages 86–93, 2010.
[22] S.Jha, S.Gulwani, S.A.Seshia, andA. Tiwari.Oracle-guided Component-based Program Synthesis. In
ICSE,pages 215–224, 2010.
[23] B.Jobstmann, A.Griesmayer, and R.Bloem. Program
Repair as a Game.In CAV,pages 287–294, 2005.
[24] S.Kaleeswaran, V. Tulsian,A.Kanade, and A.Orso.
MintHint: Automated synthesis of repair hints. In ICSE,
pages 266–276, 2014.
[25] R.Konighofer andR.Bloem. Automated ErrorLocalizati on
and Correctionfor Imperative Programs. In FMCAD,pages
91–100, 2011.
[26] C.Lattner andV. Adve. LLVM:ACompilation Framework
for LifelongProgram Analysis & Transformation. In CGO,
PaloAlto, California,Mar 2004.
[27] C.Le Goues, T.N.,S.Forrest,and W.Weimer.Genprog: A
Generic MethodforAutomatic SoftwareRepair. IEEETrans.
onSoftware Engineering , pages 54–72, 2012.
[28] F.Logozzo and T.Ball.Modular and VeriﬁedAutomatic
Program Repair.In OOPSLA,pages 133–146, 2012.
[29] F.Longand M. Rinard.Stagedprogram repair with
condition synthesis. In ESEC/FSE ,2015.
[30] F.Longand M. Rinard.Automatic patchgeneration by
learningcorrect code. In POPL,pages 298–312, 2016.
[31] S.Mechtaev, J. Yi,and A.Roychoudhury. DirectFix:
Looking for simple program repairs. In ICSE,2015.
[32] S.Mechtaev, J. Yi,and A.Roychoudhury. Angelix: Scala ble
Multiline Program PatchSynthesis via Symbolic Analysis.
InICSE,2016.
[33] L.Mou, G. Li,L.Zhang, T.Wang, and Z.Jin.Convolutiona l
neural networks over tree structures for programming
language processing. In AAAI,2016.
[34] I.Narasamdya and A.Voronkov. Findingbasic blockand
variable correspondence. In StaticAnalysis , pages 251–267.
Springer, 2005.
[35] G.C.Necula. Translation validation foran optimizing
compiler. In PLDI,pages 83–94. ACM, 2000.
[36] A.Nguyen, C.Piech, J.Huang, andL.Guibas. Codewebs:
Scalable homework search for massive open online
programming courses. In WWW, pages 491–502, 2014.
[37] H.D. Nguyen, D. Qi,A.Roychoudhury, and S.Chandra.
SemFix: Program Repair via Semantic Analysis. In ICSE,
2013.
[38] N.Partushand E.Yahav.Abstract semantic differencin g via
speculative correlation. In OOPSLA,pages 811–828. ACM,
2014.
[39] Y. Pei,Y. Wei,C.A.Furia, M.Nordio, and B.Meyer.
Code-based Automated Program Fixing. In ASE,pages
392–395, 2011.
[40] H.Peng, L.Mou, G.Li,Y.Liu,L.Zhang, andZ.Jin.
Buildingprogram vector representations for deep learning .
InKnowledge Science, Engineering and Management , pages
547–553. Springer, 2015.
[41] C.Piech, J.Huang, A.Nguyen, M. Phulsuksombati,
M. Sahami,and L.J.Guibas. Learning program embeddings
topropagate feedback onstudent code. In ICML,pages
1093–1102, 2015.
[42] A.Pnueli, M.Siegel,and E.Singerman. Translation
validation.In TACAS,pages 151–166. Springer-Verlag,1998.
[43] Y. Pu,R.Bodik, and S.Srivastava.Synthesis of ﬁrst-or der
dynamic programming algorithms. In OOPSLA,pages
83–98, 2011.
[44] K.Rivers and K.Koedinger. Automatic generation ofprogramming feedback: A data-driven approach. In AIED,
pages 4:50–4:59, 2013.
[45] R.Samanta, O.Olivo, andE.Emerson. Cost-aware
automatic program repair. In Static Analysis ,Lecture Notes
inComputer Science.2014.
[46] R.Singh, S.Gulwani, and A.Solar-Lezama.Automated
feedback generation forintroductory programming
assignments. In PLDI,pages 15–26, 2013.
[47] S.Srikant andV. Aggarwal. Asystem tograde computer
programming skillsusing machine learning. In KDD,pages
1887–1896, 2014.
[48] Y. Wei,Y.Pei,C.A. Furia,L.S.Silva,S.Buchholz,
B.Meyer, andA. Zeller.Automated Fixingof Programs with
Contracts. In ISSTA,pages 61–72, 2010.
[49] S.Xuand Y.S.Chee. Transformation-based diagnosis of
student programs forprogramming tutoringsystems.
Software Engineering, IEEETransactions on ,
29(4):360–384, April2003.