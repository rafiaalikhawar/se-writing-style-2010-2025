Sentiment Analysis for Software Engineering:
How Far Can We Go?
Bin Lin
Software Institute
Università della Svizzera italiana (USI)
SwitzerlandFiorella Zampetti
Department of Engineering
University of Sannio
ItalyGabriele Bavota
Software Institute
Università della Svizzera italiana (USI)
Switzerland
Massimiliano Di Penta
Department of Engineering
University of Sannio
ItalyMichele Lanza
Software Institute
Università della Svizzera italiana (USI)
SwitzerlandRocco Oliveto
STAKE Lab
University of Molise
Italy
ABSTRACT
Sentiment analysis has been applied to various software engineer-
ing(SE)tasks,suchasevaluatingappreviewsoranalyzingdevelop-
ers’ emotions in commit messages. Studies indicate that sentiment
analysis tools provide unreliable results when used out-of-the-box,
sincetheyarenotdesignedtoprocessSEdatasets.The silverbul-
letfor a successful application of sentiment analysis tools to SE
datasets might be their customization to the specific usage context.
Wedescribeourexperienceinbuildingasoftwarelibraryrecom-
menderexploitingdevelopers’opinionsminedfromStackOverflow.
Toreachourgoal,weretrained—onasetof40kmanuallylabeled
sentences/words extracted from Stack Overflow—a state-of-the-art
sentimentanalysistoolexploitingdeeplearning.Despitesuchan
effort-andtime-consumingtrainingprocess,theresultswerenega-
tive.WechangedourfocusandperformedathoroughinvestigationoftheaccuracyofcommonlyusedtoolstoidentifythesentimentofSErelatedtexts.Meanwhile,wealsostudiedtheimpactofdifferentdatasetsontoolperformance.Ourresultsshouldwarntheresearch
community about the strong limitations of current sentiment anal-
ysis tools.
CCS CONCEPTS
•Information systems →Sentiment analysis ;
KEYWORDS
sentiment analysis, software engineering, NLP
ACM Reference Format:
BinLin,FiorellaZampetti,GabrieleBavota,MassimilianoDiPenta,Michele
Lanza, and Rocco Oliveto. 2018. Sentiment Analysis for Software Engineer-
ing:HowFarCanWeGo?.In ICSE’18:ICSE’18:40thInternationalConference
onSoftwareEngineering,May27-June3,2018,Gothenburg,Sweden. ACM,
New York, NY, USA, 11 pages. https://doi.org/10.1145/3180155.3180195
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s). Publication rights licensed to the
Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.31801951 INTRODUCTION
Recentyearshaveseentheriseoftechniquesandtoolstoautomati-
callymineopinionsfromonlinesources[ 26].Themainapplication
ofthesetechniquesistheidentificationofthemoodandfeelings
expressedintextualreviewsbycustomers(e.g., tosummarizethe
viewers’ judgment of a movie [ 32]). Sentiment analysis [ 26]i sa
frequently used opinion mining technique. Its goal is to identify
affective states and subjective opinions reported in sentences. In
its basic usage scenario, sentiment analysis is used to classifycus-
tomers’ written opinions as negative, neutral, or positive.
The software engineering (SE) community has adopted senti-
ment analysis tools for various purposes. It has been used to assess
thepolarityofapps’reviews(e.g., Gouletal.[6]andPanichella etal.
[27]), and to identify sentences expressing negative opinions about
APIs [38]. Tourani et al.[35] used sentiment analysis to identify
distress or happiness in a development team, while Garcia et al.
[5] found that developers expressing strong positive or negative
emotionsinissuetrackersaremorelikelytobecomeinactiveinthe
open source projects they contribute. Ortu et al.[24] studied the
impactofsentimentexpressedinissues’commentsandtheissue
resolution time,while Sinha etal.[31]investigated thesentiment
of developers’ commits.
Mostpriorworksleveragesentimentanalysistoolsnotdesigned
toworkonsoftware-relatedtextualdocuments.This“out-of-the-
box” usage has been criticized due to the poor accuracy these tools
achievedwhenappliedinacontextdifferentfromtheoneforwhich
theyhavebeendesignedand/ortrained[ 16,23,35].Forexample,the
Stanford CoreNLP [32] opinion miner has been trained on movie
reviews. In essence, the silver bullet to make sentiment analysis
successful when applied on software engineering datasets might be
their customization to the specific context.
Thus,therecenttrendistocustomizeexistingsentimentanalysis
tools to properly work on software engineering datasets [ 15,36].
ThemostwidelyusedtoolintheSEcommunityis SentiStrength
[34].SentiStrength assesses the sentiment of a sentence by look-
ingatthesinglewordsthesentenceiscomposedof,thatis,itassignspositive/negativescorestothewordsandthensumsupthesescorestoobtainanoverallsentimentforthesentence.
SentiStrength can
be customizedto providethe sentimentfor domain-specificterms.
Forinstance,IslamandZibran[ 15]developed SentiStrength−SE,
which improved identification performance for SE-related texts.
942018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza and R. Oliveto
Inspired by these works, we started a research project to design
and implement an approach to recommend software libraries to
developers. The idea was to assess the quality of software libraries
exploitingcrowdsourcedknowledgebyminingdevelopers’opin-
ionsonStackOverflow.Onekeycomponentneededtosucceedwas
a reliable sentiment analysis tool (e.g., to capture positive/negative
developers’ opinions about the usability of a library). Given the
warning raised by previous work in our field [ 16,23,35] there
wastheneed fortrainingandcustomizing thesentimentanalysis
tool to the Stack Overflow context. Also, looking at the opinion
mining literature, we decided to adopt a state-of-the-art approach
basedona RecursiveNeuralNetwork(RNN),ableto compute the
sentimentofasentencenotbyjustsummingupthesentimentof
positive/negativeterms,butbygrammaticallyanalyzingtheway
words compose the meaning of a sentence [32].
We built a training set by manually assigning a sentiment score
to a total of∼40k sentences/words extracted from Stack Overflow.
Despitetheconsiderablemanualeffort,theempiricalevaluationwe
performed led to negative results, with unacceptable accuracylev-
els in classifying positive/negative opinions. Given this, we started
a thorough empirical investigation aimed at assessing the actual
performance of sentiment analysis tools when applied on software
engineering datasets with the goal of identifying atechnique able
toprovideacceptableresults.Weexperimentedwithallmajortech-
niquesusedinourcommunity,byusingthemout-of-the-boxaswell
aswithcustomizationdesignedtoworkinthesoftwareengineer-
ing context (e.g., SentiStrength−SE[15]). Also, we considered
threedifferentsoftwareengineeringdatasets:(i)ourmanuallybuilt
dataset of Stack Overflow sentences, (ii) comments left on issue
trackers [25], and (iii) reviews of mobile apps [37].
Our results show that none of the state-of-the-art tools provides
a precise and reliable assessment of the sentiments expressed in
themanuallylabeledStackOverflowdatasetwebuilt(e.g., allthe
approachesachieverecallandprecisionlowerthan40%onnegative
sentences). Results are marginally better in the app reviews and in
the issue tracker datasets, which however represent simpler usage
scenarios for sentiment analysis tools.
ThegoalofourpaperistosharewiththeSEresearchcommunity
our negative findings, showing the current difficulties in applying
sentimentanalysistoolstosoftware-relateddatasets,despitemajoreffortsintailoringthemtothecontextofinterest.Ourresultsshould
also warnresearchers to not simplyuse a (customized)sentiment
analysis tool assuming that it provides a reliable assessment of
the sentiments expressed in sentences, but to carefully evaluate its
performance. Finally, we share our large training dataset as well as
all the tools used in our experiments and the achieved results [ 18],
to foster replications and advances in this novel field.
Structure of the paper. Section2presentstheavailablesenti-
ment analysis tools, and discusses sentiment analysis applications
and studies in SE. Section 3 presents our original research plan.Section 4 reports and discusses the negative results we obtained
whenevaluatingthesentimentanalysiscomponentofourapproach.
Section 5 reports the design and results of the study we performed
toassesstheperformanceofsentimentanalysistoolsonsoftware
engineering datasets, while Section 6 discusses the threats that
could affect the validity of our results. Finally, after a discussion of
lessons learned (Section 7), Section 8 concludes the paper.2 RELATED WORK
Westartbyprovidinganoverviewofexistingsentimentanalysis
tools and discuss the applications of these tools in the software
engineeringdomain.Then,wepresentrecentstudiesquestioning
the effectiveness of sentiment analysis when applied on SE-related
datasets.Table1reportsasummaryofthemainsentimentanalysis
tools used in software engineering application to date.
Table 1: Sentiment analysis tools used for SE applications.
Tool Technique Trained on Used by
SentiStrength Rule-based MySpace [7–11] [15, 22, 24] [31, 33] [36]
NLTK/VADER Rule-based Micro-Blog [30] [28]
Stanford CoreNLP Recurs. Neural Net Movie Reviews [29], our work
EmoText Lexical Features Stack Overflow, JIRA [2]
SentiStrength−SE SentiStrength JIRA [15]
Uddin and Khomh Sentim. Orientation [13] API Reviews [36]
2.1 Sentiment Analysis Tools
There are several sentiment analysis tools available. Some of them
are commercial tools, such as MeaningCloud1,GetSentiment2,o r
WatsonNaturalLanguageUnderstanding3. There are also senti-
ment analysis libraries available in popular machine learning tools,
suchas RapidMiner4orWeka[12],aswellas SentiWordNet [1]an
extension of a popular lexicon database ( WordNet[20]) for senti-
ment analysis. The sentiment analysis tools applied to software
engineering applications are:
SentiStrength [34] is the most adopted one, originally trained
on MySpace5comments. The core of SentiStrength is based
on thesentiment word strength list, a collection of 298 positive
and 465 negative terms with an associated positive/negative
strength value. It also leverages a spelling correction algorithmaswellasotherwordlistssuchasa boosterwordlist andanegat-
ingwordlist,forabettersentimentassessment.
SentiStrength
assigns a sentimentscore to each word composing a sentence
under analysis, and derives the sentence sentiment by sum-ming up the individual scores. The simple approach behind
SentiStrength makesiteasytocustomizeforaspecificcon-
text by defining a list of domain-specific terms with associ-
ated sentiment scores. Despite this, only Islam and Zibran [ 15]
adopted a customized version in software engineering.
NLTK[14] is a lexicon and rule-based sentiment analysis tool hav-
ingVADER(ValenceAwareDictionaryandsEntimentReasoner)
atitscore. VADERisspecificallytunedtosocialmediatextsbyin-
corporatinga“gold-standard”sentimentlexiconextractedfrom
microblog-like contexts and manually validated by multiple
independent human judges.
Stanford CoreNLP [32] is built on top of a Recursive Neural Net-
work,whichdiffersfrom SentiStrength andNLTKthanksto
its ability to compute the sentiment of a sentence based on
howwordscomposethemeaningofthesentence,andnotby
summing up the sentiment of individual words. It has been
trained on movie reviews.
1https://www.meaningcloud.com/developer/sentiment-analysis
2https://getsentiment.3scale.net/
3https://www.ibm.com/watson/services/natural-language-understanding/
4https://rapidminer.com
5https://myspace.com/
95
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Sentiment Analysis for Software Engineering: How Far Can We Go? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
EmoTxt[2].Thisisatoolkitforemotionrecognitionfromtextthat
combines a n-gram approach proposed by Ortu et al.[24] with
lexical featuresconveying emotions inthe inputtext: emotion
lexicon,politeness, positiveandnegativesentimentscores (com-
putedbyusing SentiStrength )anduncertainty.Thenovelty
ofEmoTxtreliesontherecognitionofspecificemotions,such
asjoy,love,andanger.Thetoolhasbeenpreliminaryevaluated
on two datasets mined from Stack Overflow and JIRA [2].
2.2 SentimentAnalysis&SoftwareEngineering
Sentiment analysis has been applied on different software engi-
neeringartifacts,suchastechnicalartifacts( e.g.,issuesandcommit
messages) and crowd-generated content (e.g., forum messages and
users’ reviews), and to support different tasks.
Sentimentiscommonlyexpressedinthedeveloper-writtencom-
mit messages and issues [ 17]. Guzman et al.[9] analyzed the senti-
mentofcommitcommentsinGitHubandprovidedevidencethat
projects having more distributed teams tend to have a higher posi-
tivepolarityintheiremotionalcontent.Additionally,theauthors
found that those comments written on Mondays tend to express
more negative emotions. A similar study was conducted by Sinha
etal.[31]on28,466projectswithinasevenyeartimeframe.The
results indicated that a majority of the sentiment was neutral and
that Tuesdays seem to have the most negative sentiment overall.
Also,theauthorsfoundastrongpositivecorrelationbetweenthe
numberoffileschangedandthesentimentexpressedbythecom-
mits the files were part of. Ortu et al.[24] analyzed the correlation
between the sentiment in 560k JIRA comments and the time to
fix a JIRA issue finding that positive sentiment expressed in theissuedescriptionmighthelpissuefixingtime.Finally,Souzaand
Silva[33]analyzedtherelationbetweendevelopers’sentimentand
builds performed by continuous integration servers. They found
that negative sentiment both affects and is affected by the result of
the build process.
Analyzing the polarity of apps’ reviews is particularly useful to
supporttheevolutionofmobileapplications[ 3,6,11,27].Gouletal.
[6] applied a sentiment analysis tool suite to over 5,000 reviews
observing that sentiment analysis can address current bottlenecks
torequirementsengineering,butthatcertaintypesofreviewstend
to elude algorithmic analysis. Carreño et al.[3] presented a tech-
niquebasedonAspectandSentimentUnificationModel(ASUM)to
extract common topics from apps’ reviews and present users’ opin-
ionsaboutthosetopics.Guzman etal.[8,11]proposedtheuseof
SentiStrength tosupportasimilartask.Panichella etal.[27]used
aNaiveBayesclassifiertoassigneachsentenceinusers’reviews
toa “sentimentclass” amongnegative, neutral,andpositive. This
is one of the features they use to classify reviews on the basis of
theinformationtheybring(e.g., featurerequest,problemdiscovery,
etc.). Sentiment analysis has also been applied to classify tweets
relatedtosoftwareprojects[ 7].Theresultsoftheirempiricalstudy
indicated that searching for relevant information is challengingeven if this relevant information can provide valuable input for
softwarecompaniesandsupportthecontinuousevolutionofthe
applications discussed in these tweets.
Asemotionscanimpactthedeveloperproductivity,taskcomple-
tion quality, and job satisfaction, sentiment analysis has also beenusedtodetectthepsychologicalstateofdevelopers[ 30].Guzman
and Bruegge [ 10] used sentiment analysis to investigate the role of
emotionalawarenessindevelopmentteams,whileGachechiladze
et al.[4] usedsentiment analysis to builda fine-grained modelfor
angerdetection.Inaddition,thestudybyPletea etal.[28]provided
evidencethatdeveloperstendtobemorenegativewhendiscussing
security-relatedtopics.Finally,Garcia etal.[5]analyzedtherela-
tionbetweentheemotionsandtheactivityofcontributorsinthe
OpenSourceSoftwareprojectGENTOO.Theyfoundthatcontribu-
torsaremorelikelytobecomeinactivewhentheyexpressstrong
positive or negative emotions in the issue tracker, or when they
deviate from the expected value of emotions in the mailing list.
Sentiment expressed on Q&A sites such as Stack Overflow is
also leveraged by researchers to recommend comments on quality,
deficienciesorscopesfor furtherimprov ementforsourcecode[ 29]
or to identify problematic API design features [38].
2.3 Assessment of Sentiment Analysis Tools in
Software Engineering Contexts
While the authors of the above works presented an extensive eval-
uation of the relationship between sentiment and other factors, no
analysisisreportedforwhatconcernstheaccuracyofthesentiment
classification. Indeed, unsatisfactory results have been reported by
researcherswhenusingthesesentimentanalysistoolstoanalyze
texts under software engineering contexts.
Touraniet al.[35] used SentiStrength to extract sentiment
information from user and developer mailing lists of two major
successfulandmatureprojectsfromApachesoftwarefoundation:
Tomcat and Ant. However, they found SentiStrength achieved a
very low precision, i.e.,29.56% for positive sentences and 13.1% for
negative sentences. The low precision is caused by the ambiguous
technicaltermsandthedifficultyofdistinguishingextremeposi-
tive/negativetextsfromneutralones.Novielli etal.[23]highlighted
anddiscussedthechallengesofemployingsentimentanalysistech-
niques to assess the affective load of text containing technical lexi-
con, as typical in the social programmer ecosystem.
Jongeling etal.[16]conductedacomparisonoffourwidelyused
sentimentanalysistools: SentiStrength ,NLTK,Stanford CoreNLP ,
andAlchemyAPI . They evaluated their performance on a human
labeledgoldensetfromadeveloperemotionsstudybyMurgia etal.
[21]andfoundnoneofthemcanprovideaccuratepredictionsof
expressedsentimentinthesoftwareengineeringdomain.Theyalsoobservedthatdisagreementexistsnotonlybetweensentimentanal-
ysis tools and the developers, but also between different sentiment
analysis tools themselves. Their further experiment also confirmed
thatdisagreementbetweenthesetoolscanresultincontradictory
results when using them to conduct software engineering studies.
Theresultsachievedinthesestudiescallforasentimentanalysis
techniquecuratedwithsoftwareengineeringrelateddatatoaddress
the problem of low accuracy when dealing with technical terms.
Following this suggestion,sentiment analysis tools specificfor
software datasets have beenproposed. Islam and Zibran [ 15] devel-
oped SentiStrength−SEbasedon SentiStrength toaddressthe
majordifficultiesbycreatingdomaindictionaryandintroducing
otherheuristicrules.Thepresentedevaluationshowedthattheir
tool significantly outperformed SentiStrength.
96
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza and R. Oliveto
Uddin and Khomh [ 36] detected the polarity (positive, negative,
neutral)ofsentencesrelatedtoAPIusagebyusingacustomized
versionoftheSentimentOrientationalgorithm[ 13].Thealgorithm
wasoriginallydevelopedtomineandsummarizecustomeropinions
about computer products. However, Uddin and Khomh customized
the tool with words specific to API reviews. To the best of our
knowledge, these are the only cases where the authors tried to cus-
tomize state-of-the-art sentiment analysis tools to fit the software
engineering domain.
3 METHODOLOGY
We briefly describe our initial plan to build a tool to recommend
software libraries to developers given (i) a short description of a
task at hand (i.e., functional requirements) and (ii) a list of non-
functional requirements considered more/less important by the
developerforthespecificimplementationtask(e.g., securityisof
paramountimportance,whilehighperformanceisnicetohavebut
not really needed).
The basic idea was to leverage crowdsourced knowledge by
mining opinions posted by developers while discussing on Q&A
websites such as Stack Overflow. Our plan failed due to very poor
results obtained whenmining opinionsfrom SEdatasets.For this
reason, while we present a detailed description of the opinion min-
ing process we adopted, we only provide a brief overview of the
overall idea and its different components.
stackoverﬂow
Developer8
Front-endMaven
libraries
miner
1
database2ﬁne-grained linker3
45opinion miner
679
Figure 1: Our vision of the library recommender system.
TheoverallideaisdepictedinFig.1.Thedashedarrowsrepresent
dependencies (e.g., 1and3), while the full arrows indicate flows
ofinformationpushedfromonecomponenttoanother.The libraries
minermines from the maven central repository6all available Java
libraries ( 1in Fig. 1). We extract for each library its: (i) name, (ii)
description,(iii)linktothe jarofthelatestversion,(iv)license,and
(v) number and list of clients using it. All the information is stored
inourdatabase 2.Thefine-grainedlinker minesStackOverflow
discussions to establish fine-grained links between the libraries
storedinthedatabase 4andrelevantsentencesinStackOverflow
discussions 3.
6http://central.maven.org/maven2/maven/Knowingthesentencesrelatedtoaspecificlibrary,the opinion
minercomponent can retrieve them 6, identify the expressed
sentiments (i.e., positive, neutral,o r negative), classify opinions
on the basis of the non-functional requirements it refers to (e.g.,
usability,performance,security,communitysupport,etc.),andstore
them in the database 7.
Finally, the developer interested in receiving recommendations
about software libraries submits a textual query describing the
task in a Web-based front-end and important non-functional re-
quirements 8.This information is provided to a Web service 9to
identifythemostrelevantandsuitablelibrariesconsideringboth
functional and non-functional requirements.
In the following we detail our work to create the opinion miner
component, where sentiment analysis plays a vital role. We report
thenegative results we achieved in Section 4.
3.1 Mining Opinions in Software Engineering
Datasets
Previous work that attempted to mine opinions in SE datasets [ 16,
23,35]offersaclearwarning: Usingsentimentanalysis/opinionmin-
ingtechniquesout-of-the-boxonSEdatasetsisarecipefornegative
results. Indeed, these tools have been designed to work on user’sreviews of products/movies and do not take into considerationdomain-specific terms. For example, the word robusthas a clear
positivepolaritywhenreferredtoasoftwareproduct,whileitdoes
not express a specific sentiment in a movie review. This pushedresearchers to create customized versions of these tools, enrich-
ing them with information about the sentiment of domain-specific
terms (e.g., SentiStrength−SEby Islam and Zibran [15]).
Despite the effort done by some authors in developing cus-
tomizedtools,thereisasecondmajorlimitationofthesentiment
analysistoolsmostlyusedinSE(e.g.,SentiStrength[ 34]).Such
tools assess the sentiment of a sentence by looking at the single
words in isolation, assigning positive/negative scores to the words
and then summingthese scores to obtainan overall sentiment for
the sentence. Thus, the sentence composition is ignored. For exam-
ple, a sentence such as “I would not recommend this library, even
though it is robust and fast ” would be assessed by these techniques
aspositiveinpolarity,giventhepresenceofwordshavingaposi-
tivescore(i.e., robust,fast).Suchalimitationhasbeenovercome
by the Stanford CoreNLP [32] approach used for the analysis of
sentimentinmovies’reviews.TheapproachisbasedonaRecur-
siveNeuralNetwork(RNN)computingthesentimentofasentence
based on how words compose the meaning of the sentence [ 32].
Clearly, a more advanced approach comes at a cost: The effort
required to build its training set. Indeed, it is not sufficient to sim-
ply provide the polarity for a vocabulary of words but, to learnhow positive/negative sentences are grammatically built on topof positive/negative words, it needs to know the polarity of all
intermediate nodes composing a sentence used in the training set.
We discuss the example reported in Fig. 2. Gray nodes represent
(sequencesof)wordshavinga neutralpolarity,redonesindicate
negativesentiment, green ones positivesentiment. Overall, the sen-
tence has a negative sentiment (see the root of the tree in Fig. 2),
despite the presence of several positive terms (the tree’s leafs) and
intermediate nodes.
97
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Sentiment Analysis for Software Engineering: How Far Can We Go? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
I
would not
,
recommend
this libraryeven
though
it
is
robust andfast
Figure 2: Example of the labeling needed to build the Stan-
ford CoreNLP training set.
To use this sentence composed of 14 words in the training set of
theRNN,wemustprovidethesentimentofall27nodesdepictedinFig.2.ThisallowstheRNNtolearnthatwhile“itisrobustandfast ”
hasa positivepolarityiftakenin isolation,theoverallsentence is
expressing anegative feeling about thelibrary due tothe “I would
not recommend this library ” sub-sentence.
Giventhehighcontext-specificityofourworktoSEdatasets(i.e.,
Stack Overflow posts), we decided to adopt the Stanford CoreNLP
tool [32], and to invest a substantial effort in creating a customized
training set for it. Indeed, as highlighted in previous work [ 16,
23,35],itmakesnosensetoapplyanapproachtrainedonmovie
reviews on SE datasets.
3.1.1 BuildingaTrainingSetfortheOpinionMiner. Weextracted
fromthelatestavailableStackOverflowdump(datedJuly2017)the
listofalldiscussions(i)taggedwithJava,and(ii)containingone
of the following words: library/libraries, API(s). Given our original
goal(i.e.,recommendingJavalibrariesonthebasisofcrowdsourced
opinions),we wantedtobuild atrainingset asdomain-specificas
possiblefortheRNN.Byapplyingthesefilters,wecollected276,629
discussions from which we extracted 5,073,452 sentences by using
theStanford CoreNLP toolkit [19]. We randomly selected 1,500
sentences and manually labeled them by assigning a sentiment
score to the whole sentence and to every node composing it.
Thelabelingprocesswasperformedbyfiveoftheauthors(from
nowon,evaluators)andsupportedbyaWebapplicationwebuilt.
The Web app showed to each evaluator a node (extracted from a
sentence) to label with a sentiment going from -2 to +2, with -2indicating strong negative, -1 weak negative, 0 neutral, +1 weak
positive, and +2 strong positive score. The choice of the five-levels
sentimentclassificationwas notrandom,butdriven bytheobser-
vation of the movie reviews training set made publicly available
by the authors of the Stanford CoreNLP [32] sentiment analysis
tool7.Notethatanodetoevaluatecouldbeawholesentence,an
intermediatenode(thus,asub-sentence),oraleafnode(i.e., asingle
word).Toavoidanybias,theWebappdidnotshowtotheevaluator
the complete sentence from which the node was extracted. Indeed,
knowing the context in which a word/sentence is used could intro-
duce a bias in the assessment of its sentiment polarity. Finally, the
7https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zipWebapplicationmadesuretohavetwoevaluatorsforeachnode,
thusreducingthesubjectivitybias.Thisprocess,whichtook ∼90
working hours of manual labeling, resulted in the total labeling of
the sentiment polarity for 39,924 nodes (i.e., 19,962 nodes extracted
from the 1,500 sentences ×2 evaluators per node).
Once the labeling was completed, two of the authors worked on
conflicts resolution (i.e., cases in which two evaluator assigned a
differentsentimenttothesamenode).Allthe279conflictsinvolving
complete sentences (18.6% of the labeled sentences) were fixed.Indeed, it is of paramount importance to assign a consistent and
double-checked sentiment tothe complete sentences, considering
the fact that they will be used as a ground truth to evaluate our
approach. Concerning the intermediate/leaf nodes, we had a total
of 2,199conflicts (11.9% ofthe labeled intermediate/leaf nodes). We
decided to only manually solve 123 strong conflicts, meaning those
forwhichtherewasascoredifference ≥2(e.g.,oneoftheevaluators
gave 1, the other one -1), while we automatically process the 2,076
having a conflict of only one point. Indeed, slight variations of the
assigned sentiment (e.g., one evaluator gave 1 and the other 2) are
expected due to the subjectivity of the task. The final sentiment
scorewas s,incasetherewasagreementbetweentheevaluators,
while it was round[(s1+s2)/2] in case of unsolved conflict, where
roundistheroundingfunctiontotheclosestintegervalueand si
is the sentiment assigned by the ithevaluator.
4 NEGATIVE RESULTS
Before incorporating the opinion miner component, we decided to
assess it individually, and not in the context of the whole library
recommendationtask.Weperformedthisassessmentonthedataset
ofmanuallylabeled1,500sentences.Amongthosesentences,178
are positive, 1,191 are neutral, and 131 are negative. We performed
a ten-fold cross validation: We divided the 1,500 sentences into ten
different sets, each one composed of 150 sentences. Then, we used
asetasa testset(weonlyusethe150completesentencesinthetest
set, and not all their intermediate/leaf nodes), while the remaining
1,350sentences,withalltheirlabeledintermediate/leafnodes,were
usedfortraining8.Sincewearemostlyinterestedindiscriminating
betweennegative, neutral, and positiveopinions, we discretized the
sentimentin thetest setinto thesethreelevels.Sentenceslabeled
with “-2”and “-1”are considerednegative (-1),those labeledwith
“0”neutral(0),andthoselabeledwith“+1”and“+2”aspositive(+1).
We discretized the output of the RNN into the same three levels.
Weassessedtheaccuracyoftheopinionminerbycomputingrecall
andprecisionforeachcategory.Computingtheoverallaccuracy
would not be effective, given the vast majority of neutralopinions
in ourdataset (i.e., a constant neutralclassifier would obtain ahigh
accuracy, ignoring negativeandpositiveopinions).
Table2reportstheresultsachievedbyapplying Stanford Core -
NLP SO9on sentences extracted from Stack Overflow discussions.
8TheStanfordCoreNLP toolrequires—duringthe trainingoftheneuralnetwork—a
socalled development settotunesomeinternalparametersofthenetwork.Among
the 1,350 sentences with intermediate/leaf nodes in training set we randomly selected
300 sentences for composing the development set at each run.
9Stanford CoreNLP SO isthenameofthetoolwithournewmodeltrainedwithStack
Overflow discussions, while StanfordCoreNLP is the sentiment analysis component
ofStanfordCoreNLP with the default model trained using movie reviews.
98
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza and R. Oliveto
Table 2: Testing results of Stanford CoreNLP sentiment analyzer with new model trained with Stack Overflow discussions.
batch# correct
prediction# positivesentencespositiveprecisionpositiverecall
# neutralsentencesneutralprecisionneutralrecall
# negativesentencesnegativeprecisionnegativerecall
1 113 10 0.250 0.200 118 0.835 0.898 22 0.333 0.227
2 112 15 0.294 0.333 118 0.853 0.839 17 0.471 0.471
3 116 15 0.000 0.000 121 0.819 0.934 14 0.273 0.214
4 123 9 0.600 0.333 122 0.875 0.918 19 0.471 0.421
5 110 10 0.167 0.100 119 0.833 0.840 21 0.375 0.429
6 129 11 0.600 0.273 118 0.891 0.975 21 0.688 0.524
7 93 6 0.111 0.167 130 0.911 0.631 14 0.196 0.714
8 117 17 0.400 0.118 116 0.809 0.948 17 0.556 0.294
9 111 18 0.333 0.056 113 0.770 0.947 19 0.375 0.158
10 115 20 1.000 0.050 116 0.799 0.957 14 0.300 0.214
Overall 1139 131 0.317 0.145 1191 0.836 0.886 178 0.365 0.365
Thetableshowsthenumberofcorrectpredictions,thenumberof
positive/neutral/negative sentences in the batch of testing sets and
thecorrespondingprecision/recallvalues,whilethelastrowreports
the overall performance on the whole dataset. Table 3 shows some
concreteexamplesofsentimentanalysiswith Stanford CoreNLP
SO.
Table 3: Examples of sentiment analysis results of Stanford
CoreNLP SO.
Sentence Oracle Prediction
It even works on Android. Positive Positive
Hope that helps some of you with the same problem. Positive Negative
There is a central interface to access this API. Neutral Neutral
How is blocking performed? Neutral Negative
I am not able to deploy my App Engine project locally. Negative Negative
Anyway, their current behavior does not allow what you
want.Negative Neutral
The results shown in Table 2 highlight that, despite the specific
training, Stanford CoreNLP SO doesnotachievegoodperformance
in analyzing sentiment of Stack Overflow discussions. Indeed, its
precision and recall in detecting positive and negative sentiments
is below 40%, thus discouraging its usage as a fundamental part
of a recommendation system. Although Stanford CoreNLP SO can
correctly identify more negative than positive sentences, only asmall fraction of sentences with positive/negative sentiment is
identified.Also,therearemoremistakenlythancorrectlyidentified
sentences in both sets.
Based on the results we achieved, it is impracticable to build on
the top of Stanford CoreNLP SO an effective recommender system
forlibraries:Thehighpercentageofwrongsentimentclassification
willlikelyresultintherecommendationofthewronglibrary.Thus,
besides the huge effort we spent to train Stanford CoreNLP SO
withaspecificandlargesoftwaredataset,wefailedinachievingan
effective sentiment analysis estimator. For this reason, we decided
tochangeouroriginalplanandperformadeeperanalysisoftheac-
curacy of sentiment analysis tools when used on software-related
datasets. Specifically, we aim to understand whether (i) domainspecific training data really helps in increasing the accuracy ofsentiment analysis tool; and whether (ii) other state-of-the-art sen-
timent analysis tools are able to obtain good results on software
engineeringdatasets,includingourmanuallylabeledStackOver-
flow dataset. Understanding how these tools perform can also help
us in gaining deeper insights into the current state of sentiment
analysis for software engineering.
5 EVALUATING SENTIMENT ANALYSIS FOR
SOFTWARE ENGINEERING
Thegoalofthestudyistoanalyzetheaccuracyofsentimentanaly-
sistoolswhenappliedtosoftwareengineeringdatasets,withthe
purposeof investigating how different contexts can impact their ef-
fectiveness.The contextofthestudyconsistsoftextextractedfrom
threesoftware-relateddatasets,namelyStackOverflowdiscussions,
mobile app reviews, and JIRA issue comments.
5.1 Research Questions and Context
The study aims to answer the following research questions:
RQ1:How does our Stanford CoreNLP SO perform compared to
othersentimentanalysistools? Wewanttoverifywhetherother
state-of-the-art tools are able to achieve better accuracy on the
Stack Overflow dataset we manually built, thus highlighting
limitationsof Stanford CoreNLP SO .Indeed,itcouldbethatour
choice of the Stanford CoreNLP and therefore of developing
Stanford CoreNLP SO wasnotthemostsuitableone,andother
existing tools already provide better performance.
RQ2:Do different software-related datasets impact the performance
ofsentimentanalysistools? Wewanttoinvestigatetheextentto
which, analyzing other kinds of software engineering datasets,
e.g.,issue comments and app reviews, sentiment analysis tools
wouldachievedifferentperformancethanforStackOverflow
posts. For example, such sources might contain less neutral
sentencesand,theappreviewsinparticular,bemoresimilarto
the typical training sets of sentiment analysis tools.
The context of the study consists of textual documents from
threedifferentSErepositories, i.e.,(i)Question&Answerforums,
i.e.,StackOverflowdiscussions,(ii)appstores, i.e.,users’reviews
on mobile apps, and (iii) issue trackers, i.e.,JIRA issue comments.
99
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Sentiment Analysis for Software Engineering: How Far Can We Go? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
We chose these types of textual documents as they have been
studiedbySEresearchers,alsointhecontextofsentimentanaly-
sis[2,24,27,36].Asourgoalistoevaluatetheaccuracyofdifferent
sentiment analysis tools on these three datasets, we need to define
the ground truth sentiment for each of the sentences/texts they
contain.
The following process was adopted to collect the three datasets
and define their ground truth:
•Stack Overflow discussions. Wereusethegroundtruthfor
the 1,500 sentences used to evaluate Stanford CoreNLP SO.
•Mobile app reviews. We randomly selected 341 reviews from
thedatasetof3kreviewsprovidedbyVillarroel etal.[37],which
containsmanually-labeledreviewsclassifiedonthebasisofthe
main information they contain. Four categories are considered:
bugreporting, suggestionfornewfeature, requestforimproving
non-functional requirements (e.g.,performance of the app), and
other(meaning,reviewsnotbelongingtoanyoftheprevious
categories). When performing the random selection, we made
sure to respect the proportion of reviews belonging to the four
categories in the original population in our sample (e.g., if 50%
ofthe3kreviewsbelongedtothe“other”category,werandomly
selected50%ofoursamplefromthatcategory).The341selected
reviews represent a statistically significant sample with 95%
confidence level±5% confidence interval.
Onceselected,wemanuallylabeledthesentimentofeachre-
view.Thelabelingprocesswasperformedbytwooftheauthors
(from now on, evaluators). The evaluators had to decide where
the text is positive, neutral, or negative. A third evaluator was
involved to solve 51 conflict cases.
•JIRA issuecomments. WeusethedatasetcollectedbyOrtu
et al.[25], containing 4k sentences labeled by three raters with
respect to four emotions: love,joy,anger, and sadness. This
dataset has been used in several studies as the “golden set” for
evaluatingsentimentanalysistools[ 15,16].Duringtheoriginal
labeling process, each sentence was labeled with one of six
emotions: love,joy,surprise,anger,sadness,fear.Among these
six emotions, love,joy,anger, and sadnessare mostly expressed.
As also done by Jongeling et al.[16], we map the sentences
withthelabel loveorjoyintopositivesentences,andthosewith
labelangerorsadnessinto negative sentences.
Table4reportsforeachdataset(i)thenumberofsentencesex-
tracted, and (ii) the number of positive, neutral, negative sentences.
Table4:Datasetusedforevaluatingsentimentanalysistools
in software engineering
Dataset # sentences # positive # neutral # negative
Stack Overflow 1,500 178 1,191 131
App reviews 341 186 25 130
JIRA issue 926 290 0 636
5.2 Data Collection and Analysis
Onthethreedatasetsdescribedaboveweexperimentedwiththe
following tools, which are popular in the SE research community:•SentiStrength .SentiStrength does not give the sentiment
ofthetextdirectly,instead,itreportstwosentimentstrength
scoresofthetextanalyzed:onescoreforthenegativesentiment
expressed in the text from -1 (not negative) to -5 (extremely
negative),theother forthepositivesentimentexpressedfrom
1 (not positive) to 5 (extremely positive). We sum these two
scores, and map the sum of over 0, 0, and below 0 into positive,
neutral, and negative, respectively.
•NLTK.Based on VADER Sentiment Analysis, NLTKreports
foursentimentstrengthscoresforthetextanalyzed:“negative”,
“neutral”,“positive”,and“compound”.Thescoresfor“negative”,
“neutral”,and“positive”rangefrom0to1,whilethe“compound”
scoreisnormalizedtobebetween-1(mostextremenegative)
and +1 (most extreme positive). As suggested by the author
oftheVADERcomponent10,weusethefollowingthresholds
to identify the sentiment of the text analyzed: score≥0.5:
positive;−0.5<score<0.5: neutral; score≤−0.5: negative.
•Stanford CoreNLP .Bydefault, Stanford CoreNLP reportsthe
sentiment ofthetext ona five-valuescale: verynegative, neg-
ative, neutral, positive, and very positive. Since we are only
interestedindiscriminatingbetweennegative,neutral,andpos-itiveopinions,wemergedverynegativeintonegative,andvery
positive into positive.
•SentiStrength-SE .As it is a tool based on SentiStrength ,
andusesthesameformatofreportedresults,weinterpretits
sentiment score by adopting the same approach we used for
SentiStrength.
•Stanford CoreNLP SO .Similarly,weusethesameapproach
adoptedfor Stanford CoreNLP toconvertfive-scalevaluesinto
three-scalevalues.Toexaminetheperformanceonappreviews
and JIRA issue comments, we used the Stack Overflow labeled
sentences (including internal nodes) as training set11.
We assess the accuracy of the tools by computing recall and
precisionforeachofthethreeconsideredsentimentcategories(i.e.,
positive, neutral, negative) in each dataset.
5.3 Results
Table5reportstheresultsweachievedbyapplyingthefivesenti-
mentanalysisapproachesonthethreedifferentSEdatasets.The
tablereportsthenumberofcorrectpredictionsmadebythetool,andprecision/recallforpredictingsentimentofpositive/neutral/negative
sentences. For each dataset/metric, the best achieved results are
highlightedin bold.Inthefollowingwediscusstheachievedresults
aiming at answering our research questions.
5.3.1 RQ 1:Howdoesour Stanford CoreNLP SO performascom-
paredtoothersentimentanalysistools? ToanswerRQ 1,weanalyze
theresultsachievedbythefivetoolsontheStackOverflowdataset
we built.
Asforthecomparisonof Stanford CoreNLP SO withtheoriginal
model of Stanford CoreNLP , the results show that on neutral sen-
tences Stanford CoreNLP SO achievesabetterrecallwhilekeeping
almostthesamelevelofprecision.Also,onpositiveandnegative
sentences Stanford CoreNLP SO is still able to provide a good in-
crement of the precision.
10https://github.com/cjhutto/vaderSentiment
11In this case, 20% of the training set was used as development set.
100
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza and R. Oliveto
Table 5: Evaluation results for sentiment analysis tools applied in software engineering domain. In bold the best results.
dataset tool# correct
predictionpositiveprecisionpositiverecall
neutralprecisionneutralrecall
negativeprecisionnegativerecall
Stack OverflowSentiStrength 1,043 0.200 0.359 0.858 0.772 0.397 0.433
NLTK 1,168 0.317 0.244 0.815 0.941 0.625 0.084
Stanford CoreNLP 604 0.231 0.344 0.884 0.344 0.177 0.837
SentiStrength-SE 1,170 0.312 0.221 0.826 0.930 0.500 0.185
Stanford CoreNLP SO 1,139 0.317 0.145 0.836 0.886 0.365 0.365
App reviewsSentiStrength 213 0.745 0.866 0.113 0.320 0.815 0.338
NLTK 184 0.751 0.812 0.093 0.440 1.000 0.169
Stanford CoreNLP 237 0.831 0.715 0.176 0.240 0.667 0.754
SentiStrength-SE 201 0.741 0.817 0.106 0.400 0.929 0.300
Stanford CoreNLP SO 142 0.770 0.253 0.084 0.320 0.470 0.669
JIRA issuesSentiStrength 714 0.850 0.921 -- 0.993 0.703
NLTK 276 0.840 0.362 -- 1.000 0.269
Stanford CoreNLP 626 0.726 0.621 -- 0.945 0.701
SentiStrength-SE 704 0.948 0.883 -- 0.996 0.704
Stanford CoreNLP SO 333 0.635 0.252 -- 0.724 0.409
However, in this case the increment of precision has a price
topay: Stanford CoreNLP SO provideslevelsofrecalllowerthan
Stanford CoreNLP . The comparison between Stanford CoreNLP
andStanford CoreNLP SO should be read taking into account that
theoriginal Stanford CoreNLP modelistrainedonover10klabeled
sentences (i.e., >215k nodes). Stanford CoreNLP SO is trained on a
smaller training set. Thus, it is possible that a larger training set
couldimprovetheperformanceof Stanford CoreNLP SO .However,
as of now, this is a mere conjecture.
When looking at other tools, the analysis of the results reveal
that all the experimented tools achieve comparable results and—
more important—none of the experimented tools is able to reliably
assessthesentimentexpressedinaStackOverflowsentence.Indeed,
whileallthetoolsareabletoobtaingoodresultswhenpredicting
neutralsentences,theiraccuracyfallswhenworkingonpositiveand negative sentences. For example, even considering the toolhaving the highest recall for identifying positive sentences (i.e.,
SentiStrength ) (i) there is only 35.9% chance that it can correctly
spot a positive sentence and (ii) one out of five sentences that it
willlabelaspositivewillbeactuallyfalsepositives(precision=20%).
Therecallisalmostthesameasrandomlyguessingwhichhas33.3%
chance of success. These results reveal that there is still a long way
togobeforeresearchersandpractitioners canusestate-of-the-art
sentiment analysis tools to identify the sentiment expressed in
Stack Overflow discussions.
RQ1main findings: (i) the training of Stanford CoreNLP on
SO discussions does not provide a significant improvement as com-
paredtotheoriginalmodeltrainedonmoviereviews;(ii)thepre-
dictionaccuracyofalltoolsarebiasedtowardsthemajorityclass
(neutral)forwhichaverygoodprecisionandrecallisalmostalways
achieved; and (iii) all tools achieve similar performance and it is
impossibletoidentifyamongthemaclearwinneror,inanycase,
atoolensuringsufficientsentimentassessmentofsentencesfrom
Stack Overflow discussions.5.3.2 RQ 2: Do different software-related datasets impact the per-
formance of sentiment analysis tools? To answer RQ 2, we compare
the accuracy of all tools on the three datasets considered in ourstudy. When we look at results for app reviews, we can see that,
differentlyfromwhatobservedintheStackOverflowdataset,most
tools can predict positive texts with reasonable precision/recall
values. Even for negative reviews, the results are in general much
better.Itisworthnotingthat Stanford CoreNLP iscompetitivefor
identifyingpositive andnegativesentimentas comparedtoother
tools. Indeed, compared to other texts in software engineering
datasets, suchas StackOverflow discussions andJIRA issues,app
reviews can beless technical and relatively more similarto movie
reviews, with which the original model of Stanford CoreNLP is
trained.However,whenidentifyingneutralappreviews,alltools
exhibit poor accuracy. This is likely due to the fact that, while pos-
itive and negative app reviews could be easily identified by the
presence/absence of some “marker terms” (e.g., the presence of the
bugtermislikelyrelatedtonegativereviews),thisisnotthecase
for the neutral set of reviews, in which a wider and more variegate
vocabulary might be used.
When inspectingresults for JIRA issuecomments, we find that
SentiStrength andSentiStrength−SEhavebetteraccuracythan
othertools,with SentiStrength−SEprovidingabetterprecision-
recallbalanceacrossthetwocategoriesofsentiment(i.e., positive
and negative). Despite the mostly good results achieved by the
experimented toolson theJIRA dataset,there are someimportantissues in the evaluations performed on this dataset.
First,theabsenceofneutralsentencesdoesnotprovideaclear
and complete assessment of the accuracy of the tools. Indeed, as
shown in the app reviews, neutral texts might be, in some datasets,
themostdifficulttoidentify,likelyduetothefactthattheyrepresent
that “grey zone” close to both positive and negative sentiment.
Second,theJIRAdatasetisbuiltbymappingemotionsexpressed
inthecomments(e.g., joyorlove)intosentiments(e.g., positive).
101
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Sentiment Analysis for Software Engineering: How Far Can We Go? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 6: Confusion matrices on the Stack Overflow dataset.
SentiStrength
positive neutral negative
positive 47 66 18
neutral 173 919 99negative 15 86 77
NLTK
positive neutral negative
positive 32 96 3neutral 64 1121 6negative 5 158 15
Stanford CoreNLP
positive neutral negative
positive 45 30 56neutral 145 410 636
negative 5 24 149
SentiStrength-SE
positive neutral negative
positive 29 93 9
neutral 59 1108 24negative 5 140 33
Stanford CoreNLP SO
positive neutral negative
positive 19 96 16neutral 39 1055 97negative 2 111 65
However, such a mapping does not always hold. For instance, posi-
tive comments in issue tracker does not always express joy or love
(e.g.,thanksfortheupdatedpatch ),thusallowingtoobtainavery
partial view of the accuracy of sentiment analysis tools.
Tohighlighttheimportanceof neutralitemsintheevaluation
of a sentiment analysis tool, Table 6 shows the confusion matrices
obtained by the five different sentiment analysis tools on the Stack
Overflow dataset (see Table 4).
All tools are effective in discriminating between positive and
negativeitems.Forexample,our Stanford CoreNLP SO onlymis-
classifiedtwonegativesentencesaspositive,and16positivesen-
tences as negative. NLTKonly misclassifies five negative sentences
as positive, and three positive sentences as negative. The errors
aremostlyduetonegative/positivesentencesclassifiedasneutral
andviceversa.ThisconfirmstheissuesfoundbyTourani etal.[35]
whenusing SentiStrength onSEdata,andthisiswhyevaluating
sentiment analysis tools on datasets not containing neutral sen-
tencesintroducesaconsiderablebias.Similarobservationsholdfor
theappreviewsdataset,inwhichtheperformanceinclassifying
neutral reviews is, as shown in Table 5, extremely poor.RQ2main findings: Theaccuracyofsentimentanalysistools
is,ingeneral,pooronsoftwareengineeringdatasets.Weclaimthis
becausewefoundnotoolabletoreliablydiscriminatingbetween
positive/negative and neutral items. Indeed, while the accuracy
ontheappreviewsandJIRAdatasetsareacceptable(i)intheapp
reviews dataset the accuracy in identifying neutral items is verylow, and (ii) the data obtained with the JIRA dataset can not be
considered as reliable due to the discussed issues.
6 THREATS TO VALIDITY
Threats to construct validity concern therelation betweenthe-
ory and observation. The first concern is related to our manual
sentimentlabeling.Sentimentexpressedinthetextmightbemisin-
terpretedbypeople.Also,thelabelingmightbeimpactedbysub-
jective opinions of evaluators. Although we adopted an additional
conflict resolving process, it is not guaranteed that the manually
assigned sentiment is always correct.
Another threat is the sentiment score mapping, i.e.,mapping
five-scale sentiment to three-scale sentiment. Indeed, sentimentexpressed in the text have different degrees. Predicting slightlynegative sentence as neutral should be considered a smaller mis-take then predicting a very negative sentence as neutral, since
thethresholdtodrawalinebetweentheneutralandthenegative
sentiment can be more subjective.
Threats to internal validity concern internal factors we did
not consider that could affect the variables and the relations being
investigated.Inourstudy,theyaremainlyduetotheconfiguration
of sentiment analysis tools/approaches we used. In most cases, we
usethedefaultorsuggestedparameters,forexample,thethreshold
forNLTK. However, some parameters might be further tuned to
increase the sentiment prediction performance.
Threats to conclusion validity concerntherelationbetween
thetreatmentandtheoutcome.Werandomlyselectedsentences
from Stack Overflow discussions and app reviews from an existing
dataset [37]. While we considered statistically significant samples,
we cannot guarantee that our samples are representative of the
whole population.
Threats to external validity concern the generalizability of
our findings. While the evaluation has considered the most com-
monlyusedsentimentanalysistoolsinsoftwareengineering,some
lesspopulartoolsmighthavebeenignored.Constantlythereare
lots of new ideas and approaches popping up in the natural lan-guage processing domain, but few of them have been examined
and verified in the software engineering context. Since our goal is
to seek a good sentiment analysis tool for software-related texts,in this paper we only select the tools already used in previous
software engineering studies. Our datasets are limited to three fre-
quently mined software engineering repositories, while texts in
othercontexts,suchmailinglistandIRCchats,arenotconsidered.
7 LESSONS LEARNED
No tool is ready for real usage of identifying sentiment ex-pressed in SE related discussions yet.
No tool, including the
onesspecificallycustomizedforcertainsoftwareengineeringtasks,
is able to provide precision and recall levels sufficient to entail the
tooladoptionforatasksuchasrecommendingsoftwarelibraries.
102
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza and R. Oliveto
Byrelyingonsuchtools,wewouldcertainlygeneratewrongrec-
ommendations and miss good ones. Our results are a warning
to the research community: Sentiment analysis tools should al-
ways be carefully evaluated in the specific context of usage before
building something on top of them. For example, while Uddin and
Khomh [36] presented a very interesting approach to mine APIs
opinionsfrom StackOverflow,they donotreportthe accuracyof
the sentiment analysis component they exploit to identify posi-
tive/negative opinions about APIs.
Specific re-training is required, but does not represent a
silver bullet for improving the accuracy. Previous literature
haspointedourthatsentimentanalysistoolscannotbeusedout-
of-the-boxforsoftwareengineeringtasks[ 15,16,23,35].Insome
cases,toolshaveintroducedadatapreprocessingorare-trainingto
copewiththespecificsoftwareengineeringlexicon,inwhichthere
arepositiveornegativewords/sub-sentencesthatarenotpositiveornegativeinothercontexts,or viceversa (e.g.,theword buggenerally
carriesanegativesentimentwhenreferredtoalibrary,whileitcan
be consideredneutral in moviereviews). However,as results have
shown,thismightstillbeinsufficienttoguaranteegoodaccuracy
in terms of both precision and recall on all polarity levels. Also,
customizationisverydatasetspecific,andthereforeapplyingthe
tool on different datasets would require a new training. In other
words,customizingasentimentanalysistoolforJIRAdoesnotmake
itreadyforStackOverflowand viceversa.Finally,somealgorithms,
suchasrecursiveneuralnetworks,requirecostlyre-training.Inour
case, the training performed with 1,500 sentences (which turnedinto labeling almost 40k nodes) revealed to be insufficient for a
clear improvement of the Stanford CoreNLP accuracy.
Somesoftwareengineeringapplicationsmakesentiment
analysis easier than others. Sentiment analysis tools perform
betteronappreviews.Appreviewscontainsentencesthat,inmost
cases,clearlyexpresstheopinionofauser,whowantstoreward
an app or penalize it, by pointing out a nice feature or a serious
problem.Hence,thecontextisverysimilartowhatthosesentiment
toolsarefamiliarwith.Still,asobserved,thetools’performanceon
theneutralcategory is very poor. Looking at the issue tracker data,
besides the lack of neutral sentences in the JIRA dataset (which per
semakesthelifeofthesentimentanalysistoolsmucheasier),again
thepredominanceofproblem-reportingsentencesmay(slightly)
playinfavourofsuchtools.StackOverflowisadifferentbeast.Posts
mostly contain discussions on how to use a piece of technology,and between the lines somebody points out whether an API or a
codepatternisgoodorlessoptimal.Inmanycases,withouteven
expressing strong opinions. This definitely makes the applicability
of sentiment analysis much more difficult.
Shouldweexpect100%accuracyfromsentimentanalysis
tools?No, we should not. In our manual evaluation, out of the
1,500 Stack Overflow sentences we manually labeled, there were
279casesofdisagreement(18.6%).Thismeansthatevenhumansarenotabletoagreeaboutthesentimentexpressedinagivensentence.
ThisisalsoinlinewithfindingsofMurgia etal.[21]onemotion
mining:Exceptwhenasentenceexpressesclearemotionsoflove,
joy and sadness, even for humans it is hard to agree. Hence, it ishard to expect that an automated tool can do any better. Having
saidthat,advancesarestillneededtomakesentimentanalysistools
usable in the software engineering domain.Textreportingpositiveandnegativesentimentisnotsuf-
ficient to evaluate sentiment analysis tools. Asdiscussed, the
mostdifficulttaskforsentimentanalysistoolsistodiscriminatebe-
tween positive/negative vsneutral sentiment, while they are quite
effectiveindiscriminatingbetweenpositiveandnegativesentiment.
ThisiswhydatasetssuchastheJIRAonethatwe,andothers,used
in previous work [ 15,16], is not sufficient to evaluate sentiment
analysis tools. We hope that releasing our dataset [ 18] will help in
more robust evaluations of sentiment analysis tools.
8 CONCLUSION
Somesaythattheroadtohellispavedwithgoodintentions.Our
work started out with what we consider a promising idea: We
wanted to develop an approach to automatically recommend APIs
andlibrariesgivenasetoffunctionalandnon-functionalrequire-
ments.Todoso,wewantedtoleveragethelargebodyofknowledge
thatisstoredinQ&AwebsiteslikeStackOverflow.Theapproach
was going toexploit opinion miningusing deep learning through
recurrentneuralnetwork.However,aswefinalizedourworkwe
noticed that it simply did not work, because the opinion mining
component had unacceptable performance.
The reason for the failure is manifold. Firstly, it highlights how
machine learning, even in its most advanced forms, is and remains
a black box, and it is not completely clear what happens in thatblack box. To this one can add the design principle “garbage in,
garbageout ”:Nomatterhow advancedatechnique,iftheinputis
notappropriate,itis improbablethatanacceptableoutputcanbe
produced. In the specific case one might argue that Stack Overflow
is not really the place where emotions run high: It is a place where
developersdiscusstechnicalities.Thereforeitisratherobviousthat
opinion mining will have a hard time. While this might be true,
ourstudyrevealedthatalsoindatasetswhereemotionsaremore
evident, like app reviews and issue trackers, there is an intrinsicproblem with the accuracy of current state-of-the-art sentiment
analysis tools.
Intheendwedecidedto writea“negativeresults”paper .A s
Walter Tichy writes, “Negative results, if trustworthy, are extremely
important for narrowing down the search space. They eliminate use-
lesshypothesesandthusreorientands peedupthesear chforbetter
approaches ”.Wehopethatthesoftwareengineeringcommunitycan
appreciateandleveragetheinsightsthatweobtainedduringour
work.Wearealsoreleasingthecompletedatasetasareplication
package. As a final word, we would like to stress that we are not
dismissing opinion mining in software engineering as impractical,
but rather as not mature enough yet . We believe there is promise in
thefield,butthatacommunityeffortisrequiredtobringopinion
mining to a level where it actually becomes useful and usable in
practice.
ACKNOWLEDGMENTS
WegratefullyacknowledgethefinancialsupportoftheSwissNa-
tional Science Foundation for the projects PROBE (SNF Project
No. 172799) and JITRA (SNF Project No. 172479), and CHOOSE for
sponsoring our trip to the conference.
103
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. Sentiment Analysis for Software Engineering: How Far Can We Go? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]StefanoBaccianella,AndreaEsuli,andFabrizioSebastiani.2010. SentiWordNet
3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.
InProceedings of LREC 2010 (International Conference on Language Resources and
Evaluation).
[2]FabioCalefato,FilippoLanubile,andNicoleNovielli.2017. EmoTxt:AToolkit
for Emotion Recognition from Text. In Proceedings of ACII 2017 (7th International
Conference on Affective Computing and Intelligent Interaction).
[3]L. V. G. Carreño and K. Winbladh. 2013. Analysis of user comments: an ap-
proach for software requirements evolution. In Proceedings of ICSE 2013 (35th
International Conference on Software Engineering). IEEE press, 582–591.
[4]Daviti Gachechiladze, Filippo Lanubile, Nicole Novielli, and Alexander Sere-
brenik.2017. AngerandItsDirectioninCollaborativeSoftwareDevelopment.
InProceedings of ICSE 2017 (39th IEEE/ACM International Conference on Software
Engineering). IEEE, 11–14.
[5]David Garcia, Marcelo Serrano Zanetti, and Frank Schweitzer. 2013. The Role of
Emotions in Contributors Activity: A Case Study on the GENTOO Community.
InProceedings of CGC 2013 (3rd International Conference on Cloud and Green
Computing) (CGC ’13). 410–417.
[6]Michael Goul, Olivera Marjanovic, Susan Baxley, and Karen Vizecky. 2012. Man-
agingtheEnterpriseBusinessIntelligenceAppStore:SentimentAnalysisSup-
ported Requirements Engineering. In Proceedings of HICSS 2012 (45th Hawaii
International Conference on System Sciences). 4168–4177.
[7]EmitzaGuzman,RanaAlkadhi,andNorbertSeyff.2017. Anexploratorystudyof
Twittermessagesaboutsoftwareapplications. RequirementsEngineering 22,3
(2017), 387–412.
[8] Emitza Guzman, Omar Aly, and Bernd Bruegge. 2015. Retrieving Diverse Opin-
ionsfromAppReviews.In ProceedingsofESEM2015(9thACM/IEEEInternational
Symposium on Empirical Software Engineering and Measurement). IEEE, 21–30.
[9]Emitza Guzman, David Azócar, and Yang Li. 2014. Sentiment analysis of commit
commentsinGitHub:anempiricalstudy.In ProceedingsofMSR2014(11thWorking
Conference on Mining Software Repositories). ACM, 352–355.
[10]Emitza Guzman and Bernd Bruegge. 2013. Towards emotional awareness in
software development teams. In Proceedings of ESEC/FSE 2013 (9th Joint Meeting
on Foundations of Software Engineering). ACM, 671–674.
[11]Emitza Guzman and Walid Maalej. 2014. How do users like this feature? a
finegrainedsentimentanalysisofappreviews.In ProceedingsofRE2014(22nd
International Requirements Engineering Conference). IEEE, 153–162.
[12]Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann,
andIanH.Witten.2009. TheWEKADataMiningSoftware:AnUpdate. SIGKDD
Explorations 11, 1 (2009), 10–18.
[13]Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In
ProceedingsofKDD2004(10thACMSIGKDDinternationalconferenceonKnowledge
discovery and data mining). 168–177.
[14]Clayton J Hutto and Eric Gilbert. [n. d.]. In Proceedings of ICWSM 2014 (8th
International AAAI Conference on Weblogs and Social Media.
[15]Md RakibulIslam andMinhaz FZibran. 2017. Leveragingautomated sentiment
analysis in software engineering.In Proceedings of MSR 2017(14th International
Conference on Mining Software Repositories). IEEE Press, 203–214.
[16] RobbertJongeling,ProshantaSarkar,SubhajitDatta,andAlexanderSerebrenik.
2017. On negative results when using sentiment analysis tools for software
engineering research. Empirical Software Engineering (2017), 1–42.
[17]FranciscoJuradoandPilarRodriguez.2015. SentimentAnalysisinmonitoring
software development processes: An exploratory case study on GitHub’s project
issues.Journal of Systems and Software 104 (2015), 82–89.
[18]Bin Lin, Fiorella Zampetti, Gabriele Bavota, Massimiliano Di Penta, Michele
Lanza, and Rocco Oliveto. [n. d.]. Replication Package. https://sentiment-se.
github.io/replication.zip. ([n. d.]).
[19]Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J.
Bethard,andDavidMcClosky.2014. TheStanfordCoreNLPNaturalLanguage
Processing Toolkit. In Association for Computational Linguistics (ACL) System
Demonstrations. 55–60.
[20]GeorgeA.Miller.1995. WordNet:ALexicalDatabaseforEnglish. Commun.ACM
38, 11 (1995), 39–41.[21]Alessandro Murgia, Parastou Tourani, Bram Adams, and Marco Ortu. 2014. Do
developersfeelemotions?anexploratoryanalysisofemotionsinsoftwarearti-
facts.InProceedingsofMSR2014(11thWorkingConferenceonMiningSoftware
Repositories). ACM, 262–271.
[22]Nicole Novielli, Fabio Calefato, and Filippo Lanubile. 2014. Towards discovering
theroleofemotionsinstackoverflow.In ProceedingsofSSE2014(6thInternational
Workshop on Social Software Engineering). ACM, 33–36.
[23]Nicole Novielli, Fabio Calefato, and Filippo Lanubile. 2015. The Challenges of
Sentiment Detection in the Social Programmer Ecosystem. In Proceedings of
SSE 2015 (7th International Workshop on Social Software Engineering) (SSE 2015).
33–40.
[24]MarcoOrtu,BramAdams,GiuseppeDestefanis,ParastouTourani,MicheleMarch-
esi, and Roberto Tonelli. 2015. Are bullies more productive?: empirical study
ofaffectivenessvs.issuefixingtime.In ProceedingsofMSR2015(12thWorking
Conference on Mining Software Repositories). IEEE Press, 303–313.
[25]MarcoOrtu,AlessandroMurgia,GiuseppeDestefanis,ParastouTourani,Roberto
Tonelli,MicheleMarchesi,andBramAdams.2016. Theemotionalsideofsoftware
developers in JIRA. In Proceedings of MSR 2016 (13th International Conference on
Mining Software Repositories). IEEE, 480–483.
[26]Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Founda-
tions and Trends in Information Retrieval 2 (2008), 1–135.
[27]SebastianoPanichella,AndreaDiSorbo,EmitzaGuzman,CorradoAVisaggio,
Gerardo Canfora, and Harald C Gall. 2015. How Can I Improve My App? Classi-
fyingUserReviewsforSoftwareMaintenanceandEvolution.In Proceedingsof
ICSME2015(31stInternationalConferenceonSoftwareMaintenanceandEvolution)
(ICSME 2015). 281–290.
[28]DanielPletea,BogdanVasilescu,andAlexanderSerebrenik.2014. Securityand
emotion:sentimentanalysisofsecuritydiscussionsonGitHub.In Proceedings
of MSR 2014 (11th Working Conference on Mining Software Repositories). ACM,
348–351.
[29]Mohammad Masudur Rahman, Chanchal K Roy, and Iman Keivanloo. 2015. Rec-
ommendinginsightfulcommentsforsourcecodeusingcrowdsourcedknowledge.
InProceedings of SCAM 2015 (15th International Working Conference on Source
Code Analysis and Manipulation). IEEE, 81–90.
[30]Athanasios-Ilias Rousinopoulos, Gregorio Robles, and Jesús M González-
Barahona.2014. SentimentanalysisofFree/OpenSourcedevelopers:preliminaryfindingsfromacasestudy.In RevistaEletronicadeSistemasdeInformacao,Vol.13.
1–6.
[31]Vinayak Sinha, Alina Lazar, and Bonita Sharif. 2016. Analyzing developer senti-
mentincommitlogs.In ProceedingsofMSR2016(13thInternationalConference
on Mining Software Repositories). ACM, 520–523.
[32]Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Man-
ning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for
semanticcompositionalityoverasentimenttreebank.In InProceedingsofEMNLP
2013 (2013 Conference on Empirical Methods in Natural Language Processing).
Citeseer.
[33]Rodrigo Souza and Bruno Silva. 2017. Sentiment analysis of Travis CI builds.
InProceedings of MSR 2017 (14th International Conference on Mining Software
Repositories). IEEE Press, 459–462.
[34]Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas.
2010. Sentiment strength detection in short informal text. Journal of the Associa-
tion for Information Science and Technology 61, 12 (2010), 2544–2558.
[35]ParastouTourani,YujuanJiang,andBramAdams.2014. Monitoringsentiment
in open source mailing lists: exploratory study on the apache ecosystem. In
Proceedings of CASCON 2014 (24th Annual International Conference on Computer
Science and Software Engineering). IBM Corp., 34–44.
[36]GiasUddinandFoutseKhomh.2017. MiningAPIAspectsinAPIReviews.Technical
Report.10pages. http://swat.polymtl.ca/data/opinionvalue-technical-report.pdf
[37]LorenzoVillarroel,GabrieleBavota,BarbaraRusso,RoccoOliveto,andMassimil-
ianoDi Penta.2016. Releaseplanning ofmobile appsbased onuser reviews.In
Proceedings of ICSE 2016 (38th International Conference on Software Engineering) .
14–24.
[38]YingyingZhangandDaqingHou.2013.ExtractingProblematicAPIFeaturesfrom
Forum Discussions. In Proceedings of ICPC 2013 (21st International Conference on
Program Comprehension. 141–151.
104
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:50:07 UTC from IEEE Xplore.  Restrictions apply. 