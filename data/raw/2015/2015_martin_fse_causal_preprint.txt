Causal Impact Analysis for App Releases in Google Play
William Martin, Federica Sarro and Mark Harman
University College London, London, United Kingdom
{w.martin, f.sarro, mark.harman}@ucl.ac.uk
ABSTRACT
App developers would like to understand the impact of their
own and their competitors’ software releases. To address
this we introduce Causal Impact Release Analysis for app
stores, and our tool, CIRA, that implements this analysis.
We mined 38,858 popular Google Play apps, over a period
of 12 months. For these apps, we identiﬁed 26,339 releases
for which there was adequate prior and posterior time se-
ries data to facilitate causal impact analysis. We found that
33% of these releases caused a statistically signiﬁcant chan ge
in user ratings. We use our approach to reveal important
characteristics that distinguish causal signiﬁcance in Goog le
Play. To explore the actionability of causal impact analy-
sis, we elicited the opinions of app developers: 56 compa-
nies responded, 78% concurred with the causal assessment,
of which 33% claimed that their company would consider
changing its app release strategy as a result of our ﬁndings.
CCS Concepts
•Software and its engineering →Software creation
and management;
Keywords
App Store Mining and Analysis, Causal Impact
1. INTRODUCTION
Rapid release strategies can oﬀer signiﬁcant beneﬁts to
both developers and end users [18], but high code churn
in releases can correlate with decreased ratings [14]. Re-
leases occur for a number of reasons [1], such as updating
libraries [15], or stimulating downloads and ratings [9], in
addition to more traditional bug ﬁxes, feature additions and
improvements. McIlroy et al. [27] studied update frequen-
cies in Google Play, ﬁnding that 14% of their studied apps
were updated in a two-week period. Nayebi et al. [29] found
that half of surveyed developers had a clear release strategy,
and experienced developers believed it aﬀects user feedback.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
FSE’16, November 13-19, 2016, Seattle, WA, USA
c/circlecopyrt2016 ACM. ISBN 978-1-4503-4218-6/16/11. . . $15.00
DOI:http://dx.doi.org/10.1145/2950290.2950320In this paper we introduce an approach to causal impact
analysis to help app developers understand the impact of
their releases. We mine and record time-series information
about apps, in order to identify how releases can aﬀect an
app’s rating and rating frequency. Causal Impact Analy-
sis [6] is a form of causal inference [17, 21], which we use in
this paper to identify sets of app releases that have caused a
statistically signiﬁcant ( p≤0.01) change in subsequent user
rating or rating frequency. Causal inference has previously
been used primarily in economic forecasting, but it has also
seen use for software defect prediction [10, 11, 44]. We in-
troduce our “Causal Impact Release Analysis” tool, CIRA,
which runs causal impact analysis for app store datasets.
We follow up on the causal impact analysis with more tra-
ditionally familiar (frequentist) inferential statistical ana ly-
sistofurtherinvestigatetheprobabilisticevidenceforpoten -
tial causes. We investigate the inﬂuence of properties that
are under developers’ control such as price and release text,
on both the positive and negative statistically signiﬁcant re-
leases. We use information retrieval to investigate the top
terms and topics that occur in the release text of statisticall y
signiﬁcant releases, and also investigate the overall eﬀects of
release frequency on user rating behaviour. Furthermore, we
contacted the developers of releases that were identiﬁed by
CIRAas signiﬁcant, to ask whether causal impact analysis
is useful to them. To facilitate our study, we mined data,
weekly, from Google Play between February 2015 and 2016.
Our contributions are as follows:
1.We introduce our tool, CIRA, for performing causal im-
pact analysis on app store data1.
2.We contacted developers of signiﬁcant releases: 52 de-
velopers responded, 78% of whom agree with CIRA’s assess-
ment, of which 33% claimed that their company would con-
sider changing their release strategy.
3.We study Google Play app releases using CIRA, ﬁnding:
3.1.Paid app releases have a greater chance of aﬀect-
ing subsequent user ratings (a chance of 40% for paid apps,
compared with 31% for free apps).
3.2.Paid apps with releases that had signiﬁcant positive
eﬀects have higher prices.
3.3.Free apps with signiﬁcant releases have a greater
chance for their eﬀects to be positive (37% for paid apps
compared with 59% for free apps).
3.4.Releases that positively aﬀected user ratings had
more mentions of bug ﬁxes and new features.
3.5.Releases that aﬀected subsequent user ratings were
more descriptive of changes.
1Availableathttp://www0.cs.ucl.ac.uk/staﬀ/w.martin/ci ra2. DEFINITIONS
In this section we deﬁne the app metrics recorded and the
terminology used to refer to our data throughout the study.
2.1 Success Metrics
In order to assess app success, the following app-level met-
rics are used2:
(R) Rating : The average of user ratings made for the app
since its ﬁrst release on the store.
(N) Number of ratings : The total number of ratings that
the app has received.
(NW) Number of ratings per week : The number of rat-
ings that the app has received since the previous snapshot,
which is taken a week earlier.
2.2 Developer-controlled Properties
The following observable app store properties are under
developers’ control:
(P) Price : The amount a user pays for the app (in GBP) in
order to download it. This value does not take into account
in-app-purchases and subscription fees, thus it is the ‘up
front’ price of the app.
(L) Description length : The length in characters of the
description, after ﬁrst processing the text as detailed in Sec-
tion 5.3, and further removing whitespace in order to count
meaningful characters only.
(RT) Release text : The app’s description andwhat’s new
sections from its app store page, in each case included only
when they have changed from the app’s previous release. An
app’sdescription andwhat’s new sections can change at any
point in time, but changes typically coincide with releases.
Version identiﬁer : We determine a ‘release’ to have oc-
curred if and only if the version identiﬁer changes.
2.3 Data
We mine app data from Google Play between February
2015 and February 2016, as detailed in Section 5.1.
Full set : All apps mined in the time period. Some apps
drop out of the store (for unknown reasons), but they are
included in the full set for the duration in which they appear
in the store. This full set consists of 38,858 apps.
Control set : The set of apps that have no new releases over
the studied time period. We refer to this as the control set,
as it is the benchmark by which we can measure changes in
the releasing apps. Apps that drop out of the store are not
included in the control set because consistency is required
for a reliable control set. This control set consists of 680
apps.
Target set : Thesetofappreleasesthatoccurinthestudied
time period and occur at least 3 weeks after the previous
releases, and at least 3 weeks before the next release. The
target releases have some longevity which suggests they are
more than ‘hotﬁxes’ for recently introduced bugs. They also
have a non-trivial window of data on either side so that we
can observe any eﬀect the release may have had on the app’s
success. This ensures suﬃcient data availability to perform
causal impact analysis. Apps that drop out of the store
are included in the target set if they include adequate prior
and posterior information as deﬁned above. This target set
consists of 14,592 apps and 26,339 releases.
2Number of downloads can be used as a metric, however
most app stores (including the one analysed in this work)
do not make this information publicly available.
1. Train local variance
4. Local-only prediction2. Train spike coefficients
 3. Train slab coefficients
5. Counterfactual predictionFigure 1: CIRA workﬂow showing the local and
global variance components. The resultant counter-
factual predicition is compared with the observed
vector after the release in order to compute the
probability that the observed vector would have
been predicted. If this probability is low ( ≤0.01)
it indicates that there was a signiﬁcant impact at
the time of the release.
3. CAUSAL IMPACT RELEASE ANALYSIS:
CIRA
Causal inference is a method used to determine the causal
signiﬁcance of an event or events, by evaluating the post-
event changes using observational data. A traditional ap-
proach to causal inference is diﬀerences-in-diﬀerences analy-
sis [4], which compares the diﬀerences between the prior and
posterior vectors (i.e. observations) of a group (e.g. mobile
apps in a store) that receives a given intervention (which in
our case is a release), against a group that does not receive
this intervention (i.e. control set). Conversely, the Causal
Impact Analysis method [6], based on state-space models,
treats each vector separately, in each case using the full con-
trol set to provide global variance. Multiple apps typically
do not receive the same intervention (release) at the same
time, which makes the former method diﬃcult to apply in
our case. This motivates our use of Causal Impact Analysis,
which we apply to app releases using our tool, CIRA.
CIRAtrains a Bayesian structural time-series model [17,
39] on the data vector for each target release, using a set
of unaﬀected data vectors known as the control set (deﬁned
in Section 2.3). This enables the model to make a prediction
of the data vector in the posterior time period accounting
for local and global variations. Each metric (R, N, NW)
and each release, requires an individual experiment, as the
method works each time on a single data vector.
Fig. 1 shows the overall CIRAworkﬂow:
1.Train the local trend parameters using the deviation of
the observed vector in the prior time period. This is used to
sample changes and compute the conﬁdence interval.
2.Compute the ‘spike’ [39] for the observed vector from the
set of controls, and assign coeﬃcients for them, in order to
help us make an accurate prediction.
3.Use the rest of the control set as the ‘slab’ [39], assigning
equal coeﬃcients for them, in order to account for global
variations in the dataset.
4.Make a set of predictions using the local trend trained
earlier, sampling for changes and noise.
5.Combine local-only prediction with control changes mut-
liplied by their coeﬃcients. This is the counterfactual pre-
diction [6].WeekRatingFigure 2: Causal impact analysis for the rating of an
invoice management application in Google Play. Af-
ter the target release (shaded vertical bar), the ob-
served vector (solid line plotted throughout) clearly
moves outside the conﬁdence interval (shaded re-
gion) and deviates signiﬁcantly from the counter-
factual prediction (solid line inside shaded region).
We then compute the cumulative pointwise diﬀerence be-
tween the observed vector and our prediction, normalised
to the length of time at which each deviation occurs, and
compute the cumulative probability from our local trend pa-
rameters. A low value ( p≤0.01) indicates that the success
metric changed signiﬁcantly; see for example Fig. 2, which
shows that the rating of an invoice management applica-
tion deviates signiﬁcantly from the predicted vector. By
setting our threshold to be 0 .01, we have a 0 .01 probability
of claiming a signiﬁcant change where one does not exist,
and therefore expect roughly 1% false positive rate.
We use CIRA to identify the set of releases (see RQ3), for
which there is evidence of a statistically signiﬁcant eﬀect o n
the metrics we collect. Our subsequent inferential statistical
analysis complements this, pointing to the set of potential
properties which may play a role in this causal signiﬁcance.
Of course, the degree to which we can assume causality relies
on the strength of our causal assumptions, that the control
set is unaﬀected by the release, and the relationship of the
control to the released app is unchanged.
Inanyandallcausalimpactanalyses, itisofcourseimpos-
sible to identify external properties that might have played
a role in the changes observed. In the case of app stores,
our current analysis cannot, for example, identify adver-
tising campaigns, timed to coincide with the new release.
However, if such an external factor does have a signiﬁcant
eﬀect, then our causal impact analysis may detect it. We
ask developers in RQ5 if they are aware of external factors
which may have caused the observed changes, in order to
determine how often this may be the case.
4. RESEARCH QUESTIONS
This section explains the questions posed in our study and
how we approach answering them.
RQ1: Do app metrics change over time?
Before performing a detailed analysis of the changes over
time, we ﬁrst set a baseline by establishing whether app suc-
cessmetricschangeovertimeorbetweendiﬀerentsnapshots.
Using the metrics R, N, NW and L as deﬁned in Section 2,
we compute their standard deviation over 52 weeks.Thesedistributionsaredrawnusingboxplots, whichenables
us to establish whether metrics change over time, and to
what extent any such changes occur.
We determine whether app success changes more or less
for those apps that have releases, by comparing plots for
those apps that have no releases (the control set), with re-
leasing apps (the target set). If the metrics for the target
set change over time more than those in the control set,
this motivates further analysis of releases that could cause
the observed changes. It is expected that releasing apps
would show greater deviation in description length, describ-
ing changes and features.
RQ2: Do release statistics have a correlation with
app success?
We build on this analysis by measuring whether app success
is correlated with the number of app releases in the time
period studied, and whether it is correlated with the time
interval between releases. This will show whether a large (or
conversely, small) number of releases might lead to increased
success, and likewise for release interval.
For both of these experiments, only apps in the target
dataset are used. We perform correlation analysis between
the number of releases of each app and their value for the
metrics R and N at the end of the time period. We do not
use NW because this number is set on a per-week basis, but
instead use the change in R and N from the ﬁrst snapshot to
the last, denoted ∆R and ∆N respectively. Additionally we
do not use L because this does not represent app success.
RQ2.1: Does the number of releases have a high
correlation with app success? We perform correlation
analysis between the number of releases in the studied time
period and the metrics R, ∆R, N and ∆N at the end of the
time period.
RQ2.2: Does the median time interval between
releases have a correlation with app success? We
performcorrelationanalysisbetweenthemedianintervalbe-
tween releases of each app and the metrics used in RQ2.1.
RQ3: Do releases impact app success?
There are two limitations to correlation analysis. Firstly,
correlationanalysisseeksanoverallstatisticaleﬀectinwhi ch
a large number of apps and their releases participate. How-
ever, it may also be interesting, from the developers’ point
of view, to identify speciﬁcreleases after which an atypi-
cally high change occurs in subsequent success, accounting
for ‘background’ app store behaviour; general correlation
analysis is not well-suited to this more speciﬁc question.
Secondly, of course, as is well-known, any correlation ob-
served does not necessarily imply the presence of a cause
(correlation is not causation). Therefore, even were we to
ﬁnd strong correlations, this would not, in itself, help us
to identify causes. This motivates our use of causal impact
analysis. We apply causal impact analysis on each target
release to see whether it caused a statistically signiﬁcant
change in any of the metrics R, N or NW, deﬁned in Sec-
tion 2. Causal impact analysis is described in Section 3.
RQ3.1: What proportion of releases impact app
success? Wecomputetheproportionofappswhosereleases
have aﬀected success, and the proportion of overall releases.
We group results under the metrics aﬀected: R, N, NW and
the intersection of R and NW, overall and for positive or
negative changes.RQ3.2: How does the causal control set size aﬀect
results? Causal impact analysis uses a control set: a set
of unaﬀected data vectors, which in our case is the set of
apps that have zero releases in the period studied. As the
set is used in two diﬀerent ways (‘spike’ and ‘slab’ [39], as
explained in Section 3), we test whether the set size makes a
diﬀerence that could inﬂuence our results. Our approach is
similar to the experiment using diﬀerent control sets in the
study by Brodersen et al. [6]. We compute the causal impact
analysis results for each metric with the full target dataset,
using repeatedly halved control sets of size 340, 170, 85, 42,
21, 10, 5, and 3 which are each randomly sampled from the
maximal set of 680 non-releasing apps.
We compute the agreement between each set of results
and the results used to answer RQ3.1. Agreement is de-
ﬁned as/parenleftbigY Y+NN
total/parenrightbig
, where YY indicates a signiﬁcant change
as detected on both datasets, NN indicates no signiﬁcant
change detected on both datasets, and total is a count of
all results (including disagreements). We also compute the
Cohen’s Kappa [8], which takes into account the chance for
random agreement and is bounded above by the agreement.
We computea secondsetofresults using680controlvectors,
which will show the expected diﬀerence between consecutive
runs with the same control set, since there is a random com-
ponent in the predictive model.
RQ4: What characterises impactful releases?
We use the causal impact analysis results from RQ3 to ana-
lyse signiﬁcant and non-signiﬁcant releases.
RQ4.1: What are the most prevalent terms in re-
leases? We pre-process the release text from all releases in
a given store as described in Section 5.3, then identify the
‘top terms’ (most prevalent) for each set of releases using
two methods: TF.IDF [22] and Topic Modelling [5].
Wetrainbothmethodsonthereleasetextcorpus, treating
each instance of release text as a document. For both meth-
ods, we sum the resultant scores (probabilities) for terms
(topics) over each set of releases. We restrict ourselves to
only the top three terms to avoid over specialisation.
The topic model is trained using 100 topics. We chose
100 topics for three reasons: i) the number of documents
in the corpus (updated release text from target releases),
each consisting of tens to hundreds of words, which is a
large sized corpus; ii) we do not wish to over-generalise, nor
over-ﬁt; 100 topics will allow diversity without assigning t he
same topic to every document with a release; iii) 100 topics
is a common selection for non-trivial datasets, serving as the
default setting in GibbsLDA++ [35] andJGibbLDA [36].
Intuitively, the choice of 100 topics allows for diversity in t he
trained topics, without unduly elevating the risk of training
a topic that is overly speciﬁc to an app or release.
RQ4.2: How often do top terms and topics occur
in each set of releases? Wecomputethecountsineachset
of releases that contain top terms that emerge from TF.IDF
and topic modelling, as performed in RQ4.1. We apply a
bag-of-words model, which ignores the ordering of the words
in each document. This eliminates the need to check for
multiple forms of text that discuss the same topic.
RQ4.3: What are the eﬀects of each of the candi-
date causes? We select the sets of statistically signiﬁcant
and non-signiﬁcant releases, as well as the sets of signiﬁcan t
releases that increased and decreased rating, and compare
the distributions of several developer-controlled properties.This will help us to establish potential causes that may have
led to the releases being signiﬁcant, or positively aﬀecting
an important success metric for developers: the rating.
We consider properties of the release that lie within the
control of the developer: (P)rice, (RT size) the size of release
text (deﬁned in Section 2.2) in words, and (RT change) the
change in RT sizeon the week of release. For each of these
properties, we use the Wilcoxon test and Vargha and De-
laney’sˆA12eﬀect size comparison [41], to identify statisti-
cally signiﬁcant diﬀerences between causally signiﬁcant re-
leases and non-signiﬁcant releases. We use the untrans-
formed Vargha and Delaney’s comparison [31] because we
are only interested in the raw probability value it produces.
In case a statistical diﬀerence is found for a given aspect, we
use box plots to understand if this property may play a role
in the changes observed (i.e. it may be a candidate cause).
At this point we will have identiﬁed a subset of releases
that exhibit statistically signiﬁcant success eﬀects, and we
will have identiﬁed further diﬀerences between sets of signif-
icant and non-signiﬁcant releases, as well as the diﬀerences
between positive and negative signiﬁcant releases. To fur-
ther explore the actionability potential of the tool and our
ﬁndings, we ask the following research question.
RQ5: Is causal impact analysis useful to developers?
Because there is no ground truth, only diﬀerent implications
of any discovered statistical signiﬁcance, we can only answe r
this question semi-quantitatively. To determine whether our
results are useful we simply ask the developers of causally
signiﬁcantreleases, asdeterminedbyourtool, CIRA,whether
they agree with the classiﬁcation. We email developers via
the email addresses contained on their app store pages, in-
formingthemofthesigniﬁcantreleaseandproposingtosend
a report detailing the tool’s ﬁndings. We expect a large pro-
portion of these emails may fail to reach a human, and can
only conﬁrm contact is established if we hear back from a
developer. Once contact is established with the app’s devel -
opers, we ask them the following questions3:
Agree with detected signiﬁcance : we ask if the develop-
ers of the app agree with CIRA’s assessment that the release
was signiﬁcant.
External cause of changes : we ask if the company is
aware of an external event which may have caused the de-
tected signiﬁcant change, such as advertising campaigns.
Would change strategy : we ask if the company would
consider changing their release strategy based on ﬁndings.
Receiving further reports : we ask if the company is in-
terested in receiving further reports for their app releases.
Learning contributing factors : we ask if the company
is interested in learning more about the characteristics of
signiﬁcant releases, from the results of our study.
5. METHODOLOGY
This section describes the methods used in our study for
extracting and analysing app store data.
5.1 Data Mining
We gathered a set of 38,858 apps from the Google Play
store from February 2015 to February 2016, which had ap-
peared in a Google Play free, paid, or any category’s ‘top
540’ list at least once in the year before February 2015.
3Questionnaire available on the accompanying web page.We used this list because it includes apps that are likely
to have been downloaded and reviewed, and so have mea-
surable success, yet enables a large dataset that is likely to
consist of many app releases.
Only 1% of apps release software updates more frequently
than once per week, and these have already been studied in
detail by McIlroy et al. [27]. For this reason, we set the time
interval between between data collection to one week.
Some apps dropped out, were removed or deleted from the
store, or had other technical issues that prevented successful
data mining on a particular week. Gaps in the data such as
this are to be expected, and do not prevent causal impact
analysis from running.
We extract app metrics from each of the 38,858 apps in-
cluding price, rating, number of ratings, description, what’s
newand version. Google Play reports rounded app ratings
(rounded to 1 decimal place), thereby creating a potential
source of imprecision, which we would like to overcome. We
therefore calculate the (more precise) Google Play average
ratings, using the extracted numbers of ratings in each of
the ﬁve star rating categories (from 1-5).
5.2 Explanation of the Frequentist Inferential
Statistical Analysis Techniques Used
This subsection explains the role played, in our overall
analysis, bytraditionalfrequentistinferentialstatisticalt ech-
niques (with which software engineers are most likely to be
already familiar), while Section 3 explains causal impact
analysis (which is comparatively less widely used in the do-
main of software engineering).
We use correlation analysis in order to understand cor-
relations between the observed app metrics and both the
quantity and interval of their releases. However, since it is
well known that ‘correlation does not imply causation’, we
further investigate the causal eﬀect of releases using causal
impact analysis (explained in more detail in Section 3). We
follow up the causal impact analysis with a nonparametric
inferential statistical analysis, to provide further evidence
as to the relative likelihoods that each of the developer-
controlled app properties that we observed plays a role in
the eﬀects detected by causal impact analysis.
In RQ2 and RQ4 we use Pearson and Spearman statistical
correlation tests. Pearson introduced the measurement of
linear correlation [34], and Spearman subsequently extended
Pearson’s work to include rank-based correlation [40]. Each
correlation metric reports a rho value and a pvalue. The p
value denotes the probability that a rho value is diﬀerent to
zero (no correlation). A rho value of 1 indicates perfect cor-
relation, while -1 indicates perfect inverse correlation, and
0 indicates no correlation. Values between 0 and 1 (-1) in-
dicate the degree of correlation (inverse correlation, respec-
tively) present.
In RQ3 and RQ4 we also compare distributions using a
two-tailed unpaired non-parametric Wilcoxon test [42], that
tests the Null-hypothesis that the result sets are sampled
from the same distribution. We also compare the result
sets using Vargha and Delaney’s ˆA12eﬀect size comparison
test [41], which results in a value between 0 and 1, indicating
the likelihood that one measure will yield a greater value
than the other.
In our case, we apply these inferential statistical tests to
examine diﬀerences in properties between sets of releases.We compare the set of releases that have caused a signiﬁcant
change (according to the previous causal impact analysis) to
those which have not, and compare the set of releases that
positively aﬀected rating to those that negatively aﬀected
rating. The pvalue is the probability that we would observe
thediﬀerenceinmedianvaluesweﬁnd, oronemoreextreme,
given that there is, in fact, no diﬀerence in the two distribu-
tions from which the releases are drawn. It is a conditional
probability, usually used to reject the Null-hypothesis (that
there is no diﬀerence).
However, in our case, a prior causal impact analysis has
revealed that there is a signiﬁcant causal diﬀerence between
the two sets, yet it remains unknown what this cause is.
Causal impact analysis does not fully overcome the prob-
lem that ‘correlation is not causation’, but it does provide
evidence for causal signiﬁcance (in our case causal eﬀect of
a release on subsequent success). Therefore, each pvalue
from our subsequent Wilcoxon tests can be interpreted as an
indication of a potential cause for the diﬀerence observed,
which, if low ( p≤0.01) warrants further investigation of the
associated property.
Sincewearecomputingmultiple pvalues, thereadermight
expect some kind of correction, such as a Bonferroni or
Benjamini-Hochberg [3] correction for multiple statistical
testing at the (traditionally popular) 0.05 probability level
(corresponding to the 95% conﬁdence interval). However,
since we are notusingpvalues to test for signiﬁcance; should
apvalue lie above this (corrected) threshold, then this does
notnecessarily indicate that the property does not con-
tribute to the observed causal signiﬁcance. Quite the con-
trary; since we have already observed that there exists a
causal signiﬁcance, then any property that exhibits a low p
value (p≤0.01) remains that with a chance of having some
inﬂuence on the causal eﬀect, from amongst those properties
assessed using inferential statistics.
5.3 Information Retrieval Techniques
To answer RQ4, we perform information retrieval analysis
onreleasetext, usingbothTF.IDFandTopicModelling. We
use two diﬀerent techniques, to increase the conﬁdence with
which we can identify the top terms that occur in the release
text of app releases that exhibit causal signiﬁcance.
Filtering: Text is cast to lower case and ﬁltered for punctu-
ation and stopwords, using the English language stopwords
from the Python NLTK data package4.
Lemmatisation: Each word is processed by the Python
NLTKWordNetLemmatizer , in order to be transformed into
its‘lemmaform’, tohomogenisesingular/plural, gerundend-
ings and other non-germane grammatical details.
TF.IDF: TF.IDF [22] ﬁnds ‘top terms’ in release text: each
term in each document is given a score of TF (Term Fre-
quency)multipliedbytheIDF(InverseDocumentFrequency).
The IDF is equal to the log of the size of the corpus divided
by the number of documents in which the word occurs.
Topic Modelling: We use topic modelling [5] to ﬁnd the
top topics in release text. Topic modelling is a generative
techniquethattrainsaprobabilisticgraphicalmodelonaset
of unstructured textual documents, under the assumption
that they are generated from a set of latent topics.
4nltk.corpus.stopwords.words(‘english’)R
 N
 NW
 L
Figure 3: RQ1: Standard deviation box plots for
(R)ating, (N)umber of ratings, (NW) number of rat-
ings per week and (L)ength of description. Each
plot shows the standard deviations for apps in the
(Con)trol set, (Tar)get set and Full set.
6. RESULTS
This section answers the questions posed in Section 4.
RQ1: Do app metrics change over time? We can
see from the box plots in Fig. 3 that the metrics (R)ating,
(N)umber of ratings, (NW) number of ratings per week and
(L)ength of description do change over time, because their
median standard deviation is always positive. The deviation
in number of ratings and number of ratings per week is high,
but very low for rating. This is because, for apps with many
ratings, even a small change corresponds to thousands of
users rating higher or lower than the established mean.
We can see that the deviation is approximately even be-
tween the control and target datasets for rating deviation;
this is a surprising result, and indicates that either a) rat-
ings are unstable even for stable, established, non-releasing
apps, or b) app releases have little eﬀect over all, globally
detectable, ratings. Theﬁndingofalowdeviationinthetar-
get set means that a causally signiﬁcant release (that aﬀects
rating), has a good chance to ‘stand out from the crowd’.
The box plots show that deviations in number of ratings
and rating frequency length are signiﬁcantly higher for the
target releasing dataset, than for all apps in the dataset and
for the control set. This is expected, and shows some utility
to app releases: to increase user activity in downloading and
rating the apps, and perhaps to increase the user base.
The deviation in description length is higher for the target
dataset as expected, suggesting that descriptions are up-
dated to provide information about releases. This ﬁnding
supports the intuition that descriptions may be used as a
channel of communication between developers and users.
Answer to RQ1: Do app metrics change over
time? The metrics (N)umber of ratings and (NW)
number of ratings per week show a high standard de-
viation for apps between February 2015 and February
2016, but (R)ating shows only a small deviation. The
deviation in user rating frequency is higher for the tar-
get releasing dataset, suggesting that app releases lead
to user activity.
RQ2: Do release statistics have a correlation with
app success? Having established a baseline, we now mea-
sure the correlations between the number and interval of
releases and success metrics.
RQ2.1: Does the number of releases have a high cor-
relation with app success? Table 1 presents the results
of correlation analysis between release quantity and median
interval, and app metrics for the target dataset.Table 1: RQ2: Signiﬁcant ( p≤0.01) correlations be-
tween each of number of releases and median release
interval, and success metrics (R)ating and (N)umber
of ratings at the end of the time period studied, as
well as the change in these metrics from ﬁrst to last
week.
Release statistic Method R ∆R N ∆N
QuantitySpearman 0.13 0.09 0.27 0.30
Pearson 0.14 - 0.09 0.10
Median intervalSpearman -0.12 -0.06 -0.19 -0.21
Pearson -0.09 -0.02 -0.04 -0.05
We only report correlation coeﬃcients (the rho values) that
are deemed signiﬁcant ( p≤0.01), i.e., where there is suﬃ-
cient evidence that rho /negationslash= 0.
The results in Table 1, indicate only weak signiﬁcant cor-
relations between the success metrics and their change over
the time period studied. The strongest correlation, for ∆N
where rho = 0 .30, is still too weak to deﬁnitely suggest a
strong relationship. We therefore conclude that there is no
strong overall correlation between release frequency and the
app metrics we collect, but there is evidence for a weak cor-
relation between number of releases and number of reviews
accrued over a year.
RQ2.2: Does the median time interval between re-
leases have a correlation with app success? Table 1
presents the results of correlation analysis between release
interval and app metrics. As these results revealed, there
is little evidence for any strong correlation between the me-
dianinter-releasetimeperiodandtheappmetricswecollect.
Our ﬁndings corroborate and extend the recent ﬁndings by
McIlroyetal.[27], whoreportedtheratingwasunaﬀectedby
release frequency in the Google app store. This is interesting
because there is evidence that app developers release more
frequently when an app is performing poorly [9]; our results
indicate that this, perhaps rather desperate behaviour, is
unproductive.
Answer to RQ2: Do release statistics have a
correlation with app success? Neither higher num-
bers of releases nor shorter release intervals correlate
strongly with changes in success.
RQ3: Do releases impact app success? The results
from RQ1 and RQ2 have established that app rating met-
rics do vary over releases, but that the number of releases
and time intervals between releases are not important fac-
tors in determining these changes. This makes causal im-
pact analysis potentially attractive to developers. With it,
developers can seek to identify the set of speciﬁc releases
that had a higher eﬀect on success, using evidence for sig-
niﬁcant changes in post-release success compared with the
set of non-releasing apps. This is the analysis to which we
now turn in RQ3.
RQ3.1: What proportion of releases impact app suc-
cess?Table 2 presents overall summary statistics for the re-
sults of causal impact analysis. The ‘Apps’ row indicates the
number of apps summarised as part of the target dataset,
and the ‘Target releases’ row indicates the number of re-
leases these apps underwent in the studied time period that
were outside of a 3-week window of other releases.Table 2: RQ3.1: Causal impact analysis results, in-
dicating the number of casually signiﬁcant releases
over the target dataset, and the number of causally
signiﬁcant releases in each sub-group (as determined
by a release’s eﬀects on subsequent app success).
Details of target releases (percentages reported over 26,339 targ et releases)
Type Total (of target) Apps 14,592
Non-signiﬁcant 17,639 (67.0%) Target releases 26,339
Signiﬁcant 8,700 (33.0%) Control apps 680
Details of signiﬁcant releases (percentages reported over 8,700 signiﬁcant releases)
Metric Total (of signiﬁcant) +ve -ve
R 4,781 (55.0%) 2,563 (29.5%) 2,218 (25.5%)
N 4,747 (54.6%) 4,747 (54.6%) 0
NW 2,226 (25.6%) 862 (9.9%) 1,364 (15.7%)
R∩NW 701 (8.1%) 220 (2.5%) 199 (2.3%)
Table 3: RQ3.2: Results reveal minimal eﬀects arise
from diﬀerent control set size choices using the full
set of 26,339 target releases. Control sets were sam-
pled randomly from the maximal control set of 680
apps. The ‘680’ result calibrates, by assessing (max-
imal) agreement between repeated runs on maximal
sets, capturing variance due to the inherent under-
lying stochastic nature of causal analysis.
Control Set
Metric 3 5 10 21 42 85 170 340 680AgreementR 0.93 0.93 0.93 0.91 0.91 0.91 0.93 0.93 0.94
N 0.91 0.91 0.91 0.91 0.91 0.90 0.91 0.91 0.92
NW 0.97 0.97 0.97 0.97 0.97 0.95 0.97 0.97 0.98
All 0.94 0.94 0.94 0.93 0.93 0.92 0.93 0.94 0.94Cohen’s
KappaR 0.78 0.76 0.76 0.72 0.72 0.73 0.76 0.77 0.79
N 0.69 0.70 0.70 0.70 0.69 0.66 0.69 0.69 0.73
NW 0.78 0.78 0.80 0.83 0.81 0.71 0.81 0.83 0.85
All 0.75 0.74 0.75 0.74 0.73 0.70 0.74 0.76 0.78
Those releases that occur near the beginning or end of the
time period will therefore not have suﬃcient information
available, and so causal impact analysis cannot be applied.
Hence we select a subset of releases that must also belong
in the range of weeks [4, 49], out of a possible [1, 52]. Of
these target releases, some are signiﬁcant and some are not
according to causal impact analysis. As Table 2 reveals, we
found that 33.0% of releases were signiﬁcant.
The remainder of Table 2 reports the observed change in
success metrics for signiﬁcant releases, thereby identifying
candidate causes of these eﬀects. For each success metric
change, we report the total number of releases that exhib-
ited a signiﬁcant change in the associated metric and the
percentage (of all app releases) that exhibited the change.
Wefurthersubdividethistotalintothosethatareconsidered
‘positive’ and ‘negative’ (from a developer’s perspective).
From the 33.0% of signiﬁcant app releases in Google Play,
approximately a third (30.0%) aﬀected more than one suc-
cess metric. The releases of most potential interest to devel-
opers are likely to be those that aﬀect both rating and rating
frequency(duetotheirpotentialforincreasinguserbaseand
revenue). Of these, there were 701 signiﬁcant releases.
These results support the hypothesis that there is a sub-
set of releases that cause signiﬁcant changes to their app’s
success in the store.
RQ3.2: How does the causal control set size aﬀect
results? Table 3 reports the eﬀect of choosing diﬀerent con-
trol set sizes, from among those apps that did not undergo
any release during the time period studied.The results in Table 3 reveal strong agreement for each
control set size, indicating that the model is stable in our
case (using non-releasing apps). We can see from our two
runs (using the full 680 apps in the control set), that there
is between 0.92 and 0.98 agreement due to the stochastic
element of Causal Impact Analysis. Our results show that
restricting our choice of control set does not have advan-
tages, and so we opt to use the full 680 apps for our control
set for subsequent experiments.
Answer to RQ3: Do releases impact app suc-
cess?There is strong evidence ( p≤0.01) that 33% of
the target releases in the Google Play store signiﬁcantly
aﬀected a success metric, and approximately 11% sig-
niﬁcantly aﬀected more than one success metric.
RQ4: What characterises impactful releases? The
ﬁnding from RQ3, tells us that there are signiﬁcant releases,
but it cannot identify the causes, merely that there has been
a signiﬁcant change in post-release success. We now turn
to identify any candidate causes, which may have played a
signiﬁcant role in the changes we have observed, and analyse
their eﬀects.
RQ4.1: What are the most prevalent terms in re-
leases? Table 4 reports the results of information retrieval,
using TF.IDF and the topic modelling on the release text
of signiﬁcant releases. In this table, we consider only those
apps for which release text is available. The results show
only the top TF.IDF terms, and terms from the top topic,
respectively; thereby indicating the most prevalent terms
and topic.
Of the 26,339 target releases (those with suﬃcient evi-
dence for causal impact analysis), 20,014 have release text
available. The remainder of the table consists of four overall
columns, giving the metric for which a release is found to be
signiﬁcant (leftmost column), followed by the most preva-
lent terms (for TF.IDF) and topics (for topic modelling),
followed by a subdivision of these prevalent terms into those
whose eﬀects are positive and negative (from a developer’s
perspective).
Table 4 reveals that terms and topics themed around bug
ﬁxes and features occur frequently overall in the text of sig-
niﬁcant releases. The text of releases that positively or neg -
atively aﬀected rating is slightly more speciﬁc to certain sets
of apps, i.e. “card map feature”,“wallpaper live christmas”,
yet still appear to refer to features. These observations mo-
tivate our subsequent analysis in RQ4.2 for the terms ‘bug’
and ‘ﬁx’ and ‘new’ and ‘feature’.
RQ4.2: How often do top terms and topics occur
in each set of releases? Table 5 shows the number of
occurrences, within signiﬁcant and non-signiﬁcant releases,
of the terms ‘bug’ and ‘ﬁx’, and ‘new’ and ‘feature’ which
emerged as the top terms from our information retrieval in
RQ4.1.
We can see from the results in Table 5 that the terms
‘bug’ and ‘ﬁx’ are far more common than ‘new’ and ‘fea-
ture’ in both signiﬁcant and non-signiﬁcant releases. This is
unsurprising because bug ﬁxing occupies a large proportion
of development eﬀort [43]. However, it is noteworthy that,
in both cases, the terms are more common in the releases
that positively aﬀected metrics, as opposed to those that
negatively aﬀected metrics.Table 4: RQ4.1: Top release text terms: TF.IDF terms on the le ft and Topic Modelling topics on the right.
Terms which occur in all groups are removed for comparison: n ew, app, game, play, free
Type Overall Apps 14,592
Non-signiﬁcant feature ﬁx time mode challenge friend Target releases 26,339
Signiﬁcant feature ﬁx device hero monster battle Release text 20,014
Metric All +ve -ve
R feature word ﬁx hero monster battle feature word time wallpaper live christmas bible ﬁx support account mobile card
N feature word time wallpaper live christmas ﬁx feature word wallpaper live christmas
NW map feature word tip local city card map feature hero monster battle map feature time tip local city
R∩NW feature video map wallpaper live christmas account card feature account mobile card video time feature hero monster battle
Table 5: RQ4.2: Occurrences of the terms ‘bug’ and ‘ﬁx’ and ‘n ew’ and ‘feature’ in release text.
Type Overall Apps 14,592
Non-signiﬁcant 4,690 / 13,200 (35.5%) Target releases 26,339
Signiﬁcant 2,432 / 6,809 (35.7%) Release text 20,014
Metric Total +ve -ve
R 1,336 / 3,794 (35.2%) 745 / 2013 (37.0%) 591 / 1781 (33.2%)
N 1,300 / 3,693 (35.2%) 1,300 / 3,693 (35.2%) 0 / 0
NW 626 / 1,783 (35.1%) 260 / 685 (38.0%) 366 / 1098 (33.3%)
R∩NW 190 / 578 (32.9%) 63 / 179 (35.2%) 48 / 170 (28.2%)
Occurrences of the terms ‘bug’ and ‘ﬁx’ in release text.Type Overall Apps 14,592
Non-signiﬁcant 2,473 / 13,200 (18.7%) Target releases 26,339
Signiﬁcant 1,355 / 6,809 (19.9%) Release text 20,014
Metric Total +ve -ve
R 795 / 3,794 (21.0%) 437 / 2013 (21.7%) 358 / 1781 (20.1%)
N 687 / 3,693 (18.6%) 687 / 3,693 (18.6%) 0 / 0
NW 393 / 1,783 (22.0%) 151 / 685 (22.0%) 242 / 1098 (22.0%)
R∩NW 149 / 578 (25.8%) 52 / 179 (29.1%) 37 / 170 (21.8%)
Occurrences of the terms ‘new’ and ‘feature’ in release text.
Table 6: RQ4.3: Probabilistic analysis of candidate
contributions (P)rice, (RT size) release text size and
(RTchange) release text changes.
Signiﬁcant / Non-signiﬁcant +ve R / -ve R
Metric Wilcoxon ˆA12Wilcoxon ˆA12
P 0.000 0.539 0.000 0.419
RTsize 0.000 0.532 0.006 0.479
RTchange 0.110 0.505 0.434 0.499
RQ4.3: What are the eﬀects of each of the candi-
date causes? Based on the low p-values reported in Ta-
ble 6 we further analyse price and the size of the release text
as candidate causes. Fig. 4 presents box plots showing the
distribution of (paid app) price and release text size, com-
paring signiﬁcant releases against non-signiﬁcant releases,
and releases that positively aﬀect rating against those that
negatively aﬀect rating. Since 61% of the apps are free, we
plot paid apps in the price boxplot, and compute the pro-
portion of free and paid apps in each set, as well as the mean
and median prices.
The results for price are interesting, somewhat surpris-
ing, and nuanced. We found that signiﬁcant releases (pos-
itive and negative) have higher prices than non-signiﬁcant
releases. The mean prices of all signiﬁcant releases (includ-
ing free) and all non-signiﬁcant releases are £0.79 and £0.59,
respectively. Paid releases were more likely to be signiﬁcant:
40.2% of paid app releases were signiﬁcant, compared with
30.7% of free app releases. A somewhat surprising ﬁnding
is that higher priced (paid) app releases are more likely to
have a positive eﬀect: a mean of £3.25 and median of £1.72
compared with £2.45 and £1.58 for those with negative ef-
fects, respectively. However, a greater proportion of sig-
niﬁcant paid app releases negatively aﬀected rating: 62.6%
compared with 41.0% for free apps.
Overall, a larger proportion of signiﬁcant releases are paid
than non-signiﬁcant releases (29.6% compared with 21.7%,
respectively). As a result, the mean price of signiﬁcant re-
leases is higher by £0.20. Of course, a price diﬀerence of
£0.20 in Google Play appears relatively trivial. However,
the diﬀerence in revenue that accrues can be substantial.
Figure 4: RQ4.3: Box plots of Price and Release
Text size, comparing (S)igniﬁcant and (NS) non-
signiﬁcant releases, and releases that increased rat-
ing (+veR) and decreased rating (-veR).
We conservatively calculate that for the app, OﬃceSuite Pro
+ PDF (which had a release on 23rd December 2015, for
which we observed a signiﬁcant eﬀect on the subsequent
rating), over its (minimum) 50,000 installs [13], the price
diﬀerence of £0.20 extrapolates to (minimum) £10,000 in
accrued revenue. Our revenue calculation is particularly
conservative, since at the time of this release, OﬃceSuite
Pro was priced at £11.66. This ﬁnding suggests that devel-
opers need not fear a ‘race to the bottom’ with competitors
over pricing. Unsurprisingly, these results conﬁrm the intu-
ition that users can be expected to be price-sensitive.
We can also see from Table 6 that there are signiﬁcant dif-
ferences between the distributions of price and release text
size, comparing signiﬁcant with non-signiﬁcant releases and
releases that increase rating to those that decreased rating.
The median number of changed, stopword-ﬁltered words in
signiﬁcant release text is 11, compared to only 9 for non-
signiﬁcant release text. This provides evidence that users
may be inﬂuenced by release text, but the eﬀect size is rel-
atively small. Nevertheless, developers might wish to spend
time carefully choosing their release text to maximise posi-
tive inﬂuences on the users.
Answer to RQ4: What characterises impactful
releases? There is evidence that releases that signiﬁ-
cantly aﬀect subsequent app success have higher prices
and more descriptive release text. Releases that posi-
tivelyaﬀectratingaremorecommoninfreeapps, andin
paidappswithhighprices. Wealsonotethattherelease
text of releases that positively aﬀect success make more
prevalent mentions of bug ﬁxes and new features.Table 7: RQ5: Developer responses to questions af-
ter contact was established and a CIRA report was
shared with them. Not all developers responded to
all questions, and so the bottom ‘total’ row indicates
the number of developers who expressed an opinion
on a given question.
Receiving
further
reportsAgree with
detected
signiﬁcanceExternal
cause of
changesLearning
contributing
factorsWould
change
strategy
Yes 29 39 23 39 19
No 16 11 23 6 25
Total 45 50 46 45 44
RQ5: Is causal impact analysis useful to develop-
ers?We sent 4,302 emails to the email addresses available
in the Google Play app store pages of companies with sig-
niﬁcant releases, as detected by our tool, CIRA. Of course,
this is something of a ‘cold call’, and we suspect that many
emails never even reached a human. We can report that 90
immediately failed due to invalid email addresses used on
app store pages, and 127 were immediately assigned to a
support ticket. Those that received a response are the only
cases in which we can verify that contact was established
with developers, of which there were 138.
Of these 138 developers, 56 (distributed across 25 of 41
app store categories) replied to express their opinion in re-
sponse to follow up questions. All respondents’ apps were
established in the store; the smallest had 31 reviews and
the largest had 78,948 at the time of our experiments. We
summarise developers’ opinions in Table 7, in each case in-
dicating the instances where developers expressed an opin-
ion: the most answered question concerned agreement with
CIRA’s assessment, of which there were 50 respondents.
We can observe that 39 out of 50 development teams who
expressed an opinion, agreed with CIRA’s assessment that
their app release was causally signiﬁcant. For example, the
developers of a dictionary application said: “in our case it
wasobviousforourselvesbecauseitwasatotallynewrelease
and with lots of new features”, resonating with our earlier
ﬁnding that new features increase the chance for signiﬁcant
releases (see RQ4.3). Only 11 developers disagreed with
CIRA’s assessment. However, some of them were still able
to identify a cause for the signiﬁcant change detected by
CIRA (but did not think that the release itself could be
the cause). For example, the developers of a security caller
app said: “We did not release anything. We just upload
builds for our beta version which uses few users :-) So your
tools are WRONG.” In this case, although the developers
did not consider a beta version as a full release, the app’s
version identiﬁer changed on its app store page resulting in
a ‘release’, by our deﬁnition in Section 2.2. This detected
release, combined with increased user activity, resulted in
the causal signiﬁcance detected by CIRA.
About one half of those who expressed an opinion (23
out of 46) indicated that they knew of an external reason
for the changes, and several teams elaborated further. We
might expect the developers who know of an external cause
to be a subset of those who believe there to be a signiﬁcant
causal eﬀect. However, 5 of 23 developers who claimed to
know of external causes also disagreed with CIRAthat the
corresponding release was signiﬁcant. One set of develop-
ers described the release as: “a minor ‘bugﬁx-only’ release.Therefore I doubt that this release was the main reason for
this change.”. However, as we know from RQ4 results, men-
tions of bug ﬁxes in release text are more prevalent in sig-
niﬁcant releases that positively aﬀect rating.
Indeed, this particular release did mention bug ﬁxes in its
release text, and signiﬁcantly and positively aﬀected rating .
Over half of the developers who expressed an opinion (29
of 45) were interested in receiving further reports. The ma-
jority of developers (39 of 45) indicated that they would
like to learn more about the characteristics of signiﬁcant
releases, and 19 of 44 indicated they would consider chang-
ing their release strategy based on our ﬁndings. Only 10 of
the 19 developers who would change their strategy knew of
an external cause of the changes, thus suggesting that this
consideration will be based on our general ﬁndings. These
results provide initial evidence that causal impact analysis
can be useful to app developers.
Answer to RQ5: Is causal impact analysis use-
ful to developers? Three quarters of developers who
expressed an opinion (39 of 50) agreed with CIRA. Most
developers (39 of 45) were keen to learn more about the
characteristics of signiﬁcant releases, and 19 of 44 said
that they would consider changing their release strategy
based on CIRA’s ﬁndings. This provides initial evidence
that causal impact analysis is useful to developers.
7. THREATS TO V ALIDITY
In this section we discuss threats to the construct, conclu-
sion and external validity of our ﬁndings.
Construct Validity: The gap between data analysis and
causality is large, forcing any user to make very strong as-
sumptions if they hope to eﬀectively imply causality. Causa l
analysis can hope to reduce this gap, but no such analysis
could hope to fully close it; there will always be unknown
factors which may nevertheless have aﬀected the data. In
the case of app stores, there will always be potential external
inﬂuences for which no data is available to capture them.
We apply causal impact analysis in our experiments, but
there are other forms of causal analysis such as diﬀerences-
in-diﬀerences. Nonetheless, we believe this method is the
most suitable due to its independent consideration of each
app release, and the ability to use all non-releasing apps as
the control set in every experiment, thus reducing the risk of
control set choice inﬂuencing results. We have shown how
causal impact analysis is a useful way to identify releases
that cause signiﬁcant changes in subsequent success. In or-
der to assess the actionability of the technique we ask RQ5.
Conclusion Validity: Our conclusion validity could be
aﬀected by the qualitative human assessment of ‘top terms’
and topics for sets of releases in RQ4.1. We mitigate against
this threat by asking a quantitative question of the number
of times ‘bug’ and ‘ﬁx’ and ‘new’ and ‘feature’ occur in each
set of releases in RQ4.2.
External Validity: Naturally, care is required when ex-
tending our ﬁndings to other app samples and app stores.
Nevertheless, the methods we used to analyse causal eﬀects
can be applied to other app stores. We believe that con-
clusions about the characteristics of signiﬁcant releases ove r
this sample will yield interesting and actionable ﬁndings for
developers, and we contact app developers for their opinions
on the usefulness of our technique in RQ5.We can only report the views of those developers with whom
contact could be established (see Section 6), and care is re-
quired when interpreting their responses. Since we were able
to eﬀectively reach 56 developers, we cannot claim that our
sample is representative of the entire population. However,
this is still a fairly large sample with respect to those used
in other studies involving app developers (e.g., [20, 30]).
8. RELATED WORK
For an overview of app store analysis literature, we point
the reader to our survey on app store analysis for software
engineering [26]. In this section, we discuss previous work on
software releases and causal analysis in software engineering.
There has been a large amount of recent work linking soft-
ware quality with user perceived quality. Ruiz et al. [37]
studied how ad library usage aﬀected user ratings. Bavota
et al. [2] investigated how the changes and faults present in
APIs used aﬀected apps’ ratings. Panichella et al. [33] clas-
siﬁed user reviews for software maintenance. Palomba et
al. [32] studied how developers responding to user feedback
can increase the rating. Moran et al. [28] presented an An-
droid bug reporting tool that can increase the engagement
between users and software quality.
It therefore stands to reason that software releases aﬀect
quality and consequently may aﬀect user rating behaviour.
In 2011 Henze and Boll [16] analysed release times and user
activity in the Apple App Store, and found that Sunday
evening is the best time for deploying games. In 2013 Datta
and Kajanan [12] found that apps receive more reviews af-
ter deploying updates on Thursday or late in the week. In
2015 Gui et al. found that 23% of releases from frequently-
releasing apps contained ad-related changes [15]. Comino
et al. [9] studied the top apps in Apple and Google stores,
ﬁnding that releases can boost user downloads.
McIlroyetal.[27]studiedupdatefrequenciesintheGoogle
Play store, ﬁnding that only 1% of studied apps received
more than one update per week. These ﬁndings support our
weekly data collection schedule, as very few releases can be
‘missed’ by collecting data weekly; additionally the target
releases we use (deﬁned in Section 2.3), mandate that very
frequentlyupdatedappsareexcludedduetolackofsuﬃcient
prior and posterior time series data. McIlroy et al. [27] also
foundthatratingwasnotaﬀectedbyupdatefrequency, how-
ever the ﬁndings by Guerrouj et al. [14] indicate that high
code churn in releases correlates with lower ratings. Nayebi
et al. [29] surveyed developers and users, ﬁnding that half of
developers had clear releasing strategies, and many experi-
enced developers thought that releasing strategy aﬀects user
feedback. Users were not more likely to install apps based
on release date or frequency, but preferred to install apps
which have been infrequently, but recently, updated.
All of these previous ﬁndings on app releases tantalisingly
point to the possibility that certain releases may have higher
causal signiﬁcance than others. However, no previous study
(excepting our technical report [25] and two-page student
research abstract [23]) has speciﬁcally addressed the ques-
tion of how we identify the set of releases that are signiﬁ-
cant. Furthermore, no previous work attempted to identify
the characteristics of highly signiﬁcant app releases: the pri-
mary technical and scientiﬁc contributions of the present pa-
per. In our preliminary work [23, 25], we studied apps which
were in ‘most popular’ lists every week in Google Play and
Windows Phone store, between July 2014 and July 2015.Thisverystrictinterpretationof‘popular’resultedinamuch
lowernumberofappsthantheonesconsideredinthepresent
study: 307 and 726 apps in the Google and Windows sets,
respectively.
In the present study, using a larger dataset that does not
suﬀer from the app sampling problem [24], we conﬁrm and
extend the ﬁndings of our preliminary study [23, 25]. In-
deed, some of the results suggest that certain ﬁndings hold
between diﬀerent stores and datasets, suggesting that they
may be more widely applicable: releases mentioning the
terms new and feature are more likely to positively aﬀect
success; higher priced (paid) app releases are more likely to
have a positive eﬀect. Other results, such as the mentions
of the terms bug and ﬁx, perhaps highlight the diﬀerence
between dataset maturity: releases are more likely to sig-
niﬁcantly aﬀect success if they mention bug ﬁxing in this
large dataset, but slightly less likely in the dataset used in
our technical report [25]. We can speculate that this is ex-
pected, as the apps used in this study are not consistently
the most popular apps in the store, so it is more acceptable
for them to ﬁx bugs in releases; while in the most popular
apps (used in our technical report [25]), it is perhaps less
acceptable to acknowledge bugs, or to spend a large propor-
tion of development eﬀort on bugs. In future work we will
investigate this possibility by comparing sets of apps which
have diﬀerent popularity.
To the best of our knowledge, we are the ﬁrst authors
to apply causal impact analysis to app store analysis. How-
ever, causal inference has been discussed in empirical science
papers [19] and it has been previously used in software engi-
neering. For example the work using the Granger causality
test by Couto et al. [10, 11] and Zheng et al. [44] as a means
of software defect prediction, and the work by Ceccarelli et
al. [7] on identifying software artefacts aﬀected by a change.
SinceCIRAallows us to apply causal impact analysis on any
time series vector, future work will use it to analyse other
metrics from app store data, and other time series datasets.
Future work may also seek to identify the causal eﬀect of
app feature migration [38] on susequent success.
9. CONCLUSIONS
In this work we propose the use of Casual Impact Analysis
to identify causally signiﬁcant releases in app stores. In par-
ticular, we introduce our tool CIRAto perform causal impact
analysis on 26,339 Google Play app releases between Febru-
ary 2015 and February 2016, for all apps that appeared at
least once in the top rated apps in the year prior to Febru-
ary 2015. For these apps, we found that overall release fre-
quency is not correlated with subsequent app success, but
that there is evidence that price and release text size and
content all play a role in whether a release is signiﬁcant and
the type of eﬀect it has on success. Higher priced releases
are more likely to be signiﬁcant and, perhaps surprisingly, to
positively aﬀect rating; there were more prevalent mentions
of new features and bug ﬁxes in releases that positively af-
fected rating, and successful releases also had longer (more
descriptive) release text. We have shown that causal analy-
sis can be a useful tool for app developers by eliciting their
opinions: most of those who responded were interested in
our ﬁndings and agreed with CIRA’s assessment, and some
said they would consider changing their releasing strategy
based on our ﬁndings.10. REFERENCES
[1] B. Adams, S. Bellomo, C. Bird, T. Marshall-Keim,
F. Khomh, and K. Moir. The practice and future of
release engineering: A roundtable with three release
engineers. IEEE Software , 32(2):42–49, 2015.
[2] G. Bavota, M. Linares-Vasquez, C. E.
Bernal-Cardenas, M. D. Penta, R. Oliveto, and
D. Poshyvanyk. The impact of API change-and
fault-proneness on the user ratings of Android apps.
IEEE Transactions on Software Engineering ,
41(4):384–407, 2015.
[3] Y. Bejamini and Y. Hochberg. Controlling the false
discovery rate: A practical and powerful approach to
multiple testing. Journal of the Royal statistical
Society (Series B) , 57(1):289–300, 1995.
[4] M. Bertrand, E. Duﬂo, and S. Mullainathan. How
much should we trust diﬀerences-in-diﬀerences
estimates? Technical report, National Bureau of
Economic Research, 2002.
[5] D. M. Blei, A. Ng, and M. Jordan. Latent Dirichlet
allocation. JMLR, 3:993–1022, 2003.
[6] K. H. Brodersen, F. Gallusser, J. Koehler, N. Remy,
and S. L. Scott. Inferring causal impact using bayesian
structural time-series models. Annals of Applied
Statistics , 9:247–274, 2015.
[7] M. Ceccarelli, L. Cerulo, G. Canfora, and
M. Di Penta. An eclectic approach for change impact
analysis. In Proceedings of the 32nd ACM/IEEE
International Conference on Software
Engineering-Volume 2 , pages 163–166. ACM, 2010.
[8] J. Cohen. A coeﬃcient of agreement for nominal
scales.Educational and Psychological Measurement ,
20(1):37–46, 1960.
[9] S. Comino, F. M. Manenti, and F. Mariuzzo. Updates
management in mobile applications. iTunes vs Google
Play.Centre for Competition Policy (CCP),
University of East Anglia , 2015.
[10] C. Couto, P. Pires, M. T. Valente, R. S. Bigonha, and
N. Anquetil. Predicting software defects with causality
tests.Journal of Systems and Software , 93:24–41,
2014.
[11] C. Couto, C. Silva, M. T. Valente, R. Bigonha, and
N. Anquetil. Uncovering causal relationships between
software metrics and bugs. In Proceedings of the 16th
European Conference on Software Maintenance and
Reengineering (CSMR’12) , pages 223–232, 2012.
[12] D. Datta and S. Kajanan. Do app launch times
impact their subsequent commercial success? an
analytical approach. In Cloud Computing and Big
Data (CloudCom-Asia), 2013 International
Conference on , pages 205–210. IEEE, 2013.
[13] Google and MobiSystems. OﬃceSuite Pro + PDF
Android Apps on Google Play.
https://play.google.com/store/apps/details?id=com.
mobisystems.editor.oﬃce registered. Retrieved 8th
March 2016.
[14] L. Guerrouj, S. Azad, and P. C. Rigby. The inﬂuence
of App churn on App success and StackOverﬂow
discussions. In Proceedings of the 22nd International
Conference on Software Analysis, Evolution and
Reengineering (SANER) , pages 321–330. IEEE, 2015.
[15] J. Gui, S. McIlroy, M. Nagappan, and W. G. J.
Halfond. Truth in advertising: The hidden cost ofmobile ads for software developers. In Proceedings of
the 37th International Conference on Software
Engineering - Volume 1 , ICSE ’15, pages 100–110.
IEEE Press, 2015.
[16] N. Henze and S. Boll. Release your app on sunday eve:
Finding the best time to deploy apps. In Proceedings
of the 13th International Conference on Human
Computer Interaction with Mobile Devices and
Services (MobileHCI’11) , pages 581–586, 2011.
[17] P. W. Holland. Statistics and causal inference. Journal
of the American Statistical Association , 81(396):pp.
945–960, 1986.
[18] F. Khomh, T. Dhaliwal, Y. Zou, and B. Adams. Do
faster releases improve software quality? an empirical
case study of Mozilla Firefox. In Proceedings of the 9th
IEEE Working Conference on Mining Software
Repositories (MSR’12) , pages 179–188, 2012.
[19] B. A. Kitchenham, T. Dyba, and M. Jorgensen.
Evidence-based software engineering. In Proceedings of
the 26th International Conference on Software
Engineering , ICSE ’04, pages 273–281, Washington,
DC, USA, 2004. IEEE Computer Society.
[20] M. Linares-V´ asquez, G. Bavota, C. E. B. C´ ardenas,
R. Oliveto, M. Di Penta, and D. Poshyvanyk.
Optimizing energy consumption of guis in android
apps: A multi-objective approach. In Proceedings of
the 2015 10th Joint Meeting on Foundations of
Software Engineering , ESEC/FSE 2015, pages
143–154, 2015.
[21] M. H. Maathuis and P. Nandy. A review of some
recent advances in causal inference. Handbook of Big
Data, page 387, 2016.
[22] C. D. Manning, P. Raghavan, and H. Sch ¨utze. Scoring,
term weighting, and the vector space model. In
Introduction to Information Retrieval , pages 100–123.
Cambridge University Press, 2008.
[23] W. Martin. Causal impact for app store analysis. In
Companion Proceedings of the 38th International
Conference on Software Engineering , ICSE
Companion 2016. ACM, 2016. 2 page Student
Research Competition (SRC) entry.
[24] W. Martin, M. Harman, Y. Jia, F. Sarro, and
Y. Zhang. The app sampling problem for app store
mining. In Proceedings of the 12th IEEE Working
Conference on Mining Software Repositories , MSR’15,
2015.
[25] W. Martin, F. Sarro, and M. Harman. Causal impact
analysis applied to app releases in Google Play and
Windows Phone Store. Technical report, University
College London, 2015.
[26] W. Martin, F. Sarro, Y. Jia, Y. Zhang, and
M. Harman. A survey of app store analysis for
software engineering. Technical report, University
College London, 2016.
[27] S. McIlroy, N. Ali, and A. E. Hassan. Fresh apps: an
empirical study of frequently-updated mobile apps in
the Google Play store. Empirical Software
Engineering , pages 1–25, 2015.
[28] K. Moran, M. Linares-V´ asquez, C. Bernal-C´ ardenas,
and D. Poshyvanyk. FUSION: A tool for facilitating
and augmenting Android bug reporting. In Proceedingsof the 38th International Conference on Software
Engineering , ICSE ’16, pages 609–612. ACM, 2016.
[29] M. Nayebi, B. Adams, and G. Ruhe. Mobile app
releases – a survey research on developers and users
perception. In IEEE 23rd International Conference on
Software Analysis, Evolution and Reengineering
(SANER’16) . IEEE, 2016.
[30] M. Nayebi, B. Adams, and G. Ruhe. Release practices
in mobile apps – users and developers perception. In
Proceedings of the 23rd IEEE International Conference
on Software Analysis, Evolution, and Reengineering
(SANER) , volume 1, pages 552–562. IEEE, 2016.
[31] G. Neumann, M. Harman, and S. M. Poulding.
Transformed vargha-delaney eﬀect size. In
Search-Based Software Engineering - 7th International
Symposium, SSBSE , pages 318–324, 2015.
[32] F. Palomba, M. Linares-V´ asquez, G. Bavota,
R. Oliveto, M. Di Penta, D. Poshyvanyk, and
A. De Lucia. User reviews matter! tracking
crowdsourced reviews to support evolution of
successful apps. In 31st International Conference on
Software Maintenance and Evolution (ICSME) ’15 ,
2015.
[33] S. Panichella, A. D. Sorbo, E. Guzman, A. Visaggio,
G. Canfora, and H. Gall. How can i improve my app?
classifying user reviews for software maintenance and
evolution. Software Maintenance and Evolution
(ICSME), 2015 IEEE International Conference on ,
2015.
[34] K. Pearson. Notes on regression and inheritance in the
case of two parents. Proc. of the Royal Society of
London, 58:240–242, June 1895.
[35] X.-H. Phan and C.-T. Nguyen. Gibbslda++.
http://gibbslda.sourceforge.net. Retrieved 3rd March
2016.[36] X.-H. Phan and C.-T. Nguyen. Jgibblda.
http://jgibblda.sourceforge.net. Retrieved 3rd March
2016.
[37] I. J. M. Ruiz, M. Nagappan, B. Adams, T. Berger,
S. Dienst, and A. E. Hassan. Impact of ad libraries on
ratings of Android mobile apps. IEEE Software ,
31(6):86–92, 2014.
[38] F. Sarro, A. A. Al-Subaihin, M. Harman, Y. Jia,
W. Martin, and Y. Zhang. Feature lifecycles as they
spread, migrate, remain and die in app stores. In
Proceedings of the Requirements Engineering
Conference, 23rd IEEE International (RE’15) . IEEE,
2015.
[39] S. L. Scott and H. R. Varian. Predicting the present
with bayesian structural time series. International
Journal of Mathematical Modelling and Numerical
Optimisation , 5(1-2):4–23, 2014.
[40] C. E. Spearman. The proof and measurement of
association between two things. The American
Journal of Psychology , 15(1):72–101, 1904.
[41] A. Vargha and H. D. Delaney. A critique and
improvement of the“CL”common language eﬀect size
statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics , 25(2):pp.
101–132, 2000.
[42] F. Wilcoxon. Individual comparisons by ranking
methods. Biometrics Bulletin , 1(6):80–83, 1945.
[43] H. Zhang, L. Gong, and S. Versteeg. Predicting
bug-ﬁxing time: An empirical study of commercial
software projects. In Proceedings of the 2013
International Conference on Software Engineering ,
ICSE ’13, pages 1042–1051, Piscataway, NJ, USA,
2013. IEEE Press.
[44] P. Zheng, Y. Zhou, M. R. Lyu, and Y. Qi. Granger
causality-aware prediction and diagnosis of software
degradation. In IEEE International Conference on
Services Computing (SCC’14) , pages 528–535, 2014.