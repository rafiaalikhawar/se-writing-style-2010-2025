Crash Consistency Validation Made Easy
Y anyan Jiang, Haicheng Cheny, Feng Qiny, Chang Xu, Xiaoxing Ma, Jian Lu
State Key Lab. for Novel Software Technology, Nanjing University, China
Dept. of Computer Science and Technology, Nanjing University, China
yDept. of Computer Science and Engineering, The Ohio State University, United States
jiangyy@outlook.com, {chen.4800,qin.34}@osu.edu, {changxu,xxm,lj}@nju.edu.cn
ABSTRACT
Software should behave correctly even in adverse conditions.
Particularly, we study the problem of automated validation
of crash consistency, i.e., le system data safety when sys-
tems crash. Existing work requires non-trivial manual ef-
forts of specifying checking scripts and workloads, which is
an obstacle for software developers. Therefore, we propose
C3, a novel approach that makes crash consistency validation
as easy as pressing a single button. With a program and an
input, C3automatically reports inconsistent crash sites. C3
not only exempts developers from the need of writing crash
site checking scripts (by an algorithm that computes edit-
ing distance between le system snapshots) but also reduces
the reliance on dedicated workloads (by test amplication).
We implemented C3as an open-source tool. With C3, we
found 14 bugs in open-source software that have severe con-
sequences at crash and 11 of them were previously unknown
to the developers, including in highly mature software (e.g.,
GNU zip and GNU coreutils sort) and popular ones being
actively developed (e.g., Adobe Brackets and T EXstudio).
CCS Concepts
Software and its engineering !Software reliability;
Keywords
File system, crash consistency, software reliability
1. INTRODUCTION
1.1 Crash Consistency Validation
Quality and reliable software is expected to behave cor-
rectly even in adverse conditions. Unfortunately, adverse
conditions are relatively infrequent in practice and some may
1This work was done when Yanyan Jiang was a visiting stu-
dent at The Ohio State University. Chang Xu and Xiaoxing
Ma are the corresponding authors.
Appl i c a t i o n
Fi l e  s ys t e m/ dr i v e r 
 Lo g g e r I nput / wo r kl o a d
Ge ne r i c  o r a c l e 
Te s t  a mpl i Ô¨Åc a t i o nChe c k e r Spe c i Ô¨Åe d b y  de v e l o pe r 
Ge ne r a t o r Bug  r e po r t Figure 1: The workow of crash consistency valida-
tion. White background cells denote the workow of
existing work [21, 28, 31]. In comparison, C3intro-
duces generic oracle (blue), test amplication (red)
and avoids user-specied checker (dashed grey).
even be tricky that developers are not aware of their exis-
tence, leaving hidden \time bombs" in the software. Once
such adverse conditions are triggered, the consequences can
be entirely out of control{maybe as minor as a mobile-app
crash (caused by an uncaught exception [30]) or as severe as
causing billions dollars of economic cost (caused by a race
condition in the blackout in 2003 [12]).
In this paper, we focus on the particular issue of crash
consistency [25], which is an important property for any
software that persists data. Crash consistency requires the
application data (e.g., documents, data, and congurations)
to be recoverable even if the system crashes [22]. Crash
consistency is of signicant importance because (1) as the
software becomes widespread, any issue will eventually be
exposed simply because of the Law of large numbers; and
(2) crash inconsistency may lead to severe consequences. It
would be shocking if your favorite document editor destroys
your paper draft when your pet accidentally hits the hard-
ware reset button at le saving.
However, developers oftentimes fail to provide crash con-
sistency, as they often lack of knowledge on the crash be-
havior of the le operations and their underlying le system.
Even experienced developers leave crash consistency bugs in
their mature software systems [31].
Crash consistency can be validated using semi-automatic
tools that simulate the crash behavior of a le system [21, 28,
31]. These approaches share a common workow (Figure 1):
(1) the program under test (e.g., a database implementation)
is fed with test inputs or workloads; (2) the program execu-
tion's le system or I/O operations are logged; (3) simulated
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
FSE‚Äô16 , November 13‚Äì18, 2016, Seattle, WA, USA
c2016 ACM. 978-1-4503-4218-6/16/11...$15.00
http://dx.doi.org/10.1145/2950290.2950327
133
Table 1: Possible recovered crash sites of Ted text editor. A le is opened and saved once. Crash sites would
be more complicated if multiple les are saved for multiple times and specifying of what are correct is a
dicult task for its developer.
Category Original File Status Backup File Status Consistent? Explanation
C1unmodiedN/AXsave operation failed with
the undamaged original le C2 corrupted
C3 unmodiedup-to-date Xrecoverable failure with the
up-to-date backup le C4 deleted
C5 deleted corrupted  data loss
C6 up-to-date N/A X the up-to-date original le
crash le system images (crash sites) are generated using the
log; and (4) each crash site is checked against a manually
specied checking script (e.g., the database's ACID checker)
for crash consistency.
While being eective in disclosing crash consistency bugs
in data storage systems with well-dened crash semantics,
existing approaches heavily rely on developers' manual ef-
forts, making them cumbersome to use in practice. To make
crash consistency validation easy, the rst challenge (test or-
acle) is that existing techniques require developers to spec-
ify checking scripts to determine consistency of a crash site.
However, they have no idea of crash consistency and are
not trained for specifying such property. The second chal-
lenge (test input) is that the existing test inputs, usually
test cases for functional validation, may not be sucient to
reveal crash consistency bugs.
1.2 How to Make it Easy?
In this paper, we propose the Crash Consistency Checker
(C3for short) to automatically validate crash consistency
of application software. C3makes crash consistency valida-
tion as easy as clicking one button. Given a program with
an input (usually a simple use case), C3either certies the
execution to be crash-consistent or reports an inconsistent
crash site for further inspection. We present the motivation,
the challenges, and the overview of C3in Section 2.
Toremove the manual eorts of writing checking script s,
we devise a generic test oracle at le system level (instead of
application level) for validating whether a crash site is con-
sistent. We observed that developers usually expect the le
system snapshot after a meta-data operation (e.g., directory
operations, le close, and fsync ) to be consistent. Accord-
ingly, we dene crash sites that can be aligned with such
a consistent snapshot via simple recovery operations to be
consistent. The oracle fully automates the crash consistency
validation procedure, exempting developers from writing the
checking scripts that relates to the program semantics.
Automating the validation is not sucient to reveal many
crash consistency bugs. We observed that a le system im-
plementation may enforce crash consistency by chance (e.g.,
truncate and overwrite a small le), causing both manual-
specied checking script and our generic oracle to miss a
crash consistency bug. Such bugs can only be manifested
by dedicated workloads (e.g., a suciently large input le),
which are dicult for developers to provide in practice.
Toreduce the reliance on dedicated workloads , we further
propose test amplication that injects benign le system
synchronization operations in the middle of a program exe-
cution to break such accidental atomicity so that our generic
test oracle can disclose more crash consistency bugs. Tech-nical details are discussed in Section 3.
We implemented C3prototype tool, made it publicly avail-
able and open-source and hope it will help developers dis-
cover more crash consistency bugs early. Our C3implemen-
tation adopts non-intrusive system call instrumentation and
virtual device that are transparent to the program and the
le system. Almost any program written in any language
running on any le system can be validated by our tool.
The implementation decisions are discussed in Section 4.
We conducted experiments on 25 popular open-source pro-
grams to evaluate the eectiveness and eciency of C3. We
discovered crash consistency bugs in 14 subjects, where 11
were previously unknown (7 cannot be manifested without
our test amplication). All the bugs lead to severe conse-
quences (data loss or corruption). Some bugs are from highly
mature software (e.g., GNU zip, GNU coreutils sort) or from
popular ones under active development (e.g., GitHub Atom,
Adobe Brackets, and jEdit). Evaluation results also show
that C3is easy to use and consumes aordable resources.
The evaluation is presented in Section 5.
Finally, we summarize the lessons learned in our exper-
iments and communications with the open-source commu-
nity in Section 6, followed by related work and conclusion
in Sections 7 and 8, respectively.
2.C3IN A NUTSHELL
2.1 Two Motivating Examples
We demonstrate the challenges of crash consistency vali-
dation by two real-world crash consistency bugs discovered
byC3, both were previously unknown and have severe con-
sequences. Inconsistent crash sites can be manifested on a
typical Linux distribution with ext4 le system with default
settings, which represents a typical user's environment.
2.1.1 Ted Text Editor
Productivity software manages user data such as docu-
ments, photos and settings. Such contents should be han-
dled with extreme care, as corrupting them can lead to catas-
trophic consequences. The following (simplied) le-saving
code is from the Ted text editor on Android, which has more
than 100K installations:
1backupPath = path + ".tmp";
2TextFileUtils.writeTextFile(backupPath,
content);
3deleteItem(path);
4renameItem(backupPath, path);
134At a rst glance, this code seems to be crash-safe: le
contents are rst saved to a temporary le; the original le
is then deleted, followed by a renaming of the temporary le.
The developer expected that, whenever the system crashes,
at least one from the original le and the backup le will
remain in the le system.
Such expectation lays on the assumption that each le sys-
tem operation is processed in-order and immediately takes
eect. Unfortunately, this assumption is not valid as both
le system and device driver reorder requests for perfor-
mance. There is no ordering guarantee between the system
calls write (inTextFileUtils.writeTextFile ) and unlink
(indeleteItem ), causing the original le being deleted be-
fore the backup le contents are persisted. The le system
after crash recovery may contain a single corrupted le (0
byte), indicating a catastrophic data loss.
To ensure the ordering between such le system operations
in the example, one can either insert a le system ush call
(fsync ) after the le contents being written or remove the
deleteItem line (ext4 le system on Android provides strong
consistency guarantee for renaming). Our patch is already
merged by the developers1.
This example demonstrates the challenge of automatically
deciding consistency of a crash site. In the example, even
the simplest use case has six categories of crash sites (Ta-
ble 1) and only one of them is inconsistent. Validating crash
consistency is a non-trivial task for software developers be-
cause (1) developers often do not master the knowledge of
crash consistency; (2) there can be many accesses to multi-
ple les and consistency should be dened for every poten-
tial intermediate state; and, (3) le system accesses can be
in libraries that are not well-understood. We address the
challenge by proposing a generic oracle for automated crash
site validation.
2.1.2 GNU coreutils sort
Sort is a command-line tool that prints input lines in
sorted order, which appeared in the rst version of Unix
and is now provided by GNU's core utilities.
Sort is mostly used by pipeline and redirection. However,
it still provides an option to write the results to a desti-
nation le (for backward compatibility of the old-time sort
that sorts a le in-place). The developer community also
considers sorting les in-place using sort data -o data a
valid option and this solution received the highest votes by
the viewers on StackOverow2.
Unfortunately, the practice (or option) of overwriting the
source le for in-place sorting leads to potential data loss
when the system crashes. The destination le is rst opened
and truncated to empty. If at this time system crashes or the
disk runs out of space, contents in data are permanently lost.
We reported this issue to the developers. They indicated
that a completely safe and portable solution is dicult to
work out (due to permission, owner, and hard-link issues)
and the bug is currently xed by explicitly documenting this
dangerous behavior3.
Surprisingly, if the input le is of small size, this bug can-
not be triggered on the ext4 le system with the default set-
ting. In all possible crash sites of a proling run, the data le
1https://github.com/xgouchet/Ted/pull/45.
2http://stackoverow.com/questions/9117274.
3http://debbugs.gnu.org/cgi/bugreport.cgi?bug=22769.
C3 Pr o g r a m a nd t e s t  i nput 
Cr a s h c o ns i s t e nc y  v a l i da t i o n
CS
CS a l i g ns  wi t h no  ESCS
CSCS. . . ES
ES
ES
Cr a s h s i t e  g e ne r a t i o n
Co ns i s t e nt  Ô¨Ål e  s y s t e m
s na ps ho t  g e ne r a t i o n
Te s t  a mpl i Ô¨Åc a t i o n
Re po r t e d c r a s h c o ns i s t e nc y bug pr o Ô¨Ål e  r una mpl i Ô¨Åe d
r uns 
I / O e v e nt  l o g g e r 
Bo unde d s e a r c h Fi l e  s ys t e m s na ps ho t 
. . . Figure 2: Architectural overview of our C3approach.
Blue, red and green components are discussed in
Sections 3.1{3.3, 3.4 and 3.2, respectively.
is either unmodied or up-to-date, because the le system
implementation ensures the atomicity of a small-le over-
write. Neither manually-specied checkers nor our generic
oracle can detect the crash consistency bug, unless dedicated
workloads (e.g., large les) are provided.
This example demonstrates the challenge of reducing the
reliance on the dedicated workloads. Among all existing
approaches [21, 28, 31], only Alice [21] has a chance to detect
the bug without dedicated workloads. However, it relies on
the correct abstract model of le systems. Alternatively,
we address this challenge by test amplication that injects
benign events in the middle of program execution.
2.2 WorkÔ¨Çow of C3
C3adopts a methodology in crash consistency validation
similar to that of storage stacks [21, 28, 31], decomposing
the problem into three sub-problems:
1. (Test input) Find suitable inputs/workloads that can
reveal potential crash-inconsistent vulnerabilities.
2. (Crash site generation) Derive possible crash sites for
each test run.
3. (Test oracle) Validate each crash site's consistency.
We explain the workow of C3in the following. The archi-
tectural overview of C3is shown in Figure 2. The technical
details are expanded in Section 3.
2.2.1 Test Input
The validation procedure of C3is driven by the software's
functional tests/use cases which are easy to obtain. Re-
call that existing work use dedicated workloads to nd bugs
135in storage stacks [21, 28, 31]. Such workloads are usually
overkill for validating application software because applica-
tions have much simpler le system access patterns than a
storage system (e.g., a database or a version-control sys-
tem). C3also adopts test amplication to further reduce
the reliance on dedicated workloads because le system im-
plementation may keep atomicity of operations by chance
(Section 2.1.2).
2.2.2 Crash Site Generation
C3takes the standard approach [28, 32] to generate sim-
ulated crash sites by intercepting I/O requests at runtime
using a virtual RAM disk. We formally dene the semantics
of I/O requests and use this denition to derive all possible
crash sites based on the specication of Linux block layer.
Based on the observation that inconsistency can mostly be
manifested by dropping a small number of metadata blocks,
C3uses a bounded-search algorithm to generate crash sites.
The algorithm enumerates the crash points and systemati-
cally drops a subset of the blocks, yielding crash sites to be
validated. Each crash site is mounted in the local le system
and validated by our generic oracle.
2.2.3 Test Oracle
The key idea of C3generic oracle is to make the expec-
tation of software developers explicit, dening a set of con-
sistent le system snapshots. Our observation is that pro-
grams are usually crash-consistent if le system operations
has atomicity and persistence (otherwise the bug can be
manifested without system crash, which is out of our scope).
Accordingly, C3collects le system snapshots after meta-
data operations (directory operations, le close and fsync )
and consider them to be consistent.
C3certies a crash site to be consistent if it has a small
editing distance to a consistent snapshot, i.e., it can be
transformed to a consistent snapshot via a series of sim-
ple recovery operations that do not involve out-of-thin-air
content creation. In other words, starting from such a crash
site, software users can easily fall back to a consistent state.
Realizing that exact computation of editing distance is in-
tractable, we adopt an alternative relaxed necessary condi-
tion that can be eciently computed in C3.
3. V ALIDATING CRASH CONSISTENCY
In this section, we provide in-depth discussion of C3in
a slightly dierent order than its workow. We rst dis-
cuss the denition of expected le system snapshots (ESs)
and why they are consistent in Section 3.1, followed by how
to obtain crash le system snapshots (CSs) in Section 3.2.
Then, we show how to validate the consistency of a CS in
Section 3.3. Finally, we discuss the design of test amplica-
tion in Section 3.4.
3.1 DeÔ¨Åning Consistent File System Snapshots
When developers are manipulating user-generated con-
tents, they usually do have considerations of data safety
(e.g., Ted intentionally writes to the backup le). There-
fore, the gap between developer's expectation and actual
implementation of le system consistency can lead to crash
consistency bugs. File systems only provide simple interface
for data management and do not have transactional seman-
tics. As a result, the highest level of le system consistency is
atomicity andpersistence of system calls, as if each is issuedin-order and immediately persisted to disk. File system im-
plementations do provide such consistency semantics to its
users, assuming the system never crashes. However, such
consistency breaks at system crash because both le system
and device driver are allowed to buer and reorder I/O re-
quests for maximized performance [21]. Unfortunately, this
phenomenon is not well-understood by the developers and
becomes the root cause of (unrealistic) expectations. For ex-
ample, Ted developers expected buered data to be always
persisted to disk before le deletion takes eect but this is
unfortunately the case in the target platform's le system
implementation (ext4).
Following this intuition, we assume that the program cor-
rectly handles crashes on a strongly consistent le system.
In other words, we assume that developers do not make ob-
vious data-safety mistakes that can be triggered without any
system crash, e.g., deleting original le before backup is writ-
ten4. Particularly, it is reasonable to believe that developers
have knowledge of the intermediate state after every meta-
data operation (directory operations, le close and fsync )
because these operations cause signicant changes to the le
system state.
Therefore, we consider le system snapshots of such in-
termediate states to be \expected" reference snapshots (Ex-
pected Snapshots, or ESs). ESs serve as the basis of dening
consistency of a crash site. ESs are collected by a proling
run in which system calls are intercepted. After each le
metadata operation, we pause the program and traverse the
le system to obtain its snapshot. Back to the Ted example
(Table 1), each of Categories 1, 3, 4, and 6 corresponds to
a consistent le system snapshot after meta-data operation:
program start (C1), close of backup le (C3), deletion of
original le (C4), and renaming (C6).
An ES consists of les in directories. We dene an ES
to be a set of tuples fhf1;c1i;:::hfn;cnig, wherefidenotes
i-th le's full path (e.g., /mnt/crashdisk/file.txt ) andci
denotes its contents. We atten the tree structure because
crash consistency focuses on safety of le contents. A le fi
with contents ci= [b1;b2;:::;b m] denotes that its size is m
bytes andj-th byte value is bj. The proling run returns E,
the set of all ESs.
3.2 Generating Crash Sites
To generate crash sites for validation, we do not actually
power o the machine. Rather, we take the standard ap-
proach of existing work [21, 28, 31] by keeping a log of I/O
requests performed by the program and synthesizing crash
sites at simulated crash points.
Physical disks are notrequired to process I/O requests in
their arrival order for performance, which is a major source
of crash inconsistency [7, 21]. When le system requires
ordering between operations, it invokes a disk barrier (indi-
cated by a REQ_FLUSH orREQ_FUA ag in a Linux block I/O
request) to ush pending requests to disk.
To capture the eect of all possible request ordering, disk
semantics is formalized as follows. A disk Dis a mapping
from sector identier to its actual stored data. For each
sectors2f1;2;:::g, we useD(s) to reference its data. At
runtime, there is an internal queue Qof requests pending
to be ushed as well as auxiliary mappings UandV.U(i)
andV(i) denotesi-th request's sector identier and data,
4There can be data loss if the program is killed in the middle.
136Algorithm 1: Crash snapshot generation algorithm
Input : A sequence of I/O requests fe1;e2;:::;e ngand
a search bound k
Output : A setCcontaining crash snapshots
1C ?;
2forj2f1;:::;ng^ejis not a barrier do
3 Let (D;Q =fei;ei+1;:::e jg;U;V ) be the state after
performingfe1;e2;:::;e jg(fe1;:::;e i 1gare persis-
ted inDand events in Qare pending to be ushed);
4 for`2f0;1;:::; minfk;j i+ 1ggdo
5 forPf0;1;:::;j i 1g^jPj=j i `do
6 Dc D;
7 fors2P[fj igdo
8 Dc Dc[U(i+s)7!V(i+s)];
9 ifDc=2Cthen
10 S mount (Dc);
11 C C[fSg;
respectively, and both are initially empty. A sequence of
write and barrier requests5fe1;e2;:::;e ngare allowed to
be performed on a disk with the following semantics.
1. A write request W(s;d) tos-th sector with data d.
Rather than being immediately persisted, the request
is queued in Q. The notation A[x7!y] denotes map
replacing, i.e., A[x7!y] =An(x;A(x))[(x;y):
ei=W(s;d)
(D;Q;U;V ))(D;Q[feig;U[i7!s];V[i7!d]):
2. A barrier request Bensures all write operations before
it to be persisted:
ei=B
Q=fep;ep+1;:::e qg
Dp=D
Dk+1=Dk[U(k)7!V(k)] (k2fp;p+ 1;:::;qg)
(D;Q;U;V ))(Dq+1;?;U;V ):
Finally, at any system state ( D;Q;U;V ), we allow the
system to crash, yielding a set of valid crash disks Dcrash:
Q=fei;ei+1;:::e jg
Di=fDg
Dk+1=Dk[fD[U(k)7!V(k)]jD2Dkg
(D;Q;U;V ))D crash =Dj+1:
This crash model describes the exact contract between a
disk driver and the Linux block I/O layer. It denes all pos-
sible outcomes of reordering (from a disk's perspective, any
eect of reordering is equivalent to dropping a subset of re-
quests). Furthermore, our crash model assumes the physical
disk to be reliable , i.e., persisted data never corrupts. Oth-
erwise, unreliable physical disk (e.g., severe faults studied
in [32]) may lead to crash sites that cannot be recovered.
Algorithm 1 displays the crash site generation algorithm.
Exhaustively enumerating all crash sites [28] is too time-
consuming. Instead, we use a bounded-search algorithm
based on the observation [31] that dropping only a few of
5Reads do not aect contents in the disk and we do not
consider them in dening crash semantics.critical I/O requests can manifest crash consistency bugs.
We accordingly enumerate the point of system crash (Line 2)
and those crash sites who drop at most krequests in the
pending queue Q(Lines 4{8). The search bound kis ad-
justable: if the time budget is limited, we can bound kto
be a small constant. A suciently large kis equivalent to
an exhaustive enumeration.
Generated disk images are mounted to the native le sys-
tem and further checked for crash consistency. A crash le
system snapshot (Crash Snapshot, or CS) is similar to an ES
described in Section 3.1: the crash snapshot S=fhfi;ciig
denotes that le fihas a contents of ci.
3.3 Validating Crash Sites
The key insight of our general oracle is based on the fol-
lowing case analysis of a crash site S:
1.Sis identical to a consistent ES S02E. Our basic
assumption of ES implies that Sis consistent.
2. We can transform StoS02Eby performing simple
recovery steps that do not involve out-of-thin-air con-
tent creation. An example is C2 of Table 1 where a
corrupted le may contain partial data and deleting
corrupted backup le yields a consistent state. If we
can obtain a consistent state from Sregardless of pro-
gram semantics, Sshould be consistent.
3. Neither (1) nor (2) applies. In this case, non-trivial
recovery scheme is required to fall back to a consistent
state. If such scheme does not exist for general appli-
cation software, Sis highly likely to be inconsistent.
This trichotomy yields a denition of crash consistency
based on the editing distance [20] that avoids both false pos-
itives (reporting a recoverable non-ES crash site as inconsis-
tent) and false negatives (failing to report inconsistent CSs).
Formally, for a crash site S=fhfi;ciigto be consistent,
there must exists an S0=fhf0
i;c0
iig2E such thatScan be
transformed to S0using a bounded number of following edit-
ing operations (assume that hf;ci2S,c= [b1;b2;:::;b m]
andf0can be arbitrary le-name other than f):
1. Creation of an empty le: S)S[hf0;?i.
2. Deletion of a le: S)Snhf;ci.
3. Renaming of a le: S)Snhf;ci[hf0;ci.
4. Moving a consecutive segment of le contents: S)Sn
hf;cinhf0;c0i[hf;[b1;:::;b m;b0
p;:::;b0
q]i[hf0;[b0
1;:::;
b0
p 1;b0
q+1;:::b0
m0]i, where [b0
p;:::;b0
q] is a substring in
the contents of le f0.
This denition echoes the trichotomy: a CS is consistent
only if it can be aligned with an ES with a small edit-
ing distance. Otherwise, a large or innite6editing dis-
tance indicates an impossible or highly non-trivial recov-
ery and we have sucient evidence to report it as inconsis-
tent. Unfortunately, this particular version of editing dis-
tance is intractable. Interested readers can refer to our NP-
Completeness proof in the auxiliary material.
We discovered that a relaxed denition of alignment is
already sucient for crash consistency validation and can
be eciently computed. Particularly, we dene a CS Sto
be consistent if Scan be transformed to an ES S0with a
nite number of editing operations.
This relaxed denition is equivalent to the existence of
an injective mapping from every byte in the les of Sto a
byte in that of S0, simply because an unbounded number of
6If it is impossible to transform a CS to an ES, the editing
distance is innite.
137editing operations allows bytes in Sto be arbitrarily per-
muted, redistributed, and deleted. This relaxed property is
also much easier to check. Formally, Sis consistent only if
there exists S02Esuch that for every byte value ,
X
hf;ci2S0fjjcj=gX
hf;ci2Sfjjcj=g:
This alternative denition of alignment naturally gives a lin-
ear time validation algorithm by comparing the number of
each byte value's occurrences.
Finally, we argue that the relaxation is also eective in
crash consistency validation. First, whenever a CS cannot
be aligned with an ES in the relaxed denition, the editing
distance must be innite. Therefore, as long as the editing
distance reports no false positive of crash inconsistency, so
does the relaxed denition. In theory, the relaxed deni-
tion may misclassify an actually inconsistent CS as consis-
tent, leading to potential false negatives. However, this is
expected to be rare in practice, as reporting crash inconsis-
tency only requires onewitness and the relaxed condition
fails to detect the issue only if it reports false negative on all
CSs. There likely exists at least one inconsistent CS that is
largely corrupted (e.g., le contests are mostly corrupted),
so our relaxed condition tends to capture it and report the
crash consistency bug.
3.4 Amplifying Test Inputs
In the GNU coreutils sort example (Section 2.1.2), the
crash consistency bug cannot be manifested without dedi-
cated workloads because the le system implementation en-
sures small-le overwrite's atomicity by chance. However,
such atomicity is not a guaranteed oer. If the le is suf-
ciently large, we can observe inconsistent crash sites that
only contain partial data and cannot be aligned to any ES.
To exempt the need of dedicated workloads (e.g., huge in-
put les), we designed a test amplication approach. Recall
the root cause of the hidden bug is (not guaranteed) atom-
icity of consecutive operations, we break such atomicity by
injecting system-wide synchronization operation ( sync) in
the middle of a program execution. Such operations are to-
tally benign, i.e., do not aect the application view of the
le system, but can manifest the inconsistent intermediate
crash sites.
Test amplication is conducted in a single separated pro-
gram execution called amplication run . In the amplica-
tion run, we intercept system calls that may silently lead to
data loss ( ftruncate andopen, which are usually contained
in library code of which developers are not aware) and inject
async after each of them. The I/O request log collected for
the amplication run is used for further crash site generation
(Section 3.2) and consistency validation (Section 3.3).
In the GNU coreutils sort example, test amplication in-
jects a sync after the data le is truncated, yielding a CS
that contains only an empty data le, which cannot be
aligned to any ES and is correctly reported as a crash con-
sistency bug.
4. IMPLEMENTATION
We implemented our C3approach as a prototype tool and
made it public and open-source7. Both the instrumentation
and the I/O requests logger in C3are transparent to the
7Available at http://jiangyy.github.io/c3/.1@prepare
2def init_setup():
3 prepare_init_file()
4@run_program
5def start_program():
6 os.system("brackets") # execute program
7@delay(5.0)
8def do_edit():
9 edit_document()
10 keypress('ctrl-s') # save document
11 keypress('alt-f4') # exit program
Figure 3: Simplied test script for the Brackets text
editor in which we found crash consistency bug.
le system. Therefore, C3can validate software written in
any language, using any libraries, and running on any le
system. The idea of C3can be also implemented on other
systems (e.g., by using a simulated iSCSI device [31]). The
rest of this section expands discussion of techniques used in
ourC3implementation.
4.1 Test Input
C3runs the program multiple times using the same test
inputs. Test inputs are specied by test scripts, which are
based on a series of decorated functions in Python (Figure 3).
A test script provides means to specify (1) an initial le sys-
tem snapshot; (2) how to load the program; and, (3) actions
to be performed at program runtime. To further ease the
testing procedure, we also developed a simple record tool [15]
that captures system-level UI events and automatically syn-
thesizes a test script for GUI software.
For each amplication run, C3instruments the program
using ptrace , intercepting the program's control ow when-
ever a system call is about to execute. C3injects a syn-
chronous sync call if a designated point is reached and then
resumes the program execution.
In this paper, we do not focus on how such inputs are
obtained. Even though the program may be large and com-
plicated, there usually are only a few places that interact
with the le system. We believe that simple use cases are
sucient to reveal many crash consistency bugs and devel-
opers will have no obstacle providing test inputs that cover
all le system operations.
4.2 Crash Site Generation
C3collects a I/O request log for crash site generation by
a virtual RAM disk driver, which is similar to eXplode [28].
Before executing the test script, the virtual disk issues an
ioctl call to the driver for capturing the initial snapshot
of the virtual disk. During the test script execution, the
virtual disk handles I/O requests like a normal RAM disk
and at the same time keeps an internal copy of all write and
barrier requests. After the termination of the test script,
these logged data are dumped back to user space via another
ioctl call and CSs are generated using Algorithm 1.
C3can only validate consistency of le system snapshots
on the virtual RAM disk. Therefore, test scripts should
place les to be manipulated on the virtual disk. However,
the program may also modify les whose paths are hard-
coded to the local le system (e.g., /install/path/.config ).
138If developers also intend to validate crash consistency of such
les, they can create its shadow copy on the virtual disk and
replace the le in the local le system by a symbolic link.
4.3 Test Oracle
C3collects a le system snapshot after each le system
metadata operation and considers such ESs to be consistent.
ESs are collected at a separate proling run in which system
calls are instrumented by ptrace . ES cannot be obtained by
the virtual disk (le system calls do not take eect immedi-
ately). Rather, the le system itself always has its consistent
\current"view and C3peeks the le system by a simple read-
only traversal of the le system. Furthermore, C3does not
consider a le system snapshot that contains no data to be
consistent, such that we can detect bugs caused by devel-
oper's accidental deletion of a le. Finally, the atomicity
of an ES collection is guaranteed via serializing le system
calls by ptrace and restricting the program to be the only
process that can access the virtual disk.
Each generated CS is mounted for crash consistency vali-
dation. A CS is scanned to obtain each byte value's statistics
and these values are compared against those of ESs to decide
whether the CS is consistent. Though the time complexity
of consistency checking is linear, we still adopt ngerprint-
ing to further improve the eciency of C3. We associate
each disk image (both CSs and ESs) with the hash nger-
print8of its contents. ESs are de-duplicated according to
their ngerprints and the CSs that have been validated are
immediately skipped to reduce time cost.
4.4 Putting Them Together
C3combines all techniques discussed in this section to re-
alize the workow in Figure 2 and creates crash consistency
bug reports. C3runs the program three times: a proling
run to collect ESs and two test runs (a normal run and an
amplication run) to collect CSs for crash consistency vali-
dation. For each inconsistent CS, we calculate dto be the
minimum editing distance to an ES (in our relaxed deni-
tion,ddenotes the number of bytes that cannot be aligned).
C3reports the crash site that has a maximumdas the in-
consistent crash site for the program.
The reported crash site has the most bytes that cannot be
aligned and thus is most likely to be inconsistent. If it is in-
deed inconsistent (i.e., a true positive), developer can x the
problem and run C3again for further validation. Otherwise,
the CS cannot be transformed to an ES using simple recov-
ery operations, suggesting that a checking script is needed.
Furthermore, we only report an inconsistent CS when
d32 to reduce false positives caused by a small degree
of non-determinism. An example is a program that writes
the timestamp into le's contents. In this case, any CS in
the test run would not align with an ES in the proling run
(i.e.,d= 0) because timestamp in the CS and the ES are
distinct. If a CS is inconsistent, it has at least one sector
of data to be corrupted, which likely to yield more than 32
bytes of data (6.25% of a sector with the size of 512 bytes)
that cannot be aligned. Therefore, this treatment both re-
duces false positives and has negligible probability of missing
a truly inconsistent CS.
8We keep 160-bit SHA-1 ngerprints such that the proba-
bility of hash collision is negligible.5. EV ALUATION
5.1 Methodology
We evaluated the eectiveness, ease-of-use, and perfor-
mance of C3using real-world software and typical use cases.
For eectiveness (Section 5.2.1), we study (1) whether C3
can discover crash consistency bugs in real-world software,
which is demonstrated by an empirical study of bugs found;
(2) whether test amplication is eective in detecting crash
consistency bugs, which is denoted by the percentage of bugs
that require test amplication to manifest; and, (3) whether
C3reports false positives, which is presented by a qualitative
study. We also evaluated more subjects studied by Alice [21]
to compare the eectiveness of the C3oracle with the manual
checking scripts.
For ease-of-use (Section 5.2.2), we study whether the man-
ual eorts to use C3are minor. C3only asks the developer
to specify a use case and the subsequent test amplication
and crash consistency validation is fully automated.
For performance (Section 5.2.3), we study whether the
cost of C3is practically aordable. We measure the follow-
ing quantities for each run of C3: number of ES collected,
number of CS checked, and time consumed in each phase.
We evaluate C3using 25 applications from two categories:
utilities for command-line use (e.g., make, gzip and indent,
10 in total) and productivity applications for editing user
generated contents (e.g., T EXstudio, Atom and Libreoce,
15 in total). We select these applications because they ma-
nipulate le system data and are of signicant popularity
based on authors' experiences and Internet search results.
For each subject, we specify onetypical use case (either
from the documents or from authors' daily use) in the test
script format (Section 4.1). The use case represents the most
common usage of the software in which we believe the crash
consistency bug will have the most severe consequences. As
C3is publicly available, developers can easily validate crash
consistency for any corner case test input.
For each use case, we validate its crash consistency by
C3. If C3reports an inconsistent CS ( C3always outputs
the crash site with the most bytes that cannot be aligned),
it is manually analyzed by studying the source code and
the system call trace. True positives (i.e., crash consistency
bugs) are reported to the developers.
All evaluations were conducted on a commodity environ-
ment for software users: a virtual machine with two virtual-
ized Intel i5 CPUs and 2GB of RAM running Ubuntu Linux
14.04 (Kernel 4.2). The virtual disk is formatted with ext4
of default options, which denotes the most prevalent le sys-
tem setting. Developers can also validate crash consistency
under other le system settings by modifying only one line
in the C3conguration.
5.2 Evaluation Results
5.2.1 Effectiveness
Bugs found . We summarize bugs found by C3in Table 2.
Among 25 evaluated subjects, even if each subject is eval-
uated by one simple use case, C3reported 14 inconsistent
crash sites (all have a suciently large editing distance d
exceeding the threshold). All crash sites are manually an-
alyzed with the system call traces in which we conrmed
the existence of data loss or corruption. Therefore, we sub-
mitted the bug reports for all 14 subjects and the developers
139Table 2: Crash consisntecy bugs discovered by C3. Bug# denotes the bug/issue ID in the issue tracking
system. A bold bug# indicates a previously unknown bug. The Amp. column indicates the bug can only be
manifested with test amplication.
Type Application LOC Language Version Bug# (tracker) Amp. Consequence d(bytes)UtilityGNU make 39.0K C 4.1 46193 (savannah) Incorrect build 7.33K
GNU zip 47.3K C 1.6 22770 (debbugs) Data loss 5.04K
bzip2 8.12K C 1.0.6 N/A (email) Data loss 8.56K
GNU coreutils sort 4.65K C 8.21 22769 (debbugs) X Data loss 23.9K
Perl 801K C 5.22 127663 (perlbug) Data loss 17.4K
Shelve 0.23K Python 2.7.11 25442 (bug tracker) Corruption 907ProductivityGimp 522K C 2.8.14 763124 (bugzilla) X Data loss 188K
CuteMarkEd 21.8K C++ 0.11.2 285(github) X Data loss 5.61K
TEXmaker 46.7K C++ 4.5 1553361 (launchpad) X Data loss 1.61K
TEXstudio 139.6K C++ 2.10.8 1693 (sourceforge) X Data loss 1.61K
Ted 3.7K Java 1.0 45(github) Data loss 4.10K
jEdit 188K Java 5.1.0 3952 (sourceforge) Data loss 1.61K
GitHub Atom 55.8K Node.js 1.5.3 10609 (github) X Data loss 1.61K
Adobe Brackets 117K Node.js 1.5.0 12103 (github) X Data loss 1.61K
conrmed 8 as previously unknown bugs and 3 as previously
known bugs (e.g., the bug is xed in the current development
branch but our validation is based on the latest stable re-
lease). The remaining 3 bug reports have yet received any
responses, but we believe they were also previously unknown
based on the search results in the bug/issue tracking system.
All bugs found by C3have severe consequences like data
loss or corruption, which are analyzed as follows. In 12
out of 14 bugs (gzip, bzip2, sort, perl and all productivity
subjects), the user's le or data can be completely lost after
crash. Furthermore, such bugs were triggered in practice.
For example, GitHub Atom users manifested the same bug
in another adverse condition: when the disk runs out of
space at the halfway of le saving. Gzip developers also
believe data loss had happened before, however, the bug is
not reported maybe due to its irreproducibility.
For the Python standard library Shelve, the bug leads to
corrupted database that cannot be analyzed. The library
provides three backends for data storage but C3found that
none of them is crash-safe. One of such backends is GDBM
whose crash consistency bug is also discussed in [21]. Some
developers believe that a SQLite backend should be provided
for data safety.
For GNU make, we validated the use case of incremental
build of foo.c . The inconsistent crash site contains a cor-
rupted foo.o whose timestamp is up-to-date. If we proceed
with incremental build after system crash, foo.c will be ig-
nored. Such behavior leads to failed (e.g., fail to link a cor-
rupted build target) or erroneous build (e.g., corrupted le
packed into the package). Implications of these real-world
bugs are further studied in Section 6.
Finally, bugs reported by C3also received positive feed-
backs from the open-source community. After we reported
the bug of gzip in the mailing list, the developers of lzip
(a functional equivalent of gzip) conrmed that lzip has the
same crash consistency bug.
These results evidently support that C3is eective and
promising in crash consistency validation.
Test amplication . Column 7 of Table 2 shows that half
(7/14) of the bugs cannot be manifested without test am-
plication using simple test inputs.
An interesting case is T EXstudio, which (1) writes the
le contents to a temporary le; (2) opens the original lewith O_TRUNC ; (3) unlinks the temporary le; and, (4) writes
the le contents to the original le. Surprisingly, the le
system implementation postpones the eect of unlink and
this seemingly-obvious data loss bug cannot be observed in
any possible crash site unless the le is huge. With C3's
test amplication, we can discover the inconsistent crash
site that only contains a truncated le.
These results indicate that the test amplication is eec-
tive in exempting the need of dedicated workloads.
False positive s. We did not observe any false positive in
the evaluated subjects. However, C3may report false pos-
itives if the crash recovery requires non-trivial eorts, e.g.,
in validating databases [31].
Nevertheless, false positive may not be a big issue for soft-
ware developers because C3reports inconsistent CS with an
explanation (the CS cannot be easily transformed to an ES).
By examining the CS and ES, the developer can quickly pin-
point the root cause of false positives and provide additional
rules to lter out actually consistent crash sites that is re-
ported by the generic oracle of C3.
Comparisons with manual checking scripts . We eval-
uated more subjects studied in Alice [21]. These crash con-
sistency bugs are discovered by manual checking scripts. We
ran them with C3using simple workloads. For GDBM [1], C3
correctly reported a corrupted database le. For LevelDB [3]
and LMDB [4], C3reported false positives{inconsistent snap-
shots that have a relatively small d256. For the reported
crash sites, running the default database recovery will ob-
tain a consistent database. For SQLite [6], C3considers it as
consistent and missed the durability bug because it relates
to the database's semantics. For Git [2] and Mercurial [5],
C3did not found crash inconsistency. The system call trace
study [21] suggests that crash may lead to corrupted data
(cannot be opened), but data is not actually lost and may
be recovered by an experienced user. These results are ex-
pected because C3trades, to some degree, the eectiveness
of the tool (detecting more bugs by learning the semantics of
each application) for the easy-of-use of the tool (automating
the crash consistency checking procedure).
5.2.2 Ease of Use
We demonstrate that the eorts of using C3are minor and
trivial for developers. To validate crash consistency, a devel-
1400 6 12 18 24 30 36 42 480246810= 18:4
lines of code of test input scripts# of subjects
Figure 4: Histogram of LOC of test input scripts.
Average LOC = 18:4.
oper only needs to provide C3a test script (Section 4.1) that
consists of (1) an initial software setup, (2) arguments to run
the program, and (3) actions to be performed at runtime (for
interactive programs only). All such eorts are contained in
the test script. We show the statistics of test script LOC in
Figure 4. Even if we are end-users of the evaluated subjects,
writing such a short test script (5{43 with the mean of 18.4
lines of code) takes only a few minutes of work, which usu-
ally consumes less time than setting up the software from
scratch. The longest test scripts are GNU make (simulat-
ing an incremental build) and GNU patch (generating patch
les from two sets of synthesized les), which are 43 and 36
lines of code, respectively. Developers can also reuse their
existing test cases by executing them in the test script.
5.2.3 Performance
We show the performance evaluation results in Table 3,
which is conducted on a machine with limited computational
power. For all evaluated subjects, C3nishes the entire pro-
cess in minutes, which is certainly aordable in a testing
environment.
There are two major factors that impact the performance
ofC3: (1) taking le system snapshots to obtain an ES and
(2) generation and validation of CSs. The CS generation and
validation consumes the most of time but we still consider
such cost is aordable because applications often interact
with le system via limited patterns and a few test cases are
sucient to reveal potential crash consistency bugs. Due to
the limitation of ptrace ,C3cannot precisely decide which
le system call is related to the virtual disk so ES are col-
lected on all possible system calls. The results show that
proling slightly slows the program but this is only a minor
issue because the slowdown is transparent to the program
(as if the system call takes longer time to return). Proling
runs of GUI subjects take longer time than command-line
subjects because we insert one-second delay between all con-
secutive GUI operations to ensure their completion.
6. LESSONS LEARNED
Handling le data demands caution. File system im-
plementations usually do not provide a strong atomicity and
persistence guarantee. Therefore, when user's contents are
being erased (even if backup had been performed), the de-
veloper should be careful. Even experienced developers of
mature software made mistakes (e.g., sort and gzip) and
many \correct" solutions in our subjects (e.g., Vim and sed)Table 3: Performance evaluation results. Values in
a row indicate #ES collected, #CS validated, prol-
ing run time (denoted as P.), CS validation time (de-
noted as V.) and total time (including initial setup,
three runs and CS validation), respectively. The
last row displays averaged number of all 25 evalu-
ated subjects.
ApplicationAmount (#) Time (minutes)
ES CS P. V. Tot.
GNU make 710 277 0.09 0.89 1.19
GNU zip 726 647 0.01 1.85 1.89
bzip2 724 846 0.01 2.68 2.71
GNU coreutils sort 716 578 0.03 1.86 1.96
Perl 718 929 0.01 3.10 3.13
Shelve 754 211 0.01 0.65 0.67
Gimp 3,648 3,168 0.33 9.76 10.74
CuteMarkEd 2,216 483 0.31 1.46 2.41
TEXmaker 1,384 423 0.33 1.30 2.30
TEXstudio 2,258 937 0.33 2.86 3.84
Ted 804 859 0.02 2.63 2.71
jEdit 1,486 1,038 0.16 3.57 4.05
GitHub Atom 14,627 1,201 0.59 3.87 5.65
Adobe Brackets 7,197 1,704 0.59 5.53 7.28
Average (all subjects) 2,176 1,166 0.22 3.70 4.21
are counter-intuitive or overkill.
Even worse, protecting data safety is much trickier than it
appears, because such data erasure may implicitly happen in
the underlying libraries of which the developer may not be
aware. For example, Python Pillow provides image.save()
for writing an image, which opens the le with O_TRUNC .
Using this function to change an image in-place is a crash
consistency bug9and such a pattern is quite likely to occur in
an image editing software10. Furthermore, developers tend
to trust the crash consistency of a mature standard library,
which also may not be valid. An example is Python standard
library Shelve that uses GDBM as its backend by default,
which does not provide any crash guarantee.
Therefore, the rule of thumb is to handle le data with
care (e.g., adding extra flush and fsync to ensure the
persistence if performance is a secondary concern) or crash
consistency should be validated with tools like C3.
Library support matters. Relying on developers to han-
dle all corner cases is impractical. It also seems impractical
for libraries and le systems to provide strong consistency
guarantee: le operations still are the bottleneck of many
applications. Rather, libraries should provide means to pro-
tect data safety or explicitly document their behaviors or
guarantee. Only well-designed libraries can relieve the de-
velopers' burden of considering le system crash behaviors.
Through the communications with the open-source com-
munity, we learned that many frameworks provide good so-
lutions to safely manipulate les. For example, Qt provides
QSaveFile and GTK provides g_file_replace to handle le
operations \in the safest way possible". We also validated
these two libraries by C3and we could not nd an incon-
9We did not report this as a bug because Pillow oers no
crash safety guarantee.
10The most famous open-source image editing software Gimp
has a similar crash consistency bug in all versions before 2.9.
141sistent crash site. Therefore, we strongly recommend devel-
opers to use libraries that explicitly document their safety
(and such safety can be easily validated by C3) in handling
le data.
On the more emerging platforms, however, there lack crash-
safe library support. In addition to the two aforementioned
Python examples, Node.js as a server-side language only pro-
vides a simple library for le system operations. This design
is valid because server programs usually persist data in a
database. However, when the application domain of Node.js
expands, such library becomes a weak link in crash consis-
tency. GitHub Atom developers found the crash consistency
bug dicult to x because Node.js lacks a portable library
for safely saving a document.
Therefore, we recommend library developers to validate
crash consistency of API use cases with tools like C3and
explicitly document whether a library function provides
crash consistency guarantee.
Crash consistency deserves more attention. A devel-
oper does not get rid of the crash consistency issue even if
the software has nothing to do with the le system. Re-
call the GNU make example (Section 5.2.1) in which crash
may lead to a corrupted objective le that has a up-to-date
timestamp. It is not GNU make developer's responsibility
to x the problem; rather, it is GCC that does not meet
crash-safety requirement of GNU make (timestamp should
not be updated until output le is persisted). Yet, GCC as
a compiler is not required to provide such guarantee. The
cascading eect of a minor crash inconsistency nally leads
to potentially severe consequences.
Even if GCC was crash-safe, GNU make allows arbitrary
scripts to be executed in objective le generation and no-
body can guarantee crash consistency for all of them. There-
fore, the only solution is to alert users of such issues and start
a build from scratch (e.g., by executing make clean ) after a
system crash.
Finally, this example suggests that developers should re-
ceive more education on crash consistency. We believe that
the results and analyses presented in this paper will be a
wake-up call for general software developers to pay more
attention to crash consistency.
7. RELATED WORK
Software reliability in various adverse conditions have been
extensively studied. Examples are external system events
that access conicting resources [8], combination of multiple
exceptional conditions [30], and reordered shared memory
accesses in a relaxed memory model system [13].
This paper focuses on crash consistency, the particular
adverse condition of system crash. As CPU and memory
state vanish after crash, crash consistency bugs are scoped
in the storage stack. The storage stack consists of layers of
abstractions (hardware interface, device driver, le system,
library, and database) and is nally used by the software.
At hardware level, the robustness of physical drives is
studied [26, 32]. At le system level, data consistency and
crash recovery are extensively studied [10, 14, 18, 25]. A le
system implementation can be validated by testing [24] or
model-checking [29].
However, even if the le system survives the crash, it does
not guarantee atomicity and persistence of each individualle system call, leading to crash consistency bugs. Our pre-
vious work devised special workloads to exposure such bugs
in databases [31]. Seminal work Alice [21] and eXplode [28]
introduced general frameworks to validate crash consistency
for both system software and applications, which largely in-
spired our work. Alice focuses on modeling crash behaviors
across le system implementations and eXplode focuses on
systematic exploration of execution paths that contain ex-
ceptional control-ow. However, such techniques are not
sucient to eciently validate crash consistency of a wide
varieties of applications. The generic oracle and test am-
plication in C3facilitate fully automated checking of crash
consistency and they are orthogonal to the technical contri-
butions of Alice and eXplode . One can integrate both the
generic oracle and the test amplication of C3into Alice
and/or eXplode . Furthermore, Alice depends on the abstract
le system behavior model extracted from a proling tool
that may not be sound (leading to false positives), while any
crash site reported by C3guarantees to be valid and can be
manifested in practice. Recent work [9] studied crash consis-
tency models, which resembles memory consistency models,
to characterize and validate crash behavior of le systems.
Checking an application's crash consistency against abstract
le system models is a promising future direction.
An alternative approach to crash consistency is providing
transaction among system calls, which can be achieved ei-
ther by operating system support [16, 23, 27] or by hardware
assistance [17, 19]. Finally, the ultimate solution to crash
consistency is a le system implementation that has provable
strong consistency guarantee (atomicity and persistence) for
each individual le system call. Such possibility has recently
been explored [11]. However, such work is still in its early
stage to be realized in performance-critical production envi-
ronments.
8. CONCLUSION
In this paper, we present C3, a novel approach for val-
idating the crash consistency of application software. The
generic oracle and test amplication facilitate the automated
validation of crash consistency for application software. Eval-
uation on real-world applications demonstrates the eective-
ness and eciency of C3in detecting crash consistency bugs.
We not only made C3public and open-source but also pre-
sented valuable lessons learned from the bugs discovered by
C3and the communications with the open-source commu-
nity. We hope the results in this paper will be a cornerstone
for further enhancement of software reliability in terms of
system crash.
9. ACKNOWLEDGMENTS
We thank the anonymous reviewers for helpful comments
and suggestions. This work was supported in part by Na-
tional Basic Research 973 Program (Grant #2015CB352202),
National Natural Science Foundation (Grant #61472177,
#91318301, #61321491) of China, NSF grants #CCF-0953
759 (CAREER Award), #CCF-1319705, the CAS/SAFEA
international Partnership Program for Creative Research Te-
ams, China Scholarship Council (#201506190103), the pro-
gram for Outstanding PhD candidate of Nanjing University,
and the Collaborative Innovation Center of Novel Software
Technology and Industrialization, Jiangsu, China.
14210. REFERENCES
[1] GDBM. http://www.gnu.org/software/gdbm/gdbm.
html.
[2] Git. http://git-scm.com.
[3] LevelDB. https://code.google.com/p/leveldb.
[4] LMDB. http://symas.com/mdb/.
[5] Mercurial. http://mercurial-scm.org.
[6] SQLite. http://www.sqlite.org/.
[7] Linux kernel block driver docs, 2005.
[8] C. Q. Adamsen, G. Mezzetti, and A. Mller. Systematic
execution of android test suites in adverse conditions.
InProceedings of the International Symposium on Soft-
ware Testing and Analysis , ISSTA, pages 83{93, 2015.
[9] J. Bornholt, A. Kaufmann, J. Li, A. Krishnamurthy,
E. Torlak, and X. Wang. Specifying and checking le
system crash-consistency models. In Proceedings of the
International Conference on Architectural Support for
Programming Languages and Operating Systems , ASP-
LOS, pages 83{98, 2016.
[10] J. Carreira, R. Rodrigues, G. Candea, and R. Majum-
dar. Scalable testing of le system checkers. In Proceed-
ings of the ACM European Conference on Computer
Systems , EuroSys, pages 239{252, 2012.
[11] H. Chen, D. Ziegler, T. Chajed, A. Chlipala, M. F.
Kaashoek, and N. Zeldovich. Using crash hoare logic
for certifying the fscq le system. In Proceedings of
the Symposium on Operating Systems Principles , SOSP,
pages 18{37, 2015.
[12] E. C. R. Council. The economic impacts of the August
2003 blackout. 2004.
[13] C. Flanagan and S. N. Freund. Adversarial memory for
detecting destructive races. In Proceedings of the SIG-
PLAN Conference on Programming Language Design
and Implementation , PLDI, pages 244{254, 2010.
[14] C. Frost, M. Mammarella, E. Kohler, A. de los Reyes,
S. Hovsepian, A. Matsuoka, and L. Zhang. Generalized
le system dependencies. In Proceedings of the Sym-
posium on Operating Systems Principles , SOSP, pages
307{320, 2007.
[15] L. Gomez, I. Neamtiu, T. Azim, and T. Millstein.
RERAN: Timing- and touch-sensitive record and re-
play for Android. In Proceedings of the International
Conference on Software Engineering , ICSE, pages 72{
81, 2013.
[16] S. Kim, M. Z. Lee, A. M. Dunn, O. S. Hofmann,
X. Wang, E. Witchel, and D. E. Porter. Improving
server applications with system transactions. In Pro-
ceedings of the ACM European Conference on Com-
puter Systems , EuroSys, pages 15{28, 2012.
[17] Y. Lu, J. Shu, J. Guo, S. Li, and O. Mutlu. High-
performance and lightweight transaction support in
ash-based ssds. IEEE Transactions on Computers ,
64(10):2819{2832, 2015.
[18] A. Ma, C. Dragga, A. C. Arpaci-Dusseau, and R. H.
Arpaci-Dusseau. Ffsck: The fast le-system checker.
InProceedings of the USENIX Conference on File and
Storage Technologies , FAST, pages 1{16, 2013.
[19] C. Min, W.-H. Kang, T. Kim, S.-W. Lee, and Y. I.
Eom. Lightweight application-level crash consistency
on transactional ash storage. In Proceedings of theUSENIX Annual Technical Conference , USENIX ATC,
pages 221{234, 2015.
[20] G. Navarro. A guided tour to approximate string
matching. ACM Computing Surveys , 33(1):31{88, 2001.
[21] T. S. Pillai, V. Chidambaram, R. Alagappan, S. Al-
Kiswany, A. C. Arpaci-Dusseau, and R. H. Arpaci-
Dusseau. All le systems are not created equal: On
the complexity of crafting crash-consistent applications.
InProceedings of the Symposium on Operating Sys-
tems Design and Implementation , OSDI, pages 433{
448, 2014.
[22] T. S. Pillai, V. Chidambaram, R. Alagappan, S. Al-
Kiswany, A. C. Arpaci-Dusseau, and R. H. Arpaci-
Dusseau. Crash consistency. Communications of the
ACM , 58(10):46{51, 2015.
[23] D. E. Porter, O. S. Hofmann, C. J. Rossbach, A. Benn,
and E. Witchel. Operating system transactions. In Pro-
ceedings of the Symposium on Operating Systems Prin-
ciples , SOSP, pages 161{176, 2009.
[24] T. Ridge, D. Sheets, T. Tuerk, A. Giugliano, A. Mad-
havapeddy, and P. Sewell. Sibylfs: Formal specication
and oracle-based testing for posix and real-world le
systems. In Proceedings of the Symposium on Operat-
ing Systems Principles , SOSP, pages 38{53, 2015.
[25] M. Rosenblum and J. K. Ousterhout. The design and
implementation of a log-structured le system. ACM
Transactions on Computer Systems , 10(1):26{52, 1992.
[26] B. Schroeder and G. A. Gibson. Disk failures in the real
world: What does an MTTF of 1,000,000 hours mean
to you? In Proceedings of the USENIX Conference on
File and Storage Technologies , volume 7 of FAST , pages
1{16, 2007.
[27] R. P. Spillane, S. Gaikwad, M. Chinni, E. Zadok,
and C. P. Wright. Enabling transactional le access
via lightweight kernel extensions. In Proceedings of the
USENIX Conference on File and Storage Technologies ,
volume 9 of FAST , pages 29{42, 2009.
[28] J. Yang, C. Sar, and D. Engler. Explode: A lightweight,
general system for nding serious storage system er-
rors. In Proceedings of the Symposium on Operating
Systems Design and Implementation , OSDI, pages 131{
146, 2006.
[29] J. Yang, P. Twohey, D. Engler, and M. Musuvathi. Us-
ing model checking to nd serious le system errors.
ACM Transactions on Computer Systems , 24(4):393{
423, 2006.
[30] P. Zhang and S. Elbaum. Amplifying tests to validate
exception handling code. In Proceedings of the Inter-
national Conference on Software Engineering , ICSE,
pages 595{605, 2012.
[31] M. Zheng, J. Tucek, D. Huang, F. Qin, M. Lillib-
ridge, E. S. Yang, B. W. Zhao, and S. Singh. Torturing
databases for fun and prot. In Proceedings of the Sym-
posium on Operating Systems Design and Implementa-
tion, OSDI, pages 449{464, 2014.
[32] M. Zheng, J. Tucek, F. Qin, and M. Lillibridge. Un-
derstanding the robustness of SSDs under power fault.
InProceedings of the USENIX Conference on File and
Storage Technologies , FAST, pages 271{284, 2013.
143