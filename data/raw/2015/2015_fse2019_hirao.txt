TheReview Linkage Graph forCodeReview Analytics
ARecovery Approachand EmpiricalStudy
ToshikiHirao
Nara Institute of Scienceand Technology
Nara, Japan
hirao.toshiki.ho7@ais.naist.jpShane McIntosh
McGillUniversity
Montréal,Canada
shane.mcintosh@mcgill.ca
AkinoriIhara
WakayamaUniversity
Wakayama, Japan
ihara@sys.wakayama-u.ac.jpKenichi Matsumoto
Nara Institute of Scienceand Technology
Nara, Japan
matumoto@is.naist.jp
ABSTRACT
Modern Code Review (MCR) is a pillar of contemporary quality
assurance approaches, where developers discuss and improve code
changespriortointegration.Sincereviewinteractions(e.g.,com-
ments,revisions)arearchived,analyticsapproacheslikereviewer
recommendation and review outcome prediction have been pro-
posed to support the MCR process.These approachesassume that
reviews evolve and are adjudicated independently; yet in practice,
reviewscan be interdependent.
Inthispaper,wesetouttobetterunderstandtheimpactofre-
view linkage on code review analytics. To do so, we extract review
linkage graphs where nodes represent reviews, while edges rep-
resent recovered links between reviews. Through a quantitative
analysis of six software communities, we observe that (a) linked
reviewsoccurregularly,withlinkedreviewratesof25%in Open-
Stack,17%inChromium ,and3%ś8%in Android ,Qt,Eclipse,and
Libreoffice ; and (b) linkage has become more prevalent over time.
Throughqualitative analysis, we discover that linksspan 16 types
that belong to five categories. To automate link category recovery,
we train classifiers to label links according to the surrounding doc-
ument content.ThoseclassifiersachieveF1-scoresof0.71ś0.79, at
leastdoublingtheF1-scoresofaZeroRbaseline.Finally,weshow
thattheF1-scoresofreviewerrecommenderscanbeimprovedby
37%ś88% (5ś14 percentage points) by incorporating information
from linked reviews that is available at prediction time. Indeed,
review linkage should be exploited by future code review analytics.
CCS CONCEPTS
·Software andits engineering →Softwareevolution .
KEYWORDS
Code review,software analytics, miningsoftware repositories
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’19, August 26ś30,2019, Tallinn,Estonia
©2019 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-5572-8/19/08...$15.00
https://doi.org/10.1145/3338906.3338949ACMReference Format:
ToshikiHirao,ShaneMcIntosh,AkinoriIhara,andKenichiMatsumoto.2019.
The Review Linkage Graph for Code Review Analytics: A Recovery Ap-
proach and Empirical Study. In Proceedings of the 27th ACM Joint European
Software Engineering Conference and Symposium on the Foundations of Soft-
ware Engineering (ESEC/FSE ’19), August 26ś30, 2019, Tallinn, Estonia. ACM,
NewYork, NY, USA, 12pages.https://doi.org/10.1145/3338906.3338949
1 INTRODUCTION
ModernCodeReview(MCR)Ðalightweightvariantoftraditional
code inspections [ 14]Ðallows developers to discuss the premise,
content,andstructureofcodechanges.Manycommunitiesadopta
Review-Then-Commit (RTC) philosophy, where each code change
must satisfy review-based criteria before integration into official
repositoriesispermitted.SinceMCRtoolsarchivereviewingactivi-
ties (e.g., patch revisions, review participants, discussion threads),
active developmentcommunities generateplenty of MCRdata.
Researchershaveproposedanalyticsapproachesthatleverage
MCR data to support practitioners. For example, reviewer recom-
menders[4,31,42,44,48] help developers to select appropriate
reviewers and review outcome predictors [16,22,23] estimate the
likelihoodofacode changeeventually being integrated.
When performing code reviewanalytics, each review has tradi-
tionally been treated as an independent observation; yet in prac-
tice,reviewsmaybeinterdependent.Byignoringconnectionsbe-
tween reviews, review analytics approaches may underperform.
For example, the discussion for Review #314319 ( https://review.
openstack.org/#/c/314319/ ) of theOpenStack Neutron project
occurred on its linked Review #225995. Without considering the
comments on the linked review, a review outcome predictor would
mistakenlypresumethatReview#314319hadnodiscussion,and
wouldthus be unlikely to be integrated. Moreover, Review#134811
(https://review.openstack.org/#/c/134811/ ) of theOpenStack Nova
project is abandoned because a competing solution in the linked
Review#134853wasintegrated.Toensureafairreviewprocess[ 15],
reviewerlistsofcompetingsolutionsmayneedtobesynchronized;
yetlinksare not analyzedbytoday’sreviewer recommenders.
In this paper, we set out to better understand the impact of
reviewlinkageoncodereviewanalytics.Todoso,weextractthe
review linkage graph from six active MCR communitiesÐa directed
graph where nodes represent reviews and edges represent links
between reviews. We analyze and leverage those graphs to address
the following fourresearch questions:ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia ToshikiHirao,ShaneMcIntosh,AkinoriIhara,andKenichi Matsumoto
(RQ1)Towhat degree are reviewslinked?
Motivation: To gain an initial intuition about the connected-
ness of reviews, we first set out to quantitatively analyze the
extractedreviewlinkage graphs.
Results:Linkageratesrangefrom3%to25%.Linkagetendsto
be more common in the two largest communities (25% in Open-
Stackand17%in Chromium ),likelybecausetheyhaveinvested
moreheavilyincodereviewing.Indeed,thesecommunitieshave
significantlymorecommentsandreviewersperreview(pairwise
Mann-Whitney U tests with Bonferroni correction, α=0.01;
andnon-negligible Cliff’sdelta effectsizes).
(RQ2)Whyare reviewsbeing linked?
Motivation: The potentialreasons for review linkageare mani-
fold.Toexplorethesereasons,wesetouttoqualitativelyanalyze
recoveredlinksbetween reviews.
Results:Using open coding [ 12] and card sorting [ 30], we dis-
cover 16 types of review links that belong to five categories,
i.e.,PatchDependency,BroaderContext,AlternativeSolution,
VersionControlIssues,andFeedbackRelated.Thesedifferent
link types have different implications for review analytics tech-
niques.Forexample,whileBroaderContextlinksindicatethata
discussionmayspanacrosslinkedreviews,AlternativeSolution
linkspointoutcompeting solutions.
(RQ3) To what degree can link categories be automatically
recovered?
Motivation: The qualitative approach that we used to address
RQ2isnotscalableenoughforlarge-scaleanalysesoflinkcat-
egories. Hence, we want to explore the feasibility of training
automaticclassifiers to identifylinkcategories.
Results:Wetrainlinkcategoryclassifiersusingfive classifica-
tiontechniques.Theseclassifiersatleastdoubletheperformance
ofaZeroRbaseline,achievingaprecisionof0.71ś0.77,arecall
of0.72ś0.92, andan F1-score of0.71ś0.79.
(RQ4)Towhatdegreedolinkedreviewsimpactcodereview
analytics?
Motivation: WhileRQ1śRQ3suggestthatlinkagemayimpact
revieweranalytics,theextentofthatimpactisunknown.Weset
outtoquantifythatimpactbycomparingpriorreviewanalytics
techniques [ 16,48]to extendedversionsthat are linkaware.
Results:Code review analytics tend to underperformon linked
reviews. In 41%ś84% of linked reviews, reviewer recommenders
omit at least one shared reviewer. Moreover, review outcome
predictorsmisclassify35%ś39%oflinkedreviews.Link-aware
approaches improve the F1-score of reviewer recommenders by
37%ś88%(5ś14percentagepoints).
Our empirical study indicates that linkage is a rich activity that
shouldbetakenintoconsiderationinfutureMCRstudiesandtools.
In addition, this paper contributes a replication package,1which
includes(a)reviewlinkagegraphsthatfeature1,466,702reviewsand
231,341linksfromthesixstudiedcommunities;(b)752manually
coded links spanning 16 types and five categories [RQ2]; and (c)
scripts that reproduce our statisticalanalyses[RQ1,RQ3, RQ4].
Paper Organization. The remainder of this paper is organized as
follows. Section 2situates this paper with respect to the related
1https://github.com/software-rebels/ReviewLinkageGraphwork.Section 3describesthestudiedcommunities andtheirMCR
processes.Sections 4ś7presenttheexperimentsthatweconducted
to address RQ1śRQ4, respectively. Section 8discusses the broader
implicationsofourresults.Section 9disclosesthreatstothevalidity
ofour study.Finally,Section 10draws conclusions.
2 RELATED WORK
Linkageofrelatedsoftwareartifactshaslongbeenconsideredan
importantphenomenon.Canfora etal.[11]foundthatlinksbetween
issue reports of different software projects are not uncommon.
BoisselleandAdams[ 8]reportedthat44%ofbugreportsinUbuntu
arelinkedtoindicateduplicatedwork.Ma etal.[29]showedthat
linkedissuesdelaythereleasecycleandincreasemaintenancecosts.
Moreover,theyfoundthatrecoveringalinkisadifficulttask,often
taking developers more thanone dayto do byhand.
Toeasetherecoveryoflinks,researchershaveproposedauto-
maticapproaches.Antoniol etal.[2]appliedNaturalLanguagePro-
cessing(NLP)techniquestodetectlinksbetweensourcecodeand
relateddocuments.Alqahtani etal.[1]proposedanautomaticap-
proach to recover links between API vulnerabilities. Guo et al.[17]
useddeeplearningtechniquesthatexploitdomainknowledgeto
detect semantic links. Rath et al.[34] detect missing links between
commits andissuesusing processandtext-relatedfeatures.
Linkagealsoappearsinpeercodereviewsettings.Zampetti et
al.[46] found that developers reference other resources in reviews
toenhancedocumentation of pullrequests. Perhaps themost sim-
ilar prior work is that of Li et al.[27], who reported that 27% of
GitHub pull requests from 16,584 Python projects on GitHub have
links,whichspansix types.Thispaperexpandsupontheworkof
Liet al.by studying linkage in six large and successful software
communities (rather than a broad sample of GitHub projects), and
the impact oflinkage onreviewanalytics approaches.
2.1 ReviewerRecommendation
Selecting appropriate reviewers plays an important role in the
value that is generated by a code review. Bosu et al.[9] found
thatthereviewerexpertiseisanimportantfactorindetermining
whetherMicrosoftdevelopersconsidercodereviewfeedbackuseful.
Kononenko et al.[25] also found that reviewer expertise is a key
factor indeveloper perceptionsof code reviewquality at Mozilla.
Tosupportauthorsinselectingappropriatereviewers,researchers
have proposed reviewer recommenders. For example, Balachan-
dran [4] proposed ReviewBot, which recommends reviewers based
on pastcontributionsto thelines thatwere modified bythepatch.
Thongtanunam et al.[42] proposed RevFinder, which recommends
reviewers based on their prior contributions to files in similar
locations within the codebase. More recent work has improved
reviewer recommendations by leveraging past review contribu-
tions [48], technological experience and experience with other
related projects [ 31], and the content of the patch itself [ 44]. Ko-
valenkoetal.[26]questionthevalueofrecommendingreviewers
incaseswhere the bestreviewers are already known.
Review links may also provide useful information for the re-
viewer recommenders. Since links may indicate a (strong) relation-
ship between connected reviews, reviewers who are recommended
for one of the linked reviews may also need to be recommendedTheReviewLinkageGraphforCode ReviewAnalytics ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
Table 1:Anoverview ofour subjectcommunities.
Product Language StudiedPeriod #Reviews #Revs #Projects
OpenStack Python 09/2011ś01/2018 533,050 12,359 1,804
Chromium JavaScript 04/2011ś01/2018 364,079 7,442 410
Android Java 10/2008ś01/2018 229,210 7,614 1,049
Qt C++ 07/2011ś01/2018 188,981 2,377 170
Eclipse Java 04/2012ś01/2018 106,515 2,191 380
Libreoffice Python 04/2012ś01/2018 44,867 822 34
Total 31.8years 1,466,702 32,805 3,847
for theothers. For example, Review #109178 is another attemptat
tackling the underlyingtaskof (theabandoned)Review #105238.2
Topreservecontinuityofthereviewdiscussion,thereviewersof
Review#105238should alsobe recommended for Review#109178.
2.2 CodeReviewOutcome
Not all changes that are submitted for code review end up being
integrated.Indeed,Weißgerber etal.[43]foundthat39%and42%
of OpenAFS and FLAC code changes were eventually integrated.
Jianget al.[23] found that 33% of Linux code reviews were eventu-
allyintegrated.Baysal etal.[5]foundthat36%ś39%ofcodechanges
inMozillaFirefox were rejectedandresubmittedat leastonce.
To better understand the chances of submissions being inte-
grated,researchershavestudiedthecharacteristicsofcodechanges
(andtheirreviews)thatwereeventuallyintegrated.Forexample,
Weißgerber etal.[43]foundthatsmallcodechangesareintegrated
more frequently than large ones. Jiang et al.[23] showed that prior
experience, patch maturity, and code churn significantly impact
the likelihood of integration. Baysal et al.[6,7] found that non-
technical issues are a common reason for abandonment in WebKit
and Blink projects. Moreover, Tao et al.[41] found that patch de-
signissueslikesuboptimalsolutionsandincompletefixesareoften
raised during the reviews of the abandoned code changes of the
EclipseandMozillaprojects.
Review links may affect orimply review outcomes. For example,
sincereviewsthatsupersedepriorreviewshavehadtheopportunity
to improve the design of the code change, they may have a higher
likelihoodofbeingintegratedthaninitialsubmissions.Moreover,
large tasks that have been divided into a series of reviews (e.g.,
Reviews#1025323and#1025434ofOpenStack )willhavereview
outcomes that are inherently linked.
Priorworksuggeststhatreviewlinkageisimportant;however,the
extentoflinkageinlargecommunitiesanditsimpactonreview
analyticsisunclear.Inthispaper,wesetouttobridgethesegaps.
3 STUDIED COMMUNITIES
The goal of our study is to extract and analyze review graphs of
largesoftwarecommunitiesthathaveinvestedintheirMCRpro-
cesses. To do so, we focus on the six popular communities that
appearintherecentrelatedwork[ 19,42,45].OpenStack isacloud
computing platform. Chromium is an open source web browser.
Android is a mobile software platform. Qtis a cross-platform
2https://review.openstack.org/#/c/105238/
3https://review.openstack.org/#/c/102532/
4https://review.openstack.org/#/c/102543/applicationframework. EclipseisanIntegratedDevelopmentEnvi-
ronment(IDE)andassociatedtools. Libreoffice isafreeandopen
implementationofthe Office software suite.
The studied communities use Gerrit for code review. To con-
ductourstudy,wecollectMCRdatausingtheGerritAPI.Table 1
providesanoverviewofthecollecteddata.Below,weprovidean
overviewofthe Gerritprocesses of the studiedcommunities.
GerritCodeReviewProcess. Gerritisapopular,web-basedcode
reviewmanagementtoolthattightlyintegrateswiththeGitversion
control system. Rather than pushing code changes directly to an
upstream Git repository, developers push changes to Gerrit, where
only after satisfying project-specific review criteria (e.g., a member
of the core team approves the changes) may the author push their
changes into the upstream Git repository. Similar to other code
reviewingprocesses(e.g.,GitHubpullrequests),eachGerritreview
goes throughthe following phases:
(1) Uploading a proposed set of changes. An author uploads
a proposed set of changes to the Gerrit system and invites
reviewers tocritique it byleaving comments for the author to
clarify,discuss, oraddress.
(2) Solicitingpeerfeedback. Reviewerscritiquethepremise,con-
tent, and structure of theproposed set of changes and provide
feedbackintheformof generalorinlinecomments thatcorre-
spondto the entire changeandlineswithin it,respectively.
(3) Revising the proposed patch. Authors may revise their set
of changes to address reviewer feedback. After revising, the
reviewreturns to phase2.
(4) Automatedtesting. Automatedtestsareexecutedoneachset
of changes to mitigate the risk of introducing regression issues.
Ifthesetestsfail,thesetofchangesisblockedfromintegration
until the author uploads a revision that addresses the issues.
Whenrevisions are uploaded,the reviewreturns to phase2.
(5) Final integration. Oncethesetofchanges satisfiesreviewer
and automated testing criteria, the author can integrate it into
upstream Git repositories.
4 REVIEW GRAPH EXTRACTION (RQ1)
In this section, we set out to study the extent to which reviews are
linked to one another in our studied communities. To do so, we
extractreviewgraphsfromthestudiedcommunities.Thereview
graphRG=(R,L)isadirectedgraphwiththefollowingproperties:
•Graph nodes Rrepresent reviewentries. Eachreview entry
r∈Ris comprised of a set of properties, such as IDr(a
uniquereviewidentifier), Dr(thechangedescription), PRr
(the set of patch revisions), REVr(the set of reviewers). GCr
andICr(generalandinline comments, respectively).
•Graph edges Lrepresent links between review entries. Each
edgel∈Lhas a type Tlthat describes why the link was
recorded,whichwe study inSections 5ś7.
The set of reviews Ris what has typically been extracted and
analyzed by prior work on code review. In this section, we first
propose a lightweight approach to recover graph edges Lfrom the
Dr,GCr, andICrfields of each r∈R(4.1). Then, we apply that
approach to the studied communities and analyze the extracted
graphs to addressRQ1 ( 4.2).ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia ToshikiHirao,ShaneMcIntosh,AkinoriIhara,andKenichi Matsumoto
4.1 LinkRecoveryApproach
Weperformapreliminaryanalysisof410randomlyselectedreviews
togaininsightintothelinkagehabitswithinthestudiedcommu-
nities. We observe that links appear in review description fields
(Dr), as well as within general and inline comment threads ( GCr
andICr). Moreover,we observe twowaysthat linksare recorded:
(1)ByChangeID(e.g., Ic8aaa0728a43936cd4c6e1ed590e01b -
a8f0fbf5b ),i.e.,a40-digithexadecimalidentifierassigned
to each review at creation time (prefixed with an Ito avoid
confusionwithGit commit IDs).
(2)ByURL(e.g., https://review.openstack.org/#/c/1111 ).
Thus,torecoverlinksfromareview r∈R,wescanitsdescription
Draswellasallofthegeneralandinlinecomments( GCr,ICr)using
regular expressions. To detect ChangeIDs, our regular expression
scansfortermsoftheform I[0-9a-f]{40} .TodetectURLs,ourreg-
ularexpressionisoftheform https?://PROJ/#/c/[1-9]+[0-9]* ,
wherePROJis replaced with the base URL of the project (e.g.,
review.openstack.org ).FortheChromium community,ourregu-
larexpressionsneededtobeadaptedto: https?://chromium-revi -
ew.googlesource.com/c/REPO/+/[1-9]+[0-9]* ,whereREPOcor-
responds to any of the 410 repositories within the Chromium com-
munity.The linkrecovery processisrepeated ∀r∈R.
4.2 Results
By applying our link recovery approach to the six studied commu-
nities, we set out to better understand (i) the prevalence of review
linkage and (ii) linkage trends over time. To do so, we measure a
linkage rate for each studied system, i.e., the proportion of reviews
that are connectedbyat leastone link.
PrevalenceofLinkage. Table2showsthat17%and25%of Chrom-
iumandOpenStack reviews are connected by at least one link,
respectively. Although the linkage rate in Qt,Eclipse, andAn-
droidreviews are lower (5%ś8%), linkage is not uncommon. How-
ever, the Libreoffice linkage rate is only 3%. We suspect that
this result is reflective of the differences in the importance that
the studied communities have placed on code review. Indeed, re-
viewsinthe OpenStack andChromium communitiesreceive17
and 12 comments on average, whereas reviews in other subject
communities receive 4ś8 comments on average. Two-tailed, un-
pairedMann-WhitneyUtestsbetween OpenStack andtheother
studied communities (after Bonferroni correction to control for
family-wise errors, α=0.05
5=0.01) indicate that OpenStack
receivessignificantlymorecommentsthantheotherstudiedcom-
munities ( p<0.001) with Cliff’s delta effect sizes of negligible
when compared to Chromium (δ=0.021<0.147), small when
comparedto Android andQt(0.147≤δ=0.288,0.287<0.330),
medium when compared to Eclipse(0.330≤δ=0.413<0.474),
and large when compared to Libreoffice (0.474≤δ=0.580).
Furthermore, OpenStack reviews tend to involve more review-
ersthantheotherstudiedcommunities,averagingtworeviewers
per review more than the next highest community ( Chromium ).
Two-tailed, unpaired Mann-Whitney U tests indicate that Open-
Stackreviews have significantly more reviewers than the other
studied communities ( p<0.001) with Cliff’s delta effect sizes of
mediumwhencomparedto Chromium (0.330≤δ=0.407<0.474),●
●●●
●●●●●●
●●
●●
●●
●●
●●
●●●●●●
●●●●●●●●●●●●●●●●●●●●
●●
●●●●●
0.0%10.0%20.0%30.0%40.0%50.0%
2008−102009−042009−102010−042010−102011−042011−102012−042012−102013−042013−102014−042014−102015−042015−102016−042016−102017−042017−102018−04
Studied periodLinked Review RateSubject System●
●OPENSTACK
CHROMIUMAOSP
QTECLIPSE
LIBREOFFICE
Figure 1:Monthly linkagerate inthe studied communities.
and large when compared to Android ,Qt,Eclipse, andLibre-
office(0.474≤δ=0.553,0.520,0.703,0.712). We also find that
there is no tendency where the larger systems are likely to re-
ceivemorecommentsintermsofanindividualcontribution(i.e.,
2ś4 comments per reviewer on average across our studied sys-
tems). Indeed, despite of the significant differences of unpaired
Mann-Whitney U tests across our six studied systems, Cliff’s delta
effectsizesarenegligiblewhencomparedto QtandLibreoffice
(δ=0.051,0.020<0.147),smallwhencomparedto Android and
Eclipse(0.147≤δ=0.219,0.213<0.330), and medium when
comparedto Chromium (0.330≤δ=0.357<0.474).
Table2also shows that the largest proportion of links are recov-
eredfromdiscussionthreads( GC,IC).Indeed,72%ś97%oflinksare
recoveredfrom GCandICthreads.Thisindicatesthatlinkstend
to emergeduringthe reviewratherthanwhen itiscreated.
Furthermore, Table 2shows that 5%ś40% of links connect re-
views across project boundaries. For example, Review #1027045
linksfromthe Novaprojecttothe Cinderprojectof OpenStack .
Furthermore, OpenStack showsagreaterrateofcross-projectlinks
than the other studied communities, likely because the OpenStack
community develops more projects than the other studied commu-
nities.Indeed,Table 1showsthat OpenStack containsatleastfour
times as manyprojectsas the otherstudiedcommunities.
LinkageTrends. Figure1showsthatlinkageratesinthefourcom-
munitiesthathavemadethelargestinvestmentsincodereview(i.e.,
personnel and activity per review) have stabilized or peaked in the
more recent studied periods. Indeed,the linkagerate in the Open-
Stackcommunityhasarapidlyincreasingtrenduntillate2014and
more gradual increases in recent months. The Chromium commu-
nity began with moderate linkage rates until mid-2014, when rates
droppedtobelow 5%. However, therates have climbedabove 30%
5https://review.openstack.org/#/c/102704/TheReviewLinkageGraphforCode ReviewAnalytics ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
Table 2:Review graphcharacteristics inthesubjectcommunities.
ProductLinked Reviews PerReview(Mean) PerReviewer Description GeneralComment Inline Comment Cross-project
#Reviews %Reviews Comments Reviewers Comments #Links %Links #Links %Links #Links %Links #Links %Links
OpenStack 133,650 25% 17 5 3 77,400 28% 151,411 54% 50,110 18% 110,964 40%
Chromium 62,065 17% 12 3 4 10,295 5% 182,929 93% 3,968 2% 44,869 23%
Android 11,584 5% 8 2 3 6,082 22% 21,491 77% 466 2% 2,508 9%
Qt 14,266 8% 8 2 3 574 3% 19,450 87% 2,309 10% 4,353 19%
Eclipse 8,227 8% 6 2 3 1,255 6% 19,995 90% 917 4% 1,359 6%
Libreoffice 1,549 3% 4 2 2 211 10% 1,774 86% 88 4% 107 5%
inmid-2017.Thisgrowthmaybeduetoseveralfactors(e.g.,com-
munityinitiative,growthintaskcomplexity).The QtandAndroid
communitieshadlinkageratesbelow7%untilmid-2014,whenrates
roughlystabilizedat 13%and9%, respectively.
On the other hand, linkage rates remainstable or are decreasing
inthetwostudiedcommunitiesthathavemadetheleastinvestment
in code review. Indeed, the Libreoffice community shows a stable
trend, whilethe Eclipsecommunity’s trend isdecreasing.
RQ1:Linkedreviewsoccurregularlyincommunitiesthathave
madelargeinvestmentsincodereview.Thus,codereviewanalytics
shouldlooktolinkageasapotentialsourceofusefulinformation.
5 QUALITATIVE ANALYSISOFREVIEW
LINKS(RQ2)
Reviews may be linked to other reviews for several reasons. For
example,alinkmayexpressadependencybetweenreviews(e.g.,
ł[Review#106918]6isdependenton[Review#106274]ž)ortheevolu-
tionofanearlierideaintoitscurrentform(e.g.,ł[Review#475649]7
isafollowuppatch to [Review#411830]ž).
Inthissection,wesetouttobetterunderstandtheunderlying
reasonsforreviewlinkagethroughaqualitativeanalysis.Tounder-
standwhyalinkhasbeenrecorded,weuseamanually-intensive
researchmethod,whichcreatespracticallimitationsonthebreadth
of communities that we can analyze. Thus, we elect to analyze the
OpenStack communityÐthe largest and most dynamic graph in
oursetofstudiedcommunities(seeTables 1ś2andFigure 1).The
OpenStack community is composed of several projects, of which,
weselectthetwolargest foranalysis,i.e., Nova(theprovisioning
managementmodule)and Neutron (thenetworkingabstraction
interface). Below, we describe our approach to classify links in the
studiedprojects( 5.1) andpresent the results ( 5.2).
5.1 Approach
We applyanopencoding approach[ 12]to classifyrandomly sam-
pled links between reviews. Open coding is a qualitative data anal-
ysis methodby which artifacts under inspection (review links in
our case) are classified according to emergent concepts (i.e., codes).
Aftercoding,weapplyopencardsorting[ 30]toliftlow-levelcodes
to higher level concepts. Below, we describe our sampling, coding,
andcardsortingprocedures inmore detail.
Sampling. Section4showsthatthereare133,650linkedreviewsin
theOpenStack community,16,144ofwhichappearinthe Novaand
6https://review.openstack.org/#/c/106918/
7https://review.openstack.org/#/c/475649/1/23/2019 Change I7b18b767: Fix wrong exception return in ﬁxed_ips v2 extention | review.openstack Code Review
https://review.openstack.org/#/c/126831/ 3/3Powered by Gerrit Code Review  (2.13.12-11-g1707fec)  | Get Help  | Press '?' to view keyboard shortcuts        body = {"reserve": "None"}  
        self.assertRaises(exceptions.NotFound,  
                          self.client.reserve_fixed_ip,  
                          "my.invalid.ip", body)  
any suggestions?
Eli.
Ken'ichi Ohmichi
Patch Set 3:
@Eli
We faced this kind of problems, and we can change Tempest code  before this like:
 -      self.assertRaises(exceptions.NotFound,  
 +      self.assertRaises((exceptions.NotFound, exceptions.BadRequest),  
                          self.client.reserve_fixed_ip,  
                          "my.invalid.ip", body)  Oct 1 1, 2014↩
Eli Qiao
Patch Set 3:
hi Ken'ichi 
thanks, I'v post a patch to tempest at 
https://review.openstack.org/#/c/127457/
let's wait for it merged ﬁrst.Oct 13, 2014 ↩
Christopher Y eoh
Patch Set 3: Code-Review+2
(pending tempest change merge)Oct 14, 2014 ↩
Alex Xu Patch Set 3: Code-Review+1 Oct 14, 2014
Ken'ichi Ohmichi Patch Set 3: recheck Oct 14, 2014
Elastic Recheck Patch Set 3: I noticed jenkins failed, I think you hit bug(s):  - check-tempest-dsvm-ironic-pxe_ssh-nv: https://bugs.launchpad.net/bugs/12 70710 If you b … Oct 14, 2014
Ken'ichi Ohmichi Patch Set 3: Code-Review+2 Workﬂow+1 Oct 14, 2014
Elastic Recheck Patch Set 3: I noticed jenkins failed, I think you hit bug(s):  - gate-grenade-dsvm: https://bugs.launchpad.net/bugs/1244457 If you believe we've correctl … Oct 14, 2014
Ken'ichi Ohmichi Patch Set 3: recheck Oct 14, 2014
Eli Qiao Patch Set 3: recheck Oct 15, 2014
Jenkins Change has been successfully merged into the git repository. Oct 15, 2014
Jordan Pittier Patch Set 3: Reverted This patchset was reverted in change: I191 b626a471626f334e4835fab602acea505f f78 Feb 10, 2016Reviewer 1
Author @Author 
@Reviewer 1 
Figure2:AnexampleofareviewlinkfromReview#126831
inNova.
Neutron projects. Since coding of all of these links is impractical,
we randomly sample NovaandNeutron reviewlinksfor coding.
To discover as complete of a set of link types as possible, we
strivefor saturation .Similartopriorwork[ 47],wesetoursaturation
criterion to 50, i.e., we continue to code randomly selected links
until nonewcodes have been discoveredfor 50 consecutive links.
To ensure that we analyze links that appear in descriptions and
comments, we aim to achieve saturation twiceÐonce when coding
description-based links and again when coding comment-based
links.Wereachsaturationaftercoding340comment-basedlinks
and146description-basedlinksin Nova,and161comment-based
linksand105description-basedlinksin Neutron .
Coding. Coding was performed by the first and second authors
during collocated coding sessions. Both coders have experience
with code reviews both in research and commercial software devel-
opment settings (acting as both patch authors and reviewers). In
total, thesecoding sessions took 56 hours (or112person-hours).
When codinglinks, the codersfocused on the key reasons why
the link was recorded. For example, Figure 2shows that comments
fromReviewer1on NovaReview#126831haveinspiredtheauthor
tocreateReview#127457.Bothcodersindependentlycodeandthen
discusseachlinkuntilaconsensusisreached.Wealsorecordthe
direction of the link, e.g., Review #126831 links to Review #127457.
In theory, multiple codes may apply to each link; yet in practice,
we find that multi-coded links are rare. Indeed, while a link may
havedifferentcodesifinterpretedindifferentdirections,wedonot
find any multi-codeddirectionallinks.
Since open coding is an exploratory data analysis technique,
it may be prone to errors. To mitigate errors, we code in three
passes. First, since codes that emerge late in the process may apply
toearlierreviews,aftercompletinganinitialroundofcoding,we
perform a second pass over all of the links to correct miscodedESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia ToshikiHirao,ShaneMcIntosh,AkinoriIhara,andKenichi Matsumoto
Table 3: The frequency of the discovered types of review
linkagein OpenStackNova andNeutron .
CategoryFrequency
Nova Neutron
C1: PatchDependency 269(55%) 148(56%)
Patch Ordering 124(26%) 62 (24%)
Root Cause 50 (11%) 28 (11%)
ShallowFix 6(2%) 4(2%)
Follow-up 28 (6%) 29 (11%)
Merge RelatedReviews 19 (4%) 13 (5%)
Multi-part 42 (9%) 12 (5%)
C2: BroaderContext 96 (20%) 50 (19%)
RelatedFeedback 43 (9%) 14 (6%)
Demonstration 29 (6%) 26 (10%)
AdditionalEvidence 24 (5%) 10 (4%)
C3: AlternativeSolution 69 (14%) 39 (15%)
Superseding 35 (8%) 17 (7%)
Duplicated 34 (7%) 22 (9%)
C4: VersionControl Issues 27 (6%) 17 (6%)
IntegrationConcern 15 (4%) 13 (5%)
GerritMisuse 5(2%) 2(1%)
Revert 7(2%) 2(1%)
C5: FeedbackRelated 23 (5%) 10 (4%)
FixRelatedIssues 11 (3%) 3(2%)
FeedbackInspiredReviews 12 (3%) 7(3%)
entries.Duringthefirstcodingpass,wecodeonlyusingthelink
source (description/comment). In several cases, more contextual
informationwasneeded.WecodedsuchcasesasłNeedsAdditional
Contextžduringthefirstcodingpass.Duringathirdcodingpass,
we check additional sources of information (e.g., the content of the
patch,thelinkedreview,commentsindiscussion threads) tocode
thesecasesmorespecifically.Afterthethreecodingpasses,allof
the sampledlinkshave been assignedto aspecific code.
CardSorting. Similartopriorstudies[ 3,18,24,39],weapplyopen
cardsortingtoconstructataxonomyofcodes.Thistaxonomyhelps
us to extrapolate general themes from our detailed coded data. The
card sorting process is comprised of two steps. First, the coded
linksaremergedintocohesivegroupsthatcanberepresentedby
a similar subgraph. Second, the related subgraphs are merged to
form categoriesthat can be summarizedbyashort title.
5.2 Results
Table3provides an overview of the categories that summarize
related labels (the complete table is available online1). We observe
that the frequenciesat which thelink labels appear are consistent
between the two studied projects. Moreover, we only coded two
of 486 links from Novaand two of 266 links from Neutron as
falsepositives(i.e.,spuriouslydetectedlinksthatdonotindicate
a relationship between reviews), suggesting that our link extrac-
tion approach does not produce much noise (precision >0.99 in
both cases).Furthermore,we requiredadditionalcontextinforma-
tion (beyond the link source) to code 63 links, all of which were
more specifically coded during the third pass when we analyzeadditionalinformationsources. Below,wedescribe thediscovered
codes according to the categoriesto whichthey belong.
Patch Dependency (C1). We find that 55% and 56% of the ana-
lyzedlinksin NovaandNeutron connectreviewstoothersthat
they depend upon. Patch Dependency links may influence inte-
gration decisions and the reviewers who should be recommended.
Indeed,theintegrationdecisioninonereviewmaybeinherently
linkedtothat ofanotherif theyshare adependency. For example,
Review #1027045of theNovaproject was only abandoned because
ofitsdependencyonReview#102705,whichwasabandonedearlier.
Moreover, reviewers of a dependent review may need to review its
dependencyaswell.Forexample,areviewerofReview#1027498
was added only because they reviewed its dependency (Review
#101424). We further explore the usefulness of these linkage-based
reviewer invitationsinSection 7(RQ4).
AB
BA
A(1) Patch Ordering
(2) Root Cause
(3) Shallow Fix
(4) Follow-up
(5) Merge Related 
Reviews
(6) Multi-PartB
C(C1) Patch Dependency
CCode 
BaseLinkage direction Patch Integration
Figure 3:The Patch Dependencysubgraphs.
Figure3showsthreeshapesthatpatchdependencylinkstake.
First, Patch Ordering, Root Cause, Shallow Fix, and Follow-up take
theshapeoftwoeventuallyintegrated(orabandoned)reviewsthat
sharealink.Whiletheyshareashape,thesemanticsofthepatterns
differ, i.e., Patch Ordering links indicate a timing dependency that
mustberespectedatintegrationtime,whileFollow-up,RootCause,
andShallowFixlinksproviderationaleforReviewBbypointingto
enabling enhancements or limitations in Review A. Second, Merge
Related Reviews links merge two or more reviews into a more
cohesivewhole.Finally,Multi-partlinksindicatethatalargereview
has been split intoaseries of smallerreviews.
Weißgerber et al.[43] observed that smaller patches tend to
be accepted in two large open source projects. Rigby et al.[35]
argue that one of the statutes of an efficient and effective code
review process is the łearly, frequent review of small, independent,
completesolutionsž.ThefrequencyoftheMulti-partpattern(i.e.,
thesplittingoflargepatchesintosmallerones)maybeanindication
that theseprior observations stillhold.
8https://review.openstack.org/#/c/102749/TheReviewLinkageGraphforCode ReviewAnalytics ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
Broader Context (C2). We find that 20% and 19% of the analyzed
linkspointtootherreviewswithrelevantresources.Theindividual
analysis of reviews that are connected with Broader Context links
may not be valid. Indeed, analyses of review outcome prediction
oftencomputethelengthofdiscussionthreads[ 16,23].However,a
discussion may span across several reviews when Broader Context
links are present. For instance, a reviewer of Review #1552239asks
the authorto refer to asimilar discussiononReview#215608.
(1) Related Feedback
(2) Demonstration
(3) Additional EvidenceABcomment
code(C2) Broader Context (1) or (3)
(2) or (3)
Code 
Base
Figure 4:The Broader Contextsubgraph.
Figure4shows that our three codes within the Broader Con-
text category share the same shape; however, the codes differ in
the artifact to which they refer. Related Feedback links connect
discussions on one review to discussions in other reviews, while
Demonstration links point to example code from other reviews.
Additional Evidence links point to other reviews as proof (code,
discussions, specifications) of the existence, removal, or relevance
oftheproblemsthatareaddressedbythereviewunderinspection.
Alternative Solution(C3). Wefindthat14%and15%oftheana-
lyzed links connectreviews to others thatimplement similar func-
tionality. Similarto Patch Dependency links, Alternative Solution
links may also impact integration decisions and reviewer recom-
mendations.Forexample,Review#6743110wasabandonedbecause
another submitted solution forthe same underlying issue (Review
#61041)waspreferred.Especiallyinsuchexampleswhereanłei-
ther orž decision needs to be made, the same reviewers should
likely be invited to all of the competing reviews for the sake of
fairness[15].Furthermore,priorworkhasdemonstratedthatalack
of awareness of concurrently developed solutions may result in re-
dundantwork[ 10,49]andisakeysourceofsoftwaredevelopment
waste[37].Theseconflatedintegrationdecisionsarenotcongruent
with review outcome or reviewer recommendation models that
assumeeachsubmissionisindependently adjudicated[ 21ś23].
(C3) Alternative Solution
(1) Superseding
(2) DuplicationA’
B ACode 
Base(1)
(2)
Figure 5:The Alternative Solutionsubgraph.
9https://review.openstack.org/#/c/155223/
10https://review.openstack.org/#/c/67431/Figure5showsthatourtwocodeswithintheAlternativeSolu-
tion category share the same shape, yet differ in their semantics.
Supersedinglinksshowthatthesolutioninanearlierreviewhas
beenreplacedwithanupdatedsolutioninthecurrentreview,while
Duplication links highlight the existence of another (competing)
solution to the same underlying problem. In a large-scale, cross-
company software organization like OpenStack , it is difficult to
coordinatedevelopmenteffort.However,thefrequencyatwhich
work isduplicatedsuggeststhat tooling [ 10,49]mayhelp.
Version Control Issues (C4). We find that 6% of analyzed links
point to reviews that introduced version control issues. Rigby and
Storey[36]alsofoundsuchissuesareoftendiscussedduringthe
broadcast-based reviews in several open source systems. Shima-
gakietal.[38]foundthat5%ofcommitsinalargeindustrialsystem
wererevertedafterbeingintegrated.SinceRevertisoneofthecodes
within our category, our review graphs can complement version
controldatatobetterunderstandthepracticeofrevertingcommits.
AB(1) Integration Concerns
(2) Gerrit Misuse
(3) Revert(C4) Version Control Issues
Conﬂict Code 
Base
Figure 6:The VersionControlIssues subgraph.
Figure6shows thatour three codes withintheVersionControl
Issues category share the same shape. Integration Conflict and
Gerrit Misuse links expose technical integration or Gerrit issues,
whileRevertlinksindicatethat apartialorcomplete rollback.
FeedbackRelated(C5). Wefindthat5%and4%ofanalyzedlinks
inNovaandNeutron connect reviews to others that resolve or
wereinspiredbyreviewercomments.Reviewsthatwereinspired
by feedback in another review might be more likely to be accepted,
since one reviewer is already in favour of the idea. For example, in
Review#167100,11areviewer’sfeedbackinspiredthecreationof
the new Review #167082. Then, one of reviewers of #167100 joined
Review #167082, and eventually approves it for integration. There
are twopossiblewaystoactuponC5-linkedreviews.The reviewer
who inspired the change may be well suited to review the inspired
change. Thus, reviewer recommenders may need to recommend
them. On the other hand, since the reviewer who inspired the
changemaynotbeimpartialwhenreviewingtheinspiredreview,
reviewer recommenders may need to recommend other reviewers.
Figure7shows thatour two codes within the Feedback Related
categorysharethesameshape.FixedRelatedIssueslinksshowthat
(part of) a raised concern has been addressed by another review.
Feedback-inspiredlinksshowanewcontributionwherefeedback
onReviewA inspires the creation of anewpatch.
RQ2:A broad variety of reasons for linkage exist. These differ-
ent types of links may introduce noise in or opportunities for
improvement ofcode review analytics.
11https://review.openstack.org/#/c/167100/ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia ToshikiHirao,ShaneMcIntosh,AkinoriIhara,andKenichi Matsumoto
(C5) Feedback Related
(1) Feedback Inspired
(2) Fix Related Issues
AB
C(1)
(2) Code 
Base
Figure 7:The FeedbackRelated subgraph.
6 AUTOMATED LINK CLASSIFICATION (RQ3)
InSection 5,wefindthatseveraltypesoflinksmayimpactreviewer
recommendation and outcome prediction. Since different review
analyticstechniquesmayneedtotraverseorignorelinksdepending
on the type, a more scalable approach to link type recovery is
needed.Indeed,ittooktheauthors112person-hourstocode752
reviewlinks(seeCodinginSection 5.1).Ifwecontinuetocodeat
thisrate,itwouldtakeanadditional19,793person-hourstocode
the remaining 132,898 reviewsinthe OpenStack data set.
Inthissection,westudythefeasibilityofusingmachinelearning
techniquestoautomaticallyclassifylinksbycategories.Todoso,
we use the manually coded data from Section 5as a sample on
whichtotrainandevaluateclassifiersthatidentifythelinkcategory
(C1śC5) based on the document that it appears within (i.e., the
review description field or the comment in the general or inline
discussion thread). Below, we describe our approach to automated
linkcategory classification ( 6.1) followedbythe results ( 6.2).
6.1 Classification Approach
Feature Extraction. Weapplystandardtextpreprocessingtech-
niques to lessen the impact of noise on our classifiers. We first
tokenizethedocumentandremovestopwordsusingthePython
NLTK stop word list. Next, we apply lemmatization to handle term
conjugationusingthePythonNLTK lemmatize function.Finally,
weconverteachsampleddescriptionorcommenttoavectorofthe
Term Frequency-Inverse Document Frequency (TF-IDF) weights
of its terms. Broadly speaking, terms that appear rarely across doc-
uments, and/or often within one document are of higher weight.
We use the Python Scikit-Learn TfidfVectorizer function to
compute TF-IDF scores for alldocuments inatraining sample.
Classifier Validation Technique. To estimate classifier perfor-
manceonunseendata,weapplytheout-of-samplebootstrapval-
idation technique [ 13], which tends to yield more robust results
than other validation techniques (e.g., k-fold cross validation) [ 40].
First, a bootstrap sample of size N is randomly drawn with replace-
mentfromtheoriginalsampleofthesamesizeN.Thisbootstrap
sampleisusedtotrainourclassifiers,whilethedocumentsfrom
the original sample that do not appear in the bootstrap sample are
set aside for testing. Since the bootstrap sample is selected with
replacement,onaverage,36.8%ofthedocumentswillnotappear
inthebootstrapsampleandcanbeusedtoevaluateclassifierper-
formance.We perform 1,000 iterations of the bootstrapprocedure
(reporting the mean performance scores across these iterations) to
ensure that our performance measurements are robust.Classification Techniques. To train our classifiers, we experi-
ment with a broad selection of popular classification techniques.
Support Vector Machines (SVM) use a hyperplane to classify docu-
ments by first transforming feature values into a multidimensional
feature space. Random Forest is an ensemble learning technique
that builds a large number of decision trees, each using a subset of
the features, and then aggregates the results from each tree to clas-
sifydocuments.MultinomialNaïveBayes(MNB)isaconditional
probabilitymodelthatusesamultinomialdistributionforeachof
thefeatures.Multi-LayerPerceptron(MLP)isasupervisedlearning
techniquewhereweightedinputsaredeliveredthroughneuronsin
sequentiallayers.MultinomialLogisticRegression(MLR)general-
izesthelogisticregressiontechniquetothemulti-classclassifica-
tionsetting. We use the Python Scikit-Learn implementations of
theclassificationtechniques( svm.SVC,RandomForestClassifier ,
MultinomialNB ,MLPClassifier ,andLogisticRegression ).
Hyperparameter Optimization. The classification techniques
that we use have configurable parameters that impact their per-
formance.Similartopriorwork[ 40],weuseagridsearchtotune
theparametersettings.Gridsearchisanexhaustivesearchingtech-
nique that examines all of the combinations of a specified set of
candidate settings to find the best combination. We explore the
same set of candidate settings as Tantithamthavorn et al.[40, p. 5].
Wesearchfortheoptimalparametersettingsforeachclassification
technique ineach bootstrap sample (i.e.,without using the testing
data) using the Scikit-Learn GridSearchCV function.
PerformanceEvaluation. Toevaluateourclassifiers,weusecom-
mon performance measures. Precision is the proportion of links
that are classified as a given category that are correct. Recall is the
proportionoflinksofagivencategorythataclassifiercandetect.
The F1-score is the harmonic mean of precision and recall. The
Area Underthe Curve (AUC) computes the area under the curve
that plots the true positive rate against the false positive rate as
the threshold that is used for classifying documents varies. AUC
rangesfrom0to1,whererandomguessingachievesanAUCof0.5.
Sinceourlinkshavemorethantwocategories,weneedtouse
multi-classgeneralizationsoftheseperformancemeasures.Each
measure is computed for each category before being aggregated
into an overall score. Since the link categories are imbalanced (see
Table3), we weighthe category scoresby their overall proportion.
WealsocompareourclassifierstoaZeroRclassifier,whichal-
ways reports the most frequently occurring class. In our setting,
a ZeroR classifier achieves a recall of one and a precision equal
to the frequency of the most frequently occurring category (C1)
forthatclass,andaprecisionandrecallofzerofortheothercate-
gories.Notethat AUCdoes not apply to ZeroRclassifiers because
likelihood estimates are not produced. We use the Scikit-Learn
metricslibrary to compute our performance measurements.
6.2 Results
Table4showsthatwhilenoclassificationtechniqueconsistently
outperforms the others, the classifiers achieve a precision of 0.71ś
0.77,a recallof0.72ś0.92,and F1-scoresof0.71ś0.79.Since these
performance scores are on par with those of prior classification
studies [28,32,33], we believe that our classifiers show promise.
Moreover,Table 4showsthatourclassifiersoutperformbaselineTheReviewLinkageGraphforCode ReviewAnalytics ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
Table 4: The performance of our five link category classi-
fiers,allofwhichoutperformtheZeroRandrandomguess-
ingbaselines inallcases.
ModelNova Neutron
Prec. Rec. F1. AUC Prec. Rec. F1. AUC
SVM 0.75 0.88 0.79 0.72 0.72 0.92 0.77 0.82
RF 0.77 0.72 0.71 0.57 0.76 0.84 0.72 0.89
MNB 0.71 0.80 0.74 0.72 0.71 0.82 0.75 0.76
MLP 0.74 0.81 0.76 0.61 0.75 0.84 0.74 0.68
MLR 0.75 0.88 0.79 0.65 0.72 0.92 0.77 0.79
ZeroR 0.26 0.50 0.34 ś 0.27 0.50 0.34 ś
approaches,achievingprecision,recall,andF1-scoresthatare22ś51
percentage points better than the ZeroR baseline and AUC values
above the 0.5 random guessingbenchmark.
RQ3:Despite the complexity of the five-class classification prob-
lem, our classifiers achieve precision, recall, and F1-scores that
exceed theZeroRbaselineby22ś51percentagepoints.
7 LINKAGEIMPACTANALYSIS(RQ4)
Our analysis in RQ2 suggests that linkage can impact code review
analytics. In this section, we measure that impact on reviewer
recommendation ( 7.1) andreviewoutcome prediction ( 7.2).
7.1 ReviewerRecommendation
Approach. In Section 5,we observe that Patch Dependency links
(C1), Alternative Solution links (C3), and Feedback Related links
(C5) may impact reviewer recommendation because reviewers of a
linking reviewmayneedto reviewits linkedreview.
To measure the degree to which reviewers contribute to both
linkingandlinkedreviews,wecomputethe OverlappingReviewer
Rate(ORR),i.e.,theproportionofreviewersfromthelinkingreview
whoalsoreviewthelinkedreview.WecomputetheORRrateson
our sampledlinksfrom Section 5.
To study the extent to which state-of-the-art reviewer recom-
menders identify overlapping reviewers, we apply cHRev [ 48],
which makes recommendations based on prior contribution and
workinghabits.WeapproximatedatasetswhereC1,C3,andC5-
linkedreviewsareknownbyapplyingourtop-performingclassifier
(SVM) from Section 6to the linked NovaandNeutron reviews.
We exclude C5-linked reviews from further analysis because we
detect too few instances (i.e., five) to draw meaningful conclusions.
We then compare the performance of cHRev to an extended ver-
sionthatranksreviewersoflinkingreviewsatthetopofthelistfor
linkedreviews.Sincelinksappearasreviewsevolve,weselectonly
those links that are available at prediction times zero, one, three,
and sixhoursafter the reviewhas been created.Moreover, we only
identify candidate overlapping reviewers who have commented on
linking reviewsat orbefore thoseprediction time settings.
Results. TheORRratesofC1andC3are50%ś51%and65%ś77%,
respectively.Bywayofcomparison,wefindthattheORRrateis
lessthan1%forrandomlyselectedpairsofnon-linkedreviews.The
resultsindicatethatreviewersaremorelikelytoparticipateinboth
reviewswhen linksare present.A closer inspection reveals that cHRev misses at least one over-
lapping reviewer in 96%ś100% of linked reviews across Top 1ś5
recommendation lists. Since medians of 12 and 15 reviewers par-
ticipate in NovaandNeutron reviews, respectively, missing at
least one overlapping reviewer is a concern. Moreover, none of the
overlapping reviewers are recommended in 41%ś81% of Novaand
48%ś84% of Neutron reviews. When overlapping reviewers are
omitted, they appear in 13thś15th place on average. Increasing the
weightofoverlappingreviewers mayimprove recommendations.
Table5shows that the precision, recall, and F1-scores of cHRev
improve by 33%ś88% (3ś17 percentage points) if reviewers of a
linked review are recommended at the top of the list. Moreover,
the degree ofthe improvement remainsconsistent acrossthe four
studiedpredictiontimesettings,indicatingthatnopredictiondelay
isnecessarytogainthebulkofthevalue.Longerdelaysmayachieve
better results but would be less useful in practice, since waiting
morethansixhoursforreviewerrecommendationsisimpractical.
7.2 ReviewOutcome
Approach. InSection 5,we reportthatC1,C2 (BroaderContext),
andC3linksmayimpactreviewoutcomepredictionbecausethe
integrationdecisioninonereviewmaybeinherentlylinkedtothat
of theother.Similarto thereviewerrecommendation experiment
above, we apply our SVM classifier to identify C1, C2, and C3 links
inthe full NovaandNeutron data sets.
ForC1,C2,andC3reviews,wecomputethe IdenticalOutcome
Rate(IOR),i.e., therate atwhich linking andlinked reviews result
inthe same outcome (i.e.,integration or abandonment). Moreover,
to study the extent to which state-of-the-art outcome predictors
misclassifyidenticaloutcomes,weapplytheoutcomeprediction
approachofGousios etal.[16].Morespecifically,wetrainprediction
modelsthatclassifyreviewsasintegratedorabandonedbasedon
review properties (e.g., # comments, # participants). In our setting,
wetrainthepredictorsonunlabeledlinkedreviewsandevaluate
themonour labeledsamples.
Results. The IOR rates of integrated C1 and C2-linked reviews
are 73%ś87% in Novaand 55%ś71% in Neutron , while the IOR
rates for abandonment are 57%ś86% and 45%ś75%, respectively.
ThissuggeststhatreviewsthatareconnectedwithC1andC2links
tend to have the same outcome. Since C3 links connect competing
solutions,itisunlikelythattheywillhavethesameoutcome.Thisis
reflectedinlowerIORratesof18%ś26%forintegration,respectively.
Ontheotherhand,theIORratesforabandonmentare46%in Nova
and 62% in Neutron , indicating that it is not uncommon for both
competing solutions to be abandoned.
Outcomepredictorsmaymisclassifyreviewoutcomesforlinked
reviewswhenthefeaturevaluesspanmultiplereviews(e.g.,discus-
sioncontexts,#participants).Indeed,wefindthatpriorapproach
misclassifies35%and39%ofC1,C2,andC3-linkedreviewsin Nova
andNeutron ,respectively.
RQ4:Reviewerrecommenderstendtoomitorpoorlyrankreview-
ers who participate in both linking and linked reviews. Moreover,
reviewoutcomepredictorstendtomisclassifylinkedreviewout-
comes. Leveraging links that are available at prediction time can
yieldconsiderable performance improvements.ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia ToshikiHirao,ShaneMcIntosh,AkinoriIhara,andKenichi Matsumoto
Table 5: The mean performance scores of cHRev [ 48] and our proposed link-aware reviewer recommenders at different pre-
diction delays.The bulk oftheperformanceimprovementisachieved intheno delay(0hour) setting.
Nova Neutron
Top1 Top3 Top5 Top1 Top3 Top5
Model Pre. Rec. F1. Pre. Rec. F1. Pre. Rec. F1. Pre. Rec. F1. Pre. Rec. F1. Pre. Rec. F1.
Baseline 0.25 0.06 0.09 0.28 0.18 0.22 0.30 0.32 0.30 0.24 0.05 0.08 0.26 0.17 0.20 0.28 0.31 0.29
0 hour 0.39 0.09 0.14 0.40 0.26 0.31 0.40 0.43 0.41 0.39 0.09 0.14 0.40 0.26 0.31 0.41 0.44 0.42
1 hour 0.39 0.09 0.14 0.40 0.26 0.31 0.40 0.43 0.41 0.41 0.09 0.14 0.41 0.26 0.32 0.42 0.44 0.43
3 hours 0.40 0.09 0.14 0.40 0.26 0.31 0.40 0.43 0.41 0.41 0.09 0.14 0.41 0.26 0.32 0.42 0.45 0.43
6 hours 0.40 0.09 0.14 0.40 0.26 0.31 0.40 0.43 0.41 0.41 0.09 0.15 0.41 0.27 0.32 0.42 0.45 0.43
8 PRACTICALIMPLICATIONS
Linkage should be taken into consideration in code review
analytics. Our quantitative study (Section 4) shows that up to
25% of reviews in our subject communities link to at least one
other review. Moreover, in the four studied communities that have
madethelargestinvestmentinreviewing( OpenStack ,Chromium ,
Android , andQt), we observe increasing or stable trends in the
monthlylinkagerate.Priorworkhasshownthatlinkedartifactscan
impactrepositorymininganalytics[ 8,20].Indeed,ourimpactanal-
ysis(Section 7)showsthatreviewerrecommendersandoutcome
predictorscan be improvedbytaking linkage intoaccount.
Duplicatedetectionwouldenhancereviewefficiency. Ourqual-
itativestudyshows that14%ś15%ofcongruent linksin Novaand
Neutron fallintheAlternativeSolutioncategory(i.e.,superseding
and duplication). Duplicate contributions are a form of waste in
software development [ 37]. Boisselle and Adams [ 8] argued that
automatic classification of bug reports that are linked due to dupli-
cationwouldbehelpful.However,inlargeprojectslike OpenStack
andChromium , it is difficult to keep track of duplicated work [ 49].
Systematicdetectionofduplicateswouldbeimportantfromare-
viewfairness[ 15]perspectiveaswell,sincethecompetingsolutions
should be subjectto the same level ofscrutiny.
Linkcategoryrecoverymaynotbenecessary. Themajorityof
linksareoftypesthatpotentiallyimpactreviewerrecommendation
(C1, C3, and C5) and outcome prediction (C1, C2, and C3). Since
we find that all studied link categories are useful, future work may
assumethat alllinksare useful andomit linktype classification.
9 THREATS TO VALIDITY
Construct validity. Constructvalidityisconcernedwiththede-
greetowhichourmeasurementscapturewhatweaimtostudy.We
recoverlinksbetweenreviewsusingregularexpressionsthatdetect
Change IDs and URLs of other reviews. However, these regular
expressions may extract Change IDs or URLs that are not intended
tobelinks(falsepositives).Ontheotherhand,Section 5showsthat
thefalsepositiverateinourmanuallyanalyzedsampleisquitelow
(<0.1%).Thus, we false positives donot appear to be ofconcern.
In our qualitative analysis, links may be miscoded due to the
subjective nature of our open coding approach. We take several
precautions to mitigate the miscoding threat. First, the code for
each link is agreed upon by two coders who have experience with
codereviewinacademicandcommercialsettings.Furthermore,we
employ a three-pass approach, where each code is revisited at least
onceto ensure that the correctcode wasselected.Internal validity. Internalvalidityisconcernedwithourability
to draw conclusions from the relationship between study variables.
Links may not be detected in all of the cases when they should
be,whichmayintroducenoisein ourlinkagerateobservationsin
Section4.Hence,ourobservationsinSection 4shouldbeinterpreted
as lower bounds ratherthanas exact linkage ratevalues.
Ifwe stopcoding too earlyduringour qualitative analysis (Sec-
tion5),itmaythreatenthecompletenessofthediscoveredsetof
link types. To mitigate the threat, we continue to code until our
samples saturateÐa concept that we operationalized by continuing
until we coded a span of 50 coded links without discovering any
newcodes. Othershave usedsimilar saturationcriteria[ 36,47].
Other classification techniques or hyperparameter settings may
yieldbetterresultsthantheonesthatwestudiedinSection 6.To
combat this, we select a broad set of popular classification tech-
niquesanduseanautomaticparameteroptimizationapproachto
selectthebestconfigurationofhyperparametersettingsforeach
bootstrap iteration. Nonetheless, exploration of other classification
techniques andhyperparameter settings mayyieldbetterresults.
External validity. External validity is concerned with our abil-
ity to generalize based on our results. We extract review graphs
(Section4)fromsixcommunitiesthatusetheGerritcodereview
tool. Due to the manually intensive nature of our coding approach,
wefocusonanin-depthanalysisofthetwolargestprojectsfrom
theOpenStack community. As such, our linkage types, classifiers,
and impact analyses may not generalize to code reviewing envi-
ronments inall software communities. Replication studies may be
neededtoarriveatmoregeneralconclusions.Tosimplifyreplica-
tion,we have madeour scripts anddata publiclyavailable.1
10 CONCLUSION
Researchershaverecentlyproposedseveralanalytics-basedtech-
niquestosupportstakeholdersintheMCRprocess.However,those
techniqueshave tacitlyorexplicitly treated eachreview as aninde-
pendentobservation,whichoverlooksrelationshipsamongreviews.
Our empirical study suggests that linkage is not uncommon
in six studied software communities. Moreover, adding linkage
awareness to review analytics approaches yields considerable per-
formanceimprovements.Thus,reviewlinkageshouldbetakeninto
considerationinfuture MCRstudiesandtools.
ACKNOWLEDGMENTS
This work was supported by the SCAT Technology Research Foun-
dationandJSPSKAKENHIGrantsJP17J09333,17H00731,18KT0013.TheReviewLinkageGraphforCode ReviewAnalytics ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
REFERENCES
[1]SultanS.Alqahtani,EllisE.Eghan,andJuergenRilling.2017.RecoveringSemantic
Traceability Links between APIs and Security Vulnerabilities: An Ontological
ModelingApproach.In ProceedingsofInternationalConferenceonSoftwareTesting,
Verification and Validation . 80ś91.
[2]Giuliano Antoniol, Gerardo Canfora, Gerardo Casazza, Andrea De Lucia, and Et-
toreMerlo.2002. Recoveringtraceabilitylinksbetweencodeanddocumentation.
IEEE Transactions onSoftwareEngineering 28,10,970ś983.
[3]AlbertoBacchelliandChristianBird.2013. Expectations,Outcomes,andChal-
lengesofModernCodeReview.In Proceedingsofthe35thInternationalConference
onSoftwareEngineering . 712ś721.
[4]VipinBalachandran.2013. ReducingHumanEffortandImprovingQualityinPeer
Code Reviews Using Automatic Static Analysis and Reviewer Recommendation.
InProceedings ofthe 35th International Conferenceon SoftwareEngineering . 931ś
940.
[5]OlgaBaysal,OleksiiKononenko,ReidHolmes,andMichaelW.Godfrey.2012.The
SecretLifeofPatches:AFirefoxCaseStudy.In Proceedingsofthe19thWorking
Conference onReverse Engineering . 447ś455.
[6]Olga Baysal, Oleksii Kononenko, Reid Holmes, and Michael W. Godfrey. 2013.
The influence of non-technical factors on code review. In Proceedings of the 20th
Working Conference onReverse Engineering . 122ś131.
[7]Olga Baysal, Oleksii Kononenko, Reid Holmes, and Michael W. Godfrey. 2016.
Investigating Technical and Non-Technical Factors Influencing Modern Code
Review,InEmpiricalSoftwareEngineering. EmpiricalSoftwareEngineering 21,3,
932ś959.
[8]VincentBoisselleandBramAdams.2015. Theimpact ofcross-distributionbug
duplicates, empirical study on Debian and Ubuntu. In Proceedings of the 15th
InternationalWorkingConferenceonSourceCodeAnalysisandManipulation .131ś
140.
[9]AmiangshuBosu,MichaelaGreiler,and Christian Bird.2015. Characteristics of
UsefulCodeReviews:AnEmpiricalStudyatMicrosoft.In Proceedingsofthe12th
InternationalConference onMiningSoftwareRepositories . 146ś156.
[10]YuriyBrun,ReidHolmes,MichaelD.Ernst,andDavidNotkin.2011. Proactive
DetectionofCollaborationConflicts.In Proceedingsofthe8thJointMeetingon
European Software EngineeringConferenceand theSymposiumontheFoundations
ofSoftwareEngineering . 168ś178.
[11]Gerardo Canfora, Luigi Cerulo, Marta Cimitile, and Massimiliano Di Penta. 2011.
Social Interactions Around Cross-system Bug Fixings: The Case of FreeBSD
andOpenBSD.In Proceedingsofthe8thWorkingConferenceonMiningSoftware
Repositories . 143ś152.
[12] Kathy Charmaz.2014. ConstructingGroundedTheory . SAGE Publications.
[13]BradleyEfronandRobertJ.Tibshirani.1993. AnIntroductiontotheBootstrap.
Chapman& Hall .
[14]MichaelE.Fagan.1976.DesignandCodeInspectionstoReduceErrorsinProgram
Development. IBM SystemsJournal 15,3,182ś211.
[15]DanielM.German,GregorioRobles,GermánPoo-Caamaño,XinYang,Hajimu
Iida, and Katsuro Inoue. 2018. łWas My Contribution Fairly Reviewed?ž: A
Framework to Study the Perception of Fairness in Modern Code Reviews. In
Proceedingsofthe40thInternationalConferenceonSoftwareEngineering .523ś534.
[16]Georgios Gousios, Martin Pinzger, and Arie van Deursen. 2014. An Exploratory
Study of the Pull-based Software Development Model. In Proceedings of the 36th
InternationalConference onSoftwareEngineering . 345ś355.
[17]Jin L. C. Guo, Jinghui Cheng, and Jane Cleland-Huang. 2017. Semantically
EnhancedSoftwareTraceabilityUsingDeepLearningTechniques.In Proceedings
ofthe 39thInternationalConference onSoftwareEngineering . 3ś14.
[18]Anja Guzzi, Alberto Bacchelli, Michele Lanza, Martin Pinzger, and Arie van
Deursen. 2013. Communication in Open Source Software Development Mailing
Lists. InProceedings of the 10th International Conference on Mining Software
Repositories . 277ś286.
[19]Kazuki Hamasaki, Raula Gaikovina Kula, Norihiro Yoshida, A. E. Camargo Cruz,
Kenji Fujiwara, and Hajimu Iida. 2013. Who Does What During a Code Review?
DatasetsofOSSPeerReviewRepositories.In Proceedingsofthe10thInternational
Conference onMiningSoftwareRepositories . 49ś52.
[20]Hideaki Hata, Christoph Treude, Raula G. Kula, and Takashi Ishio. 2019. 9.6
Million Links in Source Code Comments: Purpose, Evolution, and Decay. In
Proceedingsofthe41stInternational ConferenceonSoftwareEngineering .1211ś
1221.
[21]Vincent J. Hellendoorn, Premkumar T. Devanbu, and Alberto Bacchelli. 2015.
Will They Like This?Evaluating Code Contributions withLanguage Models. In
Proceedingsof the12thInternationalConference onMiningSoftwareRepositories .
157ś167.
[22]Gaeul Jeong, Sunghun Kim, Thomas Zimmermann, and Kwangkeun Yi. 2009.
Improving Code Review by Predicting Reviewers and Acceptance of Patches . Tech-
nical Report. Reserach On Software Analysis for Error-free Computing. 215ś226
pages.
[23]YujuanJiang,BramAdams,andDanielM.German.2013. WillMyPatchMake
It?AndHowFast?:CaseStudyontheLinuxKernel.In Proceedingsofthe10thInternationalConference onMiningSoftwareRepositories . 101ś110.
[24]Noureddine Kerzazi, Foutse Khomh, and Bram Adams. 2014. Why do Auto-
mated Builds Break? An Empirical Study. In Proceedings of the 30th International
Conference onSoftwareMaintenance and Evolution . 41ś50.
[25]OleksiiKononenko,OlgaBaysal,andMichaelW.Godfrey.2016. CodeReview
Quality:HowDevelopersSeeIt.In Proceedingsofthe38thInternationalConference
onSoftwareEngineering . 1028ś1038.
[26]VladimirKovalenko,NavaTintarev,EvgenyPasynkov,ChristianBird,andAl-
bertoBacchelli.2018. DoesReviewerRecommendationHelpDevelopers? IEEE
Transactions onSoftwareEngineering .
[27]LishaLi,ZhileiRen,XiaochenLi,WeiqinZou,andHeJiang.2018. HowareIssue
UnitsLinked?EmpiricalStudyontheLinkingBehaviorinGitHub.In Proceedings
ofthe 25thAsia-PacificSoftwareEngineering Conference . 386ś395.
[28]ZhixingLi,YueYu,GangYin,TaoWang,QiangFan,andHuaiminWang.2017.
AutomaticClassificationofReviewCommentsinPull-basedDevelopmentModel.
InProceedingsofthe29thInternationalConferenceonSoftwareEngineeringand
KnowledgeEngineering .
[29]WanwangyingMa,LinChen,XiangyuZhang,YumingZhou,andBaowenXu.
2017. How Do DevelopersFixCross-Project CorrelatedBugs? ACase Study on
the GitHub Scientific Python Ecosystem. In Proceedings of the 39th International
Conference onSoftwareEngineering . 381ś392.
[30]Peter Morville and Louis Rosenfeld. 2006. Information Architecture for the World
WideWeb: DesigningLarge-ScaleWeb Sites . O’ReillyMedia.
[31]MohammadM.Rahman,ChanchalK.Roy,andJasonA.Collins.2016. CORRECT:
CodeReviewerRecommendationinGitHubBasedonCross-ProjectandTechnol-
ogyExperience.In Proceedingsofthe38thInternationalConferenceonSoftware
Engineering . 222ś231.
[32]MohammadMasudurRahman, ChanchalK.Roy,and RaulaG. Kula.2017. Im-
pact of Continuous Integration on Code Reviews. In Proceedings of the 14th
InternationalConference onMiningSoftwareRepositories . 499ś502.
[33]MohammadMasudurRahman,ChanchalK.Roy,andRaulaG.Kula.2017. Predict-
ingUsefulnessofCodeReviewCommentsUsingTextualFeaturesandDeveloper
Experience.In Proceedingsofthe14thInternationalConferenceonMiningSoftware
Repositories . 215ś226.
[34]MichaelRath,JacobRendall,JinL.C.Guo,JaneCleland-Huang,andPatrickMäder.
2018. Traceability in the Wild: Automatically Augmenting Incomplete Trace
Links. In Proceedings of the 40th International Conference on Software Engineering .
834ś845.
[35]PeterC.Rigby,DanielM.German,andMargaret-AnneStorey.2008. Opensource
software peer review practices: a case study of the apache server. In Proceedings
ofthe 30thInternationalConference onSoftwareEngineering . 541ś550.
[36]Peter C. Rigby and Margaret-Anne Storey. 2011. Understanding Broadcast Based
Peer Reviewon OpenSource Software Projects. In Proceedings ofthe33rdInter-
nationalConference onSoftwareEngineering . 541ś550.
[37]ToddSedano,PaulRalph,andCécilePéraire.2017. SoftwareDevelopmentWaste.
InProceedings ofthe 39th InternationalConferenceon SoftwareEngineering . 130ś
140.
[38]Junji Shimagaki, Yasutaka Kamei, Shane McIntosh, David Pursehouse, and Naoy-
asuUbayashi.2016. WhyareCommitsbeingReverted?AComparativeStudy
ofIndustrialandOpenSourceProjects.In Proceedingsofthe 32ndInternational
Conference onSoftwareMaintenance and Evolution . 301ś311.
[39]Mini Shridhar, Bram Adams, and Foutse Khomh. 2014. A Qualitative Analysis of
SoftwareBuildSystemChangesandBuildOwnershipStyles.In Proceedingsofthe
8thInternationalSymposiumonEmpiricalSoftwareEngineeringandMeasurement .
29:1ś29:10.
[40]C.Tantithamthavorn,S.McIntosh,A.E.Hassan,andK.Matsumoto.2018. The
ImpactofAutomatedParameterOptimizationonDefectPredictionModels. IEEE
Transactions onSoftwareEngineering , 1ś1.
[41]Yida Tao, Donggyun Han, and Sunghun Kim. 2014. Writing Acceptable Patches:
AnEmpiricalStudyofOpenSourceProjectPatches.In Proceedingsofthe30th
InternationalConference onSoftwareMaintenance and Evolution . 271ś280.
[42]Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Raula Gaikovina Kula,
NorihiroYoshida,HajimuIida,andKenichiMatsumoto.2015.WhoShouldReview
My Code? A File Location-Based Code-Reviewer Recommendation Approach
for Modern Code Review. In Proceedingsofthe 22nd InternationalConference on
SoftwareAnalysis, Evolution, and Reengineering . 141ś150.
[43]Peter Weißgerber, Daniel Neu, and Stephan Diehl. 2008. Small patches get in!. In
Proceedingsofthe5thInternationalConferenceonMiningSoftwareRepositories .
67ś76.
[44]XinXia,DavidLo,XinyuWang,andXiaohuYang.2015. WhoShouldReviewThis
Change?PuttingTextandFileLocationAnalysesTogetherforMoreAccurate
Recommendations..In Proceedingsofthe31stInternationalConferenceonSoftware
Maintenance and Evolution . 261ś270.
[45]XinYang,RaulaGaikovinaKula,NorihiroYoshida,andHajimuIida.2016. Mining
theModernCodeReviewRepositories:ADatasetofPeople,ProcessandProduct.
InProceedingsofthe13thInternationalConferenceonMiningSoftwareRepositories .
460ś463.ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia ToshikiHirao,ShaneMcIntosh,AkinoriIhara,andKenichi Matsumoto
[46]Fiorella Zampetti, Luca Ponzanelli, Gabriele Bavota, Andrea Mocci, Massimil-
ianoDiPenta,andMicheleLanza.2017.HowDevelopersDocumentPullRequests
withExternalReferences.In Proceedingsofthe25thInternationalConferenceon
ProgramComprehension . 23ś33.
[47]Farida El Zanaty, Toshiki Hirao, Shane McIntosh, Akinori Ihara, and Kenichi
Matsumoto. 2018. An Empirical Study of Design Discussions in Code Review. In
Proceedingsofthe12thInternationalSymposiumonEmpiricalSoftwareEngineeringand Measurement . 11:1ś11:10.
[48]Motahareh Zanjani, Huzefa Kagdi, and Christian Bird. 2015. Automatically
RecommendingPeerReviewersinModernCodeReview. IEEETransactionson
SoftwareEngineering 42,6,530ś543.
[49]Shurui Zhou, Ştefan Stănciulescu, Olaf Leßenich, Yingfei Xiong, Andrzej
Wa ¸sowski,andChristianKästner.2018. IdentifyingFeaturesinForks.In Proceed-
ingsofthe 40thInternationalConference onSoftwareEngineering . 105ś116.