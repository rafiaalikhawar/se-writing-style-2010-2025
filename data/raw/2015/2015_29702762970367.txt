Multi-objective Test Report Prioritization using
Image Understanding
Y ang Fengy, James A. Jonesy, Zhenyu Chen, Chunrong Fang
yDepartment of Informatics, University of California, Irvine, USA
State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China
{yang.feng, jajones}@uci.edu, zychen@nju.edu.cn
ABSTRACT
In crowdsourced software testing, inspecting the large num-
ber of test reports is an overwhelming but inevitable soft-
ware maintenance task. In recent years, to alleviate this
task, many text-based test-report classication and prioriti-
zation techniques have been proposed. However in the mo-
bile testing domain, test reports often consist of more screen-
shots and shorter descriptive text, and thus text-based tech-
niques may be ineective or inapplicable. The shortage and
ambiguity of natural-language text information and the well
dened screenshots of activity views within mobile applica-
tions motivate our novel technique based on using image un-
derstanding for multi-objective test-report prioritization. In
this paper, by taking the similarity of screenshots into con-
sideration, we present a multi-objective optimization-based
prioritization technique to assist inspections of crowdsourced
test reports. In our technique, we employ the Spatial Pyra-
mid Matching (SPM) technique to measure the similarity
of the screenshots, and apply the natural-language process-
ing technique to measure the distance between the text of
test reports. Furthermore, to validate our technique, an ex-
periment with more than 600 test reports and 2500 images
is conducted. The experimental results show that image-
understanding techniques can provide benet to test-report
prioritization for most applications.
CCS Concepts
Software and its engineering !Maintaining soft-
ware;
Keywords
Crowdsourced Testing; Test Report Prioritization; Image
Understanding; Multi-Objective Optimization
1. INTRODUCTION
Crowdsourced techniques have recently gained wide popu-
larity in the software-engineering research domain [20]. Oneof the key advantages of crowdsourced techniques is that
they can provide engineers with information and operations
of real users, and those users provide data from tasks per-
formed on real, diverse software and hardware platforms.
For example, crowdsourced testing ( e.g., beta testing) pro-
vides validation data for a large population of varying users,
hardware, and operating systems and versions. Such ben-
ets are particularly ideal for mobile application testing,
which often needs rapid development-and-deployment iter-
ations and support many mobile platforms. In addition,
crowdsourced mobile testing can provide developers with
real users' feedback, new feature requests, and user-experience
information, which can be dicult to obtain through conven-
tional software testing practices. For these reasons, several
successful crowdsourcing mobile testing platforms (such as
uTest,1Testin,1and AppStori1) have emerged in the past
ve years [20].
Typically, crowdsourced workers provide information for
developers in the form of test reports , which may consist of
screenshots and textual content. Due to the inherent na-
ture of crowdsourced testing, which usually involves a large
number of users, the number of test reports can be great
and the resulting task of inspecting those test reports can
be quite time-consuming and expensive. As such, it is nat-
ural for developers to seek methods to assist in identifying
and prioritizing new and useful information.
In the past decades, to alleviate tedious test-report inspec-
tion, researchers have proposed many full- or semi-automatic
methods [6,9,11,26,30,34,35,37,39], in which, they mainly
focused on the problems of duplicate-report identication,
report classication, and report prioritization. To reduce the
costs of inspecting duplicate test reports, techniques have
been proposed and widely used, such as Bugzilla1and Man-
tis.1Similarly, report-classication techniques have been
proposed to group similar reports, so that ideally an ex-
pert would only need to sample some reports from each
group to gain a sucient understanding of the bugs that
they represent [6, 26, 35]. Finally, prioritization techniques
have gained wide attention in the software-testing domain.
Feng et al. [9] rst proposed the concept of prioritization
of crowdsourced testing reports. Instead of attempting to
reduce the time-cost of inspecting, the basic assumption of
prioritization techniques is \the earlier a bug is detected, the
cheaper it is to remedy," which implies that all of the reports
will be eventually inspected.
In almost all such techniques, the test reports are cap-
tured and analyzed based on their textual similarity ( e.g.,
1utest.com, itestin.com, appstori.com, bugzilla.org, mantisbt.org
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
ASEâ€™16 , September 3â€“7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970367
202
[9,11,30,34,35,37]) or based on their execution traces ( e.g.,
[6, 26, 39]). For software designed for a desktop computer,
such techniques are likely sucient. However, for mobile
software, writing long and descriptive test reports may be
more challenging on a mobile-device keyboard. In fact, test
reports written from mobile devices tend to be shorter and
less descriptive, but also to include more screenshots (pri-
marily due to the ease of taking such screenshots on mobile
platforms). Due to this paucity of textual information for
test reports, and also due to the ambiguity of natural lan-
guage and prevalence of badly written reports [41], utilizing
the screenshots to assist with such mobile crowdsourced test-
ing techniques is appealing. Moreover, the activity views of
typical mobile applications often provide distinguishable as-
pects of the software interface and feature set, and provide
more motivation for utilizing such screenshots.
In this paper, we proposed an approach to test-report pri-
oritization that utilizes a hybrid analysis technique, which is
both text-based and image-based. This approach is a fully
automatic diversity-based prioritization technique to assist
the inspection of crowdsourced mobile application test re-
ports. To facilitate this, we capture textual and image in-
formation and measure the similarity among these artifacts.
For the image analyses, we employed the Spatial Pyramid
Matching (SPM) [17] technique to measure the similarity
of screenshots. For the textual analyses, we used natural-
language textual analysis techniques to measure the simi-
larity of textual descriptions within test reports. Finally,
we combine these similarity results using a multi-objective
optimization algorithm to produce a hybrid distance matrix
among all test reports. Based on these results, we priori-
tize the test reports for inspection using a diversity-based
approach, with the goal of assisting developers of nding as
many unique bugs as possible, as quickly as possible.
To evaluate this proposed hybrid test-report prioritization
technique, we implemented the technique and conducted
an experiment. The experiment was conducted with three
companies and more than 300 students, who simulated the
crowdsourcing of testing of ve widely-used mobile applica-
tions. In all, we received and analyzed 686 crowdsourced test
reports from the crowd workers. We assessed eectiveness
of our technique using the Average Percentage of Faults De-
tected (APFD) [28] metric and the fault detection rate. To
serve as our baseline eectiveness results, we calculated the
results of two strategies: an Ideal strategy, which is a best-
case ordering to nd all bugs in the shortest order possible,
and a Random strategy, which is a random ordering.
The results of our empirical study shows that: (1) Screen-
shots are critical in the test report of mobile application,
which could signicantly improve the eectiveness of the pri-
oritization technique and the eciency of test-report inspec-
tion; (2) For certain classes of mobile applications, our multi-
objective optimized prioritization technique can outperform
the single image-based optimized technique, the text-based
optimized technique, as well as the random technique.
The main contributions of this paper are as follows:
To the best of our knowledge, this is the rst work to
take the image information as well as the text infor-
mation of test reports into consideration to assist the
inspection procedure.
A novel multi-objective optimization-based technique
is proposed to combine the image similarity and textsimilarity, which improves the eectiveness and e-
ciency of test-report maintenance.
Five mobile applications with more than 2500 screen-
shots are used to evaluate our test prioritization tech-
niques. Based on the experimental results and our
experiences, we provide some practical guidance for
crowdsourced mobile test-report prioritization.
2. BACKGROUND
Test and Bug Report Resolution. Software-maintenance
activities are known to be generally expensive and challeng-
ing. One of the most important maintenance tasks is bug-
report resolution. However, current bug-tracking systems
such as Bugzilla, Mantis, the Google Code Issue Tracker,
the GitHub Issue Tracker, and commercial solutions such as
JIRA rely mostly on unstructured natural-language bug de-
scriptions. These descriptions can be augmented with les
uploaded by the reporters ( e.g., screenshots).
Although test descriptions and execution traces are cur-
rently used to characterize and analyze test reports, how
to involve screenshots remains unsolved. Specically for
mobile crowdsourced testing, the reporters often prefer to
provide only short text descriptions along with necessary
screenshots. In this situation, how to combine short text
processing with image processing is important for test-report
prioritization.
Articial-intelligence and computer-vision researchers cre-
ated a class of analyses classied as image understanding ,
which extracts features from images and uses them for analy-
sis. Within the software-engineering research domain, image-
understanding techniques have been used in cross-browser is-
sues for web applications. Cai et al. propose the VIPS algo-
rithm [2], which segments a web page's screenshot into visual
blocks to infer the hierarchy from the visual layout, rather
than from the DOM. Choudhary et al. proposed a tool called
WEBDIFF to automatically identify cross-browser issues in
web applications. Given a page to be analyzed, the compar-
ison is performed by combining a structural analysis of the
information in the page's DOM and a visual analysis of the
page's appearance, obtained through screen captures.
However, to date, there has been no work that addresses
the use of screenshot images for use with test reports, par-
ticularly for mobile test reports produced by crowd workers
in crowdsourced testing. Unfortunately, the crowd workers
tend to describe bugs with a direct screenshot and short
descriptions rather than verbose and complex text descrip-
tions. At the same time, the developers are also interested
in screenshots rather than inspecting the workers' long natu-
ral language descriptions. But, due the complexity of image
understanding, there is a paucity of study on automated
processing of screenshots in crowdsourcing testing.
In this paper, we overcome the diculties in understand-
ing the screenshots by applying advanced image matching
techniques.
Image Understanding. Image matching is an important
problem in the area of computer vision. Matching images of
real world objects is particularly challenging as a matching
algorithm must account for factors such as scaling, lighting,
and rotation. Fortunately, the images that we compare in
this work are screen captures of application views rendered
by dierent devices by dierent workers for dierent apps. In
203Report 
Screenshots 
Text Description 
Keyword Vector NLP
Feature Histogram 
SPM 
Balance Formula chi-square 
   Screenshot 
Distance Matrix 
    Word Set 
Distance Matrix 
Balanced Distance 
identify the 
similar ones 
Jaccard 
Screenshot Set 
Distance Matrix 
Jaccard Extract verbs and nouns Figure 1: Test-report processing framework
this context, the above issues are ameliorated, and the main
problems are, for instance, the shifting of GUI elements or
the fact that some elements are not displayed at all.
A basic technique for comparing two images is to compare
their histograms, where an image histogram represents the
distribution of the value of a particular feature in the image
[1]. In particular, a color histogram of an image represents
the distribution of colors in that image ( i.e.,the number of
pixels in the image whose color belongs in each of a xed list
of color ranges, or \bins"). Obviously, if two images are the
same, their color distributions will also match. Although the
converse is not true, and two dierent images can have the
same histogram, this issue is again not particularly relevant
in our problem domain.
3. TECHNIQUE FRAMEWORK
This section elaborates the details of our method. We
assume the test reports only consist of two parts: text de-
scription and screenshots, which we will handle separately
and nally generate the balanced distance. Figure 1 shows
the framework of calculating the distance between the test
reports, which mainly contains three steps: (1) screenshot-
set distance calculation, (2) test-description distance calcu-
lation, and (3) distance balancing. After we compute the
distance matrix from the test-report set, we apply various
strategies to prioritize test reports.
3.1 Preliminary
Even though, in practice, there could be other multi-
media information that exists in the mobile test reports,
such as the short operation videos and voice messages, our
experience indicates that text descriptions and screenshots
are the most widely used types of information. In this pa-
per, we focus on the processing of mobile screenshots to
assist the test-report prioritization procedure. We assume
each of the test reports only consists of two parts: a text
description and a set of screenshots, i.e.,the test report set
R(r) =fr(Si;Ti)ji= 0:::ng, in which,Sdenotes the screen-
shots ( i.e.,images) containing the views that may capture
symptoms of the bug being reported, and Tdenotes the text
describing the buggy behavior.
3.2 Test-Description Processing
The processing of text consists of two steps: (1) keywords
set building and (2) distance calculation. Because natural-
language-processing (NLP) techniques have been widely used
to assist various software engineering tasks ( e.g., [9, 10, 30,
38]), we focus our description below on the distinguishing
features and implementation choices of our approach.Keywords Set Building. In order to extract the keywords
from the natural-language description, we rst need to seg-
ment the text. Fortunately, word segmentation is a basic
NLP task, and as such many ecient tools for word seg-
mentation for dierent natural languages have been imple-
mented [13]. In our method, we adopted the Language Tech-
nology Platform (LTP)2[4], which is the most widely used
Chinese NLP cloud platform, to process the Chinese text
descriptions. LTP segments the Text parts of test reports
and marks each word with its Part-of-Speech (POS) for its
context. In this procedure, LTP used the Conditional Ran-
dom Fields (CRF) [15] model to segment Chinese words and
adopted the Support Vector Machine (SVM) approach for
tagging the POS. After we compute the segmentation re-
sults with the POS tags, we lter out relatively meaningless
words that could negatively impact the distance calculation.
According to prior works ( e.g., [27,33]), verbs and nouns can
reveal the main information of a document. So, to simplify
the technique, we extract only the nouns and verbs to build
the keywords sets.
It is worth noting that our technique should not be lim-
ited to only the Chinese language. By applying other NLP
tools, such as the Stanford NLP toolkit,3similar text models
can be built for text descriptions written in other languages,
such as English, French, or German. However, dierent nat-
ural languages have dierent characteristics, and may need
special accommodations. For example, languages with rel-
atively more prevalent polysemy ( i.e.,many possible mean-
ings for a word or phrase) and synonyms may require special
processing, such as synonym detection and replacement, to
avoid negative impacts on analyses.
Distance Calculation. Our method focuses on processing
mobile test reports. Compared with the test reports of desk-
top or web applications, one characteristic of typical mobile
test reports, and based on our experience, is that their text
descriptions are shorter and contain more screenshots. As
such, we treat all of the words in the text description equally,
and we adopted the Jaccard Distance to measure the dier-
ence between the text descriptions Tiin the test-report set
R(r). The denition of Jaccard Distance used in our tech-
nique is presented in the following equation, in which, Ki
denotes the keyword set of test report Ti, andDT(ri;rj)
denotes the distance between the text portion of the test
reportsriand report rj.
DT(ri;rj) = 1 jKi\Kjj
jKi[Kjj
2http://www.ltp-cloud.com/
3http://nlp.stanford.edu/software/
204(a)Playing-1
 (b)Playing-2
 (c)Lyrics-1
 (d)Lyrics-2
Figure 2: Four example screenshots from the test reports
of the CloudMusic application. (a) and (b) are screenshots
of the playing view, and (c) and (d) are screenshots of the
lyrics view of two dierent songs.
3.3 Screenshot Processing
Compared with NLP techniques, image understanding tech-
niques are relatively less studied and used in the software-
engineering domain. One of our motivations of conduct-
ing this research is to proposed a method to extract the in-
formation from images to assist software-engineering tasks.
The workow of processing screenshots Sis presented in
the top branch of Figure. 1. The process is composed of
three key steps to build up the distance between screen-
shot sets: (1) building feature histograms, (2) calculating
distance between individual screenshots, and (3) computing
the distance between screenshot sets.
Feature Histogram Building. In order to compute the
dierence between the screenshots, we convert the screen-
shots into feature vectors. Bug screenshots provide not only
views of buggy symptoms, but also app-specic visual ap-
pearances. We hope to automatically identify application
behaviors based on their visual appearance in the screen-
shots. However, the screenshots often have variable reso-
lution and complex backgrounds. Therefore, modeling the
similarity between the screenshots merely based on RGB
is not an approach that is well suited for our task. To ad-
dress the challenges, we apply the Spatial Pyramid Matching
(SPM) [17] to build a global representation of screenshots.
Since the details of SPM are beyond this paper's topic, we
only briey introduce it here.
Given an image, SPM partitions it into sub-regions in
a pyramid fashion. At each pyramid level, it computes
an orderless histogram of low-level features in each sub-
region. After decomposition, it concatenates statistics of
local features over sub-regions from all levels. After build-
ing the \Spatial Pyramid" representation, we apply kernel-
based pyramid matching scheme to compute feature corre-
spondence in two images.
Figure 2 presents four original and actual screenshots from
four test reports of a popular Chinese music-playing app,
CloudMusic. Figures 2a and 2b show the music-playing
view of the application, and Figures 2c and 2d show the
lyrics view. Note that in each screenshot, the details of the
view dier: e.g., dierent music is playing, dierent back-
ground images appear, dierent lyrics are shown, and even
the screen size is dierent for the last image. The layout of
screenshots and background colors dier and provide chal-
lenges for correct matching: although Figures 2a and 2b
have the same view layout, Figures 2b and 2d share a sim-
ilar background color. If we were to directly calculate dis-
tance based on the RGB histograms, we would incorrectly
500 1000 1500 2000 2500 3000 3500 400000.0050.010.0150.020.025
FeatureValue(a) Playing-1
500 1000 1500 2000 2500 3000 3500 400000.0050.010.0150.020.025
FeatureValue (b) Playing-2
500 1000 1500 2000 2500 3000 3500 400000.0050.010.0150.020.025
FeatureValue
(c) Lyric-1
500 1000 1500 2000 2500 3000 3500 400000.0050.010.0150.020.025
FeatureValue (d) Lyric-2
Figure 3: The corresponding feature histograms of the
screenshots in Figure 2.
Table 1: Distance between screenshots of Figure 2
Playing-1 Playing-2 Lyrics-1 Lyrics-2
Playing-1 0 0.38957 0.40255 0.45109
Playing-2 0.38957 0 0.51161 0.51873
Lyrics-1 0.40255 0.51161 0 0.32029
Lyrics-2 0.45109 0.51873 0.32029 0
get a closer distance between Figures 2b and 2d. Neverthe-
less, the image-understanding technique should be able to
capture the similarities of the the similar views. Intuitively,
Figures 2a and 2b should be identied as similar views, and
Figures 2c and 2d should be identied as similar views.
Based on the four images, SPM rst builds the histograms
of features for each of image. The resulting histograms for
these images are shown in Figure 3.
Screenshot Distance Calculation. Using the screenshot
feature histograms, a distance is computed for each pair of
images. To compute such distances between feature his-
tograms, we adopt the chi-square distance metric [29]. The
chi-square metric is generally used to compute the distance
between two normalized histogram vectors, i.e., their ele-
ments sum to 1. Also, both of the pairwise histograms being
compared should contain the same number of bins ( i.e.,the
vectors should have the same number of dimensions).
We useHi(x1;x2;:::;x n) to denote the feature histogram
of screenshot si, andHi(xk) to denote the value of kth fea-
ture ofsi. The formula used to calculate chi-square distance
Ds(si;sj) between screenshot siandsjis dened as follows:
Ds(si;sj) =2(Hi;Hj)
=1
2dX
k=1(Hi(xk) Hj(xk))2
Hi(xk) +Hj(xk)(1)
Based on Equation 1, we obtain the distance matrix shown
in Table 1 from the feature histograms of Figure 3.
These results show that the calculated distance between
the same views (Playing-1 and Playing-2, and Lyrics-1 and
Lyrics-2) have relatively shorter ( i.e.,smaller) distances (0.389
between playing screenshots and 0.320 between lyrics screen-
shots) than the across-view distances.
Screenshot Sets Distance Calculation. The previous
step uses the chi-square distance metric to compute dis-
tances between pairs of screenshots. However, in practice,
205each test report may contain more than one screenshot. So,
in this step, we compute the distance between screenshot
sets. To account for the diversity of display resolutions of
mobile devices and user content ( e.g., songs, backgrounds),
we set a threshold to assess screenshots that match. The
threshold is rst used to nd representative members from
within the same screenshot set ( i.e.,from the same test re-
port). Screenshot subsets whose histograms produce chi-
square distances that are below the distance threshold ( i.e.,
assessed as representing the same situation) are rst repre-
sented as an aggregated, summary histogram which is com-
puted as the mean of the feature histograms from the con-
stituent members.
Once the representative set of screenshots are selected
from each test report, the chi-squared metric with the met-
ric is again used to compute the across-test-report screenshot
similarity between the representative screenshots. Again,
for screenshots ( i.e.,their representative histograms) whose
distance is less than , they are assessed as representing the
same view, and as such, the similar and non-similar screen-
shots from each test report can be used to calculate the inter-
test-report screenshot set distance for a pair of reports. For
this calculation, we use the Jaccard distance metric. For the
test reports riandrjand their respective screenshot sets Si
andSj, the distance metric is dened as:
DS(ri;rj) = 1 jSi\Sjj
jSi[Sjj
Note that in the special case where both SiandSjare
the empty set ( i.e.,no screenshots were included for either
test report), we assess DSto be zero.
3.4 Balanced Formula
Based on above distance computations for both the tex-
tual descriptions and the screenshot sets, we combine these
distances to produce a hybrid distance. We present Equa-
tion 2 to combine these diering distance values. Equation 2
is a step-wise formula, where the rst condition holds for
when the textual descriptions are assessed to be identical
by way of the text distance formula DT. In this case, we
assess the balanced distance metric to be similarly identi-
cal. In the next step, where DS= 0, where typically no
screenshots were included for either test report, the textual
dierence is used and scaled to make them more similar, and
thus less diverse. This diversity adjustment will make these
less descriptive test reports less likely to be highly prioritized
in the next prioritization step. In the nal step, which holds
in all other cases, the harmonic mean is calculated between
the textual distance DTand screenshot set distance DS.
The resulting balanced distance BDis used to represent the
pairwise distance of the corresponding test reports.
BD(ri;rj) =8
>><
>>:0; ifDT(ri;rj) = 0
DT(ri;rj); ifDS(ri;rj) = 0
(1 +2)DS(ri;rj)DT(ri;rj)
2DS(ri;rj)+DT(ri;rj);otherwise
(2)
3.5 Diversity-Based Prioritization
Using the computed balanced distance measures for all
test reports, we can prioritize the test reports for inspection
by developers. The guiding principle of our prioritization
approach is to promote diversity of test reports that getinspected. In other words, when a developer inspects one
test report, the next test reports that she inspects should be
as dierent as possible to allow her to witness as many di-
verse behaviors (and bugs) as possible in the shortest order.
This diversity-based prioritization strategy has been used by
other software-engineering researchers for test prioritization
(e.g., [5, 12, 32]). The goal is for software engineers to nd
as many bugs as possible in a limited time budget.
GivenQdenotes the result queue, the distance between a
test report randQ, denoted byD(r;Q), is dened by the
minimal distance between rand eachriinQ,i.e.,D(r;Q) =
Min ri2QfD(r;ri)g. The algorithm of BDDiv is shown in
Algorithm 1. In the beginning, Qis empty, we rst initialize
the algorithm by randomly choosing one report from Rand
append it to Q. The second step is to calculate the distance
between each test report ri2RandQ. As soon as we get
the distance values, we choose the largest one to append to
Q. The whole procedure completes when jRj= 0.
Algorithm 1: BDDiv(BD,R)
1:Q=?
2:Randomly choose a test report rkfromR, appendrctoQ
3:R:=R frkg
4:whilejRj6= 0do
5:maxDis := 1;rc=NULL
6: for allri2Rdo
7:minDis := 2
8: for allrj2Qdo
9: ifBD(ri;rj)<minDis then
10: minDis =BD(ri;rj)
11: end if
12: end for
13: ifminDis>maxDis then
14: maxDis =minDis
15: rc=ri16: end if
17: end for
18: AppendrctoQ
19:R:=R frcg
20:end while
21:returnQ
4. EXPERIMENT
In our experiment, we propose the following three research
questions:
RQ1: Can test-report prioritization substantially im-
prove test-report inspection to nd more unique
buggy reports earlier?
RQ2: To what extent can the image-based approaches
improve the eectiveness of the text-only-based
approach?
RQ3: How much improvement is further possible,
compared to a best-case ideal prioritization?
If the software engineers have no test report prioritiza-
tion technique, they may randomly inspect test reports, in
a non-systematic order. RQ1 is designed to inform whether
prioritization of test reports is, in fact, advantageous. To
address the RQ1, we conduct the experiment to evaluate
the eectiveness of our prioritization techniques alongside
aRandom -based strategy. RQ2 is designed to investigate
whether image-understanding techniques can assist the in-
spection procedure compared with the text-only-based tech-
nique.RQ3 is designed to investigate the gap between the
performance of our techniques and the theoretical Ideal pri-
oritization technique, which could be helpful to engineers in
206Table 2: Experimental Software Subjects
Name VersionjRjjFjjSjjRsjjRfj
SE-1800 2.5.1 192 7 856 164 99
CloudMusic 2.5.1 96 16 272 70 40
Ubook 2.1.0 99 22 719 90 99
iShopping 1.3.0 209 73 581 160 130
JustForFun 1.8.5 90 9 109 69 90
Totals 686 127 2537 553 458
selecting proper techniques in practice and inform the future
research in this eld.
4.1 Software Subject Programs
From November 2015 to January 2016, we collaborated
with three companies and more than 300 students to simu-
late a crowdsourced testing process. The ve applications on
which we simulated crowdsourced testing are as follows:
JustForFun: A picture editing and sharing application,
produced by Dynamic Digit.
iShopping: A shopping application for Taobao, produced
by Alibaba.4
CloudMusic: An application for free-sharing music as well
as a music player, produced by NetEase.5
SE-1800: A monitoring application for a power supply
company, produced by Panneng.
Ubook: An application for online education, produced by
New Oriental.6
Testing for all of these applications was crowdsourced to
workers on Kikbug.net. For these ve apps, more than 300
students were involved. To perform crowdsourced testing,
each student installed a Kikbug-Android app, chose testing
tasks, and completed testing tasks on their own phone. Dur-
ing the testing process, workers performed testing tasks ac-
cording to some guidelines, specied by the app developers.
During task performance, the workers could take screenshots
if necessary, such as experiencing some unexpected behav-
ior. After the testing task was completed, the worker could
provide a brief description on bug phenomenon on his own
phone. Finally, the student submitted a test report, includ-
ing the short descriptions and possible screenshots.
Then all the test reports are submitted to app develop-
ers, and the developers can inspect the reports and begin
the debugging process. With the help of the developers' in-
spection, Kikbug obtained ground truth assessments for the
students' reports. The detailed information of the applica-
tions is shown in Table 2, in which, the jRjdenotes the num-
ber of reports,jFjdenotes the number of faults revealed in
the test reports,jSjdenotes the number of screenshots con-
tained in the test reports, jRsjdenotes the number of test
reports containing at least one screenshot, and jRfjdenotes
the number of test reports that revealed faults.
4.2 Prioritization Strategies
Technique 1: Ideal. The best result in theory to inspect
test reports in such a way as to demonstrate the most
unique bugs as early as possible. Represented as Ideal .
Technique 2: TextDiv. The prioritization strategy based
only on the distance between test reports' text de-
scriptions, i.e., in this strategy DTwill replace BD
4https://guang.taobao.com
5http://music.163.com
6http://www.pgyer.com/y44vas the rst parameter of Algorithm 1. Represented as
TextDiv .
Technique 3: ImageDiv. The prioritization strategy based
only on the distance between test reports' screenshots,
i.e., in this strategy DSwill replace BDas the rst
parameter of Algorithm 1. Represented as ImageDiv .
Technique 4: Random. The random prioritization strat-
egy, which is used to simulate the situation without
any prioritization technique. Represented as Random .
Technique 5: Text&ImageDiv. Our prioritization strat-
egy that balances the distance of screenshot sets and
text descriptions. Represented as Text&ImageDiv .
4.3 Evaluation Metrics
We employed the APFD (Average Percentage of Fault De-
tected) metric [28], which is the most widely-used evaluation
metric for test-case prioritization techniques, to measure the
eectiveness of our techniques. For each fault, APFD marks
the index of the rst test report revealing it. We present
the formula to compute the AFPD value in Equation 3, in
which,ndenotes the number of test reports, Mdenotes the
total number of faults revealed by all test reports, Tfiis the
index of the rst test report that reveals fault i.
APFD = 1 Tf1+Tf2+:::+TfM
nM+1
2n(3)
In our experiment, a higher APFD value implies a better
prioritization result. That is, it can reveal more faults earlier
than the other methods do.
Although the APFD values reect the global performance
of prioritization techniques, in practice developers often can-
not inspect all reports in a limited time budget. Thus, we
also provide a metric to reveal the percentage of bugs that
would be found at certain milestones of inspection. For this,
we use linear interpolation [18] to evaluate the partial per-
formance of each prioritization technique. We dene linear
interpolation as following:
Qp=Mp, which is the number of faults correspond-
ing to a percentage p. Letint(Q) andfrac(Q) be the
integer part and fractional part of Q, respectively. If
frac(Q)6= 0, the linear interpolation is needed.
i,jare the indexes of reports that reveal at least Q
and Q+1 faults respectively. The linear interpolation
valueVpis calculated as Vp=i+ (j i)frac(Q)
In our experiment, we set the p2f25%;50%;75%;100%g.
4.4 Experimental Setup
In order to ensure the correctness of the implementation of
SPM, we directly used the MATLAB code provided by the
inventors of SPM. There are some key parameters aecting
the performance of SPM, which are the size of the descriptor
dictionaryDictSize , number of levels of the pyramid L, and
number of images to be used to create the histogram bins
HistBin . In our experiment, as the recommended values
of the SPM inventor, we set DictSize = 200,L= 3, and
HistBin = 100. For the NLP technique, because all of test
reports in our experiment are in Chinese, we employed the
LTP platform to assist the text description analysis.
Moreover, the size of screenshots ( i.e.,image resolution)
submitted by the crowd workers was not xed; in fact, they
varied widely. In order to apply the SPM technique, we
resize all screenshots to 480 480 pixels. Given the way
207that the SPM technique focuses on detecting features within
images, resizing the images should not produce a substantial
impact to the distance calculation.
In this experiment, we implemented all of the strategies
presented in Section 4.2. Particularly for the Text&ImageDiv
strategy, we set the threshold of determining the identity of
screenshots to 0.1, the factor that is used to weaken the
weight of test reports without any screenshots to 0.75, and
the parameter used to balanced the text-based distance
and screenshot-set distance to 1, which means, we weigh the
two kinds of distance equally.
5. RESULTS, ANALYSIS AND DISCUSSION
In this section we present the results of our experiment,
then interpret those results to attempt answers to our re-
search questions, and nally discuss the overall results. In
order to reduce the bias that was introduced by the ran-
dom initialization of the algorithm and the tie-breaking, we
conducted the experiment 30 times and present the result
in Figure 4. Figures 4 (a, c, e, g, and i) show the boxplots
of the APFD results for the ve projects, respectively, each
aggregated over the 30 experimental runs. Figures 4 (b, d, f,
h, and j) show the average fault detection rate curves. The
exact mean value of APFD is shown in Table 3, which also
includes the result of one-way ANOVA tests of all strategies:
the improved extent over Random , and the gap between our
strategies and Ideal . Furthermore, we present the mean
linear interpolation value over the 30 experiment runs in
Table 4 to demonstrate the performance of our techniques
in limited time budgets.
5.1 Answering Research Question RQ1
RQ1 : Can test-report prioritization substantially improve
test-report inspection to nd more unique bugs earlier?
Based on the results shown in Figure 4 (a, c, e, g, i) and
in the third column of Table 3, we nd, to dierent extents,
all of the three diversity-based prioritization strategies out-
perform Random . Furthermore, in Table 3, all F-values are
relatively large and the p-values0:001, which means the
APFD values of the four strategies are signicantly dierent.
Compared with the Random strategy, the percentage of im-
provement of Text&ImageDiv ranges 9.93% { 24.95%.
Summary : All of the diversity-based prioritization meth-
ods can improve the eectiveness of test report inspection
overRandom , and thus test-report prioritization can sub-
stantially, and signicantly, nd more unique buggy reports
earlier in the prioritized order.
5.2 Answering Research Question 2
RQ2 : To what extent can the image-based approaches im-
prove the eectiveness of the text-only-based approach?
Figure 4 reveals that, except on the \JustForFun" project,
theText&ImageDiv outperforms the TextDiv ,Image-
Div andRandom strategies, which means, the image-un-
derstanding technique improves the performance of the text-
only-based technique. We did a deeper investigation on this
problem and found what we speculate to be the reason for
the dierent result for the \JustForFun" project. JustFor-
Fun is an image editing and sharing application, and as
such, the inherent functionality is to process various user-
provided photos. The screenshots for this app largely con-
sist of user content, with relatively few app-specic features
in those screenshot images. Thus, the various screenshotsof \JustForFun" make the screenshot sets distance calculat-
ing procedure generate large distances, even between the
same activity views, which leads to a negative impact on
the image-based strategies. In contrast, based on Table 4,
Text&ImageDiv outperformed the single text-based pri-
oritization techniques on inspecting dierent percentage of
test report of \SE-1800", \CloudMusic" and \Ubook."
Summary : Generally, compared with the text-only-based
prioritization strategy, the image-understanding technique is
able to improve the performance of prioritizing test reports,
both globally ( i.e.,APFD) and partially ( i.e.,linear interpo-
lation at many level). However, we found that some classes
of apps are naturally less suited for image-understanding
techniques | namely apps where the bulk of the views are
composed of user contect.
5.3 Answering Research Question 3
RQ3 : How much improvement is further possible, compared
to a best-case ideal prioritization?
The fourth column of Table 3 shows the gap between our
strategies and the theoretical Ideal . We found the gap be-
tween Text&ImageDiv andIdeal vary from 15.21% to
31.98%. For more details, we can observe the growth curves
in Figure 4. The curve of Ideal grows at a fast rate. The
best situation reached top while the Text&ImageDiv only
stayed around 35%.
Summary : We nd that our prioritization methods can
provide a reasonable small gaps for the theoretical Ideal re-
sult, particularly for some subjects. However, there is room
for future work to continue to improve the prioritization or-
dering of test reports.
5.4 Discussion
Method Selection. Reecting on all of our experimental
results, we nd that image-understanding techniques can
provide benets to test-report prioritization, and that the
area of such hybrid text-and-image approaches demonstrates
promise. That said, we also observed that dierent tech-
niques may be more or less applicable for dierent types of
applications. Specically, we observed that the image editor
app produced the worst results for the image-based and hy-
brid techniques, compared to text-only. In such cases, where
the screenshots mainly represent user content, image-based
techniques may be less applicable. However, in applications
in which little user or external content is displayed, image-
based or hybrid techniques may be more applicable.
One noteworthy point is that both the TextDiv and
Text&ImageDiv are full-automated, which we believe are
more applicable in practice than the semi-automated Di-
vRisk and Risk techniques [9] that require the users to in-
put the inspection result to prioritize the crowdsourced test
reports dynamically.
Mobile Application Testing. All of our experimentation
was conducted on mobile applications, and thus we cannot
state with certainty that such results would hold for other
types of GUI software, such as desktop or web applications.
However, we speculate that while there will likely be new
and unique challenges in these domains, the basic concepts
would likely hold, at least for the class of applications with
relatively less user content. Desktop and web applications
have the potential for even more diering screen and win-
dow sizes, as well as multiple windows and pop-up dialog
windows, and each of these unique aspects would likely need
208Table 3: One-way ANOVA Tests
Method APFD Improvement Gap
MeansX Random
RandomBest X
X
SE-1800:F(3;119) = 54:966,p-value0:001
Ideal 0.982 37.47% |
Text&ImageDiv 0.852 19.32% 15.21%
TextDiv 0.817 14.46% 20.10%
ImageDiv 0.836 17.04% 17.45%
Random 0.714 | 37.47%
CloudMusic: F(3;119) = 73:170,p-value0:001
Ideal 0.917 58.65% |
Text&ImageDiv 0.722 24.95% 26.97%
TextDiv 0.664 14.98% 37.98%
ImageDiv 0.641 10.99% 42.94%
Random 0.578 | 58.65%
Ubook:F(3;119) = 84:167,p-value0:001
Ideal 0.889 40.92% |
Text&ImageDiv 0.750 18.95% 18.47%
TextDiv 0.735 16.57% 20.88%
ImageDiv 0.686 8.69% 29.65%
Random 0.631 | 40.92%
iShopping: F(3;119) = 73:178,p-value0:001
Ideal 0.825 45.08% |
Text&ImageDiv 0.625 9.93% 31.98%
TextDiv 0.614 7.88% 34.48%
ImageDiv 0.586 2.98% 40.89%
Random 0.569 | 45.08%
JustForFun: F(3;119) = 94:482,p-value0:001
Ideal 0.950 45.89% |
Text&ImageDiv 0.784 20.41% 21.16%
TextDiv 0.842 29.28% 12.85%
ImageDiv 0.681 4.54% 39.55%
Random 0.651 | 45.89%
Table 4: Linear Interpolation (average number of inspected
test reports)
Program Strategy 25% 50% 75% 100%
Ideal 1.75 3.50 5.25 7.00
Text&Image 3.51 12.32 31.30 91.70
SE- TextDiv 6.98 23.27 43.27 112.67
1800 ImageDiv 4.21 16.38 50.38 86.47
Random 4.79 31.05 79.36 145.57
Ideal 4.00 8.00 12.00 16.00
Text&Image 13.10 24.30 39.33 62.00
Cloud- TextDiv 16.10 34.57 44.57 59.00
Music ImageDiv 11.07 26.53 49.77 85.83
Random 14.10 33.97 59.20 88.83
Ideal 5.50 11.00 16.50 22.00
Text&Image 7.33 18.67 44.17 64.43
Ubook TextDiv 10.52 23.67 35.17 78.03
ImageDiv 8.05 20.20 50.73 95.40
Random 9.35 29.03 57.82 93.13
Ideal 18.25 36.50 54.75 73.00
Text&Image 37.16 66.42 119.27 201.23
iShop- TextDiv 52.89 82.82 111.30 160.07
ping ImageDiv 32.60 75.88 134.59 206.30
Random 37.20 83.72 144.13 207.13
Ideal 2.25 4.50 6.75 9.00
Text&Image 2.94 9.32 18.13 64.83
Just- TextDiv 2.88 8.07 17.28 45.23
ForFun ImageDiv 3.16 18.12 39.01 79.47
Random 2.88 22.25 49.88 80.17
to be addressed. Overall, we speculate that the success of
such image-understanding-assisted test-report prioritization
techniques would likely depend on the visual complexity of
the application views.
5.5 Threats to Validity
There are some general threats to validity in our experi-
mental results. For example, we need more projects and dif-
ferent parameter values combinations to reduce the threat
to external validity and to better generalize our results.
Crowd Workers. Due to a monetary limitation, we \simu-lated"the crowdsourced mobile testing procedure to validate
our techniques, in which, we invited the students to work as
crowd workers. Such a choice means that our population of
workers may be less diverse than the population of crowd-
sourced workers from the general populace. Theoretically,
\crowdsourcing" requires workers come from a large pool of
individuals that one has no direct relationship with the oth-
ers [20], which implies that our result may be dierent if
the crowd workers were from the internet with open calls.
However, according to the study of Salman et al. [31], if a
technique or task is new to both students and professionals,
similar performance can be expected to be observed. Based
on this study, we believe this threat may not be the key
problem for our validation procedure.
Subject Program Selection. The cost of conducting this
kind of experiment is quite expensive (involved more than
300 people), the monetary budget is limited, so we con-
ducted the experiment on only ve applications. However,
these ve applications are widely used and publicly acces-
sible. The functionalities of our subject applications vary
widely, including music player, video player, picture editor,
power monitor, and online shopping assistant. Thus, we
believe these applications can be used to validate the our
methods, at least to give initial indications of eectiveness
and applicability.
Natural Language Selection. Admittedly, in our experi-
ment, all of the test reports were written in Chinese, which
could threaten the generalizability to other natural languages.
However, the NLP techniques and text-based prioritization
technique are not the focus of this work. Even though we
used text-based techniques as one of our baselines, what
matters to the performance of these technique is the dis-
tance built from keywords set but not the languages. As for
the keyword-extraction technique, dierent languages have
their own inherent characteristics, and thus NLP researchers
have proposed keywords-extraction techniques for dierent
languages. In future research, we will validate our technique
with test reports written in English. Moreover, the focus of
this work is to study the potential for image-understanding
techniques to augment text-only-based techniques.
6. RELATED WORK
Bug Report Triage. As a large number of bug reports will
be submitted in the software testing phase, manually triag-
ing each of these reports will become an eort-consuming
task. Bug report triage is a process that includes: priori-
tizing bug reports, ltering out duplicate reports, and as-
signing reports to the proper bug xer. Among the various
bug report triaging techniques, we address two highly rele-
vant research areas: bug report prioritization and duplicate
identication techniques.
Yuet al. [40] used neural networks to predict the priority
of bug reports. Their technique also employs the reused data
set from similar systems to accelerate the evolutionary train-
ing phase. Kanwal et al. [13] used SVM and Naive Bayes
classiers to assist bug priority recommendation. Tian et
al.[36] predicted the priority of bug reports by presenting a
machine learning framework that takes multiple factors in-
cluding temporal, textual, author, related-report, severity,
and product into consideration. By analyzing the textual
description from bug reports and using text mining algo-
rithms, Lamkan et al. [16] conducted case studies on three
large-scale open source projects, and based on the result,
2090.40.50.60.70.80.91
Text&Image Text Image RandomAPFDIdeal(a) APFD on SE-1800
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91
Percentage of Inspected ReportsPercentage of Found Faults
  
Ideal
Text&Image
Text
Image
Random (b) Average Fault Detection Rates on SE-1800
0.40.50.60.70.80.91
Text&Image Text Image RandomAPFDIdeal
(c) APFD on CloudMusic
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91
Percentage of Inspected ReportsPercentage of Found Faults
  
Ideal
Text&Image
Text
Image
Random (d) Average Fault Detection Rates on CloudMusic
0.40.50.60.70.80.91
Text&Image Text Image RandomAPFDIdeal
(e) APFD on Ubook
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91
Percentage of Inspected ReportsPercentage of Found Faults
  
Ideal
Text&Image
Text
Image
Random (f) Average Fault Detection Rates on Ubook
0.40.50.60.70.80.91
Text&Image Text Image RandomAPFDIdeal
(g) APFD on iShopping
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91
Percentage of Inspected ReportsPercentage of Found Faults
  
Ideal
Text&Image
Text
Image
Random (h) Average Fault Detection Rates on iShopping
0.40.50.60.70.80.91
Text&Image Text Image RandomAPFDIdeal
(i) APFD on JustForFun
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91
Percentage of Inspected ReportsPercentage of Found Faults
  
Ideal
Text&Image
Text
Image
Random (j) Average Fault Detection Rates on JustForFun
Figure 4: Experiment Results (averaged over 30 runs)
210concluded that the technique is able to predict the severity
with a reasonable accuracy.
By applying natural language processing techniques, Rune-
sonet al. [30] took more textual features, including software
versions, tester information, and submission date into con-
sideration to detect duplicate bug reports. They validated
this technique by conducting a large-scale experiment on in-
dustrial projects. Jalbert et al. [11] used the surface features,
textual semantics, and graph clustering to identify the du-
plicate status. Besides duplicate detection, their technique
is also able to rank the existing reports that are more similar
with the new one. By measuring the textual semantic simi-
larity between the test reports, Nyugen et al. [24] applied the
topic model to detect duplicate bug reports. Podgurski et al.
proposed the rst approach to categorizing software failure
reports by applying the undirected clustering on execution
traces. Feng and Xin et al. [8, 39] adopted the multi-label
classication technique to assign the bug reports into more
than one classes based on the execution traces.
To assist test report prioritization in crowdsourced soft-
ware testing, Feng et al. [9] proposed two strategies: Div
and Risk. Both of the two strategies are text-based. Div
is a fully automated technique, which aims at assisting the
developers inspect a wide variety of test reports and to avoid
duplicates and wasted eort on falsely classied faulty be-
havior. In this paper, we denote Div technique as TextDiv ,
and treat it as a baseline. Risk is designed to assist devel-
opers to identify test reports that may be more likely to be
fault-revealing based on past observations. Because Risk re-
quires the users to input the inspection result, i.e., it is a
semi-automated technique (and not fully automatic), it is a
distinct category of technique, and thus we did not employ
it as a baseline for evaluation.
Crowdsourced Software Testing. Mao et al. conducted
a comprehensive survey on the crowdsourced software engi-
neering [20], in which, they dened the crowdsourced soft-
ware engineering as \the act of undertaking any external
software engineering tasks by an undened, potentially large
group of online workers in an open call format." In fact,
crowdsourced techniques have been widely used in indus-
trial software testing and gained popularity in usability test-
ing, localization testing, GUI testing, user-experience test-
ing, and stress&performance testing. However, it is a fairly
new research topic in the software-engineering research com-
munity. Liu et al. [19] investigated both methodological
dierences and empirical contrasts of the crowdsourced us-
ability testing and traditional face-to-face usability testing.
To solve the oracle problem, Pastore et al. [25] applied the
crowdsourcing technique to generating test inputs depend-
ing on a test oracle that requires human input in one form
or another. Dolstra et al. [7] used virtual machines to run
the system under test and enable the crowd workers to ac-
complish expensive and semi-automatic GUI testing tasks.
By introducing crowdsourced testing, Nebeling et al. [23]
conducted an experiment to evaluate the usability of web
sites and web-based services, the result of which showed that
crowdsourcing testing is an eective method to validate the
web interfaces.
Application of Image Understanding on Testing. In
[21], Michail et al. proposed a static approach, GUISearch,
to guide search and browsing of its source code by using the
GUI of an application. They further proposed a dynamic ap-proach to obtain an explicit mapping from high-level actions
to low-level implementation by identifying execution trig-
gered by user actions and visually describing actions from
a fragment of the application displayed [3]. Kurlander et
al.[14] introduced the notion of an editable graphical his-
tory that can allow the user to review and modify the actions
performed with a graphical interface. Similarly, Michail and
Xie [22] used before/after screenshots to visually describe
application state at a very high level of abstraction to help
users avoid bugs in GUI applications. However, images in
these work are provided to developers or users directly with-
out machine understanding.
Image-understanding techniques have been used in cross-
browser issues for web applications. Cai et al. propose the
VIPS algorithm [2], which segments a web page's screen-
shot into visual blocks to infer the hierarchy from the vi-
sual layout, rather than from the DOM. Choudhary et al.
proposed a tool called WEBDIFF to automatically identify
cross-browser issues in web applications. Given a page to be
analyzed, the comparison is performed by combining a struc-
tural analysis of the information in the page's DOM and a
visual analysis of the page's appearance, obtained through
screen captures.
7. CONCLUSION
In this work, we proposed a novel technique to priori-
tize test reports for inspection by software developers by
using image-understanding techniques to assist traditional
text-based techniques, particularly in the domain of crowd-
sourced testing of mobile applications. We proposed ap-
proaches for prioritizing based on text descriptions, based
on screenshot images, and based on a hybrid of both sources
of information. To our knowledge, this is the rst work to
propose using image-understanding techniques to assist in
test-report prioritization. In order to evaluate the promise of
using image understanding of screenshots to augment text-
based prioritization, we implemented our hybrid approach,
as well as a text-only- and image-only-based approaches, and
two baselines: an ideal best-case and a random average-case
baseline. We found that prioritization, in almost all cases,
is advantageous as compared to test-report inspection based
on an unordered process. We also found that for most soft-
ware applications that we studied, there was a benet to
using the screenshot images to assist prioritization. How-
ever, we also found that there exist a class of applications
for which image-understanding may not be as applicable,
and found room for improvement to narrow the gap to the
hypothetical best-case ideal result.
As such, in future work, we will investigate ways to help
prioritize for those classes of applications, and also iden-
tify application classes that are best suited for each type of
technique. Finally, in future work we will extend the set of
software systems that we use and the natural language used
to write the test reports.
8. ACKNOWLEDGEMENTS
We would like to express gratitude to Peiyun Hu for valu-
able suggestions for this paper. This work is supported
by the National Science Foundation under award CAREER
CCF-1350837, and is partly supported by National Basic Re-
search Program of China (973 Program 2014CB340702), Na-
tional Natural Science Foundation of China (No. 61373013).
2119. REFERENCES
[1] G. Bradski and A. Kaehler. Learning OpenCV:
Computer vision with the OpenCV library . " O'Reilly
Media, Inc.", 2008.
[2] D. Cai, S. Yu, J.-R. Wen, and W.-Y. Ma. Vips: a
visionbased page segmentation algorithm. Technical
report, Microsoft technical report, MSR-TR-2003-79,
2003.
[3] K. Chan, Z. C. L. Liang, and A. Michail. Design
recovery of interactive graphical applications. In
Proceedings of the 25th international conference on
Software engineering , pages 114{124. IEEE Computer
Society, 2003.
[4] W. Che, Z. Li, and T. Liu. Ltp: A chinese language
technology platform. In Proceedings of the 23rd
International Conference on Computational
Linguistics: Demonstrations , pages 13{16. Association
for Computational Linguistics, 2010.
[5] T. Y. Chen, F.-C. Kuo, R. G. Merkel, and T. Tse.
Adaptive random testing: The art of test case
diversity. Journal of Systems and Software ,
83(1):60{66, 2010.
[6] Y. Dang, R. Wu, H. Zhang, D. Zhang, and P. Nobel.
Rebucket: a method for clustering duplicate crash
reports based on call stack similarity. In Proceedings of
the 34th International Conference on Software
Engineering , pages 1084{1093. IEEE Press, 2012.
[7] E. Dolstra, R. Vliegendhart, and J. Pouwelse.
Crowdsourcing gui tests. In Software Testing,
Verication and Validation (ICST), 2013 IEEE Sixth
International Conference on , pages 332{341. IEEE,
2013.
[8] Y. Feng and Z. Chen. Multi-label software behavior
learning. In Proceedings of the 34th International
Conference on Software Engineering , pages 1305{1308.
IEEE Press, 2012.
[9] Y. Feng, Z. Chen, J. A. Jones, C. Fang, and B. Xu.
Test report prioritization to assist crowdsourced
testing. In Proceedings of the 10th Joint Meeting on
Foundations of Software Engineering. New York:
ACM , pages 225{236, 2015.
[10] M. Ilieva and O. Ormandjieva. Automatic transition
of natural language software requirements
specication into formal presentation. In Natural
Language Processing and Information Systems , pages
392{397. Springer, 2005.
[11] N. Jalbert and W. Weimer. Automated duplicate
detection for bug tracking systems. In Dependable
Systems and Networks With FTCS and DCC, 2008.
DSN 2008. IEEE International Conference on , pages
52{61. IEEE, 2008.
[12] B. Jiang, Z. Zhang, W. K. Chan, and T. Tse.
Adaptive random test case prioritization. In
Automated Software Engineering, 2009. ASE'09. 24th
IEEE/ACM International Conference on , pages
233{244. IEEE, 2009.
[13] A. Kao and S. R. Poteet. Natural language processing
and text mining . Springer Science & Business Media,
2007.
[14] D. Kurlander and S. Feiner. Editable graphical
histories. In IEEE Workshop on Visual Languages ,
pages 127{134. Citeseer, 1988.[15] J. Laerty, A. McCallum, and F. C. Pereira.
Conditional random elds: Probabilistic models for
segmenting and labeling sequence data. 2001.
[16] A. Lamkan, S. Demeyer, E. Giger, and B. Goethals.
Predicting the severity of a reported bug. In 2010 7th
IEEE Working Conference on Mining Software
Repositories (MSR 2010) , pages 1{10. IEEE, 2010.
[17] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of
features: Spatial pyramid matching for recognizing
natural scene categories. In Computer Vision and
Pattern Recognition, 2006 IEEE Computer Society
Conference on , volume 2, pages 2169{2178. IEEE,
2006.
[18] Y. Ledru, A. Petrenko, and S. Boroday. Using string
distances for test case prioritisation. In Automated
Software Engineering, 2009. ASE'09. 24th
IEEE/ACM International Conference on , pages
510{514. IEEE, 2009.
[19] D. Liu, R. G. Bias, M. Lease, and R. Kuipers.
Crowdsourcing for usability testing. Proceedings of the
American Society for Information Science and
Technology , 49(1):1{10, 2012.
[20] K. Mao, L. Capra, M. Harman, and Y. Jia. A survey
of the use of crowdsourcing in software engineering.
RN, 15:01, 2015.
[21] A. Michail. Browsing and searching source code of
applications written using a gui framework. In
Proceedings of the 24th International Conference on
Software Engineering , pages 327{337. ACM, 2002.
[22] A. Michail and T. Xie. Helping users avoid bugs in gui
applications. In Proceedings of the 27th international
conference on Software engineering , pages 107{116.
ACM, 2005.
[23] M. Nebeling, M. Speicher, M. Grossniklaus, and M. C.
Norrie. Crowdsourced web site evaluation with
crowdstudy . Springer, 2012.
[24] A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, D. Lo,
and C. Sun. Duplicate bug report detection with a
combination of information retrieval and topic
modeling. In Automated Software Engineering (ASE),
2012 Proceedings of the 27th IEEE/ACM
International Conference on , pages 70{79. IEEE, 2012.
[25] F. Pastore, L. Mariani, and G. Fraser. Crowdoracles:
Can the crowd solve the oracle problem? In Software
Testing, Verication and Validation (ICST), 2013
IEEE Sixth International Conference on , pages
342{351. IEEE, 2013.
[26] A. Podgurski, D. Leon, P. Francis, W. Masri,
M. Minch, J. Sun, and B. Wang. Automated support
for classifying software failure reports. In Software
Engineering, 2003. Proceedings. 25th International
Conference on , pages 465{475. IEEE, 2003.
[27] A.-M. Popescu and O. Etzioni. Extracting product
features and opinions from reviews. In Natural
language processing and text mining , pages 9{28.
Springer, 2007.
[28] G. Rothermel, R. H. Untch, C. Chu, and M. J.
Harrold. Prioritizing test cases for regression testing.
Software Engineering, IEEE Transactions on ,
27(10):929{948, 2001.
[29] Y. Rubner, C. Tomasi, and L. J. Guibas. The earth
mover's distance as a metric for image retrieval.
212International journal of computer vision ,
40(2):99{121, 2000.
[30] P. Runeson, M. Alexandersson, and O. Nyholm.
Detection of duplicate defect reports using natural
language processing. In Software Engineering, 2007.
ICSE 2007. 29th International Conference on , pages
499{510. IEEE, 2007.
[31] I. Salman, A. T. Misirli, and N. Juristo. Are students
representatives of professionals in software engineering
experiments? In Proceedings of the 37th International
Conference on Software Engineering-Volume 1 , pages
666{676. IEEE Press, 2015.
[32] Q. Shi, Z. Chen, C. Fang, Y. Feng, and B. Xu.
Measuring the diversity of a test set with distance
entropy.
[33] E. Shutova, L. Sun, and A. Korhonen. Metaphor
identication using verb and noun clustering. In
Proceedings of the 23rd International Conference on
Computational Linguistics , pages 1002{1010.
Association for Computational Linguistics, 2010.
[34] C. Sun, D. Lo, S.-C. Khoo, and J. Jiang. Towards
more accurate retrieval of duplicate bug reports. In
Proceedings of the 2011 26th IEEE/ACM International
Conference on Automated Software Engineering , pages
253{262. IEEE Computer Society, 2011.
[35] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo. A
discriminative model approach for accurate duplicate
bug report retrieval. In Proceedings of the 32nd
ACM/IEEE International Conference on SoftwareEngineering-Volume 1 , pages 45{54. ACM, 2010.
[36] Y. Tian, D. Lo, and C. Sun. Drone: Predicting
priority of reported bugs by multi-factor analysis. In
2013 IEEE International Conference on Software
Maintenance , pages 200{209. IEEE, 2013.
[37] Y. Tian, C. Sun, and D. Lo. Improved duplicate bug
report identication. In Software Maintenance and
Reengineering (CSMR), 2012 16th European
Conference on , pages 385{390. IEEE, 2012.
[38] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun. An
approach to detecting duplicate bug reports using
natural language and execution information. In
Proceedings of the 30th international conference on
Software engineering , pages 461{470. ACM, 2008.
[39] X. Xia, Y. Feng, D. Lo, Z. Chen, and X. Wang.
Towards more accurate multi-label software behavior
learning. In Software Maintenance, Reengineering and
Reverse Engineering (CSMR-WCRE), 2014 Software
Evolution Week-IEEE Conference on , pages 134{143.
IEEE, 2014.
[40] L. Yu, W.-T. Tsai, W. Zhao, and F. Wu. Predicting
defect priority based on neural networks. In Advanced
Data Mining and Applications , pages 356{367.
Springer, 2010.
[41] T. Zimmermann, R. Premraj, N. Bettenburg, S. Just,
A. Schr oter, and C. Weiss. What makes a good bug
report? Software Engineering, IEEE Transactions on ,
36(5):618{643, 2010.
213