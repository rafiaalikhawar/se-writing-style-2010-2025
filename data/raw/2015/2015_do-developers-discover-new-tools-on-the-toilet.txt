Do Developers Discover New Tools On The Toilet?
Emerson Murphy-Hill
Google, LLC
emersonm@google.comEdward K. Smith*
Bloomberg
tedks@riseup.netCaitlin Sadowski
Google, LLC
supertri@google.comCiera Jaspan
Google, LLC
ciera@google.comCollin Winter*
Waymo
collinwinter@waymo.com
Matthew Jorde
Google, LLC
majorde@google.comAndrea Knight
Google, LLC
aknight@google.comAndrew Trenk
Google, LLC
atrenk@google.comSteve Gross
Google, LLC
stevegross@google.com
Abstract ‚ÄîMaintaining awareness of useful tools is a
substantial challenge for developers. Physical newslet-
ters are a simple technique to inform developers about
tools. In this paper, we evaluate such a technique, called
Testing on the Toilet, by performing a mixed-methods
case study. We Ô¨Årst quantitatively evaluate how eÔ¨Äec-
tive this technique is by applying statistical causal in-
ference over six years of data about tools used by thou-
sands of developers. We then qualitatively contextual-
ize these results by interviewing and surveying 382 de-
velopers, from authors to editors to readers. We found
that the technique was generally eÔ¨Äective at increasing
software development tool use, although the increase
varied depending on factors such as the breadth of ap-
plicability of the tool, the extent to which the tool has
reached saturation, and the memorability of the tool
name.
I. Introduction
Tools can help increase developer productivity by in-
creasing velocity and code quality. For instance, tools can
Ô¨Ånd concurrency bugs [28], reduce the eÔ¨Äort to analyze
customer feedback [14], and help conÔ¨Ågure caching frame-
works [10]. With an increasing number of tools becoming
available for developers to use, the opportunity to improve
productivity by increasing tool usage is enormous.
However, as the number of tools increases, so does
the diÔ¨Éculty for developers to gain awareness of relevant
tools. As Campbell and Miller argue, tools in major de-
velopment environments suÔ¨Äer from ‚Äúdeep discoverability‚Äù
problems [9]. The problem extends beyond software de-
velopment; in Grossman and colleagues‚Äô survey of Auto-
CAD users, a ‚Äútypical problem was that users were not
aware of a speciÔ¨Åc tool or operation which was available
for use‚Äù [20]. The problem is compounded at large compa-
nies like Microsoft [39], where developers create in-house
tools and wish to share them with peers.
To increase awareness and adoption of software tools
and practices, Google uses a technique called ‚ÄúTesting
on the Toilet‚Äù, or TotT for short (Figure 1). The TotT
episodes are 1-page printed newsletters, written by de-
velopers and posted in restrooms [6]. While originally
aimed at promoting testing tools and practices ‚Äì hence the
*Research performed while at Google.	

TTesting on the Toilet Presents...  Healthy Code on the Commode	

	

	
		
  		
	
	



 


	
 

 

				



				 	
		
	

	 
	




	 	 	
	
	 
 


!	 
"
	

	

	
	

 ! !"#
 #$%!&#'(()	
*+)((),-
)
	
		
	

	
	

 ! !"#+
+

 #$%!&#'(()	
*+)
((),-
)
 	 



 	 

#		
	 $ %&		

	
	 	 	"


'

			
	 



 


	
 "
 
	
(
 	
 
)	 

 *'	
)		
 		+ 				


' 	 	
		

			 $ 

	

		
















 		   
 
Fig. 1: TotT episode promoting clang-format .
name ‚Äì over the years TotT has become more inclusive of
other kinds of software development practices and tools.
Throughout the period of our study, episodes were dis-
tributed by volunteers; more recently, facilities staÔ¨Ä have
taken up distribution. Episodes are posted in restrooms
for about a week, until the next episode is posted.
Software developers have posted episodes at Google
since May 2006, and other organizations have invested in
similar eÔ¨Äorts. One such example is the Schibsted Group‚Äôs
Testing on the Toilet, which uses a format very similar
to our own [5]. Similarly, both Johns Hopkins Univer-
4652019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 ¬©2019 IEEE
DOI 10.1109/ICSE.2019.00059
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. sity and Harvard Law School publish restroom newsletters
that occasionally contain tips about features of university-
relevant software [2], [3]. To our knowledge, none of these
eÔ¨Äorts have been evaluated.
The main contribution of this paper is the Ô¨Årst evalua-
tion of the eÔ¨Äectiveness of TotT for distributing knowledge
about software development tools. While the approach has
garnered media attention [7], [19], [11], little is known
about TotT‚Äôs overall eÔ¨Äectiveness. To this end, we per-
formed a case study, a research methodology appropriate
for investigating ‚Äúa contemporary phenomenon within its
real-life context, especially when the boundaries between
the phenomenon and context are not clearly evident‚Äù [43].
Our study analyzed the usage of 12 command-line devel-
opment tools before and after the publication of a corre-
sponding episode, to evaluate the following hypothesis:
Hypothesis : Testing on the Toilet increases us-
age of advertised developer tools.
We apply CausalImpact, a Bayesian statistical technique
that was developed to evaluate the impact of advertising
on website traÔ¨Éc [8], to determine whether TotT had a sta-
tistically signiÔ¨Åcant impact on tool usage. We then provide
context for each tool‚Äôs usage over time by interviewing and
surveying software developers who read, edit, and author
episodes. Our results suggest that the technique is gener-
ally eÔ¨Äective at increasing tool awareness, although this
increase varies depending on each software development
tool and its context, such as the breadth of applicability
of the tool, the extent to which the tool has reached sat-
uration, and the memorability of the tool name.
II. Related Work
Studies of Tool Adoption. DiÔ¨Äusion of Innovations
seeks to understand how people adopt new ideas [37]. In
our study, we measure only the Ô¨Årst phases of the diÔ¨Äusion
of innovations, from knowledge to implementation, where
developers gain awareness of the tool and begin to employ
the tool. We assume that while developers may gain aware-
ness and some knowledge about the tool through TotT,
their decisions about whether to fully adopt a tool are
driven by properties of the tool and the developers‚Äô work
context, rather than by TotT. Thus, we do not expect
that TotT itself drives long-term tool adoption, beyond
inspiring developers to try the tool.
Several researchers have investigated diÔ¨Äusion of inno-
vations in the Ô¨Åeld of software development. In earlier
work using surveys, Fichman and Kemerer found that for
database management systems, programming languages,
and computer aided software engineering tools that are
purchased, use of purchasing data is a poor predictor of
actual adoption with companies [15]. Iivari‚Äôs surveys found
that lack of management support is a signiÔ¨Åcant contrib-
utor for non-adoption of software-engineering tools [22].
More recently, Witschey and colleagues surveyed develop-
ers about their adoption of security tools; they found that
the strongest predictor of a developer‚Äôs likelihood to use atool was their ability to observe their peers using tools [42],
[41]. Murphy-Hill and colleagues interviewed software de-
velopers about how they Ô¨Årst discovered tools, Ô¨Ånding that
the most eÔ¨Äective way developers learned about new tools
was through their peers [34]. In contrast to these studies,
we rely on tool usage logs and statistical analysis to un-
derstand tool usage, which avoids the pitfalls of relying
exclusively on self-reported data.
Studies of Restroom Advertising. Outside of software
engineering, several previous research studies have eval-
uated restroom advertising. HoÔ¨Äman found that 48 men
could more often recall Ô¨Çiers placed above restroom uri-
nals than in a study area [21]. Kaltenbaugh and colleagues
found that 217 survey respondents reported that sports
advertisements placed above restroom paper towel dis-
pensers were easy to read and placed in an appropriate lo-
cation [24]. Lehmann and Shemwell found that the major-
ity of 146 survey respondents recalled features of an adver-
tisement placed above urinals and near mirrors in restroom
bars, even though respondents ‚Äúmight be somewhat im-
paired due to alcohol consumption‚Äù [26]. Our study goes
beyond such self-reported perception and recall data col-
lected in these prior studies, and instead relies on actual
usage of the product.
Like our study, Mackert and colleagues evaluated the
eÔ¨Äect of advertisements on objective outcomes. In that
study, researchers concealed for 30 hours in bathroom
stalls observed the handwashing behavior of 1005 people,
before and after introducing pro-handwashing lobby and
restroom posters [30]. The posters did not signiÔ¨Åcantly in-
crease handwashing. In contrast, we studied several soft-
ware engineering posters introduced at diÔ¨Äerent points in
time, and did not use covert restroom surveillance.
Approaches to Improving Tool Usage. Singer found
that gamiÔ¨Åcation can increase use of features of the git
version control system [38]. Cockburn and Williams found
that pair programming can inÔ¨Çuence the tools that devel-
opers discover [12].
Some systems recommend new tools by observing cur-
rent tool use [27], [31], [32]. In the context of software
development tools, Murphy-Hill and colleagues used col-
laborative Ô¨Åltering and other techniques to automatically
recommend IDE commands [33]. Other recommender sys-
tems for software engineering have taken a task-oriented
approach. Viriyakattiyaporn and Murphy‚Äôs Spyglass sys-
tem used patterns of tool non-usage to recommend pro-
gram navigation tools [40]. Similarly, WitchDoctor and
BeneFactor can recognize when a developer is refactor-
ing without a tool, and then recommend completing the
refactoring using the tool [16], [17].
While studies of these systems evaluate personalized rec-
ommender systems, ours evaluates a substantially diÔ¨Äerent
approach, that of non-personalized newsletters. Our study
also investigates tool usage with an order of magnitude
more developers than in those studies.
466
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. III. Methodology
To investigate Testing on the Toilet‚Äôs eÔ¨Äectiveness for
promoting software development tools, we synthesized
data from both quantitative and qualitative sources. We
Ô¨Årst analyzed the logs of command-line software develop-
ment tools (Section III-A) before and after twelve episodes
appeared (Section III-B). We then contextualized these
results with qualitative data from three perspectives, that
of episode authors (Section III-C), episode editors (Sec-
tion III-D), and episode readers (Section III-E). Blank
surveys and interview scripts are available online [1] to
increase the replicability of our study.
A. Analysis of Tool Logs
Google collects log data of how employee software de-
velopers use command-line tools. Most development at
Google occurs on Linux workstations using a uniform, cen-
trally built toolchain; every binary built for the worksta-
tion Ô¨Çeet creates a syslog entry on start. This data provides
the number of developers per day using each tool.
We use a statistical package called CausalImpact, which
analyzes time series data to determine whether an inter-
vention had an impact on that data [8]. For our analy-
sis, we used daily unique users as the time series and the
date on which the episode was released as the interven-
tion. CausalImpact creates a model of the counterfactual
of tool usage; that is, the expected usage of the tool had
the intervention not occurred. We used three sources of
data for the model:
‚Ä¢First, we used the usage of the tool before the episode
was published. Intuitively, for example, if a tool gains
one user per week before the episode is published, one
would naturally expect the tool to continue gaining one
user per week. CausalImpact then uses this data as an
empirical hyperparameter to its Bayesian model.
‚Ä¢A second empirical hyperparameter is a set of con-
trol groups for which the intervention did not occur.
Here we used other tools that were not promoted in an
episode, that is, every other command-line tool used at
Google, more than 10,000 tools in total. Although many
of these tools bear little resemblance to our TotT tools,
CausalImpact accounts for this by reducing the weights
of dissimilar tools, eÔ¨Äectively discarding them.
‚Ä¢Third, because usage across days is not uniform, espe-
cially on weekends, we included a 7-day seasonal hyper-
parameter into CausalImpact‚Äôs model.
For each tool, CausalImpact needs a pre-period from
which to measure trends in baseline tool usage, and a
post-period against which to compare counterfactual tool
use. To help establish these periods, we re-ran a sensitivity
analysis by generating 972 CausalImpact models with pre-
and post-periods of between one week and more than two
years, using a dozen control tools to enable suÔ¨Éciently
fast analysis. The results were largely consistent across
pre-periods of diÔ¨Äering lengths; consequently, for the mainanalysis reported in this paper, we chose six months to bal-
ance being long enough to reasonably establish baseline us-
age, but short enough to exclude potentially confounding
factors, such as other interventions like social media posts
about the tool. For varying post-period lengths, tool us-
age rates were substantially diÔ¨Äerent. Inspecting the data,
there were two reasons: tools whose usage showed an initial
uptick that later subsided, and tools whose usage is inÔ¨Çu-
enced by some later intervention. Consequently, we need
to intelligently choose a post-period duration. Since we ex-
pect the eÔ¨Äect of TotT itself to last only a short amount of
time (Section II), we chose 3 three weeks as a reasonable
period because each episode is scheduled to be posted for
one week, but may linger for some time afterwards.
B. Episodes
We next gathered all episodes describing command-line
software development tools for which we could gather suf-
Ô¨Åcient data to draw conclusions. To achieve this goal, we
used the following criteria:
‚Ä¢The episode must have been published during the six
the years for which we had tool log data available. 255
episodes met this criterion.
‚Ä¢The episode must introduce a development tool that can
be invoked on the command line so that we could cap-
ture tool log data. 27 episodes met this criterion.
‚Ä¢The episode must not have been published during a gap
in the tool log data. Such gaps occurred three times,
lasting between several weeks and several months, and
aÔ¨Äected all tool data. One tool had only six weeks of
data between the start of the tool logs and publication
of its episode, so we excluded this tool as well. A total
of seven episodes were excluded using this criterion.
‚Ä¢The tool must have had at least 10 daily users at some
point during the pre- or post-period. We excluded such
tools because we found their data too sparse to rea-
son about, either statistically or intuitively. Six episodes
were excluded using this criterion.
‚Ä¢The episode author still worked at Google and agreed to
be interviewed, to provide context. Two episodes were
excluded using this criterion.
Twelve suitable episodes remained after this process.
C. Author Perspectives
To understand the context of usage for each software de-
velopment tool, we characterized the process from the per-
spective of developers familiar with each tool: the episode
authors. These authors are developers who write the con-
tent of an episode and reÔ¨Åne it with feedback from do-
main subject matter experts, editors, and any other in-
terested developers. In many but not all cases, these were
developers who were involved with creating the tool. We
additionally talked to one developer who authored four
episodes that, coincidentally, were all published during the
tool log gaps; while we don‚Äôt discuss his episodes specif-
ically, we include his interview data. We Ô¨Årst asked in-
467
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. terviewees about their expectations before publishing the
episode and their recollection of whether it was successful.
We then showed them a plot of their tool‚Äôs usage before
and after the episode, including CausalImpact‚Äôs counter-
factual predictions; we then asked them for explanations
of trends in the tool‚Äôs usage. The Ô¨Årst author analyzed this
data by transcribing audio interviews (one author declined
audio recording; here we used handwritten notes), then
open coded the transcripts. A secondary source of data
we use were surveys of 59 episode authors, which asked re-
spondents for feedback about the publishing process, and
included questions about process satisfaction and how the
developers measured the impact of their episode.
D. Editor Perspectives
To understand editors‚Äô perspectives on the eÔ¨Äectiveness
of TotT, we used two sources of data. First, we used doc-
umentation about the tools and the history of the TotT
initiative. Second, we interviewed four developers involved
in editing episodes. These editors are responsible for choos-
ing which episodes are appropriate for publication, provid-
ing feedback on episodes, soliciting feedback from subject
matter experts, and managing the publication process. We
used the same qualitative methodology as we used with
author interviews. We selected two developers who were
involved in TotT during its inception, and two develop-
ers who are currently involved in TotT editing. We asked
editors about their general expectations about the eÔ¨Äect
of publishing episodes about tools. We then named the
tools we analyzed, allowed them to select a tool or two of
interest, showed them the usage plots for those tools, and
asked whether the plots matched their expectations.
E. Reader Perspectives
To understand the TotT reader perspective, we turned
to several sources of data. To determine how many engi-
neers actually encounter episodes, we surveyed developers
on a general mailing list about their oÔ¨Éce location and
viewing habits; we received 234 responses.
Because episode authors and editors may provide an
especially positive view on the eÔ¨Äectiveness of TotT, we
speciÔ¨Åcally sought out countervailing viewpoints from au-
thors in two ways. First, we analyzed internal communi-
cations on a channel that is known Google-wide for jokes
about developer frustrations, searching for internal jokes
that contained ‚ÄúTotT‚Äù. Second, to determine why develop-
ersdo not learn about a tool from TotT, we chose a tool
with substantial usage after the episode, then surveyed
developers who (a) became employees at least 2 months
before the episode was published, (b) used the tool, but
only started to do so 6 months or more after the episode
was published, and (c) have invoked the tool at least 100
times. We inferred that these developers found the tool
useful, then asked them why they didn‚Äôt start using the
tool immediately after the episode was published. 77 de-
velopers responded out of 276 surveyed.F. Threats to Validity
Before we discuss the results of our study, it‚Äôs worth dis-
cussing the major issues regarding validity of our methods.
‚Ä¢We assume that the number of unique users of a tool
per day is good measure of software development tool
discovery, but other measures of discovery could be used,
such as developer self-reports.
‚Ä¢How well TotT would work at another company de-
pends on the extent to which Google is similar to that
company. Google is large, multi-site, software-focused,
founded about 20 years ago, and based in Silicon Valley.
Google uses a substantial amount of shared infrastruc-
ture used by most developers in the form of a monolithic
repository [23].
‚Ä¢We focused on command-line tools, but tools invoked
diÔ¨Äerently may experience distinct usage patterns when
communicated through TotT.
‚Ä¢A post-TotT tool usage increase might not be caused
by the episode. For example, the eÔ¨Äects of co-occurring
promotions of a tool will be conÔ¨Çated with the eÔ¨Äects
of the episode. Because CausalImpact has no way to
control for this eÔ¨Äect, we attempt to mitigate this by
asking interviewees about other tool advertisements and
pointing them out to the reader in the relevant plots.
‚Ä¢Authors were asked about episodes that were up to six
years old; authors may have diÔ¨Éculty remembering the
events surrounding the episode. To mitigate this, we
brought the relevant episode for the author‚Äôs reference.
‚Ä¢Tool usage may be inÔ¨Çuenced by Ô¨Çuctuations in distribu-
tion, which may vary from week to week. For example,
volunteers who post episodes in the bathroom may go
on vacation in a given week. Likewise, the quality of the
writing in the episode may vary from episode to episode,
but we did not attempt to measure quality.
‚Ä¢Our analysis of jokes is opportunistic but incomplete, be-
cause some beneÔ¨Åts or drawbacks may not be amenable
to jokes. Thus, the joke analysis cannot be expected to
identify all beneÔ¨Åts and drawbacks of TotT.
We used member checking [13] to improve validity overall
by inviting editors and authors to comment on drafts of
this paper. Fourteen interviewees provided comments on
our results, ranging from minor wording to reinterpreta-
tion of existing plots. We adjusted our wording and inter-
pretation, sometimes after a back-and-forth discussion.
IV. Tools Studied
We next provide brief descriptions of the 12 tools
we studied. With the exception of clang-format and
iblaze , we have renamed the tools in this paper for con-
Ô¨Ådentiality reasons.
a) Code Formatters.: clang-format is an open
source code formatter for C, C++, Objective-C,
JavaScript, Java, and protocol buÔ¨Äers [4]. Google has a
strictly-enforced coding style for each of these languages.
PythonFormatter is similar to clang-format , but is
468
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. targeted at Python. PythonFormatter is an internal
tool that removes Python lint errors and reformats the
source code.
b) Build and Integration Tools.: iblaze is a wrapper
around blaze, our centralized build tool; blaze is used by
most developers at Google. iblaze re-runs blaze whenever
a dependency of any target aÔ¨Äected by the command is
modiÔ¨Åed. This allows a developer to see build or test re-
sults without having to manually re-run a command. Ver-
ifyDetermBuild is a tool designed to debug long builds.
The build system reuses cached build results when possi-
ble, but if binaries change between builds (e.g. if a times-
tamp is included in the binary), the new binary causes
a cache miss. The tool VerifyDetermBuild runs two
builds, then reports any outputs that diÔ¨Äer. The developer
then diagnoses and Ô¨Åxes the problem. EmulatorSettings
is a wrapper script on top of the standard emulator for
the Android mobile operating system, using default set-
tings designed for interactive development and debugging,
optimized for our internal development environment.
c) Quality Assurance Tools.: SandboxManager is
a framework for describing, launching, and tearing down
full system stacks, or sandboxes, for end-to-end and ex-
ploratory testing. UIDiÔ¨Ä is a tool used to evaluate
changes to web application user interfaces. Users are able
to take screenshots showing how their application appears
in diÔ¨Äerent web browsers, both before and after a pro-
posed change; the resulting images are shown side-by-side
together with a visual diÔ¨Ä. Coverage is a test coverage
tool (the real name of this tool is a German word that
may be diÔ¨Écult to remember for those who do not speak
German). While Coverage is typically run automatically,
here we analyze the manual invocations, which are useful
when the a developer wants to try the tool or check code
coverage before sending a change for review.
d) Production Tools.: EstimateResources analyzes
changes to the conÔ¨Åguration of production server jobs, to
see if the job will still Ô¨Åt into the resources (e.g. mem-
ory) available on the machines and clusters on which they
run.ChangeTimeZone can assist developers who go ‚Äúon
call‚Äù, so that they can quickly respond to problems in
deployed software. Developers specify on-call periods in
those Ô¨Åles using the time zone of Google‚Äôs headquarters,
for historical reasons. Consequently, developers not in that
time zone must do the conversion manually. Change-
TimeZone automates this conversion by letting the de-
veloper specify times in any time zone.
e) Other Tools.: GenerateDoc is an documentation
generator for source code packages. It uses the number of
imports to rank source Ô¨Åles by their popularity, and gener-
ates a README Ô¨Åle in Markdown that includes the most
popular source Ô¨Åles, the number of references to them,
and text from their top-level documentation comments.
PatchSearch searches commits, where developers can is-
sue queries like ‚ÄúWhat commits have I merged in the last
year?‚Äù Developers can issue queries through a web inter-Absolute Relative p
SandboxManager -2.4 -15% .052
UIDiff 1.2 26% .017
EstimateResources 8.3 58% .001
Coverage 9.1 68% .001
EmulatorSettings 15 80% .001
ChangeTimeZone 2.7 111% .018
PatchSearch 14 152% .007
clang-format 138 189% .002
iblaze 264 378% .001
VerifyDetermBuild 12 1000% .001
PythonFormatter 63 1710% .001
GenerateDoc 28 5233% .001
TABLE I: The eÔ¨Äect of TotT on tool usage. Absolute
means the number of unique daily users over the coun-
terfactual. Relative means the percent increase of daily
users over the counterfactual.
face or through the the command-line tool that we analyze
here. While the web interface is more commonly used, the
command line version is especially useful as part of scripts.
V. Results
The core of our results explore quantiatively whether
TotT is eÔ¨Äective (Section V-A). We then describe sitau-
tions when TotT is not eÔ¨Äective (Section V-B), explore fac-
tors beyond TotT that inÔ¨Çuence tool usage (Section V-C),
compare TotT to other tool knowledge distribution tech-
niques (Section V-D), and explain outcomes of TotT be-
yond spurring tool usage (Section V-E).
A. TotT‚Äôs EÔ¨Äectiveness
Table I quantitatively summarizes the results of the 12
tools we studied. The Ô¨Årst column indicates the name of
the tool. The second column indicates the absolute ef-
fect, the total number of unique daily users gained over
the counterfactual predicted by CausalImpact. The third
column indicates the relative eÔ¨Äect, the percent increase
of daily users over the counterfactual. The Ô¨Ånal column
indicates the p-value, the counterfactual probability of a
causal eÔ¨Äect. The rows are ordered by relative eÔ¨Äect. To
take an example from Table I, iblaze had about 264 more
users per day, compared to the predicted value, over the
three week period after its episode. This 378% increase
was statistically signiÔ¨Åcant.
Figure 2 shows the usage of each of the tools before and
after each tool‚Äôs episode. For example, Figure 2a shows
an example plot of the usage of the tool clang-format
before and after it was promoted in TotT. In each plot,
the x-axis represents time and the y-axis represents the
number of unique developers using that tool on that day.
The solid black line represents the actual usage of the tool;
all tools exhibit a weekly cycle, in which each tool is used
less on the weekends. Translucent bubbles represent Ô¨Årst-
time users of a tool; the larger the bubble, the more Ô¨Årst
time users that day. Vertical lines indicate dates of interest:
a solid grey line indicates the day prior to the tool‚Äôs episode
being published; the grey dotted line afterwards represents
469
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. 
-DQ )HE 0DU $SU 0D\ -XQ8QLTXH8VHUV3HU'D\ 8L 8 3 '
(a)clang-format
$XJ 6HS 2FW 1RY 'HF8QLTXH8VHUV3HU'D\ 8L 8 3 '
(b)PatchSearch

$SU 0D\ -XQ8QLTXH8VHUV3HU'D\ 8L 8 3 '
(c)PythonFormatter
$SU 0D\ -XQ -XO8L 8 3 '
(d)GenerateDoc

6HS 2FW 1RY 'HF -DQ )HE 0DU8QLTXH8VHUV3HU'D\ 8L 8 3 '
(e)iblaze
6HS 2FW 1RY 'HF -DQ8QLTXH8VHUV3HU'D\ 8L 8 3 '
(f)VerifyDetermBuild

0DU $SU 0D\ -XQ -XO $XJ 6HS 2FW8L 8 3 '
(g)EmulatorSettings
0DU $SU 0D\ -XQ -XO8QLTXH8VHUV3HU'D\ 8L 8 3 '
(h)SandboxManager

$XJ 6HS 2FW 1RY8L 8 3 '
(i)ChangeTimeZone
'HF -DQ )HE 0DU $SU 0D\8QLTXH8VHUV3HU'D\ 8L 8 3 '
(j)UIDiff

0D\ -XQ -XO $XJ8QLTXH8VHUV3HU'D\ 8L 8 3 '
(k)EstimateResources
6HS 2FW 1RY 'HF -DQ8QLTXH8VHUV3HU'D\ 8L 8 3 '
(l)Coverage
Fig. 2: Daily tool usage rates, before and after episodes (solid grey vertical lines).
470
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. 3 weeks after the episode was published; and, shown in
some plots, the grey dotted line prior represents the six
months before the episode was published. This last line is
shown only when the corresponding date is visible on the
plot. The blue dotted line represents the counterfactual
usage, predicted by CausalImpact. The blue shaded region
around the blue line represents the 95% conÔ¨Ådence interval
for the prediction. As explained in Section III-B, gaps in
the underlying usage data are visible for some tools, most
obviously for iblaze (Figure 2e, mid-November through
late January). In each plot we show diÔ¨Äerent x-axis limits
to illustrate particular features of each time series, but
online we include plots over full 1-year periods [1].
Overall, the results suggest that usage of all tools but
one were increased as a result of their promotion via TotT,
conÔ¨Årming our hypothesis that TotT increases usage of ad-
vertised developer tools. However, on visual inspection of
Figure 2, several usage patterns emerge. In the top plots
(Figures 2a‚Äì 2f), we see strong growth after the episode is
released. In the case of the three plots on the top-left quad-
rant (2a, 2c, 2e), growth is sustained. In contrast, the plots
in the top-right quadrant (2b, 2d, 2f) show a drop-oÔ¨Ä af-
ter the episode is released. This contrast may be attributed
to each tools‚Äô designs; clang-format ,PythonFormat-
ter, and iblaze are all designed to be used on a daily
basis, while PatchSearch ,GenerateDoc , and Veri-
fyDetermBuild are designed to be used on occasion.
The bottom plots (Figures 2g‚Äì2l) exhibit weaker growth.
B. What Limits TotT‚Äôs EÔ¨Äectiveness
Although TotT appears eÔ¨Äective at distributing tool
knowledge, our results suggest several challenges.
1) Irregular Coverage: Interviewees complained about
uneven coverage of episodes amongst oÔ¨Éces and build-
ings. When we surveyed developers via internal mailing
lists about this, several smaller oÔ¨Éces conÔ¨Årmed uneven
coverage; apparently some oÔ¨Éces did not have volunteers
distributing episodes. In our distributed oÔ¨Éces that re-
ceived more than 10 responses each (one in the US and
two in Europe), 78% of respondents reported usually see-
ing an episode in their restrooms. At our headquarters,
which contains dozens of buildings, only 47% of respon-
dents reported usually seeing an episode in their restroom.
This Ô¨Ånding was conÔ¨Årmed by our other set of survey re-
spondents, the developers who use iblaze now but did
not around the time the episode appeared. Of those, 21 re-
spondents were certain that they had not seen the episode,
often blaming poor coverage in their building. The editors
conÔ¨Årmed that this is sometimes the case, as TotT is dis-
tributed by volunteer developers; when those developers
go on vacation, move around, or leave the company, dis-
tribution can be disrupted. Since facilities staÔ¨Ä have taken
up distribution recently, this problem should be alleviated.
Similarly, developers made jokes that expressed frustra-
tion about episodes in restrooms not being regularly up-
dated. On the other hand, slow-to-update episodes canhave beneÔ¨Åts; two iblaze survey respondents reported
learning about iblaze from an old episode that had not
been replaced by a more recent episode.
2) Quality Control: Quality issues prevent software de-
velopment tools from being discovered by preventing them
from appearing in a episode in the Ô¨Årst place. While au-
thors may be primarily motivated to write an episode to
boost the user base of their tool, editors are motivated to
publish episodes that increase the productivity of Google
developers. When authors are not able to convince editors
of their tool‚Äôs merit, editors can reject the episode. Edi-
tors reported rejecting proposed episodes because the tools
described in them had too few users and thus had not es-
tablished the tool as best practice. They also reported that
rejected authors sometimes complained, asking how could
they establish the tools as a best practice without having
a wider set of users. Although editors regarded themselves
as guardians of quality and protectors of developers‚Äô best
interests, they also noted that over the history of TotT,
the supply of proposals for episodes waxed and waned,
enabling them to be more selective at times and forcing
them to accept some lower quality episodes at others.
Interviewees noted that tools are scrutinized before their
episodes are published. For example, GenerateDoc ‚Äôs au-
thors noted that the editors complained that the tool was
not yet good enough because the tool should not be a
command-line tool run by individual developers, but in-
stead should be available as a service that displays docu-
mentation on demand. The authors agreed that a service
was likely the optimal architecture for this tool. However,
to convince the editors that an episode about Generate-
Doc was worthwhile, the authors solicited and received
testimonials from inÔ¨Çuential developers managing Google-
wide C++ and Java ecosystems.
3) Other Discovery Challenges: The jokes shed light on
several viewpoints on TotT not expressed by interviewees.
One joke noted that episodes are overly idealistic about
software engineering in practice; as a consequence, advice
given in an episode may not be as as straightforward to
apply as advertised. Another joke noted that the bath-
room is a socially awkward place to spend any signiÔ¨Åcant
amount of time reading. Another complained of episodes
being posted too far from the toilet to be read.
Readers who used iblaze noted three other challenges
that prevented them from discovering iblaze from TotT:
‚Ä¢Team Incompatibility. Several respondents said iblaze
was not useful to the team they were on.
‚Ä¢Technology Incompatibility. Some respondents used an
old technology that did not work with iblaze
‚Ä¢Inability to Appreciate Usefulness. Some respondents
noted that they didn‚Äôt Ô¨Ånd iblaze useful at the time, be-
cause they were new employees, didn‚Äôt Ô¨Ånd the tool pro-
vided a big productivity gain, didn‚Äôt Ô¨Ånd the episode‚Äôs
presentation compelling, or didn‚Äôt Ô¨Åt with their devel-
opment style.
471
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. C. Other InÔ¨Çuences on Tool Discovery
We also found that the usage or lack of usage of soft-
ware development tools after TotT is partly a consequence
of the design of the tool itself, especially in terms memo-
rability, trialability, breadth of applicability, and usability.
1) Memorability: While TotT has an advantage over so-
cial media in that episodes are read in a fairly information-
sparse environment, its episodes exist across the gulf of
physical and mental space that separates developers‚Äô work
environments from their private ones. The jokes com-
mented on this tension, exultating the victory of retain-
ing a URL on the journey back to one‚Äôs workstation and
lamenting over the diÔ¨Éculty in doing so.
Three tool authors believed the memorability of their
tool name inÔ¨Çuenced usage. iblaze ‚Äôs author attributed
the success of promoting his tool in TotT to the tool‚Äôs
high memorability, because the name ‚Äò iblaze ‚Äô is a one-
letter addition to the standard build tool, blaze . On the
other hand, ChangeTimeZone ‚Äôs author attributed the
tool‚Äôs lack of wide usage to readers forgetting about it
immediately after they returned to their desks from read-
ing about the tool in the restroom. SandboxManager ‚Äôs
author stressed memorability further, arguing that it was
critical for some kinds of tools such as SandboxMan-
ager , which may not be used the same day that it‚Äôs read
about in the restroom. Instead, he argues, developers read
about the tool, then must recall it later when they have a
problem that the tool solves.
2) Trialability: How easy it was to try a tool also may
have inÔ¨Çuenced usage, a property called trialability in dif-
fusion of innovations theory [37]. Based on the number of
questions that developers asked about UIDiff on mail-
ing lists, the author initially felt that the episode had a
lot of impact, but was surprised by how little impact the
episode had on users in retrospect. The author attributes
this to diÔ¨Éculty in trying the tool, which required the user
to collect a substantial amount of up-front information to
determine if the tool would be useful in their context and
to conÔ¨Ågure it fairly extensively. Likewise, authors of Es-
timateResources attributed a lack of wide usage to low
trialability because the tool takes a while to set up.
3) Other Properties: Participants attributed several
other properties to their tools‚Äô usage rates. For Change-
TimeZone , although Google has dozens of oÔ¨Éces globally,
many developers work at the headquarters or in its time
zone. So, many developers who would see ChangeTime-
Zone ‚Äôs episode would not Ô¨Ånd it relevant. VerifyDe-
termBuild ‚Äôs author acknowledged that usability of his
tool was a challenge; the tool‚Äôs output is verbose and does
not present a solution to the problem. PythonFormat-
ter‚Äôs author believed that user expectations inÔ¨Çuenced
usage rates. While the number of users is sustained be-
yond the episode, it does not grow, in contrast to clang-
format . The author attributes this to users expecting
that PythonFormatter worked like clang-format :a t
the time of the episode, it was substantially more limitedand did not reformat Python Ô¨Åles but instead only Ô¨Åxed
a small set of lint errors. Finally, the author of Cover-
age noted that his advertisement was successful because
it was conductive to the TotT format, which forces au-
thors to concisely state why their tool is useful from a
non-toolsmith‚Äôs perspective. He noted that Coverage is
simple enough that it could easily be explained quickly, so
the episode format was ideal. Editors agreed that brevity
and motivation are critical factors in the success of TotT,
and that working with authors to draft and reÔ¨Åne their
episodes is a time-consuming process, often because de-
velopers‚Äô initial drafts lack these properties.
D. TotT Versus Other Techniques
1) Social Media: Some authors used internal social me-
dia as well as Testing on the Toilet to promote their soft-
ware development tools. However, TotT consistently re-
sulted in a larger increase in the number of developers
using the tools. Authors mentioned advertising three tools
via social media:
‚Ä¢Visible at the left of Figure 2a, the episode author pro-
moted clang-format via a social networking site and a
mailing list. While the raw number of users gained from
TotT was higher, the social media campaign yielded a
higher proportion of new users than TotT. Moreover, so-
cial media showed 10% week-over-week growth (about
4 new users per week) in the weeks following the so-
cial media posts, whereas the episode was followed by
a 6% week-over-week growth (about 23 new users per
week). CausalImpact conÔ¨Årms the social media cam-
paign yielded an increase in tool usage ( p=.002) with a
higher relative impact (529%) than the episode (264%).
While social media caused what appears to be substan-
tial initial try-outs of the tool followed by a substantial
drop in the user base, the episode appears to show sus-
tained growth even after the episode was superseded.
One reason may be that the version promoted by the
social media release was likely buggier than the version
promoted by TotT.
‚Ä¢iblaze ‚Äôs creator promoted his tool through internal so-
cial media, which was shared by one well-known de-
veloper within Google about four months before the
episode. That post helped the tool grow from a peak of
20 daily users to a peak of 36, growth that CausalImpact
reports as non-signiÔ¨Åcant ( p=.119).
‚Ä¢ForVerifyDetermBuild in Figure 2f, the fourth peak
near the end of the publication period (that is, near the
dotted vertical line) may be due to a social media post
from a developer with about 3300 followers today. The
post mentioned VerifyDetermBuild as a means to
complain about a compiler that always produces diÔ¨Äer-
ent binaries. Another social media post about Verify-
DetermBuild , 9 months before the episode, yielded a
small but statistically signiÔ¨Åcant boost (0.15 users per
day, p=.02), though the poster in this case has around
350 followers today.
472
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. These results suggest that social media posts can increase
the number of developers using tools, but the magnitude
of the increase depends on several factors, including how
many followers the developer has. In contrast, because
each episode is distributed by the same group of volun-
teers, TotT‚Äôs inÔ¨Çuence is not limited by the individual
developer‚Äôs personal network. But as we discuss in Sec-
tion V-B2, maintaining the inÔ¨Çuence of TotT is challeng-
ing.
2) Other Advertising Mechanisms: Developers used
other ways of advertising their tools. In the case of Emu-
latorSettings , we notice the three peaks several months
before the episode in (Figure 2g). As the author suggested
and follow-up investigation of emails conÔ¨Årmed, the tool
creators sent out announcement emails to mailing lists
during the week the Ô¨Årst two peaks appeared (though we
have no explanation for the peak the week after). These
announcement emails were sent to several mobile develop-
ment and testing mailing lists. These emails caused growth
from about 8 daily users to about 30 daily users, peak-
to-peak, a signiÔ¨Åcant increase ( p= .03). A major diÔ¨Äer-
ence between the TotT and mailing list promotions is that
the mailing lists were more targeted at people who were
likely to use the tool (that is, mobile developers), com-
pared to the much more widely spread episode, which in-
cludes many developers who do not develop on Android.
Interestingly, after the mailing list promotion, daily users
drop to about pre-intervention levels. In comparison, after
the episode, daily usage remained high. One possible ex-
planation is that emails are more ephemeral than episodes;
as emails are forgotten, they are deleted or buried under
more emails, but when episodes are not updated, they can
linger in bathrooms for indeÔ¨Ånite periods, educating devel-
opers who pass through.
The creators of EstimateResources unsuccessfully
advertised their tool through a poster a few months be-
fore the episode. SpeciÔ¨Åcally, they promoted the tool at an
annual project fair at Google‚Äôs largest European software
development oÔ¨Éce. The one-day tool usage around this
date shows no appreciable impact from the poster, con-
Ô¨Årmed by CausalImpact ( p=.27). The authors attributed
this lack of success to the fact that their poster had poor
placement and that the product fair‚Äôs audience was much
broader than the tool‚Äôs potential audience.
When we asked developers who didn‚Äôt learn about
iblaze from the episode where they did learn the tool
from, they mentioned several other advertising mech-
anisms through which they discovered iblaze . Many
respondents reported discovering iblaze from internal
Google tutorials, which focused on other technologies, but
which made iblaze an integral part of the activities. Three
respondents learned about iblaze from the iblaze badge,
a small hyperlinked graphic displayed on iblaze -using de-
velopers‚Äô proÔ¨Åle pages in the employee directory. While
the eÔ¨Äectiveness of these interventions relative to TotTis unclear, these results suggest they can be nonetheless
eÔ¨Äective to some extent.
3) Peer Learning: Developers, including several respon-
dents to our survey about iblaze , reported learning about
tools from peer developers. Learning tools from other de-
velopers was also mentioned by the author of the Patch-
Search episode, where usage of this tool increased grad-
ually in the years following the episode. In contrast, the
EmulatorSettings tool has seen relatively Ô¨Çat usage
over the past 1.5 years. Murphy-Hill and colleagues pro-
vide a plausible explanation [35]: EmulatorSettings is
not easily visible to others because it runs entirely on one
developer‚Äôs workstation, whereas PatchSearch is com-
monly integrated into scripts checked into version control,
which are then visible to the entire company.
Even if TotT does not itself raise awareness of a tool,
an episode can nonetheless be supportive of other forms
of tool discovery, such as through peer learning or social
media, but being a self-contained module of information
that developers can refer back to later. In the case of peer
learning, recommending developers can refer the learner
to an archived episode, and that episode can serve as a
self-contained witness to the tool‚Äôs usefulness.
E. Other TotT Motivations
In beginning this study, we assumed that authors‚Äô moti-
vation for publishing their tool in Testing on the Toilet was
to increase usage. However, we found three other factors
that motivated authors to share tools.
1) Preventing Reinvention by Establishing Usefulness:
An author noted that his desire to publish an episode
about SandboxManager was not so much about get-
ting more people to use SandboxManager , but instead
to get other teams in Google to stop building tools that
duplicate SandboxManager . He had personal experi-
ence with this; a team that he had just moved to had
implemented their own version of SandboxManager .
The team was resistant to using SandboxManager , ar-
guing that it was specialized for some other product within
Google, a variant of the Not Invented Here syndrome [25].
To evaluate how widely useful SandboxManager was
beyond the product it was built for, the author wrote an
episode. The editors of TotT acted as an informal com-
mittee to weigh in on whether SandboxManager was
widely useful.
2) Justifying Increased Investment: The PythonFor-
matter episode provided a critical mass of users that
justiÔ¨Åed increased investment in the tool‚Äôs development.
The episode author was able to spend development eÔ¨Äort
replacing the core of the tool, increasing its usefulness.
3) Inspiring Future Tools: Interviewees reported that a
major goal of releasing an episode about GenerateDoc
was to demonstrate the beneÔ¨Åts of automatic documen-
tation to the documentation infrastructure team. In fact,
the team took notice of GenerateDoc and subsequently
began work on their own approach.
473
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. VI. Discussion and Future Work
Overall, because 11 of 12 tools showed statistically sig-
niÔ¨Åcant increases in the number of daily unique users af-
ter each Testing on the Toilet was published, we conclude
that TotT was eÔ¨Äective for encouraging developers to try
new tools at Google. Compared to software engineering
recommender systems published in the research literature
(e.g. [33], [40]), TotT has several advantages: it does not
require substantial infrastructure or technical implementa-
tion, it does not require user targeting and customization,
and it can be used to showcase a variety of tools. However,
it has some disadvantages: it requires an organization-wide
network of people to distribute episodes, it requires the
writing of concise and convincing prose on a per-tool basis,
and it requires developers to be willing to have a private
space (restrooms) used for purposes other than their core
function.
In Section V-D, we saw that TotT increased the raw us-
age of tools more substantially than social media posts.
While the past 15 years in consumer advertising has
shifted towards being digital, personalized, and interac-
tive [29], our results are a reminder that physical inter-
ventions can still play an important role in shaping devel-
oper behavior. While social media can reach a wide vari-
ety of users, the physical form and context of TotT has
advantages as well. First, apart from one‚Äôs primary busi-
ness is a restroom, the generally low information density
in a restroom means developers‚Äô attentional demands are
relatively low. Second, unlike in a digital setting, a de-
veloper‚Äôs attention is sustained for a Ô¨Åxed period because
the biological processes that developers engage in while
reading TotT cannot be substantially sped up. Third, the
regularity of biological needs ensures repeated exposure to
episodes. Fourth, the one-episode-per-week pacing ensures
that developers aren‚Äôt deluged by many tools in a short
amount of time. Finally, because TotT is written by de-
velopers, a medium that might otherwise be an invasion of
personal space is instead a quirky‚Äîbut, as we have shown,
eÔ¨Äective‚Äîmedium to spread knowledge about tools.
Would TotT be equally eÔ¨Äective if posted in other
places? Lactation rooms are similar to restrooms in that
they both are used for biological needs, but lactation
rooms serve a narrower user base. Nap rooms can poten-
tially serve a wide audience, but are typically too dark
for reading in our experience. Cafeterias have similari-
ties as well, but at least at Google, time spent in cafete-
rias is culturally considered time to socialize with cowork-
ers. And while Google employees have experimented with
other spaces for posting Ô¨Çiers [6], restrooms have remained
the space of choice for TotT.
Participants complained of irregular coverage of
episodes in Section V-B1. While theoretically bathroom-
based Ô¨Çiers provide an equal opportunity to access TotT
information because ‚Äúeverybody poops‚Äù [18], the irregular
coverage across buildings practically means that not everydeveloper has an equal likelihood of encountering episodes.
Moreover, our experience as developers is that women are
less likely to encounter episodes than men. This seems sta-
tistically likely; since TotT was distributed by volunteer
developers and more developers are men, the likelihood
of a woman volunteering to post episodes in women‚Äôs re-
strooms in a given building is lower than that of a man,
all else being equal. Moreover, encouraging more women to
volunteer in an eÔ¨Äort to remedy the problem imposes a mi-
nority tax , ‚Äúthe burden of extra responsibilities placed on
[minorities] in the name of diversity‚Äù [36]. We believe that
we have resolved this recently by making Ô¨Çier distribution
a regular job responsibility of the facilities staÔ¨Ä. The mi-
nority tax problem could also be alleviated by replacing
gendered restrooms with gender-neutral restrooms.
While we have demonstrated that TotT generally in-
creased tool use at Google, we readily admit that we
have not shown that TotT consistently drives long term
tool adoption or that adopting the tools improves soft-
ware quality or developer productivity. To understand
whether TotT (or any other intervention, for that matter)
drives long-term adoption, other measures of adoption are
needed. Future researchers should consider Ô¨Çexible metrics
for software tool adoption; doing so may help us close the
loop and better understand the diÔ¨Äusion of innovations for
software engineering tools. Such metrics would ideally be
validated for construct validity and used in triangulation
with one another. Likewise, even if we were to show that
developers adopt tools over the long term, it remains an
open problem how one would quantify whether developers‚Äô
productivity or code is improved as a consequence of tool
use.
VII. Conclusion
As the number and breadth of available tools grow, soft-
ware developers need eÔ¨Äective techniques to keep abreast
of useful tools. Over the last decade, Google has been using
a paper-based technique, Testing on the Toilet. Analyzing
a large data set with six years of tool logs using causal
inference, then contextualizing that usage with perspec-
tives from authors, editors, and readers, we demonstrated
TotT‚Äôs ability to help developers to discover new tools.
VIII. Acknowledgments
Thanks to anonymous interviewees and reviewers,
Thomas Adamcik, Sameer Ajmani, Alexander Buc-
cino, Chris Conway, JeÔ¨Ä Cox, Adriano Cunha, Damien
Desfontaines, Sanjeev Dhanda, Lorenzo Dini, Marko
Ivankovic, Ivan Janicijevic, Daniel Jasper, Max Kanat-
Alexander, Thomas Knych, Raphael Menderico, Eric Nick-
ell, Maria-Hendrike Peetz, Viola Petra, Antoine Picard,
Rob Pike, Rachel Potvin, Martin Probst, Christoph Rau-
pach, Stefan Sauer, Rob Siemborski, Justin Smith, Bill
Wendling, JC van Winkel, and the CausalImpact team.
474
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. References
[1] https://Ô¨Ågshare.com/s/4b9e5bd8a6ab17a9f040.
[2] Lavnotes. Published by Sheridan Libraries at Johns Hop-
kins University, 2012. http://old.library.jhu.edu/about/news/
lavnotes/index.html.
[3] Bathroom reader. Published by Harvard Law School Li-
brary, 2017. https://www.scribd.com/collections/2835528/
Bathroom-Reader.
[4] Clangformat. LLVM Documentation, 2017. https://clang.llvm.
org/docs/ClangFormat.html.
[5] Schibsted testing on the toilet. Published on Twit-
ter, 2017. https://twitter.com/Schibsted_Eng/status/
862239498100518912.
[6] Mike Bland. Testing on the toilet, 2011. https://mike-bland.
com/2011/10/25/testing-on-the-toilet.html.
[7] Julie Bort. How Google convinced programmers to do something
called ‚Äòtesting on the toilet‚Äô, 2014. http://www.businessinsider.
com/googles-famous-testing-on-the-toilet-2014-12.
[8] Kay H Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy,
and Steven L Scott. Inferring causal impact using Bayesian
structural time-series models. The Annals of Applied Statistics ,
9(1):247‚Äì274, 2015.
[9] Dustin Campbell and Mark Miller. Designing refactoring tools
for developers. In Proceedings of the 2nd Workshop on Refac-
toring Tools , pages 1‚Äì2, 2008.
[10] Tse-Hsun Chen, Weiyi Shang, Ahmed E Hassan, Mohamed
Nasser, and Parminder Flora. CacheOptimizer: Helping de-
velopers conÔ¨Ågure caching frameworks for hibernate-based
database-centric web applications. In Procee dings of the 2016
24th ACM SIGSOFT International Symposium on Foundations
of Software Engineering , pages 666‚Äì677. ACM, 2016.
[11] Kurt Christensen. Unit testing tips from Google, 2007. https:
//www.infoq.com/news/2007/04/google-testing-tips.
[12] Alistair Cockburn and Laurie Williams. The costs and bene-
Ô¨Åts of pair programming. In Proceedings of the Conference on
Extreme Programming , pages 223‚Äì247, 2000.
[13] John W Creswell and Dana L Miller. Determining validity in
qualitative inquiry. Theory into practice , 39(3):124‚Äì130, 2000.
[14] Andrea Di Sorbo, Sebastiano Panichella, Carol V Alexandru,
Junji Shimagaki, Corrado A Visaggio, Gerardo Canfora, and
Harald C Gall. What would users change in my app? sum-
marizing app reviews for recommending software changes. In
Proceedings of the Symposium on Foundations of Software En-
gineering , pages 499‚Äì510. ACM, 2016.
[15] Robert G. Fichman and Chris F. Kemerer. The illusory diÔ¨Äusion
of innovation: An examination of assimilation gaps. Information
Systems Research , 10(3):255‚Äì275, 1999.
[16] Stephen R. Foster, William G. Griswold, and Sorin Lerner.
WitchDoctor: IDE support for real-time auto-completion of
refactorings. In Proceedings of the International Conference on
Software Engineering , pages 222‚Äì232, 2012.
[17] Xi Ge, Quinton L DuBose, and Emerson Murphy-Hill. Recon-
ciling manual and automatic refactoring. In Proceedings of the
International Conference on Software Engineering , pages 211‚Äì
221, 2012.
[18] Tar¬Ø o Gomi. Everyone poops . Kane/Miller Book Pubs., 2001.
[19] Sara Kehaulani Goo. Building a ‚ÄòGoogley‚Äô workforce, 2006.
http://www.washingtonpost.com/wp-dyn/content/article/
2006/10/20/AR2006102001461.html.
[20] Tovi Grossman, George Fitzmaurice, and Ramtin Attar. A sur-
vey of software learnability: metrics, methodologies and guide-
lines. In Proceedings of the Conference on Human Factors in
Computing Systems , pages 649‚Äì658, 2009.
[21] Karsten C Hofmann. Advertising in restrooms . PhD thesis,
Portland State University, 1988.
[22] Juhani Iivari. Why are CASE tools not used? Communications
of the ACM , 39(10):94‚Äì103, 1996.
[24] Lance P Kaltenbaugh, Janel C Molnar, Wesley N Bonadio, and
Brittany L Dorsey. A study on restroom advertising and its
[23] Ciera Jaspan, Matthew Jorde, Andrea Knight, Caitlin Sad-
owski, Edward K Smith, Collin Winter, and Emerson Murphy-
Hill. Advantages and disadvantages of a monolithic repository:
a case study at google. In Proceedings of the 40th International
Conference on Software Engineering: Software Engineering in
Practice , pages 225‚Äì234. ACM, 2018.eÔ¨Äect on awareness of campus recreation programs. Recreational
Sports Journal , 35(1):3‚Äì11, 2011.
[25] Ralph Katz and Thomas J Allen. Investigating the not invented
here (NIH) syndrome: A look at the performance, tenure, and
communication patterns of 50 R& D project groups. R&D Man-
agement , 12(1):7‚Äì20, 1982.
[26] Dominik Lehmann and Donald J Shemwell. A Ô¨Åeld test of the
eÔ¨Äectiveness of diÔ¨Äerent print layouts: A mixed model Ô¨Åeld ex-
periment in alternative advertising. Journal of Promotion Man-
agement , 17(1):61‚Äì75, 2011.
[27] Frank Linton, Deborah Joy, Hans-Peter Schaefer, and Andrew
Charron. OWL: A recommender system for organization-wide
learning. Educational Technology & Society , 3(1):62‚Äì76, 2000.
[28] Yepang Liu, Chang Xu, Shing-Chi Cheung, and Valerio Ter-
ragni. Understanding and detecting wake lock misuses for an-
droid applications. In Proceedings of the 2016 24th ACM SIG-
SOFT International Symposium on Foundations of Software
Engineering , pages 396‚Äì409. ACM, 2016.
[29] Matthew Lombard and Jennifer Snyder-Duch. Digital adver-
tising in a new age. Digital Advertising: Theory and Research ,
page 169, 2017.
[30] Michael Mackert, Ming-Ching Liang, and Sara Champlin.
‚Äúthink the sink:‚Äù preliminary evaluation of a handwashing pro-
motion campaign. American Journal of Infection Control ,
41(3):275‚Äì277, 2013.
[31] Carlos Maltzahn. Community help: discovering tools and locat-
ing experts in a dynamic environment. In Proceedings of the
Conference on Human Factors in Computing Systems , pages
260‚Äì261, 1995.
[32] Justin Matejka, Wei Li, Tovi Grossman, and George Fitzmau-
rice. CommunityCommands: command recommendations for
software applications. In Proceedings of the Symposium on User
Interface Software and Technology. , pages 193‚Äì202, 2009.
[33] Emerson Murphy-Hill, Rahul Jiresal, and Gail C. Murphy. Im-
proving software developers‚Äô Ô¨Çuency by recommending develop-
ment environment commands. In Proceedings of the 20th ACM
SIGSOFT Symposium on the Foundations of Software Engi-
neering , pages 42:1‚Äì42:11, 2012.
[34] Emerson Murphy-Hill and Gail C. Murphy. Peer interaction ef-
fectively, yet infrequently, enables programmers to discover new
tools. In Proceedings of the Conference on Computer Supported
Cooperative Work , pages 405‚Äì414, 2011.
[35] Emerson Murphy-Hill, Gail C Murphy, and Joanna McGrenere.
How do users discover new tools in software development and
beyond? Computer Supported Cooperative Work , 24(5):389‚Äì422,
2015.
[36] Jos√© E Rodr√≠guez, Kendall M Campbell, and Linda H Pololi.
Addressing disparities in academic medicine: what of the mi-
nority tax? BMC Medical Education , 15(1):6, 2015.
[37] Everett M. Rogers. DiÔ¨Äusion of Innovations . Free Press, 5th
edition, 2003.
[38] Leif Singer. Improving the Adoption of Software Engineering
Practices Through Persuasive Interventions . PhD thesis, Got-
tfried Wilhelm Leibniz Universitat Hannover, 2013.
[39] Edward K Smith, Christian Bird, and Thomas Zimmermann.
Build it yourself!: Homegrown tools in a large software company.
InProceedings of the 37th International Conference on Software
Engineering , pages 369‚Äì379. IEEE Press, 2015.
[40] Petcharat Viriyakattiyaporn and Gail C. Murphy. Improving
program navigation with an active help system. In Proceedings
of the Conference of the Center for Advanced Studies on Col-
laborative Research , pages 27‚Äì41, 2010.
[41] Jim Witschey, Olga Zielinska, Allaire Welk, Emerson Murphy-
Hill, Chris Mayhorn, and Thomas Zimmermann. Quantifying
developers‚Äô adoption of security tools. In Proceedings of the
2015 10th Joint Meeting on Foundations of Software Engineer-
ing, pages 260‚Äì271. ACM, 2015.
[42] Shundan Xiao, Jim Witschey, and Emerson Murphy-Hill. Social
inÔ¨Çuences on secure development tool adoption: Why security
tools spread. In Proceedings of Computer Supported Cooperative
Work and Social Computing , pages 1095‚Äì1106, 2014.
[43] Robert K Yin. Case Study Research . Sage, 1994.
475
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:36:42 UTC from IEEE Xplore.  Restrictions apply. 