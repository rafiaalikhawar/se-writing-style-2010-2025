Search-Based Synthesis of Probabilistic Models
for Quality-of-Service Software Engineering
Simos Gerasimou
Dept. of Computer Science
University of Y ork, UK
simos@cs.york.ac.ukGiordano Tamburrelli
Dept. of Computer Sciences
Vrije Universiteit, Netherlands
g.tamburrelli@vu.nlRadu Calinescu
Dept. of Computer Science
University of Y ork, UK
radu.calinescu@york.ac.uk
ABSTRACT
The formal veriﬁcation of ﬁnite-state probabilistic models
supports the engineering of software with strict quality-of-
service (QoS) requirements. However, its use in software
design is currently a tedious process of manual multiobjec-
tive optimisation. Software designers must build and ver-
ify probabilistic models for numerous alternative architec-
tures and instantiations of the system parameters. When
successful, they end up with feasible but often suboptimal
models. The EvoChecker search-based software engineering
approach and tool introduced in our paper employ multi-
objective optimisation genetic algorithms to automate this
process and considerably improve its outcome. We evaluate
EvoChecker for six variants of two software systems from
the domains of dynamic power management and foreign ex-
change trading. These systems are characterised by diﬀerent
types of design parameters and QoS requirements, and their
design spaces comprise between 2E+14 and 7.22E+86 rele-
vant alternative designs. Our results provide strong evidence
that EvoChecker signiﬁcantly outperforms the current prac-
tice and yields actionable insights for software designers.
1. INTRODUCTION
The design of modern software systems requires tools capa-
ble of assessing the quality and dependability of the solutio n
under design as early as possible in the engineering process.
The late discovery of defects leads to suboptimal operation
of the delivered systems, high maintenance costs, and neg-
ative impact on their users [14, 25]. This is particularly
relevant for business-critical and safety-critical applicatio ns,
where failures may cause ﬁnancial loss or loss of human life.
Model Checking (MC) [8, 24] is among the most eﬀective
means for developing software for critical applications [59].
Given a ﬁnite-state transition model of a concurrent system,
MC automatically veriﬁes if the model satisﬁes requirements
formally expressed in propositional temporal logic. MC can
be exploited during system design, to devise models that
meet the requirements of a software system, and are thenused as a basis for its implementation. Alternatively, soft-
ware engineers can build models of existing systems, and use
MC to check their compliance with requirements. Further-
more, a variant of MC called probabilistic model checking
can verify if software systems meet their reliability, perfor-
mance and other quality-of-service (QoS) requirements [44].
Probabilistic MC veriﬁes models with state transitions an-
notated by probabilities or transition rates (e.g., discrete
and continuous-time Markov chains, and Markov decision
processes), and operates with QoS requirements speciﬁed in
probabilistic variants of temporal logics.
This paper is concerned with the use of probabilistic MC at
design time to identify models that satisfy the QoS require-
ments of a software system. Currently, this is a tedious trial-
and-error process in which software designers must build and
verify probabilistic models for numerous alternative archi-
tectures and instantiations of the system parameters. Even
when this manual attempt at multiobjective optimisation
is successful, the models produced are often suboptimal in
terms of their tradeoﬀs between reliability, performance, cost
and other QoS requirements. The EvoChecker search-based
software engineering approach introduced in our paper au-
tomates this process and considerably improves its outcome.
To this end, EvoChecker synthesises (a) a set of probabilisti c
models that closely approximates the Pareto-optimal model
set associated with the QoS requirements of a software sys-
tem; and (b) the corresponding approximate QoS Pareto
front. This provides insight into the tradeoﬀs between mul-
tiple QoS requirements, enabling software designers to make
informed decisions about the architecture and parameters of
their systems. To achieve this, EvoChecker employs estab-
lished multiobjective optimisation genetic algorithms (MO-
GAs) and a ﬁtness function that captures the satisfaction of
the QoS requirements. EvoChecker uses as input:
•aprobabilistic model template that encodes alternative
architectures and parameter ranges for the system;
•a set of QoS requirements specifying both constraints
(e.g., “ At least 95% of the requests must be processed
within 200ms ”) and optimisation objectives (e.g., “ The
system should operate with minimal energy consumption ”).
EvoChecker uses the probabilistic model checker PRISM
[45] for its veriﬁcation steps. Accordingly, the probabilistic
model template is expressed in an extension of the PRISM
high-level modelling language, and EvoChecker handles all
types of probabilistic models and probabilistic temporal log-
ics supported by PRISM and shown in Table 1.The main contributions of our paper include:
1. The EvoChecker extension of the PRISM modelling lan-
guage, and the EvoChecker search-based model synthesis
approach, which we describe in Section 3, and we illus-
trate using a running example introduced in Section 2.
2. The deﬁnition of the probabilistic model synthesis problem
(Section 3.3).
3. The open-source EvoChecker tool presented in Section 4,
which we make freely available on our project webpage.
4. An extensive evaluation of the EvoChecker approach and
prototype tool in two case studies drawn from diﬀerent
application domains, which we summarise in Section 5.
Our evaluation based on established Pareto-front quality in-
dicators (i.e., unary epsilon [66], hypervolume [65] and in-
verted generational distance [58]) shows that EvoChecker
signiﬁcantly outperforms the current “trial and error” prac-
tice, makes eﬀective use of state-of-the-art MOGAs such as
NSGA-II [26], SPEA2 [49] and MOCell [64], and provides
actionable insights for software designers.
To the best of our knowledge, EvoChecker is the ﬁrst ap-
proach that uses search-based software engineering to syn-
thesise approximate Pareto-optimal sets of probabilistic mod -
els for the QoS requirements of software systems. The only
related work we are aware of is discussed in Section 6, and
belongs to the area of model repair [10, 16]. However, model
repair for probabilistic systems focuses on modifying an ex-
isting probabilistic model, only at transition probability le vel,
and only to satisfy a single temporal logic property that is
violated by the original model. In contrast, EvoChecker han-
dles multiple QoS requirements and uses multiobjective op-
timisation to derive an approximate Pareto-optimal set of
probabilistic models and its associated Pareto front starting
from a probabilistic model template.
2. RUNNING EXAMPLE
We will illustrate the EvoChecker components and opera-
tion using a software-controlled dynamic power management
(DPM) system adapted from [53, 56]. As shown in Fig. 1,
the system consists of a service provider that handles re-
quests generated by a service requester and stored in two
request queues of diﬀerent priorities. The service provider
has four states associated with diﬀerent power usage, i.e.,
busy, idle, standby andsleep. Fig. 1 depicts the power us-
age of each state (in watts), the possible transitions betwee n
states, and the energy consumed by each transitions (in
joules). These values are taken from [53], and correspond
to a Fujitsu disk drive.
Table 1: Types of models supported by EvoChecker
Type of QoS requirement
probabilistic model speciﬁcation logic
Discrete-time Markov chains PCTLa,L T Lb,P C T L *c
Continuous-time Markov chains CSLd
Markov decision processes PCTLa,L T Lb,P C T L *c
Probabilistic automata PCTLa,L T Lb,P C T L *c
Probabilistic timed automata PCTLa
aProbabilistic Computation Tree Logic [13, 34]
bLinear Temporal Logic [51]
cPCTL* is a superset of PCTL and LTL
dContinuous Stochastic Logic [7, 9]Power manager
High-priority
request queue
Low-priority
request queuebusy
2.15W
idle
0.95W
standby
0.35Wsleep
0.13WService
5.1J0.006J7J 0.067J 2J 0.001J0J 0Jstate
informationstate-transition
commands
qL
Qmax LqH
Qmax HDynamic power
management
system
provider
Service
requester
Figure 1: Dynamic power management system
Table 2: Average service-provider transition times
State Average State Average
transition time (s) transition time (s)
idle →standby 0.4 standby →idle 1.2
idle →sleep 0.67 sleep →idle 1.6
standby →sleep 0.3 sleep →standby 0.6
When the service provider is in the busy state, it processes
requests as follows. If the high-priority queue contains qH>
0 requests, then a high-priority request is processed. Oth-
erwise, if the low-priority queue contains requests (i.e., if
qL>0), a low-priority request is handled. After handling
the last request (i.e., when both queues become empty), the
service provider automatically transitions to the idlestate.
The transitions from idletobusy are also automatic, and oc-
cur whenever the empty-queue DPM system receives a new
request. In contrast, all the other transitions are controlled
by a software power manager that aims to reduce power use
while maintaining an acceptable service level for the system.
We use the real values from [53] for the state transition times
(Table 2) and the request service rate (i.e., 125s−1), and we
assume average arrival rates of 0 .05s−1and 0 .15s−1for the
high-priority and low-priority requests, respectively.
Finally, suppose that the DPM system designer must select:
1. the capacity of the request queues, Qmax HandQmax L;
2. one of two alternative power managers (described later);
3. the parameters of the selected power manager
such that the QoS requirements in Table 3 are satisﬁed.
Table 3: QoS requirements for the DPM system
ID Description Type
R1The steady-state utilisation of the high-
priority queue should be less than 90%constraint
R2The steady-state utilisation of the low-
priority queue should be less than 90%constraint
R3The system should operate with minimum
steady-state power utilisationobjective
R4The number of requests lost at the steady
state should be minimisedobjective
R5The capacity of both queues should minimised objective3. EvoChecker
3.1 Modelling Language
EvoChecker probabilistic model templates are speciﬁed in
an extended version of the PRISM high-level modelling lan-
guage [45]. This language is based on the Reactive Modules
formalism [3], which describes a system as the parallel com-
position of a set of modules . The state of a module is encoded
by a set of ﬁnite-range local variables, and its state transi-
tions are deﬁned by probabilistic guarded commands that
change these variables, and have the general form:
[action ]guard →>e 1:update 1+...+en:update n;( 1 )
In this command, guard is a boolean expression over all
the variables in the model. If guard evaluates to true, the
arithmetic expression ei,1≤i≤n, gives the probabil-
ity (for discrete-time models) or rate (for continuous-time
models) with which the update ichange of the module vari-
ables occurs. The action is optional; when present, it forces
all modules comprising commands with this action to per-
form one of these commands simultaneously (i.e., to syn-
chronise). For a detailed description of the PRISM mod-
elling language, we refer the reader to the PRISM manual,
available at http://www.prismmodelchecker.org/manual .
Example 1. Fig. 2 shows the continuous-time Markov
chain (CTMC) model of the dynamic power management
system from our running example. The model comprises
a module for each request queue, a ServiceProvider module,
and a module for one of the two power managers consid-
ered during the design of the system. The local variables
from the high-priority and low-priority request queue mod-
ules record the number of requests from the two queues, qH
andqL, and the ServiceProvider local variable spencodes the
state of the service provider. The queue modules and Ser-
viceProvider synchronise through the actions requestH,L and
serveH,L , which reﬂect requests arriving into the queues and
being served, respectively. The service provider switches be-
tween its busy andidlestates automatically (lines 20, 22, 24
and 26), and can also perform the state transitions from
Table 2, aiming to switch to lower-power states when both
request queues are empty (i.e., qH+qL= 0) and back to-
wards the busy state otherwise (lines 28–34). However, these
transitions are controlled by the power manager through
the synchronisation of all ServiceProvider andPowerManager
commands with common actions. For example, lines 43–44
have false guards and thus disable the transitions between
idle andsleep, the transition between idle andstandby is
always enabled due to the true guard in line 46, and con-
straints are placed on all other transitions.
Software designers use probabilistic models to manually ex-
plore an often very large design space comprising alternative
module implementations, parameter values and transition
probabilities. EvoChecker extends the PRISM modelling
language with three constructs that support the speciﬁca-
tion of all these design alternatives within a probabilistic
model template whose instantiations correspond to possible
system designs. The three constructs are deﬁned below.
1.Evolvable parameters. EvoChecker uses the syntax
evolve int param [min..max ];
evolve double param [min..max ];(2)
to declare model parameters of type ‘int’ and ‘double’, re-
spectively, and acceptable ranges for them. These param-1
2
evolve module PowerManager
31
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57evolve int QmaxH [3..15];
evolve int QmaxL [5..30];
evolve distribution x[0.1..0.3][0.7..0.9];
evolve distribution y[0.3..0.6][0.4..0.7];ctmc
const int QmaxH = 4;
module HighPriorityRequestQueue
qH: [0..QmaxH] init0;
[requestH ]qH<QmaxH →>0.05:(qH’=qH+1);
[dropH ]qH=QmaxH →>0.05:(qH’=qH);
[serveH ]qH>0→>(qH’=qH-1);
endmodule
const int QmaxL = 12;
module LowPriorityRequestQueue
qL: [0..QmaxL] init0;
[requestL ]qL<QmaxL →>0.05:(qL’=qL+1);
[dropL ]qL=QmaxL →>0.05:(qL’=qL);
[serveL ]qL>0→>(qL’=qL-1);
endmodule
module ServiceProvider
sp:[0..3]init0; // 0=busy, 1=idle, 2=standby, 3=sleep
// Process requests with rate 125s−1
[serveH] sp=0 & qH >0&q H + q L >1→>125:(sp’=0);
[serveH] sp=0 & qH=1 & qL=0 →>125:(sp’=1);
[serveL] sp=0 & qH=0 & qL >1→>125:(sp’=0);
[serveL] sp=0 & qH=0 & qL=1 →>125:(sp’=1);
// Automatic transition from idle to busy
[requestH] sp=1 →>(sp’=0);
[requestH] sp!=1 →>(sp’=sp);
[requestL] sp=1 →>(sp’=0);
[requestL] sp!=1 →>(sp’=sp);
// Transitions controlled by the power manager
[idle2standby] sp=1 & qH+qL=0 →>1/0.4:(sp’=2);
[idle2sleep] sp=1 & qH+qL=0 →>1/0.67:(sp’=3);
[standby2idle] sp=2 & qH+qL >0→>1/1.2:(sp’=0);
[sleep2idle] sp=3 & qH+qL >0→>1/1.6:(sp’=0);
[sleep2standby] sp=3 →>1/0.6:(sp’=2);
[standby2sleep] sp=2 →>1/0.3:(sp’=3);
endmodule
const double x1 = 0.2;
const double x2 = 0.8;
const double y1 = 0.35;
const double y2 = 0.65;
module PowerManager
p:[0..1]init0; // 0=loop, 1=sleep to standby
// Disable idle ↔sleep transitions
[idle2sleep] false →>(p’=p);
[sleep2idle] false →>(p’=p);
// Deactivate
[idle2standby] true →>(p’=p); // always enabled
[standby2sleep] qH=0 & qL <2→>(p’=p);
// Activate
[standby2idle] qH >0|qL>1→>(p’=p);
[sleep2standby] qH >1|(qH=1 & qL >0)→>(p’=p);
[sleep2standby] p=1 →>(p’=0);
// Probabilistic control of sleep-to-standby transition
[requestH] qL+qH=0 & sp=3 →>x1 : (p’=1) + x2 : (p’=0);
[requestH] !(qL+qH=0 & sp=3) →>(p’=p);
[requestL] qH=0 & qL=3 & sp=3 →>y1 : (p’=1) + y2 : (p’=0);
[requestL] !(qH=0 & qL=3 & sp=3) →>(p’=p);
endmoduleFigure 2: CTMC model of the DPM system; x–zrepresent
EvoChecker extensions of the PRISM modelling language
eters can be used in any ﬁeld of command (1) other than
action , just like constant model parameters declared using
‘const int ’ and ‘ const double ’ from the original language.2.Evolvable probability distributions. The syntax
evolve distribution dist [min 1..max 1]...[min n..max n]; (3)
where [ min i,max i]↔[0,1] for all 1 ≤i≤n, is used to
declare an n-element discrete probability distribution, and
ranges for the nprobabilities of the distribution. The name
of the distribution can then be used instead of expressions
e1,e2,. . . , enfrom an n-element command (1).
3.Evolvable modules. EvoChecker uses the syntax
evolve module mod implementation1endmodule
...
evolve module mod implementationnendmodule(4)
to deﬁne n≥2 alternative implementations of a module.
The role of the three EvoChecker constructs is described by
the following deﬁnitions.
Definition 1. A valid PRISM probabilistic model aug-
mented with a set of EvoChecker constructs (2)–(4) is called
aprobabilistic model template .
Definition 2. A probabilistic model is an instance of a
probabilistic model template Tif and only if it can obtained
from Tusing the following transformations:
•Each evolvable parameter (2) is replaced by a ‘ const int
param = val; ’o r‘ const double param = val; ’ declaration
(depending on the type of the parameter), where val∈
{min, . . . , max }orval∈[min..max ], respectively.
•Each evolvable probability distribution (3) is removed,
and the noccurrences of its name instead of expressions
e1,. . . , enof a command (1) are replaced with values
from the ranges [ min 1..max 1], . . . , [ min n..max n], respec-
tively, such that the sum of the nvalues is 1 .0.
•Each set of evolvable modules with the same name is
replaced with a single element from the set, from which
the keyword ‘ evolve ’ was removed.
Definition 3. The set of all probabilistic models that
are instances of a probabilistic model template Tis called
thedesign space ofT, and is denoted DST.
Example 2. Fig. 2 illustrates the three EvoChecker con-
structs used to transform the CTMC model from our run-
ning example into a probabilistic model template. The re-
placement of the elements from the dashed rectangles with
those from the continuous rectangles shows how: xtwo
evolvable parameters are used to specify the sizes of the
two request queues; ytwo evolvable distributions are used
to specify the transition probabilities from lines 53 and 55 of
the power manager; and zthe module PowerManager is de-
clared one of several possible implementations of the power
manager. Note that at least one additional implementation
of this module needs to be provided in a valid probabilistic
model template; due to space constraints, the second imple-
mentation is not included here, but we make it available at
http://www-users.cs.york.ac.uk/~simos/EvoChecker .
3.2 Quality-of-Service Requirements
EvoChecker supports two types of QoS requirements: con-
straints andoptimisation objectives . Both types refer to QoSattributes of the system under design, such as response time,
throughput, reliability or cost. Constraints deﬁne bounds
for the acceptable values of these attributes, while optimi-
sation objectives specify QoS attributes that should be min-
imised or maximised (subject to all constraints being satis-
ﬁed). Without loss of generality, we will assume that the
latter QoS attributes should be minimised in the remain-
der of the paper. Formally, a software system considered by
EvoChecker needs to satisfy n1≥0 constraints of the form
RC
i:attr i⊿⊳triangleleftibound i,1≤i≤n1, (5)
andn2≥1 optimisation objectives of the form
RO
i:minimise attr i,n1+1≤i≤n1+n2, (6)
where ⊿⊳trianglelefti∈{<,≤,=,≥,>}andbound i∈Rfor all 1 ≤i≤n1,
andattr 1,attr 2,..., attr n1+n2∈Rrepresent n1+n2QoS
attributes of the system. These QoS attributes are formally
expressed in the probabilistic temporal logics shown earlier
in Table 1. Given a probabilistic model Mof a possible
system design, and the probabilistic temporal logic formulae
Φ1,Φ2,. . . ,Φ n1+n2of the QoS attributes, the probabilistic
model checker PRISM [45, 46] automatically establishes the
value of the QoS attributes corresponding to this design, i.e.,
attr i(M)=PMC (M,Φi),1≤i≤n1+n2. (7)
Due to space constraints, we do not present the probabilis-
tic temporal logics supported by EvoChecker; references to
detailed descriptions of each logic are provided in Table 1.
Example 3. The QoS requirements from our running ex-
ample (Table 3) comprise two constraints ( R1andR2), and
three optimisation objectives ( R3–R5 ). The QoS attributes
for all requirements are speciﬁed using rewards probabilistic
temporal logic formulae [4, 40, 44]. To this end, positive val-
ues are associated with speciﬁc states and transitions of the
CTMC in Fig. 2 by using rewards. . . endrewards structures.
For example, the structure below enables the computation
of the total number of lost requests for requirement R4:
rewards “TotalLost”
[dropH ]true : 1;
[dropL ]true : 1;
endrewards
by associating a value of 1 with each transition that corre-
sponds to a request being dropped because of a full queue.
The corresponding temporal logic formula is Φ 4=RTotalLost
=? [S],
and represents the “ Steady-state reward” for the above re-
wards structure, i.e., the long-run average number of dropped
requests for the system.
3.3 Probabilistic Model Synthesis Problem
Consider a probabilistic model template Twith design space
DST, and a set of QoS requirements comprising n1con-
straints (5) and n2optimisation objectives (6). The proba-
bilistic model synthesis problem involves ﬁnding the Pareto-
optimal design set PS of models from DSTthat satisfy the
n1constraints and are non-dominated with respect to the
n2optimisation objectives:
PS=/braceleftbig
M∈DST|(∀1≤i≤n1•attr i(M)⊿⊳triangleleftibound i)∧/parenleftbig
∄M/prime∈DST•M/prime≺M/parenrightbig/bracerightbig
,
(8)Table 4: EvoChecker gene encoding rulesevolve int param [min..max ];
evolve double param [min..max ];
evolve distribution dist[min 1..max 1]...
...[min n..max n];
evolve module mod implementation1endmodule
...
evolve module mod implementationmendmoduleEvoChecker gene(s)
Type Cardinality Value range, Vi
int
double
double
int1
1
n
1Evolvable feature
of the probabilistic model template
{min,...,max }
[min..max ]
[min 1..max 1]
...
[min n..max n]
{1,2,. . . ,m }with the dominance relation ≺:DST×DST→Bdeﬁned by
∀M 1,M2∈DST•M 1≺M 2≡
∀n1+1≤i≤n1+n2•attr i(M1)≤attr i(M2)∧
∃n1+1≤i≤n1+n2•attr i(M1)<attr i(M2).
Finally, given the Pareto-optimal design set PS, we are also
interested in the Pareto front deﬁned by
PF={(an1+1,an1+2,...,a n1+n2)∈Rn2|
∃M ∈ PS•∀n1+1≤i≤n1+n2•ai=attr i(M)},(9)
since designers need this information alongside their domain
knowledge when choosing between the models in PS.
3.4 EvoChecker Model Synthesis
Computing the Pareto-optimal design set (8) is in most cases
unfeasible, as the design space DSTis typically extremely
large and may be inﬁnite (i.e., when the probabilistic model
template Tcontains evolvable parameters of type double
and/or evolvable distributions). Therefore, EvoChecker uses
elitist genetic algorithms for multiobjective optimisati on(e.g.,
[26, 49, 64]) to synthesise a set of probabilistic models that
closely approximates this Pareto-optimal set.
Agenetic algorithm (GA) [43] encodes possible solutions of
a search problem as a sequence of genes (i.e., binary repre-
sentations of the problem variables). For EvoChecker, each
instance of an ‘ evolvable ’ construct from Section 3.1 con-
tributes to this sequence with the gene(s) given by the en-
coding rules in Table 4. These rules are used by the Template
parser from the high-level EvoChecker architecture in Fig. 3
to extract the value ranges V1,V2,...,V nfor the n≥1 genes
associated with the model template T.
Given the solution encoding described above, GAs start with
a randomly generated set ( population ) of feasible solutions
(individuals ). This population is then iteratively evolved
into populations containing “ﬁtter” individuals by means of
GAselection ,crossover andmutation . Selection uses a real-
valued ﬁtness function to evaluate each individual created
during the GA execution, in order to select the population
for the next iteration and the mating pool of individuals
for the current iteration. Crossover randomly selects two
individuals from the mating pool, and generates a new indi-
vidual by combining their genes. Finally, mutation produces
a new individual by randomly modifying some of the genes
of an individual from the pool. GAs terminate after a ﬁxed
number of (individual) evaluations or when a predetermined
number of successive iterations generate populations with no
signiﬁcantly ﬁtter individuals.Template
parserGene value
ranges
V1,...,V nElitist
multiobjective
optimisation GA
Internal
repres. T/primeIndividual QoS
analyser
Probabilistic
model checkerIndividual
(g1,...,g n)QoS
attributes
attr 1,. . .
Model M,
formula ΦiQoS
attribute
attr iProbabilistic
model
template TPareto front
approximation
PF,
Pareto-optimal
model set
approximation PS
EvoCheckerQoS
constraints (5)
& optimisation
objectives (6)Figure 3: High-level EvoChecker architecture
As shown in Fig. 3, the GA used by EvoChecker is elitist ,
i.e., it preserves the best individuals across iterations. This
involves maintaining a ﬁnite “archive” of the ﬁttest indivi du-
als (e.g., by elitist GAs like MOCell [49] and SPEA2 [64]) or
by retaining the ﬁttest individuals from one iteration to the
next (e.g., by NSGA-II [26]). The algorithms used by Evo-
Checker are also multiobjective optimisation GAs (MOGAs),
i.e., they generate Pareto-optimal set approximations spread
as uniformly as possible across the search space. Therefore,
they use ﬁtness functions that encode in ways speciﬁc to each
MOGA both the nondomination level of each evaluated in-
dividual and the population density in its area of the search
space. For example, NSGA-II [26] associates a nondomi-
nance level of 1 to all nondominated individuals of a popu-
lation, a level of 2 to the individuals that are not dominated
when level-1 individuals are ignored, etc.1; and SPEA2 [64]
evaluates population density as the inverse of the distance t o
thek-th nearest neighbour of the individual. Providing full
technical details about the MOGAs that EvoChecker can
use is beyond the scope of this paper. The reader can ﬁnd
their descriptions in [26, 49, 64], and a comparison of their
relative merits in [28].
In EvoChecker the MOGA evaluation of the ﬁtness of indi-
viduals is supported by an Individual QoS analyser (Fig. 3).
This component takes as input the gene sequence ( g1,...,g n)
for an individual, and returns to the MOGA the QoS at-
tributes attr 1,attr 2,..., attr n1+n2of the model Massoci-
ated with this individual. To this end, the analyser uses an
internal representation T/primeof the model template produced
by the Template parser and the gene sequence ( g1,...,g n)t o
obtain the model M, and then invokes a Probabilistic model
checker to establish the n1+n2QoS attributes from (7) one
at a time. This setup enables the MOGA to generate ap-
proximations of the Pareto-optimal model set PSand of the
associated Pareto front PFas illustrated in Fig. 3.
Example 4. Consider again the DPM system from our
running example. Using the rules in Table 4, EvoChecker en-
codes each instance Mof the probabilistic model template
from Fig. 2 as the sequence of genes
(QmaxH,QmaxL,x1,x2,y1,y2,pm), (10)
1Individuals not satisfying problem constraints are assigned
a default level of ∞.1.41.61.82.0Power use [W]
0.060.080.10.12
Lost requests246810
Queue length
(qH+qL)Figure 4: Pareto front approximation for the DPM system
where QmaxH∈V1={3,4,..., 15}andQmaxL∈V2=
{5,6,..., 30}represent the capacities of the two queues;
x1∈V3=[ 0 .1,0.3] and x2∈V4=[ 0 .7,0.9] such that
x1+x2= 1, and y1∈V5=[ 0.3,0.6] and y2∈V6=[ 0.4,0.7]
such that y1+y2= 1 represent the two probability dis-
tributions; and pm∈V7={1,2}is a gene that encodes
which of the m= 2 implementations of the PowerManager
module in Fig. 2 is used by M. The gene value ranges V1
toV7are extracted by the EvoChecker Template parser and
passed to the MOGA, which uses the Individual QoS anal-
yser to iteratively generate model populations of increasing
ﬁtness and eventually outputs the Pareto front and Pareto-
optimal set approximations achieved after a pre-established
number of evaluations. As an example, Fig. 4 depicts a
Pareto front approximation obtained using NSGA-II [26] as
the EvoChecker MOGA, with an initial population of 150
individuals and after 5000 evaluations. Remember that the
dimensionality of Pareto front for a probabilistic model syn-
thesis problem is given by the number of optimisation ob-
jectives n2. Our DPM system has n2= 3 optimisation ob-
jectives, as shown in Table 3.
4. EvoChecker TOOL
To automate the EvoChecker synthesis of probabilistic mod-
els, we implemented a tool with the architecture in Fig. 3.
Our EvoChecker tool uses the established MOGA imple-
mentations provided by the Java-based framework for multi-
objective optimization with metaheuristics JMetal [27], and
the probabilistic model checker PRISM [45]. We developed
the other components of the architecture from Fig. 3 in
Java, using the Antlr2parser generator to build the Template
parser , and implementing an Individual QoS analyser specif-
ically for the EvoChecker tool. The open-source code of
EvoChecker, the full experimental results summarised in the
following section, additional information about EvoChecker
and the case studies used for its evaluation are available at
http://www-users.cs.york.ac.uk/~simos/EvoChecker .
5. EVALUATION
5.1 Research Questions
We evaluated the eﬀectiveness of EvoChecker by performing
extensive experiments to answer the next research questions.
RQ1 (Validation): How does EvoChecker perform
compared to random search? We used this research
2http://www.antlr.orgFigure 5: The FX workﬂow
question to establish if EvoChecker“ comfortably outperforms
ar a n d o ms e a r c h ” [38], as expected of eﬀective search-based
software engineering solutions.
RQ2 (Comparison): How do EvoChecker instances
using diﬀerent MOGAs perform compared to each
other? Since we devised EvoChecker to work with any
MOGA, we examined the results produced by EvoChecker in-
stances using three established such algorithms (i.e., NSGA-
II [26], SPEA2 [64] and MOCell [49]).
RQ3 (Insights): Can EvoChecker provide insights
into the trade-oﬀs between the QoS attributes of al-
ternative software architectures and conﬁgurations?
To support software designers in their decision making, Evo-
Checker must provide insights into the trade-oﬀs between
multiple QoS objectives. To address this question, we identi-
ﬁed a range of design decisions suggested by the EvoChecker
results for the software systems considered in our evaluation.
5.2 Analysed Software Systems
Our experiments used EvoChecker in multiple scenarios as-
sociated with two software systems from diﬀerent applica-
tion domains—the dynamic power management (DPM) sys-
tem from our running example, and a real-world service-
based system from the area of foreign exchange trading. The
second system, which we anonymise as FX for conﬁdentiality
reasons, is used by an European foreign exchange brokerage
company, and implements the workﬂow in Fig. 5.
An FX customer (called a trader) can use the system in two
operation modes. In the expert mode, FX executes a loop
that analyses market activity, identiﬁes patterns that satis fy
the trader’s objectives, and automatically carries out trades.
Thus, the Market watch service extracts real-time exchange
rates (bid/ask price) of selected currency pairs. This data is
used by a Technical analysis service that evaluates the cur-
rent trading conditions, predicts future price movement, and
decides if the trader’s objectives are: (i) “satisﬁed” (causing
the invocation of an Order service to carry out a trade);
(ii) “unsatisﬁed” (resulting in a new Market watch invoca-
tion); or (iii) “unsatisﬁed with high variance” (triggering an
Alarm service invocation to notify the trader about discrep-
ancies/opportunities not covered by the trading objectives).
In the normal mode, FX assesses the economic outlook ofa country using a Fundamental analysis service that col-
lects, analyses and evaluates information such as news re-
ports, economic data and political events, and provides an
assessment on the country’s currency. If satisﬁed with this
assessment, the trader can use the Order service to sell or
buy currency, in which case a Notiﬁcation service conﬁrms
the completion of the trade.
We assume that the FX designer has to select third-party
implementations for each of the n≤1 services from Fig. 5 for
which in-house implementations are not available, in order
to meet the QoS requirements from Table 5. The designer
can use any subset of the ni≥1 third-party implementa-
tions of the i-th service unavailable in-house, and either a
probabilistic or a sequential selection strategy. The prob-
abilistic strategy involves using a randomly selected third-
party service from the subset for each invocation of service
i, where the random selection is made according to a dis-
crete probability distribution decided by the designer. The
sequential strategy involves invoking the services from the
subset in a designer-speciﬁed order, until one of the in-
vocations is successful or all the invocations failed. We
used an EvoChecker probabilistic model template to cap-
ture these alternative FX designs. This template uses all
EvoChecker “evolvable” constructs, and is parameterised by
the number of services n, the number of third-party service
implementations n1,n2, . . . , and by the costs, success prob-
abilities and response times of these implementations.
To evaluate EvoChecker for multiple design space sizes, we
applied it to each of the system variants from Table 6. The
entries in this table list the int-valued probabilistic model
template parameters from (10)—for the DPM system, and
described above—for the FX system. The ‘Size’ column re-
ports the size of the design space that an exhaustive search
would need to explore, assuming two-decimal precision for
thedouble -valued parameters of the probabilistic model tem-
plates (cf. Table 4). This is a valid assumption, as the
nonlinearity of most probabilistic models means that a 0 .01
change in a state transition probability often translates into
signiﬁcant changes in the values of model properties. Finally ,
note that the n= 8 services used by FX large correspond to
using two-part composite services for the Technical analysis
andFundamental analysis operations from Fig. 5.
5.3 Evaluation Methodology
In line with the standard practice for evaluating the per-
formance of stochastic optimisation algorithms [6], we per-
formed multiple (i.e., 30) independent runs for each system
variant from Table 6 and each multiobjective optimisation
algorithm—NSGA-II, SPEA2, MOCell and random search.
Each run comprised 10,000 evaluations, and the MOGAs
Table 5: QoS requirements for the FX system
ID Description Type
R1Workﬂow executions must complete success-
fully with probability at least 0.9constraint
R2The total service response time per workﬂow
execution should be minimisedobjective
R3The probability of a service failure during a
workﬂow execution should be minimisedobjective
R4The total cost of the third-party services used
by a workﬂow execution should be minimisedobjectiveTable 6: Analysed system variants; T runrepresents the
EvoChecker running time averaged over 30 runs
Variant Int-valued parameters Size T run
DPM Small QmaxH,L∈{1,. . . ,10},m=2 2E+14 1050s
DPM Medium QmaxH,L∈{1,. . . ,15},m=2 4.5E+14 2118s
DPM Large QmaxH,L∈{1,. . . ,20},m=2 8E+14 3796s
FXSmall n=4 , n1=···=n4=3 4 . 9 8 E + 3 1 8 5 8 s
FXMedium n=6 , n1=···=n6=4 1 . 3 9 E + 6 5 1 6 9 5 s
FXLarge n=8 , n1=···=n6=4 7 . 2 2 E + 8 6 4 1 6 2 s
used an initial population of 100 individuals, single-poin t
crossover with probability pc=0.9, and single-point muta-
tion with probability pm=1/np, where npis the number
of system variant parameters. All the experiments were run
on a CentOS Linux 6.5 64bit server with two 2.6GHz Intel
Xeon E5-2670 processors and 32GB of memory. The average
run times for the six system variants are shown in Table 6.
Obtaining the actual Pareto front for our system variants is
unfeasible because of their very large design spaces. There-
fore, we adopted the established practice [63] of compar-
ing the Pareto front approximations produced by each algo-
rithm with the reference Pareto front comprising the non-
dominated solutions from all the runs carried out for the
analysed system variant. For this comparison, we employed
the widely-used Pareto-front quality indicators below, and
we will present their means and box plots as measures of
central tendency and distribution, respectively:
1.Unary additive epsilon (I/epsilon1) [66], i.e., the minimum addi-
tive term by which the elements of the objective vectors
from a Pareto front approximation must be adjusted in
order to dominate the objective vectors from the refer-
ence front. This indicator presents convergence to the
reference front and is Pareto compliant3. Smaller I/epsilon1val-
ues denote better Pareto front approximations.
2.Hypervolume (IHV) [65], which measures the volume in
the objective space covered by a Pareto front approxima-
tion with respect to the reference front (or a reference
point). IHVmeasures both convergence and diversity,
and is strictly Pareto compliant [62]. Larger IHVvalues
denote better Pareto front approximations.
3.Inverted Generational Distance (IIGD) [58], which pro-
vides an “error measure” as the Euclidean distance in
the objective space between the reference front and the
Pareto front approximation. IIGDshows both diversity
and convergence to the reference front. Smaller IIGD
values signify better Pareto front approximations.
We used inferential statistical tests to compare these quality
indicators across the four algorithms [6, 38, 63]. As is typ-
ical of multiobjective optimisation [63], the Shapiro-Wilk
test showed that the quality indicators were not normally
distributed, so we used the Kruskal-Wallis non-parametric
test with 95% conﬁdence level ( α=0.05) to analyse the re-
sults without making assumptions about the distribution of
the data or the homogeneity of its variance. We also carried
out a post-hoc analysis with pairwise comparisons between
the four algorithms by means of Dunn’s pairwise test, con-
trolling the family-wise error rate using the Bonferroni cor-
rection pcrit=α/k, where kis the number of comparisons.
3Pareto compliant indicators do not “contradict” the order
introduced by the Pareto dominance relation on Pareto front
approximations [62].Table 7: Mean quality indicator values for the system variants fro m Table 6
Problem NSGA-II SPEA2 MOCell Random
I/epsilon1(Epsilon)
DPM Small 0.0209 0.0130 0.0242 0.1403 +
DPM Medium 0.0225 0.0123 0.0489 0.1996 +
DPM Large 0.0229 0.0147 0.0884 0.2497 +
IHV(Hypervolume)
DPM Small 0.4455 0.4458 0.4396 0.4022 +
DPM Medium 0.4487 0.4499 0.4386 0.3946 +
DPM Large 0.4528 0.4549 0.4395 0.3947 +
IIGD(Inverted Generational Distance)
DPM Small 0.00023 0.00018 0.00016 0.00062 +
DPM Medium 0.00024 0.00019 0.00028 0.00091 +
DPM Large 0.00024 0.00020 0.00038 0.00109 +Problem NSGA-II SPEA2 MOCell Random
I/epsilon1(Epsilon)
FXSmall 0.6258 0.5083 0.6745 2.2274 +
FXMedium 1.6379 2.0105 2.0486 6.1529 +
FXLarge 3.8528 5.2777 4.6366 13.0234 +
IHV(Hypervolume)
FXSmall 0.611 0.628 0.608 0.593 +
FXMedium 0.719 0.725 0.702 0.606 +
FXLarge 0.657 0.675 0.633 0.555 +
IIGD(Inverted Generational Distance)
FXSmall 0.00123 0.00129 0.00125 0.00145 +
FXMedium 0.00192 0.00207 0.00200 0.00316 +
FXLarge 0.00244 0.00255 0.00272 0.00395 +
0.000.100.200.300.400.50
0.400.50
NSGA-IISPEA2 MOCell Random0.0000.0010.002
NSGA-IISPEA2 MOCell Random
NSGA-IISPEA2 MOCell RandomDPM_Small DPM_Medium DPM_Large
Iε Iε Iε
IHV IHV IHV
IIGD IIGD IIGD
0.003.006.009.0012.0015.0018.00
0.500.600.700.80
NSGA-IISPEA2 MOCell Random0.0010.0020.0030.004
NSGA-IISPEA2 MOCell Random
NSGA-IISPEA2 MOCell RandomFX_Small FX_Medium FX_Large
Iε Iε Iε
IHV IHV IHV
IIGD IIGD IIGD
Figure 6: Boxplots for the six system variants from Table 6, evaluat ed using the quality indicators I/epsilon1,IHVandIIGD
5.4 Results and Discussion
RQ1(Validation). To answer the ﬁrst research question,
we carried out the experiments described in the previous
section. The results are reported in Table 7 and Fig. 6. The
‘+’ from the last column of the table entries indicate that
the Kruskal-Wallis test showed signiﬁcant diﬀerence among
the four algorithms (p-value <0.05) for all six system variants
and all Pareto-front quality indicators.
For both systems, EvoChecker with any MOGA achieved
considerably better results than random search, for all qual-
ity indicators and system variants. The post hoc analysis of
pairwise comparisons between random search and the MO-
GAs provided statistical evidence about the superiority of
the MOGAs for all system variants and for all quality in-
dicators. The best and, when obtained, the second best
outcomes of this analysis per system variant and quality in-
dicator are shaded and lightly shaded in the result tables,
respectively. This superiority of the results obtained using
EvoChecker with any of the MOGAs over those produced by
random search can also be seen from the boxplots in Fig. 6.
We qualitatively support our ﬁndings by presenting in Fig. 7
the Pareto front approximations achieved by EvoChecker witheach of the MOGAs and by random search, for a typical run
of the experiment for the FX system variants. We observe
that irrespective of the MOGA, EvoChecker achieves Pareto
front approximations with more, better spread and higher
quality nondominated solutions than random search.
As explained in Section 2, the parameters we used for the
DPM system variants (power usage, transition rates, etc.)
correspond to the real-world system. In contrast, for the
FX system variants we chose realistic values for the relia-
bility, performance and cost of the third-party services. To
mitigate the risk of accidentally choosing values that biase d
the EvoChecker evaluation, we performed additional experi-
ments comprising 300 independent runs per FX system vari-
ant (900 runs in total) in which these parameters were ran-
domly instantiated. The results of this further analysis (re-
ported on our project webpage but not included here due to
space constraints) conﬁrm the ﬁndings presented above.
All these results provide strong empirical evidence that the
EvoChecker signiﬁcantly outperforms random search, for a
range of system variants from two diﬀerent domains, and
across multiple widely-used MOGAs. This also conﬁrms the
challenging and well-formulated nature of the probabilistic
model synthesis problem we introduced in Section 3.3.0.96
0.921.00
30 45 60 75 9016182022
ReliabilityCostTime [s]
NSGA-II
SPEA2
MOCell
Random(a) FX Small
Reliability0.96
0.921.00
20 40 60 80 10016202428
CostTime [s]
NSGA-II
SPEA2
MOCell
Random(b) FX Medium
0.96
0.921.00
60 90 120 150303540
2545
Reliability CostTime [s]
NSGA-II
SPEA2
MOCell
Random(c) FX Large
Figure 7: Typical Pareto front approximations for the FX system varia nts and optimisation objectives R2-R4from Table 5
RQ2 (Comparison). To compare EvoChecker instances
based on diﬀerent MOGAs, we ﬁrst observe in Table 7 that
NSGA-II and SPEA2 outperformed MOCell for all system
variant–quality indicator combinations except DPM Small–
IIGD. Between SPEA2 and NSGA-II, the former achieved
slightly better results for the smaller design spaces of the
DPM system variants (across all indicators) and for the
IHVindicator (across all system variants), whereas NSGA-
II yielded Pareto-front approximations with better I/epsilon1and
IIGDindicators for the larger design spaces of the FX sys-
tem variants (except the combination FX small– I/epsilon1).
Additionally, we carried out the post-hoc analysis described
in Section 5.3, for 9 system variants (counting separately
the FX system variants with chosen parameters and with
randomly instantiated parameters) ×3 quality indicators =
27 tests. Out of these tests, 22 tests (i.e., a percentage of
81.4%) showed high statistical signiﬁcance in the diﬀerenc es
between the performance achieved by EvoChecker with dif-
ferent MOGAs (Table 8). The ﬁve system variant–quality
indicator combinations for which the tests were unsuccessful
are: FX Medium– I/epsilon1, FX(random) Small– I/epsilon1, FX(random)
Medium– I/epsilon1,F X Small– IIGDand FX Medium– IIGD.
These results show that, like for any well-formulated opti-
misation problem, diﬀerent algorithms are more suitable in
dealing with speciﬁc problems. They also conﬁrm the gen-
erality of EvoChecker, showing that its functionality can be
realised using multiple established MOGAs.
RQ3 (Insights). We performed qualitative analyses of the
Pareto front approximations produced by EvoChecker, with
Table 8: System variants for which the MOGAs in rows are
signiﬁcantly better than the MOGAs in columns
NSGA-II SPEA2 MOCellNSGA-III/epsilon1: FXL,RL DPM M,L
IHV: –D P M S,M,L;
FXM,L,RM,RL
IIGD: –F X L,RLSPEA2I/epsilon1:D P M S,M,L DPM S,M,L; FX S
IHV:F X S,M,L, DPM S,M,L;
RS,RM,RL FXS,M,L,RS,RM,RL
IIGD:D P M S,M,L;
FXRSDPM L;
FXL,RS,RM,RLMOCellI/epsilon1:– –
IHV:– –
IIGD:D P M S,M DPM S
Key: S=Small, M=Medium, L=Large, R=random parametersthe purpose of identifying actionable insights. We illustrate
this for the FX and DPM Pareto front approximations from
Fig. 7 and Fig. 4, respectively. First, the EvoChecker re-
sults enable the identiﬁcation of the“point of diminishing re-
turns” for each system variant. The results from Fig. 7 show
that design options with costs above approximately 52 for
FXSmall, 61 for FX Medium and 94 for FX Large provide
only marginal response time and reliability improvements
over the best designs achievable for these costs. Likewise,
the results in Fig. 4 show that DPM designs with power use
above 1.7W yield insigniﬁcant reductions in the number of
lost requests, whereas designs with even slightly lower power
use lead to much higher request loss. This key information
helps designers avoid unnecessarily expensive solutions.
Second, we note the high density of design solutions in the
areas with low reliability (below 0.95) for the FX system
in Fig. 7, and with high request loss (above 0.09) for the
DPM system in Fig. 4. For the FX system, for instance,
these correspond to the use of the probabilistic service selec-
tion strategy, for which numerous service combinations can
achieve similar reliability and response time with relatively
low, comparable costs. Opting for a design from this area
will make the FX system susceptible to failures, as when
the only service invoked for an FX operation fails, the en-
tire workﬂow execution will also fail. In contrast, reliability
values above 0.995 correspond to high-cost designs that use
the sequential service selection strategy, e.g., FX small must
use the sequential strategy for the Market watch andFunda-
mental analysis services in order to achieve 0.996 reliability.
Third, the EvoChecker results reveal design parameters that
QoS properties are particularly sensitive to. For the FX sys-
tem, for example, we noticed a strong dependency of the
workﬂow reliability on the service selection strategy and the
number of services used for each operation. Designs from
high-reliability areas of the Pareto front not only use the
sequential selection strategy, but also require multiple ser-
vices per FX operation (e.g., three FX service providers are
needed for success rates above 0.99).
Finally, we note EvoChecker’s ability to produce solutions
that: (a) cover a wide range of values for the QoS attributes
from the optimisation objectives of the FX and DPM sys-
tems; and (b) include alternatives with diﬀerent trade-oﬀs
for ﬁxed values of one of these attributes. Thus, for 0.99 reli-
ability, the experiment from Fig. 7 generated four alternative
FXLarge designs, each with a diﬀerent cost and execution
time. Similar observations can be made for a speciﬁc value
of either of the other two QoS attributes. These results sup-
port the system designers in their decision making.5.5 Threats to Validity
Construct validity threats may be due to the simpliﬁca-
tions and assumptions we made when modelling the DPM
and FX systems. To mitigate this threat, the DPM system,
model and requirements are based on a validated real-world
case study taken from the literature [53, 56], which we were
familiar with from our previous work [19]. For the FX sys-
tem, the model and requirements were developed in close
collaboration with a foreign exchange domain expert.
Internal validity threats can originate from the stochas-
tic nature of the optimisation algorithms employed in our
study. To mitigate these threats, we adopted the recom-
mended practice for empirical studies in this research area [6,
38]. In particular, we reported results over 30 repeated runs
of each experiment, and employed statistical tests to check
for signiﬁcance in the achieved results. Thus, we used the
Shapiro-Wilk test to assess the normality of data, and the
Kruskall-Wallis and Dunn pairwise non-parametric tests to
check for statistical signiﬁcance between the considered op-
timisation algorithms. Finally, we set the chance of commit-
ting a Type I error ( αconﬁdence limit) at 0.05, which is the
standard value recommended for these studies.
External validity threats might be due to the diﬃculty of
representing a software system and its QoS requirements as
a probabilistic model synthesis problem (Section 3.3) using
EvoChecker constructs (2)–(4), constraints (5) and optimi-
sation objectives (6). We limit this threat by specifying
EvoChecker probabilistic model templates in an extended
version of the high-level modelling language of PRISM [45],
a widely-used probabilistic model checker [50]. Moreover,
given the generality of the EvoChecker constructs (2)–(4),
other probabilistic modelling languages (e.g., those of the
model checkers MRMC [40, 41] and Ymer [60]) can be natu-
rally supported. Additionally, EvoChecker supports a wide
range of probabilistic models and temporal logics (Table 1).
Finally, to further reduce the risk that EvoChecker might be
diﬃcult to use in practice, we validated it through applica-
tion to several variants of two realistic software systems with
diverse characteristics in terms of application domain, size,
complexity and requirements. Nevertheless, we are aware
that our ﬁndings are by no means conclusive for all types
of software systems, and more experiments are required to
conﬁrm the generality of the EvoChecker approach and tool.
6. RELATED WORK
Search-based software engineering (SBSE) [38] has been suc-
cessfully used in areas ranging from project management [29,
54, 57] and testing [5, 32, 36] to eﬀort estimation [48], soft-
ware repair and evolution [20, 52] and software product
lines [35, 55]. However, as reported in Harman et al. ’s recent
SBSE survey [37], this success does not yet extend to model
checking. The only SBSE applications that we are aware of
in this area are the approaches in [39, 42], which employ ge-
netic evolution to synthesise model checking speciﬁcation s,
and the work in [1, 2], which uses ant colony optimisation
to ﬁnd counterexamples for large model checking problems.
To the best of our knowledge, EvoChecker is the ﬁrst SBSE
approach to synthesising Pareto-optimal sets of probabilis-
tic models for QoS software engineering. As mentioned in
Section 1, the only related work we are aware of belongs to
the area of model repair [16, 61]. Unlike EvoChecker, most
model repair research so far has focused on non-probabilisticmodels [15, 16, 21, 22, 47]. Also diﬀerent from EvoChecker,
these approaches support a single type of model, and target
its “repair” with respect to a single temporal logic property.
In the probabilistic model checking domain, model repair
involves automatically modifying the transition probabili-
ties of a model that violates a formally-speciﬁed property
in order to obtain a new model that satisﬁes the property
and is “close” to the original model [10, 11, 23]. As prob-
abilistic model repair modiﬁes the original model only at
transition probability level, and only to satisfy a single tem-
poral logic property, its applicability is limited. In con-
trast, EvoChecker handles multiple QoS requirements and
uses multiobjective optimisation to derive a set of proba-
bilistic models that approximates the Pareto-optimal model
set associated with these QoS requirements. To conﬁrm that
EvoChecker subsumes the capabilities of probabilistic mode l
repair, we used our EvoChecker tool to successfully replicate
the results of the IPv4 Zeroconf Protocol [10] and Network
Virus Infection [23] case studies. We did not include these
additional experiments in the paper due to space constraints,
but we make the results available on our project webpage.
Finally, [31] synthesises Pareto front approximations over
the policies of Markov decision processes (MDPs). However,
its applicability is limited to fully speciﬁed MDPs, to a sub-
set of probabilistic computation tree logic (i.e., reachabili ty
and expected total reward formulae), and to the ﬁnite search
spaces that can be encoded as MDP policies. In contrast,
EvoChecker fully supports all types of models and logics
from Table 1, and solves a more general problem by starting
from incompletely speciﬁed probabilistic model templates
with potentially inﬁnite search spaces (due to evolvable dou-
bleparameters and distributions). Moreover, the implemen-
tation in [31] currently supports only up to 3 optimisation
objectives, while EvoChecker does not have this limitation.
7. CONCLUSION
We deﬁned the probabilistic model synthesis problem, and
introduced EvoChecker, the ﬁrst tool-supported search-based
approach that tackles this problem with a focus on QoS
software engineering. EvoChecker uses multiobjective op-
timisation genetic algorithms to automate the synthesis of
approximate Pareto-optimal probabilistic model sets associ-
ated with the QoS requirements of a software system. We
evaluated the EvoChecker approach and tool within two case
studies from diﬀerent domains, showing its eﬀectiveness, po-
tential applicability and ﬂexibility. The limitations of o ur
evaluation are discussed in Section 5.5.
The future research directions for our project include ex-
tending the applicability of EvoChecker to other modelling
formalisms and veriﬁcation logics by exploiting established
quantitative model checkers such as UPAAL [12], exploring
alternative multiobjective optimisation [33], and integrati ng
the EvoChecker approach with our related work on runtime
probabilistic model checking [17, 18, 30]. The further explo-
ration of the EvoChecker performance and its applicability
to other domains represents another area of future work.
Acknowledgements
This paper presents research sponsored by UK MOD. The
information contained in it should not be interpreted as rep-
resenting the views of the UK MOD, nor should it be as-
sumed it reﬂects any current or future UK MOD policy.8. REFERENCES
[1] E. Alba and F. Chicano. Finding safety errors with
ACO. In 9th Intl. Conf. on Genetic and Evolutionary
Computation (GECCO’07) ,p a g e s1 0 6 6 – 1 0 7 3 ,2 0 0 7 .
[2] E. Alba and F. Chicano. Searching for liveness
property violations in concurrent systems with ACO.
InIn 10th Intl. Conf. on Genetic and Evolutionary
Computation (GECCO’08) ,p a g e s1 7 2 7 – 1 7 3 4 ,2 0 0 8 .
[3] R. Alur and T. Henzinger. Reactive modules. Formal
Methods in System Design ,1 5 ( 1 ) : 7 – 4 8 ,1 9 9 9 .
[4] S. Andova, H. Hermanns, and J. P. Katoen.
Discrete-time rewards model-checked. In FORMATS
2003, volume 2791 of LNCS , pages 88–104. 2004.
[5] J. Andrews, T. Menzies, and F. Li. Genetic algorithms
for randomized unit testing. IEEE Trans. Softw. Eng. ,
37(1):80–94, 2011.
[6] A. Arcuri and L. Briand. A practical guide for using
statistical tests to assess randomized algorithms in
software engineering. In 33rd Intl. Conf. Softw. Eng.
(ICSE’11) ,p a g e s1 – 1 0 ,2 0 1 1 .
[7] A. Aziz, K. Sanwal, V. Singhal, and R. Brayton.
Verifying continuous time Markov chains. In Computer
Aided Veriﬁcation (CAV’96) , pages 269–276, 1996.
[8] C. Baier and J. P. Katoen. Principles of Model
Checking . MIT Press, 2008.
[9] C. Baier, J. P. Katoen, and H. Hermanns.
Approximate symbolic model checking of
continuous-time Markov chains. In 10th Intl. Conf. on
Concurrency Theory (CONCUR’99) , volume 1664 of
LNCS , pages 146–161, 1999.
[10] E. Bartocci, R. Grosu, P. Katsaros, C. Ramakrishnan,
and S. A. Smolka. Model repair for probabilistic
systems. In Tools & Algorithms Constr. & Analysis of
Systems (TACAS’11) , pages 326–340. 2011.
[11] M. Benedikt, R. Lenhardt, and J. Worrell. LTL model
checking of interval Markov chains. In Tools &
Algorithms Constr. & Analysis of Systems
(TACAS’13) , pages 32–46. 2013.
[12] J. Bengtsson, K. Larsen, F. Larsson, P. Pettersson,
and W. Yi. UPPAAL - a tool suite for automatic
veriﬁcation of real-time systems . Springer, 1996.
[13] A. Bianco and L. de Alfaro. Model checking of
probabilistic and nondeterministic systems. In 15th
Intl. Conf. on Foundations of Software Technology and
Theoretical Computer Science (FSTTCS’95) , volume
1026 of LNCS ,p a g e s4 9 9 – 5 1 3 ,1 9 9 5 .
[14] B. Boehm and V. R. Basili. Software Defect Reduction
Top 10 List. Computer , 34(1):135–137, Jan. 2001.
[15] B. Bonakdarpour and S. S. Kulkarni. Automated
model repair for distributed programs. ACM SIGACT
News ,4 3 ( 2 ) : 8 5 – 1 0 7 ,2 0 1 2 .
[16] F. Buccafurri, T. Eiter, G. Gottlob, and N. Leone.
Enhancing model checking in veriﬁcation by AI
techniques. Artiﬁcial Intelligence ,1 1 2 : 5 7 – 1 0 4 ,1 9 9 9 .
[17] R. Calinescu, C. Ghezzi, M. Kwiatkowska, and
R. Mirandola. Self-adaptive software needs
quantitative veriﬁcation at runtime. Communications
of the ACM , 55(9):69–77, September 2012.
[18] R. Calinescu, L. Grunske, M. Kwiatkowska,
R. Mirandola, and G. Tamburrelli. Dynamic QoS
management and optimization in service-based
systems. IEEE Trans. Softw. Eng. ,3 7 : 3 8 7 – 4 0 9 ,2 0 1 1 .[19] R. Calinescu and M. Z. Kwiatkowska. Using
quantitative analysis to implement autonomic IT
systems. In 31st Intl. Conf. Softw. Eng. (ICSE’09) ,
pages 100–110, 2009.
[20] G. Canfora, M. Di Penta, R. Esposito, and M. L.
Villani. An approach for QoS-aware service
composition based on genetic algorithms. In 7th Intl.
Conf. on Genetic and Evolutionary Computation
(GECCO’05) , pages 1069–1075, 2005.
[21] M. Carrillo and D. A. Rosenblueth. CTL update of
Kripke models through protections. Artiﬁcial
Intelligence ,2 1 1 ( 0 ) : 5 1–7 4 ,2 0 1 4 .
[22] G. Chatzieleftheriou, B. Bonakdarpour, S. A. Smolka,
and P. Katsaros. Abstract model repair. In NASA
Formal Methods , pages 341–355. 2012.
[23] T. Chen, E. M. Hahn, T. Han, M. Kwiatkowska,
H. Qu, and L. Zhang. Model repair for Markov
decision processes. In 7th Intl. Symp. Theoretical
Aspects of Softw. Eng. (TASE’13) , pages 85–92, 2013.
[24] E. M. Clarke, O. Grumberg, and D. Peled. Model
checking . MIT press, 1999.
[25] L. O. Damm and L. Lundberg. Company-wide
implementation of metrics for early software fault
detection. In 29th Intl. Conf. Softw. Eng. (ICSE’07) ,
pages 560–570, 2007.
[26] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. A
fast and elitist multiobjective genetic algorithm: NSGA-II.
IEEE Trans. Evol. Comp. ,6 ( 2 ) : 1 8 2 – 1 9 7 ,2 0 0 2 .
[27] J. J. Durillo and A. J. Nebro. jMetal: A Java
framework for multi-objective optimization. Advances
in Engineering Software ,4 2 : 7 6 0 – 7 7 1 ,2 0 1 1 .
[28] J. J. Durillo, A. J. Nebro, C. A. Coello Coello,
J. Garc´ ıa-Nieto, F. Luna, and E. Alba. A study of
multiobjective metaheuristics when solving parameter
scalable problems. IEEE Trans. on Evolutionary
Computation ,1 4 ( 4 ) : 6 1 8 – 6 3 5 ,2 0 1 0 .
[29] F. Ferrucci, M. Harman, J. Ren, and F. Sarro. Not
going to take this anymore: Multi-objective overtime
planning for software engineering projects. In 35th
Intl. Conf. Softw. Eng. (ICSE’13) , pages 462–471,
2013.
[30] A. Filieri, C. Ghezzi, and G. Tamburrelli. A formal
approach to adaptive software: continuous assurance
of non-functional requirements. Formal Asp. Comput. ,
24(2):163–186, 2012.
[31] V. Forejt, M. Kwiatkowska, and D. Parker. Pareto
curves for probabilistic model checking. In 10th Intl.
Symp. Automated Technology for Verif. & Analysis ,
volume 7561 of LNCS , pages 317–332. Springer, 2012.
[32] G. Fraser and A. Arcuri. Whole test suite generation.
IEEE Trans. Softw. Eng. ,3 9 ( 2 ) : 2 7 6 – 2 9 1 ,2 0 1 3 .
[33] X. Gandibleux. Metaheuristics for multiobjective
optimisation , volume 535. Springer Science & Business
Media, 2004.
[34] H. Hansson and B. Jonsson. A logic for reasoning
about time and reliability. Formal Aspects of
Computing ,6 ( 5 ) : 5 1 2 – 5 3 5 ,1 9 9 4 .
[35] M. Harman, Y. Jia, J. Krinke, W. B. Langdon,
J. Petke, and Y. Zhang. Search based software
engineering for software product line engineering: A
survey and directions for future work. In 18th Intl.Softw. Product Line Conf. , pages 5–18, 2014.
[36] M. Harman, Y. Jia, and W. B. Langdon. Strong
higher order mutation-based test data generation. In
19th ACM SIGSOFT Symposium and the 13th
European Conf. on Foundations of Software
Engineering (ESEC/FSE’11) , pages 212–222, 2011.
[37] M. Harman, S. A. Mansouri, and Y. Zhang.
Search-based software engineering: Trends, techniques
and applications. ACM Computing Surveys ,
45(1):11:1–11:61, 2012.
[38] M. Harman, P. McMinn, J. de Souza, and S. Yoo.
Search based software engineering: Techniques,
taxonomy, tutorial. In Empirical Softw. Eng. and
Verif. , volume 7007 of LNCS , pages 1–59. 2012.
[39] C. Johnson. Genetic programming with ﬁtness based
on model checking. In Genetic Programming , volume
4445 of LNCS , pages 114–124. 2007.
[40] J. P. Katoen, M. Khattri, and I. S. Zapreev. A Markov
reward model checker. In Quantitative Evaluation of
Systems (QEST’05) , pages 243–244, 2005.
[41] J. P. Katoen, I. S. Zapreev, E. M. Hahn,
H. Hermanns, and D. N. Jansen. The ins and outs of
the probabilistic model checker MRMC. Performance
Evaluation ,6 8 ( 2 ) : 9 0 – 1 0 4 ,2 0 1 1 .
[42] G. Katz and D. Peled. Synthesis of parametric
programs using genetic programming and model
checking. In 15th Intl. Workshop Verif. Inﬁnite-State
Systems (INFINITY’13) , pages 70–84, 2013.
[43] J. R. Koza. Genetic Programming: On the
Programming of Computers by Means of Natural
Selection . MIT Press, Cambridge, MA, USA, 1992.
[44] M. Kwiatkowska. Quantitative veriﬁcation: Models,
techniques and tools. In 6th ACM SIGSOFT Symp.
Foundations of Softw. Eng. (ESEC/FSE) , pages
449–458, 2007.
[45] M. Kwiatkowska, G. Norman, and D. Parker. PRISM
4.0: Veriﬁcation of probabilistic real-time systems. In
23rd Intl. Conf. on Computer Aided Veriﬁcation
(CAV’11) ,v o l u m e6 8 0 6o f LNCS , pages 585–591, 2011.
[46] M. Z. Kwiatkowska, G. Norman, and D. Parker.
Probabilistic symbolic model checking with PRISM: A
hybrid approach. Int. Journal on Software Tools for
Technology Transfer ,6 ( 2 ) : 1 2 8 – 1 4 2 ,2 0 0 4 .
[47] U. Martinez-Araiza and E. Lopez-Mellado. A CTL
model repair method for Petri Nets. In World
Automation Congress (WAC’14) , pages 654–659, 2014.
[48] L. L. Minku and X. Yao. Software eﬀort estimation as
a multiobjective learning problem. ACM Trans. Softw.
Eng. Methodol. ,2 2 ( 4 ) : 3 5 : 1 – 3 5 : 3 2 ,2 0 1 3 .
[49] A. J. Nebro, J. J. Durillo, F. Luna, B. Dorronsoro,
and E. Alba. MOCell: A cellular genetic algorithm for
multiobjective optimization. Intl. Journal of
Intelligent Systems ,2 4 ( 7 ) : 7 2 6 – 7 4 6 ,2 0 0 9 .
[50] G. Norman and D. Parker. Quantitative veriﬁcation:
Formal guarantees for timeliness, reliability and
performance. Technical report, The London
Mathematical Society and the Smith Institute, 2014.
[51] A. Pnueli. The temporal logic of programs. In 18th
Annual Symposium on Foundations of Computer
Science , pages 46–57, 1977.[52] K. Praditwong, M. Harman, and X. Yao. Software
module clustering as a multi-objective search problem.
IEEE Trans. Softw. Eng. , 37(2):264–282, March 2011.
[53] Q. Qiu, Q. Qu, and M. Pedram. Stochastic modeling
of a power-managed system-construction and
optimization. IEEE Trans. Computer-Aided Design of
Integrated Circuits & Systems ,2 0 ( 1 0 ) : 1 2 0 0 – 1 2 1 7 ,2 0 0 1 .
[54] J. Ren, M. Harman, and M. Di Penta. Cooperative
co-evolutionary optimization of software project staﬀ
assignments and job scheduling. In Search Based
Software Engineering (SSBSE’11) , volume 6956 of
LNCS , pages 127–141. 2011.
[55] A. Sayyad, J. Ingram, T. Menzies, and H. Ammar.
Scalable product line conﬁguration: A straw to break
the camel’s back. In 28th Intl. Conf. on Automated
Software Engineering (ASE’13) ,p a g e s4 6 5 – 4 7 4 ,2 0 1 3 .
[56] A. Sesic, S. Dautovic, and V. Malbasa. Dynamic
power management of a system with a two-priority
request queue using probabilistic-model checking.
IEEE Trans. on Computer-Aided Design of Integrated
Circuits and Systems ,2 7 ( 2 ) : 4 0 3 – 4 0 7 ,2 0 0 8 .
[57] C. Stylianou, S. Gerasimou, and A. Andreou. A novel
prototype tool for intelligent software project
scheduling and staﬃng enhanced with personality
factors. In 24th Intl. Conf. on Tools with Artiﬁcial
Intelligence (ICTAI’12) ,p a g e s2 7 7 – 2 8 4 ,2 0 1 2 .
[58] D. A. Van Veldhuizen. Multiobjective Evolutionary
Algorithms: Classiﬁcations, Analyses, and New
Innovations . PhD thesis, 1999.
[59] J. Woodcock, P. G. Larsen, J. Bicarregui, and
J. Fitzgerald. Formal methods: Practice and
experience. ACM Computing Surveys ,4 1 ( 4 ) : 1 9 ,2 0 0 9 .
[60] H. L. S. Younes. Ymer: A statistical model checker. In
Computer Aided Veriﬁcation (CAV’05) , volume 3576
ofLNCS , pages 429–433. 2005.
[61] Y. Zhang and Y. Ding. CTL model update for system
modiﬁcations. Journal of Artiﬁcial Intelligence
Research (JAIR) ,3 1 : 1 1 3 – 1 5 5 ,2 0 0 8 .
[62] E. Zitzler, D. Brockhoﬀ, and L. Thiele. The
hypervolume indicator revisited: On the design of
pareto-compliant indicators via weighted integration.
In4th Intl. Conf. on Evolutionary Multi-criterion
Optimization (EMO’07) , pages 862–876, 2007.
[63] E. Zitzler, J. Knowles, and L. Thiele. Quality
assessment of Pareto set approximations. In
Multiobjective Optimization , volume 5252 of LNCS ,
pages 373–404. 2008.
[64] E. Zitzler, M. Laumanns, and L. Thiele. SPEA2:
Improving the strength Pareto evolutionary algorithm.
InEvolutionary Methods for Design Optimization and
Control with Applications to Industrial Problems
(EUROGEN’01) , pages 95–100, 2001.
[65] E. Zitzler and L. Thiele. Multiobjective evolutionary
algorithms: a comparative case study and the strength
pareto approach. IEEE Trans. on Evolutionary
Computation ,3 ( 4 ) : 2 5 7 – 2 7 1 ,1 9 9 9 .
[66] E. Zitzler, L. Thiele, M. Laumanns, C. Fonseca, and
V. da Fonseca. Performance assessment of
multiobjective optimizers: an analysis and review.
IEEE Trans. on Evolutionary Computation ,
7(2):117–132, 2003.