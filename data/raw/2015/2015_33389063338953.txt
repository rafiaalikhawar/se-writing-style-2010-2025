White-Box Testing of Big Data Analytics with Complex
User-Defined Functions
Muhammad Ali Gulzar
Shaghayegh Mardani
University of California, Los Angeles
USAMadanlal Musuvathi
Microsoft Research, USAMiryung Kim
University of California, Los Angeles
USA
ABSTRACT
Data-intensive scalable computing (DISC) systems such as Google‚Äôs
MapReduce, Apache Hadoop, and Apache Spark are being lever-
aged to process massive quantities of data in the cloud. Modern
DISC applications pose new challenges in exhaustive, automatic
testing because they consist of dataflow operators, and complex
user-defined functions (UDF) are prevalent unlike SQL queries. We
design a new white-box testing approach, called BigTest to reason
about the internal semantics of UDFs in tandem with the equiva-
lence classes created by each dataflow and relational operator.
Our evaluation shows that, despite ultra-large scale input data
size, real world DISC applications are often significantly skewed and
inadequate in terms of test coverage, leaving 34% of Joint Dataflow
and UDF (JDU) paths untested. BigTest shows the potential to min-
imize data size for local testing by 105to 108orders of magnitude
while revealing 2X more manually-injected faults than the previous
approach. Our experiment shows that only few of the data records
(order of tens) are actually required to achieve the same JDU cover-
age as the entire production data. The reduction in test data also
provides CPU time saving of 194X on average, demonstrating that
interactive andfastlocal testing is feasible for big data analytics,
obviating the need to test applications on huge production data.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíCloud computing ;Soft-
ware testing and debugging ;‚Ä¢Information systems ‚ÜíMapR-
educe-based systems .
KEYWORDS
symbolic execution, dataflow programs, data intensive scalable
computing, map reduce, test generation
ACM Reference Format:
Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miry-
ung Kim. 2019. White-Box Testing of Big Data Analytics with Complex
User-Defined Functions. In Proceedings of the 27th ACM Joint European Soft-
ware Engineering Conference and Symposium on the Foundations of Software
Engineering (ESEC/FSE ‚Äô19), August 26‚Äì30, 2019, Tallinn, Estonia. ACM, New
York, NY, USA, 12 pages. https://doi.org/10.1145/3338906.3338953
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia
¬©2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-5572-8/19/08. . . $15.00
https://doi.org/10.1145/3338906.33389531 INTRODUCTION
Data-intensive scalable computing (DISC) systems such as Mapre-
duce [ 20], Apache Hadoop [ 1], Apache Spark [ 48] are commonly
used today to process terabytes and petabytes of data. At this scale,
rare and buggy corner cases frequently show up in production [ 49].
Thus, it is common for these applications to either crash after run-
ning for days or worse, silently produce corrupted output. Unfortu-
nately, the common industry practice for testing these applications
remains running them locally on randomly sampled inputs, which
obviously does not flush out bugs hiding in corner cases.
This paper presents a systematic input generation tool, called
BigTest , that embodies a new white-box testing technique for DISC
applications. BigTest is motivated by the recent successes of sys-
tematic test generation tools [ 22,24,39]. However, the nature of
DISC applications requires extending these in important ways to be
effective. Unlike general-purpose programs addressed by existing
testing tools, DISC applications use a combination of relational op-
erators, such as join andgroup-by , and dataflow operators, such
asmap,flatmap , along with user-defined functions (UDFs) written
in general purpose languages such as C/C++, Java, or Scala.
In order to comprehensively test DISC applications, BigTest rea-
sons about the combined behavior of UDFs with relational and
dataflow operations. A trivial way is to replace these operations
with their implementations and symbolically execute the resulting
program. However, existing tools are unlikely to scale to such large
programs, because dataflow implementation consists of almost 700
KLOC in Apache Spark. Instead, BigTest includes a logical abstrac-
tion for dataflow and relational operators when symbolically exe-
cuting UDFs in the DISC application. The set of combined path con-
straints are transformed into SMT (satisfiability modulo theories)
queries and solved by leveraging an off-the-shelf theorem prover,
Z3 or CVC4, to produce a set of concrete input records [ 11,19]. By
using such a combined approach, BigTest is more effective than
prior DISC testing techniques [ 31,34] that either do not reason
about UDFs or treat them as uninterpreted functions.
To realize this approach, BigTest tackles three important chal-
lenges that our evaluation shows are crucial for the effectiveness of
the tool. First, BigTest models terminating cases in addition to the
usual non-terminating cases for each dataflow operator. For exam-
ple, the output of a join of two tables only includes rows with keys
that match both the input tables. To handle corner cases, BigTest
carefully considers terminating cases where a key is only present
in the left table, the right table, and neither. Doing so is crucial,
as based on the actual semantics of the join operator, the output
can contain rows with null entries, which are an important source
of bugs. Second, BigTest models collections explicitly, which are
created by flatmap and used by reduce . Prior approaches [ 31,34]
290
ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim
1val x,y,z;
2if(x<y)
3 z = y/x; //PC1: x < y = true, Effect: z=y/x
4else
5 z = x/y; //PC2: x >= y = true, Effect: z=x/y
Figure 1: Symbolic PathFinder produces a set of path con-
straints and their corresponding effects
do not support such operators, and thus are unable to detect bugs
if code accesses an arbitrary element in a collection of objects or
if the aggregation result is used within the control predicate of
the subsequent UDF. Third, BigTest analyzes string constraints
because string manipulation is common in DISC applications and
frequent errors are ArrayIndexOutOfBoundException and String-
IndexOutOfBoundsException during segmentation and parsing.
To evaluate BigTest , we use a benchmark set of 7 real-world
Apache Spark applications selected from previous work such as
PigMix[ 35],Titian [28], and BigSift [25]. While these programs
are representative of DISC applications, they do not adequately rep-
resent failures that happen in this domain. To rectify this problem,
we perform a survey of DISC application bugs reported in Stack
Overflow and mailing lists and identify seven categories of bugs.
We extend the existing benchmarks by manually introducing these
categories of faults into a total of 31 faulty DISC applications. To the
best of our knowledge, this is the first set of DISC application bench-
marks with representative real-world faults. Such benchmarks are
crucial for further research in this area.
We assess JDU (Joint Dataflow and UDF) path coverage, sym-
bolic execution performance, and SMT query time. Our evaluation
shows that real world datasets are often significantly skewed and
inadequate in terms of test coverage of DISC applications, still leav-
ing 34% of JDU paths untested. Compared to Sedge [31],BigTest
significantly enhances its capability to model DISC applications‚ÄîIn
5 out of 7 applications, Sedge is unable to handle these applications
at all, due to limited dataflow operator support and in the rest 2
applications, Sedge covers only 23% of paths modeled by BigTest .
We show that JDU path coverage is directly related to improve-
ment in fault detection‚Äî BigTest reveals 2X more manually injected
faults than Sedge on average. BigTest can minimize data size for
local testing by 105to 108orders of magnitude, achieving the CPU
time savings of 194X on average, compared to testing code on the
entire production data. BigTest synthesizes concrete input records
in 19 seconds on average for all remaining untested paths. Below,
we highlight the summary of contributions.
‚Ä¢BigTest is the first piece of DISC white-box testing that
comprehensively models dataflow operators and the internal
paths of user-defined functions in tandem.
‚Ä¢BigTest makes three important enhancements to improve
fault detection capability for DISC applications‚Äî(1) It con-
siders both terminating andnon-terminating cases of each
dataflow operator; (2) It explicitly models collections created
byflatmap and translates aggregation logic into an iterative
aggregator; and (3) It models string constraints explicitly.
‚Ä¢It puts forward a benchmark of manually injected DISC
application faults along with generated test data, inspired
by the characteristics of real world DISC application faults
evidenced by Stack Overflow and mailing lists.1val trips = sc.textFile("trips_table.csv")
2 .map{s =>
3 val cols = s.split(",")
4 (cols(1),cols(3).toInt/cols(4).toInt) }
5 //Returns location and speed
6val zip = sc.textFile("zipcode_table.csv")
7 .map{s =>
8 val cols = s.split(",")
9 (cols(1),cols(0) }
10 // Returns location and its name
11 .filter {
12 s => s._ 2 == "Palms" }
13 val joined = trips. join (zip)
14 joined
15 .map{s =>
16 if (s._2._1 > 40) ("car",1)
17 else if (s._2._1 > 15) ("bus",1)
18 else ("walk",1)
19 }
20 .reduceByKey (_+_ )
21 .saveAsTextFile("hdfs://...")‚ûä
‚ûã
‚ûå
‚ûç
‚ûé
Figure 2: Alice‚Äôs program estimates the total number of trips
originated from ‚ÄúPalms.‚Äù
‚Ä¢BigTest finds 2X more faults than Sedge , minimizes test data
by orders of magnitude, and is fast and interactive.
Our results demonstrate that interactive local testing of big data
analytics is feasible, and that developers should not need to test
their program on the entire production data. For example, a user
may monitor path coverage with respect to the equivalent classes
of paths generated from BigTest and skip records if they belong to
the already covered path, constructing a minimized sample of the
production data for local development and testing.
The rest of the paper is organized as follows. Section 2 provides a
brief introduction to Apache Spark and symbolic execution. Section
3 describes a motivating example. Section 4 describes the design of
BigTest . Section 5 describes evaluation settings and results. Section
6 discusses related work. Section 7 concludes the paper.
2 BACKGROUND
Apache Spark. BigTest targets Apache Spark, a widely used data
intensive scalable computing system. Spark extends the MapRe-
duce programming model with direct support for dataflow and
traditional relational algebra operators ( e.g.,group-by ,join , and
filter ). Datasets can be loaded in Spark runtime using several
APIs that create Resilient Distributed Datasets (RDDs), an abstrac-
tion of distributed collection [ 47]. RDDs can be transformed by
invoking dataflow operations on them ( e.g., val filterRdd =
rdd.filter(_ >5) ). Dataflow operators such as map,reduce , and
flatmap are implemented as higher-order functions that take a user
defined function (UDF) as an input parameter. The actual evalua-
tion of an RDD occurs when an action such as count orcollect is
called. Internally, Spark translates a series of RDD transformations
into a Directed Acyclic Graph (DAG) where each vertex represents
a transformation applied to the incoming RDD. The Spark scheduler
executes each stage in a topological order.
Symbolic Execution using Java Path Finder. BigTest builds on
Symbolic Java PathFinder (SPF) [ 36]. Internally, SPF relies on the
291White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia
Trips
Zipcode
Map:	ùëìmap1
Map:	ùëìmap2
Filter:	ùëìfilter
Join:	‚®ù
Map:	ùëìmap3
ReduceByKey:	ùëìAgg
~ùëìfilter(K2,	V2)
T1
T4FalseTrue
ùëìfilter(K2,	V2)	‚ãÄK1=	K2
(K1,	V1)(K2,	V2)(K1,	(V1,	V2))(S	,1)(S	,N)
ùëìfilter(K2,	V2)	‚ãÄK1‚àâZipcode
K1‚àâZipcodeK2‚àâTrips
ùëìfilter(K2,	V2)	‚ãÄK2‚àâTrips
T2
T3TZ
(a) Dataflow operators‚Äô paths by BigTest
Map:	ùëìmap1String : T
T.split(‚Äú,‚Äù).length >= 5 ‚ãÄisInt(T.split(‚Äú,‚Äù)(3)) ‚ãÄisInt(T.split(‚Äú,‚Äù)(4)) ‚ãÄT.split(‚Äú,‚Äù)(4).toInt!= 0 K1=T.split(‚Äú,‚Äù)(1)  V1=T.split(‚Äú,‚Äù)(3).toInt/T.split(‚Äú,‚Äù)(4).toInt
=>String : Z
Z.split(‚Äú,‚Äù).length >= 2=>
Map:	ùëìmap2K2=Z.split(‚Äú,‚Äù)(1)V2=Z.split(‚Äú,‚Äù)(0)
String : K2, String : V2
V2 == ‚ÄúPalms‚ÄùTrue
Filter:	ùëìfilterString : K1, Int: V1, String : V2
V1>40=>S=‚Äúcar‚Äù
Map:	ùëìmap315<V1‚â§40
V1<15
=>S=‚Äúpublic‚Äù=>S=‚Äúwalk‚ÄùString : S , Int[K] :  [a1,a2,a3,...,aK]
ReduceByKey:	ùëìAggK==1=>N=a1
K==2
K==n
=>N=ùëìAgg([a1 , a2]) =>N=ùëìAgg(a1,ùëìAgg(a2,‚Ä¶,ùëìAgg(an-1,an)‚Ä¶) . . .
=>(b) Non-terminating path conditions of individual UDFs
Map:	ùëìmap1String : T
T.split(‚Äú,‚Äù).length < 5T.split(‚Äú,‚Äù).length >= 5 ‚ãÄNotInt(T.split(‚Äú,‚Äù)(3))T.split(‚Äú,‚Äù).length >= 5 ‚ãÄisInt(T.split(‚Äú,‚Äù)(3))  ‚ãÄNotInt(T.split(‚Äú,‚Äù)(4))T.split(‚Äú,‚Äù).length >= 5 ‚ãÄisInt(T.split(‚Äú,‚Äù)(3)) ‚ãÄisInt(T.split(‚Äú,‚Äù)(4)) ‚ãÄT.split(‚Äú,‚Äù)(4).toInt== 0
Z.split(‚Äú,‚Äù).length < 2
=>X=>X=>X=>X
Map:	ùëìmap2String : Z
=>X
(c) Path constraints for terminating paths in UDFs‚ûä‚ûã
‚ûå
‚ûç
‚ûé‚ûä ‚ûã ‚ûå
‚ûç ‚ûé
‚ûä ‚ûã
Figure 3: Solid and dotted boxes represent transformations and path constraints, respectively. BigTest identifies path con-
straints for both non-terminating and terminating program paths while symbolically executing the program.
analysis engine of Java PathFinder (JPF) model checking [ 43]. It
interprets Java bytecode on symbolic inputs and produces a set of
symbolic constraints. Each constraint represents a unique path in
the program, and can be ingested by a theorem solver to generate
test inputs. Figure 1 illustrates an example symbolic execution result.
By attaching listeners to SPF, the path conditions and the effects of
each path can be captured. For this program, SPF produces two path
conditions: (1) the first path produces the effect of z=y/x , when
the path condition x<yholds true and (2) the second path produces
z=x/y as an effect, when the path condition x‚â•yis satisfied.
3 MOTIVATING EXAMPLE
This section presents a running example to motivate BigTest . Sup-
pose that Alice writes a DISC application in Spark to analyze the
Los Angeles commuting dataset. She wants to find the total number
of trips originating from the ‚ÄúPalms‚Äù neighborhood using: (1) a pub-
lic transport whose speed is assumed to be faster than 15 but slower
than 40 mph, (2) a personal vehicle which is estimated to be faster
than 40 mph, and (3) on foot which is estimated as slower than 15
mph. Each row in the Trips dataset represents a unique identifier
for the trip, the start and end location in terms of a zip code, the
trip distance in miles, and the trip duration in hours, for example,
1,90034,90024, 10, 1 . To map an area zip code to its correspond-
ing area name, Alice uses another dataset that assigns a name to
each zip code in the following manner: 90034,Culver City
To perform this analysis, Alice writes a Spark application in
Figure 2. She loads both datasets (lines 1 and 6), parses each dataset,
selects the start location of a trip as a key, and computes the average
speed as a value by dividing the distance by duration (lines 2-4).
Alice outputs a zip code as a key and an area name as a value (lines
7-9) and filters the area name with ‚ÄúPalms" at line 12. She joins
the two data sets (line 13). In the subsequent mapoperation (line
15-18), she categorizes the trips based on the average speed into
three categories. She finally counts the frequency of each trip kindand stores them (lines 20 and 21). Though this program is only 21
lines long, it poses several challenges for modeling test paths.
Equivalence Classes of Dataflow Operators. Consider filter
‚ûåat line 11. To exhaustively test this operator, we must consider
two equivalence classes: the first where a data record satisfies the
filter and moves onto the next operator and the second where the
filter does not satisfy and its data flow terminates. If we only model
non-terminating case then test data would contain passing data
records only and hence, would not detect a fault in which filter
is removed from the DISC application. To model join at line 13,
we must have three equivalence classes‚Äîtwo terminating cases
and one non-terminating case: (1) an input record in the left table
(‚ÄúTrip‚Äù) does not have a matching key on the right table (‚ÄúZipCode‚Äù),
terminating its data flow, (2) an input in the right table does not
have a matching key on the left, terminating its data flow, and (3)
there exists a key that appears in both tables, passing the joined
result to the next operator. Modeling such terminating cases is
crucial otherwise test data generated produce the same output for
both join andleftOuterJoin and do not reveal faults that are
based on incorrect join type usage.
UDF Paths. Consider the map‚ûçat lines 15-18. There are three
internal path conditions: (1) speed >40 mph, (2) 15 mph <speed
‚â§40 mph, and (3) speed ‚â§15 mph. The sub figure ‚ûçin Figure 3b
shows corresponding path conditions and effects.
String Constraints. To analyze the second map‚ûãat lines 7 to 9,
we must reason about the entailed string constraints. Given a string
Zin sub figure ‚ûãin Figure 3b and 3a, to split the data into two
columns, it must satisfy a string constraint Z.split(",").length
‚â•2to produce the effect where the key K2is(Z.split(",")
(1)and the value V2is(Z.split(",")(0)) . String manipulation
is critical to many DISC applications. In the above example, at
least one test must contain a string zwithout delimiter ",", so
thatz.split(",")(1) leads to ArrayIndexOutOfBoundsException
which will then expose the inability of the UDF to handle exceptions.
292ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim
Table 1: Generated input data where each row represents a unique path. Variables T,Z,V, and Kare defined in Figure 3a.
# Constraint Trips Zipcode
C1 T.split(",").length < 5 "" _ _
C2 T.split(",").length ‚â•5‚àßNotInt(T.split(",")(3)) _, _, _, "", _ _, _
C3 T.split(",").length ‚â•5‚àßisInt(T.split(",")(3)) ‚àßNotInt(T.split(",")(4)) _, _, _, "-2", "" _, _
C4 T.split(",").length ‚â•5‚àßisInt(T.split(",")(3)) ‚àßisInt(T.split(",")(4)) ‚àßT.split(",")(4).toInt = 0 _, _, _, "-2", "0" _, _
C5 Z.split(",").length < 2 _ ""
C6 Z.split(",").length ‚â•2‚àßV2!= "Palms" _ _, "\x00"
C7T.split(",").length ‚â•5‚àßisInt(T.split(",")(3)) ‚àßisInt(T.split(",")(4))
‚àßT.split(",")(4).toInt != 0 ‚àßZ.split(",").length ‚â•2‚àßV2= "Palms"‚àßK1<Zipcode_, "!0!", _, _, _ "\x00", "Palms"
C8 . . .‚àßV2= "Palms"‚àßK2<Trips _, ""!0!", _, _, _ "\x00", "Palms"
C9 . . .‚àßV2= "Palms"‚àßK1= K 2‚àßV1> 40 _, "\x00", _, "41", "1" "\x00", "Palms"
C10 . . .‚àßV2= "Palms"‚àßK1= K 2‚àß15< V 1< 40 _, "\x00", _, "16", "1" "\x00", "Palms"
C11 . . .‚àßV2= "Palms"‚àßK1= K 2‚àßV1< 15 _, "\x00", _, "0", "1" "\x00", "Palms"
Otherwise, this application may crash in production, when the input
record does not have an expected delimiter.
Arrays. To analyze reduceByKey ‚ûéat line 20 (also in Figure 3b),
we must model how the UDF operates on the input array of size K,
[a1,a2,. . .,aK]and produces the corresponding output fa–¥–¥(a1
,fa–¥–¥(a2. . .fa–¥–¥(aK‚àí1,aK). . .)). For example, the UDF (_+_)
returns the sum of two input arguments. When the array size Kis
given by a user, the final output Nisa1+(a 2+. . .(aK‚àí1+aK)).
Summary. Due to the internal path conditions entailed by indi-
vidual UDFs, instead of four high-level dataflow paths shown in
Figure 3a, Alice must consider eleven paths in total, which are enu-
merated in Table 1. Figure 3 shows the symbolic execution tree at
the level of dataflow operators on the left and the internal symbolic
execution trees for individual UDFs on the right. Lastly, example
data generated by BigTest for each JDU path using Z3 is shown
in Table 1. While these example data records may not look realis-
tic, such data is necessary to exercise the downstream UDFs that
are otherwise unreachable with the original dataset. For instance,
filtering a dataset without any passing data record will result in
an empty set and consequently, the UDFs after the filter will
never get tested with the original data. Therefore, synthetic data is
necessary and crucial to expose downstream program behavior.
4 APPROACH
BigTest takes in an Apache Spark application in Scala as an input
and generates test inputs to cover all paths of the program up to a
given bound by leveraging theorem provers Z3 [ 19] and CVC4 [ 11].
4.1 Dataflow Program Decomposition
A DISC application is comprised of a direct acyclic graph where
each node represents a dataflow operator such as reduce and cor-
responding UDFs. As the implementation of dataflow operators in
Apache Spark spans several hundred thousand lines of code, it is not
feasible to perform symbolic execution of a DISC application along
with the Spark framework code. Instead, we abstract the internal
implementation of a dataflow operator in terms of logical specifi-
cations. We decompose a DISC application into a dataflow graph
where a node calls each UDF and combine the symbolic execution
of the UDFs using the logical specification of dataflow operators.
UDF Extraction. BigTest compiles the DISC application into Java
bytecode and traverses each Abstract Syntax Tree (AST) to search
for a method invocation corresponding to each dataflow operator.
The input parameters of such method invocation are UDFs repre-
sented as anonymous functions as illustrated in Figure 4b. BigTest
stores the UDF as a separate Java class shown in Figure 4c and1sc.textFile("zipcode.csv"). map{...}
2 .filter {_._2 == "Palms"}
(a) DISC Application
MethodDeclaration
Name	:	‚Äúmain‚Äù
Body
MethodInvocation
Name	:	‚Äúfitler‚Äù
parameter
ClassInstanceCreation
UDFfilter
AnonymousClassDeclaration
.	.	.	
.	.	.	.	.	.	.	.	.	.	.	.	.	.	.	
.	.	.	.	.	.	
(b) Generated AST1class filter {
2 static void main(String args[]){
3 apply(null);
4 }
5 static boolean apply(Tuple2 s){
6 return s._2().equals("Palms")
7 }
8}
(c) Extracted Filter UDF
Figure 4: BigTest extracts UDFs corresponding to dataflow op-
erators through AST traversal.
generates a configuration file required by JPF for symbolic execu-
tion. BigTest also performs dependency analysis to include external
classes and methods referenced in the UDF.
1def f(a:Int,b:Int){
2 return a+b;
3}
4//Usage in reduce
5...reduce {f}
(a)1def f_reduce(arr:Array[Int]){
2 var sum = 0;
3 for(a <- 1 to K)//K is bound
4 sum = udf(sum,arr(a));
5 return sum; }
(b)
Figure 5: (a) a normal invocation of reduce with a corre-
sponding UDF. (b) an equivalent iterative version with a
bound K
Handling Aggregator Logic. For aggregation operators, the at-
tached UDF must be transformed. For example, the UDF for reduce
is an associative binary function, which performs incremental ag-
gregation over a collection shown in Figure 5a. We translate it into
an iterative version with a loop shown in Figure 5b. To bound the
search space of constraints, we bound the number of iterations to a
user provided bound K(default is 2).
4.2 Logical Specifications of Dataflow
Operators
This section describes the equivalence classes generated by each
dataflow operator‚Äôs semantics. We use CIto represent a set of path
constraints on the input data, I, for a particular operator. A single
element cinCIcontains path constraints that must be satisfied
to exercise a corresponding unique path. We define fas the set
293White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia
Table 2: CIrepresents a set of incoming constraints from the input table I, where each constraint c‚ààCIrepresents a non-
terminating path. c(t)represents that record t‚ààImust satisfy constraint c.fdefines the set of path constraints generated by
symbolically executing ud fand f(t)represents the path constraint of a unique path exercised by input tuple t.
Operator Inputs Logical Specification
filter( ud f)I: Input Table
ud f :t‚ÜíBoolNon-Terminating ‚ù∂‚àÉt:t‚ààI‚àßc‚ààCI‚àßc(t)‚àßf(t)
Terminating ‚ù∑‚àÉt:t‚ààI‚àßc‚ààCI‚àßc(t)‚àß¬¨ f(t)
map( ud f)I : Input Table, O : Output Table
ud f :t‚Üít‚Ä≤where t‚Ä≤‚ààONon-Terminating ‚ù∏‚àÉt:t‚ààI‚àßc‚ààCI‚àßc(t)‚àßf(t)
flatmap( ud f)I : Input Table, O: Output Table
ud f :t‚ÜíCollection of t‚Ä≤where t‚Ä≤‚ààONon-Terminating ‚ùπ‚àÉt:tI‚ààI‚àßc‚ààCI‚àßc(t)‚àßf(t)
joinR : Right Table, tR‚ààR
L : Left Table, tL‚ààLNon-Terminating ‚ù∫‚àÉtR,tL:cR‚ààCR‚àßcL‚ààCL‚àßcR(tR)‚àßtR,key=tL,key‚àßcL(tL)
Terminating ‚ùª‚àÉkey,tR:cR‚ààCR‚àßcL‚ààCL‚àßcR(tR)‚àßtR,key=key‚àß(‚àÄtL‚ààL:cL(tL)‚àßtL,key,key)
Terminating ‚ùº‚àÉkey,tL:cR‚ààCR‚àßcL‚ààCL‚àßcL(tL)‚àßtL,key=key‚àß(‚àÄtR‚ààR:cR(tR)‚àßtR,key,key)
groupByKeyI : Input Table
t‚ààIandt=(tkey,tvalue)Non-Terminating ‚ùΩ‚àÉt:t‚ààI‚àßc‚ààCI‚àßc(t)‚àß|{ X|x‚ààI‚àßxkey=tkey}|>0
reduce( ud f)
reduceByKey( ud f)I : Input Table, O: Output
ud f :(t,t)‚Üít‚Ä≤where t‚Ä≤‚ààONon-Terminatingud f‚Ä≤is an iterative version of the original UDF ud f, given as an input to reduce/reduceByKey.
f‚Ä≤represents the set of path constraints generated from symbolic execution of ud f‚Ä≤.
‚ùæ‚àÉt1,t2,t3, . . . , tn‚ààI:c1,c2, . . . , cn‚ààCI‚àßc1(t1)‚àßc2(t2)‚àß. . . ..‚àßcn(tn)‚àßf‚Ä≤(I)
of symbolic path constraints of a UDF where f(t)represents con-
straints of a unique path exercised by input t. By abstracting the
implementation of dataflow operators into logical specifications,
BigTest does not have to symbolically execute the Spark framework
code (about 700KLOC), as it focuses on application level faults only
as opposed to framework implementation faults which is out of the
scope of this paper. BigTest supports all popular dataflow operators
with the exception of deprecated operators such as co-join .
Filter. Filter takes a boolean function ud f deciding if an input
record should be passed to downstream operators or not. Therefore,
we model two equivalence classes: (1) there exists a record tthat
satisfies ud f and one of the incoming constraints CIfrom input
table I(i.e.,the table produced by its upstream satisfying operator),
shown in ‚ù∂Table 2; (2) there exists a record tthat satisfies one of
the incoming constraints but not ud f, shown in ‚ù∑Table 2.
Map and Flatmap. Maptakes a UDF ud f as an input and applies
it to each input record to produce an output record. It has one
equivalence class, where there exists tuple tfrom the input table
Isatisfying one of the incoming constraints, c‚ààCIand also one
of the path constraints in fi.e.,path constraints generated by
symbolically executing ud f, shown in ‚ù∏Table 2. Mapis supported
by the previous work Sedge butSedge considers the UDF ud f as a
black box, uninterpreted function. Flatmap splits an input record
using a ud f to generate a set of records, and thus the equivalence
class of flatmap is similar to that of map, as shown in ‚ùπ.BigTest
handles flatmap by explicitly modeling a collection, described in
Section 4.3.
Join. Join performs an inner-join of two tables tron the right and
table tlon the left based on the equality of keys, assuming that
records from both tables are of the type Tuple (key, value). We
model the output records of join into three equivalence classes: (1)
the key of tuple tRin the right table matches with a key of tuple tL
on the left; (2) the key of tuple tRin the right table does not match
with any key of tuple tLon the left; and (3) the key of tuple tLin
the left table does not match with any key of tuple tRon the right.
‚ù∫,‚ùª, and‚ùºin Table 2 represent the three equivalence classes.
Reduce and ReduceByKey. reduce takes a ud f and a collection
as inputs and outputs an aggregated value, while reduceByKey
performs a similar operation per key. As discussed in Section 4.1,
BigTest generates an equivalent iterative version of the ud f with
a loop. By this refactoring of ud f toud f‚Ä≤, the equivalence classes
could be modeled similar to that of map, where there exist inputrecords t1,t2, . . . , tn‚ààIon which each of the corresponding non-
terminating constraint ( c1,c2, . . . , cn)‚ààCIfrom the input table I
holds true. In addition, each record must satisfy the constraints of
ud f‚Ä≤, satisfying f‚Ä≤([t1,t2, . . . , tn]), as shown in ‚ùæof Table 2.
4.3 Path Constraint Generation
This section describes several enhancements in Symbolic Path
Finder (SPF) to tailor symbolic execution for DISC applications.
DISC applications extensively use string manipulation operations
and rely on a Tuple data structure to enable key-value based op-
erations. Using an off-the-shelf SPF na√Øvely on a UDF would not
produce meaningful path conditions, thus, overlooking faults dur-
ing testing.
1def parse(s:String){
2 val cols = s.split(",")
3 (cols(0) , cols(1)) }
Figure 6: A UDF with string manipulation
Strings. Operations such as split for converting an input record
into a key-value pair are common in DISC applications but are not
supported by SPF. BigTest extends SPF by capturing calls to split ,
recording the delimiter, and returning an array of symbolic strings.
When an n-th element of this symbolic array is requested, SPF
returns a symbolic string encoded as splitn with a corresponding
index. By representing the effect of Figure 6 as (splitn(",",s,0),
splitn(",",s,1)) ,BigTest generates one terminating constraint,
where scan only split into fewer than two segments, and one non-
terminating constraint where scan split into at least two segments.
Due to no split support, na√Øve SPF generates a string without any
delimiter as a test input e.g.,"\x00" instead of "\x00,\x00". This input
would lead to ArrayIndexOutOfBoundsException while accessing
a string using split(",")(1) .
1def agg(arr:Array[Int]){
2 val sum = arr(0); // Bound K=3
3 for(a <- 1 to min(arr.size,2)) sum += arr(a)<0 ? 0 : arr(a);
4 sum }
Figure 7: An iterative version of aggregator UDF
Collections. Constructing and processing collections through op-
erators such as flatmap are essential in DISC applications. There-
fore, BigTest explicitly models the effect of applying a UDF on
a collection. In Figure 7, an iterative version of aggregator logic
294ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim
produced by BigTest takes a collection as input and sums up each
element, if the element is greater than or equal to zero. Given a
user-provided bound K=3 BigTest unrolls the loop three time and
generates four pairs of a path condition (P) and the corresponding
effect (E):
(1)P:a(1)<0‚àßa(2)<0 ,E:a(0)
(2)P:a(1)‚â•0‚àßa(2)<0 ,E:a(0)+a(1)
(3)P:a(1)<0‚àßa(2)‚â•0,E:a(0)+a(2)
(4)P:a(1)‚â•0‚àßa(2)‚â•0,E:a(0)+a(1)+a(2)
A na√Øve SPF does not handle collections well and thus may generate
an array of length 1 only, not exercising line 3 in Figure 7. For
example, agg({3}) outputs the same sum of 3, when arr(a)<0 is
mutated to arr(a)>0 ), because the loop starts from 1 instead of 0,
andsumis initialize to the first element of the array. Thus, it is not
possible to defect the fault using an array of length 1.
Exceptions. BigTest extends SPF to explicitly model exceptions.
For example, when an expression involves a division operator, divi-
sion by zero is possible, which can lead to program termination. In
Figure 1, BigTest creates two additional terminating path conditions,
due to division by zero (i.e., x<y‚àßx==0 andy‚â§x‚àßy==0 ).
Combining UDF symbolic execution with equivalence classes.
BigTest combines the path conditions of each UDF with the incom-
ing constraints from its upstream operator. For example, the UDF
offilter (‚ûå) in Section 3 produces a path condition of s._2 ==
"Palms" . Suppose that the upstream operator mapproduces one
non-terminating path condition s.split(",").length ‚â•2with
the effect s._2 =splitn(s,",",1) . Inside the equivalence classes
offilter ‚Äîrows ‚ù∂and‚ù∑in Table 2, BigTest plugs in the incoming
path conditions (/effects) of an upstream operator maptoCIand
the path conditions (/effects) of the filter ‚Äôs UDF to f, producing
the following path conditions.
‚Ä¢c(t)‚àßf(t):s.split(",").length ‚â•2‚àß
splitn (s,",",1) == "Palms"
‚Ä¢c(t)‚àß¬¨ f(t):s.split(",").length ‚â•2‚àß
¬¨(splitn (s,",",1) == "Palms")
Joint Dataflow and UDF Path. BigTest defines the final set of
paths of a DISC application as Joint Dataflow and UDF (JDU) paths.
We define a JDU path as follows: let G=(D,E)represent a directed
acyclic graph of a DISC application where Dis a set of vertices
representing dataflow operators and Erepresents directed edges
connecting dataflow operators. Imagine a DISC application con-
structed with a mapfollowed by filter andreduce . We represent
this dataflow graph as G=(D,E)such that D={d1,d2,d3,t1}
andE={(d1,d2),(d2,d3),(d2,t1)}where d1,d2, and d3aremap,
filter , and reduce respectively. filter introduces a terminating
edge(d2,t1)where a terminating vertex is t1.
Since each dataflow operator takes a user-defined function f, for
a vertex di, we define a subgraph Gi=(Vi,Ei)which represents
the control flow graph of f. In this subgraph, a vertex v‚ààVi
represents a program point and an edge (va,vb)‚ààEirepresents
the flow of control from vatovb.Gihasv1=start andvn=stop
corresponding to the first and last statements. Then from each
dataflow operator node di, we add a call edge from dito the start
node of Giand from the stop node of Gito the di+1. Since some
UDFs include a loop and thus have a cycle in the control flow graph,1(assert (= line2 (str.++ (str.++ line20 ",") line21)))
2(assert
3 (= line1
4 (str.++ (str.++ " " ",")
5 (str.++ (str.++ line11 ",")
6 (str.++ (str.++ " " ",") (str.++ (str.++ line13 ",")
line14))))))
7(assert
8 (and (not (= (str.to.int line14) 0))
9 (and (isinteger line14)
10 (and (isinteger line13)
11 (and (= "Palms" line21)
12 (and (= x11 line20)
13 (and (<= s21 15)
14 (and (<= s21 40) (and (= s21 x621) (and (= s1 x61) (=
s22 x622)))))))))))))))
15 (assert
16 (and (= x11 line11)
17 (and (= x12 (/ (str.to.int line13) (str.to.int line14)))
18 (and (= x61 x11)
19 (and (= x621 x12) (and (= x622 x42) (and (= x71 "walk") (=
x72 1))))))))))))
Figure 8: Output SMT query constructed by BigTest to reflect
JDU path constraint C11of Table 1 from motivating example.
we finitize the loop using a user provided bound Kand unroll the
loop Ktimes.
We enumerate a set of all unique paths PKfor the graph Gwith
expanded subgraphs and call each unique path a Joint Dataflow and
UDF (JDU) path . For an arbitrary test suite T, the JDU path coverage
is measured as a set of covered paths, PK(T)={p|p‚ààPK,‚àÉt‚àà
T and t|=Cp}where a test input tsatisfies the path condition Cp
of path p. Given a user-provided bound Kfor unrolling a loop, JDU
path coverage is|PK(T)|
|PK|.
4.4 Test Data Generation
BigTest rewrites path constraints into an SMT query. For constraints
on integer variables, BigTest uses analogous arithmetic and logical
operators available in SMT. For string constraints, BigTest uses
operations such as str.++ ,str.to.int , and str.at .BigTest in-
troduces a new splitn symbolic operation. If a path constraint con-
tains a clause v = splitn("," s,1) ,BigTest generates (assert
(= s (str.++ " " (str.++ "," v)))) that is equivalent to s =
" ,v"where vis a symbolic string. The path conditions produced
byBigTest do not contain arrays and instead model individual
elements of an array up to a given bound K.
BigTest generates interpreted functions for Java native methods
not supported by Z3. For example, BigTest replaces isInteger with
an analogous Z3 function. BigTest executes each SMT query sepa-
rately and finds satisfying assignments (i.e., test inputs) to exercise
a particular path. While executing each SMT query independently
may lead to redundant solving of overlapping constraints, in our
experiments, we do not find it as a performance bottleneck. Theoret-
ically, the number of path constraints increases exponentially due
to branches and loops; however, empirically, our approach scales
well to DISC applications, because UDFs tend to be much smaller
(in order of hundred lines) than DISC frameworks and we abstract
the framework implementation using logical specifications.
Figure 8 shows an SMT query produced by BigTest for Figure 2.
Lines 1 to 6 constrict the first table to have four segments and the
second table to have two segments separated by a comma. Lines 7 to
10 restrict a string to be a valid integer. To enforce such constraint
295White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia
Table 3: Subject Programs
Subject # of Program Characteristics JDU Paths#ProgramOutputOperatorsOperatorsString Parsing # Branches # UDFs (K=2)
P1 IncomeAggregate Total income of individuals earning ‚â§$300 weekly 3 map, filter, reduce ‚úì 2 4 6
P2 MovieRatings Total number of movies with rating ‚â•4 4 map, filter, reduceByKey ‚úì 1 4 5
P3 AirportLayover Total layer time of passengers per airport 3 map, filter, reduceByKey ‚úì 2 4 14
P4 CommuteTypeTotal number of people using each form of
transport for daily commute6map, fitler, join,
reduceByKey‚úì 3 5 11
P5 PigMix-L2 PigMix performance benchmark 5 map, join ‚úì 2 6 4
P6 Grade Analysis List of classes with more than 5 failing students 5flatmap, filter,
reduceByKey, map‚úì 2 3 30
P7 WordCount Finds the frequency of words 3 flatmap, map, reduceByKey ‚úì 1 3 4
P1 P2 P3 P4 P5 P6 P7020406080100
100
100
100
100
100
100
10016.7
40
14.3
18.2
25
13.3
2566.7
60
28.6
54.5
75
76.7
100JDU Path Co vera–¥e
(Normalized ,%)BigTest Sedge Original
Figure 9: JDU path coverage of BigTest ,Sedge , and the original
input dataset
that crosses the boundary of strings and integers, BigTest uses a
custom function isinteger and Z3 function str.to.int . Lines 11
to 14 enforce a record to contain ‚ÄúPalms‚Äù and the speed to be less
than or equal to 15. Lines 15 to 19 join these constraints generated
from a UDF to the subsequent dataflow operator.
5 EVALUATION
We evaluate the effectiveness and efficiency of BigTest using a
diverse set of benchmark DISC applications. We compare BigTest
against Sedge in terms of path coverage, fault detection capability,
and testing time. We compare test adequacy, input data size, and
potential time saving against three alternative testing methods: (1)
random sampling of k% records, and (2) using a subset of the first
k% records, and (3) testing on the entire original data.
‚Ä¢To what extent BigTest is applicable to DISC applications?
‚Ä¢How much test coverage improvement can BigTest achieve?
‚Ä¢How many faults can BigTest detect?
‚Ä¢How much test data reduction does BigTest provide?
‚Ä¢How long does BigTest take to generate test data?
Subject Programs. In terms of benchmark programs, we use seven
subject programs from earlier works on testing [ 31] and debugging
DISC applications [ 25,28], listed in Table 3. The PigMix bench-
mark package contains a data generator script that generates large
scale datasets. We utilize mapandflatmap with UDFs in Apache
Spark to translate unsupported Pig operators like load As and
split . Three programs MovieRating (P2), AirportLayover (P3),
andWordCount (P7) are adapted from BigSift [25]. Each program
is paired with a large scale dataset. The rest are self-created cus-
tom Apache Spark applications to add heterogeneity in dataflow
operators and UDFs. Table 3 shows detailed descriptions of subject
programs. All applications (1) involve complex string operations
including split ,substring , and toInt , (2) perform complex arith-
metics, (3) use type Tuple for key-value pairs, and (4) generate and
process a collection with custom logic using flatmap .
Experimental Environment. We run all large-scale data process-
ing on a 16-node cluster. Each node is running at 3.40GHz and
equipped with 4 cores, 32GB of RAM, and 1TB of storage allowingP1 P2 P3 P4 P5 P6 P7020406080100
100
100
100
100
100
100
10066.7
60
28.6
18.2
75
76.7
10066.7
60
28.6
18.2
75
76.7
100JDU Path Co vera–¥e
(Normalized ,%)BigTest Random Sample first 1% Data
Figure 10: JDU path coverage of BigTest in comparison to al-
ternative sampling methods
us to run up to 120 tasks simultaneously. For storage, we use HDFS
version 1.0.4 with a replication factor of 3. Due to a very small size
of test data generated by BigTest , we leverage Apache Spark‚Äôs local
running mode to perform experiments on a single machine.
5.1 Dataflow Program Support
BigTest supports a variety of dataflow operators prevalent in DISC
applications. For instance, Apache Spark provides flatmap and
reduceByKey for constructing and processing collections. The pre-
vious approach Sedge is designed for PIG Latin with only a limited
set of operators support [ 31].Sedge is neither open-source nor
have any implementation available for Apache Spark for direct
comparison. Therefore, we faithfully implement Sedge precisely
based on the technical details provided elsewhere [ 31]. We manu-
ally downgrade BigTest by removing symbolic execution for UDFs
and equivalence classes for certain operators to emulate Sedge . The
implementations of both Sedge and BigTest are publicly available1.
Out of seven benchmark applications written in Apache Spark, five
applications contain flatmap andreduceByKey , therefore, Sedge
is not able to generate testing data for these 5 applications.
5.2 Joint Dataflow and UDF Path Coverage
We evaluate code coverage of BigTest ,Sedge , and the original input
dataset based on JDU path coverage defined in Section 4.3.
JDU Path Coverage Evaluation. We compare BigTest with three
alternative sampling techniques: (1) random sampling of k% of the
original dataset, (2) selection of the first k% of the original dataset,
as developers often test DISC applications using head -n , and (3) a
prior approach Sedge . To keep consistency in our experiment setting,
we enumerate JDU paths for a given user-provided bound K and
measure how many of these paths are covered by each approach.
Figure 9 compares the test coverage from BigTest ,Sedge , and the
original dataset. Y axis represents the normalized JDU path coverage
ranging from 0% to 100%. Across seven subject programs, we ob-
serve that Sedge covers significantly fewer JDU paths (22% of what
1https://github.com/maligulzar/BigTest
296ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim
10‚àí110010110220304050
Lo–¥scale k(%)JDU Path Co vera–¥e
(Normalized ,%)k%Random Sample
First k %of Data
(a) JDU Path Coverage10‚àí110010110205101520
Lo–¥scale k(%)Test runnin –¥time(s)
k%Random Sample
First k %of Data
(b) Test Execution time
Figure 11: The number of JDU paths covered and the test exe-
cution time when k%of the data is randomly selected and the
f irst k %of data is selected for subject program CommuteType .
is covered by BigTest ). By not modelling the internal paths of UDFs,
Sedge fails to explore many JDU paths. Even when the complete
dataset is used, the JDU path coverage reaches only 66% of what
BigTest could achieve. The entire dataset achieves better coverage
than Sedge but it still lacks coverage compared to BigTest . In other
words, using the entire bigdata for testing does not necessarily
provide high test adequacy.
In Figure 10, both random 1% sample andfirst 1% sample provide
59% of what is covered by BigTest . We perform another experiment
to measure the impact of different sample sizes on JDU path cover-
age and test execution time. Figure 11a and Figure 11b present the
results on CommuteType . InCommuteType , the covered JDU paths
increases from two to six when the percentage of the selected data
increases from 0.1% to 50%. For those small samples, input tables
do not have matching keys to exercise downstream operators and
the time and distance columns may not have specific values to
exercise all internal paths of the UDF. In terms of running time, as
the sample size ( k) increases, the test execution time also increases
linearly (see Figure 11b in which x-axis is in log scale).
5.3 Fault Detection Capability
We evaluate BigTest ‚Äôs ability to detect faults by manually injecting
commonly occurring faults. Because DISC applications are rarely
open-sourced for data privacy reasons and there is no existing
benchmark of faulty DISC applications, we create a set of faulty
DISC applications by studying the characteristics of real world
DISC application bugs and injecting faults based on this study.
We carefully investigate Stack Overflow and Apache Spark Mail-
ing lists with keywords; Apache Spark exceptions, task errors, failures ,
and wrong outputs and inspect top 50 posts. Many errors are re-
lated to performance and configuration errors; thus, we filter out
those and analyze 23 posts related to coding errors. For each post,
we investigate the type of fault by reading the question, posted
code, error logs, answers, and accepted solutions. We categorize
our findings into seven common fault types:
(1)incorrect string offset: e.g., a user uses 1 instead of 0 as the start-
ing index in method substring and encounters StringIndex-
OutOfBoundsException [7].
(2)incorrect column selection: e.g., a user accesses a wrong col-
umn in a csv file and thus receives ArrayIndexOutOfBound-
sException [5].
(3)use of wrong delimiters:e.g., while splitting a string a user
uses "[ ]" instead of "\[\]", leading to a wrong output [8].Table 4: Fault detection capabilities of BigTest and Sedge
Subject program
P1 P2 P3 P4 P5 P6 P7
Seeded Faults 3 6 6 6 4 4 2
Detected by BigTest 3 6 6 6 4 4 2
Detected by Sedge 1 6 4 4 2 3 0
(4)incorrect branch conditions: e.g., a user places a wrong order
of control predicates, executing only one branch‚Äôs side [4].
(5)wrong join types: e.g., a user uses a wrong relational operator
such as cartesian join instead of inner join [3].
(6)swapping a key with a value: e.g., a user tries to join two
tables while the keys and values are interleaved [6].
(7)other common mutations such as incorrect arithmetic or
Boolean operator in UDFs.
When applicable, we inject one of each fault type in every ap-
plication. For example, fault types 1 and 3 could only be inserted
when substr orsplit method is used. When a fault type is appli-
cable to multiple locations, we select a location which is inspired
by and similar to the fault location in the corresponding StackOver-
flow/Mailing List post. For instance, for fault type (2) above, we
manually modify code to extract the first column instead of the
second as a key in line 4 of Figure 2. Similarly, for fault type (3), we
introduce fault by replacing the delimiter "," with ":". In total, our
benchmark comprises of 31 faulty DISC applications. While Sedge
is not designed to handle string constraints, the main goal of this
exercise is to justify the need to model UDFs and string constraints.
Sedge represents the internal UDFs as uninterpreted functions and,
therefore, is unable to model all internal UDF paths. Conversely,
BigTest treats UDFs as interpreted functions by representing them
symbolically and models all internal UDF paths (up to bound k)
which is crucial for high coverage testing of UDF‚Äôs internal.
Table 4 shows a comparison of fault detection by BigTest and
Sedge .BigTest detects 2X more injected faults than Sedge . For in-
stance, in application P4, BigTest detects 6 faults, whereas Sedge
detects 4 faults. Sedge uses concrete execution to model the UDF
exercising line 16 of Figure 2 only. Therefore, it is unable to find an
input for detecting fault at line 17 when the binary operator ">"
is replaced with "<"(i.e.,s._2._1>15 tos._2._1<15 ). Similarly,
when join in line 13 is changed to rightOuterJoin ,Sedge cannot
detect any difference in the output because the equivalence classes
do not model the terminating cases of join.
Table 5: Modelling terminating and non-terminating cases
Output from programApproach Test Input DataOriginal Faulty
BigTestTerminating CS100:41,01l
Non-terminating CS200:0,0,0,0,0,0CS200CS100
CS200
Alternative Non-terminating CS200:0,0,0,0,0,0 CS200 CS200
As another example, application P6 identifies courses with more
than 5 failing students. A faulty version of P6 replaces the filter
predicate count>5 tocount>0 to output courses with at least one
failing student. The original version of P6 uses mapandfilter to
parse each row and identify failing students, reduceByKey to count
the number of failing students, and uses filter to find courses with
more than 5 failing students. BigTest generates at least two records
to exercise both terminating and non-terminating cases of the last
filter ; thus, the original and faulty versions produce different
297White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia
P1 P2 P3 P4 P5 P6 P71011071013
6 514 1143064¬∑109
5.21¬∑1054.48¬∑1083.2¬∑1082.4¬∑108
4¬∑1071.11¬∑108#of input records(lo–¥scale)Minimal input data selected for maximal JDU coverage
Entire data
Figure 12: Reduction in the size of the testing data by BigTest
P1 P2 P3 P4 P5 P6 P7020406080100
15.5 13.759.9
29.724.252.6
2.2 2.9 4.2 6.4 4.2 5.31.8Test Runnin –¥Time(s) Entire data
Test data generated by BigTest
Figure 13: Test running time of entire data on large-scale
cluster vs. testing on local machine with BigTest
outcomes on this data. On the other hand, a record is generated to
exercise a non-terminating case only. Such data would produce the
same outcome for both the original and the faulty versions, unable
to detect the injected fault, as shown in Table 5.
5.4 Testing Data Reduction
Testing DISC applications on the entire dataset is expensive and
time-consuming. BigTest minimizes the size of the dataset, while
maintaining the same test coverage. It generates only a few data
records (in order of tens) to achieve the same JDU path coverage
as the entire production data. Four out of seven benchmarks have
an accompanied dataset, whereas the rest relies on a synthetic
dataset of around 20GB each. Figure 12 shows the comparison result.
In application P6, BigTest generates 30 rows of data to achieve
33% more JDU path coverage than the entire dataset of 40 million
records. In other words, BigTest produces testing data 106times
smaller than the original dataset. Across all benchmark applications,
BigTest generates data ranging from 5 to 30 rows. This is 105to 108
times smaller than the original dataset, showing the potential to
significantly reduce dataset size for local testing.
5.5 Time and Resource Saving
By minimizing test data without compromising JDU path coverage,
BigTest consequently reduces the test running time. The benefit of
a smaller test data is twofolds: (1) the amount of time required to
run a test case decreases, and (2) the amount of resources (worker
nodes, memory, disk space, etc.) for running tests also decreases.
We measure, on a single machine, the total running time by
BigTest and compare it with the testing time on a 16-node cluster
with the entire input dataset. We present a breakdown of the total
running time into test data generation vs. executing an application
on the generated data. Figure 13 represents the evaluation results.
In application P6, it takes 5.3 seconds on a single machine to test
with data from BigTest otherwise testing takes 387.2 CPU seconds
(24.2 seconds x 16 machines) on the entire dataset, which still lacks
complete JDU path coverage. Across the seven subject programs,P1 P2 P3 P4 P5 P6 P7020406080
10.67.374.2
23
817.6
4.78.44.470
16.6
4.112.3
2.93.7 3.8 3.5 3.9 3.8 3.8 2.6Runnin –¥Time(s) Constraint Generation
Constraint Solver
Test Execution
Figure 14: Breakdown of BigTest ‚Äôs running time
1 2 3 4 5100102104
De–¥ree of bound(K)#ofJDU PathsW ord Count Grades Anal ysis Income A –¥–¥re–¥ate
1 2 3 4 5101102103
De–¥ree of bound(K)Test Generation Time (s)
Figure 15: BigTest ‚Äôs performance when the degree of upper
bound (K) on loop iteration and collection size changes
BigTest improves the testing time by 194X, on average, compared
to testing with the entire dataset.
Figure 14 reports the complete breakdown of the total running
time of BigTest . The maximum test generation time observed is
70 seconds for Airport Layover (P3) in which 66 seconds are
consumed by constraint solving. This is because the resulting JDU
paths include integer arithmetics and complex string constraints
together. Solving such constraints that cross the boundaries of
different dimensions (integer arithmetics vs. string constraints) is
time consuming even after BigTest ‚Äôs optimizations. If we combine
both the test running time and test generation time and compare
BigTest with the testing time with the entire dataset, BigTest still
outperforms. In fact, BigTest still is 59X faster than testing on the
entire dataset.
5.6 Bounded Depth Exploration
BigTest takes a user-provided bound Kto bound the number of
times a loop is unrolled. We assess the impact of varying Kfrom
1 to 5 and present the results in Figure 15. At K=2, the number of
JDU paths for GradeAnalysis is 36. When Kis 3, BigTest generates
438 JDU paths. An exponential-like increase in the test generation
time can be seen across the subject program, as we increase K.
When K=2 in GradeAnalysis ,BigTest takes 12 seconds and with
K=3,BigTest takes 204 seconds. We empirically find K=2 to be a
reasonable upper bound for loop iteration to avoid path explosion.
5.7 Threats to Validity
As we manually seed faults in the benchmark applications, the loca-
tion of faults may introduce a bias in fault detection rate of BigTest
posing a threat to internal validity. However, as mentioned before,
most type of faults are only applicable to a single code location. If a
fault type is applicable to multiple locations, we then select the fault
location inspired by the corresponding StackOverflow/Mailing List
post. In case of external validity, our classification of DISC faults
may not be representative of all possible DISC application faults
out there, as the survey is based on 50 StackOverflow/mailing lists
298ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim
posts. Additionally, the selection of fault types in our evaluation
may be unfair to prior approaches. We attempt to mitigate this bias
by restricting the evaluation to top seven most commonly occurring
faults in DISC applications. To eliminate this threat in the future,
we plan to perform a large scale study on DISC application faults.
6 RELATED WORK
Testing Map-Reduce Programs. Csallner et al. propose the idea
of testing commutative and associative properties of Map-Reduce
programs by generating symbolic constraints [ 18]. Their goal is to
identify non-determinism in a Map-Reduce program arising from
a non-associative or non-commutative user-defined function in
thereduce operator. They produce counter examples as evidence
by running a constraint solver over symbolic path constraints. Xu
et al. add few more Map-Reduce program properties such as (1)
operator selectivity , (2)operator statefulness , and (3) partition interfer-
ence [45] . Both of these techniques test only high-level properties
of individual dataflow operators and they do not model the inter-
nal program paths of user-defined functions. Olsten et al. generate
data for Pig Latin programs [ 34]. Their approach considers each
operator in isolation and does not model internal program paths
of UDFs‚Äîtreated as black-box. Furthermore, Olsten et al. require
knowing the inverse function of a UDF given to transform .
Li et al. ( Sedge ) [31] is the most relevant approach to BigTest .
Sedge has three main limitations. First, its symbolic execution does
not analyze the internal paths of individual UDFs. It considers
UDFs as black box procedures and encodes them into uninterpreted
functions . Second, it does not support operators such as flatmap ,
reduce , and reducebyKey , which are essential for constructing
a collection and aggregating results from a collection in big data
analytics. Third, the equivalence class modeling for each dataflow
operator is not comprehensive, as it does not consider early termi-
nating cases for some operators, where a data record does not flow
to the next dataflow operator. Our empirical evaluation in Section 5
finds that these limitations lead to low defect detection in Sedge .
Table 6 compares dataflow operator support for related approaches
and shows that BigTest has the most comprehensive and advanced
support for modern DISC applications.
Test Generation in Databases. JDBC [ 41] or ODBC [ 2] enable
software developers to write applications that construct and exe-
cute database queries at runtime. Testing such programs requires
test inputs and database states from a user. Emmi et al. perform con-
colic execution of a program embedded with an SQL query [ 21] by
symbolically executing the program till the point where a query is
executed. Their approach is only applicable to basic SQL operations
such as projection, selection, etc. ( e.g.,SELECT, WHERE ). Braberman
et al. select input data to test the logic of computing additional fields
from existing columns in the database [ 13]. They do not handle
arbitrary UDFs which are prevalent in DISC applications.
Symbolic Execution. Symbolic execution is a widely used tech-
nique in software engineering [ 12,27,37] and is used to generate
test data using constraint solvers [ 14‚Äì16,23,32,33,40]. For ex-
ample, Visser et al. use JPF (Java PathFinder [ 29]) to generate test
input data [ 44]. However, the same approach cannot be applied to
DISC applications directly because it would symbolically execute
the application as well as the underlying DISC framework. Such
practice will produce an unnecessarily large number of complexTable 6: Support of dataflow operators in related work
Dataflow Operators Olston et al . Li et al . Emmi et al .Pan et al .BigTest
Load ‚úì ‚úì ‚úì ‚úì ‚úì
Map (Select) ‚úì ‚úì ‚úì ‚úì ‚úì
Map (Transform) Incomplete Incomplete ‚úó ‚úó ‚úì
Filter (Where) ‚úì ‚úì ‚úì ‚úì ‚úì
Group ‚úì ‚úì ‚úó ‚úó ‚úì
Join Incomplete Incomplete ‚úó Incomplete ‚úì
Union ‚úì ‚úì ‚úó ‚úó ‚úì
Flatmap (Split) ‚úó Incomplete ‚úó ‚úó ‚úì
Intersection ‚úó ‚úó ‚úó ‚úó ‚úì
Reduce ‚úó ‚úó ‚úó ‚úó ‚úì
path constraints, facing scalability issues. This justifies and moti-
vates our approach that abstracts dataflow operators as a logical
specifications while performing symbolic execution for the UDFs.
Rosette is a framework for designing a solver-aided language [ 42]
to ease the process of translating each language construct into sym-
bolic constraints. BigTest and Rosette both translate higher-order
types such as arrays into lower-level constraints. Bang et al. address
the problem of solving constraints crossing boundaries between
different theories (numerics, integer, and string constraints) [ 10].
Such cross-theory constraints are known to be difficult to solve
with Z3 or CVC4. They extend SPF by modeling strings into bit
vectors and by integrating numeric model counting in ABC [ 9]
which could be used for BigTest in the future.
Regression Testing. Regression testing has been extensively stud-
ied in software testing. Safe regression testing selects only those test
cases that exercise the updated regions of a program [ 26]. Rothermel
et al. summarize several regression testing techniques and evaluate
them under a controlled environment [ 38]. Test augmentation tech-
niques help developers generate new test data to cover code not ex-
ercised by the available test cases using symbolic execution [ 17,30].
Xu et al. evaluate concolic and genetic test generation approaches
and report trade-offs [ 46]. The aforementioned approaches are not
directly applicable to DISC applications, as they do not explicitly
model the combined behavior of dataflow (/relational) operators
and the internal semantics of UDFs.
7 CONCLUSION
Big data analytics are now prevalent in many domains. However,
software engineering methods for DISC applications are relatively
under-developed. To enable efficient and effective testing of big
data analytics in real world settings, we present a novel white-box
testing technique that systematically explores the combined behav-
ior of dataflow operators and corresponding UDFs. This technique
generates joint dataflow and UDF path constraints and leverages
theorem solvers to generate concrete test inputs. BigTest can de-
tect 2X more faults than the previous approach and can consume
194X less CPU time, on average than using the entire dataset. With
BigTest ,fastlocal testing is feasible and testing DISC applications
on the entire dataset may not be necessary.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their comments. The par-
ticipants of this research are in part supported by Google PhD
Fellowship, NSF grants CCF-1764077, CCF-1527923, CCF-1460325,
CCF-1723773, ONR grant N00014-18-1-2037, Intel CAPA grant, and
Samsung grant. We would also like to thank Emina Torlak and
Koushik Sen for their insightful discussions.
299White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia
REFERENCES
[1] [n.d.]. Hadoop. http://hadoop.apache.org/.
[2][n.d.]. Microsoft Open Database Connectivity (ODBC). https://msdn.microsoft.
com/en-us/library/ms710252(v=vs.85).aspx.
[3] 2015. . https://stackoverflow.com/questions/32190828.
[4] 2016. . https://stackoverflow.com/questions/40494999.
[5] 2017. . https://stackoverflow.com/questions/48021303.
[6] 2017. . https://stackoverflow.com/questions/42459749.
[7] 2018. . https://stackoverflow.com/questions/49505241.
[8] 2018. . https://stackoverflow.com/questions/52083828.
[9]Abdulbaki Aydin, Lucas Bang, and Tevfik Bultan. 2015. Automata-Based Model
Counting for String Constraints. In Computer Aided Verification , Daniel Kroening
and Corina S. PƒÉsƒÉreanu (Eds.). Springer International Publishing, Cham, 255‚Äì
272.
[10] Lucas Bang, Abdulbaki Aydin, Quoc-Sang Phan, Corina S. PƒÉsƒÉreanu, and Tevfik
Bultan. 2016. String Analysis for Side Channels with Segmented Oracles. In
Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foun-
dations of Software Engineering (FSE 2016) . ACM, New York, NY, USA, 193‚Äì204.
https://doi.org/10.1145/2950290.2950362
[11] Clark Barrett, Christopher L. Conway, Morgan Deters, Liana Hadarean, Dejan
Jovanovi‚Äôc, Tim King, Andrew Reynolds, and Cesare Tinelli. 2011. CVC4. In
Proceedings of the 23rd International Conference on Computer Aided Verification
(CAV ‚Äô11) (Lecture Notes in Computer Science) , Ganesh Gopalakrishnan and Shaz
Qadeer (Eds.), Vol. 6806. Springer, 171‚Äì177. http://www.cs.stanford.edu/~barrett/
pubs/BCD+11.pdf Snowbird, Utah.
[12] Robert S. Boyer, Bernard Elspas, and Karl N. Levitt. 1975. SELECT&Mdash;a
Formal System for Testing and Debugging Programs by Symbolic Execution. In
Proceedings of the International Conference on Reliable Software . ACM, New York,
NY, USA, 234‚Äì245. https://doi.org/10.1145/800027.808445
[13] V√≠ctor Braberman, Diego Garbervetsky, Javier Godoy, Sebastian Uchitel, Guido
de Caso, Ignacio Perez, and Santiago Perez. 2018. Testing and Validating End User
Programmed Calculated Fields. In Proceedings of the 2018 26th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations
of Software Engineering (ESEC/FSE 2018) . ACM, New York, NY, USA, 827‚Äì832.
https://doi.org/10.1145/3236024.3275531
[14] J. Burnim and K. Sen. 2008. Heuristics for Scalable Dynamic Test Generation. In
Proceedings of the 2008 23rd IEEE/ACM International Conference on Automated
Software Engineering (ASE ‚Äô08) . IEEE Computer Society, Washington, DC, USA,
443‚Äì446. https://doi.org/10.1109/ASE.2008.69
[15] Cristian Cadar and Dawson Engler. 2005. Execution Generated Test Cases: How
to Make Systems Code Crash Itself. In Proceedings of the 12th International Con-
ference on Model Checking Software (SPIN‚Äô05) . Springer-Verlag, Berlin, Heidelberg,
2‚Äì23. https://doi.org/10.1007/11537328_2
[16] Cristian Cadar, Vijay Ganesh, Peter M. Pawlowski, David L. Dill, and Dawson R.
Engler. 2006. EXE: Automatically Generating Inputs of Death. In Proceedings of
the 13th ACM Conference on Computer and Communications Security (CCS ‚Äô06) .
ACM, New York, NY, USA, 322‚Äì335. https://doi.org/10.1145/1180405.1180445
[17] Cristian Cadar, Patrice Godefroid, Sarfraz Khurshid, Corina S. PƒÉsƒÉreanu, Koushik
Sen, Nikolai Tillmann, and Willem Visser. 2011. Symbolic Execution for Software
Testing in Practice: Preliminary Assessment. In Proceedings of the 33rd Interna-
tional Conference on Software Engineering (ICSE ‚Äô11) . ACM, New York, NY, USA,
1066‚Äì1071. https://doi.org/10.1145/1985793.1985995
[18] Christoph Csallner, Leonidas Fegaras, and Chengkai Li. 2011. New Ideas
Track: Testing Mapreduce-style Programs. In Proceedings of the 19th ACM SIG-
SOFT Symposium and the 13th European Conference on Foundations of Soft-
ware Engineering (ESEC/FSE ‚Äô11) . ACM, New York, NY, USA, 504‚Äì507. https:
//doi.org/10.1145/2025113.2025204
[19] Leonardo De Moura and Nikolaj Bj√∏rner. 2008. Z3: An efficient SMT solver. In
International conference on Tools and Algorithms for the Construction and Analysis
of Systems . Springer, 337‚Äì340.
[20] Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing
on large clusters. Commun. ACM 51, 1 (2008), 107‚Äì113.
[21] Michael Emmi, Rupak Majumdar, and Koushik Sen. 2007. Dynamic Test Input
Generation for Database Applications. In Proceedings of the 2007 International
Symposium on Software Testing and Analysis (ISSTA ‚Äô07) . ACM, New York, NY,
USA, 151‚Äì162. https://doi.org/10.1145/1273463.1273484
[22] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI ‚Äô05) . ACM, New York,
NY, USA, 213‚Äì223. https://doi.org/10.1145/1065010.1065036
[23] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI ‚Äô05) . ACM, New York,
NY, USA, 213‚Äì223. https://doi.org/10.1145/1065010.1065036
[24] Patrice Godefroid, Michael Y. Levin, and David A Molnar. 2008. Automated White-
box Fuzz Testing. In Network Distributed Security Symposium (NDSS) . Internet
Society. http://www.truststc.org/pubs/499.html[25] Muhammad Ali Gulzar, Matteo Interlandi, Xueyuan Han, Mingda Li, Tyson
Condie, and Miryung Kim. 2017. Automated Debugging in Data-intensive Scal-
able Computing. In Proceedings of the 2017 Symposium on Cloud Computing (SoCC
‚Äô17). ACM, New York, NY, USA, 520‚Äì534. https://doi.org/10.1145/3127479.3131624
[26] Mary Jean Harrold, James A. Jones, Tongyu Li, Donglin Liang, Alessandro Orso,
Maikel Pennings, Saurabh Sinha, S. Alexander Spoon, and Ashish Gujarathi.
2001. Regression Test Selection for Java Software. In Proceedings of the 16th
ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages,
and Applications (OOPSLA ‚Äô01) . ACM, New York, NY, USA, 312‚Äì326. https:
//doi.org/10.1145/504282.504305
[27] W. E. Howden. 1977. Symbolic Testing and the DISSECT Symbolic Evaluation
System. IEEE Trans. Softw. Eng. 3, 4 (July 1977), 266‚Äì278. https://doi.org/10.1109/
TSE.1977.231144
[28] Matteo Interlandi, Ari Ekmekji, Kshitij Shah, Muhammad Ali Gulzar, Sai Deep
Tetali, Miryung Kim, Todd Millstein, and Tyson Condie. 2018. Adding data
provenance support to Apache Spark. The VLDB Journal 27, 5 (01 Oct 2018),
595‚Äì615. https://doi.org/10.1007/s00778-017-0474-5
[29] Sarfraz Khurshid, Corina S. PƒÉsƒÉreanu, and Willem Visser. 2003. Generalized
Symbolic Execution for Model Checking and Testing. In Proceedings of the 9th
International Conference on Tools and Algorithms for the Construction and Analysis
of Systems (TACAS‚Äô03) . Springer-Verlag, Berlin, Heidelberg, 553‚Äì568. http://dl.
acm.org/citation.cfm?id=1765871.1765924
[30] James C. King. 1976. Symbolic Execution and Program Testing. Commun. ACM
19, 7 (July 1976), 385‚Äì394. https://doi.org/10.1145/360248.360252
[31] Kaituo Li, Christoph Reichenbach, Yannis Smaragdakis, Yanlei Diao, and
Christoph Csallner. 2013. SEDGE: Symbolic example data generation for dataflow
programs. In Automated Software Engineering (ASE), 2013 IEEE/ACM 28th Inter-
national Conference on . IEEE, 235‚Äì245.
[32] Rupak Majumdar and Koushik Sen. 2007. Hybrid Concolic Testing. In Proceedings
of the 29th International Conference on Software Engineering (ICSE ‚Äô07) . IEEE
Computer Society, Washington, DC, USA, 416‚Äì426. https://doi.org/10.1109/ICSE.
2007.41
[33] David Molnar, Xue Cong Li, and David A. Wagner. 2009. Dynamic Test Gener-
ation to Find Integer Bugs in x86 Binary Linux Programs. In Proceedings of the
18th Conference on USENIX Security Symposium (SSYM‚Äô09) . USENIX Association,
Berkeley, CA, USA, 67‚Äì82. http://dl.acm.org/citation.cfm?id=1855768.1855773
[34] Christopher Olston, Shubham Chopra, and Utkarsh Srivastava. 2009. Generating
Example Data for Dataflow Programs. In Proceedings of the 2009 ACM SIGMOD
International Conference on Management of Data (SIGMOD ‚Äô09) . ACM, New York,
NY, USA, 245‚Äì256. https://doi.org/10.1145/1559845.1559873
[35] K. Ouaknine, M. Carey, and S. Kirkpatrick. 2015. The PigMix Benchmark on Pig,
MapReduce, and HPCC Systems. In 2015 IEEE International Congress on Big Data .
643‚Äì648. https://doi.org/10.1109/BigDataCongress.2015.99
[36] Corina S. P ÀáasÀáareanu, Peter C. Mehlitz, David H. Bushnell, Karen Gundy-Burlet,
Michael Lowry, Suzette Person, and Mark Pape. 2008. Combining Unit-level
Symbolic Execution and System-level Concrete Execution for Testing Nasa Soft-
ware. In Proceedings of the 2008 International Symposium on Software Testing and
Analysis (ISSTA ‚Äô08) . ACM, New York, NY, USA, 15‚Äì26. https://doi.org/10.1145/
1390630.1390635
[37] C. V. Ramamoorthy, S. B. F. Ho, and W. T. Chen. 1976. On the Automated
Generation of Program Test Data. IEEE Trans. Softw. Eng. 2, 4 (July 1976), 293‚Äì300.
https://doi.org/10.1109/TSE.1976.233835
[38] Gregg Rothermel and Mary Jean Harrold. 1996. Analyzing Regression Test
Selection Techniques. IEEE Trans. Softw. Eng. 22, 8 (Aug. 1996), 529‚Äì551. https:
//doi.org/10.1109/32.536955
[39] Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit
Testing Engine for C. In Proceedings of the 10th European Software Engineering
Conference Held Jointly with 13th ACM SIGSOFT International Symposium on
Foundations of Software Engineering (ESEC/FSE-13) . ACM, New York, NY, USA,
263‚Äì272. https://doi.org/10.1145/1081706.1081750
[40] Matt Staats and Corina P ÀáasÀáareanu. 2010. Parallel Symbolic Execution for Struc-
tural Test Generation. In Proceedings of the 19th International Symposium on
Software Testing and Analysis (ISSTA ‚Äô10) . ACM, New York, NY, USA, 183‚Äì194.
https://doi.org/10.1145/1831708.1831732
[41] Art Taylor. 2002. Jdbc: Database Programming with J2Ee with Cdrom . Prentice
Hall Professional Technical Reference.
[42] Emina Torlak and Rastislav Bodik. 2014. A Lightweight Symbolic Virtual Machine
for Solver-aided Host Languages. In Proceedings of the 35th ACM SIGPLAN Con-
ference on Programming Language Design and Implementation (PLDI ‚Äô14) . ACM,
New York, NY, USA, 530‚Äì541. https://doi.org/10.1145/2594291.2594340
[43] Willem Visser, Klaus Havelund, Guillaume Brat, Seungjoon Park, and Flavio
Lerda. 2003. Model Checking Programs. Automated Software Engg. 10, 2 (April
2003), 203‚Äì232. https://doi.org/10.1023/A:1022920129859
[44] Willem Visser, Corina S. P ÀáasÀáareanu, and Sarfraz Khurshid. 2004. Test Input Gener-
ation with Java PathFinder. In Proceedings of the 2004 ACM SIGSOFT International
Symposium on Software Testing and Analysis (ISSTA ‚Äô04) . ACM, New York, NY,
USA, 97‚Äì107. https://doi.org/10.1145/1007512.1007526
300ESEC/FSE ‚Äô19, August 26‚Äì30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim
[45] Z. Xu, M. Hirzel, G. Rothermel, and K. L. Wu. 2013. Testing properties of dataflow
program operators. In 2013 28th IEEE/ACM International Conference on Automated
Software Engineering (ASE) . 103‚Äì113. https://doi.org/10.1109/ASE.2013.6693071
[46] Zhihong Xu, Yunho Kim, Moonzoo Kim, Gregg Rothermel, and Myra B. Co-
hen. 2010. Directed Test Suite Augmentation: Techniques and Tradeoffs. In
Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foun-
dations of Software Engineering (FSE ‚Äô10) . ACM, New York, NY, USA, 257‚Äì266.
https://doi.org/10.1145/1882291.1882330
[47] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma,
Murphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Re-
silient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster
Computing. In Proceedings of the 9th USENIX Conference on Networked SystemsDesign and Implementation (NSDI‚Äô12) . USENIX Association, Berkeley, CA, USA,
2‚Äì2. http://dl.acm.org/citation.cfm?id=2228298.2228301
[48] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and
Ion Stoica. 2010. Spark: Cluster Computing with Working Sets. In Proceedings
of the 2Nd USENIX Conference on Hot Topics in Cloud Computing (HotCloud‚Äô10) .
USENIX Association, Berkeley, CA, USA, 10‚Äì10. http://dl.acm.org/citation.cfm?
id=1863103.1863113
[49] Hucheng Zhou, Jian-Guang Lou, Hongyu Zhang, Haibo Lin, Haoxiang Lin, and
Tingting Qin. 2015. An Empirical Study on Quality Issues of Production Big
Data Platform. In Proceedings of the 37th International Conference on Software
Engineering - Volume 2 (ICSE ‚Äô15) . IEEE Press, Piscataway, NJ, USA, 17‚Äì26. http:
//dl.acm.org/citation.cfm?id=2819009.2819014
301