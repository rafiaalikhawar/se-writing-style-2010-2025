Learning to Log: Helping Developers Make
Informed Logging Decisions
Jieming Zhu‚Ä†, Pinjia He‚Ä†, Qiang Fu¬ß, Hongyu Zhang‚Ä°, Michael R. Lyu‚Ä†, Dongmei Zhang‚Ä°
‚Ä†Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China
‚Ä†Ministry of Education Key Laboratory of High ConÔ¨Ådence Software Technologies (CUHK Sub-Lab)
‚Ä†Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong
¬ßMicrosoft, Washington DC, USA‚Ä°Microsoft Research, Beijing, China
‚Ä†{jmzhu, pjhe, lyu}@cse.cuhk.edu.hk¬ßqifu@microsoft.com‚Ä°{honzhang, dongmeiz}@microsoft.com
Abstract ‚ÄîLogging is a common programming practice of prac-
tical importance to collect system runtime information for post-
mortem analysis. Strategic logging placement is desired to covernecessary runtime information without incurring unintendedconsequences (e.g., performance overhead, trivial logs). However,in current practice, there is a lack of rigorous speciÔ¨Åcationsfor developers to govern their logging behaviours. Logging hasbecome an important yet tough decision which mostly dependson the domain knowledge of developers. To reduce the effort onmaking logging decisions, in this paper, we propose a ‚Äúlearningto log‚Äù framework, which aims to provide informative guidanceon logging during development. As a proof of concept, weprovide the design and implementation of a logging suggestiontool, LogAdvisor, which automatically learns the common logging
practices on where to log from existing logging instances and
further leverages them for actionable suggestions to developers.SpeciÔ¨Åcally, we identify the important factors for determiningwhere to log and extract them as structural features, textual
features, and syntactic features. Then, by applying machinelearning techniques (e.g., feature selection and classiÔ¨Åer learning)and noise handling techniques, we achieve high accuracy oflogging suggestions. We evaluate LogAdvisor on two industrial
software systems from Microsoft and two open-source softwaresystems from GitHub (totally 19.1M LOC and 100.6K loggingstatements). The encouraging experimental results, as well as auser study, demonstrate the feasibility and effectiveness of ourlogging suggestion tool. We believe our work can serve as animportant Ô¨Årst step towards the goal of ‚Äúlearning to log‚Äù.
I. I NTRODUCTION
Logging is a common programming practice in software
development, typically issued by inserting logging statements
(e.g. ,printf (),Console.Writeline()) in source code. As in-
house debugging tools (e.g. , debugger), all too often, are
inapplicable in production settings, logging has become aprincipal way to record the key runtime information (e.g. ,
states, events) of software systems into logs for postmortemanalysis. To facilitate such log analysis, the underlying loggingthat directly determines the quality of collected logs is a matterof vital importance.
Due to the criticality of logging, it would be bad to log
too little, which may miss the runtime information necessaryfor postmortem analysis. For example, systems may fail inthe Ô¨Åeld without any evidence from logs, thus signiÔ¨Åcantlyincreasing the difÔ¨Åculty in failure diagnosis [43]. However, itis also not the case that the more logging, the better. As thepractical experiences reported in [4], [13], logging too muchcan yield many problems too. First, logging means more code,which takes time to write and maintain. Furthermore, loggingconsumes additional system resources (e.g. , CPU and I/O) and
can have noticeable performance impact on system operation,for example, when writing thousands of lines to a log Ô¨Åle persecond [4]. Most importantly, excessive logging can producenumerous trivial and useless logs that eventually mask thetruly important information, thus making it difÔ¨Åcult to locatethe real issue [13]. As a result, strategic logging placementis desired to record runtime information of interest yet notcausing unintended consequences.
To achieve so, developers need to make informed logging
decisions. However, in our previous developer survey [26], wefound that even in a leading software company like Microsoft,it is difÔ¨Åcult to Ô¨Ånd rigorous (i.e., thorough and complete)speciÔ¨Åcations for developers to guide their logging behaviors.Although we found a number of online blog posts (e.g. , [1],
[2], [3], [4], [9], [11]) sharing best logging practices ofdevelopers with deep domain expertise, they are usually high-level and application-speciÔ¨Åc guidelines. Even with loggingframeworks (e.g ., Microsoft‚Äôs ULS [12] and Apache‚Äôs log4net)
provided, developers still need to make their own decisions onwhere to log and what to log, which in most cases dependon their own domain knowledge. Therefore, logging hasbecome an important yet tough decision during development,especially for new developers without much domain expertise.
Current research has seldom focused on studying how to
help developers make such logging decisions. To bridge thisgap, in this paper, we propose a ‚Äúlearning to log‚Äù frame-work, which aims to automatically learn the common logging‚Äúrules‚Äù (e.g ., where to log, what to log) from existing logging
instances, and further leverage them to provide informativeguidance for new development. Motivated by our observations(detailed in Section II-B), we extract a set of contextualfeatures from the source code to construct a learning modelfor predicting where to log. Our logging suggestion tool builton this model, named LogAdvisor, can thus provide actionable
suggestions for developers and reduce their effort on logging.As an initial step towards ‚Äúlearning to log‚Äù, this paper focuseson studying where to log (or more speciÔ¨Åcally, whether to loga focused code snippet), while leaving other aspects (such aswhat to log) of this research for future work.
We have conducted both within-project evaluation and
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.60415
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.60415
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.60415
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. TABLE I
SUMMARY OF THE STUDIED SOFTW ARE SYSTEMS (SOME ENTRIES ARE ANONYMIZED FOR CONFIDENTIALITY )
#Logging #CommitsSoftware StartDescription V ersion LOC Logging LOC of #Commits #PatchesSystems TimeStatements LoggingTotalwith Logging with Logging
System-A ‚àí Online service ‚àí 2.5M 23,624 77,945 ‚àí ‚àí ‚àí
System-B ‚àí Online service ‚àí 12.7M 69,057 240,395 ‚àí ‚àí ‚àí
SharpDevelop 2001 .NET platform IDE 5.0.2 1.4M 2,896 9,261 13,886 4,593 (33.1%) 724 (15.8%)
MonoDevelop 2003 Cross-platform IDE 4.3.3 2.5M 4,996 13,043 29,357 9,437 (32.1%) 1,157 (12.3%)
Total 19.1M 100.6K 327.6K 43.2K 14.0K (32.4%) 1.9K (13.6%)
cross-project evaluation on LogAdvisor using two industrial
software systems from Microsoft and two open-source soft-
ware systems from GitHub. Additionally, a user study isperformed to evaluate whether the suggestions provided byLogAdvisor can help developers in practice. The compre-
hensive evaluation results have demonstrated the feasibilityand effectiveness of our logging suggestion tool. For ease ofreproducing and applying our approach to future research, werelease our source code and detailed study materials ( e.g., data,
questionnaire) on our project page
1.
The rest of this paper is organized as follows. Section II
introduces our studied software systems and the motivation ofthis work. Section III provides the overview and the detailedtechniques of learning to log. Section IV reports the evaluationresults, and Section V presents our user study. We discuss thelimitations in Section VI and the related work in Section VII.Finally, we conclude this paper in Section VIII.
II. O
BSERV A TIONS AND MOTIV A TION
In this section, we Ô¨Årst introduce the subject software
systems under study. Then we provide some key observationson logging practices and present the motivation of our study.
A. Subject Software Systems
In our study, we investigate four large software systems,
including two industrial systems from Microsoft (denoted as
System-A and System-B for conÔ¨Ådentiality) and two open-source systems from GitHub (SharpDevelop and MonoDe-velop). Each of these systems contains millions of lines ofcode (LOC) written in C# language. Table I provides thesummary information of our studied software systems. Bothindustrial systems are online service systems developed byMicrosoft, serving a huge number of users globally. Thesetwo systems were also used as subjects in our empiricalstudy on logging practices [26]. To allow for reproducing andapplying our approach to future research, we choose anothertwo open-source software systems as subjects. They are twoIDE projects: SharpDevelop (supporting .NET platform) andMonoDevelop (supporting cross-platform development). Bothof them are selected due to their popularity (well-known C#projects), active updates (10000+ commits) and long historyof development (10+ years).
Our targeted systems are supposed to have reasonably good
logging implementation, because the produced logs by these
1http://cuhk-cse.github.io/LogAdvisorsystems have mostly met the requirements of usage analysis,troubleshooting, and operating, after undergoing more than 10years of evolution. This is especially true for the industrialsoftware systems, because each of them is implemented by agroup of experienced developers at Microsoft, where the codequality has been strictly controlled. Consequently, the sourcecode of these software systems is well suited for our studyon logging practices. All of our code analysis is conductedbased on an open-source C# code analysis tool, Roslyn [10].
By using Roslyn, we can perform both syntax analysis andsemantic analysis on the source code.
B. Observations
1)Pervasiveness of logging: Logging is pervasively used
in software development. As shown in Table I, our studied
systems have a total of 100.6K logging statements (containing327.6K lines of logging code) out of 19.1M LOC. That is,there is a line of logging code in every 58 LOC, as similarlyreported in [42], [44]. By drilling down according to thetype of software entities, we Ô¨Ånd that about 17.4% of thesource Ô¨Åles, 14.4% of the classes, 7.7% of the methods,and 25.3% of the catch blocks are logged respectively. Inaddition, by examining the revision histories of the systems,we Ô¨Ånd that, on average, 32.4% of the commits involvelogging modiÔ¨Åcations, and further, 13.6% of them are modiÔ¨Åedalong with patches
2. Both its pervasive existence and active
modiÔ¨Åcations reveal that logging plays an indispensable rolein software development and maintenance.
2)Where to log: The logging decisions can resolve to
where to log and what to log. Where to log determines the loc-
ations to place logging statements, while what to log denotes
the contents recorded by these logging statements. Whereasthe goal of ‚Äúlearning to log‚Äù is to handle them both, we studywhere to log in this paper. Our previous empirical study onwhere developers log [26] has shown that there are sometypical categories of logging strategies for recording errorsites and execution paths. Error sites indicate some unexpectedsituations where the system potentially runs into a problem,including exceptions and function-return errors. As two typicalways for error reporting, exception mechanisms are widelyused in modern programming languages (e.g. , C#) to handle
abnormal situations, and function-return errors indicate thesituation where an unexpected value (e.g. , -1/null/false/empty)
2We identify patches by searching commit logs for keywords such as ‚ÄúÔ¨Åx‚Äù,
‚Äúbug‚Äù, ‚Äúcrash‚Äù or issue ID like ‚Äú#42233‚Äù, the same as in [30].
416
416
416
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. TABLE II
LOGGING STA TISTICS
Software Exception Snippets Return-value-check Snippets
Systems #Exception Types #Instances #Logged Instances #Call Types #Instances #Logged Instances
System-A 188 7,580 3,320 (43.8%) 5,400 43,443 5,127 (13.5%)
System-B 1,657 25,441 5,307 (20.9%) 21,624 131,870 15,081 (11.4%)
SharpDevelop 106 1,346 252 (18.7%) 3,221 17,937 476 (2.7%)
MonoDevelop 220 4,041 771 (19.1%) 5,821 37,360 750 (2.0%)
Total 38.4K 9.7K (25.3%) 230.6K 21.4K (9.3%)
(a) Exception Snippet
(c) Extracted Contextual Features from Exception Snippet in (a)6\QWDFWLFIHDWXUHV
6HW/RJLF)ODJ (PSW\&DWFK%ORFN
7KURZ 2WKHU2SHUDWLRQ
5HWXUQ /2&
5HFRYHU)ODJ 1XP2I0HWKRGV7H[WXDOIHDWXUHV
([FHSWLRQW\SHPHWKRGV JHQGDUPHVHWWLQJVORDG
UXOHVIURPDVVHPEO\V\VWHPLRSDWKJHWIXOOSDWKUHIOHFWLRQQDPHILOHQRW 
IRXQGH[FHSWLRQ9DLDEOHV DQDPHDVVHPEO\D &RPPHQWV 1$
/DEHO /RJJHG6WUXFWXUDOIHDWXUHV
([FHSWLRQ7\SH 6\VWHP,2)LOH1RW)RXQG([FHSWLRQ
&RQWDLQLQJPHWKRG *HQGDUPH6HWWLQJV/RDG5XOHV)URP$VVHPEO\
,QYRNHGPHWKRGV6\VWHP,23DWK*HW)XOO3DWK6\VWHP5HIOHFWLRQ$VVHPEO\1DPH*HW$VVHPEO\1DPH6\VWHP5HIOHFWLRQ$VVHPEO\/RDG$FRGHH[DPSOHWDNHQIURP0RQR'HYHORSYDWILOHPDLQ?H[WHUQDO?PRQRWRROV?JHQGDUPH?FRQVROH?6HWWLQJVFVOLQH6RPHOLQHVDUHRPLWWHGIRUHDVHRISUHVHQWDWLRQ
SULYDWHLQW/RDG5XOHV)URP$VVHPEO\ VWULQJDVVHPEO\ 
^
$VVHPEO\D  QXOO
WU\^
$VVHPEO\1DPHDQDPH  $VVHPEO\1DPH*HW$VVHPEO\1DPH
3DWK *HW)XOO3DWKDVVHPEO\
D $VVHPEO\ /RDGDQDPH
`
FDWFK )LOH1RW)RXQG([FHSWLRQ^
&RQVROH(UURU:ULWH/LQH&RXOGQRWORDGUXOHV
IURPDVVHPEO\
^`
 DVVHPEO\
UHWXUQ
`
`
$FRGHH[DPSOHWDNHQIURP0RQR'HYHORSYDWILOH
PDLQ?VUF?FRUH?0RQR'HYHORS,GH?0RQR'HYHORS,GH?,PDJH6HUYLFHF V
OLQH&RQYHUWVDQLPDJHVSHFLQWRDUHDOVWRFNLFRQLGVWULQJVWRFNLG *HW6WRFN,G)RU,PDJH6SHFQDPHVL]HLIVWULQJ ,V1XOO2U(PSW\VWRFNLG^
/RJJLQJ6HUYLFH/RJ:DUQLQJ &DQ
WJHWVWRFNLGIRU  QDPH
  (QYLURQPHQW 1HZ/LQH (QYLURQPHQW 6WDFN7UDFH
UHWXUQ &UHDWH&RORU%ORFN)) VL]H
`
(b) Return-value-check Snippet
Fig. 1. Code Examples and Contextual Features
is returned from a function call. We denote their associatedcode snippets as exception snippets and return-value-checksnippets respectively (as examples shown in Fig. 1(a)(b)).They are the two most common logging strategies [26] andthus become our focused code snippets. Although recording
information of execution path is crucial for tracking down rootcauses from the error sites, existing studies ( e.g., control-Ô¨Çow
instrumentation [23], [35]) have been conducted to achievethis goal, which are orthogonal to our work.
3)Why not log everything: Log information is immensely
useful in maintaining software systems. So the question ‚Äúwhynot log everything?‚Äù (e.g. , StackOverÔ¨Çow questions [5], [7])does sound reasonable. Y uan et al. also proposed conser-vative logging (ErrLog) [43], which logs all the genericexceptions (e.g. , exceptions and function-return errors) for
failure diagnosis. However, as the logging statistics shownin Table II, we observed that, in our studied systems, themajority of exceptions (74.7% on average) and return-value-check snippets (90.7% on average) are actually not logged.To understand this fact, we posted our questions on ‚Äúwhynot log all exceptions?‚Äù to the mail lists and websites ofMonoDevelop [14], SharpDevelp [15] and StackOverÔ¨Çow [7],and received some valuable feedback from the developers.According to their feedback, ‚Äúlogging all exceptions wouldproduce a ton of garbage and make it hard to zoom in on realissues‚Äù, which conforms with our argument (not logging toomuch). There are many reasons for not logging an exception.Some exceptions are ‚Äúexpected in normal operation‚Äù, whilesome others are satisfactorily handled or ‚Äúrecovered withoutimpacting the user‚Äù. In a word, not all exceptions are ‚Äúunex-pected‚Äù (or errors) [4]. Strategic logging needs to ‚Äúdeterminewhether or not an exception is worth reporting‚Äù [6].
4)Logging decision and the context: To understand this
tradeoff in practice, we attempt to study how developers makedecisions on whether to log a focused code snippet. Fig. 1(a)presents a real-world example of an exception snippet (i.e.,try-catch block). The operations enclosed in the try blockattempt to load the rules from the input string, ‚Äúassembly‚Äù.If this assembly Ô¨Åle cannot be found, an exception with typeof ‚ÄúFileNotFoundException‚Äù will occur and then be caughtby the catch block. Here, the exception has been logged withan error message by ‚ÄúConsole.Error.WriteLine()‚Äù. Intuitively,from this example, we can see that the logging decision ishighly dependent on the context of this code snippet, includingtheexception type (e.g. , FileNotFoundException), the invoked
methods (e.g. , GetFullPath, GetAssemblyName, Load) in a
try block, etc. The contextual information is crucial becauseeach exception type generally denotes one speciÔ¨Åc type ofexceptional conditions while the invoked methods indicatethe functionality of operations. Driven by this intuition, wemeasure the logging ratio of each exception type and eachmethod. SpeciÔ¨Åcally, the logging ratio, with an exception type(or an invoked method) is measured by the number of loggedexceptions divided by the number of all the exception snippetswith this exception type (or containing this method). Theresults show that a signiÔ¨Åcant portion of exception types (82%)and methods (86%) have either high (> 80%) or low (< 20%)
logging ratios, which suggests their high correlations (i.e.,
417
417
417
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. either positive or negative correlations) with logging decisions
of developers.
C. Motivation
With the ever growing scale and complexity of software sys-
tems, it is common that each developer is only responsible for
a part of a system (e.g. , one or several components). Logging
under this situation is notoriously challenging, because deve-lopers may not have full knowledge of the whole system. Forexample, in our user study (Section V), 68% of the participantshave logging difÔ¨Åculties. However, there is a lack of rigorousspeciÔ¨Åcations or tool support for developers to aid their log-ging decisions. Without a well-structured logging strategy, it isdifÔ¨Åcult for developers to know how to make informed loggingdecisions, and thus, quite often, the decisions are made basedon their own domain knowledge (e.g. , understanding of system
behaviours, logging experience). Such domain knowledge isseldom documented and it is also hard to do so, since thelogging behaviours of developers may vary widely, not onlyfrom project to project, but also from developer to developer.Indeed, the pervasively-existing logging instances together canprovide strong indication of the developers‚Äô domain knowledgeembedded with their logging decisions. Thus, we intend toexplore whether the logging decisions of developers, such aswhere to log, can be learnt automatically from these existinglogging instances. If so, the constructed model can representthe common knowledge of logging and be further built intotool support to provide valuable suggestions (e.g. , whether
to log an exception snippet) for developers. Such a tool canimprove the logging quality as well as reduce the effort ofdevelopers. Following this motivation, we propose ‚Äúlearningto log‚Äù.
III. L
EARNING TO LOG
In this section, we present the overview as well as the
detailed techniques of ‚Äúlearning to log‚Äù.
A. Overview
Our goal, referred to as ‚Äúlearning to log‚Äù, is to automatically
learn the common logging practice as a machine learning
model, and then leverage the model to guide developers tomake logging decisions during new development. We furtherimplement the proposed ‚Äúlearning to log‚Äù approach as a tool,LogAdvisor. Fig. 2 presents the overview of ‚Äúlearning to log‚Äù,which can be described as the following steps:
1)Instances collection: As the Ô¨Årst step, we need to
extract data instances (focused code snippets) from our tar-get projects. There are two types of frequently-logged codesnippets: exception snippets and return-value-check snippets.As shown in Fig. 1(a) and Fig. 1(b), exception loggingrecords the exception context (e.g. , exception message) after
an exception is captured in the catch block, while return-value-check logging is used to log the situation where an unexpectedvalue (e.g. , -1/null/false/empty) is returned from a function
call. By employing Roslyn, we extract all these focused codesnippets, and use them as training data to learn the loggingpractices of developers.
Feature
ExtractionFeature
SelectionModel
ConstructionSoftware
RepositoriesFocused Code
Snippets
Instances
Collection
Logged
Instances
Unlogged
InstancesContextual
Features
Feature
Vectors
Label
IdentificationFeature
VectorPredictive
Model
C
o
n
t
e
x
t
u
a
New
Instance
Logging
SuggestionDeveloper
Log?
(1) (2) (3) (4) (5) (6)Contextual
Features
Logging Suggestion Tool (LogAdviosr)
Fig. 2. The Overview of Learning to Log
2)Label identiÔ¨Åcation: As a key step of preparing training
data, each data instance (a code snippet) is labelled ‚Äúlogged‚Äùif it contains a logging statement; or ‚Äúunlogged‚Äù, otherwise. Alogging statement denotes a statement that has an invocationto a logging method (e.g. ,Console.Writeline()). We identify
logging methods by searching some keywords in all methodnames, such as log/logging, trace, write/writeline, etc. The
logging statement identiÔ¨Åcation and labelling procedures areautomatically performed based on Roslyn.
3)F eature extraction: In our study, we need to extract
useful features (e.g., exception type) from the collected codesnippets for making logging decisions, which is one of themost important steps to determine the performance of the pre-diction model. The details on feature extraction are describedin Section III-B.
4)F eature selection: When there are too many features,
some of them are likely redundant or irrelevant since theyprovide little useful information or even act as noises todegrade the prediction performance. Feature selection [28] is akey technique to remove such redundant or irrelevant featuresto enhance the prediction performance as well as shorten thetraining time.
5)Model training: Through feature extraction and selec-
tion, we can generate a corpus of feature vectors, whereeach denotes a vector of feature values from a data instance.With these feature vectors and their corresponding labels, wecan apply a set of machine learning models (e.g. , Decision
Tree [41]) to learn the common logging practice. In our study,we learn the decision on whether to log a focused code snippetas a classiÔ¨Åcation model.
6)Logging suggestion: Through the above learning pro-
cess, we can obtain a predictive model to perform accuratelogging predictions. This predictive model can be trainedofÔ¨Çine and further be built into a logging support tool (namelyLogAdvisor ) to provide online logging suggestions for deve-
lopers. For example, when a developer composes a new pieceof code containing a try-catch block, LogAdvisor can detect
and extract its feature vector in a transparent way. ThenLogAdvisor can predict on whether to log, and provide alogging suggestion for the developer through IDE (e.g. , like
the warning message). By using LogAdvisor, developers can
make informed logging decisions.
The above learning workÔ¨Çow is generic and works similarly
to many other machine learning applications in softwareengineering (e.g. , defect prediction [30], [34], [48]).
418
418
418
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. Contextual
FeaturesError Type
Method1
Method2Identifiers
Comments
Method3
Method4
Method1
Method5Method6
ThrowSettingFlag
ReturnRecoverFlag
Empty
CatchBlock
NumOfMethodsLOC
OtherOperationMethod4
Containing
Method
Syntactic
FeaturesStructural
FeaturesTextual
Features
LabelFocused
Code
Snippet
Fig. 3. Framework of Contextual Feature Extraction
B. Contextual Feature Extraction
Feature extraction lies in the core of ‚Äúlearning to log‚Äù,
because the quality of extracted features directly determines
the performance of the model. The context information ( e.g.,
the functionality of operations, the impact of exceptions) oflogging points are crucial for developers to make loggingdecisions. However, it is challenging to effectively extractsuch context information, because the target code snippet isusually short and linguistically sparse compared to naturallanguage text. To address this issue, we propose a novel featureextraction framework, as illustrated in Fig. 3, which involvesthree types of features: structural features, textual features, andsyntactic features.
1)Structural features: Source code has a well-deÔ¨Åned
structure. It is desired to leverage the structure informationof source code to help extract context information. To achievethis goal, we extract two types of structural features: error typeand associated methods.
Error Type: The error type, such as exception type or
call type, can largely reveal the context of our focused codesnippets, which is highly correlated with logging decisions ofdevelopers (as indicated in Section II-B4). For an exception
snippet, the exception type generally denotes one speciÔ¨Åc typeof exceptional conditions with informative semantic meanings,e.g.,‚ÄúFileNotFoundException‚Äù in Fig. 1(a). For a return-value-
check snippet, the call type is denoted as the prototype of thechecked function, e.g., string GetStockIdForImageSpec(string,
int) in Fig. 1(b), which indicates one speciÔ¨Åc type of potential
function-return errors. Therefore, we extract error type as akey feature.
Each instance has a single error type, but there exist a
wide variety of error types among the training data. We avoiddirectly using each error type as a feature dimension, whichcan lead to highly sparse and ineffective feature vectors. In-stead, we construct only one feature dimension as the loggingratio of each error type, that is, the ratio of logged instancesagainst all the instances within that error type. Fig. 1(c)presents an illustration of the contextual features extractedfrom the code example in Fig. 1(a). In this example, the‚ÄúFileNotFoundException‚Äù type has a logging ratio of 39%regarding training instances in MonoDevelop, so we take thefeature value of error type as 0.39.Methods: The associated methods of a focused code snippet
also provide indicative information to help understand thefunctionality of the operations. For example, we can Ô¨Ågureout the intention of developers (i.e., to load an assembly Ô¨Åle)in the example of Fig. 1(a) according to the method names,including LoadRulesFromAssembly, GetFullPath, GetAssem-
blyName, and Load. Therefore, we extract these methods as
important contextual features.
SpeciÔ¨Åcally, there are two types of methods: the containing
method and the invoked methods. The former is the methodthat contains the focused code snippet ( e.g.,LoadRulesFro-
mAssembly in Fig. 1(a)), while the latter includes all the
methods that are invoked by the snippet. The operations can beseen as a sequence of API method invocations. Thus, instead ofusing only the methods within the code snippet, we also trackthe callee methods. Fig. 3 provides a prototype of our app-roach, where the arrows represent the invocation relationshipsbetween methods. For example, Method1 and Method2 areinvoked by the focused code snippet, where Method1 invokesMethod3 and Method4, and Method4 further invokes itselfand Method6. The extraction of methods continues trackingdown until the invoked method is a system API or externallibrary API method (e.g. ,System.IO.Path.GetFullPath) or until
a certain number of levels has been attained. The extractionprocess is implemented as a breadth-Ô¨Årst search (BFS) variant,where all the recorded (visited) methods will be skipped. Inparticular, all the logging methods are excluded in this process.Due to space limits, the details of the method extractionalgorithm is provided in our supplementary report [8].
After extracting the list of associated methods, we obtain the
full qualiÔ¨Åed name (e.g. , System.IO.Path.GetFullPath) of each
method as a feature dimension, which contains namespace,class name, and its (short) method name. Fig. 1(c) providesan example for these features.
2)T extual features: Source code is also text. Using code
as Ô¨Çat text has been widely employed in the Ô¨Åeld of miningsoftware repositories, and its effectiveness are demonstratedand reported in tasks such as API mining [47], code example
retrieval [17], etc. Driven by these encouraging results, we also
emplo y the similar approach to extract textual features from
source code text.
More speciÔ¨Åcally, we extract all the texts in the focused code
snippet excluding method names, such as variables and types.Then we combine them with the extracted list of structuralfeatures (i.e., error type and methods) as the full text. Incontrast to extracting all the text directly, our approach notonly excludes the text of logging methods, but also includesthe names of the callee methods, the containing method,as well as their namespaces and classes. With such text,we can extract the textual features using the bag-of-wordsmodel through a set of widely-used text processing operations,including tokenization, stemming, stop words removal, andTF-IDF term weighting [41]. Since the use of these techniquesin code processing has been carefully reported in [17], [18],[47], we omit the details here and refer the interested readersto our supplementary report [8]. In our study, these processing
419
419
419
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. steps are performed using Weka [29].
3)Syntactic features: As indicated in Section II-B3, there
are many situations of not logging, even for typical error sites
such as exceptions and function-return errors. Some potentialerrors have no critical impact on the normal operation ofthe whole system, some are resolved by recovery actionssuch as retry or walk-around, and some others are explicitlyreported (e.g. , by setting Ô¨Çags, re-throwing, or returning special
values) to the subsequent or upper-level operations (e.g. , caller
method) to handle.
To capture these contextual factors, we also extract some
key syntactic features from each focused code snippet: 1) Set-
tingFlag. We identify whether there is an assignment statementwith an assigned value like -1/null/false/empty. 2) Throw. We
identify whether there is a throw statement. 3) Return.W e
identify whether any special value (e.g. , -1/null/false/empty)
is returned. 4) RecoverFlag. We check whether there is a new
try statement inside. 5) OtherOperation. We check whether
there is any other operations included except the above Ô¨Åveones. 6) EmptyBlock. We Ô¨Ånd that the developers sometimes
catch and then do nothing. We thus identify whether the catchblock is empty. Note that all these identiÔ¨Åcation processeshave excluded logging statements at the Ô¨Årst place, and allthese features have Boolean values. In addition, we employ the
feature LOC to measure the lines of code in the code snippet,
and the feature NumOfMethods to measure the number of the
extracted methods. An example is shown in Fig. 1(c).
C. Feature selection
The above feature extraction process, however, can generate
tens of thousands of features, due to the large vocabulary of
methods and (textual) terms extracted from the data instances.These features further lead to high-dimensional (e.g. , 72K fea-
tures in System-B) yet highly-sparse feature vectors, becausemost of the features are actually infrequent across all datainstances. Furthermore, some of these features (e.g. , textual
features parsed from some speciÔ¨Åc variable names) may beirrelevant and have negative impact on the performance of thepredictive model.
In such a setting, we make use of a two-step feature
selection process to remove irrelevant features and reducethe dimensionality of feature vectors. First, we institute athreshold that constraints the minimum frequency of a featurethat occurs across all data instances. We set the threshold to5 in our experiments and thus eliminate a signiÔ¨Åcant number(e.g. , 68% in System-B) of infrequent features. Second, we
employ a well-known approach, information gain [16], to
perform further feature selection. Information gain is widely-used and effective in text categorization [16]. We carefully setthe minimum information gain to Ô¨Ålter out many irrelevantfeatures and reduce the feature dimensionality to around 1000.
D. Noise Handling
Another challenge lies in the data noises. In the framework
of ‚Äúlearning to log‚Äù, we implicitly assume good logging
quality in the training data, which therefore facilitates the0.8
0.7
0.50.40.5
Logged InstanceUnlogged Instance
Synthetic Instance
Fig. 4. Illustration of Noise Handling
automatic learning of good logging ‚Äúrules‚Äù for new devel-opment. However, there is no guarantee about the quality oflogging in reality, due to the lack of ‚Äúground truth‚Äù on what isoptimal logging. Considering the active maintenance and thelong history of evolution of our studied software systems, itis still reasonable to assume that ‚Äúmost‚Äù of the data instancesare enclosed with good logging decisions, while only a smallportion of them may reveal incorrect decisions, which we referto as data noises. For example, some instances that deserve
logging are actually not logged, while some others withoutthe need of logging are logged. These data noises thus haveÔ¨Çipped logging labels.
We attempt to detect and eliminate such data noises, and
help the model learn the common knowledge of logging moreeffectively. In many real-world applications, perfect data labelsare impossible (or difÔ¨Åcult) to obtain [24]. Kim et al. haveproposed a simple and effective noise detection approach(namely CLNI) for defect prediction [30]. We adapt thisapproach to deal with our speciÔ¨Åc case, and Ô¨Ånd that it workswell (as demonstrated in Section IV-D).
Traditionally, CLNI identiÔ¨Åes the k-nearest neighbours for
each instance and examines the labels of its neighbours. Ifa certain number of neighbours have an opposite label, theexamined instance will be Ô¨Çagged as a noise. However, weobserve a high imbalance ratio, for example up to 48.8:1
in MonoDevelop, between unlogged (majority) instances andlogged (minority) instances. Therefore, the majority instancestend to dominate the neighbourhood of an examined instance,which makes the identiÔ¨Åcation of k-nearest neighbours inCLNI biased to the majority class. To handle this issue,we apply a state-of-the-art imbalance handling approach,SMOTE [21]. SMOTE balances the data instances by creatingsynthetic logged instances as shown in Fig. 4. Consequently,both classes have an equal number of data instances, whicheliminates the inherent bias to the majority class when weidentify the k-nearest neighbours of an instance. Next, wequantify each examined instance iwith a noise degree value:
œï
i=/summationtext
j‚ààSiwij, whereSidenotes the set of neighbours with
opposite label with i, andwijis the weight to characterize the
different impacts of different neighbours in Si. In contrast to
CLNI that uses wij=1 , we take wijas the cosine similarity
between features of iandj. This is based on the intuition
that instances with higher similarity between each other aremore likely to share the same label. Therefore, the greater thevalueœï
iis, the higher probability the examined instance iis a
noise. For example in Fig. 4, œïi=2.5. We Ô¨Çag the instances
with top ranked œïivalues and remove them as noises, while
leveraging the remaining data for model training.
420
420
420
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. TABLE III
BALANCED ACCURACY OF DIFFERENT APPROACHES
Exception Snippets Return-value-check SnippetsApproachesSystem-A System-B SharpDev MonoDev System-A System-B SharpDev MonoDev
Random 0.499 0.500 0.496 0.503 0.500 0.494 0.505 0.503
ErrLog 0.500 0.500 0.500 0.500 0.500 0.500 0.500 0.500
Error Type 0.719 0.637 0.724 0.797 0.743 0.748 0.829 0.813
Methods 0.672 0.690 0.603 0.678 0.689 0.699 0.772 0.769
Textual Features 0.768 0.712 0.719 0.797 0.814 0.768 0.781 0.808
Syntactic Features 0.884 0.858 0.779 0.829 0.762 0.764 0.794 0.758
LogAdvisor 0.934 0.927 0.846 0.932 0.903 0.927 0.865 0.918
TABLE IV
BALANCED ACCURACY OF DIFFERENT LEARNING MODELS
Exception Snippets Return-value-check SnippetsModelsSystem-A System-B SharpDev MonoDev System-A System-B SharpDev MonoDev
Naive Bayes 0.701 0.623 0.686 0.714 0.746 0.766 0.788 0.762
Bayes Net 0.729 0.751 0.688 0.862 0.802 0.814 0.845 0.859
Logistic Regression 0.881 0.834 0.772 0.858 0.806 0.834 0.856 0.848
SVM 0.898 0.886 0.878 0.903 0.815 0.885 0.873 0.877
Decision Tree 0.934 0.927 0.846 0.932 0.903 0.927 0.865 0.918
IV . E V ALUA TION
In this section, we conduct comprehensive experiments to
evaluate the effectiveness of LogAdvisor. In particular, we
intend to answer the following research questions.
RQ1: What is the accuracy of LogAdvisor ?
RQ2: What is the effect of different learning models?
RQ3: What is the effect of noise handling?RQ4: How does LogAdvisor perform in the cross-project
learning scenario?
A. Experimental Setup
After obtaining the feature vectors and their corresponding
logging labels, we employ Weka [29] to perform model train-ing and evaluation. Due to the imbalanced nature of our data,we apply the Weka implementation of SMOTE [21] to balancethe training data for model construction. By default, we usedecision tree (J48) as the learning model, because of its goodperformance (Section IV-C) and ease of interpretation. Exceptfor the cross-project evaluation (Section IV-E), all of theexperiments are evaluated on all of the extracted data instancesby using the 10-fold cross evaluation mechanism [41].
As recommended in other related work [22], [46], we eval-
uate LogAdvisor using balanced accuracy (BA) [19], which
is the average of the proportion of logged instances and theproportion of unlogged instances that are correctly classiÔ¨Åed.BA is calculated as follows:
BA=1
2√óTP
TP+FN+1
2√óTN
TN+FP, (1)
where TP , FP , TN, and FN denote true positives, false pos-itives, true negatives, and false negatives, respectively. BAweights the performance on each of the two classes equally,thus avoiding inÔ¨Çated performance evaluation on imbalanceddata. For example, with an imbalance ratio of 48.8:1 in
MonoDevelop, a trivial classiÔ¨Åer that always predict ‚Äúnotlogging (unlogged)‚Äù can achieve 98% accuracy, but wouldresult in a low balanced accuracy of 49%. For referencepurpose, the results on other metrics such as precision, recalland F-score are provided in our supplementary report [8].
B. Results of RQ1: Prediction Accuracy
We compare LogAdvisor with two baseline approaches:
random and ErrLog [43]. By random, we mimic the situation
where a developer has no knowledge about logging andperform the logging decision with a random probability of 0.5.ErrLog is proposed in [43] that makes conservative logging(i.e., log all the generic exceptions such as exceptions andfunction-return errors) for failure diagnosis. The results areprovided in Table III.
As we can observe, both random and ErrLog have balanced
accuracy of approximately 50%. Random logging has equalaccuracy of 50% on either class. ErrLog logs every instance,achieving 100% accuracy on logged class, and 0% on un-logged class. Overall, the balanced accuracy of LogAdvisor is
high, ranging from 84.6% to 93.4%, indicating high similarityto the logging decisions manually made by developers. Thus,LogAdvisor can learn a good representation of the common
logging knowledge, and serve as a good baseline for guidingdevelopers‚Äô logging behaviors towards better logging practice.
We also evaluate the effect of different contextual features
(error type, methods, textual features, and syntactic features)on the prediction accuracy, as presented in Table III. We cansee that every type of contextual feature is useful, which leadsto much higher balanced accuracy than random and ErrLog.LogAdvisor, by combining all these useful features, makes fur-ther improvement and achieves the highest balanced accuracy.These results also reveal that the contextual features extractedfrom the focused code snippets provide good indication oflogging practices of developers.
C. Results of RQ2: The Effect of Different Learning Models
By default, we use decision tree (J48) to train our model,
due to its simplicity as well as its effectiveness shown in
our previous study [26]. We also examine the impact of
421
421
421
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. 1.0 2.0 3.0 4.0 5.000.20.40.60.81Percentage of Instances
Noise DegreeException Snippets (MonoDevelop)
Normal Instances
Noise Instances
(a) Instance Distribution over Noise DegreeSystem‚àíA System‚àíB SharpDev MonoDev0.80.850.90.951Balanced AccuracyException Snippets
W/O Noise Handling
W/ Noise Handling
(b) The Effect on Exception SnippetsSystem‚àíA System‚àíB SharpDev MonoDev0.80.850.90.951Balanced AccuracyReturn‚àívalue‚àícheck Snippets
W/O Noise Handling
W/ Noise Handling
(c) The Effect on Return-value-check Snippets
Fig. 5. Noise Handling Evaluation Results
different learning models on the prediction accuracy. We
have tried a number of popular learning models, includingNaive Bayes, Bayes Net, Logistic Regression, Supprot V ectorMachine (SVM), and Decision Tree, by using their Wekaimplementations. The evaluation results in Table IV showthat all the learning models lead to overall good predictionaccuracy. In particular, Bayes-based learning models are basedon probability theory. Unlike natural language text, the featuresextracted from source code are short and linguistically sparse,so Bayes-based learning models work slightly worse in oursettings. Logistic Regression is a linear classiÔ¨Åer, thus it maynot Ô¨Åt well with our data. Decision Tree achieves the bestoverall accuracy, because this algorithm can solve non-linearclassiÔ¨Åcation problem. Furthermore, this algorithm can implic-itly perform feature selection, which removes the redundant orirrelevant features and runs much faster than SVM for our data.
D. Results of RQ3: The Effect of Noise Handling
To evaluate the effect of noise handling approach, we
Ô¨Årst study the instance distribution across the noise degree
(œï
i) values, and then compare the prediction results with
noise handling and those without noise handling. For ease ofpresentation, we only plot the instance distribution regardingexception snippets of MonoDevelop in Fig. 5(a), while theresults of other systems are also similar. In particular, weset the number of nearest neighbours, k,t o5 .S oœï
ihas a
value range of 0‚àº5. It shows that the majority (about 88%)
of instances have a noise degree value close to 0, indicatingthat each examined instance has the same logging label withalmost all of its nearest neighbours. Only a small proportion ofinstances are likely noise data (e.g. , those with noise degree
œï
i>3). To some extent, this reveals the quality of data.
In our study, we tune the threshold and Ô¨Çag about 5% ofinstances with top ranked œï
ivalues as noises, which are
removed them in the training phase. As the evaluation resultsshown in Fig. 5(b)(c), the noise handling approach makesfurther improvement on the prediction accuracy. It indicatesthat properly removing potential noise data can make ourmodel learn the common logging knowledge more effectively.
E. Results of RQ4: Cross-Project Evaluation
In within-project learning, LogAdvisor leverages the existing
logging instances within the same project as training data to
construct the predictive model. The above experiments providepromising results on the prediction accuracy of within-projectSystem‚àíA System‚àíB SharpDev MonoDev0.50.60.70.80.91.0Balanced Accuracy
(S1)               (S2)             (S3)               (S4)Within‚àíProject Learning
Cross‚àíProject Learning
Cross-Project Learning
Settings:
(S1): SystemB /g198SystemA
(S2): SystemA /g198SystemB
(S3): MonoDev /g198SharpDev
(S4): SharpDev /g198MonoDev
Fig. 6. Cross-Project Evaluation Results
evaluation, strongly indicating that LogAdvisor likely work
well in the scenario of developing some new components in the
same project. However, many real-world projects are small ornew, which have limited training data for model construction.In such cases, it is valuable to explore whether cross-projectlearning can help.
In cross-project learning, we enrich the training data by
incorporating the data instances extracted from a similarproject (source project ), and then apply the trained model
to the target project for logging prediction. However, in
contrast to within-project learning, cross-project learning issigniÔ¨Åcantly more challenging [48], such as handling project-speciÔ¨Åc features. To address these challenges, we extract thecommon features that are shared between projects. We Ô¨Åndthat many system APIs and error types are actually commonamong different projects. We further leverage these commonfeatures to evaluate the performance of cross-project learningbetween different pairs of our studied systems (one sourceproject for training and one target project for testing).
Due to space limits, we only provide four pairs of cross-
project evaluation results in Fig. 6 (and others in our supple-mentary report [8]), with comparison to their correspondingwithin-project evaluations. The settings of these cross-projectevaluations are shown as well. For example, by using System-A as the target project, and System-B as the source project,we can get a balanced accuracy of 81.5%, compared with93.4% in within-project learning. This result, indeed, indicatesthat the performance of cross-project learning is largely deg-raded compared with within-project setting. The reason is thatdifferent projects may follow different logging practices, andsome project-speciÔ¨Åc knowledge (e.g. , domain exceptions and
methods) are challenging to adapt to other projects. However,these results can serve as a baseline for further improvementby exploring other sophisticated techniques, such as transferlearning across projects [34].
422
422
422
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. V. U SER STUDY
To further measure the effectiveness of LogAdvisor, we con-
duct a controlled user study among engineers from Microsoft
and a local IT company in China. We invited 37 participantsin total, including 23 staff developers and 14 interns, whohave an average of 4.9 years of programming experience. Inaddition, 22 (59%) of them use logging frequently while 12(32%) of them use logging occasionally. The user study isconducted through an online questionnaire, which consists of11 questions: 5 questions for the background of participantsand their understanding on logging practices, 4 questions forcase studies on logging, and 2 questions for assessment of ourlogging suggestion results. For reproducibility, a copy of thequestionnaire is provided on our project page [8].
To perform logging case studies, we randomly select 20
exception snippets and 20 return-value-check snippets fromMonoDevelop. Half of them are logged, while the other halfare not. We remove the logging statements in code snippetsand ask participants to make logging decisions on whetherto log. The original logging labels made by code owners aretaken as the ‚Äúground truth‚Äù. However, sometimes, it is hardfor participants (not code owners themselves) to understandthe code logic well by reading only a small code snippet.To mitigate this issue, we group two code snippets withdifferent logging labels (e.g. , one logged exception and one
unlogged exception) into a pair. Then we ask the participantsto choose which one is more likely to be logged from the pair,because it is easier for an participant to make choice throughcomparison. To evaluate the effectiveness of LogAdvisor,t w o
groups of pairs are provided: one group with our loggingsuggestions, and the other group without logging suggestions.The suggestion results are provided from our trained model,with an accuracy of approximately 80% on these case-studysnippets. To make a fair comparison, each participant marks anequal number of pairs in each group, and each pair is markedby at least three participants. In particular, we leverage theonline survey system, Qualtrics
3, to build 10 questionnaires,
each using 4 different pairs of code snippets. We distribute thesurvey links evenly to the participants. Furthermore, we recordthe time they spend on making each logging choice using thetiming functionality of Qualtrics.
Results: We evaluate the accuracy that the participants
correctly recover the logging decisions of the code owners. Forthe group without logging suggestions, the accuracy is 60%,while the group with logging suggestions achieves an accuracyof 75%, with a relative improvement of 25%. As for timeconsumption, the participants took 33% less time on averageto make a logging choice with our logging suggestions (28seconds v.s. 42 seconds). In addition, we query the feedbackfrom the participants by the question ‚ÄúDo you think thesuggestion result is useful for your logging choice?‚Äù, and 70%of participants think it is useful. These results provide a strongevidence in the effectiveness of our logging suggestion.
3http://qtrial.qualtrics.comVI. D ISCUSSIONS
Logging quality: The approach of ‚Äúlearning to log‚Äù works
under the premise that the training data have high loggingquality. In such a setting, the constructed model can representthe common (and good) logging knowledge and generalizewell to predictions of new instances. However, there is no‚Äúground truth‚Äù on what is high-quality (or optimal) logging.In our study, we assume that our studied software systemshave reasonably good logging implementations due to theirhigh code quality, active maintenance and long history ofevolution. To a certain degree, it has been endorsed by ourevaluation results (e.g. , high prediction accuracy, positive user
feedback). Besides, our noise handling approach can furthermitigate the data quality issue by detecting and omitting thenoisy logging instances from the training data, thus improvingthe performance of LogAdvisor.
Diversity of subject software systems: Our study was
conducted on four software systems written in C#, thusits validity may be threatened by the limited diversity ofour studied systems. To mitigate this threat, we choose thesubjects including both commercial software systems from aleading software company like Microsoft and popular open-source software systems on GitHub. These systems are activelymaintained and have a long history of evolution, which canserve as a representative of real practice. Besides, two of themare online services while the other two are IDEs, thus yieldingboth similar projects and dissimilar projects for our study. Webelieve that our approach and the results derived from thesesystems are easily reproducible and can be generalizable tomany other software systems. Future studies on more types ofsoftware systems may further reduce this threat.
Where to log v.s. what to log: To achieve good logging
quality, developers need to make informed decisions on bothwhere to log and what to log. The ideal of ‚Äúlearning to log‚Äùis to help developers resolve both decisions. However, as aninitial step towards this goal, we focus primarily on whereto log in this paper, because it is the Ô¨Årst logging decisionto make and sometimes can determine (or narrows down)what to log. For example, when developers decide to log anexception, the contents to be recorded become much morespeciÔ¨Åc, including the exception message, stack trace, etc.Besides, a recent study [45] has built an LogEnhancer tool that
can enrich the recorded contents by automatically identifyingand inserting critical variable values into the existing loggingstatements. As part of our future work, this tool can be furtherintegrated into our ‚Äúlearning to log‚Äù framework to facilitatelog automation, where LogAdvisor determines where to log
and LogEnhancer determines what to log.
Potential Improvements: Towards ‚Äúlearning to log‚Äù, we still
have a number of potential directions that deserve furtherexploration for improvements: 1) Other factors on logging
decision. The logging behaviours of developers can be quitecomplex and vary among developers. Also, the logging state-ments can be dynamically updated, such as deletion andmodiÔ¨Åcation. Thus, additional consideration of factors such as
423
423
423
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. code owner, check-in time and execution frequency of code
may further enhance the performance of logging prediction. 2)
Interdependence of logging statements. Our approach identiÔ¨Åeseach logging point sequentially and in isolation. In somecases, logging at one point may impact another. For example,atry-catch block may be enclosed in another catch block,
and the exception may be thrown to the upper one to log.Or sometimes, logging statements at critical points are usedtogether to record the execution path. Further exploration of ajoint inference model (e.g. , graphical models, Markov chains)
may help in this case. 3) Runtime logging. Current logging
statements are mostly statically inserted into the code. Thereis a new proposal for runtime logging, in which whether to logor not can be determined at runtime. For example, logs maybe recorded by adaptive sampling [43] or only be recordedwhen encountering some problems (e.g. , a failed request or
a long response) [11]. Although such sophisticated runtimelogging mechanism is not supported by our studied systems,it is a promising direction for exploration to balance utilityand overhead of logging.
VII. R
ELA TED WORK
Log Analysis: Logs contain a wealth of information that
are useful in aiding software system maintenance, and thusbecome an important data source for postmortem analysis [36].For instance, logs have been widely analyzed for various tasks,such as anomaly detection [25], [42], problem diagnosis [33],[43], program veriÔ¨Åcation [37], security monitoring [32], usageanalysis [31], etc. In addition to the usage of logs, Shanget al. [39] studied how to automatically enrich the producedlog messages with development knowledge (e.g. , source code,
commits, issue reports) and further assist users in log under-standing. Instead, our work aims to improve the underlyinglogging practice, thus can potentially beneÔ¨Åt these tasks onlog analysis and log understanding.
Logging Practices: Current research has mostly focused
on the usage of logs, but little on logging itself. Two em-pirical studies [26], [44] have recently been conducted tocharacterize the logging practices. Y uan et al. [44] reportedthe characteristics of logging modiÔ¨Åcations by investigatingthe revision histories of open-source software systems. Ourprevious work [26] focused on studying where developer logthrough both code analysis and developer survey at Microsoft,and summarized Ô¨Åve typical categories of logging strategies.Additionally, Shang et al. [38] studied the relationship betweenlogging characteristics and the code quality of platform soft-ware. All these studies provide comprehensive logging char-acteristics that shed insights into our design of LogAdvisor.
Improving Logging: Towards improving the logging qual-
ity, Y uan et al. have recently pioneered two prior studies:LogEnhancer [45] and ErrLog [43]. LogEnhancer [45] aims toenhance the recorded contents in existing logging statementsby automatically identifying and inserting critical variablevalues into them. ErrLog [43] summarizes a set of genericexception patterns (e.g. , exceptions, function-return errors) thatpotentially cause system failures, and then suggests conser-vative logging to automatically log all of them (e.g. , log all
exceptions). Their work takes the Ô¨Årst step towards automaticlogging and provides promising results in reducing diagnosistime of system failures. Our work, instead, makes an initialattempt to help developers make informed logging decisions.Furthermore, we argue that logging too much can causeunintended problems and aim to draw a good balance via‚Äúlearning to log‚Äù.
Mining Software Repositories: Some technical insights in
the design of LogAdvisor are also inspired from the exist-
ing work on mining software repositories, especially fromsoftware defect prediction [30], [48]. The defect predictionmethods extract features from the defective and non-defectivemodules, and then construct a classiÔ¨Åcation model to pre-dict the defect-proneness of a new module. Kim et al. [30]proposed the CLNI method to address the data quality issue(data noise) in defect prediction. Zimmermann et al. [48]evaluated cross-project defect predictions among 12 real-worldapplications, and highlighted the critical challenges in cross-project learning. Our work applies a similar machine learningapproach, and also considers issues such as data quality andcross-project learning.
Exception Handling: Exception handling mechanisms [27]
have been widely studied to improve the reliability and main-tainability of software systems. Cacho et al. [20] evaluatedhow changes in exceptional code can impact system robust-ness. Thummalapenta et al. [40] leveraged association rules tomine some speciÔ¨Åc exception handling patterns. In our work,we focus on logging in two types of code snippets with regardto exceptions as well as function-return errors.
VIII. C
ONCLUSION
Strategic logging is important yet difÔ¨Åcult for software
development. However, current logging practices are not welldocumented and cannot provide strong guidance on deve-lopers‚Äô logging decisions. To Ô¨Åll this gap, we propose a ‚Äúlearn-ing to log‚Äù framework, which aims to automatically learn thecommon logging practices from existing code repositories.As a proof of concept, we implement an automatic loggingsuggestion tool, LogAdvisor, which can help developers make
informed logging decisions on where to log and potentiallyreduce their effort on logging. Evaluation results on four large-scale software systems, as well as a controlled user study,demonstrate the feasibility and effectiveness of LogAdvisor.
We believe it is an important step towards automatic logging.
A
CKNOWLEDGMENT
The work described in this paper was substantially sup-
ported by the National Basic Research Program of China (973Project No. 2011CB302603), the National Natural ScienceFoundation of China (Project No. 61332010, 61272089), andthe Research Grants Council of the Hong Kong Special Ad-ministrative Region, China (No. CUHK 415113 of the GeneralResearch Fund).
424
424
424
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] 10 best practices with exceptions. http://www.wikijava.org/wiki/10 best
practices with Exceptions.
[2] 7 good rules to log exceptions. http://codemonkeyism.com/7-good
-rules-to-log-exceptions.
[3] 7 more good tips on logging. http://codemonkeyism.com/7-more-good
-tips-on-logging.
[4] The art of logging. http://www.codeproject.com/Articles/42354/The-Art
-of-Logging.
[5] Code to logging ratio? http://stackoverÔ¨Çow.com/questions/153524/code
-to-logging-ratio#153547.
[6] Exception logging in javascript. https://developer.mozilla.org/en-US/
docs/Exception logging inJavaScript.
[7] Exception logging: Why not log all exceptions? http://stackoverÔ¨Çow.
com/questions/25560953/exception-logging-why-not-log-all
-exceptions.
[8] Learning to log: Helping developers make informed logging decisions
(supplementary report). http://cuhk-cse.github.io/LogAdvisor.
[9] Logging best practices. https://idea.popcount.org/2013-12-31-logging
-best-practises.
[10] Microsoft ‚ÄùRoslyn‚Äù CTP . http://msdn.microsoft.com/en-us/vstudio/
roslyn.aspx.
[11] Optimal logging (Google) testing blog. http://googletesting.blogspot.
com/2013/06/optimal-logging.html.
[12] Overview of UniÔ¨Åed Logging System (ULS). http://msdn.microsoft.com/
en-us/library/ofÔ¨Åce/ff512738(v=ofÔ¨Åce.14).aspx.
[13] The problem with logging. http://blog.codinghorror.com/the-problem
-with-logging.
[14] Why not log all exceptions in MonoDevelop? http://lists.ximian.com/
pipermail/monodevelop-list/2014-August/016201.html.
[15] Why not log all exceptions in SharpDevelop? https://github.com/
icsharpcode/SharpDevelop/issues/554.
[16] C. C. Aggarwal and C. Zhai. A survey of text classiÔ¨Åcation algorithms.
pages 163‚Äì222, 2012.
[17] S. K. Bajracharya, J. Ossher, and C. V . Lopes. Leveraging usage
similarity for effective retrieval of examples in code repositories. InProc. of ACM FSE, pages 157‚Äì166, 2010.
[18] B. Bassett and N. A. Kraft. Structural information based term weighting
in text retrieval for feature location. In Proc. of IEEE ICPC, pages 133‚Äì
141, 2013.
[19] K. H. Brodersen, C. S. Ong, K. E. Stephan, and J. M. Buhmann. The
balanced accuracy and its posterior distribution. In Proc. of IEEE ICPR,
pages 3121‚Äì3124, 2010.
[20] N. Cacho, T. C ¬¥esar, T. Filipe, E. Soares, A. Cassio, R. Souza, I. Garc ¬¥ƒ±a,
E. A. Barbosa, and A. Garcia. Trading robustness for maintainability:an empirical study of evolving C# programs. In Proc. of ACM/IEEE
ICSE, pages 584‚Äì595, 2014.
[21] N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P . Kegelmeyer.
Smote: Synthetic minority over-sampling technique. J. Artif. Int. Res.,
16(1):321‚Äì357, 2002.
[22] I. Cohen, J. S. Chase, M. Goldszmidt, T. Kelly, and J. Symons.
Correlating instrumentation data to system states: A building block forautomated diagnosis and control. In Proc. of Usenix OSDI, pages 231‚Äì
244, 2004.
[23] O. Crameri, R. Bianchini, and W. Zwaenepoel. Striking a new balance
between program instrumentation and debugging time. In Proc. of ACM
EuroSys, pages 199‚Äì214, 2011.
[24] B. Fr ¬¥enay and M. V erleysen. ClassiÔ¨Åcation in the presence of label
noise: A survey. IEEE Trans. Neural Netw. Learning Syst. , 25(5):845‚Äì
869, 2014.
[25] Q. Fu, J.-G. Lou, Y . Wang, and J. Li. Execution anomaly detection in
distributed systems through unstructured log analysis. In Proc. of IEEE
ICDM, pages 149‚Äì158, 2009.[26] Q. Fu, J. Zhu, W. Hu, J.-G. Lou, R. Ding, Q. Lin, D. Zhang, and T. Xie.
Where do developers log? an empirical study on logging practices inindustry. In Proc. of ACM/IEEE ICSE, 2014.
[27] J. B. Goodenough. Exception handling: Issues and a proposed notation.
Commun. ACM, 18(12):683‚Äì696, 1975.
[28] I. Guyon and A. Elisseeff. An introduction to variable and feature
selection. Journal of Machine Learning Research, 3:1157‚Äì1182, 2003.
[29] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P . Reutemann, and I. H.
Witten. The weka data mining software: An update. SIGKDD Explor.
Newsl., 11(1):10‚Äì18, 2009.
[30] S. Kim, H. Zhang, R. Wu, and L. Gong. Dealing with noise in defect
prediction. In Proc. of ACM/IEEE ICSE, pages 481‚Äì490, 2011.
[31] G. Lee, J. Lin, C. Liu, A. Lorek, and D. V . Ryaboy. The uniÔ¨Åed
logging infrastructure for data analytics at twitter. VLDB Endowment,
5(12):1771‚Äì1780, 2012.
[32] M. Montanari, J. H. Huh, D. Dagit, R. Bobba, and R. H. Campbell.
Evidence of log integrity in policy-based security monitoring. In Proc.
of IEEE DSN Workshops, pages 1‚Äì6, 2012.
[33] K. Nagaraj, C. Killian, and J. Neville. Structured comparative analysis
of systems logs to diagnose performance problems. In Proc. of USENIX
NSDI, pages 26‚Äì26, 2012.
[34] J. Nam, S. J. Pan, and S. Kim. Transfer defect learning. In Proc. of
ACM/IEEE ICSE, pages 382‚Äì391, 2013.
[35] P . Ohmann and B. Liblit. Lightweight control-Ô¨Çow instrumentation and
postmortem analysis in support of debugging. In Proc. of IEEE ASE,
pages 378‚Äì388, 2013.
[36] A. J. Oliner, A. Ganapathi, and W. Xu. Advances and challenges in log
analysis. Commun. ACM, 55(2):55‚Äì61, 2012.
[37] W. Shang, Z. M. Jiang, H. Hemmati, B. Adams, A. E. Hassan, and
P . Martin. Assisting developers of big data analytics applications whendeploying on hadoop clouds. In Proc. of ACM/IEEE ICSE, pages 402‚Äì
411, 2013.
[38] W. Shang, M. Nagappan, and A. E. Hassan. Studying the relationship
between logging characteristics and the code quality of platform soft-ware. Empirical Software Engineering, 2013.
[39] W. Shang, M. Nagappan, A. E. Hassan, and Z. M. Jiang. Understanding
log lines using development knowledge. In Proc. of IEEE ICSME, 2014.
[40] S. Thummalapenta and T. Xie. Mining exception-handling rules as
sequence association rules. In Proc. of ACM/IEEE ICSE , pages 496‚Äì506,
2009.
[41] I. H. Witten and E. Frank. Data Mining: Practical Machine Learning
Tools and Techniques (Second Edition). Morgan Kaufmann PublishersInc., 2005.
[42] W. Xu, L. Huang, A. Fox, D. A. Patterson, and M. I. Jordan. Detecting
large-scale system problems by mining console logs. In Proc. of ACM
SOSP, pages 117‚Äì132, 2009.
[43] D. Y uan, S. Park, P . Huang, Y . Liu, M. M. Lee, X. Tang, Y . Zhou, and
S. Savage. Be conservative: enhancing failure diagnosis with proactivelogging. In Proc. of USENIX OSDI, pages 293‚Äì306, 2012.
[44] D. Y uan, S. Park, and Y . Zhou. Characterizing logging practices in
open-source software. In Proc. of ACM/IEEE ICSE, pages 102‚Äì112,
2012.
[45] D. Y uan, J. Zheng, S. Park, Y . Zhou, and S. Savage. Improving
software diagnosability via log enhancement. ACM Trans. Comput. Syst.,
30(1):4:1‚Äì4:28, 2012.
[46] S. Zhang, I. Cohen, M. Goldszmidt, J. Symons, and A. Fox. Ensembles
of models for automated diagnosis of system performance problems. InProc. of IEEE DSN, pages 644‚Äì653, 2005.
[47] W. Zheng, Q. Zhang, and M. Lyu. Cross-library api recommendation
using web search engines. In Proc. of ACM ESEC/FSE, pages 480‚Äì483,
2011.
[48] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy. Cross-
project defect prediction: a large scale experiment on data vs. domainvs. process. In Proc. of ESEC/SIGSOFT FSE, 2009.
425
425
425
ICSE 2015, Florence, Italy
Authorized licensed use limited to: Chinese University of Hong Kong. Downloaded on December 03,2020 at 12:46:44 UTC from IEEE Xplore.  Restrictions apply. 