Automatic Model Generation from Documentation for Java
A
PI Functions
Juan Zhai†, Jianjun Huang‡, Shiqing Ma‡, Xiangyu Zhang‡, Lin Tan§, Jianhua Zhao†, Feng Qin$
†The State key Laboratory for Novel Software Technology at Nanjing University,
‡Purdue University,§University of Waterloo,$Ohio State University
zhaijuan@seg.nju.edu.cn, {huang427,ma229,xyzhang }@cs.purdue.edu, lintan@uwaterloo.ca,
zhaojh@nju.edu.cn, qin@cse.ohio-state.edu
ABSTRACT
Modern software systems are becoming increasingly com-
plex, relying on a lot of third-party library support. Li-
brary behaviors are hence an integral part of software be-
haviors. Analyzing them is as important as analyzing the
software itself. However, analyzing libraries is highly chal-
lenging due to the lack of source code, implementation in
diﬀerent languages, and complex optimizations. We observe
thatmanyJavalibrary functionsprovideexcellentdocumen-
tation, which concisely describes the functionalities of the
functions. We develop a novel technique that can construct
models for Java API functions by analyzing the documen-
tation. These models are simpler implementations in Java
compared to the original ones and hence easier to analyze.
More importantly, they provide the same functionalities as
the original functions. Our technique successfully models
326 functions from 14 widely used Java classes. We also
use these models in static taint analysis on Android apps
and dynamic slicing for Java programs, demonstrating the
eﬀectiveness and eﬃciency of our models.
1. INTRODUCTION
Libraries are widely used in modern programming to en-
capsulate modular functionalities and hide platform speciﬁc
details from developers. They substantially improve pro-
grammers’ productivity. But on the other hand, they make
software analysis very challenging. The reason is that li-
brary behavior is an integral part of software behavior such
that software analysis cannot avoid analyzing library behav-
iors. Unfortunately, the source code of libraries may not be
available. Many libraries are mixed with many languages,
sometimes even in assembly code. Library implementations
are usually highly optimized and full of sophisticated engi-
neeringtricksthatare diﬃcultforanalysisenginestohandle.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’16, May 14 - 22, 2016, Austin, TX, USA
c/circlecopyrt2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.2884881As an important but very challenging problem, model-
ing library functions has been studied by many previous
works [27, 13, 21]. However, most of them require man-
ually constructing models. This puts a lot of burden on
the analysis developers. It can hardly scale to large projects
that usually make use of a large number of library functions.
[21] uses dynamic dependence summaries to improve analy-
sis. However, the technique does not model the functionali-
ties of library functions but rather their dependencies. The
summary may not be accurate when conditional statements
are involved and the training set is not of high quality. [23]
tries to automatically generate models for library/system
functions using satisﬁability modulo theories (SMT) solver.
The technique requires substantial manual eﬀorts and does
not scale well due to the very large search space.
According to our observation, API documentation, like
Javadoc and .NET documentation, usually contains wealthy
information about the library functions, such as the behav-
iors ofafunctionandtheexceptionsthefunctionmaythrow.
Thus it is feasible to generate models for library functions
from such API documentation. However, it is still challeng-
ing to do so since accurate linguistic analysis is needed to
analyze API documentation.
This paper proposes a novel approach to generate models
from Javadocs in natural languages. These models are code
snippets that have the same functionalities as the original
implementations but are much simpler in complexity. They
can replace the original library functions during software
analysis. Our contributions are highlighted as follows.
•We propose a novel and practical technique to gen-
erate substantially simpliﬁed Java code that models
libraries. It allows software analysis to reason about li-
brary behaviors, without suﬀering from problems such
as lack of source code and library implementations be-
ing too complex to analyze.
•We identify technical challenges of applying NLP tech-
niques in modeling libraries and propose solutions to
these problems.
•We implement a prototype which automatically mod-
els 326 functions in 14 commonly used Java container
classes. The application of these models in Android
appstatictaintanalysisandJavadynamicslicingshows
that these models precisely represent the library func-
tion behaviors and improve the eﬃciency andeﬀective-
ness of the analysis.
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   380
1public void add(intindex, E element){
2rangeCheckForAdd(index);
3ensureCapacityInternal( size+ 1);
4System.arrayCopy( elementData , index, elementData , index
+
1,size- index);
5elementData [index] = element;
6size++;
7}
Figure 1: The method a dd()in the class ArrayList
2. BACKGROUND AND MOTIV ATION
2.1 Models in Software Analysis
Many program analyses require proper reasoning about
libraries as their behaviors are an integral part of the soft-
ware behavior. For example, program slicing [9], taint anal-
ysis [19] and information ﬂow tracking [18] need to know the
dependenciesbetweeninputandoutputvariablesofalibrary
function. Symbolicexecutionengineslike[11] requireprecise
models of libraries to construct correct symbolic constraints,
and model checkers like Java PathFinder [34] need appro-
priate models of libraries to combat the state space explo-
sion problem. Unfortunately, it is usually diﬃcult and time-
consuming to obtain either this kind of dependencies or the
models due to the following reasons. First, the source code
of library functions is often beyond reach. Even if the source
code is available, it is still very challenging to analyze the
code, owningtothefactthatthesourcecodetendstobepro-
hibitively large, mix multiple programming languages, and
contain substantial optimizations and engineering tricks. It
is often the case that a single invocation to an API func-
tion may lead to a large number of invocations to functions
internal to the library, substantially adding the complexity
and cost of software analysis. Furthermore, many libraries
have inherent cross-platform support, making analysis even
harder.
Take the Java standard library as an example. The Java
Development Kit (JDK) contains the Java standard library
source code (version 8.0), the size of which is 81.7MB. There
are more than 10000 classes and 80000 methods. To achieve
thebinarylevelcompatibilityofnativelibrarymethodsacross
all Java virtual machine (JVM) implementations on a given
platform, JDK libraries invoke native methods through the
Java Native Interface (JNI) framework. These native meth-
ods are implemented in other languages such as C++ and
assembly, andtheirsourcecodeisunavailableingeneral. Be-
sides, there are multiple implementations for such a method
on diﬀerent architectures and JVMs. Fig. 1 shows a sim-
ple library function add (int index, E element) in theAr-
rayListclass. There are three function calls, including Sys-
tem.arrayCopy() , in ﬁve eﬀective lines of code (eLOC). The
implementation of the function System.arrayCopy() is un-
available in the source code folder. It is implemented in
native code using JNI. From this, we can see that analyzing
JDK functions is challenging.
Due to these reasons, library function models are usually
constructed and provided to replace the original implemen-
tation during analysis. These models are code snippets that
have the same functionality as the original library functions
but are much simpler. They can be used in place of the
original functions during software analysis. Note that even
though they are not as eﬃcient or sophisticated as the orig-
inal functions, they are much easier to analyze. Currently,the majority of models used are manually constructed [27,
11], which represents a substantial burden for the analysis
developers due to the large number of library functions and
their rapid evolution. Furthermore, library functions are
often optimized to achieve high performance. The details
of these optimizations are usually not the essential part of
the functional models. For example, as shown in Fig 2, the
model of the add()function can be represented by simple ar-
ray operations, guarded bya range checkpredicate. The low
leveldetailsof System.arrayCopy() andrangeCheckForAdd()
are not needed in the model. All these factors motivate us
to develop an approach toautomatically generate models for
library functions.
2.2 Approach Overview
It is a common practice for library developers to pro-
vide behavior description of library functions in natural lan-
guage in the Application Programming Interface (API) doc-
uments. J2SE’s Javadoc [3] is a typical example of such
API documentation, which oﬀers wealthy information such
as class/interface hierarchies and method description. Our
idea is hence to construct models from API documentation.
In recent years, natural language processing (NLP) tech-
niqueshave madetremendous progress andhavebeen shown
tobefairlyaccurateinidentifyingthegrammatical structure
ofanaturallanguagesentence[16, 17, 8, 35]. This enablesus
to leverage NLP techniques including Word Tagging/Part-
Of-Speech (POS) Tagging [16],Phrase and Clause Parsing
(Chunking) [16] and Syntactic Parsing [15] to acquire se-
mantics of sentences in API documentation.
Basic Idea: Given the documentation of a Java API func-
tion, we leverage an NLP tool to generate the parse trees
of the individual sentences. For each sentence, we match its
parse tree with the tree patterns of a set of predeﬁned primi-
tives, which are small code snippets implementingverybasic
functionalities. In particular, we try to identify a subset of
primitives whose tree patterns can cover the entire parse
tree when they are put together. The parameters in the
primitives are also instantiated by the corresponding nodes
in the parse tree. The same procedure is repeated until the
parse trees ofall sentencesare completely covered. The com-
position of the code snippets corresponding to the identiﬁed
primitives produces a model candidate. Since there are mul-
tiple possible parse trees for a sentence and many ways to
cover a tree, multiple candidates are generated. The invalid
ones are ﬁltered out by testing if a candidate behaves diﬀer-
ently from the original library function.
Example. Fig. 2(a) gives part of the documentation of the
library function add (int index, E element) in Fig. 1. The
textcan be dividedintofour parts: (1)the declaration of the
function, including explanations of the parameters (shown
as box4/circlecopyrtin the ﬁgure); (2) the functionality of the method
(boxes2/circlecopyrtand3/circlecopyrt); (3) the exception handling logic (box 5/circlecopyrt).
We take box 2/circlecopyrtas an example to demonstrate our idea. The
parse tree generated by an oﬀ-the-shelf NLP tool is shown
in Fig. 2 (b). We perform further processing on the tree,
including transforming the tree structure to accommodate
ambiguity and recognizing parameters, to produce a tree-
likeintermediate representation (IR) as shown in Fig. 2 (c).
We then try to use the primitive tree patterns to create a
tilingof theIR.Inthis case, thetree template ofthe insert
primitive (as shown in Fig. 2 (d)) alone can cover the whole
IR tree. As such, the model code for the sentence is gener-
381insert
o1
at
o2(tag:this)elements[o2] = o1
(d) Insert Primitiveinsert
element
atindex (tag:this)
(c) IR for “Insert” sentenceNP
NPPPNPPP
NPVPS
NP
list this in position speciﬁed the at element speciﬁed the InsertsVBZ
DT VBN NN IN DT VBN NN IN DT NN
(b) Syntactic parse tree of the “Insert” sentence (box 2 in subgraph a)
Method speciﬁc
informationPrimitive provides 
code templatepublic void add(int index, E element)
Inserts the speciﬁed element at the speciﬁed position in this list.
Shifts the element curr
ently at that position (if any), 
and any subsequent elements to the right (adds one to their indices).
Parameters :
index:      index at which the speciﬁed element is to be inserted
element: element to be inserted
Throws :
IndexOutOfBoundsException: if the index is out of range  (index<0 || index>size( ))
(a) The partial comments of the method add of the class ArrayList1
2
3
4
5    elements[index] = element;    size = size+1;
    for ( int i = size - 1; i > index; i—)
        elements[i] = elements[i-1];    if (index < 0 || index > size())
        throw new IndexOutOfBoundsException();public void add(int index, E element ) {
}6
0987
(f
) Generated Model
elements[index] = element;
(e) Model for “Insert” sentence
Figure 2: Motivation example.
a
ted as in 9/circlecopyrt. Note that the parse tree nodes that denote the
variable names (i.e., “ element ” and “index”) allow us to
instantiate the parameters o1ando2in the primitive. The
code for the other sentence and the exception handling de-
scription is similarly generated. Note that boxes 8/circlecopyrtand9/circlecopyrt
have a diﬀerent order than that in the text. Our technique
generates multiple candidates including those with diﬀerent
orders and use testing to prune out the invalid ones.
3. DESIGN
Fig. 3 gives the overarching design of our approach. The
whole system takes a Javadoc as input, and uses the Javadoc
parser, pre-processor, text analysis engine, tree transformer,
intermediate representation (IR)generator, model generator
and model validator to generate models. In particular, the
Javadoc parser takes the Javadoc in a structured HTML
format as input and extracts contents from both class and
method description. The pre-processor performs some syn-
onym analysis and enhances the extracted sentences. Then
a tree structure is generated for each sentence by the text
analysis engine which leverages Stanford Parser [16, 24] and
domain speciﬁc tags to perform the natural language pro-
cessing (NLP). After that, the tree transformer automati-
cally generates variants for some of those tree structures.
These variants represent the diﬀerent interpretations of the
sentence in the context of Java programming. The variants
as well as the original tree structure are processed by the IR
generator, which identiﬁes and marks function parameters
in each tree structure. The model generator searches for
tilings of the IRs using the tree patterns of the pre-deﬁned
primitives, and eventually generates multiple model candi-
dates for each function. These candidates are passed to the
model validator to ﬁlter out candidates that behave diﬀer-
ently from the original function. The model validator makes
use of Randoop [20] to automatically generate test cases.
The ﬁrst candidate that passes all the test cases is the re-
sulting class model.
3.1 Pre-processor
The pre-processor accepts the descriptions extracted by
the Javadoc parser and performs three kinds of analysis.JavadocJavadoc 
Pa
rserText Analysis 
En
gine
Tree Node 
T
ransformerModel 
G
eneratorModel 
V
alidatorClass 
Mo
delPre-processor
IR 
G
enerator
Figure 3: Overview of our solution
T
able 1: The documentation of the method indexOf
of class ArrayList
public int indexOf(Objecto)
R
eturns the index of the ﬁrst occurrence of the speciﬁed element in
this list, or -1 if this list does not contain the element. More formally,
returns the lowest index isuch that (o==null ? get(i)==null :
o.equals(get(i))) , or -1 if there is no such index.
Speciﬁed by :
indexOf in interface List<E>
O
verrides :
indexOf in classAbstractList <E>
P
arameters :
o- element to search for
Returns :
the index of the ﬁrst occurrence of the speciﬁed element in this list,
or -1 if this list does not contain the element.
Equivalence Analysis. O ur pre-processor classiﬁes words
into pre-deﬁned semantic classes based on domain dictio-
naries. For example, the words “ adds”and“inserts ”are
semantically equivalent in method description. Classifying
them into one category can relieve the eﬀort of identifying
mappings for each word in the generated IRs.
Redundant Information Elimination. We attempt to
remove sentences that are used to elaborate other sentences.
Just to name a few, sentences starting with “ in other
words”, “namely”, “More formally ”, and “That is to
say” are explanations of other sentences, and they will be
eliminated in our system to prevent redundancy in analysis.
Sentence Augmentation. The sentences in return de-
scriptions and exception descriptions in Javadocs tend to be
incomplete, which makes it diﬃcult for the Stanford parser
to analyze. Our sentence augmentation component aims to
enhance those sentences for easy parsing.
For return descriptions, the verb“ return”is often omit-
382ted. For example, “ t rue if this list contained the
specified element ”is the return description for the re-
move (Object o) method in the ArrayList class. In this case,
our approach checks whether the sentence is under the tag
“return”in thecorrespondingJavadoc. Ifso, theverb“ Re-
turns”is added to the sentence.
For exception descriptions, the throw behavior and the
exception thrown are typically omitted. For example, “ if
this vector is empty ” is the exception description for
thelastElement () method in the Vectorclass. The verb
throwand the exception NoSuchElementException are
missing. In this case, our approach checks whether the sen-
tenceisunderthetag“ throw”inthecorrespondingJavadoc.
If so, the phrase“ Throws NoSuchElementException ”is
added to the sentence to augment the original sentence.
Note that the exception name can be easily extracted and
used in the augmentation (e.g. in Fig. 2 (a)).
3.2 Text Analysis Engine
The text analysis engine generates a grammatical tree
structure for each pre-processed sentence through natural
language processing. To do this, we leverage the state-of-
the-art Stanford Parser with domain speciﬁc tags.
The Stanford parser parses a sentence and determines
POStagsassociatedwithdiﬀerentwordsandphrases. These
tags are essential to the generation of the syntactic tree
structure for the sentence. There are some words that rep-
resent nouns in programming while representing adjectives
in general linguistics. For instance, in the sentence “ Re-
turns true if this list contains the specified
element. ”, “true” should be regarded as a noun in pro-
gramming rather than an adjective. But the Stanford parser
would identify it as an adjective. In addition, the Stan-
ford parser may incorrectly identify some words as nouns
while in fact they should be marked as verbs. For example,
“Returns ” in the mentioned description is a verb, but it
is incorrectly identiﬁed as a noun by the Stanford parser.
Therefore, a POS restricting component is added to the
Stanford parser to force it to use our pre-deﬁned tags for
some programming-speciﬁc words. Some pre-deﬁned tags
are as follows:
•noun: true/false/null
•verb: returns/sets/maps/copies
•adjective: reverse/next/more/empty
3.3 Tree Transformer
The tree transformer transforms the original tree struc-
ture generated by the Stanford parser to produce variants.
Each variant corresponds to a diﬀerent interpretation of the
sentence. We need to generate multiple interpretations of a
sentence due to ambiguities in natural languages, and con-
sequently we will generate multiple model candidates which
are passed to the model validator discussed in Section 3.6
to ﬁlter out incorrect models. Text analysis engines like
the Stanford parser are able to generate multiple trees with
diﬀerent semantics. Unfortunately these parsers cannot un-
derstand the real semantic of a sentence, especially when
domain knowledge is involved. The Stanford parser can gen-
eratekparse trees for a sentence with kgiven by the user.
However, thereis noway for the parser toguarantee thatthe
tree with the expected meaning is generated for a sentence
even with a large k.Returns
the head
of
this deque, or null if this deque is emptyIf this deque is empty, returns 
the head of this deque
, or 
null.
Returns
the head
of
this deque null if this deque is emptyReturns the head of this deque, 
or returns the head of null if this 
deque is empty
.
, or
Figure 4: Syntactic trees with unexpected meanings
T
ake the method description “ Returns the head of
this deque, or null if this deque is empty ”as an
example and we set the value of kas 20. None of the 20
parse trees generated by the Stanford parser gives the exact
meaning conveyed by this sentence in the context of pro-
gramming, which should be“ Returns the head of this
deque, or returns null if this deque is empty ”.
Two of the trees with unexpected meanings are shown in
Fig. 4. Some subtrees are summarized to be one tree node
to save space. For example, the left tree expresses“ If this
deque is empty, returns the head of this deque,
or null ”while the right tree conveys“ Returns the head
of this deque, or returns the head of null if
this deque is empty ”.
If the value of kis large, a lot of computation will be
wastedingeneratingandanalyzingtreeswithincorrectmean-
ings. Through an analysis of the generated trees, we found
that in most cases, the Stanford parser is good at recogniz-
ing individual phrases of a sentence in the context of pro-
gramming, and the places where ambiguity arises are those
phrases starting with “ , or”and“, and”. We also discov-
ered that by lifting up or pushing down the node“ , or”or
“, and”and its right siblings for only a few number of times
in the tree produced by the Stanford parser1, we can get the
tree which conveys the expected meaning of the sentences.
Thus we propose Algorithm 1 to transform the parse tree
from the Stanford parser to produce a set of variants by
repositioning only the conjunctive nodes. This algorithm
takes the root of the tree generated by the Stanford parser
as an input and produces a set of tree variants, represented
byvariantSet .
First,variantSet is initialized to contain only the original
treeroot(lines 1-2). Then, the main loop is executed to gen-
erate variants (lines 3-11). In each iteration, lines 5-7 make
a transformation of each tree in variantSet to get variants
and add them back to variantSet . The transformation pro-
cess is the function transform() shown in Fig. 5. When the
set does not change any more (line 8), a ﬁx-point is reached,
meaning all possible variants have been identiﬁed, and the
process terminates.
As can be seen in Fig. 5, for each tree node n, if it is a
node representing “ ,” and its right sibling node represents
“or”or“and”, the function transforms the tree by lifting up
and pushing down the node nand all its right siblings. The
1The parser by default returns a single tree that it considers
havingthe highest probability of denotingthe real semantics
of the sentence.
383transform (r ,n) =c∈Children (n)/uniondisplay
ctransform (r,c)∪/braceleftbiggliftUp(r,n)∪pushDown (r,n) : a“,”node followed by a“or”or“and”node
∅ : otherwise
Figure 5: Function transformation
Algorithm 1 T ransforming one Tree Node: function trans-
formTree
Input: r oot- root of the original tree node
Output: v ariantSet - a set of variants of the original tree node
1:v ariantSet←∅
2:variantSet←variantSet∪root
3:whiletruedo
4:oldSet←variantSet
5:for all tree∈variantSet do
6: variantSet←variantSet∪transform (tree,tree )
7:end for
8:ifvariantSet ==oldSetthen
9: break
10:end if
11:end while
12:return variantSet ;
R
OR/
A
ND!P
C1R
OR/
A
ND,P
C1 C2
C2
(a) Lifting up.R
OR/
A
ND,P1
C1P2R
OR/
A
ND!P1
C1
 P2
(b) Pushing down.
F
igure 6: Lifting up and pushing down nodes. The
shaded nodes are repositioned.
lifting up operation is shown in Fig. 6(a) while the pushing
down operation is shown in Fig. 6(b). In other cases, the
tree keeps unchanged.
Consider the documentation in Table 1. The left tree
in Fig. 7 is generated by the Stanford parser for the sen-
tence“Returns the index of the first occurrence
of the specified element in this list, or -1 if
this list does not contain the element ”. From
this tree, we get the semantics “ Returns the index of
the first occurrence of the specified element
in this list, or returns the index of the first
occurrence of -1 if this list does not contain
the element ”. which is not the expected meaning of this
comment sentence. By lifting up the nodes “ ,”, “or”, and
“-1 if this list does not contain the element ”
on the seventh layer of the left tree ﬁve times, we get the
tree on the right, which conveys the exact meaning of this
comment. Note that since our tool does not know which
variant represents the intended meaning, it generates all of
them and then selects the right one through testing. From
Fig. 7, we can see that the number of variants generated by
lifting up the left tree is ﬁve which is acceptable.
3.4 Intermediate Representation Generator
Ourintermediaterepresentationgeneratormanipulatestrees
generated by the tree transformer to constructs IRs by sub-
stituting subtrees, identifying parameters, and adding labels
based on programming domain knowledge. We cannot di-
rectly translate a generated tree to code unless we associate
tree nodes with code artifacts. To achieve this, our IR gen-
erator performs two major tasks: (1) Parameter recognition,
which identiﬁes parameters; (2) Loop and conditional struc-
ture recognition.
Parameter Recognition: This component recognizes pa-Returns
the index
of
the first occurrenceReturns the index of the 
first occurrence of the 
specified element in this 
list, 
or returns the index 
of the first occurrence of  
-1 if this list does not 
contain the element. of of
the specified element 
in this list, ,-1 if this list does not 
contain the element
Returns
the index
of
the first occurrenceReturns the index of the 
first occurrence of the 
specified element in this 
list, 
or returns  -1 if this 
list does not contain the 
element. of ofor
or,-1 if this list does not 
contain the element
the specified element 
in this list
Figure 7: Lifting up nodes example.
r
ameters in method descriptions. Javadocs refer to parame-
ters using several diﬀerent descriptions which are summa-
rized as patterns shown in column “Pattern” in Table 2.
However, due to the complexity and ambiguity of natural
languages, these patterns may not always imply parameters.
In some cases, even the occurrences of the same word as the
parameter name do not indicate the corresponding param-
eter. For example, the description of the method set(int
index, E element) in the class ArrayList is“replaces the
element at the specified position in this list
with the specified element ”. The phase ‘ the el-
ement’ does not refer to the parameter element, but ‘the
specified element ’ does. Our solution is again to gener-
ate all possibilities and let the model validator to determine
the right mappings (of parse tree nodes to parameters).
Algorithm 2 describes the process of recognizing place-
holders of parameters in method description. It takes three
arguments, i.e., all theparameters ofthe methodbeingmod-
eledparams, the root of the parse tree root, and a predeﬁned
list of synonyms synonyms . The output of this algorithm
is represented as irSet, which is a set of IRs generated by
recognizing parameters of the tree. The algorithm works
as follows. The ﬁrst step is to initialize irSetto contain the
original tree root(lines 1-2). Next, the main loop is executed
to recognize parameters in root(lines 3-9). The algorithm
recognizes the parameters one by one with each iteration re-
sponsible for recognizingone parameter on thetree(s). Since
there are multiple possible places that indicate a parameter,
multiple trees may be generated by associating the parame-
ter with diﬀerent tree nodes. Each iteration in the loop 5-6
handles one tree irfrom the previous round. The result-
ing trees are stored in newSet. Recognizing a variable on a
tree is done through a recursive function traverse() shown
in Fig. 8, which tries to match the parameter with each tree
node. Since the parameter can match multiple nodes (cor-
responding to that there are multiple words that seem to
mean the parameter), the function may produce multiple
trees, each representing one possible match. At the end,
each tree in irSetis a tree with recognized parameters and
each tree node represents at most one parameter.
384Algorithm 2 I dentifying Parameters in one Tree: function
identifyParams
Input: p arams- all the parameters of the method being modeled
root- root of the original tree node
synonyms - the predeﬁned list of synonyms
Output: i rSet - a set of IRs after recognizing parameters
1:i rSet←∅
2:irSet←irSet∪root
3:for all param∈params do
4:newSet←∅
5:for all ir∈irSetdo
6: newSet←newSet∪traverse (ir,param )
7:end for
8:irSet←newSet
9:end for
10:return irSet;
traverse (t ,p ) =c∈Children (t)/uniondisplay
ctraverse (c,p)∪recParam (t,p)
Figure 8: The function traverse
Function recParam() in Fig. 8 is to recognize a parameter
by pattern matching. The patterns are described in Table 2.
Particularly, if a subtree rooted at tmatches a tree pattern
in the ﬁrst column and satisﬁes the condition in the second
column, tis replaced with a node representing the parame-
ter. The third column shows some examples. For example,
the rule in the ﬁrst row means that, if a subtree denoting a
phase“the w 1w2”is observed and the concatenation of w1
andw2is a synonym of the parameter name. The subtree
is replaced with a node representing the parameter. Other
rules are similarly deﬁned.
It is worthy mentioning that we take special care of the
word“this”as it often has special meanings in our context.
In particular, for every occurrence of phrase “ this WORD ”
with WORD being the class name or its abbreviation, we
label the corresponding tree node with a special tag this, in-
dicating the code (to be generated) operates on the receiver
object. For example, “ this list ”is used to represent the
receiverArrayList object in thedocumentationof ArrayList .
Take the right tree in Fig. 7 as an example. According to
the second rule, our approach substitutes the subtree repre-
senting“the specified element ”with a node represent-
ing variable o, which is further tagged with “ this” due to
the phrase“ in this list ”.
Structure Recognition: This component recognizes pro-
grammingstructuresindicatedinmethoddescriptions. Doc-
umentation descriptions use some special words to specify
how the behavior is carried out, such as the condition under
which the behavior will execute and how many times the
behavior should execute. These restrictions are projected to
programming structures, like loops and conditionals, which
are vital for generating models. Our approach recognizes
these structures by recognizing subtrees containing the spe-
cial words, and substituting subtrees as well as adding tags.
Loop structure. Plurals and singular nouns modiﬁed by
“each”tend to imply that the behavior should be executed
for multiple times, which indicates a loop structure. For
example, the phrase “ all of the elements ” in the sen-
tence “Inserts all of the elements in the speci-
fied collection into this list, starting at the
specified position ” of the method addAll(int index,
Collection <? extends E> c)inArrayList , indicates thatthe insert operation should execute multiple times. Thus
the model for this behavior must have a loop structure. In
this case, our approach adds a looptag to the IR and sub-
stitutes the subtree representing“ all of the elements ”
with a node representing“ elements ”. Some phrases in nat-
ural language do not explicitly indicate a loop structure.
Instead, they can imply the iteration order. For example,
the subtree representing“ the first occurrence of ” in
Fig. 9(a) implies that if there is a loop structure for this
behavior, it should iterate from left to right. In this case,
our approach adds a ltrtag to the IR and trims the subtree
representing“ the first occurrence of ”.
Conditional structure. Words like“ if”and“when”in natu-
ral languages indicate a conditional statement in program-
ming while words like“ otherwise ”indicate an else branch.
Our approach adds tags ifandelseto the subtree that is
modiﬁed by these words. For example in Fig. 9(a), an if
tag is added to the subtree representing“ -1”. In addition,
our technique recognizes whether the description is aﬃrma-
tive or negative to determine the condition of the ifstate-
ment. For example, the behavior “ contain ” in Fig. 9(a)
is modiﬁed by“ does not ”. It means that the condition of
theifstatement should be the negation of the result of the
contains behavior. In such cases, our approach trims the
subtree representing“ does not ”and adds a ﬂag“ -”to the
node representing“ contain ”. For instance, we get the IR
in Fig. 9(b) by recognizing structures of the IR in Fig. 9(a).
3.5 Model Generator
In this section, we introduce our method of generating
models based on IRs. For each IR, a tiling by the tree pat-
terns of primitives is identiﬁed. The corresponding code
snippets of the primitives are assembled to constitute the
model of the IR. The code snippets for all the IRs in a
methoddescriptionarefurtherintegratedtogainthemethod
model. The class model is eventually generated based on
the individual method models and the class information col-
lected earlier. Since one sentence can have multiple IRs,
multiple method/class model candidates are generated.
For all the Java container classes (e.g. lists, queues, and
stacks), weobservethatitissuﬃcienttouseaone-dimensional
array to model them. As such, many of our primitives
are essentially operations manipulating the underlying one-
dimensional array. Using primitives avoids generating mod-
els from simple and low-level expressions and statements,
which requires exploring a very large search space for the
proper combination. Table 3presents partof the pre-deﬁned
primitives. Each primitive has a tree pattern and a piece of
code template (on the underlying one-dimensional array).
The tree pattern describes the syntactic structure of the de-
scription of the primitive. It is used to cover part of an IR
tree to create a tiling.
Each cell in the table includes the name of the primitive,
the description, and the tree pattern. The corresponding
code templates are elided. Each primitive has a set of pa-
rameters, which are also denoted in the tree pattern. The
primitive insert(o1,o2) represents thefunctionalityof insert-
ing an object o1too2. The corresponding code template
implements this functionality on the one-dimensional array.
Its tree pattern essentially describes how such functionality
is expressed in a natural language and hence can be used for
IR tree tiling. We also have other primitives such as copy
andapply. We have a total of 12 primitives.
385Table 2: Patterns for Labeling a Parameter n. Symbols w,w1andw2denote words.
Pattern Condition Example
the w1 w2w1,w2a nd nare synonymous setSize(int newSize )inStringBuﬀer : “Throws ArrayIndexOut-
OfBoundsException if the new size is negative ”.
the specified wwa ndnare synonymous get(intindex)inArrayList : “Returns the element at the speciﬁed
position in this list ”.
the w specifiedwa ndnare synonymous add(int index, Attribute object) inAttributeList : “Inserts the
attribute speciﬁed as an element at the position speciﬁed ”.
the specified ww= = the type of n add(Attribute object) inAttributeList : “Addsthe Attribute spec-
iﬁedas the last element of the list ”.
the w specifiedw= = the type of n append( StringBuﬀer sb)inStringBuﬀer : “Appends the speciﬁed
StringBuﬀer to this sequence ”.
the w argumentw= = the type of n append( charc)inStringBuﬀer : “Appends the string represen-
tation of the char argument to this sequence ”.
the argumentremoveElement(Object obj) i nVector: “Removes the ﬁrst
(lowest-indexed) occurrence of the argument from this vector ”.
the wwa ndnare synonymous set(int index, E element )inArrayList : “Replaces the element at
the speciﬁed position in this list with the speciﬁed element ”.
return
the index
contain!1"or
the element o o
tag:thisif
this list
does notof
the ﬁrst 
occurr
ence
of
(a) IRs after parameter recognitionreturn
the index
tag: ltr
contain
tag: -
-1
tag: if!or
the element o o
tag:thisofthis list
(b) IRs after structure recognitionint index1 = -1;
if(o == null
) {
    for(int i=0; i<size; i++) {
        if(elements[i] == null) {
    index1 = i;
    break;
}
    }
} else {
    for(int i=0; i<size; i++) {
        if(o.equals(elements[i])) {
    index1 = i;
    break;
}
    }
}int index2 = -1;
if(o == null
) {
    for(int i=0; i<size; i++) {
        if(elements[i] == null) {
    index2 = i;
    break;
}
    }
} else {
    for(int i=0; i<size; i++) {
        if(o.equals(elements[i])) {
    index2 = i;
    break;
}
    }
}
if (index2 == -1) return -1;
else r
eturn index1;1 2
3
(c) Model
F
igure 9: IRs and Model.
The tiling algorithm is very similar to that used in com-
piler code generation [10]. It is a greedy algorithm that tries
to cover an IR tree from the root to the leaves. Particularly,
it ﬁrst tries to cover a subtree starting from the root (of the
original tree). It then tries to cover the remaining parts of
the tree, until a tiling is found.
Example. Consider the IR tree in Fig. 9(b). The algorithm
ﬁrst covers the top part of the tree with the pattern of the
primitive return, which returns diﬀerent values depending
on a condition. It then covers the left sub-tree with the
indexOf primitive and the right sub-tree with the contain
primitive. The tiling is shown in Fig. 10. The resulting code
is shown in Fig. 9 with the order of 1/circlecopyrt,2/circlecopyrt, and then 3/circlecopyrt.
Observe that the code generation is bottom up: boxes 1/circlecopyrt
and2/circlecopyrtfor theindexOf andcontainprimitives, respectively,
andthenbox 3/circlecopyrtisforreturn. Alsoobservethatalthoughthe
generated model functions correctly, it is a bit redundant as
the code templates for the ﬁrst two primitives are essentially
the same. This redundancy is easily precluded through a
post-processing step.
We will assemble code snippets of each IR to form the
method model, during which procedure, our technique also
explores the diﬀerent orders of the code snippets.
3.6 Model Validator
As pointed outbefore, our approach can generate multiple
model candidates due to the ambiguity of natural languages
and the limitation of current NLP tools. But most of them
have inconsistent behaviors with the original library, whichreturn
the index
tag: ltr
ofo
tag: thiscontain
tag: -this-1
tag: if!or
oL
L: C1<-indexOf(o,this,ltr)  R: C2<- (-contain(this, o))  T: C3<-return(C1,-1,C2)T
R
Figure 10: Tiling an intermediate representation.
m
akes it necessary to ﬁlter out the inconsistent model can-
didates. Our model validator accepts multiple model can-
didates and excludes the candidates that behave diﬀerently
from the original method. We leverage the existing work,
Randoop [20], which can automatically generate test suites
for Java classes to help us check whether the generated mod-
els preserve the behavior of the original method(s). For each
class, we generate 720 test cases on average.
4. EV ALUATION
We have implemented a prototype and conducted a set of
experiments to evaluate the prototype. In our evaluation,
we focus on two aspects:
1. The eﬀectiveness of the model generation technique.
2. Using the generated models in other analyses.
386Table 3: Primitives
copy(o1,o2) : Copy the content
speciﬁed by o1into the destina-
tiono2.
copyo1intoo2return(o1,o2,o3) : Return o2if
condition o3holds,o1otherwise.
return o1,or
o2o3throw(o) : Throw an exception
speciﬁed by o.
throw oremove(o) : Remove ob-
jectofrom the container.
remove o( tag:this)
insert(o1,o2) : Insert object o1at
a location speciﬁed by o2.
insert
o1
at/to o2( tag:this)isEmpty (o) : Check whether ois
(not) an empty container.
o
(−) ?is emptyshift(o) : Left/right shift an ob-
jecto.
shift
o1
to left/rightset(o1,o2) : Set a ﬁeld o1of
the receiver object to o2.
set
o1( tag:this)
with o2
apply(o1,o2) : Appy a primitive
operation o1to an object o2.
apply
o1
to/for o2contain (o1,o2) : Check whether
the container o1(does not) con-
tains the object o2.
o1
(−) ?contain o2indexOf (o1,o2,t) : Get the in-
dex ofo1ino2by searching el-
ements contained in o2.
the index(tag:t)
of o1(t ag:o2)elementAt (i) : get the element
at the position speciﬁed by in-
dexi.
the element
at i
Table 4: Overall Result
Class #T #M % #C GT VT #CN
ArrayList 34 29 85.29% 128 5.85 13876 490
V
ector 54 46 85.19% 512 8.22 52833 500
S
tack 6 5 83.33% 12.52 102 1
A
rrayDeque 36 35 97.22% 647.26 6922 756
L
inkedList 42 41 97.62% 8192 8.12 877281 545
H
ashMap 28 23 82.14% 128 5.85 14447 1337
L
inkedHashMap 15 14 93.33% 14.35 108 667
H
ashSet 13 12 92.31% 13.06 107 726
L
inkedHashSet 5 4 80.00% 12.50 107 641
AttributeList 15 11 73.33% 22.57 218 638
RoleList 14 9 64.29% 22.09 109 836
R
oleUnresolvedList 14 9 64.29% 11.81 109 886
StringBuffer 54 40 74.07% 114.81 107 950
S
tringBuilder 54 40 74.07% 113.51 109 1098
Summary 397 326 82.12%
If not speciﬁed in the following sections, the evaluation
w
as conducted on a machine with Intel(R) CoreTMi7-3770
CPU (3.4 GHz) and 8GB main memory. The operating
system is Ubuntu 12.04, and the JDK version is 7.
4.1 Effectiveness in Model Generation
To evaluate the eﬀectiveness of our model generation, we
applied it to 14 Java container-like classes.
Table 4 shows the results. Column“Class”lists the names
of the modeled classes, which are grouped by diﬀerent pack-
ages. The packages from top to bottom are respectively
java.util,javax.management ,javax.management.relation , and
java.lang . For each class, column“#T”lists the total num-
berofmethodsoftheoriginal JDKclass. Column“#M”lists
the number of methods that our tool can successfully model.
Here the models are functionally equivalent to the original
methods. Column“%”lists the ratio of modeled methods to
total methods. Column“#C”lists the number of model can-
didates for each class. Column “GT” lists the time used to
generate class model candidates in seconds. Column “VT”
lists the time used to validate class models using Randoop
in seconds. Column“#CN”lists the average number of test
cases generated by Randoop. Row“Summary”lists the total
numbers for columns“#T”and“#M”, and gives the average
of column“%”.
From the results in Table 4, we have the following obser-
vations. First, we can generate models for most methods
in these classes, which indicates the eﬀectiveness of our ap-
proach. For the last ﬁve classes, the percentage is relatively
low. The low ratios ofthe classes AttributeList ,RoleList and
RoleUnresolvedList result from the incompleteness of their
documentations. For example, the method add(int index,Object element) inAttributeList should throw an exception
(java.lang.IndexOutOfBoundsException ) whenindexis less
than 0 or greater than the length of this list. But no sen-
tences describe this behavior which makes it impossible for
our tool to generate the corresponding code. Our approach
handles each sentence separately while descriptions of some
methods use several sentences todepictone primitive behav-
ior. This leads to the low ratios of the classes StringBuﬀer ,
andStringBuilder . Take the method insert(int index, char[]
str, int oﬀset, int len) ofStringBuﬀer as an example. The
insertion operation is described using the following three
sentences: “ Inserts the string representation of
a subarray of the str array argument into this
sequence. The subarray begins at the specified
offset and extends len chars. The characters of
the subarray are inserted into this sequence at
the position indicated by index ”. Currently, our ap-
proach cannot handle such cases. We plan to correlate mul-
tiple sentences in the future. Second, both the time used
to generate models and the time used to verify models are
acceptable. The time used to generate models for String-
BuﬀerandStringBuilder are much longer than that of the
otherclasses, whichresults from thefact thatthereare much
more sentences to be analyzed in these two classes. Much
of the time used to verify models is spent in generating test
cases by Randoop. The average time used to generate a se-
ries of test cases for one class model is 108 seconds. Third,
the validation time and the number of the candidates has
linear relationship.
The pie charts in Fig 11 show more statistics. Fig. 11(a)
gives the distribution of the tree variants derived from a tree
due to the lift-up and push-down transformations. Each
tree corresponds to one sentence. Fig. 11(b) presents the
distribution of the number of IRs derived from a tree af-
ter parameter recognition. Fig. 11(c) shows the distribution
of the numbers of models generated for a sentence. From
these three pie charts, we can see that the number of gen-
erated models of each sentence is not simply the product
of the number of tree variants and the number of IRs of
each tree variant. The reason is that we cannot gener-
ate models for some IRs which cannot be tiled with our
proposed primitives. Fig. 11(d) shows the distribution of
the number of model candidates generated for each method.
Only one model is generated for a method in the majority
38795.68%0.36%
0.72%1.80%1.44%
1
5
6
7
8
(a) #tree vari-
a
nts97.62%1.30% 1.08%
1
2
8
(b) #IRs
94.78%4.86%0.36%
1
2
4
(c) #models of
e
ach sentence88.48%1.52%10.00%
1
2
4
(d) #models of
e
ach method
Figure 11: Distributions
5.24%
4
3.33%
24.76%10.00%6.67%9.05%0.95%
0 ~ 1
1 ~ 2
2 ~ 3
3 ~ 4
4 ~ 5
5 ~ 13
30 ~ 32
Figure 12: Line of code comparison with JDK
c
ases(88.48%), and the maximum number of models for a
method is 4 with a percentage of 1.52%, which is acceptable
for the validation step.
Fig. 12 shows the comparison of the line of code between
our models and the original JDK. We counted the line of
code for all functions including its dependencies, and di-
vided the line of code of the original JDK by ours. From
the chart we can see that for normal cases, the original JDK
is 1 to 13 times larger than our models. There are some
cases that our models are larger because some JDK meth-
ods calls JNI functions like System.arrayCopy() , while our
models are completely implemented in Java. And there are
some extremal cases that the code size is large, such as the
model for HashMap.clone() , which needs to check bound-
aries and clone all objects inside it.
Fig. 13 shows the distribution of the appearances of the
primitives in documentation. We observe that add,remove
andshifttake a large portion because our models focus on
container classes. The primitive throwtakes a large por-
tion as well because Java API document clearly deﬁnes this
behavior for many functions.
Fig. 14 shows the numberof primitives used tomodel each
method. Most methods can be constructed by only a few
primitives.
4.2 Our Models in Static Taint Analysis
To evaluate the eﬀectiveness and eﬃciency of our models
in static analysis, we conduct a taint analysis on Android
that detects the undesirable disclosures of user inputs to
1.84% 1.84% 2 .30%2.76% 2.76% 5.07%5.53%7.37%9.22%12.44%17.97%30.88%
0%10%20%30%40%
Figure 13: Distribution of primitives in documents25.5%3
9.2%
17.6% 15.7%
2.0%
0%20%40%60%
1 2 3 4 5
Figure 14: # of primitives per method model
36
2
4
15
5 52
010203040
 ~ 10% 10% ~ 20% 20% ~ 30% 30% ~ 40% 40% ~ 50% 50% ~#Apps
Figure 15: Eﬃciency improvement distribution
p
ublic channels on 96 apps that were previously known to
have user input disclosures. We limit the sinks to Inter-
net access and Log writing, and set a 30 minutes timeout
for each app in our experiment. We ﬁrst run the analysis
with the original JDK source code and then replace part of
the JDK with our models and then compare the number of
information leak warnings reported and the performance.
Among the 96 apps, 1 app does not have any use of the
modeled methods and is hence excluded; 8 other apps are
also eliminated because they time out or run out of memory
using both our models and the JDK implementation. The
results of the remaining 87 apps are summarized as follows.
Eﬀectiveness. The analysis reports the same set of infor-
mation leak warnings for both versions for almost all apps,
except app com.yes123.mobile which is reported to have 16
paths using our models and 14 paths using JDK. We manu-
ally inspect the diﬀerences and ﬁnd that the two paths con-
tain invocations of ArrayList.toArray(Object[]) . The JDK
implementation calls a JNI method System.arrayCopy() to
copy data from the 1st argument to the 3rd argument. Our
model provides an implementation of the functionality such
that static analysis is not blocked by the absence of Java
code. We have also found 51 of the 87 apps have similar
JNI calls such that the static analysis cannot handle those
calls properly. However, since those calls are not on any
leak path from the source to the sink, they do not induce
diﬀerences in the bug report.
Eﬃciency. Fig. 15 shows the performance comparison by
presenting the distribution of the improvement. The per-
formance improves more than 10% for 51 apps by using our
model. The maximum improvement is 50% and the aver-
age is 16%, even with a small portion of the JDK library
replaced by our models.
4.3 Our Models in Dynamic Slicing
In addition to the static taint analysis on Android appli-
cations discussed in Section 4.2, we also evaluate our mod-
els in dynamic program analysis techniques, namely, Java
dynamic slicing. We utilize JavaSlicer [4] and 5 benchmark
programs including SPECJBB [7], FunkyJFilter [2], ListAp-
pend [6], batik in DaCapo [1], and some unit tests with
JTreg [5]. JavaSlicer is a dynamic slicing tool. We ﬁrst run
the program with JavaSlicer and inputs. During runtime,
JavaSlicer collects traces of the execution of this program.
Then for each benchmark program, we choose one variable
as the input, and JavaSlicer reports the number of unique
Java bytecode instructions and all the detailed instructions
in the dynamic slice for this given variable. Note that an in-
struction that gets executed multiple times is reported only
once. For all the 5 benchmark programs, we run them with
parameters given as examples in their manuals, and choose
the generated result valueas the starting pointof the slicing.
388Table 5: Dynamic slicing results
Naive approach Our model
slice size time slice size time
SPECJBB 564 1.76 393 0.73
FunkyJFilter 629 2.18 5 2.16
ListAppend 7,050 12.79 504 5.9
Batik 35,721 1,973.65 5,516 295.77
Unit Tests 32 0.39 3 0.36
All experiments are conducted successfully.
T
o demonstrate the results, we compare our models with
a naive approach that considers any output of a method
depends on all the inputs. The results are shown in Ta-
ble 5. The ﬁrst column lists the names of the benchmark
programs. For each program, we show the size of the slice
and the running time of using the naive approach (column 2
and 3, respectively), and using our models (column 4 and 5,
respectively). As shown in the table, for all the programs,
JavaSlicer produces slices of smaller size with our models
than the naive approach. Due to the characters of these dif-
ferent benchmarks, we get diﬀerent results. On average, the
slice size is32timessmaller duetothehigherprecisionofour
models. For the same reason, it takes less time to produce
slices using our models than using the naive approach.
5. LIMITATIONS
Ourtechniqueheavilydependson thequalityof documen-
tation. If the documentation is incomplete or uses strange
syntax or wordings, our tool may not generate the correct
models. This is a general limitation for many NLP based
techniques [37, 30]. Our technique relies on a set of syn-
tactic patterns to recognize parameters and primitive oper-
ations. These patterns are mainly derived from Javedocs.
They may not be generally applicable. However, we argue
that our technique is general in principle and it is a valu-
able step towards automated model generation, which is a
hard problem in general. We envision the writing style of
documentation may not change as frequently as the library
implementation. As a result, our technique can serve as an
automated approach to quickly generate models for a large
number of library functions, as demonstrated by our results.
In the future, it may be a beneﬁcial practice to enforce a
ﬁxed documentation style.
Furthermore,ourcurrentstudylargelyfocuses oncontainer-
like libraries as they are the most widely used category. It
is very diﬃcult to model the precise functionalities of some
special-purpose libraries such as math libraries. However,
we also observe that having the precise models for those li-
braries are unnecessary in many applications. For example,
the input-output dependencies of most math library func-
tions are very simple, despite their complex computation.
It is also possible that the test cases used in model val-
idation are not suﬃcient so that we admit some incorrect
models. Even though we have not encountered such prob-
lems in our experience, it would be interesting to use more
rigorous validation techniquessuchas equivalencechecking.
6. RELATED WORK
Our approach is related to previous works closely on two
areas: documentation analysis and environment modeling.
Documentation analysis. Previous researchers analyze
natural language documents for many purposes. [22] pro-
poses methods to infer method speciﬁcations from API de-scriptions. [37, 14] try to detect code-document inconsis-
tencies by leveraging NLP and program analysis. [36] gives
programmers suggestions for usage of APIs based on mining
the usage patterns. [28, 25] analyze bug reports to remove
duplicates. [26] tries to help generate use cases in real world
development by inspecting language documents. [30, 29] use
commentswithothertechniquestodetectinconsistenciesbe-
tween comments and code. In contrast to these approaches,
our approach aims to help program analysis by constructing
models forlibraries. WeonlyuseJavadocs, buttheapproach
can be easily expanded to other types of natural language
documents, e.g. comments, bug reports.
Environment modeling. To reduce manual eﬀorts for
environment models, various approaches [11, 27] have been
proposed. [32]derivesenvironmentmodelsfromuser-provided
environment assumptions which capture ordering relation-
ships among program actions. OCSEGen [31] is an envi-
ronment generation tool for open components and systems,
which generates both drivers and environment stubs by an-
alyzing Java byte-code. Modgen [12] makes uses of program
slicing to generate an abstract model of a class, and it fo-
cuses on optimization of library classes by reducing their
complexity. [33] discusses these two mentioned tools on how
they can be applied to generate Android environment mod-
els. [23] acquires the input-output speciﬁcation by sampling
given binary implementations of the methods being mod-
eled, and uses SMT solver to construct models which satisfy
the speciﬁcation. Unlike previous approaches, we automat-
ically construct environment models by analyzing Javadocs
which give abundant information about the behaviors of
each library method.
7. CONCLUSION
Based on the idea of applying natural language process-
ing techniques and program analysis techniques to model
libraries, we identify and overcome the challenges of apply-
ing NLP techniques on real code generation, and build a
prototype of a modeling tool that can automatically gen-
erate simpliﬁed code for Java container-like libraries from
Javadocs. On average, the size of the generated model is
only one third of that of the original code base. We also ap-
ply our technique to help other program analysis that needs
reasoning software runtime environment. The results show
that our models can help both static and dynamic analysis.
8. ACKNOWLEDGEMENT
We thank the anonymous reviewers for their construc-
tive comments. This research was partially supported by
NSF grants CCF-1320326, CCF-0845870, CCF-0953759 and
CCF-1319705, and the CAS/SAFEA international Partner-
ship Program for Creative Research Teams, NSERC and an
Ontario Early Researcher Award, National Key Basic Re-
search Program of China No.2014CB340703, National Sci-
ence Foundation of China No.61561146394, No.91318301,
No.91418204, No.61321491 and No.61472179. Any opinions,
ﬁndings, and conclusions in this paper are those of the au-
thors only and do not necessarily reﬂect the views of our
sponsors.
9. REFERENCES
[1] Dacapo. http://dacapobench.org/.
389[2] Funkyjﬁlter benchmark.
h
ttps://github.com/olim7t/java-benchmarks/blob/
master/src/main.java/FunkyJFilterBenchmark.java.
[3] J2SE’s javadoc.
http://docs.oracle.com/javase/8/docs/api/.
[4] Javaslicer. https://github.com/hammacher/javaslicer.
[5] Jtreg. http://openjdk.java.net/jtreg/.
[6] Listappend benchmark.
https://github.com/olim7t/java-benchmarks/blob/
master/src/main.java/ListAppendBenchmark.java.
[7] Specjbb. https://www.spec.org/.
[8] The stanford natural language processing group.
http://nlp.stanford.edu/software/lex-parser.shtml,
1999.
[9] H. Agrawal and J. R. Horgan. Dynamic program
slicing. In ACM SIGPLAN Notices . ACM, 1990.
[10] W. A. Andrew and P. Jens. Modern compiler
implementation in java, 2002.
[11] C. Cadar, D. Dunbar, and D. R. Engler. Klee:
Unassisted and automatic generation of high-coverage
tests for complex systems programs. In OSDI,
volume 8, pages 209–224, 2008.
[12] M. Ceccarello and O. Tkachuk. Automated generation
of model classes for java pathﬁnder. ACM SIGSOFT
Software Engineering Notes , 39(1):1–5, 2014.
[13] D. Cristian Cadar and D. Dunbar. Klee: Unassisted
and automatic generation of high-coverage tests for
complex systems programs. OSDI, San Diego, CA,
USA (December 2008) , 2008.
[14] J. Henkel, C. Reichenbach, and A. Diwan. Discovering
documentation for java container classes. Software
Engineering, IEEE Transactions on , 33(8):526–543,
2007.
[15] D. Jurafsky and J. H. Martin. Speech and language
processing: An introduction to natural language
processing, computational linguistics, and speech
recognition.
[16] D. Klein and C. D. Manning. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1 ,
pages 423–430. Association for Computational
Linguistics, 2003.
[17] C. D. Manning and H. Schiitze. Foundations of
statistical natural language processing.
[18] A. C. Myers. Jﬂow: Practical mostly-static
information ﬂow control. In Proceedings of the 26th
ACM SIGPLAN-SIGACT symposium on Principles of
programming languages , pages 228–241. ACM, 1999.
[19] J. Newsome and D. Song. Dynamic taint analysis for
automatic detection, analysis, and signature
generation of exploits on commodity software. 2005.
[20] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball.
Feedback-directed random test generation. In Software
Engineering, 2007. ICSE 2007. 29th International
Conference on , pages 75–84. IEEE, 2007.
[21] V. K. Palepu, G. Xu, J. Jones, et al. Improving
eﬃciency of dynamic analysis with dynamic
dependence summaries. In Automated Software
Engineering (ASE), 2013 IEEE/ACM 28th
International Conference on , pages 59–69. IEEE, 2013.
[22] R. Pandita, X. Xiao, H. Zhong, T. Xie, S. Oney, andA. Paradkar. Inferring method speciﬁcations from
natural language api descriptions. In Proceedings of
the 34th International Conference on Software
Engineering , pages 815–825. IEEE Press, 2012.
[23] D. Qi, W. N. Sumner, F. Qin, M. Zheng, X. Zhang,
and A. Roychoudhury. Modeling software execution
environment. In Reverse Engineering (WCRE), 2012
19th Working Conference on , pages 415–424. IEEE,
2012.
[24] A. N. Raﬀerty and C. D. Manning. Parsing three
german treebanks: Lexicalized and unlexicalized
baselines. In Proceedings of the Workshop on Parsing
German, pages 40–46. Association for Computational
Linguistics, 2008.
[25] P. Runeson, M. Alexandersson, and O. Nyholm.
Detection of duplicate defect reports using natural
language processing. In Software Engineering, 2007.
ICSE 2007. 29th International Conference on , pages
499–510. IEEE, 2007.
[26] A. Sinha, S. M. Sutton, and A. Paradkar. Text2test:
Automated inspection of natural language use cases.
InSoftware Testing, Veriﬁcation and Validation
(ICST), 2010 Third International Conference on ,
pages 155–164. IEEE, 2010.
[27] D. Song, D. Brumley, H. Yin, J. Caballero, I. Jager,
M. G. Kang, Z. Liang, J. Newsome, P. Poosankam,
and P. Saxena. Bitblaze: A new approach to computer
security via binary analysis. In Information systems
security, pages 1–25. Springer, 2008.
[28] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo. A
discriminative model approach for accurate duplicate
bug report retrieval. In Proceedings of the 32nd
ACM/IEEE International Conference on Software
Engineering-Volume 1 , pages 45–54. ACM, 2010.
[29] L. Tan, D. Yuan, G. Krishna, and Y. Zhou. /*
icomment: Bugs or bad comments?*. In ACM
SIGOPS Operating Systems Review , volume 41, pages
145–158. ACM, 2007.
[30] S. H. Tan, D. Marinov, L. Tan, and G. T. Leavens. @
tcomment: Testing javadoc comments to detect
comment-code inconsistencies. In Software Testing,
Veriﬁcation and Validation (ICST), 2012 IEEE Fifth
International Conference on , pages 260–269. IEEE,
2012.
[31] O. Tkachuk. Ocsegen: Open components and systems
environment generator. In Proceedings of the 2nd ACM
SIGPLAN International Workshop on State Of the
Art in Java Program analysis , pages 9–12. ACM, 2013.
[32] O. Tkachuk, M. B. Dwyer, and C. S. P˘ as˘ areanu.
Automated environment generation for software model
checking. In Automated Software Engineering, 2003.
Proceedings. 18th IEEE International Conference on ,
pages 116–127. IEEE, 2003.
[33] H. van der Merwe, O. Tkachuk, B. van der Merwe,
and W. Visser. Generation of library models for
veriﬁcation of android applications. ACM SIGSOFT
Software Engineering Notes , 40(1):1–5, 2015.
[34] W. Visser, K. Havelund, G. Brat, S. Park, and
F. Lerda. Model checking programs. Automated
Software Engineering , 10(2):203–232, 2003.
[35] Z. Yang, M. Yang, Y. Zhang, G. Gu, P. Ning, and
X. S. Wang. Appintent: Analyzing sensitive data
390transmission in android for privacy leakage detection.
I
nProceedings of the 2013 ACM SIGSAC conference
on Computer & communications security , pages
1043–1054. ACM, 2013.
[36] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei. Mapo:
Mining and recommending api usage patterns. In
ECOOP 2009–Object-Oriented Programming , pages318–343. Springer, 2009.
[37] H. Zhong, L. Zhang, T. Xie, and H. Mei. Inferring
resource speciﬁcations from natural language api
documentation. In Proceedings of the 2009
IEEE/ACM International Conference on Automated
Software Engineering , pages 307–318. IEEE Computer
Society, 2009.
391