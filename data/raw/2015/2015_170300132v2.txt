Revisiting Unsupervised Learning for Defect Prediction
Wei Fu, Tim Menzies
Com. Sci., NC State, USA
wfu@ncsu.edu,tim.menzies@gmail.com
ABSTRACT
Collecting quality data from so/f_tware projects can be time-consuming
and expensive. Hence, some researchers explore ‚Äúunsupervised‚Äù
approaches to quality prediction that does not require labelled data.
An alternate technique is to use ‚Äúsupervised‚Äù approaches that learn
models from project data labelled with, say, ‚Äúdefective‚Äù or ‚Äúnot-
defective‚Äù. Most researchers use these supervised models since, it
is argued, they can exploit more knowledge of the projects.
At FSE‚Äô16, Yang et al. reported startling results where unsu-
pervised defect predictors outperformed supervised predictors for
eÔ¨Äort-aware just-in-time defect prediction. If con/f_irmed, these re-
sults would lead to a dramatic simpli/f_ication of a seemingly complex
task (data mining) that is widely explored in the so/f_tware engineer-
ing literature.
/T_his paper repeats and refutes those results as follows. (1) /T_here
is much variability in the eÔ¨Écacy of the Yang et al. predictors so
even with their approach, some supervised data is required to prune
weaker predictors away. (2) /T_heir /f_indings were grouped across N
projects. When we repeat their analysis on a project-by-project
basis, supervised predictors are seen to work be/t_ter.
Even though this paper rejects the speci/f_ic conclusions of Yang
et al., we still endorse their general goal. In our our experiments,
supervised predictors did not perform outstandingly be/t_ter than
unsupervised ones for eÔ¨Äort-aware just-in-time defect prediction.
Hence, they may indeed be some combination of unsupervised
learners to achieve comparable performance to supervised ones.
We therefore encourage others to work in this promising area.
KEYWORDS
Data analytics for so/f_tware engineering, so/f_tware repository mining,
empirical studies, defect prediction
ACM Reference format:
Wei Fu, Tim Menzies. 2017. Revisiting Unsupervised Learning for De-
fect Prediction. In Proceedings of 2017 11th Joint Meeting of the European
So/f_tware Engineering Conference and the ACM SIGSOFT Symposium on the
Foundations of So/f_tware Engineering, Paderborn, Germany, September 4-8,
2017 (ESEC/FSE‚Äô17), 12 pages.
DOI: 10.1145/3106237.3106257
1 INTRODUCTION
/T_his paper repeats and refutes recent results from Yang et al. [ 54]
published at FSE‚Äô16. /T_he task explored by Yang et al. was eÔ¨Äort-ware
just-in-time (JIT) so/f_tware defect predictors . JIT defect predictors
are built on code change level and could be used to conduct defect
prediction right before developers commit the current change. /T_hey
report an unsupervised so/f_tware quality prediction method that
ESEC/FSE‚Äô17, Paderborn, Germany
2017. 978-1-4503-5105-8/17/09. . . $15.00
DOI: 10.1145/3106237.3106257achieved be/t_ter results than standard supervised methods. We
repeated their study since, if their results were con/f_irmed, this
would imply that decades of research into defect prediction [ 7,8,10,
13,16,18,19,21,25,26,29,30,35,40,41,43,51,54] had needlessly
complicated an inherently simple task.
/T_he standard method for so/f_tware defect prediction is learning
from labelled data. In this approach, the historical log of known
defects is learned by a data miner. Note that this approach requires
waiting until a historical log of defects is available; i.e. until a/f_ter the
code has been used for a while. Another approach, explored by Yang
et al., uses general background knowledge to sort the code, then
inspect the code in that sorted order. In their study, they assumed
that more defects can be found faster by /f_irst looking over all the
‚Äúsmaller‚Äù modules (an idea initially proposed by Koru et al. [ 28]).
A/f_ter exploring various methods of de/f_ining ‚Äúsmaller‚Äù, they report
their approach /f_inds more defects, sooner, than supervised methods.
/T_hese results are highly remarkable:
/T_his approach does not require access to labelled data; i.e. it can
be applied just as soon as the code is wri/t_ten.
It is extremely simple: no data pre-processing, no data mining,
just simple sorting.
Because of the remakrable nature of these results, this paper takes
a second look at the Yang et al. results. We ask three questions:
RQ1: Do all unsupervised predictors perform better than
supervised predictors?
/T_he reason we ask this question is that if the answer is ‚Äúyes‚Äù,
then we can simply select any unsupervised predictor built from
the the change metrics as Yang et al suggested without using any
supervised data; if the answer is ‚Äúno‚Äù, then we must apply some
techniques to select best predictors and remove the worst ones.
However, our results show that, when projects are explored sepa-
rately, the majority of the unsupervised predictors learned by Yang
et al. perform worse than supervised predictors.
Results of RQ1 suggest that a/f_ter building multiple predictors
using unsupervised methods, it is required to prune the worst
predictors and only be/t_ter ones should be used for future prediction.
However, with Yang et al. approach, there is no way to tell which
unsupervised predictors will perform be/t_ter without access to the
labels of testing data. To test that speculation, we built a new
learner, OneWay , that uses supervised training data to remove all
but one of the Yang et al. predictors. Using this learner, we asked:
RQ2: Is it bene/f_icial to use supervised data to prune away
all but one of the Yang et al. predictors?
Our results showed that OneWay nearly always outperforms
the unsupervised predictors found by Yang et al. /T_he success of
OneWay leads to one last question:
RQ3: Does OneWay perform better than more complex stan-
dard supervised learners?
Such standard supervised learners include Random Forests, Lin-
ear Regression, J48 and IBk (these learners were selected based onarXiv:1703.00132v2  [cs.SE]  24 Jun 2017ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
prior results by [ 13,21,30,35]). We /f_ind that in terms of Recall
andPopt(the metric preferred by Yang et al.), OneWay performed
be/t_ter than standard supervised predictors. Yet measured in terms
ofPrecision , there was no advantage to OneWay .
From the above, we make an opposite conclusion to Yang et al.;
i.e., there are clear advantages to use supervised approaches over
unsupervised ones. We explain the diÔ¨Äerence between our results
and their results as follows:
Yang et al. reported averaged results across all projects;
We oÔ¨Äer a more detailed analysis on a project-by-project basis.
/T_he rest of this paper is organized as follows. Section 2 is a commen-
tary on Yang et al. study and the implication of this paper. Section 3
describes the background and related work on defect prediction.
Section 4 explains the eÔ¨Äort-ware JIT defect prediction methods
investigated in this study. Section 5 describes the experimental
se/t_tings of our study, including research questions that motivate
our study, data sets and experimental design. Section 6 presents
the results. Section 7 discusses the threats to the validity of our
study. Section 8 presents the conclusion and future work.
Note one terminological convention: in the following, we treat
‚Äúpredictors‚Äù and ‚Äúlearners‚Äù as synonyms.
2 SCIENCE IN THE 21st CENURY
While this paper is speci/f_ic about eÔ¨Äort-aware JIT defect prediction
and the Yang et al. result, at another level this paper is also about
science in the 21st century.
In 2017, the so/f_tware analytics community now has the tools,
data sets, experience to explore a bold wider range of options. /T_here
are practical problems in exploring all those possibilities speci/f_ically,
too many options. For example, in section 2.5 of [ 27], Kocaguneli
et al. list 12,000+ diÔ¨Äerent ways of estimation by analogy. We have
had some recent successes with exploring this space of options [ 7]
but only a/f_ter the total space of options is reduced by some initial
study to a manageable set of possibilities. Hence, what is needed are
initial studies to rule our methods that are generally unpromising
(e.g. this paper) before we apply second level hyper-parameter
optimization study that takes the reduced set of options.
Another aspect of 21st century science that is highlighted by this
paper is the nature of repeatability. While this paper disagrees the
conclusions of Yang et al., it is important to stress that their paper
is an excellent example of good science that should be emulated in
future work.
Firstly, they tried something new. /T_here are many papers in the
SE literature about defect prediction. However, compared to most
of those, the Yang et al. paper is bold and stunningly original.
Secondly, they made all their work freely available. Using the ‚ÄúR‚Äù
code they placed online, we could reproduce their result, including
all their graphical output, in a ma/t_ter of days. Further, using that
code as a starting point, we could rapidly conduct the extensive ex-
perimentation that leads to this paper. /T_his is an excellent example
of the value of open science.
/T_hirdly, while we assert their answers were wrong, the question
they asked is important and should be treated as an open and urgent
issue by the so/f_tware analytics community. In our experiments,
supervised predictors performed be/t_ter than unsupervised‚Äì but not
outstandingly be/t_ter than unsupervised. Hence, they may indeed besome combination of unsupervised learners to achieve comparable
performance to supervised. /T_herefore, even though we reject the
speci/f_ic conclusions of Yang et al., we still endorse the question
they asked strongly and encourage others to work in this area.
3 BACKGROUND AND RELATED WORK
3.1 Defect Prediction
As soon as people started programming, it became apparent that
programming was an inherently buggy process. As recalled by
Maurice Wilkes [ 52], speaking of his programming experiences
from the early 1950s: ‚ÄúIt was on one of my journeys between the
EDSAC room and the punching equipment that ‚Äòhesitating at the
angles of stairs‚Äô the realization came over me with full force that
a good part of the remainder of my life was going to be spent in
/f_inding errors in my own programs.‚Äù
It took decades to gather the experience required to quantify
the size/defect relationship. In 1971, Fumio Akiyama [ 2] described
the /f_irst known ‚Äúsize‚Äù law, saying the number of defects D was a
function of the number of LOC where D=4:86+0:018LOC.
In 1976, /T_homas McCabe argued that the number of LOC was
less important than the complexity of that code [ 33]. He argued that
code is more likely to be defective when his ‚Äúcyclomatic complexity‚Äù
measure was over 10. Later work used data miners to build defect
predictors that proposed thresholds on multiple measures [35].
Subsequent research showed that so/f_tware bugs are not dis-
tributed evenly across a system. Rather, they seem to clump in
small corners of the code. For example, Hamill et al. [ 15] report
studies with (a) the GNU C++ compiler where half of the /f_iles
were never implicated in issue reports while 10% of the /f_iles were
mentioned in half of the issues. Also, Ostrand et al. [ 44] studied
(b) AT&T data and reported that 80% of the bugs reside in 20% of the
/f_iles. Similar ‚Äú80-20‚Äù results have been observed in (c) NASA sys-
tems [ 15] as well as (d) open-source so/f_tware [ 28] and (e) so/f_tware
from Turkey [37].
Given this skewed distribution of bugs, a cost-eÔ¨Äective quality
assurance approach is to sample across a so/f_tware system, then
focus on regions reporting some bugs. So/f_tware defect predictors
built from data miners are one way to implement such a sampling
policy. While their conclusions are never 100% correct, they can be
used to suggest where to focus more expensive methods such as
elaborate manual review of source code [ 49]; symbolic execution
checking [ 46], etc. For example, Misirli et al. [ 37] report studies
where the guidance oÔ¨Äered by defect predictors:
Reduced the eÔ¨Äort required for so/f_tware inspections in some
Turkish so/f_tware companies by 72%;
While, at the same time, still being able to /f_ind the 25% of the
/f_iles that contain 88% of the defects.
Not only do static code defect predictors perform well compared to
manual methods, they also are competitive with certain automatic
methods. A recent study at ICSE‚Äô14, Rahman et al. [ 47] compared
(a) static code analysis tools FindBugs, Jlint, and Pmd and (b) static
code defect predictors (which they called ‚Äústatistical defect predic-
tion‚Äù) built using logistic regression. /T_hey found no signi/f_icant
diÔ¨Äerences in the cost-eÔ¨Äectiveness of these approaches. Given
this equivalence, it is signi/f_icant to note that static code defect
prediction can be quickly adapted to new languages by buildingRevisiting Unsupervised Learning for Defect Prediction ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
lightweight parsers that extract static code metrics. /T_he same is not
true for static code analyzers‚Äì these need extensive modi/f_ication
before they can be used on new languages.
To build such defect predictors, we measure the complexity of
so/f_tware projects using McCabe metrics, Halstead‚Äôs eÔ¨Äort metrics
and CK object-oriented code mertics [ 5,14,20,33] at a coarse gran-
ularity, like /f_ile or package level. With the collected data instances
along with the corresponding labels (defective or non-defective),
we can build defect prediction models using supervised learners
such as Decision Tree, Random Forests, SVM, Naive Bayes and Lo-
gistic Regression [ 13,22‚Äì24,30,35]. A/f_ter that, such trained defect
predictor can be applied to predict the defects of future projects.
3.2 Just-In-Time Defect Prediction
Traditional defect prediction has some drawbacks such as prediction
at a coarse granularity and started at very late stage of so/f_tware
development circle [ 21], whereas in JIT defect prediction paradigm,
the defect predictors are built on code change level, which could
easily help developers narrow down the code for inspection and
JIT defect prediction could be conducted right before developers
commit the current change. JIT defect prediction becomes a more
practical method for practitioners to carry out.
Mockus et al. [ 38] conducted the /f_irst study to predict so/f_tware
failures on a telecommunication so/f_tware project, 5ESS, by using
logistic regression on data sets consisted of change metrics of the
project. Kim et al. [ 25] further evaluated the eÔ¨Äectiveness of change
metrics on open source projects. In their study, they proposed to
apply support vector machine to build a defect predictor based
on so/f_tware change metrics, where on average they achieved 78%
accuracy and 60% recall. Since training data might not be available
when building the defect predictor, Fukushima et al. [ 9] introduced
cross-project paradigm into JIT defect prediction. /T_heir results
showed that using data from other projects to build JIT defect
predictor is feasible.
Most of the research into defect prediction does not consider
the eÔ¨Äort1required to inspect the code predicted to be defective.
Exceptions to this rule include the work of Arishom and Briand [ 3],
Koru et al. [ 28] and Kamei et al. [ 21]. Kamei et al. [ 21] conducted
a large-scale study on the eÔ¨Äectiveness of JIT defect prediction,
where they claimed that using 20% of eÔ¨Äorts required to inspect
all changes, their modi/f_ied linear regression model (EALR) could
detect 35% defect-introducing changes. Inspired by Menzies et al.‚Äôs
ManualUp model (i.e., small size of modules inspected /f_irst) [ 36],
Yang et al. [ 54] proposed to build 12 unsupervised defect predictors
by sorting the reciprocal values of 12 diÔ¨Äerent change metrics on
each testing data set in descending order. /T_hey reported that with
20% eÔ¨Äorts, many unsupervised predictors perform be/t_ter than
state-of-the-art supervised predictors.
4 METHOD
4.1 Unsupervised Predictors
In this section, we describe the eÔ¨Äort-aware just-in-time unsuper-
vised defect predictors proposed by Yang et al. [ 54], which serves
as a baseline method in this study. As described by Yang et al. [ 54],
1EÔ¨Äort means time/labor required to inspect total number of /f_iles/code predicted as
defective.their simple unsupervised defect predictor is built on change met-
rics as shown in Table 1. /T_hese 14 diÔ¨Äerent change metrics can be
divided into 5 dimensions [21]:
DiÔ¨Äusion: NS, ND, NF and Entropy.
Size: LA, LD and LT.
Purpose: FIX.
History: NDEV, AGE and NUC.
Experience: EXP, REXP and SEXP.
Table 1: Change metrics used in our data sets.
Metric Description
NS Number of modi/f_ied subsystems [38].
ND Number of modi/f_ied directories [38].
NF Number of modi/f_ied /f_iles [42].
Entropy Distribution of the modi/f_ied code across each /f_ile [6, 16].
LA Lines of code added [41].
LD Lines of code deleted [41].
LT Lines of code in a /f_ile before the current change [28].
FIX Whether or not the change is a defect /f_ix [11, 55].
NDEV Number of developers that changed the modi/f_ied /f_iles [32].
AGE /T_he average time interval between the last and the current
change [10].
NUC /T_he number of unique changes to the modi/f_ied /f_iles [6, 16].
EXP /T_he developer experience in terms of number of changes [38].
REXP Recent developer experience [38].
SEXP Developer experience on a subsystem [38].
/T_hediÔ¨Äusion dimension characterizes how a change is distributed
at diÔ¨Äerent levels of granularity. As discussed by Kamei et al. [ 21],
a highly distributed change is harder to keep track and more likely
to introduce defects. /T_he sizedimension characterizes the size of a
change and it is believed that the so/f_tware size is related to defect
proneness [ 28,41]. Yin et al. [ 55] report that the bug-/f_ixing process
can also introduce new bugs. /T_herefore, the Fixmetric could be used
as a defect evaluation metric. /T_he History dimension includes some
historical information about the change, which has been proven to
be a good defect indicator [ 32]. For example, Matsumoto et al. [ 32]
/f_ind that the /f_iles previously touched by many developers are likely
to contain more defects. /T_he Experience dimension describes the
experience of so/f_tware programmers for the current change because
Mockus et al. [ 38] show that more experienced developers are less
likely to introduce a defect. More details about these metrics can
be found in Kamei et al‚Äôs study [21].
In Yang et al.‚Äôs study, for each change metric Mof testing data,
they build an unsupervised predictor that ranks all the changes
based on the corresponding value of1
M¬πc¬∫in descending order,
where M¬πc¬∫is the value of the selected change metric for each
change c. /T_herefore, the changes with smaller change metric values
will ranked higher. In all, for each project, Yang et al. de/f_ine 12
simple unsupervised predictors (LA and LD are excluded as Yang
et al. [54]).
4.2 Supervised Predictors
To further evaluate the unsupervised predictor, we selected some
supervised predictors that already used in Yang et al.‚Äôs work.
As reported in both Yang et al.‚Äôs [ 54] and Kamei et al.‚Äôs [ 21]
work, EALR outperforms all other supervised predictors for eÔ¨Äort-
aware JIT defect prediction. EALR is a modi/f_ied linear regressionESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
model [ 21] and it predictsY¬πx¬∫
EÔ¨Äort¬πx¬∫instead of predicting Y¬πx¬∫, where
Y¬πx¬∫indicates whether this change is a defect or not (1 or 0) and
EÔ¨Äort¬πx¬∫represents the eÔ¨Äort required to inspect this change. Note
that this is the same method to build EALR as Kamei et al. [21].
In defect prediction literature, IBk (KNN), J48 and Random Forests
methods are simple yet widely used as defect learners and have
been proven to perform, if not best, quite well for defect predic-
tion [ 13,30,35,50]. /T_hese three learners are also used in Yang et
al‚Äôs study. For these supervised predictors, Y¬πx¬∫was used as the
dependant variable. For KNN method, we set K=8 according to
Yang et al. [54].
4.3 OneWay Learner
Based on our preliminary experiment results shown in the follow-
ing section, for the six projects investigated by Yang et al, some
of 12 unsupervised predictors do perform worse than supervised
predictors and there is no one predictor constantly working best
on all project data. /T_his means we can not simply say which un-
supervised predictor works for the new project before predicting
on the testing data. In this case, we need a technique to select the
proper metrics to build defect predictors.
We propose OneWay learner, which is a supervised predictor
built on the implication of Yang et al‚Äôs simple unsupervised predic-
tors. /T_he pseudocode for OneWay is shown in Algorithm 1. In the
following description, we use the superscript numbers to denote
the line number in pseudocode.
/T_he general idea of OneWay is to use supervised training data
to remove all but one of the Yang et al. predictors and then apply
this trained learner on the testing data. Speci/f_ically, OneWay /f_irstly
builds simple unsupervised predictors from each metric on training
dataL4, then evaluates each of those learners in terms of evaluation
metricsL5, like Popt,Recall ,Precision and F1. A/f_ter that, if the
desirable evaluation goal is set, the metric which performs best on
the corresponding evaluation goal is returned as the best metric;
otherwise, the metric which gets the highest mean score over all
evaluation metrics is returnedL9(In this study, we use the la/t_ter
one). Finally, a simple predictor is built only on such best metricL10
with the help of training data. /T_herefore, OneWay builds only one
supervised predictor for each project using the local data instead
of 12 predictors directly on testing data as Yang et al [54] .
5 EXPERIMENTAL SETTINGS
5.1 Research /Q_uestions
Using the above methods, we explore three questions:
Do all unsupervised predictors perform be/t_ter than supervised
predictors?
Is it bene/f_icial to use supervised data to prune all but one of the
Yang et al. unsupervised predictors?
Does OneWay perform be/t_ter than more complex standard su-
pervised predictors?
When reading the results from Yang et al. [ 54], we /f_ind that they
aggregate performance scores of each learner on six projects, which
might miss some information about how learners perform on each
project. Are these unsupervised predictors working consistentlyAlgorithm 1 Pseudocode for OneWay
Input: data train ,data test,eval goal2fF1;Popt;Recall ;Precision ; : : :g
Output: result
1:function O/n.sc/e.scW/a.sc/y.sc (data train ,data test,eval goal)
2: allscores NULL
3: formetric indata train do
4: learner buildUnsupervisedLearner( data train ,metric )
5: scores evaluate( learner )
6: //scores include all evaluation goals, e.g., Popt ;F1; : : :
7: allscores .append( scores )
8: end for
9: best metric pruneFeature( allscores ,eval goal)
10: result buildUnsupervisedLearner( data test,best metric )
11: return result
12:end function
13:function /p.sc/r.sc/u.sc/n.sc/e.scF/e.sc/a.sc/t.sc/u.sc/r.sc/e.sc (allscores ,eval goal)
14: ifeval goal == NULL then
15: mean scores getMeanScoresForEachMetric( allscores )
16: best metric getMetric(max( mean scores ))
17: return best metric
18: else
19: best metric getMetric(max( allscores¬ª‚Äúeval goal‚Äù¬º))
20: return best metric
21: end if
22:end function
across all the project data? If not, how would it look like? /T_herefore,
in RQ1, we report results for each project separately.
Another observation is that even though Yang et al. [ 54] propose
that simple unsupervised predictors could work be/t_ter than super-
vised predictors for eÔ¨Äort-aware JIT defect prediction, one missing
aspect of their report is how to select the most promising metric to
build a defect predictor. /T_his is not an issue when all unsupervised
predictors perform well but, as we shall see, this is not the case.
As demonstrated below, given Munsupervised predictors, only
a small subset can be recommended. /T_herefore it is vital to have
some mechanism by which we can down select from Mmodels
to the LMthat are useful. Based on this fact, we propose a
new method, OneWay , which is the missing link in Yang et al.‚Äôs
study [ 54] and the missing /f_inal step they do not explore. /T_here-
fore, in RQ2 and RQ3, we want to evaluate how well our proposed
OneWay method performs compared to the unsupervised predictors
and supervised predictors.
Considering our goals and questions, we reproduce Yang et al‚Äôs
results and report for each project to answer RQ1. For RQ2 and
RQ3, we implement our OneWay method, and compare it with
unsupervised predictors and supervised predictors on diÔ¨Äerent
projects in terms of various evaluation metrics.
5.2 Data Sets
In this study, we conduct our experiment using the same data sets
as Yang et al.[54], which are six well-known open source projects,
Bugzilla, Columba, Eclipse JDT, Eclipse Platform, Mozilla and Post-
greSQL. /T_hese data sets are shared by Kamei et al. [ 21]. /T_he statistics
of the data sets are listed in Table 2. From Table 2, we know that
all these six data sets cover at least 4 years historical information,
and the longest one is PostgreSQL, which includes 15 years of data.Revisiting Unsupervised Learning for Defect Prediction ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
Table 2: Statistics of the studied data sets
Project PeriodTotal
Change% of
DefectsAvg LOC
per
Change# Modi/f_ied
Files per
Change
Bugzilla 08/1998 - 12/2006 4620 36% 37.5 2.3
Platform 05/2001 - 12/2007 64250 14% 72.2 4.3
Mozilla 01/2000 - 12/2006 98275 5% 106.5 5.3
JDT 05/2001 - 12/2007 35386 14% 71.4 4.3
Columba 11/2002 - 07/2006 4455 31% 149.4 6.2
PostgreSQL 07/1996 - 05/2010 20431 25% 101.3 4.5
/T_he total changes for these six data sets are from 4450 to 98275,
which are suÔ¨Écient for us to conduct an empirical study. In this
study, if a change introduces one or more defects then this change is
considered as defect-introducing change. /T_he percentage of defect-
introducing changes ranges from 5% to 36%. All the data and code
used in this paper is available online2.
5.3 Experimental Design
/T_he following principle guides the design of these experiments:
Whenever there is a choice between methods, data, etc., we will always
prefer the techniques used in Yang et al. [54].
By applying this principle, we can ensure that our experimental
setup is the same as Yang et al. [ 54]. /T_his will increase the validity
of our comparisons with that prior work.
When applying data mining algorithms to build predictive mod-
els, one important principle is not to test on the data used in training.
To avoid that, we used time-wise-cross-validation method which is
also used by Yang et al. [ 54]. /T_he important aspect of the following
experiment is that it ensures that all testing data was created a/f_ter
training data. Firstly, we sort all the changes in each project based
on the commit date. /T_hen all the changes that were submi/t_ted in
the same month are grouped together. For a given project data
set that covers totally Nmonths history, when building a defect
predictor, consider a sliding window size of 6,
/T_he /f_irst two consecutive months data in the sliding window,
ith and i+1th, are used as the training data to build supervised
predictors and OneWay learner.
/T_he last two months data in the sliding window, i+4th and
i+5th, which are two months later than the training data, are
used as the testing data to test the supervised predictors, OneWay
learner and unsupervised predictors.
A/f_ter one experiment, the window slides by ‚Äúone month‚Äù data.
By using this method, each training and testing data set has two
months data, which will include suÔ¨Écient positive and negative
instances for the supervised predictors to learn. For any project that
includes Nmonths data, we can perform N 5 diÔ¨Äerent experiments
to evaluate our learners when Nis greater than 5. For all the
unsupervised predictors, only the testing data is used to build the
model and evaluate the performance.
To statistically compare the diÔ¨Äerences between OneWay with
supervised and unsupervised predictors, we use Wilcoxon single
ranked test to compare the performance scores of the learners in
this study the same as Yang et al. [ 54]. To control the false discover
rate, the Benjamini-Hochberg (BH) adjusted p-value is used to
2h/t_tps://github.com/WeiFoo/RevisitUnsupervisedtest whether two distributions are statistically signi/f_icant at the
level of 0 :05 [4,54]. To measure the eÔ¨Äect size of performance
scores among OneWay and supervised/unsupervised predictors, we
compute CliÔ¨Ä‚Äôs Œ¥that is a non-parametric eÔ¨Äect size measure [ 48].
As Romano et al. suggested, we evaluate the magnitude of the eÔ¨Äect
size as follows: negligible ( jŒ¥j<0:147 ), small (0 :147jŒ¥j<0:33),
medium (0 :33jŒ¥j<0:474 ), and large (0.474 jŒ¥j) [48].
5.4 Evaluation Measures
For eÔ¨Äort-aware JIT defect prediction, in addition to evaluate how
learners correctly predict a defect-introducing change, we have
to take account the eÔ¨Äorts that are required to inspect prediction.
Ostrand et al. [ 45] report that given a project, 20% of the /f_iles
contain on average 80% of all defects in the project. Although there
is nothing magical about the number 20%, it has been used as a
cutoÔ¨Ä value to set the eÔ¨Äorts required for the defect inspection
when evaluating the defect learners [ 21,34,39,54]. /T_hat is, given
20% eÔ¨Äort, how many defects can be detected by the learner. To be
consistent with Yang et al, in this study, we restrict our eÔ¨Äorts to
20% of total eÔ¨Äorts.
To evaluate the performance of eÔ¨Äort-aware JIT defect prediction
learners in our study, we used the following 4 metrics: Precision,
Recall, F1 andPopt, which are widely used in defect prediction
literature [21, 35, 36, 39, 54, 56].
Precision =True Positi /v.alte
True Positi /v.alte+False Positi /v.alte
Recall =True Positi /v.alte
True Positi /v.alte+False Ne/afii10069.italati/v.alte
F1=2PrecisionRecall
Recall +Precision
where Precision denotes the percentage of actual defective changes
to all the predicted changes and Recall is the percentage of predicted
defective changes to all actual defective changes. F1is a measure
that combines both Precision andRecall which is the harmonic mean
ofPrecision andRecall .
Figure 1: Example of an eÔ¨Äort-based cumulative li/f_t
chart [54].
/T_he last evaluation metric used in this study is Popt, which is
de/f_ined as 1 ‚àÜopt, where ‚àÜoptis the area between the eÔ¨Äort (code-
churn-based) cumulative li/f_t charts of the optimal model and the
prediction model (as shown in Figure 1). In this chart, the x-axis isESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
considered as the percentage of required eÔ¨Äort to inspect the change
and the y-axis is the percentage of defect-introducing change found
in the selected change. In the optimal model, all the changes are
sorted by the actual defect density in descending order, while for the
predicted model, all the changes are sorted by the actual predicted
value in descending order.
According to Kamei et al. and Xu et al. [ 21,39,54],Poptcan be
normalized as follows:
Popt¬πm¬∫=1 S¬πoptimal¬∫ S¬πm¬∫
S¬πoptimal¬∫ S¬πworst¬∫
where S¬πoptimal¬∫,S¬πm¬∫andS¬πworst¬∫represent the area of curve
under the optimal model, predicted model, and worst model, re-
spectively. Note that the worst model is built by sorting all the
changes according to the actual defect density in ascending order.
For any learner, it performs be/t_ter than random predictor only if
thePoptis greater than 0.5.
Note that, following the practices of Yang et al. [ 54], we measure
Precision ,Recall ,F1andPoptat the eÔ¨Äort = 20% point. In this study,
in addition to PoptandACC (i.e., Recall ) that is used in Yang et al‚Äôs
work [ 54], we include Precision andF1measures and they provide
more insights about all the learners evaluated in the study from
very diÔ¨Äerent perspectives, which will be shown in the next section.
6 EMPIRICAL RESULTS
In this section, we present the experimental results to investigate
how simple unsupervised predictors work in practice and evaluate
the performance of the proposed method, OneWay , compared with
supervised and unsupervised predictors.
Before we start oÔ¨Ä, we need a sanity check to see if we can
fully reproduce Yang et al.‚Äôs results. Yang et al. [ 54] provide the
median values of PoptandRecall for the EALR model and the best
two unsupervised models, LT and AGE, from the time-wise cross
evaluation experiment. /T_herefore, we use those numbers to check
our results.
As shown in Table 3 and Table 4, for unsupervised predictors, LT
and AGE, we get the exact same performance scores on all projects
in terms of Recall andPopt. /T_his is reasonable because unsupervised
predictors are very straightforward and easy to implement. For the
supervised predictor, EALR, these two implementations do not have
diÔ¨Äerences in Popt, while the maximum diÔ¨Äerence in Recall is only
0:02. Since the diÔ¨Äerences are quite small, then we believe that our
implementation re/f_lects the details about EALR and unsupervised
learners in Yang et al. [54].
For other supervised predictors used in this study, like J48, IBk,
and Random Forests, we use the same algorithms from Weka pack-
age [12] and set the same parameters as used in Yang et al. [54].
RQ1: Do all unsupervised predictors perform better than
supervised predictors?
To answer this question, we build four supervised predictors and
twelve unsupervised predictors on the six project data sets using
incremental learning method as described in Section 5.3.
Figure 2 shows the boxplot of Recall ,Popt,F1andPrecision for
supervised predictors and unsupervised predictors on all data sets.
For each predictor, the boxplot shows the 25th percentile, median
and 75 percentile values for one data set. /T_he horizontal dashed
lines indicate the median of the best supervised predictor, whichTable 3: Comparison in Popt: Yang‚Äôs method (A) vs. our im-
plementation (B)
ProjectEALR LT AGE
A B A B A B
Bugzilla 0.59 0.59 0.72 0.72 0.67 0.67
Platform 0.58 0.58 0.72 0.72 0.71 0.71
Mozilla 0.50 0.50 0.65 0.65 0.64 0.64
JDT 0.59 0.59 0.71 0.71 0.68 0.69
Columba 0.62 0.62 0.73 0.73 0.79 0.79
PostgreSQL 0.60 0.60 0.74 0.74 0.73 0.73
Average 0.58 0.58 0.71 0.71 0.70 0.70
Table 4: Comparison in Recall : Yang‚Äôs method (A) vs. our
implementation (B)
ProjectEALR LT AGE
A B A B A B
Bugzilla 0.29 0.30 0.45 0.45 0.38 0.38
Platform 0.31 0.30 0.43 0.43 0.43 0.43
Mozilla 0.18 0.18 0.36 0.36 0.28 0.28
JDT 0.32 0.34 0.45 0.45 0.41 0.41
Columba 0.40 0.42 0.44 0.44 0.57 0.57
PostgreSQL 0.36 0.36 0.43 0.43 0.43 0.43
Average 0.31 0.32 0.43 0.43 0.41 0.41
is to help visualize the median diÔ¨Äerences between unsupervised
predictors and supervised predictors.
/T_he colors of the boxes within Figure 2 indicate the signi/f_icant
diÔ¨Äerence between learners:
/T_heblue color represents that the corresponding unsupervised
predictor is signi/f_icantly be/t_ter than the best supervised predictor
according to Wilcoxon signed-rank, where the BH corrected
p-value is less than 0.05 andthe magnitude of the diÔ¨Äerence
between these two learners is NOT trivial according to CliÔ¨Ä‚Äôs
delta, wherejŒ¥j0:147.
/T_heblack color represents that the corresponding unsuper-
vised predictor is not signi/f_icantly be/t_ter than the best super-
vised predictor orthe magnitude of the diÔ¨Äerence between these
two learners is trivial, where jŒ¥j0:147.
/T_hered color represents that the corresponding unsupervised
predictor is signi/f_icantly worse than the best supervised predictor
andthe magnitude of the diÔ¨Äerence between these two learners
is NOT trivial.
From Figure 2, we can clearly see that not all unsupervised predic-
tors perform statistically be/t_ter than the best supervised predictor
across all diÔ¨Äerent evaluation metrics. Speci/f_ically, for Recall , on one
hand, there are only2
12,3
12,6
12,2
12,3
12and2
12of all unsupervised
predictors that perform statistically be/t_ter than the best supervised
predictor on six data sets, respectively. On the other hand, there
are6
12,6
12,4
12,6
12,5
12and6
12of all unsupervised predictors perform
statistically worse than the best supervised predictor on the six data
sets, respectively. /T_his indicates that:
About 50% of the unsupervised predictors perform worse than
the best supervised predictor on any data set;
Without any prior knowledge, we can not know which unsu-
pervised predictor(s) works adequately on the testing data.Revisiting Unsupervised Learning for Defect Prediction ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
0.00.6Supervised UnsupervisedRecall0.00.6 0.00.6 0.00.6 0.00.6 0.00.6
EALRRFJ48IBk NSNDNF
EntropyLTFIXNDEVAGEEXPREXPSEXPNUCRecall
0.00.6Supervised UnsupervisedPopt0.00.6 0.00.6 0.00.6 0.00.6 0.00.6
EALRRFJ48IBk NSNDNF
EntropyLTFIXNDEVAGEEXPREXPSEXPNUCPopt
0.00.6Supervised UnsupervisedF10.00.6 0.00.6 0.00.6 0.00.6 0.00.6
EALRRFJ48IBk NSNDNF
EntropyLTFIXNDEVAGEEXPREXPSEXPNUCF1
0.00.6Supervised UnsupervisedPrecision0.00.6 0.00.6 0.00.6 0.00.6 0.00.6
EALRRFJ48IBk NSNDNF
EntropyLTFIXNDEVAGEEXPREXPSEXPNUCPrecision
Figure 2: Performance comparisons between supervised and unsupervised predictors over six projects (from top to bottom
are Bugzilla, Platform, Mozilla, JDT, Columba, PostgreSQL).
Note that the above two points from Recall also hold for Popt.
ForF1, we see that only LT on Bugzilla and AGE on PostgreSQL
perform statistically be/t_ter than the best supervised predictor. Other
than that, no unsupervised predictor performs be/t_ter on any data
set. Furthermore, surprisingly, no unsupervised predictor works
signi/f_icantly be/t_ter than the best supervised predictor on any data
sets in terms of Precision . As we can see, Random Forests performs
well on all six data sets. /T_his suggests that unsupervised predictors
have very low precision for eÔ¨Äort-aware defect prediction and can
not be deployed to any business situation where precision is critical.
Overall, for a given data set, no one speci/f_ic unsupervised pre-
dictor works be/t_ter than the best supervised predictor across all
evaluation metrics. For a given measure, most unsupervised pre-
dictors did not perform be/t_ter across all data sets. In summary:
Not all unsupervised predictors perform be/t_ter than super-
vised predictors for each project and for diÔ¨Äerent evaluation
measures.Note the implications of this /f_inding: some extra knowledge
is required to prune the worse unsupervised models, such as the
knowledge that can come from labelled data. Hence, we must
conclude the opposite to Yang et al.; i.e. some supervised labelled
data must be applied before we can reliably deploy unsupervised
defect predictors on testing data.
RQ2: Is it bene/f_icial to use supervised data to prune away
all but one of the Yang et al. predictors?
To answer this question, we compare the OneWay learner with
all twelve unsupervised predictors. All these predictors are tested
on the six project data sets using the same experiment scheme as
we did in RQ1.
Figure 3 shows the boxplot for the performance distribution of
unsupervised predictors and the proposed OneWay learner on six
data sets across four evaluation measures. /T_he horizontal dashed
line denotes the median value of OneWay . Note that in Figure 4, blue
means this learner is statistically be/t_ter than OneWay , red means
worse, and black means no diÔ¨Äerence. As we can see, in Recall , onlyESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
0.00.6Unsupervised ProposedRecall0.00.6 0.00.6 0.00.6 0.00.6 0.00.6
NSNDNF
EntropyLTFIXNDEVAGEEXPREXPSEXPNUC
OneWay
0.00.6Unsupervised ProposedPopt0.00.6 0.00.6 0.00.6 0.00.6 0.00.6
NSNDNF
EntropyLTFIXNDEVAGEEXPREXPSEXPNUC
OneWay
0.00.6Unsupervised ProposedF10.00.4 0.000.20 0.00.4 0.00.4 0.00.4
NSNDNF
EntropyLTFIXNDEVAGEEXPREXPSEXPNUC
OneWay
0.00.6Unsupervised ProposedPrecision0.00.4 0.000.20 0.00.4 0.00.4 0.00.4
NSNDNF
EntropyLTFIXNDEVAGEEXPREXPSEXPNUC
OneWay
Figure 3: Performance comparisons between the proposed OneWay learner and unsupervised predictors over six projects (from
top to bottom are Bugzilla, Platform, Mozilla, JDT, Columba, PostgreSQL).
one unsupervised predictor, LT, outperforms OneWay in4
6data sets.
However, OneWay signi/f_icantly outperform9
12,9
12,9
12,10
12,8
12and
10
12of total unsupervised predictors on six data sets, respectively.
/T_his observation indicates that OneWay works signi/f_icantly be/t_ter
than almost all learners on all 6 data sets in terms of Recall .
Similarly, we observe that only LT predictor works be/t_ter than
OneWay in3
6data sets in terms of Poptand AGE outperforms
OneWay only on the platform data set. For the remaining experi-
ments, OneWay performs be/t_ter than all the other predictors (on
average, 9 out of 12 predictors).
In addition, according to F1, only three unsupervised predictors
EXP/REXP/SEXP perform be/t_ter than OneWay on the Mozilla data
set and LT predictor just performs as well as OneWay (and has
no advantage over OneWay ). We note that similar /f_indings can be
observed in Precision measure.
Table 5 provides the median values of the best unsupervised
predictor compared with OneWay for each evaluation measure onall data sets. Note that, in practice, we can not know which unsu-
pervised predictor is the best out of the 12 unsupervised predictors
by Yang et al.‚Äôs method before we access to the labels of testing
data. In other words, to aid our analysis, the best unsupervised
ones in Table 5 are selected when referring to the true labels of
testing data, which are not available in practice. In that table, for
each evaluation measure, the number in green cell indicates that
the best unsupervised predictor has a large advantage over OneWay
according to the CliÔ¨Ä‚Äôs Œ¥; Similarly, the yellow cell means medium
advantage and the gray cell means small advantage.
From Table 5, we observe that out of 24 experiments on all
evaluation measures, none of these best unsupervised predictors
outperform OneWay with a large advantage according to the CliÔ¨Ä‚Äôs
Œ¥. Speci/f_ically, according to Recall andPopt, even though the best
unsupervised predictor, LT, outperforms OneWay on four and three
data sets, all of these advantage are small. Meanwhile, REXP and
EXP have a medium improvement over OneWay on one and two
data sets for F1andPrecision , respectively. In terms of the averageRevisiting Unsupervised Learning for Defect Prediction ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
0.00.6Supervised ProposedRecall0.00.6 0.00.6 0.00.6 0.00.6 0.00.6
EALRRFJ48IBk
OneWayRecall
0.00.6Supervised ProposedPopt0.00.6 0.00.6 0.00.6 0.00.6 0.00.6
EALRRFJ48IBk
OneWayPopt
0.00.6Supervised ProposedF10.00.6 0.00.6 0.00.6 0.00.6 0.00.6
EALRRFJ48IBk
OneWayF1
0.00.6Supervised ProposedPrecision0.00.6 0.00.6 0.00.6 0.00.6 0.00.6
EALRRFJ48IBk
OneWayPrecision
Figure 4: Performance comparisons between the proposed OneWay learner and supervised predictors over six projects (from
top to bottom are Bugzilla, Platform, Mozilla, JDT, Columba, PostgreSQL).
Table 5: Best unsupervised predictor (A) vs. OneWay (B). /T_he
colorful cell indicates the size eÔ¨Äect: green for large; yellow
for medium; gray for small.
ProjectRecall Popt F1 Precision
A (LT) B A (LT) B A (REXP) B A (EXP) B
Bugzilla 0.45 0.36 0.72 0.65 0.33 0.35 0.40 0.39
Platform 0.43 0.41 0.72 0.69 0.16 0.17 0.14 0.11
Mozilla 0.36 0.33 0.65 0.62 0.10 0.08 0.06 0.04
JDT 0.45 0.42 0.71 0.70 0.18 0.18 0.15 0.12
Columba 0.44 0.56 0.73 0.76 0.23 0.32 0.24 0.25
PostgreSQL 0.43 0.44 0.74 0.74 0.24 0.29 0.25 0.23
Average 0.43 0.42 0.71 0.69 0.21 0.23 0.21 0.19
scores, the maximum magnitude of the diÔ¨Äerence between the best
unsupervised learner and OneWay is 0:02. In other words, OneWay
is comparable with the best unsupervised predictors on all data
sets for all evaluation measures even though the best unsupervised
predictors might not be known before testing.
Overall, we /f_ind that (1) no one unsupervised predictor signi/f_i-
cantly outperforms OneWay on all data sets for a given evaluation
measure; (2) mostly, OneWay works as well as the best unsupervised
predictor and has signi/f_icant be/t_ter performance than almost all
unsupervised predictors on all data sets for all evaluation measures.
/T_herefore, the above results suggest:
As a simple supervised predictor, OneWay has competitive
performance and it performs be/t_ter than most unsupervised
predictors for eÔ¨Äort-aware JIT defect prediction.
Note the implications of this /f_inding: the supervised learning
utilized in OneWay can signi/f_icantly outperform the unsupervised
models.
RQ3: Does OneWay perform better than more complex stan-
dard supervised predictors?
To answer this question, we compare OneWay learner with four
supervised predictors, including EALR, Random Forests, J48 andTable 6: Best supervised predictor (A) vs. OneWay (B). /T_he
colorful cell indicates the size eÔ¨Äect: green for large; yellow
for medium; gray for small.
ProjectRecall Popt F1 Precision
A (EALR) B A (EALR) B A (IBk) B A (RF) B
Bugzilla 0.30 0.36 0.59 0.65 0.30 0.35 0.59 0.39
Platform 0.30 0.41 0.58 0.69 0.23 0.17 0.38 0.11
Mozilla 0.18 0.33 0.50 0.62 0.18 0.08 0.27 0.04
JDT 0.34 0.42 0.59 0.70 0.22 0.18 0.31 0.12
Columba 0.42 0.56 0.62 0.76 0.24 0.32 0.50 0.25
PostgreSQL 0.36 0.44 0.60 0.74 0.21 0.29 0.69 0.23
Average 0.32 0.42 0.58 0.69 0.23 0.23 0.46 0.19
IBk. EALR is considered to be state-of-the-art learner for eÔ¨Äort-
aware JIT defect prediction [ 21,54] and all the other three learners
are widely used in defect prediction literature over past years [ 7,
9,13,21,30,50]. We evaluate all these learners on the six project
data sets using the same experiment scheme as we did in RQ1.
From Figure 4, we have the following observations. Firstly, the
performance of OneWay is signi/f_icantly be/t_ter than all these four
supervised predictors in terms of Recall andPopton all six data
sets. Also, EALR works be/t_ter than Random Forests, J48 and IBk,
which is consistent with Kamei et al‚Äôs /f_inding [21].
Secondly, according to F1, Random Forests and IBk perform
slightly be/t_ter than OneWay in two out of six data sets. For most
cases, OneWay has a similar performance to these supervised pre-
dictors and there is not much diÔ¨Äerence between them.
However, when reading Precision scores, we /f_ind that, in most
cases, supervised learners perform signi/f_icantly be/t_ter than OneWay .
Speci/f_ically, Random Forests, J48 and IBk outperform OneWay on
all data sets and EALR is be/t_ter on three data sets. /T_his /f_inding
is consistent with the observation in RQ1 where all unsupervised
predictors perform worse than supervised predictors for Precision .
From Table 6, we have the following observation. First of all, in
terms of Recall andPopt, the maximum diÔ¨Äerence in median values
between EALR and OneWay are 0 :15 and 0 :14, respectively, whichESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
are 83% and 23% improvements over 0 :18 and 0 :60 on Mozilla and
PostgreSQL data sets. For both measures, OneWay improves the av-
erage scores by 0 :1 and 0 :11, which are 31% and 19% improvement
over EALR. Secondly, according to F1, IBk outperforms OneWay
on three data sets with a large, medium and small advantage, re-
spectively. /T_he largest diÔ¨Äerence in median is 0 :1. Finally, as we
discussed before, the best supervised predictor for Precision , Ran-
dom Forests, has a very large advantage over OneWay on all data
sets. /T_he largest diÔ¨Äerence is 0 :46 on PostgreSQL data set.
Overall, according to the above analysis, we conclude that:
OneWay performs signi/f_icantly be/t_ter than all four supervised
learners in terms of Recall andPopt; It performs just as well
as other learners for F1. As for Precision, other supervised
predictors outperform OneWay .
Note the implications of this /f_inding: simple tools like OneWay
perform adequately but for all-around performance, more sophisti-
cated learners are recommended.
As to when to use OneWay or supervised predictors like Random
Forests, that is an open question. According to ‚ÄúNo Free Lunch
/T_heorems‚Äù[ 53], no method is always best and we show unsuper-
vised predictors are o/f_ten worse on a project-by-project basis. So
‚Äúbest‚Äù predictor selection is a ma/t_ter of local assessment, requiring
labelled training data (an issue ignored by Yang et al).
7 THREATS TO VALIDITY
Internal Validity . /T_he internal validity is related to uncontrolled
aspects that may aÔ¨Äect the experimental results. One threat to the
internal validity is how well our implementation of unsupervised
predictors could represent the Yang et al. ‚Äôs method. To mitigate this
threat, based on Yang et al ‚ÄúR‚Äùcode, we strictly follow the approach
described in Yang et al‚Äôs work and test our implementation on the
same data sets as in Yang et al. [ 54]. By comparing the performance
scores, we /f_ind that our implementation can generate the same
results. /T_herefore, we believe we can avoid this threat.
External Validity . /T_he external validity is related to the possi-
bility to generalize our results. Our observations and conclusions
from this study may not be generalized to other so/f_tware projects.
In this study, we use six widely used open source so/f_tware project
data as the subject. As all these so/f_tware projects are wri/t_ten in java,
we can not guarantee that our /f_indings can be directly generalized
to other projects, speci/f_ically to the so/f_tware that implemented in
other programming languages. /T_herefore, the future work might
include to verify our /f_indings on other so/f_tware project.
In this work, we used the data sets from [ 21,54], where totally
14 change metrics were extracted from the so/f_tware projects. We
build and test the OneWay learner on those metrics as well. How-
ever, there might be some other metrics that not measured in these
data sets that work well as indicators for defect prediction. For
example, when the change was commi/t_ted (e.g., morning, a/f_ter-
noon or evening), functionality of the the /f_iles modi/f_ied in this
change (e.g., core functionality or not). /T_hose new metrics that are
not explored in this study might improve the performance of our
OneWay learner.8 CONCLUSION AND FUTURE WORK
/T_his paper replicated and refutes Yang et al.‚Äôs results [ 54] on unsu-
pervised predictors for eÔ¨Äort-ware just-in-time defect prediction.
Not all unsupervised predictors work be/t_ter than supervised pre-
dictors (on all six data sets, for diÔ¨Äerent evaluation measures). /T_his
suggests that we can not randomly pick an unsupervised predictor
to perform eÔ¨Äort-ware JIT defect prediction. Rather, it is necessary
to use supervised methods to pick best models before deploying
them to a project. For that task, supervised predictors like OneWay
are useful to automatically select the potential best model.
In the above, OneWay peformed very well for Recall ,PoptandF1.
Hence, it must be asked: ‚ÄúIs defect prediction inherently simple?
And does it need anything other than OneWay ?‚Äù. In this context,
it is useful to recall that OneWay ‚Äôs results for precision were not
competitive. Hence we say, that if learners are to be deployed in
domains where precision is critical, then OneWay is too simple.
/T_his study opens the new research direction of applying simple
supervised techniques to perform defect prediction. As shown in
this study as well as Yang et al.‚Äôs work [ 54], instead of using tradi-
tional machine learning algorithms like J48 and Random Forests,
simply sorting data according to one metric can be a good defect
predictor model, at least for eÔ¨Äort-aware just-in-time defect pre-
diction. /T_herefore, we recommend the future defect prediction
research should focus more on simple techniques.
For the future work, we plan to extend this study on other so/f_t-
ware projects, especially those developed by the other programming
languages. A/f_ter that, we plan to investigate new change metrics
to see if that helps improve OneWay ‚Äôs performance.
9 ADDENDUM
As this paper was going to press, we learned of new papers that
updated the Yang et al. study: Liu et al. at EMSE‚Äô17 [ 31] and Huang
et al. at ICSMSE‚Äô17 [ 17]. We thank these authors for the courtesy
of sharing a pre-print of those new results. We also thank them
for using concepts from a pre-print of our paper in their work3.
Regretfully, we have yet to return those favors: due to deadline
pressure, we have not been able to con/f_irm their results.
As to technical speci/f_ics, Liu et al. use a single churn measure
(sum of number of lines added and deleted) to build an unsupervised
predictors that does remarkably be/t_ter than OneWay and EARL
(where the la/t_ter could access all the variables). While this result
is currently uncon/f_irmed, it could well have ‚Äúraised the bar‚Äù for
unsupervised defect prediction. Clearly, more experiments are
needed in this area. For example, when comparing the Liu et al.
methods to OneWay and standard supervised learners, we could
(a) give all learners access to the churn variable; (b) apply the Yang
transform of1
M¬πc¬∫to all variables prior to learning; (c) use more
elaborate supervised methods including synthetic minority over-
sampling [1] and automatic hyper-parameter optimization [7].
ACKNOWLEDGEMENTS
/T_he work is partially funded by an NSF award #1302169.
3Our pre-print was posted to arxiv.org March 1, 2017. We hope more researchers will
use tools like arxiv.org to speed along the pace of so/f_tware research.Revisiting Unsupervised Learning for Defect Prediction ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
REFERENCES
[1] Amritanshu Agrawal and Tim Menzies. ‚Äùbe/t_ter data‚Äù is be/t_ter than ‚Äùbe/t_ter data
miners‚Äù (bene/f_its of tuning SMOTE for defect prediction). CoRR , abs/1705.03697,
2017.
[2] Fumio Akiyama. An example of so/f_tware system debugging. In IFIP Congress (1) ,
volume 71, pages 353‚Äì359, 1971.
[3] Erik Arisholm and Lionel C Briand. Predicting fault-prone components in a java
legacy system. In Proceedings of the 2006 ACM/IEEE international symposium on
Empirical so/f_tware engineering , pages 8‚Äì17. ACM, 2006.
[4] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a prac-
tical and powerful approach to multiple testing. Journal of the Royal Statistical
Society. Series B (Methodological) , pages 289‚Äì300, 1995.
[5]Shyam R Chidamber and Chris F Kemerer. A metrics suite for object oriented
design. IEEE Transactions on So/f_tware Engineering , 20(6):476‚Äì493, 1994.
[6] Marco D‚ÄôAmbros, Michele Lanza, and Romain Robbes. An extensive comparison
of bug prediction approaches. In 2010 7th IEEE Working Conference on Mining
So/f_tware Repositories , pages 31‚Äì41. IEEE, 2010.
[7]Wei Fu, Tim Menzies, and Xipeng Shen. Tuning for so/f_tware analytics: Is it
really necessary? Information and So/f_tware Technology , 76:135‚Äì146, 2016.
[8] Wei Fu, Vivek Nair, and Tim Menzies. Why is diÔ¨Äerential evolution be/t_ter than
grid search for tuning defect predictors? arXiv preprint arXiv:1609.02613 , 2016.
[9]Takafumi Fukushima, Yasutaka Kamei, Shane McIntosh, Kazuhiro Yamashita,
and Naoyasu Ubayashi. An empirical study of just-in-time defect prediction
using cross-project models. In Proceedings of the 11th Working Conference on
Mining So/f_tware Repositories , pages 172‚Äì181. ACM, 2014.
[10] Todd L Graves, Alan F Karr, James S Marron, and Harvey Siy. Predicting fault in-
cidence using so/f_tware change history. IEEE Transactions on So/f_tware Engineering ,
26(7):653‚Äì661, 2000.
[11] Philip J Guo, /T_homas Zimmermann, Nachiappan Nagappan, and Brendan Mur-
phy. Characterizing and predicting which bugs get /f_ixed: an empirical study of
microso/f_t windows. In Proceedings of the 32nd ACM/IEEE International Conference
on So/f_tware Engineering - Volume 1 , volume 1, pages 495‚Äì504. ACM, 2010.
[12] Mark Hall, Eibe Frank, GeoÔ¨Ärey Holmes, Bernhard Pfahringer, Peter Reutemann,
and Ian H Wi/t_ten. /T_he weka data mining so/f_tware: an update. ACM SIGKDD
Explorations Newsle/t_ter , 11(1):10‚Äì18, 2009.
[13] Tracy Hall, Sarah Beecham, David Bowes, David Gray, and Steve Counsell. A
systematic literature review on fault prediction performance in so/f_tware engi-
neering. IEEE Transactions on So/f_tware Engineering , 38(6):1276‚Äì1304, 2012.
[14] Maurice Howard Halstead. Elements of so/f_tware science , volume 7. Elsevier New
York, 1977.
[15] Maggie Hamill and Katerina Goseva-Popstojanova. Common trends in so/f_tware
fault and failure data. IEEE Transactions on So/f_tware Engineering , 35(4):484‚Äì496,
2009.
[16] Ahmed E Hassan. Predicting faults using the complexity of code changes. In
Proceedings of the 31st International Conference on So/f_tware Engineering , pages
78‚Äì88. IEEE, 2009.
[17] Qiao Huang, Xin Xia, and David Lo. Supervised vs unsupervised models: a holis-
tic look at eÔ¨Äort-aware just-in-time defect prediction. In 2017 IEEE International
Conference on So/f_tware Maintenance and Evolution . IEEE, 2017.
[18] Tian Jiang, Lin Tan, and Sunghun Kim. Personalized defect prediction. In
Proceedings of the 28th IEEE/ACM International Conference on Automated So/f_tware
Engineering , pages 279‚Äì289. IEEE, 2013.
[19] Xiao-Yuan Jing, Shi Ying, Zhi-Wu Zhang, Shan-Shan Wu, and Jin Liu. Dictionary
learning based so/f_tware defect prediction. In Proceedings of the 36th International
Conference on So/f_tware Engineering , pages 414‚Äì423. ACM, 2014.
[20] Dennis Kafura and Geereddy R. Reddy. /T_he use of so/f_tware complexity metrics
in so/f_tware maintenance. IEEE Transactions on So/f_tware Engineering , (3):335‚Äì343,
1987.
[21] Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus,
Anand Sinha, and Naoyasu Ubayashi. A large-scale empirical study of just-in-
time quality assurance. IEEE Transactions on So/f_tware Engineering , 39(6):757‚Äì773,
2013.
[22] Taghi M Khoshgo/f_taar and Edward B Allen. Modeling so/f_tware quality with.
Recent Advances in Reliability and /Q_uality Engineering , 2:247, 2001.
[23] Taghi M Khoshgo/f_taar and Naeem Seliya. So/f_tware quality classi/f_ication model-
ing using the sprint decision tree algorithm. International Journal on Arti/f_icial
Intelligence Tools , 12(03):207‚Äì225, 2003.
[24] Taghi M Khoshgo/f_taar, Xiaojing Yuan, and Edward B Allen. Balancing misclassi-
/f_ication rates in classi/f_ication-tree models of so/f_tware quality. Empirical So/f_tware
Engineering , 5(4):313‚Äì330, 2000.
[25] Sunghun Kim, E James Whitehead Jr, and Yi Zhang. Classifying so/f_tware changes:
Clean or buggy? IEEE Transactions on So/f_tware Engineering , 34(2):181‚Äì196, 2008.
[26] Sunghun Kim, /T_homas Zimmermann, E James Whitehead Jr, and Andreas Zeller.
Predicting faults from cached history. In Proceedings of the 29th International
Conference on So/f_tware Engineering , pages 489‚Äì498. IEEE, 2007.
[27] Ekrem Kocaguneli, Tim Menzies, Ayse Bener, and Jacky W Keung. Exploiting
the essential assumptions of analogy-based eÔ¨Äort estimation. IEEE Transactions
on So/f_tware Engineering , 38(2):425‚Äì438, 2012.[28] A G¬®unes ¬∏Koru, Dongsong Zhang, Khaled El Emam, and Hongfang Liu. An
investigation into the functional form of the size-defect relationship for so/f_tware
modules. IEEE Transactions on So/f_tware Engineering , 35(2):293‚Äì304, 2009.
[29] Taek Lee, Jaechang Nam, DongGyun Han, Sunghun Kim, and Hoh Peter In.
Micro interaction metrics for defect prediction. In Proceedings of the 19th ACM
SIGSOFT Symposium and the 13th European Conference on Foundations of So/f_tware
Engineering , pages 311‚Äì321. ACM, 2011.
[30] Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. Bench-
marking classi/f_ication models for so/f_tware defect prediction: A proposed frame-
work and novel /f_indings. IEEE Transactions on So/f_tware Engineering , 34(4):485‚Äì
496, 2008.
[31] Jinping Liu, Yuming Zhou, Yibiao Yang, Hongmin Lu, and Baowen Xu. Code
churn: A neglected metric in eÔ¨Äort-aware just-in-time defect prediction. In
Empirical So/f_tware Engineering and Measurement, 2017 ACM/IEEE International
Symposium on . IEEE, 2017.
[32] Shinsuke Matsumoto, Yasutaka Kamei, Akito Monden, Ken-ichi Matsumoto, and
Masahide Nakamura. An analysis of developer metrics for fault prediction. In
Proceedings of the 6th International Conference on Predictive Models in So/f_tware
Engineering , page 18. ACM, 2010.
[33] /T_homas J McCabe. A complexity measure. IEEE Transactions on So/f_tware Engi-
neering , (4):308‚Äì320, 1976.
[34] /T_hilo Mende and Rainer Koschke. EÔ¨Äort-aware defect prediction models. In
2010 14th European Conference on So/f_tware Maintenance and Reengineering , pages
107‚Äì116. IEEE, 2010.
[35] Tim Menzies, Jeremy Greenwald, and Art Frank. Data mining static code at-
tributes to learn defect predictors. IEEE Transactions on So/f_tware Engineering ,
33(1), 2007.
[36] Tim Menzies, Zach Milton, Burak Turhan, Bojan Cukic, Yue Jiang, and Ay s ¬∏e
Bener. Defect prediction from static code features: current results, limitations,
new approaches. Automated So/f_tware Engineering , 17(4):375‚Äì407, 2010.
[37] Ayse Tosun Misirli, Ayse Bener, and Resat Kale. Ai-based so/f_tware defect pre-
dictors: Applications and bene/f_its in a case study. AI Magazine , 32(2):57‚Äì68,
2011.
[38] Audris Mockus and David M Weiss. Predicting risk of so/f_tware changes. Bell
Labs Technical Journal , 5(2):169‚Äì180, 2000.
[39] Akito Monden, Takuma Hayashi, Shoji Shinoda, Kumiko Shirai, Junichi Yoshida,
Mike Barker, and Kenichi Matsumoto. Assessing the cost eÔ¨Äectiveness of fault
prediction in acceptance testing. IEEE Transactions on So/f_tware Engineering ,
39(10):1345‚Äì1357, 2013.
[40] Raimund Moser, Witold Pedrycz, and Giancarlo Succi. A comparative analysis of
the eÔ¨Éciency of change metrics and static code a/t_tributes for defect prediction.
InProceedings of the 30th International Conference on So/f_tware Engineering , pages
181‚Äì190. ACM, 2008.
[41] Nachiappan Nagappan and /T_homas Ball. to predict system defect density. In
Proceedings of the 27th International Conference on So/f_tware Engineering , pages
284‚Äì292. IEEE, 2005.
[42] Nachiappan Nagappan, /T_homas Ball, and Andreas Zeller. Mining metrics to
predict component failures. In Proceedings of the 28th International Conference
on So/f_tware Engineering , pages 452‚Äì461. ACM, 2006.
[43] Jaechang Nam, Sinno Jialin Pan, and Sunghun Kim. Transfer defect learning. In
Proceedings of the 2013 International Conference on So/f_tware Engineering , pages
382‚Äì391. IEEE, 2013.
[44] /T_homas J Ostrand, Elaine J Weyuker, and Robert M Bell. Where the bugs are. In
ACM SIGSOFT So/f_tware Engineering Notes , volume 29, pages 86‚Äì96. ACM, 2004.
[45] /T_homas J Ostrand, Elaine J Weyuker, and Robert M Bell. Predicting the location
and number of faults in large so/f_tware systems. IEEE Transactions on So/f_tware
Engineering , 31(4):340‚Äì355, 2005.
[46] Corina S Pasareanu, Peter C Mehlitz, David H Bushnell, Karen Gundy-Burlet,
Michael Lowry, Suze/t_te Person, and Mark Pape. Combining unit-level symbolic
execution and system-level concrete execution for testing nasa so/f_tware. In
Proceedings of the 2008 International Symposium on So/f_tware Testing and Analysis ,
pages 15‚Äì26. ACM, 2008.
[47] Foyzur Rahman, Sameer Khatri, Earl T Barr, and Premkumar Devanbu. Com-
paring static bug /f_inders and statistical prediction. In Proceedings of the 36th
International Conference on So/f_tware Engineering , pages 424‚Äì434. ACM, 2014.
[48] Jeanine Romano, JeÔ¨Ärey D Kromrey, Jesse Coraggio, JeÔ¨Ä Skowronek, and Linda
Devine. Exploring methods for evaluating group diÔ¨Äerences on the nsse and
other surveys: Are the t-test and cohen/f_isd indices the most appropriate choices.
InAnnual Meeting of the Southern Association for Institutional Research , 2006.
[49] Forrest Shull, Ioana Rus, and Victor Basili. Improving so/f_tware inspections by
using reading techniques. In Proceedings of the 23rd International Conference on
So/f_tware Engineering , pages 726‚Äì727. IEEE, 2001.
[50] Burak Turhan, Tim Menzies, Ay s ¬∏e B Bener, and Justin Di Stefano. On the relative
value of cross-company and within-company data for defect prediction. Empirical
So/f_tware Engineering , 14(5):540‚Äì578, 2009.
[51] Song Wang, Taiyue Liu, and Lin Tan. Automatically learning semantic features
for defect prediction. In Proceedings of the 38th International Conference on
So/f_tware Engineering , pages 297‚Äì308. ACM, 2016.ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
[52] Maurice V Wilkes. Memoirs ofa computer pioneer. Cambridge, Mass., London ,
1985.
[53] David H Wolpert. /T_he supervised learning no-free-lunch theorems. In So/f_t
Computing and Industry , pages 25‚Äì42. Springer, 2002.
[54] Yibiao Yang, Yuming Zhou, Jinping Liu, Yangyang Zhao, Hongmin Lu, Lei Xu,
Baowen Xu, and Hareton Leung. EÔ¨Äort-aware just-in-time defect prediction:
simple unsupervised models could be be/t_ter than supervised models. In Proceed-
ings of the 2016 24th ACM SIGSOFT International Symposium on Foundations ofSo/f_tware Engineering , pages 157‚Äì168. ACM, 2016.
[55] Zuoning Yin, Ding Yuan, Yuanyuan Zhou, Shankar Pasupathy, and Lakshmi
Bairavasundaram. How do /f_ixes become bugs? In Proceedings of the 19th ACM
SIGSOFT Symposium and the 13th European Conference on Foundations of So/f_tware
Engineering , pages 26‚Äì36. ACM, 2011.
[56] /T_homas Zimmermann, Rahul Premraj, and Andreas Zeller. Predicting defects for
eclipse. In Proceedings of the /T_hird International Workshop on Predictor Models in
So/f_tware Engineering , page 9. IEEE Computer Society, 2007.