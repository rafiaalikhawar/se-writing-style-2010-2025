What Makes Killing a Mutant Hard
Willem Visser
Department of Computer Science, Stellenbosch University, South Africa
willem@gmail.com
ABSTRACT
Mutation operators have been studied at length to determine
which ones are the\best"at some metric (for example creates
the least equivalent mutants, creates hard-to-kill mutants,
etc.). These studies though have focused on specic test
suites, where the test inputs and oracles are xed, which
leads to results that are strongly inuenced by the test suites
and thus makes the conclusions potentially less general. In
this paper we consider all test inputs and we assume we
have no prior knowledge about the likelihood of any specic
inputs. We will also show how varying the strength of the
oracle have a big impact on the results. We only consider a
few mutation operators (mostly relational), only a handful
of programs to mutate (amenable to probabilistic symbolic
execution), and only consider how likely it is that a mutant
is killed. A core nding is that the likelihood of reaching
the source line where the mutation is applied, is an import-
ant contributor to the likelihood of killing the mutant and
when we control for this we can see which operators create
mutations that are too easy versus very hard to kill.
CCS Concepts
Software and its engineering !Software testing
and debugging;
Keywords
Mutation Testing, Probabilistic Symbolic Execution
1. INTRODUCTION
In mutation testing (see excellent survey of the history of
the eld in [9]) one makes many small changes to the original
program (called mutants) via the application of mutation op-
erators, and then run a test suite to see how many of these
mutants are detected (referred to as being killed); the ratio
of killed mutants over the total mutants is called the muta-
tion score for the test suite. The appeal of this mutation
score as a test adequacy measure is in large part due to itbeing related to how eective a test suite is at nding errors.
There is of course the small matter of whether mutations are
proxies for real errors, but there seems to be more and more
evidence suggesting they are (the most comprehensive study
on this topic so far is [10]). This is not the topic of the cur-
rent paper, here we are instead interested in another use of
mutations, namely as a way to seed faults in a piece of code.
More specically, when seeding faults via mutations, how
dicult is it to detect these mutations/faults?
Although fault seeding was the original motivation for the
work, the outcomes of this research is also directly applicable
to probably the biggest drawback of mutation testing: there
are too many mutations possible and thus the technique has
scalability issues. We will show not only how to reduce the
set of mutation operators, by focusing on ones that create
mutations that are in general hard to kill, but also where to
apply these mutations.
We are not the rst to consider how dicult it is to kill
mutants, the most closely related work to ours is that of
Yao et al. in [16]. However what makes our work unique is
that we basically will use an approach that is in general im-
possible: do an exhaustive analysis of all possible test inputs
to determine how hard a mutant is to kill. Our approach will
produce the percentage of the input space that will exhibit a
dierence in the oracle for the mutated program, from that
of the original. In other words on how much of the input
space is the mutant killed. What this allows us, is to remove
the bias an existing test suite might have when determining
whether a mutant can be killed. As a by-product, our ap-
proach will be able to detect equivalent mutants, i.e. ones
where zero percent of the inputs show a dierence between
the oracle results for the mutation and the original.
Obviously there needs to be a tradeo that makes this
analysis possible and indeed what we lose is the generality
of the results. We will only be able to handle programs
that t with the following requirements: must be amenable
to symbolic execution as well as model counting (that calcu-
lates the number of solutions to a constraint). In fact in this
paper we will restrict even more by imposing that only lin-
ear integer-arithmetic operations are allowed. We will only
look at some common mutation operators, but our approach
is general for any operator, but space and time limitations
dictate this limitation.
So our new idea is that we can only look at some programs,
but for those programs, we can do a much more thorough
analysis of mutation operators than what has gone before.
We will make three main contributions with regards to what
makes a mutant hard to kill:
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970345
39
Reachability of the mutation is important, for example
some mutation operators creates mutations that are
always killed if you can reach them.
Mutation operators are not all equally hard to kill (once
you control for the reachability of the mutation). We
will show which ones tend to create hard-to-kill mutants
and which creates mutants that are easily killed.
Oracle sensitivity is another major component of the de-
tection capability. We will show the delicate balance
between equivalent mutants and hard-to-kill mutants.
2. SMALL EXAMPLE
Consider the very simple Java method below
boolean Test(int i, int j) { return i <= j; }
and assume we want to apply the classic relational muta-
tion operator to it, which will change the <=to<,>=,>,
==and !=to produce 5 mutations. Assume further that we
consider the test oracle to be whether the mutations return
the same output as the original. Lastly our analysis requires
a bound on the size of the variables iand jwhich we pick
to be the range 0 i; j99 (they are of course bounded in
Java anyway, but we pick a smaller range just to illustrate
the ideas better). In this very simple case it should be obvi-
ous that the <=to<mutation is the hardest to kill and our
tool indicates only 100 out of the 10000 possible inputs will
kill this mutant (in other words 1% of the inputs); similarly
<=to>will be killed by all inputs, i.e. 100% of the inputs.
Since our analysis is actually at the bytecode level, there
is another mutation operator that can be applied and that is
to change true tofalse and vice versa (of course actually it
is changing 1 to 0 and vice versa at the bytecode level). Intu-
itively it would be obvious that both these mutations should
be killed by 100% of the inputs, but interestingly it turns
out in both cases it hovers around 50% (50 :5% and 49 :5%
respectively). Why is this? It is because to reach these
mutations you rst need to evaluate the boolean condition,
and only about half the values reach each of the locations
(i <= j and i > j ). So on the face of it, it looks like this
mutation is not bad, but in fact if you control for how many
values reach the mutation point it turns out this mutation
is terrible, since all inputs will kill it. In other words all
the input values that reach the mutation point will kill the
mutant.
The output our tool will produce for this program is the
following, note that during the translation from source code
to bytecode some operations are negated (for example the
<=becomes a >at the bytecode level):
Table 1: Analysis Output
Description Dierent Reaching Kill Ratio
1 [>,==] 5050 10000 50:5%
1 [>,!=] 4950 10000 49:5%
1 [>,<] 9900 10000 99%
1 [>,<=] 10000 10000 100%
1 [>,>=] 100 10000 1%
1 [1,0] 5050 5050 100%
1 [0,1] 4950 4950 100%The Description shows the mutation applied including the
line number, Dierent shows the number of input values
that show a dierence in output between the mutated and
original program, Reaching shows the number of inputs that
reach the point where the mutation is applied, lastly Kill
Ratio is the ratio of the values showing a dierence over the
ones that reached the mutation. In the following section we
will describe the infrastructure required to produce this data
automatically.
3. APPROACH
The approach taken here is similar to that proposed in [4]
and is just specialised for the analysis of mutations. Specic-
ally our approach is based around doing symbolic execution
of both the original and the mutated program and recording
the path conditions under which the oracle diers between
them. These path conditions combined with constraints on
the ranges for each variable is then fed to a model counter
to count the number of satisfying assignments for each path
condition. We use Symbolic PathFinder (SPF) [12] for doing
the symbolic execution and we use Green [14] to interface to
the model counting (which can then use the model counters
LattE [13] or Barvinok [7] as a back-end).
3.1 Symbolic Execution
Symbolic execution has been well studied the last few
years and we point the interested reader to the following
two surveys on the topic [2, 3]. The basic idea is just to con-
sider inputs symbolic, rather than concrete, and to collect
constraints (called path conditions) on these inputs during
the symbolic analysis when encountering branching instruc-
tions. Non-branching instructions, such as assignments, just
change the contents of variables that could later be involved
in branching. We will here use classic symbolic execution,
and not dynamic symbolic execution, that operates a little
dierently (see [3]). However please note either will work
equally well. As noted above we use SPF as our symbolic
execution engine.
For each program we want to analyse we create the fol-
lowing (pseudo) code that we will symbolically execute:
analyser(params) {
mutation = mutatedMethodX(params);
original = originalMethodX(params);
if (!Oracle(mutation,original))
count();
}
The parameters params above are symbolic and both pro-
grams (original and mutated version) takes the same (sym-
bolic) parameters. Note that if the two programs are identical
then exactly the same paths will be followed in both pro-
grams, after the rst though the path conditions will be xed
and the second one will just collect the same constraints.
An important restriction on the code being analysed is that
it may not change the input parameters, because that will
break the above rule that identical programs will follow the
same paths. This is not a serious restriction.
Another more severe restriction is that the code being
analysed may not contain loops that are dependent on the
symbolic inputs. If this happens then the mutated method
will not terminate during the symbolic analysis. One pos-
sible way around this issue is to limit loop un-rollings, but
40here we rather not do that since that makes the results par-
tial. An interesting scenario occurs though when a mutation
creates an innite loop, see Section 3.3.
A mutation is almost always dierent than the original
though, at least syntactically, but it might be that the Oracle
is not precise enough to detect the dierence. We will re-
visit this point in Section 4.3. If we consider our simple
example from Section 2 and the mutation >to>=(recall the
Oracle is identity) then the path condition where the Oracle
is unequal would be input_i <= input_j AND input_i >=
input_j where the rst conjunct comes from the mutated
code and the second conjunct from the original.
3.2 Mutator
We use an in-house mutation tool1that mutates the Java
bytecode. It takes as input the class le as well as a list of
methods on which mutation is to be applied. Each method
that we want to mutate we create a copy of and rename it
by prepending the word "mutated" (as in the pseudo code
above) and this (and the class it is from) then becomes the
input to the mutation tool. The output is a class le that
contains code as above, including the mutated method and
the original method (as in the pseudo code). Right now
we do not support high-order mutations [8], i.e. mutations
applied on programs that are already mutated.
For this study we only support some common mutation
operators: relational operators, integer constant operators
and arithmetic operators (+ to - for example). As stated
before there is no limitation to what can be used as muta-
tion operators, but for now we just focused on some popular
operators. Similarly doing a source code mutator would al-
low more exibility for the operators that we can support
(for example deleting statements), and we are working on
such an extension.
There is one extra step that the mutator needs to accom-
plish and that is to indicate where the mutation is being
applied, since we want to use that position to record the
number of inputs that reach the mutation. Our solution to
this problem is very simple, we just add a NOP instruc-
tion2right before the mutation in the control-ow. For in
case a Java compiler might add a NOP for some reason we
just check that there are not more NOPs than mutations
(currently xed at 1), which so far has never been the case.
There are probably other, more elegant, solutions to this
problem but this one ts well with our analysis, as will be
discussed next.
3.3 Analysis
The most important point about the analysis is that we re-
cord path conditions and then use a model counter to count
the number of solutions for the path conditions.
The actual analysis is done via a JPF Listener, which al-
lows code to be added on callbacks from JPF's execution.
The main method we are listening on is instructionExecuted
which is called for every bytecode instruction being executed.
We listen for these 4 specic events:
The entry method to the symbolic analysis ( analyser
from above) to nd the domain size for each symbolic
variable.
1Modications to the Jumble tool from
http://jumble.sourceforge.net/.
2The NOP bytecode instruction doesn't seem to be used by
any Java compiler we testedThe rst call to the NOP instruction on each path to
record the path condition and then to use that to cal-
culate the values owing to the point where a mutation
is applied. Note that it is possible to hit the same NOP
instruction more than once if it is in a loop or if it is
in a method that is called more than once.
Calls to count to determine the path conditions on
which the mutation and the original dier, and then
count the solutions to these path conditions.
When a property is violated ( PropertyViolated call),
which here refers to when the mutation caused the
program to throw an exception. We again record the
path condition and do the model counting for it (as
with the count case above) since the original doesn't
throw exceptions (of course it can in theory, but we
don't consider those cases at this point)
The nal step at the end of the symbolic execution run is to
produce the output as in Table 1.
To make this analysis more ecient we use Green to in-
terface to the model counting. Basically, Green uses par-
titioning, canonisation and caching of results to speed up
the analysis. It was previously shown that when analysing
mutations, where most of the program is unchanged, the
speed up by using Green can be substantial [14]. Since we
are only using a model counter for linear integer arithmetic
here (LattE to be precise) we are of course putting one more
restriction on the type of programs we can analyse, i.e. pro-
grams that only contain linear integer arithmetic (nothing
non-linear for example). This is also a semi-articial restric-
tion, since we can use model counters for more domains, but
for some of these, the counting becomes approximate.
One tricky scenarios that can occur during the analysis
is when a mutation creates an innite loop. Detecting in-
nite loops is of course non-trivial and we just assume it is
an innite loop if a loop is un-rolled too often during the
analysis (knowing that the original program didn't contain
an innite loop). Since this is quite rare we simply label the
mutants for which this occurs and try to validate by hand
that it is in fact an innite loop.
4. EVALUATION
We will do three dierent experiments in this section3.
Firstly we will show how our analysis compares to [16] where
a similar, but manually-driven, approach was taken, then we
will show that controlling for the location of the mutation
is important when considering mutations that are hard to
detect, and lastly, we will show that the power of the oracle
is crucial for killing mutants. We will use the following two
denitions:
Stubborn Mutant We will consider a mutant hard-to-kill,
or in the terminology of [16], stubborn, if less than
0:1% of the inputs can kill the mutation. We will refer
to mutations that are stubborn even after controlling
for the number of values reaching the mutation loca-
tion as really stubborn . Sometimes, when the con-
text is obvious, we will refer to really stubborn mutants
as just stubborn.
Equivalent Mutant If zero (percent) of the inputs can kill
a mutant it is considered equivalent to the original.
3Artefacts available from http://www.cs.sun.ac.za/~wvisser
41We are not reporting any runtime for our experiments,
but no run took more than 20 mins and the vast majority
nished in seconds (due to reusing results cached by Green).
4.1 Hard-to-Kill
From [16] it seems relational mutation operators is an in-
teresting class where they found that these operators create
about the same amount of equivalent and stubborn mutants.
Their denition of stubborn mutants are that a branch-
adequate test suite cannot kill the mutant, but that it is not
equivalent (which they determined by hand). We picked
two of the programs they analysed, namely Triangle and
TCAS and also analysed them. We greatly appreciate that
they made all their examples available, plus all their res-
ults: see www.cs.ucl.ac.uk/sta/Y.Jia/projects/equivalent
mutants. We also added two of our own versions of Triangle
(there are tens of examples of this classic problem available
online with and without bugs and thus it is great to study
in mutation analysis). Interestingly the example from [16] is
incorrect, but that makes no dierence to their results, nor
this comparison. Since we do our analysis at the bytecode
level we can also get results for logical connector mutation
operators (&& to kkand vice versa), by lifting them out of
the results for the bytecode mutations - we did this by hand
for the TCAS example. We used the following ranges for our
symbolic variables: [0..99] for triangle and [  1000..10000] for
TCAS.
Table 2: Relational Mutation Operator Examples
Program Muts Stubborn Equivalent
T0-[16] 40 3 7
T0-us 40 0 5
T1 85 6 6
T2 55 0 6
TCAS-[16] 70 18 20
TCAS-us 85 20 22
TCAS-LC-[16] 17 10 0
TCAS-LC-us 17 3 0
Program is the program being analysed where T0 is the
Triangle version from [16] and the second entry is our ana-
lysis of the same code. Since we are only looking at rela-
tional operators here for mutation, Muts refer to the number
of these mutations, Stubborn is the number of hard-to-kill
mutants, but has a dierent meaning for us than for [16].
Equivalent is the number of equivalent mutants.
First thing to note is that [16] gets two more equivalent
mutants: this shows how hard it is to determine whether
something is in fact equivalent by hand; we could conrm
that the two extra ones are in fact not equivalent. Neither
of them are even stubborn by our denition (0 :9% and 32%
likely). Since T0 is in fact not semantically equivalent to T1
and T2 means we cannot compare them any further.
However if we consider T1 and T2 against each other
where they both correctly solve the triangle problem one can
see what a big inuence even just dierent solutions to the
same problem has when looking at mutations: T1 is a much
more verbose solution and thus have more scope for muta-
tions and even 6 stubborn ones, whereas T2 that solves the
exact same problem has no stubborn mutants. Note how-
ever that the two semantically equivalent programs has the
same number of equivalent mutants.The TCAS results from [16] seems to have missed two lines
where a number of mutations were possible, hence only con-
sidering 70 mutations versus our 85, it also explains why
they missed two equivalent mutants. Note that TCAS has
the same number of mutations as the larger triangle ex-
ample, but a lot more stubborn and equivalent mutants.
The last two entries in Table 2 is where we also looked at
the Logical Connector (LC) mutations, which [16] found to
be particularly good for creating stubborn mutants but few
equivalent mutants. Our results mirror their's here, but we
nd less stubborn mutations.
4.2 Controlling for Location
A well known principle in mutation testing is the so-called
RIP requirements for killing a mutant [15], which dictates
you must be able to (R)each the mutation point, it must
(I)nfect the state of the program and this must (P)ropogate
to somewhere it can be observed by an oracle. Test cases
that can show infections are said to weakly kill the muta-
tions and ones that can show a dierence in the oracle are
strongly killing the mutants. In this section we will consider
only strongly killing the mutations (i.e. with an oracle), but
we will take a novel approach to the reachability issue by not
just looking at whether it is reachable, but rather howreach-
able it is (by considering the number of inputs that reach
the mutation point). Furthermore, we will show that if you
consider only the values that reach the mutation then one
can have a much better handle on which mutation operators
create stubborn mutants, and which do not.
Table 3: Stubborn after Controlling for Location
Prog Muts Stubb Really Always Easy
<0:1% <0:1% 100% >33%
T0 45(40) 0 0 9(5) 29(24)
T1 104(85) 7(6) 3(3) 12(4) 79(61)
T2 63(55) 1(0) 0 8(3) 45(38)
TCAS 223(185) 40(32) 28(24) 21(12) 64(46)
Table 3 shows the results when we control for the number
of values that reach the mutation (after the jj).Muts are
the total number of mutations applied (in these examples
there are only relational, constant replacements and arith-
metic operator replacement). Numbers in parenthesis are for
just the relational mutation operators (hence comparable to
Table 2 except for TCAS where these are the raw numbers
whereas in Table 2 we post-processed them to lift it to the
source level for comparison with [16]). The Stubb column
indicates the number of hard-to-kill mutants, i.e. less than
0:1% of the inputs kill the mutation, and after the jjwe
see the ones that are really stubborn ( Really ) as well as the
ones that now become inevitably killed ( Always ) and those
that are now Easy to kill since more than 33% of the inputs
reaching the mutation can kill it.
What this data is showing, should come as somewhat of
a surprise to most users of mutation testing: when taking
how hard it is to reach a mutation point out of the equation
it seems the majority of mutations are easy to kill.
First thing to notice is that the constant and arithmetic
replacements seem to account for almost half the 100% killed
cases; in fact if we consider the Allways-Kill ratio, i.e. 100%
killed over total number of these mutants we get 37%: T0
(4/5), T1 (8/19), T2 (5/8) and TCAS (9/38).
42The problem is that even for the relational replacement,
which is probably the most popular class of mutation op-
erator, they are simply too easy to kill. If we consider the
Easy-Kill ratio, i.e. >33% killed over the total relational
mutations we get 46%: T0 (24/40), T1 (61/85), T2 (38/55)
and TCAS (46/185).
The good news is that not all relational replacements are
created equally, and when we look at the operators individu-
ally we see that some are more prone to creating easy/always
killed mutants and some are better at creating stubborn
mutants. Table 4 show the operations in terms of the per-
centage that fall into each category.
Table 4: Relational Replacement Always above 20%
and Stubborn above 20%
Operator Equiv Stubborn Always Easy
!=,==(17) 0:00% 5:88% 23:53% 64:71%
<,>=(5) 80:00% 0:00% 20:00% 20:00%
<=,>(18) 0:00% 0:00% 22:22% 88:69%
==,!=(24) 0:00% 8:33% 20:83% 62:50%
==,>(24) 0:00% 8:33% 20:83% 62:50%
>,<=(6) 0:00% 0:00% 50:00% 83:33%
>=,<(3) 0:00% 0:00% 33:33% 100:00%
<,<=(5) 80:00% 20:00% 0:00% 0:00%
<=,<(24) 22:22% 27:78% 0:00% 5:58%
>,>=(6) 16:67% 33:33% 0:00% 0:00%
>=,>(3) 0:00% 100:00% 0:00% 0:00%
Table 4 shows the results per operator ( Operator ) with
the number in the parenthesis indicating how many of these
mutations there were, Equiv for the percentage equivalent
mutants, Stubborn the percentage mutants that are hard to
kill (less than 0 :1% of the inputs reaching the mutation),
Always the percentage killed with 100% of the inputs reach-
ing and Easy the percentage that kills the mutant with more
than 33% of the inputs reaching the mutation. Above the di-
viding line we have all the operators that were Always above
20% (i.e. the bad operators since they are too easy to kill)
and below the line we have all the ones where Stubborn is
above 20% (i.e. the good operators that are hard to kill even
after controlling for reaching the location of the mutation).
With one exception we have the NOT operations above
the line (only [ ==,>] doesn't t) and we have the OFF-
BY-ONE cases below the line. At this point pretty much
anyone will think Dah!, I did not need all this machinery to
tell me something this obvious. Of course one could argue
that we have too little data, which is a fair point, but it is
very hard to argue against these intuitive results though.
4.3 Oracle Strength
In the previous two sections we looked at code where the
oracle has been rather precise, in fact the identity function
applied to the outputs of the mutated and original program.
This means that we made sure of reaching the mutation,
since we consider all inputs (and our examples has no dead
code, since we never have 0% of the inputs reach a muta-
tion), and we made sure that if the error infected the state
and propagate, the oracle will catch it, the only real vari-
ables were the mutation and the semantics of the code. The
problem is that in the real world the oracle is not always
that precise and this could also inuence things.
Here we will analyse a Binary Search Tree (this specicexample has been well studied by the testing community [1])
with two dierent oracles: the representation invariant (re-
pOk) and one where the linearisation of the tree is checked
(linearise). This code also introduces loops. Basically we
will look at all sequences of operations to the tree of length
4 where each step is either adding or removing something
from the tree; after the complete sequence we check the or-
acle (i.e. check that the mutant is strongly killed). In the
mutated case either the add or remove operation is mutated.
All variables that we add or remove are symbolic (with a
range of [0..9]).
Table 5: BST with dierent oracles.
Linearise repOK
Oper Equiv Alw Easy Equiv Alw Easy
All(67) 30% 21% 57% 66% 15% 31%
!Rel(12) 83% 0% 0% 83% 0% 0%
Rel(55) 18% 25% 69% 62% 18% 38%
NOT(23) 4% 48% 78% 47% 34% 52%
Table 5 shows the results for the two oracles ( Linearise
and repOk ). We again show the percentages of equivalent
(Equiv ), always killed ( Alw) and too easily killed ( Easy), but
no stubborn cases, since there were none. The results are
shown for all ( All) the mutations, the non-relational ( !Rel)
ones (arithmetic and constant replacement), all the rela-
tional cases ( Rel) and lastly the negated cases ( NOT ) which
we saw previously to produce lots of easily killed mutations
(including always killed). We don't show any data for the
o-by-one mutations, since there was only one in the code
and that produced an equivalent mutant. As before these
results control for reaching the mutation.
What should be immediately obvious is that the repres-
entation invariant (repOk) for binary trees seem not to be
very good at nding errors, since it has more than double the
amount of equivalent mutants than when you compare the
linearisation of the trees (produced by the mutated versus
correct code). Conversely since the linearisation is so much
more precise it has almost double the amount of easily killed
mutants compared to repOk.
The relational operators that negate conditionals again
perform badly when it comes to producing easily killed mutants.
For the precise oracle it has 78% easily killed mutants, of
which the majority is in fact always killed. Even for the im-
precise oracle things are not much better 52% easily killed
and again the majority is always killed.
5. DISCUSSION
We know that reaching a mutation is required for killing
it, but here we show that justreaching it can be enough for
many relational mutation operators to be killed.
The one obvious criticism of this work is that the programs
we looked at here are rather small: triangle (largest one
around 40 LOC), TCAS around 170 LOC and BST around
150 LOC. We know that larger programs have more op-
portunity for mutations and larger programs will also have
more paths (branches) in all likelihood. However, what we
are really analysing here is in some sense the width of the
paths (i.e. the number values that ow down that path)
not the number of them. If you have more branches/paths
then by denition they will on average become less wide
(every branching point will reduce the width of the paths
43from that point on). Following this line of reasoning to its
conclusion would mean that the larger the program the more
likely it is, on average, that reaching the point of mutations
will become less likely. This would lead to more stubborn
mutations (since the eect of just reaching the mutation will
dominate), but should have very little eect on really stub-
born mutants, i.e ones that only look at the values that reach
the point of mutation. Of course this would be in the case
where the oracle is precise, however all bets are o if you
have a bad oracle.
Luckily even in the case of an imprecise oracle we have
shown that relational mutation operators that negate a con-
ditional (i.e. completely change the direction of the ow of
values), leads to mutations that are too easy to kill. Hence
they should best be avoided. However, if you have concerns
over equivalent mutants, then this class is great, because be-
ing easy to kill implies less equivalent mutants (Table 4 top
part and Table 5 last line).
On the other end of the scale, we have the o-by-one re-
lational operators that are by denition hard to kill, since
they make small changes to the ow of values. Guess what
though, making small changes would also mean it would be
easier for the oracle to miss those changes and hence we
could see more equivalent mutations (Table 4 bottom part).
6. RELATED WORK
This is a well-studied eld, hence we will try and focus on
what we consider the most closely related work to ours.
We have already discussed the relationship with [16] at
length, but it is by some distance the most closely related.
On a operational level we use the core ideas from the ap-
proach of Probabilistic Symbolic Execution [5] and more
specically [4] to do our analysis.
Discovering equivalent mutants has been a particularly
active area of research (see [16] for an extensive list). Here
we restrict this problem to a domain where we can get pre-
cise results and we can show that Grun et al. [6] has taken
the correct approach by looking at the impact of a mutation
on the execution, with higher impact leading to less equival-
ence: when you have a large impact, as in the case of the
negated conditional, you see less equivalent mutations.
There are not many studies looking at how hard it is to
kill a mutant [11, 16] and they both take the approach of
evaluating against a test suite. We show here that when
doing the analysis exhaustively gives one interesting new
insights that are hidden when the test inputs are limited
and one doesn't consider the strength of the oracle.
7. CONCLUSIONS
A simple takeaway from this paper is that if you want to
test whether your test suite is good at nding subtle errors or
you want to only seed faults that are hard to detect (by some
testing technique) then only use the o-by-one relational
operators (a reduction of 30 to 4). Extending this work to
more operators is clearly possible and left for future work.
A few interesting puzzles are left unanswered. Firstly,
Just et al. [10] in a large study observed a positive correl-
ation between mutation detection and real fault detection
independent of coverage . Our results seem to suggest cov-
erage is very important. Secondly, what about higher-order
mutations (mutating a program already mutated) [8], will
reachability play as much of a role there? Lastly, can weaddress the link between real faults and mutations in our
setting? On this last point we evaluated all the buggy ver-
sions of triangle and no single mutation we tried matched
any of them. Of course higher-order might work, especially
if you allow the delete statement mutation.
8. REFERENCES
[1] SIR web page. http://sir.unl.edu
[2] C. Cadar, P. Godefroid, S. Khurshid, C. S. P as areanu,
K. Sen, N. Tillmann, and W. Visser. Symbolic
execution for software testing in practice: Preliminary
assessment. In Proceedings of ICSE 2011 , pages
1066{1071, New York, NY, USA, 2011. ACM.
[3] C. Cadar and K. Sen. Symbolic execution for software
testing: Three decades later. Commun. ACM ,
56(2):82{90, Feb. 2013.
[4] A. Filieri, C. S. Pasareanu, and G. Yang.
Quantication of software changes through
probabilistic symbolic execution. In Proceedings of
ASE 2015 , pages 703{708, Nov 2015.
[5] J. Geldenhuys, M. B. Dwyer, and W. Visser.
Probabilistic symbolic execution. In Proceedings of
ISSTA 2012 , pages 166{176, New York, NY, USA,
2012. ACM.
[6] B. J. M. Gr un, D. Schuler, and A. Zeller. The impact
of equivalent mutants. In Software Testing,
Verication and Validation Workshops, 2009. ICSTW
'09, pages 192{199, April 2009.
[7] Inria. Barvinok. http://barvinok.gforge.inria.fr/.
[8] Y. Jia and M. Harman. Constructing subtle faults
using higher order mutation testing. In Proceedings of
SCAM 2008 , pages 249{258, Sept 2008.
[9] Y. Jia and M. Harman. An analysis and survey of the
development of mutation testing. IEEE Transactions
on Software Engineering , 37(5):649{678, Sept 2011.
[10] R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst,
R. Holmes, and G. Fraser. Are mutants a valid
substitute for real faults in software testing? In
Proceedings of FSE 2014 , pages 654{665, New York,
NY, USA, 2014. ACM.
[11] T. Laurent, A. Ventresque, M. Papadakis, C. Henard,
and Y. Le Traon. Assessing and Improving the
Mutation Testing Practice of PIT. ArXiv e-prints ,
Jan. 2016.
[12] C. S. Pasareanu, W. Visser, D. H. Bushnell,
J. Geldenhuys, P. C. Mehlitz, and N. Rungta.
Symbolic pathnder: integrating symbolic execution
with model checking for java bytecode analysis.
Autom. Softw. Eng. , 20(3):391{425, 2013.
[13] UC Davis, Mathematics. LattE.
http://www.math.ucdavis.edu/~latte.
[14] W. Visser, J. Geldenhuys, and M. B. Dwyer. Green:
Reducing, reusing and recycling constraints in
program analysis. In Proceedings of FSE 2012 , pages
58:1{58:11, New York, NY, USA, 2012. ACM.
[15] J. M. Voas and G. McGraw. Software Fault Injection:
Inoculating Programs Against Errors . John Wiley &
Sons, Inc., New York, NY, USA, 1997.
[16] X. Yao, M. Harman, and Y. Jia. A study of equivalent
and stubborn mutation operators using human
analysis of equivalence. In Proceedings of ICSE 2014 ,
pages 919{930, New York, NY, USA, 2014. ACM.
44