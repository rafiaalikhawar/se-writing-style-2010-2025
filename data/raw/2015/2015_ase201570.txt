Dynamically Testing GUIs Using
Ant Colony Optimization
Santo Carino∗and James H. Andrews∗†
∗Department of Computer Science†Google Inc.
University of Western Ontario 1600 Amphitheatre Parkway
London, Ontario, Canada Mountain View, CA, USA
Email: scarino,andrews@csd.uwo.ca
Abstract —In this paper we introduce a dynamic GUI test
generator that incorporates ant colony optimization. We created
two ant systems for generating tests. Our ﬁrst ant system im-plements the normal ant colony optimization algorithm in orderto traverse the GUI and ﬁnd good event sequences. Our secondant system, called AntQ, implements the antq algorithm thatincorporates Q-Learning, which is a behavioral reinforcementlearning technique. Both systems use the same ﬁtness functionin order to determine good paths through the GUI. Our ﬁtnessfunction looks at the amount of change in the GUI state thateach event causes. Events that have a larger impact on the GUIstate will be favored in future tests. We compared our two antsystems to random selection. We ran experiments on six subjectapplications and report on the code coverage and fault ﬁndingabilities of all three algorithms.
I. I NTRODUCTION
Software testing is a large and expensive part of the
development life-cycle [28]. A lot of research has gone into
traditional black- and white-box testing. Such methods includeusing dynamic symbolic execution [32] [11] [37], using mu-tants [21] [15] [2], random testing [4] [3] [16], and search-based software testing [14] [24]. With graphical user interfaces(GUI) being commonplace in most modern software, rangingfrom the web, to mobile, and desktop, the need for GUI testshas arisen, which further increases the cost. As with automatedblack-box test generation, there is a need for automated testgenerators for GUIs. While traditional test generators are ableto generate tests similar to tests a programmer would write,that is a series of methods calls and variable declarations,GUI tests typically consist of a series of events to be run onthe system, for example, clicking buttons or entering data intotext boxes. Currently, GUI tests can be written manually orcreated using a record and playback system [31] [1]. Manuallywritten tests consist of methods to locate a speciﬁc GUI widgetand executing its event. Record and playback systems recordthe user’s interaction with the application and can then playback the test script at a later time. Both methods are time-consuming.
In this paper we introduce a test sequence generator for
GUIs. The system uses ant colony optimization [13] in orderto generate tests that have an impact on the state of the GUI.Ant systems have been shown to be useful for ﬁnding pathsthrough a graph. Since GUIs are best represented by graphs,where each vertex is an event and each edge an action, itfollows that ant colony optimization may be a good methodto explore the GUI graph. Furthermore, we can represent theGUI test generation problem as an optimization problem, suchas trying to generate tests that maximize code coverage. Oursystem’s goal is to ﬁnd sequences of events that have a largeimpact on the GUI state.
There are generally two types of architectures for testing
GUI systems: model-based systems and dynamic systems [5].Model-based systems will ﬁrst try to discover the structureof the GUI hierarchy and generate a model. The models aregenerally represented as event-ﬂow graphs or state machines.Test cases are then generated ofﬂine based on the model’sstructure. Model-based systems can generate infeasible testcases due to the fact that some events can only be selectedif the system is in a speciﬁc state. A dynamic testing systemwill explore the GUI as it generates tests and create a model asevents are discovered. Since the system can determine whichevents are available at each state, the tests are always feasible.Due to the beneﬁts of the dynamic architecture, we opted to useit, and therefore all tests generated in our system are feasible.
The contributions of this paper are as follows.
• We adapt the concept of using the GUI state change
ratio to ant colony systems and show how it can beused as an effective ﬁtness function.
• We apply two ant colony systems to GUI testing:the Ant System and AntQ, and show how they cangenerate effective test cases.
• We evaluate both ant systems and compare themagainst random search.
The paper is organized as follows. Section II discusses
related work and other GUI testing systems. Section III showshow we calculate our state change values, which we use asa ﬁtness function. Section IV looks at the two ant systemswe implemented: a traditional Ant System and AntQ, whichcombines ant colony optimization and Q-Learning. Section Vis where we evaluate our system by testing the two algorithmson six subject applications and comparing their results againstone another, as well we compare against random selection.Finally, we conclude in Section VI.
II. B
ACKGROUND AND RELATED WORK
Here we describe ant colony optimization in general,
as well as other systems designed for automated GUI test
2015 30th IEEE/ACM International Conference on Automated Software Engineering
978-1-5090-0025-8/15 $31.00 © 2015 IEEE
DOI 10.1109/ASE.2015.70138
sequence generation.
A. Ant Colony Optimization
An ant colony optmization algorithm [13] is a swarm
intelligence system typically used to solve problems that can
be represented as a graph, such as the travelling salesmanproblem. The ant system’s main components are positivefeedback and distributed computation. Furthermore, the antsystem makes use of a greedy heuristic when traversing agraph. The system uses a set of agents, called ants, to explorethe graph in order to ﬁnd an optimal path. Once a generationof ants has completed its search, the edges of the graph areupdated. The edges are updated by having a value, calledpheromones, deposited on them. The amount of pheromonedeposited on an edge depends on the ﬁtness of the ants thattraversed it. The ﬁtness function used for many problems isthe inverse of the tour length for each ant. The more antsthat traverse an edge, the more pheromone it will receive.Furthermore, each edge’s pheromones reduce over time dueto evaporation. Another component of the ant system is thegreedy heuristic. The heuristic is used during the selectionprocess in order to determine the next vertex to visit. Theheuristic value is combined with the pheromone value, whichdetermines the vertex’s desirability. The heuristic is generallythe inverse of the edge’s distance, so that shorter edges willbe favored.
Ant systems have been combined with Q-Learning [12]
to further improve their results. Q-Learning [34] is a behav-ioral reinforcement algorithm that teaches agents how to actin controlled Markovian domains. It works by continuouslyimproving evaluations of speciﬁc actions in speciﬁc states. Ourant system incorporates Q-Learning into each ant’s exploration.In the traditional ant system, the pheromone values are onlyupdated after each ant has successfully completed its run. Inantq, the edges are updated at two points. First, when an anttraverses an edge it immediately updates the edge’s pheromoneusing the Q-Learning technique. Second, once each ant hascompleted its run, the edges are updated again as in the classicant system. Another difference between the two systems is thatin the ant system, all edges are updated after each generation,whereas in antq only edges that were traversed by an ant areupdated.
B. GUI Testing Methods
A popular and well researched model-based testing system
is GUITAR [29]. GUITAR works by ripping [26] the GUI
into a GUI hierarchy ﬁle, which can then be translated toan event-ﬂow graph (EFG)[25]. EFGs represent all possibleevent interactions that can occur. The graph can be traversedand tests generated that consist of a list of events to execute.Various techniques can be used to generate tests, such as usingcovering arrays [38] or using automated planning to coverdeﬁned goals [27]. The major ﬂaw with GUITAR is that it cangenerate infeasible tests. These broken tests can be repaired[20]; however, the process can be expensive.
Bauersfeld et al. [6] developed a system that incorporates
ant colony optimization in order to test GUIs. The goal oftheir system is to generate test sequences that result in alarge maximum call tree (MCT). MCTs are trees that representmethod calls in a system. A large MCT is a tree with many leafnodes. The idea behind generating large MCTs is that methodscan be called in different contexts, such as from other methods,and by generating large MCTs the context in which a method iscalled is likely to change. For example, a method m1() can be
called by both methods m2() andm3(). It would be beneﬁcial
to havem1() called in both contexts to ensure it is properly
tested as either one of the calling methods can change variablesor objects that m1() relies on. The authors were successful in
generating test sequences that generated large MCTs.
Gross et al. [17] developed EXSYST, which is a dynamic
test sequence generator that uses a genetic algorithm to evolvean entire test suite. The authors begin by showing that using aunit test generator such as RANDOOP [30] can result in manyfalse failures, as the implicit contraints of the AUT imposed bythe GUI are ignored. EXSYST works by generating randomtest suites and determining their ﬁtness based on branchdistance. However, the system takes into account multipletarget branches instead of just a single branch. The systemperforms genetic manipulation in the form of crossover andmutation. Crossover happens at the test suite level by havingtest suites exchange a number of test cases. Furthermore,both test suites and test cases can be mutated. Test suitesare mutated by adding test cases or modiﬁng an existing testcase. Test cases are mutated by deleting, changing, or insertingoperations. The mutation phase can cause tests to break. Inorder to ﬁx infeasible test cases, a model of the GUI is stored.The model contains sequences of actions and states that thoseactions lead to. The model can be used to repair broken testcases.
Mariani et al. [22] developed AutoBlackTest, which is
built on top of IBM Rational Tester, to automate GUI testsequence generation. AutoBlackTest uses Q-Learning to ﬁndgood sequences of events. The system works by exploring theGUI and assigning values to edges based on a reward and a Qfunction. The reward is the amount of change that occurs in theGUI when an event is executed. The Q-value is assigned basedon the event’s reward and the Q-value of its best successorevent. The system is also able to start testing from any foundstate. As a result, AutoBlackTest stores the states found andthe steps required to enter that state. When a new test isbeing generated, it will attempt to start the test at a previouslyfound state; however, if this is not possible, it starts testingfrom its current state. The authors incorporate heuristic actionsfor different event types, where the action can encompassmany different events (for example, for handling ﬁle open andsave dialogs). The authors compared AutoBlackTest againstGUITAR and show that it is able to achieve higher levels ofcode coverage and ﬁnd more faults.
III. GUI S
TATE CHANGE AS A FITNESS
The goal of ant colony optimization algorithms is to try
to ﬁnd an optimal path through a graph, for example theshortest path to solve the travelling salesman problem. In GUItesting systems, the value to optimize can differ. In the caseof [6], the authors try to maximize Maximum Call Trees. Inother meta-heuristic search systems, such as EXSYST [17], theauthors try to maximize code coverage. Our system attemptsto maximize the amount of change in the GUI state for eachtest case. That is, it will prefer interacting with widgets that
139have a large impact on the GUI, such as events that enable new
widgets or open windows. This concept is borrowed directlyfrom AutoBlackTest [22].
We can measure the amount of change on a system’s GUI
by inspecting the GUI’s state before an event is run, runningthe event, and inspecting the resulting GUI state. The statechange value is then calculated as the difference betweenthe two states. The system takes into account two factorswhen calculating the state change value: ﬁrst, it compares theproperties of the widgets that exist in both states; second, itlooks for any new widgets that exist in the new state.
A GUI’s state can be broken down into its individual
widgets, such as buttons, sliders, and panels. Each widgetcontains a set of properties that describe it. For example, abutton’s properties will contain its text, whether or not it isenabled, its position, and so on. Furthermore, some properties,which we call traits, can be used to identify a widget. This is
needed because when we compare states, we want to ensurewe are comparing the same widgets. Formally we say a state S
contains a set of widgets <w
1,w2,...,wn>, where each widget
wcontains a set of properties <p1,p2,...,pn>, we say that a
propertypis a trait if it can be used to identify the widget.
When comparing two widgets, w1andw2,w es a yw 1=t
w2iff|w1|=|w2|∧ ∀ti∈w1,∃tj∈w2s.t.ti=tj, wheret
is a trait in w. That is, two widgets are equivalent if all of their
traits are equal. If one widget has a different number of traitsthan the other, the two widgets are considered to be different.
When comparing two states, we only look at a subset of the
widgets and their properties, which we call an Abstract State.
An abstract state, AS, contains a set of of Abstract Widgets,
where each abstract widget contains a subset of a real widget’sproperties. We only use a subset of a widget’s propertiesfor state comparison, as not all properties are relevant. Forexample, an AbstractTextBox will contain the text box’s
text, GUI hierarchy position, class, and whether or not it isenabled and editable.
If we have two abstract states, AS={w
1,...wn}
andAS/prime={w/prime
1,...w/prime
n}, we deﬁne the restriction operator
AS\tAS/prime={wi|wi∈AS∧∄wk∈AS/primes.t.wi=twk}.
This restriction operator is used in our calculation of statechange ratios.
Equation (1) is used to calculate the ratio of change
between two widgets if w
1=tw2. The equation determines
the ratio of change based on each widget’s properties.
diffw(w1,w2)=|P1\P2|+|P2\P1|
|P1|+|P2|(1)
wherePiis a property in wi.
We use (2) to compare two states and determine the ratio
of change. The equation takes into account the ratio of changebetween widgets that appear in both states, as well as newwidgets that appear in the new state. It does not includewidgets that may have been removed from the ﬁrst state, suchas when a window is closed. This is done to favor widgets thatallow access to more events.diff
AS(AS1,AS 2)=
|AS2\tAS1|+/summationtext
w1∈AS 1,w2∈AS 2,w1=tw2diffw(w1,w2)
|AS2|
(2)
The ﬁnal change ratio is used as the ﬁtness to our ant
system. The beneﬁt of using the state change ratio as a ﬁtnessis that it only relies on the information within the GUI and itdoes not require any code instrumentation.
IV . A
NTSYSTEMS
A. Architecture
The architecture of our system is shown in Fig. 1. Here we
describe each component in detail.
Application Under Test
The application under test (AUT) is the program being
tested. In our case it is a Java Swing application, but it can be
generalized to any GUI system.
Window Listener
The Window Listener implements the WindowListener in-
terface provided with the JDK. It is able to listen for window
events such as when a window opens, closes, or is activated.Every time a window update event happens, the windowlistener captures the affected window and scrapes it. Thewindow listener determines if the window is modal or notby checking the window’s properties. A modal window is awindow that has control over the application. No events outsideof that window are able to be executed until the windowhas closed. An example modal window is the the save dialogwindow used in many applications. If a window is modal, onlyits events will be scraped, otherwise all windows are scraped.
GUI Scraper
The GUI Scraper accepts inputs from the Window Listener.
The Window Listener passes the scraper the set of windows to
be scraped. For each window, the scraper extracts the window’swidgets and determines if the widget is executable, such as abutton, or if it is a container, such as a panel. Widgets thatare executable are stored in a widget list, whereas containersare scraped further. A window can contain many layers ofcontainers. The scraper recursively scrapes each container untilit reaches the bottom of the GUI hierarchy.
The scraper can work in two modes: full state extraction or
executable widgets only. The ‘full state’ mode will extract allavailable widgets and containers and return them. This data isused to calculate the state change ratio for the ant algorithms.The second mode, ‘executable widgets only’, extracts andreturns widgets that can be executed and are valid. By validwe mean enabled and visible.
140Application
Under TestWindowListenerGUI Scraper
Ant System Ant
Ants RunTrail UpdaterGUI State
Fitness
CalculatorException
Reporter
Event
SelectorEvent
ExecutorExceptions
AntQ
Q-Learning
Reinforcement<<Attach>>
Fig. 1: Ant System Architecture.
Ant System
The Ant System is the controller for the set of ants that
will test the AUT. The Ant System deploys each ant and
sets the ants tour length based on the conﬁguration set bythe test engineer. When an ant has completed its tour, theedges traversed by the ant and the cumulative state changeratio are stored in an ant database called ‘Ants Run’. After thegeneration of ants has completed its run, the edges of the graphare updated using the Trail Updater and the ant database.
Trail Updater
The Trail Updater is the component responsible for updat-
ing the edges traversed by the set of ants in the ant database.
The updater only uses the ants from the current generation toupdate the trails, and not the ants from any previous generation.The Trail Updater looks at the edges traversed by each ant anddeposits pheromones. If an edge was not traversed by an ant,its pheromone is reduced. The amount of pheromone depositeddepends on the algorithm being used (Ant System or AntQ),and the pheromone evaporation rate.
Ant
The Ant represents the actual test case. After being
spawned by the Ant System, the Ant receives input from the
scraper about the available executable events. Before executingan event, the Ant generates an abstract state by retrieving theAUT’s state data from the scraper module and passing it to theGUI State Fitness Calculator. The Ant then passes the availableevents to the Event Executor. After execution is complete, theAnt captures the resulting state and retrieves the abstract statefrom the ﬁtness calculator. The two abstract states are passedto the calculator module in order to determine the ratio ofchange. The resulting difference is added to the cumulativestate change of the test case, which is to be used by the trailupdater. The ant continues this process until it reaches its tourlength.GUI State Fitness Calculator
The GUI State Fitness Calculator takes as input the raw
widget data provided by the GUI Scraper module. The calcula-tor extracts the necessary properties and creates an abtract staterepresentation of the state and returns it to the Ant module.The calculator can also compare two abstract states and returnthe difference. The return value is in the range [0,1].
Event Selector
The Event Selector takes as input the list of available events
that can be executed. It applies the pseudo random proportional
rule to the list of events and selects an event to execute. Theselected event is sent to the Event Executor.
Event Executor
The Event Executor takes as input an event to execute.
The event is compared against a list of possible event types,
such as buttons, sliders, and text boxes. Depending on thetype of the event, a different action is taken. Some events canhave multiple actions, for example, a table can have a singlecell ﬁlled, an entire row or column ﬁlled, or the entire tablecan be ﬁlled with data. For complex events we use a randomselection in order to determine the type of action to take. Inthe case of the table, each action would have a 25% chance ofbeing selected. The data used to ﬁll in text boxes and tablesis selected at random from a predeﬁned list of inputs. Table Ishows the list of events and their respective actions.
The Event Executor has heuristics in place to deal with
two speciﬁc scenarios: ﬁle dialogs and color choosers. Whenthe system detects either type of window, it executes a set ofdeﬁned actions. For ﬁle dialogs, the executor sets the directoryto ‘home’, types in a random ﬁle name, selects a random ﬁletype, and clicks the conﬁrm (save/open) button. For the colorchooser window, the executor simply clicks the ‘ok’ button.The ﬁrst heuristic is in place so that the test does not pollute
141TABLE I
Event Actions
Event Action
Button, Spinner, Toggle Button Click()
Text Area, Password Field Type(input)
Table Type(cell/row/column/table)
List Select(row/rows)
Tab Panel SelectTab(tabIndex)
Slider SetSelection(value)
Combo Box SetSelection(index)
the host environment. Without the heuristic it is possible that
important ﬁles or directories will be overwritten or renamed,which would interfere with the testing. The color chooserheuristic is used to avoid waisting time in the dialog window,since the color chooser has little affect on the test, if any.
Q-Learning Reinforcement
The Q-Learning Reinforcement module is called only by
the AntQ algorithm. During the run of the AntQ algorithm,
when an event is executed, the edge traversed is immediatelyupdated. The module updates the speciﬁc edge and returns tothe Ant module.
Exception Reporter
The Exception Reporter implements the UncaughtExcep-
tionHandler interface provided with the JDK. Any exception
thrown during the execution of an event is caught. The reporter
logs the exception message, stack trace, and the list of eventsthat lead up to the event being thrown. The data is then storedin a database.
B. Event Selection
Events are selected by using the pseudo random pro-
portional rule shown in (3). We use the pseudo random
proportional rule as it has been shown to be more effective
than using the random proportional rule alone [12].
y=/braceleftBigg
max
y∈allowed x{τα
xy·ηβ
xy}ifq≤q0
Y otherwise(3)
whereallowed xis the set of events that can follow event x,τ
is the pheromone value, ηis the heuristic value, qis a random
value where 0≤q≤1, andq0is a value determined by the
engineer beforehand. This rule states that we select the bestedge with probability q, or we select an edge using the random
proportional rule using probability 1−q. The equation for the
random proportional rule is (4). The rule states that an eventis selected randomly based on its proportion determined by itspheromone and heuristic values.
P
xy=⎧
⎨
⎩τα
xy·ηβ
xy/summationtext
u∈allowed x(τα
xu·ηβ
xu)ify∈allowed x
0 otherwise(4)
Our system does not implement a hueristic, so only the
edge’s pheromone values are taken into account when selectingevents. We used a default pheromone value of 0.5 for every
edge in the graph. Over time the value will increase or decreasedepending on the ﬁtness function.
C. Updating Pheromones
The main difference between the Ant System and the AntQ
system is in how they update their pheromone values. The Ant
System updates its pheromones using (5).
AQ(x,y)=( 1−α)·AQ(x,y)+ΔAQ(x,y) (5)
whereαis the pheromone evaporation rate.
When all ants of the current generation have ﬁnished their
run, the system updates allof the edges in the graph found so
far. The edges are updated using (6), which states that edgesappearing in least one run of the kbest ants, where k≤n
and where nis the number of ants in a generation, receives
pheromones. The amount of pheromones received is equal tothe average ﬁtness of the ant runs that the edge appears in.For example, if an ant appears in three of the k
best ant runs,
it will receive pheromone equal to the average ﬁtness of thosethree runs. Edges not appearing in any of the k
best runs have
their pheromones reduced based on the evaporation rate α.
ΔAQ(x,y)=⎧
⎨
⎩k/summationtext
i=1SC(i)
kif (x,y) cov. by any ant∈kbest
0 otherwise(6)
wherekbest is the set of the best kants in the current
generation.
The AntQ system updates its pheromones using (7). As the
ant traverses the graph, it updates each edge it touches. This is
the reinforcement learning borrowed from Q-Learning. Duringthis stage the ΔAQ(x,y)is 0 for all edges. Once all ants in
the generation have completed their run, the system updatesonly those edges that were traversed by an ant, all other edgeskeep their current pheromone value. The traversed edges areupdated using (7), where γ· Max
u∈allowed yAQ(y,u)is 0 for all
edges.
AQ(x,y)=( 1−α)·AQ(x,y)+
α·/parenleftbigg
ΔAQ(x,y)+γ·Max
u∈allowed yAQ(y,u)/parenrightbigg
(7)
whereγis the cooling factor, which is a part of the Q-Learning
model.
For both the Ant System and AntQ system, the amount of
pheromone deposited is calculated using (8).
SC(anti)=StateChange( anti)
TourLength (anti)·UniqueEvents( anti)
(8)
142function ANTCOLONY (Generations, Ants, Length)
fori∈Generations do
stateDiffs ←{ }
forj∈Ants do
cuDiff←0
StartAUT ()
fork∈Length do
event←selectEvent()
curState ←getState()
execute(event)
newState ←getState()
diff←getDiff(curState,newState )
cuDiff←cuDiff+diff
curState ←newState
end for
ShutdownAUT ()
stateDiffs.add( cuDiff)
end forUpdateAllTrails( stateDiffs)
//Eq. 5. Ant Collaboration
end for
end function
Fig. 2: Ant System Algorithm
whereant
iis the index of the i-th best ant run in the current
generation, StateChange() is the cumulative state change of
the entire ant’s run, TourLength() is the length of the ant’s
run, and UniqueEvents() is the number of unique events
executed during the ant’s run, where 0≤UniqueEvents() ≤
TourLength(). When calculating the state change using theStateChange() method, the system only counts the amount
of change once for each event in an ant’s run, regardless if theevent is executed multiple times. This is to discourage testsrepeating the same, high state changing events. Furthermore,we further favor unique events by dividing by the length ofthe tour and multiplying by the number of unique events. Thiswill favor tests with more unique events.
Algorithms 2 and 3 highlight the differences between the
two ant systems. During the AntQ algorithm, the Q-Learningreinforcement is updated using (7) after each edge is traversed,where the ΔAQ(x,y)value is 0. At the end of each generation,
the same equation is used to update the ant trails, however, theγ·Max
u∈allowed yAQ(y,u)values are 0 for all edges.
V. E V ALUATION
A. Applications Under Test
We evaluated both ant systems, as well as random selection,
on six subject applications. The details of the applications canbe seen in Table II. The line and branch information was foundusing Cobertura [10], which we used to measure the codecoverage. Cobertura counts the executable lines of code andignores the rest. The six applications chosen have all appearedin the literature previously. They are of moderate size andrepresent real-world applications.
B. Parameter Tuning
In order to determine which values to set for the available
tuning parameters, we ran a small set of experiments onfunction A
NTQ(Generations, Ants, Length)
fori∈Generations do
stateDiffs ←{ }
forj∈Ants do
cuDiff←0
StartAUT ()
fork∈Length do
event←selectEvent()
curState ←getState()
execute(event)
newState ←getState()
diff←getDiff(curState,newState )
cuDiff←cuDiff+diff
curState ←newState
updateTrail (k−1,k)
//Eq. 7. Q-Learning Reinforcement
//ΔAQ(x,y)is 0
end forShutdownAUT ()
stateDiffs.add( cuDiff)
end forUpdateTouchedTrails(stateDiffs)//Eq. 7. Ant Collaboration//γ·Max
u∈allowed yAQ(y,u)is 0
end for
end function
Fig. 3: AntQ Algorithm
TABLE II
Application Summary
Application Classes Statements Branches
ArgoUML 1233 54632 24074
Buddi 1529 101949 43670
Gantt Project 689 27804 8926
TerpSpreadsheet 137 5449 2135
TerpWord 208 10340 3625
TimeSlotTracker 487 10090 3115
Total 4283 210264 85545
TerpSpreadsheet, TerpWord, and ArgoUML. We ran test suitesof size 100 that consisted of tests of length 20. We used 10ants per generation and we used the best 7 ants to update theedges. We repeated the process ﬁve times and took the averagenumber of statements covered. The tuning parameters for theAnt System are the pheromone evaporation rate (α ) and the
pseudo random proportional selection value (q
0). Both values
are in the range [0,1]. For the AntQ algorithm, an additionalparameter is required, which is the cooling factor (γ ). For
all three parameters, we chose the values 0.3, 0.6, and 0.9.Since we do not know the effects of these parameters in thiscontext, we chose a spread of values ranging from low tohigh. For the Ant System, we combined each αvalue against
eachq
0value, which resulted in nine combinations. For the
AntQ system we used a 2-way combinatorial system to ﬁnd allpairs between the three parameters. This, too, resulted in ninecombinations. We chose to use a 2-way combinatorial selectionfor the factors as the number of experiments is too large tofully test in a reasonable amount of time. It is possible thatvalues or combinations of values not chosen would perform
143TABLE III
Ant System Parameter Tuning
Statements
αq0 TerpS TerpW Argo
0.3 0.6 3627 5750 19578
0.6 0.9 3592 5823 18982
0.9 0.3 3563 5715 19807
0.3 0.3 3711 6286 20011
0.6 0.3 3666 6107 20264
0.9 0.6 3508 5564 19096
0.3 0.9 3526 5893 19075
0.6 0.6 3598 5767 20041
0.9 0.9 3444 5511 18338
TABLE IV
AntQ Parameter Tuning
Statements
αγq0 TerpS TerpW Argo
0.3 0.9 0.6 3791 6423 20258
0.6 0.6 0.9 3792 6304 19293
0.9 0.3 0.3 3801 6230 19629
0.3 0.3 0.9 3801 6348 19395
0.6 0.3 0.6 3814 6416 19450
0.9 0.6 0.6 3765 6248 19466
0.6 0.9 0.3 3774 6412 19916
0.3 0.6 0.3 3788 6458 20170
0.9 0.9 0.9 3833 6471 20031
better; however, we are only looking for guidance and not for
the optimal values for each application.
The results for the Ant System are shown in Table III.
The parameter values that resulted in the most coverage are α
= 0.3 and q0= 0.3 for both TerpSpreadsheet and TerpWord,
andα= 0.6 and q0= 0.3 for ArgoUML. We performed a
factorial ANOV A and Tukey test to determine if the factorshad a signiﬁcant impact on the results, as well as to determineif any value of each factor was signiﬁcant. The results of theANOV A showed that in all three applications, both the αand
q
0factors had a signiﬁcant impact on the statements covered,
but that their interactions did not. Parameter values for αof
0.3 and 0.6 and a q0of 0.3 were found to have a signiﬁcant
impact for TerpSpreadsheet. For TerpWord, an αvalue of 0.3
and aq0value of 0.3 were signiﬁcant. Finally for ArgoUML,
αvalues of 0.3 and 0.6 and a q0value of 0.3 had signiﬁcant
impacts on the coverage.
The results for the AntQ algorithm are shown in Table
IV. The parameter values that resulted in the most statementscovered were α= 0.9,γ= 0.9, and q
0= 0.9 for both
TerpSpreadsheet and TerpWord, and α= 0.3,γ= 0.6, and
q0= 0.3 for ArgoUML.
Again we performed a factorial ANOV A and Tukey test.
The ANOV A showed that no single factor had a signiﬁcantimpact on the results for all three applications. The Tukeytest results for both TerpSpreadsheet and TerpWord showedthat no value for each parameter resulted in signiﬁcantly morecoverage. The Tukey test results for ArgoUML showed that anαvalue of 0.3 and a γvalue of 0.9 were signiﬁcantly better.TABLE V
Test Suite
Algorithm αγq0 Ants k-best Length Tests
Random - - - - - 30 300
Ant System 0.3 - 0.3 20 15 30 300
AntQ 0.3 0.9 0.3 20 15 30 300
TABLE VI
Statement Coverage (LOC)
Application Random Ant System AntQ
ArgoUML 20950 21209 21610
Buddi 15008 15185 15565
Gantt Project 17992 17330 18028
TerpSpreadsheet 4010 3900 3957
TerpWord 7076 6966 7264
TimeSlotTracker 6837 6638 6910
Total 71873 71228 73334
C. Random Testing
The random search algorithm in which the two ant al-
gorithms are compared is built on top of the same dynamicsystem. The random module is given the available, validcomponents to execute, selects one at random, and passes itto the event executor. We chose to compare against randomselection as the evaluation criteria for meta-heuristic search[19] state that any proposed algorithm should perform betterthan random search.
D. Test Suites
Table V shows the data regarding the test suites and
parameters chosen. Each algorithm was run using tests of
length 30 and each suite contained 300 test cases. For thetwo ant algorithms, we used 20 ants per generation and usedthe best 15 ants when updating the trail values. We repeatedthe process six times for each AUT to account for the randomnature of each algorithm. We did not limit the running time,but rather allowed each algorithm to run the same number ofevents. This was done to ensure fairness.
E. Coverage Metrics
The statement coverage results are shown in Table VI.
In ﬁve out of six applications, AntQ performed better than
random selection. Random selection performed better thanboth ant algorithms for TerpSpreadsheet. We performed aTukey test comparing each algorithm and found that AntQwas signiﬁcantly better than random selection and the AntSystem for both ArgoUML and Buddi. Random selection wasnot signiﬁcantly better than AntQ for any AUT; however, itdid perform signiﬁcantly better than the Ant System for bothTerpSpreadsheet and TimeSlotTracker.
In ﬁve out of the six applications under test, AntQ coversmore statements than random selection.
Figs. 4 to 9 show the increase in coverage over time for
all three algorithms. In the case of ArgoUML, Buddi, Gantt
144TABLE VII
Statement Coverage Comparison (%)
AUT Rand Ant AntQ GUITAR EXSYST ABTest
Argo. 38 39 40 25 - -
TerpS. 74 72 73 28 39 -
TerpW. 68 67 70 27 54 -
TimeS. 68 66 68 55 - 68
TABLE VIII
Exceptions Found - Average(Total)
Application Random Ant System AntQ
ArgoUML 8.8(19) 9.8(18) 10.7(17)
Buddi 0 0 0
Gantt Project 0 0 0
TerpSpreadsheet 6(10) 5.8(8) 6(11)
TerpWord 9.7(13) 11.5(15) 10.7(15)
TimeSlotTracker 4.2(7) 3.5(8) 5.3(9)
Average(Total) 4.8(49) 5.1(49) 5.4(52)
Project, TerpWord, and TimeSlotTracker, the AntQ system
is consistently better than random selection. In the case ofTerpSpreadsheet, the AntQ and random algorithms grow at asimilar pace.
In ﬁve out of the six applications under test, AntQ’scoverage growth dominates random selection’s coveragegrowth.
We compare our statement coverage results with those
published in [18] [22] [29]. The results can be seen in TableVII. In the case of GUITAR and EXSYST, our three algorithmsoutperform them in all cases. In the case of AutoBlackTest,our system ﬁnds the same level of coverage.
F . Unhandled Exceptions
Table VIII shows the number of uncaught exceptions
found. Neither Buddi nor Gantt Project threw any uncaught
exceptions.
The ant algorithms found on average an equal or greaternumber of exceptions compared to random selection.
AntQ found a greater number of uncaught exceptions thanboth the Ant System and random selection.
Again we performed a Tukey test and found that AntQ
found a signiﬁcantly higher number of exceptions than theAnt System for TimeSlotTracker. No other pairs of algorithmsfor any AUT were found to have signiﬁcant differences.
G. Cost
Table IX shows the average running time for each test
suite on each application. The random selection algorithm
outperforms both the Ant System and AntQ algorithms in allTABLE IX
Average Time to Run (Hours)
Application Random Ant System AntQ
ArgoUML 5.5 5.6 5.6
Buddi 4 5.4 5.2
GanttProject 4 4.5 4.7
TerpSpreadsheet 3.5 3.9 3.9
TerpWord 3.4 4.1 3.7
TimeSlotTracker 3.9 4.6 4.6
Total 24.3 28.1 27.7
TABLE X
Fitness Function Evaluation
Application Pearson Value
ArgoUML 0.414
Buddi -0.026
Gantt Project 0.339
TerpSpreadsheet -0.213
TerpWord 0.228
TimeSlotTracker 0.497
cases. The results are not surprising as both ant algorithmsmust maintain a trail graph. Furthermore, the ant algorithmshave more complex selection procedures.
H. Evaluating the Fitness Function
In order to determine if our ﬁtness function is effective,
we measured the correlation of the cumulative state change
ratio with the statement coverage for every test case for eachapplication using AntQ. We compared the values using thePearson test with α=0.05. Table X shows the results.
In the cases of ArgoUML, Gantt Project, TerpWord and
TimeSlotTracker, there is a weak to strong positive correlationbetween the state change ratio and the statement coverage.In the case of Buddi and TerpSpreadsheet, there is a weaknegative correlation between the two values. Figs. 10 to 15show the scatter plots comparing the state change ratio valuesto the number of statements covered for the AntQ algorithm.There is evidence that the ﬁtness function provides positiveresults; however, the effectiveness of the function depends onthe application under test.
I. The Effectiveness of Random Testing
This evaluation shows that random selection for dynamic
GUI testing can be effective in ﬁnding faults. Little to no re-
search has been conducted on the viability of random selectionfor GUIs. Many of the systems discussed in the literature donot compare against random selection. AutoBlackTest [22] iscompared against GUITAR; EXSYST [18] is compared againstGUITAR, RANDOOP, and EvoSuite [14]; Bauersfeld et al. [6]compare their ant system to random selection, but they onlyreport on the size of the maximum call tree and not on thecode coverage or faults found; GUITAR [29] does implementa random algorithm, but when compared to dynamic systems,it has been shown to perform worse [5]. Our results show thatrandom selection is a viable option for dynamic GUI testing.
1450 30 70 110 150 200 250 30010000110001200013000140001500016000170001800019000200002100022000ArgoUML Coverage Over Time
Test NumberStatements
Random
AntColonyAntQ
Fig. 4: ArgoUML0 30 70 110 150 200 250 3005000600070008000900010000110001200013000140001500016000Buddi Coverage Over Time
Test NumberStatements
Random
AntColony
AntQ
Fig. 5: Buddi0 30 70 110 150 200 250 30070008000900010000110001200013000140001500016000170001800019000GanttProject Coverage Over Time
Test NumberStatements
Random
AntColonyAntQ
Fig. 6: Gantt Project
0 30 70 110 150 200 250 300200022002400260028003000320034003600380040004200TerpSpreadsheet Coverage Over Time
Test NumberStatements
Random
AntColonyAntQ
Fig. 7: TerpSpreadsheet0 30 70 110 150 200 250 300200025003000350040004500500055006000650070007500TerpWord Coverage Over Time
Test NumberStatements
Random
AntColony
AntQ
Fig. 8: TerpWord0 30 70 110 150 200 250 300350040004500500055006000650070007500TimeSlot Coverage Over Time
Test NumberStatements
Random
AntColony
AntQ
Fig. 9: TimeSlot
0129000100001100012000130001400015000ArgoUML State Ratio vs. Coverage
State Change RatioStatements
Fig. 10: ArgoUML012344000500060007000800090001000011000Buddi State Ratio vs. Coverage
State Change RatioStatements
Fig. 11: Buddi01234600070008000900010000110001200013000GanttProject State Ratio vs. Coverage
State Change RatioStatements
Fig. 12: Gantt Project
0123100020003000TerpSpreadsheet State Ratio vs. Coverage
State Change RatioStatements
Fig. 13: TerpSpreadsheet012310002000300040005000TerpWord State Ratio vs. Coverage
State Change RatioStatements
Fig. 14: TerpWord0123300040005000TimeSlot State Ratio vs. Coverage
State Change RatioStatements
Fig. 15: TimeSlot
146J. Threats to V alidity
Internal validity, which refers to our ability to determine
a causal relationship in our observations, in this case the
algorithm used and the code coverage and exceptions found,is held by the fact that only the algorithms used could haveaffected the coverage metrics and exceptions. Furthermore, theapplications under test have all been used in the literatureand are therefore less prone to selection bias. TerpWord andTerpSpreadsheet were used in [9] [23], Gantt Project in [36][8], ArgoUML in [29] [33], and Buddi and TimeSlotTrackerwere used to evaluate AutoBlackTest [22]. When testing eachof the AUTs, we used the default settings provided; as well,we cleared any changes to the properties after the completionof each test case.
External validity, which refers to our ability to generalize
our results, is also held. GUIs are a general concept that canbe implemented in any programming language. The widgetsused in most GUIs (buttons, text boxes, etc.) can be expectedto work the same as the widgets provided by Java Swing. Fur-thermore, the concept of a GUI heirarchy that can be traveresedcan also be expected to exist in other GUI implementations.There exist GUIs with a rich array of input types, such asfor mobile applications, that deal with touch input. Thesetypes of inputs can usually be programatically executed andso methods such as ours should still be applicable. Therefore,the methods presented in this paper should be generalizable toother systems.
Construct validity, which refers to how we measured the
results, is also upheld. The code coverage was measured usingCobertura [10], an open source application that is well main-tained. We counted uncaught exceptions by implementing ourown exception listener, which extends the exception listenerthat is a part of the JDK.
VI. C
ONCLUSION AND FUTURE WORK
In this paper we introduced a new method of testing GUIs
based on ant colony optimization. We created two ant systems.The ﬁrst system implements the normal ant colony optimiza-tion algorithm in which ants explore a graph and pheromonesare deposited after each generation. Our second algorithmimplements the antq algorithm, which uses Q-Learning toupdate the paths as ants explore the graph. Our ﬁtness functionis based on the amount of change that occurs in the GUI asevents are executed. Both ant systems use the same ﬁtnessfunction.
We compared our two ant systems against random selection
and found that the AntQ system was able to achieve thehighest levels of code coverage. When comparing the numberof uncaught exceptions, we found that the three algorithmsfound a similar number of errors on average, and that the AntQalgorithm found the most in total. Our results merit furtherstudy into the ant system and random selection in general.
In the future we would like to incorporate the ability to
evaluate correct GUI states, such as those presented in [35] asour system currently only looks at uncaught exceptions. Wewould also like to investigate an algorithm for ﬁnding goodinputs to the GUI widgets, such as the method presented in[7], where the authors extract the widget data based on textualdescriptions, either from the widget itself or surroundingwidgets. This information can be used to determine what typeof data to input during a test’s execution, such as a digit, string,or date.
A
CKNOWLEDGMENT
We would like to thank Leonardo Mariani and Oliviero
Riganelli for their input regarding the ﬁtness function calcula-tion.
R
EFERENCES
[1] SeleniumHQ: Web application testing system. http://seleniumhq.org/.
Online. Accessed Feb. 2012
[2] Andrews, J.H., Briand, L.C., Labiche, Y .: Is mutation an appropriate
tool for testing experiments? In: Proceedings of the 27th International
Conference on Software Engineering (ICSE 2005). St. Louis, Missouri(2005). 402-411
[3] Arcuri, A., Iqbal, M.Z., Briand, L.: Formal analysis of the effectiveness
and predictability of random testing. In: ACM International Conferenceon Software Testing and Analysis (ISSTA), pp. 219–230 (2010)
[4] Arcuri, A., Iqbal, M.Z., Briand, L.: Random testing: Theoretical results
and practical implications. Software Engineering, IEEE Transactionson38(2), 258–277 (2012)
[5] Bae, G., Rothermel, G., Bae, D.H.: On the relative strengths of model-
based and dynamic event extraction-based GUI testing techniques: Anempirical study. In: Software Reliability Engineering (ISSRE), 2012IEEE 23rd International Symposium on, pp. 181–190. IEEE (2012)
[6] Bauersfeld, S., Wappler, S., Wegener, J.: A metaheuristic approach to
test sequence generation for applications with a GUI. In: Search BasedSoftware Engineering, pp. 173–187. Springer (2011)
[7] Becce, G., Mariani, L., Riganelli, O., Santoro, M.: Extracting widget
descriptions from GUIs. In: Fundamental Approaches to SoftwareEngineering, pp. 347–361. Springer (2012)
[8] Brooks, P.A., Memon, A.M.: Introducing a test suite similarity metric
for event sequence-based test cases. In: Software Maintenance, 2009.ICSM 2009. IEEE International Conference on, pp. 243–252. IEEE(2009)
[9] Bryce, R.C., Sampath, S., Memon, A.M.: Developing a single model
and test prioritization strategies for event-driven software. SoftwareEngineering, IEEE Transactions on 37(1), 48–64 (2011)
[10] Cobertura Development Team: Cobertura web site (2010).
cobertura.sourceforge.net
[11] Csallner, C., Tillmann, N., Smaragdakis, Y .: DySy: Dynamic symbolic
execution for invariant inference. In: Proceedings of the 30th interna-tional conference on Software engineering, pp. 281–290. ACM (2008)
[12] Dorigo, M., Gambardella, L.: Ant-q: A reinforcement learning approach
to the traveling salesman problem. In: Proceedings of ML-95, TwelfthIntern. Conf. on Machine Learning, pp. 252–260 (2014)
[13] Dorigo, M., Maniezzo, V ., Colorni, A.: Ant system: optimization by a
colony of cooperating agents. Systems, Man, and Cybernetics, Part B:Cybernetics, IEEE Transactions on 26(1), 29–41 (1996)
[14] Fraser, G., Arcuri, A.: Evosuite: automatic test suite generation for
object-oriented software. In: Proceedings of the 19th ACM SIGSOFTsymposium and the 13th European conference on Foundations ofsoftware engineering, pp. 416–419. ACM (2011)
[15] Fraser, G., Zeller, A.: Mutation-driven generation of unit tests and
oracles. In: Intl. Symp. on Software Testing and Analysis (ISSTA),pp. 147–158 (2010)
[16] Godefroid, P., Klarlund, N., Sen, K.: DART: Directed automated ran-
dom testing. In: Proceedings of the ACM SIGPLAN 2005 Conferenceon Programming Language Design and Implementation (PLDI), pp.213–223. Chicago (2005)
[17] Gross, F., Fraser, G., Zeller, A.: EXSYST: search-based GUI testing.
In: Intl. Conf. on Software Eng. (ICSE), pp. 1423–1426 (2012)
[18] Gross, F., Fraser, G., Zeller, A.: Search-based system testing: high
coverage, no false alarms. In: Proceedings of the 2012 InternationalSymposium on Software Testing and Analysis, pp. 67–77. ACM (2012)
147[19] Harman, M., Jones, B.: Search-based software engineering. Journal of
Information and Software Technology 43, 833–839 (2001)
[20] Huang, S., Cohen, M.B., Memon, A.M.: Repairing GUI test suites using
a genetic algorithm. In: Software Testing, Veriﬁcation and Validation
(ICST), 2010 Third International Conference on, pp. 245–254. IEEE(2010)
[21] Jia, Y ., Harman, M.: An analysis and survey of the development of
mutation testing. Software Engineering, IEEE Transactions on 37(5),
649–678 (2011)
[22] Mariani, L., Pezz `e, M., Riganelli, O., Santoro, M.: AutoBlackTest: a
tool for automatic black-box testing. In: Software Engineering (ICSE),2011 33rd International Conference on, pp. 1013–1015. IEEE (2011)
[23] McMaster, S., Memon, A.M.: Call-stack coverage for GUI test suite
reduction. Software Engineering, IEEE Transactions on 34(1), 99–115
(2008)
[24] McMinn, P.: Search-based software test data generation: a survey.
Software testing, Veriﬁcation and reliability 14(2), 105–156 (2004)
[25] Memon, A.M.: An event-ﬂow model of GUI-based applications for
testing. Software Testing, Veriﬁcation and Reliability 17(3), 137–157
(2007)
[26] Memon, A.M., Banerjee, I., Nagarajan, A.: GUI ripping: Reverse
engineering of graphical user interfaces for testing. In: Working Conf.on Reverse Eng. (WCRE), pp. 260–269 (2003)
[27] Memon, A.M., Pollack, M.E., Soffa, M.L.: Hierarchical GUI test case
generation using automated planning. Software Engineering, IEEETransactions on 27(2), 144–155 (2001)
[28] Myers, G.J.: The Art of Software Testing. Wiley, New York (1979)
[29] Nguyen, B., Robbins, B., Banerjee, I., Memon, A.: GUITAR: an inno-vative tool for automated testing of GUI-driven software. Automated
Software Engineering pp. 1–41 (2013)
[30] Pacheco, C., Lahiri, S.K., Ernst, M.D., Ball, T.: Feedback-directed
Random Test Generation. In: In Proceedings of the 29th InternationalConference on Software Engineering (ICSE 2007), pp. 75–84. Min-neapolis, MN (2007)
[31] Ruiz, A., Price, Y .W.: Test-driven GUI development with testng and
abbot. Software, IEEE 24(3), 51–57 (2007)
[32] Tillmann, N., De Halleux, J.: Pex–white box test generation for. net.
In: Tests and Proofs, pp. 134–153. Springer (2008)
[33] Van Rompaey, B., Du Bois, B., Demeyer, S., Rieger, M.: On the
detection of test smells: A metrics-based approach for general ﬁxtureand eager test. Software Engineering, IEEE Transactions on 33(12),
800–817 (2007)
[34] Watkins, C.J., Dayan, P.: Q-learning. Machine learning 8(3-4), 279–292
(1992)
[35] Xie, Q., Memon, A.: Designing and comparing automated test ora-
cles for GUI-based software applications. ACM Trans. Softw. Eng.Methodol. 16(2007)
[36] Xie, Q., Memon, A.M.: Using a pilot study to derive a GUI model
for automated testing. ACM Transactions on Software Engineering andMethodology (TOSEM) 18(2), 7 (2008)
[37] Xie, T., Tillmann, N., de Halleux, J., Schulte, W.: Fitness-guided path
exploration in dynamic symbolic execution. In: Dependable Systems &Networks, 2009. DSN’09. IEEE/IFIP International Conference on, pp.359–368. IEEE (2009)
[38] Yuan, X., Cohen, M., Memon, A.M.: Covering array sampling of input
event sequences for automated GUI testing. In: Automated SoftwareEng. (ASE), pp. 405–408 (2007)
148