Statistical Errors in Software Engineering Experiments: A
Preliminary Literature Review
Rolando P. Reyes Ch.
Universidad PolitÃ©cnica de Madrid
Madrid, Spain
Universidad de las Fuerzas Armadas ESPE
SangolquÃ­, Ecuador
rpreyes1@espe.edu.ecOscar Dieste
Universidad PolitÃ©cnica de Madrid
Madrid, Spain
odieste@fi.upm.es
EfraÃ­n R. Fonseca C.
Universidad de las Fuerzas Armadas ESPE
SangolquÃ­, Ecuador
erfonseca@espe.edu.ecNatalia Juristo
Universidad PolitÃ©cnica de Madrid
Madrid, Spain
natalia@fi.upm.es
ABSTRACT
Background: Statisticalconceptsandtechniquesareoftenapplied
incorrectly,eveninmaturedisciplinessuchasmedicineorpsychol-
ogy. Surprisingly, there are very few works that study statistical
problems in software engineering (SE). Aim:Assess the existence
of statistical errors in SEexperiments. Method: Compile the most
commonstatisticalerrorsinexperimentaldisciplines.Surveyex-
perimentspublishedinICSEtoassesswhethererrorsoccurinhigh
qualitySEpublications. Results: Thesameerrorsasidentifiedin
othersdisciplineswerefoundinICSEexperiments,where30%of
thereviewedpapersincludedseveralerrortypessuchas:a)missing
statistical hypotheses, b) missing sample size calculation, c) failure
toassessstatisticaltestassumptions,andd)uncorrectedmultiple
testing.Thisratherlargeerrorrateisgreaterforresearchpapers
where experiments are confined to the validation section. The ori-
ginoftheerrorscanbetracedbackto:a)researchersnothaving
sufficient statistical training, and, b) a profusion of exploratory
research. Conclusions: This paper provides preliminary evidence
that SE research suffers from the same statistical problems as other
experimentaldisciplines.However,theSEcommunityappearsto
be unaware of any shortcomings in its experiments, whereas other
disciplinesworkhardtoavoidthesethreats.Furtherresearchisnec-
essarytofindtheunderlyingcausesandsetupcorrectivemeasures,
buttherearesomepotentiallyeffectiveactionsandareapriorieasy
to implement: a) improve the statistical training of SE researchers,
andb)enforcequalityassessmentandreportingguidelinesinSE
publications.
CCS CONCEPTS
â€¢General and reference â†’Surveys and overviews;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
Â© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180161KEYWORDS
Literature review, Survey, Prevalence, Statistical errors
1 INTRODUCTION
Experimentationmakesextensiveuseofstatistics.Severalstudies
warn about the existence of scientific articles using inappropri-
atestatisticalprocedures[ 5,32,62].Thishappenseveninmature
disciplines, such as the health sciences [6].
In turn, there are very few papers studying statistical errors
insoftwareengineering(SE)articles.Therearepapersdiscussing
statistical power [ 23], heterogeneity in meta-analysis [ 52], and the
relative strengths and weaknesses of cross-over designs [ 41,81]
in SE. This stands in contrast to other disciplines where papers
warningaboutproblemsinsimplestatisticalconceptssuchashy-
pothesis statement [ 16,58], interpretation of p-values [ 62], sample
size calculation [ 2,25], significance levels [ 58], etc., are quite com-
mon.
We aim to assess the prevalence of these problems in the SE
literature. We have compiled the most common statistical errors in
experimental disciplines and surveyed empirical papers published
in ICSE between 2006 and 2015, to check whether or not these
papers are subject to the compiled errors. Our results suggest that
SE experiments have the same weaknesses as in other sciences. SE
researchersdonotuserelativelysimpleconceptslikehypothesis
statement, sample size estimation, inference, and post-hoc testing
correctly. These problems seem to be related to poor statistical
training, and the use of exploratory research.
Our contributions confirm the shortcomings in experimental
SEresearchandidentifytheirorigin.Inouropinion,theSEcom-
munity should improve researchersâ€™ statistical training and, more
importantly,establishmechanisms(e.g.,qualityassessmenttools,
reporting guidelines) to identify and correct statistical problems in
SE experiments before they are published.
The structure of this paper is as follows. Section 2 provides
background on the topic of statistical errors in science and SE.
Section 3 presents a short literature review identifying severalstatistical errors. We screen articles reporting experiments for a
subset of the above errors in Section 4. The origin of the identified
11952018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Reyes et al.
errors is evaluated in Section 5. Section 6 offers a critical appraisal
of this. Finally, the conclusions are reported in Section 7.
2 BACKGROUND
2.1 Statistical Errors in Experimental
Disciplines
Scientificandengineeringresearchersapplystatisticaltechniques
to analyze and interpret many of their research results. Hence, sta-
tisticaltechniqueshaveexperiencedanincreaseinuse,particularly
inmedicine[ 2,63,83],psychology[ 5],education[ 21],andsocial
science [25, 58].
There is a relatively large collection of publications that provide
information about the existence of statistical problems in virtually
alldisciplines. Notall publicationsare recent;theyhave beenavail-
ablesincethewidespreadadoptionofexperimentalresearchintheir
respectiveareas.Thereportedproblemshaveabroadscope[ 51],
including: the definition of statistical hypotheses [ 16,58], interpre-
tation of p-values [ 62], sample size calculation [ 2,25], significance
levels [58], and confidence intervals [16], and so on.
Papersaboutstatisticalshortcomingsinother disciplineshave
derivedtheirresultsfromsometypeofliteraturereviewofprimary
studiesfromoneormorespecializedjournals.Theirconclusions
are surprising and worrying since they report high error rates:
â€¢Welch[83]studied145articlesfromoneofthemostrenowned
medical journals, the American Journal of Obstetrics and Gy-
necology,andfoundthat52.6%ofthearticlescontainedin-
adequate or incomplete statistical descriptions.
â€¢Bakker [5] evaluated 218 articles from high and low impact
psychologyjournals.Theauthorreportedthatlowimpact
journalsexhibitstatisticalinconsistenciesmorefrequently
than high impact journals. Bakker determined that about
15%ofallthepapersfrombothhighandlowimpactjournals
have at least one incorrect statistical conclusion.
â€¢Ercanetal.[ 25]evaluated164and145articulesinpsychiatry
andobstetrics,respectively.Ofthepsychiatricandobstetrics
publications, 40% and 19%, respectively, contained mistakes
regarding: sampling, sample sizecalculation, and contradic-
tory interpretations of inferential tests.
â€¢Kilkenny et al. [ 39] assessed the experimental design of 271
papers published in MedlineandEMBASE from 2003 to 2005.
More than 60% of the papers are subject to biases during
theassemblyofthestudycohort,weakstatisticalanalysis,
missing information, etc.
The origin ofthe statistical errors can betraced back to several
causes:
â€¢AccordingtoCastroetal.[ 75],theanalysisandinterpreta-
tionofempiricalresultsinanyscientificdisciplinedepend
primarily on how well researchers understand inferential
statistics. Theauthors suggestedthat researchersin theed-
ucation community, especially PhD students, are prone to
misconceptions,particularlywhentheyareusingabstract
statisticalconcepts,suchasconfidenceintervals,sampling
distributionswithsmallnumbers,samplingvariability,dif-
ferent types of distributions, and hypothesis tests.â€¢Cohen et al. [ 19] conducted an empirical study with degree
students.Theyfoundthatstudentslackstatisticalknowledge,
whichleadstothemisinterpretationofstatisticalconcepts
and biased judgements.
â€¢Brewer[12]evaluated18statisticalhandbooksbyrenowned
publishers, e.g., Academic Press, Addison-Wesley, McGraw-
Hill, Prentice-Hall, John Wiley , etc. These books contained
inaccurate statements in topics such as sampling distribu-
tions, hypothesis testing, and confidence levels.
2.2 Statistical Errors in SE
The SE community apparently has limited awareness of the ex-
istence and impact of statistical shortcomings in its publications.
WhenwesearchedforSEpapersrelatedtostatisticalproblems,the
only results were: DybÃ¥ et al.â€™s paper regarding statistical power
[23],Millerâ€™spaperonmeta-analysis[ 52],andtwopapersbyKitchen-
ham [41] and Vegas et al. [ 81] that focused on within-subject de-
signs.
Several other papers discuss specific statistical issues. For in-
stance, Kitchenham introduced robust statistical methods [ 42],
while Arcuri and Briand discussed statistical tests for the assess-
ment of randomized algorithms [ 4]. These papers do not assess
theweaknessesincurrentresearch.Theysuggestopportunitiesfor
improvement in the toolset that SE researchers currently use.
There is a manifest difference between SE and other experimen-
tal disciplines regarding statistical errors. In medicine and other
sciences,statisticalproblemsareroutinelyidentifiedinpublications;
this issue is almost completely overlooked in SE.
Otherdisciplineshavenotaddressedstatisticaldefectsandmethod-
ological problems until relatively late on. For instance, while thefirst formal randomized clinical trial in medicine was conductedin the 1940s [
8], the first publication about statistical defects in
this field that we are aware of was published in the 1970s [ 30].
Given thatSE isonly justadopting experimental methodsand the
associated statistical techniques, its failure to pay attention to the
assessment of statistical issues should come as no surprise.
Thispaperreportsanexploratorystudyaimingtoanswerthe
following research questions:
RQ1: What are the most common problems associ-
ated with the use of experimental procedures in exper-imental disciplines?
RQ2: What is the rate of statistical errors in SE re-
search?
3 STATISTICAL ERRORS IN EXPERIMENTAL
DISCIPLINES
3.1 Review Strategy
ToanswerRQ1,wereviewedseveralspecializedbookspublished
onthetopic,suchasGoodetal.[ 29],Vickers[ 82],andHuck[ 34].
Thesebooksprovideagoodstartingpointforourexploratorystudy
because they are not related to any specific discipline (althoughthere is some bias toward the health sciences) and they focus on
serious errors often inspired by real research.
1196
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Statistical Errors in Software Engineering Experiments ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
3.2 Collected Data
Tworesearchers(O.DiesteandR.P.Reyes)reviewedtheabovethree
books.Theyfound93textsectionsclearlypointingtosometypeof
error that canbe frequently found inthe literature. Discrepancies
were solved by consensus. The complete listing of paragraphs is
available at https://goo.gl/8zb9LU, including links to the reference
books and related literature.
3.3 Analysis Method
We applied thematic synthesis to classify the statistical errors. We
applied the guidelines by Creswell [ 21] and Cruzes et al. [ 22]t o
avoidbiasandachievemethodologicalrigorinthesynthesisand
interpretation of results [ 11]. The analysis consisted of two stages:
coding and theme definition. It was conducted by the same two
researchers than collected the data.
During the coding stage, both researchers independently as-
signed low-level codes to each text section, which were later re-
viewed and harmonized. We created 93 different codes. During the
themedefinitionstage,codesweregroupedtogetherbymeansof
higher-level codes. This procedure was aligned with our purposes
since the high-level themesrepresenterror-prone areas. Bothre-
searchersworkedcollaboratively.Theyorganizedthelow-levelcon-
cepts into high-level themes according to a directed graph, shown
inFig.1.Themesandconnectionsbetweenthemesandconcepts
are available at https://goo.gl/8zb9LU.
Nodes represent categories of statistical errors. Categories be-
comeprogressivelymoreabstractasthetreeistraversedfromright
to left. For instance, the node study design (Fig. 1, bottom left) is
connected to the nodes assignment andsampling. This means that
thehigh-levelcategory studydesign containstwotypesoferrors:
assignment andsampling errors. Likewise, assignment splits into
furtherlower-levelerrortypes,suchas matching, randomization,
etc. Notice that Fig. 1 shows only one subset of the error types that
we have identified to keep the graph within page limits. The full
graph is available at https://goo.gl/qovXQw.
Theremaybemultipleconnectionsbetweencodesandhigh-level
themes and between one high-level theme and another because
theyarementionedinseveralbooks,ormorethanonceinthesame
book in different contexts. For instance, randomization is discussed
twice in terms of the representativeness of the random samples:
(item#40) Misconception:Ifatrulyrandomprocessis
used to select a sample from a population, the resulting
sample willturn outto bejust likethe population, but
smaller.[34, pp. 123]
(item#41) Misconception:Asampleofindividualsdrawn
fromalarger,finitegroupofpeopledeservestobecalled
arandomsamplesolongas(1)everyoneinthelarger
group has an equal chance of receiving an invitation to
participateinthestudyand(2)randomreplacements
arefoundforanyoftheinitialinviteeswhodeclineto
be involved. [34, pp. 127]
andonceagainwithregardtotheequivalenceofexperimental
groups formed by random assignment:
(item#90) Theideabehindrandomizationistomake
thegroupsassimilaraspossible[...].Baselinedifferencesat the beginning of the trial, such as in age o gender,
are due to chance. [...] giving a p-value for baseline
differencebetween groupscreated byrandomizationis
testinganullhypothesisthatweknowtobetrue. [82,
pp. 100]
These repeated associations are an indication of relevance, and
thus the arcs connecting the corresponding nodes have been made
proportionally wider. The number next to the arc indicates the
numberoftimesthattheconnectionappearsintherawdata.Dotted
lines represent connections that appear just once.
3.4 Review Results
Statistical errors can be classified in to three groups: a) experimen-
tation,b)meta-analysis,andc)prediction.Mosterrorsarerelatedto
experimentation.Nevertheless,itisnoticeablethatmeta-analysis
appears three times in connection with subgroup analysis and the
combinationofstudieswithdifferentdesigns.Predictionappears
just once, with respect to with linear modeling. In what follows,
we focus on problems associated exclusively with experiments.
Analysisistheexperimentationfacetmostoftenmentionedin
connection with statistical errors. In the three reviewed books,analysis errors appear 63 times. There are two main sources ofproblems with analysis: the application of inferential techniques
and the interpretation of results:
â€¢The inferential techniques most often used during experi-
mentaldataanalysisareclassicaltests,suchast-tests,and
theirrelatedconcepts,suchasp-valuesandtails.Researchers
often make wrong assumptions about the tests (e.g., robust-
nessoft-test),andtheyselecttestsincircumstancesinwhichthey cannot be applied (e.g., ordered alternative hypotheses)
or are sub-optimal (e.g., low power tests). All common tests,
includingt-tests,correlations,andANOVA,arementioned
in this context.
â€¢Anotherfrequentlymentionedinferentialtechniqueislin-
earmodeling;multiplelinearregressionisthebestknown
exampleoflinearmodeling.Themostfrequentlymentioned
problemistherationalebehindthedefinitionofthelinear
model. Other issues, such as the violation of assumptions
and usage beyond limits (e.g., outside the linear phase), are
also reported.
â€¢Manysupposedlybasicconcepts,suchasconfidenceinter-
vals, statistical significance, or p-valuesare frequently mis-
interpreted.
Studydesignissecondtoanalysis.Thisconceptincludesmethod-
ologicalissuesconnectedtothemanagementofexperimentalunits,
such as sampling and assignment. In both cases, the sources of the
problemsareinappropriateormissingrandomizationandsample
size calculation.
Reporting is another troublesome issue, which is mentioned the
samenumberoftimes(ten)asstudydesign.Therearemanysources
of reporting defects (e.g., overlooking experimental incidents or
multipletesting),althoughtheabsenceofdescriptivestatistics(e.g.,
means) is emphasized (tree times) in the reviewed books.
The last prominent issue is goal definition. Researchers often
do notstate statisticalhypotheses. Failureto explicitlydefine null
hypotheses appears three different times in Fig. 1.
1197
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Reyes et al.G
 










 

S




	








 



	

	5
S















 



S




	





	










N






 



	

	3
A













 



	

	

p





	E












 

6
R


 




1

S





	


9A




	

	6
3
M


	







T

	

	L






 



	D

	








	




	



	3
A
	
	






D


 











	

	2
S





	












 
S






4
D







 

 



















B






I







3
6
D







 




 
3
I










 
2
4
T



	S






 
6
A
	
	





 

	1
3
p


	
P
 
	

p

 



	

	C
 







 
2
C

p
	






R



 
	U
















 



	6
M


















 



	2
G

















 



	C









	



	4
D

	



	

 




	



	R



 






 
2
M






22
9
D

	







 
	R



















2
B
 
 

	







2 3 3 6 2
3C
 















4
M


E





	



C

 






	





Figure 1: Classification of statistical errors in experimental research papers
1198
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Statistical Errors in Software Engineering Experiments ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
4 STATISTICAL ERRORS IN ICSE
EXPERIMENTS
The aim of RQ2 is to find out the rate of statistical errors in SE
research. To answer RQ2, we evaluated the experiments published
in ICSE over 10 years (2006-2015). ICSE is SEâ€™s flagship conference.
Ourevaluationshouldidentifytherateofcommonstatisticalerrors
inthebestSEresearch;thesituationoflowerqualitySEresearch
is likely to be worse.
4.1 Evaluation Instrument
Thecomplete listof statisticalerrors thatwe have compiled con-
tains almost 100 items. Since statistical errors are ubiquitous in the
generalresearchliterature,itishighlylikelythatseveralofthe âˆ¼
100 problem typeswould appear in virtually any SEpaper as well.
Therefore,anexhaustivereviewofSEexperimentswoulddrawa
too pessimistic picture of our field.
Wehavefocusedonrecurrenttypesoferrors(denotedbywidear-
rowsinFig.1.Forinstance,nullhypothesis-relatedproblemsareref-
erenced multiple times in Fig. 1, as well as test assumptions or cen-
tralmeasures.Wehaveselectedthemosterror-pronestatisticalcon-
cepts,developedappropriatequestions,andcreatedthe10-questionchecklist shown in Table 1. All these questions can be easily traced
back to Fig. 1 (or the online version at https://goo.gl/qovXQw).
Further clarification is required at this point:
â€¢Q1.1 and Q1.2 may look outdated due to the increasingcriticisms of the null-hypothesis and significance testing
(NHST), and the recommendations to adopt other statistical
approaches such as confidence intervals and effect size in-dices [
10,71,77]. However, SE research has not yet taken
up these recommendations. For instance, only four out of
21experimentspublishedinICSEfrom2006to2015report
anymeasureofeffectsize,andtwooutof21refertoconfi-
dence intervals. Nowadays, NHST is still the main statistical
approach used in SE.
â€¢Q4 (Have subjects been randomly assigned to treatments? )
may not be applicable to some types of experiments, e.g.,
when two defect prediction algorithms are applied to the
same code, that is, matched pairs or similar designs. In such
cases, the question is answered as N/A. A similar strategy is
appliedforanyquestionthatdoesnotmakesenseforagivenexperiment,e.g.,Q5(Havethetestassumptions(i.e.,normality
andheteroskedasticity)beencheckedor,atleast,discussed? )
when an experiment does not use statistical tests.
â€¢Testassumptionsvaryfromtesttotest.Inmanycases,ref-
erence books state incomplete or even questionable assump-
tions.Thus, inQ5 (Havethe testassumptions (i.e.,normality
andheteroskedasticity)beencheckedor,atleast,discussed? ),
wewillpayattentiononlytothemostusualconditions(nor-
mality and heteroskedasticity) that have to be examined
before applying virtually any parametric test.
â€¢Q7(Havetheanalysisresultsbeeninterpretedwithreferenceto
applicable statistical concepts, such as p-values, confidence in-
tervals,andpower? )wouldappeartobearathercrucialques-
tion.Fig.1showsthatthenode interpretation isconnected
bywidearcswithnodesrepresentingrelativelysimplesta-
tisticalconcepts,suchaspower,confidenceinterval,p-value,and so on. However, we doubt that we can answer this ques-tionobjectively.Whileauthorstypicallydiscusstheirresults
atlength, theymaysimplify oromit somestatisticalissues
required to clearly transmit their message to readers. Thus,
we face the risk of making mistakes, e.g., evaluating Q7 neg-
atively due to incomplete reporting. We decided to skip this
question (and there is a line through it in Table 1).
â€¢MultipletestingdoesnotappeartobeakeyissueinFig.1.
However, it was cited three times as a source of problems
duringbothanalysisandreporting;notethattherearethreeincomingarcsforthisnodeinFig.1.ThisjustifiesQ9(Ismul-tipletesting,e.g.Bonferronicorrection,reportedandaccounted
for?).
â€¢Q10(Aredescriptivestatistics,suchasmeansandcounts,re-
ported?) is important for both analysis and reporting. We
considerthisissueinthecontextofreportingonly,soasnot
to inflate the number of defects found.
Table 1: Evaluation checklist
#Question
Q1.1 Are null hypotheses explicitly defined?
Q1.2 Are alternative hypotheses explicitly defined?
Q2 Has the required sample size been calculated?
Q3 Have subjects been randomly selected?
Q4 Have subjects been randomly assigned to treatments?Q5
Havethetestassumptions(i.e.,normalityandheteroskedas-
ticity) been checked or, at least, discussed?
Q6 Has the definition of linear models been discussed?
Q7Havetheanalysis resultsbeeninterpreted bymaking
reference torelevantstatistical concepts, suchasp-values,
confidence intervals, andpower?
Q8Do researchers avoid calculating and discussing post hoc
power?
Q9Is multiple testing, e.g. Bonferroni correction, reported and
accounted for, ?
Q10Aredescriptivestatistics,suchasmeansandcounts,reported?
4.2 Target studies
OuroriginalaimwastosurveyonlyICSEexperimentalpapersfrom
2006to2015.However,thedecisionsoonprovedtobequestionable.
We conducted a pilot study on ICSE 2012 edition to check the
feasibilityofourstudy.Weimmediatelyrealizedthatthenumber
of fully-fledged experiments was quite low: we found only four
experiments.Inturn,wefoundmanysmall-scaleexperimentsaimedatevaluatingthepropertiesofnewtechniquesormethods,typically
reportedinresearchpaperEvaluationSections.Tobeprecise,we
identified 16 experiments as evaluations (18.4% of the total number
of papers in ICSE 2012).
The question was whether the survey should be extended to
experimentsasevaluations,orrestrictedtostandaloneexperiments.
Experiments as evaluations often apply an experimental methodol-
ogy,butaretypicallyonlyonetothreepageslong.Thecompressed
reporting format may lead to writing practices that may be mis-conceived as statistical errors by reviewers. On the other hand,
experimentsasevaluationsaccountforalargeshareofempirical
research,andtheresultsofthissurveywouldbeincompleteifthey
were overlooked.
1199
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Reyes et al.
We decided to evaluate both types of studies separately. In a
firststage,wesearchedforallstandaloneexperimentspublished
in ICSE from 2006 to 2015. We found 21 a total of papers. We then
collected a similar number1of experiments as evaluations to avoid
over-representation.
4.3 Study selection
Two researchers (O. Dieste, E. R. Fonseca, and R. P. Reyes) worked
separately to screen the tables of contents of the ICSE 2006-2015
Technical Tracks. They reviewed the title and abstract for any
indication that the paper reported an experiment. If in doubt, they
examined the full text in search of further evidence of at least two
treatmentsbeing compared,thatis,the minimumconditionto be
met by any experiment.
The total number of papers and the papers pre-selected after
screeningareshowninTable2.Thepre-selectionagreementwas
calculatedusingFleissâ€™ Îº,asrecommendedbyK.L.Gwet[ 31,pp .
52].Îº=0.45, typically considered as moderate [27]. This implies
that we may have failed to identify some experiments. Note that it
isnotstraightforwardtoidentifyexperimentsusingmetadata,such
as titles and abstracts, due to missing methodological descriptors.
Three researchers (O. Dieste, E. R. Fonseca, and R. P. Reyes)
individually reviewedthe pre-selectedpapers andclassified them
into theexperiment andnon-experiment categories. Disagreement
was resolved by consensus. A total of21 papers were classified as
standaloneexperiments.Thisrepresents2.7%ofthetotalnumber
of papers published in ICSE. Previous research has already pointed
outthelownumberofcontrolledexperimentspublishedinICSE
[91]. The agreement level for this step of the selection process
wasFleissâ€™ Îº=0.52,typicallyconsideredas moderate [27,31].As
reportedbelow,thislowagreementisduetotheexistenceofmissing
information (e.g., hypothesesor randomization procedures) in the
manuscripts.Furtherdetailsareavailableathttps://goo.gl/jHWpq3.
Table 2: Summary of the selection process. Experiments as
evaluations between parentheses
YearTotal papers
(TP)After
screeningSelected %
2006 72 8 2 (3) 2.8% (4.1%)
2007 64 7 2 (3) 3.1% (4.7%)
2008 85 8 1 (3) 1.2% (3.5%)
2009 70 7 0 (3) 0.0% (4.3%)
2010 62 5 1 (3) 1.6% (4.9%)
2011 62 5 1 (3) 1.6% (4.9%)
2012 87 31 4 (3) 4.6% (3.5%)
2013 85 8 1 (3) 1.2% (3.5%)
2014 99 11 5 (3) 5.0% (3.1%)
2015 84 11 4 (3) 4.8% (3.5%)
Total 770 101 21 (30) 2.7% (3.9%)
Finally, three ICSE experiments as evaluations per year were
selectedatrandomfromthetablesofcontentsoftheICSETechnical
Track. The three researchers independently reviewed these papers,
and discrepancies were resolved by consensus. The process was
1We rounded up from 21 to 30, i.e., three papers per editionÃ—10ICSE years =30
papers.repeateduntilthreeexperimentsasevaluationshadbeenidentified
for each ICSE conference from 2006 to 2015.
4.4 Execution
The three researchers individually evaluated all papers and gave a
yes/no/notapplicable answertoeachchecklistquestion(seeTable7).
Thelevelofagreementwas substantial toalmostperfect inmany
cases,whichincreasesthereliabilityofourresults.Detailsofthe
evaluation are available at https://goo.gl/3iy9eL (standalone experi-
ments) and https://goo.gl/qCboSX (experiments as evaluations).
Table 3: Agreement levels per question
Standalone
Exp.Exp. as Eval.
Sec.
Stage ÎºAgree ÎºAgree
Goal definitionQ1.1 0.839 Almost perfect 0.643Substantial
Q1.2 0.746 Substantial 0.788Substantial
Study designQ2 1,000 Perfect 1.000Perfect
Q3 0.092 Slight 0.389Fair
Q4 0.541 Moderate 0.585Moderate
AnalysisQ5 0.752 Substantial 0.662Substantial
Q6 1.000 Perfect 0.558Moderate
Q8 0.894 Almost perfect 0.803Almost perfect
Reporting Q9 0.592 Moderate 0.659Substantial
Q10 1.000 Perfect 0.480Moderate
4.5 Survey Results
Table 4 summarizes the survey results. Percentages are calculated
as{Yes|No|N/A}
9. The No column represents the percentage of
papers in the sample that are affected by the error indicated by
the corresponding question, i.e., the prevalence of the statistical
error rate. Q1 was split into two parts to differentiate the problems
related to the null (Q1.1) and the alternative (Q1.2) hypotheses.
Table 4: Defect rates
Standalone
ExperimentsExperiments
asEvaluation
Sections
Stage Yes No N/A Yes No N/A
Goal definitionQ1.166.7% 33.3% 0.0%13.3% 83.3% 3.3%
Q1.257.1% 42.9% 0.0% 6.7% 90.0% 3.3%
Study designQ20.0%100.0% 0.0% 3.3% 96.7% 0.0%
Q328.6% 71.4% 0.0%13.3% 86.7% 0.0%
Q466.7% 28.6% 4.76% 20.0% 0.0%80.0%
AnalysisQ561.9% 33.3% 4.76% 13.3% 20.0% 66.7%
Q64.8% 0.0% 95.24% 3.3% 0.0% 96.7%
Q885.7% 9.5%4.76% 36.7% 0.0%63.3%
Reporting Q99.5%71.4%19.07% 3.3%26.7% 70.0%
Q1095.2% 0.0%4.76% 76.7% 13.3% 10.0%
Wefoundclearevidenceoftheexistenceofstatisticalerrorsin
ICSE papers. The rate of the different errors varies, but it is rather
large in many cases, e.g., Q1, Q2, Q3 and Q5 (hypothesis definition,
samplesizecalculation,randomselectionandassumptionchecking,
respectively). Although the current situation is rather serious, it
has improved as compared to previous reports [ 91]. The results are
1200
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Statistical Errors in Software Engineering Experiments ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
somewhatdifferentforstandaloneexperimentsandexperiments
as evaluations. For experiments as evaluations, the number of N/A
responses is much higher. There appear to be reasons for this:
(1)Mostoftheexperimentsasevaluationsapplyamatchedpairs
design.Random assignment(Q4)istypically notapplicable
in this case, e.g., two different bug prediction algorithms are
applied to the same code [78, 85].
(2)Alargenumberofstudies,e.g.,[ 46,90],conducttheanalysis
using descriptive statistics only. Descriptive statistics do not
have assumptions to check (Q5). When inferential statistics
are not used, Q6-9 (linear modelling, power, and post-hoc
testing) are not applicable either.
We ran a CHAID tree2classification to confirm the above ob-
servations.AN/AvalueinQ4generatesasubsetcontaining80%
of all the experiments as evaluations studies ( Ï‡2=29.7,df=
2,pâˆ’value<0.001). The classification tree confirms that non-
random assignment due to matching is a differential characteristic
of the experiments as evaluations.
Focusingontheerrorrate,wefoundthatbothstandaloneexper-
iments and experiments as evaluations yield similar values3when
examinedusing QuestionÃ—StudyType contingencytables,withthe
exceptionofQ1.1( Ï‡2=15.4,df=1,pâˆ’value<0.001)andQ1.2
(Ï‡2=20.7,df=1,pâˆ’value<0.001). In both cases, standalone
experiments define null hypotheses (Q1.1) often and alternative
hypotheses(Q1.2)five(66.7%vs.13.3%)andeighttimes(57.1%vs.
6.7%), respectively, more often than experiments as evaluations.
Both types of studies do not show statistically significant dif-
ferences for theremaining questions, although some may befalse
negatives. There are a large number of N/As for several questions,
which reduces the amount of usable data, thus lowering the power
of the tests. However, the low p-values for both the Ï‡2and the
Fisherâ€™sexact testsuggest thatQ3,Q4, Q5,Q10could achievesta-
tistical significance with larger samples. In all cases, standalone
experimentsperform randomselection(Q34,random assignment
(Q4), assumption checking (Q5) and reporting of descriptive statis-
tics (Q10) more frequently than experiments as evaluations. Albeit
not as large as in the case of Q1.1 and Q1.2, differences are still
substantial, e.g., 61.9% vs. 13.3% for Q5.
We also find from Table 4 that:
â€¢The required sample size (Q2) has been calculated in just
one study. The definition of the linear model (Q6) has been
considered in just two cases.
â€¢Multiple testing(Q9) is a pervasive problem in SEresearch.
Most studies fail to report or correct for multiple testing
using adequate, e.g., Bonferroni or False Discovery Rate,
methods.
2The response variable was the studytype (standalone experiments and experiments
asevaluations)andthepredictorswerethequestionsQ1-10.WeusedtheSPSSdefault
CHAID parameters, with the exception of the parent and child nodes, which were set
to 10 and 5 cases, respectively, due to the small number of cases.
3Notice that N/A values may suggest misleading relations. For instance, Q9 yields
Yes/Novaluesof9.5%and71.4%forstandaloneexperiments,andof3.3%and26.7%for
experimentsasevaluations.Valuesdiffergreatly,buttheodds71.4
9.5=7.5âˆ¼26.7
3.3=8.1
are rather similar.
4Notice that Q3 yields Îº=0.09andÎº=0.39for standalone experiments and
experiments as evaluations, respectively. Random sampling is a controversial issue in
SE. Results for Q3 should be viewed with caution.â€¢There isa high rate ofrandom selection (Q3).Nevertheless,
thisproblemisnoteasytosolveinhumanexperimentsbe-
cause it is troublesome to assemble cohorts. In turn, randomselectioncouldbeeffectivelyappliedinnon-humanresearch,
e.g., when data is extracted from code repositories.
This survey shows that common statistical errors that occur in
other sciences happen in SE as well. We have been able to survey a
very limited number of experimental papers in one SE conference.
However, both the type and number of problems found suggest
that SE is facing the same challenges as in other sciences.
5 DISCUSSION
The most likely explanation for the occurrence of the statistical
errorsassociatedwithQ1-10istherecentadoptionofexperimental
methods in SE. Many researchers have not taken formal courses
onexperimentalmethodologyandinferentialstatisticsaspartof
theirpostgraduatetraining.Self-educationtendstoleadtomajor
differences among individuals. If these assumptions were true, two
scenarios would be logical consequences:
(1)The studies conducted by skilled researchers would be ofhigher quality (where quality means error freeness, e.g.,
Yes answers
All answers) than experiments conducted by less skilled re-
searchers. We could thus expect the quality values spread torange0%to100%.Aserrorsareindependent,qualityfollows
a normal distribution5.
(2)Any statistical concepts closely related to practice, e.g., ran-
dom assignment (Q4), assumption checking (Q5), and re-porting (Q10), would have a lower error probability than
theoretical notions, e.g., hypotheses definition (Q1), sample
sizecalculation(Q2),randomselection(Q3),linearmodeling
(Q6), post-hocpower calculation (Q8), andpost-hoc testing
(Q9).
In order to check Scenario 1 above, Fig. 2 shows the histograms
for both types of studies. In the case of standalone experiments
(Fig. 2a), the histogram matches the assumption: the quality scores
rangeacrossthe0%to100%interval,andthedistributionisnormal
(Shapiroâˆ’Wilk =.947,df=21,pâˆ’value =.300).Experimentsas
evaluations(Fig.2b)paintaratherdifferentpicture.Thedistributionisskewedtotheleft(
skewness =1.02),indicatingthatpaperquality
isconcentratedaroundthelowscores.Thisisaclearlynon-normal
distribution ( Shapiroâˆ’Wilk =.863,df=30,pâˆ’value =.001).
Theaboveanalysissuggeststhatthecausesbehindthestatistical
errorsdifferdependingonthestudytype.Inthecaseofstandalone
experiments, poor statistical training may explain the observed
errors.
In the case of experiments as evaluations, training alone cannot
explain the data. In our opinion, the low scores point to the sec-ondary role of statistics and experimental methodology in these
papers.Notonlydoexperimentsasevaluationstakeuparelatively
smallspaceofpapers(whichprovidesanexcuseforsummarizing
â€œunnecessarystuffâ€),butstatisticalrigoralsoprobablytakessecond
5Statistical errors are probably dependent. When a researcher learns a statistical topic,
e.g., sample size calculation, this knowledge is likely to lead to the avoidance of other
errors, e.g., post-hoc power calculation. Ho wever, the errors underlying Q1-10 are too
wide-ranging to appear strongly clustered in papers.
1201
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Reyes et al.
place in the authorsâ€™ objectives (they are probably more interested
in providing a convincing case for their proposals).
To check the Scenario 2 above, Table 5 contains the odds of
makinganerror.Theoddsarethesameconceptasintroducedin
footnote 3; they represent the probability of an event occurring
(providing a negative response to the Qiquestion, i.e., the paper
includesastatisticalerror)ratherthananother(providingapositive
response to Qi, i.e., there is no such error).
Table 5: Odds of making the error indicated in Q1-10
Odds (NoÃ·Yes)
Concept type QStandalone
experimentsExperiments
as evaluations
Practical conceptsQ4 0.4 0.0
Q5 0.5 1.4
Q10 0.0 0.2
Theoretical conceptsQ1.1 0.5 5.0
Q1.2 0.8 10.0
Q2 +Inf +Inf
Q3 2.5 5.0
Q6 0.0 0.0
Q8 0.1 0.0
Q9 10.0 10.0
Inthecaseofexperimentsasevaluations,thedata exactlymatches
ourassumption6.Alltheoreticalconceptshavelargeodds( â‰¥5.0),
asopposedtopracticalnotionswhoseoddsaresmall( â‰¤1.4).For
standaloneexperiments, thesituation is moreor less thesame. For
the theoretical concepts, odds are smaller than for experiments
as evaluations, with the only exception of Q9. This is consistent
withthehighererrorrateoftheexperimentsasevaluationsstud-
ies. However, Q1.1 and Q1.2 odds are much smaller (0.5 and 0.8,
respectively) and comparable to the odds that appear in the group
of practical concepts.
Theaboveanalysisconfirmsthatpoortrainingisthemostlikely
explanation for the presence of statistical errors in experiments. In
thecaseofexperimentsasevaluationsstudies,amorecasualusage
of statistics increases the error rate, but the final outcome is the
same.
One anomaly in Table 5 is the large odd that Q9 exhibits for
standalone experiments. It has the same value as in experimentsas evaluations. This value is even less plausible given the small
odds for Q1.1 and Q1.2: any researcher with a good knowledge
of statistical hypotheses should be aware of the impact of multi-
pletestingon Î±levels.Themostlikelyreasonisthat,inaddition
totestingthestatisticalhypotheses,standaloneexperimentsalso
performexploratoryresearch(whichshowsupasalargenumber
of uncorrectedpost-hoc tests). Exploratoryresearch is acommon
feature of many SE experiments, e.g., [7, 86].
Post-hoc testing is associated to p-hacking, that is, the accep-
tance of outcomes that fit expectations [ 55]. p-hacking leads to
publication bias. JÃ¸rgensen et al. [ 38] evaluated the existence of
publication bias in SE publications following Ioannidisâ€™ critical per-
spectiveformedicine[ 35].Bothpaperscametoasimilarconclusion:
6WearecrossingoutQ6andQ8because:a)Q6wasapplicableonlyintwooutof51
studies, and b) post-hoc power analysis (Q8) is a commission, not omission, error;
authors may perform correctly simply by not conducting a power analysis. Their
inclusion would not have challenged our conclusions.the likelihood of publication bias is rather high. More importantly
forourpurposes,bothpapersreportthattheunderlyingreasons
for publication bias are statistical, for example, multiple inference
testsandapreferenceforstatisticallysignificancetesting.Ourdata
supports JÃ¸rgensen et al.â€™s observations: post-hoc testing increases
thenumberoftests,andthefailuretousecorrectionmethodsfor
multipletestingprobablyinflatesthenumberoffalsepositives,thus
leading to publication bias.
6 THREATS TO VALIDITY
Thisstudy appliedtworesearch protocols:aliterature reviewand
apapersurvey.Thetwoprotocolsareverysimilar.Theyhaveto
meet a number of criteria concerning the relevance of the primary
studies with respect to the research questions and the consistency
across studies. Table 6 shows an assessment according to the ap-
praisalcriteriasuggestedbyThompsonetal.[ 80]7.Onthewhole,
the results of the evaluation were positive. We can be relatively
confidentthattheliteraturereviewandthesurveyresultsaretrust-
worthy.However,theyare incompleteduetothelimitednumberof
theprimarysourcesused;threewell-knownbooksaboutstatistical
errors and experimental papers from one SE conference were used
inthestudy.Theexternalvalidityofthisresearchisthuslimited.
Additionally,theliteraturereviewfollowedasimplified,butwell-
definedprotocol.Wetooknoteofthepagenumbersofthebooks
fromwhichweextractedinformationaboutstatisticalerrors.We
disclosed the entire thematic analysis, including codes and high-
levelthemes.Alldecisionsweremadebyatleasttworesearchers.
These precautions increased the validity of the literature review.
With regard to the paper survey, we have taken reasonable pre-
cautionstoavoidbiases.Threeresearchersparticipatedinthepaper
selection and evaluation. All decisions were recorded and made
publicforreview.Agreementlevels(usingFleissâ€™ Îº)werecalculated
and disclosed.
However,theprecautionstakendidnotmeanthatweperformed
a correct assessment in all cases. The selection process yielded a
low Fleiss Îºvalue, which suggests that we may have skipped some
experimentsand,thus,potentiallybiasedtheresults.Evenso,there
isnoquestionabouttherebeingstatisticalerrorsinSE,although
their occurrence rates or percentages may vary.
We do not claim that the reported rates are representative of
alltypesofSEresearch.Actually,theratesreportedinthispaper
probablyrepresentthebestpracticeinSEresearch,withthepossible
exceptionsoftheESEMandEASEconferences,andmaybesome
journals,likeEmpiricalSoftwareEngineering.Aswemoveaway
from outlets of repute, the number and severity of statistical errors
is likely to increase.
Finally, we should point out that the results addressed in the
DiscussionSectionaresomewhatspeculative.Wecannotruleout
alternativeexplanationsforthedistributionofqualityscores.As
usual, further research will be required to confirm our deductions.
7There are many appraisal procedures; we have chosen [ 80] because it is quite simple
and domain-independent.
1202
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Statistical Errors in Software Engineering Experiments ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
(a) Standalone experiments (b) â€Experiments as evaluationsâ€
Figure 2: Histograms
Table 6: Appraisal criteria used for review
Appraisal criteria RQ1 assessment RQ2 assessment
Wastheliteraturesearchcomprehensive? No No
Were appropriate criteria used to select
articles for inclusion?Partially:Theselectedbookswereappropriate,butwere
notspecialized.Thebooksmayhaveomittedthediscussion
ofspecificstatisticalerrorsthatprobablyappearinother
sources, such as research papers.Partially: ICSE is the flagship conference in SE.
Other conferences may publish lower quality
experiments.
Werestudiesthatweresufficientlyvalid
for the type of question asked included?Yes: The three books specifically addressed the topic of
statistical errors.Yes: Experiments published in ICSE represent
the best practice in ESE.
Were the results similar from study to
study?Yes: They were very consistent. Several errors were identi-
fied by two or three books simultaneously. The same high-
level themes were synthetized from the different books.Yes: Statistical problems repeated across experi-
ments.
7 CONCLUSIONS
The results of this preliminary review suggest that SE is subject to
thesametypeofstatisticalerrorsasarefoundinotherscientificdis-
ciplines.Theseproblemsarenotcomplicatedorsophisticated.They
are surprisingly simple and include undefined hypotheses, miss-
ingsamplesizecalculations,randomization,andmultipletesting,
amongothers.Itisrathersurprisingthatthereisnoinformation
abouttheexistenceofsuchproblemsinSE.TheSEmethodological
literature has not widely addressed this topic; only some papers
[23,41,52,81] have scratched the surface. Researchers may not be
awareoftheexistenceofstatisticalerrors,andmuchlesssooftheir
prevalence and potential impact.
There are two reasons that appear to explain the presence of
statistical errors in SE research: a) the recent widespread adoption
ofexperimentationinSE,andb)thefrequentuseofexploratoryre-search.Inouropinion,therapidadoptionofexperimentalmethods
in SE research has forced researchers into statistical self-education.
Additionally, it is rather unlikely that SE research teams include
or have access to statistical consultants. As a result, errors tend
slip into designs and ultimately published papers. This situation
matchesothersciencesthathavealongexperimentaltradition,such
as medicine and ecology, which have only recently paid attention
to statistical errors.As empirical research in SE approaches a mature stage, there
will be a greater awareness about statistical errors and the need to
avoid them. However, it would be unwise for the SE community
sit back and wait for the day to come. Besides setting up formal
training courses at universities and professional societies (which is
nowafoot),theSEcommunityshallenforcegoodpractices,such
as reporting guidelines and quality standards, that have proved
tobe usefulin othersciences, e.g.,medicine[ 72],psychology [ 74]
andeducation[ 28].Furthermore,thesegoodpracticescanbeeasily
enforcedbyjournaleditorsandconferencePCchairsatrelatively
little cost and effort.
Exploratory research is another source of problems. From the
viewpoint of this research, exploratory research takes the form
of missing statistical hypotheses and the execution of multiple
uncorrectedtests.However,theseerrorsleadtopublicationbias,asalreadydetectedinSE[
38].Experimentpre-registrationisprobably
thebestwaytofightagainstpublicationbias[ 15],butitisnoteasy
tosetupandenforce.Tothebestofourknowledge,pre-registration
hasnotbeendiscussedsofarinSE.Furtherresearchisneededto
find out effective ways to combat publication bias in SE. In themeantime, the establishment of reporting guidelines and quality
standards may improve the situation.
1203
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Reyes et al.
8 ACKNOWLEDGMENTS
This work was partially supported by the Spanish Ministry of
Economy and Competitiveness research grant TIN2014-60490-P,
EmpiricalSoftwareEngineeringResearchGroup(GrISE),Labora-
torio Industrial en IngenierÃ­a del Software EmpÃ­rica (LI2SE) and
SENESCYT.
REFERENCES
[1]Saba Alimadadi, Sheldon Sequeira, Ali Mesbah, and Karthik Pattabiraman. 2014.
Understanding JavaScript event-based interactions. In Proceedings of the 36th
International Conference on Software Engineering. ACM, 367â€“377.
[2]Douglas G Altman.1998. Statistical reviewing for medical journals. Statistics in
medicine 17, 23 (1998), 2661â€“2674.
[3]Paul V Anderson, Sarah Heckman, Mladen Vouk, David Wright, Michael Carter,
JanetEBurge,andGeraldCGannod.2015. CS/SEinstructorscanimprovestudent
writing without reducing class time devoted to technical content: experimental
results.In Proceedingsofthe37thInternationalConferenceonSoftwareEngineering-
Volume 2. IEEE Press, 455â€“464.
[4]AndreaArcuriandLionelBriand.2014. AHitchhikerâ€™sguidetostatisticaltests
forassessingrandomizedalgorithmsinsoftwareengineering. SoftwareTesting,
VerificationandReliability 24,3(2014),219â€“250. https://doi.org/10.1002/stvr.1486
[5]Marjan Bakker and Jelte M Wicherts. 2011. The (mis) reporting of statistical
results in psychology journals. Behavior Research Methods 43, 3 (2011), 666â€“678.
[6]Kirk R Baumgardner. 1997. A review of key research design and statistical
analysis issues. Oral Surgery, Oral Medicine,Oral Pathology, Oral Radiology, and
Endodontology 84, 5 (1997), 550â€“556.
[7]GabrieleBavota,BogdanDit,RoccoOliveto,MassimilianoDiPenta,DenysPoshy-
vanyk,andAndreaDeLucia.2013. Anempiricalstudyonthedevelopersâ€™per-
ception of software coupling. In Proceedings of the 2013 International Conference
on Software Engineering. IEEE Press, 692â€“701.
[8]A Bhatt. 2010. Evolution of Clinical Research: A History Before and Beyond
James Lind. Perspectives in Clinical Research 1, 1 (March 2010), 6â€“10.
[9]ChristianBird,NachiappanNagappan,PremkumarDevanbu,HaraldGall,and
BrendanMurphy.2009. Doesdistributeddevelopmentaffectsoftwarequality?:
an empirical case study of windows vista. Commun. ACM 52, 8 (2009), 85â€“93.
[10]MarcBranch.2014. Malignantsideeffectsofnull-hypothesissignificancetesting.
Theory & Psychology 24, 2 (2014), 256â€“277.
[11]Virginia Braun and Victoria Clarke.2006. Using thematic analysis in psychology.
Qualitative research in psychology 3, 2 (2006), 77â€“101.
[12]James K Brewer. 1985. Behavioral statistics textbooks: Source of myths and
misconceptions? Journal of Educational and Behavioral Statistics 10, 3 (1985),
252â€“268.
[13]Yan Cai and WK Chan. 2012. MagicFuzzer: scalable deadlock detection for large-
scale applications. In Proceedings of the 34th International Conference on Software
Engineering. IEEE Press, 606â€“616.
[14] Mariano Ceccato, Alessandro Marchetto, Leonardo Mariani, Cu D Nguyen, and
Paolo Tonella. 2012. An empirical study about the effectiveness of debugging
when random test cases are used. In Proceedings of the 34th International Confer-
ence on Software Engineering. IEEE Press, 452â€“462.
[15]Chris Chambers, Marcus Munafo, and more than 80 signatories. 2013. Trust
insciencewouldbeimprovedbystudypre-registration. TheGuardian,5June
2013. Available: https://www.theguardian.com/science/blog/2013/jun/05/trust-in-
science-study-pre-registration [Last accessed: 16 August 2017]. (2013).
[16]Hyun-ChulChoandShuzoAbe.2013.Istwo-tailedtestingfordirectionalresearch
hypothesestestslegitimate? JournalofBusinessResearch 66,9(2013),1261â€“1266.
[17]Ilinca Ciupa, Andreas Leitner, Manuel Oriol, and Bertrand Meyer. 2008. ARTOO:
adaptive random testing for object-oriented software. In Proceedings of the 30th
international conference on Software engineering. ACM, 71â€“80.
[18]JamesClauseandAlessandroOrso.2010. LEAKPOINT:pinpointingthecausesofmemoryleaks.In Proceedingsofthe32ndACM/IEEEInternationalConference
on Software Engineering-Volume 1. ACM, 515â€“524.
[19]Steve Cohen, George Smith, Richard A Chechile, Glen Burns, and Frank Tsai.
1996. Identifying impediments to learning probability and statistics from an
assessment of instructional software. Journal of Educational and Behavioral
Statistics 21, 1 (1996), 35â€“54.
[20]Lucas Cordeiro and Bernd Fischer. 2011. Verifying multi-threaded software
using smt-based context-bounded model checking. In Proceedings of the 33rd
International Conference on Software Engineering. ACM, 331â€“340.
[21]JohnWCreswell.2002. Educationalresearch:Planning,conducting,andevaluating
quantitative. Prentice Hall.
[22]DanielaSCruzesandToreDybÃ¥.2011.Recommendedstepsforthematicsynthesis
in software engineering. In Empirical Software Engineering and Measurement
(ESEM), 2011 International Symposium on. IEEE, 275â€“284.[23]ToreDybÃ¥,VigdisByKampenes,andDagIKSjÃ¸berg.2006.Asystematicreviewof
statistical power in software engineering experiments. Information and Software
Technology 48, 8 (2006), 745â€“755.
[24]Stefan Endrikat, Stefan Hanenberg, Romain Robbes, and Andreas Stefik. 2014.
How do api documentation and static typing affect api usability?. In Proceedings
of the 36th International Conference on Software Engineering. ACM, 632â€“642.
[25]Ilker Ercan, Yaning Yang, Guven Ã–zkaya, Sengul Cangur, Bulent Ediz, Ismet Kan,
et al. 2008. Misusage of statistics in medical research. (2008).
[26]Filomena Ferrucci, Mark Harman, Jian Ren, and Federica Sarro. 2013. Not going
totakethisanymore:multi-objectiveovertimeplanningforsoftwareengineering
projects. In Proceedings of the 2013 International Conference on Software Engineer-
ing. IEEE Press, 462â€“471.
[27]Joseph L Fleiss,Bruce Levin, and MyungheeCho Paik. 2013. Statistical methods
for rates and proportions. John Wiley & Sons.
[28]Christine A Franklin. 2007. Guidelines for assessment and instruction in sta-
tisticseducation(GAISE)report:Apre-Kâ€“12curriculumframework.American
Statistical Association.
[29]PhillipIGoodandJamesWHardin.2012. Commonerrorsinstatistics(andhow
to avoid them). John Wiley & Sons.
[30]SheilaMGore,IanGJones,andEilifCRytter.1977. Misuseofstatisticalmethods:
criticalassessmentofarticlesinBMJfromJanuarytoMarch1976. BMJ1,6053
(1977), 85â€“87.
[31]K.L. Gwet. 2014. Handbook of Inter-Rater Reliability. The Definitive Guide to
Measuring the Extent of Agreement Among Raters (4 ed.). Advanced Analytics,
LLC.
[32]M Sayeed Haque and Sanju George. 2007. Use of statistics in the Psychiatric
Bulletin: author guidelines. The Psychiatrist 31, 7 (2007), 265â€“267.
[33]Hwa-YouHsuandAlessandroOrso.2009. MINTS:Ageneralframeworkandtool
for supporting test-suite minimization. In Software Engineering, 2009. ICSE 2009.
IEEE 31st International Conference on. IEEE, 419â€“429.
[34] Schuyler W Huck. 2009. Statistical misconceptions. Routledge.
[35]John P.A. Ioannidis. 2005. Why most published research findings are false. PLoS
Medicine 2, 8 (2005), 696â€“701. https://doi.org/10.1002/stvr.1486
[36]DavidSJanzen,JohnClements,andMichaelHilton.2013. Anevaluationofinter-
activetest-drivenlabswithWebIDEinCS0.In Proceedingsofthe2013International
Conference on Software Engineering. IEEE Press, 1090â€“1098.
[37]LingxiaoJiang,GhassanMisherghi,ZhendongSu,andStephaneGlondu.2007.
Deckard:Scalableandaccuratetree-baseddetectionofcodeclones.In Proceedings
of the 29th international conference on Software Engineering. IEEE Computer
Society, 96â€“105.
[38]Magne JÃ¸rgensen, Tore DybÃ¥, Knut LiestÃ¸l, and Dag IK SjÃ¸berg. 2016. Incorrect
results in software engineering experiments: How to improve research practices.
Journal of Systems and Software 116 (2016), 133â€“145.
[39]Carol Kilkenny, Nick Parsons, Ed Kadyszewski, Michael FW Festing, Innes CCuthill, Derek Fry, Jane Hutton, and Douglas G Altman. 2009. Survey of the
qualityofexperimentaldesign,statisticalanalysisandreportingofresearchusing
animals. PloS one4, 11 (2009), e7824.
[40]Andrew King, Sam Procter, Dan Andresen, John Hatcliff, Steve Warren, William
Spees, Raoul Jetley, Paul Jones, and Sandy Weininger. 2009. An open test bed for
medicaldeviceintegrationandcoordination.In SoftwareEngineering-Companion
Volume, 2009. ICSE-Companion 2009. 31st International Conference on. IEEE, 141â€“
151.
[41]B. Kitchenham, J. Fry, and S. Linkman. 2003. The case against cross-over designs
insoftwareengineering.In SoftwareTechnologyandEngineeringPractice,2003.
Eleventh Annual International Workshop on. 65â€“67.
[42]BarbaraKitchenham,LechMadeyski,DavidBudgen,JackyKeung,PearlBrereton,
StuartCharters,ShirleyGibbs,andAmnartPohthong.2016. RobustStatistical
Methods for Empirical Software Engineering. Empirical Software Engineering
(2016), 1â€“52. https://doi.org/10.1007/s10664-016-9437-5
[43]Fredrik Kjolstad, Danny Dig, Gabriel Acevedo, and Marc Snir. 2011. Transforma-
tion for class immutability. In Proceedings of the 33rd International Conference on
Software Engineering. ACM, 61â€“70.
[44]Christian FJ Lange and Michel RV Chaudron. 2006. Effects of defects in UML
models:anexperimentalinvestigation.In Proceedingsofthe28thinternational
conference on Software engineering. ACM, 401â€“411.
[45]OtÃ¡vioAugustoLazzariniLemos,FabianoCutigiFerrari,FÃ¡bioFagundesSilveira,
and Alessandro Garcia. 2012. Development of auxiliary functions: should you be
agile? an empirical assessment of pair programming and test-first programming.
InProceedings of the 34th International Conference on Software Engineering. IEEE
Press, 529â€“539.
[46]RupakMajumdarandKoushikSen.2007. Hybridconcolictesting.In Software
Engineering, 2007. ICSE 2007. 29th International Conference on. IEEE, 416â€“426.
[47]David Mandelin, Doug Kimelman, and Daniel Yellin. 2006. A Bayesian approach
to diagram matching with application to architectural models. In Proceedings of
the 28th international conference on Software engineering. ACM, 222â€“231.
[48]MikaVMÃ¤ntylÃ¤,KaiPetersen,TimoOALehtinen,andCasperLassenius.2014.
Time pressure: a controlled experiment of test case development and require-
ments review. In Proceedings of the 36th International Conference on Software
1204
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Statistical Errors in Software Engineering Experiments ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
Engineering. ACM, 83â€“94.
[49]CollinMcMillan,MarkGrechanik,DenysPoshyvanyk,QingXie,andChenFu.
2011. Portfolio:finding relevantfunctions andtheir usage.In Proceedingsof the
33rd International Conference on Software Engineering. ACM, 111â€“120.
[50]Lijun Mei, WK Chan, and TH Tse. 2008. Data flow testing of service-oriented
workflow applications. In Proceedings of the 30th international conference on
Software engineering. ACM, 371â€“380.
[51]Habshah Midi, AHM Rahmatullah Imon, and Azmi Jaafar. 2012. The Misconcep-
tionsofSomeStatisticalTechniquesInResearch. JurnalTeknologi 47,1(2012),
21â€“36.
[52]James Miller. 1999. Can results from software engineering experiments be safely
combined?. In Software Metrics Symposium, 1999. Proceedings. Sixth International.
IEEE, 152â€“158.
[53]Rahul Mohanani, Paul Ralph, and Ben Shreeve. 2014. Requirements fixation. In
Proceedingsofthe36thInternationalConferenceonSoftwareEngineering.ACM,
895â€“906.
[54]Sebastian C MÃ¼ller and Thomas Fritz. 2015. Stuck and frustrated or in flow and
happy:Sensingdevelopersâ€™emotionsandprogress.In SoftwareEngineering(ICSE),
2015 IEEE/ACM 37th IEEE International Conference on, Vol. 1. IEEE, 688â€“699.
[55]Marcus R MunafÃ², Brian A Nosek, Dorothy VM Bishop, Katherine S Button,
Christopher D Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wa-
genmakers, Jennifer J Ware, and John PA Ioannidis. 2017. A manifesto for
reproducible science. Nature Human Behaviour 1 (2017), 0021.
[56]Noboru Nakamichi, Kazuyuki Shima, Makoto Sakai, and Ken-ichi Matsumoto.
2006.Detectinglowusabilitywebpagesusingquantitativedataofusersâ€™behavior.
InProceedings of the 28th international conference on Software engineering. ACM,
569â€“576.
[57]THNg,ShingChiCheung,WKChan,andYuen-TakYu.2007. Domaintainersuti-
lize deployed design patterns effectively?. In Proceedings of the 29th international
conference on Software Engineering. IEEE Computer Society, 168â€“177.
[58]Raymond S Nickerson. 2000. Null hypothesis significance testing: a review of an
old and continuing controversy. Psychological methods 5, 2 (2000), 241.
[59]Adrian Nistor, Qingzhou Luo, Michael Pradel, Thomas R Gross, and Darko Mari-
nov.2012. Ballerina:Automaticgenerationandclusteringofefficientrandomunit
testsformultithreadedcode.In Proceedingsofthe34thInternationalConference
on Software Engineering. IEEE Press, 727â€“737.
[60]AdityaVNoriandSriramKRajamani.2010. Anempiricalstudyofoptimizations
inYOGI.In Proceedingsofthe32ndACM/IEEEInternationalConferenceonSoftware
Engineering-Volume 1. ACM, 355â€“364.
[61]RenatoNovais,CamilaNunes,CaioLima,ElderCirilo,FranciscoDantas,Alessan-
dro Garcia, and Manoel MendonÃ§a. 2012. On the proactive and interactive
visualizationforfeatureevolutioncomprehension:Anindustrialinvestigation.
InProceedings of the 34th International Conference on Software Engineering. IEEE
Press, 1044â€“1053.
[62] Regina Nuzzo et al. 2014. Statistical errors. Nature506, 7487 (2014), 150â€“152.
[63]Cara H Olsen. 2003. Review of the use of statistics in infection and immunity.
Infection and immunity 71, 12 (2003), 6689â€“6692.
[64]SangminPark,RichardWVuduc,andMaryJeanHarrold.2010. Falcon:faultlocal-izationinconcurrentprograms.In Proceedingsofthe32ndACM/IEEEInternational
Conference on Software Engineering-Volume 1. ACM, 245â€“254.
[65]Fayola Peters, Tim Menzies, and Lucas Layman. 2015. LACE2: Better privacy-preserving data sharing for cross project defect prediction. In Proceedings of
the37thInternationalConferenceonSoftwareEngineering-Volume1.IEEEPress,
801â€“811.
[66]YuhuaQi,XiaoguangMao,YanLei,ZiyingDai,andChengsongWang.2014. The
strengthofrandomsearchonautomatedprogramrepair.In Proceedingsofthe
36th International Conference on Software Engineering. ACM, 254â€“265.
[67] Steven P Reiss. 2008. Tracking source locations. In Proceedings of the 30th inter-
national conference on Software engineering. ACM, 11â€“20.
[68]Filippo Ricca, Massimiliano Di Penta, Marco Torchiano, Paolo Tonella, and Mari-
ano Ceccato. 2007. The role of experience and ability in comprehension tasks
supported by UML stereotypes. In ICSE, Vol. 7. 375â€“384.
[69]PaigeRodeghero,CollinMcMillan,PaulWMcBurney,NigelBosch,andSidney
Dâ€™Mello. 2014. Improving automated source code summarization via an eye-
trackingstudyofprogrammers.In Proceedingsofthe36thInternationalConference
on Software Engineering. ACM, 390â€“401.
[70]Norsaremah Salleh, Emilia Mendes, John Grundy, and Giles St J Burch. 2010. An
empiricalstudyoftheeffectsofconscientiousnessinpairprogrammingusingthe
five-factor personality model. In Proceedings of the 32nd ACM/IEEE International
Conference on Software Engineering-Volume 1. ACM, 577â€“586.
[71]JesperWSchneider.2015. Nullhypothesissignificancetests.Amix-upoftwo
differenttheories:thebasisforwidespreadconfusionandnumerousmisinterpre-
tations.Scientometrics 102, 1 (2015), 411â€“432.
[72]KennethFSchulz,DouglasGAltman,andDavidMoher.2010. CONSORT2010
statement: updated guidelines for reporting parallel group randomised trials.
BMC medicine 8, 1 (2010), 18.
[73]Janet Siegmund, Christian KÃ¤stner, Sven Apel, Chris Parnin, Anja Bethmann,
Thomas Leich, Gunter Saake, and AndrÃ© Brechmann. 2014. Understandingunderstanding source code with functional magnetic resonance imaging. In
Proceedingsofthe36thInternationalConferenceonSoftwareEngineering.ACM,
378â€“389.
[74]Janice Singer. 1999. Using theAmerican Psychological Association (APA) style
guidelinestoreportexperimentalresults.In Proceedingsofworkshoponempirical
studies in software maintenance. 71â€“75.
[75]Ana Elisa Castro Sotos, Stijn Vanhoof, Wim Van den Noortgate, and Patrick
Onghena. 2007. Students misconceptions of statistical inference: A review of the
empiricalevidencefromresearchonstatisticseducation. EducationalResearch
Review2, 2 (2007), 98â€“113.
[76]Matt Staats, Gregory Gay, and Mats PE Heimdahl. 2012. Automated oraclecreation support, or: how I learned to stop worrying about fault propagation
and love mutation testing. In Proceedings of the 34th International Conference on
Software Engineering. IEEE Press, 870â€“880.
[77]DenesSzucsandJohnIoannidis.2017. Whennullhypothesissignificancetesting
isunsuitableforresearch:areassessment. FrontiersinHumanNeuroscience 11
(2017), 390.
[78]Jianbin Tan, George S Avrunin, and Lori A Clarke. 2006. Managing space forfinite-state verification. In Proceedings of the 28th international conference on
Software engineering. ACM, 152â€“161.
[79]Shin Hwei Tan and Abhik Roychoudhury. 2015. relifix: Automated repair of soft-
ware regressions. In Proceedings of the 37th InternationalConference on Software
Engineering-Volume 1. IEEE Press, 471â€“482.
[80]MatthewThompson,ArpitaTiwari,RongweiFu,EstherMoe,andDavidIBuckley.
2012. A Framework To Facilitate the Use of Systematic Reviews and Meta-
Analyses in the Design of Primary Research Studies. (2012).
[81]S.Vegas,C.Apa,andN.Juristo.2016. CrossoverDesignsinSoftwareEngineeringExperiments:BenefitsandPerils. IEEETransactionsonSoftwareEngineering 42,2
(February 2016), 120â€“135.
[82]Andrew Vickers. 2010. What is a P-value anyway?: 34 stories to help you actually
understand statistics. Addison-Wesley Longman.
[83]Gerald E Welch and Steven G Gabbe. 1996. Review of statistics usage in the
AmericanJournalofObstetricsandGynecology. Americanjournalofobstetrics
and gynecology 175, 5 (1996), 1138â€“1141.
[84]RichardWettel,MicheleLanza,andRomainRobbes.2011. Softwaresystemsas
cities:Acontrolledexperiment.In Proceedingsofthe33rdInternationalConference
on Software Engineering. ACM, 551â€“560.
[85]Michael W Whalen, Suzette Person, Neha Rungta, Matt Staats, and DanielaGrijincu. 2015. A flexible and non-intrusive approach for computing complex
structuralcoveragemetrics.In Proceedingsofthe37thInternationalConference
on Software Engineering-Volume 1. IEEE Press, 506â€“516.
[86]Stefan Winter, Oliver Schwahn, Roberto Natella, Neeraj Suri, and DomenicoCotroneo. 2015. No PAIN, no gain?: the utility of PArallel fault INjections. In
Proceedings of the 37th International Conference on Software Engineering-Volume1. IEEE Press, 494â€“505.
[87]
ChangXu,Shing-ChiCheung,andWing-KwongChan.2006. Incrementalcon-
sistencycheckingforpervasivecontext. In Proceedingsofthe28thinternational
conference on Software engineering. ACM, 292â€“301.
[88]KoenYskout,RiccardoScandariato,andWouterJoosen.2012. Doesorganizing
security patterns focus architectural choices?. In Proceedings of the 34th Interna-
tional Conference on Software Engineering. IEEE Press, 617â€“627.
[89]KoenYskout,RiccardoScandariato,andWouterJoosen.2015.Dosecuritypatterns
really help designers?. In Software Engineering (ICSE), 2015 IEEE/ACM 37th IEEE
International Conference on, Vol. 1. IEEE, 292â€“302.
[90]Yanbing Yu, James A Jones, and Mary Jean Harrold. 2008. An empirical study of
the effects of test-suite reduction on fault localization. In Proceedings of the 30th
international conference on Software engineering. ACM, 201â€“210.
[91]Carmen Zannier, Grigori Melnik, and Frank Maurer. 2006. On the success ofempirical studies in the international conference on software engineering. InProceedings of the 28th international conference on Software engineering. ACM,
341â€“350.
[92]Fadi Zaraket, Adnan Aziz, and Sarfraz Khurshid. 2007. Sequential circuits forrelationalanalysis.In SoftwareEngineering,2007.ICSE2007.29thInternational
Conference on. IEEE, 13â€“22.
[93]Dina Zayan, MichaÅ‚ Antkiewicz, and Krzysztof Czarnecki. 2014. Effects of us-
ingexamplesonstructuralmodelcomprehension:acontrolledexperiment.InProceedingsofthe36thInternationalConferenceonSoftwareEngineering.ACM,
955â€“966.
[94]Lingming Zhang, Dan Hao, Lu Zhang, Gregg Rothermel, and Hong Mei. 2013.
Bridging the gap between the total and additional test-case prioritization strate-
gies. InProceedings of the 2013 International Conference on Software Engineering.
IEEE Press, 192â€“201.
1205
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Reyes et al.
Table 7: Problems found in standaline experiments and â€experiments as evaluationsâ€ published in ICSE between 2006-2015
Empirical Studies CodeGoal definition Study design Analysis Reporting
(Q1)Null
hipothesis(Q2)Sample size
calculation(Q3)Random
sampling(Q4)Randomassignment(Q5)Assumptions (Q6)Model
definition(Q8)Post-hoc
power(Q9)Multiple
testing(Q10)Means
Standalone Experiments2006-EX01 [56] No/Yes No No Yes No N/A No No Yes
2006-EX02 [44] Yes/Yes No Yes No Yes N/A Yes No Yes
2007-EX03 [57] Yes/No No No No No N/A Yes No Yes
2007-EX04 [68] Yes/Yes No No Yes No N/A Yes No Yes
2008-EX05 [90] No/No No Yes Yes N/A N/A N/A N/A Yes
2010-EX06 [70] Yes/Yes No No Yes No N/A Yes No Yes
2011-EX07 [84] Yes/Yes No No Yes Yes N/A Yes No Yes
2012-EX08 [14] Yes/Yes No No Yes Yes N/A Yes No Yes
2012-EX09 [88] Yes/Yes No No Yes Yes N/A Yes No Yes
2012-EX10 [61] Yes/Yes No No No Yes N/A Yes N/A Yes
2012-EX11 [45] Yes/Yes No No Yes Yes N/A Yes No Yes
2013-EX12 [7] No/No No No No Yes N/A Yes Yes Yes
2014-EX13 [93] Yes/Yes No No Yes Yes N/A Yes No Yes
2014-EX14 [48] Yes/Yes No Yes Yes Yes N/A Yes No Yes
2014-EX15 [73] No/No No Yes No No N/A Yes N/A N/A
2014-EX16 [24] No/No No No Yes Yes N/A Yes No Yes
2014-EX17 [53] Yes/Yes No Yes Yes Yes N/A Yes N/A Yes
2015-EX18 [86] Yes/Yes No Yes N/A No N/A Yes Yes Yes
2015-EX19 [89] Yes/No No No Yes Yes N/A Yes No Yes
2015-EX20 [54] No/No No No Yes Yes Yes No No Yes
2015-EX21 [3] No/Yes No No No No N/A Yes No Yes
â€Experiments as Evaluationsâ€2006-CM01 [78] No/No No No N/A N/A N/A N/A N/A N/A
2006-CM02 [87] No/No No No N/A N/A N/A N/A N/A No
2006-CM03 [47] No/No No No Yes N/A N/A N/A N/A No
2007-CM04 [92] No/No No No N/A N/A N/A N/A N/A Yes
2007-CM05 [37] No/No No No N/A N/A N/A N/A N/A N/A
2007-CM06 [46] No/No No No N/A N/A N/A N/A N/A Yes
2008-CM07 [50] No/No No Yes N/A N/A N/A N/A N/A Yes
2008-CM08 [67] No/No No No N/A N/A N/A N/A N/A Yes
2008-CM09 [17] No/No No No N/A N/A N/A N/A N/A Yes
2009-CM10 [40] No/No No No N/A N/A N/A N/A N/A Yes
2009-CM11 [33] No/No No No N/A N/A N/A N/A N/A Yes
2009-CM12 [9] Yes/No No No N/A Yes Yes Yes No Yes
2010-CM13 [60] No/No No No N/A N/A N/A N/A N/A Yes
2010-CM14 [18] No/No No No N/A N/A N/A N/A N/A Yes
2010-CM15 [64] No/No No No N/A N/A N/A N/A N/A Yes
2011-CM16 [43] N/A/N/A No No N/A N/A N/A N/A N/A N/A
2011-CM17 [49] Yes/Yes No No Yes No N/A Yes No Yes
2011-CM18 [20] No/No No No N/A N/A N/A N/A N/A Yes
2012-CM19 [13] No/No No No N/A N/A N/A N/A N/A Yes
2012-CM20 [59] No/No No No N/A N/A N/A N/A N/A Yes
2012-CM21 [76] Yes/Yes No Yes N/A N/A N/A Yes No Yes
2013-CM22 [26] No/No No No N/A Yes N/A Yes Yes Yes
2013-CM23 [36] No/No No No Yes No N/A Yes No Yes
2013-CM24 [94] No/No No Yes N/A No N/A Yes No No
2014-CM25 [66] No/No No No N/A No N/A Yes No Yes
2014-CM26 [1] No/No No No Yes Yes N/A Yes No No
2014-CM27 [69] Yes/No Yes Yes Yes Yes N/A Yes No Yes
2015-CM28 [79] No/No No No N/A N/A N/A N/A N/A Yes
2015-CM29 [85] No/No No No Yes No N/A Yes N/A Yes
2015-CM30 [65] No/No No No N/A No N/A Yes N/A Yes
1206
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. 