Efﬁcient Test Generation Guided by
Field Coverage Criteria
Ariel Godio
Department of Software Engineering
ITBA
Buenos Aires, Argentina
agodio@itba.edu.arValeria Bengolea
Department of Computer Science
UNRC
Rio Cuarto, Argentina
vbengolea@dc.exa.unrc.edu.arPablo Ponzio
Department of Computer Science
UNRC
Rio Cuarto, Argentina
pponzio@dc.exa.unrc.edu.ar
Nazareno Aguirre
Department of Computer Science
UNRC
Rio Cuarto, Argentina
naguirre@dc.exa.unrc.edu.arMarcelo F. Frias
Department of Software Engineering
ITBA
Buenos Aires, Argentina
mfrias@itba.edu.ar
Abstract —Field-exhaustive testing is a testing criterion suitable
for object-oriented code over complex, heap-allocated, data
structures. It requires test suites to contain enough test inputs
to cover allfeasible values for the object’s ﬁelds within a certain
scope (input-size bound). While previous work shows that ﬁeld-
exhaustive suites can be automatically generated, the generation
technique required a formal speciﬁcation of the inputs that
can be subject to SA T-based analysis. Moreover, the restriction
of producing all feasible values for inputs’ ﬁelds makes test
generation costly.
In this paper, we deal with ﬁeld coverage as testing criteria
that measure the quality of a test suite in terms of coverage and
mutation score, by examining to what extent the values of inputs’
ﬁelds are covered. In particular, we consider ﬁeld coverage in
combination with test generation based on symbolic execution
to produce underapproximations of ﬁeld-exhaustive suites, using
the Symbolic Pathﬁnder tool. To underapproximate these suites
we use tranScoping , a technique that estimates characteristics of
yet to be run analyses for large scopes, based on data obtained
from analyses performed in small scopes. This provides us with
a suitable condition to prematurely stop the symbolic execution.
As we show, tranScoping different metrics regarding ﬁeld
coverage allows us to produce signiﬁcantly smaller suites using
a fraction of the generation time. All this while retaining the
effectiveness of ﬁeld exhaustive suites in terms of test suite quality.
Index T erms —Field-exhaustive testing, ﬁeld-based testing, sym-
bolic execution, transcoping
I. I NTRODUCTION
The functional correctness of software systems, i.e., the
degree to which a software system meets the purpose for
which it has been built, is among the most challenging
problems in software engineering [12], [14]. While many
techniques and methodologies have been proposed to guar-
antee software correctness, testing (consisting in evaluating
the software under analysis by executing it in a number
of cases) is by far the most widely used, product-centric,
technique for software correctness assurance [2]. An essential
part of testing is selecting the cases in which the software
under test is going to be run. This process is very costly inpractice and is performed mostly manually [2]. At the same
time, its importance is acknowledged by software development
methodologies that strongly encourage writing tests, e.g., agile
methods supporting writing tests together with (and even
before) the software that should be tested [4].
It is well known that tests are inherently incomplete as a
measure for guaranteeing functional correctness. Thus, testing
software appropriately , i.e., attempting to cover as much as
possible of software functionality with a ﬁnite (and limited)
set of test cases, is crucial. Different coverage criteria , i.e.,
constraints on how a set of tests should exercise software,
have been proposed [30], and some have been adopted by
development models and software certiﬁcations (e.g., [20]).
These constraints are in many cases very difﬁcult to comply
with, making manual test development even more complex
and time-consuming, especially in certain application domains.
This situation has led to the active development of automated
test generation techniques , a family of approaches that at-
tempt to produce tests automatically. These techniques vary
in their underlying automation approaches, including random
generation [6], [18], [21], generation based on constraint-
solving [1], [27], and generation based on search [5] (including
evolutionary approaches [9]). These techniques can also be
categorized as white box orblack box . White box techniques
are generally driven by code coverage, with tools based, e.g.,
in symbolic, concrete, or “concolic” execution [27], [28];
these tend to produce relatively small suites, with every test
covering exactly one path, branch, or whatever the “unit” for
coverage is driven by. Black box automated test generation
techniques, on the other hand, generally produce large suites,
as witnessed by tools based on random generation [6], [18],
[21] and constraint solving or (exhaustive) search [5], [16].
Finally, evolutionary computation [9], whose categorization
as white box or black box depends on the ﬁtness function
used, also produces large suites. This latter observation has
some negative implications. For some tools, the size of the
912019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
978-1-7281-2508-4/19/$31.00 ©2019 IEEE
DOI 10.1109/ASE.2019.00019
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. produced suites affects the efﬁciency of the technique (e.g.,
degradation in test generation using random testing, or high
cost of suite minimization in evolutionary computation based
generation). And, more importantly, large test suites imply a
high cost of test execution, and unsuitability in contexts where
suites are continuously and repeatedly executed (e.g., in test-
driven development, or continuous integration environments).
The above-mentioned problems of black-box driven test in-
put generation motivated the recent work on a testing criterion
called Field-exhaustive testing [23]. This black-box criterion is
particularly well-suited for object-oriented code over complex,
heap-allocated, data structures, and requires suites to contain
enough test inputs to cover allfeasible values for object ﬁelds
within a certain input-size bound. While the criterion comes
equipped with a technique for automatic generation of ﬁeld-
exhaustive suites, this technique requires a formal speciﬁcation
of the inputs that can be subject to SAT-based analysis [23].
Moreover, the restriction of producing allfeasible values for
object ﬁelds, makes test generation costly and ﬁeld-exhaustive
testing difﬁcult to generalize to further testing domains.
In this paper, we deal with ﬁeld coverage as testing criteria
that measure test suite quality by examining to what extent the
values of inputs’ ﬁelds are covered. Intuitively, the approach
we propose receives as input a family of sets of pairs
ﬁeld 1={/angbracketleftobj1,ﬁeldVal 1/angbracketright,/angbracketleftobj2,ﬁeldVal 2/angbracketright,...},
ﬁeld 2={/angbracketleftobj1,ﬁeldVal 3/angbracketright,/angbracketleftobj2,ﬁeldVal 4/angbracketright,...},
...
Each set (for instance set ﬁeld 1), describes a set of pairs
to cover. For example, if pair /angbracketlefto,a/angbracketright∈ ﬁeld 1, then a suite
satisfying ﬁeld coverage must contain a data structure in which
objectomust be a part, and for which o.ﬁeld 1=a.
In order to determine the pairs covered by a test suite, we
will look at inputs (heap-stored data structures), as rooted,
labeled, graphs . The graph root is the receiver object, and arcs
relating objects are labeled with ﬁeld names. In this setting, an
input is considered redundant if the relations among objects
that it establishes have all been covered by previous inputs.
Figure 1 presents an example using three different binary trees.
Nodes are indexed T0,N0,N1andN2. Above the trees, we
include the relations between nodes that each tree establishes,
partitioned according to ﬁeld names. The grayed out arcs and
pairs between objects correspond to those covered by previous
inputs (considering a left to right generation order).
We show that this notion generalizes ﬁeld-exhaustive test-
ing, withdrawing the need for a SAT-analyzable formal spec-
iﬁcation, and thus can be combined with any test generation
technique to produce smaller test suites. In particular, we
consider ﬁeld coverage in combination with test input gener-
ation based on symbolic execution of repOk() procedures that
code data structure representation invariants. Since symbolic
execution may be expensive, we will produce underapproxi-
mations of adequate test suites using the Symbolic Pathﬁnder
tool. To underapproximate these suites we use tranScoping
[[left]] = {<N0,N1>, <N1,null>, <N2,null>}
[[right]] = {<N0,N2>, <N1,null>, <N2,null>}[[root]] = { <T0,N0> }
T0
N0
N1 N2
null null null nullroot
left rightT0
N0
null nullroot
left right[[left]] = {<N0,null>}
[[right]] = {<N0,null>}[[root]] = {<T0,N0>}
[[left]] = { <N0,N1>, <N1,null> }
[[right]] = { <N0,null>, <N1,null> }[[root]] = { <T0,N0> }
T0
N0
N1
nullnull
nullroot
left right
Fig. 1. Viewing heap-stored data structures as rooted, labeled, graphs.
[24], a technique that estimates characteristics of yet to be
run analyses for large scopes, based on data obtained from
analyses performed in small scopes. This imposes a suitable
condition to prematurely stop symbolic execution. As we
show, tranScoping different metrics regarding ﬁeld coverage
helps us produce signiﬁcantly smaller suites using a fraction of
the generation time. All this while retaining the effectiveness
of ﬁeld exhaustive suites, in terms of test suite quality.
We show that this criterion, when combined with test gener-
ation based on symbolic execution and tranScoping, improves
testing as follows:
•It produces test suites that are well suited for testing
programs with complex, heap-allocated, inputs.
•Signiﬁcantly improves the time of the corresponding test
generation technique, when compared to the generation
of ﬁeld-exhaustive test suites. The experiments we will
report in Section IV show, for the TreeSet subject, a
speedup of 167X.
•It produces test suites whose bug-detection ability is com-
parable to that of ﬁeld-exhaustive suites while improv-
ing testing efﬁciency with its signiﬁcantly fewer tests.
Moreover, if we approximate bug-detection by measuring
branch coverage and mutation score, for the TreeSet
subject in Section IV we achieve the same coverage
and mutation score is slightly smaller (72.4% instead of
72.6%).
The paper is organized as follows. In Section II we introduce
ﬁeld-coverage criteria as a class of coverage criteria and focus
on ﬁeld-exhaustive suites [23] and the weaker versions to be
discussed in this paper. In Section III we show that suites
satisfying ﬁeld-coverage can be automatically generated using
symbolic execution. In Section IV we present experimental
results that support the applicability of the presented criteria
for bug-ﬁnding. Finally, in Section VI we present some
concluding remarks on this work.
II. F IELD -COVERAGE TESTING CRITERIA
Testing criteria deﬁne testing standards that indicate how
thoroughly a piece of code is being examined by test cases.
Moreover, when testing criteria are selected as quality targets,
they serve as a measure for testing engineers on how to
proceed to test a given piece of code. Engineers typically
92
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. design testing criteria that, when met, have as a consequence
good testing properties. For instance, a suite that fails to
execute a certain program statement cannot detect failures
originating in that statement; statement coverage is a testing
criterion that attempts to avoid such omissions by prescribing
that one should include test cases that exercise allthe exe-
cutable statements in the program under test. Notice also that
certain criteria help to build test suites able to detect certain
classes of errors. For instance, multiple condition coverage
takes into consideration all possible combinations of outcomes
in conditions within program decision points, thus aiming
at logical errors or oversights in those points, affecting the
desired program control ﬂow.
Field coverage criteria aim at detecting errors in code oper-
ating on data structures, as for instance those arising in code
that handles heap-allocated objects with a complex structure,
or that operate on heavily constrained memory heaps. In this
section, we will provide a deﬁnition of the memory heaps
that programs under test are going to handle, and afterward
introduce ﬁeld coverage criteria.
Let
C/primem(arg1:C1,..., argk:Ck)
be the method under test, declared in class C, where
arg1,..., argkare the formal parameters for m,C1,..., Ck
their respective types and C/primethe return type of m.
Deﬁnition 1: An input memory heap for method m
(or simply, memory heap , when the method under anal-
ysis is clear from the context), is a labeled graph
/angbracketleftV,E,L,r,p 1,...,p k,s1,...,s n/angbracketrightwhere:
•Vis a set of objects and values, containing the value
null. Objects are typed, and a single object may have
more than one type.
•Eis a set of labeled edges between members of V.L,
the labeling function, assigns class ﬁeld names to edges.
L(o1→o2)=f⇐⇒o1.f=o2. We assume the
labelling function respects the heap typing.
•r∈Vis the receiver object, from class C.
•p1,...,p k∈Vare the actual parameters for m.
•s1,...,s n∈Vare the static objects/values declared in
class C.
Before deﬁning ﬁeld coverage criteria, we need to introduce
some notation about the semantics we will use. Intuitively, the
semantics for a class Cis given by the set of those objects from
the memory heap with type C. For a ﬁeld f:C/primedeclared in a
class C, the semantics of fare those pairs from the edge-set
whose label is f.
Deﬁnition 2: LetH=/angbracketleftV,E,L,r,p 1,...,p k,s1,...,s n/angbracketright
be a memory heap. Let C1,..., Cjbe the classes for objects
inV. We deﬁne the semantics of class Ci(1≤i≤j)b y
Ci={v∈V:vhas type Ci}. Let class Ci(1≤i≤j)
include a ﬁeld f:C/prime. The semantics of ﬁeld finH, denoted
by[[f]]H, is the function f:Ci→(C/prime∪{null})deﬁned by
[[f]]H={/angbracketlefto1,o2/angbracketright∈E:L(/angbracketlefto1,o2/angbracketright)=f}.
A memory heap models a single test input. We will extend
the notion of class ﬁeld semantics to sets of memory heaps.Intuitively, we will join the semantics of all ﬁelds over all the
heaps, into a single set of labeled pairs.
Deﬁnition 3: Given a test suite (set of memory heaps) H=
{H1,...,H n}and a ﬁeld f, the semantics of finH, denoted
by[[f]]H, is deﬁned as/uniontext
H∈H[[f]]H. Let F={f1,...fk}be
the set of ﬁelds in the class hierarchy for H. We deﬁne the
class ﬁeld semantics of suiteH, denoted by SH,b ySH=/uniontext
f∈F[[f]]H.
Deﬁnition 4 below introduces ﬁeld coverage criteria. In-
tuitively, any criterion based on populating the semantics of
class ﬁelds (i.e., pairs /angbracketlefto1,o2/angbracketrightlabeled with the name of the
ﬁeld they are expected to belong to) in some speciﬁc way, is a
ﬁeld coverage criterion. A suite will satisfy the criterion if the
semantics of class ﬁelds, as collected from the heap objects
of the suite, cover ﬁeld values as speciﬁed by the criterion.
Deﬁnition 4: ‘Let C1,..., Cnbe a class hierarchy for a
methodmunder test, i.e., method mmay be executed in
memory heaps whose objects are typed with types C1,..., Cn.
LetSbe a labeled set of object pairs, i.e., Sis expected to be
of the form S={/angbracketleftl1,r1/angbracketright,...,/angbracketleftlk,rk/angbracketright}satisfying, for each
1≤j≤k,
•/angbracketleftlj,rj/angbracketrightis labeled with a ﬁeld fdeclared in C1,..., Cn,
•/angbracketleftlj,rj/angbracketrightis correctly typed as per the labeling, i.e., if pair
/angbracketleftlj,rj/angbracketrightis labeled f, ﬁeld fis declared in class Cand has
type C/prime, thenljhas type Candrjhas type C/prime.
Field-coverage criterion FCC Sis satisﬁed by a test suite
Tiff the class ﬁeld semantics (see Def. 3) STof heaps in T
satisﬁesST⊇S.
Figure 2 presents an example. Method removeMin in class
BinSearchTree removes the BinSearchTreeNode holding the
least value in the binary search tree. Field coverage criteria are
black-box, and therefore, as the example shows, we only need
to know the interface of the method under test. The example
presents a set S0that needs to be fully covered, i.e., the
ﬁeld-coverage criterion is FCC S0. The three memory heaps
induced by the given trees (let us call these memory heaps H)
cover set S0and therefore are a valid suite according to this
testing criterion FCC S0. Another example, this time given
by comprehension, is non-null ﬁeld coverage ; this criterion
establishes that each ﬁeld semantics must contain at least
one pair whose second component is non-null. Any suite
containing the second or third tree in Fig. 2 would satisfy
this criterion, whereas a suite containing only the ﬁrst one
would not.
It is important to notice that memory heaps as we have
described them require objects to be somehow identiﬁed since
objects are the vertices of heap graphs. Selecting appropriate
identiﬁers is important, and may be related to the programming
language in which the software under test is provided. Lan-
guages in which the speciﬁc memory address where an object
is stored is relevant (in particular, languages supporting pointer
arithmetic) may lead one to choose object memory addresses
as object identiﬁers. Notice that choosing such a concrete
notion of object identiﬁer can have a signiﬁcant impact in how
ﬁeld-coverage criteria are described, as well as in how difﬁcult
it may be to guarantee that certain ﬁeld-coverage criteria are
93
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. class BinSearchTree { 
  root : BinSearchTreeNode;
  
  // invariant: binSearchTreeRepOK();
  
  // requires: root != null;
  BinSearchTreeNode removeMin(){
     . . . 
  }
  boolean binSearchTreeRepOK(){
     . . . 
  }
}
class BinSearchTreeNode {
  left : BinSearchTreeNode;
  value : int;
  right : BinSearchTreeNode;
}S0,left= {<N0,N1>, <N1,N3>, <N2,N3>}
0,rightS = {<N0,N2>, <N1,null>, <N2,N4>}0,rootS = {<T0,N0>}
T0
N0
null nullroot
left rightT0
N0
N1 N2
null null nullN3
null nullroot
left rightT0
N0
N1 N2
null nullN3
null nullN4
null nullroot
left right
Fig. 2. Suite based on ﬁeld coverage: an example.
T0
N0root
left right
N1
null nullN2
null nullT0
N1root
left right
N3
null nullN2
null nullT0
N2root
left right
N3
null nullN4
null null
Fig. 3. Achieving ﬁeld-base coverage by permuting object identiﬁers.
met. In this paper, we use a more abstract notion of object
identiﬁer, based simply on the position of each object in the
breadth-ﬁrst traversal of the heap. This is, in fact, the object
identiﬁer notion employed in Fig. 2 (where the order is given
on a per-type basis). This choice not only simpliﬁes in our
case the deﬁnition of ﬁeld coverage criteria; it also allows us
to deal with symmetry breaking . Symmetry breaking requires
canonical representations of memory heaps, thus guaranteeing
that, if two heaps differ in their canonical representations, then
they are non-isomorphic, or equivalently, if two memory heaps
are isomorphic, their canonical representations will be the
same. Symmetry breaking is important in test generation since
it removes redundant cases, and mechanisms to guarantee
it are available as part of various test generation tools. For
example, Symbolic PathFinder [28] uses Lazy Initialization
[15] to explore memory heaps, and in the process it removes
symmetries. Similarly, Korat [5] generates non-isomorphic
objects. TACO [10] uses automatically generated symmetry
breaking axioms to remove symmetries. Notice that, if iso-
morphic structures are allowed, then one may achieve better
coverage without actually considering different cases, e.g.,
the suite depicted in Fig. 3 satisﬁes FCC S0(see Fig. 2) by
permuting objects identiﬁers in only one structure.
Another example of a ﬁeld-coverage criterion is ﬁeld-
exhaustive testing [23]. In this criterion, the underlying set
Sof pairs to be covered is deﬁned as containing all pairs
that occur in valid (where validity is relative to some input’s
speciﬁcation) memory heaps, bounded in size by a user-
provided bound k. Field-exhaustive testing, as presented in
[23], requires a speciﬁcation of valid inputs, e.g., through adeclarative precondition or a declarative object invariant. How
“strong” such speciﬁcation is, has an impact on the feasible
pairs of ﬁeld semantics (since these are restricted to those
feasible in valid inputs).
Finally, let us remark that suites complying with ﬁeld-
coverage criteria do not need to be of minimal size. For
instance, notice that the ﬁrst tree in Fig. 2 may be safely
removed, and the remaining two trees still conform a valid
suite according to criterion FCC S0. Also, a ﬁeld-coverage
criterion may not lead to any valid suite, since it may require
certain infeasible ﬁeld semantics pairs to be covered. For
instance, if we add to S0,rootthe pair/angbracketleftT0,null/angbracketright, no memory
heap satisfying the method precondition ( root !=null) may
cover that pair.
III. A UTOMATED GENERATION OF FIELD -COVERAGE
SUITES
Field-coverage, as deﬁned in the previous section, is a very
broad class of black-box coverage criteria. In this section, we
will concentrate on how to automatically generate test inputs
satisfying some speciﬁc ﬁeld-coverage criteria. More precisely,
we consider the use of ﬁeld-coverage to reduce the number of
symbolic paths during test generation via symbolic execution
of arepOK() method, i.e., a routine that checks the represen-
tation invariant. In this case, the reduction is associated with
imposing an estimated metric regarding ﬁeld semantics, and
prematurely stopping the systematic visit of symbolic paths
when such metric is achieved. This approach then implies a
lossy technique , in the terminology of [29], in the sense that
it will compute an underapproximation of the set of required
test inputs. Notice in particular that this work generalizes ﬁeld-
exhaustive testing, as presented in [23] since here we do not
require formal declarative speciﬁcations to compute suites.
Moreover, we will also present a series of test generation
policies that do not demand fullﬁeld-exhaustive suites but
approximate these suites. In Section IV we will analyze the
bug-ﬁnding capabilities of suites produced following these
approaches, in comparison with the corresponding original
techniques.
A. Approaching Field-Exhaustive T esting with TranScoping
The notion of scope , as a bounding parameter, is present in
many automated analysis techniques. As a term, it originates
in the context of Alloy [13], where it indicates the maximum
number of atoms to be considered as part of signatures, thus
making decidable the satisﬁability of formulas in an undecid-
able logic (the scope makes domains ﬁnite, and thus formulas
in relational logic can be “ﬂattened” into propositional ones).
This concept is directly mapped to other analysis tools, notably
the test generation tools Korat [5] and TestEra [16], that
borrowed the concept as well as the term. Other analysis
techniques also require a scope of some kind:
•Models in model checking [8] need to be ﬁnite, and
therefore bounds that limit the number of states are
common.
94
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. •Test generation through symbolic execution, often fol-
lowing a depth-ﬁrst traversal, imposes a maximum depth
size for bounded path exploration.
•Software model checkers impose a maximum number of
iterations and recursive calls.
•Random testing bounds the length of test sequences
during test generation [21].
TranScoping [24] is a technique for estimating characteris-
tics of yet to be run analyses for large scopes, based on data
obtained from analyses performed in small scopes. Essentially,
the idea is to perform complete analyses in small scopes,
where they are still affordable, and extrapolate harvested data
to larger scopes, that can be exploited to improve analysis,
e.g., by identifying irrelevant parts of analysis that may be
avoided, or selecting values for parameters of the analysis
that showed better performance in small scopes. In this article,
we will extrapolate information regarding ﬁeld semantics, that
will allow us to stop test input generation before it would
naturally terminate, with the goal of producing test suites that
approximate ﬁeld-exhaustive ones and retain good coverage
and bug ﬁnding capabilities.
Field-exhaustive suites cover all the pairs of object identi-
ﬁers that may occur in valid (as per method preconditions,
representation invariants, and scopes) memory heaps [23].
A problem that arises when computing test inputs through
symbolic execution over an operational input speciﬁcation,
i.e.,repOK() , is that allbounded symbolic paths must be
traversed along the search of inputs that might contribute new
pairs to the ﬁeld semantics. Thus, even when our goal may be
to achieve ﬁeld-coverage, in practice one ends up generating a
suite that guarantees bounded-paths coverage. Of course, some
of the inputs generated along path traversal will not be added
to the suite because they will be deemed superﬂuous (in case
they do not contribute any new pairs to the ﬁeld semantics),
but still, the computational cost ends up being the same as
that of path coverage.
As we will discuss in Section IV, when symbolically
executing a repOK() invariant, it is possible to generate
most of the required inputs early in the symbolic execution.
Therefore, deciding a suitable condition to stop the execution
in a way that good coverage/bug-ﬁnding is achieved, will allow
us to reduce generation time considerably. We use tranScoping
for this task.
1) TranScoping Field Size: Field-exhaustive suites cover
those pairs of identiﬁers /angbracketleftid0,id1/angbracketrightfsuch that id0.f=id1
in some valid memory heap. A ﬁeld-exhaustive suite then
allows us to compute ﬁeld f’s semantics ( [[f]]), which is a set
of pairs whose size we will note as size f. Therefore, given
a class hierarchy for the method under test, from a ﬁeld-
exhaustive suite we will be able to extract ﬁeld’s semantics
[[f1]],..., [[fk]], whose sizes will be noted size f1,..., size fk,
respectively. Notice that given a scope sfor the number
of objects, these sizes will vary depending on s. There-
fore, for scopes s1,...,s nwe can obtain families of sizes
sizes1
f1,..., sizes1
fk,..., sizesn
f1,..., sizesn
fk. In this setting, we
will use the following tranScoping procedure. We will com-pute the actual sizes for scopes 1 through 5, i.e., compute the
values size1
f1,..., size1
fk,..., size5
f1,..., size5
fk. These values
are computed by generating ﬁeld-exhaustive suites for scopes
1 through 5; our rationale is that for these small scope values,
ﬁeld-exhaustive generation can be performed efﬁciently.
Algorithm 1 shows a ﬁrst approach to suite computation,
based on tranScoping size information. It receives as input an
integer smallScope , which characterizes those scopes in the
range 1 to smallScope for which ﬁeld-exhaustive suites will
be computed. Also, it receives Scope , the scope for which a
ﬁeld-coverage suite will be generated. The algorithm assumes
that class Analysis wraps a data generation tool such as
Java/Symbolic PathFinder. We also assume this tool provides
an iterator interface that allows us to ask for the “next” datum.
Class FieldSizes stores, for each ﬁeld f, the number of pairs
that the inputs visited so far contribute to [[f]]. Therefore,
lines 3–6 store in array fSizes the sizes of ﬁelds according to
ﬁeld-exhaustive suites for each of the scopes considered small ,
namely, scopes 1 to smallScope . Line 7, using an appropriate
regression function (we will discuss such functions below),
predicts the expected size of the ﬁelds’ semantics in scope
Scope . These predicted values will be used in line 11 to
characterize the termination criterion of the while loop; the
loop will iterate as long as any of the actual sizes so far stored
inﬁeldsSemantics has not reached the predicted value stored
intranscopedSizes (such is the meaning of operator /triangleleft). Inside
the while loop (lines 12–19), a new input is retrieved from
the analysis and the current semantics is updated with the aid
of function updateSemantics. In case the new input actually
contributes with new pairs to the ﬁeld semantics, the input is
stored as part of the generated suite. If there is no next input
(lines 13–15), the algorithm terminates. The underlying testing
criterion FCC Sthat Algorithm 1 satisﬁes is that the size of
[[f]]for each ﬁeld is greater than or equal to the minimum
between the actual ﬁeld semantics size and the tranScoped
ﬁeld semantics size for the given Scope . We present a running
example of Algorithm 1 further below.
We will consider three extrapolation methods in order to
infer ﬁeld sizes for larger scopes, namely,
•Using a linear regression line.
•Using a quadratic regression parabola.
•The function computed as the average of the functions
above.
We will now discuss the reasons for choosing these re-
gression functions. Linear functions usually underestimate the
number of tuples in the ﬁeld’s semantics (cf. [23]). Yet on
occasions, as may be the case for singly linked lists, they pro-
duce a good estimation. Also, even when they underestimate
the number of tuples, this is useful when we aim at obtaining
small test suites. Notice that the largest size a ﬁeld’s semantics
may have, for scope n,i sn×(n+1) (each one of the at most n
objects in the ﬁeld’s domain may point to at most nobjects or
the nullvalue). Therefore, a quadratic function may provide a
good estimation of the actual ﬁeld size. On occasions, though,
the inferred value may be too close (or even exceed) the
95
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. Algorithm 1 Field-coverage test suite generation by ﬁeld size
tranScoping.
1:function COMPUTE -SUITE (int : smallScope ,Scope )
2: Analysis analysis = new Analysis();
3: FieldSizes[] fSizes = new FieldSizes[ smallScope ];
4: for(inti=0,i<smallScope ,i++) do
5: fSizes [i] = computeFieldSizesForScope( i+1,analysis );
6: end for
7: FieldSizes tranScopedSizes = tranScopeFieldSizes( fSizes ,Scope );
8: Set /angbracketleftInput /angbracketrightsuite = new Set /angbracketleftInput /angbracketright;
9: Map /angbracketleftFieldName ,Pair /angbracketleftObject,Object /angbracketright/angbracketrightﬁeldsSemantics =
10: new Map /angbracketleftFieldName ,Pair /angbracketleftObject,Object /angbracketright/angbracketright();
11: while tranScopedSizes /triangleleftsizesOf( ﬁeldsSemantics )do
12: Input input = getNextInput( analysis );
13: if(input ==NULL )then
14: break;
15: end if
16: boolean addedPair =updateSemantics( ﬁeldSemantics ,input );
17: if(addedPair )then
18: suite .add( input );
19: end if
20: end while
21: return suite ;
22: end function
actual ﬁeld size, leading to almost no proﬁt in test generation
time, compared to exploring the whole space of alternatives.
Therefore, taking the average between the quadratic and the
linear functions leads to a (still quadratic) function that grows
slower than the original quadratic function. In Section IV we
will evaluate input generation algorithms that use tranScoping
to determine when generation must terminate, according to
each of these stopping criteria.
2) TranScoping Generation Time: An alternative to
tranScoping the size of the semantics of ﬁelds as a termination
criterion is to consider the time required to reach the full-
size ﬁeld semantics. The algorithm for test suite generation is
similar to Alg. 1, and is presented as Alg. 2.
Algorithm 2 Field-coverage test suite generation by time
tranScoping.
1:function COMPUTE -SUITE (int : smallScope ,Scope )
2: Analysis analysis = new Analysis();
3: Time[] times = new Times[ smallScope ];
4: for(inti=0,i<smallScope ,i++) do
5: times [i] = computeTimesForScope( i+1,analysis );
6: end for
7: Time tranScopedTime = tranScopeTime( times ,Scope );
8: Set /angbracketleftInput /angbracketrightsuite = new Set /angbracketleftInput /angbracketright;
9: Map /angbracketleftFieldName ,Pair /angbracketleftObject,Object /angbracketright/angbracketrightﬁeldsSemantics =
10: new Map /angbracketleftFieldName ,Pair /angbracketleftObject,Object /angbracketright/angbracketright();
11: while elapsedTime <tranScopedTime do
12: Input input = getNextInput( analysis );
13: if(input ==NULL )then
14: break;
15: end if
16: boolean addedPair = updateSemantics( ﬁeldSemantics ,input );
17: if(adde dPair )then
18: suite .add( input );
19: end if
20: end while
21: return suite ;
22: end function
Notice that unlike Alg. 1, which kept track of the size of
the semantics of each ﬁeld, in this case, we only need to keeptrack of the elapsed time of the current analysis in order to
determine when generation must be stopped. The elapsed time
is maintained by the variable elapsedTime . The underlying
testing criterion FCC Swhich Algorithm 2 satisﬁes is that
each ﬁeld contains the fraction of the actual ﬁeld semantic
discovered using the tranScoped time as a timeout. We present
a running example of Algorithm 2 further below.
The extrapolation methods considered when tranScoping
generation time will be the following:
•A quadratic regression parabola.
•An exponential regression.
•The function computed as the average of functions above.
We will now discuss the reasons for selecting these regres-
sion functions. The number of feasible paths in a program
usually grows exponentially as the program size increases.
Therefore, an exponential function may provide a good es-
timation of the actual time. However, the exponential function
has the same limitations that the quadratic function had when
tranScoping ﬁeld size, i.e. inferring a value too close (or
even exceeding) the actual generation time. On the other
hand, a quadratic function may provide a suitable lower
bound to ﬂatten the average regression while providing a good
underestimation of the actual time.
In order to compute the regression functions, we use the
Ordinary Least Squares method (OLS) [11] to estimate the
parameters of a multiple linear regression model. In partic-
ular, to compute an exponential regression we transform the
observations to a linear model.
Table I presents a running example for BinSearchTree
in order to illustrate how Algorithms 1 and 2 work. Each
row corresponds to a different execution of the Symbolic
PathFinder tool for the given scope. Columns Suite size
and Generation time present the size of the generated suite
and the time in seconds required for the generation, using
3 different test generation policies, namely, Fet (for ﬁeld-
exhaustive testing), Size-a (for Algorithm 1 using the average
between the linear and quadratic regressions) and Time-a
(for Algorithm 2 using the average between the quadratic
and exponential regressions). Notice that the suite size and
generation time for scopes 1 to 5 are the same for the three test
generation policies since Algorithms 1 and 2 compute ﬁeld-
exhaustive suites for these scopes. Column |[[f]] |presents the
actual ﬁeld semantic sizes for ﬁelds leftand right whereas
T|[[f]] |presents the tranScoped ﬁeld semantic sizes using the
average regression. Notice that in order to compute |[[f]] |we
need to compute a ﬁeld-exhaustive suite, while Algorithm 1
only needs to reach the bound T|[[f]] |, e.g., for scope 8 the
generation will stop when the ﬁeld semantics size of ﬁeld
leftreaches 21 pairs andthe ﬁeld semantics size of ﬁeld right
reaches 25 pairs. This condition is met in 140 seconds and the
generated suite contains 33 non-isomorphic structures whose
bug-detection ability is comparable to that of ﬁeld-exhaustive
suites as we will see in section IV. Finally, columns Time
andTT i m e present the time required to reach the full-size
ﬁeld semantics and the tranScoped time, respectively, using
the average regression. Notice the difference between columns
96
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. TABLE I
RUNNING EXAMPLE WITH BINSEARCH TREE
Suite size Generation time |[[f ]] |T|[[f ]] |Time T Time
Scope Fet Size-a Time-a Fet Size-a Time-a left/right
1 2 1 1/1 - 1 -
2 4 2 3/3 - 2 -
3 7 2 5/6 - 2 -
4 11 3 8/9 - 2 -
5 16 8 11/13 - 4 -
6 22 20 17 31 9 9 15/17 14/16 9 5
7 29 25 17 136 32 9 19/22 17/20 32 6
8 37 33 17 641 140 9 24/27 21/25 142 8
9 46 38 22 2907 634 11 29/33 24/29 633 11
Generation time and Time . The former is the time required to
generate a ﬁeld-exhaustive suite, i.e. exploring the whole space
of alternatives and the latter is the time required to reach the
full-size ﬁeld semantics, i.e. when it was the last time an input
contributed new pairs to ﬁeld semantics. Column Time can
only be computed once the generation has ﬁnished. However,
we can use these values from scopes 1 to 5 to make predictions
for larger scopes which is the key concept of Algorithm 2,
e.g., for scope 8 the generation will stop when the tranScoped
timeout of 8 seconds is reached, generating a suite containing
17 non-isomorphic structures whose bug-detection ability is
comparable to that of ﬁeld-exhaustive suites as we will see in
section IV.
3) T ermination and Correctness of the Field-Coverage Suite
Generation Algorithms: Termination of Alg. 2 follows imme-
diately because the while loop execution time is bounded by
tranScopedTime . Termination of Alg. 1, on the other hand,
is not obvious. Notice that variable tranScopedSizes may
contain an inferred size for a ﬁeld fthat is larger than the
actual maximum size for the semantics of said ﬁeld. Therefore,
the loop will only terminate in case getNextInput( analysis )
returns null. To this end, we will only consider analyses that,
for a user-given scope, become ﬁnite-state. Of course, the
number of states may be large, and in this case, the generation
will terminate when the underlying analysis terminates. This is
why the presented techniques, which allow us to predict when
to stop the analyses, are essential. Regarding correctness, let
us deﬁne set S={/angbracketlefti,o/angbracketrightf:/angbracketlefti,o/angbracketright∈ﬁeldSemantics (f)}, i.e.,
for each ﬁeld f, setScontains those pairs computed in variable
ﬁeldSemantics with label f. It is then clear that variable suite
stores a test suite that satisﬁes testing criterion FCC S.
IV . E XPERIMENTAL EV ALUATION
In this section, we present the experimental details and we
will evaluate several ﬁeld-coverage criteria. Since our goal is
to approximate ﬁeld-exhaustive testing, we will compare, for
each criterion, the achieved branch coverage and the mutation
score, as well as the suite size and the generation time,
against the values obtained for ﬁeld-exhaustive testing. We
will consider an implementation of the following subjects for
our experimental evaluation:
(i) Sorted singly linked lists,
(ii) Binary search trees,(iii) TreeSets,
(iv) A VL trees, and
(v) Binomial heaps.
A. Experimental details
Algorithm 3 shows the driver needed to generate inputs with
Symbolic PathFinder (SPF) tranScoping ﬁeld size. Line 3 in-
structs SPF to treat Xas a symbolic variable, thus enabling lazy
initialization over Xitself as well as its ﬁelds. Line 4 prunes
the search whenever an invalid instance is found (we generate
suites of valid inputs). Symbolically executing this line would
potentially spawn inﬁnite paths since the size of a binary
search tree is not restricted by the repOK() . However, this
potentially inﬁnite exploration is bounded by the scope of the
analysis. The methods used in lines 5–8 are captured by the lis-
tener, except for Verify.writeObjectToFile() which
is part of the Verify API of SPF. Method addedPair()
checks whether the new input Xcontributes with new pairs
to the ﬁeld semantics. if so, line 6 solves the remaining path
condition. Notice that the symbolic execution of repOK()
only concretizes (as a consequence of lazy initialization) ﬁelds
left andright . However, for basic types as the ones used
in ﬁeld value we must resort to the SMT-solver and request
concrete values to generate the actual input. Line 7 serializes X
and saves it to the unique path returned by getNextPath() .
Finally, line 8 checks whether the termination condition holds
and stops the analysis if so. Notice that tranScoping ﬁeld
size imposes a synchronic termination criterion, i.e ﬁnding
an input could lead to prematurely stop the analysis, whereas
tranScoping generation time imposes an asynchronous termi-
nation criterion in a timeout-wise manner.
In algorithms 1 and 2 we separated the computation
of the suites in 2 stages for the sake of simplicity. The
ﬁrst stage where we call tranScopeFieldSizes and
tranScopeTime for algorithms 1 and 2 respectively to
estimate the target of each termination criterion and the
second stage where we use these criteria to stop the analysis.
This requires to run SPF twice, the ﬁrst time up to scope
smallScope and the second one up to the desired scope,
however, there is no such separation in algorithm 3. As we dis-
cuss below, we imposed a search heuristic that explores inputs
in ascending order, considering the size of the structures. That
said, it is straightforward to take advantage of this property to
compute the suite up to scope smallScope , tranScope the
termination criteria and continue the analysis, running SPF
only once.
To measure the coverage and mutation score we considered
3 different methods of each case study, namely insert ,
remove andcontains . All of this methods take as input
an integer which we selected in a bounded exhaustive manner,
this is, to run the tests with a suite generated up to scope n,
we considered the integers in the range [0, n-1]. The tests
were written with the help of JUnit [31], and the coverage
and mutation analyses were performed with Jacoco [32] and
MuJava [19], respectively.
97
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. Algorithm 3 Driver to run SPF for binary search trees
tranScoping ﬁeld size.
1public static void main(String[] args) {
2BSTree X = new BSTree();
3X = (BSTree) Debug.makeSymbolicRef("X", X);
4Verify.ignoreIf(X == null || !X.repOK());
5if(addedPair()){
6 solvePathCondition();
7 Verify.writeObjectToFile(X, getNextPath());
8 stopIfTerminationConditionHolds();
9}
10}
TABLE II
SUITE SIZE AND TIME TO GENERATE SUITES WITH SYMBOLIC
PATHFINDER TRAN SCOPING FIELD SIZE (s=SCOPE ).
size time size time size time size time size time
s=6 s=1 5 s=2 5 s=3 5 s=4 5SortedListfet 62 15 4 25 13 35 44 45 121
fbt-l 62 15 4 25 13 35 43 45 120
fbt-a 62 15 4 25 13 35 44 45 123
fbt-q 62 15 4 25 13 35 43 45 121
s=6 s=7 s=8 s=9 s=1 0BSTreefet 22 31 29 136 37 641 46 2907 56 TO
fbt-l 18 9 23 31 27 33 31 138 37 142
fbt-a 20 9 25 32 33 140 38 634 47 2955
fbt-q 22 10 29 33 37 628 46 2950 56 TO
s=6 s=7 s=8 s=9 s=1 0TreeSetfet 18 165 25 1025 33 6713 40 TO --
fbt-l 14 35 19 170 21 177 26 1048 --
fbt-a 16 38 19 172 26 1036 31 1061 --
fbt-q 18 42 23 181 29 1064 36 6812 --
s=8 s=1 0 s=1 2 s=1 4 s=1 5A vlTreefet 28 44 41 199 58 817 85 5127 93 TO
fbt-l 20 16 28 35 35 70 42 210 45 232
fbt-a 24 23 35 70 47 371 62 849 70 1647
fbt-q 28 35 41 202 58 813 85 4307 93 TO
s=6 s=8 s=1 0 s=1 2 s=1 3BinHeapfet 71 7 9 109 11 633 13 3258 14 TO
fbt-l 71 4 96 5 10 237 11 480 12 1461
fbt-a 71 4 96 5 11 467 13 2235 14 6761
fbt-q 71 7 9 107 11 627 13 3272 14 TO
The experiments were conducted on a 3.6GHz Intel Core
i7 processor with 8 GB RAM, running archlinux 4.19. The
replication package, as well as the complete results, can be
found in https://sites.google.com/itba.edu.ar/fbt-ase19.
B. Results
In Tables II–VI we denote ﬁeld-exhaustive testing by fet. We
denote by fbt-l, fbt-q, and fbt-e the criteria that tranScopes a
given attribute using a linear, quadratic or exponential regres-
sion function, respectively. The criterion that uses the average
of the two corresponding regression functions is denoted by
fbt-a.
In Tables II and III, times are expressed in seconds and a 2
hours timeout (marked as TO when reached) is set. In Tables V
and VI, branch coverage and mutation score are expressed as
percentages. Table IV shows the number of mutants generated
for each case study. A total of 30 mutation operators were
used for this task. The list of these operators can be found in
the replication package.TABLE III
SUITE SIZE AND TIME TO GENERATE SUITES WITH SYMBOLIC
PATHFINDER TRAN SCOPING GENERATION TIME (s=SCOPE ).
size time size time size time size time size time
s=6 s=1 5 s=2 5 s=3 5 s=4 5SortedListfet 62 15 4 25 13 35 44 45 121
fbt-q 62 62 62 62 62
fbt-a 62 15 4 63 35 44 45 123
fbt-e 62 15 4 25 13 35 43 45 120
s=6 s=7 s=8 s=9 s=1 0BSTreefet 22 31 29 136 37 641 46 2907 56 TO
fbt-q 17 9 17 9 17 9 22 11 22 13
fbt-a 17 9 17 9 17 9 22 11 22 12
fbt-e 17 9 17 9 17 9 22 11 17 9
s=6 s=7 s=8 s=9 s=1 0TreeSetfet 18 165 25 1025 33 6713 40 TO --
fbt-q 13 33 13 33 16 40 18 47 --
fbt-a 13 33 13 34 13 33 18 43 --
fbt-e 13 33 13 33 13 33 18 45 --
s=8 s=1 0 s=1 2 s=1 4 s=1 5A vlTreefet 28 44 41 199 58 817 85 5127 93 TO
fbt-q 14 9 17 11 18 14 20 19 22 21
fbt-a 17 11 18 15 24 26 35 81 31 60
fbt-e 17 11 22 20 28 38 35 72 35 98
s=6 s=8 s=1 0 s=1 2 s=1 3BinHeapfet 71 7 9 109 11 633 13 3258 14 TO
fbt-q 71 4 72 1 74 1 86 4 97 7
fbt-a 71 4 71 8 73 1 98 5 86 4
fbt-e 71 4 72 1 84 9 73 8 85 1
TABLE IV
NUMBER OF MUTANTS GENERATED FOR EACH CASE STUDY .
Case study # of mutants
SortedList 595
BSTree 1280
TreeSet 7287
AvlTree 5145
BinHeap 10354
TABLE V
COMPARING COVERAGE AND MUTATION SCORE OF FIELD -EXHAUSTIVE
AND FIELD COVERAGE SUITES TRAN SCOPING FIELD SIZE (s=SCOPE ).
cov mut cov mut cov mut cov mut cov mut
s=6 s=1 5 s=2 5 s=3 5 s=4 5SortedListfet 27 85 27 86 27 86 27 86 27 86
fbt-l 27 85 27 86 27 86 27 86 27 86
fbt-a 27 85 27 86 27 86 27 86 27 86
fbt-q 27 85 27 86 27 86 27 86 27 86
s=6 s=7 s=8 s=9 s=1 0BSTreefet i 29 85 29 85 29 85 29 85 29 85
fbt-l 29 85 29 85 29 85 29 85 29 85
fbt-a 29 85 29 85 29 85 29 85 29 85
fbt-q 29 85 29 85 29 85 29 85 29 85
s=6 s=7 s=8 s=9 s=1 0TreeSetfet 33 72 33 72 33 73 33 73 --
fbt-l 32 67 33 72 33 73 33 73 --
fbt-a 33 72 33 72 33 73 33 73 --
fbt-q 33 72 33 72 33 73 33 73 --
s=8 s=1 0 s=1 2 s=1 4 s=1 5A vlTreefet 35 84 35 84 35 84 35 84 35 84
fbt-l 35 84 35 84 35 84 35 84 35 84
fbt-a 35 84 35 84 35 84 35 84 35 84
fbt-q 35 84 35 84 35 84 35 84 35 84
s=6 s=8 s=1 0 s=1 2 s=1 3BinHeapfet 24 74 25 75 25 75 25 82 25 84
fbt-l 24 74 25 75 25 75 25 75 25 75
fbt-a 24 74 25 75 25 75 25 82 25 84
fbt-q 24 74 25 75 25 75 25 82 25 84
98
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. TABLE VI
COMPARING COVERAGE AND MUTATION SCORE OF FIELD -EXHAUSTIVE
AND FIELD COVERAGE SUITES TRAN SCOPING GENERATION TIME (s=
SCOPE ).
cov mut cov mut cov mut cov mut cov mut
s=6 s=1 5 s=2 5 s=3 5 s=4 5SortedListfet 27 85 27 86 27 86 27 86 27 86
fbt-q 27 85 27 86 27 86 27 86 27 86
fbt-a 27 85 27 86 27 86 27 86 27 86
fbt-e 27 85 27 86 27 86 27 86 27 86
s=6 s=7 s=8 s=9 s=1 0BSTreefet 29 85 29 85 29 85 29 85 29 85
fbt-q 29 85 29 85 29 85 29 85 29 85
fbt-a 29 85 29 85 29 85 29 85 29 85
fbt-e 29 85 29 85 29 85 29 85 29 85
s=6 s=7 s=8 s=9 s=1 0TreeSetfet 33 72 33 72 33 73 33 73 --
fbt-q 32 67 32 67 33 72 33 72 --
fbt-a 32 67 32 67 32 67 33 72 --
fbt-e 32 68 32 68 32 68 33 73 --
s=8 s=1 0 s=1 2 s=1 4 s=1 5A vlTreefet 35 84 35 84 35 84 35 84 35 84
fbt-q 35 82 35 83 35 83 35 84 35 84
fbt-a 35 83 35 84 35 84 35 84 35 84
fbt-e 35 83 35 84 35 84 35 84 35 84
s=6 s=8 s=1 0 s=1 2 s=1 3BinHeapfet 24 74 25 75 25 75 25 82 25 84
fbt-q 24 74 24 74 24 74 25 75 25 75
fbt-a 24 74 24 74 24 74 25 75 25 75
fbt-e 24 74 24 74 25 75 24 75 25 75
C. Discussion
Tables II and III show that the generation of test suites
resorting to the presented techniques is most times signiﬁ-
cantly faster than generating ﬁeld-exhaustive ones. Of course,
such speedup would be worthless if the produced suites were
of poorer quality. Interestingly, Table V shows that suites
produced by tranScoping ﬁeld size achieve, in almost all cases
(all but 2) the same branch coverage and mutation score than
ﬁeld-exhaustive suites. As it may be expected (see Table VI),
tranScoping suite generation time is less precise, although still
achieves good branch coverage and mutation score. We believe
the results obtained when tranScoping ﬁeld size, are quite
compelling.
There is an interesting observation to be made regarding
sorted singly linked lists. Table II shows no gains for the
proposed techniques. The reason for this phenomenon is
twofold. On the one hand, as we said before, a linear function
provides a good estimation when tranScoping ﬁeld semantic
sizes (indeed, the computed linear regression is exact) and the
quadratic coefﬁcient of the quadratic regression is almost 0
(−3.31e−16), which produces an almost linear function for
the values of x in the range [1,50]. This causes the linear,
the quadratic and the average regressions to produce the same
(and exact) estimations. On the other hand, since there is only
one valid instance per scope and this case study, in particular,
is not fully beneﬁted by the imposed search heuristic since it
always requires a new node to produce a new instance and
has no previous ﬁeld (discussed below), we can not take
advantage of the observation that most of the required inputs
can be generated early in the symbolic execution.
Table III also shows an interesting result regarding singlylinked lists. TranScoping generation time is, by deﬁnition,
sensitive to execution times, and in this particular case, the
generation up to scope smallScope usually takes less than
2 seconds. The lack of data as well as its variability, some-
times makes the OLS method to produce negative quadratic
regression functions, stopping the analysis too early compared
with the exponential regression.
Even though the last two observations are negative, Tables V
and VI show these shortcomings have no effect over the quality
of the generated suites, which in turn have the same coverage
and mutation score as those generated using ﬁeld exhaustive
testing. The reason for this to happen is that the methods
that handle singly linked lists are simple enough (they usually
consider at most 3 different scenarios) to be covered and get
their mutations killed even with a few inputs. The counterpart
of singly linked lists, in terms of code complexity, are binomial
heaps and the results obtained tranScoping time show that
fewer inputs (Table III) negatively impact the mutation score,
in particular for higher scopes (Table VI). However, the results
obtained tranScoping size for the same case study are quite
compelling.
As we said before, when symbolically executing a
repOK() invariant, it is possible to generate most of the
required inputs early in the symbolic execution. This obser-
vation has a signiﬁcant impact on the presented techniques
since it allows us to prematurely stop the search and generate
a signiﬁcative fraction of the inputs that would be generated
using ﬁeld-exhaustive testing. This is achieved in Symbolic
Pathﬁnder by imposing a search heuristic that prioritizes the
use of previously created objects and the null reference over
new objects, as candidates when lazy initializing objects’
ﬁelds. As a side effect, this heuristic explores inputs in order,
considering the size of the structures.
The fact a repOK() procedural invariant is required may
be perceived as a limitation of these techniques. However, as
reported in [17], having such methods is a good programming
practice. Alternatively, one may generate test inputs via sym-
bolic execution of constructor methods. This alternative has 2
problems:
•It is not obvious how to combine these methods to explore
all the possibilities (for instance, for red-black trees it is
required to combine methods insert andremove in
order to explore all valid tree nodes colorings).
•The symbolic execution of combinations of complex
constructor methods, like insert andremove is by
far more expensive than the symbolic execution of a
repOK() method.
D. Threats to V alidity
Experimental evaluations may be inﬂuenced by the selected
subjects. To reduce bias, we selected the same subjects used
in the evaluation of ﬁeld-exhaustive testing in [23]. The
selected subjects are good examples of programs with heavily
constrained memory heaps with a broad range of complexity,
going from those with simple invariants to some with complex
constraints. Moreover, they are often used as benchmarks
99
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. in the evaluation of other analysis tools [5], [29]. A more
thorough evaluation is pending, including more data structures,
but also general purpose software using said data structures.
V. R ELATED WORK
Automated test generation is a research area that has re-
ceived substantial attention in recent years, and important
advances, leading to the development of many tools and
techniques, have taken place. Some of the most effective and
successful automated testing approaches are either based on
random generation [6], [18], [21], evolutionary computation
[9], model checking [29], constraint solving (including SMT
[27] and SAT solving [1]) or some forms of exhaustive search
[5].
Test suites generated by tools based on random generation
and evolutionary computation such as Randoop [21], AutoTest
[18], QuickCheck [6] and EvoSuite [9], generate large test
suites. A consequence of such large suites is increased testing
time, a problem we try to avoid with ﬁeld-exhaustive testing.
Our approach corresponds to systematic test generation, in the
terminology of [26], which makes it closer to tools like Pex
[27], FAJITA [1], Symbolic PathFinder [28] and Korat [5].
These tools are also speciﬁcation based , i.e., they produce tests
from input speciﬁcations, as opposed to tools like EvoSuite or
Randoop, that use routines/methods of the tested subject to
produce test sequences.
In [26] a set of experiments compare random testing and
systematic testing for container classes, arriving at the conclu-
sion that random testing produces suites that are comparable
in quality (coverage, mutation score) to systematically pro-
duced suites (using shape abstraction) while consuming less
computational resources (less generation time).
In our approach, computing ﬁeld-exhaustive suites produces
ﬁeld extensions. These extensions are equivalent to (upper)
tight ﬁeld bounds , and thus our work is related to approaches
to compute such bounds, e.g., [3], [10], [22]. However, none of
these related approaches focuses on test generation; [10] does
not collect instances produced along the tight bound compu-
tation process (and even if it did so, it would be signiﬁcantly
more inefﬁcient than our approach, since it requires a cluster
of computers for tight bound computation); [3], [22] follow a
bounded exhaustive enumeration of instances to compute tight
bounds, as opposed to our ﬁeld-exhaustive mechanism.
VI. C ONCLUSIONS
We proposed a generalization of the ﬁeld-exhaustive testing
criterion, that we called ﬁeld-coverage testing criterion, whose
satisfaction is associated with the coverage of feasible values
for object ﬁelds. Besides formally deﬁning the criterion, we
developed an algorithm that automatically produces under-
approximations of ﬁeld-exhaustive suites using tranScoping,
which estimates characteristics of yet to be run analyses
for large scopes, based on data obtained from analyses per-
formed in small scopes. The experimental results showed
that tranScoping ﬁeld size provides a suitable condition to
prematurely stop the search while producing suites whose bugdetection ability is comparable to those satisfying the ﬁeld-
exhaustive testing criterion, measured in terms of mutation
score and branch coverage, in a fraction of the time.
Another conclusion we draw, is that ﬁeld-exhaustive testing
is a very powerful criterion which, even if weakened, still
produces good coverage and mutation score.
REFERENCES
[1] P. Abad, N. Aguirre, V . Bengolea, D. Ciolek, M. F. Frias, J. P. Galeotti,
T. Maibaum, M. Moscato, N. Rosner and I. Vissani, “Improving test
generation under rich contracts by tight bounds and incremental SAT
solving”, in Proceedings of Sixth IEEE International Conference on
Software Testing, Veriﬁcation and Validation, ICST 2013, Luxembourg,
Luxembourg, March 18-22, IEEE, 2013.
[2] P. Ammann and J. Offutt, “Introduction to software testing”, Cambridge
University Press, 2008.
[3] H. Bagheri and S. Malek, “Titanium: efﬁcient analysis of evolving
alloy speciﬁcations”, Proceedings of the 2016 24th ACM SIGSOFT
International Symposium on Foundations of Software Engineering - FSE
2016, 2016.
[4] K. Beck, “Embracing change with extreme programming”, IEEE Com-
puter 32(10), IEEE CS, 1999.
[5] C. Boyapati, S. Khurshid and D. Marinov, “Korat: Automated testing
based on Java predicates”, in Proceedings of International Symposium
on Software Testing and Analysis ISSTA 2002, ACM, 2002.
[6] K. Claessen and J. Hughes, “QuickCheck: a lightweight tool for random
testing of Haskell programs”, in Proceedings of the ﬁfth ACM SIGPLAN
international conference on Functional programming ICFP 2000, ACM,
2000.
[7] M. Cohen, M. Dwyer and J. Shi, “Constructing interaction test suites
for highly-conﬁgurable systems in the presence of constraints: A greedy
Approach”, IEEE Transactions on Software Engineering. 34, 5, 633-650,
2008.
[8] E. Clarke, O. Grumberg and D. Peled, “Model Checking”, MIT Press,
2000.
[9] G. Fraser and A. Arcuri, “EvoSuite: automatic test suite generation for
object-oriented software”, in Proceedings of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on Foundations of
Software Engineering ESEC/FSE 2011, ACM, 2011.
[10] J. P. Galeotti, N. Rosner, C. L ´opez Pombo and M. F. Frias, “Analysis
of invariants for efﬁcient bounded veriﬁcation”, in Proceedings of the
19th International Symposium on Software Testing and Analysis ISSTA
2010, ACM, 2010.
[11] F. Galton, “Regression Towards Mediocrity in Hereditary Stature”,
Journal of the Anthropological Institute, 15:246-263, 1886
[12] C. Ghezzi, M. Jazayeri and D. Mandiroli, “Fundamentals of Software
Engineering”, Second Edition, Prentice-Hall, 2003.
[13] D. Jackson, “Software Abstractions: Logic, language, and analysis”,
MIT Press, 2006.
[14] P. Jalote, “An Integrated Approach to Software Engineering”, 3rd.
Edition, Springer, 2005.
[15] S. Khurshid, C. P ˘as˘areanu and W. Visser, “Generalized symbolic ex-
ecution for model checking and testing”, in Proceedings of the 9th
International Conference on Tools and Algorithms for the Construction
and Analysis of Systems TACAS 2003, LNCS 2619, Springer, 2003.
[16] S. Khurshid and D. Marinov, “TestEra: Speciﬁcation-Based Testing of
Java Programs Using SAT”, Autom. Soft. Eng. 11(4), Kluwer Academic,
2004.
[17] B. Liskov, “Program development in Java: abstraction, speciﬁcation, and
object-oriented design”, Addison-Wesley, 2000.
[18] L. Liu, B. Meyer and B. Schoeller, “Using contracts and boolean queries
to improve the quality of automatic test generation”, in Proceedings of
the 1st International Conference on Tests and Proofs TAP 2007, LNCS
4454, Springer, 2007.
[19] Y .-S. Ma, J. Offutt and Y .-R. Kwon: “MuJava: An Automated Class Mu-
tation System”, Journal of Software Testing, Veriﬁcation and Reliability,
15(2), 2005
[20] S. Nelson, “Certiﬁcation processes for safety-critical and mission-critical
aerospace software”, Report NASA/CR-2003-212806, Ames Research
Center, 2003.
100
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. [21] C. Pacheco, S. K. Lahiri, M. D. Ernst and T. Ball, “Feedback-directed
random test generation”, in Proceedings of the 29th international con-
ference on Software Engineering ICSE 2007, IEEE, 2007.
[22] P. Ponzio, N. Rosner, N. Aguirre and M. Frias: “Efﬁcient tight ﬁeld
bounds computation based on shape predicates”, Lecture Notes in
Computer Science FM 2014: Formal Methods. 531546, 2014.
[23] P. Ponzio, N. Aguirre, M. Frias and W. Visser, “Field-Exhaustive Test-
ing”, in Proceedings of the ACM SIGSOFT International Symposium
on the Foundations of Software Engineering FSE 2016, ACM, 2016.
[24] N. Rosner, C. G. L ´opez Pombo, N. Aguirre, A. Jaoua, A. Mili and
M. Frias, “Parallel Bounded Veriﬁcation of Alloy Models by TranScop-
ing”, in Proceedings of the 5th International Conference on Veriﬁed
Software: Theories, Tools, Experiments VSTTE 2013, LNCS 8164,
Springer, 2013.
[25] D. Shao, S. Khurshid and D. E. Perry, “Whispec: White-box testing
of libraries using declarative speciﬁcations”, Proceedings of the 2007
Symposium on Library-Centric Software Design - LCSD ’07, 2007.
[26] R. Sharma, M. Gligoric, A. Arcuri, G. Fraser and D. Marinov, “Testingcontainer classes: random or systematic?” Fundamental Approaches to
Software Engineering Lecture Notes in Computer Science. 262-277,
2011.
[27] N. Tillmann, J. de Halleux, “Pex: White Box Test Generation for .NET”,
in Proceedings of the 2nd International Conference on Tests and Proofs
TAP 2008, LNCS 4966, Springer, 2008.
[28] W. Visser, C. S. Pasareanu and S. Khurshid, “Test Input Generation with
Java PathFinder”, in Proceedings of the ACM SIGSOFT International
Symposium on Software Testing and Analysis ISSTA 2004, ACM, 2004.
[29] W. Visser, C. S. Pasareanu and R. Pel ´anek, “Test Input Generation
for Java Containers using State Matching”, in Proceedings of the
International Symposium on Software Testing and Analysis ISSTA 2006,
ACM, 2006.
[30] H. Zhu, P. Hall and J. May, “Software Unit Test Coverage and Ade-
quacy”, ACM Computing Surveys 29(4), ACM, 1997.
[31] (2019, Sep.) JUnit. [Online]. Available: https://junit.org/
[32] (2019, Sep.) Jacoco. [Online]. Available:
https://www.eclemma.org/jacoco/
101
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:08:56 UTC from IEEE Xplore.  Restrictions apply. 