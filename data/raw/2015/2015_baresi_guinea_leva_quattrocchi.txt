See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/309614140
A discrete-time feedback controller for containerized cloud applications
Conf erence Paper  Â· No vember 2016
DOI: 10.1145/2950290.2950328
CITATIONS
93READS
261
4 author s:
Luciano Bar esi
Politecnic o di Milano
355 PUBLICA TIONS Â Â Â 9,690  CITATIONS Â Â Â 
SEE PROFILE
Sam Guine a
Politecnic o di Milano
75 PUBLICA TIONS Â Â Â 2,627  CITATIONS Â Â Â 
SEE PROFILE
Albert o Leva
Politecnic o di Milano
288 PUBLICA TIONS Â Â Â 3,186  CITATIONS Â Â Â 
SEE PROFILE
Giovanni Quattr occhi
Politecnic o di Milano
67 PUBLICA TIONS Â Â Â 534 CITATIONS Â Â Â 
SEE PROFILE
All c ontent f ollo wing this p age was uplo aded b y Giovanni Quattr occhi on 21 Sept ember 2023.
The user has r equest ed enhanc ement of the do wnlo aded file.A Discrete-Time Feedback Controller
for Containerized Cloud Applications
Luciano Baresi, Sam Guinea, Alberto Leva, and Giovanni Quattrocchi
DEIB - Politecnico di Milano, Piazza Leonardo Da Vinci 32 - Milan, Italy
{luciano.baresi|sam.guinea|alberto.leva|giovanni.quattrocchi}@polimi.it
ABSTRACT
Modern Web applications exploit Cloud infrastructures to
scale their resources and cope with sudden changes in the
workload. While the state of practice is to focus on dynam-
ically adding and removing virtual machines, we advocate
that there are strong benets in containerizing the applica-
tions and in scaling the containers. In this paper we present
an autoscaling technique that allows containerized applica-
tions to scale their resources both at the virtual machine
(VM) level and at the container level. Furthermore, ap-
plications can combine this infrastructural adaptation with
platform-level adaptation. The autoscaling is made possible
by our planner, which consists of a grey-box discrete-time
feedback controller. The work has been validated using two
application benchmarks deployed to Amazon EC2. Our ex-
periments show that our planner outperforms Amazon's Au-
toScaling by 78% on average without containers; and that
the introduction of containers allows us to improve by yet
another 46% on average.
CCS Concepts
Networks!Cloud computing; Social and pro-
fessional topics !Software selection and adapta-
tion; Quality assurance; Computing methodologies
!Computational control theory;
Keywords
Adaptive systems; cloud computing; control theory; contain-
ers; software adaptation
1. INTRODUCTION
Nowadays many Web applications are deployed and exe-
cuted in the cloud to scale more easily according to the cur-
rent workload. Industry has developed various techniques
for automating and improving the management of these kinds
of applications. A concrete example is Amazon's AutoScal-
ing [1], which allows system administrators to determinewhen and how an application's resources should dynamically
increase or decrease. Academia has also provided a large
body of work on cloud management [19, 24, 27, 47, 49], with
a strong focus on self-adaptation [25,32,40] and on dynamic
resource allocation at the infrastructure layer [37,42,44,53].
The state of the art comprises many MAPE-based [38]
approaches where planning is based on heuristics [22,26,50],
articial intelligence [34,35,45], queueing theory [33,51] and
control theory [30,41,46]. Moreover, literature distinguishes
among three kinds of possible adaptations. Infrastructure
adaptation changes the number of computing resources allo-
cated to the application. Platform adaptation re-congures
the platform stack on which the application runs its code
(e.g., by changing the number of workers dedicated to an
application server). Feature adaptation [39] temporarily re-
moves optional features from the application when the sys-
tem is saturated (e.g., by turning o a recommendation sys-
tem of an e-commerce site).
In this context, the paper introduces a novel MAPE-based
self-adaptation framework for cloud-based Web applications
centered around three important novelties.
The rst is that we allow system administrators to take
advantage of both VM- and container-based resource man-
agement, eectively allowing them to mix two very dierent
levels of granularity when conceiving infrastructure elastic-
ity. Containerization is an emerging virtualization technique
in which many processes |called containers| are run on
the same physical machine without interference. Containers
share a host operating system and are more lightweight and
faster to boot than traditional VMs [29,52]. Renting a clus-
ter of VMs and using it to execute dierent containerized
applications (or micro-services) is an emerging architectural
pattern [16, 17, 31, 43, 48], but it still lacks a comprehensive
approach for resource management.
The second is that we focus on coordinated infrastruc-
ture and platform adaptation, although the use of containers
would also allow us to support feature adaptation. To the
best of our knowledge, there are no approaches that support
all the dierent kinds of adaptations. While infrastructure
adaptation |at the VM level| is an industrial best prac-
tice, platform adaptation has yet to be thoroughly investi-
gated. One of the main reasons for this is that the more
aspects you want to adapt, the more your planner becomes
complex, to the point in which it can lose generality and be-
come over-t for a specic application. Feature adaptation,
on the other hand, has been implemented in the past.
Containers play an essential role in this coordinated ap-
proach. While the planning phase of our MAPE control loopApplication LogicApplication ServerDevelopment RuntimeOperating System(Virtualized) InfrastructureNodeApp 1Tier 1App 2Tier 1App 3Tier 2Bins/LibsBins/LibsBins/LibsGuestOSGuestOSGuestOSHypervisorHost OSHost ServerApp 1Tier 1App 2Tier 1App 3Tier 2Bins/LibsBins/LibsBins/LibsContainer EngineHost OSHost Server(a)(b)Figure 1: (a) Multi-level nature of a node; (b) Vir-
tual machines (left) vs. containers (right).
treats containers as black boxes and focuses on their horizon-
tal and vertical scaling, the containers themselves present
well-dened technology- and application-specic callbacks
through which their internals can be adapted. These call-
backs are triggered every time something changes at the con-
tainer infrastructure level.
The third is a novel MAPE planner that consists of a
discrete-time feedback controller. Peculiar to our proposal
is the structure of the said controller; it is tailored to the
structure of the command-to-metrics dynamics to be gov-
erned, using a grey-box approach. The resulting controller
is composed of a linear, time-invariant block plus a static lin-
earization one. The planner is also endowed with an internal
saturation management (anti-windup) mechanism.
The evaluation of our approach has been conducted using
two cloud applications: RUBiS, an application for on-line
auctions, and Pwitter, a simple Twitter-like social network.
Our experiments show that our approach outperforms Ama-
zon's AutoScaling (i.e., the industrial state of practice) by
78% on average when used solely with VMs and without
containers. They also show that, if we introduce containers,
and therefore adopt a ner granularity, we can improve the
performance of our adaptation by another 46% on average.
The rest of the paper is organized as follows. Section 2
introduces our use of container-based technology. Section 3
presents a high-level overview of our approach. Section 4
discusses our control theory planner, while Section 5 ex-
plains how we combine infrastructure and platform adapta-
tion. Section 6 presents the experiments done to validate
our work. Section 7 discusses related approaches, while Sec-
tion 8 concludes the paper.
2. ANATOMY OF A CLOUD APPLICATION
Our work focuses on Web applications that are deployed
and executed on public, private, or hybrid clouds. The vir-
tualized infrastructure is considered to be a black box: this
means that we do not have access to the hypervisor or to
the underlying physical machines.
Modern Web applications are typically developed using a
multi-tier architecture. A tierlogically and physically sepa-
rates software components that deal with dierent functional
aspects of an application. For example, a traditional 3-tier
application comprises a presentation tier, which manages
the user interface; a logic tier, which executes the applica-
tion's business logic; and a data tier, which handles user
data. Modern applications, however, can be made of more
than three tiers. For example, one might decide to use two
data tiers: one for a traditional relational database and one
for a NoSQL database.Each tier contains multiple nodes. A node is a computing
instance that runs platform software (e.g., a JVM and an
application server), as well as some actual application code
(see Figure 1(a)). Given the denition of tier that we use
within this paper, all the nodes in a tier execute the same
technological stack. This is not an uncommon assumption;
in fact, it directly mimics the way Amazon AWS denes
application architectures in OpsWorks1.
Managing multi-tiered cloud applications, while satisfy-
ing a set of functional and non-functional requirements, is
a complex task. It requires understanding the dependencies
that exist between nodes belonging to dierent tiers. For ex-
ample, in a 3-tier application, a load balancer must know all
the IP addresses of the application tier's nodes. Moreover,
the nodes in a tier could change over time due to scale-in
and scale-out policies.
2.1 Virtual Machines vs. Containers
A node is traditionally materialized as a VM. However,
nowadays a node might also be a container. Containers pro-
vide a virtualization technique that operates at the Operat-
ing System (OS) level. They exploit several features of the
Linux kernel, such as namespaces andcgroups , to create iso-
lated views of the operating environment for dierent appli-
cations. A container has its own process space, virtualized
network interface, and le system; and the operating sys-
tem can allocate dierent amounts of resources (e.g., CPU,
memory, and I/O) to each of them.
Figure 1(b) shows a layered comparison between VMs and
containers. If we start our comparison by looking at the
lower layers we can see that multiple VMs are managed by
a single hypervisor that resides on a single host operating
system. Each VM then contains its own guest operating sys-
tem, its own platform stack composed of dierent libraries,
middleware, and application servers, and its own application
code.
On the other hand, containers are executed directly on
top of the host operating system, optionally with the help
of a container manager like Docker [5]. Each container has
its own platform stack and its own application code. Con-
tainers have various advantages when compared to VMs:
they are more lightweight and they are faster to boot and
to terminate because they do not have to deal with a guest
operating system [29,52].
Industry is widely adopting containers as a means to favor
portability and they are considered to be one of the main
technological enablers of the DevOps movement [36]. Dif-
ferent development teams may use dierent operating sys-
tems and dierent platform stacks, making feature integra-
tion hard. However, thanks to containerization technology,
features can be developed in isolation, with the guarantee
that they will work the exact same way on any machine that
supports containers.
Many containerization technologies exist. We chose Docker
because it has the highest industry penetration and its per-
formance can be considered best in class [29]. Docker is
written in Go and it uses the libcontainer library to manage
the Linux kernel. It distinguishes between images and con-
tainer instances: the former are container snapshots that can
then be used to generate new instances of these snapshots.
1OpsWorks is a conguration management tool provided by
Amazon AWS for conguring and operating complex appli-
cations using DevOps technology.Amazon EC2 CloudApp 1  Tier 1Bins/LibsECoWare SensorAdaptation HooksECoWare ContainerApp 2  Tier 3Bins/LibsECoWare SensorAdaptation HooksECoWare Container
createVM /terminateVMGuest OSDocker EngineECoWare AgentVM 1Guest OSDocker EngineECoWare AgentVM nApp 1  Tier 2Bins/LibsECoWare SensorAdaptation HooksECoWare Container
CloudWatchcreateContainerterminateContainerupdateContainercreateContainerterminateContainerupdateContainerEC2providesmonitorshosted onhosted onhosted on
interfacemonitoring dataincoming eventFigure 2: A deployment of ECoWare.
To conclude, there is no need to choose between VMs and
containers. Containers can run inside VMs to increase secu-
rity [6], and it is becoming quite common to nd containers
being used in conjunction with virtualized cloud infrastruc-
tures. For example, with Amazon EC2 Container Service [2]
one can run Docker images directly across a cluster of EC2
VM instances.
3. ECOWARE
The solution presented in this paper extends ECoWare [20],
our framework for the development of self-adaptive systems2.
The main extensions in this paper are: (i) the support for
containerized applications; (ii) an entirely new planner, de-
signed from the ground up and based on control theory; and
(iii) an execution processing model that allows for coordi-
nated infrastructure and platform adaptation.
Figure 2 shows how containerized applications are de-
ployed using ECoWare. EcoWare can also be used without
containers, by deploying applications directly onto VMs, but
in that case it looses the capability of performing a more
ne-grained adaptation.
ECoWare is not limited to supporting a single application:
it can manage multiple multi-tiered applications running on
the same shared virtualized infrastructure. Each node in
a tier is a container that is hosted on a VM. Each node is
equipped with (i) an ECoWare Sensor , a component that has
access to the node's internals and is responsible for gener-
ating application-specic monitoring data; and (ii) a set of
Adaptation Hooks for platform adaptation.
Each employed VM also deploys an ECoWare Agent . This
component is responsible for providing adaptation actions
for creating and manipulating containers on that VM, and
for collecting container-specic monitoring data, such as the
containers' use of CPU and memory.
In the deployment shown in Figure 2 ECoWare uses Ama-
zon EC2. It exploits EC2's VM management APIs to pro-
vide infrastructure adaptation actions for creating and ma-
nipulating VMs, and Amazon CloudWatch to obtain moni-
toring information about the VMs themselves.
Monitoring and Analysis. We have not extended ECo-
Ware with any major novel monitoring and analysis capa-
bilities. We did, however, produce new kinds of sensors to
2The implementation of all the components of ECoWare can
be found at: https://github.com/deib-polimi/ecoware.support both containers and VMs, and created a new Java
template to facilitate the creation of application-specic sen-
sors.
In EcoWare sensors generate data under the form of Ser-
vice Data Objects (SDOs) and deliver them to a distributed
messaging infrastructure implemented using RabbitMQ [13].
In turn, the distributed messaging infrastructure can deliver
collected data to three dierent kinds of data manipulation
tools: Aggregators ,KPI Processors , and Analyzers .
ECoWare's data manipulation tools were built using Es-
per [28], an instrument for implementing complex event pro-
cessing activities. Aggregators collect data from multiple
sensors to create new composite SDOs. KPI Processors cor-
relate and aggregate data coming from various sensors to
generate business-level Key Performance Indicators, such as
response times and throughputs. Analyzers predicate over
the actual contents of an SDO, e.g., to see whether a re-
sponse time goes beyond a given threshold.
Planning and Execution. Planning can be considered
technology- and application-agnostic, since it only focuses
on choosing how to adapt the infrastructural resources that
are dedicated to each node in a tier. Now that ECoWare
supports containers we can distinguish between two kinds of
infrastructural adaptation: we can change either the VMs
or the containers being used by an application, or both.
Throughout the rest of this paper we will refer to the for-
mer in terms of VM adaptation and to the latter in terms
ofCNT adaptation ; the term infrastructure adaptation will
be used to refer to both jointly.
CNT adaptation operates directly on an application's con-
tainers. It allows for instant reaction to workload changes,
both through horizontal and vertical container scaling. Hor-
izontal scaling implies the addition or removal of containers;
vertical scaling implies increasing or decreasing the amount
of resources dedicated to an already existing container.
The delays imposed by VM management are removed.
While booting a VM on a public cloud may take more than
one minute, adding a container, or changing its resources,
is practically instantaneous given the adopted time scale.
Since multiple applications that share the same virtualized
resources may see dierent workloads, the ner granularity
and the higher adaptation speed of working with contain-
ers allow us to use the VMs that we have more eciently,
without necessarily having to add new ones.
Since containers are deployed inside VMs our planner can
choose between ve kinds of infrastructure adaptations. cre-
ate_vm() allocates a new VM through the cloud provider.
terminate_vm(vm_id) removes a VM identied by an id
(vm_id ).create_container(vm_id, node_type, resour-
ces) launches a container of type node_type onvm_id allo-
cating to it resources (CPU cores and memory). termina-
te_container(container_id) removes the container with
container_id .scale_container(container_id, new_re-
sources) changes the resources allocated to container_id
tonew_resources .
ECoWare also supports platform adaptation through the
use of adaptation callbacks. Whenever ECoWare operates
a CNT adaptation a notication is sent to the interested
ECoWare Agents . Their containers can then react by in-
ternally adapting their platform assets in technology- and
application-specic ways. This is a novel contribution of
this work and will be discussed in detail in Section 5.3.1 Applications
ECoWare needs a thorough description of the self-adapting
systems that it is supposed to manage. This includes both
a description of the cloud infrastructure to use and a de-
scription of the applications being deployed. The latter in-
cludes the denition of their tiers and of the dependencies
that exist between them. This information is collected in an
ECoWare Applications Description JSON le inspired by
TOSCA [21].
Listing 1 provides a (partial) description of the two appli-
cations and of the infrastructure that we used for the evalua-
tion in Section 6. We use it here to illustrate the information
that is stored in the le.
Listing 1: Example of an Applications Description
le.
f
" i n f r a s t r u c t u r e ": f
" c l o u d d r i v e r ":f
"name ": "aws ec2 " ,
" c r e d e n t i a l s ": "/ usr /me/ u t i l s /aws . p r o p e r t i e s "
"vm flavor " : "t2 . small "
"vm image id " : "ami e f f 8 1 4 8 f "
g,
"max vms ": 1 0
g,
"apps ": [
f
"name ": "RUBiS" ,
" t i e r s ":f
"loadbalancer ": f
"name ": " Front Load Balancer " ,
"max node ": "1 " ,
"docker image ": " haproxy " ,
"depends on ": [
"app server "
] ,
"on dependency scale ": "/ usr /me/
u t i l s / r e l o a d s e r v e r p o o l " ,
"max rt ": 0 . 1
g,
"app server ":f
"name ": " Application Logic Tier " ,
"docker image ": " polimi / rubis  j b o s s " ,
"depends on ": [
"db"
] ,
"on node scale ": "/ usr /me/ u t i l s /
jboss hook " ,
"on dependency scale ": "/ usr /me/ u t i l s /
r e l o a d c o n n e c t i o n s " ,
"max rt ": 0 . 6
g,
"db ":f
"name ": "Data t i e r " ,
"max node ": "1 " ,
"docker image ": " mysql " ,
"on node scale ": "/ usr /me/ u t i l s /
mysql hook " ,
"max rt ": 0 . 2
g
g
g,
f
"name ": " Pwitter " ,
" t i e r s " :f. . .g
g
]
g
The infrastructure JSON object describes the infras-
tructure layer on which the applications will be deployed. Its
attribute cloud_driver contains the name of the provider,
thecredentials needed to access the provider, and the
type ( vm_flavor ) and image id ( vm_image_id ) of the VM
instances to use. Attribute max_vm sets a limit to the al-locable VMs to avoid innitely scaling the application and
producing too high a bill.
In our example we used Amazon EC2 t2.small VM in-
stances; each has 1 CPU core and 2GB of memory. For
our image id we used an Amazon Machine Image identier3.
This VM image is congured to launch both Docker and an
ECoWare Agent as soon as the VM has nished booting.
Each application is identied by a name and its tiers are de-
scribed using attribute tiers . Each tier has a name, a maxi-
mum number of nodes permitted within that tier ( max_node ),
a reference to the Docker image to be used for its nodes
(docker_image ), a list of the dependencies that the tier has
with other tiers ( depends_on ), and any additional meta-data
about its adaptation strategy. In our example, we do not
want the load balancer to be replicated, so we set its at-
tribute max_node to 1; the image for the load balancer tier
is called haproxy ; and the load balancer manages a pool of
application servers, so it declares a dependency with that
tier. Currently we support one-to-one, one-to-many, and
many-to-one dependency relationships between tiers, where
byoneandmany we mean the number of nodes in each tier.
We do not support cyclic dependencies for the correctness
of our processing model (see Section 5.1).
Each tier also species what adaptation callbacks it sup-
ports and when they should be executed. This is stated
through two types of Adaptation Hooks . A container's on_-
node_scale hook is invoked every time the container is scaled
at run time; a container's on_dependency_scale hook is in-
voked after something changes on a tier that the container's
tier depends on. For example, a system administrator may
write an on_node_scale hook to scale the number of work-
ers of an application server when the resources allocated to
the relevant container change. Similarly, s/he may write
anon_dependency_scale hook to have the load balancer
change its routing policy depending on the resources allo-
cated to the servers.
Finally, the tier also declares the Service Level Agreement
(SLA) that we want to guarantee using attribute max_rt ,
which refers to the maximum acceptable average response
time passed to the planner.
4. PLANNING
This section describes the control-theoretical design path
followed to obtain the planner algorithm. Our goal was to
develop a decentralized solution, in which each application
tier is endowed with a local controller devoted to maintain-
ing a desired performance metric (i.e., response time) in the
presence of exogenous disturbances, by computing the re-
sources (i.e., CPU cores and memory) that must be made
available to that tier. Once this has been done, we translate
the computed resource allocations into concrete adaptation
actions, taking into account the current state of the system.
4.1 Controlled System
We shall now start by formalizing the hypotheses used
to derive the model for the controlled system, in control-
theoretical terms.
Hypothesis 1. In any steady-state situation the metric is a
function of the assigned resources and of some disturbance
input that reects the system's \load". This function does
3http://docs.aws.amazon.com/AWSEC2/latest/
UserGuide/AMIs.htmlnot need to be linear but it must be regular enough to be in-
verted (at least) in well dened regions of the resource/load
space. We assume that the steady-state response time de-
pends on the ratio between the number of assigned cores and
the request rate. Of course, the steady-state response time
may also depend on the available memory but we focused on
cores as they inherently provide a better granularity. Mem-
ory is either sucient for the application (hence assigning
more is useless) or insucient. In the latter case the perfor-
mance degradation will depend on many ne-grained facts
(e.g., caches, swap system, disk, etc.) making the actuator-
to-metric relation more dicult to model.
Hypothesis 2. The static function that we are describing
acts on the metric through an asymptotically stable, linear,
time-invariant dynamic system with unity gain and relative
degree. This means that once the resource and the load stay
constant, the metric will eventually reach its corresponding
steady-state value, but not immediately. For example, this
can be the case when acquiring a new core yields its re-
sponse time improvement as some queue gets emptied. The
hypothesis of unity relative degree means that the eects
of an action on the assigned resource start showing up in
the metrics from the control instant immediately after the
action is applied.
Under these hypotheses, the evolution over the discrete
time index kof the response time r(k), as an eect of the
assigned cores c(k) and the request rate r(k), is ruled by the
following nonlinear, time-invariant dynamic system

eu(k) =f 
c(k)=r(k)
r(k) =pr(k 1) + (1 p)eu(k 1)(1)
that corresponds to the block diagram of Figure 3.
c(k)
r(k)Ã· f(Â·)1âˆ’p
zâˆ’pÏ„R(k)
Figure 3: Block diagram of the dynamic model for
the controlled system.
In this particular case, function f() is intuitively mono-
tonically decreasing towards a possible lower horizontal a-
symptote, as it can be assumed that once the parallelism
degree of an application is fullled by the available cores,
adding new ones causes no further decrease in the response
time. More specically, we found a practically acceptable
function to be
fc(k)
r(k)
=c1+c2
1 +c3c(k)
r(k)(2)
where parameters c1,c2, andc3were obtained through pro-
ling.
4.2 Control Synthesis
The model structure of Figure 3 suggests a controller that
is the compound of a lineariser plus a linear, time-invariant
feedback regulator. Such a scheme is shown in Figure 4.
The key point is to select function fC(;) in such a way
thateu(k) =u(k), thereby leading the controller to see just
a linear block. This means setting
Ï„â—¦
R(k)+âˆ’
R(z) fC(Â·,Â·)u(k)
r(k)Ã·c(k)
f(Â·)1âˆ’p
zâˆ’p/tildewideu(k)
Ï„R(k)Figure 4: Block diagram of the dynamic model for
the closed-loop control system.
f1
c(k)fC 
u(k);r(k)
=u(k): (3)
which, assuming f() invertible at least in the signal range
of interest, gives
fC 
u(k);r(k)
=r(k)f 1 
u(k)
: (4)
With the selected fC(;), the directed relationship from
u(k) tor(k) reduces to the Z-domain transfer function (1  
p)=(z p), thus the relationship between 
r(k) andr(k)
also reduces to the Z-domain transfer function
Tr(z)
Tr(z)=R(z)1 p
z p
1 +R(z)1 p
z p: (5)
As frequently done in other control domains, we conduct
the selection of R(z) by prescribing the transfer function
from set point to controlled variable.
The target for this transfer function must have a relative
degree of at least one for realizability reasons. We addi-
tionally select a unity gain to ensure asymptotic set point
tracking and disturbance rejection and use the single pole 
|chosen in the range (0 ;1)| to require a faster ( !0) or
slower (!1) error convergence. In summary, we set

r(k)Tr(z)
Tr(z)=1 
z (6)
and, solving for R(z), we obtain
R(z) = 1
p 1z p
z 1: (7)
The eigenvalues of the obtained control system are the
prescribed one 2(0;1) and the cancelled one p, in the
range ( 1;1) in force of the stability properties we assumed
for the controlled system. Hence, the closed-loop system is
guaranteed to be asymptotically stable.
To turn (7) into the control algorithm required by the
planner, we rst rewrite (7) to highlight its direct feedthrough
and strictly proper dynamics terms, that is
R(z) = 1
p 1
1 +1 p
z 1
(8)
and we obtain the controller in state-space form as
(
xR(k) =xR(k 1) + (1 p) 

r(k 1) r(k 1)
c(k) =r(k)f 1
 1
p 1(xR(k) +
r(k) r(k))
(9)
This provides the control algorithm but it does not man-
age saturations. When confronted with a large change in
the set point, or a sudden and relevant disturbance, the
controller may compute an action c(k) that is not feasible,typically either because it is negative or because it exceeds
the maximum number of available cores. If cminandcmax
denote the minimum and maximum number of cores respec-
tively,c(k) must be clamped within the range, and the state
of (9) must be recomputed to maintain consistency with the
input and output, that is
xR(k) =p 1
 1fc(k)
r(k)
 
r(k) +r(k) (10)
Algorithmically, if we omit initializations and highlight
the errore(k) =
r(k) r(k), we have
e:=
r r;
xR:=xRp+ (1 p)ep;
c:=rfinv(( 1)=(p 1)(xR+e));
c:=max(cmin;min (cmax;c));
xRp:= (p 1)=( 1)f(c=r) e;
ep:=e;
where the \ p" subscript denotes \previous" values, i.e.,
those corresponding to the previous step, while \ f" and
\finv" correspond to function f() and its inverse, respec-
tively.
Finally, we present a simulation example that shows the
controller in action. Figure 5 reports the behavior of a sys-
tem subject to the load in the top-left plot, while the re-
quired response time is the dashed line in the top-right plot.
The dark line in the same plot is the actual response time
obtained with the model matching the real process, while
the light line represents the same results with a quite rele-
vant parametric mismatch (up to 20% in the coecients of
f() and up to 10% in p). The bottom plot presents the al-
located cores in the nominal (dark line) and parametrically
perturbed (light line) cases. Although a formal robustness
proof is out of the scope of this paper, and is therefore de-
ferred to future ones, the controller behaves satisfactorily
even in the presence of unavoidable model mismatches.
Figure 5: Simulation example to test the synthesised
controller.
4.3 From Resource Allocation to Actions
As previously stated, each tier of each application has
its own controller. The controllers are all deployed onto acentralized ECoWare Planner node and are synchronized.
After each control step the outputs of these controllers are
aggregated to create a new resource allocation le. This
le contains the number of CPU cores and memory units
needed to sustain the current workload for each tier of each
application. (One memory unit is equal to 512MB.)
The Planner Translator takes as input the new desired
resource allocations for each tier of each application and
translates them into a mix of infrastructure adaptation ac-
tions that must be activated. It uses the actions discussed
in Section 3: create_vm ,terminate_vm ,create_container ,
scale_container ,terminate_container .
For example, let us imagine that the controllers dedicated
to managing a RUBiS application request that the load bal-
ancer tier receive 3 cores and 2 memory units, and that the
business logic tier receive 5 cores and 3 memory units. In
this case, the list of adaptation actions could be to (i) create
a new VM; (ii) create a new container on that VM for the
business logic tier, with 3 cores and 2 memory units; and
(iii) update the container dedicated to load balancing by in-
creasing its resources by 2 cores and 1 memory unit. These
actions will depend on the resources already associated with
the various tiers at that point in time.
The Planner Translator uses the information it receives
to search for a new VM and container allocation through
the formulation of an Integer Programming problem, i.e., a
variation of the two-dimensional bin packing problem . The
bins are VMs and we must pack the containers inside them.
The two dimensions that we take into account are CPU cores
and memory.
To guarantee that the problem will terminate we calculate
the upper-bound for the number of VMs as the total number
of VMs needed to satisfy the desired resource allocation for
each tier in each application. This number will depend on
thevm_flavor being used, which is described in the ECoW-
areApplications Description le. For example, if tier t1
needs 3 cores, tier t2needs 1 core, and we are using a VM
avor that provides 2 cores, then we would need 2 VMs for
t1and 1 VM for t2. The upper-bound for VMs would be the
sum, that is 3 VMs.
We additionally consider the following constraint: a VM
can only host one container per tier of an application. The
reason is that we are able to vertically scale containers, thus
there is no need to create more than one instance per VM.
Each of the ve infrastructure adaptation actions is associ-
ated with a weight w, such that w(terminatecontainer )<
w(scalecontainer )< w (createcontainer )<< w (termi -
natevm)< w (createvm). These weights always favor
container manipulation over VM manipulation; moreover,
termination has a lower weight than creation to prefer the
removal of unused resources.
We also add the additional constraint that the sum of
the resources that are dedicated to the containers that are
deployed on a single VM must be less than, or equal to,
the total resources provided by the VM itself (as in the bin
packing problem).
By minimizing the weighted usage of containers and VMs,
we can nd an optimal setup of containers and VMs. We
solve the problem using or-tools , an ILP solver by Google4.
Its output is the input for our Executor , i.e., the list of
infrastructure adaptation actions that need to be performed.
4https://developers.google.com/optimization/.5. EXECUTION
The Executor takes the list of actions created by the
Planner Translator and enacts them using various actu-
ators. These actuators are used to ask the cloud provider to
create/terminate VMs, to ask Docker to create/update/ter-
minate containers, and to access the container Adaptation
Hooks defned in the Applications Description le.
When we started our project Docker could not dynami-
cally update the resources allocated to a running container.
This led us to extend Docker5; however, Docker 1.10.0 (re-
leased in February of 2016) now supports an update com-
mand that fullls our needs. This command dynamically
changes the CPU cores and the memory that are allocated
to a container using options --cpuset-cpus and --memory .
The former species what CPU cores the container can use,
e.g., --cpuset-cpus="0,2" states that the container can
only use core 0 andcore 2 . The latter limits the amount of
memory that the container can use, e.g., --memory="512M"
means that the container can only use 512MB of RAM. The
same options are also available to the runcommand, which
is used to create a new container.
ECoWare's Executor node uses three sub-modules: Topol-
ogy Graph ,Topology Manager , and Cloud Driver .Topol-
ogy Graph uses the ECoWare Applications Description
le to generate and maintain tier-dependency graphs for the
applications being managed. Topology Manager keeps track
of how the applications are deployed on containers and VMs.
It also maintains the metadata required to interact with the
containers: VM and container ids, IP addresses, ports, etc.
Finally, Cloud Driver is used to interact with the VM man-
agement APIs of the selected cloud provider. Our current
implementation of ECoWare supports AWS EC2 as our main
public cloud provider and uses Vagrant [15] for simple pri-
vate cloud setups. However, the system is modular and can
support new cloud providers.
ECoWare Agent s are pre-installed into the VM image that
is specied in the ECoWare Applications Description le.
The agents ensure that the sets of CPU cores allocated to dif-
ferent containers do not intersect (i.e., each CPU core must
be used by only one container), and invoke the Adaptation
Hooks when specic events occur in the system.
5.1 Adaptation Hooks and Processing Model
Adaptation Hooks are the mechanism through which ECo-
Ware keeps the planner technology- and application-agnostic,
yet still oering the capability of performing platform adap-
tations.
Adaptation Hooks are bash or python scripts that are
mounted directly onto their related Docker containers. These
scripts are launched by the ECoWare Agent s using the Docker
exec command. The on_node_scale hook takes as input the
previous and the new resource allocations: old_cpu_cores ,
new_cpu_cores ,old_mem_units ,new_mem_units . The on_-
dependency_scale hook takes as input the old_tier_state
and the new_tier_state . These parameters contain meta-
data about the old and the new states of the nodes in a tier
that the container depends on (e.g., the IP address of the
nodes, the resources allocated to the nodes, etc.).
The Executor identies the order of action execution. It
starts by executing all the VM adaptation actions (i.e., VM
5The forked version of Docker is available at: https://
github.com/deib-polimi/ecoware-dockercreates and terminates) through Cloud Driver . These ac-
tions are not application specic and they take a relatively
long time to execute. As soon as the new VMs (if any) are
up and running, the CNT adaptation actions are executed
through the ECoWare Agent s installed on each machine.
The order in which the CNT adaptation actions and Adap-
tation Hooks are executed depends on the dependencies
stored in the Executor's Topology Graph sub-module.
If a tiert1depends on tier t2, we start by executing the
CNT actions of t2, whilet1waits. The ECoWare Agent s in
t2proceed to execute their assigned CNT adaptation ac-
tions. If the CNT adaptations they must execute are a
create_container or a scale_container , once that action
has been completed the agent proceeds to execute that con-
tainer's on_node_scale hook.
When all the CNT adaptation actions and on_node_scale
hooks for all the containers in tier t2have completed, adap-
tation ont1can be re-activated, but only if t1does not
depend on any other tiers that are still performing adapta-
tion.
Adaptation for t1proceeds in the following order. Its
ECoWare Agents start by executing the CNT adaptation ac-
tions for the containers of t1, proceed to execute the on_no-
de_scale hooks for those containers, and conclude by exe-
cuting their on_dependency_scale hooks. This is possible
since all the tier's dependencies have been previously re-
solved. Once completed, if there are any tiers that, in turn,
depend ont1they will activate their adaptation, and so on.
6. EVALUATION
We evaluated our work by using two Web applications:
RUBiS and Pwitter. RUBiS [14] is a well-known Internet ap-
plication benchmark that simulates an on-line auction site.
Our deployment of RUBiS uses three tiers: a load balancer,
a scalable pool of JBoss application servers that run Java
Servlets, and a MySQL database. Pwitter was developed
in-house6. It is a simple Twitter-like social network that
stores pweets, i.e., texts that are not limited in length and
that are indexed by the polarity of their sentiment. Pwit-
ter is written in Python and is also a 3-tier application. It
has a load balancer, a scalable pool of Gunicorn application
servers, and a Redis database.
For our experiments we implemented two application-spe-
cic sensors; they capture the data required to calculate the
average response time of a JBoss Application Server and
of a Gunicorn Application server, respectively. We also im-
plemented two application-specic Adaptation Hooks to en-
able platform adaptation; these hooks dynamically recon-
gure JBoss and Gunicorn to better exploit the available
resources following industry best practices [8{10]. For ex-
ample, best practices for Gunicorn suggest that the number
of workers be (2numcores ) + 1.
The goal of our experiments was to maintain the aver-
age response time of the application servers below a cer-
tain threshold. After a proling phase, we set the thresh-
old (SLA) to 0 :6 seconds for both applications. Indeed,
both are able to sustain this value |under various kinds
of workloads| using from 1 to 10 cores (i.e., the maximum
amount of resources that we can aord for the experiments).
The experiments answer the following questions:
6Pwitter is available at https://github.com/deib-polimi/
pwitter.Question 1 : if we only use VMs, and no containers,
will ECoWare perform better or worse than the current
state of practice (i.e., AWS AutoScaling)?
Question 2 : if we take into account containers, will
ECoWare perform better or worse than with VMs only?
Performance is evaluated using the coresecond metric,
i.e., we calculate how many cores are used during the ex-
periments. The lower the value, the better the adaptation
works. To simulate a varying workload we used JMeter [11].
Question 1 . We compared ECoWare against Amazon
EC2's AutoScaling capabilities. We made this choice be-
cause it is, in practice, the most widely adopted scaling so-
lution. For both applications we focused on adapting the
business logic tiers. We created two AutoScaling groups on
EC2, one per application. These groups were congured to
scale from 1 to 10 t2.small VM instances (each instance had
1 CPU core and 2GB of memory). Both AutoScaling groups
were congured to use Amazon Elastic Load Balancers. The
databases were deployed on m4.xlarge instances (each in-
stance had 4 CPU cores and 16GB of memory). With this
setup and workload the databases were over-allocated and
never became the bottleneck during the experiments.
The EC2 AutoScaling groups were each given two poli-
cies. The rst added 1 VM instance when the average CPU
utilization, for the entire group, was over 90% for 1 inter-
val of 1 minute. The second removed 1 VM instance when
the average CPU utilization, for the entire group, was below
40% for 1 interval of 1 minute. These thresholds were cho-
sen following real-world best practices [12]. We also decided
to set the duration of the adaptation control to be as fast
as possible, to allow the AWS AutoScaling system to be as
quick as possible to react to workload changes. Note, how-
ever, that Amazon's AutoScaling does not activate a scaling
action if another action is executing. This means that, for
example, when a new VM is added we must wait for it to
turn on, boot-up, and be linked to the load balancer.
We measured that, on average, it took a VM 150 seconds
to complete its boot-up process; this was evaluated from
the launch of the VM creation command to when it had
successfully booted and linked to the Elastic Load Balancer.
This has two intertwined consequences. First, to preserve
the unity relative degree hypothesis (see Hypothesis 2 in
Section 4.1), the control period had to be longer than this
delay. Second, unless an unacceptably long period was se-
lected, a conservative control action (i.e., an alpha close to
one) was required to prevent oscillations or even instability.
This is not equally relevant if containers are used, allowing
for a lower alpha and for a more aggressive control action and
stronger disturbance rejection. For these reasons we param-
eterized our planner as follows: we set the control interval to
180 seconds, we measured the response time and averaged it
over 30 seconds, we set the planner to always read the most
up-to-date measured response time before computing the
next plan, and we set the single pole alpha to 0 :95. We also
dened the set point of the planner to be 0 :5 seconds, i.e.,
about 10% less than the SLA. This way the planner would
have been able to range near the set point without violating
the SLA. Finally, we stimulated the two applications with
three dierent workloads: low,medium , and high. All the
experiments lasted 75 minutes; the number of users for each
interval (in minutes) is shown in Table 1. We repeated the
experiments ve times.
(a) AWS/Pwitter/Low(b) AWS/Pwitter/Medium
(c) AWS/Pwitter/High(d) ECoWare/Pwitter/Low
(e) ECoWare/Pwitter/Medium(f) ECoWare/Pwitter/High
(g) AWS/RUBiS/Low(h) AWS/RUBiS/Medium
(i) AWS/RUBiS/High(j) ECoWare/RUBiS/Low
(k) ECoWare/RUBiS/Medium(l) ECoWare/RUBiS/High
(m) ECoWare/Pwitter/Float(n) ECoWare/CNT/RUBiS/High
(o) ECoWare/CNT/Pwitter/FloatFigure 6: Obtained results. (CNT stands for Con-
tainers).Table 1: Workloads: number of users per time in-
terval (in minutes).
Experiment 0-15 15-30 30-45 45-60 60-75
Pwitter/Low 10 20 40 20 10
Pwitter/Med 15 35 60 35 15
Pwitter/High 20 50 100 50 20
RUBiS/Low 50 200 400 200 400
RUBiS/Med 100 300 600 300 100
RUBiS/High 100 500 1000 500 100
Pwitter/Float 20 40 20 40 20
Figures 6(a)-(c) show AWS AutoScaling's behavior with
Pwitter stimulated with the three dierent workloads. This
behavior is compared against the experiments illustrated in
Figures 6(d)-(f), which used ECoWare's adaptation capabil-
ities. The horizontal dotted line is the SLA and the thin line
is the response time; they both refer to the left y-axis. The
thicker line represents the allocated cores, and refers to the
right y-axis. The metric coresecond corresponds to the
area under the thicker line (cores).
AWS AutoScaling clearly over-allocates resources. This
occurs even for lowworkloads and even if we used a very high
threshold for CPU utilization. This is because both JBoss
and Gunicorn tend to use nearly 100% of their allocated
CPUs, even with moderate workloads. Furthermore, this
value is also aected by how quickly a spike in the number
of users is reached.
ECoWare, on the other hand, allocates less cores on av-
erage. The SLA is only violated with the high workload
for around 5 minutes, which is understandable since the
approach is reactive. After a large spike ECoWare over-
allocates resources for one control interval and then slowly
deallocates them and converges to a stable value, as can be
seen in Figure 6(d) at minute 35 and in Figures 6(e)-(f) at
around minutes 20 and 35. Though the two example ap-
plications are completely dierent, we see similar results in
Figures 6(g)-(l), which focus on RUBiS. ECoWare never in-
curs in violations, while AWS over-allocates resources in all
the scenarios. AWS can only add a static number of VMs
when reacting to an increment in CPU utilization, so it is
easy to nd a workload (with high spikes) in which AWS
is too slow to allocate resources, causing various SLA vio-
lations to occur. We did not nd it useful to show such a
case, given the lack of space. Table 2 shows the results of
these experiments using the coresecond metric. ECoW-
are outperforms AWS by 107% on average when managing
Pwitter and by 49% on average when managing RUBiS.
Question 2 . With the second set of experiments we
wanted to assess the benet of using containers, instead of
focusing solely on VMs. To do this we compared ECoW-
are, as used previously, against a new deployment that used
an Amazon m4.2xlarge VM instance (8 cores and 32GB of
memory). On this machine we installed the dockerized ver-
sions of the business logic tiers of the two applications7. Our
hypothesis was that the two applications had dierent work-
loads, which means that there had to be moments in which
one application would request a lot of resources, while the
other would not. This allows us to exploit the advantages of
using containers. We only used one VM in our experiments
to make the benets of using containers emerge clearer.
7Dockerized RUBiS can be found at: https://hub.docker.
com/u/polimi/.Table 2: AWS vs ECoWare without containers. Val-
ues are in CPU coresecond .
Experiment AWS ECoWare Gain
Pwitter/Low 18810 7920 138%
Pwitter/Med 19965 9090 120%
Pwitter/High 20970 12930 62%
RUBiS/Low 15390 8970 72%
RUBiS/Med 16320 10830 50%
RUBiS/High 20265 16290 24%
Table 3: ECoWare with VMs vs ECoWare with con-
tainers. Values are in CPU coresecond .
Experiment VMs Containers Gain
Pwitter/Float 8685 6580 32%
RUBiS/High 16290 10210 60%
The use of containers allows ECoWare to have a faster
control rate since creating, updating, and terminating con-
tainers can be done in milliseconds (around 4 to 6 ms). We
parametrized the planner with a control interval of 20 sec-
onds, we measured the response time and averaged it over
10 seconds, we set the planner to always read the most up-
to-date measured response time before computing the next
plan, and we set the single pole alpha to 0 :45. We then
simulated two dierent workloads for Pwitter and RUBiS
simultaneously. For RUBiS we used the high workload, for
Pwitter we used the oat workload (see Table 1).
Figure 6(m) shows that ECoWare without containers vi-
olated the SLA on Pwitter and allocated a peak of 5 cores
at minute 50. Figure 6(o) shows that ECoWare with con-
tainers never violated the SLA and remained quite close to
the optimal resource allocation (1 core for 20 users, 2 cores
for 40 users). Similar results emerged with RUBiS. Work-
ing with VMs only (Figure 6(l)) required allocating up to
8 cores, while the use of containers (Figure 6(n)) allowed
us to allocate no more than 6 cores (the optimal alloca-
tion for 1000 users is 5 cores). This is due to the dierent
parametrization of the planner, and to the dierent control
intervals. Table 3 shows the results in detail. If we aggregate
the two experiments, ECoWare with containers outperforms
ECoWare with VMs by 46%.
To conclude, the experiments show that a high level of
CPU utilization does not always equate to a saturated sys-
tem. Indeed, both JBoss and Gunicorn continue to oper-
ate acceptably (i.e., they continue to satisfy their SLAs),
even when their CPU utilization is close to 100% (see Fig-
ure 6(a,g)). Furthermore, the experiments also show that,
in these two applications, memory saturation is not an issue.
6.1 Threats to Validity
Even though we performed our experiments on two appli-
cations built on dierent technologies, the two are similar
both in terms of domain and architectural design. They are
both CPU-bound, and in both cases the bottleneck lies in
the application tier. Nevertheless, these two applications
cover a wide range of real-world cases. Further research
will evaluate ECoWare with applications that follow a com-
pletely dierent architectural design (e.g., Map/Reduce, me-
dia streaming applications, etc.).
In our experiments we used the cloud infrastructure in two
dierent ways, depending on whether we were dealing with
VMs or with containers. When dealing with VMs we used
up to 10 EC2 instances per application; each VM had 1 core.When dealing with containers we used a single EC2 instance
with 8 cores and shared it between both applications. The
reason for using a VM with 8 cores is that they were enough
to satisfy the SLAs, given the used workloads.
The reason for this dierence is that to truly take advan-
tage of container technology one must have multiple applica-
tions running (with dierent workloads) on multiple shared
VMs. This kind of setup is becoming increasingly common
in companies, due to the success of micro-service architec-
tures. Furthermore, to eciently manage containers, one
should have a number of cores (on each shared VM) that is
suciently large so that CNT adaptations can be eective.
Ideally, one should have (at least) a number of cores that is
equal to the number of application tiers being managed on
that VM plus one.
Memory was not an issue in these experiments. Our con-
troller only computes the amount of cores required per tier
and requests an appropriate amount of memory units based
on this computation, i.e., the controller requests 1GB of
RAM per core. This is always possible because EC2's VMs
always provide more GBs of RAM than cores. The impor-
tance of memory must be re-evaluated with applications that
are memory-bound; this will be part of our future work.
7. RELATED WORK
The solution presented in this paper nds \competitors"
in both industry and academia.
Amazon recently presented the EC2 Container Service [2],
which allows one to deploy Docker containers inside an EC2
VM cluster. ECoWare is more open and exible: it pro-
vides full access to Docker to build the Adaptation Hooks
mechanism and allows for the vertical scaling of containers.
Apache Mesos [3] is a cluster manager that supports Dock-
er containers. It is a middleware platform that hides the
complexity of a distributed infrastructure and provides ap-
plication level APIs for many services (e.g., Hadoop, Spark,
and Elastic Search) for resource management and for sched-
uling. Google Kubernetes [7] is another container cluster
manager. It oers a comprehensive solution for deploying
applications using containers and oers an API for scaling
and replicating groups of containers. Both Mesos and Ku-
bernetes focus on the infrastructure layer and lack compre-
hensive multi-level adaptation. ECoWare may embed these
solutions as alternative ways to manage containers and it
could complement them with advanced adaptation capabil-
ities. Cloudify [4] can only drive infrastructure auto-scaling
based on a simple rule-based solution very similar to Ama-
zon Autoscaling.
As for research initiatives, Brun et al. [23] witness the
importance of control loops and autonomic computing when
dealing with complex software systems, while Dustdar et
al. [27] provide an interesting denition of elastic computing.
They advocate that elastic processes should be modeled on
three metrics: resources, cost, and quality. Our work focuses
on resources and cost, and partially on quality. Our main
goal is to keep response time under a certain threshold, while
quality provides more facets and ECoWare could be further
extended to support them in the future.
Dierent approaches deal with the problem of resource al-
location. For example, Hu et al. [33] propose a resource allo-
cation system for cloud computing. They render application
environments (AEs) by means of queuing models and use a
global arbiter to allocate resources among AEs. Ardagna etal. [18] present a game theoretical approach for the runtime
management of the infrastructure of a IaaS provider that
rents resources (VMs) to multiple SaaS providers to fulll
their SLAs. They formulate the Generalized Nash Equilib-
rium problem and prove the existence of at least one equi-
librium. Padala et al. [46] present a hierarchical controller
that allocates resources to multiple multi-tier applications in
data centers. They show how the controller can improve the
utilization of servers and help applications meet their SLAs.
Our approach is dierent with respect to these works, and
to other similar ones (e.g., [25,37,42,44,53]), because of the
use of containers and the ability to exploit platform adap-
tation. Furthermore, our solution adopts the perspective of
cloud users, with no access to the policies that regulate the
behavior of the hypervisor.
Finally, as for feature adaptation, Klein et al. [39] intro-
duce the Brownout adaptation paradigm based on optional
code that can be dynamically (de-)activated through a con-
troller. As already said, we are condent that our Adapta-
tion Hooks mechanism can also support feature adaptation
and provide a similar solution, but this is not the focus of
this paper.
8. CONCLUSIONS AND FUTURE WORK
The paper proposed a novel self-adaptation framework
based on: (i) VM- and container-based resource manage-
ment, (ii) coordinated infrastructure and platform adapta-
tion, and (iii) a planner that implements a discrete-time
feedback controller. Experiments demonstrate that ne-
grained adaptation capabilities can greatly improve perfor-
mance when autoscaling Cloud-based Web applications.
Our future work comprises the integration of feature adap-
tation by extending the adaptation hook mechanism, an ex-
tension of the planner to make it work hierarchically with
respect to the controlled resources, an even ner-grained so-
lution to control the CPU cores allocated to a container, and
further evaluation on more case studies of dierent kinds.
Acknowledgments
We would like to thank Lorenzo Aetti and Dmitrii Ste-
bliuk for their contributions, and Danilo Ardagna for his
most valuable feedback. The work presented in this pa-
per has been partially supported by project EEB - Edi-
ci A Zero Consumo Energetico In Distretti Urbani Intelli-
genti (Italian Technology Cluster For Smart Communities)
- CTN01 00034 594053.
9. REFERENCES
[1] Amazon EC2 Autoscaling.
https://aws.amazon.com/autoscaling/.
[2] Amazon EC2 Container Service (ECS).
https://aws.amazon.com/ecs/.
[3] Apache Mesos. http://mesos.apache.org.
[4] Cloudify. Cloud Orchestration and Automation Made
Easy. http://getcloudify.org.
[5] Docker. https://docker.com.
[6] Docker: Container Security White Paper. https:
//www.docker.com/sites/default/les/WP Intro%
20to%20container%20security 03.20.2015%20(1).pdf.
[7] Google Kubernetes. http://kubernetes.io.
[8] Gunicorn Documentation: How Can I Change the
Number of Workers Dynamically?http://docs.gunicorn.org/en/stable/faq.html#
how-can-i-change-the-number-of-workers-dynamically.
[9] Gunicorn Documentation: How Many Workers?
http://docs.gunicorn.org/en/stable/design.html#
how-many-workers.
[10] JBoss Perfomance Tuning Guide.
https://www.redhat.com/f/pdf/JB JEAP5
PerformanceTuning wpweb.pdf.
[11] JMeter. https://jmeter.apache.org.
[12] Netlix: AutoScaling in the Amazon Cloud.
http://techblog.netix.com/2012/01/
auto-scaling-in-amazon-cloud.html.
[13] RabbitMQ. Robust Messaging for Applications.
http://rabbitmq.com.
[14] RUBiS. The Rice University Bidding System.
http://rubis.ow2.org.
[15] Vagrant. https://www.vagrantup.com.
[16] Announcing Amazon EC2 Container Service (ECS) -
Container Management for the AWS Cloud.
https://aws.amazon.com/blogs/aws/
cloud-container-management, 2014.
[17] Microsoft - Windows Containers.
https://msdn.microsoft.com/en-us/virtualization/
windowscontainers/about/about overview, 2016.
[18] D. Ardagna, B. Panicucci, and M. Passacantando. A
Game Theoretic Formulation of the Service
Provisioning Problem in Cloud Systems. In
Proceedings of the International Conference on World
Wide Web , pages 177{186. ACM, 2011.
[19] M. Armbrust, A. Fox, R. Grith, A. D. Joseph,
R. Katz, A. Konwinski, G. Lee, D. Patterson,
A. Rabkin, I. Stoica, and M. Zaharia. A View of
Cloud Computing. Communications of the ACM ,
53(4):50{58, 2010.
[20] L. Baresi and S. Guinea. Event-Based Multi-level
Service Monitoring. In Proceedings of the 20th
International Conference on Web Services , pages
83{90, 2013.
[21] T. Binz, G. Breiter, F. Leyman, and T. Spatzier.
Portable Cloud Services Using TOSCA. IEEE Internet
Computing , 16(3):80{85, 2012.
[22] P. Bodk, R. Grith, C. Sutton, A. Fox, M. Jordan,
and D. Patterson. Statistical Machine Learning makes
Automatic Control Practical for Internet Datacenters.
InProceedings of the 2009 conference on Hot topics in
Cloud Computing , pages 12{12, 2009.
[23] Y. Brun, G. D. M. Serugendo, C. Gacek, H. Giese,
H. Kienle, M. Litoiu, H. M uller, M. Pezz e, and
M. Shaw. Engineering Self-adaptive Systems through
Feedback Loops. In Software Engineering for
Self-adaptive Systems , pages 48{70. Springer, 2009.
[24] R. N. Calheiros, R. Ranjan, A. Beloglazov, C. A.
De Rose, and R. Buyya. CloudSim: a toolkit for
Modeling and Simulation of Cloud Computing
Environments and Evaluation of Resource
Provisioning Algorithms. Software: Practice and
Experience , 41(1):23{50, 2011.
[25] R. Calinescu, C. Ghezzi, M. Kwiatkowska, and
R. Mirandola. Self-adaptive Software needs
Quantitative Verication at Runtime.
Communications of the ACM , 55(9):69{77, 2012.[26] J. S. Chase, D. C. Anderson, P. N. Thakar, A. M.
Vahdat, and R. P. Doyle. Managing Energy and
Server Resources in Hosting Centers. In ACM
SIGOPS Operating Systems Review , volume 35, pages
103{116. ACM, 2001.
[27] S. Dustdar, Y. Guo, B. Satzger, and H.-L. Truong.
Principles of Elastic Processes. IEEE Internet
Computing , (5):66{71, 2011.
[28] EsperTech. Complex event processing.
http://esper.codehaus.org, 2010.
[29] W. Felter, A. Ferreira, R. Rajamony, and J. Rubio.
An Updated Performance Comparison of Virtual
Machines and linux Containers. In Proceedings of the
2015 IEEE International Symposium on Performance
Analysis of Systems and Software , pages 171{172.
IEEE, 2015.
[30] A. Gandhi, P. Dube, A. Karve, A. Kochut, and
L. Zhang. Adaptive, Model-driven Autoscaling for
Cloud Applications. In Proceedings of the 11th
International Conference on Autonomic Computing ,
pages 57{64. USENIX Association, 2014.
[31] Google Cloud Platform. An Introduction to
Containers, Kubernetes, and the Trajectory of Modern
Cloud Computing.
https://cloudplatform.googleblog.com/2015/01/
in-coming-weeks-we-will-be-publishing.html, 2015.
[32] P. Hoenisch, S. Schulte, S. Dustdar, and S. Venugopal.
Self-adaptive Resource Allocation for Elastic Process
Execution. In Proceedings of the Sixth IEEE
International Conference on Cloud Computing , pages
220{227. IEEE, 2013.
[33] Y. Hu, J. Wong, G. Iszlai, and M. Litoiu. Resource
Provisioning for Cloud Computing. In Proceedings of
the 2009 Conference of the Center for Advanced
Studies on Collaborative Research , pages 101{111.
IBM Corp., 2009.
[34] P. Jamshidi, A. Ahmad, and C. Pahl. Autonomic
Resource Provisioning for Cloud-based Software. In
Proceedings of the 9th International Symposium on
Software Engineering for Adaptive and Self-Managing
Systems , pages 95{104. ACM, 2014.
[35] P. Jamshidi, A. M. Sharioo, C. Pahl, A. Metzger, and
G. Estrada. Self-Learning Cloud Controllers: Fuzzy
Q-Learning for Knowledge Evolution. In Proceedings
of the 2015 International Conference on Cloud and
Autonomic Computing , pages 208{211. IEEE, 2015.
[36] JavaWorld. How Containers Change Everything.
http://www.javaworld.com/article/2940858/
cloud-computing/how-containers-change-everything.
html, 2015.
[37] J. W. Jiang, T. Lan, S. Ha, M. Chen, and M. Chiang.
Joint VM Placement and Routing for Data Center
Trac Engineering. In Proceedings of the 31st IEEE
International Conference on Computer
Communications , pages 2876{2880. IEEE, 2012.
[38] J. O. Kephart and D. M. Chess. The Vision of
Autonomic Computing. Computer , 36(1):41{50, 2003.
[39] C. Klein, M. Maggio, K.-E. Arz en, and
F. Hern andez-Rodriguez. Brownout: Building more
Robust Cloud Applications. In Proceedings of the 36th
International Conference on Software Engineering ,
pages 700{711. ACM, 2014.[40] J. Li, J. Chinneck, M. Woodside, M. Litoiu, and
G. Iszlai. Performance Model Driven QoS Guarantees
and Optimization in Clouds. In Proceedings of the
2009 ICSE Workshop on Software Engineering
Challenges of Cloud Computing , pages 15{22. IEEE
Computer Society, 2009.
[41] X. Liu, X. Zhu, S. Singhal, and M. Arlitt. Adaptive
Entitlement Control of Resource Containers on Shared
Servers. In Proceedings of the 9th IFIP/IEEE
International Symposium on Integrated Network
Management , pages 163{176. IEEE, 2005.
[42] X. Meng, V. Pappas, and L. Zhang. Improving the
Scalability of Data Center Networks with Trac-aware
Virtual Machine Placement. In Proceedings of the
2010 IEEE International Conference on Computer
Communications , pages 1{9. IEEE, 2010.
[43] D. Merkel. Docker: Lightweight Linux Containers for
Consistent Development and Deployment. Linux
Journal , 2014(239):2, 2014.
[44] H. Nguyen, Z. Shen, X. Gu, S. Subbiah, and
J. Wilkes. Agile: Elastic Distributed Resource Scaling
for Infrastructure-as-a-Service. In Proceedings of the
2013 USENIX International Conference on Automated
Computing , 2013.
[45] A. Y. Nikravesh, S. A. Ajila, and C.-H. Lung. Towards
an Autonomic Auto-scaling Prediction System for
Cloud Resource Provisioning. In Proceedings of the
10th International Symposium on Software
Engineering for Adaptive and Self-Managing Systems ,
pages 35{45. IEEE Press, 2015.
[46] P. Padala, K. G. Shin, X. Zhu, M. Uysal, Z. Wang,
S. Singhal, A. Merchant, and K. Salem. Adaptive
Control of Virtualized Resources in Utility Computing
Environments. In Proceedings of the 2nd ACM
SIGOPS/EuroSys European Conference on Computer
Systems , pages 289{302. ACM, 2007.[47] P. Pawluk, B. Simmons, M. Smit, M. Litoiu, and
S. Mankovski. Introducing STRATOS: A Cloud
Broker Service. In Proceedings of the Fifth IEEE
International Conference on Cloud Computing , pages
891{898. IEEE, 2012.
[48] Rackspace. Carina by Rackspace Simplies Containers
with Easy-To-Use, Instant-On Native Container
Environment. http://goo.gl/Khf84h, 2015.
[49] W. Rankothge, J. Ma, F. Le, A. Russo, and J. Lobo.
Towards Making Network Function Virtualization a
Cloud Computing Service. In Proceedings of the 13rd
IFIP/IEEE International Symposium on Integrated
Network Management , pages 89{97. IEEE, 2015.
[50] S. Schulte, D. Schuller, P. Hoenisch, U. Lampe,
S. Dustdar, and R. Steinmetz. Cost-driven
Optimization of Cloud Resource Allocation for Elastic
Processes. International Journal of Cloud Computing ,
1(2):1{14, 2013.
[51] F. Seracini, M. Menarini, I. Krueger, L. Baresi,
S. Guinea, and G. Quattrocchi. A Comprehensive
Resource Management Solution for Web-based
Systems. In Proceedings of the 11th International
Conference on Autonomic Computing . USENIX, 2014.
[52] S. Soltesz, H. P otzl, M. E. Fiuczynski, A. Bavier, and
L. Peterson. Container-based Operating System
Virtualization: A Scalable, High-performance
Alternative to Hypervisors. In Proceedings of the 2nd
ACM SIGOPS/EuroSys European Conference on
Computer Systems , volume 41, pages 275{287. ACM,
2007.
[53] Z. Xiao, W. Song, and Q. Chen. Dynamic Resource
Allocation using Virtual Machines for Cloud
Computing Environment. IEEE Transactions on
Parallel and Distributed Systems , 24(6):1107{1117,
2013.
View publication stats