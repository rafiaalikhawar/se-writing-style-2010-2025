Syntactic and Semantic Differencing
for Combinatorial Models of Test Designs
Rachel Tzoref-Brill
School of Computer Science, Tel Aviv University
and IBM Research, IsraelShahar Maoz
School of Computer Science, Tel Aviv University
Abstract —Combinatorial test design (CTD) is an effective test
design technique, considered to be a testing best practice. CTD
provides automatic test plan generation, but it requires a manual
deﬁnition of the test space in the form of a combinatorial
model. As the system under test evolves, e.g., due to iterative
development processes and bug ﬁxing, so does the test space, and
thus, in the context of CTD, evolution translates into frequent
manual model deﬁnition updates. Manually reasoning about the
differences between versions of real-world models following such
updates is infeasible due to their complexity and size. Moreover,
representing the differences is challenging.
In this work, we propose a ﬁrst syntactic and semantic
differencing technique for combinatorial models of test designs.
We deﬁne a concise and canonical representation for differences
between two models, and suggest a scalable algorithm for auto-
matically computing and presenting it. We use our differencing
technique to analyze the evolution of 42 real-world industrial
models, demonstrating its applicability and scalability. Further, a
user study with 16 CTD practitioners shows that comprehension
of differences between real-world combinatorial model versions is
challenging and that our differencing tool signiﬁcantly improves
the performance of less experienced practitioners. The analysis
and user study provide evidence for the potential usefulness of
our differencing approach.
Our work advances the state-of-the-art in CTD with better
capabilities for change comprehension and management.
I. I NTRODUCTION
One of the effective techniques for coping with the ver-
iﬁcation challenge of increasingly complex software systems
is Combinatorial Test Design (CTD), a.k.a. combinatorial test-
ing [4], [8], [10], [13], [17], [35], [36]. CTD requires a manual
deﬁnition of the test space in the form of a combinatorial
model , consisting of a set of parameters, their respective
values, and constraints on the value combinations. A valid test
in the test space is deﬁned to be an assignment of one value to
each parameter that satisﬁes the constraints. A CTD algorithm
automatically constructs a subset of the set of valid tests so that
it covers all valid value combinations of every tparameters,
where tis usually a user input. This systematic selection of
tests is based on empirical data that shows that in most cases,
the appearance of a bug depends on the interaction between a
small number of features of the system under test [10], [18],
[31].
An under-explored challenge for wide deployment of CTD
in industry is the manual process for modeling and maintaining
the test space. In practice, creating a CTD model is not a one
time effort; when the system under test evolves, so shouldthe models. In face of the move to agile methodology and to
continuous delivery mode, where software development cycles
are getting ever shorter, test design needs to frequently adjust
to changes, which in the context of CTD means frequent
model deﬁnition updates. However, although in these settings
technologies for handling model changes are increasingly
necessary, we are unaware of any work that reasons about
the evolution process of combinatorial models or provides
tool support for it. A recent survey by Nie et al. [26] reveals
that only around 5% of the publications on CTD explore the
crucial modeling process, and the topic of model maintenance
is not even mentioned in [26]. Close to 40 CTD tools are listed
in [27], e.g., PICT [9], ACTS [19], Jenny [16], and AETG [6],
but to the best of our knowledge, none of these existing tools
provides indication on the effect of change operations on the
model, i.e., what is the relation between the original model
and the new one, and how they differ. Without such tool
support, the practitioner is “left in the dark” as to whether
the performed change will result in the intended effect, and
what other changes may be required. When a series of such
change operations is performed, as is typically the case, this
problem exacerbates.
In this work, we propose a ﬁrst approach for syntactic and
semantic differencing of combinatorial models. Computing
and representing the differences between models are chal-
lenging tasks. First, some parts of a model can implicitly
change due to explicit changes to other parts, and thus the
resulting differences will not be revealed by a purely syntactic
comparison. For example, adding a new value to a parameter
that appears in the constraints can result in new tests being
deﬁned as invalid, even without any explicit changes to the
constraints. Second, there can be different syntactic represen-
tations to the same model (i.e., representations that result in the
same set of valid tests). Speciﬁcally, it is challenging to clearly
and concisely represent differences between constraints, which
are propositional logic formulas over multi-valued parameters.
Finally, as our evidence from the ﬁeld shows, real-world
models can be huge, thus computing a semantic differencing
must scale well in order to be used in practice.
Our work addresses these challenges by proposing a canon-
ical representation of a combinatorial model, in terms of the
value combinations that are excluded from its set of valid
tests. The importance of this representation is in its use
as a basis for a comparison between models. Other non-
2017 IEEE/ACM 39th International Conference on Software Engineering
 
DOI 10.1109/ICSE.2017.63619
2017 IEEE/ACM 39th International Conference on Software Engineering
1558-1225/17 $31.00 © 2017 IEEE
DOI 10.1109/ICSE.2017.63621
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. canonical representations cannot be used for comparison. Our
differencing approach consists of two parts. The syntactic
differencing shows the additions and removals of parameters,
values, and constraints. The semantic differencing concisely
computes and presents the differences in the set of valid tests,
based on the canonical representation of the models.
To scale the computation to real-world model sizes, we use
an efﬁcient representation of the sets of valid tests and their
differences, which is based on Binary Decision Diagrams [3],
a compact data structure for representing and manipulating
Boolean functions.
It is important to note that differencing of CTD models
isnot a mere theoretical exercise. First, the model, and not
the test suite that is directly and automatically derived from
it, is the main artifact that CTD test designers create, read,
revise, and maintain. Second, the cost of an incorrect model
update can be very high: if the model change is incomplete
then new tests generated from the model will not adequately
cover the software change; if the model change is incorrect it
may result in generating redundant or erroneous tests. Finally,
as we will demonstrate in Section III, CTD model updates
can be quite tricky due to the implicit dependencies between
the different parts of the model. Thus, in this work we focus
on differencing at the model level. Our differencing technique
serves as a review tool for practitioners to verify that their
model updates are complete and correct.
We implemented the differencing technique within the
industrial-strength commercial CTD tool IBM Functional Cov-
erage Uniﬁed Solution (IBM FOCUS) [14], [30]. IBM FOCUS
has been in use already for several years by hundreds of CTD
practitioners inside and outside of IBM.
To evaluate the applicability of our ideas and implemen-
tation to real-world models and their versions, we applied
our differencing technique to 42 real-world industrial models
with a total of 107 versions (2-5 versions per model, 65
commits). Our analysis reveals that while real-world commits
of combinatorial models of test designs tend to be large
and complex in practice, our approach provides acceptable
performance times in almost all cases.
We further evaluated the effect of our differencing technique
on users by conducting a user study with 16 CTD practitioners
on two real-world models, each consisting of two real-world
versions. The results show that comprehension of differences
between real-world combinatorial model versions is challeng-
ing and that our differencing tool improves the performance of
practitioners. This was most evident for the 8 less experienced
practitioners, whose score improved by over 40 percent when
using our differencing tool.
To conclude, our contributions are as follows: a ﬁrst ap-
proach for syntactic and semantic differencing of combinato-
rial models of test designs; a scalable implementation for our
differencing technique, implemented in an industrial-strength
commercial CTD tool; the application of the new differencing
technique to 42 real-world industrial models with a total of
107 versions; and a user study with CTD practitioners that
shows that our differencing technique signiﬁcantly improvesthe performance of less experienced practitioners in correct
comprehension of differences between real-world model ver-
sions. The proposed differencing technique advances the state-
of-the-art in CTD with new tools for change comprehension
and management.
II. B ACKGROUND
We provide background on combinatorial models and their
semantics and on the use of binary decision diagrams to
represent them.
Combinatorial Models and Their Semantics . A combinato-
rial model is deﬁned as follows. Let P={p1,...,p n}be a
labelled set of parameters, V={V1,...,V n}a labelled set
of ﬁnite value sets, where Viis the set of values for pi, and
Ca set of Boolean propositional constraints over P. A test
(v1,...,v m), where∀i,vi∈Vi, is a tuple of assignments to
the parameters in P.
The semantics used in practice by CTD tools [27] is Boolean
semantics. In this semantics, a valid test is a test that satisﬁes
all constraints in C. The semantics of the model is the set of
all its valid tests, denoted by S(P, V, C ).
Using Binary Decision Diagrams to Represent Combina-
torial Models . In [30], a compact representation of com-
binatorial models using Binary Decision Diagrams (BDDs)
was presented. BDDs [3] are a compact data structure for
representing and manipulating Boolean functions, commonly
used in formal veriﬁcation [5] and in logic synthesis [21].
[30] utilizes the efﬁcient computation of Boolean operations
on BDDs such as negation, conjunction and disjunction, to
compute the BDD representing the set of valid tests from
the user-speciﬁed constraints. The set of invalid tests in the
model is represented using the conjunction of the BDDs for
each of the constraints. Multi-valued parameters are handled
using standard Boolean encoding and reduction techniques
to BDDs [25]. The set of valid tests is represented by the
negation of the BDD for the invalid tests, conjunct with a BDD
that represents the legal multi-valued to Boolean encodings of
the parameter values. This BDD-based representation of the
combinatorial model is the basis for the implementation of
our semantic differencing.
III. R UNNING EXAMPLE AND OVERVIEW
We start off with an example and overview of our work. The
presentation in this section is semi-formal. Formal deﬁnitions
appear in Section IV.
Table I depicts the parameters, values, and constraints of
a combinatorial model for an on-line shopping system, which
we use as a running example. The model deﬁnes the test space
and which tests in it are valid. For example, the test (IS=
InStock ,OS=Air,DT=Immediate )is valid, while the
test(IS=InStock ,OS=Ground ,DT=Immediate )is
invalid.
Below we follow a series of updates to the model, inspired
by similar updates we have seen in the evolution of real-world
models, adding a value, updating a constraint, splitting a pa-
rameter etc. We describe the updates, discuss their semantics,
620
622
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. TABLE I: Example on-line shopping model
Parameter Values
ItemStatus (IS) InStock, OutOfStock, NoSuchProduct
OrderShipping (OS) Air, Ground
DeliveryTimeframe (DT) Immediate, OneWeek, OneMonth
Constraints
DT=Immediate →OS=Air
DT=OneMonth →OS=Ground
and demonstrate how our differencing solution handles them.
The updates are relatively small and local. We use them to
demonstrate the basic principles of our analysis, as well as
the challenges associated with differencing, even with such
seemingly simple updates.
From V1 to V2. Following the addition of a new feature to the
system, a practitioner added the value Sea to the parameter
OrderShipping , and committed a second version of the
model. One may view this form of change as an extension ,
where parts are added to the model to describe the test space
in more detail. Figure 1 depicts the result of our differencing
analysis between the ﬁrst two versions of the model.
The differencing consists of two parts, syntactic and seman-
tic. The syntactic part reports the changes that were made to
the parameters, constraints, and values. As expected, in our
example it reports solely on the addition of the Sea value
to the OS parameter. The second, semantic part, reports the
changes in the set of valid tests, in terms of its strongest
exclusions . A strongest exclusion is a combination that is
excluded by the constraints in the model (and thus does not
appear in any valid test), but whose every strict subset is
included in the model. The motivation for showing the changes
to the strongest exclusions is twofold. First, they constitute a
canonical representation of the test space, as we will show in
Section IV, and therefore can be used as a basis for comparison
between different test spaces. Second, they represent the tests
that were excluded from the model in a concise form, and thus
can be used to provide information to the practitioner about
the differences between the test spaces. Note that while the
constraints were not explicitly updated following the addition
of the Sea value, i.e., syntactically the constraints are identical
in the two versions, our analysis reveals that two new strongest
exclusions were added to the model: (OS=Sea,DT=
Immediate ), and (OS=Sea,DT=OneMonth ).
From V2 to V3. When reviewing these added exclusions, the
practitioner may have realized that the constraints need to be
updated as well, in order to reﬂect the intended use of the new
feature. Thus, she deleted the second constraint and wrote a
new one instead: DT=OneMonth →OS=Ground∨OS=
Sea , and committed a third version of the model. When the
differencing analysis is performed between the third and the
second versions, the strongest exclusion (OS=Sea,DT=
OneMonth )will be marked as removed, since it is no longer
an exclusion in the third version. One may view this form of
change, where a strongest exclusion becomes included in thetest space, without being part of a new strongest exclusion, as
acorrection .
From V3 to V4. After further inquiries, the practitioner real-
ized that delivery time frame of one month actually consists
of two different values, 6To10WorkingDays and
Over10WorkingDays , which represent two separate log-
ical paths of the application under test. Thus, she replaced
theOneMonth value with these two values, deleted the
second constraint and wrote a new one instead: DT =
6To10WorkingDays ∨DT=Over10WorkingDays →
OS=Ground ∨OS=Sea ,and committed a fourth version
of the model. Figure 2 depicts the result of our differencing
analysis between the third and fourth versions of the model.
One may view this form of change, where a parameter
or a value is divided into several different cases, as a split .
The split of the value is displayed on the left side, and the
split of the strongest exclusion is displayed on the right side,
where the original one is removed and replaced with new
strongest exclusions, one for each case. The split pattern is
easy to detect when viewing the changes to the strongest
exclusions, whereas it becomes less obvious when viewing
the removed and added constraints. Furthermore, a constraint
can have different syntactic representations that are seman-
tically equivalent. For example, the second constraint in the
fourth version could have been DT/negationslash=Immediate ∧DT/negationslash=
OneWeek →OS=Ground ∨OS=Sea , resulting in a
syntactically different yet semantically equivalent model. The
model with this representation of the second constraint would
completely hide the fact that it splits OneMonth into two new
values, as these values do not even appear in the constraint.
Moreover, this version of the constraint can be used also in the
third version of the model, in which case no explicit changes
would have been made to the constraints when moving to the
fourth version, though the split of the strongest exclusions
would still occur.
IV . F ORMAL SOLUTION FOR MODEL DIFFERENCING
We now formally present our proposed differencing tech-
nique for combinatorial models, followed by a description of
our algorithm for computing it. Throughout this section we
will use the notations deﬁned in Section II and demonstrate
the ideas on the running example presented in Section III.
A. Model Differencing Deﬁnitions
Given a combinatorial model S(P, V, C ), for every parame-
terp∈P, we deﬁne p.name to be its identiﬁer. Similarly, for
every Vi∈V,v.name denotes the identiﬁer of every value
v∈Vi. We deﬁne corresponding sets for the identiﬁers of the
parameters and their values, PN={p.name|p∈P}and
∀Vi∈V,VNi={v.name|v∈Vi}. For every constraint
c∈C, we deﬁne c.expr to be its Boolean expression. We
deﬁne the set CE={c.expr|c∈C}for the expressions of
the constraints.
Given two models S1(P1,V1,C1)and S2(P2,V2,C2),
we deﬁne a syntactic differencing Diffsyn(S1,S2)and a
621
623
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. Fig. 1: Differencing between the ﬁrst and second versions. On the left, the syntactic differencing of parameters, constraints,
and values. On the right, the semantic differencing of strongest exclusions.
Fig. 2: Differencing between the third and fourth versions. On the left, the syntactic differencing of parameters, constraints,
and values. On the right, the semantic differencing of strongest exclusions.
complete semantic differencing Diffcsem(S1,S2)between the
two models as follows.
1) Syntactic Differencing: Diffsyn(S1,S2)consists of the
following parts:
1) Parameter additions: {p|p.name∈PN1\PN2}
2) Parameter removals: {p|p.name∈PN2\PN1}
3) Constraint additions: {c|c.expr∈CE1\CE2}
4) Constraint removals: {c|c.expr∈CE2\CE1}
5) V alue additions:/uniontext
pi∈P1,pj∈P2{v|v∈V1i∧v.name∈
VN1
i\VN2
j∧pi.name =pj.name}
6) V alue removals:/uniontext
pi∈P1,pj∈P2{v|v∈V2j∧v.name∈
VN2
j\VN1
i∧pi.name =pj.name}
For example, the left side of Figure 2 presents the syntactic
differencing between V3and V4of the on-line shopping
model, which consists of the addition of two values to the
common parameter DT, the removal of one value from it, no
parameter additions or removals, the addition of one constraint,
and the removal of another.
2) Semantic Differencing: While syntactic differencing rea-
sons about the syntactic changes in the parameters and values,complete semantic differencing reasons about all the differ-
ences in the test space of S1andS2.
To enable a comparison of the two test spaces, we ﬁrst
deﬁne a concise and canonical representation of a test space.
The representation we propose is based on the notion of
strongest exclusions . A strongest exclusion is a combination
that is excluded by the constraints in the model (and thus does
not appear in any valid test), but whose every strict subset is
included in the model and thus appears in at least one valid
test. The set of strongest exclusions of a model, together with
the set of parameters and their values, uniquely deﬁne the test
space of the model.
Theorem 4.1: LetSE be the set of strongest exclusions
of a combinatorial model S(P, V, C ). Then (P, V, SE )is a
canonical representation of S(P, V, C ).
Proof: We will show that for two models S1(P, V, C1)
andS2(P, V, C2),i fS1=S2(they have the same set of valid
tests), then SE1=SE2. Assume to the contrary, i.e., that
there exists a strongest exclusion e∈SE1such that e/negationslash∈SE2.
Then it follows from the deﬁnition of a strongest exclusion that
one of the following holds: (1) estrictly contains a strongest
exclusion e/prime∈SE2. Since S1=S2, it must hold that e/prime
622
624
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. is excluded from S1, contrary to our initial assumption that
e∈SE1. (2) eis not an exclusion in S2, which means there
is a test t∈S2that contains e. Since S1=S2, it holds that
t∈S1, contrary to our initial assumption that e∈SE1.
For example, for the on-line shopping model from Sec-
tion III, SEV1={(OS=Air,DT=OneMonth ),(OS=
Ground ,DT =Immediate )}. Note that each strongest
exclusion represents multiple complete tests that are excluded
from the model. Note also that each constraint deﬁnes one or
more strongest exclusions.
Diffcsem(S1,S2)consists of the following parts:
1) Strongest exclusion additions: {e|e∈SE1\SE2};
2) Strongest exclusion removals: {e|e∈SE2\SE1}.
For example, for the on-line shopping model, SEV2=
SEV1∪{(OS=Sea,DT=Immediate ),(OS=Sea,DT=
OneMonth )}. Hence, Diffcsem(V2,V1)contains the latter
two strongest exclusions, as depicted on the right side of
Figure 1.
The canonical representation of a model using strongest
exclusions allows us to deﬁne the completion level (CL) of a
version of a model, as well as of a commit (differencing two
model versions). The CL of a model version is the maximal
arity of a single strongest exclusion. It indicates the maximal
depth of exclusions that needs to be explored in order to reach
a complete canonical representation of the model.
In our running example, the CL of V5is 3; V5’s canonical
representation includes no strongest exclusions of arity larger
than 3.
CL is extended naturally from a single model version to the
comparison of a commit consisting of two model versions.
Speciﬁcally, the CL of a commit is the maximum between the
CLs of the two model versions, before and after the commit.
It represents an upper bound on the maximal number of
parameter values appearing in all strongest exclusions required
for presenting all the commit differences. The CL of a commit
is only an upper bound on the largest strongest exclusion that
will appear in the diff, because the diff contains only strongest
exclusions that are not common to both model versions. As
in the single model case, we use the CL of a commit as an
indicator for the maximal depth that needs to be explored in
order to reach a complete differencing between two model
versions. In Section IV-B, we present a way to compute the
CL of a model (and of a commit).
While presenting the differences in terms of strongest exclu-
sions is purely semantic and independent of the syntactic rep-
resentation of the model, it is also beneﬁcial to link semantic
information to syntactic information. As shown in Figures 1
and 2, we achieve this information linking by mapping the
strongest exclusions that appear in the differencing back to
their excluding constraints, as speciﬁed by the CTD practi-
tioner. Each strongest exclusion in Diffcsem(S1,S2)serves as
a witness for the test space change induced by its excluding
constraint under the changes to the model parameters and
values. For example, in Figure 1, the strongest exclusion
(OS=Sea,DT=Immediate )is mapped to its excludingconstraint DT=Immediate →OS=Air . The information
that this constraint is responsible for the added strongest
exclusion is particularly valuable in this case, since no explicit
changes were made to the constraints in version V2to exclude
this combination, following the addition of the Sea value.
Discussion of Alternatives. A rather na ¨ıve approach for
(partial) semantic differencing could have been to present
complete tests that are valid in one version of the model and
not in the other. One problem with this approach is that since
the versions may have different parameters and values, the
question whether a test in one version is valid in another
version may not be well deﬁned, i.e., the constraints in one
version may refer to parameters and values that do not exist in
the other version, and vice versa. Another problem with this
na¨ıve approach is that it must be partial, since the number of
such complete tests may be huge. Our solution avoids these
two problems. First, by computing the strongest exclusions
in each model, and then comparing the two resulting sets of
value combinations, our differencing is well deﬁned. Second,
since each strongest exclusion represents numerous complete
tests that are excluded from the model, our solution concisely
represents all the test space differences between the two
models, and is orders of magnitude smaller than one that relies
on complete tests.
Another alternative approach to the use of strongest ex-
clusions for semantic differencing could be to use partial
inclusions or strongest inclusions. Partial inclusions are value
combinations that are contained in at least one valid test.
Strongest inclusions are value combinations whose every ex-
tension to a complete test forms a valid test. However, both op-
tions of using inclusions are signiﬁcantly less informative than
using strongest exclusions. Typically, all parameter values of
a model are valid in combination with at least one assignment
to the other parameters. Hence, all model values are partial
inclusions. A diff based on partial inclusions would simply
result in the list of added and removed values, making the
semantic differencing identical to our syntactic one. Moreover,
partial inclusions do not form a canonical representation of a
model. Strongest inclusions form a canonical representation,
but are usually complete tests. Thus, a diff based on strongest
inclusions will result in a signiﬁcantly larger and less concise
Diffcsem(S1,S2)than when using strongest exclusions.
3) Reducing the size of the semantic differencing: While
a complete semantic differencing is desirable, it is also ben-
eﬁcial to reduce the amount of strongest exclusions that the
practitioner needs to review. Two techniques to achieve this are
ﬁltering out redundant information, and dividing the presented
information into categories.
Derived Exclusions. Redundant information may exist in
Diffcsem(S1,S2)due to derived exclusions . A derived exclu-
sion is an invalid value combination (i.e., it is excluded from
the model) that is excluded not due to any single constraint but
rather due to the interaction between different constraints. For
example, consider the ﬁrst constraint in Table I, and assume
we add a third constraint to the model: OS=Air→IS=
InStock . The interaction of the two constraints yields a de-
623
625
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. rived exclusion, (DT=Immediate ,IS=OutOfStock ),
which is not directly excluded by any of the three constraints.
Some of the strongest exclusions that appear in the diff
may be derived exclusions, and thus contain redundant infor-
mation – the fact that they are excluded can be concluded
from other excluded combinations in the diff. To eliminate
this redundancy and reduce the amount of information in
Diffcsem(S1,S2), we remove derived exclusions from the
result of the comparison between SE1andSE2.
Note that due the removal of derived exclusions, Diffcsem
(S1,S2)is no longer canonical, since the same exclusion can
be derived in one model and not in another semantically-
equivalent model (i.e., that has the same set of valid tests),
depending on the syntactic representation of the constraints.
We note however that the importance of strongest exclusions
as a canonical representation of a model is for comparison
purposes (other non-canonical representations cannot be used
for comparison), and reduction to a non-canonical form is done
only on the diff results after the comparison, for presentation
purposes.
Removing derived exclusions has two important advantages.
First, it enables tighter linkage of the semantic information
to syntactic information, because each strongest exclusion
presented in Diffcsem(S1,S2)can be linked to a single user-
speciﬁed constraint that excludes it, as presented in Figures 1
and 2. Second, it ﬁlters out redundant information from the
presentation and reduces the number of exclusions that the
practitioner needs to review.
Categorization. Another technique that helps with the analysis
of the diff is to divide the computed information into cate-
gories. We observe that most of the analysis effort concentrates
on the strongest exclusions that contain common parameters,
because they indicate changes in the parts of the test space
that belong to both model versions. When a set of parameters
is removed from the model, it is natural that all the constraints
on their inter-relations are also removed from the model, and
similarly for the case of parameter additions.
To this end, and for clarity of presentation, we divide the
semantic diff view into three separate views: (1) additions
and/or removals that are deﬁned on at least one common
parameter, which is the main diff view, (2) additions that are
deﬁned only on added parameters, and (3) removals that are
deﬁned only on removed parameters.
In all our examples from Section III, all exclusions differ-
ences are of the ﬁrst type, hence only the main view is shown.
Note that an added or removed strongest exclusion may be
deﬁned both on common parameters and on parameters unique
to one model, in which case it will be only presented in the
main view.
B. Computing the Differencing
While computing Diffsyn(S1,S2)is straightforward and
can be achieved by a simple traversal over the parameters,
values, and constraints of the two versions, computing
Diffcsem(S1,S2)is much more challenging. Speciﬁcally, it
requires computing the set of strongest exclusions SE of ainput : The BDD Va l i dS1of all valid tests of S1.
The BDD Va l i dS2of all valid tests of S2.
The set of constraints C1ofS1.
The set of constraints C2ofS2.
output : The BDD seAdded of strongest exclusions that are in S1
and not in S2. The BDD seRemoved of strongest exclusions
that are in S2and not in S1.
1Init:completed 1←FA L SE
2completed 2←FA L SE
3SE1←newlist
4SE2←newlist
5level←1
6while¬(completed 1∧completed 2)do
7 fori∈1,2do
8 if¬completedi then
9 computeSE (Va l i dSi, level, SEi)
10 if/logicalortext
(SEi)= =¬Va l i dSithen
11 completedi ←TR UE
12 end
13 end
14 end
15 level ++
16end
17seAdded ←SE1\SE2
18seRemoved ←SE2\SE1
19removeDerived (seAdded, C1)
20removeDerived (seRemoved, C2)
Algorithm 1: Semantic diff of combinatorial models
model version. To achieve efﬁcient computation of SE, we use
a symbolic computation based on Binary Decision Diagrams
(BDDs) [3] as our primary data structure for the set of valid
tests as well as for all other computed artifacts. As explained
in Section II, to handle domains of discrete values rather than
only Boolean ones, we use a BDD that represents the legal
multi-valued to Boolean encoding of the parameter values. For
clarity of presentation, we omit the handling of this standard
encoding from the following pseudo code and accompanying
algorithm description.
In Algorithm 1, we introduce a novel algorithm for com-
puting the semantic differencing of two model versions. The
algorithm is iterative, where in each step it computes the
strongest exclusions up to arity level (line 9), until reaching
the CL. Once reached, it compares the two sets of strongest
exclusions to identify their differences (l. 17), and removes
derived exclusions from these differences (l. 19). BDDs are
used to represent the set of valid tests of each model version
(Va l i d Si), to compute and represent the strongest exclu-
sions of each version ( SE1andSE2) and their differences
(seAdded andseRemoved ), to check whether the CL has
been reached (l. 10), and to remove the derived exclusions
from the differencing results. The algorithm relies on the
efﬁciency of negation, conjunction, disjunction, and existential
quantiﬁcation of BDDs, as explained in Section II.
Algorithm 2 presents the method computeSE , which com-
putes in each iteration the strongest exclusions of arity level
for a given model. For every set of parameters tof size level ,
it computes the BDD excluded tof all exclusions on t. This
is achieved by ﬁrst computing the BDD of valid assignments
tot, which is the projection of the Va l i d BDD on t, and then
negating the resulting BDD (l. 3). A projection on a set of
624
626
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. parameters tis computed by existentially quantifying out the
parameters not appearing in t.
Next, to ﬁlter out exclusions on tthat are not strongest
exclusions, the method conjuncts the BDD excluded twith
the negation of every strongest exclusion BDD from a level
smaller than level whose parameters are contained in t(l. 7).
Thus, all strongest exclusions for each set of parameters tare
symbolically computed at once using BDD operations.
At the end of each call to computeSE , the main algorithm
checks whether the CL has been reached, by checking whether
the strongest exclusions collected so far represent the entire
valid test space. This is achieved by checking whether the
disjunction of the strongest exclusions collected so far is
identical to the BDD of invalid tests (l. 10).
Finally, once the CL of both models is reached (and hence
the CL of the commit), the iteration in Algorithm 1 terminates,
and the set of differences is computed. The derived exclusions
are removed from the result in the removeDerived method.
This is also computed symbolically for every set of parameters
tof size up to CL, by projecting each of the constraints on t,
and removing from the BDD of strongest exclusions for tall
tuples that are not included in one of these projections, using
BDD conjunction and disjunction operations. For additional
details about the computation of derived exclusions see [11].
Note that the removal of the derived exclusions cannot precede
the identiﬁcation of the exclusion differences, because the
same strongest exclusion can be derived in one version and
not the other. Had the two operations been swapped, such an
exclusion would be falsely identiﬁed as a difference between
the two versions.
In practice, for test spaces with a huge number of parameters
and high CLs, computeSE as described so far might require
large memory or long computation time for computing the
projections of the legal space BDD on all parameter tuples
of size level . This step (Line 3 of computeSE ) is the main
contributor to the complexity of our algorithm, since for each
model it requires/parenleftbign
k/parenrightbig
project operations on the BDD of the
valid test space, where nis the number of parameters, and k
is the CL. Thus, we choose to limit the size of the BDDs used
during this computation. If the intermediate BDDs exceeds a
given size threshold, the iteration on the parameter tuples is
interrupted, and the result achieved so far from the previous
level is used instead. In such cases, the user will be notiﬁed
that the differencing is incomplete, and that differences are
shown only for strongest exclusions up to the last fully
computed level. In Section V, we will show that most real-
world models we analyzed reach the CL. Of course, a higher
threshold could be used to allow the remaining models reach
their CL.
V. E V ALUA TION
We present an evaluation of our work in terms of the
results of our differencing technique when applied to real-
world model evolution. We then continue with a user study
evaluating the effectiveness of our differencing technique in
helping CTD practitioners comprehend model changes.input : The BDD Va l i d Sof all valid tests of S.
The arity level of strongest exclusions to compute.
The list of BDDs SE of strongest exclusions for
levels smaller than level .
output : The list of BDDs SE of strongest exclusions for levels up to and
including level .
1Init:T←all parameter tuples of size level
2fort∈Tdo
3 excluded t←¬project (Va l i d S,t)
4 sizeSE ←size (SE)
5 fori∈0,...,s i z e S E −1do
6 ift⊇param (SE(i))then
7 excluded t←excluded t∧¬SE(i))
8 end
9 end
10 SE←{SE, excluded t}
11end
Algorithm 2: computeSE
A. Real-World Evidence
The research questions guiding our ﬁrst evaluation are:
RQ1A How do combinatorial models evolve in practice? In
particular, what are the common kinds and sizes of
changes?
RQ1B How does our differencing computation perform on
real model versions in practice?
To answer these questions, we applied our differencing tool
to the evolution of a large corpus of real-world models.
1) Models Used and Setup: We applied our differencing
technique to the evolution of 42 real-world industrial models,
with 107 versions and 65 version commits1. The models were
written by different CTD practitioners over a period of 7 years,
and originate from 12 different domains: ﬁrmware, PaaS,
IaaS, ﬁle system, operating system, database, storage, analyt-
ics, banking, telecom, networking, and software applications
(email, document management, ﬁnance, etc.). The models also
capture different levels of testing, such as function test, system
test, etc. We did not select the models according to any criteria,
and they represent all data available to us. The versions result
from user-deﬁned commits; the time difference between two
consecutive versions ranges between a day and 13 months.
All runs of our differencing computation were performed on a
Linux machine with 16 1.5 GHz cores and 16 GB RAM. Only
one core was used in our experiments. The BDD package used
was JDD [15].
In the following we provide observations on the collected
model data and on our differencing results. Complete per
model data as well as all versions data and differencing results
are available from [2].
2) Results and Observations: Variability in Size and
Complexity. The data shows high variability in size and
complexity of both versions and commits. The size of models
ranges from 4 to 109 parameters with median 11.75 and stan-
dard deviation (SD) of 13.78 (median and SD are computed
on average across commits per model). There were 2 to 500
1Unfortunately, all models are conﬁdential since they were created for IBM
or its clients. We are in the process of checking the option of sharing most
of them after obfuscation.
625
627
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. values per parameter (median: 4.4, SD: 2.73), and 0 to 381
constraints (median: 13.6, SD: 46.7).
The resulting test space sizes are ranging from 69 to 3∗1042
valid tests (median: 106.05, SD: 107.6), and CLs ranging from
0 to 9 (median: 2.5, SD: 1.55). The size of the changes
resulting from commits ranges from the addition or removal
of a single parameter, constraint and/or value to 56 parameters
added (median: 2, SD: 7.2) and 58 removed (median: 1,
SD: 6.4), 266 constraints added (median: 9, SD: 34.8) and
375 removed (median: 5.15, SD: 29.2), 112 values added
to a common parameter (median: 1.85, SD: 17.9) and 205
removed (median: 1.55, SD: 31.27), and from no changes in
the strongest exclusions to 3096 strongest exclusions added
(median: 19.25, SD: 228.5) and 266 removed (median: 8.75,
SD: 29.05).
While 50% of the models had rather small (2 or less)
parameter additions on average across commits, 25% of the
models had 7.6 or more parameter additions on average, and
while 50% of the models had 1 or less parameter removals
on average, 25% had 4.3 or more such removals on average.
Similarly, while 50% of the models had less than 2 value
additions to common parameters on average, 25% had 9.5 or
more such additions on average, and while 50% of the models
had less than 2 value removals on average, 25% had 10 or
more such removals on average.
Syntactic Diff. Based on the observed data, changes in
constraints (additions in 82% of the commits, removals in
71% of the commits) are more likely to occur than the other
syntactic changes, which are more or less equally common
(parameter/value additions as well as value removals in 65%
of the commits, parameter removals in 55% of the commits).
Additions and Removals. When considering both syntactic
and semantic diff, 89% of the commits included both additions
to the model and removals from it. 8% included only removals,
and 3% only additions. We conclude that a commit tends to
contain complex changes rather than simpler changes of only
additions to the model or only removals from it.
To answer [RQ1A], the results show high variability
in size and complexity of both versions and commits.
Commits tend to contain large and complex changes that
typically involve a combination of additions, removals,
and modiﬁcations to the constraints.
The large size and complexity of the models and changes
involved, as evident above, require an efﬁcient and scalable
differencing solution to handle real-world model evolution.
Performance. Figure 3 presents the runtime results of
our semantic differencing computation. Out of a total of 65
commits, computing differencing (in the tool, to display the
differences as shown in the screenshots from our running
example) took less than a second for 45 of the models (70%),
less than 10 seconds for 53 (82%), and only 4 required over
a minute (6% of the commits, from only two models). These
two latter models involved an exceptionally high number of
parameters, large test space BDDs, and a CL greater than 2,
all contributing to the exceptionally long running time.
Fig. 3: Performance of our semantic differencing on real-world
model commits
To answer [RQ1B], the results show that our differenc-
ing computation performs in acceptable times on almost
all real model versions in practice.
B. User Study
The research questions guiding our user study are:
RQ2A Is comprehension of changes in models challenging,
and does our presentation of syntactic and semantic
differencing help practitioners in better understand-
ing the changes done and their consequences?
RQ2B Does the expertise level inﬂuence the performance
and conﬁdence of practitioners when attempting to
understand such changes?
RQ2C Is there a difference in difﬁculty of comprehension
between syntactic and semantic changes?
1) Setup and Participants: Our study included two real-
world models, each with two real-world versions. The ﬁrst
model, model A, describes a system test space for features
of IBM R/circlecopyrtPOWER7 R/circlecopyrt[36]. Its ﬁrst version contains 7 pa-
rameters and 33 constraints, and its second version contains
6 parameters and 24 constraints. The second model, model
B, describes an end-to-end test space for a PaaS. Its ﬁrst
version contains 19 parameters and 2 constraints, and its
second version contains 21 parameters and 4 constraints. The
sizes and changes in the two models are comparable to real-
world model sizes and changes, as reﬂected by the evidence
shown in Section V-A.
We asked 5 questions about each model, related to the
changes between the two versions. One question about the
syntactic differences between the two versions, and 4 questions
about the semantic differences. The syntactic questions asked
about the number of parameters or values that were added or
removed from the second version.
We presented two types of semantic questions. The ﬁrst
type referred to the effect of adding values or parameters to
the model, e.g., “By adding the value vto parameter Xin
the model, how many new combinations of vand a value u
of parameter Yare added to the space of valid tests?”.
The second type of semantic question referred to speciﬁc
value combinations and whether there was a change in their
626
628
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. validity status, e.g., “The value combination Cis excluded
from the valid test space of version 1; is it also excluded from
version 2?”.
Before running the study, we reviewed these questions with
the two CTD practitioners who created the two models we
used in our study, and veriﬁed that they reﬂect real-world
comprehension tasks of model changes.
Each study participant ﬁlled an online questionnaire in an
IBM corporate network, containing the questions about the two
models (10 questions in total). For one model only the model
version deﬁnitions were provided, and for the other model the
syntactic and semantic diff report was provided as well (as
shown in Section III). The decision on the order of models
in the questionnaire as well as on the identity of the model
that included the differencing report was randomly made per
participant. In addition, after each question the participants
were asked to rate their conﬁdence in the correctness of their
answer from 1 (least) to 5 (most). 16 practitioners participated
in the study, all of whom are CTD practitioners regularly
involved in creating and comprehending combinatorial models
for test designs, as part of their work at IBM. All participants
were previously unfamiliar with the two study models.
2) Results: We separate the question scores and conﬁdence
into two groups: questions answered without using our differ-
encing report (score/conﬁdence without diff), and questions
answered while using our differencing report (score/conﬁdence
with diff). Note that due to the setup explained above, some
participants had the differencing report available for model A,
while others had it for model B. Similarly, some participants
had the differencing report available for the ﬁrst model they
were asked about (be it AorB) while others had it for the
second one.
Figure 4 summarizes the score results of our study. The
average score without diff was 68.75 (out of 100), and with
diff 76.25. The average conﬁdence without diff was 4.11 (out
of 5) and with diff 4.16. These numbers indicate an overall
improvement of 10% in performance and a slight improvement
in conﬁdence when using the differencing report.
To answer [RQ2A], the results show that comprehension
of changes in models is challenging, and that our
presentation of differencing improves practitioners com-
prehension of the changes done and their consequences.
A deeper analysis is required in order to understand the
impact of the differencing report on different practitioners. To
this end, we explore the differences between the practitioners
based on their level of expertise, as follows. The overall
average score was 72.5. The 16 participants are composed of
two distinct groups. 8 participants are expert CTD practitioners
with a high level of expertise. All these participants scored
between 80 and 100, much above the overall average score.
8 participants are less experienced ones with a relatively low
level of expertise in CTD. These participants scored between
40 and 70, all below the overall average score.
Figure 4 summarizes also the score results of the expert
practitioners and less experienced practitioners, separately. Forthe expert practitioners, the average score was 95 without
the diff report and 92.5 with it. Out of 40 questions per
model collectively, the experts answered correctly 37 with
diff and 38. We consider this difference negligible. Thus, the
expert practitioners performed very well regardless of the diff.
Their average conﬁdence was 4.42 without the diff report
and 4.55 with it, indicating only a very slight increase in
conﬁdence. In contrast, for the less experienced practitioners,
the average score was 42.5 without the diff report and 60 with
it, indicating a signiﬁcant improvement in performance. Their
average conﬁdence was 3.8 without the diff report and 3.78
with it, relatively low in both cases, and rightfully so.
Fig. 4: Average score for the total 16 practitioners, as well as
separately for the 8 experts and 8 less experienced practition-
ers.
To answer [RQ2B], for less experienced practitioners,
the differencing report signiﬁcantly improves perfor-
mance while not impacting conﬁdence; for expert prac-
titioners, the diff report does not impact performance
and conﬁdence.
All participants answered all the syntactic questions cor-
rectly, regardless of their level of experience and regardless
of whether the diff report was available. For the semantic
questions alone, the average score was 60.94 without the diff
report and 70.31 with it. The average conﬁdence for a syntactic
question was 4.34, above the overall average conﬁdence of
4.14.
To answer [RQ2C], the results show, as one might ex-
pect, that syntactic questions are much easier to answer
than semantic ones, hence our differencing technique is
more useful for understanding semantic differences.
The complete study results are available from [2].
C. Threats to V alidity
We discuss threats to the validity of our results, starting
with internal validity. First, our implementation of differencing
computation may not be free of bugs. To mitigate this, we
manually veriﬁed the results of the diff computation on small
synthetic models and on many of the real-world models.
Second, there could have been a bias in the results of our user
study due to the order of models as well as the order between
questions for which the diff report was provided and those
for which it was not provided. To mitigate this, as explained
above, we randomized the order of models in the user study, as
627
629
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. well as the decision whether the ﬁrst or second model would
be accompanied with the diff report.
There are also threats to external validity. First, the models
and versions used in the user study and in the analysis
might not be representative of real-world model evolution. To
eliminate this threat, we chose real-world models and versions,
all that were available to us. Second, our study participants
might not be representative of real-world practitioners. To
mitigate this, we chose 16 real-world CTD practitioners who
are regularly involved in creating and comprehending combi-
natorial models for test designs. However, a larger group of
practitioners and more models and questions could strengthen
the generalizability of our results.
VI. R ELA TED WORK
Much work has been published on syntactic and semantic
differencing of programs and models within the more general
ﬁeld of Software Evolution (see, e.g. [12], [28], [37]).
Most relevant to our present work is the work of Maoz et
al. on semantic model differencing for Class Diagrams [24]
(using SA T) and Activity Diagrams [23] (using BDD-based
symbolic algorithm). Other recent work considered semantic
differencing for Feature Models [1], [32], in the context of
software product lines. These works address similar challenges
to the ones we deal with, including the efﬁcient computation
of the semantic differences and their effective presentation to
the engineer. A framework for relating syntactic and semantic
model differences has been presented in [22]. To the best
of our knowledge, our work is the ﬁrst to consider syntactic
and semantic differencing for combinatorial models. In addi-
tion, the existing model differencing work provided prototype
implementations and evaluated performance, but none has
evaluated the usefulness of diff representations to real-world
practitioners.
While there is a large body of work on various aspects
of combinatorial testing, to the best of our knowledge, none
of it addresses the problem of differencing of combinatorial
models in the context of their evolution. The survey by Nie et
al. [26] considers 93 academic papers on combinatorial testing
but does not mention model evolution. Many CTD tools [6],
[9], [16], [19], [27] exist, but to the best of our knowledge,
they provide no support for model differencing. Our practical
experience, in contrast, shows that managing and comprehend-
ing changes in models is a challenge encountered frequently
by practitioners. One notable exception is the work of Qu
et al. [29], which examines the effectiveness of combinatorial
testing prioritization and re-generation strategies on regression
testing in evolving programs with multiple versions. However,
the work does not address evolution at the model level.
Derived exclusions in combinatorial models, a.k.a. implicit
constraints , have been discussed in previous works and were
shown to complicate the solving of the CTD problem [20]
and the modeling process [7]. [11] uses derived exclusions
to review and debug the model constraints. While it suggests
to present only minimal derived exclusions, the concept of
strongest exclusions is not discussed independently. Moreover,the work does not suggest the use of strongest exclusions for
a canonical representation of a model. Finally, all these works
do not consider derived exclusions in the context of evolution.
In a recent work, we presented a lattice-based semantics
for interpreting the evolution of combinatorial models [33].
The new semantics replaces the inadequate Boolean semantics
which is currently in use by CTD tools for interpreting
model changes. It provides a theoretical base for a consistent
interpretation of atomic changes to the model, and exposes
which additional parts of the model must change following an
atomic change, in order to restore validity. The differencing
approach we present and evaluate in this paper ﬁts well with
this new semantics, since strongest exclusions can be precisely
deﬁned using our lattice-based semantics.
Finally, in a research roadmap presented in a recent re-
view on combinatorial testing [38], Yilmaz et al. suggest the
challenge “to handle evolving [combinatorial] models as they
change over time”. Our work starts going in this direction.
VII. C ONCLUSION AND FUTURE WORK
In this work we propose a ﬁrst syntactic and semantic
differencing technique for combinatorial models of test de-
signs, and present an efﬁcient algorithm for computing it.
To enable semantic differencing, we suggest a concise and
canonical representation of a model that is used as the basis for
differencing. We implemented our technique in our CTD tool
IBM FOCUS, and evaluated it on 42 real-world models with
107 versions, demonstrating its acceptable performance times.
Our analysis reveals that real-world commits of combinatorial
models tend to be large and complex in practice.
We further conducted a user study with CTD practition-
ers on real-world combinatorial model versions. The study
showed a signiﬁcant improvement of over 40 percent in the
performance of less experienced practitioners when using our
differencing techniques, and an improvement in conﬁdence of
expert practitioners in their comprehension of model changes.
The present paper is part of our larger research project,
on comprehension and evolution of combinatorial models
for test designs. In a recent paper we presented the use of
visualization for comprehension of combinatorial models and
test plans [34]. As part of this project, we plan to further
analyze model differences and identify change patterns that
commonly occur in real-world combinatorial model evolution,
such as abstraction, reﬁnement, and refactoring. Finally, we
plan to investigate co-evolution of models and the test plans
derived from them, and ﬁnd practical and efﬁcient ways to
update a test plan following changes to the model from which
it was derived.
VIII. A CKNOWLEDGMENTS
This research was done under the terms of a joint study
agreement between IBM Corporation Inc (via the IBM Re-
search Lab - Haifa) and Tel Aviv University. Additionally, part
of the research leading to these results has received funding
from the European Community’s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement no. 610802.
628
630
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] M. Acher, P . Heymans, P . Collet, C. Quinton, P . Lahire, and P . Merle.
Feature model differences. In Advanced Inf. Sys. Eng. , pages 629–645.
IEEE Computer Society, 2012.
[2] Suplementary Material. http://smlab.cs.tau.ac.il/ctd/.
[3] R. E. Bryant. Graph-Based Algorithms for Boolean Function Manipu-
lation. IEEE Trans. on Comp. , 35(8):677–691, 1986.
[4] K. Burroughs, A. Jain, and R. Erickson. Improved quality of pro-
tocol testing through techniques of experimental design. In SUPER-
COMM/ICC , pages 745–752, 1994.
[5] E. M. Clarke, O. Grumberg, and D. A. Peled. Model Checking . The
MIT Press, 1999.
[6] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG
System: An Approach to Testing Based on Combinatorial Design. IEEE
Trans. on Softw. Eng. , 23(7):437–444, 1997.
[7] M. B. Cohen, M. B. Dwyer, and J. Shi. Interaction testing of highly-
conﬁgurable systems in the presence of constraints. In ISSTA , pages
129–139, 2007.
[8] M. B. Cohen, J. Snyder, and G. Rothermel. Testing across conﬁgu-
rations: implications for combinatorial testing. SIGSOFT Softw. Eng.
Notes , 31(6):1–9, 2006.
[9] J. Czerwonka. Pairwise testing in the real world: Practical extensions
to test-case scenarios. In PNSQC , 2006.
[10] S. R. Dalal, A. Jain, N. Karunanithi, J. M. Leaton, C. M. Lott, G. C.
Patton, and B. M. Horowitz. Model-Based Testing in Practice. In ICSE ,
pages 285–294, 1999.
[11] E. Farchi, I. Segall, and R. Tzoref-Brill. Using projections to debug
large combinatorial models. In ICSTW , pages 311–320, 2013.
[12] B. Fluri, M. Wursch, M. Pinzger, and H. Gall. Change distilling: Tree
differencing for ﬁne-grained source code change extraction. Software
Engineering, IEEE Transactions on , 33(11):725–743, 2007.
[13] M. Grindal, B. Lindstr ¨om, J. Offutt, and S. F. Andler. An evaluation
of combination strategies for test case selection. Empirical Softw. Eng. ,
11(4):583–611, 2006.
[14] IBM Functional Coverage Uniﬁed Solution (IBM FOCUS). http:
//researcher.watson.ibm.com/researcher/view group.php?id=1871.
[15] JDD. http://javaddlib.sourceforge.net/jdd/.
[16] Jenny website. http://burtleburtle.net/bob/math/jenny.html.
[17] D. R. Kuhn, R. N. Kacker, and Y . Lei. Introduction to Combinatorial
Testing . Chapman & Hall/CRC, 2013.
[18] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software Fault Interactions
and Implications for Software Testing. IEEE Trans. on Softw. Eng. ,
30(6):418–421, 2004.
[19] R. Kuhn, Y . Lei, and R. Kacker. Practical Combinatorial Testing: Beyond
Pairwise. IT Professional , 10(3):19–23, 2008.
[20] C. Lott, A. Jain, and S. Dalal. Modeling requirements for combinatorial
software testing. SIGSOFT Softw. Eng. Notes , 30(4):1–7, 2005.
[21] S. Malik, A. Wang, R. Brayton, and A. Sangiovanni-Vincentelli. Logic
V eriﬁcation Using Binary Decision Diagrams in a Logic Synthesis
Environment. In ICCAD’88 , pages 6–9, 1988.
[22] S. Maoz and J. O. Ringert. A framework for relating syntactic and
semantic model differences. In MODELS , pages 24–33. IEEE Computer
Society, 2015.
[23] S. Maoz, J. O. Ringert, and B. Rumpe. ADDiff: semantic differencing
for activity diagrams. In ESEC/FSE , pages 179–189, 2011.
[24] S. Maoz, J. O. Ringert, and B. Rumpe. CDDiff: Semantic differencing
for class diagrams. In ECOOP , pages 230–254, 2011.
[25] S. Minato. Graph-Based Representations of Discrete Functions , pages
1–28. Springer US, 1996.
[26] C. Nie and H. Leung. A Survey of Combinatorial Testing. ACM Comput.
Surv. , 43(2):11:1–11:29, 2011.
[27] Pairwise testing website. http://www.pairwise.org/tools.asp.
[28] S. Person, M. B. Dwyer, S. G. Elbaum, and C. S. Pasareanu. Differential
symbolic execution. In SIGSOFT FSE , pages 226–237, 2008.
[29] X. Qu, M. B. Cohen, and K. M. Woolf. Combinatorial interaction
regression testing: A study of test case generation and prioritization.
InICSM , pages 255–264, 2007.
[30] I. Segall, R. Tzoref-Brill, and E. Farchi. Using Binary Decision
Diagrams for Combinatorial Test Design. In ISSTA , pages 254–264,
2011.
[31] K. Tai and Y . Lie. A Test Generation Strategy for Pairwise Testing.
IEEE Trans. on Softw. Eng. , 28(1):109–111, 2002.[32] T. Thum, D. Batory, and C. Kastner. Reasoning about edits to feature
models. In ICSE , pages 254–264, 2009.
[33] R. Tzoref-Brill and S. Maoz. Lattice-based semantics for combinatorial
model evolution. In ATVA , pages 276–292, 2015.
[34] R. Tzoref-Brill, P . Wojciak, and S. Maoz. Visualization of Combinatorial
Models and Test Plans. In ASE , pages 144–154. ACM, 2016.
[35] A. W. Williams. Determination of test conﬁgurations for pair-wise
interaction coverage. In TestCom , pages 59–74, 2000.
[36] P . Wojciak and R. Tzoref-Brill. System Level Combinatorial Testing
in Practice – The Concurrent Maintenance Case Study. In ICST , pages
103–112, 2014.
[37] Z. Xing and E. Stroulia. Differencing logical UML models. Autom.
Softw. Eng. , 14(2):215–259, 2007.
[38] C. Yilmaz, S. Fouch ´e, M. B. Cohen, A. A. Porter, G. Demir ¨oz, and
U. Koc. Moving forward with combinatorial interaction testing. IEEE
Computer , 47(2):37–45, 2014.
629
631
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 16:53:32 UTC from IEEE Xplore.  Restrictions apply. 