Tell Them Apart: Distilling Technology Differences from
Crowd-Scale Comparison Discussions
Yi Huang
Australian National
University Australia
u6039034@anu.edu.auChunyang Chen∗
Faculty of Information
Technology Monash
University Australia
chunyang.chen@monash.
eduZhenchang Xing
Australian National
University Australia
zhenchang.xing@anu.edu.
auTian Lin
Yang Liu
Nanyang Technological
University Singapore
yangliu@ntu.edu.sg
ABSTRACT
Developers can use different technologies for many software de-
velopmenttasks intheirwork.However,whenfacedwith several
technologies with comparable functionalities, it is not easy for de-
velopers to select the most appropriate one, as comparisons among
technologies are time-consuming by trial and error. Instead, devel-
operscanresorttoexpertarticles,readofficialdocumentsorask
questions in Q&A sites for technology comparison, but it is oppor-
tunistictoget acomprehensivecomparisonas onlineinformation
isoftenfragmentedorcontradictory.Toovercometheselimitations,
we propose the diffTechsystem that exploits the crowdsourced dis-
cussionsfromStackOverflow,andassiststechnologycomparison
with an informative summary of different comparison aspects. We
first build a large database of comparable software technologies by
miningtagsinStackOverflow,andlocatecomparativesentences
aboutcomparabletechnologieswithNLPmethods.Wefurthermine
prominentcomparisonaspectsbyclusteringsimilarcomparative
sentences and represent each cluster with its keywords. The evalu-
ation demonstrates both the accuracy and usefulness of our model
and we implement a practical website for public use.
CCS CONCEPTS
•Information systems →Data mining ;•Software and its
engineering →Software libraries and repositories;
KEYWORDS
differencing similar technology, Stack Overflow, NLP
ACM Reference Format:
Yi Huang, Chunyang Chen, Zhenchang Xing, Tian Lin, and Yang Liu. 2018.
TellThemApart:DistillingTechnologyDifferencesfromCrowd-ScaleCom-
parison Discussions. In Proceedings of the 2018 33rd ACM/IEEE Interna-
tional Conference on Automated Software Engineering (ASE ’18), Septem-
ber 3–7, 2018, Montpellier, France. ACM, New York, NY, USA, 11pages.
https://doi.org/10.1145/3238147.3238208
∗Co-first and corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238208
Figure1:Acomparativesentenceinapost(#1008671)thatis
not explicitly for technology comparison
1 INTRODUCTION
A diverse set of technologies (e.g, algorithms, programming lan-
guages,platforms, libraries/frameworks,concepts forsoftwareen-
gineering) [ 12,15] is available for use by developers and that set
continuesgrowing.Byadoptingsuitabletechnologies,itwillsig-
nificantly accelerate the software development process and also
enhance the software quality. But when developers are looking for
propertechnologiesfortheirtasks,theyarelikelytofindseveral
comparable candidates. For example, they will find bubble sort and
quicksort algorithmsforsorting, nltkandopennlplibrariesforNLP,
EclipseandIntellijfor developing Java applications.
Faced with so many candidates, developersare expected to have
a good understanding of different technologies in order to makea proper choice for their work. However, even for experienceddevelopers, it can be difficult to keep pace with the rapid evolu-
tionof technologies.Developerscantry eachofthe candidatesin
theirworkforthecomparison.Butsuchtrial-and-errorassessment
is time-consuming and labor extensive. Instead, we find that the
perceptions of developers about comparable technologies and the
choicestheymakeaboutwhichtechnologytouseareverylikely
tobeinfluencedbyhowotherdevelopers seeandevaluatethetech-
nologies. So developers often turn to the two information sources
on the Web [8] to learn more about comparable technologies.
First,theyreadexperts’articlesabouttechnologycomparison
like“Intellij vs. Eclipse: Why IDEA is Better” . Second, developers
can seek answers on Q&A websites such as Stack Overflow orQuora (e.g., “Apache OpenNLP vs NLTK” ). These expert articles
and community answers are indexable by search engines, thus
enablingdeveloperstofindanswerstotheirtechnologycomparison
inquiries.
However,therearetwolimitationswithexpertarticlesandcom-
munity answers.
214
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yi Huang, Chunyang Chen, Zhenchang Xing, Tian Lin, and Yang Liu
•Fragmentedview: Anexpertarticleorcommunityanswerusually
focusesonaspecificaspectofsomecomparabletechnologies,and
developers have to aggregate the fragmented information into a
completecomparisonindifferentaspects.Forexample,tocom-
paremysqlandpostgresql,onearticle[ 2]contraststheirspeed,
whileanother[ 3]comparestheirreliability.Onlyafterreading
both articles, developers can have a relatively comprehensive
overview of these two comparable technologies.
•Diverse opinions: One expert article or community answer is
basedontheauthor’sknowledgeandexperience.However,the
knowledge and experience of developers vary greatly. For exam-
ple,onedevelopermayprefer EclipseoverIntellijbecauseEclipse
fitshisprojectsettingbetter.Butthatsettingmaynotbeextensi-
ble to other developers. At the same time, some developers may
preferIntellijoverEclipseforotherreasons.Suchcontradictory
preferences among different opinions may confuse developers.
Theabovetwolimitationscreateahighbarrierfordevelopers
to effectively gather useful information about technology differ-
ences on the Web in order to tell apart comparable technologies.
Althoughdevelopersmaymanuallyaggregaterelevantinformation
by searching and reading many web pages, that would be veryopportunistic and time consuming. To overcome the above limi-
tations,wepresentthe diffTechsystemthatautomaticallydistills
andaggregatesfragmentedandtrustworthytechnologycompari-
son information from the crowd-scale Q&A discussions in Stack
Overflow,andassiststechnologycomparisonwithaninformativesummary of different aspects of comparison information.
Oursystemismotivatedbythefactthatawiderangeoftechnolo-
gieshavebeendiscussedbymillionsofusersinStackOverflow[ 14],
and users often express their preferences toward a technology and
compare one technology with the others in the discussions. Apart
frompostsexplicitlyaboutthecomparisonofsometechnologies,
manycomparativesentences hideinpoststhatareimplicitlyabout
technology comparison. Fig.1 shows such an example: the answer
“accidentally”comparesthesecurityof POSTandGET,whilethe
question “How secure is a HTTP post?” does not explicit ask for
thiscomparison.Inspiredbysuchphenomenon,wethenpropose
our system to mine and aggregate the comparative sentences in
Stack Overflow discussions.
As shown in Fig. 2, we consider Stack Overflow tags as a collec-
tion of technology terms and first find comparable technologies by
analyzing tag embeddings and categories. And then, our system
distillsandclusterscomparativesentencesfromQ&Adiscussions,
whichhighlylikelycontainsdetailedcomparisonsbetweensome
comparable technologies, and sometimeseven explains why users
like or dislike a particular technology.Finally, we use word mover
distance [ 28] and community detection [ 21] to cluster compara-
tive sentences into prominent aspects by which users compare the
two technologies and present the mined clusters of comparative
sentences for user inspection.
Asthereisnogroundtruthfortechnologycomparison,weman-
uallyvalidatetheperformanceofeachstepofourapproach.The
experiment results confirm the the accuracy of comparable tech-
nology identification (90.7%), and distilling comparative sentences
(83.7%) from Q&Adiscussions. By manually buildingthe ground
truth,weshowthatourclusteringmethod(wordmoverdistance
Figure 2: The overview of our approach
and community detection) for comparative sentences significantly
outperforms the two baselines (TF-IDF with K-means and Doc2vec
withK-means).Finally,wefurtherdemonstratetheusefulnessof
oursystemforansweringquestionsoftechnologycomparisonin
Stack Overflow. The result show that our system can cover the
semantics of 72% comparative sentences in five randomly selected
technology comparisonquestions, and alsoinclude some unique
comparisonsfromotheraspectswhicharenotdiscussedinoriginal
answers.
Our contributions in this work are four-fold:
•This is the first work to systematically identify comparable
software-engineering technologies and distill crowd-scale
comparative sentences for these technologies.
•Our method automatically distills and aggregates crowdopinions into different comparison aspects so that devel-
opers can understand technology comparison more easily.
•Ourexperimentsdemonstratetheeffectivenessofourmethod
by checking the accuracy and usefulness of each step of our
approach.
•Weimplementourresultsintoapracticaltoolandmakeit
public to the community. Developers can benefit from the
technology comparison knowledge in our website1.
1https://difftech.herokuapp.com/
215
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Tell Them Apart: Distilling Technology Differences from Crowd-Scale Comparison... ASE ’18, September 3–7, 2018, Montpellier, France
word embedding of tଷ ݐଵ tହ tସ ݐଶ 
tଷ 
(a) Continuous skip-gram modelwordembedding ofଷݐଵ ݐହ ݐସ ݐଶ
ଷ
(b) Continuous bag-of-words model
Figure3:Thearchitectureofthetwowordembeddingsmod-
els. The continuous skip-gram model predicts surrounding
wordsgiventhecentralword,andtheCBOWmodelpredicts
thecentralwordbasedonthecontextwords.Notethediffer-
ences in arrow direction between the two models.
2 MINING SIMILAR TECHNOLOGY
Studies [9,11,44] show that Stack Overflow tags identify com-
puter programming technologies that questions and answers re-
volve around. They cover a wide range of technologies, from algo-
rithms(e.g., binarysearch,mergesort ),programminglanguages(e.g.,
python, java ), libraries and frameworks (e.g., tensorflow, django ),
anddevelopmenttools(e.g., vim,git).Inthiswork,weregardStack
Overflowtagsasacollectionoftechnologiesthatdeveloperswould
like to compare. We leverage word embedding techniques to infer
semanticallyrelatedtags,anddevelopnaturallanguagemethodsto
analyzeeachtag’sTagWikitodeterminethecorrespondingtech-
nology’s category (e.g., algorithm, library, IDE). Finally, we build a
knowledge base of comparable technologies by filtering the same-category, semantically-related tags.
2.1 Learning Tag Embeddings
Word embeddings are dense low-dimensional vector representa-
tionsofwordsthatarebuiltontheassumptionthatwordswithsim-ilar meanings tend to be present in similar context. Studies [
11,35]
show that word embeddings are able to capture rich semantic and
syntactic properties of words for measuring word similarity. In ourapproach,givenacorpusoftagsentences,weusewordembedding
methods to learn the word representation of each tag using the
surrounding context of the tag in the corpus of tag sentences.
There are two kinds of widely-used word embedding meth-
ods[35], the continuous skip-gram model [ 36] and the continuous
bag-of-words (CBOW) model. As illustrated in Fig. 3, the objective
of the continuous skip-gram model is to learn the word representa-
tionofeachwordthatisgoodatpredictingtheco-occurringwords
inthesamesentence(Fig. 3(a)),whiletheCBOWistheopposite,
that is, predicting the center word by the context words (Fig. 3(b)).
Note that word order within the context window is not important
for learning word embeddings.
Specifically,givenasequenceoftrainingtextstream t1,t2, ...,tk,
theobjectiveofthecontinuousskip-grammodelistomaximizethe
following average log probability:
L=1
KK/summationdisplay.1
k=1/summationdisplay.1
−N/precedesequalj/precedesequalN,j/nequal0logp(tk+j|tk) (1)Tag Wiki:            Matplotlib is a     plotting     library for     Python
Part of Speech:      NNP        VBZ DT        JJ              NN IN         NNP
Figure 4: POS tagging of the definition sentence of the tagMatplotlib
while the objective of the CBOW model is:
L=1
KK/summationdisplay.1
k=1logp(tk|(tk−N,tk−N+1, ...,tk+N)) (2)
wheretkis the central word, tk+jis its surrounding word with the
distancej,andNindicatesthewindowsize.Inourapplicationof
thewordembedding,atagsentenceisatrainingtextstream,and
each tag is a word. As tag sentence is short (has at most 5 tags), weset
Nas5inourapproachsothatthecontextofonetagisallother
tags in the current sentences. That is, the context window contains
allothertagsasthesurroundingwordsforagiventag.Therefore,
tagorderdoesnotmatterinthisworkforlearningtagembeddings.
To determine which word-embedding model performs better in
our comparable technology reasoning task , we carry out a com-
parison experiment, and the details are discussed in Section 5.1.3.
2.2 Mining Categorical Knowledge
InStackOverflow, tagscanbeofdifferentcategories,suchaspro-
gramminglanguage,library,framework,tool,API,algorithm,etc.
Todetermine thecategoryofatag,we resorttothetagdefinition
intheTagWikiofthetag.TheTagWikiofatagiscollaboratively
edited by the Stack Overflow community. Although there are no
strictformattingrulesinStackOverflow,theTagWikidescription
usually starts with a short sentence to define the tag. For example,
the tagWiki of the tag Matplotlib starts with the sentence “Mat-
plotlib is a plotting library for Python”. Typically, the first noun
justafterthe beverbdefinesthecategoryofthetag.Forexample,
fromthetagdefinitionof Matplotlib,wecanlearnthatthecategory
ofMatplotlib islibrary.
Based on the above observation of tag definitions, we use the
NLP methods [ 11,27] to extract such noun from the tag definition
sentenceasthecategoryofatag.GiventhetagWikiofataginStack
Overflow, we extract the first sentence of the TagWiki description,
and clean up the sentence by removing hyperlinks and bracketssuch as “{}”, “()”. Then, we apply Part of Speech (POS) tagging to
the extracted sentence. POS tagging is the process of marking up a
word in a text as corresponding to a particular part of speech, such
asnoun,verb,adjective.NLPtoolsusuallyagreeonthePOStags
ofnouns,andwefindthatPOStaggerinNLTK[ 10]isespecially
suitableforourtask.InNLTK,thenounisannotatedbydifferent
POStags[ 1]includingNN(Noun,singularormass),NNS(Noun,
plural),NNP(Propernoun,singular),NNPS(Propernoun,plural).
Fig.4showstheresultsforthetagdefinitionsentenceof Matplotlib.
Based on the POS tagging results, we extract the first noun (libraryinthisexample)afterthebeverb(is inthisexample)asthecategory
of the tag. That is, the category of Matplotlib islibrary. Note that if
thenounissomespecificwordssuchas system,development,w e
willfurthercheckitsneighborhoodwordstoseeifitis operating
systemorindependent development environment.
216
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yi Huang, Chunyang Chen, Zhenchang Xing, Tian Lin, and Yang Liu
Withthismethod,weobtain318categoriesforthe23,658tags
(about67%ofallthetagsthathaveTagWiki).Wemanuallynormal-
izethese318categorieslabels,suchasmerging appandapplications
asapplication, librariesandlibaslibrary,andnormalizinguppercase
andlowercase(e.g., APIandapi).Asaresult,weobtain167cate-
gories.Furthermore,wemanually categorizethese167categories
into five general categories: programming language, platform, li-
brary, API, and concept/standard [ 48]. This is because the meaning
of the fine-grained categories is often overlapping, and there is
no consistent rule for the usage of these terms in the TagWiki.
This generalization step is necessary, especially for the library tags
that broadly refer to the tags whose fine-grained categories can
belibrary,framework,api,toolkit,wrapper,andsoon.Forexam-
ple,inStackOverflow’sTagWiki, junitisdefinedasaframework,
google-visualization isdefinedasanAPI,and wxpython isdefined
as a wrapper. All these tags are referred to as library tags in our
approach.
Although the above method obtains the tag category for the
majority of the tags, the first sentence of the TagWiki of sometags is not formatted in the standard “tag be noun phrase” form.For example, the first sentence of the TagWiki of the tag itextis
“LibrarytocreateandmanipulatePDFdocumentsinJava”,orfor
markermanager,thetagdefinitionsentenceis“AGoogleMapstool”,
orforghc-pkg,thetagdefinitionsentenceis“Thecommandghc-
pkgcanbeusedtohandleGHCpackages”.Asthereisno beverbin
this sentence, the above NLP method cannot return a noun phrase
as the tag category. According to our observation, for most of such
cases, the category of the tag is still present in the sentence, but
ofteninmanydifferentways.Itisverylikelythatthecategoryword
appearsasthefirstnounphrasethatmatchtheexistingcategory
words in the definition sentence. Therefore, we use a dictionary
look-upmethodtodeterminethecategoryofsuchtags.Specially,
we use the 167 categories obtained using the above NLP method as
adictionarytorecognizethecategoryofthetagsthathavenotbeencategorizedusingtheNLPmethod.Givenanuncategorizedtag,we
scanthefirstsentenceofthetag’sTagWikifromthebeginning,and
search for the first match of a category label in the sentence. If a
match is found, the tag is categorized as the matched category. For
example,thetag itextiscategorizedas libraryusingthisdictionary
look-upmethod.Usingthedictionarylook-upmethod,weobtain
the category for 9,648 more tags.
Note that we cannot categorize some (less than 15%) of the tags
usingtheaboveNLPmethodandthedictionarylook-upmethod.
This is because these tags do not have a clear tag definition sen-
tence,forexample,theTagWikiofthetag richtextbox statesthat
“TheRichTextBoxcontrolenablesyoutodisplayoreditRTFcon-
tent”. This sentence is not a clear definition of what richtextbox
is. Or no category match can be found in the tag definition sen-
tence of some tags. For example, the TagWiki of the tag carousel
statesthat“Arotatingdisplayofcontentthatcanhouseavariety
ofcontent”.Unfortunately,wedonothavethecategory“display”
inthe167categorieswecollectusingtheNLPmethod.Whenbuild-
ing comparable-technologies knowledge base, we exclude these
uncategorized tags as potential candidates.Table 1: Examples of filtering results by categorical knowl-
edge (in red)
Source Top-5 recommendations from word embedding
nltk nlp, opennlp, gate, language-model, stanford-nlp
tcp tcp-ip,network-programming, udp, packets,tcpserver
vim sublimetext, vim-plugin, emacs, nano, gedit
swift objective-c, cocoa-touch,storyboard,launch-screen
bubble-sort insertion-sort, selection-sort, mergesort, timsort, heapsort
2.3 Building Similar-technology Knowledge
Base
Givenatechnologytag t1withitsvector vec(t1),wefirstfindmost
similar library t2whose vector vec(t2)is most closed to it, i.e.,
argmax
t2∈Tcos(vec(t1),vec(t2)) (3)
whereTisthesetof technologytagsexcluding t1,andcos(u,v)is
the cosine similarity of the two vectors.
Note that tags whose tag embedding is similar to the vector
vec(t1)maynotalwaysbeinthesamecategory.Forexample,tag
embeddingsofthetags nlp,language-model aresimilartothevector
vec(nltk).Thesetagsarerelevanttothe nltklibraryastheyreferto
someNLPconceptsandtasks,buttheyarenotcomparablelibraries
to thenltk. In our approach, we rely on the category of tags (i.e.,
categoricalknowledge)toreturnonlytagswithinthesamecategory
as candidates. Some examples can be seen in Table 1.
In practice, there could be several comparable technologies t2
to the technology t1. Thus, we select tags t2with the cosine sim-
ilarity in Eq. 3above a threshold Thresh. Take the library nltk(a
NLP library in python) as an example. We will preserve several
candidates which are libraries such as textblob,stanford-nlp.
3 MINING COMPARATIVE OPINIONS
Foreachpairofcomparabletechnologiesintheknowledgebase,weanalyzetheQ&AdiscussionsinStackOverflowtoextractplausiblecomparativesentencesbywhichStackOverflowusersexpresstheir
opinions on the comparable technologies. We may obtain manycomparative sentences for each pair of comparable technologies.
Displaying all these sentences as a whole may make it difficult for
developerstoreadanddigestthecomparisoninformation.There-
fore, we measure the similarity among the comparative sentences,
andthenclusterthemintoseveralgroups,eachofwhichmayiden-
tify a prominent aspect of technology comparison that users are
concerned with.
3.1 Extracting Comparative Sentences
Therearethreestepstoextractcomparativesentencesofthetwo
technologies. We first carry out some preprocessing of the Stack
Overflowpostcontent.Then welocatethesentencesthatcontain
thenameofthetwotechnologies,andfurtherselectthecomparative
sentences that satisfy a set of comparative sentence patterns.
3.1.1 Preprocessing. To extract trustworthy opinions about the
comparisonoftechnologies,weconsideronlyanswerpostswith
positive score points. Then we split the textual content of such
answer posts into individual sentences by punctuations like “.”, “!”,
“?”.Weremoveallsentencesendedwithquestionmark,aswewant
217
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Tell Them Apart: Distilling Technology Differences from Crowd-Scale Comparison... ASE ’18, September 3–7, 2018, Montpellier, France
Table 2: The 6 comparative sentence patterns
No.Pattern Sequence example Original sentence
1TECH * VBZ * JJR innodb has 30 higher InnoDB has 30% higher performance than MyISAM on average.
2TECH * VBZ * RBR postgresql is a more Postgresql is a more correct database implementation while mysql is less compliant.
3JJR * CIN * TECH faster than coalesce Isnull is faster than coalesce.
4RBR JJ * CIN TECH more powerful than velocity Freemarker is more powerful than velocity.
5CV * CIN TECH prefer ant over maven I prefer ant over maven personally.
6CV VBG TECH recommend using html5lib I strongly recommend using html5lib instead of beautifulsoup.
Table 3: Examples of alias
Tech term Synonyms Abbreviation
visual studio visualstudio, visual studios, visual-studio msvs
beautifulsoup beautiful soup bs4
objective-c objectivec, objective c objc, obj-c
depth-first search deep first search, depth first search, depth-first-search dfs
postgresql postgre sql, posgresq, postgesql pgsql
to extract facts instead of doubts. We lowercase all sentences to
makethesentencetokensconsistentwiththetechnologynames
because all tags are in lowercase.
3.1.2 Locating Candidate Sentences. Tolocate sentences mention-
ing a pair of comparable technologies, using only the tag names is
notenough.AspostsinStackOverflowareinformaldiscussionsabout programming-related issues, users often use alias to refer
to the same technology [ 16]. Aliases of technologies can be abbre-
viations,synonymsandsomefrequentmisspellings.Forexample,
“javascript” are often written in many forms such as “js” (abbre-
viation),“java-script”(synonym),“javascrip”(misspelling)inthe
discussions.
Thepresenceofsuchaliaseswillleadtosignificantmissingof
comparative sentences, if we match technology mentions in a sen-
tence with only tag names. Chen et al.’s work [ 17] builds a large
thesaurus of morphological forms of software-specific terms, in-
cludingabbreviations,synonymsandmisspellings.Table 3shows
some examples of technologies aliases in this thesaurus. Based on
this thesaurus, we find 7310 differentalias for 3731 software tech-
nologies. These aliases help to locate more candidate comparative
sentences that mention certain technologies.
3.1.3 Selecting Comparative Sentences. To identify comparative
sentencesfromcandidatesentences,wedevelopasetofcompar-ative sentence patterns. Each comparative sentence pattern is a
sequence of POS tags. For example, the sequence of POS tags “ RBR
JJIN”is apatternthat consistsofacomparative adverb(RBR ),an
adjective(JJ )andsubsequentlyapreposition(IN ),suchas"moreef-
ficientthan",“lessfriendlythan”,etc.Weextendthelistofcommon
POS tags to enhance the identification of comparative sentences.
More specifically, we create three comparative POS tags: CV(com-
parative verbs, e.g. prefer, compare, beat), CIN(comparative prepo-
sitions,e.g.than,over),and TECH(technologyreference,including
the name and aliases of a technology, e.g. python, eclipse).
Basedondataobservationsofcomparativesentences,wesum-
marise six comparative patterns. Table 2shows these patterns and
thecorrespondingexamplesofcomparativesentences.Tomakethe
patternsmoreflexible,weuseawildcardcharactertorepresenta
listofarbitrarywordstomatchthepattern.Foreachsentencemen-
tioningthetwocomparabletechnologies,weobtainitsPOStags
and checkif it matches anyone of six patterns. If so, thesentence
will be selected as a comparative sentence.
Figure5:Anillustrationofmeasuringsimilarityoftwocom-parative sentences
3.2 Measure Sentence Similarity
To measure the similarity of two comparative sentences, we adopt
theWordMover’sDistance[ 28]whichisespeciallyusefulforshort-
text comparison. Given two sentences S1andS2, we take one word
ifromS1and one word jfromS2. Let their word vectors be vi
andvj. The distance between the word iand the word jis the
Euclidean distance between their vectors, c(i,j)=||vi−vj||2.T o
avoid confusionbetweenword and sentencedistance, we will refer
toc(i,j)asthecostassociatedwith“traveling”fromonewordto
another.Oneword iinS1maymovetoseveraldifferentwordsinthe
S2, but its total weight is 1. So we use Tij≥0 to denote how much
of wordiinS1travels to word jinS2. It costs/summationtext.1
jTijc(i,j)to move
one word ientirely into S2. We define the distance between the
twosentencesastheminimum(weighted)cumulativecostrequired
to move all words from S1toS2, i.e.,D(S1,S2)=/summationtext.1
i,jTijc(i,j).
This problem is very similar to transportation problem i.e., how
to spend less to transform all goods from source cities A1,A2, ...
totargetcities B1,B2, ....Gettingsuchminimumcostactuallyisa
well-studied optimization problem of earth moverdistance [ 32,38].
Tousewordmover’sdistanceinourapproach,wefirsttraina
wordembeddingmodelbasedonthepostcontentofStackOverflow
sothatwegetadensevectorrepresentationforeachwordinStack
Overflow.Wordembeddinghasbeenshowntobeabletocapture
rich semantic and syntactic information of words. Our approach
doesnotconsiderwordmover’sdistanceforallwordsinasentence.
Instead, for each comparative sentence, we extract only keywords
with POS tags that are most relevant to the comparison, including
adjectives(JJ),comparativeadjectives(JJR)andnouns(NN,NNS,
NNP and NNPS), not including the technologies under comparison.
Then,wecomputetheminimalwordmovers’distancebetweenthe
keywordsinonesentenceandthoseintheothersentences.Base
on the distance, we further compute the similarity score of the two
sentences by
similarityscore (S1,S2)=1
1+D(S1,S2)
218
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yi Huang, Chunyang Chen, Zhenchang Xing, Tian Lin, and Yang Liu
Figure 6: Communities in the graph of comparative sen-
tences
The similarity score is in range (0,1), and the higher the score, the
more similar the two sentences. If the similarity score between
thetwosentencesislargerthanthethreshold,weregardthemas
similar. The threshold is 0.55 in this work, determined heuristically
by a small-scale pilot study. We show some similar comparative
sentences by word mover’s distance in Table 4.
Tohelpreaderunderstandwordmovers’distance,weshowan
exampleinFigure 5withtwocomparativesentencesforcomparing
postgresql andmysql:“Postgresqloffersmoresecurityfunctionality
thanmysql ”and“’Mysqlprovideslesssafetyfeaturesthanpostgresql ’.
The keywords in the two sentences that are most relevant to the
comparison are highlighted in bold. We see that the minimumdistance between the two sentences is mainly the accumulation
ofworddistancebetweenpairsofsimilarwords (offers,provides),
(more, less), (security, safety), and (functionality, features). As the
distance between the two sentences is small, the similarity score is
higheventhoughthetwosentencesuseratherdifferentwordsand
express the comparison in reverse directions.
3.3 Clustering Representative Comparison
Aspects
Foreachpairofcomparabletechnologies,wecollectasetofcom-
parativesentencesabouttheircomparisoninSection 3.1.Within
thesecomparative sentences, wefindpairsof similarsentencesin
Section3.2. We take each comparative sentence as one node in the
graph. If the two sentences are determined as similar, we add an
edge between them in the graph. In this way, we obtain a graph of
comparativesentencesforagivenpairofcomparativetechnologies.
Although some comparative sentences are very different in
words or comparison directions (examples shown in Fig. 5and
Table4), they may still share the same comparison opinions. In
graph theory, a set of highly correlated nodes is referred to as a
community (cluster) in the network. Based on the sentence sim-
ilarity, we cluster similar opinions by applying the community
detection algorithm to the graph of comparative sentences. In this
work,weusetheGirvan-Newmanalgorithm[ 21]whichisahierar-
chicalcommunitydetectionmethod.Itusesaniterativemodularitymaximizationmethodtopartitionthenetworkintoafinitenumber
of disjoint clusters that will be considered as communities. Eachnode must be assigned to exactly one community. Fig. 6shows the
graph of comparative sentences for the comparison of TCPand
UDP(twonetworkprotocols),inwhicheachnodeisacomparative
sentence, and the detected communities are visualized in the same
color.
AsseeninFig. 6,eachcommunitymayrepresentaprominent
comparisonaspectofthetwocomparabletechnologies.Butsome
communities maycontain toomany comparativesentences toun-
derstandeasily.Therefore,weuseTF-IDF(TermFrequencyInverse
DocumentFrequency)toextractkeywordsfromcomparativesen-
tence in one community to represent the comparison aspect of this
community.TF-IDFisastatisticalmeasuretoevaluatetheimpor-
tance of a word to a document in a collection. It consists of two
parts:termfrequency(TF,thenumberoccurrencesofatermina
document) and inverse document frequency (IDF, the logarithm of
thetotalnumberofdocumentsinthecollectiondividedbythenum-
berofdocumentsinthecollectionthatcontainthespecificterm).
For each community, we remove stop words in the sentences, and
regardeachcommunityasadocument.Wetakethetop-3words
withlargestTF-IDFscoresastherepresentativeaspectforthecom-
munity.Table 5showsthecomparisonaspectsoffourcommunities
for comparing postgresql withmysql. The representative keywords
directlyshowthatthecomparisonbetween postgresql withmysql
mainly focuses on four aspects: s peed, security , popularity, and
usability.
4 IMPLEMENTATION
4.1 Dataset
WetakethelatestStackOverflowdatadump(releasedon13March
2018)asthedatasource.Itcontains14,995,834questions,23,399,083
answers,50,812uniquetags.WiththeapproachinSection 2,wecol-
lect in total 14,876 pairs of comparable technologies. Among these
technologies, we extract 14,552 comparative sentences for 2,074
pairsofcomparabletechnologies.Weusethesetechnologiesand
comparativesentencestobuildaknowledgebasefortechnology
comparison.
4.2 Tool Support
Apart from our abstract approach, we also implement a practi-
caltool2fordevelopers.Withtheknowledgebaseofcomparable
technologies and their comparative sentences mined from Stack
Overflow, our site can return an informative and aggregated view
of comparative sentences in different comparison aspects for com-
parable technology queries. In addition, thetool provides the link
of each comparative sentence to its corresponding Stack Overflow
post so that users can easily find more detailed content.
5 EXPERIMENT
In this section, weevaluate each step of our approach. Asthereis
no ground truth for technology comparison,we have to manually
checktheresultsofeachsteporbuildthegroundtruth.Andasitisclear to judge whether a tag is of a certain category from its tag de-scription,whethertwotechnologiesarecomparable,andwhetherasentenceisacomparativesentence,werecruittwoMasterstudents
2https://difftech.herokuapp.com/
219
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Tell Them Apart: Distilling Technology Differences from Crowd-Scale Comparison... ASE ’18, September 3–7, 2018, Montpellier, France
Table 4: Examples of similar comparative sentences by Word Mover’s Distance
Comparable technology pair Comparative sentences
vmware&virtualboxVirtualbox is slower than vmware.
In my experience I’ve found that vmware seems to be faster than virtualbox.
strncpy&strcpyIn general strncpy is a safer alternative to strcpy.
So that the strncpy is more secure than strcpy.
google-chrome &safariSafari still uses the older Webkit while Chrome uses a more current one.
Google Chrome also uses an earlier version of Webkit than Safari.
quicksort &mergesortMergesort would use more space than quicksort.
Quicksort is done in place and doesn’t require allocating memory, unlike mergesort.
nginx&apacheServing static files with nginx is much more efficient than with apache.
There seems to be a consensus that nginx serves static content faster than apache.
Table 5: The representative keywords for clusters of postgresql and mysql.
Representative keywords Comparative sentences
speed,slo wer, fasterIn most regards, postgresql is slower than mysql especially when it comes to fine tuning in the end.
I did a simple performance test and I noticed postgresql is slower than mysql.
According to my own experience, postgresql run much faster than mysql.
Postgresql seem to better than mysql in terms of speed.
security, safety, functionalityTraditionally postgresql has fewer security issues than mysql.
Postgresql offers more security functionality than mysql.
Mysql provides less safety features than postgresql.
popularWhile postgresql is less popular than mysql, most of the serious web hosting supports it.
Though mysql is more popular than postgresql but instagram is using postgresql maybe due to these reasons.
It’s a shame postgresql isn’t more popular than mysql, since it supports exactly this feature out-of-the-box.
easier, simplicityMysql is more widely supported and a little easier to use than postgresql.
Postgresql specifically has gotten easier to manage while mysql has lost some of the simplicity.
However, people often argue that postgresql is easier to use than mysql.
tomanuallychecktheresultsofthesethreesteps.Onlyresultsthat
they both agree will be regarded as ground truth for computing
relevant accuracy metrics, and those results without consensus
will be given to the third judge who is a PhD student with more
experience. All three students are majoring in computer scienceand computer engineering in our school, and they have diverse
research and engineering background with different software tools
and programming languages in their work. In addition, we release
all experiment data and results in our website3.
5.1 Accuracy of Extracting Comparable
Technologies
This section reports our evaluation of the accuracy of tag category
identification, the important of tag category for filtering out irrele-
vant technologies, and the impact of word embedding models and
hyperparameters.
5.1.1 The Accuracy of Tag CCategory. From33,306tagswithtag
category extractedby ourmethod, werandomly sample1000 tags
whosecategoriesaredeterminedusingtheNLPmethod,andthe
other 1000 tags whose categories are determined by the dictionary
look-up method (see Section 2.2). Among the 1000 sampled tag
categories by the NLP method, categories of 838 (83.8%) tags are
correctly extracted by the proposed method. For the 1000 sampled
3https://sites.google.com/view/difftech/tags by the dictionary look-up method, categories of 788 (78.8%)
tags are correct.
Accordingtoourobservation,tworeasonsleadtotheerroneous
tag categories. First, some tag definition sentences are complex
which can lead to erroneous POS tagging results. For example, the
tagWikiofthetag rpy2statesthat“RPyisaverysimple,yetrobust,
PythoninterfacetotheRProgrammingLanguage”.ThedefaultPOStaggingrecognizes simpleasthenounwhichisthenregardedasthe
category by our method. Second, the dictionary look-up method
sometimesmakesmistakes,asthematchedcategorymaynotbethe
real category. Forexample, the TagWiki ofthe tag honeypot states
“A trap set to detect or deflect attempts to hack a site or system”.
Our approach matches the systemas the category of the honeypot.
5.1.2 The Importance of Tag Category. Tochecktheimportanceof
tagcategoryfortheaccuratecomparabletechnologyextraction,we
set up twomethods, i.e., one isword embedding and tagcategory
filtering, and the other is only with word embedding. The word
embedding modelin twomethods areboth skip-grammodel with
thewordembeddingdimensionas800.Werandomlysample150
technologiespairsextractedfromeachmethod,andmanuallycheck
the if the extracted technology pair is comparable or not. It shows
thattheperformanceofmodelwithtagcategory(90.7%)ismuch
better than that without the tag category filtering (29.3%).
5.1.3 The Impact of Parameters of Word Embedding. There are
two important parameters for the word embedding model, and we
220
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yi Huang, Chunyang Chen, Zhenchang Xing, Tian Lin, and Yang Liu
Table 6: The accuracy of comparative sentences extraction
No. Pattern #right #wrong Accuracy
1TECH * VBZ * JJR 44 6 88%
2TECH * VBZ * RBR 45 5 90%
3JJR * CIN * TECH 43 7 86%
4RBR JJ * CIN TECH 47 3 94%
5CV * CIN TECH 37 13 74%
6CV VBG TECH 35 15 70%
Total 251 49 83.7%
test its impact on the the performance of our method. First, we
comparetheperformanceofCBOWandSkip-grammentionedin
Section2.1byrandomlysampling150technologypairs extracted
byeachmethodunderthesameparametersetting(thewordem-
beddingdimensionis400).TheresultsshowthatSkip-gram(90.7%)
outperforms the CBOW (88.7%), but the difference is marginal.
Second, we randomly sample 150 technologies pairs by the skip-
gram model with different word embedding dimensions, and man-
ually check the accuracy. From the dimension 200 to 1000 with
the step as 200, the accuracy is 70.7%, 72.7%, 81.3%, 90.7%, 87.3%.
We can see that the model with the word embedding dimension as
800 achieves the best performance. Finally, we take the Skip-gram
modelwith800word-embeddingdimensionasthewordembedding
model to obtain the comparable technologies in this work.
5.2 Accuracy and Coverage of Comparative
Sentences
We evaluate the accuracy and coverage of our approach in finding
comparative sentences from the corpus. We first randomly sample
300 sentences (50 sentences for each comparative sentence pattern
inTable2)whichareextractedbyourmodel.Wemanuallycheck
theaccuracyofthesampledsentencesandTable 6showstheresults.
Theoverallaccuracyofcomparativesentenceextractionis83.7%,
and our approach is especially accurate for the first 4 patterns.
The lasttwo patterns donot workwelldue tothe relatively loose
conditions.
Wefurthercheckthewrongextractionofcomparativesentences
and find that most errors are caused by wrong comparable tech-
nologiesextractedinSection 2.Forexample, implodeandexplode
are not comparable technologies, but they are mentioned in sen-tence “I’m not sure why you’d serialize it in php either becauseimplodeandexplodewould be more appropriate”. In addition, al-
though some sentences do not contain the question mark, they are
actuallyinterrogativesentencesuchas“Ialsowonderifpostgresql
will be a win over mysql ”.
5.3 Accuracy of Clustering Comparative
Sentences
We evaluate the performance of our opinion clustering method by
comparing it with the baseline methods.
5.3.1 Baseline. We set up two baselines to compare with our com-
parative sentence clustering method. The first baseline is the tradi-
tionalTF-IDF [ 40]with K-means[ 23],and thesecondbaselineis
basedonthedocument-to-vectordeeplearningmodel(i.e.,Doc2vec[ 29])Table 7: Ground Truth for evaluating clustering results
No. Technology pair #comparative sentences#clusters
1compiled & interpreted language 27 5
2sortedlist & sorteddictionary 11 4
3 ant & maven 47 7
4 pypy & cpython 51 3
5google-chrome & safari 35 6
6quicksort & mergesort 54 4
7lxml & beautifulsoup 32 4
8 awt & swing 30 3
9 jackson & gson 31 3
10 swift & objective-c 72 10
11 jruby & mri 19 3
12memmove & memcpy 21 3
with K-means. Both methods first convert the comparative sen-
tencesforapairofcomparabletechnologiesintovectorsbyTF-IDF
andDoc2vec.Thenforbothmethods,wecarryoutK-meansalgo-
rithms to cluster the sentence vectors into Nclusters. To make the
baseline as competitive as possible, we set Nat the cluster number
of the ground truth. In contrast, our method specifies its cluster
numberbycommunitydetectionwhichmaydifferfromthecluster
number of the ground truth.
5.3.2 Ground Truth. Asthereisnogroundtruthforclusteringcom-
parative sentences, we ask two Master students mentioned before
to manually build a small-scale ground truth. We randomly sam-
ple15pairsofcomparabletechnologieswithdifferentnumberof
comparative sentences. For each technology pair, the two students
read each comparative sentence and each of them will individually
create several clusters for these comparative sentences. Note some
comparativesentencesareuniquewithoutanysimilarcomparativesentence,andweputallthosesentencesintoonecluster.Thenthey
will discuss with the Ph.D student about the clustering results, and
changetheclustersaccordingly.Finally,theyreachanagreement
for 12pairs ofcomparable technologies.Wetake these12 pairsas
the ground truth whose details can be seen in Table 7.
5.3.3 Evaluation Metrics. Giventhegroundtruthclusters,many
metricshavebeenproposedtoevaluatetheclusteringperformance
in the literature. In this work, we take the Adjusted Rand Index
(ARI)[25],NormalizedMutualInformation(NMI)[47],homogene-
ity, completeness, V-measure [ 39], and Fowlkes-Mallows Index
(FMI)[20].Forallsixmetrics,highervaluerepresentsbettercluster-
ingperformance.Foreachpairofcomparabletechnologies,wetake
all comparative sentences as a fixed list, and Gas a ground truth
cluster assignment and Cas the algorithm clustering assignment.
AdjustedRand Index(ARI) measuresthe similaritybetween
twopartitionsinastatisticalway.ItfirstcalculatestherawRand
Index (RI) by RI=a+b
CN
2whereais the number of pairs of elements
that arein thesame cluster in Gand alsoin thesame cluster in C,
andbisthenumberofpairsofelementsthatareindifferentclusters
inGand also in different clusters in C.CN
2is the total number of
possible pairs in the dataset (without ordering) where Nis the
number of comparative sentences. To guarantee that random label
assignments will get a value close to zero, ARI is defined as
ARI=RI−E[RI]
max(RI)−E[RI]
221
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Tell Them Apart: Distilling Technology Differences from Crowd-Scale Comparison... ASE ’18, September 3–7, 2018, Montpellier, France
whereE[RI]is the expected value of RI.
NormalizedMutualInformation(NMI) measuresthemutual
information betweenthe groundtruth labels Gand thealgorithm
clustering labels C, followed by a normalization operation:
NMI(G,C)=MI(G,C)/radicalbig
H(G)H(C)
whereH(G)istheentropyofset Gi.e.,H(G)=−/summationtext.1|G|
i=1P(i)log(P(i))
andP(i)=Gi
Nis the probability than an objet picked at random
fallsintoclass Gi.TheMI(G,C)isthemutualinformationbetween
GandCwhereMI(G,C)=/summationtext.1|G|
i=1/summationtext.1|C|
j=1P(i,j)log(P(i,j)
P(i)P(j))
Homogeneity (HOM) is the proportion of clusters containing
only members of a single class by
h=1−H(G|C)
H(G)
Completeness(COM) istheproportionofallmembersofagiven
class are assigned to the same cluster by
c=1−H(C|G)
H(C)
whereH(G|C)istheconditionalentropyoftheground-truthclasses
given the algorithm clustering assignments.
V-measure (V-M) is the harmonic mean of homogeneity and
completeness
v=2×h×c
h+c
Fowlkes-Mallows Index (FMI) is defined as the geometric
mean of the pairwise precision and recall:
FMI=TP/radicalbig
(TP+FP)(TP+FN)
whereTPisthenumberofTruePositive(i.e.thenumberofpairs
of comparativesentences that belong tothe same clustersin both
thegroundtruthandthealgorithmprediction), FPisthenumber
of False Positive (i.e. the number of pairs of comparative sentences
that belong to the same clusters in the ground-truth labels but not
inthealgorithmprediction)and FNisthenumberofFalseNegative
(i.ethenumberofpairsofcomparativesentencesthatbelongsin
the same clusters in the algorithm prediction but not in the ground
truth labels).
5.3.4 Overall Performance. Table8showstheevaluationresults.
TF-IDFwithK-meanshassimilarperformanceastheDoc2vecwith
K-means, but our model significantly outperforms both models in
all six metrics.
According to our inspection of detailed results, we find two rea-
sonswhyourmodeloutperformstwobaselines.First,ourmodelcan
capture the semantic meaning of comparative sentences. TF-IDF
canonlyfindsimilarsentencesusingthesamewordsbutcountsim-
ilar words like “secure” and “safe” as unrelated. While the sentence
vector from Doc2vec is easily influenced by the noise as it takes
all words in the sentence into consideration. Second, constructing
the similar sentences as a graph in our model explicitly encodes
the sentence relationships. The community detection based on the
graph can then easily put similar sentences into clusters. In con-
trast, for the two baselines, the error brought from the TF-IDF andTable 8: Clustering performance
Method ARI NMI HOM COM V-M FMI
TF-IDF+Kmeans 0.12 0.28 0.29 0.27 0.28 0.41Doc2vec+Kmeans -0.01 0.11 0.10 0.14 0.11 0.43Our model 0.66 0.73 0.75 0.72 0.73 0.79
Doc2vecisaccumulatedandamplifiedtoK-meansintheclustering
phase.
6 USEFULNESS EVALUATION
ExperimentsinSection 5haveshowntheaccuracyofourapproach.
In this section, we further demonstrate the usefulness of our ap-
proach. According to our observation of Stack Overflow, there are
some questions discussing comparable technologies such as “Whatis the difference between Swing and AWT ”. We demonstrate the use-
fulness of the technology-comparison knowledge our approach
distills from Stack Overflow discussions by checking how well the
distilled knowledge by our approach can answer those questions.
6.1 Evaluation Procedures
Weusethenameofcomparabletechnologieswithseveralkeywords
suchascompare,vs,difference tosearchquestionsinStackOverflow.
Wethenmanuallycheckwhichofthemaretrulyaboutcomparable
technologycomparison,andrandomlysamplefivequestionsthat
discusscomparabletechnologiesindifferentcategoriesandhave
at least five answers. The testing dataset can be seen in Table 9.
WethenaskthetwoMasterstudentstoreadeachsentencein
all answers and cluster all sentences into several clusters whichrepresent developers’ opinions in different aspects. To make thedata as valid as possible, they still first carry out the clusteringindividually and then reach an agreement after discussions. Foreach comparative opinion in the answer, we manually check ifthat opinion also appears in the knowledge base of comparativesentences extracted by our method. To make this study fair, our
method does not extract comparative sentences from answers of
questions used in this experiment.
6.2 Results
Table10showstheevaluationresults.Wecanseethatmostcom-
parison(72%)aspectscanbecoveredbyourknowledgebase.For
two questions (#5970383 and #46585), the technology-comparison
knowledge distilled by our method can cover all of comparison
aspectsintheoriginal answerssuch asspeed,r eliability, data size
for comparing postandget. While for the other three questions,
our model can still cover more than half of the comparison aspects.
We miss some comparison aspects for the other three questions,
such as“One psychologicalreason that hasnot been givenis simply
that Quicksort is more cleverly named, i.e. good marketing.”, “ The
VMWareWorkstationclientprovidesanicerend-userexperience(sub-
jective,Iknow...) ”and“AnotherstatementwhichIsawisthatswingis
MVC based and awt is not.”. Such opinions are either too subjective
ortoodetailed whichrarelyappearagainin otherStackOverflow
discussions, leading to not having them in our knowledge base.
Apartfromcomparisonaspectsappearedintheoriginalanswers,
ourtoolcanprovidesomeuniqueopinionsfromotheraspects,such
222
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Yi Huang, Chunyang Chen, Zhenchang Xing, Tian Lin, and Yang Liu
Table 9: Comparative questions
Question ID Question title Tech pair Tech category #answers
70402 Why is quicksort better than mergesort? quicksort & mergesort Algorithm 29
5970383 Difference between TCP and UDP tcp & udp Protocol 9
630179 Benchmark: VMware vs Virtualbox vmware & virtualbox IDE 13
408820 What is the difference between Swing and AWT? swing & awt Library 8
46585 When do you use POST and when do you use GET? post & get Method 28
Table 10: Distilled knowledge by our approach versus origi-
nal answers
Question ID #Aspects #Covered #Unique in our model
70402 6 4 (66.67%) 2
5970383 3 3 (100%) 5
630179 7 4 (57.1%) 1
408820 5 3 (60%) 4
46585 4 4 (100%) 2
Total 25 18 (72%) 14
as“Inmyexperience,udpbasedcodeisgenerallylesscomplexthantcp
basedcode ”forcomparing tcpandudp,“howeverIfoundthatvmware
is much more stable in full screen resolution to handle the iphone
connectionviausb ”forcomparing vmwareandvirtualbox,and“GET
would obviously allow for a user to change the value a lot easier than
POST” for comparing postandget. As seen in Table 10, our model
canprovidemore thanoneuniquecomparativeaspects whichare
not in the existing answers for each technology pair. Therefore,our knowledge base can be a good complement to these existing
technology-comparison questions with answers. Furthermore, our
knowledge base contains the comparison knowledge of 2074 pairs
ofcomparabletechnologies,manyofwhichhavenotbeenexplicitlyaskedanddiscussedinStackOverflow,suchas swiftandobjective-c,
nginxandapache.
7 RELATED WORKS
Findingsimilarsoftwareartefactscanhelpdevelopersmigratefromonetooltotheotherwhichismoresuitabletotheirrequirement.Butitisachallengingtasktoidentifysimilarsoftwareartefactsfromthe
existinglargepoolofcandidates.Therefore,muchresearcheffort
hasbeenputintothisdomain.Differentmethodshasbeenadopted
to mine similar artefacts ranging from high-level software [ 33,
43], mobile applications [ 19,31], github projects[ 49] to low-level
third-partylibraries[ 11,13,42],APIs[22,37],codesnippets[ 41],
or Q&A questions [ 18]. Compared with these research studies,
the mined software technologies in this work has much broader
scopeincludingnotonlysoftware-specificartefacts,butalsogeneral
software concepts (e.g., algorithm, protocol), tools (e.g., IDE).
Givenalistofsimilartechnologies,developersmayfurthercom-
pare and contrast them for the final selection. Some researcherinvestigate such comparison, the comparison is highly domain-specific such as software for traffic simulation [
26], regression
models [24], x86 virtualization [ 7], etc. Michail and Notkin [ 34]
assess different third-party libraries by matching similar compo-nents (such as classes and functions) across similar libraries. But
it can onlywork for library comparisonwithout the possibility to
beextendedtootherhigher/lower-leveltechnologiesinSoftwareEngineering. Instead, we find developers’s preference of certain
software technologies highly depends on other developers’ usage
experienceandreportofsimilartechnologycomparisons.There-fore,UddinandKhomh[
45,46]extractAPIopinionsentencesin
differentaspectstoshowdevelopers’sentimenttothatAPI.Liet
al.[30]adoptNLPmethodstodistillcomparativeuserreviewabout
similar mobile Apps. Different from their works, we first explicitly
extract a large pool of comparable technologies. In addition, apart
fromextractingcomparativesentences,wefurtherorganizethem
into different clusters and represent each cluster with some key-
wordstohelpdevelopersunderstandcomparativeopinionsmore
easily.
Finally, it is worth mentioning some related practical projects.
SimilarWeb[ 6]isawebsitethatprovidesbothusersengagementsta-
tisticsandsimilarcompetitorsforwebsitesandmobileapplications.
AlternativeTo[ 4]isasocialsoftwarerecommendationwebsitein
which users can find alternatives to a given software based on user
recommendations. SimilarTech [ 5] isa siteto recommendanalogi-
calthird-partylibrariesacross differentprogramming languages.
Thesewebsitescanhelpusersfindsimilaroralternativewebsites
or software applications without detailed comparison.
8 CONCLUSION AND FUTURE WORK
Inthispaper,wepresentanautomaticapproachtodistillandaggre-
gatecomparativeopinionsofcomparabletechnologiesfromQ&A
websites.Wefirstobtainalargepoolofcomparabletechnologiesby
incorporating categorical knowledge into word embedding of tags
in Stack Overflow, and then locate comparative sentences about
these technologies by POS-tag based pattern matching, and finally
organizecomparativesentencesintoclustersforeasierunderstand-
ing. The evaluation shows that our system covers a large set of
comparable technologies and their corresponding comparative sen-
tences with high accuracy. We also demonstrate the potential of
our system to answer questions about comparing comparable tech-
nologies, because the technology comparison knowledge mined
usingoursystemlargelyoverlapwiththeoriginalanswersinStack
Overflow.
Apartfromcomparativesentencesexplicitlymentioningboth
comparable technologies, some comparative opinions may hide
deeper.Forexample,onedeveloperexpresseshisopinionsaboutonetechnologyinoneparagraphwhilediscussingtheothertechnology
in the next paragraph. Therefore, we will improve our system to
distilltechnologycomparisonknowledgefromthecurrentsentence
leveltopostlevel.Inaddition,wealsoplantosummarizehigher-
levelopinionsorpreferencesfromseparatedindividualcomparative
sentences for easier understanding.
223
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Tell Them Apart: Distilling Technology Differences from Crowd-Scale Comparison... ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]2003. Alphabetical list of part-of-speech tags used in the Penn Treebank
Project.https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_
pos.html. Accessed: 2018-02-02.
[2]2017. Millions of Queries per Second: PostgreSQL and MySQLâĂŹs Peaceful
Battle at TodayâĂŹs Demanding Workloads. https://goo.gl/RXVjkB/. Accessed:
2018-04-05.
[3]2017.MySQLvsPostgres. https://www.upguard.com/articles/postgres-vs-mysql/.
Accessed: 2018-04-05.
[4]2018. AlternativeTo - Crowdsourced software recommendations. https://
alternativeto.net/. Accessed: 2018-04-05.
[5]2018. SimilarTech: Find alternative libraries across languages. https://
graphofknowledge.appspot.com/similartech/. Accessed: 2018-04-05.
[6] 2018. SimilarWeb. https://www.similarweb.com/. Accessed: 2018-04-05.
[7]Keith Adams and Ole Agesen. 2006. A comparison of software and hardware
techniquesforx86virtualization. ACMSIGARCHComputerArchitectureNews 34,
5 (2006), 2–13.
[8]LingfengBao,JingLi,ZhenchangXing,XinyuWang,XinXia,andBoZhou.2017.
Extracting and analyzing time-series HCI data from screen-captured task videos.
Empirical Software Engineering 22, 1 (2017), 134–174.
[9] Anton Barua, Stephen W Thomas, and Ahmed E Hassan. 2014. What are devel-
operstalkingabout?ananalysisoftopicsandtrendsinstackoverflow. Empirical
Software Engineering 19, 3 (2014), 619–654.
[10]Steven Bird and Edward Loper. 2004. NLTK: the natural language toolkit. In
Proceedings of the ACL 2004 on Interactive poster and demonstration sessions.
Association for Computational Linguistics, 31.
[11]Chunyang Chen, Sa Gao, and Zhenchang Xing. 2016. Mining analogical libraries
inq&adiscussions–incorporatingrelationalandcategoricalknowledgeintoword
embedding. In Software Analysis, Evolution, and Reengineering (SANER), 2016
IEEE 23rd International Conference on, Vol. 1. IEEE, 338–348.
[12]Chunyang Chen and Zhenchang Xing. 2016. Mining technology landscape from
stack overflow. In Proceedings of the 10th ACM/IEEE International Symposium on
Empirical Software Engineering and Measurement. ACM, 14.
[13]ChunyangChenandZhenchangXing.2016. Similartech:automaticallyrecom-
mendanalogicallibrariesacrossdifferentprogramminglanguages.In Automated
SoftwareEngineering(ASE),201631stIEEE/ACMInternationalConferenceon .IEEE,
834–839.
[14]Chunyang Chen and Zhenchang Xing. 2016. Towards correlating search ongoogle and asking on stack overflow. In Computer Software and Applications
Conference (COMPSAC), 2016 IEEE 40th Annual, Vol. 1. IEEE, 83–92.
[15]Chunyang Chen, Zhenchang Xing, and Lei Han. 2016. Techland: Assisting
technologylandscapeinquirieswithinsightsfromstackoverflow.In Software
Maintenance and Evolution (ICSME), 2016 IEEE International Conference on. IEEE,
356–366.
[16]ChunyangChen,ZhenchangXing,andYangLiu.2017. BytheCommunity&For
theCommunity:ADeepLearningApproachtoAssistCollaborativeEditingin
Q&A Sites. Proceedings of the ACM on Human-Computer Interaction 1, 32 (2017),
1–32.
[17]Chunyang Chen, Zhenchang Xing, and Ximing Wang. 2017. Unsupervised
software-specificmorphologicalformsinferencefrominformaldiscussions.In
Proceedings of the 39th International Conference on Software Engineering. IEEE
Press, 450–461.
[18]Guibin Chen, Chunyang Chen, Zhenchang Xing, and Bowen Xu. 2016. Learning
adual-languagevectorspacefordomain-specificcross-lingualquestionretrieval.
InAutomated Software Engineering (ASE), 2016 31st IEEE/ACM International Con-
ference on. IEEE, 744–755.
[19]Ning Chen, Steven CH Hoi, Shaohua Li, and Xiaokui Xiao. 2015. SimApp: A
frameworkfordetectingsimilarmobileapplicationsbyonlinekernellearning.In
Proceedingsof theEighthACM InternationalConferenceon WebSearchand Data
Mining. ACM, 305–314.
[20]Edward B Fowlkes and Colin L Mallows. 1983. A method for comparing twohierarchical clusterings. Journal of the American statistical association 78, 383
(1983), 553–569.
[21]MichelleGirvanandMarkEJNewman.2002. Communitystructureinsocialand
biological networks. Proceedings of the national academy of sciences 99, 12 (2002),
7821–7826.
[22]XiaodongGu,HongyuZhang,DongmeiZhang,andSunghunKim.2017.DeepAM:
MigrateAPIswithmulti-modalsequencetosequencelearning. arXivpreprint
arXiv:1704.07734 (2017).
[23]John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means
clustering algorithm. Journal of the Royal Statistical Society. Series C (Applied
Statistics) 28, 1 (1979), 100–108.
[24]Nicholas J Horton and Stuart R Lipsitz. 2001. Multiple imputation in practice:
comparison of software packages for regression models with missing variables.
The American Statistician 55, 3 (2001), 244–254.
[25]Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of
classification 2, 1 (1985), 193–218.[26] Steven L Jones, Andrew J Sullivan, Naveen Cheekoti, Michael D Anderson, and
DMalave.2004. Trafficsimulationsoftwarecomparisonstudy. UTCAreport 2217
(2004).
[27]JunâĂŹichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as ex-
ternalknowledgefornamedentityrecognition.In Proceedingsofthe2007Joint
ConferenceonEmpiricalMethodsin NaturalLanguageProcessingandComputa-
tional Natural Language Learning (EMNLP-CoNLL). 698–707.
[28]Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word
embeddingstodocumentdistances.In InternationalConferenceonMachineLearn-
ing. 957–966.
[29]Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and
documents. In International Conference on Machine Learning. 1188–1196.
[30]Yuanchun Li, Baoxiong Jia, Yao Guo, and Xiangqun Chen. 2017. Mining UserReviews for Mobile App Comparisons. Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies 1, 3 (2017), 75.
[31]Mario Linares-Vásquez, Andrew Holtzhauer,and Denys Poshyvanyk. 2016. On
automatically detecting similar android apps. In Program Comprehension (ICPC),
2016 IEEE 24th International Conference on. IEEE, 1–10.
[32]Haibin Ling and Kazunori Okada. 2007. An efficient earth mover’s distance
algorithmforrobusthistogramcomparison. IEEEtransactionsonpatternanalysis
and machine intelligence 29, 5 (2007), 840–853.
[33]Collin McMillan, Mark Grechanik, and Denys Poshyvanyk. 2012. Detecting
similar software applications. In Proceedings of the 34th International Conference
on Software Engineering. IEEE Press, 364–374.
[34]Amir Michail and David Notkin. 1999. Assessing software libraries by browsing
similarclasses,functionsandrelationships.In Proceedingsofthe21stinternational
conference on Software engineering. ACM, 463–472.
[35]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
estimationofwordrepresentationsinvectorspace. arXivpreprintarXiv:1301.3781
(2013).
[36]TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean.2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111–3119.
[37]TrongDucNguyen,AnhTuanNguyen,HungDangPhan,andTienNNguyen.
2017. Exploring API embedding for API usages and applications. In Software
Engineering (ICSE), 2017 IEEE/ACM 39th International Conference on. IEEE, 438–
449.
[38]OfirPeleandMichaelWerman.2009. Fastandrobustearthmover’sdistances.In
Computer vision, 2009 IEEE 12th international conference on. IEEE, 460–467.
[39]AndrewRosenbergandJuliaHirschberg.2007. V-measure:Aconditionalentropy-
basedexternalclusterevaluationmeasure.In Proceedingsofthe2007jointcon-
ferenceonempiricalmethodsinnaturallanguageprocessingandcomputational
natural language learning (EMNLP-CoNLL).
[40]Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its
application in retrieval. Journal of documentation 28, 1 (1972), 11–21.
[41]Fang-Hsiang Su, Jonathan Bell, Gail Kaiser, and Simha Sethumadhavan. 2016.
Identifying functionally similar code in complex codebases. In Program Compre-
hension (ICPC), 2016 IEEE 24th International Conference on. IEEE, 1–10.
[42]CédricTeyton,Jean-Rémy Falleri,andXavierBlanc.2013. Automaticdiscovery
of function mappings between similar libraries. In Reverse Engineering (WCRE),
2013 20th Working Conference on. IEEE, 192–201.
[43]FerdianThung,DavidLo,andLingxiaoJiang.2012. Detectingsimilarapplica-
tions with collaborative tagging. In Software Maintenance (ICSM), 2012 28th IEEE
International Conference on. IEEE, 600–603.
[44]Christoph Treude, Ohad Barzilay, and Margaret-Anne Storey. 2011. How doprogrammers ask and answer questions on the web?: Nier track. In Software
Engineering (ICSE), 2011 33rd International Conference on. IEEE, 804–807.
[45]Gias Uddin and Foutse Khomh. 2017. Automatic summarization of API reviews.
InAutomated Software Engineering (ASE), 2017 32nd IEEE/ACM International
Conference on. IEEE, 159–170.
[46]Gias Uddin and Foutse Khomh. 2017. Opiner: an opinion search and summariza-
tionengineforAPIs.In Proceedingsofthe32ndIEEE/ACMInternationalConference
on Automated Software Engineering. IEEE Press, 978–983.
[47]NguyenXuanVinh,JulienEpps,andJamesBailey.2010. Informationtheoretic
measures for clusterings comparison: Variants, properties, normalization andcorrection for chance. Journal of Machine Learning Research 11, Oct (2010),
2837–2854.
[48]Deheng Ye, Zhenchang Xing, Chee Yong Foo, Zi Qun Ang, Jing Li, and Nachiket
Kapre.2016. Software-specificnamedentityrecognitioninsoftwareengineering
social content. In Software Analysis, Evolution, and Reengineering (SANER), 2016
IEEE 23rd International Conference on, Vol. 1. IEEE, 90–101.
[49]Yun Zhang, David Lo, Pavneet Singh Kochhar, Xin Xia, Quanlai Li, and Jianling
Sun. 2017. Detecting similar repositories on GitHub. In Software Analysis, Evolu-
tionandReengineering(SANER),2017IEEE24thInternationalConferenceon.IEEE,
13–23.
224
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. 