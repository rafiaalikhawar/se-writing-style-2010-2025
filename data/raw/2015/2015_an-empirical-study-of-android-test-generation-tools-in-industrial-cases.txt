An Empirical Study of Android Test Generation Tools in
Industrial Cases
Wenyu Wang
University of Illinois at
Urbana-Champaign, USA
wenyu2@illinois.eduDengfeng Li
University of Illinois at
Urbana-Champaign, USA
dli46@illinois.eduWei Yang
University of Texas at Dallas, USA
weiyang.utd@gmail.com
Yurui Cao
University of Illinois at
Urbana-Champaign, USA
yuruic2@illinois.eduZhenwen Zhang
Yuetang Deng
adazhang@tencent.com
yuetangdeng@tencent.com
Tencent Inc., ChinaTao Xie
University of Illinois at
Urbana-Champaign, USA
taoxie@illinois.edu
ABSTRACT
User Interface (UI) testing is a popular approach to ensure the
qualityofmobileapps.Numeroustestgenerationtoolshavebeen
developed to support UI testing on mobile apps, especially for An-
droid apps. Previous work evaluates and compares different test
generation tools using only relatively simple open-source apps,
whilereal-worldindustrialappstendtohavemorecomplexfunc-
tionalities and implementations. There is no direct comparison
among test generation tools with regard to effectiveness and ease-
of-use on these industrial apps. To address such limitation, we
studyexistingstate-of-the-artorstate-of-the-practicetestgenera-
tion tools on 68 widely-used industrial apps. We directly compare
the tools with regard to code coverage and fault-detection abil-
ity.Accordingtoourresults,Monkey,astate-of-the-practicetool
from Google, achieves the highest method coverage on 22 of 41
appswhose methodcoverage datacanbe obtained.Of all68apps
under study, Monkey also achieves the highest activity coverageon 35 apps, while Stoat, a state-of-the-art tool, is able to triggerthe highest number of unique crashes on 23 apps. By analyzingthe experimental results, we provide suggestions for combining
differenttestgenerationtoolstoachievebetterperformance.We
alsoreportourexperienceinapplyingthesetoolstoindustrialapps
under study. Our study results give insights on how Android UI
testgenerationtoolscouldbeimprovedtobetterhandlecomplex
industrial apps.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3240465KEYWORDS
Android UI testing, test generation, empirical study
ACM Reference Format:
WenyuWang,DengfengLi,WeiYang,YuruiCao,ZhenwenZhang,Yuetang
Deng, and Tao Xie. 2018. An Empirical Study of Android Test Genera-
tion Tools in Industrial Cases. In Proceedings of the 2018 33rd ACM/IEEE
International Conference on Automated Software Engineering (ASE ’18), Sep-
tember3–7,2018,Montpellier,France. ACM,NewYork,NY,USA, 11pages.
https://doi.org/10.1145/3238147.3240465
1 INTRODUCTION
As the fast pace of Android app development and evolution con-
tinues,effectivequalityassuranceforindustrialAndroidappsbe-
comesincreasinglynecessaryanddemanding.UserInterface(UI)
testing, aiming to uncover potential app defects (e.g., crashing)
by mimicking human interactions, has long been an important
approach to ensure the quality of Android apps before their de-livery to end users. To facilitate UI test automation, the Android
developertoolkitfromGoogleprovidesMonkey[ 17],anautomatic
test generation tool that sends randomly generated UI event se-quences to an app under test. In addition, researchers have alsoproposed various test generation tools to automate Android UI
testing [1, 3,6,7,23,24,26,27,29,31–33].
Theseindustrialoracademictestgenerationtoolsallshowsatis-
factory performanceaccording to their ownrespective evaluation
on various open-source or industrial apps. Table 1shows the statis-
ticsofsubjectsusedforevaluatingexistingAndroidtestgeneration
tools(publishedinmajorsoftwareengineeringconferences).The
lastrowofthetablealsoshowsastudyconductedbyChoudharyetal.[
8]in2015bycomparingdifferentAndroidtestgenerationtools.
Inthetable,coveragecomparison(denotedas‘Emp.Comp.’)shows
thenumbersofopen-sourceapps(denotedas‘#Opn.’)andindus-
trialapps(denotedas‘#Ind.’)usedinevaluatingtheproposedtool’scapability in terms of code coverage or/and fault detection (against
othertools).Bydefault,thecodecoverageiscomparedacrosstools.
We mark with♣these entries where both code coverage and fault
detectionarecompared.‘Case#Ind.’showsthenumbersofindus-
trial apps used in case studies for the proposed tools. These case
studies does notreport code coverage or compare the proposed
tool against other related previous tools. The sole purpose of these
738
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France W. Wang, D. Li, W. Yang, Y. Cao, Z. Zhang, Y. Deng, and T. Xie
Table 1: Overview of existing Android test generation tools
and their evaluation subjects
Tool/Study VenueEmp. Comp. Case
#Ind.#Opn.#Ind.
A3E[6]OOPSLA’13 00/Diamond28
ACTEve [3] FSE’12 050
DroidBot [24] ICSE-C’17 020
Dynodroid [26] FSE’13 0501000
GUIRipper [1] ASE’12 0♣10
Monkey [17] - ---
Sapienz [27] ISSTA’16 0♣681000
Stoat [29] FSE’17 0♣931661
SwiftHand [7] OOPSLA’13 010 0
WCTester [32] FSE-Ind’16/triangle100
Study by [8] ASE’15 0♣68 0
♣: Both code coverage and fault detection are compared across
tools on open source apps.
/Diamond: Only code coverage is measured whereas fault detection is not
measured on industrial apps.
/triangle: The tool is compared with only Monkey but no other tools.
studies is to evaluate the proposed tools’ applicability on indus-
trialAndroidtestingtasks(byreportingtheresultsof onlyfault
detection).OneexceptionthereisA3E[6],whichisevaluatedon
only code coverage without on fault detection (note that no tool
comparison is conducted there).
AsshowninTable 1,thereexists nocomparisonamongexist-
ingtoolsoverindustrialappsintermsofbothcodecoverage
and fault detection . Subjects for empirical tool comparison (in
the‘Emp.Comp.’column)includeonlyopen-sourceapps,withone
exception (WCTester [ 32]) where the proposed tool is compared
withonly onebaselinetool(Monkey)ononly oneindustrialapp
(WeChat). In addition, although the case-study evaluation of a few
toolsincludesindustrialapps(inthe‘#CaseInd.’column),notool
comparisonisconductedthere,andnocasestudiesonindustrial
apps measureboth codecoverage and faultdetection. Thereexist
agapandyetastrongneedtoinvestigateandcomparehowwell
theseproposedtoolsperformonindustrialappsthat,incontrastto
open-source apps, are usually (1) much more complex with regard
tofunctionalitiesandimplementations,(2)bettermaintainedand
tested, and (3) with much larger user bases and higher impacts.
To fill this gap and give practitioners and researchers insights
on how existing tools perform on industrial apps, in this paper, we
present the first empirical study that conducts comparison among
existingtoolsonindustrialappsintermsofbothcodecoverageand
fault detection. In particular, we investigate how existing available
state-of-the-art or state-of-the-practice test generation tools per-
formon68widely-usedindustrialappsintermsofcodecoverage
(method and activity coverage) and fault detection (the number
ofdistincttriggeredcrashes).Theseappsspanacross30different
categories and each of these apps has at least 1 million installs
according to GooglePlay [ 15]. We empirically studythe coverage
and fault-detection results to gain insights on each tool’s strengths
and weaknesses. We also study how to efficiently combine some of
these tools to achieve better code coverage or fault detection capa-
bilities on testing industrial apps. We also report our experience in
applying these tools to testing tasks for industrial Android apps.Inthispaper,ourempiricalstudyprovidesappdevelopersand
toolresearchers/vendors withinsightsonthe strengthsandweak-
nessesofexistingtestgenerationtools,helpingthemimprovetheir
tools’ design and implementation and their handling of realistic
tasks for industrial apps. In particular, we address four main re-
search questions in our study:
•RQ1:Whatisthecodecoverage(methodcoverageandac-
tivity coverage) achieved by each test generation tool under
study on applicable industrial apps?
•RQ2:How many unique crashes can each test generation
tool trigger on each applicable industrial app? What are the
causes of these crashes?
•RQ3:How to efficiently combine multiple test generation
toolsonapplicableindustrialappstoachievebettercoverage
and fault detection than applying these tools individually?
•RQ4:How much effort does it require to set up each test
generation tool for testing industrial apps?
Werundifferenttestgenerationtoolsunderstudyonselected
industrialappstostudytheeffectivenessofthesetools.Accordingto
ourresults,Monkeyachievesthehighestmethodcoverageon22of
41appswhosemethodcoveragedatacanbeobtained.Ofall68apps
understudy,Monkeyalsoachievesthehighestactivitycoverageon35apps,whileStoatisabletotriggerthehighestnumberofunique
crashes on 23 apps.
Togain betterunderstandingofthe toolperformanceon indus-
trialapps,similartoapreviousstudymethodology[ 21],werank
each covered method/activity or triggered unique crash in eachindustrial app based on the number of test generation tools that
havecoveredthemethod/activityortriggeredtheuniquecrash.For
instance, a method/activity or unique crash is considered rank-1if only one test generation tool has covered the method/activity
ortriggeredtheuniquecrash.Ourresultsshowthat,onmanyin-
dustrial apps, Monkey has the highest numbers of rank-1 methods
andactivities,andStoatisabletotriggerthehighestnumbersof
rank-1uniquecrashes.Ouranalysisalsoprovidessuggestionsfor
combining multiple tools for better coverage and/or fault detection
than applying these tools individually.
In summary, this paper makes the following main contributions:
•Empiricalinvestigationontheeffectivenessofexistingavail-ableAndroidUItestgenerationtoolswhenbeingappliedon
industrial apps.
•Detailedanalysisofthecoverageresultsachievedbyeach
tool to provide insights on the strengths and weaknesses of
thetestgenerationtoolsunderstudyandonhowtobetter
leverage these tools.
•Hands-on experience report of applying multiple state-of-
the-art or state-of-the-practice test generation tools on com-
plex industrial apps.
•A strong implication that testing researchers for Androidtest generation tools should empirically compare a newly
proposed tool with related previous tools on industrial apps
besidesopen-sourceapps,going beyondthecurrentcommon
research practice of comparing tools on onlyopen-source
apps.
739
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Android Test Generation Tools in Industrial Cases ASE ’18, September 3–7, 2018, Montpellier, France
2 BACKGROUND
In this section, we present an overview of the Android app compo-
nents and the Android OS architecture.
2.1 Android App Components
From the view of users and other apps, an Android app consists of
fourtypesofcomponents:Activities,IntentFilters,Services,and
Content Providers [14].
Activities. ActivitiesaredesignedtoshowUIscreensconsistingof
setsoflayoutsandUIwidgets(e.g.,buttons).Widgetsareassociated
withsetsofattributes(e.g.,sizesandpositions)andcanbeboundto
callbackmethodstohandleUIevents(e.g.,shortclicks).Anactivity
is typically used for a single specific scenario such as logging in
and user registration.
Intent Filters. Intents are messaging objects used by components
withinanapporacrossdifferentappstocommunicatewitheach
other. Intent filters are used to allow only the designated intent
types to be received and processed by the components. Launching
an app, for instance, is achieved by sending a specific intent to the
app main activity that intercepts such intents.
Services.Services are intended to perform tasks in the back-
ground without being attached to a UI screen. Downloading tasks,
forexample,areusuallyimplementedusingservicestoavoidblock-
ing the usage of app main functionalities.
Content Providers. Content providers enable apps to expose and
manage a globally shared set of data. For instance, a user’s contact
informationisstoredinanAndroidsystemappandmaybeaccessed
by other apps using the specific content provider.
2.2 Android OS Architecture
TheAndroidOSisanopen-sourceLinux-basedsoftwarestack[ 18].
Androidappsrunwithinindividualsandboxes,namelyinstances
of the Dalvik Virtual Machine (DalvikVM) [ 12], on top of native
libraries and the Linux kernel. Android frameworks, which are
responsible for low-level functionalities of the Android OS includ-
ingUIandactivitymanagement,alsorunwithininstancesofthe
DalvikVMandcanbereachedbyappsviaAndroidAPIs.System
apps are pre-installed on the Android OS to provide users with
basic features such as phone calling and SMS sending.
The Java source code of an Android app is compiled into dex-
code[11]andsubsequentlypackagedasanAndroidPackage(APK)
file along with other resource files. Developers are also allowed to
write their app libraries in C/C++ as native libraries and invoke
them through the Java Native Interface (JNI). The app can then
be installed on a compatible Android OS. At runtime, the system’s
DalvikVM reads the app’s dex-code and executes it. Starting from
Android4.4,theAndroidRuntime(ART)isincludedwiththeAn-
droid OS, where the ART translates and optimizes dex-code to
native machine code during installation to enable faster execution.
NotethatbothDalvikVMandARThavethe 64Kreferencelimitation
(i.e.,therecannotbemorethan65,536methodsinasingle .dexfile
thatcontainsanapp’sdex-code)duetothedesignoftheDalvikVM
instruction set. Android provides multidex support [13] to mitigate
this limitation.3 SELECTION OF ANDROID TEST
GENERATION TOOLS
We choose 6 state-of-the-art or state-of-the-practice UI test genera-
tion tools for our study. Monkey [ 17] is the official test generation
tool shipped with all Android devices, while the rest are all pub-
lishedattopvenuesofsoftwareengineering.Weselecttoolsthat
are applicable on at least half of the industrial apps under study.
Table2presentsanoverviewofthetestgenerationtoolsthatwe
examine and our decision on tool selection.
Inthissection,wefirstpresenttheselectedtoolsforourstudy.
Wethenillustratetheexcludedtoolsalongwiththereasonswhy
these tools cannot be included for the study.
3.1 Selected Tools under Study
3.1.1 Monkey. Monkey [ 17] is a purely randomized Android
test generation tool(from Google)that generatespseudo-random
streamsofUIevents(e.g.,clicks,touches,andgesturesonUI)and
limitedtypesofsystem-levelevents(suchasvolumecontrols)to
unmodified Android apps. Monkey is the most widely used tool in
industrialsettingsduetoitsapplicabilitytoavarietyofapplication
settings (e.g., easeof use andcompatibility with differentAndroid
platforms) [8].
3.1.2 WCTester. To inherit the advantages of Monkey while ad-
dressingitsmajorlimitations,theWeChatteamdevelopsanewap-proach[
32,33]incorporatingthreemainstrategies.First,WCTester
findsandtriggersonlyenabledeventsoneachUIscreen.Second,
WCTester focuses on generating events with higher chances to
change current UI states. Third, WCTester considers the UI statehistory and avoids repetitions during exploration. The new ap-
proachleadstosignificantperformanceimprovementsontesting
the WeChat app, one of the most popular messenger apps in the
world with over 1 billion monthly active users [22].
3.1.3 Sapienz. Sapienz[27,28]isanevolutionary-testing-based
testgenerationtoolforAndroidUItesting.Itleveragesagenetic
algorithm[ 9]toevolvegeneratedseedinputsequencestosearch
fortheoptimizedtestsuitescontainingshortinputsequenceswhile
maximizingcodecoverageandfaultrevelation. Pre-definedinput
sequences(i.e.,motifgenes)areleveragedtocomplementtheran-
dom exploration and provide local exercise for different types of
UIwidgets. Stringresources insideapps areextracted asseedsfor
text inputs. Multi-level instrumentation is supported to accommo-
datevariousapps.Testsuitescanbeevaluatedsimultaneouslyon
multiple devices to speedupthesear chprocess.
3.1.4 Stoat. Stoat[29]isaUItestgenerationtoolforAndroid
apps,with model-basedevolutionary testing.It constructsa prob-
abilistic UI state-transition model through dynamic explorationand optional static analysis at the first stage. It then evolves themodel to search for the optimized model with regard to compre-
hensivefitnessscoresoftheconcreteinputsequencesderivedfromGibbssampling[
4]onmodels.Codecoverage,modelcoverage,and
test-suitediversityarereflectedinthefitnessscore.System-level
events are also randomly injected to further enhance the testing
effectiveness.
740
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France W. Wang, D. Li, W. Yang, Y. Cao, Z. Zhang, Y. Deng, and T. Xie
Table 2: Overview of Android test generation tools under study
Tool Open SourceNo Need of Modification Exploration
StrategyNo Need of App
Source Code App Platform
Monkey [17]    Random 
WCTester [32, 33]    Random 
Sapienz [27]  * Evolutionary 
Stoat [29]  * Model-based Evolutionary 
DroidBot [24]    Model-based 
A3E-Depth-First [6]    Systematic 
*Instrumentation is optional for Sapienz and Stoat.
3.1.5 DroidBot. DroidBot [ 24] is a programmable, light-weight,
and model-based Android UI testing tool. It generates UI-guided
test inputs based on a state-transition model constructed on thefly.Italsoallowsdeveloperstowritetestingscriptstocustomize
the exploration strategy. Detailed testing reports are provided after
eachtesttohelpdevelopersunderstandapps’behavior.DroidBot
has received over 300 stars on GitHub [25] at the time of writing.
3.1.6 A3E-Depth-First. A3E[6] includes a systematic testing
tool (i.e.,A3E-Depth-First) that performs a depth-first search strat-
egy during exploration. Such a search strategy mimics user actions
and aims to thoroughly cover app functionalities. Another strategy
namedTargeted Exploration is alsoproposed for fast, directexplo-
ration of activities (as opposed to the general-purpose exploration
thataims forhighercode coverageorfault detection)inA3E.The
strategy is based on high-level control flow graphs capturing ac-
tivity transitions and constructed by performing static dataflow
analysis on apps’ bytecode.
3.2 Excluded Tools and Reasons
This section describes the Android test generation tools that are
published in top venues but are not included in our study. Wefurther provide reasons why these Android test generation tools
are not applicable for the study.
3.2.1 Dynodroid. Dynodroid [ 26] is a guided random testing
toolthatgeneratesuserUIeventsandsystem-levelevents.Byinstru-
mentingtheAndroidOS,Dynodroidcomputesthesetofrelevant
events that can execute code of the app under test. Furthermore,
Dynodroid generates more system-level events than Monkey such
as incoming phone calls and geolocation changes.
Reason.DynodroidworksononlyemulatorswithAndroidOS
version2.3duetotherequirementofinstrumentingtheAndroid
platform, and the authors of Dynodroid publish only the instru-mented version for Android 2.3. Very few industrial apps under
our study still support such an outdated Android system that was
released in 2010.
3.2.2 GUIRipper. GUIRipper[ 1]isamodel-basedtestingtool.
It constructs a finite-state-machine (FSM) model of the UI and
performsthedepth-firstsearch(DFS)explorationstrategy.Tobuild
the model, GUIRipper instruments the APK file of the app under
test and dynamically analyzes the app UI to obtain relevant events
related to UI widgets. The tool then systematically traverses the
appUI,generatingandexecutingobtainedrelevanteventswhen
new states are encountered.Reason.WefailtoadaptGUIRippertorealdevices(notethatonly
a binary version of GUIRipper for the Windows OS is available). In
addition,evenonemulators,GUIRipperworksononlyAndroid4.0
and it fails to process most industrial apps under study.
3.2.3 SwiftHand. SwiftHand[ 7]isamodel-basedtestingtool.
It features a specialized active learning algorithm to approximate a
modeloftheappundertesttoguideexplorationintounexplored
parts of the app’s state space. Unlike traditional active learning
algorithmssuchas L∗[5],suchdesignminimizesthenumberof
restartsduringexploration.SwiftHandrequirestoinstrumentthe
APK file of the app under test to obtain the app UI information
during testing.
Reason.Due to possible implementation defects, SwiftHand fails
on most of the industrial apps under study during instrumentation
witherrormessagessuchas ArrayIndexOutOfBoundsException,
orsimplyneverfinishesinstrumentationandproducesGiB-sized
log files.
3.2.4 ACTEve. ACTEve [ 3] is a concolic-testing [ 10]t o o lf o r
Androidapps.ByinstrumentingboththeAndroidSDKandtheapp
undertest,ACTEvesymbolicallytrackseventsfromtheoriginating
points (e.g., tap coordinates on screen) to the code handling theevents. Such approach limits the search space for feasible eventsand avoids generating redundant inputs. The tool also identifies
read-only or ineffective events to further reduce the sizes of event
sequences.
Reason.Similar toDynodroid, ACTEve works ononly Android
2.3anditrequirestoinstrumentboththeAndroidSDKandAndroid
apps.
4 STUDY METHODOLOGY
In this section, we present our study methodology including the
industrial-appselection,coverage/crashmeasurement,andstudy
setup.
4.1 Industrial-App Selection
WechoosetoobtainindustrialappsfromGooglePlay,theofficial
Android app market by Google with huge user base. We sample
multipletop-recommendedappswiththehighestnumbersofdown-loadsfromeachcategory,andmanagetoharvest68industrialapps
that are compatible with Android 4.4, the most recent version of
Androidsupportedbymostofthetop-recommendedappsandall
test generation tools under study. Note that the WeChat app is
specificallyexcludedduetothefactthatWCTester,oneofthetools
741
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Android Test Generation Tools in Industrial Cases ASE ’18, September 3–7, 2018, Montpellier, France
Table 3: Overview of industrial apps under study and their applicability on selected test generation tools
App Name Version Category #Install Login #Method #ActivityApplicability
M. W. Sa. St. D. A.
Abs 4.2.0 Health & Fitness 10m+  47217 31 
AccuWeather 5.3.5-free Weather 50m+  59429 43  
Adobe Acrobat 18.1.0 Productivity 100m+  - 42  
Amazon Kindle 8.5.0.77 Books & Reference 100m+  - 123  
Autolist 5.2.2 Auto & Vehicles 1m+  - 30  
AutoScout24 9.3.14 Auto & Vehicles 10m+  104043 40  
Best Hairstyles 1.17 Beauty 1m+  5703 4  
CNN 5.1 News & Magazines 10m+  52677 31  
Crackle 5.2.1 Entertainment 10m+  52393 16  
Duolingo 3.75.1 Education 100m+  60702 55  
ES File Explorer 4.1.6 Productivity 100m+  96067 105  
Evernote 7.12 Productivity 100m+  89512 160  
Excel 16.0.9126 Productivity 100m+  84138 27 
Facebook 164.0.0 Social 1b+  - 587 
Filters For Selfie 1.0.0 Beauty 1m+  2883 8  
Flipboard 4.1.1 News & Magazines 500m+  27527 74 
Floor Plan Creator 3.2 Art & Design 5m+  8847 13  
Fox News 3.0.0 News & Magazines 10m+  - 31  
G.P. Books 4.0.47 Books & Reference 1b+  - 33  
G.P. Music 8.7.6773 Music & Audio 1b+  23713 65  
G.P. Newsstand 4.7.0 News & Magazines 1b+  70514 32  
Gmail 8.3.12 Communication 1b+  - 60 
GO Launcher Z 2.51 Personalization 100m+  170699 182  
GoodRx 5.3.6 Medical 1m+  50105 61  
Google 7.24.32 Tools 1b+  - 117 
Google Calendar 5.8.24 Business 500m+  - 32 
Google Chrome 65.0.3325 Communication 1b+  - 84 
ibisPaint X 5.1.5 Art & Design 10m+  28106 36  
Instagram 38.0.0 Social 1b+  - 40 
inStar 0.9.8 Art & Design 5m+  52911 23  
LINE Camera 14.2.4 Photography 100m+  83214 64 
Marvel Comics 3.10.3 Comics 5m+  25563 44  
Match 18.03.01 Dating 10m+  52519 66  
McDonald 5.12.0 Food & Drink 10m+  - 62  
Merriam-Webster 4.1.2 Books & Reference 10m+  25554 17  
Messenger 160.0.0 Communication 1b+  - 310 
Mirror 30 Beauty 1m+  7215 12  
My baby Piano 2.22.2614 Parenting 5m+  726 3  
NFL 14.3.46 Sports 50m+  - 46  
Nike Run Club 2.14.1 Health & Fitness 10m+  - 113 
NOOK 4.7.0.39 Books & Reference 10m+  91032 132  
OfficeSuite 9.3.11997 Business 100m+  - 126  
OneNote 16.0.9126 Business 100m+  - 76 
Photos 3.18.0 Photography 1b+  - 114 
Pinterest 6.59.0 Lifestyle 100m+  100420 33  
Quizlet 3.15.2 Education 10m+  71511 58  
realtor.com 8.13.2 House & Home 10m+  44723 34  
Sing! 5.4.1 Music & Audio 100m+  - 53 
Sketch 8.0.A.0.2 Art & Design 50m+  - 46  
Speedometer 3.6 Auto & Vehicles 1m+  17030 11  
Spotify 8.4.48 Music & Audio 100m+  206474 113 
TED 3.1.16 Education 10m+  - 27  
The Weather Chnl. 8.10.0 Weather 50m+  - 99  
Ticketmaster 1.11.0 Events 5m+  - 121 
Translate 5.18.0 Tools 500m+  29666 33  
TripAdvisor 25.6.1 Food & Drink 100m+  106519 213  
trivago 4.9.4 Travel & Local 10m+  34790 29  
UC Browser 11.5.0 Communication 500m+  - 63  
WatchESPN 2.5.1 Sports 10m+  22686 16  
Wattpad 6.82.0 Books & Reference 100m+  89639 93  
Waze 4.36.0.1 Maps & Navigation 100m+  - 203  
WEBTOON 2.0.4 Comics 10m+  81503 62 
Wish 4.16.5 Shopping 100m+  31512 74 
Word 16.0.9126 Productivity 100m+  77895 27 
Yelp 9.33.0 Food & Drink 10m+  204308 277  
YouTube 13.12.60 Video Player & Editor 1b+  - 48 
Zedge 5.38.7 Personalization 100m+  138309 35  
Zillow 9.4.2 House & Home 10m+  - 82  
under study, is specifically optimized for the app and could poten-
tiallycausebiasintheresult.Wealsomanuallyregisteraccounts
forappsthatrequireloggingintoaccesstheirmajorfunctionalities.
Inaddition,appsrequiringspecial/sensitiveinformation(e.g.,bank-
ing) or related to real-world services (e.g., taxi calling) are skipped
to minimize undesirable side effects in the study.
Table3showsthedetailedinformationofeachselectedindustrial
app and its applicability on the selected test generation tools. ‘#In-
stall’ shows the number of installs of the app according to Google
Play. ‘Login’ denotes whether logging in is required by the app for
its majority functionalities to be available. ‘#Method’ indicates the
numberofmethodsineachappasreportedbytheinstrumentation
tool, for which‘-’ indicates that wedo not instrument the appfor
methodcoverage(moredetailsareavailableinSection 4.2).‘#Ac-
tivity’showsthenumberofactivitiesineachappasextractedfrom
AndroidManifest.xml . In the ‘Availability’ header section, ‘M.’,‘W.’, ‘Sa.’, ‘St.’, ‘D.’, and ‘A.’ stand for Monkey, WCTester, Sapienz,
Stoat,DroidBot,andA3E-Depth-First,respectively.Notethatthe
sameabbreviationconventionisusedinsubsequentanalysis.As
showninthetable,mostoftheselectedappshavemorethan100
million installs, while each app has at least 1 million installs. These
apps span across 30 different categories and are popularly used by
Androiduserseveryday.Suchfactorsdistinguishtheseindustrial
apps from open-source apps, which often have only a few users
and very limited functionalities.
4.2 Coverage/Crash Measurement
For code-coverage measurement, we use Ella [ 2] to instrument all
theindustrialapps,andcollectstatisticsofmethodcoveragedur-
ing testing. To avoid potential issues by dual-instrumentation (i.e.,
instrumentation duplicately conducted by both Ella and a test gen-eration tool under study to collect method coverage), we share the
742
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France W. Wang, D. Li, W. Yang, Y. Cao, Z. Zhang, Y. Deng, and T. Xie
Ella-collectedmethodcoverageinformationwithtestgeneration
tools that need the coverage information during testing instead of
lettingthetoolsinstrumenttheappagain.Notethatwefocuson
coverage of only Java code without considering the native code
because Android apps’ main functionalities are typically imple-
mentedinJava5.Inpractice,wefindthatEllafailstoinstrumentsome large industrial apps under study due to the 64K reference
limitation of DalvikVM(see Section 2.2for details), andsome suc-
cessfullyinstrumentedappsfailtorunproperlyonAndroiddevices
due toself-protection mechanisms. Toavoid potential biason app
selection caused by instrumentation, we still keep all these apps in
the study without collecting their method coverage information.
Table3alsoshowswhethereachappisactuallyinstrumentedinthe
experiments as indicated by the ‘#Method’ column. In addition, we
measureactivitycoveragebyperiodicallymonitoringtheactivity
stackonthetestingdeviceandextractingallactivitynamesfrom
AndroidManifest.xml in each app.
For crash measurement, we monitor the Logcat [ 19] on target
devicesduringtestingandrecorderrormessagesrelatedtostack
traces. We filter out stack traces that are not related to the app
under test by checking whether the app’s package name is present.
Only unique stack traces are counted, achieved by hashing all codelocationsineachstacktrace(insteadoftheentirestacktrace,which
might contain environment related information).
4.3 Study Setup
We run each test generation tool continuously for 3 hours on each
of their applicable industrial apps under study. Note that for Stoat,
we follow the settings described in the tool’s corresponding pa-
per[29]byallocating1hourformodelconstructionand2hours
formodelevolution. Each test( i.e.,acombinationof onetestgen-
erationtool andoneapplicableapp) isrun3times tocompensate
potentialinfluencebroughtbyrandomnessduringtesting.Alltests
of the same app are run on the same device. For the fairness of
comparison, when we run each test, the tool is allowed to use only
onedevice.Forappsrequiringloggingintoexposemostoftheir
functionalities,wechoosetomanually log intotheseappsbefore
eachtestbeginsinordertofacilitatein-depthtesting(notethatthecodecoveragebeforethetestbeginsisnotincludedintheanalysis).
In addition, the original implementation of Sapienz clears app data
beforeevaluatingeachinputsequence,revertingtheeffortsofman-
ual logging in. In order to set up a normalized testing environment
while keeping the tool’s original design as much as possible, we
modify the tool so that it backs up the app data right after manual
logging in and later restores the app data instead of clearing them.
WeconductourstudyonofficialAndroidx86emulatorsand4
realphones,allrunningAndroid4.4.Eachemulatorisconfigured
with 4 CPU cores, 2 GiB of RAM, and 1 GiB of SD card. For each
appunderstudy,iftheappsupportsx86devices,itistestedona
standardemulatoreachtime;otherwise,itistestedonacertainrealphone.Apps’dataandmodificationstotheSDcardareallreverted
aftereachtest.Suchdesignservesasanefforttokeepthetesting
environmentefficient,unified,yetversatiletoallowtestingvarious
industrial apps. Note that Android ARM emulators are not useddue to their poor performance, which could potentially limit thepower of test generation tools given a bounded amount of time.Accordingtoourobservationduringtesting,mostx86emulators
seldomuseupalldedicatedCPUcores,indicatingtheirgoodper-
formance. Also note that we modify each tool’s implementation in
onlytwosituations:adaptingthetooltoourtestingenvironment,
or dealing with an easy-to-fix implementation defect that prevents
the tool from functioning properly (with reference to the tool’s
corresponding document or paper).
5 CODE COVERAGE RESULTS ON
INDUSTRIAL APPS
In this section, we answer RQ1 (what is the code coverage achieved
byeachtestgenerationtoolunderstudyonapplicableindustrialapps )
by measuring and comparing the method and activity coverage
achievedbyeachtestingtoolonindustrialappsinourexperiments.
Table4shows the statistics ofmethod and activity coverageon
each app achieved by each test generation tool under study after 3
hours of testing.‘-’ in a table cellindicates that the corresponding
toolisnotapplicableonthecorrespondingindustrialapp(dueto
instrumentationortoolapplicabilityissues).Tablecellswithgray
backgroundsindicatethehighestvaluescomparedwithothertools
for the same app and coverage type, and multiple tools might have
the same highest coverage on the same app (as shown by multiple
tablecellswithgraybackgroundsforthesameappandcoverage
type). All coverage percentage numbers are the averaged values of
3 repetitions and are rounded to the nearest integer. Note that due
to the number rounding, there might be two tools achieving the
samepercentagenumberbutonlyonehavingthegraybackground.
Also note that we use the same convention in subsequent analysis.
AscanbeseenfromTable 4,Monkeymanagestogainthehighest
method coverage on 22 of 41 apps whose method coverage data
canbeobtained,althoughthetooldoesnotachievemuchhigher
methodcoveragecomparedwithothertools(especiallySapienz)on
multiple apps. Sapienz comes after Monkey by gaining the highest
method coverage on 14 apps, while other tools perform the best
with regard to method coverage on fewer than three apps. Such
findingisdifferentfromtheevaluationresultsonopen-sourceappsconductedbytheauthorsofsomeofthesetools.Accordingtotheseauthors’evaluationresults,theyfindthattheirtoolsachievehigher
code coverage on more apps compared with Monkey. It can also be
seenthatnotoolmanagestocovermorethan50%ofmethodsonany
app, with the only exception being Sapienz on the app ‘Floor Plan
Creator’.Inaddition,themajorityofthetoolsachievelessthan30%ofmethodcoverageonmostappsevenafter3hoursoftesting.Such
findings suggest that there is still much space for improving these
toolsonindustrialapps.Anotherinterestingfindingisthatanapp’s
larger code base is not necessarily more difficult to be covered. For
example,the app‘Spotify’hasover 200,000methods,andSapienz
manages to cover 1/3 of these methods. However, the app ‘Google
PlayMusic’(abbreviatedas‘G.P.Music’)hasabout23,000methods,
but none of 6 tools cover more than 5% of these methods. Such
resultalsosuggeststhatdifferentindustrialappsmighthavevery
different characteristics even under the same category.
The statistics of activity coverage are similar to those of method
coverage.Monkeygains thehighestactivitycoverageon35ofall
68 apps (including 3 ties, i.e., there are 3 apps on which Monkey
has the same activity coverage as another tool), while Sapienz
743
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Android Test Generation Tools in Industrial Cases ASE ’18, September 3–7, 2018, Montpellier, France
Table 4: Statistics of code coverage/fault detection on industrial apps by test generation tools under study
App NameMethod Coverage (%) Activity Coverage (%) # of Unique Crashes
M. W. Sa. St. D. A. M. W. Sa. St. D. A. M. W. Sa. St. D. A.
Abs 26 23 25 15 14 - 1316 10 0 6 - 51300-
AccuWeather 24 18 22 13 18 17 30 14 19 9 16 9 13 1 5 1 2 1
Adobe Acrobat ------ 31 2 3 622 5 00 3000
Amazon Kindle ------ 23122 1 000000
AutoScout24 22 17 17 19 10 6 885 13 3 5 210 14 0 2
Autolist ------ 3367 50 3 3 3 00 100 1
Best Hairstyles 40 35 40 39 35 9 100100100100100 25 100000
CNN 32 31 23 22 21 12 48 32 26 26 35 6 413000
Crackle 33 25 32 27 27 27 38 25 38 25 25 19 12 0 11 0 1 0
Duolingo 26 24 26 26 28 25 16 16 15 13 22 9 012 300
ES File Explorer 20 21 22 13 10 11 22 20 19 10 6 3 111 500
Evernote 2330 26 15 23 14 1122 14 3 11 2 00 100 1
Excel 23 16 16 6 15 - 74444 - 01100-
Facebook ------ 4731- - 37 81--
Filters For Selfie 50 4 32 1 45 1 50 25 38 13 63 13 912010
Flipboard 32 32 37 28 29 - 12 14 16 8 11 - 420 40-
Floor Plan Creator 43 36 53 11 32 16 54 38 54 15 31 8 000 200
Fox News ------ 29323 2 31 33 57 8040
G.P. Books ------ 24 15 24 18 15 0 822 10 1 1
G.P. Music 444 543 65333 2 224 711
G.P. Newsstand 544443 60 600 0 11111 2
GO Launcher Z 23 6 18 11 9 10 1 41511 1 000000
Gmail ------ 13 12 17 10 18 - 203 15 0 -
GoodRx 32 31 31 26 29 22 43 41 31 20 30 7 10 17 8 0 1
Google ------ 93419 - 0510 0-
Google Calendar ------ 22 16 13 9 9 - 704 14 0 -
Google Chrome ------ 42 422 - 00 220-
Instagram ------ 25 25 28 10 30 - 35 18 0 0 -
LINE Camera 19 28 36 20 7 - 16 28 3 999 - 00 200-
Marvel Comics 19 16 19 14 9 13 50 41 50 30 9 11 512 900
Match 10 10 14 12 12 8 98 938 2 000000
McDonald ------ 13 3 15 3 15 8 100000
Merriam-Webster 31 20 34 27 10 19 29 24 24 24 6 6 414 500
Messenger ------ 59311 - 00000-
Mirror 22 22 23 21 22 20 33 25 33 17 25 8 43 9530
My baby Piano 12 3 42 31 30 29 333333333333000 100
NFL ------ 1 74 1 347 4 101 201
NOOK 7376 13 1 6276 12 1 000000
Nike Run Club ------ 30 27 3 711 - 30 13 0 0 -
OfficeSuite ------ 28 18 11 6 9 1 100000
OneNote ------ 1720 16 1 13 - 20100-
Photos ------ 2532 25 11 17 - 20 20 13 21 5 -
Pinterest 27 23 26 12 0 6 15 12 2 160 3 32 3100
Quizlet 47 37 46 35 15 32 38 38 40 14 3 9 101 300
Sing! ------ 13 19 23 6 15 - 001 40-
Sketch ------ 3743 26 13 22 2 817 1 5 0 0
Speedometer 2933 32 24 29 24 7373 45 18 45 18 2410 00
Spotify 25 31 33 16 19 - 91 1 1 213 - 00000-
TED ------ 70 30 56 30 22 15 822400
The Weather Chnl. ------ 91 0 11 10 6 1 142 512
Ticketmaster ------ 62 712 - 1210 3-
Translate 32 21 32 14 30 19 58 52 52 12 39 15 000 200
TripAdvisor 31 31 29 14 14 1 2428 24 4 10 0 125 901
UC Browser ------ 32 322 2 000000
WEBTOON 26 23 21 19 24 - 5052 31 16 39 - 10 210-
WatchESPN 32 21 33 29 13 23 44 31 38 31 13 19 20 11 6 0 0
Wattpad 27 37 44 4 30 5 17 32 4 2 11 61 12 77 0 0 0
Waze ------ 2 2283 1 3 1 200 200
Wish 33 27 32 21 13 - 35 22 28 5 7 - 02200-
Word 23 14 16 6 19 - 74404 - 00000-
Yelp 20 11 20 13 14 4 1 47 1 247 0 13 2 26 3 6 2
YouTube ------ 1 068 13 2 - 13 2 8 12 0 -
Zedge 36 30 35 21 3 3 23 14 2 663 0 12 1 5 9 0 2
Zillow ------ 26 12 20 16 9 4 612 701
ibisPaint X 15 18 18 11 16 7 28 28 31 19 31 6 2300 20
inStar 21 14 21 8 13 3 17 9 1 749 4 10 100 1
realtor.com 30 29 30 26 24 19 29 15 24 9 9 6 1210 00
trivago 40 26 38 18 25 12 41 28 41 17 28 3 100 510
gains the highest activity coverage on 28 apps (also including 2
ties). WCTester comes after Sapienz by having the highest activity
coverageon15apps(including3ties).Suchfindingsuggeststhat
WCTester might be better at breadth-first exploration than at in-
depth exercising. It can also be seen that, although the overall
activity coverage is higher than the method coverage on industrial
apps under study, many of the apps still have very low activity
coverage. A possible explanation is that many of the apps’ main
functionalitiesareactuallynotreached.Thus,itmightbehelpful
toprioritizeunexploredfunctionalitiesinordertobettersaturate
the coverage of industrial apps.
To better understand the tools’ coverage performance, we inves-
tigate into each tool’s behavior over time during testing. Figures 1
and2showthetrendofaveragemethodandactivitycoverageby
each test generation tool under study with regard to the elapsed
time during testing. Note that we average the coverage percentagenumbersofdifferentappsinsteadofcountsofmethodsoractivities
to avoid imbalanced influence by apps in different sizes. As shown
in Figure 1, Sapienz almost always has the highest average method
coverage, although its advantage over Monkey becomes smaller
astimegoesby.Whenitcomestotheactivities,asshowninFig-
ure2,Monkeyconstantlyhashigheraverageactivitycoveragethan
Sapienz.Thesetwotoolsbothhavemuchhighercoveragethanthe
remainingfourtools.Thetwotoolsalsogainnewcoveragefaster
than all other four tools on average, leading to more significant
advantages over time. It can also be seen that A3E-Depth-First (ab-
breviatedas‘A3E’)hascomparableorhigheraveragecoveragewith
WCTester, Stoat, and DroidBot at the beginning of testing. How-
ever,A3E-Depth-Firstalmoststopsgainingnewcoverageafterthat.
According to our observation during testing, such result might be
caused by the tool’s outdated implementation, which often causes
the tool to hang completely (see Section 8for more discussion).
744
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France W. Wang, D. Li, W. Yang, Y. Cao, Z. Zhang, Y. Deng, and T. Xie
0.00%2.00%4.00%6.00%8.00%10.00%12.00%14.00%16.00%18.00%
0 30 60 90 120150 180Coverage
Elapsed Time (min)
Monkey WCTester Sapienz
Stoat DroidBot A3E
Figure 1: Trend of average method coverage of industrial
apps achieved by test generation tools under study
0.00%5.00%10.00%15.00%20.00%25.00%30.00%
0 30 60 90 120 150 180Coverage
Elapsed Time (min)
Monkey WCTester Sapienz
Stoat DroidBot A3E
Figure 2: Trend of average activity coverage of industrial
apps achieved by test generation tools under study
6 FAULT DETECTION RESULTS ON
INDUSTRIAL APPS
In this section, we answer RQ2 (how many unique crashes can each
test generation tool trigger on each applicable industrial app, and
what are the causes of these crashes ) by showing the statistics of
unique crashes triggered by each testing tool on industrial apps in
our experiments.
Table4showsthenumberofuniquecrashestriggeredbyeach
test generationtool oneach applicableindustrial app understudy.
Note that each number reports the total number of unique crashes
triggered by the tool on the app after 3 repetitions. As shown in
Table4, Stoat triggers the highest numbers of unique crashes on
23 apps, outperforming all other tools. Sapienz triggers the highest
numbersofuniquecrasheson19apps,whileMonkeyaccomplishes
so on 16 apps. Other three tools trigger the highest numbers of
uniquecrashesonfewerthan10apps.Also,thenumbersofunique
triggeredcrasheshavemuchhigherdeviationsacrossdifferenttools
for the same app, compared with method and activity coverage.
Itissomewhatsurprisingtoseethatthefault-detectionstatistics
differfromthemethodandactivitycoveragestatistics.Aimingto
understand the differences, we manually investigate into a case
involving Stoat and a case involving Sapienz, and examine the
details of crashes with the findings as below.
Stoatontheapp‘Photos’ .Stoathasthehighestnumberofunique
crashes on this app. Stoat triggers many NullPointerException s
duringstartingofactivitiesthattakean Intent(seeSection 2.1for
details) as input. Meanwhile, Monkey and other tools trigger other
typesofexceptionsincluding ArrayIndexOutOfBoundsExceptionandStackOverflowError . Stoat’s triggering these crashes during
activitystartingmightbenefitfrominjectingsystem-levelevents
during testing.
Sapienz on the app ‘Wattpad’. This combination has much more
unique crashes than any other combinations. We find that Sapienz
triggersnumerous SQLiteException sonthisappforeachofthe
three runs. The exception causes are mostly about querying onmultiple non-existent tables in the app’s SQLite database. As theapp seems to heavily rely on the SQLite database but does notproperly handle related exceptions, these fatal SQL queries arefrequently triggered from multiple locations of the app, causingdifferent stack traces. None of other tools is able to trigger suchnumber of exceptions during testing. A possible explanation isthat triggering such crashes requires special preconditions (e.g.,
forciblyterminatingtheappduringinitialization,whichinvolves
SQLoperationsforcreatingthesetables)thatothertoolsmightnot
be able to create.
7 RANK-1 ANALYSIS ON EXPERIMENT
RESULTS
Inthissection,inordertoprovideadditionalinsightsforanswer-
ing RQ3 (how to efficiently combine multiple test generation toolson applicable industrial apps to achieve better coverage and fault
detection), we measure and analyze the statistics of rank-1 method
and activity coverage plus rank-1 unique crashes achieved by each
test generation tool on industrial apps in our experiments. We also
analyze the results from previous sections to answer RQ3.
Arank- nmethod/activityoruniquecrashindicatesthatthereare
ntestgenerationtoolsbeingabletocoverthemethod/activityor
unique crash [ 21]. Specifically, a rank-1 method/activity or unique
crashindicatesthatonlyonetestgenerationtoolunderstudycovers
the method/activity or trigger the unique crash in at least one run
of our experiments. For each tool under study, the numbers of
itscoveredrank-1methods/activitiesandtriggeredrank-1unique
crashes reflect the tool’s unique value to testing an app.
Table5shows the statistics of rank-1 methods, activities, and
unique crashes on applicable industrial apps by the test generation
tools under study. A table cell with ‘ m/n’ indicates that, on the
correspondingapp,the rank-1methods/activitiesoruniquecrashes
covered by the corresponding test generation tool account for m
percent of covered methods/activities or triggered unique crashes
by all the six test generation tools, and allof the tool’s covered
methods/activities or triggered unique crashes are npercent of
coveredmethods/activitiesortriggereduniquecrashesbyallthe
sixtestgenerationtools.Withsuchdefinition,weknowthatona
specificapp,iftestgenerationtoolA’smethod/activityorunique
crash statistic is ‘ a/b’ and tool B’s method/activity or unique crash
statistic is ‘ c/d’, by running both tool A and tool B (i.e., combining
toolAandtoolB)wecouldachieveatleast max( a+d,b+c)percent
coverage of methods/activities or unique crashes that are covered
or triggered by all the six test generation tools.
AsshowninTable 5,formanyindustrialappsunderstudy,com-
bining Monkey and Sapienz facilitates good saturation of covering
the app code as they together contribute to over 90% of all covered
methods by all the six tools on these apps. These two tools alsohave the highest numbers of rank-1 covered methods on many
745
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Android Test Generation Tools in Industrial Cases ASE ’18, September 3–7, 2018, Montpellier, France
Table 5: Statistics of rank-1 methods, activities, and unique crashes on industrial apps by test generation tools under study
App Name% of Rank-1 Covered Methods % of Rank-1 Covered Activities % of Rank-1 Unique Crashes
M. W. Sa. St. D. A. M. W. Sa. St. D. A. M. W. Sa. St. D. A.
Abs 4/7616/88 1/74 0/46 0/63 - 0/5743/100 0/57 0/14 0/29 - 56/56 11/11 33/33 0/0 0/0 -
AccuWeather 5/98 0/75 1/93 1/51 0/74 0/66 33/83 0/39 0/50 17/44 0/44 0/22 47/87 0/7 0/33 7/7 0/13 7/7
Adobe Acrobat --- -- - 6/88 0/12 12/94 0/12 0/12 0/12 0/0 0/0 100/100 0/0 0/0 0/0
Amazon Kindle --- -- - 14/50 43/79 0/7 0/21 0/14 0/7 0/0 0/0 0/0 0/0 0/0 0/0
AutoScout24 9/91 1/66 0/73 3/81 3/46 0/23 9/36 0/36 0/36 55/82 0/9 0/18 11/11 5/5 0/0 74/74 0/0 11/11
Autolist --- -- - 0/73 9/91 0/82 0/5 0/5 0/5 0/0 0/0 50/50 0/0 0/0 50/50
Best Hairstyles 0/95 0/95 0/96 0/95 2/87 0/30 0/100 0/100 0/100 0/100 0/100 0/50 100/100 0/0 0/0 0/0 0/0 0/0
CNN 6/91 1/91 1/62 0/65 0/81 0/34 13/100 0/75 0/69 0/63 0/81 0/13 50/50 13/13 38/38 0/0 0/0 0/0
Crackle 1/98 0/78 1/97 0/86 0/92 0/82 0/86 0/57 0/86 14/71 0/57 0/57 43/57 0/0 43/52 0/0 0/5 0/0
Duolingo 0/90 1/93 1/88 2/90 2/93 0/87 6/65 6/71 0/59 12/65 0/71 0/47 0/0 17/17 33/33 50/50 0/0 0/0
ES File Explorer 4/79 6/82 4/76 4/55 0/54 0/37 5/74 0/69 3/64 13/51 0/33 0/8 13/13 13/13 13/13 63/63 0/0 0/0
Evernote 3/75 7/86 4/78 0/43 1/71 0/35 2/55 14/86 6/61 0/18 0/55 0/6 0/0 0/0 50/50 0/0 0/0 50/50
Excel 17/98 1/77 1/69 0/25 0/67 - 50/100 0/50 0/50 0/50 0/50 - 0/050/50 50/50 0/0 0/0 -
Facebook --- -- - 9/38 50/83 0/36 0/9 - - 11/17 33/39 44/44 6/6 - -
Filters For Selfie 8/96 0/8 0/84 0/2 4/86 0/2 0/80 0/40 0/80 0/20 20/100 0/20 80/90 0/10 10/20 0/0 0/10 0/0
Flipboard 2/72 2/76 11/88 1/65 1/66 - 4/52 0/61 22/70 4/39 0/39 - 40/40 20/20 0/0 40/40 0/0 -
Floor Plan Creator 2/79 0/63 11/95 1/36 1/64 0/40 0/88 0/75 0/88 13/63 0/63 0/25 0/0 0/0 0/0 100/100 0/0 0/0
Fox News --- -- - 8/85 8/92 0/85 0/8 0/23 0/8 7/36 36/50 14/57 0/0 0/29 0/0
G.P. Books --- -- - 0/80 0/60 0/80 20/80 0/60 0/0 26/42 0/11 0/11 53/53 0/5 5/5
G.P. Music 9/86 0/89 1/90 0/87 0/88 0/74 0/100 0/100 0/50 0/50 0/50 0/25 8/17 8/17 25/33 50/58 0/8 0/8
G.P. Newsstand 19/96 0/78 1/78 0/78 0/74 0/74 0/100 0/0 0/100 0/0 0/0 0/0 0/50 0/50 0/50 0/50 0/50 50/100
GO Launcher Z 31/89 0/29 5/63 0/36 4/48 0/35 62/88 0/3 6/32 0/3 6/9 0/3 0/0 0/0 0/0 0/0 0/0 0/0
Gmail --- -- - 0/60 0/47 0/73 20/47 7/80 - 5/11 0/0 11/16 79/79 0/0 -
GoodRx 1/96 0/94 1/93 0/86 0/89 0/66 0/81 3/76 0/51 14/62 0/62 0/11 0/4 0/0 62/65 31/31 0/0 4/4
Google --- -- - 13/87 0/67 0/47 0/7 0/73 - 0/083/83 17/17 0/0 0/0 -
Google Calendar --- -- - 0/78 0/56 0/56 0/33 11/67 - 28/28 0/0 16/16 56/56 0/0 -
Google Chrome --- -- - 0/75 0/50 0/75 25/75 0/50 - 0/0 0/0 50/50 50/50 0/0 -
Instagram --- -- - 0/81 0/81 0/81 13/38 0/75 - 4/13 17/22 65/78 0/0 0/0 -
LINE Camera 1/66 5/81 15/92 0/53 0/29 - 4/61 0/75 21/96 0/21 0/21 - 0/0 0/0 100/100 0/0 0/0 -
Marvel Comics 2/91 0/80 5/94 3/78 0/45 0/61 3/79 0/66 0/76 17/76 0/21 0/17 20/33 0/7 7/13 60/60 0/0 0/0
Match 0/95 0/84 2/97 0/83 0/85 2/54 0/100 0/83 0/100 0/33 0/83 0/17 0/0 0/0 0/0 0/0 0/0 0/0
McDonald --- -- - 14/86 0/14 0/64 0/21 0/64 14/57 100/100 0/0 0/0 0/0 0/0 0/0
Merriam-Webster 1/83 0/83 15/91 0/68 0/26 0/51 0/75 0/75 13/63 13/50 0/13 0/13 29/29 7/7 29/29 36/36 0/0 0/0
Messenger --- -- - 0/37 53/95 5/26 0/11 0/11 - 0/0 0/0 0/0 0/0 0/0 -
Mirror 0/94 1/94 3/98 1/92 0/92 0/83 0/100 0/75 0/100 0/75 0/75 0/25 0/31 0/23 31/69 23/38 0/23 0/0
My baby Piano 10/25 0/6 19/87 2/67 0/63 0/61 0/100 0/100 0/100 0/100 0/100 0/100 0/0 0/0 0/0 100/100 0/0 0/0
NFL --- -- - 45/100 0/36 0/55 0/36 0/45 0/18 20/20 0/0 20/20 40/40 0/0 20/20
NOOK 0/40 0/21 2/41 5/48 46/79 0/5 0/32 0/20 8/40 16/56 32/76 0/4 0/0 0/0 0/0 0/0 0/0 0/0
Nike Run Club --- -- - 4/84 0/71 12/88 0/2 0/6 - 13/20 0/0 80/87 0/0 0/0 -
OfficeSuite --- -- - 22/84 8/67 4/31 0/24 0/24 0/2 100/100 0/0 0/0 0/0 0/0 0/0
OneNote --- -- - 6/83 6/83 6/72 0/6 0/56 - 67/67 0/0 33/33 0/0 0/0 -
Photos --- -- - 2/77 2/89 0/75 7/55 0/55 - 15/30 18/30 17/20 30/32 3/8 -
Pinterest 6/8112/85 2/77 0/38 0/0 0/19 0/55 9/64 9/64 9/36 0/0 0/9 33/33 22/22 33/33 11/11 0/0 0/0
Quizlet 1/88 1/69 7/95 1/67 0/28 0/70 0/64 0/69 25/92 3/28 0/6 0/25 20/20 0/0 20/20 60/60 0/0 0/0
Sing! --- -- - 0/44 11/78 6/83 6/28 0/56 - 0/0 0/0 20/20 80/80 0/0 -
Sketch --- -- - 0/64 18/82 0/50 7/32 4/46 0/4 26/26 55/55 3/3 16/16 0/0 0/0
Speedometer 2/87 0/90 7/87 0/63 0/79 0/72 0/100 0/100 0/75 0/25 0/63 0/38 0/5025/100 0/25 0/0 0/0 0/0
Spotify 6/91 1/87 3/91 0/44 0/66 - 22/78 4/65 4/65 0/4 0/17 - 0/0 0/0 0/0 0/0 0/0 -
TED --- -- - 4/83 0/50 0/67 17/67 0/21 0/21 47/53 13/13 7/13 27/27 0/0 0/0
The Weather Chnl. --- -- - 11/59 7/52 0/48 26/63 0/26 0/4 0/8 15/31 15/15 38/38 0/8 15/15
Ticketmaster --- -- - 0/20 0/20 80/100 0/10 0/20 - 0/33 0/67 0/33 0/0 0/100 -
Translate 2/96 0/89 1/96 0/57 1/90 0/56 0/95 0/90 0/100 0/15 0/75 0/25 0/0 0/0 0/0 100/100 0/0 0/0
TripAdvisor 7/88 4/85 2/81 0/43 0/38 0/0 6/78 8/86 3/74 1/19 0/34 0/0 6/6 6/12 24/29 53/53 0/0 6/6
UC Browser --- -- - 20/60 0/40 40/80 0/20 0/20 0/20 0/0 0/0 0/0 0/0 0/0 0/0
WEBTOON 12/96 0/68 2/76 0/63 0/72 - 3/97 0/92 0/71 0/39 0/63 - 25/25 0/0 50/50 25/25 0/0 -
WatchESPN 1/96 0/63 3/98 1/94 0/37 0/68 11/89 0/56 0/78 11/78 0/22 0/33 11/11 0/0 58/58 32/32 0/0 0/0
Wattpad 4/76 1/83 7/92 0/8 1/65 0/14 5/39 4/65 16/86 0/2 5/35 0/2 1/1 3/3 96/96 0/0 0/0 0/0
Waze --- -- - 45/93 0/9 2/32 0/20 2/52 0/9 50/50 0/0 0/0 50/50 0/0 0/0
Wish 12/90 2/77 4/85 0/56 0/38 - 25/86 3/53 8/72 0/17 0/19 - 0/050/50 50/50 0/0 0/0 -
Word 6/94 3/88 1/68 0/26 1/81 - 0/100 0/100 0/50 0/50 0/50 - 0/0 0/0 0/0 0/0 0/0 -
Yelp 16/94 1/36 2/77 1/61 1/66 0/17 31/87 4/26 1/54 1/24 1/36 0/3 12/32 5/5 39/63 0/7 12/15 5/5
YouTube --- -- - 18/55 0/27 0/36 45/64 0/9 - 29/42 6/6 13/26 39/39 0/0 -
Zedge 6/96 0/81 2/90 1/58 0/8 0/11 8/83 0/50 8/83 8/42 0/8 0/8 26/52 0/4 0/22 39/39 0/0 9/9
Zillow --- -- - 20/77 9/46 0/51 9/43 3/20 0/11 33/40 0/7 7/13 40/47 0/0 7/7
ibisPaint X 4/86 1/81 3/73 1/67 0/79 0/29 6/94 0/81 0/69 0/63 0/81 0/13 20/40 40/60 0/0 0/0 0/40 0/0
inStar 1/96 0/69 3/98 0/55 0/60 0/16 0/80 0/40 20/100 0/40 0/40 0/20 33/33 0/0 33/33 0/0 0/0 33/33
realtor.com 5/94 1/79 2/90 0/76 0/66 0/52 36/100 0/36 0/64 0/29 0/21 0/14 33/33 33/67 0/33 0/0 0/0 0/0
trivago 4/96 0/60 1/93 2/58 0/65 0/29 0/65 5/50 0/65 30/60 0/45 0/5 14/14 0/0 0/0 71/71 14/14 0/0
apps.Whenitcomestoactivities,combiningMonkeywithSapienz
and/orStoatseemstobeagoodoptionformostoftheapps,because
Monkeyhasthehighestnumbersofcoveredactivities(regardless
of ranking) on many apps while Sapienz and/or Stoat can be good
complements when Monkey is not able to cover most activities.
For fault detection, combining Stoat with Sapienz and/or Monkey
seems to be more effective for most of the apps, as Stoat has the
highestnumbersofuniquecrashes(regardlessofranking)onmanyappswhileSapienzand/orMonkeycanbegoodcomplements.Such
suggestion is consistent with the results of manual investigation
from Section 6, where we find that Stoat and Sapienz/Monkey can
triggerverydifferenttypesofcrashes.Also,accordingtothefact
that WCTester is designed for WeChat, the tool might be a good
complementwithMonkeywhentheappundertestinvolvessimilarscenariosasthoseinWeChat(e.g.,chatting,social,andinformationbrowsing). Rank-1 activity statistics also show hints on this sug-
gestion: WCTester covers the highest numbers of unique activities
on ‘Facebook’, ‘Messenger’, ‘Pinterest’, and ‘TripAdvisor’. All these
apps share similar usage scenarios with some functionalities of
WeChat.
8 EXPERIENCE IN APPLYING TEST
GENERATION TOOLS ON INDUSTRIAL
APPS
Inthissection,weanswerRQ4(howmucheffortdoesitrequiretoset
up each test generation tool for testing industrial apps ) by reporting
our experience on setting up each test generation tool under study
and applying them on selected industrial apps. We additionallyreport our experience with Ella [
2] and the Android Framework
(illustrated in Section 2).
746
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France W. Wang, D. Li, W. Yang, Y. Cao, Z. Zhang, Y. Deng, and T. Xie
8.1 Test Generation Tools
Monkey. As the built-in test generation tool shipped with each
Androiddevice,MonkeycanbeinvokeddirectlyusingtheAndroid
Debug Bridge [ 16] shell interface.We spend no efforton setting up
Monkey for industrial apps under study.
WCTester. Due to defects in the UIAutomator Python wrap-
per[20]beingused,WCTesteroftenhaltsduringexplorationand
produceserrormessagessuchas“RPCservernotconnected”.We
spend about 5 hours investigating and fixing the defects, and after
that WCTester becomes much more stable.
Sapienz.TheoriginalimplementationofSapienzsupportsonly
emulators. Given that many apps under study include native li-
braries compiled against only ARM processors, we modify the
tool’s implementation to add support for real devices. Since the
toolistestedononlyAndroid4.4andneedstoinstall MotifCore
tothesystempartition,formaximumcompatibility,wedowngrade
all real devices to Android 4.4 and acquire the root privileges onall of them. We also modify the tool’s implementation so that itrestores the app data to the point right after manual logging ininstead of clearing them. Finally, we spend more than 10 hours
getting Sapienz to work in our settings.
Stoat. Theoriginal implementation ofStoat has multipleissues
withourtestinginfrastructure.Forexample,itforciblykillsallJava
and ADB processes on the underlying computer to clean up the
environment,unexpectedlyterminatingourtoolsformonitoring
thetesting. Stoatalso usestheproblematic UIAutomator Python
wrapper.Overall,wespendabout10hoursinvestigatingandfixing
the implementation of Stoat.
DroidBot.DroidBotneedstorunitsclientappundertheacces-
sibility mode, which requires granting the privilege manually in
Androidsystemsettings. We alsosometimesencountererrormes-
sages such as “Please enable DroidBot manually in accessibility
settings”evenifthetoolworksinpreviousruns.Overall,wespend
about 2 hours writing a script to mitigate this issue.
A3E-Depth-First.A3E-Depth-First has several issues in the im-
plementation, such as not being able to click buttons with labels
containing special characters. Due to the outdated implementation
andtheneedofrunningthetargetappunderitsinstrumentation,
the tool causes many apps to crash at beginning, preventing them
frombeingtested.Italsohangsduringexplorationforunknown
reasons even after we try to fix this issue. We spend about 5 hours
trying to fix the issues for the tool.
Note that we have already submitted bug reports on most of
theprecedingpatchestotheseexistingtoolsfortheoriginaltool
authorstoimprovethequalityandrobustnessofthesetools.Ad-
ditionally, due to the fact that some tool issues appear only after
the experiments have lasted for some time, it takes a lot of manual
effortstoinspecttheexperimentresultstofindoutsuchissues,and
the wasted time of running these experiments (with these issues
still existing in the tools) adds up to tens of hours.
8.2 Ella and the Android framework
Ella.Ellahasmultipleimplementationissuesindifferentmodules.
In addition, the tool’s original implementation does not supportinstrumenting apps with multidex enabled, which is commonlyused by large industrial apps. We spend more than 10 hours fixing
the issues and adding multidex support to Ella.
Androidframework.Weevenencounteranissueinthesystem
framework on Android 4.4. Specifically, the issue in the UIAutoma-
torframeworkcausestheservicetostopworkingwhenthereisanyspecialcharacter(e.g.,anEmojiicon)onthescreen.WefixtheissuebymodifyingthecorrespondingAndroidsourcecodeplusrecompil-ingandreplacingtheUIAutomatorframework(
uiautomator.jar ).
We spend about 5 hours addressing this issue.
9 THREATS OF VALIDITY
Themainthreattoexternalvalidityistherepresentativenessofthe
studiedsubjects(i.e.,thedegreetowhichthestudiedindustrialapps
and tools arerepresentative of true practice). Ourcurrent tool set
containsonlysixtestgenerationtoolsduetonotbeingabletoapply
other test generation tools on most industrial apps under study.
However,thesesixtoolsarestate-of-the-artonesthatarealready
comparedwithmorestate-of-the-art orstate-of-the-practicetools
suchasMonkey,whichispopularlyusedinindustry.Thesethreats
could be reduced by more experiments on wider types of subjects
in future work.
The threats to internal validity are instrumentation effects that
can bias our results. Issues in Ella’s handling of the apps’ binarycode, faults in our modification of the existing tools or in our ex-
periment scripts, etc. might cause such effects. To reduce these
threats, we manually inspect traces of our experiments for sample
apps. In addition, we are not able to obtain method coverage for
abouthalfoftheindustrialappsunderstudyduetoElla’sfailing
to instrument these apps or these apps not running normally after
instrumentation. We also try coverage collection tools based on
Soot [30] and they simply fail or cause problems on more apps. We
arenotawareofothertoolsthatcanflawlesslyinstrumentthese
large, complex, and closed-source apps. Also, it might cause bias to
the selection of apps if we simply discard these apps that fail to be
instrumented.
10 CONCLUSION
In this paper, we have presented an empirical study of existing
Android test generation tools’ applicability on industrial apps. We
directly comparethe tools with regardto code coverageand fault-
detectionability.Byanalyzingthestudyresults,weprovidesugges-tionsforcombiningdifferenttestgenerationtoolstoachievebetterperformance.Wealsoreportourexperienceinapplyingthesetools
toindustrialappsunderstudy.Ourstudyresultsgiveinsightson
howAndroidUItestgenerationtoolscouldbeimprovedtobetter
handle industrial apps.
Ourstudyresultsofferastrongimplicationthattestingresearchers
for Android test generation tools should empirically compare a
newly proposed tool with related previous tools on industrial apps
besidesopen-source apps, going beyondthe current common re-
search practice of comparing tools on onlyopen-source apps.
ACKNOWLEDGMENTS
ThisworkwassupportedinpartbyNationalScienceFoundation
under grants no. CNS-1513939 and CNS-1564274.
747
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. An Empirical Study of Android Test Generation Tools in Industrial Cases ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]Domenico Amalfitano, Anna Rita Fasolino, Porfirio Tramontana, Salvatore
De Carmine, and Atif M. Memon. 2012. Using GUI Ripping for Automated
Testing of Android Applications. In Proceedings of the 27th IEEE/ACM Interna-
tionalConferenceonAutomatedSoftwareEngineering (ASE2012).ACM,NewYork,
NY, USA, 258–261. https://doi.org/10.1145/2351676.2351717
[2]Saswat Anand. 2016. ELLA: A Tool for Binary Instrumentation of Android Apps.
https://github.com/saswatanand/ella
[3]Saswat Anand, Mayur Naik, Mary Jean Harrold, and Hongseok Yang. 2012.
Automated Concolic Testing of Smartphone Apps. In Proceedings of the ACM
SIGSOFT 20th International Symposium on the Foundations of Software Engi-
neering (FSE ’12). ACM, New York, NY, USA, Article 59, 11 pages. https:
//doi.org/10.1145/2393596.2393666
[4]Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan.
2003. An Introduction to MCMC for Machine Learning. Machine Learning 50, 1
(01 Jan 2003), 5–43. https://doi.org/10.1023/A:1020281327116
[5]DanaAngluin.1987. Learningregularsetsfromqueriesandcounterexamples.
Information and Computation 75, 2 (1987), 87 – 106. https://doi.org/10.1016/
0890-5401(87)90052-6
[6]TanzirulAzimandIulianNeamtiu.2013. TargetedandDepth-firstExploration
forSystematicTestingofAndroidApps.In Proceedingsofthe2013ACMSIGPLAN
InternationalConferenceonObjectOrientedProgrammingSystemsLanguages&
Applications (OOPSLA ’13). ACM, New York, NY, USA, 641–660. https://doi.org/
10.1145/2509136.2509549
[7]Wontae Choi, George Necula, and Koushik Sen. 2013. Guided GUI Testing of
AndroidAppswithMinimalRestartandApproximateLearning.In Proceedingsof
the2013ACMSIGPLANInternationalConferenceonObjectOrientedProgramming
Systems Languages & Applications (OOPSLA ’13). ACM, New York, NY, USA,
623–640. https://doi.org/10.1145/2509136.2509552
[8]Shauvik Roy Choudhary, Alessandra Gorla, and Alessandro Orso. 2015. Auto-
matedTestInputGenerationforAndroid:AreWeThereYet?.In Proceedingsof
the 2015 30th IEEE/ACM International Conference on Automated Software Engi-
neering (ASE) (ASE ’15). IEEE Computer Society, Washington, DC, USA, 429–440.
https://doi.org/10.1109/ASE.2015.89
[9]K.Deb,A.Pratap,S.Agarwal,andT.Meyarivan.2002. AFastandElitistMultiob-jectiveGeneticAlgorithm:NSGA-II. Trans.Evol.Comp 6,2(April2002),182–197.
https://doi.org/10.1109/4235.996017
[10]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
ProgrammingLanguageDesignandImplementation (PLDI’05).ACM,NewYork,
NY, USA, 213–223. https://doi.org/10.1145/1065010.1065036
[11]Google.2017. AndroidDalvikExecutableformat. https://source.android.com/
devices/tech/dalvik/dex-format
[12]Google. 2017. ART and Dalvik. https://source.android.com/devices/tech/dalvik/
[13]Google. 2018. Android 64K Method limit. https://developer.android.com/studio/
build/multidex
[14]Google. 2018. Android App Components. https://developer.android.com/guide/
components/fundamentals#Components
[15]Google. 2018. Android Apps on Play Store. https://play.google.com/store/apps
[16]Google. 2018. Android Debug Bridge (adb). https://developer.android.com/
studio/command-line/adb
[17]Google. 2018. Android Monkey. https://developer.android.com/studio/test/
monkey
[18]Google.2018. AndroidPlatformArchitecture. https://developer.android.com/
guide/platform/
[19]Google. 2018. Logcat command-line tool. https://developer.android.com/studio/
command-line/logcat[20]Xiaocong He. 2018. Python wrapper of Android uiautomator test tool. https:
//github.com/xiaocong/uiautomator
[21] K. Inkumsah and T. Xie. 2008. Improving Structural Testing of Object-Oriented
Programs via Integrating Evolutionary Testing and Symbolic Execution. In 2008
23rdIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering.297–
306.https://doi.org/10.1109/ASE.2008.40
[22]BusinessInsider.2018.WeChathashit1billionmonthlyactiveusers. http://www.
businessinsider.com/wechat-has-hit-1-billion-monthly-active-users-2018-3
[23]Wing Lam, Zhengkai Wu, Dengfeng Li, Wenyu Wang, Haibing Zheng, Hui Luo,
PengYan,YuetangDeng,andTaoXie.2017. RecordandReplayforAndroid:Are
WeThereYetinIndustrialCases?.In Proceedingsofthe201711thJointMeetingon
FoundationsofSoftwareEngineering (ESEC/FSE’17).ACM,NewYork,NY,USA,
854–859. https://doi.org/10.1145/3106237.3117769
[24]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2017. DroidBot: A
LightweightUI-guidedTestInputGeneratorforAndroid.In Proceedingsofthe
39thInternationalConferenceonSoftwareEngineeringCompanion(ICSE-C’17).
IEEE Press, Piscataway, NJ, USA, 23–26. https://doi.org/10.1109/ICSE-C.2017.8
[25]Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. 2018. DroidBot: A
lightweight test input generator for Android. https://github.com/honeynet/
droidbot
[26]Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An Input
GenerationSystemforAndroidApps.In Proceedingsofthe20139thJointMeeting
onFoundationsofSoftwareEngineering (ESEC/FSE’13).ACM,NewYork,NY,USA,
224–234. https://doi.org/10.1145/2491411.2491450
[27]KeMao,MarkHarman,andYueJia.2016. Sapienz:Multi-objectiveAutomated
TestingforAndroidApplications.In Proceedingsofthe25thInternationalSym-
posiumonSoftwareTestingandAnalysis (ISSTA’16).ACM,NewYork,NY,USA,
94–105. https://doi.org/10.1145/2931037.2931054
[28]KeMao,MarkHarman,andYueJia.2017. CrowdIntelligenceEnhancesAuto-
mated Mobile Testing. In Proceedings of the 32Nd IEEE/ACM International Confer-
enceonAutomatedSoftwareEngineering (ASE’17).IEEEPress,Piscataway,NJ,
USA, 16–26. http://dl.acm.org/citation.cfm?id=3155562.3155569
[29]TingSu,GuozhuMeng,YutingChen,KeWu,WeimingYang,YaoYao,GeguangPu,
YangLiu,and ZhendongSu.2017. Guided,StochasticModel-basedGUITesting
of Android Apps. In Proceedings of the 2017 11th Joint Meeting on Foundations
of Software Engineering (ESEC/FSE ’17). ACM, New York, NY, USA, 245–256.
https://doi.org/10.1145/3106237.3106298
[30]RajaVallée-Rai,PhongCo, EtienneGagnon,Laurie Hendren,PatrickLam, and
Vijay Sundaresan. 1999. Soot - a Java Bytecode Optimization Framework. (1999),
13–.http://dl.acm.org/citation.cfm?id=781995.782008
[31]Wei Yang, Mukul R. Prasad, and Tao Xie. 2013. A Grey-box Approach for Au-
tomated GUI-model Generation of Mobile Applications. In Proceedings of the
16thInternationalConferenceonFundamentalApproachestoSoftwareEngineering
(FASE’13).Springer-Verlag,Berlin,Heidelberg,250–265. https://doi.org/10.1007/
978-3-642-37057-1_19
[32]XiaZeng,DengfengLi,WujieZheng,FanXia,YuetangDeng,WingLam,Wei
Yang, and Tao Xie. 2016. Automated Test Input Generation for Android: Are
We Really There Yet in an Industrial Case?. In Proceedings of the 2016 24th ACM
SIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering (FSE
’16).ACM,NewYork,NY,USA,987–992. https://doi.org/10.1145/2950290.2983958
[33]Haibing Zheng, Dengfeng Li, Beihai Liang, Xia Zeng, Wujie Zheng, Yuetang
Deng,WingLam,WeiYang,andTaoXie.2017. AutomatedTestInputGeneration
forAndroid:TowardsGettingThereinanIndustrialCase.In Proceedingsofthe
39thInternational Conferenceon SoftwareEngineering: SoftwareEngineeringin
Practice Track (ICSE-SEIP ’17). IEEE Press, Piscataway, NJ, USA, 253–262. https:
//doi.org/10.1109/ICSE-SEIP.2017.32
748
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:26:09 UTC from IEEE Xplore.  Restrictions apply. 