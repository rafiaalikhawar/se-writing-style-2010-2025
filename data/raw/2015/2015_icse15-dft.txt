Combining Symbolic Execution and Model
Checking for Data Flow Testing
Ting SuZhoulai FuyGeguang PuzJifeng HeZhendong Suy
Shanghai Key Laboratory of Trustworthy Computing, East China Normal University, Shanghai, China
yDepartment of Computer Science, University of California, Davis, USA
Email: tsuletgo@gmail.com, zlfu@ucdavis.edu,
ggpu@sei.ecnu.edu.cn (zcorresponding author), jifeng@sei.ecnu.edu.cn, su@cs.ucdavis.edu
Abstract â€”Data ï¬‚ow testing (DFT) focuses on the ï¬‚ow of data
through a program. Despite its higher fault-detection ability over
other structural testing techniques, practical DFT remains a
signiï¬cant challenge. This paper tackles this challenge by intro-
ducing a hybrid DFT framework: (1) The core of our framework
is based on dynamic symbolic execution (DSE), enhanced with
a novel guided path search to improve testing performance;
and (2) we systematically cast the DFT problem as reachability
checking in software model checking to complement our DSE-
based approach, yielding a practical hybrid DFT technique that
combines the two approachesâ€™ respective strengths. Evaluated
on both open source and industrial programs, our DSE-based
approach improves DFT performance by 60 80% in terms of
testing time compared with state-of-the-art search strategies,
while our combined technique further reduces 40% testing time
and improves data-ï¬‚ow coverage by 20% by eliminating infeasible
test objectives. This combined approach also enables the cross-
checking of each component for reliable and robust testing results.
I. I NTRODUCTION
Testing is the most widely adopted software validation
technique. Structural coverage criteria, such as statement,
branch and logical [1]â€“[3], have been widely used to assess
test adequacy. In contrast to these structural criteria, data ï¬‚ow
criteria [4]â€“[7] focus on the ï¬‚ow of data through a program,
i.e.the interactions between variable deï¬nitions and their
corresponding uses. The motivation is to verify the correctness
of deï¬ned variable values by observing that all corresponding
uses of these values produce the desired results.
According to several empirical studies [1], [8], [9], data-
ï¬‚ow criteria are more effective than control ï¬‚ow-based testing
criteria ( e.g.statement or branch). However, several reasons
hinder the adoption of data ï¬‚ow testing in practice. First, few
data ï¬‚ow coverage tools exist . To our knowledge, ATAC [10],
[11] is the only available tool to compute data ï¬‚ow coverage for
C programs developed two decades ago. Second, the complexity
of identifying data ï¬‚ow-based test data [12], [13] overwhelms
software testers: test objectives w.r.t. data-ï¬‚ow criteria are much
more than those of structural criteria; more efforts are required
to derive a test case to cover a variable deï¬nition and its
corresponding use than just covering a statement or branch.
Third, infeasible test objectives (i.e.paths from deï¬nitions to
their uses are infeasible) and variable aliases make data ï¬‚ow
testing more difï¬cult.
The aforementioned challenges underline the importance of
effective automated data ï¬‚ow testing. To this end, this paper
presents a combined approach to automatically generate data
ï¬‚ow-based test data. Our approach synergistically combines twotechniques: dynamic symbolic execution and counterexample-
guided abstraction reï¬nement-based model checking. At the
high level, given a program as input, our approach (1) outputs
test data for feasible test objectives and (2) eliminates infeasible
test objectives â€”without any false positives .
Dynamic Symbolic Execution [14], [15] (DSE) is a widely
accepted and effective approach for automatic test data gener-
ation. It intertwines traditional symbolic execution [16] with
concrete execution, and explores as many program paths as
possible to generate test cases by solving path constraints.
As for Counter example- Guided Abstraction Reï¬nement-based
(CEGAR) model checking [17]â€“[19], given the program source
and a temporal safety speciï¬cation, it either statically proves
that the program satisï¬es the speciï¬cation or produces a
counterexample path that demonstrates the violation. It has been
applied to automatically verify safety properties of OS device
drivers [17], [20], [21] and generate test data w.r.t. statement
or branch coverage [22] from counterexample paths.
Although DSE has been widely adopted to achieve different
coverage criteria ( e.g. branch, logical, boundary value and
mutation testing [23]â€“[27]), little effort exists to adapt DSE
to data ï¬‚ow testing. To mitigate path explosion in symbolic
execution, we design a guided path search strategy to cover
data-ï¬‚ow test objectives as quickly as possible. With the help
of concrete executions in DSE, we can also more easily and
precisely detect deï¬nitions due to variable aliasing. Moreover,
we introduce a simple, powerful encoding of data ï¬‚ow testing
using CEGAR-based model checking to complement our DSE-
based approach: (1) We show how to encode any data-ï¬‚ow
test objective in the program under test and systematically
evaluate the techniqueâ€™s practicality; and (2) we describe a
combined approach that combines the relative strengths of the
DSE and CEGAR-based approaches. An interesting by-product
of this combination is to let the two independent approaches
cross-check each otherâ€™s results for correctness and consistency.
We have implemented our data ï¬‚ow testing framework and
the guided path search strategy on top of a DSE engine named
CAUT, which has been continuously developed and reï¬ned in
previous work [25], [28]â€“[30]. We perform data ï¬‚ow testing
on four open source and two industrial programs in C. By
comparing the performance of our proposed search strategy
against other popular search strategies [31], [32], our DSE-based
approach can improve data ï¬‚ow testing by 60 80% in terms of
testing time. In addition, we have adapted the CEGAR-based
approach to complement the DSE-based approach. Evaluation
results show that it can reduce testing time by 40% than using
the CEGAR-based approach alone and improve coverage by
20% than using the DSE-based approach alone. Thus, indeedour combined approach provides a more practical means for
data ï¬‚ow testing.
In summary, we make the following main contributions:
 We design a DSE-based data ï¬‚ow testing framework
and enhance it with an efï¬cient guided path search
strategy to quickly achieve data-ï¬‚ow coverage criteria.
To our knowledge, our work is the ï¬rst to adapt DSE
for data ï¬‚ow testing.
 We describe a simple, effective reduction of data ï¬‚ow
testing to reachability checking in software model
checking [20], [22] to complement our DSE-based
approach. Again to our knowledge, we are the ï¬rst to
systematically adapt CEGAR-based approach to aid
data ï¬‚ow testing.
 We realize the DSE-based data ï¬‚ow testing approach
and conduct empirical evaluations on both open source
and industrial C programs. Our results show that the
DSE-based approach is both efï¬cient and effective.
 We also demonstrate that the CEGAR-based approach
effectively complements the DSE-based approach by
further reducing data ï¬‚ow testing time and detecting
infeasible test objectives. In addition, these two ap-
proaches can cross-check each other to validate the
correctness and effectiveness of both techniques.
The rest of the paper is organized as follows. Section II
introduces necessary background and gives an overview of
our data ï¬‚ow testing approach. Section III details our DSE-
based approach and our reduction of data ï¬‚ow testing to
reachability checking in model checking. Next, we present
details of our implementation (Section IV) and empirical
evaluation (Section V). Section VI surveys related work, and
Section VII concludes.
II. O VERVIEW
A. Problem Setting
A program path is a sequence of control points (denoted by
line numbers), written in the form l1; l2; : : : ; l n. We distinguish
two kinds of paths. A control ï¬‚ow path is a sequence of control
points along the control ï¬‚ow graph of a program; an execution
path is a sequence of executed control points driven by a
program input.
Following the classic deï¬nition from Herman [4], a def-use
pairdu(ld; lu; x)occurs when there exists at least one control
ï¬‚ow path from the assignment ( i.e.deï¬nition , ordefin short)
of variable xat control point ldto the statement at control
point luwhere the same variable xis used ( i.e.use) on which
no redeï¬nitions of xappear ( i.e.the path is def-clear ).
Deï¬nition 1 (Data Flow Testing): Given a def-use pair
du(ld; lu; x)in program P, the goal of data ï¬‚ow testing is to
ï¬nd an input tthat induces an execution path that covers (i.e.,
passes through) ldand then luwith no intermediate redeï¬nitions
ofxbetween ldandlu. The requirement to cover all def-use
pairs at least once is called all def-use coverage criterion .
In this paper, we use dynamic symbolic execution (DSE) [14],
[15] to generate test inputs to satisfy def-use pairs. The DSE-
based approach starts with an execution path triggered by
an initial test input and then iterates the following: from
an execution path p=l1; : : : ; l i 1; li; : : : ; l n, DSE picks an1double power( int x,int y){
2 int exp;
3 double res;
4 if(y>0)
5 exp = y;
6 else
7 exp = -y;
8 res=1;
9 while (exp!=0){
10 res *= x;
11 exp -= 1;
12 }
13 if(y<=0)
14 if(x==0)
15 abort ;
16 else
17 return 1.0/res;
18 return res;
19}
1: input x,y5: exp = y7: exp = -y8: res = 110: res *= x
17: return 1.0/res18: return res15: abort11: exp -= 14: y > 0
13: y <= 09: exp != 0
14: x == 0TF
FFFTTT
Fig. 1. An example: power .
executed branch ( i.e.a branching node1) of a conditional
statement at li(the choice depends on an underlying search
strategy). It then solves the path constraint collected along
l1; : : : ; l i 1conjuncted with the negation of the executed branch
condition at lito ï¬nd a new test input. This input will be used
as a new test case in the next iteration to generate a new path
p0=l1; : : : ; l i 1;li; : : :, which deviates from the original path
patli(the opposite branch direction of the original executed
branch at li), but shares the same path preï¬x l1; : : : ; l i 1with
p. If the target def-use pair is covered by this new path p0
(cf.Deï¬nition 1), we obtain the test case which satisï¬es this
pair. Otherwise, the process will continue until a termination
condition ( e.g.a time bound is reached or the whole path space
has been explored) is met.
Although the DSE-based technique is an effective way to
generate test inputs to cover speciï¬ed program points, it faces
two challenges when applied in data ï¬‚ow testing:
1) The DSE-based technique by nature faces the noto-
rious path-explosion problem. It is challenging, in
reasonable time, to ï¬nd an execution path from the
whole path space to cover a target pair .
2) The test objectives from data ï¬‚ow testing include
feasible andinfeasible pairs. A pair is feasible if there
exists an execution path which can pass through it.
Otherwise it is infeasible . Without prior knowledge
about whether a target pair is feasible or not, DSE-
based approach may spend a large amount of time, in
vain, to cover an infeasible def-use pair.
B. An Illustrative Example
We give an overview of our approach via a simple program
power (Figure 1), which takes as input two integers xandyand
outputs xy. Its control ï¬‚ow graph is shown in the right column
in Figure 1. For illustration, we will explain how our approach
deals with the aforementioned challenges demonstrated by the
1Abranching node is an execution instance of an original branch in the
code. When a conditional statement is inside a loop, it can correspond to
multiple branching nodes along an execution path.following two pairs w.r.t. the variable res:
du1= (l8; l17; res) (1)
du2= (l8; l18; res) (2)
We combine DSE to quickly cover du1and CEGAR to prove
du2is infeasible. The details of these two approaches are
explained in Section III.
DSE-based Data Flow Testing DSE starts by taking an
arbitrary test input t,e.g.t= (x7!0; y7!42). This test
input triggers an execution path p
p=l4; l5; l8; l9; l10; l11; l9; l10; l11; : : :| {z }
repeated 42times; l9; l13; l18 (3)
which covers the defofdu1atl8. To cover its use, the classical
DSE approach ( e.g.with depth-ï¬rst or random path search [32])
will systematically ï¬‚ip branching nodes on pto explore new
paths until the useis covered. However, the problem of path
explosion â€” hundreds of branching nodes on path p(including
nodes from new paths generated from p) can be ï¬‚ipped to fork
new paths â€” could greatly slow down the exploration. We use
two techniques to tackle this problem.
First, we use the redeï¬nition pruning technique to remove
invalid branching nodes: resis detected as redeï¬ned on pat
l10in dynamic execution, so it is useless to ï¬‚ip the branching
nodes after the redeï¬nition point (the paths passing through
the redeï¬nition point cannot satisfy the pair). To illustrate, we
cross out these invalid branching nodes on pand highlight the
rest in (4). As we can see, a large number of invalid branching
nodes can be pruned.
p=l4; l5; l8;l9; l10; l11;l9; l10; l11; : : :
| {z }
repeated 42times;l9;l13; l18 (4)
Second, we use the Cut Point-Guided Search (CPGS) strategy
to decide which branching node to select ï¬rst. The cut points
w.r.t. a pair is a sequence of control points that must be passed
through when searching for a path to cover the pair. They
serve as intermediate goals during the dynamic search and
narrow down the search space. For example, the cut points
ofdu1(l8,l17,res) arefl4,l8,l9,l13,l14,l17g. Since the
pathpin(4)covers the cut points l4,l8andl9, the uncovered
cut point l13is set as the next search goal. From p, there are
two unï¬‚ipped branching nodes, 4Fand9F(denoted by their
respective line numbers followed with TorFto represent the
true orfalse branch direction). Because 9Fis closer to cut
point l13in control ï¬‚ow graph than 4F, so9Fis ï¬‚ipped. As a
result, a new test input t= (x7!0; y7!0)can be generated
and leads to a new path p0=l4; l6; l7; l8; l9; l13; l14; l15. Now
the path p0has covered the cut points l4; l8; l9; l13andl14
and the uncovered cut point l17becomes the goal. From all
remaining unï¬‚ipped branching nodes, i.e.4F,13Fand14F,
the branching node 14Fis chosen because it is closer than
the others toward the goal. Consequently, a new test input
t= (x7!1; y7!0)is generated which covers all cut points,
anddu1(l8,l17,res) itself. Here, the cut point-guided path
search takes only three iterations to cover this pair.
CEGAR-based Data Flow Testing The def-use pair du2(l8,
l18,res) is infeasible: if there were a test input that could
reach the use, it must satisfy y>0 atl13. Since yhas not
been modiï¬ed in the code, y>0 also holds at l4. As a result,1double power( int x,int y){
2 bool cover_flag = false;
3 int exp;
4 double res;
5 ...
6 res=1;
7 cover_flag = true;
8 while (exp!=0){
9 res *= x;
10 cover_flag = false;
11 exp -= 1;
12 }
13 ...
14 if(cover_flag) check_point();
15 return res;
16}
Fig. 2. The transformed function power and the encoded test requirement in
highlighted statements.
reswill be redeï¬ned at l10since the loop guard at l9istrue.
Clearly, no such a path exists for this pair which can both
avoid redeï¬nitions in the loop and reach the use. In this case,
if the DSE-based approach is used, it may enter into an inï¬nite
loop-unfolding and cannot conclude the infeasiblity of this pair.
To mitigate this problem, we leverage the CEGAR-based
approach [18] to check feasibility. This approach starts with
a coarse program abstraction and iteratively reï¬nes it. If a
property violation is found, it analyzes the feasibility ( i.e., is
the violation genuine or the result of an incomplete abstraction?).
If the violation is feasible, a counterexample path is returned.
Otherwise, the proof of infeasibility is used to reï¬ne the
abstraction and the checking continues. In our context, the basic
idea is to encode the test requirement of a pair into the program
under test and reduce the testing problem into this reachability
checking problem. Figure 2 shows the transformed function
power which is encoded with the test requirement of du2in
highlighted statements. We introduce a variable cover ï¬‚agat
l2. It is initialized to false and set as true immediately after the
defatl7, and set to false immediately after the other deï¬nitions
on variable resatl10. Before the use, we set a checkpoint
to see whether cover ï¬‚agistrue atl14. If the checkpoint is
unreachable, this pair can be proved infeasible. Otherwise, a
counter-example, i.e.a test case that covers this pair through
a def-clear path, can be generated. Here, the CEGAR-based
approach can quickly conclude du2is an infeasible pair.
Combined DSE-CEGAR-based Data Flow Testing From the
above two examples, we can see that the DSE-based approach,
as a dynamic path-based testing approach, can efï¬ciently cover
feasible pairs, while the CEGAR-based approach, as a static
model checking-based approach, can eliminate infeasible ones.
It is beneï¬cial to combine the two approachesâ€™ respective
strengths to tackle the challenges in data ï¬‚ow testing.
FeasibleInfeasibleDSECEGAR
The ï¬gure above shows the relation between the two
approaches: The DSE-based approach is able to cover feasible
pairs more efï¬ciently (but in general, it cannot identify
infeasible pairs because of path explosion) while the CEGAR-Data Flow AnalysisDominator AnalysisProgramCoverage TestDSE EngineSyntax TransformerCEGAR EngineInfeasible Def-Use PairTest Input Covering the Def-Use PairDef-Use PairProgram
Static AnalysisDSE-based ApproachRandom Test Input
Execution Path Cut Points c1,c2...cN
[Time Out]A New Test InputCEGAR-based Approach
Infeasibility ProofCounter-Example[All Paths Explored][Time Out][Yes][No]Uncovered Cut Point ci Fig. 3. The workï¬‚ow of the combined DSE-CEGAR-based approach
based approach is capable of identifying infeasible pairs more
effectively (it can also cover feasible pairs as well).
Figure 3 illustrates the basic workï¬‚ow of the combined DSE-
CEGAR approach in our data ï¬‚ow testing framework. The
static analysis is used to ï¬nd def-use pairs and their cut points
from the program under test. The DSE-based approach is ï¬rst
used to cover as many feasible pairs as possible (within a
time bound on each pair). After the DSE-based testing, for the
remaining uncovered pairs, we use the CEGAR-based approach
to identify infeasible pairs and cover new feasible pairs (which
have not yet been covered in DSE) within a time bound. After
one run of the DSE-CEGAR-based testing, we can increase
the time bound for both approaches, and repeat the above
process. If testing budgets permit, it can cover more feasible
pairs and identify more infeasible pairs. The details of these
two approaches are explained in Section III.
III. A PPROACH
In this section, we explain the combined DSE-CEGAR-based
data ï¬‚ow testing framework in detail. It consists of a static
analysis phase and a dynamic analysis phase.
A. Static Analysis Phase
We use standard iterative data-ï¬‚ow analysis algorithms [33],
[34] to identify def-use pairs from the program under test (see
Section IV for details).
Deï¬nition 2 (Cut Point): Given a def-use pair, its cut points
are a sequence of control points that have to be passed through
in succession by any control ï¬‚ow path covering the pair. Each
control point in this sequence is called a cut point .
HQWU\OBXOOBGOOOOO
For illustration, consider the ï¬gure above: Let du(ld; lu; x)be
the target def-use pair, the sequence of cut points of duis
l1,l3,ld,l6andlu. Here, l2is not a cut point because a
pathl1; l3; ld; l5; l6can be constructed to cover the def-use pair
without passing through l2. The cut points of each def-use pair
are computed via a context-sensitive dominator analysis [35]
on the inter-procedural control ï¬‚ow graph.Algorithm 1: DSE-based Data Flow Testing
Input :du(ld; lu; x): a def-use pair
Input :c1,c2,: : :,cn: cut points of du
Output : input tthat satisï¬es duornilif none is found
1letWbe a worklist of branching nodes (initialized as
empty)
2lettbe an initial test input
3repeat
4 letpbe the execution path triggered by t
5 ifpcovers duthen return t
6 W W[fbranching nodes on pg
// the redeï¬nition pruning heuristic
7 ifvariable x(indu) is redeï¬ned after ldonpthen
8 letXdenote the set of branching nodes after the
redeï¬nition location
9 W WnX
10 lett=guided search (W)
11until t== nil
12return nil
13
14Procedure guided search( reference worklist W)
15letb0denote the branch to be ï¬‚ipped
16ifWis empty then
17 return nil
//jis the index of a cut point, dis the distance variable
18j 0,d 0
19forall the branching node b2Wdo
//lbis the program location of b
20 letppbe the path preï¬x of b,i.e.l1,l2,: : :,lb
//c1,: : :,ci 1are sequentially covered, while cinot yet
21 i index of the uncovered cut point cionpp
//bis the opposite branch of b
22 ifi > j_(i==j^distance( b,ci)< d)then
23 b0 b,j i,d distance( b,ci)
24W Wnfb0g,
//lb0is the opposite branch direction of the original b0atlb0
25if9input tdriving the program through l1,l2,: : :,lb0
then
26 return t
27else
28 return guided search( W)
B. DSE-based Approach for Data Flow Testing
This section explains the DSE-based data ï¬‚ow testing
approach enhanced with our cut point-guided path search
strategy . This search strategy embodies several intuitions to
perform efï¬cient path search to cover a target def-use pair.
Algorithm 1 details this approach.
Algorithm 1 takes as input a target def-use pair du(ld; lu; x)
and the sequence of its cut points. If an execution path pcovers
the target pair, the algorithm returns the input t(at Line 5).
Otherwise, it stores the branching nodes on pinto the worklist
W, which contains all branching nodes from the explored
execution paths. We ï¬rst use the redeï¬nition pruning technique
(explained later) to remove invalid branching nodes (at Lines
7-9). Then we start the path search to generate a new test input
in the procedure guided search . In this procedure, we ï¬rst aimto ï¬nd a branching node bwhose path preï¬x has covered the
deepest cut point of the pair (at Lines 21-23). The path preï¬x
of a branching node bis the path preï¬x of the corresponding
execution path which reaches up to the location of b,i.e.,l1,
l2,: : :,lb. If the path preï¬x of bhas sequentially covered the
cut points c1,c2,: : :,ci 1, butciis uncovered, then ci 1is
the deepest covered cut point. The intuition is that the deeper
the cut point a path can reach, the closer a path toward the
pair is. The cut points of a pair act as a set of search goals to
follow during the dynamic search. Note the defanduseof a
pair also servers as cut points.
If two branching nodes have reached the same deepest cut
point (indicated by i==jat Line 22), the algorithm picks the
branching node whose opposite branch has the shortest distance
toward the uncovered cut point ci(at Lines 22-23). Here we use
distance( b,ci)to represent the distance between the opposite
branch of b(i.e.,b) and the uncovered cut point ci. The intuition
is that a shorter path is easier to reach the goal. Here, the
distance is approximated as the number of instructions between
one statement and another. The shortest distance is the number
of instructions along the shortest control ï¬‚ow path from the start
statement to the target statement in the control ï¬‚ow graph [36],
[37]. If the picked branching node can be exercised ( i.e., the
corresponding path constraint is satisï¬able), a new test input
will be returned (at Lines 25-26). Otherwise, it will continue
to pick another branching node from W(at Line 28).
In addition, Deï¬nition 1 requires that no redeï¬nitions appear
on the subpath between the defand the use. Thus it is useless to
pick the branching nodes that follow the redeï¬nition locations.
We can prune these invalid branching nodes to reduce testing
time (in Aglorithm 1 Lines 7-9). Further, by taking advantage
of concrete program executions in DSE, we can track variable
redeï¬nitions caused by variable aliases more easily than the
static symbolic execution techniques [37], [38]. Variable aliases
appear at a program statement during execution when two or
more names refer to the same variable. We use a lightweight
algorithm to detect variable redeï¬nitions w.r.t. a target def-use
pair. In our framework, we operate upon a simpliï¬ed three-
address form of the original source code2. So we mainly focus
on the following statement forms where variable aliases and
variable redeï¬nitions may appear:
 Alias inducing statements: (1) p:=q(pis an alias to
q), (2) p:=&v(pis an alias to v)
 Variable deï¬nition statements: (3) p:=x(pis deï¬ned
byx), (4) v:=x(vis deï¬ned by x)
Here, pandqare pointer variables, v,xare non-pointer
variables, := is an assignment operation. The variable deï¬nitions
are detected by static analysis beforehand. The algorithm
works as follows to dynamically identify variable redeï¬nitions:
we maintain a set Ato record variable aliases w.r.t. a pair
du(ld; lu; v). Initially, Aonly contains the variable vitself. If
statement (1) or (2) is executed and qorv2A, a new variable
aliaspwill be added into Abecausepbecomes an alias
toqorv. If statement (1) is executed and q =2Abutp2
A, thenpwill be removed from Abecausepbecomes an
alias to another variable instead of v. If statement (3) or (4) is
executed andporv2A, then the variable vis redeï¬ned by
another variable x.
2We use CIL as the C parser to transform the source code into an equivalent
simpliï¬ed form using the â€“dosimplify option, where one statement contains at
most one operator.C. CEGAR-based Approach for Data Flow Testing
The counterexample-guided abstract reï¬nement (CEGAR)-
based approach [18] has been extensively used to check safety
properties of software as well as test case generation [22]
(e.g.statement or branch testing). The CEGAR-based approach
operates in two phases, i.e.,model checking and tests from
counter-examples . It ï¬rst checks whether the program location
lof interest is reachable such that a target predicate p(i.e.
a safety property) is true at l. If so, from the program path
that witnesses patl, a test case can be generated from the
counterexamples to establish the validity of patl. Otherwise,
iflis unreachable, the model checker can conclude that no
test input can reach this point.
In data ï¬‚ow testing, due to the conservativeness of data-ï¬‚ow
analysis, test objectives contain infeasible pairs [13]. In order
to identify infeasible pairs, we introduce a simple but powerful
encoding of data ï¬‚ow testing using the CEGAR-based approach.
We instrument the original program PtoP0and reduce the
problem of test data generation to reachability checking on
P0. A variable cover ï¬‚agis introduced and initialized to false
before the def. This ï¬‚ag is set to true immediately after the def
and set to false immediately after the other deï¬nitions on the
same variable. Before the use, we set the target predicate pas
cover ï¬‚ag==true . As a result, if the uselocation is reachable
when pholds, we obtain a counterexample and conclude that
the pair is feasible. Otherwise, the pair is infeasible (or, since
the general problem is undecidable, it does not terminate, and
the result can only be concluded as unknown ).
IV. I MPLEMENTATION
The data ï¬‚ow testing framework is built on top of a DSE
engine named CAUT3[25], [28]â€“[30], which includes two
analysis phases: a static analysis and a dynamic analysis.
The static analysis phase collects def-use pairs, cut points, and
other static program information from programs by using CIL4
(an infrastructure for C program analysis and transformation).
We perform standard iterative data-ï¬‚ow analysis [33], [34] to
ï¬nd intra-procedural and inter-procedural def-use pairs for C
programs. We ï¬rst build the control ï¬‚ow graph (CFG) for
each function and then construct the inter-procedural control
ï¬‚ow graph (ICFG). For each variable use, we compute which
deï¬nitions on the same variable may reach this use. A def-
use pair is created as a test objective for each use with its
corresponding deï¬nition. We consider local and global variables
in our data-ï¬‚ow analysis, and treat each formal parameter
variable as deï¬ned at the beginning of its function, each actual
parameter variable as used at its function call site ( e.g.library
function calls), global variables as deï¬ned at the beginning of
the entry function ( e.g.main ). Following recent work on data
ï¬‚ow testing [39], we currently do not consider def-use pairs
caused by pointer aliasing. Thus, we may miss some def-use
pairs, but this is an independent issue and does not affect the
effectiveness of our approach. More sophisticated data-ï¬‚ow
analysis techniques [40] could be used to mitigate this problem.
The dynamic analysis phase performs dynamic symbolic
execution on programs. We extend the original DSE engine
to whole program testing, which uses Z35as the constraint
solver. Function stubs are used to simulate C library functions
3CAUT: https://github.com/tingsu/caut-lib
4CIL: http://kerneis.github.io/cil/
5Z3: http://z3.codeplex.com/such as string, memory and ï¬le operations to improve symbolic
reasoning ability. We use CIL to encode the test requirement
of a def-use pair into the program under test, which is used as
the input to model checkers. The DSE engine and the model
checkers works on the same CIL-simpliï¬ed code.
V. E VALUATION AND RESULTS
This section presents our evaluation to demonstrate the
practical effectiveness of our approach for automated data
ï¬‚ow testing. Our results on six benchmark programs show that
(1)Our DSE-based approach can greatly speed up data ï¬‚ow
testing : It reduces testing time by 60 80% than standard search
strategies from state-of-the-art symbolic executors CREST [32]
and KLEE [31]; and (2) Our combined approach is effective : It
applies the DSE-based tool CAUT to cover as many feasible def-
use pairs as possible, and then applies the CEGAR-based model
checkers BLAST [20]/CPAchecker [21] to identify infeasible
pairs. Overall, it reduces testing time by 40% than the CEGAR-
based approach alone and improves data-ï¬‚ow coverage by 20%
than the DSE-based approach alone.
A. Evaluation Setup
All evaluations were run on a laptop with 4 processors
(2.67GHz Intel(R) i7) and 4GB of memory, running 32bit
Ubuntu GNU/Linux 12.04.
Search Strategies To assess the performance of our proposed
guided path search strategy for data ï¬‚ow testing, we choose
the following search strategies to compare against:
 Random Input (RI): It generates random test inputs to
drive program executions, which is a classic method
to generate data-ï¬‚ow based test data [41].
 Random Path Search (RPS): It randomly chooses a
path to exercise, which is commonly adopted in many
symbolic executors [25], [31], [32].
 CFG-Directed Search (in CREST [32]): It prefers to
drive the program execution down the branch with the
minimal distance toward uncovered branches on the
ICFG and also uses a heuristic to backtrack or restart
the search under certain failing circumstances.
 RP-MD2U Search (in KLEE [31]): It uses a round-
robin of a breadth-ï¬rst search strategy with a Min-
Distance-to-Uncovered heuristic. The breadth-ï¬rst strat-
egy favors shorter paths but treats all paths of the
same length equally. The Min-Distance-to-Uncovered
heuristic prefers the paths with minimal distance to
uncovered statements on ICFG.
 Shortest Distance Guided Search (SDGS): It prefers to
choose the path that has the shortest distance toward
a target statement in order to reach the statement as
quickly as possible. This strategy has been applied in
single target testing [36], [37], [42]. In the context of
data ï¬‚ow testing, the search ï¬rst sets the defas the
ï¬rst goal and then the useas the second goal after the
defis covered.
Bechmarks We use six benchmark programs. Four are from
the Software-artifact Infrastructure Repository (SIR6) including
tcas,replace ,printtokens2 and space . They are also used
in other work on data ï¬‚ow testing [9], [43]. We also take
6SIR: http://sir.unl.edu/php/previewï¬les.phpTABLE I. T ESTSUBJECTS FROM SIR AND INDUSTRIAL RESEARCH
PARTNERS
Benchmark #LOC #DU Description
tcas 192 124 collision avoidance system
replace 584 385 pattern matching and substitution
printtokens2 494 304 lexical analyzer
space 5643 3071 array deï¬nition language interpreter
osek os 5732 527 engine management system
space control 8763 1491 satellite gesture control
two industrial programs from research partners. One is an
engine management system [44], [45] running on an automobile
operating system ( osek os) conforming to the OSEK/VDX
standard. The other is a satellite gesture control program [25]
(space control ). These two industrial programs feature compli-
cated execution logic. The detailed descriptions and statistics
are listed in Table I, where LOC denotes the number of lines
of source code and DU the number of collected def-use pairs.
Research Questions In our evaluation, we intend to answer
the following key research questions:
RQ1: In data ï¬‚ow testing w.r.t. all def-use coverage,
what is the performance difference among different
search strategies, i.e., RI, RPS, CREST ( i.e., CFG-
Directed Search), KLEE ( i.e., RP-MD2U Search),
SDGS and CPGS (cut point-guided search) in terms
of search time, program iterations and coverage level?
RQ2: How effective is the combined approach (the
DSE-based approach complemented with the adapted
CEGAR-based approach) for data ï¬‚ow testing?
Metrics and Setup In the evaluation, we use the following
metrics and experimental setup: (1) In RQ1 : The search time,
number of program iterations and coverage are recorded to
measure the performance of different search strategies in the
DSE-based approach. We target one def-use pair at a time. The
coverage percentage is calculated by C=nFeasible /(nTestObj -
nInfeasible )100%, ( nFeasible /nInfeasible is the number of
identiï¬ed feasible/infeasible pairs, nTestObj is the total number
of pairs. In the DSE-based approach, nFeasible is the number
of covered pairs. Since this approach in general cannot identify
infeasible pairs, nInfeasible is set as 0in coverage computation.)
The search time is calculated by T=PnCovered
k=1nSearchTime i
(nSearchTime iis the time spent on the ith covered def-
use pair). The number of program iterations is calculated by
I=PnCovered
k=1nIterations i(nIterations iis the iterations of
theith covered def-use pair). Since the testing budget ( e.g.,
search time and program iterations) spent on an uncovered
def-use pair (maybe an infeasible pair) cannot indicate which
search strategy is better, only the testing budget spent on
covered pairs is tabulated. The maximum search time on
each pair is set to 20 seconds in case of its infeasiblilty. We
repeat the testing process 30 times for each program/strategy
and use their the average values as the ï¬nal results for all
measurements. The total testing time requires about two days.
(2) In RQ2 : Both the latest versions of the two state-of-the-art
CEGAR-based model checkers, BLAST7and CPAchecker8are
respectively used as black-box testing engines. In the evaluation,
we use the following command options and conï¬gurations
7BLAST 2.7.2: http://forge.ispras.ru/projects/blast
8CPAchecker 1.3.4: http://cpachecker.sosy-lab.org/according to the suggestions from the tool maintainers and
usage documentations:
BLAST : ocamltune blast -enable-recursion
-cref -lattice -noprofile -nosserr
-timeout 120 -quiet
CPAchecker :cpachecker -config
config/predicateAnalysis.properties
-skipRecursion -timelimit 120
Here, the maximum testing time on each def-use pair is set
to 120 seconds. We use ocamltune , an internal script to
improve memory utilization for large programs, to invoke
BLAST. For CPAchecker, we use its default analysis con-
ï¬guration. The options -enable-recursion in BLAST
and-skipRecursion in CPAchecker are both used to set
recursion functions as skip. For each def-use pair, we run 30
times and use the average execution time as its ï¬nal value. We
run the single CEGAR-based approach and the combined DSE-
CEGAR-based approach, which operates as follows: the DSE-
based approach (use the cut point-guided path search strategy
with the same time setting in RQ1) is ï¬rst used to cover as many
feasible pairs as possible; for the remaining uncovered pairs,
the CEGAR-based approach is used to identify infeasible paris
and may cover some feasible pairs which have not been found
by the DSE-based approach. In the evaluation, we conduct one
run of the combined DSE-CEGAR-based approach.
The distribution on testing time for one pair is also given:
Median , the semi-interquartile range SIQR (i.e., (Q3-Q1)/2, Q1:
the lower quartile, Q2: the upper quartile) and the number of
outliers which fall 3*SIQR below Q1 or above Q3.
Comparison Criterion A better search strategy or approach in
data ï¬‚ow testing requires less testing time, costs fewer program
iterations and/or achieves higher data-ï¬‚ow coverage.
B. Results and Analysis
RQ1: The cut point-guided search performs the best In
Table II, the performance statistics of different search strategies
are listed. In Column Evaluation , RI, RPS, CREST, KLEE,
SDSG, CPGS respectively represents the corresponding search
strategies introduced before. In Column Performance Result , it
shows the search time ( T), program iterations ( I) and data-ï¬‚ow
coverage percentage ( C). In Column Distribution , it shows the
distribution on testing time for a single pair: Median ( M) and
SIQR. From Table II, we can see that the cut point-guided path
search strategy (CPGS) achieves the overall best performance
against the other search strategies. As expected, in general, the
CPGS strategy requires less testing time and costs much fewer
iterations than other search strategies to achieve higher data-ï¬‚ow
coverage. It narrows the search space by following the cut points
and further improves the performance by pruning redeï¬nition
paths. From Column Distribution , the median search time of
one def-use pair in the CPGS strategy is lower than that of
RP, CREST, KLEE, SDGS respectively. Thus, CPGS is more
efï¬cient in data ï¬‚ow testing.
In Figure 4, we give the column diagrams on each strate-
gy/program in the terms of search time, program iterations
and data-ï¬‚ow coverage, respectively. On average, compared
with CREST, KLEE and SDSG, the CPGS strategy reduced
69.7%, 78.6% and 39.0% in search time and 73.0%, 76.1%,
48.3% in program iterations, respectively. Compared with RPS,
it roughly improves data-ï¬‚ow coverage by 14.7%. Compared
with these novel and widely-used search strategies from manyTABLE II. P ERFORMANCE STATISTICS OF DIFFERENT SEARCH
STRATEGIES IN DATA FLOW TESTING . RI: RANDOM INPUT , RPS: RANDOM
PATH SEARCH , CREST: CFG-DIRECTED SEARCH IN CREST, KLEE:
RP-MD2U SEARCH IN KLEE, SDGS: SHORTEST DISTANCE GUIDED SEARCH ;
CPGS: THE CUT POINT -GUIDED SEARCH , â€1â€MEANS IT EXCEEDS 30
MINUTES , â€-â€ MEANS WE DO NOT COUNT ,TIME UNIT IS IN SECONDS .
Evaluation Performance Result Distribution
Benchmark Strategy T I C M SIQR
tcasRI
RPS
CREST
KLEE
SDGS
CPGS1
1.7
3.4
7.6
2.8
2.6-
778
1459
1477
997
55459.2%
73.4%
73.4%
73.4%
73.4%
73.4%-
0.02
0.02
0.02
0.01
0.01-
0.02
0.02
0.04
0.02
0.01
replaceRI
RPS
CREST
KLEE
SDGS
CPGS1
1,078.3
1,345.3
1,289.3
514.3
328.0-
25947
17824
16202
6929
185422.4%
48.7%
52.8%
51.5%
52.2%
60.9%-
4.26
4.72
4.52
2.13
1.32-
0.86
1.64
1.70
0.66
0.52
printtokens2RI
RPS
CREST
KLEE
SDGS
CPGS1
225.5
376.9
342.7
123.6
55.8-
21950
13040
12857
5791
279465.5%
56.9%
56.9%
56.9%
56.9%
56.9%-
1.18
1.21
1.05
0.65
0.12-
0.56
0.61
0.79
0.52
0.13
spaceRI
RPS
CREST
KLEE
SDGS
CPGS1
1853.4
1954.6
1880.7
1,057.4
656.7-
68715
47115
48248
18773
97829.2%
22.3%
27.7%
28.6%
22.7%
30.2%-
1.85
2.01
1.84
1.27
0.62-
0.91
0.78
0.96
0.65
0.49
osek osRI
RPS
CREST
KLEE
SDGS
CPGS1
476.3
654.1
774.6
204.8
91.6-
21431
11475
17329
7497
478319.4%
47.6%
64.7%
66.5%
61.3%
71.6%-
1.14
1.34
1.29
0.55
0.21-
0.44
0.74
0.84
0.29
0.22
space controlRI
RPS
CREST
KLEE
SDGS
CPGS1
657.8
1323.5
1472.6
490.3
287.3-
43160
29596
35937
13870
887927.3%
49.5%
60.7%
59.5%
56.7%
64.3%-
0.75
1.29
1.33
0.45
0.21-
0.44
0.44
0.76
0.38
0.35
symbolic execution executors, the data clearly shows that the
CPGS strategy is a more effective strategy for data ï¬‚ow testing.
However, there are some interesting phenomenons worth
elaborating. RI gains higher data-ï¬‚ow coverage for the program
printtokens because it is easier for RI to quickly generate
random combinations of characters inputs than other path-
oriented search strategies, but in general it can only cover
limited number of def-use pairs. RPS is faster than all the
other search strategies for the program tcas because it is a
small program with ï¬nite paths and other advanced strategies
incur higher path scheduling overhead. Two novel search
strategies, CREST and KLEE can usually cover more pairs
in most programs than RPS but they require more testing
time. We note that RPS incurs much lower computation
cost on path scheduling than CREST and KLEE. These two
advanced strategies try to satisfy a pair by improving as much
branches/statements coverage as possible, which demands more
testing time. The SDGS strategy is more effective than the other
three general strategies ( i.e., RPS, CREST, KLEE) since it is
guided by the distance metric toward the target pair. However,
it is less effective than the CPGS strategy because the latter0	 Â 300	 Â 600	 Â 900	 Â 1200	 Â 1500	 Â 1800	 Â 
tcas	 Â replace	 Â prin4okens2	 Â space	 Â osek_os	 Â space_control	 Â Time RI	 Â RPS	 Â CREST	 Â KLEE	 Â SDGS	 Â CPGS	 Â (a) the search time (in seconds)
0	 Â 10000	 Â 20000	 Â 30000	 Â 40000	 Â 50000	 Â 60000	 Â 70000	 Â 
tcas	 Â replace	 Â prin4okens2	 Â space	 Â osek_os	 Â space_control	 Â Itera9on RI	 Â RPS	 Â CREST	 Â KLEE	 Â SDGS	 Â CPGS	 Â  (b) the program iterations
0.0%	 Â 20.0%	 Â 40.0%	 Â 60.0%	 Â 80.0%	 Â 100.0%	 Â 
tcas	 Â 	 Â replace	 Â prin4okens2	 Â space	 Â osek_os	 Â space_control	 Â Coverage	 Â RI	 Â RPS	 Â CREST	 Â KLEE	 Â SDGS	 Â CPGS	 Â  (c) the data-ï¬‚ow coverage percentage
Fig. 4. The column diagrams: the search time, the program iterations and the data-ï¬‚ow coverage of each benchmark/search strategy.
TABLE III. P ERFORMANCE STATISTICS OF THE DSE- BASED , CEGAR- BASED ,AND THE COMBINATION APPROACH ON DATA FLOW TESTING . â€DSEâ€,
â€CEGAR1â€ AND â€CEGAR2â€ CORRESPONDING TO THE APPROACH OF CAUT, BLAST AND CPA CHECKER ,RESPECTIVELY . â€-â€ MEANS IT DOES NOT APPLY .
Evaluation Coverage Result Time Result Distribution
Benchmark Approach C CD FT IT TT Mf(S/O) Mi(S/O)
tcasDSE
CEGAR1
CEGAR2
DSE+CEGAR1
DSE+CEGAR273.4%
72.5%
100%
92.9%
100%91/-/33
71/26/27
91/33/0
91/26/7
91/33/02.6s
12m21s
07m57s
2.6s
2.6s-
04m30s
02m38s
04m49s
02m58s23.3s
01h10m
10m25s
18m53s
03m01s1.91(0.90/16)
5.11(0.35/0)3.64(3.54/2)
4.40(0.43/0)
replaceDSE
CEGAR1
CEGAR2
DSE+CEGAR1
DSE+CEGAR260.9%
65.9%
61.0%
78.5%
77.5%234/-/151
242/18/125
217/29/139
288/18/79
276/29/8005m28s
01h23m
01h09m
31m02s
29m58s-
15m06s
13m50s
21m06s
23m30s55m28s
05h45m
05h16m
03h56m
03h58m4.18(14.83/23)
15.73(6.01/12)42.29(10.92/1)
19.22(8.13/1)
printtokens2DSE
CEGAR1
CEGAR2
DSE+CEGAR1
DSE+CEGAR256.9%
38.7%
55.4%
70.9%
77.1%173/-/131
101/43/159
138/55/111
185/43/76
192/55/5755.8s
01h45m
02h55m
19m12s
31m51s-
23m21s
10m30s
37m41s
28m50s44m36s
07h22m
06h18m
03h54m
03h12m72.68(39.89/0)
77.00(15.33/0)28.15(9.16/3)
9.73(2.32/4)
spaceDSE
CEGAR1
CEGAR2
DSE+CEGAR1
DSE+CEGAR230.1%
32.7%
35.7%
37.5%
38.1%925/-/2146
842/498/1731
908/524/1639
964/498/1609
971/524/157610m57s
24h37m
26h50m
01h32m
01h47m-
06h14m
05h33m
08h00m
08h28m11h44m
88h34m
87h02m
72h06m
71h32m105.21(7.57/8)
107.65(6.39/14)43.54(11.42/23)
35.98(7.35/27)
osek osDSE
CEGAR1
CEGAR2
DSE+CEGAR1
DSE+CEGAR271.6%
87.9%
95.2%
93.9%
97.1%377/-/150
372/104/51
399/108/20
397/104/26
407/108/1291.6s
49m19s
50m35s
10m02s
14m52s-
09m22s
08m06s
44m02s
44m06s51m22s
02h40m
01h39m
01h54m
01h27m5.78(1.87/21)
6.05(1.78/6)5.32(1.39/13)
4.47(2.15/8)
space controlDSE
CEGAR1
CEGAR2
DSE+CEGAR1
DSE+CEGAR264.3%
94.8%
94.3%
94.8%
94.3%959/-/532
1157/270/64
1048/380/63
1157/270/64
1048/380/6304m47s
05h23m
07h31m
01h26m
44m59s-
56m40s
01h10m
02h27m
02h06m03h02m
07h57m
10h47m
06h22m
05h18m4.53(1.12/97)
6.90(1.78/149)12.72(1.95/14)
8.51(2.36/6)
0.00%	 Â 20.00%	 Â 40.00%	 Â 60.00%	 Â 80.00%	 Â 100.00%	 Â 
tcas	 Â replace	 Â prin4okens2	 Â space	 Â osek_os	 Â space_control	 Â Total	 Â Time	 Â DSE	 Â CEGAR1	 Â CEGAR2	 Â DSE+CEGAR1	 Â DSE+CEGAR2	 Â 
(a) the total time
0.0%	 Â 20.0%	 Â 40.0%	 Â 60.0%	 Â 80.0%	 Â 100.0%	 Â 
tcas	 Â replace	 Â prin4okens2	 Â space	 Â osek_os	 Â space_control	 Â Coverage	 Â DSE	 Â CEGAR1	 Â CEGAR2	 Â DSE+CEGAR1	 Â DSE+CEGAR2	 Â  (b) the data-ï¬‚ow coverage
0%	 Â 20%	 Â 40%	 Â 60%	 Â 80%	 Â 100%	 Â DSE	 Â CEGAR1	 Â CEGAR2	 Â DSE+CEGAR1	 Â DSE+CEGAR2	 Â DSE	 Â CEGAR1	 Â CEGAR2	 Â DSE+CEGAR1	 Â DSE+CEGAR2	 Â DSE	 Â CEGAR1	 Â CEGAR2	 Â DSE+CEGAR1	 Â DSE+CEGAR2	 Â DSE	 Â CEGAR1	 Â CEGAR2	 Â DSE+CEGAR1	 Â DSE+CEGAR2	 Â DSE	 Â CEGAR1	 Â CEGAR2	 Â DSE+CEGAR1	 Â DSE+CEGAR2	 Â DSE	 Â CEGAR1	 Â CEGAR2	 Â DSE+CEGAR1	 Â DSE+CEGAR2	 Â tcas	 Â replace	 Â prin;okens2	 Â space	 Â osek_os	 Â space_control	 Â Coverage	 Â Details 
unknown	 Â infeasible	 Â feasible	 Â  (c) the coverage details (feasible/infeasible/unknown pairs)
Fig. 5. The column diagrams: the total testing time, the data-ï¬‚ow coverage and the coverage details of the DSE, CEGAR, and DSE+CEGAR approach.
contains more optimization techniques.
RQ2: The combined approach is effective Table III lists
the performance statistics of different approaches in data ï¬‚owtesting. In Column Evaluation ,DSE,CEGAR1 andCEGAR2
represents the single testing approach from CAUT, BLAST and
CPAchecker, respectively. DSE+CEGAR1/CEGAR2 representsthe combined approach of DSE and CEGAR. Column Coverage
Result lists the data-ï¬‚ow coverage ( C) and the coverage details
(CD) (the number of feasible/infeasible/unknown pairs. If
a testing approach cannot give a deï¬nite conclusion on the
feasibility of a pair within the given time bound, we call this
pairunknown ). Column Time Result lists the testing time spent
on feasible pairs ( FT), infeasible pairs ( IT) and total pairs
(TT). Note the total testing time TTis the sum of FT,IT
and the time spent on unknown pairs, so TTshould be longer
thanTT+FT. In Column Distribution ,MfandMirepresent
the median testing time (in seconds) on identifying a feasible
and an infeasible pair, respectively, Sstands for SIQR and
Ois the number of outliers in the CEGAR-based approach.
Note, in general, the DSE-based approach can only identify the
feasible and unknown ( i.e., uncovered) pairs (so we treat the
uncovered pairs as unknown pairs) while the CEGAR-based
approach can identify both feasible and infeasible pairs.
From Table III, we can observe that the DSE-based approach
can cover a large portion of feasible pairs detected by the
CEGAR-based approach (see Column CD). Moreover, by
comparing the testing time spent on feasible pairs between
the DSE-based and the CEGAR-based approach (see Column
FT), we can see that the DSE-based approach is very effective
in covering feasible pairs. A reasonable explanation is that the
DSE-based approach is a dynamic explicit path-based testing
method while the CEGAR-based approach is a static model
checking-based testing method. The static approach requires
much higher testing overhead. On the other hand, it is easier
for the CEGAR-based approach to identify infeasible pairs (see
Column IT) while the DSE-based approach has to check all
possible paths before conï¬rming which pairs are infeasible. So
it is beneï¬cial to combine the DSE-based approach with the
CEGAR-based approach to gain their respective advantages
i.e., reduce testing time on feasible pairs and improve coverage
by eliminating infeasible pairs.
In Figure 5, we present the column diagrams of the total
testing time (normalized in percentage), the data-ï¬‚ow coverage
and the coverage details of the DSE-based, CEGAR-based, and
the combined approach. In detail, the combination strategy can
on average reduce total testing time by 40% than the CEGAR-
based approach alone. It can also eliminate infeasible pairs
more easily and on average improve data-ï¬‚ow coverage by
20% than the DSE-based approach alone. Thus, the combined
approach can provide a more practical way of data ï¬‚ow testing.
In Table III, we also ï¬nd that the testing performances of
the two model checkers and their conclusions on the number
of feasible/infeasible pairs have some differences within the
constrained testing time (see Column CD). A reasonable expla-
nation is that their underlying constraint solvers, implementation
languages, search heuristics have different impact on testing per-
formance. They also have different performances on programs
exhibiting different features. For example, in tcas, all program
inputs are integral, while space control involves much ï¬‚oating-
point computation and many struct/union manipulations. In
addition, the number of infeasible pairs detected by BLAST
is generally fewer than that of CPAchecker (see Column CD).
BLAST is slightly faster in identifying feasible pairs while
CPAchecker can usually identify infeasible pairs more quickly
(see Column MfandMi).
Discussion We have developed a simple, yet powerful method
to reduce data ï¬‚ow testing into model checking and used twoCEGAR-based state-of-the-art model checkers to evaluate its
practicality. From the evaluations on two combination instances
(CAUT+BLAST and CAUT+CPAchecker), we observe a con-
sistent trend: the combined DSE-CEGAR-based approach can
greatly reduce testing time and improve data-ï¬‚ow coverage
than the two approaches alone. In addition, we have also used
the DSE and CEGAR-based approach to cross-check each other
by comparing their results on the same def-use pairs. This helps
to validate the correctness and consistency of both techniques
and also make our testing results more reliable.
C. Threats to Validity
First, we implemented all search strategies on our CIL-based
tool CAUT. The original RP-MD2U strategy in KLEE uses the
number of LLVM instructions to measure the minimal distance
between one instruction to another while CAUT uses CIL
instructions as its distance metric. KLEE is a static symbolic
executor, while CAUT is a dynamic symbolic executor. These
differences may affect the performance of the RP-MD2U
strategy on the benchmarks. In addition, the implementation of
the two search strategies from CREST and KLEE may differ
from their original versions. For these threats, we carefully
inspected the relevant source code and technical reports [46] to
ensure our implementation conformance and correctness. The
decision to engineer the data ï¬‚ow testing framework on our
DSE-based tool is based on the following considerations: (1)
CREST does not support real numbers, composite structures or
pointer reasoning, but these features are required in testing real-
world programs; and (2) KLEE is a static symbolic executor,
thus it is more difï¬cult to reason about possible variable
redeï¬nitions caused by variable aliasing than dynamic symbolic
executors. Implementing all search strategies on top of the same
tool provides the convenience to record and compare the testing
performances of different search strategies.
Second, in our current implementation, we do not identify def-
use pairs caused by pointer aliasing in the risk of missing some
def-use pairs. More sophisticated static or dynamic analysis
techniques [40] could be adopted to identify more def-use pairs.
However, we believe that this is an independent issue and not
the focus of the work. The effectiveness of our DSE-based and
CEGAR-based approach should remain.
As for possible external threats, we have evaluated our
approach on a small set of benchmarks, which include programs
used in previous data ï¬‚ow testing research as well as industrial
programs. From these programs, the effectiveness of our
approach is evident. Although it is interesting to consider
additional test subjects, due to our novel, general methodologies,
we believe that the results should be consistent.
VI. R ELATED WORK
This section discusses three strands of related work: (1) data
ï¬‚ow testing, (2) DSE-based advanced coverage testing, and (3)
directed symbolic execution.
Data Flow Testing Data ï¬‚ow testing has been empirically
demonstrated to be more effective [8], [9] than other structural
testing techniques, but with much higher testing cost [12], [13].
Much research has considered how to aid data ï¬‚ow testing.
Some efforts use random testing combined with program
slicing [41], while a few others use the collateral coverage
relationship [47], [48] between branch/statement and data-ï¬‚ow
criteria to generate data-ï¬‚ow test data. There is also work that
applies search-based techniques [39], [49]â€“[51] to perform dataï¬‚ow testing. For example, Ghiduk et al. [49] use a genetic
algorithm, which takes as input an initial population of random
inputs, and adopts a designated ï¬tness function [52] to measure
the closeness of execution paths against a target def-use pair.
It then uses genetic operations ( e.g.selection, crossover and
mutation) to search the next promising input to cover this pair.
Other efforts include Nayak et al. [50], who use a particle
swarm optimization algorithm, and Ghiduk [51], who uses the
ant colony algorithm to derive test data for the target def-use
pair. Recently, Vivanti et al. [39] also use a genetic algorithm
to conduct data ï¬‚ow testing on classes [7]. They use a ï¬tness
function to guide the search to reach the defand then the use.
In contrast, we adopt an enhanced dynamic symbolic execution
technique to perform data ï¬‚ow testing, and demonstrate how to
combine our DSE-approach with our CEGAR-based approach
to effectively deal with infeasible test objectives, which has
not been investigated in prior work.
Classic symbolic execution [38] is also used to statically
select control ï¬‚ow paths to do data ï¬‚ow testing. Buy et al. [53]
combine data-ï¬‚ow analysis, symbolic execution and automated
deduction to perform data ï¬‚ow testing on classes. Symbolic
execution ï¬rst identiï¬es the relation between the input and
output values of each method in a class, and then collects the
method preconditions from a feasible and def-clear path that
can cover the target pair. An automated backward deduction
technique is later used to ï¬nd a sequence of method invocations
(i.e., a test case) to satisfy these preconditions. However, little
evidence is provided on the practicality of this approach. Hong
et al. [54] use a model checking approach to generate data-ï¬‚ow
oriented test data. It models the program as a Kriple structure
and characterizes data-ï¬‚ow criteria via a set of CTL property
formulas. A counterexample for a property formula represents
a test case for a speciï¬c def-use pair. However, this method
requires manual annotation with unclear scalability since it
is evaluated on only a single function. Baluda et al. [55] use
a combined method of concolic execution [32] and abstract
reï¬nement [56] to compute accurate branch coverage. It
reï¬nes the abstract program from a sequence of failed test
generation operations to detect infeasible branches. In contrast,
we directly encode test objectives into the program under test
and use interpolation-based model checkers (different from
their reï¬nement method) as black-box engines, which is fully
automatic and ï¬‚exible.
Advanced Coverage Testing via DSE Extensive work exists
to apply the DSE-based technique [14], [15], [28], [31] for
test case generation w.r.t. certain advanced coverage criteria.
Bardin et al. [57] propose a label coverage criterion to imply
a number of advanced criteria (MC/DC and weak mutation
criteria). This label coverage criterion is both expressive and
amenable to efï¬cient automation. However, it cannot handle
those criteria that impose constraints on paths ( e.g., data-ï¬‚ow
criteria) rather than program locations. Pandita et al. [24]
propose a trade-off approach to achieve a speciï¬ed coverage
criterion through source code transformations. The block
coverage in the transformed program implies the MC/DC
coverage in the original program. Augmented DSE [26] enforces
advanced criteria such as boundary, logical and mutation criteria
by augmenting path conditions with additional conditions.
However, in this paper, we aim to automate data ï¬‚ow testing,
which has not been considered before in the context of DSE.
We have designed an efï¬cient search strategy to ï¬nd paths thatcover def-use pairs.
Directed Symbolic Execution Much research [36], [37], [42],
[58], [59] has been done to guide path search toward a speciï¬ed
program location in symbolic execution. Do et al. [59] make
use of data dependency analysis [60] to guide the search process
to a program location of interest, while we use a dominator
analysis. Ma et al. [37] propose a call chain backward search
heuristic to ï¬nd a feasible path, backward from the target
program location to the entry. However, it is difï¬cult to adapt
this approach on data ï¬‚ow testing, because it requires that
a function can be decomposed into logical parts when the
target locations ( e.g.thedefand the use) are located in the
same function [61]. But decomposing a function itself is a
nontrivial task. Zamï¬r et al. [36] narrow the path search space
by following a limited set of critical edges and a statically-
necessary combination of intermediate goals. On the other hand,
our approach ï¬nds a set of cut points from the program entry
to the target locations, which makes path exploration more
efï¬cient. Xie et al. [58] integrate ï¬tness-guided path search
strategy with other heuristics to reach a program point. The
proposed strategy is only efï¬cient for those problems amenable
to its ï¬tness functions. Marinescu et al. [42] use a shortest
distance-based guided search method (like the adapted SDGS
heuristic in our evaluation) with other heuristics to quickly
reach the line of interest in patch testing. In comparison, we
combine several search heuristics to guide the path exploration
to traverse two speciï¬ed program locations ( i.e.thedefand
use) sequentially for data ï¬‚ow testing.
VII. CONCLUSION
We have proposed a combined symbolic execution and model
checking approach to automate data ï¬‚ow testing. First, we have
adapted dynamic symbolic execution (DSE) for data ï¬‚ow testing
and introduced a novel path search strategy to make the basic
approach practical. Second, we have devised a simple encoding
of data ï¬‚ow testing via counterexample-guided abstraction
reï¬nement (CEGAR). The two approaches offer complementary
strengths: DSE is more effective at covering feasible def-use
pairs, while CEGAR is more effective at rejecting infeasible
pairs. Indeed, evaluation results have demonstrated that their
combination reduces testing time by 40% than the CEGAR-
based approach alone and improves coverage by 20% than
the DSE-based approach alone. This work not only provides
novel techniques for data ï¬‚ow testing, but also suggests a
new perspective on this problem to beneï¬t from advances in
symbolic execution and model checking. In further work, we
would like to apply this data ï¬‚ow testing technique on larger
programs and make deeper combinations between DSE and
CEGAR, e.g., learning some predicates from DSE to help
CEGAR avoid unnecessary explorations and save testing time.
ACKNOWLEDGMENT
We would like to thank the anonymous reviewers for their
valuable feedback. Ting Su is partly supported by ECNU
Project of Funding Overseas Short-term Studies, Domestic
Academic Visit and International Conference and NSFC Project
No. 91118007. Jifeng He is partially supported by NSFC Project
No. 61321064. Geguang Pu is supported by NSFC Project No.
61361136002 and Shanghai Collaborative Innovation Center of
Trustworthy Software for Internet of Things (ZF1213). Zhoulai
Fu and Zhendong Su are partially supported by United States
NSF Grants 1117603, 1319187, and 1349528.REFERENCES
[1] A. Khannur, Software Testing - Techniques and Applications . Pearson
Publications, 2011.
[2] P. Ammann, A. J. Offutt, and H. Huang, â€œCoverage criteria for logical
expressions,â€ in 14th International Symposium on Software Reliability
Engineering (ISSRE 2003), 17-20 November 2003, Denver, CO, USA ,
2003, pp. 99â€“107.
[3] R. Inc, â€œDo-178b: Software considerations in airborne systems and
equipment certiï¬cation,â€ Requirements and Technical Concepts for
Aviation , December 1992.
[4] P. M. Herman, â€œA data ï¬‚ow analysis approach to program testing.â€
Australian Computer Journal , vol. 8, no. 3, pp. 92â€“96, 1976.
[5] S. Rapps and E. J. Weyuker, â€œSelecting software test data using data ï¬‚ow
information,â€ IEEE Trans. Software Eng. , vol. 11, no. 4, pp. 367â€“375,
1985.
[6] L. A. Clarke, A. Podgurski, D. J. Richardson, and S. J. Zeil, â€œA formal
evaluation of data ï¬‚ow path selection criteria,â€ IEEE Trans. Software
Eng., vol. 15, no. 11, pp. 1318â€“1332, 1989.
[7] M. J. Harrold and G. Rothermel, â€œPerforming data ï¬‚ow testing on classes,â€
inSIGSOFT â€™94, Proceedings of the Second ACM SIGSOFT Symposium
on Foundations of Software Engineering, New Orleans, Louisiana, USA,
December 6-9, 1994 , 1994, pp. 154â€“163.
[8] P. G. Frankl and S. N. Weiss, â€œAn experimental comparison of the
effectiveness of branch testing and data ï¬‚ow testing,â€ IEEE Trans. Softw.
Eng., vol. 19, no. 8, pp. 774â€“787, Aug. 1993.
[9] M. Hutchins, H. Foster, T. Goradia, and T. J. Ostrand, â€œExperiments
of the effectiveness of dataï¬‚ow- and controlï¬‚ow-based test adequacy
criteria,â€ in Proceedings of the 16th International Conference on Software
Engineering, Sorrento, Italy, May 16-21, 1994. , 1994, pp. 191â€“200.
[10] J. R. Horgan and S. London, â€œData ï¬‚ow coverage and the c language,â€
inProceedings of the symposium on Testing, analysis, and veriï¬cation ,
ser. TA V4. New York, NY , USA: ACM, 1991, pp. 87â€“97.
[11] â€”â€”, â€œAtac: A data ï¬‚ow coverage testing tool for c,â€ in Proceedings
of Symposium on Assessment of Quality Software Development Tools ,
1992, pp. 2â€“10.
[12] E. J. Weyuker, â€œThe cost of data ï¬‚ow testing: An empirical study,â€ IEEE
Trans. Software Eng. , vol. 16, no. 2, pp. 121â€“128, 1990.
[13] G. Denaro, M. Pezz `e, and M. Vivanti, â€œQuantifying the complexity
of dataï¬‚ow testing,â€ in Proceedings of the International Workshop on
Automation of Software Test , ser. AST â€™13. IEEE, 2013, pp. 132â€“138.
[14] K. Sen, D. Marinov, and G. Agha, â€œCUTE: A concolic unit testing
engine for C,â€ in Proceedings of the 10th European software engi-
neering conference held jointly with 13th ACM SIGSOFT international
symposium on Foundations of software engineering . New York, NY ,
USA: ACM, 2005, pp. 263â€“272.
[15] P. Godefroid, N. Klarlund, and K. Sen, â€œDART: Directed automated
random testing,â€ in Proceedings of the 2005 ACM SIGPLAN conference
on Programming language design and implementation . New York, NY ,
USA: ACM, 2005, pp. 213â€“223.
[16] J. C. King, â€œSymbolic execution and program testing,â€ Commun. ACM ,
vol. 19, no. 7, pp. 385â€“394, Jul. 1976.
[17] T. Ball and S. K. Rajamani, â€œThe SLAM project: debugging system
software via static analysis,â€ in Conference Record of POPL 2002:
The 29th SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, Portland, OR, USA, January 16-18, 2002 , 2002, pp. 1â€“3.
[18] T. A. Henzinger, R. Jhala, R. Majumdar, and G. Sutre, â€œLazy abstraction,â€
inConference Record of POPL 2002: The 29th SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, Portland, OR,
USA, January 16-18, 2002 , 2002, pp. 58â€“70.
[19] S. Chaki, E. M. Clarke, A. Groce, S. Jha, and H. Veith, â€œModular
veriï¬cation of software components in C,â€ in Proceedings of the 25th
International Conference on Software Engineering, May 3-10, 2003,
Portland, Oregon, USA , 2003, pp. 385â€“395.
[20] D. Beyer, T. A. Henzinger, R. Jhala, and R. Majumdar, â€œThe software
model checker BLAST: Applications to software engineering,â€ Int. J.
Softw. Tools Technol. Transf. , vol. 9, no. 5, pp. 505â€“525, Oct. 2007.
[21] D. Beyer and M. E. Keremoglu, â€œCpachecker: A tool for conï¬gurable
software veriï¬cation,â€ in Computer Aided Veriï¬cation - 23rd Interna-
tional Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011.
Proceedings , 2011, pp. 184â€“190.[22] D. Beyer, A. J. Chlipala, T. A. Henzinger, R. Jhala, and R. Majumdar,
â€œGenerating tests from counterexamples,â€ in Proceedings of the 26th
International Conference on Software Engineering , ser. ICSE â€™04.
Washington, DC, USA: IEEE Computer Society, 2004, pp. 326â€“335.
[23] K. Lakhotia, P. McMinn, and M. Harman, â€œAutomated test data
generation for coverage: Havenâ€™t we solved this problem yet?â€ in
Proceedings of the 2009 Testing: Academic and Industrial Conference
- Practice and Research Techniques . Washington, DC, USA: IEEE
Computer Society, 2009, pp. 95â€“104.
[24] R. Pandita, T. Xie, N. Tillmann, and J. de Halleux, â€œGuided test
generation for coverage criteria,â€ in Proceedings of the 2010 IEEE
International Conference on Software Maintenance . Washington, DC,
USA: IEEE Computer Society, 2010, pp. 1â€“10.
[25] T. Su, G. Pu, B. Fang, J. He, J. Yan, S. Jiang, and J. Zhao, â€œAutomated
coverage-driven test data generation using dynamic symbolic execution,â€
inEighth International Conference on Software Security and Reliability,
SERE 2014, San Francisco, California, USA, June 30 - July 2, 2014 ,
2014, pp. 98â€“107.
[26] K. Jamrozik, G. Fraser, N. Tillmann, and J. de Halleux, â€œAugmented
dynamic symbolic execution,â€ in IEEE/ACM International Conference
on Automated Software Engineering , 2012, pp. 254â€“257.
[27] L. Zhang, T. Xie, L. Zhang, N. Tillmann, J. de Halleux, and H. Mei,
â€œTest generation via dynamic symbolic execution for mutation testing,â€
in26th IEEE International Conference on Software Maintenance (ICSM
2010), September 12-18, 2010, Timisoara, Romania , 2010, pp. 1â€“10.
[28] Z. Wang, X. Yu, T. Sun, G. Pu, Z. Ding, and J. Hu, â€œTest data generation
for derived types in C program,â€ in TASE 2009, Third IEEE International
Symposium on Theoretical Aspects of Software Engineering, 29-31 July
2009, Tianjin, China , 2009, pp. 155â€“162.
[29] T. Sun, Z. Wang, G. Pu, X. Yu, Z. Qiu, and B. Gu, â€œTowards scalable
compositional test generation,â€ in Proceedings of the Ninth International
Conference on Quality Software, QSIC 2009, Jeju, Korea, August 24-25,
2009 , 2009, pp. 353â€“358.
[30] X. Yu, S. Sun, G. Pu, S. Jiang, and Z. Wang, â€œA parallel approach to
concolic testing with low-cost synchronization,â€ Electr. Notes Theor.
Comput. Sci. , vol. 274, pp. 83â€“96, 2011.
[31] C. Cadar, D. Dunbar, and D. R. Engler, â€œKLEE: Unassisted and automatic
generation of high-coverage tests for complex systems programs,â€ in
USENIX Symposium on Operating Systems Design and Implementation ,
2008, pp. 209â€“224.
[32] J. Burnim and K. Sen, â€œHeuristics for scalable dynamic test generation,â€
in23rd IEEE/ACM International Conference on Automated Software
Engineering (ASE 2008), 15-19 September 2008, Lâ€™Aquila, Italy , 2008,
pp. 443â€“446.
[33] M. J. Harrold and M. L. Soffa, â€œEfï¬cient computation of interprocedural
deï¬nition-use chains,â€ ACM Trans. Program. Lang. Syst. , vol. 16, no. 2,
pp. 175â€“204, Mar. 1994.
[34] H. D. Pande, W. A. Landi, and B. G. Ryder, â€œInterprocedural def-use
associations for C systems with single level pointers,â€ IEEE Trans. Softw.
Eng., vol. 20, no. 5, pp. 385â€“403, May 1994.
[35] A. V . Aho, R. Sethi, and J. D. Ullman, Compilers: Principles, techniques,
and tools . Boston, MA, USA: Addison-Wesley Longman Publishing
Co., Inc., 1986.
[36] C. Zamï¬r and G. Candea, â€œExecution synthesis: a technique for
automated software debugging,â€ in European Conference on Computer
Systems, Proceedings of the 5th European conference on Computer
systems, EuroSys 2010, Paris, France, April 13-16, 2010 , 2010, pp.
321â€“334.
[37] K. Ma, Y . P. Khoo, J. S. Foster, and M. Hicks, â€œDirected symbolic
execution,â€ in Static Analysis - 18th International Symposium, SAS 2011,
Venice, Italy, September 14-16, 2011. Proceedings , 2011, pp. 95â€“111.
[38] M. R. Girgis, â€œUsing symbolic execution and data ï¬‚ow criteria to aid
test data selection,â€ Softw. Test., Verif. Reliab. , vol. 3, no. 2, pp. 101â€“112,
1993.
[39] M. Vivanti, A. Mis, A. Gorla, and G. Fraser, â€œSearch-based data-ï¬‚ow
test generation,â€ in IEEE 24th International Symposium on Software
Reliability Engineering, ISSRE 2013, Pasadena, CA, USA, November
4-7, 2013 , 2013, pp. 370â€“379.
[40] G. Denaro, M. Pezz `e, and M. Vivanti, â€œOn the right objectives of data
ï¬‚ow testing,â€ in IEEE Seventh International Conference on SoftwareTesting, Veriï¬cation and Validation, ICST 2014, March 31 2014-April 4,
2014, Cleveland, Ohio, USA , 2014, pp. 71â€“80.
[41] M. Kamkar, P. Fritzson, and N. Shahmehri, â€œInterprocedural dynamic
slicing applied to interprocedural data how testing,â€ in Proceedings
of the Conference on Software Maintenance, ICSM 1993, Montr Â´eal,
Quebec, Canada, September 1993 , 1993, pp. 386â€“395.
[42] P. D. Marinescu and C. Cadar, â€œKATCH: high-coverage testing of soft-
ware patches,â€ in Joint Meeting of the European Software Engineering
Conference and the ACM SIGSOFT Symposium on the Foundations
of Software Engineering, ESEC/FSEâ€™13, Saint Petersburg, Russian
Federation, August 18-26, 2013 , 2013, pp. 235â€“245.
[43] M. Marr Â´e and A. Bertolino, â€œUsing spanning sets for coverage testing,â€
IEEE Trans. Software Eng. , vol. 29, no. 11, pp. 974â€“984, 2003.
[44] J. Shi, J. He, H. Zhu, H. Fang, Y . Huang, and X. Zhang, â€œORIENTAIS:
formal veriï¬ed OSEK/VDX real-time operating system,â€ in 17th IEEE
International Conference on Engineering of Complex Computer Systems,
ICECCS 2012, Paris, France, July 18-20, 2012 , 2012, pp. 293â€“301.
[45] Y . Peng, Y . Huang, T. Su, and J. Guo, â€œModeling and veriï¬cation
of AUTOSAR OS and EMS application,â€ in Seventh International
Symposium on Theoretical Aspects of Software Engineering, TASE 2013,
1-3 July 2013, Birmingham, UK , 2013, pp. 37â€“44.
[46] J. Burnim and K. Sen, â€œHeuristics for scalable dynamic test generation,â€
EECS Department, University of California, Berkeley, Tech. Rep.
UCB/EECS-2008-123, Sep 2008.
[47] N. Malevris and D. Yates, â€œThe collateral coverage of data ï¬‚ow criteria
when branch testing,â€ Information and Software Technology , vol. 48,
no. 8, pp. 676 â€“ 686, 2006.
[48] R. Santelices and M. J. Harrold, â€œEfï¬ciently monitoring data-ï¬‚ow test
coverage,â€ in Proceedings of the twenty-second IEEE/ACM international
conference on Automated software engineering , ser. ASE â€™07. New
York, NY , USA: ACM, 2007, pp. 343â€“352.
[49] A. S. Ghiduk, M. J. Harrold, and M. R. Girgis, â€œUsing genetic algorithms
to aid test-data generation for data-ï¬‚ow coverage,â€ in Proceedings of
the 14th Asia-Paciï¬c Software Engineering Conference , ser. APSEC â€™07.
Washington, DC, USA: IEEE Computer Society, 2007, pp. 41â€“48.
[50] N. Nayak and D. P. Mohapatra, â€œAutomatic test data generation for
data ï¬‚ow testing using particle swarm optimization,â€ in Contemporary
Computing - Third International Conference, IC3 2010, Noida, India,
August 9-11, 2010, Proceedings, Part II , 2010, pp. 1â€“12.[51] A. S. Ghiduk, â€œA new software data-ï¬‚ow testing approach via ant colony
algorithms,â€ Universal Journal of Computer Science and Engineering
Technology , vol. 1, no. 1, pp. 64â€“72, October 2010.
[52] J. Wegener, A. Baresel, and H. Sthamer, â€œEvolutionary test environment
for automatic structural testing,â€ Information & Software Technology ,
vol. 43, no. 14, pp. 841â€“854, 2001.
[53] U. A. Buy, A. Orso, and M. Pezz `e, â€œAutomated testing of classes,â€ in
ISSTA , 2000, pp. 39â€“48.
[54] H. S. Hong, S. D. Cha, I. Lee, O. Sokolsky, and H. Ural, â€œData ï¬‚ow
testing as model checking,â€ in Proceedings of the 25th International
Conference on Software Engineering, May 3-10, 2003, Portland, Oregon,
USA, 2003, pp. 232â€“243.
[55] M. Baluda, P. Braione, G. Denaro, and M. Pezz `e, â€œStructural coverage
of feasible code,â€ in Proceedings of the 5th Workshop on Automation of
Software Test , ser. AST â€™10. New York, NY , USA: ACM, 2010, pp.
59â€“66.
[56] N. E. Beckman, A. V . Nori, S. K. Rajamani, R. J. Simmons, S. Tetali,
and A. V . Thakur, â€œProofs from tests,â€ IEEE Trans. Software Eng. ,
vol. 36, no. 4, pp. 495â€“508, 2010.
[57] S. Bardin, N. Kosmatov, and F. Cheynier, â€œEfï¬cient leveraging of
symbolic execution to advanced coverage criteria,â€ in IEEE Seventh
International Conference on Software Testing, Veriï¬cation and Validation,
ICST 2014, March 31 2014-April 4, 2014, Cleveland, Ohio, USA , 2014,
pp. 173â€“182.
[58] T. Xie, N. Tillmann, J. de Halleux, and W. Schulte, â€œFitness-guided
path exploration in dynamic symbolic execution,â€ in Proceedings of the
2009 IEEE/IFIP International Conference on Dependable Systems and
Networks, DSN 2009, Estoril, Lisbon, Portugal, June 29 - July 2, 2009 ,
2009, pp. 359â€“368.
[59] T. Do, A. C. M. Fong, and R. Pears, â€œPrecise guidance to dynamic
test generation,â€ in ENASE 2012 - Proceedings of the 7th International
Conference on Evaluation of Novel Approaches to Software Engineering,
Wroclaw, Poland, 29-30 June, 2012. , 2012, pp. 5â€“12.
[60] R. Ferguson and B. Korel, â€œThe chaining approach for software test
data generation,â€ ACM Trans. Softw. Eng. Methodol. , vol. 5, no. 1, pp.
63â€“86, 1996.
[61] K. K. Ma, â€œImproving program testing and understanding via symbolic
execution,â€ Ph.D. dissertation, University of Maryland, 2011.