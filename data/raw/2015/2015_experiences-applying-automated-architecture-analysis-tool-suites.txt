Experiences Applying Automated Architecture Analysis Tool
Suites
Ran Mo
Central China Normal University
Wuhan, China
moran@mail.ccnu.edu.cnWill Snipes
ABB Corporate Research
Raleigh, NC, USA
will.snipes@us.abb.comYuanfang Cai
Drexel University
Philadelphia, PA, USA
yc349@drexel.edu
Srini Ramaswamy
ABB Inc.
Cleveland, OH, USA
srini@ieee.orgRick Kazman
SEU/CMU & U. of Hawaii
Honolulu, HI, USA
kazman@hawaii.eduMartin Naedele
ABB Inc.
Baden, Switzerland
martin.naedele@ch.abb.com
ABSTRACT
In this paper, we report our experiences of applying three com-
plementaryautomated software architectureanalysis techniques,
supportedbyatoolsuite,calledDV8,to8industrialprojectswithin
a large company. DV8 includes two state-of-the-art architecture-
level maintainabilitymetrics—DecouplingLeveland Propagation
Cost,anarchitecture flawdetectiontool,andanarchitectureroot
detection tool. We collected development process data from the
project teams as input to these tools, reported the results back
tothepractitioners,andfollowedupwithtelephoneconferences
andinterviews.Ourexperiencesrevealedthatthemetricsscores,
quantitativedebt analysis,andarchitectureflaw visualizationcan
effectively bridge the gap between management and development,
help them decide if, when, and where to refactor. In particular, the
metrics scores, compared against industrial benchmarks, faithfully
reflectedthepractitioners’intuitionsaboutthemaintainabilityof
their projects, and enabled them to better understand the main-
tainability relative to other projects internal to their company, and
tootherindustrialproducts.Theautomaticallydetectedarchitec-
tureflawsandrootsenabledthepractitionerstopreciselypinpoint,
visualize,andquantifythe“hotspots "withinthesystemsthatare
responsibleforhighmaintenancecosts.Exceptforthetwosmallest
projectsforwhichbotharchitecturemetricsindicatedhighmain-
tainability, all other projects are planning or have already begun
refactorings to address the problems detected by our analyses. We
areworkingonfurtherautomatingthetoolchain,andtransformingtheanalysis suiteinto deployableservices accessibleby allprojects
within the company.
CCS CONCEPTS
•Software and its engineering →Software architectures;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3240467KEYWORDS
Software Architecture, Software Quality, Software Maintenance
ACM Reference Format:
RanMo,WillSnipes,YuanfangCai,SriniRamaswamy,RickKazman,andMar-
tin Naedele. 2018. Experiences Applying Automated Architecture Anal-
ysis Tool Suites. In Proceedings of the 2018 33rd ACM/IEEE International
Conference on Automated Software Engineering (ASE ’18), September 3–7, 2018, Montpellier, France. ACM, New York, NY, USA, 11pages.https:
//doi.org/10.1145/3238147.3240467
1 INTRODUCTION
Although software measurement and source code analysis tech-
niques have been researched for decades, making project decisions
that have significant economic impact—especially decisions about
technical debt and refactoring—is still a challenge for management
and development teams. Development teams feel the increasing
challenges of maintenance as the architecture degrades, and of-ten have intuitions about where the problems are, but have diffi-
culty pinpointing which files are problematic and why. It is still
achallengeforthedevelopmentteamstoquantifytheirprojects’
maintenanceproblems—theirdebts—asawayofjustifyingthein-
vestmentinrefactoring.Inthispaper,wepresentourexperience
ofapplyingthreeautomatedarchitectureanalysisandquantifica-
tion techniques, supported by a tool suite, called DV81, on eight
large-scale projects within ABB.
The first technique is measuring architecture maintainability
usingapairofarchitecture-levelmaintainabilitymetrics: decoupling
level(DL)[16]andpropagationcost (PC)[13].DLmeasureshowwell
asoftwaresystemis decoupled intosmallandindependentmodules
thatcanbedevelopedandmaintainedinparallel.PCmeasureshow
tightlythefilesinasoftwaresystemare coupled,whichindicates
theprobabilitythatchangestoonefilepropagatetootherfiles.Both
metrics were proposed recently by different researchers. Applying
both to the same projects could help us evaluate which metric is
moreeffectiveandreliable,andifandhowtheycanrevealdifferent
aspects of the same project.
Thesecondtechniqueis architectureflaw detection.Moetal.[ 15]
formally defined a set of architecture flaws that incur high mainte-
nancecosts.Filesinvolvedinsuchflawssufferfromoneormore
architecturedesignmistakes;theseflawshavesignificantimpacton
the bug-proneness and change-proneness ofthe system. To make
1https://www.archdia.net/products-and-services/
779
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Mo, W. Snipes, Y. Cai, S. Ramaswamy, R. Kazman, and M. Naedele
thepenaltyincurredbytheseflawsexplicit,wequantifythenumber
ofbugsandchanges,aswellasthebugchurnandchangechurn,for
eachflawusingprojecthistorydata.Theuserscanalsovisualize
each flaw as a design structure matrix (DSM) [1, 23].
The third technique we applied is architecture root analysis pro-
posed by Xiao et al. [ 24]. They proposed the notion of a design
rule space (DRSpace)—a set of architecturally connected files to
implementapattern,afeature,orotherimportantsystemconcerns.
Theyalsoproposedtheconceptof architectureroots —designrule
spacesthatclustertogetherthemosterror-pronefilesinthesystem.
AsreportedbyXiaoetal.[ 24],fivearchitecturerootsinaproject
almostalwayscover50%to90%ofthemosterror-pronefileswithin
a project.
InABBwe assembled and integrated these tools to create our
automated architecture analysis framework. Using this framework,
we analyzed eight projects within the company. These projects
aredevelopedatmultiplelocations(India,USA,andSwitzerland)
and differ in their ages, domains, and sizes. Our study had thefollowing steps: first, the development teams of ABBgranted us
access to their code repository, from which we collected file de-
pendencyinformation,historydata,andworkitems.Usingthese
data as input, we ran DV8, which automatically generated metrics
scores, and visualizable architecture flaws and roots along with
supportingquantitativedata.Finally,wecombinedtheoutputfromthesetoolsintoareportforeachproject,andpresentedthesetothe
development teams. After we ensured that the development teams
understood the reports,we conducted a phoneor email interview
with each team to collect their feedback and, most importantly,
toseeifthesetechniqueshelpedthemtodetermineif,when,and
where to refactor.
Our experiences have shown that the two metrics—PC and DL—
canfaithfullyreflecttheextenttowhichaprojectisexperiencing
maintenancedifficulty.ThecomplementarynatureofPCandDL
can provide useful insights as we will show. The architecture flaw
detector can effectively pinpoint which files are suffering from
which specific design problems. This visualization and quantifi-
cation has effectively bridged the gap between management and
developmentteams.Exceptforthetwosmallestprojects,containing
justafewhundredsoffileseach(andthehighestmetricsscores),
allotherprojectsarenowundergoingmajorrefactoringstoaddressthedetectedflaws.Finally,thefeedbackwereceivedregardingroot
analysis isdivided: someteams foundthat it providedan effective
way to detect architectural problems since they only needed to
examinefivegroupsofrelatedfiles.Butotherteamsfoundthata
rootcanbemisleadingwhentherearehighlyinfluentialutilityfiles
that may distort their results.
2 RESEARCH QUESTIONS
Ourgoalinemployingtheseanalysistechniqueswithin ABBwas
to investigate the following research questions:
•RQ1: does DV8 help to close the gap between management
and development? That is, does it help them to decide if,
when, and where to refactor?
•RQ2: doesDV8 help practitionersunderstand the maintain-
ability of their systems relative to other projects internal tothecompany,andrelativetoamorebroad-basedbenchmark
suite?
•RQ3:doesDV8helpdeveloperspinpointthe hotspotsoftheir
systems—thatis,thegroupsoffileswithseveredesignflaws?
We investigated these questions by interviewing the develop-
mentteams,analyzingtheirexperienceswiththeprovidedtools,
andsolicitingfeedback.Thisinterviewprocessalsoallowedusto
understand the limitations of DV8.
3 PROCEDURE
In this section, we present the projects where we applied the archi-
tectureanalysisframework,thedataneededtorunthetoolsuite,
and the overall structure of the analysis framework as shown in
Figure1.
Subjects. Table1presentstheeightprojectsthatweanalyzed.
The “Lanд .” column shows the main language used. The “# Files”
column shows the number of files in the project. For Proj_CHand
Proj_EC, we measured multiple releases of each, so we listed the
rangein the number of files in these projects. The column “# Com ."
presents the number of commits over the time period studied. The
column “ Period" shows the period of time we analyzed for each
project.Thecolumn“# P"presentsthenumberofpeoplewhomade
commits during this time period. Consider Proj_BMas an example:
it contains 371 source files, and the history studied is from April
2015toJuly2017.Itwasmaintainedby8full-timedevelopers,whomade536commits.Therealprojectandfilenamesareanonymized
in this paper.
Table 1: Studied Projects
Lang. #Files #Com. Period #P
Proj_EO C# 144 95305/15-01/17 8
Proj_BM C/C++ 371 53604/15-07/17 8
Proj_CH C/C++ 6,242-6,948 2,568 01/11-01/17 28
Proj_EP C# 1,541 1,668 03/15-04/17 21
Proj_SS C/C++ 15,333 6,400 02/13-10/16 30
Proj_OP C/C++ 7,754 39,074 01/11-01/17 86
Proj_CO C# 491 36003/16-04/17 16
Proj_EC C/C++ 2,493-4,125 1,043 08/13-02/16 N/A
Data needed for the tool suite (DV8). Once a development
teamagreedtoparticipateinourstudy,theygrantedusaccessto
their code repository, and the specific version(s) they wanted toanalyze.Fromtherepository,weextractedthreetypesofdataas
the input to DV8, as shown in Figure 1:
(1)Thedependencyinformationamongsourcefiles.Givenaproject
sourcecode,wefirstused Understand2,acommercialstaticanalysis
tool, togenerate a file-leveldependency report in XMLformat for
each project version to be analyzed.
(2)Therevisionhistoryoftheproject.Weextractedtherevision
history of a project from the Team Foundation Server3(TFS) system
usedinABB.TFSrecordseachcommitasachangesetandprovides
console commands through which we can export the changesetintoaplaintextfile.Thisfilerecords,foraspecifiedtimeperiod,
whichfileswerechangedtogetherinwhichcommit,andhowmany
lines of code were changed in each file.
2https://scitools.com/
3https://www.visualstudio.com/tfs/
780
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. Experiences Applying Automated Architecture Analysis Tool Suites ASE ’18, September 3–7, 2018, Montpellier, France
(3)Bug records. TFS also has a work item tracking system where
thedeveloperscanrecordbugsorothertasks,suchasaddinganew
feature. When a developer makes a commit, he/she can link the
commit (changeset)to a workitem. From the work item tracking
system,wecandownloadallthebugreportsandtheirassociated
changesets in XML format.
The XML files recording dependency information among source
files, the plain text file recording all changesets in the revision
history,andtheXMLfilerecordingbugfixesweretheinputneeded
by the automated tool suite, as shown in Figure 1.
Automated architecture analysis framework . This frame-
work contains the following components:
(1)SDSM generator. The representation used by DV8 is a design
structure matrix (DSM), first proposed by Baldwin and Clark [ 1].
We will elaborate the model using examples in Section V. The
SDSM(shortfor structureDSM )generatortransformsaXMLfile
containing file dependency information into a DSM.
(2)HDSM generator. This component transfers the plain text file
recording changeset information into a DSM format, which we callaHDSM(shortfor historyDSM ),sothatbothstructuralandhistory
information can be processed simultaneously.
(3)DL & PC calculators. These two calculators take a SDSM
as input and output the decoupling level and propagation cost
scoresrespectively,calculatedusingthefiledependenciesoutput
byUnderstand.
(4)Flaw detector. This component takes both a SDSM and a
HDSM as input, and generates a set of new DSM files, each of
which contains an architecture flaw (which we will describe in
Section V), as defined in Mo et al.’s work [ 15]. Each flaw DSM can
alsobeexportedintoaspreadsheetforfurtheranalysisandbroad
dissemination.
(5)Rootdetector. ThiscomponenttakesaSDSM,aHDSM,andthe
changesetfileasinput,andgeneratesasetofDSMs,eachcontaining
an architecture root [ 24] capturing the most change-prone or bug-
prone files in the system. The change-proneness or bug-proneness
of files can be ranked by the number of changes/bugs to any given
file, which can be calculated from the project’s changeset file. A
DSM containing the root can also be exported into a spreadsheet.
(6)Flawcostcalculator. Thiscomponentextendsthe flawdetec-
torcomponentbycalculatingthenumberofLOC,thenumberof
changes,andthenumberofbugfixesincurredbyeachflaw.Ittakes
the output of the flaw detector as input, and uses the changeset file
and bug records to calculate the maintenance costs related to each
flaw. It outputs the results into a spreadsheet, as we will elaborate
later.
(7)Debt calculator. Inspired by the experiences reported in [ 12],
this component calculates the expected number of additional bugs,
changes, and churn incurred by a root or a flaw, as compared to
systemaverages.Theoutputofthiscomponentisalsoaspreadsheet.
(8)Reportgenerator. Thiscomponentautomaticallyputstogether
areportbysummarizingtheoutputoftheothercomponents,in-
cludingthemetricsscores,thesummarizationofflawsandroots,
andthecostsanddebts.Wecanmanuallyaddproject-specificprose,
as needed, to the report before sending it back to the development
team, along with supporting data. For each project, the report con-
tains its basic facts, DL and PC values with the corresponding
percentile rankings, detected flaws and roots with the involvedfiles,andthecostsanddebts.Wethenpresentthereportandthe
associated data to the development team to help them properly
interpret the results.
Ofallthe8componentsinDV8,thefirstfivewereobtainedfrom
theresearcherswhooriginallycreatedthesetechnologies.Thefinal
three components are extensions newly developed for ABB. All
these 8 components have been integrated into our framework to
automate the architecture analysis.
Afterthedevelopmentteamhadtime(atleastoneweek)tofully
digestthereport,wesetupafollow-upinterviewtoevaluatethe
effectiveness of the tools and their results. We asked the architects
andprojectmanagersasetofpre-definedinterviewquestions,as
explained in section 7, so that we could confidently answer the
research questions posed in section 2.
Figure 1: Analysis Framework
4 ARCHITECTURE MEASUREMENT
We first measured the maintainability of each project using the
DL and PC calculators as shown in Figure 1, and compared each
project’s scores with an industrial benchmark suite, so that both
the management and development team can understand how their
projectcomparestoothersinindustry.Forprojectswheremultiple
releases were available to us, we also calculated the variation of
DL and PC over time to see if the trend matched the practitioners’
intuitions. Next we first introduce these two metrics, and then
present the results.
4.1 Two Architecture-level Metrics
DecouplingLevel(DL) wasproposedbyMoetal.[ 16]tomeasure
howwellasoftwaresystemis decoupled intoindependentmodules.
ThetheoreticalfoundationbehindDLisBaldwinandClark’sdesign
ruletheory[ 1]:themoreindependent,small,andactivemodules
there are, the higher option values the system can produce. Based
on this rationale, their algorithm first clusters all source files into a
designrulehierarchy(DRH)[ 5,23],ahierarchicalstructurewith
two features: (1) files in lower layers only depend on files in higher
layers; (2) files within the same layer are grouped into mutually
781
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Mo, W. Snipes, Y. Cai, S. Ramaswamy, R. Kazman, and M. Naedele
independentmodules.BasedonDRH,DLiscalculatedasfollows:
the more independent modules there are, the higher the DL; the
largeramoduleis,theloweritsDL.Foramodulethatinfluences
others, the more dependents it has, the lower its DL.
PropagationCost(PC) wasproposedbyMacCormacketal[ 13]
to measure how tightly coupleda system is. The calculation of a
system’sPC valueisbased onadesign rulematrix[ 1],whose rows
and columns are labeled with the files in the same order. Based on
this matrix,their algorithmfirst representsthe directdependency
relations among files in a system, and then calculates its transitive
closure to add indirect dependencies to the matrix until no more
dependenciescanbeadded.Anonemptycellinthematrixindicates
anindirectordirectdependencybetweenthefileontherowandthe
file on the column. Given the final matrix containing all direct and
indirectdependencies,PCiscalculatedasthenumberofnonempty
cells divided by the total number of cells. PC has been used to
analyze large projects with similar domains and sizes [13].
Mo et al. calculated the PC and DL of 129 projects [ 16], and
published the data as an industrial benchmark. Figure 2depicts
the cumulative distributions of benchmark DL and PC values. The
red solid line represents the probability that a random DL valueis less than or equal to a specific value. For example, the mark
“80th,74.9%” on the red solid line indicates that 80% of the projects
in the benchmarkdata have DL scores less thanor equal to 74.9%.
ThebluddashedlinerepresentstheprobabilitythatarandomPC
value is larger than or equal to a specific value. For example, the
mark“80th,7.7%”atthebluedashedlineindicatesthat80%ofthe
projectsinthebenchmarkdatahavePCscoreslargerthanorequal
to7.7%.ThefigureshowsthatDLandPCcomplementwitheach
other: ahighDL and a lowPC indicate bettermodularity and
maintainability, and a lowDL is usually associated with highPC,
indicating lowerlevel of modularity and maintainability.
Figure 2: Cumulative probability distribution of DL and PC
4.2 Measuring and Ranking of Maintainability
Foreachproject,wefirstcalculateditsDLandPCscores,thencalcu-
lated itspercentile ranking as comparedwith the benchmarkdata.
Forexample,theDLandPCofProj_EOare78%and6%respectively,
ranked the 85th among the 129 projects for both metrics, mean-
ing that its modular structure is better than 85% of the benchmarkprojects. Table 2summarizes the DL and PC scores for the latest
version of each project, showing their metric values, percentile
rankings, and the number of files.
Therowfor Proj_EPshowsthatitsDLis72%,betterthan74%of
all benchmark projects. The PC of this project is 7%, lower (better)
than 83% of the benchmark projects. As we can see from the table,
in general, a higher DL is associated with a lower PC and their
percentile rankings are largely consistent, differing by less than 10percentile points. There are exceptions, however, such as Proj_CH:
itsDLis76%(ranking81stofallprojects),butitsPCis16%,only
ranking the 54th.
Considering that this project has 6,948 files, changes to one
file may affect—directly or indirectly—approximately 1,000 files,
suggestingthatthispartofthesystemwillsufferfromconsiderable
maintenance difficulty, even though the majority of the systemis reasonably decoupled. This observation was confirmed by thedevelopment team. As we discuss later, using architecture flaw
androotanalyses, we were able to pinpoint the file groups, and
their architecture flaws, that are responsible for this maintenance
difficulty.
For two of the eight projects the product organization requested
that we calculate the DL and PC values of multiple snapshots to
assesswhetherthearchitectureisdegradingornot,andiftheassess-
mentisconsistentwiththepractitioners’intuitions.For Proj_CH,
theDLofitslatestreleasewas76%.Sincethen,thesystemhasbeen
changed considerably. The development team asked us to measure
a more recent working version and obtained a DL of 64%, showing
thatthearchitecturehasdegradedsincethelastrelease.Itsarchitectconfirmedthatthedegradationwasexpectedasthereleasewasstill
inalphatestingafterimplementingamajortechnologyimprove-
ment.Theprojectteamplanstocontinueworkingonrefactoring
the code to improve the architecture.
We also calculated the scores for six releases of Proj_EC. All
these releases have very low DL scores (26% - 30%) and high PC
scores(60%-68%).Thepractitionersconfirmedthatthisprojecthas
always been difficult to maintain: a seemingly simple change often
causedalargenumberofunexpectedchangesinvolvingmultiple
files. Forthe latestversionof Proj_EC,its DLis only28%, ranking
in the 5th percentile, and is the worst of all eight projects. Onepractitioner commented: “... revising even two lines of code would
require double digit man-months", indicating that the project has
been extremely difficult to understand and maintain.
Wereportedourresultstothearchitectofeachprojectandasked
whetherDLandPCanalysesfaithfullyreflectthemaintainability
oftheirprojects.AllthearchitectsconfirmedthattheDLandPC
scoresindeedreflectedtheirknowledgeofthesoftwarearchitecture,
andfurtherhelpedthemtopresentthearchitecturequalityissues
to managementin a quantitative way.All four projects whoseDL
valuesrankedlowerthanthe50thpercentilereportedthattheywere
experiencingseveremaintenancedifficulty.Oftheeightprojects,
other than Proj_EO and Proj_BM that are small and have high PC
and DL values, the other six project teams have decided, or have
alreadybegunrefactoringtoaddresstheproblemspresentedinour
reports. The architects also expressed their willingness to leverage
DLandPCmetricstocontinuouslymeasure(quarterlyorevenat
every release) the architecture of their projects. By tracking the
782
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. Experiences Applying Automated Architecture Analysis Tool Suites ASE ’18, September 3–7, 2018, Montpellier, France
variations of DL and PC values regularly, they believe that they
could monitor whether their architecture is improving or decaying.
Table 2: Data of each project’s DL and PC
DLPercentile PCPercentile #Files
Proj_EO 78% 85th 6% 85th 144
Proj_BM 77% 85th 2% 98th 371
Proj_CH 76% 81st 16% 54th 6,948
Proj_EP 72% 74th 7% 83th 1,541
Proj_SS 57% 49th 20% 45th 15,333
Proj_OP 57% 49th 21% 41th 7,754
Proj_CO 55% 43rd 17% 52th 491
Proj_EC 28% 5th 62% 2nd 4,125
5 ARCHITECTURE FLAW ANALYSIS
TheDLandPCscoresonlyprovideacoarseassessmentofaproject’s
overall modularity. But a development team needs to precisely de-
terminewhereandhowthesystemshouldbeimproved.In[ 15],Mo
et al. formally defined a set of architecture design flaws4, shown to
behighlycorrelatedwitherror-pronenessandchange-proneness.
We applied their flaw detector to the eight ABBprojects, and cre-
ated an extension of the tool to calculate the maintenance costs
of each flaw, which we call the flaw cost calculator. We reported
theresultstopractitionerstoseeifthisanalysiscouldhelpthem
pinpointthe filegroups responsiblefor maintenanceproblems, to
visualizearchitectureflaws,andtomakerefactoringdecisions.In
thissection,we introducetheconceptofarchitectureflaws,and
then report the flaws detected in ABBprojects.
5.1 Six Types of Architecture Design Flaws
Moetal.[ 15]firstdefinedfivetypesofarchitecturedesignflaws
that were repeatedly observed from many software systems, in-cluding: 1) Unstable Interface, where an influential file changes
frequently with its dependents as recorded in the revision history;
2)Modularity Violation, where structurally decoupled modules fre-
quently change together; 3) Unhealthy Inheritance, where a base
classdependsonitssubclassesoraclientclassdependsonboththebaseclassandoneormoreofitssubclasses;4) CyclicDependency or
Clique,whereagroupoffilesformastronglyconnectedgraph;and5)PackageCycle,wheretwopackagesdependoneachother(rather
than forming a hierarchical structure, as it should). Their tool was
recentlyextendedtodetecta6thtypeofflaw:6) Crossing,where
a file with both high fan-in and high fan-out changed frequently
with its dependents and the files it depends on.
A system may have multiple instances of any flaw, and each can
be visualized as a DSM using existing tools, such as Titan [ 25]. As
exemplifiedinFigure 3,aDSMisasquarematrixinwhichcolumns
androwsarelabeledusingthesamesetoffilesinthesameorder.
Theannotationsineachcellindicatethestructuralandevolutionary
relations between the file on the row and the file on the column.
For example, the cell in row 3, column 1, cell(r3,c1) contains “ d,14",
which means that path1.File3_cpp depends on path1.File1_cpp, and
they have changed together 14 times as recorded in the revision
history.Cellswithnumbersonlyindicatethattherearenostructural
dependencies between the files, but they were changed together.
Cellswithlettersonlyindicatethatthefileontherowsyntactically
4Which are also called as “ hotspots ”o r“ issues”depends on the file on the column, and they were not changedtogether. The cells along the diagonal indicate self-dependency.
Figure3depicts two DSMs, each representing a flaw instance from
aABBproject. Figure 3apresents the DSM of an instance of Clique
(theactualfilesnamesareanonymized),whichshowsthatthereare
16filesintheClique;changestoanyoneofthemcouldinfluence
the other 15.
5.2 Architecture Flaws Detected in Practice
Theflawdetector component,asdepictedinFigure 1,automatically
detectedtheflawswithineachproject,andoutputaDSMfilefor
each flaw instance, which became the input of the flaw cost cal-
culatorcomponentthatquantifiedthemaintenancecostsofeach
flaw, so that the users could better prioritize and rank them. In this
component, four measures extracted from revision history were
used to quantify maintenance costs: 1) change frequency (CF)—the
number of commits a file is involved in; 2) change churn (CC)—the
number of lines of code (LOC) committed to make changes; 3) bug
frequency(BF)—thenumberofbugfixesafileisinvolvedin;4)bug
churn(BC)—thenumberofLOCcommittedforbugfixes.Notall
projectshaveallthedataneeded.Ifthecommitsofaprojectdonot
linktoissues,orissueswerenotcategorizedintobugs,features,etc.,
thiscomponentwillonlycalculateCFandCC.Somelegacysystems
do not keep records about the LOC changed in each commit. In
thesecases,thiscomponentonlycalculatesBForCF.Theoutput
generated by these two components has three parts:
(1) Flaw summary. As an example, Table 3summarizes the flaws
detectedin Proj_EP,theirscopesandmaintenancecosts. Thefirst
lineshowsthatthereare322files(21%ofallthefiles)involvedin
26Cliqueinstances.Thesefileswerechanged1,790timesinvolving
26,294 LOC, 41% of all the LOC changed in the revision history.
643 of the changes are for bug fixing, involving 16,557 LOC, which
is 45% of all the LOC spent for bug fixing. This table shows that
Cliques are very expensive to maintain in this project.
(2) Flaw costs. As an example, Table 4shows that Clique1 in-
volves 99 files and incurred the most maintenance costs, definitely
worth attention. Clique5, although it contains just 16 files, also
appearstobeverycostly.Usingthistable,thedevelopmentteam
can prioritize which flaws need to be addressed in which order.
Bycomparingwithsystemaveragebugandchangerates,wecan
see that files involved in these flaws are causing high maintenance
difficulty.
(3) Flaw DSM. For each instance of each flaw, the tool generated
aDSMwhichweexportedasaspreadsheetsothatthedevelopment
teamcouldvisualizetherelevantstructure.Figures 3a-3bpresent
theDSMsofseveraltypicalflawsthatwereportedtothearchitects,
together with their maintenance effort.
•Figure3ashowsaCliquewherefilesarehighlycoupledby
cyclic dependencies. For example, path1.File1_cpp,
path1.File2_cpp,and path1.File3_cpp formtwodependency
cycles, and these files changed together frequently.
•Figure3bshowsaCrossing,centeredon path2.File2_h,which
dependsonfourfiles andinfluenceseightother files.This
filechangesfrequentlywithallthese12files,andrankedthe
8thmosterror-prone(e.g.,changedforbugfixes11times)
and change-prone among all 6,984 files in Proj_CH.
783
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Mo, W. Snipes, Y. Cai, S. Ramaswamy, R. Kazman, and M. Naedele
Table 3: Architecture Flaws in Proj_EP
Pt. : Percentage; Flaw CF - BC : maintenance costs, quantified by CF, CC, BF and BC, of the files in each flaw
#Instances #Files Pt.Flaw CF Pt.Flaw CC Pt.Flaw BF Pt.Flaw BC Pt.
Clique 26 32221% 1,790 28%26,294 41% 643 34%16,557 45%
Crossing 91 36824% 3,146 50%40,247 63% 1,051 55%25,177 68%
ModularityViolation 667 58838% 4,538 72%46,224 72% 1,438 75%27,648 74%
PackageCycle 175 49932% 2,417 38%29,906 47% 778 41%18,889 51%
UnstableInterface 6 31621% 1,669 26%19,898 31% 388 20%11,457 31%
UnhealthyInheritance 72 25717% 1,528 24%22,007 34% 480 25%13,481 36%
Table 4: Maintenance Costs of Clique instances
Tot. CF - BC: the total CF - BC of all files in each clique instance
Instance Name SizeTot. CF Tot. CC Tot. BF Tot. BC
Clique1 99226 7,847 112 4,673
Clique2 78181 431 7 212
Clique3 28181 1,686 49 897
Clique4 18246 3,130 39 1,427
Clique5 16168 3,553 89 2,662
(a) A Clique: highlighted cells form the dependency cycles.
Effort: Tot. CF: 456; Tot. CC:15,162; Tot. BF:116; Tot. BC:2,676.
(b) A Crossing: the cell in red is the Crossing file; Blue cells
show dependencies and their co-changes.
Effort: Tot. CF: 183; Tot. CC:13,179; Tot. BF:66; Tot. BC:2,936.
Figure 3: Example DSMs of Architecture Flaws
d: depend; number: co-changes
Aftersubmittingthereporttothedevelopmentteams,wealso
conducted a telephone conference to review the results in case the
developerswerenotfamiliarwithDSMs.Duringthepresentation
andinteraction,thedevelopmentteamsallcommentedthatthese
architectureflaws revealedkey problemsthat theyhadsuspected
but had no way to specify or quantify before.6 ARCHITECTURE ROOT ANALYSIS
Unlike the flaw detectors that identify many file groups, the root
detector,asshowninFigure 1,generates10(orfewer)filegroups
that typically capture the majority of a system’s most error-prone
files.Ourobjectiveistoassesswhetherrootanalysiscanhelpde-
velopment teams pinpoint architectural problems more effectively.
6.1 Architecture Roots
Xiao et al. [ 24] proposed that software architecture can be mod-
eled as multiple overlapping design rule spaces (DRSpaces), each
containinganarchitecturallyconnectedgroupoffiles.Theyalso
define the top few DRSpaces that capture most error-prone files
asarchitecture roots (orrootsfor short). They have shown that five
roots can typically cover 50% to 90% of all the error-prone files in a
system,anobservationvalidatedoverdozensofindustrialandopen
source software systems. The implication is that most error-prone
filesarearchitecturallyconnected,andthatthemoreerror-prone
thefilesare,themorelikelythattheyarearchitecturallyconnected
and that errors propagate through the connections.
A root can also be modeled using a DSM. Figure 4presents
a root detected from one of the projects. This DSM reveals mul-tiple architecture design flaws. For example: 1)
p1.F1, an unsta-
ble interface, is depended upon by most of the files, and most of
thesedependents changed togetherwithitfrequently; 2)Multiple
dependency cycles are identified, such as, p1.F5↔p2.F2, and
p2.F2→p2.F1→p1.F6→p1.F5→p2.F2; 3)p1.F1 depends
onitschild,whichisUnhealthyInheritance;4)Manymodularity
violationsarehighlightedinred:structurallyindependentmodules
thathave changedtogetherfrequently.Wealsoshowed thechange
rateandrankingofeachfileinthefirsttwocolumnsoftheDSM.
For example, the file “ p4.F3" in row 26 was changed 361 times,
andrankedthemostchange-proneamongall2,403changedfiles
inProj_SS.Inthisroot,84%ofthefilesrankedwithintop10per-
centile most change-prone, and 6 out of the 31 files ranked within
top1percentile,whichindicatesthatthisrootisarealmaintenance
hotspot.
6.2 Root Analysis in Practice
We useProj_EPas an example to illustrate the roots detected in
theseeightprojects.In Proj_EP,the rootdetector detected4roots,
and generated the following data:
(1) The scope and cost of each root, as shown in Table 5. For
example, the first row shows that the first root involves 147 files.
Thesefileswerechanged1,109times,consuming13,487LOC.Of
these changes, 414 were bug fixes involving 9,347 LOC. As we can
see from the table, even though a Root only covers a small portion
784
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. Experiences Applying Automated Architecture Analysis Tool Suites ASE ’18, September 3–7, 2018, Montpellier, France
Figure 4: DRH-Clustered Architecture Root
d: depend; i: inherit; CF: Change Frequency; Top: percentile rank
ofthe system,itisa hotspotwheremuchmaintenance effortwas
spent.
Table 5: Data of each detected Architecture Root
%: percentage; Rt. CF - BC: the total CF - BC of all files in each root
Size (%) Rt. CF (%) Rt. CC (%) Rt. BF (%) Rt. BC (%)
root1 147 (10%) 1,109 (18%) 13,487 (21%) 414 (22%) 9,347 (25%)
root2 93 (6%) 1,050 (17%) 11,486 (18%) 452 (24%) 6,696 (18%)
root3 79 (5%) 601 (10%) 5,453 (9%) 183 (10%) 3,821 (10%)
root4 104 (7%) 486 (8%) 10,794 (17%) 166 (9%) 6,236 (17%)
(2)Thecumulativedatafor allRoots.Afilemayparticipatein
more than one architecture root; that is, roots overlap with each
other.DV8alsocalculatestheircumulativedata,asshowninTable 6.
Inthistable,“ Size"meansthenumberofdistinctfilesinthefirst
nroots, where, n=1,2, ...,4. The “% Size" column presents the
percentage of the root size compared with the total number of
files in the project. For example, “222" in the second row meansthat
root1 androot2 (the first 2 Roots) contain 222 distinct files,
whichcovers14%ofallfilesintheproject.The“ Coveraдe "column
presents the cumulative coverage of change-prone or bug-prone
files by these roots. The fourth row of this table indicates all these
4 roots contain only 24% of all the files in this project, but cover
55% of all change-prone files and 65% of all bug-prone files. Filesin each root are architecturally connected, hence it appears thatchange-proneness or bug-proneness may be propagated among
these files.
Moreover,followingtheexperiencereportedin[ 12],andconsid-
eringeacharchitecturerootasa debt,wecreatedthe debtcalculator
to compute the penalty incurred by these roots as the difference
betweentheactualmaintenanceeffortspentandthe expectedmain-
tenance effort spent on them. We use the average change/bug rate
of all the files in each project as the expectedmaintenance effort,
following [ 12]. Theexpectedeffort columns “ ExtraCF"- “ExtraBC"
represent the cumulative maintenance penalty from the roots. For
example, “615" in the second row of “ ExtraBF" column indicates
that the 222 files in root1 androot2 are involved in bug fixes 615timesmoreoftenthanaveragefiles.The“ Percentaдe "rowpresents
the percentage of the extra maintenance effort to whole project.
The last row indicates that, 28% of all the changes, 41% of all the
LOC spent, 40% of bug-fixing changes, and 47% of bug-fixing LOC
spentontheentireprojectareincurred(orpenalties)bytheseroots.
Table 6: Cumulative Data of Architecture Roots (Proj_EP )
Coverage
Root Size % Size Change Bug
root1 147 10% 24% 29%
root2 222 14% 38% 52%
root3 263 17% 47% 57%
root4 364 24% 55% 65%
Penalty of Architecture Roots
Extra CF Extra CC Extra BF Extra BC
root1 612 8,450 263 6,418
root2 1,332 16,601 615 11,175
root3 1,687 19,570 724 13,552
root4 1,754 26,110 763 17,314
Percentage 28% 41% 40% 47%
Wehaveobservedconsistentresultsfromalleightprojects,as
summarized in Table 7. Column “ All Roots Tot .Size%" shows that,
for all the projects, their detected architecture roots contain 2% -
24%ofallfilesineachproject,butcoveramuchlargerportionofthe
project’schange-prone(37%-68%)orbug-pronefiles(47%-81%)
asshownincolumn“ Tot.Coveraдe ".Duetothelackoftraceability
ofthebugs,wedidnotconductthebug-relatedanalysisonthelast
three projects. The “ Penalty" columns show that, for all projects, a
large portion of maintenance effort spent on a project is generated
from the detected architecture roots.
Aswewillelaboratelater,thefeedbackregardingthedetected
rootswasdivided.Someteamsfoundthatrootscancapturehotspots
more effectively than the flaw analysis since they only need to
examine a few file groups, but other teams found that the detected
roots can be distorted.
Table 7: Architecture Root Analysis results of all projects
Tot. Ext CF - BC %: showing the ratio of penalty from all roots in
each project to the total maintenance effort spent on the project
Project #RootAll Roots Tot. Coverage
Tot. Size % Change Bug
Proj_CH 4 8% 53% 47%
Proj_CO 1 18% 59% 56%
Proj_EP 4 24% 55% 65%
Proj_OP 8 14% 51% 47%
Proj_EC 7 13% 46% 81%
Proj_SS 5 2% 63% -
Proj_BM 1 11% 37% -
Proj_EO 2 19% 68% -
Penalty Percentage (%) of all Architecture Roots
Tot. Ext CF % Tot. Ext CC % Tot. Ext BF % Tot. Ext. BC %
Proj_CH 53% 83% 64% 89%
Proj_CO 39% 56% 46% 62%
Proj_EP 28% 41% 40% 47%
Proj_OP 28% 43% 35% 45%
Proj_EC 9% 21% 11% 3%
Proj_SS 30% 59% - -
Proj_BM 35% 56% - -
Proj_EO 47% 68% - -
7 INTERVIEWS AND FEEDBACK
Tobetterunderstandhowtheinformationpresentedbytheanalysis
framework was understood and used, we formulated a set of ques-
tionsforthepilotparticipantstoansweraftertheyhadreviewed
785
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Mo, W. Snipes, Y. Cai, S. Ramaswamy, R. Kazman, and M. Naedele
the results. Participants were provided with the questions in ad-
vanceofa telephoneconferenceconductedto record theiranswers.
Thequestionsweredesignedtofollowthe keydeliverablesofthe
report,themetrics,architectureflaws,andarchitectureroots.We
posed the questions to five participants who represented one or
more of the 8 projects we analyzed. Three of the participants were
software architects and two were R&D managers. Our objective
was to explore how the development teams intended to react to
andusethereporttoimprovetheirarchitecturequality.Nextare
the questions and the summary of their responses:
Q1: What did the report reveal that you didn’t know about your
software? Prior to having this report, the participants had intuitive
understandingoftheirarchitectures.Oneofthemcommented:“We
understandintuitivelyhowourcodeisstructured."TheDLandPC
scoresweresurprisingforthearchitectsofProj_OPandProj_SS,
whothoughtthereportindicatedtheircodewasbetterthantheir
opinion of it. For Proj_OP, their maintenance effort was higher
thantheythoughtthemetricsindicateditshouldbe.ForProj_SS,
some programming languages used in the project were not fully
supported by Understand, therefore the scores were better than
they should be due to missing elements.
Q2: Are the metrics useful for reflecting the architecture of your
software? All the participants commented that the report provided
them quantifiable results and actionable items to improve theirarchitecture, as well as a way to discuss the importance of refac-
toring their architecture with managers. For Proj_CH, the trend
of metrics between releases was useful to understand whether the
architecturewasimprovingordegrading.ForProj_ECandProj_SS,
the participant thought the scores were useful and would like to
rundailyanalysestocomparethevariationsofthescoresovertime.
TheneedforupdatedreportswasalsoexpressedbyprojectsBM,
CO,andEP.Proj_EOhadaveryhighDLandalowPCscore,but
they still had more rework effort than expected as they attempted
to evolve their product, which indicates that metrics derived from
syntactical relations only may not be sufficient.
Q3: What did the architecture design flaws reveal about your soft-
ware?Proj_EO was surprised by their architecture design flaws
and found a few false-positives. All other projects reported that
the detected flaws confirmed what they intuitively knew about the
structure of their software, and made their intuitive knowledge
visualizable and quantifiable.
Q4: What actions have you planned as a result of the architecture
design flaws report? Six of the projects said they planned to per-
form refactoring to address the detected architecture design flaws.
Proj_BM and Proj_EO do not plan to refactor, and indeed their DL
andPCscoreswerehigh.ThearchitectofProj_CHcommentedthatthearchitectureflawreportprovidedthemquantitativedatatohelp
prioritize where to refactor the code. Proj_EC and Proj_SS, which
are already actively refactoring, desired more specific guidance
towards how they should refactor.
Q5: What did the architecture roots reveal about your software?
Proj_OP reported that roots in a particularcomponent were unex-
pected, and probably indicated a higher amount of development
activity in normally stable code. The participant commented: “ We
may have underestimated the risk of changing this part of the code."
For Proj_CH and Proj_OP, the architecture roots pointed to utility
files that should not be considered as roots of the structure. Thesemoduleswereusedbymanyothercomponentsthustheanalysis
consideredthemasroots.Proj_ECandProj_SSfoundthefilesre-
portedasrootswereexpectedbecausetheycontaindefinitionsthat
change frequently.
Q6:Whatactionsdoyouplantotaketoaddressarchitectureroots?
Proj_OP had a plan to componentize the architecture, which is
expectedtoaddresstherootsandimprovethemetricsscores.They
commented: “We plan to monitor our progress in architecture decou-
plingusingthese metricsovertime." Proj_ECandProj_SS reported
being unsure about how to proceed with improving their architec-
turerootsandfelttheylackedanaccuratementalmodelofwhat
anarchitecturerootis.Proj_CHexpressedasimilarsentimentbe-
cause utility files were identified as roots and they did not fully
understand the purpose of root identification. By contrast, Proj_EP
confirmed that the detected roots are composed of defect-prone
files and they were planning to refactor them to improve quality.
8 ANSWERS TO RESEARCH QUESTIONS
We summarize the feedback provided by the development teams to
answer each research question below.
RQ1: does DV8 help to close the gap between management and de-
velopment?Thatis,doesithelpthemtodecideif,when,andwhereto
refactor?Threeoftheparticipantsinchargeof5projects(Proj_BM,
Proj_CO,Proj_EP,Proj_CH,andProj_OP)verifiedthattheinfor-
mation provided was useful in closing the understanding gap with
management. Even though the other two participants didn’t ex-
plicitlycommentonthisaspect,thefactthatsixofeightprojects
planned or had already begun refactoring their code to address the
flawsandrootssuggeststhatourreportplayedaroleinreaching
these refactoring decisions.
RQ2: does DV8 help practitioners understand the maintainability
oftheirsystemsrelativetootherprojectsinternaltothecompany,andrelativetoamorebroad-basedbenchmarksuite? Alltheparticipants
said the report gave them quantifiable results with which to judge
theircodebase.Twoweresurprisedonhowgoodtheirproducts
wererated,thatis,closetothe50thpercentilewithintheindustrial
benchmark,giventheirintuitiveunderstandingofthemaintenance
effortinvolvedfortheirproducts.Thecomparisonwithindustrial
benchmark makes it clear that maintenance difficulty caused by
degrading architecture is very common.
RQ3: does DV8 help developers pinpoint the hotspots of their
systems—thatis,thegroupsoffileswithseveredesignflaws? Based
on the feedback from all 5 participants, the answer to this ques-tion is clearly yes. Six of the eight projects planned to or already
started refactoring to address the detected flaws. The project with
thelowestDLscoreisundergoingamajorrewrite.Onepractitioner
expressedtheneedformoredetailedguidanceonhowtorefactor
the detected flaws.
In summary, we can answer all three questions positively.
9 LESSONS LEARNED
Inthissection,wediscussthelessonslearnedinterms:theeffec-
tivenessofthesetechniques,thedataquality,andthelimitations
that lead to future work.
UsingDLandPC. The practitioners adopted DL and PC easily,
andexpressedtheneedtocomparemultipleprojectsandanalyze
786
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. Experiences Applying Automated Architecture Analysis Tool Suites ASE ’18, September 3–7, 2018, Montpellier, France
multiplereleasesofthesameprojectusingjustafewmetricssothat
theycanmonitorthequalityofthearchitecture.Wesummarized
the following lessons regarding how DL and PC should be used in
a complementary way:
(1) If a system has a low DL and high PC score, it means that
maintenancedifficultyisinevitable,andthisconclusionisconsis-
tent with the experience of practitioners: so far we have seen no
exceptions.
(2) If the DL and PC scores are both highly ranked, it means
the system is likely not experiencing severe problems. If the devel-
opmentteamisexperiencing maintenancedifficulty,thissuggests
thatthesystemhasalargenumberofimplicitdependencies.Inthis
case, architecture flaw detector should be executed to pinpoint the
problematic file groups.
(3)IfthesystemhasahighlyrankedDL,butamuchlowerranked
PC, such as Proj_CH where DL is ranked the 81st percentile, but
PC is only ranked the 54th percentile, it means that there couldbe a small portion of the system that is highly coupled, which isconfirmed by the development team. This result implies that an
overallDLscoreonlymaynotbeabletoreflecttheexistenceofa
high-maintenance subsystem.
(4) If both scores are ranked medium, e.g. the DL of Proj_CO
ranked the 43rd and its PC ranked the 52nd, then the project is
likely experiencing maintenancedifficulties already, as confirmed
by the development teams.
We have not observed a case where the system has a highly
rankedPCscore,butitsDLrankingisverylow.SincePCisvery
sensitivetothesizeoftheproject,asreportedin[ 16],wesuggest
that these two scores should be used together. Another lessonis that a good DL and PC score does not necessarily mean that a
systemishealthy.Ifthedevelopmentteamisexperiencingdifficultydespitegoodscores,itisworthwhiletouseflawdetectorstofurther
pinpoint where the difficulty comes from.
Usingflawsandroots. Inadditiontoquantifyingflaws,visual-
ization of each flaw augments the intuition of developers, bridging
the gap between development and management. Currently we ex-
portflawDSMsintospreadsheets,andmarkthefilesinvolvedin
each flaw manually. In the future, we will further automate this
process.
The experiences of root analysis are divided. Some teams like
thefactthatusingroots,theyonlyneedtoinspectafewfilegroups,andobservehowmosterror-pronefilesareconnected.Otherteams
foundthatsomerootsarecausedbyhigh-impactutilityfiles.Be-
cause these files have a large number of dependents, they can form
a large DRSpace that captures a large number of files, including
both error-proneand healthyones, thusdistorting theresult. The
lesson here is that the high-impact utility files should be excluded
from this analysis.
We also learned that the pilot participants required comparison
reports which showed the evolution of a product between releases.
These reports were usefulbecausethe teamcould understandthe
trend of whether their architecture was degrading or improving.
The report contained comparisons of the key metrics plus compar-
isons of the architecture flaws and which classes were involved in
architectureroots.Wefoundsomerootspersistedbetweenreleases,
which means that they were a consistent source of development
effort across releases.One comment: “Since the guidance of the report is towards refac-
toring, it makes sense that a release to release comparison would be a
useful way to integrate it into the software development life-cycle... I
wouldlovetohavethisintegratedintothedailybuild,withautomatic
production of guidance to architects to help them with day to day
architectural governance."
Dataquality. Anotherlessonwastoconsidermultiplebranches
when collecting history data. What the participants provided as
a known good source code branch frequently was a branch with
veryfewactualdevelopercommits(theyweremostlymerges)thus
littlehistoryofeachfilecouldbecollected.Byconsideringmultiplebranchesforhistoricalchanges,weexpandedthesetofcommitsin
theanalysisandhadamorecompleteviewofthehistoryofeach
file.
OfallthethreetypesofanalysisprovidedbyDV8,theDLand
PC are structure metrics, calculated based on syntactic relationsonly. Three architecture flaws require structural relations only:
Clique, Improper Inheritance, and Package Cycles. The other three
flaws,aswellasrootanalysisrelyonrevisionhistorydata.Ifthe
issue tracking data is available, issues can be categorized, e.g., into
bug-fixing,featureaddition,etc.,andeachcommitislinkedwith
anissue,thenwecancalculatebothchangerelatedcosts(change
frequencyandchangechurn)andbug-relatedcosts(bugchurnand
bug frequency). If the issue tracking data is not available, or notlinked with commits, then we can only calculate change-related
costs.
Consistent with our observations with other industrial projects,
mostofthecomany’sprojectsdonothavethedataneededforall
analyses. Some projects have a long history, back to the time when
modern version control systems were not available. Other projects
used their own issue tracking systems that do not support issue
categorization. There are also projects in which the commits were
not linked to issues.
Aftertalkingtothedevelopmentteams,werealizedthatinsome
projects, the developers were not required to link commits with
issuessincetheydidn’tenvisionthepossibleusageofthedata.Now
that they have seen the benefits of these analyses, a more rigorous
management process was being discussed.
LimitationsandFutureWork. Theprocessofanalyzingthe8
projectsalsorevealedseverallimitationsofDV8andtheunderlyingtechnologies.Firstofall,thedefinitionofcertainarchitectureflaws
should be further refined. For example, the tool may detect a large
numberofmodularityviolations(MV)thatoverlapwitheachother.
Sometimesthe numberof filesinvolved inMV isso largethat the
instances were just ignored all together. We are exploring the pos-
siblewaystofurtherrefinethedefinitionofMVanditsdetection
algorithmtoobtainmorefocusedresults.Second,basedonthefeed-
back we received about root analysis, we plan to further refine the
root detectionmethods sothat utilityfiles canbe excluded.Third,
DV8 is limited to C, C++, C# and Java, the mainstream program-minglanguagesthatcanbeaccuratelyprocessedby
Understand .
Processing multi-language systems is a future direction of the tool
suite. Finally, we will further increase the number of projects inthe benchmark database so that the users can compare projects
withsimilarsizesanddomains.Eventhoughwehavenotobserved
that DL is affected by these characteristics, PC can be significantly
affected by the number of files in the project.
787
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Mo, W. Snipes, Y. Cai, S. Ramaswamy, R. Kazman, and M. Naedele
Findings with respect to the research questions have a threat to
external validity in that they are based on the opinions of only 5
practitioners and the set of practitioners is focused on architects
and managers not developers.
During this process, we have observed the need to further auto-
matetheprocesstoenabletheanalysisofmorecomplexcodebases;forexample,asystemmaycontainmultiplecomponents.Eachcom-
ponent should be analyzed individually,and the system should be
analyzed as a whole. We have worked to automate the analysis by
addingconfigurationmanagementtotheinterimdataandapplyingscriptstoautomatethegenerationofreportdata.Thepractitioners
alsosuggestedthattheywouldliketoknowwhetheraproduct’s
DL andPC values have gotten worse over night.If so, theywould
liketoknowhowtolocatethespecificproblemarea.Theyenvision
avisualdiffoftheDSM.Thentheywouldneedautomatedguidance
on what to change (which file and which refactoring) to repair the
situation before problems accumulate. So far, our analysis is at the
filelevel,oneofthepractitionerssuggestedthatwecoulddigmore
details at the method level. If so, they would like to know which
methods or attributes are responsible to the specific flaws or roots.
10 RELATED WORK
In this section, we compared with the related work in the areas of
software metrics, defect prediction and technical debt.
SoftwareMetrics.Numerousresearchhasbeenconductedtomea-
sure software systems. McCabe [ 14] measures code complexity by
calculating the number of linearly independent paths in the source
code. Various metrics were proposed to measure OO projects, such
as C&K metrics[ 8] and MOOD Metrics [ 11]. Yu at. el’s [ 26–28]
proposed multiple coupling metrics and reported that they were
correlated to history changes, reuse effort and software perfor-
mance respectively. Bouwers et. al. [ 4] showed that the measuring
results from two architecture metrics [ 2,3] matched practitioner’s
intuitions and could help in the decision-making process. There
is no substantial evidence showing that these metrics can be used
toeffectivelycompareandcontrastdifferentprojectsormultiple
versionsofthesameproject,andthusformaneffectivebenchmark
to bridge the gap between management and development teams.
Sahraouiet.al.[ 20]presentedameasurementprogram,MQL.Their
results showed that using MQL could significantly impact the qual-
ity of software systems, such as, maintainability, evolvability, code
complexity, etc. But they didn’t have a benchmark to follow anddidn’t clearly present how to guide the development teams for
further refactorings.
Defectprediction. Defectpredictionhasalsobeenwidelystudied.
Codemetrics,historymeasuresorbothwereusedfordefectpredic-
tion. Nagappan et al. [ 17] presented a combination of code metrics
used for defect prediction. However, they also reported that the
bestcombinationofmetricsvariesindifferentprojects.Selbyand
Basili [22] presented that dependency structure is a good indicator
of software defects. Cataldo et al.’s [ 7] showed that the density
of change coupling is strongly correlated with failure proneness.
Ostrand et al. [ 19] demonstrated that a combination of file metrics
andfilechangehistorycanbeusedtoeffectivelypredictdefects.All
the above studies treat files individually in the analysis, not takingarchitecturalconnectionsamongfilesintoconsideration.Bycon-
trast,therootandflawdetectionweappliedcanrevealarchitecture
problems that propagate errors among multiple files. Schwanke et.
at. [21] reported their experience of using structure dependency
and history measures to predict defects and detect architecture
issues,buttheirexperiencewasbasedononeindustrialcaseand
focused on the detection of molecularity violation. By contrast, we
report our experiences of applying software measurement, flaw
and root detection comprehensively.
Technical Debt Analysis. In the past decade, a number of heuris-
ticshavebeenproposedtoanalyzetechnicaldebt[ 9]ofsoftware
systems. Kazman et. al. [ 12] presented their experience of using
economicmodelstoassessthecostsandbenefitsofrefactoringsoft-
warearchitecturedebts,inwhichtheyonlyreportedtheexperience
from one system, without the application of DL/PC benchmark,
flawdetection, orthe automatedcalculationof maintenancecosts
of each flaw. Carriere et. al. [ 6] used a cost-benefit model to esti-
mate the effort and benefits of applying refactoring to decouple
the architecture. Their study only considered the coupling level of
architectureinonecase,anditdidnotprovideinformationabout
when and where to refactor. Curtis et. al. [ 10] presented a model
to estimate technical debt principal in terms of cost which is deter-
mined by static analysis of source code. Nord et. al. [ 18] developed
a formula to assess the impact of technical debt in architecture,and presented that their approach could be used to optimize the
long-termevolutionofaproduct.Thesestudiesonlyreportedinfor-
mationforassessingtechnicaldebt,butdidn’treportinformation
about where to refactor.
11 CONCLUSIONS
In this paper, we reported our experiences of applying three ar-chitecture analysis techniques, supported by an automated toolsuite with 8 components, to 8 projects in ABB. Our experiences
demonstrated that: 1) DL and PC could effectively reflect the main-
tainability of a software project by comparing with a publishedindustrial benchmark; 2) architecture flaw analysis enables prac-titionerstopinpointandvisualizeseveredesignflaws,aswellasto quantify their maintenance costs, so that developers can tar-
get refactoring actions towards the most severe architecture flaws;
and3)architecturerootanalysiscouldrevealhowbug-proneand
change-pronefilesareconnectedmoreeffectively.Thesetechniques
and our tools have been adopted within ABB, we are now working
on integrating thethree techniques into adeployable service that
can be used by all projects in the company. We will also create a
commandlineversionofthetools,sothatthekeyanalyses,suchasDLandPCcalculations,canbemoreeasilyintegratedintoexistingsoftwarequalitycontroltools,suchasSonarQube
5.Ourobjectiveis
tomeasureprojectswitheachbuildsothatanyqualitydegradation
can be detected immediately.
ACKNOWLEDGMENTS
ThisworkwassupportedinpartbytheNationalScienceFoundation
under grants CCF-1514315 and CCF-1514561.
5https://www.sonarqube.org/
788
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. Experiences Applying Automated Architecture Analysis Tool Suites ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]C. Y. Baldwin and K. B. Clark. Design Rules, Vol. 1: The Power of Modularity.M I T
Press, 2000.
[2]E. Bouwers, J. P. Correia, A. van Deursen, and J. Visser. Quantifying the analyz-
ability of software architectures. In Proc. 12thWorking IEEE/IFIP International
Conference on Software Architecture, pages 83–92, 2011.
[3]E. Bouwers, A. van Deursen, and J. Visser. Dependency profiles for software
architecture evaluations. In Proc. 27thIEEE International Conference on Software
Maintenance, pages 540–543, 2011.
[4]E. Bouwers, A. van Deursen, and J. Visser. Evaluating usefulness of software
metrics: An industrial experience report. In Proc. 35thInternational Conference on
Software Engineering, pages 921–930, 2013.
[5]Y.CaiandK.J.Sullivan. Modularityanalysisoflogicaldesignmodels. In Proc.21st
IEEE/ACM International Conference on Automated Software Engineering, pages
91–102, Sept. 2006.
[6]J. Carriere, R. Kazman, and I. Ozkaya. A cost-benefit framework for making
architecturaldecisionsinabusinesscontext. In Proc.32ndInternationalConference
on Software Engineering, pages 149–157, 2010.
[7]M. Cataldo, A. Mockus, J. A. Roberts, and J. D. Herbsleb. Software dependencies,
work dependencies, and their impact on failures. IEEE Transactions on Software
Engineering, 35(6):864–878, July 2009.
[8]S. R. Chidamber and C. F. Kemerer. A metrics suite for object oriented design.
IEEE Transactions on Software Engineering, 20(6):476–493, June 1994.
[9]W.Cunningham. TheWyCashportfoliomanagementsystem. In Addendumto
Proc. 7th ACM SIGPLAN Conference on Object-Oriented Programming, Systems,
Languages, and Applications, pages 29–30, Oct. 1992.
[10]B.Curtis,J.Sappidi,andA.Szynkarski. Estimatingtheprincipalofanapplica-
tion’s technical debt. IEEE Software, 29(6):34–42, 2012.
[11]F. B. e Abreu. The mood metrics set. In Proc. ECOOP’95 Workshop on Metrics,
1995.
[12]R.Kazman,Y.Cai,R.Mo,Q.Feng,L.Xiao,S.Haziyev,V.Fedak,andA.Shapochka.
Acasestudyinlocatingthearchitecturalrootsoftechnicaldebt. In Proc.37th
International Conference on Software Engineering, May 2015.
[13]A.MacCormack,J.Rusnak,andC.Y.Baldwin. Exploringthestructureofcom-
plexsoftwaredesigns:Anempiricalstudyofopensourceandproprietarycode.
Management Science, 52(7):1015–1030, July 2006.
[14]T. J. McCabe. A complexity measure. IEEE Transactions on Software Engineering,
2(4):308–320, Dec. 1976.[15]R.Mo,Y.Cai,R.Kazman,andL.Xiao. Hotspotpatterns:Theformaldefinition
andautomaticdetectionofarchitecturesmells. In Proc.12thWorkingIEEE/IFIP
International Conference on Software Architecture, May 2015.
[16] R. Mo, Y. Cai, R. Kazman, L. Xiao, and Q. Feng. Decoupling level: A new metric
for architectural maintenance complexity. In Proc. 38thInternational Conference
on Software Engineering, pages 499–510, 2016.
[17]N.Nagappan,T.Ball,andA.Zeller. Miningmetricstopredictcomponentfailures.
InProc. 28th International Conference on Software Engineering, pages 452–461,
2006.
[18]R.L.Nord,I.Ozkaya,P.Kruchten,andM.Gonzalez-Rojas.Insearchofametricfor
managingarchitecturaltechnicaldebt. In 2012JointWorkingIEEE/IFIPConference
on Software Architecture and European Conference on Software Architecture, pages
91–100, 2012.
[19]T.J.Ostrand,E.J.Weyuker,andR.M.Bell. Predictingthelocationandnumber
offaultsinlargesoftwaresystems. IEEETransactionsonSoftwareEngineering,
31(4):340–355, 2005.
[20]H.Sahraoui,L.C.Briand,Y.-G.Gueheneuc,andO.Beaurepaire. Investigatingtheimpactofameasurementprogramonsoftwarequality. InformationandSoftware
Technology, 52(9):923–933, 2010.
[21]R.Schwanke,L.Xiao,andY.Cai. Measuringarchitecturequalitybystructureplus
historyanalysis. In Proc.35rdInternationalConferenceonSoftwareEngineering,
pages 891–900, May 2013.
[22]R. W. Selby and V. R. Basili. Analyzing error-prone system structure. IEEE
Transactions on Software Engineering, 17(2):141–152, Feb. 1991.
[23]S.Wong,Y.Cai,G.Valetto,G.Simeonov,andK.Sethi. Designrulehierarchiesand
parallelism in software development tasks. In Proc. 24th IEEE/ACM International
Conference on Automated Software Engineering, pages 197–208, Nov. 2009.
[24]L.Xiao,Y.Cai,andR.Kazman. Designrulespaces:Anewformofarchitecture
insight. In Proc. 36rd International Conference on Software Engineering, 2014.
[25]L.Xiao,Y.Cai,andR.Kazman.Titan:Atoolsetthatconnectssoftwarearchitecture
with quality analysis. In 22nd ACM SIGSOFT International Symposium on the
Foundations of Software Engineering, 2014.
[26]L. Yu, K. Chen, and R. Ramaswamy. Multiple-parameter coupling metrics for
layered component-based software. Software Quality Journal, 17(1):5–24, 2009.
[27]L.YuandR.Ramaswamy. Componentdependencyinobject-orientedsoftware.
J. Comput. Sci. Technol., 22(3):379–386, May 2007.
[28]L.YuandR.Ramaswamy.Examiningtherelationshipsbetweensoftwarecoupling
andsoftware performance:across-platformexperiment. JournalofComputing
and Information Technology, 2011.
789
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:28 UTC from IEEE Xplore.  Restrictions apply. 