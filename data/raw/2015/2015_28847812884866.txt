VDTest: An Automated Framework to Support Testing for
Virtual Devices
Tingting Yu
Dept. of Comp. Sci.
University of Kentucky
Lexington, KY , 40506, USA
tyu@cs.uky.eduXiao Qu
ABB Corporate Research
Raleigh, NC, 27606, USA
xiao.qu@us.abb.comMyra B. Cohen
Dept. of Comp. Sci. & Eng.
Univ. of Nebraska - Lincoln
Lincoln, NE, 68588, USA
myra@cse.unl.edu
ABSTRACT
The use of virtual devices in place of physical hardware is in-
creasing in activities such as design, testing and debugging.Yet virtual devices are simply software applications, and like
all software they are prone to faults. A full system simulator
(FSS), is a class of virtual machine that includes a large setof virtual devices – enough to run the full target softwarestack. Defects in an FSS virtual device may have cascading
eﬀects as the incorrect behavior can be propagated forward
to many diﬀerent platforms as well as to guest programs. Inthis work we present VDTest, a novel framework for testing
virtual devices within an FSS. VDTest begins by generat-
ing a test speciﬁcation obtained through static analysis. It
then employs a two-phase testing approach to test virtual
components both individually and in combination. It lever-ages a diﬀerential oracle strategy, taking advantage of theexistence of a physical or golden device to eliminate the need
for manually generating test oracles. In an empirical study
using both open source and commercial FSSs, we found 64faults, 83% more than random testing.
CCS Concepts
•Software and its engineering →Software defect anal-
ysis;•Computer systems organization →Embedded
software;
Keywords
Testing, Virtual Devices, Device Drivers, Test Oracles
1. INTRODUCTION
A full-system simulator (FSS), is a virtual machine (VM)
(or software implementation) of the complete environmentthat executes target software on a physical machine [5,14,
36]. Unlike process VMs or hypervisor-based systems (such
as VMware) that rely on the host architecture to run thetarget machines [9], an FSS encompasses a variety of virtualdevices that can simulate not only the processor cores and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributedfor proﬁt or commercial advantage and that copies bear this notice and the full cita-tion on the ﬁrst page. Copyrights for components of this work owned by others thanACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior speciﬁc permissionand/or a fee. Request permissions from permissions@acm.org.
ICSE ’16, May 14-22, 2016, Austin, TX, USA
c/circlecopyrt2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2884781.2884866memory, but also the entire hardware platform including
the network, buses, and peripheral devices (e.g., keyboards,USBs, video adaptors). Usually, diﬀerent devices can becombined to provide a large number of unique platforms.
FSSs are becoming widely used for many purposes in em-
bedded and mobile domains where hardware is diverse, andnew platforms are being released at a rapid pace. FSSs areusedfortasksincludingsystemdesign, development, testing,debugging and security analysis. This relieves the engineer
from having to own many diﬀerent physical devices, and al-
lows them to adapt quickly during hardware and softwareevolution. Developers can also interact with the FSS to im-plement customized tools for their target device, such as test
casegenerators, or theycanuseadditionalFSSfeaturessuch
as proﬁling and provisioning.
Developing a virtual device can be a challenging task. The
oﬃcial documentation of hardware devices often contain in-accuracies and ambiguities [29], and thus the corresponding
software implementation is unlikely to be fault-free. Yet
defects in an FSS can have cascading eﬀects. One of theearlier versions (v4.4) of the Android emulator did not ro-tate screens, preventing developers from testing or debug-
ging any application that rotated [21]. Several studies have
shown that software faults in virtual devices can cause secu-rity vulnerabilities [2–4]. For example, a critical vulnerabil-ity (called Venom) exists in the virtual ﬂoppy disk controller(FDC)codeintheQuickEmulator(QEMU)FSS.
1Thefault
stems from the FIFO buﬀer that the virtual FDC simulatesto store commands from the CPU. The FIFO fails to resetthe index allowing writes by the FDC to overﬂow. This se-curity fault can propagate to the programs operating on the
host platform [3].
Despite the existence of many automated software test-
ing techniques, applying these directly to virtual devices
is not straightforward. First, execution environments re-quirespecialtestdrivers. Second, failuresareoftentriggered
by interactions between components. Third, failures often
do not lead to crashes, making oracles diﬃcult to obtain.Fourth, output states can be masked by input and outputdata points that are shared.
To overcome these challenges we propose an automated
framework, VDTest, that allows engineers to eﬀectively
test virtual devices within an FSS. First, VDTest provides
a test template generator that extracts basic device proper-ties with little manual eﬀort and uses these to generate test
cases. Second, it utilizes a two phase testing approach. The
ﬁrst phase tests the behavior of individual components (e.g.,
1QEMU has both a hypervisor and FSS version.
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   583
registers and data buﬀers), and the second phase integrates
components, testing for interactions among them. Third,
to address the oracle problem, VDTest employs a physi-
cal device or a gold-standard virtual device, (also called the
oracle device ), which leverages diﬀerential testing [29,30].
Finally, to avoid masking eﬀects [13], VDTest manipulates
only necessary input parameters in a test case, leaving in-
ert parameters unchanged. It uses a feedback-driven test-
ing process to identify invalid/ineﬀective parameters thatdo not impact the hardware state (e.g., read-only registers),and then enforces constraints to omit these parameters in
further iterations.
To evaluate VDTest we conducted an empirical study us-
ing 11 pairs of devices, obtained from one physical machine,
two open-source FSSs and one commercial FSS. The resultsshow that VDTest is eﬀective at revealing faults; 82.3%
more faults were detected than random testing. When com-pared with a traditional combinatorial interaction testingtechnique [11], it reveals 42% more faults given the sametesting budget. The contributions of this work are:
•VDTest, a framework for testing virtual devices in an
FSS; and
•An empirical study that shows VDTest can ﬁnd new
faults in existing FSS virtual devices, many of which
have been conﬁrmed by developers.
In the next section we present a motivating example and
background. We then describe VDTest in Section 3. Our
empirical study follows in Sections 4 and 5, followed by dis-cussion in Section 6. We present related work in Section 7,
and end with conclusions in Section 8.
2. MOTIV ATION AND BACKGROUND
An FSS sits on top of host operating system emulating all
of the hardware for each supported device. It also can run
guest operating systems and applications (both of which arenormal versions of their respective programs). Figure 1 isa snippet of code extracted from a real virtual device, the
PL031timer device in the QEMU FSS. It contains a bug at
line 19 [27]. The interrupt mask bit s->imis incorrectly set.
The interrupt status register s->isshould be set instead.
The result is that interrupt alarms are not ﬁred as expected.
Theﬁrstchallengeisthattestingthiscoderequiresspecial
test drivers – the execution environment must be properly
modeled. For instance, I/O interface functions (i.e., entryfunctions) such as pl031_write on line 1 are invoked by the
FSS to pass inputs to the virtual device. And the device
transaction functions such as pl031_set_alarm (line 4) are
invoked to perform I/O commands that may ﬁre interrupts
by calling qemu_set_irq (line 20).
Second, the function (line 16) containing the faulty state-
ment can not be executed unless certain bits are set in both
the data load register ( RTC_LR) and interrupt mask (RTC_MR)
register. As such, interactions between device components
must be considered when testing a virtual device.
Third, faults often fail to propagate their eﬀects to pro-
gram outputs (e.g., crash). In such cases, output-based testoracles are inadequate. Thus, internal oracles are needed
that allow engineers to inspect internal device states (e.g.,register values) for correctness. Such states are referred toasobservable output points [8]. In Figure 1 the results of
RTC_LRand RTC_MRregisters are observable output points.1. static void pl031_write(...)
2. {3. case RTC_MR:
4. pl031_set_alarm(s);
5. case RTC_LR:6. ticks +=value - pl031_get_count(s); /*value is register content*/7. s->mr = value & 1 /*change RTC_MR*/8. ...9. }
10. static void pl031_set_alarm(...)
11. {
12. if (ticks == 0)
13. pl031_interrupt(s);14. ...15. }
16. static void pl031_interrupt(...)
17. {18. PL031State *s = (PL031State *)state;19. s->im = 1; // should be s->is = 1;
20. qemu_set_irq(s->irq, s->is & s->im);
21. }
Figure 1: Example virtual device code
Finally, the observable output points are often the same
as the input points, so they can be masked by an input value
written to the same point. The input register components(RTC_LR and RTC_MR) are also used to observe device state.
Setting the inputs of RTC_LRmay aﬀect the output of RTC_MR
(line 7). If both registers are set with input values, the
output eﬀect of RTC_MRmay be masked by its input.
These issues motivate our need for a special testing frame-
work for virtual devices.
2.1 Virtual Devices
Figure 2 illustrates a typical I/O system with peripheral
devices (the gray area reﬂects the devices and their I/O
memory). A peripheral device is a device that is connectedto a computer but is not part of the core computer archi-
tecture (e.g., CPU, motherboard and memory). It is con-
trolled by reading and writing to its registers either withinthe memory address space (memory-mapped I/O) or theI/O address space (port-mapped I/O). For memory-mappedI/O, the device registers are mapped into the CPU’s address
space. Thus, the device is accessed in the same way as a reg-
ular memory access. For port-mapped I/O, I/O device reg-isters have a separate address space from the CPU addressspace. Thus, the device is often accessed through a special
class of CPU instructions (e.g., inband outbin X86).
Most devices have at least two types of registers. The
ﬁrst type is the data register, through which the input/out-
put data is read from or written to the device. The second
type is a control register , which selects and shows the mode
of operations of the device. Certain bits in the registers arewrite-only or read-only. A device can also have data buﬀersthat temporarily store the data from the CPU or its periph-eral devices. Other devices interact with it, for example,the UART device use a FIFO buﬀer to transmit and receivedata from the CPU.
In this work we are concerned with virtual peripheral de-
vices, hence when we use the term virtual device we are
referring to those which are peripheral.
2.2 Device Modeling
Werepresentagenericdevice Dasa5-tuple, <base[word ],
mmap[bool ],regs[R],bufs[B],deps[D]>, where the value
within each bracket indicates the typeof the property. For
example, baseis the base address of the device of the word
584 
 
CPU 
 
 
peripheral 
device 1  
Memory 
 I/O port-mapped  
Memory 
 
I/O memory-mapped 
peripheral 
device 2 peripheral 
device 3 read data 
write data 
Figure 2: System with peripheral devices
type. The mmapis abooltype that describes if the device
is memory-mapped (true) or port-mapped ( false). The
regs[R] describes the register set in the device, which in-
cludes varieties of registers. The type R(register) is mod-
eled by<oﬀset, size, value >,d e s c r i b i n gt h e oﬀsetfrom the
base address, the sizeof the register (e.g., 16-bit), and the
contentvaluein the register, where 0 ≤value<2size.T h e
bufs[B] describes the set of buﬀers in the device. This prop-
erty is optional as not all devices have buﬀers. The type
B(buﬀer) is modeled as <addr,value>whereaddris the
memory address of the buﬀer and valueis the content in the
buﬀer. The registers and buﬀers are device components that
compose the basic structure of the device under test ( DUT).
The last element deps[D] represents other hardware devices
whose states can be aﬀected by this device (i.e., dependent
devices DD). For example, an interrupt controller can change
the state when a DUTsets its interrupt bit.
We next model the state of a device, which will be later
used in the VDTest’s algorithm. A state Sof a device
Dis denoted as SD=(regs, bufs ), where regs=<r1,
...,ri, ...>andbufs=<b1, ...,bi, ...>. ridenotes
the value contained in the ithregister, and bispeciﬁes the
content in the ithdata buﬀer. There are two types of ac-
tions that can trigger state transitions — readand write
commands issued by the CPU. Last, we model the behav-i o ro fad e v i c eD using a state transition system ( S
D,δD),
whereδD:SD→S/prime
Dis the state-transition function which
changes a device state SD=(regs ,bufs)i n t oan e ws t a t e
S/prime
D=(regs/prime,bufs/prime), driven by the readand/or writeac-
tions. In the case where a DUThas dependent devices (e.g.,
deps/negationslash=null), the transition system is extended to ( SDUT∪
SDD1...∪SDD i∪...,δDUT∪δDD1...∪δDD i∪...), where
DD i∈deps.
2.3 Test Case Modeling
The behavior of a device is often changed by ﬂagging reg-
ister bits (0 and 1) or manipulating buﬀer contents. Thus, atest case is deﬁned as tc={R
0,0...R i,j,B0...B k}, where
Ri,jindicates that the jthbit in the ithregister is ﬂagged,
andBkindicates contents in the kthbuﬀer are changed.
Test cases for testing hardware device can be classiﬁed as
beingstateless orstateful. A stateless test case does not de-
pend on the previous test case; the device state depends onlyon new values written by the test case. On the other hand,
a stateful test case is a sequence of ordered test cases (test
sequence), where the j
thtest case in a sequence depends on
the state resulting from the execution of the ithtest case,
whereihappens before j. For example, a test case ﬂagging
an interrupt clear bit takes eﬀect only when the interruptenable bit is set by its earlier test case. In this case, TC=
{ti,tj}forms a stateful test case. A stateless/stateful test
case execution is guided by a state transition on the DUT.
VDTest models three state transitions:
1.Read(DUT):S→S/prime. This describes the device state
changed from StoS/primeafter reading the registers of
theDUT, where no test inputs are supplied. While it
might seem counter-intuitive, a state change can occurby just reading from a register. Such a transition of-ten applies to the registers with self-clearing bits. Forexample, the modem status register (MSR)i nt h e UART
device is reset each time when it is read after a write
operation.
2.Write(DUT):tc⇒S→S
/prime. This describes the de-
vice state change from StoS/primeafter ﬂipping the bits
speciﬁed in a test case ( tc)i nt h eDUT.
3.WriteS(DUT):tc1∪...tc l⇒S→S/prime. This describes
the device state changed from StoS/primeafter execut-
ing a stateful test case (test sequence) composed of
tc1∪...tc lin the DUT, where li st h el e n g t ho ft h et e s t
sequence.
2.4 Test Oracle Modeling
An expected device state can be used as a test oracle .T h e
most accurate way to obtain this state, is to use a hardware
oracleand to compare the output (i.e., state) of the DUTto
the state of its corresponding physical device. In the case
of non-existing physical devices, an alternative approach isto use a well-developed virtual device from another FSS to
conduct diﬀerential testing [29,30]. It is also possible thatno readily available oracles exist, when a device is newly de-signed for example. VDTest can still work in this context,
presumably the device speciﬁcations can be converted intotest oracles. In any of the above cases, the artifact (i.e., aphysical device, a device model from other FSSs, or a devicespeciﬁcation) that can produce an expected output state is
considered a golden device, denoted by D
oin this paper.
3. VDTEST
Weintroduce VDTest whosearchitectureisshowninFig-
ure 3. The dark gray boxes contain the major components:
astatic analyzer,a test case generator ,a n da n output diﬀer .
VDTest ﬁrst employs static analysis (SA) to generate test
speciﬁcations. The static analyzer takes as input the devicesource code, a pre-deﬁned test template skeleton that spec-
iﬁes device properties (e.g., register address oﬀset), and a
user provided annotation plugin. Next, VDTest runs the
test generator to translate the test speciﬁcation into two
test driver s ,o n ef o rt h eo r a c l ed e v i c e( s h o w na sp a r to ft h e
Golden System ) and the other for the virtual device under
test (shown as part of the Target System ). The test gener-
ator uses combinatorial testing to create test cases.
After test cases are generated, VDTest executes each test
case both on the oracle device (i.e., D
o) and the DUT(i.e.,
Dt), using the three types of state transitions described in
Section 2.3. Last, the output diﬀer detects and shows the
diﬀerences between the output of each test case from Doand
theDUT. A diﬀerence indicates a possible fault in the DUT.
Both the static analyzer and test generator modules are
conﬁgurable so that we can, for example, use a variant ofcombinatorial test generation, or perhaps random test gen-eration instead. We can also completely disable the static
585Annotation 
Plugin 
Test 
Generator 
 Static 
Analyzer Test  
Specification 
Test 
Driver   
(TDg) Test Driver  
(TDt) 
Test  
Cases 
Output  
Differ Virtual 
Device  
  
 
 
 
  
 Target System 
Execution 
Engine Execution 
Observer 
Cases
Constraints Golden System  
  
 
 
 
  
 Golden 
Device 
Execution 
Engine Execution 
Observer 
T
ETest  
Cases 
Output 
Output Test  
Template 
Skeleton 
SiUser 
Figure 3: VDTest architecture
analyzer and generate test speciﬁcations manually in situa-
tions when the source code of DUTis unavailable.
We describe each component of VDTest in more detail
next. Figure 4 (left) provides a code snippet for the pl050
device from QEMU.T h epl050 is a keyboard interface de-
vice that directs communications between the CPU and ex-
ternal keyboards. We will use this example throughout therest of this section.
3.1 Test Speciﬁcation
VDTest utilizes a test speciﬁcation skeleton, which spec-
iﬁes a list of device properties needed to generate test driversand test cases. The speciﬁcation skeleton is deﬁned only
once and generic to all DUTs, so engineers do not need to
manually write it for each DUT.
A speciﬁcation skeleton models the basic properties for
aDUT, including both mandatory and optional properties.
Figure 4 (middle) is an example that presents an overviewof the test speciﬁcation. The device properties are deﬁned as
elements (e.g., name). The value of each mandatory property
(indicated by “
”) and optional property (indicated by “
”) is initially set to nullby default in the skeleton, and can
then be instantiated by the static analysis. The root elementdevicerefers to the DUT. There are three properties deﬁned
for this element, name, base,a n d io.T h e nameproperty
deﬁnes the device’s name, baseproperty speciﬁes the base
address of the DUT, and the property ioindicates the I/O
mapping policy.
The elements at the second level refer to the device com-
ponents, (i.e., registers and data buﬀers). The children el-ements under each component specify the properties of the
component. In this example, the offsetand sizeelements
undertheregistercomponent, andthe address andsizeun-
derthebuﬀercomponentaremandatoryproperties, whereas
the other elements are optional properties. Here, specifying
inputenables VDTest to limit input space to speciﬁc val-
ues rather than an entire range of the input domain (e.g, 1
-2
16). The read-only indicates that the register is a read-
only register. The interrupt indicates that writing to the
register may aﬀect interrupt status, and thereby the state
of interrupt should be checked at the output diﬀer compo-
nent. The dependspeciﬁes components that may have com-
binatorial eﬀects with the current component. The output
indicates that reading/writing the current component mayaﬀect other components, such that the current component
must be tested individually ﬁrst.3.1.1 Static Analysis
The speciﬁcation skeleton can be instantiated by SA, with
the mandatory property values and possible optional prop-erty values analyzed and speciﬁed. The only manual step is
the annotation plugin, which speciﬁes a list of device entry
points. The information is provided by users who are sup-posed to be domain experts. This annotation strategy hasbeen widely adopted by existing static analysis techniques
for device drivers [7,22,24]. Figure 4 (right) illustrates a
sample of annotation plugin for the pl050code snippet.
The SA module ﬁrst identiﬁes the base address of the DUT
in the system-speciﬁc source ﬁles which are speciﬁed by thefileelement under pluginsConfig.System. Here the base
address is found in arm/versatilepb.c inQEMUfor ARM
Versatile Platform. The two properties for the root elementdeviceare instantiated in the same way.
The register entry points are identiﬁed by pluginsCon-
fig.EntryWrReg andpluginsConfig.EntryRdReg annotations,
indicating register write and read points. The function
speciﬁes the name of the entry function for all registers,theaddress indicates the variable used for a register ad-
dress oﬀset, and the valuedeﬁnes the actual data written
to a register. The buﬀer entry points are speciﬁed by thepluginsConfig.EntryBuf annotations. The optional buﬀer
property address is often deﬁned by a data structure, which
is obtained by analyzing disassembling ﬁles.
Toidentifypropertyvaluesforregistersandbuﬀers, VDTest
constructs a system dependency graph [10,19] starting fromthe entry point of registers and buﬀers, as shown in Figure 5.The offsetvalue for a register can be obtained by its deﬁni-
tion. KMIDATA is control dependent on offsetand its value
is0x008deﬁned in the header ﬁle. The sizeof input can be
obtained by examining the size of value(i.e., sizeof(uint64
t)). To obtain input values, VDTest tracks valuepropaga-
tion from the entry point to the constant values assigned. In
Figure 4 (left), the possible input values of kmiData can be
traced to the function ps2_write_keyboard and to the con-
stants 0x00and 0x05(line 16 and line 19). The constants
are mapped into the inputelement in the test speciﬁca-
tion. If the constant value can not be located, the range ofthe input value is [0, 2
n-1], where nis the register bit size.
Since KMICRis control dependent on KMIDATA, the two com-
ponents are considered to have combinatorial eﬀects on the
device state. KMIMRis control dependent on KMICR, but is
written by a value. This implies that change of KMICRmay
aﬀect the output of KMIMR.
5861. #define KMIDATA 0x008
2. ...
3. void pl050_write(addr offset,uint64_t value,...){
4. switch (offset) {5. case KMIDATA:6. ps2_write_keyboard(s->dev, value);7. if (KMICR == 0x3)8. ...9. case KMICR:10. KMIMR = val11 ...
12. }
13. }
14. void ps2_write_keyboard(void *opaque, int val){
15. switch(val) {16. case 0x00:17. ps2_queue(&s->common, KBD_REPLY_ACK);18. break;19. case 0x05:
20. ps2_queue(&s->common, KBD_REPLY_RESEND);
21. break;22. ...23. }
24. }
25. void ps2_queue(void *opaque, int b){
26. ...27. qemu_set_irq(s->irq, raise);
28. }<device name="pl050_keyboard " base=0x10006000
io="memory-mapped ">
<register1>
<offset> 0x008 </offset>
<size> 64 </size>
<input> [0x00, 0x05,...]</input>
<read-only> false </read-only>
<interrupt> true </interrupt>
<depend> [register2,...] </depend>
<affect> [register3,...] </affect>
...
</register1>
<buffer1>
<address> 0x168 </address>
<size> 16 </size>
</buffer1>
</device>pluginsConfig.System = {
name = "pl050_keyboard"
file = "arm/versatilepb.c"
io = "memory-mapped"interrupt = "qemu_set_irq"
}
pluginsConfig.EntryWrReg = {
function = "pl050_write"
address = "offset"value = "value"
}
pluginsConfig.EntryRdReg = {
...
}pluginsConfig.EntryBuf = {
function = "ps2_read_data"
address = ""
value = "PS2Queue.data"
}
Figure 4: Code snippet from gem5 (left), test speciﬁcation (middle), and sample Annotation plugin (right)
Ifaregisterisincludedinthereadentryfunction( plugin-
sConfig.EntryRdReg), but is not included in the write entry
function (pluginsConfig.EntryWrReg), it is a read-only reg-
ister. Since read-only registers are independent from otherregisters, changing them does not aﬀect the device state.Thus, a test case involving interactions between read-only
registers and other components can be eliminated. In thisexample, KMIDATA is writable, so its read-only property is
false.
Writing to a device may lead to changing status of inter-
rupts. To determine whether an entry point is associatedwith the interrupt, we track the data and control ﬂow fromthe entry point of a register/buﬀer to the function that canset interrupt status. In this example, writing to registerKMIDATA causes pl050to raise interrupts (line 27). As such,
interrupt status is considered as an observable point, whichis included in the oracles.
If optional property values cannot be obtained by static
analysis, developers can choose to manually add these val-
ues. For example, the inputproperty may not be always
presentedasconstantsintheprogram, soengineerscanman-
ually select range or speciﬁc values as inputs.
3.2 Test Driver
VDTest converts the test speciﬁcation into a test driver
(TD). VDTest considers two classes of test drivers, which
can be applied to both physical and virtual devices. Theﬁrst class handles applications running on a machine withoperating systems. Since such systems do not allow user-level programs to access hardware or memory, the TD wasimplemented as a kernel module used to communicate with
the device. Speciﬁcally, VDTest maintains an operation
table that maps the elements of the test speciﬁcation (e.g.,
elements in Figure 4 (middle)) into kernel-level system calls.These system calls are used to communicate with hardware
(e.g., ioremap, inb, outb, inl, outl). VDTest then parses
the test speciﬁcation and replaces its elements with the cor-Pl050_write(…) 
offset value 
KMIDATA KMICR KMIMR 0x008 
Ps2_write_keyboard(…) 
value 
0x00 0x05 … @input @address 
ps2_queue 
qemu_set_irq @interrupt 
 R 
 KMIMR
 MIDATA  
 KMICR
MICR
depend affect 
Figure 5: Static analysis
respondingsystemcallsbyqueryingtheoperationtable. Forexample, the attribute iois replaced with ioremap and re-
quest_mem_region to allocate I/O port region. Finally, a
C ﬁle is generated and compiled into the test driver (kernel
module). As an example, we refer to the test speciﬁcation in
Figure 4 (middle), which generates the following C snippet:
...
ret = ioremap(IO_BASE, io_size);
outl(input, IO_BASE + IFLS);...
ThesecondclassofTDhandlesa bare-metal machine (i.e.,
a computer on which an application is running without the
operating system). We distinguish it from the ﬁrst classbecause executing test cases does not require calling kernelAPIs. In this case, VDTest converts the read and write
operations with respect to speciﬁc memory addresses fromthe test speciﬁcation into a TD source ﬁle.
3.3 Testing Approach
VDTests testing approach is guided by the test genera-
torandoutput diﬀer and works both with test cases and
test oracles. We show its main algorithm in Figure 6. Thealgorithm VDTest takes a DUT(D
t) and an oracle device
(Do) as its inputs, and outputs the faults (denoted as F)d e -
tected in the DUT.T h eCheckResult function is called by
theoutput diﬀer component to compare the output states of
DtandDo(lines 51-57). The algorithm begins by checking
the initial state S0ofDtandDoby thereadstate transition
587Algorithm VDTest
1:Inputs: Dt,Do
2:Outputs: F
3:begin
4: (S/prime
0t,S/prime
0o)=Read(S 0t,S0o)
5:F=F∪CheckResult(S/prime
0t,S/prime
0o)
6:St=So=S0o7:foreach read-only register RinDt
8: verify RagainstDo
9:endfor
10:foreach non-read-only register R/primeinDt
11: UnitTest(R/prime)/ * t e s ta tu n i t - l e v e l * /
12:endfor
13: IntegrationTest(R [C]) /*test at integration-level*/
14:end
Function UnitTest
15:Inputs: R/*single registers*/
16:begin
17:TC 1=computeTC(R ,1 )/*strength-1 tests*/
18:foreach test tc∈TC 1
19: (S/prime
t,S/prime
o)=Write(tc ,tc)
20: F=F∪checkResult(S/prime
t,S/prime
o)
21: (S/prime/prime
t,S/prime/prime
o)=Read(S/prime
t,S/prime
o)
22: F=F∪checkResult(S/prime/prime
t,S/prime/prime
o)
23: endif
24:endfor
25:whilet>1a n dt<N
26: TC t=computeTC(R ,t)
27: foreach test tcinTC t
28: (S/prime
t,S/prime
o)=Write(tc ,tc)
29: F=F∪checkResult(S/prime
t,S/prime
o)
30: endfor
31:endwhile
32:end
Function IntegrationTest
33:Inputs: R[C]
34:begin
35:whilet>1a n dt<N
36: TC t=computeTC(R [C],t)
37: foreach test tc∈TC t
38: (S/prime
t,S/prime
o)=Write(tc ,tc)
39: F=F∪CheckResult(S/prime
t,S/prime
o)
40: ifCompare(S/prime
o,S/prime
0o)i sfalse
41: TC/prime
t=TC/prime
t∪tc
42: endif
43: endfor
44:endwhile
45: SEQ = TestSequence(TC/prime
t∪TC p1,l)
46:whileeach test sequence ts∈SEQ
47: (S/prime/prime
t,S/prime/prime
o)=writeS(ts ,ts)
48: F=F∪CheckResult(S/prime/prime
t,S/prime/prime
o)
49:endwhile
50:end
Function CheckResult
51:Inputs: S,S/prime
52:begin
53:ifCompare(S ,S/prime)i strue
54: print“a fault is detected“
55: reset to S0
56:endif
57:end
Figure 6: VDTest algorithm
(lines 4-5). A fault is reported if the initial states of the two
devices are diﬀerent. Next, the states of both DtandDoare
reset (line 6). The reset operation is performed throughout
the testing process upon the completion of a test execution.
This is important as test cases are not independent. A testcase can change the state of the hardware, so two test casesmay yield diﬀerent hardware states, even if both are identi-
cal. After setting the initial state, the algorithm veriﬁes the
read-only registers speciﬁed in the test speciﬁcation (line 8).To do this, for each such register in the oracle device, thealgorithm ﬂips all its bits. If the state remains unchanged,the register is truly read-only, otherwise a fault is reported.
Next, the algorithm begins with unit-level (Phase 1) test-
ing for each non-read-only register (lines 10-12). It thenproceeds to integration-level (Phase 2) testing for the wholedevice taking inputs as a set of registers that have potential
combinatorial eﬀects (line 13). Both phases use a combina-
torial testing approach.
3.4 Unit-level Testing
The Phase 1 testing tests individual components of DUT
(lines 15-32). The components that are not tested are left intheir original state. As such this is analogous to unit testing.
Since the data buﬀer does not contain parameters like regis-
ters, its content is randomly generated at this level. As fortheregisters, each t-bitcombinationinaregisterisﬂippedat
least once, where tis the strength of testing. The test cases
generated by ﬂipping bits are done by ComputeTC. This is
related to fault injection [25], but applied in a diﬀerent con-text. The algorithm begins with testing using strength 1(line 17). A strength-1 test case for register iis deﬁned as
tc
i,1=S0ˆ(1<< j) ,w h e r eˆi st h ee x c l u s i v eO Ro p e r a t i o n
andjis the position of the bit to be ﬂipped while other bits
in the register stay unchanged. This is important becausethe input and observable output points on a hardware deviceare the same, so writing values to any of these points mayprevent errors contained in such points from being observed
at the output. For example, on a register STAT with initial
state is 0x4 (0010), there are four test cases generated forstrength-1 coverage by ﬂipping each single bit one at a time:tc1=( 1,−,−,−),tc2=(−,1,−,−),tc3=(−,−,0,−)a n d
tc4=(−,−,−,1), where“-”indicates the bit is untouched.
Each test case is executed on both D
tandDousing the
writestate transition (line 19). The algorithm then com-
pares the resulting states of DtandDo(line 20). Note that
the change of one device component may aﬀect the states
of other components, so the state comparison is done at
the entire device-level, including interrupt controller if theinterrupt property is set to true in the test speciﬁcation.
For example, ﬂagging bits in IERregister changes the value
ofIIRregister in the same device, as well as the interrupt
controller. After the writestate transition is done, the al-
gorithm performs a readstate transition from S
/primetoS/prime/primeto
check whether DUTcorrectly simulates the self-clearance reg-
isters (line 21). If an observable diﬀerence is found, a fault
is reported (line 22).
VDTest nextenablesincrementalstrength- ttesting, where
tis increased up to N, the maximum strength determined
by the testing budget allocated for phase 1 (lines 25-31).
3.5 Integration-level Testing
The objective of the Phase 2 testing is to test interactions
among registers and data buﬀers (lines 33-50). We focus onregisters that have potential combinatorial eﬀects, denoted
byR[C], which are speciﬁed in the test speciﬁcation under
theregister elementandits dependproperty. LikePhase1,
the data buﬀer values are randomly generated. A strength- t
combinatorial testing at the integration level aims to test
588value combinations of each tregisters in R[C]. Here, the
test cases generated for each register at the Phase 1 are its
values. For example, the STATregister (example used in
Phase 1) has four values. Similar to the Phase 1 algorithm,
the registers not contained in the new combination remainin their original states. Suppose we have a strength-2 testsuite created for a DUTthat contains four registers — R1,
R2, R3, and R4, where R2 is read-only (i.e., no need tobe combined with others) and the dependproperty of R3
is [R1, R4]. Thus, R[C] = {R3, R1, R4}. There are three
combinations to be covered — (R1, R3), (R1, R4) and (R3,
R4) in Phase 2. The algorithm separates out the test cases
that can yield device state change and adds them into TC
/prime
t;
TC/prime
tis further used to generate stateful test cases.
Next, the algorithm generates stateful test cases (test se-
quences) to test consecutive state transitions. Given a test
lengthland stateless test cases from Phase 1 ( TC p1)a n d
Phase 2 ( TC/prime
t), the algorithm iteratively selects ltest cases
to form a stateful test case tsand adds it into SEQ(line
45). Thisisdonebyselectingonlystatelesscasesthatinduce
changesintheﬁrsttwophasesratherthantheexhaustivesetof permutations of all stateless tests (line 40). Note that a
testsequenceisanorderedeventsequence, becausechangingthe order of the state transitions may bring the device intoa diﬀerent state. Suppose R1:0xband (R1:0xa, R3:0xb)
are two test cases generated from Phase 1 and Phase 2 re-spectively, and both yield state changes. The two statelesstest cases form two test sequences with length 2 – R1:0xb
-> (R1:0xa, R3:0xb) and (R1:0xa, R3:0xb) -> R1:0xb.
4. EMPIRICAL STUDY
To assess VDTest we explore three research questions.
RQ1:How eﬀective is VDTest at detecting faults in real-
world virtual devices?
RQ2:Towhatextentdoestheuseoforacledevicein VDTest
aﬀect its eﬀectiveness?
RQ3:What types of faults can VDTest detect?
Theﬁrstresearchquestionevaluateswhetherthe VDTest
approach is cost-eﬀective in terms of fault detection. Thesecond research question lets us further investigate whetherthe use of an oracle device can improve VDTest’s eﬀective-
ness. The third research question allows us to study thefaults and classify them into diﬀerent categories.
4.1 Objects of Analysis
To obtain objects of analysis, we used virtual devices
from three FSSs that are widely used in both industry and
academia — Simics[14], QEMU [5], and GEM5[6].Sim-
icsis a commercial virtual platform. with no source code
provided. In this case, the test speciﬁcations were writ-
ten according to the speciﬁcation manuals by a graduatestudent who has three years experience in embedded sys-
tem development. QEMUandGEM5are two open source
FSSs, containing 30K and 287K non-comment lines of code
in the device codebases, respectively. To evaluate VDTest,
we need both DUTs and oracle devices. We searched virtual
devices that are contained in at least two FSSs with respectto their device models. Eight virtual devices matched thiscriteria. All are based on the ARM architecture. We usedthe eight VDs in GEM5asDUTs, and the VDs in QEMUas
oracle devices, because QEMU is more widely used. Each
DUTis paired with an oracle device, resulting in eight pairsTable 1: Objects of Analysis
R B
E U
VD G F Nd Td NC1 NC2 NR1 NR2
uart8250(s-p) 12219,042 22.32,128 17,432 20,980 2,098,000
uart8250(g-p) 12218,430 20.22,094 17,988 18,982 1,898,200
dec21143(s-p) 41228,456 32.8 33825,954 29,443 2,944,300
pl011(g-q) 14117,344 2074415,864 18,475 1,847,500
pl031(g-q) 8210,298 10.6 360 9,542 12,006 1,200,600
pl050(g-q) 5213,840 16.9 48312,368 14,868 1,486,800
sp804(g-q) 7215,482 16.5 37714,682 16,902 1,690,200
a9scu(g-q) 304,318 3.9 256 4,028 5,088 508,800
pl110(g-q) 13220,562 24.7 72118,058 21,873 2187,300
i8254(g-q) 405,082 2.2 110 3,532 3,902 390,200
mcrtc(g-q) 405,948 2.9 156 7,488 8,560 856,000
of objects. Additionally, given the availability of physicalhardware, we also selected two VDs from Simicsand one
from GEM5asDUTs for which we had hardware, and use
their corresponding physical devices as oracle devices. To-gether there are eleven pairs of analysis objects selected, ass h o w ni nT a b l e1 ,c o l u m n1( V D ) .
Table 1 provides the characteristics of each device and
pair, The notation X-Yin parentheses indicates a pair of
DUT(X) and its oracle device ( Y), where “P” indicates a
physical machine,“S”indicates Simics,“G”indicates gem5,
and “Q” indicates qemu. We give the number of registers
(REG, column 2) and buﬀers (BUF, column 3).
Other columns are described in Section 4.3. UART8250
is an integrated circuit implementing the interface for serialcommunications. DEC21143 is a fast Ethernet LAN con-
trollerprovidingadirectinterfacetothePCIbus. DEC21143
has 19 conﬁguration registers, 18 command and status reg-
isters (CSRs) and 4 CardBus status changed registers; we
consideronlyCSRsbecausetheotherregistersareﬁlledwithpre-deﬁned values. PL011i saU A R Td e v i c ef o rt h eA R M
architecture. PL031is a real time clock device used to pro-
vide a basic alarm function or as a long term base counter.PL050is a Keyboard/Mouse Interface. SP804is a Dual-
Timer module that can generate interrupts on reaching zero.A9SCU is a snoopy control unit that connects processors to
thememory. PL110isaColorLCDControllerthatprovides
control signals to interface directly via a variety of color andmonochrome LCD panels. i8254i sa nI n t e li n t e r v a lt i m e r
device designed to solve the common timing control prob-lems in microcomputer system design. It is used to bring
down the timing frequency to customized levels. mcrtc,
short for mc146818rtc is a Real-Time clock with alarm
and one hundred year calendar, a programmable periodic
interrupt and square-wave generator, and a static RAM.
4.2 Variables and Measures
Independent variable. Our independent variable involves
the testing techniques used. In addition to VDTest,t oa d -
dress RQ1 we enable two testing techniques — a combina-torial interaction testing approach ( CIT) [11] and a random
testing approach ( Random ). Test suites generated by CIT
contain every combination of tinput parameters at least
once in the test suites. In our context, the input parametersin Phase 1 are bits and those in Phase 2 are register andbuﬀers. While CITmay be less expensive than the VDTest
engine (it generates fewer test cases), by maximizing inputparameter coverage in each test case, it neither eliminatesmaskingeﬀectsnorprunesindependentoptions. Inaddition,CITdoes not generate stateful test cases without additional
modeling.
LikeVDTest, CITwasappliedintwophases, togenerate
589tests for both individual registers and for the whole device.
We generated two diﬀerent sets of CITtest cases. For the
ﬁrst (denoted as CIT 1), we used the same testing strengths
as used in the VDTest;t h i sl e t su se x a m i n et h er e l a t i v e
eﬀectiveness of the two approaches on equivalent strengths.For the second set (CIT
2), we used the same testing time
required by VDTest and generated multiple test suites; this
lets us examine the relative eﬀectiveness of the approacheswhen each is given the same amount of testing time.
We also use a random testing technique (Random ).Ran-
domhas been well studied as a baseline approach for tra-
ditional CIT techniques [32]. Test cases are generated byrandomly changing the bit values of registers and buﬀers.Like CIT, we generated two sets of Random test cases.
The ﬁrst (denoted as Random
1) used the same amount of
testing time required by VDTest. The second (denoted as
Random 2) used the number of test cases in the ﬁrst set mul-
tiplied by 100 (to give random a higher chance of success).This lets us examine how the eﬀectiveness of the VDTest
test generator compares to that of a more robust random
testing process.
To address RQ2, we disabled the oracle devices and used
only observable outputs as oracles (i.e., output-based or-
acles). This includes exceptional behavior that results inprogram crashes and error messages (e.g., missing function-
ality, invalid memory access). We compared VDTest with
oracle devices to those with only output-based oracles, de-
noted by VDTest
no.
To address RQ3, we manually classiﬁed fault types. We
classiﬁed the faults into ﬁve categories. The ﬁrst category(C1)areinitializationfaults, inwhichdeviceregisters/buﬀersare initialized with incorrect values. The second category(C2) is when we have an incorrect property of single reg-ister bits. For example, we use C2 when a read-only bit is
writable and a reserved bit changes its status during testing.
The third category (C3) was used when incorrect function-ality of a device component/bit was aﬀected by the compo-nent/bit actually being written. For example, when writing
to register A leads to an incorrect state of register B. The
fourth category (C4) is used for missing functionality – cer-tain features in the device are not implemented. The ﬁfthcategory (C5) are faults that are interaction faults – theyare triggered by at least two device components/bits.
Dependent variables. As dependent variables, we mea-
sure the eﬀectiveness in terms of the fault detection .W e
compare the numbers of uniquefaults (determined by in-
specting source code) detected by test cases for each tech-
nique, VDTest, CIT
1,CIT 2,a n d Random 1andRandom 2.
4.3 Study Operation
We conducted our experiment on a physical X86 machine
(the physical devices used in our study) and three FSSs thatcan simulate both X86 and ARM machines (the virtual de-
vices used in our study). Each X86 physical/virtual ma-
chine runs a preemptive Linux kernel version of Fedora Core2.6.15. The ARM machine is bare-mental, which means itdoesnotcomewithanoperatingsystem. WechoseLinuxbe-
cause it runs on a wide range of known architectures, which
makes it possible to, with small modiﬁcations, extend theVDTest to the hardware on other architectures. In addi-
tion, the popularity and complexity of the x86 and ARMarchitectures make it easier to port VDTest to other ar-
chitectures. On the simulated machine, we used the pro-gramming interfaces that FSSs provide, which allow us todirectly control and observe the hardware states. However,othervirtualplatformssuchas OVPsim [34]canalsobeused
to instantiate the framework. The static analyser is built on
CodeSurfer [10] using the system dependence graph (SDG).We implemented a plugin module that takes as input theAnnotation Plugin andTest Template Skeleton along with
the SDG to generate test speciﬁcations. We enabled both
data dependencies and control dependencies to track data
and control ﬂow from device entry points.
On the FSSs, the execution engine is a built-in module,
which takes the test driver to exercise test cases by writ-
ing and reading registers and hardware buﬀers. We imple-
mented the execution observer as external modules attached
to FSSs. The APIs provided by FSSs allow us to observesystem states at arbitrary point. On the X86 physical ma-chine, the execution engine is the operating system, which
executesthetestdriverasakernelmodule. Weimplemented
theexecution observer by using source code instrumentation
(i.e., printk)t ol o gd e v i c es t a t e si n t oaﬁ l e .
To implement the testing process of VDTest, the maxi-
mum testing strength in the ﬁrst phase is set to 3 and that in
the second phase is left at 2. The length of the test sequencelis set to 2. The insight behind choosing the strengths is the
observation that in most cases the appearance of a failuredepends on the combination of a small number of parameter
values of the DUT[23].
Returning to Table 1, we show the details of our test gen-
eration. Column 5 ( N
d) lists the number of test cases gener-
ated by VDTest. Columns 6 (T d) reports the time required
for running these test cases in minutes. Columns 7 and 8
(NC1andNC2) list the number of test cases generated by
CIT 1andCIT 2respectively. We used the ACTS[ 1 ]t o o lt o
generateCITtestcases. Foreachofthetwo CITtechniques,
we set the same strengths as used in VDTest (i.e., 3 for the
ﬁrst phase and 2 for the second phase). Column 9 (N R1)a n d
Column 10 ( NR2) list the number of test cases generated by
Random 1andRandom 2.
All four techniques involve randomization, therefore we
ran each ten times. In addition, as the buﬀer value is ran-domly set, the eﬀectiveness of the tests depends not only onthe registers but also on the actual content of the buﬀers.As such, for each test (register conﬁguration), we run it tentimes with diﬀerent random buﬀer values. In total, then, we
conducted 100 runs for each of the four techniques.
4.4 Threats to Validity
The primary threat to external validity involves the repre-
sentativeness of our programs. Other programs may exhibit
diﬀerent behaviors and cost-beneﬁt tradeoﬀs. The programs
we investigate are from several popular FSSs and the faults
we aim to detect are real. Another threat is when choosinga virtual device as the oracle, it does not always guaranteethat the oracle device is correct. This threat can be con-
trolled by selecting more robust FSSs as oracles.
The primary threat to internal validity is possible faults in
the implementation of our approach. We controlled for this
threat by extensively testing our tools and verifying theirresults against a smaller program for which we can manually
determine the correct results. A second source of potential
threats involves test oracles used. Any deviations from theoracle devices are reported as failures. It is possible thatthe oracle devices may contain faults that may lead to the
590report of false positives. We controlled for this threat by
using robust FSSs. We also conﬁrmed the deviations with
developers on both sides. It is possible that the deviation
between DUTand physical device does not mean a real fault,
but it is just because that the testing on the physical deviceresult in nondeterministic results due to irregular state ofother hardware components (e.g., cache, scheduling, lack of
memory, etc). We controlled this threat by running each
technique multiple times.
Where construct validity is concerned, numbers of faults
detected are just two variables of interest. Other metrics
such as the cost of manual eﬀort could be valuable.
5. RESULTS
Table 2 reports the results observed (the cumulative faults
detected) in our study; we use this table to address our ﬁrst
two research questions.2The numbers in parentheses indi-
cate the number of faults detected in Phase 2. The numbers
in brackets are the standard deviations across runs.
RQ1: Eﬀectiveness of VDTest. Column 2 of Table 2
reportsthenumberoffaultsdetectedby VDTest. VDTest
detected 64 real faults. We reported the deviations between
gem5andqemuto the gem5developers and the faults were
conﬁrmed although priorities and ﬁxes have not yet been
implemented. We also reported the faults in Simics, but
because we are using an academic version, there is limited
support and were not able to get a conﬁrmation.
Columns 3-4 in Table 2 report the numbers of faults de-
tected by the two sets of CITtechniques (CIT 1,CIT 2). As
the data shows, CIT 1detected 42 faults and CIT 2detected
t h r e em o r ef a u l t s .A l l4 5f a u l t sd e t e c t e db yCIT techniques
were also detected by VDTest. VDTest detected 19 ad-
ditional faults. On eight pairs (out of eleven), VDTest de-
tected more faults than CITs, with improvements ranging
from 33.3% to 100%. These results show that the VDTest
is more eﬀective at detecting faults than traditional CIT.
As shown in Column 5 and Column 6 of Table 2, the two
Random test suites together detected only 34 faults. Com-
pared to VDTest, VDTest w a sm o r ee ﬀ e c t i v ei nn i n eo u t
ofelevensubjectpairs(allexcept a9scu(G-Q) andi8254(g-
q)), with fault detection improvement ranging from 50% to
133.3%. These results show that the VDTest is substan-
tially more eﬀective than Random .
RQ2: Eﬀectiveness of Device-based Oracles. Col-
umn 7 in Table 2 reports the numbers of faults detected
without an internal oracle VDTest no. Of the 64 faults de-
tectedby VDTest, VDTest norevealedonly23faults. Clearly,
device-based oracles substantially improved the fault detec-
tion eﬀectiveness, compared to observable output-based or-
acles. We further examined the data and discovered that all23 faults were related to missing features that led to outputerrors or crashes (i.e., observable faults). For example, when
the test case tried to write to an unimplemented register in
gem5, an error“writing to invalid memory”was generated,
followed by program termination.
RQ3: Fault Categorization. 11 faults (17%) belong to
the initialization ( C1:Initcategory). For example, the ini-
tial values of two registers (RIS andFR)i nt h ePL011 are in-
consistent with those in golden devices. Six out of 64 (10%)
2Artifacts and experimental data are available at http://cs.uky.edu/
˜tyu/research/vdtestfaults, belong to the C2:bits error , these stem from incorrect
implementations of speciﬁc bits (i.e., read-only, write-only
and reserved bits). For example, on UART8250 in Sim-
ics, the values of two read-only registers, IIRand MSR,w e r e
changed unexpectedly by some test cases. The write-only
register THRis expected to return zero, whereas the device
returnsanon-zerovalue. Afewreservedbits3inDEC21143
do not respond when they are written. The writes to the re-served bits changed the register values while these bits werenot modiﬁed in the physical device.
16 faults (25%) belong to the C3: incorrect functionality
category, and are caused by incorrect implementation in thevirtual devices. For example, the writes to the IERregister
of the UART8250 returned diﬀerent values than the phys-ical device did. The reason is that the IIRregister in the
UART8250 did not react to the change of the interrupt bits.
The reason is because the automatic self-clearance mecha-
nism was not implemented for the IIRandLSRregisters. On
the device pl031, when LRregister is written, MRregister is
updated to the same value as LRas, but the golden device
did not update MR.O nd e v i c eDEC21143, in a few cases, the
device failed to enable interrupt bits when the receiving andtransmitting processes were stopped, but the golden devicedid. On device SP804, when the 7
thbit is set to 1, the CD
register was not changed but the golden device did.
Nine faults (14%) are caused by C4:missing functionality.
For example, the FIFO buﬀer in the UART8250 onGEM5
is missing. Three registers ( RSR_ECR, DMACR,a n dLPR)i n
PL011are not implemented.
Finally, there are 22 faults (34%) that we classiﬁed as
interaction faults C5: Interaction category, of which 9 faults
were detected at the register level, and 13 at the devicelevel. For example, on Simics UART8250, the interaction
fault occurred in the FIFO data buﬀer. This buﬀer was not
enabled when the associated registers (IIR and FCR)w e r e
set. A few interesting faults occurred due to interactions
among registers and interrupts. On DEC21143, the device
can raise early interrupts – right after a frame has been put
to the internal transmit FIFO buﬀer. The device failed to
trigger the interrupt on these early interrupts.
Of the 22 interaction faults, ten faults were detected by
the test sequences (i.e., stateful test cases) from VDTest,
and only 5 faults were detected by CIT (also detected by
VDTest). This indicates that exercising consecutive state
transitionscontributestoenhancingtheeﬀectivenessfortest-ing virtual devices. We also observed the diﬀerences whenchanging the order of stateless test cases in a test sequence.
For example, a fault occurred in PL011when executing a
statefultestcase (DR:0x40, IMSC:0x20) -> ICR:0x20.T h e
ﬁrst stateless test case (DR:0x40, IMSC:0x20) changed the
values of the RISand MISregisters and the status of the
interrupt controller PIC. A fault occurred because executing
second stateless test case ICR:0x20 did not revert the bit
values in MISand PICto their original values as the golden
device did. This fault was not detected when the order of
the two stateless test cases were ﬂipped in the sequence. In
fact, interrupts are sensitive to the order of state transitions.
6. DISCUSSION
Inthissection, weexaminetheinﬂuenceofseveraltunable
3reserved bits or registers are reserved for future special use and do
not perform any function.
591Table 2: Fault Detection Eﬀectiveness. Total Cumulative Faults, (Phase 2 Faults) and [standard deviation]
Virtual Device VD CIT 1 CIT 2 Ran 1 Ran 2 VD Test Test no
uart8250(s-p) 8( 3 )[ 0 ] 5( 1 )[ 0 ] 5( 1 )[ 0 ] 4 (1) [0.32] 4 (1) [0.32] 4( 1 )[ 0 ]
uart8250(g-p) 5 (1) [0.32] 3 (0) [0.32] 3 (0) [0.32] 3( 0 )[ 0 ] 3 (0) [0.32] 0( 0 )[ 0 ]
dec21143(s-p) 21 (4) [0.32] 13 (2) [0.32] 15 (2) [0.32] 10 (1) [0.52] 11 (1) [0.52] 12 (2) [0]
pl011(g-q) 7( 2) [ 0 ] 5 (1) [0.48] 5 (1) [0.48] 3( 0 )[ 0 ] 3( 0 )[ 0 ] 3( 0 ) [ 0 ]
pl031(g-q) 2( 1 )[ 0 ] 1( 1 )[ 0 ] 1( 1 )[ 0 ] 1 (0) [0.32] 1( 0 )[ 0 ] 0( 0 ) [ 0 ]
pl050(g-q) 4( 2 ) [ 0 ] 3( 1 )[ 0 ] 3( 1 )[ 0 ] 2 (1) [0.48] 2 (1) [0.32] 0( 0 )[ 0 ]
sp804(g-q) 8( 2 )[ 0 ] 4( 1 )[ 0 ] 5 (1) [0.32] 4 (1) [0.52] 4 (1) [0.48] 0( 0 )[ 0 ]
a9scu(g-q) 1( 0 )[ 0 ] 1( 0 )[ 0 ] 1( 0 )[ 0 ] 1 (0) [0.52] 0 (0) [0.48] 0( 0 )[ 0 ]
pl110(g-q) 4 (2) [0.32] 3 (1) [0.32] 3( 1 )[ 0 ] 2 (1) [0.32] 2 (1) [0.52] 2( 0 ) [ 0 ]
i8254(g-q) 1( 0 )[ 0 ] 1( 0 )[ 0 ] 1 (0) [0.32] 1 (0) [0.48] 1 (0) [0.48] 0( 0 ) [ 0 ]
mcrtc(g-q) 3( 1 )[ 0 ] 3( 1 )[ 0 ] 3( 1 )[ 0 ] 2 (0) [0.32] 2 (0) [0.48] 2( 0 ) [ 0 ]
total 64 (18) [0.12] 42 (9) [0.15] 45 (9) [0.16] 33 (5) [0.34] 34 (5) [0.32] 23 (3) [0]
parameters on the eﬀectiveness of VDTest.
Combinatorial testing strength. We further examined
ourdatatoassesstheeﬀectsofcombinatorialtestingstrengths.
For each DUT, we increased the strength of Phase 1 testing to
4 and that of Phase 2 to 3. With the new testing strengths,
one more interaction fault was revealed at Phase 2 for de-vices of PL110andDEC21143. Speciﬁcally, on PL110,t h e
BGPregister was not updated but the golden device did when
certain values in three registers were set. On DEC21143,
the transmit process did not function correctly when three
registers were set at the same time. An implication of thisdiscovery is that strength matters more to programs with
larger number of registers. Thus, higher testing strength
may be recommended to test such programs.
Test sequence length. To investigate whether the length
of a test sequence can aﬀect the eﬀectiveness, we increased
the length lfrom 2 to 3. No additional faults were detected.
We further examined the data and discovered that all ten
faults detected by the test sequences involve less than threestateless test cases. While additional studies may be needed
to generalize the results, based on our current discovery,
length 2 is suﬃcient.
False identiﬁcation of register properties. There are
two registers in the SimicsFSS – one in each of UART8250
andDEC21143 devices, that were incorrectly speciﬁed as
read-only registers. Because we did not have source code,
we were unable to identify the read-only registers staticallyand during testing we prematurely assumed these registers
were read-only in the oracle device (line 8 in the algorithm
of Figure 6). For example, when multiple registers share thesame I/O port (e.g., UART chip), the read-only register canbecome writable when certain bits are set in other controlregisters. Without exhaustively permuting register bits, it
is impossible to precisely determine read-only registers.
7. RELATED WORK
There has been work on testing embedded systems using
simulators [18,20,35]. However, these techniques take ad-
vantage of FSSs rather than test FSSs. While speciﬁcation-
based testing techniques [17] can be used to ﬁnd cases in
which the virtual device does not behave consistently withwhat has been deﬁned in its hardware speciﬁcations, obtain-ing well documented, reusable, and accurate speciﬁcations
can be diﬃcult. By using the golden oracle, our approach
does not rely on hardware speciﬁcations.
Cong et al. presented a technique to analyze behaviors of
virtual device models [12] and detect the diﬀerences between
virtual and physical devices. In later work, they extend the
technique to a commercial software tool [26,37]. Their ap-proach leverages symbolic execution to explore states of vir-tual devices in qemu. However, their approach is limited
as all symbolic execution engines are, and does not generate
integration test cases or stateful test cases. As shown in ourstudy, a large portion of faults are interaction faults that
can only be detected by combinatorial test cases. In addi-
tion, their technique does not generate test speciﬁcations.Engineers need to annotate and instrument source code foreach virtual device. Ormandy et al. [33] use random testingto detect security vulnerabilities in the implementation of
virtual machines. The author also shows several interesting
examples on security faults caused by incorrect implementa-tion of virtual machines. However, this work does not havea systematic testing approach for virtual devices such as test
drivers, unit and integration testing, and test oracles.
The idea of diﬀerential testing [30] has been used in a
variety of contexts, including ﬂash ﬁle systems [16] and CPU
emulators [29]. Martignoni et al. [28,29] utilize the physicalCPUtoanalyzeCPUemulators/virtualizestoﬁnddefectsin
their implementation, however their work focus on the CPU
virtualization. In the hypervisor-based VMs they do nottake into account the peripheral virtual devices, which areessential to an FSS. Our work instead considers the distinct
characteristics of peripheral devices.
In this paper we use ideas and language from combina-
torial interaction testing [11,23,31] such as the strength of
testing and incremental testing [15]. However, we are per-forming integration testing rather than system testing and
only manipulate tparameters at a time.
8. CONCLUSIONS AND FUTURE WORK
In this paper we presented VDTest, a framework for test-
ing virtual devices within an FSS. VDTest solves two es-
sential aspects of software testing — test case and test or-acle generation while also handling unique characteristics
of real hardware devices. The approach is mostly auto-mated and requires little knowledge of hardware speciﬁca-tions. Our study shows that VDTest is eﬀective in detect-
ing real faults. It found 33% more faults than the best vari-
ant of CIT and doubled fault detection over random testing.
We also found that using stateful testing as well as testingfor interactions improved our results. In future work we willdevelop oracles that detect timing faults, as well as exper-
iment with diﬀerent levels of granularity in our oracles. In
addition, we will perform more extensive experiments.
9. ACKNOWLEDGMENTS
This work was supported in part by NSF grants CCF-
1464032, CCF-1161767 and CNS-1205472.
59210. REFERENCES
[1] ACTS. http://csrc.nist.gov/groups/SNS/acts/.
[2] Buﬀer Overrun on Invalid State Load.
https://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2014-3689.
[3] CVE-2015-3456. https:
//access.redhat.com/security/cve/CVE-2015-3456.
[4] VGA Driver Bug. https://bugzilla.redhat.com/show
bug.cgi?id=CVE-2013-4529.
[5] F. Bellard. QEMU: A fast and portable dynamic
translator. pages 41–41, 2005.
[6] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt,
A. Saidi, A. Basu, J. Hestness, D. R. Hower,T .K r i s h n a ,S .S a r d a s h t i ,R .S e n ,K .S e w e l l ,M. Shoaib, N. Vaish, M. D. Hill, and D. A. Wood. The
gem5 simulator. ACM SIGARCH Computer
Architecture News , 39(2), 2011.
[7] V. Chipounov, V. Kuznetsov, and G. Candea. S2e: A
platform for in-vivo multi-path analysis of software
systems. In International Conference on Architectural
Support for Programming Languages and Operating
Systems, pages 265–278, 2011.
[8] S. Chiu and C. A. Papachristou. A design for
testability scheme with applications to data path
synthesis. In Proceedings of the Design Automation
Conference, pages 271–277, 1991.
[9] J. Chow, B. Pfaﬀ, T. Garﬁnkel, K. Christopher, and
M. Rosenblum. Understanding data lifetime via whole
system simulation. In Proceedings of the Conference
on USENIX Security Symposium , pages 22–22, 2004.
[10] Grammar tech static analysis. http://www.
grammatech.com/research/technologies/codesurfer,
2015.
[11] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C.
Patton. The AETG system: an approach to testingbased on combinatorial design. IEEE Transactions on
Software Engineering , 1997.
[12] K. Cong, F. Xie, and L. Lei. Symbolic execution of
virtual devices. In International Conference on
Quality Software, pages 1–10, 2013.
[13] E. Dumlu, C. Yilmaz, M. B. Cohen, and A. Porter.
Feedback driven adaptive combinatorial testing. InProceedings of the International Symposium onSoftware Testing and Analysis , pages 243–253, 2011.
[14] J. Engblom, D. Aarno, and B. Werner. Full-system
simulation from embedded to high-performancesystems, pages 25–45. 2010.
[15] S. Fouch´ e, M. B. Cohen, and A. Porter. Incremental
covering array failure characterization in largeconﬁguration spaces. In International Symposium on
Software Testing and Analysis (ISSTA) , pages
177–187, July 2009.
[16] A. Groce, G. Holzmann, and R. Joshi. Randomized
diﬀerential testing as a prelude to formal veriﬁcation.
InProceedings of the International Conference on
Software Engineering , pages 621–631, 2007.
[17] A. Hessel, K. G. Larsen, M. Mikucionis, B. Nielsen,
P. Pettersson, and A. Skou. Formal methods and
testing. chapter Testing real-time systems using
UPPAAL, pages 77–117. 2008.
[18] M. Higashi, T. Yamamoto, Y. Hayase, T. Ishio, and
K. Inoue. An eﬀective method to control interrupthandler for data race detection. In Proceedings of the
Workshop on Automation of Software Test,p a g e s
79–86, 2010.
[19] S. Horwitz, T. Reps, and D. Binkley. Interprocedural
slicing using dependence graphs. ACM Transactions
on Programming Languages and Systems , 12(1):26–60,
1990.
[20] M. Iqbal, A. Arcuri, and L. Briand. Environment
modeling and simulation for automated testing of softreal-time embedded software. Software & Systems
Modeling , pages 1–42, 2013.
[21] Android 4.4 emulator does not support orientation
changes. https://code.google.com/p/android/issues/detail?id=61671,2013.
[22] A. Kadav and M. M. Swift. Understanding modern
device drivers. In International Conference on
Architectural Support for Programming Languages andOperating Systems , ASPLOS XVII, pages 87–98, 2012.
[23] D. R. Kuhn, D. R. Wallace, and A. M. Gallo, Jr.
Software fault interactions and implications forsoftware testing. IEEE Transactions on Software
Engineering, 30:418–421.
[24] V. Kuznetsov, V. Chipounov, and G. Candea. Testing
closed-source binary device drivers with ddt. InUSENIX Conference on USENIX Annual TechnicalConference, pages 12–12, 2010.
[25] A. Lanzaro, R. Natella, S. Winter, D. Cotroneo, and
N. Suri. An empirical study of injected versus actual
interface errors. In Pr
 oceedings of the International
Symposium on Software Testing and Analysis ,p a g e s
397–408, 2014.
[26] L. Lei, F. Xie, and K. Cong. Post-silicon conformance
checking with virtual prototypes. In Proceedings of
the Annual Design Automation Conference,p a g e s
29:1–29:6, 2013.
[27] The pl031 model doesn’t seem to raise alarm
interrupts. https:
//bugs.launchpad.net/qemu-linaro/+bug/931940,2012.
[28] L. Martignoni, R. Paleari, G. Fresi Roglia, and
D. Bruschi. Testing system virtual machines. In
Proceedings of the International Symposium on
Software Testing and Analysis , pages 171–182, 2010.
[29] L. Martignoni, R. Paleari, A. Reina, G. F. Roglia, and
D. Bruschi. A methodology for testing CPUemulators. ACM Transactions on Software
Engineering and Methodology , 22:29:1–29:26, 2013.
[30] W. M. McKeeman. Diﬀerential testing for software.
Digital Technical Journal , 10:100–107, 1998.
[31] C. Nie and H. Leung. A survey of combinatorial
testing. ACM Comput. Surv. , 43(2):11, 2011.
[ 3 2 ]C .N i e ,H .W u ,X .N i u ,F . - C .K u o ,H .L e u n g ,a n d
C. J. Colbourn. Combinatorial testing, randomtesting, and adaptive random testing for detecting
interaction triggered failures. Information and
Software Technology , 62:198–213.
[33] T. Ormandy. An Empirical Study into the Security
Exposure to Hosts of Hostile Virtualized
Environments. In Proceeding of the CanSecWest
Applied Security Conference , 2007.
593[34] Open virtual platforms.
http://www.ovpworld.org/technology ovpsim, 2015.
[35] J. Regehr. Random testing of interrupt-driven
software. In Proceedings of the ACM International
Conference on Embedded Software , pages 290–298,
2005.[36] B. L. Titzer, D. K. Lee, and J. Palsberg. Avrora:
Scalable sensor network simulation with precise
timing. In Proceedings of the International
Symposium on Information Processing in Sensor
Networks, pages 477–482, 2005.
[37] Virtual device technologies.
http://virtualdevicetech.com/index.html, 2013.
594