Discovering Relational Specifications
Calvin Smith
University of Wisconsin–Madison
USAGabriel Ferns
University of Wisconsin–Madison
USAAws Albarghouthi
University of Wisconsin–Madison
USA
ABSTRACT
Formal specifications of library functions play a critical role in a
number of program analysis and development tasks. We present
Bach, a technique for discovering likely relational specifications from
data describing input–output behavior of a set of functions com-
prising a library or a program. Relational specifications correlate
different executions of different functions; for instance, commuta-
tivity, transitivity, equivalence of two functions, etc. Bach combines
novel insights from program synthesis and databases to discover a
rich array of specifications. We apply Bach to learn specifications
from data generated for a number of standard libraries. Our experi-
mental evaluation demonstrates Bach’s ability to learn useful and
deep specifications in a small amount of time.
CCS CONCEPTS
•Theory of computation →Logic and verification; Logic and
databases; •Software and its engineering →Dynamic analy-
sis;•Security and privacy →Logic and verification ;
KEYWORDS
Hyperproperties, Datalog, Specification Mining
ACM Reference Format:
Calvin Smith, Gabriel Ferns, and Aws Albarghouthi. 2017. Discovering
Relational Specifications. In Proceedings of 2017 11th Joint Meeting of the
European Software Engineering Conference and the ACM SIGSOFT Symposium
on the Foundations of Software Engineering, Paderborn, Germany, September
4–8, 2017 (ESEC/FSE’17), 11 pages.
https://doi.org/10.1145/3106237.3106279
1 INTRODUCTION
Formal specifications of library functions play a critical role in a
number of settings: In program analysis and verification, specifica-
tions are essential to efficiently and precisely analyzing applications
that use libraries whose code is unavailable or too complex to ana-
lyze. In software engineering, formal specifications can be used to
unambiguously document libraries and apis, as well as aid develop-
ers in program evolution. We address the problem of discovering a
rich class of specifications defining behaviors of a set of functions.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
©2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5105-8/17/09. . . $15.00
https://doi.org/10.1145/3106237.3106279Problem setting Imagine we are given a set of functions f1, . . . , fn
along with a dataset Drepresenting a partial picture of the input–
output behavior of each function fi, perhaps collected through
random testing or instrumentation. We ask the following question:
What can we learn about the set of functions f1, . . . , fn
by simply analyzing the dataset D?
We present a novel and expressive algorithm, called Bach, that is
able to discover likely relational specifications that correlate ( i) differ-
ent executions of a single function or ( ii) different executions within
collections of functions. In other words, Bach learns hyperproper-
ties[6]—this is in contrast to traditional techniques that discover
properties of single executions (invariants) [ 8]. For instance, Bach
may learn the following specifications from some input–output
data, where all variables are implicitly universally quantified:
add (x,y)=z⇔add (y,x)=z (1)
gt(x,y)=t∧gt(y,z)=t⇒gt(x,z)=t (2)
trim (uppercase (x))=y⇔uppercase (trim (x))=y (3)
x>0∧abs (x)=y⇒abs (y)=x (4)
Specification 1 is a bi-implication specifying that addis a commuta-
tive function. Specification 2, on the other hand, is an implication
specifying transitivity of gt(greater than). Bach may also discover
specifications that correlate different functions; e.g., Specification 3
specifies that the composition trim◦uppercase is equivalent to
uppercase◦trim (where trim removes whitespace from a string
and uppercase turns all characters to uppercase). Further, Bach
may discover sophisticated specifications that are refined with ad-
ditional constraints; for instance, Specification 4 specifies that the
function abs(absoulte value) is invertible on positive integers.
Primary challenges There are three primary challenges that arise
in learning relational specifications from a dataset: ( i)What does it
mean for a specification to explain (partial) input–output behavior?
(ii)How do we efficiently handle large amounts of input–output data?
(iii)The space of possible relational specifications is vast, so how do
we efficiently traverse the search space?
We now describe how Bach tackles these challenges. Figure 1
provides a high-level overview of Bach.
Consistency verifier The first piece of the puzzle is defining what
it means for a specification to explain a dataset. We view a specifi-
cation as a first-order formula F, and the given dataset as a partial
interpretation D. We formalize what it means for Dto be a model of
F. Bach employs a notion of evidence to rank specifications. If there
exists any negative evidence —e.g., a counterexample to transitiv-
ity of a function—then the specification is considered inconsistent
with the data and discarded. Otherwise, a specification is consid-
ered more likely to be true depending on a measure of the positive
evidence that is available for it.
The specification consistency verifier checks specifications on a
given dataset. Since we are potentially dealing with thousands of
616
ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Calvin Smith, Gabriel Ferns, and Aws Albarghouthi
input–output examples per function, we could easily incur a pro-
hibitive polynomial blowup when evaluating a specification, e.g.,
evaluating f(д(x),h(y))requires us to evaluate fon the Cartesian
product of the available outputs of дandhin the dataset. To effi-
ciently handle large amounts of examples, we exploit the insight
that we can encode the specification consistency checking problem
as a set of non-recursive, relational Horn clauses (aunion of conjunc-
tive queries , which is a subset of sql), allowing us to delegate the
problem to efficient, scalable databases or Datalog solvers.
Induction and abduction engines The second piece of the puzzle
is how to automatically discover specifications. Our first insight is
that we are searching for a specification (a logical formula) that is
comprised of a setof “programs” (compositions of functions) and
connections between them. Consider the transitivity formula:
f(x,y)=t|       {z       }
p1∧f(y,z)=t|      {z      }
p2⇒f(x,z)=t|       {z       }
p3
Here we have 3 programs, two on the left of the implication ( p1,p2)
and one on the right ( p3). The programs are connected by sharing
their inputs and outputs through quantified variables.
Following this observation, to discover specifications, we utilize
aspecification induction engine that traverses the space of sets of
programs and connections between them. This is analogous to how
an inductive synthesis algorithm searches for a single program
satisfying some property—here, we search for a set of programs.
If the induction engine discovers a specification Sthat is too
strong to hold on the given dataset, the guard abduction engine
asks the question: what do we need to know in order to make the
specification hold? Viewed logically, the abduction engine weakens
the specification Sby qualifying it with some guard G, resulting
inG⇒S. In practice, we exploit the insight that the guard abduc-
tion problem can be reduced to a classification problem by splitting
data into positive and negative sets, those that satisfy the specifica-
tion and those that do not. By alternating between induction and
abduction, Bach is able to learn a rich array of specifications.
Implementation We implemented Bach and applied it to learn
specifications of a range of Python libraries, including a geometry
module and an smt solver’s api. Our results demonstrate Bach’s
ability to discover useful and elegant specifications explaining in-
teractions between functions. While Bach learned a number of
expected specifications, we were pleasantly surprised by some of
the non-obvious specifications it managed to infer (see Section 6).
Most related work The most closely related work to ours is Claessen
et al.’s [ 5], which discovers equational specifications through ran-
dom testing. The class of specifications learnable by Bach is richer
in a number of dimensions: In addition to learning equivalences (as
bi-implications) between pairs of programs, Bach is able to ( i) learn
implications between pairs of programs; ( ii) learn equivalences and
implications over sets of programs, e.g., for properties like transi-
tivity, which correlate executions between more than two copies of
a program; and ( iii) abduce guards on specifications. Further, Bach
operates in a black-box setting: we do not assume access to library
code or a random test generator. Instead, we directly operate on
data, making our approach general, perhaps even beyond software
specifications, e.g., hardware components and networks.
inoutinout...f1fnDatasetinput–output data
Speciﬁcation induction
Guard abductionSpeciﬁcation consistency veriﬁcation}Learning engineThe speciﬁcation induction engineattempts to learn speciﬁcations ofthe formY()ForY)F,whereYandFare conjunctions offunction applications.
guard abduction enginespeciﬁcation consistency veriﬁerThe speciﬁcation consistency veriﬁerchecks whether a given speciﬁcationis consistent with the given data set—that is, the data set does not falsifythe speciﬁcation and that there issomeevidence of it holding.
8x,y.j()y8x,y.j)y8x,y.......Learned speciﬁcationsoutput streamThe output of Bach is a stream ofspeciﬁcations learned from the givendata set.The abduction engine reﬁnes speci-ﬁcations produced by induction en-gine, by augmenting them with ad-ditional constraints, e.g.,Y()FbecomesG)(Y()F).i1o1............i1o1
SG)SSpeciﬁcationReﬁnedspeciﬁcation
+ve/-ve evidencespeciﬁcation induction engine}The input to Bach is a relational datasetDwhere each relation describesa subset of the input–output rela-tion induced by some functionfi.Figure 1: Main components of the Bach algorithm
In comparison with likely invariant discovery techniques, e.g.,
Daikon [ 8], our problem is more general and theoretically more
complex. Checking whether an invariant holds on a dataset rep-
resenting program states requires a linear traversal of the set of
observed states, while ensuring that the invariant holds on each
state. In our relational setting, however, we need to simultaneously
consider multiple executions, which is why we delegate specifica-
tion checking to a database engine. For instance, to falsify a loop
invariant, e.g., x>0, all we need is a single execution where xis
negative; however, to falsify transitivity of a function, we need a
set of 3 executions that together demonstrate that a function is not
transitive. Section 7 makes a detailed comparison with other works.
Summary of contributions To summarize, the primary contri-
butions of this paper are as follows:
–We formally define the relational specification learning prob-
lem as that of discovering likely specifications with respect to a
dataset of input–output behaviors.
–We present Bach, an automated tool that learns relational speci-
fications from input–output data of a library of functions. Bach
utilizes a novel inductive and abductive synthesis technique to
learn a rich class of specifications.
617Discovering Relational Specifications ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
–We show that checking if a specification is consistent with the
given data can be reduced to conjunctive queries, allowing us to
use efficient databases or Datalog solvers to check specifications.
–We describe our implementation of Bach and present a thorough
experimental evaluation on a range of libraries, demonstrating
Bach’s ability to learn a spectrum of useful specifications.
2 ILLUSTRATIVE EXAMPLES
In this section, we demonstrate the operation of Bach through a
set of simple illustrative examples. Each function discussed has an
associated set of observations in Figure 2.
E1: Properties of Addition
Suppose we have a function addthat takes two numbers and returns
their sum, and we have observed the input–output relation of add
in Figure 2, where i1andi2are the inputs and ris the return value.
Induction phase The induction engine of Bach searches the space
of specifications and proposes candidate specifications. Suppose
that the induction phase proposes the following candidate:
add (x,y)=z⇔add (y,x)=z
where x,y,zare implicitly universally quantified variables.
Consistency verification Now, the specification goes to the con-
sistency verifier, which checks if the specification is consistent with
the provided dataset. The consistency verifier asks two questions:
+ve evidence Is there evidence that the specification holds?
-ve evidence Is there evidence that the specification does not hold?
To find positive evidence, the verifier attempts to find three constant
values, a,b, and c, such that add (a,b)=cand add (b,a)=c, as
per the given dataset. The set of all possible values of (a,b,c)is
considered the set of positive evidence. To characterize positive
evidence, we view the set of input–output examples of addas a
ternary relation Radd (x,y,z), where xandyare the inputs and zis
the corresponding output. Now, every possible tuple (a,b,c)that is
evidence that the candidate specification holds is in the relation
Radd (x,y,z)▷ ◁Radd (y,x,z)
where ▷ ◁is the standard joinoperation from relational algebra. In
the rest of the paper, we will use the formalism of Horn clauses (in
non-recursive Datalog) to define positive evidence. Specifically, we
will say that the set of positive evidence Pis defined as follows:
P(X,Y,Z)←Radd (X,Y,Z),Radd (Y,X,Z).
If the relation Pis empty, then there is no positive evidence. Se-
mantically, the above Horn clause defines Pas the smallest relation
such that if (a,b,c)∈Raddand (b,a,c)∈Radd, then (a,b,c)∈P. In
our example, we see that there is at least one tuple in P:(3,4,7).
We now try to find negative evidence, i.e., tuples that falsify the
specification. Since the specification is a bi-implication , we need to
find evidence that holds for one side but not the other. Let us try to
find a tuple that satisfies the left hand side but not the right hand
side. We do this as follows:
N(X,Y,Z)←Radd (X,Y,Z),Radd (Y,X,Z′),Z,Z′
If the relation Nis not empty, then we know that there exists a
tuple (a,b,c)such that add (a,b),add (b,a). In our example, the
relation Nis empty, and therefore Bach infers the specification
stating that addis a commutative function.add
i1i2r
1 2 3
3 4 7
5 6 11
4 3 7
.........gt
i1i2r
1 2 F
2 1 T
2 3 F
3 2 T
3 1 T
.........abs
i1 r
1 1
2 2
-10 10
-3 3
......
concat
i1i2r
a b ab
aϵ a
.........len
i1 r
a 1
ϵ 0
b 1
ab 1
......
Figure 2: Example observed function executions.
Intuitively, our goal is to discover specifications with ( i) no nega-
tive evidence associated with them (we say they are consistent with
the data), and that ( ii) have some positive evidence, which we use
as a proxy to the likelihood of a specification.
E2: Transitivity of Comparison
Consider gt, the function implementing the greater-than operation
for integers with data in Figure 2. The specification induction phase
will propose the following specification:
(gt(x,y)=w∧gt(y,z)=w)⇒gt(x,z)=w
Note that this is an implication; Bach is able to infer both implica-
tions and bi-implications, as we shall describe in detail in Section 4.
The consistency verifier will be able to find positive evidence and
no negative evidence for this specification, thus declaring it a possi-
ble specification for gt. Specifically, it will solve the following two
Horn clauses on the given dataset, and discover that the set Pis
non-empty, while Nis empty.
P(. . . )←Rgt(X,Y,W),Rgt(Y,Z,W),Rgt(X,Z,W)
N(. . . )←Rgt(X,Y,W),Rgt(Y,Z,W),Rgt(X,Z,W′),W′,W
E3: Identity of Absolute Value
Consider the function abs(absolute value) with data in Figure 2.
Induction phase Suppose that the induction phase proposes the
following candidate specification:
abs (x)=x
which can be viewed as the implication true⇒abs (x)=x
Consistency verification The consistency verifier will solve the
following set of Horn clauses to discover positive and negative
evidence, and store them in two relations P(X)andN(X):
P(X)←Rabs (X,X)
N(X)←Rabs (X,X′),X,X′
In this example, both relations will not be empty. Specifically, P=
{1,2, . . .}andN={−10,−3, . . .}.
Guard abduction The guard abduction phase attempts to weaken
the specification by finding a formula Gsuch that
G⇒abs (x)=x
618ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Calvin Smith, Gabriel Ferns, and Aws Albarghouthi
has no negative evidence, and has all (or most) of the positive
evidence. In other words, we can view the guard abduction phase
as solving a classification problem, that of finding a classifier—a
formula G—that labels elements of the set Pwith trueand elements
of the set Nwith false.
To find such a G, we assume we are given a set of predicates
and functions with which we can construct G. For instance, if we
are learning specifications of an apithat operates over integers,
we might instantiate our algorithm with standard operations over
integers, e.g., >, <, +,−. In this example, the abduction phase might
return the formula x⩾0, resulting in the correct specification
x⩾0⇒abs (x)=x
There are many ways to approach such a classification task, e.g.,
using decision-tree learning. In practice, we employ a simple al-
gorithm for learning a conjunction of predicates that separates
positive and negative evidence.
E4: String Operations
Let us now consider an example with multiple functions. Suppose
we are given a dataset (provided in Figure 2) describing the input–
output relations of concat , which concatenates two strings, and
len, which returns the length of a string. Here, ϵis the empty string.
Suppose that the induction phase proposes the specification
len (concat (x,y))=z⇔len (x)=z
Obviously, this is not true. However, using the positive and negative
evidence, the guard abduction phase will discover that the specifi-
cation holds when y=ϵ, resulting in the following specification:
y=ϵ⇒ (len (concat (x,y))=z⇔len (x)=z)
Additionally, Bach will infer other properties of concat and len,
e.g., that the order of concatenation does not change the length of
the resulting string.
len (concat (x,y))=z⇔len (concat (y,x))=z
We have illustrated the operation of Bach through a series of
simple examples. In Sections 3-5, we formalize Bach. In Section 6,
we thoroughly evaluate Bach on a range of Python libraries.
3 SPECIFICATIONS AND EVIDENCE
We now formalize the core definitions needed for our algorithm.
Formulas We assume formulas are over an interpreted theory,
where we have a finite set of uninterpreted functions Σ={f1, . . . , fn},
where each fihas arity ar(fi). AformulaFis of the form
∀V.G⇒ (Ψ⇔Φ)or∀V.G⇒ (Ψ⇒Φ)
where ( i)Vis a set of variables. ( ii)G, the guard , is a formula
over an interpreted set of predicate and function symbols. ( iii)Ψ
(analogously, Φ) is defined asV
iψi, where each ψiis an atom of
the form t=o, where ois a variable in Vandtis anested function
application over the functions Σand variables V. We assume thatF
has no free variables. For simplicity, and w.l.o.g., we assume that all
variables and functions are over the same domain D(e.g., integers).
Example 3.1. Consider the following formula with Σ={f,д}:
∀x,y.x>0|{z}
G⇒*..
,f(д(x))=y|        {z        }
Ψ⇔д(f(x))=y|        {z        }
Φ+//
-Observe that Gis a subformula using an interpreted predicate
over integers, and each of ΨandΦare composed of a single atom
containing nested uninterpreted function applications. ■
Interpretations and models Aninterpretation Igives a definition
to each function f∈Σ; i.e., for each fandi∈Dar(f),Iassigns
a value o∈D such that f(i)=o. For each f∈Σ, we use Ifto
denote the definition of finI:
If={i7→o|f(i)=o}
We will consider the interpretation IasI=S
f∈ΣIf, a union of
sets indexable by functions in Σ.
Given a formulaF, an interpretation Iis amodel ofF, denoted
I|=F, ifIsatisfiesF, using the standard definition of first-order
satisfiability.
Datasets Intuitively, a dataset Din our setting is a partial interpre-
tation , that is, an interpretation that defines each function f∈Σ
on a finite subset of the domain Dar(f). Given a dataset Dand
interpretation I, we say Iis acompletion ofD, denoted D⊆I, if for
allf∈Σ,Df⊆If.
Consistency Our goal is to define what it means for a formula F
to explain a dataset D. We thus define a notion of consistency . A
formulaFisinconsistent with D, denoted D̸|=cF, if
∀I⊇D.I̸|=F
Otherwise, we say that Disconsistent withF, orD|=cF.
In other words, if no matter how we complete a dataset it re-
sults in an interpretation that falsifies F, then we say that Fis
inconsistent with D. Otherwise, we say it is consistent.
Positive and negative evidence Note that while a formula Fcan
be consistent with D, this could happen vacuously. The simplest
case is the empty dataset, which is consistent with any satisfiable
formulaF. Our goal is not only to find a consistent formula, but
one that explains the data well. We therefore define the notions of
positive andnegative evidence. First, we define D-restricted assign-
ments.
Definition 3.2 ( D-restricted assignment). Given quantifier-free
formulaϕwith variables V, aD-restricted assignment σDis a map
from eachv∈Vto a constant that appears in the dataset D. Addi-
tionally, for every term f(i)∈σD(ϕ),i7→o∈Df, for some o∈D,
whereσD(ϕ)isϕwith all variables replaced by their assignment in
σD. ■
Example 3.3. LetDf={17→2}.σD={x7→1}is aD-restricted
assignment to f(x)=x. This is because σD(f(x)=x)isf(1)=1,
and1is in the domain of Df. On the other hand, σ′
D={x7→2}is
not a valid assignment, because fis not defined on 2inDf.
In case of nested terms, e.g., f(д(x)), aD-restricted assignment
needs to set xto a value cin the domain of Dдsuch that Dд(c)is
in the domain of f. ■
Definition 3.4 (Positive evidence). Given DandF, whereFis
of the form∀V.G⇒ (Ψ⇔Φ)or∀V.G⇒ (Ψ⇒Φ), we define
positive evidence as:
pos (D,F)={σD|∀I⊇D.I|=σD(G∧Ψ∧Φ)}
Informally, positive evidence is the set of instantiations of variables
Vthat non-vacuously satisfy the body of F, i.e., G∧Ψ∧Φ.■
619Discovering Relational Specifications ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
Definition 3.5 (Negative evidence). We define negative evidence
as the set of D-restricted assignments such that
neg (D,F)={σD|∀I⊇D.I|=σD(G∧¬ (Ψ⇔Φ))}
or withσD(G∧¬ (Ψ⇒Φ))in the caseFis an implication. Infor-
mally, we can view negative evidence as the set of witnesses to the
fact thatFis inconsistent with D. ■
Example 3.6. Consider formulaF≜∀x,y.f(x)=y⇔д(x)=y
and dataset D, where Df={17→1}andDд={27→3}. Here, there
is no negative evidence. However, there is no positive evidence
either—i.e., there is no witness to the fact that fandдare equiva-
lent. If we update Dдto{27→3,17→1}, then pos (D,F)will be the
singleton set with the assignment that sets xto 1 andyto 1. Alter-
natively, if we update Dдto{27→3,17→2}, then negative evidence
will be the set of two assignments {{x7→1,y7→1},{x7→1,y7→2}}.
■
The following lemma captures the fact that existence of negative
evidence implies that the formula is inconsistent with the dataset.
Lemma 3.7. neg (D,F),∅=⇒D̸|=cF
The proof follows immediately from the definitions of consis-
tency and negative evidence. We will see the utility of this lemma
in our problem specification.
Specification learning problem Given a dataset D,Fis alikely
specification ifpos (D,F),0and neg (D,F)=0. In other words, D
supports the specification F, and by the above lemma, we cannot
show that D̸|=cF.
Anoptimal specification F⋆is a likely specification that maxi-
mizes some function hofDandF. Formally,
F⋆= arg max
Fstneg (D,F)=∅h(D,F)
An immediate choice for his given by h(D,F)=|pos (D,F)|. In-
tuitively, this uses the amount of positive evidence as a proxy for
how wellFexplains D. Of course, hcan also be adjusted to bias
our search further if necessary, e.g., to formulas of smaller size.
4 SPECIFICATION LEARNING ALGORITHM
We are now ready to formalize Bach. The algorithm is shown in
Figure 3 as a set of non-deterministic rules: if the premise above
the horizontal line is true, then the instruction below the line is
executed. At a high-level, the operation of Bach is simple: it ( i) iter-
atively constructs specifications and ( ii) checks whether they are
consistent with the data.
The state maintained by Bach consists of two sets: ( i)J, a set of
conjunctions of atoms, which are used to construct specifications;
e.g., given Ψ,Φ∈J, we can construct ∀V.Ψ⇔Φ. (ii)S, a set of
discovered likely specifications. Both sets grow monotonically.
Search We assume there is a fixed signature Σ, dataset D, and set
of variables V. Recall that an atom is of the form f(t1, . . . , tn)=v,
where each tiis a function application or a variable. The rules add,
expv, and expfconstruct new atoms and conjoin them to formulas
in the setJ. The symbol  is used to denote a wildcard , a hole that
can be filled to complete an atom. A conjunction Φ∈J iscomplete
if it contains no wildcards (denoted cmp in Figure 3).
Induction and abduction The rules ind⇔andabd⇔form the
core of the algorithm. They apply when a specification is learned,which they add to the set of likely specifications S. The analogous
rules ind⇒andabd⇒learn specifications with implications (not
shown in the figure due to similarity).
Let us walk through ind⇔. It picks two conjunctions of complete
atoms, ΨandΦ, from the setJ. It then constructs a formula F=
∀V.Ψ⇔Φ. IfFhas positive but no negative evidence, then it is
added to the set of specifications S. For now, we use posand neg
declaratively; in Section 5, we present an algorithm that constructs
the sets of evidences.
The rule abd⇔applies when ( i)Fhas non-empty sets of neg-
ative and positive evidence, and ( ii) the two sets can be separated .
We assume we have an oracle classify that returns a formula G
that separates the positive and negative evidence. Formally, classify
returns a formula Gwithout uninterpreted functions and with free
variables in V, such that:
(1)∀σ∈neg (D,F).σ(G)is unsatisfiable.
(2)∃X⊆pos (D,F).X,∅∧∀σ∈X.σ(G)is satisfiable.
In other words, Geliminates all negative evidence (point 1), and
maintains some of the positive evidence (point 2). Observe that we
do not need Gto maintain allpositive evidence—we only need a
non-empty set, and that gives us a likely specification.
Soundness We view the soundness of Bach as only adding likely
specifications to the set S. This is maintained by construction
through ( i) the rules ind⇔andabd⇔, (ii) and the definition of
classify , which ensures that all negative evidence is excised and
some positive evidence is preserved.
Rule-application schedule Our presentation of the algorithm as
a set of rules allows us to dictate the search order by varying the
scheduling of rule application. For instance, if we are interested in
learning relations between pairs of programs, we can restrict appli-
cations of the rule add to formulas Φthat are true. This ensures that
there is only a single conjunct on either side of the (bi-)implication.
We must also decide when to apply ind⇔andabd⇔. In practice,
we apply abd⇔right after a failed application of an induction
rule. Specifically, if a failed application of ind⇔results in positive
evidence andnegative evidence, then we apply abd⇔with the hope
that we can find a guard that eliminates the negative evidence.
5 CONSISTENCY VERIFICATION
We now describe our technique for verifying consistency of a for-
mulaFwith respect to a dataset D.
5.1 Background and Overview
The principal idea underlying our technique is that positive and
negative evidence of a formula Fand dataset Dcan be character-
ized using a union of conjunctive queries (ucq) [1]. A conjunctive
query (cq) is a first-order logic query that can model a subset of
database queries written in sql—specifically, a conjunctive query
corresponds to a non-recursive Horn clause. Therefore, a ucqcorre-
sponds to a non-recursive Datalog program—a set of Horn clauses—
whose evaluation results in the positive and negative evidence. Our
formulation of consistency verification as database query evalu-
ation allows us to leverage efficient, highly engineered database
engines and Datalog solvers.
620ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Calvin Smith, Gabriel Ferns, and Aws Albarghouthi
init
J←{ true} S←∅f∈Σ Φ∈J Φ′=Φ∧f( 1. . . ar(f))= 
add
J←J⊕ Φ′
Φ∈J ∈Φv∈V
expv
J←J⊕ Φ[ 7→v]Φ∈J ∈Φf∈Σ
expf
J←J⊕ Φ[ 7→f( 1, . . . , ar(f))]
Ψ,Φ∈cmp (J)F=∀V.Ψ⇔Φ
pos (D,F),∅ neg (D,F)=∅
ind⇔
S←S⊕FΨ,Φ∈cmp (J)F=∀V.Ψ⇔Φ
P=pos (D,F),∅ N=neg (D,F),∅ G=classify (P,N)
abd⇔
S←S⊕∀ V.G⇒ (Ψ⇔Φ)
Notes : (i)S⊕xis short for S∪{x}, (ii){ 1. . . ar(f), }inadd andexpfare fresh, and ( iii) inexpfis assumed to be an argument to a function
Figure 3: Bach’s main algorithm (the rule init is only applied at initialization)
We provide a brief description of Datalog and refer the reader
to Abiteboul et al.’s textbook for a formal presentation of Datalog
semantics [1]. A Horn clause is of the form:
H(X0)←B1(X1), . . . , Bn(Xn).
where H,B1, . . . , Bnare relation symbols; each Xiis a vector of
variables of size equal to the arity of the corresponding relation; the
atom H(X0)is the head of the clause; and the set of atoms{Bi(Xi)}i
is the body of the clause. A Datalog program Cis a set of Horn
clauses. Semantically, a solution of a Datalog program is the least
interpretation of the relations that satisfies all the clauses. For our
purposes, we will enrich our language with inequalities of the form
X,Y, where XandYare variables, which can appear in the bodies
of clauses. We will use underscores, e.g., R(X,_), to denote that the
second argument of Risunbound , i.e., can take any value.
5.2 Detailed Description
Figure 4 describes the algorithm used to construct a set of Horn
clauses encoding the positive/negative evidence of Fwith respect
toD. We assumeFis of the form∀x.Ψ⇔Φ, where Ψ=V
iψi,
Φ=V
jϕj, eachψi(andϕj) is an atom of the form f(t1, . . . , tn)=x,
andxis the vector of universally quantified variables.
We assume that input–output data of each n-ary function fis
stored in a (n+1)-ary relation Rf. The Horn-clause construction
decomposes into three steps:
(1)Ψis encoded in a relation AH(Xa)(andΦinBH(Xb)) by
flattening the atoms;
(2)positive evidence is encoded in the relation P(X)by collect-
ing variable assignments satisfying ΨandΦ; and
(3)negative evidence is encoded in the relation N(X)by satis-
fying Ψ(Φ) while negating Φ(Ψ).
Procedure Encode aggregates all generated Horn clauses into a
single Datalog program C. Note that we expect the formula Fto
be a bi-implication. If Fis of the form∀x.Ψ⇒Φ, we construct
negative evidence by only considering data satisfying Ψand¬Φ.
Given a vector of variables xappearing in a formula F, we
will construct a vector of Datalog variables Xindexed by x∈
x(i.e., x∈ximplies Xx∈X). We use H(X)←S, where Sis
the set of terms{Ri(Xi)}n
i=1, to denote the Horn clause H(X)←
R1(X1),R2(X2), . . . , Rn(Xn).In addition, we use ⊕for adding a
single element to a set. For example, x⊕y,z	=x,y,z	.Encoding specifications Let us walk through the construction of
Horn clauses encoding ΨandΦ. We focus on Ψ, as the encoding for
Φis symmetric. By assumption, the conjunction Ψconsists of atoms
{ta
i=xa
i}, where ta
iis a term of the form f(t1, . . . , tn); we extract
those atoms using the atoms subroutine. In the first for-loop of
Encode , we iterate over every atom and encode it as a Horn clause.
The atom ta
i=xa
iis encoded in the relation Ai(Xa
i,Xa
i), where
Xa
irepresents the inputs to the term ta
iandXa
irepresents the
output (in this case, the value of the formula variable xa
i). Because
each atom can have nested function applications, this procedure is
recursive, and so we make use of the subroutine Flatten . Finally,
we encode Ψas the conjunction of each atom, which translates into
the Horn clause AH(Xa)←(
Ai(Xa
i,Xa
i))n
i=1.
Example 5.1. We now demonstrate Horn clause construction on
a simple example. Consider the following formula:
∀x,y.f(д(x))=y⇔h(x)=y
which states that f◦дis equivalent to h. Encoding the atom
f(д(x))=yrequires three recursive calls to Flatten (once for
x,д(x), and f(д(x))). Working from the inside out, we see Flat-
ten(x)converts the term xto the pair∅,Xx. The call to Flat-
ten(д(x))uses this pair to construct the pair(
Rд(Xx,O))
,O. Fi-
nally, Flatten( f(д(x)))expands on this value to return the pair(
Rf(O,O′),Rд(Xx,O))
,O′. The first for-loop in Encode uses these
results to tie the formula variable yto the output of the term f(д(x))
by constructing the clause:
A1(Xx,Xy)←O′=Xy,Rf(O,O′),Rд(Xx,O).
The right-hand side of ⇔is encoded similarly. ■
Positive evidence Positive evidence consists exactly of those vari-
able assignments that non-trivially satisfy both ΨandΦ. AsΨand
Φare already fully encoded in the relations AH(Xa)andBH(Xb),
this requirement is immediately encodable as the Horn clause
P(X)←AH(Xa),BH(Xb).
Negative evidence Let us now describe the construction of the
Horn clauses encoding negative evidence. Intuitively, negative evi-
dence occurs when we satisfy the left side Ψbut falsify the right
sideΦ. In other words, we want to falsify at least one of the atoms
ϕ1, . . . ,ϕm. We thus construct mclauses, each one encoding assign-
ments that falsify one of the ϕj’s. For instance, assignments that
621Discovering Relational Specifications ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
defFlatten( t: term ):
case tisx, where x∈x:
return∅,Xx
case tisf(t1, . . . , tn):
fori∈1, . . . , n:
Ri,Oi←Flatten( ti)
O=fresh variable
R=Rf(O1, . . . , On,O)⊕Sn
i=1Ri
return R,O
defEncode( Ψ⇔Φ: spec ):
C=∅(
ta
i=xa
i)n
i=1=atoms( Ψ)(
tb
j=xb
j)m
j=1=atoms( Φ)
# encode Ψ
fori∈1, . . . , n:
Ra
i,Oa
i=Flatten( ta
i)
Xa
i=vars( Ra
i)
C=C⊕Ai(Xa
i,Xxa
i)←Ra
i⊕(Oa
i=Xxa
i)
C=C⊕AH(Xa)←(
Ai(Xa
i,Xa
i))n
i=1
# encode Φ
forj∈1, . . . , m:
# ...omitted...
C=C⊕BH(Xb)←(
Bj(Xb
j,Xb
j))m
j=1
# encode positive evidence
C=C⊕P(X)←AH(Xa),BH(Xb)
# encode left negative evidence
forj∈1, . . . , m:
O=fresh variable
bad=(
Bj(Xb
j,O),(O,Xb
j))
C=C⊕N(X)←{AH(Xa)}∪bad∪(
Bi(Xb
i,_))
i,j
# encode right negative evidence
fori∈1, . . . , n:
# ...omitted...
returnC
Figure 4: Encoding formulas as Horn clauses. Omitted for-loops are
symmetric (by exchanging AandB) to those immediately preceding.
We use =for assignment and ←for Datalog implication.
falsifyϕ1are encoded by the clause:
N(X)←AH(Xa),B1(Xb
1,O),O,Xb
1,B2(Xb
2,_), . . . , Bm(Xb
m,_)
The fresh variable Ois used to encode the fact that B1should output
a value that is not equal to Xb
1, thus falsifying the right-hand side
of the bi-implication. Recall that B1encodes a term of the form
f(. . . )=x. Effectively, the above clause states that f(. . . ),x.
Example 5.2. Recall Example 5.1. Negative evidence, as con-
structed by Encode , is written as follows:
N(Xx,Xy)←AH(Xx,Xy),B1(Xx,O),O,Xy
N(Xx,Xy)←BH(Xx,Xy),A1(Xx,O),O,Xy
The first clause encodes the requirement that f(д(x))=yis satis-
fied, but h(x)=yis not; the second clause encodes the opposite
fact. ■Correctness Once we have constructed the Horn clauses, we eval-
uate them on the given dataset to construct the relations PandN.
The following theorem states correctness of the construction:
Theorem 5.3. Given a dataset Dand specificationFof the form
∀x.Ψ⇔Φor∀x.Ψ⇒Φ, let N(X)andP(X)be the relations
computed using the Horn clauses Cfrom Figure 4. Then, P(X)=
pos (D,F)andN(X)=neg (D,F).
Complexity It is important to note that the decision problem of
solving a conjunctive query is np-complete ( combined complexity ).
If the size of the query is fixed and the only variable is the size
of the data, the problem is in ptime (data complexity ) [1]. This is
why database engines are efficient: queries are typically small, but
data is large. These classic results shed light on the difficulty of the
problem of finding positive/negative evidence: One could easily
reduce conjunctive query solving to finding positive evidence in
our setting, thus our consistency verification problem is np-hard.
6 IMPLEMENTATION AND EVALUATION
In this section, we ( i) describe our implementation of Bach, ( ii) present
an exploratory study in which we apply Bach to a number of li-
braries, and ( iii) present an empirical evaluation to investigate the
performance and precision characteristics of Bach.
6.1 Implementation
Bach is implemented in OCaml. It takes as input ( i) a signature of
simply typed functions, ( ii) input–output data for each function,
and ( iii) a set of predicates to compute the guards. Bach uses the
Soufflé Datalog engine [ 14] to compute positive/negative evidence.
Ordering the search The search rules add,expv, and expfare
scheduled to implement a frontier search with respect to the size of
specifications. That is, we visit specifications in order from smallest
to largest. The search rules are augmented with types, ensuring
that only well-typed specifications are explored.
Pruning the search Top-down enumerative synthesis tools typi-
cally have exponential branching of the search space, and Bach is
no exception. To combat this explosion of the search space, Bach
employs a series of search-space pruning techniques: First, Bach
uses a representation of specifications that guarantees that each
explored specification is unique with respect to conjunct reordering
(by commutativity of conjunction) and variable renaming. Second,
whenever Bach proves a specification F=Ψ⇔Φcorrect, it
records one of ΨandΦ(the larger with respect to number and size
of atoms, if it is obvious). During the search, Bach will never apply
the search rules to generate the recorded term.
Specification preference Bach combines induction and abduc-
tion rule application as follows: Given two sets of conjunctions,
Ψ,Φ∈J, it first attempts to learn the bi-implication Ψ⇔Φ, using
theind⇔rule. If ind⇔fails to apply due to existence of negative
evidence, then Bach examines the negative evidence to determine
if it is only one-sided . If so, then Bach learns an implication Ψ⇒Φ,
using the rule ind⇒. If no implication can be learned, Bach resorts
to abduction. Specifically, it solves a number of abduction problems
to learn guards that make the following specifications likely ones:
G1⇒ (Φ⇔Ψ),G2⇒ (Φ⇒Ψ),G3⇒ (Ψ⇒Φ)
622ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Calvin Smith, Gabriel Ferns, and Aws Albarghouthi
Table 1: List of benchmarks; number of functions is in parentheses.
Benchmark Description
list (7) standard list operations, including hd,tl,cons , etc.
matrix (7) matrix operations from Python’s sympy library.
trig (4) trig. functions ( sin,cos, etc.) in Python’s math module.
z3(5) apito Python’s z3library, including satandvalid .
geometry (5) manipulations of shapes from Python’s sympy library.
sets (10) functions from Python’s setmodule.
dict (5) functions from Python’s dict (dictionary) module.
fp199 (4) arithmetic on F199, the finite field of order 199.
strings (9) string operations from Python’s string module.
Bach then picks the specification with the highest positive evidence.
Abduction Guard abduction is done by a simple classification
algorithm that finds a small conjunction of the provided predicates.
Each predicate is instantiated with every combination of variables.
For instance, if the predicate a>bis provided, andFcontains the
variables xandy, abduction will use x>yandy>x. Bach learns
a conjunction that separates the positive and negative evidence of
Fwhile retaining as much positive evidence as possible.
6.2 Exploratory Evaluation
Setup In order to test the efficacy of Bach, we targeted a set of 9
Python libraries (Table 1). Each benchmark consists of ( i) a finite
signature, ( ii) a set of predicates, and ( iii) a dataset of 1000 randomly
sampled executions for each function. These random samples are
generated by uniformly sampling function inputs from a subdomain
of the relevant type and then evaluating the function.
We are interested in examining a variety of specifications. To
cover as much of the search space as possible, we run many inde-
pendent executions of Bach in parallel. Each execution is configured
to search over a different subset of functions from the signature,
or at a different initial depth. This gives a mix of large and small
likely specifications with a variety of combinations of functions.
After letting each execution run for a short amount of time (1-2
minutes), all the resultant likely specifications are collected and
presented together. A partial list of specifications found is provided
in Figure 5. The output of Bach contains many specifications that
are possibly of interest, a few of which are discussed below.
z3specifications z3is a high-performance smtsolver with apis
for many programming languages. The z3benchmark contains
functions from a subset of Python’s z3api. Bach finds the expected
specifications relating and,or, and negthrough DeMorgan’s laws,
distributivity, etc. However, the benchmark also contains valid
andsat, which check for the validity or satisfiability of a formula.
Consequently, Bach discovers the specification
p=true⇒ (valid (x)=p⇒sat (x)=p),
which states that valid formulas are always satisfiable (but not
the opposite). Bach also finds interactions between valid and the
logical connectives. For example,
valid (x)=p∧valid (y)=p⇒valid (and (x,y))=p,
which encodes the fact that validity is preserved by and.
strings specifications Thestrings benchmark contains the typ-
ical set of functions for manipulating strings. Bach finds likely
specifications which encode idempotence properties, such as
lstrip (x)=y⇒lstrip (y)=y,Learned specifications for list
sorting a list preserves length
length (x)=a⇔length (sort (x))=a
hdis the destructor of cons
cons (a,y)=x⇒hd(x)=a
revis an involution
true⇔rev (rev (x))=x
Learned specifications for matrix
identity matrix is upper triangular
identity (x)∧p=true⇒ (upper (x)=p⇔true )
transpose preserves symmetric-ness
symmetric (x)=p⇔symmetric (transpose (x))=p
transpose is an involution
transpose (transpose (x))=x
Learned specifications for trig
sinis the inverse of arcsin
arcsin (z)=x⇒sin (x)=z
sinhas period 2π
∃k.x=2πk+y⇒ (sin (x)=z⇔sin (y)=z)
sinandcosare shifted by π/2
x=y−π/2⇒ (sin (x)=z⇔cos (y)=z)
Learned specifications for strings
a string is a prefix of itself
p=true⇒prefix (x,x)=p
stripping whitespace is idempotent
lstrip (x)=y⇒lstrip (y)=y
palindromes are preserved by reverse
concat (y,reverse (y))=x⇒reverse (x)=x
Learned specifications for z3
validity implies satisfiability
p=true⇒ (valid (x)=p⇒sat (x)=p)
andis commutative
and (x,y)=z⇔and (y,x)=z
andpreserves validity
valid (x)=p∧valid (y)=p⇒valid (and (x,y))=p
Learned specifications for sets
the empty set contains nothing
p=false⇒ (clear (x)=y⇒contains (y,a)=p)
the empty set is contained in every set
p=true⇒ (clear (x)=y⇒subset (y,z)=p)
the subset relation is inclusive
p=true⇒subset (x,x)=p
Learned specifications for geometry
if enclosed shape contains point, then encloser also contains it
b=true⇒ (encl (x,y)=b∧encl_pt (y,p)=b⇒encl_pt (x,p)=b)
rotating a shape by a multiple of 2πresults in same shape
∃k.x=2πk⇒rotate (y,x)=y
Figure 5: A sample of learned specifications on our benchmark suite.
Formulas∃k.x=2πk+y,∃k.x=2πk,x=y−π/2, and p=true,false
are guards.
623Discovering Relational Specifications ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
Table 2: Average correctness results ( T1is the type I error, T2is the type II error, and Size is the number of specifications produced)
10 observations 50 observations 100 observations 500 observations
Benchmark T1 T2 Size T1 T2 Size T1 T2 Size T1T2 Size
ff199 1.8 17.6 4.2 5.6 13.2 12.4 6.4 10.2 16.2 6.4 6.2 20.2
trig 2 19.2 10.8 2 0.8 29.2 0 0 28 0 0 28
dict 5.2 0.2 20 1.4 0 16.4 1 0 16 1 0 16
geometry 18 12 25 9 3.8 24.2 4 1 22 1 0 20
lists 40 17.6 52.4 4.8 0.4 34.4 1.4 0 31.4 0.4 0 30.4
matrices 18.2 11.2 25 15.4 3.2 30.2 7.8 0.6 25.2 5.2 0 19.4
sets 52.8 53.4 79.4 6.2 3.8 82.4 0.4 0 80.4 0 0 80
strings 159.6 234 215.6 85.6 50.8 324.8 22.2 1.6 310.6 0.2 0 290.2
where lstrip (x)removes all whitespace on the left of x, as well as
useful facts like
p=true⇒ (prefix (x,x)=p),
which states that string prefix is a reflexive relation. Amusingly,
Bach also learns that we can construct palindromes by concatenating
a string and its reverse:
concat (y,reverse (y))=x⇒reverse (x)=x.
trig specifications The trig benchmark contains trigonometric
functions (from Python’s math module), which have a rich set of
semantics. Bach has no problem finding many of these properties
as likely specifications. These include the fact that trigonometric
functions are periodic,
∃k.x=2πk+y⇒ (sin (x)=z⇔sin (y)=z),
where∃k.x=2πk+yis provided as a predicate on xandy. Bach
also discovers that sinand arcsin arealmost inverses of each
other:
arcsin (z)=x⇒sin (x)=z.
Note the implication. This is because arcsin is sometimes unde-
fined, and so sinand arcsin are only inverses on the range of
arcsin ([−π/2,π/2]).
geometry specifications Thegeometry benchmark contains func-
tions from sympy ’s [23] (a popular Python library) geometry module,
which supplies operations over 2D shapes on a plane. Bach learns
the following specification:
b=true⇒
(encl (x,y)=b∧encl_pt (y,p)=b⇒encl_pt (x,p)=b)
which states that if ( i) 2D shape xencloses shape y, and (ii)point
pis in shape y, then pis in shape x.
Another insightful property that Bach detects is that rotating a
shape by a multiple of 2πresults in the shape itself:
∃k.x=2πk⇒rotate (y,x)=y
Finding interesting specifications In order to extract the previ-
ous specifications (and those in Figure 5), we rank the output of Bach
in decreasing order by a function h(D,F)=⟨|F|−1,pos (D,F)⟩,
where comparison is done lexicographically and |·|is computed
by counting astnodes. Optimal specifications, in this context, are
those that are small yet have large amounts of positive evidence.
While this ranking function worked to produce a variety of
interesting specifications, it also obscured a few that we expected
to see ranked more highly; associativity of matrix multiplication
did not show up until late in the list. Finding improved ranking
functions for various domains and tasks is an area of future research.6.3 Empirical Evaluation
We now investigate ( i) the scalability of Bach and ( ii) the significance
of Bach’s learned specifications.
Scalability The Horn clauses for negative evidence can, in some
cases, result in a polynomial increase in the size of the relations.
To evaluate the impact of this behavior, we measure the number of
checked specifications per second (i.e., calls to Datalog solver).
Search performance is dependent more on the structure of the
formula and the amount of data than any inherent semantic mean-
ing of the library functions. As such, we test scalability on a rep-
resentative benchmark, in this case ff199 . For k=10,50,100,500,
and1000, we sample kobservations for each function to construct
the dataset Dk. We run Bach for 5 minutes on Dk, and report the
number of checked specifications at every point in time. The results
are presented in Figure 6(a).
The results are as anticipated: with more data, Bach checks less
specifications in the same amount of time. The best-performing
benchmark, k=10, checks≈9x more specifications than the worst-
performing benchmark, k=1000 . Of note are the plateaus in the
k>10results, which indicate Soufflé getting slowed down with
queries with large output. These plateaus suggest that too much
data can overwhelm the external Datalog solver to the point of
losing performance. In the future, we would like to experiment
with approximate queries , where we sample a subset of the data
with the goal of falsifying a query, before trying the full dataset.
Error analysis To evaluate correctness of Bach, we need to deter-
mine how often Bach is wrong. We proceed by fixing a notion of
ground truth and computing type I/II error. Type I error is Bach pre-
senting an incorrect specification (false positive), while type II error
is Bach failing to present a correct specification (false negative).
Accurately determining ground truth for the domains Bach op-
erates on requires enumerating all possible hypotheses and asking
a human expert or an automated verifier to label them as true or
false. This is an infeasible: ( i) there are infinitely many possible
hypotheses, as our formulas are not size-bounded, and ( ii) even if
we bound the size of formulas, there are exponentially many specifi-
cations to consider. Using an automated verifier is a possibility, but
state-of-the-art checking of relational specifications is limited to
simple programs and properties [ 22]. In our setting, we are dealing
with non-trivial, dynamic Python code.
To evaluate error rates, we conducted an experiment where
weapproximate ground truth by running Bach on a large—1000
observed executions per function—dataset per benchmark, and en-
sured Bach checked every formula up to size 7 (measured by ast
leaves). We chose 1000 observations to generate ground truth be-
cause ( i) we observed that the number of discovered specifications
624ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Calvin Smith, Gabriel Ferns, and Aws Albarghouthi
(a) (b) (c)
Figure 6: (a) # of specifications per unit time for ff199 . (b) Error analysis for ff199 (band is 95% confidence interval). (c) Error analysis for sets.
stabilizes at ⩾1000 , and ( ii) at 1000 observations, the number of
type I errors is very low. For point ( ii), we corroborated the ac-
curacy of our approximate ground truth by randomly sampling
30 specifications from those discovered for each benchmark with
1000 observations, and manually classifying them as correct/incor-
rect specifications. Manual inspection showed that ≈100% of the
sampled specifications were correct.
To understand how well Bach performs on varying amounts of
data, for k=10,50,100,500, we randomly sample kobservations
per function to construct a dataset Dkthat is independent from
the approximate ground truth. We run Bach on the same search
space ground truth was generated on (formulas up to size 7) and
compute type I and type II with respect to ground truth. For each k
we repeat this process multiple times, each time randomly sampling
a different dataset. See Table 2 for the results.
For every benchmark, type I/II error tends to decrease as we in-
crease the amount of data. By 100 observations, type II errors have
nearly disappeared for every benchmark except ff199 . By 500 ob-
servations, the number of type I errors has also fallen dramatically.
For example, in strings , which has over 290 average specifications
produced, Bach generates on average 0.2 type I errors.
To provide a clearer picture, for representative best- and worst-
case benchmarks ( ff199 andsets , respectively), we also compute
error rates at each k=25,50,75, . . . , 500. The results are presented
in Figure 6(b,c). Here, false positive error rate is FP/ (TP+FP),
where FPandTPare the number of false and true positives. Nega-
tive error rate is defined symmetrically. By 500 observations, our
worst-performing benchmark, ff199 , has a positive error rate of
≈0.1, meaning we expect Bach to discover an incorrect specification
10% of the time. Conversely, our best-performing benchmark, sets ,
has converged to ground truth by 150 observations.
These results indicate that, for most benchmarks, Bach can
achieve reasonable results before the decrease in performance found
in our scalability experiments becomes prohibitive. The exceptions
to are ff199 andmatrices , which are both numeric benchmarks.
This is due to the difficulty of sampling corner cases (e.g., 0, or
non-invertible matrices) with low numbers of observations.
7 RELATED WORK
Specification inference In the introduction, we compared Bach
with QuickSpec [ 5] and Daikon [ 8]. The work of Henkel et al. [ 11]
for documenting Java container classes is also very closely related
to QuickSpec, and has the same comparison with Bach.A number of specification learning techniques use positive and
negative examples to learn safe preconditions for calling a func-
tion [ 10,18,20,21]. For example, Padhi et al.’s pie[18] and Sankara-
narayanan et al. ’s work [ 20] use input–output data to learn precon-
ditions that ensure a given postcondition is satisfied for a single
function. These approaches synthesize a Boolean formula over a
fixed set of predicates; piecan additionally infer new predicates
by searching over a given grammar. Gehr et al. [ 10] use positive
and negative examples to synthesize a precondition that ensures
that two function calls commute, with the goal of discovering safe
parallel execution contexts. Bach discovers specifications that cor-
relate executions of multiple functions, and does not require anno-
tated positive/negative examples. Bach’s abduction engine solves a
Boolean classification task, like the aforementioned works. Thus, it
can technically be instrumented for inferring safe preconditions.
A number of other techniques aim to learn temporal specifica-
tions (e.g., [ 2–4,9,12,13,15,24,25]), which specify acceptable
sequences of events, e.g., calls to an api.
ILP Inductive logic programming (ilp) [17] is a machine learn-
ing technique that infers Horn clauses to logically classify a set
of positive and negative examples. More than twenty years ago,
Cohen [ 7] used ilpto infer specifications by observing behaviors
of a switching system. Sankaranarayanan et al. used ilp[21] to
infer Horn clauses that explain when exceptions are thrown in var-
ious data structure implementations. ilptechniques, like foil [19]
and Progol [ 16], are optimized for learning Horn clauses and tend
to sacrifice correctness (full classification precision) for scalability.
Bach, on the other hand, does not require annotated examples—
i.e., it is unsupervised—and can, in principle, discover Horn clause
specifications, in addition to arbitrary (bi-)implications.
8 CONCLUSION
We presented Bach, an automated technique for learning rela-
tional specifications from input–output data. Our evaluation demon-
strated Bach’s ability to learn interesting specifications of real-
world libraries. There are many potential uses of Bach, which we
plan on investigating in the future: it could be used to detect ax-
ioms useful for verification of applications using library code, for
lemma discovery, e.g., in interactive theorem provers like Coq, or to
automatically annotate library documentation with specifications.
ACKNOWLEDGEMENTS
This work is supported by NSF awards 1566015, 1652140, and a
Google Faculty Research Award.
625Discovering Relational Specifications ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
REFERENCES
[1]Serge Abiteboul, Richard Hull, and Victor Vianu. 1995. Foundations of databases:
the logical level . Addison-Wesley Longman Publishing Co., Inc.
[2]Mithun Acharya, Tao Xie, Jian Pei, and Jun Xu. 2007. Mining API patterns
as partial orders from source code: from usage scenarios to specifications. In
Proceedings of the the 6th joint meeting of the European software engineering
conference and the ACM SIGSOFT symposium on The foundations of software
engineering . ACM, 25–34.
[3]Rajeev Alur, Pavol Čern `y, Parthasarathy Madhusudan, and Wonhong Nam. 2005.
Synthesis of interface specifications for Java classes. ACM SIGPLAN Notices 40, 1
(2005), 98–109.
[4]Glenn Ammons, Rastislav Bodík, and James R Larus. 2002. Mining specifications.
ACM Sigplan Notices 37, 1 (2002), 4–16.
[5]Koen Claessen, Nicholas Smallbone, and John Hughes. 2010. QuickSpec: Guessing
formal specifications using testing. In International Conference on Tests and Proofs .
Springer, 6–21.
[6]Michael R. Clarkson and Fred B. Schneider. 2010. Hyperproperties. JCS6 (2010).
[7]William W Cohen. 1994. Recovering software specifications with inductive logic
programming. In AAAI , Vol. 94. 1–4.
[8]Michael D Ernst, Jeff H Perkins, Philip J Guo, Stephen McCamant, Carlos Pacheco,
Matthew S Tschantz, and Chen Xiao. 2007. The Daikon system for dynamic
detection of likely invariants. Science of Computer Programming 69, 1 (2007),
35–45.
[9]Mark Gabel and Zhendong Su. 2008. Symbolic mining of temporal specifications.
InProceedings of the 30th international conference on Software engineering . ACM,
51–60.
[10] Timon Gehr, Dimitar Dimitrov, and Martin Vechev. 2015. Learning commuta-
tivity specifications. In International Conference on Computer Aided Verification .
Springer, 307–323.
[11] Johannes Henkel, Christoph Reichenbach, and Amer Diwan. 2007. Discover-
ing documentation for Java container classes. IEEE Transactions on Software
Engineering 33, 8 (2007), 526–543.
[12] Thomas A Henzinger, Ranjit Jhala, and Rupak Majumdar. 2005. Permissive
interfaces. In ACM SIGSOFT Software Engineering Notes , Vol. 30. ACM, 31–40.
[13] Guofei Jiang, Haifeng Chen, Cristian Ungureanu, and Kenji Yoshihira. 2007. Mul-
tiresolution abnormal trace detection using varied-length n-grams and automata.IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and
Reviews) 37, 1 (2007), 86–97.
[14] Herbert Jordan, Bernhard Scholz, and Pavle Subotic. 2016. Soufflé: On Synthesis of
Program Analyzers. In Computer Aided Verification - 28th International Conference,
CAV 2016, Toronto, ON, Canada, July 17-23, 2016, Proceedings, Part II . 422–430.
[15] Claire Le Goues and Westley Weimer. 2009. Specification mining with few false
positives. In International Conference on Tools and Algorithms for the Construction
and Analysis of Systems . Springer, 292–306.
[16] Stephen Muggleton. 1995. Inverse entailment and Progol. New generation com-
puting 13, 3-4 (1995), 245–286.
[17] Stephen Muggleton, Luc De Raedt, David Poole, Ivan Bratko, Peter A. Flach,
Katsumi Inoue, and Ashwin Srinivasan. 2012. ILP turns 20 - Biography and
future challenges. ML86, 1 (2012), 3–23. DOI: http://dx.doi.org/10.1007/
s10994-011-5259-2
[18] Saswat Padhi, Rahul Sharma, and Todd Millstein. 2016. Data-driven precondi-
tion inference with learned features. In Proceedings of the 37th ACM SIGPLAN
Conference on Programming Language Design and Implementation . ACM, 42–56.
[19] J. Ross Quinlan. 1990. Learning logical definitions from relations. Machine
learning 5, 3 (1990), 239–266.
[20] Sriram Sankaranarayanan, Swarat Chaudhuri, Franjo Ivančić, and Aarti Gupta.
2008. Dynamic inference of likely data preconditions over predicates by tree
learning. In Proceedings of the 2008 international symposium on Software testing
and analysis . ACM, 295–306.
[21] Sriram Sankaranarayanan, Franjo Ivanci, and Aarti Gupta. 2008. Mining library
specifications using inductive logic programming. In 2008 ACM/IEEE 30th Inter-
national Conference on Software Engineering . IEEE, 131–140.
[22] Marcelo Sousa and Isil Dillig. 2016. Cartesian Hoare logic for verifying k-safety
properties. In Proceedings of the 37th ACM SIGPLAN Conference on Programming
Language Design and Implementation . ACM, 57–69.
[23] Sympy. 2017. Python library for symbolic mathematics. http://www.sympy.org/
en/index.html. (2017).
[24] Andrzej Wasylkowski and Andreas Zeller. 2011. Mining temporal specifications
from object usage. Automated Software Engineering 18, 3-4 (2011), 263–292.
[25] Jinlin Yang, David Evans, Deepali Bhardwaj, Thirumalesh Bhat, and Manuvir Das.
2006. Perracotta: mining temporal API rules from imperfect traces. In Proceedings
of the 28th international conference on Software engineering . ACM, 282–291.
626