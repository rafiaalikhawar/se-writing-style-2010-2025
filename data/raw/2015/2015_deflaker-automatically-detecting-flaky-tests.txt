DeFlaker: Automatically Detecting Flaky Tests
Jonathan Bell1, Owolabi Legunsen2, Michael Hilton3,
Lamyaa Eloussi2, Tifany Yung2, and Darko Marinov2
1George Mason University, Fairfax, VA, USA
2University of Illinois at Urbana-Champaign, Urbana, IL, USA
3Carnegie Mellon University, Pittsburgh, PA, USA
bellj@gmu.edu,{legunse2,eloussi2,yung4,marinov}@illinois.edu,mhilton@cmu.edu
ABSTRACT
Developers oftenrun tests tocheck thattheir latest changesto a
coderepositorydidnotbreakanypreviouslyworkingfunctionality.
Ideally, any new test failures would indicate regressions caused by
thelatestchanges.However,sometestfailuresmaynotbeduetothe
latestchangesbutduetonon-determinisminthetests,popularly
calledflakytests.Thetypicalwaytodetectflakytestsistorerun
failingtestsrepeatedly.Unfortunately,rerunningfailingtestscan
be costly and can slow down the development cycle.
We present the first extensive evaluation of rerunning failing
testsandproposeanewtechnique,calledDeFlaker,thatdetects
if a test failure is due to a flaky test without rerunning and with
very low runtime overhead. DeFlaker monitors the coverage of
latest code changes and marks as flaky any newly failing test that
did not execute any of the changes. We deployed DeFlaker live,
inthebuildprocessof96JavaprojectsonTravisCI,andfound87
previously unknown flaky tests in 10 of these projects. We also ran
experiments on project histories, where DeFlaker detected 1 ,874
flaky tests from 4 ,846 failures, with a low false alarm rate (1 .5%).
DeFlaker had a higher recall (95 .5% vs. 23%) of confirmed flaky
tests than Maven’s default flaky test detector.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
Software testing, flaky tests, code coverage
ACM Reference Format:
JonathanBell,OwolabiLegunsen, MichaelHilton,LamyaaEloussi,Tifany
Yung, and Darko Marinov. 2018. DeFlaker: Automatically Detecting Flaky
Tests.InProceedingsofICSE’18:40thInternationalConferenceonSoftware
Engineering, Gothenburg, Sweden, May 27-June 3, 2018 (ICSE ’18), 12 pages.
https://doi.org/10.1145/3180155.3180164
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.31801641 INTRODUCTION
Automatedregressiontestingiswidelyusedinmodernsoftware
development. Whenever a developer pushes some changes to a
repository, tests are run to check whether the changes broke some
functionality. Ideally, every new test failure would be due to the
latest changes that the developer made and the developer could
focus on debugging these failures. Unfortunately, some failures are
not due to the latest changes but due to flaky tests. As in previous
work, we define a flaky test as a test that can non-deterministically
pass or fail when run on the same version of the code.
Flaky tests are frequent in most large software, and create prob-
lems in development, as described by many researchers and practi-
tioners [1,25,26,36,37,43,44,48,51,54,56–58,61,62,65,76,80,
82,85].Forexample,accordingtoHerzigandNagappan[ 48],the
Microsoft’sWindowsandDynamicsproductteamsestimatetheir
proportionofflakytestfailurestobeapproximately5%.Similarly,
Pivotal developers estimate that half of their build failures involve
flaky tests [ 49], Labuschagne et al. [ 56] reported that 13% of builds
inaTravisCIdatasetfailedbecauseofflakytests,andLuoetal.[ 61]
reportedthatflakytestsaccountedfor73Kofthe1.6M(4.56%) daily
test failures in the Google TAP system for regression testing.
When a test fails, developers need automated techniques that
canhelpdeterminewhetherthefailureisduetoaflakytestorto
a recently introduced regression [ 48,54]. The most widely-used
technique to identify flaky test failures, Rerun, is to rerun each
failingtestmultipletimesafterwitnessingthefailure:ifsomererun
passes, the test is definitely flaky; but if all reruns fail, the statusis unknown. Rerun is supported by several testing frameworks,e.g., Android [
21], Jenkins [ 52], Maven [ 77], Spring [ 75], and the
Google TAP system [ 42,63]. Developers do notproactively search
forflakytestsasamaintenanceactivity,insteadsimplyusingRerun
to identify that a given test failure is flaky.
Thereislittleempiricalguidancedescribinghowtorerunfailing
tests in order to maximize the likelihood of witnessing the testpass. Reruns might need to be delayed to allow the cause of the
failure(e.g.,anetworkoutage)toberesolved.Flakytestsarenon-
deterministic by definition, so there is no guarantee that rerunning
a flaky test will change its outcome. The performance overhead of
Rerunscaleswiththenumberoffailingtests—foreachfailedtest,
Rerun will rerun it a variable number of times, potentially also
injectingadelaybetweeneachrerun.Rerunning everyfailedtest
is extremely costly when organizations see hundreds to millions of
test failures per day. Even Google, with its vast compute resources,
doesnotrerunall(failing)testsoneverycommit[ 64,87]butreruns
only those suspected to be flaky, and only outside of peak test
execution times [64].
4332018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Bell et al.
<>…<><>…<>Magic
.java:4,5,6,7
List of changes to monitorVersion 
Control 
System<>…<><>…<>public  
class 
Magic {Current version
 of code
<>…<><>…<>public  
class 
Magic {
Previous version of codeDeFlaker Coverage 
Analyzer
AST 
BuilderDiff Tool<>…<><>…<>Magic
.java:4,5
Differential coverage reports,
one per testCoverage 
Instrumenter
Coverage Recorder
Test Outcome 
Monitor/RerunnerDeFlaker Reporter<>…
<><>…
<>TestMa
gic.mag
icTest()
List of likely 
flaky testsCurrent commit: 
Last build:  	Reporting
(after test execution)Coverage Collection 
(during test execution)Differential Coverage Analysis
(before test execution )Inputs OutputDeFlaker
Figure 1: High-level architecture of DeFlaker, with three phases: before, during and after test execution.
We performed an extensive evaluation of Rerun (§3.1) on 5 ,328
test failures in 5 ,966 historical builds of 26 open-source Maven-
basedJavaprojects.TheflakytestdetectorinMaven,whichreruns
each test shortly after it failed and in the same JVM in which it
failed,markedonly23%ofthe5 ,328testfailuresasflaky.Byiso-
latingeachreruninitsownJVM,andfurtherrebootingthebuild
system to clean the state between reruns, we confirmed that in
fact, at least 95% of those failing tests were flaky. Maven likely
doesnotisolatetestrerunsbecauseofthehighcostofcreatinga
process,loadingclasses,andpreparingcleanstateforeachtest;ourprior study found that isolating tests can add a 618% overhead [
25].
Hence, to effectively find flaky tests, Rerun introduces substantial
performance overhead. Developers should ideally be able to know
immediately after a test fails that it is flaky. Even if a developer
suspectsatesttobeflaky,ourgoalistoprovideevidenceforthat
suspicion from the outcome of a single test execution: if the test
fails, is it due to a regression or flakiness?
We propose a new and efficient technique, DeFlaker, that is
complementary to Rerun and often marks failing tests as flaky
immediatelyaftertheirexecution,withoutanyreruns.Recallthatatest isflaky ifit both passesand fails when the codethat is executed
by the test did not change ; moreover, a test failure is newif the
test passed on the previous version of code but fails in the current
version. A straw-man technique to detect flaky tests is to collect
complete statement coverage for each test (of both the test code
and the code under test), intersect coverage with the changes, and
reportasflakynewtestfailuresthatdidnotexecuteanychanged
code.However,collectingfullstatementcoveragecanbeexpensive.
OurkeyinsightinDeFlakeristhatoneneednotcollectcoverage
of theentire codebase . Instead, one can collect only the coverage of
the changed code, which we call differential coverage. Differential
coverage first queries a version-control system (VCS) to detect
codechangessincethelastversion.Itthenanalyzesthecodeand
constructsanabstract-syntaxtreeforeachchangedfiletodetermine
whereinstrumentationneedstobeinsertedtotrackexecutionof
each change. Finally, when tests run, it monitors change execution,
generating an individual change-coverage report for each test.
WepresentourDeFlakertoolthatdetectsflakyteststhrough
lightweightdifferentialcoveragetracking.Ifatestfailsbutdoesnotcoveranychangedcode,DeFlakerreportsthetestasflaky without
requiringanyreruns.Ourevaluationof DeFlakerusesatraditionalexperimental methodology on historical builds on our own servers,
andwealsoproposeanovelmethodologyforevaluatingtestingand
analysis tools on open-source projects in real time and in the exact
same build environments that the projects’ developers use. This
new methodology allowed us to evaluate DeFlaker on complexprojects that we could not easily get to compile and execute in our
own local environments (the traditional methodology). Replicating
software builds in a lab environment can be tricky, when complex
projectsmayincludeahandfulofmanualconfigurationstepsbefore
they can compile. Even then, subtle differences in environment
(e.g., the exact version of Java, and the distinction between an
Oracle JVM and an OpenJDK JVM) can lead to incorrect results.
The marginal human cost of adding a new project to an evaluation
canbeveryhigh.Incontrast,whenprojectsarecurrentlydesignedto be automatically compiled and tested in a standard environment
(e.g., TravisCI), it can be much easier to study more projects.
Ourexperimentsintheliveenvironmentsinvolved93projects
and 614 commits, and we found 87 previously unknown flaky tests
in 10 of these projects. We reported 19 of these newly-detected
flakytests,anddevelopershavealreadyfixed7ofthem.Inordertoperform a larger evaluation (without abusing TravisCI’s resources),
wealsoperformedatraditionalevaluationon26projectsand5
,966
commits running in our own environment, in which DeFlaker
detected4 ,846flakytestfailuresamong5 ,328confirmedflakytests
(95.5%), with a low false positive rate (1 .5%). In comparison, the
currentReruninMavenfoundonly23%ofthesesametestfailures
to be flaky. For projects with very few test failures, DeFlaker can
impose almost no overhead: only collecting coverage in a single
rerunoffailedtests.Forprojectswithtoomanyfailurestorerun,
we found that DeFlaker imposes a low enough overhead (4 .6%) to
be used with every test run, eliminating the need for reruns.
The primary contributions of this paper are:
•New Idea: A generalpurpose, lightweight techniquefor detect-
ing flaky tests by tracking differential coverage.
•Robust Implementation: A description of our open-source
flaky test detector, DeFlaker [28].
•Extensive Evaluation: Experiments with various Rerun ap-
proaches and DeFlaker on 5 ,966 commits of 26 projects, taking
5 CPU-years in aggregate; to our knowledge, this is the first
empirical comparison of different Rerun approaches.
•Novel Methodology: A new research methodology for evalu-
atingtestingtoolsbyshadowingthelivedevelopmentofopen-
source projects in their original build environment.
2 DEFLAKER
DeFlaker detects flaky tests in a three-phase process illustrated inFigure 1. In the first phase, differential coverage analysis, DeFlaker
usesasyntacticdifffromVCSandanASTbuildertoidentifyalistof
changes to track for each program source file. In the second phase,
coveragecollection,DeFlakerinjectsitselfintothetest-execution
process,monitoringthecoverageofeachchangeidentifiedinthe
434
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. DeFlaker: Automatically Detecting Flaky Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
public class SuperOld {
public void magic() {
}
}
public class SuperNew extends SuperOld {
public void magic() {
assert( false);// causes test to fail
}
}
public class Appextends SuperOld /∗SuperNew∗/ {
}
public class TestApp {
@Testpublic void testApp() {
newApp() .magic() ; //unchanged linechanges be havior
}
}
Figure 2: Sample change that challenges a syntactic diff
prior phase. Finally, once tests have finished executing, DeFlaker
analyzes the coverage information and test outcomes to determine
thesetoftestfailuresthatarelikelyflaky.Inprinciple,thesereports
could also be printed immediately, as tests fail, but we report them
at the end of the test run to conform with existing testing APIs.
2.1 Differential Coverage Analysis
DeFlakeranalyzesprogramcodeandversionhistorytodetermine
how to track the impact of changed code, combining syntactic
changeinformationfromaVCS(inourcase,Git),withstructural
informationfromeachprogramsourcefile.DeFlakertracksthe
coverageofchangesto allprogramsourcefiles(includingbothtest
codeandprogramcode).Theoutputofthisphasearelocationsin
the program code in which to add coverage probes that are used at
runtime to determine if a test executes changed code.
Asotherresearcherspointedout,e.g.,inthecontextofregression-
testselection[ 39,66]andchange-impactanalysis[ 23,69,84],using
solely syntactic change information is often insufficientin object-
oriented languages. In other words, it is necessary to monitor even
somesyntacticallyunchangedlinestodeterminethatachangegets
executed.For instance,changesthatmodify theinheritancestruc-
ture of a class or method overloading may cause dynamic dispatch
to occur differently at a call site that itself is unchanged.
Figure2showsanexampletestpronetosomeofthesechallenges:
changingthesupertypeof Appfrom SuperOld toSuperNew would
cause the (unchanged) magicmethod call to refer to a different
implementation,causingthetesttofailinsteadofpass.Similarly,
addingan emptyimplementationofthe magicmethodto the App
class would change the test behavior as well. To handle these
sortsofchanges,priorworkeither(1)performsstaticanalysisto
model changes at a fine granularity or (2) instead, simply tracks
thechangesatacoarsegranularity.Traditionalapproaches—e.g.,
JDiff[23],Chianti[ 69],DejaVOO[ 66],andFaultTracer[ 84]—model
the dynamic dispatch behavior of the JVM to infer the exact se-
manticsof eachchange. Theseapproaches canpreciselyidentify
theimpactofaddingorremovingamethodtoorfromaclass,or
changing a type hierarchy, allowing downstream tools to take into
account any potential changes to method invocation.
Morerecentworkhasshownthatinsomecontextsitcanbemore
cost-effective to model these changes more coarsely, greatly reduc-
ingthecostoftheanalysis,atthecostofsomelossinprecision[ 39].
Thesetoolstrackclassfilecoverageratherthanstatementcoverage:
if a test references a class in any way, then that class is consideredcovered.Thisapproach issafebecauseitwillidentify whenatest
might be impacted by some change, but may flag some changes as
impacting a test, even if they do not. Referring again to Figure 2, a
classcoveragetoolwoulddeclareclasses App,SuperOld ,SuperNew ,
andTestApp as covered by testApp() , which references all these
classes when executed. This coarse granularity introduces impreci-
sion: a change to a statement not covered by testApp() in any of
those classes would be considered covered. However working at a
coarser granularity is fast and does not require the type resolution
of heavyweight static analysis. In our context, working only atcoarser granularity could lead to false negatives: flaky tests may
befalselyconsideredtohaveexecutedchangedcode,preventing
DeFlaker from detecting them.
DeFlakertakesa hybridapproach(similartorecentworkinhy-
bridregressiontestselection[ 83]),benefitingfromthelowupfront
analysis cost of class-level coverage, while often still achieving
the high precision of statement-level coverage. DeFlaker does not
need to find that everychange was covered by a test, but only that
somechangewascoveredbyatest.Therefore,DeFlakercanignore
trackingsomechanges,butinfertheircoveragefromotherfacts.
For each statement that is identified as added, removed or changed
(based on the syntactic diff), DeFlaker classifies it as: (1) safe to
track using statement-level coverage (e.g., changing a statement in
amethodbody);(2)notsafetotrackusingstatement-levelcoverage,
so instead should be tracked using class-level coverage; or (3) does
not need to be tracked. §3.3 evaluates performance improvements
of hybrid coverage over class-level coverage.
DeFlaker identifies new, changed, and removed files directly
from the VCS. DeFlaker assumes that removed classes, if refer-
enced by other classes, will result in changes in those other classes,
which it will track. For each new or changed file, DeFlaker builds
an abstract-syntax tree (AST) representation of the file. For newfiles, DeFlaker marks every type (i.e., class, interface, enum, orannotation)definedinthatfiletobetrackedwithclasscoverage.If the change is to a statement in a method body or to a variable
initializer,DeFlakertracksthatstatementdirectly.Ifthechange
adds/removesanexceptionhandler,DeFlakermarksthechangeas
covered if the first statement enclosed by the try block is executed.
DeFlakerdeferstoclass-leveltrackingoftheenclosingtypeifa
changealterstheparenttypeorlistofinterfacesimplementedby
thattype,orifthechangeremoves(orreducesthevisibilityof)a
method declared in that type — changes that may impact dynamic
dispatchinnon-trivialways.Addinganewmethodtoatypemight
also change dynamic dispatch, but in a predictable way: the new
methodwouldbecalledinsteadofanother.Therefore,DeFlaker
simply tracks the coverage of the first statement in newly added
methods.DeFlakerdoesnottrackchangesthataddorremovefield
definitions, assuming that for such changes to impact some test,
other code must change (e.g., code referencing that field). Changes
to field initializer code are tracked using statement-level coverage.
Limitations .Thekindsofchangeslistedabovearetheonlyones
that DeFlaker tracks. Hence, DeFlaker’s differential coverage is
notcomplete,becauseitmaynotreportthatsomechangedcode
wascoveredevenifitis.Forinstance,DeFlakerignorestheimpact
of various changeson code that uses reflection:adding a field, for
example, will change the array of fields returned by the Class.
getDeclaredFields() method, which some test might call, but
435
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Bell et al.
DeFlaker would not track. Similarly, DeFlaker does not consider
theimpactofchangestoannotationsoftypes,fields,ormethods.
Theselimitationsof DeFlaker’shybridcoverageareanintentional
trade-off,becausethegoalof DeFlakeristodetect,asefficiently
as possible, whether a test executed changed code.
It is important to highlight what problems can arise if a tool
under-approximates code coverage. When DeFlaker misses that a
testexecutessomechangedcode,itonlyresultsina falsepositive,
as DeFlaker reports the test as flaky when it may not be. In other
words, DeFlaker does not miss a flaky test if it misses some cover-
age.Incontrast,aregressiontest-selectiontool[ 34,39,45,66,70]
that under-approximates coverage could miss to run a test and
thuscouldmissabug,resultingina falsenegative.Hence,regres-
sion test-selection tools aim to over-approximate code coverage.
Additionally,becauseDeFlakeronlytrackschangestoJavafiles,
DeFlaker may completely miss some non-Java changes that im-
pact the execution ofa test, e.g., changing aconfiguration file. This
limitationisnotfundamental,andindeed,DeFlakercouldtrack
moredetaileddependenciesbetweentestsandexternalresources
using approaches from regression test-selection work [ 26,32]. Our
experiments showed that considering anychange to a non-Java
files as impacting every test (a safe, but imprecise approach) did
not substantially change DeFlaker’s false-alarm rate.
2.2 Coverage Collection
DeFlaker uses bytecode injection to insert statement and class
coverage-tracking probes as classes are loaded into the JVM dur-ing test execution. Tracking statements is simple: for each line
L
containingastatementthatshouldbetracked,DeFlakerinserts
a probe just before the bytecode instructions for line Lto record
itsexecution.Totrackclasscoverageoftype T,DeFlakerinserts
probes in the static initializer of T, in each constructor of T, and
in each non-private static method of T. This approach is safe: it
alwaysdetectstheuseoftype Tintheabsenceoftest-orderdepen-
dencies [25,26,43,85], similar to the regression-test selection tool
Ekstazi [39]. DeFlaker generates a coverage report that lists each
changedline/classcoveredbyeachtest.DeFlakeralsomonitors
the outcome of each test, ensuring that this information can be
tracked for historical comparisons.
2.3 Reporting
Once the test run finishes, DeFlaker collects outcomes and cov-
erageofeachtestandgeneratesareport.DeFlakermarksatest
as flaky if: (1) it previously passed, now fails, and did not cover
any code that changed; or (2) it failed, was rerun on the same code,
andsubsequentlypassed.Ifanynon-codefileschangedsincethe
lastrun,DeFlakerwarnsthatthefailingtest mightbeflakyand
thatthenon-codechangesmayhavecausedthefailure.Foreach
failing test that covered changed code DeFlaker prints a message
containing each part of the changes that the test covered. This can
help developers debug tests (flaky or not) that failed due to codechanges. Lastly, for changes that were not covered by any test,
DeFlaker prints a warning that changes are not being tested.
2.4 Implementation
WhiletheDeFlakerapproachisgenericandcanapplytonearly
anylanguageortestingframework,weimplementedourtoolfor<extensions >
<extension >
<groupId >org.deflaker</ groupId >
<artifactId >deflaker −maven −extension</ artifactId >
<version >1.4</version >
</extension >
</extensions >
Figure 3: Adding DeFlaker to a Maven build is trivial, con-sisting of just these 7 lines to be inserted to the build file.
Java,theMavenbuildsystem,andtwotestingframeworks(JUnit
and TestNG). DeFlaker is thread-safe and fully supports multi-threaded code. DeFlaker is available under an MIT license on
GitHub[28]withbinariespublishedonMavenCentral.Moreinfor-
mation on DeFlaker is available at http://www.deflaker.org.
DeFlakerconsistsofsixcorecomponents:aMavenbuildexten-
sion, two Maven plugins, a Maven Surefire test execution provider,
a Java instrumentation agent, and a JUnit/TestNG test-outcomelistener. DeFlaker relies only on public, documented, and sup-
portedAPIstointerfacewiththesesystems,allowingittobewidely
compatiblewithmanyversionsofeachtool.DeFlakersupports
arbitrarily complex Maven configurations (with multiple modules,
multipleexecutionsofthe testorverifyphases,andarbitrarycon-
figurationarguments passedto eachphase). We tested DeFlaker
onthemostrecentversions(attimeofwriting)ofMaven(3.2.5,3.39,
3.5.0), the Surefire test execution Maven Plugin (2.18, 2.19, 2.20),
JUnit(4.6–4.12)andTestNG(5.5-5.14.9and6.8-6.11).Despitehav-
ing several components, configuring a project to use DeFlaker is
trivial: a developer need only add the Maven extension to the build
file, as shown in Figure 3. We briefly describe the implementation
and functionality of each DeFlaker component.
DeFlaker Maven Extension injects our coverage analysis
plugin and reporting plugin into each module being built. Maven’s
BuildExtensioninterface[ 22]allowsthird-partytoolstocustomize
how Maven processes build files, including the ability to rewrite
buildfilesastheyareloaded.Ourextensionalsomodifiestheex-
ecution of the Surefire and Failsafe plugins to include DeFlakerinstrumentation agent when tests are run, and the appropriate
DeFlaker test listener (JUnit or TestNG, based on the project).
DeFlaker Coverage Analyzer Maven Plugin performs the
changeanalysisdescribedinSection2.1.WeusetheJGitlibrary[ 53]
to perform the syntactic diff, and the Eclipse JDT library [ 78]t o
construct the ASTs. The coverage analyzer runs just once even for
amulti-modulebuild,detectingwhatchangesneedtobetracked
for every file in the repository, regardless of what module that file
isin.Outofthebox,Surefiresupportsrerunningfailedtestswithin
thesameJVM, just after they fail. While evaluating DeFlaker, we
foundthatMaven’sReruntechniquedidnotevencomecloseto
flaggingasmanytestfailuresasflakyasDeFlaker(§3.1).Hence,tohelpwithdebuggingandverifyingthataflakyreportrepresentsanactualflakytest,DeFlakerincludesa
SurefireProvider tore-run
eachfailedtestina freshJVM,whichwefoundcanhelpshowmore
testsasflakythanMaven’sRerun.Thisoptionalfeatureisexposed
through the system property deflaker.rerunFlakiesCount . An-
other property, deflaker.delayedCoverage , instructs DeFlaker
to collect coverage only during reruns and not while running tests
normally. The latter configuration has effectively no overhead for
coverage tracking (unless some test fails).
436
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. DeFlaker: Automatically Detecting Flaky Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
DeFlaker Java Instrumentation Agent rewrites all class-
filesastheyareloadedintotheJVMduringtestexecution.Ituses
cachedcoverageanalysisresultstodeterminewhatcoverageprobes
to insert in which classes. DeFlaker Test Listener interfaces
with the testing framework (JUnit or TestNG) and records when
tests finish, collecting coverage results and resetting internal coun-
ters.DeFlakerReportingMavenPlugin executesaftertheunit
testandintegrationtestphases,collectingcoverageandtestout-
comes, and handling all functionality outlined in Section 2.3.
3 EVALUATION
We performed an extensive evaluation of DeFlaker and Rerun
with the goal of answering several key research questions:
RQ1:HowmanytestfailuresdodifferentRerunapproachesmark
as flaky?
RQ2:How many test failures does DeFlaker mark as flaky?
RQ3:Given that DeFlaker only detects flaky tests that do not
coverchangedcode,canitdetectrealflakytests?DoesDe-
Flaker’shybridcoveragefindmoreflakesthanclass-level
coverage?
RQ4:What is the overhead of running DeFlaker for alltests?
Experimental Environments Toanswerthesequestions,we
createdtwoevaluationenvironments.The historical environment
simulates how DeFlaker could have worked had developers used
it in the past, and allows us to run experiments on thousands of
commits. The historical environment was constructed on our own
servers,andrequiredustomanuallyidentifyprojectsthatwecould
successfullybuildandtest.Thisevaluationmethodologyiscommoninrecentstudiesonregressiontesting[
26,32,39,41,59,60,68,73].
Incontrast,the liveenvironmentrequires noconfigurationortuning
ofindividualprojectstogetthemtosuccessfullycompileandrun
their tests. Instead, the live environment evaluates DeFlaker on a
numberofopen-sourceprojectsintheexactsameenvironmentthat
thedevelopersuseandwhiledevelopmentishappeningonthose
projects,i.e.,exactlyasadeveloperwoulduseDeFlaker.Thelatter
environment allowed us to evaluate DeFlaker on more projects
thanwouldhavebeenpossiblehadwemanuallyconfiguredeach
project to build and test. This environment is a newevaluation
methodology, to our knowledge, not used before this paper. Previ-
ous studies havelooked at builds on Travis [ 29,56] buthave not
deployed tools to instrument those builds.
Our historical evaluation environment consisted of 250 Amazon
EC2 “c3.large” instances, each with 2 Intel Xeon E5-2680 v2 vCPUs,
3.75GB of RAM. Each instance ran Ubuntu 14.04 “trusty”, and used
eitherJava1 .7.0_45or1 .8.0_131forbuilds(depending ontheage
of the code being compiled and executed), with Maven 3.3.9. ToanswerourRQs,wecompletedatotalof47
,748Mavenbuildsin
thishistoricalenvironment,taking over5CPU-years torun.Thelive
environmentranontheTravisCIplatform.Eachprojectincludedin
the live evaluation was mirrored and configured to use DeFlaker;
when developers pushed commits to their own repositories and
triggered builds, our forks synced and triggered a build of our
repository as well. Because we selected projects that used TravisCI
for their own builds, our build environment was identical to theirs.
The live environment completed 614 builds between May 12, 2017
andAug10,2017.Notethatweconductedfarfewerbuildsintheliveenvironmentduetoresourceconstraints:wedidnotwantto
abusethefreeTravisCIservice.Allofourprojectswereconfigured
on TravisCI under the same organization, allowing TravisCI to
easily throttle and restrict our builds if needed.
Project Selection. To identify projects to include in the live
environment,weselectedprojectsfromGitHubthatuseTravisCI,
have Java as the primary language, and use Maven, yielding a total
of 96 projects. A complete list of all of these projects appears in
the following tables, and additional details (including the revisions
used) are available on our supplemental website [27].
For our historical environment, we selected projects that we
knewhadatleastoneflakytest(andarangeofcommitsthathad
the flaky behavior), which served as a ground truth, allowing us to
calculate how often DeFlaker would find bonafide flaky tests. We
selected96knownflakytestsfrom26projects,consistingof:(1)4
projects (achilles, checkstyle, jackrabbit-oak, and togglz) from our
live experiment in which we identified 5 flaky tests along with the
starting and ending commits exhibiting that flaky behavior; and
(2)22otheropen-sourceprojectsinwhichtheprojectdevelopers
previously found and fixed a total of 91 flaky tests.
The 22 projects with previously fixed flaky tests contain 17
projects thatwe obtainedfrom queryingGitHub forterms related
toflakytests(“intermit”or“flak”)and5projectsthatweselected
from a prior study on flaky tests [ 61]. From the results of querying
GitHub, we selected 81 tests (from 17 projects) where we could
confirm by manual inspection that the commit message actually
fixedaflakytest,andforwhichwecouldstillruntheseoldtests.
We also consulted a prior study of flaky tests [ 61], selecting from it
10 flaky tests (from 5 projects) that we could build and run. These
flakytestsandprojectscomefromvariouscategoriesdomains(net-
working, databases, etc.), offering a relatively diverse sample of
flakytests.Foreachofthese96flakytests,weidentifiedaprecise
set of commits where the test was flaky by manually investigating
the cause of flakiness. To limit the time for our experiments, we
run DeFlaker only on 500 randomly selected commits from those
ranges. The test could have failed for any build of those commits.
3.1 RQ1: Finding Flaky Tests through Rerun
While we knew of 96 flaky tests in the projects for the historical
environment,weexpectedthatthereweremore,too,andwanted
to study each approach’s ability to find these. The methodology
for this study was to run each of the 5 ,966 builds, rerunning tests
as they failed. At first, we considered only the Rerun approach
implemented byMaven’s Surefire test runner[ 77], whichreruns
each failed test in the sameJVM in which it originally failed. We
were surprised to find that this approach only resulted in 23% of
test failures eventually passing (hence, marked as flaky) even if we
allowedforuptofivereruns! The strategybywhichtestisrerun
mattersgreatly:makeapoorchoice,andthetestwillcontinueto
failforthesamereasonthatitfailedthefirsttime.Hence,toachieve
a better oracle of test flakiness, and to understand how to best use
reruns to detect flaky tests, we experimented with the followingstrategies,rerunning failedtests:(1)
Surefire:up tofive timesin
the same JVM in which the test ran (Maven’s Rerun); then, if itstill did not pass (2)
Fork: up to five times, with each execution
inaclean,newJVM;then,ifitstilldidnotpass(3) Reboot:upto
437
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Bell et al.
Table 1: Number of flaky tests found by re-running 5,966 builds of 26 open-source projects. We consider only new test failures,
where a test passed on the previous commit, and report flakes reported by each phase of our Rerun strategies. DeFlaker found more flaky
tests than the Surefire or Fork rerun strategies: only the very costly Reboot strategy found more flaky tests than DeFlaker.
DeFlaker labeled as: Test Methods
in ProjectConfirmed flaky by
Rerun strategy Flaky Not Flaky
Project #SHAs Total FailingTotal
New
Failures Surefire +Fork ++Reboot Confirmed Unconf. Confirmed Unconf.
achilles 227 337 77 242 13 14 230 225 4 58
ambari 500 896 7 75 52 71 74 74 0 01
assertj-core 29 6 ,261 2 3 22 2 20 01
checkstyle 500 1 ,787 1 1 00 0 00 01
cloudera.oryx 332 275 23 29 55 5 52 0 04
commons-exec 70 89 2 22 22 22 22 21 0 10
dropwizard 298 428 1 60 60 60 60 55 0 50
hadoop 298 2 ,361 365 1 ,081 284 865 1 ,054 1,028 25 26 2
handlebars 27 712 7 9 37 7 62 10
hbase 127 431 106 406 62 242 390 383 12 74
hector 159 142 12 87 07 4 7 9 72 4 74
httpcore 34 712 2 2 22 2 10 10
jackrabbit-oak 500 4 ,035 26 34 10 33 34 32 0 20
jimfs 164 628 7 21 21 21 21 15 0 60
logback 50 964 11 18 18 18 18 18 0 00
ninja 317 307 37 122 37 77 110 94 2 16 10
okhttp 500 1 ,778 129 333 296 305 310 231 0 79 23
oozie 113 1 ,025 1 ,065 2 ,246 42 2 ,032 2 ,244 2,234 0 10 2
orbit 227 86 9 86 84 85 85 73 0 12 1
oryx 212 200 38 46 14 14 46 14 0 32 0
spring-boot 111 2 ,002 67 140 73 107 135 135 3 02
tachyon 500 470 4 5 35 5 50 00
togglz 140 227 21 28 51 4 2 8 28 0 00
undertow 7 340 0 0 00 0 00 00
wro4j 306 1 ,160 114 217 39 96 99 80 8 19 110
zxing 218 415 2 15 15 15 15 15 0 00
26 Total 5,966 28 ,068 2 ,135 5 ,3281,162 4 ,186 5 ,075 4,846 80 229 173
fivetimes,runninga mvn clean betweentestsandrebootingthe
machine between runs.
Table 1 shows the results of this study, including the number of
testfailuresconfirmedasflakybyeachRerunstrategy.Overall,we
observed2 ,135teststhatexhibitedsomepotentiallyflakybehavior,
havingnewfailures (passing on one commit, then failing on the
followingcommit).Collectively,thesetestshadatotalof5 ,328new
failures, with 1 ,162 detected by the Surefire (same JVM) reruns,
4,186 detected by the Surefire strategy orthe Fork strategy, and
5,075 detected by the Surefire, Fork, orReboot strategy. This result
is striking: the existing flaky test detector in Maven only identified
23% of the flaky failures identified by all three strategies (including
the heavyweight Reboot strategy)!
Itwouldbedifficulttofairlystatethe costofthesevariousreruns,
as the cost of rerunning a test varies with many factors (how long
the test took to run the first time, how much shared state it might
needtosetup,etc.).Ifalltestsfail,thenthecostofrerunningthemalloncewouldbeatleastthecostofrunningthetestsuitethefirsttime.
Even when (re)running fewer tests, any Rerun strategy aside fromMaven’s Rerun will incur the high computational cost of isolatingtests in their own JVM as documented by prior work [
25], or more
if employing stronger isolation similar to our Reboot strategy [ 65].
Table 2 summarizes Rerun results by strategy, including the
numberofrerunsneededtowitnesstheflake.Fromtheseresults,we
may conclude that only one rerun is needed for each kind of rerun:
first run a failing test in the same JVM once, and if it fails, run in a
newJVMonce,andifstillfails,runafterareboot.Performingmore
runs of the same kind increases the cost but does not substantially
increase the chance to obtain a pass. In other words, changing the
kindofrerunismorelikelytohelpthanjustincreasingthenumber
of reruns, and various testing frameworks [ 21,42,52,63,75,77]
thatsupportrerunsandofferdefaultssuchas3,5,or10rerunsofthesamekindshouldratheroffernewkindsofreruns.DeFlaker
allowsMavenuserstoautomaticallyhavetestsreruninanewJVM.
3.2 RQ2: Finding Flaky Tests with DeFlaker
We evaluated DeFlaker’s efficacy in marking test failures as flaky
on the same test executions as in the previous section. That is,
438
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. DeFlaker: Automatically Detecting Flaky Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 2: Number of reruns required to confirm the flakes
fromTable1,andthepercentofflakesconfirmedbyreruns
ateachtieralsoconfirmedbyDeFlakerwithoutanyrerunsrequired.
If a flake was confirmed, we stopped rerunning it; we
executed the three rerun strategies in the order listed.
# Reruns to Find Flaky
Strategy 1 2 3 4 5 Total% Also Found
byDeFlaker
Same JVM 994 90 38 24 16 1 ,162(22 .9%) 87.6%
New JVM 2 ,913 32 32 19 28 3 ,024(59 .6%) 98.4%
Reboot 889 0 0 0 0 889(17 .5%) 95.8%
All 5,075(100 .0%) 95.5%
when running tests in our historical environment, we also ran
DeFlaker.Weusedthererunsasanoracleforwhetheratestfailure
wastrulyflaky,whichallowedustoidentifyDeFlaker-reported
flaky failures that were confirmed as failures versus those that
remain unconfirmed. Note that we may over-estimate the number
of false alarms for DeFlaker because the test could still be flaky
if investigated further. Table 1 reports these results. In summary,
DeFlaker reported 4 ,846 failures as flaky (95 .5% of confirmed
flakes) with a very low false alarm rate, just 1 .5%. These reports
representatotalof1 ,874flakytests.DeFlakerfindssignificantly
moreflakytestfailuresthantheSurefirestrategyandslightlymore
than those found using the Fork JVM strategy.
Giventhatmostoftheflakes(77%)couldn’tbeconfirmedthrough
a simple rerun in the same JVM, we believe that DeFlaker is even
morevaluabletodevelopers,asitcanprovideimmediate,trusted
feedback with significantly less delay. Table 2 shows what percent
of flakesdetected byeach rerun techniquewerealso detectedby
DeFlaker. Of those 3 ,024 flaky tests detected only after rerunning
tests in a new JVM, DeFlaker accurately marked 98 .4% of them as
flaky,suggestingthatthedeveloperscouldhavedetectedtheseflaky
tests without paying the cost to rerun these tests. Most compelling
is that for the 889 failures that required the most expensive reruns
to confirm as flaky tests – running a mvn clean between tests and
rebootingthemachinebetweenruns–DeFlakerdetected95 .8%
of these flaky test runs without any expensive rerun.
Overall,basedontheseresults,wefindDeFlakertobeacost-
beneficialapproachtorun beforeortogetherwith Rerun,andwe
alsosuggestapotentiallyoptimalwaytorunRerun.Forprojects
thathavelotsoffailingtests,DeFlakercanberunonallthetestsin
the entire test suite, because DeFlaker immediately detects many
flaky tests without needing anyrerun. For projects that have a few
failingtests,DeFlakercanberunonlywhenrerunningfailedtests
in a new JVM; if the tests still fail but do not execute any changed
code, then reruns can stop without costly reboots.
To evaluate DeFlaker on a wider set of projects, and to find
previously unknown flaky tests, we performed experiments in the
live environment, the results of which are summarized in Table3. Of the 96 open-source Java projects that we shadowed, rela-
tively few were actively developed (Labuschagne et al. reported
asimilarfinding[ 56]),havingmorethanahandfulofbuildsover
the three-month time period. Of particular note are the projects
where DeFlaker was only run on a handful of builds (i.e., achilles,Table 3: Results from live environment, showing only
projectsthathadtestsfailafterpreviouslypassing. Showing
thetotalfailures,andforDeFlakerflakereports: Confirmedflakes,
Reportsofflakessenttodevelopers, Addressedflakesbydeveloper.
Flake Reports
Project Tests # SHAsNew
FailsC R A Issue Links
achilles 573 5 2 2 2 2 [2, 3]
checkstyle 26 ,935 96 1 1 1 1 [4]
geoserver 4 ,919 60 39 39 1 0 [5]
jackrabbit-oak 9 ,788 99 5 5 2 1 [6, 7]
jmeter-plugins 1 ,571 19 1 1 1 0 [8]
killbill 14 ,827 31 26 26 1 0 [9]
nutz 1,117 87 1 1 1 1 [10]
presto 4 ,554 203 11 11 7 0 [11–16]
quickml 98 2 2 2 2 0 [17, 18]
togglz 748 12 3 3 2 2 [19, 20]
10 Total 65,130 614 91 91 19 7
quickml),yetstillidentifiedflakytests.Intotal,only10projectshad
atleast onetest thatDeFlakerdetectedas acandidate flake(that
had passed in the previous commit, then failed in the current com-
mit). We found a total of 91 failures that were potential flakes, and
confirmedthattheywereallflakesbyrepeatedlyrerunningthemon
our local machines: if they eventually passed given the same code,
and no other changes, we declared them true flakes. Although we
performed far fewer builds in the live environment (constrained by
the resources provided by TravisCI), DeFlaker actually identified
moreflakytestfailuresper-buildintheliveenvironment(546)than
in the historical environment (91 /614). We found no false positives
in thisstudy (butDeFlaker can havefalse positives, asdiscussed
previously). Unfortunately, we cannot comment on the efficacy of
individual rerun strategies here, as we began collecting this datain the field before automating the three-strategy rerun approach
described in the previous section.
Out of the 91 previously unknown flaky tests that DeFlaker
detected,wereported19todevelopers(onetestintogglzandthreetestsinprestohadbeenpreviouslydetectedasflakybythedevelop-
ers).Ofthe19reports,7havebeenaddressed,mostbyremovingor
reducingflakiness,butonebyremoving/ignoringthetest(which
is why we use the term “addressed” rather than “fixed,” because
removing a test is not a fix for that test, but it was a fix for the test
suiteasthedevelopersfoundmorevalueinremovingthistestthan
keeping it and having to deal with its flakiness). In several cases,
we found that flaky tests previously believed to have been fixed by
developers were still flaky, due to the same or different causes [ 16].
Wereceivedseveralverypositiveresponses,suchas“Thankyou
very muchfor your help withthis. I just committeda fix. Looking
forwardtoseemoregreenbuildsnow.”[ 19]and“@flakycov,thanks
alot!”[4].Allremainingreportsarestillopenatthetimeofwriting,
with theexception ofone geoserverissue: thosedevelopers could
not reproduce the failure and were not interested in debugging
it[5].Incaseswhereaprojecthadmultipleflakytests,webegan
by opening an issue on just one of the flaky tests, and did not open
moreissuesifthedevelopersdidn’trespond.Wereportedtwoflaky
tests in presto with similar root causes in one issue.
439
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Bell et al.
3.3 RQ3: Coverage tracking for flake detection
WeevaluatedtheefficacyofusingDeFlaker’shybridstatement-
and class-level coverage in comparison to simple class-level cov-
erageusedbyastate-of-the-artregressiontestselectiontool[ 39].
Because DeFlaker uses differential coverageto judge when a fail-
ing test is flaky, it can only identify test failures as flaky when the
testdoesnotcoveranyrecentlychangedcode.Ifthetestdoescover
changed code, DeFlaker cannot determine if the failure is flaky,
orifthetestwillalwaysfail.Todeterminethelimitationsofthis
coverage-based approach, we evaluate how often flaky tests cover
changed code, for two kinds of coverage.
Toanswerthisquestion,weneedtoknowpreciselytheversions
of code in which each flaky test was present and could have flaked
on.Toperformthisanalysison all4,846flakyfailureswouldnot
be possible, as debugging flaky tests is a very time consuming
task. Hence, to answer this question, we focused on the 96 known
flaky tests that we had manually identified in the 5 ,966 commits
ofcodethatwebuiltandtested.Wecalculatedthepercentageof
test runs on which DeFlaker would have marked the test as flaky
ifitfailed,regardlessofwhetherthetestactuallyfailedornotin
our experiments on that given commit. Because the test was flaky
for each of these commits, it could have failed in any of those runs.
Also,weassumethat,onaverage,atestrunthatcoversnochanged
code when the test passes would also cover no changed code when
the test fails (although the exact coverage between a test failure
and a test pass does differ, at least for one branch or exception that
determinesthetestoutcome).Table4showstheresultsperproject,
forflakydetectionusingbothDeFlaker’shybridstatement/class
coverage and only class coverage [ 39]. DeFlaker identified that
theflakytestdidnotcoveranycodechange90%ofthetime.The
improvementsoverclasscoverageareroughly11percentagepoints,showing the importance of DeFlaker’s precise analysis, compared
with a regression test selection tool that uses class coverage [39].
3.4 RQ4: Performance
RecallthatDeFlakercanberunononlyfailingtests(potentially
when rerun) or on all tests. In some cases, it might be preferred
that DeFlaker runs for every test, regardless of the outcome, to
potentiallyeliminatetheneedforreruns.Inthiscase,itisimportant
to understand what slowdown DeFlaker might add to the testing
process. We measured the relative performance of running tests
without any coverage tracking tool, with DeFlaker, and with the
most recent versions of three most popular code-coverage tools for
Java: JaCoCo (0.7.9), Cobertura (2.1 with Maven plugin 2.7), and
Clover (OpenClover 4.2.0) [ 24]. We also used a recent research tool
thattracksclasscoverage:Ekstazi(version4.6.1)[ 39].Weconfig-
uredeachofthesetoolsfollowingtheinstructionsprovidedontheirrespectiveinstallationpages.Forourowntool,DeFlaker,weused
version1.4(availableonMavenCentralandourwebsite [ 27]).By
default,JaCoCoandCoberturaonlycreateasinglecoveragereport
containing the overall results for the entire test suite, which is not
usefulforidentifyingifaparticulartestexecutedanychangedcode.
Hence, we measured also the performance of using these two tools
togeneratecoveragereportsforeachtestbyconnectingourtest
listenertoeachtoolandtriggeringacoveragedumpandresetafter
each test.Table 4: DeFlaker’s efficacy finding 96 known flaky tests
across 5,966 different commits of 26 open-source projects,comparing its novel hybrid statement/class coverage withonly class coverage.
% of Runs Flaky by:
ProjectKnown
Flaky Tests # SHAs Hybrid Cov Class Cov
achilles 1 227 77% 71%
ambari 1 500 100% 100%
assertj-core 1 29 97% 83%
checkstyle 1 500 100% 98%
cloudera.oryx 1 332 95% 94%
commons-exec 1 70 94% 69%
dropwizard 1 298 83% 79%
hadoop 3 298 92% 85%
handlebars 7 27 89% 78%
hbase 3 127 86% 72%
hector 1 159 100% 96%
httpcore 1 34 82% 82%
jackrabbit-oak 1 500 75% 66%
jimfs 1 164 54% 24%
logback 11 50 91% 83%
ninja 2 317 90% 80%
okhttp 33 500 91% 81%
oozie 1 113 91% 75%
orbit 1 227 81% 74%
oryx 3 212 100% 100%
spring-boot 5 111 98% 98%
tachyon 7 500 77% 58%
togglz 3 140 97% 94%
undertow 3 7 75% 17%
wro4j 1 306 68% 56%
zxing 2 218 98% 98%
26 Total 96 5 ,966 88% 77%
Weconsideredrunningthisexperimentonall26projectsstudied
inRQ1andRQ2,butfoundthatseveralwerenotwellamenable
toperformancemeasurements,withaveryhighvarianceintest-
execution times, even without any coverage tool (e.g., cloudera.
oryx,whichtookonaverage241secondswithastandarddeviation
of 110 seconds), and filtered out 9 such projects. For each of theremaining 17 projects, we executed its build (using
mvn verify )
on the 10 consecutive versions (most recent as of August 10, 2017)
with each tool, and repeated this process 15 times to reduce noise.
Consideringalloftheexecutions,weperformedatotalof21 ,600
builds over a total of approximately 445 CPU-days (counting the
total time needed to checkout and build each project).
Wecollectedtheoutcomesofalltestsfromeachexecutionand
markedthetoolas‘n/a’ifthetoolcausederrors.Table5showsthe
results,includingthebaselinetimespentrunningeachtestsuite
withoutcollectinganycoverage(plusorminusasinglestandard
deviation), and the relative overhead of each tool. To provide a fair
comparison, we timed only the tool’s instrumentation phase (if
applicable),andthetestsrunningphase—weexcludetimetofetch
dependencies,compilecode,generatereports,etc.Coberturaruns
theentire testsuite twice(once without,andoncewithcoverage
440
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. DeFlaker: Automatically Detecting Flaky Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 5: Measurements comparing the runtime performance overhead of DeFlaker and four other coverage tools vs. the
baselinetestexecution. CoberturaandCloverdidnotworkformanyprojects(markedas‘n/a’),EkstaziandCoberturaeachtimedout
(after a four hour limit) on one project (marked as ‘t/o’).
JaCoCo Cobertura
ProjectTest
MethodsBaseline
Time (sec) DeFlaker Ekstazi Per-test Default Per-test Default Clover
achilles 563 184 .52±6.72 6 .2% 62 .3% 34 .9% 45 .2% n/a n/a n/a
ambari 5,186 3 ,726.11±573.55 6 .2% 29 .1% 52 .9% 52 .3% n/a n/a n/a
assertj-core 10 ,334 24 .56±5.09 12 .2% 100 .6% 30 .9% 141 .9% 180 .1% 194 .3% 129 .2%
checkstyle 2,452 75 .49±10.61 3 .7% 27 .0% 25 .3% 39 .6% 18 .5% 45 .9% n/a
commons-exec 96 64 .85±.79 3 .7% 2 .9% 1 .5% 1 .4% 2 .8% 4 .7% 4 .4%
dropwizard 1,455 219 .62±7.06 5 .6% 94 .4% 59 .6% 68 .9% 33 .5% 50 .5% n/a
hector 349 481 .96±141.28 4 .8% 29 .6% 16 .7% 14 .0% 3 .0% 10 .6% 9 .5%
httpcore 1,059 80 .77±10.12 6 .5% 18 .8% 14 .9% 18 .6% 18 .8% 41 .2% 30 .2%
jackrabbit-oak 9 ,694 1 ,290.23±336.05 0 .2% 34 .4% 31 .1% 34 .6% n/a n/a n/a
killbill 801 354 .66±63.47 3 .8% 21 .3% 24 .8% 19 .8% n/a n/a 4 .7%
ninja 797 55 .53±4.83 8 .4% 87 .6% 101 .8% 385 .0% 41 .2% 68 .3% 20 .2%
spring-boot 6,180 1 ,161.90±48.57 0 .0% t/o 18 .7% 24 .7% n/a n/a n/a
tachyon 2,126 2 ,299.54±194.84 3 .0% 17 .8% 35 .1% 36 .7% 40 .1% t/o n/a
togglz 400 174 .70±5.98 4 .8% 36 .6% 34 .0% 35 .2% 7 .4% 10 .2% n/a
undertow 725 180 .29±3.94 4 .6% 18 .2% 11 .2% 0 .1% n/a n/a n/a
wro4j 1,280 153 .18±6.19 1 .5% 26 .9% 29 .6% 33 .6% 6 .9% 24 .3% 8 .1%
zxing 415 114 .60±2.21 2 .8% 16 .2% 36 .6% 37 .3% n/a n/a 425 .8%
Average 626.03±83.61 4 .6% 39 .0% 32 .9% 58 .2% 35 .2% 50 .0% 79 .0%
tracking);wereportonlythetimespentrunningthetestsuitewith
coveragetracking(otherwise,Coberturawouldalwayshaveover
100%overhead).CoberturaandCloverdidnotworkcorrectlyon
many of the projects, typically due to incompatibilities between
the tool and various parts of the Java 8 syntax. (We found that
theseissueswerereportedbefore1.)Inonecase(spring-boot),we
encountered a deadlock while running Ekstazi.
DeFlakerwasveryfastinnearlyeverycase,withanaverage
slowdownof only4 .6%acrossall oftheseprojects. Theworstper-
formance from DeFlaker (12 .2%) occurred in the assertj-core
project,whichhadrelativelyfasttestexecution(25seconds):De-
Flaker’s impact on the actual test-execution time was insignifi-cant, but opening the Git repository and scanning it for changesstill required several seconds, contributing to the overhead. De-
Flaker performed far better than the other coverage tools, both in
assertj-core andotherprojects.Clovershowedhighlyvariable
performanceamongtheprojects,andisuniqueamongthecover-
age toolscompared: it adds coverageinstrumentation to program
source code and not to bytecode.
In summary, the results show two important points. First, De-
Flaker has arelatively low overhead,4 .5% on average,compared
to test runs without any coverage tool, low enough that we believeitcanbe“alwayson”inmanyprojects.Evenif DeFlakerwereonly
run on failing tests, its 4 .5% overhead is still low compared with
thecostofrerunningfailingtestsinisolation.Second,theoverhead
of DeFlaker is substantially lower than the overhead of the other
coveragetools.WeexpectedDeFlakertohavealoweroverhead
becauseitcollectslessinformation,butwedidnotexpectittobe
that much lower than the very mature JaCoCo (originally released
1https://github.com/cobertura/cobertura/issues/166,https://jira.atlassian.com/browse/
CLOV-1839in 2009) or Cobertura (originally released in 2005). In particular,
Cloverwasacommercialproduct,originallyreleasedin2006and
soldandmaintainedasoneuntil2017whenitwasreleasedasopen-
sourcesoftware.Finally,wedidnotyetoptimizetheperformance
of DeFlaker and believe we can improve it further, e.g., using
techniquessuchassmartinsertionofcoveragetrackingbasedon
control-flow graph analysis [79].
4 THREATS TO VALIDITY
While ourexperiments showthat DeFlakercan detectflaky tests
with low overhead, there are threats to generalizing our result.
External: Theprojectsusedinourevaluationmaynotberep-
resentative. To alleviate this threat, we consider a large number
of projects from different application domains, with different code
sizesandnumberoftestclasses.Wealsoconsideredflakytestswith
varying characteristics. In experiments in our historical environ-
ment, we considered only commits ofeach project in which there
was at least one flaky test. Our results could differ for a different
range of commits. We chose this range because it allows us to pre-
ciselymeasureDeFlaker’sabilitytofindknownflakytests.Further,
webelievethatthenumberofcommitsthatwetestedDeFlaker
on(5 ,966)isreasonable,especiallyconsideringthelargeamount
of time needed to run these experiments (over 5 CPU-years).
Internal: The tools we use in our evaluation may have bugs.
Toincreaseconfidenceinourexperiments,weuseasmanytools
thatareadoptedbytheopen-sourcecommunityaspossible.Totest
DeFlaker, we compared its coverage reports with those from the
stable and popular statement coverage tool JaCoCo.
Construct: To evaluate DeFlaker’s ability to detect known
flakytests,wecomputedthepercentageofcommitsofeachproject
with a known flaky test in which the test did not cover any code
441
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Bell et al.
changed by that commit, without regard for the test outcome. The
assumptionhereisthatifaflakytestcoverssomechange(which
caused it to fail), it will cover that change regardless of whether it
passesorfails.Weexpectthatthelikelihoodofaflakytestcovering
changed code is independent of it failing, because the pass/fail out-
comeoftheflakytestdependsonsomesourceofnon-determinism
unrelated to the code changes.
Reproducibility: To enable other researchers to reproduce our
results and build upon DeFlaker we have taken several steps. We
havereleasedDeFlakerundertheMITlicenseonGitHub[ 28]and
have publishedbinaries on MavenCentral [ 74]. We havereleased
acompanionpageonourDeFlakerwebsite[ 27],whichlistseach
commit of each project studied, their URLs, as well as each test
identified as flaky in our runs.
5 RELATED WORK
Flaky Tests. There have been several recent studies of flaky tests.
Luoetal.conductedthefirstextensivestudyofflakytests,studying
over200flakytestsfoundfromcommitlogsandissuetrackersof51
Apache projects, categorizing their root causes, common fixes, and
waystomanifestthem[ 61].ManyoftheflakytestsusedinSection3
came from Luo et al.’s dataset. Palomba and Zaidman studied 18
open-source Java projects, executing each test 10 times to detect
flakytestsandthenstudyingautomatedtechniquestorepairthe
flakiness in each test [ 82]. We used a similar rerun technique to
detectwhetheratestfailurewasatruefailureorduetoflakiness.
Whereas both of these papers specifically searched projects to seek
outflakytests,wepresentinsteadanevaluationoftechniquesto
find flaky tests from regular test failures.
The most common approach to detecting flaky tests is to re-
runfailingtests,whichwecallRerun.Severalpopularbuildand
test systems provide support for this, such as Google TAP [ 42,63],
MavenSurefire[ 77],Android[ 21],Jenkins[ 52],andSpring[ 75].To
the best of our knowledge, TAP (or any other similar system) does
notoffertheisolatedrerun(inanewJVM/process)thatwefoundto
bemoreeffectivethananin-processrerun.Wehaveshownthatthe
way that each rerun is performed can have a significant impact on
theoutcomeofthetest.Ourpreviouswork[ 33]consideredusing
coverage information to identify flaky tests but did not considerdifferential coverage, which is key to DeFlaker’s performance.
Also related are a variety of machine-learning techniques for cate-
gorizingtestfailuresasfalsealarms,flaky,orrelatedtoaspecific
change [48,54]. Such approaches could be used simultaneously
with Rerun or DeFlaker.
Oneapproachmaybethatdevelopersperformregularmainte-
nance, running a tool toidentify tests that mightfail in the future
due to flakiness (i.e., can become flaky tests), and then repair those
tests proactively [ 26,38,43,72]. However, these tools can be too
expensivetorunaftereverycodechangeandmayreporttoomany
warnings, hindering their adoption. For instance, specific tech-
niques have been recently proposed for handling order-dependent
tests. Zhang etal. proposeseveral methodsto detectsuch testsby
rerunningthemindifferentorders[ 85].HuoandClausepropose
another technique that can detect such tests [ 51], although their
technique was originally proposed to detect brittle assertions (that
may cause non-deterministic failures). Bell and Kaiser [ 25]p r o -
pose an approach to tolerate the effects of order-dependent testsbyisolatingtheminthesameJVM.However,noneofthesetech-
niques focuses on detecting whether a given test failure is due to a
flaky testor not, andnone of thetechniques handle generalcase of
arbitrary flaky tests.
Change-ImpactAnalysis. Change-impactanalysis(CIA)tech-
niquesaimtodeterminetheeffectsofsourcecodechanges,using
static,dynamic,orcombinedanalysis[ 31,69,71].Forexample,Ren
et al. [69] proposed Chianti, a CIA technique that uses static analy-
sistodecomposethedifferencebetweentwoprogramversionsinto
atomic changes and uses dynamic call graphs to determine the set
of tests whose behavior might be affected by these changes. It also
uses these call graphs to determine, for each affected test, whichsubset of changes can affect the test’s behavior. Our differentialcoverage technique also collects coverage but fully dynamically,
requires no expensive static analysis or call-graph generation, and
has a much lower overhead.
RegressionTestSelection. Regressiontestselection(RTS)tech-
niques determine which tests can be affected by a code change andonlyrunthosetospeedupregr essiontesting.ManyRTStechniques
havebeenproposed[ 35,40,45–47,70,86]andaresummarizedin
two literature reviews [ 30,81]. Most RTS techniques collect cover-
age,firstforallthetests,andthenrecollectcoverageonlyforthe
teststhatarerunaspotentiallyaffectedbythecodechanges.We
comparedtheperformanceof DeFlakertothepubliclyavailable
EkstaziRTStool[ 39].Insomeways,DeFlakerisanextensionof
residual coverage, a high-level approach to reduce the overhead of
program coverage tracking by only tracking code that has not-yet
beencovered[ 67].Inourterms,thecodechangedbyanewcommit
is de-facto not-yet-covered, and hence, tracked.
6 CONCLUSIONS AND FUTURE WORK
Flakytestscandisruptdevelopers’workflows,becauseitisdifficult
to know immediately if a test failure is a true failure or a flake.
We presented DeFlaker, an approach and a tool for evaluating
whether a test failure is flaky immediately after it occurs, with
very low overhead. Even if developers still want to rerun their test
failurestodetermineiftheyareflakyornot,DeFlakerisstilluseful
because it can provide its results immediately after the first failure,
rather than requiring tests to be delayed and reran.
We implemented DeFlaker for Java, integrating it with popular
buildandtesttools,andfound87previouslyunknownflakytestsin
10 projects, plus 4 ,846 flakes in old versions of 26 projects. We are
interested in exploring other applications of differential coverage.
Forinstance,ifatestfailsandcoverssomechangedcode,report-
ing the covered changed code may help debugging [69]. Similarly,
reportingwhenachangeisnotcoveredmayhelptestaugmenta-
tion [55]. Our results are very promising, and we plan to continue
working with the open-source software community to find flaky
tests and encourage the adoption of DeFlaker, which we have
released under the MIT license [28].
ACKNOWLEDGMENTS
WethankTraianŞerbănuţăandGrigoreRoşuforhelpindebugging
flaky tests using RV-Predict [ 50], and Alex Gyori for help in de-
buggingflakytestsusingNonDex[ 72].DarkoMarinov’sgroupis
supportedbyNSFgrantsCCF-1409423,CCF-1421503,CNS-1646305,
and CNS-1740916; and gifts from Google and Qualcomm.
442
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. DeFlaker: Automatically Detecting Flaky Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]2008. TotT:AvoidingFlakeyTests. (2008). http://googletesting.blogspot.com/
2008/04/tott-avoiding-flakey-tests.html.
[2]2017. Achilles Issue Tracker Issue Number 309. (2017). https://github.com/
doanduyhai/Achilles/issues/309.
[3]2017. Achilles Issue Tracker Issue Number 310. (2017). https://github.com/
doanduyhai/Achilles/issues/310.
[4]2017. checkstyleIssueTrackerIssueNumber4664. (2017). https://github.com/
checkstyle/checkstyle/issues/4664.
[5]2017. geoserver Issue Tracker Issue Number 8213. (2017). https://osgeo-org.
atlassian.net/browse/GEOS-8213.
[6]2017. jackrabbit-oak Issue Tracker Issue Number 6512. (2017). https://issues.
apache.org/jira/browse/OAK-6512.
[7]2017. jackrabbit-oak Issue Tracker Issue Number 6524. (2017). https://issues.
apache.org/jira/OAK-6524.
[8]2017. JMeterConcurrencyThreadGroupTest::testFlowflakytestfailure. (2017).
https://groups.google.com/forum/#!topic/jmeter-plugins/Fxg2ojVuxBs.
[9]2017. killbill Issue Tracker Issue Number 769. (2017). https://github.com/killbill/
killbill/issues/769.
[10]2017. nutz Issue Tracker Issue Number 1283. (2017). https://github.com/nutzam/
nutz/issues/1283.
[11]2017. presto Issue Tracker Issue Number 8374. (2017). https://github.com/
prestodb/presto/issues/8374.
[12]2017. presto Issue Tracker Issue Number 8491. (2017). https://github.com/
prestodb/presto/issues/8491.
[13]2017. presto Issue Tracker Issue Number 8492. (2017). https://github.com/
prestodb/presto/issues/8492.
[14]2017. presto Issue Tracker Issue Number 8493. (2017). https://github.com/
prestodb/presto/issues/8493.
[15]2017. presto Issue Tracker Issue Number 8494. (2017). https://github.com/
prestodb/presto/issues/8494.
[16]2017. presto Issue Tracker Issue Number 8666. (2017). https://github.com/
prestodb/presto/issues/8666.
[17]2017. quickml Issue Tracker Issue Number 152. (2017). https://github.com/
sanity/quickml/issues/152.
[18]2017. quickml Issue Tracker Issue Number 154. (2017). https://github.com/
sanity/quickml/issues/154.
[19]2017. togglz Issue Tracker Issue Number 233. (2017). https://github.com/togglz/
togglz/issues/233.
[20]2017. togglz Issue Tracker Issue Number 240. (2017). https://github.com/togglz/
togglz/issues/240.
[21]AndroidFlaky 2017. Android FlakyTest annotation. (2017). http://developer.
android.com/reference/android/test/FlakyTest.html.
[22]Apache Software Foundation. 2017. Maven Extension API.
(2017). http://maven.apache.org/ref/3.5.0/apidocs/org/apache/maven/
AbstractMavenLifecycleParticipant.html.
[23]TaweesupApiwattanapong,AlessandroOrso,andMaryJeanHarrold.2004. A
Differencing Algorithm for Object-Oriented Programs. In ASE.
[24]Atlassian. 2017. Comparison of code coverage tools. (2017). https://confluence.
atlassian.com/clover/comparison-of-code-coverage-tools-681706101.html.
[25]Jonathan Bell and Gail Kaiser. 2014. Unit Test Virtualization with VMVM. In
ICSE.
[26]Jonathan Bell, Gail Kaiser, Eric Melski, and Mohan Dattatreya. 2015. Efficient
Dependency Detection for Safe Java Test Acceleration. In ESEC/FSE.
[27]Jonathan Bell, Owolabi Legunsen, Michael Hilton, Lamyaa Eloussi, Tifany Yung,
andDarkoMarinov.2017. DeFlakerCompanionWebsite. (2017). http://www.
deflaker.org/icsecomp.
[28]Jonathan Bell, Owolabi Legunsen, Michael Hilton, Lamyaa Eloussi, Tifany Yung,
and Darko Marinov. 2017. DeFlaker source code. (2017). https://github.com/
gmu-swe/deflaker.
[29]MoritzBeller,GeorgiosGousios,andAndyZaidman.2017. TravisTorrent:Synthe-
sizing Travis CI and GitHub for Full-Stack Research on Continuous Integration.
InMSR.
[30]Swarnendu Biswas, Rajib Mall, Manoranjan Satpathy, and Srihari Sukumaran.
2011. Regression Test Selection Techniques: A Survey. Informatica (2011).
[31] Shawn A Bohner. 1996. Software change impact analysis. (1996).
[32]AhmetCelik,MarkoVasic,AleksandarMilicevic,andMilosGligoric.2017. Re-
gression Test Selection Across JVM Boundaries. In ESEC/FSE.
[33]Lamyaa Eloussi. 2015. Determining Flaky Tests from Test Failures. Master’s thesis.
University of Illinois at Urbana-Champaign, Urbana, IL.
[34]Emelie Engström, Per Runeson, and Mats Skoglund. 2010. A Systematic Review
on Regression Test Selection Techniques. I&ST-J(2010).
[35]Emelie Engström, Mats Skoglund, and Per Runeson. 2008. Empirical evaluations
of regression test selection techniques: a systematic review. In ESEM.
[36]FlakinessDashboardHOWTO 2017. Flakiness Dashboard HOWTO. (2017). http:
//www.chromium.org/developers/testing/flakiness-dashboard.[37]Martin Fowler. 2011. Eradicating Non-Determinism in Tests. (2011). http:
//martinfowler.com/articles/nonDeterminism.html.
[38]Alessio Gambi, Jonathan Bell, and Andreas Zeller. 2018. Practical Test Depen-
dency Detection. In ICST.
[39]MilosGligoric,LamyaaEloussi,andDarkoMarinov.2015. PracticalRegression
Test Selection with Dynamic File Dependencies. In ISSTA.
[40]Todd L. Graves, Mary Jean Harrold, Jung-Min Kim, Adam Porter, and GreggRothermel. 2001. An empirical study of regression test selection techniques.
TOSEM(2001).
[41]MarcoGuarnieri,PetarTsankov,TristanBuchs,MohammadTorabiDashti,and
DavidBasin.2017. TestExecutionCheckpointingforWebApplications.In ISSTA.
[42]Pooja Gupta, Mark Ivey, and John Penix. 2011. Testing at the speed and
scale of Google. (2011). http://google-engtools.blogspot.com/2011/06/
testing-at-speed-and- scale-of-google.html.
[43]Alex Gyori, August Shi, Farah Hariri, and Darko Marinov. 2015. Reliable Testing:
Detecting State-polluting Tests to Prevent Test Dependency. In ISSTA.
[44]Dan Hao, Tian Lan, Hongyu Zhang, Chao Guo, and Lu Zhang. 2013. Is This a
Bug or an Obsolete Test?. In ECOOP.
[45]Mary Jean Harrold, James A. Jones, Tongyu Li, Donglin Liang, Alessandro Orso,
MaikelPennings,SaurabhSinha,S.AlexanderSpoon,andAshishGujarathi.2001.
Regression Test Selection for Java Software. In OOPSLA.
[46]MaryJeanHarrold,DavidS.Rosenblum,GreggRothermel,andElaineJ.Weyuker.
2001. Empirical Studies of a Prediction Model for Regression Test Selection. TSE
(2001).
[47] Mary Jean Harrold and Mary Lou Soffa. 1988. An incremental approach to unit
testing during maintenance. In ICSM.
[48]KimHerzigandNachiappanNagappan.2015. EmpiricallyDetectingFalseTest
Alarms Using Association Rules. In ICSE SEIP.
[49]MichaelHilton, NicholasNelson,Timothy Tunnell,DarkoMarinov,and Danny
Dig. 2017. Trade-Offs in Continuous Integration: Assurance, Security, and Flexi-
bility. InESEC/FSE.
[50]JeffHuang,PatrickMeredith,andGrigoreRosu.2014. MaximalSoundPredictive
Race Detection with Control Flow Abstraction. In PLDI.
[51]ChenHuoandJamesClause.2014. ImprovingOracleQualitybyDetectingBrittle
Assertions and Unused Inputs in Tests. In FSE.
[52]JenkinsRandomFail 2016. Jenkins RandomFail annotation. (2016).https://github.com/jenkinsci/jenkins-test-harness/blob/master/src/main/
java/org/jvnet/hudson/test/RandomlyFails.java.
[53] JGitWebPage 2017. JGit Home Page. (2017). http://www.eclipse.org/jgit/.[54]
HeJiang,XiaochenLi,ZijiangYang,andJifengXuan.2017. WhatCausesMyTest
Alarm?:AutomaticCauseAnalysisforTestAlarmsinSystemandIntegration
Testing. In ICSE.
[55]Wei Jin, Alessandro Orso, and Tao Xie. 2010. Automated Behavioral Regression
Testing. In ICST.
[56]AdriaanLabuschagne,LauraInozemtseva,andReidHolmes.2017. Measuringthe
CostofRegressionTestinginPractice:AStudyofJavaProjectsUsingContinuous
Integration. In ESEC/FSE.
[57]F.J. Lacoste. 2009. Killing the Gatekeeper: Introducing a Continuous Integration
System. In Agile.
[58] Tim Lavers and Lindsay Peters. 2008. Swing Extreme Testing.
[59]OwolabiLegunsen,FarahHariri,AugustShi,YafengLu,LingmingZhang,and
Darko Marinov. 2016. An Extensive Study of Static Regression Test Selection in
Modern Software Evolution. In FSE.
[60]Yafeng Lu, Yiling Lou, Shiyang Cheng, Lingming Zhang, Dan Hao, Yangfan
Zhou, and Lu Zhang. 2016. How Does Regression Test Prioritization Perform in
Real-world Software Evolution?. In ICSE.
[61]Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, and Darko Marinov. 2014. An
Empirical Analysis of Flaky Tests. In FSE.
[62]AtifM.MemonandMyraB.Cohen.2013. AutomatedtestingofGUIapplications:
Models, tools, and controlling flakiness. In ICSE.
[63]John Micco. 2013. Continuous Integration at Google scale. (2013).http://eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-03-24%
20Continuous%20Integration%20at%20Google%20Scale.pdf.
[64]John Micco. 2017. The State of Continuous Integration Testing @Google. (2017).
https://research.google.com/pubs/pub45880.html.
[65]Kıvanç Muşlu, Bilge Soran,and Jochen Wuttke. 2011. Finding bugs by isolating
unit tests. In ESEC/FSE.
[66]Alessandro Orso, Nanjuan Shi, and Mary Jean Harrold. 2004. Scaling Regression
Testing to Large Software Systems. In FSE.
[67]ChristinaPavlopoulouandMichalYoung.1999. ResidualTestCoverageMonitor-
ing. InICSE.
[68]Leandro Sales Pinto, Saurabh Sinha, and Alessandro Orso. 2012. Understanding
Myths and Realities of Test-suite Evolution. In FSE.
[69]Xiaoxia Ren, Fenil Shah, Frank Tip, Barbara G. Ryder, and Ophelia Chesley. 2004.
Chianti: A Tool for Change Impact Analysis of Java Programs. In OOPSLA.
[70]Gregg Rothermel and Mary Jean Harrold. 1997. A Safe, Efficient Regression Test
Selection Technique. TOSEM(1997).
443
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Bell et al.
[71]BarbaraGRyderandFrankTip.2001. Changeimpactanalysisforobject-oriented
programs. In PASTE.
[72]August Shi, Alex Gyori, Owolabi Legunsen, and Darko Marinov. 2016. Detecting
Assumptions on Deterministic Implementations of Non-deterministic Specifica-
tions. InICST.
[73]AugustShi,SureshThummalapenta,ShuvenduK.Lahiri,NikolajBjorner,and
Jacek Czerwonka. 2017. Optimizing Test Placement for Module-level Regression
Testing. In ICSE.
[74] Sonatype. 2017. Maven Central Repository. (2017). https://search.maven.org.
[75]spring-junit-page 2017. Spring Repeat Annotation. (2017). https:
//docs.spring.io/spring/docs/current/javadoc-api/org/springframework/
test/annotation/Repeat.html.
[76]Pavan Sudarshan. 2012. No more flaky tests on the Go team. (2012). http:
//www.thoughtworks.com/insights/blog/no-more-flaky-tests-go-team.
[77]SurefireRerun 2017. Surefire rerunFailingTestsCount Option. (2017).
http://maven.apache.org/surefire/maven-surefire-plugin/examples/
rerun-failing-tests.html.
[78]The Eclipse Foundation. 2017. Eclipse Java Development Tools (JDT). (2017).
http://www.eclipse.org/jdt/.[79]MustafaM.TikirandJeffreyK.Hollingsworth.2002. EfficientInstrumentation
for Code Coverage Testing. In ISSTA.
[80]Arash Vahabzadeh,Amin Milani Fard, and AliMesbah. 2015. Anempirical study
of bugs in test code. In ICSME.
[81]ShinYooandMarkHarman.2012. RegressionTestingMinimization,Selection
and Prioritization: A Survey. STVR(2012).
[82]AndyZaidmanandFabioPalomba.2017. DoesRefactoringofTestSmellsInduce
Fixing Flaky Tests?. In ICSME.
[83] Lingming Zhang. 2018. Hybrid Regression Test Selection. In ICSE.
[84]Lingming Zhang, Miryung Kim, and Sarfraz Khurshid. 2011. Localizing Failure-
Inducing Program Edits based On Spectrum Information. In ICSM.
[85]Sai Zhang, Darioush Jalali, Jochen Wuttke, Kivanc Muslu, Michael Ernst, and
DavidNotkin.2014. Empirically RevisitingtheTestIndependence Assumption.
InISSTA.
[86]JiangZheng,BrianRobinson,LaurieWilliams,andKarenSmiley.2006. Applying
Regression Test Selection for Cots-Based Applications. In ICSE.
[87]Celal Ziftci and Jim Reardon. 2017. Who Broke the Build?: Automatically Identi-
fying Changes That Induce Test Failures in Continuous Integration at Google
Scale. InICSE-SEIP.
444
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:44 UTC from IEEE Xplore.  Restrictions apply. 