QUICKAR: Automatic Query Reformulation for Concept
Location using Crowdsourced Knowledge
Mohammad Masudur Rahman Chanchal K. Roy
Department of Computer Science, University of Saskatchewan, Canada
{masud.rahman, chanchal.roy}@usask.ca
ABSTRACT
During maintenance, software developers deal with numer-
ous change requests made by the users of a software sys-
tem. Studies show that the developers nd it challenging
to select appropriate search terms from a change request
during concept location. In this paper, we propose a novel
technique{QUICKAR{that automatically suggests helpful
reformulations for a given query by leveraging the crowd-
sourced knowledge from Stack Overow. It determines se-
mantic similarity or relevance between any two terms by
analyzing their adjacent word lists from the programming
questions of Stack Overow, and then suggests semantically
relevant queries for concept location. Experiments using 510
queries from two software systems suggest that our tech-
nique can improve or preserve the quality of 76% of the
initial queries on average which is promising. Comparison
with one baseline technique validates our preliminary nd-
ings, and also demonstrates the potential of our technique.
CCS Concepts
Software and its engineering !Software mainte-
nance tools; Requirements analysis; Traceability; Main-
taining software;
Keywords
Query reformulation, crowdsourced knowledge, semantic rel-
evance, word co-occurrence, adjacency list, Stack Overow
1. INTRODUCTION
Studies show that about 85%{90% of the total eort is
spent in software maintenance and evolution [3, 4]. Dur-
ing maintenance, software developers deal with numerous
change requests made by the users of a software system.
Although the users might be familiar with the application
domain of the software, they generally lack the idea of how
a particular software feature is implemented in the sourcecode. Hence, the requests from them generally involve do-
main related concepts (e.g., application features), and they
are written in an unstructured fashion using natural lan-
guage texts. The developers need to prepare appropriate
search query from those concepts, and then identify the rel-
evant location(s) in the code to implement the requested
change(s). Unfortunately, preparing such a query is highly
challenging and error-prone for the developers [5, 13]. Based
on a user study, Kevic and Fritz [13] report that develop-
ers were able to suggest good quality search terms for only
12.2% of the change tasks. Furnas et al. [5] suggest that
there is a little chance (i.e., 10%{15%) that developers guess
the exact words used in the source code. One way to as-
sist the developers in this regard is to automatically suggest
helpful reformulations for the initially executed query.
Existing studies use relevance feedback from developers
[6] or information retrieval techniques [10], query quality
[8, 9] and the context of query terms within the source
code [12, 23] in suggesting reformulated queries. Gay et al.
[6] make use of explicit feedback on document relevance
from the software developers, and then suggest reformu-
lated queries using Rocchio's expansion. Haiduc et al. and
colleagues [7, 8, 9, 10, 11] take quality of the query into
consideration, and suggest the best reformulation strategy
for a given query using machine learning. Howard et al.
[12] analyze leading comments and method signatures from
the source code for mining semantically similar word pairs,
and then suggest reformulated query using those word pairs.
While these above techniques are reported to be novel or
eective, they are also limited in certain aspects. First, col-
lecting explicit feedback from the developers could be highly
expensive, and such study [6] could also be hard to repli-
cate. Second, machine learning model of Haiduc et al. is
reported to be performing well in the case of within-project
training, and only 51{72 queries are considered from each
of the ve projects for training and testing [10]. Given such
small dataset, the reported performance possibly could not
be generalized for large systems. Third, Howard et al. re-
quire the source code to be well documented for the mining
of word pairs, and hence, might not perform well if the code
is poorly documented [12]. Thus, we need a technique that
is neither subject to the training data nor the availability of
comments in the source code. One way to possibly overcome
those concerns is to apply crowd generated knowledge in the
reformulation of queries for concept location.
In this paper, we propose a novel technique{QUICKAR{
that automatically identies semantically similar words to
an initial query not only from project source code but also
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970362
220
from crowdsourced content of Stack Overow Q & A site,
and then suggests a reformulated query. The technique col-
lects adjacent word lists from the programming questions
of Stack Overow for any two terms, and determines their
semantic similarity by comparing their corresponding adja-
cency lists [24]. In short, QUICKAR not only follows the
essence of a nearest neighbour classier [16] in the context
of natural language texts but also harnesses the technical
corpus developed by a large crowd over the years in esti-
mating semantic similarity orrelevance . Such simple but
intuitive estimation of semantic relationship could be highly
useful for suggesting an alternative version of a given query.
QUICKAR also addresses the overarching vocabulary mis-
match problem [5]. First, Stack Overow is curated by a
large crowd of four million technical users, and the mil-
lions of questions and answers posted by them are a great
source for technical vocabulary (e.g., API names) [21]. Thus,
QUICKAR mines semantically similar words not only from
a larger corpus (i.e., compared to a single project [23]) but
also from a more appropriate vocabulary (i.e., compared to
WordNet [22, 23]). Second, Rahman et al. [20] reported
a signicant overlap (i.e., 73%) between the vocabulary of
real life code search queries and that of question titles from
Stack Overow. QUICKAR mines 500K programming re-
lated questions from Stack Overow carefully, and reaps
the benet through meaningful vocabulary extension. To
the best of our knowledge, no existing studies apply crowd-
sourced knowledge yet in the reformulation of queries for
concept location which makes our technique novel.
Experiments using 510 concept location queries from two
subject systems{ ecfand eclipse.pde.ui {suggest that our
technique can improve or preserve the quality of 76% (i.e.,
improves 66% and preserves 10%) of the initial queries through
reformulation which is promising according to relevant lit-
erature [6, 10]. Comparison with one baseline technique{
Rocchio's expansion [1, 10]{validates our preliminary nd-
ings, and also demonstrates the potential of our technique
for query reformulation. While the preliminary ndings
are promising, they must be validated using further experi-
ments. In this paper, we make the following contributions:
Construction of a word adjacency list database by min-
ing 500K questions from Stack Overow for the estima-
tion of semantic similarity or relevance between words.
A novel technique that suggests helpful reformulations
for a given query for concept location by leveraging
crowdsourced knowledge from Stack Overow.
2. MOTIVATING EXAMPLE
Software change requests and project source code are of-
ten written by dierent people, and they use dierent vo-
cabularies to describe the same technical concept. Concept
location community has termed it as vocabulary mismatch
problem [5, 10]. Table 1 shows three dierent questions from
Stack Overow that are marked as duplicates orlinked by
the users of the site. These questions use three dierent
verbs{ `create', `cause' and `track' { to describe the same
programming issue{ locating memory leaks , and this can be
considered as a real life parallel for vocabulary mismatch
issue. Although, these verbs have dierent semantics in En-
glish literature, they share the same or almost similar se-
mantic in this technical literature{ programming questions
[23]. More interestingly, their semantic similarities can also
be approximated from their adjacent word lists. In graphTable 1: Duplicate Questions from Stack Overow
ID Title of Question
6470651 Creating a memory leak with Java
4948529 Easiest way to cause memory leak in Java?
1071631 Tracking down a memory leak/garbage-collection issue in Java
theory, two nodes are considered to be connected if they
share the same neighbour nodes [16]. We adapt that idea
for natural language texts, and apply to the estimation of
semantic connectivity between words. The co-occurred word
lists (i.e., sentence as a context unit) of `create', `cause' and
`track' {fmemory, leak, Java g,feasiest, way, memory, leak,
Javagandfdown, memory, leak, garbage, collection, issues,
Javag{respectively share multiple words among themselves.
Thus, comparison between any two such lists can potentially
approximate the semantic similarity or relevance of their cor-
responding words [24]. In this research, we apply the above
methodology in semantic similarity estimation, and then use
the similar terms for the reformulation of an initial query.
3. QUICKAR: QUERY REFORMULATION
USING CROWDSOURCED KNOWLEDGE
Programming questions and answers from Stack Overow
were previously mined for API elements [20, 21], Q & A dy-
namics [14, 17] or post suggestions [18, 19]. In this research,
we make a novel use of such questions in query reformulation
for concept location. Since these questions contain unstruc-
tured information, relevant items (i.e., potential alternative
query term) should be carefully extracted and then applied
to query reformulation. We rst construct a database con-
taining adjacent word list for each of the individual words
taken from the title of the questions, and then leverage
that information in suggesting reformulated queries. Fig.
1 shows the schematic diagram of our proposed technique{
QUICKAR{for query reformulation. Thus, our technique
can be divided into two major parts{(a) Construction of an
adjacency list database from Stack Overow questions, and
(b) Reformulation of an initial query{as follows:
3.1 Construction of Adjacency List Database
Word co-occurrence is often considered as a proxy for se-
mantic relevance between words in natural language texts
[15]. Yuan et al. [24] rst propose to use contextual words
(i.e., word co-occurrence) from the programming questions
and answers of Stack Overow in identifying semantically
similar software-specic word pairs. While they introduce
the idea, we adapt that idea for a specic software main-
tenance task{search query reformulation for concept loca-
tion. Given the signicant overlap (i.e., 73%) between real
life code search queries and titles of Stack Overow ques-
tions [20], we collect the titles of 500K Java related ques-
tions from Stack Overow using Stack Exchange Data Ex-
plorer1. We check for <java> tag in the tag list for identi-
fying the Java related questions. We perform standard nat-
ural language preprocessing (i.e., word splitting, stop word
removal), and turn each of those titles into a sequence of
words (Step 3, 4, Fig. 1-(a)). We decompose each camel case
word (e.g., GenericContainerInstantiator ) into separate
tokens (e.g., Generic, Container, Instantiator ), and ap-
ply a standard list2for stop word removal. It should be
noted that we avoid stemming to ensure a meaningful refor-
1http://data.stackexchange.com/stackoverow
2https://code.google.com/p/stop-words/
221Figure 1: Proposed technique for query reformulation{(a) Construction of word adjacency list database from
Stack Overow questions, and (b) Reformulation of an initial query for concept location
mulation of the query. After preprocessing step, we found
a large set of 81,394 individual words (i.e., 2,660,257 words
in total) from our collected titles. Given that source code
of a software project is often authored by a small group of
developers, such a large set could possibly extend the vocab-
ulary of the code. We then use a sliding window ofwindow
size = 2 to capture co-occurred words from the titles [15],
and construct an adjacency list for each of the individual
words. For example, based on Table 1, the adjacency list for
the word `memory' would be{fcreating, leak, cause, down g.
We collect adjacency list for each of the 81,394 words where
each list contains 21 co-occurred words on average (Step 5,
6, Fig. 1-(a)). These lists comprise our database which is
later accessed frequently for query reformulation.
3.2 Reformulation of an Initial Query
Fig. 1-(b) shows the schematic diagram and Algorithm 1
shows the pseudo code for our query reformulation technique{
QUICKAR. We collect semantically similar words (i.e., with
initial query) from two dierent but relevant sources{project
source code and Stack Overow questions, and then reformu-
late a given query for concept location. We discuss dierent
intermediate steps involved in such reformulation as follows:
Collection of Candidate Terms: Existing literature
mostly relies on the source code of a software system [10,
12, 23] or WordNet [22] for reformulated query suggestion.
Unfortunately, source code often might not contain a rich vo-
cabulary [12] and WordNet might also not be appropriate for
technical words [22, 24]. We thus collect candidate terms for
possible query expansion not only from the source code but
also from another technical literature{programming ques-
tions of Stack Overow (Line 6{Line 8, Algorithm 1). In
the case of project source, we perform code search using
the reduced keywords ( K) from the initial query ( Q). We
useApache Lucene [18], a popular implementation of Vector
Space Model (VSM), for code search, and then collect the
Top-5 (i.e., cut o) retrieved documents as the source for
candidate terms. The cut-o value is chosen based on iter-
ative experiments. We perform standard natural language
preprocessing on those documents, and extract each of the
terms as the reformulation candidates ( Tp) (Step 3{5, Fig. 1-
(b)). Relevant literature [10] also follows the same procedure
for candidate term selection. In the case of questions from
Stack Overow, we collect such words as candidates ( Tso)
that frequently co-occurred in those questions with the key-
words from the initial query. The underlying idea is that if
two words frequently co-occur in various technical contexts,
they share their semantics and thus, are possibly semanti-
cally relevant [15, 24]. We use our adjacency list database
(Section 3.1) for identifying the second set of candidates.
Estimation of Semantic Similarity or Relevance:
Since we target to reformulate a query using meaningful al-Algorithm 1 Query Reformulation using Crowd Knowledge
1:procedure QUICKAR (Q). Q: initial search query
2: Q0 fg .reformulated search query
3: .collecting keywords from the initial search query
4: K collectKeywords( Q)
5: K reduceKeywords( K)
6: .collecting candidate terms
7: Tp getCandidateTermsFromProject( K)
8: Tso getCandidateTermsFromSO( K)
9: .estimating semantic similarity of the candidates
10: forCandidate Ti2Tpdo
11: AdjTi getAdjacencyListFromDB( Ti)
12: forKeyword Kj2Kdo
13: AdjKj getAdjacencyListFromDB( Kj)
14: .contextual similarity between words
15: Scos getCosineSimilarity( AdjTi; Adj Kj)
16: Rp[Ti]:score Rp[Ti]:score +Scos
17: end for
18: end for
19: forCandidate Ti2Tsodo
20: forKeyword Kj2Kdo
21: .co-occurrence between words
22: Scof getCo-occurrenceFreq( Ti; Kj)
23: Rso[Ti]:score Rso[Ti]:score +Scof
24: end for
25: end for
26: .ranking and selection of candidates
27: SRTopp selectTopK(sortByScore( Rp))
28: SRTopso selectTopK(sortByScore( Rso))
29: R selectiveCombine( SRTopp; SR Topso)
30: .reformulate the initial query
31: Q0 selectiveReformulate( Q; K; R )
32: return Q0
33:end procedure
ternatives, we need to choose such terms from the candidates
that are either semantically similar or highly relevant to the
initial query. Yuan et al. use contextual words (i.e., based on
co-occurrence) from Stack Overow for automatically identi-
fying similar software-specic words. In the context of query
reformulation for software maintenance, we similarly apply
adjacency list to the estimation of semantic relevance be-
tween two terms. We collect adjacency list (i.e., from the
adjacency list database) for each of the candidate terms and
the keywords of the initial query, and estimate their similari-
ties using cosine similarity measure ( Scos). Cosine similarity
is frequently used in information retrieval for determining
textual similarity between two given documents. It returns
a value between zero (i.e., completely dissimilar) and one
(i.e., completely similar). Thus, a candidate term achieves
score only if it shares its context (i.e., adjacent word list,
AdjTi) with that (i.e., AdjKj) of the keywords across vari-
ous questions from Stack Overow. QUICKAR iterates this
222Table 2: Experimental Dataset
Subject System Release ID #Files #Methods #Queries
ecf 170170 5,781 21,447 222
eclipse.pde.ui I20151110-0800 7,579 31,468 288
Total { 13,360 52,915 510
process for each of the candidates ( Tp), and accumulates
their similarity scores against the keywords (Line 9{Line 18,
Algorithm 1, Step 6, Fig. 1-(b)). We also determine co-
occurrence frequency ( Scof) between each candidate term
and each keyword in the titles of Stack Overow questions
[20], and derive another set of scores for the candidates ( Tso)
(Line 19{Line 25, Algorithm 1, Step 6, Fig. 1-(b)). Finally,
we end up with two sets of candidates (i.e., collected from
two dierent sources), and QUICKAR determined their rel-
evance to the initial query in terms of their context or direct
co-occurrences in the Stack Overow questions.
Candidate Term Ranking & Top-K Selection : Once
relevance estimates for both candidate sets{ RpandRso{are
collected, they are ranked based on their estimates. We then
collect the Top- K= 5 candidates from each ranked list, se-
lectively choose the top candidates ( R) from both lists, and
then treat them as similar or relevant to the initial query ( Q)
(Line 26{Line 29, Algorithm 1). In particular, the nominal
terms (i.e., nouns) from both lists are chosen [20]. Thus, we
select such terms for reformulation that co-occur with the
initial query keywords not only in the project source but
also in the titles of the questions from Stack Overow.
Query Reduction & Expansion: We apply both refor-
mulation strategies{ reduction and expansion {to the initial
query [1]. In the case of reduction , we apply a conservative
strategy as was also applied by Haiduc et al.. We discard
the keywords from initial query that either are non-nominal
[20] or occurred in more than 25% of the documents of the
project corpus. Such keywords are not specic enough and
thus, are not useful for document retrieval [10]. In the case
ofexpansion , we apply the semantically similar or relevant
terms ( R) returned by QUICKAR to the query. If there ex-
istMterms after the reduction step (i.e., Line 5), we append
(10 M) relevant terms from Rto the query, and prepare an
alternative query ( Q0) (Line 29{Line 32, Algorithm 1, Step
7, 8, Fig. 1-(b)). We also decompose each camel case term
into separate tokens, and preserve both the separate and
the camel case terms into the reformulated query [10]. It
should be noted that if our reduction step already improves
the initial query, we conveniently avoid its expansion.
Working Example: Let us consider a change request
(ID: 408030) from ecfproject. We select title of the request{
\RestClientService ignores content encoding" {as the initial
search query for the request, as was also used by the exist-
ing literature [10]. The query returns the rst relevant doc-
ument (i.e., Java class) at 44thposition when tested using
Apache Lucene . On the other hand, QUICKAR returns the
reformulated query{ \Rest Client Service RestClientService
content Web Java Executor WebService Http" {that returns
the same relevant document at 1stposition in the search
result. Please note that our technique expands the initial
query using several relevant terms such as \WebService",
\Executor" and\Http" and also discards some terms such as
\encoding " or \ ignore ", which improved the query. We also
expand the query simply using terms from project source{
\RestClientService ignores content encoding call container
service http test" {that returns the document at 11thposition.
This clearly demonstrates that candidates from the sourcecode of a project might always not be sucient, and crowd
generated vocabulary from Stack Overow can complement
them, and thus, can assist in eective query reformulation.
4. EXPERIMENT
One of the most eective ways to evaluate a query re-
formulation technique is to check whether the reformulated
query improves the search results or not. We dene improve-
ment of search results as the bubbling up of the rst relevant
document to the top positions of the result list [10]. That is,
a good reformulation of query provides a better rank than
a baseline query for the rst relevant document. We con-
duct experiments using 510 change requests from two Java
subject systems of Eclipse {ecfandeclipse.pde.ui { and a
well known search engine{ Apache Lucene [10, 18]. We also
compare with a baseline technique for query reformulation
to validate our ndings. In particular, we attempt to answer
the following research questions using our experiments:
RQ 1:How does QUICKAR perform in the reformu-
lation of a query for concept location?
RQ 2:Can crowdsourced knowledge from Stack Over-
ow improve a given query signicantly?
RQ 3:How does QUICKAR perform compared to the
baseline technique for query reformulation?
4.1 Experimental Dataset & Corpus
Dataset Collection: Table 2 shows details of our se-
lected subject systems. We rst collect the RESOLVED
change requests (i.e., bug reports) from BugZilla for each of
the selected systems. Then we identify such commits that
implemented those requests in their corresponding GitHub
repositories. We consider a commit as eligible only if its ti-
tle contains a specic request identier (e.g., Bug: 408030).
This practice is common for relevant literature for evalua-
tion [2]. The step provides 495 and 542 requests from ecf
and eclipse.pde.ui respectively. We also collect change
setfrom each of the identied commits which are later used
as the solutions for the corresponding change requests. We
then consider title of each request as the baseline query, and
identify such queries that return their rst relevant results
with poor rank (i.e., rank> 10). That is, the baseline query
needs reformulation for returning a better rank. This ltra-
tion step left us with 222 and 288 baseline queries from ecf
and eclipse.pde.ui respectively for the experiments.
Corpus Preparation: Unlike an unstructured natural
language document, a source code document contains items
beyond regular texts such as classes, interfaces, methods and
constructors . One should consider such structures for eec-
tive retrieval of the source documents from a project. We
thus decompose each Java document into methods, and con-
sider each of those methods as a single document of the
corpus. This step provides 21,447 and 31,468 Java methods
from ecfandeclipse.pde.ui respectively. We collect these
methods using Javaparser3, and apply natural language pre-
processing to them. In particular, we remove all punctua-
tion marks, Java programming keywords and English stop
words from the body and signature of the method, and also
decompose each camel case token into individual tokens.
4.2 Evaluation of QUICKAR
We execute each of our reformulated queries and base-
line queries from each subject system with Apache Lucene ,
3https://github.com/javaparser/javaparser
223Table 3: Performance of QUICKAR
System #QueriesImprovement Worsening Preserving
#Improved Mean Q1 Q2 Q3 Min. Max. #Worsened Mean Q1 Q2 Q3 Min. Max. #Preserved
ecf 222 159 (71.62%) 194 10 27 156 1 2335 41 (18.47%) 764 67 245 1173 18 4330 22 (9.90%)
pde.ui 288 177 (61.46%) 219 17 51 171 1 4766 83 (28.82%) 558 93 244 630 17 2996 28 (9.72%)
Total=510 Avg=66.54% Avg=23.65% Avg=9.81%
pde.ui =eclipse.pde.ui ,Mean =Mean rank of rst relevant document in the search result, Qi= Rank value for ithquartile of all result ranks
Table 4: Role of Crowdsourced Knowledge from SO
Technique Improved Worsened Preserved
Baseline query (preprocessed) 17.84% 9.90% 72.27 %
QUICKAR P 49.15% 48.41% 2.44%
QUICKAR SO 47.83% 49.91% 2.27%
QUICKAR red 55.55% 24.46% 19.99%
QUICKAR ALL 66.54 % 23.65% 9.81%
and compare their topmost ranks for evaluation. We iden-
tify the queries that were improved, worsened or preserved
based on those ranks. Table 3 reports the outcome of our
preliminary investigation. On average, QUICKAR was able
to improve 66% of the baseline queries while preserving the
quality of 10% which are highly promising according to rel-
evant literature [6, 10, 12]. We see that QUICKAR can
return the top results within the 10thposition for 25% of
159 requests from ecfsystem. It also performs similarly
foreclipse.pde.ui , and returns the top results within the
17thposition. One might argue about the verbatim use of
the title of a change request as the baseline query. How-
ever, we also experimented with preprocessed version (i.e.,
stop word removal, camel case decomposition) of the title.
We found that the preprocessing step discarded important
information, and did not provide much improvement in the
query quality, which possibly justies our choice about base-
line query. Thus, to answer RQ 1, our proposed technique
for query reformulation{QUICKAR{improves or preserves
76% of the 510 baseline queries which is promising.
We investigate how dierent reformulation decisions in-
uence the end performance of our technique, and Table 4
reports our ndings. We rst experimented using a pre-
processed version of the baseline queries, and found that
the preprocessing step did not improve the queries much
(i.e., only 18% improvement). Since candidate terms for
reformulation are extracted from both project source code
and Stack Overow questions, we need to examine their im-
pact in the reformulated queries. When we rely solely on
source code, QUICKAR Pcan improve 49% of the queries
but degrades the quality of 48%. Although the candidate
terms from Stack Overow questions alone might not be
sucient (i.e., QUICKAR SOimproves 48% and degrades
50%), they denitely can complement the candidate terms
from the project source which leads to overall query quality
improvement (i.e., QUICKAR ALL improves 66%). We also
performed Mann Whitney U (MWU) -test on the provided
ranks by QUICKAR Pand QUICKAR ALL, and found that
QUICKAR ALL returns the results at signicantly higher
ranks (i.e., p-values 0.007 <0.05 and 0.001 <0.05 for ecfand
eclipse.pde.ui respectively, Table 6) in the result list. The
negative mean rank dierence (MRD) in Table 6 suggests
that QUICKAR ALL returns the results at relatively closer
to the top of the list than the counterpart, i.e., extent of re-
sult rank improvement . Thus, to answer RQ 2, crowdsourced
knowledge from Stack Overow questions can signicantly
improve the quality of a baseline query during reformulation.
Since QUICKAR involves two steps during query refor-
mulation { reduction andexpansion , an investigation is war-Table 5: Comparison with Baseline Technique
Technique System Improved Worsened Preserved
Rocchio's ecf 39.64% 59.46% <1.00%
Expansion [1] pde.ui 40.63% 59.38% 0.00%
QUICKAR Pecf 53.15% 43.69% 3.15%
pde.ui 45.14% 53.13% 1.74%
QUICKAR ALLecf 71.62 % 18.47% 9.90%
pde.ui 61.46 % 28.82% 9.72%
pde.ui =eclipse.pde.ui
Table 6: Result of Mann Whitney U-Tests
Technique pairecf eclipse.pde.ui
p-value MRD p-value MRD
QUICKAR ALL vs. QUICKAR P 0.007 <0.05 -248 <0.001 -369
QUICKAR ALL vs. QUICKAR SO 0.115 >0.05 -109 <0.001 -253
QUICKAR ALL vs. Rocchio [1] <0.001 -388 <0.001 -332
MRD =MeanRankDierence
ranted on how these two steps impact the end performance.
According to our preliminary investigation, the reduction
step (i.e., QUICKAR red) dominates over expansion step es-
pecially with ecfsystem. One possible explanation could be
that we used a smaller version of the adjacency list database
constructed from 50,000 (i.e., 10%) questions from the dataset.
Since accessing a large database is time-consuming, we made
this feasible choice during experiment. However, further in-
vestigation and experiments are essential to mitigate such
concern which we consider as a future work. In short, the
potential of crowdsourced knowledge is yet to be explored.
4.3 Comparison with Baseline Technique
Although the conducted evaluation demonstrates the po-
tential of our proposed technique{QUICKAR, we still in-
vestigate to at least partially validate our performance. We
compare with a baseline technique{Rocchio's expansion [1,
10]{that is reported to be eective for query reformulation.
Rocchio's method rst collects candidate terms from the
Top-K (i.e., K= 5) source code documents returned by
a baseline query. Then, it selects the most important candi-
date terms ( t) for reformulation by calculating their TF-IDF
in each of those Top-K documents ( D) as follows:
Rocchio (t) =X
d2RTFIDF (t; d)
We implemented Rocchio's method in our working environ-
ment, experimented on the same corpus and applied similar
natural language preprocessing. Table 5 reports the compar-
ative analysis between our technique and Rocchio's method.
We see that our technique can improve 60%{70% of the base-
line queries from each of the subject systems whereas such
measure for Rocchio's method is close to 40%. More impor-
tantly, QUICKAR degrades less number of queries compared
to its counterpart. While Rocchio's method worsened the
quality of 60% of the baseline queries during reformulation,
such measure for QUICKAR is between 18% to 29%. We
also performed Mann Whitney U -test on the returned re-
sult ranks from both techniques, and found that QUICKAR
provides signicantly better ranks (i.e., p-values <0.001 and
<0.001 for ecfand eclipse.pde.ui respectively, Table 6)
than Rocchio's expansion. We also compared using a equiv-
224alent variant of our technique{QUICKAR P, and found that
the variant still performed better than Rocchio's method for
both subject systems. All these above preliminary ndings
clearly demonstrate the potential of our proposed technique.
Thus, to answer RQ 3, our technique{QUICKAR{performs
signicantly better than the baseline technique [1] in query
reformulation for concept location.
5. RELATED WORK
Existing studies from the literature use relevance feedback
from developers [6] or information retrieval techniques [10],
query quality [8, 9] or the context of a query in the source
code [12, 23] for suggesting query reformulations. Gay et al.
[6] capture explicit feedback on document relevance from the
software developers, and then suggest reformulated queries
using Rocchio's expansion. Although their adopted method-
ology is meaningful, capturing feedback from the develop-
ers could be expensive, and such study is often dicult to
replicate. Haiduc et al. and colleagues [7, 8, 9, 10, 11] ana-
lyze quality of the query, and suggest the best reformulation
strategy for any given query using machine learning. Al-
though their reported performance is signicantly higher,
such performance might not be generalized for large sys-
tems given their use of small dataset (i.e., only 51{72 queries
from each system). Howard et al. [12] analyze leading com-
ments and method signatures from the source code, and sug-
gest reformulated queries by extracting semantically similar
word pairs. However, their technique requires the source
code to be well documented, and thus, might not perform
well with poorly documented code. On the other hand, our
technique{QUICKAR{complements the source code vocab-
ulary by capturing appropriate candidate terms from the
programming questions of Stack Overow. Carpineto and
Romano [1] conduct a survey on the automatic query ex-
pansion (AQE) mechanisms applied to information retrieval.
Rocchio's expansion is one of such mechanisms which was
adapted by earlier studies [6, 10] in the context of software
engineering. We consider this mechanism as the baseline
reformulation technique, and compared with it using exper-
iments (Section 4.3). Yuan et al. [24] rst apply contextual
words from Stack Overow questions and answers for iden-
tifying semantically similar software-specic words. While
they introduce the idea, we successfully adapt that idea for
a software maintenance task, i.e., query reformulation for
concept location. From a technical point of view, we col-
lect candidate query terms opportunistically not only from
project source code but also from questions of Stack Over-
ow, and determine their relevance to the initial query us-
ing crowdsourced knowledge (i.e., adjacency list database).
We then apply the most relevant terms from both source
code and Stack Overow to query reformulation, and such
methodology was not applied yet by any existing studies.
6. CONCLUSION AND FUTURE WORK
Studies show that software developers face diculties in
preparing an appropriate search query from a change request
during concept location. In this paper, we propose a novel
technique{QUICKAR{that automatically suggests eective
reformulations for an initial query by leveraging the crowd
generated knowledge from Stack Overow. The technique
collects candidate query terms from both project source and
questions of Stack Overow, and then determines their ap-plicability for the reformulated query by applying the crowd-
sourced knowledge. Experiments using 510 change requests
from two software systems suggest that our technique can
improve or preserve the quality of 76% of the baseline queries
which is promising. Comparison with one baseline technique
also validates our preliminary ndings. While the prelimi-
nary ndings are promising, further experiments and inves-
tigations are warranted.
REFERENCES
[1] C. Carpineto and G. Romano. A Survey of Automatic Query
Expansion in Information Retrieval. ACM Comput. Surv. , 44
(1):1:1{1:50, 2012.
[2] B. Dit, M. Revelle, M. Gethers, and D. Poshyvanyk. Feature
Location in Source Code: a Taxonomy and Survey. Journal of
Software: Evolution and Process , 25(1):53{95, 2013.
[3] L. Erlikh. Leveraging Legacy System Dollars for E-Business. IT
Professional , 2(3):17{23, 2000.
[4] L. Favre. Modernizing Software & System Engineering Processes.
InProc. ICSENG , pages 442{447, 2008.
[5] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Du-
mais. The Vocabulary Problem in Human-system Communica-
tion. Commun. ACM , 30(11):964{971, 1987.
[6] G. Gay, S. Haiduc, A. Marcus, and T. Menzies. On the Use
of Relevance Feedback in IR-based Concept Location. In Proc.
ICSM , pages 351{360, 2009.
[7] S. Haiduc and A. Marcus. On the Eect of the Query in IR-based
Concept Location. In Proc. ICPC , pages 234{237, June 2011.
[8] S. Haiduc, G. Bavota, R. Oliveto, A. De Lucia, and A. Marcus.
Automatic Query Performance Assessment during the Retrieval
of Software Artifacts. In Proc. ASE , pages 90{99, 2012.
[9] S. Haiduc, G. Bavota, R. Oliveto, A. Marcus, and A. De Lucia.
Evaluating the Specicity of Text Retrieval Queries to Support
Software Engineering Tasks. In Proc. ICSE , pages 1273{1276,
2012.
[10] S. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. De Lucia, and
T. Menzies. Automatic Query Reformulations for Text Retrieval
in Software Engineering. In Proc. ICSE , pages 842{851, 2013.
[11] S. Haiduc, G. De Rosa, G. Bavota, R. Oliveto, A. De Lucia,
and A. Marcus. Query Quality Prediction and Reformulation for
Source Code Search: The Refoqus Tool. In Proc. ICSE , pages
1307{1310, 2013.
[12] M.J. Howard, S. Gupta, L. Pollock, and K. Vijay-Shanker. Au-
tomatically Mining Software-based, Semantically-Similar Words
from Comment-Code Mappings. In Proc. MSR , pages 377{386,
2013.
[13] K. Kevic and T. Fritz. Automatic Search Term Identication for
Change Tasks. In Proc. ICSE , pages 468{471, 2014.
[14] L. Mamykina, B. Manoim, M. Mittal, G. Hripcsak, and B. Hart-
mann. Design Lessons from the Fastest Q & A Site in the West.
InProc. CHI , pages 2857{2866, 2011.
[15] R. Mihalcea and P. Tarau. TextRank: Bringing Order into Texts.
InProc. EMNLP , pages 404{411, 2004.
[16] F. Moreno-Seco, L. Mico, and J. Oncina. A New Classication
Rule based on Nearest Neighbour Search. In Proc. ICPR , pages
408{411, 2004.
[17] S. M. Nasehi, J. Sillito, F. Maurer, and C. Burns. What Makes a
Good Code Example?: A Study of Programming Q & A in Stack
Overow. In Proc. ICSM , pages 25{34, 2012.
[18] L. Ponzanelli, A. Bacchelli, and M. Lanza. Seahawk: Stack Over-
ow in the IDE. In Proc. ICSE , pages 1295{1298, 2013.
[19] M. M. Rahman, S. Yeasmin, and C. K. Roy. Towards a Context-
Aware IDE-Based Meta Search Engine for Recommendation
about Programming Errors and Exceptions. In Proc. CSMR-
WCRE , pages 194{203, 2014.
[20] M. M. Rahman, C. K. Roy, and D. Lo. RACK: Automatic
API Recommendation using Crowdsourced Knowledge. In Proc.
SANER , pages 349{359, 2016.
[21] P. C. Rigby and M.P. Robillard. Discovering Essential Code
Elements in Informal Documentation. In Proc. ICSE , pages 832{
841, 2013.
[22] G. Sridhara, E. Hill, L. Pollock, and K. Vijay-Shanker. Iden-
tifying Word Relations in Software: A Comparative Study of
Semantic Similarity Tools. In Proc. ICPC , pages 123{132, 2008.
[23] J. Yang and L. Tan. Inferring Semantically Related Words from
Software Context. In Proc. MSR , pages 161{170, 2012.
[24] T. Yuan, D. Lo, and J. Lawall. Automated Construction of a
Software-specic Word Similarity Database. In Proc. CSMR-
WCRE , pages 44{53, 2014.
225