AutoConfig: Automatic Configuration Tuning for Distributed
Message Systems
LiangBao
School of Computer Science and Technology,
XiDianUniversity
Xi‚Äôan,ShaanXi,China
baoliang@mail.xidian.edu.cnXin Liu
Departmentof Computer Science,
Universityof California, Davis
Davis, California, USA
xinliu@ucdavis.edu
ZihengXu
School of Computer Science and Technology,
XiDian University
Xi‚Äôan, ShaanXi, China
13096956618@163.comBaoyinFang
School of Computer Science and Technology,
XiDianUniversity
Xi‚Äôan,ShaanXi, China
fbyxdu@163.com
ABSTRACT
Distributed message systems (DMSs) serve as the communication
backbone for many real-time streaming data processing applica-
tions. To support the vast diversity of such applications, DMSsprovide a large number of parameters to configure. However, Itoverwhelmsformostuserstoconfiguretheseparameterswellforbetter performance. Although many automatic configuration ap-proaches have been proposed to address this issue, critical chal-lengesstillremain:1)totrainabetterandrobustperformancepre-dictionmodelusingalimitednumberofsamples,and2)tosearchfor a high-dimensional parameter space efficiently within a time
constraint. In this paper, we propose AutoConfig ‚Äì an automatic
configuration system that can optimize producer-side throughputonDMSs.AutoConfigconstructsanovelcomparison-basedmodel
(CBM) that is more robust that the prediction-based model (PB-
M)usedbypreviouslearning-basedapproaches.Furthermore,Au-toConfig uses a weighted Latin hypercube sampling (wLHS) ap-proach to select a set of samples that can provide a better cov-erage over the high-dimensional parameter space. wLHS allows
AutoConfigtosearchformorepromisingconfigurationsusingthe
trainedCBM.WehaveimplementedAutoConfigontheKafkaplat-
form, and evaluated it using eight different testing scenarios de-
ployedonapubliccloud.ExperimentalresultsshowthatourCBMcanobtainbetterresultsthanthatofPBMunderthesamerandomforestsbasedmodel.Furthermore,AutoConfigoutperformsdefaultconfigurationsby215.40%onaverage,andfivestate-of-the-artcon-figurationalgorithms by 7.21%-64.56%.
Permission to make digital or hard copies of all or part of this work for personal or
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedfor profit or commercial advantage and that copies bear this notice and the full cita-tiononthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthan
ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orre-
publish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/or a fee.Request permissions frompermissions@acm.org.
ASE‚Äô18, September 3‚Äì7, 2018, Montpellier,France
¬© 2018Association for Computing Machinery.
ACMISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238175CCSCONCEPTS
‚Ä¢Software and its engineering ‚ÜíSearch-based software en-
gineering ;Softwaremaintenancetools ;‚Ä¢Computingmethodolo-
gies‚ÜíClassification and regressiontrees;
KEYWORDS
distributed message system, automatic configuration tuning,
comparison-based model, weighted Latin hypercube sampling
ACMReferenceFormat:
Liang Bao, Xin Liu, Ziheng Xu, and Baoyin Fang. 2018. AutoConfig: Auto-
matic Configuration Tuning for Distributed Message Systems. In Proceed-
ingsofthe201833rdACM/IEEEInternationalConferenceonAutomatedSoft-
ware Engineering (ASE ‚Äô18), September 3‚Äì7, 2018, Montpellier, France. ACM,
NewYork,NY,USA, 12 pages. https://doi.org/10.1145/3238147.3238175
1 INTRODUCTION
Distributedmessagesystems(DMSs),suchasKafka[33],RabbitMQ
[58], ActiveMQ [2], and RocketMQ [61], have been widely adopt-
ed as the underlying communication backbone to support manydifferent real-time stream data processing applications. These ap-plications have vastly diverse characteristics. To support such di-
versities,DMSsprovidealargenumberofparameters forusersto
configure. For example, both Kafka and RocketMQ have 200+ pa-rametersthat an application can configure[33,61].
The configuration of these parameters significantly affects the
applicationperformanceonDMSs[42,84].Forinstance,changing
thebatch.size parameter of Kafka for three different test cases (#3,
4,and7,seeTable2)from1to60canresultin5-10timesthrough-
putgaingivenanapplicationworkload,asshowninFigure1.Such
variationisevenmoresignificantiftheworkloadisrecurringona
daily base, which is likely, especially for realtime reactive applica-
tionssupportedbyDMSs.Ontheotherhand,whenfixingthevalueofbatch.size , changing the num.network.threads parameter from 1
to20onlyleadsto8%-50%throughputvariation,whichmeansthatnum.network.threads has much less influence on throughput than
thatofbatch.size .
Pursuing good performance of different applications on DMSs
is non-trivial as DMSs are complex systems with a large numberof configurable parameters that control nearly all aspects of their
29
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France L. Bao et al.


 7KURXJKSXW0%V
 7HVWFDVH

 
QXPQHWZRUNWKUHDGV

EDWFKVL]H0%   


 7KURXJKSXW0%V
 7HVWFDVH

 
QXPQHWZRUNWKUHDGV

EDWFKVL]H0%   

7KURXJKSXW0%V

7HVWFDVH
 
QXPQHWZRUNWKUHDGV

EDWFKVL]H0%   

Figure1: Performance surfaces of three differenttest cases on Kafka
runtime behaviors. Previous investigations have found that opti-
mizingaDMStomeettheperformancerequirementofanapplica-tionhasfarsurpassedtheabilitiesoftypicalsystemusers[84].Asaresult,usersoften(haveto)acceptthedefaultsettings[42].Alter-natively, many organizations choose to hire expensive experts toconfigure the DMSs for their applications. Unfortunately, manualconfiguring is labor-intensive, time-consuming, and often subop-timal. Therefore, there is a dire need for automatical application
configurationson DMSs.
Becauseconfigurationparametershavehighdimensionality,naive
exhaustive search is infeasible. Recently, learning-based configu-
ration has received much attention. In general, such an approach
constructsaperformance-predictionmodelusingtrainingsamplesofdifferentconfigurations,andthenexploresbetterconfigurationsusingsome searchingalgorithms. Although previousstudies onlearning-based configuration have shown promising results, onecriticalchallengeistoconstructbetterandmorerobustprediction
modelswithalimitednumberofsamples,becausegeneratingsam-
plesneedstorunDMSandistimeconsuming.Anotherchallengeistosearchformorepromisingconfigurationsinahigh-dimensionalparameterspace.
Toovercomethesechallenges,weproposeAutoConfig‚Äìanau-
tomaticconfigurationsystemthataimstoconfiguresystemparam-eters for a specific DMS within a given time constraint. AutoCon-fig consists of three important steps: initial sampling and modeltraining,explorationandexploitation(E&E)process,andbestcon-
figurationselection.First,weconstructacomparison-basedmodel
(CBM)fromexistingsamples.ThemotivationofconstructingCM-
B instead of a conventional prediction-based model (PBM) is that
the ultimate goal of configuring a DMS is to find the best config-urations rather than to predict the performance of a given systemunderdifferentconfigurations.Inanotherword,wecareabouttherelative performance instead of the absolute one. As discussed inSection4,CBMismorerobustthanPBM,becauseCBMusescom-binationsoftheoriginaltrainingsamples,whichresultsinasignif-
icant larger number of available samples than PBM. Furthermore,
AutoConfig searches for better configurations by integrating the
CBM and E&E process under the time constraint. The key of the
AutoConfig is to train a more robust CBM using combinations of
original samples, and to search for promising area of parameter s-
pace using skewed random sampling strategy (i.e. wLHS). It alsoneeds to balance the effort on the initial sampling and the E&Eprocess.
Insummary,our workmakes the followingcontributions:
‚Ä¢We propose a novel comparison-based performance model
(CBM),whichisdifferentfromtheconventionalprediction-
based model (PBM) used by previouslearning-basedapproaches. CBM allows us to obtain more ‚Äúcombinatorial‚Äùtraining samples from original samples, and thus can pro-
duce a more accurate and robust prediction model given a
number of original samples.
‚Ä¢WedeveloptheAutoConfigalgorithm.ItusesweightedLatinhypercube sampling (wLHS) to generate effective samplesin the high-dimensional parameter space,and multiplebound-and-searchtoselectpromisingconfigurationsintheboundedspacesuggestedbytheexistingbestconfigurations.
‚Ä¢WeevaluatetheperformanceofAutoConfigthroughexten-siveexperimentsusingeightdifferenttestingscenariosina
publiccloud.WeshowthatAutoConfigoutperformsdefault
configurationsby215.40%onaverage,andfivestate-of-the-art tuning algorithms by 7.21%-64.56%.
2 RELATEDWORK
DMS configuration has received much attention from both indus-try and academia. Previous studies on this problem can be classi-fiedintofourcategories:model-based,measurement-based,search-based, and learning-based configuration approaches, as discussednext. The best practices on DMS configuration are also reviewedatthe end of this section.
2.1 Model-based Configuration
In model-based configuration, an analytical model is constructedon the early-cycle of the development of a software system [80].Balsamoetal.[5]reviewedthemodel-basedsoftwareperformanceprediction methods before 2004. Koziolek et al. [38] surveyed themodel-basedperformanceevaluationmethodsforcomponent-basedsoftware systems. Commonly used models include queueing net-
work[43,44,47],PetriNet[20,35‚Äì37],PalladioComponentModel
(PCM)[59],and others [23,27, 48,56].
Model-based configuration has two main limitations. First, it
relies on analytical or design models derived from mathematicaltheories or software architecture abstraction, which are typically
30
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. AutoConfig: Automatic Configuration Tuning for Distributed Message Systems ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
coarse grained and could be imprecise [13]. Second, model-based
configurationcannotadaptasframeworksorhardwareevolve.Oncethearchitectureofaspecificsoftwaresystemchanges,theexistinganalytical models may not work and need to be redesigned. The
sameissuearisesasclustersevolvewithnewprocessors,memory,
andstorage technologies [77].
2.2 Measurement-based Configuration
Measurement-based configuration aims to evaluate software ap-
plication focusing on the performance quality features such as re-
sponsetimeandthroughput.Theseapproachesmainlyrelyonsta-tistical inferencing techniques to derive performance predictionsbasedonbenchmarkedmeasurementdata,suchas[4,9,10,15,17,21, 24, 39, 41, 55, 67, 70, 79, 89]. In this process, many strategies,suchassampling[12,46,68],profiling[13,87],symbolicevaluation[60], and feature interactions [69], are proposed to accelerate per-formance inference. The prevalent DMS benchmarks are SPECjm-s2007 [64], jms2009-PS [63], DDSBench [19], and two benchmark
suitesintroduced in [3, 11].
Measurement-basedapproacheshavetwomaindrawbacks.First,
they collect program profiles to identify performance bottlenecks,
which often fail to capture the overall program performance [13].
Second, most of these approaches lack generality [1], as they are
applicableonlytospecificapplicationscenariosorinfrastructures[15,85].
2.3 Search-based Configuration
Search-basedapproachesregardconfigurationproblemasablack-box optimization problem and uses search algorithms to solve it,such as in [26, 49, 51, 54, 72, 78]. The objective of the search isto evaluate candidate solutions of different parameters to mini-mizeanobjectivefunction.Thesearchstrategiesincluderecursiverandom search (RRS) heuristics [52, 86], evolutionary algorithms
[16, 53, 81], hill-climbing algorithms [83], and recursive bound &
search[90].
Search-based configuration is simpler and more general com-
paredtootherapproaches,becauseittakesthetargetsoftwaresys-temasablack-boxfunctionanddoesnotneeddetailedinformationabouttheinternals.However,theseapproacheshavetoruntheap-plication at each iteration of the search, which is time consuming
(andimpractical)whenoptimizinglarge-scaleproductionsystems.
2.4 Learning-based Configuration
Most relevant to our work is learning-based configuration. Theseapproaches try to construct performance prediction models firstby observing a large collection of running results under differentparameterconfigurations,andthenapplysomesearchalgorithms
tofindtheoptimalconfigurationbasedonthesemodels.Forexam-
ple,Sarkaretal.[65]adaptedprogressiveandprojectivesamplingstrategiestoperformancepredictionofconfigurablesystems.Guoet al. [22] used CART to predict the performance of configurablesystems.Zhangetal.[88]proposedanalgorithmbasedonFourierlearning (FL) for the performance prediction of configurable sys-tems.Sayyadetal.[66]employedacombinationofstaticandevo-lutionary learning of model structure to find sound and optimumconfigurations.Soltanietal.[71]employedanartificialintelligenceplanning technique to automatically select suitable features thatsatisfytheusers‚Äôbusinessconcernsandresourcelimitations.Tang[73]provided an approach to find better configurations using ran-domforestsmodelandgeneticalgorithmbasedsearchingstrategy.
Tantithamthavorn et al. [74] applied Caret method to investigate
the performance of defect prediction models. Nair et al. [50] pro-posed a spectral learning based method to explore the configura-tion space efficiently to find the measurement that reveal key per-formance characteristics. Jamshidi et al. [29] proposed a Gaussianprocesses (GPs)-based method to iteratively capture posterior dis-tributions of the configuration spaces and sequentially drive theexperimentation. Bei et al. [6] integrated the random-forest andgenetic algorithm to automatically configure the Hadoop config-
uration parameters for optimized performance for a given appli-
cation and cluster. Jamshidi et al. [30, 31] conducted an empiricalstudyonfourpopularsoftwaresystemstoidentifythekeyknowl-edge pieces that can be exploited for transfer learning. Chen et al.[14] proposed a transfer learning based approach to facilitate theconfigurationof large-scale computing systems.
Learning-basedapproachesrequireconsiderablenumbersofsam-
plestoconstructausefulperformance-predictionmodelforasoft-
ware system [51]. This requirement is challenging here because
only a limited set of samples can be acquired under a given timeconstraint,sinceitistimeconsumingtosampleonadeployedpro-duction system. To address this issue, we need to derive a robustprediction model by sampling the high-dimension configurationspace wisely, which motivated us to propose a novel comparison-based model and a weighted LHS sampling method.
2.5 Best Practices on DMS Configuration
There have been many best practices devoted to configure DMSsfromdifferentperspectives.Forexample,Johnetal.[32]focusedontwopopularDMSs(KafkaandAMQP)andexploredthedivergence
intheirfeaturesaswellastheirperformanceundervariedtesting
workloads. Rubio-Conde et al. [62] compared the performance of
twoopensourcemiddleware,namelyZeroCIceandAMQP,inthe
contextofmedicalsystems.Dobbelaereetal.[18]conductedaqual-itativeandquantitativecomparisonofthecommonfeaturesofthetwomessagesystems,namelyRabbitMQandKafka.Prazeresetal.[57]proposedamessage-serviceorientedmiddlewarenamedFoT-MSOMfortheFogofThings(FoT)paradigm.Xuetal.[84]providedquantitativeevidencefortheover-deliveredflexibilityrepresented
by configuration parameters through the study of the large-scale
configurationsettingsofrealusers.Leetal.[42]madeathoroughevaluation of different configurations and performance metrics ofKafka in order to allow users to avoid bottlenecks, and leveragesomegood practice for efficient streamprocessing.
3 PROBLEM STATEMENT
In this paper, we study the configuration problem for distributedmessage systems (short for CDMSproblem). As illustrated in Fig-
ure 2, a distributed message system (e.g. Kafka, RocketMQ, Rab-bitMQ, etc.) is often deployed on a cloud environment comprised
ofacollectionofinterconnectedvirtualmachines(VMs).Itserves
as a real-time streaming data pipelines that transfer messages formany systems. Once a DMS has been deployed, the user needs to
31
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France L. Bao et al.
Application
Producer
Producer
ProducerConsumerConsumer
Distributed Message System
(DMS)
Configuration 
ParametersTransaction 
System
Cloud 
Environment
Figure2: Overviewof CDMSproblem
specify its configurations according to the specific application s-
cenario, and such configurations have a significant impact on theperformance of the DMS [3,42,62, 84].
The goal of CDMSproblem is to find an optimal configuration
thatmaximizesthe(producer-side)throughputofaDMSwithinapredefined time period, given a specific DMS, an application, and
the underlying runtime environment. The reason we choose the
throughput metricisbecauseitcantellhowmuchdataaDMScan
storeinaunitoftime,whichcharacterizesthemostimportantca-pabilityofaDMS.Othermeasurement,suchasend-to-endlatency
(i.e. how long does it take for a message to go though the DMS),
can be also introduced as a target metric, because our approach is
with generality, and is independent to the metric being optimized.
Specially, CDMSproblemhas the followingcomponents.
Application: An application represents a real-time streaming
dataproducingsystem(producers)thatwantstotransfermessagestotransaction systems (consumers).Wemodel it as a 5-tupleA=/angbracketleftl,p,s,r,q /angbracketright, where lrepresents the length of a message; pis
the number of producers; sindicates the message sending mode
(synchronous or asynchronous); rdenotes the message receiving
mode (with or without acknowledgement); and qdesignates the
number of available servers(queues).
Runtime Environment : Runtime environment is the execu-
tionenvironmentprovided to a DMS. Wemodel it as a 5-tuple
E=/angbracketlefto,f,m,d,w /angbracketright, where odenotes the number of CPU cores; f
is the CPU frequency; mrepresents the physical memory size; d
indicates the available disk space; and wreflects the networking
setup.
Configuration and Throughput: Let C=(c
1,c2,¬∑¬∑¬∑,cn)be
the configuration of a DMS. For example, one can configure as
many as 200+ parameters in a Kafka platform, such as the num-ber of network processing threads, the number of I/O processingthreads, maximum number of requests in a queue, memory buffersize, etc., as shown in Table 1. Given a configuration Cfor an ap-
plication Aand its environment E, the throughput is denoted as
TP(A,E,C).
TimeConstraint :Inpractice,thetimeforconfigurationtuning
isoftenrestricted.Wedefinethisrestrictedtuningtimeas timecon-
straint, denoted as TC. Any solution to the CDMSproblem must
terminatewhen TCis met.
Best
ConfigurationAutoConfig
Application
RuntimeDMSTime
 Constraint Initial Sampling 
and Model-
training
Exploration and 
Exploitation
Best 
Configuration 
SelectionIterationsTransaction 
Systems
Figure3: Overviewof AutoConfigSystem
Insummary,the CDMSproblemcan be stated as follows:
max
C‚ààCBTP(A,E,C) (1)
s.t.tuning time ‚â§TC (2)
where(1)statesthatthegoalof CDMSproblemistofindaconfigu-
rationCthatmaximizesthroughputofaDMSforagivenapplica-
tionAanditsenvironment E.InC,thevalueofeachcomponent
parameter cimust be within CB, the configuration bound prede-
fined by the DMS. The constraint (2) says that any tuning processofa solution must terminate after a TCamountof time.
4 AUTOCONFIGAPPROACHFOR CDMS
PROBLEM
The key idea of our approach is to generate a set of samples thatis used to train a comparison-based model, and search for morepromising configurations using the trained model. Figure 3 illus-tratesthreeimportantstepsonourproposedAutoConfigapproach.
Initial sampling and model-training . This step generates a
set of different configurations using classic Latin hypercube sam-
pling (LHS) method first, given the configuration bound (CB) for
each parameter. It then runs the target application on the DMS
withtheseconfigurations,collectthroughputvalues,andgenerate
the initial training set. Finally, we choose the best of bconfigura-
tionsto generate the initial good configuration set.
Exploration and exploitation. The purpose of this step is to
searchforbetterconfigurationsbyintegratingthecomparison-basedperformance model and the exploration and exploitation (E&E)process under the time constraint. More specifically, we first es-
timate the weight value for each configuration parameter using
Lassomethodandthetrainingset,andinitiateourweighted-LHS(wLH-S)method.Intheexplorationphase,werandomlygenerateasetofconfigurationsusingourwLHSwithinCBtoformtheexplorationconfigurationset,andtrainacomparison-basedmodel(CBM)fromthetrainingset.Intheexploitationphase,weapplyawLHS-basedbound and search algorithm (BS) [90] to find potential better con-figurationsnearknowngoodconfigurationsusingthetrainedCB-
Mand generate the exploitationconfiguration set.
Bestconfigurationselection .Ateachiteration,weupdatethe
goodconfigurationsetbyselectingthebest bconfigurationsusing
32
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. AutoConfig: Automatic Configuration Tuning for Distributed Message Systems ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
the trained CBM from the current good configuration set, the ex-
plorationconfigurationset,andtheexploitationconfigurationset.Ifourtimeconstraint(TC)permitsmoretests,werepeatourexplo-rationandexploitationprocess.Otherwise,weruntheapplication
on the DMS with every configuration in the good configuration
set,and returnthe best one.
4.1 Comparison-based Model
Thekeystepofalearning-basedconfigurationapproachistocon-structaperformance-predictionmodelfortheDMS.Typically,sucha model is evaluated based on its accuracy or error. The error per-centagecan be computed as:
MMRE =|predicted ‚àíactual|
actual¬∑100 (3)
In this paper, we propose a novel approach that constructs a
comparison-basedmodel (CBM)whichisdifferentfromtheprediction-
based model (PBM) used by most of the previous learning-basedapproaches as described in Section 2.4. Specially, given the fixedAandE, and two different configurations C
1andC2, our model
(denoted as f:C√óC‚Üí{0,1}) can compare the throughput
values of the DMS with C1andC2:
f(C1,C2)=/braceleftBigg
1,T P(A,E,C 1)‚â§TP(A,E,C 2)
0,T P(A,E,C 1)>TP(A,E,C 2)(4)
Thereareanumberofadvantagesofusingacomparison-based
approach:
‚Ä¢Performance comparison is extremely robust since it is only
mildly affected by outliers [51]. In the context of parame-ter configuration, a practitioner is often more interested inknowingtherank(bycomparison)ratherthanthepredictedperformance scores.
‚Ä¢Performance comparison is the ultimate goal of our CDMSproblem.Ausermayjustwanttoidentifythebestconfigura-tions rather than to predict the performance of a given sys-
tem under different configurations. For example, an admin-
istratortryingtooptimizeaKafkaplatformistosearchforasetofconfigurationsthatcanobtainmaximumthroughput,andis not interested in the whole configuration space.
‚Ä¢Performance comparison increases the number of availabletraining samples significantly . Suppose we have a set con-
tainingnsamplesbyrunning ndifferentconfigurations.We
can generate/parenleftbig
n
2/parenrightbig
=n(n‚àí1)
2samples by simply grouping
any two configurations and their throughput values. We
willdemonstrateintheexperimentsectionthatthecomparison-
basedmodelhasbetterperformancethanconventionalprediction-basedapproaches,sincethenumberoftrainingsamplesavail-abletoconstructacomparison-basedapproachisincreasedconsiderably.
Besides,insteadofusingresidualmeasuresoferrors,asdescribed
inEquation(3),whichdependonresiduals(r =actual‚àípredicte ),
we use a rank-based measure [50]. Once the comparison-based
model is trained, the accuracy of the model is measured by sort-ingthe throughputvalues of ndifferentconfigurations, that is:
TP(A,E,C
1)‚â§TP(A,E,C 2)‚â§¬∑¬∑¬∑‚â§TP(A,E,C n)(5)Parameter XParameter Y
Figure4: Twosets of LHS samples for a 2D space
Obviously, Equation (5) can be generated by simply using somesortalgorithm (e.g.quick sort) and continuously applying ourcomparison-basedmodeltocomparethethroughputvaluesbetweentwo configurations. After obtaining the predicted rank order, it iscomparedtotheactualrankorder.Therankaccuracy(RA)isthuscalculated using the mean rank difference:
RA=1‚àí1
k¬∑k/summationdisplay
i=1|rank(yi)‚àírank(TP(A,E,C i))|
n‚àí1(6)
wherenisthenumberofallconfigurations, kisnumberofoptimal
configurationswewanttofindfromthese ncandidates.Thismea-
sure simply counts how many of the pairs in the first kelements
of the test data were ordered incorrectly by the prediction model
andmeasuresthe averageof magnitude of the ranking difference.
4.2 Weighted Latin Hypercube Sampling
OneimportantcomponentinourAutoConfigalgorithmisthesam-
pling strategy. Because the configuration parameter space is highdimensional, naive sample methods such as random or grid sam-pling can become very expensive, and is hard to provide a goodcoverage in it, especially with a small number of samples [86, 90].
To address this issue, we note that the Latin hypercube sampling(LHS) performs better, compared to random or grid sampling, be-
cause it allows each of the key parameters to be represented in afullystratifiedmanner,nomatterwhichparametersareimportant
[45]. Specifically, LHS divides the range of each parameter into k
intervals and take only one sample from each interval with equal
probabilities[45].Figure4illustratestwosetsofLHSsampleswithfiveintervalsina2Ddimension,onedenotedbydotsandtheotherbytriangles.
TomakeLHSmoreefficient,weobservethattheperformanceof
a DMS is often dominated by a few key configuration parameters
(e.g.seeFigure1),andtheimpactofaninfluentialparameter‚Äôsval-
uesontheperformancecanbedemonstratedthroughcomparison-
sofperformances,regardlessotherparameters‚Äôvalues[90].Based
onthisobservation,weproposedweightedLHS(wLHS),whichex-tendsthestandardLHSalgorithmtotakeintoaccountknowledgeabout the correlations between configuration parameters and sys-temperformance.Suchcorrelationscorrespondtoalinearapproxi-mationoftheobjectivefunction,whichcanbeestimatedusingthesampled points. The correlation information (i.e. weights) can be
33
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France L. Bao et al.
combinedwiththeLHStogenerate skewedrandomsearchsamples
thatarelikely to lead to moreefficient searches.
Among many models, we adopt linear regression, a simple yet
effectivestatisticalmethod,tomeasurethestrengthoftherelation-
shipbetweenoneormoredependentvariables(y)andeachofthe
independent variables (x). These relationships are modeled usingalinearpredictorfunctionwhoseweights(i.e.,coefficients)arees-timated from the data. More specifically, wLHS employs a regu-larized version of least squares, known as Lasso [75], to estimatetheweightforeachconfigurationparameter.Lassocanreducetheeffectofirrelevantvariablesinlinearregressionmodelsbypenaliz-ingmodelswithlargeweights.ThemajoradvantageofLassooverother methods is that it is interpretable, stable, and computation-
ally efficient [75]. There are also many practical and theoretical
studies that have proven its effectiveness as a promising featureselection algorithm [76].
Lasso performs L1 regularization, which adds a penalty equal
to the sum of absolute weights times a constant Œªto the regres-
sion error. This type of regularization can result in sparse models
withfewnonzerocoefficients.Therefore,Lassoregressionisableto
perform variable selection in the liner model. Variables with non-zeroregressioncoefficientsvariablesaremoststronglyassociated
withtheresponsevariable. Œªistheturningfactorthatcontrolsthe
amount of regularization. As the value of Œªincreases, more coef-
ficients will be set to zero (provided fewer variables are selected)andsoamongthenonzerocoefficients,moreshrinkagewillbeem-ployed.wLHSusestheLassopathalgorithmtodeterminetheorderofimportanceoftheconfigurationparametersofaDMS.Thealgo-rithmstartswithahighpenaltysettingwhereallweightsarezeroand thus no configurations are selected in the regression model.It then decreases the penalty in small increments, recomputes the
regression, and tracks what configurations are added back to the
model at each step. wLHS uses the order in which the configura-tions first appear in the regression to determine how much of animpact they have on the target metric (e.g., the first configurationselected is the most important).
The key ideas in wLHS is that samples should follow the corre-
lation pattern exhibited by the Lasso-based weight calculation. Ifthe past measurements show that smaller values of configurationc
i‚ààCtendtomaketheperformancebetter(i.e.,astrongpositive
correlation), then smaller values are more important than largerones. Hence we should sample more on the smaller values for pa-rameterc
i.
To realize the above weighted sampling idea, we use a truncat-
edexponentialdensityfunctionproposedbyXi[83]forgeneratingthe samples. For each component parameter c
iof a configuration
(i=1,2,¬∑¬∑¬∑,n),weassumeatruncatedexponentialdensityfunc-
tionof the form:
f(c,d,x)=de‚àíœâicx(7)
on sampling range x=[A,B], whereœâiis determined by Lasso
method and represents the correlation between performance andc
ithroughallpastobservations.Parameter cisusedtoreflecthow
aggressivetheuserwantstheweightedsamplingtobe.Parameterdis the normalizing factor so that f(c,d,x)isa density function.
While sampling, we need to divide the interval [A,B]intok
intervals with equal probability 1/k. Letz
jbe thej-th dividing0 2 04 06 08 0 1 00
Parameter values00.010.020.030.040.050.06Sampling possibility
Figure5: Applying wLHS to a parameter ciwithœâi=0.7
point, with j=1,2,¬∑¬∑¬∑,kandz0=Aandzk=B. Then
j
k=/integraldisplayzj
z0f(c,d,x)dx (8)
Wenowneedtodrawonepoint Œµjfromgiveninterval [zj,zj+1]
which follows the conditional probability f(c,d,x)=h, where h
isthe scaling constant:
1=/integraldisplayzj+1
zjf(c,d,x)
hdx (9)
TodrawŒµj,wefirstdrawarandomnumber u,fromtheuniform
distributionin [0,1]andneed to satisfy the followingequation:
u=/integraldisplayŒµj
zjf(c,d,x)
hdx (10)
Finally,wehave:
Œµj=‚àílog(e‚àíœâiczj‚àíu(e‚àíœâiczj‚àíe‚àíœâiczj+1))
œâic(11)
Thesolutionsforintermediatevariables d,zj,hcanbefoundin
[83]. It is worth noting that the weighted LHS takes advantage of
knowledge about the correlations between configuration parame-
ters and system performance from past observations. The weight
can be based on other metrics (rather than correlation), also onecan choose other density functions rather than exponential to re-alize a similar concept of assigning different weights on differentsamplingregions[83].
To explain our wLHS method more intuitively, suppose there
is a particular parameter c
iwith a positive correlation œâi=0.7
tothethroughput.wLHSthatusesthetruncatedexponentialden-sity function (Equation (7)) would divide the sampling space, say
[1,100],into3equal-probabilityintervals,asshowninFigure5.We
can observe from Figure 5 that clearly smaller values are stressedmore under weighted sampling. The constant c=5in Equation
(7) can be determined through some preliminary studies. Larger c
wouldresultin moreaggressiveweighted sampling [83].
34
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. AutoConfig: Automatic Configuration Tuning for Distributed Message Systems ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
4.3 AutoConfigAlgorithm
Basedonthecomparison-basedmodelandtheweightedLHSmethod,
wecanimplementourAutoConfigalgorithm.Thedetailedprocess
isspecified in Algorithm 1.
Algorithm1 AutoConfig(DMS,CB,TC)
Require: DMS: the target DMS; CB: configuration bounds; TC:
time constraint.
1:Generate hdifferentconfigs using LHS within CB;
2:Run these hconfigs on DMS, collect throughput results, and
generate the initial training set T;
3:B‚Üêthebestbconfigs in T;
4:whileTCpermits moretests do
5:Calculate weight set Œ©for config parameters using Lasso
method and T;
6:Generate hdifferentconfigsusingwLHSand Œ©withinCB,
and then choose bout ofhconfigs randomly;
7:Run these bconfigs on DMS, collect throughput results,
and generate the explorationset EP;
8:T‚ÜêT/uniontextEP;
9:Traina comparison-based prediction model MusingT;
10:foreachconfig Ci‚ààBdo
11: Set exploitationset EI‚Üê‚àÖ;
12: Selecthconfigs using wLHS in the bounded space of
Ci;
13: Choose the best config C‚àó
ifrom these hconfigs using
M;
14: RunDMSwithC‚àó
i, collect the throughput TP‚àó
i;
15: EI=EI/uniontext{(C‚àó
i,TP‚àó
i)};
16:end for
17:B‚Üêthebestbconfigs from B/uniontextEP/uniontextEI;
18:T‚ÜêT/uniontextEI;
19:end while
20:returnThebest config in B;
AutoConfig initially samples hconfigurations using standard
LHS(line1)andpicks bconfigurationswiththebestperformance
(line 2-3). With these hinitial samples, we use Lasso method to
calculate weight set Œ©for configuration parameters (line 5). To
explore the configuration space, we apply wLHS with Œ©andhin-
tervals, and choose bconfigurations randomly (line 6). After that,
werun these bconfigurations on the DMSand use the resultsto
generatethe explorationset EP (line 7).
Based on already generated samples, we train a comparison-
based prediction model based on random forests (line 9), which
indicatesbetterperformanceamongmanydifferentmachinelearn-ingalgorithms.
Toexploitthepreviously-foundbestconfigurations,weapplya
bound and search (BS) algorithm [90] to find potential better con-
figurations near already known good configurations (line 10-16).
This strategy works well in practice because there is a high possi-bilitythatonecanfindotherconfigurationswithsimilarorbetterperformancesaroundtheconfigurationwiththebestperformanceinthesampleset[90].Morespecifically,foreachconfiguration C
i
inB, BS generates another set of samples in the bounded space
aroundCi:Foreachparameter cjinCi,BSfindsthelargestvalueC1C3
C2
C4
C5
Parameter XParameter YBounded 
Space
Figure6:Theboundandsearch(BS)algorithmina2Dspace
cl
j(lower bound) that is represented in Band is smaller than that
ofCi. It also finds the smallest value cuj(upper bound) that is rep-
resented in Ciand that is larger than that of Ci. The same bound-
ing mechanism are carried out for every component parameter in
Ci. Figure 6 illustrates this bounding mechanism of BS with 2D s-
pace(fivedots C1‚ÄìC5).Afterdeterminingtheboundfor Ci,weuse
wLHS again to divide each bound into hintervals and generate h
samplescloseto Ci(line12).Giventhese hconfigurations,weuse
the trained prediction model Mto choose the best configuration
C‚àó
i(line 13), and then collect the corresponding throughput TP‚àó
i
by running the DMSwithC‚àó
i(line 14). After that, the sample
(C‚àó
i,TP‚àó
i) is added to the exploitation set EI(line 15). We repeat
theseboundandsearchstepsuntileveryconfigurationin Bistest-
ed,andupdate Bwiththebest bconfigurationsfrom B/uniontextEP/uniontextEI
(line 17). We refine MandŒ©by adding new samples from explo-
rationandexploitationphasestothetrainingset T(line8and18).
Oncethetimebudgetonexplorationandexploitationismet,we
stopsearchingand returnthe best configuration in B(line 20).
To satisfy the overall time constraint, we divide the AutoCon-
figalgorithmintotwophases,i.e.initialsampling,andexplorationand exploitation. Suppose the time constraint is TC, the propor-
tion of time spent in these two phases is denoted as Œ±andŒ≤re-
spectively, where Œ±+Œ≤=1and0‚â§Œ±,Œ≤‚â§1. Let the average
timeofrunninganapplicationwithoneconfigurationonthe DMS
equals to t, we have h‚âà‚åä
Œ±‚àóTC
t‚åãand andb‚âà‚åäŒ≤‚àóTC
2*iter‚àót‚åã, where
iterrepresent the approximate iterations in the exploration and
exploitation phase, and h,bare hyperparameters in AutoConfig
algorithm.
5 EXPERIMENTS
We have implemented our approach and conducted extensive ex-
perimentsunderdifferenttestingscenarios.Thesourcecodecanbefoundinourprojectwebsite:https://github.com/sselab/autoconfig.In this section, we first describe our experiment setup, and thenpresent the experimental results to prove the efficiency and effec-tivenessof the proposed approach.
5.1 Experimental Settings
Runtime environment. We conduct our experiments on a publiccloudinfrastructurenamedAliyun
1.Weuse4AliyunECSinstances
1https://www.aliyun.com
35
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France L. Bao et al.
Table1: Performance-relevantparameters in Kafka
Parameters Default Value
num.network.threads 3
num.io.threads 8
queued.max.requests 500
num.replica.fetchers 1
socket.receive.buffer.bytes 102400
socket.send.buffer.bytes 102400
socket.request.max.bytes 104857600
buffer.memory 33554432
compression.type 16384
batch.size 0
linger.ms none
consisting of two types: 1 memory type instance (r5) for produc-
ers,and3generaltypeinstance(g5)forDMSnodes.Theproducernodeisequippedwithan8-coreIntelSkylakeXeonPlatinum8163
2.5GHz processor, 16GB RAM, and 50G disk. Each of the DMS n-
odes is equipped with a 4-core Intel Skylake Xeon Platinum 81632.5GHz processor, 8GB RAM, and 50G disk. Both instances haveCentOS6.8(64bit)installed.AlloftheVMinstancesareconnected
via a high-speed1.5Gbps LAN.
System and configuration setup . We choose Kafka as our experi-
mental system. Kafka is a distributed streaming platform for pub-lishing and subscribing to streams of records, which is similar toa message queue or enterprise messaging system. We choose Kaf-
kabecauseitisawidelyadoptedopen-sourcedistributedmessage
system.
BasedontheKafkamanual[34]andpreviousstudies[18,42,82],
we identify 11 out of 200+ parameters that are considered critical
to the performance of Kakfa platform, as listed in Table 1. Notethat even with only 11 parameters, the search space is still enor-mous, and exhaustive search is infeasible. The reason we choosea small subset of parameters that have great impact on perfor-mance instead of all configurable parameters is because reducing
the number of tuning parameters can reduce the search space ex-
ponentially, and most existing automatic configuration approach-es [18, 42, 82] also adopt this feature selection strategy. It‚Äôs worthnoting that because of the linear computing complexity of wLHSonthenumberofparameters,ourapproachisgeneralenoughand
worksfor high-dimensional parameter spaces.
Benchmark. Based on the previous Kafka testing experiences
[25, 40], we design eight testing scenarios with the combination-s of different numbers of producers, message sizes, and message
acknowledgement modes. Table 2 lists these testing setups.
5.2 Baseline Algorithms
To evaluate the performance of AutoConfig, we compare it with
five state-of-the-art algorithms, namely random search [7], Best-Config[90],RFHOC[6],Hyperopt[8],andSMAC[28].Weprovide
abriefdescriptionforeachalgorithmandreportitshyperparame-ters(if necessary)as follows:
Randomsearch(Random) isasearch-basedapproachthatex-
plores each dimension of parameters uniformly at random. It ismore efficient than grid search in high-dimensional configurationspaces,and is a high-performance baseline,as suggested in [7].Table2: Testingscenarios of our experiment
No.# of Producers MessageSize AckModes1
1 1 0.1KB -1
2 1 1.0KB -1
3 1 0.1KB 1
4 1 1.0KB 1
5 3 0.1KB -1
6 3 1.0KB -1
7 3 0.1KB 1
8 3 1.0KB 1
1In Kafka,Ack=1means that the leader will write the recordto its local
logbut will respond without awaiting full acknowledgement fromall
followers.Ack=-1means that the leader will wait for the full set ofin-syncreplicasto acknowledge the record.For moredetails, see in [34].
BestConfig2is a search-based approach that uses divide-and-
divergesamplingandrecursivebound-and-searchalgorithmtofind
abest configuration. Wefollowthe suggestions in [90].
RFHOC is a learning-based approach that constructs a predic-
tion model using random forests, and a genetic algorithm to auto-maticallyexploretheconfigurationspace.Weusethehyperparam-
eterssuggested in [6].
Hyperopt
3isalearning-basedapproachbasedonBayesianop-
timization. It is widely used for hyperparameter optimization. We
usethe suggested settings in [8].
SMAC4is a learning-based method using random forests and
anaggressiveracing strategy.
InAutoConfigalgorithm,wesetthenumberofiterationsto 100
forrandomforestmodel,anddonotlimitthedepthofthedecisiontree and the number of available features for the tree; the propor-tionsoftimespentininitialsamplingandE&Ephasesare0.4and
0.6(i.e.Œ±=0.4,Œ≤=0.6);thevaluesofothertwohyperparameters,i.e.
h,andbforeach testing scenario areset to 10 and 5, respectively.
For each run in our experiments, every algorithm is executed
under the same time constraint and stops once the constraint is
met.
5.3 Evaluation Metrics
We consider two performance metrics in our experiments for per-
formance evaluation, namely rank accuracy (RA) and throughput
(TP).RAisametricthatevaluatesthepredictionmodel,andTPis
theultimate performance metric for AutoConfig.
Rank accuracy (RA). As defined in Section 4, RA is used here
toevaluatethequalityofapredictedranking(theprediction)fora
set of configurations by comparing it with the corresponding real
ranking (the truth). Note again that the ranking-ability of a pre-diction model is more important than its prediction accuracy in
AutoConfig, because we need to use a prediction model to choose
the best configurations in each exploitation phase of Algorithm 1
(line 13 in Algorithm 1).
Throughput (TP) . TP is the rate of successful message deliv-
erytoadistributedmessagesystem(i.e.producer-sidethroughput).
2Code is available from:https://github.com/zhuyuqing/bestconf
3Code is available from:http://jaberg.github.io/hyperopt/
4Code is available from:http://www.cs.ubc.ca/labs/beta/Projects/SMAC
36
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. AutoConfig: Automatic Configuration Tuning for Distributed Message Systems ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
Table 3: The RA values of random forests based CBM and
PBM
No.#of Samples in
TrainingSetRA
(CBM)RA
(PBM)Best?1
(CBM)Best?
(PBM)
1 10 0.8592 0.8490 No No
2 20 0.8469 0.7959 Yes No
3 30 0.9163 0.7918 Yes No
4 40 0.9000 0.8653 Yes Yes
5 50 0.9408 0.9347 Yes No
6 60 0.9020 0.7020 Yes Yes
7 70 0.8673 0.8408 Yes Yes
8 80 0.9224 0.8857 Yes Yes
9 90 0.9306 0.8327 Yes Yes
10 100 0.9122 0.8592 Yes Yes
1Whetheror not the model can find the best configuration in the test
TheTPimprovementofanalgorithmsoverthebaselinealgorithm
incomparison is defined as:
Imp(baseline)=TP‚àíTPbaseline
TPbaseline√ó100%,
whereTPbaselineisthethroughputofthebaseline,and TPisthat
ofthe algorithm being evaluated.
To ensure consistency, we run each application five times and
calculate the average of these five runs. The standard deviation ofthe execution time is 0.02 or smaller, which indicates the stabilityofthe performance.
5.4 Experiment Results
RA.Thefirstexperimentistoevaluatethequalityofrankingwithdifferent prediction models. We first construct a sample pool by
running Kafka with randomly generated 150 different configura-tions. After that, 50 samples are selected randomly and then re-movedfromthepooltogeneratethe testingset.Last,werandomly
choose10,20, ¬∑¬∑¬∑,100samplesfromthepooltoconstruct10differ-
enttrainingsets.Tworandomforestsbasedpredictionmodels,one
is a comparison-based model (CBM) and another is a prediction-basedmodel(PBM),aretrainedusingthesetrainingsets.Wethenuse these trained models to find the best 10 configurations fromtesting set, respectively. Table 3 lists the RA values on CBM and
PBM. We find that all these RA values of CBM outperform those
of PBM, and the average improvement is 5.3%. The results also in-dicatethattheCBMcanfind9bestconfigurationsoutof10exper-iments,wherePBM only finds 6 out of 10.
Throughput. Given a fixed time constraint (TC) for each test-
ing scenario, we run six different tuning algorithms plus defaultconfiguration independently. Table 4 lists the throughput (MB/s).
Asexpected,thedefaultconfigurationdoesnotperformwell.Our
algorithm achieves an average of 215.40% improvement over the
default configurations. Furthermore, AutoConfig outperforms al-
lotherfivealgorithms:11.95%-25.22%improvementoverRandom,
7.21%-27.45%improvementoverBestConfig,9.52%-38.44%improve-
ment over RFHOC, 10.41%-49.58% improvement over Hyperopt,and9.66%-64.56% improvementoverSMAC.
Finally, we plot the overall throughput improvement percent-
age of BestConfig, RFHOC, Hyperopt, SMAC and AutoConfig inFigure 7, using the random algorithm as the baseline. In the Fig-ure7,x-axisliststheeighttestingscenariosand y-axisrepresents
the improvement percentage over the random algorithm. We ob-serve that compared with the random algorithm, our approach
achieves11.95%‚Äì35.51%improvementamongalltestingscenarios.
AutoConfigachievesanaverageof20.78%improvementoverRan-
dom, 17.71% improvement over BestConfig, 22.03% improvement
over RFHOC, 20.52% improvement over Hyperopt, and 23.68% im-provement over SMAC. We can conclude from Figure 7 that Au-toConfig achieves stable and significant improvements compared
with the other five algorithms. Another interesting observation
fromFigure7isthattherandomsearchachievessurprisinglygoodresults in our experiments. This is consistent with the findings of
Bergstraand Bengio in [7].
5.5 Threatsto Validity
Internal validity : To increase internal validity, we performed a
controlledbenchmarkexperimentbyexecutingeachtestcasefive
times and calculate the average of these five runs. Such methodcan avoid misleading effects of specifically selected test cases andensuresthestabilityofthethroughputresultforeachscenario.Inaddition, we use the same sample set and the same random-forestmodel with identical hyperparameter values to compare the rankaccuracy for the CBM and the PBM in each test case. Such results
are reliable and can be treated as the ground truth. In our experi-
ments, the value of the time proportion Œ±andŒ≤(they control the
time ratio between model training and E&E phases, and in turndecide the hyperparameters handbof AutoConfig) are set to 0.4
and 0.6 respectively, and the value of constant cin Equation (7) is
setto5(assuggestedin[83]).However,thevaluesoftheseparam-eters are domain-specific, and can be set by a domain expert. N-evertheless,incaseswheretheprecisevaluesareunknown,usingthemiddlevalue(for Œ±andŒ≤)andthevaluesuggestedbyprevious
study [83] (for c) seems to be reasonable choices. In addition, we
tried multiple values of these parameters in our experiments andobservedthatthegoodvaluesoftheseparametersthatcanleadtobetter throughputresultsaredifferentfromtest case to test case.
Externalvalidity :Weaimedatincreasingexternalvalidityby
choosingeighttestingscenarioswithvariousapplication-levelpa-rameters on a mainstream open source software Kafka. Further-more, we are aware that because the two key components of Au-toConfig,i.e.comparison-basedmodelandweightedLHS,aregen-
eralenoughandindependenttothesystemundertune(SUT)[90],
theresultsof our evaluationsaretransferable to other DMSs.
6 CONCLUSION
In this paper, we propose AutoConfig ‚Äì an automatic configura-tion system to optimize throughput for DMSs. AutoConfig con-structs a novel comparison-based model (CBM) different from theprediction-basedmodel(PBM)usedbypreviouslearning-basedap-proaches. Intensive experiments show that our CBM can obtainbetter results than that of PBM, under the same random forestsbasedmethodandagiventimeconstraint.Furthermore,theAuto-
ConfigalgorithmusesweightedLHSmethodtoselectasetofsam-
ples that can provide a better coverage over the high-dimensionalparameter space, and searches for more promising configurations
37
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France L. Bao et al.
Table4: Throughputresults(MB/s) fromdifferentalgorithms with fixed time constraints
No.TC (h)Default
Imp(Default)Random
Imp(Random)BestConfig
Imp(BestConfig)RFHOC
Imp(RFHOC)Hyperopt
Imp(Hyperopt)SMAC
Imp(SMAC)AutoConfig
154.28
382.48%17.28
19.50%17.46
18.27%17.57
17.53%17.25
19.71%17.41
18.61%20.65
257.09
426.80%30.08
24.17%29.39
27.08%28.18
32.54%24.97
49.58%28.34
31.79%37.35
3518.20
47.20%19.77
35.51%21.02
27.45%20.41
31.26%20.89
28.24%21.00
27.57%26.79
4524.85
52.76%31.21
21.63%31.53
20.39%32.80
15.73%32.65
16.26%32.56
16.58%37.96
556.42
448.60%31.46
11.95%31.50
11.81%25.44
38.44%31.90
10.41%30.34
16.08%35.22
6510.18
293.12%31.96
25.22%37.33
7.21%32.55
22.95%35.81
11.76%24.32
64.56%40.02
7545.5
37.03%54.87
13.63%56.27
10.81%54.31
14.80%53.96
15.55%56.86
9.66%62.35
8568.35
44.59%86.24
14.60%85.37
15.77%90.24
9.52%83.82
17.91%82.06
20.44%98.83
12345678Testing Scenarios-30-20-10010203040Imp(Random)BestConfig RFHOC Hyperopt SMAC AutoConfig
Figure7: Performance comparison among differentalgorithms
usingthetrainedCBM.IndepthexperimentsonAutoConfigdemon-
strateitssuperiorperformancetoexistingfivestate-of-the-artcon-figurationalgorithms on eight differenttesting scenarios.
Our further work includes refining our AutoConfig approach
by supporting the automatic selection of the appropriate hyper-parameters (i.e. Œ±,Œ≤, andc) given a specific test case and a time
constraint.WewillalsoinvestigatethepossibilityofapplyingourapproachtootherDMSssuchasRabbitMQ,ActiveMQ,andRocket-MQ.Last, wehope to abstract the proposed algorithm and releaseit as an automatic parameter configuring service for other config-urablesoftwaresystems.
ACKNOWLEDGMENTS
TheauthorswouldliketothanktheanonymousrefereesofASE‚Äô18fortheirvaluablecommentsandhelpfulsuggestions.ThisworkissupportedbytheNationalNaturalScienceFoundationofChinaun-der Grant No. 61202040 with XiDian University and U.S. NationalScience Foundation under Grant No. CNS-1547461, CNS-1718901,andCCF-1423542withUniversityofCalifornia,Davis.Thisworkis
38
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. AutoConfig: Automatic Configuration Tuning for Distributed Message Systems ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
alsosupportedbytheFundamentalResearchFundsfortheCentral
Universities(GrantNo.JB171005).
REFERENCES
[1] AAbdelaziz,WKadir,andAddinOsman.2011.Comparativeanalysisofsoftware
performance prediction approaches in context of component-based system. In-
ternational Journal of Computer Applications 23,3 (2011), 15‚Äì22.
[2] ActiveMQ.2018. http://activemq.apache.org/.
[3] SanjayPAhujaandNaveenMupparaju.2014.PerformanceEvaluationandCom-
parisonofDistributedMessagingUsingMessageOrientedMiddleware. Comput-
er and information science 7, 4 (2014), 9.
[4] Stefan Appel, Kai Sachs, and Alejandro Buchmann. 2010. Towards benchmark-
ing of AMQP. In Proceedings of the Fourth ACM International Conference on Dis-
tributed Event-Based Systems. ACM,99‚Äì100.
[5] Simonetta Balsamo, Antinisca Di Marco, Paola Inverardi, and Marta Simeoni.
2004. Model-based performance prediction in software development: A survey.IEEE Transactionson SoftwareEngineering 30,5 (2004), 295‚Äì310.
[6] Zhendong Bei, Zhibin Yu, Huiling Zhang, Wen Xiong, Chengzhong Xu, Lieven
Eeckhout, and Shengzhong Feng. 2016. RFHOC: A Random-Forest Approachto Auto-Tuning Hadoop‚Äôs Configuration. IEEE Transactions on Parallel and Dis-
tributed Systems 27,5 (2016), 1470‚Äì1483.
[7] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter
optimization. Journalof Machine Learning Research 13, Feb (2012), 281‚Äì305.
[8] James Bergstra, Dan Yamins, and David D Cox. 2013. Hyperopt: A python li-
brary for optimizing the hyperparameters of machine learning algorithms. InProceedings of the 12th Python in Science Conference.Citeseer,13‚Äì20.
[9] Fabian Brosig, Nikolaus Huber, and Samuel Kounev. 2011. Automated extrac-
tion of architecture-level performance models of distributed component-basedsystems.In AutomatedSoftwareEngineering(ASE),201126thIEEE/ACMInterna-
tional Conferenceon. IEEE, 183‚Äì192.
[10] Marc Br ¬®unink and David S Rosenblum. 2016. Mining performance specifica-
tions.InProceedingsofthe201624thACMSIGSOFTInternationalSymposiumon
Foundations of SoftwareEngineering. ACM,39‚Äì49.
[11] AntonioCarzanigaandAlexanderLWolf.2002. Abenchmarksuitefordistributed
publish/subscribe systems. Technical Report. COLORADO UNIV AT BOULDERDEPT OF COMPUTER SCIENCE.
[12] GiulianoCasale.2017. Acceleratingperformanceinferenceoverclosedsystems
by asymptotic methods. Proceedings of the ACM on Measurement and Analysis
of Computing Systems 1, 1 (2017), 8.
[13] BihuanChen,YangLiu,andWeiLe.2016. Generatingperformancedistribution-
s via probabilistic symbolic execution. In Proceedings of the 38th International
Conferenceon SoftwareEngineering. ACM,49‚Äì60.
[14] HaifengChen,WenxuanZhang,andGuofeiJiang.2011. Experiencetransferfor
the configuration tuning in large-scale computing systems. IEEE Transactions
on Knowledge and Data Engineering 23,3 (2011), 388‚Äì401.
[15] ShipingChen,YanLiu,IanGorton,andAnnaLiu.2005. Performanceprediction
of component-based applications. Journal of Systems and Software 74, 1 (2005),
35‚Äì43.
[16] Ivan M Delamer, JL Martinez Lastra, and Oscar Perez. 2006. An evolutionary
algorithmforoptimizationofXMLpublish/subscribemiddlewareinelectronicsproduction. In Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE
International Conferenceon.IEEE, 681‚Äì688.
[17] Giovanni Denaro, Andrea Polini, and Wolfgang Emmerich. 2004. Early perfor-
mance testing of distributed software applications. In ACM SIGSOFT Software
Engineering Notes ,Vol.29. ACM,94‚Äì103.
[18] PhilippeDobbelaereandKyumarsSheykhEsmaili.2017.KafkaversusRabbitMQ:
A comparative study of two industry reference publish/subscribe implementa-
tions:IndustryPaper.In Proceedingsofthe11thACMInternationalConferenceon
Distributed and Event-based Systems .ACM,227‚Äì238.
[19] Christian Esposito, Stefano Russo, and Dario Di Crescenzo. 2008. Performance
assessmentofOMGcompliantdatadistributionmiddleware.In ParallelandDis-
tributed Processing, 2008. IPDPS 2008. IEEE International Symposium On. IEEE,
1‚Äì8.
[20] StÀÜenioFLFernandes,WellingtonJo ÀúaoSilva,MauroJCSilva,NelsonSRosa,Paulo
RomeroMartinsMaciel,andDjamelFawziHadjSadok.2004. Onthegeneralised
stochastic petri net modeling of message-oriented middleware systems. In Per-
formance, Computing, and Communications, 2004 IEEE International Conference
on. IEEE, 783‚Äì788.
[21] Krzysztof Grochla, Mateusz Nowak, Piotr Pecka, and S≈Çawomir Nowak. 2017.
Influence of Message-Oriented Middleware on Performance of Network Man-
agement System: A Modelling Study. In Multime
 dia and Network Information
Systems.Springer,379‚Äì393.
[22] JianmeiGuo,KrzysztofCzarnecki,SvenApely,NorbertSiegmundy,andAndrzej
Wasowski. 2013. Variability-aware performance prediction: A statistical learn-
ing approach. In Proceedings of the 28th IEEE/ACM International Conference onAutomated SoftwareEngineering . IEEE Press,301‚Äì311.
[23] Jens Happe, Holger Friedrich, Steffen Becker, and Ralf H Reussner. 2008. A
pattern-based performance completion for Message-oriented Middleware. InProceedingsofthe7thinternationalworkshoponSoftwareandperformance.ACM,
165‚Äì176.
[24] Jens Happe, Dennis Westermann, Kai Sachs, and Lucia Kapov ¬¥a. 2010. Statisti-
calinferenceofsoftwareperformancemodelsforparametricperformancecom-pletions. In International Conference on the Quality of Software Architectures .
Springer,20‚Äì35.
[25] Gaurav Harsola. 2018. Kafka Benchmarking.
https://blog.talentica.com/2016/11/30/kafka-benchmarking/
[26] Christopher Henard, Mike Papadakis, Mark Harman, and Yves Le Traon. 2015.
Combining multi-objective search and constraint solving for configuring largesoftwareproductlines.In Proceedingsofthe37thInternationalConferenceonSoft-
wareEngineering-Volume1. IEEE Press,517‚Äì528.
[27] Robert Henjes, Michael Menth, and Christian Zepfel. 2006. Throughput per-
formance of java messaging services using websphereMQ. In Distributed Com-
puting Systems Workshops, 2006. ICDCS Workshops 2006. 26th IEEE InternationalConferenceon.IEEE, 26‚Äì26.
[28] FrankHutter,HolgerHHoos,andKevinLeyton-Brown.2011. SequentialModel-
Based Optimization for General Algorithm Configuration. LION5 (2011), 507‚Äì
523.
[29] PooyanJamshidiandGiulianoCasale.2016. Anuncertainty-awareapproachto
optimal configuration of stream processing systems. In Modeling, Analysis and
Simulation of Computer and Telecommunication Systems (MASCOTS), 2016 IEEE24th International Symposium on.IEEE, 39‚Äì48.
[30] Pooyan Jamshidi, Norbert Siegmund, Miguel Velez, Christian K ¬®astner, Akshay
Patel, and Yuvraj Agarwal. 2017. Transfer learning for performance modelingof configurable systems: An exploratory analysis. In Proceedings of the 32nd
IEEE/ACM International Conference on Automated Software Engineering. IEEEPress,497‚Äì508.
[31] Pooyan Jamshidi, Miguel Velez, Christian K ¬®astner, Norbert Siegmund, and
Prasad Kawthekar. 2017. Transfer learning for improving model predictionsin highly configurable software. In Software Engineering for Adaptive and Self-
Managing Systems (SEAMS), 2017 IEEE/ACM 12th International Symposium on .
IEEE, 31‚Äì41.
[32] VineetJohnandXiaLiu.2017. ASurveyofDistributedMessageBrokerQueues.
arXiv preprintarXiv:1704.00411 (2017).
[33] Kafka.2018. http://kafka.apache.org.[34] Kafka.2018. http://kafka.apache.org/documentation/configuration.[35] Samuel Kounev. 2006. Performance modeling and evaluation of distributed
component-basedsystemsusingqueueingpetrinets. IEEETransactionsonSoft-
wareEngineering 32, 7 (2006), 486‚Äì502.
[36] SamuelKounevandChristoferDutz.2009. QPME:aperformancemodelingtool
basedonqueueingPetriNets. ACMSIGMETRICSPerformanceEvaluationReview
36, 4 (2009), 46‚Äì51.
[37] Samuel Kounev, Kai Sachs, Jean Bacon, and Alejandro Buchmann. 2008. A
methodology for performance modeling of distributed event-based systems. InObject Oriented Real-Time Distributed Computing (ISORC), 2008 11th IEEE Inter-national Symposium on. IEEE, 13‚Äì22.
[38] Heiko Koziolek. 2010. Performance evaluation of component-based software
systems: A survey. PerformanceEvaluation 67,8 (2010), 634‚Äì658.
[39] Stephan Kraft, Sergio Pacheco-Sanchez, Giuliano Casale, and Stephen Dawson.
2009. Estimating service resource consumption from response time measure-ments. In Proceedings of the Fourth International ICST Conference on Perfor-
mance Evaluation Methodologies and Tools. ICST (Institute for Computer Sci-ences, Social-Informatics and Telecommunications Engineering), 48.
[40] Jay Kreps. 2018. Benchmarking Apache Kafka: 2 Mil-
lion Writes Per Second (On Three Cheap Machines).https://events.static.linuxfound.org/sites/events/files/slides/HTKafka2.pdf
[41] Dinesh Kumar, Li Zhang, and Asser Tantawi. 2009. Enhanced inferencing: Esti-
mationofaworkloaddependentperformancemodel.In ProceedingsoftheFourth
InternationalICSTConferenceonPerformanceEvaluationMethodologiesandTools .
ICST (Institute for Computer Sciences, Social-Informatics and Telecommunica-tions Engineering), 47.
[42] PaulLeNoac‚ÄôH,AlexandruCostan,andLucBoug ¬¥e.2017. Aperformanceevalu-
ationofApacheKafkainsupportofbigdatastreamingapplications.In BigData
(BigData), 2017 IEEE International Conferenceon.IEEE, 4803‚Äì4806.
[43] YanLiuandIanGorton.2005.PerformancepredictionofJ2EEapplicationsusing
messagingprotocols. In International
 Symposiumon Component-BasedSoftware
Engineering. Springer,1‚Äì16.
[44] YanLiu,IanGorton,andAlanFekete.2005.Design-levelperformanceprediction
ofcomponent-basedapplications. IEEETransactionsonSoftwareEngineering 31,
11 (2005), 928‚Äì941.
[45] MichaelDMcKay,RichardJBeckman,andWilliamJConover.1979.Comparison
ofthreemethodsforselectingvaluesofinputvariablesintheanalysisofoutputfroma computer code. Technometrics 21, 2 (1979), 239‚Äì245.
39
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France L. Bao et al.
[46] Fl¬¥avioMedeiros,ChristianK ¬®astner,M ¬¥arcioRibeiro,RohitGheyi,andSvenApel.
2016. Acomparisonof10samplingalgorithmsforconfigurablesystems.In Pro-
ceedingsofthe38thInternationalConferenceonSoftwareEngineering.ACM,643‚Äì
654.
[47] Michael Menth and Robert Henjes. 2006. Analysis of the message waiting time
for the fioranoMQ JMS server. In Distributed Computing Systems, 2006. ICDCS
2006. 26th IEEE International Conferenceon. IEEE, 1‚Äì1.
[48] Gero M ¬®uhl, Arnd Schr ¬®oter, Helge Parzyjegla, Samuel Kounev, and Jan Richling.
2009. Stochasticanalysisofhierarchicalpublish/subscribesystems.In European
Conferenceon Parallel Processing. Springer,97‚Äì109.
[49] Alexandr Murashkin, Micha≈Ç Antkiewicz, Derek Rayside, and Krzysztof Czar-
necki. 2013. Visualization and exploration of optimal variants in product lineengineering. In Proceedings of the 17th International Software Product Line Con-
ference. ACM,111‚Äì115.
[50] VivekNair,TimMenzies,NorbertSiegmund,andSvenApel.2017. Fasterdiscov-
ery of faster system configurations with spectral learning. Automated Software
Engineering (2017),1‚Äì31.
[51] Vivek Nair, Tim Menzies, Norbert Siegmund, and Sven Apel. 2017. Using bad
learnerstofindgoodconfigurations.In Proceedingsofthe201711thJointMeeting
on Foundations of SoftwareEngineering. ACM,257‚Äì267.
[52] Jeho Oh, Don Batory, Margaret Myers, and Norbert Siegmund. 2017. Finding
near-optimalconfigurationsinproductlinesbyrandomsampling.In Proceedings
ofthe201711thJointMeetingonFoundationsofSoftwareEngineering.ACM,61‚Äì
71.
[53] Rafael Olaechea, Derek Rayside, Jianmei Guo, and Krzysztof Czarnecki. 2014.
Comparison of exact and approximate multi-objective optimization for soft-
wareproductlines.In Proceedingsofthe18thInternationalSoftwareProductLine
Conference-Volume1 .ACM,92‚Äì101.
[54] TakayukiOsogamiandSeiKato.2007.Optimizingsystemconfigurationsquickly
by guessing at the performance. In ACM SIGMETRICS Performance Evaluation
Review, Vol.35. ACM,145‚Äì156.
[55] GiovanniPacifici,WolfgangSegmuller,MikeSpreitzer,andAsserTantawi.2006.
Dynamic estimation of cpu demand of web traffic. In Proceedings of the 1st in-
ternational conference on Performance evaluation methodolgies and tools. ACM,26.
[56] MartinPinzger.2008.Automatedwebperformanceanalysis.In Proceedingsofthe
200823rdIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering .
IEEE Computer Society,513‚Äì516.
[57] C¬¥assio Prazeres, Jurandir Barbosa, Leandro Andrade, and Martin Serrano. 2017.
Designandimplementationofamessage-serviceorientedmiddlewareforfogofthings platforms. In Proceedings of the Symposium on Applied Computing. ACM,
1814‚Äì1819.
[58] RabbitMQ.2018. https://www.rabbitmq.com/.[59] ChristophRathfelder,SamuelKounev,andDavidEvans.2011.Capacityplanning
for event-based systems using automated performance predictions. In Proceed-
ings of the 2011 26th IEEE/ACM International Conference on Automated SoftwareEngineering. IEEE Computer Society,352‚Äì361.
[60] ElnatanReisner,CharlesSong,Kin-KeungMa,JeffreySFoster,andAdamPorter.
2010. Using symbolic evaluation to understand behavior in configurable soft-
ware systems. In Proceedings of the 32nd ACM/IEEE International Conference on
SoftwareEngineering-Volume1 .ACM,445‚Äì454.
[61] RocketMQ.2018. https://rocketmq.apache.org/.[62] Paloma Rubio-Conde, Diego Villar ¬¥an-Molina, and Marisol Garc ¬¥ƒ±a-Valls. 2017.
Measuring performance of middleware technologies for medical systems: Ice
vs AMQP. ACMSIGBED Review 14, 2 (2017), 8‚Äì14.
[63] Kai Sachs, Samuel Kounev, Stefan Appel, and Alejandro Buchmann. 2009. A
Performance Test Harness For Publish/Subscribe Middleware. In SIGMETRIC-
S/Performance.
[64] Kai Sachs, Samuel Kounev, Jean Bacon, and Alejandro Buchmann. 2009. Per-
formance
evaluation of message-oriented middleware using the SPECjms2007
benchmark. Performance Evaluation 66,8 (2009), 410‚Äì434.
[65] Atri Sarkar, Jianmei Guo, Norbert Siegmund, Sven Apel, and Krzysztof Czar-
necki.2015. Cost-efficientsamplingforperformancepredictionofconfigurablesystems.In AutomatedSoftwareEngineering(ASE),201530thIEEE/ACMInterna-
tional Conferenceon. IEEE, 342‚Äì352.
[66] Abdel Salam Sayyad, Joseph Ingram, Tim Menzies, and Hany Ammar. 2013. S-
calable product line configuration: A straw to break the camel‚Äôs back. In Auto-
matedSoftwareEngineering(ASE),2013IEEE/ACM28thInternationalConferenceon. IEEE, 465‚Äì474.
[67] Abhishek B Sharma, Ranjita Bhagwan, Monojit Choudhury, Leana Golubchik,
Ramesh Govindan, and Geoffrey M Voelker. 2008. Automatic request catego-rization in internet services. ACM SIGMETRICS Performance Evaluation Review
36, 2 (2008), 16‚Äì25.
[68] NorbertSiegmund,AlexanderGrebhahn,SvenApel,andChristianK ¬®astner.2015.
Performance-influencemodelsforhighlyconfigurablesystems.In Proceedingsofthe 2015 10th Joint Meeting on Foundations of Software Engineering. ACM, 284‚Äì294.
[69] Norbert Siegmund, Sergiy S Kolesnikov, Christian K ¬®astner, Sven Apel, Don Ba-
tory, Marko Rosenm ¬®uller, and Gunter Saake. 2012. Predicting performance vi-
a automated feature-interaction detection. In Software Engineering (ICSE), 2012
34th International Conferenceon . IEEE, 167‚Äì177.
[70] Julio Sincero, Wolfgang Schroder-Preikschat, and Olaf Spinczyk. 2010. Ap-
proaching non-functional properties of software product lines: Learning fromproducts. In Software Engineering Conference (APSEC), 2010 17th Asia Pacific.
IEEE, 147‚Äì155.
[71] Samaneh Soltani, Mohsen Asadi, Marek Hatala, Dragan Ga Àásevi¬¥c, and Ebrahim
Bagheri. 2011. Automated planning for feature model configuration based onstakeholders‚Äôbusinessconcerns.In AutomatedSoftwareEngineering(ASE),2011
26th IEEE/ACMInternational Conferenceon. IEEE, 536‚Äì539.
[72] James Styles, Holger H Hoos, and Martin M ¬®uller. 2012. Automatically configur-
ingalgorithmsforscalingperformance. In LearningandIntelligentOptimization.
Springer,205‚Äì219.
[73] Chong Tang. 2017. System performance optimization via design and configura-
tion space exploration. In Proceedings of the 2017 11th Joint Meeting on Founda-
tions of SoftwareEngineering.ACM,1046‚Äì1049.
[74] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi
Matsumoto. 2016. Automated parameter optimization of classification tech-niques for defect prediction models. In Software Engineering (ICSE), 2016
IEEE/ACM38th International Conferenceon. IEEE, 321‚Äì332.
[75] RobertTibshirani.1996. Regressionshrinkageandselectionviathelasso. Jour-
nal of the RoyalStatistical Society.Series B (Methodological) (1996), 267‚Äì288.
[76] RyanJTibshirani,AlessandroRinaldo,RobertTibshirani,andLarryWasserman.
2015. Uniform asymptotic inference and the bootstrap after model selection.arXiv preprintarXiv:1506.06266 (2015).
[77] Pavel Valov, Jean-Christophe Petkovich, Jianmei Guo, Sebastian Fischmeister,
and Krzysztof Czarnecki. 2017. Transferring performance prediction modelsacross different hardware platforms. In Proceedings of the 8th ACM/SPEC on In-
ternational Conferenceon Performance Engineering . ACM,39‚Äì50.
[78] Tiantian Wang, Mark Harman, Yue Jia, and Jens Krinke. 2013. Searching for
betterconfigurations:arigorousapproachtocloneevaluation.In Proceedingsof
the 2013 9th Joint Meeting on Foundations of Software Engineering. ACM, 455‚Äì
465.
[79] Dennis Westermann, Jens Happe, Rouven Krebs, and Roozbeh Farahbod. 2012.
Automatedinferenceofgoal-orientedperformancepredictionfunctions.In Pro-
ceedings of the 27th IEEE/ACM International Conference on Automated SoftwareEngineering. ACM,190‚Äì199.
[80] Murray Woodside, Greg Franks, and Dorina C Petriu. 2007. The future of soft-
ware performance engineering. In 2007 Future of Software Engineering. IEEE
Computer Society,171‚Äì187.
[81] Fan Wu, Westley Weimer, Mark Harman, Yue Jia, and Jens Krinke. 2015. Deep
parameteroptimisation.In Proceedingsofthe2015AnnualConferenceonGenetic
and Evolutionary Computation.ACM,1375‚Äì1382.
[82] Jane Wyngaard. 2018. High throughput kafka for science.
https://events.static.linuxfound.org/sites/events/files/slides/HTKafka2.pdf
[83]
Bowei Xi, Zhen Liu, Mukund Raghavachari, Cathy H Xia, and Li Zhang. 2004.
Asmarthill-climbingalgorithmforapplicationserverconfiguration.In Proceed-
ings of the 13th international conferenceon WorldWide Web .ACM,287‚Äì296.
[84] Tianyin Xu, Long Jin, Xuepeng Fan, Yuanyuan Zhou, Shankar Pasupathy, and
RukmaTalwadker.2015. Hey,youhavegivenmetoomanyknobs!:understand-inganddealingwithover-designedconfigurationinsystemsoftware.In Proceed-
ingsofthe201510thJointMeetingonFoundationsofSoftwareEngineering .ACM,
307‚Äì319.
[85] SherifYacoub.2002. Performanceanalysisofcomponent-basedapplications.In
International Conferenceon SoftwareProduct Lines. Springer,299‚Äì315.
[86] Tao Ye and Shivkumar Kalyanaraman. 2003. A recursive random search algo-
rithm for large-scale network parameter configuration. ACM SIGMETRICS Per-
formance Evaluation Review 31, 1 (2003), 196‚Äì205.
[87] Sai Zhang and Michael D Ernst. 2014. Which configuration option should i
change?. In Proceedings of the 36th International Conference on Software Engi-
neering.ACM,152‚Äì163.
[88] Yi Zhang, Jianmei Guo, Eric Blais, and Krzysztof Czarnecki. 2015. Performance
prediction of configurable software systems by fourier learning. In Automat-
ed Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on .
IEEE, 365‚Äì373.
[89] Tao Zheng, C Murray Woodside, and Marin Litoiu. 2008. Performance model
estimation and tracking using optimal filters. IEEE Transactions on software en-
gineering 34, 3 (2008), 391‚Äì406.
[90] YuqingZhu,JianxunLiu,MengyingGuo,YungangBao,WenlongMa,Zhuoyue
Liu, Kunpeng Song, and Yingchun Yang. 2017. BestConfig: tapping the perfor-mance potential of systems via automatic configuration tuning. In Proceedings
of the 2017 Symposium on Cloud Computing.ACM,338‚Äì350.
40
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. 