Quality and Productivity Outcomes Relating to
Continuous Integration in GitHub
Bogdan Vasilescuy,Yue Yuzy,Huaimin Wangz,Premkumar Devanbuy,Vladimir Filkovy
yDepartment of Computer SciencezCollege of Computer
University of California, Davis National University of Defense Technology
Davis, CA 95616, USA Changsha, 410073, China
{vasilescu, ptdevanbu, vÔ¨Ålkov}@ucdavis.edu {yuyue, hmwang}@nudt.edu.cn
ABSTRACT
Software processes comprise many steps; coding is followed
by building, integration testing, system testing, deployment,
operations, among others. Software process integration and
automation have been areas of key concern in software engi-
neering, ever since the pioneering work of Osterweil; market
pressures for Agility, and open, decentralized, software de-
velopment have provided additional pressures for progress in
this area. But do these innovations actually help projects?
Given the numerous confounding factors that can inuence
project performance, it can be a challenge to discern the ef-
fects of process integration and automation. Software project
ecosystems such as GitHub provide a new opportunity in
this regard: one can readily nd large numbers of projects
in various stages of process integration and automation, and
gather data on various inuencing factors as well as produc-
tivity and quality outcomes. In this paper we use large,
historical data on process metrics and outcomes in GitHub
projects to discern the eects of one specic innovation in
process automation: continuous integration . Our main nd-
ing is that continuous integration improves the productivity
of project teams, who can integrate more outside contribu-
tions, without an observable diminishment in code quality.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging|
Testing tools
General Terms
Experimentation, Human Factors
Keywords
Continuous integration, GitHub, pull requests
Bogdan Vasilescu and Yue Yu are both rst authors, and
contributed equally to the work.1. INTRODUCTION
Innovations in software technology are central to economic
growth. People place ever-increasing demands on software,
in terms of features, security, reliability, cost, and ubiquity;
and these demands come at an increasingly faster rate. As
the appetites grow for ever more powerful software, the hu-
man teams working on them have to grow, and work more
eciently together.
Modern games, for example, require very large bodies of
code, matched by teams in the tens and hundreds of devel-
opers, and development time in years. Meanwhile, teams
are globally distributed, and sometimes ( e.g., with open
source software development) even have no centralized con-
trol. Keeping up with market demands in an agile, orga-
nized, repeatable fashion, with little or no centralized con-
trol, requires a variety of approaches, including the adop-
tion of technology to enable process automation. Process
Automation per se is an old idea, going back to the pio-
neering work of Osterweil [32]; but recent trends such as
open-source, distributed development, cloud computing, and
software-as-a-service, have increased demands for this tech-
nology, and led to many innovations. Examples of such in-
novations are distributed collaborative technologies like git
repositories, forking, pull requests, continuous integration,
and the DEVOPS movement [36]. Despite rapid changes, it
is dicult to know how much these innovations are helping
improve project outcomes such as productivity and quality.
A great many factors such as code size, age, team size, and
user interest can inuence outcomes; therefore, teasing out
the eect of any kind of technological or process innovation
can be a challenge.
TheGitHub ecosystem provides a very timely opportu-
nity for study of this specic issue. It is very popular (in-
creasingly so) and hosts a tremendous diversity of projects.
GitHub also comprises a variety of technologies for dis-
tributed, decentralized, social software development, com-
prising version control, social networking features, and pro-
cess automation. The development process on GitHub is
more democratic than most open-source projects: anyone
can submit contributions in the form of pull requests . A pull
request is a candidate, proposed code change, sometimes
responsive to a previously submitted modication request
(orissue ). These pull requests are reviewed by project in-
siders ( akacore developers, or integrators), and accepted if
deemed of sucient quality and utility. Projects that are
more popular and widely used can be expected to attract
more interest, and more pull requests; these will have to bebuilt, tested, and reviewed by core developers prior to ac-
tual inclusion. This process can slow down, given the limited
bandwidth of core developers; thus popular, innovative, and
agile projects need process automation. One key innovation
is the idea of continuous integration (CI); essentially,CIat-
tempts to automatically build and deploy the software in a
\sandbox", and automatically run a collection of tests when
the pull request is received. By automating these steps, a
project can hope to gain both productivity (more pull re-
quests accepted) and quality (the accepted pull requests are
prescreened by the automation provided by CI).
Starting from a newly mined data set of the usage of CI
inGitHub projects, in this paper we looked at the soft-
ware engineering outcomes which present dierentially with
the introduction of CIversus without. In particular, our
contributions are:
We collected a comprehensive data set of 246 GitHub
projects which at some point in their history added
the Travis-CI functionality to the development pro-
cess. Our data is available online at https://github.com/
yuyue/pullreq_ci .
We found that after CIis added, more pull requests
from core developers are accepted, and fewer are re-
jected; and fewer submissions from non-core develop-
ers get rejected. This suggests that CIboth improves
the handling of pull requests from insiders, and has an
overall positive eect on the initial quality of outside
submissions.
Despite the increased volume of pull requests accepted,
we found that introduction of CIisnotassociated with
any diminishment of user-reported bugs, thus suggest-
ing that user-experienced quality is not negatively af-
fected. We did see an increase in developer reported
bugs, which suggests that CIis helping developers dis-
cover more defects.
2. BACKGROUND
The focus of our work is the eect of continuous integra-
tion in the context of open source projects that use the pull
request based model of development. We begin with some
background.
2.1 Continuous Integration
The concept ofCIis often attributed to Martin Fowler
based on a 2000 blog entry [12]. The basic notion is that
all developers' work within a team is continually compiled,
built, and tested. This process is a perpetual check on
the quality of contributed code, and mitigates the risk of
\breaking the build", or worse, because of major, incom-
patible changes by dierent people or dierent sub-teams.
Arguably,CIoriginated from the imperatives of agility [11],
viz., responding quickly to customer requirements. It can
be viewed as a type of process automation; rather than wait
for some sort of human-controlled gate-keeping of code prior
to building and integration testing, automated mechanisms
are incorporated into the development environment to carry
out these steps continually and automatically. In software
engineering, continuous integration is viewed as a paradigm
shift, \perhaps as important as using version control" [22].
WithoutCI, software is considered broken until proven to
work, typically during a testing or integration stage. WithCI, assuming a comprehensive automated test suite, soft-
ware is proven to work with every new change, and serious
regressions can be detected and xed immediately.
In the context of distributed, globalized development, cul-
tural, geographical and time dierences raise the spectre of
process variation and non-repeatability, and thus amplify
the imperatives to adopt process automation. This applies
even more strongly to open source software (OSS) projects
where, in addition to the above issues, volunteers are in-
volved, and there is also a lack of centralized control [10,
20, 21].CIhas become quite popular in OSS projects, and
many projects in GitHub are using it [19]. Numerous tools
that supportCIexist [29]. Therefore, one might expect that
these teams are seeing benets from adopting CI, and that
one could obtain quantitative evidence of these benets.
2.2 Pull-based Software Development
The pull-based development model [2], used to integrate
incoming changes into a project's codebase, is becoming the
de facto contribution model in distributed software teams [19].
Enabled by git, pull-based development means that con-
tributors to a software project can propose changes without
the need for them to share access to a central repository;
instead, contributors can work locally in a dierent branch
or fork (local clone) of the central repository and, whenever
ready, request to have their changes merged into the main
branch by submitting a pull request .
Compared to patch submission and acceptance via mailing
lists and issue tracking systems, which has been the tradi-
tional model of collaboration in open source [5,15], the pull-
based model oers several advantages, including centraliza-
tion of information ( e.g., the contributed code|the patch|
resides in the same source control management system as the
rest of the system, therefore authorship information is eort-
lessly maintained; on modern collaborative coding platforms
such as BitBucket ,Gitorius , and GitHub , a wealth of
data about the submitter's track record is publicly available
to project managers via user prole pages [8,28]) and process
automation ( e.g.,GitHub provides integrated functionality
for pull request generation, automatic testing, contextual
discussion, in-line code review, and merger).
By \decoupling the development eort from the decision
to incorporate the results of the development in the code
base" [17], the pull-based model also oers an unprecedent-
edly low barrier to entry for potential contributors ( i.e., the
so-called\drive-by"commits [35]), since anyone can fork and
submit pull requests to any repository. Therefore, projects
can use pull requests complementarily to the shared reposi-
tory model, such that core team members push their changes
directly, and outside contributors submit changes via pull re-
quests. However, projects can also use pull requests in many
scenarios beyond basic patch submission, e.g., for conduct-
ing code reviews, and discussing new features [19]. As a
result, in many projects all contributions are submitted as
pull requests, irrespective of whether they come from core
developers with write access to the repository or from out-
siders, which ensures they adhere to the same evaluation
process ( e.g., only reviewed code gets merged) [19].
The pull-based model is widely used. For example, on
GitHub alone, almost half of all collaborative projects use
pull requests [19], and this number is only expected to grow.
Ruby on Rails ,1one of the most popular projects on GitHub ,
1https://github.com/rails/railsResults
Submitting
Profile
‚àöSocial Activities
‚àöContributionsEvaluate
Contributor
Patch
‚àöSize
‚àöQualityPull 
RequestIssue Tracker
PRPR
PRPRPR
PR
DiscussingContinuous Integration
System
Merging
Core Team
Management
‚àöWorkload
‚àöTeam size
‚àöPriority    InputTesting 
Branch
Build
Test
NotifyingOutputFigure 1: Overview of the pull request evaluation process
receives upwards of three hundred new pull requests each
month. The high volume of incoming pull requests poses a
serious challenge to project integrators, i.e., the core team
members responsible for evaluating the proposed changes,
deciding whether to apply them or not, and integrating them
into the main development branch [19, 35, 45]. Integrators
play a crucial role in pull-based development [9,19]. Dabbish
et al. [9] identify management of pull requests as the most
important project activity on GitHub . Gousios et al. [19]
consider integrators to be \guardians for the project's qual-
ity". They must ensure not only that pull requests are eval-
uated in a timely matter and eventually accepted, to secure
the project's growth, but also that all contributions meet
the project's quality standards.
2.3 Pull Request Evaluation
Prior work on pull request evaluation on social coding
platforms like GitHub [17, 19, 45, 46] points to a complex
process inuenced by a multitude of social and technical fac-
tors. The level of transparency available on GitHub , where
a number of social signals become more readily available,
has shaped the way developers make inferences about each
other and their work [8,28]. For example, the integrated so-
cial media features ( e.g.,following other developers, watch-
ingrepositories and commenting on pull requests) enable
participants in these communities to build social relation-
ships [3,8,40,52,53], while public prole pages make salient
information about one's track record as developer [8,19,27]
and even demographic features ( e.g., gender [47, 48]). In-
tegrators use social signals to build trust in the submitted
contributions, e.g., they build mental proles of the submit-
ters' competence by evaluating their track record, and they
base judgements of the contributions on personal relation-
ships with the submitters [19, 28, 45]. Prior work has also
uncovered which technical factors are associated with con-
tribution acceptance, e.g., code quality [19, 45], adherence
to coding styles and project conventions [19], existence of
testing code in the pull request [19,45], and how active the
particular project area aected by the pull request is [17].
In trying to handle pull requests eciently without com-
promising software quality, especially when faced with an
increasing volume of incoming pull requests, integrators of-ten resort to automated testing, as supported by CIser-
vices [19,35]. On GitHub , 75% of projects that use pull re-
quests frequently also use CI, either in hosted services ( e.g.,
Travis-CI [49], Jenkins [29]) or in standalone setups [19].
In prior work [51], we found that presence of CIplays a
prominent role in the pull request evaluation process, being
a strong positive predictor of pull request evaluation latency.
The following is a simplied description of how CIis involved
in pull request evaluation (from [51]). Whenever a new pull
request is received by a project using CI, the contribution
is merged automatically into a testing branch, and the ex-
isting test suites are run. If tests fail, the pull request is
typically rejected (closed and not merged in GitHub par-
lance) by one of the integrators, who may also comment
on why it is inappropriate and how it can be improved. If
tests pass, core team members proceed to do a team-wide
code review, by commenting inline on (parts of) the code,
including requests for modications to be carried out by the
submitter (who can then update the pull request with new
code), if necessary. After a cycle of comments and revi-
sions, and if everyone is satised, the pull request is closed
and merged. In rare cases, pull requests are merged even if
(some) tests failed. Only core team members (integrators)
and the submitter can close (to merge or reject|integrators;
to withdraw|submitter) and reopen pull requests. The pro-
cess is summarized in Figure 1.
The continuous application of quality control checks (as
imposed byCI) aims to speed up the development process
and to ultimately improve software quality [11]. For exam-
ple, Addam Hardy, a developer active on GitHub , explains
in his blog:2
[CI] enables us to automate more of our process which
frees us up to focus on the important things | like im-
plementing and shipping features! [...] [The integration
of CI in GitHub] enables the team to rapidly nd in-
tegration errors or regression failures in the test suite.
This tightens the feedback loop and not only enables
more defect free code, but greatly speeds up our process.
2http://addamhardy.com/blog/2013/09/28/automate-all-the-things-
continuous-integration-and-continuous-deployment-at-revunit/2.4 Research Questions
Prior work in evaluating such beliefs has been mostly qual-
itative and survey based. Based on the conceptual founda-
tion laid by these studies, reviewed above, the central aim
of this paper is to quantitatively explore eects associated
with the adoption of CIinGitHub projects that use the
pull request model. We explicate our research questions. To
begin, there is a strong and immediate expectation that CI
should improve productivity of both teams and individuals.
Staal and Bosch [42], for example, argue with some survey-
based evidence [41], that build and test automation saves
programmers time for more creative work, and thus should
increase productivity. Stolberg [43] argues that CIleads to
improved integration intervals, and thus faster delivery of
software. Miller [30] reports the experiences with CIof one
distributed team at Microsoft in 2007, and estimates that
moving to aCI-driven process may achieve a 40% reduction
in check-in overhead when compared to a check-in process
that doesn't leverage CI, for the same code base and prod-
uct quality. Bhattacharya [4] reports on an industrial case
study from an insurance company, where developer produc-
tivity increases through the usage of CI. However, not all
studies agree on eects associated with adoption of CI,e.g.,
Parsons et al. [33] nd no clear benets of CIon either
productivity or quality. These prior studies prompt a large-
scale, quantitative analysis to determine the productivity
eects ofCIon distributed teams. We ask:
RQ1. Productivity.
How is the productivity of teams aected by CI?
A key aspect ofCIistesting ; integration, and unit (and
perhaps other) tests are run automatically with every change.
Testing is, after all, all about nding bugs. Indeed, Fowler,
one of early proponents of CI, has emphasized the benets
to software quality [12]. Other researchers make support-
ing claims about automated testing. For example, Karhu et
al.[24] report on an industrial case study conducted in ve
commercial organizations, and nd, using interview data,
that testing automation leads to fewer defects. Rady and
Con [38] claim, without providing additional details, that
\many software development teams have recognized the de-
sign and quality benets of creating automated test suites".
The literature also points to indirect benets of using CI
on software quality, related to test quality increases [38],
e.g., through test coverage increases [29]. Fortunately, in
GitHub there are a range of projects that have adopted CI
to varying degrees, making it possible to study the eects of
CIon quality. We ask:
RQ2. Quality.
What is the eect of CIon software quality?
It should be noted here that there are many factors that
could inuence productivity and quality in software projects,
and if we are to tease out the eect of CIper se , we shall need
enough data to provide adequate controls for all these fac-
tors. Furthermore, the introduction of CIcould be expected
to aect productivity and quality in a variety of complex
ways, including second-order eects. For example, the use
CImight attract more people to the project; if pull requests
are accepted expediently, more contributions might arrive,
and more people might be induced to contribute. The test-
automation convenience of CImight encourage people to
write more tests; contrariwise, the very ability of CIto de-tect more defects might lead people to inaccurately perceive
a diminishment of quality; it might also encourage people to
submit higher-quality pull requests, for fear of failing a lot
of tests, and thus losing hard-won community status.
For the purposes of this study, we adopt two experimental
postures (discussed in more detail below). First, we gather
metrics on a large number of projects, over a signicant pe-
riod of time, focusing on a broad set of aspects that are
known to aect the rate of growth of projects' source base,
and the quality thereof. By controlling for several known fac-
tors that aect productivity and quality, we hope to discern
the eects ofCIper se . Second, in this paper we consider
specically the overall quality and productivity eects of CI,
without delving into further causal analysis as to whether
the eects are rst- or second-order; that is left for future
work.
3. METHODS
3.1 Data Collection
3.1.1 Selecting Projects
Our goal was to identify projects with suciently long his-
torical records, which had also switched to continuous inte-
gration at some point; this would enable us to model the ef-
fects ofCI. We began with the GHTorrent [16] dump dated
10/11/2014. We focused on main-line projects ( i.e., not
forks) written in the most popular languages34onGitHub :
Ruby, Python, JavaScript, PHP, Java, Scala, C and C++.
We excluded projects with fewer than 200 pull requests; we
only focused on projects where pull requests were integral
to the development eort, rather than being an infrequent
modality adopted by occasional external contributors.
This set of selection criteria resulted in a candidate set of
1,884 projects. Then, for each of them, we checked whether
it used aCIservice or not. There are two popular CIser-
vices used by projects in GitHub , Travis-CI and Jenkins [29]
(in addition to others, used less frequently). Travis-CI is
a hosted service ( i.e., it is supported by a remote build
server) integrated with GitHub (i.e., pull requests can be
tested automatically by Travis-CI, and the GitHub pull re-
quest UI is updated automatically with test results), there-
fore all projects using it operate in the same context. The
entire build history of projects using Travis-CI is available
through the Travis-CI API. Jenkins is a self-hosted system,
i.e., projects using it set up their CIservice locally. Typ-
ically, only data on recent builds is stored by Jenkins. To
reduce potentially confounding eects based on variations in
how Jenkins is implemented by each project, and in order
to have access to complete build histories (to be able to dis-
cern eects associated with adoption ofCI), we restrict our
attention in this study to the projects using Travis-CI.
We used the available Travis-CI API to detect whether
a given project used TRAVIS-CI or not [49]. We found
918 projects, 48.7% of 1,884, that used Travis-CI. For each
project, we collected data about closed pull requests ( i.e.,
ignoring pull requests that are still open) from GHTorrent.
The data we collected included metadata ( e.g., number of
comments on the pull request), as well as the title, descrip-
3http://redmonk.com/dberkholz/2014/05/02/github-language-
trends-and-the-fragmenting-landscape/
4http://githut.info/Table 1: Programming language statistics for the selected
246GitHub projects. Approximately half of all pull
requests closed (column pull reqs) arrived after projects
adopted Travis-CI ( pull reqs after CI); most of these under-
went CI testing ( pull reqs CItested ).
Language n pull reqs pull reqs pull reqs
after CI CI tested
JavaScript 65 45,781 23,010 22,453
Python 60 46,787 24,510 23,916
Ruby 36 26,559 15,058 14,634
PHP 24 22,907 10,227 9,836
Java 22 9,682 5,021 4,892
C++ 16 16,543 7,464 7,284
C 16 11,097 6,158 6,028
Scala 7 2,700 1,509 1,466
Total 246 182;056 92;957 90;509
tion, body, and the actual code contents, collected sepa-
rately from the GitHub API; we also collected Travis-CI
data ( e.g., the timestamps and outcomes of each build).
A further step of ltering was required to select projects
where the use of Travis-CI had endured long enough to reach
some kind of steady state. Some pull-request-based projects
had usedCIsince the outset, while others had only been
using it a few months at the time of our observation; fur-
thermore, some projects made inconsistent use of CIon
their pull requests, i.e., despite adopting Travis-CI, they
only used it sporadically (on few of their incoming pull re-
quests).5We therefore selected projects that had a good
level of activity after the adoption ofCI. Specically, we
selected projects where between 25% and 75% of the pull
requests were received after the adoption of CI (to ensure
sucient history both before and after CI),and 88.4% of
the pull requests received after the adoption of CIwere ac-
tually tested using CI. The 88.4% threshold was chosen to
cover 75% of the projects that were plausibly in a steady-
state use ofCI. The nal dataset consisted of 246 projects
(65 JavaScript, 60 Python, 36 Ruby, 24 PHP, 22 Java, 16
C++, 16 C, and 7 Scala), balanced with respect to use of CI:
51.1% (92,957 out of 182,056) of pull requests were submit-
ted after the adoption of Travis-CI (computed as the date
of the earliest pull request tested by CI), and 48.9% before,
as shown in Table 1.
3.1.2 Collecting Data on Source and Test Files
Starting with the creation date of each project, we col-
lected 3-month snapshots6of their source code repository
(by obtaining the identifying commit hashes| shavalues|
for the most recent commit in each snapshot, and performing
subsequent git reset commands). For each snapshot, we
identied source and test les using standard lename ex-
tension conversions on GitHub [18, 54]. Finally, we used
CLOC7to calculate the number of les and the number of
executable lines of code.
5Travis-CI does not attempt to build pull requests having
the[ci skip] ag anywhere in the commit message.
6We resorted to this approximation due to the otherwise
much greater computational eort required to reset each
project's repository to its state at the time of each incoming
pull request, before computing the metrics.
7http://cloc.sourceforge.net/Table 2: Summary statistics on the various metrics related
to productivity, for the selected 246 GitHub projects, for
a period of 24 months centered around adoption of Travis-
CI (4,148 rows of monthly project data; outliers removed as
described in Section 3.2).
Statistic Mean St. Dev. Min Median Max
proj age 23:84 14:161 22 78
nsrcloc 33 ;675:45 54 ;727:58 1 12 ;842:5 454 ;899
ntestloc 9;120:5815;867:9403;380:5113;977
nstars 784 :15 1 ;128:05 0 314 5 ;453
nforks 141:68 175:130 80 1;126
ciuse 0 :52 0 :50 0 1 1
team size 3:44 2:391 3 12
npropen 19 :63 21 :31 0 12 125
nprmerged 15:96 18:960 9 120
nprrejected 3 :68 5 :54 0 2 83
nprcore 10:35 17:170 3 121
ncore merged 9 :32 15 :77 0 2 119
ncore rejected 1:04 3:050 0 62
nissues open 11 :97 15 :59 0 7 95
3.1.3 Collecting Productivity Data
To reason about productivity , generally understood as the
amount of output per unit input [14] ( e.g., work per unit
time), we focused on integrator productivity, i.e., the num-
ber of pull requests merged per month. We computed var-
ious related metrics at monthly intervals for a period of 24
months, centered on the adoption date of Travis-CI ( i.e., 12
months prior, and 12 months after adoption of CI; we ig-
nored the actual adoption month). We recorded: the project
age (proj age; in months); the project size ( nsrcloc, the
number of source lines of code, in source les; ntestloc,
the number of source lines of code, in test les);8the num-
ber of forks ( nforks ) and the number of stars ( nstars )
of the project's main repository, as measures of popular-
ity (cumulative count since the project's creation until the
current month); whether the project uses CIat this time
(ciuse, binary);9the team size ( team size, the number of
core developers active each month);10the number of pull
requests received and the number of issues opened during
the current month, as measures of activity ( npropen ,
nissues open ); the number of merged ( nprmerged ) and
rejected ( nprrejected ) pull requests; we further distin-
guished between pull requests submitted by core developers
and those submitted by external contributors. The data is
summarized in Table 2.
3.1.4 Collecting Quality Data
We operationalize code quality by the number of bugs per
unit time, a commonly used measure [25]. There are two
main ways to identify the incidence of bugs when mining
software repositories. First, given that it is common practice
for developers to describe their work in commit messages,
one way to reason about the rate of bugs is to identify bug-
8nsrcles, the number of source les, and ntestles,
the number of test les, yielded qualitatively similar results,
therefore we exclude them from presentation.
9We also recorded a project's CIage| ciage|in months,
ranging from 12 to +12, but did not nd its eects to be
statistically signicant.
10We identied core developers as those developers who ei-
ther had write access to a project's code repository, or had
closed issues and pull requests submitted by others.Table 3: Summary statistics on the various metrics related
to quality, for the 42 GitHub projects we found to use is-
sue tagging consistently, for a period of 24 months centered
around adoption of Travis-CI (562 rows of monthly project
data; outliers removed as described in Section 3.2).
Statistic Mean St. Dev. Min Median Max
proj age 16:12 9:70 114:5 45
nsrcloc 48 ;712:02 47 ;770:60 117 40 ;912 196 ;436
ntestloc 7;969:439;689:29 03;419 43;406
nstars 165 :32 254 :38 0 43 1 ;105
nforks 41:66 55:82 0 19 294
ciuse 0 :55 0 :50 0 1 1
npropen 572:05 407:49210 447 2;783
nissues open 20 :49 18 :42 1 15 96
nbugissues 5:96 7:52 0 3 34
ncore bugs 5 :02 6 :81 0 2 33
nuser bugs 0:94 2:04 0 0 13
x commits ,e.g., by searching for defect-related keywords
(error ,bug,x, etc.) in commit messages [31]. However,
this approach may have low accuracy, even when augmented
with a topic-based prediction model [13]. Second, in some
projects developers assign labels to issues reported in the
project's issue trackers ( e.g.,bug,feature request ) for eas-
ier coordination and management [44], which may enable a
more accurate identication of bugs. However, on GitHub
and its integrated issue tracker, the use of tagging is not
enforced and, consequently, only few projects tag their issue
reports [6]. In the current study we adopt a conservative
stance, and trade scale for accuracy; relying on heuristics
for identifying bug-x commits would allow us to perform
the analysis on more projects (larger scale), but would ar-
guably be less accurate; instead, we choose to investigate a
smaller sample of GitHub projects, i.e., those that use is-
sue tagging consistently, in trying to maximize data quality
(accuracy).
From our list of candidate projects that adopted Travis-
CI, we select those that use the GitHub issue tracker (at
least 100 issues reported) and tag their issues (at least 75%
of issues have tags), for a total of 42 projects.11For each
project we separate bugissues (indicative of quality) from
other issue types ( e.g., discussions, feature requests), as fol-
lows. Since tagging is project specic, we start by manually
reviewing how project managers use tags to label bugs in
some highly active GitHub projects ( e.g.,rails ,scipy ),
and compile a list of bug-related keywords, i.e.,defect ,error ,
bug,issue ,mistake ,incorrect ,fault, and aw, after lowercas-
ing and Porter stemming. We then search for these stems
in issue tags in all projects, and label the issue as bugif any
of its tags (potentially multiple) contains at least one stem.
We further distinguish between core developer bugs ,i.e.,
those reported internally, by core developers, and user bugs ,
i.e., those reported externally, by users. As discussed in Sec-
tion 2, adoption of CIis expected to shorten the feedback
loop, and allow developers to discover bugs quicker. At the
same time, the eects of this should be positive on external
software quality, i.e., users should not experience an increas-
ing number of defects. The distinction between internally-
reported and externally-reported bugs should allow us to
more clearly observe eects associated with adoption of CI
1143 projects met these criteria, but one was additionally
removed because we did not nd any of its test les.on external quality. Note that we again adopt a conserva-
tive stance, and label as core developer bugs those issues
reported by people who eventually became core developers
for that project, even if they were not yet core developers at
the time of reporting the issue. This ensures that user bugs
are in fact bugs reported by people that have never been di-
rectly aliated with the project ( i.e., part of its core team),
at least by the end date of our data collection eorts.
Then, similarly as before, we compute monthly project-
level data for a period of 24 months centered around the
adoption date of Travis-CI, for a total of 562 rows of data
summarized in Table 3. In addition to measures described
above (Table 2), we recorded the number of bugs reported
during the current month ( nbugissues ), broken down by
reporter ( ncore bugs ;nuser bugs ).
3.2 Analysis
To answer each of our research questions, we use multiple
regression modeling to describe the relationship between a
set of explanatory variables (predictors, e.g., usage ofCI)
and a response (outcome, e.g., number of bugs reported per
unit time), while controlling for known covariates that might
inuence the response.
Our data is challenging in two ways: (i) most of our pre-
dictors and all our response variables are counts ( e.g., of
bugs reported, pull requests merged, developers, etc.); some
outcome variables are over-dispersed, i.e., the variance is
much larger than the mean; and (ii) some response variables
present an excess number of zeros ( e.g., when modeling the
number of bugs reported by external developers in a given
project per month, as a measure of quality, there is an over-
abundance of months during which no bug issues had been
submitted, i.e., an excess number of zero values compared to
non-zero values). To deal with the former, we use negative
binomial (NB) regression, a class of generalized linear mod-
els used to model non-negative integer responses, suitable
for modeling over-dispersed count data [1].
In the latter case, tting a single model on the whole data
would assume that both the zero and non-zero values come
from the same distribution, which may not be the case ( e.g.,
zeros might be due to core developers not being active dur-
ing those months, or to them being busy with other activ-
ities). We deal with this issue by using zero-inated mod-
els [26], i.e., mixture models capable of dealing with excess
zero counts, by combining a count component (we use NB)
and a point mass at zero (we use a logit model to model
which of the two processes the zero outcome is associated
with). In R [37], zero-inated models are implemented in
thepscl package [23]. To ensure that using zero-inated
models is necessary (having many zeros doesn't necessarily
mean that they were generated by dierent processes, i.e.,
that they come from dierent distributions), we compare the
t of the zero-inated model to that of a conventional NB
model using Vuong's test of non-nested model t [50].
Where appropriate, we also log transform predictors to
stabilize the variance and improve model t [7]. To avoid
multicollinearity between explanatory variables, we consider
the VIF (variance ination factor) of the set of predictors,
comparing against the recommended maximum of 5 [7] (in
our case all remained well below 3, indicating absence of
multicollinearity).
In regression analysis, few data points may sometimes
have disproportionate eects on the slope of the regressionequation, articially inating the model's t. This is a
common occurrence when explanatory variables have highly
skewed distributions, as most of ours do ( e.g., very few
projects have an extremely large number of forks). When-
ever one of our variables xis well tted by an exponential
distribution, we identify, and reject as outliers, values that
exceedk(1 + 2=n)median (x) +[34], where is the ex-
ponential parameter [39], and kis computed such that not
more than 3% of values are labeled as outliers. This reduces
slightly the size of the data sets onto which we build the
regression models, but ensures that our models are robust
against outliers.
4. RESULTS AND DISCUSSION
4.1 Team Productivity (RQ1)
The rst question we address pertains to team-level ( i.e.,
project-level) productivity, understood here as the ability
of core team members (integrators) to handle pull requests
eectively and eciently. As discussed in Section 2, a de-
sirable pull-based development workow should be both ef-
fective , in that pull requests are eventually accepted ( i.e.,
merged into the codebase), and ecient , in that a large vol-
ume of incoming pull requests can be handled quickly by
core developers, without compromising quality [17].
To address this question, we model the number of pull
requests merged per month as a response against explana-
tory variables that measure test coverage (measured as a
count of the number of source lines of code in test les ;
ntestloc) and usage ofCI(ciuse, binary variable that
encodes whether or not the current project month falls after
the date of adoption of Travis-CI), while controlling for vari-
ous confounds, which we recall below (see also the discussion
in Section 3.1.3).
team size is the number of developers on the project's core
team, at that time; larger teams are expected to be able to
handle a higher volume of pull requests.
nforks , the number of project forks at that time; we use
this as a proxy measure for the size of the contributor base;
contributors create forks, and contributions in the form of
pull requests arise from forks.
nsrcloc, the number of source lines of code in source code
les at that time, after excluding documentation les, test
les, etc; larger projects are expected to receive, and merge,
more pull requests.
proj age, the number of months since the project's cre-
ation; growth dynamics may be dierent in younger vs. older
projects.
nstars , the number of stars received by the project's main
repository by that time, used as a proxy for project popu-
larity. Starring is one of the GitHub -specic social media
features, that allows anyone to ag, i.e., star, projects they
nd interesting; more popular projects can be expected to
receive more pull requests.
Modeling Considerations.
Our primary measure of project performance is the num-
ber of pull requests received and processed by the project's
core team. There are two possible outcomes of each pull-
request: merged andrejected . These are two distinct process
outcomes; it is quite reasonable to expect that the processes
that lead to them, and their determinants, are dierent, viz.,
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè125102050100
TimeNumber of pull requests received
2012‚àí01 2012‚àí07 2013‚àí01 2013‚àí07 2014‚àí01 2014‚àí07 2014‚àí11
0.00.20.40.60.81.0
TimeFraction of core developer pull requests
2012‚àí01 2012‚àí07 2013‚àí01 2013‚àí07 2014‚àí01 2014‚àí07 2014‚àí11Figure 2: Top: Number of pull requests received monthly
per project, excluding outliers (log y-axis). Bottom : Frac-
tion of core developer pull requests.
drawn from dierent samples. Therefore, we create separate
models for merged (accepted) and rejected pull requests.
In addition, we expect that pull request processing would
function dierently for insiders and outsiders. Prior work [19,
28,45,51] suggests that the strength of the social relationship
between submitter and evaluators plays an important role
in contribution acceptance, and that pull requests submit-
ted by core developers are treated preferentially. With time,
an increasingly larger fraction of the pull requests received
each month by projects are submitted by core developers
(Figure 2). We therefore split the data into two groups by
pull request submitter ( core developers versus non-core de-
velopers, or external contributors ), and present separate sets
of models for each group.
Results and Discussion.
Table 4 presents the zero-inated negative binomial (ZINB)
core developer models, for merged (left) and rejected (right)
pull requests. Similarly, Table 5 presents the zero-inated
negative binomial (ZINB) external contributor ( i.e., non
core developer) models, for merged (left) and rejected (right)
pull requests. In both cases, all models ( Merged and Re-
jected ) t the data signicantly better than the correspond-
ing null models ( i.e., the intercept-only models), as evident
from chi-squared tests on the dierence of log likelihoods.
Additionally, both models represent improvements over a
standard NB regression, as conrmed by Vuong tests (ZINB,
NB) in each case.
Each model consists of two parts, a count part (the columns
\Count"), and a zero-ination part (the columns\Zero-in").
In the count part, we present the negative binomial regres-
sion coecients for each of the variables, along with their
standard errors. In the ination part, similarly, we present
the logit coecients for predicting excess zeros, along with
their standard errors. The statistical signicance of coe-
cients is indicated by stars.
We discuss the eects of each explanatory variable across
all models. The coecient for team size is statisticallyTable 4: Zero-inated core developer pull request models.
The response is the number of core developer pull requests
merged (left) and rejected (right) per month.
Merged PRs Rejected PRs
Count Zero-in Count Zero-in
(Intercept) 0 :625 0:834  0:9452:886
(0:134) (0:498) (0:225) (0:620)
team size 0 :212 1:2590:163 1:398
(0:009) (0:095) (0:015) (0:165)
proj age  0:0100:0260:0120:032
(0:002) (0:005) (0:003) (0:007)
log(n stars+0.5)  0:031 0 :3750:038 0 :101
(0:016) (0:059) (0:028) (0:069)
log(n forks+0.5)  0:155 0:028  0:099 0:070
(0:021) (0:066) (0:039) (0:091)
log(n srcloc+0.5) 0 :1300:073 0 :008  0:030
(0:015) (0:046) (0:026) (0:067)
log(n testloc+0.5) 0 :059 0:022 0 :085 0:012
(0:009) (0:024) (0:017) (0:046)
ciuseTRUE 0 :187 0:894 0:353 0:714
(0:049) (0:144) (0:076) (0:215)
Log(theta)  0:124 0:595
(0:037) (0:064)
AIC 22079 :594 22079 :594 9249 :581 9249 :581
Log Likelihood  11022 :797 11022 :797 4607:790 4607:790
Num. obs. 4148 4148 4148 4148
p < 0:001,p < 0:01,p < 0:05
signicant in the core developer models (Table 4). As ex-
pected, we can see a positive relationship between team size
and number of pull requests merged: the expected increase
in the response for a one-unit increase in team size is 1.236
(e0:212), holding other variables constant. Stated dierently,
adding one member enables the team to merge 23.6% more
core developer pull requests. Team size also has a signi-
cant positive relationship with the number of pull requests
rejected, i.e., larger teams also tend to reject more pull re-
quests. Taken together, the two results conrm the expected
eect of team size: larger teams are able to process more
pull requests, irrespective of whether these will be eventu-
ally merged or rejected. Interestingly, this is not the case
in the external contributor models (Table 5), where the co-
ecient for team size is not signicant in the Merged part,
i.e., larger teams do not also merge more external contribu-
tions; in fact, they reject more. In the core developer models
(Table 4), team size is also signicant, and has a negative
eect, in the zero ination models. The log odds of being
an excessive zero for merged pull requests would decrease by
1.26 (1.4 for rejected pull requests) for every additional team
member. Overall, the larger the team, the less likely that
the zero would be due to core developers not submitting pull
requests. Put plainly, the larger the team, the more likely
that core developers would submit pull requests, and also
the more likely that these would be merged.
In the core developer models (Table 4), project age has
a signicant negative eect on merged pull requests, and a
signicant positive eect on rejected ones. Older projects
accept fewer / reject more pull requests from core members.
The expected decrease in the number of pull requests merged
for a one-month increase in project age holding other vari-
ables constant is, however, very small ( 1%). In both zero
ination parts, the relationship is positive and signicant,
suggesting also that it is more likely that core developers
would not submit pull requests in older projects. This can
be explained, perhaps, by individual core developer activityTable 5: Zero-inated external contributor pull request
models. The response is the number of external contributor
pull requests merged (left) and rejected (right) per month.
Merged PRs Rejected PRs
Count Zero-in Count Zero-in
(Intercept) 0 :355 1:109  0:791 2:101
(0:129) (1:048) (0:142) (0:801)
team size 0 :015 0 :074 0 :053 0:000
(0:008) (0:049) (0:009) (0:038)
proj age  0:0080:010  0:0070:004
(0:002) (0:013) (0:002) (0:010)
log(n stars+0.5) 0 :010  0:2080:003  0:307
(0:013) (0:065) (0:014) (0:047)
log(n forks+0.5) 0 :096 0:8410:221 0:655
(0:018) (0:128) (0:020) (0:076)
log(n srcloc+0.5) 0 :172 0:095 0 :0690:196
(0:013) (0:111) (0:014) (0:073)
log(n testloc+0.5)  0:0600:1770:0440:210
(0:007) (0:082) (0:008) (0:071)
ciuseTRUE 0 :085 1 :335 0:2320:569
(0:045) (0:471) (0:050) (0:247)
Log(theta)  0:224 0:052
(0:031) (0:046)
AIC 23914 :789 23914 :789 16824 :317 16824 :317
Log Likelihood  11940 :394 11940 :394 8395:158 8395:158
Num. obs. 4148 4148 4148 4148
p < 0:001,p < 0:01,p < 0:05
generally diminishing with time. In the external pull request
models (Table 5), project age has a signicant negative ef-
fect on both merged and rejected pull requests. The older
the projects, the fewer external pull requests they merge,
and also the fewer they reject. Stated dierently, the older
the projects, the fewer external pull requests they receive.
Project popularity (nstars) has a signicant eect only
in the merged ination part in the core developer models
(Table 4). The log odds of being an excessive zero would
increase in more popular projects. There is no signicant
eect of popularity on rejected PRs, in either count or zero-
ination; nor on the number of merged PRs. Only the num-
ber of periods with zero merged pull-requests increases. This
suggests that the more popular the project, the less the core
developers are likely to submit pull requests. This can be
explained by the presence of larger pools of external contrib-
utors in these projects. Indeed, we can see in the ination
parts of the external contributor pull request models (Ta-
ble 5) that external contributors are more likely to submit
pull requests in more popular projects (the coecients are
negative and signicant in the ination parts of both the
Merged and the Rejected models).
The negative eect of number of forks in the count part
in both core developer models (Table 4) can be explained by
the relatively small size of a project's core team compared
to the size of its community of external contributors, who
create forks of the project in order to submit pull requests.
The more forks a project has, the fewer pull requests are sub-
mitted by core developers. Indeed, we can see very clearly
in the external contributor models (Table 5) that the more
forks a project has, the more pull requests are submitted by
non-core developers.
Project size (nsrcloc) has an expected positive eect
on the number of core developer pull requests being merged
(Table 4). Naturally, the project size grows as a result of core
developers integrating their contributions. In the Rejected
model, project size is not signicant. In contrast, projectsize has a signicant positive eect on both merged and re-
jected pull requests by external contributors (Table 5), i.e.,
larger projects simply receive more external contributions.
However, the disparate signicance of project size in the
Merged and Rejected models conrms that core developer
pull requests are accepted preferentially.
Next, we turn our attention to the main explanatory vari-
ables,CIuse and test coverage. Naturally, the eectiveness
ofCIdepends on the availability and scale of existing test
suites. In the core developer models (Table 4), the size
of test les (ntestloc) has a signicant positive eect on
the pull request count in both the Merged and the Rejected
models. That is, having more tests allows team members to
process more core developer pull requests, but there is no
dierential eect on their acceptance and rejection due to
testing alone. Interestingly, the external contributor mod-
els (Table 5) show that having more tests is associated with
fewer pull requests by non-core members being accepted,
and more being rejected. This suggests that the availability
and coverage of tests enables team members to nd more
defects in code submitted by external contributors, which
results in fewer of their pull requests being accepted. How-
ever, these eects are mediated by the presence of CI, as we
discuss next.
CIuse has a signicant positive eect on the number
of merged pull requests, and a signicant negative eect on
the number of rejected pull requests, in the core developer
models (Table 4). Holding other variables constant, the ex-
pected increase in the number of merged core developer pull
requests in projects that use CI(i.e., a one-unit increase in
ciuse, from FALSE, encoded as 0, to TRUE, encoded as 1)
is 20.5%. Similarly, the expected decrease in the number
of rejected pull requests in projects that use CIis 42.3%.
In the external contributor models (Table 5), CIuse has a
negative eect on the count of pull requests rejected, with a
sizable 26% eect. This suggests that external contributors
may also benet from the availability of CI: by being able to
receive immediate feedback on whether their contributions
can be merged ( i.e., pass integration testing), they can more
easily improve their code if necessary ( i.e., if tests fail) and
update the pull request, resulting in overall fewer of their
pull requests being rejected.
Result 1 :Teams usingCIare signicantly more eec-
tive at merging pull requests submitted by core members.
Availability ofCIis also associated with external con-
tributors having fewer pull requests rejected.
4.2 Code Quality (RQ2)
To address this question, we model the number of bug
reports (i.e., issues clearly labeled as bugs, as discussed in
Section 3.1.4) raised in a project each month, as a response
against explanatory variables that measure test coverage
(measured as a count of the number of source lines of code in
test les ;ntestloc) and usage ofCI(ciuse, binary vari-
able that encodes whether or not the current project month
falls after adoption of Travis-CI). Similarly as in the previous
research question, we control for various confounds: the size
of of the project's contributor base ( number of project forks ;
nforks ), the size of the project ( number of source lines
of code in source code les ;nsrcloc), the project's age(proj age), and the project's popularity ( number of stars ;
nstars ). In addition, we control for the number of non-bug
issue reports received that month ( nnonbugissues ), as
a measure of activity or general interest in the project.
We model separately the counts of bugs reported by core
developers, and those reported by external contributors, since
we expect adoption of CImay impact these sub-populations
dierently. For example, core developers may report bugs
regularly as part of their development process, while exter-
nal contributors may be more inclined to report bugs when
they experience defects while using the software.
Table 6 presents the zero-inated negative binomial (ZINB)
models for bugs reported by core developers (left, denoted
Core Dev. Bugs ) and external contributors (right, denoted
External Bugs ). Both models provide a signicantly better
t for the data than the corresponding null models ( i.e., the
intercept-only models), as evident from chi-squared tests on
the dierence of log likelihoods. Additionally, both models
represent improvements over a standard NB regression, as
conrmed by Vuong tests (ZINB, NB) in each case.
We start by discussing the eects associated with our con-
trols. As expected, the number of non-bug issue reports
has a signicant and positive eect on the response, in both
models. Projects with an increased activity on the issue
tracker are more likely to receive more bug reports, both
from external contributors as well as from core developers.
Project age has a signicant negative eect on the count
of bugs reported by core developers. The older the project,
the fewer bug reports it receives from core developers. Sim-
ilarly, the project's popularity has a signicant negative
eect on the count of bugs reported by core developers, i.e.,
the more popular the project, the fewer bug reports it re-
ceives from internal developers. These two results indicate
an increasing reliance of the core team on external contrib-
utors, with time, and as the project becomes more popular
and has access to a larger pool of external contributors or,
perhaps, a shifting focus in the core team, from xing bugs
to implementing new features. The number of forks has
a signicant positive eect on the bug report count in both
models, supporting the popularity result. The more forks a
project has, the more contributors is has (both internal and
external), and therefore the more bug reports it receives.
Thesize of test les is signicant in the ination part
of the external bugs model, and has a negative eect. The
log odds of being an excessive zero for external bug reports
would decrease by 1.96 for every unit increase in the log of
the number of test lines. In other words, the larger the test
suite, the less likely that the zero would be due to exter-
nal contributors not submitting bug reports, i.e., the more
likely that external contributors would report bugs. We at-
tribute this association to the post hoc addition of test cases
by developers: when a bug is reported by users, it is good
practice to augment the repaired code with an automated
test to ensure that this bug doesn't re-occur by progression.
Since we cumulate our variables monthly, we observe an as-
sociation between increased test cases, and the presence (or
rather, non-absence) of user-reported bugs.
CIusehas a signicant positive eect on the count of bug
reports by core developers. Holding other variables constant,
the expected increase in the number of bug reports by core
developers in projects that use CIis 48%. In contrast, CI
use does not have any eect on the count of bug reports by
external contributors. That is, all other things being equal,Table 6: Zero-inated project quality models. The response
is the number of bugs reported per project each month by
core developers (left) and external contributors (right).
Core Dev. Bugs External Bugs
Count Zero-in Count Zero-in
(Intercept) 2 :0123:185 0 :001 1 :385
(0:397) (1:975) (0:673) (2:115)
nnonbugissues 0 :039 0:1970:033 0:364
(0:005) (0:078) (0:008) (0:242)
proj age  0:026 0:031  0:002 0 :049
(0:008) (0:075) (0:010) (0:045)
log(n stars+0.5)  0:122 0:161 0 :006  0:386
(0:041) (0:244) (0:083) (0:457)
log(n forks+0.5) 0 :155 0:311 0 :3320:219
(0:057) (0:347) (0:116) (0:339)
log(n srcloc+0.5)  0:068  0:084  0:104 0 :475
(0:046) (0:252) (0:080) (0:249)
log(n testloc+0.5)  0:023  0:211  0:071  0:674
(0:026) (0:155) (0:069) (0:197)
ciuseTRUE 0 :392 0:899 0 :164  0:201
(0:120) (0:965) (0:206) (0:936)
Log(theta)  0:035  0:518
(0:103) (0:155)
AIC 1309 :820 1309 :820 2827 :910 2827 :910
Log Likelihood  637:910 637:910 1396:955 1396:955
Num. obs. 562 562 562 562
p < 0:001,p < 0:01,p < 0:05
adoption ofCIenables team members to discover more bugs,
but this does not seem to have any inuence on the project's
external quality, as indicated by the count of bug reports by
non-core developers. As observed earlier, the overall num-
ber of pull requests managed (both merged and rejected)
increases afterCI; and this increased volume is being man-
aged by the core developers without a corresponding increase
in the number of user-reported bugs. This suggests that CI
allows an increase in productivity (as per Result 1) without
a signicant negative eect on user-experienced quality.
Result 2 :Core developers in teams using CIare able to
discover signicantly more bugs than in teams not using
CI. This does not come at a cost to external software
quality, as external contributors do not experience an
increasing number of defects.
5. CONCLUDING REMARKS
Process integration and automation have been an impor-
tant topic of late, specially with the rise of DEVOPS. In this
paper, we leverage the growth and diversity of the projects
onGitHub to study the eect of one aspect of process in-
tegration and automation: the eect of introducing Contin-
uous Integration to the pull request process. Our ndings
clearly point to the benets of CI: more pull requests get
processed; more are being accepted and merged, and more
are also being rejected. Moreover, this increased productiv-
ity doesn't appear to be gained at the expense of quality.
This must be considered a preliminary study. The exact
mechanisms that allow developers to process more pull re-
quests need to be better understood, perhaps viadetailed
case studies. However, our models yield results of quanti-
tative signicance, indicating that the ndings are robustly
manifested in our data.Several threats should be noted. First, some of the rel-
evant properties, such as popularity of projects with users,
and developer interest in the project, are clearly confounds
that aect our outcomes. More user attention will certainly
lead to more bugs; and more (non-core) developer interest
will increase project productivity. However, both are di-
cult to measure directly, so we measure both indirectly. User
attention is measured using the stars awarded in GitHub
(nstars ); we assume that more stars are a good proxy for
more user interest. Likewise, we use the number of forks
(nforks ) as a proxy for developer interest. While these
measures are intuitively justiable, the use of such indirect
measures is a potential internal validity threat.
Second, one might like to take the experimental posture
thatCIintroduction is an independent, causal factor, and
seek to identify the eects thereof. However, some projects
may have introduced CIbecause they were experiencing high
interest; also, they may have introduced CIbecause they al-
ready had a strong quality culture. It's also possible that the
introduction ofCIcauses people to behave dierently, for
example to maintain reputations in the face of more rigorous
testing. Our study cannot distinguish between these various
eects; all we can study with our modeling is the quality and
productivity eects associated with the introduction of CI.
Perhaps in the end, whatever be the modality of the eects,
our ndings, that CIuse appears to be associated with pro-
ductivity gains, without signicantly sacricing quality, is
per se good enough to support its use.
Third, the data we gathered comes from a relatively small
number of projects, compared to the size of GitHub . One of
the reasons was that we set on the Travis-CI system; while
others are available and in use in GitHub , we wanted to
make sure the comparison was fair and even across projects.
The amount of data we did get guarantees sucient power
for our models and results; however it is always possible
that our sample was biased in some unknown way, thus di-
minishing the generalizability of our results; independent
replication remains the best way to mitigate this threat.
6. ACKNOWLEDGEMENTS
BV, PD, and VF are partially supported by NSF under
grants 1247280 and 1414172. YY and HW gratefully ac-
knowledge support from the National Science Foundation of
China (grants 61432020 and 61472430).
7. REFERENCES
[1] P. D. Allison and R. P. Waterman. Fixed{eects
negative binomial regression models. Sociological
Methodology , 32(1):247{265, 2002.
[2] E. T. Barr, C. Bird, P. C. Rigby, A. Hindle, D. M.
German, and P. Devanbu. Cohesive and isolated
development with branches. In FASE , pages 316{331.
Springer, 2012.
[3] A. Begel, J. Bosch, and M.-A. Storey. Social
networking meets software development: Perspectives
from GitHub, MSDN, Stack Exchange, and TopCoder.
IEEE Software , 30(1):52{66, 2013.
[4] A. Bhattacharya. Impact of continuous integration on
software quality and productivity. Master's thesis,
Ohio State University, 2014.[5] C. Bird, A. Gourley, P. Devanbu, A. Swaminathan,
and G. Hsu. Open borders? Immigration in open
source projects. In MSR , pages 6{6. IEEE, 2007.
[6] T. F. Bissyande, D. Lo, L. Jiang, L. Reveillere,
J. Klein, and Y. Le Traon. Got issues? Who cares
about it? A large scale investigation of issue trackers
from GitHub. In ISSRE , pages 188{197. IEEE, 2013.
[7] J. Cohen, P. Cohen, S. G. West, and L. S. Aiken.
Applied multiple regression/correlation analysis for the
behavioral sciences . Routledge, 2013.
[8] L. Dabbish, C. Stuart, J. Tsay, and J. Herbsleb. Social
coding in GitHub: Transparency and collaboration in
an open software repository. In CSCW , pages
1277{1286. ACM, 2012.
[9] L. Dabbish, C. Stuart, J. Tsay, and J. Herbsleb.
Leveraging transparency. IEEE Software , 30(1):37{43,
2013.
[10] B. J. Dempsey, D. Weiss, P. Jones, and J. Greenberg.
Who is an open source software developer? CACM ,
45(2):67{72, 2002.
[11] P. M. Duvall, S. Matyas, and A. Glover. Continuous
integration: improving software quality and reducing
risk. Pearson Education, 2007.
[12] M. Fowler. Continuous integration, 2006. http:
//martinfowler.com/articles/continuousIntegration.html .
[13] Y. Fu, M. Yan, X. Zhang, L. Xu, D. Yang, and J. D.
Kymer. Automated classication of software change
messages by semi-supervised Latent Dirichlet
Allocation. Information and Software Technology ,
57:369{377, 2015.
[14] R. Gadagkar. Evolution of eusociality: the advantage
of assured tness returns. Philosophical Transactions
of the Royal Society B: Biological Sciences ,
329(1252):17{25, 1990.
[15] M. Gharehyazie, D. Posnett, B. Vasilescu, and
V. Filkov. Developer initiation and social interactions
in OSS: A case study of the Apache Software
Foundation. Emp. Softw. Eng. , pages 1{36, 2014.
[16] G. Gousios. The GHTorrent dataset and tool suite. In
MSR , pages 233{236. IEEE, 2013.
[17] G. Gousios, M. Pinzger, and A. v. Deursen. An
exploratory study of the pull-based software
development model. In ICSE , pages 345{355. ACM,
2014.
[18] G. Gousios and A. Zaidman. A dataset for pull-based
development research. In MSR , pages 368{371. ACM,
2014.
[19] G. Gousios, A. Zaidman, M.-A. Storey, and
A. Van Deursen. Work practices and challenges in
pull-based development: The integrator's perspective.
InICSE . IEEE, 2015.
[20] A. Hars and S. Ou. Working for free? Motivations of
participating in open source projects. In HICSS , pages
1{9. IEEE, 2001.
[21] J. Holck and N. Jrgensen. Continuous integration
and quality assurance: A case study of two open
source projects. Australasian Journal of Information
Systems , 11(1), 2007.
[22] J. Humble and D. Farley. Continuous delivery: reliable
software releases through build, test, and deployment
automation . Pearson Education, 2010.[23] S. Jackman. pscl: Classes and methods for R
developed in the Political Science Computational
Laboratory, 2008.
[24] K. Karhu, T. Repo, O. Taipale, and K. Smolander.
Empirical observations on software testing
automation. In ICST , pages 201{209. IEEE, 2009.
[25] F. Khomh, T. Dhaliwal, Y. Zou, and B. Adams. Do
faster releases improve software quality? An empirical
case study of Mozilla Firefox. In MSR , pages 179{188.
IEEE, 2012.
[26] D. Lambert. Zero-inated Poisson regression, with an
application to defects in manufacturing.
Technometrics , 34(1):1{14, 1992.
[27] J. Marlow and L. Dabbish. Activity traces and signals
in software developer recruitment and hiring. In
CSCW , pages 145{156. ACM, 2013.
[28] J. Marlow, L. Dabbish, and J. Herbsleb. Impression
formation in online peer production: activity traces
and personal proles in GitHub. In CSCW , pages
117{128. ACM, 2013.
[29] M. Meyer. Continuous integration and its tools. IEEE
Software , 31(3):14{16, 2014.
[30] A. Miller. A hundred days of continuous integration.
InAgile , pages 289{293. IEEE, 2008.
[31] A. Mockus and L. G. Votta. Identifying reasons for
software changes using historic databases. In ICSME ,
pages 120{130. IEEE, 2000.
[32] L. Osterweil. Software processes are software too. In
ICSE , 1987.
[33] D. Parsons, H. Ryu, and R. Lal. The impact of
methods and techniques on outcomes from agile
software development projects. In Organizational
Dynamics of Technology-Based Innovation:
Diversifying the Research Agenda , pages 235{249.
Springer, 2007.
[34] J. K. Patel, C. Kapadia, and D. B. Owen. Handbook of
statistical distributions . M. Dekker, 1976.
[35] R. Pham, L. Singer, O. Liskin, and K. Schneider.
Creating a shared understanding of testing culture on
a social coding site. In ICSE , pages 112{121. IEEE,
2013.
[36] B. Phifer. Next-generation process integration: CMMI
and ITIL do devops. Cutter IT Journal , 24(8):28,
2011.
[37] R Development Core Team. R: A language and
environment for statistical computing. R Foundation
for Statistical Computing, Vienna, Austria , 2008.
[38] B. Rady and R. Con. Continuous Testing: with
Ruby, Rails, and JavaScript . Pragmatic Bookshelf,
2011.
[39] P. J. Rousseeuw and C. Croux. Alternatives to the
median absolute deviation. Journal of the American
Statistical Association , 88(424):1273{1283, 1993.
[40] J. Sheoran, K. Blincoe, E. Kalliamvakou, D. Damian,
and J. Ell. Understanding watchers on GitHub. In
MSR , pages 336{339. ACM, 2014.
[41] D. St ahl and J. Bosch. Experienced benets of
continuous integration in industry software product
development: A case study. In The 12th IASTED
International Conference on Software Engineering,
(Innsbruck, Austria, 2013) , pages 736{743, 2013.[42] D. St ahl and J. Bosch. Modeling continuous
integration practice dierences in industry software
development. Journal of Systems and Software ,
87:48{59, 2014.
[43] S. Stolberg. Enabling agile testing through continuous
integration. In Agile Conference, 2009. AGILE'09. ,
pages 369{374. IEEE, 2009.
[44] M.-A. Storey, C. Treude, A. van Deursen, and L.-T.
Cheng. The impact of social media on software
engineering practices and tools. In FSE/SDP
Workshop on Future of Software Engineering
Research , pages 359{364. ACM, 2010.
[45] J. Tsay, L. Dabbish, and J. Herbsleb. Inuence of
social and technical factors for evaluating contribution
in GitHub. In ICSE , pages 356{366. ACM, 2014.
[46] J. Tsay, L. Dabbish, and J. Herbsleb. Let's talk about
it: Evaluating contributions through discussion in
GitHub. In FSE, pages 144{154. ACM, 2014.
[47] B. Vasilescu, V. Filkov, and A. Serebrenik.
Perceptions of diversity on GitHub: A user survey. In
CHASE , 2015.
[48] B. Vasilescu, D. Posnett, B. Ray, M. G. J. van den
Brand, A. Serebrenik, P. Devanbu, and V. Filkov.Gender and tenure diversity in GitHub teams. In CHI,
pages 3789{3798. ACM, 2015.
[49] B. Vasilescu, S. van Schuylenburg, J. Wulms,
A. Serebrenik, and M. G. J. van den Brand.
Continuous integration in a social-coding world:
Empirical evidence from GitHub. In ICSME , pages
401{405. IEEE, 2014.
[50] Q. H. Vuong. Likelihood ratio tests for model selection
and non-nested hypotheses. Econometrica , pages
307{333, 1989.
[51] Y. Yu, H. Wang, V. Filkov, P. Devanbu, and
B. Vasilescu. Wait for it: Determinants of pull request
evaluation latency on GitHub. In MSR . IEEE, 2015.
[52] Y. Yu, H. Wang, G. Yin, and C. Ling. Reviewer
recommender of pull requests in GitHub. In ICSME ,
pages 609{612. IEEE, 2014.
[53] Y. Yu, H. Wang, G. Yin, and C. Ling. Who should
review this pull request: Reviewer recommendation to
expedite crowd collaboration. In APSEC , pages
335{342. IEEE, 2014.
[54] J. Zhu, M. Zhou, and A. Mockus. Patterns of folder
use and project popularity: A case study of GitHub
repositories. In ESEM , pages 30:1{30:4. ACM, 2014.