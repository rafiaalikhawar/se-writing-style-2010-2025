CLAMI: Defect Prediction on Unlabeled Datasets
Jaechang Nam and Sunghun Kim
Department of Computer Science and Engineering
The Hong Kong University of Science and Technology, Hong Kong, China
Email:fjcnam,hunkimg@cse.ust.hk
Abstract ‚ÄîDefect prediction on new projects or projects with
limited historical data is an interesting problem in software
engineering. This is largely because it is difÔ¨Åcult to collect defect
information to label a dataset for training a prediction model.
Cross-project defect prediction (CPDP) has tried to address this
problem by reusing prediction models built by other projects
that have enough historical data. However, CPDP does not
always build a strong prediction model because of the different
distributions among datasets. Approaches for defect prediction
on unlabeled datasets have also tried to address the problem by
adopting unsupervised learning but it has one major limitation,
the necessity for manual effort.
In this study, we propose novel approaches, CLA and CLAMI,
that show the potential for defect prediction on unlabeled datasets
in an automated manner without need for manual effort. The key
idea of the CLA and CLAMI approaches is to label an unlabeled
dataset by using the magnitude of metric values. In our empirical
study on seven open-source projects, the CLAMI approach led
to the promising prediction performances, 0.636 and 0.723 in
average f-measure and AUC, that are comparable to those of
defect prediction based on supervised learning.
I. I NTRODUCTION
Defect prediction plays an important role in software qual-
ity [1]. Defect prediction techniques provide a list of defect-
prone source code so that quality assurance (QA) teams can
focus on the most defective parts of their products in advance.
In this way, QA teams can effectively allocate limited resources
on which to review and test their software products before
releasing them. In industry, defect prediction techniques have
been actively adopted for software quality assurance [2], [3],
[4], [5], [6].
Researchers have proposed and facilitated various defect
prediction algorithms and metrics [1], [4], [7], [8], [9], [10],
[11], [12], [13], [14], [15], [16], [17], [18]. Most defect predic-
tion models are based on supervised learning for classiÔ¨Åcation
(e.g., predicting defect-proneness of a source code Ô¨Åle) or
regression (e.g., predicting the number of defects in a source
code Ô¨Åle) [1], [17], [18], [19]. Rather than using a machine
learning technique, Kim et al. proposed BugCache , which
manages defect-prone entities in source code by adapting the
cache concept used in operating system [12]. Metrics for defect
prediction can be divided into code and process metrics [20].
Code metrics represent how the source code is complex while
process metrics represent how the development process is
complex [1], [20].
In particular, studies on defect prediction metrics have
been actively conducted as the use of software archives such
as version control systems and issue trackers has become
popular [20]. Most metrics proposed over the last decade such
as change/code metric churn, change/code entropy, popularity,and developer interaction have been collected from various
software archives [7], [9], [10], [11], [13], [16], [17].
However, typical defect prediction techniques based on
supervised learning are designed for a single software project
and are difÔ¨Åcult to apply to new projects or projects that have
limited historical data in software archives. Defect prediction
models based on supervised learning can be constructed by
using a dataset with actual defect information, that is, a
labeled dataset . Defect information usually accumulates in the
software archives, thus new projects or projects with a short
development history do not have enough defect information.
This is a major limitation of the typical defect prediction
techniques based on supervised learning.
To address this limitation, researchers have proposed vari-
ous approaches to enable defect prediction on projects with
limited historical data. Cross-project defect prediction that
builds a prediction model using data from other projects has
been studied by many researchers [19], [21], [22], [23], [24],
[25], [26], [27]. Defect prediction techniques on unlabeled
datasets were proposed as well [28], [29]. Recently, an ap-
proach to build a universal defect prediction model by using
multiple project datasets was introduced [30].
However, there is still an issue of different distributions
among datasets in existing approaches for cross-project defect
prediction (CPDP) and universal defect prediction (UDP). In
CPDP and UDP, the major task is to make the different
distributions of datasets similar since prediction models work
well when the datasets for training and testing a model have the
same distributions [31], [32]. However, the approaches based
on making the different distributions similar between training
and test datasets may not always be effective [22], [23].
Compared to CPDP and UDP, the existing approaches
for defect prediction on unlabeled datasets are relatively less
affected by the issue of different distributions among datasets
but will always require manual effort by human experts [28],
[29]. Zhong et al. proposed the expert-based defect prediction
on unlabeled datasets where a human expert would label
clusters from an unlabeled dataset after clustering [29]. Catal
et al. proposed the threshold-based approach where labeling
datasets is conducted based on a certain threshold value of a
metric [28]. A proper metric threshold for the threshold-based
approach is decided by the intervention of human experts [28].
Since these approaches on unlabeled datasets are conducted on
the test dataset itself, the issue of the different distributions
among datasets is not affected [28], [29]. However, these
approaches require manual effort by human experts [28], [29].
The goal of this study is to propose novel approaches,
CLA and CLAMI, which can conduct defect prediction onClassification  
Software Archives 9"8"5"6"B"1"2"3"1"C"0"1"4"5"C"8"7"5"9"B"... Instances with Metrics and Labels 9"8"5"B"1"2"3"C"8"7"5"B"... Training Instances 
(Preprocessing) Model 7"9"9"?"
New Instance Generate Instances Build a Model 0"1"4"C"7"9"9"B"Fig. 1: The typical defect prediction process based on supervised
learning.
unlabeled datasets in an automated manner. The key idea of
the CLA/CLAMI approaches is to label an unlabeled dataset
by using the magnitude of metric values.
In our empirical study, the CLA/CLAMI approach led
to promising prediction performances of 0.636 (average f-
measure) and 0.723 (average AUC), while the typical de-
fect prediction based on supervised learning showed 0.585
and 0.694 in average f-measure and AUC respectively. In
addition, the CLA/CLAMI approaches outperformed or were
comparable to the existing approaches for defect prediction on
unlabeled datasets with statistical signiÔ¨Åcance. These promis-
ing results show that our CLA/CLAMI approaches have the
potential for defect prediction on projects that have limited
historical data.
The contributions of this study are as follows:
Proposing novel approaches, CLA and CLAMI, for defect
prediction on unlabeled datasets in an automated manner.
An empirical study to evaluate the CLA/CLAMI ap-
proaches against existing defect prediction approaches.
II. B ACKGROUND AND RELATED WORK
A. Typical Defect Prediction Process
Fig. 1 shows the typical defect prediction process for a
single software project based on supervised machine learning
(classiÔ¨Åcation). Since this whole process is conducted ‚Äòwithin‚Äô
the single software project, it is called within-project defect
prediction (WPDP).
In this process, we Ô¨Årst collect various data such as
development artifacts and historical information from software
archives such as the version control system and the issue
tracker of the software project. Using the data from software
archives, we can measure the complexity of the software
project and its development process. The complexity measure-
ment can be conducted at different granularities such as the
function (method), Ô¨Åle (class), or subsystem (package) levels.
In addition, we can collect defect information for the software
project from the issue tracker and commit messages in the
version control system.
With the collected data, we can generate instances that
consist of metrics (features in a machine learning sense) and
labels. In Fig. 1, an entity with numeric values and labels,
Cross prediction model 
4 Target project (T est set) Source project (Training set) 
Model Build Predict Fig. 2: Cross-Project Defect Prediction
‚ÄòB‚Äô and ‚ÄòC‚Äô, is called an instance. Each instance represents
a function, a Ô¨Åle, or a subsystem according to their granu-
larity. Metrics measure the complexity of the software or its
development process and each metric has a numeric value in
the instance. There are various kinds of metrics such as lines
of code, the number of functions, and the number of authors
touching a source code Ô¨Åle [1], [16]. The instance is also
labeled as either buggy (B) or clean (C) by using the collected
defect information as in Fig. 1.
With the set of labeled instances, we can apply some
preprocessing techniques used in machine learning. The rep-
resentative preprocessing techniques are normalization and
feature selection and are widely used in defect prediction
studies [1], [22], [23], [33]. However, preprocessing may not
be necessary depending on the defect prediction models [10],
[13], [22].
We can then generate a dataset of training instances to
build a prediction model. Since the prediction model is built by
using supervised learning, we can use various machine learners
to build a classiÔ¨Åcation model such as Logistic Regression,
Decision Tree, Naive Bayes, and Random Forest [10], [13],
[22], [34], [35], [36].
Using the prediction model built by the dataset of training
instances, we can predict new instances as buggy or clean. For
example, as shown in Fig. 1, the new instance marked with
‚Äò?‚Äô is classiÔ¨Åed as buggy by the prediction model.
This typical defect prediction for a single project, WPDP,
has a major limitation. For any new projects or projects
with limited historical data, it is difÔ¨Åcult to build defect
prediction models since we cannot generate the labeled dataset
for training a model without defect information [18], [22]. In
Section II-B, we list related studies addressing this limitation.
B. Defect Prediction on Projects with Limited Historical Data
Researchers have tried to address the issue on new projects
or projects with limited historical data to build defect predic-
tion models by proposing techniques for cross-project defect
prediction [21], [22], [23], [24], [26], [30], [37] and defect
prediction on unlabeled datasets [28], [29].
1) Cross-Project Defect Prediction (CPDP): Fig. 2 shows
the typical CPDP process. In Fig. 2, the small shaded circles
inSource project represent labeled instances and the small
rectangles in Target project represent unlabeled instances. We
Ô¨Årst build a prediction model using the source project with
labeled instances. Then, using the model, we can predictwhether an instance in the target project is defect-prone or
not.
However, CPDP has a challenging issue that prediction
performance was not practical [18]. Zimmermann et al. con-
ducted 622 cross predictions but only 21 predictions were
successful in their experimental setting [18]. Watanabe et al.
proposed the metric compensation approach to improve CPDP
models. The approach by Watanabe et al. makes a target
project similar to a source project by normalizing metric values
using the average metric values [24]. The metric compensation
improved the CPDP models, but the prediction performance
of CPDP with the metric compensation is worse than that
of WPDP [24]. Turhan et al. proposed the nearest neighbour
(NN) Ô¨Ålter for CPDP [23]. The NN Ô¨Ålter selects the 10 nearest
source instances for each target instance [23]. In other words,
when building a prediction model, the NN Ô¨Ålter approach uses
the most similar source instances to the target instances [23].
However, its prediction performance is still worse than that of
WPDP [23].
To resolve the CPDP issue, Ma et al. and Nam et al. facil-
itated transfer learning techniques from the machine learning
community [21], [22]. Ma et al. proposed Transfer Naive Bayes
(TNB) which provides more weight to the source instances that
are similar to target instances when building a Naive Bayes
model [21]. The TNB led to better prediction performance
than the approach based on the NN Ô¨Ålter [21]. Nam et al.
adopted transfer component analysis (TCA) which is a state-
of-the-art transfer learning technique and proposed TCA+
for CPDP [22]. In the empirical study of Nam et al., the
performance of CPDP was comparable to that of WPDP [22].
Recently, CPDP models have been evaluated with a view to
its cost-effectiveness [25], [26], [27]. Rahman et al. conÔ¨Årmed
that CPDP models can outperform WPDP models in terms
of cost-effectiveness. Canfora et al. proposed multi-objective
approach for CPDP [26]. Multi-objective models built using
a genetic algorithm help software engineers choose prediction
models having different objectives such as high recall or low
cost [26]. In their empirical study, multi-objective models
achieved better prediction results than a WPDP model in
terms of cost-effectiveness [26]. Panichella et al. proposed a
combined defect predictor (CODEP) for CPDP [27]. CODEP
combines defect prediction results from different machine
learning models and led to better prediction performance than
a single prediction model in AUC and cost-effectiveness [27].
Zhang et al. addressed the CPDP issue by proposing the
universal defect prediction model [30]. Since the individual
project may have its speciÔ¨Åc defect characteristic, the universal
model may not work for all projects [30]. To resolve this
limitation, Zhang et al. proposed context-aware rank transfor-
mations that change metric values ranging from 1 to 10 across
all projects [30]. In this way, the universal model could be built
using 1,398 projects from SourceForge and Google code. In
their experimental setting it showed a comparable prediction
performance to WPDP [30].
The related studies about CPDP actually addressed the
same issue we address in this study, that is, defect prediction
on unlabeled dataset. However, in contrast to our CLA/CLAMI
approaches, the CPDP approaches always require abundant
source project datasets and the prediction model can be con-structed when both source and target projects have the same
metric set.
Most studies for CPDP try to make the different distribu-
tions of source and target datasets similar by using techniques
such as transforming metric values [24], [30], selecting similar
instances [23], and using transfer learning [21], [22]. However,
those techniques cannot always effectively make different
distributions similar when compared to the case where the
same project dataset is used. Therefore, they can still suffer
from the dataset shift problem [31]. Recently, this problem
has also been observed even in different releases of the same
project [38].
CLA and CLAMI do not need any techniques to make
different distributions of datasets similar since we just use the
same project dataset where we want to predict defects. In other
words, the CLA/CLAMI approaches do not need to consider
any source projects and the limitation of different metric sets
in project datasets to build a prediction model.
2) Defect Prediction on Unlabeled Datasets: There are
a couple of studies for defect prediction on unlabeled
datasets [28], [29].
Zhong et al. proposed the expert-based approach [29]. The
expert-based approach Ô¨Årst clusters unlabeled instances using
a clustering algorithm such as K-means, then asks a human-
expert whether a cluster is defect-prone or not, after providing
average metric values of the cluster, that is, centroid [29].
Using the expert-based approach, Zhong et al. achieved a
12.08% false positive rate and a 31.13% false negative rate
in the best cases.
The expert-based approach always requires human experts
to decide whether a cluster is defect-prone or not. Thus, this
approach cannot fully automate defect prediction on unlabeled
datasets. However, our approaches, CLA and CLAMI, need no
human experts and can automate the defect prediction process
on unlabeled datasets.
Catal et al. proposed a one-stage threshold based ap-
proach [28]. After initially proposing a two-stage approach
based on both clustering and threshold [28], they concluded
that the one-stage threshold based approach is easier than
the two-stage approach and still effective enough [28]. The
threshold-based approach predicts an instance as buggy when
any metric value is greater than the given metric threshold
values [28]. The threshold values were decided based on
‚Äòexperience and hints from the literature‚Äô, past defect-prone
modules, and analysis of past versions of a project [28]. The
one-stage threshold-based approach achieved a 32.14% false
postive rate and a 20% false negative rate [28].
The threshold-based approach needs to decide metric
threshold values in advance. In other words, additional effort
is required for metric threshold values. However, CLA and
CLAMI do not need additional effort and can build a prediction
model using only unlabeled datasets.
III. A PPROACH
Fig. 3 shows the overall process of our CLA and CLAMI
approaches for defect prediction on an unlabeled dataset. The
key idea of our approaches is to label unlabeled instances by
using the magnitude of metric values.CLAMI Approach Overview 
69 
Unlabeled Dataset 
(1) Clustering (2) Labeling (3) Metric Selection (4) Instance Selection (5) Metric      Selection CLAMI Model  Build 
Predict 
Training dataset 
Test dataset 
CLA Model  (1) Clustering (2) Labeling 
Fig. 3: The overview of CLA and CLAMI for defect prediction on an
unlabeled dataset. We name our approaches CLA or CLAMI based
on the two or four steps respectively.
The Ô¨Årst two steps for CLA and CLAMI are (1) Clustering
instances and (2) Labeling instances in clusters. With these
two steps, we can label/predict all instances in the unlabeled
dataset. CLA consists of only these two steps. Based on the
two steps, we name our Ô¨Årst approach CLA .
CLAMI has two additional steps to generate a training
dataset, (3) Metric selection, and (4) Instance selection. Based
on all four steps, we name our second approach CLAMI . In
the machine learning community, metric (feature) selection and
instance selection have been widely used to improve prediction
models by removing noisy metrics and instances [39]. In
addition, metric selection and instance selection have also been
applied to improving various prediction models in the software
engineering community [33], [40], [41], [42], [43].
For CLAMI, (5) we also select metrics from the unlabeled
dataset to generate a test dataset with the same set of metrics as
in the training dataset generated by CLAMI since both training
and test datasets should have the same metric set. The test
dataset includes all the instances from the original unlabeled
dataset but with only selected metrics. When the training and
test datasets are ready for CLAMI, we can build a prediction
model using various machine learning classiÔ¨Åers and conduct
defect prediction for the unlabeled test dataset.
The following subsection describes the four steps in detail.
A. Steps for CLA and/or CLAMI
1) Clustering Instances: Fig. 4 shows how to cluster in-
stances in a dataset. In Fig. 4, X1‚ÄìX7represent metrics of the
unlabeled dataset, while Inst. A‚ÄìGrepresent instances of the
dataset. We Ô¨Årst identify higher metric values that are greater
than a speciÔ¨Åc cutoff threshold. In the example of Fig. 4, we
use a median value for each metric as the threshold to decide
higher metric values.1For example, the median of the metric,
X1, is1fromf0,1,1, 1,1,2,3g. Then we can identify the higher
metric values of X1, i.e.f2,3g. In the same way, we compute
the higher metric values for other metrics as well. In Fig. 4,
the higher metric values that are greater than a corresponding
median value are in bold font. After identifying the higher
1In Section VI, we apply various cutoff thresholds to decide higher metric
values and compare prediction performances on the various thresholds.
{X1,X4}''Cluster, K=3 Unlabeled Dataset X1#X2#X3#X4#X5#X6#X7#Label#3"1'3"0'5'1'9"?'1'1'2'0'7"3"8'?'2"3"2'5"5'2"1'?'0'0'8"1'0'1'9"?'1'0'2'5"6"10"8'?'1'4"1'1'7"1'1'?'1'0'1'0'0'1'7'?'1'1'2'1'5'1'8'Median Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F inst. G Instances K = the number of metrics        whose values are        greater than Median.   C Cluster, K=4 A, E B, D,  F Cluster, K=2 G Cluster, K=0 
(1) Clustering 
(4) Instance Selection X1#X2#X3#X4#X5#X6#X7#Label#3"1'3"0'5'1'9"Buggy"1'1'2'0'7"3"8'Clean"2"3"2'5"5'2"1'Buggy'0'0'8"1'0'1'9"Clean'1'0'2'5"6"10"8'Buggy"1'4"1'1'7"1'1'Clean'1'0'1'0'0'1'7'Clean'Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F Inst. G 1'‚Äì'7'3'‚Äì'7'3'‚Äì'7'1'‚Äì'7'4'‚Äì'7'2'‚Äì'7'3'‚Äì'7'X1#X5#X7#Label#3"5"9"Buggy"1'7"8"Buggy"2'5"1'Clean'1'0'9"Clean'5"5"8"Buggy"0'2'1'Clean'X1#X5#X7#Label#3"5"9"Buggy"5"5"8"Buggy"0'2'1'Clean'
Final Training Dataset (2) Labeling Clusters Instance Conflicts 1/7#2/7#3/7#4/7#1'3'3'4'0'1'1'2'0'0'1'3'0'0'1'2'0'1'2'4'1'1'1'1'0'0'0'1'Metric Conflict Scores 
1/7'2/7'3/7'4/7'
Selected'Metrics'
X1#X4#Label#3"0'Buggy"1'0'Clean"2"5"Buggy'0'1'Clean'1'5"Buggy"1'1'Clean'1'0'Clean'Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F Inst. G 
2'‚Äì'7'X1#X4#Label#1'0'Clean"2"5"Buggy'0'1'Clean'1'1'Clean'1'0'Clean'Inst. B Inst. C Inst. D Inst. F Inst. G Final Training Dataset Fig. 4: Clustering and Labeling. Any metric values greater than the
median values are in bold font.
metric values, we can count the number of the higher metric
values in each instance. For example, in the instance A, the
number ( K) of the higher metric values is three (3 in X1, 3 in
X3, and 9 in X7). After computing Kvalues of all instances,
we can group instances that have the same Kvalue. As shown
in Fig. 4, we can form four clusters with K=4,3,2, and 0.
2) Labeling: By considering Kvalues, we divide clusters
into two groups, a top half and a bottom half, and label the
instances in the top half of the clusters as buggy and the others
as clean. In Fig. 4, the instances A,C, and Ein the clusters,
K=4 and K=3, are labeled as buggy and other instances are
labeled as clean.
The intuition of this labeling process is based on the defect-
proneness tendency of typical defect prediction datasets, that
is,higher complexity causes more defect-proneness [1], [10],
[20]. In other words, since typical defect prediction metrics
measure the complexity of the source code and development
process, there is a tendency that buggy instances have higher
metric values than clean instances [1], [7], [9], [10], [11], [16],
[17], [20], [44].
In this sense, we label instances in the top half of the
clusters as buggy since the number of the higher metric values
in the instances of the top half of the clusters is more than that
of the bottom half of the clusters.
3) Metric Selection: After labeling the instances in the top
and bottom clusters, we conduct metric selection based on the
metric violation scores (MVS) for CLAMI. Since the quality of
defect prediction models is highly dependent on the quality of
the metrics, metric selection to choose the most informative
metrics for prediction models has been widely adopted in
defect prediction [33], [40], [41]. Metric selection in CLAMI
is based on removing metrics that can minimize violations in
the defect-proneness tendency of defect datasets [1], [10], [20].
Not all metrics follow the defect-proneness tendency so that
metric selection of CLAMI can be helpful to build a better
prediction model.
A violation is a metric value that does not follow the defect-
proneness tendency. The metric value (1) in X1of the instance
Elabeled as buggy is not greater than the median (1) so this
case is counted as a violation. The metric value ( 4) ofX2in the
instance Flabeled as clean is greater than its median value (1)
so this case is also considered to be a violation. By counting
all these violations, we can deÔ¨Åne the metric violation score{X1,X4}''Cluster, K=3 Unlabeled Dataset X1#X2#X3#X4#X5#X6#X7#Label#3"1'3"0'5'1'9"?'1'1'2'0'7"3"8'?'2"3"2'5"5'2"1'?'0'0'8"1'0'1'9"?'1'0'2'5"6"10"8'?'1'4"1'1'7"1'1'?'1'0'1'0'0'1'7'?'1'1'2'1'5'1'8'Median Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F inst. G Instances K = the number of metrics        whose values are        greater than Median.   C Cluster, K=4 A, E B, D,  F Cluster, K=2 G Cluster, K=0 
(1) Clustering 
(4) Instance Selection X1#X2#X3#X4#X5#X6#X7#Label#3"1'3"0'5'1'9"Buggy"1'1'2'0'7"3"8'Clean"2"3"2'5"5'2"1'Buggy'0'0'8"1'0'1'9"Clean'1'0'2'5"6"10"8'Buggy"1'4"1'1'7"1'1'Clean'1'0'1'0'0'1'7'Clean'Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F Inst. G 1'‚Äì'7'3'‚Äì'7'3'‚Äì'7'1'‚Äì'7'4'‚Äì'7'2'‚Äì'7'3'‚Äì'7'X1#X5#X7#Label#3"5"9"Buggy"1'7"8"Buggy"2'5"1'Clean'1'0'9"Clean'5"5"8"Buggy"0'2'1'Clean'X1#X5#X7#Label#3"5"9"Buggy"5"5"8"Buggy"0'2'1'Clean'
Final Training Dataset (2) Labeling Clusters Instance Conflicts 1/7#2/7#3/7#4/7#1'3'3'4'0'1'1'2'0'0'1'3'0'0'1'2'0'1'2'4'1'1'1'1'0'0'0'1'Metric Violation Scores 
1/7'2/7'3/7'4/7'
Selected'Metrics'
X1#X4#Label#3"0'Buggy"1'0'Clean"2"5"Buggy'0'1'Clean'1'5"Buggy"1'1'Clean'1'0'Clean'Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F Inst. G 
2'‚Äì'7'X1#X4#Label#1'0'Clean"2"5"Buggy'0'1'Clean'1'1'Clean'1'0'Clean'Inst. B Inst. C Inst. D Inst. F Inst. G Final Training Dataset Fig. 5: Computing metric violation scores (MVS) and metric selec-
tion. Violated metric values are shaded in dark gray. The metrics with
the minimum MVS are selected.
{X1,X4}''Cluster, K=3 Unlabeled Dataset X1#X2#X3#X4#X5#X6#X7#Label#3"1'3"0'5'1'9"?'1'1'2'0'7"3"8'?'2"3"2'5"5'2"1'?'0'0'8"1'0'1'9"?'1'0'2'5"6"10"8'?'1'4"1'1'7"1'1'?'1'0'1'0'0'1'7'?'1'1'2'1'5'1'8'Median Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F inst. G Instances K = the number of metrics        whose values are        greater than Median.   C Cluster, K=4 A, E B, D,  F Cluster, K=2 G Cluster, K=0 
(1) Clustering 
(4) Instance Selection X1#X2#X3#X4#X5#X6#X7#Label#3"1'3"0'5'1'9"Buggy"1'1'2'0'7"3"8'Clean"2"3"2'5"5'2"1'Buggy'0'0'8"1'0'1'9"Clean'1'0'2'5"6"10"8'Buggy"1'4"1'1'7"1'1'Clean'1'0'1'0'0'1'7'Clean'Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F Inst. G 1'‚Äì'7'3'‚Äì'7'3'‚Äì'7'1'‚Äì'7'4'‚Äì'7'2'‚Äì'7'3'‚Äì'7'X1#X5#X7#Label#3"5"9"Buggy"1'7"8"Buggy"2'5"1'Clean'1'0'9"Clean'5"5"8"Buggy"0'2'1'Clean'X1#X5#X7#Label#3"5"9"Buggy"5"5"8"Buggy"0'2'1'Clean'
Final Training Dataset (2) Labeling Clusters Instance Conflicts 1/7#2/7#3/7#4/7#1'3'3'4'0'1'1'2'0'0'1'3'0'0'1'2'0'1'2'4'1'1'1'1'0'0'0'1'Metric Violation Scores 
1/7'2/7'3/7'4/7'
Selected'Metrics'
X1#X4#Label#3"0'Buggy"1'0'Clean"2"5"Buggy'0'1'Clean'1'5"Buggy"1'1'Clean'1'0'Clean'Inst.  A Inst. B Inst. C Inst. D Inst. E Inst. F Inst. G 
2'‚Äì'7'X1#X4#Label#1'0'Clean"2"5"Buggy'0'1'Clean'1'1'Clean'1'0'Clean'Inst. B Inst. C Inst. D Inst. F Inst. G Final Training Dataset 
Fig. 6: Instance selection and the Ô¨Ånal training dataset. Violated
metric values are shaded in dark gray. Instances without violations
are selected.
of an i-th metric ( MV S i) as follows:
MV S i=Ci
Fi(1)
, where Ciis the number of violations in the i-th metric and
Fiis the number of metric values in the i-th metric.
Fig. 5 shows how to compute MVS and how to select
metrics by MVS. In Fig. 5, shaded metric values represent
violations. For example, in X3, the number of violations is
‚Äò3‚Äô. By equation (1), the violation score of X3,MV S 3, is
3
7(0.429). In this way, we can compute the violation scores
of all the metrics. Then, we can select metrics whose violation
scores are minimum. In Fig. 5, metrics that have the minimum
MVS, that is,1
7, areX1andX4. Thus, we Ô¨Ånally select X1
andX4as metrics for the training dataset.
4) Instance Selection: Fig. 6 shows the Ô¨Ånal step to gener-
ating a training dataset for CLAMI, that is, instance selection.
After the metric selection, there may still be violations in the
metric values. We remove instances that have any violated
metric value. As shown in Fig. 6, the instances AandEhave
violations in their values, thus we remove instances Aand
Eto generate the Ô¨Ånal training dataset. Instance selection is
widely used in various prediction models as well [45], [46],
[42], [47].
After instance selection, it might be that there are no buggy
and/or clean instances because of many violations. It is not
possible to build defect prediction models based on supervised
learning when both buggy and clean instances do not exist
together. In this case, we can use the next minimum MVS to
select metrics until we can generate a training dataset with
both buggy and clean instances together.TABLE I: The seven defect datasets from two groups.
Group Dataset# of instances # of
metricsPrediction
Granularity All Buggy(%)
NetGene [44]Httpclient 361 205(56.79%)
465 FileJackrabbit 542 225(41.51%)
Lucene 1671 346(10.71%)
Rhino 253 109(43.08%)
ReLink [48]Apache 194 98(50.52%)
26 File Safe 56 22(39.29%)
ZXing 399 118(29.57%)
The reason we propose CLAMI models that are built
by a machine learner is to get advantage from metric and
instance selection. CLA can label all instances. However, CLA
may have violated metrics and instances that do not follow
the defect-proneness tendency of typical defect prediction
metrics. In this reason, CLA may label instances incorrectly.
To minimize the instances that might be incorrectly labeled, we
apply metric and instance selection by removing the violations.
Then, we predict defects of the unlabeled instances by using
the machine learner built using a training set that consists of
most representative metrics and instances selected by CLAMI.
IV. E XPERIMENTAL SETUP
A. Research Questions
To evaluate the CLA/CLAMI approaches, we set the fol-
lowing research questions.
RQ1: Are the prediction performances of CLA/CLAMI
comparable to those of typical defect prediction based on
supervised learning for a single software project?
RQ2: Are the prediction performances of CLA/CLAMI
comparable to those of existing approaches for defect
prediction on unlabeled datasets?
In RQ1, we Ô¨Årst compare CLA/CLAMI to the typical de-
fect prediction using labeled data, that is, supervised learning.
If CLA/CLAMI shows comparable prediction results to the
typical defect prediction, then in practice it can be used for
projects without labeled data.
We also compare CLA/CLAMI to the existing de-
fect prediction approaches on unlabeled datasets. In RQ2,
CLA/CLAMI is compared with two baselines, that is,
threshold-based and expert-based approaches [28], [29].
B. Benchmark Datasets
Table I lists seven datasets from two groups, NetGene [44]
and ReLink [48], used in our empirical study.2The prediction
performances of defect prediction models are signiÔ¨Åcantly
affected by noisy defect data [43]. The noisy defect data are
usually caused when the defects are collected by automatic
tools for mining software archives [48], [43]. For this reason,
we choose these seven datasets as our experimental subjects
since the defect data in the datasets are manually veriÔ¨Åed or
linked to code changes [44], [48].
2Herzig et al. [44] used the combined metrics of Net work [19] and
Gene alogy in their empirical study. We call this dataset group NetGene, after
several letters of two names.Experimental Settings (RQ1) - Supervised learning model -  
74 T est set (50%)  Training set (50%) 
Supervised Model (Baseline1) Training 
Predict 
CLA/ CLAMI Model Training Predict Predict 
Threshold- Based (Baseline2) 
Expert- Based (Baseline3) Training Predict Training Predict Fig. 7: Experimental setup for baselines and CLA/CLAMI.
The NetGene datasets from the study by Herzig et al. [44]
were generated by using more than 7,400 issue reports manu-
ally classiÔ¨Åed. Herzig et al. discussed the problem of misclas-
siÔ¨Åed issue reports [49]. Since the misclassiÔ¨Åed issue reports
provide wrong labels to defect datasets, it is critical for defect
prediction models [49]. Thus, for our empirical study, we used
all the NetGene datasets used in the study by Herzig et al. [44].
The NetGene datasets include network and change geneal-
ogy metrics [44]. Herzig et al. proposed metrics based on
change genealogies and validated the metrics with existing
metrics such as complexity metrics and network metrics [44].
In their empirical study, the combined metrics using network
and change genealogy metrics led to the best prediction
performance in their empirical study [44].
We also used the golden datasets of ReLink as experimental
subjects [48]. Bird et al. discussed the problem about missing
links between issue reports and change logs that impact on de-
fect prediction performance [50]. Wu et al. addressed the miss-
ing link problem by ReLink and used golden datasets based
on manually inspecting links in their empirical study [48].
Each ReLink dataset consists of 26 code complexity metrics
extracted by the Understand tool [48], [51].
C. Experimental Design with Baselines
To evaluate CLA/CLAMI, we set three baselines: super-
vised learning, threshold-based, and expert-based approach.
1) Baseline1: We use supervised learning models trained
by labeled data as a baseline to validate RQ1 since the typical
defect prediction model for a single software project is based
on supervised learning [1], [10], [13], [19], [20], [34], [35],
[37]. Fig. 7 shows how to build a supervised learning model
as well as a CLA/CLAMI model for a dataset. We compare
the prediction performance of a typical defect prediction model
based on the supervised learning model (Baseline1) and our
CLA/CLAMI model. In supervised learning, we need labeled
data, thus to build a supervised learning model, we divided
the datasets into two splits as in Fig. 7. In the Ô¨Årst prediction
round, we build a prediction model using the Ô¨Årst split as
a training set and test the model on the second split as atest set. Then, for the second round, the Ô¨Årst split is used
as a test set and the second split is used as a training set.
As shown in Fig. 7, the supervised learning model is built
using the training set (50%) with labeled instances (small
shaded circles) and predicts defects on the test set (50%)
with unlabeled instances (small unshaded circles). Since our
approaches, CLA and CLAMI, are for an unlabeled dataset,
we build CLA/CLAMI models using only the test set with
unlabeled instances as in Fig. 7. From both the supervised
learning model and the CLA/CLAMI model, we can measure
the prediction performance and evaluate whether the prediction
performance of our CLA/CLAMI model is comparable to that
of the supervised learning model or not (RQ1). Since there
is randomness to split a dataset into training and test sets,
we repeat the Ô¨Årst and second rounds (i.e. two-fold cross
validation) 500 times to conduct 1000 predictions and report
the averaged results [22], [52].
As a machine learning classiÔ¨Åer for both the supervised
learning model and CLAMI, we use Logistic regression that
has been widely used and shown good prediction performance
in defect prediction studies [18], [22], [36], [53], [54], [55].
In addition, we use other classiÔ¨Åers such as Bayesian net-
work, J48 decision tree, Logistic model tree, Naive Bayesian,
Random forest, and Support vector machine [10], [13], [22],
[34], [35], [56]. We discuss prediction performances on various
classiÔ¨Åers in Section VI.
2) Baseline2 and Baseline3: Fig. 7 also shows how to pre-
dict defects in an unlabeled dataset using existing approaches
based on threshold (Baseline2) and expert (Baseline3) and the
CLA/CLAMI models. Since the two baseline approaches and
the CLA/CLAMI models are for the unlabeled dataset, each
prediction is conducted on the same test set with unlabeled
instances.
The threshold-based approach predicts a source code Ô¨Åle
as buggy when any metric value of the Ô¨Åle is greater than
the designated values of a threshold vector [28]. Catal et al.
set the threshold vector for metrics including lines of code,
cyclomatic complexity, unique operator, unique operand, total
operator, and total operand as (65;10;25;40;125;70)in their
study. With this threshold vector, if the cyclomatic complexity
of a source code Ô¨Åle is greater than the threshold value of 10,
the Ô¨Åle is predicted to be buggy. Catal et al. actually proposed
threshold-based and clustering-based approaches together but
opted to use the threshold-based approach since it is easier and
more effective than the clustering-based approach [28]. Thus,
we use the threshold-based approach as one of baselines from
existing approaches for defect prediction on unlabeled datasets.
We generate threshold vectors for each dataset in Table I
by using the tuning machine technique as did Catal et al. [28],
[57]. Catal et al. used three techniques to determine a threshold
vector such as ‚Äòexperience and hints from the literature‚Äô, ‚Äòtun-
ing machine‚Äô, and ‚Äòanalysis of multiple versions‚Äô as proposed
by Marinescu [28], [57]. However, we only apply the tuning
machine technique that decides a threshold value maximizing
the number of correctly predicted instances by considering
past defects [28], [57] since ‚Äòexperience and hints from the
literature‚Äô and ‚Äòanalysis of multiple versions‚Äô are not available
for our subject datasets. Past defects on each subject dataset
are not available either, therefore we compute threshold values
maximizing the number of correctly predicted instances in theTABLE II: Comparison of results between baselines and CLA/CLAMI in precision, recall, f-measure, and AUC. (SL: Supervised Learning,
THD: Threshold-based, EXP: Expert-based) The better CLA/CLAMI results that SL, THD, and EXP with statistical signiÔ¨Åcance (Wilcoxon
signed-rank test, p <0.05) are in bold font, in gray, and with an asterisk (*) respectively. The better results between CLA and CLAMI with
statistical signiÔ¨Åcance are underlined.
ProjectPrecision Recall F-measure AUC
SL THD EXP CLA CLAMI SL THD EXP CLA CLAMI SL THD EXP CLA CLAMI SL CLAMI
Httpclient 0.728 0.990 0.814 0.759 0.774 0.726 0.037 0.825 0.711 0.677 0.725 0.019 0.818 0.734 0.722 0.722 0.772
Jackrabbit 0.653 0.649 0.765 0.634 0.631 0.645 0.117 0.634 0.754 * 0.752 * 0.648 0.128 0.689 0.689 0.686 0.727 0.751
Lucene 0.538 0.340 0.619 0.301 0.287 0.483 0.207 0.225 0.639 * 0.632 * 0.508 0.256 0.243 0.409 * 0.395* 0.706 0.596
Rhino 0.656 0.831 0.818 0.724 0.727 0.599 0.349 0.746 0.764 * 0.777 * 0.623 0.069 0.775 0.743 0.750 0.686 0.777
Apache 0.671 0.663 0.794 0.727 0.722 0.641 0.553 0.717 0.684 0.714 0.653 0.634 0.750 0.705 0.718 0.712 0.753
Safe 0.611 0.708 0.964 0.636 0.627 0.621 0.471 0.815 0.725 0.757 0.603 0.533 0.878 0.677 0.685 0.699 0.770
ZXing 0.455 0.759 0.626 0.378 0.399 0.265 0.085 0.283 0.570 * 0.651 * 0.331 0.033 0.365 0.454 * 0.494 * 0.603 0.643
Average
Rank3.714 2.286 1.429 3.714 3.857 3.429 5.000 2.429 2.143 2.000 3.429 4.857 1.929 2.357 2.429-Rank Sum: 7
+Rank Sum: 21
same test dataset for this experiment. This means the threshold
values computed by the tuning machine should work better on
our subject datasets than those computed with the past defect
although we only use the tuning machine.
As the second baseline, we use the expert-based approach
proposed by Zhong et al. [29]. The expert-based approach is
based on clustering and human experts. In other words, this
approach Ô¨Årst clusters instances in a dataset by using clustering
algorithms such as K-means and then human experts decide
whether a cluster is buggy or clean [29]. This approach requires
human experts therefore it is impossible to automate the whole
process. For our empirical study, we cluster instances using the
K-means clustering algorithm and then decide upon the label
of each cluster as human experts know the actual label of the
cluster [29]; if the number of buggy instances in the cluster
is greater than that of clean instances, we label the cluster as
buggy, otherwise as clean. For the number of clusters, we used
20 as Zhong et al. suggest [29].
D. Measures
To measure the prediction performance of baselines and
CLA/CLAMI, we use precision, recall, f-measure and/or the
area under the receiver operating characteristic curve (AUC).
F-measure is a widely used prediction measure in defect
prediction studies since it represents the harmonic mean of
precision and recall [13], [20], [37], [44], [58]. Precision
represents the rate of correctly predicted buggy instances
among all instances predicted as buggy. Recall measures the
rate of correctly predicted buggy instances among all actual
buggy instances.
AUC is known to be useful for comparing different pre-
diction models [25], [34], [35], [59]. AUC is plotted using
true positive rate (recall) and false positive rate by changing
different prediction thresholds [25]. Thus, AUC is a proper
measure for comparing the overall prediction performances of
different models. Since f-measure is highly affected by class
imbalance and prediction threshold, it can make it difÔ¨Åcult to
fairly compare prediction models [25]. For this reason, AUC
is also widely used in the defect prediction literature [25],
[34], [35], [59]. The AUC of 0.7 is considered as promising
prediction performance [34], [59].
When comparing CLA/CLAMI models to baselines, we
report precision, recall, and f-measure. When comparing asupervised learning model and a CLAMI model, we addition-
ally report AUC. Both the supervised learning model and the
CLAMI model are based on statistical models so that AUC can
be computed by prediction threshold values. For f-measure, we
use 0.5 as a prediction threshold as have most defect prediction
studies reporting f-measure [13], [20], [44].
For statistical tests, we conduct the Friedman test (p <0.05)
with the Nemenyi test as a post-hoc test when comparing
multiple models [60] over the seven benchmark datasets. After
the Friedman test with the Nemenyi test, we report the visual
representation of our results following Dem Àásar guidelines [60].
Dem Àásar also suggested using the Wilcoxon signed-rank test
when comparing two models [60]. Thus, to compare the
supervised learning model and the CLAMI model in AUC, we
conduct the Wilcoxon signed-rank test (p <0.05). In addition,
when comparing two models (a baseline vs CLA/CLAMI)
for each dataset, we conduct the Wilcoxon signed-rank test
(p<0.05) to test if the performances from 1000 predictions
between the baseline and the CLA/CLAMI models are differ-
ent with statistical signiÔ¨Åcance.
V. R ESULTS
Table II shows the prediction performances between base-
lines and the CLA/CLAMI in various measures such as pre-
cision, recall, f-measure, and AUC. For supervised learning
and CLAMI, we use Logistic regression as a classiÔ¨Åer. As a
threshold to decide higher metric values in CLA/CLAMI, we
use a median value of each metric. For precision, recall, or f-
measure, we conduct the Friedman test and report the average
ranks for the Ô¨Åve approaches such as supervised learning
(SL), the threshold-based approach (THD), the expert-based
approach (EXP), CLA, and CLAMI as in the last row of
Table II [60]. The Friedman test ranks the Ô¨Åve approaches
in each dataset [60]. For example, in terms of precision, the
ranks of the Ô¨Åve approaches in Apache are 4 (SL), 5 (THD),
1 (EXP), 2 (CLA), and 3 (CLAMI); the approach with the
best precision is ranked in ‚Äò1‚Äô. After computing the ranks for
the seven datasets, we can compute the average rank for each
approach. The Friedman test compares whether the average
ranks are statistically signiÔ¨Åcant or not [60].
Table II, for each dataset, if the results of CLA/CLAMI
are better than those of SL, THD, and EXP with statistical
signiÔ¨Åcance (the Wilcoxon signed-rank test, p <0.05), the re-
sults of CLA/CLAMI are in bold font, in gray, and asterisked1 2 5 4 3 THD CLA CLAMI EXP SL 
Fig. 8: Comparison of average ranks of baselines and CLA/CLAMI
in f-measure. Approaches that are not signiÔ¨Åcantly different (the
Nemenyi test, at p = 0.05) are connected.
(*) respectively. The better results between CLA and CLAMI
with statistical signiÔ¨Åcance are underlined. In terms of AUC,
the CLAMI results that better SL with statistical signiÔ¨Åcance
(the Wilcoxon signed-rank test) are in bold font.
CLA and CLAMI outperform SL and THD in most datasets
in terms of f-measure with statistical signiÔ¨Åcance. CLA and
CLAMI outperform SL in 6 and 5 datasets respectively and
outperform THD in all datasets. In AUC, CLAMI outperforms
SL in most datasets except Lucene with statistical signiÔ¨Åcance.
Fig. 8 visualises the results of post-hoc tests by the Ne-
menyi test after the Friedman test in terms of f-measure [60].
The Friedman test (p <0.05) computes the p-value as 0.004
for f-measure results in Table II. This represents that there
are statistical difference between the average ranks of Ô¨Åve
approaches in f-measure. Then, we conduct the Nemenyi test
as a post-hoc test for each pair of the approaches. The top
line in Fig. 8 represents the axis where average ranks of Ô¨Åve
approaches are plotted. Approaches that are not statistically
signiÔ¨Åcant are connected. In Fig. 8, there are two groups,
(THD, SL) and (SL, CLAMI, CLA, EXP) based on the
connected approaches. The lower average rank (the right side
in the axis) represents the better prediction performance.
From Fig. 8, we could observe CLA, CLAMI, and EXP
outperform THD. CLA, CLAMI, and EXP seem to have an
equivalent performance as they are in the same group. SL is on
the border between two groups so that it is difÔ¨Åcult to conclude
whether SL performs the same as other approaches because of
insufÔ¨Åcient resulting data.
In terms of AUC, CLAMI shows comparable results to
SL after conducting the Wilcoxon signed-rank test (p=0.05)
between SL and CLAMI for all seven datasets [60]. The
Wilcoxon signed-rank test compares the sums of the positive
and negative ranks between SL and CLAMI in Table II and
the computed p-value is 0.297. Thus, the difference of the rank
sums between SL and CLAMI is not statistically signiÔ¨Åcant.
Table II also shows the comparison results between base-
lines and CLA/CLAMI in precision. For each dataset, CLA
and CLAMI outperform SL in four datasets. However, CLA
and CLAMI do not outperform THD and EXP in most datasets
with statistical signiÔ¨Åcance (only one gray cell against THD
and no asterisk against EXP).
Fig. 9 shows the results of the post-hoc tests after the
Friedman test (the computed p-value is 0.010) in terms of
precision. The average ranks of CLA and CLAMI are worse
than EXP with statistical signiÔ¨Åcance. Compared to SL and
THD, CLA and CLAMI do not show critical difference as
they are grouped together.
1 2 5 4 3 THD CLA CLAMI EXP SL 
Fig. 9: Comparison of average ranks of baselines and CLA/CLAMI in
precision. Approaches that are not signiÔ¨Åcantly different (the Nemenyi
test, at p = 0.05) are connected.
1 2 5 4 3 THD CLA CLAMI EXP SL 
Fig. 10: Comparison of average ranks of baselines and CLA/CLAMI
in recall. Approaches that are not signiÔ¨Åcantly different (the Nemenyi
test, at p = 0.05) are connected.
Table II shows the comparison results between baselines
and CLA/CLAMI in recall. For each dataset, CLA and CLAMI
outperform SL and THD in most datasets; results in six
datasets are in bold font and all results in CLA and CLAMI
are shaded in gray. CLA and CLAMI also outperform EXP in
four datasets.
Fig. 10 shows results of the post-hoc tests after the Fried-
man test (the computed p-value is 0.002) in terms of recall.
The average ranks of CLA and CLAMI are better than those of
THD with statistical signiÔ¨Åcance. Compared to SL and EXP,
CLA and CLAMI do not show critical difference as they are
grouped together.
The prediction performances between CLA and CLAMI
do not show a signiÔ¨Åcant difference as shown in Fig. 8, 9,
and 10. The difference in average ranks between CLA and
CLAMI is marginal, e.g., 2.143 vs 2.000 in recall. The average
rank (3.714) of CLA in precision is slightly better than that
(3.857) of CLAMI but its difference is marginal as well. Since
CLA does not require any machine learning classiÔ¨Åer, CLA is
a simpler approach compared to CLAMI. Thus, we suggest
to use CLA. However, in some datasets such as Rhino and
Zxing, CLAMI outperforms CLA in precision, recall, and f-
measure as in Table II. In this sense, it would be interesting to
investigate when CLAMI works better than CLA. We remain
this as future work.
Overall, CLA and CLAMI show comparable results to
SL (RQ1) and EXP (RQ2) and outperform THD (RQ2) in
recall, f-measure, and/or AUC. However, in terms of precision,
CLA and CLAMI show the worst ranks although they are
not statistically signiÔ¨Åcant against SL and THD. In terms of
recall, CLA and CLAMI show the best ranks compared to
other approaches although there are no statistical signiÔ¨Åcances
against SL and EXP. Menzies et al. already discussed that
prediction models with low precision and high recall are useful
in many industrial situations [61]. In this sense, CLA and
CLAMI that only use a little knowledge about the defect-
proneness tendency of metric values show the potential for
defect prediction on unlabeled datasets. Note that CLA and
CLAMI do not need initially labeled instances and manual1 2 7 6 5 4 3 LMT NB LR BN RF J48 SVM 
Fig. 11: Comparison of all classiÔ¨Åers against each other in AUC.
Approaches that are not signiÔ¨Åcantly different (the Nemenyi test, at
p = 0.05) are connected.
1 2 7 6 5 4 3 LMT NB LR BN RF J48 SVM 
Fig. 12: Comparison of all classiÔ¨Åers against each other in f-measure.
Approaches that are not signiÔ¨Åcantly different (the Nemenyi test, at
p = 0.05) are connected.
effort but achieve comparable prediction performances to most
baselines in terms of recall, f-measure, and AUC.
VI. D ISCUSSION
A. Performance on Various ClassiÔ¨Åers
We evaluate whether CLAMI models work with other
machine learning classiÔ¨Åers. To build CLAMI models, we
use Bayesian Network (BN), J48 decision tree (J48), Logistic
model tree (LMT), Logistic regression (LR), Naive Bayesian
(NB), Random forest (RF), and Support vector machine (SVM)
which are widely used in defect prediction [10], [13], [22],
[34], [35], [56]. Since we compare multiple classiÔ¨Åers, we
conduct the Friedman test with the Nemenyi test.
Fig. 11 visualises the results of post-hoc tests by the
Nemenyi test after the Friedman test (the p-value was 0.0005)
in AUC [60]. NB and LMT show better average ranks than
SVM and J48 in terms of AUC. However, for RF, BN, and LR,
it is difÔ¨Åcult to conclude that their average ranks are different
from other classiÔ¨Åers with statistical signiÔ¨Åcance. The average
AUCs are 0.702 (BN), 0.697 (J48), 0.730 (LMT), 0.723 (LR),
0.726 (NB), 0.704 (RF), and 0.656 (SVM). Most AUCs are
around 0.700 except SVM.
Fig. 12 shows the results of post-hoc tests by the Nemenyi
test after the Friedman test (the p-value was 0.018) in f-
measure [60]. The average f-measures are 0.636 (BN), 0.635
(J48), 0.634 (LMT), 0.636 (LR), 0.635 (NB), 0.636 (RF), and
0.534 (SVM). Most f-measures are around 0.635 except SVM.
Ghotra et al. compared various classiÔ¨Åers for defect pre-
diction [56]. SVM was one of the lowest ranked classiÔ¨Åers in
their empirical study. In this sense, the low ranks of CLAMI
models built by SVM conÔ¨Årm their study [56].
B. Performance on Various Cutoffs
To decide the higher metric values, we apply various cutoff
values: n-th percentiles where nis 10, 20,. . . , 80, and 90 as
well as the Ô¨Årst and third quartiles (25th and 75th percentile).
In total, we use 11 percentiles, P10 (for the 10th percentile),
P20, P25 (the Ô¨Årst quartile), P30, P40, P50 (median), P60, P70,
P75 (the third quartile), P80, and P90.
1 2 7 6 5 4 3 P25 P70 P80 P50 P60 P30 P20 
8 9 10 11 P90 P75 
P40 P10 
Fig. 13: Comparison of CLAMI models using various cutoffs in f-
measure. Approaches that are not signiÔ¨Åcantly different (the Nemenyi
test, at p = 0.05) are connected.
1 2 7 6 5 4 3 P25 P70 P80 P50 P60 P30 P20 
8 9 10 11 P90 P75 
P40 P10 
Fig. 14: Comparison of CLAMI models using various cutoffs in AUC.
Approaches that are not signiÔ¨Åcantly different (the Nemenyi test, at
p = 0.05) are connected.
As shown in Fig. 13 and 14, the median cutoff threshold
(P50) shows the best ranks in f-measure and AUC although
CLAMI with P50 does not outperform that with most other cut-
off thresholds (no statistical signiÔ¨Åcance). CLA shows the sim-
ilar results on various cutoffs; the P40 (3.000) and P50 (3.571)
show the best ranks in f-measure. In this sense, we suggest
using a median metric value as the threshold for CLA/CLAMI
in the very early stage of the software development phases
when there is no information about the best threshold for
CLA/CLAMI. However, as EXP results show, human effort
is helpful to achieve better prediction performance. In this
sense, CLA/CLAMI with additional human effort to decide
a proper threshold might lead to better prediction performance
as well. Then, the cutoff thresholds for CLA/CLAMI can be
properly set by software engineers by using related projects
that have similar distributions. Thus, we have a plan to extend
CLA/CLAMI models with human effort as future work.
C. Metric Distribution Analysis of Datasets
We investigate whether each metric is correlated with
defect-proneness by observing the distributions of metric val-
ues of buggy and clean instances. In Fig. 15, the box plots
compare the distributions of the metric values of the Safe
dataset. Since the Safe dataset has 26 metrics, there are 26
pairs of plots in Fig. 15. A pair of plots shows two distributions
of buggy or clean instances for one metric respectively. The
distributions of metric values of buggy instances are plotted in
gray while those of clean instances are plotted in white. The
solid horizontal line in a box represents the median value in
each distribution. The top and bottom of boxes represent the
third and Ô¨Årst quartiles respectively. Individual points in Fig. 15
are outliers. We normalized all metric values to compare the
distributions of metrics in the same scale (Normalized Metric
Values in Fig. 15).
The distributions of individual metrics in Fig. 15 show
different tendencies of defect-proneness. For example, the
metric M18 shows a high degree of discrimination between
buggy and clean instances. If we classify instances as buggy0.000.250.500.751.00
M01M02M03M04M05M06M07M08M09M10M11M12M13M14M15M16M17M18M19M20M21M22M23M24M25M26MetricsNormalized metric valuesLabelbuggycleanFig. 15: Distributions of metric values of buggy and clean instances in Safe.
when the normalized metric value of M18 is greater than 0.12,
about 75% of buggy and clean instances can be classiÔ¨Åed
correctly. The higher metric value of M18 implies more
defect-proneness. However, metrics such as M06 have less
discriminative power between buggy and clean instances as
shown in Fig. 15. Thus, M06 may be a less effective predictor
for the Safe dataset when building a prediction model. The
defect-proneness tendency of the metric, M22, is reversed; its
higher metric values show less defect-proneness. In CLAMI,
metrics such as M22 are automatically ignored since a majority
of values in M22 are considered as violations by CLAMI.
The supervised models (WPDP) for the Safe dataset are
built using all 26 metrics so that some metrics that do not have
discriminative power in terms of defect-proneness can degrade
the prediction performance of the models. The metrics, M06,
M08, and M22, have relatively low discriminative power as
there is little difference in the defect-proneness tendencies.
The CLAMI models are built using around six metrics
on average of the Safe dataset since CLAMI applies metric
selection to generate a training dataset. The most frequently
selected six metrics from the Safe dataset are M13, M15, M16,
M18, M23, and M25.
In Fig. 15, these six metrics show a clear tendency of
defect-proneness in Safe. In other words, the CLAMI models
can be constructed with metrics that have more discriminative
power than using all 26 metrics that also include several
metrics with relatively low defect-proneness tendencies. This
can be a major reason CLAMI models could outperform
supervised learning models in many project datasets as shown
in Table II. There are previous studies where defect prediction
performance can be improved further by using a small subset
of selected metrics [33], [40], [62]. Our study also conÔ¨Årms
these studies on the impact of metric selection.
We observe similar trends with other datasets except for
Lucene after investigating the distributions of the metrics of
each dataset. In the case of Lucene, the selected metrics by
CLAMI do not follow the defect-proneness tendency of the
typical defect prediction metrics. For this reason, CLAMI does
not outperform the supervised learning models in f-measure
and AUC as shown in Table II.
D. Threats to Validity
We carefully chose publicly available defect datasets such
as NetGene and ReLink dataset groups that were generatedusing manually veriÔ¨Åed issue reports and links between is-
sue reports and code changes respectively. However, Net-
Gene datasets may have the quality issue of links between
the manually veriÔ¨Åed issue reports and code changes since
linking issue reports and code changes was still conducted
automatically [44], [63]. In the case of ReLink, the issue
reports were not manually veriÔ¨Åed although linking the issue
reports to code changes was conducted manually [48]. Validat-
ing CLA/CLAMI with more reliable datasets generated with
manual effort may be needed. However, to the best of our
knowledge, these two defect dataset groups were generated
with manual veriÔ¨Åcation compared to other available datasets.
The CLA/CLAMI approaches are evaluated on defect
datasets from seven open-source projects. Thus, the general-
ization of our results may be an external threat. However, we
observe the potential of CLA/CLAMI as they can work well
on datasets that follow the design rationale of defect prediction
metrics. However, CLA/CLAMI may not work on the datasets
that do not follow the defect-proneness tendency of typical
defect prediction metrics. Since this can be a limitation of
CLA/CLAMI, we have a plan to conduct additional experi-
ments on various defect datasets as future work.
We implement THD and EXP approaches as real experts
know the correct knowledge about threshold values and cluster
labels. However, our implementation may not be the same as
the real experts work so there could be a bias to compare CLA
and CLAMI to THD and EXP.
VII. C ONCLUSION
Enabling defect prediction for new projects or projects
with limited historical information has long been a challenging
issue. To address this limitation, we proposed CLA/CLAMI,
which can build a prediction model on unlabeled datasets
in an automated manner. In our empirical study on seven
open-source projects, CLA/CLAMI models led to better or
comparable results to typical defect prediction models and
other baseline approaches in most projects in terms of recall,
f-measure, and/or AUC. In addition, we observed CLAMI
models work well on project datasets just using a small subset
of selected metrics that follow the typical defect-proneness
tendencies of the datasets. This result implies that in practice
CLA and CLAMI have the potential for defect prediction on
unlabeled datasets without need for manual effort. To evaluate
the applicability of our approach in industry, we plan to apply
CLA and CLAMI to proprietary software projects.REFERENCES
[1] T. Menzies, J. Greenwald, and A. Frank, ‚ÄúData mining static code
attributes to learn defect predictors,‚Äù IEEE Trans. Softw. Eng. , vol. 33,
pp. 2‚Äì13, January 2007.
[2] E. Engstr ¬®om, P. Runeson, and G. Wikstrand, ‚ÄúAn empirical evaluation
of regression testing based on Ô¨Åx-cache recommendations,‚Äù in Software
Testing, VeriÔ¨Åcation and Validation (ICST), 2010 Third International
Conference on , April 2010, pp. 75‚Äì78.
[3] N. Nagappan, T. Ball, and A. Zeller, ‚ÄúMining metrics to predict
component failures,‚Äù in Proceedings of the 28th International
Conference on Software Engineering , ser. ICSE ‚Äô06. New
York, NY , USA: ACM, 2006, pp. 452‚Äì461. [Online]. Available:
http://doi.acm.org/10.1145/1134285.1134349
[4] N. Ohlsson and H. Alberg, ‚ÄúPredicting fault-prone software modules
in telephone switches,‚Äù Software Engineering, IEEE Transactions on ,
vol. 22, no. 12, pp. 886‚Äì894, Dec 1996.
[5] T. Ostrand, E. Weyuker, and R. Bell, ‚ÄúPredicting the location and
number of faults in large software systems,‚Äù Software Engineering,
IEEE Transactions on , vol. 31, no. 4, pp. 340‚Äì355, April 2005.
[6] P. Tomaszewski, H. Grahn, and L. Lundberg, ‚ÄúA method for an accurate
early prediction of faults in modiÔ¨Åed classes,‚Äù in Software Maintenance,
2006. ICSM ‚Äô06. 22nd IEEE International Conference on , Sept 2006,
pp. 487‚Äì496.
[7] A. Bacchelli, M. D‚ÄôAmbros, and M. Lanza, ‚ÄúAre popular classes more
defect prone?‚Äù in Proceedings of the 13th International Conference
on Fundamental Approaches to Software Engineering , ser. FASE‚Äô10.
Berlin, Heidelberg: Springer-Verlag, 2010, pp. 59‚Äì73.
[8] V . R. Basili, L. C. Briand, and W. L. Melo, ‚ÄúA validation of object-
oriented design metrics as quality indicators,‚Äù IEEE Trans. Softw. Eng. ,
vol. 22, pp. 751‚Äì761, October 1996.
[9] C. Bird, N. Nagappan, B. Murphy, H. Gall, and P. Devanbu, ‚ÄúDon‚Äôt
touch my code!: Examining the effects of ownership on software
quality,‚Äù in Proceedings of the 19th ACM SIGSOFT Symposium and the
13th European Conference on Foundations of Software Engineering ,
ser. ESEC/FSE ‚Äô11. New York, NY , USA: ACM, 2011, pp. 4‚Äì14.
[Online]. Available: http://doi.acm.org/10.1145/2025113.2025119
[10] M. D‚ÄôAmbros, M. Lanza, and R. Robbes, ‚ÄúEvaluating defect prediction
approaches: a benchmark and an extensive comparison,‚Äù Empirical
Software Engineering , vol. 17, no. 4-5, pp. 531‚Äì577, 2012.
[11] A. E. Hassan, ‚ÄúPredicting faults using the complexity of code changes,‚Äù
inProceedings of the 31st International Conference on Software Engi-
neering , ser. ICSE ‚Äô09, 2009, pp. 78‚Äì88.
[12] S. Kim, T. Zimmermann, E. J. Whitehead Jr., and A. Zeller, ‚ÄúPredicting
faults from cached history,‚Äù in Proceedings of the 29th international
conference on Software Engineering , ser. ICSE ‚Äô07, 2007, pp. 489‚Äì498.
[13] T. Lee, J. Nam, D. Han, S. Kim, and I. P. Hoh, ‚ÄúMicro interaction met-
rics for defect prediction,‚Äù in Proceedings of the 16th ACM SIGSOFT
International Symposium on Foundations of software engineering , 2011.
[14] M. Li, H. Zhang, R. Wu, and Z.-H. Zhou, ‚ÄúSample-based software
defect prediction with active and semi-supervised learning,‚Äù Automated
Software Engineering , vol. 19, no. 2, pp. 201‚Äì230, 2012. [Online].
Available: http://dx.doi.org/10.1007/s10515-011-0092-1
[15] H. Lu and B. Cukic, ‚ÄúAn adaptive approach with active learning
in software fault prediction,‚Äù in Proceedings of the 8th International
Conference on Predictive Models in Software Engineering , ser.
PROMISE ‚Äô12. New York, NY , USA: ACM, 2012, pp. 79‚Äì88.
[Online]. Available: http://doi.acm.org/10.1145/2365324.2365335
[16] R. Moser, W. Pedrycz, and G. Succi, ‚ÄúA comparative analysis of
the efÔ¨Åciency of change metrics and static code attributes for defect
prediction,‚Äù in Proceedings of the 30th international conference on
Software engineering , ser. ICSE ‚Äô08, 2008, pp. 181‚Äì190.
[17] N. Nagappan and T. Ball, ‚ÄúUse of relative code churn measures to
predict system defect density,‚Äù in Proceedings of the 27th international
conference on Software engineering , ser. ICSE ‚Äô05, 2005, pp. 284‚Äì292.
[18] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and B. Murphy,
‚ÄúCross-project defect prediction: a large scale experiment on data vs.
domain vs. process,‚Äù in Proceedings of the the 7th joint meeting of
the European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering . New York,
NY , USA: ACM, 2009, pp. 91‚Äì100.[19] T. Zimmermann and N. Nagappan, ‚ÄúPredicting defects using network
analysis on dependency graphs,‚Äù in Proceedings of the 30th interna-
tional conference on Software engineering , 2008, pp. 531‚Äì540.
[20] F. Rahman and P. Devanbu, ‚ÄúHow, and why, process metrics are better,‚Äù
inProceedings of the 2013 International Conference on Software
Engineering . Piscataway, NJ, USA: IEEE Press, 2013, pp. 432‚Äì441.
[21] Y . Ma, G. Luo, X. Zeng, and A. Chen, ‚ÄúTransfer learning for cross-
company software defect prediction,‚Äù Inf. Softw. Technol. , vol. 54, no. 3,
pp. 248‚Äì256, Mar. 2012.
[22] J. Nam, S. J. Pan, and S. Kim, ‚ÄúTransfer defect learning,‚Äù in Proceed-
ings of the 2013 International Conference on Software Engineering .
Piscataway, NJ, USA: IEEE Press, 2013, pp. 382‚Äì391.
[23] B. Turhan, T. Menzies, A. B. Bener, and J. Di Stefano, ‚ÄúOn the relative
value of cross-company and within-company data for defect prediction,‚Äù
Empirical Softw. Eng. , vol. 14, pp. 540‚Äì578, October 2009.
[24] S. Watanabe, H. Kaiya, and K. Kaijiri, ‚ÄúAdapting a fault prediction
model to allow inter languagereuse,‚Äù in Proceedings of the 4th Interna-
tional Workshop on Predictor Models in Software Engineering . New
York, NY , USA: ACM, 2008, pp. 19‚Äì24.
[25] F. Rahman, D. Posnett, and P. Devanbu, ‚ÄúRecalling the ‚Äùimprecision‚Äù
of cross-project defect prediction,‚Äù in Proceedings of the ACM SIG-
SOFT 20th International Symposium on the Foundations of Software
Engineering . New York, NY , USA: ACM, 2012, pp. 61:1‚Äì61:11.
[26] G. Canfora, A. De Lucia, M. Di Penta, R. Oliveto, A. Panichella,
and S. Panichella, ‚ÄúMulti-objective cross-project defect prediction,‚Äù
inSoftware Testing, VeriÔ¨Åcation and Validation, 2013 IEEE Sixth
International Conference on , March 2013, pp. 252‚Äì261.
[27] A. Panichella, R. Oliveto, and A. De Lucia, ‚ÄúCross-project defect
prediction models: L‚Äôunion fait la force,‚Äù in Software Maintenance,
Reengineering and Reverse Engineering (CSMR-WCRE), 2014 Software
Evolution Week - IEEE Conference on , Feb 2014, pp. 164‚Äì173.
[28] C. Catal, U. Sevim, and B. Diri, ‚ÄúClustering and metrics thresholds
based software fault prediction of unlabeled program modules,‚Äù in
Information Technology: New Generations, 2009. ITNG ‚Äô09. Sixth
International Conference on , April 2009, pp. 199‚Äì204.
[29] S. Zhong, T. Khoshgoftaar, and N. Seliya, ‚ÄúUnsupervised learning for
expert-based software quality estimation,‚Äù in High Assurance Systems
Engineering, 2004. Proceedings. Eighth IEEE International Symposium
on, March 2004, pp. 149‚Äì155.
[30] F. Zhang, A. Mockus, I. Keivanloo, and Y . Zou, ‚ÄúTowards building
a universal defect prediction model,‚Äù in Proceedings of the 11th
Working Conference on Mining Software Repositories , ser. MSR 2014.
New York, NY , USA: ACM, 2014, pp. 182‚Äì191. [Online]. Available:
http://doi.acm.org/10.1145/2597073.2597078
[31] B. Turhan, ‚ÄúOn the dataset shift problem in software engineering
prediction models,‚Äù Empirical Software Engineering , vol. 17, no. 1-2,
pp. 62‚Äì74, 2012. [Online]. Available: http://dx.doi.org/10.1007/s10664-
011-9182-8
[32] S. J. Pan and Q. Yang, ‚ÄúA survey on transfer learning,‚Äù IEEE Trans. on
Knowl. and Data Eng. , vol. 22, pp. 1345‚Äì1359, October 2010.
[33] S. Shivaji, E. J. Whitehead, R. Akella, and S. Kim, ‚ÄúReducing features
to improve code change-based bug prediction,‚Äù IEEE Transactions on
Software Engineering , vol. 39, no. 4, pp. 552‚Äì569, 2013.
[34] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, ‚ÄúBenchmarking
classiÔ¨Åcation models for software defect prediction: A proposed frame-
work and novel Ô¨Åndings,‚Äù Software Engineering, IEEE Transactions on ,
vol. 34, no. 4, pp. 485‚Äì496, 2008.
[35] Q. Song, Z. Jia, M. Shepperd, S. Ying, and J. Liu, ‚ÄúA general software
defect-proneness prediction framework,‚Äù Software Engineering, IEEE
Transactions on , vol. 37, no. 3, pp. 356‚Äì370, 2011.
[36] T. Hall, S. Beecham, D. Bowes, D. Gray, and S. Counsell, ‚ÄúA systematic
literature review on fault prediction performance in software engineer-
ing,‚Äù Software Engineering, IEEE Transactions on , vol. 38, no. 6, pp.
1276‚Äì1304, Nov 2012.
[37] T. Fukushima, Y . Kamei, S. McIntosh, K. Yamashita, and N. Ubayashi,
‚ÄúAn empirical study of just-in-time defect prediction using cross-project
models,‚Äù in Proceedings of the 11th Working Conference on Mining
Software Repositories . New York, NY , USA: ACM, 2014, pp. 172‚Äì
181.[38] M. Harman, S. Islam, Y . Jia, L. Minku, F. Sarro, and K. Srivisut,
‚ÄúLess is more: Temporal fault predictive performance over multiple
hadoop releases,‚Äù in Search-Based Software Engineering , ser. Lecture
Notes in Computer Science, C. Le Goues and S. Yoo, Eds. Springer
International Publishing, 2014, vol. 8636, pp. 240‚Äì246.
[39] A. L. Blum and P. Langley, ‚ÄúSelection of relevant features and
examples in machine learning,‚Äù Artif. Intell. , vol. 97, no. 1-2, pp. 245‚Äì
271, Dec. 1997. [Online]. Available: http://dx.doi.org/10.1016/S0004-
3702(97)00063-5
[40] K. Gao, T. M. Khoshgoftaar, H. Wang, and N. Seliya, ‚ÄúChoosing
software metrics for defect prediction: An investigation on feature
selection techniques,‚Äù Softw. Pract. Exper. , vol. 41, no. 5, pp. 579‚Äì606,
Apr. 2011. [Online]. Available: http://dx.doi.org/10.1002/spe.1043
[41] H. Wang, T. M. Khoshgoftaar, and N. Seliya, ‚ÄúHow many
software metrics should be selected for defect prediction?‚Äù
inFLAIRS Conference , R. C. Murray and P. M. McCarthy,
Eds. AAAI Press, 2011. [Online]. Available: http://dblp.uni-
trier.de/db/conf/Ô¨Çairs/Ô¨Çairs2011.htmlWangKS11
[42] E. Kocaguneli, T. Menzies, J. Keung, D. Cok, and R. Madachy, ‚ÄúActive
learning and effort estimation: Finding the essential content of software
effort estimation data,‚Äù Software Engineering, IEEE Transactions on ,
vol. 39, no. 8, pp. 1040‚Äì1053, 2013.
[43] S. Kim, H. Zhang, R. Wu, and L. Gong, ‚ÄúDealing with noise
in defect prediction,‚Äù in Proceedings of the 33rd International
Conference on Software Engineering , ser. ICSE ‚Äô11. New
York, NY , USA: ACM, 2011, pp. 481‚Äì490. [Online]. Available:
http://doi.acm.org/10.1145/1985793.1985859
[44] K. Herzig, S. Just, A. Rau, and A. Zeller, ‚ÄúPredicting defects using
change genealogies,‚Äù in Software Reliability Engineering (ISSRE), 2013
IEEE 24th International Symposium on , Nov 2013, pp. 118‚Äì127.
[45] C.-L. Chang, ‚ÄúFinding prototypes for nearest neighbor classiÔ¨Åers,‚Äù
Computers, IEEE Transactions on , vol. C-23, no. 11, pp. 1179‚Äì1184,
Nov 1974.
[46] E. Kocaguneli, T. Menzies, A. Bener, and J. Keung, ‚ÄúExploiting the
essential assumptions of analogy-based effort estimation,‚Äù Software
Engineering, IEEE Transactions on , vol. 38, no. 2, pp. 425‚Äì438, March
2012.
[47] Y . F. Li, M. Xie, and T. N. Goh, ‚ÄúA study of project selection and
feature weighting for analogy based software cost estimation,‚Äù J. Syst.
Softw. , vol. 82, no. 2, pp. 241‚Äì252, Feb. 2009. [Online]. Available:
http://dx.doi.org/10.1016/j.jss.2008.06.001
[48] R. Wu, H. Zhang, S. Kim, and S. Cheung, ‚ÄúRelink: Recovering links
between bugs and changes,‚Äù in Proceedings of the 16th ACM SIGSOFT
International Symposium on Foundations of software engineering , 2011.
[49] K. Herzig, S. Just, and A. Zeller, ‚ÄúIt‚Äôs not a bug, it‚Äôs a feature:
How misclassiÔ¨Åcation impacts bug prediction,‚Äù in Proceedings of the
2013 International Conference on Software Engineering , ser. ICSE
‚Äô13. Piscataway, NJ, USA: IEEE Press, 2013, pp. 392‚Äì401. [Online].
Available: http://dl.acm.org/citation.cfm?id=2486788.2486840
[50] C. Bird, A. Bachmann, E. Aune, J. Duffy, A. Bernstein, V . Filkov,
and P. Devanbu, ‚ÄúFair and balanced?: Bias in bug-Ô¨Åx datasets,‚Äù in
Proceedings of the the 7th Joint Meeting of the European Software
Engineering Conference and the ACM SIGSOFT Symposium on TheFoundations of Software Engineering , ser. ESEC/FSE ‚Äô09. New
York, NY , USA: ACM, 2009, pp. 121‚Äì130. [Online]. Available:
http://doi.acm.org/10.1145/1595696.1595716
[51] Understand 2.0. [Online]. Available: http://www.scitools.com/products/
[52] A. Arcuri and L. Briand, ‚ÄúA practical guide for using statistical tests to
assess randomized algorithms in software engineering,‚Äù in Proceedings
of the 33rd International Conference on Software Engineering . New
York, NY , USA: ACM, 2011, pp. 1‚Äì10.
[53] A. Meneely, L. Williams, W. Snipes, and J. Osborne, ‚ÄúPredicting
failures with developer networks and social network analysis,‚Äù in
Proceedings of the 16th ACM SIGSOFT International Symposium on
Foundations of software engineering , 2008, pp. 13‚Äì23.
[54] E. Shihab, A. Mockus, Y . Kamei, B. Adams, and A. E. Hassan,
‚ÄúHigh-impact defects: a study of breakage and surprise defects,‚Äù in
Proceedings of the 19th ACM SIGSOFT symposium and the 13th
European conference on Foundations of software engineering . New
York, NY , USA: ACM, 2011, pp. 300‚Äì310.
[55] J. Nam, ‚ÄúSurvey on software defect prediction,‚Äù Department of Compter
Science and Engineerning, The Hong Kong University of Science and
Technology, Tech. Rep., 2014.
[56] B. Ghotra, S. McIntosh, and A. E. Hassan, ‚ÄúRevisiting the impact
of classiÔ¨Åcation techniques on the performance of defect prediction
models,‚Äù in Proc. of the 37th Int‚Äôl Conf. on Software Engineering
(ICSE) , ser. ICSE ‚Äô15, 2015, pp. 789‚Äì800.
[57] R. Marinescu, ‚ÄúDetection strategies: metrics-based rules for detecting
design Ô¨Çaws,‚Äù in Software Maintenance, 2004. Proceedings. 20th IEEE
International Conference on , Sept 2004, pp. 350‚Äì359.
[58] X.-Y . Jing, S. Ying, Z.-W. Zhang, S.-S. Wu, and J. Liu, ‚ÄúDictionary
learning based software defect prediction,‚Äù in Proceedings of the 36th
International Conference on Software Engineering , ser. ICSE 2014.
New York, NY , USA: ACM, 2014, pp. 414‚Äì423. [Online]. Available:
http://doi.acm.org/10.1145/2568225.2568320
[59] E. Giger, M. D‚ÄôAmbros, M. Pinzger, and H. C. Gall, ‚ÄúMethod-level bug
prediction,‚Äù in Proceedings of the ACM-IEEE International Symposium
on Empirical Software Engineering and Measurement . New York, NY ,
USA: ACM, 2012, pp. 171‚Äì180.
[60] J. Dem Àásar, ‚ÄúStatistical comparisons of classiÔ¨Åers over multiple data
sets,‚Äù J. Mach. Learn. Res. , vol. 7, pp. 1‚Äì30, Dec. 2006. [Online].
Available: http://dl.acm.org/citation.cfm?id=1248547.1248548
[61] T. Menzies, A. Dekhtyar, J. Distefano, and J. Greenwald, ‚ÄúProblems
with precision: A response to ‚Äùcomments on ‚Äôdata mining static code
attributes to learn defect predictors‚Äô‚Äù,‚Äù Software Engineering, IEEE
Transactions on , vol. 33, no. 9, pp. 637‚Äì640, Sept 2007.
[62] P. He, B. Li, X. Liu, J. Chen, and Y . Ma, ‚ÄúAn
empirical study on software defect prediction with a
simpliÔ¨Åed metric set,‚Äù Information and Software Technology ,
vol. 59, no. 0, pp. 170 ‚Äì 190, 2015. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0950584914002523
[63] T. Zimmermann, R. Premraj, and A. Zeller, ‚ÄúPredicting defects
for eclipse,‚Äù in Proceedings of the Third International Workshop
on Predictor Models in Software Engineering , ser. PROMISE ‚Äô07.
Washington, DC, USA: IEEE Computer Society, 2007, pp. 9‚Äì.
[Online]. Available: http://dx.doi.org/10.1109/PROMISE.2007.10