Feature-Interaction A ware Conﬁguration
Prioritization for Conﬁgurable Code
Son Nguyen∗, Hoan Nguyen†, Ngoc Tran∗, Hieu Tran∗, and Tien N. Nguyen∗
∗Computer Science Department, The University of Texas at Dallas, USA,
∗Email: {sonnguyen,nmt140230,trunghieu.tran,tien.n.nguyen}@utdallas.edu
†Amazon Corporation, USA, Email: nguyenanhhoan@gmail.com
Abstract —Unexpected interactions among features induce most
bugs in a conﬁgurable software system. Exhaustively analyzing all
the exponential number of possible conﬁgurations is prohibitively
costly. Thus, various sampling techniques have been proposed
to systematically narrow down the exponential number of le-
gal conﬁgurations to be analyzed. Since analyzing all selected
conﬁgurations can require a huge amount of effort, fault-based
conﬁguration prioritization, that helps detect faults earlier, can
yield practical beneﬁts in quality assurance. In this paper, we
propose C OPRO, a novel formulation of feature-interaction bugs
via common program entities enabled/disabled by the features.
Leveraging from that, we develop an efﬁcient feature-interaction-
aware conﬁguration prioritization technique for a conﬁgurable
system by ranking the conﬁgurations according to their total
number of potential bugs. We conducted several experiments
to evaluate C OPROon the ability to detect conﬁguration-related
bugs in a public benchmark. We found that C OPROoutperforms
the state-of-the-art conﬁguration prioritization techniques when
we add them on advanced sampling algorithms. In 78% of
the cases, C OPRO ranks the buggy conﬁgurations at the top
3 positions in the resulting list. Interestingly, C OPRO is able to
detect 17 not-yet-discovered feature-interaction bugs.
Keywords -Conﬁgurable Code, Feature Interaction; Conﬁgura-
tion Prioritization; Software Product Lines;
I. I NTRODUCTION
Several software systems enable developers to conﬁgure
to different environments and requirements. In practice, a
highly-conﬁgurable system can tailor its functional and non-
functional properties to the needs and requirements of users. It
does so via a very large number of conﬁguration options [9],
[10] that are used to control different features [5], [29]. For
example, Linux Kernel supports more than 12,000 compile-
time conﬁguration options, that can be conﬁgured to generate
speciﬁc kernel variants for billions of scenarios.
In a conﬁgurable system, features can interact with one an-
other in a non-trivial manner. As a consequence, such interac-
tion could inadvertently modify or inﬂuence the functionality
of one another [59]. Unexpected interactions might induce
bugs. In fact, most conﬁguration-related bugs are caused by
interactions among features [2], [25], [42], [46], [56]. Unfortu-
nately, traditional methods cannot be directly applied to work
on conﬁgurable code since they focus on detecting bugs in
a particular variant. Furthermore, exhaustively analyzing the
systems is infeasible due to the exponential number of all pos-
sible conﬁgurations. In practice, conﬁguration testing is oftenperformed in a manual and ad-hoc manner by unsystematically
selecting common variants for analysis [26], [39].
To systematically perform quality assurance (QA) for a
highly-conﬁgurable system (Figure 1), researchers have pro-
posed several techniques to narrow the conﬁguration space
byeliminating invalid conﬁgurations that violate the feature
model of the system, which deﬁnes the feasible conﬁgurations
via the constraints among the features [17], [18], [27], [30],
[29], [50]. However, the number of conﬁgurations that need
to be tested is still exponential. To address this explosion
problem, researchers introduce various conﬁguration selec-
tion strategies. The popular strategies include the sampling
algorithms which achieve feature interaction coverage such as
combinatorial interaction testing [48], [47], [28], [40], one-
enabled [42], one-disabled [2], most-enabled-disabled [52],
statement-coverage [53], to reduce the number of conﬁgu-
rations to be analyzed. Still, those algorithms assume the
chances of detecting interaction bugs are the same for all those
combinations . Thus, interaction faults might be discovered
only after the last variants in such samples is tested. Thus,
after conﬁguration selection, the selected set of conﬁgurations
need to be prioritized for QA activities [3]. Note that conﬁg-
uration prioritization is different from test case prioritization
because after conﬁguration prioritization, any QA activities
can be applied on the ranked list of prioritized conﬁgurations,
including test generation and testing, static bug detection, or
manual code review (Figure 1).
To motivate conﬁguration prioritization, let us take an
example of Linux Kernel. In Linux, the number of differ-
ent conﬁguration options is over 12,000, leading to + 212K
different conﬁgurations. After applying all the constraints on
various combinations of options, the number of valid conﬁg-
urations for QA is an exponential number. For conﬁguration
selection, by using six-wise sampling algorithm, the number
is still extremely large, up to +500K conﬁgurations [42].
Hence, without conﬁguration prioritization, many bugs that
are dependent on conﬁgurations might still be hidden due to
this large conﬁguration space, especially when the resources
for QA ( e.g., time and developers’ efforts) are limited.
In practice, developers even do not perform QA activities
on a particular conﬁguration until it was reported to have
defects by the users. In this case, users have already suffered
the consequences of those defects. Due to the large number
of conﬁgurations after selection for QA, even compile-time
4892019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
978-1-7281-2508-4/19/$31.00 ©2019 IEEE
DOI 10.1109/ASE.2019.00053
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Figure 1. The QA Process of Conﬁgurable System
errors and ﬂaws cannot be quickly detected by a compiler or
a bug detection in the appropriate conﬁguration. Indeed, in
the V ariability Bugs Database (VBDb) [2], a public database
of real-world conﬁguration-related bugs reported for the Linux
kernel, there are 42 out of 98 bugs and ﬂaws that are compile-
time: 25 declaration errors, 10 type errors, and 7 cases of
dead code .
Despite the importance of conﬁguration prioritization, the
state-of-the-art methods for such prioritization are still lim-
ited in detecting feature-interaction bugs. The similarity-based
prioritization method ( SP ) [3] is based on the idea that dis-
similar test sets are likely to detect more defects than similar
ones [3]. In SP , the conﬁguration with the maximum number
of features is selected to be the ﬁrst one under test. The
next conﬁguration under test is the conﬁguration with the
lowest feature similarity compared to the previously chosen
one. Despite its success, there are two key problems with SP .
First,SP aims to cover as many features different from the
previous ones. The different features to be considered next
might not be the ones that potentially causes violations. SP
does not examine the interaction between features, which is
the key aspect causing interaction bugs in a variant. Second,
inSP , the quality of the resulted prioritization order strongly
depends on the selection of the ﬁrst conﬁguration.
In this paper, we propose C OPRO, a novel conﬁguration
prioritization approach for conﬁgurable systems by analyzing
their code to detect feature-interaction bugs. Our key idea in
COPRO is as follows. In a conﬁgurable system, features are
implemented as blocks of code, which are expressed via the
program instructions/operations (e.g., declarations, references,
assignments, etc.) on the data structures/program entities (e.g.,
variables, functions, etc.) . Features interaction occurs when the
operations on the program entities shared between the features
have impacts on each other.
Those operations, when the features are enabled or dis-
abled, potentially create a violation(s) that makes the pro-
gram not-compilable or having a run-time error . Detecting
feature interactions via operations would help identify po-
tential feature-interaction bugs. An example of a violation is
that a feature disables the only initialization of a variable
while another enables one of its dereferences (the violation
of“dereferencing an un-initialized variable” ). This violation
could lead to a NULL pointer exception. It is clear that the
conﬁguration in which the former feature is disabled and the
latter is enabled, is more suspicious than the one where both
of them are either enabled or disabled. The suspiciousness of a
conﬁguration is indicated via the potential feature-interaction
violations. Hence, a higher number of potential violations
makes the conﬁguration more suspicious. The suspiciousness
Figure 2. A Simpliﬁed Bug in Linux Kernel
levels are used to rank the conﬁgurations, which helps testing,
bug detection, or other QA activities more efﬁcient.
We conducted several experiments to evaluate C OPRO in
two complementary settings. First, in a benchmark setting, we
ran C OPRO on the V ariability Bugs Database (VBDb) [2].
We compared C OPRO with the two state-of-the-art approaches
inrandom prioritization and similarity-based prioritization
(SP) [3], when we added each of the compared techniques
on top of several state-of-the-art sampling algorithms [42].
We found that C OPRO signiﬁcantly outperforms the other
techniques. In 78.0% of the cases, C OPRO ranks the buggy
conﬁgurations at the top-3 positions in the list, while the SP
andRandom approaches rank them at the top-3 positions for
only 41.3% and 26.1% of the cases. Interestingly, C OPRO was
able to detect 17 feature-interaction bugs that were not yet
discovered in VBDb including high-degree interaction bugs,
memory leaking bugs, etc. In the second setting, we connect
COPRO with a compiler to run on large, open-source conﬁg-
urable systems, and C OPRO can detect 4 newly discovered
bugs and programming ﬂaws.
In summary, in this paper, our main contributions include:
•A formulation of feature-interaction bugs using common
program entities enabled/disabled by the features;
•COPRO: an efﬁcient feature-interaction-aware conﬁgura-
tion prioritization technique for conﬁgurable systems;
•An extensive experimental evaluation showing the effec-
tiveness of C OPRO over the state-of-the-art approaches.
II. M OTIV A TING EXAMPLE
In this section, we illustrate the challenges of conﬁguration
prioritization and motivate our solution via an example.
490
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. A. Examples of Bugs in Conﬁgurable Code
Let us consider the simpliﬁed version of the real buggy code
in the Linux kernel [2] at the commit 40410715715 of Linux-
stable at https://git.kernel.org (shown in Figure 2). This version
has more than 5,200 compile-time conﬁguration options and
about 30,000 ﬁles. The code in Figure 2 contains two feature-
interaction bugs that were discovered in certain conﬁgurations:
•A compile-time error occurs ( use the undeclared func-
tion of_platform _populate on line 24) in the vari-
ants where CONFIG _TWL4030 _CORE, CONFIG _OF_DEVICE,
and CONFIG _SPARC are enabled.
•A run-time error occurs ( dereferencing the NULL pointer ops
on line 12) in the conﬁgurations where CONFIG _TWL4030-
_CORE, CONFIG _OF_DEVICE are enabled, and CONFIG _SPARC
andCONFIG _OF_IRQ are disabled.
For this example in Linux kernel, brute-force testing of
all possible variants to discover these interaction bugs faces
the problem of combinatorial explosion in the exponential
conﬁguration space (up to 25,200possible conﬁgurations). With
a huge number of conﬁgurations and without an assessment
of the potential buggy level of those conﬁgurations, the QA
process ( e.g., debugging) will require a great deal of effort
from developers. To deal with such large number of conﬁgu-
rations, ﬁrst, one will eliminate the invalid conﬁgurations that
violate the constraints among the features in the system [17],
[18], [27], [29], [30], [50]. However, the number of conﬁgu-
rations after this step is still exponential. To balance between
bug detection rate and the number of conﬁgurations to be
examined, the conﬁguration selection process is applied. An
example of selection algorithms is the k-way combinatorial
approach [28], [40], [47], [48], which considers the system
under test as a blackbox and selects a subset with at most k
features. However, even with a small value of k,e.g.,k=6 ,
inspecting a very large number of selected conﬁgurations
without prioritizing the variants most likely having defects
is still inefﬁcient. Therefore, one would need a prioritization
strategy to rank the conﬁgurations to be examined.
The current state-of-the-art conﬁguration prioritization al-
gorithm is the similarity-based conﬁguration prioritization
(SP) [3]. Unfortunately, SP is still ineffective in detecting
feature-interaction bugs. Let us illustrate this via our example.
Table I shows the partial set of conﬁgurations chosen by 4-
wise sampling algorithm and prioritized by SP [3]. The variant,
where TWL4030 _CORE ,IRQ_DOMAIN, OF _IRQ , and OF_DEVICE
are enabled, and SPARC is disabled, with the maximum num-
ber of features is selected to be examined ﬁrst by the SP
algorithm. For the next conﬁguration, the conﬁguration that
has the minimum number of similar features compared to
the previously selected conﬁguration is picked ( i.e., the one
in which TWL4030 _CORE ,IRQ_DOMAIN ,OF_IRQ , and OF_DEVICE
are disabled, and SPARC is enabled). Although this second
conﬁguration is most dissimilar to the ﬁrst one, it does
not contain the features whose interactions cause violations,
and there is no bug revealed by the second conﬁguration.
As a result, by SP’s strategy, the result is not an efﬁcientTable I
THE CONFIGURA TIONS ORDERED BY SP ALGORITHM [3] FOR FIGURE 2
# OF_IRQ IRQ_DOMAIN OF_DEVICETWL4030
_CORESPARC
1 T T T T F
2 F F F F T
3 F T F F T
4 F T T T F
5 T T F T T
6 T T T T T
7 F T T T T
order for inspection because the aforementioned compile-time
and run-time errors are not detected until the 4thand 6th
conﬁgurations are inspected respectively. The conﬁguration
with both interaction bugs would only be discovered via the
7thconﬁguration. In our experiment (will be presented in
Section V), 36.2% of the feature-interaction bugs in the public
benchmark, the V ariability Bugs Database (VBDb) [2], cannot
be revealed until at least 10 conﬁgurations are inspected in the
resulting list ranked by the SP approach.
B. Observations
Let us consider the code in Figure 2 with the two fol-
lowing feature interactions that can cause the violations
of program semantics: 1) the declaration of the function
of_platform _populate in feature L(line 5) and its use in
Z(line 24), and 2) the assignment of ops in feature Y(line 19)
and its reference in K(line 22). There are two potential bugs:
1) the use of the function of_platform _populate without
its declaration; and 2) the reference to the variable ops
without its initiation. The conﬁguration that enables Zand dis-
ables L(CONFIG _OF_DEVICE=T ,CONFIG _SPARC=T ) and enables
Kand disables Y(CONFIG _TWL4030 _CORE=T ,CONFIG _OF_IRQ=F )
should be inspected earlier to detect the two bugs. Based on
this observation, those interactions between features should
be comprehended to quickly discover these above interaction
bugs. That motivates us to propose an approach that ﬁrst
analyzes the source code to more precisely detect the potential
interactions among features, and then assesses the probabilities
to be faulty of the conﬁgurations to prioritize to inspect/test
them in a more efﬁcient order.
C. COPROOverview
Detecting all interactions among features in a sound and
complete manner requires an analysis on all combinations
of conﬁguration options. That is prohibitively expensive and
impractical. To deal with this problem, we statically analyze
the source code to locally and heuristically identify the inter-
actions between features via the shared program entities and
the operations on them . For example, Lshares the function
of_platform _populate with Z(which is declared on line 5
and used on line 24) and Kinteracts with Yvia the variable
ops (which is assigned on line 19 and referred to on line 22).
Importantly, the operations such as declaration, assignment,
or references on the shared entities could become invalid
491
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. when certain features (via conﬁguration options) are enabled
or disabled. As a consequence, that could lead to a violation.
For instance, a violation occurs when CONFIG _TWL4030 _CORE ,
CONFIG _OF_DEVICE , and CONFIG _SPARC are enabled because
the function of_platform _populate would be used ( Kand Z
are enabled) while its declaration is turned off ( Lis disabled).
The other violation occurs in the case that CONFIG _TWL4030 _-
CORE istrue , that enables K, while Yis disabled as
CONFIG _OF_IRQ is disabled. This would induce the bug of
dereferencing the NULL pointer on variable ops (line 12). With
our strategy, the 7thvariant in Table I is more suspicious than
the 4th,6th, and any other ones. Generally, the suspiciousness
of a variant is determined by the number of violations that it
might induce. Finally, a conﬁguration can be ranked according
to its suspiciousness score, thus, we could create a prioritiza-
tion order of variants that maximizes fault detection rate.
III. F ORMULA TION
Let us formulate the problem of feature-interaction-aware
conﬁguration prioritization.
A. Program Entities and Operations
In a program, we are interested in the program entities and
the operations performed on them.
Deﬁnition 1. (Program Entity ). A program entity is a pro-
gram element that is uniquely identiﬁed with a name and a
scope. The scope and the name of an entity are used together
as the identity of the entity.
In our formulation, we are interested in two types of pro-
gram entities: variable and function . An entity is represented
in the form of [scope.ent _name] , where scope andent_name
are the scope and the name of the program entity respectively.
For example, the code in Figure 2 contains the variables
GLOBAL.irq _domain _simple _ops ,twl_probe.ops , the func-
tion GLOBAL.twl _probe ,etc.
We deﬁne 4 types of operations on variables and functions.
Deﬁnition 2. (Program Operation ). We deﬁne four types of
operations on variables and functions: declare, assign, use
and destruct. Let OP be the set of program operations, OP=
{declare,assign,use,destruct }. All of those four operations
are applicable to variables, while declare anduse are only
applicable on functions.
For variables, the assign operation is used to assign a non-
null value to a variable. A NULL assignment to a variable is
treated as a special case of an assignment. In Figure 2, function
GLOBAL.of _platform _populate _probe isdeclared at line 5,
and used at line 24. twl_probe.ops isdeclared (line 17),
assigned a value (line 19), and then used/referred to (line 22).
B. Conﬁgurations and Features
A conﬁgurable system contains several segments of code
that are present in any variant that implements its basic
functionality. Those segment form the core of the system.
In practice, a conﬁgurable system usually provides a large
number of conﬁguration options to conﬁgure several optionalsegments of code to be present or absent, in addition to the
core of the system. Those optional segments of code are aimed
to realize the optional features of the system. For example, in
the Linux Kernel, the conﬁguration options have the preﬁx of
CONFIG _, and they can have different values. Without loss of
generality, we assume that the value of a conﬁguration option
is either true(T) orfalse(F) (We can consider the entire
conditional expressions of non-boolean options as boolean
ones, e.g., CONFIG _A>10 asCONFIG _A>10=T/F ).
Deﬁnition 3. (Conﬁguration Option ). A conﬁguration option
(option for short) is an element that is used to conﬁgure the
source code of a conﬁgurable system, such that the option’s
value determines the presence or absence of one or more
segments of code.
In a conﬁgurable system, the presence or absence of code
segments is dependent on the values of multiple options. In
Figure 2, the lines 19 and 20 are presented only when both
CONFIG _TWL4030 _CORE andCONFIG _OF_IRQ areT. Thus, at line
19,irq_domain _simple _ops is potentially used to assign as a
value to the variable ops when both of those options are T.
Deﬁnition 4. (Selection Functions ). In a conﬁgurable system,
we deﬁne selection functions as the functions from O×Vto
2P, whereOis the set of conﬁguration options, V={T, F},
andPis the set of program entities used in the code of the
conﬁgurable system. We deﬁne four selection functions:
•α:O×V→2P,α(o,v)=D, whereo∈O,v∈{T, F},
andDis the set of entities potentially declared ifo=v.
•β:O×V→2P,β(o,v)=D, whereo∈O,v∈{T, F},
andDis the set of entities potentially assigned ifo=v.
•γ:O×V→2P,γ(o,v)=D, whereo∈O,v∈{T, F},
andDis the set of entities potentially used ifo=v.
•δ:O×V→2P,δ(o,v)=D, whereo∈O,v∈{T, F},
andDis the set of entities potentially destructed ifo=v.
For example, in Figure 2:
•α(CONFIG _SPARC, F )={GLOBAL.of _platform _populate,
of_platform _populate.node }
•β(CONFIG _OF_IRQ, T)={twl_probe.ops }
•γ(CONFIG _OF_IRQ, T)={GLOBAL.irq _domain _simple _
ops}
Deﬁnition 5. (Conﬁguration ). Given a conﬁgurable system,
a conﬁguration is a speciﬁc selection of conﬁguration options,
which deﬁnes a variant of the system.
Conﬁguration options are used to control the features that
are represented by certain segments of code. For example, in
Figure 2, the feature represented by the segment of code X
(feature X) is enabled if the value of the conﬁguration option
CONFIG _IRQ_DOMAIN istrue , whereas feature Yis enabled if
both CONFIG _OF_IRQ andCONFIG _TWL4030 _CORE aretrue .
Deﬁnition 6. (Feature ). In a conﬁgurable system, a feature f
is implemented by applying program operations on a set of
program entities, whose presence/absence is controlled by
certain conﬁguration options. We denote it by f∼OP×ρ
492
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Table II
DIFFERENT KINDS OF FEA TURE INTERACTIONS
Kind of Interaction Condition
1 declare-declare ∃e∈ρ1∩ρ2,eis declared in both f1andf2
2 declare-assign ∃e∈ρ1∩ρ2,eis declared in f1and then assigned in f2
3 declare-use ∃e∈ρ1∩ρ2,eis declared in f1and used in f2
4 declare-destruct ∃e∈ρ1∩ρ2,eis declared in f1, and destructed in f2
5 assign-assign ∃e∈ρ1∩ρ2,eis assigned in both f1andf2
6 assign-use ∃e∈ρ1∩ρ2,eis assigned in f1and used in f2
7 assign-destruct ∃e∈ρ1∩ρ2,eis assign in f1and destructed in f2
8 use-destruct ∃e∈ρ1∩ρ2,eis used in f1and destructed in f2
9 destruct-destruct ∃e∈ρ1∩ρ2, the entity is destructed in both f1andf2
whereOP is the set of program operations and ρis the set
of program entities.
A special case of features is that fisthe core feature (F),
A∪B∪Γ∪Δ=ρ, whereA,B,Γ,Δare the sets of program
entities that are declared, assigned, used and destructed in the
core system. Fis not controlled by any conﬁguration option.
C. Feature Interactions
In a conﬁgurable system, a feature may inﬂuence or mod-
ify (often called interact with) the functions offered by
other features through shared program entities that are
used to implement the features. For example, features X,
Kand Zinteract with one another via the variables GLO-
BAL.irq _domain _simple _ops andtwl_probe.temp . The man-
ners the features interacting with each other depend on how
the shared entities are operated. For example, feature Yassigns
&irq _domain _simple _ops toops and feature Kuses that vari-
able (line 22). If no assignment was done in Y, dereferencing
inKwould be invalid, causing a NULL pointer exception.
Multi-way feature-interaction. We present only on the in-
teractions between pairs of features because the interactions
between more than two features can be modeled as the opera-
tions on the shared variables between pairs of features. Let us
provide a sketch of the proof for this statement. We assume
that there exists an interaction among mfeatures ( m>2). For
simplicity, we consider the case of m=3 , and the interaction
amongf1∼OP×ρ1,f2∼OP×ρ2andf3∼OP×ρ3.
There are two cases of this interaction. First, there exists an
entity that shared by all 3 features, ρ1∩ρ2∩ρ3=ω/negationslash=∅. Since
ρ1∩ρ2⊃ωandρ2∩ρ3⊃ω, identifying interactions between
pairs directly captures the interaction among 3 features. The
second case is that ρ1∩ρ2=ω1,ρ2∩ρ3=ω2andω1∩ω2=∅.
Meanwhile, f3is inﬂuenced by f1(because the roles of f1and
f3features in this case are equal). This leads to that there exist
entities:e1∈ω1,e2∈ω2, such that e2=p(e1), wherepis
a value propagation function. This means the value of e1is
propagated to e2, and that inﬂuences f3. Hence, the interaction
among 3 features is still captured by determining interactions
between pairs of features.
For instance, the interaction among features X,KandZcan
be broken down into the shared program entities between
two pairs of features as follows: ( X,K) via the variable
GLOBAL.irq _domain _simple _ops , and ( K,Z) via the variabletwl_probe.temp . Thus, our solution can still model the inter-
actions with more than two features via the operations on their
shared program entities. From now on, we refer to a feature
interaction as an interaction determined via the shared program
entities between a pair of features.
In C OPRO, we focus on the feature interaction through the
shared program entities. The feature interactions when the
variables are associated with the external data such as when
they interfere with each other’s behaviors on ﬁles or databases
are beyond the scope of our static analysis-based solution.
Similarly, we will not detect the interactions through pointers
or arrays in this work. As a consequence, if both features use
(refer to) a program entity, they will not change the program’s
state. Thus, there is no interaction between two features if they
only use shared functions and variables.
With the above design focuses, in C OPRO, the interactions
between two features f1∼OP×ρ1, andf2∼OP×ρ2with
ρ1∩ρ2/negationslash=∅, can be categorized into nine kinds of interactions
that are displayed in Table II (the use-use case is eliminated
as explained).
D. Feature Interaction Detection
In a conﬁgurable system, the features (except the core
features of the system) are controlled by certain conﬁguration
options. Thus, if there exists an interaction among the features,
the interaction will be one of the following:
•declare-declare , there exist two option o1,o2and their
selected values v1,v2, such that α(o1,v1)∩α(o2,v2)/negationslash=∅
•declare-assign , there exist two option o1,o2and their
selected values v1,v2, such that α(o1,v1)∩β(o2,v2)/negationslash=∅
•declare-use , there exist two option o1,o2and their se-
lected values v1,v2, such that α(o1,v1)∩γ(o2,v2)/negationslash=∅
•declare-destruct , there exist two option o1,o2and their
selected values v1,v2, such that α(o1,v1)∩δ(o2,v2)/negationslash=∅
•assign-assign , there exist two option o1,o2and their
selected values v1,v2, such that β(o1,v1)∩β(o2,v2)/negationslash=∅
•assign-use , there exist two option o1,o2and their selected
valuesv1,v2, such that β(o1,v1)∩γ(o2,v2)/negationslash=∅
•assign-destruct , there exist two option o1,o2and their
selected values v1,v2, such that β(o1,v1)∩δ(o2,v2)/negationslash=∅
•use-destruct , there exist two option o1,o2and their se-
lected values v1,v2, such that γ(o1,v1)∩δ(o2,v2)/negationslash=∅
•destruct-destruct , there exist two option o1,o2and their
selected values v1,v2, such that δ(o1,v1)∩δ(o2,v2)/negationslash=∅
493
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Based on the above rules, our feature-interaction detection
algorithm statically analyzes the source code and conﬁguration
options, and then computes the sets α,β,γ, andδfor
any two options o1ando2. For example, we can detect a
declare-declare interaction between 2 features if there exists
2 options o1ando2, such that α(o1,v1)∩α(o2,v2)/negationslash=∅,
wherev1,v2are their selected values. Other detection rules are
similarly derived. For example, because β(CONFIG _OF_IRQ,
T)∩γ(CONFIG _TWL4030 _CORE, T)={ops}, there is a potential
assign-use interaction among features. Thus, in this case, the
actual assign-use interaction among YandKexists.
For the core feature, if F and other features interact
with one another, depending on the kinds of the interac-
tion, there exists a selection vof an option o, such that
α(o,v),β(o,v),γ(o,v),δ(o,v)intersect with A,B,Γ,Δ,i.e.,
intersecting with the entities in the core. Interactions among
core features and others are similarly identiﬁed.
In this version of C OPRO, we formulate feature interaction
statically through the completed set of operations on the
entities that are shared between features. More sophisticated
interactions relevant to pointers and external data such as ﬁles
or databases can be detected by using different data structures
in the same principle and using other types of analysis.
IV . C ONFIGURA TION PRIORITIZA TION
A. Overview
In general, to prioritize a given set of conﬁgurations under
test, our algorithm assigns a suspiciousness score to each
conﬁguration. The suspiciousness score is determined via the
number of the potential feature-interaction bugs in different
kinds that the variant corresponding to that conﬁguration might
potentially have.
Feature-interaction bugs can be induced by any kinds of
interaction. Table III shows 10 different kinds of feature-
interaction bugs that are potentially caused by the respec-
tive kinds of interactions listed in Table II of Section III.
The interactions in Table III are called sensitive interactions
with their suspicious selection of options. A conﬁguration
containing a suspicious selection potentially has the corre-
sponding violation. For example, at line 6, if β(o1,v1)∩
γ(o2,v2)/negationslash=∅, there is an assignment-use potential interac-
tion between f1andf2. Wheno1=v/prime
1,o2=v2, where
v/prime
1/negationslash=v1,f1might be disabled while f2is enabled, which
poses a violation of use without assignment . In Figure 2,
becauseα(CONFIG _SPARC, F )∩γ(CONFIG _OF_DEVICE,T )=
{GLOBAL.of _platform _populate }, the program might not be
compiled if CONFIG _SPARC =Tand CONFIG _OF_DEVICE =T
(use without declaration ).
B. Detailed Algorithm
The listing 1 shows the pseudo-code of C OPRO, our feature-
interaction aware conﬁguration prioritization algorithm. Given
a conﬁgurable system, we ﬁrst extract the set of options used
in the system. Then, for each selection vof each option o, the
setsα(o,v),β(o,v),γ(o,v), andδ(o,v)are computed via the
functionCollectProgramEntities (lines 4–5). After that, forAlgorithm 1 COPRO: Feature-Interaction aware Conﬁguration
Prioritization Algorithm
1:procedure DETECT SUSPICIOUS SELECTIONS (Code )
2: Options =ExtractOptions (Code )
3: for all o∈Options do
4: TSe lc =CollectP rogramEntities (o, T, Code )
5: FSe lc =CollectP rogramEntities (o, F, Code )
6: Selections.add (TSe lc )
7: Selections.add (FSe lc )
8: for all selc∈Selections do
9: for all other ∈Selections do
10: ifExistInteraction (selc, other )then
11: ifIsSensitiveInteraction (selc, other )then
12: ss=ExtractSuspSelection (selc, other )
13: SuspiciousSelections.add (ss)
14: procedure PRIORITIZE (Configurations ,SuspSelections )
15: for all c∈Configurations do
16: SScore =CaculateSuspScore (c,SuspSelections )
17: SetScore (c,SScore )
18: OrderBySuspiciousnessScoreDesc (Configurations )
each pair of option selections, it detects the potential interac-
tions among the features and checks whether the interactions
are sensitive as described in Table III. Sensitive interactions are
used to specify suspicious selections. This information is used
to compute the suspiciousness score for each conﬁguration
after conﬁguration selection (line 16). This score is the number
of suspicious selections contained by a conﬁguration, and
equal to the number of potential bugs that the corresponding
variant might have. Finally, the conﬁgurations are ranked
descendingly by their suspiciousness scores.
C. Static Analysis
In this version of C OPRO, to compute α,β,γ , andδfor
the value vof an option oinCollectProgramEntities ,
COPRO analyzes the code by using TypeChef , a variability-
aware parser [33]. For a given conﬁgurable code, TypeChef
is used to analyze and generate the corresponding variability-
aware control-ﬂow graph. In a variability-aware control-ﬂow
graph, the nodes refer to statements and the edges, which
are annotated with the corresponding presence conditions,
refer to the possible successor statements (conditional state-
ments). For the example in Figure 2, the successor of the
statement at line 22 is the conditional statement at line 24
ifCONFIG _OF_DEVICE is on, otherwise the statement at line
26 is the direct successor of the statement at line 22. After
that, C OPRO analyzes every conditional statements in the
generated control-ﬂow graph to identify the entities that are
either declared, deﬁned, used, or destructed in the statement
and compute α,β,γ , andδfor the options and its values in the
corresponding presence conditions. For the statement at line
24 in Figure 2, if the value of CONFIG _OF_DEVICE isT, the
variable status is deﬁned by using of_platform _populate
and n. This leads to that the variable status is in
β(CONFIG _OF_DEVICE,T), andγ(CONFIG _OF_DEVICE,T)con-
tains the function of_platform _populate and the variable n.
494
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Table III
DIFFERENT KINDS OF FEA TURE -INTERACTION DEFECTS
Kind of interaction Detection condition Suspicious selection Potential violation
1 declare-declare α(o1,v1)∩α(o2,v2)/negationslash=∅o1=v1,o2=v2 Declaration duplication
2 declare-use α(o1,v1)∩γ(o2,v2)/negationslash=∅o1=v/prime
1,o2=v2 Use without declaration
3 declare-use α(o1,v1)∩γ(o2,v2)/negationslash=∅o1=v1,o2=v/prime
2Unused variables/functions
4 declare-destruct α(o1,v1)∩δ(o2,v2)/negationslash=∅o1=v/prime
1,o2=v2 Destruction without declaration
5 declare-assign β(o1,v1)∩β(o2,v2)/negationslash=∅o1=v1,o2=v2 Assignment without declaration
6 assign-use β(o1,v1)∩γ(o2,v2)/negationslash=∅o1=v/prime
1,o2=v2 Use without assignment
7 assign-destruct β(o1,v1)∩δ(o2,v2)/negationslash=∅o1=v/prime
1,o2=v2 Destruction without deﬁnition
8 assign-destruct β(o1,v1)∩δ(o2,v2)/negationslash=∅o1=v1,o2=v/prime
2Memory leak
9 destruct-destruct δ(o1,v1)∩δ(o2,v2)/negationslash=∅o1=v1,o2=v2 Destruction duplication
10 destruct-use δ(o1,v1)∩γ(o2,v2)/negationslash=∅o1=v1,o2=v2 Use after destruction
Table IV
CONFIGURA TION OPTIONS AND THE V ALUES OF 4SELECTION FUNCTIONS α,β,γ,ANDδFOR THE EXAMPLE IN FIGURE 2
Option V alue α β γ δ
OF_IRQ T twl_probe.ops GLOBAL.irq _domain _simple _ops
IRQ_DOMAIN T GLOBAL.irq _domain _simple _ops,
GLOBAL.irq _domain _add,
irq_domain _add.irq,
irq_domain _add.opsirq_domain _add.irq irq_domain _add.ops
OF_DEVICE T GLOBAL.of _platform _populate
SPARC F GLOBAL.of _platform _populate,
of_platform _populate.node,
of_platform _populate.t
TWL4030 _CORE T GLOBAL.twl _probe, twl _probe.n,
twl_probe.status, twl _probe.temp,
twl_probe.opstwl_probe.node,
twl_probe.temp,
twl_probe.status,
twl_probe.opsGLOBAL.irq _domain _simple _ops,
GLOBAL.of _platform _populate,
GLOBAL.irq _domain _add,
twl_probe.node, twl _probe.temp,
twl_probe.status, twl _probe.ops
Table V
TOP-3 CONFIGURA TIONS RANKED BY COPRO FOR FIGURE 2
Rank by Rank OF_IRQ_ OF_x SPARC TWL4030 _Score
COPRO by SP IRQ DOMAIN DEVICE CORE
1 7 F T T T T 3
2 6 T T T T T 2
3 4 T F T T F 2
D. Running Example
Let us illustrate our algorithm via the example shown in
Figure 2. C OPRO computes the sets of the selection functions
for each option, and the result is shown in Table IV. Based on
the description on Table III, the suspicious selections include:
•CONFIG _OF_IRQ=F ,CONFIG _TWL4030 _CORE = T
•CONFIG _SPARC=T ,CONFIG _TWL4030 _CORE = T
•CONFIG _SPARC=T ,CONFIG _OF_DEVICE = T
•CONFIG _IRQ_DOMAIN=F ,CONFIG _OF_I R Q=T
•CONFIG _IRQ_DOMAIN=F ,CONFIG _TWL4030 _CORE = T
Based on the suspicious selections, C OPRO assigns the sus-
piciousness scores and ranks all the conﬁgurations accordingly.
Table V shows the ranked conﬁgurations for our example with
their corresponding scores. The top-ranked conﬁguration by
COPRO is the 7thconﬁguration in the order generated by
the ACTS tool [58], a combinatorial test generation tool (see
Table I). The conﬁguration covers both interaction bugs. Thus,
after inspecting/testing the ﬁrst conﬁguration, those two bugs
will be detected. In other words, C OPRO effectively ranks
higher the potential buggy variant than the SP algorithm.V. E MPIRICAL EV ALUA TION
To evaluate our conﬁguration prioritization approach, we
sought to answer the following:
RQ1 [Performance against a benchmark]. How does C OPRO
perform on V ariability Bugs Database (VBDb) [2], a
public dataset of bugs in conﬁgurable code?
RQ2 [Comparison]. How does C OPRO improve over the base-
line random prioritization and similarity-based prioritiza-
tion [3] approaches when we add each of them on top of
advanced sampling conﬁguration selection algorithms?
RQ3 [Performance in the wild]. How does it perform on not-
yet discovered interaction bugs in conﬁgurable systems?
RQ4 [Time Complexity] What is C OPRO’s running time?
To answer RQ1 and RQ2, we conducted an experiment to
evaluate C OPRO in a controlled environment with the VBDb
public benchmark of conﬁguration-related bugs [2]. Answer-
ing RQ2 helps evaluate how much improvement C OPRO
gains over the random prioritization and the state-of-the-art
similarity-based prioritization [3], when adding C OPRO on
top of the advanced conﬁguration selection techniques [42].
We answer RQ3 to evaluate C OPRO in the real-world set-
ting. While the bug detection tools cannot directly work on
conﬁgurable code, with C OPRO, we run them on the list of
suspicious conﬁgurations ranked by C OPRO.
A. Subject Systems
To evaluate C OPRO, we used two datasets in two dif-
ferent experiments. To answer RQ1 and RQ2, we used the
495
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Table VI
SUBJECT SYSTEMS IN VARIABILITY BUGS DA T ABASE
Systems MinOpt MaxOpt MinFile MaxFile #Bugs
Linux 3463 5504 18886 34012 43
Busybox 349 1449 236 799 18
HTTPD 602 791 264 426 23
Marlin 243 715 38 135 14
V ariability Bugs Database (VBDb) [2] as a benchmark. This
publicly available bug database has 98 manually veriﬁed
conﬁguration-related bugs in different versions of highly-
conﬁgurable systems: the Linux kernel [38], BusyBox [13],
Marlin [41], and Apache HTTPD [4]. Because the VBDb con-
tains conﬁguration-related bugs other than feature-interaction
ones, we kept only 46 feature-interaction bugs in those sys-
tems. Table VI shows their information including the minimum
and maximum numbers of conﬁguration options ( MinOpt ,Max-
Opt ), the minimum and maximum numbers of ﬁles ( MinFile ,
MaxFile ), and the number of feature-interaction bugs ( Bugs ).
For the second experiment of RQ3, we selected an open-
source conﬁgurable system with a long history: libpng [36]
v0.89 with 40KLOC in 19 ﬁles and 80 options, and xterm [57]
v2.24 with 50KLOC in 50 ﬁles, and 501 conﬁguration options.
B. Experimental Procedure
For each known buggy version of a subject system, we
chose to include the maximum number of ﬁles of 100 and the
maximum number of inclusion level of 3 (due to the limitation
of the TypeChef tool [32] that we used for variability-aware
parsing). We ﬁrst applied a conﬁguration selection process.
That is, to produce the sampled sets of conﬁgurations for
each buggy version, we ran sampling algorithms to select a
subset of conﬁgurations. For each buggy system version and
a particular sampling algorithm, we ran C OPRO on the set
of conﬁgurations selected by a sampling algorithm. For com-
parison, we ran the random prioritization and similarity-based
prioritization techniques [3] on the same conﬁgurations.
To evaluate C OPRO on detecting not-yet reported interaction
bugs in VBDb, we ﬁrst ran it on a subject system to achieve
the ranked list of the conﬁgurations. We also collected and
analyzed the sensitive interactions and potential suspicious
selections reported by our tool to detect unknown bugs. For the
top-ranked conﬁgurations in the list with the reported potential
suspicious interactions, we used a compiler to detect bugs.
C. Evaluation Metric
For evaluation, we adopted the Average Percentage Faults
Detected (APFD) [51], a widely-used metric in evaluating
test prioritization techniques. APFD is originally applied for
evaluating the average percentage bugs detected by a test suite.
In this work, since we used C OPRO with a bug detection tool,
we used APFD to measure prioritization effectiveness in term
of the rate of bug detection of a conﬁguration set, which is
deﬁned by the following formula:
APFD=1−/summationtextm
i=1CF i
n×m+1
2×nTable VII
AVERAGE APFD FOR COPRO VERSUS SP AND Random PRIORITIZA TION
(ADDED ON TOP OF ADV ANCED SAMPLING ALGORITHMS )
APFD A VG Rank
Random SP COPRO Random SP COPRO
Pairwise 0.68 0.75 0.93 5.12 4.11 1.55
Three-wise 0.83 0.89 0.96 7.80 4.79 2.39
Four-wise 0.88 0.94 0.97 11.57 6.26 3.77
Five-wise 0.89 0.93 0.97 11.03 6.74 3.49
One-enabled 0.64 0.69 0.91 36.87 30.55 13.19
One-disabled 0.60 0.56 0.88 37.34 38.21 14.76
Most-enabled
-disabled0.52 0.55 0.57 1.70 1.43 1.43
Statement
-coverage0.61 0.57 0.88 37.30 38.25 17.80
wherenandmdenote the number of conﬁgurations and
the number of bugs, respectively. CF iis the smallest number
of conﬁgurations in the list, which is needed to be inspected
to detect the ithbug. The APFD score is from 0to1. For the
ﬁxed numbers of faults and conﬁgurations, the higher APFD ,
the higher fault-detection rate and the better ranking order.
D. Effectiveness and Comparison (RQ1 and RQ2)
1)Comparative Results :Table VII shows the comparative
results in term of the average APFD and average rank (A VG
Rank) between C OPRO and the state-of-the-art prioritization
methods, when we ran all of them on the results of the ad-
vanced sampling techniques [42]. As seen, COPRO achieves
2–32% higher APFD (14.9% on average) compared to
SP and 5–28% higher (17.8% on average) compared to
Random approach .C OPRO also achieves much better ranking
compared to SP and Random . For example, using C OPRO
with One-disabled , which is recommended by the authors of
VBDb [2], the interaction bugs are revealed after no more than
15 conﬁgurations on average in the resulting ranked list by
COPRO are inspected, instead of more than 37 conﬁgurations
in the lists prioritized by SPand Random . Especially, in 78.0%
of the cases, C OPRO ranks the buggy conﬁgurations at
the top-3 positions in the list, while the SPand Random
approaches rank them at the top-3 positions for only 41.3%
and 26.1% of the cases .
We can also see that C OPRO outperforms the SP and Ran-
dom prioritization techniques consistently on the resulting con-
ﬁgurations selected by various advanced sampling algorithms.
That is, if one uses C OPRO to rank the conﬁgurations selected
by advanced algorithms, the inspection order by C OPRO is
better than those of the SP and Random prioritization. Note
that in the case of Most-enabled-disabled [42], for each buggy
system, there are only two conﬁgurations selected by the
sampling algorithm, and 23 out of 46 bugs cannot be revealed
by the selected set of conﬁgurations. That makes all three
prioritization approaches do not perform well in this case
and achieve nearly equal average APFDs and ranks. In brief,
COPRO is able to rank the buggy conﬁguration in a much
higher rank than SP and Random approaches . In other words,
if we add COPROas the prioritization technique on top of the
most advanced sampling algorithms, we would achieve a more
496
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. effective solution than adding other prioritization approaches
upon the selection algorithms .
2)Further Analysis :We further studied the cases in which
COPRO correctly ranks the buggy conﬁgurations at the top
positions. For the cases with correct ranking (1–3), we found
that in 77% (30 out of 39) of these bugs, the features interact
with one another via shared program entities. Thus, our rules
in Section III are applicable to detect the majority of feature-
interaction bugs in the public VBDb benchmark.
We also found an interesting scenario of indirect feature-
interactions that C OPRO detected. In some of those 30
cases, C OPRO identiﬁes sensitive interactions among features
indirectly via entities. For example, variable xis initialized in
the feature controlled by option Awith A=T .xis assigned to yin
the feature enabled if the option Bis on. Then, yis referred to
in another place that controlled by option C,C=T . In this case, if
A=F ,B=T , and C=T ,anull pointer exception might be induced.
In this case, since of the propagation of variables’ values,
the interaction between two features controlled by Aand C
can be captured by C OPRO via the feature controlled by B.
Thus, the buggy conﬁgurations are ranked on the top. This
also indicates C OPRO’s capability in detecting conﬁgurations
containing bugs relevant to more than two features .
3)Examples on Feature-Interaction Bugs :Let us present
the conﬁguration-related bugs involving high-degree feature
interactions and the cases that C OPRO detected the feature-
interaction bugs not-yet-discovered in the VBDb benchmark.
A bug involving 6 conﬁguration options .Figure 3 shows a
bug in Apache HTTPD at commit 2124ff4. The bug is in the
ﬁlemod_cgid.c . In this example, the bug is observed when
RLIMIT _CPU ,RLMIT _NPROC ,RLIMIT _DATA ,RLIMIT _VMEM , and
RLIMIT _ASare disabled, while RLIMIT _NPROC is enabled. With
the selections of the combinations of those options, the ﬁeld
limits of any variable of the type cgid _red_t(e.g. req ) used
in any features is not declared (line 3). Meanwhile, the ﬁled
limits is used in req.limits on line 12 when RLIMIT _NPROC
is enabled. By identifying the suspicious interactions between
the features controlled by the pairs of RLIMIT _NPROC and each
of these 5 other options via the ﬁeld req.limits ,C OPRO
speciﬁes that the selection that RLIMIT _NPROC =T,RLIMIT _CPU
=F,RLMIT _NPROC =F,RLIMIT _DATA =F,RLIMIT _VMEM =F,
andRLIMIT _AS=Fis more suspicious than all other selections
containing those six conﬁguration options.
Not-yet discovered feature-interaction bugs in VBDb
benchmark . Interestingly, while using VBDb, we were able
to use C OPRO detect the interaction bugs that were neither
discovered and reported in those systems nor in VBDb. In
total, we found 17 such feature-interaction bugs including
12 using-without-declaration bugs, 2 memory-leak bugs, 2
declaration duplication bugs, and 1 dead code issue .
Figure 4 shows 2 not-yet-discovered bugs: a memory leak
issue and an assignment without declaration bug at commit
fac312d78bf (which also has use without declaration bug and
destruction without declaration bug). The assignment without
declaration bug occurs only if BB_FEATURE _LS_SORTFILES =F1typedef struct {
2#ifdefined (RLIMIT _CPU) || defined (RLMIT _NPROC) ||
defined (RLIMIT _DATA) || defined(RLIMIT _VMEM ) ||
defined(RLIMIT _AS)
3 cgid _rlimit _t limits;
4#endif
5} cgid _req_t;
6static apr_status _t send _req(){
7 cgid _req_t req = {0};
8#ifdefined(RLIMIT _DATA) || defined(RLIMIT _VMEM) ||
defined(RLIMIT _AS)
9 req.limits.limit _mem_s e t=1 ;
10#endif
11#ifdef RLIMIT _NPROC
12 req.limits.limit _nproc = 0;
13#endif
14}
Figure 3. A 6-way Feature-Interaction Bug in Apache Httpd
1void showdirs( struct dnode **dn, int ndirs){
2#ifdef BB_FEATURE _LS_SORTFILES
3int dndirs;
4struct dnode **dnd;
5#endif
6subdnp = list _dir(dn[i]->fullname);
7#ifdef CONFIG _FEATURE _LS_RECURSIVE
8dnd = splitdnarray(subdnp, nfiles);
9dndirs = countsubdirs(subdnp, nfiles);
10#ifdef CONFIG _FEATURE _LS_SORTFILES
11 shellsort(dnd, dndirs);
12#endif
13 showdirs(dnd, dndirs);
14 free(dnd);
15 free(subdnp);
16#endif
17}
Figure 4. Two Not-yet-discovered Bugs in Busybox
andCONFIG _FEATURE _LS_RECURSIVE =T. In this case, dndirs
anddnd are not declared since lines 3–4 are not included, but
they are used at lines 11 and 13. Moreover, dnd is destructed
on line 15. This bug was ﬁxed at commit ea224be6aa8 (in
almost 6 years later). 3 years after that, a memory leak
issue was reported and ﬁxed at commit ffd4774ad25: as
CONFIG _FEATURE _LS_RECURSIVE is disabled, the memory con-
trolled by subdnp is initialized at line 9 and not released. With
COPRO, it would have been ﬁxed earlier.
A run-time feature-interaction Bug in Busybox COPRO is
also able to detect run-time errors caused by feature in-
teractions. Figure 5 shows a simpliﬁed bug in Busybox
extracted from http://vbdb.itu.dk/#bug/busybox/061fd0a. In
this case, a bug occurs when CONFIG _FEATURE _HDPARM _-
HDIO _UNREGISTER _HWIF =Tifc=‘U’ and p=NULL . The ex-
ecution goes to expected _hwif _error . However, this label is
visible only when CONFIG _FEATURE _HDPARM _HDIO _SCAN _HWIF
=T. Otherwise, we would have a run-time error.
497
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. 1int main( int argc, char **argv){
2int r = rand() % 2;
3char *p;
4char c;
5scanf( "%c" , &c);
6switch (c){
7 case ’W’:
8 if(r)
9 p= *argv++, --argc;
10 break ;
11#ifdef CONFIG _FEATURE _HDPARM _HDIO _UNREGISTER _HWIF
12 case ’U’:
13 if(!p)
14 goto expected _hwif _error; //ERROR
15 break ;
16#endif /*CONFIG _FEATURE _HDPARM _HDIO _UNREGISTER _HWIF */
17#ifdef CONFIG _FEATURE _HDPARM _HDIO _SCAN _HWIF
18 case ’R’:
19 if(!p)
20 goto expected _hwif _error;
21expected _hwif _error:
22 printf( "expected hwif value" );
23
24#endif /*CONFIG _FEATURE _HDPARM _HDIO _SCAN _HWIF */
25}
26return 0;
27}
Figure 5. A Run-time Feature-Interaction Bug in Busybox
E. Effectiveness in Detecting Bugs in the Wild (RQ3)
To evaluate the effectiveness of C OPRO on the real-world,
open-source projects, we ran it on the conﬁgurable systems
libpng v0.89 and xterm v2.24 to detect interaction bugs. Inter-
estingly, with C OPRO, we were able to detect 4 interaction
bugs that have not been reported/discovered before . They
have the same nature of using variables/functions without
declarations . Let us discuss two of them in details. The other
one can be found on our website [1].
In Figure 6, the code contains 2 bugs. The ﬁrst one is
observed when the option PNG_READ _INTERLACING _SUPPORTED
orPNG_WRITE _INTERLACING _SUPPORTED is enabled (line 4)
and PNG_INTERNAL is disabled (line 1). In this case, the
function png_set_interlace _handling is declared (line 5),
and PNG_INTERLACE (line 6) is used inside this function.
Meanwhile, the constant PNG_INTERLACE (line 2) is declared
only if PNG_INTERNAL is enabled. Thus, if PNG_INTERNAL
is disabled, and either PNG_READ _INTERLACING _SUPPORTED
orPNG_WRITE _INTERLACING _SUPPORTED is enabled, we will
have a compiling error at line 6. The second bug
occurs when both PNG_READ _INTERLACING _SUPPORTED and
PNG_WRITE _INTERLACING _SUPPORTED areF. In this case, png_-
read _image use an undeclared function (line 10).
F . Time Complexity (RQ4)
We run our experiments on a computer with Intel Core i5
2.7GHz processor, 8GB RAM. The running time to analyze
the most complex case that contains 43KLOC and 194 con-
ﬁguration options and rank 156 conﬁgurations is 211,020ms.1#ifdefined(PNG _INTERNAL)
2#define PNG _INTERLACE 0x0002
3#endif /*PNG_INTERNAL */
4#ifdefined(PNG _READ _INTERLACING _SUPPORTED) ||
defined(PNG _WRITE _INTERLACING _SUPPORTED)
5int png_set_interlace _handling(png _structp png _ptr){
6 png_ptr->transformations |= PNG _INTERLACE;
7}
8#endif
9void png_read _image(png _structp png _ptr){
10 int pass = png _set_interlace _handling(png _ptr);
11}
Figure 6. Two Not-yet-discovered Bugs in libpng
G. Limitations and Potential Solutions
For the cases that C OPRO did not rank well the buggy
conﬁgurations, we found that the majority of them are not
in the kinds of interaction-related defects listed in Section III.
For example, a variable xis assigned a value vif option A
is enabled, otherwise x=v’ . Then, xis referred to in a feature
controlled by option B. In this case, C OPRO detects the inter-
actions between those features. However, as a static technique,
COPRO could not conclude which option selections are more
suspicious. To overcome such limitation, one could use a
dynamic analysis approach for conﬁgurable code [45].
Figure 7 shows a simpliﬁed bug in HTTPD (commit
9327311d30f) that C OPRO did not rank well the buggy con-
ﬁgurations. In Figure 7, a use without assignment is exposed
when APU_HAS_LDAP andAPU_HAS_SHARED _MEMORY are on. C O-
PRO did not work since there is no feature where rmm_lock
is assigned. Consequently, no assign-use interaction exists.
H. Extension to COPRO
Generally, to detect more kinds of bug such as in the
above example, one can extend our set of conditions with
the corresponding violations in Table III. One can de-
ﬁne a new condition to detect this bug as follows: i)
α(APU_HAS_LDAP,T)∩γ(APU_HAS_SHARED _MEMORY,T)/negationslash=∅=
{util _ldap _cache _init.rmm _lock}and ii) there is no def-
inition of rmm_lock in its scope, which is the function
util _ldap _cache _init .
Interestingly, note that for this buggy system, C O-
PRO ranked the conﬁguration to reveal another ﬂaw of
unused variable (rmm_lock ) when APU_HAS_LDAP=T and
APU_HAS_SHARED _MEMORY=F .
In 14 cases out of 368 cases, the interactions that cause
the interaction bugs are really detected, but the conﬁgurations
that reveal the bugs are still ranked lower than others. The
reason for these cases is that other conﬁgurations containing
more suspicious selections that actually do not cause the bugs.
To faster detect the bug in these situations, one can apply
the Additional Priortization strategy [22] to rank the set of
conﬁgurations according to their numbers of potential bugs in
an incremental manner. By this strategy, the next conﬁguration
to be selected is the one containing the largest number of
498
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. 1void apr_rmm_init( char *rmm_lock){
2printf( "%s\n" , rmm _lock);
3}
4#ifdef APU_HAS_LDAP
5void util _ldap _cache _init(){
6char *rmm_lock;
7#ifdef APR_HAS_SHARED _MEMORY
8apr_rmm_init(rmm _lock); // ERROR: rmm _lock
uninitialized
9#endif
10}
11#endif
Figure 7. C OPRO did not rank well buggy conﬁgurations
potential bugs that have not been contained by the previous
selected conﬁgurations in the previous steps. Moreover, for the
interaction bugs relevant to pointers and external data ﬁles, one
can deﬁne new rules to add to our framework.
VI. R ELA TED WORK
COPRO is most closely related to the work by Al-Hajjaji
et al. [3] on similarity-based Prioritization (SP). The key
idea of SP approach dissimilar test sets are likely to detect
more defects than similar ones [3]. In SP , the conﬁguration
with the maximum number of features is selected to be the
ﬁrst one under test and the next conﬁguration must have the
minimum number of similar features as the previously selected
conﬁguration. In comparison, SP does not analyze the nature
of feature interactions, while C OPRO does. This avoids the
problem in SP that the different features to be considered next
might not be the ones that potentially causes violations.
COPRO is also related to the work on conﬁguration se-
lection approaches to reduce the number of conﬁgurations to
be tested [42]. They focus on the step before conﬁguration
prioritization, therefore the resulting set of conﬁgurations is
not ranked as in C OPRO. Thet-wise (i.e., k-way) sampling
algorithm covers all combinations of toptions [28], [35], [46],
[48], while pair-wise checks all pairs of conﬁguration op-
tions [40], [47]. Recent study by Medeiros et al. [42] showed
that realistic constraints among options, global analysis, header
ﬁles, and build-system information inﬂuence the performance
of most sampling algorithms substantially; and several algo-
rithms are no longer feasible in practice. Importantly, they lack
conﬁguration prioritization, thus, developers need to spend
efforts to perform QA on all the variants.
COPRO is also related to Variability-aware (V A) analy-
sis [37]. V A analysis is a variation of a traditional analysis
that considers the variability in the conﬁgurable code. The
variability-aware analysis techniques have been proposed for
type checking [16], [31], [37], [54], model checking [19], [34],
data-ﬂow analysis [11], [12], [37], and other analyses [21] on
multiple compile-time conﬁgurations of a system at a time.
The main drawback of this approach is that it cannot reuse
existing static analysis tools, and each type of analysis must
be rewritten in a variability-aware fashion. For example, todetect NULL exception, one must rewrite such an analysis to
consider all different conﬁgurations in a conﬁgurable code.
In our experiment, we connect C OPRO with an existing bug
detection tool to work on conﬁgurable code. V ariability-aware
execution [45], [43] explores multiple paths of execution at
the same time to detect feature-interaction bugs. However, it
suffers scalability issue.
Several approaches were proposed to detect feature inter-
actions [6], [25]. V eriﬁcation [7] is also used to detect feature-
interaction bugs. Other prioritization approaches aim for state-
ment coverage [52], [53] via static checkers. The issue is that
computing an optimal solution for the coverage problem is NP-
hard, and including each block of optional code at least once
does not guarantee that all possible combinations of individual
blocks of optional code are considered [42]. To avoid ﬁnding
optimal coverage solution, the most-enabled-disabled [52]
algorithm checks two samples independently of the number
of conﬁguration options. When there are no constraints among
conﬁguration options, it enables all options and then it disables
all conﬁguration options. One-(enabled/disabled) algorithm [2]
enables/disables one conﬁguration option at a time. Despite
different levels of heuristics, they do not analyze the entities
in source code.
Several pproaches are aimed for testing for conﬁgurable
systems [14], [20], [26], [39]. In product-line testing [49] and
framework testing [15] it is a common strategy to unit test
components or plug-ins in isolation, while integration tests are
often neglected or performed only for speciﬁc conﬁgurations.
Greiler et al. suggest shipping test cases with plug-ins and
running them in client systems [26]. In essence, this postpones
tests of conﬁgurations until the conﬁguration is actually used.
Other approaches have been proposed for static analysis
of product lines [11], [12], [16], [19], [21], [31], [54], [55].
Researchers explore to represent and reason about partial
but ﬁnite conﬁguration spaces compactly with BDDs or SA T
solvers (as used in our variability contexts) [8], [31], [44],
choices of structures [23] and complex structures [24], [37].
VII. C ONCLUSION
We propose C OPRO, a novel formulation of feature-
interaction bugs via common program entities enabled/dis-
abled by the features. Leveraging from that, we develop
efﬁcient feature-interaction-aware conﬁguration prioritization
technique for a conﬁgurable system by ranking the conﬁgu-
rations according to their total number of potential bugs. We
evaluated C OPRO in two complementary settings: detecting
conﬁguration-related bugs in a benchmark and a real-world
open-source systems. C OPRO outperforms the other tech-
niques in which in 78% of the cases, it ranks the buggy conﬁg-
urations at the top 3 positions. Interestingly, it is able to detect
17 not-yet-discovered, high-degree, feature-interaction bugs.
ACKNOWLEDGMENT
This work was supported in part by the US National Science
Foundation (NSF) grants CCF-1723215, CCF-1723432, TWC-
1723198, CCF-1518897, and CNS-1513263.
499
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] . https://doubledoubleblind.github.io/copro/.
[2] Iago Abal, Claus Brabrand, and Andrzej Wasowski. 42 V ariability
Bugs in the Linux Kernel: A Qualitative Analysis. In Proceedings of
the 29th ACM/IEEE International Conference on Automated Software
Engineering , ASE ’14, pages 421–432, New Y ork, NY , USA, 2014.
ACM.
[3] Mustafa Al-Hajjaji, Thomas Thüm, Jens Meinicke, Malte Lochau, and
Gunter Saake. Similarity-based prioritization in software product-line
testing. In Proceedings of the 18th International Software Product Line
Conference - V olume 1 , SPLC ’14, pages 197–206, New Y ork, NY , USA,
2014. ACM.
[4] Apache Httpd. http://httpd.apache.org/.
[5] Sven Apel and Christian Kästner. An overview of feature-oriented
software development. JOURNAL OF OBJECT TECHNOLOGY , 8(5).
[6] Sven Apel, Sergiy Kolesnikov, Norbert Siegmund, Christian Kästner,
and Brady Garvin. Exploring feature interactions in the wild: The new
feature-interaction challenge. In Proceedings of the 5th International
Workshop on Feature-Oriented Software Development , FOSD ’13, pages
1–8, New Y ork, NY , USA, 2013. ACM.
[7] Sven Apel, Hendrik Speidel, Philipp Wendler, Alexander von Rhein,
and Dirk Beyer. Detection of Feature Interactions Using Feature-
aware V eriﬁcation. In Proceedings of the 26th IEEE/ACM International
Conference on Automated Software Engineering , ASE ’11, pages 372–
375, Washington, DC, USA, 2011. IEEE Computer Society.
[8] Don Batory. Feature models, grammars, and propositional formulas.
InProc. Int’l Software Product Line Conference (SPLC) , volume 3714
ofLecture Notes in Computer Science , pages 7–20, Berlin/Heidelberg,
2005. Springer-V erlag.
[9] T. Berger, S. She, R. Lotufo, A. Wasowski, and K. Czarnecki. A study of
variability models and languages in the systems software domain. IEEE
Transactions on Software Engineering , 39(12):1611–1640, Dec 2013.
[10] Thorsten Berger, Ralf Rublack, Divya Nair, Joanne M. Atlee, Martin
Becker, Krzysztof Czarnecki, and Andrzej Wkasowski. A survey of
variability modeling in industrial practice. In Proceedings of the Seventh
International Workshop on V ariability Modelling of Software-intensive
Systems , V aMoS ’13, pages 7:1–7:8, New Y ork, NY , USA, 2013. ACM.
[11] Eric Bodden, Társis Tolêdo, Márcio Ribeiro, Claus Brabrand, Paulo
Borba, and Mira Mezini. Spllift: Statically analyzing software product
lines in minutes instead of years. In Proc. Conf. Programming Language
Design and Implementation (PLDI) , pages 355–364, New Y ork, 2013.
ACM Press.
[12] Claus Brabrand, Márcio Ribeiro, Társis Tolêdo, and Paulo Borba.
Intraprocedural dataﬂow analysis for software product lines. In Proc.
Int’l Conf. Aspect-Oriented Software Development (AOSD) , pages 13–
24, New Y ork, 2012. ACM Press.
[13] Busy Box. https://busybox.net/.
[14] Isis Cabral, Myra B. Cohen, and Gregg Rothermel. Improving the testing
and testability of software product lines. In Proceedings of the 14th
International Conference on Software Product Lines: Going Beyond ,
SPLC’10, pages 241–255, Berlin, Heidelberg, 2010. Springer-V erlag.
[15] Sheng Chen, Martin Erwig, and Eric Walkingshaw. Extending type
inference to variational programs. Technical report (draft), School of
EECS, Oregon State University, 2012.
[16] Sheng Chen, Martin Erwig, and Eric Walkingshaw. Extending type
inference to variational programs. ACM Trans. Program. Lang. Syst.
(TOPLAS) , 2013.
[17] Andreas Classen, Patrick Heymans, Pierre-Yves Schobbens, and Axel
Legay. Symbolic model checking of software product lines. In Pro-
ceedings of the 33rd International Conference on Software Engineering ,
ICSE ’11, pages 321–330, New Y ork, NY , USA, 2011. ACM.
[18] Andreas Classen, Patrick Heymans, Pierre-Yves Schobbens, Axel Legay,
and Jean-François Raskin. Model checking lots of systems: Efﬁcient
veriﬁcation of temporal properties in software product lines. In Pro-
ceedings of the 32nd ACM/IEEE International Conference on Software
Engineering - V olume 1 , ICSE ’10, pages 335–344, New Y ork, NY ,
USA, 2010. ACM.
[19] Andreas Classen, Patrick Heymans, Pierre-Yves Schobbens, Axel Legay,
and Jean-Francois Raskin. Model checking lots of systems: Efﬁcient
veriﬁcation of temporal properties in software product lines. In Proc.
Int’l Conf. Software Engineering (ICSE) , pages 335–344, New Y ork,
2010. ACM Press.[20] Myra B. Cohen, Matthew B. Dwyer, and Jiangfan Shi. Interaction
testing of highly-conﬁgurable systems in the presence of constraints. In
Proceedings of the 2007 International Symposium on Software Testing
and Analysis , ISST A ’07, pages 129–139, New Y ork, NY , USA, 2007.
ACM.
[21] Krzysztof Czarnecki and Krzysztof Pietroszek. V erifying feature-based
model templates against well-formedness OCL constraints. In Proc. Int’l
Conf. Generative Programming and Component Engineering (GPCE) ,
pages 211–220, New Y ork, 2006. ACM.
[22] S. Elbaum, A. G. Malishevsky, and G. Rothermel. Test case prioriti-
zation: a family of empirical studies. IEEE Transactions on Software
Engineering , 28(2):159–182, Feb 2002.
[23] Martin Erwig and Eric Walkingshaw. The choice calculus: A representa-
tion for software variation. ACM Trans. Softw. Eng. Methodol. (TOSEM) ,
21(1):6:1–6:27, 2011.
[24] Martin Erwig and Eric Walkingshaw. V ariation programming with the
choice calculus. In Generative and Transformational Techniques in
Software Engineering IV , pages 55–100. Springer Berlin Heidelberg,
2013.
[25] Brady J. Garvin and Myra B. Cohen. Feature interaction faults
revisited: An exploratory study. In Proceedings of the 2011 IEEE 22nd
International Symposium on Software Reliability Engineering , ISSRE
’11, pages 90–99, Washington, DC, USA, 2011. IEEE Computer Society.
[26] Michaela Greiler, Arie van Deursen, and Margaret-Anne Storey. Test
confessions: A study of testing practices for plug-in systems. In Pro-
ceedings of the 34th International Conference on Software Engineering ,
ICSE ’12, pages 244–254, Piscataway, NJ, USA, 2012. IEEE Press.
[27] Alexander Gruler, Martin Leucker, and Kathrin Scheidemann. Modeling
and model checking software product lines. In Proceedings of the
10th IFIP WG 6.1 International Conference on F ormal Methods for
Open Object-Based Distributed Systems , FMOODS ’08, pages 113–131,
Berlin, Heidelberg, 2008. Springer-V erlag.
[28] Martin Fagereng Johansen, Oystein Haugen, and Franck Fleurey. An
algorithm for generating t-wise covering arrays from large feature
models. In Proceedings of the 16th International Software Product Line
Conference - V olume 1 , SPLC ’12, pages 46–55, New Y ork, NY , USA,
2012. ACM.
[29] Kyo C Kang, Sholom G Cohen, James A Hess, William E Novak, and
A Spencer Peterson. Feature-oriented domain analysis (foda) feasibility
study. Technical report, Carnegie-Mellon Univ Pittsburgh Pa Software
Engineering Inst, 1990.
[30] Christian Kästner. Virtual separation of concerns: toward preprocessors
2.0. it-Information Technology Methoden und innovative Anwendungen
der Informatik und Informationstechnik , 54(1):42–46, 2012.
[31] Christian Kästner, Sven Apel, Thomas Thüm, and Gunter Saake. Type
checking annotation-based product lines. ACM Trans. Softw. Eng.
Methodol. (TOSEM) , 21(3):14:1–14:39, 2012.
[32] Christian Kästner, Paolo G. Giarrusso, Tillmann Rendel, Sebastian
Erdweg, Klaus Ostermann, and Thorsten Berger. V ariability-aware
parsing in the presence of lexical macros and conditional compilation.
InProceedings of the 2011 ACM International Conference on Object
Oriented Programming Systems Languages and Applications , OOPSLA
’11, pages 805–824, New Y ork, NY , USA, 2011. ACM.
[33] Andy Kenner, Christian Kästner, Steffen Haase, and Thomas Leich.
TypeChef: Toward Type Checking #Ifdef V ariability in C. In Proceed-
ings of the 2nd International Workshop on Feature-Oriented Software
Development , FOSD ’10, pages 25–32, New Y ork, NY , USA, 2010.
ACM.
[34] Kim Lauenroth, Klaus Pohl, and Simon Toehning. Model checking
of domain artifacts in product line engineering. In Proc. Int’l Conf.
Automated Software Engineering (ASE) , pages 269–280, Los Alamitos,
CA, 2009. IEEE Computer Society.
[35] Y u Lei, Raghu Kacker, D. Richard Kuhn, V adim Okun, and James
Lawrence. Ipog-ipog-d: Efﬁcient test generation for multi-way com-
binatorial testing. Softw. Test. V erif. Reliab. , 18(3):125–148, September
2008.
[36] libpng. http://www.libpng.org/.
[37] Jörg Liebig, Alexander von Rhein, Christian Kästner, Sven Apel, Jens
Dörre, and Christian Lengauer. Scalable analysis of variable software. In
Proceedings of the 2013 9th Joint Meeting on F oundations of Software
Engineering , ESEC/FSE 2013, pages 81–91, New Y ork, NY , USA, 2013.
ACM.
[38] Linux Kernel. https://www.kernel.org/.
500
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. [39] Ivan Do Carmo Machado, John D. Mcgregor, Yguaratã Cerqueira
Cavalcanti, and Eduardo Santana De Almeida. On strategies for testing
software product lines: A systematic literature review. Inf. Softw.
Technol. , 56(10):1183–1199, October 2014.
[40] Dusica Marijan, Arnaud Gotlieb, Sagar Sen, and A ymeric Hervieu.
Practical pairwise testing for software product lines. In Proceedings
of the 17th International Software Product Line Conference , SPLC ’13,
pages 227–235, New Y ork, NY , USA, 2013. ACM.
[41] Marlin. http://marlinfw.org/.
[42] Flávio Medeiros, Christian Kästner, Márcio Ribeiro, Rohit Gheyi, and
Sven Apel. A comparison of 10 sampling algorithms for conﬁgurable
systems. In Proceedings of the 38th International Conference on
Software Engineering , ICSE ’16, pages 643–654, New Y ork, NY , USA,
2016. ACM.
[43] Jens Meinicke, Chu-Pan Wong, Christian Kästner, Thomas Thüm, and
Gunter Saake. On essential conﬁguration complexity: Measuring in-
teractions in highly-conﬁgurable systems. In Proceedings of the 31st
IEEE/ACM International Conference on Automated Software Engineer-
ing, ASE 2016, pages 483–494, New Y ork, NY , USA, 2016. ACM.
[44] Marcílio Mendonça, Andrzej Wkasowski, and Krzysztof Czarnecki.
SA T-based analysis of feature models is easy. In Proc. Int’l Software
Product Line Conference (SPLC) , pages 231–240, New Y ork, 2009.
ACM Press.
[45] Hung Viet Nguyen, Christian Kästner, and Tien N. Nguyen. Exploring
variability-aware execution for testing plugin-based web applications. In
Proceedings of the 36th International Conference on Software Engineer-
ing, ICSE 2014, pages 907–918, New Y ork, NY , USA, 2014. ACM.
[46] Changhai Nie and Hareton Leung. A survey of combinatorial testing.
ACM Comput. Surv. , 43(2):11:1–11:29, February 2011.
[47] Sebastian Oster, Florian Markert, and Philipp Ritter. Automated incre-
mental pairwise testing of software product lines. In Proceedings of
the 14th International Conference on Software Product Lines: Going
Beyond , SPLC’10, pages 196–210, Berlin, Heidelberg, 2010. Springer-
V erlag.
[48] Gilles Perrouin, Sagar Sen, Jacques Klein, Benoit Baudry, and Yves le
Traon. Automated and scalable t-wise test case generation strategies for
software product lines. In Proceedings of the 2010 Third InternationalConference on Software Testing, V eriﬁcation and V alidation , ICST ’10,
pages 459–468, Washington, DC, USA, 2010. IEEE Computer Society.
[49] Klaus Pohl, Günter Böckle, and Frank J. van der Linden. Software
Product Line Engineering: F oundations, Principles and Techniques .
Springer-V erlag, Berlin/Heidelberg, 2005.
[50] H. Post and C. Sinz. Conﬁguration lifting: V eriﬁcation meets software
conﬁguration. In Proceedings of the 2008 23rd IEEE/ACM International
Conference on Automated Software Engineering , ASE ’08, pages 347–
350, Washington, DC, USA, 2008. IEEE Computer Society.
[51] G. Rothermel, R. H. Untch, Chengyun Chu, and M. J. Harrold. Priori-
tizing test cases for regression testing. IEEE Transactions on Software
Engineering , 27(10):929–948, Oct 2001.
[52] Reinhard Tartler, Christian Dietrich, Julio Sincero, Wolfgang Schröder-
Preikschat, and Daniel Lohmann. Static analysis of variability in system
software: The 90,000# ifdefs issue.
[53] Reinhard Tartler, Daniel Lohmann, Christian Dietrich, Christoph Egger,
and Julio Sincero. Conﬁguration coverage in the analysis of large-scale
system software. SIGOPS Oper . Syst. Rev. , 45(3):10–14, January 2012.
[54] Sahil Thaker, Don Batory, David Kitchin, and William Cook. Safe com-
position of product lines. In Proc. Int’l Conf. Generative Programming
and Component Engineering (GPCE) , pages 95–104, New Y ork, 2007.
ACM Press.
[55] Thomas Thüm, Sven Apel, Christian Kästner, Martin Kuhlemann, Ina
Schaefer, and Gunter Saake. Analysis strategies for software product
lines. Technical Report FIN-004-2012, School of Computer Science,
University of Magdeburg, April 2012.
[56] Thomas Thüm, Sven Apel, Christian Kästner, Ina Schaefer, and Gunter
Saake. A classiﬁcation and survey of analysis strategies for software
product lines. ACM Comput. Surv. , 47(1):6:1–6:45, June 2014.
[57] xterm. https://invisible-island.net/xterm/.
[58] Linbin Y u, Y u Lei, Raghu N Kacker, and D Richard Kuhn. Acts: A
combinatorial test generation tool. In 2013 IEEE Sixth International
Conference on Software Testing, V eriﬁcation and V alidation , pages 370–
375. IEEE, 2013.
[59] Pamela Zave. Programming methodology. chapter An Experiment in
Feature Engineering, pages 353–377. Springer-V erlag New Y ork, Inc.,
New Y ork, NY , USA, 2003.
501
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. 