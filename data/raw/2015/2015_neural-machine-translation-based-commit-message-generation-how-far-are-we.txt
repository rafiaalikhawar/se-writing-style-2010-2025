Neural-Machine-Translation-Based Commit Message
Generation: How FarAre We?
Zhongxin Liu
Zhejiang University
China
liu_zx@zju .edu.cn
David Lo
Singapore Management University
Singapore
davidlo@smu.edu.sg
ABSTRACTXin Xia
Monash University
Australia
xin.xia@monash .edu
Zhenchang Xing
Australian National University
Australia
zhenchang.xing@anu.edu.au
KEYWORDSAhmed E.Hassan
Queen 'sUniversity
Canada
ahmed@cs.queensu.ca
Xinyu Wang
Zhejiang University
China
wangxinyu@zju.edu.cn
Commit messages can be regarded as the documentation ofsoft ­
ware changes .These messages describe thecontent andpurposes
ofchan ges, hence areuseful forprogram comprehension andsoft­
ware maintenance. However , due to the lack oftime and direct
motivation, commit messages sometimes areneglected by develop­
ers. To address thisproblem, Jiang et al. proposed anapproach (we
refer to it as NMT) ,which leverages aneural machine translation
algorithm toautomatically generate short commit messages from
code. The reported performance oftheir approach ispromisin g,
however ,they didnotexplore why their approach performs well.
Thus , in this paper , we first perform anin-depth analysis oftheir
experimental results. We find that (1) Most of the test diffs from
which NMT can generate high-quality messages are similar to one
ormore training diffsat the token level. (2) About 16%ofthe
commit messages inJiang etal.s dataset arenoisy due to being
automatically generated or due to them describing repetitive trivial
changes . (3) The performance ofNMT declines by a large amount
after removing such noisy commit messages . Inaddition ,NMT is
complicated and time-consuming .Inspired by our first finding , we
proposed asimpler and faster approach ,named NNGen (Nearest
Neighbor Generator) , togenerate concise commit messages using
thenearest neighbor algorithm. Our experimental results show
that NNGen is over 2,600 times faster than NMT ,andoutperforms
NMT interms ofBLEU (anaccuracy measure that iswidely used
toevaluate machine translation systems) by 21 %.Finally , we also
discuss some observations for the road ahead forautomated commit
message generation toinspire other researchers.
CCS CONCEPTS
•Software and itsengineering --->Software maintenance tools ;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise. or republish.
to post on servers or to redistribute to lists. requires prior specific permission and /or a
fee. Reque st permi ssion s from p enni ssions @acm.or g.
ASE '18,September 3-7,2018, Montpellie r;France
©2018Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09... $15.00
http s://doL org/l O .1145/3238147.3238190
373Commit message generation ,Nearest neighbor algorithm ,Neural
machine translation
ACM Reference Format :
Zhongxin Liu , Xin Xia,Ahm edE.Hassan, David Lo,Zhenchang Xing ,
and Xinyu Wan g. 2018. Neural-Ma chine-Translation-Based Com m it Mes­
sage G eneration : Ho w Far Are We ? In Proceedings ofthe 2018 33rd ACM/IEEE
International Confe rence on Automated Softwar e Engineering (ASE '18), Sep­
tember 3- 7, 2018, Montpellier, France. ACM ,New York , NY, USA , 12 pages.
http s://d oL org/10 .1145/3238147.3238190
1INTRODUCTION
Insoftware projects ,version control systems arewidely used to
manage theevolving code base. While committing achange to a
version control system, developers document their changes using
acommit message . Acommit message is afree-form textual de­
scription of its corresponding change. The message may summarize
what happened in the change and/orexplain why thechange was
made [13, 44]. There isempirical evidence that the use ofcommit
messages iscommonplace in code that ismanaged with version
control systems [13, 30 ,44].
Documentation plays an important role in program comprehen­
sion andsoftware maintenance [17, 49]. As the documentation of
changes, commit messages can help developers understand the ra­
tionales behind changes before they dig into details [20, 23, 44, 64].
Commit messages also provide information tounderstand the evo­
lution ofsoftware [13, 32]. However , due to the lack of direct moti­
vation and time pressure ,writing high -quality commit messages
remains aneglected issue. Dyer et al. report that around 14% of
the commit messages in 23K+ Java SourceForge projects were com­
pletely empty [18].
Many tools have been proposed togenerate commit messages
automatically [13, 16,30 ,38 ]. The commit messages created bythem
canassist developers inwriting high-quality commit messages or
replace empty commit messages. Given the dif fof a change, most
ofthese tools, e.g., DELTADoc [13] andChangeScribe [16, 38], are
able to produce detailed messages which cananswer what was
changed and where this change happened. But their generated
messages areverbose, and fail to reveal therationale behind a
change.
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE '18,September 3-7,2018 ,Montpellier ,France
Itishard toautomatically generate high-quality commit mes­
sages ,since answering why a change happened usually requires syn­
thesis of different kinds of knowledge and context. However ,recent
studies report that commit messages follow some patterns [31,44],
andit ispossible tolearn patterns ofsoftware artifacts from large
datasets [25].Based on these findings ,Jiang et al. proposed the adap­
tion ofaneural machine translation (NMT) technique togenerate
commit messages from chan gediffs [30].In the remainder of the
paper ,forsimplicity sake,we refer to their approach asNMT. NMT
aims to learn how to write commit messages from prior changes and
their commit messages. Different from prior work, NMT focuses on
producing short messages which canreveal therationales behind
chan ges.Jiang etal.reported theperformance oftheir approach
using adataset built from the top IKJava projects inGitHub .
NMT hasmany advantages :(1) In contrast to existing commit
message g eneration methods, NMT produ cesshort summaries in­
stead ofexhaustive descriptions ofchanges. (2)NMT does notre­
quire manually defined templates ,asneeded bymany prior tools,
e.g.,DELTADoc andChangeScribe .(3)NMT cangenerate commit
messages forchanges tomany types ofsoftware artifacts, not only
source code changes .
However, Jiang et al. did notexplore why NMT performs so
well in their paper. Understanding theapplicable scenario ofan
approach can help us apply it in practice. So in this paper ,we first
investigate therationale forNMT's good performance .
Additionally ,NMT isquite complex , and its tr aining process is
very slow .Jianget al. spent 38hours training their NMT model
on an Nvidia GeForce GTX 1070 [ 30].However ,according to the
suggestions of Fu and Menzies [19],it is a good practice to explore
simple and fast techniques before applying deep learning methods
on SE t asks. Therefore ,wewish toinvestigate theconstruction of
amuch simpler and faster approach toaddress the same problem
that issolved byNMT.
Our study aims to answer thefollowing research questions :
RQ1: Why does NMT perform sowell?
Weconduct ananalysis ofthegenerated commit messages by
NMT (using thedata published onJiangetal.s website [1]).We
randomly read 200commits inJiang et al.stestresults ,andmanually
identify those high -quality generated commit messages byNMT
(we call them good messages) .Then, those identified good messages
andtheir corresponding commits arefurther analy zedby us. From
the analysis, we find the codediffs of most of the good messa ges
aresimilar to one or more training dif fsat the token level.
We also find that Jiang et al.'s dataset contains noisy commit
messages ,like messa gesthat areautomatically produced byother
development tools ,e.g.,acontinuous integration (CI) bot named
liferay-continuous -integration ,ormessages that arewritten by hu ­
man butcontain little and redundant information ,e.g.,"update
readme.md ".Such amessage describes neither what was changed
in the readme filenorwhy thechan gehappened, hence contains
little information .Inaddition ,since the information can be obtained
easily bylooking at the list ofchanged files,it is also redundant.
Itmakes little sense totrain and test approaches (e.g.,NMT) for
automated commit message generation onsuch noisy messages .
Therefore ,wemanually derive thepatterns ofsuch noisy commit
messages ,and build a new dataset bydeleting such noisy commit
374Liu,Xia,Hassan ,l,o,Xing ,and Wang
messages and their corresponding diffs from Jiang et al .sdataset.
We re -train NMT on this cleaned dataset andobtain anew model.
Compared to the model trained on the original dataset ,theper­
formance ofthenew model declines by a large amount ,and the
BLEUscore [ 51](anaccuracy measure that is widely used to evalu­
atemachine translation systems) decreases by 55 .5%from 31.92 to
14.19.
RQ2 :Can asimpler and faster approach outperform NMT?
InRQl ,we found that the code dif fsof most of the good me s­
sages are lexically similar to one or more dif fsin the training set.
Inspired by this finding ,wepropose asimpler and faster approach,
named NNGen (Nearest Neighbor Generator) ,toautomatically gen­
erate commit messages from dif fs. Our approach is based on the
nearest neighbor algorithm, and does not require atraining process .
Togenerate a commit message for a newdiff, NNGen first finds
thediff which is most similar to the newdiff, i.e.,thenearest
neighbor ,from the training set.Then it simply outputs thecommit
message of the nearest neighbor as the generated commit message .
Our experimental results show that NNGen outperforms NMT
onJiang et al.'s dataset and thecleaned dataset interms ofBLEU
by a substantial margin. Moreover ,it only takes 23to30seconds to
run NNGen on a CPU instead of24to38hours on a GPU ,which
means that NNG enisover 2,600 times faster than NMT .We also
perform ahuman evaluation to compare NNG enand NMT .Our
evaluation shows that NNG enperforms better than NMT ,and the
improvement issignificant.
Finally ,weconduct afurther analysis ofautomated commit
message generation. Wepoint outthat only diffs and commit
messages arenotenough forthis task. By answering theabove
research questions ,wejust move one step further ,butthere is still
a lon gway to go.
The main contributions ofthiswork are as follows:
(1) We perform anin-depth analysis of the experimental results
in Jiang et al.swork ,and analyze the reasons ofNMT's good
performance.
(2) We propose asimpler andfaster appro achcalled NNG ento
generate short commit messages. NNGen is over 2,600 times
faster than NMT ,andsignificantly outperforms NMT.
The remainder ofthispaper isorganized as follows. Section 2
introduces thebackground ofourstudy. Section 3describes our
experimental settin gs,including research questions and dataset.
Section 4-5details our experiments and the experimental results of
each research question respectively. Section 6discusses thereason
behind NNGen 'sbetter performance ,thecases where NMT out­
performs NNGen ,theimplications ofourstudy and threats to the
validity of our reported findings. Section 7discusses some observa ­
tions for the road ahead forautomated commit message generation .
Section 8surveys therelated work. Section 9concludes thepaper.
2BACKGROUND
2.1 Commit, Diff, Commit Messages
Jiang et al.sdataset isextracted from Git repositories .Git [ 2]is one
of the most popular version control systems. Each time a developer
commits achange ,Git will create a"commit" for this change and
allow thedeveloper towrite atextual message called a"commit
message "to describe the change. A commit in Git contains achan ge
and acommit message (whi chmay beempty). A ch ange can be
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Neural-Machine-Translation-Based Commit Message Generation
represented by adiff,which captures the difference between two
program versions and can be generated using the gitdiffcommand
in Git. In this work, bymentioning acommit, we are referring to
thepair ofa code diff and itscorresponding commit message .
Given acommit, werefer to its original commit message, which
isextracted from the Git repository ,as the reference message, the
produced commit message byNMT as the NMT message and the
produced commit message byNNGen as the NNGen message .
2.2 Jiang etal/s NMT Approach
The NMT model adapted byJiang et al. is the attentional RNN
Encoder-Decoder model [11], which is an extension to the RNN
Encoder-Decoder model [ 14].The RNN Encoder-Decoder model was
originally designed fortranslating between natural languages. There
aretwo parts in this model: the encoder and thedecoder ,each of
which is aRecurrent Neural Network (RNN). Given asource sen­
tence, i.e., a sentence written in the source language, theencoder
reads andencodes it into a fixed-length vector. This vector can be
regarded as the intermediate representation of the source sentence,
andcontains theneeded information fortranslation .The decoder
outputs thetarget sentence from the encoded vector .The encoder
and the decoder arejointly trained using a large number of pairs of
source sentences and target sentences .In the machine translation
community, this kind ofdataset isreferred to as a parallel corpora.
Compared to the RNN Encoder-Decoder model ,theattentional RNN
Encoder-Decoder model introduces theattention mechanism to cope
with long source sentences .
InJiang et al.swork, thesource sentences arediffs ,and the
target sentences arereference messages. The parallel corpora are
collected from GitHub ,which contains pairs ofhistorical dif fs
and thecorresponding reference messages. After training on the
special parallel corpora, Jiang et al.'s model can"translate" a new
dif finto ashort textual description which may summarize the
corresponding change.
2.3 BLEU
To align with Jiang et al., we use the BLEU-4score [ 51]toevaluate
theperformance ofNNGen .The BLEUscore is an accuracy measure,
that iswidely used toassess thequality ofmachine translation
systems [11,14,27,29,50 ]. The score first calculates themodified
n-gram (for BLEU-4 ,n=I,2,3,4) precisions of acandidate sequence
to the reference message ,then measures theaverage modified n­
gram precision with apenalty foroverly short sentences. Inour
case, we regard agenerated commit message (anNMT message or
anNNGen message) as acandidate. Considering the fact that BLEU
aims tomatch human judgment at a corpus level [ 51]andJiang
et al. use a corpus-level BLEU-4 score toevaluate NMT, we also
calculate the BLEU-4 score at the corpus level.
Inaddition ,NNGen leverages the BLEU-4 score internally to
measure thesimilarity between two di ffs.However, it calculates
BLEU -4score at the sentence level.
3EXPERIMENTAL SETUP
3.1 Research Questions
Jiang et al. did notinvestigate why their approach performs so
well. Understanding thereasons isimportant forapplying NMT in
practice. So, first of all, we want toinvestigate:
375ASE '18 ,September 3-7,2018 ,Montpellier ,France
RQl :Why does NMT perform so well ?
Additionally ,NMT isquite complicated and its training process
isvery slow and costly (e.g.,requiring specialized and dedicated
hardware). Simple and fast methods areusually easier to be adopted
inpractice. Fuand Menzies also recommend theexploration of
simple methods first while dealing with SEtasks [19].Therefore ,
wewould like to know:
RQ2:Can a simpler and faster approach outperform NMT?
3.2 Dataset
Since we wish toinvestigate thereason behind NMT's good per­
formance and compare theperformance ofNNGen and NMT, we
simply useJiang et al.'s dataset toconduct ourexperiments. Jiang
et al. have gratefully published their dataset [1].To make our paper
self-contained, we briefly describe thebuilding process ofJiang et
al.sdataset infollowing paragraphs.
Collecting Data: Jiang et al. collected 2Mcommits from the most
starred lKjava projects in GitHub.
Preprocessing: They first extracted the first sentence of each col­
lected commit message .Next, to reduce their vocabulary sizeand
improve theperformance ofNMT ,they removed commit ids from
dif fs,andremoved issue ids from reference messages .Then, they
removed merge commits, rollback commits and commits with a
dif fthat islarger than 1MB. Finally, they broke reference mes­
sages and diff sinto tokens .But they didnotconvert tokens
into lowercase ,and nor did they split the CamelCase tokens. After
preprocessing, 1.8M commits remained .
Filtering: To apply the NMT algorithm, Jiang et al. needed to filter
commits bylength (i.e., the number oftokens in asequence). They
only kept commits with adi f f length ofnomore than 100and
areference message length ofno more than 30.Only 75K commits
meet these length requirements .Inaddition, Jiang et al. introduced
the Verb-Direct Object (V -DO) filter for the reference messages. They
did so because the NMT algorithm performs better on such pattern
ofmessages .Their V-DO filter identifies theVerb-Direct Object
pattern, e.g., "delete a method", through the "dobj "dependency in
theStanford CoreNLP library [40].They removed theextracted
sentences which do not begin with a"dobj" dependency. Jiang et al.
only preserved 32Kfrom the 75K messages that begin with a "dobj"
dependency.
After preprocessing andfiltering, Jiang et al. randomly divided
the32Kcommits into 3 sets, i.e., training set,validation set and test
set. The training setcontains 26K commits. The validation setand
thetestset each contain 3Kcommits.
4RQ1: WHY DOES NMT PERFORM SOWELL?
4.1 Analyzing NMT Messages
Toinvestigate RQl,we closely analyze thegenerated commit mes­
sages byNMT, i.e.,NMT messages. We first randomly select 200
commits from Jiang et al.stest set.Then, the first author and a
master student independently evaluate theNMT messages ofthese
200commits .
InJiang et al.swork, they conducted ahuman study toevaluate
thequality ofNMT messages. Given a commit, human experts were
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE '18,Sept emb er 3 -7,2 018,Montp ellier ,Fran ce Liu,Xia,Hassan ,Lo,Xing,and W a ng
00-1,7 +O,0 00
I
-#Changelog
21-##0.1 (2014-02-20)
3I
4 ;Messag e :
upua recnanqeloq
Message Generated by NM T:
Updated c hangelogI!!ICHANGELOG,mdID E LETEDIFigure 4:Anexample ofabotmessage3
4 4 [subr ep o]
5 5 cm dver=liferay
61- com m i t =2f03e545085c 159d922 fbgeac9 b166ee821
6+ com m i t =c3d68dbcaaa 18c18e76bb46697c52e4d8el
7 7 mode=push
8I_..•...........Message :
..,8.1a te 'modules /apps/foundation /porta l/.
...9...._.93enerated by NM T:
Ignore Update 'modules /apps/foundation /portal/.00-3,7+3,7001 import as ,json, unittest, time,shutil, sys
2 2 sys . pa t h .extend([ '.','..','py' ])
31-import h2o,h2o_cmd
3
~~4~"'=':~4'o~~ssage:
~moduleslappslfo undat ion /portal l.gitrepo ICHANGED I00-1,7+1,700
Figure 3:Asimilar training commit toFigu re2
canseethat without con siderin gthe cas e,messaqe, is identical to
messaqei , andmessage J'sdi f f is similar t othatofmessaqe ;at
the t oken level. Thisfindin gis sur prising since it me ansthateven
using s uch a complicated NMT algor ithm, NMT is still n obetter
than a nearest-neighbor-based recommend er.
Byreading the commits ofthesegood mess ages, we a lso obs erve
that many (37 %)oftheirref erence messages arenoisy. Weidentif y
two ca tego riesforsuch n oisy messages .One category isnam ed
byus as botmessages, whichreferstoreference m essages that are
automatically gene ra ted b y otherdevelopm enttools.The othe r
category,whichwe calltrivia l m essages, repr e sentsref erence mes­
sages that are w rittenbyhum an sbutcontain little and redundant
inform ation thatone caneasilyinfer, for example, b yjustlooking
at the li stof changed file s.
Figure 5:Anexample ofatrivial message
Figure 4shows an e xampl eofabot message. The commit in
Figure4is co llected fr omthe repository ofliferay -portal [6].We
notethattheNMT message isnearly the sameastheref erence
message. However ,after s earchin ginGitHub, w efind that this com­
mitis pushed b y a co ntinuous int egrati on (C I) botnamed liferay­
continuous-i ntegratio n,whichinturn automatically genera tes thi s
ref erence message. Therefore, theref erence message in Fi gure4is a
bot m essage.
Anexample ofatrivia l message is presented in Figure5.NMT
also ge nerates anearly i dentical commit m essage totheref erence26.0%;JGO-r---------------------,
§
840
'0
a;20
~i0
1 import unitt est, random , sy s , tim e
2 2 sys. pa t h .extend([' . ','.. ','py' ] )
3+import h2o_host s
3 4
~L"'~" 5'~~ssage:
Message Genera tedby NM T:
Add h2o_hostso 2 3 4 5 G 7
Scores
Figure 1:Distribution ofthescores ofNMT message s
~py/testd ir _singlejvml tesCp layers _NA.py ICHANGED I
00-1,5 +1,6 00
Figure 2:The commit ofagood message
asked t oreaditsrefere nce m essage andNMT message, andgivea
score between 0 to 7tomeasurethe sema ntic similarity b etween
thetwomessages .A score of 0 m eans thatthetwo m essages have
noth ing i ncommon, andascore of 7means thatthey are id enti cal
in meaning. To g rade th e 200NMT messages, thetworaters care fully
read the sco ring examples p rovided byJian g e tal. [1], andrated
eachNMT messages following Ji an g e tal.sevalua tion criterion.
Wefind ahighlevel o fagree ment b etween thetworater s witha
Cohen' s K app a coe fficient [ 15]of0.67,which shows a substantial
agreement among the d ifferent rater s.Afterrating,the two r aters
discussed theirdisagreem entsto reach co nsensus. F igure1shows
thefinaldistributi onof th e sco res.
Next, we identif ythosehigh-qu ality co mmit m essages ge nerated
byNMT. For si mplicity sake, w e refer tothosemessages as good
messages .To align with Jian g e tal.,weregard theNMT messages
that arescored 6or7 asgood messages .Wefind 35.5%good messages
fromthe200NMT messages ,whichisclosetothe res ultsof Jiang e t
al.shum an study (30.7%good messages) onadifferenttestset.
Afterpickin g outgood messages, wecarefully r eadthesemes­
sages and th eir cor respo nding commits again to t ryto rec ognize
somesimple p attern sin th em.But w edonotfind any ob vious tex­
tualpattern s w hich areshared among allgood messages. Itappears
thatNMT canprodu ce va rious t ypes o f high-quality messages .Con­
sidering th atmachine learningmethod slearn from th etrainin g set
before predictin g,wethen sea rch th etrain ing se t tofind similar
commit m essages for eac hgood messages through i ts keywords,
and tr yto gainsome in sightsfrom the se similar tr aining commit
messages. Wefind th at, fo r nearly eve rygood messages (70out
of71), w e can find outoneor mo retrainin g co mmit m essages
that are nearly identic altothegood message and th ediffsof
thesetrain ing messages ar esimilar to th atofthegood message at
thetoken level. For exa mple ,Figure 2presents atest co mmit ,of
which th eNMT message is agood message .Werefertothisgood
message asmessaqei, Figure 3shows a commit, which i sfound in
thetrainin g set by sea rching th eref erence messages that co ntain
"h2o_hosts". W e call theref erence message in Figure 3messaqe i.We
376
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Neural-Machine-Translation-Based Commit Message Generation ASE '18 ,September 3-7,2018 ,Montpellier ,France
Table 1:Our trivial message patterns Table 3:BLEU-4 scores oforiginal and new NMT models
update changelog /gitignore /readme [md/file]
prepare version [version number]
bump version [version number]Dataset
Original
CleanedBLEU-4
31.92
14.1938.1
24.8pz
31.1
14.6P3
29.5
11.429.7
9.9
modify dockerfile /makefile
update submodule
*"[]"means optional,"]"refers to"or" and version nu mber refers to th e
version number introdu cedinachange.
message .However, both messages only mention that thechangelog
file was updated, and fail to describe what waschanged in detail nor
why thischange occurred. Therefore ,such a message contains little
information. Since a programmer with rudimentary knowledge of
version control systems is able to obtain theinformation byglancing
thename ofthechanged file, the information isoflittle value .
Moreover, this kind of messages can be automatically produced by
some rule-based tools. For example ,we can write ascript toparse
a new change .If the change only modified the changelog file of the
project ,ourscript will first extract thefilename of the changed file,
then simply output "update filename" .
From these examples we can see that there is little useful infor­
mation involved inthese two categories ofmessages .Moreover,
both botmessages and trivial messages can be generated through
rule-based methods (e.g., liferay-continuous-integration or a simple
script). Therefore, itmakes little sense tolearn from orproduce
these two kinds ofmessages through machine learning methods.
4.2 Evaluating NMT ontheCleaned Dataset
Based on the discovery of noisy messages, a question emerges in our
mind: if we deleted such noisy messages and their corresponding
diffs (i.e.,thenoisy commits) from Jiang et al.'s dataset and re­
trained NMT on the new dataset ,how much would the performance
ofNMT beaffected ?
Inorder toanswer theabove question, we first build anew
dataset byremoving thenoisy commits from Jiang et al.'s dataset.
To delete such noisy commits ,we need to automatically identify bot
messages and trivial messages. Forbot messages, weonly find the
messages that aregenerated bylijeray-continuous-integration. Since
all such messages follow the same pattern, which is "ignore update
filename ",it is easy to identify them through aregular expression .
However, there aremore than one types oftrivial messages. To
identify them, we manually derive some common patterns oftrivial
messages byskimming thecommits in Jiang et al.'s dataset. Table 1
presents ourtrivial message patterns .The exact regular expressions
are available in our online appendix [3].Table 2shows thepropor­
tions of ouridentified bot messages and trivial messages inJiang et
al.sdataset. We can see that bot messages and trivial messages are
common inJiang et aI.'s dataset.
After identifying bot messages and trivial messages ,their corre­
sponding commits areregarded as noisy commits .We build the new
training set,validation setand testset by deleting noisy commits
from thetraining set,validation setand test setofJiang et al.'sdataset, respectively .Please note we do notclaim that wehave
found anddeleted allnoisy commits inJiang et aI. 'sdataset. Only
those commits ofwhich thereference messages match ourextracted
patterns arecleaned by us.
Then were-train and test NMT on the cleaned dataset, the BLEU­
4 score is computed toevaluate the new model, just likeJiang et al.
The experimental results areshown in Table 3.Since the dataset,
theimplementation ofNMT and the training and testscripts used
by us are provided by Jiang et aI., we simply use the results that are
reported intheir work [30]forperformance comparison .We can
see from Table 3that theperformance ofthenew model declines
by a large amount. The BLEU-4 score ofthenew model is 55 .5%
lower than theoriginal model. These results show that thegood
performance ofNMT mainly comes from those noisy commits in
Jiang et al.'s dataset.
Insummary, after an in-depth analysis ofNMT messages, we find
that(1)Thediffs of most (70 out of71 in our randomly selected
test set) good messages aresimilar to one or more training diffs
at the token level. (2) About 16%ofJiang et al.'s commits arenoisy
commits .(3) The performance ofNMT declines by a large amount
after removing such noisy commits.
NNGen leverages thenearest neighbor (NN) algorithm toproduce
commit messages. The NNalgorithm is a lazy learning method
which issimple and does notrequire atraining phase .Just like
NMT, ourapproach takes as input anew diffand a training set,
and outputs aone-sentence commit message for the new diff .
Our approach first extracts dif fsfrom the training set. Next, the
training di f f sand the new dif farerepresented asvectors in the
form of "bags of words" [ 41].In abag-of-words model, the grammar
and theword order ofadiffareignored, only term frequencies
are kept. We refer to this kind of vector as adiffvector. Then, NNGen
calculates thecosine similarity between thenew diffvector and
each training diffvector, and selects the top k training dif fswith
highest similarity scores. After that, the BLEU-4score between the
new di f f and each ofthetop-k training diffsarecomputed.
The training diffwith thehighest BLEU-4 score isregarded as
thenearest neighbor of the new diff.Finally ,ourapproach simply
outputs thereference message ofthenearest neighbor as the final
result. In summary, given a new diff, ourapproach will first find5.1 NNGenNMT leverages thecomplex ,slow and resource-consuming NMT
algorithm togenerate commit messages. Inspired by our first finding
in RQ1, we propose anearest-neighbor-based approach, named
NNGen, which is simpler and faster than NMT while outperforming
NMT interms ofBLEU-4 score onJiang etal.s dataset and the
cleaned dataset.5 RQ2: CAN ASIMPLER AND FASTER
APPROACH OUTPERFORM NMT?"Driginal" r efers tothe original data setprovided byJiangeta1.'s.
"Cleane d "refer s totheclean ed data set. The BLE U-4 sco res ar ecalculated
on the whole t est se t.Pn(0=1,2,3,4) refers tothemodified n-gram pre­
cision.
15.6%
16.0%16.3%All trivial messages
2.9%3.1%
3.2%Bot Messages
13.4%12.6%
12.8%Table 2:Proportions ofidentified messages.
Dataset
Original Validation
Original TestOriginal Training
377
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE'18,Septe mbe r 3-7,2018 ,Mon tpel lier ,France
Table4:BLEU -4scores ofNMT and NNGen
Dataset Approac hBLEU-4 PI pz P3 P4
Orig ina lNMT 31.92 38.1 31.1 29 .5 29.7
NNGe n 38.55 44.6 37.4 36.3 37 .2
CleanedNMT 14.19 24.8 14.6 11.4 9.9
NNGen 16.42 27 .6 16.813.4 11.8
"IheBLEU-4scoresarecalculate donthetest se ts.Pn(0=1,2,3,4 )referstothemodified
n-gramprecision.
Table 5: Time costs ofNMT and NNGen
Dataset Appro achDevice Training Time Testing Time
NMT GTX1070 38hours 4.5mins
Original NMT GTX 1080 34hours 17mins
NNGen CPU N/A 30sees
CleanedNMT GTX 1080 24hours 13mins
NNGen CPU N/A 23sees
*GTX1070 an dGTX1080 referto Nv idiaGeForce GTX 1070and1080,respectivel y. CPU i s
Intel Coreis2.5GHz.
itsnearest neighbor inthetraining set,then reuse thereference
message oftheneares tneighbor as thegenera tedmessage forthe
newdif f,
Asdescrib edin Sec tion2.3, BLEU score [51]is apopular and
automated metric forevaluating thequalityof mac hine t ranslation.
Itcan also be used to meas ure t he sim ilarity between twosente nces.
Compared tocosine similar ity,theBLEU score takes into account
theorder ofwords .However ,thecompu tatio nofBLEU score is
relative ly slow. To speed up o urapproach ,we do not find t heneares t
neigh bor by calculating BLEU sco resfor al l thetraining di.ffs.
Instead,we firs t usethecosine similarity between diffvectorsto
find the k nearest neig hbor candidates. Then, thebest candida teis
selected accor dingto BLEU scores .This strategy balances the time
cost andaccuracy ofNNGen .Bydefau lt,we se tk as 5 .
5.2Automati cEvaluation
We eva luate theNNGen onJiang et al.sdataset andthecleaned
dataset (described in4.2)using the corp us-level BLEU -4score, and
compare the test res ultsofNNGen with th ose o fNMT. The test
results ofNMT on Jia ngetal.sdatase t arerepor tedin[30],and we
directly usethem for performance comparison .To eva lua te NMT
on thecleaned datase t ,we use theimplement ation ofNMT and
thetraining and test scripts which areprovided onJiang et al.s
websi te [1],thentrain an d testNMT usingan Nv idiaGeForce GTX
1080 wi th8GB memory .
Table 4prese ntstheevaluation resultsforNMT and NNGen ,we
can seethatour NNGe napproac houtperforms NMT interms of
BLEU-4score oneach dataset. TheBLEU improvemen tsachieved
byourapproa chrange from 16 %to 21%.Moreover ,Allthe modified
n-gram precis ions (PI-P4in Ta ble4)ofourapproac harehigher
thanthose o fNMT.
To investigate whe therNNGen isfaster thanNMT ,we meas ure
thetime costs ofNNG enonJiang etal.s datasetand thecleaned
dataset, and compare them withthetime cos tsofNMT .The time
costs ofNMT on Jiang et al.sdatase tisprovided by Jia ngetal.
in[30].Jiangetal.conductedthetrainingand testofNMT on a n
Nvidia GeForce GTX 1070GPU wit h8GB memory .Whileevaluating
NMT ,wecond uctthetraining and testing processes on an Nvidia
GeForce GTX 1080 GPU wi th8GB memory .However ,NNGen is
evaluated onaCPU ( Intel Core is 2 .5GH z)with8GB RAM .Table5
378Liu,Xia,Hassa n,l,o,Xing ,andWang
~jOOQ/src/main/jav a/ org/jooq/impI/ CurrentUser.java ICHANGEOI
00-64,6 +64,7 <iX\I"'~la~s"'C~'r'~e'ntuser extends AbstractFunction<String> {
64 case POSTGRES:
64
65 65
66 66
67
67 68
68 69 Message :
69 7@,rtfortheFirebird da tabase - Fixe dCURRENT _USER ()
Generated Message 1:
Addsupport f ortheFirebird database -Fixedmulti -record INSERT
Genera tedMessage 2 :
Add@Implement ation to Shadow Application .CheckPermis sion()
Score o fGenerated Message 1:
ScoreofGenerated M essage 2 :-----
Figure 6:Aquestion inoursurvey
Score 2:
Definition: T womessages have so mesimilarinformatio n, buteachofthem co ntai ns
some informa tion w hich isnotmen tioned bythe ot her.
Example :
reference m essage :"reduce the heap to 1 4 for je nkins "
generated m essage :"increased h ea p 5 ->10"
Explanat ion: The two messages alltalkabout th emodification o f heap size, but one
mentions thedecrease o f heap s ize, th e other m en tions t heincrease.
Figure 7:Apart ofourscoring criterion
shows thetime costs o fNMT and NNGe n.Forcomparison, we a lso
presen tthetime costsofNMT on Jiang et al .sdatase tusing our
server .We can see that ittakes 24 to 38 hour s to train NMT and 4.5
to17 minutes t otest on t hetwo datasets .Since NNGe ndoes not
need training ,itstraining time is marked as"N/A". The time cost of
itstesting processes isonly 23to 33 seconds .Thismeans NNGe nis
cons iderab ly(more than 2,600 times) fas ter th anNMT onthe two
datasets .
5.3Human Evaluation
We also co nduct ahum an eva luation toevaluate NNGen andcom­
pare NNGen withNMT .We inv ite6 Ph .D.studen ts toparticipa te
in oursurvey ,allofwhom arenotco-au thors ,majorincompu ter
science and haveindustria lexperience inJava pro grammi ng(rang ­
ingfrom 1-4years) .Eachpartic ipan tisasked toread 100commits
and assess thesemantic similari tiesbetween reference messages and
each of com mit m essagesgene ra ted byNNG enandNMT .
5.3.7Procedure. We ra ndomly select200commits from the cleaned
datase t(described in Sec tion 4.2),divide them even lyintotwo
groupsand make a questionnaire foreach group.Inourquestion­
naires ,each questio nfirstprese nts t he info rma tion of onecommit,
i.e.,itsdif f,itsrefere ncemessage, itsNMT message and i tsNNG en
message, thenasks participa ntsto givetwo sco resbetween0 to
4 to measure thesemantic similarities between therefer ence mes­
sage and thetwogenera tedmessages. Score 0means thereis no
similari tybetwee n the two messages, and score4means two mes­
sages are identica linmeaning. Figure 6shows one q uestion in our
survey .Participants aretoldthat the first message isthereference
message,buttheorder of the NMT message andtheNNGen message
is random lydecided. So,participa ntsdonotknow which message
isgenera tedbywhic happroa ch,and they are asked to en terto
score each generated message separately .
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Neural-Machin e-Tran slati on-Based Co mmit M essage Ge nera tion ASE'18, Se pte mber 3-7,2018,Montp ellier ,Fran ce
124
89O,.iginal TestDiffs Random Test Ditt s
repart Level ='low'
saurce Sets=[s au rceSets.mainltaalVersian ='3.0.1'
effort =lmax'124
l25 125
l26 126
127 +
l27 128l28 129 Mes
sage :
l29 130 gson main o nly
MessageGeneratedby NM T:
Add ultim ateandroiduicomponent proje ct in d emoofui
89
90 90 findbu9s {
91 91 ignareFailures =false
92+ saurceSets=[s au rceSets. mai nl
92 93
93 94 Messa ge :
q4 qe; rdbuq sfor main@@-89,6 +89,7 @€Isubprojects {
Figure10:Thenearest neighbor found byNNGen
compute th e cos inesimilarities betwee neach testdiffvector and
each trainingdi./fvector .Finally ,for eac h testdiff, we se lect th e
trainin gdiff with thehighest c osine similarity score,and record
this sco re.There are3,000 co mmits in Ji an g e t al.'stest se t,hen ce
we o btain a setof3,000cosine similari ty scores.
Toreducethepotentialvariance cause d b yhowJian g e tal.s
dataset isdivided ,we a lsoperform a10-fold c ross-validation. Specif­
ically ,wefirst shuffle th e 32,208commits in Jian g e taI.'sdat aset
and divide these c ommits i ntoto fold s.Then we runthe same p ro­
cedureasthe above e xperim enttotime s. Eac h time o ne foldof
3,220(3,228 forthelastrun) commits i sused as thetest se t,and
theremainin gfolds o f28,988 (28,980 forthelastrun) commits i s
used as the tr aining se t. We obtain a setof 32 ,208 cosinesimilarity
scores from th eto-fold c ross-va lidation.
We visualize th edistribut ion o feachsetofcosinesimilarity
scores u sing a v iolin pl ot[26],as show n in F igur e 8.Theleft p lot is
thedistributi on o f thescores ofJiang etal.'stest se t,and th e right
oneisthedistribution o f the sco resobtained from th e10-fold c ross­
validation .The visualization r esults show th atthedistribution o fMessage Generat edby NNGen :
Only run findbu gsformain
Figure 9: A test commit
EIbuild.grad leICH AN G E DI~build.gradleICHANGED I
00-124,6 +124,7 @1lfindbugs {Figure8:Distribution ofthecosine similarities between test
diffs andtheirneare stneighbor s0.40.9
~.!0.8
0.5E
Vi0.7
~c
.~0.6
uNNGe n 57.9% 1 4.3% 2 7.8% 1.46Approac h Lo w Me dium H igh Mea nScore
NMT 63.8% 8.8% 27.4% 1.34Table 6:The results ofour userstudy
6DISCUSSION
6.1 Why Does NNGen Perform Better?
Given anewdiff ,NNGe nfirst find sthediff which is most
similar t oitatthetoken level fro mthetrainin g set,then simply
outputs th e commit m essage o f thetrainin gdiff asthe generated
commit m essage .Hence ,we s pec ulate th atthe reas onofNNGe n's
betterperforman ce is that given atest co mmit ,there is a high
chance th atthere w illalways exista verysimilar tr ainin g commit
to it.
To ve rify ourconjecture, we co nduct anexpe rime nt on Jian g
etal.'sdataset. We firstconve rt alldiffs in thetrainin g setand
thetest se tintodi./fvectors (descr ibed inSection 5.1).Then ,we"Low", "Medium" and"High "refer tolow-qualit y,medium -qualit yand hi gh­
qualitymessages,respective ly.
Eachcommit group i s eva lua ted b y 3participants.Ourscoring
criterion islisted i n thebeginn ing o feach que stionnaire to guide
participants.Figure7presents apartofourscoring criterion. Our
complete scoring criterion can be found in ouronlin e appendix [ 3].
Inadditio n, Participa nts areallowe d t o sea rch th eInternet for re­
lated inform ation .
Different fromJiang e tal.shum an study, fir st,we selectcommits
from thecleaned dataset i nstea d ofJiang e tal.'sdataset, si nceit is
meanin gless toevalua te the commit me ssages ge nera ted for noisy
commits. Seco nd, th e score ra ngeofoursur vey i s 0-4 instead o f 0-7.
Alarge score range re quires our parti cipant s tospend m ore e fforts
disting uishing subtle sema ntic differe nces ,butinthiswork , we
care more a bou t th e rough qu alit y o fgenera ted m essages instead
of subtle semantic d ifferences. Inaddition ,a 5-point scale is widely
used i nprior softwareeng in eeringstudies [ 34, 35 ,62,63].Third ,
we a lso p rovide diff sinourquestionnaires tohelppart icip ants
maketheir judgments
5.3.2Results.We obtain 600pairsofscores from ourhum an eva l­
uation. Eachpair co ntains a sco refor the NMT message anda score
for the correspo nding NNGe n message. Weregard a sc oreof 0 a nd 1
aslowquality,a score o f2asmedium quality a nda sco reof3and 4
ashigh quality.T able 6present s the res ults of our user study. W e can
seethattheproportion of high -quality NNGe nmessages is slightly
higher thanthatof hi gh-quality NMT messages. 14.3%of theNNGe n
messages areofmedium quality ,while forNMT messages ,thepro ­
portion is only8.8%.Thenumber oflow-qu alit y NNGe nmess ages
is smallerthanthat of low-qualit yNMT messages. Moreover ,the
mean sco reofNNGen mess ages ishigher thanthat of NMT messages.
These results sh ow t ha tNNGe noutpe rforms NMT.We also u se a
Wilc oxon s igned-rank t est[60]ata 95%significance l evel to check
whe the r th eperfor ma nce differe nces between NNGen andNMT
aresignifican t. The p- value is0.01,which me an stheimpr ovement
achieve d b yNNGe nis significant.
Insummary ,NNGe nismuch simpler andfaster th anNMT.Our
experimental r esult s s how th atNNGen outperforms NMTin terms
of BLEU-4 score on Jian g e tal.sdataset a nd th eclean ed dataset .
Inaddition, ourhuman evaluation shows th atNNGen outpe rforms
NMT, and th eperform ance impr ovement is statistically significant.
379
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE '18,September 3-7,2018 ,Montpellier ,France Liu,Xia,Hassan ,l,o,Xing ,and Wang
46
[§pom .xmI ICHANGEO IFigure 11:Anexample where NMT performs betterMessage Generated by NNGen :
Remo vedDoubleCheckedLocking froQ).,checkst yle.xml. ""<m odu lename ="E m pt ySt at em e nt"/ ><m odu lename ="A r rayTypeSt yl e "/ >
<m odu lename ="M i ss i ngO ve r ri de" / >
<groupl d> com . zaxxe r</ groupl d>
<art i fac t l d> H i ka r i C P</ art ifact l d>
<ve rs i on> 1. 3. 8 - SN A PS HO T</ve rs i on>@@-6,7 +6,7 @@
6
7
1-7
846
47 47
48 48
49_ Message :
49. _.~0Final izechecksty lerule
Message Generated byNMT:
Added Defa ultComesLast checkstyle rule
Figure 12:Another example where NMT performs better
are similar in the meaning, while compared to this NNGen message,
this NMT message ismore specific andaccurate. Therefore, NMT
performs better in this case. After further searching in the training
set,we find that there exist some training commit messages which
share the same pattern as this NMT message .For example, a training
commit message is"prepare release HikariCP-1.3 .2".This means
NMT has the ability togeneralize, but its generalization ability is
very limited sothat its overall performance is worse than our simple
approach ,i.e.,NNGen.
Due to the space constraint ,more details of the aforementioned
examples can be found inouronline appendix [3].8+ <ve rs i on> 1.3.8</ve rsi on>
9 .9 Message :
10 10 ease HikariCP - 1 .3 . 8
Message Generated by NMT:
Prepare release HikariCP - 1. 3 . 9
Message Generateo oyNNG~en~:------------- """
~repa refor next develo~ ""m1Be",nt",it",e",ra",tio""n.'- .....[§build/checkst yle.xml lCHANGEO I
@'!-46,6 +46,7 @:£l
6.3 Implications
From this work, we distill some general suggestions which isbeyond
the specific task andapproaches .
Clean upthedata carefully. Insoftware repositories ,there may
exist some noisy data, e.g .,the noisy commits inJiang et al.'s dataset.
Itmakes little sense totrain and test our models on the noisy
data. Worse still,wemay getmisleading results if we conduct
experiments ondataset with noisy examples [21].Therefore, we
recommend researchers toalways clean up their datasets carefully
before training and testing their models .
Consider simple approaches first. Our study shows that it is
worth trying simple and fast methods before applying complicated
and time-consuming techniques onsoftware engineering tasks.
This "try-with-simpler" practice is also recommended by Fu and
Menzies [19].Implementing and applying simple methods only
costs a little effort, but may bring a deep understanding of the data .
Moreover, for some SE tasks, simple approaches are able to achieve
equal or even better performance in less time ,e.g.,NNGen vsNMT .each setofscores ishighly skewed. Specifically ,foreach set, the
cosine similarities between most test dif fsandtheir most similar
training dif fsarehigher than 0.7.Hence ,themajority oftest
commits arevery similar toanother training commit at the token
level.
We also manual examine some examples from Jiang et al .s
dataset toexplain thebetter performance ofNNGen. Figure 9and10
show oneofsuch examples .Figure 9presents atest commit, its
NMT message and its NNGen message. Itsnearest neighbor found
byNNGEN isshown inFigure 10. We can see that thediffs of
thetwo commits aresimilar atthetoken level, and their refer­
ence messages areidentical inmeaning. By finding out this nearest
neighbor, NNGen produces ahigh-quality commit message for this
testcommit. However, thecommit message generated byNMT is
notrelevant to this testcommit at all, which means NMT fails to
generate agood commit message.
Insummary ,weargue that given atestcommit ,there is ahigh
chance that there will exist a training commit which is very similar
to it,and forthose testcommits that arevery similar toanother
training commit, NNGen cangenerate better commit messages than
NMT.
6.2 Where Does NMT Perform Better?
Although theoverall performance ofNNGen isbetter than that
ofNMT on the two datasets ,in some cases ,NMT messages obtain
higher scores than their corresponding NNGen messages. To figure
outsuch cases, we compare theaverage scores ofeach NMT mes­
sage anditscorresponding NNGen message, and find there are 30
commits where NMT performs better than NNGen .
Wemanually analyze these 30commits and their generated
commit messages. We find that for 20 outofthe 30 commits, the
NMT messages and NNGen messages are all oflow quality (i.e.,
Score 0 or 1). In each case,themeanings ofthereference message
and the two generated messages aredifferent ,butNMT generates
theright verb at the beginning oftheNMT message. Therefore,
theaverage scores ofthe 20 NMT messages are all 1, and those
ofthecorresponding NNGen messages are all O.Forexample, the
reference message ofatestcommit is "Add AbstractProcessingFil­
ter.getAuthenticationDetailsSourceO ",itsNMT message is "Added
getter forauthoritiesPopulator" ,and its NNGen message is"Properly
handle empty layout ingetFirstVisiblePositionO" .We can see that
themeanings of the three messages are different ,butNMTgenerates
thecorrect verb "add "at the beginning. Thus ,thisNMT message is
better than thisNNGen message .
As for the other 10commits ,we find that without considering
the case ,theNMT messages of9commits areidentical to one or
more training commit messages. In these cases, the reason ofNMT' s
better performance may be that NMT captures better nearest neigh­
bors than NNGen. Figure 11shows anexample. After comparing
thetraining diffsofthenearest neighbors captured byNMT
andNNGen with this test dif f,we find that although thetraining
dif fselected byNNGen is lexically more similar to this test diff,
themeaning and thewriting style ofthis NMT message iscloser
to this reference message. Thus ,this NMT message obtains higher
average score.
There is also a special case where theNMT message can not be
found in the training set, as shown in Figure 12. The three messages
380
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Neural-Machine-Translation-Based Commit Message Generation ASE '18,September 3-7,2018 ,Montpellier ,France
1049
Figure 13:Anexample ofaunique commit .~src!com/ google/javascri pVjscom p/Com piler.java lCflANGEOI
@@..!1049 , 6 +1049,7 @@public class Compiler extends AbstractCompiler {
~library /srclandroidTest/assetslt sisample.ts.O.dump ICH ANGEDI
00-54,7 +54,7 00track 257:
encoderPadding =-1
subsampleOffsetUs =9223372036854775807
selection Flags =0
language =null
language =und54
57+54
55 55
56 56
57
58 58
59 59 Message :
-;';'···-:.:;::'.ctortests1049
l50 1050
l511051
1052
~~~ ._.1_~5_~ Message:
'53 1054 move references to the orig inalComp ilerln putafter the hot -swap is done.
Figure 14:Another example ofaunique commit.
isused toextract data from the MPEG-2 TS container format [8].
This information isproject-specific knowledge. Without thetoken
"Tslixtractor", the reference message would fail to reveal the specific
rationale behind thiscommit. Therefore ,thistoken isessential.
These twoexamples highlight that there exists some information
which cannot befound in the dif fsandprior historical commit
messages, yet such information isessential forhigh-quality commit
messages. Hence, only dif fsandhistorical commit messages are
notenough forcommit messages generation. This task requires
synthesis ofinformation from different data sources.
Take more types ofinformation into consideration. Accord ­
ing to the above finding, we recommend theinclusion ofother types
ofinformation when designing new approaches forcommit mes­
sage generation .We have showed two types ofinformation which
is useful for commit message generation ,i.e.,context information
ofdiffs (Figure 13)and project-specific knowledge (Figure 14).
Tocapture theformer one, we cansimply extend dif fs to con­
tain more lines ofcode, or leverage program analysis (for code
changes) and semantic analysis techniques toextract data depen­
dencies and type information from context ofdiffs(e.g.,the lines
before and after diffs, the code which diffs aredependent on,
etc.) and add extracted information into ourdataset. Tocapture
project-specific knowledge, other types ofdocuments insoftware
repositories can be used while training andgenerating .For example,
ifanew commit aims to fixa bug, the corresponding bug report
may provide important context information of the bug. In addition ,
We can also build aknowledge base (e .g.,Knowledge Graph) for
each project andincorporate these knowledge bases with machine
learning methods .
Westill have along way togo.Weshowed that if an approach
only takes dif fsandprior historical commit messages asinput ,
itcannot generate perfect commit messages forunique commits.
Assume that there was an approach, which performed perfectly
on the test commits that arenotunique commits ,andperformed
likeNMT on the unique commits. Specifically ,for the test commits
without unique tokens ,thisperfect approach would generate the6.4 Threats toValidity
One ofthethreats tovalidity isabout themanual inspection in
Section 4. We ask two raters toevaluate thequality of200 randomly
selected NMT messages according toJiang et al.scriterion. We
cannot guarantee that ourjudgments are fully in line with the
results ofJiang et al.'s human study .However ,ourscores are only
leveraged toidentify high quality commit messages generated by
NMT, and are not used for performance comparison. Additionally ,
theproportion ofhigh quality commit messages found by us is
close to (a little higher than) that observed inJiang et al.shuman
evaluation on a different testset,which makes us more confident
about ourjudgments.
The second threat tovalidity isabout theevaluation ofNMT
on the cleaned dataset. To build the cleaned dataset ,weremove
about 16%ofcommits from Jiang etal.sdataset. The reduction of
thedataset may be one ofthereasons ofthedecrease ofNMT's
BLED score on the cleaned dataset.
Another threat tovalidity isthat weonly compare NNGen and
NMT onJiang etal,'sdataset and thecleaned dataset. On the two
datasets ,NNGen outperforms NMT by a substantial margin. But
we do not claim that this finding can be generalized to all datasets.
Weonly stress that compared toNMT, thesimplicity, thespeed
and theeffectiveness ofNNGen make it a competitive baseline on
dif f-natural language translation task.
7ROAD AHEAD FOR COMMIT MESSAGE
GENERATION
Based onourabove mentioned findings, wenow present some
observations about the road ahead for commit message generation .
Only diffs and historical commit messages arenotenough
forcommit message generation. Both NMT and NNGen only
takediffs andhistorical commit messages asinput ,sothey can­
notgenerate tokens thatare not contained in the training set and the
testdiffs, but only appear in the test reference messages .We call
these tokens unique tokens, andrefer to the test commits ofwhich
thereference messages contain at least one unique token asunique
commits. To figure outtheamount ofunique tokens inJiang et al.'s
dataset ,we first tokenize all the dif fsand reference messages,
convert alltokens into lowercase, and remove thetokens which
arenumbers. Then, webuild two vocabularies, one ofwhich is
constructed from the tokens in the training set and the test diffs,
and the other vocabulary is formed from only the tokens in the test
reference messages. Finally ,wecompare the two vocabularies .We
find that there are 296 unique tokens inJiang et al.'s dataset, and 6 %
(180 out of 3 ,000) of the test commits areunique commits. Figure 13
and 14show two unique commits inJiang et al.'s dataset.
Figure 13presents acommit in the closure-compiler project [5].
The token "Cornpilerlnput" is a unique token .After searching in
GitHub and reading more lines before the dif f,we find that "Com­
pilerlnput "is the type ofthevariable "oldlnput'' (also the element
type ofthe list "inputs"). The token "Compilerlnput" provides the
type information which isnotcontained in the diff, yet it plays
animportant role in describing thiscommit.
Figure 14isextracted from the repository ofExoPlayer [4].The
token "TsExtractor" is a unique token. Byreading thedocumentation
ofExoPlayer, we find that TsExtractor is a class ofExoplayer, and
381
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE '18, September 3-7,2018 ,Montpellier ,France
same messages astheir reference messages. For unique commits,
thisperfect approach would generate messages that areidentical
totheir NMT messages. The BLED score ofthisperfect approach
onJiang eta1.'sdataset would be 92 .94.But now, the BLED scores
ofNMT and NNGen areonly 31.92 and 38.55, respectively. The
performance differences between thisperfect approach and NMT
and NNGen areconsiderable. Moreover, after deleting thenoisy
commits inJiang etal.sdataset, theperformance of both NMT and
NNGen declines, which shows that the two approaches are still not
powerful enough .Inaddition, it isworth mentioning thatJiang et
a1.'sdataset represents only 1.75% outof2million commits that are
collected from GitHub .The performance ofNMT and NNGen on
big-scale datasets is still an open question. Therefore, there is still
a long way to go to automatically generate commit messages.
8 RELATED WORK
8.1 Commit Message Generation
Anumber oftechniques have been proposed toautomatically gen­
erate commit messages. Some ofthem take source code changes
asinput. Forinstance, DELTADoc [13]obtains path predicates by
symbolically executing source code changes ,then generates com­
mitmessages using a set ofpredefined rules and transformations .
ChangeScribe [16, 38] first extracts thestereotype, the type and the
impact setofacommit byanalyzing corresponding source code
changes and the abstract syntax trees. Then it fills predefined tem­
plates with theextracted information todocument this commit.
The approach proposed byShen eta1.issimilar toChangeScribe ,
butitconstrains thelength ofthegenerated message byremov­
ingrepeated information in the change [55].Le et a1.proposed a
framework to infer the semantic differences between twoversions
ofa code base through dynamic analysis [36].Commit messages
that aregenerated bythese tools areusually verbose, and cannot
describe theintent ofcommits concisely.
Several approaches make use of various documents insoftware
repositories toproduce commit messages .Forexample, toanswer
why achange happened, Rastkar andMurphy proposed anapproach
toextract motivational information ofcommits from multiple rel­
evant documents [53]. Moreno eta1.have built ARENA [ 46,47],
atool which combines multiple kinds ofchanges, i.e.,changes to
source code, libraries, documentation and licenses, with issues from
software repositories togenerate release notes. Hassan and Holt
proposed anapproach, named Source Sticky Notes, tobetter ex­
plain the static dependencies of asoftware system using historical
modification records [24].
Inaddition, Huang eta1.proposed anapproach toproduce com­
mitmessages for code changes byreusing thecommit messages of
similar existing commits [28].The similarity between twocommits
ismeasured bysyntactic similarity andsemantic similarity between
their changed code fragments. Their approach only focuses on code
changes. However, we aims to generate commit messages from
diffs,which includes both code changes andnon-code changes .
Wecannot analyze the code syntax ofnon-code changes .Moreover,
we find that inJiang et a1.'s dataset, over 70% ofthechanges are
non-code changes. Hence, Huang et al.sapproach only solves part
ofthisproblem.
382Liu,Xia,Hassan ,l,o,Xing ,and Wang
8.2 Source Code Summarization
Source code summarization techniques may also be used togen­
erate commit messages. Most of the techniques adapt a two phase
framework tosummarize source code. Specifically, they first se­
lect important content from source code, then transform the se­
lected content into natural language descriptions through prede­
fined rules .For example, to summarize Java method, theframework
proposed bySridhara eta1.first identifies significant statements of
a Java method according tostructural andlinguistic clues, then ex­
presses extracted content innatural language using predefined text
templates [56].This framework hasbeen extended tosummarize
high -level actions within methods [57],crosscutting source code
concern [54],Java classes [ 45, 48], C++ methods [9],context of]ava
methods [42,43]andobject-related statement sequences [59].Some
studies also leverage this framework togenerate explanation or
summarization for special types of code, e.g., exceptions [12],failed
tests [ 65],parameters ofJava methods [58],unit test cases [ 33, 37 ]
anddatabase usages andconstrains [39].
Some work leverages information retrieval techniques tosumma­
rize source code [ 10, 22, 61 ,62].For example, Wong et a1.proposed
amethod which mines code-description mappings from StackOver­
flow [ 7]andautomatically generates code comments byreusing the
descriptions of similar code snippets in the extracted database [62].
Inaddition ,machine translation techniques are also applied to
produce source code summaries [29,52].CODE-NN [ 29],proposed
by Iyer et al., leverages an NMT algorithm togenerate descriptions
for C# and SQL code .Phan eta1.adapted phrase-based statisti­
calmachine translation totranslate between behavioral exception
documentation andsource code of API methods [52].
9CONCLUSION
Automatically generating high-quality commit messages is a chal ­
lenging task. Recently, Jiang eta1.proposed leveraging an NMT
algorithm togenerate one-sentence commit messages from diffs.
Their approach (NMT) learns from historical data, summarizes com­
mits using one-sentence messages and shows promising results.
However, they do not investigate why NMT performs so well, and
NMT is quite complicated and time-consuming .
In this paper, we first analyze Jiang eta1.'sexperimental results .
We find that there arenoisy commit messages intheir dataset ,
andthat thegood performance ofNMT benefits from those noisy
commit messages. Then, inspired byourfindings, wepropose a
simple ,nearest-neighbor-based approach, named NNGen, togener­
ateshort commit messages from diffs. Our experimental results
show that NNGen ismuch faster and performs better than NMT
onJiang etal.s dataset and thecleaned dataset. Finally, we con­
duct a further analysis ofcommit message generation, anddiscuss
some challenges in the road ahead for this task toinspire other
researchers .
ACKNOWLEDGEMENT
Xin Xia andXinyu Wang are the corresponding authors. This re­
search was partially supported by the National Key Research and
Development Program ofChina (2018YFB1003904) andNSFC Pro­
gram (No. 61602403) .
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Neura l-Mac hine-Translation -Based Commit Message Generation
REFERENCES
[1] 2017. Ji an g e tal.swebsite .https:llsjian gl.github.i o/connnit gen/.
[2]2018.Git.https: llgil-scm.com /.
[3] 2 018. Our online appendix. https:llgoo.gIl63B976 .
[4] 2018.ExoPlayer. https:llgithub .com /google /ExoPlayer.
[5]2018.Goo gle C losure Compiler .https:llgithub .com/ google /closure -compiler .
[6] 2018. Life rayPortal . https:llgithub .com/lifera y /lifer ay-portal .
[7] 2 018.Stack O verflo w. https :llstackoverflow .com/.
[8]2018.TsExtractor inExoPlayer. https:llgoo.gIlD sbdjf .
[9]Nahl a] Abid ,Nata lia Dr agan ,Michael LCollard,and Jonathan1Maletic .2015.
Usingstereo ty pes in th e auto ma tic gene ra tion of natural lan gu age s ummaries
forc++ method s.InSo f tware Maint enan ce and Evolution (I CSME), 2015IEEE
Internati ona l Conference on .IEEE,561-565.
[10] Miltiadi s Allaman is ,EarlT Barr, Ch ristia n Bird , andCha rles Sutto n .2015.Sug­
gesting accur ate method and classnam es.InProceedin gs ofthe 2015 10th Joint
Meeting on Foundati ons ofSof twa re Engineering. ACM,38-49 .
[II] Dzmitr yBahdan au ,Kyunghyun Cho,andYos hua Ben gio . 2 014. Neura l ma­
chine tr an slation byjointl ylearnin gtoalign andtranslat e. arXiv preprint
arXiv :1409.0473 ( 2014).
[12]Raymond PL Bu se and W estle y R W eim er. 2 008. Auto matic d ocum entati on
infer en ce forexce ptions. In Proceedings ofthe 2008 int ernati ona l s y mpos ium on
Software t esting and a nalys is .ACM ,273- 282.
[13]Raymond PL B useand Wes tley R W eim er .2010 .Au tomatically do cum entin g
program changes . InProceedings oftheIEEE/ACM internat ional c onfe rence on
Automated software engineering .ACM,33-42.
[14] K yunghyun Cho,Bart V anMerrienboer ,CaglarGulceh re ,Dzmitr yBahdan au ,
Fethi Bougares,Holger Sc hwe nk,and Yo shu a Ben gio. 2 014.Learnin gphra se
representations using RNNencoder-decoder for statistical ma chine translation .
arXiv preprint arXi v:1406.1078 ( 2014).
[15]Jacob C ohe n .1968. Weighted kapp a:Nomina l scale agreem entprovision for
scaled d isagr eement orpartial credit. Psycholog ical bulletin 70,4 (196 8), 2 13.
[16] Lu isFernand o Co rtes-C oy,Mari oLinare s-Vasqu ez ,jairo Aponte ,and Den ys
Poshyvan yk. 2 014.OnAutomatically Gen erating Commit Mess ages viaSumma ­
rization ofSource Code Changes. InIEEEInternation alWork ing C onfe rence on
Source Code A nalys is andManipul ati on. 275- 284.
[17] S ergio Cozze tti B d e Souza ,NicolasAnquetil ,andKathia M d eOliveira .2005. A
study of thedocumentati on ess ential to softwa re maintenan ce.InProcee dings of
the 23rd annual international conferenceon Design of communication:documenting
&designin g for pervasive inf ormati on.ACM ,68-75 .
[18]Robert Dyer,Hoan A nhNguye n, Hr idesh Rajan , a nd Ti en N Ng uye n .2013 .Boa: A
languageand infra structureforanalyzing ultra-large-scale so ftwa re rep ositorie s.
InProceedings ofthe 2013 Int ernati onal Conf erence on Sof tware Engineering .IEEE
Press,422-43 I.
[19] W ei Fuand Tim Menzies .2017.Easy over har d: a case s tudy on deeplearnin g.In
Joint Meeting on F ound ati ons ofSof tware Engin eering. 49-60.
[20] YingFu,Men gYan,Xiaohon g Zhang ,LingXu,Dan Yan g,andJeffrey DKyrn er,
2015.Automated cla ssification ofsoftw are change m essages by semi-superv ised
LatentDirichl et A llocatio n. Informa tion a ndSof tware T echnolo gy57(2015) ,
369-377.
[21] Baljind er G ho tra ,Sha ne McI nt osh ,and Ahm ed E Hassan .2015.Revisitin g theim­
pactof cl assificati on techniqu eson th eperformanc eof defect prediction model s.
InProceedings ofthe 37th Int ernat ional Co nference on Soft ware Engi neering-Volume
1.IEEE Pr ess,789-800.
[22] S on ia H aidu c,jairo Apo nte, Laura Moreno ,andAndri anMarcu s.2010 .On th e
useof autom ated text s ummarization t echniqu es forsummarizing source code.
InReverse E ngineering (WCRE), 2010 1 7thWorkin g Conf erence on.IEEE,35-44.
[23] A hmed E Ha ssan. 2 008. Automa ted cla ssification ofcha nge m essages inopen
source p rojects .InProceedings o f the 2008ACMsymposi um on A pplied comput ing.
ACM,837-841.
[24] A hme d E Hassan and RichardCHolt.2004. U sin g developm enthistory sticky
notestounderstand softwarearchite cture .InProgram Comp rehens ion, 2004.
Procee dings. 1 2thIEEE Internati onal Workshop on .IEEE,183-1 92.
[25] A bra m Hindl e, Ea rl T B ar r,Zhendong Su,Mark Ga bel,and Pr emkum arDevanbu.
2012.On th enaturalne ss ofsoftware. In Sof twa re Engineering (ICSE),201234th
Internati onal Conf e rence on. IEEE,837-847.
[26] J erryLHint zeand RayDNelson .1998.Violin plots:a bo xplot-den sit y trace
syne rgism. TheAmerican S tatistician 52,2 (199 8),181-1 84.
[27] X ing Hu ,GeLi,XinXia,David Lo,and Zhi Jin.2018.Deep c ode com me nt
gene ration. InProceedings ofthe26thConference on ProgramComp rehe nsion,
ICPC2018, Gothe nburg ,Sweden ,May 27-28 ,2018. 200 -210. https:lldoLor g /l0.
1145/3196321.3196334
[28] Yuan Hu an g,Qiaoyan g Z heng ,Xiangpin g Che n,Yingfei Xiong,Zhiyong Liu ,and
Xiaon an Luo.2017.Minin gVersion Co ntrol S ystem for Automatically G ener atin g
Comm it Comment. InEmpirica l Sof twa re Enginee ring and Measurement (ESE M),
2017 A CM/IEEE Internati onal Sy mposi um on.IEEE ,414-4 23.
[29] S rinivasan Iy er,Ioanni sKonstas,Alvin Che u ng ,and Luke Zettlemoy er. 2016 .
Summarizing source code u sing a neur alattention mod el.InProceedings of the
383[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]
[38]
[39]
[40]
[41]
[42]
[43]
[44]
[45]
[46]
[47]
[48]
[49]
[50]
[51]ASE'18,Septe mber 3-7,2018 ,Montpe llier,Fra nce
54th Annu alMeeti ng oftheAssociati on fo rCom putational Lin guistics ( Volume 1:
Long Papers) ,Vol. I.2073-2083.
SiyuanJian g,Ameer Arma ly ,and Collin McM illan. 2017 .Automa tically generat­
ing commit messages from diffsusing neur al machin e tran slation. InProceedings
of the 32nd IEEE/ACMInternational Conference on Automated So ft ware Enginee ring .
IEEE Pr ess,135-146.
Siyuan Jiangand Collin M cMillan .2017 .Toward s automa tic generation ofshort
summ aries ofcommits. In Proc eedings ofthe25thInternat ional C onfe rence on
Program Compr ehension.IEEEPress,320-323.
MiraKajk o-Matt sson ,2005.A sur vey of doc umentation pr actice w ithin corrective
maintenanc e.Empirical Sof twa re Engineering 10,1(2005) ,31-55.
Manabu Kamimura and Gail CMurph y.2013.Towards genera ting human ­
orient ed summa ries ofunit testcases .InProgram Comprehension (ICPC), 2 013
IEEE 21stInternatio nalConference on. IEEE,215-218.
Pavneet Sing h Ko chhar ,Xin Xi a,DavidLo,andShanping Li .2016 .Practition ers'
expect ation son aut omat edfault localization.InProceeding softhe25thInt erna­
tional Sy mposiu m on So f twa re T esting an d Analysis .ACM,165-176.
Jan-Pet er Kram er,JoelBrandt ,andJan Borch er s. 2016.Usin gruntim etraces
to impro vedocum entation and unit te stauthorin gfor d ynamic languages. In
Proceedings ofthe 2016CHIConference onHuman Factors inComputin g Syste ms.
ACM,3232 -3237 .
Tien -Duy B Le,Jooyon g Yi,David Lo,Ferdian Thun g,and Abhik Roychoudhur y.
2014 .Dyn amic infer en ce ofchange contra ct s. InSo f tware Maint enan ce and
Evoluti on ( ICSME), 2014 IEEE Int ernati onal Conferenceon.IEEE,451- 455 .
BoyangLi,Ch ristophe r V endome ,Mario Linar es-Va sque z,Den ysPosh yvanyk,
and NicholasAKraft .2016. Automati cally documentin gunit testcases. InSof t­
ware Tes ting,Verification a nd Validati on (ICST) ,2016 I EEE Internationa l Confe rence
on.IEEE ,341-352.
Mari oLinare s-Va sque z,LuisFernand o Cortes-C oy,jairo Aponte,and Den ys
Posh yvan yk. 2 015. Changes cribe: Atool for automatically gene ra ting commit
messages. InSof tware Engineering (ICSE), 2015 IE EE/A CM 37 th IEEE International
Conference on,Vol. 2 .IEEE,709-712.
Mario Linares-Vasque z, BoyangLi,Christopher Vendo me ,andDen ysPosh y­
vany k. 2016 .Documentin gdatab aseusages and schema cons traints in d ataba se­
centri cappli cation s.InProceedings ofthe 25th Int ernational Sy mpos ium on Sof t­
wareTesting a nd A nalysis. ACM ,270-281.
Christophe r M annin g,MihaiSurd eanu ,John Bauer,Jenny Finkel ,Steve n Betha rd ,
and Da vidMcClosky.2014 .The Stan ford CoreNLP n atural languageproce ssing
toolkit. InProceedings o f 52nd a nn ual m eeting ofthe associa tion f or co mputa tional
lingui stics: sys tem demon strati ons. 5 5-60.
Christophe r D M annin g,Prabh akar Ragh avan ,Hinri ch Sc hiitze ,etal.2008 .Intro­
duction to info rma tion retrieva l. Vol.I.Cam bridge univ ersity press Ca mbridge .
PaulW M cBurne y andCollin M cMill an. 2014. Automat icdocumentation gen­
eration viasour ce code summarization ofmethod context. In Proceedings ofthe
22ndInternati ona l Conference on P rogram Comprehension.ACM,279-290.
Paul W M cBurne y andCollinMcMillan.2016.Automa tic sourc e code sum ma­
rization ofcontext forjava method s.IEEETran sactions on Sof tware Engineering
42,2(2016) ,103-119 .
Audri s Mockus and L awren ceG Votta . 2000. Identif yin g Reason sfor Soft ware
Cha nges u sin g Historic Databa ses.Inicsm. 120-1 30.
Laura Moreno,jairo Aponte ,Giripra sad S ridhara ,Andrian Mar cus, Lori Po llock,
and K Vijay -Sha nker, 2013. A utomatic genera tion ofnaturallan guage s umma ries
forjava classes. InProgram Compr ehension(ICPC) ,2013IEEE21stInternati ona l
Conference o n.IEEE,23-32.
Laura Moreno,Gabriel eBavota ,Massimiliano Di P ent a ,RoccoOliveto ,Andrian
Marcu s, and G erard o Ca nfora. 2014 .Automa tic genera tion ofrelea senote s.In
Proceedings o f the 22nd A CM S IGSOFT Int ernati onal S y mposium on Foundati onsof
Software Engineering .ACM,484-495 .
Laura Moreno,Gabriel eBavota ,Massimilian oDi P ent a, RoccoOliveto ,Andrian
Marcu s,and Gerardo Ca nfora. 201 7.AREN A:anapproa ch for the auto ma te d
gene ra tion ofrelease notes.IEEE Tran saction sonSof tware Engi neering 43,2
(2017) ,106-127.
Laura Moreno,Andrian Marcu s,Lori Pollock ,and K Vijay-Shan ker ,2013.jsum­
mari zer:Anautomatic genera tor of natural langu age summar ies for ja vaclasses.
InProgram Comprehe nsion (I CPC) ,2013 I EEE 2 1st Int ernational Confe rence on .
IEEE ,230- 232 .
David GNovick and K aren Ward. 2006 .Whatusers s ay th eywant in docum ent a­
tion.InProceedings o f the 24thannual A CM in terna tional conference onDesign o f
com munication .ACM,84-91.
Yusuk e Oda ,Hiro yuki Fudaba ,Graham Neubi g,Hideaki Hata,Sakr iani Sakti,
Tomoki Toda ,and SatoshiNakam ura .2015.Learnin gtogenera te p seudo -cod e
from so urce code using s tatistica lmachin e tran slati on (t).InAutomated So ft ware
Engi neering (ASE), 2015 30th IE EE/A CM Internati onal Conference on .IEEE,574­
584.
Kishore Papin eni,SalimRouko s,Todd Ward ,and W ei-lin gZhu.2002 .BLEU: a
method forautomatic e valuation ofmachine translation. InProceedin gsofthe
40thannual meeting onassociati on forcomp utational lin guistics. Association for
Compnt ational Lingui stic s ,311-3 18.
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE '18,Septembe r3-7,2018 ,Montpe llier,France
[52] Hung Ph an,Hoan A nhNguyen, TienN Ng uyen, and Hridesh R ajan. 2017.Sta­
tisticallearni ng forinference b etween im plementa tions and do cumen tation.In
Sof tware Engineering: New Ideas a ndEmerg ing T echnologies Results Track ( ICSE­
NIER),2017IEEEIACM39t h International Conference on.IEEE, 27-30.
[53]Sarah R astkar a ndGailC Murphy. 2013.Why did thiscodechange? In Proceedings
of the 2013International Confere nceon So f tware Enginee ring. IEEE Press, 1193­
1196 .
[54]Sarah Ra stkar , GailC M urphy, and Alexander W] Bradl ey.2011. Gener ating
natur allanguage summar iesforcrossc utt ingsource code con cerns. InSoftware
Maintenance ( ICSM), 201127th IEEE International Conference on.IEEE, 103-112.
[55]]infeng Shen, X iaobing Sun,BinLi, Hu i Yang,an d ]iajun Hu. 2016.OnAutomat ic
Summariza tion of WhatandWhyInformation in Source CodeChanges. In Com­
puterSof tware andApplications Conference (COMPSAC), 2016IEEE 40thAnnual,
Vol.I.IEEE, 103-112 .
[56]Giriprasa d Sridhara, Emily Hill ,Divya M uppaneni, Lori P ollock, and K Vijay­
Shan ker. 2010. Towards a utoma tically gene rating summary commentsforjava
method s. InProcee dings oftheIEEE IACM international conference on Automa ted
software engineering. ACM, 43- 52.
[57]Giriprasad Sridhara, Lo ri Pollock , and K Vijay-Shanker. 2011. Automatically
detecting a nd de scribing highlevelactions within me thods.InPro ceedings of t he
33rd International Con ference on So ftware Engineer ing.ACM ,101-110.
[58]Girip rasad Sridhara, Lori Po llock, andKVijay-Shanker. 2011. Generating para m­
eter commen tsand in tegrating w ith me thod su mmaries. InPro gram Com prehen­
sion(ICPC),2011IEEE 19th International Confere nceon.IEEE, 71-80.
384Liu,Xia,Hassa n,Lo,Xing,andWang
[59]Xiaoran Wang, L ori Po llock,and K Vijay-Shanker, 2017.Automatica llygenerating
natura l l anguage descrip tions for object-re latedstatement sequences. I nSoft ware
Analysis, Evolution and R eengineering (SANER), 2017 IEEE 24 thInternatio nal
Confe rence on.IEEE, 205-216.
[60] Frank Wilcoxon. 1945.Individua lcomparisons b yrank ingmethod s.Biometrics
bulletin 1, 6 (1945 ),80-83.
[61] Edmund W ong, Taiyue L iu,andLin Tan.2015. Clocom: Mining existingsource
code for a utom atic commen tgenera tion. In Sof tware Analys is,Evolutionand
Reengi neering (SANER), 2015IEEE 22nd International Conference on.IEEE, 380­
389.
[62] Edmund Wong, ] inqiu Yang,and Li nTan.2013. Autoco mmen t:Mining qu estion
and an swer si tes forautom atic com ment genera tion. InAutomated So f tware
Engineering (ASE), 2013IEEEIACM 28thInternational Con ference on.IEEE, 562­
567.
[63]XinXia, Lingfeng Ba o, DavidLo,Pavneet S ingh Koch har,Ahmed E Hass an,and
Zhenc hang X ing. 2017.Whatdo developers sea rch fo ron th e we b?Empirical
Software Engineering 22,6(2017), 3149-3 185.
[64] MengYan,YingFu,Xiaohong Zhang, Dan Yang, Ling Xu,and ]e ffrey 0 Kymer.
2016. Automatica llyclassifying softwa rechanges via discrimi native top ic model:
Supporti ng multi-category and cross-project. Journalof Systems a nd So ft ware
113 (2016), 296 - 308.
[65]Sai Z hang, Cheng Zhang, andMichael 0 Erns t.2011.Automa ted documen tation
inference t o explain fa iled t ests. InProceedings of t he201126th IEEEIAC MInter­
nationalConf erence onAutomated Sof tware Engineering. IEEE C omputer Society,
63-72.
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. 