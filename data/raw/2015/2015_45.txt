Jens Knoop, Uwe Zdun (Hrsg.): Software Engineering 2016,
Lecture Notes in Informatics (LNI), Gesellschaft f ¨ur Informatik, Bonn 2016 45
NoPAIN, No Gain?
The Utility of PArallel Fault INjections
Stefan Winter1OliverSchwahn1Roberto Natella2Neeraj Suri1Domenico Cotroneo2
Abstract: The article reports on interferences between concurrent fault injection test executions.
Keywords: Software fault injection, robustness testing, test interference
Software Fault Injection (SFI) emulates defects in software components to assess the ro-
bustness of other software components the yinteract with. After such defects ha vebeen
introduced, the software composition is exposed to aworkload and the effects of the intro-
duced defect on the component under assessment are monitored. SFI tests entail relati vely
long execution times for three reasons: (1) The yoperate on afully integrated software
system, which entails corresponding loading and initialization times. (2) Toactivate the
introduced defects and assess their impact on possibly comple xcomponent interactions,
theexecution of comple xworkloads is required. (3) After each SFI test execution, the
entire software system under test (SUT) needs to be reset to aknown fault-free state to
prevent residual side effects of injected faults from affecting subsequent tests. Especially
the last point entails signiﬁcant execution time overhead. As the possible effects of an in-
jection cannot be predicted (if the ycould, no SFI tests were needed), resetting the SUT
usually requires acomplete termination and re-initialization, sometimes even of its execu-
tion environment (e.g., the test machine’ sﬁle system) if it can be affected by the injected
fault.
Toimpro vetest throughput, we propose to exploit parallel hardware and execute SFI tests
concurrently .While this appears to be asimple and straight-forward solution, it is based
on an assumption of non-interference between SFI tests. In apaper [Wi15] that we pre-
sented at ICSE this year ,w ee xperimentally evaluated this assumption. Weexecuted SFI
tests on the Android OS kernel by injecting faults into the SD card dri ver.Tocontain the
effects of fault acti vations during these tests, the system wasexecuted in an emulator that
wasreset after each test. Werepeated the tests with varying degrees of concurrenc yb y
instantiating varying numbers of emulator instances. Toassess, whether concurrenc yhas
an effect on the experiment outcome, we compared the result distributions for the varying
degrees of concurrenc y.Our initial results showed signiﬁcant deviations for higher degrees
of concurrenc y,indicating that an unreﬂected replication of SUT instances threatens the
validity of test results. Weidentiﬁed the SUT instances’ competition for shared system
resources and the resulting execution latenc yincreases, which directly affected some of
1Technische Uni versit¨at Darmstadt, DEEDS Group, Hochschulstr .10, 64289 Darmstadt, German y,
{sw|os|suri}@cs.tu-darmstadt.de
2Federico II Uni versity of Naples, DIETI, via Claudio 21, 80125 Naples, Italy,
{roberto.natella |cotroneo}@unina.it46 S. Winter,O.Schwahn, R. Natella, N. Suri, D. Cotroneo
the employed test oracles, as the root cause for the observed deviations. Wethen devised
apre-test measurement approach to adjust these oracles for the concurrent execution of a
givennumber of SUT instances on agiven test machine. Using this approach, we were able
toexecute up to 44 SFI tests concurrently without an ysigniﬁcant test result deviations on
amachine with 16 CPU cores and 64 GiB main memory .The highest throughput for this
conﬁguration was157experiments per hour with 36 concurrent instances, amore than 12-
fold throughput increase compared to sequential test execution. Forthis conﬁguration we
also observed the lowest correlation between the degree of concurrenc yand the test result
distribution in a χ2test for independence, which indicates that the initially observed result
deviations were indeed caused by performance interference of concurrent test executions.
Besides the direct impact of our result on SFI and other robustness testing approaches,
where performance sensiti veoracles are used to detect so-called hang failures, our result
indicates that test parallelization requires careful analysis to obtain valid results when-
ever tests rely on execution latencies .Forexample, an yJUnit tests that use the timeout
parameter or Timeout rulewould be similarly affected. An interesting observation from
ourexperiments wasthat the SUT initialization contributed signiﬁcantly to the observed
test latencies. This is not surprising, as we used heavy-weight isolation measures to make
testexecutions as independent as possible from each other by running them in (almost,
as our results show) completely isolated environments. This opens up the possibility for a
trade-off: Performance interference decreases with less isolation, which on the other hand
increases the risk for other types of test interference [Zh14].
While our pre-test measurement approach pro vedeffectivefor time-dependent oracle ad-
justment, it required the execution of around 800 tests for reliable predictions. Our goal
is to reduce this calibration overhead and, ideally ,devise an analytical model for accurate
predictions of safetime-dependent oracles and achie vable concurrenc ydegrees for agiven
test type and test machine conﬁguration, that do not even require additional test executions
for calibration. Toachie vethis, we need to better understand the root causes behind latency
increases. Wehope the related research to also shed some light on the factors that caused
throughput to degrade when more than 36 concurrent SUT replica were instantiated in our
experiments and to guide hardware and scheduler conﬁguration for better test throughput.
Acknowledgments: This research has been supported in part by DFG GRK 1362, CASED,
EC-SPRIDE, EC H2020 #644579, CECRIS FP7 (GA no. 324334), and SVEVIA MIUR
(PON02 00485 3487758).
References
[Wi15] Winter,Stefan; Schwahn, Oli ver; Natella, Roberto; Suri, Neeraj; Cotroneo, Domenico: No
PAIN, No Gain?: The Utility of PArallel Fault INjections. In: Proceedings of the 37th
International Conference on Software Engineering -Volume 1. ICSE ’15, IEEE Press,
Piscata way,NJ, USA, pp. 494–505, 2015.
[Zh14] Zhang, Sai; Jalali, Darioush; Wuttke, Jochen; Mus ¸lu, Ki vanc¸;Lam, Wing; Ernst,
Michael D.; Notkin, David: Empirically Revisiting the Test Independence Assumption. In:
Proceedings of the 2014 International Symposium on Software Testing and Analysis. ISSTA
2014, ACM, Ne wYork, NY ,USA, pp. 385–396, 2014.