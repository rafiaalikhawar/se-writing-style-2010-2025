Noise and Heterogeneity in Historical Build Data
An Empirical Study of Travis CI
Keheliya Gallaba
McGill University
Montr√©al, Canada
keheliya.gallaba@mail.mcgill.caChristian Macho
University of Klagenfurt
Klagenfurt, Austria
christian.macho@aau.at
Martin Pinzger
University of Klagenfurt
Klagenfurt, Austria
martin.pinzger@aau.atShane McIntosh
McGill University
Montr√©al, Canada
shane.mcintosh@mcgill.ca
ABSTRACT
Automated builds, which may pass or fail, provide feedback to a
development team about changes to the codebase. A passing build
indicatesthatthechangecompilescleanlyandtests(continueto)
pass.Afailing(a.k.a.,broken)buildindicatesthatthereareissues
that require attention. Without a closer analysis of the nature of
buildoutcomedata,practitionersandresearchersarelikelytomake
twocriticalassumptions:(1)buildresultsarenotnoisy;however,
passing builds may contain failing or skipped jobs that are actively
or passively ignored; and (2) builds are equal; however, builds vary
in terms of the number of jobs and configurations.
Toinvestigatethedegreetowhichtheseassumptionsaboutbuild
breakagehold,weperformanempiricalstudyof3.7millionbuild
jobsspanning1,276opensourceprojects.Wefindthat:(1)12%of
passingbuildshaveanactivelyignoredfailure;(2)9%ofbuildshave
a misleading or incorrect outcome on average; and (3) at least 44%
of the broken builds contain passing jobs, i.e., the breakage is local
to a subset of build variants. Like other software archives, build
data is noisy and complex. Analysis of build data requires nuance.
CCS CONCEPTS
‚Ä¢Software and its engineering ‚ÜíSoftware verification and
validation; Software post-development issues ;
KEYWORDS
Automated Builds, Build Breakage, Continuous Integration
ACM Reference Format:
KeheliyaGallaba,ChristianMacho,MartinPinzger,andShaneMcIntosh.
2018. Noise and Heterogeneity in Historical Build Data: An Empirical
StudyofTravisCI.In Proceedingsofthe201833rdACM/IEEEInternational
Conference on Automated Software Engineering (ASE ‚Äô18), September 3‚Äì
7, 2018, Montpellier, France. ACM, New York, NY, USA, 11pages.https:
//doi.org/10.1145/3238147.3238171
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
¬© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.32381711 INTRODUCTION
Aftermakingsourcecode changes,developersexecuteautomated
buildstochecktheimpactonthesoftwareproduct.Thesebuilds
are triggered while features are being developed, when changes
have been submitted for peer review, and/or prior to integration
into the software project‚Äôs version control system.
Tools such as Travis CI facilitate the practice of Continuous
Integration (CI), where code changes are downloaded regularly
ontodedicatedserverstobecompiledandtested[ 1].Thepopularity
of development platforms such as GitHub and CI services such
as Travis CI have made the data about automated builds from a
plethora of open source projects readily available for analysis.
Characterizing build outcome data will help software practition-
ers and researchers when building tools and proposing techniques
to solve software engineering problems. For example, Rausch et
al.[18]identifiedthemostcommonbreakagetypesin14Javaap-
plications and Vassallo et al. [ 26] compared breakages from 349
opensourceJavaprojectstothoseofafinancialorganization.While
these studies make important observations, understanding the nu-
ances and complexities of build outcome data has not received suf-
ficientattention bysoftwareengineering researchers.Earlywork
by Zolfagharinia et al. [ 29] shows that build failures in the Perl
project tend to be time- and platform-sensitive, suggesting that
interpretation of build outcome data is not straightforward.
Tosupportinterpretationofbuildoutcomedata,inthispaper,
we set out to characterize build outcome data according to two
harmful assumptions that one may make. To do so, we conduct
anempiricalstudyof3,702,071buildresultsspanning1,276open
source projects that use the Travis CI service.
Noise.First, one may assume that build outcomes are free of noise.
However,wefindthatinpractice,somebuildsthataremarkedas
successfulcontainbreakagesthatneedattentionyetareignored.For
example,developersmaylabelplatformsintheirTravisCIconfigu-rations as
allow_failure to enable experimentation with support
for a new platform. Theexpectation is that once platform support
has stabilized, developers will remove allow_failure ; however,
thisisnotalwaysthecase.Forexample,the zdavatz/spreadsheet1
projecthashadthe allow_failure featureenabledfortheentire
lifetimeoftheproject(fiveyears).Exampleslikethissuggestthat
noise is likely present in build outcome data.
1https://github.com/zdavatz/spreadsheet
87
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Keheliya Gallaba, Christian Macho, Martin Pinzger, and Shane McIntosh
There are also builds that are marked as broken that do not
receive the immediate attention of the development team. It is
unlikelythatsuchbrokenbuildsareasdistractingfordevelopment
teams as one may assume. For example, we find that on average,
two in every three breakages are stale, i.e., occur multiple times in
aproject‚Äôsbuildhistory.Toquantifytheamountofnoiseinbuild
outcome data, we propose an adapted signal-to-noise ratio.
Heterogeneity. Second,one mayassume thatbuilds are homoge-
neous. However, builds vary in terms of the number of executed
jobsandthenumberofsupportedbuild-timeconfigurations.For
example,iftheTravisCIconfigurationincludesfourRubyversions
and three Java versions to be tested, twelve jobs will be created
per build because 4 √ó3 combinations are possible. Zolfagharinia et
al.[29]observedthatautomatedbuildsforPerlpackagereleases
take place on a median of 22 environments and seven operating
systems.Buildsalsovaryintermsofthetypeofcontributor.Indeed,buildoutcomeandteamresponsemaydifferdependingontherole
of the contributor (core, peripheral).
In this paper, we study build heterogeneity according to ma-
trixbreakagepurity,breakagereasons,andcontributortype.Wefind that (1) environment-specific breakages are as common as
environment-agnostic breakages; (2) the reasons for breakage vary
and can be classified into five categories and 24 subcategories; and
(3) broken builds that are caused by core contributors tend to be
fixed sooner than those of peripheral contributors.
Take-away messages. Build outcome data is noisy and hetero-
geneous in practice. If build outcomes are treated as the ground
truth, this noise will likely impact subsequent analyses. Therefore,
researchersshouldfilteroutnoiseinbuildoutcomedatabeforecon-ductingfurtheranalyses.Moreover,tooldevelopersandresearchers
whodevelopandproposesolutionsbasedonbuildoutcomedata
need to take the heterogeneity of builds into account.
In summary, this paper makes the following contributions:
‚Ä¢Anempiricalstudyofnoiseandheterogeneityofbuildbreak-
age in a large sample of Travis CI builds.
‚Ä¢A replication package containing Travis CI specificationfiles, metadata, build logs at the job level, and our data ex-
traction and analysis scripts.2
‚Ä¢A taxonomy of breakage types that builds upon prior work.
Paper organization. Theremainderofthepaperisorganizedas
follows:Section 2describestheresearchmethodology.Sections 3
and4present our findings related to noise in build outcome and
buildheterogeneity,respectively.Section 5discussesthebroader
implicationsofourstudyfortheresearchandtoolbuildingcommu-
nities.Section 6outlinesthethreatstovalidity.Section 7surveys
related work. Finally, Section 8concludes the paper.
2 STUDY DESIGN
Inthissection,wedescribeourrationaleforselectingthecorpusof
studiedsystemsandourapproachtoanalyzethislargecorpusof
builddata,whichfollowsMockus‚Äôfour-stepprocedure[ 17]formin-
ing software data. Figure 1provides an overview of our approach.
2https://github.com/software-rebels/bbchch2.1 Corpus of Candidate Systems
We conductthis studyby usingopenly available projectmetadata
andbuildresultsof GitHubprojectsthatusetheTravisCIservice
to automate their builds. GitHub is the world‚Äôs largest hosting
serviceofopensourcesoftware,witharound20millionusersand57
million repositories, in 2017.3A recent analysis shows that Travis
CI is the most popular CI service among projects on GitHub.4
2.2 Retrieve Raw Data
We begin by retrieving the TravisTorrent dataset [ 3], which con-
tainsbuildoutcomedatafromGitHubprojectsthatusetheTravisCIservice.Asofourretrieval,theTravisTorrentdatasetcontains
data about 3,702,595 buildjobs that belong to 680,209 buildsspan-
ning 1,283 GitHub projects. Those builds include one to 252 build
jobs (median of 3). In addition to build-related data, the Travis-
TorrentdatasetcontainsdetailsabouttheGitHubactivitythat
triggeredeachbuild.Forexample,everybuildincludesacommit
hash(areferencetothebuildtriggeringactivityinits Gitreposi-
tory), the amount of churn in the revision, the number of modified
files,andtheprogramminglanguageoftheproject.TravisTorrent
also includes the number of executed and passed tests.
TravisTorrentalonedoesnotsatisfyalloftherequirementsof
ouranalysis.SinceTravisTorrentinfersthebuildjoboutcomeby
parsingtherawlog,itisunabletodetecttheoutcomeof794,334
jobs(21.45%).Furthermore,TravisTorrentprovidesasingle bro-
kencategory,whereasTravisCIrecordsbuildbreakageinthree
different categories (see Subsection 2.4).
To satisfy our additional data requirements, we complement
the TravisTorrent dataset by extracting additional data from the
RESTAPIthatisprovidedbyTravisCI.FromtheAPI,wecollect
the CI specification (i.e., .travis.yml file) used by Travis CI to
create each build job and the outcome of each build job. To enable
furtheranalysisofbuildbreakages,wealsodownloadtheplain-text
logs of each build job in the TravisTorrent dataset.
2.3 Clean and Process Raw Data
Since we focus on build breakages, we filter away projects that do
not have any broken builds. This excludes from our analysis toy
projectsthathaveconfiguredCIinitiallybutdonotuseCIservices.
1,276 projects (out of 1,283) survive this filter.
Weobservethat996buildlogsdonotparsecleanly.Whenretriev-
ingtheselogs,theTravisCIAPIreturnedatruncatedorinvalid
response. We also filter these logs out of our analysis; however, we
donotethatthese996logsaccountforanegligibleproportionof
the sample of analyzed build logs (996/3,702,595 = 0.03%).
2.4 Construct Meaningful Metrics
Inthissubsection,wefirstdefinetheTravisCIconceptsthatare
useful for understanding our work. Then, we define the metrics
that we use to operationalize the study dimensions.
Core Concepts in Travis CI. In this paper, we adhere to the
terminology as defined in the official Travis CI documentation.5
3https://github.com/blog/2345-celebrating-nine-years-of-github-with-an-
anniversary-sale
4https://github.com/blog/2463-github-welcomes-all-ci-tools
5https://docs.travis-ci.com/user/for-beginners/
88
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Noise and Heterogeneity in Historical Build Data ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France

	


















 !
"










			
	

		

	

	
				
	

 
	!"

 
	
#
"$
	
"$

"$




		
			
Figure 1: An overview of the approach we followed for data analysis.
Ajobis anautomated process that clonesa particular revision
ofaGitrepositoryintoa(virtual)environmentandthencarriesout
a series of tasks, such as compiling the code and executing tests.
Eachjobiscomprisedofthreemain phases:install,script,and
deploy.Eachphasemaybeprecededbyabeforesub-phaseorfol-
lowed by an after sub-phase. These sub-phases are often used to
ensurethatallofthepre-conditionsaresatisfiedbeforethemain
phase is executed (before install, before script, before deploy ), and
allofthepost-conditionsaremetafterexecutingthemainphase
commands (after success, after failure, after deploy ).
Abuildis comprised of jobs. For example, a build can have
multiplejobs,eachofwhichteststheprojectwithadifferentvariant
ofthedevelopmentorruntimeenvironment.Onceallofthejobs
in the build are finished, the build is also finished.
For each job, Travis CI reports one of four outcomes:
‚Ä¢Passed.The project was built successfully and passed all
tests. All phases terminate with an exit code of zero.
‚Ä¢Errored. If any of the commands that are specified in the
before_install, install, orbefore_script phases of the build
lifecycle terminate with a non-zero exit code, the build is
labelled as errored and stops immediately.
‚Ä¢Failed.Ifacommandinthe scriptphaseterminateswitha
non-zero exit code, the build is labelled as failed, but execu-
tion continues with the after_failure phase.
‚Ä¢Cancelled. A Travis CI user with sufficient permissions
can abort the build using the web interface or the API. Such
builds are labelled as cancelled.
Projectsthat use theTravis CIserviceinform Travis CIabout
how build jobs are to be executed using a .travis.yml configura-
tionfile.Thepropertiesthataresetinthisconfigurationfilespecify
which revisions will initiate builds, how the build environments
are to be configured for executing builds, and how different teams
orteammembersshouldbenotifiedabouttheoutcomeofthebuild.
Furthermore, the configuration file specifies which tools are re-
quired during the build process and the order in which these tools
need to be executed.
Metrics. Based on the above concepts, we define seven metrics
to analyze build breakage. These metrics are not intended to be
complete, but instead provide a starting point for inspecting build
breakage for suspicious entries that future work can build upon.
Our initial set of metrics belong to two dimensions.‚Ä¢Build noise metrics. In this dimension, we compute the
rateatwhichbuildbreakageis activelyignored andpassively
ignored.Inaddition,wemeasurethe stalenessofeachbroken
build, i.e., the rate at which breakages are recurring. Finally,
we compute the Signal-To-Noise Ratio (SNR) to measure the
proportionofnoiseinbuildoutcomedatacausedbypassively
and actively ignored build breakage.
‚Ä¢Build heterogeneity metrics. Inthisdimension,foreach
broken build, we compute the matrix breakage purity and
classifybrokenbuildsbythe rootcause.Forpracticalreasons,
weextractrootcausesforbuildbreakagefromthe67,267jobs
that use the Maven build tool. This allows us to build upon
the MavenLog Analyzer[ 15], whichcan classifyfive types
and24subtypesofMavenbuildbreakage.Finally,weclassify
each of the version control revisions that are associated
with each build, according to contributor type (i.e., core or
peripheral contributors).
2.5 Analyze and Present Results
Using the metrics that we define in Section 3.4, we (1) plot their
valuesusingbarcharts,linegraphs,scatterplots,andbeanplots[ 10];
and (2) conduct statistical analyses using Spearman‚Äôs œÅ, Wilcoxon
signed rank tests, and Cliff‚Äôs Œ¥.
3 NOISE IN BUILD BREAKAGE DATA
The final build outcome does not always tell the complete story.
Indeed,abrokenbuildoutcomemaynotindicateaproblemwith
thesystem,butratheraproblemwiththebuildsystemortestsuite.
Conversely,apassingbuildoutcomemayonlybelabeledassuch
because breakages in particular jobs are being ignored.
Inthissection,wepresenttheresultsofournoisinessstudyin
termsofoutcomesofbuildsthatareactivelyignored(3.1),passively
ignored (3.2), or stale (3.3). We also provide an overview of the
signal-to-noise ratio in the studied corpus (3.4).
3.1 Actively Ignored by Developers
Motivation. Supportfornewruntimeenvironmentsisoftenslowly
rolledoutthroughadaptivemaintenance[ 20].Whilesupportfor
newplatformsareintheexperimentalstage,developersmayignore
build breakage on these platforms.
89
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Keheliya Gallaba, Christian Macho, Martin Pinzger, and Shane McIntosh
Topreventfailingjobsinexperimentalareasofthecodebasefrom
causingbuildbreakage,TravisCIuserscansetthe allow_failure
propertywhentestingagainstversionsorconfigurationsthatdevel-
opersarenotreadytoofficiallysupport.6Inotherwords,ajobmay
fail; however, because the developers chose to ignore the outcome
ofitsconfiguration,theoutcomeofthebuildispassing.Soifanaly-
ses assume the build is successful because the reported outcome is
passing,actively ignored breakages may introduce noise. Therefore,
weanalyze how oftenbreakages areactively ignoredin our corpus.
Approach. We begin by selecting all of the 496,240 passing builds
inourdataset.Fromthosebuilds,weselecttheoneswithfailingjobs.
Then,weretrievethecorrespondingversionofthe .travis.yml
foreachofthoseselectedbuildsandcheckifthe allow_failure
property is enabled for the failing jobs.
Results. Inadditiontocomputinghowoftenpassingbuildscontain
failing jobs, Figure 2shows how the percentage of actively ignored
failing jobs is distributed in passing builds that had at least one
ignored failed job.
Observation 1 : 12% of passing builds have an actively ignored
failure.Ofthe496,240passingbuildsinourcorpus,59,904builds
had at least one actively ignored failure. Moreover, Figure 2shows
that in the passing builds that had at least one actively ignored
failing job, the median percentage of ignored failing jobs is 25%.
In an extreme case, 87% of the jobs were actively ignored. We
observethisinthe rubycas/rubycas-client7projectwherethe allow-
_failure property is set in 33 out of the 38 jobs.8Upon closer
inspection, we observe that this is an example of the intended use
of the allow_failure property. This build specifies eleven Ruby
versions as runtimes and four Gemfiles for dependency manage-
ment.Sixofthecombinationsareexplicitlyexcluded.Thus,38jobs
arecreatedforeachbuild(11 √ó4‚àí6).Allofthe33jobsthathave
theallow_failure property set fail. In subsequent builds, after
several source code changes by the development team, all of these
failingjobsbegintopass.Finally,thedevelopmentteamremoves
theallow_failure property from these jobs with an accompany-
ing commit message that states that ‚Äúbuilds should fail on released
versions of ruby and rails‚Äù. The development team only ignored
failureswhiletheyimprovedtheirsupportformultiplerubyand
rails versions.
Onthe otherhand,the allow_failure settingcan bemisused.
Forexample,inthe zdavatz/spreadsheet9project,the allow_failure
property,whichissetintheinitialbuildspecificationoftheproject,
is never removed from the build specification throughout the five-
yearhistoryoftheproject.10Furthermore,inourcorpus,wedetect
23 projects that had the allow_failure property set in all of their
builds.Theseprojectswerenotshort-lived,with31to769builds
in each project (median of 151). This suggests that although the
intendedpurposeofthe allow_failure propertyistotemporar-
ilyhidebreakages,developmentteamsdonotalwaysdisablethis
property after it has been set, leaving the breakages hidden.
6https://docs.travis-ci.com/user/customizing-the-build/#Rows-that-are-Allowed-to-
Fail
7https://github.com/rubycas/rubycas-client
8https://travis-ci.org/rubycas/rubycas-client/builds/5604025
9https://github.com/zdavatz/spreadsheet
10https://github.com/zdavatz/spreadsheet/blame/master/.travis.yml05000100001500020000
0 1 02 03 04 05 06 07 08 09 0
Percenta ge of I gnored Failed JobsFrequency
Figure2:Percentageofignoredfailedjobsinpassingbuilds
thathadatleastoneignoredfailedjobacrossallprojects.Upto 87% of the jobs are actively ignored.
Passingbuildoutcomesdonotalwaysindicatethatthebuild
was entirely clean.
3.2 Passively Ignored by Developers
Motivation. Buildbreakageisconsideredtobedistractingbecause
it draws developer attention away from their work to fix build-reported issues [
11,13,19]. If development can proceed without
addressing a build breakage, we suspect that the breakage is not
distracting. Sincethese passively ignoredbreakages may introduce
noiseinanalysesthatassumethatallbreakagesaredistracting.We
set out to analyze how often breakages are passively ignored.
Approach. To detect passively ignored breakages, we construct
andanalyzethedirectedgraphofrevisionsfromtheversionhistory
that have been built using Travis CI.
(1)Build Filtering. We start by selecting the git_trigger-
_commit and the git_prev_built_commit fields of each
buildfromTravisTorrent.The git_trigger_commit field
referstotherevisionwithintherepositorythatisbeingbuilt.
The git_prev_built_commit field refers to the revision
thatwasthetargetoftheimmediatelyprecedingbuild.Multi-plebuildsmaybeassociatedwithone
git_trigger_commit
because developers can configure Travis CI to run buildsat scheduled time intervals, even if no new commits have
appeared in the repository.11Builds can also be triggered by
theTravisCIAPI,regardlessofwhethertherearenewcom-
mits in the repository.12We remove such duplicate builds
by checking for builds that have event_type property set
tocronorapi. This reduces the number of builds from
680,209to676,408.TravisCIalsotriggersbuildswhen Git
tags are created even if the tagged commit has already been
built.We remove buildsthat weretriggered bytag pushes
bycheckingfornon-nullvaluesforthe tagsproperty.This
reduces the number of builds to 659,048. However, there are
11https://docs.travis-ci.com/user/cron-jobs/
12https://docs.travis-ci.com/user/triggering-builds/
90
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Noise and Heterogeneity in Historical Build Data ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
multiple builds remaining for one git_trigger_commit be-
cause manual build invocations can be made via the Travis
CIwebinterface.Thesemanualinvocationscannotbedis-
tinguished from regular builds that were triggered by Git
pushes. Therefore, when multiple builds are encountered
forone git_trigger_commit ,theearliestbuildisselected.
This reduces the number of builds to 610,550.
(2)GraphConstruction. Nodesinthegraphrepresentbuild-
triggeringcommits,whileedgesconnectbuildschronolog-
ically. All nodes are connected by edges from git_prev-
_built_commit node to git_trigger_commit node.
(3)Graph Analysis. We use the directed graph to identify
build-triggering commits from which others branch. We se-
lectthosebranchpointbuild-triggeringcommitsthathavea
non-passing outcome. Then, we traverse all of the branches
of such builds in a breadth-first manner to find the earli-
est build where the outcome is passing. Finally, we countthe number of builds along the shortest path between the
breakage branch point and the earliest fix.
Results. Figure3showsthebrokenbuildsthatareatthebranch
points in the version history of the project. To some degree, devel-
opers passively ignored these failures by not immediately fixing
them and continuing development in multiple paths.
Observation 2 : Breakages often persist after branching. Of the
23,068 builds that are triggered by commits at branch points, 4,136
(18%) are broken. Of those commits that are branched when the
build was broken, 3,426 builds (83%) are not fixed in the immedi-ately subsequent build. These breakages are suspicious because
developershavenotimmediatelyfixedthesebreakagesandhave
continued development.
Figure3shows that commits are branched from up to twelve
times when the build was broken. In the 13,102 builds that were
not immediately fixed, several commits appear before the fix does.
Figure4shows the maximum and median durations where the
projects remained broken in the studied projects.
Observation 3 : Breakages persist for up to 423 days, and seven
days on average, before being fixed. In one extreme case, the orbeon-
/orbeon-forms project13had485consecutivebuildbreakages over
423daysbeforefinallythebreakagewasaddressed.Uponfurther
investigationofthisbreakage,wefindthatthebuildisbrokendueto
multiple test failures over time. By analyzing the commit messages
of the broken builds in this sequence, we find only 10% of these
commits mention fixing the broken build (# of Occurrence of each
term: build=3, regression=2, test=46). However, near the end of the
longbuildbreakagesequence,twocommitsbeforethebuildstarted
passing again, the developer has started skipping tests mentioning
‚ÄúFornow,don‚Äôtrunintegrationanddatabasetests‚Äù.14Thisshows
that the build breakages were not the focus of the development
activityuntiltheendofthesequencewhentheyturnedoffthetests
that were causing the breakage.
Wefindthat761projectshavebreakagesthatpersistformore
thanoneday,547projectshavebreakagesthatpersistformorethan
one week, and 227 projects have breakages that persist for more
13https://github.com/orbeon/orbeon-forms
14https://github.com/orbeon/orbeon-forms/compare/f137cfb555f1...eb1a8095a0250%5%10%15%20%
23456789 1 0 1 1 1 2
# of BranchesPercentage of Builds
Type of Builds: Failed After Branching Failed
Figure 3: Developers branch out into multiple development
paths (branches) even after build breakages. Percentage ofbroken builds at branch points are shown in white. Per-centage of broken builds that continued to be broken afterbranching are shown in grey. There are no broken builds
with 11 branches.
1 Hour 1 Day 1 Week 1 Month 1 Y ear
Broken Time (in log scale )Projectmax
median
Figure 4: In some cases, builds can remain broken for 423
days.Thegraphshowsthemaximumandmediandurationsthat each project‚Äôs build remained broken, ordered by themaximum duration.
than one month before getting fixed. In eight projects, consecutive
build breakages persist for more than one year before getting fixed.
The overall median length of the failure sequences is five, while
project-specific medians range between 2‚Äì29.
In83%ofbranchesfrombrokenbuilds,thebreakagepersists.
These breakages persist for up to 485 commits.
3.3 Staleness of Breakage
Motivation. Developers can passively ignore breakages for differ-
ent reasons. We identify the stalenessof a build breakage (whether
the project has encountered a given breakage in the past) as one of
the reasons for ignoring a build breakage. A new breakage is dif-
ferent from a stale breakage because developers may have become
desensitized to stale breakages.
91
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Keheliya Gallaba, Christian Macho, Martin Pinzger, and Shane McIntosh
0%25%50%75%100%
ProjectStale Breakages Percentage
Figure 5: Percentage of stale breakages in each project can
range from 7% to 96%.
Approach. Inthissection,weinvestigatehowmanytimesdevel-
operscomeacrossthesamebreakagerepeatedlyinthehistoryof
a project with respect to the length of build breakage sequences.
Thesestalebreakagescanoccureitherconsecutivelyorintermit-
tently. Hence, we extend the Maven Log Analyzer developed by
Machoetal. [ 15].WeuseittocomparetwoTravisCIbuildjobs
and check the similarity of the breakages. To make the comparison
efficient,thisisdoneintwosteps.First,thelogsofbuildjobsare
parsed and checked if they are breaking due to the same reason
(e.g.,compilationfailure,testexecutionfailure,dependencyreso-
lutionfailure).Ifthereasonforfailuresareequalthenthedetails
of the failure are also checked (e.g., if both breakages are due to
compilation failure, check if the compilation error is the same).
Results. Figure5shows the percentage of stale build breakages in
each project in descending order.
Observation 4 : 67% of the breakages (6,889 out of 10,816) that
we analyze are stale breakages. On the project level, staleness of
breakages ranges from 7% to 96% with a median of 50%. In the
eirslett/frontend-maven-plugin15project,whereweobservethemax-
imumpercentageofstalebreakages(96%),itwasduetothesame
dependency resolution failure recurring in 23 builds.
Two of every three build breakages (67%) that we analyze
are stale.
3.4 Signal-To-Noise Ratio
Motivation. Inpreviousanalyses,wefindthatbuildbreakagesthat
are ignored by developers and build successes that include ignored
breakages can introduce noise in build outcome data. Ho wever, the
overall rate of noise in build outcome data is not yet clear. Such an
overviewisusefulforresearcherswhousebuildoutcomedatain
their work, to better understand the degree to which noise may be
impacting their analyses.
Approach. Toquantifytheproportionofnoiseinbuildoutcome
datacausedbypassivelyandactivelyignoredbuildbreakage,we
15https://github.com/eirslett/frontend-maven-plugin5678910
0 100 200 300 400 500
Build Failure Sequence Length Threshold (t c)Signal‚àíto‚àíNoise RatioParameter
Overall
Branches‚àíonly
Figure 6: For every 11 builds there is at least one build
withanincorrectstatus.TheSignal-To-Noiseratioincreases
when a higher build breakage sequence length is chosen.
adopt the Signal-To-Noise ratio (SNR) as follows:
SNR=#TrueBuildBreakages + #TrueBuildSuccesses
#FalseBuildBreakages + #FalseBuildSuccesses(1)
where#TrueBuildBreakages (i.e., signal) is the number of broken
builds that are not ignored by developers, #TrueBuildSuccesses (i.e.,
signal) is the number of passing builds without ignored breakages,
#FalseBuildBreakages (i.e., noise) is the number of broken builds
thatareignoredbydevelopers,and #FalseBuildSuccesses (i.e.,noise)
is the number of passing builds with ignored breakages.
Tocompute #FalseBuildBreakages,athreshold tcmustbeselected
such that if the number of consecutive broken builds is above tc,
all buildsin such sequences areconsidered false build breakages.
Instead of picking any particular tcvalue, we plot an SNR curve as
the threshold ( tc) is changed.
Results. Figure6shows the SNR curve for the subject systems.
Observation 5 :A stcdecreasesfrom485to1,theSNRdecreases
from 10.62 to 6.39. Since#FalseBuildSuccesses is not impacted by tc,
themaximumSNRisobservedwhen #FalseBuildBreakages iszero
(i.e.,when tcissettothemaximumvalue).TheminimumofSNR
is observed when tcis one and therefore all broken builds that are
not immediately fixed are considered false build breakages. If false
breakages are defined to be only in consecutive breakages withbranches in them, the Signal-to-Noise ratio ranges from 10.19 to
10.62.
Oneinevery7to11builds(9%‚Äì14%)isincorrectlylabelled.
Thisnoisemayinfluenceanalysesbasedonbuildoutcome
data.
4 HETEROGENEITY IN BUILD BREAKAGE
DATA
Thewayinwhichbuilds areconfiguredandtriggeredvaryfrom
project to project. This heterogeneity should be taken into con-sideration when designing studies of build breakage. Below, we
demonstrate build heterogeneity using three criteria.
92
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Noise and Heterogeneity in Historical Build Data ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè
‚óè‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
‚óè‚óè‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè
0%25%50%75%100%
1 10 100
# of jobs in buildImpure Breakages Percentage
Figure 7: Percentage of impure build breakages increases
with the number of jobs in each build.
4.1 Matrix Breakage Purity
Motivation. Ifasoftwareprojectneedstobetestedinmultipleen-
vironmentswithdifferentruntimeversions,CIserviceslikeTravis
CIprovidetheabilitytodeclaretheseoptionsinamatrixof runtime,
environment, andexclusions/inclusions sections.Abuildwillexecute
jobs for each combination of included runtimes and environments.
If a build is broken only within a subset of its jobs, the break-
age may be platform- or runtime-specific. These environment-
specificbuildbreakagesmayneedtobehandleddifferentlyfrom
theenvironment-agnosticbreakages.Thus,wewanttoknowthe
extent to which real breakages are environment specific.
Approach. To study the environments that are affected by a build
breakage, we define Matrix Breakage Purity as follows:
Matrix Breakage Purity =#FailedJobsInBuild
#AllJobsInBuild(2)
AMatrix Breakage Purity below one indicates that the jobs that
were run in some environments passed. We compute the Matrix
Breakage Purity for all builds in our dataset and count the number
of builds with values below one. We label all builds that have a
Matrix Breakage Purity below one as impure build breakages.
Results. Figure7showshowthepercentageofimpurebuildbreak-
ages varies with respect to the number of jobs per build.
Observation 6 :Atleast44%ofbrokenbuildscontainpassingjobs.
Indeed,environment-specificbreakagesarealmostascommonas
environment-agnostic breakages.
Giventhedifferenceinsemanticsbetweenpureandimpurebuild
breakage, researchers should take this into account when selecting
build outcome data for research. For example, in build outcome
prediction,ifpredictionmodelsaretrainedusingdatathattreats
environment-specific and environment-agnostic breakages iden-
tically,themodelfitnesswilllikelysuffer.Moreover,theinsights
thatarederivedfromthemodelswilllikelybemisleading,sincethe
two conflated phenomena will be modelled as one phenomenon.
Observation 7 : Builds with a greater number of jobs are more
likely to suffer from impure build breakages. Figure7shows that the
number of jobs in a build and the percentage of broken builds that
havepassingjobsarehighlycorrelated.ASpearmancorrelationtestyieldsaœÅof0.8,withp=2.2<10‚àí16.Whilepurebuildbreakage
iscommoninbuildswithfewjobs,whenthenumberofjobsper
build exceeds three, impure build breakage are more frequent than
pure ones (i.e., impure breakage percentage >50%).
Environment-specific breakage is commonplace. Once the
numberofjobsexceedsthree,impurebreakagesoccurmore
frequently than pure breakages.
4.2 Reason for Breakage
Motivation. Builds can break for reasons that range from style
violations to test failures. Different types of failures have different
implications. For example, while a style violation might be cor-
rectedeasily,fixingatestfailuremightrequiretimeandeffortto
understand and address. Since subsequent analyses of build data
should handle different types of breakages in different ways, we
want to know how types of build breakage vary in reality.
Approach. Toanalyzethereasonsforbuildbreakageinourcorpus,
we extend the Maven Log Analyzer (MLA) [15]. Our extension first
parses the Travis CI log file and extracts the sections of the log
thatcorrespondtoexecutionsofthe Mavenbuildtool.Then,each
of theseMavenexecutions are fed to MLA to automatically classify
the status of each execution. In addition to the breakage types that
wereidentifiedintheoriginalwork[ 15],ourextendedversionof
MLAalsodetectsthebuildbreakagetypesthatwerereportedby
Vassallo et al. [ 26] and Rausch et al. [ 18], as well as ten previously
unreported breakage categories.
If MLA classifies all Mavenexecutions within a broken build
assuccessful,thebuildislabelledasa Non-Maven breakage.Non-
Maven breakages are further classified as Pre-Maven if a failing
commandisdetectedintheTravisCIlogbeforethe Mavencom-
mands and Post-Maven otherwise.
Intotal,usingourextendedMLA,weclassify67,267brokenbuild
jobs of projects that use Mavenas the build tool.
Results. Table1classifies the broken Mavenbuilds by reason.16
Observation 8 : Although a large proportion of build breakages
are due to the execution of Antfrom within Maven, most of these
breakages belong to one project. Table1shows that there are 15,850
instances of breakage where the external goal of executing an Ant
build from within a Mavenbuild failed. This accounts for 92.59%
of the goal failed breakages in our corpus. However , this is an
exampleofananomalythatdissipateswhenexaminedmoreclosely.
Indeed, we find that all of these breakages occur in only two of the
studiedprojects,theoverwhelmingmajority(15,857)ofwhichoccur
in thejruby/jruby17project. According to developer discussions,
Antis used inside the Mavenbuild ofjruby/jruby for executing
tests.18However, this complex build setup, which requires 250MB
of dependencies, causes build to fail intermittently. The developers
hopethatthebreakageswillnotoccuroncethebuildiscompletely
migrated to Maven.
Observation 9 : In our corpus, most breakage is due to commands
otherthanmainbuildtool, Maven.Weobserve41%(27,289)jobsare
16More details about reasons for breakage are available online: https://github.com/
software-rebels/bbchch/wiki/Build-Breakages-in-Maven
17https://github.com/jruby/jruby
18https://gitter.im/jruby/jruby/archives/2016/05/27
93
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Keheliya Gallaba, Christian Macho, Martin Pinzger, and Shane McIntosh
Table 1: Distribution of Build Breakages in MavenProjects
basedontheCategoriesproposedbyVassalloetal.[26].and
Rausch et al. [18]. Global percentage of each category isshown in brackets.
Category Subcategory # % Projects
Dependency Resolution*2,257 (3.41%) 18
Test Execution Failed Unit 10,759 62.87% 165
Integration 6,354 37.13% 18
Total 17,113 (25.89%) 171
Compilation Failed Production 2,015 87.08% 18
Test 248 10.72% 12
Total 2,314 (3.50%) 47
Goal Failed Pre-processing 44 0.26% 4
Static-Analysis 210 1.23% 10
Dynamic-Analysis 8 0.05% 3
Validation 33 0.19% 5
Packaging 25 0.15% 4
Documentation 25 0.15% 7
Release Preparation 1 0.01% 1
Deployment - Remote 120 0.70% 9
Deployment - Local 7 0.04% 1
Support 3 0.02% 1
Ant inside Maven*15,850 92.59% 2
Run system/Java program*70 0.41% 2
Run Jetty server*8 0.05% 1
Manage Ruby Gems*65 0.38% 1
Polyglot for Maven*32 0.19% 1
Total 17,119 (25.90%) 47
Broken Outside Maven No Log available*1,554 5.69% 28
Failed Before Maven*808 2.96% 3
Failed After Maven*7,151 26.20% 46
Travis Aborted*16,141 59.15% 172
Travis Cancelled*1,635 5.99% 20
Total 27,289 (41.29%) 175
*New build breakage categories that did not appear in prior work.
broken due to reasons other than Mavenexecutions failing. This
can be either due to a command that was executed by Travis CI
(outside of Maven) returning an error, the user canceling the build,
or Travis CI runtime aborting the build because it exceeded the
allocated time.
Observation 10 :Onlyasmallamountofbreakagecanbeauto-
matically fixedby focusing ontool-specific breakage. For example,
2,257 build jobs are broken because dependency resolution has
failed. This suggests that recent approaches that automatically fix
dependency-relatedbuildbreakage[ 15]willonlyscratchthesur-
faceofthebuildbreakageproblem.Moreover,CompilationandTest
Execution failures only account for another 29.39% of the breakage
inourcorpus.Futureautomaticbreakagerecoveryeffortsshould
look beyond tool-specific breakages to the CI scripts themselves in
order to yield the most benefit for development teams.
41%ofthebrokenbuildsinourcorpusfailedduetoproblems
outside of the execution of the main build tool. Since tool-
specificbreakageisrare,futureautomaticbreakagerecovery
techniques should tackle issues in the CI scripts themselves.
4.3 Type of contributor
Motivation. Both core and peripheral contributors trigger builds.
Sincecorecontributorslikelyhaveadeeperunderstandingofthe
project than peripheral contributors, builds that are triggered bycorecontributorsmighthavebreakageratesandteamresponses
0 2 04 06 08 0 1 0 0
Broken PassedCore contributor
Peripheral contributor
Figure8:Percentageofbrokenandpassingbuildsclassified
bycontributortype.Horizontalblacklinesshowthemedian
values.
that differ from those of peripheral contributors. We set out to
investigatethedifferencesofbuildoutcome,inthesetwocategories
of contributors.
Approach. For analyzing this dimension, we use the two main
outcomesof eachbuild (passed orfailed) andwhetherthe builds
were triggered by a commit that was authored by a core team
member.WeusethecorememberindicatorfromtheTravisTor-
rent dataset,19which is set for contributors who have commit-
ted at least once within the three months prior to this commit
(gh_by_core_team_member ). Then, we use the broken time and
thelengthofbrokenbuildsequencestoinvestigatetherelationship
between the contributor type and build breakage.
Results. Figure8showshowthepercentageofbuildoutcomesare
distributed across projects classified by contributor type.
Observation 11 :Buildstriggeredbycoreteammembersarebreak
significantly more often than those of peripheral contributors. A
Wilcoxon signed rank test indicates that breakage rates in core
contributors are higher than those of peripheral contributors ( p=
1.28<10‚àí8); however, the effect size is negligible (Cliff‚Äôs Œ¥=0.13).
Duetohavingmoreexperience,coreteammembersinthedevel-
opmentteamsareassignedtocomplextasks,whichmayexplain
why breakage ratestend to be alittle higher. The Wilcoxontest is
inconclusivewhencomparingratesofpassingbuildsamongcore
and peripheral contributors.
Figure9shows how long build breakages persist classified by
contributor type. Figures 9aand9bshow the length of build break-
age sequences in terms of commits and time, respectively.
Observation 12 : Breakages that are caused by core contribu-
torstendtobefixedsoonerthanthoseofperipheralcontributors. A
Wilcoxon signed rank test indicates that breakages caused by core
contributors tend to persist for significantly less time than those of
peripheralcontributors( p=1.86<10‚àí7);however,theeffectsizeis
negligible (Cliff‚Äôs Œ¥=0.09). Another Wilcoxon signed rank test in-
dicatesthatbreakagesofcorecontributorspersistforfewerconsec-utivebuildsthanthoseofperipheralcontributors(
p=1.81<10‚àí8);
however, the effect size is also negligible (Cliff‚Äôs Œ¥=0.09).
19https://travistorrent.testroots.org/page_dataformat/
94
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Noise and Heterogeneity in Historical Build Data ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France2 5 10 20 50 200 500
Broken Sequence LengthPeripheral contributor
Core contributor
(a) Chains of consecutive
breakages caused by periph-
eral contributors tend to be
longer.
1 10 100 1000 10000
Broken Time (in hours)Peripheral contributor
Core contributor
(b) Build breakages caused by
peripheral contributors take
more time to repair.
Figure9:Buildbreakagescausedbyperipheralcontributors
remain broken significantly longer than those of core con-
tributors. Horizontal black lines show the median values.
Thelongertimetakenbyperipheralcontributorsmightbedue
to multiple attempts of trial and error before fixing a breakage,
while core members might be able to identify the root cause of
the breakage sooner. Therefore, it may be worthwhile for the re-
searchers working on automatic build breakage repair to focus on
build breakages that are caused by peripheral contributors.
Brokenbuildsthatarecausedbycorecontributorstendtobe
fixed sooner than those of peripheral contributors.
5 IMPLICATIONS
Wenowpresentthebroaderimplicationsofourobservationsfor
researchers and tool builders.
5.1 Research Community
Buildoutcomenoiseshouldbefilteredoutbeforesubsequent
analyses. Passingbuildsmightcontainbreakagesthatareignored.
Long sequences of repeated breakages might be ignored by thedevelopers as false breakages. If the noise due to false successes
andfalsebreakagesisnotfilteredout,theresultsfrom prediction
models may lead to spurious or incorrect conclusions.
Heterogeneityofbuildsshouldbeconsideredwhentraining
buildoutcome predictionmodels. Somebreakagesarelimited
tospecificenvironmentswhileothersarenot.Thereasonforbreak-
ages vary from trivial issues like style violations to complex test
failures. Breakages are often not caused by development mistakes,
but by resource limitations in the CI environment. Indeed, build
outcomeincludesmanycomplexcategoriesthatcannotbeaccu-
ratelyrepresentedinthepredictionmodelsusingonlya‚Äúbroken‚Äù
or ‚Äúclean‚Äù label.
5.2 Tool Builders
Automaticbreakagerecoveryshouldlookbeyondtool-specific
insight.Whilerecentlyproposedtoolscanautomaticallyrecoverfrom tool-specific build breakage [ 15], we find that this category
only accounts for a small proportion of CI build breakage in ourcorpus. Future efforts in breakage recovery should consider CI-
specific scripts, for example, detecting those scripts that are at risk
of exceeding the allocated time prior to execution.
Richerinformationshouldbeincludedinbuildoutcomere-
portsanddashboards. Currently, build tools and CI services pro-
vide users with dashboards that show passing builds in green and
broken builds in red. However, we found hidden breakages among
passing builds and non-distracting breakages among broken builds.
Moreover, heterogeneity of breakages introduce further complexi-
ties.Buildoutcomereportingtoolsanddashboardsshouldconsiderprovidingmorerichinformationabouthidden,non-distracting,and
stale breakages, as well as breakage purity and type.
6 THREATS TO VALIDITY
ConstructValidity. Threats to construct validity refer to the rela-
tionshipbetweentheoryandobservation.Itispossiblethatthere
are build failure categories that our scripts are unable to detect. By
implementing the categories that were reported in prior work [ 15,
18,26]andthenmanuallycheckingasubsetoflogsalongwiththeir
detectedfailurecategories,weensuremostofthe mavenplug-ins
and their failure categories are covered by our scripts.
There are likely to be other factors that introduce noise and
variabilityinbuildoutcomes.Asaninitialstudy,wefocusonthe
aspectsthatwethinkdemonstratenoiseandheterogeneityofbuilds
in this work. Our list of aspects is not intended to be exhaustive.
InternalValidity. Threatstointernalvalidityarerelatedtofactors,
internal to our study, that can influence our conclusions. In the
analysisofpassivelyignoredbreakages,weassociatecontinuous
breakageofabuildwithdevelopersignoringthebreakage.However,
developersmaybeunsuccessfullyattemptingtofixthesebreakages
during the breakage chain. We do not suspect that this is the most
frequentexplanationbecausewefindseveralcaseswheretheinitial
breakage has several branches (implying that several developersinherited the breakage). Although it can be assumed that thesebranches are created to fix the build, it is unlikely that twelve
branchesarecreatedonlyforbugfixing.Wefurtherinvestigatethestalenessofbreakages,observingthatthesamebuildbreakagesare
often repeated in these long chains.
ExternalValidity. Threatstoexternalvalidityareconcernedwith
the generalizability of our findings. We only consider open source
projects that use the Travis CI service and are hosted on GitHub.
However, because GitHub is one of the most popular hosting plat-
forms for open source software projects and Travis CI is the most
widelyadoptedCIserviceamongopensourceprojects,ourfindings
are applicable to a large number of open source projects. Similar
to that, we only consider projects that use Mavenfor analyzing
reasons for build breakage. However, Mavenis one of the most
popularbuildtoolsforJavaprojects[ 16]andthereforeourfindings
arewidelyapplicable.Nonetheless,replicationstudiesusingdata
from other hosting platforms, other CI services, and other build
tools may provide additional insight.
95
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France Keheliya Gallaba, Christian Macho, Martin Pinzger, and Shane McIntosh
7 RELATED WORK
Inthissection,we describe therelatedworkwithrespecttobuild
breakage and continuous integration.
7.1 Build Breakage
Build breakage has attracted the attention of software engineering
researchers at many occasions during the past decade.
The rate at which builds are broken has been explored in the
past. Kerzazi et al. [ 11] have conducted an empirical study in a
large software company analyzing 3,214 builds that were executed
overaperiodofsixmonthstomeasuretheimpactofbuildbreak-
ages, observing a build breakage rate of 17.9%, which generates
an estimated cost of 904.64 to 2034.92 person hours. Seo et al. [ 19]
studied nine months of build data at Google, finding that 29.7%
and37.4%ofJavaandC++buildswerebroken.Tufanoetal.[ 21]
found only 38% of the change history of 100 subject systems is
successfully compilable and that broken snapshots occur in 96% of
thestudiedprojects.Hassanetal.[ 7]showedthatatleast57%of
the broken builds from the top-200 Java projects on GitHub can
be automatically resolved.
To better understand and predict build breakage, past studies
have fit prediction models. Hassan and Zhang [ 6] have demon-
stratedthatdecisiontreesbasedonprojectattributescanbeused
topredictthecertificationresultofabuild.Wolfetal.[ 27]useda
predictive model that leverages measures of developer communica-
tion networks to predict build breakage. Similarly, Kwan et al. [ 12]
usedmeasuresofsocio-technicalcongruence,i.e.,theagreementof
thecoordinationneedsestablishedbythetechnicaldomainwith
theactualcoordinationactivitiescarriedoutbyprojectmembers,
to predict build outcome in a globally distributed software team. In
recent work, Luo et al. [ 14] have used the TravisTorrent dataset
to predict the result of a build based on 27 features. They found
that the number of commits in a build is the most important factor
that can impact the build result. Dimitropoulos et al. [ 4] use the
samedatasettostudythefactorsthathavethelargestimpacton
build outcome based on K-means clustering and logistic regression.
For communicating the current status of the build, Downs et
al.[5]proposed theuseof ambientawarenesstechnologies.They
have observed by providing a separate, easily perceived commu-nication channel distinct from standard team workflow for com-municating build status information, the total number of builds
increasedsubstantially,andthedurationofbrokenbuildsdecreased.
Tohelpdeveloperstodebugbuildbreakage,Vassalloetal.[ 25]pr o-
pose a summarization technique to reduce the volume of build
logs. Formitigating the impactof buildbreakage in thecontext of
component-based software development, van der Storm [ 22] have
shown how backtracking can be used to ensure that a working
version is always available, even in the face of failure.
Broadlyspeaking,the priorworkhastreated buildbreakageas
aboolean,passorfaillabel.Inthispaper,weadvocateforamore
nuanced interpretation of build breakage that recognizes the noise
in build outcome data and heterogeneity of build executions.
7.2 Continuous Integration
Recent work has studied adoption of CI in the open source com-
munity. Beller et al. [ 3] have released a dataset based on Travis CIand GitHub that provides easy access to hundreds of thousandsof CI builds from more than 1,000 open-source projects. Efforts
similartothishavemadeseveralstudiesofCIbuildspossible.By
analyzing open source projects on GitHub and surveying devel-
opers, Hilton et al. [ 9] report on which CI systems developersuse,
how developers use CI, and reasons for using CI (or not).
TheadoptionofCIhasanimpactonotherprojectcharacteristics.
Hilton et al. [ 8] found that, when using CI, developers have to
choose between speed and certainty, better access and information
security, and more configuration options and greater ease of use.
Vasilescu et al. [ 24] observe that CI adoption is often accompanied
by a boost in the productivity of project teams.
Recent community interest in CI has yielded a resurgence of
build breakage research. For example, Vasilescu et al. [ 23] studied
223GitHubprojectsandfoundthattheCIbuildsstartedbypull
requestsaremorelikelytofailthanthosestartedbydirectcommits.Belleretal.[
2]havestudiedtestingpracticesinCIof JavaandRuby
projects, observing that testing is the most frequently occurring
typeofbuildbreakage.Rauschetal.[ 18]identifiedthemostcom-
monerrorcategoriesinCIbuildsbasedon14opensourceJavaappli-cations.Zampettietal.[
28]havestudiedtheusageofstaticanalysis
tools in 20 open source Java projects that are hosted on GitHub
and using Travis CI. Vassallo et al. [ 26] compare the CI processes
andoccurrencesofbuildbreakagesin349opensourceJavaprojects
and 418 projects from a financial organization. Labuschagne et
al.[13]havealsoobservedthattherearelongstretchesofbroken
buildsduetoprojectsnotconfiguringTravisCIcorrectlyornot
examiningtheTravisCIoutput.Toaccountforthis,theyremoved
the 20% of the projects that had the most and the fewest failures.
While prior work helps to understand build breakage and CI in
practice, we focus on the trustworthiness of off-the-shelf historical
CI build data. Our observations yield insights that can guide futurestudiesofandtooldevelopmentforbuildbreakageintheCIcontext
(see Section 5).
8 CONCLUSION
Automated builds are commonly used in software development to
verifyfunctionalityanddetectdefectsearlyinsoftwareprojects.An
off-the-shelfusageofbuildoutcomedataisimplicitlysusceptibletoharmfulassumptionsaboutbuildbreakage.Byempiricallystudyingbuildjobsof1,276opensourceprojects,weinvestigatewhethertwo
assumptionshold.First,thatbuildresultsarenotnoisy;however,
we find in every eleven builds, there is at least one build with a
misleading or incorrect outcome on average. Second, that builds
arehomogeneous;however,wefindbreakagesvarywithrespect
to the number of impacted jobs and the causes of breakage.
Researcherswhomakeuseofbuildoutcomedatashouldmake
sure that noise is filtered out and heterogeneity is accounted forbefore subsequent analyses are conducted. Build reporting tools
anddashboardsshouldalsoconsiderprovidingaricherinterface
to better represent these characteristics of build outcome data.
Infuturework,weplantostudyhowmuchofanimpactnoise
andheterogeneitycanhaveoncommonanalysesofhistoricalbuilddata.Wealsoplantoinvestigatewhetherbreakagetypevarieswith
respect to contributor type and other commit factors.
96
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. Noise and Heterogeneity in Historical Build Data ASE ‚Äô18, September 3‚Äì7, 2018, Montpellier, France
REFERENCES
[1]Bram Adams and Shane McIntosh. 2016. Modern Release Engineering in a Nut-
shell:WhyResearchersshouldCare.In ProceedingsoftheInternationalConference
on Software Analysis, Evolution, and Reengineering (SANER), Vol. 5. 78‚Äì90.
[2]Moritz Beller, Radjino Bholanath, Shane McIntosh, and Andy Zaidman. 2016.
Analyzing the State of Static Analysis: A Large-Scale Evaluation in Open Source
Software. In Proceedings of the International Conference on Software Analysis,
Evolution, and Reengineering (SANER). 470‚Äì481.
[3]MoritzBeller,GeorgiosGousios,andAndyZaidman.2017. TravisTorrent:Synthe-
sizing Travis CI and GitHub for Full-Stack Research on Continuous Integration.
InProceedings of the International Conference on Mining Software Repositories
(MSR). 447‚Äì450. https://doi.org/10.1109/msr.2017.24
[4]Panagiotis Dimitropoulos, Zeyar Aung, and Davor Svetinovic. 2017. Continuous
integration build breakage rationale: Travis data case study. In International
ConferenceonInfocomTechnologiesandUnmannedSystems(TrendsandFuture
Directions) (ICTUS). https://doi.org/10.1109/ictus.2017.8286087
[5]John Downs, Beryl Plimmer, and John G. Hosking. 2012. Ambient awareness
ofbuildstatusincollocatedsoftwareteams.In ProceedingsoftheInternational
Conference on Software Engineering (ICSE). https://doi.org/10.1109/icse.2012.
6227165
[6]Ahmed E. Hassan and Ken Zhang. 2006. Using Decision Trees to Predict the
CertificationResultofaBuild.In ProceedingsoftheInternationalConferenceon
Automated Software Engineering (ASE). https://doi.org/10.1109/ase.2006.72
[7]Foyzul Hassan, Shaikh Mostafa, Edmund S.L. Lam, and Xiaoyin Wang. 2017. Au-
tomaticBuildingofJavaProjectsinSoftwareRepositories:AStudyonFeasibility
and Challenges. In Proceedings of the International Symposium on Empirical Soft-
wareEngineeringandMeasurement(ESEM). https://doi.org/10.1109/esem.2017.11
[8]MichaelHilton, NicholasNelson,Timothy Tunnell,DarkoMarinov,and Danny
Dig.2017.Trade-offsincontinuousintegration:assurance,security,andflexibility.
InProceedingsofJointMeetingoftheEuropeanSoftwareEngineeringConference
and the International Symposium on the Foundations of Software Engineering
(ESEC/FSE). 197‚Äì207. https://doi.org/10.1145/3106237.3106270
[9]MichaelHilton,TimothyTunnell,KaiHuang,DarkoMarinov,andDannyDig.
2016. Usage,costs,andbenefitsofcontinuousintegrationinopen-sourceprojects.
InProceedings of the International Conference on Automated Software Engineering
(ASE). 426‚Äì437. https://doi.org/10.1145/2970276.2970358
[10]PeterKampstra.2008. Beanplot:ABoxplotAlternativeforVisualComparison
of Distributions. Journal of Statistical Software 28, Code Snippet 1 (2008), 1‚Äì9.
https://doi.org/10.18637/jss.v028.c01
[11]NoureddineKerzazi,FoutseKhomh,andBramAdams.2014. WhyDoAutomated
Builds Break? An Empirical Study. In Proceedings of the International Conference
onSoftwareMaintenanceandEvolution(ICSME). https://doi.org/10.1109/icsme.
2014.26
[12]IrwinKwan,AdrianSchroter,andDanielaDamian.2011. DoesSocio-Technical
Congruence Have an Effect on Software Build Success? A Study of Coordination
in a Software Project. IEEE Transactions on Software Engineering (TSE) 37, 3
(2011), 307‚Äì324. https://doi.org/10.1109/tse.2011.29
[13]AdriaanLabuschagne,LauraInozemtseva,andReidHolmes.2017. Measuringthe
costofregressiontestinginpractice:astudyofJavaprojectsusingcontinuousin-tegration.In ProceedingsoftheJointMeetingonFoundationsofSoftwareEngineer-
ing (ESEC/FSE). ACM Press, 821‚Äì830. https://doi.org/10.1145/3106237.3106288
[14]Yang Luo, Yangyang Zhao, Wanwangying Ma, and Lin Chen. 2017. What are theFactorsImpactingBuildBreakage?.In ProceedingsoftheWebInformationSystems
andApplications Conference(WISA).IEEE. https://doi.org/10.1109/wisa.2017.17
[15]Christian Macho, Shane McIntosh, and Martin Pinzger. 2018. Automatically
RepairingDependency-RelatedBuildBreakage.In ProceedingsoftheInternational
Conference on Software Analysis, Evolution, and Reengineering (SANER). 106‚Äì117.
https://doi.org/10.1109/SANER.2018.8330201
[16]Shane McIntosh, Meiyappan Nagappan, Bram Adams, Audris Mockus, and
Ahmed E. Hassan. 2015. A Large-Scale Empirical Study of the Relationshipbetween Build Technology and Build Maintenance. Empirical Software Engineer-
ing20, 6 (2015), 1587‚Äì1633.
[17]Audris Mockus. 2007. Software Support Tools and Experimental Work. In
Empirical Software Engineering Issues. Critical Assessment and Future Directions:
InternationalWorkshop,DagstuhlCastle,Germany,June26-30,2006.RevisedPapers,
Victor R. Basili, Dieter Rombach, Kurt Schneider, Barbara Kitchenham, Dietmar
Pfahl,andRichardW.Selby(Eds.).SpringerBerlinHeidelberg,Berlin,Heidelberg,
91‚Äì99.https://doi.org/10.1007/978-3-540-71301-2_25
[18]ThomasRausch,WaldemarHummer,PhilippLeitner,andStefanSchulte.2017.An
Empirical Analysis of Build Failures in the Continuous Integration Workflows of
Java-Based Open-Source Software. In Proceedings of the International Conference
onMiningSoftwareRepositories(MSR).345‚Äì355. https://doi.org/10.1109/msr.2017.
54
[19]Hyunmin Seo, Caitlin Sadowski, Sebastian Elbaum, Edward Aftandilian, and
Robert Bowdidge. 2014. Programmers 'build errors: a case study (at google).
InProceedings of the International Conference on Software Engineering (ICSE).
724‚Äì734. https://doi.org/10.1145/2568225.2568255
[20]E. Burton Swanson. 1976. The Dimensions of Maintenance. In Proceedings of
International Conference onSoftware Engineering (ICSE). 492‚Äì497. http://dl.acm.
org/citation.cfm?id=800253.807723
[21]Michele Tufano, Fabio Palomba, Gabriele Bavota, Massimiliano Di Penta, Rocco
Oliveto,AndreaDeLucia,andDenysPoshyvanyk.2016. Thereandbackagain:
Canyoucompilethatsnapshot? JournalofSoftware:EvolutionandProcess(JSEP)
29, 4 (2016), e1838. https://doi.org/10.1002/smr.1838
[22]TijsvanderStorm.2008. BacktrackingIncrementalContinuousIntegration.In
ProceedingsoftheEuropeanConferenceonSoftwareMaintenanceandReengineering
(CSMR). https://doi.org/10.1109/csmr.2008.4493318
[23]BogdanVasilescu,StefvanSchuylenburg,JulesWulms,AlexanderSerebrenik,
andMarkG.J.vandenBrand.2015. Continuousintegrationinasocial-coding
world:EmpiricalevidencefromGitHub.**Updatedversionwithcorrections**.
CoRRabs/1512.01862 (2015). arXiv:1512.01862 http://arxiv.org/abs/1512.01862
[24]BogdanVasilescu,YueYu,HuaiminWang,PremkumarDevanbu,andVladimir
Filkov.2015. QualityandProductivityOutcomesRelatingtoContinuousInte-
grationinGitHub.In ProceedingsoftheJointMeetingoftheEuropeanSoftware
EngineeringConferenceandtheInternationalSymposiumontheFoundationsof
Software Engineering (ESEC/FSE). 805‚Äì816.
[25]CarmineVassallo,SebastianProksch,TimothyZemp,andHaraldC.Gall.2018.
Un-BreakMyBuild:AssistingDeveloperswithBuildRepairHints.In Proceedings
of the International Conference on Program Comprehension (ICPC). To appear.
[26]CarmineVassallo,GeraldSchermann,FiorellaZampetti,DanieleRomano,Philipp
Leitner, Andy Zaidman, Massimiliano Di Penta, and Sebastiano Panichella. 2017.
ATaleofCIBuildFailures:AnOpenSourceandaFinancialOrganizationPer-
spective. In Proceedings of the International Conference on Software Maintenance
and Evolution (ICSME). 183‚Äì193. https://doi.org/10.1109/icsme.2017.67
[27]Timo Wolf, Adrian Schroter, Daniela Damian, and Thanh Nguyen. 2009. Predict-
ing build failures using social network analysis on developer communication.InProceedings of the International Conference on Software Engineering (ICSE).
https://doi.org/10.1109/icse.2009.5070503
[28]Fiorella Zampetti, Simone Scalabrino, Rocco Oliveto, Gerardo Canfora, and
Massimiliano Di Penta. 2017. How Open Source Projects Use Static Code
Analysis Tools in Continuous Integration Pipelines. In Proceedings of the In-
ternational Conference on Mining Software Repositories (MSR) . 334‚Äì344. https:
//doi.org/10.1109/msr.2017.2
[29]MahdisZolfagharinia,BramAdams,andYann-Ga√´lGu√©h√©neuc.2017. DoNot
TrustBuildResultsatFaceValue-AnEmpiricalStudyof30MillionCPANBuilds.
InProceedings of the International Conference on Mining Software Repositories
(MSR). 312‚Äì322. https://doi.org/10.1109/msr.2017.7
97
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:27:29 UTC from IEEE Xplore.  Restrictions apply. 