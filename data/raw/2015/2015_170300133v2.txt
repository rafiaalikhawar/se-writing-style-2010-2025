Easy over Hard: A Case Study on Deep Learning
Wei Fu, Tim Menzies
Com.Sci., NC State, USA
wfu@ncsu.edu,tim.menzies@gmail.com
ABSTRACT
While deep learning is an exciting new technique, the bene/f_its of
this method need to be assessed with respect to its computational
cost. /T_his is particularly important for deep learning since these
learners need hours (to weeks) to train the model. Such long train-
ing time limits the ability of (a) a researcher to test the stability
of their conclusion via repeated runs with diÔ¨Äerent random seeds;
and (b) other researchers to repeat, improve, or even refute that
original work.
For example, recently, deep learning was used to /f_ind which
questions in the Stack Over/f_low programmer discussion forum can
be linked together. /T_hat deep learning system took 14 hours to
execute. We show here that applying a very simple optimizer called
DE to /f_ine tune SVM, it can achieve similar (and sometimes be/t_ter)
results. /T_he DE approach terminated in 10 minutes; i.e. 84 times
faster hours than deep learning method.
We oÔ¨Äer these results as a cautionary tale to the so/f_tware analyt-
ics community and suggest that not every new innovation should
be applied without critical analysis. If researchers deploy some new
and expensive process, that work should be baselined against some
simpler and faster alternatives.
KEYWORDS
Search based so/f_tware engineering, so/f_tware analytics, parameter
tuning, data analytics for so/f_tware engineering, deep learning, SVM,
diÔ¨Äerential evolution
ACM Reference format:
Wei Fu, Tim Menzies. 2017. Easy over Hard: A Case Study on Deep Learning.
InProceedings of 2017 11th Joint Meeting of the European So/f_tware Engineering
Conference and the ACM SIGSOFT Symposium on the Foundations of So/f_t-
ware Engineering, Paderborn, Germany, September 4-8, 2017 (ESEC/FSE‚Äô17),
12 pages.
DOI: 10.1145/3106237.3106256
1 INTRODUCTION
/T_his paper extends a prior result from ASE‚Äô16 by Xu et al. [ 74]
(herea/f_ter, XU). XU described a method to explore large programmer
discussion forums, then uncover related, but separate, entries. /T_his
is an important problem. Modern SE is evolving so fast that these
forums contain more relevant and recent comments on current
technologies than any textbook or research article.
In their work, XU predicted whether two questions posted on
Stack Over/f_low are semantically linkable. Speci/f_ically, XU de/f_ine
a question along with its entire set of answers posted on Stack
Over/f_low as a knowledge unit (KU). If two knowledge units are
ESEC/FSE‚Äô17, Paderborn, Germany
2017. 978-1-4503-5105-8/17/09. . . $15.00
DOI: 10.1145/3106237.3106256semantically related, they are considered as linkable knowledge
units.
In their paper, they used a convolution neural network (CNN), a
kind of deep learning method [ 42], to predict whether two KUs are
linkable. Such CNNs are highly computationally expensive, o/f_ten
requiring network composed of 10 to 20 layers, hundreds of millions
of weights and billions of connections between units [ 42]. Even
with advanced hardware and algorithm parallelization, training
deep learning models still requires hours to weeks. For example:
XU report that their analysis required 14 hours of CPU.
Le [40] used a cluster with 1,000 machines (16,000 cores) for
three days to train a deep learner.
/T_his paper debates what methods should be recommended to
those wishing to repeat the analysis of XU. We focus on whether
using simple and faster methods can achieve the results that are cur-
rently achievable by the state-of-art deep learning method. Speci/f_i-
cally, we repeat XU‚Äôs study using DE (diÔ¨Äerential evolution [ 62]),
which serves as a hyper-parameter optimizer to tune XU‚Äôs base-
line method, which is a conventional machine learning algorithm,
support vector machine (SVM). Our study asks:
RQ1 : Can we reproduce XU‚Äôs baseline results (Word Embedding +
SVM)? Using such a baseline, we can compare our methods to those
of XU.
RQ2 : Can DE tune a standard learner such that it outperforms
XU‚Äôs deep learning method? We apply diÔ¨Äerential evolution to tune
SVM. In terms of precision, recall and F1-score, we observe that the
tuned SVM method outperforms CNN in most evaluation scores.
RQ3 : Is tuning SVM with DE faster than XU‚Äôs deep learning
method? Our DE method is 84 times faster than CNN.
We oÔ¨Äer these results as a cautionary tale to the so/f_tware an-
alytics community. While deep learning is an exciting new tech-
nique, the bene/f_its of this method need to be carefully assessed with
respect to its computational cost. More generally, if researchers
deploy some new and expensive process (like deep learning), that
work should be baselined against some simpler and faster alterna-
tives
/T_he rest of this paper is organized as follows. Section 2 describes
the background and related work on deep learning and parameter
tuning in SE. Section 3 explains the case study problem and the
proposed tuning method investigated in this study, then Section 4
describes the experimental se/t_tings of our study, including research
questions, data sets, evaluation measures and experimental design.
Section 5 presents the results. Section 6 discusses implications from
the results and the threats to the validity of our study. Section 7
concludes the paper and discusses the future work.
Before beginning, we digress to make two points. Firstly, just
because ‚ÄúDE + SVM‚Äù beats deep learning in this application, this
does not mean DE is always the superior method for all other
so/f_tware analytics applications. No learner works best over all
problems [ 73]‚Äì the trick is to try several approaches and select thearXiv:1703.00133v2  [cs.SE]  24 Jun 2017ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
one that works best on the local data. Given the low computational
cost of DE (10 minutes vs 14 hours), DEs are an obvious and low-cost
candidate for exploring such alternatives.
Secondly, to enable other researchers to repeat, improve, or
refute our results, all our scripts and data are freely available on-
line Github1.
2 BACKGROUND AND RELATED WORK
2.1 Why Explore Faster So/f_tware Analytics?
/T_his section argues that avoiding slow methods for so/f_tware ana-
lytics is an open and urgent issue.
Researchers and industrial practitioners now routinely make
extensive use of so/f_tware analytics to discover (e.g.) how long
it will take to integrate the new code [ 17], where bugs are most
likely to occur [ 54], who should /f_ix the bug [ 2], or how long it will
take to develop their code [ 34,35,50]. Large organizations like
Microso/f_t routinely practice data-driven policy development where
organizational policies are learned from an extensive analysis of
large data sets collected from developers [7, 65].
But the more complex the method, the harder it is to apply the
analysis. Fisher et al. [ 20] characterizes so/f_tware analytics as a
work /f_low that distills large quantities of low-value data down to
smaller sets of higher value data. Due to the complexities and
computational cost of SE analytics, ‚Äúthe luxuries of interactivity,
direct manipulation, and fast system response are gone‚Äù [ 20]. /T_hey
characterize modern cloud-based analytics as a throwback to the
1960s-batch processing mainframes where jobs are submi/t_ted and
then analysts wait, wait, and wait for results with ‚Äúli/t_tle insight into
what is really going on behind the scenes, how long it will take, or
how much it is going to cost‚Äù [ 20]. Fisher et al. [ 20] document the
issues seen by 16 industrial data scientists, one of whom remarks
‚ÄúFast iteration is key, but incompatible with the
jobs are submi/t_ted and processed in the cloud. It
is frustrating to wait for hours, only to realize you
need a slight tweak to your feature set‚Äù.
Methods for improving the quality of modern so/f_tware analytics
have made this issue even more serious. /T_here has been continuous
development of new feature selection [ 25] and feature discover-
ing [ 28] techniques for so/f_tware analytics, with the most recent
ones focused on deep learning methods. /T_hese are all exciting in-
novations with the potential to dramatically improve the quality of
our so/f_tware analytics tools. Yet these are all CPU/GPU-intensive
methods. For instance:
Learning control se/t_tings for learners can take days to weeks to
years of CPU time [22, 64, 69].
Lam et al. needed weeks of CPU time to combine deep learning
and text mining to localize buggy /f_iles from bug reports [39].
Gu et al. spent 240 hours of GPU time to train a deep learning
based method to generate API usage sequences for given natural
language query [24].
Note that the above problem is not solvable by waiting for faster
CPUs/GPUs. We can no longer rely on Moore‚Äôs Law [ 51] to double
our computational power every 18 months. Power consumption and
heat dissipation issues eÔ¨Äect block further exponential increases to
1h/t_tps://github.com/WeiFoo/EasyOverHardCPU clock frequencies [ 38]. Cloud computing environments are
extensively monetized so the total /f_inancial cost of training models
can be prohibitive, particularly for long running tasks. For example,
it would take 15 years of CPU time to learn the tuning parameters
of so/f_tware clone detectors proposed in [ 69]. Much of that CPU
time can be saved if there is a faster way.
2.2 What is Deep Learning?
Deep learning is a branch of machine learning built on multiple lay-
ers of neural networks that a/t_tempt to model high level abstractions
in data. According to LeCun et al. [ 42], deep learning methods are
representation-learning methods with multiple levels of represen-
tation, obtained by composing simple but non-linear modules that
each transforms the representation at one level (starting with the
raw input) into a representation at a higher, slightly more abstract
level. Compared to the conventional machine learning algorithms,
deep learning methods are very good at exploring high-dimensional
data.
By utilizing extensive computational power, deep learning has
been proven to be a very powerful method by researchers in many
/f_ields [ 42], like computer vision and natural language process-
ing [4,37,47,60,63]. In 2012, Convolution neural networks method
won the ImageNet competition [ 37], which achieves half of the error
rates of the best competing approaches. A/f_ter that, CNN became the
dominant approach for almost all recognition and detection tasks
in computer vision community. CNNs are designed to process the
data in the form of multiple arrays, e.g., image data. According to
LeCun et al. [ 42], recent CNN methods are usually a huge network
composed of 10 to 20 layers, hundreds of millions of weights and
billions of connections between units. With advanced hardware
and algorithm parallelization, training such model still need a few
hours [ 42]. For the tasks that deal with sequential data, like text
and speech, recurrent neural networks (RNNs) have been shown
to work well. RNNs are found to be good at predicting the next
character or word given the context. For example, Graves et al. [ 23]
proposed to use long short-term memory (LSTM) RNNs to perform
speech recognition, which achieves a test set error of 17 :7% on the
benchmark testing data. Sutskever et al. [ 63] used two multiplelay-
ered LSTM RNNs to translate sentences in English to French.
2.3 Deep Learning in SE
We study deep learning since, recently, it has a/t_tracted much at-
tentions from researchers and practitioners in so/f_tware commu-
nity [ 15,24,39,52,68,70,71,74,77]. /T_hese researchers applied deep
learning techniques to solve various problems, including defect pre-
diction, bug localization, clone code detection, malware detection,
API recommendation, eÔ¨Äort estimation and linkable knowledge
prediction.
We /f_ind that this work can be divided into two categories:
Treat deep learning as a feature extractor, and then apply other
machine learning algorithms to do further work [15, 39, 68].
Solve problems directly with deep learning [ 24,52,70,71,74,77].
2.3.1 Deep Learning as Pre-Processor. Lam et al. [ 39] proposed
an approach to apply deep neural network in combination with
rVSM to automatically locate the potential buggy /f_iles for a givenEasy over Hard: A Case Study on Deep Learning ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
bug report. By comparing it to baseline methods (Naive Bayes [ 32],
learn-to-rank [ 76], BugLocator [ 79]), Lam et al. reported, 16.2-46.4 %,
8-20.8 % and 2.7-20.7 % higher top-1 accuracy than baseline methods,
respectively [ 39]. /T_he training time for deep neural network was
reported from 70 to 122 minutes for 6 projects on a computer with
32 cores 2.00GHz CPU, 126 GB memory. However, the runtime
information of the baseline methods was not reported.
Wang et al. [ 68] applied deep belief network to automatically
learn semantic features from token vectors extracted from the stud-
ied so/f_tware program. A/f_ter applying deep belief network to gener-
ate features from so/f_tware code, Naive Bayes, ADTree and Logistic
Regression methods are used to evaluate the eÔ¨Äectiveness of fea-
ture generation, which is compared to the same learners using
traditional static code features (e.g. McCabe metrics, Halstead‚Äôs
eÔ¨Äort metrics and CK object-oriented code mertics [ 13,26,31,45]).
In terms of runtime, Wang et al. only report time for generating
semantics features with deep belief network, which ranged from
8 seconds to 32 seconds [ 68]. However, the time for training and
tuning deep belief network is missing. Furthermore, to compare
the eÔ¨Äectiveness of deep belief network for generating features
with methods that extract traditional static code features in terms
of time cost, it would be favorable to include all the time spent on
feature extraction, including paring source code, token generation
and token mapping for both deep belief network and traditional
methods (i.e., an end-to-end comparison).
Choetkiertikul et al. [ 15] proposed to apply deep learning tech-
niques to solve eÔ¨Äort estimation problems on user story level.
Speci/f_ically, Choetkiertikul et al. [ 15] proposed to leverage long
short-term memory (LSTM) to learn feature vectors from the title,
description and comments associated with an issue report and af-
ter that, regular machine learning techniques, like CART, Random
Forests, Linear Regression and Case-Based Reasoning are applied
to build the eÔ¨Äort estimation models. Experimental results show
that LSTM has a signi/f_icant improvement over the baseline method
bag-of-words. However, no further information regarding runtime
as well as experimental hardware is reported for both methods and
there is no cost of this deep learning method at all.
2.3.2 Deep Learning as a Problem Solver. White et al. [ 70,71]
applied recurrent neural networks, a type of deep learning tech-
niques, to address code clone detection and code suggestion. /T_hey
reported, the average training time for 8 projects were ranging from
34 seconds to 2977 seconds for each epoch on a computer with two
3.3 GHz CPUs and each project required at least 30 epochs [ 70].
Speci/f_ically, for the JDK project in their experiment, it would take
25 hours on the same computer to train the models before ge/t_ting
prediction. For the time cost for code suggestions, authors did not
mention any related information [71].
Gu et al. [ 24] proposed a recurrent neural network (RNN) based
method, D EEPAPI, to generate API usage sequences for a given natu-
ral language query. Compared with the baseline method SWIM [ 57]
and Lucene + UP-Miner [ 67], D EEPAPI improved the performance
signi/f_icantly. However, that improvement came at a cost: that
model was trained with a Nivdia K20 GPU for 240 hours [24].
XU [ 74] utilized neural language model and convolution neural
network (CNN) to learn word-level and document-level features to
predict semantically linkable knowledge units on Stack Over/f_low.In terms of performance metrics, like precision, recall and F1-score,
CNN method was evaluated much be/t_ter than the baseline method
support vector machine (SVM). However, once again, that perfor-
mance improvement came at a cost: their deep learner required 14
hours to train CNN model on a 2.5GHz PC with 16 GB RAM [74].
Yuan et al. [ 77] proposed a deep belief network based method
for malware detection on Android apps. By training and testing
the deep learning model with 200 features extracted from static
analysis and dynamic analysis from 500 sampled Android app, they
got 96 :5% accuracy for deep learning method and 80% for one
baseline method, SVM [ 77]. However, they did not report any
runtime comparison between the deep learning method and other
classic machine learning methods.
Mou et al. [ 52] proposed a tree-based convolutional neural net-
work for programming language processing, in which a convolution
kernel is designed over programs‚Äô abstract syntax trees to capture
structural information. Results show that their method achieved
94% accuracy, which is be/t_ter than the baseline method RBF SVM
88:2% on program classi/f_ication problem [ 52]. However, Mou et
al. [52] did not discuss any runtime comparison between the pro-
posed method and baseline methods.
2.3.3 Issues with Deep Learning. In summary, deep learning is
used extensively in so/f_tware engineering community. A common
pa/t_tern in that research is to:
Report deep learning‚Äôs bene/f_its, but not its CPU/GPU cost [ 15,
52, 71, 77];
Or simply show the cost, without further analysis [ 24,39,68,70,
74].
Since deep learning techniques cost large amount of time and com-
putational resources to train its model, one might question whether
the improvements from deep learning is worth the costs. Are there
any simple techniques that achieve similar improvements with less
resource costs? To investigate how simple methods could improve
baseline methods, we select XU [ 74] study as a case study. /T_he
reasons are as follows:
Most deep learning paper‚Äôs baseline methods in SE are either
not publicly available or too complex to implement [ 39,70]. XU
de/f_ine their baseline methods precisely enough so others can
con/f_idently reproduce it locally. XU‚Äôs baseline method is SVM
learner, which is available in many machine learning toolboxes.
Further, it is not yet common practice for deep learning re-
searchers in SE community to share their implementations and
data [15, 24, 39, 68, 70, 71], where a tiny diÔ¨Äerence may lead to
a huge diÔ¨Äerence in the results. Even though XU do not share
their CNN tool, their training and testing data are available on-
line, which can be used for our proposed method. Since the
same training and testing data are used for XU‚Äôs CNN and our
proposed method, we can compare results of our method to their
CNN results.
Some studies do not report their runtime and experimental envi-
ronment, which makes it harder for us to systematically compare
our results with theirs in terms of computational costs [ 15,52,71,
77]. XU clearly report their experimental hardware and runtime,
which will be easier for us compare our computational costs to
theirs.ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
2.4 Parameter Tuning in SE
In this paper, we use DE as an optimizer to do parameter tuning
for SVM, which achieves results that are competitive with deep
learning. /T_his section discusses related work on parameter tuning
in SE community.
Machine learning algorithms are designed to explore the in-
stances to learn the bias. However, most of these algorithms are
controlled by parameters such as:
/T_he maximum allowed depth of decision tree built by CART;
/T_he number of trees to be built within a Random Forest.
Adjusting these parameters is called hyperparameter optimzia-
tion. It is a well well explored approach in other communities [ 9,44].
However, in SE, such parameter optimization is not a common
task (as shown in the following examples).
In the /f_ield of defect prediction , Fu et al. [ 21] surveyed hundreds
of highly cited so/f_tware engineering paper about defect prediction.
/T_heir observation is that most so/f_tware engineering researchers did
not acknowledge the impact of tunings (exceptions: [ 43,64]) and
use the ‚ÄúoÔ¨Ä-the-shelf‚Äù data miners. For example, Elish et al. [ 18]
compared support vector machines to other data miners for the
purposes of defect prediction. However, the Elish et al. paper makes
no mention of any SVM tuning study [ 18]. More details about their
survey refer to [21].
In the /f_ield of topic modeling , Agrawal et al. [ 1] investigated the
impact of parameter tuning on Latent Dirichlet Allocation (LDA).
LDA is a widely used technique in so/f_tware engineering /f_ield to
/f_ind related topics within unstructured text, like topic analytics on
Stack Over/f_low [ 5] and source code analysis [ 10]. Agrawal et al.
found that LDA suÔ¨Äers from conclusion instability (diÔ¨Äerent input
orderings can lead to very diÔ¨Äerent results) that is a result of poor
choice of the LDA control parameters [ 1]. Yet, in their survey of
LDA use in SE, they found that very few researchers (4 out of 57
papers) explored the bene/f_its of parameter tuning for LDA.
One troubling trend is that, in the few SE papers that perform
tuning, they do so using methods heavily deprecated in the ma-
chine learning community. For example, two SE papers that use
tuning [ 43,64], apply a simple grid search to explore the potential
parameter space for optimal tunings (such grid searchers run one
for-loop for each parameter being optimized). However, Bergstra
et al. [ 9] and Fu et al. [ 22] argue that random search methods (e.g.
the diÔ¨Äerential evolution algorithm used here) are be/t_ter than grid
search in terms of eÔ¨Éciency and performance.
3 METHOD
3.1 Research Problem
/T_his section is an overview of the the task and methods used by
XU. /T_heir task was to predict relationships between two knowledge
units (questions with answers) on Stack Over/f_low. Speci/f_ically, XU
divided linkable knowledge unit pairs into 4 diÔ¨Äerence categories
namely, duplicate ,direct link ,indirect link andisolated , based on its
relatedness. /T_he de/f_inition of these four categories are shown in
Table 1 [74]:
In that paper, XU provided the following two methods as base-
lines [74]:Table 1: Classes of Knowledge Unit Pairs.
Class Description
Duplicate/T_hese two knowledge units are addressing the
same question.
Direct linkOne knowledge unit can help to answer the
question in the other knowledge unit.
Indirect linkOne knowledge provides similar information to
solve the question in the other knowledge unit,
but not a direct answer.
Isolated/T_hese two knowledge units discuss unrelated
questions.
TF-IDF + SVM: a multi-class SVM classi/f_ier with 36 textual
features generated based on the TF and IDF values of the words
in a pair of knowledge units.
Word Embedding + SVM: a multi-class SVM classi/f_ier with word
embedding generated by the word2vec model [47].
Both of these two baseline methods are compared against their
proposed method, Word Embedding + CNN.
In this study, we select Word Embedding + SVM as the baseline
because it uses word embedding as the input, which is the same as
the Word Embedding + CNN method by XU.
3.2 Learners and /T_heir Parameters
SVM has been proven to be a very successful method to solve text
classi/f_ication problem. A SVM seeks to minimize misclassi/f_ication
errors by selecting a boundary or hyperplane that leaves the max-
imum margin between positive and negative classes (where the
margin is de/f_ined as the sum of the distances of the hyperplane
from the closest point of the two classes [29]).
Like most machine learning algorithms, there are some parame-
ters associated with SVM to control how it learns. In XU‚Äôs experi-
ment, they used a radial-bias function (RBF) for their SVM kernel
and setŒ≥to 1¬ùk, where kis 36 for TF-IDF + SVM method and
200 for Word Embedding + SVM method. For other parameters,
XU mentioned that grid search was applied to optimize the SVM
parameters, but no further information was disclosed.
For our work, we used the SVM module from Scikit-learn [ 55], a
Python package for machine learning, where the parameters shown
in Table. 2 are selected for tuning. Parameter Cis to set the amount
of regularization, which controls the tradeoÔ¨Ä between the errors
on training data and the model complexity. A small value for C
will generate a simple model with more training errors, while a
large value will lead to a complicated model with fewer errors.
Kernel is to introduce diÔ¨Äerent nonlinearities into the SVM model
by applying kernel functions on the input data. Gamma de/f_ines
how far the in/f_luence of a single training example reaches, with
low values meaning ‚Äòfar‚Äô and high values meaning ‚Äòclose‚Äô. coef0 is
an independent parameter used in sigmod and polynomial kernel
function.
As to why we used the ‚ÄúTuning Range‚Äù shown in Table 2, and not
some other ranges, we note that (a) those ranges include the defaults
and also XU‚Äôs values; (b) the results presented below show that by
exploring those ranges, we achieved large gains in the performance
of our baseline method. /T_his is not to say that larger tuning rangesEasy over Hard: A Case Study on Deep Learning ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
Table 2: List of Parameters Tuned by /T_his Paper.
Parameters Default Xue et al. Tuning Range Description
C 1.0 unknown [1, 50] Penalty parameter C of the error term.
kernel ‚Äòrbf‚Äô ‚Äòrbf‚Äô [‚Äòliner‚Äô, ‚Äòpoly‚Äô, ‚Äòrbf‚Äô, ‚Äòsigmoid‚Äô] Specify the kernel type to be used in the algorithms.
gamma 1/nfeatures 1¬ù200 [0, 1] Kernel coeÔ¨Écient for ‚Äòrbf‚Äô, ‚Äòpoly‚Äô and ‚Äòsigmoid‚Äô.
coef0 0 unknown [0, 1] Independent term in kernel function. It is only used in ‚Äòpoly‚Äô and ‚Äòsigmoid‚Äô.
might not result in greater improvements. However, for the goals
of this paper (to show that tuning baseline method does ma/t_ter),
exploring just these ranges shown in Table 2 will suÔ¨Éce.
3.3 Learning Word Embedding
Learning word embeddings refers to /f_ind vector representations
of words such that the similarities between words can be captured
by cosine similarity of corresponding vector representations. It is
been shown that the words with similar semantic and syntactic are
found closed to each other in the embedding space [47].
Several methods have been proposed to generate word embed-
dings, like skip-gram [ 47], GloVe [ 56] and PCA on the word co-
occurrence matrix [ 41]. To replicate XU work, we used the contin-
uous skip-gram model (word2vec), which is a unsupervised word
representation learning method based on neural networks and also
used by XU [74].
/T_he skip-gram model learns vector representations of words by
predicting the surrounding words in a context window. Given a sen-
tence of words W=w1,w2,‚Ä¶,wn, the objective of skip-gram model
is to maximize the the average log probability of the surrounding
words:
1
nn√ï
i=1√ï
 cjc;j,0lo/afii10069.italp¬πwi+jjwi¬∫
where cis the context window size and wi+jandwirepresent
surrounding words and center word, respectively. /T_he probability
ofp¬πwi+jjwi¬∫is computed according to the so/f_tmax function:
p¬πwOjwI¬∫=exp¬π/v.altTwO/v.altwI¬∫
√çjWj
w=1exp¬π/v.altTw/v.altwI¬∫
where/v.altwIand/v.altwOare the vector representations of the input and
output vectors of w, respectively.√çjWj
w=1exp¬π/v.altTw/v.altwI¬∫normalizes
the inner product results across all the words. To improve the
computation eÔ¨Éciency, Mikolove et al. [ 47] proposed hierachical
so/f_tmax and negative sampling techniques. More details can be
found in Mikolove et al.‚Äôs study [47].
Skip-gram‚Äôs parameters control how that algorithm learns word
embeddings. /T_hose parameters include window size anddimen-
sionality of embedding space , etc. Zucoon et al. [ 80] found that
embedding dimensionality and context window size have no con-
sistent impact on retrieval model performance. However, Yang et
al. [75] showed that large context window and dimension sizes
are preferable to improve the performance when using CNN to
solve classi/f_ication tasks for Twi/t_ter. Since this work is to compare
performance of tuning SVM with CNN, where skip-gram model
is used to generate word vector representations for both of these
methods, tuning parameter of skip-gram model is beyond the scope
of this paper (but we will explore it in future work).1. Given a model (e.g., SVM) with ndecisions (e.g., n=4), TUNER calls
SAMPLE N=10ntimes. Each call generates one member of the
population popi2N.
2. TUNER scores each popiaccording to various objective scores o. In
the case of our tuning SVM, the objective ois to maximize F1-score
3. TUNER tries to each replace popiwith a mutant mbuilt using Storn‚Äôs
diÔ¨Äerential evolution method [ 62]. DE extrapolates between three other
members of population a;b;c. At probability p1, for each decision
ak2a, then mk=ak_¬πp1<rand¬π¬∫^¬π bk_ck¬∫¬∫.
4. Each mutant mis assessed by calling EVALUATE¬πmodel, prior=m¬∫;
i.e. by seeing what can be achieved within a goal a/f_ter /f_irst assuming
that prior =m.
5. To test if the mutant mis preferred to popi, TUNER simply compare
SCORE( m) with SCORE( popi). In case of our tuning SVM, the one with
higher score will be kept.
6. TUNER repeatedly loops over the population, trying to replace items
with mutants, until new be/t_ter mutants stop being found.
7. Return the best one in the population as the optimal tunings.
Figure 1: Procedure TUNER: strives to /f_ind ‚Äúgood‚Äù tunings
which maximizes the objective score of the model on train-
ing and tuning data. TUNER is based on Storn‚Äôs diÔ¨Äerential
evolution optimizer [62].
To train our word2vec model, 100 ;000 knowledge units tagged
with ‚Äújava‚Äù from Stack Over/f_low posts table (include titles, ques-
tions and answers) are randomly selected as a word corpus2. A/f_ter
applying proper data processing techniques proposed by XU, like
remove the unnecessary HTML tags and keep short code snippets
incode tag, then /f_it the corpus into gensim word2vec module [ 58],
which is a python wrapper over original word2vec package.
When converting knowledge units into vector representations,
for each word wiin the post processed knowledge unit (including
title, question and answers), we query the trained word2vec model
to get the corresponding word vector representation /v.alti. /T_hen the
whole knowledge unit with swords is converted to vector repre-
sentation by element-wise addition, U/v.alt=/v.alti/v.alt2:::/v.alts. /T_his
vector representation is used as the input data to SVM.
3.4 Tuning Algorithm
A tuning algorithm is an optimizer that drives the learner to explore
the optimal parameter in a given searching space. According to our
literature review, there are several searching algorithms used in
SE community: simulated annealing [19,46]; various genetic algo-
rithms [3,27,30] augmented by techniques such as diÔ¨Äerential evo-
lution [1,12,21,22,62],tabu search andsca/t_ter search [6,16,49];par-
ticle swarm optimization [72]; numerous decomposition approaches
2Without further explanation, all the experiment se/t_tings, including learner algorithms,
training/testing data split, etc, strictly follow XU‚Äôs work.ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
that use heuristics to decompose the total space into small prob-
lems, then apply a response surface methods [36];NSGA-II [78]and
NSGA-III [48].
Of all the mentioned algorithms, the simplest are simulated
annealing (SA) and diÔ¨Äerential evolution (DE), each of which can
be coded in less than a page of some high-level scripting language.
Our reading of the current literature is that there are more advocates
for diÔ¨Äerential evolution than SA. For example, Vesterstrom and
/T_homsen [ 66] found DE to be competitive with particle swarm
optimization and other GAs. DEs have already been applied before
for parameter tuning in SE community to do parameter tuning (e.g.
see [1,14,21,22,53]) . /T_herefore, in this work, we adopt DE as our
tuning algorithm and the main steps in DE is described in Figure 1.
4 EXPERIMENTAL SETUP
4.1 Research /Q_uestions
To systematically investigate whether tuning can improve the per-
formance of baseline methods compared with deep learning method,
we set the following three research questions:
RQ1 : Can we reproduce XU‚Äôs baseline results (Word Embedding +
SVM)?
RQ2 : Can DE tune a standard learner such that it outperforms
XU‚Äôs deep learning method?
RQ3 : Is tuning SVM with DE faster than XU‚Äôs deep learning
method?
RQ1 is to investigate whether our implementation of Word Em-
bedding + SVM method has the similar performance with XU‚Äôs
baseline, which makes sure that our following analysis can be gen-
eralized to XU‚Äôs conclusion. RQ2 and RQ3 lead us to investigate
whether tuning SVM comparable with XU‚Äôs deep learning from
both performance and cost aspects.
4.2 Dataset and Experimental Design
Our experimental data comes from Stack Over/f_low data dump of
September 20163, where the posts table includes all the questions
and answers posted on Stack Over/f_low up to date and the postlinks
table describes the relationships between posts, e.g., duplicate and
linked . As mentioned in Section 3.1, we have four diÔ¨Äerent types
of relationships in knowledge unit pairs. /T_herefore, linked type is
further divided into indirectly linked anddirectly linked . Overall,
four diÔ¨Äerent types of data are generated according the following
rules [74]:
Randomly select a pair of posts from the postlinks table, if the
value in PostLinkTypeId /f_ield for this pair of posts is 3, then this
pair of posts is duplicate posts. Otherwise they‚Äôre directly linked
posts.
Randomly select a pair of posts from the posts table, if this pair
of posts is linkable from each other according to postlinks table
and the distance between them are greater than 2 (which means
they are not duplicate or directly linked posts), then this pair of
posts is indirectly linked. If they‚Äôre not linkable, then this pair
of posts is isolated.
3h/t_tps://archive.org/details/stackexchange
Word2Vec
WordEmbeddingsLookup
Testing	KU	vectors
Predict
Results
Training	KU	pairs
New	Training	KU	vectorsTuningKU	vectorsLookup
SVM
100,000	KU	textsTrainEvaluateTrain
SVM
ParametersDE
Best	TuningsTrainTrain	Word2Vec
Train	LearnerTest	LearnerParameter	Tuning
Testing	KU	pairsFigure 2: /T_he Overall Work/f_low of Building Knowledge
Units Predictor with Tuned SVM
In this work, we use the same training and testing knowledge
unit pairs as XU [ 74]4, where 6,400 pairs of knowledge units for
training and 1,600 pairs for testing. And each type of linked knowl-
edge units accounts for 1 ¬ù4 in both training and testing data. /T_he
reasons that we used the same training and testing data as XU are:
It is to ensure that performance of our baseline method is as
closed to XU‚Äôs as possible.
Since deep learning method is way complicated compared to
SVM and a li/t_tle diÔ¨Äerence in implementations might lead to
diÔ¨Äerent results. To fairly compare with XU‚Äôs result, we can use
the performance scores of CNN method from XU‚Äôs study [ 74]
without any implementation bias introduced.
For training word2vec model, we randomly select 100,000 knowl-
edge units (title, question body and all the answers) from posts table
that are related to ‚Äújava‚Äù. A/f_ter that, all the training/tuning/testing
knowledge units used in this paper are converted into word embed-
ding representations by looking up each word in wrod2vec model
as described in Section 3.3.
As seen in Figure 2, instead of using all the 6,400 knowledge units
as training data, we split the original training data into new training
data and tuning data, which are used during parameter tuning
procedure for training SVM and evaluating candidate parameters
oÔ¨Äered by DE. A/f_terwards, the new training data is again /f_i/t_ted
into the SVM with the optimal parameters found by DE and /f_inally
the performance of the tuned SVM will be evaluated on the testing
data.
To reduce the potential variance caused by how the original train-
ing data is divided, 10-fold cross-validation is performed. Speci/f_ically,
each time one fold with 640 knowledge units pairs is used as the
tuning data, and the remaining folds with 5760 knowledge units
are used as the new training data, then the output SVM model will
be evaluated on the testing data. /T_herefore, all the performance
scores reported below are averaged values over 10 runs.
In this study, we use Wilcoxon single ranked test to statistically
compare the diÔ¨Äerences between tuned SVM and untuned SVM.
Speci/f_ically, the Benjamini-Hochberg (BH) adjusted p-value is used
to test whether a diÔ¨Äerence is statistically signi/f_icant at the level of
0:05 [8]. To measure the eÔ¨Äect size of performance scores between
4h/t_tps://github.com/XBWer/ASEDatasetEasy over Hard: A Case Study on Deep Learning ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
tuned SVM and untuned SVM, we compute CliÔ¨Ä‚Äôs Œ¥that is a non-
parametric eÔ¨Äect size measure [ 59]. As Romano et al. suggested,
we evaluate the magnitude of the eÔ¨Äect size as follows: negligible
(jŒ¥j<0:147 ), small (0 :147 <jŒ¥j<0:33), medium (0 :33<jŒ¥j<
0:474 ), and large (0.474 jŒ¥j) [59].
4.3 Evaluation Metrics
When evaluating the performance of tuning SVM on the multi-
class linkable knowledge units prediction problem, consistent with
XU [ 74], we use accuracy, precision, recall and F1-score as the
evaluation metrics.
Table 3: Confusion Matrix.
Classi/f_ied as
C1 C2 C3 C4ActualC1c11 c12 c13 c14
C2c21 c22 c23 c24
C3c31 c32 c33 c34
C4c41 c42 c43 c44
Given a multi-classi/f_ication problem with true labels C1,C2,C3
andC4, we can generate a confusion matrix like Table 3, where the
value of ciirepresents the number of instances that are correctly
classi/f_ied by the learner for class Ci.
Accuracy of the learner is de/f_ined as the number of correctly
classi/f_ied knowledge units over the total number of knowledge
units, i.e.,
accurac/y.alt=√ç
icii√ç
i√ç
jcij
where√ç
i√ç
jcijis the total number of knowledge units. For a given
type of knowledge units, Cj, the precision is de/f_ined as probability
of knowledge units pairs correctly classi/f_ied as Cjover the number
of knowledge unit pairs classi/f_ied as Cjand recall is de/f_ined as the
percentage of all Cjknowledge unit pairs correctly classi/f_ied. F1-
score is the harmonic mean of recall and precision. Mathematically,
precision, recall and F1-score of the learner for class Cjcan be
denoted as follows:
prec j=precision j=cjj√ç
icij
pdj =recall j=cjj√ç
icji
F1j =2pdjprec j¬ù¬πpdj+prec j¬∫
Where√ç
icijis the predicted number of knowledge units in class
Cjand√ç
icjiis the actual number of knowledge units in class Cj.
Recall from Algorithm 1 that we call diÔ¨Äerential evolution once
for each optimization goal. Generally, this goal depends on which
metric is most important for the business case. In this work, we
useF1 to score the candidate parameters because it controls the
trade-oÔ¨Ä between precision and recall, which is also consistent with
XU [74] and is also widely used in so/f_tware engineering community
to evaluate classi/f_ication results [21, 33, 46, 68].
5 RESULTS
In this section, we present our experimental results. To answer
research questions raised in Section 4.1, we conducted two experi-
ments:
Figure 3: Score Delta between Our SVM with XU‚Äôs SVM
in [74] in Terms of Precision, Recall and F1-score. Positive
Values Mean Our SVM is Better than XU‚Äôs SVM in Terms of
DiÔ¨Äerent Measures; Otherwise, XU‚Äôs SVM is better.
Compare performance of Word Embedding + SVM method in
XU [74] and our implementation;
Compare performance of our tuning SVM with DE method with
XU‚Äôs CNN deep learning method.
Since we used the same training and testing data sets provided by
XU [ 74] and conducted our experiment in the same procedure and
evaluated methods using the performance measures, we simply
used the results reported in the work by XU [ 74] for the perfor-
mance comparison.
RQ1: Can we reproduce XU‚Äôs baseline results (Word Em-
bedding + SVM)?
/T_his /f_irst question is important to our work since, without the
original tool released by XU, we need to insure that our reimple-
mentation of their baseline method (WordEmbedding + SVM) has a
similar performance to their work. Accordingly, we carefully follow
XU‚Äôs procedure [ 74]. We use the SVM learner from scikit-learn
with the se/t_ting Œ≥=1
200andkernel =‚Äúrbf‚Äù, which are used by XU.
A/f_ter that, the same training and testing knowledge unit pairs are
applied to SVM.
Table 4: Comparison of Our Baseline Method with XU‚Äôs. /T_he
Best Scores are Marked in Bold.
Metrics Methods DuplicateDirect
LinkIndirect
LinkIsolated Overall
PrecisionOur SVM 0.724 0.514 0.779 0.601 0.655
XU‚Äôs SVM 0.611 0.560 0.787 0.676 0.659
RecallOur SVM 0.525 0.492 0.970 0.645 0.658
XU‚Äôs SVM 0.725 0.433 0.980 0.538 0.669
F1-scoreOur SVM 0.609 0.503 0.864 0.622 0.650
XU‚Äôs SVM 0.663 0.488 0.873 0.600 0.656
AccuracyOur SVM 0.525 0.493 0.970 0.645 0.658
XU‚Äôs SVM - - - - 0.669
Table 4 and Figure 3 show the performance scores and corre-
sponding score delta between our implementation of WordEmbed-
ding + SVM with XU‚Äôs in terms of accuracy5, precision, recall and
5XU just report overall accuracy, not for each class, hence it is missing in this table.ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
Figure 4: Score Delta between Tuned SVM and CNN
method [74] in Terms of Precision, Recall and F1-score. Pos-
itive Values Mean Tuned SVM is Better than CNN in Terms
of DiÔ¨Äerent Measures; Otherwise, CNN is better.
F1-score. As we can see, when predicting these four diÔ¨Äerent types
of relatedness between knowledge unit pairs, our Word Embedding
+ SVM method has very similar performance scores to the baseline
method reported by XU in [ 74], with the maximum diÔ¨Äerence less
than 0 :2. Except for Duplicate class, where our baseline has a
higher precision (i.e., 0 :724 v.s. 0 :611) but a lower recall (i.e., 0 :525
v.s.0 :725).
Figure 3 presents the same results in a graphical format. Any
bar above zero means that our implementation has a be/t_ter perfor-
mance score than XU‚Äôs on predicting that speci/f_ic knowledge unit
relatedness class. As we can see, most of the diÔ¨Äerences (8
12) are
within 0.05 and the score delta of overall performance shows that
our implementation is a li/t_tle worse than XU‚Äôs implementation. For
this chart we conclude that:
Overall, our reimplementation of WordEmbedding + SVM
has very similar performance in all the evaluated metrics
compared to the baseline method reported in XU‚Äôs study [ 74].
/T_he signi/f_icance of this conclusion is that, moving forward, we are
con/f_ident that we can use our reimplementation of WordEmbed-
ding+SVM as a valid surrogate for the baseline method of XU.
RQ2: Can DE tune a standard learner such that it outper-
forms XU‚Äôs deep learning method?
To answer this question, we run the work/f_low of Figure 2, where
DE is applied to /f_ind the optimal parameters of SVM based on the
training and tuning data. /T_he optimal tunings are then applied on
the SVM model and the built learner is evaluated on testing data.
Note that, in this study, since we mainly focus on precision, recall
and F1-score measures where F1-score is the harmonic mean of
precision and recall, we use F1-score as the tuning goal for DE. In
other words, when tuning parameters, DE expects to /f_ind a pair of
candidate parameters that maximize F1-score.
Table 5 presents the performance scores of XU‚Äôs baseline, XU‚Äôs
CNN method and Tuned SVM for all metrics. /T_he highest score
for each relatedness class is marked in bold. Note that: Without
tuning, XU‚Äôs CNN method outperforms the baseline SVM in10
12
Figure 5: Score Delta between Tuned SVM and XU‚Äôs Baseline
SVM in Terms of Precision, Recall and F1-score. Positive Val-
ues Mean Tuned SVM is Better than XU‚Äôs SVM in Terms of
DiÔ¨Äerent Measures; Otherwise, XU‚Äôs SVM is better.
Table 5: Comparison of Tuned SVM with XU‚Äôs CNN Method.
/T_he Best Scores are Marked in Bold.
Metrics Methods DuplicateDirect
LinkIndirect
LinkIsolated Overall
PrecisionXU‚Äôs SVM 0.611 0.560 0.787 0.676 0.658
XU‚Äôs CNN 0.898 0.758 0.840 0.890 0.847
Tuned SVM 0.885 0.851 0.944 0.903 0.896
RecallXU‚Äôs SVM 0.725 0.433 0.980 0.538 0.669
XU‚Äôs CNN 0.898 0.903 0.773 0.793 0.842
Tuned SVM 0.860 0.828 0.995 0.905 0.897
F1-scoreXU‚Äôs SVM 0.663 0.488 0.873 0.600 0.656
XU‚Äôs CNN 0.898 0.824 0.805 0.849 0.841
Tuned SVM 0.878 0.841 0.969 0.909 0.899
evaluation metrics across all four classes. /T_he largest performance
improvement is 0 :47 for recall on Direct Link class. Note that this
result is consistent with XU‚Äôs conclusion that their CNN method
is superior to standard SVM. A/f_ter tuning SVM, the deep learning
method has no such advantage. Speci/f_ically, CNN has advantage
over tuned SVM in4
12evaluation metrics across all four classes.
Even when CNN performs be/t_ter that our tuning SVM method, the
largest diÔ¨Äerence is 0 :065 for Recall on Direct Link class, which is
less than 0 :1.
Figure 4 presents the same results in a graphical format. Any bar
above zero indicates that tuned SVM has a be/t_ter performance score
than CNN. In this /f_igure: CNN has a slightly be/t_ter performance
on Duplicate class for precision, recall and F1-score and a higher
recall on Direct link class. Across all of Figure 4, in8
12evaluation
scores, Tuned SVM has be/t_ter performance scores than CNN, with
the largest delta of 0 :222.
Figure 5 compares the performance delta of tuned SVM with
XU‚Äôs untuned SVM. We note that DE-based parameter tuning never
degrades SVM‚Äôs performance (since there are no negative values
in that chart). Tuning dramatically improves scores on predicting
some classes of KU relatedness. For example, the recall of pre-
dicting Direct link is increased from 0 :433 to 0 :903, which is 108%
improvement over XU‚Äôs untuned SVM (To be fair for XU, it is stillEasy over Hard: A Case Study on Deep Learning ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
Figure 6: Score Delta between Tuned SVM and Our Untuned
SVM in Terms of Precision, Recall and F1-score. Positive Val-
ues Mean Tuned SVM is Better than Our Untuned SVM in
Terms of DiÔ¨Äerent Measures; Otherwise, Our SVM is Better.
84% improvement over our untuned SVM). At the same time, the
corresponding precision and F1 scores of predicting Direct Link
are increased from 0 :560 to 0 :851 and 0 :488 to 0 :841, which are 52%
and 72% improvement over XU‚Äôs original report[ 74], respectively.
A similar pa/t_tern can also be observed in Isolated class. On average,
tuning helps improve the performance of XU‚Äôs SVM by 0 :238, 0 :228
and 0 :227 in terms of precision, recall and F1-score for all four KU
relatedness classes. Figure 6 compares the tuned SVM with our un-
tuned SVM. We note that we get the similar pa/t_terns that observed
in Figure 5. All the bars are above zero, etc.
Based on the performance scores in Table 5 and score delta in
Figure 4, Figure 5 and Figure 6, we can see that:
Parameter tuning can dramatically improve the performance of
Word Embedding + SVM (the baseline method) for the multi-
class KU relatedness prediction task;
With the optimal tunings, the traditional machine learning
method, SVM, if not be/t_ter, is at least comparable with deep
learning methods (CNN).
When discussing this result with colleagues, we are sometimes
asked for a statistical analysis that con/f_irms the above /f_inding.
However, due the lack of evaluation score distributions of the CNN
method in [ 74], we cannot compare their single value with our
results from 10 repeated runs. However, according to Wilcoxon
singed rank test over 10 runs results, tuned SVM performs sta-
tistically be/t_ter than our untuned SVM in terms of all evaluation
measures on all four classes ( p<0:05). According to CliÔ¨Ä Œ¥values,
the magnitude of diÔ¨Äerence between tuned SVM and our untuned
SVM is not trivial ( jŒ¥j>0:147) for all evaluation measures.
Overall, the experimental results and our analysis indicate that:
In the evaluation conducted here, the deep learning method,
CNN, does not have any performance advantage over our
tuning approach.
RQ3: Is tuning SVM with DE faster than XU‚Äôs deep learn-
ing method?When comparing the runtime of two learning methods, it obvi-
ously should be conducted under the same hardware se/t_tings. Since
we adopt the CNN evaluation scores from [ 74], we can not run on
our tuning SVM experiment under the exactly same system set-
tings. To allow readers to have a objective comparison, we provide
the experimental environment as shown in Table 6. To obtain the
runtime of tuning SVM, we recorded the start time and end time of
the program execution, including parameter tuning, training model
and testing model.
Table 6: Comparison of Experimental Environment
Methods OS CPU RAM
Tuning SVM MacOS 10.12 Intel Core i5 2.7 GHz 8 GB
CNN Windows 7 Intel Core i7 2.5 GHz 16 GB
According to XU, it took 14 hours to train their CNN model into
a low loss convergence ( <e 3) [74]. Our work, on the other hand
only takes 10 minutes to run SVM with parameter tuning by DE
on a similar environment. /T_hat is, the simple parameter tuning
method on SVM is 84 Xfaster than XU‚Äôs deep learning method.
Compared to CNN method, tuning SVM is about 84 Xfaster
in terms of model building.
/T_he signi/f_icance of this /f_inding is that, in this case study, CNN
was neither be/t_ter in performance scores (see RQ2) nor runtimes.
CNN‚Äôs extra runtimes are a particular concern since (a) they are
very long; and (b) these would be incurred anytime researchers
wants to update the CNN model with new data or wanted to validate
the XU result.
6 DISCUSSION
6.1 Why DE+SVM works?
Parameter tuning matters . As mentioned in Section 2.4, the de-
fault parameter values set by the algorithm designers could generate
a good performance on average but may not guarantee the best
performance for the local data [ 9,21]. Given that, it is most strange
to report that most SE researchers ignore the impacts of parame-
ter tuning when they utilize various machine learning methods to
conduct so/f_tware analytic (evidence: see our reviews in [ 1,21,22]).
/T_he conclusion of this work must be to stress the importance of this
kind of tuning, using local data, for any future so/f_tware analytics
study.
Better explore the searching space . It turns out that one
exception to our statement that ‚Äúmost researchers do not tune‚Äù is
the XU study. In that work, they unsuccessfully perform parameter
tuning, but with with grid search. In such a grid search, for N
parameters to be tuned, Nfor loops are created to run over a range
of se/t_tings for each parameter. While a widely used method, it
is o/f_ten deprecated. For example, Bergstra et al.[ 9] note that grid
search jumps through diÔ¨Äerent parameter se/t_tings between some
minandmax values of pre-de/f_ined tuning range. /T_hey warn that
such jumps may actually skip over the critical tuning values. On
the other hand, DE tuning values are adjusted based on be/t_ter
candidates from previous generations. Hence DE is more likely
than grid search to ‚Äú/f_ill in the gaps‚Äù between the initialized values.ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
/T_hat said, although DE +SVM works in this study, it does not
mean DE is the best parameter tuner for all SE tasks. We encourage
more researchers to explore faster and more eÔ¨Äective parameter
tuners in this direction.
6.2 Implication
Beyond the speci/f_ics of this case study, what general principles can
we take from the above work?
Understand the task. One reason to try diÔ¨Äerent tools for the
same task is to be/t_ter understand the task. /T_he more we understand
a task, the be/t_ter we can match tools to that task. Tools that are
poorly matched to task are usually complex and/or slow to execute.
In the case study of this paper, we would say that
Deep learning is a poor match to the task of predicting whether
two questions posted on Stack Over/f_low are semantically link-
able since it is so slow;
DiÔ¨Äerential evolution tuning SVM is a much be/t_ter match since
it is so fast and obtain competitive performance.
/T_hat said, it is important to stress that the point of this study is
not to deprecate deep learning. /T_here are many scenarios were
we believe deep learning would be a natural choice (e.g. when
analyzing complex speech or visual data). In SE, it is still an open
research question that in which scenario deep learning is the best
choice. Results from this paper show that, at least for classi/f_ication
tasks like knowledge unit relatedness classi/f_ication on Stack Over-
/f_low, deep learning does not have much advantage over well tuned
conventional machine learning methods. However, as we be/t_ter
understand SE tasks, deep learning could be used to address more
SE problems, which require more advanced arti/f_icial intelligence.
Treat resource constraints as design challenges. As a gen-
eral engineering principle, we think it insightful to consider the
resource cost of a tool before applying it. It turns out that this
is a design pa/t_tern used in contemporary industry. According to
Calero and Pa/t_tini [ 11], many current commercial redesigns are
motivated (at least in part) by arguments based on sustainability
(i.e. using fewer resources to achieve results). In fact, they say that
managers used sustainability-based redesigns to motivate extensive
cost-cu/t_ting opportunities.
6.3 /T_hreads to Validity
/T_hreats to internal validity concern the consistency of the results
obtained from the result. In our study, to investigate how tuning
can improve the performance of baseline methods and how well
it perform compared with deep learning method. We select XU‚Äôs
Word Embedding + SVM baseline method as a case study. Since
the original implementation of Word Embedding + SVM (baseline
2 method in [ 74]) is not publicly available, we have to reimplement
our version of Word Embedding + SVM as the baseline method
in this study. As shown in RQ1, our implementation has quite
similar results to XU‚Äôs on the same data sets. Hence, we believe
that our implementation re/f_lect the original baseline method in
Xu‚Äôs study [74].
/T_hreats to external validity represent if the results are of rele-
vance for other cases, or the ability to generalize the observations in
a study. In this study, we compare our tuning baseline method with
deep learning method, CNN, in terms of precision, recall, F1-scoreand accuracy. /T_he experimental results are quite consistent for
this knowledge units relatedness prediction task. Nonetheless, we
do not claim that our /f_indings can be generalized to all so/f_tware
analytics tasks. However, those other so/f_tware analytics tasks o/f_ten
apply deep learning methods on classi/f_ication tasks [ 15,68] and so
it is quite possible that the methods of this paper (i.e., DE-based
parameter tuning) would be widely applicable, elsewhere.
7 CONCLUSION
In this paper, we perform a comparative study to investigate how
tuning can improve the baseline method compared with state-of-
the-art deep learning method for predicting knowledge units relat-
edness on Stack Over/f_low. Our experimental results show that:
Tuning improves the performance of baseline methods. At least
for Word Embedding + SVM (baseline in [ 74]) method, if not
be/t_ter, it performs as well as the proposed CNN method in [ 74].
/T_he baseline method with parameter tuning runs much faster
than complicated deep learning. In this study, tuning SVM runs
84Xfaster than CNN method.
8 ADDENDUM
As this paper was going to going to press we learned of a new
deep learning methods that, according to its creators, runs 20 times
faster than standard deep learning [ 61]. Note that in that paper, the
authors say their faster method does not produce be/t_ter results‚Äì in
fact, their method generated solutions that were a small fraction
worse than ‚Äúclassic‚Äù deep learning. Hence, that paper does not
invalidate our result since (a) our DE-based method sometimes
produced be/t_ter results than classic deep learning and (b) our DE
runs 84 times faster (i.e. much faster runtimes than those reported
in [61]).
/T_hat said, this new fast deep learner deserves our close a/t_tention
since, using it, we conjecture that our DE tools could solve an open
problem in the deep learning community; i.e. how to /f_ind the best
con/f_igurations inside a deep learner faster.
Based on the results of this study, we recommend that before
applying deep learning method on SE tasks, implement simpler
techniques. /T_hese simpler methods could be used, at the very least,
for comparisons against a baseline. In this particular case of deep
learning vs DE, the extra computational eÔ¨Äort is so very minor (10
minutes on top of 14 hours), that such a ‚Äútry-with-simpler‚Äù should
be standard practice.
As to the future work, we will explore more simple techniques
to solve SE tasks and also investigate how deep learning techniques
could be applied eÔ¨Äectively in so/f_tware engineering /f_ield.
ACKNOWLEDGEMENTS
/T_he work is partially funded by an NSF award #1302169.
REFERENCES
[1]Amritanshu Agrawal, Wei Fu, and Tim Menzies. 2016. What is wrong with
topic modeling?(and how to /f_ix it using search-based se). arXiv preprint
arXiv:1608.08176 (2016).
[2] John Anvik, Lyndon Hiew, and Gail C Murphy. 2006. Who should /f_ix this bug?.
InProceedings of the 28th International Conference on So/f_tware Engineering . ACM,
361‚Äì370.Easy over Hard: A Case Study on Deep Learning ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany
[3] Andrea Arcuri and Gordon Fraser. 2011. On parameter tuning in search based
so/f_tware engineering. In International Symposium on Search Based So/f_tware
Engineering . Springer, 33‚Äì47.
[4] Itamar Arel, Derek C Rose, and /T_homas P Karnowski. 2010. Research Frontier:
Deep Machine Learning‚Äìa New Frontier in Arti/f_icial Intelligence Research. IEEE
Computational Intelligence Magazine 5, 4 (2010), 13‚Äì18.
[5] Anton Barua, Stephen W /T_homas, and Ahmed E Hassan. 2014. What are devel-
opers talking about? an analysis of topics and trends in stack over/f_low. Empirical
So/f_tware Engineering 19, 3 (2014), 619‚Äì654.
[6] Ricardo P Beausoleil. 2006. ‚ÄúMOSS‚Äù multiobjective sca/t_ter search applied to non-
linear multiple criteria optimization. European Journal of Operational Research
169, 2 (2006), 426‚Äì449.
[7] Andrew Begel and /T_homas Zimmermann. 2014. Analyze this! 145 questions for
data scientists in so/f_tware engineering. In Proceedings of the 36th International
Conference on So/f_tware Engineering . ACM, 12‚Äì23.
[8] Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate: a
practical and powerful approach to multiple testing. Journal of the royal statistical
society. Series B (Methodological) (1995), 289‚Äì300.
[9] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter
optimization. Journal of Machine Learning Research 13, Feb (2012), 281‚Äì305.
[10] David Binkley, Daniel Heinz, Dawn Lawrie, and Justin Overfelt. 2014. Under-
standing LDA in source code analysis. In Proceedings of the 22nd International
Conference on Program Comprehension . ACM, 26‚Äì36.
[11] Coral Calero and Mario Pia/t_tini. 2015. Green in So/f_tware Engineering. (2015).
[12] Jos¬¥e M Chaves-Gonz ¬¥alez and Miguel A P ¬¥erez-Toledano. 2015. DiÔ¨Äerential evolu-
tion with Pareto tournament for the multi-objective next release problem. Appl.
Math. Comput. 252 (2015), 1‚Äì13.
[13] Shyam R Chidamber and Chris F Kemerer. 1994. A metrics suite for object
oriented design. IEEE Transactions on So/f_tware Engineering 20, 6 (1994), 476‚Äì493.
[14] Ibtissem Chiha, J Ghabi, and Noureddine Liouane. 2012. Tuning PID controller
with multi-objective diÔ¨Äerential evolution. In 2012 5th International Symposium
on Communications, Control and Signal Processing . IEEE, 1‚Äì4.
[15] Morakot Choetkiertikul, Hoa Khanh Dam, Truyen Tran, Trang Pham, Aditya
Ghose, and Tim Menzies. 2016. A deep learning model for estimating story
points. arXiv preprint arXiv:1609.00489 (2016).
[16] Anna Corazza, Sergio Di Martino, Filomena Ferrucci, Carmine Gravino, Federica
Sarro, and Emilia Mendes. 2013. Using tabu search to con/f_igure support vector
regression for eÔ¨Äort estimation. Empirical So/f_tware Engineering 18, 3 (2013),
506‚Äì546.
[17] Jacek Czerwonka, Rajiv Das, Nachiappan Nagappan, Alex Tarvo, and Alex
Teterev. 2011. Crane: Failure prediction, change analysis and test prioritization in
practice‚Äìexperiences from windows. In Proceedings of the 4th IEEE International
Conference on So/f_tware Testing, Veri/f_ication and Validation . IEEE, 357‚Äì366.
[18] Karim O Elish and Mahmoud O Elish. 2008. Predicting defect-prone so/f_tware
modules using support vector machines. Journal of Systems and So/f_tware 81, 5
(2008), 649‚Äì660.
[19] Martin S Feather and Tim Menzies. 2002. Converging on the optimal a/t_tainment
of requirements. In Proceedings of the 10th Anniversary IEEE Joint International
Conference on Requirements Engineering . IEEE, 263‚Äì270.
[20] Danyel Fisher, Rob DeLine, Mary Czerwinski, and Steven Drucker. 2012. Interac-
tions with big data analytics. interactions 19, 3 (2012), 50‚Äì59.
[21] Wei Fu, Tim Menzies, and Xipeng Shen. 2016. Tuning for so/f_tware analytics: Is
it really necessary? Information and So/f_tware Technology 76 (2016), 135‚Äì146.
[22] Wei Fu, Vivek Nair, and Tim Menzies. 2016. Why is diÔ¨Äerential evolution be/t_ter
than grid search for tuning defect predictors? arXiv preprint arXiv:1609.02613
(2016).
[23] Alex Graves, Abdel-rahman Mohamed, and GeoÔ¨Ärey Hinton. 2013. Speech
recognition with deep recurrent neural networks. In 2013 IEEE International
Conference on Acoustics, Speech and Signal Processing . IEEE, 6645‚Äì6649.
[24] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep
API learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo-
sium on Foundations of So/f_tware Engineering . ACM, 631‚Äì642.
[25] Mark A Hall and GeoÔ¨Ärey Holmes. 2003. Benchmarking a/t_tribute selection
techniques for discrete class data mining. IEEE Transactions on Knowledge and
Data engineering 15, 6 (2003), 1437‚Äì1447.
[26] Maurice Howard Halstead. 1977. Elements of so/f_tware science . Vol. 7. Elsevier
New York.
[27] Mark Harman. 2007. /T_he current state and future of search based so/f_tware
engineering. In 2007 Future of So/f_tware Engineering . IEEE Computer Society,
342‚Äì357.
[28] Tian Jiang, Lin Tan, and Sunghun Kim. 2013. Personalized defect prediction. In
Proceedings of the 28th IEEE/ACM International Conference on Automated So/f_tware
Engineering . IEEE, 279‚Äì289.
[29] /T_horsten Joachims. 1998. Text categorization with support vector machines:
Learning with many relevant features. In European Conference on Machine Learn-
ing. Springer, 137‚Äì142.
[30] Bryan F Jones, H-H Sthamer, and David E Eyres. 1996. Automatic structural
testing using genetic algorithms. So/f_tware Engineering Journal 11, 5 (1996),299‚Äì306.
[31] Dennis Kafura and Geereddy R. Reddy. 1987. /T_he use of so/f_tware complexity
metrics in so/f_tware maintenance. IEEE Transactions on So/f_tware Engineering 3
(1987), 335‚Äì343.
[32] Dongsun Kim, Yida Tao, Sunghun Kim, and Andreas Zeller. 2013. Where should
we /f_ix this bug? a two-phase recommendation model. IEEE Transactions on
So/f_tware Engineering 39, 11 (2013), 1597‚Äì1610.
[33] Sunghun Kim, E James Whitehead Jr, and Yi Zhang. 2008. Classifying so/f_tware
changes: Clean or buggy? IEEE Transactions on So/f_tware Engineering 34, 2 (2008),
181‚Äì196.
[34] Ekrem Kocaguneli, Tim Menzies, Ayse Bener, and Jacky W Keung. 2012. Ex-
ploiting the essential assumptions of analogy-based eÔ¨Äort estimation. IEEE
Transactions on So/f_tware Engineering 38, 2 (2012), 425‚Äì438.
[35] Ekrem Kocaguneli, Tim Menzies, and Jacky W Keung. 2012. On the value of
ensemble eÔ¨Äort estimation. IEEE Transactions on So/f_tware Engineering 38, 6
(2012), 1403‚Äì1416.
[36] Joseph Krall, Tim Menzies, and Misty Davies. 2015. Gale: Geometric active
learning for search-based so/f_tware engineering. IEEE Transactions on So/f_tware
Engineering 41, 10 (2015), 1001‚Äì1018.
[37] Alex Krizhevsky, Ilya Sutskever, and GeoÔ¨Ärey E Hinton. 2012. Imagenet classi/f_ica-
tion with deep convolutional neural networks. In Advances in Neural Information
Processing Systems . 1097‚Äì1105.
[38] Rakesh Kumar, Keith I Farkas, Norman P Jouppi, Parthasarathy Ranganathan,
and Dean M Tullsen. 2003. Single-ISA heterogeneous multi-core architectures:
/T_he potential for processor power reduction. In Proceedings of the 36th Annual
IEEE/ACM International Symposium on Microarchitecture . IEEE, 81‚Äì92.
[39] An Ngoc Lam, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen. 2015.
Combining deep learning with information retrieval to localize buggy /f_iles for
bug reports (n). In Proceedings of the 2015 30th IEEE/ACM International Conference
on Automated So/f_tware Engineering . IEEE, 476‚Äì481.
[40] /Q_uoc V Le. 2013. Building high-level features using large scale unsupervised
learning. In 2013 IEEE International Conference on Acoustics, Speech and Signal
Processing . IEEE, 8595‚Äì8598.
[41] R¬¥emi Lebret and Ronan Collobert. 2013. Word emdeddings through hellinger
PCA. arXiv preprint arXiv:1312.5542 (2013).
[42] Yann LeCun, Yoshua Bengio, and GeoÔ¨Ärey Hinton. 2015. Deep learning. Nature
521, 7553 (2015), 436‚Äì444.
[43] Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. 2008.
Benchmarking classi/f_ication models for so/f_tware defect prediction: A proposed
framework and novel /f_indings. IEEE Transactions on So/f_tware Engineering 34, 4
(2008), 485‚Äì496.
[44] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet
Talwalkar. 2016. Hyperband: a novel bandit-based approach to hyperparameter
optimization. arXiv preprint arXiv:1603.06560 (2016).
[45] /T_homas J McCabe. 1976. A complexity measure. IEEE Transactions on So/f_tware
Engineering 4 (1976), 308‚Äì320.
[46] Tim Menzies, Jeremy Greenwald, and Art Frank. 2007. Data mining static code
a/t_tributes to learn defect predictors. IEEE Transactions on So/f_tware Engineering
33, 1 (2007).
[47] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and JeÔ¨Ä Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems . 3111‚Äì3119.
[48] Mohamed Wiem Mkaouer, Marouane Kessentini, Slim Bechikh, Kalyanmoy Deb,
and Mel ¬¥O Cinn ¬¥eide. 2014. High dimensional search-based so/f_tware engineer-
ing: /f_inding tradeoÔ¨Äs among 15 objectives for automating so/f_tware refactoring
using NSGA-III. In Proceedings of the 2014 Annual Conference on Genetic and
Evolutionary Computation . ACM, 1263‚Äì1270.
[49] Julian Molina, Manuel Laguna, Rafael Mart ¬¥ƒ±, and Rafael Caballero. 2007. SSPMO:
A sca/t_ter tabu search procedure for non-linear multiobjective optimization. IN-
FORMS Journal on Computing 19, 1 (2007), 91‚Äì100.
[50] Kjetil Molokken and Magen Jorgensen. 2003. A review of so/f_tware surveys on
so/f_tware eÔ¨Äort estimation. In Proceedings of the 2003 International Symposium on
Empirical So/f_tware Engineering . IEEE, 223‚Äì230.
[51] Gordon E Moore and others. 1998. Cramming more components onto integrated
circuits. Proc. IEEE 86, 1 (1998), 82‚Äì85.
[52] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional Neural
Networks over Tree Structures for Programming Language Processing. In Pro-
ceedings of the /T_hirtieth AAAI Conference on Arti/f_icial Intelligence . AAAI Press,
1287‚Äì1293.
[53] Mahamed GH Omran, Andries Petrus Engelbrecht, and Ayed Salman. 2005.
DiÔ¨Äerential evolution methods for unsupervised image classi/f_ication. In 2005
IEEE Congress on Evolutionary Computation , Vol. 2. IEEE, 966‚Äì973.
[54] /T_homas J Ostrand, Elaine J Weyuker, and Robert M Bell. 2004. Where the bugs
are. In ACM SIGSOFT So/f_tware Engineering Notes , Vol. 29. ACM, 86‚Äì96.
[55] Fabian Pedregosa, Ga ¬®el Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand /T_hirion, Olivier Grisel, Mathieu Blondel, Peter Pre/t_tenhofer, Ron Weiss,
Vincent Dubourg, and others. 2011. Scikit-learn: machine learning in Python.
Journal of Machine Learning Research 12, Oct (2011), 2825‚Äì2830.ESEC/FSE‚Äô17, September 4-8, 2017, Paderborn, Germany Wei Fu, Tim Menzies
[56] JeÔ¨Ärey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
global vectors for word representation. In EMNLP , Vol. 14. 1532‚Äì1543.
[57] Mukund Raghothaman, Yi Wei, and Youssef Hamadi. 2016. SWIM: synthesizing
what I mean: code search and idiomatic snippet synthesis. In Proceedings of the
38th International Conference on So/f_tware Engineering . ACM, 357‚Äì367.
[58] Radim Rehurek and Petr Sojka. 2010. So/f_tware framework for topic modelling
with large corpora. In In Proceedings of the LREC 2010 Workshop on New Challenges
for NLP Frameworks . Citeseer.
[59] Jeanine Romano, JeÔ¨Ärey D Kromrey, Jesse Coraggio, JeÔ¨Ä Skowronek, and Linda
Devine. 2006. Exploring methods for evaluating group diÔ¨Äerences on the NSSE
and other surveys: Are the t-test and Cohen/f_isd indices the most appropriate
choices. In Annual Meeting of the Southern Association for Institutional Research .
[60] J¬®urgen Schmidhuber. 2015. Deep learning in neural networks: An overview.
Neural networks 61 (2015), 85‚Äì117.
[61] Ryan Spring and Anshumali Shrivastava. 2017. Scalable and sustainable deep
learning via randomized hashing. In Proceedings of the 23rd ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining . ACM.
[62] R. Storn and K. Price. 1997. DiÔ¨Äerential evolution‚Äìa simple and eÔ¨Écient heuristic
for global optimization over continuous spaces. Journal of global optimization
11, 4 (1997), 341‚Äì359.
[63] Ilya Sutskever, Oriol Vinyals, and /Q_uoc V Le. 2014. Sequence to sequence
learning with neural networks. In Advances in Neural Information Processing
Systems . 3104‚Äì3112.
[64] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi
Matsumoto. 2016. Automated parameter optimization of classi/f_ication techniques
for defect prediction models. In Proceedings of the 38th International Conference
on So/f_tware Engineering . ACM, 321‚Äì332.
[65] Christopher /T_heisen, Kim Herzig, Patrick Morrison, Brendan Murphy, and Laurie
Williams. 2015. Approximating a/t_tack surfaces with stack traces. In Proceedings
of the 37th International Conference on So/f_tware Engineering-Volume 2 . IEEE,
199‚Äì208.
[66] Jakob Vesterstrom and Rene /T_homsen. 2004. A comparative study of diÔ¨Äerential
evolution, particle swarm optimization, and evolutionary algorithms on numer-
ical benchmark problems. In Proceedings of the 2004 Congress on Evolutionary
Computation , Vol. 2. IEEE, 1980‚Äì1987.
[67] Jue Wang, Yingnong Dang, Hongyu Zhang, Kai Chen, Tao Xie, and Dongmei
Zhang. 2013. Mining succinct and high-coverage API usage pa/t_terns from
source code. In Proceedings of the 10th Working Conference on Mining So/f_tware
Repositories . IEEE, 319‚Äì328.[68] Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically learning semantic
features for defect prediction. In Proceedings of the 38th International Conference
on So/f_tware Engineering . ACM, 297‚Äì308.
[69] Tiantian Wang, Mark Harman, Yue Jia, and Jens Krinke. 2013. Searching for
be/t_ter con/f_igurations: a rigorous approach to clone evaluation. In Proceedings of
the 2013 9th Joint Meeting on Foundations of So/f_tware Engineering . ACM, 455‚Äì465.
[70] Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk.
2016. Deep learning code fragments for code clone detection. In Proceedings of
the 31st IEEE/ACM International Conference on Automated So/f_tware Engineering .
ACM, 87‚Äì98.
[71] Martin White, Christopher Vendome, Mario Linares-V ¬¥asquez, and Denys Poshy-
vanyk. 2015. Toward deep learning so/f_tware repositories. In Proceedings of the
12th Working Conference on Mining So/f_tware Repositories . IEEE, 334‚Äì345.
[72] Andreas Windisch, Stefan Wappler, and Joachim Wegener. 2007. Applying
particle swarm optimization to so/f_tware testing. In Proceedings of the 9th Annual
Conference on Genetic and Evolutionary Computation . ACM, 1121‚Äì1128.
[73] David H Wolpert. 1996. /T_he lack of a priori distinctions between learning
algorithms. Neural computation 8, 7 (1996), 1341‚Äì1390.
[74] Bowen Xu, Deheng Ye, Zhenchang Xing, Xin Xia, Guibin Chen, and Shanping Li.
2016. Predicting semantically linkable knowledge in developer online forums via
convolutional neural network. In Proceedings of the 31st IEEE/ACM International
Conference on Automated So/f_tware Engineering . ACM, 51‚Äì62.
[75] Xiao Yang, Craig Macdonald, and Iadh Ounis. 2016. Using word embeddings in
twi/t_ter election classi/f_ication. arXiv preprint arXiv:1606.07006 (2016).
[76] Xin Ye, Razvan Bunescu, and Chang Liu. 2014. Learning to rank relevant /f_iles for
bug reports using domain knowledge. In Proceedings of the 22nd ACM SIGSOFT
International Symposium on Foundations of So/f_tware Engineering . ACM, 689‚Äì699.
[77] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue. 2014. Droid-
sec: deep learning in android malware detection. In ACM SIGCOMM Computer
Communication Review , Vol. 44. ACM, 371‚Äì372.
[78] Yuanyuan Zhang, Mark Harman, and S. Afshin Mansouri. 2007. /T_he multi-
objective next release problem. In Proceedings of the 9th Annual Conference on
Genetic and Evolutionary Computation . 1129‚Äì1137.
[79] Jian Zhou, Hongyu Zhang, and David Lo. 2012. Where should the bugs be /f_ixed?-
more accurate information retrieval-based bug localization based on bug reports.
InProceedings of the 34th International Conference on So/f_tware Engineering . IEEE,
14‚Äì24.
[80] Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif Azzopardi. 2015. Inte-
grating and evaluating neural word embeddings in information retrieval. In
Proceedings of the 20th Australasian Document Computing Symposium . ACM, 12.