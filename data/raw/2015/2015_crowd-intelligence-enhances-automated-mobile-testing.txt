Crowd Intelligence Enhances Automated
Mobile Testing
Ke Mao∗, Mark Harman∗, and Y ue Jia∗
Facebook London, Facebook, 10 Brock Street, London, NW1 3FG, UK
CREST, University College London, Malet Place, London, WC1E 6BT, UK
{kemao, markharman, yuej}@fb.com
Abstract —We show that information extracted from crowd-
based testing can enhance automated mobile testing. We
introduce P OLARIZ , which generates replicable test scripts
from crowd-based testing, extracting cross-app ‘motif’ events:
automatically-inferred reusable higher-level event sequences com-
posed of lower-level observed event actions. Our empirical study
used 434 crowd workers from Mechanical Turk to perform 1,350
testing tasks on 9 popular Google Play apps, each with at least1 million user installs. The ﬁndings reveal that the crowd was
able to achieve 60.5% unique activity coverage and proved to
be complementary to automated search-based testing in 5 out ofthe 9 subjects studied. Our leave-one-out evaluation demonstrates
that coverage attainment can be improved (6 out of 9 cases, with
no disimprovement on the remaining 3) by combining crowd-based and search-based testing.
Index T erms—Crowdsourced Software Engineering, Mobile
App Testing, Test Generation
I. I NTRODUCTION
There has been much recent progress in automated testing
[1], [2], with recent advances in automated testing of mobile
apps [3]. However, automated test data generation techniqueslack domain knowledge, and may generate either unrealistictest cases or fail to ﬁnd test cases that explore aspects of func-tionality that matter to users [4]. A recent study of open sourceAndroid apps with relatively simple user ﬂows [3] reportedthat current state-of-art automated mobile testing techniquesachieve only approximately 50% statement coverage.
Fortunately, Linares-V ´asquez et al. [5] recently showed how
app execution usage data can be mined for valuable insights,while Moran et al. subsequently introduced CrashScope [6]
which supports the discovery of app crashes and their repli-
cation. There has also been considerable recent interest inthe possibilities of crowdsourcing as a means of collectingsuch usage data in a cost-effective manner. This recent worksuggests that data mining and extraction, perhaps from crowd
sourced usage, might discover useful cross-app patterns thatimprove automated app testing performance.
We explore the complementarity between automated
machine-generated tests and human (crowd-generated) tests.
We speciﬁcally focus on the ability of crowd-based tests to
∗This research forms part of the PhD work of Ke Mao, the lead author,
supervised by Y ue Jia and Mark Harman while all three were at University
College London. Dr. Mao, Dr. Jia and Prof Harman moved to Facebook fulltime in February 2017 and Dr. Jia and Prof. Harman also retain part time
positions at UCL.assist the state-of-the-art search based testing tool, Sapienz.
An open source research prototype of Sapienz [7] was releasedin 2016, and the technology that underpins it has been underdevelopment at Facebook since February 2017. In this paperwe use the open source version of Sapienz to facilitate repli-cation. The Sapienz approach to search based testing is well-suited to augmentation with crowd-based tests due to Sapienz’concept of a motif gene: a sequence of low-level events thathas a (context-sensitive) meaning to the app’s users, therebydenoting an ‘atomic’ event (to users), although appearing tobe a non-atomic event sequence (to the device and any testingapproach that lacks the necessary context-awareness).
We show that these strands of work on mining, crowd-
sourcing and automated test generation can be combined in amutually-complementary hybrid. Our hybrid uses automatedtest generation to explore the search space of test cases,informed by data mined from crowdsourced tests to identifymotif genes. To do this, we introduce a crowd-based approach,called P
OLARIZ , which collects and analyses test inputs from
a crowd call to non-technical users with no speciﬁc softwaretesting expertise or experience. That is, the call is open to anycrowd workers to participate, whether or not they have testing
expertise. However, since it is an open call (in the spirit ofcrowdsourcing) we cannot guarantee that we do not recruit
any crowd workers with testing expertise.
P
OLARIZ uses a platform with a mobile device infrastruc-
ture, remote device control and screen streaming, automated
subject distribution, permission control and crowd trace col-lection. With this approach, a non-professional crowd fromthe general public (such as those from Amazon MechanicalTurk) can contribute to mobile testing from anywhere withany clients with a web browser (e.g., desktop PC, Android,iOS or Windows Phone mobile devices).
Following Linares-V ´asquez et al. [5], we introduce a novel
data mining algorithm to extract ‘motif’ event sequences;sequences composed of lower-level events that our approachinfers may denote higher-level atomic units, thereby providingone source of guidance for automated testing. We deﬁne a‘motif’ event sequence (or ‘motif pattern’) as a common user
interaction pattern that is learned from some apps, and can
be subsequently generalised to other apps, such that the motifsequence can play the role of a higher-level atomic event thatcan be re-used to assist automated mobile testing.
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research16
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. A ‘motif pattern’ may occur multiple times (there may
be many instances of a motif pattern), each occurrence of
which we refer to as ‘motif events’. Our approach thus bridgesthe gap between automated mobile test input generationtechniques and human domain/context awareness, using non-professional crowd testers.
The primary contribution of our work is the scientiﬁc
evidence that motifs extracted from crowdsourced tests can
complement and extend state-of-the-art automated test gener-ation. More speciﬁcally, the contributions of our work are asfollows:
1) We introduce and implement the P
OLARIZ approach for
harnessing crowd intelligence to support mobile testing.
Using our implementation, we report the results of theﬁrst empirical study of crowdsourcing for mobile test
automation. We posted 1,350 tasks on Amazon Mechanical
Turk to test 9 popular Google Play apps, each with at least1 million user installs. The crowd was able to attain 60.5%overall (unique) activity coverage.
2) We compare the results of app activities covered by the
crowd with those by the automated search based Android
testing tool, S
APIENZ , revealing complementarity between
the two. We chose S APIENZ because it has recently [7]
been shown to outperform other state-of-the-art and state-
of-practice tools. Unsurprisingly, the crowd, imbued withits superior domain knowledge, achieved higher activitycoverage on all but one subject (Google Translate, forwhich S
APIENZ produced slightly higher coverage). More
importantly, for 5 the 9 subjects the two techniques ex-hibited complementary coverage, motivating our goal ofcombining them.
3) We introduce a motif-extraction algorithm and demonstrate
its effectiveness, by showing that it can enhance S
APIENZ ’
coverage. For 6 of the 9 subjects the coverage is improvedby motif extraction, with the best case improvement in-creasing coverage obtained from 6 to 12 (of 27 possible)unique activities.
II. T
HEPOLARIZ APPROACH
The P OLARIZ approach is designed to tackle the two main
challenges involved in harnessing the non-professional crowdto perform remote mobile testing, and further to learn fromcrowd intelligence embodied in the crowdsourced manuallyconstructed tests. The ﬁrst challenge requires an intermediary
platform, able to harness a general public crowd to work on
remote mobile testing tasks. The second challenge involvesthe representation and extraction of useful crowd intelligence.
Figure 1 depicts the high-level P
OLARIZ workﬂow. Three
actors are involved in the workﬂow:
1) App developer/researcher who seeks mobile test automa-
tion enhancement;
2) Crowd workers/testers;
3) The intermediary platform (i.e., P OLARIZ platform) on
which the crowd works.
Fig. 1. Overall workﬂow of Polariz
POLARIZ uses its own device infrastructure; users do not
execute apps on their own devices. This insulates the user from
security issues, while insulating P OLARIZ against Android
device fragmentation. It also gives P OLARIZ full control over
real-time data collection and monitoring. However, it means
that P OLARIZ can only collect touch-screen events, not device-
speciﬁc events such as GPS and accelerometer events. It
also involves a latency (since testing activity occurs over thenetwork), which we checked and report on. Fortunately, theseresults indicate that the latency is acceptable.
The outputs consist of three parts: First, the bug reports
automatically generated during the crowd testing process;
Second, the replicable test scripts generated based on crowd-sourced test manual traces, which can be replayed via an An-droid test replayer (such as RERAN [8]); Third, the automati-cally summarised motif events learned from crowd interactiontraces, which can be further used to enhance existing search-based mobile test generators (such as S
APIENZ ). Both the
Android test replayer and test generator can remotely connect
to P OLARIZ ’ mobile device infrastructure for test execution.
POLARIZ ’ top level consists of its crowd testing platform
(for manual trace collection), and its crowd motif extractionalgorithm (for learning from crowd intelligence). Our platformuses crashing as an implicit oracle [9], automatically capturingcrash-triggering stack traces, event sequences and witnessvideos using the existing S
APIENZ infrastructure [7].
A. The Polariz Platform
The platform is illustrated in Figure 2. Given a set of mobile
apps under test, P OLARIZ ’ subject dispatcher component auto-
matically instruments, assigns and installs each app on a devicein its mobile device infrastructure. The screen streamer anddevice controller provide web services for controlling these
devices. Crowd users simply access the remote devices which
install apps via web browsers from any user platform.
Exposing our hosted mobile devices to the general public
might raise security concerns, so P
OLARIZ has a permission
control component that monitors crowd interactions, onlypermitting testing activities on the speciﬁed subjects. Duringthe the crowd testing process, P
OLARIZ ’ logging components
such as a crash detector and trace collector automaticallycollect the information from which P
OLARIZ generates its
reports.
17
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. Fig. 2. Detailed components of Polariz platform
B. The Crowd Motif Extraction Algorithm
Our use of crowd ‘motif’ patterns stems from DNA se-
quence motif. According to D’Haeseleer [10], a DNA se-
quence motif is a short, over-represented pattern with anassumed biological function. The crowd motif extraction prob-lem is to ﬁnd a set of recurring substrings within a set ofstrings, which can be described as follows:
Given a set of Ncrowd-generated test event sequences S=
{S
1,...,S N}, each formed by events from an event set:
Y={Swipe, Rotate, F lip, P inch, Click ROI 1,Cl i c k ROI 2,...,
Pre s s key1,Pr e s s key2,...}(1)
the motif extraction problem is to ﬁnd a set of instances
M={m1,...,m n}(n≤N), where each miis a w-
sized subsequence of sequences in Sthat maximises M’s
information content (IC M) according to the equation:
ICM=w/summationdisplay
i=1/summationdisplay
y∈Ypy,ilogpy,i
By, (2)
where py,i is the probability of event yat position iin
M, and Byis the probability of event yin the background
distribution. Thus, ICMcomputes the relative entropy of the
event sequences in M, favouring those sub-sequences that
are prevalent in the crowd’s behaviour, yet are relatively lessprevalent in the overall distribution. Our approach is inspiredby DNA sequence motif discovery [11]–[13].
We use a genetic algorithm to extract crowd motif events for
mobile testing. The adapted crowd motif extraction algorithmis listed in Algorithm 1. The algorithm extracts multiple motifpatterns from a set of collected log-trace pairs. The log pro-vides subject execution state information, such as transitionsfrom one app activity to another, and the trace saves manualinteractions that were used to trigger the app’s state changes.The log and trace items are linked via their timestamps.
In order to learn from ‘the wisdom of the crowd’, Lines
2-3 extract the minimum trace that enables a transition fromone activity to another. That is, there may exist many ways(sequences of events) through which the user interactionstrigger the same app activity. We favour the ‘minimum trace’that requires the fewest operations. The generated minimumtransitional trace collection, S, is represented as a list of
strings, in which each string is a minimum trace that triggers
a speciﬁc ‘A to B’ activity transition.Algorithm 1: Crowd Motif Extraction Algorithm
1Description: Findmmotif patterns from nsubject log-trace pairs.
Input: A list of log-trace pairs D=[ (L1,T1), (L 2,T2) ,..., (L n,Tn)], where
Liis the app execution state log and Tiis the app event traces for the ith
app; Number of motif patterns to ﬁnd m.
Output: A list of recurring motif patterns R=[r1,r2, ...,rm].
2R←[],S←[]; ⊿initialisation
⊿get the minimum operations to switch from one activity
to another
3for each (L,T)inDdo
4S←S∪getMinimumActivityTransitionTrace((L,T ));
⊿findmmotif patterns by evolving candidate motif
substring locations
5foriinrange (0,m )do
6 generation g←0;
⊿for each individual generate random candidate motif
locations in S
7P←initialisePopulation(S );
8 evaluatePby calculating ICMfor each individual in Q; ⊿see
Equation 2
9 whileg < max generations do
10 P/prime←tournamentSelection(P );
11 Q←variation (P/prime);⊿crossover and mutate motif
locations and length
12 evaluateQby calculating ICMfor each individual in Q;
13 Q←elitismSelection(Q,P );
14 g←g+1 ;
15 P←Q;
16r←getBestIndividual (P);⊿may contain zero or one
motif location for s∈S
17R←r∪R;
⊿excluding found motif substrings for next motif
pattern
18S←removeMotifSubstrings (S,r );
19returnR;
Lines 5-18 use a genetic algorithm to ﬁnd multiple motif
patterns. At each iteration, the algorithm ﬁnds a single motif
pattern and excludes the matched motif substrings from S
(Line 18). Each individual genetic algorithm population mem-
ber represents a candidate motif pattern; a list of candidate
motif locations in S. The individual’s ﬁtness is evaluated based
on the information content score, as described by Equation 2.
The variation operator (Line 11) applies both crossover
and mutation to manipulate the location and length of eachmotif substring. The best individuals (with highest information
content score, i.e., their motif substrings are most conservative)are selected for the next generation. In this way our genetic
algorithm uses elitism in its selection and retention. Evolutionstops after a given maximum number of generations, savingthe best individual found. The overall process repeats until all
mmotif patterns have been discovered.
Our implementation consists of the two top level com-
ponents as described in Section II to produce the platform
shown in Figure 2. P
OLARIZ implementation’s mobile device
infrastructure consists of 9 Nexus-7 tablets, connected to a hostPC via a USB hub. We adapt the Android ‘getevent’ tool fortrace recording, and use the RERAN [8] tool for trace replay:The ‘getevent’ tool captures a list of low-level Android eventson-the-ﬂy, saving to a script which is subsequently interpretedby the RERAN tool. For remote device control, we use theopen sourced ‘openstf’ project
1, and we deployed the platform
to a server with a proxy service to speed up global visits.
1https://github.com/openstf/stf
18
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. We use the S APIENZ implementation obtained from the
open source prototype, made available by the S APIENZ re-
search project [7]. When improving S APIENZ , we integrate the
learned motifs into S APIENZ ’MOTIFCORE component, which
combines the motif patterns (which S APIENZ calls ‘motif
genes’) with atomic ‘genes’. In order to learn the crowd motifs
from the collected event traces, we implement the P OLARIZ
motif extraction component in Python, according to Algorithm1. S
APIENZ can record and replay event sequences and uses
the low-level Android monkey representation of events to gothis. This low level representation is not immediately human-readable, but since our event sequences (and motifs extracted)are intended purely for machines this is not a problem.
III. E
MPIRICAL EV ALUA TION OF POLARIZ
We want to investigate the usefulness of P OLARIZ , both as a
source of test data, garnered from an untrained and technicallynon-speciﬁc crowd, and also as a mechanism for augmentingexisting automated techniques for test data generation. In thissection we outline and motivate the four research questionsthat we choose to pose and answer in this paper:
RQ1: Demographics and behviour: Before investigating the
nature of test cases generated and analysed from our crowdworkers, we ﬁrst report on the demographic diversity andbehavioural characteristics of the crowd. We do this to supportcomparison and replication and subsequent study, which willexhibit inherent variability due to the nature of the crowdrecruited for any such subsequent study.
RQ1.1: The demographic diversity of the crowd: RQ1.1
reports on the diversity of the crowd workers recruited in orderto perform the testing activities in our study. In order for thecrowd to denote an affective source of test data, which exhibitsdiversity, the crowd itself will need to be diverse. This diversity
is important in order to ensure that the test cases explore app
behaviour exercised by all of the types of users who may usethe application under test. It would also be important for themotif extraction approach, because this needs to generalisefrom a set of instances, observed from crowd behaviour. If welack diversity in the crowd, then there is the possibility that
the motif extraction algorithm will overﬁt.
We report on the diversity of the crowd in terms of de-
mographic distribution, gender, educational background, and
prior experience, both with mobile applications in general, andsoftware testing in particular. We also report on the level ofreturning workers; those who come back to complete furthertesting tasks that we set, having already tackled our previoustesting tasks.
RQ1.2: The crowd’s interest level: In order to be sufﬁciently
motivated to tackle the testing tasks we set, the crowd needsto feel interested in these tasks. The tasks we set are notspeciﬁcally related to testing, but simply involve using theapplications under test. We survey the crowd for their self-
assessed, level of interest, on a standard Lickert scale, in orderto provide some initial evidence relating to the level of crowdinterest.RQ1.3: The crowd’s response rates: We also investigate
the behaviour of the crowd with respect to response rates,reporting on the distribution of the number of tasks submittedper crowd worker, and their performance in terms of speedof acceptance and completion of tasks. It is impossible toaccurately measure the time a crowd member speciﬁcallydevotes to a task, because we cannot know whether they aresolely focused on the task. Nevertheless, we can report on thetime between creation and acceptance of the task, betweenacceptance and submission of the task and also the total loggedtime that a crowd member spends working on a task.
RQ2: The crowd’s coverage attainment: We use the crowd
as a source of test data in its own right, as well as the abilityof the crowd to provide observations from which we canextract motif patterns for automated test techniques. In orderto investigate the crowd’s value as a source of test data in its
own right, we report on the coverage obtained by the crowd asthe number of tasks completed increases. We report both the
overall number of unique and non-unique activities covered,
and also the level of activity coverage per subject, for each ofthe nine subject apps under test.
Having investigated the demographics and behaviour of the
crowd of their ability to generate test data, we move on toconsider the relationship between crowd-based testing andautomated testing. In particular, we compare crowd testing forAndroid, with a recently proposed, state-of-the-art technique,
S
APIENZ [7], for automated test data generation for Android
using search based software testing. We ﬁrst compare thecrowd’s and S
APIENZ ’ coverage attainment, in terms of unique
activities covered, and then investigate the degree to whichmotif patterns, extracted from the crowd (using our motif-
extraction algorithm) can improve the coverage performance
of S
APIENZ .
RQ3: The comparative activity coverage achieved by thecrowd and by S
APIENZ : In order to answer this question,
we report on the number of unique activities covered bythe crowd and by S
APIENZ , and their intersection. This
allows us to explore the degree to which the two techniquesare complementary to one another, and also the degree ofoverlap between automated testing and crowd-based testing.Should it turn out that the automated technique subsumes thehuman-based technique, then there would be little point ininvestigating motif pattern extraction, but if crowd testing canachieve better or different (complementary) coverage, then thissuggests that there may be scope to improve automated testdata generation with motif pattern extraction from observedcrowd-based tests. The current version of P
OLARIZ records
coverage but not faults found; future work will extend it torecord detailed fault context.
RQ4: The improvement in S
APIENZ performance when
using motif patterns extracted from crowd-based tests: Fi-
nally, we investigate the degree to which S APIENZ ’ coverage is
improved, for each of the nine apps under test, when S APIENZ
is imbued with information extracted from the crowd-basedtesting in the form of motif patterns.
19
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. TABLE I
NINE POPULAR GOOGLE PLA Y SUBJECT APPS (‘#A ’ FOR NUMBER OF ACTIVITIES ; ‘#M’ FOR NUMBER OF METHODS ;‘ I NSTALLS ’IS IN MILLIONS )
Subject Ver. Category Description #A #M Rating Installs
HP All-in-One Printer
Remote4.1.18 Productivity Help users scan and print documents with HP
printers.74 13,616 4.1 10-50M
TuneIn Radio 17.1 Music & Audio Let users listen to radio stations for free. 27 13,474 4.4 100-500M
Trainline 2.5.0 Maps & Navigation A railway information provider. 41 7,497 4.3 1-5M
Power Security 1.0.18 Tools Scan and kill viruses, malware and spyware. 38 5,802 4.4 5-10M
Google Translate 5.6.0 Tools Translate between 103 languages. 17 4,765 4.4 100-500M
Brightest Flashlight 1.35 Productivity A multi-functional ﬂashlight app. 27 6,087 4.3 5-10M
Duolingo 3.39.1 Education Learn multiple languages fast, fun and free. 29 9,949 4.7 50-100M
Clean My Android 1.1.9 Productivity A light phone cleaner and app manager. 16 1,804 4.7 1-5M
Citymapper 6.15 Maps & Navigation A journey planner and route ﬁnder. 32 25,998 4.5 1-5M
A. Subject Applications
We perform the empirical evaluation on 9 randomly-selected
real-world Google Play apps from top 500 most popular
free/in-app-purchase apps as listed in the Google Play appstore on December 20, 2016. We chose 9 subjects from alist of all apps, ﬁltered based on their availability for ourhardware resources (9 Nexus-7 tablets in the P
OLARIZ device
infrastructure), and constraints imposed by a desire to use thesubjects in experiments on the crowd-based testing.
That is, when we perform the random selection, we ﬁrst
exclude gaming apps that are not based on standard Androidnative UI components. Also, to protect the crowd testers’privacy, we also exclude apps that request user account in-formation after launching. The crowd was also notiﬁed thatthey should not disclose any personal information during thetesting process.
The 9 apps that were selected randomly after this ﬁltering
process, are closed-sourced and cover multiple app categories.Each app has at least 1 million user installs (according toGoogle Play). Detailed subject information including versionnumbers, sizes, ratings and the number of installs are presented
in Table I.
B. Experimental Settings
For each subject, we assign the same app running en-
vironment, i.e., the same software and hardware conﬁgura-
tions. These conﬁgurations mimic general real-world end-usertesting scenarios, e.g., with real devices that have Googleservice framework and WIFI network connection, but withoutproviding app-speciﬁc contexts. For example, the ‘HP All-in-One Printer Remote’ app may require an HP printer fortesting some of its functionalities. In our experiments, we donot provide such app-speciﬁc equipment for the generalisationpurpose.
We also need to recruit crowd workers and manage pay-
ments by using a third-party intermediary. We report on ourapproach to tackling these issues in the remainder of the
section in order to support replication and to give the contextto the results we present for crowd-based testing.Crowd recruitment: We use Amazon Mechanical Turk
2
(AMT) for recruiting non-professional crowd workers from
the general public. AMT is currently one of the most popularcrowdsourcing platforms for micro tasks with general crowd
workers. We recruit AMT workers to perform remote mobile
testing tasks on our P
OLARIZ platform by posting human
intelligence tasks (HITs) on AMT. Anyone, from any country,who is eligible to work on AMT is allowed to work onour HIT assignments. We only disclose the task informationand our P
OLARIZ web service URL via the AMT HIT for
controlling the worker sources (i.e., only AMT workers are
expected) because this may interfere the recruitment speed and
POLARIZ ’ visitor statistics.
To motivate the crowd, we provide 1.5 USD payment for
each approved submission, as the extrinsic incentive to the
crowd workers. We expect each worker will spend 10 minutes
or less on one HIT assignment. The payment rate is higher
than current UK national minimum wage (7.2 GBP/hour) andalso the US standard (7.25 USD/hour). Intrinsic incentivesinclude the opportunity to experience manual mobile testing,and maybe, also to test the remote apps for fun (we investigatethe task interest level in the results section).Task design and quality control: A clear task descriptionis considered to be one of the most important factors forsuccessful software crowdsourcing tasks [14]. This motivatesour careful design of our HIT. The general workﬂow of our
designed HIT task is as follows:
1) The crowd worker views the task assignment description
on AMT and can choose to accept or decline the task.
2) The worker follows the task instruction and works on our
P
OLARIZ platform via any devices with a browser and
performs manual testing on one arbitrarily selected app.
3) Upon ﬁnishing the testing task, the worker copies the
POLARIZ generated app execution log as the proof of task
completion and goes back to the AMT HIT.
4) The worker submits the automatically generated log and
answers a questionnaire which contains 6 brief questions.
5) The worker waits for requester’s review and gets paid via
AMT, assuming their submission meets our sanity check
for appropriate engagement.
2https://www.mturk.com
20
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. In the task description, for comprehensive testing, we instruct
the workers that the goal is to explore and trigger as manyfunctionalities of the subject app as possible. A few detailedsteps for accessing our P
OLARIZ platform are illustrated using
snapshots. In the questionnaire, we ask 6 short questions
to collect their feedback on the interest level of the task,and background information regarding the workers, includingtheir daily mobile usage duration, software testing experience,country, gender and education level. No personal informationthat may reveal the worker’s personal identity is collected.
For quality control, we give three criteria to the workers,
which form our ‘sanity check’ for task approval and con-sequent payment: First, the worker has tried to explore and
trigger multiple app functions (preferably as many as possible).Second, the worker has tested the app for at least 3 minutes.Third, the submitted app execution log contains at least 300
lines. Normally these criteria can be easily satisﬁed by testingthe app for a few minutes. We review each submission bychecking above three criteria in a semi-automated manner.
We measure these three criteria based on the submitted logs(for Criteria 1, we use at least two activities as the lower
bound). As a further sanity check, we also manually inspectthe submissions periodically. If a submission is rejected, wedo not repost that assignment.
We posted 1,350 assignments from December 22, 2016 to
January 2, 2017, in a continuous manner, in order to leave timeto perform daily reviews. These 1,350 assignments were splitinto 150 HITs, each containing 9 assignments. All HITs andtheir assignments are the same. Each HIT may contain oneor more task assignments. Each worker can work on multiple
HITs, but can only work on one assignment in one HIT. Our
quality control ﬁlter removed 20.4% of these HITs to leave1,075 for subsequent testing and motif extraction.
P
OLARIZ deployment: We deploy P OLARIZ as a publicly
accessible web service, at a server located in the UK, plusa Linode cloud server as a proxy to speed up global visits.
The mobile device infrastructure is hosted at the author’s lab
and connected to the front-end server. Accessing the remotedevice does not require authentication but the mobile devicesare monitored and manipulated under P
OLARIZ ’ permission
control component, where changes to environment settings areprohibited.
The 9 subjects are pre-installed on 9 Nexus-7 tablets;
one per device. User interactions are logged with timestampinformation which can be mapped to the submitted logs. Allsubjects are reset to their initial states every half an hour. Thisis to avoid the case that one worker drives the app into a state
from which the subsequent workers cannot recover. Of course,
we could have chosen reset app state per worker, but we foundthat multiple workers can collectively work on one AUT, bysetting the reset duration to 30 minutes.Performance metrics: We measure coverage attained, sincethis is a fundamental metric for testing [15]–[17]. In terms of
granularity of coverage, we measure app activity coverage; anapproach to coverage measurement that has been adopted inprevious studies on automated mobile testing [7], [18].Fig. 3. Crowd worker demographic information based on 1,350 submitted
assignments
This metric thus gives us a baseline against which to assess
and compare the ability of tests to ‘explore’ the AUT.
Motif extraction: We learn generalised event patterns be-
cause high-level events learned from only a single subjecthas already been proved useful in the literature [19]. In ourexperiment, we perform a leave-one-out evaluation on theextracted crowd motifs. That is, when evaluating a subject,we will use only the motifs extracted from the remaining 8subjects’ event traces. For each subject, we apply the P
OLARIZ
motif extraction algorithm to learn three motif patterns.
Improving S APIENZ : To examine whether the learned gener-
alised motifs are helpful in improving app activity coverage,
we run S APIENZ without any motif information and compare
results to these obtained from running S APIENZ with the
learned crowd motifs. On each subject, we run S APIENZ
for 60 minutes wall-clock time. We set the delay betweeneach two events to 200 ms so that, given the same amountof wall-clock time, roughly the same number of events willbe used. This setting aims for a fair comparison betweenthe two S
APIENZ versions with and without motif patterns.
For S APIENZ parameters, we use the default settings, as
reported by the authors of the S APIENZ paper [7]. In all
experiments, the parameters were not tuned, to avoid anyimplicit experimental biases that might otherwise arise.
We run all experiments on the same MacBook Pro with
2.3GHz Intel Core i7 CPU and 16G RAM. The mobile sidefor app execution is a Nexus-7 real device.
IV . R
ESULTS
The results show that P OLARIZ successfully assisted the
crowd workers to complete all 1,350 AMT tasks, from De-cember 22, 2016 to January 2, 2017. We also ﬁnd evidence tosupport the claim that there is complementarity between thecrowd-based tests and search based tests found by S
APIENZ .
We further report evidence to support the claim that motif pat-terns, extracted using or algorithm, can improve the attainmentof activity coverage by S
APIENZ .
21
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. A. RQ1: Demographics and Behaviour
RQ1 is decomposed into three sub-questions concerning
demographics, interest level and crowd behaviour, each of
which we report on below.
RQ1.1: The demographic diversity of the crowd: According
to visitor tracking data from Google Analytics, from December22nd. 2016 to January 2nd. 2017, there were 1,931 sessions of
visits to our remote crowd testing service. Of these sessions,
56.9% come from new visitors and 43.1% from returningvisitors. The records show the trafﬁc comes from at least 9countries, with most coming from the USA (60.90%) andIndia (24.91%). Note that there are other countries with largepopulations (such as China) whose workers are ineligible towork on AMT, so there are no visits from these countries.The average response times (from the 14 global sites accessedby P
OLARIZ during the course of our empirical study) range
from 0.465 ms (London) to 295.921 ms (Sydney). This resultindicates a reasonably good connectivity of our distributedinfrastructure for performing remote testing over a wide range
of geographical locations.
During the 12 days of experimentation time, our 1,350
posted HIT assignments were all ﬁnished by the crowdworkers. Of all submitted solutions, 1,075 (79.6% ) wereapproved, according to the criteria for quality control discussed
in Section III-B. We received 1,350 submissions from 434 dis-tinct workers. Results from our questionnaire show that theseworkers come from 24 countries, while 99% submissions arefrom the top 10 most frequently submitting countries (as listed
in Figure 3). Note that the number of countries is inconsistent
with the trafﬁc we observed according to Google Analytics;the questionnaire reveals a far wider country participation thanthat would be suggested by the Google Analytics data. Ourinterpretation is that a small number of AMT workers may
use proxies to visit the AMT (to overcome the fact that the
service is disabled in their countries).
Figure 3 presents worker demographic information based
on the 1,350 responses submitted by the crowd. This self-assessment is broadly consistent with the analytics data re-ported by Google; most workers come from USA (76.7%)and India (14.1%). However, using the self-assessment ques-tionnaire, we were able to obtain further demographic infor-mation: more male workers (64.4%) submitted than femaleworkers (31.4%). Regarding the educational level, 88.3%workers at least attended some college education (includingundergraduate students). This generally high educational levelis consistent with previous studies [20], [21], although ourresults show that there are more workers with some collegeeducation than those holding a Bachelor’s degree.
Since our remote testing tasks require basic skills for inter-
acting with mobile apps, we expected the crowd to have reg-ular (daily) mobile usage. Our questionnaire results on ‘DailyMobile Usage’ suggest that only 0.9% of the respondentsspend less than 1 hour per day on mobile usage, indicatingthat our expectation is reasonable.Fig. 4. Worker feedback on self-assessed interest level of the task (1 = V ery
Interesting;5=V e r yBoring)
We also recruit the crowd from the general public rather
than software testing experts. As the distribution presented in
Figure 3 indicates, 72.3% respondents have less than one yeartesting experience, and the remaining 27.7% have at least oneyear’s experience in software testing. Given that we recruitfrom the general public, a proportion of over a quarter havingtesting experience was a surprise to us (since testers do notoccupy 1/4 of the world’s population).
Our understanding is that their experience may come from
working on testing tasks posted on AMT or other crowd testingplatforms such as uTest or they may have professional careerexperience in software testing. Furthermore, those with testingexperience may favour our HIT, while those without suchexperience may have self-selected out. It is interesting to notethat an open call with no pre-requisites for test experience stillends up recruiting a crowd with higher-than-average testingexpertise.
Finally, we observe that 66.4% of the crowd workers
recruited are ‘new’. That is, they only completed one task,while the remaining workers completed at least two tasks.This high rate of returning workers may be correlated withthe interest level of our task: a topic to which we now turn.
RQ1.2: The crowd’s interest level: Figure 4 shows the
feedback from the crowd on the interest levels of our task. Theboxplot suggests a mean rating of 2.3 (between 2-‘Interesting’and 3-‘Normal’; lower values to note higher interest levels),and a median rating of 2. This relatively high rating of interestlevel may explain the high rate of returning workers revealedin the results of Figure 3. A detailed distribution of the numberof submitted tasks by each worker is given in Figure 5.The distribution shows that, although there is a high rate ofreturning workers, the total submissions are not dominated bya small number of ‘super workers’, thereby giving cause foroptimism regarding the crowd’s diversity.
RQ1.3: The crowd’s response rates: We investigated crowd
performance along two dimensions, the speed of task perfor-mance and the thoroughness of crowdsourced manual testingin terms of activity coverage. The speed data is extractedfrom AMT task records and also the app execution logs
submitted by the crowd. The app activity coverage data iscalculated based on the submitted logs, which are produced
by Android L
OGCA T . The log information contains detailed
activity launch, warning and error information.
22
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. Fig. 5. Task distribution per worker
Many testing scenarios may be sensitive to test speed.
Figure 6 presents three boxplots on the crowdsourced mobile
testing enabled by P OLARIZ . The ‘Create-Accept’ time is
the elapsed time from posting a task on AMT to a workerconsenting/accepting to work on the task. In the ﬁrst boxplot,the time for the 75th percentile is 61.0 minutes and 73.3%of the posted tasks were accepted within one hour. The‘Accept-Submit’ time reports the time from task acceptanceto submission of a solution by the crowd worker. The secondboxplot reveals that all 1,350 posted tasks ﬁnished within onehour, with a median value of 18.1 minutes.
Note that this time cost may not reﬂect the actual working
time, because the worker may simply accept the task, and work
on something else ﬁrst. Thus we regard the data presented inthe second boxplot as an upper bound on the working time.To further examine the lower bound, we check the crowd’sworking time based on the logs submitted. The logged timemay not reﬂect the time required to become familiar with our
P
OLARIZ platform, thus we regard it as a lower bound. As
shown in the third boxplot in Figure 6, the interquartile range
(25th to 75th percentiles) area shows a range of 2.3 to 6.2minutes, which falls into our expectation on the working time,i.e., within 10 minutes.
RQ2: The crowd’s level of coverage attainment: The
crowd’s performance in terms of test coverage attained isshown in Figures 7 to 9. First we examine the overall coverageand then consider the detailed coverage results for each of thesubjects.
Figure 7 shows boxplots that depict the number of covered
unique and non-unique activities per task. For non-uniqueactivities, the number of triggered activities is 7 to 23 forthe interquartile range, while the number for unique activities
is 3 to 9. Considering the real-world complexity of thesubjects, and the fact that our testing tasks are designed
to be lightweight/micro tasks, this coverage performance isreasonable and is within our expectation.
Figure 8 shows the crowd’s cumulative coverage over all 9
subjects. The horizontal axis represents tasks in chronologicalsubmission order, while the vertical axis reports activity cov-
erage. In total, 21,440 non-unique activities were manipulatedby the crowd, which covered 182 (out of 301 total) uniqueFig. 6. Task acceptance and completion times
Fig. 7. Number of covered activities per task
activities over all 9 subjects; 60.5% unique activity coverage.
Figure 9 reports the coverage achieved on each of the 9
subjects. Each subject is randomly assigned, so the x-axis
for the number of tasks may vary slightly between subjects,but each subject corresponds to at least 100 tasks. In all 9cases, the cumulative coverage grows rapidly for the ﬁrst 10tasks and subsequently ‘plateaus out’. In a few cases (e.g.,‘TheTrainline’), the coverage was still able to grow after morethan 100 tasks have been considered.
The highest coverage is achieved on the ‘CleanMyAndroid’
subject (87.5%). While the lowest coverage is on the ‘All-in-
One Printer Remote’ subject (21.6%), which is the only subjectwith a coverage below 60%. This low coverage is caused bythe app-speciﬁc contexts which require external hardware to bepresent, such as connecting to a HP printer. In our experiments,such external hardware was unavailable.
RQ3: The comparative activity coverage achieved by the
crowd and by S
APIENZ : We map the S APIENZ coverage
for each subject to the coverage achieved by the crowd,as the V enn diagrams illustrate in Figure 10. From the 9V enn diagrams we can see that the crowd covered more appactivities than the fully automated S
APIENZ approach in 8
of the 9 cases. As expected, the superior to main knowledgeof the crowd and the high-level understanding of the purpose
of the apps under test gives them an advantage in coveringactivities, compare to (cheaper) fully automated techniques.
23
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. Fig. 8. Overall cumulative crowd test coverage
Fig. 9. Crowd test coverage by subject, with total number of activities (in
parentheses).
There is only one case (Google Translate), for which
SAPIENZ triggered more activities than the crowd. However,
Figure 10 also reveals that the two approaches complement
one another in 5 out of the 9 cases, including the GoogleTranslate case.
RQ4: The improvement in S
APIENZ performance when
using motif patterns extracted from crowd-based tests :W e
analyse the effectiveness of the crowd motifs learned by our
POLARIZ motif extraction algorithm. On all 9 subjects, we
conﬁrmed all the learned motif events were, indeed, reused by
Sapienz. In Figure 11, we draw the coverage achieved by both
SAPIENZ with and without the learned motif events, where
the blue (darker grayscale, when viewed in black and white)
lines indicate the performance of S APIENZ with motifs, and
the red (lighter grayscale) lines denote results for S APIENZ
without a motif. As suggested by the line charts of cumulative
coverage on each of the subjects, the learned motifs were ableto enhance S
APIENZ in achieving higher test coverage in 6
out of 9 cases.Fig. 10. Activities by Sapienz and the crowd
Fig. 11. Improvement in coverage. Lower (red/lighter gray) lines denote
Sapienz without motif patterns.
In the remaining 3 cases, the integrated motif patterns
led to neither improvement nor disimprovement in terms of
app activity coverage. However, in the best case (Brightest
ﬂashlight), activity coverage improved by 100%, rising from
6 to 12 unique activities covered out of 27 total possible uniqueactivities.
The parameters used in the experiments have not been tuned
so our results represent fair lower bounds on the improvementthat could be expected to accrue; parameter tuning and more‘targeted’ learning might improve the results we report here.
24
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. B. Threats to V alidity
The primary threat to validity of our empirical studies is the
threat to external validity. Our subject dataset excluded two
types of mobile apps, i.e., games (with non-standard AndroidUI components) and those having an initial login activity (forprotecting the crowd’s privacy). Our results may therefore failto generalise to these kinds of apps.
To partly mitigate the generalisation issue, we randomly
chose apps, and we note that they did fall into multiple app cat-egories from widely-installed real-world apps, each of whichhas at least 1 million installs. We also cannot be sure that theimproved coverage observed for S
APIENZ would necessarily
be observed for other automated testing approaches. We also
found that our crowd contains a surprisingly high level oftesting expertise for the ‘general public’, a characteristic thatmay also fail to generalise to other scenarios
To minimise internal threats to validity, we tested both the
components of P
OLARIZ and the scripts for data collection and
analysis. One threat to internal validity that we cannot avoid
is related to the permission control component of P OLARIZ
platform: To guarantee that the app testing contexts (e.g., WIFIconnection) will not be changed by the crowd workers, thepermission component disallows any call to external activitiesthat are not part of the app under test.
It is possible that, for certain subjects, such calls to external
activities are a precondition to trigger some of their own activ-ities. Although the same restriction applies to both techniquesstudied, we cannot discount the possibility that such security-sensitive blocking might have disproportionately affected oneor other of our two treatments.
V. R
ELA TED WORK
Our work is most closely related to previous work on
extraction of useful patterns for app testing, crowdsourcingand automated test generation, the three areas it combines.
Pattern Extraction: Linares-V ´asquez et al. introduced Mon-
keyLab [5]. Like MonkeyLab, P
OLARIZ extracts patterns
from app usage data. However, unlike MonkeyLab, P OLARIZ
exploits a crowdsourced model which is context free (whereas
MokeyLab is concernewd with context in its model building).Furthermore, while MonekyLab focuses on extraction of valuefor a single app under test and is agnostic about its downstreamuse, P
OLARIZ introduces a novel crowdsourcing platform and
extraction algorithm that targets common patterns extractedfrom (and for) multiple apps, for subsequent exploitation by
the speciﬁc downstream application of automated test data
generation.
Crowdsourced Testing: Crowdsourcing is increasingly popu-
lar in software engineering research [22]–[27]. Previous workon crowdsourced software testing has formulated the testdesign problem as one to be outsourced to the crowd. Forexample, Dolstra et al. [28] and Vliegendhart et al. [29]demonstrated the usefulness of using Amazon MechanicalTurk workforce to perform continuous GUI testing.Schneider and Cheung [30] proposed to employ on-demand
crowd users for usability testing. Chen and Kim [31] proposeda Puzzle-based Automatic Testing (PA T) technique that trans-forms object mutation problems into puzzles for the crowdto solve. Pastore et al. [32] used crowdsourcing to tacklethe oracle problem [9]. In this previous work, crowdsourcingis used as an independent source of test data, whereas ourapproach uses the crowd to help guide automated testing.
Automated Test Generation: There exist several mature
semi-automated testing frameworks such as Appium [33] andRobotium [34] that are widely used in industry, but theseframeworks automate capture and replay, but not test case
design. By contrast, fully-automated mobile test generationresearch prototypes have rarely proved able to outperform
random testing [3], [35]. For example, Dynodroid [36] usesa biased random strategy, while SwiftHand [37] and OR-BIT [38] and PUMA [39] used model-based approaches.Other approaches such as ACTEve [40] and TrimDroid [41]are based on program analysis. Nevertheless, the coverageachieved by the state-of-practice tool Android Monkey hastended to achieve higher coverage than all of these researchprototypes, according to recent empirical results [3]. EvoDroid[42] was the ﬁrst search-based software testing system for
Android reported in the literature. In this work we chose
to use S
APIENZ [7], partly because it is publicly available
(unlike EvoDroid), but primarily because it has been recentlydemonstrated to signiﬁcantly outperform both the state-of-the-art automated testing (Dynodroid [36]) and the state-of-practice (Android Monkey). We thus used S
APIENZ in
order to ensure that our approach can further improve on the
current best-obtainable results for automated Android testing.This allows us to be sure that our approach advances the
current state-of-the-art in automated testing by hybridisingwith mining from crowdsourced usage patterns.
Compared to this previous work P
OLARIZ is the ﬁrst to
combine automated (search-based) testing and crowdsourcing
and also the ﬁrst to leverage cross-app usage patterns for
improved mobile testing. Our results demonstrate that thiscombination complements and extends the state-of-the-art insearch based testing.
VI. S
UMMARY
We introduced the P OLARIZ approach to crowd-based test-
ing, which leverages a non-professional crowd to provide testcases from which we extract motif patterns to help guide the
S
APIENZ automated testing technique. Our evaluation on 9
popular Google Play apps showed that P OLARIZ was able
to harness 434 crowd workers from 24 countries to perform1,350 testing assignments. The automatically-learned motifpatterns improved S
APIENZ ’ activity coverage of 6 out of 9
subjects, leaving it no worse on the remaining 3. We also foundthat S
APIENZ and crowd-based approaches complemented
one another in 5 out of 9 subject apps, further motivatingapproaches, such as ours, that seek to combine them.
25
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] C. Cadar and K. Sen, “Symbolic execution for software testing: Three
decades later,” Communications of the ACM, vol. 56, no. 2, pp. 82–90,
February 2013.
[2] M. Harman, Y . Jia, and Y . Zhang, “Achievements, open problems and
challenges for search based software testing,” in Proc. of ICST’15, 2015,
pp. 1–12.
[3] S. R. Choudhary, A. Gorla, and A. Orso, “Automated test input gen-
eration for Android: Are we there yet?” in Proc. of ASE’15, 2015, pp.
429–440.
[4] M. Bozkurt and M. Harman, “Automatically generating realistic test
input from web services,” in Proc. of SOSE’11, 2011, pp. 13–24.
[5] M. Linares-V ´asquez, M. White, C. Bernal-C ´ardenas, K. Moran, and
D. Poshyvanyk, “Mining Android app usages for generating actionable
GUI-based execution scenarios,” in Proc. of MSR’15, 2015, pp. 111–
122.
[6] K. Moran, M. Linares-V ´asquez, C. Bernal-C ´ardenas, C. V endome, and
D. Poshyvanyk, “Automatically discovering, reporting and reproducing
Android application crashes,” in Proc. of ICST’16, 2016, pp. 33–44.
[7] K. Mao, M. Harman, and Y . Jia, “Sapienz: Multi-objective automated
testing for Android applications,” in Proc. of ISSTA’16, 2016, pp. 94–
105.
[8] L. Gomez, I. Neamtiu, T. Azim, and T. Millstein, “RERAN: Timing-
and touch-sensitive record and replay for android,” in Proc. of ICSE’13,
2013, pp. 72–81.
[9] E. T. Barr, M. Harman, P . McMinn, M. Shahbaz, and S. Y oo, “The
oracle problem in software testing: A survey,” IEEE Transactions on
Software Engineering, vol. 41, no. 5, pp. 507–525, 2015.
[10] P . D’haeseleer, “What are dna sequence motifs?” Nature biotechnology,
vol. 24, no. 4, pp. 423–425, 2006.
[11] M. K. Das and H.-K. Dai, “A survey of dna motif ﬁnding algorithms,”
BMC bioinformatics, vol. 8, no. 7, p. 1, 2007.
[12] H. Huo, Z. Zhao, V . Stojkovic, and L. Liu, “Optimizing genetic
algorithm for motif discovery,” Mathematical and Computer Modelling,
vol. 52, no. 11, pp. 2011–2020, 2010.
[13] M. Kaya, “Mogamod: Multi-objective genetic algorithm for motif dis-
covery,” Expert Systems with Applications, vol. 36, no. 2, pp. 1039–1047,
2009.
[14] B. Fitzgerald and K.-J. Stol, “The Dos and Don’ts of Crowdsourcing
Software Development,” in SOFSEM 2015: Theory and Practice of
Computer Science, ser. Lecture Notes in Computer Science, 2015, vol.
8939, pp. 58–64.
[15] H. Zhu, P . A. Hall, and J. H. May, “Software unit test coverage and
adequacy,” ACM Computing Surveys, vol. 29, no. 4, pp. 366–427, 1997.
[16] A. M. Memon, M. L. Soffa, and M. E. Pollack, “Coverage criteria for
gui testing,” ACM SIGSOFT Software Engineering Notes, vol. 26, no. 5,
pp. 256–267, 2001.
[17] G. J. Myers, C. Sandler, and T. Badgett, The art of software testing.
John Wiley & Sons, 2011.
[18] L. Clapp, O. Bastani, S. Anand, and A. Aiken, “Minimizing gui event
traces,” in Proc. of FSE’16, 2016, pp. 422–434.
[19] M. Ermuth and M. Pradel, “Monkey see, monkey do: Effective genera-
tion of gui tests with inferred macro events,” in Proc. of ISSTA’16, 2016,
pp. 82–93.
[20] J. Ross, A. Zaldivar, L. Irani, and B. Tomlinson, “Who are the Turkers?
worker demographics in Amazon mechanical turk,” Department of
Informatics, University of California, Irvine, USA, Tech. Rep., 2009.[21]
J. Ross, L. Irani, M. Silberman, A. Zaldivar, and B. Tomlinson, “Who
are the crowdworkers? shifting demographics in mechanical turk,” inProc. of CHI’10, 2010, pp. 2863–2872.
[22] K. Mao, L. Capra, M. Harman, and Y . Jia, “A survey of the use
of crowdsourcing in software engineering,” Journal of Systems and
Software, vol. 126, pp. 57 – 84, 2017.
[23] F. Chen and S. Kim, “Crowd debugging,” in Proc. of FSE’15, 2015, pp.
320–332.
[24] J. Wang, S. Wang, Q. Cui, and Q. Wang, “Local-based active classiﬁca-
tion of test report to assist crowdsourced testing,” in Proc. of ASE’16,
2016, pp. 190–201.
[25] L. Ponzanelli, A. Bacchelli, and M. Lanza, “Leveraging crowd knowl-
edge for software comprehension and development,” in Proc. of
CSMR’13, 2013, pp. 57–66.
[26] K. Mao, Y . Yang, M. Li, and M. Harman, “Pricing Crowdsourcing Based
Software Development Tasks,” in Proc. of ICSE’13 (NIER Track), 2013,
pp. 1205–1208.
[27] K. Mao, Y . Yang, Q. Wang, Y . Jia, and M. Harman, “Developer
recommendation for crowdsourced software development tasks,” in Proc.
of SOSE’15, 2015, pp. 347–356.
[28] E. Dolstra, R. Vliegendhart, and J. Pouwelse, “Crowdsourcing GUI
tests,” in Proc. of ISSTA’13, March 2013, pp. 332–341.
[29] R. Vliegendhart, E. Dolstra, and J. Pouwelse, “Crowdsourced user
interface testing for multimedia applications,” in Proc. of CrowdMM’12,
2012, pp. 21–22.
[30] C. Schneider and T. Cheung, “The power of the crowd: Performing
usability testing using an on-demand workforce,” in Information Systems
Development. Springer, 2013, pp. 551–560.
[31] N. Chen and S. Kim, “Puzzle-based automatic testing: Bringing humans
into the loop by solving puzzles,” in Proc. of ASE’12, 2012, pp. 140–
149.
[32] F. Pastore, L. Mariani, and G. Fraser, “CrowdOracles: Can the crowd
solve the oracle problem?” in Proc. of ISSTA’13, March 2013, pp. 342–
351.
[33] “Appium: Automation for iOS and Android apps,” http://appium.io.
[34] “Robotium: User scenario testing for Android,” https://github.com/
RobotiumTech/robotium.
[35] X. Zeng, D. Li, W. Zheng, F. Xia, Y . Deng, W. Lam, W. Yang, and
T. Xie, “Automated test input generation for android: Are we really
there yet in an industrial case?” in Proc. of FSE’16, 2016, pp. 987–992.
[36] A. Machiry, R. Tahiliani, and M. Naik, “Dynodroid: An input generation
system for Android apps,” in Proc. of ESEC/FSE’13 , 2013, pp. 224–234.
[37] W. Choi, G. Necula, and K. Sen, “Guided GUI testing of Android apps
with minimal restart and approximate learning,” in Proc. of OOPSLA’13,
2013, pp. 623–640.
[38] W. Yang, M. R. Prasad, and T. Xie, “A grey-box approach for automated
GUI-model generation of mobile applications,” in Proc. of F ASE’13,
2013, pp. 250–265.
[39] S. Hao, B. Liu, S. Nath, W. G. Halfond, and R. Govindan, “PUMA:
Programmable UI-automation for large-scale dynamic analysis of mobileapps,” in Proc. of MobiSys’14, 2014, pp. 204–217.
[40] S. Anand, M. Naik, M. J. Harrold, and H. Yang, “Automated concolic
testing of smartphone apps,” in Proc. of FSE’12, 2012, pp. 59:1–59:11.
[41] N. Mirzaei, J. Garcia, H. Bagheri, A. Sadeghi, and S. Malek, “Reducing
combinatorics in GUI testing of Android applications,” in Proc. of
ICSE’16, 2016, pp. 559–570.
[42] R. Mahmood, N. Mirzaei, and S. Malek, “EvoDroid: Segmented evolu-
tionary testing of Android apps,” in Proc. of FSE’14, 2014, pp. 599–609.
26
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:56 UTC from IEEE Xplore.  Restrictions apply. 