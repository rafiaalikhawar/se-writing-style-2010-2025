Design and Validation ofPrecooked DeveloperDashboards
VladimirIvanov
Innopolis University
Innopolis, Russia
v.ivanov@innopolis.ruVladislavPischulin
Innopolis University
Innopolis, Russia
dalv6666@gmail.comAlan Rogers
Innopolis University
Innopolis, Russia
a.rogers@innopolis.ru
GiancarloSucci
Innopolis University
Innopolis, Russia
g.succi@innopolis.ruJooyongYi
Innopolis University
Innopolis, Russia
j.yi@innopolis.ruVasilii Zorin
Innopolis University
Innopolis, Russia
v.zorin@innopolis.ru
ABSTRACT
Despiteincreasingpopularityofdeveloperdashboards,theeffec-
tiveness of dashboards is still in question. In order to design a
dashboardthatiseffectiveandusefulfordevelopers,itisimportant
to know (a) what information developers need to see in a dash-
board,and(b)howdeveloperswanttouseadashboardwiththat
necessaryinformation.Toanswerthesequestions,weconducted
two series of face-to-face individual interviews with developers. In
thefirststepweanalyzedanswers,buildaGoal-Question-Metric
model and designeda precookeddeveloper dashboard.Then, dur-
ingthesecondseparateseriesofinterviews,wevalidatedtheGQM
and derived feedback on the designed dashboard. Given that the
costofdashboardcustomizationpreventsdevelopersfromutilizing
dashboards,webelievethatourfindingscanprovideasolidstarting
point to build precooked developer dashboards that can be readily
utilizedbysoftware companies.
CCS CONCEPTS
·Softwareanditsengineering →Softwaredevelopmentpro-
cess management ;
KEYWORDS
developer dashboards,interviewswithdevelopers, GQMmethod
ACMReference Format:
VladimirIvanov,VladislavPischulin,AlanRogers,GiancarloSucci,Jooyong
Yi,andVasiliiZorin.2018.DesignandValidationofPrecookedDeveloper
Dashboards.In Proceedingsofthe26thACMJointEuropeanSoftwareEngi-
neering Conference and Symposium on the Foundations of Software Engineer-
ing (ESEC/FSE ’18), November 4ś9, 2018, Lake Buena Vista, FL, USA. ACM,
NewYork, NY, USA, 6pages.https://doi.org/10.1145/3236024.3275530
1 INTRODUCTION
There is increasing interest and use of developer dashboards in
the software engineering industry [ 4,8,25]. Developer dashboards
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’18, November 4ś9, 2018, Lake BuenaVista,FL,USA
©2018 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-5573-5/18/11...$15.00
https://doi.org/10.1145/3236024.3275530aretypicallyusedtovisualizetheoverallstatusofaprojectÐthe
assumptionhereisthatvisualizationhelpsmanagers/developersbe
awareoftheoverallstatusoftheprojectstheyareworkingon,and
makepropercollaborative/individualdecisionswhiledeveloping
software, which will improve the overall productivity of a devel-
opmentteam[ 8,15,25].However, this promisingassumption ofa
developer dashboard would hold, only when a dashboard displays
informationneededbytheusers,withoutdistractingtheuserswith
unwantedinformation.
What kinds of information do developers want to see in a dash-
board?Whilethere would be no single answertothese questions,
weseektofindgeneralanswersfromsoftwareengineersinthefield.
Ourfindingscanbeusedtoconstructausefuldefaultdashboard.Al-
thoughmostmoderndashboardsarecustomizable,developersoften
usethedefaultdashboardduetothecostentailedbycustomization
Ðforexample,developersoftendonotwanttoreadthroughthefull
functionalities the dashboard provides, and consequently do not
customize the dashboard, as reported in [ 25]. The same paper [ 25]
reports that managers often do not customize dashboards either,
andcallsforabetterdefaultdashboard.Ourworktacklesthisniche.
Weconductface-to-faceindividual interviews withrepresenta-
tives of 44 different companies. We designedour questionnaire/ses-
sions,takingintoaccounttheGoal-Question-Metric(GQM)method[ 3].
Our key contributions in this paper are: (1) to the best of our
knowledge,weforthefirsttimeidentifymetricsdeveloperswanttosee
in dashboards, by employing an observational study [ 20], (2) provide
aGQMmodeltounderstandwhytheidentifiedmetricsmatterforthe
developers,and(3)identifyhowdeveloperswanttousedashboards
in their development processes. Given that the cost of dashboard
customizationpreventsdevelopersfromutilizingdashboards[ 25]
(our participants expressed similar responses), we believe that our
findingscanprovideasolidstartingpointforsoftwaredevelopment
organizationswho seekto improve team productivitybyutilizing
developerdashboards.Lastbutnotleast,ourvalidationprocesshas
revealedanadditionalaspectinrequirementsrepresentationand
visualization.
2 BACKGROUNDAND RELATED WORK
Thetaskofbuildingadashboardinvolvesselectingtheappropriate
metrics for measuring the software product, as well as the min-
imum required set of functionality in the dashboard. One of the
approachestoselectingthenecessarymetricsisthecollectionofESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA V.Ivanov,V.Pischulin,A.Rogers,G.Succi, J. Yi, andV.Zorin
allpossiblemeasurements.Theeffectivenessofthismethodisques-
tionable. First,data collection is a time-consumingprocess. Second,
the question of interpreting the results remains open. Another op-
tion for metrics selection is based on the Goal-Question-Metric
approach.Thisapproachdeliberatelyforcesthescreeningofmet-
ricsbybindingthemtopre-establishedgoals.Thisconceptprovides
interpretationof thecollecteddata andis the defacto standardin
buildingadashboard[ 5].Thus,metricselectionshouldbeguided
bytheproperselectionandprioritizationofgoalsofdashboard’s
users.Inthissectionwefirstreviewrelatedworkaboutwhatisthe
purposeofdashboardsinsoftwareengineeringandthenanalyze
work that describes howto apply GQMindashboardsdesign.
2.1 Dashboards forSoftwareDevelopment
Dashboardsincreasecollaborationandmakethecommunication
process more transparent [ 6,25], since they spread information
intheorganization,helpingtocontrolprojectstatus,findingbot-
tlenecks,increasingperipheralawareness,andcomparingteams.
Dashboardshavealsobeenfoundusefulindistributedteams[ 13].In
essence,properlydesigneddashboardscanbeusedasanextremely
effectivetoolforvisualcontrolforthedevelopmentofhigh-quality
software products [ 14].
Substantiallytherearethreebasictypesofdashboards:(1)strate-
gic, (2) operational and (3) analytical [ 10]. In [14], the following
two basic interaction modes of a dashboard is listed: pull and push.
In the pull, the user wants to use a dashboard in order to get some
specificinformation.Inthepush,adashboardisusedlikeanotifi-
cation panel, which pushes highly important information to a user,
triggeringalmostpredefinedfollowup actions.
The types of, and mode of interactions with dashboards are
strongly connected with their specific purposes. Yigitbasioglu and
Velcu[27]listthefollowingfourbasicpurposesforcreatingadash-
board:(1)performancemonitoring,(2)measurementconsistency,
(3) communication, and (4) planning. Lastly, to collect the data for
the dashboard there are mainly two approaches: manual [ 9] and
automatic(non-invasive)[ 21].
2.2 Designing Dashboards andGQM
At the end of this analysis, it should be clear that creating an ef-
fective and useful dashboard is not easy: stated simplistically it
requires one to select the łrightž data and the łrightž visualization
techniques.
The concept of łrightž is indeed hard to define. A step toward
the identification of such data and visualization techniques can
be performed using theGoal-Question-Metricsapproach [ 1].This
approachallowscompaniestodefinewhichdatashouldbecollected
and how it can be interpreted. It provides a goal-based framework
for software measurement.
In [22], the authors formed a GQM model for the dashboard,
whichfocusesonthesize,complexity,planning,costandquality
of the product being created, and validated it using a survey of 110
industryprofessionalsusingtheLikertscale.Respondentsratedthe
following metrics: Lines of Code (LOC), Function Points, McCabe’s
CyclomaticComplexity.Asaresult,74%ofrespondentsconsider
these 8 metrics to be the basis for software product measurements,
noticingfor the fact that LOCisthe leastsignificant metric.In[18]theauthoraimstodevelopasetofmetricsforsoftware
development groups to find out what software development teams
should measure to effectively monitor their progress. The work
was conducted in the infrastructure of a real company with 9 agile-
teams, through the collection of requirements, joint discussions of
requirements for thedashboardfunctionality, and thesubsequent
use andevaluation ofthe createddashboardinreal work.
The result of the work is 5 measurements in 3 different spheres,
covering the most important needs of agile-team of software devel-
opers for effective monitoring of their progress. There are Number
ofopendefects, Numberofestimatedandremaining storypoints,
Number of planned, Successfully executed tests, Integration speed.
3 GENERALSTRUCTUREOFTHE STUDY
The studyreported inthis articlehas two main stages (Fig. 1).We
startwithbuildingałtypicalžGQMneededfordashboarddesign
by running a series of interviews with industry. Then, we validate
theresultusingthesameinstrumentwithanothersetofcompanies.
Figure1shows the key stagesofthework.This diagram depicts 2
mainstages:"GQMmodelgeneration"and"GQMmodelvalidation".
The paper isstructuredinasimilar way.
Figure 1:Structure ofthe study
3.1 Building a łTypicalžGQM
Through the first step we collected information from software
engineers and developers to build a łtypicalž GQM, from which
to derive the dashboard. There is one proviso: we did not pose
explicit questions on company goals and strategies, to avoid being
exposed,alsounintentionally,tosensitiveinformation;rather,we
remained at a more generic level that allowed us to preserve full
confidentiality while stillcollecting everything practicallyneeded.
Designinganeffectivequestionnaireforourinterviewisnoteasy,
since the way questions are defined, organized, and laid down may
significantly influence the overall result. Therefore, we followed
the rules defined in [ 1,2,7,26]. We selected a closed questionnaire
usingthetechniquesproposedbyFurnham[ 11]andofPodsakoff
et al. [19] to minimize the biases in the responses, including redun-
dancyandreplication.Moreover,weemployedtheGQMitselfto
designthequestionnaire[ 1,2].Inotherwords,weusedtheGQM
approach to design a questionnaire aimed at defining a łtypicalž
GQM [12]. Taking this approach, the goalof the investigation was
to try to build a łtypicalž GQM used in software organizations,
or a significant subset of it, and the łtypicalž visualization to use
effectivelyin(several)companiesnothavingtheefforttobuilda
fully fledgeddashboard.DesignandValidation ofPrecookedDeveloperDashboards ESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA
3.2 ValidationofGQM
AfterbuildingtheGQMitneedtobevalidated.Theresultswere
validatedusing aface-to-faceinterviewswith anotherset ofcom-
panies. Itconsists of4main parts:
•part1:verifyingtheestablishedrelationshipsbetweenthe
indicators andthe goal, similar goals,dashboardfeatures;
•part2:checking availability ofmetrics incompanies;
•part3:reidentifyingthe goalsofthe developmentprocess;
•part4:collecting demographicdata.
Relationshipsbetweenmetricsandgoals: inordertoverify
therelationshipofmetricstothegoal,aprototypeofthedashboard
andsyntheticscenarioswereused,oneforeachgoal.Dashboard
isimplemented in aformofan HTMLpagewith JavaScriptinsets
for dynamic content changes (Fig. 2). The page contains 13 metrics
selected for the study. Each metric is placed in a tile. Three metrics
arerepresentedasagraph,10metricsarerepresentedasnumerical
values.On thepagethere isa sliderthat isresponsiblefor theday
of the iteration. Changing its position changes the state of metrics.
This is necessary to keep track of the change history. Each tile has
a color indication to indicate the acceptability of the state of the
containedmetric.
This prototype simulates the software development situation
byateamof5peopleusingtheScrummethodology,10iterations
were completed, each iteration consists of 14 days. The task of this
prototype is to find out whether the relationship between the met-
rics andthegoal isobvious toadeveloper, and alsotounderstand
whichmetrics are the mostrepresentative for this relationship.
Figure 2:Dashboard interface forinterview
Features of a dashboard: after a personal immersion in the
interactionwiththedashboardandtheendoftheanswerstothe
questions of the first scenario, the respondent was given the op-
portunity to comment on his/her experience of interaction. The
respondent was asked to comment on the interaction, point out
theextraelementsandwhatfunctionalityhe/shelacked.Afterthis,
thetransition to thesecond scenariowas carried outandafterthe
completion of which the respondent was asked about the desire to
supplementhisthoughtsonthe dashboard.
Metrics availability: the main source of metrics is the tools
used in companies. The Project Management Tool (PMT) is one of
themainsuchsupplierofquantitativeindicatorsofthedevelopment
process.Here we askedwhichcompany managementtoolisused.Goalsofthedevelopmentprocess: to reidentify the existing
goals(thatareusuallyrelatedtocommonissuesofthedevelopment
process), weasked thisquestiondirectly.Questionsabout thecur-
rentdevelopmentcycle,theplaceofwork,thecommunicationteam
were asked earlier. We also asked questions about the validation of
already identifiedgoals.
Demographics: the fourth part is devoted to demographic is-
sues; ithelpsto understandhowthe results can be generalized.
3.3 AdministrationoftheInstrument
To increase the external validity of our findings, we have followed
thebestpracticessuggestedintheliterature[ 16,17,24].Inparticu-
lar, we drew interview participants from multiple different compa-
nies located in a high-tech city, and conducted a face-to-face inter-
viewwitheachparticipant.Morespecifically,thequestionnairewas
firstsenttotheparticipants,thenamemberoftheresearchteam
interviewed them face-to-face, recorded the results in a written
document, shared the written document back to the participants to
ensure that theright information wascaptured,andcorrectedthe
resultsifrequired.Eachinterviewlastedaboutonehourandthe
overallpreparationforitandfollowupcheckrequiredabouttwo
hours.
4 BUILDINGłTYPICALžGQMAND
DEVELOPER’S DASHBOARD
4.1 Demographics
WhenbuildingałtypicalžGQMwehaveinterviewed44companies.
The working experience of respondents ranged from 1 to 20 years,
withamedianvalueof7(Figure 3(a)).Majorityofthem(89%)hadat
least a graduate degree. (Figure 3(b)): 86% hold a Master degree, 3%
haveaPhD,6%areBachelors,and5%havenouniversityeducation.
(a) Workingexperienceofthepartic-
ipants
(b) Profiles of the participants
Figure 3:Informationaboutparticipants
Overall,theparticipantswerehomogeneouslydistributedacross
the participating companies and they appear a reasonable repre-
sentation of the overall population of the developers and software
engineers present inthe area andaround the world [ 23].
4.2 Resulting łTypicalžGoals
Themajority(54%)answeredthateffortestimationisthebiggest
obstacle.Whiletheeffectivenessofeffortestimation(thatis,how
effective the estimated effort is) can be measured with a metricESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA V.Ivanov,V.Pischulin,A.Rogers,G.Succi, J. Yi, andV.Zorin
G1: More effective 
effort estimationG2: More efﬁcient use 
of resourcesG3: Better software quality 
and development process
Q1: What is the level of 
collaboration with 
customers?Q2: How adherent is 
the project with the 
original plan?Q3: How many features 
have been implemented 
so far?Q4: What is the quality 
of the project?Q5: Is the consumption 
of resources adherent to 
the budget?
M1: Progress state of 
the projectM2: Speed of the work 
performedM3: Status of testingM4: Status of software 
qualityM5: Effectiveness of 
effort estimation
•Iteration burndown chart
•Release burndown chart
•Cumulative ﬂow diagram
•% of feature cut•Team velocity
•Average cycle time
•Schedule variance
•(Amount of tasks)/(unit of time)
•Lead time•Code coverage
•% of passed tests
•Defect removal efﬁciency•# of detected defects
•# of unresolved defects
•Defect density
•Coupling
•Class/method length
•System spoilage•Effort estimation accuracy
Figure 4: The resulting łtypicalž GQM model where strong
labelsandhigh-frequencymetrics are depicted inboldface
(indeed,ourGQMmodelcontainseffortestimationaccuracyasone
ofitshigh-frequencymetrics),effortestimationitselfiscurrently
conducted in a rather ad-hoc way, relying on the experience of
the developers. Given this situation, we find strong need from the
industry for more systematic data-based effort predictionÐfurther
research on this area is warranted. Overall, we summarize our first
step results as follows:
Based on our results, we have extracted a łtypicalž GQM model (shown
in Figure 4), consisting of three goals: more effective effort estimation
(G1), more efficient use of resources (G2), better software quality and
development process (G3); five questions; five metric categories. We have
alsonoticedthatourinformantsshowlessinterestinsoftwarequalitythan
otherkindsofmetrics.Meanwhile,ourinformantsshowedthestrongest
interestin iterationburndown charts (81%).
4.3 Questions
We have identified the following łtypicalž questions ś three strong
questions and two moderate questions. We consider a question
strong,ifthequestionisidentifiedbasedonahigh-frequencyan-
swer.
Q1.Whatisthelevelofcollaborationwithcustomers? This
question mainly reflects the importance of customer satisfaction
(86%).Thisquestionalsocoversnumberofchangerequests(13%)
andteam velocity(36%), taskswiththe biggestcycletime (12%).
Q2.How adherent is the project with the original plan?
This question reflects the importance of delivering a product on
time (71%), percentage of implemented features (73%), and amount
of unimplemented tasks or user stories (75%). This question also
covers schedule variance (40%) and number of overdue tasks (13%)
andteam velocity(36%).
Q3.How many features have been implemented so far?
This question reflects the importance of implementation of fea-
tures(58%),percentageofimplementedfeatures(73%),andamount
of unimplemented tasks or user stories (75%), most critical defects
(36%).
Q4.What isthequalityoftheproject? Thisquestioncovers
quality-relatedresponsessuchasmeetingqualityandsafetystan-
dards(34%),qualitymetrics(27%),andmostcriticaldefects(36%).
This question also covers reports of crashes and other problems
(15%),percentageofpassedtests(13%)andtaskswiththebiggest
cycletime (12%).Q5.Is the consumption of resources adherent to the bud-
get?It covers budget-related responses such as staying within the
budget supported (30%), over-budgeted tasks (18%), cost perfor-
mance index (15%), costvariance (15%), andteam velocity(36%).
4.4 Metrics
We have identified the five distinct kinds of metrics: (1) metrics
to show the current progress state of the project, (2) metrics to
show the speed of workperformed,(3) metrics to show thestatus
oftesting,(4)metricstoshowthestatusofsoftwarequality,and(5)
metricstoshowtheeffectivenessofeffortestimation.Wehavealso
connected these metrics with the goals developers want to achieve,
throughaGQM model (Fig. 4). We have observedthat, ingeneral,
developersaremoreconcernedaboutmonitoringwhetherthede-
velopment process is on track than monitoring software quality.
Webelievethatwiththiscategorization,softwareengineerswillbe
abletounderstandmoreeasilywhatkindsofmetricsothersoftware
engineersaretypicallyinterestedin,thanwhentheysimplyread
throughthe listofmetrics.
M1.Progressstateoftheproject :Thismetriccategorymainly
representsiterationburndowncharts(81%)ofTable 1.Italsocovers
release burndown charts (28%)and cumulative flow diagram(13%)
ofTable1,and percentageoffeatures cutduringtheproject(18%)
ofTable3.
M2.Speed of theworkperformed : This metric category con-
cernsthedynamicaspectofthesoftwaredevelopmentprocessÐthat
is,howfastprogressismadeovertime.Thiscategorymainlyrep-
resents team velocity (51%) of Table 1. Other metrics that can be
includedinthiscategoryare:averagecycletime(10%)ofTable 1,
andschedulevariance(39%),amountoftasksperunitoftime(34%),
andleadtime (13%) ofTable 3.
M3.Status of testing :Thismetriccategorymainlyrepresents
code coverage (67%) of Table 2. Other metrics that can be included
inthiscategoryare:percentageofpassedtests(31%)ofTable 1,and
defectremovalefficiency (25%) of Table 3.
M4.Statusofsoftwarequality :Thesemetricsincludenumber
ofdetecteddefects(39%)andnumberofunresolveddefects(36%)of
Table1;defectdensity(49%),coupling(16%),andmethodorclass
length (10%) ofTable 2; andsystemspoilage(19%) of Table 3.
M5.Effectivenessofeffortestimation :Thismetriccategory
covers effort estimation accuracy (55%) of Table 3. The previous
metric categories do not sufficiently capture this high-frequency
(55%) metric, and we separate this single-item category from the
rest.
Developerswantadashboardtobeabletonotifythemwhenthe
developmentprocessisonthewrongtrack.However,wehavealso
spotted the difficulty of supporting this notification effectively Ð it
is difficult to picture the right track, when developers are currently
having difficulties in estimating the effort necessary to complete a
task. For this reason, we argue that research on effort prediction
deserves strongattention.
5 VALIDATION OFGQMAND DASHBOARD
5.1 Demographics
Asaresultoftheinterviews,responsesfrom30differentcompanies
werecollected.Mostofrespondents(66%)haveaBachelor’sdegree,DesignandValidation ofPrecookedDeveloperDashboards ESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA
Table 1:Metrics forsoftware development process
Metric Frequency
Iterationburndowncharts 81%
Team velocity 51%
Number ofdetecteddefects 39%
Number ofunresolveddefects 36%
Percentageofpassedtests 31%
Release burndowncharts 28%
Code coverage 24%
Cumulative flowdiagram 13%
Leadtime 13%
Averagecycletime 10%
Table 2:Metrics forsoftware quality
Metric Frequency
Code coverage 67%
Defectdensity 49%
Coupling 16%
Method/class length 10%
Lack ofCohesion ofMethods(LCOM) 4%
Table 3:Metrics forprocess quality
Metric Frequency
Effortestimationaccuracy 55%
Schedulevariance 39%
Amountofcompletedtasksper unitoftime 34%
Defectremovalefficiency 25%
Systemspoilage 19%
Percentageoffeatures cut duringthe project 18%
27% hold Master’s degree, and a 5% are PhDs. Work experience
differsfrom1to30yearswiththemedian5years.Themajorityare
men(86%),14%arewomen.Fiftyfivepercentofrespondentsworkin
micro-companies (less than 10 employees), 24% in large companies
(morethan500employees),14%inmedium-sizedcompanies(250to
500employees),and11%insmallcompaniesemployees(from10to
50). Percentage of respondents that apply a methodology based on
Agileprinciplesis65%,while19%ofrespondentsworkonScrum
methodology, and only 5% use Waterfall methodology; 11% use
othermethodologies.Almostahalf(55%)ofrespondentsworkin
ateamoflessthan5,36%inateamof6to10people,and9%ina
teamof11to15people.Sincetherespondentcancombineposts,
the distribution of the roles ofthe respondentsis asfollows:88%
of respondents are developers, 22% analysts, 19% technical leaders,
and13%one ofthe scrum-master,managerortester.
5.2 łTypicalžGQMValidation
In the first part of the survey we were checking the relationship
between metrics and goals. Validation of goal G2did not give
enoughresultsfor analysisduetorespondents’lackof awarenessofthecompany’sbudget.Therefore,wedonotconsiderthegoal
G2inthis section.
Forthegoal G178%ofrespondentsrecognizedtheproblemwith
the assessment of efforts. The key metric for 90% of respondents
was the łIteration Burndown Chartž; 76% mentioned the łEffort
EstimationAccuracyž;66%focusedonthełTeamVelocityChartž.
Amongthosewhorecognizedtheproblem79%confirmedthatthey
experiencedasimilarsituationinthelast5iterations.Thisconfirms
theimportanceoftheestimationofeffortintheGQMandinthe
corresponding dashboard. Relevant metricsfor this goal are based
on the łIteration Burndownž, łEffort Estimation Accuracyž, and
łTeam Velocityž (listedinthe order of importance).
ForG3we got the following result. During the interviews, 100%
ofrespondentsrecognizedtheproblemwiththequalityofthecode.
Thus, all respondents confirmed the importance of the goal related
to the quality of the code and the corresponding problem. The
distributionofakey metric isthe following:
•PassedTests(97%),
•Code Coverage(91%),
•UnresolvedDefects(76%),
•Class / MethodLength(64%),
•IterationBurndownChart(55%),
•DefectRemoval(55%),
•DetectedDefects(55%) andDefectDensity(45%).
Amongrespondentswhorecognizedtheproblem60%confirmed
that they had experienced similar situation in last 5 iterations. The
łAverageleadžtimeandłAveragecycležtimemetricswerenottaken
into account due to the impossibility of their interpretation by the
respondents. In the second part of the interview we were checking
theavailabilityofmetrics inreal-world environment. Indeed,vali-
dationofthetypicalGQMdependsontheavailabilityofmetrics.
RespondentswereaskedaboutthePMTandaboutsoftwaresolu-
tionsforqualitycontrolofthecode.Developers usethefollowing
toolsformetricscollection:Jira(35%),YouTrack(22%),Trello(19%),
Team Foundation Server (14%); and 10% of respondents were using
othersoftwareproducts.Forcodequalitytrackingrespondentsuse
Sonar(57%),Coverage.py(11%),differentproducts(21%);and11%
ofrespondentsuse notools.
5.3 Dashboard Validation
Inthefirstpartoftheinterview,therespondentsinteractedwith
theprototypeofthedashboard,seeFigure 2.Itdisplayed13metrics:
Iteration burndownchart, Team velocity, Release burndown chart,
Averagecycletime,Averageleadtime,Codecoverage,Passedtests,
Defect removal efficiency, Defected density, Class / method length,
Effortestimationaccuracy.Eachmetricwasplacedonaseparate
tile.Afterinteractionwiththedashboard,respondentwereasked
thequestion:łWhatwouldyouliketoaddtothefunctionalityof
the dashboardandwhy?ž.The answers were as follows:
•history andforecastedchangeinthe metric (71%),
•ability to view metrics for an individual team member (43%)
•possibility offiltering andaggregating metrics (37%)
•explanationofpossiblereasonsforchangingthemetric(24%)
•indicationofthe number of completedtasks(17%)
•indicationofthe remaining time of the iteration (10%)
•displayingahintaboutactionsinthecurrentsituation(10%)ESEC/FSE’18,November4–9, 2018, Lake Buena Vista,FL,USA V.Ivanov,V.Pischulin,A.Rogers,G.Succi, J. Yi, andV.Zorin
The respondents also reported that the function of displaying
critical changes (implemented in the prototype) was most useful
during the decision-making process. Thus, the functionality of
thedashboard shouldhave the following objectives, coherentand
complementary to the outcomes ofthe firstpart ofthis research:
•viewchanging metric,
•predict the changeinthe metric,
•displaycriticalvaluesofmetrics,
•filterandaggregate metrics,
•recommendaction onthe situation,
•displayprogressofwork.
6 VALIDITYOFRESULTS
Inthecontextandwiththeintrinsiclimitationsofanobservational
study run with an empirical constructivist approach, we claim that
itisreasonabletogeneralizełsomehowžtheresultsofthisstudy
beyondour study.
Such a claim, indeed, is łloosež since (1) all the work focuses
mostlyonobservational analysis anddoes not employ anyinfer-
ential statistics to assess the significance of the results (problem
of internal validity), (2) using a questionnaire with closed answers
exposes us to the risk of the respondent bias (problem of construct
validity), (3) the respondents were self-selected, and no random-
ization process occurred (problem of external validity) However,
wedothinkthatourfindingshavethepotentialtobewidespread
to a wider population, since we took precautions to mitigate the
above three classes of threats to validity even if it is not possible
toeliminatethem.Ourworkisafirstobservationalstudynotem-
ploying inferential statistics, thus the issue of the significance of
thefindingsisimportantbutnotessential.Still,onlyreplications
to this study would make our results stronger, and for this purpose
we are ready to share our instrument to any interestedresearcher.
In particular, we conducted the validation of the GQM and the
precookeddashboard.
7 CONCLUSIONS
Inthisstudy,wehaveintervieweddevelopersfromvariouscom-
panies,inanattempttoobtaininformationnecessarytobuildan
effective developer dashboards. We have identified five distinct
kindsofmetrics:(1)metricstoshowthecurrentprogressstateof
the project,(2) metricsto show the speedofwork performed,(3)
metrics to show the status of testing, (4) metrics to show the status
of software quality, and (5) metrics to show the effectiveness of
effort estimation. We have observed that, in general, developers
are more concerned about monitoring whether the development
processisontrackthanmonitoringsoftwarequality.Developers
want a dashboard to be able to notify them when the development
process is on the wrong track. We have connected these metrics
with the goals developers want to achieve, through a GQM model.
Finally,we have validated thetypical GQM using interviews with
aseparate setofrespondents.
ACKNOWLEDGMENT
The authors thank Innopolis University for funding this study and
Victor Basili for providing very valuable feedback on an earlier
versionofthis manuscript.REFERENCES
[1]Victor R Basili. 1992. Software modeling and measurement: the Goal/Question/-
Metricparadigm. (1992).
[2]Victor R. Basili, Gianluigi Caldiera, and H. Dieter Rombach. 1994. The Goal
Question MetricApproach. In EncyclopediaofSoftwareEngineering . Wiley.
[3]VictorR.BasiliandDavidM.Weiss.1984. AMethodologyforCollectingValid
SoftwareEngineering Data. IEEE Trans. SoftwareEng. 10,6 (1984), 728ś738.
[4]OlgaBaysal,ReidHolmes,andMichaelW.Godfrey.2013. DeveloperDashboards:
The Need for QualitativeAnalytics. IEEE Software 30,4 (2013), 46ś52.
[5]PatrikBeranderandPerJönsson.2006. Agoalquestionmetricbasedapproachfor
efficientmeasurementframeworkdefinition.In Proceedingsofthe2006ACM/IEEE
international symposiumonEmpiricalsoftwareengineering . ACM,316ś325.
[6]Jacob T Biehl, Mary Czerwinski, Greg Smith, and George G Robertson. 2007.
FASTDash: a visual dashboard for fostering awareness in software teams. (2007),
1313ś1322.
[7]TrevorGBondandChristineMFox.2013. ApplyingtheRaschmodel:Fundamental
measurement inthe humansciences . Psychology Press.
[8]Jan Bosch and Helena Olsson. 2017. Towards Evidence-Based Organizations:
LearningsFromEmbeddedSystems,OnlineGamesAndInternetofThings. IEEE
SoftwarePP, 99(2017). Early Access Article.
[9]EricBouwers,ArievanDeursen,andJoostVisser.2013. SoftwareMetrics:Pitfalls
andBestPractices.In Proceedingsofthe2013InternationalConferenceonSoftware
Engineering (ICSE’13) . IEEE Press,Piscataway, NJ, USA,1491ś1492.
[10] StephanFew. 2006. Information Dashboard Design. (2006).
[11]Adrian Furnham. 1986. Response bias, social desirability and dissimulation.
Personalityand individual differences 7,3 (1986), 385ś400.
[12]Vladimir Ivanov, Alan Rogers, Giancarlo Succi, Jooyong Yi, and Vasilii Zorin.
2017. What Do Software Engineers Care About? Gaps Between Research and
Practice. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software
Engineering (ESEC/FSE 2017) . ACM, New York, NY, USA, 890ś895. DOI:http:
//dx.doi.org/10.1145/3106237.3117778
[13]MikkelRJakobsen,RolandFernandez,MaryCzerwinski,KoriInkpen,OlgaKulyk,
and George G Robertson. 2009. WIPDash: Work item and people dashboard for
softwaredevelopmentteams. (2009), 791ś804.
[14]Andrea Janes, Alberto Sillitti, and Giancarlo Succi. 2013. Effective dashboard
design.CutterITJournal 26,1 (2013).
[15]Andrea Janes and Giancarlo Succi. 2014. Lean Software Development
in Action . Springer, Heidelberg, Germany. DOI:http://dx.doi.org/10.1007/
978-3-642-00503-9
[16]YasserKhazaal,MathiasvanSinger,AnneChatton,SophiaAchab,DanieleZullino,
Stephane Rothen, Riaz Khan, Joel Billieux, and Gabriel Thorens. 2014. Does Self-
SelectionAffectSamples’RepresentativenessinOnlineSurveys?AnInvestigation
inOnlineVideoGameResearch. JournalofMedicalInternet Research 16,7(july
2014),e164.
[17]Walid Maalej, Rebecca Tiarks, Tobias Roehm, and Rainer Koschke. 2014. On the
ComprehensionofProgramComprehension. ACMTrans.Softw.Eng.Methodol.
23,4,Article31(Sept.2014),37pages.
[18]Wilhelm Meding. 2017. Effective monitoring of progress of agile software de-
velopment teams in modern software companies: an industrial case study. In
Proceedings of the 27th International Workshop on Software Measurement and 12th
International Conference on Software Process and Product Measurement . ACM,
23ś32.
[19]PhilipM.Podsakoff,ScottB.MacKenzie,JeongYeonLee,andNathanP.Podsakoff.
2003. CommonMethodBiases in BehavioralResearch: ACritical Review ofthe
Literature and Recommended Remedies. Journal of Applied Psychology 88, 5 (10
2003),879ś903.
[20]Paul R. Rosenbaum. 2010. Design ofObservational Studies . Springer,New York.
[21]AlbertoSillitti,GiancarloSucci,andStefanoDePanfilis.2006. Managingnon-
invasivemeasurementtools. JournalofSystemsArchitecture 52,11(2006),676ś
683.
[22]Prashanth Harish Southekal and Ginger Levin. 2011. Validation of a generic
GQM based measurement framework for software projects from industry prac-
titioners.In CognitiveInformatics&CognitiveComputing(ICCI*CC),201110th
IEEE InternationalConference on . IEEE,367ś372.
[23]StackOverflow. 2017. Developer Survey Result 2017 . Technical Report. Available
onlineaturl: https://insights.stackoverflow.com/survey/2017 andretrievedon
August 16th,2017.
[24]Gergely Szolnoki andDieter Hoffmann. 2013. Online, face-to-face andtelephone
surveys ś Comparing different sampling methods in wine consumer research.
Wine Economicsand Policy 2,2 (2013), 57ś 66.
[25]ChristophTreudeandMargaret-AnneStorey.2010. Awareness2.0:stayingaware
of projects, developers and tasks using dashboards and feeds. In ICSE. 365ś374.
[26]DavidL.VannetteandJonA.Krosnick.2014. AnsweringQuestions:AComparison
ofSurveySatisficingand Mindlessness . John Wiley& Sons,Ltd,312ś327.
[27]OganMYigitbasiogluandOanaVelcu.2012. Areviewofdashboardsinperfor-
mance management: Implications for design and research. International Journal
ofAccountingInformationSystems 13,1 (2012), 41ś59.