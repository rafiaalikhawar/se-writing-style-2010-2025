Release Planning of Mobile Apps Based on User Reviews
Lorenzo Villarroel1, Gabriele Bavota1, Barbara Russo1,
Rocco Oliveto2, Massimiliano Di Penta3
1Free University of Bozen-Bolzano, Bolzano, Italy
2University of Molise, Pesche (IS), Italy ‚Äî3University of Sannio, Benevento, Italy
lorenzo.villarroel@stud-inf.unibz.it, {gabriele.bavota, barbara.russo}@unibz.it,
rocco.oliveto@unimol.it, dipenta@unisannio.it
ABSTRACT
Developers have to to constantly improve their apps by x-
ing critical bugs and implementing the most desired features
in order to gain shares in the continuously increasing and
competitive market of mobile apps. A precious source of
information to plan such activities is represented by reviews
left by users on the app store. However, in order to ex-
ploit such information developers need to manually analyze
such reviews. This is something not doable if, as frequently
happens, the app receives hundreds of reviews per day. In
this paper we introduce CLAP (Crowd Listener for rele Ase
Planning), a thorough solution to (i) categorize user reviews
based on the information they carry out ( e.g., bug reporting),
(ii) cluster together related reviews ( e.g., all reviews reporting
the same bug), and (iii) automatically prioritize the clusters
of reviews to be implemented when planning the subsequent
app release. We evaluated all the steps behind CLAP , show-
ing its high accuracy in categorizing and clustering reviews
and the meaningfulness of the recommended prioritizations.
Also, given the availability of CLAP as a working tool, we
assessed its practical applicability in industrial environments.
CCS Concepts
Software and its engineering !Software maintenance
tools;
Keywords
Release Planning, Mobile Apps, Mining Software Repositories
1. INTRODUCTION
The market of mobile apps is exhibiting a tangible growth
and it is expected to reach $70 billion in annual revenue by
2017 [15]. The typical app delivery mechanism is a store in
which on the one hand new releases of the app are available
for download, and, on the other hand, users rate the app and
post reviews. User reviews have the purpose of explaining
why the users like or do not like the app, report bugs or
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14-22, 2016, Austin, TX, USA
c2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.2884818request new features. In such a scenario, there is an enormous
competition among stakeholders producing similar apps. If
an app does not satisfy the users, and if relevant suggestions
to improve the app are simply ignored, it is very likely that
the app would loose its market share.
User reviews and ratings are therefore very important
assets in the development and evolution of mobile apps.
Indeed, satisfactorily addressing requests made through user
reviews is likely to increase the app rating [25]. However,
manually read each user review and verify if it contains
useful information ( e.g., bug reporting or request for a new
feature) is not doable for popular apps receiving hundreds of
reviews per day. For such reasons, researchers have developed
approaches to analyze the content of user reviews with the
aim of crowd-source requirements [14, 19, 21, 26]. Among
others, AR-Miner [14] is able to discern informative reviews,
group and rank them in order of importance.
While approaches to identify and classify relevant and
informative reviews have been proposed, it would be desirable
to have a fully-automated (or semi-automated) solution that,
given the user reviews for an app, recommends which ones|
being them requests for new features or for bug xes|should
be addressed in the next release.
In this paper we propose CLAP , an approach to (i) au-
tomatically categorize user reviews into suggestion for new
feature, bug report, and other (including non-informative
reviews); (ii) cluster together related reviews in a single re-
quest, and (iii) recommend which review cluster developers
should satisfy in the next release. Unlike AR-Miner [14],
CLAP classies reviews into bug report and feature sugges-
tion, providing additional insights to the developer about the
nature of the review. Also, while AR-Miner simply provides
a ranking of the user reviews based on their importance as
assessed by a pre-dened formula, CLAP learns from past
history of the same app or of other apps to determine the fac-
tors that contribute to determining whether a review should
be addressed or not.
We thoroughly evaluated each step of CLAP , as well as
of the whole tool. First, we performed a study to assess the
accuracy of CLAP in classifying reviews. The second valida-
tion stage aimed at comparing the review clusters generated
byCLAP with respect to manually-produced clusters. The
third validation assessed the ability of CLAP to recommend
features and bug xes for the next app releases. Last, but
not least, in the fourth validation stage we provided our
tool to managers of three Italian software companies to get
quantitative and qualitative feedback about the applicability
ofCLAP in their everyday decision making process.
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   14
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   14
2. CLAP IN A NUTSHELL
CLAP provides support to developers for the release plan-
ning of mobile apps by automatically importing and mining
user reviews through a three-step process detailed in the
following subsections.
2.1 Categorizing User Reviews
The rst step aims at classifying user reviews into three
categories: bug report ,suggestion for new feature , and other .
The rationale is that, as it will be clear in Section 2.3, de-
velopers can use dierent motivations to decide upon imple-
menting bug xes or requests for new features. Other tools
such as AR-Miner [14] classify reviews into informative and
non-informative. In our case, we make a more specic classi-
cation of informative reviews, whereas the non-informative
ones fall into the other category. Also, we are aware that,
besides bug reports and suggestions for new features, other
relevant information could be contained in user reviews ( e.g.,
general comments on the user experience in using the mo-
bile app). However, in this version of CLAP we choose to
focus the classication on bugs to x and features to be
implemented, since we believe that they concern the macro
activities in the maintenance and evolution of apps (as also
conrmed by the three project managers that participated
to the evaluation of CLAP ).
CLAP uses the Weka [8] implementation of the Random
Forest machine learning algorithm [11] to classify user reviews.
The Random Forest algorithm builds a collection of decision
trees with the aim of solving classication-type problems,
where the goal is to predict values of a categorical variable
from one or more continuous and/or categorical predictor
variables. In our work, the categorical dependent variable is
represented by the type of information reported in the review
(bug report, suggestion for new feature, or other ), and we
use the rating of the user reviews and the terms/sentences
they contain as predictor variables. We have chosen Random
Forest after experimenting with dierent machine learner
algorithms (details of the comparison are in the replication
package [29]). We adopt a customized text preprocessing to
characterize each review on the basis of its textual content.
The steps of such a process are detailed in the following
and the benets brought by each step will be shown in our
empirical evaluation (Section 3).
Handling Negations. Mining text in code reviews
present challenges related to negation of terms. For instance,
reviews containing the words \lag" and \glitches" generally
indicate bug reporting. However, there is a strong exception
to this general trend that is due to the negation of terms.
Consider the following review: \ I love it, it runs smooth no
lags or glitches ". It is clear that in this case, the context
in which the words \lag" and \glitches" are used does not
indicate any bug. However, the presence of these words in
the review could lead to misclassications from the prediction
model. Thus, we adopt the Stanford parser [28] to identify
negated terms in the reviews and remove them. For example,
we convert \ I love it, it runs smooth no lags or glitches " into
\I love it, it runs smooth ", a set of words better representing
the message brought by the review.
Stop-words and Stemming. Terms belonging to the
English stop-words list [2] are removed to reduce noise from
the user reviews. Also, we apply the Porter stemmer [27] to
reduce all words to their root.
Unifying Synonyms. One possibility to unify synonyms
rating < 2.5
TRUE FALSE
bug >= 1
TRUE FALSE
bug reporting needs update >= 1
TRUE FALSE
bug reporting wish >= 1
TRUE FALSE
feature suggestion should add >= 1
TRUE FALSE
other...
feature suggestionFigure 1: Example of regression tree generated by
CLAP when categorizing user reviews.
would be to use existing thesaurus such as WordNet [23].
However, in the context of user reviews, we found that
general-purpose thesaurus are not adequate. Thus, we rely
on a customized dictionary of synonyms that we dened by
manually looking at 1,000 reviews (not used in the empirical
evaluation) we collected for a previous work [10]. Examples
of synsets we obtained are ffreeze, crash ,bug, error ,fail,
glitch ,problemg(terms related to a bug/crash) or fadd,miss,
lack, wishg(terms related to the need for adding a new fea-
ture). Noticeably, words such as freeze, crash ,bug, and glitch
would not be considered synonyms by a standard thesaurus,
while they are very likely to indicate the same concept in
mobile apps reviews.
N-grams extraction. Besides analyzing the single words
contained in each review, we extract the set of n-grams com-
posing it, considering n2[2:::4]. For instance, the extrac-
tion of n-grams from a review stating \ The app resets itself;
Needs to be xed " will result in the extraction of n-grams
likeresets itself ,needs to be xed, etc.Note that the three
preprocessing steps described above are not performed on
the n-grams ( i.e.,they only aect the single words extracted
from the review). This is done to avoid loosing important
information embedded in n-grams. For example, managing
negations is not needed when working with n-grams, since
n-grams extracted from a review like \ I love it, it runs smooth
no lags or glitches " will include no lags, no lags or glitches ,
etc. Synonyms merging also is not applied to n-grams to
avoid changing their meaning, e.g.,\the app freezes" should
not be converted in \the app bug" (being freeze and bug
synonyms according to our thesaurus).
After text preprocessing, we classify a user review using
as predictor variables: (i) its rating, (ii) the list of n-grams
derived from it, and (iii) the bag of words left as output of the
previously described steps. Training data, with pre-assigned
values for the dependent variables are used to build the
Random Forest model. An example of generated classication
tree is shown in Figure 1 (due to limited space, we just show
the left-hand subtree of the root node).
2.2 Clustering Related Reviews
In order to identify groups of related reviews ( e.g., those
reporting the same bug), we cluster reviews belonging to the
same category ( e.g., those in bug report ). Clustering reviews
is needed for two reasons: (i) developers having hundreds of
reviews in a specic category could experience information
overloading, wasting almost all advantages provided by the
review classication, and (ii) knowing the number of users
who are experiencing a specic problem (bug) or that desire a
specic feature, already represents an important information
about the urgency of xing a bug/implementing a feature.
Note that we only cluster reviews classied as bug report
orsuggestion for new feature since those are the ones the
developer should be interested in for planning the next release
of her app.
15
15|reviews| < 1.5
TRUE FALSE
rating > 4.0 low priority
TRUE FALSE
|devices| < 1.5 low priority
TRUE FALSE
low priority ...Figure 2: Example of regression tree generated by
CLAP when prioritizing clusters.
Review clustering is performed by applying DBSCAN [17],
a clustering algorithm identifying clusters as areas of high
element density, assigning the elements in low-density regions
to singleton clusters ( i.e.,clusters only composed by a single
element). In CLAP , the elements to clusters are the reviews
in a specic category and the distance between two reviews ri
andrjis computed as: dist(r i;rj) = 1 VSM (ri;rj), where
VSM is the Vector Space Model [9] cosine similarity between
riandrjadopting tf-idf [9] as term-weighting schema. Before
applying VSM the text in the reviews is processed as de-
scribed in the categorization step (Section 2.1), with the only
exception of the synonyms merging. Indeed, merging syn-
onyms before clustering could be counterproductive since, for
example, a review containing \freezes" and a review contain-
ing \crash" could indicate two dierent bugs. DBSCAN does
not require the denition a-priori of the number of clusters to
extract. However, it requires the setting of two parameters:
(i)minPts, the minimum number of points required to form
a dense region, and (ii) , the maximum distance that can
exist between two points to consider them as part of the
same dense region (cluster). We set minPts = 2, since we
consider two related reviews sucient to create a cluster,
while in Section 3 we describe how to set .
2.3 Prioritizing Review Clusters
The clusters of reviews belonging to the bug report and
suggestion for new feature categories are prioritized with the
aim of supporting release planning activities. Also in this
stepCLAP makes use of the Random Forest machine learner
with the aim of labelling each cluster as high orlowpriority,
where high priority indicates clusters CLAP recommends to
be implemented in the next app release. Thus, the dependent
variable is represented by the cluster implementation priority
(high orlow), while we use as predictor features:
The number of reviews in the cluster ( jreviewsj). The
rationale is that a bug reported (feature suggested) by several
users should have a higher priority to be xed (implemented)
than a bug (feature) experienced (wanted) by a single user.
The average rating of the cluster ( rating ). We hypoth-
esize that a review cluster having a low average rating has
a higher chance to indicate a higher priority bug (or a fea-
ture to be implemented urgently) than a cluster containing
highly-rated reviews, and thus should have a higher priority.
For example, people frustrated by the presence of critical
bugs are more likely to lowly rating the app.
The dierence between the average rating of the clus-
ter and the average rating of the app ( rating app). This
feature aims at assessing the impact of a specic cluster on
the app total rating. We expect a lower dierence (especially
negative ones) to indicate higher priority for the cluster.
The average dierence of the ratings assigned by users
in the cluster who reviewed older releases of the app
(rating u). A Google Play user can review multiple releases
of an app over time. Clearly, her rating can change over time
as a consequence of her satisfaction in using the dierent
1
2 3
4Figure 3: User interface of CLAP .
releases. Given a cluster Ccontaining a set of reviews R
referring to the release ri, we compute the average dierence
of the ratings assigned by authors of Rwith respect to last
rating (if any) they assigned to the releases rx, withx<i . If
the authors of Rdid not review the app before ri, she is not
considered in the computation of  rating u. If none of the
authors ofRevaluated the app in the past,  rating u= 0.
The number of dierent hardware devices in the clus-
ter (jdevicesj ). One of the information available to app
developers when exporting their reviews from Google Play
is the \Reviewer Hardware Model", reporting the name of
the device used by the reviewer. We conjecture that the
higherjdevicesj , the higher the priority of a cluster. For
example, if a cluster of bug report reviews contains reviews
written by users exploiting several dierent devices, the bug
object of the cluster is likely to aect a wider set of users
with respect to a bug only reported by users working with a
specic device. Similarly, this holds in the case of \desired
features", since the same app can expose dierent features
on dierent devices ( e.g., on the basis of the screen size).
Also in this case, historical data with known (and labeled)
value of the dependent variable is used to build the Random
Forest decision tree. Note that, given the dierent nature of
reviews reporting bugs and those suggesting new features, the
prioritization is performed separately for clusters containing
the two types of reviews (bugs and features). A portion of a
tree generated in this step can be found in Figure 2.
2.4 CLAP Prototype
Figure 3 reports an excerpt of the user interface of
CLAP (we removed side and top menu due to lack of space).
In the example shown in Figure 3 the user imported in
CLAP a set of user reviews from the Facebook app. As
a result, the reviews have been categorized into suggestion
for new feature ,bug report, and other (see e.g., element 1 in
Figure 3).
In the example, the bug report category is expanded to
unveal its review clusters. Each cluster (see e.g., element 2)
has a label composed of (i) a simple identier ( e.g., C1), and
(ii) the ve most frequent terms in the reviews belonging
to it. Red clusters ( e.g., C0 in Figure 3) are those marked
byCLAP as \high priority" clusters, while grey clusters
(e.g., C1) represent the \low priority" ones. The tool also
16
16provides a feedback mechanism to allow the developer to
indicate whether or not she is going to implement the reviews
contained in a cluster (element 3 in Figure 3). Such a manual
feedback can be used by the developer to expand/revise
the automatic prioritization training set according to the
features/bugs actually implemented.
Finally, by expanding a cluster, one can see the reviews it
contains. As it can be seen in the example, the three reviews
of cluster C0 report a similar problem: despite the users
installed the Facebook app, when clicking on the app icon
they receive the \app not installed" message. Also in this
case there is a feedback mechanism (element 4 in Figure 3)
to change the review category or to assign it to a dierent
cluster.
3. EMPIRICAL STUDY DESIGN
The goalof this study is to evaluate CLAP in terms of its (i)
accuracy in categorizing user reviews in the three categories
of interest ( i.e., bug report, suggestion for new feature , and
others ), (ii) ability in clustering related user reviews belonging
to the same category (e.g., all reviews reporting the same
bug), (iii) ability in proposing meaningful recommendations
on how to prioritize the bugs to be xed and new features to
be implemented while planning the next release of the app,
and (iv) its suitability in an industrial context. The context
of the study consists of 1,763 reviews of 210 Android mobile
apps and three Italian software companies.
The material used in this evaluation along with its working
data set is publicly available in our replication package [29].
3.1 Research Questions
In the context of our study we formulated the following
four research questions (RQ):
RQ 1:How accurate is CLAP in classifying user re-
views in the considered categories? This RQ assesses
the accuracy of CLAP in classifying user reviews in
thebug report, suggestion for new feature , and others
categories. It aims at evaluating the step \categorizing
user reviews" described in Section 2.1.
RQ 2:How meaningful are the clusters of reviews gener-
ated by CLAP ?This RQ focuses on the meaningfulness
of clusters of reviews extracted by CLAP in a specic
category of reviews ( e.g., those reporting bugs). We are
interested in assessing the dierences between clusters
of reviews automatically produced by CLAP with re-
spect to those manually produced by developers. RQ 2
evaluates the step \clustering related reviews" described
in Section 2.2.
RQ 3:How accurate is the new features/bug xing pri-
oritization recommended by CLAP ?Our third RQ aims
at evaluating the relevance of the priority assigned by
CLAP to the bugs to x and new features to be im-
plemented in sight of the next release of the app. We
assess the ability of CLAP in predicting which bugs
will be xed (features will be implemented) by develop-
ers among those reported (requested) in user reviews of
releaseriwhen working on release ri+1. This RQ eval-
uates the prioritization step described in Section 2.3.
RQ 4:Would actual developers of mobile applications
consider exploiting CLAP for their release planningTable 1: Objects used in our research questions.
R
Q #Apps #Reviews Origin
R
Q1 200 1,000 Randomly selected from [13]
RQ 2 5 200 Reviews from popular apps referring to the same app release
RQ 3 5 463 Selected on the basis of specic criteria from [13]
RQ 4 2 100 Reviews from two very popular apps (Facebook and Twitter)
activities? For a tool like CLAP , a successful techno-
logical transfer is the main target objective. In RQ 4we
investigate the industrial applicability of CLAP with
the help of three software companies developing An-
droid apps. Thus, RQ 4evaluates the CLAP prototype
tool as a whole, as described in Section 2.4.
3.2 Context Selection and Data Analysis
Table 1 summarizes the objects ( i.e.,apps and user reviews)
used in each of our research questions. To address RQ 1we
manually classied a set of 1,000 users reviews randomly
selected from 200 dierent Android apps extracted from the
dataset by Chen et al. [13]. In particular, two of the authors
independently analyzed the 1,000 reviews by assigning each
of them to a category among bugs report ,suggestion for new
feature, and others . Then, they performed an open discussion
to resolve any conict and reach a consensus on the assigned
category. This was needed for 69 out of the 1,000 reviews.
In total, of the considered 1,000 reviews we labeled 235 as
bug report, 179 as suggestion for new feature , and 596 as
others . Then, we used this dataset to perform a 10-fold cross
validation, computing the overall average accuracy of the
model and reporting the obtained confusion matrix.
ForRQ 2we manually collected a second set of 200 user
reviews among ve Android apps, i.e.,Facebook ,Twitter,
Yahoo Mobile Client, Viber, and Whatsapp . For this
research question we have selected very popular apps since
we needed to collect from each app a good number of reviews
(i) related to the same app's release, and (ii) belonging to the
bug report or to the suggestion for new feature category. In
particular, we randomly selected from each of these apps 40
reviews, 20 bug reports and 20 suggestions for new features ,
referring to the same app's release1. Then, we asked three
industrial developers having over ve years of experience each
to manually clustering together the set of reviews belonging
to the same category ( e.g., bugs report ) in each app. We
clearly explained to the developers that the goal was to
obtain clusters of reviews referring to the same bugs to be
xed or feature to be implemented.
The three developers independently analyzed each of the
200 reviews to cluster them. After that, they reviewed to-
gether their individual clustering results and provided us a
single \oracle" reecting their overall point of view of the ex-
isting clusters of reviews. Once obtained the oracle, we used
CLAP , and in particular the process detailed in Section 2.2,
to cluster together the same sets of reviews.
As previously explained, to apply the DBSCAN clustering
algorithm we need to tune its parameter. We performed
such a tuning by running the DBSCAN algorithm on the
Yahoo app varying between 0.1 and 0.9 at steps of 0.1
(i.e., nine dierent congurations). Note that it does not
make sense to run DBSCAN with = 0:0 or= 1:0 since
the output would trivially be a set of singleton clusters in
the former case and a single cluster with all reviews in the
second case.
1As previously said, in CLAP we are not interested in clus-
tering reviews belonging to the other category.
17
17MoJoFM
10
02030405060708090100
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0
ŒµFigure 4: Tuning of the DBSCAN parameter.
To dene the best conguration among the nine tested
ones, we measured the similarity between the two partitions
of reviews ( i.e.,the oracle and the one produced by CLAP )
by using the MoJo eFfectiveness Measure (MoJoFM) [30],
a normalized variant of the MoJo distance based on the
minimum number of Move orJoin operations one needs to
perform in order to transform a partition Ainto a partition
B.MoJoFM returns 0 if partition Ais the farthest partition
away fromB; it returns 100 if Ais exactly equal to B. The
results of this tuning are shown in Figure 4. In general, values
between 0.5 and 0.7 allows to achieve good performances,
with the highest MoJoFM reached at 0.6. This is the default
value in CLAP , and thus the one we will use in the evaluation.
In order to evaluate the CLAP 's clustering step, we mea-
sured the MoJoFM distance on reviews of the remaining apps
(i.e.,excluding Yahoo ). We report the MoJoFM achieved
in two dierent scenarios:
Intermediate feedback available. As explained in Sec-
tion 2.4, CLAP allows the user to x review misclassications
(e.g., a review reporting a bug classied as a suggestion for
new features). Thus, we rstly simulate a scenario in which
the categorization of the reviews has been manually checked
(and xed, when needed) by the developer. This is done
by separately running the clustering algorithm on the 20
bug report reviews and on the 20 suggestions for new fea-
ture reviews available for each app. This scenario allows us
to assess the performances of the CLAP 's clustering step
\in isolation", without the risk of having the achieved Mo-
JoFM values aected by misclassication performed in the
categorization step.
Fully automated approach. To simulate a fully auto-
mated CLAP 's usage scenario in which the developer does
not act on the review categorization, we use CLAP to auto-
matically categorize the 40 reviews of each app identifying
those reporting bugs and suggesting new features. Then, we
cluster them by using the DBSCAN algorithm and com-
pare via MoJoFM the produced clusters of reviews with the
oracle dened by the developers. Note that in this case
it is possible that some of the 40 reviews are categorized
byCLAP in the other category. In a real usage scenario,
CLAP would not cluster these reviews; thus, we remove
them before the clustering. However, since it is not possi-
ble to compute the MoJoFM between two dierent sets of
clustered entities (reviews), we also removed these reviews
from the oracle. For this reason, in this evaluation scenario
we report (i) the MoJoFM achieved by our approach when
automatically categorizing the reviews, and (ii) the numberof instances wrongly discarded by our approach due to a
misclassication.
To answer RQ 3we exploited the Android user reviews
dataset made available by Chen et al. [13]. This dataset
reports user reviews for multiple releases of 21K apps,
showing for each review: (i) the date in which it has been
posted, (ii) the app's release it refers to, (iii) the user who
posted it, (iv) the hardware device exploited by the user,
(v) the rating, and (vi) the textual content of the review
itself. In addition, each app in the dataset is associated to
a metadata le containing its basic information, including
the \updated" optional eld that app's developers can use to
describe the changes they made to the dierent app's releases
(i.e.,a sort of release note shown in the Google Play store).
We exploited such a dataset to build, for a given app, an
oracle reporting which of the reviews left by its users for the
releaserihave been implemented by the developers in the
releaseri+1(i.e., high priority reviews) and which, instead,
have been ignored/postponed ( i.e., low priority reviews). To
reach such an objective, rstly we identied the apps in the
dataset having all the information/characteristics required
to build the oracle:
1. A non-empty \updated" eld containing at least one
English word . As said before, this is an optional eld where
the app developers can report the changes they applied in
a specic app's review. This rst ltering was automated
by looking for the \updated" elds matching at least one
term (excluding articles) in the Mac OS X English dictionary.
This left us with 11K apps.
2. Explicitly reporting the app's version to which the \up-
dated" eld refers . Often developers simply put in the \up-
dated" eld the changes applied to the last release of the
app without specifying the \release number" ( e.g., release
2.1). This is an information needed to build our oracle. In-
deed, starting from the release note ( i.e.,the content of the
updated eld) of a specic releaseri+1, we have to look at
the reviews left by users of the release rito verify which of
them have been actually implemented by the developers. We
adopt regular expressions ( e.g., version | release | v |
rfollowed by at least two numbers separated by a dot) to
automatically identify apps reporting the release number in
the \updated" eld. This left us with 1.4K apps.
3. Having a non-ambiguous release note (update eld) .
Release notes only containing sentences like \xed several
bugs" or \this release brings several improvements and new
features" are not detailed enough to understand which of the
user reviews have been implemented by developers. For this
reason, one of the authors manually looked into each of these
1.4K apps for those containing a non-ambiguous release note.
This selection led to only 73 apps remaining.
4. Having available at least 30 reviews for the release ri
preceding the ri+1described in the release note. The dataset
by Chen et al. does not report reviews for all releases of an
app. Thus, it is possible that the reviews for riare not avail-
able or are too few for observing something interesting. This
further selection process led to the ve apps considered in
our study: barebones 3.1.0 ,hmbtned 4.0.0 ,timeriffic
1.11,ebay 2.6.1 , and viber 4.3.1 .
The ve selected app releases received a total of 18,591 user
reviews from the rireleases to be labeled as \implemented"
or \not implemented" in ri+1. Since manually labeling all of
them would not be feasible in a reasonable time, we randomly
18
18selected from each app a statistically signicant sample of
reviews with 95% condence level and 5% condence in-
terval. This resulted in 463 reviews that were manually
analyzed by two of the authors independently, and labeled
as implemented/not implemented on the basis of the infor-
mation contained in the related release note. Also in this
case, conicts (raised for 37 reviews) were solved with an
open discussion. Once built the oracle for the ve apps, we
performed a 10-fold validation to assess the ability of CLAP ,
and in particular of the prioritization step described in Sec-
tion 2.3, to correctly identify the clusters of reviews that
should be prioritized in sight of the next app's release ( i.e.,
those that have been actually implemented by developers in
theri+1reviews). As already done for RQ 2, we report the
performance of CLAP considering two dierent scenarios:
1. A scenario in which the categorization of the reviews
into bug report, suggestion for new feature , and other , as well
as the result of the clustering step has been manually checked
(and xed, when needed) by the developer. To support such
a scenario, the two authors who labeled the 463 reviews as
implemented/not implemented also (i) categorized them in
one of the three supported categories, and (ii) manually clus-
tered them. This manual process led to the identication of
55 clusters of bug report reviews and 30 clusters of suggestion
for new features of which 5 of those reporting bugs and 9 of
those suggesting features contain implemented reviews ( i.e.,
arehigh priority clusters). In this rst scenario we assess
the accuracy of the CLAP 's prioritization step in \isolation",
by performing a ten-fold validation on the set of clusters
manually dened. Given the unbalanced distribution of high
priority and low priority clusters ( e.g., 5vs55 for those
related to bug reporting), we balanced at each iteration of
the ten-fold validation the training set via an under-sampling
procedure, randomly selecting from the training set an equal
number ( i.e., the number of the underrepresented cluster
category) of high and low priority clusters. To avoid any
bias, no changes were applied to the test set.
2. A fully automated scenario, in which we used CLAP to
categorize, cluster, and prioritize the obtained clusters. Note
that, dierently from the manually dened clusters, it is
possible that automatically generated clusters contain both
\implemented" and \not implemented" reviews (as manually
dened in the oracle), due to errors in the clustering step. In
this case we consider a cluster as correctly classied as high
(low) priority if its centroid has been marked as\implemented"
(\not implemented") in the manually dened oracle. Also
in this case, a ten-fold validation has been performed and
under-sampling applied in case of unbalanced training sets.
Finally, to answer RQ 4we conducted semi-structured inter-
views with the project managers of three software companies
developing Android apps2. Before the interviews, one of the
authors showed a demo of CLAP , and let the participant
interact with the tool. To avoid biases and evaluate the tool
with a consistent set of reviews, all project managers worked
with a version of CLAP having the reviews for Twitter and
Facebook imported. Note that, using the reviews of the
apps developed by the three companies was not an option,
since most of the reviews they receive are not in English
and the current implementation of CLAP only supports
such a language. The interviews lasted for two hours with
2RQ 4's participants were not the same involved in RQ 2.Table 2: RQ 1: Classication Accuracy of Reviews.
Ca
tegoryNo text N-gram Handling Stop words/ Unifying
preprocessing analysis & Negations Stemming Synonyms
R P R P R P R P R P
b
ug report 68% 78% 70% 78% 72% 80% 73% 81% 76% 88%
sugg. new feature 63% 68% 66% 70% 66% 72% 66% 78% 67% 87%
other 84% 77% 90% 76% 91% 79% 93% 80% 96% 86%o
verall accuracy 70% 73% 77% 81% 86%
Table 3: RQ 1: Confusion Matrix.
b
ug report sugg. new feature other Recall Precision
b
ug report 178 4 53 76% 88%
sugg. new feature 19 115 45 67% 87%
other 10 13 572 96% 86%
each company. Each interview was based on the think-aloud
strategy. Specically, we showed all the tool features to the
managers to get qualitative feedback on both the tool and
the underlying approach. In addition, we explicitly asked
the following questions:
Usefulness of reviews . Do you analyze user reviews
when planning a new release of your apps?
Factors considered for the prioritization phase . Are
the factors considered by CLAP reasonable and sucient
for the prioritization of bugs and new features?
Review categories . Is the categorization of reviews into
bug report andsuggestion for new feature sucient for release
planning or there are other categories that should be taken
into account?
Tool usefulness . Would you use the tool for planning
new releases of your apps?
Participants answered each question using a score on a
four-point Likert scale: 1=absolutely no, 2=no, 3=yes, 4=ab-
solutely yes. The interviews were conducted by one of the
authors, who annotated the provided answers as well as addi-
tional insights about the CLAP 's strengths and weaknesses
that emerged during the interviews.
4. STUDY RESULTS
This section reports the analysis of the results for the four
research questions formulated in Section 3.1.
RQ 1: How accurate is CLAP in classifying user re-
views in the considered categories? Table 2 reports
the Recall (R), Precision (P) and overall accuracy achieved
byCLAP when classifying user reviews. In particular, we
show the accuracy of our classier when considering/not con-
sidering the dierent text preprocessing steps. The second
column on the left (no text preprocessing) reports the clas-
sication accuracy (70%) obtained without performing any
text preprocessing ( i.e.,by providing to the machine learner
all terms present in the user reviews). By moving toward
the right part of Table 2, we can observe the impact on the
accuracy of CLAP when: (i) including the extracted n-grams
(+3%=73%), (ii) handling negations (+4%=77%), (iii) per-
forming stop word removal and stemming (+4%=81%), and
(iv) unifying synonyms (+5%=86%). As it can be seen, the
text preprocessing steps adopted in CLAP ensure a +14%
of accuracy over the baseline.
Table 3 reports the confusion matrix of CLAP , along
with recall and precision values for each category, detailing
the overall 86% accuracy (865 correct classications out of
1,000 reviews) achieved by CLAP . The most frequent case
of failure for CLAP is represented by the misclassication
of reviews belonging to bug report and suggestion for new
feature categories as other , accounting for a total of 98 errors
19
19Table 4: RQ 2: MoJoFM achieved by CLAP .
F
acebook Twitter Viber Whatsapp Average
Sc
enario I: Manual Categorization
b
ug report 76% 75% 67% 72% 73%
sugg. new feature 71% 83% 94% 100% 87%
Sc
enario II: Automatic Categorization
a
ll 72% (2) 77% (3) 70% (3) 80% (2) 77%
(72% of the overall errors). A manual inspection revealed
that this is mainly due to reviews, related to bugs or new
features, not containing any of the keywords that, according
to the learned decision tree, lead towards a bugornew feature
classication.
RQ 2: How meaningful are the clusters of reviews
generated by CLAP ?Table 4 shows the MoJoFM between
the clusters of reviews manually dened by developers and
those resulting from the clustering step of CLAP in the two
evaluation scenarios described in Section 3.2. In the rst
evaluation scenario, in which we assume that the developer
has xed possible categorization errors made by CLAP (e.g.,
abug report review classied as a suggestion for new feature ),
the average MoJoFM is 73% for bugs, and 87% for new
features, suggesting a high similarity between manually- and
automatically-created clusters. In one case, i.e., the clus-
tering of reviews suggesting new features in Whatsapp, the
partition is exactly the same, indicating the meaningfulness
of the clusters generated by CLAP .
In order to give a better idea of the meaning of such
MoJoFM values, Figure 5 shows the two partitions of reviews
manually-created by the developers involved in the study
(left side) and automatically generated by CLAP (right side)
for the 20 Twitter reviews suggesting new features . The
points in light grey represent the eleven reviews considered
both by developers and by CLAP as singleton clusters ( i.e.,
each of these reviews recommended the implementation of
a dierent feature). The points in black represent, instead,
the reviews clustered by developers into four non-singleton
clusters, depicted with dierent colors in Figure 5. The rst
two clusters (the grey and the green ones) are exactly the
same in the oracle and in the clusters generated by CLAP .
The yellow cluster is similar between the two partitions.
However, in the automatically generated partition it does
not include the review R5, isolated as a singleton cluster
byCLAP . Finally, the blue cluster composed of R8and
R9is the only one totally missed by CLAP , that does not
recognize the two reviews as semantically related. Overall,
these dierences resulted in a MojoFM of 83%.
MoJoFM(Twitter_Features) =  83%
Oracle Recommended
R2R1
R4R3 R5R7R6
R9R8R2R1
R4R3R5R7R6
R9
R8
Figure 5: RQ 2:CLAP vsoracle when clustering sug-
gestions for new features on Twitter.Table 5: RQ 3: Prioritization accuracy.
c
orrectly classied false positive false negative
Sc
enario I: Manual Categorization and Clustering
b
ug report 87% 9% 4%
sugg. new feature 80% 20% 0%
Sc
enario II: Automatic Categorization and Clustering
b
ug report 73% 15% 12%
sugg. new feature 69% 18% 13%
The example reported in Figure 5 is very representative of
the errors made by CLAP in clustering related reviews. We
observed as it tends to be more conservative in clustering the
reviews with respect to the manually produced oracle ( i.e.,it
generates more singleton clusters). While this could suggest
a wrong calibration of the parameter, we also replicated
this study with = 0:7 and= 0:8, since higher values of
should promote the merging of related reviews. However,
these settings resulted in lower values of the MoJoFM across
all experimented systems, due to a too aggressive merging of
reviews.
The bottom part of Table 4 reports the MoJoFM achieved
in the second evaluation scenario, where the automated cat-
egorization of CLAP has been applied. Note that in this
case we report the overall MoJoFM without distinguishing
between bug reports and suggestions for new features, since
we run the clustering on the whole dataset of 40 reviews for
each app. This is needed since the automatic categorization
of reviews could lead to the introduction of misclassication,
and it is not possible to compare via MoJoFM partitions
composed of dierent elements. For example, if a bug report
review is misclassied by CLAP as a suggestion for new
features , the set of reviews clustered in the oracle and those
clustered by CLAP would be dierent if looking into the
specic categories. For a similar reason, as explained in
Section 3.2, we excluded from the comparison reviews mis-
classied in the other category. The number of such reviews
are indicated in parenthesis in Table 4, and always account
for less than 8% of the classied reviews ( i.e.,no more than
three out of the 40 reviews categorized in each app is wrongly
put in the other category). As for the MoJoFM, it uctuates
between 70% (Viber) and 80% (Whatsapp), showing again
the ability of CLAP to generate clusters of reviews close to
those manually dened by developers.
RQ 3: How accurate is the new features/bug xing
prioritization recommended by CLAP ?Table 5 reports
the accuracy achieved by CLAP in classifying clusters of bug
report andsuggestion for new feature reviews as high priority
(i.e.,the cluster of reviews has been actually implemented
by the apps developers in the subsequent release) and low
priority . False positives are clusters wrongly classied by
CLAP ashigh priority , while false negatives are clusters
wrongly classied as low priority . Results are reported for
both the scenarios described in Section 3.2.
In the rst scenario (top part of Table 5), simulating a sit-
uation in which the CLAP user has manually xed possible
categorization and clustering errors, CLAP correctly priori-
tizes 87% of clusters containing bug report reviews (48 out
of 55), producing ve (9%) false positive and two false nega-
tives. The accuracy is slightly lower when prioritizing new
features to be implemented, with 80% of correctly classied
clusters (24 out of 30), six false positives (20%) and zero false
negatives. For example, a bug report cluster correctly highly
prioritized by CLAP is the one from the ebay app, in which
141 dierent users using a total of nine dierent hardware
20
20devices were pointing out a bug present in the release 2.6.0
that prevented the app user to visualize the seller's feedbacks.
This cluster also had a very low average rating ( rating =
2.2), much lower that the average app rating ( rating app=
-1.9); moreover, the 30 reviewers in this cluster who already
evaluated past ebay releases assigned a much lower score to
this specic release ( rating u= -1.5). The ebay developers
xed this bug in the release 2.6.1, reporting in the release
note: \ Fixed bug where seller feedback would not load ".
A false negative generated by CLAP when prioritizing
clusters reporting suggestions for new features is a single-
ton cluster from the barebones app, a lightweight mobile
browser. One of the users reviewing the release 3.0 assigned
ve stars to the app and asked for the implementation of
search suggestions (\ I wish it can have search suggestions
in the search bar "). Despite the single user requiring such
a feature and the high rating she assigned to the app, the
barebones developers implemented search suggestions in the
release 3.1: \ Added Google Search Suggestions ". The char-
acteristics of this cluster led CLAP to a misclassication,
since the decision trees generated in the prioritization step
tend to assign a high priority to clusters having high val-
ues forjreviewsjandjdevicesj , and low values for rating ,
rating app, rating u(see the example in Figure 2). Note
that these classication trees are the results of the training
performed on the ve considered apps. In a real scenario,
theCLAP user can explicitly indicate which clusters she is
going to implement, allowing the machine learner to adapt
the classication rules on the basis of the user feedback.
When considering the clusters as produced automatically
(bottom part of Table 5), the prioritization accuracy of
CLAP exhibits an expected decrease. 73% of clusters related
to bug reporting and 69% of those grouping suggestions for
new features are correctly prioritized, with the majority of
prioritization errors (15% and 18% for the two categories of
clusters) due to false positives ( i.e., low priority clusters clas-
sied as high priority ones). Such a decrease of performance
is due to misclassications in the review categorization step
(as shown in the RQ 1results, CLAP misclassies15% of
the reviews) and to errors introduced in the clustering step
(RQ 2). However, we still believe that this level of accuracy
represents a good starting point for helping app developers
during release planning activities. Indeed, as highlighted by
all the feedback mechanisms we implemented in CLAP , we
did not envision our tool to be used as a black box taking
user reviews as input and producing a list of prioritized clus-
ters. We rather look at it as a support for app developers
interacting with them in order to gather as much information
as possible from the user reviews.
Comparison with AR-Miner. To further assess the pri-
oritization performances of CLAP , we compared them with
the prioritization performed by the state-of-the-art technique
AR-Miner [14]. Note that we did not compare AR-Miner
with CLAP in the previous steps (i.e., the categorization
and clustering of reviews), since the categories exploited by
the two techniques are dierent, with AR-Miner limiting
its categorization to informative vs non-informative reviews
andCLAP looking for bugs reports and suggestions for new
features . Instead, both techniques aim at prioritizing groups
of reviews based on their \importance" for developers ( i.e.,
their relevance when working on a subsequent release). The
prioritization applied by AR-Miner focuses on the reviews
classied as informative and it is based on a weighted sum ofthree factors: (i) the number of reviews in the group (cluster),
(ii) the average rating of the reviews in the group, and (iii)
the temporal distribution of reviews (more recent reviews
are considered more important). Since AR-Miner is not
available3, we reimplemented its prioritization feature, and
tuned the weighting parameters as reported in [14]. Then,
we applied AR-Miner on the same set of clusters prioritized
byCLAP . In particular, we considered as informative, the
reviews that were manually tagged as bug reporting or as
suggestion for new feature and as cluster to prioritize those
manually dened for these two categories of reviews ( i.e.,
exactly the same clusters prioritized in this evaluation by
CLAP ). Then, we compare the Area Under the Curve (AUC)
for both techniques. We use AUC as AR-Miner produces a
ranked list whereas CLAP produces a classication, hence it
is not possible to directly compare precision and recall values.
As expected, CLAP obtained a higher AUC when prioritizing
bugs (0.86), while the AUC is lower (0.81) when prioritiz-
ing features. However, in both cases, the AUC achieved by
CLAP is much higher than the one achieved by AR-Miner
when prioritizing the same set of informative reviews (0.51).
Note that the prioritization step of the two techniques has
been compared exactly on the same set of manually created
clusters of informative reviews. The only dierence is that
CLAP separately prioritizes bug reports from suggestions for
new features , while AR-Miner prioritizes all the informative
reviews as a whole (this is why we only have one value of
AUC for it).
RQ 4: Would actual developers of mobile applica-
tions consider exploiting CLAP for their release plan-
ning activities? In order to answer our last research
question, we qualitatively discuss the outcomes of the semi-
structured interviews we conducted with project managers
of three Italian companies aimed at analyzing the practical
applicability of CLAP in a real development context.
Nicola Noviello, Project manager @ Next [5] . Nicola
answered our rst question ( i.e.,usefulness of user reviews)
with \absolutely yes", specifying that before planning a new
release of an app, the developers of his company manually
analyze the app reviews to identify critical bugs or feature
recommendations. Nicola also conrmed that such a task
is time consuming: when planning the release 2.0 of the
appUnlikely quotes [7] \A developer spent two days in
analyzing more than 1,000 reviews. While the need to x some
bugs and to implement some features was easily spotted due
to the fact that they were reported (required) by several users,
there were also interesting features and critical bugs hidden
in a large amount of non informative reviews. I strongly
believe that CLAP would have sensibly reduced the eort
we spent to identify such reviews. " Nicola also positively
answered to our questions related to the completeness of
the categories of the reviews considered by CLAP and the
factors it uses for prioritization (\yes" and \absolutely yes",
respectively). Concerning the review categories considered by
CLAP , Nicola suggested an additional category that could be
considered: \ reviews referring to the app sales plan ". Nicola
considers these reviews \ very important " and he explained
that \ the version 2.0 of the app Unlikely quotes was released
both in a free and non-free versions, with the latter introducing
some features for which the users explicitly claimed (in their
reviews) that they will to pay for having such functionalities. "
3We contacted the authors on July 29, 2015 and they con-
rmed that the tool is not publicly available.
21
21Thus, user reviews could not only be useful to plan what to
implement in the next release of an app, but also to dene
the sale strategies. Finally, Nicola was really enthusiastic
about CLAP and he will be happy to use it in his company.
Indeed, he considers the tool highly usable and ready to the
market. He also pointed out two possible improvements for
CLAP . First, it would be useful to make the tool able to store
and analyze user reviews coming from dierent stores ( e.g.,,
Google Play and Apple App Store): \putting together reviews
posted by users running the app on dierent platforms could
be important to discriminate between bugs strongly related
to the app from those lying in the server-side. For example,
if the bug is reported by both Android and iOS users, it is
very likely that the bug is in the services exploited by the
app, rather than in the app itself. " Also, Nicola suggested
to integrate in CLAP a mechanism that allows to read and
analyze \the reviews of competitive apps in order to identify
features for my app that are not explicitly required by my
users, but that have been suggested by users of competitive
apps. In other words, I do not want to listen only to my users
but also the users of competitive apps! " Clearly, this would
require the implementation of techniques to automatically
identify similar apps. We consider this point as part of our
future work agenda.
Giuseppe Socci, Project manager @ Genialapps [3] .
As well as Nicola, Giuseppe answered \absolutely yes" to our
rst question related to the usefulness of user reviews: \ Very
often reviews are informative and useful to understand which
are the directions for new releases. I usually analyze the
reviews manually and such a task is really time consuming.
In the rst year of life of our app Credit for 3 [1], I analyzed
more than 11,000 reviews, dedicating six or seven hours per
week to this specic task. However, keep up with the reviews
helps a lot in making the app more attractive. " Giuseppe also
answered \absolutely yes" to our second question related to
the completeness of the review categories: \When I manually
analyze the user reviews I classify them in exactly the same
categories. " Instead, Giuseppe answered \yes" to the question
related to the completeness of the factors used to prioritize
the bugs and the new features: \ While the exploited factors
are reasonable, in my experience I also implemented several
features and xed some bugs that require few hours of work
even if they were reported by just one person who is also
already happy about the app. For instance, a user of the
app Happy Birthday Show [6] rated the app with ve stars,
and requested to change the color of some buttons. Such
a request required just a couple of hours of work. Thus, I
decided to implement it. Considering the change impact of a
new request or a bug x might make the prioritization even
more useful ". In addition, Giuseppe highlighted that the
prioritization of the new features should take into account
the kind of revision to perform, i.e.,minor or major revision:
If a major revision is planned, I tend to include as many
feature requests as possible. Instead, if I am working on a
minor revision, I really look for the most important feature
requests to include (those having the highest payo). In this
case, the factors considered by CLAP in the prioritization
are certainly valid. Finally, Giuseppe answered positively
(\absolutely yes") to our last question and he is willing to
useCLAP as a support for the release planning of his future
apps. The only showstopper for the application of CLAP in
Genialapps is that most of the user reviews are written
in languages dierent from English ( e.g., Spanish, Italian,French). We are currently adapting the tool aiming at making
it multi-languages by exploiting automatic translation tools.
Luciano Cutone, Project manager @ IdeaSoftware
[4]. While Luciano considers the user reviews useful for plan-
ning new releases, in his company, in general, user reviews
are not analyzed. The reason is simple. IdeaSoftware usually
develops app on commission. Thus, instead of considering
user reviews, the developers of IdeaSoftware implement the
features and x the bugs required by their customers. De-
spite this, Luciano claimed that \ some of the features and
bug xes required by the customers of our apps were derived
from the (in)formal analysis of user reviews. ". Luciano an-
swered \yes" to the questions related to the completeness of
the review category and the factors used to prioritize the
bugs and the new features. However, he also noticed that the
tool could be more usable if a criticality index is provided
for each feature and bug. Specically, \ instead of having fea-
tures/bugs classied as high and low priority, I would like to
see a list of features/bugs to be implemented ranked according
to a criticality index ranging between 0 (low priority) and
1 (high priority). This would provide a better support for
release planning especially when the number of features/bugs
classied as high priority is quite high and I do not have
enough resources to implement all of them. " Finally, Luciano
claimed that the tool seems to be useful \ especially when a
high number of reviews needs to be analyzed " and he is willing
to use the tool in his company for analyzing the user reviews
of the apps they plan to develop for the mass market (as
opposed to those they currently implemented on commission
for specic customers). Luciano also suggested to capture
more information on how and when feature requests and bug
xes clusters have been implemented: \ For each cluster I
would like to store the version of the app in which I imple-
mented it. In this way I can maintain in CLAP the revision
history of my apps and I could automatically generate release
notes for each version ".
5. THREATS TO VALIDITY
Threats to construct validity are mainly related to impreci-
sions made when building the oracles used in the rst three
research questions. As explained in Section 3, the manual
classications performed for RQ 1andRQ 3, as well as the
golden set clusters for RQ 2have been performed by multi-
ple evaluators independently, and their results discussed to
converge when discrepancies occurred.
Threats to internal validity concern factors internal to our
study that could have inuenced our ndings. One threat
is related to the choice of the machine learning algorithm
(Random Forest). As explained in Section 2 we have exper-
imented various approaches and chosen the one exhibiting
the best performance, but we cannot exclude that machine
learners we did not consider (or dierent settings of the algo-
rithm) could produce better accuracy. Similar considerations
apply for the clustering algorithm. The parameter of the
DBSCAN algorithm has been chosen using the tuning ex-
plained in Section 2.2. For the comparison with AR-Miner
review prioritization, we use default weights reported in the
paper [14], but it could be the case that they are not the
most suited ones for our dataset. Finally, we are aware that
planning the next release is a very complex process which
involve dierent factors. Therefore, the prioritization simply
based on the factors we considered in CLAP is only a recom-
22
22mendation that need to be complemented by factors related
to the expertise and experience of software engineers.
Threats to external validity concern the generalization of
our ndings. In the context of RQ 1we chose to select the
1,000 reviews from a high number of apps (200) instead
that from just one or two apps to obtain a more general
model. Indeed, training the machine learner on reviews
of a specic app iwould likely result in a model eectively
working on appi's reviews, but exhibiting low performances
when applied on other apps. Still, while we tried to assess our
approach on a relatively large and diversied set of apps, it is
possible that results would not generalize to other apps, e.g.,
those developed for other platforms such as iOS or Windows
Phone, or the approach adaptation to reviews written in
languages dierent from English might not exhibit the same
performances we obtained.
6. RELATED WORK
Several works have focused the attention on the mining
of app reviews with the goal of analyzing their topics and
content [18,21,22,24], the correlation between rating, price,
and downloads [19], and the correlation between reviews and
ratings [24]. Also, crowdsourcing mechanisms have been used
outside the context of mobile development for requirements
engineering, for example to suggest product features for
a specic domain by mining product descriptions [16], to
identify problematic APIs by mining forum discussions Zhang
and Hou [31], and to summarize positive and negative aspects
described in user reviews [20].
Due to lack of space, we focus our discussion on approaches
aimed at automatically mining requirements from app re-
views. Galvis and Winbladh [12] extract the main topics in
app store reviews and the sentences representative of those
topics. While such topics could certainly help app develop-
ers in capturing the mood and feelings of their users, the
support provide by CLAP is wider, thanks to the automatic
classication, clustering, and prioritization of reviews.
Iacob and Harrison [21] provided empirical evidence of the
extent users of mobile apps rely on reviews to describe feature
requests, and the topics that represent the requests. Among
3,279 reviews manually analyzed, 763 (23%) expressed feature
requests. Then, linguistic rules were exploited to dened an
approach, coined as MARA to automatically identify feature
requests. Linguistic rules have also been recently exploited by
Panichella et al. [26] to classify sentences in app reviews into
four categories: Feature Request, Problem Discovery, Infor-
mation Seeking, and Information Giving. CLAP , dierently
from MARA [21] and the approach by Panichella et al. [26],
also provides clustering and prioritization functionalities to
help the developers in planning the new release of their app.
In our classication, we only consider categories relevant to
the subsequent clustering and prioritisation.
Chen et al. [14] pioneered the prioritization of user reviews
withAR-Miner, the closest existing approach to CLAP .AR-
Miner automatically lters and ranks informative reviews.
Informative reviews are identied by using a semi supervised
learning-based approach exploiting textual features. Once
discriminated informative from non-informative reviews, AR-
Miner groups them into topics and ranks the groups of
reviews by priority. The main dierences between AR-Miner
andCLAP are:1. Bug/new feature reviews vs. informative/non-
informative reviews .CLAP explicitly indicates to developers
the category to which each review belongs ( e.g., \bug re-
port" vs\suggestion for new feature"), while AR-Miner only
discriminates between \informative" and \non-informative"
reviews. Clearly, this dierent treatment also aects the
grouping step. Indeed, while in AR-Miner a specic topic
(e.g., a topic referred to a specic app's feature) could indi-
cate both suggestions on how to improve the feature as well
as bugs reports, in CLAP the review clustering is performed
separately between the dierent review categories. Also,
while both techniques exploit textual features to categorize
reviews, CLAP introduces a set of pre-processing steps ( e.g.,
n-grams extraction, negations management, customized syn-
onyms list) that, as shown in Table 2, help in substantially
increase the classication accuracy.
2. Recommending next release features/xes vs. ranking
reviews .CLAP exploits a machine learner to prioritize the
clusters to be implemented in the next app release. This
allows our approach to learn from the actual decisions made
by developers over the change history of their app (see also
the next point). On the opposite, AR-Miner ranks the
importance of reviews based on a prioritization score, i.e.,
a weighted sum of \prioritization factors". As shown in our
evaluation, CLAP outperforms AR-Miner in predicting the
items that will be implemented by developers in the next
release of their app. Above all, since CLAP recommends
reviews to be addressed in the next release based on the past
history, it would be able to weigh dierent features of the
prediction model dierently for dierent apps and in general
for dierent contexts. Finally, the bug/feature classication
permits the use of dierent prioritization models for dierent
kinds of change requests.
7. CONCLUSION AND FUTURE WORK
This paper described CLAP , a tool supporting the release
planning activity of mobile apps by mining information from
user reviews. The evaluation of CLAP highlighted its (i) high
accuracy (86%) in categorizing user reviews on the basis of
the contained information ( i.e., bug report ,suggestion for new
feature, and other ), (ii) ability to create meaningful clusters
of related reviews ( e.g., those reporting the same bug)|77%
of MoJoFM, (iii) accuracy ( 72%) in recommending the
features to implement and the bugs to x in sight of the
next app release, and (iv) suitability in industrial contexts,
where we gathered very positive qualitative feedbacks about
CLAP .
Such qualitative feedbacks will drive our future work
agenda, aimed at improving CLAP with novel features and
in particular: (i) the identication of similar apps in the
store with the goal of mining user reviews from competitive
apps; (ii) the multi-store support; and (iii) the automatic
translation of reviews in English to overcome the current
language limitation of CLAP .
Acknowledgements
The authors would like to thank all the people involved in
the evaluation of CLAP . Bavota is supported by the Free
University of Bozen-Bolzano through the RPMA project
(IN2026).
23
238. REFERENCES
[1] Credit for 3. https://itunes.apple.com/it/app/
credito-per-tre-soglie-in/id376583617?mt=8.
[2] English stopwords.
https://code.google.com/p/stop-words/.
[3] Genial apps website.
http://www.genialapps.eu/portale/.
[4] Ideasoftware website. http://lnx.space-service.it.
[5] Next website. http://www.nextopenspace.it/.
[6] Sing happy birthday songs.
http://happybirthdayshow.net/en/.
[7] Unlikely quotes. https://itunes.apple.com/it/app/
citazioni-improbabili-2.0/id555656654?mt=8.
[8] Weka. http://www.cs.waikato.ac.nz/ml/weka/.
[9] R. Baeza-Yates and B. Ribeiro-Neto. Modern
Information Retrieval . Addison-Wesley, 1999.
[10] G. Bavota, M. L. V asquez, C. E. Bernal-C ardenas,
M. Di Penta, R. Oliveto, and D. Poshyvanyk. The
impact of API change- and fault-proneness on the user
ratings of Android Apps. IEEE Trans. Software Eng. ,
41(4):384{407, 2015.
[11] L. Breiman. Random forests. Machine Learning,
45(1):5{32, 2001.
[12] L. V. G. Carreno and K. Winbladh. Analysis of user
comments: An approach for software requirements
evolution. In 35th International Conference on Software
Engineering (ICSE'13) , pages 582{591, 2013.
[13] N. Chen, S. C. Hoi, S. Li, and X. Xiao. Simapp: A
framework for detecting similar mobile applications by
online kernel learning. In Proceedings of the Eighth
ACM International Conference on Web Search and
Data Mining, WSDM '15, pages 305{314. ACM, 2015.
[14] N. Chen, J. Lin, S. C. H. Hoi, X. Xiao, and B. Zhang.
AR-miner: Mining informative reviews for developers
from mobile app marketplace. In Proceedings of the
36th International Conference on Software Engineering ,
ICSE 2014, pages 767{778, 2014.
[15] Digi-Captial. Mobile internet report q1 2015.
http://www.digi-capital.com/reports.
[16] H. Dumitru, M. Gibiec, N. Hariri, J. Cleland-Huang,
B. Mobasher, C. Castro-Herrera, and M. Mirakhordi.
On-demand feature recommendations derived from
mining public product descriptions. In 33rd
IEEE/ACM International Conference on Software
Engineering (ICSE'11) , pages 181{190, 2011.
[17]M. Ester, H. Kriegel, J. S, and X. Xu. A density-based
algorithm for discovering clusters in large spatial
databases with noise. In 2nd International Conference
on Knowledge Discovery and Data Mining (KDD-96) ,
pages 226{231, 1996.
[18] B. Fu, J. Lin, L. Li, C. Faloutsos, J. Hong, and
N. Sadeh. Why people hate your app: Making sense of
user feedback in a mobile app store. In 19th ACMSIGKDD International Conference on Knowledge
Discovery and Data Mining , pages 1276{1284, 2013.
[19] M. Harman, Y. Jia, and Y. Zhang. App store mining
and analysis: MSR for app stores. In 9th IEEE
Working Conference of Mining Software Repositories,
MSR 2012, June 2-3, 2012, Zurich, Switzerland , pages
108{111. IEEE, 2012.
[20] M. Hu and B. Liu. Mining and summarizing customer
reviews. In 10th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
pages 168{177, 2004.
[21] C. Iacob and R. Harrison. Retrieving and analyzing
mobile apps feature requests from online reviews. In
10th Working Conference on Mining Software
Repositories (MSR'13) , pages 41{44, 2013.
[22]H. Khalid, E. Shihab, M. Nagappan, and A. E. Hassan.
What do mobile App users complain about? a study on
free iOS Apps. IEEE Software , (2-3):103{134, 2014.
[23] G. A. Miller. WordNet: A lexical database for English.
Commun. ACM, 38(11):39{41, 1995.
[24] D. Pagano and W. Maalej. User feedback in the
appstore: An empirical study. In 21st IEEE
International Requirements Engineering Conference ,
pages 125{134, 2013.
[25] F. Palomba, M. Linares-V asquez, G. Bavota,
R. Oliveto, M. Di Penta, D. Poshyvanyk, and A. De
Lucia. User reviews matter! tracking crowdsourced
reviews to support evolution of successful apps. In
Proceedings of the 31st International Conference on
Software Maintenance and Evolution, ICSME 2015,
page To appear, 2015.
[26]S. Panichella, A. Di Sorbo, E. Guzman, C. A. Visaggio,
G. Canfora, and H. C. Gall. How can i improve my
app? classifying user reviews for software maintenance
and evolution. In Proceedings of the 31st International
Conference on Software Maintenance and Evolution,
ICSME 2015, page To appear, 2015.
[27] M. F. Porter. An algorithm for sux stripping.
Program , 14(3):130{137, 1980.
[28] R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng.
Parsing With Compositional Vector Grammars. In
ACL. 2013.
[29] L. Villarroel, G. Bavota, B. Russo, R. Oliveto, and
M. Di Penta. Replication package. http:
//www.inf.unibz.it/~gbavota/reports/app-planning.
[30] Z. Wen and V. Tzerpos. An eectiveness measure for
software clustering algorithms. In Proceedings of the
12th IEEE International Workshop on Program
Comprehension , pages 194{203, 2004.
[31] Y. Zhang and D. Hou. Extracting problematic API
features from forum discussions. In 21st International
Conference on Program Comprehension (ICPC'13) ,
pages 141{151, 2013.
24
24