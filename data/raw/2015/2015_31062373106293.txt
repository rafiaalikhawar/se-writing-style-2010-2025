Understanding the Impact of Support for Iteration on Code
Search
Lee Martie, André van der Hoek, Thomas Kwak
University of California, Irvine
Department of Informatics
Irvine, California 92697-3440, U.S.A.
{lmartie,andre,tkwak1}@uci.edu
ABSTRACT
Sometimes, when programmers use a search engine they know
more or less what they need. Other times, programmers use the
search engine to look around and generate possible ideas for the
programming problem they are working on. The key insight we
explore in this paper is that the results found in the latter case tend
to serve as inspiration or triggers for the next queries issued. We
introduce two search engines, CodeExchange and CodeLikeThis,
both of which are specifically designed to enable the user to directly
leverage the results in formulating the next query. CodeExchange
does this with a set of four features supporting the programmer to
use characteristics of the results to find other code with or without
those characteristics. CodeLikeThis supports simply selecting an
entire result to find code that is analogous, to some degree, to
that result. We evaluated how these approaches were used along
with two approaches not explicitly supporting iteration, a baseline
and Google, in a user study among 24 developers. We find that
search engines that support using results to form the next query
can improve the programmers’ search experience and different
approaches to iteration can provide better experiences depending
on the task.
CCS CONCEPTS
•Software and its engineering →Software notations and
tools ;
KEYWORDS
Code search, internet-scale, iterative
ACM Reference format:
Lee Martie, André van der Hoek, Thomas Kwak. 2017. Understanding the
Impact of Support for Iteration on Code Search. In Proceedings of 2017 11th
Joint Meeting of the European Software Engineering Conference and the ACM
SIGSOFT Symposium on the Foundations of Software Engineering, Paderborn,
Germany, September 4–8, 2017 (ESEC/FSE’17), 12 pages.
https://doi.org/10.1145/3106237.3106293
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
©2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5105-8/17/09. . . $15.00
https://doi.org/10.1145/3106237.31062931 INTRODUCTION
Unlike when people search on the Internet to be aware of the latest
news or current temperature, a distinctly informational activity
[60], programmers routinely search for source code on the Internet
[13,68,72] when they are looking for solutions to aid in their
current programming problem [ 19]. Sometimes the programmer
uses a search engine to find one specific code snippet. For example,
the programmer might need to remember how to write a few lines of
code (e.g., the code to open a database in PHP) [ 15,25]. Other times,
the programmer uses a search engine when she is not quite sure
what she is searching for and there is not exactly one code snippet
she has in mind. For example, the programmer might need to learn
a concept [ 15,66], such as database transactions, and needs to look
at multiple examples illustrating different aspects of databases and
alternative examples to clarify her understanding [ 15,25,36,61,66].
For another example, the programmer might need to get ideas
[72], such as when designing a new game and wants to see, and
sometimes compare [ 25], how other code handles game characters,
board state, or visualization.
Given how important searching for code on the Internet is to
programmers, researchers are investigating how to improve code
search engines. Some, for instance, have been investigating how
to support more expressive queries (e.g., searching by test case or
method signatures) that afford more precise matching of code com-
pared to keywords (e.g., [ 1,10,17,37,41,45,54,59,70,75]). Others
have investigated new matching and ranking algorithms (e.g., rank-
ing code higher with method names or class names matching the
keywords) so that more results presumed to better match the topic
described by the keywords are returned and appear towards the
top of the list (e.g., [14, 20, 27, 29, 35, 42, 43, 49, 79]).
While many different approaches for improving code search
exist, these approaches are generally similar in one very visible
design decision: they are non-iterative approaches. They expect a
query and optimize on returning the best matching results for the
query, occasionally offering filters to help scope the results (e.g.,
programming language or file type filters) [ 2,5]. This focus on a
non-iterative design for search engines is mirrored in how search
engines are evaluated [ 46]. Typically, a group of experts score the
performance of search engines by the results returned for some
representative set of queries, with the score reflecting how on topic
the results are.
While a search engine that returns the code the programmer
is looking for after the first query appears ideal, many times the
programmer is not sure what she is looking for and does not search
for code with a single query. Instead, the programmer issues mul-
tiple queries [ 12,15,34,67,71], where, after receiving results, the
774
ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Lee Martie, André van der Hoek, Thomas Kwak
programmer modifies their query by removing keywords, adding
keywords, or some combination of both, and repeats this process
multiple times [ 12,34,67]. That is, search looks like an iterative pro-
cess where programmers often submit a query, get results, reflect
on and learn from the results, submit a modified query in response
to the results, get new results, and so on, until the programmer
stops searching.
Cognitive processes in which programmers engage possibly ex-
plain why code search is often iterative. Particularly, when pro-
grammers are working on a programming problem, what they
are working on, a solution, is often not immediately understood
[19,32,69]. However, as programmers begin to look at some code or
consider possible ideas, they are faced with constraints or different
perspectives not previously considered, changing their understand-
ing, and a new understanding will often change the next code and
ideas considered [ 19,21,44,53]. The implication of this on code
search is that, when programmers search for code not clearly un-
derstood (e.g., cases when learning or needing ideas) code results
can cause them to change their understanding of what they are
looking for and, thus, the next code searched for — making the
search iterative.
Our research investigates what happens when programmers are
explicitly supported in searching iteratively for code. It particularly
answers the following research question:
What is the impact of explicitly supporting software developers in
searching iteratively on the experience, time, and success of the code
search process on the Internet?
The key insight we explore to support iteration is that the code
returned for a query tend to serve as inspiration or triggers for
the next queries issued. We introduce two search engines, Code-
Exchange (CE) and CodeLikeThis (CLT), specifically aimed to en-
able the user to directly leverage the results in formulating the
next query. CE [ 47], previously developed but now built on the
Specificity ranking algorithm [ 43], provides a set of four features
supporting the programmer to use characteristics of the results to
find other code with or without those characteristics. For exam-
ple, if a result is undesirable because it is too complex, then the
user can refine her query to find code that is less complex than
the undesirable result. Rather than using particular characteristics,
CLT supports simply selecting an entire result to find code that
is analogous, to some degree, to that result. For example, if the
user receives an implementation of an AI for chess but wants to
see other similar approaches to learn from, then she can select the
entire result to find other similar approaches.
We conducted a user study with 24 developers evaluating the
iterative approaches with two non-iterative approaches (a baseline
and Google). The baseline was a control used to measure the impact
of a lack of iteration support, while maintaining the same code index
used in the iterative approaches. As such, the baseline was created
by removing the iterative features from CE, leaving a traditional
looking search engine. While Google is not a code search engine
per se and indexes a much greater amount code on web pages
than our iterative approaches, it is important to evaluate the mostpopular form of search today [ 6] to gain an understanding of how
developers iteratively search with it.
The rest of the paper is organized as follows. Section 2 presents
relevant background in code search. Section 3 introduces each of
the search engines used in the user study. In Section 4 we discuss
the design of the experiment. Section 5 presents the results in detail.
Section 6 presents threats to validity. In Section 7 we conclude with
the implications of the results and future work.
2 BACKGROUND
Previous research in code search can be divided into empirical
studies on how developers search for code on the Internet and
tool research that seeks to provide new ways of supporting code
search on the Internet. In this section, we present a summary of
both groups of research.
2.1 Empirical Studies on Internet Code Search
Several types of studies (e.g., surveys, search log analysis, field
studies, and lab studies) have been conducted to understand why
and how developers search for code. Surveys asking programmers
why they search for code on the Internet have been conducted by
Simet al. [66], Sadowski et al. [61], Stolee et al. [72], and Hucka and
Graham [ 36]. Overall, these survey studies find that the motivations
to search for code on the Internet include getting ideas/inspiration,
learning, remembering, clarifying knowledge, and finding code to
reuse as-is.
Lab studies have been conducted looking at how programmers
search for code by Scott Henninger [ 30], Sim et al. [67], and Brandt
et al. [15]. Henninger and Sim et al. ’s findings suggest that search-
ing for code is highly iterative, spanning a sequence of queries
(from 2.38 to 7.25 on average), where each new query is often a
modification of the previous (often making it more specific, called a
refinement, or less specific, called a generalization). Further, Brandt
et al.’s findings confirm the motivation to search found in the survey
studies above.
Studies by Bajracharya et al. [12], Brandt et al. [15], and Holmes
[34] examined patterns in search engine logs to understand how and
why programmers search for code. Overall, these studies confirm
that programmers submit multiple queries on average and some of
their motivations include learning new concepts or being reminded
of how to accomplish a programming task.
Lastly, Rosalva Gallardo-Valencia et al. [25] conducted a field
study onsite at a company in Peru where they observed employees
search for code on the Internet. The study found that the majority
of searches were concerned with learning, remembering, gaining a
deeper understanding, solving a bug, translating from English to
Spanish, and comparing candidate solutions.
2.2 Tool Support for Code Search on the
Internet
Research has explored a wide range of tools and techniques to
support code search on the Internet. These tools are organized in
the following six subsections.
2.2.1 More Expressive Queries. Recognizing that keywords do
not allow developers to easily target their search to the content
775Understanding the Impact of Support for Iteration on Code Search ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
of code, several search tools support structural queries, usually
submitted with an advanced query form. These approaches sup-
port search by the code’s method signature [ 65], packages [ 33],
framework [ 77], and language constructs (e.g., if statements) and
relationships (e.g., one method calls another) [43].
Rather than searching for functionality based on the structure
of the code, several approaches have investigated how to support
developers in more meaningfully searching for specific functional-
ity by supplying semantic queries. A common approach is to use
test cases as queries to find code passing them [37, 41, 59, 78].
Several approaches support the user to write part of the code
she needs (at the method or statement granularity) and to submit it
as a query to get results that complete it. This approach is intended
to more seamlessly go from code to results with no keyword query
in between [16, 17, 51, 54, 62].
2.2.2 Better Ranking Algorithms. Other research has investi-
gated how to improve the ranking of the code that is returned. To
return more on topic results, one approach automatically adds re-
lated words to the current keywords to match topically related code
that would be missed by matching only against the programmer’s
keywords. The terms added can come from a variety of thesauruses
[42], rule systems mapping keywords to related terms [ 20], related
Java documentation [ 28], or code the developer is currently writing
[14]. A similar, but different, approach is to index code in the search
engine not only with terms occurring in it, but also with descriptive
terms elsewhere [18, 33, 77, 82].
Linstead et al. found that it is possible to improve ranking perfor-
mance for code by matching keywords against the most qualified
parts of fully qualified type names in code (e.g., class names) in an
algorithm called Specificity [43].
2.2.3 Query Creation Support. Several approaches attempt to
eliminate the effort of formulating queries by automatically con-
structing queries on behalf of the user and continuously pushing
results to the user. The insight behind these approaches is that the
context (e.g., opened projects) of the developer can determine, in
part, some initial queries [8, 27, 35, 79].
While not Internet code search approaches, local code search
approaches, for various maintenance tasks (e.g., finding a method
to change in a local project), support the programmer in replacing
or completing their keyword queries when results are too few or
deemed inadequate for the maintenance task. These approaches
utilize issue tracking systems [ 29] and various statistics in the
project [26, 58].
2.2.4 Result Usability. While returning topically related code
to a query is crucial, other research has noted that the usability of
the results in terms of their quality, understandability, and ease of
integration are also important for search. To control quality, several
approaches match against code that is more popular. Measuring
popularity has been done by counting the number of times code is
used by other code [ 43,49] and extracting high level patterns from
the code indexed and counting how often those patterns occur in
the search engine [22, 39, 50].
Another critical part of code search is the ability of the user to
understand the code results. One very early approach supported the
user to select parts of the code to issue “why” questions to retrievemanually created documentation explaining the selected parts [ 24].
Another approach collapses code results into groups by different
functionalities in the code and supports annotations [ 63,64]. Some
other methods include documentation and comments from the web
with the results [ 57,80], summarizations [ 81], or examples of usage
[52].
2.2.5 Result Navigation. Traditionally, navigation of code search
engine results is done by paging through 10 ranked results at a
time. However, some code search engines, many commercial, sup-
port navigating the results by scoping them with descriptive fields
called filters. For example, the commercial search engine Krugle [ 3]
supports scoping results by known projects, file types, and authors
of the code indexed.
2.2.6 Iteration Support. Little preexisting work supports how
developers search. In particular, little work exists on using the re-
sults to create the next query. Henninger’s work around 1994 was
first to present iteration as an issue and demonstrated in CodeFinder
that it is possible to recommend keyword refinements from the
results by using the spreading activation algorithm on a LISP reposi-
tory of 1800 snippets [ 31]. Henninger showed the recommendations
helped find code for ill-defined code search tasks. Mica [ 74] offers
Java SDK specific refinement recommendations by recommending
keywords in the results that also occur in the Java SDK libraries
(shown to be used in half the queries in a field study). More re-
cently, Bajracharya et al. presented ideas, yet to be evaluated, on
recommending query refinements by function calls and types used
in functions occurring in the results [ 11]. However, much more
research can be done beyond refinement recommendations, and it
is important to do so given that in 100% of the empirical studies
that looked at the behavior of code search suggest that code search
on the Internet is often iterative.
3 SUMMARY OF SEARCH ENGINES
In this section, we describe the details and rationale behind the iter-
ative approaches and briefly introduce the baseline approach. Since
Google is well known in general, we do not include a description.
3.1 CodeLikeThis (CLT)
Sometimes it is easier for people to recognize something that resem-
bles what they want rather than to say what they want [ 38,56]. To
support using a result that does or does not resemble helpful code
in order to find other helpful code, we designed a new method of
search by similarity, implemented in CLT. After the first keyword
query, CLT presents the user with a diverse set of results, using the
Hybrid diversity ranking algorithm [ 48], where the results are on
topic, but diverse across other characteristics (e.g., libraries used,
authors, and implementations). From this diverse set of results, the
developer has different kinds of results to select as a query (called
a like-this query) to find code that is more, somewhat, or less simi-
lar to the selected result. Code returned from the like-this query
can continually be used to issue another like-this query to support
iterative search.
A screen-shot of CLT is presented in Figure 1 illustrating how
to use a quick sort example to find many different kinds of quick
sort implementations and other ways of sorting. On the bottom
776ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Lee Martie, André van der Hoek, Thomas Kwak
Figure 1: CodeLikeThis (CLT).
left in Figure 1 is the main page showing the top 10 results after a
developer issued the keyword query quick sort. At the bottom of
each code result are buttons to issue a query to find other code that
is less, somewhat, or more similar to that result. Shown above and
to the right of the main page are the top two results after clicking
on each of “like this” buttons for the highlighted implementation
of quick sort on the bottom left. When the programmer selects the
“More Like This” button, she gets results (A) that are also quick sort
implementations, but use different styles and methods to implement
quick sort (i.e., similar quick sort implementations but not exact
clones). When the programmer clicks “Somewhat Like This” on
the quick sort implementation, she gets results (B) that rely more
on other classes (e.g., extending parent classes to implement quick
sort) or include comments in other human languages. Lastly, when
the programmer clicks “Less Like This”, she gets results (C) that are
no longer quick sort implementations, but are examples of other
kinds of sorting algorithms (in this case merge sort and heap sort).
3.1.1 Like-This Ranking Algorithm. To process a like-this query
Qon selected result R, CLT performs the following algorithm:
(1)Find and order the top Ncode snippets by their similarity
toR, where similarity is calculated with the SimST
2function
[48].
(2) If Qis a more-like-this query, return the top 10 of N.
(3)IfQis a somewhat-like-this query, return the 10 code snip-
pets that are an average distance away from R.
(4) If Qis a less-like-this query, return 10 at the tail of N.In our implementation, we set N=300in an attempt to limit
completely off topic results from a less-like-this query and to speed
up our processing of queries.
3.2 CodeExchange (CE)
Sometimes particular characteristics of a code result appear helpful
and sometimes not. To support using characteristics of the results
to find other code with or without those characteristics, CE [ 47]
provides four features. Each of these features are presented in Figure
2 (a partial screen-shot) in the context of CE after the user has been
searching for an implementation of an HTTP servlet. Each feature
is discussed next.
3.2.1 Language Constructs Feature. Language constructs (A)
support the developer in searching by structural characteristics
she likes about a code result. In particular, language constructs
highlight the structural properties of the code results so that they
can be clicked on to refine the query by that structural property. For
example, the user can click on a method call (e.g., setContentType)
or an import (e.g., javax.servlet) in a result to refine the query to
find all code that also has that method call or import.
3.2.2 Critiques Feature. Critiques (B) support the developer to
search by what she does not like about a code result. In particular,
the user can issue a query that the results need to have more/less
size, complexity, or number of imports than the result she does
not like. This is done by clicking an up/down arrow above the size,
complexity, or number of imports appearing above the result, and
when clicked will refine the query to find results with more/less
777Understanding the Impact of Support for Iteration on Code Search ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
Figure 2: CodeExchange (CE).
size, complexity, or imports. For example, if a code result is simply
too long for a user to want to read through, she can click the down
arrow below the size value for that result and the next results will
be shorter in size compared to that result. Clicking the down arrow
below the size value for the snippet in Figure 2 would refine the
query to find code less than 3155 characters.
3.2.3 Refinement Recommendations Feature. Refinement Rec-
ommendations (C) use the results after each query to present ways
for refining the current query by domain related keywords or com-
mon imports, parent classes, and interfaces implemented. These
recommendations alleviate the work for the developer trying to
see what is commonly used or related in the entire set of results
returned (often in the thousands). For example, if the developer
issues the keyword query HTTP servlet , she gets keyword refine-
ment recommendations request andresponse (both having a domain
specific meaning in HTTP servers) and parent class recommenda-
tion HttpServlet . When the recommendation for the parent class
HttpServlet is taken, the query will be refined for code that extends
the class HttpServlet and the recommendations are updated using
the latest results. In this way, the developer can iteratively take
recommendations to refine her query to steer the search engine
toward desired results.
3.2.4 Query Parts Feature. Query parts (D) decompose the query
by the result characteristics and keywords used to refine the query
and can be toggled on/off. When a part is toggled off, it acts to
generalize the query by deactivating the refinement and when
toggled on reactivates it to refine the query. When a query part is
on, it appears appear yellow, and when it is off, it appears white.
In Figure 2, the user has generalized the query by toggling the
keyword query part off. In this way, the programmer can quickly
modify her query by previous result characteristics to try different
combinations in response to the current results.
CE’s features are orthogonal to any ranking algorithm for key-
words. In this study, it uses the Specificity ranking algorithm [ 43]
because Specificity is a successful code ranking algorithm compared
to more basic ranking algorithms (e.g., TF-IDF).3.3 Baseline
The baseline search engine (shown in Figure 3) was constructed to
resemble a traditional search engine, while controlling for confound-
ing factors. Our method to do so was to remove all the iterative
features of CE, leaving basic search features, but preserving the
same ranking algorithm of CE and the same code index used by
CE and CLT. The basic features included a keyword text box with
autocomplete, a list of code results, and a paging mechanism. We
called our baseline search engine “SearchIt” when introducing it to
participants in order to hide the fact we were using it as a baseline.
4 EXPERIMENT DESIGN
To answer our research question, what is the impact of explicitly
supporting developers in searching iteratively, we conducted a user
study measuring the experience, time, and success of each partic-
ipant in searching for code with each approach [ 23,40]. Further,
we logged how each approach was used as an indication of what
features helped, and we collected reasons why code was chosen to
give insight into why code is used on the Internet when the search
for code is initially uncertain.
The participants in the study consisted of 24 developers who
reported to have approximately 4 years of professional development
experience on average ( s=2.67), above intermediate Java skill level
(median of 5 skill level on an ordinal scale of 1 as beginner and 7 as
expert and with s=1.03), an average age of 26.2 ( s=3.61), and a
20/4 male-female ratio. We recruited the participants by sending out
advertisements to industry affiliated mailing lists targeted towards
developers [4].
The user studies were held in a closed lab setting where the
participants sat alone in a room completing eight different and
independent search tasks, in sequence. Each participant was as-
signed two search engines for the entire experiment and completed
each search task using only one. We chose to set the number of
search engines to two per person to reduce the learning curve effect
with using all four search engines, but still allowing them to make
comparisons between the search engines in their feedback.
We designed the experiment as follows:
778ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Lee Martie, André van der Hoek, Thomas Kwak
•We used the Latin Square [ 73] design for our experiment
to evenly distribute the tasks among the search engines
and participants. As such, each search engine was used in
48 tasks in total and used for each task six times, which
yielded a total of 192 data points.
•To address ordering effects, the search engines alternated
with each task and all the tasks came in a random order for
each participant. This was accomplished by assigning each
search engine to a task (done with Latin Square) creating
pairs like ( S1,T1), (S2,T2), (S1,T3),· · ·, (S2,T8), and when
a participant finished a task, then a pair with the other
search engine was chosen at random and used as the next
task and search engine to use.
•Each participant received a different task for each search
engine, so each participant never repeated a task on two
different treatments — making the experiment a between
subject design to reduce carry over effects. In addition, each
participant used different search engines, making it also a
within subject design. As such, we had a mixed design.
•To control for differences in code indexed by the search
engine, CE, CLT, and the baseline search engine all index
the same 10 million Java classes mined from github.com.
With respect to Google, in Section 6 we discuss implications
about its index.
•Lastly, to push participants to give each task some thought
and effort, we asked them to include explanations for what
the code does and why they chose it. This was a tactic to
get the participants to genuinely attempt each task.
The search tasks were designed to cover a space of tasks that are
broad to more focused. The broad tasks were designed to model
situations when the programmer is looking for multiple and possi-
bly different examples that can help. The more focused tasks were
designed to model situations when the programmer is looking for
one example or examples of a particular kind. This space of tasks
is presented in a 2x2 matrix shown in Table 1. The broader tasks
are found in the “Find 4” row and the “No Specific Role for Code”
column. The more focused tasks are found in the “Find 1” row and
the “Algorithm/Data Structure” column. The tasks in the upper
right and lower left are a mix of the broad and focused tasks. The
participants had 20 minutes for the “Find 4” tasks and 10 minutes
for “Find 1” tasks. We set the time limits for completing tasks based
Figure 3: Baseline (BL).on our pilot studies, where we found our participants could finish
tasks in the given time limits.
The topics of the tasks were created to mirror real-world topics
for code search. The topics were derived by reverse engineering
plausible topics from real queries ( tic tac toe - T1, mail sender - T2,
AWT events - T3, combinations n per k - T4, array multiplication -
T5,database connection manager - T6, JSpinner - T7, and binary
search tree - T8) found across four different code search engine logs
[48]. With the topics, we created the tasks in a style similar to those
used in other code search studies [ 31,45], where the tasks do not
give the participant a query or the actual code snippet to find (as
is not usually done in the real-world) and are composed only of
one or two sentences expressing a high-level problem for which
the participant needs to find code to help solve.
To start the experiment, the participants watched a tutorial video
on each of their assigned search engines that explained all features
(including the advanced search feature for Google). Further, to warm
up, each participant had a few minutes to play around with the
search engines as they pleased. Once done, each participant used
a survey system, which presented the time she had for a question,
a search engine hyper-link indicating which search engine to use
(when clicked opened the search engine), the search task, five tabs
each containing an editor to paste the code she finds for the search
task, a text box to explain what the code does, and a text box to
explain why she chose the code. We gave the participants five tabs in
case they wanted to find more than the required number of snippets.
When a participant hit the done button to indicate finishing or when
time ran out, the participant then rated her experience, from 1 to
7, for using the assigned search engine for the given task, where
1 was labeled “bad”, 4 was labeled “neutral”, and 7 labeled “great”.
Once the participant was done giving her experience rating, she hit
submit and received the next task and treatment. Finally, once the
participant finished all eight tasks, she filled out a questionnaire
about the treatments used and then had an open interview with us.
From the entire experiment, we collected a total of 192 experience
scores, 192 time durations to find one snippet (also counting time to
find first snippet in the “Find 4” tasks), 96 time durations to find four
snippets, and 24 interviews. Further, we logged the search behavior
across all the search engines, giving us data for what features were
used and how often. We also collected 463/480 reasons why code
was chosen (each participant was asked to find 20 snippets in total
and give a reason why she chose each snippet).
5 RESULTS
In this section, we examine the impact of supporting iterative search
on the experience, time, and success of searching for code. Further,
we look at how the search engines were used and the reasons people
reported for searching for code. For significance tests, we set alpha
to 0.1 as done in smaller-scale experiments [7].
5.1 Experience, Time, and Success
Our first analysis compared the experience scores for using each
search engine for each search task. To compare experience scores,
we observed when one approach had a higher median score than
others. It is standard practice to use the median and not mean when
the data is ordinal, such as the experience scores. We created the
779Understanding the Impact of Support for Iteration on Code Search ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
Table 1: Task Matrix
No Specific Role for Code — Broader Algorithm/Data Structure — More FocusedFind 4 -
Broader
(20 min limit)T3Scenario: You are building a sketching application.
Task: Find 4 snippets of Java source code that you
think will help.T1Scenario: You are making your favorite video game.
Task: Find 4 snippets of Java source code that you
think will help implement the algorithms and data
structures.
T7Scenario: You are building a program to survey
people’s preferences.
Task: Find 4 snippets of Java source code that you
think will help.T5Scenario: You are building a program to help teach
students basic algebra.
Task: Find 4 snippets of Java source code that you
think will help implement the algorithms and data
structures.Find 1 -
More Focused
(10 min limit)T2Scenario: You are building a new email client.
Task: Find Java source code that counts new messages.T4Scenario: You are teaching a class and need to split
your class up into groups of four.
Task: Find Java source code that implements an
algorithm and data structures to find all possible
groups of students.
T6Scenario: You are building a large healthcare patient
record keeping system.
Task: Find Java source code to add or edit records
in the system.T8Scenario: You are building the world’s first online
phone book.
Task: Find Java source code that implements an
algorithm and data structures to retrieve phone
numbers by name.
box plot summary, shown in Figure 4, of the experience scores
for each search engine by task. Each box is color coded by search
engine, shows the median score with a black horizontal bar through
it (sometimes on top or bottom), and has a height summarizing
the spread of the scores. For each task, the search engine with the
highest median scores has its plot annotated with its initials. If there
are ties between search engines, then both names appear above
their corresponding plots.
We found that an iterative approach existed with a higher me-
dian than the baseline for six tasks and equal median for two tasks
(p<0.1, where p=0.002with χ2on a 2×3contingency table com-
paring best medians). These results suggest supporting iterationwith the features of CE and CLT can significantly (using Bonfer-
roni correction to set alpha to 0.05 for comparing Iterative twice)
improve the developers’ experience in searching for code.
We found that an iterative approach existed with a higher median
than Google for three tasks, equal median for two tasks, and lower
median for three tasks ( p=1with χ2on a 2×3contingency table
comparing best medians). These results suggest that by supporting
the technique of iteration in a code search engine can provide
an experience comparable to a large-scale web search engine like
Google. However, it is also clear that the participants sometimes
had a better experience with Google. Whether this is a consequence
of the different indexes, kinds of content, participant familiarity, or
ranking algorithms is uncertain.
Figure 4: Box Plot Comparing Experience Scores of Search Engines Across all Eight Tasks.
780ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Lee Martie, André van der Hoek, Thomas Kwak
Table 2: CE and CLT Median Comparison.
No Specific Role Algorithm/Data Structure
Find 4 CE(T3) CE(T7) CE(T1) CLT(T5)
Find 1 CE(T6) CLT(T2) CLT(T4) CLT(T8)
Interestingly, we found that CE and CLT complemented each
other in the types of tasks they support. In Table 2, each cell displays
which iterative approach had a higher experience median for each
task. The table shows that CE provided a better experience for tasks
that were broader (“Find 4” and “No Specific Role”) and that CLT
provided a better experience for tasks that were more focused (“Find
1” and “Algorithm/Data Structure”). Such a complementary pattern,
as shown in Table 2, only has a 16/6561 (0.2%) chance of occurring
(given equal probability of CE, CLT, or ties occurring in each cell).
The data in Table 2 suggests that using characteristics, supported
by CE, of the results supports finding code when initially the search
task is broad, but as the search task becomes more focused, then
using the entire result to search with, supported by CLT, provides
a better search experience.
Time differences to find code were examined by measuring how
long it took to paste code for each of the tasks by search engine (time
writing why/what explanations was not counted). If the task was
not completed, the max time allotted was used. We used ANOVA
to find significant differences among the groups of search engines
for each task. If we did find a significant difference, we conducted
a post hoc pair-wise analysis on the corresponding group using
Tukey’s honest significant difference [ 76] to see what might be
causing the difference.
The time analysis is presented in Table 3. In most cases, we did
not find significant differences. However, we did find that Google
was significantly faster than CLT for finding the first code snippet
on task three and CLT was significantly faster than Google for
finding the first code snippet on task seven. We were not able to
find any significant differences in time for finding four snippets.
Finally, we looked at the number of task incompletions (no code
found) by search engine as a measure of unsuccessful searches. We
found that the iterative search approaches had less incomplete tasks
(11) than the others (13) with CLT having the least number of in-
complete tasks (4), baseline having 6, and CE and Google each with
7. However, we did not find these differences statistically signifi-
cant with χ2. Further, we found that the “Find 4” tasks had higher
incompletion rates (19/96) than the “Find 1” tasks (5/96). Further,
the “Algorithms/Data Structure” tasks had a higher incompletion
rate (16/96) than the “No Specific Role” tasks (8/96).
5.2 Feature Usage
We looked at how the features of the search engines were used and
some participant explanations of the usefulness of the features. In
Table 3: Mean Seconds Until First Paste.
T1 T2 T3 T4 T5 T6 T7 T8
CE 219 294 167 392 488 263 301 372
CLT 185 302 359 357 373 285103
(CLT>G
p=0.083)189
BL 163 178 174 446 154 278 158 301
G 176 24485
(G>CLT
p=0.03)393 410 182 349 169
ANOVA p=.8p=.5 p=.04 p=.8p=.2p=.5 p=.06 p=.2Table 4: CodeExchange Search Behavior.
T1 T2 T3 T4 T5 T6 T7 T8 T
RFreq. 9 2 10 2 4 7 9 3 46
CFreq. 3 0 1 1 4 1 4 2 16
LCFreq. 6 5 5 0 14 8 17 6 61
QPFreq. 13 18 25 14 41 19 36 10 176
KFreq. 26 22 31 42 54 22 32 13 242
ASFreq. 1 1 23 0 7 1 2 1 36
HFreq. 3 6 0 0 0 0 2 2 13
Copies after
only Keywords10
(.38)5
(.22)16
(.51)5
(.11)17
(.31)6
(.27)8
(.25)3
(.23)70
(.28)
Copies after
R/C/LC14
(.77)2
(.28)15
(.93)1
(.33)8
(.36)5
(.31)15
(.5)3
(.27)63
(.51)
Copies after
QP3
(.23)1
(.05)6
(.24)4
(.28)7
(.17)5
(.26)11
(.3)5
(.5)42
(.23)
Copies after
AS0
(0)0
(0)4
(.17)0
(0)0
(0)0
(0)0
(0)0
(0)4
(.1)
this section, we report what we found for CE, CLT, and Google.
Since the baseline is limited to keywords and paging we leave it
out of this section.
5.2.1 CodeExchange (CE). To understand which features of CE
helped, we looked at how often features were used, when feature
usages were used to find code that was copied, and what the partic-
ipants had to say during our interview. The logs for CE recorded
when the participants used iterative features ( Recommendations,
Critiques, Language Constructs, Query Parts), Keywords, History,
andAdvanced Search. Table 4 presents the usage frequency counts
of features by task. The features that support iteration were used
most often (50.6%) and then keyword text box (41.0%) second, ad-
vanced search (6.0%) third, and history (2.2%) fourth. Further, we
found the iterative features were used significantly more often than
keywords ( p<0.1, where p=0.01with χ2on a 2×1contingency
table).
To get a better idea of the iterative features’ impact on searching
we looked at how many copies happened after only keywords
versus after queries that include a recommendation, critique, or
a language construct. We separated counting query parts from
the other iterative features in this part of the analysis because
counting copies after query part usages also counts copies after a
query composed only of keywords, which we are trying to separate
out in this analysis. Further, we look at the copy/keyword and
copy/iterative-feature ratios (these ratios measure how many copies
happen per keyword query and how many copies happen per query
created with an iterative feature). The ratios appear in parenthesis
after the copy counts in the table. We found that when the iterative
features were used to refine a query, they lead to higher number
of copies on average (0.51) than when only keywords were used
(0.28).
Our participants told us that, at a high level, CodeExchange was
better for drilling down. . . CodeExchange helped me go in a particular
direction, where CodeLikeThis did not tell me . They found language
constructs useful, saying I liked clicking the import. . . If I found a
project that seemed to do what I needed. . . I could just click. . . and my
search is in the project . They felt query parts helped, saying query
parts helped explore . They reported that refinement recommenda-
tions gave them ideas and helped them remember, saying initially I
typed in SMTP and got suggestion for a mail validator, then I added
import and it guided me to think of ideas and I typed just “draw” in
here, and it recommended AWT. . . I was like “Oh yeah, it was AWT” .
781Understanding the Impact of Support for Iteration on Code Search ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
Table 5: CodeLikeThis Search Behavior.
T1 T2 T3 T4 T5 T6 T7 T8 T
Back Freq. 25 6 12 5 11 1 21 12 93
Forward Freq. 0 0 2 0 0 0 0 0 2
More Freq. 14 5 15 4 14 4 10 8 74
Somewhat Freq. 5 1 3 3 3 1 12 2 30
Less Freq. 1 0 2 1 2 0 3 0 9
Keywords Freq. 33 20 29 22 40 10 36 18 208
Like-This Freq. 20 6 21 8 19 5 25 10 114
Copies after
Keywords17
(.51)11
(.55)15
(.5)3
(.13)13
(.32)6
(.6)15
(.41)8
(.44)88
(.42)
Copies after a
Like-This5
(.25)2
(.33)12
(.6)3
(.37)12
(.63)3
(.6)9
(.36)2
(.2)48
(.42)
5.2.2 CodeLikeThis (CLT). Feature usage of CLT data was ex-
amined to look at how often features were used and when using
a feature was used to find code that was copied. The logs for CLT
recorded when any of the like-this queries were used, keywords
issued, back/forward buttons pressed, and copies that came after
keywords or after a like-this query. Table 5 shows the usage fre-
quency counts of features by task.
We found keywords were used twice as much as the like-this
queries ( p<0.1, where p=1×10−7with χ2on a 2×1contingency
table), which could suggest that like-this queries were less helpful.
However, we looked at how many copies happened after keywords
versus after like-this queries and the copy/keyword and copy/like-
this ratios. The ratios appear in parenthesis after the copy counts.
We found that keywords and like-this queries lead to an equal num-
ber of copies on average when they were used (.42). This suggests
that keywords and like-this queries both had an equally important
impact in searching for code. Some example event sequences in the
logs recorded using CLT were: keyword query ,back button ,keyword
query ,somewhat-like-this query ,more-like-this query ,code copied
andkeyword query ,more-like-this query ,code copied .
At a high level, participants explained like-this queries helped
them get a new perspective. They said with CLT you get to see dif-
ferent codes and different ideas. . . andit gave me a new perspective. . .
andit was sort of a way of exploring . They explained the like-this
queries were helpful because I wasn’t sure exactly what I was looking
for. . . and that it works well for queries that are common and precise:
quicksort, hash table, game loop or game examples for instance .
However, it is clear that like-this queries do not always work,
one participant explained more like this is pretty much what you
expect, but the other two doesn’t really follow the semantics in my
mind and another saying I felt a little lost .
5.2.3 Google. For Google, we recorded normal keyword queries,
advanced keyword queries (e.g., restricting search to a site with
the “site” qualification), and domains visited by clicking hyperlinks.
We found that keyword queries were used (84.7%) about five times
as much advanced queries (15.2%) with p<0.1(p=1.4×10−28)
with χ2on a 2×1contingency table. This suggests either keywords
sufficed or advanced search was less helpful or harder to use for
the participants.
We looked at the number of domains that were visited by clicking
a hyperlink and how often those domains were visited across partic-
ipants (we do not double count a visit when one participant clicks
the same link multiple times). We found that 126 different domains
were visited a total of 529 times, with the frequency following along tail distribution. github.com (130/529 visits) and stackover-
flow.com (105/529 visits) were visited most often, but thereafter
a sharp drop in number of visits to other domains appears. How-
ever, while individually the other 124 domains are visited much
less frequently, collectively they are visited more (294/529) than
github.com or stackoverflow.com. This suggests that the size and
variety of Google’s index was helpful for participants in their search.
Participants told us they appreciated Google for the context and
comments they would find on web pages. Some said sometimes
I was like, oh man I wish I could use Google at this point, just to
get some context so I can understand what I need to search in Code-
Exchange. . . and I use Google to find best practices and Google had
comments . However, there are times when Google did not help.
Our participants told us sometimes they asked Why am I getting
this? andGoogle is lacking in digging down. . . andfor simple straight
forward [questions] I felt CodeLikeThis was better .
Lastly, we looked at the query behavior of search with Google
and present the results in Table 6. Each row presents on average
how many queries were issued per user, the average number of
terms in the keyword queries, and the average number of terms
deleted and added when modifying a query. The results support
the hypothesis that code search on the Internet, even with a large-
scale web search engine, is iterative and can have a vast range of
queries (1.67 to 11.3). These results suggest Google could benefit
from features that explicitly support iterative code search.
5.3 Why Code Was Searched For
Using the explanations given by our participants that stated why
they chose code (one explanation for each snippet they found), we
present categories of reasons for selecting code being searched
for. To create these categories, three graduate students (two of
the authors and another software engineering graduate student)
participated in an affinity diagramming session to cluster the “why”
explanations. Each graduate student took a why explanation and
either assigned it a category (if one had already been created by the
group) or created a new category to assign it to. The group was free
to discuss names of categories and modify them if needed. From
the 463 “why” explanations 28 categories emerged. The majority of
reasons fell into five clusters; we show the top 17 clusters in Figure
5 (the other 11 clusters had only 1 to 3 explanations in them). Often,
programmers searched for code because it
•helped implement a feature the participant had in mind
(e.g., A user would want to save their sketch, so we need
. . . save and load . . . )
•to support a design decision the programmer made about
what the code should do (e.g., I want to support 3D too. )
•meets the problem specifications (e.g., . . . performs the job
really well and hence I choose it )
Table 6: Google Search Behavior.
T1 T2 T3 T4 T5 T6 T7 T8
Queries per User 6.33 3 5.83 5.67 11.3 3 5.83 1.67
Terms per Query 5.92 5.52 4.23 5.24 4.07 5.93 4.8 4.5
Terms Deleted per
Query2.43 1 1.47 1.61 1.42 1.57 2.03 0.67
Terms Added per
Query2.57 1.05 1.53 1.88 1.45 1.79 1.94 1
782ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Lee Martie, André van der Hoek, Thomas Kwak
Figure 5: Why Programmers Selected Code.
•serves as a starting point to solve the programming prob-
lem (e.g., . . . an ideal starting point for building connect
four. . . contains the correct sequence of gameplay. . . )
•serves as an example or a reference for how to solve the
programming problem (e.g., . . . demonstrates how calcula-
tors work. . . )
These results suggest that when the code being searched for is
not completely specified (as in our tasks), that the programmer will
make decisions on what to search for as they search. Often, she
makes decisions related to design (features she thinks are needed
or other design decisions). Making design decisions is often argued
as an iterative process [ 9,55], suggesting that making decisions
might play a role in making search iterative. Further, it is clear that
often programmers found code simply because they felt it satisfied
requirements or would serve as a useful starting place to write code
that would satisfy requirements for the problem described in the
search task.
6 THREATS TO VALIDITY
Several possible threats to validity exist with our study. First, while
we put in our best effort to make this lab study realistic, it still lacks
the realism one would find in a field study with each of the search
engines. As such, further studies need to be performed to examine
whether our results hold in real-world environments.
Second, as all of our participants have used Google for years,
and Google indexes more code and different information than our
prototypes, our experiment inherently is unbalanced. However, we
felt it was important to tolerate this imbalance, since Google is so
ubiquitous and represents a ‘gold standard’ for how developers
search. It is not surprising to us that Google performed better on a
number of cases, even though CE and CLT offer the same ability
to search with keywords only. What is more important, however,
is that CE and CLT did outperform Google on a sufficient number
of cases to show the promise of dedicated support for iteration in
search.
Third, the search tasks we used by no means cover all possible
types of search tasks. We intentionally focused on broader search
tasks, given the goal of this paper of addressing code search when
developers do not know exactly what they want and search aroundmore exploratorily. However, even within this narrower focus, we
could have chosen to use other search tasks. While we attempted
to ameliorate this issue by modeling our tasks on real searches in
real code search engines, a longitudinal study with CE and CLT is
needed to examine if their features apply beyond the eight search
tasks we used.
Fourth, it is possible that participants did not seriously attempt
the tasks. Two of the authors and a colleague graduate student not
involved with the research each individually inspected all snippets
and explanations, assessing if they represented genuine attempts.
In 97.3% of the cases, unanimous agreement was that they were
genuine attempts (1 result was ranked not genuine by all 3 people,
6 results by 2, and 6 results by 1; these were spread across search
engines and participants). This gives us confidence that most of our
results represent genuine attempts by our participants.
7 CONCLUSION
In this paper, we investigated code search on the Internet from
the point of view that it often is an iterative process, because the
developer does not always know in advance what she may need.
Our results suggest that providing features explicitly designed to
support iterative code search can improve the search experience.
Compared to the baseline, we found that both CE and CLT lead to
higher median experience scores. Compared to Google, we found
that that CE and CLT each can, on a number of occasions, lead to
an experience that is comparable to that of using Google, even as
participants were much less versed in CE and CLT than Google —
which they use daily.
Our results are also nuanced, as CE and CLT provide better ex-
periences for opposing kinds of search tasks. CE provides a better
experience for broader searches looking for multiple and possibly
different examples that can help the programmer. CLT tended to
better support tasks that were more focused, looking for examples
of a particular kind. This in many ways is surprising, as CLT was
designed to support diversity in results and CE to focus more on re-
finement of results. Further research will be necessary to determine
why the difference emerged. Regardless, it is clear that both CE and
CLT offer features that assist at important times when keyword
searches seem to not lead to the desired, or at least less desirable,
results.
Overall, we believe that our results point at the need to explore
the design of novel search engines combining traditional keyword
search with the distinctive features of both CE and CLT. This is
not a trivial undertaking, yet, if successful, would offer developers
multiple ways of ‘jumping out’ of a current search path without
having to think of how to formulate a new keyword query. That is
an important ability given the premise of this paper: developers do
not always know exactly what they are looking for and need help
formulating what they search for as they search. Our experiment
has identified two particular ways of doing so, but we expect other
approaches to need to be explored as well.
ACKNOWLEDGMENTS
This work was sponsored by NSF grant CCF-1321112. Thanks to
Tariq Ibrahim for his help in this project.
783Understanding the Impact of Support for Iteration on Code Search ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany
REFERENCES
[1] [n. d.]. Eclipse Code Recommenders. ([n. d.]). Retrieved December 16, 2012 from
http://www.eclipse.org/recommenders/
[2] [n. d.]. GitHub ·Build software better, together. ([n. d.]). Retrieved December
16, 2012 from https://github.com/
[3] [n. d.]. Home | krugle - software development productivity. ([n. d.]). Retrieved
December 16, 2012 from http://www.krugle.com/
[4] [n. d.]. Institute for Software Research. ([n. d.]). Retrieved August 27, 2016 from
http://isr.uci.edu/
[5][n. d.]. Ohloh Code Search. ([n. d.]). Retrieved December 16, 2012 from http:
//code.ohloh.net/
[6][n. d.]. Search engine market share. ([n. d.]). Retrieved August 27,
2016 from https://www.netmarketshare.com/search-engine-market-share.aspx?
qprid=4&qpcustomd=0
[7]2008. Alpha, Significance Level of Test. In Encyclopedia of Survey Re-
search Methods . Sage Publications, Inc., 2455 Teller Road, Thousand Oaks,
California 91320, United States of America. http://methods.sagepub.
com/reference/encyclopedia-of-survey-research-methods/n13.xml DOI:
10.4135/9781412963947.n13.
[8] A. Bacchelli, L. Ponzanelli, and M. Lanza. 2012. Harnessing Stack Overflow for
the IDE. In 2012 Third International Workshop on Recommendation Systems for
Software Engineering (RSSE) . 26 –30. https://doi.org/10.1109/RSSE.2012.6233404
[9]Gregg "Skip" Bailey. 1993. Iterative Methodology and Designer Training in
Human-computer Interface Design. In Proceedings of the INTERACT ’93 and CHI
’93 Conference on Human Factors in Computing Systems (CHI ’93) . ACM, New
York, NY, USA, 198–205. https://doi.org/10.1145/169059.169163
[10] Sushil Bajracharya, Trung Ngo, Erik Linstead, Paul Rigor, Yimeng Dou, Pierre
Baldi, and Cristina Lopes. 2006. Sourcerer: A Search Engine for Open Source
Code Supporting Structure-Based Search. In In Proc. Int’l Conf. Object-Oriented
Programming, Systems, Languages, and Applications (OOPSLA)’06 . 25–26.
[11] Sushil Bajracharya, Joel Ossher, and Cristina Lopes. 2010. Searching API Usage
Examples in Code Repositories with Sourcerer API Search. In Proceedings of 2010
ICSE Workshop on Search-driven Development: Users, Infrastructure, Tools and
Evaluation (SUITE ’10) . ACM, New York, NY, USA, 5–8. https://doi.org/10.1145/
1809175.1809177
[12] Sushil Krishna Bajracharya and Cristina Videira Lopes. 2012. Analyzing and
Mining a Code Search Engine Usage Log. Empirical Software Engineering 17, 4-5
(Aug. 2012), 424–466. https://doi.org/10.1007/s10664-010-9144-6
[13] O. Barzilay, O. Hazzan, and A. Yehudai. 2009. Characterizing Example Embedding
as a software activity. In ICSE Workshop on Search-Driven Development-Users,
Infrastructure, Tools and Evaluation, 2009. SUITE ’09 . 5 –8. https://doi.org/10.1109/
SUITE.2009.5070011
[14] Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R. Klemmer. 2010.
Example-Centric Programming: Integrating Web Search into the Development
Environment. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems (CHI ’10) . ACM, New York, NY, USA, 513–522. https://doi.
org/10.1145/1753326.1753402
[15] Joel Brandt, Philip J. Guo, Joel Lewenstein, Mira Dontcheva, and Scott R. Klemmer.
2009. Two Studies of Opportunistic Programming: Interleaving Web Foraging,
Learning, and Writing Code. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems (CHI ’09) . ACM, New York, NY, USA, 1589–1598.
https://doi.org/10.1145/1518701.1518944
[16] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from Exam-
ples to Improve Code Completion Systems. In Proceedings of the the 7th joint
meeting of the European software engineering conference and the ACM SIGSOFT
symposium on The foundations of software engineering (ESEC/FSE ’09) . ACM, New
York, NY, USA, 213–222. https://doi.org/10.1145/1595696.1595728
[17] Marcel Bruch, Thorsten Schäfer, and Mira Mezini. 2006. FrUiT: IDE Support
for Framework Understanding. In Proceedings of the 2006 OOPSLA workshop
on eclipse technology eXchange (eclipse ’06) . ACM, New York, NY, USA, 55–59.
https://doi.org/10.1145/1188835.1188847
[18] Shaunak Chatterjee, Sudeep Juvekar, and Koushik Sen. 2009. SNIFF: A Search
Engine for Java Using Free-Form Queries. In Fundamental Approaches to Software
Engineering , Marsha Chechik and Martin Wirsing (Eds.). Number 5503 in Lecture
Notes in Computer Science. Springer Berlin Heidelberg, 385–400. http://link.
springer.com/chapter/10.1007/978-3-642-00593-0_26 DOI: 10.1007/978-3-642-
00593-0_26.
[19] Janet E. Davidson and Robert J. Sternberg PhD (Eds.). 2003. The Psychology of
Problem Solving . Cambridge University Press, Cambridge, UK ; New York.
[20] Frederico A. Durão, Taciana A. Vanderlei, Eduardo S. Almeida, and Silvio R. de
L. Meira. 2008. Applying a Semantic Layer in a Source Code Search Tool. In
Proceedings of the 2008 ACM Symposium on Applied Computing (SAC ’08) . ACM,
New York, NY, USA, 1151–1157. https://doi.org/10.1145/1363686.1363952
[21] Françoise Détienne. 2002. Software Design — Cognitive Aspects . Springer-Verlag
New York, Inc., New York, NY, USA.
[22] Ethan Fast, Daniel Steffee, Lucy Wang, Joel R. Brandt, and Michael S. Bernstein.
2014. Emergent, Crowd-scale Programming Practice in the IDE. In Proceedingsof the 32nd Annual ACM Conference on Human Factors in Computing Systems
(CHI ’14) . ACM, New York, NY, USA, 2491–2500. https://doi.org/10.1145/2556288.
2556998
[23] Henry A. Feild, James Allan, and Rosie Jones. 2010. Predicting Searcher Frustra-
tion. In Proceedings of the 33rd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’10) . ACM, New York, NY, USA,
34–41. https://doi.org/10.1145/1835449.1835458
[24] G. Fischer, S. Henninger, and D. Redmiles. 1991. Cognitive Tools for Locating
and Comprehending Software Objects for Reuse. In [1991 Proceedings] 13th
International Conference on Software Engineering . 318–328. https://doi.org/10.
1109/ICSE.1991.130658
[25] Rosalva E. Gallardo-Valencia and Susan Elliott Sim. 2011. What Kinds of De-
velopment Problems can be Solved by Searching the Web?: A Field Study. In
Proceedings of the 3rd International Workshop on Search-Driven Development:
Users, Infrastructure, Tools, and Evaluation (SUITE ’11) . ACM, New York, NY, USA,
41–44. https://doi.org/10.1145/1985429.1985440
[26] X. Ge, D. Shepherd, K. Damevski, and E. Murphy-Hill. 2014. How Developers Use
Multi-Recommendation System in Local Code Search. In 2014 IEEE Symposium
on Visual Languages and Human-Centric Computing (VL/HCC) . 69–76. https:
//doi.org/10.1109/VLHCC.2014.6883025
[27] Max Goldman and Robert C. Miller. 2009. Codetrail: Connecting Source Code
and Web Resources. Journal of Visual Languages & Computing 20, 4 (Aug. 2009),
223–235. https://doi.org/10.1016/j.jvlc.2009.04.003
[28] Mark Grechanik, Chen Fu, Qing Xie, Collin McMillan, Denys Poshyvanyk, and
Chad Cumby. 2010. A Search Engine for Finding Highly Relevant Applica-
tions. In Proceedings of the 32nd ACM/IEEE International Conference on Soft-
ware Engineering - Volume 1 (ICSE ’10) . ACM, New York, NY, USA, 475–484.
https://doi.org/10.1145/1806799.1806868
[29] Sonia Haiduc, Gabriele Bavota, Andrian Marcus, Rocco Oliveto, Andrea De Lucia,
and Tim Menzies. 2013. Automatic Query Reformulations for Text Retrieval
in Software Engineering. In Proceedings of the 2013 International Conference
on Software Engineering (ICSE ’13) . IEEE Press, Piscataway, NJ, USA, 842–851.
http://dl.acm.org/citation.cfm?id=2486788.2486898
[30] S. Henninger. 1994. Using Iterative Refinement to Find Reusable Software. IEEE
Software 11, 5 (Sept. 1994), 48–59. https://doi.org/10.1109/52.311059
[31] Scott Robert Henninger. 1993. Locating Relevant Examples for Example-Based
Software Design . Ph.D. University of Colorado at Boulder, United States – Col-
orado. http://search.proquest.com/dissertations/docview/304037932/abstract/
26191C944149424FPQ/1
[32] J.-M. Hoc, T.R.G. Green, R. Samurçay, and D.J. Gilmore (Eds.). 1990. Psychology
of Programming . Academic Press, London.
[33] Raphael Hoffmann, James Fogarty, and Daniel S. Weld. 2007. Assieme: Finding
and Leveraging Implicit References in a Web Search Interface for Programmers.
InProceedings of the 20th annual ACM symposium on User interface software and
technology (UIST ’07) . ACM, New York, NY, USA, 13–22. https://doi.org/10.1145/
1294211.1294216
[34] R. Holmes. 2009. Do Developers Search for Source Code Examples Using Multiple
Facts?. In ICSE Workshop on Search-Driven Development-Users, Infrastructure,
Tools and Evaluation, 2009. SUITE ’09 . 13 –16. https://doi.org/10.1109/SUITE.2009.
5070013
[35] R. Holmes and G.C. Murphy. 2005. Using Structural Context to Recommend
Source Code Examples. In 27th International Conference on Software Engineering,
2005. ICSE 2005. Proceedings . 117 – 125. https://doi.org/10.1109/ICSE.2005.1553554
[36] Michael Hucka and Matthew J. Graham. 2016. Software search is not a science,
even among scientists. arXiv:1605.02265 [cs] (May 2016). http://arxiv.org/abs/
1605.02265 arXiv: 1605.02265.
[37] Oliver Hummel, Werner Janjic, and Colin Atkinson. 2008. Code Conjurer: Pulling
Reusable Software out of Thin Air. IEEE Softw. 25, 5 (Sept. 2008), 45–52. https:
//doi.org/10.1109/MS.2008.110
[38] Jeff Johnson. 2014. Designing with the Mind in Mind, Second Edition: Simple
Guide to Understanding User Interface Design Guidelines (2 edition ed.). Morgan
Kaufmann, Amsterdam ; Boston.
[39] Iman Keivanloo, Juergen Rilling, and Ying Zou. 2014. Spotting Working Code
Examples. In Proceedings of the 36th International Conference on Software Engi-
neering (ICSE 2014) . ACM, New York, NY, USA, 664–675. https://doi.org/10.1145/
2568225.2568292
[40] Diane Kelly. 2009. Methods for Evaluating Interactive Information Retrieval
Systems with Users. Foundations and Trends in Information Retrieval 3, 1—2 (Jan.
2009), 1–224. https://doi.org/10.1561/1500000012
[41] O. A. L. Lemos, Sushil Krishna Bajracharya, and Joel Ossher. 2007. CodeGenie:
A Tool for Test-driven Source Code Search. In Companion to the 22nd ACM
SIGPLAN Conference on Object-oriented Programming Systems and Applications
Companion (OOPSLA ’07) . ACM, New York, NY, USA, 917–918. https://doi.org/
10.1145/1297846.1297944
784ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Lee Martie, André van der Hoek, Thomas Kwak
[42] O. A. L. Lemos, Adriano C. de Paula, Felipe C. Zanichelli, and Cristina V.
Lopes. 2014. Thesaurus-based Automatic Query Expansion for Interface-
driven Code Search. In Proceedings of the 11th Working Conference on Min-
ing Software Repositories (MSR 2014) . ACM, New York, NY, USA, 212–221.
https://doi.org/10.1145/2597073.2597087
[43] Erik Linstead, Sushil Bajracharya, Trung Ngo, Paul Rigor, Cristina Lopes, and
Pierre Baldi. 2008. Sourcerer: Mining and Searching Internet-Scale Software
Repositories. Data Mining and Knowledge Discovery 18, 2 (Oct. 2008), 300–336.
https://doi.org/10.1007/s10618-008-0118-x
[44] George F. Luger, Peder Johnson, Carl Stern, Jean E. Newman, and Ronald Yeo.
1994. Cognitive Science: The Science of Intelligent Systems (1 edition ed.). Academic
Press, San Diego.
[45] David Mandelin, Lin Xu, Rastislav Bodík, and Doug Kimelman. 2005. Jungloid
Mining: Helping to Navigate the API Jungle. In Proceedings of the 2005 ACM
SIGPLAN Conference on Programming Language Design and Implementation (PLDI
’05). ACM, New York, NY, USA, 48–61. https://doi.org/10.1145/1065010.1065018
[46] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. In-
troduction to Information Retrieval . Cambridge University Press, New York, NY,
USA.
[47] L. Martie, T. D. LaToza, and A. v d Hoek. 2015. CodeExchange: Supporting
Reformulation of Internet-Scale Code Queries in Context (T). In 2015 30th
IEEE/ACM International Conference on Automated Software Engineering (ASE) .
24–35. https://doi.org/10.1109/ASE.2015.51
[48] L. Martie and A. van der Hoek. 2015. Sameness: An Experiment in Code Search.
In2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (MSR) .
76–87. https://doi.org/10.1109/MSR.2015.15
[49] Collin McMillan, Mark Grechanik, Denys Poshyvanyk, Qing Xie, and Chen Fu.
2011. Portfolio: Finding Relevant Functions and Their Usage. In Proceedings of
the 33rd International Conference on Software Engineering (ICSE ’11) . ACM, New
York, NY, USA, 111–120. https://doi.org/10.1145/1985793.1985809
[50] Amir Michail. 2000. Data Mining Library Reuse Patterns Using Generalized
Association Rules. In Proceedings of the 22nd International Conference on Software
Engineering (ICSE ’00) . ACM, New York, NY, USA, 167–176. https://doi.org/10.
1145/337180.337200
[51] M. Mooty, A. Faulring, J. Stylos, and B. A. Myers. 2010. Calcite: Completing
Code Completion for Constructors Using Crowds. In 2010 IEEE Symposium on
Visual Languages and Human-Centric Computing . 15–22. https://doi.org/10.1109/
VLHCC.2010.12
[52] Laura Moreno, Gabriele Bavota, Massimiliano Di Penta, Rocco Oliveto, and
Andrian Marcus. 2015. How Can I Use This Method?. In Proceedings of the 37th
International Conference on Software Engineering - Volume 1 (ICSE ’15) . IEEE
Press, Piscataway, NJ, USA, 880–890. http://dl.acm.org/citation.cfm?id=2818754.
2818860
[53] Allen Newell. 1972. Human Problem Solving . Prentice Hall, Englewood Cliffs,
N.J.
[54] A.T. Nguyen, T.T. Nguyen, H.A. Nguyen, A. Tamrawi, H.V. Nguyen, J. Al-Kofahi,
and T.N. Nguyen. 2012. Graph-Based Pattern-Oriented, Context-Sensitive Source
Code Completion. In 2012 34th International Conference on Software Engineering
(ICSE) . 69 –79. https://doi.org/10.1109/ICSE.2012.6227205
[55] Jakob Nielsen. 1993. Iterative User-Interface Design. Computer 26, 11 (Nov. 1993),
32–41. https://doi.org/10.1109/2.241424
[56] Jakob Nielsen and Robert L. Mack (Eds.). 1994. Usability Inspection Methods (1
edition ed.). Wiley, New York.
[57] L. Ponzanelli, A. Bacchelli, and M. Lanza. 2013. Seahawk: Stack Overflow in
the IDE. In 2013 35th International Conference on Software Engineering (ICSE) .
1295–1298. https://doi.org/10.1109/ICSE.2013.6606701
[58] D. Poshyvanyk, A. Marcus, and Yubo Dong. 2006. JIRiSS - an Eclipse Plug-in
for Source Code Exploration. In 14th IEEE International Conference on Program
Comprehension (ICPC’06) . 252–255. https://doi.org/10.1109/ICPC.2006.32
[59] Steven P. Reiss. 2009. Semantics-Based Code Search. In Proceedings of the 31st In-
ternational Conference on Software Engineering (ICSE ’09) . IEEE Computer Society,
Washington, DC, USA, 243–253. https://doi.org/10.1109/ICSE.2009.5070525
[60] Daniel E. Rose and Danny Levinson. 2004. Understanding User Goals in Web
Search. In Proceedings of the 13th International Conference on World Wide Web
(WWW ’04) . ACM, New York, NY, USA, 13–19. https://doi.org/10.1145/988672.
988675
[61] Caitlin Sadowski, Kathryn T. Stolee, and Sebastian Elbaum. 2015. How Developers
Search for Code: A Case Study. In Proceedings of the 2015 10th Joint Meeting on
Foundations of Software Engineering (ESEC/FSE 2015) . ACM, New York, NY, USA,
191–201. https://doi.org/10.1145/2786805.2786855
[62] Naiyana Sahavechaphan and Kajal Claypool. 2006. XSnippet: Mining For Sample
Code. In Proceedings of the 21st Annual ACM SIGPLAN Conference on Object-
oriented Programming Systems, Languages, and Applications (OOPSLA ’06) . ACM,
New York, NY, USA, 413–430. https://doi.org/10.1145/1167473.1167508
[63] H. Sanchez and J. Whitehead. 2015. Source Code Curation on StackOverflow:
The Vesperin System. In 2015 IEEE/ACM 37th IEEE International Conference on
Software Engineering , Vol. 2. 661–664. https://doi.org/10.1109/ICSE.2015.217[64] H. Sanchez, J. Whitehead, and M. Schäf. 2016. Multistaging to Understand:
Distilling the Essence of Java Code Examples. In 2016 IEEE 24th International
Conference on Program Comprehension (ICPC) . 1–10. https://doi.org/10.1109/
ICPC.2016.7503708
[65] R.C. Seacord, S.A. Hissam, and K.C. Wallnau. 1998. AGORA: A Search Engine
for Software Components. IEEE Internet Computing 2, 6 (Nov. 1998), 62–. https:
//doi.org/10.1109/4236.735988
[66] S.E. Sim, C.L.A. Clarke, and R.C. Holt. 1998. Archetypal Source Code Searches: A
Survey of Software Developers and Maintainers. In , 6th International Workshop
on Program Comprehension, 1998. IWPC ’98. Proceedings . 180 –187. https://doi.
org/10.1109/WPC.1998.693351
[67] Susan Elliott Sim, Megha Agarwala, and Medha Umarji. 2013. A Controlled
Experiment on the Process Used by Developers During Internet-Scale Code
Search. In Finding Source Code on the Web for Remix and Reuse , Susan Elliott
Sim and Rosalva E. Gallardo-Valencia (Eds.). Springer New York, 53–77. http:
//link.springer.com/chapter/10.1007/978-1-4614-6596-6_4 DOI: 10.1007/978-1-
4614-6596-6_4.
[68] Susan Elliott Sim, Medha Umarji, Sukanya Ratanotayanon, and Cristina V. Lopes.
2011. How Well Do Search Engines Support Code Retrieval on the Web? ACM
Trans. Softw. Eng. Methodol. 21, 1 (Dec. 2011), 4:1–4:25. https://doi.org/10.1145/
2063239.2063243
[69] Herbert A. Simon and Allen Newell. 1971. Human Problem Solving: The State
of the Theory in 1970. American Psychologist 26, 2 (1971), 145–159. https:
//doi.org/10.1037/h0030806
[70] Renuka Sindhgatta. 2006. Using an Information Retrieval System to Retrieve
Source Code Samples. In Proceedings of the 28th International Conference on
Software Engineering (ICSE ’06) . ACM, New York, NY, USA, 905–908. https:
//doi.org/10.1145/1134285.1134448
[71] J. Starke, C. Luce, and J. Sillito. 2009. Working with Search Results. In ICSE Work-
shop on Search-Driven Development-Users, Infrastructure, Tools and Evaluation,
2009. SUITE ’09 . 53 –56. https://doi.org/10.1109/SUITE.2009.5070023
[72] Kathryn T. Stolee, Sebastian Elbaum, and Daniel Dobos. 2014. Solving the Search
for Source Code. ACM Transactions on Software Engineering and Methodology 23,
3 (June 2014), 26:1–26:45. https://doi.org/10.1145/2581377
[73] A P Street and D J Street. 1986. Combinatorics of Experimental Design . Oxford
University Press, Inc., New York, NY, USA.
[74] J. Stylos and B.A. Myers. 2006. Mica: A Web-Search Tool for Finding API Compo-
nents and Examples. In IEEE Symposium on Visual Languages and Human-Centric
Computing, 2006. VL/HCC 2006 . 195 –202. https://doi.org/10.1109/VLHCC.2006.32
[75] Suresh Thummalapenta and Tao Xie. 2007. Parseweb: A Programmer Assistant
for Reusing Open Source Code on the Web. In Proceedings of the twenty-second
IEEE/ACM international conference on Automated software engineering (ASE ’07) .
ACM, New York, NY, USA, 204–213. https://doi.org/10.1145/1321631.1321663
[76] J. W. Tukey. 1949. Comparing Individual Means in the Analysis of Variance.
Biometrics 5, 2 (June 1949), 99–114.
[77] Taciana A. Vanderlei, Frederico A. Durão, Alexandre C. Martins, Vinicius C.
Garcia, Eduardo S. Almeida, and Silvio R. de L. Meira. 2007. A Cooperative
Classification Mechanism for Search and Retrieval Software Components. In
Proceedings of the 2007 ACM Symposium on Applied Computing (SAC ’07) . ACM,
New York, NY, USA, 866–871. https://doi.org/10.1145/1244002.1244192
[78] Yuepeng Wang, Yu Feng, Ruben Martins, Arati Kaushik, Isil Dillig, and Steven P.
Reiss. 2016. Type-Directed Code Reuse using Integer Linear Programming.
arXiv:1608.07745 [cs] (Aug. 2016). http://arxiv.org/abs/1608.07745 arXiv:
1608.07745.
[79] Yunwen Ye and Gerhard Fischer. 2002. Supporting Reuse by Delivering Task-
relevant and Personalized Information. In Proceedings of the 24th International
Conference on Software Engineering (ICSE ’02) . ACM, New York, NY, USA, 513–523.
https://doi.org/10.1145/581339.581402
[80] Yunwen Ye, Yasuhiro Yamamoto, Kumiyo Nakakoji, Yoshiyuki Nishinaka, and
Mitsuhiro Asada. 2007. Searching the Library and Asking the Peers: Learning to
Use Java APIs on Demand. In Proceedings of the 5th International Symposium on
Principles and Practice of Programming in Java (PPPJ ’07) . ACM, New York, NY,
USA, 41–50. https://doi.org/10.1145/1294325.1294332
[81] Annie T. T. Ying and Martin P. Robillard. 2013. Code Fragment Summarization. In
Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering
(ESEC/FSE 2013) . ACM, New York, NY, USA, 655–658. https://doi.org/10.1145/
2491411.2494587
[82] A. Zagalsky, O. Barzilay, and A. Yehudai. 2012. Example Overflow: Using So-
cial Media for Code Recommendation. In 2012 Third International Workshop
on Recommendation Systems for Software Engineering (RSSE) . 38 –42. https:
//doi.org/10.1109/RSSE.2012.6233407
785