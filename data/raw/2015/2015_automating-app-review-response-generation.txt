Automating App Review Response Generation
Cuiyun Gao‚Ä†Jichuan Zeng‚Ä†Xin Xia‚Ä°David Lo¬ßMichael R. Lyu‚Ä†Irwin King‚Ä†
‚Ä†Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China
‚Ä°Faculty of Information Technology, Monash University, Australia
¬ßSchool of Information Systems, Singapore Management University, Singapore
{cygao,jczeng,lyu,king }@cse.cuhk.edu.hk xin.xia@monash.edu davidlo@smu.edu.sg
Abstract ‚ÄîPrevious studies showed that replying to a user
review usually has a positive effect on the rating that is givenby the user to the app. For example, Hassan et al. foundthat responding to a review increases the chances of a userupdating their given rating by up to six times compared to
not responding. To alleviate the labor burden in replying to the
bulk of user reviews, developers usually adopt a template-basedstrategy where the templates can express appreciation for usingthe app or mention the company email address for users to followup. However, reading a large number of user reviews every dayis not an easy task for developers. Thus, there is a need for more
automation to help developers respond to user reviews.
Addressing the aforementioned need, in this work we propose
a novel approach RRGen that automatically generates reviewresponses by learning knowledge relations between reviews andtheir responses. RRGen explicitly incorporates review attributes,such as user rating and review length, and learns the relationsbetween reviews and corresponding responses in a supervised
way from the available training data. Experiments on 58 apps
and 309,246 review-response pairs highlight that RRGen out-performs the baselines by at least 67.4% in terms of BLEU-4(an accuracy measure that is widely used to evaluate dialogueresponse generation systems). Qualitative analysis also conÔ¨Årmsthe effectiveness of RRGen in generating relevant and accurate
responses.
Index T erms ‚ÄîApp reviews, response generation, neural ma-
chine translation.
I. I NTRODUCTION
Mobile apps are software applications designed to run on
smartphones, tablets and other mobile devices. They alreadyserve as an integral part of people‚Äôs daily life, and continuouslygain traction over the last few years. The apps are typicallyavailable from app stores, such as Apple‚Äôs App Store and
Google Play. These app stores allow users to express their
opinions to apps by writing reviews and giving ratings. Userexperience determines if users will keep using an app or unin-stall it, possibly posting favorable or unfavorable feedbacks.For example, a survey in 2015 [1] reported that 65% users
chose to leave a rating or review after a negative experience,
and only 15% users would consider downloading an app witha 2-star rating. To compete with the bulk of the apps offeringsimilar functionalities, ensuring good user experience is crucialfor app developers.
App reviews act as one direct communication channel
between developers and users, delivering users‚Äô instant ex-perience after their interactions with apps. Analysis on appreviews can assist developers in discovering in a timely mannerimportant app issues, such as bugs to Ô¨Åx or requested features,for app maintenance and development [2], [3]. Currently, both
Apple‚Äôs App Store and Google Play provide a review responsesystem for developers to manually respond to a review, afterwhich the corresponding user who posted the review will be
notiÔ¨Åed and have the option to update their reviews [4], [5].
In the response, developers can talk about the roadmap aboutusers‚Äô proposed feature requests, explain the usage of appfunctionalities, or just thank users for their shared opinions.
Empirical studies [6]‚Äì[9] that analyze the interactions be-
tween users and developers demonstrate that responding touser feedback in a timely and accurate manner can (1)
enhance app development and (2) improve user experience.SpeciÔ¨Åcally, Nayebi et al. [9] automatically summarized userrequests which was proven to shorten the cycle between issueescalation and developers‚Äô Ô¨Åx. McIlroy et al. [7] observed
that users change their rating 38.7% of the time following a
developer response. Hassan et al. [8] found that developers of34.1% of the apps they analyzed respond to at least one review,and also conÔ¨Årmed the positive effect of the responses onrating change. For example, they discovered that the number
of users who increases their ratings after receiving a response
are six times more than those who receive no response. Appdevelopers can also solve 34% of the reported issues withoutdeploying an update. In spite of the beneÔ¨Åts of the review-response mechanism, due to the large and ever-increasingnumber of reviews received daily, many reviews still did notreceive timely response [4], [8]. This highlights the necessityand importance of automatic response generation, which is thefocus of our work.
Dialogue generation has been extensively studied in the
natural language processing Ô¨Åeld [10]‚Äì[12], for facilitating
social conversations, e.g., the Microsoft XiaoIce chatbot [13].
Such work is generally grounded in the basic RNN Encoder-Decoder model (or Neural Machine Translation model, ab-breviated as NMT) [14], [15], where the context and corre-sponding response are regarded as source and target sequencesrespectively. The RNN Encoder-Decoder model is an end-
to-end learning approach for automated translation. It has
been applied to a number of software engineering tasks, suchas producing a sequence of APIs given a natural languagequery [16], parsing natural language into machine interpretablesequences (e.g., database queries) [17], generating commit
messages according to code changes [18], [19], and infer-
ring variable types based on contextual code snippets [20].However, the applicability of the NMT model for app review
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXB SF&OHJOFFSJOH	"4&
¬•*&&&
%0*"4&
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. response generation has not been studied. To Ô¨Åll in this gap,
we explore the usability of the NMT model in the app review-response dialogue scenario here, by regarding user reviews andthe corresponding replies as the source and target sequencesrespectively.
Directly applying the NMT model to app dialogue gener-
ation may not be appropriate, since the app review-responsedialogues and social conversations are different in many ways.First, the purpose of app dialogues is to further understandusers‚Äô complaints or solve user requests, while social conver-sations are mainly for entertainment purpose. This implies thatapp reviews require more accurate and clearer response [4].Second, users‚Äô sentiment expressed in reviews should beprecisely identiÔ¨Åed. Although reviews contain the informationof star ratings, the ratings and actual emotions may not betotally consistent [21], [22]. For example, one user may write
positive feedback like ‚Äú Great ‚Äù, but only give one-star rating.
Third, app reviews are generally short in length and usuallywith only one round of dialogue. According to Hassen etal. [8], 97.5% of the app dialogues end after one iteration.Such limited context increases the difÔ¨Åculty of generating aconcise response.
In this paper, we propose an improved NMT model, named
RRGen, for accurate Review Response Gen eration. We extend
basic NMT by incorporating review-speciÔ¨Åc characteristics(e.g., star ratings and review lengths) to capture user‚Äôs senti-ment and complaint topics. To evaluate the effectiveness of our
model, we collected 309,246 review-response pairs from 58popular apps published on Google Play. For a comprehensivecomparison, besides the basic NMT model, we also choose
the state-of-the-art approach in commit message generationbased on code changes [23], named NNGen, as one baselinemodel. Because NNGen adopts basic information retrieval
technique which is commonly used in traditional dialogue
generation tasks [24]‚Äì[26], and claims better performancethan the basic NMT model. Our experimental results showthat RRGen signiÔ¨Åcantly outperforms the baseline models by67.4%‚àº450% in terms of BLEU-4 score [27] (an accuracy
measure that is widely used to evaluate dialogue response
generation systems). Human evaluation done through a userstudy also indicates that RRGen can generate a more rele-vant and accurate response than NNGen. Besides reportingthe promising results, we investigate the reason behind the
superior performance of our model and the key constraints on
automatic response generation.
The main contributions of our work are as follows:
‚Ä¢To our knowledge, we are the Ô¨Årst to consider the problem
of automatic review response generation, and propose adeep neural network technique for solving the problem.We propose a novel neural machine translation model,
RRGen
1, to learn both topics and sentiments of reviews
for a accurate response generation.
‚Ä¢The accuracy of RRGen is empirically evaluated using
a corpus of more than 300 thousand real-life review-
1available at: https://github.com/ReMine-Lab/RRGenresponse pairs. A user study was also conducted to verifyRRGen‚Äôs effectiveness in generating reasonable reviews.
Paper structure. Section II illustrates the background of
review-response system, and neural encoder-decoder model.Section III presents our proposed model for user reviewresponse generation. Section IV and Section V describe ourexperimental setup and the quantitative evaluation results.Section VI details the results of a human evaluation of ourproposed model. Section VII discusses the advantages, limita-tion, and threats of our work. Related work and Ô¨Ånal remarksare discussed in Section VIII and Section IX, respectively.
II. B
ACKGROUND
Our work adopts and augments advanced techniques from
deep learning and neural machine translations [28]‚Äì[30]. Inthis section, we introduce the user-developer dialogue anddiscuss the background of these techniques.
A. User-Developer Dialogue
Figure 1 depicts an example of the user-developer dialogue
of the TED app in Google Play. A user initiates the dialogueby posting a review, including a star rating, for an app.User reviews convey valuable information to developers, such
as major bugs, feature requests, and simple complaints or
praise about the experience [31]. As encouraged by the AppStore [4], responding to feedback in a timely and consistentmanner can improve user experience and an app‚Äôs ranking.For example, the review in Fig. 1 was complaining about theunclear functionality usage related to adding ‚Äú video subtitles ‚Äù.
The TED developer then responded with detailed steps for
putting subtitles, and later, the user changed the star rating toÔ¨Åve.
Generally, developers could not reply to all app reviews due
to their limited time and efforts, and also a large number ofreviews. As studied by Hassan et al. [8], developers respondto 2.8% of the collected user reviews, and they tend to replyreviews with low ratings and long contents. The App Storealso suggests developers to consider prioritizing reviews with
the lowest star ratings or those mentioning technical issues
for responding [4]. However, ranking reviews for developers‚Äôreply is out of the scope of this work, and the related studiescan be found in [7], [8]. We focus on alleviating the manuallabor in responding to feedback and aim at automating the
process. Moreover, since 97.5% of the app dialogues end after
one round [8], in this study, we concentrate on one iterationof user review reply.
B. RNN Encoder-Decoder Model
The RNN Encoder-Decoder [14] model is an effective
and standard approach for neural machine translation and
sequence-to-sequence (seq2seq) [32] prediction. In general,the RNN encoder-decoder models aim at generating a targetsequence y=(y
1,y2,...,yTy)given a source sequence x=
(x1,x2,...,xTx), whereTxandTyare sequence lengths of
the source and target respectively. Fig. 2 illustrates an overallarchitecture of the RNN encoder-decoder model.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. February 11, 2019
February 11, 2019Hey how do you put subtitles on videos? I get a hard time understanding English
without subtitles even if I know how to speak it. Thanks! Will give 5 stars if you
can help me.
To use subtitles on the Android app: -Open the talk /g92/g82/g88/g182/g71 like to watch /g177 Tap
the play arrow /g177 Tap the /g179/g85/g72/g71 /g78/g72/g92/g69/g82/g68/g85/g71/g180 icon at the bottom of the video page
/g177 Choose your language /g177 Return to the talk if /g92/g82/g88/g182/g85/g72 trying to add subtitles
to a download, follow the above steps first before you download the video. Ihope this helps!
Fig. 1: Example of TED developer‚Äôs response to one user review.
The red underlines highlight some topical words of the dialogue.
lot of ad !sorry for the inconvenience </s>
/g22/g135/g147/g151/g135/g144/g133/g135/g3/g8/g144/g133/g145/g134/g135/g148/g22/g135/g147/g151/g135/g144/g133/g135/g3/g7/g135/g133/g145/g134/g135/g148
.
/g2205/g2869 /g2205/g2870 /g2205/g2871 /g2205/g2872/g2190/g2869 /g2190/g2870 /g2190/g2871 /g2190/g2872/g2190/g1314/g2869 /g2190/g1314/g2870 /g2190/g1314/g2871 /g2190/g1314/g2872 /g2190/g1314/g2873 /g2190/g1314/g2874/g2205/g1314/g2869 /g2205/g1314/g2870 /g2205/g1314/g2871 /g2205/g1314/g2872 /g2205/g1314/g2873 /g2205/g1314/g2874
/g2185
/g2206/g483/g2207/g483
sorry for theinconvenience . <s>
Fig. 2: An overall architecture of RNN encoder-decoder model.
To do so, an encoder Ô¨Årst converts the source sequence
xinto a set of hidden vectors {h1,h2,...,hTx}, whose
size varies regarding the source sequence length. The con-text representation cis generated using a Recurrent Neural
Network (RNN) [33]. The encoder RNN reads the sourcesentences from the Ô¨Årst token until the last one, where h
t=
f(ht‚àí1,wt), andc=hTx. Here,wtis the word embedding
of the source token xt, where word embeddings [34] are
distributed representations of words in a continuous vector
space, and trained with a text corpus. The fis a non-linear
function that maps a the word embedding wtinto a hidden
statehtby considering the previous hidden state ht‚àí1.
Then, the decoder, which is also implemented as an RNN,
generates one word ytat each time stamp tbased on the hidden
stateh/prime
tas well as the previous predicted word yt‚àí1:
Pr(yt|yt‚àí1,...,y1,c)=g(h/prime
t,yt‚àí1,c), (1)
wheregis a non-linear mapping function, and the context
vectorcreturned by the encoder is set as an initial hidden
state, i.e., h/prime
1=c. The decoder stops when generating the
end-of-sequence word <\s>.
The two RNN encoder-decoder models are jointly trained
to maximize the conditional log-likelihood:
L(Œ∏) = max
Œ∏1
NN/summationdisplay
i=1logpŒ∏(yi|xi), (2)
whereŒ∏is the set of the model parameters (e.g., weights in the
neural network) and each (xi,yi)is a (source sequence, target
sequence) pair from the training set. The pŒ∏(yi|xi)denotes
the likelihood of generating the i-th target sequence yigiventhe source sequence xiaccording to the model parameters
Œ∏. Through optimizing the loss function using optimization
algorithms such as gradient descent, the optimum Œ∏values
can be estimated.
C. Attention Mechanism
A potential issue with the RNN encoder-decoder model is
that a neural network needs to compress all the necessary
information of a source sequence into a Ô¨Åxed-length vector. Toalleviate this issue, Bahdanau et al. [28] proposed the attentionmechanism to focus on relevant parts of the source sequenceduring decoding. We use the attention mechanism in our work
because previous studies [35]‚Äì[37] prove that attention-based
models can better capture the key information (e.g., topicalor emotional tokens) in the source sequence. Fig. 3 shows agraphical illustration of the attentional RNN encoder-decodermodel.
During decoding, besides the hidden state h
/prime
tand previous
predicted word yt‚àí1, an attention vector atis also involved
for generating one word ytat each time stamp t:
Pr(yt|yt‚àí1,...,y1,c)=g(h/prime
t,yt‚àí1,c,at). (3)
The attention vector atdepends on the relevance between
the hidden state h/prime
tand the encoded source sequence
(h1,...,hTx):
at=Tx/summationdisplay
j=1Œ±tjhj, (4)
whereTxis the length of the source sequence, and the attention
weightŒ±tjmeasures how helpful the j-th hidden state of the
source sequence hjis in predicting next word ytwith respect
to the previous hidden state h/prime
t‚àí1. In this way, the decoder
decides parts of the source sentence to pay attention to.
/g2190/g2869 /g2190/g2870 /g2190/g2871 /g2190/g2872 /g2190/g1314/g2869 /g2190/g1314/g2870/g2205/g1314/g2869/g2205/g1314/g2870/g2009/g2869/g2869/g4/g150/g150/g135/g144/g150/g139/g145/g144/g3/g25/g135/g133/g150/g145/g148
/g2009/g2869/g2870 /g2009/g2869/g2871 /g2009/g2869/g2872/g2183/g2869
/g1855
Fig. 3: Graphical illustration of the attentional RNN encoder-decoder
model. The dotted line without arrow marks the division between theencoder (left) and decoder (right), and the dotted lines with arrowsindicate that we simplify the RNN encoder-decoder [14] steps forclearness.
III. RRG EN:A PPREVIEW RESPONSE GENERA TION
In this section, we present the design of RRGen that
extends the basic attentional RNN Encoder-Decoder model for
app review response generation. We regard user reviews asthe source sequence and developers‚Äô response as the targetsequence. Fig. 2 shows an example of the RNN Encoder-Decoder model for generating a sequence of tokens as adeveloper‚Äôs response from a sequence of tokens that constitute
a user review ‚Äú Lot of ad! ‚Äù. For accurately capturing the topics

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. User Review 
Repository/g39/g72/g89/g72/g79/g82/g83/g72/g85/g86/g182/g3
Response/g155
User 
ReviewsRating
SentimentLengthCategory /g4/g3/g6/g145/g143/g146/g145/g144/g135/g144/g150
/g28433/g4/g150/g150/g148/g139/g132/g151/g150/g135/g149 /g28434
/g14/g3/g6/g145/g143/g146/g145/g144/g135/g144/g150
/g523/g14/g135/g155/g153/g145/g148/g134/g149/g524Keywordlot ofad !
Encoder<O> <O> <C> <O>/g2205/g2869/g2205/g2870/g2205/g2871/g2205/g2872/g2193/g2869/g2193/g2870/g2193/g2871/g2193/g2872/g2204/g2869/g2204/g2870/g2204/g2871/g2204/g2872
/g2206/g483
/g2244/g483sorry for <s>sorry fortheDecoder
Attention Vector/g2207/g483 /g485/g485
/g485</s>
.
ModelTrainingUser Review
Review 
Responsea. Data 
preparationb. Data parsing c. Model training d. Response 
generation
/g2028
/g1864
/g1870
/g1871
/g2244/g2028
/g1864
/g1870
/g1871/g2206
(a) Overall architecture of RRGenlot ofad !
Encoder<O> <O> <C> <O>/g2205/g2869/g2205/g2870/g2205/g2871/g2205/g2872/g2193/g2869/g2193/g2870/g2193/g2871/g2193/g2872/g2204/g2869/g2204/g2870/g2204/g2871/g2204/g2872
/g2206/g483
/g2244/g483sorry for <s>sorry fortheDecoder
/g4/g150/g150/g135/g144/g150/g139/g145/g144/g3/g25/g135/g133/g150/g145/g148/g2207/g483 /g485/g485
/g485</s>
.
/g14/g3/g6/g145/g143/g146/g145/g144/g135/g144/g150/g2028
/g1864
/g1870
/g1871/g2190/g3099
/g2190/g3039
/g2190/g3045
/g2190/g3046/g16/g15/g19/g16/g15/g19
/g4/g3/g6/g145/g143/g146/g145/g144/g135/g144/g150/g2185/g2185/g4593
/g2190/g2869/g2190/g2870/g2190/g2871/g2190/g2872 /g16/g15/g19
(b) Detailed structure of RRGen
Fig. 4: Structure of the review response generative model.
and sentiment embedded in the input review sequence, we
explicitly incorporate both high-level attributes (e.g., app cat-egory, review length, user rating, and sentiment) and keywordsinto the original RNN Encoder-Decoder model. We adopt thekeywords provided by Di Sorbo et al. [3] which were manuallycurated to identify 12 topics (e.g., GUI, contents, pricing, etc.)commonly covered in user reviews. We refer to the high-levelattributes and keywords extracted from a review as its A and
K components, respectively.
Figure 4 (a) shows the overall architecture of our RRGen
model. RRGen mainly consists of four stages: Data prepara-tion, data parsing, model training, and response generation. We
Ô¨Årst collect app reviews and their responses from Google Play,and conduct preprocessing. The preprocessed data are parsedinto a parallel corpus of user reviews and their corresponding
responses, during which the two components of reviews are
also extracted and processed. Based on the parallel corpus ofapp reviews and responses, we build and train a generativeneural model with the two pieces of extracted information(high-level attributes and keywords) holistically considered.The major challenge during the training process lies in the
effective consideration of both components of reviews for ef-
fective response generation. In the following, we will introducethe details of the RRGen model and the approach we proposeto resolve the challenge.
A. Component Incorporation
Here, we elaborate on how we incorporate the two com-
ponents, including high-level attributes (or A Component)
and keywords (or K Component), into RRGen. The detailed
structure of RRGen is displayed in Fig. 4 (b).
1) A Component: The A component contains four attributes
of one user review: App category, review length, user rating,and sentiment. We choose app category considering that appsof different categories generally contain different functional-
ities, and major topics delivered by their reviews would be
different. Review length is involved because it is an importantindex of whether the review is informative or not, i.e., longerreviews usually convey richer information [8], [38]. We takeuser rating into account since it can directly impact the
response style of developers, e.g., expressing an apology fornegative feedback or thanks for the positive feedback. As userratings may not be consistent with the sentiment described
by the reviews [39], we also regard the predicted actual usersentiment as one attribute.
Review attributes such as app category, review length, and
user rating are easy to acquire. For predicting user sentiment,we exploit SentiStrength [40], a lexical sentiment extractiontool specialized in handling short and low-quality texts. We
Ô¨Årst divide review text into sentences, and then assigns a pos-
itive integer value (in the range [+1, +5]) and a negative integervalue (within the range [-5, -1]) based on StentiStrength toeach sentence because users may express both positive andnegative sentiments in the same sentence. A higher absolute
sentiment score indicates that the corresponding sentiment isstronger. Following Guzman and Maalej‚Äôs work [39], when
the sentence‚Äôs negative score multiplied by 1.5 is less than thepositive score, we assign the sentence a negative sentimentscore; Otherwise, the sentence is assigned a positive sentimentscore [39]. The sentiment of an entire review is computed
based on the rounded average sentiment scores of all sentences
in the review.
We denote the app category, review length, user rating, and
sentiment score of the source sequence xasœÑ,l,r, and
s, respectively. To incorporate these attributes into RRGen,
we Ô¨Årst represent the attribute values into continuous vectorsvia multilayer perceptions (MLPs), i.e., the conventional fullyconnected layer [41]. We call the vector representations ofthe attributes as attribute embeddings. The embedding of appcategory œÑis deÔ¨Åned as:
h
œÑ= tanh(WŒìEmb(œÑ)),‚àÄœÑ=1,2,...,NŒì, (5)
whereWŒìis the matrix of trainable parameters in the MLP ,
andhœÑ,g=1,...,NŒìare the embedding vectors of all individ-
ual categories. Emb(œÑ)‚ààRNŒìis the vector representation of
œÑ, andEmb(¬∑)indicates one general embedding layer to obtain

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. the latent features of œÑ. Similarly, we obtain the embedding
vectors for user rating rand sentiment score s:
hr= tanh(WREmb(r)),‚àÄr=1,2,...,NR, (6)
hs= tanh(WSEmb(s)),‚àÄs=1,2,...,NS, (7)
wherehrandhsare embeddings for the attribute values rand
s, respectively. For review length l, we convert the continuous
variable into its categorical form l/primeusing the pandas package2
before feeding into MLP .
hl= tanh(WLEmb(l/prime),‚àÄl/prime=1,2,...,NL. (8)
We integrate the embedded attribute values at review level
by concatenating together with the last hidden state cof the
encoder, i.e.,
c/prime= tanh(WH[c;hœÑ;hl;hr;hs]), (9)
where[a;b]is the concatenation of these two vectors. WHis
the matrix of trainable parameters in the MLP , and His the
number of hidden units. The output vector c/primeindicates the Ô¨Ånal
hidden state (or context vector) of the encoder. For simplicity,
we assume that the dimensions of all attribute embeddings,
i.e.,hœÑ,hl,hr, andhs, are the same.
2) K Component: K component speciÔ¨Åcally refers to key-
words in the input review sequence, since the keywords
generally relate to the review topic or sentiment, and arepotentially helpful to learn which word to attend to duringresponse generation.
T ABLE I: One example of topic-keywords pair in the keyword
dictionary provided by Di Sorbo et al. [3].
Topic Keywords
GUIscreen, trajectory, button, white, background, interface,
usability, tap, switch, icon, orientation, picture, show, list,category, cover, scroll, touch, clink, snap, underside, backside,witness, rotation, ui, gui,...
We adopt the keyword dictionary provided by Di Sorbo
et al. [3]. Di Sorbo et al. summarize 12 topics3commonly
covered by user reviews based on manual analysis, and build
a keyword dictionary based on WordNet [42] to extract relatedwords for each topic. One topic-keywords pair can be seenin Table I. Di Sorbo et al. utilized the dictionary to predicttopics of user reviews and achieved >90% classiÔ¨Åcation ac-
curacy; this indicates the semantic representativeness of these
keywords for each topic. This motivates us to use the keywords
too in our work.
To explicitly integrate the keyword information into RRGen,
we establish a keyword sequence Œ∫=(Œ∫
1,Œ∫2,...,Œ∫Tx)for
each input review sequence x. SpeciÔ¨Åcally, for the token
xtinx, we check the keyword dictionary to determine its
subordinate topic, i.e., Œ∫t. For example, as shown in Fig. 4 (b),
the keyword sequence corresponding to the source sequence
2https://pandas.pydata.org/pandas-docs/stable/
3The 12 topics are app, GUI, contents, pricing, feature, improvement,
updates/versions, resources, security, download, model, and company.‚Äúlot of ad ! ‚Äùi s‚Äú<O>< O>< C>< O>‚Äù, where we denote the
keyword symbol for the token ‚Äú ad‚Äùa s‚Äú<C>‚Äù since ‚Äú ad‚Äùi s
one keyword for topic contents . The keyword symbols of non-
topical words (e.g., ‚Äú of‚Äù) are labeled as ‚Äú <O>‚Äù. We Ô¨Ånally
integrate the embedded keyword sequence and the sourcesequence at token level via MLP:
k
Œπ= tanh(WKEmb(Œ∫Œπ)),‚àÄŒπ=1,2,...,NK
vt= tanh(WV[kt;wt])(10)
wherekŒπ,Œπ=1,...,NKare the embedding vectors of all
individual keyword symbols, WKandWVare the matrices
of trainable parameters in the MLPs, and vtis the keyword-
enhanced embedding for the t-th token xtin the source
sequence. The dimension of kŒπis similar to the attribute
embeddings in the A component, e.g, hœÑ.
B. Model Training and Testing
1) Training: We adopt the attention mechanism, described
in Section II-C, for review response generation. The RNN hasvarious implementations, we use bidirectional Gated RecurrentUnits (GRUs) [14] which is a popular RNN encoder-decodermodel and performs well in many tasks [43], [44]. All GRUshave 200 hidden units in each direction. Each attribute in the
two components is encoded into an embedding with dimension
at 90, i.e., the embedding size of h
œÑ,hl,hr,hs, andkŒπ. Word
embeddings are initiated with pre-trained 100-dimensionalGloV e vectors [45]. We set the maximum sequence length at200 and save the model every 200 batches. We discuss thedetails of parameter tuning in Section V -C. The training goalis cross-entropy minimization based on Equ. (11):
L(Œ∏) = max
Œ∏1
NN/summationdisplay
i=1logpŒ∏(yi|xi,œÑ,l,r,s, Œ∫i), (11)
whereœÑ,l,r,s,Œ∫icorrespond to the app category, review
length, user rating, sentiment score, and keyword sequence ofthei-th source sequence x
i, respectively. The whole model is
trained using the minibatch Adam [46], a stochastic optimiza-tion approach and automatically adjusting the learning rate. We
set the batch size (i.e., number of review instances per batch)as 32. For training the neural networks, we limit the sourceand target vocabulary to the top 10,000 words that are mostfrequently used in user reviews and developers‚Äô responses.
For implementation, we use PyTorch [47], an open-source
deep learning framework. We train our model in a server withone Nvidia TIT AN V GPU with 12GB memory. The training
lasts‚àº80 hours with two epochs.
2) Testing: We evaluate on the test set when the trained
model after one batch shows an improvement on the validation
set regarding BLEU score [27]. We take the highest test scoreand corresponding generated response as the evaluation result.We use the same GPU as we used in training. The testingprocess took around 25 minutes.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. IV . E XPERIMENT AL SETUP
A. Data Preparation
1) Data Collection: We select the subject apps for collect-
ing the user-developer dialogues from Google Play based onapp popularity. We focus on popular apps since they contain
more reviews than unpopular apps [48], which should facilitate
enough data for studying user-developer dialogues. We selectthe top 100 free apps in 2016 according to App Annie [49],an app analytics platform, as these apps were top apps twoyears prior to the start of our study. The decision was made
to ensure the studied apps had enough reviews to collect and
also avoid the inÔ¨Çuence of an app‚Äôs price on developers‚Äô reviewresponse behavior [8]. We further remove the apps that are nolonger available in Google Play on April 2018 and those withfewer than 100 user reviews, which leaves us with 72 appsthat match our selection criteria.
For each selected app, we created a Google Play crawler to
collect user-developer dialogues from Google Play, speciÔ¨Åcally
including review title, review text, review post time, user name,
rating, developer response time, and the text in the developerresponse. We run our crawler from April 2016 to April 2018.During that period, we collected 15,963,612 reviews for the 72apps. We Ô¨Ånd that 58/72 apps and 318,973 collected reviews
have received a response from the app developer. Table II
describes the statistics of the 58 subject apps which belong to15 app categories.
2) Data Preprocessing: Since app reviews are generally
submitted via mobile terminals and written using limitedkeyboards, they contain massive noisy words, such as repet-itive words and misspelled words [2]. We Ô¨Årst convert allthe words in the reviews and their response into lowercase,and adopt the method in [50] for lemmatization. We thenreplace all digits with ‚Äú <digit>‚Äù. We also detect email address
and URL with regular expressions, and substitute them into‚Äú<email>‚Äù and ‚Äú<url>‚Äù respectively. Besides, we build an
app list containing all the app names, and a user list with
all the user names. For the app names and user names
mentioned in the dialogue corpus, we replace them with‚Äú<app>‚Äù and ‚Äú<user>‚Äù respectively. We Ô¨Ånally adopt the
rule-based methods based on [50], [51] to rectify repetitivewords and misspelled words. After removing empty review
texts or review texts with only one single alphabet, we obtained
309,246 review-response pairs. We randomly split the datasetby 8:1:1, as the training, validation, and test sets, i.e., there are279,792, 14,727, and 14,727 pairs in the training, validation,and test sets, respectively.
T ABLE II: Mean and Ô¨Åve-number summary of collected data for
every studied app.
Avg. Min. 1st Qu. Med. 3rd Qu. Max.
#reviews per app 203,025 5,582 83,317 179,457 287,286 665,203
#reviews with5,406 2 181 1,149 4,290 55,165responses per app
B. Similarity Measure - BLEU
BLEU [27] is a standard automatic metric for evaluating
dialogue response generation systems. It analyzes the co-occurrences of n-grams in the ground truth yand the generatedresponses ÀÜy, wherencan be 1, 2, 3, or 4. BLEU- N, where
Nis the maximum length of n-grams considered, measures
the proportion of co-occurrences of nconsecutive tokens
between the ground truth yand generated response ÀÜy. The
most commonly used version of BLEU uses N=4 [19],
[23], i.e., BLEU-4. Also, BLEU-4 is usually calculated at thecorpus-level, which is demonstrated to be more correlated withhuman judgments than other evaluation metrics [52]. Thus, weuse corpus-level BLEU-4 as our evaluation metric.
C. Baseline Approaches
We compare the performance of our model with a random
selection approach, the basic attentional RNN encoder-decoder(NMT) model [28] (as introduced in Section II-C), and a state-of-the-art approach for code commit message generation [23],
namely NNGen. In the following, we elaborate on the Ô¨Årst and
last baselines:
Random Selection: This is a strawman baseline. This
baseline randomly picks a response in the training set and
uses it as a response to a review in the test set.
NNGen: We choose NNGen as one comparing approach
since it is demonstrated to perform better than the basic NMT
model [19] in producing code commit message based on
code changes. NNGen leverages the nearest neighbor (NN)algorithm to retrieve the most relevant developer response.Based on the training set and the new user review, NNGen Ô¨Årstrepresents them as vectors in the form of ‚Äúbags of words‚Äù [26],
and then selects the top Ô¨Åve training user reviews which
present highest cosine similarities to the new review. Afterthat, the BLEU-4 score between the new review and eachof the top Ô¨Åve training reviews is computed. NNGen Ô¨Ånallyregards the response of the training review with the highestBLEU-4 score as the result.
V. E
V ALUA TION USING ANAUTOMA TIC METRIC
In this section, we conduct quantitative analysis to evaluate
the effectiveness of RRGen. In particular, we intend to answerthe following research questions.
RQ1: What is the accuracy of RRGen?
RQ2: What is the impact of different component attributes
on the performance of RRGen?
RQ3: How accurate is RRGen under different parameter
settings?
A. RQ1: What is the accuracy of RRGen?
The comparison results with baseline approaches are shown
in Table III. We can see that our RRGen approach outperformsall the three baselines. SpeciÔ¨Åcally, the result that random se-lection approach achieves the lowest BLEU-4 score (6.55), in-
dicates that learning knowledge from existing review-response
pairs can facilitate generating the response for a newly-arrivedreview. Also, we Ô¨Ånd that the NMT model performs better thanthe non-deep-learning-based NNGen model, which shows anincreasing rate of 53.48% in terms of BLEU-4 score. This
is opposite to the conclusion achieved by Liu et al. [23].
One possible reason is that the tasks between ours and Liu

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. et al.‚Äôs [23] are different, i.e., Liu et al. aim at producing
texts based on code, while we focus on generating textsfor dialogues and modeling code is different from modelingdialogue texts [53], [54]. The higher BLEU-4 score of theproposed RRGen model than that of the NMT model explainsthat the response generated by the RRGen model is moresimilar to developers‚Äô response than the response generated bythe NMT model. We then use Wilcoxon signed-rank test [55]for statistical signiÔ¨Åcance test, and Cliff‚Äôs Delta (or d)t o
measure the effect size [56]. The signiÔ¨Åcance test result(p‚àívalue < 0.01) and large effect size on BLEU-4 scores
(d=0.74) of RRGen and NMT conÔ¨Årm the superiority of
RRGen over NMT.
T ABLE III: Comparison results with baseline approaches. The pn
indicates the n-gram precision when comparing the ground truth and
generated responses. Statistical signiÔ¨Åcance results are indicated with
*(p‚àívalue < 0.01).
Approach BLEU-4 p1 p2 p3 p4
Random 6.55 27.64 6.90 3.55 2.78
NNGen [23] 14.08 34.47 13.85 9.77 8.59NMT [28] 21.61 40.55 20.75 16.78 15.47
RRGen 36.17*53.24*35.83*31.73*30.04*
B. RQ2: What is the impact of different component attributes
on the performance of RRGen?
To evaluate the effectiveness of different component at-
tributes in response generation, we perform contrastive exper-iments in which only a single component attribute is added tothe basic NMT model [28]. Table IV shows the results.
Unsurprisingly, the combination of all component attributes
gives the highest improvements, and all the attributes arebeneÔ¨Åcial on their own. User sentiment gives the lowestimprovement (+0.58 in terms of BLEU-4 score) comparingto the NMT model, while the app category yields highestimprovement (+9.92 in terms of BLEU-4 score). Also, theresult that user rating contributed more on the BLEU-4 scorethan user sentiment indicates that user ratings would be more
helpful in review response generation. Moreover, the gain
from different component attributes is not fully cumulativesince the information encoded in these component attributesoverlaps. For instance, both the user sentiment and user ratingattributes encode the user emotion expressed by user reviews.
Also, the keywords in the K component highlights the words
belonging to the same topics, and such information may bealready captured by the word embeddings [34].
T ABLE IV: Contrastive experiments with individual component
attributes.
Approach BLEU-4 p1 p2 p3 p4
NMT [28] 21.61 40.55 20.75 16.78 15.47
A Component+App Category 31.53 47.49 30.64 26.84 25.30
+Review Length 24.22 41.96 22.30 18.16 16.76
+Rating 26.90 46.19 26.06 21.69 20.12
+Sentiment 22.19 40.42 20.95 16.99 15.69
K Component +Keyword 24.34 43.41 23.66 19.27 17.74
RRGen 36.17 53.24 35.83 31.73 30.04Dimension  of 
Word EmbeddingBLEU- 4 p1 p2 p3 p4
50 35.80 52.78 35.09 30.67 28.91
100 36.17 53.24 35.83 31.73 30.04
200 35.61 51.77 33.77 29.63 28.00
300 35.54 51.25 33.18 29.09 27.39
(a) Different dimensions of word embedding.
# Hidden UnitsBLEU-4 (%)
(b) Different numbers of hidden
units.
BLEU-4 (%)
Attribute Dimension
(c) Different dimensions of com-
ponent attribute embedding.
Fig. 5: BLEU-4 scores of different parameter settings.
C. RQ3: How accurate is RRGen under different parameter
settings?
We also quantitatively compare the accuracy of RRGen
in different parameter settings. We analyze three parameters,that is, the dimension of word embeddings, the number ofhidden units, and also the dimension of component attributeembeddings. We vary the values of these three parameters andevaluate their impact on the BLEU-4 scores.
Figure 5 shows the inÔ¨Çuence of different parameter settings
on the test set. We choose the four different dimensionsof word embeddings provided by GloV e [45], i.e., 50, 100,
200, and 300, and the result in Fig. 5 (a) indicates that the
RRGen model achieves the best BLEU-4 score when the wordembedding size equals to 100. For the number of hidden units,we can see that more hidden units may not be helpful forimproving accuracy, as shown in Fig. 5 (b). RRGen generatesthe best result when we deÔ¨Åne the number of hidden units as
200. Fig. 5 (c) shows that the accuracy of RRGen also changes
along with the variations of attribute embedding dimension.The optimum dimension of attribute embedding is around 90.
VI. H
UMAN EV ALUA TION
In this section, we conduct a human evaluation to comple-
ment the evaluation in Section V that uses BLEU, since BLEUonly measures the textual similarity between the generatedresponses and ground truth while the human study can evaluateusers‚Äô general satisfaction on the responses.
A. Survey Procedure
We conduct a human evaluation to evaluate the outputs of
RRGen and compare RRGen with NMT and NNGen. Weinvite 20 participants, including 14 PhD students, two master
students, one bachelor, and three senior researchers, all of
whom are not co-authors and major in computer science.Among the participants, 15 of them have industrial experiencein software development for at least a year, and eight of themhave developed one or two mobile apps. Each participantis asked to read 25 user reviews, and assess the responses
generated by NNGen, NMT, RRGen, and the app developers.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. B. Survey Design
We randomly selected 100 review-response pairs in total,
divide them evenly into four groups, and make a questionnairefor each group. We ensure that each review-response pair isevaluated by Ô¨Åve different participants. In our questionnaire,each question presents the information of one review-responsepair, i.e., its user review, the developer‚Äôs response, its outputfrom NNGen, and its responses generated by NMT andRRGen. The order of the responses from NNGen, NMT,RRGen, and ofÔ¨Åcial developers is randomly decided for eachquestion.
Inspired by [11], [57], all the response types are evalu-
ated considering three aspects - ‚Äú grammatical Ô¨Çuency ‚Äù, ‚Äú rel-
evance ‚Äù, and ‚Äú accuracy ‚Äù. We provided the following instruc-
tions at the beginning of each questionnaire to guide partic-ipants: The ‚Äú grammatical Ô¨Çuency ‚Äù (or readability) measures
the degree of whether a text is easy to understand; The metric‚Äúrelevance ‚Äù relates to the extent of topical relevance between
the user review and response; And the metric ‚Äú accuracy ‚Äù
estimates the degree of the response accurately answering auser review.
All the three metrics are rated on a 1-5 scale (5 for fully sat-
isfying the rating scheme, 1 for completely not satisfying therating scheme, and 3 for the borderline cases), since a 5-pointscale is widely used in prior software engineering studies [3],
[23], [58]. Besides the three metrics, each participant is asked
to rank responses generated by the three tools and those fromdevelopers based on their preference. The ‚Äú preference rank ‚Äù
score is rated on a 1-4 scale (1 for the most preferred). Fig. 6shows one question in our survey. Participants do not knowwhich response is generated by which approach or whether it
is written by developers, and they are asked to enter to score
each response separately.
C. Results
We obtained 500 sets of scores from the human evaluation.
Each set contains scores for the three metrics regarding theresponse of NNGen, NMT, RRGen, and ofÔ¨Åcial developersrespectively, and also a ranking score of the four typesof responses. The median time cost for one participant tocomplete his/her questionnaire is 0.94 hour, with an averagevalue of 2.72 hours. We compute the agreement rate on the thepreference ranks given by the participants, and Ô¨Ånd that 81%
of the total 100 review-response pairs received at least three
identical preference ranks from the participants. SpeciÔ¨Åcally,31%, 36%, and 14% were given the same preference ranks bythree, four, and Ô¨Åve participants respectively. This indicatesthat the participants achieved reasonable agreement on theperformance of the generated responses.
Table V shows the results of human evaluation. Bold
indicates top scores. As expected, we can see that the response
from ofÔ¨Åcial developers is preferred over the three approaches‚Äô
outputs, which can be observed given the example in Fig. 6.SpeciÔ¨Åcally, the developers‚Äô response (Response 1) is morerelevant to the user review and provides more accurate solutionto the app issue (e.g., reduced picture clarity) complainedUser Review: Picclarityisreduced/g87/g75/g68/g87/g182/g86whygiveonly<digit>star.
Response 1:Hello<user>,thanksforyourhonestreview!Youcaneasilysolve
thisissuebygoing toyour/g31/g68/g83/g83/g33/g182/g86settingmaximagesizeandclickingonthepreferableimagesize.Iftheproblemstillcontinues,pleaseemailusat<email>.
Response 2:Hey<user>,thanksforyourreview.Weapologizefortheissueyou
arefacingandweareheretohelp.Pleasesendourteamyourdevicemodel<app>
versionand<app>osversionto<email>.Oursupportteamwillfurtherassistyou
onthematter.
Response 3:Hi,/g44/g182/g80Dianafrom<app>.Couldyoutell<app>whatkindofads
youdonotlike?Whatarethelocationsofthem?
Response 4:Hi<user>,thanksforyourreview.Wearereallysorrythatyoufeel
thiswayabouttheapp.
Note: This is a photography app, and the user rating is one star. In the sentences, the
symbols <digit>, <user>, <email>, and <app> denote one digit, user name, email
address, and app name, respectively.
Very Dissatisfied Very Satisfied
/g53/g72/g86/g83/g82/g81/g86/g72/g3/g20/g182/g86/g3/g41/g79/g88/g72/g81/g70/g92
/g53/g72/g86/g83/g82/g81/g86/g72/g3/g20/g182/g86/g3/g53/g72/g79/g72/g89/g68/g81/g70/g72
/g53/g72/g86/g83/g82/g81/g86/g72/g3/g20/g182/g86/g3/g36/g70/g70/g88/g85/g68/g70/g92
Your Preference Rank of the Four Responses:............
Fig. 6: A question in our survey. Response 1, 2, 3, and 4 correspond
to the developer‚Äôs response, the outputs of our RRGen model, and the
responses produced by NNGen and NMT, respectively. Participants
do not know the order of the four types of response during the survey,and are asked to score the three metrics for each response type. Thetwo-dot symbols indicate the simpliÔ¨Åed grading schemes of Response2, 3, and 4. The words highlighted in yellow are topical words in thedescriptions, and the double-underlined words mean they are topically
irrelevant to the user review.
by the user. In terms of grammatical Ô¨Çuency, however, the
RRGen model does quite well, achieving scores that arerather close to those of developers‚Äô responses, as shown in
Table V. In addition, we see that our RRGen model performssigniÔ¨Åcantly better across all the metrics in comparison to the
baseline approaches, which further indicates the effectivenessof RRGen in review response generation.
T ABLE V: Human evaluation results for review response generation.
Bold indicates top scores. Two-tailed t-test results are shown for
our RRGen approach compared to NNGen and NMT (StatisticalsigniÔ¨Åcance is indicated with
*(p‚àívalue < 0.01).).
Grammatical
FluencyRelevance AccuracyPreference
Rank
NNGen [23] 4.520 3.160 3.104 3.339NMT [28] 4.609 3.273 3.017 2.680
RRGen 4.626
*3.536*3.458*2.244*
Developer 4.644 3.804 3.712 1.736
VII. D ISCUSSION
A. Why does Our Model Work?
We have identiÔ¨Åed three advantages of RRGen that may
explain its effectiveness in review response generation.
Observation 1: RRGen can better capture salient top-
ics of user reviews. Unlike bag-of-words-based techniques,
RRGen learns review and response representations with atten-tional deep learning. Characteristics of reviews, such as topicalwords and word orders, are naturally considered in these
models [16]. Moreover, keywords that are indicative of review
topics are explicitly incorporated into the deep learning model,which would be helpful to better recognize the semantics ofreview and response. For example, it can learn that the review

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. User Review: Toomanyadsandslow.
/g39/g72/g89/g72/g79/g82/g83/g72/g85/g182/g86 Response: Dearuser,thanksforsupportinguc.Ifyoumeantheads
inwebsitethenplsmakesureyou'veenabledtheadblockpowerfuladblock
frommenuadblock.Ifnouse,plssendthespecificlinkandscreenshotofads
andotherquestionviamenufeedbackpageopeninuctohelpyou.Wereally
appreciateyour<digit>starvote!Niceday.
NNGen:Sorry about that. Could you please email us <email>? Tell more
information relate to your problem when do you notice app is making your
systemslow.AlsotrycheckoutourFAQ<url>.Thankyouverymuch.
NMT:Hi,thanksforusing<app>andsorryfortheads.<app>isfreeweneed
supportfromadstodevelopnewproductandtoimproveexistproduct.Weare
strive to balance user satisfaction and ads. Thus if you feel there are any
inappropriate ads or any ads you hate please contact us <email> much
appreciate.
RRGen:Hi,dearfriendsorrytohearthat.Youcanalsosendusad'slinkand
screenshottou3menufeedbackpageopeninuc?Herewecanhelpyousolve
theproblemfasterok?Waitforyourinformationand<digit>starvote!Niceday!
(a) First example.
User Review: Itsuckssomuchthatmyphonealmostoverheat.Thisbattery
saverisstupidanddumb./g39/g82/g81/g182/g87wasteurtime.
/g39/g72/g89/g72/g79/g82/g83/g72/g85/g182/g86 Response: Hi<user>.Thanksforyourfeedbackandnowweare
workingonthecoolertoimprovethisissue.Couldyoupleasesendyourdetail
problemtoouremail<email>sowhenwefigureoutthesolutionwecancontact
youimmediately?
NNGen:Hi thanks for trying clean master. We hope you have a good
experiencewithourappifyouhaveanyproblemorsuggestionyoucancontact
mevia<email>./g44/g182/g80readytohelp.
NMT:Dearuserwearesorryforads.However<app>isafreeapplication.To
keepitfreewhileofferagreatservice,weneedtopublishadsasanalternative
source of income. If you see any ads that are inappropriate, please take a
screenshotofthemandsendthroughthefeedbackoptionfindintheapplication.
RRGen:Dear user sorry for the inconvenience. We notice you have some
concerns about battery. Could you tell us what happened and provide some
informationincludingscreencontrastscreensh otsofbatterystatusofyourphone
settinggt.batterygt.?Pleasesendusviaemail<email>muchappreciated.
(b) Second example.
Fig. 7: Two sample review-response pairs where RRGen can generate
responses with more related topic. The meanings of the highlightedwords and double-underlined words are the same as Fig. 6.
‚ÄúToo many ads and slow ‚Äù is talking about the ad issue, and
generate response related to the in-app ads, as shown in Fig. 7(a). In the example in Fig. 7 (b), RRGen can well learn that thereview is discussing about the battery issue, while NMT infersthe topic wrongly. For the bag-of-words approach, NNGen, itmay be easily confused by non-topical words. For the example
in Fig. 6, NNGen (i.e., Response 3) focuses more on the words
‚Äúgive ‚Äù, ‚Äú<digit>‚Äù, and ‚Äú star ‚Äù, and selects the closest review
‚ÄúIf it has no advertising, I will give <digit>star ‚Äù which has
totally different topics comparing to the given review.
Observation 2: RRGen can generate responses with
more accurate sentiment. User sentiment can be explicitly
(e.g., the ‚Äú horrible ‚Äù word in Fig. 8 (a)) or implicitly (e.g., the
‚Äúslow ‚Äù word in Fig. 7 (a)) reÔ¨Çected in user reviews. For the
bag-of-words approach, the effect of sentiment words may beweakened by other words since their occurrence frequencies
are similar. As can be seen in Fig. 8 (a), NNGen fails toinfer the negative sentiment expressed by the review, and
considers it as an endorsement message; while RRGen canaccurately capture the negative information embedded in thereview. Another example can be found in Fig. 8 (b), where bothNNGen and NMT do not recognize that the positive sentiment
of the given review. Without review attributes such as userUser Review: Whathappenedtothelauncher?Allmyappnamesgotvanished
and there is no setting to show the name as well. Kindly help or else. /g44/g87/g182/g86 ahorribleapp.
/g39/g72/g89/g72/g79/g82/g83/g72/g85/g182/g86 Response: Dear<user>,youcansetappnamebyfollowingstep
launcher->seticon,thenyoucanfindtherightoptiontoshowyourappnamesup.Bestregard.
NNGen:Hi<user>,thanksforyourawesomesupportandendorsement.Letus
know if you have suggestions for us to improve the app for you by email
<email>.
NMT:Dearuser,sorryfortheinconveniencecauseforyou.Pleasesendusa
note<url>contactsowecanfollowup.
RRGen:Dear <user>, sorry for your inconvenience. Thank you for your
descriptionandI'llforwardyourfeedbacktoourteam.We'llchecktheproblem
whichyoumentionedassoonaspossible.Pleasekeepusing<app>forthebest
foryou.Thankyouforyoursupport.Bestregard.
(a) First example.
User Review: Nicebrowserwithgreatspeedandsomanyshortcutswhichsave
minetime.Thankyouteamuc.TheonlythingIwantyoutodoisjustaddingread mode. I have to make my note from Internet only and read too much
causingpaininmyeyes,soitsmyhumblerequesttoyou.Plzaddreadmodein
nextupdate.Thankyou.
/g39/g72/g89/g72/g79/g82/g83/g72/g85/g182/g86 Response: Dear friend, we are so happy to get your approval!
Wouldyoupleasegiveus<digit>star!Itmeansalottous.Wearewaitingfor
your<digit>star.Thanksforsupport.
NNGen:Sorryfortheproblem.Weneedyourcooperation.Couldyougiveus
thedetails,thanks!
NMT:Dearfriend,weapologizefortheinconvenienceyoumeet.Wouldyou
pleasetellusmoreinformationabouttheerrorcode?
RRGen:Hidearfriendthanksforsupportingourbrowser!Yoursatisfactionis
whatwebecaremost.Weareveryeagerforyour<digit>starrating.Thanksforsupport.
(b) Second example.
Fig. 8: Two sample review-response pairs where RRGen can generate
responses with more accurate sentiment. The words underlined in red
are indicative of the description sentiments, and the meanings of the
highlighted and double-underlined words are the same as Fig. 6.
ratings involved, NMT also fails to ask the user to increase
his/her given rating.
Observation 3: RRGen can effectively capture knowl-
edge relations between reviews and their correspondingresponses. RRGen learns the correspondence between reviews
and response mainly through the high-dimensional hidden
units and attention layer. The topical words in reviews tendto produce hidden states of semantically similar words inthe RNN decoder. Fig. 9 visualizes the latent alignment overthe user review to help generate the response based on theattention weights Œ±
tjfrom Equ. (4). Each column indicates the
weight distribution over the user review for generating eachword. From this we can see which words in the user reviewwere considered more important when generating the targetword in the response. We can observe the obvious correlationsbetween the word ‚Äú save ‚Äù (in the review) and ‚Äú save ‚Äù (in the
response), ‚Äú hd‚Äù (in the review) and ‚Äú max ‚Äù (in the response),
and ‚Äú pixel ‚Äù (in the review) and ‚Äú image ‚Äù (in the response), as
shown in Fig. 9. This illustrates that RRGen is able to build
implicit relations between the topical words in reviews and
corresponding responses, which can help generate relevant andaccurate response given a review.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. B. Post-Processing Steps
RRGen generates responses with placeholders, e.g.,
‚Äú<email>‚Äù, ‚Äú<url>‚Äù, etc. Moreover, RRGen may not gen-
erate perfect responses and developers may want to ver-ify RRGen responses for some more ‚Äúsensitive‚Äù cases.To partially address the above-mentioned limitations, wepropose several post-processing steps. First, we build aplaceholder-value dictionary for automatically replacing place-holders (e.g., ‚Äú <url>‚Äù) with corresponding values (e.g.,
‚Äúhttps://www.facebook.com/groups/vivavideoapp/‚Äù) for eachapp. Second, we design a quality assurance Ô¨Ålter to automati-cally detect the generated responses that require further check .
The placeholder-value dictionary for each app is saved dur-
ing preprocessing, and for simplicity, only the most commonvalue for each placeholder is saved. We deÔ¨Åne a generated
response requiring further check based on its token length
l, the overlapped keyword ratio œâwith the corresponding
review, and also the review rating r. SpeciÔ¨Åcally, we deÔ¨Åne
responses that satisfy the following constraint, i.e., œâ<0.05
or(l<38andr‚â§2)to require further check. The thresholds
are determined as follows: 0.05 is determined by following the
keyword overlapping threshold in [3], 38 is the Ô¨Årst quartile ofresponse token lengths in the whole dataset, and the constraintfor review rating is set as such as reviews with lower ratings(e.g., 1, 2) tend to express users‚Äô strong dissatisfaction withcertain aspects of apps [38], [59].
We evaluate our solution after the above mentioned post-
processing strategy using a similar experiment setting used to
produce results presented in Section V -A. We Ô¨Ånd that the
BLEU-4 score is 34.63. It is only slightly lower than theBLEU-4 score (36.17) reported in our earlier experiment usingground truths with placeholders rather than actual values.
C. Limitations
Although our proposed RRGen model aims at producing
accurate responses to user reviews, not all the reviews require
responses, some reviews require carefully crafted replies, and
some other reviews can be delegated to an automated bot.We have tried to address this issue partially by adding somepreliminary post-processing steps (see Section VII-B).
Admittedly, our post-processing steps are not perfect.
First, our preliminary post-processing steps may generateresponses with inappropriate values due to the coarsely-deÔ¨Ånedplaceholder-value dictionary. This issue can be improved bycreating a context-sensitive dictionary for each app. Also, oursimple rule-based detection of responses that require furthercheck can be improved further. For this, we can learn thethresholds of the rule conditions or design new detection cri-teria. We leave the design, implementation, and evaluation ofa full-Ô¨Çedged system that can route reviews to do not respond ,
require human careful response , and can be responded by an
automated bot queues for future work. As our work is the Ô¨Årst
to automate app review generation, although it is not perfect,it opens up way for future research to continue our study andimprove it further.D. Threats to V alidity
One of the threats to validity is about the limited number
of studied apps. We studied developer responses for reviewsof free apps only. One of the main reasons for removing non-free apps is that the pricing of an app is likely to impactdevelopers‚Äô response behavior [8]. Also, we only considerGoogle Play apps in this work, because Apple‚Äôs App Storestarted to support review response from 2017 while the featurehas been standard in Google Play since 2013 [60]. Althoughour study is based on apps from various categories and largenumbers of review-response pairs, future work can be extendedto multiple app stores and paid apps.
The second threat to validity is about the component at-
tributes incorporated into our proposed model. Although weinvolve both high-level attributes and keywords, some other
characteristics such as review title length and post date, which
would be helpful for response generation, are not considered.Besides, the review sentiment predicted by SentiStrength [40]might not be reliable [61], and could inÔ¨Çuence the generatedresponse. However, accurate sentiment prediction based on re-views is out of the scope of this paper, and the effectiveness of
StentiStrength in detecting user sentiment about app features
has been demonstrated in [39]. In the future, we will explorethe impact of more review characteristics on automatic reviewresponse generation.
Another threat to validity is about manual inspection in
Section VI. The results of the human evaluation are impactedby the experience of the participants and their intuition of theevaluation metrics. To reduce the errors in the manual analysis,we ensure that each review-response pair was evaluated by Ô¨Åvedifferent participants. As our participants are mainly students,they may not be representative of (CRM) professionals whoare likely to beneÔ¨Åt from our tools in practice [62], [63]. Wetry to mitigate this threat by inviting the students with at least
one year of software development experience. In addition, we
randomly disrupt the order of the three types of responsefor each question, so that the results are not inÔ¨Çuenced byparticipants‚Äô prior knowledge about the response orders.
VIII. R
ELA TED WORK
A. User Review Mining
Identifying the complaint topics expressed by user reviews
is the basis for user review mining [64]‚Äì[66]. Iacob et al. [67]manually label 3,278 reviews, and discover the most recurringissues users report through reviews. To alleviate the laborin manual labeling, many studies focus on automating theprocess. For example, Iacob and Harrison [68] design MARAfor retrieving app feature requests based on linguistic rules.Maalej and Nabil [31] adopt probabilistic techniques to clas-
sify reviews. Di Sorbo et al. [3] separately categorize user
intentions and topics delivered by app reviews. Understandinguser sentiment about speciÔ¨Åc app aspects is another typicaldirection of review mining. Guzman and Maalej [39] usetopic modeling approach and StentiStrength [40] (a lexical
sentiment extraction tool) to predict sentiment of app features.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. User ReviewGenerated Response by RRGen
Fig. 9: A heatmap representing the alignment between the user review (left) and generated response by RRGen (top). The columns represent
the distribution over the user review after generating each word. Each pixel shows the weight Œ±tjof the annotation of the j-th source word
for thet-th target word (see Equ. (4)). A higher attention weight (indicated in darker color) manifests a stronger correlation between the
target word and source word. The red dotted rectangles highlight partial topical words in corresponding descriptions.
Gu and Kim [69] propose SUR-Miner to exploit grammatical
structures for aspect-opinion identiÔ¨Åcation. More research ofmobile review analysis can be found in [70]. Different from
these existing review analysis research, we contribute to facili-tating the bidirectional dialogue between users and developersinstead of analyzing only the feedback from user side.
B. Analysis of User-Developer Dialogues in App Stores
Oh et al. [6] conduct a survey on 100 smartphone users
to understand how developers and users interact. They Ô¨Ånd
that most users (69%) tend to take a passive action such as
uninstalling apps, and the main reason for such behavior isthat these users think that their inquiries (e.g., user reviews)would take long time to be responded or receive no response.McIlroy et al. [7] analyze reviews of 10,000+ free Google
Play apps and Ô¨Ånd that 13.8% of the apps respond to at least
one review. They also observe that users would change theirratings 38.7% of the time following a response. Such positiveimpact of developers‚Äô response is also conÔ¨Årmed by Hassan etal. [8]. Although these studies do highlight the importance ofresponding to user reviews, they do not provide an explicit
method to alleviate the burden in the responding process,
which is the focus of this work.
C. Short Text Dialogue Analysis
Short text dialogue analysis is one popular topic in the Ô¨Åeld
of natural language processing, in which given a messagefrom human, the computer returns a reasonable response tothe message [24], [71]. Short text dialogue can be formalizedas a search or a generation problem. The former formalization
is based on a knowledge base consisting of a large number of
message-response pairs. Information retrieval techniques [26]are generally utilized to select the most suitable response tothe current message from the knowledge base. The majorbottleneck for search-based approaches is the creation of
the knowledge base [72]. Ritter et al. [73] and Vinyals and
Le [74] are the Ô¨Årst to treat generation of conversational dialogas a data-driven statistical machine translation (SMT) [75]
problem. Their results show that the machine translation-based approach works better than one IR approach, vector
space model (VSM) [76], in terms of BLEU score [27].
However, generation-based approaches cannot guarantee thatthe response is a legitimate natural language text. In this work,we propose to integrate app reviews‚Äô unique characteristics foraccurate response generation.
IX. C
ONCLUSION AND FUTURE WORK
Replying to user reviews can help app developers create
a better user experience and improve apps‚Äô ratings. Due tothe large numbers of reviews received for popular apps each
day, automating the review response process is useful for app
developers. In this work, we propose a novel approach namedRRGen by explicitly incorporating review attributes and oc-currences of speciÔ¨Åc keywords into the basic NMT model.Analysis using automated metric and human evaluation shows
that our proposed model outperforms baseline approaches. Infuture, we will conduct evaluation using a larger dataset and
deploy the model with our industry partners.
A
CKNOWLEDGEMENT
The work described in this paper was supported by the
Research Grants Council of the Hong Kong Special Adminis-trative Region, China (No. CUHK 14210717 and No. CUHK14208815 of the General Research Fund), and MicrosoftResearch Asia (2018 Microsoft Research Asia CollaborativeResearch A ward).
R
EFERENCES
[1] ‚ÄúSurvey on user ratings and reviews,‚Äù https://www.apptentive.com/blog/
2015/05/05/app-store-ratings-reviews-guide/.
[2] C. Gao, J. Zeng, M. R. Lyu, and I. King, ‚ÄúOnline app review analysis for
identifying emerging issues,‚Äù in Proceedings of the 40th International
Conference on Software Engineering (ICSE) . ACM, 2018, pp. 48‚Äì58.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. [3] A. Di Sorbo, S. Panichella, C. V . Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, ‚ÄúWhat would users change in myapp? summarizing app reviews for recommending software changes,‚ÄùinProceedings of the 24th SIGSOFT International Symposium on
F oundations of Software Engineering (FSE) . ACM, 2016, pp. 499‚Äì
510.
[4] ‚ÄúRatings, reviews, and responses in app store,‚Äù https://developer.apple.
com/app-store/ratings-and-reviews/.
[5] ‚ÄúView and analyze your app‚Äôs ratings and reviews,‚Äù https://support.
google.com/googleplay/android-developer/answer/138230?hl=en.
[6] J. Oh, D. Kim, U. Lee, J. Lee, and J. Song, ‚ÄúFacilitating developer-user
interactions with mobile app review digests,‚Äù in 2013 ACM SIGCHI
Conference on Human Factors in Computing Systems, CHI ‚Äô13, Paris,France, April 27 - May 2, 2013, Extended Abstracts , 2013, pp. 1809‚Äì
1814.
[7] S. McIlroy, W . Shang, N. Ali, and A. E. Hassan, ‚ÄúIs it worth responding
to reviews? studying the top free apps in google play,‚Äù IEEE Software ,
vol. 34, no. 3, pp. 64‚Äì71, 2017.
[8] S. Hassan, C. Tantithamthavorn, C. Bezemer, and A. E. Hassan, ‚ÄúStudy-
ing the dialogue between users and developers of free apps in the googleplay store,‚Äù Empirical Software Engineering , vol. 23, no. 3, pp. 1275‚Äì
1312, 2018.
[9] M. Nayebi, L. Dicke, R. Ittyipe, C. Carlson, and G. Ruhe, ‚ÄúEssmart way
to manage user requests,‚Äù CoRR , vol. abs/1808.03796, 2018.
[10] D. Wang, N. Jojic, C. Brockett, and E. Nyberg, ‚ÄúSteering output style and
topic in neural response generation,‚Äù in Proceedings of the Conference
on Empirical Methods in Natural Language Processing, EMNLP 2017,Copenhagen, Denmark, September 9-11, 2017 , 2017, pp. 2140‚Äì2150.
[11] J. Li and X. Sun, ‚ÄúA syntactically constrained bidirectional-
asynchronous approach for emotional conversation generation,‚Äù in Pro-
ceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October 31 - November 4,2018 , 2018, pp. 678‚Äì683.
[12] A. Sordoni, M. Galley, M. Auli, C. Brockett, Y . Ji, M. Mitchell,
J. Nie, J. Gao, and B. Dolan, ‚ÄúA neural network approach to context-sensitive generation of conversational responses,‚Äù in NAACL HLT 2015,
The Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Denver ,Colorado, USA, May 31 - June 5, 2015 , pp. 196‚Äì205.
[13] L. Zhou, J. Gao, D. Li, and H. Shum, ‚ÄúThe design and implementation
of xiaoice, an empathetic social chatbot,‚Äù CoRR , vol. abs/1812.08989,
2018.
[14] K. Cho, B. van Merrienboer, C ¬∏. G ¬®ulc¬∏ehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y . Bengio, ‚ÄúLearning phrase representations usingRNN encoder-decoder for statistical machine translation,‚Äù in Proceed-
ings of the 2014 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar , A meeting
of SIGDAT, a Special Interest Group of the ACL , 2014, pp. 1724‚Äì1734.
[15] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to sequence learning
with neural networks,‚Äù CoRR , vol. abs/1409.3215, 2014.
[16] X. Gu, H. Zhang, D. Zhang, and S. Kim, ‚ÄúDeep API learning,‚Äù in
Proceedings of the 24th ACM SIGSOFT International Symposium onF oundations of Software Engineering, FSE 2016, Seattle, WA, USA,November 13-18, 2016 , 2016, pp. 631‚Äì642.
[17] L. Dong and M. Lapata, ‚ÄúLanguage to logical form with neural atten-
tion,‚Äù in Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin,Germany, V olume 1: Long Papers , 2016.
[18] S. Jiang and C. McMillan, ‚ÄúTowards automatic generation of short sum-
maries of commits,‚Äù in Proceedings of the 25th International Conference
on Program Comprehension, ICPC 2017, Buenos Aires, Argentina, May22-23, 2017 , 2017, pp. 320‚Äì323.
[19] S. Jiang, A. Armaly, and C. McMillan, ‚ÄúAutomatically generating
commit messages from diffs using neural machine translation,‚Äù in Pro-
ceedings of the 32nd IEEE/ACM International Conference on AutomatedSoftware Engineering, ASE 2017, Urbana, IL, USA, October 30 -November 03, 2017 , 2017, pp. 135‚Äì146.
[20] V . J. Hellendoorn, C. Bird, E. T. Barr, and M. Allamanis, ‚ÄúDeep learning
type inference,‚Äù in Proceedings of the 2018 ACM Joint Meeting on
European Software Engineering Conference and Symposium on theF oundations of Software Engineering, ESEC/SIGSOFT FSE 2018, Lake
Buena Vista, FL, USA, November 04-09, 2018 ,2018, pp. 152‚Äì162.
[21] M. R. Islam, ‚ÄúNumeric rating of apps on google play store by senti-
ment analysis on user reviews,‚Äù in 2014 International Conference onElectrical Engineering and Information & Communication Technology .
IEEE, 2014, pp. 1‚Äì4.
[22] K. Sharma and K. Lin, ‚ÄúReview spam detector with rating consistency
check,‚Äù in ACM Southeast Regional Conference 2013, ACM SE‚Äô13,
Savannah, GA, USA, April 4-6, 2013
, 2013, pp. 34:1‚Äì34:6.
[23] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, ‚ÄúNeural-
machine-translation-based commit message generation: how far arewe?‚Äù in Proceedings of the 33rd ACM/IEEE International Conference
on Automated Software Engineering, ASE 2018, Montpellier , France,September 3-7, 2018 , 2018, pp. 373‚Äì384.
[24] Z. Ji, Z. Lu, and H. Li, ‚ÄúAn information retrieval approach to short text
conversation,‚Äù CoRR , vol. abs/1408.6988, 2014.
[25] Y . Song, C. Li, J. Nie, M. Zhang, D. Zhao, and R. Y an, ‚ÄúAn ensemble
of retrieval-based and generation-based human-computer conversationsystems,‚Äù in Proceedings of the Twenty-Seventh International Joint
Conference on ArtiÔ¨Åcial Intelligence, IJCAI 2018, July 13-19, 2018,Stockholm, Sweden. , 2018, pp. 4382‚Äì4388.
[26] C. D. Manning, P . Raghavan, and H. Sch ¬®utze, Introduction to informa-
tion retrieval . Cambridge University Press, 2008.
[27] K. Papineni, S. Roukos, T. Ward, and W . Zhu, ‚ÄúBleu: a method for
automatic evaluation of machine translation,‚Äù in Proceedings of the 40th
Annual Meeting of the Association for Computational Linguistics, July6-12, 2002, Philadelphia, PA, USA. , 2002, pp. 311‚Äì318.
[28] D. Bahdanau, K. Cho, and Y . Bengio, ‚ÄúNeural machine translation by
jointly learning to align and translate,‚Äù CoRR , vol. abs/1409.0473, 2014.
[29] R. Collobert and S. Bengio, ‚ÄúLinks between perceptrons, mlps and
svms,‚Äù in Machine Learning, Proceedings of the Twenty-Ô¨Årst Interna-
tional Conference (ICML 2004), Banff, Alberta, Canada, July 4-8, 2004 ,
2004.
[30] T. Luong, H. Pham, and C. D. Manning, ‚ÄúEffective approaches to
attention-based neural machine translation,‚Äù in Proceedings of the 2015
Conference on Empirical Methods in Natural Language Processing,EMNLP 2015, Lisbon, Portugal, September 17-21, 2015 , 2015, pp.
1412‚Äì1421.
[31] W . Maalej and H. Nabil, ‚ÄúBug report, feature request, or simply praise?
on automatically classifying app reviews,‚Äù in 23rd IEEE International
Requirements Engineering Conference, RE, Ottawa, ON, Canada, Au-gust 24-28, 2015 , 2015, pp. 116‚Äì125.
[32] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to sequence learning
with neural networks,‚Äù in Advances in Neural Information Processing
Systems 27: Annual Conference on Neural Information Processing
Systems 2014, December 8-13 2014, Montreal, Quebec, Canada , 2014,
pp. 3104‚Äì3112.
[33] T. Mikolov, M. KaraÔ¨Å ¬¥at, L. Burget, J. Cernock ¬¥y, and S. Khudanpur,
‚ÄúRecurrent neural network based language model,‚Äù in INTERSPEECH
2010, 11th Annual Conference of the International Speech Communi-cation Association, Makuhari, Chiba, Japan, September 26-30, 2010 ,
2010, pp. 1045‚Äì1048.
[34] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
‚ÄúDistributed representations of words and phrases and their composi-tionality,‚Äù in Advances in Neural Information Processing Systems 26:
27th Annual Conference on Neural Information Processing Systems2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe,Nevada, United States. , 2013, pp. 3111‚Äì3119.
[35] A. M. Rush, S. Chopra, and J. Weston, ‚ÄúA neural attention model for
abstractive sentence summarization,‚Äù in Proceedings of the Conference
on Empirical Methods in Natural Language Processing, EMNLP 2015,Lisbon, Portugal, September 17-21, 2015 , 2015, pp. 379‚Äì389.
[36] Z. Lin, M. Feng, C. N. dos Santos, M. Y u, B. Xiang, B. Zhou, and
Y . Bengio, ‚ÄúA structured self-attentive sentence embedding,‚Äù CoRR , vol.
abs/1703.03130, 2017.
[37] J. Zeng, J. Li, Y . Song, C. Gao, M. R. Lyu, and I. King, ‚ÄúTopic memory
networks for short text classiÔ¨Åcation,‚Äù in Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing,Brussels, Belgium, October 31 - November 4, 2018 , pp. 3120‚Äì3131.
[38] N. Chen, J. Lin, S. C. Hoi, X. Xiao, and B. Zhang, ‚ÄúAr-miner: mining
informative reviews for developers from mobile app marketplace,‚Äù inProceedings of the 36th International Conference on Software Engi-
neering (ICSE) . ACM, 2014, pp. 767‚Äì778.
[39] E. Guzman and W . Maalej, ‚ÄúHow do users like this feature? a Ô¨Åne
grained sentiment analysis of app reviews,‚Äù in Proceedings of the 22nd
International Conference on Requirements Engineering (RE) . IEEE,
2014, pp. 153‚Äì162.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. [40] M. Thelwall, K. Buckley, G. Paltoglou, D. Cai, and A. Kappas, ‚ÄúSenti-
ment in short strength detection informal text,‚Äù JASIST , vol. 61, no. 12,
pp. 2544‚Äì2558, 2010.
[41] D. J. Montana and L. Davis, ‚ÄúTraining feedforward neural networks
using genetic algorithms,‚Äù in Proceedings of the 11th International Joint
Conference on ArtiÔ¨Åcial Intelligence. Detroit, MI, USA, August 1989 ,
1989, pp. 762‚Äì767.
[42] G. A. Miller, ‚ÄúWordnet: A lexical database for english,‚Äù Commun. ACM ,
vol. 38, no. 11, pp. 39‚Äì41, 1995.
[43] J. Chung, C ¬∏. G ¬®ulc¬∏ehre, K. Cho, and Y . Bengio, ‚ÄúEmpirical evaluation
of gated recurrent neural networks on sequence modeling,‚Äù CoRR , vol.
abs/1412.3555, 2014.
[44] Z. Wu and S. King, ‚ÄúInvestigating gated recurrent networks for speech
synthesis,‚Äù in IEEE International Conference on Acoustics, Speech and
Signal Processing, ICASSP 2016, Shanghai, China, March 20-25, 2016 ,
2016, pp. 5140‚Äì5144.
[45] ‚ÄúGlove: Global vectors for word representation,‚Äù https://nlp.stanford.edu/
projects/glove/.
[46] D. P . Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
in3rd International Conference on Learning Representations, ICLR, San
Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.
[47] ‚ÄúPytorch,‚Äù https://pytorch.org/.[48] M. Harman, Y . Jia, and Y . Zhang, ‚ÄúApp store mining and analysis: MSR
for app stores,‚Äù in 9th IEEE Working Conference of Mining Software
Repositories, MSR, June 2-3, 2012, Zurich, Switzerland , 2012, pp. 108‚Äì
111.
[49] ‚ÄúApp annie,‚Äù https://www.appannie.com/.[50] Y . Man, C. Gao, M. R. Lyu, and J. Jiang, ‚ÄúExperience report: Under-
standing cross-platform app issues from user reviews,‚Äù in 27th IEEE
International Symposium on Software Reliability Engineering, ISSRE2016, Ottawa, ON, Canada, October 23-27, 2016 , 2016, pp. 138‚Äì149.
[51] P . M. Vu, T. T. Nguyen, H. V . Pham, and T. T. Nguyen, ‚ÄúMining
user opinions in mobile app reviews: A keyword-based approach (T),‚Äùin30th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015 , 2015,
pp. 749‚Äì759.
[52] C. Liu, R. Lowe, I. Serban, M. Noseworthy, L. Charlin, and J. Pineau,
‚ÄúHow NOT to evaluate your dialogue system: An empirical study ofunsupervised evaluation metrics for dialogue response generation,‚Äù inProceedings of the 2016 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2016, Austin, Texas, USA, November 1-4,2016 , 2016, pp. 2122‚Äì2132.
[53] P . Yin and G. Neubig, ‚ÄúA syntactic neural model for general-purpose
code generation,‚Äù in Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2017, V ancouver , Canada,July 30 - August 4, V olume 1: Long Papers , 2017, pp. 440‚Äì450.
[54] S. Liu, H. Chen, Z. Ren, Y . Feng, Q. Liu, and D. Yin, ‚ÄúKnowledge
diffusion for neural dialogue generation,‚Äù in Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics, ACL2018, Melbourne, Australia, July 15-20, 2018, V olume 1: Long Papers ,
2018, pp. 1489‚Äì1498.
[55] F. Wilcoxon, ‚ÄúIndividual comparisons by ranking methods,‚Äù Biometrics
bulletin , vol. 1, no. 6, pp. 80‚Äì83, 1945.
[56] S. E. Ahmed, ‚ÄúEffect sizes for research: A broad application approach,‚Äù
Technometrics , vol. 48, no. 4, p. 573, 2006.
[57] X. Du and C. Cardie, ‚ÄúHarvesting paragraph-level question-answer
pairs from wikipedia,‚Äù in Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics, ACL 2018, Melbourne,Australia, July 15-20, 2018, V olume 1: Long Papers , 2018, pp. 1907‚Äì
1917.
[58] P . S. Kochhar, X. Xia, D. Lo, and S. Li, ‚ÄúPractitioners‚Äô expectations on
automated fault localization,‚Äù in Proceedings of the 25th International
Symposium on Software Testing and Analysis, ISSTA 2016, Saarbr ¬®ucken,
Germany, July 18-20, 2016 , 2016, pp. 165‚Äì176.
[59] C. Gao, H. Xu, J. Hu, and Y . Zhou, ‚ÄúAr-tracker: Track the dynamics
of mobile apps via user review mining,‚Äù in 2015 IEEE Symposium on
Service-Oriented System Engineering, SOSE 2015, San Francisco Bay,
CA, USA, March 30 - April 3, 2015 ,2015, pp. 284‚Äì290.
[61] N. Novielli, D. Girardi, and F. Lanubile, ‚ÄúA benchmark study on
sentiment analysis for software engineering research,‚Äù in Proceedings[60] ‚ÄúDevelopers can Ô¨Ånally respond to app
store reviews,‚Äù https://techcrunch.com/2017/03/28/
developers-can-Ô¨Ånally-respond-to-app-store-reviews-heres-how-it-works/.
of the 15th International Conference on Mining Software Repositories,
MSR 2018, Gothenburg, Sweden, May 28-29, 2018 , 2018, pp. 364‚Äì375.
[62] I. Salman, A. T. Misirli, and N. J. Juzgado, ‚ÄúAre students represen-
tatives of professionals in software engineering experiments?‚Äù in 37th
IEEE/ACM International Conference on Software Engineering, ICSE
2015, Florence, Italy, May 16-24, 2015, V olume 1 , 2015, pp. 666‚Äì676.
[63] R. Feldt, T. Zimmermann, G. R. Bergersen, D. Falessi, A. Jedlitschka,
N. Juristo, J. M ¬®unch, M. Oivo, P . Runeson, M. J. Shepperd, D. I. K.
Sj√∏berg, and B. Turhan, ‚ÄúFour commentaries on the use of students andprofessionals in empirical software engineering experiments,‚Äù Empirical
Software Engineering , vol. 23, no. 6, pp. 3801‚Äì3820, 2018.
[64] F. Palomba, P . Salza, A. Ciurumelea, S. Panichella, H. Gall, F. Ferrucci,
and A. D. Lucia, ‚ÄúRecommending and localizing change requests formobile apps based on user reviews,‚Äù in IEEE/ACM 39th International
Conference on Software Engineering (ICSE‚Äô17) , 2017, pp. 106‚Äì117.
[65] G. Grano, A. Ciurumelea, S. Panichella, F. Palomba, and H. C. Gall,
‚ÄúExploring the integration of user feedback in automated testing of an-droid applications,‚Äù in IEEE 25th International Conference on Software
Analysis, Evolution and Reengineering (SANER‚Äô18) , 2018, pp. 72‚Äì83.
[66] C. Gao, W . Zheng, Y . Deng, D. Lo, J. Zeng, M. R. Lyu, and I. King,
‚ÄúEmerging app issue identiÔ¨Åcation from user feedback: experienceon wechat,‚Äù in Proceedings of the 41st International Conference on
Software Engineering: Software Engineering in Practice, ICSE (SEIP),
Montreal, QC, Canada, May 25-31 , 2019, pp. 279‚Äì288.
[67] C. Iacob, V . V eerappa, and R. Harrison, ‚ÄúWhat are you complaining
about?: a study of online reviews of mobile applications,‚Äù in BCS-
HCI ‚Äô13 Proceedings of the 27th International BCS Human ComputerInteraction Conference, Brunel University, London, UK, 9-13 September2013 , 2013, p. 29.
[68] C. Iacob and R. Harrison, ‚ÄúRetrieving and analyzing mobile apps feature
requests from online reviews,‚Äù in Proceedings of the 10th Working
Conference on Mining Software Repositories, MSR ‚Äô13, San Francisco,CA, USA, May 18-19, 2013 , 2013, pp. 41‚Äì44.
[69] X. Gu and S. Kim, ‚Äú‚Äùwhat parts of your apps are loved by users?‚Äù (T),‚Äù
in30th IEEE/ACM International Conference on Automated Software
Engineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015 , 2015,
pp. 760‚Äì770.
[70] W . Martin, F. Sarro, Y . Jia, Y . Zhang, and M. Harman, ‚ÄúA survey of app
store analysis for software engineering,‚Äù IEEE Trans. Software Eng. ,
vol. 43, no. 9, pp. 817‚Äì847, 2017.
[71] J. Zeng, J. Li, Y . He, C. Gao, M. R. Lyu, and I. King, ‚ÄúWhat you say
and how you say it: Joint modeling of topics and discourse in microblogconversations,‚Äù TACL , vol. 7, pp. 267‚Äì281, 2019.
[72] G. Chen, E. Tosch, R. Artstein, A. Leuski, and D. R. Traum, ‚ÄúEvalu-
ating conversational characters created through question generation,‚Äù inProceedings of the Twenty-F ourth International Florida ArtiÔ¨Åcial Intel-ligence Research Society Conference, May 18-20, 2011, Palm Beach,Florida, USA , 2011.
[73] A. Ritter, C. Cherry, and W . B. Dolan, ‚ÄúData-driven response generation
in social media,‚Äù in Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2011, 27-31 July2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting ofSIGDAT, a Special Interest Group of the ACL , 2011, pp. 583‚Äì593.
[74] O. Vinyals and Q. V . Le, ‚ÄúA neural conversational model,‚Äù CoRR , vol.
abs/1506.05869, 2015. [Online]. Available: http://arxiv.org/abs/1506.05869
[75] P . Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico,
N. Bertoldi, B. Cowan, W . Shen, C. Moran, R. Zens, C. Dyer, O. Bojar,
A. Constantin, and E. Herbst, ‚ÄúMoses: Open source toolkit for statisticalmachine translation,‚Äù in ACL 2007, Proceedings of the 45th Annual
Meeting of the Association for Computational Linguistics, June 23-30,2007, Prague, Czech Republic , 2007.
[76] G. Salton, A. Wong, and C. Y ang, ‚ÄúA vector space model for automatic
indexing,‚Äù Commun. ACM , vol. 18, no. 11, pp. 613‚Äì620, 1975.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:48:24 UTC from IEEE Xplore.  Restrictions apply. 