Inferring and Asserting Distributed System Invariants
Stewart Grant
University of British Columbia
Vancouver, BC, Canada
stewbertgrant@gmail.comHendrik Cech
University of Bamberg
Bamberg, Germany
hendrik.cech@gmail.comIvan Beschastnikh
University of British Columbia
Vancouver, BC, Canada
bestchai@cs.ubc.ca
ABSTRACT
Distributedsystemsaredifficulttodebugandunderstand.Akey
reason for this is distributed state, which is not easily accessible
andmustbepiecedtogetherfromthestatesoftheindividualnodes
in the system.
We proposeDinv, anautomatic approachto helpdevelopers of
distributedsystemsuncovertheruntimedistributedstatepropertiesoftheirsystems.Dinvusesstaticanddynamicprogramanalysestoinferrelationsbetweenvariablesatdifferentnodes.Forexample,inaleaderelectionalgorithm,Dinvcanrelatethevariable
leaderatdif-
ferent nodes to derive the invariant ∀nodesi,j,leaderi=leaderj.
This can increase the developer’s confidence in the correctness of
theirsystem.ThedevelopercanalsouseDinvtoconvertaninferred
invariant into a distributed runtime assertion on distributed state.
We applied Dinv to several popular distributed systems, such as
etcd Raft, Hashicorp Serf, and Taipei-Torrent, which have between
1.7K and 144K LOC and are widely used. Dinv derived useful in-
variants for these systems, including invariants that capture the
correctness of distributed routing strategies, leadership, and key
hashdistribution.WealsousedDinvtoassertcorrectnessofthein-
ferred etcd Raft invariants at runtime, using these asserts to detect
injected silent bugs.
1 INTRODUCTION
Developing correct distributed systems remains a formidable chal-
lenge[7,58].Onereasonforthisisthatdeveloperslackthetools
to help them extract and reason about the distributed state of their
systems [ 44,49]. The state of a sequential program is well defined
(stack and heap), easy to inspect (with breakpoints) and can be
checked for correctness with assertions. However, the state of a
distributed execution is resident across multiple nodes and it is un-
clearhowtobestcomposethesenodestatesintoacoherentpicture,
let alone check these distributed node states for correctness.
In this work we propose a program analysis tool-chain called
Dinvfor inferring likely data properties, or invariants, between
variablesatdifferentnodesinadistributedsystem,andforchecking
these distributed invariants at runtime.
Dinv-inferredinvariantshelpdevelopersreasonaboutthedis-
tributedstateof theirsystemsinvarious ways.Inparticular, they
can confirm expected relationships between variables separated
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180199by a network to improve developer confidence in their system’s
correctness.
Forexample,consideratwo-phasecommitprotocol[ 4]inwhich
the coordinator first queries other nodes for their vote and if all
nodes, including the coordinator, voted Commit then the coordina-
tor broadcasts a TX Commit, otherwise it broadcasts a TX Abort.
At the end of this protocol all nodes should either commit or abort.
To check if several non-faulty runs of the system are correct, a
developercanexaminetheDinv-inferreddistributedstateinvari-
ants for this set of executions. In this case they can check whether
Dinv mined the invariant coordinator.commit=replicai.commit
for each replica iin the system. This would mean that the commit
stateacrossallnodeswasidenticalatconsistentsnapshotsofthe
system.TheycanalsouseDinvtoaddaruntimeassertiontocheck
this invariant in future runs.
Dinvisthefirst automated end-to-endtooltoinferdistributed
system state invariants. The closest prior work by Yabandeh et
al. [56] requires developers to manually identify variables to log,
instrument their systems, identify distributed cuts, and so on. Dinv
automates the entire process and requires minimal input from the
developer.Dinvisalsocomplementarywithmanyexistingtoolsfor
checking distributed systems like Modist [ 57] and D3S [ 35]. These
tools expect the developer to manually specify properties to check;
Dinv can make these tools easier to use. Finally, Dinv includes a
light-weight and probabilistic assertion mechanism that can detect
invariant violations with low, controllable, overhead.
Dinvworksbyfirststaticallyinstrumentingthesystem’scode,
either automatically or with user-supplied annotations. Dinv uses
staticprogramslicingtocapturethosevariablesthataffectorare
affected by network communication at eachnode. During system
execution, Dinv instrumentation tracks these variables, collects
their concrete runtime values, tags them with a vector timestamp,
andlogsthevaluesateachnode.Oncethedeveloperhasdecided
thatthesystemhasrunlongenough(e.g.,executionofatestsuite),theyrunDinvonthegeneratedlogs.Dinvusesvectortimestampsin
thelogstocomposedistributedstates,andthenmergesthesestates
using three novel strategies into a series of system snapshots. DinvthenusesaversionoftheDaikontool[
19]toinferlikelydistributed
stateinvariantsoverthetrackedvariablesinthemergedsnapshots.
Our approach with Dinv is pragmatic: it does not require the
developer to formally specify their system and it scales to large
productionsystemsandlongexecutions.AlthoughDinvusesdy-
namic analysis, which is incomplete (Dinv cannot reason about
executions it does not observe), we believe that it is useful because
(1)mostdistributedsystemsdeveloperstodayusedynamicanalysis
tochecktheirsystems(e.g.,withtesting)and(2)wehavebeenable
to use Dinv to infer and assert useful properties in several large
systems.
11492018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Stewart Grant, Hendrik Cech, and Ivan Beschastnikh
Instrumentation Network usage 
detectorVector clock injection
System execution Mining distributed stateDaikonInput
Go code
Detecting invariantsSystem execution Global state extraction Global state groupingDetected
InvariantsRuntime invariant assertions
Figure 1: Overview of the invariant inference steps in Dinv.
We evaluated Dinv by using it to study four systems written
in Go: Coreos’s etcd [ 14], Taipei-Torrent [ 27], Groupcache [ 20],
andHashicorpSerf[ 25].Wetargeteddifferentsafetyandliveness
properties in these large systems and ran Dinv on a variety of
executions of each system. For example, etcd uses the Raft con-sensus algorithm [
42] and we checked that the Raft executions
weinducedsatisfythestrongleaderprinciple,logmatching,and
leader agreement. We used Dinv over several iterations to infer
distributedstateinvariantsthatconfirmedthattheexecutionswe
studied satisfy each of the targeted properties. We also used Dinv’s
assertionmechanismtocatchbugsinjectedintoRaftthatsilently
violate three key Raft invariants. We evaluated Dinv’s overhead
and found that it can instrument etcd Raft in a few seconds and
that 5 logging annotations in a Raft cluster of 6 nodes induced a
13% system slowdown and used 1KB/s of extra bandwidth.
To summarize, this paper makes the following contributions:
⋆We propose a static analysis technique for automatically detect-
ing variables which comprise distributed state.
⋆Weproposeahierarchyofthreestrategiesforgroupingglobal
system states (snapshots) for invariant inference and show that
eachstrategyisusefulfordetectingdifferenttypesofdistributed
invariants.
⋆Wedescribearuntimedistributedinvariantcheckingmechanism
based onreal-time snapshots.We evaluateits efficacyby using
it to find injected silent bugs in etcd Raft.
⋆We implemented the above techniques in Dinv, an open source
tool[17],andevaluateitonavarietyofcomplexandwidelyused
Go systems that have between 1.7K and 144K LOC.
2 DISTRIBUTED STATE BACKGROUND
In this section we overview our model of distributed state and how
it can be observed from the partially ordered logs of an execution.
Weconsiderasystemcomposedofanumberof nodes,eachof
which has an independent thread of execution. During a node’s
execution, each instruction is an eventand anevent instance refers
to a specific event(wesometimes use eventfor both). The stateof a
nodeataneventinstanceisthesetofvaluesforallthevariables
resident in the nodes memory. The state of a node can be recorded
by writing the variable values to a log. Event instances on a single
node are totally ordered.
In this paper we consider message passing systems in which
sending and receiving events create a partial ordering of event
instances across nodes. Dinv uses vector clocks to establish this
happened-before ordering [ 39]. Using a log of vector clocks, causal
chains of events from an execution can be analyzed. We use log
to refer to the sequence of node states paired with vector clocksgenerated in a single execution of the system. Section 3.1 discusses
howDinvautomaticallyinstrumentssystemstowritestates,and
vector clocks to a log.
MostofDinv’sanalysesrunonalogproducedbyasystemafterit
has executed. Detecting meaningful distributed invariants requires
determiningvalidcombinationsofconcretenodestatestousefor
inference. For a given log, a consistent cut is a set of events such
that if an event ehappened before event f(according to vector
clocks), then if fis in the cut, eis also part of the cut. Local states
on the frontier of a cut form a global state. The complete set of
global states which occur during the execution of a system forma lattice. A point in this lattice is an n-tuple of local node states
composing a single global state. A lattice edge connects two global
states,д→h,i fдhappened before h(again, according to vector
clocks)andthevectorclocktimestampsof дandhareseparatedby
asingleincrementinlogicaltime.A groundstate isaglobalstatein
whichallmessagessentupuntilthatpointhavealsobeenreceived
(i.e.,nomessagesareinflight)[ 1].Sections3.2and3.3coverDinv’s
global state analysis. Next, we detail Dinv’s design.
3 DINV DESIGN
Automatically inferring distributed invariants requires resolving
three research challenges:
(1) What state should be logged and when?(2) How to infer distributed invariants from logged state?(3) How to enforce inferred distributed invariants?
Dinv’s analyses are a step by step procedure for solving these
challenges (Figure 1). This section details each step of the analysis.
3.1 System instrumentation
Challenge 1 :What state should be logged and when? Determining
state to log is difficult because state with interesting distributed
propertiesishardtoidentify.Forexample,somestateislocaltothe
node and is unaffected by other nodes in the system; distributed
invariantsoversuchstateareuninteresting.Weproposeandusethe
following heuristic: interesting distributed state must have dataflow
to or from the network.
Variables which interact with the network can be detected stati-
callyusingprogramslicing[ 43,52].Forwardslicesrootedatnet-
workreads,andbackwardslicesfromnetworkwritesidentifythe
affectedandaffectingvariables,respectively.Wedevelopedaninter-
procedural slicing library for Go, and use Go’s networking con-
ventions to statically identify network reads and writes. Variables
contained in slices rooted at network calls comprise the set of net-
work interacting variables. Figure 2 lists partial code from Serf [ 25]
that implements the SWIM protocol [ 16]. We will use this example
throughout the paper. Logging point L1 on line 12 logs variables
1150
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Inferring and Asserting Distributed System Invariants ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Instrumentation strategy Location choice Variables choice
Function entrances/exits Auto Auto
Network calls Auto Auto
User-defined annotations Manual Manual or Auto
Table 1: Instrumentation strategies and the control (auto-
matic/manual) offered by each strategy for selecting state
logging location and the set of logged variables.
transitivelyaffectedbythenetworkreadonline3.Affectedvari-
ables are underlined in the listing.
When should state be logged? Network interacting variables may
be used throughout a system’s codebase. Invariant inference de-
pends crucially onwhere inthe codethe valuesof thesevariables
are logged — logging at different points may produce wildly differ-
entinvariants.Forexample,toinferthemutualexclusioninvariants
variablesmustbeloggedinsideacriticalsection;ifnot,thenthecap-
tured state would not reflect that the node ever executed a critical
section, a critical omission!
Dinv provides developers with three mechanisms to control
wheretologstate(Table1).Twooftheseautomatethechoiceof
locations (function entrances/exits or network calls) and choice of
state(allnetworkinteractingvariables).Thethirdstrategyprovides
the developer with fine-grained control over where and what state
to log.
Figure2illustratestwologgingannotations: L2a//@dump an-
notation(line6)and L1aparameterizedDump statement(line12).
The first logs distributed state when a Pingis received, the second
logs state before checking for timeouts.
Trackingpartialorder. Vectorclocksareacanonicalmeansfor
recording the happens-before relation in a distributed system [ 39].
Dinv automatically instruments any Go program that uses Go’s
networking netlibrary with vector clocks. It does this by detecting
usesofthe netlibraryandbymutatingtheabstractsyntaxtree.Dinv
supportscommonprotocolslikeIP,UDP,TCP,RPC,andIPC.Vector
clocksareappendedor stripped fromnetworkpayloads, andthe
original function isexecuted on the instrumentedarguments. For
example,anetworkwritelike conn.Write(buffer) istransformed
into dinv.Write(conn.Write,buffer).
Insummary,oursolutiontochallenge1istologonlythevari-
ables that interact with the network, at automatically generated,and user specified lines of code. Next, we explain how the logs
generated by the execution of instrumented nodes are analyzed to
infer distributed invariants.
3.2 Extracting global states
Challenge 2 :How to infer distributed invariants from logged state?
Anomnipotentobservercanestablishatotalorderonalldistributed
events. In practice, an ordering that is feasible to derive is the
happens-before relation,definedbythecausalprecedenceofsent,
and received messages. The happens-before is a partial ordering
on events. We use latticeto refer to this ordering.
Eachpointinthelatticeisaconsistentcut(definedinSection2)of
theloggedexecution,andtheentirelatticeisthesetofallconsistent
cuts [3,12]. Figure 3 relates a message sequence diagram, to its1  func (s serfNode) serf(conn UDPConnection) {
2   for true {
3     msg := conn.Read()
4     switch msg.Type {
5     case  PING:
7       conn.WriteToUDP("ACK", msg.Sender)
8       break
9     case  GOSSIP:
10      s.Events = append( s.Events, msg.Event)
11    }
13    timeout := s.CheckForTimeouts()
14    switch timeout.Type {
15    case  PING:
16      conn.WriteToUDP("PING",timeout.Node)17      break18    case  GOSSIP:
19      gossip(
s.Events)
20      break21  } } }6       //@dump
12    dinv.Dump("L1",msg.Type,msg.Sender,msg.Event,s.Events)L2
L1
Figure 2: Code excerpt from Serf with underlined network
interacting variables contained in the forward slice from
conn.Read ()on line 3. Line 6 (L2 ) and line 12 (L1) are exam-
ple instrumenting annotations.
Node 1 Node 0
Ping
Ack[0,1]
[1,2]
[1,3]
[1,4][2,0][1,0]
[3,0]
[4,3][0,0]
[0,1] [1,0]
[1,1] [2,0]
[2,1] [3,0] [1,2]
[2,2] [3,1] [1,3]
[2,3] [3,2] [1,4]
[3,3]
[4,3]
(b) (a)0 +1
+1
-1
+10
0 0……
…[0,2]
Figure 3: (a) Message-sequence diagram of an execution of
code in Figure 2 with two nodes, their local vector clocks
paired with each event (in brackets), and message deltascomputed for each event (-1,0,+1). (b) (Partial) lattice of theexecution. Each vertex displays the corresponding vectortime and ground states are bolded.
corresponding lattice for an execution of the code in Figure 2 with
two nodes. A lattice point, is a global state composed of an n-tuple
of local states. Global invariants, or invariants that hold across
multiple nodes, are testable by extracting the corresponding global
state from a log, and asserting the invariant on that state.
Due to its generality, this lattice analysis incurs significant com-
plexity. In the worst case, an execution without messages with n
nodes and eevents will produce a lattice of size en.
Becausetestinginvariantsonanexponentialnumberofstatesis
infeasible, we propose ground states [ 1] as a heuristic for reducing
thenumberoflatticepointstotestforinvariants.A groundstate isa
consistent cut with the additional constraint that all sent messages
1151
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Stewart Grant, Hendrik Cech, and Ivan Beschastnikh
Node 0 Node 1
L1
AckNode 2 Node 3
Timeout Ping
L2
L1
L1L1
Timeout
L1Gossip
TimeoutGossip
L1Global
State 1
GlobalState 2
GlobalState 3
Figure 4: Execution of SWIM code from Figure 2. Node 1 re-
sponds to a Pingfrom Node 0. Concurrently Node 2 propa-
gates Node 3’s Gossip message. L1&L2mark local logged
state (messages & events in Figure 2). Dashed lines markthreeglobalstates(whicharealsogroundstates),eachann-tuple of the closest local node states above the dashed line.
have been received (i.e., no messages are in flight). Intuitively a
groundstate isaline throughamessage sequencediagramwhichdoes not cross any messages (see Global States in Figure 4).
Analyzing just the ground states reduces completeness: a global
statewhichisnotagroundstatemayviolateaninvariant.However,
distributed algorithms are typically specified with invariants over
ground states, when the system has acquiesced and all messages
havebeenprocessed.Aswell,inferenceongroundstates reduces
analysistimebyordersofmagnitude,andprovidesaqualitysample
of global states in real systems1.
A variety of lattice construction algorithms exist [ 21]. Long-
runningexecutionsoflooselycommunicatingsystemscaneasily
generate lattices larger than main memory. To avoid this, Dinv
utilizesasequentialantichainalgorithmwhichgeneratesalattice
levelbylevel.Generatinglevel nofalatticerequiresalogandlevel
n−1 of the lattice. This allows Dinv to flush most of the lattice to
disk as just two levels must be maintained in memory.
Ground states are computable with a linear scan of both a log
andlattice.First,alogisscannedanda deltaof (sent−received )
number of messages is calculated per local event on each node.For example, if by some event
ea node had sent3 messages, and
received 1 message, e’s delta is +2. The lattice is then scanned, and
for each global state the deltas of each local event in a global state
are summed. A sum of 0 identifies a ground state (e.g., Figure 3).
Lost messages pose a theoretical threat to ground state analysis:
a single message loss rules out future ground states. In practice,
lost messages do not affect receiver’s state and are functionally
equivalenttolocaleventsatthesender.Dinvhandlesexecutions
withlostmessagesbydetectinglostmessagesusingvectorclock
timestamps and omitting them from the ground state computation.
Thefirstcomponentofoursolutiontothechallengeofinferring
distributed invariants from logged state is to infer invariants over
1ForcompletenessDinvallowsuserstoanalyzeallglobalstatesatthecostsubstantial
runtime. All results in this paper are based on ground state analysis.AS: [N0.L1, N1.L1, N2.L1, N3.L1]
SR: [N1.L1, N2.L1], [N2.L1, N3.L1]TO: [N1.L1, N2.L1, N3.L1]Node 0 Node 1
L1
Ping
L2Node 2 Node 3
L1GossipL1Global State 1
AS: [N0.L1, N1.L2, N2.L1, N3.L1]SR: [N0.L1, N1.L2], [N2.L1, N3.L1]TO: [N0.L1, N1.L2], [N2.L1, N3.L1]
Node 0 Node 1 Node 2 Node 3
L1GossipL1Global State 2
SR: [N0.L1,N1.L1], [N2.L1,N3.L1]TO: [N0.L1, N1.L1], [N2.L1, N3.L1]
L1AckL1
Node 0 Node 1 Node 2 Node 3
L1Gossip L1Global State 3
L1
L1GossipAS: [N0.L1, N1.L1, N2.L1, N3.L1]
Figure5:Themergingoflocalstatesfrom3globalstatesby
our three grouping strategies. On the left each Global State
corresponds to a dashed line from Figure 4. Logged localstatesaremarkedbygraycircles,eachcontainsthevariableslisted on Figure 2 line 12. On the right are group id’s fromtherespectivestrategiesAS:all-states,SR:send-receive,and
TO: total order. Highlighted is a group of states merged by
all-states.Inboldisagroupofstatesmergedbysend-receive.
groundstates.Wechoosethisapproachbecauseitisscalableand
makes no assumptions about the system.
3.3 Strategies to group global states
Some invariants hold globally at all times, but others hold dur-ing protocol-specific event sequences and between select nodes.
Without apriori system knowledge, many possible combinations of
global states may support or refute an invariant. The possible com-
binations of global states in a lattice is 2en, which is intractable to
analyze(andalsolargelyredundant)forrealsystems.Ourgoalistoautomatically tease apart distinct protocols, group the global states
in which they executed, and infer invariants on the group. Our
solutionreliesontheobservationthatmanyprotocolsarespecified
ascausalchainsofevents.Oneofourresearchcontributionisaset
of 3 strategies for grouping global states for invariant inference:
(1) group all global states together (all-states)
(2)groupstatesbysendingandreceivingnodepairs(send-receive)
(3)group states by totally ordered message sequences (total-order)
Figure 4 is an example execution showing two protocol spe-
cific causal chains (Ping-Ack andGossip-Gossip ), included are three
globalstates(shownasdashedlines).Figure5furthershowshow
the global states in Figure 4 are decomposed by each strategy, and
how the state tuples with matching identifiers (highlighted) aregrouped for further invariant inference. We will use Figure 5 to
explain each strategy.
1152
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Inferring and Asserting Distributed System Invariants ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
All-statesstrategy. Themostgeneralformofsysteminvariant,
isonewhichholdsonallobservablestate,andbetweenarbitrary
nodes.All-states merges all local states of a global state together
regardless of causality between them. Each local state has an ID:
(nodeID.loдID ),wherenodeIDdenotesthenodeandlogIDdenotes
theloggingstatement.AmergedsetofsuchIDsformagroupID:
[(node0.loдi),..., (noden.loдj)]. Merged local states which share a
groupID, are grouped together for invariant inference.
Figure5shows All-states (AS)mergingalllocalstatesfromthe
three global states. The groupID resulting from all-states merging
GlobalState1 is[N0.L1,N1.L2,N2.L1,N3.L1];noothermergedstate
shares this groupID. Merged states from Global State 2, and Global
State 3, highlighted in orange, share the groupID [N0.L1, N1.L1,
N2.L1, N3.L1]. GroupIDs produce multiple groups when there is
more than one logging statement per node. Using the logID as a
component of a groupID ensures that each merged state in a group
containsexactlythesamevariables,asseparateloggingstatements
neednotcontainthesamesetofvariables.Invariantsinferredby
the all-states strategy are the strongest, as they hold across the
largest sample of observable states and across all nodes.
Send-receivestrategy. Manyprotocolsdictatethebehaviorbe-
tweenpairsofnodes.Thesend-receivestrategymergestogether
thestatesofdirectlycommunicatingpairsofnodes.Send-receive
groupIDshavetheform [ (nodep.loдi),(nodeq.loдj)],wherenodep
communicated with nodeqandloдiexecuted before the communi-
cation and loдjexecuted after the communication. For example, in
Figure5GlobalState1send-receive(SR)hastwogroups:thefirst
group,[N0.L1,N1.L2],correspondingtothe PingbetweenNode0
and Node 1; and, group [N2.L1,N3.L1] captures the Gossipmessage.
Anadvantagetothisstrategyisthatpropertieswhichholdaf-
ter communication, but not at all times, are tested on subsets ofstates. Invariants like those that depend on eventual consistency
require thisfor detection. Fromour running example;each SWIM
node maintains a list of eventswhich are synchronized with Gossip
messages.Iftheinvariant N1.events=N2.events wastestedonthe
ASgroup from Figure 5 highlighted in orange, the invariant would
beviolatedbecauseinGlobalState2Node1hasnotyetreceived
theGossipmessage. In contrast, if the same invariant was tested
on the send-receive group [N1.L1, N2.L1] from Global State 3, it
would hold because Node 1 synchronized its events after receiving
the message.
Total-order strategy. Fine-grained protocols, such as leader
election, dictate a causal behavior across multiple nodes. The total-
order strategy merges the local states from causal chainsof com-
municating nodes. The groupID for this strategy has the form
[(nodep.loдi),..., (nodeq.loдj)],suchthatthelocalstate (nodep.loдi)
happenedbefore (nodeq.loдj)andallintermediate (nodeID,loдID )
pairs. InFigure 5the TOmerged group[N1.L1,N2.L1,N3.L1] from
Global State 3 is the result of merging all local states along the
Gossipmessages causal path from Node 3 to Node 1.
Total-order has thesame ability assend-receive to detecteven-
tual consistency in Serf, but it detects it in a stronger context. In
the case of Global State 3 group [N1.L1,N2.L1,N3.L1], the invariant
N1.events = N2.events = N3.events would be inferred.
Our complete solution to challenge 2 is to infer invariants on
groups of global states merged by one of three strategies. These
strategiesencodeheuristicsinformedbybestpracticesindistributed1func AssertLeadershipAgreement() bool
2fori:= 0; i<len(assert.Node) −2;i++// i is a nodeID index
3ifassert.GetVar(i, "leader") != assert.GetVar(i+1, "leader")
4 return false
5return true
Listing 1: A distributed assertion that checks that allnodes in a cluster agree on the leader.
systemdesignandradicallydecreasethespaceofpossiblegroup-
ings. Dinv further scales its analysis by, for example, discarding
identicalloggedinstancesofnodestateswhichspanseparateglobal
states since these provide no new information.
Next, we explain how Dinv infers invariants using a modified
version of Daikon.
3.4 Inferring distributed invariants
Daikon[19]isdesignedforsequentialsystemanddoesnotsupport
inferenceoverpartiallyorderedcollectionsofstateswithdisjoint
variable sets. Further, Daikon includes templates for binary and
ternaryinvariants,anddoesnotsupportn-aryinvariantsnecessary
for distributed specifications.
Dinv uses Daikon by presenting it with a synthetic program
point that corresponds to a distributed state. However, in a se-
quential program the same variables are always present at each
programpointandmergedstatesmaybecomposedofdifferentsets
of variables from various logging points. Our solution, reviewed in
Section 3.3, is to only merge states with identical sets of variables2.
We also added several n-ary templates, such as equality, to
Daikon. Inferred invariants span the local state of all nodes. For
example the group [N1.L1, N2.L1, N3.L1] from Figure 5’s Global
State 3TOwould have the invariant Node1.Events = Node2.Events =
Node3.Events,ratherthanthetwobinaryinvariants.Thisreduces
effort in comprehending relations spanning more than two nodes.
3.5 Asserting inferred invariants
Challenge 3 :Howtoenforceinferreddistributedinvariants? Dinv
includes an assertion library to help developers check inferred dis-
tributed safety properties at runtime with user-defined assertions
(Listing 1 shows an example). Prior approaches to checking dis-
tributed predicates [ 35,46] rely on variants of the global snapshot
algorithm based on logical clocks [ 9]. By contrast, Dinv uses a
light-weight real-time global snapshot algorithm for assertions.
Dinv’sreal-timeassertionshave3components:aroundtriptime
(RTT) estimator, a physical clock synchronizer, and an assertion
algorithm.TheRTTestimatorperiodicallypingsothernodesand
computes an estimate of RTT betweeneach node. To synchronize
clocks Dinv uses the Berkeley algorithm [24].
Theassertionalgorithmworksasfollows.Whenanode Aexe-
cutes an assertion statement, Ablocks until the asserted predicate
isresolved.First,usingtheRTTestimator, Aschedulesastatesnap-
shot time tthat is in the future by the largest RTT from Ato the
othernodes.Next, Asendsasnapshotrequestwithtime tandre-
questedvariablenamestoallnodes.Onreceivingarequestfrom
2As a more advanced heuristic, Dinv also supports analysis of intersections of logged
variables
1153
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Stewart Grant, Hendrik Cech, and Ivan Beschastnikh
A,an o d eBcreates a thread that sleeps until t. OnceBreaches
t,thisthreadsnapshotsthevaluesoftherequestedvariablesand
sends these to A. OnceAhas received all the snapshots it needs, it
evaluates the asserted predicate.
Scheduling snapshots with physical clocks, even if they are syn-
chronized, has the disadvantage that the resulting snapshot may
violatethe happens-beforerelation(e.g.,node Csnapshotsits state,
sendamessagetonode D,andthen Dsnapshotsitsstate).Toavoid
inconsistent snapshots Dinv uses vector clock timestamps to deter-
mine if a snapshot represents a ground state for the system. If not,
Dinv logs the failure to assert, skips the assertion, and will retry
the assertion the next time it is reached.
Due to blocking semantics, asserts in frequently executed code
canreduceperformance.Dinvallowsdeveloperstomitigatethis
withprobabilistic assertions, which execute with some user-defined
probability. We expect developers to use high probabilities dur-
ingtesting,forprecision,andlowprobabilitiesinproduction,for
performance.
Our solution to challenge three is a mechanism for probabilistic
distributed assertions.
4 IMPLEMENTATION
Dinvisimplementedin8K3linesofGocode[ 17].Ithasbeentested
on Ubuntu 14.04 & 16.04, and relies on Go 1.6. Dinv implements
an optimized version of the vector clock algorithm and uses a
manually-constructed database of wrapper functions for Go’s net.
We use Go’s AST library to build, traverse, and mutate the AST of
aprogramforinstrumentation.Dinv’sstateinstrumentationbuilds
oncontrolanddataflowalgorithmsinGoDoctor[ 28].Dinvuses
Daikon version 5.2.4.
5 APPLYING DINV TO COMPLEX SYSTEMS
InthissectionwedescribeourmethodologyforevaluatingDinv
on complex systems in Section 6. For our example we use Serf’s
eventually consistent group membership property. Prior to this
evaluationwehadnoknowledgeaboutthesystemsweanalyzed,
with the exception of the paper describing Raft [ 42]. In the evalua-
tion we used Dinv in concert with documentation and source code
to understand each system. We highlight four techniques we used
and that we believe make Dinv more usable.
When applying Dinv to a new codebase we used its completely
automated facilities to survey the systems invariants to learn
about its behavior. Initially, we instrumented Serf4, which injected
400loggingstatementsthatlogged20-40variableseach.Weranthe
system’s test suite, and processed the logs with Dinv. The merging
strategies parsed the log into 1000 groups of global states. In ag-
gregate across all groups, Daikon inferred approximately 1 million
invariants, many of which related constants. We used the massive
set of invariants to identify variables relevant to consistency. Us-
inglinenumbersasindexesintothesourcecodewe refined our
analysis to a smaller set of functions.
A second execution, resulted in 50 groups and a total of 1,700
invariants. all-states invariantsfalsifiednodestateequality,sowe
3All LOC counts in the paper were collected using cloc [ 2]; test code is omitted from
the counts.
4Serfusesencodersandrequiredthemanualadditionof20lines,oneforeachsending
and receiving line of code.deduced that consistency did not hold globally at all times. We ex-
aminedsend-receive invariantsforafine-grainedviewofthesystem.
The updates were fully observable by monitoring assignments to a
single structure which maintained the cluster’s health, so we com-
posed dump statements to instrument this structure. Running
thetestsagain,whileloggingonlythecluster’shealth,resultedin25groups,withatotalof40inferredinvariants. Send-receive,and total-
orderoutputs were composed of the desired equality invariants
between the node states.
In our evaluation of Dinv with three other complex systems,
we followed the above approach of iterativly refining the logged
variables to zero in on properties over key distributed state.
Togenerateassertionsfromtheinferredinvariantswemanually
wrotebooleanfunctions(e.g.,Listing1)tocheckaninvariantacross
nodesatruntime.Section7describesourexperienceswithusing
the assertion mechanism.
6 EVALUATION: INFERRING INVARIANTS
In the following section we use Dinv to analyze four systems:
HashicorpSerf[ 25](ourrunningexample),Groupcache[ 20],Taipei-
Torrent [27], and Coreos’s etcd [ 14]. We describe each system and
theirproperties,andreportoninvariantsdetectedbyDinvandwhat
they tell us about the correctness of the system. Table 2 overviews
the invariants we targeted in our study.
Experimental setup. All inference experiments were run on
an Intel machine with a 4 core i5 CPU and 8GB of memory, run-ning Ubuntu 14.04. All applications were compiled using Go 1.6
forLinux/AMD64.Experimentswererunonasinglemachineus-
ing a mixture of locally referenced ports, and iptable configura-
tionstosimulateanetwork.Runtimestatisticswerecollectedusing
runlim[47]formemoryandtiminginformation,andiptablesfor
tracking bandwidth overhead.
6.1 Analyzing the SWIM protocol in Serf
Serf [25] is a system for cluster membership, failure detection, and
event propagation. Serf has 6.3K LOC and is used by HashiCorp
inseveral majorproducts.Serf buildsona gossipprotocollibrary
based on SWIM [16].
Each SWIM node maintains an array of all other nodes liveness
state:alive,suspectedordead.Asuspectedfailureisgossipedwhen
heartbeatmessagesarenotacknowledged,andcompletefailures
(dead) is gossiped once a specified subset of nodes suspect a fail-
ure. Throughout this process, node state updates are attached topings, ping-reqs and acks, and spread by gossip messages to en-sure eventual consistency. A receiving node
japplies updates it
receives only if it does not have more recent information. Dinvcan observe this property by logging state changes, i.e., node
i
sets node k’s state to alive. Thesend-receive strategy inferred
the invariant nodei.stateOfK =nodej.stateOfK , for all pairs of
nodesiandj. Further, the total-order strategy inferred the invari-
antnodei.stateOfK =nodej.stateOfK =···=nodem.stateOfK
onalltransitivesequencesofgossipmessagesonclustersupto4
nodes. The detection of this invariant required all orderings of gos-sip propagation to occur many times and thus required the longest
executions, and analysis time.
1154
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Inferring and Asserting Distributed System Invariants ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
System and
Targeted propertyDinv-inferred invariant Description
Serf
Eventual consistency∀nodesi,j,NodeState i=NodeState j Nodes distribute membership changes correctly.
Groupcache
Key ownership∀nodesi,j,i/nequalj,
OwnedKeys i∩OwnedKeys j=∅Nodes are responsible for disjoint key sets.
KademliaLog. resource resolution∀requestr,/summationtext(msgs forr)≤log(|peers|)
All resource requests must be satisfied in no more than
O(loд(n))messages.
Kademlia
Minimal distance routing∀keyk,n od ex,
if XOR(k,x) minimal, then xstoreskDHT nodesstores avalue onlyif itsID hasthe minimalXORdistance in the cluster to the value ID.
RaftStrong leader principle∀followeri, len(leader log)≥len(i’s log) All appended log entries must be propagated by the leader.
RaftLog matching∀nodesi,j,i fi-log[c]=j-log[c]→
∀(x≤c),i-log[x]=j-log[x] If two logs contain an entry with the same index and term,
then the logs are identical in all previous entries.
Raft
Leader agreementIf∃nodei, s.t.ileader, then∀j/nequali,jfollower If a leader exists, then all other nodes are followers.
Table 2: Invariants listed by system, their corresponding distributed state invariants, and descriptions.
Ininstrumentingnetworkcalls,twocodepathshadtobeconsid-
ered,oneforTCPandoneforUDP.Dinv’sautomaticinstrumen-
tationworkedforUDP.IntheTCPcasecustomstreamdecoding
prevented automatic instrumentation; instead, we wrote 20 LOC to
insert/extract vector clocks.
We setup an execution environment where nodes were peri-
odicallypartitionedtoforcefrequentpropagationofmembership
updates. Observing 3-4 nodes in an execution with 100 such parti-
tions resulted in Dinv inferring all invariants. We were also able to
observe and gather similar results about Serfs’ behavior in more
complex executions, i.e., round-robin partitions.
Anexecutionwith100partitionswasrunningfor24minutes5
and produced 1.6 MB of log files, which Dinv analyzed in less than
2minutes.Theresultsandlackofcontradictinginvariantsleaves
us confident that Serf’s update dissemination is correct.
6.2 Analyzing Groupcache
GroupcacheisanopensourceGoimplementationofmemcached,
writtenin1786LOC[ 20].Groupcachenodesactasbothclientsand
servers for key requests. Like memcached, Groupcache assigns key
ownership to nodes, but nodes hold no state apart from multiple
caches.Eachnodeisresponsibleforauniquesetofkeyswhichit
owns exclusively.
Groupcacherequiresuserstoprovidea Getterfunctionwhich
maps keys to values. Getmessages are encoded using Protobuf
andexchangedoverHTTP.ProtobufencapsulatesGo’sstandard
networkinglibrary,makingthecallsinvisibletoDinv’svectorclock
instrumentation.WemanuallyaugmentedtheHTTPheaderwith
vector clocks, which required 6 additional LOC.
Because of Groupcache’s static key partitioning, the invariant
nodei.keys/nequalnodej.keysholds globally at all times, and not on
precise protocol-specific sequences. Our Groupcache test program
wasrunonconfigurationsof2–8nodes,eachofwhichrequested
2Kkeys.Dinvdetectedthecentralkeydistributionproperty(see
Table 2) with each merging strategy. Here all-states provides the
5Serf was given 7 seconds after and before each partition to detect and propagate
membership changes.strongest evidence of the invariants correctness, as the key owner-
ship can be demonstrated to hold on all observable global states.
ThekeyownershippropertywasquicklyidentifiedwithDinv
byathird-yearundergraduatestudent,whohadlittleexperience
with Dinv or Groupcache.
6.3 Analyzing Taipei-Torrent
Taipei-TorrentisanopensourceBitTorrentclientandtracker.Its
client program uses Nictuku’s implementation of the Kademlia
distributed hash table (DHT) protocol to resolve peer and resource
queries [29,40]. Taipei-Torrent and Nictuku are implemented in
5.8K and 4.9K LOC, respectively.
Kademlia uses a virtual binary tree routing topology structured
on unique IDs to resolve resources and peers. Peers maintain rout-
inginformationaboutasinglepeerineverysub-treeonthepath
from their location to the rootof the tree. Kademlia has 2 primary
types of messages: StoreandFind_Value.
Storeinstructs a peer to store a value. Find_Value resolves re-
questsforstoredvalues.Apeer’sresponsetoa Find_Value query
is the list of peers on its sub-tree with the closest XOR distances
to the requested value. Find_Value is executed iteratively on the
peer list until the value is found. These queries are resolved within
O(loд (|peers|))where|peers|is the total number of peers.
WeautomaticallyinjectedvectorclocksintoTaipei-Torrentin3s.
Manual logging functions were used because variables containing
routing information were not readily available. We introduced our
owncounterwith2linesofcodetotrackthenumberof Find_Value
messages propagated in the cluster. Taipei-Torrent has sparse com-
munication between nodes; the result is a large space of partial
orderings. Lattices built from the traces of Taipei-Torrent consisted
of 20–100 million points. Log analysis took upwards of 15 minutes,
with an upper limit of 2 hours, requiring frequent writes to disk as
thelatticeexceededavailablememory.Dinvsucceededinanalyz-
ingtheseexecutions,althoughthecommunicationpatternwasa
challenge for our techniques. Lattice inflation limited our analysis
to executions with at most 7 peers.
Kademlia specifies that peers must store and serve resources
withtheminimumXORdistancetotheirIDs.Further, Find_Value
1155
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Stewart Grant, Hendrik Cech, and Ivan Beschastnikh
requestsmustresolvetotheminimumdistancepeer.Totestthecor-
rectnessof Find_Value requestsweaddeda5linefunctionwhich
output the minimum distance of the peers and resources in the
routing table and logged it. To test routing we ran clusters with
3–6 peers using a variety of topologies by controlling peer IDs. We
logged state after the results of a Find_Value request were added to
a peer’srouting table.On eachexecution wefound that ∀peersi,j,
peeri.min_distance =peerj.min_distancein all total-order groups.
This invariant, in conjunction with O(Loд (n)) message bound, pro-
vides strong evidence for the correctness of Nictuku’s implementa-
tion of Kademlia.
6.4 Analyzing etcd Raft
Etcdisadistributedkey-valuestorewhichreliesontheRaftconsen-
susalgorithm[ 42].Raftspecifiesthatonlyleadersserverequests,
andfollowersreplicatealeader’sstate.Followersuseaheartbeat
to detect leader failure, starting elections on heartbeat timeouts.Etcd is used by applications such as Kubernetes [
32], fleet [13],
andlocksmith[ 15],makingthecorrectnessofitsconsensusalgo-
rithmparamounttolargetechcompaniessuchaseBay.EtcdRaft
is implemented in 144K LOC.
Etcdusesencoderstowrapnetworkconnections,somanualvec-
tor clock instrumentation was required. Log analysis took between
10-15s. Etcd was controlled using scripts. One to launch a clusters
of 3-5 nodes, another to partition nodes, and one to issue a 30s
YCSB-A workload (50% put, 50% get requests) [11].
Strong leadership . An integral property of Raft is strong lead-
ership:onlytheleadermayissueanappendentriescommandtothe rest of the cluster. This property manifests itself in a num-ber of data invariants. A leader’s log should be longer than thelog of each follower. Further, the leader’s commit index, and logtermshouldbelargerthanthatofthefollowers.Weloggedcom-mit indices, and the length of the log. In each case the invari-ant
leader.loдsize≥follower.loдsize, andleader.commitIndex≥
follower.commitIndex was detected by the send-receive strategy.
Log matching. Raft asserts “if two logs contain an entry with the
same index and term, then the logs are identical in all entries upto the given index ”[
42]. This property is hard to detect explicitly
because it requires conditional logic on arrays. We were able to
detectthatinallcases nodei.commitIndex =nodej.commitIndex∧
nodei.loд[commitIndex ]=nodej.loд[commitIndex ]→nodei.loд=
nodej.loдup to the all-states grouping. This shows that if any two
nodes have the same log index, and the value at that index match,
theirentirelogsmatch;thisisevidenceofthelogmatchingproperty.
Leadership agreement . At most one leader can exist at a time
in an unpartitioned network, and all unpartitioned members ofa cluster must agree on a leader after partitioning. By logging
leadershipstatevariableswhenleadershipwasestablished,wewereabletoderivethat:
nodeistate=Leader→∀jnodej.leader=nodei
∧∀j/nequali,nodej.state=Follower.Theseinvariantsweredetectedin
bothsend-receive andtotal-order groups.Thisindicatesthatafter
thepartitionoccurred,allnodesagreeonaleader,andthatallnodes
but the leader are followers.
Strongleadership,logmatching,andleadershipagreementare
invariants ofa correctRaft implementation. Bychecking their exis-tence,weproducedstrongevidenceforthecorrectnessofetcdRaft.Raft invariant LOC P=1.0 P=0.1 P=0.01
Strong leadership 11 0.07 0.05 2.96
Leadership agreement 13 0.36 0.34 6.75
Log matching 72 2.22 4.35 6.07
Table3:LOCtoimplementandtime(sec)todetectaninvari-
ant violation with probabilistic asserts.
Further,wehaveshownDinv’sabilitytodetectusefulproperties
over distributed state of large and non-trivial system.
7 EVALUATION: ASSERTING INVARIANTS
Dinv-inferred invariants can be used for comprehension. However,
they can also be converted into assertion predicates to find regres-
sionerrorsatruntime.HerewedetailhowweusedtheDinvassert
mechanism to check the inferred etcd Raft invariants at runtime.
Wedevelopeddistributedassertionsforeachofetcd’sinvariants.
We then evaluated the ability of these assertions to find bugs by
using them with buggy versions of Raft. For this we manuallycreated three bugs, each of which violates one of the three Raft
invariants. All bugs cause a violation withoutcausing Raft to crash,
or impact its ability to serve client requests. That is, each bug
produces silent errors and is difficult to detect.
Strong leadership bug. In Raft only the leader may issue the
command to append entries to a replicated log. In our two line bug
an unauthorized follower broadcasted append entries, and commit-
tedtoitsownlog.Raft’salgorithmtoleratesthisbugbecausethe
leader has authority to overwrite followers logs. However, oncealeaderhaswrittentodiskinaterm,thesystemexpectsthatall
followers’ logs are synchronized. Etcd does not verify synchroniza-
tionsothebugcausestheleadertoperpetuallyissuelogcorrectionmessagestothebuggyfollower.Theinvariantforstrongleadership
isthattheleaderslogsizeisgreaterthanallthefollowers’logsif
the leader has committed in the current term.
Leadership agreement bug. Ifaleaderexistsinagiventerm,
allnodesmustagreeonthisleaderforleadershipagreementtohold.
We introduced a 4-line bug which caused followers to randomly
select a leader from their list of peers post election. Etcd continues
to execute with this bug. However, followers periodically time out
waitingformessagesfromafalseleaderandinitiateanewelection.
In a non-buggy execution if any two nodes agree on a leader for a
given term then they agree on the same leader.
Logmatching bug. Logmatchingiscriticaltoetcd’sfaulttol-
erance.Iflogmatchingdoesnotholdetcd’skeyvaluestorereturns
inconsistent results depending on which node is the leader. Etcd
assumesthatalllogentrieswrittentodiskarecorrect.Toviolate
the log matching invariant we injected a 7-line bug to corrupt acommitted log at a random place and time. With this bug etcdexecutes as normal and assumes that the nodes’ logs are correct.
The log matching invariant states that if any two nodes have an
entrywiththesameterm,index,anddata,thenallpriorlogentries
match.
Table 3 shows our experimental results. Assertions for above
invariants ranged in size. Log matching was the most complex
(72 LOC): checking it requires a comparison of logs from every
pair of nodes. The assertion iterates through all pairs of node logs,
checking themfor inconsistencies. Theother two assertions were
expressed in under 15 LOC.
1156
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Inferring and Asserting Distributed System Invariants ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Number of
annotationsExecuted
annotationsLog size
(MB)Runtime
(s)Runtime
overhead %
0 0 0 2.66 0
1 2.8K 3.2 2.70 1.5
2 5.6K 4.3 2.77 4.0
5 14K 9.7 3.01 12.9
10 28K 18.0 3.31 24.3
30 85K 51.7 4.48 68.0
100 261K 167.9 7.66 187.5
Table 4: Impact of Dinv annotations on Raft performance.
WeranRaftwitheachbugandusedassertionswithprobabilities
of 1.0, 0.1, and 0.01. We measured the average time delay between
the instant a bug was injected and when it was detected. We found
that all asserts found the bugs, but they took longer with lower
probabilities.Consideringtheseverityofthesebugs,webelievethat
thedelayofafewmoresecondstodetecttheproblemisreasonable
(given no other alternative). We discuss the associated decrease in
overhead with using probabilistic assertions in Section 8.1.
8 EVALUATION: DINV OVERHEAD
Dinv imposes severaloverheads. Theseinclude thetime toinstru-
mentthesystem,runtimeandnetworkoverheadsduetologging
and injected vector clocks, and the running time of the dynamic
analysis that Dinv must perform on the collected logs. This section
details these overheads.
Static analysis runtime. To benchmark the performance of
Dinv’sstaticanalysis(detectingnetworkingcalls,addinglogging
code,etc)weusedetcdRaft,whichcontains144KLOCandthou-
sandsofvariables.Wemeasuredinstrumentationtimewithincreas-ingcountsofrandomlylocated dumpannotations.Instrumentation
timeremainedconstantat3suntil4Kannotationsatwhichpointit
increased slightly to 3.2s. At 64K annotations (far beyond practical
use) runtime was 4.7s.
Logging overhead. Logging state at runtime slows down the
system. We instrumented etcd Raft with increasing number of log-
ging statements, each one logging 7 variables. We benchmarked
aclusterwith3nodes,andaYCSB-Aworkload.Eachclusterwas
run 3 times and we averaged the total running time. Table 4 shows
a linear relationship between the number of logging statementsand runtime. In practice just two annotations were sufficient to
detecttheRaftinvariants.Theaverageexecutiontimeofasingle
logging statement is 20 microseconds. In our local area network
with a round trip time of 0.05ms while running etcd with 1 second
timeouts we can introduce approximately 50K logging statements
per node before perturbing the system.
Bandwidthoverhead. Vectorclocksintroducebandwidthover-
head. Each entry in Dinv’s vector clocks timestamp has two 32 bit
integers: one to identify the node, and the other is the node’s logi-
cal clock timestamp. The overhead of vector clocks is a product of
thenumberofinteractingnodesinanexecutionandthenumber
of messages: 64 bits×nodes×messaдes . To evaluate bandwidth
overhead in a real system we executed etcd Raft using the setup
abovewhilevaryingthenumberofnodes.Thebandwidthsofall
nodes was aggregated together for these measurements. We found
that adding vector clocks to Raft slowed down the broadcast ofheartbeats and caused a reduction in bandwidth of 10KB/s for all
nodes in a 4 node cluster. At 5 nodes and above the bandwidthSystem
runtime (s)Raft
log (MB)Raft
analysis(s)GCache
log (MB)GCache
analysis(s)
30 5.1 12.7 0.3 2.8
60 10.5 28.1 0.3 3.0
90 13.7 35.9 1.7 19.6
120 17.4 48.7 1.4 21.2
150 22.5 68.8 1.8 11.3
180 27.7 99.1 2.1 18.6
Table 5: Generated Dinv log size and Dinv’s dynamic analy-
sis running time for varying system run times, for two sys-
tems: etcd Raft and GroupCache (GCache).
overhead grew linearly with an overhead of 1KB/s for 5 nodes and
10KB/s for 6 nodes.
Dynamicanalysisruntime. Dinv’sdynamicanalysisruntime
is affected by the size of the log and the number of nodes in the
execution. To measure its performance versus the length of execu-
tion, we analyzed etcd Raft and Groupcache. We exercised them byissuing10requestspersecondtoeachsystem.TodemonstratehowDinv’sanalysisperformswithregardtothelengthofexecution,we
analyzedtheresultinglogsof3nodeclusters,whichwererunfor
intervals in increments of 30s. Results in Table 5 show that Dinv’s
log analysis scales linearly with system running time.
To measure how analysis time is affected by the number of
nodes in an execution we ran etcd for 30s, exercising it with 10client requests per second and running clusters with increasingnumber of nodes. Our results show that Dinv’s runtime growsexponentially with the number of nodes. We measured analysistimes of 25s, 75s, and 725s for logs containing 4, 5, and 6 nodes,
respectively. Dinv’s runtime is exponential in the number of nodes
due to the exponential growth of partial orderings our analysis
techniquescompute.ThisindicatesthatDinviscurrentlylimited
to analyzing distributed systems with a small number of nodes.
8.1 Distributed assertions overhead
WeevaluatedtheoverheadofDinv’sassertionmechanismonMi-
crosoftAzure.Thesetupconsistedof4VMs(3serversand1client),
allrunningUbuntu16.04.TheserverVMshad3.5GBofmemory
and a single core capable of performing 3200 IOPS. The client was
used to saturate the servers and had 16 cores and 56GB of memory,
and could perform 51200 IOPS. Below we measure the end-to-end
latency of client requests to the etcd cluster.
We established a baseline using unmodified etcd. The system
wasexercisedat3loadlevels:100,150,and200clientrequestper
second,eachtestwasrunfor100s.EachassertinSection7wasrun
underthesameconditions.Assertionswereplacedinetcd’sinner
event loop which executed on every received message and timer
event.Onaverage5eventsoccurredperclientrequest.Wealsoranexperimentswithprobabilisticassertswithtwoprobabilities:P=0.1
andP=0.01andmeasuredthemedianslowdowninclientrequest
response times.
Thegreatestslowdownwasincurredwhenassertswereplacedat
bottleneckprogrampoints.Forexample,assertingstrongleadership
withP=1.0causeda52xslowdown,aseachclientrequestwasforced
to wait for multiple asserts to execute. Using P=0.1 reduced this to
2.5x, and P=0.01 reduced it further to 1.02x. Leader agreement and
logmatchingassertswereperformedbyfollowers,whicharenot
1157
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Stewart Grant, Hendrik Cech, and Ivan Beschastnikh
onthecriticalpathforclientrequestprocessing.Bothassertions
with P=1.0 introduced only a 1.09x slowdown.
9 DISCUSSION
Effort in using Dinv. In our evaluation we considered four large
distributed systems, none of which we were familiar with prior to
this study. In each case we used all of the resources available to
us (papers, source code, documentation) to understand the desired
system properties and to interpret Dinv’s output. As we had no
prior knowledge about the four systems in our evaluation, and
weresuccessfulininferringinterestingproperties,weareconfident
that with proper training developers would be able to similarly
instrument their systems.
AlthoughwedidnotformallyevaluateDinvwithdevelopers,we
do have two pieces of anecdotal evidence that Dinv is not difficult
touse.First,graduatestudentswithnopriordistributedsystems
backgroundsuccessfullyusedDinvontheirsystemsinadistributed
systems course. Second, Groupcache (Section 6.2) was analyzed by
anundergraduatestudentwhowasfamiliarwithdistributedsys-
temsbutnotwithDinv.AfterinstallingGroupcacheandbecomingfamiliarwithitstestsuit,hewasabletoisolatethekeydistribution
invariant withina workday. Althoughanecdotal, webelieve these
experiences indicate that Dinv is usable by developers. We plan to
evaluate Dinv with developers in our future work.
Dynamic analysis. Dinv infers likelyinvariants because it is
a dynamic analysis approach that only considers a finite set of
system behaviors. The inferred invariants are not a verification of
the system, but they could be used for runtime checking (as we
demonstrated in Section 7), or to bootstrap verification [41].
Executionscontainingfailures .Dinv’sinferencepipelinewas
designed to infer invariants from executions with no node failures.
Dinv’sassertionmechanism,however,candetectinvariantviola-
tions even when failures occur.
10 RELATED WORK
Mining distributed systems information. Dinv is a specifica-
tion mining tool that builds on Daikon [ 19], which cannot mine
distributed state invariants on its own. Daikon has been previously
used to assist a theorem prover in v erifying distributed algorithms
by running over simulated execution traces [41].
The closest work to Dinv is work by Yabandeh et al. [ 56] who
inferalmost-invariantsindistributedsystems:invariantsthatare
true in most cases. They also build on Daikon but the process
ofidentifyingvariablestolog,instrumentingthesystem,piecing
together distributed cuts and composing logs from different nodes,
isamanualprocess.Ourapproachactuallyinstrumentsthesystem,
computes ground states, and also checks invariants at runtime.
Other approaches that mine distributed/concurrent specifica-
tions produce symbolic message sequence graphs to group ma-chines into classes based on similarities in their communicationpatterns [
33], LTL properties relating events between nodes [ 6],
and infer communicating finite state machines [ 5]. This prior work
focuseson eventsandcantraceitsrootsbacktoCookandWolf’s
original work that noted concurrency as a challenge [ 10]. None of
these techniques can detect distributed dataproperties.Otherworkminesavarietyofdistributedsysteminformation
for SE purposes. For example, some work uses mining to detect
dependencies [ 37], anomalies [ 53,55], and performance bugs [ 48].
Other analysis of distributed systems. Dynamicanalysisof
distributed systems has yielded several tools to aid developers.
For example, DistIA [ 8] implements impact analysis, Googles Dap-
per [51] analyzes traces to produce call graphs and performance
information, and lprof[59] instruments Java bytecode with syn-
chronized timestamps and uses logs to infer temporal properties.
TwopriortoolsuseDaikontoderiveinvariantsofnetworkedand
concurrentsystems.InvarScope[ 23]detectsinvariantsinJavaScript
applications, but does not generalize beyond client-server systems.
Udon[34]infersdatainvariantsofmulti-threadedprogramswhere
program state is shared between threads.
Monitoring systems such as Fay [ 18] and Pivot tracing [ 38] use
dynamic instrumentation for real-time diagnosis of distributed
systemsbyactivatingtracepointsatruntime.Thesetoolsdonot
infer properties from the traces they capture.
Formalmethodsfordistributedsystems. Unlikerecentmeth-
odsthatusetheoremprovingtosynthesizecorrectsystemsbycon-struction[
26,50,54],ourworkisimmediatelyapplicabletoexisting
productionsystems.Previousworkalsoconsiderscheckingexist-
ingsystemimplementationsdirectly[ 31,57],orcheckingsystem
properties at runtime or during program replay [ 22,30,35,36,45].
Dinvalsoincludesaruntimeassertioncheckingmechanism.But,
incontrasttopriorworklikeD3S[ 35],Dinv’smechanism schedules
node state snapshots using synchronized physical clocks and uses
probabilistic assertions to decrease overhead.
More fundamentally, previously work assumes that a developer
can,andiswillingto,specifypropertiesoftheirsystem.Bycontrast,Dinvdoesnotrequirethedevelopertoformallyspecifytheirsystem
and aims at elucidating the runtime properties of the system.
11 CONCLUSION
Distributed state is a key element of distributed systems that im-pacts consistency, performance, reliability, and other system fea-
tures.However,distributedstateisdifficulttoteaseout,understand,
andcheck.Wepresentedanovelautomatedanalysisapproachto
(1) identify distributed state, (2) instrument it and record it at run-
time,(3)combineitusingthreedifferentstrategies,and(4)useit
to infer likely distributed state invariants. We also introduced a
lightweight probabilistic assertion mechanism to check distributed
state invariants at runtime using real-time snapshots.
We realized our approach in Dinv, a tool for systems written in
Go.WeevaluatedDinvwithfourcomplexandwidelyusedsystems.OurevaluationdemonstratesthatDinvcaninfercriticalcorrectness
properties of these systems, and that Dinv assertions can detect
silentviolationsoftheseproperties.Forexample,Dinvdetecteda
violation of each of the three invariants of etcd Raft in under 7s
with assertion overhead of just 1.02x.
Dinv is an open source tool [17].
Acknowledgments
This work was supported in part by an NSERC Discovery Grant,
Huawei,andtheInstituteforComputing, InformationandCogni-
tive Systems (ICICS) at UBC.
1158
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. Inferring and Asserting Distributed System Invariants ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]M. Ahuja, A. D. Kshemkalyani, and T. Carlson. A basic unit of computation
in distributed systems. In International Conference on Distributed Computing
Systems (ICDCS), 1990.
[2] AlDanial. cloc: Count Lines of Code. https://github.com/AlDanial/cloc, 2016.
[3]O.BabaogluandM.Raynal. SpecificationandVerificationofDynamicProperties
in Distributed Computations. Journal of Parallel and Distributed Computing,
28(2):173 – 185, 1995.
[4]P.BernsteinandE.Newcomer. PrinciplesofTransactionProcessing:FortheSystems
Professional. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1997.
[5]I. Beschastnikh, Y. Brun, M. D. Ernst, and A. Krishnamurthy. Inferring Models of
ConcurrentSystemsfromLogsofTheirBehaviorwithCSight. In International
Conference on Software Engineering (ICSE), 2014.
[6]I. Beschastnikh, Y. Brun, M. D. Ernst, A. Krishnamurthy, and T. E. Anderson.
MiningTemporalInvariantsfromPartiallyOrderedLogs. SIGOPSOper.Syst.Rev. ,
45(3):39–46, Jan. 2012.
[7]I.Beschastnikh,P.Wang,Y.Brun,andM.D.Ernst.Debuggingdistributedsystems:
Challenges and options for validation and debugging. Communications of the
ACM, 59(8):32–37, Aug. 2016.
[8]H. Cai and D. Thain. DistIA: A Cost-effective Dynamic Impact Analysis for Dis-
tributedPrograms. In InternationalConferenceonAutomatedSoftwareEngineering
(ASE), 2016.
[9]K. M. Chandy and L. Lamport. Distributed snapshots: determining global states
of distributed systems. ACM TOCS, 3(1):63–75, Feb. 1985.
[10]J. E. Cook and A. L. Wolf. Discovering Models of Software Processes from
Event-based Data. ACM TOSEM, 7(3):215–249, July 1998.
[11]B.F.Cooper,A.Silberstein,E.Tam,R.Ramakrishnan,andR.Sears. Benchmarking
CloudServingSystemswithYCSB. In SymposiumonCloudComputing(SoCC),
2010.
[12]R. Cooper and K. Marzullo. Consistent Detection of Global Predicates. In
ACM/ONR Workshop on Parallel and Distributed Debugging (PADD), 1991.
[13] CoreOS. A Distributed init System. https://github.com/coreos/fleet, 2013.[14]
CoreOS. Distributed reliable key-value store for the most critical data of a
distributed system. https://github.com/coreos/etcd, 2013.
[15]CoreOS. Reboot manager for the CoreOS update engine.
https://github.com/coreos/locksmith, 2014.
[16]A.Das,I.Gupta,andA.Motivala. Swim:Scalableweakly-consistentinfection-
styleprocessgroupmembershipprotocol. In InternationalConferenceonDepend-
able Systems and Networks (DSN), 2002.
[17] Dinv homepage. https://bitbucket.org/bestchai/dinv/.
[18]Ú. Erlingsson, M. Peinado, S. Peter, and M. Budiu. Fay: Extensible Distributed
Tracing from Kernels to Clusters. In Symposium on Operating Systems Principles
(SOSP), 2011.
[19]M.D.Ernst,J.H.Perkins,P.J.Guo,S.McCamant,C.Pacheco,M.S.Tschantz,and
C.Xiao. TheDaikonsystem fordynamicdetectionoflikelyinvariants. Science
of Computer Programming, 69(1–3):35–45, Dec. 2007.
[20] B. Fitzpatrick. Groupcache. https://github.com/golang/groupcache, 2014.
[21]V.K.Garg. MaximalAntichainLatticeAlgorithmsforDistributedComputations.
InDistributed Computing and Networking, pages 240–254. Springer, 2013.
[22]D. Geels, G. Altekar, P. Maniatis, T. Roscoe, and I. Stoica. Friday: Global Compre-
hension for Distributed Replay. In Symposium on Networked Systems Design and
Implementation (NSDI), Cambridge, MA, USA, 2007.
[23]F.Groeneveld,A.Mesbah,andA.VanDeursen. Automaticinvariantdetection
indynamic webapplications. Technical report,DelftUniversity ofTechnology,
Software Engineering Research Group, 2010.
[24]R. Gusella and S. Zatti. The Accuracy of the Clock Synchronization Achieved by
TEMPO in Berkeley UNIX 4.3BSD. IEEE TSE, 15(7):847–853, July 1989.
[25]Hashicorp. Service orchestration and management tool.
https://www.serf.io/docs/internals/gossip.html, 2014.
[26]C.Hawblitzel,J.Howell,M.Kapritsos,J.R.Lorch,B.Parno,M.L.Roberts,S.Setty,
and B. Zill. IronFleet: Proving Practical Distributed SystemsCorrect. In Sympo-
sium on Operating Systems Principles (SOSP), pages 1–17, New York, NY, USA,
2015. ACM.
[27]Jackpal. A(nother) Bittorrent client written in the go programming language.
https://github.com/jackpal/Taipei-Torrent, 2010.
[28]R. A. Jeff Overbey. Go Doctor - The Golang Refactoring Engine.
http://gorefactor.org/index.html, 2014.
[29]Y. Junqueira. Kademlia/Mainline DHT node in Go.
https://github.com/nictuku/dht, 2012.
[30]A. Khurshid, X. Zou, W. Zhou, M. Caesar, and P. B. Godfrey. VeriFlow: Verifying
Network-Wide Invariants in Real Time. In Symposium on Networked Systems
Design and Implementation (NSDI), 2013.
[31]C.Killian,J.W.Anderson,R.Jhala,andA.Vahdat. Life,death,andthecritical
transition:findinglivenessbugsinsystemscode. In SymposiumonNetworked
Systems Design and Implementation (NSDI), Cambridge, MA, USA, 2007.[32]Kubernetes. Production-Grade Container Scheduling and Management.
http://kubernetes.io/, 2014.
[33]S.Kumar,S.-C.Khoo,A.Roychoudhury,andD.Lo. InferringClassLevelSpecifica-
tionsforDistributedSystems. In InternationalConferenceonSoftwareEngineering
(ICSE), 2012.
[34]M. Kusano, A. Chattopadhyay, and C. Wang. Dynamic Generation of Likely
InvariantsforMultithreadedPrograms. In InternationalConferenceonSoftware
Engineering (ICSE), 2015.
[35]X.Liu,Z.Guo,X.Wang,F.Chen,X.Lian,J.Tang,M.Wu,M.F.Kaashoek,and
Z. Zhang. D3S: Debugging Deployed Distributed Systems. In Symposium on
NetworkedSystems Designand Implementation(NSDI),San Francisco,CA, USA,
2008.
[36]X. Liu, W. Lin, A. Pan, and Z. Zhang. WiDS Checker: Combating Bugs in Dis-
tributed Systems. In Symposium on Networked Systems Design & Implementation
(NSDI), 2007.
[37]J.G.Lou,Q.Fu,Y.Wang,andJ.Li. Miningdependencyindistributedsystems
through unstructured logs analysis. SIGOPS Oper. Syst. Rev., 44(1):91–96, Mar.
2010.
[38]J. Mace, R. Roelke, and R. Fonseca. Pivot tracing: Dynamic causal monitoring for
distributed systems. In Symposium on Operating Systems Principles (SOSP), 2015.
[39]F. Mattern. Virtual Time and Global States of Distributed Systems. In Parallel
and Distributed Algorithms, pages 215–226, 1989.
[40]P. Maymounkov and D. Mazières. Kademlia: A Peer-to-Peer Information System
Based on the XOR Metric. In International Workshop on Peer-to-Peer Systems
(IPTPS), 2002.
[41]T. Ne Win, M. D. Ernst, S. J. Garland, D. Kırlı, and N. Lynch. Using simulatedexecution in verifying distributed algorithms. Software Tools for Technology
Transfer, 6(1):67–76, July 2004.
[42]D.OngaroandJ.Ousterhout. InSearchofanUnderstandableConsensusAlgo-
rithm. In USENIX ATC, 2014.
[43]K. J. Ottenstein and L. M. Ottenstein. The Program Dependence Graph in a
Software Development Environment. SIGPLAN Not., 19(5):177–184, Apr. 1984.
[44]J.K.Ousterhout. TheRoleofDistributedState. In InCMUComputerScience:a
25th Anniversary Commemorative, pages 199–217. ACM Press, 1991.
[45]P. Reynolds, C. Killian, J. L. Wiener, J. C. Mogul, M. A. Shah, and A. Vahdat. Pip:
DetectingtheUnexpectedinDistributedSystems. In SymposiumonNetworked
Systems Design and Implementation (NSDI), 2006.
[46]K. Romer and J. Ma. PDA: Passive distributed assertions for sensor networks.
InInternationalConferenceonInformationProcessinginSensorNetworks(IPSN),
2009.
[47] RunLim. RunLim. http://fmv.jku.at/runlim/, 2016.
[48]R. R. Sambasivan, A. X. Zheng, M. D. Rosa, E. Krevat, S. Whitman, M. Stroucken,
W.Wang,L.Xu,andG.R.Ganger. DiagnosingPerformanceChangesbyCom-
paring Request Flows. In Symposium on Networked Systems Design and Imple-
mentation (NSDI), 2011.
[49]F. B. Schneider. Implementing fault-tolerant services using the state machine
approach: a tutorial. ACM Comput. Surv., 22(4):299–319, Dec. 1990.
[50]I.Sergey,J.R.Wilcox,andZ.Tatlock. ProgrammingandProvingwithDistributed
Protocols. In Symposium on Principles of Programming Languages (POPL), 2018.
[51]B.H. Sigelman,L.A.Barroso,M.Burrows,P.Stephenson,M.Plakal, D.Beaver,
S. Jaspan, and C. Shanbhag. Dapper, a large-scale distributed systems tracing
infrastructure. Technical report, Google, Inc., 2010.
[52]N. Walkinshaw, M. Roper, M. Wood, and N. W. M. Roper. The Java System
Dependence Graph. In International Workshop on Source Code Analysis and
Manipulation (SCAM), 2003.
[53]R.J.Walls,Y.Brun,M.Liberatore,andB.N.Levine. Discoveringspecification
violationsinnetworkedsoftwaresystems.In InternationalSymposiumonSoftware
Reliability Engineering (ISSRE), 2015.
[54]J. R. Wilcox, D. Woos, P. Panchekha, Z. Tatlock, X. Wang, M. D. Ernst, and T. An-derson.Verdi:AFrameworkforImplementingandFormallyVerifyingDistributed
Systems. In ConferenceonProgrammingLanguageDesignandImplementation
(PLDI), 2015.
[55]W.Xu,L.Huang,A.Fox,D.Patterson,andM.I.Jordan. DetectingLarge-Scale
System Problems by Mining Console Logs. In Symposium on Operating Systems
Principles (SOSP), 2009.
[56]M. Yabandeh, A. Anand, M. Canini, and D. Kostic. Finding Almost-Invariants in
Distributed Systems. In International Symposium on Reliable Distributed Systems
(SRDS), 2011.
[57]J.Yang,T.Chen,M.Wu,Z.Xu,X.Liu,H.Lin,M.Yang,F.Long,L.Zhang,and
L. Zhou. MODIST: Transparent Model Checking of Unmodified Distributed
Systems. In SymposiumonNetworkedSystemsDesignandImplementation(NSDI),
2009.
[58]P. Zave. Using Lightweight Modeling to Understand Chord. SIGCOMM Comput.
Commun. Rev., 42(2):49–57, Mar. 2012.
[59]X. Zhao, Y. Zhang, D. Lion, M. F. Ullah, Y. Luo, D. Yuan, and M. Stumm. Lprof: A
Non-intrusive Request Flow Profiler for Distributed Systems. In Symposium on
Operating System Design and Implementation (OSDI), 2014.
1159
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:57:59 UTC from IEEE Xplore.  Restrictions apply. 