On the Dichotomy of Debugging Behavior Among Programmers
Moritz Beller
Niels Spruit
m.m.beller@tudelft.nl
spruit.niels@gmail.com
Delft University of Technology
The NetherlandsDiomidis Spinellis
dds@aueb.gr
Athens University of Economics and
Business
GreeceAndy Zaidman
a.e.zaidman@tudelft.nl
Delft University of Technology
The Netherlands
ABSTRACT
Debugging is an inevitable activity in most software projects, often
difÔ¨Åcult and more time-consuming than expected, giving it the nick-
name the ‚Äúdirty little secret of computer science.‚Äù Surprisingly, we
have little knowledge on how software engineers debug software
problems in the real world, whether they use dedicated debugging
tools, and how knowledgeable they are about debugging. This study
aims to shed light on these aspects by following a mixed-methods
research approach. We conduct an online survey capturing how 176
developers reÔ¨Çect on debugging. We augment this subjective survey
data with objective observations on how 458 developers use the
debugger included in their integrated development environments
(IDEs) by instrumenting the popular E CLIPSE and I NTELLI J IDEs
with the purpose-built plugin W ATCH DOG2.0. To clarify the in-
sights and discrepancies observed in the previous steps, we followed
up by conducting interviews with debugging experts and regular de-
bugging users. Our results indicate that IDE-provided debuggers are
not used as often as expected, as ‚Äúprintf debugging‚Äù remains a fea-
sible choice for many programmers. Furthermore, both knowledge
and use of advanced debugging features are low. These results call
to strengthen hands-on debugging experience in computer science
curricula and have already reÔ¨Åned the implementation of modern
IDE debuggers.
CCS CONCEPTS
‚Ä¢Software and its engineering Software testing and debug-
ging;
ACM Reference format:
Moritz Beller, Niels Spruit, Diomidis Spinellis, and Andy Zaidman. 2018.
On the Dichotomy of Debugging Behavior Among Programmers. In Proceed-
ings of ICSE ‚Äô18: 40th International Conference on Software Engineering,
Gothenburg, Sweden, May 27-June 3, 2018 (ICSE ‚Äô18), 12 pages.
DOI: 10.1145/3180155.3180175
1 INTRODUCTION
Debugging, the activity of identifying and Ô¨Åxing faults in soft-
ware [ 1], is a tedious, but inevitable activity in almost every software
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô18, Gothenburg, Sweden
¬©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5638-1/18/05. . . $15.00
DOI: 10.1145/3180155.3180175development project [ 2]. Not only is it inevitable, but according to
Kernighan and Plauger [ 3] and Zeller [ 4], so difÔ¨Åcult that it often
consumes more time than creating the bogus piece of software in
the Ô¨Årst place.
During debugging, software engineers need to relate an observed
failure to its underlying defect [ 5]. To complete this step efÔ¨Åciently,
they often need to acquire a deep understanding and build a mental
model of the software system at hand [ 6]. This is where modern
debuggers come in: they aid software engineers in gathering observ-
ing the system‚Äôs dynamic behavior. However, they still require them
to select the parts on which to focus and to perform the deductive
reasoning to pinpoint the fault from the observed behaviors.
While the scientiÔ¨Åc literature is rich in terms of proposals for
(automated) debugging approaches, e.g., [ 4,7‚Äì10], there is a gap in
knowledge of how practitioners actually debug. Debugging has thus
remained the dirty little secret of computer science [11]. How and
how much do software engineers debug at all? Do they use modern
debuggers? Are they familiar with their capabilities? Which other
tools and strategies do they know?
The lack of knowledge regarding developers‚Äô debugging behavior
is in part due to an all too human characteristic: admitting, demon-
strating, and letting others do research on how one approaches what
are essentially one‚Äôs own faults is a precarious situation for both
a developer and a researcher. Nevertheless, by continuing to keepdebugging practices secret, we miss an important opportunity for
advancing software engineering theory and for delivering efÔ¨Åciency
improvements in software development practice.
Knowledge on how developers debug can help researchers to
invent more practice-relevant techniques, educators to improve their
debugging curricula, and tool builders to tailor debuggers to the
actual needs of developers. To open up the art of debugging, we con-
ducted a large-scale behavioral Ô¨Åeld study on what developers think
about debugging and how they debug in their IDEs. The following
main questions steer our research:
RQ1 What do developers know about debugging and how do
they reÔ¨Çect on it?
RQ2 How do developers debug in their IDEs?
RQ3 How do individual debugger users and experts interpret our
Ô¨Åndings from RQ1 and RQ2?
The key contributions of this paper are:
‚Ä¢A triangulated, large-scale empirical study of how developers
debug in reality using a mixed methods approach, supported by a
replication package.1
1https://archive.org/details/debugging-replication-package
5722018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden Moritz Beller, Niels Spruit, Diomidis Spinellis, and Andy Zaidman
	

	
 	






Figure 1: Research design overview.
‚Ä¢The addition of debugging features in W ATCH DOG2.0, an open-
source, multi-platform infrastructure that allows detailed tracking
of developers‚Äô debugging behavior.2
‚Ä¢Improvement suggestions for current IDE debuggers that have in
part already been implemented in practice.
Research Design
To answer these research questions, we employed a multi-faceted
research approach outlined in Figure 1. 1/circlecopyrtWe conducted an online
survey to capture developers‚Äô opinions on debugging and obtained an
overview of the state of the practice (see Section 3, Survey Results,
SR). 2/circlecopyrtSimultaneously, we began using the automated W ATCH DOG
2.0 infrastructure to track developers‚Äô Ô¨Åne-grained debugging activi-
ties in the IDE (see Section 4, W ATCH DOGResults, WR). By instru-
menting the IDE, we obtained objectively measured data, which we
can compare against subjective, but richer data from the survey. We
came up with a list of several, sometimes conÔ¨Çicting, observations
that needed further explanation. 3/circlecopyrtTo help us explain the Ô¨Åndings
in depth, we conducted interviews with developers, some of whom
are actively developing debugging tools (see Section 5, Interviews).
2 RELATED WORK
Work related to our study comprises debugging tools, processes,
techniques, empirical debugging evidence, and IDE instrumentation.
Debugging Tools. By ‚Äúdebuggers,‚Äù we usually mean symbolic de-
buggers, such as the GNU Project Debugger (GDB) [ 12]. These
debuggers allow developers to specify points in the program where
the execution should halt, breakpoints. A typical symbolic debugger
supports different types of breakpoints, such as line, method, data
access, or more advanced exception orclass prepare breakpoints,
and options to reÔ¨Åne the breakpoint [ 13]. Examples include speci-
fying a conditional breakpoint,ahit count,asuspension policy,o r
whether the entire program or one thread should pause upon hitting
a breakpoint.
Once a program halts, developers can use the symbolic debug-
ger to permanently watch or ad-hoc inspect memory entities such
as variables, work through the call stack, line-wise step through
the code, or evaluate arbitrary expressions [ 12,13]. Graphical de-
buggers like the early dbxtool [14] and DDD [ 15] evolved from
command line symbolic debuggers, such as V AX DEBUG [ 16],
dbx[17], and GDB [ 18]. Most symbolic debugging features have
since been integrated in the integrated graphical debuggers of IDEs,
such as E CLIPSE , Visual Studio, NetBeans, and I NTELLI J. This
study focuses on how developers use IDE debuggers.Debugging Process.
Researchers have developed systematic pro-
cess descriptions of debugging and recommendations to reduce the
time programmers have to spend on Ô¨Ånding and Ô¨Åxing a defect that
2https://testroots.org/testroots_watchdog.htmlcauses a program failure. We investigate whether developers explic-
itly or implicitly use debugging strategies inspired by the scientiÔ¨Åc
method; for example, Zeller‚Äôs TRAFFIC approach [ 4] comprises
seven steps that cover every action in the debugging process, from
the discovery of a problem until the correction of the defect. Three
of the steps regard ‚Äúthe most time consuming‚Äù Find-Focus-Isolate
loop, as developers often need to follow them iteratively to Ô¨Ånd the
root cause of a failure. Therefore, much research has gone into tech-
niques to, at least partially, automate this loop to reduce debugging
effort [19].
In 1991, Gilmore suggested a new psychological model to under-
standing debugging [ 20]. Component 1 of his model, namely that
debugging is a ‚ÄúÔ¨Çexible, incomplete comprehension process [...] ac-
cording to task demands, tools and skill,‚Äù provides a theory-grounded
description of our observations.
Automated Debugging Techniques. Arguably the most researched
debugging technique is delta debugging, which can be used to sys-
tematically narrow down possible failure causes by comparing a
successful and an erroneous program execution [ 21]. Other types of
debugging technique include slicing [4], focusing on anomalies [4],
mining dynamic call graphs [22],statistical debugging [23],spectra-
based fault localization [7],angelic debugging [8],data structure
repair [10],relative debugging [24],automatic breakpoint genera-
tion[25],automatic program Ô¨Åxing using contracts [9], and combina-
tions thereof [ 26‚Äì30]. Orso presents a detailed overview of some of
these automated debugging techniques [ 31]. However, as our study
shows, automated debugging techniques have not yet reached the
mainstream debugging practices and are not part of IDE debuggers.
As such, we do not discuss them further.
Empirical Debugging Evidence. Only few studies exist that empir-
ically evaluate how developers debug.
Perhaps most closely related to our study, Perscheid et al. and
Siegmund et al. studied debugging practices of professional soft-
ware developers [ 5,32] via a survey and manual observations of
each of their eight participants over ‚Äúsome hours during one work-
day‚Äù through think-aloud protocols and short interviews. Despite the
fact that our studies differ signiÔ¨Åcantly in population, length, and
methodology, we could replicate most of their key results: the wide
use of printf, a lagging adoption of advanced debugging tools and
features, and developers‚Äô generally low education on debugging. We
partly reÔ¨Åned these observations, showing 1) that there is a strongdichotomy on printf use among developers, 2) which debuggingfeatures are empirically used and 3) that the complexity of operat-ing debuggers is a main reason for these usage patterns. As in our
survey, concurrency issues and external libraries seem to be the root
causes of the hardest bugs. However, we also partly refuted [35]:
Our developers did not run the debugger in 91% of IDE sessions and
they did not spend a ‚Äúhuge amount of their daily work‚Äù [ 32]i nt h e
debugger, but less than 14%.
In a general 2006 study on how developers use Eclipse, Murphy
et al. found that 90% of their 41 studied developers used the debug-
ger [33]. This is similar to our debugger use rate in WR1 when only
considering the top 10% of users. Parnin and Rugaber observed that
13% of their 10,000 recorded sessions included debugging, com-
pared to an IDE debugger use in 9% of the sessions in our study [ 34]
(WR1).
573
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. On the Dichotomy of Debugging Behavior Among Programmers ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
Despite differences in study populations and methods, Layman
et al. found similar challenges and improvement wishes such as
concurrency (SQ13) and back-in-time debugging [ 35]. However,
they do not mention some of the critical challenges found in this
paper, such as debugging across languages.
Piorkowski et al. studied qualitatively how programmers forage
for information [ 36,37]. They found that developers spent half of
their debugging time foraging for information. This complements
our study as it shows what parts of the IDE are often used for Ô¨Ånding
information during debugging.
B√∂hme et al. studied individual steps in the debugging process,
i.e., how developers localize, diagnose, and Ô¨Åx faults [ 38,39]. Through
an experiment with 12 professional software engineers they observed
that fault localization is complex due to errors from an interactions
of several statements. They also found that participants diagnosedbugs in a remarkably similar way. However, when Ô¨Åxing a fault,
while the patches submitted by the participants were plausible, only
58% were correct.IDE Instrumentation.
Petrillo et al. developed the Swarm Debug
Infrastructure (SDI), which ‚Äúprovides [Eclipse] tools for collecting,
sharing, and retrieving debugging data‚Äù [ 40]. Developers can use
the collective knowledge of previous debug sessions to ‚Äúnavigate se-
quences of invocation methods‚Äù and ‚ÄúÔ¨Ånd suitable breakpoints.‚Äù SDI
was evaluated in a controlled experiment involving 10 developers.
Our E CLIPSE instrumentation for RQ2 is technically similar to SDI,
but focuses on understanding current behavior. To increase gener-
alizability, we also support I NTELLI J and performed a longitudinal
study of how dozens of developers debug in the wild.
While several W ATCH DOG-like plugins for IDE-instrumentation
exist [ 41‚Äì44], none of them have been used to study the debugging
behavior of developers, manifesting our knowledge gap of empirical
debugging. Ko and Myers showed the practical usefulness of the
‚Äúlive IDE‚Äù wish expressed in SQ13 with their Whyline prototype [ 19],
which helps developers reason about assumed program behavior.
3 DEBUGGING SURVEY
In this section, we describe our online survey.
3.1 Research Methods
Survey Design. To investigate developers‚Äô self-assessed knowledge
on debugging for RQ1, we set up a survey, consisting of 13 short
questions (SQ1‚ÄìSQ13) organized in four sections; the Ô¨Årst gathers
general information about the respondents, such as programmingexperience and favorite IDE. The second asks if and how respon-dents use the IDE-provided debugging infrastructure. Developers
who do not use it were asked for the reason why, while others got
questions on speciÔ¨Åc debugging features, thus assessing how well
the respondent knows and uses several types of breakpoints. In ad-
dition, we asked questions about other debugging features ranging
from stepping through code to more advanced features like editing atrun time (hot swapping). The third part, presented to all respondents,
assessed the importance of codiÔ¨Åed tests in the debugging process;
we gauged whether the participant uses tests for reproducing bugs,
checking progress, or to verify possible bug Ô¨Åxes. SQ13 was an
open, non-mandatory question about participants‚Äô opinion on thestatement ‚Äúthe best invention in debugging was printf debugging,‚Äùinspired by Brian Kernighan‚Äôs quote ‚Äú[t]he most effective debugging
tool is still careful thought, coupled with judiciously placed printstatements‚Äù [
45,46]. We included the question because research
on survey design has shown that posing a concrete, controversial
statement that evokes strong opinions leads to more insightful an-
swers [ 47]. Before publicly releasing the survey, we sharpened it in
several iterations and ran six trials with outsiders.
Card Sort. To gain an overview of the topics that concern develop-
ers, we performed an open card sort [48]o n SQ13. The Ô¨Årst two
authors individually built and then mutually agreed on a set of 33tags from a sub-sample of responses. After labeling all responses
(possibly with multiple labels), the fourth author sampled
20% of
the tagged responses, re-tagged them independently and compared
them to the reference tag set. We then converged our tag sets into a
homogeneous classiÔ¨Åcation with 34 tags, agreed upon by all authors.
Dependency Analysis. To gain insights into the correlation between
survey answers, we performed statistical tests. For SQ7‚Äì12,w e
had to convert each categorical answer to an ordinal scale using alinear integer transformation on its rank. This was sound becauseour predeÔ¨Åned answer options have a naturally ranked order (‚ÄúIdon‚Äôt know‚Äù = 1, ‚ÄúI know‚Äù = 2, . . . ). We then computed a pair-
wise Pearson Chi-Squared ( œá2) test of independence [49], as we are
dealing with categorical variables. If variables depended on eachother (
Œ±=0.05), we calculated the strength of their relationship
with a Spearman rank-order correlation test for non-parametric
distributions [ 50]. For interpreting the results of dependency analyses
œÅ, we use Hopkins‚Äô guidelines [ 51]. They call 0‚â§|œÅ|<0.3no,
0.3‚â§|œÅ|<0.5weak, 0.5‚â§|œÅ|<0.7moderate and 0.7‚â§|œÅ|‚â§1
a strong correlation.Subject Recruitment.
To attract survey participants (SP), we spread
the link to the survey through social media, especially Twitter, and
via an in-IDE W ATCH DOGregistration dialog, advertising a rafÔ¨Çe
with three 15 Euro Amazon vouchers.
Study Subjects. We attracted 176software developers who com-
pleted our survey. The majority of them have at least three years of
experience in software development, with a third over 10 years ( <1
year: 2.8% , 1‚Äì2 years: 6.8% , 3‚Äì6: 31.8% , 7‚Äì10: 21.6% ,>10years
36.9% ).84.1% indicated that they use Java, followed by 55.1% for
JavaScript and 39.2% for Python. The languages PHP, C, C++ and
C# were each selected by around 25% of participants, followed by
R(16.5% ), Swift ( 6.3% ) and Objective-C ( 5.1% ). Finally, 44devel-
opers indicated the use of another language ( 24different in total),
of which Scala ( 11) and Ruby ( 8) prevail. The most used IDEs are
Eclipse ( 31.8% ), IntelliJ ( 30.7% ), and Visual Studio ( 11.9% ). We
asked for the language to understand whether we can compare the
survey results to our Java-based Ô¨Åeld study, and because certain lan-
guage features deÔ¨Åne their debugging possibilities, for example the
availability of a virtual machine in Java [ 52] or Pharo‚Äôs introspection
design concept, which lends itself to debugging [53, 54].
3.2 Results
Analysis of Survey Answers. In this section, we describe key re-
sults of our survey and RQ1.
SR1: Most developers use IDE debuggers in conjunction with log
Ô¨Åles and print statements. In our Ô¨Årst question, 143 developers
574
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden Moritz Beller, Niels Spruit, Diomidis Spinellis, and Andy Zaidman
(81.3% ) indicated that they use the IDE-provided debugging in-
frastructure, 15(8.5% ) that they do not, and 18(10.2% ) that their
selected IDE does not have a debugger. Besides using the IDE debug-
ger, respondents indicated they examine log Ô¨Åles ( 72.2% ), followed
closely by using print statements ( 71.6% ). Other answers included
the use of an external program ( 21.0% ), or additional other, internal
or non-generalizable techniques ( 30.1% ).19developers indicated
the use of a complementary method, of which adding or running
tests and using web development tools built into the browser were
mentioned most (both four times).
SR2: Developers not using the IDE debugging Ô¨Ånd external pro-
grams, tests, print statements, or other techniques more effective or
efÔ¨Åcient. Of the 15developers not using the debugging infrastructure,
eight think that print statements and six that techniques other than
print statements are more effective or efÔ¨Åcient. Six use an external
program, while four do not know how to use a debugger.
SR3: Line breakpoints are used by the vast majority of developers.
More advanced types are unknown to most. The143developers us-
ing an IDE debugger were asked more detailed questions on whether
they know and use speciÔ¨Åc debugging features. The Likert scale
plots in Figure 2 show that most developers are familiar with line,
exception, method and Ô¨Åeld breakpoints, while temporary line break-
points and class prepare breakpoints are known by fewer developers.
The vast majority of developers also uses line breakpoints, but other
breakpoint types are used by less than half of the respondents; Class
prepare breakpoints are used by almost none.
SR4: Most developers answered to be familiar with breakpoint condi-
tions, but not with hit counts and suspend policies. Figure 2 indicates
that the majority of developers specify conditions on breakpoints.However, specifying the hit count or setting a suspend policy are
both known and used less. The results in Figure 2 show that over
80% of the developers seem to know all major debugging features
found in modern IDEs, strengthening Siegmund‚Äôs Ô¨Åndings [ 5]. The
more advanced features, like deÔ¨Åning watches or a suspend policy,
seem to be known and used less.
SR5: Survey answers indicate testing is an integral part of the debug-
ging process, especially at the beginning and end. Figure 3 shows
the use of codiÔ¨Åed tests throughout the debugging process based on
all176responses. It indicates that tests are often used at the start and
end of the debugging process, for reproducing bugs and verifying
bug Ô¨Åxes, but slightly less during the process.
SR6: Experience has limited to no impact on the usage of the IDE-
provided debugging infrastructure and tests. Examining our survey
answers for dependencies allows us to understand how certain an-swers relate, for example whether and how strongly programmer
experience correlates with the use of debugger features like break-
points, watches or the use of testing to guide debugging. We Ô¨Ånd
that there is no correlation between the use of an IDE debugger or
(unit) tests for debugging and experience in software development.
There is a weak correlation between experience and specifying hit
counts and a moderate correlation between experience and the usage
of watches during debugging.
SR7: Developers who use tests for reproducing bugs are likely to use
them for checking progress and very likely to use them for verifying1%
18%
28%
25%
46%
74%90%
57%
47%
44%
31%
6%9%
25%24%
31%
24%20%Line breakpoint
Temporary line breakpoint
Class prepare breakpointMethod breakpointException breakpoint
Field watchpoint
100 50 0 50 100
18%
34%
68%57%
23%
13%25%
43%
19%Specifying a condition
Specifying hit/pass count
Setting the suspend policy
1%2%
4%
7%
15%12%
19%90%90%
80%
73%
60%60%
47%10%8%
16%
20%
26%28%
34%Stepping through the codeInspecting variable values
Inspecting the call stack
Defining watchesEvaluating expressions
Modifying variable values
Editing code at runtime
100 50 0 50 100
Percentage
Response I don't know I know I know and I use
Figure 2: Answers in SQ7‚Äì9 on breakpoint types, breakpoint
options, and debugging features (n =143).
20%
25%
47%80%
75%
53%Reproducing bug
Checking debugging progressVerifying possible bug fixes
100 50 0 50 100
Percentage
Response No Y es
Figure 3: Answers in SQ10‚Äì12 on unit tests (n =176).
bug Ô¨Åxes. We also Ô¨Ånd that there is a moderate correlation between
the use of tests at the beginning and end of the debugging process to
reproduce and verify bug Ô¨Åxes, and a weak to moderate correlation
between using tests at the beginning or end and throughout the
process for checking progress.
Card Sorting. In total, 108respondents gave a response to the state-
ment that ‚Äúthe best invention in debugging still was printf debugging.‚Äù
In the open card sorting process, we identiÔ¨Åed 34different tags. To
understand important topics and their co-occurrence, we use an
intuitive graph-based representation of the tag structure. Vertices
correspond to the tags and undirected, weighted edges to the strengthof relation between two tags. The size of the vertices in is determined
by the occurrence frequency of the tag, while the weight of the edges
is determined by the relative number of co-occurrences. To ease
the interpretation the graph, (1) we normalized the weights of the
edges based on the occurrence frequencies of its end points, (2) we
Ô¨Åltered out all edges with a very low normalized weight (cleaning the
graph from ‚Äúbackground noise‚Äù), and (3) we removed vertices that
did not have any outgoing or incoming edge (removing unrelated
concepts). The resulting graph in Figure 4 allows us an intuitive un-
derstanding and overview of responses and how they relate to each
other, without having to read hundreds of responses [ 55].3The tags
abbreviate concepts given as answers by survey respondents and are
self-explanatory. The tags ‚Äòdebugger jittery‚Äô, ‚Äòdebugger overhead‚Äôand ‚Äòdebugger interference‚Äô mean that respondents think debug-
gers have too much impact on the thinking process, performance or
program execution, respectively. ‚ÄòFirst printf‚Äô means that develop-
ers Ô¨Årst use printf debugging and ‚Äòbefore debugger‚Äô indicates that
3We explicitly avoided statistical tests. Given open-ended survey answers, the meaning
of such tests is unclear, or might convey a false sense of statistical precision at worst.
The graph conveys our understanding having intensely worked with survey answers.
575
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. On the Dichotomy of Debugging Behavior Among Programmers ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden






Figure 4: Intuitive visualization of the tag network in SQ13.
developers use some other technique(s) before using the debugger.
As the two main strongly connected subgraphs 1/circlecopyrtand 2/circlecopyrtin Fig-
ure 4 suggest, there was a strong dichotomy between survey respon-
dents: Many enthusiastically agreed with our statement (‚ÄúTotally
agree!‚Äù, SP13, SP23), while others rejected it, stating that ‚Äú[p]eople
saying that never learned how to use a debugger‚Äù ( SR54). Developers
mostly seemed to agree that IDE debuggers are methodologically
superior to print statements, explaining the strong link 2/circlecopyrt. Indepen-
dently, reasons for resorting back to printf are when no debugger is
available or when the presence of the debugger interferes with the
program execution order 3/circlecopyrt. Many of the respondents who agreed
with the statement also saw drawbacks of printf debugging, like
SP10: ‚ÄúPrint is often most Ô¨Çexible but often least efÔ¨Åcient.‚Äù De-
velopers indicated to use printf debugging as an ad-hoc, universal
technique that is easy to do and often the Ô¨Årst step in a possibly
longer debugging strategy. However, sometimes it is not enough 4/circlecopyrt,
as a combination of techniques is required. The answers also pointedto problems with IDE debuggers: they are sometimes too jittery, pro-
vide too many features and are not suited for concurrent debugging
as they interfere too much. Moreover, their complicated graphicaluser interface (GUI) can get in the way of working (fast). Instead
of printf debugging, developers seemed to prefer a live IDE with
a console that has a read-eval-loop (REPL)
5/circlecopyrt. Summarizing this
discussion, SP75 concluded that ‚Äúprintf is travelling by foot, a GUI
debugger is travelling [by] plane. You can go to more places by foot,
but you can only go that far.‚Äù Few developers also tried to avoid
debugging by testing better 6/circlecopyrt.
4 IDE FIELD STUDY
In this section, we describe our Ô¨Åeld study with W ATCH DOG2.0.
4.1 Study Methods
Data Collection. To investigate the debugging habits of developers
in the IDE, we extended our W ATCH DOGinfrastructure [ 56‚Äì59]
to also track developers‚Äô debugging behavior for RQ2, resulting in
WATCH DOG2.0 for both E CLIPSE and I NTELLI J. We had previ-
ously used W ATCH DOGas a research vehicle to verify common
expectations and beliefs about testing [ 56‚Äì58]. W ATCH DOGis tech-
nically centered around the concept of intervals that capture the
start and end of common development activities like reading andwriting code as well as running JU NITtests. We extended its interval
concept to cover debugging sessions and introduce a new, orthogo-
nal concept, singular events, that unlike intervals have no end date.
Such events track when developers add, change or remove break-
points, for example. An IDE session is an uninterrupted sequence of
WATCH DOGintervals in which the developer does not close the IDE
or suspend the computer. A debugging session is an IDE session, in
which the developer used the debugger at least once.Analysis Methods.
To analyze the data collected with W ATCH DOG
2.0, we created an open-source data processing pipeline. The pipeline,
which comprises 4,000 lines of code, extracts the data from W ATCH -
DOGS‚ÄôM ONGO DB and loads it into R for further analysis. The
analysis methods we used for some of these research questions
require some more explanation detailed below.
For RQ2.1 and RQ2.2, we assessed activity measured via
WATCH DOGintervals. For RQ2.4, we assessed the intervals that
occur before a debugging session is started. We chose a search range
of16seconds before, matching the interval inactivity timeout of 16
seconds in W ATCH DOG. This means that activity-based intervals
like reading or typing intervals are automatically closed after thisperiod of inactivity to account for e.g. coffee breaks. A timeout
length of 15 seconds is standard in IDE plugins [41, 58].
For RQ2.4 and RQ2.5, we consider a Ô¨Åle ‚Äúunder debugging‚Äù if
we receive reading or typing intervals during a debugging interval
on it, i.e. for all the Ô¨Åles the user steps through, reads, or otherwise
modiÔ¨Åes during a debugging session.Subject Recruitment.
To attract participants to our Ô¨Åeld study, we
relied on W ATCH DOG‚Äôs recruitment processes [ 58]. Users could join
or leave the Ô¨Åeld study at any time.Study Subjects.
Since the release of W ATCH DOG2.0 on 22 April
2016, we collected user data for a period over two months, until
28 June 2016. Of the 458 users, 21% come from China, 12% from
India, 12% from the US, 5% from Brazil, 4% from Germany, and
the remaining 46% from 65 other countries. Users could opt to share
their programming experience, and 186 (41%) did: 68% had up
to two years of programming experience, 16% between three to
six years, and 16% seven years and more. Nine users were running
MacOS X (5%), 14 Linux (6%), 162 Windows (89%), and 272 chose
not to answer. Our study includes a heterogeneous mix of private,
open-source, and commercial projects, with sizes ranging from green
Ô¨Åeld projects to several 100,000 lines of code
The median study participation was 6 days (mean: 13 days), the
maximum the full 66 days. In this period, we received 1,155,189
intervals from 458users in 603projects. Of these, 3,142 were debug
intervals from 132developers. In total, we recorded 18,156 hours
in which the IDE was open, which amounts to 10.3 observed de-
veloper years based on the 2015 average working hours for OECD
countries [ 60]. We also collected 54,738 debugging events from 192
users, 218projects and 723IDE sessions. Only 48 users ( top10%)
are responsible for 90% of the sessions, but they represent a globally
diverse population with varying experience and companies working
in different domains (consultancies, tool creators, Ô¨Ånancial institu-
tions, mobile application development). In total, we recorded both
at least one debug interval and one event for 108 users.
576
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden Moritz Beller, Niels Spruit, Diomidis Spinellis, and Andy Zaidman
4.2 Results
In this section, we describe key results of our W ATCH DOG2.0
observational Ô¨Åeld study for RQ2.
RQ2.1: How prevalent and frequent is IDE debugging?
WR1: The majority of developers does not use the IDE-provided de-
bugging infrastructure. Table 1 presents the number of occurrences
of the different event types. Only 132of the 458users ( 28.8% )
started a debugging session during the data collection period, with
no signiÔ¨Åcant difference between E CLIPSE (28.9% ) and I NTELLI J
(27.6% ) users. Of these, 108study subjects ( 23.6% ) have used the
debugger and at least one of its features (transferred both intervals
and events). In top10%, every user had at least one debugging session
(100% debugger use). No debugger use is therefore likely a result
of little transferred data. However, it is not contradictory to use the
debugger and not transfer any of the events listed in Table 1, since
the debugger provides several other beneÔ¨Åts like hot-swapping of
code. In total, we observed a debugger run in 9% of all 723 IDEsessions. In the onward analyses, we only take into account data
from users who used the debugger.
WR2: About 20% of the developers are responsible for over 80% of
the debugging intervals in our sample. For RQ2.1 we are interested
in knowing the frequency and length of debugging sessions. We
Ô¨Årst analyzed the number of debug intervals per user for the 132
developers that have used the debugger during the collection period.
The resulting numbers range from a single debug interval to 598
debugging intervals, with an average of 23.8 and a median of 4
debug intervals per user. Next, we analyzed the duration of the 3,142
debug intervals and found values ranging from 3milliseconds to
90.8 hours, with an average and median duration of 13.8 minutes
and42.3 seconds, respectively. About half of the users using the
IDE-provided debugging infrastructure have launched the debugger
four times or less during the data collection, 21% launched their
debugger more than 20 times.
RQ2.2: How much time is spent in IDE debugging?
WR3: Debugging consumes, on average, less than 14% of the active
in-IDE development time. For RQ2.2, we Ô¨Årst computed the total
duration of all intervals of a particular type and based it on the
total duration of ‚ÄòIDE open‚Äô intervals (18,156.9 hours, 100% )i n
the collection period. We recorded 25.2 hours of running unit tests
(0.1% ), 721.5 hours of debugging intervals ( 4.0% ), 2,568.8 hours of
reading ( 14.1% ), and 1,228.6 hours of typing ( 6.8% ). These intervals
are the main contributors of how developers spend their time inthe IDE, included in the ‚ÄòIDE active‚Äô intervals (
28.9% ). Next, we
analyzed the duration and percentages on a per user basis. For the
users with at least one debug interval, Table 2 shows the descriptive
statistics of the interval duration and percentages. From the results
in Table 2 and the fact that the total recorded active IDE time was
5250.7 hours, we conclude that debugging consumes 13.7% of the
total active in-IDE development time, while reading or writing code
and running tests take 48.6%, 23.4% and 0.5%.
WR4: Most debugging sessions consume less than 10 minutes. Fur-
thermore, about half of the debugging sessions take at most 40
seconds, while about 12% of them last more than 10 minutes.
RQ2.3: Which IDE debugger features do developers use?WR5: Line breakpoints are used most and by most developers, other
breakpoint types are used less and by fewer developers. The results
in Table 1 show that line breakpoints are by far the most used break-
point type. The other, more advanced, types account for less than
7%of all breakpoints set during the collection period. Furthermore,
line breakpoints are used by most developers using the debugging
infrastructure, while the other types of breakpoints are used by only
7.6‚àí20.5% of these developers.
WR6: Breakpoint options are not used by most WatchDog 2.0 users;
the most frequently used option is changing their enablement. When
considering how breakpoints evolve over their lifetime, the break-
point change type frequencies in Table 1 (second column) indicate
that almost all of these changes are related to the enablement or
disablement of the breakpoints. The other change types account for
only 10.9% of all breakpoint changes. Furthermore, the number of
users that generated these events range from 1(0.8% )t o12(9.1% ).
Moreover, events related to specifying a hit count on the breakpoint
have not been recorded during the collection period.
WR7: Setting breakpoints and stepping through code is done most,
other debugging features are far less used. Table 1 shows that most
of the recorded debugging events are related to the creation (4,544),
removal (4,362) or adjustment of breakpoints, hitting them during
debugging and stepping through the source code. The more advanced
debugging features like deÔ¨Åning watches and modifying variable
values have been used much less. Furthermore, the same holds for
the number of users generating these events: the majority of users
have added and/or removed breakpoints and stepped through thecode, while only
2.3‚àí15.2% modiÔ¨Åed variable values, evaluated
expressions and/or deÔ¨Åned watches.
RQ2.4: What is the relation between testing and debugging?
WR8: Most debugging sessions start after reading or changing the
code, not after running tests. Regarding RQ2.4, we assessed the
intervals that occur immediately before a debugging session starts.
The resulting frequencies and their percentages of all intervals occur-
ring before any debug interval are: 46(0.5% ) for running unit tests,
119(1.2% ) for other debug intervals, 4,991 ( 51.9% ) for reading and
1,802 ( 18.7% ) for typing intervals. About 70% of the debugging
sessions start after reading or writing code, only 0.5% of them start
after a failing or passing test run.
WR9: Developers who spend more time executing tests are likely
to proportionally debug more. Next, we investigated the relation
between the total duration of running unit tests and debug intervals
per user. We only considered the 25developers with at least one
debug interval and one unit test execution. At œÅ=0.58, we Ô¨Ånd a
moderate correlation between the two duration spans.
WR10: Developers who read or modify test classes longer are not
likely to debug less. To complete RQ2.4, we studied the relation
between the amount of time the user spends inside test classes (i.e.,
either reads or modiÔ¨Åes tests) and the debugging time. For the 248
developers with at least one debug interval or one opened test class,
we Ô¨Ånd no correlation at œÅ=‚àí0.08 . Furthermore, we Ô¨Ånd no corre-
lation ( œÅ=0.23) when focusing on the 84users with both at least
one debug interval and one opened test class.RQ2.5: How are Ô¨Åle length and debugging effort related?
577
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. On the Dichotomy of Debugging Behavior Among Programmers ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
Table 1: Frequencies of breakpoint types, modiÔ¨Åcations, and W ATCH DOG2.0 debugging events.
Breakpoint type Frequency Breakpoint modiÔ¨Åcation Frequency Event type Frequency Event type Frequency
Class prepare 99 Change condition 3 Add breakpoint 4,544 (continued)
Exception 37 Disable condition 1 Change breakpoint 247 Resume client 8,292
Field 78 Enable condition 19 Remove breakpoint 4,362 Suspend by breakpoint 13,276
Line 4,229 Disable 180 DeÔ¨Åne watch 343 Suspend by client 16
Method 77 Enable 40 Evaluate expression 101 Step into 3,480
UndeÔ¨Åned 24 Change suspend policy 4 Inspect variable 179 Step over 19,543
Modify variable value 4 Step out 351
Œ£4,544 Œ£247 (continuing ...) Œ£54,738
Table 2: Descriptive usage statistics for key interval types (relative to total observed time).
Variable Unit Min 25% Median Mean 75% Max Log-Histogram
Debugging Hours (%) 0.00 (0.0%) 0.03 (0.1%) 0.30 (0.5%) 5.47 (2.5%) 1.42 (2.4%) 333.70 (30.8%)
Reading Hours (%) 0.00 (0.0%) 0.14 (1.7%) 0.60 (3.2%) 5.70 (4.9%) 2.07 (5.7%) 591.10 (52.7%)
Typing Hours (%) 0.00 (0.0%) 0.21 (1.5%) 1.01 (3.6%) 2.95 (4.8%) 2.78 (6.9%) 63.87 (28.3%)
Running JU NITtests Hours (%) 0.00 (0.0%) 0.00 (0.0%) 0.01 (0.0%) 0.68 (0.2%) 0.56 (0.2%) 9.19 (2.1%)
WR11: Smaller classes are debugged more than larger classes. Here
we examined whether there is a correlation between the Ô¨Åle size
of a class (in source lines of code [ 61]), and the number of times
the developer visits it in the source code editor in a debugging
session. At œÅ=‚àí0.75 , we Ô¨Ånd a strong negative correlation. We also
investigated the relation between the Ô¨Åle sizes and the duration of
the debug intervals in which they are opened and found no apparent
correlation ( œÅ=0.19). For RQ2.5, we aggregated and compared the
number of classes in single debug intervals to: (1) the total number of
classes we observed with W ATCH DOGfor this project (also through
other intervals such as reading, writing, or running tests); and (2) the
number of different classes that have been debugged during any
debug interval of the project.
For 1), we found that on average only 4.8% (median: 1.7% )o f
all project classes we observed in W ATCH DOGintervals were ever
debugged. The value ranges from 0.2% to100% , where the 100% -
cases possibly stem from small projects with only one or two classes.
For 2), the results range from 0.8% to100% with an average of
14.5% (median: 4.5% ). Both results seem to indicate that debugging
is focused on a relatively small set of classes in the project. In
75% of debugging sessions, at most 5%of the project‚Äôs classes are
debugged.
RQ2.6: Do developers often step over the point of interest?
WR12: Developers might step over the point of interest and have
to start over again in 5% of debugging sessions. To answer RQ2.6,
we Ô¨Årst computed the total duration of all debug intervals per user.
Then, we performed a Spearman rank-order correlation test using
these values and the programming experience the user entered dur-
ing W ATCH DOG2.0‚Äôs registration process by applying a linear
integer transformation (see Section 3.2). For the 58users that have
entered their experience and generated at least one debug interval,
this resulted in a weak correlation ( œÅ=0.38), i.e. more experienced
developers are more likely to spend more time in the IDE debugger.
During our research into debugging, we sometimes heard anecdo-
tal reports of frustrated developers stepping over the point of interest
while debugging. To this end, we sought objective data to support
how severe the problem is by identifying possible cases of steppingover the point of interest. ‚ÄúStepping over‚Äù means that the developer
steps one time too far and has to start debugging all over again. Rea-
sons for this include pressing the proceed key too fast or realizing
too late that the actually interesting location was in a past step. To
model this with our interval and event concept, we look for a set
of debug intervals that satisfy the following conditions: (1) the last
event occurring within the debug interval is a stepping event; and
(2) the interval is followed by another debug interval in the same
IDE session. We then created subsets of these debug intervals by
imposing a maximum time tmaxbetween two consecutive debug
intervals. Figure 5 shows the possible cases of stepping over the
point of interest for the subsets with tmax‚â§15 minutes.
The trend line in Figure 5 shows that the amount of new possible
cases of stepping over the point of interest starts to decrease signif-
icantly after about four minutes. At this point, about 150possible
overshoot cases can be identiÔ¨Åed, which corresponds to 4.8% of the
debugging intervals.
050100150200
0 250 500 750
Maximum time between debu g intervals (in seconds)Number of possible overshoot cases
Figure 5: Possible cases of stepping over the point of interest per
maximum time period between consecutive debug intervals.
578
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden Moritz Beller, Niels Spruit, Diomidis Spinellis, and Andy Zaidman
Table 3: Interviewed developers and debugging experts
ID Occupation Dev. Experience Country Area
I1 Freelancer >20 years Germany Rich Client Platforms
I2 Developer ‚â•15 years India E-commerce
I3 Developer 11 years USA Real-Time Systems
I4 Developer 10 years UK Data Scraping
E1 3 Eclipse Debugging Project Leaders Switzerland, India Eclipse Development
E2 Professor >20 years Greece Software Engineering
E3 Debugger Developer 18 years Russia IDE Development
5 INTERVIEWS
In this section, we describe how we conducted developer interviews
forRQ3 and merge and discuss results from RQ1 and RQ2.
5.1 Study Methods
Interview Design & Method. To validate and obtain a deeper un-
derstanding of our Ô¨Åndings from RQ1 and RQ2 and to mitigate
apparent controversies, we ran the combined observations from sur-
vey, objective IDE measurements, and anecdotal interview insights
across two sets of debugging experts. A question sheet helped us
steer the semi-structured interviews, which we conducted remotely
via Skype and took from 36 minutes to 67 minutes. In one case (E3),
we performed the interview asynchronously via email. Subsequently,
we transcribed the interviews and extracted insightful quotes.
Study Subjects. Table 3 gives an overview of our nine interviewees.
We sampled the set of ‚Äúregular developers‚Äù from our survey popu-
lation to gain insights into what hinders the use of debuggers, why
printf debugging is still widely used, and whether they regularly
step over the line of interest. We chose the experts based on their
industrial and academic position in the debugging Ô¨Åeld.
5.2 Results
This section juxtaposes survey ( RQ1) and IDE study (RQ2) results
and discusses them with the qualitative insights from RQ3.
Use of the IDE Debugger. InWR1, we found that two thirds of the
WATCH DOG2.0 users were not using the IDE-provided debugger
in our observation period, an obvious contradiction to SR1, in which
80% of respondents claimed to use it. Moreover, no single user spent
more than 30% of his development time debugging. There might be
several reasons for the discrepancy: 1) The study populations aredifferent, and the survey respondents were likely self-selecting on
their interest in debugging, resulting in a higher than real use of the
debugger. 2) As often observed in user studies, most relevant datastems from a relatively small percentage of users. 3) W
ATCH DOG
users were free to start and stop using the plugin at any time in the
observation period. Hence, for some users the actual observation
period might be much shorter, perhaps coinciding with not having
to debug a problem. 4) Almost equally many developers conceded
to use printf statements for debugging in SR2. We have anecdotal
evidence from RQ3 that they might use them even more: When
we asked I3about printf debugging, he was very negative about it.
Later in the interview, he still conceded to use printf ‚Äúvery rarely.‚Äù
We believe a similar observation might hold for many W ATCH DOG
users. As we cannot capture printf debugging or debugging outside
the IDE with W ATCH DOG, our Ô¨Ånding does not mean two thirds
of developers did not debug. 5) The phenomenon of a discrepancy
between survey answers and observed behavior is not new. Belleret al. observed a similar phenomenon with developers claimingto spend more time on testing than they really were [
57]. As a
consequence, we emphasize their Ô¨Ånding that survey answers always
be cross-validated by other methods.
Printf Debugging. From RQ1 and RQ2, it seemed that developers
were well-informed about printf debugging and that it is a conscious
choice if they employ it, often the beginning of a longer debugging
process. Interviewees praised printf as a universal tool that one
can always resort back to, helpful when learning a new languageecosystem, in which one is not yet familiar with the tools of the
trade. About left-over print statements that escape to production, I2
was ‚Äúnot worried at all, because we have a rigorous code review
process.‚Äù While frequently used, developers are also aware of its
shortcomings, saying that ‚Äúyou are half-way toward either telemetry
or toward tracing‚Äù and ‚Äúthat it is insufÔ¨Åcient for concurrent programs,
primarily because the [output] interleave[s] in strange ways‚Äù (I3).Use of Debugging Features.
SR3and SR4indicated that most devel-
opers use line breakpoints, but do not use more advanced breakpoint
types like class prepare breakpoints. While many developers knew
and used conditional breakpoints, they were widely ignorant of hit
counts and the debugger‚Äôs other more advanced functions. WR5 to
WR7 support this result, Ô¨Ånding that conditional breakpoints are
indeed the second most feature in the IDE debugger. A similar result
is visible in other debugging features like stepping through code. In
both cases we found that these features get used less as they become
more advanced. However, the observed numbers on the use of these
features are much lower than the claimed usage visualized in Fig-
ure 2. For example, while 60% of the survey respondents indicated
to deÔ¨Åne watches during debugging, only 15.2% of the W ATCH DOG
2.0 users who use the debugger have deÔ¨Åned a watched expression.
Through our interviews with the debugging experts, we identify
three possible causes for this.
1) More advanced debugging features are seldom required. I1 and
I2said that specifying conditions or hit counts is often ‚Äúfuzzy (is it
going to happen the 16th, 17th, or 18th time?)‚Äù and that once one
knows the condition, one almost automatically understands the prob-
lem. Then, there is no need for the conditional breakpoint anymore.
Moreover, ‚Äúthe types of problems where you need a conditionalbreakpoint happen very rarely‚Äù (I2). For example, when we pre-sented the breakpoint export feature of E
CLIPSE toI2, he replied
‚ÄúI did not know such a feature exists.‚Äù Others said it is a ‚Äúvery eso-
teric thing‚Äù and that they have used it ‚Äúmaybe once or twice‚Äù (I3).
This strengthens our intuition that debugging is an internal thought
process not usually shared and that breakpoints are ‚Äúlike a one-shot.
Ideally I wouldn‚Äôt like them to be, but I just set them anew‚Äù (I4).
2) Debuggers are difÔ¨Åcult to use. Another reason given by inter-
viewees, even though seasoned engineers, was that ‚Äúthe debugger
is a complicated beast‚Äù (I2) and that ‚Äúdebuggers that are available
now are certainly not friendly tools and they don‚Äôt lend toward
self-exploration.‚Äù Given our results on the use of features, we asked
interviewees whether it might simply be enough to reduce the feature
set. Both developers and E1toE3emphatically declined, arguing
that ‚Äúonce you get into these crazy cases, they are really useful‚Äù ( I2).
3) There is a lack of knowledge on how to use the debugger . When
we asked developers where their knowledge of debugging comes
from, many said that ‚Äúbig chunks are self-taught‚Äù and ‚Äú[I] picked
579
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. On the Dichotomy of Debugging Behavior Among Programmers ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
up various bits and pieces on the Internet‚Äù (I4). Even I3, the only
interviewee who indicated that ‚Äúdebugging was explicitly covered
[in my undergraduate],‚Äù said it is ‚Äúpartly self taught, partly [...]
through key mentor ships." Making a case for hands-on teaching,
he elaborated that ‚Äúone of the engineers that mentored me [...] was
some kind of wizard with GDB. I think when you meet someone
who knows a very powerful tool it‚Äôs very impressive and their speed
to resolving something is much faster but it takes a lot of time toget to that point.‚Äù Since we measured experience to have limited
to no impact on (which) debugging features developers used (SR6 ),
this hints at a lack of education on debugging that is pervasive from
beginners and Computer Science students to experts. New Computer
Science curricula that put debugging upfront could be an effective
way to steer against it [62].
Time Effort for Debugging. Our study results WR3 toWR4 point
to the fact that debugging in most cases is a short, ‚Äúget-it-done‚Äù (I1)
type of activity that, with only 14% of active IDE time (WR3)w e
found to consume signiÔ¨Åcantly less than the 30‚àí90% for testing
and debugging reported by Beizer [ 63] and the estimations by our
interviewees, who gave a range of 20% to 60% of their active work
time. One reason why our measured range is so much lower might
be that developers (and humans in general) have a tendency to over-
estimate the duration of unpleasant tasks, as previously observed
with testing [ 57]. Another is that developers included debugging
tasks such as printf and the use of external tools, which we cannot
measure. We need more studies to quantify this initial surprising
Ô¨Ånding. A common intuition in Software Engineering is that ‚Äúsmall
is better,‚Äù since it is easier to manage and understand, see for ex-ample the recommendations to micro services, small commits, or
short Ô¨Åles. Contrary to this claim, we found that short classes need
considerably more debugging (WR11) and that the longer amount of
time developers spend in larger classes does not nearly compensate
for it. Our interviewees agreed in unison that the hardest problems todebug are ones where interfaces or transactions between components
are involved. Interfaces are typically short since they contain little
logic, but represent a common source of integration problems andthus, the answers suggest, debugging effort. Then, longer classes
are likely to have increased locality of features, which makes them
often easier to understand [ 64] and thus probably also easier to trou-
bleshoot. We need more research on this interaction between Ô¨Åle
length and debugging probability. Future studies could try to exploit
the Ô¨Ånding to recommend optimal system designs as a compromise
between modularity and the ability to debug them.Use of Tests for Debugging.
In the survey, most respondents think
(unit) testing is an integral part of the debugging process, especially
for reproducing bugs at the beginning of the process (SR5, SR7 ).
However, there is mixed evidence on this in RQ2, as shown by WR8,
WR9 and WR10. On the one hand, failing tests do not seem to be
a trigger for the start of debugging sessions. On the other hand,running tests in the IDE seems to be correlated with debugging
more, while reading or modifying tests is not. Two factors can play
a role: Developers who are more quality concerned execute their
tests more often and therefore also debug more. This is contrary to
intuition and the answers of some of our interviewees, who claimed
that as testing goes up, the debugging effort should decrease (E2):
‚ÄúDebugging is born of unknowns, and effective testing reduces these‚Äù(I3). An explanatory Ô¨Ånding might be that the creation of tests itself
adds code and complexity that might need to be debugged. We need
more studies to research this interesting discovery.Stepping Over the Point of Interest.
We found that in less than
about 5%of the debugging sessions the developer might have stepped
over the point of interest and had to start debugging anew ( WR12).
This indicates that there is a limited, but existent gap in current de-
buggers process that might be Ô¨Ålled by back-in-time debuggers [65].
Back-in-time debuggers allow developers to step back in the pro-gram execution in order to arrive at the point of interest withouthaving to completely restart the debugging process. All our inter-
viewees could relate to situations in which this occurred to them,
stating that ‚Äúit happens all the time‚Äù (I1) to ‚Äúback in time debugger
would be wonderful‚Äù (I3). However, WR12 indicates that it might not
be as frequent as some stated. While the drop frame feature allows
developers to go to the beginning of the current method, it does
not revoke side effects that already occurred and was therefore only
found to be ‚Äúhelpful in a limited way‚Äù (I3). Currently, mainstream
IDEs do not support back-in-time debugging.Improvements in IDE Debuggers.
We asked our interviewees how
debugger creators could better support them. Their answers fallinto two categories: 1) Make the core features easier to use while
preserving all existing functionality. 2) Create tools that capture the
holistic debugging process better. Elaborating on 2), I1denotes: ‚ÄúIf
you‚Äôre in Java and have to debug across language boundaries, [...] you
really get to a point where you feel helpless.‚Äù Other wishes included
the ability to do back-in-time debugging similar to C HRONON [66],
to have a live REPL, a feature the IDE XCODE introduced [67].
To improve the design of existing IDE debuggers with Ô¨Åndings
from our study, we arranged a meeting with three debugging project
leads from E CLIPSE ,E1, and an IDE developer from a commercial
company, E2. The E CLIPSE leads said that, while they had sporadic
evidence on how some individual developers use their debugger, they
were unaware of the debugging behavior of a large population and
the usage detail our study could provide. They started or updated six
feature requests for the debugger based on our study, commencing
work on bugs that had been dormant since 2004.4In the following,
we focus on two already implemented features that are scheduled to
roll out as part of Eclipse release 4.7.
In our Ô¨Åeld study and interviews, we identiÔ¨Åed left-over break-
points as a recurrent annoyance, which developers have to removemanually, with I1saying that suspending on old breakpoints unex-
pectedly interrupts his Ô¨Çow and that ‚Äúevery so often, once a week or
so, I just delete all of them.‚Äù After making the Eclipse leads aware
of this problem, they implemented age deprecation for breakpoints.
It lets developers remove old breakpoints with one click. Although
often referred to as a ‚Äúdirty hack‚Äù (since it interferes with and pol-lutes production code), our study found that printf debugging alsoprovides an advantage over the debugger‚Äôs watch view in that it
preserves the history of past logs (for example, of memory entities
in the watch view). Conversely, developers cannot enrich third-party
libraries for which no source code is available with printf statements,
but they can place debugger breakpoints in e.g. their Java byte code.
To keep a history of logs when using the debugger, before our study,
Eclipse and IntelliJ users had to set up an artiÔ¨Åcial construction of
4See umbrella bug 498469: https://bugs.eclipse.org/bugs/show_bug.cgi?id=498469 .
580
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden Moritz Beller, Niels Spruit, Diomidis Spinellis, and Andy Zaidman
placing a conditional breakpoint that would print the information and
always return false, thus never suspend. This hack of a ‚Äúconditional
breakpoint that is not conditional‚Äù (Bugtracker description) required
intimate familiarity with the idiosyncrasies of the debugger and had
bad performance, since code embedded in conditional breakpoints
runs via the Java Debugging Infrastructure, which adds unnecessary
overhead for a simple printout. By offering the new breakpoint type
‚Äútracepoint,‚Äù developers can now conveniently produce fast logs of
debug traces. The Eclipse project implemented this simpliÔ¨Åed solu-
tion in Bug 71020, which had been in hibernation since 2004 and on
which work commenced after our discussion.
6 THREATS TO V ALIDITY
In this section, we examine threats to the validity of our study and
show how we mitigated them.
Construct Validity. The manual implementation of new functional-
ity, such as the addition of the debug infrastructure to W ATCH DOG,
is prone to human errors. To minimize these risks, we extended
WATCH DOG‚Äôs automated test suite. Furthermore, we use this test
suite to make sure we introduced no regressions. In addition, we
tested our plugins manually. Finally, we performed rigorous codereviews before we integrated the changes. Debug sessions mightnot correspond to actual debug work, e.g. a user might have inad-
vertently left the debugger in the IDE running, explaining our 90
hour outlier. However, such outliers are expected in an observational
study of several months [ 56,58]. Similarly, we approximate the
number of classes in a project by the number of different classes we
observe with W ATCH DOG. Due to privacy reasons, we cannot mine
the repositories of projects to gain an entirely correct Ô¨Ågure.Internal Validity.
Since our survey in RQ1 dealt with debugging,
participation might have been self-selecting, i.e. developers more
interested and knowledgeable in debugging are more likely to have
responded. We tried to contrast this with objective W ATCH DOG
observations, which is not advertised speciÔ¨Åcally as a debugging tool.
An important internal threat is that the populations for RQ1 and RQ2
are different and their intersection is small (six users participated
in both studies). However, we are conÔ¨Ådent we only encounter a
small sampling or comparison bias because key characteristics ofboth populations are similar, as 1) 80% of respondents answeredthe survey for Java, which both plugins work with in RQ2, 2) the
majority in RQ1 used one of the IDEs supported in RQ2,3 )t h e
experience distributions of both populations are similar and 4) both
populations should be large enough to even out individual inÔ¨Çuences.
Due to the fact that W ATCH DOGgathers data automatically, it is
harder for potentially evil-minded users to fabricate data than in
surveys. Moreover, that the majority of data comes from a relatively
small ‚Äúpower user‚Äù population (48 developers in our case, top10%in
WR1) is both normal in service use, for example on Twitter [ 68], and
other observational studies [ 41,69]. Discrepancies between some
survey answers and the objective IDE observations have previously
been observed in other studies [58].External Validity.
During our data collection period of more than
two months we collected 1,155,189 intervals with a total duration of
over ten developer years, spread over 458users. The fact that over
80% of the survey respondents stem from the Java community means
that little survey data is available about other communities. The sameholds for the analysis of the W
ATCH DOG2.0 data, which is restrictedto the Java programming language and to the E CLIPSE and I NTELLI J
IDEs. Other IDEs are not included in our analysis and the results with
them might deviate. However, at least imperative, statically typedlanguages similar to Java, like C, C++, C#, or Objective-C, would
likely yield similar results and are so widespread that researching
them alone impacts many, if not the majority of, developers.
7 CONCLUSION
We set out to obtain a Ô¨Årst cross-validated understanding of devel-
opers‚Äô debugging knowledge and contrasted it with their real-world
IDE debugging behavior.
We found strong dichotomies in developers‚Äô opinions, knowledge,
and behavior: Many believe modern debuggers to be superior to
printf debugging, yet still employ it for many good reasons. IDEobservations conÔ¨Årmed this Ô¨Ånding, as only a third of developers
ever invoked the debugger. We found that debugging is a technique
deÔ¨Åned by necessities: It is a relatively fast-paced and short-lived ac-
tivity that is by nature so complicated that the tools around it should
be as simple as possible. Consequently, developers use only basicfeatures and seldom resort to more advanced breakpoint types or
debugging techniques. Developers spend surprisingly little time in
the debugger; only 13% of their total development time on average,
in stark contrast to previous Ô¨Åndings claiming more than 50%. As
developers become more experienced, they seem to use the debug-
ger slightly more, possibly because they educated themselves on
its advanced affordances over printf debugging. We also found that
having more tests in the code generally does not reduce the debug-
ging burden, possibly because test code adds to the overall code that
needs debugging.
In general, developers‚Äô theoretical knowledge and practical use
of specialized debugging features are relatively shallow, just theamount that is seemingly sufÔ¨Åcient for their debugging problems.
Most developers said debugging was self-taught and not part of their
curriculum. We believe that more educators can include practical,
hands-on teaching, start in Ô¨Årst year courses. Astonishingly, althoughbugs are inevitably linked with software and students learn program-
ming in their introductory courses, they are only taught to properly
debug much later, if ever.
Adding to this lack of debugging education, even experienced
developers admitted that debuggers are not easy to use. Apart from
the wish for back-in-time debuggers, developers never expressed the
wish for more debugging features. Instead of introducing ever more
esoteric features, we therefore call to make using the already existingdebugger features easier to use and more accessible. With the help of
three E CLIPSE project leads, we identiÔ¨Åed several areas of improve-
ment in the E CLIPSE debugger, leading to new simpliÔ¨Åed debugging
features. One example is the introduction of a new breakpoint type in
Eclipse that combines the advantages of debugger-instrumentation
with the Ô¨Çexibility of printf debugging. Other IDE and debugger
creators could follow this example and use our Ô¨Åndings to further
improve their debuggers.
ACKNOWLEDGMENTS
We thank all study participants, who, in spite of showing their fal-
libility, allowed us to research their debugging behavior. We thank
Georgios Gousios and Earl Barr for reviewing this manuscript.
581
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. On the Dichotomy of Debugging Behavior Among Programmers ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]D. Spinellis, Effective Debugging: 66 SpeciÔ¨Åc Ways to Debug Software and
Systems. Addison-Wesley, 2016.
[2] Q. Zhao, R. Rabbah, S. Amarasinghe, L. Rudolph, and W.-F. Wong, ‚ÄúHow to do a
million watchpoints: efÔ¨Åcient debugging using dynamic instrumentation,‚Äù in Pro-
ceedings of the Joint European Conferences on Theory and Practice of Software
and 17th international conference on Compiler construction (CC‚Äô08/ETAPS‚Äô08).
Springer, 2008, pp. 147‚Äì162.
[3]B. W. Kernighan and P. J. Plauger, ‚ÄúThe elements of programming style,‚Äù The
elements of programming style, by Kernighan, Brian W.; Plauger , PJ New York:
McGraw-Hill, c1978., 1978.
[4] A. Zeller, Why Programs Fail, Second Edition: A Guide to Systematic Debugging,
2nd ed. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2009.
[5]B. Siegmund, M. Perscheid, M. Taeumel, and R. Hirschfeld, ‚ÄúStudying the ad-
vancement in debugging practice of professional software developers,‚Äù in Software
Reliability Engineering Workshops (ISSREW), 2014 IEEE International Sympo-
sium on. IEEE, 2014, pp. 269‚Äì274.
[6]P. Oman, C. Cook, and M. Nanja, ‚ÄúEffects of programming experience in de-
bugging semantic errors,‚Äù Journal of Systems and Software, vol. 9, no. 3, pp.
197‚Äì√¢ ÀòA¬∏S207, 1989.
[7] L. Naish, H. J. Lee, and K. Ramamohanarao, ‚ÄúA model for spectra-based software
diagnosis,‚Äù ACM Trans. Softw. Eng. Methodol., vol. 20, no. 3, pp. 11:1‚Äì11:32,
Aug. 2011.
[8] S. Chandra, E. Torlak, S. Barman, and R. Bodik, ‚ÄúAngelic debugging,‚Äù in Software
Engineering (ICSE), 2011 33rd International Conference on. IEEE, 2011, pp.
121‚Äì130.
[9]Y . Wei, Y . Pei, C. A. Furia, L. S. Silva, S. Buchholz, B. Meyer, and A. Zeller,
‚ÄúAutomated Ô¨Åxing of programs with contracts,‚Äù in Proceedings of the 19th Inter-
national Symposium on Software Testing and Analysis, ser. ISSTA ‚Äô10. ACM,
2010, pp. 61‚Äì72.
[10] M. Z. Malik, J. H. Siddiqi, and S. Khurshid, ‚ÄúConstraint-based program debugging
using data structure repair,‚Äù in Software Testing, V eriÔ¨Åcation and V alidation (ICST),
2011 IEEE F ourth International Conference on. IEEE, 2011, pp. 190‚Äì199.
[11] H. Lieberman, ‚ÄúThe debugging scandal and what to do about it (introduction to
the special section),‚Äù Commun. ACM, vol. 40, no. 4, pp. 26‚Äì29, 1997.
[12] R. Stallman, R. Pesch, S. Shebs et al., Debugging with GDB, 10th ed. Free
Software Foundation, 2011.
[13] E. Burnette, Eclipse IDE Pocket Guide. " O‚ÄôReilly Media, Inc.", 2005.
[14] E. Adams and S. S. Muchnick, ‚Äúdbxtool: A window-based symbolic debugger
for Sun workstations,‚Äù Software: Practice and Experience, vol. 16, no. 7, pp.
653‚Äì669, Jul. 1986.
[15] N. Matloff and P. J. Salzman, The Art of Debugging with GDB, DDD, and Eclipse .
San Francisco: No Starch Press, 2008.
[16] B. Beander, ‚ÄúV AX DEBUG: An interactive, symbolic, multilingual debugger,‚Äù in
Proceedings of the Software Engineering Symposium on High-Level Debugging,
M. Johnson, Ed. ACM SIGSOFT/SIGPLAN, Mar. 1983, pp. 173‚Äì179.
[17] B. Tuthill and K. J. Dunlap, ‚ÄúDebugging with dbx,‚Äù in UNIX Programmer‚Äôs
Supplementary Documents, V olume 1. Berkeley, California 94720: Computer
Systems Research Group, Department of Electrical Engineering and Computer
Science, University of California, Apr. 1986, 4.3 Berkeley Software Distribution.
[18] A. Zeller and D. L√ºtkehaus, ‚ÄúDDD ‚Äì a free graphical front-end for unix debuggers,‚Äù
ACM Sigplan Notices, vol. 31, no. 1, pp. 22‚Äì27, 1996.
[19] A. Ko and B. Myers, ‚ÄúDebugging reinvented,‚Äù in Software Engineering, 2008.
ICSE‚Äô08. ACM/IEEE 30th International Conference on. IEEE, 2008, pp. 301‚Äì
310.
[20] D. J. Gilmore, ‚ÄúModels of debugging,‚Äù Acta psychologica, vol. 78, no. 1, pp.
151‚Äì172, 1991.
[21] A. Zeller, ‚ÄúIsolating cause-effect chains from computer programs,‚Äù in Proceedings
of the 10th ACM SIGSOFT symposium on Foundations of software engineering.
ACM, 2002, pp. 1‚Äì10.
[22] F. Eichinger, K. Krogmann, R. Klug, and K. B√∂hm, ‚ÄúSoftware-defect localisation
by mining dataÔ¨Çow-enabled call graphs,‚Äù in Machine Learning and Knowledge
Discovery in Databases. Springer, 2010, pp. 425‚Äì441.
[23] S. Parsa, M. Vahidi-Asl, S. Arabi, and B. Minaei-Bidgoli, ‚ÄúSoftware fault local-
ization using elastic net: A new statistical approach,‚Äù in Advances in Software
Engineering. Springer, 2009, pp. 127‚Äì134.
[24] D. Abramson, C. Chu, D. Kurniawan, and A. Searle, ‚ÄúRelative debugging in an
integrated development environment,‚Äù Software: Practice and Experience, vol. 39,
no. 14, pp. 1157‚Äì1183, 2009.
[25] C. Zhang, J. Yang, D. Yan, S. Yang, and Y . Chen, ‚ÄúAutomated breakpoint genera-
tion for debugging,‚Äù Journal of Software, vol. 8, no. 3, 2013.
[26] J. R√∂√üler, G. Fraser, A. Zeller, and A. Orso, ‚ÄúIsolating failure causes through
test case generation,‚Äù in Proceedings of the 2012 International Symposium on
Software Testing and Analysis, ser. ISSTA 2012. ACM, 2012, pp. 309‚Äì319.
[27] Y . Lei, X. Mao, Z. Dai, and C. Wang, ‚ÄúEffective statistical fault localization using
program slices,‚Äù in Computer Software and Applications Conference (COMPSAC),
2012 IEEE 36th Annual, July 2012, pp. 1‚Äì10.[28] S. Parsa, F. Zareie, and M. Vahidi-Asl, ‚ÄúFuzzy clustering the backward dynamic
slices of programs to identify the origins of failure,‚Äù in Experimental Algorithms.
Springer, 2011, pp. 352‚Äì363.
[29] M. Perscheid and R. Hirschfeld, ‚ÄúFollow the path: Debugging tools for test-
driven fault navigation,‚Äù in Software Maintenance, Reengineering and Reverse
Engineering (CSMR-WCRE), 2014 Software Evolution Week-IEEE Conference
on. IEEE, 2014, pp. 446‚Äì449.
[30] K. Yu, M. Lin, J. Chen, and X. Zhang, ‚ÄúPractical isolation of failure-inducing
changes for debugging regression faults,‚Äù in Automated Software Engineering
(ASE), 2012 Proceedings of the 27th IEEE/ACM International Conference on.
IEEE, 2012, pp. 20‚Äì29.
[31] A. Orso, ‚ÄúAutomated debugging: Are we there yet?‚Äù https://www.youtube.com/
watch?v=WJHQnzLpVXk&feature=youtu.be, 2014, [Online; accessed 11 July
2016].
[32] M. Perscheid, B. Siegmund, M. Taeumel, and R. Hirschfeld, ‚ÄúStudying the ad-
vancement in debugging practice of professional software developers,‚Äù Software
Quality Journal, vol. 25, no. 1, pp. 83‚Äì110, 2016.
[33] G. C. Murphy, M. Kersten, and L. Findlater, ‚ÄúHow are Java software developers
using the Elipse IDE?‚Äù IEEE software, vol. 23, no. 4, pp. 76‚Äì83, 2006.
[34] C. Parnin and S. Rugaber, ‚ÄúResumption strategies for interrupted programming
tasks,‚Äù Software Quality Journal, vol. 19, no. 1, pp. 5‚Äì34, 2011.
[35] L. Layman, M. Diep, M. Nagappan, J. Singer, R. Deline, and G. Venolia, ‚ÄúDe-
bugging revisited: Toward understanding the debugging needs of contemporary
software developers,‚Äù in Empirical Software Engineering and Measurement, 2013
ACM/IEEE International Symposium on. IEEE, 2013, pp. 383‚Äì392.
[36] D. Piorkowski, S. D. Fleming, C. ScafÔ¨Ådi, M. Burnett, I. Kwan, A. Z. Henley,
J. Macbeth, C. Hill, and A. Horvath, ‚ÄúTo Ô¨Åx or to learn? how production bias
affects developers‚Äô information foraging during debugging,‚Äù in Proceedings of
the 2015 IEEE International Conference on Software Maintenance and Evolution
(ICSME). IEEE, 2015, pp. 11‚Äì20.
[37] D. J. Piorkowski, S. D. Fleming, I. Kwan, M. M. Burnett, C. ScafÔ¨Ådi, R. K.
Bellamy, and J. Jordahl, ‚ÄúThe whats and hows of programmers‚Äô foraging diets,‚Äù in
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
ACM, 2013, pp. 3063‚Äì3072.
[38] M. B√∂hme, E. O. Soremekun, S. Chattopadhyay, E. Ugherughe, and A. Zeller,
‚ÄúHow developers debug software the dbgbench dataset: Poster,‚Äù in Proceedings of
the 39th International Conference on Software Engineering Companion (ICSE
Companion). IEEE, 2017, pp. 244‚Äì246.
[39] ‚Äî‚Äî, ‚ÄúWhere is the bug and how is it Ô¨Åxed? an experiment with practitioners,‚Äù in
Proceedings of the 11th Joint Meeting of the European Software Engineering Con-
ference and Symopsium on the F oundations of Software Engineering (ESEC/FSE).
IEEE, 2017, pp. ??‚Äì??
[40] F. Petrillo, Z. Soh, F. Khomh, M. Pimenta, C. Freitas, and Y .-G. Gu√Él‚Äôh√Él‚Äôneuc,
‚ÄúUnderstanding interactive debugging with swarm debug infrastructure,‚Äù in Pro-
ceedings of the 24th International Conference on Program Comprehension.
ACM, 2016, pp. 1‚Äì4.
[41] S. Amann, S. Proksch, S. Nadi, and M. Mezini, ‚ÄúA study of visual studio usage in
practice,‚Äù in 23rd International Conference on Software Analysis, Evolution, and
Reengineering (SANER), vol. 1. IEEE, 2016, pp. 124‚Äì134.
[42] L. Hattori and M. Lanza, ‚ÄúSyde: a tool for collaborative software development,‚Äù
inProceedings of the International Conference on Software Engineering (ICSE).
ACM, 2010, pp. 235‚Äì238.
[43] S. Negara, N. Chen, M. Vakilian, R. E. Johnson, and D. Dig, ‚ÄúA comparative study
of manual and automated refactorings,‚Äù in Proceedings of the 27th European
Conference on Object-Oriented Programming, 2013.
[44] P. M. Johnson, ‚ÄúSearching under the streetlight for useful software analytics,‚Äù
IEEE software, vol. 30, no. 4, pp. 57‚Äì63, 2013.
[45] B. W. Kernighan, UNIX for Beginners . Bell Laboratories Murray Hill, NJ, 1978.
[46] D. Spinellis, ‚ÄúDebuggers and logging frameworks,‚Äù IEEE Software, vol. 23, no. 3,
pp. 98‚Äì99, May/June 2006.
[47] M. Das, P. Ester, and L. Kaczmirek, Social and behavioral research and the
internet: Advances in applied methods and research strategies. Routledge, 2010.
[48] D. Spencer, Card sorting: Designing usable categories. Rosenfeld Media, 2009.
[49] P. E. Greenwood and M. S. Nikulin, A guide to chi-squared testing. John Wiley
& Sons, 1996, vol. 280.
[50] J. L. Devore and N. Farnum, Applied Statistics for Engineers and Scientists .
Duxbury, 1999.
[51] W. G. Hopkins, A new view of statistics, 1997, http://newstatsi.org, Accessed 16
March 2015.
[52] C. S. Horstmann and G. Cornell, Core Java 2: V olume I, Fundamentals. Pearson
Education, 2002.
[53] J. Ressia, A. Bergel, and O. Nierstrasz, ‚ÄúObject-centric debugging,‚Äù in Proceed-
ings of the 34th International Conference on Software Engineering. IEEE Press,
2012, pp. 485‚Äì495.
[54] A. Chi¬∏ s, T. G√Ærba, and O. Nierstrasz, ‚ÄúThe moldable debugger: A framework for
developing domain-speciÔ¨Åc debuggers,‚Äù in International Conference on Software
Language Engineering. Springer, 2014, pp. 102‚Äì121.
582
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. ICSE ‚Äô18, May 27-June 3, 2018, Gothenburg, Sweden Moritz Beller, Niels Spruit, Diomidis Spinellis, and Andy Zaidman
[55] D. A. Keim, ‚ÄúVisual exploration of large data sets,‚Äù Commun. ACM, vol. 44, no. 8,
pp. 38‚Äì44, 2001. [Online]. Available: http://doi.acm.org/10.1145/381641.381656
[56] M. Beller, G. Gousios, A. Panichella, S. Proksch, S. Amann, and A. Zaidman,
‚ÄúDeveloper testing in the ide: Patterns, beliefs, and behavior,‚Äù IEEE Transactions
on Software Engineering, vol. PP, no. 99, pp. 1‚Äì1, 2017, to appear. Pre-print:
http://ieeexplore.ieee.org/document/8116886/.
[57] M. Beller, G. Gousios, A. Panichella, and A. Zaidman, ‚ÄúWhen, how, and why
developers (do not) test in their IDEs,‚Äù in Proceedings of the 2015 10th Joint
Meeting on F oundations of Software Engineering, ser. ESEC/FSE 2015. ACM,
2015, pp. 179‚Äì190.
[58] M. Beller, G. Gousios, and A. Zaidman, ‚ÄúHow (much) do developers test?‚Äù in
Proceedings of the 37th International Conference on Software Engineering (ICSE),
NIER Track. IEEE, 2015, pp. 559‚Äì562.
[59] M. Beller, I. Levaja, A. Panichella, G. Gousios, and A. Zaidman, ‚ÄúHow to catch
‚Äôem all: WatchDog, a family of IDE plug-ins to assess testing,‚Äù in Proceedings of
the 3rd International Workshop on Software Engineering Research and Industrial
Practice, ser. SER&IP ‚Äô16. ACM, 2016, pp. 53‚Äì56.
[60] Organisation for Economic Co-Operation and Development, ‚ÄúAverage an-nual hours actually worked per worker,‚Äù http://stats.oecd.org/index.aspx?
DataSetCode=ANHRS, 2015, [Online; accessed 11 July 2016].
[61] V . Nguyen, S. Deeds-Rubin, T. Tan, and B. Boehm, ‚ÄúA SLOC counting standard,‚ÄùinCOCOMO II F orum, vol. 2007, 2007.
[62] D. H. O‚ÄôDell, ‚ÄúThe debugging mind-set,‚Äù Communications of the ACM, vol. 60,
no. 6, pp. 40‚Äì45, 2017.
[63] B. Beizer, ‚ÄúSoftware testing techniques. 1990,‚Äù New York, ISBN: 0-442-20672-0 .
[64] ‚ÄúRemodularizing Java programs for improved locality of feature implementations
in source code,‚Äù Science of Computer Programming, vol. 77, no. 3, pp. 131‚Äì151,
2012.
[65] G. Pothier and √â. Tanter, ‚ÄúBack to the future: Omniscient debugging,‚Äù IEEE
software, vol. 26, no. 6, pp. 78‚Äì85, 2009.
[66] C. Systems, ‚ÄúChronon, a DVR for Java,‚Äù http://chrononsystems.com/, 2015, [On-
line; accessed 24 August 2016].
[67] A. Inc., ‚ÄúXcode 8,‚Äù https://developer.apple.com/xcode/, 2015, [Online; accessed
24 August 2016].
[68] G. M. Chen, ‚ÄúTweet this: A uses and gratiÔ¨Åcations perspective on how active
twitter use gratiÔ¨Åes a need to connect with others,‚Äù Computers in Human Behavior,
vol. 27, no. 2, pp. 755‚Äì762, 2011.
[69] R. Minelli, A. Mocci, and M. Lanza, ‚ÄúI know what you did last summer: aninvestigation of how developers spend their time,‚Äù in Proceedings of the 2015
IEEE 23rd International Conference on Program Comprehension. IEEE Press,
2015, pp. 25‚Äì35.
583
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:54:18 UTC from IEEE Xplore.  Restrictions apply. 