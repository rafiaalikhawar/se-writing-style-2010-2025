BigDebug: Debugging Primitives for Interactive Big Data
Processing in Spark
Muhammad Ali Gulzar, Matteo Interlandi, Seunghyun Y oo, Sai Deep T etali
T yson Condie, T odd Millstein, Miryung Kim
University of California, Los Angeles
ABSTRACT
Developers use cloud computing platforms to process a large quan-
tity of data in parallel when developing big data analytics. De-bugging the massive parallel computations that run in today’s data-centers is time consuming and error-prone. To address this chal-
lenge, we design a set of interactive, real-time debugging primi-
tives for big data processing in Apache Spark, the next generationdata-intensive scalable cloud computing platform. This requires re-thinking the notion of step-through debugging in a traditional de-
bugger such as gdb, because pausing the entire computation across
distributed worker nodes causes signiﬁcant delay and naively in-
specting millions of records using a watchpoint is too time con-suming for an end user.
First, B
IGDEBUG ’s simulated breakpoints and on-demand
watchpoints allow users to selectively examine distributed, inter-mediate data on the cloud with little overhead. Second, a usercan also pinpoint a crash-inducing record and selectively resumerelevant sub-computations after a quick ﬁx. Third, a user can de-
termine the root causes of errors (or delays) at the level of indi-
vidual records through a ﬁne-grained data provenance capability.Our evaluation shows that B
IGDEBUG scales to terabytes and its
record-level tracing incurs less than 25% overhead on average. Itdetermines crash culprits orders of magnitude more accurately and
provides up to 100% time saving compared to the baseline replay
debugger. The results show that B
IGDEBUG supports debugging at
interactive speeds with minimal performance impact.
Categories and Subject Descriptors
D.2.5 [Software Engineering]: Testing and Debugging—debug-
ging aids, distributed debugging, error handling and recovery
Keywords
Debugging, big data analytics, interactive tools, data-intensive scal-
able computing (DISC), fault localization and recovery
1. INTRODUCTION
An abundance of data in many disciplines of science, engineer-
ing, national security, health care, and business has led to the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citationon the ﬁrst page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc permissionand/or a fee. Request permissions from permissions@acm.org.
ICSE ’16, May 14 - 22, 2016, Austin, TX, USA
c/circlecopyrt2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2884781.2884813emerging ﬁeld of Big Data Analytics that run in a cloud computing
environment. To process massive quantities of data in the cloud, de-
velopers leverage Data-Intensive Scalable Computing (DISC) sys-tems such as Google’s MapReduce [14], Hadoop [3], and Spark[38]. These DISC systems expose a programming model for au-
thoring data processing logic, which is compiled into a Directed
Acyclic Graph (DAG) of data-parallel operators. The root DAGoperators consume data from some input source ( e.g., GFS [18] or
HDFS [33]), while downstream operators process the intermediateoutputs from DAG predecessors. Scaling to large datasets is han-
dled by partitioning the data and assigning tasks that execute the
operator on each partition.
Currently, developers do not have easy means to debug DISC
applications. The use of cloud computing makes application de-
velopment feel more like batch jobs and the nature of debugging
is therefore post-mortem. Developers are notiﬁed of runtime fail-
ures or incorrect outputs after many hours of wasted computingcycles on the cloud. DISC systems such as Spark do provide exe-cution logs of submitted jobs. However, these logs present only the
physical view of Big Data processing, as they report the number
of worker nodes, the job status at individual nodes, the overall job
progress rate, the messages passed between nodes, etc. These logsdo not provide the logical view of program execution e.g., system
logs do not convey which intermediate outputs are produced fromwhich inputs, nor do they indicate what inputs are causing incor-rect results or delays, etc. Alternatively, a developer may test theirprogram by downloading a small subset of Big Data from the cloudonto their local disk, and then run the DISC application in a local
mode. However, using a local mode, she may not encounter the
same failure, because the faulty data may not be included in thegiven data sample.
The vision of B
IGDEBUG is to provide interactive, real-time de-
bugging primitives for Big Data processing. Designing B IGDEBUG
requires re-thinking the traditional step-through debugging primi-tives as provided by tools such as gdb. Pausing the entire compu-
tation across distributed worker nodes causes signiﬁcant delay andreduces overall throughput. Naively inspecting millions of records
ﬂowing through a data-parallel pipeline is too time-consuming and
infeasible for an end user. B
IGDEBUG must tag how individual
records are ﬂowing through individual worker nodes and transferthe requested debug information from the distributed worker nodes
to the driver in an efﬁcient manner. In other words, B
IGDEBUG
must meet the requirements of low overhead, scalability, and ﬁne-
granularity, while providing expressive debugging primitives.
To solve these technical challenges, B IGDEBUG provides simu-
lated breakpoints, which create the illusion of a breakpoint withthe ability to inspect program state in distributed worker nodes
and to resume relevant sub-computations, even though the pro-
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   784
/g3/g4/g65/g79/g64/g77/g3/g60/g71/g71/g3/g68/g78/g3/g78/g60/g68/g63/g3
/g60/g73/g63/g3/g63/g74/g73/g64/g3/g72/g74/g77/g64/g3/g68/g78/g3
/g78/g60/g68/g63/g3/g79/g67/g60/g73/g3/g63/g74/g73/g64
/g79/g67/g60/g73/g3/g63/g74/g73/g64/g63/g74/g73/g64/g3/g72/g74/g77/g64/g3/g68/g78/g3/g78/g60/g68/g63/g3/g4/g65/g79/g64/g77/g3/g60/g71/g71/g3/g68/g78/g3
/g78/g60/g68/g63/g3/g60/g73/g63/g4/g65/g79/g64/g77/g3/g60/g71/g71
/g68/g78/g3
/g78/g60/g68/g63/g3
/g60/g73/g63/g3
/g63/g74/g73/g64
/g72/g74/g77/g64/g3
/g68/g78/g3
/g78/g60/g68/g63/g3
/g79/g67/g60/g73/g3
/g63/g74/g73/g64/g159/g4/g65/g79/g64/g77/g134/g188/g160/g3/g159/g60/g71/g71/g134/g188/g160
/g159/g68/g78/g134/g188/g160
/g159/g78/g60/g68/g63/g134/g188/g160/g3
/g159/g60/g73/g63/g134/g188/g160
/g159/g63/g74/g73/g64/g134/g188/g160/g3/g159/g72/g74/g77/g64/g134/g188/g160/g3
/g159/g68/g78/g134/g188/g160
/g159/g78/g60/g68/g63/g134/g188/g160
/g159/g79/g67/g60/g73/g134/g188/g160/g159/g63/g74/g73/g64/g134/g188/g160/g159/g4/g65/g79/g64/g77/g134/g188/g160/g3/g159/g60/g71/g71/g134/g188/g160
/g159/g68/g78/g134/g189/g160/g159/g60/g73/g63/g134/g188/g160
/g159/g72/g74/g77/g64/g134/g188/g160/g159/g78/g60/g68/g63/g134/g189/g160
/g159/g79/g67/g60/g73/g134/g188/g160
/g159/g63/g74/g73/g64/g134/g189/g160/g4/g65/g79/g64/g77/g134/g188/g3
/g60/g71/g71/g134/g3/g188
/g68/g78/g134/g3/g189
/g60/g73/g63/g134/g3/g188
/g72/g74/g77/g64/g134/g3/g188
/g78/g60/g68/g63/g134/g3/g189/g63/g74/g73/g64/g134/g3/g189
/g79/g67/g60/g73/g134/g3/g188/g9/g71/g60/g79/g16/g60/g75 /g16/g60/g75 /g21/g64/g63/g80/g62/g64/g5/g84/g14/g64/g84 /g6/g74/g71/g71/g64/g62/g79 /g23/g64/g83/g79/g9/g68/g71/g64
/g23/g60/g78/g70/g3/g188
/g23/g60/g78/g70/g3/g190/g23/g60/g78/g70/g3/g188
/g23/g60/g78/g70/g3/g190/g23/g60/g78/g70/g3/g189 /g23/g60/g78/g70/g3/g189/g22/g79/g60/g66/g64/g3/g188 /g22/g79/g60/g66/g64/g3/g189
Figure 1: Data transformations in word count with 3 tasks
gram is still running in the cloud. To help a user inspect millions
of records passing through a data-parallel pipeline, B IGDEBUG
provides guarded watchpoints , which dynamically retrieve only
those records that match a user-deﬁned guard predicate. B IGDE-
BUG supports ﬁne-grained forward and backward tracing at the
level of individual records by leveraging prior work on data prove-
nance within Spark [22]. To avoid restarting a job from scratch incase of a crash, B
IGDEBUG provides a real-time quick ﬁx and
resume feature where a user can modify code or data at runtime.
It also provides ﬁne-grained latency monitoring to notify a user
which records are taking much longer than other records.
We evaluate B IGDEBUG in terms of performance overhead, scal-
ability, time saving, and crash localizability improvement on threeSpark benchmark programs with up to one terabyte of data. With
the maximum instrumentation setting where B
IGDEBUG is enabled
with record-level tracing, crash culprit determination, and latency
proﬁling, and every operation at every step is instrumented withbreakpoints and watchpoints, it takes 2.5 times longer than the
baseline Spark. 2.5X is a very conservative upper bound, as a devel-
oper typically would not need to monitor the latency of each recordat every transformation and would not need to set breakpoints onevery operator. If we disable the most expensive record-level la-tency proﬁling, B
IGDEBUG introduces an overhead less than 34%
on average. We also measure the time saving achieved, since itsquick ﬁx and resume feature enables a user to recover appropriatelyfrom a crash and resume the rest of computation. We also com-pare B
IGDEBUG ’s capability to determine crash-inducing records
against baseline Spark, which reports failures at the level of tasks,
each of which handles millions of records.
BIGDEBUG offers the following contributions:
•BIGDEBUG provides breakpoints and watchpoints with min-
imal performance impact.
•BIGDEBUG exhibits less than 24% overhead for record-level
tracing, 19% overhead for crash monitoring, and 9% for on-demand watchpoint on average.
•B
IGDEBUG ’s quick ﬁx and resume feature allows a user to
avoid re-running a program from scratch, resulting in up to100% time saving.
•B
IGDEBUG narrows down the scope of failure-inducing
data by orders of magnitude through ﬁne-grained tracingof individual records within the distributed, data processing
pipeline.
Several frameworks [30, 23, 13] have been proposed to debug
DISC applications but none provides a comprehensive set of real-
time inspection features for DISC systems. Existing debuggers are
either a post-mortem replay debugger [13] or data provenance so-
lutions that keep track of which intermediate outputs are producedfrom which inputs by storing metadata in external storage [27, 21].
Our recent study ﬁnds that these solutions are limited in terms of
scalability and performance [22]. NEWT [27] can scale up to only100GB and this takes 85X of baseline Spark, in comparison to
B
IGDEBUG handling up to 1TB with only 2.5X .
BIGDEBUG is available for download at http://web.cs.ucla.edu/
~miryung/software.html. The website also includes B IGDEBUG ’s
manual and API description. The rest of the paper is organized asfollows. Section 2 discusses the background on large data-parallelprocessing in Apache Spark and why we chose Spark. Section 3
introduces a set of interactive debugging primitives with a running
example. Section 4 describes the design and implementation ofindividual features. Section 5 describes evaluation settings and theresults. Section 6 describes related work. Section 7 concludes withfuture work.
2. BACKGROUND: APACHE SPARK
Apache Spark [5] is a large scale data processing platform
that achieves orders-of-magnitude better performance than HadoopMapReduce [3] for iterative workloads. B
IGDEBUG targets Spark
because of its wide adoption—with over 800 developers and 200
companies leveraging its capabilities—and support for interactivead-hoc analytics, allowing programmers to explore the data as theyreﬁne their data-processing logic. Spark’s high-level programming
model provides over 80types of data manipulating operations and
supports language bindings for Scala, Java, Python, and R. Further-
more, a variety of domain speciﬁc extensions have been built onSpark, including MLLib [28] for machine learning, GraphX [19]for graph processing, SparkSQL [9] for relational queries, and sta-
tistical analysis in R. Spark can consume data from a variety of data
sources, including distributed ﬁle systems (like HDFS [33]), objectstores (like Amazon S3 [1]), key-value stores (like Cassandra andHBase), and traditional RDBMS (like MySQL and Postgres).
The Spark programming model can be viewed as an extension
to the MapReduce programming model that includes direct supportfor traditional relational algebra operators (e.g., group-by, join, ﬁl-ter), and iterative computations through a “for” loop language con-struct. These extensions offer orders-of-magnitude better perfor-
mance over previous Big Data processing frameworks like Apache
Hadoop [3] for iterative workloads like machine learning. Sparkalso comes with a relaxed fault tolerance model—based on work-ﬂow lineage [10]—that is built into its primary abstraction: Re-
silient Distributed Datasets (RDDs) [38], which exposes a set of
data processing operations called transformations (e.g., map, re-
duce, ﬁlter, group-by, join) and actions (e.g., count, collect).
Spark programmers leverage RDDs to apply a series of trans-
formations to a collection of data records (or tuples) stored in a
785distributed fashion e.g., in HDFS [33]. Calling a transformation on
an RDD produces a new RDD that represents the result of apply-
ing the given transformation to the input RDD. Transformations are
lazily evaluated. The actual evaluation of an RDD occurs when anaction is called. At that point the Spark runtime executes all trans-formations leading up to the RDD, on which it then evaluates theaction e.g., the count action counts the number of records in the
RDD. A complete list of transformations and actions can be foundin the Spark documentation [6].
1val textFile = spark.textFile("hdfs://...")
2val counts = textFile
3 .flatMap(line => line.split("" ))
4 .map(word => (word, 1))
5 .reduceByKey(_ +_).collect()
Figure 2: Scala word count application in Apache Spark
Figure 2 shows a word count program written in Spark using
Scala. The frequency of each unique word in the input text ﬁle iscalculated. It splits each word using a space as a separator, andmaps each word to a tuple containing the word text and 1 (the ini-tial count). The reduceByKey transformation groups the tuples
based on the word (i.e., the key) and sums up the word counts in
the group. Finally, the collect action triggers the evaluation of
the RDD referencing the output of the reduceByKey transfor-
mation. The collect action returns a list of tuples to the driver
program—containing each unique word and its frequency.
The Spark platform consists of three modules: a driver, a master,
and a worker. A master node controls distributed job execution
and provides a rendezvous point between a driver and the workers.The master node monitors the liveliness of all worker nodes andtracks the available resources (i.e., CPU, RAM, SSD, etc.). Worker
nodes are initiated as a process running in a JVM. Figure 3 shows
an example Spark cluster containing three worker nodes, a masternode, and a driver.
A Spark job consists of a series of transformations that end with
an action. Clients submit such jobs to the driver, which forwardsthe job to the master node. Internally, the Spark master translatesa series of RDD transformations into a DAG of stages , where each
stage contains some sub-series of transformations until a shufﬂe
step is required (i.e., data must be re-partitioned). The Spark sched-
uler is responsible for executing each stage in topological order,with tasks that perform the work of a stage on input partitions.
Each stage is fully executed before downstream dependent stagesare scheduled. The ﬁnal output stage evaluates the action. The ac-
tion result values are collected from each task and returned (via the
master) to the driver program, which can initiate another series oftransformations ending with an action.
Figure 1 represents the execution plan (stage DAG) for our word
count example in Figure 2. The input text is divided into three par-
titions. The driver compiles the program into two stages. Stage 1
applies the flatmap andmap transformations to each input par-
tition. A shufﬂe step is then required to group the tuples by the
word. Stage 2processes the output of that shufﬂe step by summing
up the counts for each word. The ﬁnal result is then collected andreturned to the driver. In this example, both stages are executed bythree tasks. It is also worth noting that each task runs on a sep-arate thread, so each worker may run multiple tasks concurrentlyusing multiple executors based on resource availability such as the
number of cores.
3. MOTIV ATING SCENARIO
This section overviews B IGDEBUG ’s features using a motivating
/g16/g60/g78/g79/g64/g77
/g26/g74/g77/g70/g64/g77
/g26/g74/g77/g70/g64/g77
/g7/g77/g68/g81/g64/g77
/g26/g74/g77/g70/g64/g77
/g189/g138/g3/g10/g64/g79/g3/g82/g74/g77/g70/g64/g77/g78
/g24/g78/g64/g77/g3
/g19/g77/g74/g66/g77/g60/g72
/g188/g138/g3/g21/g80/g73/g3/g19/g77/g74/g66/g77/g60/g72/g3
/g190/g138/g3/g6/g74/g73/g73/g64/g62/g79/g3/g79/g74/g3
/g82/g74/g77/g70/g64/g77/g78/g3/g60/g73/g63/g3/g60/g78/g78/g68/g66/g73/g3
/g79/g60/g78/g70/g78/g190
/g190
/g190/g191
/g191
/g191
/g191/g138/g3/g22/g64/g73/g63/g3/g79/g67/g64/g3/g77/g64/g78/g80/g71/g79/g3/g79/g74/g3/g79/g67/g64/g3/g63/g77/g68/g81/g64/g77
/g192/g138/g3/g22/g67/g74/g82/g3/g74/g80/g79/g75/g80/g79/g3/g79/g74/g3/g79/g67/g64/g3/g80/g78/g64/g77/g3
/g5/g68/g66/g7/g64/g61/g80/g66/g3
/g6/g74/g73/g79/g77/g74/g71/g78
/g136/g3/g7/g64/g61/g80/g66/g66/g64/g77/g3/g12/g73/g78/g79/g60/g73/g62/g64
/g189 /g136/g3/g6/g74/g72/g72/g80/g73/g68/g62/g60/g79/g68/g74/g73/g3/g6/g67/g60/g73/g73/g64/g71
Figure 3: Architecture of Spark with B IGDEBUG
example. Suppose that Alice writes a Spark program to parse and
analyze election poll logs. The log consists of billions of log entriesand is stored in Amazon S3. The size of the data makes it difﬁcult
to analyze the logs using a local machine only. Each log entry con-
tains the phone number, the candidate preferred by the callee, thestate where the callee lives, and a UNIX timestamp, for example:
249-904-9999 Clinton Texas 1440023983
1val log = "s3n://xcr:wJY@ws/logs/poll.log"
2val text_file = spark.textFile(log)
3val count = text_file
4 .filter( line => line.contains("Texas"))
5 .filter( line => line.split("" )[3].toInt
>1440012701)
6 .map(line = >(line.split("" )[1] , 1))
7 .reduceByKey(_ +_).collect()
Figure 4: Election poll log analysis program in Scala
Figure 4 shows the program written by Alice, which totals the
number of “votes” in Texas for each candidate, across all phone
calls that occurred after a particular date. Line 2 loads the log entry
data stored in Amazon S3 and converts it to an RDD object. Line 4selects lines containing the term ‘Texas.’ Line 5 selects lines whosetimestamps are recent enough. Line 6 extracts the candidate nameof each entry and emits a key-value pair of that vote and the number
1. Line 7 counts the votes for each candidate by summing by key.
Alice already tested this program by downloading the ﬁrst mil-
lion log entries from the Amazon S3 onto a local disk and running
the Spark program in a local mode. When she tests her program
with the subset of the data using a local mode, there is no fail-
ure. However, when she runs the same program on a much big-ger data stored in S3 using a cluster mode, she encounters a crash.Spark reports to Alice the physical view of the crash only—the typeof crash, in this case NumberFormatException, with a stack
trace, the id of a failed task, the id of an executor node encoun-tering the crash, the number of re-trials before reporting the crash,etc. However, such physical-layer information does not help Al-ice to debug which speciﬁc input log entry is causing the crash.
Though Spark reports the task ID of a crash, it is impossible for
Alice to know which records were assigned to the crashed executorand which speciﬁc entry is causing the crash. Even if she identiﬁesa subset of input records assigned to the task, it is not feasible forher to manually inspect millions of records assigned to the failed
task. She tries to rerun the program several times but the crash is
persistent, making it less probable to occur due to a hardware fail-ure in the cluster.Crash Culprit Determination. With B
IGDEBUG , Alice is pro-
vided with a speciﬁc record causing the crash, in this casea record, "312-222-904 Trump Illinois 2015-10-11.” B
IGDE-
786BUG ﬁrst reports a speciﬁc transformation responsible for the
crash as well—line 5 in Figure 4 at time.toInt, which
tries to change the timestamp from String to Integer, causing a
NumberFormatException. Using B IGDEBUG ’s backward
tracing feature, Alice then locates the speciﬁc log entry in the S3input causing the crash. She then sees that this log entry uses atimestamp in Date format rather than UNIX format.
RealTime Code Fix and Resume. Without B
IGDEBUG , Alice can
only modify the input data and restart the job from scratch, incur-
ring wasted computation in running Amazon EC2 services. Using
BIGDEBUG , Alice can ﬁx the code on the ﬂy by replacing the orig-
inal ﬁlter at line number 5 with the following one and resuming thefailed computation.
1filter{ line =>
2 var time = line.split("" )[3]
3 val date = new Date("YYYY-MM-DD")
4 if( date.checkFormat( time ) )
5 time = date.getTimeInUnix( time )
6 time.toInt >1440012701 }
Guarded Watchpoint. Even after ﬁxing the crash, Alice sees that
the total number of votes found by the program is greater than whatshe expected to ﬁnd. Using B
IGDEBUG ’s breakpoint, Alice inves-
tigates the intermediate result right after the second transformationat line number 5 in Figure 4. Without B
IGDEBUG , investigat-
ing such intermediate result is not possible, because Spark com-bines all transformations within a single stage and evaluates themall at once. B
IGDEBUG allows a user to set a breakpoint at any
transformation step and investigate the intercepted intermediate re-sults. She suspects that some UNIX timestamps may be in the 13-
digit millisecond format, while the code assumes timestamps are
in the 10-digit second format. She therefore installs a watchpointguarded by the following predicate (an ordinary function): (line
=> line.split(" ")[3].length > 10.B
IGDEBUG dy-
namically retrieves the data matching this guard, and she can con-tinue to modify the guard iteratively in order to investigate further.
4. DEBUGGING PRIMITIVES
To provide interactive step-wise debugging primitives in Spark,
BIGDEBUG must address three technical challenges. First, it must
bescalable to handle large data sets on the order of terabytes. Sec-
ond, since the debugger process on the driver must monitor andcommunicate with a large number of worker nodes performing
tasks on the cloud, B
IGDEBUG must have a low overhead, min-
imizing unnecessary communication and data transfer. Third, to
help localize the cause of errors, B IGDEBUG must support ﬁne-
grained data inspection and monitoring capabilities at the level of
individual records rather than tasks. Currently, Spark reports fail-
ures at the level of tasks only. Since a single task processes millions
of records, locating a failed task is inadequate, as it is impossiblefor a user to manually inspect millions of records.
B
IGDEBUG tackles these challenges by adopting a tight integra-
tion approach with Spark’s runtime. Instead of creating a wrapperof existing Spark modules to track the input and output of eachstage, B
IGDEBUG directly extends Spark to monitor pipelined,
intra-stage transformations. To maximize the throughput of bigdata processing, B
IGDEBUG provides simulated breakpoints that
enable a user to inspect a program state in a remote executor nodewithout actually pausing the entire computation. To reduce de-velopers’ burden in inspecting a large amount of data, on-demand
watchpoints retrieve intermediate data using a guard and transfer
the selected data on demand. These primitives are motivated byprior user studies in Inspector Gadget [30], where they interviewed1abstract class RDD[T: ClassTag]( ....
2def watchpoint(f: T => Boolean): RDD[T]
3def breakpoint
4def breakpoint(f:T => Boolean)
5def enableLatencyAlert(set : Boolean)
6def setCrashConfiguration(set :
CrashConfiguration)
7def setFunction(f:T= >U)
8def goBackAll: LineageRDD
9def goNextAll: LineageRDD
10def goBack: LineageRDD
11def goNext: LineageRDD
12....
Figure 5: B IGDEBUG ’s API
DISC developers in Yahoo and found that DISC developers wantstep-through debugging, crash culprit determination, and tracing
features. Inspector Gadget proposes desired primitives but does
not implement them.
+ +  
 
 
 
  
 
  /g41/g41/g1/g33/g33/g33/g33/g1
/g41/g42/g1/g27/g9/g18/g1/g25/g13/g29/g25/g4/g16/g18/g13/g1/g51/g24/g22/g9/g23/g17/g1/g1/g1/g1/g1/g1
/g41/g43/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g33/g25/g13/g29/g25/g4/g16/g18/g13/g37/g40/g15/g12/g14/g24/g32/g36/g36/g33/g33/g33/g40/g38/g1/g41/g44/g1/g27/g9/g18/g1/g11/g21/g26/g20/g25/g24/g1/g51/g1/g25/g13/g29/g25/g4/g16/g18/g13/g1
/g41/g45/g1/g1/g1/g1/g1/g1/g1/g1/g1/g33/g14/g18/g9/g25/g8/g9/g22/g37/g18/g1/g51/g52/g1/g18/g33/g24/g22/g18/g16/g25/g37/g40/g1/g40/g38/g38/g1
/g41/g46/g1/g1/g1/g1/g1/g1/g1/g1/g1/g33/g19/g9/g22/g37/g28/g21/g23/g12/g1/g51/g52/g1/g37/g28/g21/g23/g12/g31/g1/g41/g38/g38/g1
/g41/g47/g1/g1/g1/g1/g1/g1/g1/g1/g1/g33/g23/g13/g12/g26/g11/g13/g3/g30/g6/g13/g30/g37/g39/g1/g50/g1/g39/g38/g1
/g41/g48/g1/g11/g21/g26/g20/g25/g24/g33/g11/g21/g18/g18/g13/g11/g25/g1
/g41/g49/g1/g33/g33/g33/g33/g1/g1/g1 /g1
/g41/g46
/g1
/g1
/g41/g47
/g1
/g1
/g41/g48
/g11
/g41/g49
/g1
/g3/g13/g6/g7/g12/g9/g1/g5/g6/g17/g8/g10/g15/g14/g11/g13/g17/g1/g1
/g19/g20/g1/g9/g51/g52/g9/g33/g11/g21/g20/g25/g9/g16/g20/g24/g37/g34/g41/g42/g35/g38 
/g1/g33/g33/g33/g33/g1/g27/g9/g18/g1/g11/g21/g26/g20/g25/g24/g1/g51/g1/g25/g13/g29/g25/g4/g16/g18/g13/g1
/g1/g1/g1/g1/g33/g14/g18/g9/g25/g8/g9/g22/g37/g18/g1/g51/g52/g1/g18/g33/g24/g22/g18/g16/g25/g37/g40/g1/g40/g38/g38/g1
/g1/g1/g1/g1/g33/g19/g9/g22/g37/g28/g21/g23/g12/g1/g51/g52/g1/g37/g28/g21/g23/g12/g31/g1/g41/g38/g38/g1
/g1/g1/g1/g1/g33/g23/g13/g12/g26/g11/g13/g3/g30/g6/g13/g30/g37/g39/g1/g50/g1/g39/g38/g1
/g33/g33/g33/g33
/g1 /g1/g1 /g1/g1/g33/g33/g33/g33/g1/g27/g9/g18/g1/g28/g21/g23/g12/g1/g51/g1/g25/g13/g29/g25/g4/g16/g18/g13/g1
/g1/g1/g1/g33/g14/g18/g9/g25/g8/g9/g22/g37/g18/g1/g51/g52/g1/g18/g33/g24/g22/g18/g16/g25/g37/g40/g1/g40/g38/g38/g1
/g1/g1/g1/g1/g28/g21/g23/g12/g33/g13/g20/g9/g10/g18/g13/g7/g9/g25/g13/g20/g11/g30/g2/g18/g13/g23/g25/g37/g38/g1
/g27/g9/g18/g1/g11/g21/g26/g20/g25/g24/g1/g51/g1/g28/g21/g23/g12/g1
/g1/g1/g1/g33/g28/g9/g25/g11/g15/g22/g21/g16/g20/g25/g37/g9/g51/g52/g9/g33/g11/g21/g20/g25/g9/g16/g20/g24/g37/g34/g41/g42/g35/g38/g38/g1
/g1/g1/g1/g33/g19/g9/g22/g37/g28/g21/g23/g12/g1/g51/g52/g1/g37/g28/g21/g23/g12/g31/g1/g41/g38/g38/g1
/g1/g1/g1/g33/g23/g13/g12/g26/g11/g13/g3/g30/g6/g13/g30/g37/g39/g1/g50/g1/g39/g38/g1
/g33/g33/g33/g33
/g1 /g1/g1/g5/g20/g24/g25/g23/g26/g19/g13/g20/g25/g1
/g1
/g41/g43/g1
/g41/g44
/g27
/g41/g45
/g1
/g1
/g3/g13/g6/g7/g12/g9/g1/g4/g6/g17/g9/g13/g8/g18/g1
/g2/g12/g9/g16/g17 
Figure 6: B IGDEBUG instruments a program automatically
based on debugger control commands entered by a user.
The API for B IGDEBUG is shown in Figure 5 and targets Scala.
A user may also use a web-based debugger UI to automatically
insert corresponding API calls in the code. For example, the in-strumented code on the bottom-right of Figure 6 is automaticallygenerated from the debugger commands at the top of the ﬁgure.
4.1 Simulated Breakpoint
Doing a step by step execution to inspect intermediate outputs is
a common debugging strategy. There are two technical challengesin implementing breakpoints in Spark. First, traditional breakpointswill pause the entire execution at the breakpoint, while a user in-vestigates an intermediate program state. If we naively implementa normal breakpoint, a driver will communicate with all executor
nodes so that each executor will process data until the breakpoint in
the DAG and pause its computation until further debug commandsare provided. This naive approach causes all the computing re-sources on the cloud to be temporarily wasted, decreasing through-
put. Second, Spark optimizes its performance through pipelining
transformations in a single stage. Therefore there is a mismatchbetween the logical view of data transformation and the physicalview of data processing during debugging. For example, whentwo transformations t1andt2are applied to x, these are com-
bined to t2(t1(x)) in a single stage and the intermediate result
oft1(x) is not viewable, as it is not materialized or stored.
Simulated Breakpoint. A simulated breakpoint enables a user to
inspect intermediate results at a given breakpoint and resume the
execution to create an illusion of a breakpoint, even though the
program is still running on the cloud in the background. When
787/g22/g79/g60/g66/g64/g3/g188 /g22/g79/g60/g66/g64/g3/g189/g3
/g16/g60/g75/g22/g79/g60/g66/g64/g3/g190
/g21/g64/g63/g80/g62/g64/g5/g84/g14/g64/g84 /g6/g74/g71/g71/g64/g62/g79 /g9/g71/g60/g79/g16/g60/g75 /g21/g64/g63/g80/g62/g64/g5/g84/g14/g64/g84/g5/g77/g64/g60/g70/g75/g74/g68/g73/g79
/g22/g188 /g22/g189 /g22/g190 /g22/g191
Figure 7: Illustration of Simulated Breakpoint
a simulated breakpoint is hit, B IGDEBUG spawns a new process to
record the transformation lineage of the breakpoint, while letting
the executors continue processing the task. For example, in Fig-
ure 7, when a user sets a breakpoint afterflatmap, program
stateS2is captured from the original workﬂow without affecting
its execution. Therefore, setting a simulated breakpoint has almostzero overhead, as it only retains information to re-generate the pro-
gram state from the latest materialization point, i.e., the last stage
boundary before the simulated breakpoint, in this case S1.
When a user requests intermediate results from the simulated
breakpoint, B
IGDEBUG then recomputes the intermediate results
and caches the results. If a user queries data between transforma-tions such as flatmap andmap in Figure 7 within the same stage,
B
IGDEBUG forces materialization of intermediate results by insert-
ing abreakpoint andwatchpoint (described in Section 4.2)
API call on the RDD object to collect the intermediate results.
Resume and Step Over. When a user enters a resume command
using B IGDEBUG ’s user interface, B IGDEBUG will automatically
jump to the original workﬂow running in the background. This
procedure improves the overall throughput of the distributed pro-
cessing. When a user enters a step over command to investi-
gate the state after the next transformation in the UI, B IGDEBUG
replays the execution to the next instruction only from the latest
materialization point. This feature differentiates B IGDEBUG from
an existing replay debugger such as Arthur [13], which restarts a
job from the beginning. In Figure 7, when a user selects step
over, a new workﬂow will start from the nearest possible materi-
alization point, in this case, S1.B IGDEBUG uses the materialized
stateS1and executes later operations while capturing S3on the
go.Realtime Code Fix. When a user ﬁnds anomalies in intermediate
data, currently the only option is to terminate the job and rewrite theprogram to handle the outliers. Terminating a job at a later stagewill waste all computations before. Because running tasks on cloud
costs lots of money and even days to process billions of records, we
hypothesize that developers are less likely to terminate the programafter inspecting it at the breakpoint.
To save the cost of re-run, B
IGDEBUG allows a user to re-
place any code in the succeeding RDDs after the breakpoint.If a user wants to modify code, B
IGDEBUG applies the ﬁx
from the last materialization point rather than the beginningof the program to reuse previously computed results. Assum-ing that a breakpoint is in place, a user submits a new func-
tion (i.e., a data transformation operator) at the driver. The
function is then compiled using Scala’s NSC [4] library andshipped to each worker to override the call to the original func-tion, when the respective RDD is executed. Suppose a user
sets a breakpoint after flatmap and the program is paused in
Figure 7. A user can replace the function in the map trans-
formation from ((word => (word,1)) to{word => if
(word!=null) (word,1); else (word,0);}. When a
user resumes after the ﬁx at S2, a new workﬂow will start from
S1and later RDDs including modiﬁed ones will be computed until
the end of the workﬂow is reached and the background job of theoriginal workﬂow is terminated.
B
IGDEBUG checks whether the supplied function has the same
type as the original function through static type checking. There-fore, a user cannot provide a code ﬁx that breaks the integrity ofthe used data type. When a user replaces function fwith a new
function g,B
IGDEBUG applies gto all records from the last mate-
rialization point to ensure consistency.
4.2 On-Demand Watchpoint with Guard
Similar to watching a variable in a traditional debugger like
gdb,B IGDEBUG provides a watchpoint to inspect intermediate
data. Because millions of records are passing through a data-parallel pipeline, it is infeasible for a user to inspect all intermediaterecords. Such data transfer would also incur high communication
overhead, as all worker nodes must transfer the intermediate results
back to the driver node. To overcome these challenges, B
IGDEBUG
provides an on-demand watchpoint with a guard closure func-
tion. A user can provide a guard to query a subset of data matchingthe guard. For example, (r=>r>4) is an anonymous function
that takes ras input and returns true, if r is greater than 4, and
rdd.watchpoint(r=>r>4) sets a watchpoint to retrieve all
records greater than 4. B
IGDEBUG automatically compiles such
user-provided guard and distributes it to worker nodes to retrieve
the matching data.
On-Demand Batch Transfer. To reduce communication over-
head, B IGDEBUG batches intermediate data and sends them to the
driver when needed. If no request is made for the watchpointeddata from the user, it will be kept at the workers until the end of
the stage or the next breakpoint in the same stage, if there is any.
When the computation passes the end of the stage, the remainingﬁltered data are ﬂushed, so that there are enough memory availablefor other Spark operations.
Dynamic Guard Modiﬁcation. A user can modify a guard func-
tion to narrow down the scope of captured data. This feature is
called a dynamic guard, as the function can be reﬁned iteratively
while the Spark job is executing. A watchpoint guard is builton top of Scala and Java and is written like a normal Spark pro-
gram. For example, an expression (value._1.length > 50
&& value._2 == 1) ﬁlters values where the length of ﬁrst el-
ement in tuple is greater than 50 and the value of second element
in tuple is 1. When a user updates a guard during a Spark job,
B
IGDEBUG uses Scala’s NSC library [4] to compile the guard at
the driver node and ships it to all workers. Individual workers thenload the new guard at each executor. Using a dynamic guard, a usercan tune the amount of data being transferred and presented.
4.3 Crash Culprit and Remediation
DISC systems are limited in their ability to handle failures at run-
time. In Spark, crashes cause the correctly computed stages to sim-ply be thrown away. Remediating a crash at runtime can save timeand resources by avoiding a program re-run from scratch. B
IGDE-
BUG leverages ﬁne-grained tracing to be discussed in Section 4.4 to
identify a crash-inducing input, not just a crash culprit record in the
intermediate stage. While waiting for a user intervention, B IGDE-
BUG runs pending tasks continuously to utilize idle resources and
to achieve high throughput.
Crash Culprit Determination. When a crash occurs at an execu-
tor, B IGDEBUG sends all the required information to the driver, so
that the user can examine crash culprits and take actions as depictedin Figure 8. When a crash occurs, B
IGDEBUG reports (1) a crash
culprit—an intermediate record causing a crash (2) a stack trace,(3) a crashed RDD, and (4) the original input record inducing a
crash by leveraging backward tracing in Section 4.4.
Remediation. B
IGDEBUG avoids the re-generation of prior stages
by allowing a user to either correct the crashed record, skip thecrash culprit, or supply a code ﬁx to repair the crash culprit. A
788/g21/g64/g62/g74/g77/g63
/g147/g194/g195/g148
/g147/g195/g196/g104/g148/g147/g200/g201/g148/g147/g199/g148/g21/g64/g62/g74/g77/g63
/g3/g3/g194/g195/g3/g3/g200/g201/g3/g3/g199/g72/g60/g75/g159/g77/g3/g211/g213/g3/g77/g137/g79/g74/g12 /g73/g79/g160
/g16/g60/g75
/g6/g77/g60/g78/g67/g3/g21/g64/g75/g74/g77/g79/g3
/g79/g74/g3/g7/g77/g68/g81/g64/g77
/g147/g195/g196/g148
/g7/g77/g68/g81/g64/g77/g147/g195/g196/g104/g148
/g189/g190
Figure 8: An intermediate record “23s” at the map transforma-
tion causes a crash. B IGDEBUG reports the crash culprit, “23s”
to the user and a user supplies the corrected record, “23”.
naive resolution approach is to pause the execution, report the crash
culprit to the driver and wait until a resolution is provided from theuser. This method has the disadvantage of putting the workers in
the idle mode, reducing throughput. Therefore, B
IGDEBUG pro-
vides the following two methods.
•Lazy Repair of Records. In case of a crash, the executor re-
ports a crash culprit to the driver but continues processing the
remaining pending records. Once the executor reaches the
end of the task, it then waits for a corrected record from the
user. This approach parallelizes the crash resolution with-out holding back the executor. If there are multiple crashculprits, B
IGDEBUG accumulates the crashed records at the
driver and lets all executors terminate, except the very lastexecutor. The last executor on hold then processes the groupof corrected records provided from the user, before the end ofthe stage. This method applies to the pre-shufﬂe stage only,because the record distribution must be consistent with exist-
ing record-to-worker mappings. This optimization of replac-
ing crash-inducing records in batch improves performance.
•Lazy Code Fix. B
IGDEBUG accumulates crash culprits at
the driver and lets all executors continue processing the pend-
ing records. It then asks a user to supply a code ﬁx to re-
pair the crash culprits, i.e., a new repair function to applyto the crash-culprits. Our assumption is that the new func-tion extends the original function to clean the crash-inducingrecords and a user would like to see some results rather than
nothing, because the current Spark does not provide any out-
put to the user when a task crashes, even for successful in-puts. If a user wants to apply a completely different functiongto all records, she can use our Realtime Code Fix at simu-
lated breakpoint instead. Similar to the above lazy repair ofrecords, the last executor on hold applies the supplied func-tion to the crash culprits in batch before the end of the stage.
4.4 Forward and Backward Tracing
BIGDEBUG supports ﬁne-grained tracing of individual records
by invoking a data provenance query on the ﬂy. The data prove-
nance problem in the database community refers to identifying
the origin of ﬁnal (or intermediate) output. Data provenance sup-port for DISC systems is challenging, because operators such asaggregation, join, and group-by create many-to-one or
many-to-many mappings for inputs and outputs and these mappings
are physically distributed across different worker nodes.
B
IGDEBUG uses data provenance capability implemented
through an extension of Spark’s RDD abstraction (called Linea-
geRDD) that leverages Spark’s built-in fault tolerance and datamanagement infrastructure [22]. The LineageRDD abstraction pro-
vides programmers with data provenance query capabilities. Prove-nance data is captured at the record level granularity, by taggingrecords with identiﬁers and associating output record identiﬁerswith the relevant input record identiﬁer, for a given transforma-
tion. From any given RDD, a Spark programmer can obtain a Lin-
0,   After is said14, and18, done more is         
      said than done
              ...(After, 1)
(is, 1)
...
(is, 1)
...
(done, 2)(After, 1)
(is, 2)
...
(done, 2)offset, text (key, partial count) (key, total count)
Agent1
input output
0 a
14 b
18 c
... ...Agent2
input output
a x
a y
... ...
c w
... ...
c zAgent3
input output
x 0
y 1
w 1
... ...
z 100
input output
a y
c wStep 1 input output
0 a
18 cStep 2
Figure 9: A logical trace plan that recursively joins data lineagetables, back to the input lines
eageRDD reference and use it to perform data tracing—i.e., the
ability to transition backward (or forward) in the Spark program
dataﬂow, at the record level.
B
IGDEBUG instruments submitted Spark programs with tracing
agents that wrap transformations at stage boundaries. These agents
implement the LineageRDD abstraction and have two responsibil-
ities: (1) tag input and output records with unique identiﬁers for a
given transformation and (2) store the associations between input
and output record identiﬁers as a data provenance table in Spark’snative storage system. For example, Figure 9 shows intermediateresults and a corresponding data provenance table for each agent.
For example, the ﬁrst line at offset 0 “After is said” is assigned
with a unique output identiﬁer, a. As this line is split into multiple
records such as (After, 1) and (is, 1), unique output identiﬁers suchasxandyare assigned to the corresponding key and partial count
pairs. B
IGDEBUG utilizes speciﬁc tracing agents based on the type
of transformation e.g., data ingested from Hadoop Distributed File
System (HDFS) and Amazon S3, and all native Spark transforma-tions; agents are pluggable, making it easy to support other inputdata storage environments e.g., Cassandra, HBase, and RDBMS.
Once the provenance data is recorded, tracing queries can be is-
sued using the lineage-related methods of the API in Figure 5. ThegoBackAll andgoNextAll methods are used to compute the
full trace backward and forward respectively. That is, given someresult record(s), goBackAll returns all initial input records ( e.g.,
in HDFS) that contributed in the generation of the result record(s);goNextAll returns all the ﬁnal result records that a starting input
record(s) contributed to in a transformation series. A single stepbackward or forward is supported by the goBack andgoNext
respectively. At any given point in the trace, the user can interac-tively issue a native Spark collect action to view the raw data
referenced by the LineageRDD.
When a tracing query is issued, B
IGDEBUG logically recon-
structs the path connecting input to output records by recursively
joining the provenance tables generated by the tracing agents, as
shown in Figure 9 using the word count example. After executing aword count job, a user may want to perform a full backward tracingfrom the output value (is,2).B
IGDEBUG does this by ﬁrst retriev-
ing the identiﬁer for output (is,2)from the output provenance table.
Figure 9 shows that identiﬁer to be equal to 1, and the correspond-
ing mappings have two input identiﬁers yandw. Tracing proceeds
by (recursively) joining the provenance tables, at neighboring cap-ture agents, along the output and input values. 1represents a join
operation of provenance tables. For instance, the join of Agent 3
789/g21/g64/g75/g74/g77/g79/g3/g79/g74/g3
/g63/g64/g61/g80/g66/g66/g64/g77/g21/g64/g62/g74/g77/g63
/g77/g188
/g77/g189
/g77/g190
/g77/g191/g21/g64/g62/g74/g77/g63 /g23/g68/g72/g64/g3/g159/g72/g78/g160
/g77/g188/g120/g192
/g77/g189/g120/g189/g187/g187
/g77/g190/g120/g195
/g77/g191/g120/g193/g21/g7/g7
Figure 10: When latency monitoring is enabled, straggler
records are reported to the user.
and Agent 2produces the Step 1trace result. Subsequently, join-
ing Step 1with Agent 1produces the Step 2trace result, which is
the ﬁnal trace result referencing HDFS input records at offsets 0
and18, both containing the word “is”.
Supporting data provenance, while logically simple, is difﬁcult
to achieve in a DISC environment such as Spark because the size of
input-output identiﬁer mappings is as large as all intermediate re-sults, and data provenance tables are physically distributed acrossworker nodes. For these reasons, Spark’s internal storage service is
used for storing provenance data, and a distributed join implemen-
tation uses partition information embedded into the record identi-ﬁers to optimize the shufﬂe step. Implementing an optimized dis-
tributed join of data provenance tables in Spark is the subject of
another paper [22], which details the advantage of an optimizeddistributed join in Spark over storing data provenance tables in ex-ternal storage services (e.g., HDFS, MySQL).
4.5 Fine-Grained Latency Alert
In big data processing, it is important to identify which records
are causing delay. Spark reports a running time only at the
level of tasks, making it difﬁcult to identify individual straggler
records—records responsible for slow processing. To localize per-
formance anomalies at the record level, B IGDEBUG wraps each
operator with a latency monitor. For each record at each trans-
formation, B IGDEBUG computes the time taken to process each
record, keeps track of a moving average, and sends to the monitor,
if the time is greater than kstandard deviations above the mov-
ing average where default kis 2. Figure 10 shows an example of
how straggler records are tagged and reported to the debugger. As
we show in Section 5, record-level latency alert poses the highest
overhead among B IGDEBUG ’s primitives due to the cost of taking
a timestamp for processing each record and computing the movingaverage among millions of records per executor.
5. EV ALUATION
We evaluate B IGDEBUG ’s (1) scalability, (2) performance over-
head, (3) localizability improvement in determining crash-inducingrecords, and (4) time saving w.r.t to an existing replay debug-ger. The main purpose of our evaluation is to investigate whether
B
IGDEBUG keeps performance similar to the original Spark and
retains its scalability, while supporting interactive debugging.
•How does B IGDEBUG scale to massive data?
•What is the performance overhead of instrumentation and ad-ditional communication for debugging primitives?
•How much localizability improvement does B
IGDEBUG
achieve by leveraging ﬁne-grained, record level tracing?
•How much time saving does B IGDEBUG provide through its
runtime crash remediation, in comparison to an existing re-play debugger?
We use a cluster consisting of sixteen i7-4770 machines, each
running at 3.40GHz and equipped with 4 cores (2 hyper-threadsper core), 32GB of RAM, and 1TB of disk capacity. The oper-ating system is a 64bit Ubuntu 12.04. The datasets are all storedon HDFS version 1.0.4 with a replication factor of 1—one copy
of each dataset across the cluster. We compare B
IGDEBUG ’s per-formance against the baseline Spark, version 1.2.1. The level of
parallelism was set at two tasks per core. This conﬁguration allowsus to run up to 120 tasks simultaneously.
We use three Spark programs for our performance experiments:
WordCount, Grep, and PigMix query L1. WordCount computes the
number of word occurrences grouped by unique words. WordCount
comprises of 4 transformations, which are divided into 2 stages: theﬁrst stage loads the input ﬁle ( textfile), splits each line into a
bag of words (flatmap), creates a tuple of (word, 1) (map), andthe second stage reduces the key-value pairs using each word as thekey (reduceByKey). Grep ﬁnds the lines in the input datasets
that contain a queried string. Grep comprises of only 1 stage with
2 transformations : textfile reads the input ﬁle line by line and
filter applies a predicate to see if the line contains a substring.
PigMix’s latency query L1 is a performance benchmark for DISCsystems in which an unstructured data is transformed and analyzed.L1 comprises of 2 stages: the ﬁrst stage contains textfile,2
maps, flatmap followed by 2 maps and the second stage has
reduceByKey followed by a map.
For our performance experiment, we vary input size from
500MB to 1TB by using an unstructured data made up of Zipf
distribution over a vocabulary of 8000 terms. All runs are repeated10 times. We compute a trimmed mean by removing the shortest
2 runs and the longest 2 runs, because the running time of DISCprograms depends on various factors such as a warm up of HDFScache, garbage collection, and network I/O. In big data systems, a
variation of 5% is considered noise, because of these factors.
5.1 Scalability
We perform experiments to show that B IGDEBUG scales to mas-
sive data, similar to Spark. More speciﬁcally, we assess the scale up
property—B IGDEBUG can ingest and process massive data and its
running time increases in proportion to the increase in data size. We
assess the scale out property—as the number of worker nodes (the
degree of parallelization) increases, the running time decreases.Scaling Up. Figure 11(a) shows the scale-up experiment while
varying the data size from a few gigabytes to one terabyte. In thisexperiment, B
IGDEBUG is used with the maximum instrumenta-
tion where breakpoints and watchpoints are set in every line and
latency monitoring, crash remediation, and data provenance are en-
abled for every record at every transformation. The running timeof B
IGDEBUG grows steadily in proportion to the running time of
Spark. As the data size increases, B IGDEBUG ’s running time also
increases, because large input data requires the scheduler to createmore tasks, assign the tasks to workers, and coordinate distributedexecution to improve data locality, throughput, and HDFS caching.
With such maximum instrumentation, B
IGDEBUG scales well
to massive data (1TB). Figure 11(b) shows the overhead in Word-
Count.B IGDEBUG takes 2.5X longer on average in comparison
to the baseline Spark. This 2.5X overhead is a very conservativeupper bound, as a developer may not need to monitor the latency ofeach record at every transformation and may not need to set break-
points on every operator. When disabling the most expensive fea-
ture, record-level latency monitoring, B
IGDEBUG poses an average
overhead of 34% only. In Grep and L1, the overheads are 1.76X
and 1.38X with the maximum instrumentation and 7% and 29%respectively when latency monitoring is disabled (see Table 1).
OverheadBenchmark Dataset( GB)Max w/o Latency
PigMix L1 1, 10, 50, 100, 150, 200 1.38X 1.29X
Grep 20, 30, 40, ...90 1.76X 1.07X
Word Count 0.5 to 1000 (increment with a log scale) 2.5X 1.34X
Table 1: Performance Evaluation on Subject Programs
7901 10 100 1,0001001,00010,000
Dataset (GB)Ti m e (s)BIGDEBUG −Maximum
ApacheSpark
(a) Scale Up1 10 100 1,00000.511.522.533.544.555.5
Dataset (GB )Times of Apache SparkBIGDEBUG ’s Maximum Overhead
(b) Maximum Instrumentation234567850100150200250300350400Baseline Spark 10GB
Baseline Spark 30GB
Baseline Spark 50GB
Wo rke rsTi m e (s)10GB
30GB
50GB
(c) Scale Out
0123451002003004005006007008009001,0001,100
Instructions with breakpoint (out of 5)Ti m e (s)
Traditional Breakpoint
Simulated Breakpoint
(d) Traditional vs. Simulated Breakpoint0 500 1,000 1,500 2,000 2,50001020304050
Data captured with watchpoint (MB)Overhead (%)Watchpoint Overhead on 40GB Dataset
(e) Watchpoint20 40 60 80010203040506070
Dataset (GB)Overhead (%)Tracing+Crash Monitoring+Watchpoint
Tracing
Crash Monitoring
Watchpoint
(f) Overhead Per Feature
Figure 11: B IGDEBUG ’s Scalability and Overhead
Scaling Out. The property of scaling out is essential for any DISC
system. We change the number of worker nodes (essentially the
total number of cores) on a ﬁxed size dataset and record a running
time for both B IGDEBUG and the baseline Spark. As the number
of cores increases, the running time should decrease. Figure 11(c)
shows that B IGDEBUG does not affect the scale-out property of the
baseline Spark. When there are too many workers, it makes it hardfor the Spark scheduler to retain locality, and B
IGDEBUG also ends
up moving data around at stage boundaries between workers.
5.2 Overhead
We measure the overhead as the increase in the running time
of B IGDEBUG w.r.t the baseline Spark. The performance over-
head comes from instrumentation and communication between thedriver and the workers and data transfer to carry the requested de-bug information to the user. B
IGDEBUG works at the record level.
Therefore, the overhead increases in proportion to the number of
records in each stage. For example, when the WordCount program
splits input lines into a bag of words at the flatmap transforma-
tion, the overhead increases.Simulated Breakpoint. B
IGDEBUG offers almost 0% overhead
to pause and instantly resume. This overhead is the same across
all subject programs because instantly resuming simulated break-point does not involve any additional instrumentation. On the otherhand, a traditional breakpoint adds overhead because it must pause
and cache the intermediate state entirely. Figure 11(d) shows the
comparison results between simulated breakpoints and traditionalbreakpoints, while setting a breakpoint at different transformations.Traditional breakpoints incur orders of magnitude higher overhead,
since they materialize all intermediate results regardless of whether
a user requests them or not. For example, a traditional breakpoint’soverhead rises sharply after a flatmap transformation that emits
a large number of unaggregated records.On-Demand Watchpoint. We place a watchpoint between two in-
structions such that a custom guard reports only a certain percent-age of intermediate results, e.g., 2%, 5% , 10% etc. Once captured,
the intermediate data is sent to the driver for a user to inspect. In
Figure 11(e), the x-axis shows the amount of captured data rang-ing from 500MB to 3GB, when the total intermediate data size is
40GB. When 500MB of data is transferred to the user at the watch-
point, it incurs only 18% overhead. This is a very conservative setup, since the user is unlikely to read 500MB records at once. To iso-late the overhead of setting watchpoints from the overhead of data
transfer, we set a watchpoint at every transformation with a guard
that always evaluates to false. Among all subject program, Word-
Count poses the maximum overhead of 9% on average, whereas the
overhead for Grep and L1is 5% and 3% respectively.
Crash Monitoring. We enable crash monitoring for every trans-
formation and vary the data size from 20GB to 90 GB to measureoverhead. See Figure 11(f). Crash monitoring imposes 19% over-head in L1. WordCount and Grep incur a lower overhead of 18%
and 4% respectively. This overhead comes from monitoring ev-
ery record transformation for any kind of a failure, and checking if
there are any pending crashed records to be repaired at the end oflocal execution in a task.Backward and Forward Tracing. We enable record-level data
provenance capturing for every transformation and vary the data
size from 20GB to 90GB. See Figure 11(f). The tracing primitive
poses, on average, 24% overhead over the baseline Spark in L1.
WordCount and Grep pose, on average, 22% and 5% overhead re-
spectively. The majority of the overhead comes from the generation
of the data provenance tables maintaining the associations between
input and output record identiﬁers. The cost of storing each dataprovenance table into Spark’s storage layer is small, because it isperformed asynchronously.Fine-Grained Latency Alert. We enable record-level latency
monitoring on every transformation. Record-level latency moni-toring incurs the highest overhead among B
IGDEBUG ’s features.
Latency monitoring alone takes 2.4X longer than baseline Spark ondata size varying from 500MB to 1TB (see Figure 12(a)) in Word-
7911 10 100 1,00000.511.522.533.544.555.5
Datasize (GB)T imesofApacheSparkLatency Alert
(a) Latency Alert Overhead2,000 4,000 6,000 8,000 10,00012345678
Number of Tasks in JobAverage Records Per Task (Millions )
Records Per Task
Execution Time
80120160200240
Ti m e (s)
(b) Localizability vs. PerformanceS1 S2 S3 S4050100150200
Location of crash (Stage)Ti m e (s)BIGDEBUG
Arthur
(c) Time Saving
Figure 12: Latency monitoring overhead, time saving through crash remediation, and localizability improvement
Count.I n Grep and L1, the overhead is 1.74X and 1.24X. The
signiﬁcant overhead comes from performing a statistical analysis
for each record transformation. A timestamp is recorded before
and after the transformation of each record to see if its latency lies
within 2 standard deviations from the average. The overhead is alsoincurred by updating a moving average and a standard deviation.If we disable record-level latency monitoring from the maximum
instrumentation setting, the overhead decreases from 150% (i.e.,
2.5X of baseline Spark) to 34% (i.e., 1.34X of baseline Spark) inthe case of the WordCount program.
5.3 Crash Localizability Improvement
Spark reports failures at the level of tasks, while B IGDEBUG
reports speciﬁc failure-inducing inputs. B IGDEBUG also detects
speciﬁc straggling records, while Spark only reports a stragglertask causing delay. By construction, our record-level tracing ap-
proach has 100% accuracy with zero false positive because it lever-
ages data provenance to identify all inputs contributing to a failure.
Delta Debugging [39] can further isolate this combination of multi-ple failure-inducing inputs, but will require a larger number of runs,
as opposed to B
IGDEBUG that requires only a single run.
Therefore, we measure the improvement in localizing failed (or
delayed) records in comparison to the baseline Spark. When de-
bugging a program using Spark, a developer may want to increasethe number of tasks to improve fault localizability at the cost of
running time. Note that the running time increases as you increase
the number of tasks, since more resources are used for communica-tion and coordination among distributed worker nodes. To quantifythis, we vary the number of tasks and measure the average number
of records per task and the total running time. See Figure 12(b).
When conﬁguring Spark with 1000 tasks and 60GB dataset, eachtask handles 7.52 million records on average. If a single recordamong 7.52 million records crashes the task, it is impossible for ahuman being to identify a crash culprit. To improve fault localiz-
ability, if a developer increases the number of tasks from 1000 to
10,000 to reduce the number of records assigned to each task, eachtask still handles 0.72 million records, and additional communica-tion and coordination increases the total running time by 2.5 times.
As Figure 11(f) shows, B
IGDEBUG incurs less than 19% overhead
for reporting crash culprits and takes only 2.4X time for latency
alert, while improving fault localizability by orders of millions.
5.4 Time Saving through Crash Remediation
When a task crashes, the current Spark does not provide any out-
put to the user, even for successful inputs and terminates the taskimmediately. On the other hand, B
IGDEBUG allows a user to re-
move or modify a crash culprit at runtime to avoid termination.
Therefore, B IGDEBUG avoids additional runs when a user tries toremove crash-inducing records from the original input. For the
same scenario, using a post-hoc instrumentation replay debugger
Arthur [13] requires at least three runs. In the ﬁrst run, a programcrashes and Spark reports failed task IDs. In the second run, a user
must write a custom post-hoc instrumentation query (a new data
ﬂow graph) with those failed task IDs and run the query to recom-pute the intermediate results for the failed tasks. In the third run, auser removes the crash-inducing records and re-runs the job again.
Crashes in later stages result in more redundant work in the second
and third runs and hence more time for completion. When a crashoccurs at the 9th stage of a 10 stage job, Arthur must recompute theﬁrst 9 stages twice, while B
IGDEBUG avoids such re-computation
completely by allowing a user to resolve crashes in the ﬁrst run.
Figure 12(c) shows our experiment result on time saving. We
compare B IGDEBUG ’s time saving with a post-hoc instrumen-
tation replay debugger like Arthur. We conservatively estimateArthur’s performance by running the original Spark for its ﬁrst
and third runs. We measure time saving by dividing the ad-
ditional time required by Arthur by B
IGDEBUG ’s completion
time. We seed crashes in different transformation locations byupdating an original program to throw an exception at a givenstage, because the magnitude of time saving depends on which
stage a crash occurs. For example, we replace the map func-
tion{word => (word,1)} with {word=> if(word ==
"Crash") crash(); (word,1)} where crash() always
throw a NullPointerException. S1 is a program where
crashes are seeded in the ﬁrst stage, S2 is a program where crashesare seeded in the second stage, etc. B
IGDEBUG saves the execution
time by 80% on average and reaches up to 100% after S2. In theexperiment, the most time consuming stage is S2 and a crash in S2or later saves a large amount of time.
6. RELATED WORK
Debugging Needs for DISC. Fisher et al. [15] interviewed 16 data
analysts at Microsoft and studied the painpoints of big data analyt-ics tools. Their study ﬁnds that a cloud-based computing solutionmakes it far more difﬁcult to debug. Data analysts often ﬁnd them-selves digging through trace ﬁles distributed across multiple VMs.
Zhou et al. [40] manually categorize 210 randomly sampled escala-
tion of a big data platform at Microsoft. The study ﬁnds that 36.2%of in-ﬁeld failures (i.e., escalations) are caused by system-side de-fects, which include logical and design errors of DISC applications.
Job failures and slowdowns are common in DISC applications, ac-
counting for 45% and 27% of the escalations. These ﬁndings mo-tivate B
IGDEBUG .
Execution Log Analysis of DISC applications. Several ap-
proaches help developers debug DISC applications by collecting
792and analyzing execution logs. Boulon et al. [11] monitor Hadoop
clusters at runtime and store the log data. Their system collects
logs for understanding a runtime failure, but does not provide real-
time debug primitives. Developers typically develop DISC appli-cations using a small sample of data in a local mode or a pseudocloud environment ﬁrst and deploy the application on a larger cloudwith a considerably larger data set and processing power. Shang et
al. [34] compare the execution log captured on the cloud with the
log captured using a local mode. Their system abstracts the exe-cution logs, recovers the execution sequences and compares the se-quences between the pseudo and cloud deployments. Tan et al. [35]
analyze Hadoop logs to construct state-machine views of the pro-
gram execution to help a developer understand a Hadoop execu-tion log. Their approach computes the histogram of the duration ofeach state and detects anomalies in the program execution. Xu etal. [37] parse console logs and combine source code analysis to de-
tect abnormal behavior. Fu et al. [16] map free-form text messages
in log ﬁles to logging statements in source code. None of thesepost-mortem log analysis approaches help developers debug DISCapplications realtime.
Debuggers for DISC applications. Inspector Gadget [30] is a
framework proposal for monitoring and debugging data ﬂow pro-
grams in Apache Pig [31]. The proposal is based on informal in-terviews with ten Yahoo employees who write DISC applications.While Inspector Gadget proposes features such as step-through de-
bugging, crash culprit determination, tracing, etc., it simply lists
desired debug APIs, but leaves it to others to implement the pro-posed APIs. The tracing API proposed by Inspector Gadget targetscoarse-grained off-line tracing using a centralized server, falling
behind B
IGDEBUG ’s ability to trace individual records at runtime.
Arthur [13] is a post-hoc instrumentation debugger that targets
Spark and enables a user to selectively replay a part of the original
execution. However, a user can only perform post-mortem analysis
and cannot inspect intermediate results at runtime. It also requires
a user to write a custom query for post-hoc instrumentation. To
localize faults, Arthur requires more than one run. For example, toremove crash-inducing records from the original input, in the ﬁrstrun, a program crashes and Spark reports failed tasks IDs. In the
second run, a user must write a custom post-hoc instrumentation
query (a new data ﬂow graph) with those failed task IDs and runthe query to recompute the intermediate results for the failed tasks.In the third run, a user removes the crash-inducing records and re-runs the job again. Such post-hoc instrumentation incurs signiﬁcant
debugging time, as demonstrated by Section 5.4. Recon [25] opts
for a post-hoc instrumentation strategy like Arthur.
Graft [32] is a debugger for a graph-based DISC computing
framework, Apache Giraph [2]. Graft requires a user to select ver-
tices—similar to executors in Spark—to capture events and replay
the execution later. Graft assumes that a user has adequate priorknowledge to know buggy vertices. Similar to Arthur, Graft is apost-hoc instrumentation debugger. Moreover, Graft is built forprocessing graphs only and is not applicable to a data ﬂow frame-
work, like Spark.
Daphne [23] lets users visualize and debug DryadLINQ pro-
grams. It provides a job object model for viewing the running tasks
and enables a user to attach a debugger to a remote process on the
cluster. This approach works in DryadLINQ because all communi-
cations between tasks is through disk. Such approach could workfor Hadoop or MapReduce that persist intermediate results in theﬁle system, but does not work for an in-memory processing frame-work such as Spark that achieves orders-of-magnitude better per-
formance through in-memory processing and lazy evaluation.
Data Provenance for DISC applications. There is a large body ofwork that studies techniques for capturing and querying data prove-nance in data-oriented workﬂows [10]. RAMP [21] instrumentsApache Hadoop with agents that wrap the user-provided map and
reduce functions. RAMP agents store data provenance tables
in HDFS, and enable a user to query data provenance data usingApache Hive [36] and Pig [31]. NEWT [27] captures data prove-nance tables and stores in MySQL clusters. A user must submitSQL queries to join data provenance tables in an iterative loop. Be-
cause both RAMP and NEWT do not store the referenced raw data,
a user can see record identiﬁers only and cannot view intermediateresults on the ﬂy.
B
IGDEBUG supports ﬁne-grained tracing by leveraging prior
work on data provenance within Spark [22]. The data provenance
capability used by B IGDEBUG is orders-of-magnitude faster, be-
cause it stores data provenance tables in-memory within Spark’s
runtime and performs an optimized distributed join of the prove-nance tables. The design of an optimized, distributed join algo-
rithm in Spark is a subject of another paper and is described else-where [22]. Data provenance alone cannot support realtime debug-
ging, since a user needs primitives such as simulated breakpointsand guarded watchpoints to interact with a data parallel pipeline atruntime, and data provenance queries must be invoked in the con-
text of crashes, failures, or a breakpoint.
Replay Debugger. Replay debugging for distributed systems has
been extensively studied [29, 24] through systems such as li-blog [17], R2 [20], and DCR [8]. These systems are designed to
replay general distributed programs, and thus recording all sources
of non-determinism, including message passing order across nodes,system calls, and accesses to memory shared across threads. Theirgoal is to reproduce errors using the captured events. These replaydebuggers incur signiﬁcant overhead at runtime and even larger
slowdown at replay time. In contrast, B
IGDEBUG leverages the
structure of a data ﬂow graph to replay sub computations, and a
partial replay is to support step-through debugging, while a pro-
gram is still running. Frameworks like D3S [26], MaceODB [12]
and Aguilera et al. [7] are distributed debuggers for ﬁnding frame-
work bugs, not application bugs.
7. CONCLUSION
Big data debugging is currently a painstakingly long and expen-
sive process. B IGDEBUG offers interactive debugging primitives
for an in-memory data-intensive scalable computing (DISC) frame-work. To emulate traditional step-wise debugging in the contextof in-memory big data processing, B
IGDEBUG offers simulated
breakpoints and guarded watchpoints with little performance over-head. B
IGDEBUG enables a user to determine crash culprits and
resolve them at runtime, avoiding a program re-run from scratch.By leveraging ﬁne-grained data provenance, B
IGDEBUG reports
the origin of a crash culprit and supports tracing intermediate re-sults forward and backward at the record level. It scales to massive
data in the order of terabytes, improves fault localizability by or-
ders of millions than baseline Spark, and provides up to 100% timesaving with respect to a posthoc instrumentation replay debugger.
In terms of future work, we plan to construct Spark program
benchmarks and conduct user studies with professional software
engineers. Instead of having a user specify a guard for an on-
demand watchpoint, extracting data invariants from intercepted in-termediate results may be useful for helping the user debug a DISCprogram. Another area for future work is tool-assisted automated
fault localization in B
IGDEBUG . For example, with the help of au-
tomated fault localization, we envision that a user can isolate the
trace of a failure-inducing workﬂow, diagnose the root cause of anerror, and resume the workﬂow for only affected data and code.
793Acknowledgements
We thank the anonymous reviewers for their comments. Partic-
ipants in this project are in part supported through NSF CCF-
1527923, CCF-1460325, IIS-1302698, CNS-1351047, and NIH
U54EB020404. We would also like to thank our industry partnersat IBM and Intel for their gifts.
8. REFERENCES
[1] Amazon s3. https://aws.amazon.com/s3/.
[2] Apache giraph. http://giraph.apache.org/.[3] Hadoop. http://hadoop.apache.org/.[4] Scala.tool.nsc. http://www.scala-lang.org/api/2.11.0/
scala-compiler/index.html#scala.tools.nsc.package.
[5] Spark. https://spark.apache.org/.[6] Spark documentation. http://spark.apache.org/docs/1.2.1/.[7] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and
A. Muthitacharoen. Performance debugging for distributed
systems of black boxes. In ACM SIGOPS Operating Systems
Review, volume 37, pages 74–89. ACM, 2003.
[8] G. Altekar and I. Stoica. Dcr: Replay debugging for the
datacenter. Technical Report UCB/EECS-2010-74, EECS
Department, University of California, Berkeley, May 2010.
[9] M. Armbrust, R. S. Xin, C. Lian, Y . Huai, D. Liu, J. K.
Bradley, X. Meng, T. Kaftan, M. J. Franklin, A. Ghodsi, and
M. Zaharia. Spark sql: Relational data processing in spark.
InProceedings of the 2015 ACM SIGMOD International
Conference on Management of Data, pages 1383–1394.ACM, 2015.
[10] O. Biton, S. Cohen-Boulakia, S. B. Davidson, and C. S.
Hara. Querying and managing provenance through user
views in scientiﬁc workﬂows. In Proceedings of the 2008
IEEE 24th International Conference on Data Engineering,
ICDE ’08, pages 1072–1081, Washington, DC, USA, 2008.IEEE Computer Society.
[11] J. Boulon, A. Konwinski, R. Qi, A. Rabkin, E. Yang, and
M. Yang. Chukwa, a large-scale monitoring system. In Cloud
Computing and its Applications (CCA ’08), pages 1–5, 102008.
[12] D. Dao, J. Albrecht, C. Killian, and A. Vahdat. Live
debugging of distributed systems. In Compiler Construction,
pages 94–108. Springer, 2009.
[13] A. Dave, M. Zaharia, and I. Stoica. Arthur: Rich post-facto
debugging for production analytics applications. Technicalreport, University of California, Berkeley, 2013.
[14] J. Dean and S. Ghemawat. Mapreduce: Simpliﬁed data
processing on large clusters. In Proceedings of the 6th
Conference on Symposium on Opearting Systems Design &Implementation - V olume 6, OSDI’04, pages 10–10,
Berkeley, CA, USA, 2004. USENIX Association.
[15] D. Fisher, R. DeLine, M. Czerwinski, and S. Drucker.
Interactions with big data analytics. interactions,
19(3):50–59, May 2012.
[16] Q. Fu, J.-G. Lou, Y . Wang, and J. Li. Execution anomaly
detection in distributed systems through unstructured log
analysis. In Proceedings of the 2009 Ninth IEEE
International Conference on Data Mining, ICDM ’09, pages149–158, Washington, DC, USA, 2009. IEEE ComputerSociety.
[17] D. Geels, G. Altekar, P. Maniatis, T. Roscoe, and I. Stoica.
Friday: Global comprehension for distributed replay. In
Proceedings of the 4th USENIX Conference on NetworkedSystems Design & Implementation, NSDI’07, pages 21–21,
Berkeley, CA, USA, 2007. USENIX Association.
[18] S. Ghemawat, H. Gobioff, and S.-T. Leung. The google ﬁle
system. In ACM SIGOPS operating systems review,
volume 37, pages 29–43. ACM, 2003.
[19] J. E. Gonzalez, R. S. Xin, A. Dave, D. Crankshaw, M. J.
Franklin, and I. Stoica. Graphx: Graph processing in adistributed dataﬂow framework. Proceedings of OSDI, pages
599–613, 2014.
[20] Z. Guo, X. Wang, J. Tang, X. Liu, Z. Xu, M. Wu, M. F.
Kaashoek, and Z. Zhang. R2: An application-level kernel forrecord and replay. In Proceedings of the 8th USENIX
conference on Operating systems design and implementation,pages 193–208. USENIX Association, 2008.
[21] R. Ikeda, H. Park, and J. Widom. Provenance for generalized
map and reduce workﬂows. In In Proc. Conference on
Innovative Data Systems Research (CIDR), 2011.
[22] M. Interlandi, K. Shah, S. D. Tetali, M. A. Gulzar, S. Yoo,
M. Kim, T. Millstein, and T. Condie. Titian: Data
provenance support in spark. Proc. VLDB Endow.,
9(3):216–227, Nov. 2015.
[23] V . Jagannath, Z. Yin, and M. Budiu. Monitoring and
debugging dryadlinq applications with daphne. In Parallel
and Distributed Processing Workshops and Phd F orum(IPDPSW), 2011 IEEE International Symposium on, pages
1266–1273. IEEE, 2011.
[24] T. J. LeBlanc and J. M. Mellor-Crummey. Debugging
parallel programs with instant replay. Computers, IEEE
Transactions on,
 100(4):471–482, 1987.
[25] K. H. Lee, N. Sumner, X. Zhang, and P. Eugster. Uniﬁed
debugging of distributed systems with recon. In Dependable
Systems & Networks (DSN), 2011 IEEE/IFIP 41stInternational Conference on, pages 85–96. IEEE, 2011.
[26] X. Liu, Z. Guo, X. Wang, F. Chen, X. Lian, J. Tang, M. Wu,
M. F. Kaashoek, and Z. Zhang. D3s: Debugging deployeddistributed systems. In Proceedings of the 5th USENIX
Symposium on Networked Systems Design and
Implementation, NSDI’08, pages 423–437, Berkeley, CA,
USA, 2008. USENIX Association.
[27] D. Logothetis, S. De, and K. Yocum. Scalable lineage
capture for debugging disc analytics. In Proceedings of the
4th annual Symposium on Cloud Computing, page 17. ACM,
2013.
[28] X. Meng, J. K. Bradley, B. Yavuz, E. R. Sparks,
S. Venkataraman, D. Liu, J. Freeman, D. B. Tsai, M. Amde,
S. Owen, D. Xin, R. Xin, M. J. Franklin, R. Zadeh,M. Zaharia, and A. Talwalkar. Mllib: Machine learning inapache spark. CoRR, abs/1505.06807, 2015.
[29] R. H. Netzer and B. P. Miller. Optimal tracing and replay for
debugging message-passing parallel programs. The Journal
of Supercomputing, 8(4):371–388, 1995.
[30] C. Olston and B. Reed. Inspector gadget: A framework for
custom monitoring and debugging of distributed dataﬂows.InProceedings of the 2011 ACM SIGMOD International
Conference on Management of data, pages 1221–1224.
ACM, 2011.
[31] C. Olston, B. Reed, U. Srivastava, R. Kumar, and
A. Tomkins. Pig latin: a not-so-foreign language for data
processing. In Proceedings of the 2008 ACM SIGMOD
international conference on Management of data, pages
1099–1110. ACM, 2008.
[32] S. Salihoglu, J. Shin, V . Khanna, B. Q. Truong, and
794J. Widom. Graft: A debugging tool for apache giraph. In
Proceedings of the 2015 ACM SIGMOD International
Conference on Management of Data, pages 1403–1408.
ACM, 2015.
[33] J. Shafer, S. Rixner, and A. L. Cox. The hadoop distributed
ﬁlesystem: Balancing portability and performance. InPerformance Analysis of Systems & Software (ISPASS), 2010IEEE International Symposium on, pages 122–133. IEEE,
2010.
[34] W. Shang, Z. M. Jiang, H. Hemmati, B. Adams, A. E.
Hassan, and P. Martin. Assisting developers of big data
analytics applications when deploying on hadoop clouds. InProceedings of the 2013 International Conference onSoftware Engineering, ICSE ’13, pages 402–411,
Piscataway, NJ, USA, 2013. IEEE Press.
[35] J. Tan, X. Pan, S. Kavulya, R. Gandhi, and P. Narasimhan.
Salsa: Analyzing logs as state machines. In Proceedings of
the First USENIX Conference on Analysis of System Logs,
W ASL’08, pages 6–6, Berkeley, CA, USA, 2008. USENIX
Association.
[36] A. Thusoo, J. S. Sarma, N. Jain, Z. Shao, P. Chakka,
N. Zhang, S. Antony, H. Liu, and R. Murthy. Hive-a petabyte
scale data warehouse using hadoop. In Data Engineering
(ICDE), 2010 IEEE 26th International Conference on, pages996–1005. IEEE, 2010.
[37] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan.
Detecting large-scale system problems by mining consolelogs. In Proceedings of the ACM SIGOPS 22nd symposium
on Operating systems principles, pages 117–132. ACM,2009.
[38] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauley, M. J. Franklin, S. Shenker, and I. Stoica.
Resilient distributed datasets: A fault-tolerant abstraction for
in-memory cluster computing. In Proceedings of the 9th
USENIX conference on Networked Systems Design andImplementation, pages 2–2. USENIX Association, 2012.
[39] A. Zeller and R. Hildebrandt. Simplifying and isolating
failure-inducing input. Software Engineering, IEEE
Transactions on, 28(2):183–200, 2002.
[40] H. Zhou, J.-G. Lou, H. Zhang, H. Lin, H. Lin, and T. Qin. An
empirical study on quality issues of production big dataplatform. In International Conference on Software
Engineering, Software Engineering In Practice (ICSE SEIP).
IEEE, May 2015.
795