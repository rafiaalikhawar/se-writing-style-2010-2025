POLLUX : Safely Upgrading Dependent Application
Libraries
Sukrit Kalra∗
IIIT Delhi, IndiaAyush Goel∗
IIIT Delhi, IndiaDhriti Khanna
IIIT Delhi, India
Mohan Dhawan
IBM Research, IndiaSubodh Sharma
IIT Delhi, IndiaRahul Purandare
IIIT Delhi, India
ABSTRACT
Software evolution in third-party libraries across version upgrades
can result in addition of new functionalities or change in existing
APIs. As a result, there is a real danger of impairment of
backward compatibility. Application developers, therefore, must
keep constant vigil over library enhancements to ensure application
consistency, i.e., application retains its semantic behavior across
library upgrades. In this paper, we present the design and
implementation of P OLLUX , a framework to detect application-
affecting changes across two versions of the same dependent non-
adversarial library binary, and provide feedback on whether the
application developer should link to the newer version or not.
POLLUX leverages relevant application test cases to drive execution
through both versions of the concerned library binary, records
all concrete effects on the environment, and compares them to
determine semantic similarity across the same API invocation for
the two library versions. Our evaluation with 16popular, open-
source library binaries shows that P OLLUX is accurate with no false
positives and works across compiler optimizations.
CCS Concepts
•Software and its engineering →Software maintenance tools;
Software testing and debugging; Dynamic analysis; Software
evolution; Software libraries and repositories;
Keywords
Software maintenance, Library upgrade, Dynamic binary analysis.
1. INTRODUCTION
In current times, library driven software development is a reality
and use of third-party libraries is central to the development of
a large number of applications. However, this software reuse
comes at a cost—the included libraries can severely impact the
maintainability of software systems. Evolution of third-party
libraries may not always ensure backward compatibility, and
may introduce new functionalities altering existing APIs across
∗Both authors contributed equally.(47) (48) static void dump(double value, string &out) {
(48) -char buf[32];
(49) -snprintf(buf, sizeof buf, "%.17g", value);
(50) -out += buf;
(49) +if (std::isfinite(value)) {
(50) +char buf[32];
(51) +snprintf(buf, sizeof buf, "%.17g", value);
(52) +out += buf;
(53) + }else {
(54) +out += "null";
(55) + }
(51) (56) }
Figure 1: Dropbox’s minor ﬁx [ 6] could break applications.
major upgrades. Thus, developers must keep constant vigil over
library enhancements to ensure application consistency, i.e., the
application retains its semantic behavior across library upgrades.
A seven year study [ 44] of library release history in Maven
Central, involving 150K binary JAR ﬁles, revealed that one third
of all releases introduced at least one change that broke backward
compatibility. This ﬁgure remained unaffected whether the library
release was a major or a minor upgrade. Thus, choosing to update
the library dependencies of an application is a double-edged sword,
and demands thorough assessment of the effort needed to update
the dependencies and the potential beneﬁts achieved by updating.
For example, according to the JSON standard the values NaN
andInfinity should be serialized to null . However, Dropbox’s
json11library, which provides JSON parsing and serialization,
usedsnprintf in itsdump function to emit a string that was not
compliant with the JSON standard. Thus, a minor ﬁx as shown
in Fig. 1, emitted signiﬁcantly different JSON output for several
applications, potentially breaking some functionality. This paper
tackles the problem of whether a developer can safely upgrade a
dependent library without affecting application functionality.
Prior work [ 30,41,43–45] has acknowledged the importance of
dependencies in software management, and has empirically studied
the effects of “update lag”, “freshness”, “quality”, and “popularity”
on dependency management. Teyton et al. [47] study library
upgrades for J AVA software, but focus entirely on reasons and
frequency of upgrades. However, none of the prior work focuses
on the application developer’s dilemma of whether library upgrades
would break critical application functionalities. Also, most prior
work rely on the analysis of source code, which may not even be
available for several third-party libraries.
In this work, we present P OLLUX , a framework that detects
application-affecting changes across two versions of the same
library binary, and provides feedback on whether the application
developer should link to the newer version. P OLLUX builds upon
the observation that any critical, functionality-affecting API change
in the newer version would manifest as a new or distinct concrete
effect, such as memory writes and system calls. In absence of
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
FSE’16 , November 13–18, 2016, Seattle, WA, USA
c2016 ACM. 978-1-4503-4218-6/16/11...$15.00
http://dx.doi.org/10.1145/2950290.2950345
290any such differences, the API invocation in the newer version is
semantically similar to the older version. In other words, if the
test cases elicit the same concrete effects upon invocation of the
APIs across both versions, the execution is functionally similar;
thus migrating to the newer version will not impact the application.
POLLUX leverages relevant application test cases (or the
exhaustive library test suite, if it is open source) to drive execution
through both versions of the concerned library binary, and records
all concrete effects on the environment. P OLLUX then uses a
custom algorithm to compare these effects and determines the
behavioral similarity across the same API invocation for the two
library versions. In case of any dissimilarity, P OLLUX lists the
offending API call (in the newer version). While P OLLUX is
generic and applicable to all binaries, it is speciﬁc to libraries in the
way that it assumes consistency of API interfaces across releases.
POLLUX is not a binary differencing tool; its goal is to detect
semantic incompatibilities resulting from library upgrades. Note
that P OLLUX ’s effectiveness is contingent upon the exhaustiveness
of the test suite that drives it. Test suite expansion will
monotonically increase the differentials discovered by P OLLUX .
Further, any code path either added, removed or modiﬁed, if not
traversed by the test suite, will result in P OLLUX missing the
changes. In general, determining behavioral differences across
binaries is challenging for two major reasons. First, binaries
might be obfuscated or compiled with separate optimization
levels (say -O0 and-O3) resulting in different binaries. Thus,
graph isomorphism based techniques relying solely upon structural
similarities in control ﬂow also fail in light of these optimizations.
Second, issue of scalability is ever present. Static and symbolic
execution techniques perform precise semantic analysis, but suffer
from issues of scaling to large binaries.
POLLUX overcomes these challenges using a path-sensitive
dynamic binary analysis technique to identify behaviorally similar
code segments in a library binary. Speciﬁcally, P OLLUX generates
a dynamic call graph for all library functions traversed during each
library API invocation for both versions of the library. It then
populates each node in the call graph with metadata to reﬂect
concrete effects of execution associated with that function call, i.e.,
writes to stack and heap, and system calls observed during that
speciﬁc function invocation. P OLLUX aggregates these concrete
effects into a function signature, i.e., a minimal set of concrete
effects uniquely identifying the function invocation even across
multiple executions for the same given inputs.
POLLUX leverages these function signatures to correlate similar
functionality traversed during the API execution across the two
library binaries. Any pair of non-matching function nodes
indicates changes in either structural (i.e., code refactoring) and/or
semantic functionality across the two binaries. P OLLUX collates
all such functionality changes for the given application library and
notiﬁes the application developer what speciﬁc API functionality is
affected by linking to the newer library binary.
We have implemented a prototype of P OLLUX for x86binaries
using Intel’s PIN dynamic binary instrumentation framework [ 18,
39]. We have applied it to 16popular, open-source libraries, and
our evaluation shows that P OLLUX correctly identiﬁes semantic
differences with no false positives for libraries under consideration.
POLLUX ’s core signature extraction algorithm reports a precision
of>99% on call graphs generated for library APIs corresponding
to adjacent versions for all 16open-source libraries.
This paper makes the following contributions:
(1) We provide a practical design (§ 4) for P OLLUX along with
its novel dynamic binary analysis framework, which is robust,
accurate, and precise.(1)int main() {
(3)int*ptr;
(4)ptr = (int *)malloc(sizeof(int));
(10)*ptr =25;
(11)printf("%d",*ptr);
(12)free(ptr);
(13)return 0;
(14) }
Figure 2: Example source code.
(1)push %rbp
(2)mov %rsp,%rbp
(3)sub $0x10,%rsp
(4)mov $0x4,%edi
(5)callq 400510 < malloc@plt >
(6)mov %rax,-0x8(%rbp)
(7)mov -0x8(%rbp),%rax
(8)movl $0x19,(%rax)
(9)mov -0x8(%rbp),%rax
(10)mov (%rax),%eax
(11)mov %eax,%esi
(12)mov $0x400709,%edi
(13)mov $0x0,%eax
(14)callq 4004e0 < printf@plt >
(15)mov -0x8(%rbp),%rax
(16)mov %rax,%rdi
(17)callq 4004c0 < free@plt >
(18)mov $0x0,%eax
(19)leaveq
(20)retq
(21)nopw 0x0(%rax,%rax,1)
(a) Example compiled w/ gcc -O0.(1)push %rbx
(2)mov $0x4,%edi
(3)callq 400520
<malloc@plt >
(4)mov $ 0x19,%edx
(5)mov %rax,rbx
(6)movl $ 0x19,(%rax)
(7)mov $0x400719,%esi
(8)mov $0x1,%edi
(9)xor %eax,%eax
(10)callq 400530
<__printf_chk@plt >
(11)mov %rbx,%rdi
(12)callq 4004e0 < free@plt >
(13)xor %eax,%eax
(14)pop %rbx
(15)retq
(b) Example compiled w/ gcc -O3.
Figure 3: Example code depicting challenges in binary analysis.
(2) We implement P OLLUX (§5) for x86binaries and evaluate
it (§ 6) on16open-source libraries to show its effectiveness in
determining functional similarity across library versions.
2. MOTIV ATION AND OVERVIEW
In this section, we motivate the need for P OLLUX . First,
we discuss two concrete scenarios describing the possible
incompatibilities arising due to library upgrades. Second, we
discuss the issues involving precise binary analysis that make the
problem of detecting semantic differences challenging.
(1)libxml crash : libxml v 2.7.6encountered a segmentation fault
with the upgraded zlib v 1.2.3.5because of a complete re-write
of the gz* APIs, which read and write gzip ﬁles [ 13]. The ﬁx to
libxml is to check the version of zlib and to use the current code for
ZLIB_VERNUM less than0x1230 . However, since both libxml and
zlib are widely deployed and numerous applications link to them
dynamically, older versions of libxml and also other applications,
without this ﬁx, will fail with newer versions of zlib.
(2)Winamp crash : Winamp v 5.666build3516 crashed due to a
buggy component plugin (in_mp 3.dll) [ 24]. The bug in the newer
version of the plugin was an unintended consequence of ﬁxing an
older bug in the metadata editor. The proposed solution involved
roll back of the plugin to build 3512 .
The above examples highlight two issues commonly observed in
software evolution. First, dependent API changes cascade all the
way down to the application, which might crash if the appropriate
changes are not handled gracefully. Second, feature enhancements
in third party components, specially libraries and plugins, can
easily introduce bugs leading to software crashes.
While newer technologies, like Docker [ 5], do remedy problems
introduced by dependencies, they are, aimed primarily for software
distribution alone. The core “dependency” issues (as discussed
above) still remain unsolved in the context of system software,
thereby motivating the need for P OLLUX , which provides feedback
291to the developer on whether a dependency upgrade would affect or
preserve the application’s semantic behavior.
KEY CHALLENGES .Precise analysis of generic software binaries
poses several major challenges [ 37].
•Binary formats do not strictly differentiate between code and
data making their analysis difﬁcult. Moreover, function boundaries
are not well-marked because return instructions are not mandatory.
•Binaries lack rich data types available in the source, and may
also lack symbolic information in release versions. Symbols and
types can otherwise be used to improve precision of the analysis.
•Modern microprocessor instruction sets are large and complex,
and many instructions have subtle differences, which, if ignored,
can make an analysis unsound. In addition, presence of indirect
jumps that calculate targets on-the-ﬂy, and overlapping instructions
that get resolved at runtime can make the analysis incredibly hard.
•The basic purpose of call andret instructions is to execute
a function call and a return, respectively. However, their usage to
perform indirect jumps is abusive and can confuse the analysis.
•Lastly, some machine architectures allow self-modifying
assembly code that can overwrite earlier code at the same address.
Thus, the actual instructions executed may not be even present in
the static disassembly of the binary.
OVERVIEW .Consider the example code shown in Fig. 2where
memory is allocated for an integer, updated to the value 25, and
ﬁnally deallocated. The compiled output under optimizations -O0
and-O3 are shown in Fig. 3aand 3b, respectively. Despite
signiﬁcant syntactic differences among the two versions, writes
to memory such as movl $0x19, (%rax) and system calls
(malloc ,printf , andfree ) are preserved along execution
paths. P OLLUX computes the signature of each function invocation
in an execution of the program by capturing the set of memory
writes and system calls performed by the function. The write set is
a singleton set containing write of 25to an address stored in %rax
register. The set does not change across compiler optimizations,
the optimizations can be thought of as idempotent operations with
respect to the function signature set. P OLLUX traverses through the
call graph obtained from a program execution and performs the said
activity repeatedly. At the end of the analysis, if no two function
nodes are found to be behaviorally dissimilar, P OLLUX declares the
two versions to be behaviorally similar.
3. FORMAL OVERVIEW
A software binary can be considered to be a ﬁnite set of ordered
pairs of inputs and execution traces (where trace is a sequence of
events executed under the input; formal deﬁnition of a trace is
presented in the ensuing text). Let this set be denoted by B. For
the purposes of this paper, the regression test suite or applications
invoking libraries-under-test (LUT) deﬁne the domain of the input
set, denoted by I. Thus, for each input, i∈ I, we denote a set of
execution traces of the program to be Bi:={τ|/an}bracketle{ti,τ/an}bracketri}ht ∈ B} . Such
a deﬁnition subsumes alterations to the program via semantically
equivalent code refactorings, compiler optimizations, speculative
out-of-order execution semantics of the hardware and (in the case
of concurrency) runtime scheduling. The set of all traces is denoted
byT. For each execution trace, an output value is produced. Since
IandTare ﬁnite sets and assuming programs to be deterministic,
the set of output values is also ﬁnite.
Given two binaries B,B′of a software and a ﬁxed regression
suiteI, we wish to discover whether for each b:=/an}bracketle{ti,τ/an}bracketri}ht ∈ B
(wherei∈ I,τ∈ T ) there exists a b′:=/an}bracketle{ti,τ′/an}bracketri}ht ∈ B′(where
i∈ I,τ′∈ T′), such that bandb′have the same output and samestate of system memory. This is commonly referred as input-output
equivalence in the literature. There are a few scenarios relating to
the above discussion that are relevant to the context of our problem
i.e., differential analysis of libraries:
(1) output of bandb′is same for a given iandτ=τ′, then clearly
semantics have remain unchanged across the two versions,
(2) output of bandb′is same for a given i, however τ/ne}ationslash=τ′, then
it mandates further analysis; while it is possible that the output of
two traces is equivalent, the side-effects of traces may differ leading
to different states of the system memory, altogether. Discovering
precise side-effects is a hard problem in the average-case, while
undecidable in the worst-case setting, and
(3) output of bandb′is not same for a given i; in such a case the
application developer must be notiﬁed that it may not be entirely
safe to upgrade the LUT.
3.1 Concrete Semantics
We begin by deﬁning a simple low-level language that captures
the essence of this work. The language uses pcas the program
counter (note that we interpret pcto be pointing to the location
of the instruction under execution), a ﬁnite set of integer registers
R={r1,···,rn}, a store m[.]that returns the contents at the
memory location of the argument. The set of expressions in the
language is denoted by Exp . For simplicity, we do not specify the
expressions in this language, although the expressions are allowed
to contain pc,R , andm[.]The set of program statements is denoted
byStmt . A statement s∈Stmt can be one of the following:
•a variable assignment, ri:=ewithri∈R,e∈Exp ,
•a memory access, m[e1] :=e2orr2=m[e1],e1,e2∈Exp ,
•a guarded jmp,jmp e 1,e2, wheree1,e2∈Exp , which jumps
thepcto the address evaluated from e2given the (guard) e1
evaluates to zero,
•a procedure call, p().
A statesof a program deﬁned in the above language is given
by a triple: /an}bracketle{tM,l,fr/an}bracketri}htwhereM:/an}bracketle{tρ,ζ/an}bracketri}htcaptures the state of
system’s memory. Function ρ:R→Zprovides valuations to
the registers, ζ:N→Zprovides the contents of the memory
addresses, l∈Nis the current address at which the control is , and
fr= (xi,···,xmax−1)is the sequence of addresses indicating
the frame structure of the stack. xmax is the maximum address
to which stack can grow and xiis the least address on the stack
(stack grows downwards). A state transitions to a new state upon
the execution of a statement in the following manner:
T[[ri:=e]](s) :=s[ρ(ri)/mapsto→ρ([[e]](s))][l/mapsto→s(l)+1]
T[[ri:=m[e1]]](s) :=s[ρ(ri)/mapsto→ζ([[e1]](s))][l/mapsto→s(l)+1]
T[[m[e1] :=e2]](s) :=s[ζ([[e1]](s))/mapsto→[[e2]](s)][l/mapsto→s(l)+1]
T[[jmpe1,e2]](s) :=/braceleftBigg
s[l/mapsto→sl+1] if[[e1]](s)/ne}ationslash= 0
s[l/mapsto→[[e2]](s)]otherwise
We assume that [[e]](s)is a deterministic evaluation function of
statement ein states.T[[e]](s)is essentially a state transformer
function that produces a resultant state when statement eis
executed from state s. The map /mapsto→updates/adds speciﬁc
entries within a state. Finally, T[[q()]](s)is modeled by register
assignment statements modeling two important aspects of the
function call: stack frame allocation and deallocation. The body
of the procedure qis modeled by statements of the language. At
the time of allocation, the stack is extended by the frame size of
q(). Thus, for stack allocation:
292T[[rfp=rfp−c]](s) =s[ρ(xi)/mapsto→(ρ(xi)−c)(s)]
[f/mapsto→(xi−c,xi···,xmax−1)]
rfpis the register reserved for storing current frame pointer.
Similarly, after the execution of the body of the function, the
current stack frame at state swould be deallocated with fr:=
(xi−c,xi···,xmax−1):
T[[rfp=rfp+c]](s) =s[ρ(xi)/mapsto→(ρ(xi)+c)(s)]
[f/mapsto→(xi···,xmax−1)]
A traceτof a program is a sequence of states s0,···sn−1with
s0as the start state. We assume that there exists function that maps
program behaviors to outputs, O:B →Z. We deﬁne the notion of
behavioral equivalence of two traces by the following deﬁnition:
Def. 3.1. Two traces τ,τ′arestrictly-similar when on input ithe
state of memory is equal at their ﬁnal states sn,s′
n, i.e.,sn(M) =
s′
n(M)andO(b) =O(b′)whereb=/an}bracketle{ti,τ/an}bracketri}htandb′=/an}bracketle{ti,τ′/an}bracketri}ht.
Although observing the output of a trace is self-evident,
observing the effects of a library code execution on system’s
memory is not straightforward. For instance, programs could
legitimately be writing addresses of memory locations as values
into registers or memory locations. Such memory-address writes
are bound to change even when the same program is executed
multiple times. On account of such complexity, we deﬁne the
notion of α-similarity of behaviors (as opposed to strict equivalence
as deﬁned above). The motivation behind α-similarity is to
accommodate such address-based writes to registers/locations. Let
Wτ:={ζ[[e1]](s)|s′,s∈τ,T[[m[e1] :=e2]](s′) =s}be the
multiset of values written into memory in the trace τ. Further,
letWτ|qbe the projection of writes performed by the procedure
qcalled in τ. Register writes are not taken into account since we
assume that often local or temporary values are written to registers.
Even if that were not the case, scalability of P OLLUX ’s analysis
demands that we drop tracking writes to registers (per § 4).
Def. 3.2. Two behaviors b,b′from binaries B,B′, respectively, on
a given input iareα-similar when the following conditions hold:
O(b) =O(b′)and|Wτ∩Wτ′|
|Wτ∪Wτ′|=α.
We denote α-similarity of behaviors by a relational operator ≃α,
ifb,b′areα-similar then b≃αb′. For a given input i, we abuse the
notation and apply it for traces τ≃ατ′when behaviors are found
to beα-similar. Note that Def. 3.2reduces to Def. 3.1whenα=1.
4.POLLUX
KEYIDEA.POLLUX relies on a key observation that any critical or
functionality affecting change in third-party code is accompanied
by corresponding side-effects, such as additional memory writes
or system calls. In other words, key semantic behavior, such
as memory writes external to the stack frame and system call
sequence, remains unchanged despite compiler optimizations.
POLLUX takes as input the two library binaries and a test suite
to drive execution of those binaries. For every test case, P OLLUX
generates a call graph with additional metadata, characterizing the
signature for each function invocation. Next, P OLLUX uses a
custom algorithm that analyzes the two execution traces to identify
semantically similar execution fragments. Execution segments that
do not match are reported to the developer.4.1 Execution Driver
The execution driver executes the test suite for both versions
of the binaries. Speciﬁcally, it invokes the trace collector to
start recording the effects of execution of each test case in the
test suite. Once execution with the ﬁrst binary is complete, the
execution driver (i) serializes the recordings to external storage,
(ii) dynamically links the test cases to second binary and executes
them, and (iii) signals the trace collector to start recording again.
4.2 Trace Collector
Out of the several possible function level features, such as count
of memory reads and writes, system calls, branching instructions,
and indirect jumps, only critical, functionality-preserving memory
writes and sequence of system call invocations remain unchanged
in face of different compiler optimizations ( -O0against-O3). This
unambiguity is because both these features abstract out all the
syntactic sugar (or operational mechanics), and are tightly linked to
the functionality (or semantics) itself. Hence, P OLLUX uses these
two features to determine semantic similarity.
The trace collector is responsible for detecting and recording
these two effects upon each test case execution. While it may be
best to passively monitor these side-effects, it is not possible to do
so for all effects, like writes to the memory. Recording such effects
would entail instrumenting the entire execution environment,
which would be prohibitively expensive. Thus, P OLLUX leverages
dynamic binary instrumentation for capturing effects of interest at
a ﬁne-grained level. This instrumentation preserves the intended
execution effects of the binary, while also executing the hooks to
capture additional metadata.
While binary instrumentation frameworks provide both coarse-
and ﬁne-grained hooks, P OLLUX instruments the given binary at a
per-instruction level granularity, thereby sacriﬁcing low execution
overhead in favor of high accuracy. For each instruction, P OLLUX
records the x 86instruction, and the corresponding data/address
values. Thus, the instrumented binary upon execution enables
POLLUX to keep precise track of: (1) data/addresses written to
memory, with distinction between stack and heap, (2) system calls
invoked along with their arguments, (3) calls to other imported
library functions via the Procedure Linkage Table ( plt), and (4)
function return values, if available. At the end of test suite
execution, the trace collector serializes the recorded values and
trace analyzer is invoked, which is described next.
4.3 Trace Analyzer
The trace analyzer takes as input the serialized recordings for
test suites corresponding to both library binaries, and determines
semantic similarity using a layered two phase analysis. First,
POLLUX deserializes each trace and creates a call graph with
each node decorated with function-level metadata corresponding
to memory writes and system call invocations. P OLLUX then
computes a precise signature for each function using metadata
on memory writes and asynchronous system call invocations, and
matches these function call nodes across the two execution traces
for structural similarities based on (i) caller-callee relationship, and
(ii) potential code refactoring. Second, if no dissimilar call nodes
are observed, then P OLLUX determines the execution sequences to
be semantically similar iff the sequences of synchronous system
call invocations observed across both the executions are same.
CALL GRAPH CONSTRUCTION .POLLUX determines call
context per instruction, and groups instructions with the same
context to build nodes in the call graph. Thus, while distinct
invocations of the same function generate distinct nodes in the call
graph, a recursive invocation generates a single node in the graph.
293GRAPH _MATCH (a, b, MatchSet)
Input: a,b : Call nodes in execution graph of current library.
MatchSet : Set of all matching node pairs.
Output: MatchSet : Set of all matching node pairs.
MatchSet = MatchSet ∪{/angbracketlefta, b/angbracketright};
Γa= Γa−Γb; Γb= Γb−Γa;
Ca= C HILDREN (a);Cb= C HILDREN (b);
foreach x∈Ca,y∈Cb: node_match(x,y) ∧ /angbracketleftx, y/angbracketright /negationslash∈MatchSet do
MatchSet = G RAPH _MATCH (x, y, MatchSet);
end
foreach x∈Ca: node_match(x, b) ∧ /angbracketleftx, b/angbracketright /negationslash∈MatchSet do
MatchSet = G RAPH _MATCH (x, b, MatchSet);
end
foreach y∈Cb: node_match(a, y) ∧ /angbracketlefta, y/angbracketright /negationslash∈MatchSet do
MatchSet = G RAPH _MATCH (a, y, MatchSet);
end
foreach x∈Ca,y∈Cb:/angbracketleftx, -/angbracketright /negationslash∈MatchSet ∧ /angbracketleft-, y/angbracketright /negationslash∈MatchSet do
/angbracketleftp, q/angbracketright= FIND(x, y);
MatchSet = G RAPH _MATCH (p, q, MatchSet);
end
FIND(a, b)
Input: a,b : Call nodes in execution graph for current library.
Output: O: Set of matching node pairs from the two graphs
Initialize: O=∅.
ifnode_match∗(a, b) thenO=O∪{/angbracketlefta, b/angbracketright};
foreach x∈{a}∪CHILDREN (a), y∈{b}∪CHILDREN (b)do
if/angbracketleftx, y/angbracketright /negationslash=/angbracketlefta, b/angbracketrightthenO=O∪FIND(x, y);
end
GRAPH _SIMILARITY (r, r′, MatchSet)
Input: r,r’ : Root node in library L and L’, respectively.
MatchSet : Set of all matching node pairs.
Output: Res: Boolean variable for match or no match
Initialize: MatchSet = ∅, Res = false .
foreach/angbracketlefta, b/angbracketright ∈FIND(r, r′)do
MatchSet = MatchSet ∪GRAPH _MATCH (a, b, MatchSet);
end
if|MatchSet |> tv∧Ωr= Ωr′then Res = true;
Algorithm 1: Match nodes with code refactoring.
4.3.1 Function-level Similarity
POLLUX constructs precise call graphs from deserialized
recordings using a shadow execution context, and updates
it on every new call instruction encountered in the trace.
Simultaneously, it populates the nodes in the call graph
with function-level metadata, such as writes to memory and
asynchronous system call invocations. P OLLUX considers an
asynchronous system call equivalent to a memory write due to
its non-blocking nature. Additionally, it maintains graph-level
metadata that includes the exact sequence of synchronous system
calls. The key observation here is that asynchronous system calls
can be reordered with other operations, but synchronous calls must
occur in sequence. Thus, semantic similarity must ensure that the
sequence of synchronous system calls is preserved across both the
executions. Any out of order synchronous system calls, which are
blocking in nature (unlike an asynchronous call), could potentially
indicate a different behavior, and hence, is not semantically similar.
Algorithm 1depicts the steps P OLLUX uses to determine
function-level similarity across two call graphs. P OLLUX leverages
node metadata to compute a signature for every function in the two
call graphs. It then uses the Sørensen-Dice index [ 20] (sv) and an
empirically determined threshold ( θ) to determine a partial match
between two call nodes. P OLLUX also considers code refactorings,
such as node splitting and inlining, while evaluating a function-
level match. Observe that Sørensen-Dice index is essentially an
instance of α−similarity as noted in Def. 3.2.
FUNCTION SIGNATURE .The signature of a function must (i)
be unique, and (ii) encapsulate semantic functionality. It follows
linearly from Def. 3.2that function signature of procedures must
factor in the writes performed by it. It may not always be
possible to track writes (writes performed by system calls), hencewe conservatively treat asynchronous system call invocations as
writes. Thus, for procedure q, its function signature is:
Γq=Wτ|q≈ /an}bracketle{t/hatwiderWτ|q,Sq/an}bracketri}ht (1)
where/hatwiderWτ|qis the multiset of all the memory writes of qthat are
observable, and Sqis the multiset of system call invocations in q.
The function signature for a speciﬁc call site, however, contains
values which are execution dependent, such as addresses generated
due to dynamic memory allocation; this introduces signiﬁcant
problem in deterministically assigning function signatures that do
not ﬂuctuate across repeated executions of the program. This
problem stems from the fact that at the instruction level, P OLLUX
cannot distinguish between concrete data values and memory
addresses, and accumulates both of them in the same write set.
Keeping just the concrete data value and removing the addresses
from the set of all memory writes would eliminate signiﬁcant
randomness in the function signature. Also, note that a function
must export values out of its scope to perform useful functionality,
which means the writes to function local variables result in no
critical semantic behavior. Thus, P OLLUX leverages process maps
to determine address ranges for the current stack frame, and
discards all write values within this range. Subsequently, function
return values via the stack and registers (refer § 3) are not included
in the function signature. Prior work [ 34] also notes that return
values do not contribute signiﬁcantly to the function signature.
SIGNATURE MATCH .A desirable signature matching scheme
must ensure (i) few or no false negatives, and (ii) low false
positives. However, an “exact” signature match solely from (1)
can potentially result in high false positives in case of compiler
optimizations, since these transformations may change the set of
concrete data values produced by the function. Thus, P OLLUX
leverages a similarity-based match between the write sets generated
by two functions aandb, based on the Sørensen-Dice index:
sv=2|Γa∩Γb|
|Γa|+|Γb|, where set operators ∪,∩,+are applied separately
to/hatwiderWτ|xandSq,x∈a,b. We consider α-similarity of functions
only when the index is greater than a certain threshold ( θ∈R>0).
The caveat is that due to reliance on a threshold value, it is possible
that the value is not sufﬁciently high leading to false negatives (i.e.,
functions that should match but did not) or sufﬁciently low, leading
to false positives (i.e., functions that should not match but did).
POLLUX uses function names, if available, to further improve
the precision of the matching scheme. We observe that in a non-
stripped binary where the symbol names are present, overloaded
functions have different names. Furthermore, in C ++, a function
name is a mangled version of its class hierarchy and its parameters,
which entails that every function name in the binary is unique.
Thus, in a non-adversarial setting where symbols may be present,
POLLUX utilizes the function names in addition to Sørensen-Dice
index to match functions across binary executions. Finally, it
is common for library developers to refactor code, i.e., inline or
outline functions, or split a function into several smaller units. Prior
art [ 27,34,49] has used function names as a heuristic in speciﬁc
cases and P OLLUX can leverage any of these more sophisticated
techniques as its precision isn’t contingent upon function names.
Note that with such function splitting, matching with function
names is futile. In addition, the refactored functions make
Sørensen-Dice index ineffective, since the index is based on
function similarity rather than inclusion relationship. In such cases,
using Sørensen-Dice index may lead to several false negatives.
In order to make signature-matching more meaningful in the
context of inclusion relationship, we introduce a new index
tv=|Γa∩Γb|
min(|Γa|,|Γb|)for functions aandb; this index, incidentally,
294also captures the results with the same precision as the Sørensen-
Dice index would have for cases where inclusion relationship was
absent. Thus, ﬁnally the node matching function is deﬁned as:
node _match(a,b) =/braceleftBiggtrue:tv≥θ
true:λa=λb∧θ′< tv< θ
false:Otherwise
whereλxreturns the name of the function x. Whentv< θ by a
small margin (i.e., θ−θ′= 5% ), then, function names are matched.
Refactoring suggests that our matching mechanism should be
capable of performing partial signature matches and also matches
with the remainder of the signatures after a partial match is
performed. In order to deal with partial matching and to maximize
structurally meaningful matching, P OLLUX uses the notion of
residual signatures [ 42]. Speciﬁcally, whenever P OLLUX matches
the signatures of two functions, it also updates their current
signatures with residual signatures (to be used for further matching)
as follows: Γa= Γa−ΓbandΓb= Γb−Γa.
(1)Inlining/Outlining : Library functions are often inlined to
achieve better performance. A function that is inlined adds its
memory writes and asynchronous system calls data to its caller’s
signature, i.e.,if function ahas its callee function binlined in a
newer version, it will result in the following signature of the new
function: Γa′=Wτ|a∪ Wτ|b. Similarly, a function that is
outlined as bin a newer version will have an opposite effect on
the signature of the counterpart of b’s caller function aas follows:
Γa′=Wτ|a−Wτ|b. The inclusion index tvcaptures this inclusion
relationship allowing nodes to be matched correctly.
(2)Splitting/Combining : To achieve stronger cohesion and better
maintainability, developers split functions. Splitting functions have
an impact on the signatures and matching which is similar to
outlining. More formally, when a function fthat is split/combined
into/from nfunctions f1,...,f n, the following relation holds:
Γf=Wτ|f1∪...∪Wτ|fn.
EMPIRICAL THRESHOLD (θ).POLLUX randomly selects test
cases for APIs that remain unmodiﬁed across the two neighboring
versions of the library (corroborated by the commits), and
determines tviteratively till the number of unmatched function
nodes across the call graphs (corresponding to the two library
executions) is less than 1%. In other words, at least 99% function
nodes must match at this tv. This ﬁnal value of tvis the threshold θ.
The above iterative approach has the beneﬁt that for most common
cases,θreﬂects the lower bound of similarity between semantically
similar functions. Any value of tv≤θthat causes the number of
unmatched function nodes to increase above 1%indicates, with a
high probability, that the functionality has indeed changed.
4.3.2 Graph-level Similarity
In order to demonstrate similarity of two call graphs, P OLLUX
additionally handles the case when there are blocking system calls
issued. It uses function Ωx=/an}bracketle{tsi,...,s j/an}bracketri}htandsi,...,s jis the
sequence of synchronous system calls observed in the execution.
4.4 Diagnosis
In case of dissimilar nodes, P OLLUX traverses the function
signature to determine the cause as either an extraneous
data/address value or system call (or their order). In each case, the
application developer receives a feedback indicating the offending
API invocation, along with the entire call sequence leading up to
the function responsible for the unmatched data/address value or
system call that caused the dissimilarity.
COMPARISON WITH PRIOR ART .POLLUX ’s signature for
matching functions across two execution graphs is robust andeffective (as will be shown later in § 6). Unlike prior art [ 33–35,42],
POLLUX leverages a layered approach to determining semantic
similarity, and uses only the most critical side-affecting features,
i.e., memory writes and system calls, which remain constant
even across various compiler optimizations. Like BLEX [ 34],
POLLUX also leverages dynamic binary analysis, but uses far fewer
features to create succinct signatures. Additionally, BLEX aims for
instruction coverage and generates random inputs for differential
analysis that is not path-directed, thereby exploring infeasible
paths and leading to several false positives. Unlike BinDiff [ 33]
and [ 35], which use graph isomorphism techniques, P OLLUX does
not rely upon structural similarity and function names alone. Thus,
POLLUX ’s signature built using dynamic mechanism is robust
even under various compiler optimizations. Unlike [ 42], where
a function signature includes allvalues read or written and are
humongous, P OLLUX uses only concrete data values and system
call sequence to generate crisp function signatures.
4.5 Compiler Optimizations
Compiler optimization levels, such as -O3, are extremely
aggressive and typically generate an execution graph that is
signiﬁcantly different from the one generated at level -O0. In fact,
optimization -O3 is akin to code refactoring at the assembly level.
However, no amount of optimization should alter the functionality
critical memory writes and sequence of system call invocations.
In the absence of any structural similarity, P OLLUX discards
the function level signature matching and instead compares the
aggregate set of write, and sequence of system call invocations.
POLLUX leverages the index tvfor comparison across optimization
levels. Since level -O3 discards several intermediate memory
writes,Γa⊆Γbfor two binaries aandbcompiled for the same
source code with levels -O3 and-O0 respectively. In other words,
tv≈1indicates semantic similarity between binaries aandb.
Note that P OLLUX ’s target is primarily application developers
who include benign, third-party libraries, and typically, developers
do not change compiler optimizations frequently for production-
level code. Thus, matching semantic similarity across optimization
levels is not the common case for P OLLUX .
5. IMPLEMENTATION
We implemented a prototype of P OLLUX based on the design
described in § 4. While the trace analyzer and collector were
automated and required ∼900 lines of C++to implement, the
execution driver was triggered manually. We leveraged the Intel
PIN [ 18,39] dynamic binary instrumentation framework (v 2.14)
because of its ease of use in instrumenting the library binaries, and
recording execution side-effects. We wrote a minimal “pintool”,
which is code that the PIN framework injects dynamically at
selected points during instruction sequence, to extract relevant
execution metadata and build a call graph for the given execution.
(1)Call graph construction . In assembly, function transitions,
i.e., invocations and returns, happen via the call*,jmp*andret
family of instructions. P OLLUX maintains a shadow stack of call
context by leveraging PIN APIs to extract the function name at each
transition instruction. However, we observed that a few functions
did not have an explicit retinstruction, leading to anomalous call
graphs. P OLLUX overcomes this challenge by discarding the use of
instruction-level instrumentation and switching to instrumentation
at the granularity of a T RACE1. Since, a T RACE is part of exactly
1A T RACE is a straight-line instruction sequence with exactly one entry
point. It usually ends with an unconditional branch, such as a call, return or
unconditional jump. However, a T RACE may include multiple exit points
295one function invocation, P OLLUX invokes PIN APIs at the start
of each T RACE to determine the function name, which helps to
reliably maintain the shadow stack of call contexts. Note that PIN
cannot reliably instrument functions in the presence of tail calls or
when return instructions cannot reliably be detected [ 19]. Thus we
did not use PIN’s function-level instrumentation.
(2)Detecting writes to stack : Data values written to the stack
mostly correspond to non-critical function local operations. Hence,
it is important to discard them, so that the function signature
uniquely identiﬁes only critical functionality. Thus, P OLLUX
determines the address range available to the current process for
writing to the stack frame, and removes from its write set any
value written to an address within this range. To do so, P OLLUX
determines the process id for the currently executing test case, and
reads the corresponding process maps from the /proc ﬁle system
to determine the permissible stack range.
POLLUX leverages several optimizations to speed up the overall
analysis and improve precision.
•TRACE -level instrumentation : Instruction instrumentation
incurs signiﬁcant overheads (due to dynamic code injection before
every instruction for call graph construction), and also induces
anomalies in construction as discussed earlier. P OLLUX ’s use of
PIN’s T RACE -level instrumentation not only improves accuracy but
also reduces the number of instrumentation points, which speeds up
analysis by an order of magnitude.
•Pruning the call graph : P OLLUX prunes the call graph for
faster analysis. Speciﬁcally, the call graph starts at the API entry
point and continues till the execution hits any glibc method
invocations, like those corresponding to memory allocation and
management, system calls, etc. The key observation here is that
glibc and other system libraries, like ldlinux , provide access
to fairly low-level functionalities to several system components,
which change much less frequently compared with application
libraries. Furthermore, a change in system libraries often
necessitates upgrading the entire system and its dependencies. Not
traversing the call graph before the API entry point and after the
glibc function invocations signiﬁcantly reduces the size of the call
graph to be analyzed for semantic similarity.
•Improving signature precision : As explained in § 4.3.1 ,
POLLUX identiﬁes function local writes to memory to remove noise
from the function signature. To further improve the signature,
POLLUX executes each test case twice (linked to the same library
binary), and takes an intersection between the side-effects observed
across the two executions. The intuition here is that functionality
preserving side-effects, such as writes and system calls, would
remain unaffected and be present in the intersection.
6. EV ALUATION
In § 6.1, we evaluate P OLLUX for accuracy of detecting
semantically relevant changes across several macrobenchmarks
consisting of user applications. In § 6.2, we determine the precision
of P OLLUX ’s signature matching algorithm. In § 6.3, we determine
the effectiveness of the various optimizations described earlier
in § 5. In § 6.4, we check P OLLUX ’s robustness across compiler
optimizations. Lastly, in § 6.5, we present our experiences with
POLLUX and demonstrate its utility in diverse conditions.
EXPERIMENTAL SETUP .All experiments were performed atop a
VM having 4VCPUs at 2.50GHz, provisioned with 8GB of RAM,
and running 64bit Ubuntu v 12.04with Intel PIN v 2.14installed.
as long as they are conditional. If PIN detected a branch to a location within
a TRACE , it will end the T RACE at that location and start a new T RACE .DATA SET .We chose 16popular, open-source C/C ++libraries
from GitHub repositories (see Table 1), and randomly selected
commit versions along with their test suites. We then manually
inspected source code and the release notes corresponding to
these commit versions and corroborated each such change. While
POLLUX ’s analysis is entirely automatic, this manual involvement
to validate our results limits the number of libraries analyzed.
EMPIRICAL DETERMINATION OF THRESHOLDS . We
empirically determined the thresholds θandθ′for each library
(as described earlier in § 4.3). We observed that at θ= 0.95,
the fraction of unmatched nodes was <1%across all libraries in
our data set. Higher value of θmeans a stricter check and would
increase the fraction of unmatched nodes, while a lower value of θ
indicates a more relaxed check and would have fewer unmatched
nodes. Note that for a different corpus of libraries, θmight vary.
We further selected θ′= 0.95∗θ.
6.1 Accuracy
We determine P OLLUX ’s accuracy when one or more dependent
libraries for a user application have changed. We consider
POLLUX ’s output as accurate if it correctly determines changes in
library code based on unmatched function nodes ( <1%) in the call
graphs and system call order, which must be preserved for semantic
similarity. We capture ground truth for the concerned scenarios
using commits from the open-source corresponding repositories.
We observe that barring a few security updates where code may
get removed, as in the Heartbleed bug [ 22], most bug patches and
feature enhancements either add new code or alter existing library
code syntactically. We leverage the library test suites since the
existing application test suites may not cover the entire gamut of
functionality and run the suite with the two different versions. If
the fraction of unmatched nodes is greater than 1%atθ= 0.95,
or there was a change in the system call order or count, P OLLUX
concludes a semantic change in the existing library version.
Table 1reports our results. We observe that P OLLUX manages
to capture even subtle changes, such as in json 11, where a mere 4
line change in the dump function (see Fig. 1) introduced signiﬁcant
changes across several other API executions, leading to ∼7%
unmatched nodes across the test suite.
POLLUX correctly detects semantic changes in 28out of the
30scenarios tested. P OLLUX reports false alarms for some
Capstone andValijson test cases. On manual inspection of
the commit logs, we observed that the Capstone library did not
have a test case that traversed the modiﬁed code. Since P OLLUX
leverages dynamic analysis, paths not traversed in the code are
not validated for semantic changes. Hence, P OLLUX reported no
change in semantic similarity across the two Capstone versions.
InValijson , we observed that the modiﬁed API introduced no
extra nodes. Furthermore, the code introduced only a conditional
statement, which was not traversed by any of the test cases, similar
to theCapstone:ppc scenario. Thus, P OLLUX correctly reports
semantic similarity in each of the 30cases, thereby having a 100%
accuracy for libraries under consideration.
6.2 Precision
We determine the precision of P OLLUX ’s function signature
matching algorithm under the setting where there are no semantic
differences for a given library API across the two versions. In such
a scenario, we deﬁne precision as: η= 1−(n/N), whereNis
the total number of nodes in the call graph, and nis the unmatched
nodes across the two versions of the library.
We select test cases for each library where the API does not
change semantically across the two versions. We corroborate this
296Table 1: P OLLUX accuracy across patches, minor and major revision changes for various libraries at θ= 0.95. Note that P OLLUX
detects a semantic change if unmatched nodes are >1%and system call order is not preserved. ∗indicates all APIs in the test suite.
# Application Library APIVersion / Commit # Nodes System callsDetectionOld New Total Unmatched % preserved
1 ArmExec Capstone [ 1] arm 3.0.3 3.0.4 5095 901.766 ✗ ✓
2 ArmExec Capstone mips 3.0.3 3.0.4 1291 201.549 ✓ ✓
3 ArmExec Capstone ppc 3.0.3 3.0.4 1856 60.323 ✓ ✗
4 ArmExec Capstone x86 3.0.3 3.0.4 4045 177 4.376 ✓ ✓
5 ArmExec Capstone xcore 3.0.3 3.0.4 1233 312.514 ✗ ✓
6 Visual Studio Catch [ 2] ∗ 1.2.0build451.3.5latest 26032 4432 17.025 ✓ ✓
7 Gazebo DevIL [ 4] ∗ 1.7.8:1f0d1.7.8:7241 64 46.250 ✓ ✓
8 TensorFlow Eigen [ 7] ∗ 3.2.7 3.2.8 366 41.093 ✓ ✓
9 TensorFlow Eigen ∗ 3.2.7 3.2.8 3340 601.796 ✓ ✓
10 TensorFlow Eigen ∗ 3.2 3.2.8 251 8132.271 ✓ ✓
11 Boost Fit [8] ∗ e3bf390 52b54cd 76 67.895 ✓ ✓
12 Boost Fit ∗ 52b54cd 66976be 230 114.783 ✓ ✓
13 SageMath GSL [ 9] ∗ 1.9 1.10 11932 354 2.967 ✓ ✓
14 SageMath GSL ∗ 1.9 2.0 11932 787 6.596 ✓ ✓
15 NodeJS http-parser [ 10] parse_url ab0b16 7d75dd 99 22.020 ✓ ✓
16 NodeJS http-parser execute ab0b16 7d75dd 71786 7161 9.975 ✓ ✓
17 Dropbox json11 [ 6] dump 019364 0e8c5b 534 417.678 ✓ ✓
18 Dropbox json11 parse 019364 0e8c5b 1077 746.871 ✓ ✓
19 CMake libarchive [ 11] ∗ 3.0.2 3.1.0 14991 8327 55.55 ✓ ✓
20 AlsaPlayer libcurl [ 3] ∗ 7.20.0 7.21.1 1372 25018.222 ✓ ✓
21 AlsaPlayer libcurl ∗ 7.20.0 7.47.0 1008 39639.286 ✓ ✓
22 Deluge libtorrent [ 12] ∗ 1.0 1.1 1459 55437.971 ✓ ✓
23 Aisoy Onion [ 14] response_new c812b35 88a659 786 486.107 ✓ ✓
24 Quinoa PEGTL [ 16] ∗ 1.2.1 1.2.2 34010 3013 8.859 ✓ ✓
25 Quinoa PEGTL ∗ 1.1.0 1.2.2 25182 1535 6.096 ✓ ✓
26 CBDM spdlog [ 21] ∗ c7864ae e248895 1880 22011.702 ✓ ✓
27 Puppet Valijson [ 23] ∗ b241b37 e9b5016 3343 170.509 ✓ ✗
28 OpenSSH zlib [ 25] ∗ 2689b c58f7a 1648 935.643 ✓ ✓
29 OpenSSH zlib ∗ 1.2.7 1.2.8 1278 856.651 ✓ ✓
30 OpenSSH zlib ∗ 1.2.5 1.2.8 1204 48740.449 ✓ ✓
 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995 1
 0.85  0.9  0.95  1Pollux Precision ( η)
Theta ( θ)libarchive
libcurl
libtorrent
valijson
zlib
(a) Variation in precision with θ. 99.5 99.6 99.7 99.8 99.9 100
capstonecatchdevileigenfitgslhttp-parserjson11libarchivelibcurllibtorrentonionpegtlspdlogvalijsonzlib% of Total MatchesFrequency
1 2 3 4
(b) Frequency of node matches at θ= 0.95. 10 100 1000 10000 100000
 1000  10000  100000Match Time (ms)
# Nodes in Call Graphθ=0.95
Linear fit
(c) Variation in graph comparison time.
Figure 4: Variation in P OLLUX ’s precision and other properties.
Table 2: P OLLUX Precision with 16libraries at θ= 0.95.
# LibraryVersion / Commit # NodesPrecisionOld New Total Unmatched
1Capstone b560c2c508c456076 160.999
2Catch ae5ee2cf895e0d27753 01.000
3DevIL 1.7.8:cdc31.7.8:c806 264 20.992
4Eigen a1430dd08e2e06 2323 110.995
5Fit 953b721 ec7a043 2002 01.000
6GSL 5c14 002f 13851 01.000
7http-parser bee48 4e382f72537 01.000
8json11 a6a66 e1d5b 2395 01.000
9libarchive 3.1.0:58183.1.2:19f215222 50.999
10 libcurl 3c2e e506 2652 01.000
11 libtorrent 1.0.9 1.1.0 4270 390.990
12 Onion d7eb5b7801bb9b4540 01.000
13 PEGTL 02ba 9e3b 44078 01.000
14 spdlog c0c5c01a6a661e587 01.000
15 Valijson 1ade1c5b241b37 3343 170.995
16 zlib 1.2.7.21.2.7.3 1281 20.998
API similarity by inspecting commits to the library repository. We
run P OLLUX for these test cases and measure the number of nodes
that match across the API call graphs for the two executions. Weobserve that on average P OLLUX reports a high precision ( >0.99)
across all test cases (see Table 2). We also note that libtorrent
reports a high number of unmatched nodes, because the particular
test case downloads a ﬁle, and thus several operational parameters,
such as available network bandwidth and bytes downloaded,
change signiﬁcantly across two execution traces, thereby causing
POLLUX to report the high number of unmatched nodes.
(1)Variation in precision with θ: We select ﬁve libraries from
our data set and plot the variation in P OLLUX ’s precision for
different values of θ=0.85,0.90,0.95, and1.00. Fig. 4aplots
the results. We observe that as θincreases, precision ηdecreases
because in a stricter setting fewer function nodes match.
(2)Frequency of node matches : We measure the frequency of
matches for nodes whose function signature matched to determine
the aggressiveness of P OLLUX ’s signature matching algorithm.
Fig.4bplots the results for all libraries in our data set at θ= 0.95.
The y-axis in the graph starts at 99.5%. We observe that across
all libraries P OLLUX correctly matches >99.5%of nodes, which
297Table 3: Effect of pruning glibc nodes at θ= 0.95.
#LibraryVersion # Nodes (w/o opt.) # Nodes (w/ opt.) Savings
Old New Total Unmatched Total Unmatched (%)
1GSL5c14002f25966 013851 046.66
2json11a6a66e1d5bc7814 632395 069.35
3Oniond7eb5801bb725 8699 03.59
4PEGTL02ba9e3b53621 25732706 039.01
5zlib1.2.7.21.2.7.31969 31281 234.94
Table 4: Effect of re-execution on signature size θ= 0.95.
# LibraryVersion/ Avg. Signature Size Savings
Commit w/o opt. w/ opt. (%)
1 Capstone b560c2 21.61 21.11 2.34
2 Eigen 3.2.7 52.37 43.62 16.72
3 Fit 9e132 17.81 16.13 9.42
4 Onion 801bb 17.51 15.43 11.90
5 libtorrent 1.0.9 22.14 20.82 5.96
Table 5: Effect of re-execution on precision at θ= 0.95.
# LibraryVersion # Total Unmatched Nodes Savings
Old New Nodes w/o opt. w/ opt. (%)
1 Eigen a1430 08e2e2323 267 1195.88
2 libarchive 5818 19f215222 1833 599.73
3 libtorrent 1.0.91.1.04270 1460 3797.47
4 Onion d7eb5801bb4540 686 0100.00
5 PEGTL 02ba9e3b44078 2161 0100.00
indicates the effectiveness of the algorithm. Only Catch ,Eigen ,
libarchive andzlib had minuscule number of nodes with
multiple matches due to sparse or common function signatures.
(3)Variation in graph comparison time : We determine variation
in the match time with increase in nodes in the call graph. Fig. 4c
plots the results for all libraries, except GSL andlibarchive ,
atθ= 0.95. We observe that in general as the nodes increase,
the match time increases exponentially. However, both GSL and
libarchive show much higher matching times than normal,
which is possible since matching also depends on the structure of
the graph. Code refactoring can also alter the graph structure.
6.3 Effectiveness of Optimizations
(1)Pruningglibc nodes : We selected ﬁve libraries and executed
their entire test suites with and without this optimization enabled.
Table 3lists the results. We observe that pruning glibc nodes
alone not only decreases the number of unmatched nodes, but also
signiﬁcantly reduces the graph size by an average of ∼39% across
the ﬁve libraries under consideration.
(2)Library re-execution : We selected ﬁve libraries and executed
their test suites twice with P OLLUX . We then took the intersections
of the signature values to determine the function ﬁngerprints, and
subsequently the improvement in precision with and without this
optimization enabled. The rows in Tables 4and5indicate that the
intersection of function signatures from two executions provides
signiﬁcant savings, and reduces function signature by an average of
9.27% across the ﬁve libraries. Further, this optimization decreased
the unmatched nodes by >95% across the libraries.
6.4 Compiler Optimizations
We run P OLLUX against ﬁve library test suites dynamically
linked with corresponding libraries compiled with -O2 and-O3
compiler optimizations, and measure its effects on P OLLUX ’s
precision. Table 6lists the results. We observe that even across
the two optimization levels, P OLLUX retains reasonable precision
for most libraries except zlib . However, it drops signiﬁcantly
from>99% (per § 6.2) observed when determining semantic
similarity for binaries with the same optimization level. Note
that-O3 is an aggressive optimization level and includes function
inlining, unswitching loops, among others. Thus, θ= 0.95, whichTable 6: Effect of optimization on precision at θ= 0.95.
# LibraryVersion/ # Nodes Unmatched Precision
Commit -O2-O3 nodes (-O2 ) (%)
1 http-parser 5651a72455 72465 1786 97.54
2 json11 afcc87803 7855 250 96.80
3 libcurl 7.47.02652 2508 1252 52.79
4 Onion 51ceb 699 600 140 79.97
5 zlib 50893 1701 1262 1065 37.39
Table 7:tvacross-O0and-O3compiler optimizations
# Library VersionAvg. Signature Size Signaturetv-O0 -O3 Intersection
1 Capstone d17fc399613 226594 191841 0.847
2 Eigen 3.2.7 2598 2142 1990 0.929
3 Fit 9e132 30 30 301.000
4 http-parser 5651a758369 442938 428776 0.968
5 json11 afcc8d78758 10900 10663 0.978
6 libtorrent 508cc 4270 4069 3889 0.956
7 Onion 51ceb 2778 2364 2256 0.954
8 spdlog c6f8f 467 467 4671.000
9 Valijson e9b50435678 325634 320004 0.983
10 zlib 50893 1033231 522940 522265 0.999
indicates an error margin of just 5%in similarity, is insufﬁcient
across optimization levels. We therefore need to recalibrate θfor
detecting semantic similarity across optimization levels.
We now brieﬂy compare P OLLUX ’s effectiveness with
BinDiff [ 33,35] and BLEX [ 34]. Since, BinDiff is a proprietary
tool, and BLEX’s source and binary are unavailable, we use
accuracy numbers available in [ 34]. Since P OLLUX ’s signature
matching is ineffective across huge structural changes, it leverages
mechanism as described in § 4.5to detect semantic similarity
across optimization levels. Table 7liststvobserved for 10libraries
from our data set across -O0and-O3compiler optimization levels.
We observe that P OLLUX determines semantic similarity with
96.1%accuracy on average across the 10libraries. In contrast,
BLEX and BinDiff report an accuracy of ∼50% across the same
optimization levels (per [ 34]), thereby making P OLLUX ’s accuracy
comparable to both BLEX and BinDiff.
6.5 Case Studies
(1)json11: json11is an open-source C ++library from Dropbox.
Commit0e8c5baﬁxes a bug where values like NaNandInfinity
were serialized to non-compliant values by snprintf causing the
deserializer to fail. The ﬁx involved adding a condition which
would return null if the number failed the std::isfinite check.
Even this small ﬁx resulted in an increase in the number of
unmatched nodes, especially in test cases concerning numbers. A
test case traversing the True branch of the conditional had 25of
286nodes (or 8.75% ) left unmatched in the call graph. In contrast,
theFalse branch had 11of235nodes (or 4.68% ) left unmatched.
We observed that across the entire test suite, 1716 nodes were
generated, of which 125 were unmatched (or 7.28% ). Also, a
change in the total number of nodes was observed only for test
cases that executed the changed code path.
(2)zlib: zlib is a hugely popular library used by git, rsync, libpng,
etc. Commit c58f7abreplaced unsafe functions like strcpy with
safer alternatives like snprintf . Such changes, wherein a function
has been replaced results in signiﬁcantly different memory writes.
For example, the return values of the two functions (although
unused), are totally different. strcpy returns the destination
character array, while snprintf returns the number of bytes
written. Also, unlike strcpy ,snprintf appends a NULL byte to
the buffer. P OLLUX detects this change and reports 93out of1648
nodes (or 5.64%) as unmatched in the graph.
(3)http-parser : HTTP-Parser is a dependent library for NodeJS.
Commit4e382f9had only minor changes to the documentation
298and did not modify the source code. P OLLUX generated a total of
72537 nodes for the entire test suite that match perfectly at θ=
0.95, thereby indicating semantic similarity.
Commit7d75dd73introduced support for Zone ID in IPv 6scoped
addresses in the http_parse_host API. Only one test case
(test_parse_url ) in the entire test suite invoked this API. This
test case generated a total of 99nodes of which 2were unmatched
(2.02% ) causing P OLLUX to ﬂag the change. For all other test
cases, a total of 72433 nodes were generated that matched perfectly
atθ= 0.95, thereby pin-pointing the modiﬁed API.
Commit0097dechanged the way the tokens are parsed by the
library’shttp_parser_execute API. Across the test suite, 3
tests did not invoke this API and generated a total of 154 nodes
that matched completely. Further, 6tests generated a total of 2337
nodes,10of which were unmatched ( 0.42% ) causing P OLLUX to
conclude that they did not traverse the affected code path. Lastly,
2tests that traversed the changed code path, generated 6355 nodes,
of which6300 were unmatched ( 99.13% ) indicating changes to the
API. Further, decreasing θto0.9caused all nodes to match.
(4)Capstone : Capstone is a disassembly framework popular in
the reverse engineering community. Commit c508c4a0added
support for the Travis CI build system and did not affect the source
code. P OLLUX reported a near perfect match across the entire test
suite. Of the 56076 nodes generated, only 16were unmatched
(0.03% ), indicating a trivial change.
Across a minor release upgrade from v 3.0.3to v3.0.4,seven cases
in the test suite had only 0.76% unmatched nodes indicating
insigniﬁcant changes to these modules. The remaining 5test
cases for the ARM, MIPS, x 86and XCore modules generated
15350 nodes, of which 385were unmatched ( 2.51% ). All changes
except those in the PowerPC module were correctly detected by
POLLUX . Upon further inspection, we observed that additional
error checking conditions were introduced. However, the test
cases did not traverse this newly introduced code change and thus,
POLLUX reported a false positive for PowerPC.
7. LIMITATIONS AND FUTURE WORK
•POLLUX cannot handle multi-threaded interleaved executions.
This limitation stems from its design, which requires deterministic
comparison of side-effects resulting from individual test case
executions. In contrast, multi-threaded executions introduce
signiﬁcant non-determinism in the set of captured side-effects.
•Since P OLLUX leverages test suites for its dynamic binary
analysis, it cannot detect changes in code paths not traversed by
the test case. For example, P OLLUX cannot detect bug patches that
comment out an entire code path, such as the OpenSSH bug [ 15],
where the vulnerable code in the client was completely disabled.
•Like prior work [ 33–35,42,50], POLLUX cannot reliably detect
semantic similarities where side-effects involve random numbers,
time of day, etc. However, in our observation, most critical features
in mature libraries do not involve signiﬁcant randomness.
•POLLUX does not include writes to registers or stack in its
function signatures. Thus, it may miss values passed via stack or
when entire function computation leverages registers alone.
•There can be several programmatic ways to encode the desired
functionality. For example, a multiplication operation might be
achieved using repeated additions or just bit shifts. P OLLUX in
its present form cannot detect such semantic similarity, since it
leverages side-effects, which could be signiﬁcantly different for
both mechanisms. In future, we plan to augment P OLLUX with
symbolic execution capabilities to detect such semantic similarities.•POLLUX ’s function signature matching is O(n2), and requires
signiﬁcant computation for comparing graphs with several hundred
thousand nodes. We plan to optimize it as part of future work.
8. RELATED WORK
GENERAL PURPOSE PLATFORMS .BitBlaze [ 46] is a binary
analysis platform that leverages static and dynamic analysis
techniques, dynamic symbolic execution, and whole-system
emulation and binary instrumentation. Phoenix [ 17] requires
debugging information and thus, unlike BitBlaze, is not a binary-
only analysis platform. Both BitBlaze and Phoenix can also be used
to detect changes to binary dependencies. However, they employ
heavy machinery to achieve the desired result. In contrast, P OLLUX
uses light-weight and robust dynamic binary analysis techniques.
STATIC ANALYSIS .There exist several static binary analysis
platforms such as BAP [ 29], CodeSurfer/x 86 [26], and
Jakstab [ 38]. CodeSurfer/x 86and Jakstab ﬁrst disassemble binary
code, reconstruct call and control ﬂow graphs, and then perform
static analysis over the reconstructed control ﬂow. BAP lifts the
instructions to an intermediate language (IL), and then performs
analysis at the IL level. In contrast, P OLLUX leverages dynamic
binary analysis to develop an execution call graph, and examines it
to determine semantic dissimilarities.
DYNAMIC ANALYSIS .POLLUX is most closely related to
BLEX [ 34], which observes the side effects of function execution
under a controlled randomized environment. Two functions are
deemed similar, if their corresponding side effects are similar.
POLLUX also uses the notion of similarity in side-effects, but
unlike BLEX, does not require any controlled environment. Like
BLEX, P OLLUX is also robust to compiler optimizations, but
is signiﬁcantly more light-weight in its approach. Additionally,
POLLUX , like Zhang et al. [50] and Nagarajan et al. [42], augments
its call graph analysis with features, such as intermediate values, to
ﬁngerprint functions across binaries.
SYMBOLIC ANALYSIS . Several frameworks, such as
BinHunt [ 36], Bouncer [ 32], BitFuzz [ 31], FuzzBall [ 40],
and McVeto [ 48], operate solely on binaries. BinHunt is similar to
POLLUX in spirit, and determines semantic differences in binary
programs. However, unlike P OLLUX , BinHunt detects semantic
similarity using control ﬂow analysis using graph isomorphism
technique, symbolic execution, and theorem proving mechanisms.
Brumley et al. [28] use symbolic mechanisms to determine
whether different implementations of the same speciﬁcation are
semantically similar or not.
GRAPH ISOMORPHISM .Unlike P OLLUX , BinDiff [ 33,35] and
BinSlayer [ 27] use graph isomorphism techniques that performs
extremely well in both correctness and speed if the two binaries are
similar. However, graph isomorphism, in general, does not perform
well when the change between two binaries is large.
9. CONCLUSION
We present the design and implementation of P OLLUX , a
framework that leverages relevant application test cases to drive
execution through two versions of the concerned library binary,
records all concrete effects on the environment, and compares them
to determine semantic similarity for the same API invocation across
the two library versions. Our evaluation of P OLLUX with16open-
source libraries conﬁrms its utility, and also indicates both high
accuracy and precision even in the face of compiler optimizations.
10. ACKNOWLEDGEMENTS
We thank the anonymous reviewers for their valuable feedback.
29911. REFERENCES
[1] Capstone. https://github.com/aquynh/capstone .
[2] Catch. https://github.com/philsquared/Catch .
[3] Curl.https://github.com/curl/curl .
[4] DevIL. https://github.com/DentonW/DevIL .
[5] Docker. https://www.docker.com/ .
[6] dropbox/json11. https://github.com/dropbox/json11 .
[7] Eigen. https://bitbucket.org/eigen/eigen/ .
[8] Fit.https://github.com/pfultz2/Fit .
[9] GSL - GNU Scientiﬁc Library.
https://github.com/ampl/gsl .
[10] HTTP parser.
https://github.com/nodejs/http −parser .
[11] libarchive.
https://github.com/libarchive/libarchive .
[12] libtorrent. https://github.com/arvidn/libtorrent .
[13] libxml violates the zlib interface and crashes.
https://mail.gnome.org/archives/xml/2010 −
January/msg00035.html .
[14] Onion. https://github.com/davidmoreno/onion .
[15] OpenSSH Patches Critical Flaw That Could Leak Private Crypto
Keys.http://www.openssh.com/txt/release −7.1p2 .
[16] PEGTL. https://github.com/ColinH/PEGTL .
[17] Phoenix Compiler and Shared Source Common Language
Infrastructure. http://research.microsoft.com/en −us/
collaboration/focus/cs/phoenix.aspx .
[18] Pin - A Dynamic Binary Instrumentation Tool.
https://software.intel.com/en −us/articles/
pin−a−dynamic −binary−instrumentation −tool .
[19] Pin 2.11 User Guide.
https://software.intel.com/sites/landingpage/
pintool/docs/49306/Pin/html/ .
[20] Sørensen-Dice coefﬁcient. https://en.wikipedia.org/
wiki/S%C3%B8rensen%E2%80%93Dice_coefficient .
[21] spdlog. https://github.com/gabime/spdlog .
[22] The Heartbleed Bug. http://heartbleed.com/ .
[23] Valijson.
https://github.com/tristanpenman/valijson .
[24] Winamp crashes at launch. http://forums.winamp.com/
showthread.php?t=374649 .
[25] zlib.https://github.com/madler/zlib .
[26] B ALAKRISHNAN , G., G RUIAN , R., R EPS, T., AND TEITELBAUM ,
T. CodeSurfer/x86—A Platform for Analyzing x86 Executables. In
CC’05 .
[27] B OURQUIN , M., K ING, A., AND ROBBINS , E. BinSlayer: Accurate
Comparison of Binary Executables. In PPREW ’13 .
[28] B RUMLEY , D., C ABALLERO , J., L IANG , Z., N EWSOME , J., AND
SONG , D. Towards Automatic Discovery of Deviations in Binary
Implementations with Applications to Error Detection and
Fingerprint Generation. In USENIX Security’07 .
[29] B RUMLEY , D., J AGER , I., A VGERINOS , T., AND SCHWARTZ , E. J.
BAP: A Binary Analysis Platform. In CAV’11 .
[30] B USINGE , J., S EREBRENIK , A., AND VAN DEN BRAND , M. An
Empirical Study of the Evolution of Eclipse Third-party Plug-ins. InIWPSE-EVOL ’10 .
[31] C ABALLERO , J., P OOSANKAM , P., M CCAMANT , S., B ABI ´C, D.,
AND SONG , D. Input Generation via Decomposition and
Re-stitching: Finding Bugs in Malware. In CCS ’10 .
[32] C OSTA , M., C ASTRO , M., Z HOU , L., Z HANG , L., AND PEINADO ,
M. Bouncer: Securing Software by Blocking Bad Input. In SOSP
’07.
[33] D ULLIEN , T., AND ROLLES , R. Graph-based comparison of
Executable Objects. In SSTIC ’05 .
[34] E GELE , M., W OO, M., C HAPMAN , P., AND BRUMLEY , D. Blanket
Execution: Dynamic Similarity Testing for Program Binaries and
Components. In Security ’14 .
[35] F LAKE , H. Structural Comparison of Executable Objects. In DIMVA
’04.
[36] G AO, D., R EITER , M., AND SONG , D. BinHunt: Automatically
Finding Semantic Differences in Binary Programs. In ICICS ’08 .
[37] K INDER , J.Static Analysis of x86 Executables . PhD thesis, TU
Darmstadt, 2010.
[38] K INDER , J., AND VEITH , H. Jakstab: A Static Analysis Platform for
Binaries. In CAV ’08 .
[39] L UK, C.-K., C OHN , R., M UTH, R., P ATIL , H., K LAUSER , A.,
LOWNEY , G., W ALLACE , S., R EDDI , V. J., AND HAZELWOOD , K.
Pin: Building Customized Program Analysis Tools with Dynamic
Instrumentation. In PLDI ’05 .
[40] M ARTIGNONI , L., M CCAMANT , S., P OOSANKAM , P., S ONG , D.,
AND MANIATIS , P. Path-exploration Lifting: Hi-ﬁ Tests for Lo-ﬁ
Emulators. In ASPLOS ’12 .
[41] M ILEVA , Y. M., D ALLMEIER , V., B URGER , M., AND ZELLER , A.
Mining Trends of Library Usage. In IWPSE-Evol ’09 .
[42] N AGARAJANA , V., G UPTA , R., Z HANG , X., M ADOU , M., AND DE
SUTTER , B. Matching Control Flow of Program Versions. In ICSM
’07.
[43] R AEMAEKERS , S., VAN DEURSEN , A., AND VISSER , J. Measuring
software library stability through historical version analysis. In ICSM
2012 .
[44] R AEMAEKERS , S., VAN DEURSEN , A., AND VISSER , J. Semantic
Versioning Versus Breaking Changes: A Study of the Maven
Repository. In SCAM ’14 .
[45] S EO, H., S ADOWSKI , C., E LBAUM , S., A FTANDILIAN , E., AND
BOWDIDGE , R. Programmers’ Build Errors: A Case Study (at
Google). In ICSE 2014 .
[46] S ONG , D., B RUMLEY , D., Y IN, H., C ABALLERO , J., J AGER , I.,
KANG , M. G., L IANG , Z., N EWSOME , J., P OOSANKAM , P., AND
SAXENA , P. BitBlaze: A New Approach to Computer Security via
Binary Analysis. In ICISS ’08 .
[47] T EYTON , C., F ALLERI , J., P ALYART , M., AND BLANC , X. A Study
of Library Migrations in Java. Journal of Software: Evolution and
Process, 2014 .
[48] T HAKUR , A., L IM, J., L AL, A., B URTON , A., D RISCOLL , E.,
ELDER , M., A NDERSEN , T., AND REPS, T. Directed Proof
Generation for Machine Code. In CAV’10 .
[49] T IFFANY BAO, J. B., AND WOO, M. ByteWeight: Learning to
Recognize Functions in Binary Code. In Security ’14 .
[50] Z HANG , F., J HI, Y.-C., W U, D., L IU, P., AND ZHU, S. A First Step
Towards Algorithm Plagiarism Detection. In ISSTA ’12 .
300