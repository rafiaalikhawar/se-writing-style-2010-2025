Towards the Automatic ClassiÔ¨Åcation of
Traceability Links
Chris Mills
Department of Computer Science
Florida State University
Tallahassee, USA
chris.mills0905@gmail.com
Abstract ‚ÄîA wide range of text-based artifacts contribute to
software projects (e.g., source code, test cases, use cases, project
requirements, interaction diagrams, etc.). Traceability Link Re-
covery (TLR) is the software task in which relevant documents
in these various sets are linked to one another, uncovering infor-
mation about the project that is not available when considering
only the documents themselves. This information is helpful for
enabling other tasks such as improving test coverage, impact
analysis, and ensuring that system or regulatory requirements
are met. However, while traceability links are useful, performing
TLR manually is time consuming and fraught with error.
Previous work has applied Information Retrieval (IR) and other
techniques to reduce the human effort involved; however, that
effort remains signiÔ¨Åcant. In this research we seek to take the next
step in reducing it by using machine learning (ML) classiÔ¨Åcation
models to predict whether a candidate link is valid or invalid
without human oversight. Preliminary results show that this
approach has promise for accurately recommending valid links;
however, there are several challenges that still must be addressed
in order to achieve a technique with high enough performance
to consider it a viable, completely automated solution.
Index Terms‚Äîsoftware traceability, traceability link recovery,
machine learning
I. I NTRODUCTION
Software projects are made up of different types of artifacts,
such as source code Ô¨Åles, requirements speciÔ¨Åcations, legal
regulations, design documents, etc. which contain important
knowledge about the different facets of a system. Information
about the relationship between related artifacts of different
types can be leveraged in various software tasks such as
program comprehension and impact analysis. It can also serve
as a means of ensuring that regulations and requirements
are met as the project evolves. Traceability Link Recovery
(TLR) is the task associated with deriving links between
relevant documents in different artifact sets for the purpose
of improving the software development process.
Unfortunately, performing TLR manually is arduous and
error prone. In fact, despite extensive research efforts, adoption
of traceability in industry is still limited [1] due to the percep-
tion that its beneÔ¨Åts are outweighed by the associated costs
[2]. This is especially true for small teams or projects without
rules imposed by a regulatory body. From the perspective of a
software company, performing traceability recovery represents
an immense investment of human capital. However, software
traceability is widely accepted as an indicator of a well-constructed system in the research community [3], [4], has
been shown to improve software maintenance activities [5],
and beneÔ¨Åt even small project teams [6]. Therefore, lowering
the barrier to adoption by further reducing the required human
effort is an important research task.
A large body of existing work has focused on partially
automating TLR through applying Information Retrieval (IR)
[7] and machine learning (ML) [8], as well as model [9] and
rule-based approaches [10]. While these techniques reduce
the human effort involved in TLR, the process still relies on
signiÔ¨Åcant human intervention. For example, most state-of-
the-art IR approaches require a human operator to manually
inspect documents or links in a ranked list to establish trace-
ability. These approaches also frequently require a threshold
of similarity above which links are considered to be valid;
in practice, this is a difÔ¨Åcult parameter to tune. Existing
ML approaches determine similarity using the presence of
indicator words mined from requirements or another high-
level source (e.g., architectural tactics or legal requirements).
This process can be difÔ¨Åcult when documents in the training
set of artifacts have few words in common, such as code
classes, test cases, or use cases. Additionally, model-based
approaches are only applicable in speciÔ¨Åc scenarios where
artifacts can be represented as (even informal) models. Finally,
rule-based approaches require human operators to create and
maintain rules, which are often project-speciÔ¨Åc and apply only
to artifacts comprised of structured language. As an alterna-
tive, we propose a completely automated ML classiÔ¨Åcation
approach that does not require a predeÔ¨Åned similarity threshold
and is applicable to any text-based software artifact without
assumptions about internal structure.
The ideal version of such an approach would produce a
perfect prediction of the valid and invalid links, therefore com-
pletely automating TLR without human intervention. While
this perfect approach may be hard to reach, intermediary
versions that are able to accurately reveal at least a subset
of the valid links automatically would still be beneÔ¨Åcial. For
example, small teams working on unregulated projects that
currently have no established TLR process can beneÔ¨Åt from
even an imperfect starting point for adopting software trace-
ability. For large, safety-critical, stringently regulated projects,
on the other hand, complete automation might not be possible
or even advisable. Because the model would need to meet
978-1-5386-2684-9/17/$31.00 c2017 IEEEASE 2017, Urbana-Champaign, IL, USA
Doctoral Symposium1018
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:31:34 UTC from IEEE Xplore.  Restrictions apply. prohibitively stringent, provable accuracy criteria, some degree
of human oversight will always be required for veriÔ¨Åcation.
However, even in these cases, we seek an approach that further
minimizes effort through additional automation.
The next sections provide an overview of our proposed
approach, show the results of an initial evaluation, outline
existing challenges and ways to address them in future work,
and discuss related research.
II. A PPROACH AND PRELIMINARY EVALUATION
Our proposed approach considers all possible links between
two sets of artifacts simultaneously and based on a ML clas-
siÔ¨Åcation model and a set of descriptive features determines
which of those links are likely to be valid and which are
likely to be invalid. The main contribution of this research
compared to previous work is a completely automated, black-
box approach to TLR that accepts two sets of software artifacts
and then provides a set of recommended candidate links that
are expected to be valid. The next subsection describes the
feature set used to represent traceability links in the model.
A. Traceability Link Representation
A potential traceability link consists of two documents.
When the link is valid, these documents are related to one
another. For example, one document could be a use case and
the other a code class that implements it. When the link is
invalid, the documents are not related to one another. In the
proposed approach, we represent each potential link with an
extensive set of features designed to capture the similarity
between the two artifacts, as well as information on the internal
quality of either artifact. In our initial implementation we used
features that can be divided into three main categories:
IR Ranking Features: Building on the long-standing suc-
cess of IR-based techniques in capturing the semantics of
software artifacts, we make use of several such approaches
for generating features that reÔ¨Çect the semantic relationship
between each pair of artifacts. For each IR technique con-
sidered and each pair of documents d1andd2in artifact
setsD1andD2respectively, we compute two features. The
Ô¨Årst is the rank of d2in the list of results from D2when
usingd1as a query, and the other is the rank of d1in the list
of results from D1when using d2as a query. By including
both features, we exploit asymmetries in IR-based document
similarity that have been shown to impact IR results [11].
Document Statistics Features: The two artifacts in a po-
tential link also have basic statistical properties that reveal
information about both similarity and document terseness,
which has been shown to be a factor in hard-to-trace artifacts
[12]. There are Ô¨Åve features in this category representing a
pair of artifacts: the number of unique terms in each of the
two documents, the total number of terms in each of the
documents, and the Jaccard measure between the terms in
the two documents.
Query Quality (QQ) Features: We have previously used
QQ features in our work to capture the internal quality of
software engineering artifacts and to identify hard-to-traceartifacts [13] in TLR. Here we make use of the set of 28 pre-
and post-retrieval QQ metrics used in our previous work and
consider each of them as individual features for each artifact.
QQ features help the classiÔ¨Åcation model differentiate cases
when similarity is low between a pair of artifacts because
they are unlikely to be related from cases where similarity
is dampened because one or both of the documents is of
poor semantic quality (i.e., a hard-to-trace artifact).
B. Preliminary Implementation
Using the aforementioned features, we constructed a pre-
liminary implementation of our ML classiÔ¨Åcation approach as
a proof of concept, in order to gauge its potential and obtain a
baseline for performance. In this implementation, we consider
four TR approaches: traditional VSM with cosine similarity,
BM25, and two methods based on language model smoothing
techniques: Jelinek-Mercer and Dirichlet. Because we employ
four different TR approaches, our link representations are quite
large, including 111 different features. We also investigated
four classiÔ¨Åcation algorithms: Random Forest (RF), ClassiÔ¨Å-
cation Trees (J48), k-Nearest Neighbors (KNN) (k=5), and
N¬®aive Bayes to understand which of these is most suitable
for classifying potential traceability links. We use the default
Weka1implementations of all four algorithms to establish
baseline performance without any special algorithm tuning.
Furthermore, because the approach considers all possible
links between two sets of artifacts, we expect the data to be
highly imbalanced (i.e., there should be many more invalid
than valid links). Therefore, there is an intrinsic class imbal-
ance, which poses an obstacle for accurate link classiÔ¨Åcation
that must be addressed [14]. In this implementation we apply
Synthetic Minority Oversampling TEchnique (SMOTE) [15]
and random majority undersampling [16] to balance the data.
C. Evaluation
A preliminary evaluation was conducted on eleven datasets
from six software projects, involving six different types of
artifacts. Table I shows the details of each dataset. This data
was selected because it contains projects that are frequently
used to evaluate new techniques for TLR [17]‚Äì[19]. As antic-
ipated, we found a signiÔ¨Åcant class imbalance in this data, at
an average valid-to-invalid link ratio of 1 : 13 .
For the evaluation, we had access to an oracle for each
dataset that contains known links between documents estab-
lished by the original designers of the system. Using these
oracle Ô¨Åles as a ground truth, we employed ten-fold cross-
validation to compute performance metrics for models using
each classiÔ¨Åcation algorithm trained on data balanced with
either SMOTE or undersampling. Note that the testing set
was not rebalanced, as doing so would bias the experiment by
artiÔ¨Åcially boosting performance. Table II shows the results of
this evaluation expressed in true positive rate (TPR) and false
positive rate (FPR). TPR is equivalent to recall and shows the
percentage of valid links that were recovered by the classiÔ¨Åer.
1http://www.cs.waikato.ac.nz/ml/weka/
1019
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:31:34 UTC from IEEE Xplore.  Restrictions apply. TABLE I
SOFTWARE SYSTEMS USED IN EVALUATION
System Possible Links Valid Links Artifact Types
CM-1 1166 45 (3.86%) High-R, Low-R
eAnci 7645 554 (7.25%) UC, CC
eTour 6728 366 (5.44%) UC, CC
SMOS 6700 1044 (15.58%) UC, CC
iTrust 1551 58 (3.74%) UC, CC
EasyClinic 1410 93 (6.60%) UC, CC
EasyClinic 2961 204 (6.89%) TC, CC
EasyClinic 1260 83 (6.59%) ID, TC
EasyClinic 600 26 (4.33%) ID, UC
EasyClinic 1890 63 (3.33%) TC, UC
EasyClinic 940 69 (7.34%) ID, CC
Total 32821 2605 (7.94%)
High-R = High-level Requirements, Low-R = Low-level Requirements,
UC = Use Cases, CC = Code Classes, ID = Interaction Diagrams,
TC = Test Cases
TABLE II
AVERAGE TPR AND FPR FOR EACH CLASSIFICATION ALGORITHM USING
EITHER SMOTE OR MAJORITY UNDERSAMPLING
SMOTE Undersampling
Algorithm TPR FPR TPR FPR
J48 0.657 0.041 0.870 0.152
KNN 0.656 0.057 0.882 0.276
N¬®aive Bayes 0.741 0.194 0.836 0.234
Random Forest 0.695 0.017 0.927 0.122
FPR is the number of invalid links that were predicted to be
valid. A perfect model would get 1.00 TPR (i.e., the model
identiÔ¨Åes all of the valid links) with 0.00 FPR (i.e., there are
no invalid links predicted to be valid).
Typically, recall is the primary metric by which TLR
approaches are measured, as even with human intervention
an approach cannot fully support TLR unless it is capable
of identifying all (or a vast majority) of the traceability links
[12]. In our case, because we ultimately seek a fully automated
system, it is imperative that we also minimize FPR; otherwise,
the approach could recommend a set of links containing a
prohibitively large amount of noise that must be manually
validated. That is to say, a model with extremely high TPR
but moderate FPR is not useful in a practical sense.
Our data shows that a baseline model with no feature selec-
tion applied and no parameter tuning is capable of achieving an
average TPR of 0.695 (i.e., it recovers 69.5% of the valid links
on average across projects) with a fairly low FPR of 0.017
when using RF with only SMOTE balancing. Furthermore,
using only majority undersampling, the same model is capable
of identifying 92.7% of the valid links, but at the cost of a
much higher FPR (.122). This illustrates the importance of data
balancing when addressing TLR as a classiÔ¨Åcation problem.
Future work will investigate how parameter tuning and hybrid
balancing approaches can be used to further optimize a model
to maximize TPR for a minimal FPR.
III. C HALLENGES AND FUTURE WORK
While our proposed approach has several advantages com-
pared to existing techniques, it also has several unique chal-
lenges. First, because we use ML classiÔ¨Åcation, we rely onexisting historical data to train our models. Therefore, the
approach is not directly applicable to greenÔ¨Åeld projects for
which historical data is extremely limited or not available. To
address this shortcoming, future work will focus on applying
cross-project training in which similar projects are used to train
models. Because project-speciÔ¨Åc conÔ¨Ågurations are usually
able to outperform generic ones, we will also utilize techniques
from cold start software analytics [20] to achieve near-optimal
conÔ¨Ågurations without the need for project-speciÔ¨Åc data. Ad-
ditionally, we will investigate the use of genetic algorithms for
deriving optimized parameter conÔ¨Ågurations [21] for a set of
classiÔ¨Åcation algorithms, rather than relying on defaults.
Moreover, the feature set used in the initial implementation
is extremely large (111 features) and will only grow if addi-
tional IR techniques are included. Therefore, it is important to
incorporate an appropriate feature selection process capable of
extracting features that most efÔ¨Åciently differentiate valid and
invalid links while introducing minimal noise.
Finally, we compare this approach to other state-of-the-art
semi-automatic approaches. We will also perform a user study
to empirically establish if this approach is truly able to reduce
the human effort required for TLR and if so, to what degree.
IV. R ELATED WORK
The Ô¨Årst area of related work focuses on applying IR
approaches to automate TLR. These include probabilistic [22],
[23] and vector space models (VSM) [24], Latent Semantic
Indexing (LSI) [25], and Latent Dirichlet Allocation (LDA)
[26] among others. Due to space constraints, we direct the
interested reader to Borg et. al. [7], which provides a detailed
systematic mapping study of the area. Additionally, Binkley
and Lawrie [27] applied learning-to-rank to traceability, which
is similar to this approach in that they too combine multiple IR
approaches to better estimate document similarity. Gethers et.
al. [17] also combine IR approaches in their work leveraging
orthogonal information for TLR. Duan and Cleland-Huang
[28] combined clustering and IR to develop an approach that
recommends groupings of artifacts that are likely related to
a query document. These works differ from ours in that they
follow a standard process that requires a user to inspect a
list of results for each document in a query set, or specify a
similarity threshold for classiÔ¨Åcation, while ours does not.
In addition to IR-based approaches, there have also been
various automation efforts based on other techniques including
annotated dependency graphs [29], model-driven engineering
[9], and rules-based approaches using requirement-to-object
model rules [10], [30]. However, these approaches differ
signiÔ¨Åcantly from our work. Rules-based approaches require
signiÔ¨Åcant effort to establish rules and discipline to maintain
artifacts that adhere to those rules. Finally, the approach
proposed by Egyed and Grunbacher [29] relies on mapping
artifacts to shared ‚Äùcommon ground‚Äù, which may not be
applicable for all artifact types. Alternatively, our approach
is broadly applicable to any text-based software artifact.
Also related to this research are other classiÔ¨Åcation ap-
proaches to TLR. Cleland-Huang et. al. [31] Ô¨Årst proposed
1020
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:31:34 UTC from IEEE Xplore.  Restrictions apply. a probabilistic classiÔ¨Åer trained on a set of indicator words
for non-functional requirements, which was subsequently used
for linking regulatory codes with project requirements [8] and
architectural tactics with source code [32]. This work differs
from ours in that our approach does not rely on the extraction
of identiÔ¨Åer word sets as part of training, instead IR ranks
between two documents in a potential link are used to express
document similarity. Therefore, our approach is applicable
when tracing two artifact sets with many individual documents
such as test and use cases, without the need to map them
Ô¨Årst to indicator words. Finally, Falessi et al. [33] applied
classiÔ¨Åcation for predicting the number of true links left in
an IR result list. This work differs from ours in that we are
predicting the validity of individual links.
V. C ONCLUSION
Given that traceability has the capacity to improve the
development process, resulting in software that is higher
quality and projects that are more likely to be on time and
budget, reducing the required investment of human effort is an
important research task. Here we propose a machine learning
classiÔ¨Åcation approach based on an extensive feature set that
leverages document similarity via several IR engines in the
context of query quality to guage which artifacts are hard-to-
trace. As shown in our preliminary evaluation, the approach
has promise, but additional work is required to boost per-
formance, improve support for greenÔ¨Åeld projects, and build
sufÔ¨Åcient tooling for a full-scale, side-by-side comparison with
existing state-of-the-art approaches.
REFERENCES
[1] J. Cleland-Huang, O. C. Gotel, J. Huffman Hayes, P. M ¬®ader, and
A. Zisman, ‚ÄúSoftware traceability: Trends and future directions,‚Äù in
Conf. on Future of Software Engineering, 2014, pp. 55‚Äì69.
[2] P. Arkley and S. Riddle, ‚ÄúOvercoming the traceability beneÔ¨Åt problem,‚Äù
inInternational Conf. on Requirements Engineering, 2005, pp. 385‚Äì389.
[3] O. C. Gotel and C. Finkelstein, ‚ÄúAn analysis of the requirements trace-
ability problem,‚Äù in International Conf. on Requirements Engineering,
1994, pp. 94‚Äì101.
[4] B. Ramesh and M. Jarke, ‚ÄúToward reference models for requirements
traceability,‚Äù IEEE Transactions on Software Engineering, vol. 27, no. 1,
pp. 58‚Äì93, 2001.
[5] P. Rempel and P. Mader, ‚ÄúPreventing defects: The impact of require-
ments traceability completeness on software quality,‚Äù IEEE Transactions
on Software Engineering, 2016.
[6] C. Neumuller and P. Grunbacher, ‚ÄúAutomated software traceability in
very small companies: A case study and lessons learned,‚Äù in Interna-
tional Conf. on Automated Software Engineering, 2006, pp. 145‚Äì156.
[7] M. Borg, P. Runeson, and A. Ard ¬®o, ‚ÄúRecovering from a Decade : a
Systematic Mapping of Information Retrieval Approaches to Software
Traceability,‚Äù Empirical Software Engineering, vol. 19, no. 6, pp. 1565‚Äì
1616, 2014.
[8] J. Cleland-Huang, A. Czauderna, M. Gibiec, and J. Emenecker, ‚ÄúA
machine learning approach for tracing regulatory codes to product
speciÔ¨Åc requirements,‚Äù in International Conf. on Software Engineering,
2010, pp. 155‚Äì164.
[9] I. Galvao and A. Goknil, ‚ÄúSurvey of traceability approaches in model-
driven engineering,‚Äù in IEEE International Enterprise Distributed Object
Computing Conf., 2007, pp. 313‚Äì313.
[10] G. Spanoudakis, A. Zisman, E. P ¬¥erez-Minana, and P. Krause, ‚ÄúRule-
based generation of requirements traceability relations,‚Äù Journal of
Systems and Software, vol. 72, no. 2, pp. 105‚Äì127, 2004.
[11] C. Mills and S. Haiduc, ‚ÄúThe impact of retrieval direction on ir-
based traceability link recovery,‚Äù in International Conf. on Software
Engineering: New Ideas and Emerging Results Track, 2017, pp. 51‚Äì54.[12] J. Cleland-Huang, B. Berenbach, and S. Clark, ‚ÄúBest Practices for
Automated Traceability,‚Äù Computer, no. 40(6), pp. 27‚Äì35, jun 2007.
[13] C. Mills, G. Bavota, S. Haiduc, R. Oliveto, A. Marcus, and A. D. Lucia,
‚ÄúPredicting query quality for applications of text retrieval to software
engineering tasks,‚Äù ACM Transactions on Software Engineering and
Methodology, vol. 26, no. 1, p. 3, 2017.
[14] H. He and Y . Ma, Imbalanced Learning: Foundations, Algorithms, and
Applications. John Wiley & Sons, 2013.
[15] N. V . Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, ‚ÄúSmote:
Synthetic minority over-sampling technique,‚Äù Journal of ArtiÔ¨Åcial Intel-
ligence Research, vol. 16, pp. 321‚Äì357, 2002.
[16] H. He and E. A. Garcia, ‚ÄúLearning from imbalanced data,‚Äù IEEE
Transactions on Knowledge and Data Engineering, vol. 21, no. 9, pp.
1263‚Äì1284, Sep 2009. [Online]. Available: http://dx.doi.org/10.1109/
TKDE.2008.239
[17] M. Gethers, R. Oliveto, D. Poshyvanyk, and A. De Lucia, ‚ÄúOn inte-
grating orthogonal information retrieval methods to improve traceability
recovery,‚Äù in International Conf. on Software Maintenance, 2011, pp.
133‚Äì142.
[18] N. Ali, Y .-G. Gueheneuc, and G. Antoniol, ‚ÄúRequirements traceability
for object oriented systems by partitioning source code,‚Äù in Working
Conf. on Reverse Engineering, 2011, pp. 45‚Äì54.
[19] J. H. Hayes, A. Dekhtyar, S. K. Sundaram, E. A. Holbrook, S. Vadla-
mudi, and A. April, ‚ÄúRequirements tracing on target (retro): Improving
software maintenance through traceability recovery,‚Äù Innovations in
Systems and Software Engineering, vol. 3, no. 3, pp. 193‚Äì202, 2007.
[20] J. Guo, M. Rahimi, J. Cleland-Huang, A. Rasin, J. H. Hayes, and
M. Vierhauser, ‚ÄúCold-start software analytics,‚Äù in International Conf.
on Mining Software Repositories, 2016, pp. 142‚Äì153.
[21] A. Panichella, B. Dit, R. Oliveto, M. Di Penta, D. Poshyvanyk, and
A. De Lucia, ‚ÄúHow to effectively use topic models for software engineer-
ing tasks? an approach based on genetic algorithms,‚Äù in International
Conf. on Software Engineering, 2013, pp. 522‚Äì531.
[22] G. Antoniol, G. Canfora, G. Casazza, A. De Lucia, and E. Merlo, ‚ÄúTrac-
ing object-oriented code into functional requirements,‚Äù in International
Workshop on Program Comprehension, 2000, pp. 79‚Äì86.
[23] G. Antoniol, G. Canfora, A. De Lucia, and E. Merlo, ‚ÄúRecovering code
to documentation links in oo systems,‚Äù in Working Conf. on Reverse
Engineering, 1999, pp. 136‚Äì144.
[24] G. Antoniol, G. Canfora, G. Casazza, and A. De Lucia, ‚ÄúInformation
retrieval models for recovering traceability links between code and
documentation,‚Äù in International Conf. on Software Maintenance, 2000,
pp. 40‚Äì49.
[25] A. Marcus, J. Maletic, and A. Sergeyev, ‚ÄúRecovery of traceability
links between software documentation and source code,‚Äù International
Journal of Software Engineering and Knowledge Engineering, vol. 15,
no. 05, pp. 811‚Äì836, 2005.
[26] H. U. Asuncion, A. U. Asuncion, and R. N. Taylor, ‚ÄúSoftware traceabil-
ity with topic modeling,‚Äù in International Conf. on Software Engineer-
ing, 2010, pp. 95‚Äì104.
[27] D. Binkley and D. Lawrie, ‚ÄúLearning to rank improves ir in se,‚Äù in
International Conf. on Software Maintenance and Evolution, 2014, pp.
441‚Äì445.
[28] C. Duan and J. Cleland-Huang, ‚ÄúClustering support for automated
tracing,‚Äù in International Conf. on Automatic Software Engineering,
2007, pp. 244‚Äì253.
[29] A. Egyed and P. Grunbacher, ‚ÄúAutomating requirements traceability:
Beyond the record & replay paradigm,‚Äù in International Conf. on
Automatic Software Engineering, 2002, pp. 163‚Äì171.
[30] G. Spanoudakis, A. S. d. Garcez, and A. Zisman, ‚ÄúRevising rules to cap-
ture requirements traceability relations: A machine learning approach.‚Äù
inSEKE, 2003, pp. 570‚Äì577.
[31] J. Cleland-Huang, R. Settimi, X. Zou, and P. Solc, ‚ÄúAutomated clas-
siÔ¨Åcation of non-functional requirements,‚Äù Requirements Engineering,
vol. 12, no. 2, pp. 103‚Äì120, 2007.
[32] M. Mirakhorli, Y . Shin, J. Cleland-Huang, and M. Cinar, ‚ÄúA tactic-
centric approach for automating traceability of quality concerns,‚Äù in
International Conf. on Software Engineering, 2012, pp. 639‚Äì649.
[33] D. Falessi, M. Di Penta, G. Canfora, and G. Cantone, ‚ÄúEstimating the
number of remaining links in traceability recovery,‚Äù Empirical Software
Engineering, pp. 1‚Äì32, 2016.
1021
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 15:31:34 UTC from IEEE Xplore.  Restrictions apply. 