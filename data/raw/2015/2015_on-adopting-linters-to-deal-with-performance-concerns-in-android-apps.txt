On Adopting Linters to Deal with Performance Concerns in
Android Apps
Sarra Habchi
Inria
University of Lille
Lille, France
sarra.habchi@inria.frXavier Blanc
University of Bordeaux
Bordeaux, France
xavier.blanc@u-bordeaux.frRomain Rouvoy
University of Lille
Inria
Lille, France
romain.rouvoy@inria.fr
ABSTRACT
With millions of applications (apps) distributed through mobile
markets,engagingandretainingend-userschallengeAndroidde-
veloperstodeliveranearlyperfectuserexperience.Asmobileapps
run in resource-limited devices, performance is a critical criterion
for the quality ofexperience. Therefore,developers are expected to
pay much attention to limit performance bad practices. On the one
hand, many studies already identified such performance bad prac-
tices and showed that they can heavily impact app performance.
Hence, many static analysers, a.k.a.linters, have been proposed
to detect and fix these bad practices. On the other hand, other
studies have shown that Android developers tend to deal with per-
formancereactivelyandtheyrarelybuildonlinterstodetectand
fix performancebad practices.In thispaper, wetherefore perform
a qualitative study to investigate this gap between research and
developmentcommunity.Inparticular,weperformedinterviews
with14 experiencedAndroid developerstoidentify theperceived
benefits and constraints of using linters to identify performance
bad practices in Android apps. Our observations can have a direct
impact on developers and the research community. Specifically,
wedescribewhyandhowdevelopersleveragestaticsourcecode
analysers to improve the performance of their apps. On top of that,
we bring to light important challenges faced by developers when it
comes to adopting static analysis for performance purposes.
CCS CONCEPTS
•Software and its engineering →Software performance ;Soft-
ware maintenance tools ;
KEYWORDS
Android, performance, linters, static analysis.
ACM Reference Format:
SarraHabchi,XavierBlanc,andRomainRouvoy.2018.OnAdoptingLinters
to Deal with Performance Concerns in Android Apps. In Proceedings of
the 2018 33rd ACM/IEEE International Conference on Automated Software
Engineering(ASE’18),September3–7,2018,Montpellier,France. ACM,New
York, NY, USA, 11pages.https://doi.org/10.1145/3238147.3238197
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.32381971 INTRODUCTION
Mobileapplications(apps)arenowadayscomplexsoftwaresystems
that must be designed carefully to meet the user expectations and
stayaheadoftheappstorescompetition.Reportsshowthat75%of
appsareuninstalledwithin3months[ 2]andthatthesecondtop
reason of these uninstalls is poor performance [ 1]. Therefore, app
developers are expected to pay a careful attention to performance
in their development, and particularly avoid performance bad prac-
tices. Previous studies have already identified and characterised
different development practices that hinder the performance of
mobileapps[ 20,21,29,35].Researchersalsoassessedtheimpact
of such bad practices on different performance aspects [ 16,22]. To
detectthesebadpractices,differentstaticanalysers, a.k.a.linters,
like Paprika [ 23], PerfChecker [ 29], and aDoctor [ 32]w e r ep r o -
posed. The development community also proposed tools to detect
andfixthesebadpractices.Forinstance,AndroidStudio,theofficial
IDEforAndroiddevelopment,integratesalinter—calledAndroid
Lint—that detects performance bad practices.
However, despite the availability of these linters and evidences
of performance penalties due to bad practices, Android developers
donotrelyheavilyonlinterstodealwithperformanceconcerns.AsurveyconductedbyLinarez etal.[
27]with485Androiddevelopers
showedthat,whenusingtoolstomanageperformance,mostdevel-
opersrelyonprofilersandframeworktools,andonly5participants
reportedusingalinter.Inordertoconfirmthisphenomenonand
laythefoundationforourstudy,wepublishedanonlinesurveyand
askedAndroiddevelopersabouttheirlinterusage.Allthedetailsof
the survey and its results are available in our technical report [ 12].
Theresultsofthissurvey reportedthatonly51% ofAndroidLint
users rely on it for performance purposes. Given that performance
checksareenabledbydefaultinAndroidLint,suchobservations
raisemanyquestionsabouthow Androiddevelopersperceivethe
usefulnessoflinterswhenitcomestoperformance.Inparticular,it
isimportanttohighlightthebenefitsandchallengesofadopting
linters for performance issues.
Inthispaper,wethereforeconductaqualitativestudytoinvesti-
gatethebenefitsandconstraintsofusinglintersforperformance
purposes.Weinterview14experiencedAndroiddeveloperswho
use Android Lint for performance purposes in order to understand:
(1)Why do Android developers use linters for performance
purposes?
(2)What are the constraints of using linters for performance
purposes?
It is also important to understand what fashions would allow
Androiddeveloperstoachievetheeventualbenefitsoflintersfor
performance, thus we also investigate the question:
6
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sarra Habchi, Xavier Blanc, and Romain Rouvoy
(3)HowdoAndroiddevelopersuselintersforperformancepur-
poses?
Ourfindingsfromthisstudyhavedirectimplicationstodevelopers,
researchers and tool makers.
The remainder of this paper is organised as follows. We start
with a brief background about linters for Android in Section 2,
thenwedescribeourmethodologyinSection 3.Section 4reports
and discusses the results of our qualitative study. We identify in
Section5theimplicationsofourresults,andinSection 6thelimita-
tionsofourstudy.WepresentinSection 7therelatedworksbefore
concluding in Section 8.
2 BACKGROUND
Thissectionprovidesgeneralbackgroundinformationonlinters
for Android. Many linters, like PMD [ 11], CheckStyle [ 5], In-
fer [9], and FindBugs [ 8] can be used to analyse Android projects
written in Java. PMD detects programming flaws like dead code.
CheckStyle checks that codingconventions are respected. Infer
identifiesissuesaspotentialbugs,nullpointerexceptions,resource
leaks, etc. FindBugs analyses the Java bytecode to detect potential
bugs.Detekt[ 6]isalinterthatcanbeusedonAndroidprojects
written in Kotlin. It computes source code complexity and iden-
tifies some code smells. Ktlint [ 10] is another linter for Kotlin,
but that focuses on checking code conventions. All the mentioned
lintersdetectissuesrelatedtoeitherJavaorKotlin,buttheydonot
consider issues or practices specific to the Android framework.
Android Lint [ 4] is the mainstream linter for Android. It is
integrated in Android Studio, the official IDE for Android. It can
be run on Android projects from the command line or in Android
Studio interactively. It scans the code to identify structural code
problemsthatcanaffectthequalityandperformanceofAndroid
apps.
Lint targets 339 issues related to correctness, security, perfor-
mance,usability,accessibility,andinternationalization.Thecate-
gory performance includes 34 checks,1a.k.a.rules. As an example,
weexplaintherule HandlerLeak thatchecksifa Handler isusedas
non-static inner class or not. This situation is problematic because
theHandlerholdsareferencetotheouterclass.Thus,aslongas
theHandler is alive the outer class cannot be garbage collected,
thus causing memory leaks. This issue has been addressed in some
research studies as a code smell, named Leaking Inner Class [23].
AndroidLintreportseachproblemwithabriefdescriptionmessage,
apriority,andaseveritylevel.Thepriorityisanumberfrom1to
10,andtheseverityhasthreelevels, ignore,warning,and error.
All the Android Lint checks have a default priority and severity.
The severity is a factor that can be configured by developers to
classify the problems on which they want to focus.
Wechose touseAndroidLintinthisstudyasit isthemostused
linter for Android, and it detects a large set of performance bad
practices.
3 METHODOLOGY
Our objective is to investigate with an open mind the benefits and
limitations of using a linter for performance purposes in Android.
Therefore, we follow a qualitative research approach [ 18] based on
1As for March 2018.classic Grounded Theory concepts [ 13]. With this approach, we
aimtodiscovernewideasfromdatainsteadoftestingpre-designed
researchquestions.Specifically,weconductedinterviewswith14
experienced Android developers. The interview design and the
selectedparticipantsarepresentedinSections 3.1and3.2,respec-
tively. Afterwards, we transcribed and analysed the interviews, as
explained in Section 3.3.
3.1 Interviews
As commonly done in empirical software engineering research, we
designedsemi-structuredinterviews.Thiskindofinterviewscon-
sistsofalistofstarterquestionswithpotentialfollowupquestions
thatareaskedthroughout.WefollowedtheadvicesgivenbyHoveandAnda etal.[
24]todesignandconducttheinterviews.Inpartic-
ular, we paid a careful attention to explaining the objectives of the
interviews. We explained that interviews are not judgemental and
we incited the participants to talk freely. We asked open-questions,
such as:“Why doyou thinkthat Lint isuseful forperformance pur-
poses?”,andweaskedfordetailswheneverpossible.Wedesigned
the interview with basically 12 questions, and depending on partic-ipants’replies,weaskedadditionalquestionstoexploreinteresting
topics. The main questions are the following:
(1)Why do you use the linter to deal with performance con-
cerns?
(2) What are the benefits that you perceived with this usage?(3)
How do you use the linter to deal with performance con-
cerns?
(4) How do you configure the linter?(5) Do you use it individually or in a team?(6)
In acollaborative project,how doyouconfigure the linter
to optimize the app performance?
(7) Do you integrate the linter in the build or CI? Why?(8)
Doyouchangethepriorityorseverityofperformancechecks?
Why?
(9)Do you ignore or suppress performance checks sometimes?
Why?
(10)Are there any performance checks that you consider irrele-
vant? Why?
(11) Do you write your own performance checks? Why?(12)
In your opinion, what are the constraints of using the linter
for performance purposes?
Withthepermissionofinterviewees,theinterviewswererecorded
and they lasted from 18 to 47 minutes, with an average duration of
30 minutes.Weperformed two interviewsface-to-face, andthe 12
otherswereperformedwithanonlinecall.Oneparticipantwasnotabletoparticipateinthecallandinsteadreceivedalistofquestions
via email and gave written answers.
3.2 Participants
OurobjectivewastoselectexperiencedAndroiddeveloperswho
use the linter for performance. We did not wantto exclude open-
source software (OSS) developers, nor developers working on com-
mercial projects.For thatpurpose, werelied on manychannels to
contact potential participants.
7
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. On Adopting Linters to Deal with Performance Concerns in Android Apps ASE ’18, September 3–7, 2018, Montpellier, France
GitHub.First, we selected the most popular Android apps on
GitHub relying on the number of stars. Afterwards, we selected
the projects that use the linter by looking for configuration files
—e.g., lint.xml , or configurations in Gradle files. Then, we manu-
allyanalysedthetop-100projectstoidentifythedeveloperswho
actively work with the linter. We only found 41 developers thatcontributed to the linter configuration. As it is complex to guess
ifdevelopersaremotivatedtousethelinterforperformanceonly
from the configuration files, we contacted these developers to ask
them if they use the linter for performance or not. Out of 41 mails
sent,wereceived18answers, i.e.,aresponserateof43%.13develop-
ers answered that they use the linter for performance, and 5 others
answered negatively. We replied to the 13 developers to explain
the objectivesof ourinterview andinvite themto participate.We
received6acceptancesand2rejects,theotherdevelopersdidnot
answer.
Forums and meetups. To select commercial Android developers,
wesentformsindeveloperforums[ 3].Intheforms,weexplicitly
explained that we are looking for Android developers who usethe linter for performance. We received 6 answers from forums,
1 of them was irrelevant because the developer did not have a
real experience with Android Lint. We also communicated thesame message in Android development meetups. From meetups,
we selected 3 persons who satisfied our criteria.
Overall,thisselectionprocessresultedin14participants.After
conducting14interviews,weconsideredthatthecollectedinfor-
mation is enough to provide us with theoretical saturation [ 19],
i.e., all concepts in the theory are well-developed. Thus we did not
perform a second batch of selection and interviews.
To keepthe anonymity ofthe 14 selected participants,we refer
tothemwithcodenames.Wealsoomitallthepersonalinformation
like company or project names. Table 1shows the participants
codes, their experience in Android and Android Lint in terms of
yearsofprofessionaldevelopment,andthetypesofprojectsthey
workon.Itisworthmentioningthatwiththeterm“commercial”
werefertoprojectsthataredevelopedinanindustrialenvironment
andarenotbasedonanopen-sourcecommunity.Alsowefound
that all the developers selected from GitHub were also involved in
commercialprojects,andtwodevelopersspottedfromforumswere
involved in both commercial and OSS projects
Table1showsthat,outof14participants,11have morethan5
yearsofexperienceinAndroid.ComparedtotheageoftheAndroidframework(8years),thisexperienceisquitestrong.AsforAndroidLint,whichhasbeenintroducedwithAndroidStudioin2013,10of
our participants have more than four years of experience in using
it.
3.3 Analysis
We carefully followed the analytical strategy presented by Schmidt
et al.[37], which is well adapted for semi-structured interviews.
Thisstrategyhasproveditselfinthecontextofresearchapproaches
that postulate an open kind of theoretical prior understanding, but
donotrejectexplicitpre-assumptions[ 37].Beforeproceedingto
the analysis, we transcribed the interviews recordings into texts
using a denaturalism approach. The denaturalism approach allows
us to focus on informational content, while still working for a “fullTable 1: Partcipants’ code name, years of experience in An-
droid and Android Lint, and the type of projects they workon
Participant Android Lint Project
code experience experience type
P1 8 5 OSS & Commercial
P2 8 4 OSS & Commercial
P3 8 4 Commercial
P4 8 5 OSS & Commercial
P5 8 4 OSS & Commercial
P6 8 5 OSS & Commercial
P7 6 4 OSS & Commercial
P8 5 3 OSS & Commercial
P9 4 4 Commercial
P10 5 3 Commercial
P11 2 2 Commercial
P12 5 4 Commercial
P13 4 1 Commercial
P14 8 4 OSS & Commercial
and faithful transcription” [31]. In what follows, we show how we
adopted the analytical strategy steps.
3.3.1 FormMaterial-OrientedAnalyticalCategories. Inthisstep,
wedefinethesemanticcategoriesthatinterestus.Inourcase,we
investigatethemotivationandargumentsofAndroiddevelopers
thatusethelinterforperformancepurposes,andtheconstraints
of such usage. Therefore, our categories are initially the following
two topics:
•WhydoAndroiddevelopersuselintersforperformancepur-
poses?
•What are the constraints of using linters for performance
purposes?
After our discussions with participants, we noticed that an addi-
tional category is highlighted by developers, namely:
•HowdoAndroiddevelopersuse lintersforperformancepur-
poses?
We found this additional topic enlightening, thus we included it
inouranalyticalcategories.Oncethecategoriesareset,wereadand
analysed each interview to determine which categories it includes.
In the analysis, we do not only consider answers to our questions,
but also how developers use the terms and which aspects theysupplement or omit. After this analysis, we can supplement or
correct our analytical categories again.
3.3.2 AssembletheAnalyticalCategoriesintoaGuideforCoding.
Weassemblethecategoriesintoananalyticalguide,andforeach
category different versions are formulated. The versions stand for
differentsubcategoriesidentifiedintheinterviewsinreferenceto
oneofthecategories.Inthefollowingsteps,theanalyticalguidecan
betestedandvalidated.Indeed,thecategoriesandtheirversions
may be refined, made more distinctive or completely omitted from
the coding guide.
8
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sarra Habchi, Xavier Blanc, and Romain Rouvoy
3.3.3 CodetheMaterial. Atthisstage,wereadtheinterviews
and try to relate each passage to a category and a variant formula-
tion.Aswefocusonlabellingthepassages,wemayomitspecial
features and individualdetails of each interview.Yet, these details
will be analysed and highlighted in the last step. To strengthen the
reliabilityofourconclusions,weuseaconsensualcoding.There-
fore,eachinterviewiscodedbyatleasttwoauthors.Initiallyevery
author codes the interview independently, afterwards, the authors
discussandcomparetheirclassification.Incaseofdiscrepancies,
the authors attempt to negotiate a consensual solution.
4 RESULTS
We present in Table 2the results of the coding process in order to
contribute to the transparency and verifiability of our study.
4.1 Why Do Android Developers Use Linters
for Performance Purposes?
4.1.1 Linter Helps Developers to Learn about the Framework. As
theAndroidframeworkisrelativelyyoung,developersareeager
tolearnaboutitsunderlyingperformanceconstraints. Half ofthe
participants stated that the linter is very instructive in that respect
(P1,P2,P3,P7,P9,P10,P11). “Lintwillactuallyhelpyoubecomea
betterAndroidprogrammer” (P3).Indeed,theperformancechecks
ofthelintermentorthedeveloperstousetheframeworkefficiently:
“IseetheperformancechecksasaguideoftheAndroidframework”
(P7). Other participants mentioned that their understanding of
performance, and their programming habits evolved thanks to the
linter:“Everytime I learn a new thing about performance, and then it
becomesahabit” (P11).Asanexampleofthesecases,participantP9
mentioned DrawAllocation,abadpracticethatconsistsofallocating
newmemoryspacesinthe onDraw() method:“Iwascreatinganew
object in a paint method so Lint gave me a warning. That was the
first and last time I see it, because now I pay attention to this” (P9).
SomeparticipantsemphasisedthatjuniorAndroiddevelopers
should particularly use the linter for performance: “Lint checks are
extremelyusefulforhelpingoutbeginnerAndroiddevelopersorjunior
membersofmyteamtoenforcebettercodeperformance” (P2).Indeed,
junior Android developers, even if they have a prior experience
in desktop development, may lack understanding of mobile frame-
workspecificities.Thus,theyarepronetoperformance-relatedbad
practicesandtheyneedthelintertolearnhowtokeeptheirmobile
apps efficient.
Discussion. Participantsfrompreviousstudieshavealsoreported
that learning is one of the main benefits of using linters [ 17,38].
Specifically,thedeveloperslearnedwiththelinteraboutthesyntax
of the programming language, idioms, or libraries. In the case of
ourstudy,theparticipantsshowedthatevenimportantconcepts
about the Android framework can be learned through the linter.
This can be a great incentive for Android developers to use linters
with performance and other framework related checks.
4.1.2 LinterCanAnticipateSomePerformanceBottlenecks. Many
participantsreportedthatthelintersupportsthemindetectingbad
practices that can cause performance bottlenecks (P1, P2, P5, P6,
P10, P13). “Lint is very useful to tell in advance what is going wrong”(P1).Theparticipantsstatedthatthelinter isveryefficientinde-
tecting code-level performance issues, “Lint is very good at finding
patterned issues that follow a specific rule and can be solved very
simply”(P2).Whenaskedwhytheywanttoanticipateperformance
problems,theparticipantsreportedthatperformanceissuesshould
notbenoticedbyusers,“itisalwaysbettertodetecteventualprob-
lems before they are reported by end-users or managers.”(P10). In
fact, when end-users notice performance issues, they can uninstall
theapporgivebadreviews,andfromthereitcanbehardtogain
back users confidence. Moreover, the participants explained that
oncebottlenecksarereported byusers,itcanbecomplex toiden-
tify their root cause and fix them. “by using Lint, we try to detect
performance issues as soon as possible, because when they occur, theyaregenerallyverydiffusesoitishardtodetectthemwiththeprofiler”
(P5).For instance, “whenI have8 threadswith cross-threadmethod
calls,locatingaproblemwiththeprofilerbecomesverydifficult” (P5).
In that respect, anticipating performance bottlenecks saves also
time for developers.
Discussion. Previous studies have shown that developers prefer
to manage performance reactively, and hence to wait the problems
to occur before handling them, with the objective to gain time [ 27].
Here, our participants express a different point of view. They ex-plain that when performance bottlenecks occur, they require somuch time and effort to be fixed. They are hence in favour of a
more pro-actively approach that aims to detect and fix bottlenecks
before they occur. It is important to transmit this information to
developerscommunity,andespeciallynoviceAndroiddevelopers.
The latter may be unaware of the complexity of locating and fix-
ingperformancebottlenecks,thustheycanmakewrongstrategic
choices. As there should be a trade-off between reactive and proac-
tiveapproaches,wealsoencouragefutureworkstomakerealworld
comparisons between them.
4.1.3 Linter Is Easy to Use. Many developers said that they use
the performance checks of Android Lint because they found it sim-
ple and easy to use (P2, P4, P6, P8, P11, P14). Indeed, Android Lint
isalreadyintegratedintheIDEandallthechecks,includingper-
formance, are enabled by default: “it is built into Android studio, the
checks performed by the linter appear automatically so we get those
benefits just kind of immediately through the syntax highlighting”
(P2). Hence,the usage ofthe linter isseamless and effortless: “it is
just built into the tooling so well and so seamlessly, and it does not
really cost me anything to have Lint running” (P2). The participants
also appreciated the integration in other development tools: “we
can easily integrate it in the Gradle and continuous integration, so
we can ensure that performance and other Lint checks are respected”
(P6).
Discussion. Thisbenefitismorerelatedtotheuseofthelinter
itselfratherthantoperformanceconcerns.Previousstudiesshowed
that static analysers should be easy and seamless to encourage de-
veloperstousethem[ 25].Ourfindingsconfirmthisasparticipants
stated clearly that they use the linter because it does not cost them
anything. Now, AndroidLint has anadditional privilege bybeing
integrated in the official IDE and activated by default and this mo-
tivated developers to adopt it. This fact aligns with previous works
[17,25,36] where researchers suggested that the integration in the
9
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. On Adopting Linters to Deal with Performance Concerns in Android Apps ASE ’18, September 3–7, 2018, Montpellier, France
Table 2: Results of the coding process.
Categories Versions (subcategories) P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14
BenefitsLearn about the framework xxx x xxx
Anticipate performance bottlenecks xx xx x x
The linter is easy to use x x x x x x
Develop the performance culture x x x x
Save time xx x
Contribute to credibility and reputation x xx
Raise awareness of app performance x x
Ease performance debugging x
FashionsIntegrate from scratch x x xxx x
Target milstones x xxx
Adopt performance sprints x x
Improve performance incrementally x
Support individual development x x x xx x x
Check performance rules in a team x x xx xx x x
Prioritise performance aspects xx xx x x
ConstraintsStatic analysis is not suitable for performance xxxx x x
Nobody complained x xxx xx x
We do not have time x xx
Performance is not important in our case x x x
Some rules are irrelevant x x
Results are not well presented x x x x xx
The linter lacks precision xx x
The linter does not have enough checks xx x
It is difficult to write new rules x
development workflow helps static analysers to accumulate the
trust of developers.
4.1.4 LinterCanDevelopthePerformanceCulture. Indevelop-
mentteams,developerscanhavedifferentperceptionsandlevelsof
interestregardingperformance.Thus,itsoundsimportanttorely
on a linter for enforcing performance good practices (P1, P5, P7,
P10).Theuseofalinterensuresthatallteammembersfollowthe
same performance guidelines: “we have to make sure that everyone
respects the rules and understands why we are using the performance
checksofLint” (P10).Ontopofthat,theperformancechecksthat
willoccurwillcertainlybethesourceofdiscussionsamongteam
members: “theobjectiveistoshareandlevelourknowledgeonper-
formance. When Lint reports performance problems, we can disagree
with each other on whether to consider it or not, so we will discuss
andunderstandtheproblem,thenmakeawisedecisionaboutit” (P5).
That being said, the usage of the linter at a team level is fruitful in
many ways. On one side, it allows to keep all the team members
at the same page about performancechoices. Besides, it arises dis-
cussions about performance, and thus enriches the performance
culture in the team.
Discussion. Previousstudyshowedthatdevelopersusethelinter
to have an objective tool that avoids endless discussions about
codestyle[ 38].Thestatementsofourparticipantsshowthatthe
linteritselfcantriggerdiscussionsinthecontextofperformance.
Unlikecodestyle,performanceisanimportantaspectthatrequires
adeepthinkingfromdevelopersespeciallyinthecontextofmobileapps. Hence it is normal that developers appreciate the discussions
triggered by the linter about it.4.1.5 LinterCan Savethe Developer’sTime. Someparticipants
explainedthattheyusethelintertoautomatetime-consumingtasks
(P2,P3,P9).Inparticular,thelintercanfillinforrepetitivetasks:
“Lint helps you to save time in a lot of ways. One that comes to my
mindistheidentificationofunusedresources.Thelintersavesmea
lot of time as I don’t have to cross-check all resources manually” (P2).
Indeed, keeping unused resources is a performance bad practice
thatincreasestheAPKsize.Developersusedtomanuallycheckand
remove all the useless resources, which can be tedious and error
pronewhentherearemanyresourcesandwhentheappisbuiltwith
many flavors. Using Android Lint to detect unused resources then
helpsdevelopersandsavestheirtime.Anothertaskwheretheusageoflinterhelpssavingtimeiscodereview: “itisaquickandeasycode
review.Itgivesyouabunchofinformation.Now,whetherIwantto
implementthosemessagesornot,thatisanotherdecision” (P3).Code
reviewisanimportantrepetitivetaskinsoftwaredevelopmentthat
can be very time consuming, especially at team level. As stated by
the participants, the linter can partially automate this task, so that
developers do not have to review trivial performance issues and
can therefore focus on important aspects.
Discussion. Lintersarealsoknownforsavingtimeincontexts
whereeventualissuescanbeautomaticallydetected, e.g.,bugdetec-
tion[25].Theparticularityofthecasesreportedbyourparticipants
isthatontopofsavingtimebydetectingeventualissues,thelinterautomatesrepetitivetasks.AsthesetasksareconcreteandconcernalltypesofAndroidapps,thiscanbemoreappealingfordevelopers
to adopt the linter.
10
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sarra Habchi, Xavier Blanc, and Romain Rouvoy
4.1.6 Linter Contributesto Increase Credibilityand Reputation.
Developersalwayswanttomaintainagoodreputationamongtheir
peers, and the linter can help them to do that (P1, P6, P7). First,
developers need to keep their credibility among colleagues: “I have
to use Lint before making a release to make sure that my work meets
the expectations of my superiors and colleagues” (P6). Another con-
text where reputation is important is open-source projects: “before
releasing code to open-source, I am required to check that all the
warningsareawayandthatmycodehasahighquality” (P1).(P7)
added:“I work on a company where we do some open-source projects.
BeforepublishingcodeIalwayshavetorunthelintertomakesureof
not making obvious performance mistakes. This impacts not only my
credibility but also the image of the company” (P7).
Discussion. SinceperformanceiscriticalinAndroidapps,making
obvious mistakes can seriously affect the credibility of the devel-
oper among her peers. Hence, the linter performance rules can
giveavaluablesupportforAndroiddeveloperstomaintaintheir
reputation.
4.1.7 LinterRaisestheAwarenessoftheAppPerformance. An-
droid apps are nowadays very complex software systems, and it is
hard fordevelopers tobeaware ofthe implicationsof theirdevel-
opment choices in terms of performance. Some participants stated
thattheyusethelintertoalwaysbeawareoftheirappperformance
(P3,P5).“SometimesLintwillcatchperformanceaspectsthatIdidnot
reallythinkabout,andsoitwillgivemeatimeoramomenttothinkaboutanddecide” (P3).Thisshowsthatthelintermessagesincitede-
velopers to carefully think and give more attention to performance.
This awareness can be particularly important when developers are
workingaloneontheproject, “ifIamtheonlydeveloper,formethis
thinkingiscriticalandLintisamust-have” (P3).Thisappliesalsoto
the cases where the issues reported by the linter are not applicable:
“insomecasesIamnotabletoapplythechangesrequestedbyLint.
ButstillIneedtohaveagoodandvalidreasonforthis.SoIamaware
of the trade-off I made ” (P5).
Discussion. Itisimportanttodistinguishtheawarenessofapp
performancewiththelearningofperformancegoodpractices.Here
the linter incites the developers to think and understand their app
performance. When developers understand their apps, they can
make decisions or solve eventual problems more easily.
4.1.8 LinterEasesPerformanceDebugging. Someparticipants
didnotonlyusethelintertodirectlyimproveperformance,buttheyalsoobeyednon-performancerulestoensurehighcodequality,and
consequentlyeaseeventualperformanceprofilinganddebugging
(P9).“By using Lint and other static analysers like Sonar, I ensure
thatmycodeiswelldesignedandreadable.Sowhenaperformance
issueisreported,debuggingandprofilingbecomeseasier.Ialsocan
easilyandquicklyapplythefixes” (P9).Thelinterthenalsohelps
tobuildcleanandmaintainableappsthatfurtherdeveloperscan
easily debug and refactor for improving performance.
Discussion. Thisrepresentsabenefitofusingthelinteringeneral,
and not related to the usage of performance checks. However, itis still interesting to observe that some developers have a deep
understandingofthesoftwaredevelopmentprocess.Linterscannotpreventallperformancebottlenecks,bugsorotherissues.Therefore,developers should always keep their code clean and maintainable,
because it makes further maintenance operations easier.
4.2 How Do Android Developers Use Linters
for Performance Purposes?
4.2.1 LinterIntegratesalongtheProjectLifeCycle. Thepartic-
ipants reported different strategies to use the linter through the
projectlifecycleinordertokeeptheirappseffective.Intheremain-
der, we report on the strategies they identified.
Integratingfromscratch. Manyparticipantsreportedthatthey
preferusingthelinterfromtheprojectstartup(P2,P5,P9,P10,P11,
P13). Participant P5 explained that, when starting a project fromscratch, she tries to keep the code very clean by considering allthe Lint checks. When asked about the configuration adopted inthis case, the participants said that they keep the default config-uration provided by the linter, “in this situation I do not need any
additional setting, Lint is configured by default” (P9). We also asked
these participants about the motivation behind this strategy. They
explainedthat,whentheprojectadvanceswithoutthelinter,itis
more difficult to control performance a posteriori. For instance: “we
hadacasewhereweretrievedaprojectthatwasbuiltbyanotherteam
withoutLint.Wehavegotthousandsoferrorsandwarnings.Sowe
werelessmotivatedtoputbackLintandrecovertheappperformance”
(P7). Indeed, it is easy with this strategy to motivate developers
torespectperformancechecksbecausethecodebaseiscleanand
there is no existing debt to tackle.
Targetingmilestones. Fiveparticipantsmentionedthattheyex-
tensively use the linter at the end of features or for releases (P1,
P5, P6,P7). “I neveruse Lintin thebeginning of theproject orwhile
prototyping. I use it for releases to make sure that my code meetsthe expectations” (P6). As for features: “towards the end of adding
a new feature, I will run through Lint then I will go through all of
themandIdeterminewhetherornotIwanttospendthetimetodo
it”(P3). When asked about the configuration used for this strategy,
participantP5stated: “wehavedifferentLintprofiles, intherelease
profileweactivateadditionalrules”.Thisstrategyallowsdevelopers
to go fast while producing new features or prototyping without
hindering the final app performance.
Adopting performance sprints. Two participants reported that
they dedicate sprints for improving the app performance (P5, P12).
Participant P12 stated: “while working, we do not have concrete per-
formance objectives. But when we notice that the app starts lagging,
wedecidetodedicatetimeforthisandimprovetheappperformance”.
AsforparticipantP5: “generallywhilecoding,wetrytorespectthe
performance checks just as other Lint checks. Then, we regularly pro-
gram performance sprints and there we will be specifically interested
inperformancerules”.Whilethestrategyreportedbyparticipant
P12ispurelyreactive,thestrategyofparticipantP5isstillproactive.
Improving performance incrementally. One participant explained
howshedealswithlegacycodewherethelinterwasnotusedbefore
(P9).“IconfigureLinttoincrementallyincreasetheappperformance.
I define a baseline, then I work to decrease this baseline progressively.
Ialsotrytoensurethatthenewcodeisalwaysmoreeffectivethanthe
old one”(P9). Android Lint allows to define a baseline file—i.e.,a
11
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. On Adopting Linters to Deal with Performance Concerns in Android Apps ASE ’18, September 3–7, 2018, Montpellier, France
snapshotofthecurrentwarningsoftheproject.Thebaselineallows
touseLintwithoutaddressingoldissues.Whenaskedabouthow
theincrementalimprovementhappens,theparticipantP9replied
“I change the severity of some checks, for example I configure some
rules to block the compilation”.
Discussion. Integratingthelinterfromprojectstart-upiscom-
monly advised [ 38]. Moreover, previous studies show that devel-
opersarelesslikelytofixthelinterwarningsonlegacycode[ 14].
The statements of some participants are aligned with this common
wisdom.However,thestrategies(b)and(c)showthatdevelopers
can adopt the linter differently according to their work style. In
particular,developerswhoareprototypingorareinrushtorelease
can adopt strategy (b) and apply the linter after finishing their
core development. Interestingly, developers who prefer to manage
performance reactively can also leverage the linter by following
strategy(c).Finally,strategy(d)showsthattheconfigurabilityof
the linter maximises its chances to be adopted.
4.2.2 Linter Can Be Used under Different Settings. Some partici-
pants reported using the linter individually, while others explained
how they use it with their team.
Supportingindividualdevelopment. Halfoftheparticipantsre-
portedthattheusageofthelinterwasapersonalchoice(P1,P3,P5,
P8,P9,P11,P13). “IonlyrunLintasmyselfaspartofareviewthat
I want to do” (P3). These participants usually use it from the IDE
interactively: “It is through Android studio interactively” (P13).
Checking performance rules in a team. Other participants re-
portedthattheusageofthelinterforperformancepurposeswas
requiredonateamlevel(P2,P4,P6,P7,P9,P10,P12,P14). “Inthe
team,Lintavoidsaccumulatingproblems.Wehaveadefinedsetof
rulesandeveryteammembermustrespectthem” (P10).Inthiscases,
thelinterisgenerallyastepinthecontinuousintegrationchain: “it
is set up with continuous integration, so Lint runs after every commit
and you will get a report. Then, you can choose to look at it or not”
(P2).
Discussion. Thereportedsettingsofusingthelinterdonotapply
exclusively for performance. The participants explanations under-
linetheimportanceofthelinterinteractivityandintegrationinthe
development workflow.
4.2.3 Linter Prioritises Performance Aspects. Many participants
said that while they use the linter, they prioritise performance
related issues (P2, P3, P5, P6, P9, P13). For instance: “there are so
manydifferentchecksbutIwouldsayperformanceusuallycatchesmyeye”(P2).Someparticipantsgavealsodistinctprioritiestodifferent
performance aspects: “if it is anything about threading, I will take a
look at it and review it before deciding if I want to fix it or not” (P3),
“IgivesomuchimportancetoUIperformanceandallmemory-related
issues”(P13).Someparticipantsexpressedtheseprioritieswitha
configuration: “I changed the severity of rules that interest me, so
they block the compilation” (P9).
Discussion. Eachappcanhavedifferentspecificitiesandneedsin
termsofperformance.Thankstoconfigurability,thelintercanhelp
developers to focus on performance aspects that sound relevant
and critical for them.4.3 What Are the Constraints of Using Linters
for Performance Purposes?
Theconstraintsreportedbyparticipantswerestructuredaround
two main topics: (i) social challenges and (ii) tool limitations.
4.3.1 Linter Faces Social Challenges. The participants reported
cultural elements that make the use of linter for performance chal-
lenging. The participants encountered these issues in their work
environment, with colleagues or superiors.
Static analysis is not suitable for performance. Many participants
described that developers generally think that static analysis is not
suitableforimprovingperformance(P1,P2,P3,P4,P7,P12).Partici-pantP1stated: “Ithinkthatthereisagapinunderstandingwhystatic
analysis is useful forperformance”. Participants explained thatthis
mindset is due to the nature of static analysis: “because Lint is only
looking at the code. Some developers feel that there should be a better
tool that analyses the app while running” (P3). Other participants
thought this gap is due to the complexity of performance issues:
“for the actual real-world bottlenecks that most apps face, it is not the
Lintthatwillhelpyou.Performanceissuesareverycomplicatedor
have multiple causes that cannot be pinpointed to a one line of Java
code”(P2). Participant P4 stated that this gap may be due to the
confusionoftheterm“performanceissue”: “Foreachperformance
issue there is a root cause and an observation. The term performance
issueisoftenusedtorefertoboth.Butitisnecessarytodistinguish
them.ThedefaultLintrulescontainsomebasicandtrivialrootcauses,
which could statically be identified. But in some cases you have an
observation and you cannot guess the root cause. So here Lint cannot
help you. To sump up, Lint requires you to define in advance what
youarelookingfor,itishardtouseittomatchtheobservationand
the cause”.
Nobody complained. Many participants reported that they regu-
larly deal with colleagues and superiors who believe that perfor-
manceshouldbemanagedreactively(P1,P3,P4,P5,P8,P9,P12).
For example, participant P5 stated that the common rule in her
environmentis “onlywhenthesuperiorsortheend-userscomplain
thattheappislaggingornotsmooth,wesayokwehavetocareaboutperformance”.ParticipantP5highlightedacasewherethispressure
came from a superior: “performance refactoring is a back-office task
that the product owner cannot see. It is hard to negotiate these tasks”.
Some participants underlined that this mindset is particularly tied
to performance more than any other software quality aspect: “with
performance you do not want to do a lot, you want to make sure that
you are really looking at the issue and not trying to over optimise the
code”(P3).
Wedonothavetime. Thismindsetisveryrelatedtotheprevious
one.However,weobservedcaseswherethedevelopersexplainthat
the performance checks of the linter are not considered only for
time constraints without any explicit agreement on the manage-
ment of performance reactively (P1, P6, P7). Participant P1 reports
observing this mindset in many companies: “why waste time on
performancerules?Letusmoveaheadandwewillfigureaboutthis
later.Unfortunatelythatlaterneverehappens,orcomesonlywhen
the problem is big enough”.
12
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sarra Habchi, Xavier Blanc, and Romain Rouvoy
Performanceisnotimportantinourcase. Someparticipantsre-
ported working in contexts where performance was considered
irrelevant (P1, P8, P12). Participant P1 reports experiencing this
situation in young projects: “when you build a small app and do not
know whether it will scale or not, it does not seem useful to spend
timeinmakingsurethatstaticanalysisofperformanceisdoneright”.Participant P8 described a case where performance was consideredirrelevantforatypeofapps: “wedidverybasicappdevelopmentand
not particularly hardware-based development. We developed Uberclones, and all those apps did not require any specific performance
from the device”.
The linter rules are irrelevant. Two participants described that
someperformancechecksareconsideredirrelevant(P2,P12).Par-
ticipantP12gaveexamplesofLintperformancerulesthatdonot
reallyhaveanimpact: “OverDrawisarulethat usedtoberelevanta
long time ago, but now with powerful smartphones it is not anymore.
Moreover, the Android system detects all these trivial issues and fixes
themautomatically.DevelopersareobsessedwithOverdraw,itbecame
a cult actually, but this is not what really hinders the performance
ofyourapp”.Theproblem OverDraw occurswhenabackground
drawable is set on a root view. This is problematic because thetheme background will be painted in vain as the drawable will
completely cover it.
Discussion. Calcagno etal.[15]have alreadyreferredto theso-
cial challenge while describing their experience in integrating astatic analyser into the software development cycle at Facebook.
Someofthereportedchallenges, e.g.,wedonothavetime,apply
tolinters ingeneral.However, theothermindsetsare particularly
resistanttotheuseoflintersforperformance.Thebeliefthatstatic
analysis is not suitable for performance seems to be prevalent, six
participants mentioned it explicitly. Developers are used to linters
as tools that report minor issues like styling violations and dep-
recations.Theyarenotawareenoughofthecapabilitiesofstatic
analysisindetectingperformanceissues.Toolmakersshouldput
moreeffortsoncommunicatingaboutlintersastoolsthatcanac-
companydevelopersindifferentdevelopmentaspects.Themindset
that performance should be managed reactively confirms previous
observationsaboutAndroidappsdevelopment[ 27].Thisfinding
shows that Android developers stilllack understanding about the
implicationsofperformancebottlenecks.Asforlinterrulesthatare
consideredirrelevant,thisincitestheresearchcommunitytodig
deeperintotheimpactofthesepractices.Formanybadpractices
likeOverDraw,westilllackprecisemeasurementsoftheirpenalties
on performance in real world contexts.
4.3.2 Linter Suffers from Limitations. The participants reported
several linter limitations that make it complicated to use it for
performance purposes.
Notwellpresented. Severalparticipantsmentionedthatthelinter
rulesarenotwellorganisedorexplained(P1,P4,P6,P11,P13,P14).
Interestingly three participants said explicitly that, for a long time,
theydid not even knowthat AndroidLint hadperformance related
checks(P1,P11,P14), “Ididnotknowtherearedifferentcategories
in Lint. For me it is just Lint, I do not distinguish between them” (P1).
Furthermore,participantP14explainedthatinthebeginningshe
did not know that some rules are related to performance, and thusshetreatedthemasotherchecksliketypography.Otherparticipants
complainedabouttheunclarityofthemessages, “itisnotalways
clear, for example performance checks about graphics, I cannot really
understand them at all”. On top of that, some participants found
that rules are not well organised: “there is a hierarchy with levels of
importance,butIfindituseless,itdoesnothelpme.SoifIwanttofocus
only on performance aspects, I have to search everywhere” (P6). The
same participant underlined the unclarity of the priorities given by
the linter to the checks: “I try to obey, but I do not really understand
thelogicbehindthepriorityandseverity”.Indeed,AndroidLintdoes
not give explanations about the priority and severity attributed to
each check, so we cannot understand the rationales behind them.
Imprecision. Some participants complained about the impreci-
sionofthedetectionperformedbythelinterforperformancechecks
(P9,P10,P13).ParticipantP9describedsituationswherethecode
contained a performance bad practice, but the corresponding lintercheckwasunabletodetectit: “insomedrawingmethods,Iwascalling
a method that called another one that made an intensive computing
with memory allocations. Lint did not detect this as DrawAllocation,
it actually does not look so deep into the code”. Other participantsreported false positives in performance checks. Participant P13
said:“Iregularlyhavefalsewarningsaboutunusedresourceswhen
theresourceisusedviaalibrary”,andparticipantP10stated: “Lint
indicates an unused resource but the image is actually used with a
variable”.
Poverty.Some participants mentioned that the linter is not very
rich with performance checks (P5, P6, P9). Participant P5 stated: “I
do not see so many performance-related Lint checks. And the existing
ones are so generic”. In the same vein, participant P9 said: “I rarely
see suggestions or important warnings about performance aspects.
Very few!”. Furthermore, the participants complained about the
absence of linter checks for Kotlin (P6, P9).
The difficultyof writingnew rules. Participant P2describedher
trial to write a linter check and the difficulties she faced: “I wanted
towritespecificLintrulesandafterafewtrialsIendedupdiscovering
that it is difficult to define something general enough to warrant a
Lintcheck.Also,theeffortputintobuildacustomLintcheckispretty
high. That is why it is not a common tactic for development teamsespeciallyonaperprojectbasis”.Theparticipantpointedalsothe
complexityofusingthecreatedruleinateam: “tobuildaLintcheck,
then distribute it to the team, then have the whole team use it, is
difficult”.
Discussion. Thefactthatatleastthreeparticipantsreportedthat
for a long time they used the linter without noticing that it has
performance checks was striking. Tool makers have to work more
on showcasing different checks categories. Also, the linter mes-
sagesshouldhighlightmoretheimpactofperformancepractices
tomotivatedeveloperstoconsiderthemseriously.Theimprecision
is a common limitation of linters [ 25], and performance checks
are no different in that respect. Similarly, the participants state-
ments about the difficulty to write linter rules align with previous
works[17].Asmanyothertools,AndroidLintprovidesthepossibil-
itytowritenewrulesbutthistaskiscomplexandtime-consuming.
Thus, developers are not motivated to write their own rules.
13
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. On Adopting Linters to Deal with Performance Concerns in Android Apps ASE ’18, September 3–7, 2018, Montpellier, France
5 IMPLICATIONS
Wesummariseinthissectiontheseveralimplicationsofourresults
for developers, researchers, and tools creators. Our findings are
based on the usage of linters for performance purposes in Android.
Nevertheless, they can also apply to other development platforms.
5.1 For Developers
Our results provide motivations for developers to use linters for
performance and show them how to maximise their benefits.
5.1.1 Benefits. Developerscanfindseveralbenefitsinusingthe
linter for performance. In particular, developers can use the linter
with performance checks to:
•Learnaboutthemobileframeworkanditsperformancecon-
straints,
•Anticipate performance bottlenecks that can be arduous to
identify and fix,
•Develop the performance culture in the team,
•Save time by automating concrete repetitive tasks,
•Save their reputation among peers,
•Increase their awareness and understanding of their apps per-
formance.
Developers should also be aware that the usage of the linter is
seamlessandcanbeintegratedinalongthedevelopmentworkflow.
5.1.2 Usage Fashions. Our participantsrecommend touse the
linter for performance in the following ways:
•From project startup to motivate developers to keep the code
clean,
•Only before releases to dedicate the early development stages
only for prototyping and making the important features,
•In performance sprints: developers can configure the linter to
focusonlyonperformanceinsomesprints.Thisapproachworks
also for developers who prefer to manage the performance reac-
tively,
•Improveperformanceincrementally:developersshouldconfig-
urethelintercarefullyonlegacycodetoavoidchaosanddevelopers
discouragement,
•IndividuallyinaninteractivewayintheIDEorinateamwith
the continuous integration,
•Prioritising performance aspects: developers can configure
the linter to focus on performance aspect that interest them and fit
with their app needs.
5.2 For Researchers
Our findings confirm hypotheses from several previous works and
open up perspectives for new research directions:
•We confirm that the mindset of managing performance re-
activelyisprevalent[ 27].Asthereshouldbeatrade-offbetween
reactive andproactive approaches,weencourage futureworks to
make real world comparisons between them;
•OurstudyshowsthatsomeAndroiddevelopersareindiffer-
ent to performance bad practices. Future studies should further
investigate this observation using quantitative methods.
•Somedeveloperschallengetherelevanceandimpactofper-
formancebadpractices. Wethereforeencouragefutureworkstoinvestigate and provide precise evidences about the impact of such
practices.
•Some developers are eager to consider more performance-
related checks. This should incite researchers to identify and char-
acterise more practices that can hinder the performance of mobile
apps.
5.3 For Tool Creators
Our findings confirm the importance of some linter features and
highlight new needs and challenges:
•Our findings align with previous works that suggest that sim-
plicity and integration in the development workflow help static
analysers to increase the trust of developers [ 17,25,36]. We en-
courage tool creators to ease the integration of their linters in
development tools;
•Our findings show that linters should be more clear and ex-
plicit about the categories of checks. Clarity is also required in the
explanationsofthepotentialimpactsofperformancechecks.We
cannot expect from developers to seriously consider the rules if wedonotprovideenoughinformationabouttheirnatureandimpacts;
•Providing the possibility to write linter rules is not enough.
Writing a linter rule should be simple and less time-consuming to
motivate developers to do it;
•Tool makers should put more efforts in communicating about
the capabilities of static analysis in detecting performance bad
practices;
•Given the benefits reported by participants, we invite more
tool makers to include performance-related checks in their linters.
6 THREATS TO VALIDITY
We discuss in this section the main issues that may threaten the
validity of our study.
Transferability. Onelimitationtothegeneralisabilityofourstudy
is the sample size. The sample size is not large and thus it may not
berepresentativeofallAndroiddevelopers.Toalleviatethisfact,
we interviewed highly-experienced Android developers. Nonethe-
less,thisselectionmayalsointroduceanewbiastothestudy.As
a matter of fact, the benefits and constraints reported by junior
Android developers can be different. We would have liked more
participants. However, transcribing interviews is a manual and
time-consuming task. Hence, having more interviews may involve
more workload and would affect the accuracy and quality of the
analysis. We found our results valuable and enough for theoretical
saturations. The study conducted by Tómasdóttir et al.[38], which
we discuss in our related works, approaches a similar topic with
asimilarsamplesize(15participants).Anotherpossiblethreatisthat we only interviewed Android developers who use the linter
forperformancepurposes.Androiddeveloperswhousethelinter
withoutperformancechecksmayalsohavetheirwordtosayabout
thelimitationsofusingalinterforperformance.Thus,morestudies
should be conducted to understand why some Android developers
usethelinteranddisableperformancechecks.Aswefocusedour
study on Android Lint, our results cannot be generalisable to other
Android linters. However, we believe that the choice of Android
Lint was sound for two reasons. First, it is a built-in tool of An-droid Studio and is activated by default, thus a large proportion
14
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Sarra Habchi, Xavier Blanc, and Romain Rouvoy
of Android developers should be using it. This fact was confirmed
in our preliminary online survey, where 97% of the participants
who used the linters were actually relying on Android Lint [ 12].
Secondly, it is the only linter that has performance checks specific
to the Android platform, and this detail is the core of our study.
Credibility. One possible threat to our study results could be the
credibilityofparticipantsanswers.Weintervieweddeveloperswho
haveastrongknowledgeaboutthe topic.However,wecannotbe
sure that their answers were based on real experience or knowl-
edgeacquiredfromexternalresources.Toalleviatethisissue,we
tried always to ask for details and relate to developers project and
workingenvironment.Also,weemphasisedbeforetheinterviews
that the process is not judgmental.
Confirmability. One possible threat could be the accuracy of the
interviews analysis and particularly the coding step. We use a con-
sensual coding to alleviate this threat and strengthen the reliability
ofourconclusions.Eachinterviewhasbeenencodedbyatleasttwo
authors. Initially, every author coded the interview independently
to avoid influencing the analysis of other authors. Afterwards, the
authors discussed and compared their classifications.
7 RELATED WORK
Mobile Performance Bad Practices. Mobile performance bad prac-
tices —a.k.a. code smells—have been addressed in various stud-
ies.Manystudiesinvestigatedtheenergyaspect,theyconsidered
code smells related to networking, sensors, non-sleep, and gen-
eral[28,33,34,39–41].Whileallthesestudiesreliedonappreposi-
tories and forums, Linarez et al.[26] used power profiling and API
calls analysis to identify energy code smells. Other studies focused
onidentifyingandcharacterisingmobileperformancesmellsandde-tectingthem.Guo etal.[
20]studiedresourceleaksinAndroidapps.
TheyproposedtheReldatooltodetectresourcesthatareexclusive,
memory-consuming,orenergyconsuming.Liu etal.[29]studied
70 performance bugs from eight Android apps and identified their
characteristics and common patterns. They also implemented Per-
fChecker,astaticanalyserthatdetectstheidentifiedperformance
bugs. The main identified bugs are lengthy operations in the main
thread, wasted operations for GUI, and frequently-called heavycallbacks. For bug characteristics, they found that performancebugs are more difficult to debug and fix than non-performance
bugs.Intermsofdebugging,profilersandperformancemeasure-
ment tools have demonstrated to be more helpful than traditional
stacktraceinformation.Additionally,worksthatproposedcatalogs
ofmobile-specificcodesmellsalsoincludedperformancerelatedissues[
21,35].Asmobileperformancesmellswereidentified,re-
searcherswereinterestedinassessingtheirrealimpactondifferentperformanceaspects.Hecht etal.[
22]conductedanempiricalstudy
abouttheindividualandcombinedimpactsofthreeAndroidper-
formance smells. They measured the performance of two apps
withandwithoutsmellsusingthefollowingmetrics:frametime,
number of delayed frames, memory usage, and number of garbage
collection calls. The measurements showed that refactoring theMember Ignoring Method smell improves the frames metrics by
12.4%. Carette et al.[16] studied the same code smells but focused
ontheenergy impact.Theyanalysed5open-source Androidappsand observed that in one of them the refactoring of the three code
smells reduced the global energy consumption by 4 ,83%.
PerformanceManagement. Linarezet al.[27] surveyeddevelop-
ers to identify the common practices and tools for detecting and
fixingperformance issuesin Androidopen-source apps.Based on
485answers,theydeductedthatmostofdevelopersrelyonreviews
andmanual testingfordetectingperformance bottlenecks.When
askedabouttools,developersreportedusingprofilersandframe-
work tools, only five of them mentioned using static analysers.
Developerswere alsoopenly questionedabout thetargetsof their
performanceimprovementpractices.From72answers,thestudy
established the following categories: GUI lagging, memory bloats,
energy leaks, general performance, and unclear benefits. With the
aim of helping developers understand and predict performanceproblems in mobile apps. Nistor et al.[
30] proposed SunCat, a
tooled-approach that based on a run with small inputs explainshow would an app behave with large inputs. In five apps, Sun-
Cat identified 29 usage scenarios and five confirmed performance
problems.
Qualitative Studies on Linters. Tómasdóttir et al.[38] conducted
aqualitativestudytoinvestigatethebenefitsofusinglintersina
dynamicprogramminglanguage.Theyinterviewed15developers
to understand why and how JavaScript developers use ESLint [ 7]
in OSS. They found that linters can be used to augment test suites,
theysparenewcomers’feelings whenmaking theirfirstcontribu-
tion,andsavetimethatgoesintodiscussingcodestyles.Christakis
and Bird [ 17] conducted an empirical study combining interviews
and surveys to investigate the needs of developers from static anal-
ysis. Among otherresults, they found thatperformance issues are
thesecondmostseverecodeissuesthatrequireanimmediateinter-
ventionofdevelopers.Performanceissueswerealsointhetopfour
needs of developers. Johnson et al.[25] conducted 20 interviews
to understandwhy developers do notuse static analysistools like
FindBugs [ 8] to find bugs. They found that all the participants are
convincedbythebenefitsofstaticanalysistools.Ho wever,thefalse
positives and warnings presentation are the main barriers of tools
adoption.
8 CONCLUSION
We investigated in this paper the benefits and the constraints of
usinglintersforperformancepurposesinAndroidapps.Wecon-
ducted a qualitative study based on interviews with experienced
Androiddevelopers.Ourresultsprovidemotivationsfordevelopers
touselintersforperformanceandsharewiththemhowtomake
this usage the most beneficial. Our findings highlight also the cur-
rent challenges of using linters for performance. These challenges
openupnewresearchperspectivesandshownewneedsfortool
makers.
ACKNOWLEDGMENTS
Theauthorswouldliketothanktheintervieweesfortheirtimeand
involvement. They also thank all developers who participated to
the online survey that motivated this study.
15
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. On Adopting Linters to Deal with Performance Concerns in Android Apps ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]2015. Mobile Stats. https://www.soasta.com/blog/
22-mobile-web-performance-stats/. [Online; accessed April-2018].
[2]2016. Mobile App Retention Challenge. https://dazeinfo.com/2016/05/19/
mobile-app-retention-churn-rate-smartphone-users/. [Online; accessed April-
2018].
[3]2018. AndroidDev. https://www.reddit.com/r/androiddev/. [Online;accessed
April-2018].
[4]2018. Android Lint. https://developer.android.com/studio/write/lint.html. [On-
line; accessed March-2018].
[5]2018. CheckStyle. http://checkstyle.sourceforge.net/. [Online;accessedApril-
2018].
[6]2018. Detekt. https://github.com/arturbosch/detekt. [Online; accessed April-
2018].
[7] 2018. ESLint. https://eslint.org/. [Online; accessed April-2018].
[8]2018. FindBugs. http://findbugs.sourceforge.net/. [Online; accessed April-2018].
[9] 2018. Infer. http://fbinfer.com/. [Online; accessed April-2018].
[10] 2018. Ktlint. https://github.com/shyiko/ktlint. [Online; accessed April-2018].
[11] 2018. PMD. https://pmd.github.io/. [Online; accessed April-2018].
[12] 2018. Technical Report. https://zenodo.org/record/1320453.
[13]Steve Adolph, Wendy Hall, and Philippe Kruchten. 2011. Using grounded theory
tostudytheexperienceofsoftwaredevelopment. EmpiricalSoftwareEngineering
16, 4 (2011), 487–513.
[14]Nathaniel Ayewah, David Hovemeyer, J David Morgenthaler, John Penix, and
WilliamPugh.2008. Usingstaticanalysistofindbugs. IEEEsoftware 25,5(2008).
[15]Cristiano Calcagno, Dino Distefano, Jérémy Dubreil, Dominik Gabi, Pieter
Hooimeijer, Martino Luca, Peter O’Hearn, Irene Papakonstantinou, Jim Pur-
brick, and Dulma Rodriguez. 2015. Moving fast with software verification. In
NASA Formal Methods Symposium. Springer, 3–11.
[16]Antonin Carette, Mehdi Adel Ait Younes, Geoffrey Hecht, Naouel Moha, and
RomainRouvoy.2017. Investigatingtheenergyimpactofandroidsmells.In Soft-
ware Analysis, Evolution and Reengineering (SANER), 2017 IEEE 24th International
Conference on. IEEE, 115–126.
[17]Maria Christakis and Christian Bird. 2016. What developers want and need
fromprogram analysis:anempiricalstudy.In Proceedingsofthe 31stIEEE/ACM
International Conference on Automated Software Engineering. ACM, 332–343.
[18]John W Creswell and J David Creswell. 2017. Research design: Qualitative, quan-
titative, and mixed methods approaches. Sage publications.
[19]BarneyGGlaserandJudithHolton.2007. Remodelinggroundedtheory. Historical
Social Research/Historische Sozialforschung. Supplement (2007), 47–68.
[20]Chaorong Guo, Jian Zhang, Jun Yan, Zhiqiang Zhang, and Yanli Zhang. 2013.
Characterizing and detecting resource leaks in Android applications. In Pro-
ceedingsofthe28thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering. IEEE Press, 389–398.
[21]SarraHabchi,GeoffreyHecht,RomainRouvoy,andNaouelMoha.2017. Code
Smells in iOS Apps: How do they compare to Android?. In Proceedings of the 4th
International Conference on Mobile Software Engineering and Systems . IEEE Press,
110–121.
[22]Geoffrey Hecht, Naouel Moha, and Romain Rouvoy. 2016. An empirical study of
theperformanceimpactsofandroidcodesmells.In ProceedingsoftheInternational
Workshop on Mobile Software Engineering and Systems. ACM, 59–69.
[23]GeoffreyHecht,BenomarOmar,RomainRouvoy,NaouelMoha,andLaurence
Duchien. 2015. Tracking the Software Quality of Android Applications along
theirEvolution.In 30thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering. IEEE, 12.
[24]SiwElisabethHoveandBenteAnda.2005. Experiencesfromconductingsemi-
structured interviews in empirical software engineering research. In Software
metrics, 2005. 11th ieee international symposium. IEEE, 10–pp.[25]Brittany Johnson, Yoonki Song, Emerson Murphy-Hill, and Robert Bowdidge.
2013. Whydon’tsoftwaredevelopersusestaticanalysistoolstofindbugs?.In
Proceedings of the 2013 International Conference on Software Engineering. IEEE
Press, 672–681.
[26]Mario Linares-Vásquez, Gabriele Bavota, Carlos Bernal-Cárdenas, Rocco Oliveto,
Massimiliano Di Penta, and Denys Poshyvanyk. 2014. Mining energy-greedy
api usage patterns in android apps: an empirical study. In Proceedings of the 11th
Working Conference on Mining Software Repositories. ACM, 2–11.
[27] Mario Linares-Vásquez, Christopher Vendome, Qi Luo, and Denys Poshyvanyk.
2015. Howdevelopersdetectandfixperformancebottlenecksinandroidapps.In
Software Maintenance and Evolution (ICSME), 2015 IEEE International Conference
on. IEEE, 352–361.
[28]Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2013. Where has my battery
gone? Finding sensor related energy black holes in smartphone applications.
InPervasiveComputingandCommunications(PerCom),2013IEEEInternational
Conference on. IEEE, 2–10.
[29]Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing and detect-
ing performance bugs for smartphone applications. In Proceedings of the 36th
International Conference on Software Engineering. ACM, 1013–1024.
[30]Adrian Nistor and Lenin Ravindranath. 2014. Suncat: Helping developers under-
stand and predict performance problems in smartphone applications. In Proceed-
ings of the 2014 International Symposium on Software Testing and Analysis. ACM,
282–292.
[31]Daniel G Oliver, Julianne M Serovich, and Tina L Mason. 2005. Constraints
and opportunities with interview transcription: Towards reflection in qualitative
research. Social forces 84, 2 (2005), 1273–1289.
[32]FabioPalomba,DarioDiNucci,AnnibalePanichella,AndyZaidman,andAndrea
De Lucia. 2017. Lightweight detection of Android-specific code smells: The
aDoctor project. In Software Analysis, Evolution and Reengineering (SANER), 2017
IEEE 24th International Conference on. IEEE, 487–491.
[33]Abhinav Pathak, Y Charlie Hu, and Ming Zhang. 2011. Bootstrapping energydebugging on smartphones: a first look at energy bugs in mobile devices. In
Proceedings of the 10th ACM Workshop on Hot Topics in Networks. ACM, 5.
[34]AbhinavPathak,AbhilashJindal,YCharlieHu,andSamuelPMidkiff.2012. What
is keeping my phone awake?: characterizing and detecting no-sleep energy bugs
in smartphone apps. In Proceedings of the 10th international conference on Mobile
systems, applications, and services. ACM, 267–280.
[35]JanReimann,MartinBrylski,andUweAßmann.2014. ATool-SupportedQuality
Smell Catalogue For Android Developers. In Proc. of the conference Modellierung
2014intheWorkshopModellbasierteundmodellgetriebeneSoftwaremodernisierung
– MMSM 2014.
[36]CaitlinSadowski,JeffreyvanGogh,CieraJaspan,EmmaSöderberg,andCollin
Winter.2015. Tricorder:BuildingaProgramAnalysisEcosystem. 2015IEEE/ACM
37th IEEE International Conference on Software Engineering 1 (2015), 598–608.
[37]Christiane Schmidt. 2004. The analysis of semi-structured interviews. A com-
panion to qualitative research (2004), 253–258.
[38]KristínFjólaTómasdóttir,MaurícioAniche,andArievanDeursen.2017. Why
andhowJavaScriptdevelopersuselinters.In Proceedingsofthe32ndIEEE/ACM
InternationalConferenceonAutomatedSoftwareEngineering.IEEEPress,578–589.
[39]Panagiotis Vekris, Ranjit Jhala, Sorin Lerner, and Yuvraj Agarwal. 2012. Towards
VerifyingAndroidAppsfortheAbsenceofNo-SleepEnergyBugs..In HotPower.
[40]Jack Zhang, Ayemi Musa, and Wei Le. 2013. A comparison of energy bugs for
smartphoneplatforms.In EngineeringofMobile-EnabledSystems(MOBS),2013
1st International Workshop on the. IEEE, 25–30.
[41]Lide Zhang, Mark S Gordon, Robert P Dick, Z Morley Mao, Peter Dinda, and
Lei Yang. 2012. Adel: An automatic detector of energy leaks for smartphone
applications.In Proceedingsof theeighth IEEE/ACM/IFIPinternational conference
on Hardware/software codesign and system synthesis. ACM, 363–372.
16
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:28:00 UTC from IEEE Xplore.  Restrictions apply. 