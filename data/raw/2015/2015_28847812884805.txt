Behavioral Log Analysis with Statistical Guarantees
Nimrod Busany Shahar Maoz
School of Computer Science
Tel Aviv University, Israel
ABSTRACT
Scalability is a major challenge for existing behavioral log
analysis algorithms, which extract nite-state automaton
models or temporal properties from logs generated by run-
ning systems. In this paper we present statistical log anal-
ysis, which addresses scalability using statistical tools. The
key to our approach is to consider behavioral log analysis as
a statistical experiment. Rather than analyzing the entire
log, we suggest to analyze only a sample of traces from the
log and, most importantly, provide means to compute sta-
tistical guarantees for the correctness of the analysis result.
We present the theoretical foundations of our approach
and describe two example applications, to the classic k-Tails
algorithm and to the recently presented BEAR algorithm.
Finally, based on experiments with logs generated from
real-world models and with real-world logs provided to us
by our industrial partners, we present extensive evidence for
the need for scalable log analysis and for the eectiveness of
statistical log analysis.
CCS Concepts
Software and its engineering !Dynamic analysis;
Software verication;
Keywords
Log analysis, specication mining
1. INTRODUCTION
Running systems, be it web servers, virtual machines on
the cloud, industrial robots, or network routers, generate
logs that document their actions. The analysis of these logs
carries much potential to improve software engineering tasks
from documentation and comprehension to test generation
and verication. Existing algorithms and tools for behav-
ioral log analysis include various specication mining and
model inference approaches, which extract nite-state au-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14-22, 2016, Austin, TX, USA
c2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.2884805tomaton (FSA) models or temporal properties. Example
approaches include [3,11,15,18,22,23,25,34].
However, a major challenge for all behavioral log analysis
algorithms is scale. Indeed, recent work has reported that
existing implementations of several well-known log analysis
algorithms run out of memory or take hours to complete
when executed over logs of several hundred MBs [35].
In this work we propose statistical log analysis ,
which addresses scalability of behavioral log anal-
ysis using statistical tools. Rather than analyzing the
entire log, we suggest to analyze only a sample of entries
from the log and, most importantly, provide means to com-
pute statistical guarantees for the correctness of the analysis
result.
Specically, we develop an approach to adding sta-
tistical guarantees to behavioral log analyses. Given
an analysis of interest and a sample of entries from a log, we
compute the statistical condence one may have in the anal-
ysis results. Conversely, given an analysis of interest and a
required level of condence, we compute a stopping criteria,
e.g., a sample size, for which indeed the analysis results can
be trusted at the required statistical condence level.
Following the presentation of our general approach, we
demonstrate it by concretely applying it to two dierent
previously published analyses. First, we apply statistical log
analysis to the well-known k-Tails algorithm [4], which ex-
tracts a candidate behavioral model from a log of execution
traces, based on the set of sequences of kconsecutive events
found in the log (we use the variant presented by Beschast-
nikh et al. in [2]). Second, we apply statistical log analysis
to the BEAR algorithm, recently presented by Ghezzi et al.
in [17], which builds a set of Discrete Time Markov Chains
(DTMC) of users' navigational behaviors recorded in logs,
based on the frequencies of transitions found in the log.
We have implemented our approach and evaluated it on
three sets of logs. Our experiments show that as the size of
the logs grow, their analysis becomes challenging, and that
statistical log analysis can be used to signicantly reduce
the computation eort of the analysis while still providing
highly reliable results. We provide a detailed account of the
evaluation we have performed in Sect. 4.
It is important to note that the scalability we aim for in
statistical log analysis is sub-linear: the complexity of the
analysis we do is less than linear to the size of the input log.
Note that this is a rather strong notion of scalability. In fact,
our experiments show that as the log size increases, the size
of the sample we analyze approaches a constant! Indeed, the
stopping criteria we compute depend on the condence one
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   877
may want in the correctness of the results, but are indepen-
dent of the log size. As real-world log sizes are unbounded,
this strong notion of scalability is indeed required.
We further note that our present work does not make any
new claims regarding the usefulness of the results of behav-
ioral log analyses such as k-Tails and BEAR. Based on claims
and evidence provided in previous works, by others, as cited
above, we assume that these analyses' results could be useful
for software engineers, and focus in the present work solely
on the challenge of scalability and on the statistical means
we propose to use in order to address it.
The approach we suggest is a pure black box approach.
We do not look at the code of the program that created the
log and take only the log itself as input. In many real-world
situations, indeed the code is not available or is just too
complex and too large to be a subject for static analysis.
Finally, we note that our approach assumes that relevant
entries (e.g., traces) are randomly and independently sam-
pled from the log and that the log adequately reects the
behavior of the system under investigation. This assump-
tion may not always hold, e.g., if traces and logs depend on
running tests that were generated according to some strat-
egy. Our approach should therefore not be used as is as a
means to evaluate the quality of test suites.
Paper organization. We start o with a short discus-
sion of related work. Sect. 3 presents an overview of our
approach and its unique features, using two concrete exam-
ple applications. Sect. 4 presents an extensive evaluation
over three sets of logs. Finally, Sect. 5 concludes with a
summary of contributions and future work directions.
2. RELATED WORK
While there has been much research on developing be-
havioral log analysis algorithms (see, e.g., [2,3,11,13{19,22,
23, 25, 27, 28, 30, 31, 34, 36]), only very little has been pub-
lished on their scalability and on the condence one may
have in their output. Recently, Cohen and Maoz [7] have
presented k-condence, a condence measure for k-Tails [4],
which computes the probability that the log is complete (this
work is extended to two other algorithms in [8]). These
works do not provide statistical guarantees. In contrast, our
work is based on statistical log analysis and is thus much
more robust and general than the one presented in [7, 8]; it
can answer many other questions, beyond log completeness,
as we later demonstrate, and provides engineers with much
exibility in adapting to dierent algorithms and setting up
required levels of accuracy on the one hand and statistical
condence on the other hand.
One means to achieve scalability in log analysis is dis-
tributed computing. In [35], the authors cast several dif-
ferent specication mining algorithms to the MapReduce
model. In [26], the authors present the use of MapReduce
to scale k-Tails. We consider distribution to be an orthogo-
nal approach to ours. We note that a distributed computing
framework is not always available or easy to use. Moreover,
the scale-up in running time is in theory at most linear in the
number of computing resources, and in practice even less; in
the results reported in both works [26,35], the marginal con-
tribution of adding more resources quickly decreases.
Finally, in a recent FSE NIER track short paper [6], we
presented an overview of our approach with preliminary
evaluation. Our present paper extends [6] signicantly, witha comprehensive presentation of our approach and an exten-
sive evaluation performed over real-world data.
3. STATISTICAL LOG ANALYSIS
The key to statistical log analysis is to consider
behavioral log analysis as a statistical experiment .
For example, if the log is partitioned into traces, each new
trace in the log is considered a trial which may contribute
to the acceptance or rejection of an hypotheses about the
system that generated the log. The granularity of the parti-
tion is up to the user to dene, based on the specic analysis
of interest. The key property for the selection of partition
granularity is that its parts (e.g., traces) can be indepen-
dently sampled from the log.
To cast the behavioral analysis into a statistical experi-
ment, we assume that the log is a good representation of
the system generating it. Further, we assume that the sys-
tem and its environment do not change over time, to ex-
clude inuence of external biases on the traces. In stochastic
terms, we assume that the system corresponds to a time-
homogeneous process. The traces are assumed to be gener-
ated identically and independently from the same underly-
ing distribution. Finally, our approach as presented here can
only be applied to incremental inference algorithms, where
each trace can be processed independently and its contribu-
tion can be determined with respect to the current state of
the algorithm.
Still this most general setup allows us to apply well-known
tools for statistical inference. We present basic denitions
and show two example concrete applications.
3.1 Basic DeÔ¨Ånitions
A trace over an events alphabet  is a nite word
=he1;e2;:::;emiwheree1;:::;em2. LetSbe a sys-
tem generating traces over an alphabet . We denote by
D(S) : ![0;1], the discrete probability distribution of
traces when executing the system in its environment. A log
ofS, denoted by l, is a nite set of traces from D(S). In
the behavioral log analysis setup, SandD(S) are unknown.
All we know is a log lfromD(S). We assume that the dis-
tribution of traces in lclosely resembles the distribution of
traces inD(S), i.e., that the log includes typical uses of the
system under investigation. We have no information about
the system Sother than the log l.
Filters may be applied to the log l, so as to remove events
that should not be considered by the analysis. Without
loss of generality, we further assume that the log l(possibly
after ltering) is partitioned according to some user-dened
criteria, such that each event that appears in lbelongs to
exactly one part of the partition. An example partition is
one that is based on traces. Another example is a partition
where each part consists of exactly one event.
3.2 Example I: k-Tails
k-Tails [4] is a well-known algorithm for extracting a can-
didate behavioral model from a log of execution traces. It
has been extensively used in the dynamic specication min-
ing literature and tools, in several variants, e.g., [1, 9, 24,
25, 32]. One variant of k-Tails, presented in [2], reads each
trace and collects k-sequences, i.e., sequences of kconsecu-
tive events. It then uses these k-sequences to construct the
k-Tails FSA. Most importantly, the FSA is uniquely dened
by the set of k-sequences observed in the log.
878Since a log may be too large to analyze, one would like
to dene a stopping criterion to indicate that\enough"traces
were seen. For this purpose, we dene a notion of -similarity
as follows: a log lis-similar when the total probability of
all the unobserved k-sequences to be observed in the next
trial (i.e., in the next randomly selected trace from D(S)), is
smaller than or equals .is a statistical bound. Intuitively,
in our context, can be viewed as a target insensitivity level
to infrequent sequences.
Hypothesis testing as a stopping criteria. Our null
hypothesis is that the log is not -similar, i.e., that the prob-
ability of a new random trace to reveal new information
(include ak-sequence that has not appeared in previously
analyzed traces) is larger than . We stop reading new traces
when this hypothesis can be rejected.
The experiment protocol is iterative. At each step we pick
a random trace from the log and check if the trace includes
previously unobserved k-sequences; if so, we start a new
experiment for the new knowledge base (i.e., the new set of
all observed sequences so far); otherwise, we increment the
number of trials since the last new k-sequence was observed;
when this number is larger than N, the null hypothesis can
be safely rejected. But where does Ncome from?
We model our analysis as a series of Binomial experi-
ments [33]. Given a target insensitivity , and a statistical
signicance level , we apply a Binomial proportion test [5]
to compute N, the number of consecutive trials (new traces)
that do not reveal new information required to reject the null
hypothesis, i.e., to safely conclude that the log is -similar.
More formally, we describe the setup of iteratively ran-
domly selecting a trace from a log and comparing the facts
extracted from it (i.e., k-sequences in our example) against a
knowledge base in terms of a Binomial experiment model [33]
(this corresponds to a single experiment in the series):
The experiment consists of Nrepeated independent
trials (in our context, traces we have read since the
last trace that revealed at least one new k-sequence)
Each trial has a probability of success p(in our context,
a success is a trace that reveals at least one new k-
sequence)
The probability of success does not change throughout
the experiment (in our context, the knowledge base of
k-sequences observed so far determines p)
Given the above modeling as a Binomial experiment, the
distribution to observe ssuccessful trials (i.e., traces reveal-
ing at least one new k-sequence) out of Ntrials is:
Bin(N;s ) = 
N
s!
ps(1 p)N s
Ideally, we would stop analyzing traces once all the k-
sequences have been observed, i.e., when p= 0. However, in
our context, we do not know the complete set of k-sequences
andpis unknown. Therefore, we bound it inside a Binomial
proportion condence interval [5].
Denition 1 (Binomial proportion condence interval) .Let
^pdenote the proportion of successful trials over Nrandom
trials from Bin(N;s ); letzdenote the 1 1
2percentile
of a standard normal distribution, where we refer to as
the error percentile. Then, phas a probability of (1  )to be contained within the Binomial proportion condence
interval

^p zr
1
N^p(1 ^p);^p+zr
1
N^p(1 ^p)
Remark 1. The above formula relies on Normal approx-
imation to a Binomial. We chose to present it due to its
relative simplicity. However, it is inadequate when p0.
Other, superior methods to compute a Binomial proportion
condence interval exist in the literature. In our calculations
in the evaluation (Sect. 4), we used Wilson interval recom-
mended by [5], which provides high coverage probability for
a broad range of pvalues. We note that for p0 andp1,
Agresti-Coull interval may be more appropriate. Lastly, we
note that Jereys prior interval is less conservative and tends
to yield coverage probability closer to 1  .
Denition 2 ((,)-condence) .A (,)-condence reects
a 1 condence that the true (but unknown) probability
of successpis less than , i.e.,
^pupperbound(1  )
Based on the Binomial proportion condence interval for-
mula, we nd the stopping criteria by computing the number
of unsuccessful consecutive trials Nrequired to guarantee
(,)-condence.
Remark 2. The proportion of success changes every time
a new fact is learned. In k-Tails, when the knowledge base
is empty (i.e., no event sequence is known), the probability
of success on any trace (longer than k) equals one. On the
other extreme, when the knowledge base is complete (i.e., all
possiblek-sequences are known), the probability of success is
zero. Therefore, when computing the proportion condence
interval, we must re-approximate ^ pafter learning a new fact,
as the probability of success changes. For this reason, we
start a new Binomial experiment after every success (i.e.,
after observing a trace with a new k-sequence).
We conclude with a concrete example. If we select a target
insensitivity level of = 0:05 and a signicance level of =
0:01, Willson's interval gives us N= 130. If we follow the
protocol presented above, we can stop reading traces from
the log once we analyze N= 130 consecutive traces without
new information. The statistical guarantees are about the
completeness of the obtained set of k-sequences. When the
analysis terminates and the null hypothesis is rejected, we
have a 1 = 0:99 condence that the total probability of
any of the unobserved k-sequences to appear in a random
trace, denoted by p, is less than = 0:05. In short, we say
that the sample is -similar with a signicance level .
Remark 3. Interestingly, and perhaps surprisingly, note
thatNdepends on ^ p,, and, but noton any specic detail
of the k-Tails algorithm, not even the chosen k. However,
this independence can be explained: the details of the k-
Tails algorithm and the choice of the parameter kaect the
very success or failure of each trial in the experiment. This
points to a major advantage of our approach (also in contrast
to [7,8]); it can be easily extended to any analysis algorithm
that one can cast into the Binomial experiment protocol.
Remark 4. Note that while reading additional data is ex-
pected to increase the reliability of the resulting model, by
denition of the stopping criteria, it is redundant (and a
waste of resources) in terms of the required statistical bound.
8793.3 Example II: BEAR inference algorithm
BEAR [17] is a recently presented inference algorithm that
constructs a set of Discrete Time Markov Chains (DTMC)
of users' navigational behaviors recorded in logs. The in-
ferred models are then analyzed in order to verify quanti-
tative properties by means of probabilistic model checking.
The algorithm constructs DTMCs according to user classes.
Each user is assigned to one or more of these classes, and
her user sessions (i.e., traces) are used in the construction of
the corresponding DTMC. DTMCs are constructed accord-
ing to transitions' frequencies in the log from which they are
mined. Most importantly, the set of transitions' frequencies
found in the log determines the DTMC model that will be
built.
Consider an engineer applying BEAR over a log from a
particular user class. Running BEAR over the entire log may
be very slow, therefore we apply our statistical log analysis to
the problem. Rather than reading the entire log, we sample
events from it. Each sampled event is a trial in an experi-
ment. A transition is created by connecting each event in a
user session to its preceding event. We estimate the transi-
tions' frequencies, and would like to stop sampling once we
know that the condence interval around the estimated fre-
quencies is smaller than for a given statistical signicance
level.
We describe the procedure for a single transition, then
simply apply it over the set of all observed transitions. We
refer to the true (but unknown) transition frequency in the
system under investigation, i.e., in D(S), byp, and the es-
timated frequency by ^ p. We dene a notion of -similarity
as follows: a log lis-similar if the estimated frequency ^ p,
computed as the proportion of transitions which equal to
the desired transition in the traces in l, satisesj^p pj.
Given this notion, we propose the following iterative ex-
periment protocol: pick a random trace from the log and
traverse its transitions. If a transition equals to the tran-
sition at hand, increase the transition occurrence counter
and the total transitions occurrence counter, and update
its frequency ^ p. Otherwise, if a transition is not equal to
the transition at hand, increase the total transitions occur-
rence counter, and update its frequency ^ p; compute d=
j^pupperbound(1  ) ^plowerbound(1  )j; ifdis smaller than 
stop sampling. But where would the bounds come from?
Condence interval as a stopping criteria. As done
in the previous section, we dene a Binomial experiment.
Here, we change the interpretation of a successful trial. First,
a new trial is dened every time a transition is read. Second,
a new transition is considered a success, if it equals to the
transition at hand. The analysis of the condence interval
remains the same. In this context, the null hypothesis is
that the true frequency pof the transition lays outside of
the condence interval. We stop sampling (i.e., reject the
null hypothesis), once we observe that the size of the con-
dence interval is suciently small. That is, when we stop,
the log we have analyzed so far is -similar with a statistical
signicance level . In other words, the probability that the
true frequency of a transition, p, is far from ^ pby more than
, is less than .
As an example for a transition of interest, consider web
application of a hotel. A log from such an application can be
partitioned into traces based on IP addresses and a time win-
dow. Now, consider a user browsing through the catalog of
rooms, selecting a room and reaching the make-a-reservationpage. Then, instead of making a reservation, the user hits
the back button. An engineer may be interested in the fre-
quency of users who take this transition, in order to make an
informed decision regarding a possible change in the make-
a-reservation page.
Consider a concrete example. If we set = 0:05, after
observing a transition in 10 out of 50 transitions (with a
frequency ^p= 0:2), we achieve 95% (1- ) condence that
the true frequency of the transition, p, is in the interval
0:112p0:33 (d1= 0:218). By reading more user
sessions, we may be able to narrow this interval and get
a more precise result, at the same signicance level. For
example, after observing the desired transition in 200 out
of 1000 transitions, for the same 95% (1- ) condence we
achieve 0:176p0:226 (d2= 0:05). Thus, if we set
= 0:05 and use the procedure presented above, we would
stop reading new traces at this stage.
Further, reducing the signicance level (1- ) narrows the
intervaldas well; e.g., after observing the desired transition
in 200 out of 1000 transitions (^ p= 0:2), with= 0:10 we get
0:18p0:222 (d3= 0:042). This occurs since increasing
, increases the probability for an error (i.e., the probability
that the true frequency lays outside of the interval).
Finally, an observation: ^ paects the interval size as well,
e.g., with= 0:05, and after observing the desired transi-
tion in 50 out of 1000 transitions, we get 0 :038p0:065
(d4= 0:027). The interval size d4is nearly half the size of
d2, obtained for the same sample size and signicance level.
In this experiment, the statistical guarantees are about
the accuracy of the obtained frequency. When the anal-
ysis terminates, we have 1- condence that the distance
between the true and obtained frequencies is smaller than .
Remark 5. As in the k-Tails example, here too, the stop-
ping criteria depends on ^ p,, and, but does notdepend
on the transition of interest. Note that the interval size 
is monotonically decreasing in the signicance level and
monotonically decreasing in the sample size N. We note
that themay not be symmetric around the estimated pro-
portion ^p(depnding on the selected interval). Therefore, to
achieve-similarity, the interval size dmust be less than .
Remark 6. Note that our key assumption is that the anal-
ysis is able to decide, given a transition, whether it equals to
the transition at hand. Thus, the example application to the
BEAR algorithm, which estimates the frequencies of tran-
sitions, is actually an instance of a more general approach
to estimate the frequency of satisfaction of any property
of interest, given that satisfaction in a sampled part (e.g.,
a trace) can be decided. In this experiment, multiple in-
dependent trials are performed. In each we check for the
satisfaction of a property of interest. A trial is considered
a success if the sampled part satises the property of inter-
est and a failure otherwise. Calculating bounds for property
satisfaction frequency is done similarly to the way we have
done it for BEAR in the special case of transitions.
Remark 7. One may notice that the trials in the procedure
above are not entirely independent (violating an important
assumption). In fact, transitions in the same part, i.e., a user
session, are in most cases highly correlated. Nevertheless,
our method is applied over large logs, containing many parts,
whose length is usually signicantly smaller than the log size.
Furthermore, parts are randomly selected, which eliminates
correlation between events from dierent parts. Therefore,
880the correlation between events in similar sessions may not
have a signicant eect on the analysis. We empirically test
the soundness of our approach in Sect. 4.
4. EV ALUATION
The research questions guiding our evaluation are:
RQ1 As real-world logs become larger, will existing log
analysis algorithms scale?
RQ2 Does our approach reach a required high reliability
(soundness)?
RQ3 Does our approach allow one to reduce the number of
traces read while maintaining high reliability?
RQ4 What is the overhead of computation time in our ap-
proach and does it actually allow scalable sub-linear
execution times?
4.1 Logs Used in Evaluation
In the evaluation we have used three sets of logs.
4.1.1 Log Set I - Logs generated from FSA models
We conducted the k-Tails experiment (Sect. 3.2) over logs
that we have generated from four FSA models of real sys-
tems, taken from three previously published works [10, 20,
29]. The models varied in size and complexity: alphabet size
ranged from 10 to 42 (mean 24.5), number of states from 6
to 18 (12.25), and number of transitions from 28 to 209
(88.75). To generate the logs from the models we used the
trace generator from [24], with path coverage. We xed the
number of traces in all logs to 25K; the number of events
ranged from about 252K to about 605K, and the average
trace length from 7.37 to 24.21. To parse the above logs, we
used the parser from the implementation of Synoptic [3].
4.1.2 Log Set II - Daily regression logs from a telecom-
munications company
We repeated the k-Tails experiment (Sect. 3.2) with a set
of ve real-world logs, which we received from our partner,
a team in a large multi-national telecommunications equip-
ment company. The set includes logs of high-level API calls,
system events, distribution of tasks, DB queries, and com-
munication between processes over dierent machines. All
logs were generated by a single run of a daily regression,
which was executed in debug mode. Again to parse the
logs we used Synoptic's parser cited above. As the parser
requires the user to specify regular expressions, we have de-
ned these for each of the ve logs, after consulting with
our colleagues at the company. The logs varied in size and
complexity: alphabet size ranged from 10 to 115, number
of events from 10.7K to 40K, number of traces from 194 to
1261, and average trace length from 13.8 to 55.31.
The number of traces contained in the logs above was
rather small (as they only contained a single day of regres-
sion run). Since gaining statistical condence requires large
logs, we decided to duplicate the original les, by duplicat-
ing each entry multiple times, thus creating multiple copies
of the same day. To duplicate the logs, we rst read the
entire log, then duplicated indexes, which we mapped to
the parsed traces. We duplicated each index multiple times,
shued them, popped one index at a time, and read its cor-
responding trace. The advantage of duplicating the original
logs is that it enables us to x the amount of informationthat the log contains, while increasing its size. This allows
us to better interpret the experiments' results.
4.1.3 Log Set III - Real-Estate REST application logs
To conduct the BEAR algorithm experiment (Sect. 3.3),
we used BEAR's dataset, generously given to us by the au-
thors of [17]. The set consists of a single log, which was
generated by a Real-Estate REST application, executed by
an IT consultancy company.
The log contains rows which record requests of web re-
sources issued by clients to the application's Web server.
Each request is considered to be an event. To parse the
log we used the code and lters (expressed by regular ex-
pressions) provided by the authors of [17]. The log consists
of about 390K events, collected over two years. The al-
phabet size is 29. We followed [17] and ltered out events
(i.e., requests) that correspond to secondary resources (e.g.,
requests for CSS or Javascript resources), as they do not
represent users' navigational behaviors. After applying the
lters, about 39.5K events are used in the construction of
the DTMCs.We partitioned the events in the log into users'
sessions based on IP addresses and a user window of 100
minutes. Events with similar IP, separated by a longer time
period were assigned to dierent traces (i.e., user sessions).
This ltering and partitioning are consistent with the ones
chosen by the authors of [17].
Finally, since almost all of the events originate from users
of the Mozilla Firefox browser (about 38K events), we fo-
cused on constructing a model for this user class. This
yielded a single DTMC, which captures these users' interac-
tions with the application. We ltered out events that did
not belong to this user class.
For more details about the log and lters used, we refer the
reader to the original paper about the BEAR approach [17].
4.2 Setup and Methodology
We executed the experiments on an ordinary laptop com-
puter, Dell XPS 15, Intel Core i7-4712HQ CPU@2.3GHz,
16GB RAM, Samsung SSD PM851 mSATA 512GB.
4.2.1 Binomial conÔ¨Ådence interval calculator
We implemented the Binomial condence interval calcu-
lator used in the evaluation with the ConfidenceInterval
class included in Apache Commons Math 3.3.
4.2.2 k-Tails experiment
To implement the k-Tails experiment (Sect. 3.2), we mod-
ied the code of Invarimint [2] to support the use of the Bi-
nomial condence interval calculator when analyzing traces.
To apply our approach with k-Tails, we rst read the en-
tire log and shued the traces. We then run the k-Tails
algorithm with Invarimint's implementation [2], which we
modied to identify if new information was revealed after
a new trace is read. Then, we performed ( ;)-condence
check after reading each trace with the Binomial condence
interval calculator. We ended the experiment once the de-
sired level of condence was obtained. The k-Tails parame-
ters we used were k= 1;2;3.
4.2.3 BEAR algorithm experiment
In this experiment we rst selected a set of transitions
whose frequencies' we want to bound. We decided to con-
sider any transition observed as we process the log as an ex-
88105001000150020002500
1 2 3Time (ms)
K
cvs.net DatagramSocket Socket ZipOutputStream
12481632641282565121024204840968192
1 2 4 8 16 32 64 128Time (ms)
Duplication factor
log1 log2 log3 log4 log5Figure 1: Running k-Tails over Log Set I (left) and Log Set II (right), see Section 4.3.1
.
periment, i.e., once a new transition is observed, it initiates
a new experiment that is added to the set of running exper-
iments. We stopped processing the log once all the running
experiments had achieved the desired (; )-condence. To
test how reliable is the stopping criterion, we stop updating
the frequency of transitions whose experiment has ended.
In other words, we avoid updating transitions for which we
already achieved a condence interval smaller than .
To implement, we modied BEAR's implementation to
support the use of the Binomial condence intervals. First,
for the Binomial condence interval to be correctly com-
puted, user interaction sessions must be randomly selected
from the log. To cope with this, we modied BEAR's parser
to rst split the log into user sessions according to IP and a
user window. Then, at each iteration of the algorithm, we
select a user session and pop its events sequentially. After we
read all the user session events, we remove it and randomly
select a new user session.
When selecting an event, we use the BEAR algorithm
to construct a transition that connects it to the preceding
event in the user session (initial events are connected with
a dummy `start' node). Then, we update the transitions'
frequencies for all of the active experiments accordingly, as
explained in Sect. 3.3. Further, if the transition is observed
for the rst time, we initiate a new experiment for it. To
do this, we initialize a new Binomial condence intervals
calculator setting its initial trials count to the number of
read events and its number of successes to one.1
Finally, we compute the condence intervals around all
transitions with active experiments and terminate any ex-
periment with a calculated interval of size smaller than .
We stop reading events once all the experiments are termi-
nated (i.e., when the desired (; )-condence level is ob-
tained for all transitions), or when the entire log is read.
4.2.4 Measures
The measures we use to evaluate the results of our experi-
ments are similarity, reliability, absolute and ratio of sample
size, and absolute and ratio of analysis execution time.
1We note that an experiment is not initiated for a new tran-
sition after we obtained a tight condence interval for non-
observed transitions. For example, if we already saw 450
transitions, then the condence interval of any unobserved
transition is between 0 to 0.008, hence smaller than , so we
treat it as a completed experiment.Similarity. We dene similarity as the distance between
the true parameter that the experiment estimates and its
estimated value when the experiment is completed.
In the k-Tails experiment, the parameter used in the Hy-
pothesis testing as stopping criteria experiment is the prob-
abilitypof a new random trace to reveal new information
(see Sect. 3.2). Since we do not know p, we use the Maxi-
mum Likelihood Estimator (MLE) [12] ofpover the entire
log, denoted by pMLE. We compute it by taking the ra-
tio between traces containing k-sequences missing from the
partial log, and the total number of traces in the entire log.
Then, similarity is dened as 1 jpMLE ^pj. Since the
k-Tails experiment only ends when ^ p= 0, the similarity is
simply 1 jpMLEj, and can also be interpreted as a measure
of completeness.
In the BEAR algorithm experiment, the property used
in the Condence interval as stopping criteria experiment is
the true frequency of a transition, denoted by p. Since we do
not knowp, we compute the MLE ofp, denoted by pMLE,
which is the frequency of the transition in the entire log.
Then, we compute the distance between the transition's fre-
quencies in the partial log, denoted by ^ p, andpMLE. Again,
similarity is dened as 1 jpMLE ^pj. Intuitively, when the
two are identical, similarity equals 1, and when the two are
most distant, similarity equals 0.
-Reliability (precision). We dene-reliability as the
experiment's precision, where a true positive (false positive)
is the case where the experiment was correctly (incorrectly)
terminated.
In the k-Tails experiment, the null hypothesis is that the
true probability of a new random trace to reveal new infor-
mation (i.e., a new k-sequence), p, is greater than . The
experiment terminates once we have (; )-condence that
pis smaller than . Since we use the pMLE ofp, we declare
the experiment successful if pMLE. Since 1 cap-
tures the theoretical reliability of the experiment (assuming
the underlying assumptions hold, see Sect. 3), we expect the
success rate, i.e., the -reliability of a set of experiments, to
be 1 .
In the BEAR algorithm experiment, we stop once we gain
1 condence that the estimated frequency ^ pof a tran-
sition and the true frequency of a transition pare within
a distance of . Since we use the pMLE ofp, we declare
the experiment successful if jpMLE ^pj. Again, since
8821 captures the theoretical reliability of the experiment,
we expect the -reliability of a set of experiments to be 1  .
Absolute and ratio sample size. The absolute sample
size is the number of trials performed (traces or transitions
analyzed) in the experiment. The ratio sample size is the
ratio between the absolute sample size and the size of the
entire log (number of traces or transitions).
Absolute and ratio of analysis execution time. The
absolute sample analysis execution time is the time required
to analyze the sample (e.g., to extract k-sequences). The
ratio of analysis execution time is the ratio between the ab-
solute sample analysis execution time and the time required
to analyze the entire log.
4.3 Results
4.3.1 RQ1
To answer RQ1, we conducted the following experiments.
We run k-Tails over Log Set I and computed the execution
time of extracting k-sequences from traces. Execution times
as function of k are presented in Fig. 1 (left). These results
illustrate that the complexity of analysis (i.e., the chosen k)
can greatly aect the execution time of mining k-sequences.
Furthermore, the richness of the log is a prominent fac-
tor. As an example, one may observe the large gap between
the results for the DatagramSocket log and the ZipOutput-
Stream log. Indeed, the DatagramSocket model contains a
larger alphabet and more transitions than the ZipOutput-
Stream model. As a result, its traces tend to be longer and
so is the log size. Moreover, while the number of events in
theDatagramSocket log is 2.44 times larger, the computa-
tion time it requires is 4.43, 5, and 5.51 times longer than
that of the ZipOutputStream log for k=1 ;2;3 resp.
We run k-Tails over Log Set II. Since these logs were
rather small, we also run k-Tails over the duplicated logs.
Fig. 1 (right) shows the execution times for duplication fac-
tors 1; 2;4;:::; 128. These experiments illustrate a linear
increase in the execution time as the logs increase in size,
which shows that for truly large logs, even a simple collection
ofk-sequences may not be feasible.
Finally, we run the original implementation of BEAR over
Log Set III and measured the execution time. We repeated
it 10 times. The average time measured was 12 minutes.
To answer RQ1, we see that the size and com-
plexity of logs and the complexity of the analysis
algorithms can greatly aect the analysis compu-
tation time. As the size of the logs grow, their
analysis becomes challenging.
4.3.2 RQ2
To answer RQ2, we conducted the following experiments.
First, we run the k-Tails experiment over Log Set I and Log
Set II. For Log Set I, we xed the target insensitivity at
0:05, and used three dierent statistical signicance levels
, 0: 01, 0:05, and 0:15. We invoked the Binomial interval
condence calculator after reading each trace. The numbers
of unsuccessful consecutive trials Nrequired to guarantee
(,)-condence are 130, 73, and 40 resp. We repeated each
experiment 30 times for each of the four logs and the three
values ofandk.Fig. 2 (left)2reports the similarity levels obtained when
reaching the stopping criteria. As can be seen, the values of
the rst quartile for = 0:01; 0:05 in all models are higher
than 95%. This shows that in the large majority of ex-
periemnts the desired similarity level was indeed obtained
with these signicance levels. Furthermore, 92 :01% of all
experiments obtained an average similarity level of 90% and
higher. This illustrates reliability for Log Set I.
Interestingly, in deeper analysis of results not shown in
the gure, we observed that the average -reliability(0: 05)
levels for= 0:01 are 100% for all models. Moreover, when
setting= 0:05, the -reliability(0:05) levels for the four
logs were 98: 8%, 91: 6%, 100%, and 92: 2%. For= 0:15, the
-reliability(0:05) levels for the four logs were 90%, 95 :5%,
54:4%, and 31 :1%. This shows that for low values of the
intervals tend to be too conservative, while for high values
of, the intervals may be not conservative enough.
For Log Set II we conducted these experiments in a similar
way (experimenting with similar parameters). Fig. 2 (right),
reports the average similarity levels obtained when reaching
the stopping criteria. Since the original logs are small, the
stopping criteria for many of them was never reached, and
the logs were fully read, yielding similarity of 1. Therefore,
we repeated the experiments for the duplicated logs. The
gure reports the average similarity levels, with = 0:05,
= 0:05, and k= 2; 3. As can be seen, the similarity
levels obtained for all logs exceed the 95% threshold, with a
moderate reduction as the log size increases. Furthermore,
97:7% of all experiments obtained an average similarity of
90% and higher, which demonstrases high reliability.
For lack of space, we do not provide the reliability levels
here. The trend for the average reliability levels observed
earlier was repeated in these experiments, with = 0:01
being too conservative and = 0:85 not being strict enough.
Second, we run the BEAR experiment over Log Set III.
As many of the 343 transitions of the DTMC have very low
frequencies, in this experiment we included low insensitivity
values. We used = 0:05 and= 0:005; 0:01; 0:02;0:05
and repeated the experiment 10 times with each .
Fig. 3 shows 1-similarity values as function of a target
insensitivity level for the transitions observed during the
experiments and achieving ( ;)-condence. These values
capture the absolute distances between the true transitions'
frequencies and the estimated frequencies. As expected,
these distances reduce with . Further, for a large major-
ity of the transitions, the obtained distance lays below .
The average similarity values measured are 99 :72%, 99:61%,
99:49%, 98:97% for= 0:005; 0:01; 0:02;0:05 resp., showing
a consistent increase in similarity as insensitivity decreases.
Finally, the -reliability levels measured (not shown in
the gure) are 90:89%, 92 :88%, 96:83%, 99:29% for=
0:005;0:01; 0:02;0:05 resp., which illustrates that reducing
insensitivity level increases the error rate of the experiments.
This is explained by the fact that reducing the insensitivity
level requires capturing more transitions with higher preci-
sion.
We conclude that the results demonstrate that our method
reaches high reliability levels, which is reected by the small
distances (high similarity values) reported above.
2We did not include the cvs.net log in Fig. 2, as both its
rst and third quartiles equal 1, which shrinks the body of
its boxplot to a single point.
8830.80.820.840.860.880.90.920.940.960.981Similarity
0.9450.9550.9650.9750.9850.9951.005
1 2 4 8 16 32 64 128Similarity
Duplication factor
log1 log2 log3 log4 log5Figure 2: Similarity obtained for Log Set I and Log Set II, see Section 4.3.2.
00.010.020.030.040.05
0.005 0.01 0.02 0.051-similarity (abs. distance)
Insensitivity( Œ¥)
1500110001150012000125001
0.005 0.01 0.02 0.05Read transitions
Insensitivity( Œ¥)
Figure 3: The gures report the results for Log Set III. (left) 1 - similarity (distance) between the true and estimated transitions'
frequencies as function of , see Section 4.3.2. (right) Number of read transitions as function of , see Section 4.3.3.
Remark 8. The reader may notice that we discarded unob-
served transitions from our analysis. We did so since large
majority of the transitions had frequencies lower than the
values we used. Therefore, they would nearly always be
considered within a distance of from the estimation. This
makes the similarity and reliability values so high that the
inuence of changes in can no longer be observed. By
discarding these transitions, we excluded the long tail (of
the transitions' distribution) from our results and facilitated
their comprehension.
To answer RQ2, our method is able to obtain high
similarity levels, which in the case of k-Tails, in-
dicate that most of the information is indeed cap-
tured. Further, the expected -reliability level of
1 is obtained, when the selected condence
level is above 95%. Our experiments with BEAR
provide evidence that the sampled transitions' fre-
quencies guarantee the required similarity of ,
maintaining, as required, an error rate of or less
for= 0:02; 0:05.
4.3.3 RQ3 and RQ4
To answer RQ3 and RQ4, we run the k-Tails experi-
ment over Log Set I and Log Set II, with the same setup
and parameters as in RQ2. For better visualization of the
data, we present the average results for the experiments with
= 0:05; = 0:05, and k= 1; 2;3, and note that we ob-tained similar results with other values of . We repeated
all experiments 30 times.
For Log Set I, Fig. 4 (top, left) presents the percentage of
read traces when reaching the desired ( ;)-condence. As
can be seen, this increases as the complexity of the analysis
increases. This is not surprising as increasing k, increases
the amount of information that needs to be extracted from
the log. As an example, the number of k-sequences extracted
from the DatagramSocket log is 500, 8774, and 99259 for
k=1,2,3 resp. Therefore, condence is quickly reached for
k= 1, but is never reached for k= 3, in which on average
67:5% of the traces revealed at least a single new k-sequence
when they were processed. This shows that for logs with
high redundancy levels (where the information can be ex-
tracted from a fairly small set of randomly selected traces),
our method is able to signicantly reduce the number of
read traces. It also shows that when the log does not con-
tain much redundancy, as in the case of the DatagramSocket
and Socket logs, withk= 3, all of the traces are read. We
interpret this as the cost required to ensure the high re-
liability levels discussed in RQ2. The trend was repeated
with= 0:01; 0:15 (not shown in the gure), with a higher
percentage of read traces for = 0:01 and a lower one for
= 0:15, as one would expect.
Fig. 4 (top, right) presents the ratio between the sam-
ple analysis execution time and the entire log analysis exe-
cution time. Two encouraging conclusions can be derived.
First, when the log contains much redundancy, our method
8840%10%20%30%40%50%60%70%80%90%100%
1 2 3Read traces  (%)
K
cvs.net DatagramSocket Socket ZipOutputStream
0%20%40%60%80%100%120%140%
1 2 3Ratio of analysis execution time
K
cvs.net DatagramSocket Socket ZipOutputStream
050010001500200025003000
1 2 4 8 16 32 64 128Read traces (abs. number)
Duplication factor
log1 log2 log3 log4 log5
0%20%40%60%80%100%120%140%160%180%200%
1 2 4 8 16 32 64 128Ratio of analysis execution time
Duplication factor
log1 log2 log3 log4 log5Figure 4: Read traces and execution times for Log Set I and Log Set II, see Section 4.3.3.
achieves signicant reduction in execution time. As an ex-
ample, when setting k= 1, the sample times were between
1:5% and 5:77% of the log times. Second, even in the cases
where all traces are sampled, applying our method increases
the required time by only a reasonable factor of 20% to 35%.
Note that our implementation is rather simple and may be
further optimized.
Fig. 4 (bottom, left) presents the absolute number of read
traces when increasing the size of the dierent logs in Log Set
II. Since the logs were duplicated, the same information is
contained in all versions of each log. The dierence between
the versions of the logs is the amount of redundancy they
contain. As can be seen, starting with a duplication factor
of 4, the number of read traces stabilizes and does not in-
crease with the size of the log anymore. The trend shows
that our method is able to not be fooled by the redundancy
and to capture the meaningful information using a constant
number of traces! This justies the claim that our method
allows for sub-linear log analysis. In fact, as demonstrated
by the trend in Fig. 4 (bottom, left), the analysis indeed con-
verges to a constant sample size. Finally, Fig. 4 (bottom,
right), shows the ratio between the sample times and log
times for the dierent logs. As can be seen, the gap between
these two measures widens as the log size increases. This
can be explained by our earlier observation that the number
of sampled traces converges to a constant (from a certain
point on), in comparison with reading the entire log. One
may also note that for the majority of the logs, a benet is
obtained with a duplication factor of four and more. Finally,
similarly to the experiments over Log Set I, one may observe
an acceptable overhead when the entire log is read, ranging
from 34:4% to 92% with an average overhead of 62:4%.
Indeed, the statistical analysis does not come for free.
Still, note that the entire log is read if and only if ( ;)-
condence is not obtained. In such cases, the extra compu-
tations we perform provide the engineer with an indication
that the computed model does nothave the required sta-tistical guarantees. We consider this to be an important,
useful side contribution of our work.
To further investigate RQ3 and RQ4, we run the BEAR
algorithm experiment over Log Set III. To answer RQ3, we
used the same parameters as in RQ2. First, we measured
the number of read transitions required for each of the tran-
sitions to obtain ( ;)-condence. Fig. 3 (right) reports
these numbers as function of (as before, we did not include
transitions that did not obtain ( ;)-condence). The re-
sults show, as expected, that reducing the insensitivity level
comes at the cost of reading more transitions. The average
numbers of transitions read were 5796, 1684, 613, and 174 for
= 0:005; 0:01; 0:02;0:05 resp. We also report the average
number of read transitions when each run was terminated.
As described in the setup, a run was terminated once all
of the observed transitions achieved ( ;)-condence. The
values were 38042, 13201, 3237, and 502 (the entire log)
transitions for = 0:005, 0:01, 0 :02, 0:05 resp.
Finally, to answer RQ4, we present the average execution
times recorded for each of the experiments. The measured
times in minutes are 12.77, 4.23, 1.1 ,0.15 for = 0:005, 0:01,
0:02, 0:05 resp. In contrast, we measured an average of 12
minutes for the original BEAR implementation, as stated in
RQ1. This shows that our method can indeed lead to sig-
nicant time reductions. Further, one can observe that even
in the cases where the experiment only ended after all the
transitions have been read, such as in the case of = 0:005
(very low insensitivity), the overhead is very moderate with
an average of 6: 4% additional execution time. Here again,
we claim that the extra computation is not for nothing, as
it provides the user with important information about the
reliability of the constructed model.
We conclude that the results demonstrate our method's
ability to yield major reduction in the number of read tran-
sitions and in total computation time. Further, the results
emphasize the trade-o between sensitivity and scalability.
885To answer RQ3, after experimenting with values
that were shown to achieve high reliability lev-
els in RQ2, we conclude that our method is able
to reduce the number of traces (or transitions)
read. Our method eectively addresses redun-
dancy and converges to a constant sample size,
while still capturing the information available in
the log. Further, the method is able to identify
logs that do not contain much redundancy and in
these cases read most traces (or transitions) to
ensure high reliability.
To answer RQ4, our method is able to dramati-
cally reduce the analysis time of large logs with
high redundancy. It also has an acceptable over-
head in cases where the entire log needs to be
read, i.e., when (;)-condence is never obtained.
In such cases, it indicates that more data is re-
quired to construct a more reliable model.
4.4 Threats to Validity and Limitations
We now discuss threats to validity of our answers to the
research questions and additional limitations of our work.
One may argue that the duplication of logs in Log
Set II may not be representative of real-world long
logs. Still, as every system has a certain degree of complex-
ity with respect to an algorithm, as logs become longer, re-
dundancy with respect to the k-Tails algorithm is inevitable
(this holds for many other behavioral log analysis algorithms
as well). Our use of duplications allowed us to control for
this redundancy in our experiments. That said, we have also
used real-world large logs without duplications (Log Set III).
The similarity measure does not compare the -
nal output of the selected algorithms. We have de-
cided to focus on the data elements from which models are
constructed ( k-sequences, transitions) and not on the nal
output of the algorithms. Our decision is motivated by the
fact that these elements determine the nal output of the
algorithms. There may be dierent ways on how to measure
similarity between the nal outputs; choosing between them
may depend on the specic intended use.
The lters and criteria used to partition the log
into traces, can have a dramatic eect on the entire
analysis and on the resulting model (e.g., dening user
classes in BEAR). For dierent lters and dierent parti-
tion criteria, one may obtain results that are dierent than
the ones we have seen in our experiments. Note, though,
that our method does not restrict the partition, but only re-
quires that the traces which are derived from the partition
(e.g., read traces in k-Tails, single events in BEAR) can be
randomly and independently selected.
The analysis of traces may only take a small part of
the complete analysis computation time . In this work,
we focus on applying statistical means to reduce the amount
of analyzed data. Clearly, there are other steps in applying
a behavioral log analysis algorithm, such as parsing the log,
ltering and partitioning it into traces, and constructing a
model from the extracted data. These are all important
practical aspects which are beyond the scope of our present
paper. For example, in the present work, to obtain a random
trace we read the entire log (or duplicated it in memory).
As part of future work, we will explore means to randomlysample traces from a log without rst reading it entirely
(and without assuming a predened log structure).
The log may not represent the behavior of the sys-
tem that generated it. Generating a representative log
of a system is a problem that deserves its own investiga-
tion and is beyond the scope of the present paper. In the
present paper we limit our focus to investigate if behavioral
log analysis can be made scalable by sampling a portion of
the available log and obtaining results that are similar to
the results that may be obtained by processing the entire
available log, with required statistical guarantees.
The results of statistical log analysis, albeit cor-
rect, may not be useful when the given log has a long
tailed distribution with many very infrequent prop-
erties. For example, in the case of the BEAR algorithm, if
90% of the transitions in the log have frequency of less than
1%, setting the insensitivity to be greater than 1% will
cause most of the transitions to be missed completely. We
do not know whether many real-world logs exhibit such long
tailed distributions. Note that the distribution depends on
the chosen lters and partition (see above).
5. CONTRIBUTIONS AND FUTURE WORK
In this paper we presented statistical log analysis , which
uses trace sampling and statistical inference to address the
scalability challenge in the behavioral analysis of large logs.
The key to the approach is to consider each new part in a
log as a trial in an experiment. We demonstrated the appli-
cation of our approach to two dierent analyses: the classic
k-Tails algorithm and the recently introduced BEAR infer-
ence algorithm. Extensive evaluation with logs generated
from real-world models and with real-world logs provided
by our industrial partners provides evidence for the need for
scalability and for the eectiveness of statistical log analysis.
We believe that statistical log analysis has much potential
to help in scaling up existing behavioral log analysis algo-
rithms and thus in bringing these algorithms to the hands of
software engineers in practice. We consider the following fu-
ture directions. First, we investigate other, more elaborated
and robust stopping criteria, which can result in less con-
servative yet still correct analysis results. Second, together
with our industrial partners, we look for additional analysis
algorithms where statistical log analysis can be applied, for
example, in scenario-based trigger/eect analysis [21] and in
log comparisons in the context of software evolution. Lastly,
we investigate a more sophisticated machinery to remove
some of the underlying assumptions of our approach (i.e.,
time-homogeneity, incrementality).
Acknowledgements. We thank our colleagues in the un-
named multi-national telecommunications equipment com-
pany for sharing their logs with us. We thank the authors
of [17] for sharing their web log data with us. We thank
the anonymous reviewers of the conference for their helpful
comments. Part of this work was done while SM was on sab-
batical as visiting scientist at MIT CSAIL. This work has
been partly supported by Len Blavatnik and the Blavatnik
Family Foundation.
8866. REFERENCES
[1] M. Acharya, T. Xie, J. Pei, and J. Xu. Mining API
patterns as partial orders from source code: from
usage scenarios to specications. In ESEC/SIGSOFT
FSE, pages 25{34, 2007.
[2] I. Beschastnikh, Y. Brun, J. Abrahamson, M. D.
Ernst, and A. Krishnamurthy. Unifying FSM-inference
algorithms through declarative specication. In ICSE,
pages 252{261, 2013.
[3] I. Beschastnikh, Y. Brun, S. Schneider, M. Sloan, and
M. D. Ernst. Leveraging existing instrumentation to
automatically infer invariant-constrained models. In
SIGSOFT FSE , pages 267{277, 2011.
[4] A. W. Biermann and J. A. Feldman. On the synthesis
of nite-state machines from samples of their behavior.
IEEE Trans. Comput. , 21(6):592{597, June 1972.
[5] L. Brown, T. Cai, and A. DasGupta. Interval
Estimation for a Binomial Proportion (with
discussion). Statistical Science , 16(2):101{133, 2001.
[6] N. Busany and S. Maoz. Behavioral log analysis with
statistical guarantees. In ESEC/FSE , pages 898{901.
ACM, 2015.
[7] H. Cohen and S. Maoz. The Condence in Our
k-Tails. In ASE, pages 605{610, 2014.
[8] H. Cohen and S. Maoz. Have we seen enough traces?
In M. B. Cohen, L. Grunske, and M. Whalen, editors,
ASE, pages 93{103. IEEE, 2015.
[9] J. E. Cook and A. L. Wolf. Discovering models of
software processes from event-based data. ACM
Trans. Softw. Eng. Methodol. , 7(3):215{249, 1998.
[10] V. Dallmeier, N. Knopp, C. Mallon, G. Fraser,
S. Hack, and A. Zeller. Automatically generating test
cases for specication mining. IEEE Trans. Software
Eng., 38(2):243{257, 2012.
[11] F. C. de Sousa, N. C. Mendon ca, S. Uchitel, and
J. Kramer. Detecting implied scenarios from execution
traces. In WCRE, pages 50{59, 2007.
[12] M. H. DeGroot and M. J. Schervish. Probability and
Statistics . Addison Wesley, 3rd edition, 2002.
[13] M. El-Ramly, E. Stroulia, and P. G. Sorenson. From
run-time behavior to usage scenarios: an
interaction-pattern mining approach. In KDD, pages
315{324, 2002.
[14] M. Ernst, J. Cockrell, W. Griswold, and D. Notkin.
Dynamically discovering likely program invariants to
support program evolution. TSE, 27(2):99{123, 2001.
[15] D. Fahland, D. Lo, and S. Maoz. Mining
branching-time scenarios. In ASE, pages 443{453,
2013.
[16] M. Gabel and Z. Su. Online inference and enforcement
of temporal properties. In ICSE, pages 15{24, 2010.
[17] C. Ghezzi, M. Pezz e, M. Sama, and G. Tamburrelli.
Mining behavior models from user-intensive web
applications. In ICSE , pages 277{287, 2014.
[18] S. Kumar, S.-C. Khoo, A. Roychoudhury, and D. Lo.Mining message sequence graphs. In ICSE, pages
91{100, 2011.
[19] C. Lee, F. Chen, and G. Rosu. Mining parametric
specications. In ICSE , pages 591{600, 2011.
[20] D. Lo and S.-C. Khoo. QUARK: Empirical assessment
of automaton-based specication miners. In WCRE,
2006.
[21] D. Lo and S. Maoz. Mining scenario-based triggers
and eects. In ASE, pages 109{118, 2008.
[22] D. Lo and S. Maoz. Scenario-based and value-based
specication mining: better together. In ASE, 2010.
[23] D. Lo, S. Maoz, and S.-C. Khoo. Mining modal
scenario-based specications from execution traces of
reactive systems. In ASE, pages 465{468, 2007.
[24] D. Lo, L. Mariani, and M. Santoro. Learning extended
FSA from software: An empirical assessment. Journal
of Systems and Software , 85(9):2063{2076, 2012.
[25] D. Lorenzoli, L. Mariani, and M. Pezz e. Automatic
generation of software behavioral models. In ICSE,
pages 501{510, 2008.
[26] C. Luo, F. He, and C. Ghezzi. Inferring Software
Behavioral Models with MapReduce. In Proc. of
SETTA , volume 9409 of LNCS , pages 135{149.
Springer, 2015.
[27] L. Mariani, S. Papagiannakis, and M. Pezz e.
Compatibility and regression testing of
COTS-component-based software. In ICSE, pages
85{95, 2007.
[28] L. Mariani and M. Pezz e. Behavior capture and test:
Automated analysis of component integration. In
ICECCS, pages 292{301, 2005.
[29] M. Pradel, P. Bichsel, and T. R. Gross. A framework
for the evaluation of specication miners based on
nite state machines. In ICSM , pages 1{10, 2010.
[30] M. Pradel and T. R. Gross. Automatic generation of
object usage specications from large method traces.
InASE, pages 371{382, 2009.
[31] J. Quante and R. Koschke. Dynamic protocol
recovery. In WCRE, pages 219{228, 2007.
[32] S. P. Reiss and M. Renieris. Encoding program
executions. In ICSE, pages 221{230, 2001.
[33] S. M. Ross. Simulation, Fourth Edition . Academic
Press, Inc., Orlando, FL, USA, 2006.
[34] N. Walkinshaw and K. Bogdanov. Inferring nite-state
models with temporal constraints. In ASE, pages
248{257, 2008.
[35] S. Wang, D. Lo, L. Jiang, S. Maoz, and A. Budi.
Scalable Parallelization of Specication Mining. In
C. Bird, T. Menzies, and T. Zimmermann, editors,
The Art and Science of Analyzing Software Data .
Morgan Kaufmann, 2015.
[36] J. Yang, D. Evans, D. Bhardwaj, T. Bhat, and
M. Das. Perracotta: mining temporal API rules from
imperfect traces. In ICSE, pages 282{291, 2006.
887