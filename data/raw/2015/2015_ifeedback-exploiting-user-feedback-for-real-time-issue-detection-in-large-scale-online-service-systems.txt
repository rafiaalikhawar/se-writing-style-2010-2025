iFeedback: Exploiting User Feedback for
Real-time Issue Detection in Large-Scale
Online Service Systems
Wujie Zheng∗‡, Haochuan Lu∗†, Yangfan Zhou∗†, Jianming Liang‡, Haibing Zheng‡, Y uetang Deng‡
∗School of Computer Science, Fudan University, Shanghai, China
†Shanghai Key Laboratory of Intelligent Information Processing, Shanghai, China
‡Tencent Inc., Shenzhen, China
Abstract —Large-scale online systems are complex, fast-
evolving, and hardly bug-free despite the testing efforts. Backend
system monitoring cannot detect many types of issues, such as
UI related bugs, bugs with small impact on backend system
indicators, or errors from third-party co-operating systems,
etc. However, users are good informers of such issues: They
will provide their feedback for any types of issues. This ex-
perience paper discusses our design of iFeedback , a tool to
perform real-time issue detection based on user feedback texts.
Unlike traditional approaches that analyze user feedback with
computation-intensive natural language processing algorithms,
iFeedback is focusing on fast issue detection, which can serve as
a system life-condition monitor . In particular, iFeedback extracts
word combination-based indicators from feedback texts. This
allows iFeedback to perform fast system anomaly detection
with sophisticated machine learning algorithms. iFeedback then
further summarizes the texts with an aim to effectively present
the anomaly to the developers for root cause analysis. We present
our representative experiences in successfully applying iFeedback
in tens of large-scale production online service systems in ten
months.
I. I NTRODUCTION
Online service systems, especially those of large service
providers ( e.g., Microsoft, Facebook, and Google), are getting
larger in scale and more complex in its functionality. They
typically provide various services to different applications,
serving millions of users concurrently1. Serving huge amount
of users is a huge challenge to the reliability of such systems.
In typical industry practice ( e.g., that in Microsoft), service
developing teams usually test services in a smaller in-house
scale. In the production environments, massive users will
exercise the service with unpredictable circumstances that may
not be expected during development. Such a gap between the
environments of development and deployment is a notorious
typical source of bugs [1].
It has been well-accepted that testing can also be conducted
in the wild after a software artifact is released to the users
(e.g., [2], [3]), especially for online service systems. In a
testing prospective, the large amount of user requests to a
1For instance, services provided by Azure can be found at:
https://azure.microsoft.com/en-us/services/service actually exercise the system extensively. Speciﬁcally-
tailored system indicators ( e.g., performance indicators such as
the latency of critical method calls [4]) collected during sys-
tem operations can facilitate developers in fault removal and
further service upgrade [5]. However, deﬁning such speciﬁc
indicators is labor-intensive, which largely depends on a deep
understanding to the target service. Moreover, a monitoring
on such conventional indicators can only discover issues that
greatly affect the backend performance. Those issues related
with frontend user interface, bugs with few backend impact,
or errors from third-party co-operating systems can hardly
be successfully detected, not to mention the newly-occurring
issues that have never been encountered before.
Unlike existing efforts that rely on monitoring service-
speciﬁc system indicators, we show that with a proper text
processing, natural-language texts in user feedback can also
serve as a good, timely indicator of service runtime issues.
This experience paper introduces our efforts to analyze user
feedback, automatically generate key performance indicators,
and accordingly achieve real-time identiﬁcation on runtime
issues in large-scale online service systems. We present such
a system, namely, iFeedback , as well as our experiences in
applying iFeedback in tens of online production services,‘each
serving tens of millions of users concurrently.
User feedback, typically written in natural language texts
by end users, are usually complaints of bad experiences,
e.g., buggy behaviors and annoying features. Usually, such
feedback may be given via certain channels provided by each
service ( e.g. app built-in user response interface) when a user
encounter service issues. It is long been proposed that user
feedback can assist software maintenance [6]–[10]. However,
existing work in general focuses on conducting exhaustive text
mining to provide an ofﬂine report that helps manual analysis
of software issues. iFeedback , in contrast, shows that it is fea-
sible to perform a real-time issue detection by generating key
performance indicators automatically based on user feedback,
which in turn can be monitored so as to efﬁciently identify
service runtime issues online in a timely manner.
However, for real-world production service systems, to
enable a ﬁne-grained identiﬁcation of service runtime issues,
3522019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)
978-1-7281-2508-4/19/$31.00 ©2019 IEEE
DOI 10.1109/ASE.2019.00041
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. the number of such key performance indicators should be
huge. iFeedback faces huge challenges, includeing: 1) How to
properly generate a large amount of effective indicators that
can best describe potential issues automatically ,2 )h o wt o
identify service issues by monitoring such massive indicators
efﬁciently in a timely manner.
To address these challenges, iFeedback speciﬁcally extracts
word combinations to automatically generate such indicators
from tremendous user feedback text. It wipes off exten-
sive noisy/non-informative information. Then, to automatically
identify issues, iFeedback collect features from the indicators’
historical trend and the corresponding text content. Such
features are further used to achieve the issue detection via a
machine learning approach, i.e., a 2-class classiﬁcation model
determining whether or not an issue occurs. Conventional
approaches in detecting issues in a set of indicators typically
resort to unsupervised statistical methods [4], which locate
outliers in time-series trends. In contrast, iFeedback models the
problem as a 2-class classiﬁcation problem. This also allows
us to take advantage of other well-designed features in the
feedback, e.g., text diversity, to train the classiﬁer with high
accuracy. Our experiences show that iFeedback can achieve
very satisfactory results in detecting service issues, comparing
with traditional unsupervised methods.
We summarize the contributions of this paper as follows.
•We present iFeedback , a tool that can generate key perfor-
mance indicators automatically based on user feedback.
It can provide real-time issues detection for large-scale
online services based on the indicators.
•We propose an effective detection approach via a machine
learning-based classiﬁer. Unlike traditional regression-
based approaches, it is more suitable to handle the noisy
nature of feedback data via a set of hybrid features from
indicator values and feedback text. Thus, it can achieve
satisfactory results in both efﬁciency and accuracy. Such
a classiﬁcation-based perspective can shed light to other
system operation/maintenance tasks.
•We show that iFeedback is an effective tool via presenting
our successful experiences in applying iFeedback to tens
of online production services, each serving tens of mil-
lions of users concurrently. We expect such experiences
can inspire AIOps [5] in real-world production service
systems.
The rest of the paper is organized as follows. Section
II introduces the related work. In Section III, we discuss
some preliminaries and our design motivations. Sections IV
overviews our iFeedback design. Section V elaborates our
detailed design considerations in iFeedback . We present our
experiences in applying iFeedback in Section VI to show its
effectiveness. Section VII provides some further discussions
on the future work and limitations of this work, and we
conclude the paper in Section VIII.
II. R ELA TED WORK
Artiﬁcial Intelligence for IT Operations (AIOps) is a con-
cept that introduces intelligent algorithmic mechanisms ( e.g.machine learning) into IT operations [11]. It helps realize
automatic IT operations with an aim to reduce human efforts.
Extensive research work has suggested AIOps in solving
different practical problems, e.g., in failure handling [12]–[14],
in log analysis [15], [16] and in resource arrangement [17].
iFeedback , similarly, is a tool designed for AIOps, which aims
at automatically detecting emerging software system issues in
large-scale online service systems.
iFeedback mainly considers user feedback texts regarding
each speciﬁc online service as data source, and detects re-
ported issues accordingly. It has long been suggested that user
feedback texts contain signiﬁcant information and can be of
great value in supporting software development and mainte-
nance [6], [8], [9], [18]–[20]. For instance, Di Sorbo et al.
[18] propose SURF, a system that captures user requirements
in user feedback, which can in turn facilitate developers in
performing software maintenance tasks. It relies on machine
learning techniques to conduct topic classiﬁcation and then
summaries user requirements. Vu et al. [19] propose PUMA
that uses a phrase-based clustering approach to extract user
opinions in app reviews. However, existing work generally
focuses solely on generating speciﬁc topics, i.e., unveiling user
requirements [8], [18].
Gao et al. [10] propose IDEA, in which a topic modeling
algorithm is introduced to automatically interpret topics in user
reviews, and perform issue mining accordingly. Issue mining
is a difﬁcult task. Current approaches in general consider
user feedback as static data, and perform ofﬂine issue mining
without considering the time-related information, e.g., the
topic trend, in the feedback texts. They are not capable of
monitoring issues in a real-time manner and raising alarms
timely. In contrast, real-time issue detection, critical to large-
scale online service systems, is what iFeedback focuses on.
One key functionality of iFeedback is system runtime
anomaly detection. It is also a critical task in AIOps attracting
lots of efforts in both industrial practitioners and academic
researchers [21]–[24]. Speciﬁcally, anomaly detection in key
performance indicators (KPIs) has been widely used to moni-
tor systems and indicate issues [25]–[30]. For example, Laptev
et al. [27] propose EGADS, which achieves both accuracy and
scalability in anomaly detection on time-series KPIs with a
collection of anomaly detection and forecasting models. Liu
et al. [29] propose Opprentice. It adopts machine learning
techniques to classify KPIs’ anomaly within a given time
window. Xu et al. [28] introduce V ariational Auto-Encoder
(V AE) to realize unsupervised anomaly detection. Such a
mechanism can be successfully deployed to monitor periodic
KPIs in Web applications. However, building proper KPIs
is generally labor-intensive, which requires domain-speciﬁc
expertise. No experience on building such KPIs on user feed-
back texts has been introduced before. In contrast, iFeedback
takes an automatic approach. iFeedback is the ﬁrst approach
that transforms real-time issue detection automatically from a
feedback processing problem to a KPI monitoring problem. It
automatically generates a large amount of indicators from user
feedback texts, where system anomaly is detected with a set
353
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. of speciﬁc-tailored machine learning techniques. In this way,
iFeedback realizes real-time issue detection in user feedback.
It is worth noting that there is also a body of work which
conducts bug detection [31]–[33], bug classiﬁcation [34], [35],
and bug prediction [36], in bug or issue reports. Examples
include iDice proposed by Lin et al. [31], which monitors at-
tributes within issue reports, and identifying particular attribute
combinations to characterize emerging issues. However, such
reports require to contain a strict, structural data form, which
directly provides the required attributes. Such forms may vary
among different services. User feedback texts, in contrast,
are written in non-structural natural languages. iFeedback is
designed to deal with such non-structural feedback texts,
allowing it to be adaptive to different types of services.
III. P RELIMINARIES AND MOTIV A TIONS
We will ﬁrst illustrate the target systems, i.e, large-scale
online service systems, in which iFeedback is speciﬁcally
designed to conduct emerging issue monitoring and detection.
Since iFeedback is based on user feedback to perform such a
task, we will then introduce typical user feedback in our target
services.
A. Large-scale online service systems and their defects
In recent years, many service providers have launched on-
line cloud-based services for the public. Such online services
usually serve a large amount of users. For example, the target
service system of this work has billions of monthly active
users. Typically, the online services are implemented with
complicated programs and deployed in an extra-large scale,
i.e., on a cloud consisting of hundreds of servers. Online
service systems typically work in a 7 ×24 manner. Although
in industrial practice, a service must undergo a careful testing
phase, not all system defects can be removed before online
deployment, especially due to its complications. For example,
defects may manifest when there is a sudden boosting of
concurrent requests or when a corner use-case is encountered.
When a defect manifests and causes bad user experiences, it
is a critical task for service developers to ﬁx the defect in a
timely manner to avoid a broader inﬂuence.
However, our experiences in operating large-scale cloud-
based service systems reveal that it usually takes quite a long
time, before the developers are aware of the occurrence of an
issue. In this regard, backend systems can adopt an indicator
mechanism, where system runtime metrics ( i.e., KPIs) can be
designed to reﬂect the conditions of service. For example, the
faulty behavior of a service caused by a sudden increase of
concurrent user requests can be detected by an indicator built
to reﬂect backend workload. However, such mechanisms can
only cope with system defects that have signiﬁcant impact on
system runtime metrics. Faulty behaviours of the services at
the user end, for example, those related to user interface, may
not manifest as distinct abnormal behaviors of the backend
systems. Hence, it is a more efﬁcient way to detect user-end
faulty behaviors in the user end. User feedback is then a natural
data source to such a task. In this work, we will show how weUser A: I met some problems when purchasing with Wechat-Pay
User B: Payment failure when using payment code
User C: Why can’t I transfer money?
User D: What’s wrong with my Wechat-Pay account? There should be some
money. But I can’t ﬁnd it.
Fig. 1. Examples of a major form of feedback
build system indicators from user feedback, and how we can
rapidly identify different types of defects occurred in large-
scale cloud-based service systems.
B. User feedback
User feedback of a service is a direct reﬂection of user’s
experience on using the service. In modern online service
systems, especially those for mobile applications, when a user
encounter unexpected troubles, she may conveniently report
the issues through a feedback interface. Naturally, not all users
would like to report issues. But, given the large number of
users (typically millions of users) of an online service system,
the daily user feedback is still huge in volume.
User feedback is typically in non-structural raw texts. Figure
1 shows an example of a list of feedback texts for an online
service product2. Each text is provided by an individual user.
It is long been suggested that user feedback can be used to
evaluate the usability of services [20]. But retrieving useful
information from such non-structural raw texts and detect
issues accordingly is a quite challenging task. In our practical
experiences, we found that over 70% of the user feedback
items collected from over tens of online services are on
bad user experiences. Although it seems promising that we
can perform issue mining from the tremendous bad user-
experience feedback. However, our analysis shows that most
of reported bad experiences are caused simply by the incorrect
user-interface operations, which can be solved easily by a
better user guidance. In other words, they are not related to
an emerging system issue. Solving such a challenging task is
the major aim of iFeedback .
IV . IFEEDBACK OVERVIEW
Figure 2 shows the overall system design of iFeedback .
The workﬂow of iFeedback mainly consists of two phases:
1) service runtime indicator building phase, and 2) machine
learning-based issue detection phase.
iFeedback takes real-time user feedback texts as input.
It performs a speciﬁc-tailored natural language processing
approach to automatically generate tremendous ﬁne-grained
system indicators. iFeedback then applies a rule-based ﬁlter to
select possible indicators of faulty behaviors as candidates for
the issue detection phase.
In the issue detection phase, we focus on these candidate
indicators and accordingly perform feature engineering on
them. In this way, iFeedback generates vectorized features
to describe the service runtime condition. Then, a machine
learning-based classiﬁer is applied on these features to quickly
2The original feedback texts are written in Chinese.
354
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. Fig. 2. iFeedback Overview
perform anomaly detection. Its results are fed into a cluster-
ing algorithm that further summarizes similar anomalies, inorder to minimize human inspection efforts. The clusters ofanomalies are then reported as a clear summary of potentialissues detected in the user feedback. They can facilitate furtheranalysis of potential service issues.
We summarize the key steps shown in Fig 2 as follows,
where their technical details and practical concerns will be
elaborated in Section V.
•Indicator Generation: Extracting millions of indicators
automatically from non-structural user feedback texts isone key concern of
iFeedback . User feedback texts are
noisy in nature, which contain much useless informationin practice. We synthesize several natural language pro-cessing techniques to combat these noisy data.
iFeedback
can automatically extract meaningful word-combinationsas indicators. Details will be described in Section V.A.
•Indicator Filtering : The indicators generated with user
feedback texts can be large in volume. It is not cost-effective to monitor every indicator.
iFeedback then per-
forms a rule-based ﬁltering strategy based on historicaldata. The ﬁltering approach can remove most indicatorsand retain possible indicators of faulty behaviors.
iFeed-
back considers the occurrences of these candidate indica-
tors as metrics to be monitored. Section V.B elaboratesuch a ﬁltering approach.
•Anomaly Detection: A conventional approach to performanomaly detection is to model the problem as a regressionproblem. The measurement values (i.e., the metrics of theindicators) are compared against their prediction valuesin each given time interval of interest. In contrast, wemodel the problem as a classiﬁcation problem, where the
features constructed from the candidate indicators’ his-
torical trends as well as the content information collectedfrom corresponding feedback texts. In this way, previoushuman experiences can be considered as labels to trainthe classiﬁer. As a result, it can be more informative.The anomaly detection scheme, as well as the prerequisitefeature generation approach will be described in SectionV.C.•Issue Clustering: Several different anomalous cases mayreﬂect the same service issue. A summary after anomalydetection is necessary to minimize human inspectionefforts. We ﬁrst take the corresponding feedback texts foreach anomalous case. They are vectorized as auxiliarydata based on the context of the indicators (i.e., the
word phases). A distant-based clustering algorithm is
then designed to group the anomalous cases based onthe vectorized data. Each cluster is then deemed as asummary of a detected issue in the user feedback. Suchan issue clustering process can improve the efﬁciency ofhuman inspection in analyzing potential service issues.Secton V discusses the details.
V. I
MPLEMENTA TION DETAILS
In this section, we will elaborate the iFeedback design,
together with its technical details.
A. Indicator Generation
As illustrated in Section III, user feedback texts are written
by end users on their experiences when using a service. Un-
fortunately, end users are typically not well-trained to provide
tidy, informative feedback. In fact, our ﬁeld experiences show
that there is large amount of irrelevant information in thefeedback texts. For example, we have found a lot of uselessfeedback such as irrelevant descriptions. Some users wouldeven complain about their own life in their feedback. Suchfeedback will result in unnecessary indicators, and misleadthe issue detection task. As generating indicators from userfeedback texts is critical to
iFeedback performance in detecting
issues, we should ﬁrst minimize the negative impact of noisy,useless feedback.
Moreover, the indicators generated from the feedback texts
should help
iFeedback to achieve both high accuracy and wide
coverage towards detecting service issues. In other words, theindicators should contain enough information that can leadto effective issue detection. In addition, they should also becapable of covering issues that are rarely-occurring or newly-appearing. Finally, we should design good metrics for the
indicators that can encode enough information for the issue
detection task.
355
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. Fig. 3. User feedback texts on Wechat-Moments
We illustrate the measures iFeedback takes as follows to
combat those challenges.
1) Preprocessing feedback texts: Feedback texts are ﬁrst
preprocessed similarly as in conventional natural language pro-
cessing. For example, for languages like Chinese which does
not have explicit word segmentation, we perform word seg-
mentation with a state-of-the-art word segmentation approach
[37]. Moreover, stop words are removed. Then iFeedback starts
to extract informative feedback.
2) Word combination-based indicators (WCI): We propose
the Word Combination-based Indicators (WCIs), which build
indicators with a combination of words. Speciﬁcally, for each
feedback text, we combine every two words together as a WCI
after text preprocessing.
WCIs provide more information. For example, the WCI
[“payment”, “fail”] may indicate something goes wrong in
the payment process. [“picture”, “load”] may present an issue
when displaying pictures. [“button”, “cover”] may infer that
the user interface encounters problems. As user feedback are
usually written as short texts, WCI shows highly-recapitulative
indications toward issues. For comparison, a single word
may be a commonly-used one. Its occurrence data in the
user feedback can hardly tell that something is specially
concerned by the users. For example, the word “payment”
may appear a lot in user feedback throughout the runtime of
the service. In contrast, the combination of words may occur
far less frequently, which can greatly help issue detection. For
example, [“payment”, “ﬁngerprint”] occurs far less frequently
than “payment” does. If it appears more often, i.e., in case
that its number of occurrence increases dramatically, we can
easily detect such an anomaly and infer that ﬁngerprint-based
payment process may contain issues.
WCIs can also achieve a wide coverage of issues. We
can build billions of indicators with every combination of
words, we may easily identify and distinguish different sources
of issues. Comparing with topic clustering-based methods,
such a huge amount of ﬁne-grained indicators built from
word combinations are more capable to capture issues in
Fig. 4. Example historical occurrence trend
more details. Figure 3 shows an example, where complaints
like “Videos in Wechat-Moments get stuck” appear with the
overwhelming complaints like “Can’t see pictures in Wechat-
Moments”. Summarizing them with the topic “problems with
Wechat-Moments” is too general and provide less information
for issue detection. But if we use WCIs, we can identify
the ﬁrst issue with combinations like [“videos”, “Wechat-
Moments”] and [“videos”, “stuck”], and the second issue with
the combinations like [“pictures”, “Wechat-Moments”]. As a
result, WCIs can capture both issues.
WCIs can automatically adapt to newly-occurring issues.
Traditional approaches based on static text-analysis models
usually fail to deal with unseen, new issues in feedback
texts, or require extra cost to achieve such adaption ( e.g.,b y
retraining the model). In contrast, WCIs can be automatically
generated from any texts by combining words. A new issue
may automatically result in a new set of WCIs, which can
in turn be monitored. Note that an important concern is the
cold-start problem caused by the lack of historical data for
newly-occurring issues. We will further discuss how we solve
it in Section V-C.
3) Metrics for WCIs: How to quantify WCIs so as to
monitor their values is important to iFeedback . We choose the
occurrence of each WCI among feedback texts as a metric to
describe the WCI. We count the occurrence number within a
certain time period. The results form a historical occurrence
trend (HOT) curve. Figure 4 shows an example.
As we can see, every WCI is corresponding to a speciﬁc
HOT. The HOT curve of a WCI clearly shows its occurrence
numbers in each time interval and their changes. Thus, we can
distinguish dramatic changes from reasonable changes, and
identify the occurrence of an issue. We will discuss how we
extract such features from the HOT, and form feature vectors
for each interval of the service runtime. Such vectorized data
can then be analyzed by our machine learning-based anomaly
detection approach.
With the WCIs and the corresponding HOTs, iFeedback can
successfully transform issue detection problem from a text
analysis problem to an anomaly detection problem in a set of
vectorized data. We will show how this problem can be solved
356
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. effectively with a rapid machine learning-based approach. This
is more practical for large-scale online service systems.
B. Indicator Filtering
According to the indicator generation approach introduced
above, billions of WCIs along with their HOTs can be built
for further anomaly detection. However, though generating
WCIs can be an efﬁcient process since few computation is
needed, applying machine learning-based anomaly detection
on billions of HOTs can be an extremely time-consuming task.
In our ﬁeld experience, we have found that HOTs of most
WCIs are usually stable. Only those with dramatic changes
may indicate potential issues. Therefore, a rapid algorithm to
ﬁlter out those WCIs with stable HOTs can signiﬁcantly reduce
the numbers of WCI candidates to be analyzed by the machine
learning-based anomaly detection, and consequently improve
its efﬁciency.
In particular, we propose the ﬁltering strategy as follows.
R(WCI,t )=⎧
⎨
⎩1,H O T t>δ 1and HOT t>=δ2∗HOT
0,o therwise
WhereR(WCI,t )evaluates whether a WCI should be
retained or not after ﬁltering given a speciﬁc time interval
t. It should be ﬁltered out if R(WCI,t )is 0.
HereHOT tdenotes the occurrence number of the WCI
in the time interval t, wheretis typically one hour in our
practice.HOT denotes the average occurrence number of the
WCI in the past, given the same length of time intervals. We
calculate HOT with the data in the past seven days before t.
We consider the manifest of an issue will result in more
complaints in the user feedback texts. Therefore, we ﬁrst
considerHOT t>δ 1as a condition that an WCI should be of
concern, where δ1is a given threshold. δ1is typically set to
10 based on our experiences if the interval is one hour. Such
a value may vary according to the number of service users.
We also consider HOT t>=δ2∗HOT , so as to capture a
substantial increase of the HOT value. δ2is typically set to
two in our practice.
WithR(WCI,t ),iFeedback then removes most WCIs, and
those remained are considered as suspicious indicators of
potential issues, which we name candidate WCIs. Such a
ﬁltering criterion is quite helpful in cutting down the number
of candidate WCIs for further anomaly detection. In fact,
according to our statistics of our production system, more than
99.9% of WCIs will be ﬁltered out. Our weekly data show that
typically only tens of thousands of WCIs are candidate WCIs,
comparing to billions of WCIs in origin.
Finally, it is worth noting that such a ﬁltering approach is
based on simple calculation. It can be efﬁciently implemented,
which is speciﬁcally suitable for our problem domain since
efﬁciency is our major concern towards handling a large-
scale online service system that serves millions of users
concurrently.C. Feature Extraction and Anomaly Detection
We now have the candidate WCIs and their corresponding
HOTs for anomaly detection. A traditional anomaly detection
approach to deal with such time-series data is to model it
as a regression problem, which can be applied with several
forecasting techniques including ARIMA [38] and LSTM [39].
Unfortunately, such a regression-based method can only lever-
age information from time-series aspect, which turns out to
be quite unstable in our problem scenario. And the additional
content-based features are not considered. Hence, iFeedback
models the anomaly detection problem as a classiﬁcation
problem.
Before we elaborate our classiﬁcation approach, let us
discuss how we conduct proper feature engineering as input
for classiﬁcation task. On the one hand, as the historical
data curve describes the trend of how WCI changes. we
should extract time-series features from the HOTs to describe
the service condition in each time interval (one hour in our
practical implementation). On the other hand, we consider
those feedback corresponding to an anomalous WCI should
share centralized content indicating a certain issue. Hence,
content-based feature on text diversity is another important
component of the vectorized feature.
To better collect features, we propose a long/short-term
sliding windows strategy. The long-term sliding window is
seven days, while the short-term sliding window is 24 hours
in our practical implementation. Given the current hour and
a WCI, it selects the occurrence numbers of the WCI in the
previous 24 hours ( i.e., in the short-term window), as well
as the occurrence numbers of the WCI in the previous seven
days ( i.e., in the long-term window), to construct time-series
features. As the target time interval moves, the two windows
slide and follow, which capture the features of the occurrence
numbers of WCI within them respectively. Figure 5 shows two
examples. In particular, we collect the occurrence numbers of
a WCI among all the feedback texts in every hour within
the short-term window, as well as those within the long-
term window. Moreover, we also take the ratio of occurrence
numbers of WCI to the total number of feedback in the time
window as features.
Similarly, we compute the text diversity via Jaccard distance
on bag of words(BoW) within the same periods marked by
time windows to construct content-based feature. Such feature
checks the unique number of feedback related to a WCI. Our
experiences prove this to be a vital feature in recalling severe
issues, as users tend to give the similar feedback text urgently
when they occur.
We rely on historical human efforts in inspecting whether
an issue exists to label some of such vectorized data. In this
way, previous human efforts can be efﬁciently exploited to
our anomaly detection task. We can then train an XGBoost -
based 2-class classiﬁcation model [40]. The model can then
be applied to classify whether current time interval contains
an issue. For the interval reported as anomalous one, the
corresponding WCIs can be considered as those associated
357
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. (a) HOT and sliding windows of an existing WCI
(b) HOT and sliding windows of a newly-generated WCI
Fig. 5. Long short term time window for existed and new indicators
with potential issues, and in turn presented for further analysis.
Finally, it is worth noting that our sliding windows-based
approach can solve the cold start problem mentioned in Sec-
tion V-A. Figure 5(b) shows the mechanism. When a new WCI
is generated, the monitoring process for the corresponding data
of the WCI starts from the initial state. It simply considers
the historical occurrence numbers before the generation of the
new WCI as zero. A HOT can be formed accordingly, and the
feature vector of current time interval can also be extracted. If
a newly-added WCI exhibits a substantial increase in its HOT
(comparing with the zero values in the historical data), it is
reasonable if it is classiﬁed as an anomaly.
D. Bug Clustering
As discussed in Section V-C, iFeedback can produce a set of
anomalous time intervals with corresponding WCIs and user
feedback texts. Unfortunately, for each issue, there are many
anomalous WCIs iFeedback detects. They are also associated
with tremendous user feedback texts. Although such outputs
can direct human inspection of issues, our experiences show
that each engineer can only inspect and handle a few of such
cases per day. We need to minimize such human efforts.
We notice that typically WCIs may share similar words.
Moreover, similar user feedback texts may describe the same
issue, as shown in Fig 6. Therefore, we propose to cluster the
WCIs and the feedback texts of the anomalous cases, in order
to obtain better summaries of the potential issues.
Again, a proper vectorization approach for the WCIs is
required to facilitate the clustering process, through which
the distance between the corresponding vectors can correctly
measure the similarity between WCIs.
However, the design of such a vectorization approach can be
hard due to the diverse ways of natural language expression
in the user feedback. Moreover, a speciﬁc word may haveWCI: [“QR-code”, “failure”]
Related Feedback 1: I scanned the QR-code while paying,
but a failure message showed up.
Related Feedback 2: My paying by scanning QR-code
resulted in failure!
WCI: [“payment”, “fail”]
Related Feedback 1: I scanned the payment code from my
customers but it failed.
Related Feedback 2: Why it failed all the time when
paying with the payment code?
Fig. 6. Examples of feedback texts that can be clustered
Fig. 7. Example of context-based vectorization of texts, where 1 indicates a
word appears in the context, and 0 otherwise.
different meanings under different scenarios, and should be
embedded as different vectors.
Therefore, we adopt a dynamic vectorization approach,
which is based on the contexts of the WCIs. The underlying
idea is that two words with similar contexts share similar
meanings [41]. We generate the vector by collecting the bag
of words on the context of WCI words, and counting the
occurrence on each word in BoW to form the vector. An
example is shown in Figure 7. Note that such an approach
requires no training process when applying to different text
sets. iFeedback can hence maintain its ability to be adaptive
to monitoring different online services with different forms of
feedback.
We can then apply a hierarchical clustering algorithm [42],
which can automatically decide the number of clusters, on
the generated vectors of the feedback text associated with the
anomalous WCIs. Such clustering is based on cosine distance
to gather the similar vectors. Each cluster of texts are then
considered as those describing the same issue. In this way,
the number of reported issues are below ten for each service,
which is a reasonable amount of workload for one engineer
to efﬁciently perform further inspection.
VI. E XPERIENCES IN APPLYING IFEEDBACK
In this section, we will describe how effective iFeedback
performs in detecting real-life service issues through our real
practical experiences in applying iFeedback to different large-
scale online services.
A. Overview of Field Deployment
iFeedback has been deployed in tens of large-scale online
service systems for ten months. Example systems include a
358
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. (a) Issue detected in Wechat-Reading
 (b) Issue detected in Wechat Ofﬁcial
Accounts
Fig. 8. Examples of reporting issues
social media application (Wechat), a business communica-
tion application (Wechat-Work), an online payment system
(Wechat-Pay), a mini-game hosting platform (Wechat-Games),
a searching engine (Wechat-Search), a reading application
(Wechat-Reading), a music player (QQ-Music), a news reader
(Wechat-News), a browser (QQ-Browser), a video player (QQ-
Video) and a game (Tencent-Royal). A system can has one to
several iFeedback instances, monitoring the user feedback of
different functions of the same system.
Each product line typically serves tens of millions of users,
is maintained by over one hundred developers in general, and
contains tens of online services which use iFeedback to perform
real-time issue detection from user feedback. Such large-scale
production systems are complicated both in its codes and in
how it works to serve a large amount of users. They require
signiﬁcant workload in their daily operations.
On the other hand, the large amount of users can contribute
to a rich source of feedback every day. About two million
feedback text items will be collected by iFeedback daily. The
total historical data volume has reached the scale of hundreds
of GigaBytes (GBs) in its ten-month operation.
iFeedback has a user-friendly frontend to facilitate developers
in processing a reported issue. Figure 8 shows two examples
of the iFeedback mobile-end user interface. We can see that
the detected issue will be reported together with the time
information, the clustered anomalous WCIs, the number of
relevant feedback text items, and the corresponding feedback
texts provided as a list. Meanwhile, a HOT is shown as a red
curve. As a reference, a blue curve, which is the 7-day moving
average of the HOT, is also shown.
In the past ten months, we have witnessed the success of
iFeedback in helping AIOps. We have interviewed many core
engineers from different product lines who rely on iFeedback to
perform issue detection. We found that iFeedback are embraced
by the product lines, which has greatly reduce human efforts
in issue detection and debugging. The success of iFeedback
in diverse services from different product lines shows that it
(a) UI-related Issue 1
 (b) UI-related Issue 2
Fig. 9. Examples for UI-related issue
can well adapt to different types of services. It is suitable for
general large-scale online service systems.
Our statistics show that iFeedback has reported around 380
issues in each week. Among them 26% are those that do not
manifest in the backend system. In other words, they do not
leave distinct traces in backend system monitors. Specially,
for some speciﬁc services such as those in Wechat-Pay, such
a ratio can reach 43%. These results indicate that without
iFeedback , it would be quite labor-intensive, if not infeasible,
to detect such issues. iFeedback can achieve a signiﬁcant im-
provement in detecting issues during system operations. In our
following discussions, we will provide several representative
cases, where iFeedback successfully detect the issues which are
not able to be detected by the backend system monitors.
B. Case 1: Detecting user interface (UI) Defects
UI is critical to user experiences when accessing a service.
UI may encounter problems. For example, contents are not
displayed correctly in some smartphone models. It is hard, if
not impossible, for a backend system monitor to be aware
of such problems, since they typically do not manifest in
the backend system. During our ﬁeld deployment, iFeedback
has successfully found many UI issues from user feedback. It
meanwhile brings informative reports that help debugging.
Figure 9 shows example reports of such issues. In Figure
9(a), the reported issue is on a mistakenly-set font size,
which incurs complaints from several users. With the reports
iFeedback generates, such an issue can be easily located and
ﬁxed. Also, as shown in Figure 9(b), iFeedback reports an
issue, where we can easily ﬁnd that the advertisement content
mistakenly covers a button in a service in Wechat-Games.
We can see that such UI-related issues may severely annoy
the users. Unfortunately, nothing can be found from the
backend system monitors. In such cases, user feedback is the
sole way for developers to be aware of such issues. iFeedback
effectively detects such issues in a timely manner.
359
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. (a) Silent Issue 1
 (b) Silent Issue 2
Fig. 10. Examples for silent issues
C. Case 2: Detecting Silent Backend Issues
In contrast to UI issues that may hardly manifest in backend
systems, some issues are caused by the backend logic, but
are still hard to be detected by the backend system monitor.
In other words, they may have little impact on the indicators
monitored by the backend systems. We name such issues silent
ones. Such issues may deteriorate user experiences. However,
they are particularly difﬁcult to detect without iFeedback . They
do not cause alarms by their corresponding backend system
indicators. As a result, they tend to be neglected by developers.
In contrast, we have found that iFeedback can perform quite
satisfactorily in detecting such silent issues, since users are
good informers of them. Figure 10 shows two examples of
such issues reported by iFeedback .
The ﬁrst reported issue shows that the messages received in
the Android end of Wechat cannot be successfully synchro-
nized to a Windows client. The issue is due to the backend
logic. But it affects only a small portion of the users. Although
there are designed backend system indicators for the relevant
backend logic, the indicators receive no signiﬁcant inﬂuence
to signal an alarm.
The second reported issue indicates users can still receive
notiﬁcations even after they stop following some ofﬁcial
accounts in Wechat. Its cause is that the “stop-following”
instruction is mistakenly intercepted in the backend system.
Although there exists certain backend system indicators for
such interceptions, the indicator records the total amount of
such interceptions, which is unable to detect the issue.
iFeedback , in contrast, can capture user complaints on such
issues. It then provides informative reports, which can instantly
direct human inspection towards bug removal.
D. Case 3: Detecting Newly-occurring Issues
Online services are typically experiencing rapid upgrading
to fulﬁll new user requirements. However, rapid upgrading
may incur new bugs. Unfortunately, developers can hardly
predict possible problems that may encounter in the future.
They may not be able to prepare good backend system
(a) Newly-occurring issue 1
 (b) Newly-occurring issue 2
Fig. 11. Examples for newly-occurring bugs
indicators for all potential problems, especially those caused
by the newly-added functionalities.
Again, users are good informers of such newly-occurring
issues. With iFeedback , developers can conveniently detect
newly-occurring issues reported by the users.
Figure 11 gives two example cases. In Figure 11(a).(a), it
is reported that users cannot participate in the sailing compe-
tition activity in a minigame hosted by Wechat-Games. Such
functionality is newly added. Its corresponding information
has never been collected before ( e.g the word ”sailing” has
never appeared before”). Therefore, this issue can never be
captured using the existing indicators, However, with the
ability of automatically constructing news WCIs, iFeedback can
successfully detect and report such an issue, and helps devel-
opers rapidly locate and ﬁx it. Figure 11(b) shows another
case, where the comments in the Wechat ofﬁcial accounts
are not shown correctly. The issue is actually caused by the
new advertisement policy enforced in the backend logic. It
unexpectedly results in blocking the comments. Again, we
can see that iFeedback can successfully detect and report such
issues with helpful information in debugging.
VII. F URTHER DISCUSSIONS
In our previous section, we present that iFeedback has suc-
cessfully detected service issues that are hard to be found by
traditional backend system monitor-based approaches. Next,
we presented a further evaluation of iFeedback in terms of
its accuracy and effectiveness in issue detection. We then
discuss several possible threats to validity, together with how
we address them. The lessons learnt from our past ten-month
operation experiences are ﬁnally provided.
A. Accuracy and Effectiveness
First, for an issue detection tool, an important consideration
is its detection accuracy. In other words, how many detected
issues are conﬁrmed as true positives( i.ethe precision), and
how many real issues are missed as false negatives( i.e. the
recall). We present some of our statistics. For precision, we
count their numbers from a two-month operations of four
360
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. TABLE I
ACCURACY ON FOUR SERVICES
WechatWechat-
GamesWechat-
ReadingWechat-
WorkTotal
Detected # 71 75 6 37 189
Conﬁrmed # 49 69 6 20 144
Precision 69.0% 92.0% 100.0% 54.1% 76.2%
TABLE II
RECALL ON SEVERE ISSUES
Total Detected # Missed # Recall
Severe issues 44 41 3 93.2%
different product lines. The ratio of the conﬁrmed issues is
the precision. The results are shown in Table I. We can
see iFeedback has reached a satisfactory precision. Note that
the number of the original feedback texts are in millions.
With iFeedback , developers can be directed to analyze a small
number of potential issues, where over 50% (in some product
line, such a ratio can reach over 90%) are true-positive issues.
As for recall, we collect results for severe issues, as they
will be reported eventually if iFeedback fails to detect them.
Results through ten months’ operation are shown in Table II.
Along with such a high recall value, most issues are success-
fully detected in advance of traditional backend monitoring.
Moreover, we also use more training data to improve our
classiﬁer, which can successfully detect the missed three bugs
in postmortem backtesting.
We further study the efﬁciency improvement introduced by
iFeedback in issue detection. Our past practice of debugging
from user feedback without iFeedback is relying heavily on
human efforts. Frequent keywords will be reported by a
feedback management system. Engineers will perform human
inspection of such keywords. They may accordingly search
relevant feedback texts of suspicious keywords in the system.
By reading the texts, they may infer possible issues. In
contrast, iFeedback improves such a process with an automatic
approach. As an example, we focus on a one-week feedback
data set of a product line. We examine how many feedback
texts are ﬁltered in each steps of iFeedback as a reference to
see how iFeedback can reduce possible human efforts. In the
beginning, we have collected 13,295,217 feedback text items
in total, along with 38,444,607 WCIs generated from them.
After the indicator ﬁltering process of iFeedback , there are
82,250 remaining candidate WCIs. After the machine learning-
based anomaly detection, iFeedback has selected 12,595 WCIs
that are considered to be related to system issues. Finally,
the clustering process has eventually produced 379 potential
issues, which means that less than 4 potential issues will be
produced as report for each single application in average. We
can see that iFeedback is able to locate hundreds of issues out
of tens of millions of feedback texts automatically, which can
greatly reduce human efforts in handling the feedback texts.
B. Threat to V alidity
First of all, we design iFeedback for our production service
systems and shows its effectiveness. One may concern thatwhether iFeedback works only for a speciﬁc application sce-
nario. Actually, in our iFeedback design, we aim at diverse,
general production lines. These production lines are all in
nature large-scale online service systems. But they do not share
much similarity: they are developed for different purposes by
different teams. They possess various code logic, implemen-
tation languages, and architectures. iFeedback has shown its
effectiveness in all these product lines, which can prove its
adaptability to different software service systems.
As we have discussed, iFeedback relies on light conﬁgura-
tions ( e.g., the proper lengths of the long/short time windows)
that may be based on service speciﬁcs. Our experiences show
that such efforts are minor. It is worth noting that in iFeedback
design, we focus on a general way in building WCIs from
user feedback text. We rely on the WCIs together with their
features ( e.g., their HOTs) to detect issues instead of using
service-speciﬁc information. It is why iFeedback can preserve
its good adaptability to divers service scenarios.
iFeedback relies heavily on the quality of user feedback. A
second possible threat is whether the feedback that iFeedback
uses is specially with high quality, comparing with other user
feedback in general service systems. Actually, our service
users are Internet service users from the general public. They
are representative users of large-scale online service systems.
The feedback that iFeedback uses is not with relatively high
quality: The feedback texts are with tremendous irrelevant
information and inexperience expressions on user experiences.
We carefully design methods to handle noisy information
generally existed in the feedback texts in iFeedback . Hence,
it can work for feedback from general users of large-scale
online service systems.
VIII. C ONCLUSIONS
This paper discusses our design of iFeedback , a tool to
perform real-time issue detection in large-scale online systems
based on the user feedback texts. We also presents our rep-
resentative experiences in successfully applying iFeedback in
tens of large-scale online service systems for ten months.
It’s a relatively new attempt in AIOps for large-scale online
service systems to detect potential issues in real-time based
solely on user feedback texts. In contrast to the existing work
that considers user feedback as a static text-based dataset,
iFeedback proposes a dynamic, adaptive approach to capture
the historical trends of user feedback. Indicators, in partic-
ular, WCIs, are built to facilitate feature extraction of the
trends. Machine learning-based approaches can then be applied
towards a successful issue detection. Such a prospective to
exploit information in user feedback can shed light to other
tasks based on user experiences. Examples include dynamic
service adaptation to user requirements and user proﬁling.
ACKNOWLEDGEMENT
This work was supported by the National Natural Science
Foundation of China (Project No. 61672164). Yangfan Zhou
is the corresponding author.
361
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] Q. Wang, Y . Brun, and A. Orso, “Behavioral execution comparison: Are
tests representative of ﬁeld behavior?” in Software Testing, V eriﬁcation
and V alidation (ICST), 2017 IEEE International Conference on . IEEE,
2017, pp. 321–332.
[2] A. Tosun, O. Turkgulu, D. Razon, H. Y . Aydemir, and A. Gureller,
“Predicting defects using test execution logs in an industrial setting,”
inSoftware Engineering Companion (ICSE-C), 2017 IEEE/ACM 39th
International Conference on . IEEE, 2017, pp. 294–296.
[3] M. V . M ¨antyl ¨a, B. Adams, F. Khomh, E. Engstr ¨om, and K. Petersen, “On
rapid releases and software testing: a case study and a semi-systematic
literature review,” Empirical Software Engineering , vol. 20, no. 5, pp.
1384–1425, 2015.
[4] H. Mi, H. Wang, Y . Zhou, M. Lyu, and H. Cai, “Toward ﬁne-grained,
unsupervised, scalable performance diagnosis for production cloud com-
puting systems,” IEEE Transactions on Parallel and Distributed Systems ,
vol. 24, pp. 1245–1255, September 2013.
[5] L. Bass, I. Weber, and L. Zhu, DevOps: A Software Architect’s Perspec-
tive. Addison-Wesley Professional, 2015.
[6] D. Pagano and W. Maalej, “User feedback in the appstore: An empirical
study,” in Proc. of the 21st IEEE International Conference on Require-
ments Engineering Conference (RE) , 2013, pp. 125–134.
[7] N. Chen, J. Lin, S. C. Hoi, X. Xiao, and B. Zhang, “AR-Miner: mining
informative reviews for developers from mobile app marketplace,” in
Proc. of the 36th International Conference on Software Engineering
(ICSE) , 2014, pp. 767–778.
[8] F. Palomba, M. L. V asquez, G. Bavota, R. Oliveto, M. D. Penta,
D. Poshyvanyk, and A. D. Lucia, “User reviews matter! tracking
crowdsourced reviews to support evolution of successful apps,” in Proc.
of the IEEE International Conference on Software Maintenance and
Evolution (ICSME) , 2015, pp. 291–300.
[9] L. Villarroel, G. Bavota, B. Russo, R. Oliveto, and M. D. Penta, “Release
planning of mobile apps based on user reviews,” in Proc. of the 38th
International Conference on Software Engineering (ICSE) , May 2016,
pp. 14–24.
[10] C. Gao, J. Zeng, M. R. Lyu, and I. King, “Online app review analysis
for identifying emerging issues,” in Proc. of the 40th International
Conference on Software Engineering (ICSE) , 2018, pp. 48–58.
[11] S. Mohanty and S. Vyas, “It operations and ai,” in How to Compete in
the Age of Artiﬁcial Intelligence . Springer, 2018, pp. 173–187.
[12] B. Dong, Z. Chen, H. Wang, L.-A. Tang, K. Zhang, Y . Lin, Z. Li, and
H. Chen, “Efﬁcient discovery of abnormal event sequences in enterprise
security systems,” in Proc. of the ACM on Conference on Information
and Knowledge Management (CIKM) , 2017, pp. 707–715.
[13] C. Luo, J.-G. Lou, Q. Lin, Q. Fu, R. Ding, D. Zhang, and Z. Wang,
“Correlating events with time series for incident diagnosis,” in Proc. of
the 20th ACM International Conference on Knowledge Discovery and
Data Mining (SIGKDD) , 2014, pp. 1583–1592.
[14] S. Zhang, Y . Liu, W. Meng, Z. Luo, J. Bu, S. Yang, P . Liang, D. Pei,
J. Xu, Y . Zhang, Y . Chen, H. Dong, X. Qu, and L. Song, “PreFix:
Switch failure prediction in datacenter networks,” ACM SIGMETRICS
Performance Evaluation Review , vol. 46, pp. 64–66, January 2019.
[15] M. Du, F. Li, G. Zheng, and V . Srikumar, “DeepLog: Anomaly detection
and diagnosis from system logs through deep learning,” in Proc. of the
ACM Conference on Computer and Communications Security (CCS) ,
2017, pp. 1285–1298.
[16] Q. Lin, H. Zhang, J.-G. Lou, Y . Zhang, and X. Chen, “Log clustering
based problem identiﬁcation for online service systems,” in Proc. of
the 38th International Conference on Software Engineering Companion
(ICSE) , 2016, pp. 102–111.
[17] O. Alipourfard, H. H. Liu, J. Chen, S. V enkataraman, M. Y u, and
M. Zhang, “CherryPick: Adaptively unearthing the best cloud conﬁgu-
rations for big data analytics,” in Proc. of the 14th USENIX Symposium
on Networked Systems Design and Implementation (NSDI) , 2017, pp.
469–482.
[18] A. Di Sorbo, S. Panichella, C. V . Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, “What would users change in my
app? summarizing app reviews for recommending software changes,”
inProc. of the 24th ACM SIGSOFT International Symposium on
F oundations of Software Engineering (FSE) , 2016, pp. 499–510.
[19] P . M. Vu, H. V . Pham, T. T. Nguyen, and T. T. Nguyen, “Phrase-
based extraction of user opinions in mobile app reviews,” in Proc. ofthe 31st IEEE/ACM International Conference on Automated Software
Engineering (ASE) , 2016, pp. 726–731.
[20] B. Fu, J. Lin, L. Li, C. Faloutsos, J. Hong, and N. Sadeh, “Why people
hate your app: Making sense of user feedback in a mobile app store,” in
Proc. of the 19th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD) , 2013, pp. 1276–1284.
[21] M. Raginsky, R. M. Willett, C. Horn, J. Silva, and R. F. Marcia,
“Sequential anomaly detection in the presence of noise and limited
feedback,” IEEE Transactions on Information Theory , vol. 58, pp. 5544–
5562, August 2012.
[22] C. Huang, G. Min, Y . Wu, Y . Ying, K. Pei, and Z. Xiang, “Time series
anomaly detection for trustworthy services in cloud computing systems,”
(to appear) IEEE Transactions on Big Data , pp. 1–1, June 2017.
[23] M. A. Siddiqui, A. Fern, T. G. Dietterich, R. Wright, A. Theriault, and
D. W. Archer, “Feedback-guided anomaly discovery via online optimiza-
tion,” in Proc. of the 24th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD) , 2018, pp. 2200–2209.
[24] M. Ma, S. Zhang, D. Pei, X. Huang, and H. Dai, “Robust and rapid
adaption for concept drift in software system anomaly detection,” in
Proc. of the IEEE 29th International Symposium on Software Reliability
Engineering (ISSRE) , 2018, pp. 13–24.
[25] Z. Li, Y . Zhao, R. Liu, and D. Pei, “Robust and rapid clustering of
kpis for large-scale anomaly detection,” in Proc. of the IEEE/ACM 26th
International Symposium on Quality of Service (IWQoS) , 2018, pp. 1–
10.
[26] Y . Sun, Y . Zhao, Y . Su, D. Liu, X. Nie, Y . Meng, S. Cheng, D. Pei,
S. Zhang, X. Qu, and X. Guo, “Hotspot: Anomaly localization for
additive kpis with multi-dimensional attributes,” IEEE Access , vol. 6,
pp. 10 909–10 923, 2018.
[27] N. Laptev, S. Amizadeh, and I. Flint, “Generic and scalable framework
for automated time-series anomaly detection,” in Proc. of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD) , 2015, pp. 1939–1947.
[28] H. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y . Liu, Y . Zhao,
D. Pei, Y . Feng, J. Chen, Z. Wang, and H. Qiao, “Unsupervised
anomaly detection via variational auto-encoder for seasonal kpis in web
applications,” in Proc. of the World Wide Web Conference (WWW) , 2018,
pp. 187–196.
[29] D. Liu, Y . Zhao, H. Xu, Y . Sun, D. Pei, J. Luo, X. Jing, and
M. Feng, “Opprentice: Towards practical and automatic anomaly detec-
tion through machine learning,” in Proc. of the Internet Measurement
Conference (IMC) , 2015, pp. 211–224.
[30] R. Ding, Q. Wang, Y . Dang, Q. Fu, H. Zhang, and D. Zhang, “Y ADING:
Fast clustering of large-scale time series data,” Proc. VLDB Endow. ,
vol. 8, pp. 473–484, January 2015.
[31] Q. Lin, J.-G. Lou, H. Zhang, and D. Zhang, “iDice: Problem identiﬁca-
tion for emerging issues,” in Proc. of the 38th International Conference
on Software Engineering (ICSE) , 2016, pp. 214–224.
[32] M. Gegick, P . Rotella, and T. Xie, “Identifying security bug reports via
text mining: An industrial case study,” in Proc. of the 7th IEEE Working
Conference on Mining Software Repositories (MSR) , 2010, pp. 11–20.
[33] T. Menzies and A. Marcus, “Automated severity assessment of software
defect reports,” in Proc. of the IEEE International Conference on
Software Maintenance , 2008, pp. 346–355.
[34] Y . Zhou, Y . Tong, R. Gu, and H. Gall, “Combining text mining and data
mining for bug report classiﬁcation,” J. Softw. Evol. Process , vol. 28,
pp. 150–176, March 2016.
[35] G. Antoniol, K. Ayari, M. D. Penta, F. Khomh, and Y .-G. Guhneuc, “Is
it a bug or an enhancement?: A text-based approach to classify change
requests,” in Proc. of the Conference of the Center for Advanced Studies
on Collaborative Research: Meeting of Minds (CASCON) , 2008, pp.
304–318.
[36] A. Lamkanﬁ, S. Demeyer, E. Giger, and B. Goethals, “Predicting the
severity of a reported bug,” in Proc. of the 7th IEEE Working Conference
on Mining Software Repositories (MSR) ,2010, pp. 1–10.
[37] J. Xu and X. Sun, “Dependency-based gated recursive neural network
for chinese word segmentation,” in Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL) , 2016, pp. 567–572.
[38] J. Contreras, R. Espinola, F. J. Nogales, and A. J. Conejo, “Arima
models to predict next-day electricity prices,” IEEE transactions on
power systems , vol. 18, no. 3, pp. 1014–1020, 2003.
[39] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation , vol. 9, no. 8, pp. 1735–1780, 1997.
362
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. [40] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting system,”
inProceedings of the 22nd acm sigkdd international conference on
knowledge discovery and data mining . ACM, 2016, pp. 785–794.
[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efﬁcient Estimation of
Word Representations in V ector Space . https://arxiv.org/abs/1301.3781,2013.
[42] L. Rokach and O. Maimon, Data Mining and Knowledge Discovery
Handbook . Springer US, 2005.
363
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:03 UTC from IEEE Xplore.  Restrictions apply. 