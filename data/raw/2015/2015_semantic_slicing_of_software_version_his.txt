1
Semantic Slicing of Software Version Histories
Yi Li, Chenguang Zhu, Julia Rubin, Member, IEEE , and Marsha Chechik, Member, IEEE
Abstract —Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug ﬁx, from
one branch of a conﬁguration management system to another. That can be a challenging task as the existing conﬁguration management
tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either
manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of
the change history, “inheriting” additional, unwanted functionality.
In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing
a particular functionality, which is deﬁned by a set of tests. We formally deﬁne the semantic slicing problem, provide an algorithm for
identifying a set of commits that constitute a slice, and propose techniques to minimize the produced slice. We then instantiate the overall
approach, CS LICER , in a speciﬁc implementation for Java projects managed in Git and evaluate its correctness and effectiveness on a set
of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of
interest but are substantially smaller than the original ones.
Index Terms —Software changes, version control, dependency, program analysis.
✦
1 I NTRODUCTION
REAL software is seldom created “all at once”, and
changes are inevitable [2]. Software development is
typically an incremental and iterative process where many
program versions are created, each evolving and improving
the previous ones. For example, new requirements, bugs
and errors emerge during use and developers can take
advantage of the knowledge and insights they gain to repair,
enhance and optimize earlier versions of the system through
incremental updates. This makes version history a crucial
artifact in the software development process.
Software conﬁguration management systems (SCM), such
as Git [3], SVN [4] and Mercurial [5], are commonly used
for hosting software development artifacts. They allow the
developers to periodically submit their ongoing work, storing
it as an increment over previous version. Such an increment
is usually referred to as a commit (Git and SVN) or a change
set(Mercurial), and we use these two terms interchangeably.
Commits are stored sequentially and ordered by their time
stamps, so that it is convenient to trace back to any version
in the history. Branching is another construct provided by
most modern SCM systems. Branches are used, for example,
to store a still-in-development prototype version of a project
or to store multiple project variants targeting different
customers.
However, the sequential organization of changes is inﬂex-
ible and lacks support for many tasks that require high-level,
semantic understanding of program functionality [6], [7].
For example, developers often need to locate and transfer
•Y. Li, C. Zhu, and M. Chechik are with the Department of Computer
Science, University of Toronto, Toronto, ON, Canada, M5S3G4.
E-mail:{liyi, czhu, chechik }@cs.toronto.edu
•J. Rubin is with the Department of Electrical and Computer Engineering,
University of British Columbia, Vancouver, BC, V6T1Z4.
E-mail: mjulia@ece.ubc.ca
This article extends and improves the results presented in [1]. It
contains novel techniques for history slice minimization, additional
data in the experimental results and a more detailed review of the
related work.functionality, either for porting bug ﬁxes between branches
or for propagating features from development to release
branches [8].
Several SCM systems provide mechanism of “replaying”
commits on a different branch, e.g., the cherry-pick
command in Git. Yet, little support is provided for matching
high-level functionality with commits that implement it:
SCM systems only keep track of temporal and text-level
dependencies between the managed commits. The job of
identifying the exact set of commits implementing the
functionality of interest is left to the developers.
Even in very disciplined projects, when such commits
can be identiﬁed by browsing their associated log messages,
the functionality of interest might depend on earlier commits
in the same branch. To ensure correct execution of the
desired functionality, all change dependencies have to be
identiﬁed and migrated to the new branch as well, which
is a tedious and error-prone manual task [9]. For example,
consider the feature “make Groovy method blacklist truly
append-only”, introduced in version 1.3.8 of the Elasticsearch
project [10] – a real-time distributed data search and analytics
framework written in Java. This feature and its corresponding
test case are implemented in a single commit (# 647327f4 ).
Yet, propagating this commit to a different branch will fail
because one of the added statements makes use of a ﬁeld
whose declaration was introduced in an earlier commit
(#64d8e2ae ).
Including unwanted functionality and unnecessary com-
mits in patches is often considered to be bad practice. For
instance, most of the software projects implement strict
guidelines of accepting only small and focused patches [11],
[12], [13]. The main rationale behind these guidelines is
to keep changes that have a different purpose separate,
which leads to a speed up in the code review process,
fewer merge conﬂicts, and easier future maintenance. For
example, the Bitcoin Core [11] contributor guideline stresses
the importance of simplicity of pull requests:
“Patchsets should always be focused. For example, a pull2
request could add a feature, ﬁx a bug, or refactor code;
but not a mixture. Please also avoid super pull requests
which attempt to do too much, are overly large, or overly
complex as this makes review difﬁcult.”
In this paper, we look at the problem of identifying the
exact minimal subset of history that implements a particular
functionality of interest. Inspired by the concept of program
slicing [14], we refer to this subset of semantically-related
commits as a semantics-preserving slice . We assume that a
functionality is deﬁned by a set of tests exercising it. We
propose a system CS LICER , which enhances the support
provided by current SCM tools by mapping high-level
functionalities to low-level commits.
CSLICER has two main phases: semantic slicing and slice
minimization . The ﬁrst phase consists of a generic history
slicing algorithm which is independent of any speciﬁc SCM
system in use, and an SCM adaptation component that
adapts the output produced by the slicing algorithm to
speciﬁcs of SCM systems. The slicing algorithm relies on
static and dynamic program analysis techniques to conser-
vatively identify all atomic changes in the given history that
contribute to the functional and compilation correctness of the
functionality of interest. The SCM adaptation component
then maps the collected set of atomic changes back to the
commits in the original change history. It also takes care of
merge conﬂicts that can occur when cherry-picking commits
in text-based SCM systems, e.g., SVN or Git. This step can
optionally be skipped when using language-aware merging
tools [15] or in semantic-based SCM systems [16]. However,
such systems are not dominant in practice yet.
The generic semantic slicing algorithm is conservative
and can be imprecise. The second phase of CS LICER miti-
gates the imprecision and improves the quality of history
slices through slice minimization. We investigate various
sources of imprecision that commonly appear in practice
and design several techniques to detect and remove the false
positives. We ﬁrst use a light-weight screening technique
to ﬁlter out changes that are less likely to affect the target
functionality according to heuristics. Then we enumerate all
possible combinations of the remaining commits and ﬁnd
a minimal subset of the original history which preserves
the functionality of interest. Empirical results show that
our proposed slice minimization techniques can effectively
improve the solution quality of CS LICER .
The use case of the CS LICER system is not limited to func-
tionality porting; it can also be used for refactoring existing
branches, e.g., by splitting them into functionality-related
ones. We instantiate CS LICER for Java projects hosted in
Git. To empirically evaluate the effectiveness and scalability
of our approach, we experiment with a set of open source
software projects. The results show that our approach can
identify functionality-relevant subsets of original histories
that (a) correctly capture the functionality of interest while
being (b) minimal in many cases or (c) substantially smaller
than the original ones.
Contributions. In our prior work [1], we presented an
algorithm which computes an over-approximated semantic
history slice and evaluated the prototype implementation on
a few subjects subjects. In this paper, we propose novel slice
minimization techniques on top of the original algorithmto improve quality of the computed semantic slices. We
also extend and restructure the empirical studies to better
evaluate the effectiveness of our approach. We summarize
the contributions as follows.
•We formally deﬁne the semantic slicing problem for soft-
ware version histories and propose a generic semantic
slicing algorithm that is independent of underlying SCM
infrastructures and tools.
•We extend the generic algorithm to bridge the gap
between language semantic entities and text-based
modiﬁcations, thus making it applicable to existing text-
based SCM systems.
•We propose a number of heuristic-based techniques
which can effectively improve slice quality by reducing
false positives and minimizing history slices.
•We instantiate the overall approach, CS LICER , by provid-
ing a fully automated semantic slicing tool applicable for
Git projects implemented in Java. The source code of the
tool, as well as binaries and examples used in this paper,
are available at https://bitbucket.org/liyistc/gitslice.
•We evaluate the tool on a number of real-world medium-
to large-scale software projects. We compare our two-
phase minimization technique with the state-of-art
– delta debugging [17]. Our experiments show that
CSLICER is able to correctly identify functionality-
relevant minimal subsets of change histories more
efﬁciently.
Organization. The rest of the paper is organized as follows.
We start with a simple example in Section 2, illustrating
CSLICER . It is followed by necessary background and
deﬁnitions in Section 3. In Section 4, we formalize the
semantic slicing algorithm and prove its correctness. In
Section 5, we deﬁne the notion of minimal history slice
and propose several techniques that can improve quality of
semantic slices. In Section 6, we describe the implementation
details and optimizations. In Section 7, we report on our case
studies and empirical ﬁndings. Finally, in Section 8 and 9,
we compare CS LICER with related work and conclude the
paper, respectively.
2 CS LICER BY EXAMPLE
In this section, we illustrate CS LICER on a simple schematic
example inspired by the feature migration case in the
Elasticsearch project [10]. Figure 1 shows a fragment of
the change history between versions v1.0 and v1.1 for the
ﬁleFoo.java . Initially, as shown in version v1.0, the ﬁle
contains two classes, AandB, each having a member method
gandf, respectively.
Later, in change set C1, a line with a textual comment was
inserted right before the declaration of method A.g. Then, in
change set C2, the body of B.f was modiﬁed from {return
x+1;}to{return x-1;}. In change set C3, the body of
A.g was updated to return the value of a newly added ﬁeld
yin classB. In change set C4, a ﬁeld declaration was inserted
in classAand, ﬁnally, in change set C5, a new method hwas
added to class A. The resulting program in v1.1 is shown in
Figure 2 on the left.
Each dashed box in Figure 1 encloses a commit written
in the uniﬁed format (the output of command diff -u ). The3
v1.1
 class A {
+  // hunk deps
   int g()
   {return 0;} class B {
   static int f(int x) {
-  {return x + 1;}
+  {return x - 1;}
 }   // hunk deps
   int g() {
-  {return 0;}
+  {return (new B()).y;}
 }
 class B {
+  int y = 0;
   static int f(int x)
   {return x - 1;} class A {
+  int x;
   // hunk deps
   int g() class A {
   int x;
+  int h()
+  {return B.f(x);}
   // hunk deps
   int g()C5
C4
C3
C2
C11  class  A {
2    int g()
3    {return  0;}
4  }
5  class B {
6    static int f(int x)
7    {return x + 1;}
8  }v1.0
Fig. 1. Change history of Foo.java .
1 c l a s s A{
2 i n t x ;
3 i n t h ( )
4{return B . f ( x ) ; }
5 // hunk deps
6 i n t g ( )
7{return (new B ( ) ) . y ; }
8}
9 c l a s s B{
10 i n t y = 0 ;
11 s t a t i c i n t f (i n t x )
12{return x−1;}
13}c l a s s A{
i n t x ;
i n t h ( )
{return B . f ( x ) ; }
// hunk deps
i n t g ( )
{return 0;}
}
c l a s s B{
s t a t i c i n t f (i n t x )
{return x−1;}
}
Fig. 2.Foo.java before and after semantic slicing.
lines starting with “ +” are inserted while those starting with
“-” are deleted. Each bundle of changed lines is called a
hunk and comes with a context – a certain number of lines of
surrounding text that stay unchanged. In Figure 1, these are
the lines which do not start with “ +” or “-”. The context that
comes with a hunk is useful for ensuring that the change is
applied at the correct location even when the line numbers
change. A conﬂict is reported if the context cannot be matched.
In the current example, the maximum length of the contexts
is four lines: up to two lines before and after each change.
Suppose the functionality of interest is that the method
A.h()returns “−1”. This functionality was introduced in
C5and now needs to be back-ported to v1.0. Simply cherry-
pickingC5would result in failure because (1) the body of
methodB.f was changed in C2and the change is required
to produce the correct result; (2) the declaration of ﬁeld A.x
was introduced in C4but was missing in v1.0, which would
cause compilation errors; and (3) a merge conﬂict would
arise due to the missing context of C5– the text that appears
immediately after the change. This text was introduced in
C1.
In fact, the change histories form a dependency hierarchyTextual ContextsCompilation
SetFunctional
SetCorrectness
Well-formedness
ApplicabilityC1Examples
C4C2 , C5
HunkCompilationFunctionalDependency
Types
Fig. 3. Change dependency hierarchy.
with respect to the target functionality (see Figure 3). At
its core, the functional set contains program components
which directly participate in the test execution to deliver the
target functionality, e.g., methods A.h andB.f. To start with,
CSLICER examines the history and identiﬁes functional depen-
dencies that are essential for the semantic correctness of the
functional set, e.g., C2,C5. In addition, CS LICER computes
thecompilation set which connects the functional core with its
structural supporting components, i.e., classes A,Band the
ﬁeld declaration int x inA. Similarly, the corresponding
contributing changes are called the compilation dependencies ,
e.g.,C4. They are necessary to guarantee program well-
formedness including syntactic correctness and type safety.
Finally, to ensure the selected changes can be applied using
a text-based SCM system, some additional changes which
provide textual contexts should be included as well. We call
these changes the hunk dependencies , e.g.,C1. In the semantic
slicing phase, our proposed algorithms compute a set of
commits that are required for porting the functionality of
interest to v1.0 successfully: {C1,C2,C4,C5}. This process
is formalized in Section 4.
Then in the slice minimization phase, CS LICER attempts
to reduce from the identiﬁed commits and in this particular
case fails since the solution is already optimal – there is no
smaller set of commits that can preserve all of the desired
properties. We investigate more complex cases in Section 5.
Applying the set of commits in sequence on top of v1.0
produces a new program shown in Figure 2 on the right.
It is easy to verify that the call to A.h in both programs
returns the same value. Changes introduced in commit C3–
an addition of the ﬁeld B.y and a modiﬁcation of the method
A.g – do not affect the test results and are not part of any
other commit context. Thus, this commit can be omitted.
3 B ACKGROUND
In this section, we provide the background needed in the rest
of the paper.
3.1 Language Syntax
To keep the presentation of our algorithm concise, we step
back from the complexities of the full Java language and
concentrate on its core object-oriented features. We adopt a
simple functional subset of Java from Featherweight Java [18],
denoting it by P. The syntax rules of the language P
are given in Figure 4. Many advanced Java features, e.g.,
interfaces, abstract classes and reﬂection are stripped from P,
while the typing rules which are crucial for the compilation4
P::=L
L::=classCextends C{C f;KM}
K::=C(C f){super(f);this.f=f;}
M::=C m(C x){return e;}
e::=x|e.f|e.m(e) | new C(e) | (C)e
Fig. 4. Language syntax rules [18].
C <:CC <:D D <:E
C <:EclassCextends D{...}
C <:D
Fig. 5. Subtyping rules [18].
correctness are retained [19]. We discuss additional language
features in Section 7.
We say that pis asyntactically valid program of language P,
denoted by p∈P, ifpfollows the syntax rules. A program
p∈Pconsists of a list of class declarations ( L), where
the overhead bar Lstands for a (possibly empty) sequence
L1,...,L n. We use/a\}bracketle{t/a\}bracketri}htto denote an empty sequence and
comma for sequence concatenation. We use |L|to denote
the length of the sequence. Every class declaration has
members including ﬁelds (C f),methods (M) and constructors
(K). A method body consists of a single return statement;
the returned expression can be a variable, a ﬁeld access, a
method lookup, an instance creation or a type cast.
The subtyping rules of P, shown in Figure 5, are straight-
forward. We write C <:Dwhen class Cis a subtype of
D. As in full Java, subtyping is the reﬂexive and transitive
closure of the immediate subclass relation implied by the
extends keyword. The ﬁeld and method lookup rules are
slightly different from the standard ones (see Figure 6) – ﬁeld
overshadowing and method overloading are not allowed while
method overriding is allowed in Featherweight Java [18]. For
example, when resolving a method call C.m , the method
listMof classCis ﬁrst consulted. If mis deﬁned in C
then its type and body are returned as a pair (B→B,x.e).
Otherwise, the lookup continues recursively on the super
class ofC.
3.2 Abstract Syntax Trees
A valid program p∈Pcan be parsed as an abstract syntax tree
(AST), denoted by A ST(p). We adopt a simpliﬁed AST model
where the smallest entity nodes are ﬁelds and methods.
Formally, r=AST(p)is a rooted tree with a set of nodes
V(r). The root of ris denoted by ROOT(r)which represents
the compilation unit, i.e., the program p. Each entity node
xhas an identiﬁer and a value, denoted by id(x)andν(x),
respectively. In a valid AST, the identiﬁer for each node is
unique (e.g., fully qualiﬁed names in Java) and the values
are canonical textual representations of the corresponding
entities. We denote the parent of a node xby P ARENT(x).
For example, Figure 7 shows two ASTs for the program
Foo.java before and after the change set C3is applied. In
the left AST, the following facts are true about the node f,
id(f) =“foo.B.f(int) ”,
ν(f) =“static int f(int x) {return x-1; }”,
PARENT(f) =B.FIELDS(Object) =/a\}bracketle{t/a\}bracketri}ht
classCextends D{C f;KM}FIELDS(D) =D g
FIELDS(C) =D g,C f
classCextends D{C f;KM}B m(B x){return e;}∈M
METHODS(m,C) = (B→B,x.e)
classCextends D{C f;KM}m /∈M
METHODS(m,C) =METHODS(m,D)
Fig. 6. Fields and methods lookup [18].
foo
B A
y:int g() f(int)foo
B A
g() f(int)INS(y:int, B)
UPD(A.g)
C3
Fig. 7. Visualize C3as a sequence of atomic changes applied on ASTs.
The children are unordered – the ordering of child nodes is
insigniﬁcant. Therefore, each program has its unique AST
representation.
3.3 Changes and Change Histories
LetΓbe the set of all ASTs. Now we deﬁne what change,
change set and change history as AST transformation opera-
tions.
Deﬁnition 1. (Atomic Change). An atomic change operation
δ: Γ→Γis either an insert ,delete orupdate (see Figure 8). It
transforms r∈Γproducing a new AST r′such that r′=δ(r).
An insertion INS((x,n,v),y)inserts a node xwith iden-
tiﬁernand value vas a child of node y. A deletion DEL(x)
removes node xfrom the AST. An update UPD(x,v)replaces
the value of node xwithv. A change operation is applicable
on an AST if its preconditions are met. For example, the
insertion INS((x,n,v),y)is applicable on rif and only if
y∈V(r). Insertion of an existing node is treated the same
as an update.
Deﬁnition 2. (Change Set). Let randr′be two ASTs. A change
set∆ : Γ→Γis a sequence of atomic changes /a\}bracketle{tδ1,...,δ n/a\}bracketri}htsuch
that∆(r) = (δn◦···◦δ1)(r) =r′, where◦is standard function
composition.
A change set ∆ = ∆ −1◦δ1is applicable to rifδ1is
applicable to rand∆−1is applicable to δ1(r). Change sets
between two ASTs can be computed by tree differencing
algorithms [21]. For instance, in Figure 7, C3consists of an
insertion of a new node ytoBfollowed by an update of the
nodeg.
Deﬁnition 3. (Change History). A history of changes is a
sequence of change sets, i.e., H=/a\}bracketle{t∆1,...,∆k/a\}bracketri}ht.
Deﬁnition 4. (Sub-history). A sub-history is a sub-sequence of a
history, i.e., a sequence derived by removing change sets from H
without altering the ordering.5
y∈V(r)INS((x,n,v),y)V(r′)←V(r)∪{x}PARENT(x)←y
id(x)←n ν(x)←v
x∈V(r)DEL(x)V(r′)←V(r)\{x}x∈V(r)UPD(x,v)ν(x)←v
Fig. 8. Types of atomic changes [20].
We write H′⊳Hindicating H′is a sub-history of Hand
refer to/a\}bracketle{t∆i,...,∆j/a\}bracketri}htasHi..j. The applicability of a history
is deﬁned similar to that of change sets.
3.4 Test Cases
We assume that semantic functionalities can be captured by
test cases and the execution trace of a test case is deterministic.
For simplicity, a test case can be abstracted into two parts –
the setup code which initializes the testing environment and
executes the target functionalities using speciﬁc inputs, as
well as the oracle checks which verify that the produced
results match with the expected ones. A test execution
succeeds if all checks pass.
Deﬁnition 5. (Test Case). A test case tis a function t:P→B
such that for a given program p∈P,t(p)is true if and only if the
test succeeds, and false otherwise.
A test suite is a collection of unit tests that can exercise
and demonstrate the functionality of interest. Let test suite
Tbe a set of test cases {ti}. We write p|=Tif and only if
program ppasses all tests in T, i.e.,∀t∈T·t(p).
4 CS LICER PHASE 1: S EMANTIC SLICING
In this section, we deﬁne the semantic slicing problem and
present our slicing algorithms in detail.
4.1 Overview of the Workﬂow
We start with the formal problem deﬁnition followed by a
high level overview of our approach.
4.1.1 Problem Deﬁnition
Deﬁnition 6. (Semantics-preserving Slice). Consider a program
p0and itsksubsequent versions p1,...,p ksuch that pi∈P
andpiis well-typed for all integers 0≤i≤k. LetHbe the
change history from p0topk, i.e.,H1..i(p0) =pifor all integers
0≤i≤k. LetTbe a set of tests passed by pk, i.e.,pk|=T.
A semantics-preserving slice of history Hwith respect to Tis a
sub-history H′⊳Hsuch that the following properties hold:
1)H′(p0)∈P,
2)H′(p0)is well-typed,
3)H′(p0)|=T.
Our goal is to (conservatively) identify such a semantics-
preserving slice , or sometimes referred to as semantic slice for
short. A trivial but uninteresting solution to this problem is
the original history Hitself. Shorter slicing results are pre-
ferred over longer ones, and the optimal slice is the shortest
sub-history that satisﬁes the above properties. However, the
optimality of the sliced history cannot always be guaranteed
by polynomial-time algorithms. Since the test case can be         …H
p0 pkT
t1 … tm
Compute 
Functional SetCompute 
Compilation set
AST Diff
pi pi-1SlicingΛ Π
∆i
∆1’, …, ∆k’H’ ∆i’SCM 
Adaptation
H’Hunk
Dependencies
History Slice
Fig. 9. High-level over view of the semantic slicing algorithms.
arbitrary, it is not hard to see that for any program and history,
there always exists a worst case input test that requires
enumerating all 2ksub-histories to ﬁnd the shortest one. The
naive approach of enumerating sub-histories is not feasible
as the compilation and running time of each version can be
substantial. Even if a compile and test run takes just one
minute, enumerating and building all sub-histories of only
twenty commits would take approximately two years. In fact,
it can be shown that the optimal semantic slicing problem is
NP-complete by reduction from the set cover problem. We
omit the details of this argument here.
To address this problem, we devise an efﬁcient algorithm
which requires only a one-time effort for compilation and
test execution, but may produce sub-optimal results. An
optimal algorithm which runs the test only once cannot
exist in any case: in order to determine whether to keep a
change set or not, it needs to at least be able to answer the
decision problem, “given a ﬁxed program pand test t, for
any arbitrary program p′, will the outputs of tbe different
on both?” which is known to be undecidable [22].
4.1.2 Workﬂow
Figure 9 illustrates the high-level workﬂow of the semantic
slicing algorithms. First, the functional set ( Λ) and compi-
lation set ( Π) are computed based on the latest version pk
and the input tests T. The original version history His then
distilled as a sequence of change sets /a\}bracketle{t∆1,...,∆k/a\}bracketri}htthrough
AST differencing. This step removes cosmetic changes (e.g.,
formatting, annotations, and comments) and only keeps
in∆iatomic changes over code entities. Each such set
∆ithen goes through the core slicer component which
decides whether to keep a particular atomic change or not.
This component outputs a sliced change set ∆′
i, which is
a subsequence of ∆i. Finally, the sliced change sets are
concatenated and returned as a sub-history H′. Optionally,
a post-processing step (SCM Adaption) of H′is needed if
the sliced history is to be applied using text-based SCM
systems. Below we describe each step in turn, illustrating
them through the running example presented in Section 2.
Step 1: Computing Functional Set. CSLICER executes the
test on the latest version of the program (left-hand side
of Figure 2), which triggers method A.h. It dynamically
collects the program statements traversed by this execution.6
These include the method bodies of A.h andB.f. The set of
source code entities (e.g., methods or classes) containing the
traversed statements is called the functional set , denoted by
Λ. The functional set in the current example is {A.h,B.f}.
Intuitively, if (a) the code entities in the functional set and
(b) the execution traces in the program after slicing remain
unchanged, then the test results will be preserved. Special
attention has to be paid to any class hierarchy and method
lookup changes that might alter the execution traces, as
discussed in more detail later.
Step 2: Computing Compilation Set. To avoid causing any
compilation errors in the slicing process, we also need to
ensure that all code entities referenced by the functional set
are deﬁned even if they are not traversed by the tests. To-
wards this end, CS LICER statically analyzes all the reference
relations based on pkand transitively includes all referenced
entities in the compilation set , denoted by Π. The compilation
set in our case is {A,A.x,B}. Notice that the classes Aand
Bare included as well since the ﬁelds and methods require
their enclosing classes to be present.
Step 3: Change Set Slicing. In the change set slicing stage,
CSLICER iterates backwards from the newest change set ∆k
to the oldest one ∆1, collecting changes that are required to
preserve the “behavior” of the functional and compilation
set elements. Each change is divided into a set of atomic
changes (see Deﬁnition 1). Having computed the functional
and compilation set (highlighted in Figure 2), CS LICER then
goes through each atomic change and decides whether it
should be kept in the sliced history ( H′) based on the entities
changed and their change types. In our example, C2and
C5are kept in H′since all atomic changes introduced by
these commits – B.f andA.h – are in the functional set. C4
contains an insertion of A.b which is in the compilation set.
Hence, this change is also kept in H′.C3can be ignored
since the changed entities are not in either set.
During the slicing process, CS LICER ensures that all
entities in the compilation set are present in the sliced
program, albeit their deﬁnitions may not be the most updated
version. Because the entities in the compilation set are not
traversed by the tests, differences in their deﬁnitions do not
affect the test results.
Optional: SCM Adaptation. In the SCM adaptation phase,
change sets in H′are mapped back to the original commits.
As some commits may contain atomic changes that sliced
away by the core slicing algorithm, including these commits
in full can introduce unwanted side-effects and result in
wrong execution of the sliced program. We eliminate such
side-effects by reverting unwanted changes. That is, we
automatically create an additional commit that reverts the
corresponding code entities back to their original state. In
addition, we compute hunk dependencies of all included
commits and add them to the ﬁnal result as well. For example,
the comment line added in C1forms a context for C5.
Therefore, C1is required in the sliced history to avoid merge
conﬂicts when cherry-picking C5. The details of this process
are discussed in Section 4.3.
4.2 Semantic Slicing Algorithm
Now we present in detail the semantic slicing algorithm
which is independent from the underlying SCM systemsRequire:|H|>0∧H(p0)∈P∧H(p0)|=T
Ensure:H′⊆H∧H′(p0)∈P∧H′(p0)|=T
1:procedure SEMANTIC SLICE (p0,H,T )
2:H′,k←/a\}bracketle{t/a\}bracketri}ht,|H| ⊲initialization
3:pk←H(p0) ⊲ pkis the latest version
4:Λ←FUNC DEP(pk,T) ⊲functional set
5:Π←COMP DEP(pk,Λ) ⊲compilation set
6: fori∈[k,1]do ⊲iterate backwards
7: ∆′
i←/a\}bracketle{t/a\}bracketri}ht ⊲initialize sliced change set
8: forδ∈∆ido
9: if¬LOOKUP(δ,H1..i(p0))then⊲keep lookup
10: ifδis D EL∨id(δ)/∈Πthen
11: continue ⊲skip non-comp and deletes
12: ifδis U PD∧id(δ)/∈Λthen
13: continue ⊲skip non-test updates
14: ∆′
i←∆′
i,δ ⊲ concatenate the rest
15: end for
16: H′←H′,∆′
i ⊲growH′
17: end for
18: returnH′
19:end procedure
Fig. 10. Algorithm 1: the semantic slicing algorithm.
and it follows essentially the workﬂow depicted in Figure 9.
The optional SCM adaptation phase will be discussed in
Section 4.3.
4.2.1 Algorithm 1
The main S EMANTIC SLICE procedure is shown in Figure 10.
It takes in the base version p0, the original history H=
/a\}bracketle{t∆1,...,∆k/a\}bracketri}htand a set of test cases Tas the input. Then
it computes the functional and compilation set ΛandΠ,
respectively (Lines 4 and 5).
FUNC DEP(pk,T).Based on the execution traces of running
Tonpk, the procedure F UNC DEPreturns the set of code
entities (AST nodes) traversed by the test execution. This set
(Λ) includes all ﬁelds explicitly initialized during declaration
and all methods (and constructors) called during runtime.
COMP DEP(pk,Λ).The procedure C OMP DEPanalyzes refer-
ence relations in pkand includes all referenced code entities
ofΛinto the compilation set Π. We borrow the set of rules
for computing Πfrom K ¨astner and Apel [19], where the
authors formally prove that their rules are complete and
ensure that no reference without a target is ever present in a
program. Applying these rules, which are given in Figure 11
and described below, allows us to guarantee type safety of
the sliced program.
L1 a class can only extends a class that is present;
L2 a ﬁeld can only have type of a class that is present;
K1 a constructor can only have parameter types of classes
that are present and access to ﬁelds that are present;
M1 a method declaration can only have return type and
parameter types of classes that are present;
E1 a ﬁeld access can only access ﬁelds that are present;
E2 a method invocation can only invoke methods that
are present;
E3 an instance creation can only create objects from
classes that are present;
E4 a cast operation can only cast an expression to a class
that is present;7
C <:D C∈Π[L.1]D∈Πf:C∈Π[L.2]C∈ΠC(D f){super(f);this.f=f;}∈Π[K1]
C∈ΠD∈Πf∈Π
C m(D x){return e;}∈Π[M1]
C∈ΠD∈Π...{return e.f;}∈Π[E1]f∈Π...{return e.m(e);}∈Π[E2]m∈Π
...{return new C(e);}∈Π[E3]C∈Π...{return (C)e;}∈Π[E4]C∈Πx∈Π[P1]PARENT(x)∈Πx∈Λ[T1]x∈Π
Fig. 11. C OMPDEPreference relation rules.
P1 an entity is only present when the enclosing entities
are present;
T1 an entity is in the compilation set if it is in the
functional set.
We iterate backwards through all the change sets in the
history (Lines 6-17) and examine each atomic change in the
change set. An atomic change δis included into the sliced
history if it is an insertion or an update to the functional
set entities, or an insertion of the compilation set entities.
Updates to the compilation set entities are ignored since they
generally do not affect the test results.
Our language Pdoes not allow method overloading
or ﬁeld overshadowing, which limits the effects of class
hierarchy changes. Exceptions are changes to subtyping
relations or casts which might alter method lookup (Line 9).
Therefore we deﬁne function L OOKUP to capture such
changes,
LOOKUP(δ,p)/defines∃m,C·
METHODS(m,C)/\e}atio\slash=METHODS′(m,C),
where METHODS and METHODS′are the method lookup
function for pandδ(p), respectively. Finally, the sliced history
H′is returned at Line 18.
4.2.2 Correctness of Algorithm 1
Assume that every intermediate version of the program pis
syntactically valid and well-typed. We show that the sliced
program p′produced by the S EMANTIC SLICE procedure
maintains such properties.
Lemma 1. (Syntactic Correctness). H′(p0)∈P.
Proof. From the assumption, every intermediate version
p0,...,p kis syntactically valid. As a result, their ASTs are
well-deﬁned and every change operation δ∈His applicable
given all preceding changes. Updates on tree nodes do not
affect the tree structure and, therefore, do not have effect
on the preconditions of the changes. We can safely ignore
updates when considering syntactic correctness.
We prove the lemma by induction on the loop counter i.
The base case is when i=kandH′=/a\}bracketle{t/a\}bracketri}ht. By deﬁnition,
H′(pk) =pkis inP. Assume that H′◦H1..i(p0)∈P.
We must show that (H′,∆′
i)◦H1..i−1(p0)∈P. From the
condition on Lines 10 and 12, we know that changes affecting
only the entities outside of Πare ignored. So for any change
δ∈H′, we have id(δ)∈Π. Depending on the change type
ofδ, the precondition of δis eitherid(δ)itself or its parent
should present (Figure 8). Because of the C OMP DEPrule (P1),
i.e.,x∈Π⇒PARENT(x)∈Π, changes to entities in Πand
their parents are kept. Therefore, any change δ∈H′stays
applicable.Lemma 2. (Type Safety). H′(p0)is well-typed.
Proof. Entities outside of compilation set stay unchanged,
except for method lookup changes (which might be kept and
do not affect type soundness); and their referenced targets are
preserved since deletions are omitted. Thus, non-compilation
set entities remain well-typed. By similar inductive argument
as in Lemma 1 and the completeness of the C OMP DEPrules,
we have that the compilation set entities also stay well-typed
after the slicing. Thus, H′(p0)is well-typed.
Theorem 1. (Correctness of Algorithm 1). Let /a\}bracketle{tp1,...,p k/a\}bracketri}htbe
kconsecutive subsequent versions of a program p0such that
pi∈Pandpiis well-typed for all indices 0≤i≤k. Let
H=/a\}bracketle{t∆1,...,∆k/a\}bracketri}htsuch that ∆i(pi−1) =pifor all indices
1≤i≤k. LetTbe a test suite such that pk|=T. Then the
sliced history H′=SEMANTIC SLICE(p0,H,T)is semantics-
preserving with respect to T.
Proof. According to Deﬁnition 6, we need to show that H′
satisﬁes the following properties,
1)H′(p0)∈P,
2)H′(p0)is well-typed,
3)H′(p0)|=T.
From Lemma 1 and Lemma 2 we know that (H′◦H1..i)(p0)
satisﬁes (1) and (2) is an invariant for the outer loop (Lines 6-
17) of Algorithm 1. The original history Hhas a ﬁnite length
k, so upon termination we have H′(p0)satisﬁes (1) and (2).
Since all functional set insertions and updates are kept in H′,
any functional set entity that exists in H(p0)can be found
identical in H′(p0). Because all changes that alter method
lookups are also kept (Line 9), the execution traces do not
change either. Due to that reason, and by the deﬁnition of
functional set, (3) also holds. Thus, H′(p0)satisﬁes (1), (2)
and (3).
4.3 SCM Adaptation
The proposed semantic slicing algorithm operates on the
atomic change level and can directly be used with semantic-
based tools, such as SemanticMerge [15]. As an optional step,
SCM adaptation integrates the generic Algorithm 1 with
text-based SCM systems such as Git.
4.3.1 Eliminating Side-Effects
To make the integration with text-based SCM systems easier,
each atomic change has to be mapped back to a commit in
the original history. The sub-history H′=δi=/a\}bracketle{t∆′
1,...,∆′
k/a\}bracketri}ht
(∆′
iis possibly empty) returned by S EMANTIC SLICE is a
sequence of atomic changes labeled by indices indicating
their corresponding original commits. A non-empty sliced
change set ∆′
ican thus be mapped to its counterpart in the
original history, i.e., ∆i.8
pj-1 pi-1 pi
…    ∆i
-
-
-+
+∆j   …   ∆i-1
+
+
+δ2: insert
-
-δ3: deleteδ1: updateδ4
δ5
Fig. 12. Illustration of direct Hunk dependencies.
However, original commits may contain changes that
are sliced away by Algorithm 1. These changes might
create unwanted side-effects which break the type safety
of the compilation set entities. We deal with this issue by
restoring entities that are outside of the compilation set to
their original state as in the initial version of the program,
thereby “selectively” ignoring these unwanted changes and
eliminating the side-effects. We do that by creating an
additional commit that reverts unwanted changes on the
corresponding code entities.
4.3.2 Calculating Hunk Dependencies
Algorithm 1 treats changes between versions as tree edit
operations. Another view of changes used by text-based
SCM tools is called hunk . A hunk is a group of adjacent or
nearby line insertions or deletions with surrounding context
lines which stay unchanged. For simplicity, we reuse the
notations of tree change operations for hunk changes. For
example, Figure 12 shows an abstract view of the changes
made between pi−1andpi, where blocks with “ -” represent
lines removed and blocks with “ +” represent lines inserted.
Grey blocks surrounding the changed lines represent the
contexts. From the text-based view, the difference between
pi−1andpiconsists of three hunks, i.e., δ1,δ2andδ3.
We deﬁne two auxiliary functions, left(δ)andright(δ),
which return the lines involved before and after the hunk
changeδ, respectively. Special cases are right(δ)whenδis a
deletion and left(δ)whenδis an insertion. In both cases, the
functions return a zero-length placeholder at the appropriate
positions.
In order to apply the sliced results with text-based SCM
tools where changes are represented as hunks, it is needed
to ensure that no conﬂict arises due to unmatched contexts.
Informally, a change set ∆idirectly hunk-depends on another
change set ∆j, denoted by ∆i/squiggleright∆j, if and only if ∆j
contributes to the hunks or their contexts in ∆i. In contrast,
if∆idoes not directly hunk-depend on ∆j, we say they
commute [23], i.e., reordering them in history does not cause
conﬂict. The procedure H UNK DEP(H′)returns the transitive
hunk dependencies for all change set in H′, i.e.,
HUNK DEP(H′)/defines/uniondisplay
∆i∈H′{∆j|∆j∈H/H′∧∆i/squiggleright∗∆j}.1:procedure DIRECT HUNK (Bi,H1..i)
2:D←∅
3:Bi−1←Li
4: forδ∈∆i−1do
5: ifδis D EL∧right(δ)∈range(Bi)then
6: D←D∪∆i−1
7: else ifδis INS∧right(δ)∩Bi/\e}atio\slash=∅then
8: D←D∪∆i−1
9: Bi−1←Bi−1/right(δ)
10: end for
11:D←D∪DIRECT HUNK(Bi−1,H1..(i−1))
12: returnD
13:end procedure
Fig. 13. Algorithm 2: the D IRECT HUNK procedure.
Once a sub-history H′is computed and returned by Algo-
rithm 1, we augment H′with H UNK DEP(H′)and the result
is guaranteed to apply to p0without edit conﬂicts.
Given a change set ∆i, we collect a set of text lines
Biwhich are required as the basis for applying ∆i. For
example, Bifor∆iincludes left(δ)for allδ∈∆iand
their surrounding contexts (all shaded blocks under pi−1in
Figure 12). Figure 13 describes the algorithm for computing
the set of direct hunk dependencies ( /squiggleright) by tracing back in
history and locating the latest change sets that contribute
to each line of the basis. Starting from ∆i−1, we iterate
backwards through all preceding change sets. If a change
set∆contains a deletion that falls in the range of the basis
(Line 5) or an insertion that adds lines to the basis (Line 7),
then∆is added to the direct dependency set D. In Figure 12,
∆i/squiggleright∆jbecause ∆jhas both an insertion ( δ4) and a
deletion ( δ5) that directly contribute to the basis at pi−1.
When the origin of a line is located in the history, the line
is removed from the basis set (Line 9). The algorithm then
recursively traces the origin of the remaining lines in Bi−1.
Upon termination, Dcontains all direct hunk dependencies
of∆i. In the worst case, H UNK DEPcalls D IRECT HUNK for
every change set in H′. Thus, the running time of H UNK DEP
is bounded above by O(|H′|×|H|×max∆∈H(|∆|)).
5 CS LICER PHASE 2: S LICE MINIMIZATION
The problem of ﬁnding optimal semantics-preserving sub-
history is intractable in general, as we showed in Section 4.1.1.
However, there are many cases where a minimal sub-history
is preferred or even required. For example, when submitting
pull requests for review, contributors should refrain from
including unrelated changes as suggested by many project
contribution guidelines [11], [12], [13]. Also, developers
commonly suggest to split a mixed patch into multiple ones,
e.g., as indicated by these code review comments:
“Okay, nevertheless we need to split this up because it is
unrelated to the issue we’re talking about.”1
Therefore, we would like to produce logically clean
and easy-to-merge history slices by reducing all irrelevant
changes. Despite the complexity of ﬁnding shortest slices,
we have identiﬁed a number of heuristic-based techniques
that could help shorten history slices and possibly derive
1. https://github.com/apache/commons-lang/pull/419
1 c l a s s Dog{
2 i n t age ;
3 Set<Dog>enemies = new HashSet <Dog>() ;
4 public Dog ( i n t a ){age = a ; }
5 void barking ( ) {
6 System . out . p r i n t l n ( ” bark ! ” ) ;
7}
8 boolean f i g h t i n g (Dog other ) {
9 barking ( ) ;
10 enemies . add ( other ) ;
11 return ! ( age<1||age>5) && age >other . age ;
12}
13}
14
15 c l a s s TestDog {
16 @Test
17 public t e s t F i g h t ( ) {
18 Dog d1 = new Dog ( 2 ) ;
19 Dog d2 = new Dog ( 1 ) ;
20 assertTrue ( d1 . f i g h t i n g ( d2 ) ) ;
21}
22}
Line Descriptions
δ1 3new HashSet<Dog>() →new HashSet<>()
δ2 6 bark!→bark!bark!
δ3 11 !(age<1||age>5) →age>0&&age<6
Fig. 14. Example illustrating different types of false positives.
minimal ones. In this section, we deﬁne minimal semantic slice
and discuss a few such techniques for minimizing history
slices obtained from Phase 1.
5.1 Minimal Semantic Slice
We say a sub-history H∗ofHis a minimal semantic slice
ifH∗is semantics-preserving and it cannot be further
shortened without losing the semantics-preserving properties
(see Deﬁnition 6).
Deﬁnition 7. (Minimal Semantic Slice). Given a semantics-
preserving slice H∗such that H∗⊳TH.H∗is a minimal
semantic slice of Hif,∀Hsub⊳H∗·(|Hsub|<|H∗|) =⇒
¬(Hsub⊳TH).
For our running example in Section 2, the solution
produced by CS LICER is not only minimal but also optimal as
there does not exist any other sub-history which is semantics-
preserving. However, a minimal semantic slice does not
always correspond to the global optimal slice. In other words,
there might exist a shorter semantic slice Hoptwhich is
not a sub-history of H∗. Empirical evidences show that
minimal slice of a semantics-preserving slice (such as H′
returned by Algorithm 1) is a good approximation to Hopt
(see Section 7.2.2).
5.2 Sources of Imprecision
The CS LICER algorithm (Algorithm 1) presented in Section 4
assumes that any change on the functional set can potentially
alter the ﬁnal test results and thus all functional changes are
kept during slicing. But this assumption is often found to be
too conservative in practice. We observed many cases of false
positives during change classiﬁcation in our experiments
(details in Section 7) which can be divided into two groups,
namely (1) semantics-preserving changes, and (2) oracle
unobservable changes.
Figure 14 shows an example illustrating the two types
of false positives. The fighting method of the Dog classis tested using two newly created instances. The executed
code entities include initialization of the ﬁeld enemies ,
class constructor, the barking and thefighting methods.
However, none of the changes ( δ1,δ2, andδ3) has any
inﬂuence on the asserted result (Line 20). Speciﬁcally, δ1
is a syntactic rewriting which does not change the semantics
of the program at all; δ2updates the barking method to
produce a different console output, but the output is never
checked against an oracle; δ3changes the returned expression
of method fighting , but the returned value is not affected
at runtime.
Semantics-Preserving Changes. An example of semantics-
preserving changes is code refactoring [24]. Refactoring
changes are program transformations that change the struc-
ture or appearance of a program but not its behavior (e.g.,
δ1). Refactoring is important for improving code readability
and maintainability. However, refactoring changes create
problems for text-based SCM systems, especially during
merging. Based on the study by Dig et al. [25], merging
changes along with code refactoring causes signiﬁcantly
more merge conﬂicts, compilation and runtime errors. The
common practice is thus separating refactoring from func-
tional changes [25] which gives developers the ﬂexibility
to replay the refactoring changes after merging is done.
Therefore, the slice minimization phase aims to produce
logically clean and easy-to-merge history slices by isolating
all semantics-preserving changes.
Renaming of code entities is one commonly seen refac-
toring change. In fact, existing code refactoring detection
techniques [26], [27], [28] focus on renaming and movement
of structural nodes, i.e., packages, classes, methods and ﬁelds.
However, such changes would alter AST structures as well
as node identiﬁers and thus often have a repercussion on
later changes. For instance, once a class renaming change
is applied, all successive references to that class have to
use the updated name. To preserve correctness of Algo-
rithm 1, we only consider self-contained local and low-level
refactorings [29] as candidates to be dropped. For example,
one common change pattern that can be ignored is the
usage of syntactic sugars and updated language features.
In the Apache Maven [30] change history, we observe the
adoption of new Java 7 features try-with-resources
statement [31] and the diamond operator [32] in a massive
scale.
Oracle Unobservable Changes. Execution results of a test
suite depend on both the explicit and implicit checks embed-
ded in the tests. In the case of Java, a JUnit [33] test case fails
either due to assertion failures (explicit checks deﬁned by
developers) or runtime errors (implicit checks performed by
runtime system) [34]. Some changes, even if may alter the
program behaviors, are non-observable to the test oracle. The
reason is that the updated behaviors are not checked either
explicitly or implicitly and therefore would not affect test
results in any way (e.g., δ2andδ3). Algorithm 1 does not
distinguish such changes from the ones that do affect test
behaviors.
5.3 Techniques for Slice Minimization
As discussed in Section 4.1.1, enumerating all sub-histories
of a history His highly unrealistic when the length of His10
Require: H(p0)|=T
Ensure:∀Hsub⊳H∗·(|Hsub|<|H∗|) =⇒ ¬(Hsub⊳TH)
1:procedure MINIMIZE (p0,H,T )
2:HS1←H
3: for all∆∈HS1do
4: ifMATCH PATTERN(∆) thenHS1←HS1/∆
5: ifHS1(p0)|=Tthen
6: H∗←ENUMERATE (p0,HS1,T)
7: elseH∗←ENUMERATE (p0,H,T)
8: returnH∗
9:end procedure
10:
11:procedure ENUMERATE (p0,HS2,T)
12:L←Sort(PICKABLE(HUNK GRAPH(HS2)))
13: for allHsub∈Ldo
14: ifHsub(p0)|=Tthen return Hsub
15: returnHS2 ⊲no shorter history found
16:end procedureRequire: (V,E)is a DAG
Ensure:∀Hsub∈L·Hsubis cherry-pickable on p0
17:procedure PICKABLE (V,E )
18: if|V|= 1 then return{/a\}bracketle{t/a\}bracketri}ht}
19:L←∅
20:R←ROOT NODES(V,E)
21: forr∈Rdo
22: V←V/r ⊲ remove a root
23: E←E/Out(r)⊲remove out edges
24: L←L∪({V}∪PICKABLE(V,E))
25: end for
26: returnL
27:end procedure
Fig. 15. Algorithm 3: ﬁnding minimal semantic slice of a given history H.
large (requiring in worst case 2|H|−1test runs). In contrast,
a minimal slice of Hcan be derived more efﬁciently from a
much shorter sub-history, e.g., a semantic slice H′returned
by Algorithm 1 (now requiring in worst case 2|H′|−1test
runs). However, the cost of a direct enumeration of H′
can still be prohibitive. We propose two heuristic-based
techniques that can be applied in combination to reduce false
positives in H′and minimize H′much more efﬁciently: (S1)
change pattern matching which analyzes changes statically and
ﬁlters out common false positives according to predeﬁned
patterns; and (S2) sub-history enumeration which takes hunk
dependencies into consideration and efﬁciently examines
only cherry-pickable sub-histories, i.e., sequence of commits
that can be applied without causing merge conﬂicts.
The overall minimization workﬂow is given as the
procedure M INIMIZE in Figure 15. M INIMIZE takes as input
a base version program p0, a semantics-preserving history
sliceHand the target test suite T. Since the static pattern
matching approach is much cheaper than enumerating
all sub-histories, we ﬁrst use M ATCH PATTERN as a pre-
processing step to opportunistically shorten Hby ﬁltering
out commits containing only “insigniﬁcant” changes (Lines 3-
4). Then the intermediate result HS1is veriﬁed against the
tests. If tests pass, then the procedure E NUMERATE is called
to enumerate and verify all cherry-pickable sub-histories of
HS1(Line 6). Otherwise we enumerate the original input
historyHinstead (Line 7). The enumeration procedure can
be terminated prematurely based on the available resources
and still returns a valid slice with best-effort.
5.3.1 Static Pattern Matching (S1)
The AST differencing algorithm used in Algorithm 1 treats
each method as a single structural node. To detect local
refactorings and unobservable changes within method bod-
ies, we apply a ﬁner-grained differencing algorithm at
the statement-level granularity and then categorize atomic
changes according to their signiﬁcance level [20]. Similar to
the deﬁnition in Fluri and Gall [20], we consider an atomic
change as less signiﬁcant if the likelihood of it affecting the
test results or other code entities is low. We opportunisticallydrop low-signiﬁcance changes if they happen to match with
predeﬁned patterns.
Some examples of the patterns include local refa-
cotring/rewriting, low impact modiﬁer changes such as re-
moval of the final keyword and update from protected
topublic , as well as white list statement updates such as
modiﬁcations to printing and logging method invocations.
We also allow users with domain knowledge to provide
insights on which components (such as classes, methods,
ﬁelds and statements) do not affect the test results to further
prune the functional sets. These patterns are generally
applicable to different code bases. More project-speciﬁc
rules and white lists can also be devised with insights from
the project developers. The low signiﬁcance changes are
processed separately, and we provide users with the options
to keep or exclude them as they wish.
5.3.2 Dynamic Sub-history Enumeration (S2)
Several types of oracle unobservable changes cannot be
matched using predeﬁned patterns. More sophisticated static
analysis such as program slicing [14] is able to identify more
precisely the set of program statements (a program slice)
which have the potential to affect the test oracle. However, a
program slice does not always subsume an oracle observable
set since a change outside of the program slice can still be
executed and affect the test results.
The most precise and reliable approach for minimizing
history slices is through the exhaustive enumeration and
verifying the test results directly when the number of
candidates is small. When hunk dependencies are present,
not all sub-histories are cherry-pickable on text-based SCM
systems. Instead of enumerating all sub-histories of H, which
are exponential to the length of H, we only consider cherry-
pickable sub-histories that can be veriﬁed through test runs.
This insight enables us to ﬁnd minimal solutions for many
examples in our benchmark (cf. Section 7.2.2). The procedure
PICKABLE in Figure 15 uses an approach similar to the Kahn’s
topological sorting algorithm [35] to generate all cherry-
pickable proper sub-histories for a given history slice.11
The idea behind the procedure P ICKABLE (Lines 17-27) is
as follows. The hunk dependencies among commits can be
represented using a directed acyclic graph (DAG) where V
is the set of vertices (commits) and Eis the set of edges
(hunk dependencies). There is an edge pointed from v1
tov2if and only if v1directly or indirectly hunk-depends
onv2. A commit is not cherry-pickable if any of its hunk
dependencies is missing. The algorithm prevents this from
happening by starting from Vand only removing vertices
that have no incoming edge (Line 20). After a vertex is
removed, the remaining history is added to the current set
of cherry-pickable sub-histories L, and the remaining graph
is processed recursively.
Lemma 3. (Soundness of PICKABLE ).PICKABLE(V,E)returns
all cherry-pickable proper sub-histories of HS2.
Proof. The proof is by induction. Considering the base
case where|V|= 1 , the only proper sub-history is
/a\}bracketle{t/a\}bracketri}ht. Now suppose P ICKABLE(V,E)returns all cherry-
pickable proper sub-histories for |V| ≤k. At any stage,
only root nodes (vertices with no outgoing edges) can
be removed without breaking the hunk dependencies.
When|Vk+1|=k+ 1, removing either one of the
root nodes produces a cherry-pickable proper sub-history,
Vk+1/rwhere the overhead bar stands for a sequence of
change set derived from the vertices. Taking the union
of each case gives us/uniontext
r∈ROOT NODES(Vk+1,Ek+1){Vk+1/r}∪
PICKABLE(Vk+1/r,Ek+1/Out(r)).
Theorem 2. (Correctness of Algorithm 3). MINIMIZE(p0,H,T)
returns a minimal semantic slice H∗such that∀Hsub⊳H∗·
(|Hsub|<|H∗|) =⇒ ¬(Hsub⊳TH).
Proof. Assume only cherry-pickable sub-histories are consid-
ered2. The theorem trivially holds given Lemma 3 and the
fact that the sub-histories are traversed in increasing order
of their lengths.
Algorithm 3 always terminates since there are only ﬁnitely
many sub-histories. However, in practice, enumerating all
combinations can still be time consuming even with the
pre-processing step. We report our empirical ﬁndings in
Section 7.2.2.
6 I MPLEMENTATION AND OPTIMIZATIONS
In this section, we describe the implementation details of our
semantic slicing tool – CS LICER , and discuss practical issues
as well as some optimizations applied.
6.1 Implementation
Figure 16 shows the high-level architecture of our CS LICER
implementation. We implemented CS LICER in Java, using
the JaCoCo Java Code Coverage Library [36] for byte
code instrumentation and collecting execution data dur-
ing runtime. We modiﬁed the Java source code change
extraction tool ChangeDistiller [37] for AST differencing
and change classiﬁcation. We also used the Apache Byte
Code Engineering Library (BCEL) [38] for entity reference
relation analysis. The hunk dependency detection component
2. This assumption can be relaxed in semantic-based SCM systems.Change
Distiller
CSLICERJGit JaCoCo BCEL
         …H
p0 pkT
t1 … tm
Λ Π
pi pi-1 SlicingSCM 
AdaptationHUNKDEP
(V , E)
MinimizingH*
H’ ∆i
Fig. 16. Architecture of the CS LICER implementation.
HUNK DEPwas developed based on the Java-based Git
implementation, JGit [39]. The H UNK DEPcomponent can
also be used as a stand-alone hunk dependency analysis
tool for Git repositories. Given a set of commits, H UNK DEP
generates a hunk dependency graph which visualizes the
hunk-level relationship among commits and can be used to
reorder commit histories without causing conﬂicts.
Our CS LICER implementation works with Java projects
hosted in Git repositories. The test-slice-verify process is
fully-automated for projects built with Maven [30]. For other
build environments, a user is required to manually build
and collect test execution data through the JaCoCo plugins.
When the analysis is ﬁnished, CS LICER automatically cherry-
picks the identiﬁed commits and veriﬁes the test results.
CSLICER can also run in the minimization mode where
all cherry-pickable sub-histories are enumerated for further
investigation.
The implementation of CS LICER takes about 20 KLOC,
and the source code is made available online at https://
bitbucket.org/liyistc/gitslice.
6.2 Optimizations and Adaptations
In order to deal with real-life software projects, we imple-
mented a number of techniques and adaptations on top of
the CS LICER algorithm which address speciﬁc challenges in
practice. We describe them below.
6.2.1 Handling Advanced Java Features
We presented our algorithms based on the simpliﬁed lan-
guageP. When dealing with full Java, advanced language
features including method overloading, abstract class and
exception handling need to be taken into account. For exam-
ple, various constructs such as instanceof and exception
catch blocks test the runtime type of an object. Therefore,
class hierarchy changes may alter runtime behaviors of the
test [40]. To address this, we treat class hierarchy changes
as an update to the methods that check the corresponding
runtime types, to signal possible behavior changes. Changes
that may affect method overloading and ﬁeld overshadowing
are detected and included in the sliced history to keep our
approach sound. Since reﬂection related changes are rare
in practice, e.g., none of our case studies contained such
changes, we disregard them in this work.12
6.2.2 Handling Changes in Non-Java Files
Real software version histories often contain changes to non-
Java ﬁles, e.g., build scripts, conﬁguration ﬁles and binary
ﬁles. Sometimes changes to non-Java ﬁles are entangled with
Java ﬁle changes in the same commits. To avoid false hunk
dependencies, we ignore non-Java changes in the analysis
and conservatively update all non-Java ﬁles to their latest
versions unless they are explicitly marked irrelevant by the
user. In extremely rare cases, this may cause compilation
issues, when older Java components are incompatible with
the updated non-Java ﬁles. Then the components which
cause the problem should be updated or reverted accordingly.
None of these affects the target functionalities.
6.2.3 Additional Conﬁgurability
We noticed in the experiments that domain knowledge that
users have about their projects can enhance the precision of
the slicing results. To make the technique more conﬁgurable,
we allow users to encode their domain knowledge during
both the analysis and the cherry-picking processes. Speciﬁc
packages, ﬁles, classes and methods can be included or
excluded following user guidance. By default, all changes
in the test ﬁles, which are not part of the target system, are
ignored, as are changes to the internal debugging code.
Another easy-to-implement optimization is ignoring a
commit and its revert, if these are found in the history, since
removing such a pair does not affect the correctness of the
approach.
7 E VALUATION
In this section, we measure the effectiveness and applica-
bility of CS LICER through both case studies and empirical
evaluations. Speciﬁcally, we evaluate CS LICER from three
different angles.
1)Qualitative assessment of CS LICER in practical settings
(Section 7.1). We carried out a number of case studies
to test the applicability of our techniques in practice,
for software maintenance tasks such as porting patches,
creating pull request, etc.
2)Quantitative evaluation of CS LICER (Section 7.2). We
conducted several experiments to get deeper insights
on the proposed algorithm to answer questions such as
“how much reduction can CS LICER achieve”, “how well
does CS LICER scale with increasing history length”, etc.
3)Comparison with the state-of-the-art change minimiza-
tion technique – delta debugging [17] (Section 7.3). We
tested CS LICER and delta debugging end-to-end and
compared the quality of the produced slices as well as
performance of the two systems.
7.1 Qualitative Assessment of CS LICER
We have conducted three case studies and thoroughly investi-
gated the results produced by CS LICER , to better understand
its applicability, effectiveness, and limitations. In particular,
we looked at six open source software projects of varying
sizes, different version control workﬂows, and disparate
committing styles – Apache Hadoop [41], Elasticsearch [10],
Apache Maven [30], Apache Commons Collection [42],
Apache Commons Math [43], and Apache Commons IO [44].TABLE 1
Statistics of case study subjects. Each row lists the number of Java ﬁles
(#Files), lines of code (LOC) of the studied projects, the length of the
chosen history fragment ( |H|), the number of changed ﬁles (f), lines
added (+), and lines deleted (-) for the chosen range, and the number of
test cases ( |T|) in the target test suites.
Case Project #Files LOC |H|Changed|T|
f + -
1 Hadoop 5,861 1,291K 267 1,197 111,119 14,064 58
2 Elasticsearch 3,865 616K 51 75 1,755 304 2
3Maven 967 81K 50 16 1,012 250 7
Collections 525 62K 39 46 1,678 323 13
Math 1,410 188K 33 34 1,531 359 1
IO 227 29K 26 59 975 468 13
We chose one target functionality from each project based
on the criterion that the functionality should be well-
documented and accompanied by deterministic unit tests.
Statistics of the subjects are given in Table 1 grouped
by their case numbers (Column “Case”). For example, the
Hadoop project has 6,608 ﬁles, out of which 5,861 are Java
ﬁles. The selected history segment covered changes to 1,197
ﬁles where over 100 KLOC were added and about 14 KLOC
were deleted.
All of the following studies were conducted on a desktop
computer running Linux with an Intel i7 3.4GHz processor
and 16GB of RAM. Now we describe each case study in
detail.
7.1.1 Case 1: Branch Refactoring
We applied CS LICER to Hadoop for branch refactoring . The
feature “HDFS-6581” was developed in a feature branch (also
called a topic branch) which was separated from the main
development branch. However, when the development cycle
of a feature is long, it is reasonable to merge changes from
the main branch back to the feature branch periodically, in
order to prevent divergence and resolve conﬂicts early. And
that is exactly the workﬂow followed by the Hadoop team
on their feature branches. As a result, not all commits on
the feature branch are logically related to the target feature
or required to pass the feature tests. That is, the branch
“origin/HDFS-6581” is mixed with both feature commits and
merge commits from the main branch. Using CS LICER , we
were able to re-group commits according to their semantic
functionalities and reconstruct a new feature branch that is
fully functional and dedicated to the target feature.
We started with the original feature branch which consists
of 42 feature commits and 47 merge commits. There are 34
auto merges (“fast-forward merges” in Git terms) which are
simply combinations of commits from both branches without
conﬂicts or additional edits. The other 13 are conﬂict resolution
merges which contain additional edits to resolve conﬂicts. To
achieve higher granularity when analyzing merge commits,
we kept the resolution merges and expanded the auto merges
by replaying (cherry-picking) the corresponding commits
from the main branch onto the feature branch. Effectively, we
converted the branched history into an equivalent expanded
linear history by splitting bulk merge commits (see Figure 17).
This was all done automatically as a preprocessing step. The
expanded feature branch has 267 commits in total.13
cherry-pickingmain feature main feature’
   
auto merge
Fig. 17. Illustration of expanding auto merges through cherry-picking
corresponding commits from the main branch to the feature branch.
We executed 58 feature-related unit tests speciﬁed in the
test plan, which took about 750 seconds to ﬁnish. CS LICER
identiﬁed 65 commits which are required for preserving
the test behavior as well as compilation dependencies, and
additional 26 commits for hunk dependencies. Note that
some commits from the main branch are actually required
by the target feature. The refactored feature branch passed
the feature test suite and it only contains 91 commits in total,
which achieves∼66% reduction.
7.1.2 Case 2: Back-porting Commits
The second use case of CS LICER is to identify the set of
commits required for back-porting a functionality to earlier
versions of a software project. We took a fragment of history
between v1.3.6 to v1.3.8 of Elasticsearch and a feature
enhancement on the “Groovy interface”. There are 2 unit tests
clearly marked by the developers in the commit messages
intending to test the target functionality introduced in v1.3.8.
As discussed in Section 4, there is no efﬁcient algorithm
that returns the optimal solution in general. Finding the
shortest semantics-preserving sub-history for a given set
of tests is a highly challenging task even for programmers
with expertise within the software projects. Yet, we manually
identiﬁed the optimal solutions for this case, which requires
4 out of 51 commits to be ported to v1.3.6 in order for the
functionality to work correctly.
CSLICER without using minimization identiﬁed 17 com-
mits achieving a 67% reduction of the unrelated commits.
However, compared with the optimal solution, CS LICER
reported 13 false positives. We examined all the false alarms
and concluded that the main reason causing them is that
the actual test execution exposes more behaviors of the
system than what were intended to be veriﬁed. For instance,
the test case “DynamicBlacklist” invoked not only the
components implementing the “dynamic black list” but
also those that implement the logging functions for debug
purposes. Obviously, changes to the logging functions do
not affect the test results. But without prior knowledge,
CSLICER would conservatively classify them as possibly
affecting changes. We investigate the effectiveness of slice
minimization techniques on reducing such false alarms in
Section 7.2.
7.1.3 Case 3: Creating Pull Requests
Another important use case of CS LICER is creating logically
clean and easy-to-merge pull requests. Often, a developerTABLE 2
Pull requests recreating results. The Columns “ID” and “Status” show the
ID and status of the corresponding pull requests. The Column “ |H∗|”
shows the length of the pull requests created and submitted by the
developers. The Columns “ |H′|”, “P(%)”, and “R(%)” list the length,
precision, and recall of the pull requests created by CS LICER ,
respectively.
Project ID Status |H∗|CSLICER
|H′|P(%)R(%)
Maven #74 Open 3 3 100.0 100.0
Collections #7 Merged 8 5 100.0 62.5
Math #10 Open 2 2 100.0 100.0
IO #17 Accepted 9 8 87.5 77.8
works on multiple functionalities at the same time which
could result in mixed commit histories concerning different
issues (see the quote of code review comments in Section 5).
Despite the efforts of keeping the development of each issue
on separate branches, isolating each functional unit as a
self-contained pull request is still a challenging task.
To assess the effectiveness of CS LICER in assisting and
automating the process of creating pull requests, we selected
four public pull requests3from four different software
projects (see Table 2). We browsed the pull request lists
available from the project repositories and selected the most
recent pull requests which are non-trivial (containing more
than one commit) and accompanied by unit tests. We used
the test cases submitted along with the pull requests as our
target tests and used CS LICER to identify the closely related
commits from the developers’ local histories in their forked
repositories.
Two pull requests (#7 in Commons Collections and #17
in Commons IO) have already been ﬁnalized or merged
into the public repositories. The other two are still awaiting
approval. For the pull requests #74 in Maven and #10 in
Commons Math Library, CS LICER successfully recreated the
exact same results as the ones submitted by the developers.
For Commons IO, CS LICER included one extra commit
(#bccf2af4) and missed two commits (#62535cc and #3b71609).
For Commons Collections, CS LICER missed three commits.
After a detailed analysis, we discovered a few reasons for
CSLICER to miss certain commits. First, several commits in
pull request #17 simply reorganize Java import statements,
i.e., remove unused imports (#3b71609) and replace groups of
imports by wild card (#62535cc). CS LICER currently ignores
all changes to import statements since they do not affect test
executions when every version of the program compiles. Sec-
ond, CS LICER ignores commits which only modify comments
and Javadoc. This is currently a limitation of our tool. In fact,
accurate identiﬁcation of changes to relevant documentation
is an interesting open research problem. Finally, CS LICER
correctly ignores an empty merge commit (#4cc49d78) in pull
request #7 which was included by the developer.
7.2 Quantitative Evaluation of CS LICER
To have more insights of the internals of our algorithm, we
also empirically evaluated the efﬁciency and applicability
3.The pull requests are subject to future modiﬁcations. The pull
requests used in this study were taken from the projects’ public
repositories on Sep 10, 2016.14
of CS LICER by measuring its performance and the history
reduction rate achieved when applied on a benchmark set
obtained from real-world software projects. Speciﬁcally, we
aimed to answer the following research questions:
RQ1: How effective is the CS LICER slicing algorithm , in
terms of the number of irrelevant commits identiﬁed?
RQ2: How efﬁcient is CS LICER when applied to histories of
various lengths ?
RQ3: How effective are the slice minimization techniques
used in CS LICER ?
7.2.1 Experiment 1: History Slicing
The ﬁrst experiment aims to address both RQ1 and RQ2
by running CS LICER using the “normal” mode (without
minimization).
Subjects and Methodology. In addition to the ﬁve projects
introduced before, we selected one more project, i.e., Apache
ActiveMQ [45]. The selected benchmark projects cover a
variety of sizes, qualities, objectives, collaboration models,
and project management disciplines. For example, Hadoop
has the largest code base ( ∼1,300 KLOC) among the ﬁve;
Elasticsearch is the most collaborative project which has over
830 contributors; and ActiveMQ has the largest number of
sub-projects (36 in total).
From each project, we randomly chose three to four
functionalities (e.g., a feature, an enhancement or a bug
ﬁx) that are accompanied by good documentation and unit
tests. All of the selected projects have, to a certain extent,
requirements concerning test adequacy – a patch must be
adequately tested before being merged into the repository.
For example, the Maven development guidelines [46] state
that:
“It is expected that any patches relating to functionality
will be accompanied by unit tests and/or integration
tests.”
It is also often possible to link speciﬁc commits with on-line
issue tracking documentation via ticket numbers embedded
in the commit messages. For each functionality, we referred
to the log messages and ticket numbers to locate the target
commit where the functionality was completed and tested.
The set of tests are either explicitly mentioned in the
accompanied test plan or implicitly enclosed within the same
commit as the functionality itself.
The description and target commit for each functionality
are shown in Table 3.4For example, H1(“Add nodeLabel
operations in help command”) is a feature which adds
a new label in the help option of the resource manager
administration client command line interface of the Hadoop
project. There was one test case (TestRMAdminCLI#testHelp)
introduced at revision “#e1ee0d4” to validate the implemen-
tation of this functionality.
The lifetime of a functionality typically spans a period of
1-4 months which corresponds to around 100 commits for
a mid-sized project under active development [8]. In order
to evaluate the effectiveness and performance of CS LICER
under different contexts, we took three sets of histories of
lengths 50 (short), 150 (medium) and 250 (long) tracing back
4.A more detailed version of the subject descriptions is available at:
http://www.cs.toronto.edu/∼polaris/tse/data.html.H1I2I1A1M3M2M1M4L1I3H2E3E4L2E2E1A2A3H3H4020406080100Slice compositions (%)FUNC
COMP
HUNK
01,0002,0003,0004,0005,000
Functional set sizes|Λ|
Fig. 18. Compositions of sliced histories for all benchmarks shown in the
order of increasing functional set sizes. Left y-axis represents percentage
of functional (FUNC), compilation (COMP) and hunk (HUNK) dependen-
cies over |H|(%). Right y-axis represents sizes of the functional set
(|Λ|).
from the target commits. We separated project source code
from test code and used CS LICER to perform the semantic
slicing on source code only. After applying the sliced histories
on top of the base version, we then veriﬁed that the resulting
programs compile successfully and pass the original tests.
Results. The slice compositions for medium-length histo-
ries are reported in Figure 18. The history slices returned
by CS LICER consist of functional, compilation and hunk
dependencies. Each stacked bar in Figure 18 represents the
percentage of all three types of dependencies within the
original histories. The dotted line rising from left to right
represents the sizes of functional sets, i.e., the number of
source code entities traversed by the test execution. For
example, the functional set size for H4is 4,846. Its sliced
history consists of 20.0% functional, 0.7% compilation, and
11.3% hunk dependencies of the original history commits.
The ﬁrst observation is that simpler and clear-cut func-
tionalities tend to produce smaller slices. The sizes of
functionalities are reﬂected by the functional set sizes. In
general, increasing functional set size leads to increasing size
of the history slice (without considering hunk dependencies).
Another interesting observation is that the number of
hunk dependencies for Hadoop and Maven is much larger
than those of the other projects. The functional set sizes
have no obvious relationship with the number of hunk
dependencies, which corroborates our conjecture that the
level of text-level coupling among commits is project speciﬁc.
Answer to RQ1. CSLICER effectively reduces irrelevant
commits given a target test suite.
Finally, we compare the average time taken by each
CSLICER component, i.e., the functional set computation,
the compilation set computation, the core slicing component
and the hunk dependency computation, when analyzing
short, medium and long histories on all the 23 benchmarks.
The results are shown in Figure 19. For example, the core
slicing component takes 2.9, 6.0, and 8.7 seconds on average
to ﬁnish for short, medium and long histories, respectively.15
TABLE 3
Target functionalities used in the experiments. Column “Project” shows the names of the projects where the functionalities were chosen from.
Columns “ID”, “Type”, “Description” and “Commit” represent the identiﬁers, functionality types (i.e., either feature, bug ﬁx, or enhancement),
descriptions and commit IDs of each target functionality. Column “Test Class # Test Methods” shows the test class and method names separated by
“#”, where some of the method names are omitted when they cannot be ﬁt into the space. Column “ |T|” lists the number of test cases in the
corresponding test suites.
Project ID Type Description Commit Test Class # Test Methods |T|
HadoopH1 Feature Add NodeLabel operations in help command e1ee0d4 TestRMAdminCLI # {testHelp} 1
H2 Bug Fix FileContext.getFileContext can stack overﬂow if
default fs is mis-conﬁguredb9d4976 TestFileContext #
{testDefaultURIWithoutScheme }1
H3 Feature LazyPersistFiles: add ﬂag persistence, ability to write
replicas to RAM disk, lazy writes to disk, etc.3f64c4a TestLazyPersistFiles # {...} 11
H4 Enhance HDFS inotify: add defaultBlockSize to CreateEvent 6e13fc6 TestDFSInotifyEventInputStream # {testBasic} 1
ElasticE1 Feature Calculate Alder32 Checksums for legacy ﬁles in
Store#checkIntegrityb2621c9 StoreTest # {testCheckIntegrity } 1
E2 Enhance Make groovy sandbox method blacklist dynamically
additive64d8e2a GroovySandboxScriptTests #
{testDynamicBlacklist }1
E3 Feature Adding parse gates for valid GeoJSON coordinates 418de6f GeoJSONShapeParserTests #
{testParse invalidPolygon}1
E4 Enhance Enable ClusterInfoService by default 4683e3c ClusterInfoServiceTests #
{testClusterInfoServiceCollectsInformation }1
ActiveMQA1 Enhance Add trace level log to shared ﬁle locker keepAlive c17b7fd SharedFileLockerTest # {...} 4
A2 Bug Fix Fix MQTT virtual topic queue restore preﬁx 4a8fec4 PahoMQTTTest #
{testVirtualTopicQueueRestore }1
A3 Enhance Only start connection timeout if not already started
the rest of the monitoringe5a94bf DuplexNetworkTest # {testStaysUp} 1
MavenM1 Bug Fix ToolchainManagerPrivate.getToolchainsForType()
returns toolchains that are not of expected type2d0ec94 DefaultToolchainManagerPrivateTest # {...} 3
M2 Feature Add DefaultToolchainsBuilder and Toolchains-
BuildingException99f763d DefaultToolchainsBuilderTest # {...} 6
M3 Bug Fix Fix: execution request populate ignores plugin repos-
itoriesd745f8c DefaultMavenExecutionRequestPopulatorTest
#{testPluginRepositoryInjection }1
M4 Enhance Fail, rather than just warning, on empty entries b8dcb08 DefaultModelValidatorTest #
{testEmptyModule }1
IOI1 Enhance Add ByteArrayOutputStream.toInputStream() 8960862 ByteArrayOutputStreamTestCase # {...} 3
I2 Enhance FieUtils: broken symlink support b9d4976 FileUtilsCleanSymlinksTestCase #
{testIdentiﬁesBrokenSymlinkFile }1
I3 Bug Fix FilenameUtils should handle embedded null bytes 63cbfe7 FilenameUtilsTestCase # {...} 37
MathL1 Enhance Add estimation types and NaN handling strategies
for Percentileafff37e MedianTest # {testAllTechniquesSingleton } 1
L2 Bug Fix Using diagonal matrix to avoid exhausting memory b07ecae PolynomialFitterTest # {testLargeSample} 1
FUNC COMP SLICE HUNK02040
2.0
4.4
2.9
12.12.2
4.4
6.0
27.22.3
4.5
8.7
38.8Average time taken (s)short
medium
long
Fig. 19. Average time taken by CS LICER when running on short, medium
and long histories. Each vertical bar plots the average time taken by the
functional set computation (FUNC), compilation set computation (COMP),
core slicing component (SLICE) and the hunk dependency computation
(HUNK) when running on histories with different lengths.
Overall, CS LICER takes on average under 10 seconds to
ﬁnish without computing hunk dependencies. The corre-sponding minimum and maximum times are 2.6 and 27.0
seconds, respectively. The time spent for FUNC and COMP
stays almost constant across different history lengths while
the SLICE time grows linearly. The majority of time is spent
in computing HUNK which also grows linearly over history
length.
Answer to RQ2. CSLICER scales well on real-world software
projects and histories of moderate lengths.
7.2.2 Experiment 2: Minimization
The second experiment is designed to answer RQ3 by run-
ning CS LICER in the minimization mode. We are interested
in the number of false positives that can be reduced by both
the static pattern matching and the dynamic sub-history
enumeration techniques.
Subjects and Methodology. In order to study the effective-
ness of our slice minimization techniques, we chose a subset
(10 out of 20) of the benchmarks used in Experiment 1,
which have relatively short semantic slices (the number of
FUNC and COMP commits is smaller than or equal to 11)
so that we could exhaustively enumerate all sub-histories to16
TABLE 4
Results for Experiment 2. Columns “ |H′|” and “|H∗|” show the lengths of
slices produced in Experiment 1 and the minimal slices, respectively.
Columns “S1” and “S2” list the number of reduced commits by each
technique. Columns “T1” and “T2” list the number of test runs required in
order to ﬁnd the minimal sub-history with and without using S1,
respectively.
Subject|H′| |H∗|S1 S2 T1 T2
H1 3 2 1 0 2 2
H2 2 2 0 0 3 3
A2 4 2 1 1 7 5
E2 9 1 6 2 5 2
M3 3 3 0 0 6 6
M4 4 2 0 2 3 3
I2 2 1 0 1 3 3
I3 6 2 1 3 12 8
L1 2 2 0 0 3 3
L2 6 2 0 4 8 8
establish the ground truth. For each of them, we exhaustively
enumerated all cherry-pickable sub-histories and found at
least one minimal semantic slice. The lengths of slices before
and after minimization are given in Table 4.
For each of the subjects, we ﬁrst applied static pattern
matching (S1) to identify and eliminate change sets with low
signiﬁcance. This includes 2 local code refactorings, 7 white
list statement updates, and 4 low signiﬁcance changes. Then
we applied dynamic sub-history enumeration (S2) on the
remaining history slices to ﬁnd a minimal solution. Note that
in our experiments, all minimal solutions were found after
the S1 step. In the general case, no solution might exist after
this stage because S1 could eliminate an essential commit.
Then we need to exhaustively enumerate all sub-histories of
the original slice before the S1 reduction (see Algorithm 3 for
details).
Results. The detailed experimental results are reported in
Table 4. For instance, the original slice for E2has nine
commits. Applying S1 allows us to remove six commits
and to minimize the remaining three commits using S2 we
enumerate all singletons, then all pairs, and so on. The actual
number of test runs used for this case is two: one failed test
on the empty history slice, and one successful test on the ﬁrst
try of the singleton slices. In contrast, without applying S1,
the minimization needed ﬁve test runs.
As a result of our experiment, we were able to prove that
the slices produced for H2,M3andL1are already at minimal
lengths by enumerating all their candidate sub-histories with
3, 6 and 3 failed executions, respectively. For the rest, we
found minimal solutions on the ﬁrst few tries.
Also, comparing Columns S1 and S2, we conclude that the
heuristic-based change ﬁltering patterns are both effective
and generally applicable to different subjects in reducing
false positive commits. Notably, 6 commits were ﬁltered
out during the S1 step for E2. In addition, applying S1
signiﬁcantly reduced the number of sub-histories needed
to be enumerated for S2.
Finally, taking into account hunk dependencies existing
inherently among commits in the input slices helps mitigate
the exponential explosion in sub-history enumeration. The
number of combinations to verify for E2is reduced from100101102103100101102103
10x100x
#Test runs by CSlicer#Test runs by delta debugging100101102103
100101102103Time (s) taken by CSlicer
Time (s) taken by delta debuggingTest runs
Time
Fig. 20. End-to-end comparison results of delta debugging and CS LICER .
The left and right y-axes represent the number of tests and the total
running time required by delta debugging to ﬁnd a minimal solution,
respectively. The bottom and top x-axes represent the number of tests
and the total running time required by CS LICER to ﬁnish, respectively. All
axes use the log scale.
29−1down to 54.
Answer to RQ3. The two types of slice minimization tech-
niques are complementary to each other – the heuristic-based
static pattern matching technique (S1) reduces the search
space with little performance overhead, and the dynamic sub-
history enumeration technique (S2) guarantees minimality
of the results.
7.3 Comparison with Delta Debugging
Delta debugging [17] is an automatic debugging technique
developed by Andreas Zeller in 1999. Its goal is to simplify
and isolate a minimal cause of a given test failure. The
high-level idea is to repeatedly partition the input and
opportunistically reduce the search space when the target
test fails on one of the partitions until a minimal partition
is reached. This problem can be considered to be a form
of semantic slicing with respect to the failure-inducing
properties. Therefore, it is natural to apply the same idea for
minimizing semantic slices.
We implemented delta debugging within our tool frame-
work to allow a fair end-to-end comparison between the two
approaches. Delta debugging follows a divide-and-conquer-
style history partition process. In each iteration, a subset
of the commits was reverted and if the resulting program
passed the tests, the process was continued recursively on the
remaining sub-history. Otherwise, a different partition was
attempted. We applied both delta debugging and CS LICER
on 10 benchmarks for which we have established the ground
truth (see Table 4). We looked at both the total running
time and the number of tests required to reach the minimal
solution.
The detailed results are shown in Figure 20. CS LICER
performs better than delta debugging for points above the
diagonal line which was observed for all examples we ran.
There were three examples ( H1,H2, and M3) where delta17
debugging could not ﬁnd a minimal solution within the
two-hour time limit (points lying on the top x-axis). On
average, CS LICER took only 143.7 seconds to ﬁnish while
delta debugging took over 2,331.0 seconds (not including
the ones where it ran out of time). The main reason behind
CSLICER ’s win is that repeated test executions are rather
expensive and Phase 1 of CS LICER can effectively reduce
the search space. Moreover, Phase 1 is relatively inexpensive,
taking only 37.4% of the total running time. Overall, CS LICER
performs constantly better than delta debugging, with the
majority of the cases seeing a 10X to 100X speedup.
7.4 Summary
To summarize, we analyzed CS LICER both qualitatively and
quantitatively. We demonstrated that apart from assisting
developers in porting functionalities, CS LICER can also
be applied for other maintenance tasks such as branch
refactoring and creating logically clean pull requests. The
comparison with manually identiﬁed optimal sub-histories
indicates that the precision of Algorithm 1 is limited by
how accurately we can decide whether a change affects the
test results. Further evaluation showed that our proposed
slice minimization techniques are effective for slice quality
improvement and thus are good complements to Algorithm
1. Furthermore, the results of quantitative studies suggest
that CS LICER is able to achieve good reduction of irrelevant
commits at a relatively low cost when applied to real-world
software repositories, which justiﬁes its value in practice.
Finally, from the comparison with the delta-debugging-style
minimization approach, we highlight the beneﬁts of the two-
phase design of CS LICER . By combining the inexpensive
but over-approximating Phase 1 with the heavyweight
but precise Phase 2, CS LICER achieves both precision and
efﬁciency.
Threats to Validity. Due to limitations of the tool, we only
selected software projects which could be conﬁgured and
built using Maven. The reduction rate, however, depends
on many factors – the committing styles, the complexities
of the test (how many components it invokes), and coding
styles (how closely the components are coupled), etc. While
our results are encouraging, we do not have enough data to
conclude that they will generalize to all software projects.
As an assumption of our technique, functionalities of
interest should be accompanied by appropriate test suites
in order to get intended results. Although we were able
to identify the corresponding unit tests for all the target
functionalities in our selection of the experiment subjects, we
understand that this may not always be possible for other,
less disciplined, projects. In the absence of well-designed
tests, additional expert knowledge might be necessary for
reﬁning the slicing results.
Experiment 2 was limited to subjects whose initial slice
is relatively short so that we could exhaustively enumerate
all sub-histories and ﬁnd a minimal slice. The false positive
rates of CS LICER may not be generalizable when applied on
other experimental subjects.
8 R ELATED WORK
To the best of our knowledge, the software history semantic
slicing problem was ﬁrst deﬁned in our prior work [1] and ithas not been studied in the literature prior to that or since
then. However, our work does intersect with different areas
spanning history understanding and manipulation, code
change classiﬁcation, change impact analysis and software
product line variants generation. We compare CS LICER with
the related work below.
8.1 History Understanding and Manipulation
There is a large body of work on analyzing and understand-
ing software histories. The basic research goals are retrieving
useful information from change histories to help understand
development practices [29], [47], localize bugs [17], [48],
and support predictions [49], [50]. Delta debugging [17]
uses divide-and-conquer-style iterative test executions to
narrowing down the causes of software failures. It has
been applied to minimize the set of changes which cause
regression test failures. However, in contrast to CS LICER
which extracts semantic information from successful test
runs, there is no easy way to exploit failed executions.
Regarding slice quality, Zeller and Hildebrandt [48] consider
an approximated version of minimality, i.e., 1-minimal , which
only guarantees that removing any single change set breaks
the target properties. This trade-off on solution quality
enables the authors to use an efﬁcient divide-and-conquer
search method. In contrast, Algorithm 3 relies on semantic
information and much cheaper heuristic-based change ﬁlter-
ing techniques to ﬁrst shorten the input before exhaustive
enumeration so that true minimality becomes tractable.
Algorithm 3 guarantees the quality of the minimized slice
– removing any sub-history from the slice breaks the target
properties.
Another interesting take on history analysis is history
transformation [7], [51]. Mu s ¸lu et. al [7] introduced a concept
ofmulti-grained development history views. Instead of using
a ﬁxed representation of the change history, they propose
a more ﬂexible framework which can transform version
histories into different representations at various levels of
granularity to better facilitate the tasks at hand. Such trans-
formation operators can be combined with CS LICER to build
a history view which clusters semantically related changes
as high-level logical groups. This semantics summarization
view [7] is a much more meaningful representation for
history understanding and analysis.
8.2 Change Classiﬁcation
The CS LICER algorithm relies on sophisticated structural
differencing [21], [52], [53] and code change classiﬁcation [20],
[37], [54] algorithms. We use the former to compute an opti-
mal sequence of atomic edit operations that can transform
one AST into another, and the latter to classify the atomic
changes according to their change types.
The most established AST differencing algorithm is
ChangeDistiller [37]. It uses individual statements as the
smallest AST nodes and categorizes source code changes
into four types of elementary tree edit operations, namely,
insert, delete, move and update. We use a slightly different
AST model in which all entity nodes are unordered. For
example, the ordering of methods in a class does not matter
while the ordering of statements in a method does. Hence,
the move operation is no longer needed and thus never18
reported by CS LICER . We also label each AST node using a
unique identiﬁer to represent the fully qualiﬁed name of each
source code entity. The rename of an entity is thus treated
as a deletion followed by an insertion. This modiﬁcation
helps avoid confusion in functional set matching using
identiﬁers. Finally, deletion is only deﬁned over leaf nodes in
ChangeDistiller. In contrast, we lift this constraint and allow
deletion of a subtree to gain more ﬂexibility and ensure
integrity of the resulting AST.
8.3 Change Impact Analysis
Change Impact Analysis [40], [55], [56], [57], [58] solves the
problem of determining the effects of source code modiﬁ-
cations. It usually means selecting a subset of tests from
a regression test suite that might be affected by the given
change, or, given a test failure, deciding which changes might
be causing it.
Research on impact analysis can be roughly divided
into three categories: the static [55], [59], dynamic [56] and
combined [40], [58], [60] approaches. The work most related
is on the combined approaches to change impact analysis.
Ren et. al [40] introduced a tool, Chianti, for change impact
analysis of Java programs. Chianti takes two versions of
a Java program and a set of tests as the input. First, it
builds dynamic call graphs for both versions before and
after the changes through test execution. Then it compares
the classiﬁed changes with the old call graph to predict
the affected tests; and it uses the new call graph to select
the affecting changes that might cause the test failures.
FaultTracer [58] improved Chianti by extending the standard
dynamic call graph with ﬁeld access information.
CSLICER uses similar techniques to identify affecting
changes. However, the real challenge in our problem is to
process and analyze the identiﬁed affecting changes and
ensure that all related dependencies are included as well.
Moreover, we consider a sequence of program versions rather
than only two versions, and our algorithm can operate on
both the atomic change level and the text-based commit level.
8.4 Product Line Variant Generation
The software product line (SPL) [61], [62] community faces
similar challenges as we do. An SPL is an efﬁcient means
for generating a family of program variants from a common
code base [19], [63]. Code fragments can be disabled or
enabled based on user requirements, e.g., using “ #ifdef ”
statements in C, often resulting in ill-formed programs.
Therefore, variant generation algorithms need to check the
implementation of SPL to ensure that the generated variants
are well-formed.
K¨astner et. al [63] introduced two basic rules for enforcing
syntactic correctness of product variants, namely, optional-
only and subtree . The optional-only rule prevents removal
of essential language constructs such as class name and
only allows optional entities, e.g., methods or ﬁelds, to
be removed. The subtree rule requires that when an AST
node is removed, all of its children are removed as well,
Our ﬁeld- and method-level AST model and the syntactic
correctness assumption over intermediate versions together
automatically guarantee the satisfaction of the two rules.K¨astner and Apel [19] proposed an extended calculus for
reasoning about the type-soundness of product line variant
programs written in Featherweight Java. They formally
proved that their annotation rules on SPL are complete. Our
COMP DEPreference relation rules are directly inspired by
theirs. We are, however, able to discard some of the rules
since we only deal with ﬁeld- and method-level granularity.
Despite the similarities in syntactic and type safety
requirements on the ﬁnal products, the inputs for both prob-
lems differ. Unlike the SPL variant generation problem where
a single static artifact is given, the semantic slicing algorithm
needs to process a sequence of related yet distinct artifacts
under evolution. And on top of low-level requirements on
program well-formedness, semantic slices also need to satisfy
high-level semantic requirements, i.e., some functionality as
captured by test behaviors.
9 C ONCLUSION AND FUTURE WORK
In this paper, we proposed CS LICER , an efﬁcient semantic
slicing algorithm which resides on top of existing SCM
systems. Given a functionality exercised and veriﬁed by
a set of test cases, CS LICER is able to identify a subset of
the history commits such that applying them results in a
syntactically correct and well-typed program. The computed
semantic slice also captures the interested functional behavior
which guarantees the test to pass.
We have also implemented a novel hunk dependency
algorithm which ﬁlls the gap between language semantic
entities and text-based modiﬁcations. We identiﬁed a number
of sources that can cause imprecision in the slicing process
and addressed them using a combination of static matching
and dynamic enumeration techniques. We carried out several
case studies and empirically evaluated our prototype imple-
mentation on a large number of benchmarks obtained from
open source software projects. We conclude that CS LICER is
effective and scale in practical applications.
We see many avenues for future work. First, it is useful
to extend CS LICER in order to handle distributed change his-
tories where a special treatment for branching and merging
operations is needed. Second, semantic slicing is a part of
our larger goal to enable history-aware feature “copy and
paste” – transferring functional units to arbitrary branches
within the same repository. Semantic slicing is essential for
ﬁnding and extracting existing software functionalities from
change histories. A natural next step is to investigate the
possibility of merging the extracted history slices with other
code bases. The biggest challenge here is to precisely detect
both syntactic and semantic conﬂicts that might occur during
slice integration and search for correct ﬁxes automatically
or interactively. Finally, another interesting direction is to
integrate the CS LICER algorithm with language-aware merge
tools and investigate possible trade-offs.
REFERENCES
[1] Y. Li, J. Rubin, and M. Chechik, “Semantic Slicing of Software
Version Histories,” in Proceedings of the 30th IEEE/ACM International
Conference on Automated Software Engineering , Lincoln, NE, USA,
November 2015, pp. 686–696.
[2] I. Sommerville, Software Engineering (7th Edition) . Pearson Addison
Wesley, 2004.19
[3] Git Version Control System. [Online]. Available: https://git-scm.
com
[4] Apache Subversion (SVN) version control system. [Online].
Available: http://subversion.apache.org
[5] Mercurial Source Control Management System. [Online]. Available:
http://mercurial.selenic.com
[6] J. Rubin, A. Kirshin, G. Botterweck, and M. Chechik, “Managing
Forked Product Variants,” in Proc. of SPLC’12 , 2012, pp. 156–160.
[7] K. Mu s ¸lu, L. Swart, Y. Brun, and M. D. Ernst, “Development History
Granularity Transformations,” in Proceedings of the 30th IEEE/ACM
International Conference on Automated Software Engineering , Lincoln,
NE, USA, November 2015, pp. 697–702.
[8] Y. Li, C. Zhu, J. Rubin, and M. Chechik, “Precise Semantic History
Slicing through Dynamic Delta Reﬁnement,” in Proceedings of
the 31st IEEE/ACM International Conference on Automated Software
Engineering , Singapore, Singapore, September 2016, pp. 495–506.
[9] J. Rubin, A. Kirshin, G. Botterweck, and M. Chechik, “Managing
Forked Product Variants,” in Proceedings of the 16th International
Software Product Line Conference , vol. 1. New York, NY, USA: ACM,
2012, pp. 156–160.
[10] Elasticsearch: Distributed, Open Source Search and Analytics
Engine. [Online]. Available: https://www.elastic.co/products/
elasticsearch
[11] How to Contribute Code to Bitcoin Core. [Online]. Available:
https://bitcoincore.org/en/faq/contributing-code
[12] How to Get Your Change Into the Linux Kernel.
[Online]. Available: https://www.kernel.org/doc/Documentation/
SubmittingPatches
[13] Microsoft Azure: Ways to Contribute. [Online]. Available:
https://azure.github.io/guidelines
[14] F. Tip, “A Survey of Program Slicing Techniques,” Journal of
Programming Languages , vol. 3, pp. 121–189, 1995.
[15] The Diff and Merge Tool that Understands Your Code –
SemanticMerge. [Online]. Available: https://www.semanticmerge.
com
[16] Cow: Semantic Version Control. [Online]. Available: http:
//jelv.is/cow
[17] A. Zeller, “Yesterday, My Program Worked. Today, It Does Not.
Why?” in Proceedings of the 7th European Software Engineering
Conference Held Jointly with the 7th ACM SIGSOFT International
Symposium on Foundations of Software Engineering . London, UK,
UK: Springer-Verlag, 1999, pp. 253–267.
[18] A. Igarashi, B. C. Pierce, and P . Wadler, “Featherweight Java: A
Minimal Core Calculus for Java and GJ,” ACM Transactions on
Programming Languages and Systems , vol. 23, no. 3, pp. 396–450, May
2001.
[19] C. K ¨astner and S. Apel, “Type-Checking Software Product Lines - A
Formal Approach,” in Proceedings of the 23rd IEEE/ACM International
Conference on Automated Software Engineering . Washington, DC,
USA: IEEE Computer Society, 2008, pp. 258–267.
[20] B. Fluri and H. C. Gall, “Classifying Change Types for Qualifying
Change Couplings,” in Proceedings of the 14th IEEE International
Conference on Program Comprehension . IEEE, 2006, pp. 35–45.
[21] S. S. Chawathe, A. Rajaraman, H. Garcia-Molina, and J. Widom,
“Change Detection in Hierarchically Structured Information,” in
Proceedings of the 1996 ACM SIGMOD International Conference on
Management of Data , 1996, pp. 493–504.
[22] G. Rothermel and M. J. Harrold, “A Framework for Evaluating
Regression Test Selection Techniques,” in Proceedings of the 16th
International Conference on Software Engineering . Los Alamitos, CA,
USA: IEEE Computer Society Press, 1994, pp. 201–210.
[23] (2016, March) Understanding Darcs/Patch Theory. [Online].
Available: http://en.wikibooks.org/wiki/Understanding Darcs/
Patch theory
[24] M. Fowler, Refactoring: Improving the Design of Existing Code . Boston,
MA, USA: Addison-Wesley, 1999.
[25] D. Dig, K. Manzoor, R. E. Johnson, and T. N. Nguyen, “Effective
Software Merging in the Presence of Object-Oriented Refactorings,”
IEEE Transactions on Software Engineering , vol. 34, no. 3, pp. 321–335,
May 2008.
[26] D. Dig, C. Comertoglu, D. Marinov, and R. Johnson, “Automated
Detection of Refactorings in Evolving Components,” in Proceedings
of the 20th European Conference on Object-Oriented Programming .
Berlin, Heidelberg: Springer-Verlag, 2006, pp. 404–428.
[27] C. Gorg and P . Weissgerber, “Detecting and visualizing refactorings
from software archives,” in Proceedings of the 13th International
Workshop on Program Comprehension , 2005.[28] D. Kawrykow and M. P . Robillard, “Non-essential Changes in
Version Histories,” in Proceedings of the 33rd International Conference
on Software Engineering . New York, NY, USA: ACM, 2011, pp.
351–360.
[29] E. Murphy-Hill, C. Parnin, and A. P . Black, “How We Refactor,
and How We Know It,” IEEE Transactions on Software Engineering ,
vol. 38, no. 1, pp. 5–18, Jan 2012.
[30] Apache Maven Project. [Online]. Available: https://maven.apache.
org
[31] (2016, March) The try-with-resources Statement. [Online].
Available: https://docs.oracle.com/javase/tutorial/essential/
exceptions/tryResourceClose.html
[32] Type Inference for Generic Instance Creation. [Online].
Available: https://docs.oracle.com/javase/8/docs/technotes/
guides/language/type-inference-generic-instance-creation.html
[33] A Unit Testing Framework for the Java Programming Language.
[Online]. Available: http://junit.org
[34] D. Schuler and A. Zeller, “Assessing Oracle Quality with Checked
Coverage,” in Proceedings of the 4th IEEE International Conference on
Software Testing, Veriﬁcation and Validation . Washington, DC, USA:
IEEE Computer Society, 2011, pp. 90–99.
[35] A. B. Kahn, “Topological Sorting of Large Networks,” Communica-
tions of the ACM , vol. 5, no. 11, pp. 558–562, Nov. 1962.
[36] JaCoCo Java Code Coverage Library. [Online]. Available:
http://www.eclemma.org/jacoco
[37] B. Fluri, M. Wuersch, M. PInzger, and H. Gall, “Change Distilling:
Tree Differencing for Fine-Grained Source Code Change Extraction,”
IEEE Transactions on Software Engineering , vol. 33, no. 11, pp. 725–
743, Nov. 2007.
[38] Apache Byte Code Engineering Library. [Online]. Available:
https://commons.apache.org/proper/commons-bcel
[39] JGit: A Lightweight, Pure Java Library Implementing the Git
Version Control System. [Online]. Available: https://eclipse.org/
jgit
[40] X. Ren, F. Shah, F. Tip, B. G. Ryder, and O. Chesley, “Chianti: A Tool
for Change Impact Analysis of Java Programs,” in Proceedings
of the 19th annual ACM SIGPLAN conference on Object-oriented
Programming, Systems, Languages, and Applications . New York,
NY, USA: ACM, 2004, pp. 432–448.
[41] Apache Hadoop Project. [Online]. Available: https://hadoop.
apache.org
[42] “Commons Collections: The Apache Commons Collections Library,”
https://commons.apache.org/proper/commons-collections.
[43] “Commons Math: The Apache Commons Mathematics Library,”
https://commons.apache.org/proper/commons-math.
[44] “Commons IO: The Apache Commons IO Library,”
https://commons.apache.org/proper/commons-io.
[45] Apache ActiveMQ. [Online]. Available: http://activemq.apache.
org
[46] Developing Maven. [Online]. Available: http://maven.apache.org/
guides/development/guide-maven-development.html
[47] Y. Brun, R. Holmes, M. D. Ernst, and D. Notkin, “Early Detection
of Collaboration Conﬂicts and Risks,” IEEE Transactions on Software
Engineering , vol. 39, no. 10, pp. 1358–1375, Oct. 2013.
[48] A. Zeller and R. Hildebrandt, “Simplifying and Isolating Failure-
inducing Input,” IEEE Transactions on Software Engineering , vol. 28,
no. 2, pp. 183–200, 2002.
[49] T. Zimmermann, P . Weisgerber, S. Diehl, and A. Zeller, “Mining
Version Histories to Guide Software Changes,” in Proceedings of the
26th International Conference on Software Engineering . Washington,
DC, USA: IEEE Computer Society, 2004, pp. 563–572.
[50] K. Herzig and A. Zeller, “The Impact of Tangled Code Changes,”
inProceedings of the 10th Working Conference on Mining Software
Repositories . Piscataway, NJ, USA: IEEE Press, 2013, pp. 121–130.
[51] S. Hayashi, T. Omori, T. Zenmyo, K. Maruyama, and M. Saeki,
“Refactoring Edit History of Source Code,” in Proceedings of the
28th IEEE International Conference on Software Maintenance . IEEE,
September 2012, pp. 617–620.
[52] J.-R. Falleri, F. Morandat, X. Blanc, M. Martinez, and M. Montperrus,
“Fine-grained and Accurate Source Code Differencing,” in Proceed-
ings of the 29th ACM/IEEE International Conference on Automated
Software Engineering , Sep. 2014, pp. 313–324.
[53] P . Bille, “A Survey on Tree Edit Distance and Related Problems,”
Theoretical Computer Science , vol. 337, no. 1-3, pp. 217–239, Jun. 2005.
[54] M. Hashimoto and A. Mori, “Diff/TS: A Tool for Fine-Grained
Structural Change Analysis,” in Proceedings of the 15th Working20
Conference on Reverse Engineering . Antwerp: IEEE, October 2008,
pp. 279–288.
[55] R. S. Arnold, Software Change Impact Analysis . Los Alamitos, CA,
USA: IEEE Computer Society Press, 1996.
[56] J. Law and G. Rothermel, “Whole Program Path-Based Dynamic
Impact Analysis,” in Proceedings of the 25th International Conference
on Software Engineering . IEEE, May 2003, pp. 308–318.
[57] S. Zhang, Z. Gu, Y. Lin, and J. Zhao, “Change impact analysis
for AspectJ programs,” in Proceedings of the 24th IEEE International
Conference on Software Maintenance , September 2008, pp. 87–96.
[58] L. Zhang, M. Kim, and S. Khurshid, “Localizing Failure-inducing
Program Edits Based on Spectrum Information,” in Proceedings of
the 27th International Conference on Software Maintenance . IEEE,
2011, pp. 23–32.
[59] D. C. Kung, J. Gao, P . Hsia, F. Wen, Y. Toyoshima, and C. Chen,
“Change Impact Identiﬁcation in Object Oriented Software Main-
tenance,” in Proceedings of the International Conference on Software
Maintenance . Washington, DC, USA: IEEE Computer Society, 1994,
pp. 202–211.
[60] A. Orso, T. Apiwattanapong, and M. J. Harrold, “Leveraging Field
Data for Impact Analysis and Regression Testing,” in Proceedings of
the 9th European software engineering conference held jointly with 11th
ACM SIGSOFT International Symposium on Foundations of Software
Engineering . New York, NY, USA: ACM, 2003, pp. 128–137.
[61] P . C. Clements and L. Northrop, Software Product Lines: Practices and
Patterns . Addison-Wesley, 2001.
[62] K. Pohl, G. Boeckle, and F. van der Linden, Software Product Line
Engineering : Foundations, Principles, and Techniques . Springer, 2005.
[63] C. K ¨astner, S. Apel, S. Trujillo, M. Kuhlemann, and D. Batory,
“Guaranteeing Syntactic Correctness for all Product Line Variants:
A Language-Independent Approach,” in Proceedings of the 47th
International Conference on Objects, Models, Components, Patterns
(TOOLS EUROPE) , ser. LNBI, vol. 33. Zurich, Switzerland:
Springer Berlin Heidelberg, June 2009, pp. 175–194.
Yi Li is a PhD candidate in the Department of
Computer Science at the University of Toronto.
Yi received his B.Comp. degree with First Class
Honors from the National University of Singa-
pore in 2011 and the M.Sc. degree in Computer
Science from the University of Toronto in 2013.
His research interests include program analysis,
software veriﬁcation, software requirements and
history analysis. His research also addressed
important problems in SMT solving techniques
and artiﬁcial intelligence. His recent work on
software history analysis won an ACM Distinguished Paper Award at
the 30th International Conference on Automated Software Engineering
(ASE’15). Yi Li also served as a member of the Artifact Evaluation
Committee of the 28th International Conference on Computer Aided
Veriﬁcation (CAV’16).
Chenguang Zhu received the B.E. degree in
software engineering from the Harbin Institute
of Technology in 2015. He is working toward
the M.Sc. degree in Computer Science and is
a research assistant working in the Software En-
gineering group in the Department of Computer
Science at the University of Toronto. His current
research interests include software version his-
tory analysis, veriﬁcation, and testing.
Julia Rubin is an Assistant Professor at depart-
ment of Electrical and Computer Engineering at
the University of British Columbia. She received
her PhD in Computer Science from the University
of Toronto and both M.Sc. and B.Sc. degrees
in Computer Science from the Technion. Dur-
ing the 2014-2016 academic years, Julia was
a postdoctoral researcher in the department of
Electrical Engineering and Computer Science at
MIT. Earlier, she spent more than 10 years in
industry, working for a startup company and then
for the IBM Research Lab in Israel, where she was a research staff
member and, part of the time, a research group manager. Julia’s research
interests are in software engineering, program analysis, software security,
and software sustainability, focusing on topics related to ¡¡¡¡¡¡¡ paper.tex
the construction of reliable software in an efﬁcient manner. =======
the construction of reliable software in an efﬁcient manner. ¿¿¿¿¿¿¿
1.19 Julia serves as a member of the program committee for several
major conferences ¡¡¡¡¡¡¡ paper.tex in Software Engineering. She was
co-chair of the program committees of SPLC 2014, ECMFA in 2014, will
serve as a co-chair of FASE 2017, MODELS Doctoral Symposium in
2017, and ICSE Doctoral ======= in Software Engineering. She was
co-chair of the program committees of SPLC’14, ECMFA’14, will serve
as a co-chair of FASE’17, MODELS Doctoral Symposium in 2017, and
ICSE Doctoral ¿¿¿¿¿¿¿ 1.19 Symposium in 2018. Julia also served as
a member-at-large of the TCSE Executive Committee from 2013 to 2016,
and chaired the TCSE Distinguished Women in Science and Engineering
Leadership Award Committee.
Marsha Chechik is Professor and Bell University
Labs Chair in Software Engineering in the De-
partment of Computer Science at the University
of Toronto. Prof. Chechik’s research interests
are in modeling and reasoning about software.
She has authored over 100 papers in formal
methods, software speciﬁcation and veriﬁcation,
computer security and requirements engineering.
Marsha Chechik has been Program Committee
Co-Chair of a number of conferences in veriﬁca-
tion (TACAS’16, VSTTE’16, CONCUR’08) and
software engineering (ASE’14, FASE’09, CASCON’08), and is gearing
up to co-chair the technical program committee of the 2018 International
Conference on Software Engineering (ICSE’18). She has been fortunate
to work with many extremely talented graduate students and postdocs,
some of whom are now conducting research in top universities in Canada,
the US, Chile, Luxembourg, and China.