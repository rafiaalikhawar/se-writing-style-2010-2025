Perceived Language Complexity in GitHub Issue
Discussions and Their Effect on Issue Resolution
David Kavaler∗, Sasha Sirovica∗, Vincent Hellendoorn∗, Raul Aranovich†, Vladimir Filkov∗
University of California at Davis, USA
∗Department of Computer Science†Department of Linguistics
{dmkavaler, sasirovica, vhellendoorn, raranovich, vﬁlkov}@ucdavis.edu
Abstract —Modern software development is increasingly col-
laborative. Open Source Software (OSS) are the bellwether; they
support dynamic teams, with tools for code sharing, communica-
tion, and issue tracking. The success of an OSS project is reliant
on team communication. E.g., in issue discussions, individuals relyon rhetoric to argue their position, but also maintain technical
relevancy. Rhetoric and technical language are on opposite ends
of a language complexity spectrum: the former is stylisticallynatural; the latter is terse and concise. Issue discussions embody
this duality, as developers use rhetoric to describe technical issues.
The style mix in any discussion can deﬁne group culture andaffect performance, e.g., issue resolution times may be longer if
discussion is imprecise.
Using GitHub, we studied issue discussions to understand
whether project-speciﬁc language differences exist, and to what
extent users conform to a language norm. We built project-
speciﬁc and overall GitHub language models to study the effect
of perceived language complexity on multiple responses. Weﬁnd that experienced users conform to project-speciﬁc language
norms, popular individuals use overall GitHub language rather
than project-speciﬁc language, and conformance to project-speciﬁc language norms reduces issue resolution times. We also
provide a tool to calculate project-speciﬁc perceived language
complexity.
I. I NTRODUCTION
Broadly speaking, short words are best...
Winston Churchill
Written and spoken language is part and parcel of modern
software development. Millions of developers use GitHub to
code amazing new software. They collaborate and communi-cate about the code, underlying design and arising issues, cre-ating multi-layered socio-technical communities. Those OSSdevelopers come from different geographical and social back-grounds. They have differing social and cultural identities,
informed by their socioeconomic status, cultural background,and myriad other factors. This inﬂuences how they talk,how they listen and how they understand written and spokenlanguage.
Sociological theories tell us that community membership
must be considered part of an individual’s social identity, andOSS being analogous to a workplace, or a social activity,GitHub and OSS projects take on the roles of communities.In a sociolinguistic sense, GitHub and OSS projects frame theconversations and provide context. Like in other communities,
these too have their own speciﬁc language idiosyncracies,composed of project-speciﬁc terms and related jargon. Dialingin the “correct” linguistic markers when talking to other
developers in the project is a mark of identity as well asstanding. It is natural to ask, then, to what extent are theselinguistic markers apparent on GitHub and within speciﬁcprojects? And do they impact productivity or effectiveness?
Answering such questions is contingent on being able to
measure a) community speciﬁc language idiosyncracies, or
even norms, if they exist, and b) the amount of departurefrom them in any given written or spoken language. There
are many different characteristics of language that can bemeasured. One which is particularly salient in communicationand collaboration is understandability, or simplicity of the
language used. This can be operationalized through variousinformation theoretic measures, especially entropy, as we do
later in the paper. Then, community linguistic norms can bestudied by looking at corpora of text written or spoken bymembers of that community. Models built from those corporacan then deﬁne the community language. Once a linguistic
norm is found, we can use entropy-like measures to compare
the departure of new text from the linguistic model of thenormative language (built from existing corpora of communitycommunication). This understandability as a function of boththe speaker/writer’s language use and the listener/reader’sperception, will be referred to as the perceived language
complexity. It is a notion of how understandable a writer’s
text is to a community of readers.
In GitHub projects, discussions on issues are a natural focus
for a sociolinguistic study of community linguistic norms.Discussions on issues involve multiple developers, and oftenturn into back-and-forth conversations on the merits of theissue and potential solution(s). These conversations are acombination of rhetoric, where people present their argumentsin a narrative style,
1and technical arguments arising from
the project particulars. With the abundance of projects onGitHub, issue discussions provide a rich data source for study-ing linguistic determinants for outcomes relevant to software
engineering.
In this work, we study community speciﬁc language com-
plexity and perceived complexity, measured by Shannon en-
tropy and language models, from issue discussions in tens
1When we refer to rhetoric, we do not identify with the modern colloquial
term, with potential negative connotations. We refer to rhetoric in the
philosophical sense - as the art of argumentation, where one posits a premise
and argues soundly.
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research72
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. of the most starred and followed GitHub projects. The data
consist of 90,722 issues, comprising 456, 669 total posts. We
explore the speciﬁc questions: is there evidence of a standardGitHub perceived language complexity? And does this carry toprojects? Is there migration in perceived language complexitytowards the norm? I.e., do new project participants gravitate
towards the project perceived language complexity norm? And
what may drive this linguistic acculturation? What is the
relationship, if any, between perceived language complexityand issue resolution latency? Remaining mindful of variousconfounds and threats to validity, we provide the followingcontributions:
•We collected a data set comprised of many issue dis-
cussions, including issue post text, user-related metadata,
and constructed a social network deﬁned by @mentions
to other users within and outside the project to determineuser project popularity deﬁned by contributions in issuediscussions.
•We use unique language modeling strategies to createa global GitHub language model, along with nesteddynamic project-speciﬁc language models to identify andstudy perceived language complexity through languagemodel entropy. We ﬁnd that project-speciﬁc languagemodels outperform the global GitHub language model(statistically signiﬁcant), indicating that project-speciﬁc
terms and perceived language complexity exist, and some
projects have more project-speciﬁc perceived languagecomplexity than others.
•Using our language models inside of regression models,we ﬁnd that users gravitate towards the project language
norm and popular users (measured by the number of times
they are @mentioned in issue discussions) are betterrepresented by the GitHub global language model thanproject-speciﬁc language models.
•We also ﬁnd that increased project perceived language
complexity increases issue latency.
•We provide a tool2to calculate perceived language com-
plexity given a user’s input and speciﬁed project.
This paper is an initial foray into language complexity
analysis and its relationship to OSS and communities ofpractice in general. We hope to help bridge the two ﬁelds,providing a coarse study of the links between them.
The remainder of this paper is organized as follows: Sec-
tion II presents theory and related work; Section III out-lines our research questions; Section IV describes our dataand methods; Section V presents our results and discussion;Section VI presents how to apply our results in practice;Section VII presents our conclusions.
II. T
HEORY AND RELA TED WORK
To understand the emergence of OSS project based lin-
guistic communities, as well as the extent of their linguistic
separation, we build theories that draw from a number ofbackgrounds. We ﬁrst tackle the connection between language
2https://github.com/normative-team/normativeand community, by examining two sociolinguistic models:
communities of practice, and speech communities, and thenreason how GitHub communities ﬁt in. Then, we turn ourattention to the ongoing conﬂict between two language com-plexity paradigms, the technical and the rhetorical, as foundin GitHub issue discussions. To model this conﬂict, we startfrom the anthropological theory of “homo narrans”, which can
explain the overly narrative structure of our communications,
even technical ones. We also mention anthropological modelsof acculturation/assimilation that confound language use whencommunities merge. Further, we go over work on entropy as ameasure of language complexity, which underlies the choicesfor our measures. Finally, we review research on collaborationduring software development, which sets the larger contextof building and maintaining relationships and communities inOSS.
A. Speech Communities and Communities of Practice
Sociolinguistics is the study of how societal aspects inﬂu-
ence language, and the effects of language on society [1]. The
two primary theoretrical frameworks regarding the inﬂuence of
communities on language are those of speech communities and
communities of practice. Here, we argue that GitHub projects
do not fall solely into either theoretical framework, but requirea blend of both.
A speech community (SC) describes a group of people who
use language in a way that is mutually accepted among thegroup [1]. To belong to an SC, one must have communitativecompetence [2]; they must speak in a manner that is standardwithin the SC. SC members often speak with specialized termsor form jargon that is understood within the community [3].Another way to express this is the idea of norm conformity,
where the language norm is set by the community and individ-uals are expected (or aspire) to speak in a manner according
to said norm.
First coined by Wenger and Lave [4], the notion of a
community of practice (CoP) is widely discussed and debated
in the ﬁeld of sociolinguistics. Broadly, a CoP can be describedsimilarly to a SC - members use language in a specialized waythat is deﬁned by the group, and group members are expectedto conform to the group language norm. The precise deﬁnition
of this notion has been expanded and expounded over time by
Wenger and many others [5], [6]. Wenger proposed multiplecritical characteristics of a CoP , outlined by Holmes andMeyerhoff [7], contained in Table I
3.
Figure 1 is an example where a member of the out-group (an
end-user) has an issue. They attempt to express their problem,but the contributing member (in-group) is unable to understandthe description, and further asks the end-user to conform toproject-speciﬁc norms (i.e., ﬁll out the standard issue reportform)
4. This is an example where a lack of community norm
conformity causes confusion.
3Note that all the characteristics listed in [7] can be applied to GitHub; we
select the most relevant for brevity.
4Note that the in-group member seems to also be confused by the English
language description, not just the lack of a standard issue report form.
73
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. Fig. 1. Issue thread 14357 from the atom/atom project.
TABLE I
DIFFERENCES BETWEEN A SPEECH COMMUNITY AND A COMMUNITY OF
PRACTICE [7].
Speech Community Community of Practice
Shared norms and evaluations of
norms are required.Shared practices are required.
Shared membership may be de-ﬁned externally. Membership is internally con-structed.
Nothing to say about relationship
between an individual’s group andpersonal identities.Actively constructed dependenceof personal and group identities.
Non-telelogical. Shared social or instrumental goal.
Nothing to say about maintenanceor (de)struction of boundaries be-
tween categories.Boundares are maintained but not
necessarily deﬁned in contrasts
with outgroups.
Acquistion of norms Social process of learning.
Research in CoPs initially focused on the classroom and
businesses in which students and employees are physically co-
located. Important to us is the work of Dub ´eet al., who argued
that members of a CoP do not have to be physically co-located,but can form a “virtual community of practice” [8] with thesame attributes as a standard CoP . In addition, Kleinnijenhuiset al. discuss Networks of Practice, a further reﬁnement of so-
ciolinguistic ideas w.r .t. community organization, speciﬁcally
for online communities [9].
Table I shows partially overlapping characteristics between
SCs and CoPs in each row. Those within the same project sharepractices, as all members are on GitHub. However, member-
ship is not entirely internally constructed. Llamas et al. [10]
present an example of a CoP within the workplace, saying that
individuals regularly engage in scheduled social practices (e.g. ,
business mettings), and mutually deﬁne themselves as CoP
members. Eckert [11] describes SCs as having membershipdeﬁned by “broad and fundamental social categories”, such associoeconomic status, age, and ethnicity.
Fig. 2. Issue thread 28464 from the rails/rails project.
In essence, membership in a CoP is much more structured,
where members of the in-group determine who else is amember, while membership in an SC lies in factors that the
group itself may not control, e.g., cultural background or self-
identiﬁcation. The idea of one’s social identity is of extreme
importance in the ﬁeld of sociolinguistics, where it is well-known that social identity is informed by, e.g., one’s workplace
and social activities, and contributes heavily to language useand variety [7].
On GitHub, developers of a project fall heavily into the CoP
characteristic of internally constructed membership, analogous
to a workplace. The community of users, we argue, is more
like that of an SC than that of a CoP , due to their abilityto self-identify with the project through their use of theproduct. One cannot unilaterally separate the users from thedevelopers of an OSS project, as they are a vital part of the
ecosystem. In fact, our data shows that many issues are not
initially posted by project members ( i.e. committers), but by
general users. Thus, in terms of membership, GitHub projectsshare aspects of an SC and a CoP . Due to the inclusion ofthe users as part of the project ecosystem, one can make asimilar argument for the remaining listed characteristics. Asone can see, GitHub projects are not solely classiﬁed by eithertheoretical framework; the framework necessary to describethe complex interactions in GitHub projects is a feedbackbetween both the ideas of SCs and CoPs.
B. Homo Narrans
GitHub issues serve a direct purpose within the community:
to,e.g., raise concerns or report bugs in the related software.
However, their form is that of discussion; a user posts an issue,
and anyone on GitHub is free to post responses and expresstheir opinion, as in a message board. This provides opportu-nities for rhetorical communication. An example of this canbe seen in Figure 2. The ﬁrst poster posits an issue (“I’d liketo improve the docs...”) and suggests a solution (“I think thatmethods like ... should appear on ...”). Another poster respondswith a counter-idea, and the conversion continues (not shown).
We can imagine that each developer resolves an internal
conﬂict when posting on these discussions, a conﬂict about
how technical, or terse, vs. rhetorical, their post should be.
74
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. The following theory offers an explanation for how such a
conﬂict is likely to be resolved, in a population of developers.
It is accepted that technical language (e.g. , within a technical
manual) is often terse, concise, and prone to the inclusion of
jargon when compared to natural language [3], [12]. One maybelieve that rhetorical language is likewise comparatively terseand concise. However, in his seminal work, Fisher [13] argues(in summary by Hauser [14]) (speciﬁcally w.r .t. rhetorical
communication) that “all humans are deﬁnitively story tellers;they are of the species homo narrans... their communication
assumes the basic form of stories”. Essentially, Fisher arguesthat, by nature, all communication must be seen in the lensof narration, and thus even in rhetorical language the argu-
mentator forms a narrative. Although GitHub issues contain
signiﬁcant amounts of technical language, their function asrhetorical communication separates them from being terse andconcise. GitHub discussions are a push-and-pull effort betweenthe push to use technical language that is relevant to theproject, and the pull to provide a narrative which frames one’s
position through rhetoric. Thus, GitHub discussions may lendtheir language classiﬁcation to be somewhere between purely
technical language and narrative language.
C. Acculturation and Assimilation
Acculturation and assimilation are the processes that result
from the merger of fusion of two cultures [15], typically a
majority one and a minority one [16]. Both are two wayprocesses, but the former term describes an outcome in whichboth cultures change, and the minority one retains culturaldistinctions like language, cuisine, etc. The latter, assimilation,
is when the identifying characteristics of the minority culturegradually get dissolved into the majority culture. Assimilationreduces the communication overhead and conﬂicts inside agroup [17], while acculturation exempliﬁes diversity [15].
Linguistic acculturation in particular is the process of cul-
ture phenomena inﬂuencing the language used. Such accultura-tion can happen fairly rapidly, and has been studied in both im-migrants who learn a second language [18] and communitiesof practice [19]. Communities of practice, especially those onthe social web, are good examples of linguistic acculturationstanding in for performance and status based metrics. Thus,adjusting to the local culture and language fairly rapidly canbe seen as a successful migration into a community.
In the context of developer communities on GitHub, devel-
opers who join new projects may be able to acculturate oreven assimilate if the project culture is sufﬁciently dissimilarfrom their own. Prior results on work/talk culture in OSScommunities [20], [21] points in the direction that strongincompatibility of an individual and a project culture may leadto failure to acculturate, and subsequent departure from theproject.
D. Entropy and Language Complexity
Language models, generally, attempt to predict the next
word in a text given the word’s context (normally the pre-
ceding nwords). The metric by which language modelsare generally evaluated is entropy (described precisely in
Section IV-C). Kolmogorov [22] presented a formulation ofthe connection between entropy and complexity in terms ofpredictability, which has been discussed in length [23], [24].
There has been work in using entropy as a description of
language complexity and style. Kontoyiannis [25] studied hwoentropy represents the complexity of language in the literarysense. They computed entropy per character of the Bible andthe writing of Jane Austen and James Joyce using a specializedlanguage model that utilizes a sliding window of context. Theyfound that a larger context window leads to a lower entropyestimate for the Bible and a higher estimate for Jane Austenand James Joyce novels. Repetitive language style, as in theBible
5, has lower entropy and is easier to understand and read6,
while the style of, e.g., James Joyce seems more complex and
harder to read.
In this work, we hypothesize that norm conformity (as, e.g.,
members of the out-group are assimilated to the in-group) canbe measured in terms of perceived language complexity. Thus,
we deﬁne the following:
A community’s language is deﬁned by sociolinguistic
norms set by members of the group. In this work, we refer
to the GitHub community (global) and nested project-speciﬁc (local) community languages. These communitylanguages are represented by a language models.
Perceived language complexity is the distance between acommunity’s language and a given text. This is quantiﬁedusing the (cross-) entropy distance (Section IV-C).
As perceived language complexity increases, the under-
standing of the text by members of the community willdecrease.
Additionally, we study the difference between global and
local language, represented by separate language models. Juolaand Baayen [26] found success in using cross-entropy to settleauthorship disputes. This lends creedance to our method ofcomparing global GitHub language model entropy to project-speciﬁc language model entropy in describing project differ-ences. Kwon et al. [27] proposed a model for representing
narrative complexity, and use entropy as a quantitative measureto describe this narrative complexity. As we state, based onFisher’s work [13], that rhetoric is really a form of narrative,the decision of Kwon et al. to use entropy as a measure of
narrative complexity supports our approach.
E. Collaborative Work in Software Engineering
Dabbish et al. [28] studied the effects of transparency (e.g. ,
the ability to see everyone’s contributions) on GitHub projects
and found that project members use this transparency to makerich inferences about others’ goals. This lends creedance to
5They state that it is a well-known fact in linguistics that, excluding proper
names, there are only about 500 roots in the Bible
6Though the thoughts and feelings invoked as reported by human subjects
can be much deeper than that of a less repetitive style.
75
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. 4567
Local model Global modelProject−level mean entropy
Fig. 3. Project-level mean post entropy for all projects under study (paired
Welch two sample t-test and paired Wilcoxon rank sum test p<0.001).
0123
0 25k 50k 75k 100k
Within−project issue post number (time ordered; k = thousand)Global model entropy − local model entropy
Per project
Fig. 4. Difference (i.e., separation) between global and local model entropy
per observed post, per project. Smoothed by cubic regression spline (shrink-ing).
the existence of a CoP structure; CoP membership is internally
constructed, and in-group members use transparent details todetermine who should be a member of the in-group. Tsay et
al.[29] examined pull-request discussions and the effects of
various measures (e.g. , test inclusion, prior social interaction
of submitter, etc.) on acceptance. Blincoe et al. [30] found that
popular users attract their followers to new projects. This ﬁnd-
ing is of interest as new developers to projects are likely fromthe out-group, which means they will need to conform to local
linguistic norms in order to communicate effectively. Xuanet al. [20], [21] studied work/talk patterns in OSS projects
and found that these patterns eventually converge in time,
or the users with dissimilar patterns will leave the project.
Similar to ours is the work of Y u et al. [31], who examined
determinants of pull request latency. Our work concerns issuesin general, not just pull-requests, and focuses on languageas a predictor rather than other measures. Given the strong
theoretical backing in sociolinguistics, there is a distinct lack
of work on quantiative linguistic analysis of communicationand collaboration traces (e.g. , issues) and effects on software
engineering outcomes. We present an initial look into thisdomain.
III. R
ESEARCH QUESTIONS
Guided by the discussion in our theory (Section II), we
investigate a number of research questions.
The GitHub community is organized into projects, or its
sub-communities. SC and CoP theory both state that thereexist community language norms, as effective communication
relies on expressing oneself in a manner that resonates with
the community. Thus, we hypothesize that project-speciﬁclanguage differences will work to separate the project lan-guage, analogous to, e.g., software engineering, from the larger
GitHub community language, analogous to, e.g., the Computer
Science academic community. The theory also suggests that
given that GitHub issue discussions are a complex combination
of rhetoric and technical terms, their perceived languagecomplexity should lie somewhere in the spectrum betweenrhetorical narratives (e.g. novels) and technical manuals.
RQ 1: Is there evidence for a standard for the GitHubcommunity language? Does this also carry over to projects,i.e., is there evidence for a project-speciﬁc language?
As posited by SC and CoP theory, those who wish to
communicate effectively within a community must do so ina way that is guided by the community language. Thus, we
expect that the perceived language complexity of posts will
migrate, in time, toward the project norm.
RQ 2a: Is there migration in perceived language complex-
ity, over time, toward the project norm?
As OSS projects are dynamic, there are always users coming
and going. As posited by SC and CoP theory, new members ofthe community will adopt the community-speciﬁc language inorder to communicate effectively and signal themselves partof the in-group. Thus, as a corollary of research question 2a,
we hypothesize that new contributors (be they end-users ordevelopers) to issue discussions are better represented by theglobal language than the local language, and will eventuallyconform to the local norm. In addition, we hypothesize thatexperienced or popular users will be more in-tune to theproject, and that their perceived language complexity willreﬂect this.
RQ 2b: Do users (popular or experienced) conform to their
associated project language? And does perceived language
complexity inﬂuence popularity?
Finally, we seek to identify any effects of perceived lan-
guage complexity on outcomes of particular interest to soft-ware engineering. Speciﬁcally, if the language in a post ordiscussion is more uncommon given the global or local norm
(i.e., less understandable to the community), we hypothesizethat issue resolution latency will increase.
RQ 3: What is the relationship, if any, between perceived
language complexity and issue resolution latency?
IV . D
ATA A N D METHODS
A. Issues
GitHub projects have issue trackers with a rich feature set,
including ticket labeling, milestone tracking, and code tagging.For each project on Github, individuals can open up an issuethread where others can comment and discuss a speciﬁc issue.
GitHub contributions are centered around pull-requests.
Users who wish to add a feature can submit a pull-request
to be incorporated into the main project. GitHub treats pull-requests as a subtype of issues; thus, we consider both ordinaryissues and pull-requests in our data.
76
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. Fig. 5. Overview of dataset formation.
We queried GitHub’s public API using PyGithub.7Our data
is a sample of 48projects from the top 900 most starred
and followed projects.8The number of stars and followers are
proxies for project popularity, and can identify projects likely
to contain enough issues to build robust language models.Figures 1 and 2 depict issue threads. For every post in an issuethread, we gathered the following information: date of post,post body, login of poster, and issue closing time. Post bodieswere used to extract text-related metadata and to build ourlanguage models. In addition to examining comments we alsocollected a variety of metadata from projects. This includedcommit-related metrics: number of lines added and deleted,date of commit, commit author; and user data: full name, timethey joined Github, and location. Figure 5 presents a graph ofour data pipeline.
B. Social Networks
We constructed a social network for each project using
@mentions in their issue comment threads, similar to Zhang
et al. [32]. @mentions are commonly used to either reference
or reply to someone else in a discussion. Figures 1 and 2 show
examples of @mentions in use.
In our networks every edge (u, v)is represented by user
u@mentioning vsomewhere in their post. In addition, we
track changes in network measures over time to construct time-varying networks, used to more accurately represent a user’sattributes at the time of posting. The key metric extracted fromthe social network is indegree, used as a measure of within-
project user popularity. Additional metrics calculated from thenetworks include outdegree, betweenness, and degree central-ity, but were not used in our models due to multicollinearity.
C. Language Models
Measuring perceived language complexity relies on two
components: the training data and the language model. The
choice of training data (or corpus) determines what sequencesof tokens (e.g. , words, punctuation) the language model will
deem likely or unlikely at test time, whereas a good choice oflanguage model should capture useful patterns and ensure that
7https://github.com/PyGithub/PyGithub
8We sampled 50projects, but with replacement, leading to 48projects. 50
projects is the upper bound for what is reasonable to process within a day.rare or novel sequences are not assigned zero probabilities.This combination allows for a distance metric between twocorpora in terms of the (left-to-right) predictability of thetokens t
iin the test corpus, given a language model LM
trained on the training corpus:
dLM(train, test)=|test|/productdisplay
i=1pLM(ti|t1···t i−1)
In practice, instead of a product over probabilities, we use theaverage of (binary) log-probabilities, which yields an entropy
score that reﬂects how many bits of information the averagetoken in the test corpus has w.r .t. the training corpus:
d
LM(train, test )=1
|test||test|/summationdisplay
i=1−log2(pLM(ti|t1···t i−1))
As mentioned, entropy is our measure of perceived language
complexity.
1) Choice of Corpus: To create a representation of the
GitHub community language, we must create a corpus that is
representative of GitHub as a whole. To accomplish this, foreach project, we construct a corpus consisting of all posts in all
other projects. Thus, each project has a corresponding model
that is representative of the GitHub community as a whole;we call this the global model. Note that each project has a
different global model, as we must be careful not to overlap
project-speciﬁc community language with our representationof the whole GitHub community language. On average, eachproject’s global model is trained on ∼25M tokens.
To create a representation of project-speciﬁc languages,
we use each project’s associated global model as a static
representation, and chronologically update a project-speciﬁclanguage model with each observed post from a given project.This yields an evolving model that captures project-speciﬁcidiosyncracies in the presense of a global norm; we call thisthe local model. Each local model is a dynamic mix-model,
mixing a static global model with a dynamic local model. Asthis is an atypical use-case for conventional language models,we implemented it using SLP-Core [33], a library designedfor mixing and dynamically updating language models.
2) Choice of a Model: RNN/LSTM models [34] are some
of the most powerful language models available, but ourcross-project setting is such that it would be computationallyinfeasible to use these models.
9Furthermore, updating the
model per post is poorly supported by these models. Instead,we use n-gram language models, which are proven as powerful
yet simple tools to capture much of the complexity of bothnatural language and source code, even rivaling LSTMs in the
latter. In addition, n-gram models are much faster when used
for prediction than their neural network counterparts, allowing
us to build a tool for fast predictions (Section VI).
Ann-gram model uses the counts of sequences in the
training data. It assigns a conditional probability to a tokengiven the (n−1)preceeding tokens by dividing the count of
9In total, we train and test on ≈1.2 billion tokens of data, twice.
77
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. TABLE II
POSTER MEAN ENTROPY MODELS .U SER -LEVEL OBSERV A TIONS .
Dependent variable:
Poster mean post entropy (local) Poster mean post entropy (global)
Log mean # tokens in posts by user −1.262∗∗∗−0.789∗∗∗
Log # of posts by user −0.0003∗∗∗0.00001
Log mean # URLs in posts by user −0.300∗∗∗−0.129∗∗∗
Log mean # non-English words in posts by user 1.287∗∗∗1.016∗∗∗
Mean Flesch Reading Ease of posts by user −0.002∗∗∗−0.002∗∗∗
Log poster indegree (as of last post) −0.003 −0.047∗∗∗
Log # additions + deletions by poster (as of last post) 0.003 0.019∗∗∗
Log # followers of poster (as of last post) −0.014∗∗∗−0.033∗∗∗
Log # public repos for poster (as of last post) 0.018∗∗∗0.023∗∗∗
Log # public gists by poster (as of last post) 0.010∗−0.013∗∗∗
Log poster’s GitHub age (hours) (as of last post) −0.015∗∗∗−0.013∗∗∗
Log poster’s project age (hours) (as of last post) −0.017∗∗∗−0.020∗∗∗
Intercept 7.084∗∗∗6.713∗∗∗
R20.259 0.238
Note:∗p<0.05;∗∗p<0.01;∗∗∗p<0.001
the whole sequence (length n) by the count of the context
sequence (length n−1), effectively making a Markovian
assumption about word distribution. The model is generally
smoothed by combining this maximum likelihood estimate
probability with those returned by repeating the process fora shorter (e.g. , length n−2) context, until reaching the empty
context, where the model just uses the vocabulary rate ofthe token. V arious smoothing methods propose different rulesto mix longer contexts with shorter ones depending on how“conﬁdent” they are in the longer context. We comparedvarious nfor various smoothing methods and found little
difference in entropies scores across all 3-5-gram models.Thus, we use a 4-gram Witten-Bell smoothed model; a generalpurpose smoothing approach that has historically been usedin text compression [35]. Finally, before training each model,we extract a vocabulary from the training data and replace alltokens seen fewer than 10 times with a generic “unknown”
token in the train and test data, and replace all well-formatted
code segments with a special “code” token. This standardpreprocessing step prevents the models from having to predictcompletely novel or very rare words at test time. On average,the vocabulary used for each model spans 36,164 tokens
(standard deviation 796 tokens).
D. Regressions
To seek answers for our research questions, we use ordinary
least squares (OLS) linear regression. This allows us to inspectthe relationship between our response (dependent variable)and our explanatory variables of interest (predictors orco-
variates, e.g., user age), under the effects of various controls.
When performing regression modeling, one is not only in-
terested in standard “goodness-of-ﬁt” measures (e.g. ,R
2), but
also the results of model diagnostics [36]. In OLS regression,R
2literally measures the percentage of variance captured by
a model. However, a “low” R2value alone does not mean that
the model cannot be inferred from, or that a model is somehow“incorrect” [37], [38], [39], [40]. For example, if the variance
in the dependent variable is large (as is the case for our data),a “low” R
2can mean a great deal, as even a small percentageof a large variance is meaningful.10When searching the space
for the best model, we tried many model types, including
Poisson and Quasi-Poisson generalized linear regression, andmixed-effects models. In all cases, we could construct a modelwith a much higher R
2value (from 60% to70%); however,
these models do not pass diagnostics; they had a multitudeof problems, including heteroscedasticity, autocorrelation ofresiduals, conditionally non-normal errors, etc.
Most important is that our models meet the assumptions
of the given regression approach, as indicated by modeldiagnostics. We take great care to make sure that our modelspass these diagnostics, and thus can be inferred from, ratherthan providing artiﬁcially inﬂated R
2with incorrect models.
When appropriate, we employ log transformations to stabilize
the variance and improve model ﬁt [41]. We remove variablesthat introduce multicollinearity measured by variance inﬂation
factor >5, as multicollinearity reduces inferential ability [41].
We control for many potential confounds through our controls,and make a best-effort to build models that are statisticallyrobust, in spite of what may be considered a “low” R
2value.
Our models can be found in Tables II, III, and IV and arediscussed in Section V.
V. R
ESULTS AND DISCUSSION
In the models in Tables II, III, IV, our variables of interest
are listed in bold; other variables act as controls for confounds,i.e., post readability, measures of time, and code contributions.
All project factors are omitted for brevity.
RQ 1: Is there evidence for a standard for the GitHub
community language? Does this also carry over to projects,i.e., is there evidence for a project-speciﬁc language?
Recall from Section IV that a local model learns to adjust
over time to the community language; a global model is static.As shown in Figure 3, the local language model signiﬁcantlyoutperforms the global model for each project (p<0. 001).
The mean entropy for global models is 6.98bits; 6.15bits for
local models (Cohen’s d1.689 [42] for the difference). We
compare these results to that of n-gram language models built
10The raw value of explained variance is then large.
78
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. 012
0 25k 50k 75k 100k
Within−project issue post number (time ordered; k = thousand)Global model entropy − local model entropy
Per person
(a) Linear ﬁts for per person per post difference between global
and local model entropy over time (50 people with more than
50total posts each sampled to reduce visual clutter).101000
−40 4
Linear fit slopeCount (log scale)
(b) Histogram of slopes following the logic in Figure 6a forpeople with at least 2posts. Bins of width 0.1. Not sampled.
Fig. 6. Plots indicating changes in language complexity over time for each person.
TABLE III
P
OSTER INDEGREE MODEL .U SER -LEVEL OBSERV A TIONS .
User indegree (as of last post)
Mean entropy of posts by user (local) 0.026∗∗∗
Mean entropy of posts by user (global) −0.030∗∗∗
Log mean # URLs in posts by user 0.015∗∗
Log mean # non-English words in posts by user 0.041∗∗∗
Mean Flesch Reading Ease of posts by user −0.001∗∗∗
Log poster outdegree (as of last post) 0.777∗∗∗
Log # additions + deletions by poster (as of last post) 0.030∗∗∗
Log # followers of poster (as of last post) 0.010∗∗∗
Log # public repos for poster (as of last post) −0.011∗∗∗
Log # public gists by poster (as of last post) −0.004
Log poster’s GitHub age (hours) (as of last post) 0.018∗∗∗
Log poster’s project age (hours) (as of last post) 0.071∗∗∗
Intercept −0.131∗∗∗
R20.685
Note:∗p<0.05;∗∗p<0.01;∗∗∗p<0.001
TABLE IV
ISSUE RESOLUTION LA TENCY MODEL .ISSUE -LEVEL OBSERV A TIONS .
Log issue resolution latency (hours)
Mean issue entropy (local) 0.116∗∗∗
Mean issue entropy (global) −0.008
First post entropy (local) 0.023∗
First post entropy (global) 0.004
Log # URLs in ﬁrst post 0.175∗∗∗
Log # non-English words in ﬁrst post 0.220∗∗∗
Flesch Reading Ease of ﬁrst post −0.0001
Log poster indegree 0.001
Hour of day (ﬁrst post) −0.012∗∗∗
Monday (factor) (ﬁrst post) −0.244∗∗∗
Tuesday (factor) (ﬁrst post) −0.311∗∗∗
Wednesday (factor) (ﬁrst post) −0.212∗∗∗
Thursday (factor) (ﬁrst post) −0.209∗∗∗
Friday (factor) (ﬁrst post) −0.099∗
Saturday (factor) (ﬁrst post) 0.125∗
Log # additions + deletions by poster −0.073∗∗∗
Log # followers of poster 0.027∗∗∗
Log # public repos for poster −0.011
Log # public gists by poster 0.009Log poster’s GitHub age (hours) 0.016∗
Log poster’s project age (hours) 0.054∗∗∗
Log number of posts in issue 0.572∗∗∗
Log number of unique posters in issue 1.758∗∗∗
Log ordered time of ﬁrst post (ordinal) −0.096∗∗∗
Intercept −1.207∗∗∗
R20.254
Note:∗p<0.05;∗∗p<0.01;∗∗∗p<0.001
on technical manuals written in English, reported by Hasan
and Ney [43] to lie between 5.58and4.85bits, depending on
model type, and those built on English, reported to be ∼8bits
by Hindle et al. [44] and Tu et. al [45] on a combination ofthe Brown and Gutenberg corpora11. Note that the Brown and
Gutenberg corpora may be considered more terse and concisethan a corpus consisting of, e.g., James Joyce novels, and still
has higher entropy than our models.
We hypothesized in Section II that the post entropy of
GitHub issues would lie somewhere between that of technicalmanuals and narrative language (at a comparative scale), andsee that the performance of both the global and local models liein this range, so our results are consistent with this hypothesis.
Figure 4 shows that projects separate from the global
standard to varying degrees in time, and sometimes from eachother. Recall that global model entropy is static; thus, theplot measures how local model performance changes in timew.r .t. the static global model. The idea is that if the local
model performs better than the global model (lower entropy),the difference (global - local entropy) is larger, leading to alarger y-axis value. As shown, after an initial (slightly chaotic)
period, projects generally settle within a region where the localmodel outperforms the global model, with varying degree,depending on project. This indicates that projects do divergefrom the global norm and towards a local norm; i.e., posts
tend to conform to the local norm.
Research Answer 1: We ﬁnd that we can construct a
language model representative of the norm for GitHuboverall. We ﬁnd that perceived project-speciﬁc languagecomplexity exists, and has signiﬁcantly lower complexitythan the GitHub standard. Projects diverge from theglobal standard to varying degrees, and from each other .
RQ 2a: Is there migration in perceived language complex-ity, over time, toward the project norm?
Figure 6 contains two plots. Figure 6a contains linear model
ﬁts for the separation between global and local model entropiesper post in time, for each person, analogous to Figure 4. If theslope of the ﬁt is positive, the separation between the globaland local model for each person becomes larger in time; the
opposite is true if the ﬁt is negative. Figure 6b is a histogram
of slopes following the logic in Figure 6a for people with at
11The Brown corpus consists of a mixture of news articles, letters, books,
etc. from different genres. The Gutenberg corpus consists of books.
79
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. least 2posts. There is a large concentration of users around a
slope of 0, indicating that most people do not become further
separated by the global and local models in time. Note that
a0slope does not mean that local norm conformity does not
exist, just that the separation may not increase with time. It
could be that these individuals are already at the local projectnorm, and thus have reached a saturation in their ability to
separate themselves from the global norm. It could also be
that once people hit this saturation point, they do not furtherconform to the local norm. This is supported by evidencefrom Posnett et al. that users do not become better at asking
questions over time on Stack Overﬂow [46]. It may also bethat the project is dominated by “tourists”, e.g., those that
brieﬂy “visit” the project, post an issue and participate in asingle discussion, then leave. These individuals would thennot further conform to the local norm, as they are not activeparticipants. To investigate if there is conformance to the localnorm for each person, we build models where we control forconfounds that may otherwise affect interpretation.RQ 2b: Do users (popular, experienced, or otherwise)conform to their associated project language? And doesperceived language complexity inﬂuence popularity?
Table II contains our model describing poster mean entropy
(local and global). As this model has user-level observations,the dependent variables are mean entropy across all posts bythe given user. For each variable with the description “ as of
last post ”, we mean the last post by the user under observation,
not the last post overall. Poster’s GitHub age is the time fromthe creation of their GitHub account to the time of their lastobserved post; poster’s project age is the time from the ﬁrst
authoring of a commit to the time of their last observed post.
Our affectors of interest are popularity (measured by inde-
gree in the @mention network) and measures of age (poster’s
GitHub age and poster’s project age). We argue that popularityis best measured by indegree as an in-link in the @mentionnetwork indicates that someone has mentioned the poster
directly in context of the discussion, serving as a measure
of how many others are aware of the @mentioned user in theproject; thus a proxy for popularity.
For the local model, we see signiﬁcant negative effects of
our age measures. This indicates that older (GitHub age) andmore experienced (project age) users have posts with lowerlocal perceived language complexity, indicating conformity tothe local norm. We see no signiﬁcant effect of popularity orcode contributions for the local model.
For the global model, we see a similar effect of age as
for the local model. However, in the global model, popularityand code contributions are signiﬁcant as well. A more popularposter has lower global perceived language complexity, but ahigher code contributor has a higher global perceived languagecomplexity. The coefﬁcient for code contributions can beexplained by the fact that code contributors are likely in-tunewith the technical details of a project, and thus discuss them
more frequently than others. As technical terms are project-speciﬁc, we expect that someone who uses them more hasa higher global language complexity, indicating divergencefrom the global norm. The negative effect of popularity on
global language complexity is initially somewhat puzzling -why would a popular person within a project use languageﬁtting to the global norm? When analyzed in tandem with ourpopularity models, we can investigate this ﬁnding.
Table III contains our model for describing poster popular-
ity, which also has user-level observations. We see a positive
coefﬁcient for both age metrics, indicating that older andexperienced users generally have higher popularity. However,the effects of poster’s mean entropy have differing signs:positive for the local case, and negative for the global case.In other words, being more like the global norm (decreasedglobal entropy) leads to higher within-project popularity, andbeing more like the local norm (decreased local entropy) leadsto decreased within-project popularity.
There are a variety of social and technical mechanisms that
could explain this. In general, people value outside opinions;
prior work has shown that diversity (geographic, cultural, etc.)
is important in OSS [47], [48]. The explanation could lie in theexistence of cross-project correlated bugs [49], where peoplediscuss issues with members of required library projects.Perhaps those with high indegree are most called-upon bypeople in the out-group (e.g. , users within the SC but not
the CoP), and thus express themselves using language thatout-group individuals would better understand, i.e., the global
language. The precise mechanism behind this phenomenoncould be the focus of future work.
Research Answer 2: We ﬁnd evidence of migration
towards a perceived project-speciﬁc language complexitynorm in time. We ﬁnd that those with higher GitHub and
project age conform to both the local and global norms,while popular users conform more to the global norm. We
ﬁnd that conformity to the local norm leads to decreasedpopularity, while conformity to the global norm leads toincreased popularity.
RQ 3: What is the relationship, if any, between perceivedlanguage complexity and issue resolution latency?
Table IV is our model for describing a software engineering
outcome: issue resolution latency. This model has issue-levelobservations; we look at the ﬁrst post’s entropy and user-levelattributes of the poster, along with summary metrics for therest of the discussion, e.g., mean issue entropy - measured
across the entire issue discussion. This choice was made asthe ﬁrst post in an issue generally explains the issue in detailand sets the stage for the following discussion. When “poster”is in the coefﬁcient name, we are referring to the ﬁrst post(i.e., opening post) in the issue thread. For indegree and code
contribution variables, we use the calculated value at the timeof posting. The “ordered time of ﬁrst post” variable is anordinal time variable that essentially counts the total number
of posts across all issues preceding the observed in the project;e.g., if the ﬁrst post of a given issue is the 100th post overall,
the value of this variable will be 100. Age measures are deﬁned
by the relevant starting time (account creation or ﬁrst authoring
80
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. of a commit) to the time of the post under observation.
The coefﬁcient for local entropy of the ﬁrst post is positive;
those posts which do not conform to the local norm experience
e0.023≈2.3% longer response time (in hours) for each bit
of increased entropy. We see something similar for the meanentropy of the discussion as a whole: each bit of increasedentropy for the local model leads to a e
0.116≈12.2%
longer response time. The effect for the ﬁrst post is small inpercentage, but as resolution latencies increase, the raw effect
can be noticeable. The effect for mean discussion entropy
is more palpable; if an issue would otherwise take 8hours
to resolve, an increase to it of 12.2% may take longer than
a working day, reducing its usefulness, as responsiveness toimportant issues (e.g. , bugs) is vital for software success. Thus,
if one can inﬂuence an issue discussion to be more towards
the local norm, there can be software related beneﬁts.
Research Answer 3: Conformity towards local language
norm can reduce issue resolution times. Increase in
ﬁrst post entropy leads to 2.3% increase in resolution
time, while increase in mean entropy of posts across thediscussion leads to 12.2% increase in resolution time.
VI. I
MPLICA TION FOR SOFTW ARE PRACTICE
Here we distill some practice-related conclusions from our
studies. Some mix of technical and social conversation may beexpected in each conversation in GitHub. From a communityperspective, developers may take less time to acculturate ifthey know ahead of time that such a process is expected, andmay in fact be imminent. This result, in conjunction with priorresults on work/talk culture in OSS communities [21], pointsin the direction that strong incompatibility of an individualand a project culture may lead to a failure to acculturate, andresult in subsequent departure from the project. Developingexplicit acculturation plans, beyond just “learning the ropes”may aid in early discovery of incompatibilities with the projectand also prevent attrition when the learning curve is too steep.
We found that smaller perceived language complexity in
discussions, w.r .t. the local norm, is most helpful to issue
resolution latency. So, an abstract recommendation is to havedevelopers use language in issue discussion as close as pos-sible to the project-speciﬁc language. There are several waysin which this can be made more speciﬁc and even actionable.
Newcomers to a project should be patient and learn the lan-
guage and culture before actively participating in project issuediscussions. This may not be a desirable solution, especiallyin the cases when developer labor is needed to solve an issue,so the next solution may be more appropriate.
Developers should try and use language consistently that is
close in complexity to the project norm. To aid in this we pro-vide a prototype example feedback tool, called Normative,
12
which works with ∼50 projects for now. When a project is
selected, and a developer types a paragraph in the tool’s open
ﬁeld, Normative calculates the perceived language complexity
of that text to the speciﬁed project. Then, the developer can
12https://github.com/normative-team/normativechange that text at will and observe how the distance changes.As per feedback learning theory [50], repeated trials in thistool can result in learning to write closer to the norm. Wewill enhance this tool in the near future with examples fromrelated posts that are similar to the input text.
VII. C
ONCLUSION
In this work, we presented an initial look into perceived
language complexity from an analysis of communication
traces (issue discussions) on GitHub, guided by sociolinguistictheory. We constructed language models representative of a“global” GitHub language, and compared these models tonested “local” project-speciﬁc language models, and saw thatlocal language norms exist, and differ by project. We foundthat users gravitate towards perceived project-speciﬁc languagecomplexity norms, as expected by theory, and that popularusers are interestingly better represented by the global normthan the related local norm. Also, lack of conformity to thelocal norm increases issue resolution times, by a small amount.To our knowledge, this is the ﬁrst work that examines GitHublanguage complexity and conformity towards project languagenorms, and their effects on software engineering outcomes.We hope followup work along these lines will understandbetter how OSS community language style and project-speciﬁcquirks affect outcomes important for OSS success.
Threats to Validity: Some model R
2may be considered
low. As discussed in Sect. IV-D, we favored better model
diagnostics over higher R2, so as to not overﬁt. We did ﬁt
models with R2between 60% and70% , but these did not
pass diagnostics tests and thus are not valid for inference. Wenote that lower goodness of ﬁt does not negate the individualeffects of the independent variables.
We acknowledge that the effect size of some of our vari-
ables are low; e.g.,2.3% increased resolution time for each
additional bit of entropy for the ﬁrst post. Nevertheless, the
raw values can be large if latency is high. They also needto be taken in the larger context where the full discussion, ifmore complex, can add 12.2% to the resolution time for each
additional bit of mean entropy, and the ﬁrst post may set the
stage for similar posts.
As with any model, we have the threat of missing con-
founds. We attempted to control for many aspects that could
affect our outcomes, and performed a best-effort to gather asmuch data as we reasonably could to use in our models.
Our measure of popularity using @mention networks has
no precedent in prior work; others have looked into the social
network of @mentions, but it is not known whether thismeasure of popularity is comprehensive. We chose it due toits natural occurence in issue discussions - our focus here.
Our work is an initial foray into the topic of differing
language use in GitHub. Mixed method approaches, includingqualitative studies, e.g., regarding language use in studied
projects, would strengthen future studies on his topic.
A
CKNOWLEDGMENTS
We thank members of the UC Davis DECAL group for
comments and advice regarding this work.
81
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] S. K. Deckert and C. H. Vickers, An introduction to sociolinguistics:
Society and identity. A&C Black, 2011.
[2] D. Hymes, “Two types of linguistic relativity,” in Sociolinguistics:
proceedings of the UCLA Sociolinguistics Conference , 1964, pp. 114–
67.
[3] K. V arantola, “Special language and general language: Linguistic and
didactic aspects,” Unesco Alsed-LSP Newsletter (1977-2000), vol. 9,
no. 2, 1986.
[4] J. Lave and E. Wenger, Situated learning: Legitimate peripheral partic-
ipation. Cambridge university press, 1991.
[5] E. Wenger, Communities of practice: Learning, meaning, and identity.
Cambridge university press, 1998.
[6] E. Wenger, R. A. McDermott, and W. Snyder, Cultivating communities
of practice: A guide to managing knowledge. Harvard Business Press,
2002.
[7] J. Holmes and M. Meyerhoff, “The community of practice: Theories and
methodologies in language and gender research,” Language in society,
vol. 28, no. 02, pp. 173–183, 1999.
[8] L. Dub ´e, A. Bourhis, and R. Jacob, “The impact of structuring charac-
teristics on the launching of virtual communities of practice,” Journal of
Organizational Change Management, vol. 18, no. 2, pp. 145–166, 2005.
[9] J. Kleinnijenhuis, B. van den Hooff, S. Utz, I. V ermeulen, and M. Huys-
man, “Social inﬂuence in networks of practice: An analysis of organiza-tional communication content,” Communication Research, vol. 38, no. 5,
pp. 587–612, 2011.
[10] C. Llamas, L. Mullany, and P . Stockwell, The Routledge companion to
sociolinguistics. Routledge, 2006.
[11] P . Eckert, “Communities of practice,” Encyclopedia of language and
linguistics, vol. 2, no. 2006, pp. 683–685, 2006.
[12] J. S. Justeson and S. M. Katz, “Technical terminology: some linguistic
properties and an algorithm for identiﬁcation in text,” Natural language
engineering, vol. 1, no. 01, pp. 9–27, 1995.
[13] W. R. Fisher, “Human communication as narration: Toward a philosophy
of reason, value, and action,” 1989.
[14] G. A. Hauser, L. C. Hawes, G. L. Wilson, G. Cheney, P . K. Tompkins,
C. R. Burgchardt, C. J. Stewart, E. C. Clark, J. M. Hogan, F. J. Boster,
G. M. Phillips, R. T. Craig, S. B. Shimanoff, C. Oravec, J. R. Bennett,
E. Smokewood, C. L. Bartow, J. Blankenship, M. P . Graves, R. J.Connors, C. Kramarae, G. Berquist, R. M. Ogles, S. R. Brydon, S. R.
Hankins, W. M. Purcell, V . O’Donnell, B. K. Duffy, S. H. Browne,
M. Weiler, M. Cooper, and W. R. Fisher, “Book reviews,” Quarterly
Journal of Speech, vol. 74, no. 3, pp. 347–400, 1988.
[15] D. Birman, Acculturation and human diversity in a multicultural society.
Jossey-Bass, 1994.
[16] R. H. Teske and B. H. Nelson, “Acculturation and assimilation: A
clariﬁcation,” American Ethnologist, vol. 1, no. 2, pp. 351–367, 1974.
[17] P . G. Zimbardo, “Involvement and communication discrepancy as de-
terminants of opinion conformity.” The Journal of Abnormal and Social
Psychology, vol. 60, no. 1, p. 86, 1960.
[18] E. P . Dozier, “Two examples of linguistic acculturation: The yaqui of
sonora and arizona and the tewa of new mexico,” Language, vol. 32,
no. 1, pp. 146–157, 1956.
[19] M. Lea, D. Barton, and K. Tusting, “Communities of practice in higher
education,” Beyond communities of practice: Language, power and
social context, pp. 180–197, 2005.
[20] Q. Xuan, M. Gharehyazie, P . T. Devanbu, and V . Filkov, “Measuring
the effect of social communications on individual working rhythms: Acase study of open source software,” in Social Informatics (SocialInfor-
matics), 2012 International Conference on. IEEE, 2012, pp. 78–85.
[21] Q. Xuan, P . Devanbu, and V . Filkov, “Converging work-talk patterns in
online task-oriented communities,” PloS
 one, vol. 11, no. 5, p. e0154324,
2016.
[22] A. N. Kolmogorov, “Three approaches to the quantitative deﬁnition
oﬁnformation’,” Problems of information transmission, vol. 1, no. 1,
pp. 1–7, 1965.
[23] T. M. Cover, P . Gacs, and R. M. Gray, “Kolmogorov’s contributions
to information theory and algorithmic complexity,” The annals of
probability, vol. 17, no. 3, pp. 840–865, 1989.
[24] T. M. Cover and J. A. Thomas, Elements of information theory. John
Wiley & Sons, 2012.
[25] I. Kontoyiannis, The complexity and entropy of literary styles . Depart-
ment of Statistics, Stanford University, 1997.[26] P . Juola and R. H. Baayen, “A controlled-corpus experiment in author-
ship identiﬁcation by cross-entropy,” Literary and Linguistic Computing,
vol. 20, no. Suppl, pp. 59–67, 2005.
[27] H. Kwon, H. T. Kwon, and W. C. Y oon, “An information-theoretic
evaluation of narrative complexity for interactive writing support,”Expert Systems with Applications, vol. 53, pp. 219–230, 2016.
[28] L. Dabbish, C. Stuart, J. Tsay, and J. Herbsleb, “Social coding in
github: transparency and collaboration in an open software repository,”
inProceedings of the ACM 2012 conference on Computer Supported
Cooperative Work. ACM, 2012, pp. 1277–1286.
[29] J. Tsay, L. Dabbish, and J. Herbsleb, “Inﬂuence of social and technical
factors for evaluating contribution in github,” in Proceedings of the 36th
international conference on Software engineering. ACM, 2014, pp.
356–366.
[30] K. Blincoe, J. Sheoran, S. Goggins, E. Petakovic, and D. Damian,
“Understanding the popular users: Following, afﬁliation inﬂuence andleadership on github,” Information and Software Technology, vol. 70,
pp. 30–39, 2016.
[31] Y . Y u, H. Wang, V . Filkov, P . Devanbu, and B. V asilescu, “Wait
for it: Determinants of pull request evaluation latency on github,” inMining Software Repositories (MSR), 2015 IEEE/ACM 12th Working
Conference on. IEEE, 2015, pp. 367–371.
[32] Y . Zhang, H. Wang, G. Yin, T. Wang, and Y . Y u, “Exploring the use
of@-mention to assist software development in github,” in Proceedings
of the 7th Asia-Paciﬁc Symposium on Internetware. ACM, 2015, pp.
83–92.
[33] V . J. Hellendoorn and P . Devanbu, “Are deep neural networks the best
choice for modeling source code?” in Proceedings of the 2017 11th
Joint Meeting on F oundations of Software Engineering , ser. ESEC/FSE
2017. New Y ork, NY , USA: ACM, 2017, pp. 763–773. [Online].Available: http://doi.acm.org/10.1145/3106237.3106290
[34] Y . Bengio, R. Ducharme, P . Vincent, and C. Jauvin, “A neural proba-
bilistic language model,” Journal of machine learning research, vol. 3,
no. Feb, pp. 1137–1155, 2003.
[35] S. F. Chen and J. Goodman, “An empirical study of smoothing tech-
niques for language modeling,” in Proceedings of the 34th annual
meeting on Association for Computational Linguistics. Association
for Computational Linguistics, 1996, pp. 310–318.
[36] D. A. Belsley, E. Kuh, and R. E. Welsch, Regression diagnostics:
Identifying inﬂuential data and sources of collinearity. John Wiley
& Sons, 2005, vol. 571.
[37] F. L. Schmidt and J. E. Hunter, Methods of meta-analysis: Correcting
error and bias in research ﬁndings. Sage publications, 2014.
[38] M. Hu, “What does it mean to have a low r-squared? a warning about
misleading interpretation,” http://humanvarieties .org/2014/03/31/what-
does-it-mean-to-have-a-low-r-squared-a-warning-about-misleading-interpretation/#more-3185. Human V arieties, 2014.
[39] P . Birnbaum, “On correlation, r, and r-squared,” http:
//blog. philbirnbaum.com/2006/08/on-
 correlation-r-and-r-squared.html.
Sabermetric Research, 2006.
[40] P . Birnbaum, “r-squared abuse,” http://blog.philbirnbaum.com/2007/10/
r-squared-abuse.html. Sabermetric Research, 2007.
[41] J. Cohen, Applied multiple regression/correlation analysis for the be-
havioral sciences. Lawrence Erlbaum, 2003.
[42] J. Cohen, “Statistical power analysis for the behavioral sciences (revised
ed.),” 1977.
[43] S. Hasan and H. Ney, “Clustered language models based on regular
expressions for smt,” in Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT). Citeseer, 2005.
[44] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P . Devanbu, “On the
naturalness of software,” in Software Engineering (ICSE), 2012 34th
International Conference on. IEEE, 2012, pp. 837–847.
[45] Z. Tu, Z. Su, and P . Devanbu, “On the localness of software,” in
Proceedings of the 22nd ACM SIGSOFT International Symposium on
F oundations of Software Engineering. ACM, 2014, pp. 269–280.
[46] D. Posnett, E. Warburg, P . Devanbu, and V . Filkov, “Mining stack
exchange: Expertise is evident from initial contributions,” in Social In-
formatics (SocialInformatics), 2012 International Conference on. IEEE,
2012, pp. 199–204.
[47] B. V asilescu, A. Capiluppi, and A. Serebrenik, “Gender, representation
and online participation: A quantitative study of stackoverﬂow,” in
Social Informatics (SocialInformatics), 2012 International Conference
on. IEEE, 2012, pp. 332–338.
82
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. [48] B. V asilescu, D. Posnett, B. Ray, M. G. van den Brand, A. Serebrenik,
P . Devanbu, and V . Filkov, “Gender and tenure diversity in github teams,”
inProceedings of the 33rd Annual ACM Conference on Human Factors
in Computing Systems. ACM, 2015, pp. 3789–3798.
[49] W. Ma, L. Chen, X. Zhang, Y . Zhou, and B. Xu, “How do developers
ﬁx cross-project correlated bugs?” To be presented at ICSE, 2017.[50] H. Pashler, N. J. Cepeda, J. T. Wixted, and D. Rohrer, “When does feed-
back facilitate learning of words?” Journal of Experimental Psychology:
Learning, Memory, and Cognition, vol. 31, no. 1, p. 3, 2005.
83
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:47:57 UTC from IEEE Xplore.  Restrictions apply. 