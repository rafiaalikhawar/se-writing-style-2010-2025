Transfer Learning for Performance Modeling of
Conﬁgurable Systems: An Exploratory Analysis
Pooyan Jamshidi
Carnegie Mellon University, USANorbert Siegmund
Bauhaus-University Weimar, GermanyMiguel Velez, Christian K ¨astner
Akshay Patel, Yuvraj Agarwal
Carnegie Mellon University, USA
Abstract —Modern software systems provide many conﬁgura-
tion options which signiﬁcantly inﬂuence their non-functional
properties. To understand and predict the effect of conﬁgurationoptions, several sampling and learning strategies have beenproposed, albeit often with signiﬁcant cost to cover the highlydimensional conﬁguration space. Recently, transfer learning hasbeen applied to reduce the effort of constructing performancemodels by transferring knowledge about performance behavioracross environments. While this line of research is promising tolearn more accurate models at a lower cost, it is unclear whyand when transfer learning works for performance modeling. Toshed light on when it is beneﬁcial to apply transfer learning, weconducted an empirical study on four popular software systems,varying software conﬁgurations and environmental conditions,such as hardware, workload, and software versions, to identifythe key knowledge pieces that can be exploited for transfer
learning. Our results show that in small environmental changes
(e.g., homogeneous workload change), by applying a lineartransformation to the performance model, we can understandthe performance behavior of the target environment, while forsevere environmental changes (e.g., drastic workload change) wecan transfer only knowledge that makes sampling more efﬁcient,e.g., by reducing the dimensionality of the conﬁguration space.
Index T erms—Performance analysis, transfer learning.
I. I NTRODUCTION
Highly conﬁgurable software systems, such as mobile apps,
compilers, and big data engines, are increasingly exposed to
end users and developers on a daily basis for varying use cases.Users are interested not only in the fastest conﬁguration butalso in whether the fastest conﬁguration for their applicationsalso remains the fastest when the environmental situation hasbeen changed. For instance, a mobile developer might beinterested to know if the software that she has conﬁguredto consume minimal energy on a testing platform will alsoremain energy efﬁcient on the users’ mobile platform; or, ingeneral, whether the conﬁguration will remain optimal whenthe software is used in a different environment (e.g. , with a
different workload, on different hardware).
Performance models have been extensively used to learn
and describe the performance behavior of conﬁgurable sys-tems [15], [19], [21], [23], [33], [43]–[45], [54], [61], [63].However, the exponentially growing conﬁguration space, com-plex interactions, and unknown constraints among conﬁgura-tion options [56] often make it costly and difﬁcult to learn
an accurate and reliable performance model. Even worse,
existing techniques usually consider only a ﬁxed environment(e.g., ﬁxed workload, ﬁxed hardware, ﬁxed versions of the
dependent libraries); should that environment change, a newperformance model may need to be learned from scratch.This strong assumption limits the reusability of performancemodels across environments. Reusing performance models or
Fig. 1: Transfer learning is a form of machine learning that takes
advantage of transferable knowledge from source to learn an accurate,reliable, and less costly model for the target environment.
their byproducts across environments is demanded by many
application scenarios, here we mention two common scenarios:
•Scenario 1: Hardware change: The developers of a soft-ware system performed a performance benchmarking of thesystem in its staging environment and built a performancemodel. The model may not be able to provide accuratepredictions for the performance of the system in the actualproduction environment though (e.g. , due to the instability
of measurements in its staging environment [6], [30], [38]).
•Scenario 2: Workload change: The developers of a databasesystem built a performance model using a read-heavyworkload, however, the model may not be able to provideaccurate predictions once the workload changes to a write-heavy one. The reason is that if the workload changes,different functions of the software might get activated (moreoften) and so the non-functional behavior changes, too.
In such scenarios, not every user wants to repeat the costly
process of building a new performance model to ﬁnd asuitable conﬁguration for the new environment. Recently, theuse of transfer learning (cf. Figure 1) has been suggestedto decrease the cost of learning by transferring knowledgeabout performance behavior across environments [7], [25],[51]. Similar to humans that learn from previous experienceand transfer the learning to accomplish new tasks easier,here, knowledge about performance behavior gained in oneenvironment can be reused effectively to learn models forthe changed environments with a lower cost. Despite itssuccess, it is unclear why and when transfer learning works
for performance analysis in highly conﬁgurable systems.
978-1-5386-2684-9/17/$31.00 c/circlecopyrt2017 IEEEASE 2017, Urbana-Champaign, IL, USA
T echnical Research497
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. To understand the why and when, in this paper, we con-
duct an exploratory empirical study, comparing performance
behavior of highly conﬁgurable systems across environmentalconditions (changing workload, hardware, and software ver-
sions), to explore what forms of knowledge can be commonly
exploited for performance modeling and analysis. Specif-
ically, we explore how performance measures and modelsacross the source and target of an environmental change arerelated. The notion of relatedness across environments gives
us insights to consolidate common knowledge that is shared
implicitly between the two environments, from knowing entire
performance distributions to knowing about the best or
invalid conﬁgurations, or knowing inﬂuential conﬁguration
options, or knowing about important interactions. The various
forms of shared knowledge, that we discovered in this em-
pirical study, provide opportunities to develop novel transferlearning that is not only based on correlation concept but alsomore diverse forms of similarities across environments.
More speciﬁcally, we explore several hypotheses about
the notion of common knowledge across environments. Ourhypotheses start with very obvious relationships ( e.g., corre-
lation) that can be easily exploited, but range toward moresubtle relationships (e.g. , inﬂuential options or invalid regions
remain stable) that can be explored with more advanced
transfer learning techniques yet to be developed. We tested ourhypotheses across 36 environmental changes in4 conﬁgurable
systems that have been selected purposefully covering different
severities and varieties. For instance, we selected simple
hardware changes (by changing computing capacity) as well assevere changes (by changing hardware from desktop to cloud).
Our results indicate that some knowledge about performance
behavior can be transferred even in the most severe changes we
explored, and that transfer learning is actually easy for manyenvironmental changes. We observed that, for small changes,we can frequently transfer performance models linearly acrossenvironments, while for severe environmental changes, we canstill transfer partial knowledge, e.g., information about inﬂuen-
tial options or regions with invalid conﬁgurations, that can stillbe exploited in transfer learning, for example, to avoid certainregions when exploring a conﬁguration space. Overall, ourresults are encouraging to explore transfer learning further forbuilding performance models, showing broad possibilities ofapplying transfer learning beyond the relatively small changesexplored in existing work (e.g. , small hardware changes [51],
low ﬁdelity simulations [25], similar systems [7]).
Overall, our contributions are the following:
•We formulate a series of hypotheses to explore the presenceand nature of common, transferable knowledge betweena source and a target environment, ranging from easilyexploitable relationships to more subtle ones.
•We empirically investigate performance models of 4 conﬁg-urable systems before and after 36 environmental changes.We performed a thorough exploratory analysis to understandwhy and when transfer learning works.
•We discuss general implications of our results for perfor-mance modeling of conﬁgurable software systems.
•We release the supplementary material including data ofseveral months of performance measurements, and scriptsfor replication: https://github.com/pooyanjamshidi/ase17.II. I
NTUITION
Understanding the performance behavior of conﬁgurable
software systems can enable (i) performance debugging [14],
[44], (ii) performance tuning [16], [20], [21], [32], [33], [36],[47], [51], [54], (iii) design-time evolution [2], [24], or (iv)runtime adaptation [10]–[12], [19], [25], [26]. A commonstrategy to build performance models is to use some formof sensitivity analysis [42] in which the system is executedrepeatedly in different conﬁgurations and machine learningtechniques are used to generalize a model that explains theinﬂuence of individual options or interactions [15], [44], [51].
In this paper, we are interested in how a performance model
for a conﬁgurable system changes when we deploy the systemin a different environment. To this end, we distinguish be-tween conﬁguration options – parameters that users can tweak
the system to select functionality or make tradeoffs among
performance, quality, and other attributes – and environment
changes – differences in how the system is deployed and used
in terms of workload, hardware, and version. If a performancemodel remains relatively stable across environments (e.g. , the
top conﬁgurations remain the top conﬁgurations, the most in-
ﬂuential options, and interactions remain most inﬂuential), we
can exploit this stability when learning performance modelsfor new environments. Instead of building the model fromscratch (as often exhaustively measuring the same conﬁg-urations on a new environment), we can reuse knowledgegathered previously for other environments in a form oftransfer learning [7], [39], [50]. That is, we can develop
cheaper, faster and more accurate performance models that
allow us to make predictions and optimizations of performancein changing environments [25].
For example, consider an update to faster hardware. We
would often expect that the system will get faster, but willdo so in a nearly uniform fashion. However, we may expectthat options that cause a lot of I/O operations (e.g. , a backup
feature) may beneﬁt less from a faster CPU than other options;so not all environment changes will cause uniform changes. Iftransfer across hardware is indeed usually easy, this encour-ages, for example, scenarios in which we learn performancemodels ofﬂine on cheap hardware and transfer it to the realsystem with few expensive measurements for adjustment. Thequestion is what kind of knowledge can be exploited acrossenvironments in practice, with simple or more advanced formsof transfer learning. Speciﬁcally, we ask whether there existscommon information (i.e., transferable/reusable knowledge,
c.f., Figure 1) that applies to both source and target environ-
ments and, therefore, can be carried over across environments.
A. Environmental Changes
Let us ﬁrst introduce what we mean by an environment, the
key concept that is used throughout this paper. An environ-
mental condition for a conﬁgurable system is determined byits hardware, workload, and software version. (i) Hardware:
The deployment conﬁguration in which the software systemis running. (ii) Workload: The input of the system on which
it operates on. (iii) V ersion: The state of the code base at a
certain point in time. Of course, other environmental changesmight be possible (e.g. , JVM upgrade). But, we limit this study
to this selection as we consider the most common changes inpractice that affect performance behavior of systems.
498
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. B. Preliminary Concepts
In this section, we provide deﬁnitions of concepts that we
use throughout this study. The formal notations enable us to
concisely convey concepts throughout the paper.
1) Conﬁguration and Environment Space: LetCiindicate
thei-th conﬁguration option of a system A, which is either
enabled or disabled (the deﬁnitions easily generalize to non-
boolean options with ﬁnite domains). The conﬁguration spaceis a Cartesian product of all options C=Dom(C
1)×···×
Dom(Cd), whereDom(Ci)={0,1}anddis the number of
options. A conﬁguration is then a member of the conﬁguration
space where all the options are either enabled or disabled.
We describe an environmental condition eby 3 variables
e=[h,w,v]drawn from a given environment space E=
H×W×V, where each member represents a set of possible
values for the hardware h, workload w, and system version
v. We use notation ec:[h,w 1→w2,v]as shorthand for an
environment change from workload w1to workload w2where
hardware and version remain stable.
2) Performance Model: Given a software system Awith
conﬁguration space Cand environment space E,aperformance
model is a black-box function f:C×E→ Rthat maps
each conﬁguration c∈C ofAin an environment e∈E to
the performance of the system. To construct a performance
model, we run Ain a ﬁxed environmental condition e∈E
on various conﬁgurations ci∈C, and record the resulting
performance values yi=f(ci,e)+/epsilon1iwhere/epsilon1i∼N(0,σi)is
the measurement noise corresponding to a normal distributionwith zero mean and variance σ
2
i. The training data for learning
a performance model for system Ain environment eis then
Dtr={(ci,yi)}n
i=1, wherenis the number of measurements.
3) Performance Distribution: We can and will compare
the performance models, but a more relax representation
that allows us to assess the potentials for transfer learningis the empirical performance distribution. The performance
distribution is a stochastic process, pd:E→Δ(R), that
deﬁnes a probability distribution over performance measures
for environmental conditions of a system. To construct a per-formance distribution for a system Awith conﬁguration space
C, we ﬁt a probability distribution to the set of performance
values,D
e={yi},e∈E, using kernel density estimation [4]
(in the same way as histograms are constructed in statistics).
4) Inﬂuential Option: At the level of individual conﬁg-
uration options, we will be interested in exploring whetheroptions have an inﬂuence on the performance of the system
in either environment; not all options will have an impact onperformance in all environments. We introduce the notion of
aninﬂuential option to describe a conﬁguration option that has
a statistically signiﬁcant inﬂuence on performance.
5) Options Interaction: The performance inﬂuence of in-
dividual conﬁguration options may not compose linearly. Forexample, while encryption will slow down the system due toextra computations and compression can speed up transfer overa network, combining both options may lead to surprising ef-fects because encrypted data is less compressible. In this work,we will look for interactions of options as nonlinear effects
where the inﬂuence of two options combined is different fromthe sum of their individual inﬂuences [44], [45].
6) Invalid Conﬁguration: We consider a conﬁguration as
invalid if it causes a failure or a timeout.C. Transferable Knowledge
As depicted in Figure 1, any sort of knowledge that can
beextracted from the source environment and can contribute
to the learning of a better model (i.e., faster, cheaper, more
accurate, or more reliable) in the target environment is con-sidered as transferable knowledge (or reusable knowledge [1]).There are several pieces of knowledge we can transfer, such as(i) classiﬁcation or regression models, (ii) dependency graphsthat represent the dependencies among conﬁgurations, and (iii)option interactions in order to prioritize certain regions in theconﬁguration space. For transferring the extracted knowledge,we need a transfer function that transforms the source model
to the target model: tf:f(·,e
s)→f(·,et). In its simplest
form, it can be a linear mapping that transforms the source
model to the target: f(·,et)=α×f(·,es)+β , whereα,β
are learned using observations from both environments [51].More sophisticated transfer learning exists that reuses source
data using learners such as Gaussian Processes (GP) [25].
III. R
ESEARCH QUESTIONS AND METHODOLOGY
A. Research Questions
The overall question that we explore in this paper is “why
and when does transfer learning work for conﬁgurable soft-
ware systems?” Our hypothesis is that performance models in
source and target environments are usually somehow “related.”To understand the notion of relatedness that we commonlyﬁnd for environmental changes in practice, we explore severalresearch questions (each with several hypotheses), from strongnotions of relatedness ( e.g., linear shift) toward weaker ones
(e.g., the stability of inﬂuential options):
RQ1: Does the performance behavior stay consistent across
environments? (Section IV)
If we can establish with RQ1 that linear changes across
environments are common, this would be promising for trans-fer learning because even simple linear transformations canbe applied. Even if not all environment changes may beamendable to this easy transfer learning, we explore what
kind of environment changes are more amendable to transferlearning than others.
RQ2: Is the inﬂuence of conﬁguration options on perfor-
mance consistent across environments? (Section V)
For cases in which easy transfer learning are not possible,
RQ2 concerns information that can be exploited for trans-fer learning at the level of individual conﬁguration options.Speciﬁcally, we explore how commonly the inﬂuential optionsremain stable across environment changes.
RQ3: Are the interactions among conﬁguration options
preserved across environments? (Section VI)
In addition to individual options in RQ2, RQ3 concerns
interactions among options, that, as described above, can oftenbe important for explaining the effect of performance varia-tions across conﬁgurations. Again, we explore how commonlyinteractions are related across environment changes.
RQ4: Are the conﬁgurations that are invalid in the source
environment with respect to non-functional constraints alsoinvalid in the target environment? (Section VII)
Finally, RQ4 explores an important facet of invalid con-
ﬁgurations: How commonly can we transfer knowledge about
invalid conﬁgurations across environments? Even if we cannot
499
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. transfer much structure for the performance model otherwise,
transferring knowledge about conﬁgurations can guide learn-ing in the target environment on the relevant regions.
B. Methodology
Design: We investigate changes of performance models
across environments. Therefore, we need to establish the per-
formance of a system and how it is affected by conﬁgurationoptions in multiple environments. To this end, we measure theperformance of each system using standard benchmarks andrepeated the measurements across a large number of conﬁgu-rations. We then repeat this process for several changes to theenvironment: using different hardware, different workloads,and different versions of the system. Finally, we perform theanalysis of relatedness by comparing the performance and howit is affected by options across environments. We performcomparison of a total of 36 environment changes.
Analysis: For answering the research questions, we for-
mulate different assumptions about the relatedness of the
source and target environments as hypotheses – from stronger
to more relaxed assumptions. For each hypothesis, we deﬁneone or more metrics and analyze 36 environment changes infour subject systems described below. For each hypothesis, wediscuss how commonly we identify this kind of relatednessand whether we can identify classes of changes for whichthis relatedness is characteristic. If we ﬁnd out that for anenvironmental change a stronger assumption holds, it meansthat a more informative knowledge is available to transfer.
Severity of environmental changes: We purposefully se-
lect environment changes for each subject system with the goal
of exploring many different kinds of changes with differentexpected severity of change. With a diverse set of changes,
we hope to detect patterns of environment changes that havesimilar characteristics with regard to relatedness of perfor-mance models. We expect that less severe changes lead to
more related performance models that are easier to exploit in
transfer learning than more severe ones. For transparency, werecorded the expected severity of the change when selecting
environments, as listed in Table II, on a scale from small
change to very large change. For example, we expect a smallvariation where we change the processor of the hardware toa slightly faster version, but expect a large change when wereplace a local desktop computer by a virtual machine in thecloud. Since we are neither domain experts nor developers ofour subject systems, recording the expected severity allows usto estimate how well intuitive judgments can (eventually) bemade about suitability for transfer learning and it allows us tofocus our discussion on surprising observations.
C. Subject Systems
In this study, we selected four conﬁgurable software systems
from different domains, with different functionalities, and
written in different programming languages (cf. Table I).
SPEAR is an industrial strength bit-vector arithmetic deci-
sion procedure and a Boolean satisﬁability (SAT) solver. It
is designed for proving software veriﬁcation conditions andit is used for bug hunting. We considered a conﬁgurationspace with 14 options that represent heuristics for solving the
problems and therefore affect the solving time. We measuredTABLE I: O VERVIEW OF THE REAL-WORLD SUBJECT SYSTEMS
System Domain d |C| |H || W|| V|
SPEAR SAT solver 14 16 384 3 4 2
x264 Video encoder 16 4 000 2 3 3
SQLite Database 14 1 000 2 14 2
SaC Compiler 50 71 267 1 10 1
d: conﬁguration options; C: conﬁgurations; H: hardware environments; W: analyzed
workload; V: analyzed versions.
how long it takes to solve a SAT problem in all 16,384 conﬁgu-
rations in multiple environments: four different SAT problemswith different difﬁculty serve as workload, measured on threehardware system, with two versions of the solver as listed inTable II. The difﬁculty of the workload is characterized by theSAT problem’s number of variables and clauses.
x264 is a video encoder that compresses video ﬁles with
a conﬁguration space of 16 options to adjust output quality,encoder types, and encoding heuristics. Due to the size of theconﬁguration space, we measured a subset of 4000 sampledrandomly conﬁgurations. We measured the time needed toencode three different benchmark videos on two differenthardware systems and for three versions as listed in Table II.Each benchmark consists of a raw video with different qualityand size and we expect that options related to optimizingencoding affect the encoding time differently. We judgedexpected severity of environmental changes based on thedifference between quality and size of benchmark videos.
SQLite is a lightweight relational database management
system, embedded in several browsers and operating systems,with 14 conﬁguration options that change indexing and fea-tures for size compression useful in embedded systems, buthave performance impact. We expect that some options affectcertain kinds of workload ( e.g., read-heavy rather than write-
heavy workloads) more than others. We have measured 1000randomly selected conﬁgurations on two hardware platformsfor two versions of the database system; as workload, we haveconsidered four variations of queries that focus on sequentialreads, random reads, sequential write, and batch writes.
SaC is a compiler for high-performance computing [41].
The SaC compiler implements a large number of high-level
and low-level optimizations to tune programs for efﬁcientparallel executions conﬁgurable with 50 options controllingoptimizations such as function inlining, constant folding, andarray elimination. We measure the execution time of a programcompiled in 71,267 randomly selected conﬁgurations to assessthe performance impact of
SaC’s options. As workloads, we
select 10 different demo programs shipped with SaC, each
computationally intensive, but with different characteristics.Workloads include Monte Carlo algorithms such as
pﬁlter with
multiple optimizable loops as well as programs heavily based
on matrix operations like srad.
To account for measurement noise, we have measured each
conﬁguration of each system and environment 3 times and
used the mean for the analyses. While many performance andquality measures can be analyzed, our primary performancemetric is wall-clock execution time, which is captured differ-ently for each systems in Table I: execution time, encodingtime, query time, and analysis time.
500
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. IV . P ERFORMANCE BEHA VIOR CONSISTENCY (RQ1)
Here, we investigate the relatedness of environments in the
entire conﬁguration space. We start by testing the strongest
assumption (i.e., linear shift), which would enable an easytransfer learning (H1.1). We expect that the ﬁrst hypothesisholds only for simple environmental changes. Therefore, wesubsequently relax the hypothesis to test whether and whenthe performance distributions are similar (H1.2), whether theranking of conﬁgurations (H1.3), and the top/bottom conﬁgu-rations (H1.4) stay consistent. Table II summarizes the results.
H1.1: The relation of the source response to the target is a
constant or proportional shift.
Importance. If the target response is related to the source
by a constant or proportional shift, it is trivial to understandthe performance behavior for the target environment using themodel that has already been learned in the source environment:We need to linearly transform the source model to get the
target model. We expect a linear shift if a central hardwaredevice affecting the functionality of all conﬁguration optionshomogeneously, changes such as the CPU, or homogeneousworkload change. Previous studies demonstrated the existenceof such cases where they trained a linear transformation toderive a target model for hardware changes [51].
Metric. We investigate whether f(c,e
t)=α×f(c,es)+
β,∀c∈C. We use metric M1: Pearson linear correlation [4]
between f(c,es)andf(c,et)to evaluate the hypothesis. If
the correlation is 1, we can linearly transform performance
models. Due to measurement noise, we do not expect perfectcorrelation, but we expect, for correlations higher than 0.9,
simple transfer learning can produce good predictions.
Results. The result in Table II show very high correlations
for about a third of all studied environmental changes. Inparticular, we observe high correlations for hardware changesand for many workload changes of low expected severity.
Hardware change: Hardware changes often result in near-
perfect correlations except for severe changes where we haveused unstable hardware (e.g. , Amazon cloud in ec
2). We
investigated why using cloud hardware resulted in weaklinear correlations. We analyzed the variance of the mea-surement noise and we observed that the proportion of thevariance of the noise in the source to the target in ec
2is
¯σ2
ecs
2/¯σ2
ect2=3 3.39, which is an order of magnitude larger
than the corresponding one in ec1(¯σ2
ecs1/¯σ2
ect1=1.51). This
suggests that we can expect a linear transformation across
environments when hardware resources execute in a stableenvironment. For transfer learning, this means that we couldreuse measurements from cheaper or testing servers in orderto predict the performance behavior [6]. Moreover, it alsosuggests that virtualization may hinder transfer learning.
Workload change:F o r
SPEAR , we observed very strong
correlations across environments where we have consideredSAT problems of different sizes and difﬁculties. Also, whenthe difference among the problem size and difﬁculty is closeracross environments (e.g. ,ec
3vs.ec4) the correlation is
slightly higher. This observation has also been conﬁrmed for
other systems. For instance, in environmental instance ec3
inSQLite , where the workload change is write-heavy from
sequential to batch, we have observed an almost perfect cor-
relation,0.96, while in the read-heavy workload ec4(randomto sequential read) the correlation is only medium at 0.5: First,
the underlying hardware contains an SSD, which has different
performance properties for reading and writing. Second, adatabase performs different internal functions when insertingor retrieving data. This implies that some environmental con-
ditions may provide a better means for transfer learning.
V ersion change:F o r
SPEAR (ec5,6,7) and x264 (ec5,6,7,8 ),
the correlations are extremely weak or non existence, while for
SQLite (ec5), the correlation is almost perfect. We speculate
that the optimization features that are determined by the
conﬁguration options for SPEAR and x264 may undergo
a substantial revision from version to version because al-
gorithmic changes may signiﬁcantly improve the way howthe optimization features work. The implication for transferlearning is that code changes that substantially inﬂuence theinternal logic controlled by conﬁguration options may requirea non-linear form of transformation or a complete set of newmeasurements in the target environment for those options only.
Insight. For non-severe hardware changes, we can linearly
transfer performance models across environments.
H1.2: The performance distribution of the source is similar to
the performance distribution of the target environment.
Importance. In the previous hypothesis, we investigated the
situation whether the response functions in the source andtarget are linearly correlated. In this hypothesis, we considera relaxed version of H1.1 by investigating if the performancedistributions are similar. When the performance distributionsare similar, it does not imply that there exists a linear mappingbetween the two responses, but, there might be a moresophisticated relationship between the two environments thatcan be captured by a non-linear transfer function.
Metric. We measure M2: Kullback-Leibler (KL) divergence
[8] to compare the similarity between the performance dis-
tributions: D
ec
KL(pds,pd t)=Σ ipds(ci)logpds(ci)
pdt(ci), where
pds,t(·)are performance distributions of the source and target.
As an example, we show the performance distributions of ec1
andec13and compare them using KL divergence in Figure 2:
The lower the value of KL divergence is, the more similar are
the distributions. We consider two distributions as similar ifD
ec
KL(pds,pd t)<3[4] and dissimilar otherwise.
Results. Here, we are interested to ﬁnd environmental changesfor which we did not observe a strong correlation, but forwhich there might be similarities between the performancedistributions of the environment. For ec
5,6inSPEAR ,ec3−7
inx264 ,ec4,6inSQLite , and ec5,8inSaC, the performance
distributions are similar across environments. This implies that
there exist a possibly non-linear transfer function that wecan map performance models across environments. Previousstudies demonstrated the feasibility of highly non-linear kernel
functions for transfer learning in conﬁgurable systems [25].
Insight. Even for some severe environmental changes
with no linear correlation across performance models,the performance distributions are similar, showing thepotential for learning a non-linear transfer function.
501
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. Fig. 2: Performance distributions of environments, depending on the
severity of change, may be dissimilar, Dec1
KL=2 5 .02(a,b), or very
similar, Dec13
KL =0.32(c,d).
H1.3: The ranking of conﬁgurations stays stable.
Importance. If the ranking of the conﬁgurations stays similar,
the response function is then stable across environments. Wecan use this knowledge to prioritize certain regions in theconﬁguration space for optimizations.
Metric. Here, we use rank correlation by measuring the
M3: Spearman correlation coefﬁcient between response vari-
ables. Intuitively, the Spearman correlation will be high whenobservations have a similar rank. We consider rank correlationshigher than 0.9as strong and suitable for transfer learning.
Results. The results in Table II show that the rank correlationsare high across hardware changes and small workload changes.This metric does not provide additional insights from whatwe have observed in H1.1. However, in one environmentalchange, where, due to excessive measurement noise, the linearcorrelation was low, ec
2for SPEAR , the rank correlation is
high. This might hint that when unstable hardware conditionsexist, the overall ranking may stay stable.
Insight. The conﬁgurations retain their relative perfor-mance proﬁle across hardware platforms.
H1.4: The top/bottom performer conﬁgurations are similar.
Importance. If the top conﬁgurations are similar across envi-
ronments, we can extract their characteristics and use that inthe transfer learning process. For instance, we can identify thetop conﬁgurations from the source and inform the optimizationin the target environment [23]. The bottom conﬁgurations canbe used to avoid corresponding regions during sampling. Notethat this is a relaxed hypothesis comparing to H1.3.
Metric. We measure M4/M5: the percentage of (10
thper-
centile) top/bottom conﬁgurations in the source that are also
top/bottom performers in the target.
Results. The results in Table II show that top/bottom con-
ﬁgurations are common across hardware and small workloadchanges, therefore, this metric does not provide additionalinsights from what we have observed in H1.1.Insight. Only hardware changes preserve top conﬁgura-tions across environments.
V. S
IMILARITY OF INFLUENTIAL OPTIONS (RQ2)
Here, we investigate whether the inﬂuence of individual
conﬁguration options on performance stays consistent acrossenvironments. We investigate two hypotheses about the inﬂu-ence strength (H2.1) and the importance of options (H2.2).
H2.1: The inﬂuential options on performance stay consistent.
Importance. In highly-dimensional spaces, not all conﬁgura-
tion options affect the response signiﬁcantly. If we observea high percentage of common inﬂuential options across envi-ronments, we can exploit this for learning performance modelsby sampling across only a subset of all conﬁguration options,
because we already know that these are the key optionsinﬂuencing performance.
Metric. In order to investigate the option-speciﬁc effects,
we use a paired t-test [4] to test if an option leads to anysigniﬁcant performance change and whether this change issimilar across environments. That is, when comparing the pairsof conﬁguration in which this option is enabled and disabledrespectively, an inﬂuential option has a consistent effect tospeed up or slow down the program, beyond random chance.If the test shows that an option makes a difference, we thenconsider it as an inﬂuential option. We measure M6/M7: the
number of inﬂuential options in source and target; We alsomeasure M8/M9: the number of options that are inﬂuential in
both/one environment.
Results. The results in Table II show that slightly more than
half of the options, for all subject systems, are inﬂuentialeither in the source or target environments. From the inﬂuentialoptions, a very high percentage are common in both. This canlead to a substantial reduction for performance measurements:we can ﬁx the non-inﬂuential options and sample only alongoptions, which we found inﬂuential from the source.
Insight. Only a subset of options is inﬂuential which is
largely preserved across all environment changes.
H2.2: The importance of options stays consistent.
Importance. In machine learning, each decision variable (here
option) has a relative importance to predict the response andimportance of the variables play a key role for in the featureselection process [4]. Here, we use this concept to determine
the relative importance of conﬁguration options, because, inconﬁgurable systems, we face many options that if prioritized
properly, it can be exploited for performance predictions [25].
Metric. We use regression trees [4] for determining the relative
importance of conﬁguration options because (i) they havebeen used widely for performance prediction of conﬁgurablesystems [15], [51] and (ii) the tree structure can provideinsights into the most essential options for prediction, becausea tree splits into those options ﬁrst that provide the highestinformation gain [15]. We derive estimates of the importance
of options for the trained trees on the source and target by
examining how the prediction error will change as a result of
502
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. options. We measure M10: correlation between importance of
options for comparing the consistency across environments.
Results. From Table II, the correlation coefﬁcient between the
importance of options for different environmental changes is
high, and the less severe a change the higher the correlation
coefﬁcients. This conﬁrms our intuition that small changesin the environment do not affect the inﬂuence strength of anoption. Some environmental changes, where the correlationwas low according to M1, show a high correlation betweenoption importance according to M10: ec
6,7inSPEAR ,ec3−7
inx264 ,ec1,2,5,7− 11,14 inSaC. This observation gives further
evidence that even though we did not observe a linear cor-
relation, there might exist a non-linear relationship betweenperformance measures. For instance, the inﬂuence of optionsstay the same, but interactions might change.
Insight. The strength of the inﬂuence of conﬁguration
options is typically preserved across environments.
VI. P RESERV ATION OF OPTION INTERACTIONS (RQ3)
We state two hypotheses about the preservation of option
interactions (H3.1) and their importance (H3.2).
H3.1: The interactions between conﬁguration options are
preserved across environments.
Importance. In highly dimensional conﬁguration spaces, the
possible number of interactions among options is exponentialin the number of options and it is computationally infeasibleto get measurements aiming at learning an exhaustive numberof interactions. Prior work has shown that a very large portionof potential interactions has no inﬂuence [29], [45].
Metric. One key objective here is to evaluate to what extent
inﬂuential interactions will be preserved from source to target.Here, we learn step-wise linear regression models; a techniquethat has been used for creating performance inﬂuence modelfor conﬁgurable systems [44]. We learn all pairwise interac-tions, independently in the source and target environments. Wethen calculate the percentage of common pairwise interactions
from the model by comparing the coefﬁcients of the pairwiseinteraction terms of the regression models. We concentratedon pairwise interactions, as they are the most common formof interactions [29], [45]. Similar to H2.1, we measure:M11/M12: The number of interactions in the source/target;M13: The number of interactions that agree on the directionof effects in the source and the target.
Results. The results in Table II show three important observa-
tions: (i) only a small proportion of possible interactions havean effect on performance and so are relevant (conﬁrming priorwork); (ii) for the large environmental changes, the differencein the proportion of relevant interactions across environmentsis not similar, while for smaller environmental changes, theproportion is almost equal; (iii) a very large proportion ofinteractions is common across environments.
The mean percentage of interactions (averaged over all
changes) are 25%,28%,10%,6%for
SPEAR, x264, SQLite,
SaC respectively, where 100% would mean that all pairwise
combination of options have a distinct effect on performance.Also, the percentage of common interactions across environ-
ments is high, 96%,81%,85%,72% for
SPEAR, x264, SQLite,SaC respectively. This result points to an important trans-
ferable knowledge: interactions often stay consistent across
changes. This insight can substantially reduce measurementefforts to purposefully measure speciﬁc conﬁgurations.
Insight. A low percentage of potential interactions are
inﬂuential for performance model learning.
H3.2: The effects of interacting options stay similar.
Importance. If the effects of interacting options are similar
across environments, we can prioritize regions in the conﬁgu-ration space based on the importance of the interactions.
Metric. We measure M14: the correlation between the coef-
ﬁcients of the pairwise interaction terms in the linear model
learned independently on the source and target environmentsusing step-wise linear regression [18].
Results: The results in Table II reveal a very high and, in
several cases, perfect correlations between interactions acrossenvironments. For several environmental changes where wepreviously could not ﬁnd a strong evidence of transferableknowledge by previous metrics: ec
8inx264 ,ec4,6,7 inSQLite
andec14inSaC, we observed very strong correlations for
the interactions. The implication for transfer learning is that
a linear transfer function (see H1.1) may not applicable forsevere changes, while a complex transfer function may exist.
Insight. The importance of interactions is typically pre-
served across environments.
VII. I NV ALID CONFIGURATIONS SIMILARITY (RQ4)
For investigating similarity between invalid conﬁgurations
across environments, we formulate two hypotheses about the
percentage of invalid conﬁgurations and their commonalitiesacross environments (H4.1) and the existence of reusableknowledge that can distinguish invalid conﬁgurations (H4.2).
H4.1: The percentage of invalid conﬁgurations is similar
across environments and this percentage is considerable.
Importance. If the percentage of invalid conﬁgurations is
considerable in the source and target environments, this pro-vides a motivation to carry any information about the invalidconﬁgurations across environments to avoid exploration ofinvalid regions and reduce measurement effort.
Metric. We measure M15/M16: percentage of invalid conﬁg-
urations in the source and target, M17: percentage of invalid
conﬁgurations, which are common between environments.
Results. The results in Table II show that for
SPEAR and
x264 , a considerable percentage ( ≈50%) of conﬁgurations
are invalid and all of them are common across environments.
For SaC, approximately 18% of the sampled conﬁgurations are
invalid. For some workload changes the percentage of common
invalid conﬁguration is low (≤ 10%). The reason is that some
options in SaC may have severe effects for some programs to
be compiled, but have lower effects for others.
Insight. A moderate percentage of conﬁgurations are
typically invalid in both source and target environments.
503
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. H4.2: Classiﬁers for distinguishing invalid from valid conﬁg-
urations are reusable across environments.
Importance. If there are common characteristics among the
invalid conﬁgurations, we can learn a classiﬁer in the source to
identify the invalid conﬁgurations and transfer the knowledge(classiﬁer model) to the target environment to predict invalidconﬁgurations before measuring them, thus decrease cost.
Metric. We learn a classiﬁer using multinomial logistic regres-
sion [4]. It is a model that is used to predict the probabilitiesof being invalid, given a set of conﬁguration options. Wemeasure M18: the correlation between the coefﬁcients (i.e.,
the probability of the conﬁguration being invalid) of the
classiﬁcation models that has been leaned independently.
Results. The results in Table II show that for
SPEAR and x264 ,
the correlations between the coefﬁcients are almost perfect.
For SaC, in environmental changes where the common invalid
conﬁgurations are high, the correlations between coefﬁcients
are also very high. For two cases, ec6,7inSPEAR , we could
not ﬁnd any reusable knowledge previously with other metrics.Here, we can observe that even when the inﬂuence of optionschange, the region of invalid conﬁgurations may stay the same.
Insight. Information for identifying invalid regions can be
transferred, with a high conﬁdence, across environments.
VIII. L ESSONS LEARNED AND DISCUSSION
Based on our analyses of 36 environmental changes, we can
discuss lessons learned, implications and threats to validity.
A. Lessons Learned
Based on the empirical results, we have learned that there
is always some similarities that relate the source and target in
different forms depending on the severity of the change:
•Simple changes: We observed strong correlations between
response functions (interpolating performance measures)and, therefore, there is a potential for constructing simplelinear transfer functions across environments (RQ1).
•Large changes: We observed very similar performance dis-tributions (e.g. , version changes). In these cases, we found
evidence of high correlations between either options (RQ2)
or interactions (RQ3) for which a non-linear transfer maybe applicable. Therefore, the key elements in a performancemodel that has been learned from the source will notchange, but the coefﬁcients corresponding to options andtheir interactions might need to be relearned for the target.
•Severe changes: We have learned that a considerable part of
conﬁguration space is invalid across environmental changesthat could be considered for sampling conﬁgurations (RQ4).
B. Implications for Transfer Learning Research
We provide explanations of why and when transfer learning
works for performance modeling and analysis of highly con-
ﬁgurable systems. While all research questions have positiveanswers for some environmental changes and negative answersfor others, as discussed above in Section IV–Section VII, theresults align well with our expectations regarding the severityof change and their correspondence to the type of transferable
knowledge: (i) For small environmental changes, the overallperformance behavior was consistent across environments and
a linear transformation of performance models provides a goodapproximation for the target performance behavior. (ii) Forlarge environmental changes, we found evidence that individ-ual inﬂuences of conﬁguration options and interactions maystay consistent providing opportunities for a non-linear map-
ping between performance behavior across environments. (iii)Even for severe environmental changes, we found evidence
of transferable knowledge in terms of reusability of detectinginvalid from valid conﬁgurations providing opportunities foravoiding a large part of conﬁguration space for sampling.
The fact that we could largely predict the severity of change
without deep knowledge about the conﬁguration spaces orimplementations of the subject systems is encouraging in thesense that others will likely also be able to make intuitivejudgments about transferability of knowledge. For example, auser of a performance analysis approach estimating low sever-ity of an environment change can test this hypothesis quicklywith a few measurements and select the right transfer learningstrategy. Transfer learning approaches for easy environmentalchanges are readily available [7], [25], [51], [64].
For more severe environmental changes, more research is
needed to exploit transferable knowledge. Our results showthat even with severe environmental change, there always issome transferable knowledge that can contribute to perfor-mance understanding of conﬁgurable systems. While somelearning strategies can take existing domain knowledge intoaccount and could beneﬁt from knowledge about inﬂuen-tial options and interactions [44], [45], it is less obvious
how to effectively incorporate such knowledge into sampling
strategies and how to build more effective learners based onlimited transferable knowledge. While we strongly suspect thatsuitable transfer learning techniques can provide signiﬁcantbeneﬁts even for severe environmental changes, more researchis needed to design and evaluate such techniques and compareto state of the art sampling and learning strategies. Speciﬁcally,we expect research opportunities regarding:
1)Sampling strategies to exploit the relatedness of environ-
ments to select informative samples using the importance
of speciﬁc regions [40] or avoiding invalid conﬁgurations.
2)Learning mechanisms to exploit the relatedness across
environments and learn either a linear or non-linear asso-ciations (e.g. , active learning [52], domain adaptation [31],
ﬁne tuning a pre-trained model [13], feature transfer [62],
or knowledge distillation [17] in deep neural networkarchitectures). However, efforts need to be made to makethe learning less expensive.
3)Performance testing and debugging of conﬁgurable sys-
tems to beneﬁt from our ﬁndings by transferring interesting
test cases covering interactions between options [49] ordetecting invalid conﬁgurations [57]–[59].
4)Performance tuning and optimization [23] beneﬁt from
the ﬁndings by identifying the interacting options and to
perform importance sampling exploiting the importancecoefﬁcients of options and their interactions.
5)Performance modeling [9] beneﬁt from the ﬁndings by
developing techniques that exploits the shared knowledge
in the modeling process, e.g., tuning the parameters of a
queuing network model using transfer learning.
504
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. C. Threats to V alidity
1) External V alidity: We selected a diverse set of subject
systems and a large number of purposefully selected environ-
mental changes, but, as usual, one has to be careful when gen-eralizing to other subject systems and environmental changes.
We actually performed experiments with more environmental
changes and with additional measurements on the same subjectsystems (e.g. , for
SaC we also measured the time it takes to
compile the program not only its execution), but we excludedthose results because they were consistent with the presenteddata and did not provide additional insights.
2) Internal and Construct V alidity: Due to the size of
conﬁguration spaces, we could only measure conﬁgurationsexhaustively in one subject system and had to rely on sampling(with substantial sampling size) for the others, which may
miss effects in parts of the conﬁguration space that we did
not sample. We did not encounter any surprisingly differentobservation in our exhaustively measured
SPEAR dataset.
We operationalized a large number of different measures
through metrics. For each measure, we considered multiple
alternative metrics (e.g. , different ways to establish inﬂuential
options), but settled usually on the simplest and most reli-
able metric we could identify to keep the paper accessibleand within reasonable length. In addition, we only partiallyused statistical tests, as needed, and often compared metricsdirectly using more informal comparisons and some ad-hocthreshold for detecting common patterns across environments.A different operationalization may lead to different results,
but since our results are consistent across a large number ofmeasures, we do not expect any changes to the big picture.
For building the performance models, calculating impor-
tance of conﬁguration options, and classifying the invalid
conﬁgurations, we elected to use different machine learn-ing models: step-wise linear regression, regression trees, andmultinomial logistic regression. We chose these learner mainlybecause they are successful models that have been used inprevious work for performance predictions of conﬁgurablesystems. However, these are only few learning mechanismsout of many that may provide different accuracy and cost.
Measurement noise in benchmarks can be reduced but not
avoided. We performed benchmarks on dedicated systems andrepeated each measurement 3 times. We repeated experimentswhen we encountered unusually large deviations.
IX. R
ELATED WORK
A. Performance Analysis of Conﬁgurable Software
Performance modeling and analysis is a highly researched
topic [53]. Researchers investigate what models are moresuitable for predicting the performance, which sampling andoptimization strategies can be used for tuning these models,and how to minimize the amount of measurement efforts.
Sampling strategies based on experimental design (such as
Plackett-Burman) have been applied in the domain of con-ﬁgurable systems [15], [43], [44]. The aim of these samplingapproaches is to ensure that we gain a high level of informationfrom sparse sampling in high-dimensional spaces.
Optimization algorithms have also been applied to ﬁnd
optimal conﬁgurations for conﬁgurable systems: Recursiverandom sampling [60], hill climbing [55], direct search [64],optimization via guessing [37], Bayesian optimization [23],
and multi-objective optimization [12]. The aim of optimizationapproaches is to ﬁnd the optimal conﬁguration in a highlydimensional space using only a limited sampling budget.
Machine learning techniques, such as support-vector ma-
chines [61], decision trees [33], Fourier sparse functions [63],active learning [44] and search-based optimization and evolu-tionary algorithms [16], [54] have also been used.
Our work is related to the performance analysis research
mentioned above. However, we do not perform a comparison
of different models, conﬁguration optimization or samplingstrategies. Instead, we concentrate on transferring performancemodels across hardware, workload and software version.Transfer learning is orthogonal to these approaches and can
contribute to making them efﬁcient for performance analysis.
B. Performance Analysis Across Environmental Change
Environmental changes have been studied before. For
example, in the context of MapReduce applications [61],
performance-anomaly detection [46], micro-benchmarking ondifferent hardware [22], parameter dependencies [64], andperformance prediction based on similarity search [48].
Recently, transfer learning is used in systems and soft-
ware engineering. For example, in the context of perfor-mance predictions in self-adaptive systems [25], conﬁgurationdependency transfer across software systems [7], co-designexploration for embedded systems [5], model transfer acrosshardware [51], and conﬁguration optimization [3]. Althoughprevious work has analyzed transfer learning in the contextof select hardware changes [7], [25], [51], we more broadlyempirically investigate why and when transfer learning works.That is, we provide evidence why and when other techniquesare applicable for which environmental changes.
Transfer learning has also been applied in software engi-
neering in very different contexts, including defect predic-
tions [28], [34], [35] and effort estimation [27].
X. C
ONCLUSIONS
We investigated when and why transfer learning works
for performance modeling and analysis of highly conﬁg-
urable systems. Our results suggest that performance models
are frequently related across environments regarding overall
performance response, performance distributions, inﬂuentialconﬁguration options and their interactions, as well as invalidconﬁgurations. While some environment changes allow simplelinear forms of transfer learning, others have less obviousrelationships but can still be exploited by transferring morenuanced aspects of the performance model, e.g., usable for
guided sampling. Our empirical study demonstrates the ex-
istence of diverse forms of transferable knowledge acrossenvironments that can contribute to learning faster, better,reliable, and more important, less costly performance models.
A
CKNOWLEDGMENT
This work has been supported by AFRL and DARPA
(FA8750-16-2-0042). Kaestner’s work is also supported byNSF awards 1318808 and 1552944 and the Science of SecurityLablet (H9823014C0140). Siegmund’s work is supported bythe DFG under the contracts SI 2171/2 and SI 2171/3-1. Wewould like to thank Tim Menzies, Vivek Nair, Wei Fu, andGabriel Ferreira for their feedback.
505
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. TABLE II: E XPERIMENTAL RESULTS
RQ1 RQ2 RQ3 RQ4
H1.1 H1.2 H1.3 H1.4 H2.1 H2.2 H3.1 H3.2 H4.1 H4.2
Environment ES M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 M14 M15 M16 M17 M18
SPEAR —Workload (#variables/#clauses): w1: 774/5934,w 2: 1008/7728,w 3: 1554/11914,w 4: 978/7498; Version:v1:1.2,v 2:2.7
ec1:[h2→h1,w1,v2] S 1.00 0.22 0.97 0.92 0.92 9770 12 52 52 5 1.00 0.47 0.45 1 1.00
ec2:[h4→h1,w1,v2] L 0.59 24.88 0.91 0.76 0.86 1 2742 0.51 41 27 21 0.98 0.48 0.45 1 0.98
ec3:[h1,w1→w2,v2] L 0.96 1.97 0.17 0.44 0.329743 12 32 32 2 0.99 0.45 0.45 1 1.00
ec4:[h1,w1→w3,v2] M 0.90 3.36 -0.08 0.30 0.117743 0.99 22 23 22 0.99 0.45 0.49 1 0.94
ec5:[h1,w1,v2→v1] S 0.23 0.30 0.35 0.28 0.326531 0.32 21 7 7 0.33 0.45 0.50 1 0.96
ec6:[h1,w1→w2,v1→v2] L -0.10 0.72 -0.05 0.35 0.045613 0.68 7 21 7 0.31 0.50 0.45 1 0.96
ec7:[h1→h2,w1→w4,v2→v1]VL -0.10 6.95 0.14 0.41 0.156422 0.88 21 7 7 -0.44 0.47 0.50 1 0.97
x264 —Workload (#pictures/size): w1:8/2,w 2:3 2/11,w 3: 128/44; Version:v1:r2389,v 2:r2744,v 3:r2744
ec1:[h2→h1,w3,v3] SM 0.97 1.00 0.99 0.97 0.92 9 10 8 0 0.86 21 33 18 1.00 0.49 0.49 1 1
ec2:[h2→h1,w1,v3] S 0.96 0.02 0.96 0.76 0.79 9980 0.94 36 27 24 1.00 0.49 0.49 1 1
ec3:[h1,w1→w2,v3] M 0.65 0.06 0.63 0.53 0.58 9 11 8 1 0.89 27 33 22 0.96 0.49 0.49 1 1
ec4:[h1,w1→w3,v3] M 0.67 0.06 0.64 0.53 0.56 9 10 7 1 0.88 27 33 20 0.96 0.49 0.49 1 1
ec5:[h1,w3,v2→v3] L 0.05 1.64 0.44 0.43 0.42 12 10 10 0 0.83 47 33 29 1.00 0.49 0.49 1 1
ec6:[h1,w3,v1→v3] L 0.06 1.54 0.43 0.43 0.37 11 10 9 0 0.80 46 33 27 0.99 0.49 0.49 1 1
ec7:[h1,w1→w3,v2→v3] L 0.08 1.03 0.26 0.25 0.22 8 10 5 1 0.78 33 33 20 0.94 0.49 0.49 1 1
ec8:[h2→h1,w1→w3,v2→v3]VL 0.09 14.51 0.26 0.23 0.258952 0.58 33 21 18 0.94 0.49 0.49 1 1
SQLite —Workload: w1:write−seq,w2:write−batch,w 3:read−rand,w 4:read−seq;Version:v1:3.7.6.3,v 2:3.19.0
ec1:[h3→h2,w1,v1] S 0.99 0.37 0.82 0.35 0.315220 11 3 9 8 1.00 N/A N/A N/A N/A
ec2:[h3→h2,w2,v1] M 0.97 1.08 0.88 0.40 0.495540 11 01 1 9 1.00 N/A N/A N/A N/A
ec3:[h2,w1→w2,v1] S 0.96 1.27 0.83 0.40 0.352310 1997 0.99 N/A N/A N/A N/A
ec4:[h2,w3→w4,v1] M 0.50 1.24 0.43 0.17 0.431100 1422 1.00 N/A N/A N/A N/A
ec5:[h1,w1,v1→v2] M 0.95 1.00 0.79 0.24 0.292410 11 21 1 7 0.99 N/A N/A N/A N/A
ec6:[h1,w2→w1,v1→v2] L 0.51 2.80 0.44 0.25 0.303411 0.31 7 11 6 0.96 N/A N/A N/A N/A
ec7:[h2→h1,w2→w1,v1→v2]VL 0.53 4.91 0.53 0.42 0.473521 0.31 7 13 6 0.97 N/A N/A N/A N/A
SaC—Workload: w1:srad,w 2:pfilter,w 3:kmeans,w 4:hotspot,w 5:nw,w 6:nbody100,w 7:nbody150,w 8:nbody750,w 9:gc,w 10:cg
ec1:[h1,w1→w2,v1] L 0.66 25.02 0.65 0.10 0.79 13 14 8 0 0.88 82 73 52 0.27 0.18 0.17 0.88 0.73
ec2:[h1,w1→w3,v1] L 0.44 15.77 0.42 0.10 0.65 13 10 8 0 0.91 82 63 50 0.56 0.18 0.12 0.90 0.84
ec3:[h1,w1→w4,v1] S 0.93 7.88 0.93 0.36 0.90 12 10 9 0 0.96 37 64 34 0.94 0.16 0.15 0.26 0.88
ec4:[h1,w1→w5,v1] L 0.96 2.82 0.78 0.06 0.81 16 12 10 0 0.94 34 58 25 0.04 0.15 0.22 0.19 -0.29
ec5:[h1,w2→w3,v1] M 0.76 1.82 0.84 0.67 0.86 17 11 9 1 0.95 79 61 47 0.55 0.27 0.13 0.83 0.88
ec6:[h1,w2→w4,v1] S 0.91 5.54 0.80 0.00 0.91 14 11 8 0 0.85 64 65 31 -0.40 0.13 0.15 0.12 0.64
ec7:[h1,w2→w5,v1] L 0.68 25.31 0.57 0.11 0.71 14 14 8 0 0.88 67 59 29 0.05 0.21 0.22 0.09 -0.13
ec8:[h1,w3→w4,v1] L 0.68 1.70 0.56 0.00 0.91 14 13 9 1 0.88 57 67 36 0.34 0.11 0.14 0.05 0.67
ec9:[h1,w3→w5,v1] VL 0.06 3.68 0.20 0.00 0.64 16 10 9 0 0.90 51 58 35 -0.52 0.11 0.21 0.06 -0.41
ec10:[h1,w4→w5,v1] L 0.70 4.85 0.76 0.00 0.75 12 12 11 0 0.95 58 57 43 0.29 0.14 0.20 0.64 -0.14
ec11:[h1,w6→w7,v1] S 0.82 5.79 0.77 0.25 0.88 36 30 28 2 0.89 109 164 102 0.96 N/A N/A N/A N/A
ec12:[h1,w6→w8,v1] S 1.00 0.52 0.92 0.80 0.97 38 30 22 6 0.94 51 53 43 0.99 N/A N/A N/A N/A
ec13:[h1,w8→w7,v1] S 1.00 0.32 0.92 0.53 0.99 30 33 26 1 0.98 53 89 51 1.00 N/A N/A N/A N/A
ec14:[h1,w9→w10,v1] L 0.24 4.85 0.56 0.44 0.77 22 21 18 3 0.69 237 226 94 0.86 N/A N/A N/A N/A
ES: Expected severity of change (Sec. III-B): S: small change; SM: small medium change; M: medium change; L: large change; VL: very large change.
SaC workload descriptions: srad: random matrix generator; pﬁlter: particle ﬁltering; hotspot: heat transfer differential equations; k-means: clustering; nw: optimal matching;
nbody: simulation of dynamic systems; cg: conjugate gradient; gc: garbage collector. Hardware descriptions (ID: Type/CPUs/Clock (GHz)/RAM (GiB)/Disk):
h1: NUC/4/1.30/15/SSD; h2: NUC/2/2.13/7/SCSI; h3:Station/2/2.8/3/SCSI; h4: Amazon/1/2.4/1/SSD; h5: Amazon/1/2.4/0.5/SSD; h6: Azure/1/2.4/3/SCSI
Metrics: M1: Pearson correlation; M2: Kullback-Leibler (KL) divergence; M3: Spearman correlation; M4/M5: Perc. of top/bottom conf.; M6/M7: Number of inﬂuential options;
M8/M9: Number of options agree/disagree; M10: Correlation btw importance of options; M11/M12: Number of interactions; M13: Number of interactions agree on effects;
M14: Correlation btw the coeffs; M15/M16: Perc. of invalid conf. in source/target; M17: Perc. of invalid conf. common btw environments; M18: Correlation btw coeffs.
506Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] A. Ahmad, P. Jamshidi, and C. Pahl. Classiﬁcation and comparison of
architecture evolution reuse knowledge - a systematic review. Wiley
Journal of Software: Evolution and Process (JSEP) , 26(7):654–691,
2014.
[2] J. P. S. Alcocer, A. Bergel, S. Ducasse, and M. Denker. Performance
evolution blueprint: Understanding the impact of software evolution on
performance. In Proc. of Working Conference on Software Visualization
(VISSOFT), pages 1–9. IEEE, 2013.
[3] M. Arta ˇc, editor. Deliverable 5.2: DICE delivery tools-Intermediate
version. 2017. http://www.dice-h2020.eu/.
[4] C. M. Bishop. Pattern recognition and machine learning. Springer, New
York, 2006.
[5] B. Bodin, L. Nardi, M. Z. Zia, H. Wagstaff, G. Sreekar Shenoy,
M. Emani, J. Mawer, C. Kotselidis, A. Nisbet, M. Lujan, B. Franke,
P. H. Kelly, and M. O’Boyle. Integrating algorithmic parameters into
benchmarking and design space exploration in 3D scene understanding.
InProceedings of the International Conference on Parallel Architectures
and Compilation (PACT), pages 57–69. ACM, 2016.
[6] A. Brunnert, A. van Hoorn, F. Willnecker, A. Danciu, W. Hasselbring,
C. Heger, N. Herbst, P. Jamshidi, R. Jung, J. von Kistowski, A. Koziolek,J. Kross, S. Spinner, C. V ¨ogele, J. Walter, and A. Wert. Performance-
oriented devops: A research agenda. SPEC-RG-2015-01, RG DevOps
Performance, 2015.
[7] H. Chen, W. Zhang, and G. Jiang. Experience transfer for the con-
ﬁguration tuning in large-scale computing systems. IEEE Trans. on
Knowledge and Data Eng. (TKDE), 23(3):388–401, 2011.
[8] T. M. Cover and J. A. Thomas. Elements of information theory. John
Wiley & Sons, 2012.
[9] P. J. Denning and J. P. Buzen. The operational analysis of queueing
network models. ACM Computing Surveys (CSUR), 10(3):225–261,
1978.
[10] A. Elkhodary, N. Esfahani, and S. Malek. Fusion: A framework for
engineering self-tuning self-adaptive software systems. In Proc. Int’l
Symp. Foundations of Software Engineering (FSE), pages 7–16. ACM,
2010.
[11] N. Esfahani, A. Elkhodary, and S. Malek. A learning-based framework
for engineering feature-oriented self-adaptive software systems. IEEE
Trans. Softw. Eng. (TSE), 39(11):1467–1493, 2013.
[12] A. Filieri, H. Hoffmann, and M. Maggio. Automated multi-objective
control for self-adaptive software design. In Proc. Int’l Symp. Founda-
tions of Software Engineering (FSE), pages 13–24. ACM, 2015.
[13] W. Ge and Y . Yu. Borrowing treasures from the wealthy: Deep
transfer learning through selective joint ﬁne-tuning. arXiv preprint
arXiv:1702.08690, 2017.
[14] A. Grebhahn, N. Siegmund, H. K ¨ostler, and S. Apel. Performance
prediction of multigrid-solver conﬁgurations. In Software for Exascale
Computing-SPPEXA 2013-2015, pages 69–88. Springer, 2016.
[15] J. Guo, K. Czarnecki, S. Apel, N. Siegmund, and A. Wasowski.
Variability-aware performance prediction: A statistical learning ap-
proach. In Proc. Int’l Conf. Automated Software Engineering (ASE),
pages 301–311. IEEE, 2013.
[16] C. Henard, M. Papadakis, M. Harman, and Y . Le Traon. Combining
multi-objective search and constraint solving for conﬁguring large soft-ware product lines. In Proc. Int’l Conf. Software Engineering (ICSE),
pages 517–528. IEEE, 2015.
[17] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural
network. arXiv preprint arXiv:1503.02531, 2015.
[18] R. R. Hocking. A biometrics invited paper. the analysis and selection
of variables in linear regression. Biometrics , 32(1):1–49, 1976.
[19]
H. Hoffmann, S. Sidiroglou, M. Carbin, S. Misailovic, A. Agarwal, and
M. Rinard. Dynamic knobs for responsive power-aware computing. In
In Proc. of Int’l Conference on Architectural Support for Programming
Languages and Operating Systems (ASPLOS), 2011.
[20] H. H. Hoos. Automated algorithm conﬁguration and parameter tuning.
InAutonomous search, pages 37–71. Springer, 2011.
[21] H. H. Hoos. Programming by optimization. Communications of the
ACM , 55(2):70–80, 2012.
[22] K. Hoste, A. Phansalkar, L. Eeckhout, A. Georges, L. K. John, and
K. De Bosschere. Performance prediction based on inherent programsimilarity. In Proc. of the International Conference on Parallel Archi-
tectures and Compilation Techniques (PACT), pages 114–122. ACM,
2006.
[23] P. Jamshidi and G. Casale. An uncertainty-aware approach to optimal
conﬁguration of stream processing systems. In Proc. Int’l Symp. on
Modeling, Analysis and Simulation of Computer and Telecommunication
Systems (MASCOTS), pages 39–48. IEEE, September 2016.[24] P. Jamshidi, M. Ghafari, A. Ahmad, and C. Pahl. A framework
for classifying and comparing architecture-centric software evolutionresearch. In Proc. of European Conference on Software Maintenance
and Reengineering (CSMR), pages 305–314. IEEE, 2013.
[25] P. Jamshidi, M. Velez, C. K ¨astner, N. Siegmund, and P. Kawthekar.
Transfer learning for improving model predictions in highly conﬁgurable
software. In Proc. Int’l Symp. Software Engineering for Adaptive and
Self-Managing Systems (SEAMS). IEEE, 2017.
[26] P. Kawthekar and C. K ¨astner. Sensitivity analysis for building evolving
and & adaptive robotic software. In Proceedings of the IJCAI Workshop
on Autonomous Mobile Service Robots (WSR), 2016.
[27] E. Kocaguneli, T. Menzies, and E. Mendes. Transfer learning in effort
estimation. Empirical Software Engineering, 20(3):813–843, 2015.
[28] R. Krishna, T. Menzies, and W. Fu. Too much automation? The
bellwether effect and its implications for transfer learning. In Proc. Int’l
Conf. Automated Software Engineering (ASE), pages 122–131. ACM,
2016.
[29] D. R. Kuhn, R. N. Kacker, and Y . Lei. Introduction to combinatorial
testing. CRC press, 2013.
[30] P. Leitner and J. Cito. Patterns in the chaos - a study of performance
variation and predictability in public IaaS clouds. ACM Trans. on
Internet Technology (TOIT), 16(3):15, 2016.
[31] M. Long, Y . Cao, J. Wang, and M. Jordan. Learning transferable features
with deep adaptation networks. In Proc. of Int’l Conference on Machine
Learning (ICML), pages 97–105, 2015.
[32] A. Murashkin, M. Antkiewicz, D. Rayside, and K. Czarnecki. Visualiza-
tion and exploration of optimal variants in product line engineering. In
Proc. Int’l Software Product Line Conference (SPLC), pages 111–115.
ACM, 2013.
[33] V . Nair, T. Menzies, N. Siegmund, and S. Apel. Faster discovery
of faster system conﬁgurations with spectral learning. arXiv preprint
arXiv:1701.08106, 2017.
[34] J. Nam and S. Kim. Heterogeneous defect prediction. In Proc. Int’l
Symp. Foundations of Software Engineering (FSE), pages 508–519.ACM, 2015.
[35] J. Nam, S. J. Pan, and S. Kim. Transfer defect learning. In Proc. Int’l
Conf. Software Engineering (ICSE), pages 382–391. IEEE, 2013.
[36] R. Olaechea, D. Rayside, J. Guo, and K. Czarnecki. Comparison of exact
and approximate multi-objective optimization for software product lines.
InProc. Int’l Software Product Line Conference (SPLC), pages 92–101.
ACM, 2014.
[37] T. Osogami and S. Kato. Optimizing system conﬁgurations quickly by
guessing at the performance. In Int’l Conference on Measurement and
Modeling of Computer Systems (SIGMETRICS), 2007.
[38] C. Pahl, P. Jamshidi, and O. Zimmermann. Architectural principles for
cloud
software. ACM Trans. on Internet Technology (TOIT), 2017.
[39] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Trans. on
Knowledge and Data Eng. (TKDE), 22(10):1345–1359, 2010.
[40] S. Ruder and B. Plank. Learning to select data for transfer learning
with Bayesian Optimization. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing, 2017.
[41] SaC compiler. www.sac-home.org.
[42] A. Saltelli, M. Ratto, T. Andres, F. Campolongo, J. Cariboni, D. Gatelli,
M. Saisana, and S. Tarantola. Global sensitivity analysis: the primer.
John Wiley & Sons, 2008.
[43] A. Sarkar, J. Guo, N. Siegmund, S. Apel, and K. Czarnecki. Cost-
efﬁcient sampling for performance prediction of conﬁgurable systems.
InProc. Int’l Conf. Automated Software Engineering (ASE), pages 342–
352. IEEE, November 2015.
[44] N. Siegmund, A. Grebhahn, S. Apel, and C. K ¨astner. Performance-
inﬂuence models for highly conﬁgurable systems. In Proc. Eu-
rop. Software Engineering Conf. Foundations of Software Engineering(ESEC/FSE), pages 284–294. ACM, August 2015.
[45] N. Siegmund, S. S. Kolesnikov, C. K ¨astner, S. Apel, D. Batory,
M. Rosenm ¨uller, and G. Saake. Predicting performance via automated
feature-interaction detection. In Proc. Int’l Conf. Software Engineering
(ICSE), pages 167–177. IEEE, 2012.
[46] C. Stewart, K. Shen, A. Iyengar, and J. Yin. Entomomodel: Under-
standing and avoiding performance anomaly manifestations. In Proc.
Int’l Symp. on Modeling, Analysis and Simulation of Computer andTelecommunication Systems (MASCOTS), pages 3–13. IEEE, 2010.
[47] J. Styles, H. H. Hoos, and M. M ¨uller. Automatically conﬁguring
algorithms for scaling performance. In Learning and Intelligent Op-
timization, pages 205–219. Springer, 2012.
[48] E. Thereska, B. Doebel, A. X. Zheng, and P. Nobel. Practical per-
formance models for complex, popular applications. In SIGMETRICS
Perform. Eval. Rev., volume 38, pages 1–12. ACM, 2010.
507
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. [49] J. Toman and D. Grossman. Staccato: A bug ﬁnder for dynamic
conﬁguration updates (artifact). In DARTS-Dagstuhl Artifacts Series,
volume 2. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.
[50] L. Torrey and J. Shavlik. Transfer learning. Handbook of Research on
Machine Learning Applications and Trends: Algorithms, Methods, and
Techniques, 1:242–264, 2009.
[51] P. Valov, J.-C. Petkovich, J. Guo, S. Fischmeister, and K. Czarnecki.
Transferring performance prediction models across different hardware
platforms. In Proc. Int’l Conf. on Performance Engineering (ICPE),
pages 39–50. ACM, 2017.
[52] X. Wang, T.-K. Huang, and J. Schneider. Active transfer learning under
model shift. In International Conference on Machine Learning, pages
1305–1313, 2014.
[53] M. Woodside, G. Franks, and D. C. Petriu. The future of software
performance engineering. In Future of Software Engineering (FOSE),
pages 171–187. IEEE, 2007.
[54] F. Wu, W. Weimer, M. Harman, Y . Jia, and J. Krinke. Deep parameter
optimisation. In Proc. of the Annual Conference on Genetic and
Evolutionary Computation, pages 1375–1382. ACM, 2015.
[55] B. Xi, Z. Liu, M. Raghavachari, C. H. Xia, and L. Zhang. A smart
hill-climbing algorithm for application server conﬁguration. In 13th
International Conference on World Wide Web (WWW), pages 287–296.
ACM, 2004.
[56] T. Xu, L. Jin, X. Fan, Y . Zhou, S. Pasupathy, and R. Talwadker. Hey,
you have given me too many knobs!: Understanding and dealing with
over-designed conﬁguration in system software. In Proc. Int’l Symp.
Foundations of Software Engineering (FSE), pages 307–319, New York,NY , USA, August 2015. ACM.
[57] T. Xu, X. Jin, P. Huang, Y . Zhou, S. Lu, L. Jin, and S. Pasupathy.Early detection of conﬁguration errors to reduce failure damage. pages619–634. USENIX Association, 2016.
[58] T. Xu, H. M. Naing, L. Lu, and Y . Zhou. How do system administrators
resolve access-denied issues in the real world? In Proceedings of the
2017 CHI Conference on Human Factors in Computing Systems, pages348–361. ACM, 2017.
[59] T. Xu, J. Zhang, P. Huang, J. Zheng, T. Sheng, D. Yuan, Y . Zhou, and
S. Pasupathy. Do not blame users for misconﬁgurations. In Proc. Symp.
Operating Systems Principles, pages 244–259, New York, NY , USA,
November 2013. ACM.
[60] T. Ye and S. Kalyanaraman. A recursive random search algorithm for
large-scale network parameter conﬁguration. In Int’l Conference on
Measurement and Modeling of Computer Systems (SIGMETRICS), pages196–205. ACM, 2003.
[61] N. Yigitbasi, T. L. Willke, G. Liao, and D. Epema. Towards machine
learning-based auto-tuning of mapreduce. In Proc. Int’l Symp. on
Modeling, Analysis and Simulation of Computer and Telecommunication
Systems (MASCOTS), pages 11–20. IEEE, 2013.
[62] J. Yosinski, J. Clune, Y . Bengio, and H. Lipson. How transferable are
features in deep neural networks? In Proc. of 12th USENIX conference
on Operating Systems Design and Implementation (OSDI), pages 3320–3328, 2014.
[63] Y . Zhang, J. Guo, E. Blais, and K. Czarnecki. Performance prediction of
conﬁgurable software systems by Fourier learning. In Proc. Int’l Conf.
Automated Software Engineering (ASE), pages 365–373. IEEE, 2015.
[64] W. Zheng, R. Bianchini, and T. D. Nguyen. Automatic conﬁguration of
internet services. ACM SIGOPS Operating Systems Review, 41(3):219–
229, 2007.
508
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 14:58:07 UTC from IEEE Xplore.  Restrictions apply. 