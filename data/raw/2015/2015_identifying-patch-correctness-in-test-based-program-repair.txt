Identifying Patch Correctness in Test-Based Program Repair
Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, Gang Huang
Key Laboratory of High Confidence Software Technologies (Peking University), MoE
Institute of Software, EECS, Peking University, Beijing, 100871, China
{xiongyf,liuxinyuan,mhzeng,zhanglucs,hg}@pku.edu.cn
ABSTRACT
Test-based automatic program repair has attracted a lot of atten-
tioninrecentyears.However,thetestsuitesinpracticeareoften
tooweaktoguaranteecorrectnessandexistingapproachesoften
generate a large number of incorrect patches.
Toreducethenumberofincorrectpatchesgenerated,wepropose
anovelapproachthatheuristicallydeterminesthecorrectnessof
the generated patches. The core idea is to exploit the behavior
similarity of test case executions. The passing tests on originaland patched programs are likely to behave similarly while the
failing tests on original and patched programs are likely to behave
differently.Also,iftwotestsexhibitsimilarruntimebehavior,the
two tests are likely to have the same test results. Based on these
observations,wegeneratenewtestinputstoenhancethetestsuites
and use their behavior similarity to determine patch correctness.
Ourapproachisevaluatedonadatasetconsistingof139patches
generated from existing program repair systems including jGen-
Prog,Nopol,jKali, ACSandHDRepair.Ourapproach successfully
prevented56.3%ofthe incorrectpatchestobegenerated,without
blocking any correct patches.
ACM Reference Format:
Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, Gang Huang. 2018.
Identifying Patch Correctness in Test-Based Program Repair. In ICSE ’18:
ICSE ’18: 40th International Conference on Software Engineering , May 27-
June 3, 2018, Gothenburg, Sweden. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3180155.3180182
1 INTRODUCTION
Inthepastdecades,alargenumberofautomatedprogramrepairap-
proaches[ 7–9,12,13,16–19,25,26,28,43,44]havebeenproposed,
and many of them fall into the category of test-based program
repair.Intest-basedprogramrepair,therepairtooltakesafaulty
program and a test suite including at least one failing test thatreveals the fault as input and then generates a patch that makes
alltestspass.However,testsuitesinrealworldprojectsareoften
Theauthorsacknowledgetheanonymousreviewersfortheconstructivecomments
andrevisionsuggestions.ThisworkissupportedbytheNationalKeyResearchand
DevelopmentProgramunderGrantNo.2016YFB1000105,andNationalNaturalScience
Foundation of China under Grant No. 61725201, 61529201, 61725201, 61672045. Lu
Zhang is the corresponding author. Xinyuan Liu and Muhan Zeng are equal contribu-
tors to the paper and their names are sorted alphabetically.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180182weak [34], and a patched program passing all the tests may still be
faulty. We call a patch plausible if the patched version passes all
tests in the test suite, and we consider a patch correctif it fixes and
only fixes the bug. As studied by Long et al. [ 20], the test suites in
realworldsystemsareusuallyweaksuchthatmostoftheplausible
patchesareincorrect,makingitdifficultforatest-basedprogram
repair system to ensure the correctness of the patches. As existing
studies [24,34,37] show, multiple automatic program repair sys-
tems produce much more incorrect patches than correct patches
on real world defects, leading to low precision in their generated
patches.
The low precision of existing program repair systems signifi-
cantlyaffectstheusabilityofthesesystems.Sincetestsuitescannotguarantee the correctness of the patches, developers have to manu-
ally verify patches. When the precision of a program repair system
is low, the developer has to verify a lot of incorrect patches, and
it is not clear whether such a verification process is more costlythan directly repairing the defect by the developers. An existingstudy [
39] also shows that, when developers are provided with
low-quality patches, their performance will drop compared to the
situationwherenopatchisprovided.Asaresult,webelieveitis
criticaltoimprovetheprecisionofprogramrepairsystems,even
at the risk of losing some correct patches.
Sinceaweaktestsuiteisnotenoughtofilterouttheincorrect
patches produced by program repair systems, a direct idea is to en-
hancethetestsuite.Indeed,existingstudies[ 42,46]haveattempted
togenerate newtest casesto identifyincorrectpatches. However,
while test inputs can be generated, test oracles cannot be automat-
ically generated in general, known as the oracle problem [ 1,31].
As a result, existing approaches either require human to determine
test results [ 42], which is too expensive in many scenarios, or rely
oninherentoraclessuchascrash-free[ 46],whichcanonlyidentify
certain types of incorrect patches that violate such oracles.
Ourgoalistoclassifypatchesheuristicallywithoutknowingthe
full oracle. Given a set of plausible patches, we try to determine
whethereachpatchislikelytobecorrectorincorrect,andreject
thepatchesthatarelikelytobeincorrect.Ourapproachisbased
on two key observations.
•PATCH-SIM . After a correct patch is applied, a passing
test usually behaves similarly as before, while a failing test
usually behaves differently.
•TEST-SIM . When two tests have similar executions, they
arelikelytohavethesametestresults,i.e.,bothtriggering
the same fault or both are normal executions.
PATCH-SIM allows us totest patches without oracles, i.e., werun
thetestsbeforeandafterpatchingthesystemandcheckthedegreeof behavior change. As our evaluation will show later, PATCH-SIM
alone already identify a large set of incorrect patches. However,
7892018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, Gang Huang
wecanonlyutilizetheoriginaltestsbutnotthenewlygenerated
test inputs as we do not know whether they pass or fail. TEST-SIM
complements PATCH-SIM by determining the test results of newly
generated test inputs.
Based on these two key observations, our approach consists
ofthefollowingsteps.First,wegenerateasetofnewtestinputs.
Second,weclassifythenewlygeneratedtestinputsaspassingor
failingtestsbycomparingthemwithexistingtestinputs.Third,we
determinethecorrectnessofthepatchbycomparingtheexecutions
before and after the patch for each test, including both the original
and the generated tests.
Wehaverealizedourapproachbydesigningconcreteformulasto
compareexecutions,andevaluatedourapproachonadatasetof139patchesgeneratedfrompreviousprogramrepairsystemsincluding
jGenProg[
24],Nopol[ 24],jKali[24],HDRepair[ 13],andACS[ 43].
Ourapproachsuccessfullyfilteredout56.3%oftheincorrectpatches
without losing any of the correct patches. The results indicate that
our approach increasesthe precisionof program repairapproaches
with limited negative impact on the recall.
In summary, the paper makes the following main contributions.
•Weproposetwoheuristics,PATCH-SIMandTEST-SIM,which
provide indicators for patch correctness.
•We design a concrete approach that automatically classifies
patches based on the two heuristics.
•We have evaluated the approach on a large set of patches,
and the results indicate the usefulness of our approach.
The rest of the paper is organized as follows. Section 2 first
discusses related work. Section 3 motivates our approach with
examples,andSection4introducesourapproachindetails.Section5
introduces our implementation. Section 6 describes our evaluation
on the dataset of 139 patches. Section 7 discusses the threats to
validity. Finally, Section 8 concludes the paper.
2 RELATED WORK
Test-based Program Repair. Test-based program repair is often
treatedasasearchproblembydefiningasearchspaceofpatches,
usuallythroughasetofpredefinedrepairtemplates,wherethegoal
is to locate correct patches in the search space. Typical ways to
locate a patch include the follows.
•Search Algorithms. Some approaches use meta-heuristic [ 16,
18] or random [33] search to locate a patch.
•Statistics. Someapproachesbuildastatisticalmodeltoselect
thepatchesthatarelikelytofixthedefectsbasedonvarious
information sources, such as existing patches [ 12,13,19]
and existing source code [43].
•Constraint Solving. Some approaches [ 3,14,25,26,28,36]
convertthesearchproblemtoasatisfiabilityoroptimization
problem and use constraint solvers to locate a patch.
Whiletheconcretemethodsforgeneratingpatchesaredifferent,
weaktestsuitesproblemstillremainsasachallengetotest-based
program repair and may lead to incorrect patches generated. As
ourevaluationhasshown,ourapproachcaneffectivelyaugment
these existing approaches to raise their precisions.
Patch Classification. Facing the challenge of weak test suites,
several researchers also propose approaches for determining the
correctness of patches. Some researchers seek for deterministicapproaches. Xin and Reiss [ 42] assume the existence of a perfect
oracle (usually manual) to classify test results and generate new
test inputs to identify oracle violations. Yang et al. [ 46] generate
test inputs and monitor the violation of inherent oracles, including
crashesandmemory-safetyproblems.Comparedwiththem,our
approachdoesnotneedaperfectoracleandcanpotentiallyidentify
incorrectpatchesthatdonotviolateinherentoracles,buthasthe
risk of misclassifying correct patches.
Other approaches also use heuristic means toclassify patches.
Tan et al. [ 38] propose anti-patterns to capture typical incorrect
patchesthatfallintospecificstaticstructures.Ourapproachmainlyreliesondynamicinformation,andastheevaluationwillshow,thetwoapproachescanpotentiallybecombined.Yuetal.[
47]studythe
approach that filters patches by minimizing the behavioral impact
on the generated tests, and find that this approach cannot increase
the precision of existing program repair approaches. Compared
withtheirapproach,ourapproachclassifiesthegeneratedtestsand
puts different behavioral requirements on different classes. Finally,
Weimeretal.[ 40]highlightpossibledirectionsinidentifyingthe
correctness of patches.
Patch Ranking. Many repair approaches use an internal ranking
component that ranks the patches by their probability of being
correct. Patch ranking is a very related but different problem from
patch classification. On the one hand, we can convert a patch clas-
sificationproblemintoapatchrankingproblembysettingaproper
thresholdtodistinguishcorrectandincorrectpatches.Ontheother
hand, a perfect patch ranking method does not necessarily leadto a perfect patch classification method, as the threshold can be
different from defect to defect.
Therearethreemaincategoriesofpatchrankingtechniques.The
firstrankspatchesbythethenumberofpassingtests.However,this
category cannot rank plausible patches. The second category uses
syntactic[ 3,14,25]andsemanticdistances[ 3,14]fromtheoriginal
program to rank patches. As our evaluation will show later, our
approach could significantly outperform both types of distances.
The third category [ 13,19,43] learns a probabilistic model from
existingrulestorankthepatches.Ourapproachcouldcomplement
theseapproaches:asourevaluationwillshowlater,ourapproach
isabletoidentify50%oftheincorrectpatchesgeneratedbyACS,
the newest approach in this category.
Approaches to the Oracle Problem. The lack of test oracle is a
long-standingprobleminsoftwaretesting,andthesummariesof
thestudiesonthisproblemcanbefoundinexistingsurveys[ 1,31].
Among them, a few studies focus on automatically generating
heuristictestoracles.Forexample,invariantminingcouldpoten-
tially mine invariants [ 4,5] from passing test executions to classify
newtestinputs.However,theeffectofsuchanapplicationonpatch
correctnessidentificationisstillunknownasfarasweareaware
and remains as future work.
Other related work. Marinescu and Cadar [ 22] propose KATCH
forgeneratingteststocoverpatches.Ourapproachcouldbepoten-
tiallycombinedwithKATCHtoimprovethequalityofthegener-
ated tests. This is a future direction to be explored.
Mutation-basedfaultlocalizationsuchMetallaxis[ 30]andMUSE[ 27]
sharesasimilarobservationtoPATCH-SIM:whenmutatingafaulty
location, passing tests would exhibit significantly smaller behavior
790
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Identifying Patch Correctness in Test-Based Program Repair ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
public void draw(...) {
+ if(true)return ;
...
(a) An incorrect patch produced by jKali [34]
public void testDrawWithNullDataset() { ...
JFreeChart chart = ChartFactory.
createPieChart3D(" Test", null,...);
try{...
chart.draw(...);
success = true;}
catch(Exception e) {
success = false;}
assertTrue(success);
}
(b) A failing test checking for a null dataset
public void testNullValueInDataset() { ...
dataset.setValue(..., null);
JFreeChart chart = createPieChart3D(dataset);
try{...
chart.draw(...);
success = true;}
catch(Exception e) {
success = false;}
assertTrue(success);
}
(c) A passing test checking for a null value in a dataset
Figure 1: An Incorrect Patch for Chart-15
changethanfailingtests.Thisdualitybetweenapproachesindif-
ferent domains indicates that the observation is general and could
potentially be applied in more domains in future.
3 PATCH CORRECTNESS AND BEHAVIOR
SIMILARITY
Inthissection, weanalyzetherelation betweenpatchcorrectness
andbehaviorsimilaritytomotivateourapproach.Wedefineapatch
asapairofprogramversions,theoriginalbuggyversionandthe
patched version. To simplify discussion, we assume the program
contains only one fault. As a result, a failing test execution must
trigger the fault and produce an incorrect output.
WeakOraclesandPATCH-SIM. Asmentioned,atestsuitemay
beweakineitherinputsororacles,orboth,tomisssuchanincorrect
patch. To see how weak oracles could miss an incorrect patch, let
us consider the example in Figure 1. Figure 1(a) shows an incorrect
patch generated by jKali [ 34] for defect Chart-15 in the defect
benchmark Defects4J [ 11]. In this example, calling drawwill result
in an undesired exception if the receiver object is initialized with a
nulldataset.Figure1(b)showsatestcasethatdetectsthisdefect
by creating such an object and checking for exceptions. Figure 1(c)
shows a passing test checking that drawing with a null value inthe dataset would not result in an exception. This patch simplyskips the whole method that may throw the exception. In boththe passing test and the failing test, the oracle only checks that
no exception is thrown, but does not check whether the output of
the program is correct. As a result, since the patch prevents the
exception, both tests pass.
Asmentionedintheintroduction,werelyonobservationPATCH-
SIMtovalidatepatches.Givenapatch,wewouldexpectthatthe
original program and the patched program behave similarly on...
+if(repeat)
for(inti = 0; i < searchList.length; i++) {
intgreater = replacementList[i].length()
- searchList[i].length();
if(greater > 0) increase += 3 * greater;
} ...
(a) An incorrect patch generated by Nopol [45]
...
for(inti = 0; i < searchList.length; i++) {
+ if(searchList[i] == null||
+ replacementList[i] == null){
+ continue ;
+}
intgreater = replacementList[i].length()
- searchList[i].length();
if(greater > 0) increase += 3 * greater;
} ...
(b) The correct patch generated by human developers
Figure 2: An Incorrect Patch for Lang-39
a passing test execution, while behaving differently on a failing
test execution. In this example, the passing test would draw some-
thing on the chart in the original program, but would skip draw
method completelyin thepatched program,leading to significant
behavioraldifference.Basedonthisdifference,wecandetermine
the patch as incorrect.
Weak Inputs and TEST-SIM. To see how the weak test inputs
could lead to the misses of incorrect patches, let us consider the
exampleinFigure2.Thisexampledemonstratesanincorrectpatch
for Lang-39 in Defects4J produced by Nopol [ 45]. The original
program would throw an undesirable exception when an element
inreplacementList orsearchList isnull. To prevent such an
exception, the correct way is to skip those elements as shown in
Figure 2(b). Ho wever, the generated patch in Figure 2(a) blocks the
whole loop based on the value of repeat, which is a parameter of
the method. Interesting, all existing tests, either previously passed
or failed, happen to produce the correct outputs in the patchedprogram. This is because (1) the value of
repeathappens to be
trueinallpassingtestsandbe falseinallfailingtests,and(2)the
condition greater>0 isnotsatisfiedbyanyelementin searchList
andreplacementList inthefailingtests. However,enhancingthe
testoraclesbyPATCH-SIMisnotusefulbecausethebehavioron
passingtestsremainsalmostthesameastheoriginalprogramwhile
the behavior on failing tests changes a lot.
Tocapturethoseincorrectpatchesmissedbyweaktestinputs,we
need new test inputs. To utilize PATCH-SIMwith new test inputs,
weneedtoknowwhethertheoutputsofthetestsarecorrectornot.
To deal with this problem, we utilize observation TEST-SIM. We
assumethat,whentheexecutionofanewtestinputissimilarto
that of a failing test, the new test input is likely to lead to incorrect
output. Similarly, when the execution of a new test input is similar
tothatofapassingtest,thenewtestinputislikelytoleadtocorrect
output.Basedonthisassumption,wecanclassifynewtestinputs
by comparing its execution with those of the existing test inputs.
In the example in Figure 2, for any new test input triggering
this bug, there will be an exception thrown in the middle of the
loop,whichissimilartotheexecutionsoffailingtests.Ontheother
791
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, Gang Huang
hand, for any test input that does not trigger this bug, the loop
willfinishnormally,whichissimilartotheexecutionsofexisting
passing tests.
Measuring Execution Similarity. Animportantprobleminre-
alizingtheapproachishowwemeasurethesimilarityoftwotest
executions.Inourapproach,wemeasurethesimilarityof complete-
pathspectrum [10]betweenthetwoexecutions.Acomplete-path
spectrum,orCPSinshort,isthesequenceofexecutedstatement
IDs during a program execution. Several existing studies [ 2,10,35]
showthatspectraareusefulindistinguishingcorrectandfailing
test executions, and Harrold et al. [ 10] find that CPS is among the
overall best performed spectra.
For example, let us consider the two examples in Figure 1 and
Figure 2. In both examples, the defect will lead to an exception,
which further leads to different spectra of passing and failing tests:
the passing tests will execute until the end of the method, while
the failing tests will stop in the middle. Furthermore, the failingtestexecutionswillbecomedifferentafterthesystemispatched:
no exception will be thrown and the test executes to the end of the
method.
Multiple Faults. Inthecaseofmultiplefaultsinaprogram,the
twoheuristicsstillapply.Whentherearemultiplefaults,thefailing
testsmayonlyidentifysomeofthem.Wesimplytreattheidentified
faults as one fault and the rest of them as correct program, and the
above discussion still applies.
4 APPROACH
4.1 Overview
Figure 3: Approach overview
Figure 3 shows the overall process of our approach. We take the
original buggy program, a set of test cases, and a patch as input,
andproduceaclassificationresultthattellswhetherthepatchis
correct or not.
Ourapproachconsistsoffivecomponentsclassifiedintothree
categories: test generation (including test input generator ), distance
measurement(including testdistancemeasurer andpatchdistance
measurer)andresultclassification(including testclassifier andpatch
classifier).First,testinputgenerator generatesasetoftestinputs.
We then run the generated tests on the original buggy program.Duringthetestexecution,wedynamicallycollectruntimeinforma-
tion about the test execution. Based on the runtime information,test distance measurer calculates the distance between the execu-
tions of each newly generated test input and each original test
case. Adistanceis a real number indicating how different two test
executions are. The result is a vector of test distances. This vector
isthenpassedto testclassifier,whichclassifiesthetestaspassing
orfailingbycomparingitsdistancestopassingtestsandthoseto
failing tests, based on TEST-SIM.
Nowwehaveanenhancedsetoftestinputswhichareclassified
aspassingorfailingandwecanusethemtodeterminepatchcor-
rectness. Givena patch, patch distancemeasurer runs eachtest on
the original program and the patched program and measure the
distancebetweenthetwoexecutions.Theresultisavectorofpatch
distances.Finally,thisvectoristakeninto patchclassifier whichde-
termines patch correctness by the distances, based on observation
PATCH-SIM.
In the rest of this section, we introduce components in the three
categories, respectively.
4.2 Test Generation
Givenaprogram, testgenerator generatestestinputsforthispro-
gram.Furthermore,sinceourgoalistodeterminethecorrectness
of the patch, we require the generated tests to cover the patched
method.Ifapatchmodifiesmultiplemethods,thegeneratedtests
should cover at least one of them.
Intheory,anytestinputgenerationtechniquescanbeusedinour
approach.Wecanutilizesymbolicexecutiontechniquestocoverthe
specific method, especially those designed for testing patches [ 22].
Wecanalsoadoptrandomtestingtechniques[ 6,21,29]andfilter
out those that do not cover any of the modified methods.
4.3 Distance Measurement
4.3.1 Measuring Distance. As mentioned previously, we mea-
sure the distance between two executions by comparing their
complete-path spectra. As a result, the problem reduces to measur-
ingthedistancesbetweentwosequences.Ingeneral,therearemany
different metrics to measure sequence distances, such as longest
common subsequence, Levenshtein distance, Hamming distance.
Asthefirstattemptinclassifyingpatchesbasedonsequencedis-
tances,weusethelongestcommonsubsequence(LCS)asadistancemeasureandleaveotherdistancemetricstofuturework.AnLCSoftwosequences
aandbisthelongestsequencethatcanbeobtained
from both aandbby only deleting elements. We then normalize
the length of LCS into a value between 0 and 1 using the following
formula, where aandbare two sequences.
distance(a,b)=1−|LCS(a,b)|
max(|a|,|b|)
4.3.2 Test Distance Measurer. Component testdistancemeasurer
takes a generated test and calculates its distance with each orig-inal test. The result is a vector of distances, where each element
represents a distance between a generated test and an original test.
To focus on the fault, we only consider the executed statements
within the calling context of the patched methods. That is, we
locatepairsofpositionsontheruntimetrace:(a)enteringapatched
method from a method call and (b) leaving the method from the
792
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Identifying Patch Correctness in Test-Based Program Repair ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
same call and keep only the statements between the positions.
Thisstepcouldhelpusfilternoises:twotestsmaybedifferentin
statement executions outside the calling context of the patched
methods,butsuchadifferenceisoftennotrelatedtothefault.Ifan
originaltestdoesnotcoveranypatchedmethod,wealsoexclude
the test from distance measurement.
4.3.3 Patch Distance Measurer. Component patch distance mea-
surertakes each test, either generated or original and calculates
the distance between its executions on the original program and
on the patched program. The result is a vector of distances, where
each element represents the distance of a test.
Differentfrom testdistancemeasurer,hereweconsiderthefull
sequence of executed statements. This is because the compared
executions come from the same test and they are unlikely to be
noises outside the patched method.
4.4 Classification
Based on the distances, we can classify the generated tests and the
patches. We describe the two components one by one.
4.4.1 Test Classifier. Thetestclassifier classifiesthetestresult
ofageneratedtestaspassingorfailing.Somegeneratedtestsare
difficulttopreciselyclassifyandwediscardthesetests.Let result(t)
denotesthetestresultoftheoriginaltest t,i.e.,either passing,failing,
ordiscarded. Let distance(t,t/prime)denotes the distance between the
executionsof tandt/primeontheoriginal program.Givenagenerated
testt/prime,weusethefollowingformulastodetermineitsclassification
result. The formula assigns the result of the nearest-neighbor to
the generated test.
classification (t/prime)=⎧⎪⎪ ⎨
⎪⎪⎩passing Ap<Af
failing Ap>Af
discarded Ap=Af
where
Ap=min({distance(t,t/prime)|classification( t)=passinд})
Af=min({distance(t,t/prime)|classification( t)=failinд})
Notethattheaboveformulacanonlybeappliedwhenthereis
atleastapassingtest.Ifthereisnopassingtest,wecomparethe
distances with all failing tests with a threshold Ktand deem the
testaspassingifthetestexecutionissignificantlydifferentfrom
all failing tests based on the assumption that the original program
worksnormallyonmostoftheinputs.Pleasenoticethatthereis
always at least one failing test which exposes the defect.
classification (t)=/braceleftbiggpassing Kt≤Af
failing Kt>Af
where
Af=min({distance(t,t/prime)|classification( t/prime)=failinд})4.4.2 Patch classifier. Thepatchclassifier classifiesapatchas
correctorincorrectbasedonthecalculateddistances.Let distance p(t)
denotes the distance between the executions of test tbefore and
afterapplyingthepatch p.Wedeterminethecorrectnessofapatch
pusing the following formula.
classification (p)=⎧⎪⎪ ⎨
⎪⎪⎩incorrect Ap≥Kp
incorrect Ap≥Af
correct otherwise
where
Ap=max({distance p(t)|classification( t)=passinд})
Af=mean({distance p(t)|classification( t)=failinд})
This formula checks the two conditions in observation PATCH-
SIM.First,thepassingtestshouldbehavesimilarly.Tocheckthis
condition, we compare the maximum distance on the passing tests
with a threshold Kpand determine the patch as incorrect if the
behaviorchangeistoolarge.Second,thefailingtestshouldbehave
differently. However, since different defects require different ways
to fix, it is hard to set a fixed threshold. As a result, we check
whethertheaveragebehaviorchangeinfailingtestsisstilllarger
than all the passing tests. If not, the patch is considered incorrect.
We use the maximum distance for passing tests while using the
average distance for failing tests. An incorrect patch may affect
onlyafewpassingtests,andweusethemaximumdistancetofocus
onthesetests.Ontheotherhand,afterpatched,thebehaviorsof
all failing tests should change, so we use the average distance.
Please note this formula requires that we have at least a passing
test, either original or generated. If there is no passing test, we
simply treat the patch as correct.
5 IMPLEMENTATION
Wehaveimplementedourapproachasapatchclassificationtool
on Java. Given a Java program with a test suite and a patch on the
program, our tool classifies the patch as correct or not.
Inourimplementation,wechoseRandoop[ 29],arandomtest-
ingtool,asthetestgenerationtool.Sinceourgoalistocoverthe
patchedmethods,testinggenerationtoolsaimingtocoveraspecific
location seem to be more suitable, such as the tools based on sym-
bolic executions [ 32] or search-based testing [ 6]. However, we did
not use such tools because they are designed to cover the program
with fewer tests. For example, Evosuite [ 6] generates at most three
test cases per each buggy program in our evaluation subjects. Such
a small number of tests are not enough for statistical analysis.
6 EVALUATION
The implementation and the evaluation data are available online.1
6.1 Research Questions
•RQ1:TowhatextentareTEST-SIMandPATCH-SIMreliable?
•RQ2: How effective is our approach in identifying patch
correctness?
1https://github.com/Ultimanecat/DefectRepairing
793
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, Gang Huang
•RQ3:Howisourapproachcomparedwithexistingapproaches,
namely, anti-patterns, Opad, syntactic similarity and seman-
tic similarity?
•RQ4: How does test generation affect the overall perfor-
mance?
•RQ5:Howdotheparameters, KtandKp,affecttheoverall
performance?
•RQ6: What are the causes of false positive and false nega-
tives?
•RQ7: How effective is our tool in classifying developers’
correct patches?
RQ1 examines how much TEST-SIM and PATCH-SIM hold in gen-
eral.RQ2focusesontheoveralleffectivenessofourapproach.In
particular, we are concerned about how many incorrect and cor-
rect patches we filtered out. RQ3 compares our approach withfour existing approaches for identifying patch correctness. Anti-patterns [
38] capture incorrect patches by matching them with
pre-definedpatterns.Opad[ 46]isbasedoninherentoraclesthat
patches should not introduce new crashes or memory-safety prob-
lem.Syntacticsimilarity[ 3,14,25]andsemanticsimilarity[ 3,14]
are patch ranking techniques that rank patches by measuring, syn-
tactically or semantically, how much their changes the program,
whichcouldbe adaptedtodeterminepatch correctnessbysetting
a proper threshold. RQ4 and RQ5 explore how different configu-
rationsofourapproachcould affecttheoverallperformance.RQ6
investigates the causes of wrong results in order to guide future
research. Finally, as will be seen in the next subsection, though we
have tried out best to collect the generated patches on Java, the
correctpatcheswerestillsmallinnumbercomparedwithincorrectpatches.Tooffsetthis,RQ7furtherinvestigatestheperformanceof
our approach on the developers’ correct patches.
6.2 Dataset
We have collected a dataset of generated patches from existing pa-
pers.Table1showsthestatisticsofthedataset.Ourdatasetconsists
of patches generated by six program repair tools. Among the tools,
jGenProg is a reimplementation of GenProg [ 15,16,41] on Java, a
repair tool based on genetic algorithm; jKali is a reimplementation
ofKali[34]onJava,arepairtoolthatonlydeletesfunctionalities;
Nopol[45]isatoolthatreliesonconstraintsolvingtofixincorrect
conditionsandtwoversions,2015[ 23]and2017[ 45],areusedin
our experiment; HDRepair [ 13] uses information from historical
bugfixestoguidethesearchprocess;ACS[ 43]isatoolbasedon
multiple information sources to statistically and heuristically fixincorrect conditions. The selected tools cover the three types of
patchgenerationapproaches:searchalgorithms(jGenProg,jKali),
constraint-solving(Nopol)andstatistical(HDRepair,ACS).More
details of the three types can be found in the related work section.
The patches generated by jGenProg, jKali and Nopol2015 are
collected from Martinez et al.’s experiments on Defects4J [ 23]. The
patches generated by Nopol2017 are collected from a recent report
on Nopol [ 45]. Patches generated by HDRepair is obtained from
Xin and Reiss’ experiment on patch classification [ 42]. The patches
generated by ACS is collected from ACS evaluation [43].
All the patches are generated for defects in Defects4J [ 11], a
widely-usedbenchmarkofrealdefectsonJava.Defects4Jconsistsof six projects: Chart is a library for displaying charts; Math is alibraryforscientificcomputation;Timeisalibraryfordate/timeprocessing; Lang is a set of extra methods for manipulating JDK
classes;ClosureisoptimizedcompilerforJavascript;Mockitoisa
mocking framework for unit tests.
Some of the patches are not supported by our implementa-
tion, mainly because Randoop cannot generate any tests for these
patches. Inparticular, Randoop cannotgenerate anytests forClo-
sure and Mockito. We removed these unsupported patches.
The patches from Martinez et al.’s experiments, the ACS eval-
uation and Qi et al.’s experiments contains labels identifying the
correctnessofthepatches,whichmarkthepatchesas correct,in-
correct,o r unknown. The patches of Nopol2017 do not contain such
labels.Wemanuallycheckedwhethertheunlabeledpatchesand
some labeled patches are semantically equivalent to the human-
writtenpatches. Sincethepatcheswhose correctnessisunknown
cannot be used to evaluate our approach, we remove these patches.
Intheend,wehaveadatasetof139patchesgeneratedbyauto-
matic program repair tools, where 110 are incorrect patches and 29
are correct patches.
ToanswerRQ6,wealsoaddedalldeveloperpatchesonDefects4J
intoourdataset.Sameasgeneratedpatches,weremovedtheun-
supportedpatches,includingallpatchesonClosureandMockito.
In the end, we have 194 developer patches. Please note that de-
veloper patches are only used in RQ6 since they have different
characteristics compared with generated patches.
6.3 Experiment Setup
Test Generation. We kept Randoop to run 3 minutes on the orig-
inalprogramandcollectedtheteststhatcoveredthepatchedmeth-
ods.Westopat3minutesbecauseformostdefects,Randooppro-
duced enough tests within three minutes, and for the remaining
defects that do not have enough tests, lengthening the time would
not lead to more tests. We then randomly selected 20 tests for each
patch. If there were fewer than 20 tests, we selected all of them. In
the end, we have 7.1 tests per patch in average, with a minimum of
0test.BasedontheclassificationofTEST-SIM,71%ofthegenerated
tests are passing tests.
RQ1.To evaluate PATCH-SIM, we measured the average distance
between test executions of patched and unpatched versions, and
check whether there is significant differences between passing and
failingtestsoncorrectandincorrectpatches.ToevaluateTEST-SIM,
we measured the distances between tests, and analyzed whether
closer distances indicate similar test results.
RQ2.Weappliedourapproachtothepatchesinourdatasetand
checkedwhetherourclassificationresultsareconsistentwiththe
labels about correctness.RQ3.
Weappliedthefourexistingapproachestothedatasetand
compared their results with our result.
Anti-patterns was originally implemented in C. To apply anti-
patternsonJavadataset,wetookthesevenanti-patternsdefined
byTanetal.[ 38]andmanuallycheckedwhetherthepatchesinour
dataset fall into these patterns.
Opad [46] uses inherent oracles that patches should not intro-
duce newcrash ormemory-safety problems. Opadwas originally
designedforCandweneedtoadaptitforJava.Ontheonehand,
794
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Identifying Patch Correctness in Test-Based Program Repair ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 1: Dataset
ProjectjGenprog jKali Nopol2015 Nopol2017 ACS HDRepair Total(Generated) Developer Patches
PCIPCIPCIPCIPCIPCIPCIPCI
Chart 6066066156062200002632325250
Lang 0000007344043121011541158580
Math 1459101915114220221511472583206384840
Time 2022021018081101101521327270
Total 2251718117295244004021156936139291101941940
P=Patches, C=Correct Patches, I=Incorrect Patches
crashesarerepresentedasruntimeexceptionsinJava.Ontheother
hand, memory-safety problems are either prevented by the Java
infrastructure or detected as runtime exceptions. Therefore, we
uniformly detect whether a patch introduces any new runtime
exception on test runs. If so, the patch is considered incorrect.
Regarding syntactic and semantic distances, different papers [ 3,
14,25] have proposed different metrics to measure the syntactic
and semantic distances and a summary can be found in Table 2.
However, as analyzed in the table, many of the metrics are defined
for a specific category of patches and cannot be applied to general
patches. In particular, many metrics are designed for expression
replacementonlyandtheirdefinitionsonothertypesofchangesare
notclear.Asaresult,wechosethetwometricsmarkedas"general"
in Table 2 for comparison: one measuring syntactic distance by
comparingASTandonemeasuringsemanticdistancebycomparing
complete-path spectrum.
TheAST-basedsyntacticdistanceisdefinedastheminimalnum-
ber of AST nodes that need to be deleted or inserted to change the
oneprogramintotheotherprogram.Forexample,changingexpres-sion
a>b+1toc<d+1needstoatleastremovethreeASTnodes
(a,b,>) and insert three AST nodes ( c,d,<), giving a syntactic dis-
tanceof6.Thesemanticdistancebasedoncomplete-pathspectrum
forprogram pisdefined usingthefollowingformula, where Tois
the set of all original tests that cover at least one modified method
anddistance pis defined in Section 4.4.2.
LED(p)=mean({distance p(t)|t∈To})
Thesyntactic/semanticdistancegavearankedlistofthepatches.
Then we checked if we could find an optimal threshold to separate
the list into correct and incorrect patches.
RQ4.Weconsideredtwodifferenttestgenerationstrategiesand
compared their results with the result of RQ1.
•Nogeneration. Thisstrategysimplydoesnotgenerateany
testinput.Thisstrategyservesasabaselineforevaluating
how much the newly generated test inputs contribute to the
overall performance.
•Repeated Randoop runs. Since Randoop is a random test
generation tool, different invocations to Randoop may lead
to different test suites. This strategy simply re-invokes Ran-
dooptogenerateapotentiallydifferentsetoftestsandthe
comparison helps us understand the effect of the random-
ness in test generation on the overall performance of our
approach.
Since the second strategy involves re-generating the tests and isexpensive to perform, we evaluated this strategy on a randomlyselectedsubsetof50patches.Toseetheeffectofrandomness,we
repeated the experiments for 5 times.
RQ5.Duringtheexperimentsforthefirstthreeresearchquestions,
we setthe parametersof ourapproach asfollows: Kp= 0.25,Kt=
0.4. These parameter values are determined by a few attempts on a
small set of patches.
ToanswerRQ4,wesystematicallysetdifferentvaluesforparam-
etersKpandKtandthenanalyzedhowtheseparametersaffectour
result on the whole dataset.RQ6.
We manually analyzed all false positives and false negatives
to understand the causes of false classification and summarize the
reasons.
RQ7.We applied our approach on human-written patches pro-
vided by Defects4J benchmark and check whether our approach
misclassified them as incorrect or not.
Hardware Platform. The experiment is performed on a server
with Intel Xeon E3 CPU and 32GB memory.
6.4 Result of RQ1: Reliability of Heuristics
Table 3 shows the results of evaluating PATCH-SIM, i.e., the dis-
tancesbetweentestexecutionsonpatchedandunpatchedversions.
As we can see from the table, for correct patches, the distances
of passing tests are very close to zero, while failing tests have a
muchlargerdistancesthatis9.5timesofpassingtests.Theresult
indicates that PATCH-SIM holds in general. On the other hand,
thepassingtestsandthefailingtestsdonotexhibitsuchastrong
propertyonincorrectpatches.Whilethedistancesoffailingtests
are still larger than passing tests, the ratio is only 1.32 times rather
than9.5times.ThisresultsindicatethatPATCH-SIMcanbeused
to distinguish correct and incorrect patches.
Figure4showstheresultsofevaluatingTEST-SIM.TheX-axis
shows intervals of distances while the Y-axis shows the percentage
of tests fall into the intervals. As we can see from the figure, when
twotestshaveashortdistance,theyaremorelikelytohavethesame
testresultsratherthandifferenttestresults.Thisresultindicates
that TEST-SIM holds in general. On the other hand, when the two
tests have a long distance, they are more likely to have different
test results rather than the same test results.
6.5 Result of RQ2: Overall Effectiveness
Table 4 and Table 5 shows the performance of our approach on
thedatasetpertoolandperproject,respectively.Asshowninthe
tables, our approach successfully filtered out 62 of 110 incorrect
plausiblepatchesandfilteredoutnocorrectpatch.Furthermore,our
795
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, Gang Huang
Table 2: Different syntactic and semantic metrics
Metric TypeScope Description
AST-based [14, 25] SynGeneral Number of AST node changes introduced by the patch.
Cosine similarity [14] SynReplacement ThecosinesimilaritybetweenthevectorsrepresentingASTnodeoccurrencesbefore
and after the change. It is not clear how to apply it to insertions and deletions because
there will be a zero vector and cosine similarity cannot be calculated.
Locality of variables
and constants [14]SynExpression
ReplacementThe distance is measured by the Hamming distance between the vectors representing
locations of variables and constants. It is not clear how to apply it to patches with
multiple changes.
Expression size dis-
tance [3]SynExpression
ReplacementThe distance is 0 when two expressions are identical, otherwise the size of the affected
expressionafterpatching.Itisnotclearhowtoapplyittochangesotherthanexpression
replacement.
Complete-path spec-
trum [3]SemGeneral The difference between complete-path spectra.
Model counting [14] SemBoolean
Expression
replacementThedistanceismeasuredbythenumberofmodelsthatmaketwoexpressionsevaluate
differently. The definition is bounded to Boolean expressions.
Output coverage [14] SemPrograms
with simple
outputsThedistanceismeasuredbytheproportionofdifferentoutputscoveredbythepatched
program. It is not clear how to define “output” in general for complex programs.
"Syn" and "Sem" stand for syntactical distance and semantic distance respectively.
Table 3: PATCH-SIM
Passing Tests Failing Tests
Incorrect Patches 0.25 0.33
Correct Patches 0.02 0.19
X-axis: intervals of distance on tests Y-axis: percent of tests
Figure 4: TEST-SIM
approachshowssimilarperformanceondifferenttoolsanddifferent
projects, indicating that our results are potentially generalizable to
different types of projects and different types of tools.
Please note that although our approach did not filter out any
correct patch on our dataset, in theory it is still possible to filter
out correct patches. For example, a patch may significantly changeTable 4: Overall Effectiveness per Tool
Tool Incorrect Correct Incorrect
ExcludedCorrect
Excluded
jGenprog 17 5 8(47.1%) 0
jKali 17 1 9(52.9%) 0
Nopol2015 24 5 16(66.7%) 0
Nopol2017 40 0 22(55.0%) 0
ACS 6 15 3(50.0%) 0
HDRepair 6 3 4(66.7%) 0
Total 110 29 62(56.3%) 0
“In/correctExcluded”showsthenumberofpatchesthatarefiltered
out by our approach and are in/correct.
Table 5: Overall Effectiveness per Project
Project Incorrect Correct Incorrect
ExcludedCorrect
Excluded
Chart 23 3 14(60.9%) 0
Lang 11 4 6(54.5%) 0
Math 63 20 33(52.4%) 0
Time 13 2 9(69.2%) 0
Total 110 29 62(56.3%) 0
the control flow of a passing test execution, e.g., by using a new
algorithmorcallingasetofdifferentAPIs,butthetestexecution
couldproducethesameresult.However,giventhestatusofcurrentprogramrepairapproaches,suchpatchesareprobablyscarce.When
applying our approach on human-written patches, some correct
patchesarefilteredout.Moredetailsfortheeffectivenessonhuman-
written patch is discussed in RQ6.
796
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Identifying Patch Correctness in Test-Based Program Repair ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Ourapproachtookabout5to10minutestodeterminethecor-
rectnessofapatchinmostcases,whilesomepatchesmighttake
upto30minutes.Mostofthetimewasspentongeneratingthetest
inputs and recording the runtime trace.
6.6 Result of RQ3: Comparing with Others
Anti-patterns. Amongall139patches,anti-patternsfilteredout
28patches,where27areincorrectand1iscorrect.Theresultshows
that our approach significantly outperforms anti-patterns. Further-
more, 13 of the 27 incorrect patched filtered out by anti-patterns
were also filtered out by our approach, while the remaining 14
patches were not filtered out by our approach. This result suggests
thatwemay potentiallycombinethetwo approachestoachievea
better performance.
Opad.When applied with the same set of test inputs as our ap-
proach,Opadfailedtorecognizeanyoftheincorrectpatches.To
further understand whether a stronger test suite could achieve bet-
ter results, we further selected up to 50 tests instead of 20 tests for
each patch. This time Opad filtered out 3 incorrect patches. This
resultsuggeststhatinherentoraclesmayhavealimitedeffecton
classifying Java patches, as the Java infrastructure has already pre-
ventedalotofcrashesandmemorysafetyproblemsanditmaynot
be very easy for a patch to break such an oracle.
X-axis: intervals of syntactic distance Y-axis: numbers of patches
Figure 5: Syntactic distance
Syntactic and Semantic Distance. Fig. 5 shows the distribution
of incorrect patches and correct patches on syntactic distance. The
x-axisshowstheintervalsofdistanceswhileY-axisshowsthenum-
bers of patches within the intervals. As we can see from the figure,
theincorrectpatchesandcorrectpatchesappearinallintervalsand
the distribution shows no particular characteristics. If we would
like to exclude 56.3% incorrect patches using syntactic distance,
we need to at least exclude 66.7% of the correct patches. This re-
sult indicates that syntactic distance cannot be directly adapted to
determine patch correctness.
Fig. 6 shows the distribution of incorrect and correct patches
on semantic distance. As we can see from the figure, both types of
patches tend to appear more frequently when the distance is small.
X-axis: intervals of semantic distance Y-axis: numbers of patches
Figure 6: Semantic distance
When the distance grows larger, both types decrease but correct
patches decrease faster. If we would like to exclude 56.3% incorrect
patches using semantic distance, we need to exclude 43.3% correct
patches. This result indicates that semantic distance could be a
bettermeasurementthansyntacticdistanceindeterminingpatch
correctness, but is still significantly outperformed by our approach.
Please note that the above results do not imply that syntactic
andsemanticdistancesarenotgoodatrankingpatches.Whileit
isdifficulttofindathresholdtodistinguishcorrectandincorrect
patches for a group of defects, it is still possible that the correct
patches are ranked higher on most individual defects.
6.7 Result of RQ4: Effects of Test Generation
Table6showstheresultwithoutgeneratingtestinputs.Without
the generated test inputs, our approach filtered out 8 less incorrect
patchesandstillfiltered0correctpatch.Thisresultsuggeststhat
PATCH-SIM alone already makes an effective approach, but test
generationandTEST-SIMcanfurtherboosttheperformancenon-
trivially.
Table 6: Comparison with no test generation
Default Approach No Generation
Incorrect Excluded 62 54
Correct Excluded 00
Regardingrandomness,werepeatedourexperimentonthese-
lected50patches5timesandgotdifferentresultsononly3incorrect
patches. The best case had only one more excluded incorrect patch
than the worst case. The result shows that randomness does affect
the result, but the impact is limited.
6.8 Result of RQ5: Parameters
Two parameters are involved in our approach, KtandKp, both
ranging from 0 to 1. The results of our approach with different
parameters are shown in Table 7 and 8, where each column shows
theresultwiththeparametervalueinthetablehead.Aswecansee
797
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Yingfei Xiong, Xinyuan Liu, Muhan Zeng, Lu Zhang, Gang Huang
Table 7: Parameter Kp
0.05 0.1 0.15 0.25 0.3 0.4 0.5 0.6 0.8 1
IE71 66 62 62 60 57 56 56 55 54
CE4 10 0 000000
IE = Incorrect Excluded, CE = Correct Excluded
Table 8: Parameter Kt
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
IE65 62 62 62 62 60 60 60 60 59 59
CE41000000000
IE = Incorrect Excluded, CE = Correct Excluded
from thetables,settingthe parameterstodifferentvalueshave a
limitedimpactontheoverallresultsandalargerangeofparameter
value could achieve the best performance. The result indicates that
our approach does not require a precise tuning of parameters.
6.9 Result of RQ6: Causes of Wrong Result
Ourapproachgavewrongresultson47incorrectpatches.Weman-
ually analyzed these patches and identified three main causes of
the wrong classification, as follows.
Too weak test suite. It is often the case (21 out of 48) that only
onefailingtestcoversthepatchedmethod.Withoutpassingtest,
our approach only relies on the threshold Ktto classify the tests
anditissometimesdifficulttogenerateteststhatpassthethreshold.
Asaresult,wemighthavenooronlyafewpassingteststoperform
the patch classification, leading to a low performance.
Unsatisfying test generation. Another common case (27 out of
48, 9 overlap with the previous case) is that test generation tool
failstogeneratesatisfyingtests.Randoopmightfailtogivetests
thatcoverthepatchedmethodorfailtogenerateteststhatcould
expose the incorrect behavior. The patches in this category have
the potential to be correctly identified if we use a stronger test
generation tool.
Unsatisfyingclassificationformula. Thefinalcase(8outof48)
wascausedbylargebehaviorchangesinsomefailingtestexecution.
Since we calculated the average distance of all failing test execu-
tions in the patch classification, if there was a very large value, the
averagevaluemightbecomelargeevenifalltherestfailingtests
had small behavior changes. As a result, the patch may be misclas-
sified.Thisproblemmaybefixedbygeneratingmorefailingtest
cases to lower down the average value, or to find a better formula
to classify the patches.
6.10 Result of RQ7: Developer Patches
Among the 194 correct developer patches, our tool classified 16
(8.25%)patchesasincorrect.Wefurtheranalyzedwhythe16patches
aremisclassifiedandfoundthatallthe16patcheshavenon-trivially
changedthecontrolflowandcausedasignificantdifferenceinCPS
inthepassingtestexecutions.Inparticular,thebehaviorsofpassing
testshavesignificantlychangedin6patches,whileintherest10
patches the behaviors remain almost the same but the executedstatementschangedsignificantly(e.g.,callingadifferentmethodwith the same functionality). The results imply that (1) human
patches are indeed more complex than those generated by current
automatedtechniques;(2)whenthecomplexityofpatchesgrows,
ourapproachisprobablystilleffectiveasonlyasmallportionofcor-
rectpatchesisexcluded;(3)Tofurtherenhancetheperformance,we
need to enhance PATCH-SIM and CPS to deal with such situations.
7 THREATS TO VALIDITY AND LIMITATIONS
The main threat to internal validity is that we discarded some
patchesfromourdataset,eitherbecausetheircorrectnesscannot
bedetermined,orbecausetheinfrastructuretoolusedinourimple-
mentationcannotsupportthesepatches.Asaresult,aselectionbias
may beintroduced. However, we believethis threat isnot serious
becausetheremovedpatchesaresmallinnumbercomparedwith
thewholedatasetandtheresultsonthesepatchesareunlikelyto
significantly change the overall results.
The main threat to external validity is whether our approach
canbegeneralizedtodifferenttypesofprogramrepairtools.While
we haveselectedrepairtoolsfrom allmain categoriesof program
repair tools, including tools based on search algorithms, constraint
solving and statistics, it is still unknown whether future tools will
havecharacteristicssignificantlydifferentfromcurrenttools.Tominimize such a threat, we have added RQ7 to test on developer
patches, which can be viewed as the ultimate goal of automatically
generated patches. The results indicates that our approach mayhave different performance on developer patches and generated
patches, but the difference is limited.
Themainthreattoconstructvalidityisthatthecorrectnessofthe
patchesaremanuallyevaluatedandtheclassificationmaybewrong.
To reduce this threat, all difficult patches are discussed through
the first two authors to make a mutual decision. Furthermore, part
oftheclassificationcomesfromMartinezetal.’sexperiment[ 23],
whoseresultshavebeenpublishedonlineforafewyearsandthere
isnoreportquestioningtheclassificationqualityasfarasweare
aware.
Therecanbemanydifferentchoicesindesigningtheformulas.
For example, we can use a different sequence distance or even a
different spectrum to measure the distance of two executions. We
canusedifferentstatisticalmethodsforclassifyingtestsandpatches.
The current paper does not and cannot explore all possibilities and
leave them as future work.
8 CONCLUSION
Inthispaper,wehaveproposedanapproachtoautomaticallyde-
terminingthecorrectnessofthepatchesbasedonbehaviorsimilar-
ities between program executions. As our evaluation shows, our
approach could effectively filter out 56.3% of the incorrect patches
generated without losing any of the correct patches. The resultsuggests that measuring behavior similarity can be a promising
way to tackle the oracle problem and calls for more research on
this topic.
REFERENCES
[1]Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo.
2015. TheOracleProbleminSoftwareTesting:ASurvey. IEEETrans.Software
Eng.41, 5 (2015), 507–525. https://doi.org/10.1109/TSE.2014.2372785
798
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Identifying Patch Correctness in Test-Based Program Repair ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
[2]William Dickinson, David Leon, and Andy Podgurski. 2001. Pursuing failure:
thedistributionofprogramfailuresinaprofilespace.In Proceedingsofthe8th
European Software Engineering Conference held jointly with 9th ACM SIGSOFT
International Symposium on Foundations of Software Engineering 2001, Vienna,
Austria, September 10-14, 2001, A. Min Tjoa and Volker Gruhn (Eds.). ACM, 246–
255. https://doi.org/10.1145/503209.503243
[3]Loris DâĂŹAntoni, Roopsha Samanta, and Rishabh Singh. 2016. Qlose: Program
repairwithquantitativeobjectives.In InternationalConferenceonComputerAided
Verification. Springer, 383–401.
[4]MichaelD.Ernst,JakeCockrell,WilliamG.Griswold,andDavidNotkin.2001. Dy-
namically Discovering Likely Program Invariants to Support Program Evolution.
IEEE Trans. Software Eng. 27, 2 (2001), 99–123. https://doi.org/10.1109/32.908957
[5]Michael D. Ernst, Jeff H. Perkins, Philip J. Guo, Stephen McCamant, Carlos
Pacheco, Matthew S. Tschantz, and Chen Xiao. 2007. The Daikon system for
dynamicdetectionoflikelyinvariants. Sci.Comput.Program. 69,1-3(2007),35–45.
https://doi.org/10.1016/j.scico.2007.01.015
[6]GordonFraserandAndreaArcuri.2011. Evosuite:automatictestsuitegeneration
for object-oriented software. In ESEC/FSE. ACM, 416–419.
[7]QingGao,YingfeiXiong,YaqingMi,LuZhang,WeikunYang,ZhaopingZhou,
Bing Xie, and Hong Mei. 2015. Safe Memory-Leak Fixing for C Programs. In
Proceedings of the 37th International Conference on Software Engineering-Volume1. IEEE Press, 459–470.
[8]
QingGao,HanshengZhang,JieWang,andYingfeiXiong.2015. FixingRecurring
Crash Bugs via Analyzing Q&A Sites. In ASE. 307–318.
[9]Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:
Fixing Common C Language Errors by Deep Learning. In AAAI. 1345–1351.
[10]MaryJeanHarrold,GreggRothermel,RuiWu,andLiuYi.1998. AnEmpirical
Investigation of Program Spectra. In Proceedings of the SIGPLAN/SIGSOFT Work-
shoponProgramAnalysisForSoftwareToolsandEngineering,PASTE’98,Montreal,
Canada, June 16, 1998, Thomas Ball, Frank Tip, and A. Michael Berman (Eds.).
ACM, 83–90. https://doi.org/10.1145/277631.277647
[11]RenéJust,DarioushJalali,andMichaelDErnst.2014. Defects4J:Adatabaseof
existingfaultstoenablecontrolledtestingstudiesforJavaprograms.In ISSTA.
437–440.
[12]Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic
patch generation learned from human-written patches. In ICSE ’13. 802–811.
[13]Xuan-Bach D Le, David Lo, and Claire Le Goues. 2016. History Driven Program
Repair. In Software Analysis, Evolution, and Reengineering (SANER), 2016 IEEE
23rd International Conference on, Vol. 1. IEEE, 213–224.
[14]Xuan-Bach D. Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser.
2017. S3: syntax- and semantic-guided repair synthesis via programming by
examples.In Proceedingsofthe201711thJointMeetingonFoundationsofSoftware
Engineering,ESEC/FSE2017,Paderborn,Germany,September4-8,2017.593–604.
https://doi.org/10.1145/3106237.3106309
[15]ClaireLeGoues,MichaelDewey-Vogt,StephanieForrest,andWestleyWeimer.
2012. A Systematic Study of Automated Program Repair: Fixing 55 out of 105
Bugs for $8 Each. In ICSE. 3–13.
[16]C. Le Goues, ThanhVu Nguyen, S. Forrest, and W. Weimer. 2012. GenProg: A
Generic Method for Automatic Software Repair. Software Engineering, IEEE
Transactions on 38, 1 (Jan 2012), 54–72.
[17]XuliangLiuandHaoZhong.2018. MiningStackOverflowforProgramRepair.
(2018), to appear pages.
[18]FanLongandMartinRinard.2015. Stagedprogramrepairwithconditionsyn-
thesis.In Proceedingsofthe201510thJointMeetingonFoundationsofSoftware
Engineering,ESEC/FSE2015,Bergamo,Italy,August30-September4,2015.166–178.
https://doi.org/10.1145/2786805.2786811
[19]Fan Long and Martin Rinard. 2016. Automatic patch generation by learning
correctcode.In Proceedingsofthe43rdAnnualACMSIGPLAN-SIGACTSymposium
on Principles of Programming Languages, POPL 2016, St. Petersburg, FL, USA,
January 20 - 22, 2016. 298–312. https://doi.org/10.1145/2837614.2837617
[20]FanLongandMartinC.Rinard.2016.Ananalysisofthesearchspacesforgenerate
andvalidatepatchgenerationsystems.In Proceedingsofthe38thInternational
Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016.
702–713. https://doi.org/10.1145/2884781.2884872
[21]Lei Ma, Cyrille Artho, Cheng Zhang, Hiroyuki Sato, Johannes Gmeiner, andRudolf Ramler. 2015. GRT: Program-analysis-guided random testing. In ASE.212–223.
[22]PaulDanMarinescuandCristianCadar.2013. KATCH:High-coverageTesting
of Software Patches. In ESEC/FSE. 235–245.
[23]Matias Martinez,Thomas Durieux, Romain Sommerard, Jifeng Xuan, andMartinMonperrus.2016. AutomaticrepairofrealbugsinJava:Alarge-scaleexperiment
on the Defects4J dataset. Empirical Software Engineering (2016), 1–29.
[24]MatiasMartinezandMartinMonperrus.2016. ASTOR:AProgramRepairLibrary
for Java. In Proceedings of ISSTA, Demonstration Track. 441–444. https://doi.org/
10.1145/2931037.2948705
[25]SergeyMechtaev,JooyongYi,andAbhikRoychoudhury.2015. DirectFix:Looking
for Simple Program Repairs. In ICSE. 448–458.
[26]Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable
Multiline Program Patch Synthesis via Symbolic Analysis. In ICSE. 691–701.
[27]Seokhyeon Moon, Yunho Kim, Moonzoo Kim, and Shin Yoo. 2014. Ask the
Mutants: Mutating Faulty Programs for Fault Localization. In ICST. 153–162.
[28]Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chan-
dra. 2013. SemFix: Program Repair via Semantic Analysis. In ICSE. 772–781.
[29]CarlosPachecoandMichaelD.Ernst.2007. Randoop:Feedback-directedRandom
Testing for Java. In OOPSLA. 815–816.
[30]Mike Papadakis and Yves Le Traon. 2012. Using Mutants to Locate "Unknown"
Faults. In ICST. 691–700.
[31]MauroPezzeandChengZhang.2015. AutomatedTestOracles:ASurvey. Ad-
vances in Computers 95 (2015), 1–48.
[32]Corina S. Păsăreanu and Neha Rungta. 2010. Symbolic PathFinder: Symbolic
Execution of Java Bytecode. In ASE. 179–180.
[33]YuhuaQi,XiaoguangMao,YanLei,ZiyingDai,andChengsongWang.2014. The
Strength of Random Search on Automated Program Repair. In ICSE. 254–265.
[34]ZichaoQi,FanLong,SaraAchour,andMartinC.Rinard.2015. Ananalysisof
patch plausibility and correctness for generate-and-validate patch generation
systems. In ISSTA. 24–36.
[35]Thomas Reps, Thomas Ball, Manuvir Das, and James Larus. 1997. The use of
programprofilingforsoftwaremaintenancewithapplicationstotheyear2000
problem. In ESEC/FSE. Springer, 432–449.
[36]RishabhSingh,SumitGulwani,andArmandoSolar-Lezama.2013. Automated
FeedbackGenerationforIntroductoryProgrammingAssignments.In PLDI.15–
26.
[37]Edward K Smith, Earl T Barr, Claire Le Goues, and Yuriy Brun. 2015. Is the
cure worse than the disease? overfitting in automated program repair. In FSE.
532–543.
[38]Shin Hwei Tan, Hiroaki Yoshida, Mukul R Prasad, and Abhik Roychoudhury.
2016. Anti-patterns in Search-Based Program Repair. In FSE. 727–738.
[39]Yida Tao, Jindae Kim, Sunghun Kim, and Chang Xu. 2014. Automatically Gener-
ated Patches As Debugging Aids: A Human Study. In FSE. 64–74.
[40]Westley Weimer,Stephanie Forrest, Miryung Kim, Claire Le Goues, and Patrick
Hurley.2016. TrustedSoftwareRepairforSystemResiliency.In DSN-W.238–241.
[41]WestleyWeimer,ThanhVuNguyen,ClaireLeGoues,andStephanieForrest.2009.
Automaticallyfindingpatchesusinggeneticprogramming.In ICSE’09.364–374.
[42]QiXinandStevenReiss.2017. IdentifyingTest-Suite-OverfittedPatchesthrough
Test Case Generation. In ISSTA.
[43]Yingfei Xiong, Jie Wang, Runfa Yan, Jiachen Zhang, Shi Han, Gang Huang, and
Lu Zhang. 2017. Precise Condition Synthesis for Program Repair. In ICSE.
[44]Yingfei Xiong, Hansheng Zhang, Arnaud Hubaux, Steven She, Jie Wang, and
Krzysztof Czarnecki. 2015. Range fixes: Interactive error resolution for software
configuration. Software Engineering, IEEE Transactions on 41, 6 (2015), 603–619.
[45]JifengXuan,MatiasMartinez,FavioDemarco,MaximeClément,SebastianLame-
las, Thomas Durieux, Daniel Le Berre, and Martin Monperrus. 2016. Nopoly:
AutomaticRepair ofConditionalStatementBugs inJavaPrograms. IEEETrans-
actions on Software Engineering (2016).
[46]JinqiuYang,AlexeyZhikhartsev,YuefeiLiu,andLinTan.2017. BetterTestCases
for Better Automated Program Repair. In FSE.
[47]ZhongxingYu,MatiasMartinez,BenjaminDanglot,ThomasDurieux,andMartin
Monperrus.2017. TestCaseGenerationforProgramRepair:AStudyofFeasibility
and Effectiveness. CoRRabs/1703.00198 (2017).
799
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. 