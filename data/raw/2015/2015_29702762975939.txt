Developer Targeted Analytics: Supporting Software
Development Decisions with Runtime Information
J√ºrgen Cito
software evolution & architecture lab
University of Zurich, Switzerland
cito@iÔ¨Å.uzh.ch
http://www.i.uzh.ch/seal/people/cito.html
ABSTRACT
Runtime information of deployed software has been used by
business and operations units to make informed decisions
under the term \analytics". However, decisions made by
software engineers in the course of evolving software have,
for the most part, been based on personal belief and gut-
feeling. This could be attributed to software development
being, for the longest time, viewed as an activity that is de-
tached from the notion of operating software in a production
environment. In recent years, this view has been challenged
with the emergence of the DevOps movement, which aim is
to promote cross-functional capabilities of development and
operations activities within teams. This shift in process and
mindset requires analytics tools that specically target soft-
ware developers. In this research, I investigate approaches
to support developers in their decision-making by incorpo-
rating runtime information in source code and provide live
feedback in IDEs by predicting the impact of code changes.
CCS Concepts
Software and its engineering !Software perfor-
mance;General and reference !Metrics;
Keywords
Developer Targeted Analytics; Software Analytics; Perfor-
mance Engineering
1. PROBLEM STATEMENT
The widespread availability of broadband internet has en-
abled many aspects of computing to move online as an \as-
a-Service" model. Many dierent types of software, rang-
ing from enterprise to end-user applications, are delivered
as Software-as-a-Service (SaaS). This modern type of soft-
ware often comes with high velocity of releases, with push of
more frequent, but smaller changes, often deployed on cloudinfrastructure. As a consequence, in modern software devel-
opment, the boundaries between building software and op-
erating it in production are blurring. This has been further
underlined by the larger DevOps movement, which aims on
achieving cross-functional capabilities of development and
operations in teams. The movement is also slowly leading
to a shift in mindset of development teams when it comes
to operability of newly produced pieces of their systems.
Decisions made during software development cannot be
considered isolated from their implications in production en-
vironments. This shift in mindset and process requires de-
velopers be able to anticipate runtime issues before they
occur and leverage data-driven decision-making based on
runtime information observed from software in production.
However, production infrastructures, specically in the cloud,
are complex distributed systems [1]. If an issue has occurred
within a production environment, we have to able to lever-
age information on the evolution of the whole stack and the
ability to establish links from dierent aspects of runtime
information back to code.
1.1 Motivating Example
To provide a frame for further discussion, let us imag-
ine a team-centric VoIP chat client (similar to Slack1or
HipChat2) where users log in and interact with multiple
teams over chat and VoIP. This application is based on a
microservice architecture, where each service is deployed as
a scalable unit in the cloud.
Scenario. One of the microservices of the application is
the login that retrieves information from a 3rd party provider
(e.g., prole pictures from Facebook). In the development
and test environment, 3rd parties are mocked out to pro-
vide isolation in testing scenarios. A new change requests
asks to enable the login for all teams of a user at once (in-
stead of logging in for one team at a time). The code change
implementing this change request passed all tests and went
live in the production environment. There, it lead to sig-
nicant performance regressions that were reported by end
users. After careful inspection of operational logs in pro-
duction and version control history, it turned out that the
change introduced a new (blocking) loop that incorporated
the request to the 3rd party service. As a single operation,
the request did not cause any performance degradations in
the past. However, when the method needed to scale to
multiple teams, it lead to a severe performance degradation.
1https://slack.com/
2https://www.hipchat.com
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2975939
892
Problem. Data combinatorics in production environ-
ments are so dierent to what prolers and tests can sim-
ulate either locally or in staging environments. Especially
in the cloud, scalable infrastructure [2] requires approaches
to leverage information gathered at production runtime and
provide feedback to software engineers during development.
2. PROPOSED RESEARCH
The objective of my research is to enable data-driven de-
cision making during development of software for the cloud.
To achieve this objective, the following research questions
are examined:
(RQ1) How is software being developed for the cloud and
what kind of tools and data are being leveraged for decision-
making?
(RQ2) How can we support data-driven decision-making
for software engineers during software development?
To answer these research questions, we either conduct em-
pirical studies or develop approaches as proof-of-concept im-
plementations. Approaches are evaluated through case stud-
ies and controlled experiments of these implementations.
2.1 Software Development for the Cloud
This research started out by conducting a mixed-method
empirical study (semi-structured interviews and survey) on
how software developers build applications for the cloud [2].
The results of this study enabled us to understand how soft-
ware developers make decisions when confronted with run-
time problems. Runtime information delivered in the form
of centralized and external dashboards are not actionable
for software developers [2]. They are struggling to incorpo-
rate this information in their daily workow. When solving
problems that have occurred at runtime, they rather rely on
beliefs and intuition than utilize metrics [2][3].
The problem statement briey characterizes the short-
comings in existing cloud analytics approaches. Data col-
lected on application services in production ( operational data )
is usually sent to monitoring and management services, and
surfaced in a way that helps to make operating decisions .
In this study, we have observed that operational data is
not made available in a manner that supports making soft-
ware development decisions . To counter this scenario and
answer the remaining research questions, I propose an ap-
proach called Developer Targeted Analytics , where runtime
information is integrated in the software developer's daily
workow to support software development decisions.
2.2 Developer Targeted Analytics
A central theme in a software developer's daily workow
is reading and writing program code [4]. The aim of Devel-
oper Targeted Analytics is to support developers in making
data-driven decisions in the process of writing code. We
achieve this by tightly integrating the collection and analy-
sis of runtime information observed from production deploy-
ments with the environment developers utilize work on new
versions of the applications (IDEs).
While the approach is not limited to specic types of run-
time information, the use cases presented in this thesis focus
on supporting decisions to improve software performance.2.2.1 Feedback Mapping & Prediction
In a process called feedback mapping , the static view of
code artifacts, depicted as a graph structure, is combined
with the dynamic view of runtime information [5]. More
specically, we map a series of operational data points to
nodes of existing artifacts in the source code. The result-
ing code view allows developers to examine their code arti-
facts annotated with data from real production traces (e.g.,
method calls with execution times or collections with size
distribution).
Given a feedback annotated graph, we can derive the im-
pact of code changes during software development. A code
change introduces a dierence, , between the originally an-
notated graph and the new graph. Feedback for unknown
new nodes in is derived using the existing feedback of its
child nodes as parameters for a statistical inference model.
In a last step, all derived feedback from changes are prop-
agated in the nodes of the graph following its dependency
edges. This kind of prediction allows us to warn develop-
ers about possible issues of their code changes prior to run-
ning the application in production. Figure 1 illustrates a
proof-of-concept implementation of this approach (tool Per-
formanceHat ) within the Eclipse IDE.
Figure 1: Eclipse-plugin PerformanceHat as an im-
plementation of Developer Targeted Analytics
2.2.2 Uncertainty
A major concern when providing predictions to aid decision-
making is uncertainty [6]. In devising this approach, we have
think about dierent sources of variation. Specically, we
have to consider both model uncertainty and system uncer-
tainty. We investigated part of system uncertainty in-depth
in an empirical study on performance variability in cloud
instances [7]. The results of this study need to be factored
into the prediction model to properly address uncertainty in
software performance prediction for the cloud.
3. VALIDATION STRATEGY
To validate the concept of Developer Targeted Analytics ,
we built a proof-of-concept implementation called Perfor-
manceHat3.
We already validated part of our approach through case
studies with industrial partners [5]. We further plan to val-
idate the end-to-end approach through a qualitative user
study with professional software developers to determine
whether it improves decision-making capabilities in tasks
concerning software performance.
Further, we need to evaluate how the mapping and propa-
gation of feedback approach scales to larger code bases. We
3https://github.com/sealuzh/PerformanceHat
893plan to conduct a study with the code base of an industrial
partner.
4. EXPECTED CONTRIBUTION
The goal of this research is to enable data-driven decision
making based on runtime information during software devel-
opment. Working on approaches and solutions by exploring
the research questions, I expect to make the following con-
tributions:
1. A better understanding of how cloud software is be-
ing built and identication of shortcomings in cloud
software development. Given the insights derived in
this research, new research directions can be properly
motivated and guided.
2. Initial evidence through an empirical study with soft-
ware developers that they struggle with incorporating
runtime information in their decision making process.
3. A conceptual framework of attaching runtime infor-
mation to source code artifacts. Further, a reusable
framework that incorporates runtime information into
the daily workow of software developers to support
decision making.
4. A model and understanding of system uncertainty in
cloud infrastructures.
5. RELATED WORK
Work related to this research can be broadly categorized
in the following categories: (1) software analytics, (2) visu-
alization of runtime behavior, (3) change impact analysis for
performance, and (4) decision-making & uncertainty.
Software Analytics.
Researchers have extensively investigated methods and
approaches to mine software repositories for a variety of
reasons and stakeholders under the term \software analyt-
ics" [8]. Often, these analytics approaches provide predic-
tion models to support software managers to make decisions
within their teams [8, 9, 10]. One of the focus points of
software analytics is bug and defect prediction. However,
a study by Lewis et al. [11] found that after deploying bug
prediction techniques, there was no identiable change in
developer behavior. Zhang et al. [12] also investigated the
impact of software analytics approaches from the research
community in practice.
In contrast to our work, most of the work in software an-
alytics mines static information (e.g., source code reposito-
ries, issue trackers, mailing lists) to build prediction models.
We want to focus on analytics based on runtime information,
specically software performance in cloud systems.
Visualization of Runtime Behavior.
Work both from software engineering and systems research
has investigated dierent ways to understand runtime be-
havior through visualization. Sandoval et al. [13] present an
approach they call performance evolution blueprint to un-
derstand the impact of software evolution on performance.
Meyer et al. [14] visualize the process runs of stored pro-
cedures in database systems. Senseo [15] is an approach
embedded in the IDE that augments the static code viewperspective with dynamic metrics of objects in Java. Beze-
mer et al. [16] investigated dierential ame graphs to un-
derstand software performance regressions. Cornelissen et
al. [17] showed that trace visualization in the IDE can sig-
nicantly improve program comprehension. ExplorViz [18]
provides live trace visualization in large software architec-
tures. Similarly to our work in Developer Targeted Ana-
lytics , Beck et al. [19] provide augmented code views with
information retrieved from prolers. Our work diers rst
in the use of data retrieved through instrumentation in pro-
duction cloud systems. Further, our approach goes beyond
displaying information on existing traces by providing live
prediction on performance of newly written code.
Change Impact Analysis & Performance Prediction.
Change impact analysis supports the comprehension, eval-
uation, and implementation of changes in software [20]. Most
of the work that is related to change impact analysis and per-
formance prediction operates on an architectural level [21,
22] and is not supposed to be \triggered" during software de-
velopment. Recent work by Luo et al. [23] uses a genetic al-
gorithm to investigate a large input search space that might
lead to performance regressions. In previous work, we ap-
plied changepoint analysis to detect distribution shifts in
performance data [24].
Our approach for change impact analysis is applied live,
during software development, and leverages an analytical
model consisting of the immediate software change and the
existing runtime information to provide early feedback to
software developers.
Decision-Making & Uncertainty.
Every decision is based on information that comes with its
own set of assumptions and uncertainties. Several works in
decision theory, but also computing have argued that con-
sidering uncertainty of systems and models is a basis for
decision making [25, 26, 27]. In areas such as performance
modeling and engineering, uncertainty and error modeling
has been been examined [28, 29]. Firoz et al. [29] show
how including uncertainty (as standard deviations) in per-
formance models improves insight from data. Researchers
have also considered interval parameters to depict model un-
certainty [30, 31]. Other work considers dierent dimensions
of model uncertainty [6].
6. PROGRESS AND OUTLOOK
The study for the rst research question has been al-
ready conducted and our results have been published and
presented at ESEC/FSE'15 [2].
Part of second research question has also been investi-
gated: The conceptual framework and model for Developer
Targeted Analytics have been published and presented at
SPLASH Onward!'15 [5]. Cloud system uncertainty and
variability have also been investigated and published in Trans-
actions on Internet Technology (TOIT'16) [7].
We will also improve our performance prediction model to
incorporate better notions of uncertainty. Further, we want
to conduct a controlled user study to evaluate our approach.
This work is supposed to be submitted to a systems confer-
ence in 2017.
8947. ACKNOWLEDGMENT
This work is advised by Prof. Harald Gall and Dr. Philipp
Leitner from the University of Zurich. The research lead-
ing to these results has received funding from the European
Community's Seventh Framework Programme (FP7/2007-
2013) under grant agreement no. 610802 (CloudWave).
8. REFERENCES
[1] G. Schermann, J. Cito, and P. Leitner, \All the services large
and micro: Revisiting industrial practice in services comput-
ing," in International Conference on Service-Oriented Com-
puting . Springer, 2015, pp. 36{47.
[2] J. Cito, P. Leitner, T. Fritz, and H. C. Gall, \The making
of cloud applications: An empirical study on software de-
velopment for the cloud," in Proceedings of the 2015 10th
Joint Meeting on Foundations of Software Engineering , ser.
ESEC/FSE 2015. New York, NY, USA: ACM, 2015, pp.
393{403.
[3] P. Devanbu, T. Zimmermann, and C. Bird, \Belief & evi-
dence in empirical software engineering," in Proceedings of
the 38th International Conference on Software Engineering .
ACM, 2016, pp. 108{119.
[4] L. Orsini, \How Software Developers Re-
ally Spend Their Time," http://www.infoworld.
com/article/2613762/application-development/
software-engineers-spend-lots-of-time-not-building-software.
html, 2015, [Online; accessed 20-November-2015].
[5] J. Cito, P. Leitner, H. C. Gall, A. Dadashi, A. Keller, and
A. Roth, \Runtime Metric meets Developer - Building better
Cloud Applications using Feedback," in Proceedings of the
2015 ACM International Symposium on New Ideas, New
Paradigms, and Reections on Programming & Software
(Onward! 2015) , 2015.
[6] R. Lipshitz and O. Strauss,\Coping with uncertainty: A nat-
uralistic decision-making analysis," Organizational behavior
and human decision processes , vol. 69, no. 2, pp. 149{163,
1997.
[7] P. Leitner and J. Cito, \Patterns in the Chaos - a
Study of Performance Variation and Predictability in Pub-
lic IaaS Clouds," ACM Transactions on Internet Technology
(TOIT) , 2016, to appear.
[8] R. P. Buse and T. Zimmermann, \Analytics for software de-
velopment," in Proceedings of the FSE/SDP workshop on
Future of software engineering research . ACM, 2010, pp.
77{80.
[9] A. T. Misirli, B. Caglayan, A. Bener, and B. Turhan, \A
retrospective study of software analytics projects: In-depth
interviews with practitioners," Software, IEEE , vol. 30, no. 5,
pp. 54{61, 2013.
[10] L. L. Minku, E. Mendes, and B. Turhan, \Data mining for
software engineering and humans in the loop," Progress in
Articial Intelligence , pp. 1{8, 2016.
[11] C. Lewis, Z. Lin, C. Sadowski, X. Zhu, R. Ou, and E. J.
Whitehead Jr, \Does bug prediction support human devel-
opers? ndings from a google case study," in Proceedings of
the 2013 International Conference on Software Engineering .
IEEE Press, 2013, pp. 372{381.
[12] D. Zhang, S. Han, Y. Dang, J.-G. Lou, H. Zhang, and T. Xie,
\Software analytics in practice," Software, IEEE , vol. 30,
no. 5, pp. 30{37, 2013.
[13] J. P. Sandoval Alcocer, A. Bergel, S. Ducasse, and
M. Denker, \Performance evolution blueprint: Understand-
ing the impact of software evolution on performance," in
Software Visualization (VISSOFT), 2013 First IEEE Work-
ing Conference on . IEEE, 2013, pp. 1{9.
[14] M. Meyer, F. Beck, and S. Lohmann, \Visual monitoring of
process runs: An application study for stored procedures,"
in2016 IEEE Pacic Visualization Symposium (PacicVis) .
IEEE, 2016, pp. 160{167.
[15] D. R othlisberger, M. H arry, A. Villaz on, D. Ansaloni,
W. Binder, O. Nierstrasz, and P. Moret, \Augmenting staticsource views in ides with dynamic metrics," in Software
Maintenance, 2009. ICSM 2009. IEEE International Con-
ference on . IEEE, 2009, pp. 253{262.
[16] C.-P. Bezemer, J. Pouwelse, and B. Gregg, \Understand-
ing software performance regressions using dierential ame
graphs," in Software Analysis, Evolution and Reengineering
(SANER), 2015 IEEE 22nd International Conference on .
IEEE, 2015, pp. 535{539.
[17] B. Cornelissen, A. Zaidman, and A. Van Deursen, \A con-
trolled experiment for program comprehension through trace
visualization," Software Engineering, IEEE Transactions
on, vol. 37, no. 3, pp. 341{355, 2011.
[18] F. Fittkau, S. Roth, and W. Hasselbring, \Explorviz: Vi-
sual runtime behavior analysis of enterprise application land-
scapes." AIS, 2015.
[19] F. Beck, O. Moseler, S. Diehl, and G. D. Rey, \In situ
understanding of performance bottlenecks through visually
augmented code," in Program Comprehension (ICPC), 2013
IEEE 21st International Conference on . IEEE, 2013, pp.
63{72.
[20] S. Lehnert, \A review of software change impact analysis."
[21] S. Becker, H. Koziolek, and R. Reussner, \The palladio
component model for model-driven performance prediction,"
Journal of Systems and Software , vol. 82, no. 1, pp. 3{22,
2009.
[22] C. Heger and R. Heinrich, \Deriving work plans for solving
performance and scalability problems," in Computer Perfor-
mance Engineering . Springer, 2014, pp. 104{118.
[23] Q. Luo, D. Poshyvanyk, and M. Grechanik, \Mining
performance regression inducing code changes in evolving
software," in Proceedings of the 13th International Confer-
ence on Mining Software Repositories , ser. MSR '16. New
York, NY, USA: ACM, 2016, pp. 25{36. [Online]. Available:
http://doi.acm.org/10.1145/2901739.2901765
[24] J. Cito, D. Suljoti, P. Leitner, and S. Dustdar, \Identi-
fying Root-Causes of Web Performance Degradation using
Changepoint Analysis," in Proceedings of the 14th Interna-
tional Conference on Web Engineering (ICWE) . Springer
Berlin Heidelberg, 2014.
[25] G. Wright and P. Goodwin, \Decision making and planning
under low levels of predictability: Enhancing the scenario
method," International Journal of Forecasting , vol. 25, no. 4,
pp. 813{825, 2009.
[26] M. Famelis, R. Salay, and M. Chechik, \Partial models: To-
wards modeling and reasoning with uncertainty," in Soft-
ware Engineering (ICSE), 2012 34th International Confer-
ence on . IEEE, 2012, pp. 573{583.
[27] E. Letier, D. Stefan, and E. T. Barr, \Uncertainty, risk,
and information value in software requirements and archi-
tecture," in Proceedings of the 36th International Conference
on Software Engineering . ACM, 2014, pp. 883{894.
[28] D. Perez-Palacin and R. Mirandola, \Dealing with uncer-
tainties in the performance modelling of software systems,"
inProceedings of the 10th International ACM Sigsoft
Conference on Quality of Software Architectures , ser. QoSA
'14. New York, NY, USA: ACM, 2014, pp. 33{42. [Online].
Available: http://doi.acm.org/10.1145/2602576.2602582
[29] J. S. Firoz, M. Barnas, M. Zalewski, and A. Lumsdaine,
\The value of variance," in Proceedings of the 7th
ACM/SPEC on International Conference on Performance
Engineering , ser. ICPE '16. New York, NY, USA:
ACM, 2016, pp. 287{295. [Online]. Available: http:
//doi.acm.org/10.1145/2851553.2851573
[30] J. L uthi and C. M. Llad o, Interval parameters for capturing
uncertainties in an EJB performance model . ACM, 2001,
vol. 29, no. 1.
[31] A. Omerovic and K. Stlen, \A practical approach to un-
certainty handling and estimate acquisition in model-based
prediction of system quality," International Journal on Ad-
vances in Systems and Measurements Volume 4, Number 1
& 2, 2011 , 2011.
895