Automatic Generation of Pull Request Descriptions
Zhongxin Liu‚àó‚Ä†/bardbl, Xin Xia‚Ä°/check, Christoph Treude¬ß, David Lo¬∂, Shanping Li‚àó
‚àóCollege of Computer Science and Technology, Zhejiang University, Hangzhou, China
‚Ä†Ningbo Research Institute, Zhejiang University, Ningbo, China
/bardblPengCheng Laboratory, Shenzhen, China
‚Ä°Faculty of Information Technology, Monash University, Melbourne, Australia
¬ßSchool of Computer Science, University of Adelaide, Adelaide, Australia
¬∂School of Information Systems, Singapore Management University, Singapore, Singapore
liuzx@zju.edu.cn, xin.xia@monash.edu, christoph.treude@adelaide.edu.au, davidlo@smu.edu.sg, shan@zju.edu.cn
Abstract ‚ÄîEnabled by the pull-based development model, de-
velopers can easily contribute to a project through pull requests(PRs). When creating a PR, developers can add a free-form
description to describe what changes are made in this PR and/or
why . Such a description is helpful for reviewers and other
developers to gain a quick understanding of the PR withouttouching the details and may reduce the possibility of thePR being ignored or rejected. However, developers sometimesneglect to write descriptions for PRs. For example, in our
collected dataset with over 333K PRs, more than 34% of the
PR descriptions are empty. To alleviate this problem, we proposean approach to automatically generate PR descriptions based onthe commit messages and the added source code comments in thePRs. We regard this problem as a text summarization problemand solve it using a novel sequence-to-sequence model. To copewith out-of-vocabulary words in software artifacts and bridge
the gap between the training loss function of the sequence-to-
sequence model and the evaluation metric ROUGE, which hasbeen shown to correspond to human evaluation, we integratethe pointer generator and directly optimize for ROUGE usingreinforcement learning and a special loss function. We build adataset with over 41K PRs and evaluate our approach on this
dataset through ROUGE and a human evaluation. Our evaluation
results show that our approach outperforms two baselines bysigniÔ¨Åcant margins.
I. I NTRODUCTION
The pull-based development model [1] is popular on modern
collaborative coding platforms, e.g., GitHub [2]‚Äì[4]. It easesdevelopers‚Äô contributions to a project. In this model, a devel-
oper does not need to have access to the central repository
to contribute to a project. She only needs to fork the centralrepository (i.e., create a personal clone), make changes (e.g.,Ô¨Åx a bug or implement a feature) independently in the personalclone and submit the changes to the central repository througha pull request (from hereon, PR). Usually, the PR will be tested
by continuous integration services and reviewed by core team
members (or reviewers) before being merged into the centralrepository [5].
A PR consists of one or more interrelated commits. To
create a PR on GitHub, a developer needs to provide a titleand can add a free-form text (i.e., a PR description) to describefurther what changes are made and/or why they are needed.
PR descriptions can help reviewers gain a quick and adequate
/checkCorresponding author.understanding of PRs without digging into details and mayreduce the possibility of PRs being ignored or rejected [3], [6].In addition, PR descriptions can help software maintenance
and program comprehension tasks [6], [7].
However, PR descriptions are sometimes neglected by de-
velopers. For example, in our dataset which contains 333,001PRs collected from 1K engineered Java projects on GitHub,over 34% of the PR descriptions are empty. To alleviate thisproblem, we propose an approach to automatically generate PR
descriptions from the commits submitted with the correspond-ing PRs. Our approach can be used to generate PR descriptionsto replace existing empty ones and can also assist developers
in writing PR descriptions when creating PRs.
Some tools have been proposed to automatically generate
descriptions for software changes, e.g., generate commit mes-
sages [8]‚Äì[11] and release notes [12], [13]. Commits, PRsand releases can be regarded as software changes occurringat different granularity. Distinct from commit messages whichonly describe one commit, PR descriptions often need tosummarize multiple related commits. A release is a collection
of plenty of commits and/or PRs. Release notes are prepared
for both developers and end users (people who use the librariesor apps), while PR descriptions‚Äô readers are usually solelydevelopers. Hence they have different focuses and informationstructure. Besides, the existing technique for release note
generation [12], [13] does not explicitly summarize multipleinterrelated commits. It recovers links between commits and
bug reports, and uses bug report titles as the summaries ofcorresponding commits. In this work, we focus on explicitlysummarizing the commits in a PR to generate its description,and we treat traceability as a separate problem. Moreover,
when developers document a change of some granularity (e.g.,
a PR), the documents of the smaller changes it contains (e.g.,the commits in the PR) are usually available. Therefore, thetechniques of automatically documenting changes at differentgranularity are complementary rather than competing. To the
best of our knowledge, there is no prior work focusing on
generating descriptions for PRs.
It is challenging to automatically generate a description
for a single commit, not to mention a PR with multipleinterrelated commits. Fortunately, when a developer writes aPR description, the commit messages of the commits in this
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXB SF&OHJOFFSJOH	"4&
¬•*&&&
%0*"4&
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. PR are usually available. These valuable messages together
with the patch of each commit shed light into the generationof PR descriptions. As the Ô¨Årst step of this task, this workaims to generate PR descriptions from the commit messagesand the added source code comments in the PRs.
Given a PR, we regard the combination of its commit
messages and the source code comments added in it as an
‚Äúarticle‚Äù and its description as the summary of this ‚Äúar-ticle‚Äù. The generation of PR descriptions is then regardedas a text summarization problem by us. We have appliedtwo commonly-used extractive text summarization methods
to solve this problem but Ô¨Ånd their effectiveness is limited
(described in Section V-D). In this work, we propose a moreeffective approach for PR description generation. SpeciÔ¨Åcally.our approach builds upon the attentional encoder-decodermodel [14], which is an effective sequence-to-sequence modelfor text summarization. It Ô¨Årst learns how to write PR descrip-
tions from existing PRs, and then can generate descriptions for
new PRs.
There are two challenges which make a basic attentional
encoder-decoder model not effective enough for PR descrip-tion generation:
1) Out-of-vocabulary (OOV) words. Due to the developer-
named identiÔ¨Åers (e.g., variable names), OOV words arewidespread in software artifacts. However, the attentionalencoder-decoder model can only produce words in a Ô¨Åxedvocabulary, hence cannot deal with OOV words.
2) The gap between the training loss function and the
evaluation metric. Since different sentences may convey sim-
ilar meanings, researchers usually leverage a Ô¨Çexible discretemetric named ROUGE [15] to evaluate text summarizationsystems. ROUGE allows for different word orders between agenerated text and the ground truth, and correlates highly with
human evaluation [15]. However, the training objective of the
attentional encoder-decoder model is minimizing a maximum-likelihood loss, which is strict and will penalize all literaldifferences between a generated text and the ground truth. Dueto this gap, the model minimizing the maximum-likelihoodloss may not be the one with the best generation performance.
We observe that the OOV words in a PR description can
often be found in the corresponding ‚Äúarticle‚Äù. Therefore weintegrate the pointer generator [16] in our approach to over-come the Ô¨Årst challenge. With this component, our approachcan generate a word from either the Ô¨Åxed vocabulary or the
input. To deal with the second challenge, we need to Ô¨Ånd away to optimize for ROUGE directly when training. However,
we cannot simply use ROUGE as the training objectivesince ROUGE scores are non-differentiable, i.e., the parametergradients of our model cannot be calculated from them. Wesolve this problem by using a reinforcement learning (RL)
technique named self-critical sequence training (SCST) [17].
Based on SCST, we adopt a special loss function named RLloss [18] in our approach. This loss is both related to ROUGEscores and differentiable, with which we can train a model andguide it to produce results that are more likely to be good forhuman evaluation.As this is the Ô¨Årst work on this topic, we use two extractive
methods, i.e., LeadCM and LexRank [19], as baselines. Givena PR, LeadCM extracts its Ô¨Årst few commit messages as thegenerated description, and LexRank selects and outputs salientsentences in the PR‚Äôs ‚Äúarticle‚Äù. To evaluate our proposedapproach, we collected over 333K PRs from 1K engineeredJava projects with the most merged PRs on GitHub, buildinga dataset with over 41K PRs after preprocessing. We evaluateour approach on the dataset using ROUGE. The evaluationresults show that our approach outperforms the baselines interms of ROUGE-1, ROUGE-2 and ROUGE-L by 11.6%,25.4% and 12.2%. We also conduct a human evaluation toassess the quality of the generated PR descriptions, whichshows that our approach performs signiÔ¨Åcantly better than thebaselines and can generate more high-quality PR descriptions.
In summary, our contributions are three-fold:
‚Ä¢We propose a novel approach to generate descriptions for
PRs from their commit messages and the code comments
that are added. Our approach can cope with OOV words
with the pointer generator and directly optimize forROUGE, which has been shown to correspond to humanevaluation, with the RL loss.
‚Ä¢We build a dataset with over 41K pull requests fromGitHub for the PR description generation task.
‚Ä¢We evaluate our approach on the dataset using theROUGE metric and a human evaluation. The evaluation
results show that our approach outperforms two baselines
by signiÔ¨Åcant margins.
The remainder of this paper is organized as follows: Sec-
tion II describes the motivation, the usage scenarios and some
background knowledge of our approach. Section III elaborates
our approach, including the pointer generator and the RLloss. We describe our dataset in Section IV and presentthe procedures and results of our evaluation in Section V.Section VI discusses situations where our approach performs
badly and threats to validity. After a brief review of related
work in Section VII, we conclude this paper and point outpotential future directions in Section VIII.
II. M
OTIV ATION AND PRELIMINARY
In this section, we present the motivation and usage sce-
narios of our approach, formulate the problem of PR descrip-tion generation, and introduce the attentional encoder-decoder
model.
A. Motivating Example
Table I shows a motivating example of our approach,
which is a PR in the Pitest project
1. We can see that the
description describes the changes made, i.e., ‚Äúadded an option‚Äù
and ‚Äúactivated from maven plugin‚Äù, and the motivation, i.e.,
‚Äúignore failing tests from coverage‚Äù, of this PR. This PRcontains two commits. One source code comment is addedin Commit 1, and no code comment is added in Commit 2.We can know the changes from the commit messages and
1https://github.com/hcoles/pitest/pull/528

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. TABLE I
APULL REQUEST IN THE PITEST PROJECT
Description:Added an option to ignore failing tests from coverage, activated from
maven plugin
Commit 1:
Commit Message: Added skipFailingTests option from maven plugin
Added Code Comments: When set will ignore failing tests when comput-
ing coverage. Otherwise, the run will fail. If parseSureÔ¨ÅreConÔ¨Åg is true,
will be overridden from sureÔ¨Åre conÔ¨Åguration property testFailureIgnoreCommit 2:Commit Message: SimpliÔ¨Åed sureÔ¨Åre testFailureIgnore value retrieval
Added Code Comments: N/A
the motivation from the added code comments in Commit 1.
This example indicates that we may be able to generate thedescription of a PR by summarizing its commit messages andthe source code comments added in it.
B. Usage Scenario
Our approach aims to automatically generate descriptions
for PRs based on their commit messages and the code com-ments that are added. Its usage scenarios are as follows:
First of all, our approach can be used to generate PR
descriptions to replace existing empty ones. The generateddescriptions may help reviewers and developers quickly cap-ture PRs‚Äô key ideas without reading the detailed commits.Such key ideas can be very helpful when reviewers anddevelopers are making quick decisions, e.g., assigning a tagor estimating whether two PRs are related. The generateddescriptions may also be useful for software maintenance
and program comprehension tasks. For example, tools for PR
reviewer recommendation can use the generated description asone of the features.
Our approach can also assist developers in writing PR
descriptions. If it takes several days to Ô¨Ånish a PR, thedeveloper may forget some important information in this PRwhen writing the description. She may either ignore suchinformation, which may affect the acceptance of the PR,
or spend some time to check the detailed commits, which
may decrease her productivity. The description generated byour approach can remind the developer of the importantinformation in the PR and assist her in writing a high-qualityPR description.
C. Problem F ormulation
Inspired by the motivating example, we regard the gener-
ation of a PR description as a text summarization task withthe combination of the commit messages and the added code
comments in the PR as the ‚Äúarticle‚Äù and the PR description
as the ‚Äúsummary‚Äù. SpeciÔ¨Åcally, in this work, we treat thetext summarization task as a sequence-to-sequence learningproblem, where the source sequence is the ‚Äúarticle‚Äù and thetarget sequence is the ‚Äúsummary‚Äù. Therefore, the problemis formulated as follows: given a source sequence w=
(w
1,w2,...,w|w|)and a target sequence y=(y1,y2,...,y|y|),
Ô¨Ånd a function f so that f(w)=y.|¬∑|denotes the length of
a sequence./g5/g13/g7/g18/g10/g9 /g1/g15/g8/g10/g9/g9/g14/g16/g12/g2/g7/g21/g10/g18/g4/g3/g3 /g4/g3/g3 /g4/g3/g3 /g4/g3/g3
/g12/g15 /g12/g16 /g12/g17 /g12/g18/g2/g14/g16/g10/g7/g18 /g22 /g6/g7/g16/g13 /g22 /g5/g17/g11/g19/g15/g7/g20
/g9/g15 /g9/g16 /g9/g17 /g9/g18/g8/g15/g8/g16/g8/g17 /g8/g18/g1/g5
/g4/g3/g3 /g4/g3/g3/g2/g14/g16/g10/g7/g18 /g22 /g5/g17/g11/g19/g15/g7/g20
/g10/g16
/g10/g14/g3/g16/g13/g30/g16
/g11/g15 /g11/g16 /g11/g17 /g11/g18 /g13/g30/g14 /g13/g30/g15/g7/g23/g22/g24/g16/g6 /g4/g29/g7 /g23/g22/g24/g16/g6
/g13/g30/g15
/g2/g27/g25/g21/g19/g20/g2/g21/g25/g26/g28
/g12/g30/g16/g1/g6/g6/g2/g4/g6/g3/g5/g4
Fig. 1. Attentional encoder-decoder model with pointer generator (Attn+PG)
D. Attentional Encoder-Decoder Model
Our approach builds upon the attentional encoder-decoder
model [14] (from hereon, Attn), which is an effective model
for sequence-to-sequence learning problems. Attn‚Äôs frameworkis depicted in black in Figure 1. It uses two distinct recurrentneural networks (RNNs) as encoder and decoder, respectively.The input of the encoder and the decoder is Ô¨Årst mappedto word embeddings by a shared embedding layer. Given a
source sequence w, at time step i, the encoder calculates a
hidden state h
ibased on the word embedding xiofwiand
the previous hidden state hi‚àí1using its RNN. The last hidden
state h|w|is regarded as the intermediate representation of the
source sequence and input to the decoder as the initial hidden
state.
At decoding step j, the decoder computes a hidden state sj
from ÀÜxj, which is the embedding of the previous reference
token when training or the previously generated token whentesting. Attn leverages the attention mechanism [14], so thedecoder also calculates a context vector c
j, as follows:
ej
i=vetanh(Whhi+Wssj+be)
aj=softmax (ej) (1)
cj=|x|/summationdisplay
iaj
ihi
where Wh,Wsandbeare learnable parameters and ej
iis the
score ofxiat decoding step j.ajis the attention distribution,
which informs the decoder of the importance of each encoding
step. cjis computed as the weighted sum of all encoder hidden
states, which can be regarded as the representation of thesource sequence at decoding step j.
Then, c
jis concatenated with decoder hidden state sjto
produce the vocabulary distribution Pvocab :
Pvocab=softmax (V/prime(V[sj,cj]+b)+b/prime)

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. where V/prime,V,bandb/primeare learnable parameters. Pvocab is
used to decide which token in the vocabulary should be outputat the current decoding step. It also provides the conditionalprobability of generating the j
threference token yj, which is:
p(yj|ÀÜy0,...,ÀÜyj‚àí1,w)=Pvocab(yj) (2)
where ÀÜyis the input of the decoder.
At each training iteration, the optimization objective of Attn
is to minimize the negative log-likelihood of the referencesequence, as follows:
loss
ml=‚àí1
|y||y|/summationdisplay
j=1logp(yj|ÀÜy0,...,ÀÜyj‚àí1,w) (3)
III. A PPROACH
This section elaborates our approach, including the pointer
generator and the RL loss.
A. Pointer Generator
As described in Section II-D, Attn produces tokens by
selecting from a Ô¨Åxed vocabulary. However, out-of-vocabulary
(OOV) words are ubiquitous in software artifacts due todeveloper-named identiÔ¨Åers, such as variable names and Ô¨Ålenames. Attn alone cannot cope with such OOV words, andhence its performance is limited. We observe that the OOV
words in PR descriptions usually appear in the corresponding
source sequences. So we integrate the pointer generator [16] inour approach to solve this problem. With this component, ourapproach can either select a token from the Ô¨Åxed vocabularyor copy one from the source sequence at each decoding step.
The structure of Attn with the pointer generator (Attn+PG)
is presented in Figure 1, where the pointer generator is
highlighted in green. The switch between selection and copy
is softly controlled by the generation probability , which is
calculated from the word embedding ÀÜx
jof the decoder input
ÀÜyj‚àí1, the decoder state sjand the context vector cjat time
stepj:
pj
gen=œÉ(wT
ccj+wT
ssj+wT
xÀÜxj+bgen)
where wc,ws,wxandbgenare learnable parameters, and œÉ
is the sigmoid function. pj
genmeasures the probability that
thejthoutput of the decoder is generated from the Ô¨Åxed
vocabulary, and the probability of copy is hence 1‚àípj
gen. The
conditional probability of producing the jthreference token
(Equation 2) is then modiÔ¨Åed as:
p(yj|ÀÜy0,...,ÀÜyj‚àí1,w)=pj
genPvocab(yj)+(1‚àípj
gen)Pcopy(yj)
(4)
wherePcopy(yj)is the probability of copying yjfrom the
source sequence w, and is computed from the attention
distribution aj(Equation 1), as follows:
Pcopy(yj)=/summationdisplay
i:wi=yjaj
i
We can see that when yjis an OOV word, Pvocab(yj)is
zero, but if yjappears in the source sequence, our approachcan still generate it through Pcopy(yj). In this way, our
approach alleviates the problem of OOV words and still holdsthe capability of producing novel words from the vocabulary.In addition, the training loss is still calculated using Equa-tion 3, but the conditional probability p(y
j|ÀÜy0,...,ÀÜyj‚àí1,w)
is computed by Equation 4 now.
B. RL Loss
As described in Section II-D, Attn uses the negative log like-
lihood loss to guide the training process. This loss function isstrict and will penalize any literal difference between generatedsequences and the ground truth. For example, if the groundtruth is ‚Äúthe cat sat on the mat‚Äù but the decoder produces‚Äúon the mat sat the cat‚Äù, the loss will be high since the two
sentences only literally match at ‚Äúthe‚Äù. Therefore, researchers
do not use this loss function to measure the performance oftext summarization systems. Instead, they usually use a Ô¨Çexiblediscrete evaluation metric named ROUGE (see Section V-A),which can tolerate generated sentences with different word
orders from the ground truth and has been shown to correlate
highly with human evaluation [15]. The gap between thetraining loss function and ROUGE may result in the modelwith the least loss not being the one producing the best PRdescriptions.
We can bridge this gap by directly optimizing for
ROUGE when training. However, ROUGE scores are non-
differentiable, which means the parameter gradients of ourmodel cannot be calculated only from ROUGE scores. Hencewe cannot directly use ROUGE as the loss function. Recently,it has been shown that reinforcement learning (RL) techniques
can be incorporated to enable direct optimization of discrete
evaluation metrics [17], [18], [20]. Our approach also lever-ages an RL technique named self-critical sequence training(SCST) [17] and adopts a special loss function named RLloss [18] to solve this problem.
We can cast the generation of PR descriptions using RL
terminology. The decoder is the ‚Äúagent‚Äù, which interacts withthe ‚Äúenvironment‚Äù (the encoder‚Äôs output and the decoder‚Äôsinput). At each decoding step, the ‚Äúaction‚Äù of the ‚Äúagent‚Äù isto predict an output token according to a ‚Äúpolicy‚Äù œÄ
Œ∏with
parameters Œ∏. Actually, the neural network of the decoder
deÔ¨ÅnesœÄŒ∏andŒ∏is its parameters. Once Ô¨Ånished generating
a sequence ÀÜy, the ‚Äúagent‚Äù will observe a ‚Äúreward‚Äù, deÔ¨Åned as
follows:
r(ÀÜy)=g(ÀÜy,y)
where yis the ground truth of ÀÜyandgis a function related
to ROUGE. In this work, we deÔ¨Åned gas the ROUGE-L F1
score. The training objective of the RL problem is minimizing
the negative expected reward:
L(Œ∏)=‚àíEys‚àºœÄŒ∏[r(ys)]
According to the SCST algorithm [17], the expected gradi-
ent ofL(Œ∏)can be computed as follows:
‚àáŒ∏L(Œ∏)=‚àíEys‚àºœÄŒ∏[(r(ys)‚àír(yb))‚àáŒ∏logœÄŒ∏(ys)]

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. /g8/g6/g6
/g2/g12/g10/g18/g11/g12/g20
/g1/g22/g22/g12/g17/g22 /g14/g18/g17 /g7/g18 /g14/g17/g22/g12/g20
/g8/g6/g6
/g3/g17/g10/g18/g11/g12/g20/g21/g9/g16/g19/g15/g12
/g9/g20/g13/g16/g9 /g25/g6/g10
/g6/g7
/g8/g12/g24/g9/g20/g11
/g4/g23/g17/g10/g22/g14/g18/g17 /g6/g5/g18/g21/g21
/g4/g23/g17/g10/g22/g14/g18/g17/g4/g11/g6/g10/g12
/g4/g11/g6/g7/g12/g1/g2/g5/g5 /g9/g8/g3/g11/g6/g10/g12
Fig. 2. The RL loss
where ybis a baseline sequence also generated from œÄŒ∏.
We obtain ybthrough a greedy search. SpeciÔ¨Åcally, we
choose the token with the highest output probability (i.e.,p(y
b
j|yb
0,...,yb
j‚àí1,w)) at each decoding step to form yb.I n
practice, the expectation can be approximated by a single
Monte-Carlo sample ysfromœÄŒ∏, which means:
‚àáŒ∏L(Œ∏)=‚àí(r(ys)‚àír(yb))‚àáŒ∏logœÄŒ∏(ys)
=‚àí(r(ys)‚àír(yb))‚àáŒ∏|ys|/summationdisplay
j=1logp(ys
j|ys
0,...,ys
j‚àí1,w)(5)
We deÔ¨Åne the RL loss of our model following Paulus et
al. [18], as follows:
lossrl=‚àí(r(ys)‚àír(yb))|ys|/summationdisplay
j=1logp(ys
j|ys
0,...,ys
j‚àí1,w)(6)
r(ys)andr(yb)are non-differential and are regarded as
constant values when calculating gradients. From Equation 5and Equation 6, we can see that minimizing loss
rlis equal
to minimizing L(Œ∏).lossrlcan also be viewed as the lossml
(Equation 3) weighted by a normalized reward. If the normal-
ized reward, i.e., r(ys)‚àír(yb), is positive, i.e., the sequence
sampled from our model is better than the baseline sequence,minimizing loss
rlis equivalent to maximizing the likelihood
of the sampled sequence, and vice versa. The calculation
process of the RL loss is shown in Figure 2.
In practice, only using lossrlcan be detrimental to the
readability of the generated texts [18]. We hence combine
lossmlandlossrlto form a hybrid loss for training, as follows:
loss=Œ≥lossrl+(1‚àíŒ≥)lossml (7)
IV . D ATASET
A. Data Collection
To collect PR data from GitHub, we Ô¨Årst used the RepoRe-
apers framework [21] to select engineered software projects.We obtained all 95,804 Java repositories that had been clas-siÔ¨Åed as containing engineered software projects by RepoRe-apers‚Äôs Random Forest classiÔ¨Åcation and retrieved the number
of merged PRs for each repository. 22,700 of these repositories
contained at least one merged PR. We then sorted these 22,700repositories in descending order of their number of mergedPRs, downloading the data of merged PRs from the top 1,000projects through GitHub‚Äôs APIs. For each project, we collectedat most the Ô¨Årst 1,000 merged PRs returned by GitHub‚Äôs searchAPI. Since our approach takes commit messages and source
code comments that are added as input and PR descriptionsas output, given a PR, we retrieved its description and commitmessages, parsed the patches of its commits and extracted theadded comments in each patch. In total, we collected 333,001merged PRs from 1,000 engineered Java projects.
B. Data Preprocessing
We preprocessed the collected PRs according to the follow-
ing processes:
1) Preprocess text: To Ô¨Ålter out trivial and templated infor-
mation in PRs, we leveraged the same procedure to preprocessthe texts of PR descriptions, commit messages and sourcecode comments. Given a text, we Ô¨Årst removed the HTMLcomments and the paragraphs starting with a headline named‚Äúchecklist‚Äù from it through regular expressions, because the
text in HTML comments and checklist paragraphs usually
only describe the general procedure of Ô¨Ånishing a PR, suchas ‚Äúfunctionality works‚Äù and ‚Äúpasses all tests‚Äù.
Then we split the text into sentences using NLTK [22],
identifying and deleting the sentences with 1) url, 2) internalreference, e.g., ‚Äú#123‚Äù, 3) signature, e.g., ‚Äúsigned-off-by‚Äù, 4)emails, 5) ‚Äò@name‚Äô and 6) markdown headlines, e.g., ‚Äú##why‚Äù through regular expressions. We Ô¨Åltered sentences with1) and 2) since this work focuses on summarizing the changesin a PR and we regard recovering links between PRs and othersoftware artifacts as a separate problem. Besides, sentenceswith 3), 4), 5) and 6) usually do not describe the changes
made in a PR and may bring in many OOV words.
Next, we tokenized the text using NLTK. Previous work has
shown that NLTK outperforms other common NLP libraries interms of tokenizing software documentation [23]. The tokensthat only consist of 7 or more hexadecimal characters wereconsidered as SHA1 hash digests and replaced with ‚Äúsha‚Äù.
Similarly, version strings, e.g., ‚Äú1.2.3‚Äù, and numbers were
converted into ‚Äúversion‚Äù and 0, respectively.
Finally, tokens with non-ASCII characters were removed
and referred to as non-ASCII tokens , and texts with more than
50% non-ASCII tokens were marked as ‚Äúnon-ASCII‚Äù.
2) Construct target sequence: The target sequence of a PR
only consists of its description. To construct it, we simplypreprocessed the PR description using the general text pre-
processing procedure mentioned above. The PR descriptions
which were marked as ‚Äúnon-ASCII‚Äù when preprocessing,contain less than 5 tokens or only consist of punctuation markswere removed and referred to as trivial desc .
3) Construct source sequence: A PR‚Äôs source sequence is
constructed from the combination of its commit messagesand the added code comments in it. SpeciÔ¨Åcally, we Ô¨Årstlisted the commits in this PR in ascending order of their
creation time. Then, for each commit, we extracted its commit
message and the code comments that are added. The commitmessage was directly preprocessed using the general textpreprocessing procedure. As for the added comments, copy-right comments, license comments, function signatures in Javadocs (e.g., ‚Äú@param: param1‚Äù) and the comments with only

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. /g8/g9/g25 /g2/g21/g20/g25 /g26/g24/g27/g13/g26
/g11/g12/g24/g16/g15 /g26/g10/g15/g23/g4/g19/g22/g26/g29/g30/g3/g15/g25/g13
/g8/g9 /g5/g17/g18 /g26/g15/g24/g1/g14/g15/g23 /g27/g12/g26/g15
/g8/g9/g25/g11/g24/g17/g28/g17/g12/g18/g30/g3/g15/g25/g13
/g8/g9 /g5/g17/g18 /g26/g15/g24/g6/g21/g20/g16 /g30/g3/g15/g25/g13
/g8/g9 /g5/g17/g18 /g26/g15/g24/g2/g21/g20/g25 /g26/g24/g27/g13/g26
/g10/g21/g27/g24/g13/g15 /g10/g15/g23/g2/g21/g19/g19/g17 /g26/g7/g27/g19
/g5/g17/g18/g26/g15/g24/g6/g21/g20/g16 /g30/g10/g21/g27/g24/g13/g15
/g8/g9 /g5/g17/g18 /g26/g15/g24
Fig. 3. Procedure of Ô¨Åltering pull requests
TABLE II
STATISTICS OF OUR COLLECTED PULL REQUESTS
TypeEmpty-desc
PRTrivial-desc
PRLong-desc
PRPR with only 1
valid commitPR with >20
valid commitsLong-source
PRAdequate PR Total
Number 114,466 61,547 20,516 83,803 2,438 8,399 41,832 333,001
*Long-desc PR and Long-source PR refer to the PRs for which the target sequence and the source sequence do not meet the length constraints, respectively.
punctuation marks were Ô¨Åltered. The remaining comments
were concatenated as a comment paragraph, which was thenpreprocessed using the general text preprocessing procedure.If the preprocessed commit message or comment paragraphor both are not empty, we regard this commit as a valid
commit . Finally, we concatenated all the preprocessed commit
messages as the Ô¨Årst paragraph of the source sequence and
listed the comment paragraph of each commit as the followingparagraphs. The commit messages were sorted according tothe order of commits, i.e., ascending order of their creationtime, and were separated by a special token ‚Äú[cm-sep]‚Äù. The
comment paragraphs were also listed in the order of commits,
and all paragraphs were separated by ‚Äú[para-sep]‚Äù.
4) Filter PRs: With constructed source sequences and tar-
get sequences, PRs were Ô¨Åltered according to the procedureshown in Figure 3. The PRs with empty descriptions were Ô¨Årstremoved and referred to as empty-desc PRs . If a PR description
was a trivial desc after preprocessing, the corresponding PR
was also Ô¨Åltered and referred to as a trivial-desc PR .W e
deleted the PRs with less than 2 or more than 20 valid commits ,
because we can directly use the only commit message asthe description if a PR only contains one commit, and aPR with too many commits often aims to synchronize with
another repository instead of being a contribution from a
contributor. To reduce the training time of our approach, wealso constrained the maximal length of the source sequence tobe 400 and that of the target sequence to be 100. The PRs notsatisfying these length constraints were hence Ô¨Åltered. Afterpreprocessing, we collected 41,832 PRs. The statistics of the
removed and the adequate PRs are presented in Table II.
V. E
V ALUATION
In this section, we Ô¨Årst describe the evaluation metrics and
the baselines. Then we present our research questions (RQs)
and corresponding experiment results. Finally, we show the
procedure and results of our human evaluation.
A. Evaluation Metrics
We evaluate our approach with the ROUGE metric [15],
which has been shown to correlate highly with human assess-
ments of summarized text quality [15]. SpeciÔ¨Åcally, we useROUGE-N (N=1,2) and ROUGE-L, which are widely used to
evaluate text summarization systems [16], [18].
The recall, precision and F1 score for ROUGE-N are
calculated as follows:
Rrouge -n=/summationtext
(gen,ref )‚ààS/summationtext
gram n‚ààrefCntgen(gramn)
/summationtext
(gen,ref )‚ààS/summationtext
gram n‚ààrefCntref(gramn)
Prouge -n=/summationtext
(gen,ref )‚ààS/summationtext
gram n‚ààrefCntgen(gramn)
/summationtext
(gen,ref )‚ààS/summationtext
gram n‚ààgenCntgen(gramn)
F1rouge -n=2Rrouge -nProuge -n
Rrouge -n+Prouge -n
wheregen,ref andSrefer to a generated description, its
reference description and the test set, gramnis an n-gram
phrase and Cntgen(gramn)andCntref(gramn)refer to the
occurrence number of gramningen andref, respectively.
In summary, Rrouge -nmeasures the percentage of the n-
grams in reference descriptions that an approach can generate,andP
rouge -npresents the percentage of ‚Äúcorrect‚Äù n-grams
(i.e., n-grams appearing in reference descriptions) in generateddescriptions. F1
rouge -nis a summary measure that combines
both precision and recall. The precision, recall and F1 score forROUGE-L are similar with those for ROUGE-N, but insteadof n-grams, they are calculated using the longest commonsubsequences between generated descriptions and reference
descriptions [15]. When comparing two approaches, we care
more about F1 scores, since they balance precision and recall.
ROUGE is usually reported as a percentage value between
0 and 100. We obtained ROUGE scores using the pyrougepackage [24] with Porter stemmer enabled.
B. Baselines
As this is the Ô¨Årst work on PR description generation, we
use two extractive baselines: LeadCM and LexRank.
1) LeadCM: LeadCM is proposed by us for this task. Given
the source sequence of a PR, LeadCM outputs the Ô¨Årst 25tokens of the commit message paragraph as its generateddescription. 25 is the median length of the PR descriptionsin our dataset. According to the construction of the sourcesequence (described in Section IV-B), the generated descrip-
tion is actually the Ô¨Årst few commit messages in this PR. The

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. hypothesis behind LeadCM is that developers may commit key
changes, e.g., implementing a feature, Ô¨Årst and make other lessimportant changes, such as Ô¨Åxing typos, later, so the Ô¨Årst fewcommit messages may summarize the core of a PR.
2) LexRank: LexRank [19] summarizes an article by calcu-
lating the relative sentence importance and selecting the mostimportant sentences in the article as the generated summary. It
hypothesizes that a sentence is more salient to an article if it
is similar to many sentences in this article. The importance(or centrality) of each sentence in an article is computedby applying the PageRank algorithm [25] on the sentencesimilarity matrix of this article. Given the source sequenceof a PR, we Ô¨Årst use the continuous LexRank method [19]
to rank its sentences according to their importance. Then we
concatenate these ranked sentences and keep the Ô¨Årst 25 tokensas the output, just like LeadCM.
C. Experiment Settings
Similar to Jiang et al. [10] and Hu et al. [26], we randomly
select 10% of the 41K PRs in our dataset for testing, 10% for
validation and the remaining 80% for training. Our approachuses 128-dimensional word embeddings. The encoder is asingle-layer bidirectional LSTM, the decoder is the same butunidirectional, and both of them use 256-dimensional hiddenstates. Since our encoder and decoder share the embeddinglayer, we collect words from both the source sequences andthe target sequences of the training set to build the vocabulary.The vocabulary size is set to 50K following See et al. [16].
At training time, we Ô¨Årst train our model for 25,000 iter-
ations only using the maximum-likelihood (ML) loss loss
ml,
evaluating the model every 1,000 iterations with the validation
set. The best performing ML model is obtained after 12,000iterations. Then we continue training this best ML model withthe hybrid loss loss (deÔ¨Åned in Equation 7) for another 28,000
iterations and also perform evaluation every 1,000 iterations.We get the Ô¨Ånal best model after 22,000 iterations, i.e., 34,000iterations in total. We leverage Adam [27] with a batch sizeof 8 to train our models. As suggested by Paulus et al. [18],we set the Œ≥in Equation 7 to 0.9984, and the learning rate of
Adam is set to 0.001 for the training with loss
mland 0.0001
for the training with loss.
When testing, we leverage beam search of width 4 to gen-
erate sequences. We notice that there exist repeating phrases
in some generated sequences and adopt a heuristic rule [18],which ignores a candidate beam if its current generated tokencreates a duplicate trigram, at each decoding step to reducesuch repetition.
Our replication package which contains our dataset, source
code, trained model and test results is available online [28],
[29].
D. RQ1: The Effectiveness of Our Approach
To investigate our approach‚Äôs effectiveness, we evaluate our
approach on our dataset in terms of ROUGE-1, ROUGE-2
and ROUGE-L, and compare it with the two baselines, i.e.,
LeadCM and LexRank.The evaluation results are shown in Table III, and our
approach is referred to as Attn+PG+RL. For text generationtasks, the F1 scores for ROUGE are typically between 0.2to 0.4 [16], [18], [30]. We can see from Table III that Ô¨Årstour approach tends to generate shorter descriptions (19.21tokens on average) than LexRank and LeadCM. Second, itoutperforms LexRank in terms of all metrics by large margins(from 32.35% to 138.61%). Also, it obtains higher precisionand F1 score than LeadCM for each ROUGE metric. Theimprovements in terms of the three F1 scores are 3.54,4.53 and 3.52 points, respectively. These results indicate thatcompared to the two baselines, our approach can capture thekey points of a PR more precisely.
We also manually inspect our test results. Table IV presents
an example in the test set. According to our inspection, weargue that the better performance of our approach mainlycomes from its two advantages:
1) Our approach can accurately identify important phrases
or sentences in source sequences. It learns knowledge aboutwhich phrases are important for summarizing a PR from thetraining data, hence it is more intelligent. For example, thePR in Table IV contains 3 commit messages and multipleadded code comments. LexRank extracts the wrong sentences
as output, performing worst. LeadCM outputs all commit
messages without Ô¨Åltering or sorting, while our approachprecisely identiÔ¨Åes the salient phrase, i.e., ‚Äútomcat 0 support inthe s-ramp‚Äù in the source sequence. Therefore, our approachoutperforms the baselines.
2) Our approach is an abstractive method with the ability of
dynamic generation. LexRank and LeadCM generate descrip-tions by extracting important sentences and cannot rephraseextracted sentences, generate novel words or dynamicallydecide how many sentences/tokens to generate. However,our approach can generate descriptions with different lengths
based on the input. Moreover, our approach can rephrase
important sentences, as shown in Table IV. This advantagereduces the number of unimportant phrases produced by ourapproach and results in high precision.
We also notice that the recall of our approach for ROUGE-1
and ROUGE-L are slightly lower than that of LeadCM. This is
because LeadCM always generates as many tokens as possibleuntil it gets 25, while our approach tends to procedure short butaccurate descriptions. Some phrases output by LeadCM maybe trivial, but they may contain some tokens in the referenceand hence improve the recall. For example, in Table IV, the
third sentence output by LeadCM is not a salient sentence.
However, it contains a ‚Äúfor‚Äù, which also occurs in the referenceand makes the recall of LeadCM higher than ours in thisexample.
In addition, it is a little surprising that the relatively com-
plicated LexRank performs worse than the naive LeadCM. We
argue that the reason is the hypothesis of LexRank, which isthat a sentence similar to many sentences is more important,does not always hold in the source sequences. For instance,in Table IV there are two identical comments in the sourcesequence which are computed as the most important sentences

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. TABLE III
COMPARISONS OF OUR APPROACH (ATTN+PG+RL) WITH EACH BASELINE IN TERMS OF ROUGE SCORES
Approach Avg. lengthROUGE-1 ROUGE-2 ROUGE-L
Recall Precision F1 score Recall Precision F1 score Recall Precision F1 score
LexRank 24.21 25.72 29.28 24.11 12.88 13.35 11.40 24.02 27.20 22.42
LeadCM 24.37 33.16 36.31 30.61 20.29 20.28 17.85 31.42 34.17 28.89
Attn+PG+RL 19.21 32.47 46.35 34.15 21.82 27.76 22.38 30.94 43.56 32.41
Attn+PG+RL vs LexRank -5.00 26.27% 58.29% 41.65% 69.43% 107.92% 96.33% 28.84% 60.14% 44.52%
Attn+PG+RL vs LeadCM -5.16 -2.07% 27.65% 11.57% 7.51% 36.89% 25.40% -1.51% 27.48% 12.18%
*Avg. length refers to the average length of the generated descriptions by each approach.
TABLE IV
TEST EXAMPLE 1
Source Sequence:initial tomcat 0 support . [cm-sep] tomcat 0 support in the s-ramp installer. [cm-sep] Ô¨Åxes for tomcat support .
[para-sep]
eat the error and try the next optioneat the error and try the next option
this Ô¨Ålter can be used to supply a source of credentials that can be used
when logging in to the jcr repository ( modeshap e).i t uses the inbound
request as the source of authentication .
constructor .
login with credentials set by some external force . this may be a servletÔ¨Ålterwhen running in a servlet container , or it may be null when running in a
jaas compliant application server ( e.g . jboss ) .
note : when passing ‚Äô null ‚Äô , it forces modeshape to authenticate witheither .
Reference:added support for tomcat 0 in s-ramp .
LexRank:
eat the error and try the next option eat the error and try the next option
this Ô¨Ålter can be used to supply
LeadCM:initial tomcat 0 support . tomcat 0 support in the s-ramp installer . Ô¨Åxesfor tomcat support .
Attn+PG+RL:initial tomcat 0 support in the s-ramp installer .
by LexRank. But they have no token in common with the
reference.
In summary, our approach outperforms the two base-
lines in terms of ROUGE-1, ROUGE-2 and ROUGE-L, and can generate more accurate descriptions than thebaselines.
E. RQ2: The Effects of Main Components
Our approach generates PR descriptions based on the atten-
tional encoder-decoder model (Attn). It integrates the pointergenerator (PG) to deal with OOV words and adopts the RL lossto directly optimize for ROUGE when training. We compare
our approach, i.e., Attn+PG+RL, with Attn and Attn+PG in
terms of ROUGE to understand the inÔ¨Çuence of the pointergenerator and the RL loss. Besides, since Attn is an effectiveand popular model for text generation tasks, we also regard itas an abstractive baseline for PR description generation andcomparing our approach with it can further investigate theeffectiveness of our approach.The evaluation results are shown in Table V. We can see that
Attn+PG outperforms Attn in terms of all metrics by 29.15%to 70.73%, which means the pointer generator can effectivelycope with OOV words and the generation of PR descriptionsbeneÔ¨Åts a lot from it. Table VI presents one of our test results.We can see that ‚Äúfulltextonline‚Äù and ‚Äúwebpagearchived‚Äù are
two OOV words. Attn cannot handle them hence produces
‚Äú[UNK]‚Äù instead, while both Attn+PG and Attn+PG+RL cangenerate the two words correctly.
Compared to Attn+PG, Attn+PG+RL performs better in
terms of all recall and F1 score metrics by more than 5.8%, butslightly worse in terms of all precision metrics. To Ô¨Ågure outthe reason, we inspect the descriptions generated by Attn+PGand Attn+PG+RL, and Ô¨Ånd that Attn+PG+RL tends to gen-erate longer descriptions than Attn+PG in order to increasethe RL reward, i.e., the F1-score for ROUGE-L. For example,
Attn+PG+RL generates more relevant tokens than Attn+PG for
the PR in Table VI. On average, Attn+PG+RL produces 5.19more tokens than Attn+PG, as shown in Table V. Therefore,the reduced precision of Attn+PG+RL can be regarded as theexpense of the improved recall; and the gain in recall is higherthan the loss in precision, which translates to high F1 scores.
In summary, our approach outperforms Attn and
Attn+PG. The pointer generator and the RL loss areeffective and helpful for boosting the effectiveness of ourapproach.
F . Human Evaluation
We also conduct a human evaluation to investigate our
approach‚Äôs effectiveness. We invite 6 human evaluators toassess the quality of the PR descriptions generated by our
approach and the two baselines. All of them are Ph.D. students
with 1-5 years of experience in Java programming.
1) Procedure: We randomly select 100 PRs from the test
set and evenly divide them into two groups. Each group is
evaluated by 3 different evaluators. For each PR, we showits source sequence and reference description followed by thethree PR descriptions generated by our approach and the twobaselines to the evaluators. The three generated descriptionsare randomly ordered. Human evaluators also have no ideaabout how these approaches work, so they cannot Ô¨Ågure out
which description is generated by which approach. The evalua-

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. TABLE V
EFFECTIVENESS OF EACH COMPONENT IN OUR APPROACH
Approach Avg. lengthROUGE-1 ROUGE-2 ROUGE-L
Recall Precision F1 score Recall Precision F1 score Recall Precision F1 score
Attn 13.95 19.68 36.23 22.92 11.32 18.13 12.74 18.90 34.54 21.95
Attn+PG 14.02 27.68 47.22 31.27 19.32 28.66 21.15 26.48 44.61 29.82
Attn+PG+RL 19.21 32.47 46.35 34.15 21.82 27.76 22.38 30.94 43.56 32.41
PG +0.07 40.63% 30.31% 36.47% 70.73% 58.08% 66.10% 40.15% 29.15% 35.87%
RL +5.19 17.33% -1.84% 9.21% 12.90% -3.14% 5.81% 16.82% -2.36% 8.68%
*Attn ,PGand RLrefer to the attentional encoder-decoder model, the pointer generator and the RL loss, respectively.
TABLE VI
TEST EXAMPLE 2
Source Sequence:add some more links to fultextonline.fulltextonline and webpagearchived in parallel rather than choosing be-tween one of them ) but this should be ok.- add when 6551ex starts with ‚Äô onl ‚Äô- add when medium is rdvocab : 0- add resource ht018400499 to the test set- adjust test sets .[cm-sep]add rule to avoid redundant entries in fulltextonline and webpagearchivedintroduced with the last commit :we will now have fulltextonline and webpagearchived not in parallel butrather choose between them , with preferring webpagearchived .- adjust test sets .
Reference:we will now have fulltextonline and webpagearchived not in parallel butrather choose between them , with preferring webpagearchived .- add when 6551ex starts with ‚Äò onl ‚Äô- add when medium is rdvocab : 0- adjust test sets
Attn:adapt and [UNK] in parallels .
Attn+PG:fulltextonline and webpagearchived in parallel rather than choosing be-tween one of them ) but this should be ok.
Attn+PG+RL:fulltextonline and webpagearchived in parallel rather than choosing be-tween one of them ) but this should be ok.- add when 6551ex starts with ‚Äò onl ‚Äô- adjust test sets .
tors are asked to assign a score from 0 to 7 to each generated
description to measure the semantic similarity between thegenerated description and the reference. The higher the scorethe closer is the corresponding PR description to the reference.
Evaluators are allowed to search related information, such as
unfamiliar concepts, through the Internet.
2) Results: Each PR description obtains three scores from
three evaluators. We calculate the average score as its Ô¨Ånal
score . The distribution of the Ô¨Ånal scores is presented in
Figure 4. We can see that compared to the baselines, ourapproach produces more PR descriptions with high scores(/greaterorequalslant4) and less with low scores ( <4). Besides, our approach
generates far more PR descriptions with the average scorebetween 6 and 7. But we also notice that our generateddescriptions with the average score between 0 and 1 are alittle more than those generated by the baselines. The reason/g19/g20/g21/g22/g23/g24/g25/g26
/g36/g89/g72/g85/g68/g74/g72/g3/g54/g70/g82/g85/g72/g19/g20/g19/g21/g19/g22/g19/g49/g88/g80/g47/g72/g91/g53/g68/g81/g78
/g47/g72/g68/g71/g38/g48
/g36/g87/g87/g81/g14/g51/g42/g14/g53/g47
Fig. 4. The distribution of the Ô¨Ånal scores obtained from our human
evaluation. Each bar presents the number of the average scores obtained byan approach that fall in a speciÔ¨Åc score interval. For example, the leftmostblue bar shows that 18 descriptions generated by LexRank obtain an averagescore between 0 to 1.
may be that our approach generates PR descriptions from
scratch instead of directly extracting source sentences, andhence sometimes may fail to generate important words.
We calculate the average scores of LexRank, LeadCM and
Attn+PG+RL across all sampled PRs, which are 2.14, 2.73and 3.27, respectively. Although the average score of ourapproach is still not perfect, our approach is the Ô¨Årst step onthis topic and can inspire follow-up work. We also conductWilcoxon signed-rank tests [31] at the conÔ¨Ådence level of 95%considering the 100 Ô¨Ånal scores for each approach. The p-
values of our approach compared with LexRank and LeadCM
are all less than 0.01, which means the improvements achievedby our approach are signiÔ¨Åcant.
In summary, our human evaluation shows that our
approach outperforms the baselines signiÔ¨Åcantly, and cangenerate more high-quality PR descriptions.
VI. D ISCUSSION
In this section, we discuss situations where our approach
performs badly and threats to validity.
A. Where Does Our Approach Perform Badly
We carefully inspect the PRs where our approach does not
obtain good ROUGE-L F1 scores. We Ô¨Ånd that our approachusually performs badly if the reference description mainlypresents the information that cannot be found in the sourcesequence. We Ô¨Ånd three types of such information: 1) context
information, including the motivation and the test results of

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. TABLE VII
TEST EXAMPLE 3
Source Sequence:improve performance of preprocess . [cm-sep] improve performance ofpreprocess , cleanup code . [cm-sep] improve performance of preprocess ,cleanup code[para-sep]newcommand = newcommand.replaceall ( ‚Äô \\\\ ([ÀÜ\\\\ (]*\\\\ )‚Äô,
empty ) ;
newcommand = newcommand.replaceal l(‚Äô;.*‚Äô, empty ) ;
build up the decimal formatter .build up the regular expression .return command.replaceall ( ‚Äò \\\\ ‚Äô , empt y);.
[para-sep]only build the decimal formatter if the truncation length has changed .build up the decimal formatter .build up the regular expression .
Reference:when opening large Ô¨Åles , the preprocessing takes a very long time
. by compiling the regular expressions once rather than using the
string.replaceall methods , we can save signiÔ¨Åcant time .on my lapto p , i opened a gcode Ô¨Åle with sha lines ( laser image raster )
with my changes - 16155msmaster - 56178ms
Attn+PG+RL:improve performance of preprocess , cleanup code .
a PR; 2) implementation details; 3) subjective sentences, i.e.,
sentences describing personal feelings, plans, etc.
Table VII presents a typical example. We can see that
the reference description of this PR contains three sentences,
which respectively describe the motivation, the implementa-tion details and the test results of the PR, and such informationdoes not appear in the source sequence. The description
generated by our approach has little in common with the
reference description, hence it gets low ROUGE scores.
As for subjective sentences, we Ô¨Ånd a test PR for which the
reference description is ‚Äúwe did make awesome, shall we landit‚Äù. This description is a subjective sentence without describingthe changes made in the PR, while our approach tries to
summarize the PR by generating ‚Äúrefactored common pieces
out into httpobject.‚Äù. Upon our inspection, the descriptionproduced by our method correctly captures the meaning ofthe PR, while the reference description is not helpful forunderstanding the changes well.
Sometimes, our approach also fails to capture key phrases in
the source sequence and consequently performs badly. But thissituation is less common than the one mentioned above. Ourevaluation results in Section V-D also show that our approachcan better capture key phrases than the two baselines.
B. Threats to V alidity
One threat to validity is that our dataset was built only
from Java projects, which may not be representative of all
programming languages. However, Java is a popular program-ming language. Besides, our approach takes commit messagesand source code comments as input hence can also be appliedto projects of other programming languages.
Another threat to validity is that the non-summary infor-
mation, such as signatures and subjective sentences, in PR
descriptions may affect the effectiveness of our approach.PR descriptions are free-form text, and we cannot guarantee
their quality and content. We mitigate this threat by using aset of heuristic rules to Ô¨Ålter out non-summary informationwhen preprocessing. But it is hard to distill the patterns ofall non-summary information. Since this work focuses onlearning to generate PR descriptions from existing PRs, furtherimprovements on data preprocessing are more suitable forfuture work.
There is also a threat related to our human evaluation.
We cannot guarantee that each score assigned to every PRdescription is fair. To mitigate this threat, each sampled PRis evaluated by 3 human evaluators, and we use the average
score of the 3 evaluators as the Ô¨Ånal score.
In addition, our baseline approaches produce summaries
of length 25 tokens unless there are fewer than 25 tokens
available in the source sequence. This may result in incompletesentences in the output of these approaches, which mayhave negatively affected the corresponding ratings by human
evaluators. However, this threat does not affect the ROUGEscores which also conÔ¨Årm the superiority of our approach.
VII. R
ELATED WORK
This section discusses the related studies on documenting
software changes, understanding pull requests, and summariz-
ing and documenting other software artifacts.
A. Documenting Software Changes
Commits, PRs and releases are software changes of different
granularity. Some tools have been proposed to document com-
mits based on diverse inputs automatically [8]‚Äì[11], [32]‚Äì[35].For example, Buse and Weimer proposed D
ELTA DOC[8],
a technique that can summarize a commit by Ô¨Årst using
symbolic execution and path predicate analysis to generate
the behavioral difference and then applying some heuristictransformations to generate a natural language description.Similarly, Cortes et al. built ChangeScribe [33], a tool which
Ô¨Årst identiÔ¨Åes the stereotype of a commit from the abstractsyntax trees before and after the commit and then generates
a descriptive commit message using pre-deÔ¨Åned Ô¨Ålters andtemplates. Rastkar and Murphy [35] proposed an approach to
generate the motivation of a commit by extracting motivational
sentences from its relevant documents. Jiang et al. [10] adoptedan attentional encoder-decoder model to generate commit mes-sages from diffs . Liu et al. [11] proposed an information-
retrieval-based method to generate commit messages for new
diffs by reusing proper existing commit messages.
Researchers have also explored the automatic generation of
release notes [12], [13], [36]. For instance, Abebe et al. [36]
identiÔ¨Åed six types of information contained in release notesand leveraged machine learning techniques to decide whetheran issue should be mentioned in release notes. Moreno et
al. proposed a tool named ARENA [12], [13], which Ô¨Årst
extracts and summarizes each commit in a release, and thenuses manually deÔ¨Åned templates to organize these summarieswith their related information in the issue tracker to generate
release notes.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. Commit messages, PR descriptions, and release notes doc-
ument software changes occurring at different granularity [3],[36]‚Äì[38]. As described in Section I, PR descriptions oftenneed to summarize several related commits when compared tocommit messages, and are different from release notes in termsof target audiences and information structure. Moreover, sincethe documents of small-granularity changes (e.g., the commitsin a PR) are usually available when developers document alarge-granularity change (e.g., the PR), the techniques for gen-erating commit messages and release notes are complementaryrather than competing with our approach.
B. Understanding Pull Requests
Many empirical studies were focusing on understanding
pull requests (PRs) and the pull-based development. Someof them focused on analyzing which factors affect the PRevaluation [5], [39], [40]. For example, Rahman and Roy [39]investigated how the discussion texts, project-speciÔ¨Åc informa-
tion (e.g., project maturity) and developer-speciÔ¨Åc information
(e.g., experience) of PRs affect their acceptance. Tsay etal. [40] found that both technical and social factors inÔ¨Çuencethe acceptance of PRs.
Some other studies aimed to understand how the pull-based
development works [2], [3], [41]. For instance, Gousios etal. [2] analyzed the popularity of the pull-based development
model, characterized the lifecycle of PRs and also explored
which factors inÔ¨Çuence the merge decision and delay of a PR.In their following work, Gousios et al. [3], [41] conductedlarge-scale surveys to study how integrators (people who areresponsible for integrating PRs) and contributors collaboratein the pull-based development model. They highlighted the
challenges faced by integrators, such as maintaining projects‚Äô
quality and prioritizing external contributions, and the chal-lenges faced by contributors like unawareness of project statusand poor responsiveness from integrators.
Prior work also proposed many techniques to deal with
the challenges developers face when using the pull-baseddevelopment [6], [7], [42]‚Äì[44]. For example, Veen et al. [42]proposed PRioritizer, a tool which prioritizes PRs based on
machine learning techniques and multiple extracted features
such as the number of discussion comments. To reduce theresponse time of PRs, Yu et al. [7] proposed an approachto automatically recommend reviewers for a PR based onits title, description, and the social relations of developers.
These studies motivate our work to generate PR descriptionsto facilitate downstream tasks.
C. Summarizing and Documenting Other Software Artifacts
Besides software changes, researchers have studied the auto-
matic summarization of other software artifacts, such as source
code [26], [30], [45]‚Äì[52], bug reports [53]‚Äì[56], app re-
views [57], developer discussions [58]‚Äì[60] and developmentactivity [61]. Concerning source code, some techniques havebeen proposed to summarize source code based on programanalysis and manually-deÔ¨Åned templates [45], [47], [49], infor-mation retrieval [46], [48], and learning-based methods [26],[30], [50]. Some of them also use encoder-decoder models. For
example, Hu et al. [26] proposed a framework which leveragedan attentional encoder-decoder model to generate commentsfor Java methods. Wan et al. [30] proposed a novel encoder-decoder model with a hybrid encoder and a reinforcement-learning-based decoder to generate code comments. They usedthe actor-critic algorithm [62] with an extra neural network as
the critic. Different from Wan et al.‚Äôs work, our approach uses
the SCST algorithm [17], which is based on the REINFORCEalgorithm [63] and does not require extra networks. Besides,we do not directly leverage RL to decode but only to computea special loss for better training.
As for bug reports, previous work focused on identifying
and extracting important sentences from bug reports as their
summaries. For example, Rastkar et al. [53], [54] trained aconversion-based summarizer using a bug report corpus toidentify important sentences automatically. Mani et al. [55]and Lotufo et al. [56] proposed unsupervised approaches basedon noise reducer [55] or heuristic rules [56] to perform bug
report summarization. Different from their work, this work
aims to generate PR descriptions from commit messages andsource code comments that are added using an abstractivemethod.
VIII. C
ONCLUSION AND FUTURE WORK
In this paper, we aim to automatically generate descriptions
for pull requests from their commit messages and the sourcecode comments that are added. We formulate this problem asa sequence-to-sequence learning problem and point out twochallenges of this problem, i.e., out-of-vocabulary words andthe gap between the training loss function of sequence-to-sequence models and the discrete evaluation metric ROUGE,
which has been shown to correspond to human evaluation [15].
We propose a novel encoder-decoder model to solve thisproblem. To handle out-of-vocabulary words, our approachadopts the pointer generator to learn to copy words from thesource sequences. As for the second challenge, our approachincorporates a reinforcement learning technique and adoptsa special loss function to optimize for ROUGE directly.Comprehensive experiments on a dataset with over 41K pullrequests and a human evaluation show that our approachoutperforms two competitive baselines.
In the future, we plan to further investigate the usefulness
of our approach by using it to generate summaries for the pull
requests without descriptions. We also plan to improve our
approach by involving additional related software artifacts asinput. For example, by taking diff Ô¨Åles and relevant bug reportsas input, our approach may be able to infer the implementationdetails and the motivation of a PR.
A
CKNOWLEDGMENT
This research was partially supported by the Na-
tional Key Research and Development Program of China(2018YFB1003904), NSFC Program (No. 61972339) and theAustralian Research Council‚Äôs Discovery Early Career Re-
searcher Award (DECRA) funding scheme (DE180100153).

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] E. T. Barr, C. Bird, P. C. Rigby, A. Hindle, D. M. German, and
P. Devanbu, ‚ÄúCohesive and isolated development with branches,‚Äù in Pro-
ceedings of the International Conference on Fundamental Approachesto Software Engineering . Springer, 2012, pp. 316‚Äì331.
[2] G. Gousios, M. Pinzger, and A. v. Deursen, ‚ÄúAn exploratory study
of the pull-based software development model,‚Äù in Proceedings of the
International Conference on Software Engineering . ACM, 2014, pp.
345‚Äì355.
[3] G. Gousios, M.-A. Storey, and A. Bacchelli, ‚ÄúWork practices and
challenges in pull-based development: The contributor‚Äôs perspective,‚Äù inProceedings of the International Conference on Software Engineering .
IEEE, 2016, pp. 285‚Äì296.
[4] T. Elliott, ‚ÄúThe state of the octoverse 2018,‚Äù https://github.blog/
2018-10-16-state-of-the-octoverse/, 2018.
[5] Y . Yu, H. Wang, V . Filkov, P. Devanbu, and B. Vasilescu, ‚ÄúWait for
it: Determinants of pull request evaluation latency on github,‚Äù in Pro-
ceedings of the Working Conference on Mining Software Repositories .
IEEE, 2015, pp. 367‚Äì371.
[6] Y . Fan, X. Xia, D. Lo, and S. Li, ‚ÄúEarly prediction of merged code
changes to prioritize reviewing tasks,‚Äù Empirical Software Engineering ,
vol. 23, no. 6, pp. 3346‚Äì3393, 2018.
[7] Y . Yu, H. Wang, G. Yin, and C. X. Ling, ‚ÄúReviewer recommender of
pull-requests in github,‚Äù in Proceedings of the International Conference
on Software Maintenance and Evolution . IEEE, 2014, pp. 609‚Äì612.
[8] R. P. Buse and W. R. Weimer, ‚ÄúAutomatically documenting program
changes,‚Äù in Proceedings of the International Conference on Automated
Software Engineering . ACM, 2010, pp. 33‚Äì42.
[9] M. Linares-V ¬¥asquez, L. F. Cort ¬¥es-Coy, J. Aponte, and D. Poshyvanyk,
‚ÄúChangescribe: A tool for automatically generating commit messages,‚ÄùinProceedings of the International Conference on Software Engineering .
IEEE, 2015, pp. 709‚Äì712.
[10] S. Jiang, A. Armaly, and C. McMillan, ‚ÄúAutomatically generating
commit messages from diffs using neural machine translation,‚Äù inProceedings of the International Conference on Automated SoftwareEngineering . IEEE, 2017, pp. 135‚Äì146.
[11] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, ‚ÄúNeural-
machine-translation-based commit message generation: How far arewe?‚Äù in Proceedings of the International Conference on Automated
Software Engineering . ACM, 2018, pp. 373‚Äì384.
[12] L. Moreno, G. Bavota, M. Di Penta, R. Oliveto, A. Marcus, and
G. Canfora, ‚ÄúAutomatic generation of release notes,‚Äù in Proceedings of
the International Symposium on F oundations of Software Engineering .
ACM, 2014, pp. 484‚Äì495.
[13] L. Moreno, G. Bavota, M. Di Penta, R. Oliveto, A. Marcus, and
G. Canfora, ‚ÄúArena: An approach for the automated generation of releasenotes,‚Äù IEEE Transactions on Software Engineering , vol. 43, no. 2, pp.
106‚Äì127, 2017.
[14] D. Bahdanau, K. Cho, and Y . Bengio, ‚ÄúNeural machine translation by
jointly learning to align and translate,‚Äù arXiv preprint arXiv:1409.0473 ,
2014.
[15] C.-Y . Lin, ‚ÄúRouge: A package for automatic evaluation of summaries,‚Äù
Text Summarization Branches Out , 2004.
[16] A. See, P. J. Liu, and C. D. Manning, ‚ÄúGet to the point: Summarization
with pointer-generator networks,‚Äù in Proceedings of the Annual Meeting
of the Association for Computational Linguistics (V olume 1: LongPapers) , 2017, pp. 1073‚Äì1083.
[17] S. J. Rennie, E. Marcheret, Y . Mroueh, J. Ross, and V . Goel, ‚ÄúSelf-
critical sequence training for image captioning,‚Äù in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition , 2017,
pp. 7008‚Äì7024.
[18] R. Paulus, C. Xiong, and R. Socher, ‚ÄúA deep reinforced model for
abstractive summarization,‚Äù arXiv preprint arXiv:1705.04304 , 2017.
[19] G. Erkan and D. R. Radev, ‚ÄúLexrank: Graph-based lexical centrality
as salience in text summarization,‚Äù Journal of artiÔ¨Åcial intelligence
research , vol. 22, pp. 457‚Äì479, 2004.
[20] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, ‚ÄúSequence level train-
ingwith recurrent neural networks,‚Äù arXiv preprint arXiv:1511.06732 ,
2015.
[21] N. Munaiah, S. Kroh, C. Cabrey, and M. Nagappan, ‚ÄúCurating github for
engineered software projects,‚Äù Empirical Software Engineering , vol. 22,
no. 6, pp. 3219‚Äì3253, 2017.[22] ‚ÄúNatural language toolkit nltk 3.4.1 documentation,‚Äù http://www.nltk.
org/, 2019.
[23] F. N. A. Al Omran and C. Treude, ‚ÄúChoosing an nlp library for analyzing
software documentation: A systematic literature review and a series ofexperiments,‚Äù in Proceedings of the International Conference on Mining
Software Repositories . IEEE, 2017, pp. 187‚Äì197.
[24] ‚Äúpyrouge 0.1.3,‚Äù https://pypi.org/project/pyrouge/0.1.3/, 2019.[25] L. Page, S. Brin, R. Motwani, and T. Winograd, ‚ÄúThe pagerank citation
ranking: Bringing order to the web.‚Äù Stanford InfoLab, Tech. Rep., 1999.
[26] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, ‚ÄúDeep code comment gener-
ation,‚Äù in Proceedings of the Conference on Program Comprehension .
ACM, 2018, pp. 200‚Äì210.
[27] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
arXiv preprint arXiv:1412.6980 , 2014.
[28] ‚ÄúOur replication package,‚Äù https://tinyurl.com/y3yk6oey, 2019.[29] ‚ÄúOur source code,‚Äù https://github.com/Tbabm/PRSummarizer, 2019.[30] Y . Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu, ‚ÄúIm-
proving automatic source code summarization via deep reinforcementlearning,‚Äù in Proceedings of the International Conference on Automated
Software Engineering . ACM, 2018, pp. 397‚Äì407.
[31] F. Wilcoxon, ‚ÄúIndividual comparisons by ranking methods,‚Äù Biometrics
bulletin , vol. 1, no. 6, pp. 80‚Äì83, 1945.
[32] T.-D. B. Le, J. Yi, D. Lo, F. Thung, and A. Roychoudhury, ‚ÄúDynamic
inference of change contracts,‚Äù in Proceedings of the International
Conference on Software Maintenance and Evolution . IEEE, 2014, pp.
451‚Äì455.
[33] L. F. Cort ¬¥es-Coy, M. Linares-V ¬¥asquez, J. Aponte, and D. Poshyvanyk,
‚ÄúOn automatically generating commit messages via summarization ofsource code changes,‚Äù in Proceedings of the International Working
Conference on Source Code Analysis and Manipulation . IEEE, 2014,
pp. 275‚Äì284.
[34] J. Shen, X. Sun, B. Li, H. Yang, and J. Hu, ‚ÄúOn automatic summarization
of what and why information in source code changes,‚Äù in Proceedings
of the Annual Computer Software and Applications Conference , vol. 1.
IEEE, 2016, pp. 103‚Äì112.
[35] S. Rastkar and G. C. Murphy, ‚ÄúWhy did this code change?‚Äù in Proceed-
ings of the International Conference on Software Engineering . IEEE,
2013, pp. 1193‚Äì1196.
[36] S. L. Abebe, N. Ali, and A. E. Hassan, ‚ÄúAn empirical study of software
release notes,‚Äù Empirical Software Engineering , vol. 21, no. 3, pp. 1107‚Äì
1142, 2016.
[37] Y . Fu, M. Yan, X. Zhang, L. Xu, D. Yang, and J. D. Kymer, ‚ÄúAutomated
classiÔ¨Åcation of software change messages by semi-supervised latentdirichlet allocation,‚Äù Information and Software Technology , vol. 57, pp.
369‚Äì377, 2015.
[38] M. Yan, Y . Fu, X. Zhang, D. Yang, L. Xu, and J. D. Kymer, ‚ÄúAuto-
matically classifying software changes via discriminative topic model:
Supporting multi-category and cross-project,‚Äù Journal of Systems and
Software , vol. 113, pp. 296‚Äì308, 2016.
[39] M. M. Rahman and C. K. Roy, ‚ÄúAn insight into the pull requests of
github,‚Äù in Proceedings of the Working Conference on Mining Software
Repositories . ACM, 2014, pp. 364‚Äì367.
[40] J. Tsay, L. Dabbish, and J. Herbsleb, ‚ÄúInÔ¨Çuence of social and technical
factors for evaluating contribution in github,‚Äù in Proceedings of the
International Conference on Software Engineering . ACM, 2014, pp.
356‚Äì366.
[41] G. Gousios, A. Zaidman, M.-A. Storey, and A. Van Deursen, ‚ÄúWork
practices and challenges in pull-based development: The integrator‚Äôsperspective,‚Äù in Proceedings of the International Conference on Software
Engineering . IEEE, 2015, pp. 358‚Äì368.
[42] E. Van Der Veen, G. Gousios, and A. Zaidman, ‚ÄúAutomatically prioritiz-
ing pull requests,‚Äù in Proceedings of the Working Conference on Mining
Software Repositories . IEEE, 2015, pp. 357‚Äì361.
[43] M. B. Zanjani, H. Kagdi, and C. Bird, ‚ÄúAutomatically recommending
peer reviewers in modern code review,‚Äù IEEE Transactions on Software
Engineering , vol. 42, no. 6, pp. 530‚Äì543, 2016.
[44] J. Jiang, Y . Yang, J. He, X. Blanc, and L. Zhang, ‚ÄúWho should comment
on this pull request? analyzing attributes for more accurate commenterrecommendation in pull-based development,‚Äù Information and Software
Technology , vol. 84, pp. 48‚Äì62, 2017.
[45] G. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker,
‚ÄúTowards automatically generating summary comments for java meth-ods,‚Äù in Proceedings of the International Conference on Automated
Software Engineering . ACM, 2010, pp. 43‚Äì52.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. [46] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus, ‚ÄúOn the use of
automated text summarization techniques for summarizing source code,‚ÄùinProceedings of the Working Conference on Reverse Engineering .
IEEE, 2010, pp. 35‚Äì44.
[47] L. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. Pollock, and K. Vijay-
Shanker, ‚ÄúAutomatic generation of natural language summaries for java
classes,‚Äù in Proceedings of the International Conference on Program
Comprehension . IEEE, 2013, pp. 23‚Äì32.
[48] E. Wong, J. Yang, and L. Tan, ‚ÄúAutocomment: Mining question and
answer sites for automatic comment generation,‚Äù in Proceedings of the
International Conference on Automated Software Engineering . IEEE,
2013, pp. 562‚Äì567.
[49] P. W. McBurney and C. McMillan, ‚ÄúAutomatic documentation genera-
tion via source code summarization of method context,‚Äù in Proceedings
of the International Conference on Program Comprehension . ACM,
2014, pp. 279‚Äì290.
[50] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, ‚ÄúSummarizing source
code using a neural attention model,‚Äù in Proceedings of the Annual
Meeting of the Association for Computational Linguistics (V olume 1:Long Papers) , vol. 1, 2016, pp. 2073‚Äì2083.
[51] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, ‚ÄúSummarizing
source code with transferred api knowledge,‚Äù in Proceedings of the
International Joint Conference on ArtiÔ¨Åcial Intelligence . AAAI Press,
2018, pp. 2269‚Äì2275.
[52] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, ‚ÄúDeep code comment generation
with hybrid lexical and syntactical information,‚Äù Empirical Software
Engineering , pp. 1‚Äì39, 2019.
[53] S. Rastkar, G. C. Murphy, and G. Murray, ‚ÄúSummarizing software arti-
facts: A case study of bug reports,‚Äù in Proceedings of the International
Conference on Software Engineering . ACM, 2010, pp. 505‚Äì514.
[54] S. Rastkar, G. C. Murphy, and G. Murray, ‚ÄúAutomatic summarizationof bug reports,‚Äù IEEE Transactions on Software Engineering , vol. 40,
no. 4, pp. 366‚Äì380, 2014.
[55] S. Mani, R. Catherine, V . S. Sinha, and A. Dubey, ‚ÄúAusum: Approach
for unsupervised bug report summarization,‚Äù in Proceedings of the
International Symposium on the F oundations of Software Engineering .
ACM, 2012, p. 11.
[56] R. Lotufo, Z. Malik, and K. Czarnecki, ‚ÄúModelling the ‚Äòhurried‚Äô bug
report reading process to summarize bug reports,‚Äù Empirical Software
Engineering , vol. 20, no. 2, pp. 516‚Äì548, 2015.
[57] A. Di Sorbo, S. Panichella, C. V . Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, ‚ÄúWhat would users change in myapp? summarizing app reviews for recommending software changes,‚Äù inProceedings of the International Symposium on F oundations of SoftwareEngineering . ACM, 2016, pp. 499‚Äì510.
[58] G. Uddin and F. Khomh, ‚ÄúAutomatic summarization of api reviews,‚Äù
inProceedings of the International Conference on Automated Software
Engineering . IEEE, 2017, pp. 159‚Äì170.
[59] Q. Huang, X. Xia, D. Lo, and G. C. Murphy, ‚ÄúAutomating intention
mining,‚Äù IEEE Transactions on Software Engineering , 2018.
[60] G. Viviani, M. Famelis, X. Xia, C. Janik-Jones, and G. C. Murphy,
‚ÄúLocating latent design information in developer discussions: A studyon pull requests,‚Äù IEEE Transactions on Software Engineering , 2019.
[61] C. Treude, F. Figueira Filho, and U. Kulesza, ‚ÄúSummarizing and
measuring development activity,‚Äù in Proceedings of the Joint Meeting
on F oundations of Software Engineering . ACM, 2015, pp. 625‚Äì636.
[62] V . R. Konda and J. N. Tsitsiklis, ‚ÄúActor-critic algorithms,‚Äù in Proceed-
ings of the International Conference on Neural Information ProcessingSystems , 1999, pp. 1008‚Äì1014.
[63] R. J. Williams, ‚ÄúSimple statistical gradient-following algorithms for
connectionist reinforcement learning,‚Äù Machine learning , vol. 8, no. 3-4,
pp. 229‚Äì256, 1992.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:09:45 UTC from IEEE Xplore.  Restrictions apply. 