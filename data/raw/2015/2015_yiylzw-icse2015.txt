A Synergistic Analysis Method for Explaining
Failed Regression Tests
Qiuping Yi∗†, Zijiang Yang‡, Jian Liu∗, Chen Zhao∗and Chao Wang§
∗Institute of Software, Chinese Academy of Sciences, Beijing, China
†University of Chinese Academy of Sciences, Beijing, China
‡Department of Computer Science, Western Michigan University, Kalamazoo, Michigan, USA
§Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, Virginia, USA
Abstract—We propose a new automated debugging method
forregression testingbasedon asynergisticapplicationof both
dynamic and semantic analysis. Our method takes a failure-
inducing test input, a buggy program, and an earlier correct
version of the same program, and computes a minimal set of
code changes responsible for the failure, as well as explaining
howthecodechangesleadtothefailure.Althoughthisproblem
hasbeenthesubjectofintensiveresearchinrecentyears,existing
methodsarerarelyadoptedbydevelopersinpracticesincethey
do not produce sufﬁciently accurate fault explanations for real
applications. Our new method is signiﬁcantly faster and more
accurate than existing methods for explaining failed regression
testsinrealapplications,duetoitssynergisticanalysisframework
that iteratively applies both dynamic analysis and a constraint
solver based semanticanalysis to leverage theircomplementary
strengths.Wehave implementedournew method ina software
toolbasedontheLLVMcompilerandtheKLEEsymbolicvirtual
machine.OurexperimentsonlargerealLinuxapplicationsshow
thatthenewmethodisbothefﬁcientandeffectiveinpractice.
I. INTRODUCTION
Experiencehasshownthatsoftware updatesoftenintroduce
new bugs. Therefore, it is good practice to conduct regres-
sion testing during software development, which determines
whether new bugs have been introduced into the code with
previously working functionality. Although there exist many
tools to automate this process in practice, e.g., re-running
regression tests periodically and reporting failures as soon as
they occur, detecting these failures is only the ﬁrst step. The
more challenging task is to identifythe relevant code changes
andexplainwhy these changes lead to the failure. This is
where existing methods fall short.
Althoughtherehasbeenalargebodyofworkonautomated
debugging in the context of regression testing, few of the
existing methods are actively used by developers in practice,
for several reasons. First, they are not accurate enough in that
faulty code changes are either missed or buried in a large
number of irrelevant ones. Second, the causal relationship be-
tween faulty code changes and the manifested failures are not
explainedwellenough.Third,in manycases, merelyreverting
the faulty code changes is not enough because other code
changesmaybe neededaswell to makethe modiﬁedprogram
compile successfully. Due to these problems, developers are
forced to rely on manual efforts to interpret the failures.
We propose a new synergisticanalysis frameworkto signif-
icantly improve the accuracy of the automatically computed
faultexplanations,byleveragingare-executionbaseddynamic
analysis together with a constraint solver based semanticReplay Diff Code
causes
Semantic Analysis
Critical Predicate
IdentificationDynamic AnalysisFailed Test t Program P′Program P
∆rootπ∧ρ ∆
Fig. 1. Synergistic (Dynamic and Semantic) Analysis Framework.
analysis to take advantage of their complementary strengths.
Speciﬁcally, dynamic analysis is effective in identifying the
correlation between code changes and the manifested failure,
i.e., by reverting some of these changes and re-executing
the program to see if it still fails, but this is ineffective in
identifying the causal relationship between the code changes
and the failure. In contrast, semantic analysis is effective in
identifying the causal relationship between the code changes
and the failure, but is ineffective in identifying the actually
faulty code changesfrom a large number of possible ones. By
leveragingbothtypesof analysis, we can locatethe rootcause
more accurately as well as more quickly.
Fig. 1 shows the overall ﬂow of our method. Given a
correct program P, its faulty evolution P′, and a failed
test case t, our method ﬁrst computes the code difference
betweenPandP′(denoted ∆).Then,itreplaysthe erroneous
execution πand obtains the failed assert condition ( ρ). Here,
we assume the failure is modeled as a failed assertion. Once
π,ρ, and∆are available, our method starts the iterative
steps of applying semanticanalysis and dynamicanalysis,
which are connected with a third component called critical
predicate identiﬁcation . Initially, the critical predicate fed to
the semantic analysis is the failed assert condition ρ, based
on which our semantic analysis computes the cause (causal
chain of events responsible for ρ). In the subsequent dynamic
analysis, we identify, among the code changes in ∆, a subset
(∆root) thatis responsibleforthe criticalpredicate ρ. If∆root
can be found, we are done. Otherwise, we identify another
critical predicate from the current causes and try again. In
the end, we report ∆roottogether with all related causes and
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.46257
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.46257
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.46257
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 © 2015 IEEE
DOI 10.1109/ICSE.2015.46257
ICSE 2015, Florence, Italy--- find4.15/find.c
+++ find4.18/find.c
@@ -377,0 +421,6 @@
+
+#ifdef O_NOFOLLOW
+ options.open_nofollow_available = check_nofollow();
+#else
+ options.open_nofollow_available = false;
+#endif
Fig. 2. Incorrect code changes reported by ADD on ﬁnd-a.
present them in a tree-like structure, to highlight the causal
relationships between the changes and the failure.
Our new method has signiﬁcant advantages over existing
methodssuchasdeltadebugging(DD)[1]anditsvariantssuch
asaugmenteddeltadebugging(ADD)[2],duetoitssynergistic
application of both semantic analysis and dynamic analysis.
Delta debugging, in contrast, relies on dynamic analysis only.
As an example, consider a regression test failure in version
4.2.18 of Linux application ﬁnd, which has 24K lines of code
and 71 code changes since the last correct version 4.2.15.
Fig. 2showsthecodechangeslocalizedbyADD, whichcom-
pletely missed the real bug. Although reverting these changes
can modify the value of open_nofollow_available in
functioncheck_nofollow , therebyavoiding the buggy func-
tionsafely_chdir_nofollow , it merely dodges the failure
for the given test input, without ﬁxing the bug.
In contrast, our new method would report the changes
ch1andch2as shown in Fig. 3. A careful study of the
bug ﬁx provided by the developers shows that Change
ch1matches the actual bug ﬁx. The bug is due to
the fourth argument symlink_handling of the function
safely_chdir_nofollow , which was ignored in the new
version. The developers ﬁxed the bug by adding a switch
statementtohandlethepreviouslyignoredargument.Although
Changech2does not need to be reverted or modiﬁed in order
to ﬁx the bug, it is still important since it explains why the
faulty function is invoked in the ﬁrst place. Therefore, the
failure explanation computed by our method is more accurate
and helpful to debugging. In addition, our method identiﬁes
14 auxiliary code changes that need to be reverted together
withch1andch2to make the modiﬁed program compile
successfully – in previous methods, this time-consuming step
requires the developers’ manual effort.
Although DD reports the two faulty code changes ch1and
ch2, which is better than ADD (since it missed them), these
two changes are buried among eight other code changes that
areirrelevanttothefailure.Thedevelopershavetosiftthrough
these other changes manually to understand the root cause.
Furthermore, the program obtained by reverting only these
ten changes cannot be compiled successfully, which prevents
the developers from quickly checking the correlation between
them and the failure. Our new method, in contrast, can auto-
matically identify the auxiliary changes needed to be reverted
tomaketheprogramcompilesuccessfully.Finally,neitherDD
nor ADD can guarantee that there is a causality relationship
between the reportedcode changesand the manifested failure,
whereas our new method can.--- find4.15/find.c
+++ find4.18/find.c
@@ -987,0 +1082,78 @@ // ch1
...
+static enum SafeChdirStatus
+safely_chdir_nofollow(const char *dest,
...
+static enum SafeChdirStatus
+safely_chdir(const char *dest,
...
@@ -1368 +1641 @@ // ch2
- enum SafeChdirStatus status = safely_chdir
(name, TraversingDown, &stat_buf);
+ enum SafeChdirStatus status = safely_chdir
(name, TraversingDown, &stat_buf,
SymlinkHandleDefault);
Fig. 3. Correct code changes reported by AFTER on ﬁnd-a.
There is also a large body of work on identifying the root
cause of a manifestedfailure [3], [4], [5], [6] basedon seman-
tic analysis only. However, the problem with these methods
is that they focus only on the failures without considering
the code changes between two versions, and therefore do
not leverage the fact that the original version can serve as
a modelof the intendedprogrambehavior. Furthermore,some
existing methods rely on a test suite to provide sufﬁciently
manypassingandfailingtestruns,whichmaynotalwaysexist
in practice. Without a golden model or formal speciﬁcation,
the number of possible causes of a manifested failure tends
to be very large, since changing any part of the control or
data ﬂow along the faulty execution trace could lead to the
ﬂip of the assertion condition. Our new method, in contrast,
can mitigate the potential explosion of possible causes of a
manifested failure by restricting the analysis only to code
changes committed between the two versions.
Finally,ournewmethodcanguaranteethatthereisnotonly
correlation but also causality relation between the reported
codechangesandthemanifestedfailure.Wepresentthecauses
(causalchainsofevents)thatconnectthecodechangesandthe
failurein a tree-likestructureforease of comprehension.Each
event in the causal chain corresponds to a program statement
responsibleforpropagatingthefault.Suchexplanationismore
informativethana rankedlist ofwarningsreportedbyexisting
methods – it is the reason why we call our approach fault
explanation rather than fault localization.
We have implemented our method in a tool built upon
LLVM [7] and KLEE [8] and evaluated it on a set of
Linux applications such as ﬁnd,bc,make,gawk, anddiff.
Our experimental results show that the new method is both
accurate and efﬁcient in computing faulty code changes and
explainingthecausalityrelationbetweenthemandthefailures.
To summarize, this paper makes the following contributions:
•We propose a new synergistic analysis method for ex-
plaining failed regression testes by leveraging both se-
mantic and dynamic analysis in a uniﬁed framework.
•Weimplementthemethodinasoftwaretoolbasedonthe
LLVMcompilerandtheKLEEsymbolicvirtualmachine.
•We evaluate the new method on a set of large Linux
applicationsand demonstrateits effectivenessin practice.
The remainder of the paper is organized as follows. We
present the overall algorithm in Section II, which is then
258
258
258
258
ICSE 2015, Florence, Italyfollowed by a detailed description of each major component.
We illustrate our method using an example in Section III.
We discuss implementation details in Section IV. We present
the experimental results in Section V, review related work in
Section VI, and ﬁnally give our conclusions in Section VII.
II. THESYNERGISTIC ANALYSISMETHOD
Algorithm 1 shows the overall ﬂow of our method based
on three inputs: the correct program P, the faulty revision P′,
and the failure-inducing test input t. Let∆be the set of code
changes between PandP′,πbe the faulty execution trace,
andρ0be the failed assertion. The main part of the algorithm
is a loop with three steps: the semantic analysis, the dynamic
analysis, and the extraction of critical predicates. These three
steps are described as follows:
•In the semantic analysis, our goal is to compute a causal
chain of events explaining why the path πleads to the
critical predicate ρ. The result is a subset of executed
statements, denoted θ(called a cause), that forces ρto
become valid. We use Θto denote the accumulative set
of all causes computed by this analysis. The detailed
algorithm is presented in Section II-A.
•Inthe dynamicanalysis,ourgoalisto determinewhether
θis indeed the root cause. In the context of regression
testing, we assume that a root cause must be one that
involves some of the code changes committed between
PandP′. Giventhe codechangesidentiﬁedby semantic
analysis (/uniontext
θ∈Θ∩∆), we repeatedly execute P′with
different subsets of these code changes reverted, to see
if it can avoid the failure. The detailed algorithm is
presented in Section II-B.
•If the current iteration of semantic-dynamic analysis
fails to locate the root cause, we need to continue the
analysis in the upstream of the cause θ. Toward this end,
we identify the set Pθof critical predicates, which are
branching conditions along the faulty execution trace π
that determine whether the current cause θcan occur.
These critical predicates will in turn be used as seeds
for the next round of semantic-dynamic analysis. The
detailed algorithm is presented in Section II-C.
If the root cause is found, we return the code changes in
∆roottogether with the relevant subset of causes in Θ. The
reasonwhythisisabetterfailureexplanationresultisbecause,
in many cases, merely pointing out the faulty code changes is
not enough for the developers to understand how they lead to
the manifested failure. Therefore, we augment ∆rootwith the
relevantcausestoillustratetheircausalityrelationshipbetween
the code changes and the manifested failure.
A. Semantic Analysis
Given the faulty path πand the predicate ρ(the negated
assert condition), the objective of semantic analysis is to ﬁnd
out why ρholds in π. In other words, why the execution π
would not lead to ¬ρ(passing of the assertion).
Our semantic analysis is based on computing the weakest
preconditionof¬ρalong the faulty execution path. Following
thenotationofDijkstra[9],wedeﬁnetheweakestprecondition
of a predicate φwith respect to an instruction sas a function
mapping φto the formula wp(s,φ), such that wp(s,φ)is theAlgorithm1 Explain (Program P, Program P′, Test Input t)
1: Let∆be the set of code changes between PandP′;
2: Letπbe the faulty execution trace of P′under input t;
3: Letρ0be the ﬁrst critical predicate (failed assertion);
4: Initialization: predicate set P={ρ0}; cause set Θ =∅;
5:whileP/\e}atio\slash=∅do
6: Remove a predicate ρfromP;
7:θ←SemanticAnalysis ( π,ρ);
8:Θ = Θ∪θ;
9:∆root←DynamicAnalysis ( P′,/uniontext
θ∈Θ∩∆);
10:if∆root/\e}atio\slash=∅then
11:return∆roottogether with the relevant causes in Θ;
12:endif
13:Pθ←ExtractCriticalPredicates ( θ);
14:P=P∪Pθ;
15:endwhile
weakestcondition satisﬁed before executing sthat guarantees
ρto be satisﬁed after executing s. Formally, the weakest
precondition(WP) overan assignment,a branchingstatement,
and a sequence of instructions are deﬁned as follows:
•Assignment x:=expr: We deﬁne wp(x:=expr,φ) =
φ[x←expr]. That is, each appearance of xinφis
replaced by the right-hand-side expression expr.
•Branchif(c): We deﬁne wp(if(c),φ) = (c∧φ).
•Sequence of Instructions : We deﬁne wp(s1;s2,φ) =
wp(s1,wp(s2,φ)).
During dynamic analysis, assume that the faulty execution
trace isπ=/a\}bracketle{ts1...sn/a\}bracketri}htandρis the critical predicate that
holds at the end of π, our WP computation is an iterative
procedure wp(s1,...wp(sn,¬ρ)). Sinceπactually led to ρ,
the logical formula representing the intermediate result of
the WP computation must become false at some point. As
soon aswp(si,...wp(sn,¬ρ))becomes false, we stop the
WP computation, and invoke a satisﬁability modulo theory
(SMT) solver to compute the minimal unsatisﬁable (UNSAT)
core.Modern SMTsolverssuchas Yices [10] andZ3 [11] can
produce an UNSAT core for a unsatisﬁable logical formula,
which is a minimal subset of the conjunctive constraints that
is still unsatisﬁable. In the context of fault localization, the
UNSAT core succinctly explains why πcannot lead to¬ρ.
Unfortunately, there still is a gap because the UNSAT core
and the cause (causal chain of events) for critical predicate ρ
becausethe UNSATcoreitself doesnottelluswhichprogram
statements are responsible for generating the constraints in
the UNSAT core. Therefore, we need to map the UNSAT
corebacktotheoriginalprogramstatementsbyleveragingthe
so-called generator instructions . Given the UNSAT core, the
generator instructions of the UNSAT core are all the assign-
mentsthatparticipatein the creationof the manifestedfailure.
Intuitively, these statements collectively are responsible for
causing¬ρto become invalid at the end of the execution π.
Deﬁnition 1: Agenerator instruction of a predicate φis
either an assignment v:=exprsuch that vappears in the
transitive support of φ, or a branching condition where the
predicate φoriginally comes from.
Itisworthpointingoutthat,duringtheWPcomputation,an
if(c)conditioncanonlyaddanewconjunctiveconstraint (c)to
the existing formula, but not transform an existing predicate.
In the running example to be introduced in Section III, the
generator instructions of the predicate (sum= 4)at Line 13
259
259
259
259
ICSE 2015, Florence, ItalyAlgorithm2 DynamicAnalysisRecur (Program P′, Set∆, Sizen)
1:if|∆|< nthen
2: Execute the program P′with changes in ∆reverted;
3:return∆if the execution passes
4: and∅otherwise;
5:endif
6: Partition ∆intonsubsets:∆1,...,∆n;
7:foreach subset ∆ido
8:ifthe execution of P′with∆ireverted passes then
9:returnDynamicAnalysisRecur (P′,∆i,2);
10:elseifthe execution of P′with(∆\∆i)reverted passes then
11:returnDynamicAnalysisRecur (P′,(∆\∆i),2);
12:endif
13:endfor
14:returnDynamicAnalysisRecur (P′,∆,2n);
are{s2,s3,s7,s10,s13}, the bold line numbers in Fig. 7. We
callthesestatementscollectivelya causeastheyforma causal
chain of events that eventually triggers the failure.
However, the ﬁrst cause ( θ) computed from the failed
assertion ρ0may not be the root cause. In regression testing,
ifθis not triggered by some of the code changes committed
between the correct program Pand the buggy version P′,
we would not consider θas a root cause. The assumption is
that the failure manifested in P′but not in Pis triggered by
some of the recent code changes. In general, it is possible
for the root cause to trigger the manifested failure indirectly,
by transitively affecting the direct cause θcomputed from the
failedassertion ρ0.Therefore,followingthesemanticanalysis,
we shall apply the dynamic analysis to determine whether θ
is a root cause, and if the answer is no, we shall identify the
new starting points for the subsequent semantic analysis.
B. Dynamic Analysis
After the semantics analysis in Section II-A produces a
new cause θand updates the set of causes Θ, the objective
of dynamic analysis is to determine which cause, together
with the related code changes, is the root cause. The idea,
which is similar to the trial-and-error approach used in delta
debugging, is to execute the buggy program P′with various
combination of code changes reverted, to see if the execution
passes or fails.
Given the set of causes Θdiscovered so far, Algorithm 2
presents our approach to ﬁnding the smallest set of changes,
suchthattheexecutionof P′withthechangesrevertedpasses.
The algorithm is recursive with an initial set of changes being
∆Θ=/uniontext
θ∈Θ∩∆, i.e., any change that appears in at least
one cause, and the value n= 2. In other words, the call
DynamicAnalysis( P′,∆)in Algorithm 1 is implemented as
DynamicAnalysisRecur( P′,∆,2).
Speciﬁcally, at Line 6, the set ∆of changes is ﬁrst par-
titioned into nsubsets. Then, for each subset ∆i, we ﬁrst
executeP′with∆ireverted.Iftheexecutionpasses,wewould
like to ﬁnd out if an execution with a reverted subset of ∆i
can still pass, thus we invoke the recursive call at Line 9. If
theexecutionwithreverted ∆ifails,wetryitscomplementset
(∆\∆i)at Lines 10 and 11. If no subset or its complement
can make an execution pass, we double the value of nso
that biggersubsets can be tested. Eventually nbecomeslarger
than the size of ∆itself and it becomes the base case of the
recursive function. In Lines 1 to 5, if the number of changes1intx;
2inty;//c1: int m;
...
3 x=...;
4 y=...; //c2: m=...;
...
5 z=x+y+2; //c3: z=x+m+3;
6 assert(z=10)
Fig.4. Codesnippet cannot becom-
piled after reverting only c3.1inta=2, b=1, c=1, d=0;
2if(a>0) {
3if(b<0)
4if(c!=2)
5 c=2; //end if\@L4
6 d=c+3; //end if\@L3
} //end if\@L2
7 assert(d==5);
Fig. 5. An example for computing
the critical conditions.
underconsiderationis less than n, the dynamicanalysis either
returnsallthechangesiftheexecutionwithreverted ∆passes,
or returns∅if the execution still fails.
Algorithm 2 differs from the approach used in delta debug-
ging in that it is asymmetric – we consider passing executions
only.Asymmetric approachwouldbetoconsiderbothpassing
and failing executions: if an execution with reverted change
cpasses,cis fault related; but if an execution with reverted
changecstill fails, cis irrelevant to the failure. Although
this approachhas been employedin many priorworkssuch as
deltadebugging,itmayleadtosomefaultycodechangestobe
missed, especially whenthe failure is caused by multiplecode
changes. Consider the program in Fig. 6 as an example. The
buggyprogram P′canbeexecutedwithoutfailureonlyifboth
changesc2andc3are reverted. However, since a re-run with
c2reverted fails, a symmetric approach would wrongly claim
thatc2is irrelevant. In contrast, our asymmetric approach
would not exclude such faulty code changes.
Finally, it is worth pointing out our method presented in
Algorithm2maynotobtaintheoptimalsolution.Itisdesigned
in this way to improve the runtime performance in practice,
because re-runs can be expensive, especially when the faulty
code changes are far away from the failure in terms of the
controlﬂow distance. Therefore, in practice, we need to make
atrade-offbetweentheaccuracyofthecomputationresultand
the runtime overhead.
C. Critical Predicate Identiﬁcation
The dynamic analysis presented in section II-B may not
discover the set of code changes responsible for the failure in
one shot. In such case, the current cause merely propagates
the fault instead of causing the failure. Therefore, we need to
examine the upstream of this cause θalong the faulty path π.
Recall that in Section II-A, the generator instructions
identiﬁed from a cause mandate the validity of a critical
predicate (which initially is the failed assertion). If we make
changes to at least one generator instruction, the predicates
can be evaluated differently. Given a generator instruction
s:v:=expr, there are two ways to change s. One is to
change the conditional statement that sdepends on so that s
may not be executed.The other one is to change the values of
the variables in expsovcan be evaluated differently. Based
on this observation,we now deﬁne the critical predicateswith
respect to a generator instruction s.
Deﬁnition 2: The predicate in a branching statement bis
called acritical predicate if it has potential impact on a
generator instruction tby:
260
260
260
260
ICSE 2015, Florence, Italy•directinﬂuence b/squigglerightt:bimmediatelydetermineswhether
twill be executed; or
•indirect inﬂuence b/curlyrightt:bdetermines whether an
unexecuted statement swill be executed, and sin turn
redeﬁnes a variable read by t.
Our use of indirect inﬂuence in the above deﬁnition is sim-
ilar to the potential dependence used in relevance slicing [12].
Based on the deﬁnition, the set of critical predicates of Pθin
a causeθisPθ=/uniontext
t∈θ{b|(b/squigglerightt)∨(b/curlyrightt)}, wheretis a
generator instruction.
Consider the program in Fig. 5 as an example. It has
an assertion failure along the execution path /a\}bracketle{t1,2,3,6,7/a\}bracketri}ht,
because the value of dis 4, not 5, at Line 7. Using the
algorithm presented in Section II-A, we can obtain the ﬁrst
causeθ0={1,6,7}. Assuming θ0is not the root cause, now
we need to look for the critical predicates with respect to the
generator instruction at Line 6.
Based on Deﬁnition 2, the predicate at Line 2 is critical
sinceitcontrolswhetherLine6willbeexecuted.Thepredicate
atLine3isalsocritical:wereitevaluatedto true,thestatement
at Line 5 would have been executed and cwould have been
redeﬁned. Note that we only consider the executed predicates.
The predicate at Line 4 is not considered critical because it is
not executed during the test. The critical predicates serve as
new starting points for the subsequent semantics analysis, as
indicated in Algorithm 1.
Algorithm3 ExtractCriticalPredicates( Cause θ)
1:foreachsl∈θdo
2: let sjbe the closest enclosing branch of slnot post-dominated by sl;
3:Pθ.add(sj); // direct inﬂuence
4:foreach variable varused bysldo
5: let sbe the last instruction that deﬁnes varbeforesl;
6:forevery branch sjbetweensandsldo
7:ifindirectInﬂuence (sj,var)then
8:Pθ.add(sj); // indirect inﬂuence
9:endif
10:endfor
11:endfor
12:endfor
The pseudo code for computing the critical predicates is
shown in Algorithm 3, which follows Deﬁnition 2 with the
following modiﬁcation. For each generator instruction sl∈θ,
we choose only the immediate preceding instance of slthat
is not post-dominatedby sl(Lines 2-3), because other critical
predicates will be computed during the subsequent iterations
of our analysis. Therefore, no root cause will be missed due
to this simpliﬁcation.
In Algorithm 3, Lines 4-11 deal with the indirect inﬂuence .
Here,sdenotes the last instruction found in θthat assigns
a value to the variable var. Only direct inﬂuence in a preﬁx
denoted{s1,...,sx}can affect the causes generated in sufﬁx
{sx,...,sn}. Ifsisthe laststatementbefore slthatassignsthe
variablevarused bysl, then during the backward analysis,
sandslrepresent the upper and lower bound of indirect
inﬂuence, respectively. For each varused by sl, the loop
at Line 6 identiﬁes every branch instance sjbetweensand
sl, which can indirectly affect the value of varas a critical
predicate. An example of such instance is the one at Line 3
in Fig. 5.1 bool sorted = True;
2voidf(intx,inty,intz){
3intsum = 0;
4if(!sorted) {
5if(x > y)
6 sum += x;
else
7 sum += y;
}else
8 sum += x;
9if(z > 0)
10 sum += z;
else
11 sum += (0-z);
12 printf("sum=%d\n",sum);
13 assert(sum == 4);
}1 bool sorted = False; //c1
2voidf(intx,inty,intz){
3intsum = 0;
4if(!sorted) {
5if(x < y) //c2
6 sum += x;
else
7 sum += y;
}else
8 sum += x;
9if(z > 0)
10 sum += (0-z); //c3
else
11 sum += z; //c4
12 printf("sum=%d\n",sum);
13 assert(sum == 4);
}
Fig. 6. Correct and buggy programs, with four code changes c1,c2,c3,c4
and failure-inducing test input {x= 3,y= 2,z= 1}.
StepLine Num. Weakest Precondition (WP) Satisﬁability
113 sum=4 SAT
210 sum-z=4 SAT
3 9 z >0∧sum−z= 4 SAT
47 z >0∧sum+y-z=4 SAT
5 5 x≥y∧z >0∧sum+y−z= 4 SAT
6 4¬sorted∧x≥y∧z >0∧sum+y−z= 4 SAT
73 ¬sorted∧x≥y∧z >0∧y-z=4 SAT
82 ¬sorted∧3≥2∧1>0∧2-1=4 UNSAT
Fig. 7. Applying our semantic analysis to the example in Fig. 6, with the
faulty execution trace and the critical predicate (sum/\e}atio\slash= 4).
III. THERUNNINGEXAMPLE AND COMPARISON TO
EXISTINGMETHODS
In this section, we use an example to further illustrate the
three steps of our new method. We also compare our method
with existing approaches to show that it can return better
failure explanations in regression testing.
Fig. 6 shows, side by side, a correct program (left) that
computes max(x,y)+|z|andabuggyrevisionoftheprogram
(right) with four code changes, denoted c1,c2,c3andc4,
respectively. In this example, the variable sortedindicates
whether the three input variables have been sorted in the
descendingorderand thevariable sumstoresthe computation
result. Due to the code changes at Lines 1, 5, 10, and 11,
executing the revised program under the test input /a\}bracketle{t3,2,1/a\}bracketri}ht
(forx,y,z) leads to a wrong result (sum= 1)instead of
(sum= 4). The actual code changes responsible for this
failureare c2andc3. However,existing methodssuch as delta
debugging may either report redundant code changes or miss
the faulty code changes. Our new method, in contrast, can
identify the faulty code changes precisely as well as explain
why they are responsible for the failure.
A. Applying Our New Method
Our method starts by replaying the failed test case /a\}bracketle{t3,2,1/a\}bracketri}ht
on the buggy program (right). Based on the failed execution
traceπ=/a\}bracketle{ts1,s2,s3,s4,s5,s7,s9,s10,s12,s13/a\}bracketri}ht, our method
performs the ﬁrst semantic analysis to identify the causeof
the failed assert condition (sum/\e}atio\slash= 4). The cause returned
by the semantic analysis is a minimal set of events (a causal
chain) linking some code changes to the failed assertion.
Speciﬁcally, we negate the initial predicate (sum/\e}atio\slash= 4)at
Line 13 and compute the weakest precondition of (sum= 4)
261
261
261
261
ICSE 2015, Florence, Italyθ1:{2,5(c2)} θ2:{2, 9}
θ0:{2, 3, 7,10(c3), 13}
Fig. 8. Our tree-likestructure for explaining the cause of the failure.
along the erroneous execution trace backwardly. Since the
execution led to (sum/\e}atio\slash= 4), the weakest precondition of
(sum= 4)is guaranteed to become falseat some point
during the backward traversal. Fig. 7 shows the steps of this
computation, where the WP becomes an unsatisﬁable formula
at Line 2 after 8 steps, and the UNSAT core is shown as
follows:
(x= 3)∧(y= 2)∧(z= 1)∧(sum0= 0)∧(sum1=sum0+y)
∧(sum2=sum1+0−z) =⇒(sum2/\e}atio\slash= 4)
(1)
For ease of comprehension, we have used the static single
assignment (SSA) form in the above formula to differentiate
multiple occurrences of sum.
Next, we map the constraints in this unsatisﬁable subfor-
mula (UNSAT core) back to the program statements that
produce them (generator instructions).
We get a causal chain of events (or a cause): φ0=
{s2,s3,s7,s10,s13}, which explains why the assert condition
(sum= 4)failed. Constraints that are notin the UNSAT core
are deemed as irrelevant.
However, a cause returned by the ﬁrst semantic analysis
may not be the rootcause of the failure. In the subsequent
dynamic analysis , we check if it is root cause by inspecting
the code changes committed between the correction program
Pandthebuggyrevision P′.Sincec3istheonlycodechange
inφ0, to decide if φ0is the root cause, we revert the change
c3,re-compile,andre-executetheprogram.Sincetheassertion
still fails after c3is reverted, we conclude that φ0is not the
rootcause – otherwise, reverting c3would have ﬁxed the bug.
Unlike existing methods such as delta debugging, our use of
thistrial-and-error style dynamic analysis is guided by the
cause computed by the preceding semantic analysis.
Sinceφ0is not the root cause but a link between the root
causeandthefailure,weneedtoanalyzethechainofeventsin
φ0to identify other critical predicates. A critical predicate is
a branching condition whose value determines whether events
inφ0can occur duringthe execution.In this example,the two
critical predicates come from s5ands9, respectively, since
they determine whether assignments at Lines 7 and 10 can
be executed. These critical predicates are new starting points
for the next round of semantic analysis based on the weakest
predication computation.
The two new causes returned by the subsequent semantic
analysis are θ1={s2,s5}andθ2={s2,s9}, the ﬁrst of
which is triggered by the code change c2. Furthermore, both
changesc2andc3are included in the accumulative set Θof
discovered causes. A subsequent dynamic analysis conﬁrmed
that reverting both c2andc3would make the failure go away.
Therefore, θ1is the root cause. In contrast, θ2={s2,s9}is
irrelevant.Stepc1 c2 c3 c4 P/FMaxPass Min Fail Diff
0P{} {c1, c2, c3, c4} {c1, c2, c3, c4} √ √ √ √F
1√ √F{} { c3, c4} { c3, c4}
2√P{c4} { c3, c4}{c3}
Fig. 9. Steps of applying delta debugging to the example in Fig. 6.
To report the code changes responsible for the failure, we
presentc2andc3, as well as the causal chains of events
(causes),ina tree-likestructureshownin Fig. 8.Inthisﬁgure,
nodes are the code changes responsible for triggering the
manifested failure and the causal chains of events, whereas
edges are the critical predicates linking the causal chains
together. Speciﬁcally, the result in Fig. 8 shows that the cause
θ1, which includes the change c2, leads the incorrectoutcome
at Line 5, and the cause θ0propagates the effect of c2to the
failure at Line 13, which includes the code change c3.
B. Comparing to Other Methods
The reason why our method is more robust than existing
methods is because of its use of semantic analysis to guide
dynamic analysis, and vice versa. Therefore, our method can
identifynotonlythe correlation butalso the causality relation
between the faulty code changes and the manifested failure.
To illustrate this advantage, we apply some of the existing
methods to the example in Fig. 6 and compare the results.
Fig. 9 shows the results of applying delta debugging
(DD) [1] to the running example. In Columns c1−c4, the
symbol√means that a code change is applied to the correct
versionPand−indicates that the change is omitted. There-
fore,−−−−represents the previously correct program Pand√√√√representsthe buggyprogram P′.Column P/Fshows
whether the execution passed without failure or failed. Col-
umnMaxPassshows the maximal set of changes applied to
Pwhile the execution still passes, whereas Column MinFail
shows the minimal set of changes applied to Pwhile the
execution still fails. Delta debugging starts with the correct
program (−−−−) and the faulty program (√√√√), for which
MaxPassis empty whereas MinFailis the complete set of
changes.Thegoalistoiterativelyreduce MinFailandenlarge
MaxPasssuch that the difference, shown in Column Diff, is
minimized. When Diffcan not be reduced further, it contains
the explanation for the failure.
Initially, the set of changes is partitioned into subsets
{c1,c2}and{c3,c4}. Since applying{c3,c4}to the correct
program Pcauses the execution to fail, delta debugging
assumesthatthe faultychangesare inside {c3,c4}.Therefore,
it decreases MinFailfrom{c1,c2,c3,c4}to{c3,c4}and
partitions{c3,c4}into{c3}and{c4}. Since applying{c4}
to program Pavoids the failure, c4is added to MaxPass.
Delta debugging terminates after this step as {c3}cannot
be partitioned any further. Therefore, Diff={c3}is reported
as the explanation. However, this is not the correct result
because if we keep the changesof c1,c2andc4in the revised
version, and only revert c3, the execution still leads to the
assertion failure. Therefore, the code changes localized by
Delta Debugging is not accurate in this example.
262
262
262
262
ICSE 2015, Florence, ItalyBesides delta debugging, there are also fault localization
methods based on dynamic slicing [13] and symbolic tech-
niques such as DARWIN [14], [15]. Our method is also
moreaccuratethanthesemethods.Dynamicslicing eliminates
programstatementsthatareirrelevanttothemanifestedfailure
based on computing the data- and control-dependence. It is a
popular technique because of its low cost, but unfortunately,
is not accurate [16]. In the running example, dynamic slicing
wouldnotbeabletopruneawayanyoftheprogramstatements
in the faulty trace πbecause the failed assertion transitively
depends on all the statements.
DARWIN mayproducea betterresultthandynamicslicing,
buttheresultisstillinferiortotheonereturnedbyourmethod
because DARWIN does not apply semantic and dynamic
analysissynergistically.Speciﬁcally,DARWINtriestoexplain
failurebycomparingthe weakestpreconditionofthe assertion
along the execution paths in the correct program and its
faulty version. Applied to the running example, it would
generatetheweakestprecondition WP1inpassingexecutionas
sorted∧(z >0)∧(x+z= 4), and the weakest precondition WP2
in failing execution as ¬sorted∧(x≥y)∧(z >0)∧(y−z= 4).
Since all conditions in WP1 (WP2) areunexplained byWP2
(WP1), except for (z >0), DARWIN would report almost all
the executed statements in both the passing and the failing
executionsasfailure-related.Asaresult,thedeveloperswould
have to sift through the irrelevant code changes and program
statements in order to understandthe root cause of the failure.
Compared with [14], [15], our method exploits the use of
UNSAT cores to prune away redundant predicates, and stops
the backward exploration as soon as error-inducing code
changes are identiﬁed. In addition, our method presents a tree
based structure to illustrate the fault propogation.
IV. COMPUTING AUXILIARY CODECHANGES
Besides passing or failing, an execution of the program P′
with some reverted code changes may have a third possibility
– theprogrammaynotbe compiledsuccessfully.Considerthe
example in Fig. 4. When only the change c2is reverted, the
resulting program cannot be compiled because the variable m
has not been declared before its usage. During our study of
real-world regression test examples, we have found that such
cases are common in practice and it is time-consuming for
the developersto identify such code changesmanually.In this
section, we present a solution to this problem.
The problemwe wantto solveis formallystated asfollows.
AssumeP′can no longer be compiled with the code changes
in∆−reverted,our goalin Algorithm 4 is to ﬁnd an augment
set of code changes, denoted ∆+, such that if ∆+is reverted
together with ∆−, the resulting program can be compiled.
Whilecomputing ∆+,weensurethat ∆−remainsthesetof
codechangesthathavecausedthecompilationerrorintheﬁrst
place. In contrast, we decrease the set ∆+monotonicallydur-
ingtherecursiveapplicationofthefunction FindAuxChange() .
Initially the valueof ∆+is(∆\∆−), the set of allchangesin
P′except∆−. Since(∆−∪∆+)is the same as ∆, reverting
these changes in P′leads to P, which can be compiled
successfully. The goal of each recursive call is thus trying
to ﬁnd a subset of ∆+that can still work togetherwith ∆−to
make the program compile. This is achieved by partitioningAlgorithm4 FindAuxChange (Set ∆−, Set∆+, Sizen)
1: assert(Program P′with(∆−∪∆+)reverted can be compiled);
2:if|∆+|< nthen
3:return∆+;
4:endif
5: Partition ∆+into n subsets: ∆1,...,∆n;
6:foreach∆ido
7:ifProgram P′with(∆−∪∆i)reverted can be compiled then
8:returnFindAuxChange (∆−,∆i,n);
9:elseifP′with∆−∪(∆+\∆i)reverted can be compiled then
10:returnFindAuxChange (∆−,(∆+\∆i),n);
11:endif
12:endfor
13:returnFindAuxChange (∆−,∆+,2n);
--- FileA
+++ FileB
@@ -BeginA,SpanA +BeginB,SpanB @@
- Line_A_1
- ...
- Line_A_SpanA
+ Line_B_1
+ ...
+ Line_B_SpanB
Fig. 10. The template of the uniﬁed code changes.
∆+and trying out each subset. The invariantthat (∆−∪∆+)
solves the compilation problem is always maintained.We note
that all changes in the program P′have to be considered, not
just the changes appearing in the faulty path π.
To improve the performance, we rely on the program
structure to partition the change set. For example, a change
often grammatically depends on the changes within the
same ﬁle or function, so we partition accordingly. Further-
more, in Algorithm 4, we cache the results of FindAux-
Changeto prevent redundant computation. For example, after
∆+
1=FindAuxChange (∆−
1,∆\∆−
1,2)is computed, we need
to ﬁnd an auxiliary set for ∆−
2⊂∆−
1. In this case, we
invokeFindAuxChange (∆−
2,∆+
1,2)becausetheprogramwith
reverted(∆−
2∪∆+
1)can always be successfully compiled.
The set of all code changes in the program is computed
using the Linux utility application diff(which happens to be
a benchmark application used in our experimental evaluation
of the new method), which is used with the option uto
compute the difference between two versions. Fig. 10 shows
the template of such comparisons. It starts with two lines of
thetwo ﬁle namesundercomparison(the timepartis omitted)
and then describes the change hunks.Each change hunk starts
withaline "@@ -BeginA,SpanA +BeginB,SpanB @@" that
speciﬁes the starting and ending line numbers of the changes
between the two ﬁles. Following the ranges are the detailed
differences.That is, Lines BeginA∼BeginA+SpanA-1 from
FileAis replaced by Lines BeginB∼BeginB+SpanB-1
fromFileB. During the implementation, we have chosen the
smallest granularity possible because the size of the change
hunks may affect the precision of the subsequent analysis.
V. EXPERIMENTS
We have implemented our method in a tool based on the
LLVM compiler [7] and the KLEE symbolic virtual ma-
chine [8]. The tool, called AFTER (Automated FaulT Expla-
nation for Regression testing), can handle C/C++ applications
263
263
263
263
ICSE 2015, Florence, ItalyTABLE I
CHARACTERISTICSOF THE BENCHMARK APPLICATIONS USED INOUREXPERIMENTS .
NameLoCCorrect ( P)Buggy(P′)#ChangeFailure Description Reported Site
ﬁnd-a 24kV4.2.15 V4.2.18 71Using -L/-H produces wrong output http://savannah.gnu.org/bugs/?12181
ﬁnd-b 40kV4.3.5 V4.3.6 243 Using -mtime produces wrong output http://savannah.gnu.org/bugs/?20005
ﬁnd-c 40kV4.3.5 V4.3.6 243 Using -size produces error message http://savannah.gnu.org/bugs/?30180
bc 10kV1.05a V1.06 534 Argument processing error https://bugs.gentoo.org/show bug.cgi?id=51525
make 23kV3.80 V3.81 1,257 Using -r produces wrong output http://savannah.gnu.org/bugs/?20006
gawk 37kV3.1.0 V3.1.1 897 Use of strtonum causes abort https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=159279
diff 20kV2.8.1 V2.9 373 Adds additional newline https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=577832
that work on the LLVM/KLEE platform. We use the Yices
SMT solver [10] to implement the computation of UNSAT
cores. During our experiments, we have evaluated the failure
explanation capability of our method and compared it with
two existing techniques: the classic delta debugging (DD) [1]
and a recent improvement called augmented delta debugging
(ADD) [2] in the same tool.
Our experimental evaluation was designed to answer the
following research questions:
1) How accurately can our new method localize the set
of code changes responsible for the manifested failure?
Speciﬁcally,we wanttoknow(a)whetherthefaultycode
changes will be missed, and if the answer is no, then (b)
whether the faulty code changes will be buried in a large
number of irrelevant code changes.
2) How scalable is our new method in handling real ap-
plications? Compared to existing methods such as DD
and ADD, which use dynamic analysis but not semantic
analysis, our use of the SMT solver may bring some
overhead but may also reduce the number of redundant
tests. We want to know if our method has a good overall
runtime performance.
We conducted experiments on seven widely used Linux
applications such as find,bc,make,gawk, anddiff. Each
application has tens of thousands of lines of C code and
hundreds of code changes committed between the correct and
buggy versions. Table I shows their characteristics, including
the name, the number of lines of code, the versions of the
correctandbuggyprograms,adescriptionoftheerror,andthe
website on which the bug was reported. By studying the bug
reports and patches proposed by the developers, we identiﬁed
the minimal set of faulty code changes responsible for each
failureandunderstoodhowtheytriggeredthefailure.Then,we
ran our tool on these benchmarksand compared the results of
the following three methods: DD, ADD, and AFTER. All the
experiments were conducted on a computer with a 2.66GHz
Intel dual core CPU and 4 GB RAM.
A. Accuracy of the Failure Explanation
In computer aided debugging, we generally expect the
analysis tool to provide only hints as to where the faulty code
changes are and then rely on the developers to identify the
root cause from the reported changes. Therefore, the quality
of a fault explanation method is evaluated using the following
criteria: (a) whether the faulty code changes are included in
the set of reported changes, and if the answer to the previous
question is yes, then (b) whether the faulty code changes are
buried in a large number of irrelevant ones.To compare the performance of different methods, for each
benchmark and each method, we classiﬁed the result into one
of the following three categories:
•Matched, meaning that the reported code changes
matched the actual bug ﬁxes provided by the developers.
Inthiscase,thedevelopersproposedtoeitherrevertthese
code changes or revise them in order to ﬁx the reported
bug.
•Missed, meaning that reverting the code changes would
not avoid the failure, or merely dodge it since a repaired
program simply chose a different branch and could no
longer reach the buggy code. In this case, the result is
not helpful to debugging.
•Partial, meaning that reverting the code changes would
make the failure disappear, but the developers have de-
cided that these are necessary changes. Instead, other
parts of the code should be revised to accommo-
date these changes. For example, in the code snip-
peta=2;b=3;assert(a+b==5); changingb=3tob=2
would cause an assertion failure. Although b=2is the
actualrootcauseofthisfailure,thedevelopermaydecide
that the ﬁx should be to revise a=2toa=3.
Table II shows the results of comparing the code changes
returned by the three methods. Columns 1 and 2 show the
name of the benchmark and the total number of code changes
between the correct and buggy versions. Column 3 shows
the number of code changes localized by DD. Column 4
shows whether the root cause is included in the reported ∆.
Columns 5-6 show the result of ADD and Columns 7-8 show
the result of AFTER in the same format. Column 9 shows
the actually faulty codechangesfor each benchmarkprogram,
obtainedbyourinspectionofthesoftwarecodeandcomments
from the developers.
The results in Table II indicate that our method is more
accurate in localizing the faulty code changes. In all cases,
the changes localized by AFTER include the actual bug ﬁxes
provided by the developers. In contrast, ADD missed the
actual bug in ﬁnd-a, and both DD and ADD reported partial
results on diff. Furthermore, the false positives of AFTER are
signiﬁcantly fewer than the other two methods. On average,
among the 516.8 code changes between the two versions, our
method will report only 2.4 code changes, among which 1.7
are the actual faulty code changes.
B. Comparing the Runtime Performance
Our new method relies on a synergistic analysis framework
that leverages both the trial-and-error style dynamic analysis
264
264
264
264
ICSE 2015, Florence, ItalyTABLE II
RELEVANT CODECHANGESCOMPUTED BY DD,ADD, ANDAFTER.
Name#ChangesDD[1] ADD[2] AFTER Actual
∆Match ∆Match ∆Match ∆
ﬁnd-a 71 10 yes 1missed 2 yes 2
ﬁnd-b 243 108 yes 6 yes 5 yes 2
ﬁnd-c 243 2 yes 2 yes 1 yes 1
bc 534 1 yes 1 yes 1 yes 1
make 1,257 129 yes 63 yes 6 yes 4
gawk 897 1 yes 1 yes 1 yes 1
diff 373 1partial 1partial 1 yes 1
Avg.516.836.0 –10.7 –2.4–1.7
TABLE III
COMPARING THE RUNTIMEPERFORMANCEOF DD, ADD, ANDAFTER.
NameDD[1] ADD[2] AFTER Speedup
#Test time(s) #Test time(s) #Test time(s) #S1 #S2
ﬁnd-a 158 82 34 17 264 125 0.7X 0.1X
ﬁnd-b1,161 1,199 35 61 223 321 3.7X 0.2X
ﬁnd-c 30 37 6 10 32 39 0.9X 0.3X
bc 486 287 29 25 1 12 23.9X 2.1X
make2,368 7,640 526 1,833 257 946 8.1X 1.9X
gawk 638 4,112 7 73 1 5 822.4X 14.6X
diff 376 522 35 45 3 31 16.8X 1.5X
Avg.– 1,983 – 295 – 211 9.4X 1.4X
(which is similar to DD and ADD) and the SMT solver based
semantic analysis. Therefore, a natural question is whether
the use of the SMT solver would slow down the method. To
answer this question, we compared the runtime performance
of the three methods. Table III shows the result. Columns 2-
7 compare the number of test runs explored by the dynamic
analysis component of each method, and the total execution
time. Columns 8 and 9 show the speedup of our new method,
AFTER, over the other two methods. Speciﬁcally, we deﬁne
#S1 =#TimeDD#TimeAFTERand#S2 =#TimeADD
#TimeAFTER.
The result in Table III shows that although AFTER spends
additional time on the semantic analysis, whereas the other
two methods do not, the runtime overhead often is more
than compensated by the smaller number of test runs needed.
On average, our method is 9.4 times faster than DD and
1.4 times faster than ADD. The use of semantic analysis
can drastically reduce the number of re-executions needed by
dynamic analysis. This is especially important for programs
where the code changes are far away from the manifested
failures, for which re-execution takes a long time.
C. Statistics of our Synergistic Analysis
Recall that our method has two additional features that
DD and ADD do not have. The ﬁrst one is the capability
of computing ∆Aux, the set of code changes that must be
reverted together with ∆rootto make it compile. The second
one is the capability of reporting a tree of causal event chains
to explain how the faulty code changes lead to the failure.
Table IV shows the statistics of running our method on the
benchmarkprograms.Webreakdown ∆Total,thetotalnumber
of code changes reported by our tool, into two parts. That
is,∆Total= ∆Aux+ ∆Root, where∆Auxis the number of
addition changes that must be reverted to make the program
compile, and ∆Rootis the set of changes responsible for
the failure. One main advantage of our method over existing
methodsis the capability of computing ∆Aux. We also report,TABLE IV
STATISTICS OF RUNNINGAFTER ON THEBENCHMARK PROGRAMS .
Name#ChangesAFTER
∆Total ∆Aux ∆RootSMT-time(s) #causes
ﬁnd-a 71 16 14 2 5 2
ﬁnd-b 243 5 0 5 72 6
ﬁnd-c 243 2 1 1 3 1
bc 534 1 0 1 2 1
make 1,257 31 25 6 135 7
gawk 897 1 0 1 1 1
diff 373 1 0 1 27 5
Avg.516.8 8.1 5.7 2.4 35.0 3
--- diffutils-2.8/src/io.c
+++ diffutils-2.9.1/src/io.c
@@ -664,2 +650,2 @@
- for (; p0 != beg0; p0--, p1--)
- if (*p0 !=*p1)
+ while (p0 != beg0)
+ if (*--p0 != *--p1)
Fig.11. Thecodechange ofprogram diffreportedbyDD,ADD,andAFTER.
among the total execution time, how many seconds are spent
on running the SMT solver based semantic analysis.
The last column shows the number of causes (causal
chains of events) that link the faulty code changes to the
manifested failure. The causes computed by our SMT solver
basedanalysiscanhelpthedevelopersunderstandthecausality
relationship between the code changes and the failure – it is
the main reason why we call our approach fault explanation
instead of fault localization.
Besides establishing the causality relationship, the reported
tree of causes can providehints to programmerson how to ﬁx
the faulty program. Consider the diffbenchmark program,
where all three methods reported the code change in Fig. 11.
However, the result reported by AFTER is a matchwhereas
the results reported by the other two methods are partial, for
the following reasons. First, reverting the change in Fig. 11
can indeed make the failure go away. However, based on
the comments from the developers, this change itself was not
faultysinceitwasintroducedtoﬁxabugappearedinanearlier
version (http://git.savannah.gnu.org/cgit/diffutils.git/commit/?
id=58d0483b621792959a485876aee05d799b6470de), and the
bug eventually was ﬁxed by adding another condition to
catch and correct the affected variable. Our method correctly
explainedthisfailurebecause,inadditiontothisfaultychange,
our method also reported 5 causes, which formed a chain
of propagation. The actual bug ﬁx proposed by the devel-
opers was on our causality chain (https://bugs.debian.org/cgi-
bin/bugreport.cgi?bug=577832).
Consider another example, make, which is the benchmark
application on which DD and ADD reported signiﬁcantly
more code changes than AFTER. For this example, the bug
ﬁx provided by the developers is to remove the check of
f→is_target shown in Fig. 12. Although this change is
also reported by by DD and ADD – which earns them a
match– this faulty code change is buried among 129 and 63
irrelevant changes, respectively. In practice, it would be too
time-consuming for the developers to sift through such large
numbers of potential code changes. In contrast, our method
265
265
265
265
ICSE 2015, Florence, Italy--- make-3.80/implicit.c
+++ make-3.81/implicit.c
@@ -402 +698,2 @@
- if (lookup_file (p) != 0
+ /*@@ dep->changed check is disabled. */
+ if (((f = lookup_file (name)) != 0
&& f->is_target)
Fig. 12. Actual bug ﬁx reported by AFTER for the makebenchmark.
reported only 6 code changes, together with 25 auxiliary code
changes to make the program compile successfully.
VI. RELATEDWORK
There is a large body of work on debugging evolving pro-
grams based on the trial-and-error style analysis as pioneered
by Zeller et al.[1], [17], [18]. The approach, commonly
known as delta debugging (DD) , has been combined with
other techniques including execution coverage (also known as
theaugmenteddelta debugging(ADD) [2] ),observation-based
slicing[19],dualslicing[20]andhierarchicalinformation[21]
toimproveprecision.However,thesemethodsrelyondynamic
analysis only, whereas in our synergistic analysis framework,
wealsouseSMTbasedsemanticanalysistocomputecausality
chains and provide guidance to the dynamic analysis.
Another line of inﬂuential work on debugging evolving
programs is based on symbolic techniques. For example, the
DARWIN [14] method relies on the assumption that the
path conditions of buggy and correct executions often differ
from each other. By comparing the difference, DARWIN is
effective in ﬁnding control logic errors. However, it is less
effective in ﬁnding errors in other parts of the code such
as the assignments, since they do not alter the control ﬂow.
Banerjee et al. [15] proposesa remedialmethodfor DARWIN
by comparing the erroneous instructions against a golden
program. However, neither method is based on the synergistic
application of both dynamic analysis and semantic analysis.
Furthermore, neither provides the tree-like explanation of the
fault propagation as in our method.
The concept of as correct as the previous version has
become popular in the subﬁeld of regression veriﬁcation [22],
[23].Althoughtheseworksdonotdirectlyfocusonexplaining
failed regression tests as in this paper, they complement our
work at the high level.
Dynamic slicing [13], together with many variations [12],
[24],isanotherwidelyusederrortriagingtechnique.However,
it may not be able to remove many of the semantically irrele-
vant program statements, thereby limiting its usefulness [16].
Therefore, in practice, slicing typically is used as an auxiliary
technique to complement other methods in automated debug-
ging.Forexample,theproblemtackledinthispapercannotbe
solved by simply combining dynamic slicing with a weakest
precondition computation over the backward slice – the use
of re-execution based analysis is also crucial to identify the
actual causality relationship between the recent code changes
and the observed failure.
BugAssist[25]isaBooleanSATsolverbasedtoolforlocal-
izing potentially faulty program statements in a buggy C pro-
grams.ThetoolleveragesaSATsolver’scapabilitytocompute
minimal unsatisﬁability core in the CBMC veriﬁcation tool.Ermiset al.[26], [27] propose a Graig interpolant [28] based
method for computing error invariants , which are then used
to identify portions of a faulty trace that are irrelevant. There
are also other fault localization methods based on weakest
precondition [29], [30], [31] and inductive interpolant [32]
to explain the encountered failure. However, none of these
methodsis gearedtoward regression testing, and as such, they
donotutilize the codechangesbetween the correctandbuggy
versions.
There are bug triaging methods based on comparing the
passing and failing execution traces [3], [4], [5], [6], [33],
[34]. For a given failing execution, they ﬁnd passing exe-
cutions that are as similar to the failing one as possible.
Then, they identify the difference between passing and failing
executions and present them as an explanation of the failure.
Methods based on the use of dynamically learned likely
program invariants [35] can also be efﬁcient for catching the
differences between failing and passing executions. However,
the effectiveness of these methods is limited by the quality
and sometimes the availability of the test suite.
Fault localization methods based on identifying anomalous
events in program executions [36], [37], [38] rely on the
assumption that rarely occurring events are likely faulty. A
representative tool that falls in this category is RADAR [37],
[38], which derives multiple models from the base version
of the program and compares them with the failed execution
to identify a chain of suspicious anomalous events. Such
techniques differ from our method in that they do not rely on
semantic analysis and therefore can only discover correlation
between the anomalous events and the failure, but not the
causal relationship . Nevertheless, they are complementary to
our method in that the anomalous events can be used by our
method to further prune the irrelevant predicates.
VII. CONCLUSIONS
We have presented a new synergistic analysis method for
localizing faulty code changes in the context of regression
testingandexplaininghowthesecodechangesleadtotheman-
ifested failure. The method relies on an iterative framework
that leverages dynamic analysis to identify the correlation
between the code changes and the failure, and also leverages
semanticanalysistoidentifythecausalityrelationshipbetween
them. Our experiments on widely used Linux applications
show that the new method is effective in localizing relevant
code changes in practice. Furthermore, our method can report
a tree of causes to help explain the chain of fault propagation
events from the code changes to the manifested failure.
ACKNOWLEDGMENTS
We thank our colleagues Changhee Jung and Dongyoon
Lee for comments that greatly improved the manuscript.
This research was supported by the National Science Foun-
dation of China (NSFC) under grant 61472318, the Na-
tional Science and Technology Major Project of China under
grant2012ZX01039-004,andtheNationalScienceFoundation
(NSF) under grants CCF-1149454, CCF-1500365, and CCF-
1500024. Any opinions, ﬁndings, and conclusions expressed
in this materialare those of the authorsand do notnecessarily
reﬂect the views of the funding agencies.
266
266
266
266
ICSE 2015, Florence, ItalyREFERENCES
[1] A. Zeller, “Yesterday, my program worked. today, it does not. why?”
inACM SIGSOFT Symposium on Foundations of Software Engineering ,
1999, pp. 253–267.
[2] K. Yu, M. Lin, J. Chen, and X. Zhang, “Practical isolation of failure-
inducing changes for debugging regression faults,” in IEEE/ACM Inter-
national Conference On Automated Software Engineering , New York,
NY, USA, 2012, pp. 20–29.
[3] A. Groce and W. Visser, “What went wrong: explaining counterexam-
ples,” in International SPIN Workshop on Model Checking Software ,
2003, pp. 121–136.
[4] T. Ball, M. Naik, and S. K. Rajamani, “From symptom to cause:
localizing errors in counterexample traces,” in ACM SIGACT-SIGPLAN
Symposium on Principles of Programming Languages , 2003, pp. 97–
105.
[5] M. Renieris and S. P. Reiss, “Fault localization with nearest neighbor
queries,” in IEEE/ACM International Conference On Automated Soft-
ware Engineering , 2003, pp. 30–39.
[6] A. Groce, S. Chaki, D. Kroening, and O. Strichman, “Error explanation
with distance metrics,” Int. J. Softw. Tools Technol. Transf. ,vol. 8, no.3,
pp. 229–247, Jun. 2006.
[7] C. Lattner, “LLVM: An infrastructure for multi-stage optimization,” in
Masters Thesis , 2002.
[8] C. Cadar, D. Dunbar, and D. Engler, “KLEE: unassisted and automatic
generation of high-coverage tests for complex systems programs,” in
USENIX Symposium on Operating Systems Design and Implementation ,
2008, pp. 209–224.
[9] E. W. Dijkstra, A Discipline of Programming . Englewood Cliffs, NJ:
Prentice-Hall, 1976.
[10] B. Dutertre and L. de Moura, “A fast linear-arithmetic solver for
DPLL(T),”in International Conference on Computer Aided Veriﬁcation .
Springer, 2006, pp. 81–94, LNCS 4144.
[11] L. De Moura and N. Bjørner, “Z3: an efﬁcient SMT solver,” in
International Conference on Tools and Algorithms for Construction and
Analysis of Systems , 2008, pp. 337–340.
[12] T. Gyim´ othy, A. Besz´ edes, and I. Forg´ acs, “An efﬁcient relevant slicing
method for debugging,” in ACM SIGSOFT Symposium on Foundations
of Software Engineering , 1999, pp. 303–321.
[13] B. Korel and J. Laski, “Dynamic program slicing,” Inf. Process. Lett. ,
vol. 29, no. 3, pp. 155–163, Oct. 1988.
[14] D. Qi, A. Roychoudhury, Z. Liang, and K. Vaswani, “Darwin: an ap-
proachfordebuggingevolvingprograms,”in ACMSIGSOFTSymposium
on Foundations of Software Engineering , 2009, pp. 33–42.
[15] A. Banerjee, A. Roychoudhury, J. A. Harlie, and Z. Liang, “Golden im-
plementation driven software debugging,” in ACM SIGSOFT Symposium
on Foundations of Software Engineering , 2010, pp. 177–186.
[16] F. Tip, “A survey of program slicing techniques,” J. of programming
languages, vol. 3, pp. 121–189, 1995.
[17] A.ZellerandR.Hildebrandt, “Simplifyingandisolating failure-inducing
input,”IEEE Trans. Softw. Eng. , vol. 28, 2002.
[18] A. Zeller, “Isolating cause-effect chains from computer programs,” in
ACM SIGSOFT Symposium on Foundations of Software Engineering,
2002, pp. 1–10.
[19] D. Binkley, N. Gold, M. Harman, S. Islam, J. Krinke, and S. Yoo,
“Observation-based slicing,” Department of Computer Science, Univer-
sity College London, Tech. Rep. RN/13/13, 2013.[20] W. N. Sumner and X. Zhang, “Comparative causality: Explaining
the differences between executions,” in International Conference on
Software Engineering , 2013, pp. 272–281.
[21] G. Misherghi and Z. Su, “HDD: hierarchical delta debugging,” in
International Conference on Software Engineering , 2006, pp. 142–151.
[22] M. B¨ ohme, B. C. d. S. Oliveira, and A. Roychoudhury, “Partition-
based regression veriﬁcation,” in International Conference on Software
Engineering , 2013, pp. 302–311.
[23] F. Pastore, L. Mariani, A. E. J. Hyv¨ arinen, G. Fedyukovich, N. Shary-
gina, S. Sehestedt, and A. Muhammad, “Veriﬁcation-aided regression
testing,” in International Symposium on Software Testing and Analysis ,
2014, pp. 37–48.
[24] X. Zhang, S. Tallam, N. Gupta, and R. Gupta, “Towards locating execu-
tion omission errors,” in ACM SIGPLAN Conference on Programming
Language Design and Implementation , 2007, pp. 415–424.
[25] M. Jose and R. Majumdar, “Cause clue clauses: error localization using
maximumsatisﬁability,” in ACM SIGPLANConference on Programming
Language Design and Implementation , 2011, pp. 437–446.
[26] E. Ermis, M. Sch¨ af, and T. Wies, “Error invariants,” in International
Symposium on Formal Methods , 2012, vol. 7436, pp. 187–201.
[27] J. Christ, E. Ermis, M. Sch¨ af, and T. Wies, “Flow-sensitive fault local-
ization,” in International Conference on Veriﬁcation, Model Checking,
and Abstract Interpretation , 2013, vol. 7737, pp. 189–208.
[28] W.Craig,“Threeusesoftheherbrand-gentzen theoreminrelating model
theory and proof theory,” J. Symb. Log. , vol. 22, no. 3, pp. 269–285,
1957.
[29] C. Wang, Z. Yang, F. Ivanˇ ci´ c, and A. Gupta, “Whodunit? causal anal-
ysis for counterexamples,” in International Symposium on Automated
Technology for Veriﬁcation and Analysis , 2006, pp. 82–95.
[30] X. Zhang, N. Gupta, and R. Gupta, “Locating faults through automated
predicate switching,” in International Conference on Software Engineer-
ing, 2006, pp. 272–281.
[31] Q. Yi, Z. Yang, J. Liu, C. Zhao, and C. Wang, “Explaining software
failures by cascade fault localization,” ACM Transactions on Design
Automation of Electronic Systems , 2015.
[32] V. Murali, N. Sinha, E. Torlak, and S. Chandra, “What gives? A hybrid
algorithm for error trace explanation,” in International Conference on
Veriﬁed Software: Theories, Tools and Experiments , 2014, pp. 270–286.
[33] A.Perez,R.Abreu,andA.Riboira, “Adynamic codecoverage approach
to maximize fault localization efﬁciency,” Journal of Systems and
Software, vol. 90, no. 0, pp. 18 – 28, 2014.
[34] M. T. Befrouei, C. Wang, and G. Weissenbacher, “Abstraction and min-
ing of traces to explain concurrency bugs,” in International Conference
on Runtime Veriﬁcation, Toronto, ON, Canada, September 22-25, 2014. ,
2014, pp. 162–177.
[35] S.K.Sahoo,J.Criswell,C.Geigle,andV.Adve,“Usinglikelyinvariants
for automated software fault localization,” in International Conference
on Architectural Support for Programming Languages and Operating
Systems, 2013, pp. 139–152.
[36] L. Mariani and F. Pastore, “Automated identiﬁcation of failure causes
in system logs,” in International Symposium on Software Reliability
Engineering , 2008, pp. 117–126.
[37] F. and Pastore, , L. Mariani, , A. Gofﬁ, , M. Oriol, and M. Wahler,
“Dynamic analysis of upgrades in C/C++ software,” in International
Symposium on Software Reliability Engineering , 2012.
[38] F. Pastore, L. Mariani, and A. Gofﬁ, “RADAR: a tool for debugging
regression problems in C/C++ software,” in International Conference
on Software Engineering , 2013, pp. 1335–1338.
267
267
267
267
ICSE 2015, Florence, Italy