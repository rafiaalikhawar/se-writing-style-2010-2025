Testing Autonomous Cars for Feature Interaction Failures using
Many-Objective Search
Raja Ben Abdessalem
University of Luxembourg
Luxembourg
raja.benabdessalem@uni.luAnnibale Panichella
University of Luxembourg,
Luxembourg
Delft University of Technology
Netherlands
a.panichella@tudelft.nlShiva Nejati
University of Luxembourg
Luxembourg
shiva.nejati@uni.lu
Lionel C. Briand
University of Luxembourg
Luxembourg
lionel.briand@uni.luThomas Stifter
IEE S.A., Luxembourg
Luxembourg
thomas.stifter@iee.lu
ABSTRACT
Complex systems such as autonomous cars are typically built as
acompositionoffeaturesthatareindependentunitsoffunction-
ality.Featurestendtointeractandimpactoneanother’sbehavior
in unknown ways. A challenge is to detect and manage feature
interactions, in particular,those that violate systemrequirements,
hence leading to failures.In this paper, we propose atechnique to
detect feature interaction failures by casting this problem into a
search-basedtestgenerationproblem.Wedefineasetofhybridtest
objectives(distancefunctions)thatcombinetraditionalcoverage-
basedheuristicswithnewheuristicsspecificallyaimedatrevealing
feature interaction failures. We develop a new search-based test
generationalgorithm,calledFITEST,thatisguidedbyourhybrid
testobjectives. FITESTextendsrecently proposedmany-objective
evolutionary algorithms to reduce the time required to compute
fitness values. We evaluate our approach using two versions of an
industrial self-driving system. Our results show that our hybrid
testobjectivesareabletoidentifymorethantwiceasmanyfeature
interaction failures as two baseline test objectives used in the soft-
waretestingliterature(i.e.,coverage-basedandfailure-basedtest
objectives). Further, the feedback from domain experts indicates
thatthedetectedfeatureinteractionfailuresrepresentrealfaultsin
their systems that were not previously identified based on analysis
of the system features and their requirements.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging; Search-based software engineering;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238192KEYWORDS
Search-based Software Testing, Many-Objective Optimization, Au-
tomotive Systems, Feature Interaction Problem
ACM Reference Format:
Raja Ben Abdessalem, Annibale Panichella, Shiva Nejati, Lionel C. Briand,
and Thomas Stifter. 2018. Testing Autonomous Cars for Feature Interac-
tion Failuresusing Many-Objective Search.In Proceedings ofthe 201833rd
ACM/IEEEInternationalConferenceonAutomatedSoftwareEngineering(ASE
’18), September 3–7, 2018, Montpellier, France. ACM, New York, NY, USA,
12pages.https://doi.org/10.1145/3238147.3238192
1 INTRODUCTION
Feature-based development aims to build complex systems consist-
ing of units of functionality known as features. Individual features
are typically traceable to specific system requirements and are
mostly independent and separate from one another [ 37,44,63]. By
closely mirroring requirements, features make it easier for engi-
neers to develop complex systems iteratively and incrementally.Self-driving cars, and in general automotive systems, are among
well-known examples of feature-based systems [ 11,24,76]. A self-
driving system, for example, may include the following features,
eachautomatinganindependentdrivingfunction:Anautomated
emergencybraking (AEB),an adaptivecruise control(ACC) anda
traffic sign recognition (TSR).
Althoughfeaturesaretypicallydesignedtobeindependent,they
may behave differently when composed with other features. Afeature interaction is a situation where one feature impacts the
behavior of another feature [
17,25,44]. For example, in a self-
drivingsystem,featureinteractionsarelikelytoarisewhenseveral
features control the same actuators. More specifically, in a self-
driving system, both ACC and AEB control the braking actuator. A
featureinteractionmayarisewhenabrakingcommandissuedby
AEBtoimmediatelystopthecarisoverriddenbyACCcommanding
the car to maintain the same speed as that of the front car. Some
featureinteractionsaredesirable,andsomemayresultinviolations
of system safety requirements and are therefore undesired. For
example,theabovefeatureinteractionbetweenAEBandACCmay
lead to an accident, and hence, is undesirable.
143
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Ben Abdessalem, A. Panichella, S. Nejati, L. C. Briand, and T. Stifter
The feature interaction problem has been extensively studied in
the literature [7, 17,25,44]. Some techniques focus on identifying
feature interactions at the requirements-level by analysis of formal
orsemi-formalrequirementsmodels[ 15,18,75].Severaltechniques
detectfeatureinteractionerrorsinimplementationsusingtestcases
derivedfromfeaturemodelscapturingfeaturesandtheirdependen-cies[
8,35,58,61].Otherapproachesdevisedesignandarchitectural
resolutionstrategiestoeliminateatruntimeundesiredfeaturein-
teractionsidentifiedattherequirements-level[ 41,44,70,76].For
self-drivingsystems,ho wever,feature interactionsshouldbeidenti-
fied as early as possible and before the implementation stage since
lateresolutionofundesiredinteractionscanbetooexpensiveand
mayinvolvechanginghardwarecomponents.Further,featureinter-
actions in self-driving systems are numerous, complex and depend
on several factors such as the characteristics of sensors and actua-
tors,carandpedestriandynamics,weathercondition,roadtraffic
andsidewalkobjects.Withouteffectiveandautomatedassistance,
engineerscannotdetectundesiredfeatureinteractionswithinthe
spaceofallpossibleinteractionsandcannotassesstheimpactof
complex environmental factors on feature interactions.
Inthispaper,wedevelopanautomatedapproachtodetectunde-
sired feature interactions inself-driving systems at an early stage.
Our approach identifies undesired feature interactions based on
executable function models of self-driving systems embedded into
a realistic simulator capturing the self-driving system hardware
and environment. Building function models at an early stage is
standard practice in model-based development of control systems
and is commonly followed by the automotive and aerospace indus-
try[57,72,74].Functionmodelingtakesplaceafteridentificationof
system requirements and prior to software design and architecture
activities. Function models of control systems capture algorithmic
behaviors of software components and physic dynamics of hard-
warecomponents.Similartotheautomotiveandaerospaceindustry,
thefunction modelsandthesimulator oftheself-driving system
usedinthispaperarespecifiedintheMatlab/Simulinklanguage[ 1].
Inthispaper,wecasttheproblemofdetectingundesiredfeature
interactions into a search-based testing problem . Specifically, we
aimtogeneratetestinputsthatexposeundesiredfeatureinterac-
tionswhenappliedtoexecutablefunctionmodelsofself-driving
systems.Search-basedtechniqueshavebeensuccessfullyapplied
to simulation-based testing of control systems and self-driving fea-
tures[3,13,14,23,53,54]aswellasvariousothertestingproblems
such as unit testing [ 38,55,69], regression testing [ 49,73] and
optimizing machine learning components [66].
Contributions. Our contributions are as follows:
First,wedefinenovelhybrid testobjectives thatdeterminehow
farcandidatetestsarefromdetectingundesiredinteractions.Our
test objectives combine three different heuristics: (i) A branch cov-
erage heuristic [55] ensuring that the generated test cases exercise
all branches of the component(s) integrating features. (ii) A failure-
based heuristic based on system safety requirements ensuring that
testcasesstressthesystemintobreakingitssafetyrequirements.
(iii)Anunsafeoverridingheuristic thataimstoexhibitsystembe-
haviors where some feature output is overridden by other features
such that some system safety requirements may be violated.Second, we introduce FITEST ( FeatureInteraction TESTing), a
newmany-objectivetestgenerationalgorithm todetectundesired
featureinteractions. Weoptfor amany-objectiveoptimizational-
gorithm since test generation in our context is driven by many
competingtestobjectivesresultingfromthecombinationofheuris-
tics above. Specifically, FITEST builds on the recently proposed
many-objectivegeneticalgorithms[ 59,60]thateffectivelygener-
ate test cases satisfying a large number of test objectives. In our
work,computingtestobjectivesisexpensive.Hence,ateachiter-
ation, FITEST dynamically selects the minimum number of test
cases closest to satisfying test objectives, thus reducing the total
number of fitness computations.
Third, we evaluate FITEST using two industrial self-driving sys-
temsfromourpartnercompanyIEE[ 43].Bothsystemsrepresenta
(partial) self-driving car consisting of four features. The engineers
atIEEhaddevelopedalternativestrategiestoresolvetheknownfea-
ture interactions in these two systems. FITEST, however, was able
toidentify,onaverage,5.9and7.2undesiredfeatureinteractions
in the two systems, respectively. The engineers confirmed that the
detected interactions represent real faults that were not a priori
known tothem1. Further,we compared ourhybrid testobjectives
usedbyFITESTwithtwobaselinetestobjectivesfromthesoftwaretestingliterature(namely,coverage-based[
38,55]andfailure-based
testobjectives[ 4,13,20,23]).Ourresultsshowthatourhybridtest
objectives are able to identify more than twice as many feature
interaction failures as the coverage-based and failure-based test
objectives.
Structure. Section2motivates our work. Section 3presents our
approach.Section 4describesourevaluation.Section 5compares
with related work. Section 6concludes the paper.
2 MOTIVATION
Figure1shows an overview of a typical function model capturing
the software subsystem of a self-driving car. The system under test
(SUT) consists of a set of self-driving features and a component
capturingthedecisionalgorithmcombiningfeatureoutputs.SUT
receivesitsinputsfromsensors/camerasandsendsitsoutputsto
actuators.Bothinputsandoutputsaresequencesoftimestamped
values. The entire SUT runs iteratively at regular time steps.A t
everytimestep,thefeaturesreceivesensor/cameravaluesissued
in that step, and output values are computed and sent to actuators
by the end of the step. Each feature controls one or more actuators.
Actuatorsmayreceivecommandsfrommorethanonefeatureatthe
sametimestep,andsometimesthesecommandsareconflicting.The
integrationcomponenthastogeneratefinaloutputstoactuators
after resolving conflicting feature outputs.
As discussed in Section 1, our goal is to identify feature interac-
tionsattherequirements-levelandintermsofsystemfunctional
behavior. Hence, we base our analysis on function models specify-
ing algorithmic and control behaviors. Feature interaction failures
due to software architecture and design issues are not studied in
this paper.
Weuseacasestudysystem,called SafeDrive,fromourpartner
company IEE. SafeDrive contains the following four self-driving
features:Autonomous Cruise Control (ACC), Traffic Sign Recognition
1The material we used to get the industry feedback is available online [2].
144
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Testing Autonomous Cars for Feature Interaction Failures ASE ’18, September 3–7, 2018, Montpellier, France
System Under Test (SUT)
...sensors
camerasfeature 1
feature 2
feature n Integration 
componentactuators
Figure1:Overviewofatypicalfunctionmodelcapturingthe
software subsystem (SUT) of a self-driving car.
(TSR),PedestrianProtection(PP),and AutomatedEmergencyBraking
(AEB). ACC automatically adjusts the car speed and dir ection to
maintain a safe distance from a car ahead (or a leading car ). TSR
detectstrafficsignsandappliesappropriatebraking,accelerationorsteeringcommandstofollowthetrafficrules.PPdetectspedestriansinfrontofacarwithwhomthereisariskofcollisionandappliesa
braking command if needed. AEB is the same as PP but it prevents
accidents with objects other than pedestrians. Once the risk of an
accident is over and the road is clear, both PP and AEB issue accel-
eration commands to bring back the car to the same speed that the
car had before their intervention. All the features generate braking
andaccelerationcommandstorespectivelycontrolthebrakeand
thethrottleactuators.TSRandACC,additionally,generatesteering
commands.
TheSafeDrive featuresmayissueconflictingcommandstothe
same actuators. For example, Scenario-1: ACC orders the car to
accelerate,whileapedestrianstartscrossingtheroad.Hence,atthe
same time, PP starts sending braking commands to avoid hittingthe pedestrian. Scenario-2: The car reaches an intersection while
the traffic light turning from orange to red. ACC orders the car
toacceleratesincetheleadingcarhasalsoacceleratedtopassthe
intersectionwhilethelightisorange.Atthesametime,TSRorders
to brake since it detects that a red light is about to come.
When feature interactions are known, engineers can develop
the decision logic of the integration component (see Figure 1) such
that the interactions do not lead to failures (e.g., using existing
featureinteractionresolutiontechniques[ 44,76]).Forexample,for
Scenario-1,engineersmaydecidetoprioritizethebrakingcommand
of PP over the acceleration command of ACC to avoid hitting a
pedestrian.Theresolutionstrategyfor Scenario-2 canbeprioritiz-
ingTSRifthecarcansafelystopbythetrafficlight,andotherwise,
prioritizing ACC. However, feature interactions in SafeDrive are
numerous and many of them may not be known, particularly at
earlydevelopmentstages.Further,thefeatureinteractionresolu-
tion strategies cannot always be determined statically and may
depend on complex environment factors. For example, deciding “if
the car can safely stop” in the resolution strategy for Scenario-2
dependsonthespeed andthepositionofthecar,thedistancetothe
car behind, road topology and the weather condition. Therefore,
we need techniques that, at early development stages, (1) detect
undesiredfeatureinteractionsin SafeDrive,and(2)testwhetherthe
proposed resolution strategies can avoid failures under different
environment conditions.
In thenextsections, we present andevaluate a techniquethat
tests the functional behavior of autonomous cars to detect their
undesired feature interactions. Our technique accounts for the im-SUTSimulator
Model of the 
(ego) car or  
the physical 
plant Pedestrians
Other cars
- Roads
- Trafﬁc signs
- WeatherOutputs
Time-stamped vectors for: 
- the SUT outputs 
- the states of the physical plant and the mobile environment objectssensors
camerasactuatorsEnvironment
mobile objects
static propertiesInputs
- the initial state of the physical plant and the 
mobile environment 
objects- the static environment 
aspects
Figure2:Earlytestingofcontrolsystemfunctionmodelsus-
ing simulators.
pactoftheenvironmentfactorsontheself-drivingsystembehavior.
It, further, ensures that feature interaction resolution strategies
devised by engineers satisfy system safety requirements under dif-
ferentenvironmentconditions.WenotethatinSection 3.3,wewill
provideapreciseformalizationofthecontextuponwhichwebuild.
The formalism is generic and based on simple assumptions that
canbeaccommodatedbymanyfeature-basedsystems.Hence,in
additiontoautonomouscars,ourworkappliestoanyfeature-based
system expressible using our formalism.
3 APPROACH
Inthissection,wepresentourfeatureinteractiondetectiontech-
nique. As discussed earlier, our technique generates test inputs for
functionmodelsofself-drivingsystems,exposingtheirundesired
feature interactions. Section 3.1describes how we integrate the
function models into a high-fidelity, physics-based simulator for
self-drivingsystems.Section 3.2characterizesthetestinputsand
outputs for self-driving systems. Section 3.3introduces our hybrid
test objectives. Section 3.4presents FITEST, our proposed many-
objective test generation algorithm that utilizes our test objectives
to generate test inputs revealing feature interaction failures.
3.1 Testing Feature-Based Control Systems
Testing Cyber-Physical Systems (CPSs) at early stages is generally
performed using simulators. To test the function model in Figure 1,
weconnecttheSUTmodeltoasimulatorsuchthatitreceivesinputs
from the sensor and camera models ofthe simulator and sends its
outputs to the actuator models of the simulator (see Figure 2). The
sensor, camera and actuator models are within a physical model of
a car (or a physical plant according to general CPS terminology) in
thesimulator.Torunthesimulator,wespecifytheinitialstateofthe
simulatorphysicalplantandmobileenvironmentobjectsaswellasthestaticenvironmentproperties(e.g.,weatherconditionand
road shapes for self-driving systems). The simulator can executethe SUT in a feedback loop with the plant and the environment.
ForSafeDrive,weusePreScan,aphysics-basedsimulatorforself-
drivingsystems[ 67].PreScanreliesondynamicSimulinkmodelsto
compute movements of cars and pedestrians and is able to capture
the environment static properties such as the weather condition
andtheroadtopology.Someexamplesof SafeDrive simulationsare
available online [2].
145
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Ben Abdessalem, A. Panichella, S. Nejati, L. C. Briand, and T. Stifter
xe
0xl0xp
0yp
0ye
0,yl
0
θp
xtsx−axisy−axis
/vectorvp
0/vectorve
0/vectorvl
0X=(xp
0,yp
0,θp,/vectorvp
0,/vectorve
0,/vectorvl
0,xl
0,xts,fg) Test input vector 
Figure3:Testinputsrequiredtosimulate SafeDrive,ourcase
study system.
3.2 Test Inputs and Outputs
The test inputs for a self-driving system are the inputs required
toexecutethesimulationframeworkinFigure 2.Forexample,to
testSafeDrive, we startby instantiatingthe simulationframework
so that the simulator is able to exercise the behaviors of the PP,
AEB,TSRandACCfeatures.Oursimulationframeworkcontains
the following objects: (1) An ego carequipped with SafeDrive, (2) a
leadingcar totestboththeACCandtheAEBfeaturesoftheego
car, and (3) a pedestrian that crosses the road starting from an
initial position on the sidewalk and is used to exercise PP. Thesimulation environment, further, includes one traffic sign to test
the TSR feature. We only consider a stop sign or a speed limit sign
for our case study. This setup is meant to reduce the complexity of
simulations and was suggested by the domain experts.
The test inputs of SafeDrive are shown in Figure 3. They include
thefollowingvariables:(1)Theinitialposition xe
0,ye
0andtheinitial
speedve
0oftheegocar.(2)Theinitialposition xl
0,yl
0andtheinitial
speedvl
0oftheleadingcar.(3)Theinitialposition xp
0,yp
0,theinitial
speedvp
0and the orientation θpof the pedestrian. (4) The position
xtsof the traffic sign that varies along the x-axis, but is fixed along
they-axis. (5) The fog degree fg. In our simulator, among different
weather-relatedproperties(e.g.,snowandrain),thefoglevelhas
the largest impact on the object detection capabilities of SafeDrive.
Hence, we include the fog level in the test inputs.
Alltheabovevariablesexceptfor fgarefloatnumbersvarying
within ranges specified by domain experts. The variable fgis an
enumeration specifying ten different degrees of fog. In addition
to the domain value ranges, there are often some constraints over
test inputs to ensure that simulations start from a valid and mean-
ingful state. Specifically, we have the following two constraints
forSafeDrive : (i) The ego car starts behind the leading car with
a safety distance gap, denoted sd, and with a speed close to the
speed of the leading car. This constraint is specified as follows:
sd−ϵ≤xl
0−xe
0≤sd+ϵand|ve
0−vl
0|≤ϵ/primewhereϵandϵ/primearetwo
smallconstants,and sd,whichisthesafetydistancegapbetween
theegoandtheleadingcars,isdeterminedbasedonthecarspeeds.
(ii) The traffic sign is located within a sufficiently long distance
from the ego car to give enough time to the TSR feature to react
(i.e.,|xts−xe
0|<cwherecisconstantvalue).Finally,tosimulate
thesystem,weneedtospecifythedurationofthesimulation Tand
the simulation step size δ.
AsshowninFigure 2,thesimulatoroutputsaretime-stamped
vectors specifying (1) SUT outputs, (2) states of the physical plantsPP
AEBbPP
aPP
TSRaAEBbAEB
aTSRbTSRsTSR
ACCs ab
    : braking
    : acceleration
    : steeringbasif (condition)IntC
T
δ0T
δ040%( bAEB(0))
40%( bAEB(1))
80%( bPP(2))
80%( bPP(3))
80%( bPP(T/δ))....80%
80%80%80%
...T
δ0
PP40%
40%
...
T
δ0
bACCaACCsACC
T
δ0bPP
20%
0%60%
40%
Figure 4: Actuator command vectors generated at the
feature-level and at the system-level by simulating
SafeDrive. Vectors bf,afandsfindicate command vec-
tors generated by feature ffor the braking, acceleration
and steering actuators, respectively. The IntCcomponent
analyzesthecommandvectorsgeneratedbyallthefeatures
and issues the final command vectors b,aandsto the
braking, acceleration and steering actuators, respectively.
and (3) states of any mobile environment object. All these outputs
arevectors withT
δelementswhere theelementat position ispec-
ifies the output at time i·δ. For example, Figure 4illustrates the
SUT outputs generated by simulating SafeDrive. Specifically, the
SUT outputs in that figure include both the outputs of each feature
inside the SUT and the output of the integration component, i.e.,
the final command vector sent to the actuators.
3.3 Hybrid Test Objectives
Our test objectives aim to guide the test generation process to-
wards test inputs that reveal undesired feature interactions. We
first present our formal notation and assumptions and then we
introduceourtestobjectives.Notethatsinceinthispaperweare
primarilyinterestedinthefeatureinteractionproblem,wedesign
ourtestobjectivessuchthattheyfocusondetectingfailuresthat
ariseduetofeatureinteractions,butnotfailuresthatarisedueto
an individual feature being faulty.
Notation. Wedefineafeature-basedcontrolsystem Fasatuple
(f1,...,fn,IntC)wheref1,...,fnarefeaturesand IntCisaninte-
grationcomponent.Thesystem Fcontrolsaset Actofactuators.
Each feature ficontrols a set Actfi⊆Actof actuators. Since we
are interested in identifying feature interaction failures and not
failures due to errors inside individual features, our approach does
notrequireanyvisibilityintotheinternalsoffeatures.But,inour
work,IntCisawhite-boxcomponent.The IntCbehavioristypically
conditional where each condition checks a specific feature interac-
tion situation and resolves potential conflicts that may arise under
that condition. We assume Fhas a set of safety requirements such
thateachrequirementisrelatedtoonefeaturewhichisresponsible
for the satisfaction of that requirement. For example, the second
columnofTable 1showsthesafetyrequirementsforSafeDrive.The
feature responsible for satisfying each requirement is shown in the
first column.
146
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Testing Autonomous Cars for Feature Interaction Failures ASE ’18, September 3–7, 2018, Montpellier, France
Table 1: Safety requirements and failure distance functions
forSafeDrive.
FeatureRequirement Failure distance functions (FD 1,...,FD5)
PPNo collision with
pedestriansFD1(i)isthedistancebetweentheegocarandthe
pedestrian at step i.
AEBNo collision with
carsFD2(i)isthedistancebetweentheegocarandthe
leading car at step i.
TSRStopatastopsign Letu(i)be the speed of the ego car at time step
iif a stop sign is detected, and let u(i)=0i f
there is no stop sign. We define FD3(i)=0i f
u(i)≥5km/h;FD3(i)=1
u(i)ifu(i)/nequal0; and
otherwise, FD3(i)=1.
TSRRespectthespeed
limitLetu/prime(i)be the difference between the speed of
theegocarandthespeedlimitatstep iifaspeed-
limit sign is detected, and let u/prime(i)=0 if there
is no speed-limit sign. We define FD4(i)=0i f
u/prime(i)≥10km/h;FD4(i)=1
u/prime(i)ifu/prime(i)/nequal0; and
otherwise, FD4(i)=1.
ACCRespect the safety
distanceFD5(i)is the absolute difference between the
safety distance sdandFD2(i).
As discussed earlier, testing Fis performed by connecting Fto
asimulationframework(seeFigure 2).AtestcaseforFisavector
Xof inputs required to execute the simulation framework into
whichFis embedded (e.g., Figure 3shows the test input vector
forSafeDrive ). The test output of Fincludes: (1) a vector vf
act
generated by every feature fand for every actuator act∈Actf;
(2) a vector vactgenerated by IntCfor each actuator act∈Act;
and (3) a trajectory vectorfor the physical plant and every mobile
environment object.
Testobjectives. Akeyaspectinsearch-basedsoftwaretesting[ 40,
55]isthenotionofdistancefunctions D(.)thatmeasurehowfar
a candidate test Xis from reaching testing targets (e.g., covering
branches in white-box testing). Our testing targets aim to reveal
undesired feature interactions. An undesired feature interactionis revealed when: (1) Some safety requirement
ris violated such
that (2) the integration component (i.e., IntC)overrides the output
ofthefeatureresponsiblefor r.Wenotethatif risviolatedwhile
IntCselects the output of the feature responsible for r, then the
violation is likely to be due to the internals of that feature and
notduetofeatureinteractions.Therefore,wedefinetwodistance
functions, namely failure distance andunsafe overriding distance to
respectivelycapturetheconditions(1)and(2)above.Further,we
ensurethatthegeneratedtestsexerciseallbranchesof IntC.Hence,
our third distance corresponds to the well-known distanceused
incoverage-basedtesting[ 55].Inthefollowing,wepresenteach
distance separately and then we describe how we combine them to
build our test objectives.
Coveragedistance. First,thegeneratedtestcaseshavetoexercise
every branch of IntC. Given that IntCis white-box, we rely on two
widely-used heuristics in branch coverage, namely the approach
level[55] and the normalized branch distance [38,55]. Each branch
biinIntChasitsowndistancefunction BDitominimizewhichis
defined according to the two heuristics above. The distance BDi
is equal to zero iff a candidate test case tccovers the associated
branchbi.Failuredistance: Thefailuredistanceevaluateshowclosethesys-
temFis from violating its safety requirements at each simulation
time step. For each system safety requirement j∈{1,...,m},w e
define a failure distance FDjsuch that FDj(i)=0 iff requirement j
isviolatedattimestep i.FDjisablack-boxheuristic,i.e.,itrelies
on system outputs only.
For example, the third column of Table 1describes functions
FD1(i)toFD5(i)forthefivesafetyrequirementsof SafeDrive inthe
second column of that table. Since self-driving safety requirements
typically concern mobile environment objects and physical plants,
the failure distance is computed based on the trajectories of thephysical plant and the environment mobile objects generated by
simulation.Recallthatforeachsafetyrequirementof F,thereis
only one feature that is responsible for its satisfaction. Hence, each
FDjis related to a feature fofFsuch that fis the feature respon-
sible for satisfying j. When any of the FD1(i)toFD5(i)functions
in Table1yields a zero value at step i, it means that a requirement
failure corresponding to that function is detected. Further, smallor large values of these functions indicate that the system is, re-spectively, close to or far from exhibiting a failure. For example,function
FD1(i)related to PPmeasures the distance between the
ego car and the pedestrian. A search algorithm guided by FD1gen-
erates simulationsduring whichthe distancebetween theego car
andthepedestrianisminimized,henceincreasingthelikelihoodof
an accident. Asanother example, the distancefunctions related to
the TSR requirements are defined as the inverse of the speedofthe
ego car for the stop sign, and the inverse of the difference between
thespeedofthe egocarandthespeedlimitfor thespeedlimitsign.
Accordingtodomainexperts,thestopsignrequirementiscertainly
violated when the speed of the car never falls below 5 km/hafter
detecting the stop sign, and the speed limit sign re quirement is
certainlyviolatedwhenthes peedofthecarexceedsthespeedlimit
by more than 10 km/h. For both cases we set the concerned failure
function to zero indicating that a safety violation has occurred.
Unsafe overriding distance: This distance function aims to pri-
oritize behaviors that violate safety requirements due to errors
insideIntCoverthebehaviorsthatfailduetoerrorsinsidefeatures.
At each simulation time step, the IntCcomponent prioritizes the
outputofsomefeatureand overrides thoseoftherest.Recallthat
for each actuator act,IntCalways generates the vactvector, and
every feature fgenerates vf
actifffcontrolsact(i.e.,act∈Actf).
Ifvact(i)=vf
act(i), it means at time step i,IntCprioritizes fover
other featurescontrolling act. Dually,if vact(i)/nequalvf
act(i),it means
at time step i,IntCoverrides the command issued by fforact. For
example,inFigure 4,theIntCcomponentofSafeDriveprioritizes
AEBovertheotherthreefeaturestocontrolthebrakingactuator
at time steps 0 and 1.
For an actuator actand at time step i,w es a yIntCunsafely over-
ridesfifthecommandat vact(i)islesssafethanthecommandat
vf
act(i)foract. We say a command cis less safe than a command
c/primefor an actuator act, whenactexecuting cis more likely to break
somerequirementcomparedto actexecuting c/prime.Forexample,inthe
SafeDrivesystem,amildandlatebrakingmorelikelyleadstoviolat-ingoneoftherequirementsinTable 1comparedtoafirmandearly
braking. Dually, the requirements in Table 1are more likely to fail
147
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Ben Abdessalem, A. Panichella, S. Nejati, L. C. Briand, and T. Stifter
when we accelerate faster than when we accelerate more slowly.
Notethattestcasesthatviolatesafetyrequirementswithout IntC
unsafelyoverridinganyfeaturedonotfailduetofaultsin IntC.This
is because, for such test cases, either IntCdoes not override any
decisionofanyindividualfeatureoritsdecisiontooverrideafeature
doesnotincreasethelikelihoodofviolatingasafetyrequirement.
Hence, such test cases fail due to a fault in a feature. For IntC
to be faulty, it is necessary that vactunsafely overrides vf
actin
some simulation time step. For each feature f, we define an unsafe
overriding distance UODfsuch that UODf=0i ffIntCunsafely
overridestheoutputof fatleastonceduringthesimulation,and
otherwise, UODf>0.Suchadistanceguidesthesearchtowards
generating tests that cause IntCto unsafely override f.
To compute UODf, we define UODact
ffor each actuator actcon-
trolled by f. For actuators where higher force values are safer (e.g.,
braking), IntCunsafely overrides fwhenvf
act(i)>vact(i)(i.e.,
when,atstep i,forderstobrakemorestronglythan IntC).Weuse
the traditionalbranch distancefor the greater-than condition [ 47]
totranslatethisconditionintoadistancefunction.Thatis,forsuch
actuators, we define UODact
fat each simulation step i, as follows:
UODact
f(i)=⎧⎪⎨⎪⎩vact(i)−vf
act(i),ifvf
act(i)<vact(i)
0, otherwise
Dually, for actuators that lower force values are safer (e.g., ac-
celeration), IntCunsafely overrides fwhenvact(i)>vf
act(i)(i.e.,
whentheacceleratingcommandof fislessthanthatof IntCatstep
i).Followingthetraditionalbranchdistanceforthe less-thancondi-
tion [47], we define UODact
ffor this kind of actuators as follows:
UODact
f(i)=⎧⎪⎨⎪⎩vf
act(i)−vact(i),ifvact(i)<vf
act(i)
0, otherwise
Wecompute UODf(i)=/summationtext
act∈ActfUODact
f(i)whereeach UODact
f
is defined as either one of the above equations depending on the
type ofact. TheUODffunction is our unsafe overriding distance
function.Specifically, UODf(i)=0impliesthat IntCunsafelyover-
ridestheoutputof fatstepi.Similarly,asmallorlargevalueof
UODf(i)indicates that a test case is, respectively, close to or far
from causing IntCto unsafely override fat stepi.
Combineddistances.Wenowdescribehowwecombinethethree
distance functions to obtain our final hybrid test objectives for de-tecting undesired feature interactions. Note that coverage distance,
failuredistance andunsafeoverridingdistance havedifferentunits
of measure (e.g., km/h, meters) and different ranges. Thus, we first
normalizethesedistancesbeforecombiningthemintoonesingle
hybridfunction.Tothisaim,werelyonthewell-knownrational
function ω1(x)=x/(x+1)since prior studies [ 9] have empiri-
cally shown that, compared to other normalization functions, it
providesbetterguidancetothesearchforminimizationproblems
(e.g., distance functions in our case). In the following, we denote
thenormalizedformsofthefunctionsaboveas FD,UODandBD,
respectively.
To maximize the likelihood of detecting undesired feature inter-
actions, we aim to execute every branch of IntCsuch that while
executingthatbranch, IntCunsafelyoverrideseveryfeature f,andfurther, its outputs violate every safety requirement related to f.
Therefore, for every branch jofIntC, every safety requirement lof
F, and every simulation time step i, we define a hybrid distance
Ωj,l(i)as follows:
Ωj,l(i)=⎧⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩BDj(i)+UODmax+FDmax(1) Ifjis not covered ( BDj(i)>0)
UODf(i)+FDmax (2) Ifjis covered, but fis not unsafely
overridden ( BDj(i)=0∧UODf(i)>0)
FDl(i) (3) Otherwise ( BDj(i)=0∧UODf(i)=0)
wherefis the feature responsible for the requirement l, while
FDmax=1 andUODmax=1, indicating the maximum value of the
normalized functions.
Eachhybriddistancefunction Ωj,l(i)isdefinedforeachsimu-
lationstep i.Correspondingtoeachhybriddistancefunction,we
define atest objective Ωj,lfor the entire simulation time interval as
follows:Ωj,l=Min{Ωj,l(i)}0≤i≤T
δ. Given a test case tc, each test
objective Ωj,l(tc)alwaysyieldsa valuein[0 ..3];Ωj,l(tc)>2indi-
cates that tchas not covered branch j;2≥Ωj,l(tc)>1 indicates
thattchas covered branch j, but has not caused IntCto unsafely
override some feature frelated to requirement l;1≥Ωj,l(tc)>0
indicatesthat tchascoveredbranch j,andhascaused IntCtoun-
safely override some feature frelated to requirement l, but has
not violated requirement l; and finally, Ωj,l(tc)is zero when tchas
coveredbranch j,hascaused IntCtounsafelyoverridesomefeature
frelated to land has violated requirement l.
3.4 Search Algorithm
When testing a system we do not know a priori which safety re-
quirementsmaybeviolated.Neitherdoweknowinwhichbranches
ofIntCtheviolationsmaybedetected.Therefore,wesearchforany
violationofsystemsafetyrequirementsthatmayarisewhenexercis-
inganybranchof IntC.Thisleadsto k×ntestobjectiveswhere kis
thenumberofbranchesof IntCandnisthenumberofsafetyrequire-
ments.Moreformally,givenafeature-basedcontrolsystem Funder
test, our test generation problem can be formulated as follows:
Definition. LetΩ=/braceleftBig
Ω1,1,...,Ωk,n/bracerightBig
be the set of test objectives
forF,wherekisthenumberofbranchesin IntCandnisthenumber
of safety requirements of F. Find a test suite that covers as many
objectives Ωi,jas possible.
Our problem is many-objective as we attempt to optimize a
relatively large number of test objectives. As a consequence, we
havetoconsidermany-objectiveoptimizationalgorithms,whichare
aclassofsearchalgorithmssuitablydefinedforproblemswithmore
than three objectives. Various many-objective metaheuristics have
beenproposedintheliterature, suchasNSGA-III[ 33],HypE[ 12].
Thesealgorithmsaredesignedtoproducedifferentalternativetrade-
offs that can be made among the search objectives [48].
Recently, Panichella et al. [ 59,60] argued that the purpose of
test case generation is to find test cases that separately cover in-
dividual test objectives rather than finding solutions capturing
well-distributedanddiversetrade-offsamongthesearchobjectives.
Hence,theyintroducedanewsearchalgorithm,namelyMOSA[ 59],
that (i) rewards test cases that cover at least one objective over
those that yield a low value on several objectives without covering
any; (ii) focuses the search on the yet uncovered objectives; and
(iii) stores all tests covering one or more objectives into an archive.
148
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Testing Autonomous Cars for Feature Interaction Failures ASE ’18, September 3–7, 2018, Montpellier, France
Algorithm 1: Feature Interaction Testing (FITEST)
Input:Ω: Set of objectives
Result:A: Archive
1begin
2P←−ADAPTIVE-RANDOM-POPULATION( |Ω|)
3W←−CALCULATE-OBJECTIVES( P,Ω)
4[Ωc,Tc]←−GET-COVERED-OBJECTIVE( P,W)
5A←−Tc
6Ω←−Ω−Ωc
7whilenot (stop_condition) do
8 Q←−RECOMBINE( P)
9 Q←−CORRECT-OFFSPRINGS( Q)
10 W←−CALCULATE-OBJECTIVES( Q,Ω)
11 [Ωc,Tc]←−GET-COVERED-OBJECTIVE( P,W)
12 A←−A∪Tc // Update the archive
13 Ω←−Ω−Ωc// Update the set of objectives
14 F0←−ENVIRONMENTAL-SELECTION (P∪Q,Ω)
15 P←−F0 // New population
16returnA
MOSAhasbeenintroducedinthecontextofwhite-boxunittesting
andhasshowntooutperformalternativesearchalgorithms[ 59,60].
Inthispaper,weintroduceFITEST,anovelsearchalgorithmthat
extendsMOSAandadaptsittotestingfeature-basedself-driving
systems.Below,wedescribethemainloopofFITESTwhosepseudo-
code is shown in Algorithms 1. We then discuss the differences
between FITEST and MOSA.
Mainloop . As Algorithm 1shows, FITEST starts by generating
an initial set Pof randomly generated test cases (line 2), called
population. Each test case X∈Pis a vector of inputs required to
simulate the SUT (e.g., see Figure 3). After simulating each test
X∈P, the test objectives Ωj,lforXare computed based on the
simulation results (see Section 3.3). Next, tests are evolved through
subsequentiterations(loopinlines7-16),called generations.Ineach
generation, the binary tournament selection [34] is used to select
pairs of fittest test cases for reproduction. During reproduction
(line8),twotests(parents )arerecombinedtoformnewtestcases
(offsprings ) using the crossover andmutation operators. Finally,
fittesttestsareselectedamongtheparentsandoffspringstoform
the new population for the next generation (line 14). Below, we
describe the new and specific features of FITEST.
Initialization . The size of the initial population in FITEST is
equal to the number of test objectives.This is because, in our con-
text, running each single test case is expensive, taking up to few
minutes,asitrequiresrunningcomputationallyintensivesimula-
tions.Hence,inFITEST,weaimtocovereachtestobjectiveatmostoncebyatmostonetestcase.Therefore,wedonotneedtostartthe
search with a population larger than the number of test objectives.
We select the initial population such that it includes a diverse
and randomly selected set of test input vectors. This is because we
aim to include different traffic situations, (e.g., different trajectory
anglesandspeed sofpedestrians)inourinitialpopulation.Todo
so, weuse an adaptive randomsearch algorithm [ 51], whichis an
extension of the naive random search that attempts to maximizethe Euclidean distance between the vectors selected in the input
space.IncontrasttoFITEST,theinitialpopulationinMOSAisasetofrandomlygeneratedtestswithoutanydiversitymechanism,and
the size of the population is an input parameter of the algorithm.Genetic recombination . Since our test inputs (i.e., X) are vec-
tors of float values (see Figure 3), we use two widely-used genetic
operators proposed for real number solution encodings: the sim-
ulatedbinarycrossover [30](SBX)andthe gaussianmutation [32].
Prior studies [ 32,42] show that, for numerical vectors, these opera-
torsoutperformthemoreclassicalones.Incontrast,MOSAusesthe
classicalsingle-point crossover anduniformmutation implemented
inEvoSuite [ 38]to handledifferent typesoftest data,e.g.,strings,
Java objects, etc.
Correction operator . Recall from Section 3.2that our test in-
putsarecharacterizedbyconstraints.Hence,geneticoperatorsmay
yield invalid tests (e.g., a test input where the leading car is behind
the ego car). To modify and correct such cases, FITEST applies cor-
rection operators (line 9 in Algorithm 1). For example, in SafeDrive,
if after applying genetic operators, the leading car position ( xl
0)
and speed ( vlo), and the traffic sign position ( xts) violate any of
the constraints described in Section 3.2, we discard their values
andrandomlyselectnewvaluesforthesevariableswithinranges
enforced by the ego car position ( xe
0)andspeed( ve
0).
Archive. Similar toMOSA, every time new testsare generated
andevaluated(eitheratthebeginningorduringthesearch),FITESTusestheGET-COVERED-OBJECTIVEroutinetoidentifynewlycov-
ered objectives and the test cases covering them. These objectives
are removed from the set of test objectives (line 6, 13) to not be
usedbytheenvironmentalselectioninthesubsequentiterations.
Further,testcasescoveringtheremovedtestobjectivesareputin
anarchive[59,60,64] (i.e.,A). Thearchive atthe endcontains the
FITEST results. Each test case in the archive covers one of the test
objectives being satisfied during the search. Note that some test
objectives may not be covered within the search time or they may
be infeasible (unreachable).
Environmental selection. In FITEST, at each iteration, a new
populationwithasizenotnecessarilythesameasthepreviouspop-ulationsizeisformed(line15inAlgorithm 1)byselecting,foreach
uncoveredtestobjective Ωi,j,thetestcasein P∪Qthatisclosest
tocoveringthatobjective(preferencecriterion [59]).Thepopulation
size at each iteration is lower than the number of objectives. It can
evenbelessthanthenumberoftestobjectivesbecauseasingletest
casemaybeselectedastheclosest(fittest)testformultipleobjec-
tives.Further,thepopulationsizeislikelytodecreaseoveriterations
since, at each iteration, test objectives are covered and excluded
from the environmental selection in the subsequent iterations.
The population size represents the main difference between
FITEST and similar search-based test generation algorithms. In
classical many-objective search algorithms, the environment selec-
tion chooses a fixed number Nof tests (i.e., to maintain a constant
population size) from offsprings and their parents (i.e., from P∪Q)
using the Pareto optimality [31,34] (i.e., selecting solutions that
are non-dominated by any other solutions in P∪Q). In MOSA,
the population size is kept constant as well but the selection is
performed by first selecting the test cases in the first front F0built
using the preference criterion; then, if the size of F0is less than
N, MOSA uses the Pareto optimality criterion to select enough test
cases such that in total Ntest cases are selected.
Incontrast,FITESTminimizesthenumberoftestcasesgenerated
ateachsearchiterationbyevolvingonlytestcasesthatareclosestto
149
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Ben Abdessalem, A. Panichella, S. Nejati, L. C. Briand, and T. Stifter
satisfyinguncoveredobjectives,i.e.,thosein F0.Thishelpsreducing
the search computation time compared to existing many-objective
searchalgorithmsthattypicallymaintainandevolveafixednumber
of solutions at each iteration. This is particularly important in the
context of our work, since running each test case is expensive.
4 EVALUATION
In this section, we evaluate our approach to detecting undesired
feature interactions using real-world automotive systems.
4.1 Research Questions
The goal of our study is to assess how effectively our hybrid test
objectives(hereafterreferredtoas Hybrid)guidethesearchtoward
revealingfeatureinteractionfailures.AsdescribedinSection 3.3,
Hybridbuilds on three distance functions: (1) coverage, (2) fail-
ureand(3)unsafeoverriding.Amongthese, coveragedistance isa
well-knownheuristicthathasbeenextensivelyusedinwhite-box
testing [38,39,55]. For example, Fraser and Arcuri [ 39] showed
thatpurecoverage-baseddistancecanbeusedtogenerateunittests
capable of detecting real faults. Variations of the failure distance
havealsobeenusedindifferentcontextstogeneratetestsrevealing
requirements violations [ 4,13,20]. Therefore, we want to assess
whetherHybridprovides any benefits compared to pure coverage-
based and failure-based objectives. In particular, we formulate the
following research questions:
RQ.DoesHybridrevealmorefeatureinteractionfailurescompared
to coverage-based and failure-based test objectives?
Coveragebased-objectives,hereafterreferredtoas Cov,corre-
spondtothe BDfunctionsdescribedinSection 3.3andarecomputed
as the sum of the approach level [ 56] and the normalized branch
distance [ 56]. Therefore, Covaims to execute as many branches of
IntCas possible.
Failure-based test objectives, hereafter referred as to Fail, aim to
generatetestcasesthatexecuteasmanybranchesof IntCaspossible
while violating as many system safety requirements as possible
when executing each branch. Thus, Failis defined by combining
branch distance BDand failure distance FDfunctions described in
Section3.3. More precisely, for each branch jofIntCand every
safety requirement lofF, a failure-based test objective is defined
asMin{Failj,l(i)}0≤i≤T
δwhere
Failj,l(i)=⎧⎪⎨⎪⎩BDj(i)+FDmaxifjis not covered
FDl(i) otherwise
Inthispaper,wefocusourempiricalevaluationoncomparing
Hybridwith alternative test objectives, but we do not compare
FITESTwithalternativemany-objectivesearchalgorithmsbecause,
as discussed in Section 3.4, our changes to MOSA are primarily
motivated by the practical needs of (1) using genetic operators for
numericalvectors(oftencalledreal-codedoperators[ 32,42])and(2)
loweringtherunningtimeofouralgorithmbyreducingthenumber
of (expensive) fitnesscomputations at each generation. In ourpre-
liminary experiments, running MOSA with its default population
sizeof50[ 59]requiredmorethan24hoursforonly10generations.
Further,previousstudiesshowedthatMOSA,whichisthealgorithm
underlying FITEST, outperforms other search-based algorithms in
unittesting,suchasrandomsearch[ 26],wholesuitesearch[ 26,59],
and other many-objective evolutionary algorithms [60].4.2 Case Study Systems
We evaluate our approach by applying it to two case study sys-
tems developed by IEE. Both systems contain the four self-driving
features introduced in Section 2. However since engineers had de-
velopedtwoalternativesetsofrulestoprioritizethesefeaturesand
toresolvetheirundesiredinteractions,theydevelopedtwodifferent
functionmodelsfortheintegrationcomponent(i.e., IntC).Dueto
confidentialityreasons,wedonotsharethedetailsofthe IntCmod-
els used in these two systems. Both systems are developed in Mat-
lab/SimulinkandcanbeintegratedintoPreScan,thesimulatorused
inthispaper.Werefertothese systems as SafeDrive1 andSafeDrive2.
4.3 Experimental Settings
For the genetic operators used in FITEST, we use the parameter
valuessuggestedintheliterature[ 21,29,34]:Weusethe simulated
binary crossover (SBX) with a crossover probability 0 .60, as the
recommendedintervalis[0 .45,0.95][21,29].Thegaussianmutation
changes the test inputsby adding a random value selectedfroma
normaldistribution G(μ,σ)withmean μ=0andvariance σ2=1.0.
Astheguidelinessuggest[ 34],themutationprobabilityissetto1 /l
wherelisthelengthoftestinputs(chromosomes).InFITEST,wedo
not need to manually set the population size since, as described in
Section3.4,itisdynamicallyupdatedateachgeneration.Thesearch
stopswhenalltheobjectivesarecoveredorwhenthetimeoutof12
hoursisreached.Wesetatimeoutof12hoursbecauseaswewill
discuss in Section 4.4, the search results start to stabilize and reach
a plateau within this time budget. Further, according to domain
experts, longer search time budgets are not practical.
To account for the randomness of the search algorithm, FITEST
was executed 20 times on each case study system and with each of
the three test objectives. The total duration of the experiment was
20 (repetitions)×2 (systems)×3 (test objectives) ×12 (hours) =
1440hours(60days).Allexperimentswereexecutedonthesame
machine with a 2.5 GHz Intel Core i7-4870HQ CPU and 16 GB
DDR3 memory.
We usethe number of feature interaction failures that each of the
test objectives in our study can reveal as our evaluation metric. Wecomputethismetricbyautomaticallycheckingtestcasesgenerated
by each test objective to determine whether or not they reveal a
feature interaction failure. A test case reveals a feature interaction
failure iff: (1) it violates some system safety requirement in Table 1
whenitisappliedtoasystemconsistingofmultiplefeatures,but(2)
it does not violate that same safety requirement when it is applied
tothefeatureresponsibleforthesatisfactionofthatrequirement.
Specifically,atestcase tcrevealsafeatureinteractionif FDi(tc)=0
for some safety requirement iwhentcis applied to SafeDrive1
orSafeDrive2, but FDi(tc)>0 whentcis applied to the feature
responsible for requirement i.
4.4 Results
In this section, we answer our research question by comparingHybrid,FailandCovtest objectives. Specifically, we run FITEST
withHybrid,FailandCovastestobjectivesseparatelyandrepeat
eachrun for20 times.Figures 5(a)and (b)comparethe numberof
feature interaction failures identified over different runs of FITEST
withHybrid,FailandCovapplied to SafeDrive1 and SaveDrive2,
150
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Testing Autonomous Cars for Feature Interaction Failures ASE ’18, September 3–7, 2018, Montpellier, France
48 02 6 1 0 1 2
Time (h)(a) SafeDrive1Number of feature interaction failures02810
46
Hybrid (mean)
Fail (mean)Cov (mean)
(b) SafeDrive2
02810
46
Figure 5: The number of feature interaction failures found
byHybrid, Failand Covover time for (a) SafeDrive1 and (b)
SafeDrive2 systems.
respectively. We show the results at every one-hour interval from
0to12h.Asshowninthetwofigures,theaveragenumberoffea-
ture interaction failures computed using Hybridis always larger
than those identified by FailandCov. Specifically, after 12h, on
average,Hybridisabletofind5.9and7.2featureinteractionfail-
uresforSafeDrive1andSaveDrive2,respectively.Incontrast, Fail
uncovers, on average, 2.1 and 2.8 feature interaction failures for
SafeDrive1andSaveDrive2,respectively;and Covonlyuncovers,on
average, 0.4 and 1.8 feature interaction failures for SafeDrive1 and
SaveDrive2, respectively. Further, after executing the algorithms
for 10h, the results obtained by the three test objective alternatives
reach a plateau.
NotethateveryrunofFITESTwith Hybrid,FailandCovachieved
100% branch coverage on the function model of the integration
component(i.e., IntC)forboth SafeDrive1 andSafeDrive2.Hence,
FailandCov, despite being able to exercise all branches of IntC,
performpoorlyintermsofthenumberoffeatureinteractionfailures
that they can reveal. Further, we note that, among the Hybrid,
FailandCovtest objectives, only Covwas fully achieved by the
generated test suites, while the HybridandFailtest objectives
wereonlypartiallyachieved.Thisisexpectedsince,asdiscussed
in Section 3.4,HybridandFailsearch for violations of every safety
requirement at every branch of IntC. Some of these testobjectives
maybeinfeasible(uncoverable)becausenotallsafetyrequirementsmaybeviolatedateverybranchof
IntC.However,wecannotknow
aprioriwhichobjectivesareinfeasible,andhence,weincludeall
of them in our search.
We compare the results in Figure 5using a statistical test. Fol-
lowing existing guidelines [ 10], we use the non-parametric pair-
wise Wilcoxon rank sum test [ 27] and the Vargha-Delaney’s ˆA12
effect size [ 71]. Table2reports the results of the statistical tests ob-Table2:Statisticaltestresultscomparingthenumberoffea-
tureinteractionfailuresfoundby Hybrid, Failand Covover
time for SafeDrive1 and SafeDrive2 systems (see Figure 5).
SafeDrive1 SafeDrive2
Hybridvs.Cov Hybridvs.Fail Hybridvs.Cov Hybridvs.Fail
timep-value ˆA12p-value ˆA12p-value ˆA12p-value ˆA12
1hNA0.5 (N) NA0.5 (N) NA0.5 (N) NA0.5 (N)
2h0.663 0.53 (N) 0.663 0.53 (N) 0.330.58 (S) 0.330.58 (S)
3h8.83e-6 0.89 (L) 5.16e-5 0.86 (L) 0.003 0.77 (L) 0.009 0.73 (L)
4h7.02e-8 0.98 (L) 4.68e-6 0.91 (L) 1.97e-7 0.97 (L) 5.27e-7 0.95 (L)
5h3.08e-8 0.99 (L) 4.71e-7 0.95 (L) 9.97e-8 0.99 (L) 1.65e-7 0.98 (L)
6h3.2e-8 1 (L) 1.43e-7 0.98 (L) 7.14e-8 0.99 (L) 1.0e-7 0.98 (L)
7h3.32e-8 1 (L) 1.02e-7 0.98 (L) 5.52e-8 0.99 (L) 6.65e-8 0.99 (L)
8h3.25e-8 1 (L) 7.78e-8 0.99 (L) 5.40e-8 1 (L) 4.74e-8 1 (L)
9h2.9e-8 1 (L) 4.3e-8 1 (L) 5.54e-8 1 (L) 4.86e-8 1 (L)
10h2.84e-8 1 (L) 4.16e-8 1 (L) 5.58e-8 1 (L) 4.98e-8 1 (L)
11h2.96e-8 1 (L) 4.4e-8 1 (L) 5.58e-8 1 (L) 4.98e-8 1 (L)
12h2.96e-8 1 (L) 4.23e-8 1 (L) 5.58e-8 1 (L) 4.98e-8 1 (L)
tained when comparing the number of feature interaction failures
uncovered by Hybrid,FailandCov, over time for SafeDrive1 and
SafeDrive2.Asshowninthetable,the p-valuesrelatedtotheresults
produced when the search time ranges between 3h and 12h are all
lowerthan0.05andthe ˆA12statisticsshowlargeeffectsizes.Hence,
the number of feature interaction failures obtained by Hybridis
significantly higher (with a large effect size) than those obtained
byFailandCov.
Theanswerto RQisthatourproposedtestobjectives(Hybrid )
reveals significantly more feature interaction failures compared
to coverage-based and failure-based test objectives. In particular,
onaverage, Hybrididentifiesmorethantwiceasmanyfeature
interaction failures as the coverage-based and failure-based test
objectives.
Feedback from domain experts. We conclude this section by
summarizing the qualitative feedback of the domain experts from
IEE with whom we have been collaborating on the research pre-
sented in this paper. During two meetings, we presented to our
domain experts four test scenarios revealing different feature in-
teractionfailures.Thefourtestscenarioswereselectedrandomly
among the ones detected by our approach. Each test scenario tc
waspresentedbyshowing:(1)avideosimulationof tcgenerated
by PreScanbased onone ofour casestudy systems(SafeDrive1 or
SafeDrive2 ) and violating one of the safety requirements in Table 1
and (2) a video simulation of tcgenerated by PreScan based on
running only the feature related to the violated requirement. Note
that since tcreveals a feature interaction failure, the latter simu-
lationvideos(i.e.,theonesbasedonrunningindividualfeatures)
do not exhibit any requirements violation. After presenting the
simulations, we discussed with our domain experts each failure, its
root causes and whether or how it can be addressed by modifying
thecurrentfeatureinteractionresolutionrulesimplementedin IntC.
Wedrewthefollowingconclusionsfromourdiscussions:(1)Our
domainexpertsagreedwithusthatthefourfailuresweredueto
interactions between the features and were not caused by faults in
individual features, (2) they confirmed that the failures were not
previouslyknowntothemand(3)theyidentifiedwaystomodify
or extend the integration component (IntC ) to avoid the failures.
The simulations and the detailed failure descriptions used in our
meetings are available online [2].
151
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Ben Abdessalem, A. Panichella, S. Nejati, L. C. Briand, and T. Stifter
5 RELATED WORK
In this section, we discuss and compare with different strands of
relatedresearchintheareasoftestingautonomouscars,andtesting
and model checking feature-based systems.
Testing autonomous cars. Search-based approaches have been
usedforblack-boxtestingofdriver-assistancefeatures[ 13,14,22,
23].BühlerandWegeneruseasingle-objectivesearchalgorithmto
testavehicle-to-vehiclebrakingassistance[ 23]andanautonomous
parkingfeature[ 22].BenAbdessalemet.al.relyonmulti-objective
search [13] and learnable evolutionary algorithms [ 14] to generate
test cases violating safety requirements of self-driving systems. Re-
cently,Tianet.al.[ 68]proposedanotionofneuroncoverageand
used it to guide the generation of tests for neural networks used in
autonomouscars.Noneoftheseapproachesstudythefeaturein-
teractionprobleminautonomouscars.Weadvancetheresearchon
testingautonomouscarsbydevisingtestobjectivesthatspecifically
detect featureinteraction failures.Our testobjectives combineex-
isting software testing heuristics (i.e., branch-coverage [ 38,55,69]
and failure-based [ 4,13,20,23]) with our proposed unsafe over-
ridingheuristic.Further,wetailorexistingmany-objectivesearch
algorithms [ 59,60] to detect feature interaction failures in our con-
text.Feature interactions in softwareproduct lines.
In the context
of software product lines (SPL), testing approaches are proposed
toensureproductimplementationssatisfytheirfeaturespecifica-
tions[50,58,61].Theseapproacheslargelyfollowamodel-based
testingparadigm[ 6].Forexample,theyusecombinatorialtesting
todrivetestcasesandoraclesfromfeaturemodelstoverifyindivid-
ualproducts[ 58,61].Ourwork,incontrast,ismodeltesting[ 19].
Specifically, we take advantage of the availability of executablefunction models and test executable function models of the sys-tem and its environment. Further, in contrast to the SPL testing
work,ourapproachdoesnotneeddescriptionsoffeaturesandtheir
dependencies to be provided.
Some SPL approaches are proposed to automatically derive fea-
ture dependencies specifying valid feature combinations [ 7,36,46].
For example, interactions between observable feature behaviors
(i.e.,externalfeatureinteractions[ 7])havebeenidentifiedbystatic
analysisofsoftwarecode[ 36,46].Incontrast,ourapproachdetects
feature interactions prior to any software coding. It dynamically
detectsundesiredfeatureinteractionsbytestingfunctionmodels
capturing the SUT and its environment.Featureinteractiondetectionviamodelchecking.
Severalap-
proaches are proposed to detect feature interactions by model
checking requirements or design artifacts against formal specifica-
tions [8,11,45,62,65]. For example, Apel et. al. [ 8] verify features
describedinaformalfeature-orientedlanguageagainsttemporal
logic properties [ 28]. Arora et. al. verify features defined as state
machinesagainstlivesequencechartsspecifications.Dominguez
et. al. [45] verify features captured as StateFlows, and Sobotka and
J. Novak [ 65] specify features in timed automata [ 5]. Similar to
ourwork,theseapproachesverifyearlyrequirementsanddesign
models against system requirements. However, our work differs
withthislineofresearchinthefollowingways:First,mostofthese
approaches identify pairwise feature interactions only. We can,
however,identifyfeatureinteractionsbetweenanarbitrarynum-
ber of features. Second, these techniques model system featuresonly. However, to analyze autonomous cars, we have to capture, in
additiontofeatures,system’ssensorsandactuators,andthesystem
environment. Third, in contrast to these approaches, our approach
does not require additional formal modeling. We take advantage oftheavailabilityoffunctionmodels,whicharedevelopedanywayin
the CPS domain, to test the system in its environment. Fourth, our
function models use numerical and continuous Matlab/Simulink
computationstocapturedynamicsofcarsandpedestrians.These
modelsarenot,ingeneral,amenabletomodelcheckingduetoscala-
bilityandincompatibilityissues[ 3,52,54].Therefore,assuggested
intherecent research ontestingCPSmodels [ 3,52,54,78],instead
ofmodelchecking,werelyonsimulation-basedtestingguidedbymeta-heuristics to analyze our function models.Featureinteractionresolution.
Severalapproachesareproposed
to devise resolution strategies to eliminate undesired feature in-
teractions,forexample,byproposingspecificfeature-orientedar-
chitectures [ 44,70], by statically prioritizing features [ 41,77]o r
usingruntimeresolutionmechanisms[ 16,76].Thesetechniquesare
complementarytoourapproach.Theycanbeusedtodevelopthe
integration component (IntC ) to resolve undesired feature interac-
tions,butourapproachisstillnecessarytotestthesystembehavior
and to determine if the proposed resolution strategy can eliminate
undesired behaviors under different environment conditions.
6 CONCLUSION
We presented a technique for detecting feature interaction failures
in the context of autonomous cars. Our technique is based on ana-
lyzing executable function models typically developed in the cyber
physical domain to specify system behaviors at early development
stages.Ourcontributionsoverpriorworkinclude:(1)castingthe
problemofdetectingundesiredfeatureinteractionsintoasearch-
basedtestingproblem,(2)definingatestguidancethatcombines
existingsearch-basedtestobjectiveswithnewheuristicsspecificallyaimedatrevealingfeatureinteractionfailures,(3)tailoringexisting
many-objectivesearchalgorithms[ 59,60]toautomaticallyreveal
featureinteractionfailuresinascalableway,and(4)evaluatingour
approach usingtwo versionsof anindustrialself-driving system
and demonstrating significant improvement in feature interaction
failure identification compared to baseline search-based testing
approaches.Finally,wenotethatourresearchwasmotivatedand
carried outin the contextof a partnershipwith IEE. Thefeedback
fromdomainexpertsfromIEEindicatesthatthedetectedfeature
interaction failures represent real faults in their systems that were
not previouslyidentified based on analysis of the systemfeatures
and their requirements.
In future, we plan to devise strategies to use feature interaction
failures to localize faults and help engineers effectively debug and
refine their feature interaction resolution strategies.
ACKNOWLEDGMENTS
WegratefullyacknowledgethefundingfromtheEuropeanResearch
Council (ERC) under the European Union’s Horizon 2020 research
and innovation programme (grant agreement No 694277) and from
IEE S.A. Luxembourg.
152
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. Testing Autonomous Cars for Feature Interaction Failures ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]2018. Matlab/Simulink. https://nl.mathworks.com/products/simulink.html.
(2018).
[2]2018. SupplementaryMaterials. https://figshare.com/s/50193ea5652147d2f036.
(2018).
[3]HoussamAbbas,GeorgiosFainekos,SriramSankaranarayanan,Franjo Ivančić,
andAartiGupta.2013. Probabilistictemporallogicfalsificationofcyber-physical
systems.ACMTransactionsonEmbeddedComputingSystems(TECS) 12,2s(2013),
95.
[4]Wasif Afzal, Richard Torkar, and Robert Feldt. 2009. A systematic review of
search-based testing for non-functional system properties. Information and
Software Technology 51, 6 (2009), 957–976.
[5]RajeevAlur.1999. Timedautomata.In ProceedingsoftheInternationalConference
on Computer Aided Verification (CAV’99). Springer, Trento, Italy, 8–22.
[6]Paul Ammann and Jeff Offutt. 2008. Introduction to Software Testing (1 ed.).
Cambridge University Press, New York, NY, USA.
[7]Sven Apel, Sergiy Kolesnikov, Norbert Siegmund, Christian Kästner, and Brady
Garvin. 2013. Exploring feature interactions in the wild: the new feature-
interaction challenge. In Proceedings of the International Workshop on Feature-
Oriented Software Development (FOSD’13). ACM, Indianapolis, USA, 1–8.
[8]SvenApel,AlexanderVonRhein,ThomasThüM,andChristianKäStner.2013.
Feature-interactiondetectionbasedonfeature-basedspecifications. Computer
Networks 57, 12 (2013), 2399–2409.
[9]AndreaArcuri.2013. Itreallydoesmatterhowyounormalizethebranchdistance
in search-based software testing. Software Testing, Verification and Reliability 23,
2 (2013), 119–147. https://doi.org/10.1002/stvr.457
[10]AndreaArcuriandLionelBriand.2014. Ahitchhiker’sguidetostatisticaltests
forassessingrandomizedalgorithmsinsoftwareengineering. SoftwareTesting,
Verification and Reliability 24, 3 (2014), 219–250.
[11]Silky Arora, Prahlad Sampath, and S Ramesh. 2012. Resolving uncertainty in
automotive feature interactions. In Proceedings of the International Requirements
Engineering Conference (RE’12). Chicago, Illinois, USA, 21–30.
[12]Johannes Bader and Eckart Zitzler. 2011. HypE: An algorithm for fast
hypervolume-based many-objective optimization. IEEE Transactions on Evo-
lutionary computation 19, 1 (2011), 45–76.
[13]RajaBenAbdessalem,ShivaNejati,LionelC.Briand,andThomasStifter.2016.
Testing advanced driver assistance systems using multi-objective search and
neural networks. In Proceedings of the International Conference on Automated
Software Engineering (ASE’16). IEEE, Singapore, 63–74.
[14]RajaBenAbdessalem,ShivaNejati,LionelC.Briand,andThomasStifter.2018.
TestingVision-BasedControlSystemsUsingLearnableEvolutionaryAlgorithms.
InProceedingsoftheInternationalConferenceonSoftwareEngineering(ICSE’18).
ACM, Gothenburg, Sweden, to appear.
[15]Johan Blom, Bengt Jonsson, and Lars Kempe. 1994. Using Temporal Logic for
ModularSpecificationofTelephoneServices.In ProceedingsoftheInternational
Workshop onFeatureInteractionsinTelecommunications Systems(FIW’94).IOS
Press, Amsterdam, Netherlands, 197–216.
[16]Cecylia Bocovich and Joanne M Atlee. 2014. Variable-specific resolutions for
featureinteractions.In ProceedingsoftheACMSIGSOFTInternationalSymposium
on Foundationsof Software Engineering (FSE’14). ACM,Hong Kong, China, 553–
563.
[17]Kenneth H. Braithwaite and Joanne M. Atlee. 1994. Towards automated de-tectionof featureinteractions. In Proceedingsof theInternationalWorkshopon
Feature Interactions inTelecommunications Systems (FIW’94). IOS Press,Amster-
dam, Netherlands, 36–59.
[18]J. Bredereke. 2000. Families of formal requirements in telephone switching.
InProceedingsoftheInternationalWorkshoponFeatureInteractionsinTelecom-
munications and Software Systems(FIW’00). IOS Press, Glasgow, Scotland, UK,
257–273.
[19]Lionel Briand, Shiva Nejati, Mehrdad Sabetzadeh, and Domenico Bianculli. 2016.
Testing the untestable: model testing of complex software-intensive systems. In
ProceedingsoftheInternationalConferenceonSoftwareEngineeringCompanion
(ICSE’16). ACM, Austin, TX, US, 789–792.
[20]LionelCBriand,YvanLabiche,andMarwaShousha.2006. Usinggeneticalgo-
rithmsforearlyschedulabilityanalysisandstresstestinginreal-timesystems.
Genetic Programming and Evolvable Machines 7, 2 (2006), 145–170.
[21]Lionel C. Briand, Yvan Labiche, and Marwa Shousha. 2006. Using Genetic Algo-
rithms for Early Schedulability Analysis and Stress Testing in Real-time Systems.
Genetic Programming and Evolvable Machines 7, 2 (2006), 145–170.
[22]OliverBühlerandJoachimWegener.2004. Automatictestingofanautonomous
parking system using evolutionary computation. Technical Report. SAE Technical
Paper.
[23]Oliver Bühler and Joachim Wegener. 2008. Evolutionary functional testing.
Computers & Operations Research 35, 10 (2008), 3144–3160.
[24]Stan BÃĳhne, Kim Lauenroth, and Klaus Pohl. 2004. Modelling Features for
Multi-CriteriaProduct-LinesintheAutomotiveIndustry..In Proceedingsofthe
InternationalWorkshoponSoftwareEngineeringforAutomotiveSystems(SEAS’04),
co-located at ICSE’04. Edinburgh, UK, 9–16.[25]Muffy Calder, Mario Kolberg, Evan H Magill, and Stephan Reiff-Marganiec. 2003.
Featureinteraction:acriticalreviewandconsideredforecast. ComputerNetworks
41, 1 (2003), 115–141.
[26]José Campos, Yan Ge, Gordon Fraser, Marcelo Eler, and Andrea Arcuri. 2017. An
EmpiricalEvaluationofEvolutionaryAlgorithmsforTestSuiteGeneration.In
Proceedings of the International Symposium on Search Based Software Engineering
(SSBSE’17). Paderborn, Germany, 33–48.
[27]J. Anthony Capon. 1991. Elementary Statistics for the Social Sciences: Study Guide.
Wadsworth Publishing Company, Belmont, CA, USA.
[28]EdmundM.Clarke,Jr.,OrnaGrumberg,andDoronA.Peled.1999. ModelChecking.
MIT Press.
[29] Helen G. Cobb and John J. Grefenstette. 1993. Genetic Algorithms for Tracking
ChangingEnvironments.In ProceedingsoftheInternationalConferenceonGenetic
Algorithms(ICGA’93).Morgan KaufmannPublishers,San Francisco,CA,USA,
523–530.
[30] Kalyanmoy Deb. 1995. Simulated binary crossover for continuous search space.
Complex systems 9 (1995), 115–148.
[31]Kalyanmoy Deb. 2014. Multi-objective Optimization. In Search Methodologies.
Springer US, 403–449. https://doi.org/10.1007/978-1-4614-6940-7_15
[32]Kalyanmoy Deb and Debayan Deb. 2014. Analysing Mutation Schemes for Real-
parameter Genetic Algorithms. International Journal of Artificial Intelligence and
SoftComputing 4,1(Feb2014),1–28. https://doi.org/10.1504/IJAISC.2014.059280
[33]Kalyanmoy Deb and Himanshu Jain. 2014. An evolutionary many-objective opti-
mizationalgorithmusingreference-point-basednondominatedsortingapproach,
part I: Solving problems with box constraints. IEEE Transactions on Evolutionary
Computation 18, 4 (2014), 577–601.
[34]Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and T. Meyarivan. 2000. A
FastElitistMulti-ObjectiveGeneticAlgorithm:NSGA-II. IEEETransactionson
Evolutionary Computation 6 (2000), 182–197.
[35]StefanFerber,JürgenHaag,andJuhaSavolainen.2002. Featureinteractionand
dependencies: Modeling features for reengineering a legacy product line. In
ProceedingsoftheInternationalConferenceonSoftwareProductLines(SPLC’02).
Springer, San Diego, CA, USA, 235–256.
[36]Gabriel Ferreira, Christian Kästner, Jürgen Pfeffer, and Sven Apel. 2015. Charac-
terizing complexity of highly-configurable systems with variational call graphs:
analyzing configuration options interactions complexity in function calls. In
ProceedingsoftheSymposiumandBootcampontheScienceofSecurity(HotSoS’15).
ACM, Urbana, IL, USA, 17.
[37]K.FislerandS.Krishnamurthi.2005. DecomposingVerificationbyFeatures.In
Proceedings of the International Conference on Verified Software: Theories, Tools
and Experiments (VSTTE’05). Zurich, Switzerland.
[38]Gordon Fraser and Andrea Arcuri. 2013. Whole test suite generation. IEEE
Transactions on Software Engineering 39, 2 (2013), 276–291.
[39]GordonFraserandAndreaArcuri.2015. 1600faultsin100projects:automatically
findingfaultswhileachievinghighcoveragewithEvoSuite. EmpiricalSoftware
Engineering 20, 3 (2015), 611–639.
[40]Mark Harman,S.Afshin Mansouri,and YuanyuanZhang. 2012. Search-based
SoftwareEngineering:Trends,TechniquesandApplications. Comput.Surveys
45, 1, Article 11 (Dec 2012), 61 pages. https://doi.org/10.1145/2379776.2379787
[41]JonathanDHayandJoanneMAtlee.2000. Composingfeaturesandresolving
interactions.In ACMSIGSOFTSoftwareEngineeringNotes(SEN’00),Vol.25.ACM,
110–119.
[42]F. Herrera, M. Lozano, and A. M. SÂůnchez. 2003. A taxonomy for the crossover
operator for real-coded genetic algorithms: An experimental study. International
JournalofIntelligentSystems 18,3(2003),309–338. https://doi.org/10.1002/int.
10091
[43]IEE. 2018. International Electronics & Engineering. https://www.iee.lu/. (2018).
[44]M. Jackson and P. Zave. 1998. Distributed Feature Composition: a Virtual Archi-
tecture for Telecommunications Services. IEEE TSE 24, 10 (1998), 831–847.
[45]AlmaLJuarez-Dominguez,NancyADay,andJeffreyJJoyce.2008. Modelling
feature interactions in the automotive domain. In Proceedings of the International
WorkshoponModelinginSoftwareEngineering(MISE’08).ACM,Leipzig,Germany,
45–50.
[46]SergiyKolesnikov,NorbertSiegmund,ChristianKästner,andSvenApel.2017.
On the Relation of External and Internal Feature Interactions: A Case Study.
arXiv preprint arXiv:1712.07440 (2017).
[47]Bogdan Korel. 1990. Automated software test data generation. IEEE Transactions
on Software Engineering 16, 8 (1990), 870–879.
[48]BingdongLi,JinlongLi,KeTang,andXinYao.2015. Many-objectiveevolutionary
algorithms: A survey. ACM Computing Surveys (CSUR) 48, 1 (2015), 13.
[49]Zheng Li, Mark Harman, and Robert M Hierons. 2007. Search algorithms for
regression test case prioritization. IEEE Transactions on Software Engineering 33,
4 (2007).
[50]Malte Lochau, Sebastian Oster, Ursula Goltz, and Andy Schürr. 2012. Model-
basedpairwisetestingforfeatureinteractioncoverageinsoftwareproductline
engineering. Software Quality Journal 20, 3-4 (2012), 567–604.
[51]Sean Luke. 2013. Essentials of Metaheuristics (second ed.). Lulu, Fairfax, Virginie,
USA.https://cs.gmu.edu/$\sim$sean/book/metaheuristics/
153
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France R. Ben Abdessalem, A. Panichella, S. Nejati, L. C. Briand, and T. Stifter
[52]R.Matinnejad,S.Nejati,L.Briand,andT.Bruckmann.2018. TestGenerationand
TestPrioritizationforSimulinkModelswithDynamicBehavior. IEEETransactions
on Software Engineering (2018), to appear.
[53]RezaMatinnejad,ShivaNejati,LionelBriand,ThomasBruckmann,andClaude
Poull.2015. Search-basedautomatedtestingofcontinuouscontrollers:Frame-
work, tool support, and case studies. Information and Software Technology 57
(2015), 705–722.
[54]RezaMatinnejad,ShivaNejati,LionelCBriand,andThomasBruckmann.2016.
Automated test suite generation for time-continuous simulink models. In Pro-
ceedings of the International Conference on Software Engineering (ICSE’16). ACM,
Austin, TX, US, 595–606.
[55]PhilMcMinn.2004.Search-basedsoftwaretestdatageneration:asurvey. Software
testing, Verification and reliability 14, 2 (2004), 105–156.
[56]PhilMcMinn.2004.Search-basedsoftwaretestdatageneration:asurvey. Software
Testing Verification and Reliability Journal 14, 2 (2004), 105–156.
[57] N. S. Nise. 2004. Control Systems Engineering, 4th ed. John-Wiely Sons.
[58]SebastianOster,MariusZink,MalteLochau,andMarkGrechanik.2011. Pairwise
feature-interaction testing for SPLs: potentials and limitations. In Proceedings
ofthe InternationalSoftwareProduct LineConference,Volume2(SPLC’11).ACM,
Munich, Germany, 6.
[59]Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2015. Re-
formulating Branch Coverage as a Many-Objective Optimization Problem. In
ProceedingsoftheInternationalConferenceonSoftwareTesting,Verificationand
Validation, (ICST’15). Graz, Austria, 1–10.
[60]Annibale Panichella, Fitsum Mesheha Kifetew, and Paolo Tonella. 2018. Auto-
mated Test Case Generation as a Many-Objective Optimisation Problem with
Dynamic Selection of the Targets. IEEE Transactions on Software Engineering 44,
2 (Feb 2018), 122–158.
[61]SachinPatel,PriyaGupta,andVipulShah.2013. Featureinteractiontestingof
variability intensive systems. In Proceedings of the International Workshop on
ProductLineApproachesinSoftwareEngineering(PLEASE’13).IEEE,SanFrancisco,
CA, USA, 53–56.
[62]Malte Plath and Mark Ryan. 2001. Feature integration using a feature construct.
Science of Computer Programming 41, 1 (2001), 53–84.
[63]C.Prehofer.1997.Feature-OrientedProgramming:AFreshLookatObjects.In Pro-
ceedingsoftheEuropeanConferenceonObject-OrientedProgramming(ECOOP’97) .
JyvÃďskylÃď, Finland, 419–443.
[64]JoséMiguelRojas,MattiaVivanti,AndreaArcuri,andGordonFraser.2017. A
detailed investigation of the effectiveness of whole test suite generation. Em-
pirical Software Engineering 22, 2 (2017), 852–893. https://doi.org/10.1007/s10664-015-9424-2
[65]Jan Sobotka and Jiri Novak. 2013. Automation of automotive integration testing
process. In Proceedings of the International Conference on Intelligent Data Acquisi-
tionandAdvancedComputingSystems(IDAACS’13),Vol.1.IEEE,Berlin,Germany,
349–352.
[66]ThorstenSuttorpandChristianIgel.2006.Multi-objectiveoptimizationofsupport
vector machines. In Multi-objective machine learning. Springer, -, 199–220.
[67]TASS-International. 2018. PreScan. https://www.tassinternational.com/prescan.
(2018).
[68]YuchiTian,KexinPei,SumanJana,andBaishakhiRay.2018.DeepTest:Automated
Testing of Deep-Neural-Network-driven Autonomous Cars. In Proceedings of the
InternationalConferenceonSoftwareEngineering(ICSE’18).ACM,Gothenburg,
Sweden, to appear.
[69]PaoloTonella.2004. Evolutionarytestingofclasses.In ProceedingsoftheACM
SIGSOFT International Symposium on Software Testing and Analysis (ISSTA’04) ,
Vol. 29. ACM, Boston, MA, USA, 119–128.
[70]Rob van der Linden. 1994. Using an architecture to help beat feature interaction.
InProceedingsoftheInternationalWorkshoponFeatureInteractionsinTelecom-
munications Systems (FIW’94). IOS Press, Amsterdam, Netherlands, 24–35.
[71]András Vargha and Harold D. Delaney. 2000. A critique and improvement of
the CL common language effect size statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (2000), 101–132.
[72]GabrielAWainer.2009. Discrete-eventmodelingandsimulation:apractitioner’s
approach. CRC press.
[73]ShinYooandMarkHarman.2007. Paretoefficientmulti-objectivetestcaseselec-
tion.InProceedingsofttheACMSIGSOFTInternationalSymposiumonSoftware
Testing and Analysis (ISSTA’07). ACM, London, UK, 140–150.
[74]JustynaZander,InaSchieferdecker,andPieterJMosterman.2017. Model-based
testing for embedded systems. CRC press.
[75]Pamela Zave. 1993. Feature interactions and formal specifications in telecommu-
nications. Computer 26, 8 (Aug 1993), 20–28.
[76]MHadiZibaeenejad,ChiZhang,andJoanneMAtlee.2017. Continuousvariable-
specific resolutions of feature interactions. In Proceedings of the Joint Meeting on
FoundationsofSoftwareEngineering(ESEC/FSE’17).ACM,Paderborn,Germany,
408–418.
[77]PAnnZimmerandJoanneMAtlee.2012. Orderingfeaturesbycategory. Journal
of Systems and Software 85, 8 (2012), 1782–1800.
[78]PaoloZuliani,AndréPlatzer,andEdmundMClarke.2013. Bayesianstatistical
model checking with application to Stateflow/Simulink verification. Formal
Methods in System Design 43, 2 (2013), 338–367.
154
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:23:07 UTC from IEEE Xplore.  Restrictions apply. 