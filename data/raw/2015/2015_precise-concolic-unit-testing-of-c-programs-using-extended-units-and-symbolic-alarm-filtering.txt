Precise Concolic Unit Testing of C Programs using
Extended Units and Symbolic Alarm Filtering
Yunho Kim
School of Computing, KAIST
Daejeon, South Korea
yunho.kim03@gmail.comYunja Choi
School of Computer Science and
Engineering, Kyungpook Natl. Univ.
Daegu, South Korea
yuchoi76@knu.ac.krMoonzoo Kim
School of Computing, KAIST
Daejeon, South Korea
moonzoo@cs.kaist.ac.kr
ABSTRACT
Automated unit testing reduces manual effort to write unit test
drivers/stubsandgenerateunittest inputs.Ho wever,automatically
generated unit test drivers/stubs raise false alarms because they
often over-approximate real contexts of a target function fand
allow infeasible executions of f. To solve this problem, we have
developed a concolic unit testing technique CONBRIO. To provide
realisticcontextto f,itconstructsan extendedunit offthatcon-
sists offandclosely relevant functions to f. Also, CONBRIO filters
out a false alarm by checking feasibility of a corresponding sym-bolic execution path with regard to
fâ€™ssymbolic calling contexts
obtained by combining symbolic execution paths of fâ€™s closely
related predecessor functions.
Intheexperimentsonthecrashbugsof15real-worldCprograms,
CONBRIOshowsbothhighbugdetectionability(i.e.91.0%ofthe
targetbugsdetected)andhighprecision(i.e.atruetofalsealarm
ratio is 1:4.5). Also, CONBRIO detects 14 new bugs in 9 target C
programs studied in papers on crash bug detection techniques.
CCS CONCEPTS
â€¢Software and its engineering â†’Software testing and de-
bugging;
ACM Reference Format:
Yunho Kim, Yunja Choi, and Moonzoo Kim. 2018. Precise Concolic Unit
Testing of C Programs using Extended Units and Symbolic Alarm Filtering .
InICSEâ€™18: ICSE â€™18:40th InternationalConference onSoftware Engineering
, May 27-June 3, 2018, Gothenburg, Sweden. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3180155.3180253
1 INTRODUCTION
AlthoughunittestingiseffectivetodetectSWbugs,fieldengineers
have burden of manually generating test drivers/stubs and test
inputsforeachtargetunit.Toreducemanualefforttogeneratetest
inputs,automatedtestgenerationhasbeenapplied(e.g.,concolic
testing have been appliedto detect bugs in open source programs
[2â€“4,27,31,39] and industrial projects [ 6,17,23,24,33,45]a t
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
Â©2018 Copyright held by the owner/author(s). Publication rights licensed to the
Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180253system-level). Also, to reduce manual effort to generate unit test
drivers/stubs, automated unit testing has been applied to open
source programs [10, 14, 37] and large industrial SW [28].
Amaindrawbackoftheautomatedunittestingisalargenumber
offalse alarms raised by infeasible unit executions (i.e. unit executi-
onsthatareinfeasibleatsystem-level).Infeasibleunitexecutions
occur generated due to inaccurate unit test drivers/stubs that over-
approximate real contexts of a target unit (Sect. 2.4). This false
alarmproblemisaseriousobstacletoapplyautomatedunittesting
in practice since field engineers would not like to spend time to
manually filter out many false alarms.
Toovercomethislimitation,wehavedevelopedanautomated
unit testing framework CONBRIO ( CONcolic unit testing with
sym-Bolic alaRmfIltering using symbolic calling c Ontexts) which
operates in the following two stages:
1.To provide realistic context to a target function f, CONBRIO
constructs an extended unit offthat consists of fandclosely
relevantfunctions to fwhich can filter out infeasible unit execu-
tions caused by symbolic stubs. The relevance of a function Ğ´to
fismeasuredbythedegreeofdependencyof fonĞ´(Sect.3.2).
Then, CONBRIO performs concolic execution of an extended
unit off.
2.Tofilteroutfalsealarmsbycheckingfeasibilityofacorrespon-
dingsymbolicunitexecutionof f,CONBRIOgenerates symbolic
callingcontext offbycombining symbolic pathsof closelyrele-
vantpredecessor functions of fin a static call graph.
Asaresult,CONBRIOdetectsbugseffectivelyandpreciselybecause
itenforcesvariousandrealisticexecutionsof fthroughconcolic
executionof fwithfâ€™srealisticcontexts(i.e.,withthefunctions
closelyrelevantto f)andaccuratelyfiltersoutfalsealarmsusing
fâ€™s symbolic calling contexts.
Note that it is important to construct an extended unit and sym-
boliccallingcontextof ftocontain onlyfunctions closelyrelevant to
fsinceincludingmorefunctionswillenlargesymbolicsearchspace
anddegrade unittesting effectivenessandefficiency. Forexample,
at one extreme end, an extended unit may contain all successorfunctions of
fand fail to detect bugs due to too large symbolic
search space to explore. Also, symbolic calling context of fmay
contain symbolic execution paths of all predecessor functions of f
up to mainand fail to detect bugs.1
We have applied CONBRIO to 15 real-world C programs in
SIR[11]andSPEC2006[ 41]benchmarksandCONBRIOshowsboth
1Generatedsymboliccallingcontextmay notrepresentallfeasiblecallingcontextof f
due to the limitation of symbolic execution. Thus, a symbolic calling context becomes
more difficult to satisfy by adding symbolic execution paths of more predecessor
functions of f(i.e., via logical âˆ§. See Sect. 3.5).
3152018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Yunho Kim, Yunja Choi, and Moonzoo Kim
high bug detection ability (i.e., 91.0% of all target bugs detected)
and high precision (i.e., a true to false alarm ratio is 1:4.5) which
is more precise than the latest concolic unit testing techniques for
Cprograms(e.g.,1:5.7byUC-KLEE[ 37]).Also,CONBRIOdetects
14newbugs in the latest versions of the nine target C programs
studied in other papers on crash bug detection techniques.
The contributions of this paper are as follows:
â€¢CONBRIO achieves both high bug detection ability (91.0% of the
targetbugsdetected)andhighprecision(falsealarmratiois1:4.5)based on the two core ideas: 1) building and utilizing contexts of
a target function explicitly based on relevance of functions mea-
sured by a function dependency metric, 2) a new alarm filtering
strategy that constructs symbolic calling contexts compositio-nally and utilizes them to check feasibility of a violating unit
execution.
â€¢The extensive empirical evaluation on both bug detection ability
andprecisionofCONBRIOandtheotherconcolicunittestingtechniques on the 15 real-world C programs supports resear-
chers and practitioners to learn the pros and cons of the related
techniques (Sect. 4â€“5).
â€¢ByapplyingCONBRIO,wehavedetectedandreported14new
crash bugs in the latest versions of the 9 target programs that
were studied in other papers on crash bug detection techniques
(Sect. 5.5).
â€¢Wehavemadethereal-worldcrashbugdataoftheCbenchmarkprogramspubliclyavailable,whichwerecollectedandorganized
after examining the bug reports of the last 12â€“24 years (http:
//swtv.kaist.ac.kr/tools/conbrio),sothatresearcherscanusethem
for various testing research purposes (Sect. 4.2.1).
The remainder of the paper is as follows. Section 2 explains the
background of automated concolic unit testing. Section 3 describes
the detail of CONBRIO. Section 4 explains the experiment setupto evaluate CONBRIO compared to other techniques. Section 5reports the experiment results. Section 6 discusses related work
and Section 7 concludes the paper with future work.
2 BACKGROUND
2.1 Preliminary
Unit testing uses driversandstubs(or mock objects) to test a target
function in isolation (i.e., without the rest of a target program).
Suppose that a target function under test ftakesnarguments
a1,...anandaccesses mglobalvariables v1,...vm,anddirectlycalls
lotherfunctions Ğ´1,...Ğ´l.Toenforcediversetestexecutionsof f,
a tester develops various unit test drivers drvf
is each of which
generatesargumentvalues ai
1,...ain,globalvariablevalues vi
1,...vim
and finally invokes fwith these input values. Also, a tester builds
stubfunctions siĞ´1...siĞ´ltoreplace Ğ´1,...Ğ´l.Also,testdrivers/stubs
should satisfy constraints on the interface between fand the rest
of a target program to avoid infeasible unit test executions of f.
2.2 Concolic Unit Test Driver/Stub Generation
Foreachtargetfunction f,aconcolicunittestingtechniqueauto-
maticallygenerates symbolic stubsanda symbolic unittestdriver.
Symbolicstubssimplyreturnsymbolicvalues(withoutupdatingglobalvariablesandoutputparametersforsimplicity)andasym-
bolic driver invokes fafter assigning symbolic values to the input
variables of faccording to their types as follows:2
â€¢primitive types: primitive variables are directly assigned with
primitive symbolic values of the corresponding types.
â€¢arraytypes: eacharrayelementisassignedwithasymbolicva-
riable according to the type of the array element (for a large
array,onlythefirst nelementsareassignedwithsymbolicvalues
wherenis given by a user).
â€¢pointertypes: forapointervariable ptrpointingtoavariableofa
type T, a driver allocates memory whose size is equal to the size
ofTand assigns the address of the allocated memory to ptr(i.e.,
ptr=malloc(sizeof(T)) ). Then, a driver assigns *ptrwith a
symbolic valueof type T. If asize of Tis not known(e.g., FILE
in standard C library), NULL is assigned to ptr. If there exists
a pointer variable ptr2pointing to a symbolic variable of the
same type T, a driver assigns ptr2toptr.
â€¢structure types: a unit test driver specifies all fields of struct
variable sas symbolic variables recursively (i.e., if scontains
structvariable t, a unit test driver specifies the fields of tas
symbolic too).
A limitation of this approach is that the drivers and stubs often
over-approximatetherealenvironmentof fandallow infeasible
unitexecutions (i.e.,executionsof fwhichare notfeasibleatsystem-
level) that may raise false alarms.
2.3 Insertion of Assertions Targeting Crash
Bugs
Concolic unit testing techniques aim to detect crashes/run-time
failures such as null-pointer dereference (NPD), array index out-of-
bounds (OOB), and divide-by-zero (DBZ) as well as violations of
user-given assertions. They often focus on crashes because user-
given assertions are usually not available in real-world programs.
Concolic unit testing techniques insert assert(exp)intof
whereexpspecifiesaconditiontoavoidcrashes(e.g., denominator /nequal
0toavoidDBZ).Becauseof assert(exp)inf,concolictestingtries
togenerateatestinputwithwhich fmakesexpfalseandincreases
a chance to detect crash bugs.
2.4 Example of False Alarm
Figure 1 shows a target program with a target function funder
test (lines 10â€“16). maincalls a1if the first parameter xofmainis
greaterthan0orcalls a2,otherwise(line3). a1anda2callbatline5
and line 6, respectively, and bcalls fat line 7. ftakes an integer
parameter xand calls g(x)(line 12) (a sanity check function for
accessing arraythroughanindex x)and h(x)(line15).Aconcolic
unit testing technique generates a unit test driver driver_f and
symbolic stubs stub_gand stub_hforf. Also, it modifies fto
callstub_gandstub_hinstead of gandhrespectively (see the
commentsatline12andline15)andinsertsanOOBassertionat
line 13.
Figure2showsaunittestdriverandstubsfor f.driver_f invo-
kesfwith a symbolic argument arg1(lines 2â€“3) where int arg1
= SYM_int() setsarg1asasymbolicintegervalue(line2). stub_g
2This subsection is excerpted from [28].
316
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. Precise Concolic Unit Testing of C Programs using
Extended Units and Symbolic Alarm FilteringICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
01:// x and y are inputs of a target program
02:int main(int x,int y){
03: return (x>0) ? a1(x,y) : a2(y);}04:
05:int a1(int x, int y){if(y>0) return b(x); else return 0;}
06:int a2(int x){if(x>0) return b(x);}
07:int b(int x){if(x>0) return f(x);else return 0;}
08:09:// Target function under test
10:int f(int x){11: int array[5] = {1,3,5,7,9}, result;12: if (g(x) != 0){ //=> if (stub_g(x) != 0) {
13: // => assert(0<=x && x<5);
14: result = array[x];15: }else result=h(x);//=> else result=stub_h(x);
16: return result;}
17:18:int g(int x){ return (x<5)? 1:0;}
19:
20:int h(int x){ returnx+2 ; }
Figure 1: Target program with a target function f
01: int driver_f(){02: int arg1 = SYM_int();
03: f(arg1);}04:
05: int stub_g(int x){}
06: int ret = SYM_int();07: return ret;}
08:
09: int stub_h(int x){
10: int ret = SYM_int();
11: return ret;}
Figure 2: Generated unit test driver and stubs for f
andstub_hreturnsymbolicintegervaluesas gandhreturninteger
values(lines5â€“7andlines9â€“11respectively).Concolicexecution
ofdriver_f violates the OOB assertion at line 13 of fif a unit test
execution satisfies the following two conditions:
â€¢asymbolicargument arg1tof(line3of driver_f )islargerthan
or equal to the size of array(e.g. arg1is 5)
â€¢stub_greturns a non-zero value (e.g. 1)
However, an alarm raised in such unit test execution is a false
alarmbecause such unit test execution of fisinfeasible with the
real target program where gis invoked ( greturns 0 if arg1â‰¥5
(line 18 of Figure 1) unlike stub_g). In other words, a concolic unit
testing technique can raise a false alarm if it generates unit test
drivers/stubs different from real environment of fwhich consist of
main, a1,a2,b,g, and h.
3 CONBRIO TECHNIQUE
Figure 3 shows the overall process of CONBRIO as follows:
1.CONBRIOreceivessourcecodeofatargetprogram,alistoftarget
functions to test, and system test cases of the target program as
inputs. CONBRIO obtains function call profiles from the system
test executions (Section 3.1).2.Itchecksfunctionrelevancebycalculating dependency ofatar-
getfunction fonotherfunction Ğ´usingconditionalprobability
p(Ğ´|f)based on the observed function call profiles (Section 3.2).
With a given dependency threshold Ï„, we consider fhas ahigh
dependency onĞ´ifp(Ğ´|f)â‰¥Ï„.
3. Based on the calculated dependency of fon other functions,
â€¢Itconstructsan extendedunit offthatcontains f,fâ€™ssuccessor
functionsinastaticfunctioncallgraphonwhich fhashigh
dependency, and symbolic stubs.
â€¢It identifies calling contexts offeach of which is a maximal
call path a1â†’a2â†’...â†’fin a static function call graph
suchthat fhashighdependencyonall ais.Weuse ctx(f,k)
to indicate kth calling context of f.
Section 3.3 describes more detail.
4.CONBRIOappliesconcolictestingtoanextendedunitof ftoex-
plorediverseandrealistictargetunittestexecutions.Duringcon-
colicexecution,itbuildsasymbolicpathformula Ïƒfvithatrepre-
sents executions violating a given assertion viinf(Section 3.4).
5.Itfiltersoutanalarmraisedat vibycheckingthefeasibilityof
Ïƒfviwith regard to fâ€™scalling contexts (see Step 3). For this pur-
pose, CONBRIO constructs fâ€™ssymbolic calling context formulas
Î£ctx(f,k)and uses a SMT solver to check satisfiability of Ïƒfvi
(see Step 4) conjuncted with Î£ctx(f,k)(Section 3.5).
If the result is UNSAT for all calling contexts (i.e., there existsnofeasible execution in any calling context of
fto makeÏƒfvi
feasible), a target alarm is considered as false and ignored. Ot-
herwise (i.e., the result is SAT with at least one calling context),
a corresponding alarm is reported as a violation of viinf.
3.1 Obtaining Function Call Profile from
System Test Executions
CONBRIO executes a target program with given system test cases
and obtains function call profiles. For example, suppose that a
target program in Fig. 1 has three system test cases to main(x,y):
(-1,1),(1,1),and(5,1).Then,thefunctioncallprofilesareobtained
as follows: { mainâ†’a2 ,a2â†’b,bâ†’f,fâ†’g} with (-1,1), { mainâ†’a1 ,
a1â†’b,bâ†’f,fâ†’g} with (1,1), and { mainâ†’a1 ,a1â†’b,bâ†’f,fâ†’g,
fâ†’h} with (5,1).
3.2 Computing Dependency of a Target
Function on Other Functions
Suppose that a program has a target function fand other function
Ğ´and it has nfsystem test executions that invokes f. Based on
functioncallprofiles,wecompute dependency offonĞ´asp(Ğ´|f).
Given a static call graph G(V,E)(see Def. 1) and system test execu-
tions, we compute p(Ğ´|f)as follows:
â€¢Case 1: for Ğ´which is a predecessor offinG(V,E),p(Ğ´|f)is
calculated asn1
nfwheren1is a number of system executions
whereĞ´callsfdirectly or transitively.
â€¢Case 2: for Ğ´which is a successor offinG(V,E),p(Ğ´|f)is calcu-
lated asn2
nfwheren2is a number of system executions where f
callsĞ´directly or transitively.
â€¢Case 3: for Ğ´which is a successor andpredecessor offinG(V,E)
(i.e.,thereexistsarecursivecallcyclebetween fandĞ´),p(Ğ´|f)
317
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Yunho Kim, Yunja Choi, and Moonzoo Kim
^ÆšÄÆ‰Ï­Í—KÄÆšÄ‚ÅÅ¶ÅÅ¶ÅÄ¨ÆµÅ¶ÄÍ˜ÄÄ‚Å¯Å¯Æ‰ÆŒÅ½Ä¨ÅÅ¯ÄÄ¨ÆŒÅ½ÅµÆÇ‡ÆÆšÄÅµÆšÄÆÆšÆ ^ÆšÄÆ‰Ï®Í—Å½ÅµÆ‰ÆµÆšÅÅ¶ÅÄšÄÆ‰ÄÅ¶ÄšÄÅ¶ÄÇ‡Å½Ä¨Ä‚ÆšÄ‚ÆŒÅÄÆšÄ¨ÆµÅ¶ÄÍ˜ f
Å½Å¶Å½ÆšÅšÄÆŒÄ¨ÆµÅ¶ÄÍ˜ÆµÆÅÅ¶ÅÄÅ½Å¶ÄšÅÆšÅÅ½Å¶Ä‚Å¯Æ‰ÆŒÅ½ÄÍ˜
^ÆšÄÆ‰Ï°Í—Å½Å¶ÄÅ½Å¯ÅÄÆšÄÆÆšÅÅ¶ÅÆšÅ½ÅÄÅ¶ÄÆŒÄ‚ÆšÄÆÇ‡ÅµÄÅ½Å¯ÅÄ
Æ‰Ä‚ÆšÅšÄ¨Å½ÆŒÅµÆµÅ¯Ä‚ ÊÄ¨Ç€ÅÆšÅšÄ‚ÆšÇ€ÅÅ½Å¯Ä‚ÆšÄÆÄ‚ÅÅÇ€ÄÅ¶Ä‚ÆÆÄÆŒÆšÅÅ½Å¶ Ç€Åint main(x,y){
â€¦ a1()â€¦ a2()}
int a1(){â€¦b()â€¦}
int a2(){â€¦b()â€¦}
int b(){â€¦f()â€¦}
int f(){â€¦
g()â€¦ h()â€¦}
int g() {â€¦}int h() {â€¦}^Ç‡ÆÆšÄÅµÆšÄÆÆšÄÄ‚ÆÄÆ
^ÆšÄÆ‰Ï¯Í—Å½Å¶ÆÆšÆŒÆµÄÆšÅÅ¶ÅÄ‚Å¶ÄÇ†ÆšÄÅ¶ÄšÄÄšÆµÅ¶ÅÆšÅ½Ä¨ f
Ä‚Å¶ÄšÄ‚ÄÄ‚Å¯Å¯ÅÅ¶ÅÄÅ½Å¶ÆšÄÇ†ÆšÅ½Ä¨ fÍ¾ÊÑÏ¬Í˜Ï³Í¿
a stub_b Ä‚ÆÇ‡ÅµÄÅ½Å¯ÅÄÆÆšÆµÄÄ¨ÆµÅ¶ÄÆšÅÅ½Å¶ Ä‚ÆŒÄÄ‚Å¯Ä¨ÆµÅ¶ÄÆšÅÅ½Å¶>ÄÅÄÅ¶ÄšÄÆ‰ÄÅ¶ÄšÄÅ¶ÄÇ‡Å½Ä¨ f
Å½Å¶Å½ÆšÅšÄÆŒÄ¨ÆµÅ¶ÄÆšÅÅ½Å¶Æ
pÍ¾main|fÍ¿ÑÏ­Í˜Ï¬Ï¬
pÍ¾a1|fÍ¿ÑÏ¬Í˜Ï²Ï²
pÍ¾a2|fÍ¿ÑÏ¬Í˜Ï¯Ï¯
pÍ¾b|fÍ¿ÑÏ­Í˜Ï¬Ï¬
pÍ¾g|fÍ¿ÑÏ­Í˜Ï¬Ï¬
pÍ¾h|fÍ¿ÑÏ¬Í˜Ï¯Ï¯Å½ÅµÆ‰ÆµÆšÄ
Æ‰Í¾Í™Í®fÍ¿
f
stub_hgdriver_f&ÆµÅ¶ÄÆšÅÅ½Å¶ÄÄ‚Å¯Å¯Æ‰ÆŒÅ½Ä¨ÅÅ¯Ä
6%

6%

6%

dÄ‚ÆŒÅÄÆšÆ‰ÆŒÅ½ÅÆŒÄ‚Åµ
main
b
f
ga1main
a1
f
ghbmain
f
gb
ÄÍ—Ï­Í˜Ï¬Ï¬
Ä¨
ÅÍ—Ï­Í˜Ï¬Ï¬ ÅšÍ—Ï¬Í˜Ï¯Ï¯Ä‚Ï­Í—Ï¬Í˜Ï²Ï²ÅµÄ‚ÅÅ¶Í—Ï­Í˜Ï¬Ï¬&ÆµÅ¶ÄÆšÅÅ½Å¶ÄÄ‚Å¯Å¯Æ‰ÆŒÅ½Ä¨ÅÅ¯Ä
6%

6%

6%


main
b
f
ga1main
a1
f
ghb
YÊÄ¨Ç€Ï­
YÊÄ¨Ç€Ï®ÄÄ‚Å¯Å¯ÅÅ¶Å
ÄÅ½Å¶ÆšÄÇ†ÆšÅ½Ä¨ f
f
stub_hgdriver_f
ÊÄ¨Ç€Ï­Âš3ÄÆšÇ†Í¾Ä¨Í•Ï­Í¿ ^Dd
^Å½Å¯Ç€ÄÆŒÊÄ¨Ç€Ï®Âš3ÄÆšÇ†Í¾Ä¨Í•Ï­Í¿hE^d YÅÆÄ‚Ä¨Ä‚Å¯ÆÄÄ‚Å¯Ä‚ÆŒÅµ
^dYÅÆÄ‚ÆšÆŒÆµÄÄ‚Å¯Ä‚ÆŒÅµ^ÆšÄÆ‰Ï±Í—&ÅÅ¯ÆšÄÆŒÅÅ¶ÅÅ½ÆµÆšÄ‚Å¶Ä‚Å¯Ä‚ÆŒÅµÆŒÄ‚ÅÆÄÄšÄ‚Æš Ç€ÅÄÇ‡ÄÅšÄÄÅ¬ÅÅ¶ÅÆÄ‚ÆšÅÆÄ¨ÅÄ‚ÄÅÅ¯ÅÆšÇ‡Å½Ä¨ÄÅ½ÆŒÆŒÄÆÆ‰Å½Å¶ÄšÅÅ¶Å
ÆÇ‡ÅµÄÅ½Å¯ÅÄÆ‰Ä‚ÆšÅšÄ¨Å½ÆŒÅµÆµÅ¯Ä‚ ÊÄ¨Ç€ÅÇÅÆšÅšfÍ›ÆÆÇ‡ÅµÄÅ½Å¯ÅÄÄÄ‚Å¯Å¯ÅÅ¶ÅÄÅ½Å¶ÆšÄÇ†Æš 3ÄÆšÇ†Í¾Ä¨Í•Å¬Í¿
/ÅÅ¶Å½ÆŒÄ
ZÄÆ‰Å½ÆŒÆš^Ç‡ÅµÄÅ½Å¯ÅÄ
ÄÇ†ÄÄÆµÆšÅÅ½Å¶
Æ‰Ä‚ÆšÅšÆdriver_f
Ç†ÆšÄÅ¶ÄšÄÄš
ÆµÅ¶ÅÆšÅ½Ä¨ fÄ‚Ï®Í—Ï¬Í˜Ï¯Ï¯a2main
f
gba2
Y
Figure 3: Overall process of CONBRIO
iscalculatedasn3
nfwheren3isanumberofsystemexecutions
wherefcallsĞ´orĞ´callsfdirectly or transitively.
Forexample,Step1ofFig.3showsthreetestcases(-1,1),(1,1),and
(5,1) and their corresponding function call profiles for the program
inFig.1.Basedontheprofiles,wecalculateddependencyof fon
other functions as follows:
â€¢p(main|f)=1.00(=n1
nf=3
3)
â€¢p(a1|f)=0.66(=n1
nf=2
3)
â€¢p(a2|f)=0.33(=n1
nf=1
3)
â€¢p(b|f)=1.00(=n1
nf=3
3)
â€¢p(g|f)=1.00(=n2
nf=3
3)
â€¢p(h|f)=0.33(=n2
nf=1
3)
3.3 Constructing Extended Unit and Calling
Contexts
Given a static call graph G(V,E)of a target program (Def.1), a
targetfunction fâ€™sdependencyonotherfunctions(i.e., p(Ğ´|f)),and
adependencythreshold Ï„,CONBRIOconstructsan extendedunitoffthat consists of fandfâ€™s closely relevant successor functions
andcalling contexts off.
Definition 1. A static call graph G(V,E)is a directed graph
whereVis a set of nodes representing functions in a program and
Eisarelation VÃ—V.Eachedge (a,b)âˆˆEindicatesthat adirectly
callsb.W ecallanode pasapredecessor offifthereexistsapath
fromptof.W ec a l lan o d e sas asuccessor offif there exists a
path from ftos.
Forexample,Step3ofFig.3showshowCONBRIOconstructsan
extendedunitof fandacallingcontextof fforaprograminFig.1.
Givenastaticcallgraphwhosenodesarelabelledwithdependency
off,CONBRIOconstructsanextendedunitof fthatcontains fand
gsince fhashighdependencyon g,butnot h(i.e.,p(g|f)â‰¥Ï„but
p(h|f)<Ï„whereÏ„=0.7).Finally, driver_f invokesanextended
unit of fwith symbolic inputs. Note that CONBRIO does notraise
afalsealarminthisexampleunlikeconcolicunittestinginSect.2.4
because an extended unit provides realistic environment to fby
using gwhich is closely relevant to f. Also, CONBRIO builds a
calling context of fasbâ†’fsince fhas high dependency on b, but
nota1nora2(i.e.,p(b|f)â‰¥Ï„butp(a1|f),p(a2|f)<Ï„).
318
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. Precise Concolic Unit Testing of C Programs using
Extended Units and Symbolic Alarm FilteringICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
3.3.1 Constructing Extended Unit. For each target function f,
CONBRIO constructs an extended unit that contains fandfâ€™s
successor functions Ğ´such that fhas high dependency on all
function nodes in a call path from ftoĞ´in a static function call
graph (i.e. for all nodes nibetween fandĞ´,p(ni|f)â‰¥Ï„). A unit
testdriversetsallargumentsandallglobalvariablesaccessedby
the extended unit of fas symbolic inputs as described in Sect. 2
and invokes f.
For example, Fig. 4 shows a static call graph whose nodes are
labeledwithdependencyofatargetfunction f.Fig.4showsthatan
extendedunitof f(markedwithblackdashedlineatthebottom)
consists of f,n12,n13, andn14 functions on which fhas high
dependency (i.e. p(n12|f),p(n13|f),p(n14|f)â‰¥Ï„=0.7).
In addition, as a false alarm reduction heuristic, CONBRIO adds
SYM_assume( expr )3at thebeginning of fâ€™s extendedunit where
exprrepresentspossiblevaluerangesofsymbolicinputvariables
(which are obtained by applying a static value range analyzer [ 36]
to an entire target program code). If an input value is not in the
estimatedrange,acurrenttestexecutionimmediatelyterminates
without raising any alarms and CONBRIO continues to a next test
execution. As another heuristic, CONBRIO constructs an extended
unit to keep consistency between a pointer input variable to dyna-
micallyallocatedmemoryanditssizevariablebyfiguringoutsuch
relation between input variables based on the variable names.
3.3.2 Constructing Calling Contexts. Inastaticcallgraph G(V,E)
labelled with dependency of f, we define a calling context offas a
maximal call path from a predecessor node of ftofas follows.
Definition 2. An ithcalling context offâˆˆV(sayingctx(f,i))
is a maximal call path a1â†’a2â†’...â†’fin a static call graph
G(V,E)satisfying the following conditions:
â€¢a1is a predecessor of f
â€¢for allajinctx(f,i),p(aj|f)â‰¥Ï„
â€¢thereexistsnoothercallingcontextof fthatcontains ctx(f,i)
as its sub path (i.e., ctx(f,i)is maximal).
CONBRIO generates a calling context by traversing a static call
graph from fin a reverse direction until it reaches a node labelled
withlowdependencyof f.Forexample,Fig.4showstwocalling
contexts of f:ctx(f,1)andctx(f,2).ctx(f,1)is a call path from
n5t of(see the blue dotted line in the left part) where p(n5|f)=
p(n8|f)=0.8>Ï„=0.7 andp(n1|f)=0.5,p(n2|f)=0.6. Thus,
ctx(f,1)=n5â†’n8â†’f.Similarly, ctx(f,2)=n3â†’n6â†’n9â†’
f.
3.4 Concolic Testing to Generate Violating
Symbolic Path Formulas
CONBRIOappliesconcolictestingtoanextendedunittoexplore
diverse and realistic executions of f. During concolic execution, it
obtainsa setofsymbolicexecutionpath formulas SEf.and records
asymbolicpathformula Ïƒf(vi,j)thatviolatesanassertion viinf(j
is an index to a symbolic path formula violating visince there can
bemultiplesuchsymbolicpathformulas).Weuse Ïƒfvitodenote/logicalortext.1
jÏƒf(vi,j).
3SYM_assume( expr )is a macro of if(!expr) exit(0);.Ä¨Q QQ
Q
Q Q
Q QQ
QQ
QdÄ‚ÆŒÅÄÆš
Ä¨ÆµÅ¶ÄÍ˜
Ç†ÆšÄÅ¶ÄšÄÄš
hÅ¶ÅÆšÅ½Ä¨Ä¨QQ Q
ÄÆšÇ†Í¾Ä¨Í•Ï­Í¿
ÄÆšÇ†Í¾Ä¨Í•Ï®Í¿PDLQ
Figure4:Staticcallgraphshowinganextendedunitandtwo
calling contexts of f(ctx(f,1)andctx(f,2)) withÏ„=0.7
Tofocuson f,CONBRIOmodifiesDFSsearchstrategiesbyusing
a priority queue for branch conditions of fand a normal queue
for those of the other functions in an extended unit of f(e.g., gin
Fig. 1). CONBRIO explores various behaviors of ffirst by negating
branch conditions in a priority queue first(branch conditions in anormal queue are negated when the priority queue is empty).
3.5 Alarm Filtering by Checking Satisfiability
offâ€™sViolatingSymbolicPathFormula Ïƒfvi
withfâ€™s Symbolic Calling Context Formula
To filter out false alarms raised at viinf, CONBRIO checks the
feasibilityof Ïƒfviwithregardto fâ€™scallingcontexts (seeSect.3.3.2).
For this purpose, CONBRIO constructs Î ctx(f,k)which is a kth
symbolic calling context offand checks satisfiability of Ïƒfviâˆ§
Î ctx(f,k)using a SMT solver. Î ctx(f,k)is constructed as follows
(see Fig. 5):
â€¢For each function ajin a calling context of f(i.e.ctx(f,k)),
CONBRIO obtains SEajwhich is a set of symbolic execution
path formulas of aj.
Note that this task is the same task in Step 4 (Sect. 3.4). If ajwas
already tested as a target function and SEajwas generated in
Step 4, this alarm filtering step (Step 5) reuses SEaj.
â€¢CONBRIO obtains a slice of SEajwith regard to aj+1(saying
Slice(SEaj,aj+1)) as follows:
Slice(SEaj,aj+1)def={Ïƒ/prime|Ïƒ/primeis a prefix of ÏƒâˆˆSEajsuch
thatÏƒcontainsinvocationof aj+1andÏƒ/primedoesnotcontain
a suffix of Ïƒafter the invocation }.
Forexample,for ctx(f,1)inFig.4,Fig.5showsthat Slice(SEn5,n8)
hastwosymbolicpathformulasthatcall n8:Ïƒn5(n8,1)andÏƒn5(n8,2)
(shown as thick blue arrows) where Ïƒx(y,z)iszth symbolic path
formulaofafunction xthatterminatesimmediatelyaftercalling
a function y.Slice(SEn8,f)also has two symbolic path formulas
that callf:Ïƒn8(f,1)andÏƒn8(f,2).
319
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Yunho Kim, Yunja Choi, and Moonzoo Kim
YÊÄ¨Ç€Ï­sÅÅ½Å¯Ä‚ÆšÅÅ¶ÅÆÇ‡ÅµÍ˜
ÄÇ†ÄÄÍ˜Æ‰Ä‚ÆšÅšÆÅ½Ä¨Ä¨
^Ç‡ÅµÍ˜ÄÇ†ÄÄÍ˜
Æ‰Ä‚ÆšÅšÆÅ½Ä¨Å¶Ï±
^Å¶Ï±
Å¶Ï´Å¶Ï±
ÊÅ¶Ï±Í¾Å¶Ï´Í•Ï®Í¿ÊÅ¶Ï±Í¾Å¶Ï´Í•Ï­Í¿
Ä¨^Ç‡ÅµÍ˜ÄÇ†ÄÄÍ˜
Æ‰Ä‚ÆšÅšÆÅ½Ä¨Å¶Ï´
^Å¶Ï´ÊÅ¶Ï´Í¾Ä¨Í•Ï­Í¿ÊÅ¶Ï´Í¾Ä¨Í•Ï®Í¿^Å¯ÅÄÄÍ¾^Å¶Ï±Í•Å¶Ï´Í¿
^Å¯ÅÄÄÍ¾^Å¶Ï´Í•Ä¨Í¿
Figure5:Violatingsymbolicpathformula Ïƒfviandasymbo-
lic calling context of f(i.e.,Î ctx(f,1)) withctx(f,1)in Fig. 4
â€¢CONBRIO obtains symbolic calling context formula offwith
ctx(f,k)(i.e.,Î ctx(f,k))bycombiningsetsofslicedsymbolicexe-
cutionpathformulasof a1(i.e.,Slice(SEa1,a2)),a2(i.e.Slice(SEa2,a3)),
... ofctx(f,k)until reaching fusing logical conjunction. Thus,
Î ctx(f,k)withctx(f,k)=a1â†’a2â†’...â†’fis defined as
follows:
Î ctx(f,k)def=/logicalandtext.1
ajâˆˆ(ctx(f,k)âˆ’{f})(/logicalortext.1
ÏƒlâˆˆSlice(SEaj,aj+1)Ïƒl)
Forexample,Fig.5shows Î ctx(f,1)withctx(f,1)=n5â†’n8â†’
finFig.4asfollows(seethickbluearrowsrepresenting Ïƒn5(n8,1),
Ïƒn5(n8,2),Ïƒn8(f,1)andÏƒn8(f,2)):
Î ctx(f,1)=Î n5â†’n8â†’f=
(Ïƒn5(n8,1)âˆ¨Ïƒn5(n8,2))âˆ§(Ïƒn8(f,1)âˆ¨Ïƒn8(f,2))
Finally, CONBRIOapplies a SMTsolver to Ïƒfviâˆ§Î ctx(f,k)for
every symbolic calling context of f. If a result is UNSAT for all
callingcontexts(i.e.,thereexists nofeasibleexecutioninanycalling
contextsof ftomakeÏƒfvifeasible),atargetalarmisconsideredas
false and ignored. Otherwise (i.e. , a result is SAT with at least one
callingcontext),acorrespondingalarmisreportedasaviolationof
viinf.
3.6 Implementation
We have implemented CONBRIO in 5,000 lines of C++ code using
Clang/ LLVM-3.4 [ 29]. CONBRIO uses CROWN [ 1] for concolic
testing and LLVM-based static variable range analyzer [ 36]t o
computethepossiblerangesofvariables.CROWN(Concolictes-
ting for Real-wOrld softWare aNalysis) is a lightweight easy-to-
customizeconcolictestingtoolforreal-worldCprograms(available
athttp://github.com/swtv-kaist/CROWN).ItsupportscomplexC
features such as bitwise operators, floating point arithmetic, bit-
fields and so on. CROWN has been successfully applied to various
industrial projects.Table 1: Target programs and bugs for RQ1 to RQ4
Target Lines # of # of sys. Branch Func #o f
programs func. test cov. cov. target
and versions cases (%) (%) bugs
Bash-2.0 32714 1214 1100 46.2 89.0 6
Flex-2.4.3 7471 147 567 45.7 93.9 2
Grep-2.0 5956 132 809 50.3 94.7 5
Gzip-1.0.7 3054 82 214 55.8 87.8 2
Make-3.75 28715 555 1043 64.5 87.9 3
Sed-1.17 4085 73 360 47.3 87.7 2
Vim-5.0 66209 1749 975 35.8 91.0 6
Perl-5.8.7 79873 2240 1201 52.3 95.0 6
Bzip2-1.0.3 4737 114 6 67.4 93.9 2
Gcc-3.2 342561 5553 9 43.7 96.2 15
Gobmk-3.3.14 154583 2682 1354 65.2 92.0 5
Hmmer-2.0.42 35992 539 4 75.6 94.1 3
Sjeng-11.2 10146 144 3 77.9 91.7 2
Libquantum-0.2.4 2255 101 3 68.5 93.1 3
H264ref-9.3 51578 590 6 63.6 88.0 5
Sum 829929 15915 7654 N/A N/A 67
Average 55328.6 1061.0 510.3 57.3 91.7 4.5
4 EXPERIMENT SETUP
Wehavedesignedfiveresearchquestionstoevaluatebugdetection
ability and precision of CONBRIO and compare CONBRIO with
otherconcolicunittestingtechniqueson15real-worldCprograms.
Notethatitisimportanttoevaluatebugdetectionabilityandpreci-sion together because of a trade-offbetweenthem (i.e., a technique
mayimprovebugdetectionabilityatthecostofprecisionorvice
versa). Also, we applied CONBRIO to the latest versions of the
nine C programs studied in other papers on crash bug detection
techniques.
4.1 Research Questions
RQ1. Bug Detection Ability : How many crash bugs among the
target crash bugs does CONBRIO detect, compared to the other
concolic unit testing techniques?
RQ2. Bug Detection Precision : How much is a false alarm ratio
of CONBRIO, compared to the other techniques?
RQ3.EffectivenessoftheSymbolicAlarmFiltering :Howmuch
does the alarm filtering strategy using symbolic calling contexts
affect a number of target bugs detected and a false alarm ratio?
RQ4. Effect of the Function Selection Strategy on Bug De-
tection Ability and Precision :Howmuchdoesthefunctionse-
lection strategy based on the function relevance metric affect a
numberoftargetbugsdetectedandafalsealarmratio,compared
to a strategy based on static call graph distance?
RQ5. Effectiveness of Detecting NewCrash Bugs: How many
new crash bugs does CONBRIO detect?
4.2 Target Bugs and Programs
We target crash bugs described in Section 2.3 by inserting corre-
spondingcrashassertionsintargetprogramsbecausecrashbugs
are serious problems and CONBRIO can automatically insert such
320
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. Precise Concolic Unit Testing of C Programs using
Extended Units and Symbolic Alarm FilteringICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
Table 2: Target programs for RQ5
Target programs Lines # of # of sys. Branch Func.
and versions func. test cases cov. (%) cov. (%)
abcm2ps-8.13.9 36595 499 12 74.0 93.0
autotrace-0.31.1 18495 343 5 69.3 84.8
bib2xml-5.11 77216 1032 24 73.2 93.0
catdvi-0.14 12693 187 7 53.0 81.8
eog-3.14.1 43463 605 42 73.3 81.0
gif2png-2.5.11 4058 76 2 60.1 81.6
jpegtran-1.3.1 51828 817 33 72.0 84.9
mp3gain-1.5.2 5786 100 3 53.7 86.0
xpdf-3.03 22309 381 13 54.9 81.9
Sum 272443 4040 141 N/A N/A
Average 30271.4 448.9 15.7 64.8 85.3
assertions without user-given test oracles, which are rarely availa-
bleintargetprograms.Weusetwobenchmarks: knowncrashbug
benchmark for RQ1 to RQ4 and unknown crash bug benchmark for
RQ5 (available at http://swtv.kaist.ac.kr/tools/conbrio).
4.2.1 Known Crash Bug Benchmark. The known crash bug ben-
chmark consists of all C programs in SIR [ 11] (except Siemens
programs and spacewhich do not have available bug-fix histories)
andSPEC2006integerbenchmarks(except mcf-1.2whichhasonly
onesystemtestcase).Wetargetthecrashbugsofthebenchmark
programs that satisfy the following conditions:
â€¢crash bugs that exist in a target program version and have been
confirmed by original developers through bug-fix commits since
thereleaseofatargetprogramversion(e.g.,Dec1996for bash-2.0 )
until April 2017
â€¢crash bugs that canbe detected by unit testing (i.e.,both buggy
statement(s) reported in a bug-fix commit and violated asser-
tion(s) are located in a same target function)
Table 1 describes 15 target programs including their sizes (in
LoCincludingcommentsandemptylines),anumberoffunctions
to test, a number of system test cases used, branch coverage and
function coverage achieved by the system test cases, and a number
ofthetargetcrashbugs.Foralltargetprograms,weusedallsystem
test cases provided in the benchmarks. Each target program has
twoto15targetcrashbugs(4.5onaverage).Notethatnosystem
test case detects a target bug.
For example, we have reviewed 28 bug-fix commits reported
sincethereleaseof vim-5.0onFeb1998untilApril2017.11among
themreportcrashbugsexistingin vim-5.0.Amongthe11crash
bugs, unit testing can detect six of them, which we target for
vim-5.0(see the eighth row of the table).
4.2.2 Unknown Crash Bug Benchmark. Theunknowncrashbug
benchmark programs were selected from the literature on crash
bugdetectiontechniques.ThisisbecauseSIRandSPECbenchmark
programsdonotsatisfythefollowingcriteria:weselectedtarget
programs whose sizes are 1,000 to 100,000 LoC and which have
morethanthreecrashbugfixesinthelastthreeyears(i.e.between
April 2014 to April 2017). We excluded very large programs due to
huge manual effort required to check validity of alarms. We alsoexcluded programs with three or less crash bug fixes in the last
three years because such programs may not have a crash bug.
To obtain new crash bug benchmark programs, we surveyed
papers on crash bug detection techniques published in major SE
(ICSE,FSE,ASE,ISSTA),PL(PLDI,POPL,SPLASH),andsecurity
conferences (IEEE S&P, ACM CCS, USENIX Security) in the last
threeyearsandobtainedtheninerelevantpapers[ 3,8,18,30,37,
38,46â€“48].Then,weappliedtheabovecriteriatothelatestversions
of the target programs studied in these papers and obtained the
nine target programs in Table 2. Again, for all target programs, we
used all system test cases provided in the target program versions
and no system test case violated the crash assertions.
4.3 Concolic Unit Testing Techniques to
Compare
We have compared CONBRIO with the following concolic unit
testing techniques:
â€¢Symbolic unit testing (SUT): It generates a symbolic unit testing
driver with symbolic arguments to a target function fand sym-
bolicglobalvariableswithoutanyconstraintsonthesymbolic
values,asdescribedinSect.2.2.Also,SUTusessymbolicstubs
to replace all functions called by f.
â€¢Static call-graph distance techniques: It constructs an extended
unittoincludeallsuccessorfunctionsof fwithinacertaindis-
tanceboundfrom finastaticfunctioncallgraph.Also,acalling
context of fcontains predecessor functions of fwithin a cer-
taindistanceboundfrom f.Weusedistancebounds3,6and9.
SUT corresponds to a static call-graph distance technique with a
distance bound 0.
SUT uses DFS as a concolic search strategy. Call-graph distance
techniques and CONBRIO use the modified DFS (Sect. 3.4).4These
unit testing techniques have been implemented in 1,000 lines of
C++ code using CROWN [1].
4.4 Measurement
We consider that a target bug is detected if a unit test execution
thatviolatesanassertioncoversoneofthebuggystatementsina
target unit. To identify the buggy statements, we have manually
analyzed all crash bug-fix commits of all subsequent releases of
the target program versions in SIR and SPEC2006 benchmarks. We
considerthatastatement sofatargetprogramisabuggystatement
ifscorresponds to the changed/fixed statements in a crash bug-fix
commit.
Weanalyzealarmsreportedbythealarmfilteringstrategy(Sect.3.5).
Fortruealarms,wecountanumberofviolatedassertstatements
which satisfy the following conditions:
â€¢There exists a unit test execution Ïƒfvthat covers a buggy state-
ment and violates an assert statement in a target function f.
â€¢We can confirm that Ïƒfvis feasible at system level by manually
creating a system-level test that includes Ïƒfvand violates the
assert statement (we compared execution traces of Ïƒfvand a
corresponding system test using gdb).
4WereporttheexperimentusingonlyDFSandmodifiedDFSsincetheexperiments
using other searchstrategies such as randomnegation and CFG heuristic showonly
negligible difference.
321
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Yunho Kim, Yunja Choi, and Moonzoo Kim
We consider all other alarms as false ones.
4.5 Testbed Setting
For SUT, call-graph distance techniques, and CONBRIO, we set the
timeout of concolic testing (Step 4 in Fig. 3) as 180 seconds per a
targetfunction.5Aftertestgenerationterminates,call-graphdis-
tancetechniquesandCONBRIO performs thefalsealarmfiltering
task (Step 5 in Fig. 3). We set a function dependency threshold Ï„as
0.7.
Sincetheexperimentscaleislarge(i.e.,targeting15,915functions
for the known crash bugs and 4,040 functions for the unknown
crashbugs),theexperimentswereperformedon100machineseach
ofwhichisequippedwithIntelquad-corei54670Kand8GBram,
runningUbuntu14.04.264bitversion.Werunfourconcolicunit
test runs on a machine in parallel.
4.6 Threats to Validity
A threat to external validity is the representativeness of our target
programs. But we expect that this threat is limited since the target
programs are widely used real-world ones and tested by many
otherresearchers.Anotherthreattoexternalvalidityisthepossible
bias of the system tests we used to obtain dependency between
functions.Wetriedtoreducethisthreatbyutilizingallavailable
system test cases in the benchmarks.
A threat to internal validity is possible faults in the implementa-
tion of the concolic unit testing techniques we studied. To address
thisthreat,weextensivelytestedourimplementation.Athreatto
construct validity is the use of the crash bugs that were fixed by
thebug-fixcommitsreportedsofar(i.e.,thetargetprogramsmay
have unknown/unreported crash bugs which we do not count). We
target crash bugs confirmed by the developers through the bug-
fix commits because it would require too much effort to manually
validate numerous alarms without confirmed reports in this large
scale experiment. However, this threat seems limited because all
targetprogramsarewell-maintainedsothattheseprogramsmay
not have many new bugs.
5 EXPERIMENT RESULT
Forallcomparisonintheexperimentsinthissection,weapplied
Wilcoxn test with a significance level 0.05 to show the statistical
significance. Allcomparison resultsin this sectionare statistically
significantunlessmentionedotherwise.Theexperimentdataare
available at http://swtv.kaist.ac.kr/tools/conbrio.
5.1 Experiment Data
5.1.1 Data on Extended Units and Calling Contexts. Forthe15
known crash bug benchmark programs, each extended unit con-
structedbyCONBRIOcontains6.2functionsonaverage.CONBRIOgenerated3.0callingcontextspertargetfunctionwhereeachcalling
context has 6.6 functions on average. Call-graph distance techni-
queswithbound3,6,and9generateanextendedunitthatcontains
5.8, 13.8, and 22.5 functions on average, respectively. Also, they
5We selected timeout as 180 seconds because exploratory study with timeout 60, 180,
300, and 600 seconds suggested that timeout beyond 180 seconds had negligible effect
on the overall experiment results of CONBRIO and the other techniques.Table3:Numbersofthetargetbugsdetectedbytheconcolic
unit testing techniques
Target #o fBound of static call
program target graph distance tech. CONBRIO
bugs0(SUT)369
Bash-2.0 6 5333 5
Flex-2.4.3 2 2111 1
Grep-2.0 5 3422 4
Gzip-1.0.7 2 2111 2
Make-3.75 3 3322 3
Sed-1.17 2 2222 2
Vim-5.0 6 5422 5
Perl-5.8.7 6 6543 6
Bzip2-1.0.3 2 2222 2
Gcc-3.2 15 14 12 9 8 14
Gobmk-3.3.14 5 4333 5
Hmmer-2.0.42 3 3333 3
Sjeng-11.2 2 2222 2
Libquantum-0.2.4 3 3222 3
H264ref-9.3 5 5433 4
Sum 67 61 51 41 39 61
generate5.9, 11.1,and24.3calling contextspertargetfunction on
average, respectively.
5.1.2 Data on Unit Tests Generated and Alarm Filtering. For the
15 known crash bug benchmark programs, CONBRIO spent 1.8
hours to generate 7,979,781 unit tests for 15,915 target functionsand 2.3 hours for the symbolic alarm filtering using Z3 on 100quad-core machines. Z3 reports that a symbolic calling contextformula with a violating symbolic unit execution consists of 1.5million clauses on 0.1 million Boolean variables on average and
its maximum memory usage is around 7.6 GB. Call-graph distance
techniques with a distance bound 0, 3, 6, and 9 spent the almost
same 1.8hours for unit testgeneration (i.e., most targetfunctions
reach the timeout) and 0, 2.6, 3.9, and 6.3 hours for the symbolic
alarm filtering, respectively.
CONBRIO covered 69.8% to 88.0% of the branches of a target
program(82.5%onaverage)withtheunittestsandthegivensystem
testcases(i.e.,theunittestsincreasethebranchcoverage25.2%p
moreonaverage(=82.5%-57.3%where57.3%istheaveragebranch
coverage achieved by the system test cases (see the last row of
Table 1)).
5.2 RQ1: Bug Detection Ability
Table3describesanumberofthetargetbugsdetectedbytheconco-
lic unit testing techniques and shows that CONBRIO has high bug
detectionability.CONBRIOandstaticcall-graphdistancetechnique
withboundzero(i.e.,SUT)achievethehighestbugdetectionability
(i.e.,91.0%(=61/67))(butSUTachievesthisatthecostofmanyfalse
alarms(seeSect.5.3)).Notethatthegivensystemtestsdonotdetect
anyofthetargetbugs.Inaddition,weappliedconcolictestingat
system level using distributed concolic testing tool SCORE [ 25]
withthesameamountoftotaltimeon100machinesbutfoundthat
no target bug was detected.
322
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. Precise Concolic Unit Testing of C Programs using
Extended Units and Symbolic Alarm FilteringICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
Table 4: Numbers of false alarms and ratios of false alarms
per true alarm of the concolic unit testing techniques
Static call-graph distance techniques
Target 0 (SUT) 3 6 9 CONBRIO
programs # of F/T # of F/T # of F/T # of F/T # of F/T
false alarm false alarm false alarm false alarm false alarm
alarms ratio alarms ratio alarms ratio alarms ratio alarms ratio
Bash-2.0 484 96.8 137 45.7 69 23.0 54 18.0 18 3.6
Flex-2.4.3 142 71.0 25 25.0 12 12.0 12 12.0 6 6.0
Grep-2.0 120 40.0 34 8.5 18 9.0 18 9.0 13 3.3
Gzip-1.0.7 33 16.5 7 7.0 3 3.0 3 3.0 5 2.5
Make-3.75 664 221.3 106 35.3 59 29.5 46 23.0 9 3.0
Sed-1.17 31 15.5 9 4.5 4 2.0 4 2.0 5 2.5
Vim-5.0 906 181.2 207 51.8 123 61.5 72 36.0 25 5.0
Perl-5.8.7 392 65.3 187 37.4 64 16.0 44 14.7 57 9.5
Bzip2-1.0.3 34 17.0 12 6.0 7 3.5 7 3.5 10 5.0
Gcc-3.2 2026 144.7 503 41.9 195 21.7 147 18.4 79 5.6
Gobmk-3.3.14 791 197.8 133 44.3 62 20.7 45 15.0 39 7.8
Hmmer-2.0.42 162 54.0 48 16.0 22 7.3 22 7.3 12 4.0
Sjeng-11.2 108 54.0 13 6.5 7 3.5 7 3.5 8 4.0
Libquantum-0.2.4 55 18.3 9 4.5 4 2.0 4 2.0 5 1.7
H264ref-9.3 232 46.4 34 8.5 15 5.0 15 5.0 17 4.3
Average 412.0 82.7 97.6 22.9 44.3 14.6 33.3 11.5 20.5 4.5
As a distance bound of the call-graph distance techniques incre-
ases to 3, 6, and 9, the number of detected bugs severely decreases
to 51, 41, and 39, respectively because larger symbolic search space
should be explored within the timeout.
Amongtheundetectedsixtargetbugs(=67-61),threetargetbugs
inbash,grep, and gccwere missed because concolic execution
did not cover corresponding buggy statements within the timeout,
twobugsin flexandh264refweremissedbecauseofthealarm
filtering strategy, and one in vimwas missed because a unit exe-
cutioncoveredthecorrespondingbuggystatementandanassert
statement but did not violate the assert statement.
5.3 RQ2: Bug Detection Precision
Table4describesanumberoffalsealarmsandaratiooffalsealarms
pertruealarmofthetechniquesandshowsthatCONBRIOachieves
high bug detection precision. Among the techniques, CONBRIO
raises the lowest number of false alarms (i.e., 20.5 false alarms per
target program on average) and the lowest false alarms per true
alarms ratio (i.e., 4.5 false alarms per true alarm on average).6
Thestaticcall-graphdistancetechniquewithdistance0(i.e.SUT)
suffersthelargestnumberoffalsealarms(412.0falsealarmspertar-
get program on average). CONBRIO raises only 5.0% (=20.5/412.0),
21.0%,and46.4%and61.6%ofthefalsealarmsraisedbythestatic
call-graphdistancetechniqueswithdistancebounds0,3,6,and9
on average, respectively (see the last row of the table).
5.4 RQ3. Effectiveness of the Symbolic Alarm
Filtering
The comparison of the experiment results of CONBRIO and CON-
BRIO without the alarm filtering strategy using symbolic calling
contextformulas(Sect.3.5)demonstratesthatthealarmfilteringstrategy improves bug detection precision significantly. In other
words,CONBRIO withoutthe alarmfilteringstrategy detectstwo
6The static alarm reduction heuristics of CONBRIO decrease the number of false
alarms (23.5 to 20.5 on average) and the number of false alarms per true alarm (5.2 to
4.5onaverage)withoutdecreasingthebugdetectionability(i.e.CONBRIOwithout
the static alarm reduction heuristics detects the same 61 bugs and raises 23.5 false
alarms per target program on average).more target bugs (i.e., 63 bugs) in all target programs but with five
times higher false alarm ratio (i.e., 20.3 false alarms per true alarm
onaverage).Althoughthesymbolicalarmfilteringspentmoretime
(2.3 hours) than the unit test generation (1.8 hours), this strategy is
worthwhiletoapplytoimprovebugdetectionprecision.Detailed
experiment data is available at http://swtv.kaist.ac.kr/tools/conbrio.
5.5 RQ4. Effect of the Function Selection
Strategy on Bug Detection Ability and
Precision
The comparison on the experiment results of CONBRIO and the
call-graphdistancetechniquesconfirmsthattheideaofincluding
onlyclosely relevant functions to a target function based on the
proposeddependencymetricinextendedunitsandcallingcontexts
is effective.
Forexample,CONBRIOandthecall-graphdistancetechnique
with bound 3 generate an extended unit of a similar size (i.e., 6.2
vs.5.8functionsonaverage)andtheamountofgeneratedcalling
contextsarealsocomparable(3.0callingcontextseachofwhichhas
6.6functionsvs.5.9callingcontextseachofwhichhas2.8functions
on average) (see Sect. 5.1.1). The time taken to generate unit test
executions is almost same 1.8 hours and the time taken to apply
the alarm filtering strategy is also similar (2.3 vs 2.6 hours) .
However, CONBRIO achieves much higher bug detection ability
andprecisionthanthecall-graphdistancetechniquewithbound
3(i.e.,91.0%vs76.1%(=51/67)forbugdetectionabilityand4.5vs.
22.9falsealarms pertruealarmon average).Withlargerdistance
bounds6 and9,a numberofthe detected bugsdropsto 41and39
and the false alarm ratio decreases to 14.6 and 11.5 respectively,
which is still three to two times less precise than CONBRIO.
5.6 RQ5. Effectiveness of Detecting New Crash
Bugs
CONBRIO detects 14 new crash bugs in the seven target programs.
CONBRIO detects five new crash bugs in autotrace , two bugs
in each of abcm2ps,gif2png, and mp3gain, one bug in each of
bib2xml, eog, and jpegtran, and no bug in catdviandxpdf.7
Note that we confirmed the 14 new crash bugs by manually
creating system-level test cases that crash a target program due to
thebugsdetectedbyCONBRIO.CONBRIOraises71falsealarms
over the all target programs and its true to false alarm ratio foreach program ranges from 1:3.0 to 1:6.0 (1:4.3 on average except
catdviandxpdf). We have reported these 14 new crash bugs to
the original developers and been waiting the responses from them
(detailedexampleand explanationofthenewlydetectedbugs are
available at http://swtv.kaist.ac.kr/tools/conbrio).
6 RELATED WORK
6.1 Concolic Unit Testing Techniques
There exist concolic unit testing techniques (e.g., [ 7,35,40,43])
whichrequireausertobuildsymbolicunittestdriversandstubs.
7CONBRIOgenerated3.3callingcontextspertargetfunctioneachofwhichhas4.3
functions on average. It spent 11.2 minutes to generate 725,584 unit tests for 4,040
targetfunctionsand20.7minutestoapplythesymbolicalarmfilteringon100machines
and covered 80.9% of the branches on average.
323
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Yunho Kim, Yunja Choi, and Moonzoo Kim
DART[16]generatessymbolicunittestdrivers(butnotsymbolic
stubs) like SUT (Sect. 2.2) and test inputs for C programs. CON-
BOL [28] generates symbolic unit test drivers/stubs and test inputs
targeting large-scale embedded C programs. DART and CONBOL
generatesymbolicunittestdriverswithoututilizingcontextsofa
target function fand may suffer many false alarms.8
ChakrabartiandGodefroid[ 9]developedaunittestingtechnique
which statically partitions a static call graph using topological
information and tests each partition as a unit through symbolic
execution.Thistechniquemaysuffermanyfalsealarmsbecausethe
obtained partitions may not represent groups of relevant functions
due to insufficient information to generate partitions (i.e., using
onlytopologicalinformationofastaticcallgraphwithoutsemantic
or dynamicinformation). Their toolis notpublicly available and
the paper does not report bug detection ability nor precision [ 9].
T o m b .e ta l[ 44] reported that interprocedural program analysis
with deeper call depth bound raise fewer false alarms. However,
they did not report how to set a proper call depth bound.
Recently, UC-KLEE [ 37] directly starts symbolic execution from
atargetfunctionusinglazyinitialization[ 22].Throughthemanual
analysis of the thousands of alarms, the authors of UC-KLEE de-
tected 67new bugsin BIND,openssl, Linuxkernel andits true to
falsealarmratiois1:5.7onaverage.Wecouldnotdirectlycompare
CONBRIO with UC-KLEE because UC-KLEE is not publicly availa-
bleand BIND,openssl,andLinuxkernel(millionlinesofcode)are
too large to manually analyze alarms.
6.2 Random Method Sequences Generation
Techniques for Object-Oriented Programs
Randoop [ 32] invokes a random sequence of public methods inclu-
ding constructors of a target methodâ€™s class. MSeqGen [42] mines
codebasestoextractrelevantmethodsequencesofatargetclass
under test and extends such method sequences with symbolic exe-
cutionforhighcoverage.EvoSuite[ 13,14]testsJavamethodsusing
search-based strategies with symbolic execution. TestFul [ 5] com-
bines genetic algorithm and a local sear ch to improv e the speed
ofJavaunittestgeneration.Gargetal.[ 15]improvesRandoopby
generatinginputtestcasesofthegeneratedmethodsequenceusingconcolictestingforC++programs.Thesetechniquesmayalsosufferfalse alarms due to infeasible test inputs/method sequences genera-ted.Forexample,Grossetal.[
19]reportedthatRandoopraised181
alarms withoutdetecting anybug (i.e., allalarms werefalse ones)
onfiveJavaprogramsalthoughtheauthorsofRandoopreported
that Randoopâ€™s true to false alarm ratio is 1:0.67 on 8 Java libraries
and 6 .NET libraries on average [ 32]. Fraser et al. [ 14] reported
that the statistically estimated true to false alarm ratios range from
1:0.6to1:4.2intheirexperimentsonrandomlyselected100projects
hosted on sourceforge.net. Garg et al. [ 15] does not report detected
bugsor falsealarm ratiosbut branchcoverage obtainedusingthe
proposedtechniqueoneightprograms(except gnuchess onwhich
theauthorsreportedninenewbugsandthatatruetofalsealarm
ratio was 1:1.0). In spite of the lack of explicit context information
(e.g., class/object information) in C programs, CONBRIO detects
bugspreciselyinCprograms(i.e.,atruetofalsealarmratiois1:4.5
8DART[16]bypassesthefalsealarmissuebytargetingpublicAPIfunctionsoflibraries
which should work with all possible inputs.on average) while keeping high bug detection ability (i.e., 91.0% of
the target bug detected on average).
The aforementioned papers report only bug detection precision
(RQ2),notbugdetectionability(RQ1),whichmakesfaircomparison
between these techniques and CONBRIO difficult. This is because
thesetechniquesmayimproveatruetofalsealarmratioatthecost
ofmissingbugs.Becauseofsuchtrade-offbetweenprecisionand
recallofbugdetection,westudiedandreportedbothbugdetection
ability and precision.
6.3 Automated Unit Testing Techniques based
on System Tests
Elbaum et al. [ 12] proposed a technique to generate unit tests from
systemtests;thetechniquecapturesprogramstatesbeforeandafter
an invocation of a target function fto generate unit test inputs
andoraclesfor f.OCAT[21]capturesobjectinstancesduringsy-
stemexecutionsandgeneratesunittestsusingRandoopwiththe
capturedobjectandthemutatedobjectinstancesasseedobjects.
GenUTest [ 34] automatically generates unit tests and mock objects
using captured method sequences during system testing. A limi-
tationofthesetechniquesisthattheexecutionsofthegenerated
unit tests just replay the same behaviors [ 12,34] (or similar be-
haviors[21])ofatargetunitinalreadyperformedsystemtesting
(i.e., they are applicable to only regression testing of evolving soft-
ware,nottoasingleversionofsoftware).Also,theaforementioned
papers do not report bug detection ability nor precision.
7 CONCLUSION AND FUTURE WORK
Wehavepresentedanautomatedconcolicunittestingtechnique
CONBRIOwhichgeneratesextendedunitstocloselymimicthereal
contexts of a target function fand filters out false alarms using
symbolic calling context formulas of fusing relevant functions to
f. Through the experiments, CONBRIO demonstrates both high
bug detection ability (91.0% of all target bugs detected) and high
bugdetectionprecision(atruetofalsealarmratiois1:4.5).Further-more,CONBRIOdetects14newcrashbugsinthelatestversionsof
the nine target C programs studied in other papers on crash bug
detection techniques.
As future work, to improve the precision of automated unit tes-
tingfurther,weplantorefinethefunctiondependencymetricby
analyzing more semantic characteristic of target program executi-
ons.Also,wewillimprovebugdetectionabilityfurtherbyapplying
an invasive software testing technique [ 26] which increases test
coveragebyutilizingdiverseexecutionsofmutatedversionsofa
target program. In addition, we will utilize generated test cases to
improve precision of mutation-based fault localization [20].
ACKNOWLEDGEMENT
ThisworkissupportedbyNext-GenerationInformationComputing
DevelopmentProgram throughthe NationalResearch Foundation
ofKorea(NRF)fundedbytheMinistryofScienceandICT(MSIT)
(No.NRF-2017M3C4A7068175andNo.NRF-2017M3C4A7068177),
and Basic Science Research Program through NRF funded by
MSIT(NRF-2016R1A2B4008113),andBasicScienceResearchPro-
gram through NRF funded by the Ministry of Education (NRF-
2017R1D1A1B03035851).
324
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. Precise Concolic Unit Testing of C Programs using
Extended Units and Symbolic Alarm Filtering ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]CROWN: Concolic testing for Real-wOrld softWare aNalysis. http://github.com/
swtv-kaist/CROWN. Accessed: 2018-02-14.
[2]ShayArtzi,AdamKiezun,JulianDolby,FrankTip,DannyDig,AmitParadkar,
and Michael D. Ernst. 2008. Finding Bugs in Dynamic Web Applications. In
Proceedings of the 2008 International Symposium on Software Testing and Analysis
(ISSTA â€™08). ACM, New York, NY, USA, 261â€“272.
[3]Thanassis Avgerinos, Alexandre Rebert, Sang Kil Cha, and David Brumley. 2014.
Enhancing Symbolic Execution with Veritesting. In Proceedings of the 36th Inter-
national Conference on Software Engineering (ICSE 2014). ACM, New York, NY,
USA, 1083â€“1094.
[4]Radu Banabic, George Candea, and Rachid Guerraoui. 2014. Finding Trojan
Message Vulnerabilities in Distributed Systems. In Proceedings of the 19th In-
ternationalConferenceonArchitecturalSupportforProgrammingLanguagesand
Operating Systems (ASPLOS â€™14). ACM, New York, NY, USA, 113â€“126.
[5]LucianoBaresi,PierLucaLanzi,andMatteoMiraz.2010.TestFul:AnEvolutionary
Test Approach for Java. In Proceedings of the 2010 Third International Conference
onSoftwareTesting,VerificationandValidation(ICSTâ€™10).IEEEComputerSociety,
Washington, DC, USA, 185â€“194.
[6]Ella Bounimova, Patrice Godefroid, and David Molnar. 2013. Billions and Bil-lions of Constraints: Whitebox Fuzz Testing in Production. In Proceedings of
the 2013 International Conference on Software Engineering (ICSE â€™13). IEEE Press,
Piscataway, NJ, USA, 122â€“131.
[7]Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: Unassisted and
AutomaticGenerationofHigh-coverageTestsforComplexSystemsPrograms.
InProceedings of the 8th USENIX Conference on Operating Systems Design and
Implementation (OSDIâ€™08). USENIX Association, Berkeley, CA, USA, 209â€“224.
[8]Sang Kil Cha, Maverick Woo, and David Brumley. 2015. Program-Adaptive
MutationalFuzzing.In 2015IEEESymposiumonSecurityandPrivacy.725â€“741.
https://doi.org/10.1109/SP.2015.50
[9]Arindam Chakrabarti and Patrice Godefroid. 2006. Software Partitioning for Ef-
fectiveAutomatedUnitTesting.In Proceedingsofthe6thInternationalConference
on Embedded Software (EMSOFT â€™06). ACM, New York, NY, USA, 262â€“271.
[10]Christoph Csallner and Yannis Smaragdakis. 2004. JCrasher: An Automatic
Robustness Tester for Java. Software Practical Experience 34, 11 (Sept. 2004),
1025â€“1050.
[11]HyunsookDo,SebastianElbaum,andGreggRothermel.2005.SupportingControl-ledExperimentationwithTestingTechniques:AnInfrastructureandItsPotential
Impact.Empirical Software Engineering 10, 4 (Oct. 2005), 405â€“435.
[12]SebastianElbaum,HuiNeeChin,MatthewB.Dwyer,andMatthewJorde.2009.
CarvingandReplayingDifferentialUnitTestCasesfromSystemTestCases. IEEE
Transactions onSoftwareEngineering 35,1 (Jan2009), 29â€“45. https://doi.org/10.
1109/TSE.2008.103
[13]GordonFraserandAndreaArcuri.2011. EvoSuite:AutomaticTestSuiteGene-
ration for Object-oriented Software. In Proceedings of the 19th ACM SIGSOFT
Symposiumandthe13thEuropeanConferenceonFoundationsofSoftwareEngi-
neering (ESEC/FSE â€™11). ACM, New York, NY, USA, 416â€“419.
[14]GordonFraserandAndreaArcuri.2015.1600Faultsin100Projects:Automatically
FindingFaultsWhileAchievingHighCoveragewithEvoSuite. EmpiricalSoftware
Engineering 20, 3 (June 2015), 611â€“639.
[15]PranavGarg,FranjoIvancic,GogulBalakrishnan,NaotoMaeda,andAartiGupta.
2013. Feedback-directed Unit Test Generation for C/C++ Using Concolic Execu-
tion. InProceedings of the 2013 International Conference on Software Engineering
(ICSE â€™13). IEEE Press, Piscataway, NJ, USA, 132â€“141.
[16]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
ProgrammingLanguageDesignandImplementation(PLDIâ€™05).ACM,NewYork,NY, USA, 213â€“223.
[17]
Patrice Godefroid, Michael Y Levin, and David A Molnar. 2008. Automated
WhiteboxFuzzTesting.In Proceedingsofthe2008NetworkandDistributedSystem
Symposium, Vol. 8. 151â€“166.
[18]Denis Gopan, Evan Driscoll, Ducson Nguyen, Dimitri Naydich, Alexey Loginov,
andDavidMelski.2015. Data-delineationinSoftwareBinariesandItsApplication
toBuffer-overrun Discovery.In Proceedingsof the37thInternationalConference
onSoftwareEngineering-Volume1(ICSEâ€™15).IEEEPress,Piscataway,NJ,USA,
145â€“155. http://dl.acm.org/citation.cfm?id=2818754.2818775
[19]Florian Gross, Gordon Fraser, and Andreas Zeller. 2012. Search-based System
Testing: High Coverage, No False Alarms. In Proceedings of the 2012 International
SymposiumonSoftware TestingandAnalysis(ISSTA2012).ACM,New York,NY,
USA, 67â€“77. https://doi.org/10.1145/2338965.2336762
[20]Shin Hong, Byeongcheol Lee, Taehoon Kwak, Yiru Jeon, Bongsuk Ko, Yunho
Kim, and Moonzoo Kim. 2015. Mutation-Based Fault Localization for Real-World
MultilingualPrograms(T).In Proceedingsofthe201530thIEEE/ACMInternational
Conference on Automated Software Engineering (ASE) (ASE â€™15). IEEE Computer
Society, Washington, DC, USA, 464â€“475. https://doi.org/10.1109/ASE.2015.14[21]HojunJaygarl,SunghunKim,TaoXie,andCarlK.Chang.2010. OCAT:Object
Capture-based Automated Testing. In Proceedings of the 19th International Sym-
posiumonSoftwareTestingandAnalysis(ISSTAâ€™10).ACM,NewYork,NY,USA,
159â€“170.
[22]Sarfraz Khurshid, Corina S. PÄƒsÄƒreanu, and Willem Visser. 2003. Generalized
SymbolicExecutionforModelCheckingandTesting.In Proceedingsofthe9th
InternationalConferenceonToolsandAlgorithmsfortheConstructionandAnalysis
of Systems (TACASâ€™03). Springer-Verlag, Berlin, Heidelberg, 553â€“568.
[23]Moonzoo Kim, Yunho Kim, and Yunja Choi. 2012. Concolic testing of the multi-
sectorreadoperationforflashstorageplatformsoftware. FormalAspectsofCom-
puting24, 3 (01 May 2012), 355â€“374. https://doi.org/10.1007/s00165-011-0200-9
[24]MoonzooKim,YunhoKim,andYoonkyuJang.2012. IndustrialApplicationof
Concolic Testing on Embedded Software: Case Studies. In Proceedings of the 2012
IEEEFifthInternationalConferenceonSoftwareTesting,VerificationandValidation
(ICST â€™12). IEEE Computer Society, Washington, DC, USA, 390â€“399.
[25]Moonzoo Kim, Yunho Kim, and Gregg Rothermel. 2012. A Scalable Distribu-
ted Concolic Testing Approach: An Empirical Evaluation. In 2012 IEEE Fifth
International Conference on Software Testing, Verification and Validation. 340â€“349.
https://doi.org/10.1109/ICST.2012.114
[26]YunhoKim,ShinHong,BongseokKo,DuyLocPhan,andMoonzooKim.2018. In-
vasiveSoftwareTesting:MutatingTargetProgramstoDiversifyTestExploration
forHighTestCoverage.In 2018IEEE11thInternationalConferenceonSoftware
Testing, Verification and Validation.
[27]YunhoKim,MoonzooKim,YoungJooKim,andYoonkyuJang.2012. Industrial
Application of Concolic Testing Approach: A Case Study on Libexif by Using
CREST-BV and KLEE. In Proceedings of the 34th International Conference on
Software Engineering (ICSE â€™12). IEEE Press, Piscataway, NJ, USA, 1143â€“1152.
[28]Yunho Kim, Youil Kim, Taeksu Kim, Gunwoo Lee, Yoonkyu Jang, and Moonzoo
Kim.2013. AutomatedUnitTestingofLargeIndustrialEmbeddedSoftwareUsing
ConcolicTesting.In Proceedingsofthe28thIEEE/ACMInternationalConference
onAutomatedSoftwareEngineering(ASEâ€™13).IEEEPress,Piscataway,NJ,USA,
519â€“528. https://doi.org/10.1109/ASE.2013.6693109
[29]Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for
Lifelong Program Analysis & Transformation. In Proceedings of the International
Symposium on Code Generation and Optimization: Feedback-directed and Runtime
Optimization (CGO â€™04). IEEE Computer Society, Washington, DC, USA, 75â€“.
[30]Wei Le and Shannon D. Pattison. 2014. Patch Verification via Multiversion
Interprocedural Control Flow Graphs. In Proceedings of the 36th International
ConferenceonSoftwareEngineering(ICSE2014).ACM,NewYork,NY,USA,1047â€“
1058. https://doi.org/10.1145/2568225.2568304
[31]David Molnar, Xue Cong Li, and David A. Wagner. 2009. Dynamic Test Gene-
ration to Find IntegerBugs in x86 Binary Linux Programs. In Proceedings of the
18th Conference on USENIX Security Symposium (SSYMâ€™09) . USENIX Association,
Berkeley, CA, USA, 67â€“82.
[32]Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. 2007.
Feedback-DirectedRandomTestGeneration.In Proceedingsofthe29thInterna-
tional Conference on Software Engineering (ICSE â€™07). IEEE Computer Society,
Washington, DC, USA, 75â€“84.
[33]Yongbae Park, Shin Hong, Moonzoo Kim, Dongju Lee, and Junhee Cho. 2015.
Systematic Testing of Reactive Software with Non-deterministic Events: A Case
Study on LG Electric Oven. In Proceedings of the 37th International Conference
onSoftwareEngineering-Volume2(ICSEâ€™15).IEEEPress,Piscataway,NJ,USA,
29â€“38.
[34]Benny Pasternak, Shmuel Tyszberowicz, and Amiram Yehudai. 2009. GenUTest:
a unit test and mock aspect generation tool. International Journal on Software
Tools for Technology Transfer 11, 4 (03 Sep 2009), 273. https://doi.org/10.1007/
s10009-009-0115-4
[35]CorinaS.P Ë‡asË‡areanu,PeterC.Mehlitz,DavidH.Bushnell,KarenGundy-Burlet,
Michael Lowry, Suzette Person, and Mark Pape. 2008. Combining Unit-level
Symbolic Execution and System-level Concrete Execution for Testing NASA
Software. In Proceedings of the 2008 International Symposium on Software Testing
and Analysis (ISSTA â€™08). ACM, New York, NY, USA, 15â€“26.
[36]Fernando MagnoQuintao Pereira, RaphaelErnaniRodrigues, and VictorHugo
Sperle Campos. 2013. A Fast and Low-overhead Technique to Secure Programs
AgainstIntegerOverflows.In Proceedingsofthe2013IEEE/ACMInternationalSym-
posiumonCodeGenerationandOptimization(CGO)(CGOâ€™13).IEEEComputer
Society, Washington, DC, USA, 1â€“11.
[37]David A. Ramos and Dawson Engler. 2015. Under-constrained Symbolic Exe-
cution:CorrectnessCheckingforRealCode.In Proceedingsofthe24thUSENIX
ConferenceonSecuritySymposium(SECâ€™15).USENIXAssociation,Berkeley,CA,
USA, 49â€“64.
[38]Alexandre Rebert, Sang Kil Cha, Thanassis Avgerinos, Jonathan Foote, David
Warren,GustavoGrieco,andDavidBrumley.2014. OptimizingSeedSelection
for Fuzzing. In 23rd USENIX Security Symposium (USENIX Security 14). USE-
NIX Association, San Diego, CA, 861â€“875. https://www.usenix.org/conference/
usenixsecurity14/technical-sessions/presentation/rebert
325
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. ICSE â€™18, May 27-June 3, 2018, Gothenburg, Sweden Yunho Kim, Yunja Choi, and Moonzoo Kim
[39]Raimondas Sasnauskas, Olaf Landsiedel, Muhammad Hamad Alizai, Carsten
Weise, Stefan Kowalewski, and Klaus Wehrle. 2010. KleeNet: Discovering In-
sidious Interaction Bugs in Wireless Sensor Networks Before Deployment. In
Proceedings of the 9th ACM/IEEE International Conference on Information Proces-
sing in Sensor Networks (IPSN â€™10). ACM, New York, NY, USA, 186â€“196.
[40]KoushikSen,DarkoMarinov,andGulAgha.2005. CUTE:AConcolicUnitTesting
EngineforC.In Proceedingsofthe10thEuropeanSoftwareEngineeringConference
Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of
Software Engineering (ESEC/FSE-13). ACM, New York, NY, USA, 263â€“272.
[41]spec2006. TheSPECCPU2006BenchmarkSuite. https://www.spec.org/cpu2006/.
[42]Suresh Thummalapenta, Tao Xie, Nikolai Tillmann, Jonathan de Halleux, and
Wolfram Schulte. 2009. MSeqGen: Object-oriented Unit-test Generation via
Mining Source Code. In Proceedings of the the 7th Joint Meeting of the Euro-
pean Software Engineering Conference and the ACM SIGSOFT Symposium on The
FoundationsofSoftwareEngineering(ESEC/FSEâ€™09) .ACM,NewYork,NY,USA,
193â€“202. https://doi.org/10.1145/1595696.1595725
[43]NikolaiTillmannandJonathanDeHalleux.2008. Pex:WhiteBoxTestGeneration
for.NET.In Proceedingsofthe2NdInternationalConferenceonTestsandProofs
(TAPâ€™08). Springer-Verlag, Berlin, Heidelberg, 134â€“153.
[44]Aaron Tomb, Guillaume Brat, and Willem Visser. 2007. Variably Interprocedural
ProgramAnalysisforRuntimeErrorDetection.In Proceedingsofthe2007Interna-
tional Symposium on Software Testing and Analysis (ISSTA â€™07). ACM, New York,
NY, USA, 97â€“107. https://doi.org/10.1145/1273463.1273478
[45]WillemVisser,CorinaS.P Ë‡asË‡areanu,andSarfrazKhurshid.2004. TestInputGene-
rationwithJavaPathFinder.In Proceedingsofthe2004ACMSIGSOFTInternational
Symposium on Software Testing and Analysis (ISSTA â€™04). ACM, New York, NY,
USA, 97â€“107.
[46]JonasWagner,VolodymyrKuznetsov,GeorgeCandea,andJohannesKinder.2015.
High System-Code Security with Low Overhead. In 2015 IEEE Symposium on
Security and Privacy. 866â€“879. https://doi.org/10.1109/SP.2015.58
[47]XiaofeiXie,YangLiu,WeiLe,XiaohongLi,andHongxuChen.2015. S-looper:
Automatic Summarization for Multipath String Loops. In Proceedings of the 2015
International Symposium on Software Testing and Analysis (ISSTA 2015). ACM,
New York, NY, USA, 188â€“198. https://doi.org/10.1145/2771783.2771815
[48]FabianYamaguchi,AlwinMaier,HugoGascon,andKonradRieck.2015. Auto-
maticInferenceofSearchPatternsforTaint-StyleVulnerabilities.In 2015IEEE
Symposium on Security and Privacy. 797â€“812. https://doi.org/10.1109/SP.2015.54
326
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:52:13 UTC from IEEE Xplore.  Restrictions apply. 