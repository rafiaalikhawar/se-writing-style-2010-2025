Dynamic Generation of Likely Invariants for 
Multithreaded Programs 
Markus Kusano, Arijit Chattopadhyay, Chao Wang 
Department of ECE, Virginia Tech 
Blacksburg, Virginia, USA 
Abstract—Weproposeanewmethodfordynamicallygenerat-
inglikelyinvariantsfrommultithreadedprograms.Whilee xisting 
invariantgenerationtoolsworkwellonsequentialprogram s,we 
foundtheyareineffectiveatreasoningaboutmultithreade dpro-
gramsbothintermsofthenumberofrealinvariantsgenerate d
and in terms of their usefulness in helping programmers. We 
address this issue by developing a new dynamic invariant gen -
eratorconsistingofanLLVMbasedcodeinstrumentationfro nt
end,asystematicthreadinterleavingexplorer,andacusto mized 
invariant inference engine. We show that efﬁcient interlea ving 
exploration strategies can be used to generate a diversiﬁed set
of executions with little runtime overhead. Furthermore, w e
show that focusing on a small subset of thread-local transit ion 
invariantsisoftensufﬁcientforreasoningabouttheconcu rrency 
behavior of programs. We have evaluated our new method on 
a set of open-source multithreaded C/C++ benchmarks. Our 
experimentsshowthatourmethodcangenerateinvariantsth at
aresigniﬁcantlyhigherinqualitythanthepreviousstate- of-the-
art.
I. I NTRODUCTION 
Methods for dynamically generating likely invariants from
sequential software have been used in many applications 
including program understanding, maintenance, testing/v eri- 
ﬁcation, and error diagnosis. However, effective tools for
generating such invariants for concurrent software are sti ll
lacking. For example, Daikon [1], [2] is a highly successful
invariant generation tool for sequential programs written in 
languages such as Java, C, C++, and Perl. However, as we 
will show in Section II, for multithreaded programs, Daikon 
often produces many confusing and incorrect results.
One problem of existing methods such as Daikon is that
they depend heavily on the set of execution traces fed to the 
invariant inference engine. In general, increasing the amo unt
of program behavior exercised in the set of execution traces
increases the likelihood that the generated invariants are true.
However, generating a sufﬁciently diversiﬁed set of execu-
tion traces is difﬁcult for a multithreaded program since th e
program’s behavior depends not only on the program’s input
but also on the thread’s schedules. Thread scheduling, in a 
typicalexecutionenvironment,isdeterminedbytheunderl ying 
operating system and the threading library; naively execut ing 
the program many times does not necessarily increase the 
diversity of the thread schedules.
Another problem of existing methods is that they tend to 
report too many invariants. Even if many of these reported 
invariants are real invariants, they are unlikely to be equa lly 
useful. It is impractical to assume that the user will have ti me 
to sift through all of them individually.Multithreaded 
C/C++ Code 
Executable LLVM Based 
Instrumentation Block 
size Likely 
Invariants 
Exploration of 
Interleavings Strategy Trace Logs Test Oracle Based 
Trace Classifier Daikon Based 
Invariant Gen. 
Good Traces Bad Traces 
Figure 1. Our new dynamic invariant generation tool named Udon .
Multithreaded programs, due to the subtle interactions 
amongstthreadsand potentially large numberof interleavi ngs,
pose challenges both in their design and analysis. Typicall y,
whenthefocusofananalysisisonconcurrentnondeterminis m,
one assumes the sequential part of the computation is correc t:
instead, the problem comes from rare and complex thread 
interactions. In such cases, we argue, that the focus should
be put on a small subset of thread-local invariants, called t he 
transition invariants , that capture the relations among shared 
variables directly related to concurrencycontrol. For exa mple,
a certain code block should be kept atomic, or instances of 
certain operations should be made mutually exclusive.
Tothisend,we proposea newdynamicinvariantgeneration 
method for multithreaded programs. By leveraging systemat ic 
thread interleaving exploration algorithms to generate di versi- 
ﬁed execution traces, our method signiﬁcantly improves the
quality of generated invariants over existing methods.
The overall ﬂow of our method is shown in Figure 1. Our 
method takes a multithreaded C/C++ program as input and 
returns likely invariants as output. First, we instrument t he 
code using a new LLVM based front end to add monitoring 
capabilities for dynamic analysis. Then, we execute the pro -
gram under the control of a systematic interleaving explore r.
The generated traces are fed to a classiﬁer which separates 
the passing traces from the failing traces. Finally, we feed a
subsetofthetracestoacustomized Daikon invariantinference 
engine which returns the likely invariants as output.
Anothercontributionofourworkisinvestigatingtheimpac t
of differentinterleavingexplorationstrategiesonthe pr ecision 
andperformanceofinvariantgeneration.Intheory,allpos sible 
thread interleavings of a concurrent program should be give n
to the invariant inference engine to obtain the most precise
invariants. However, due to the well-known interleaving ex -Thread 1 
1p = &A; 
2if (p!=NULL){ 
3... 
4p->x += 10; 
5... 
6}
7Thread 2 
6p = NULL; 
7... 
8... 
9... 
10 ... 
11 ... 
12 
Figure 2. Two threads sharing a pointer. In thread 1, the if statement and 
the subsequent write to pis intended to be atomic.
plosion problem, the number of thread interleavings can be 
exponential with respect to the number of concurrent opera-
tions. Therefore, we propose the use of selective exploration 
strategies, as opposed to exhaustive exploration, to reduc e
runtime overhead. Our experimental evaluation shows that
selectiveexplorationoftenleadstoinvariantsofsimilar quality 
to sound reduction methods, such as dynamic partial order 
reduction [3], but with an order-of-magnitude faster run ti me.
Athirdcontributionof ourworkis a focusongeneratingin- 
variants that are most relevant to concurrencyrelated prog ram 
behaviors. In general, the invariantsgenerated by an infer ence 
engine fall into two categories: state invariants and trans ition 
invariants. For example, consider the program in Figure 2,
where A.x isinitializedtozero.Thepredicate A.x == 10 at
Line 4 is a state invariant—it holds at a thread-local progra m
location and is expressed in terms of the program variables 
visible at that location. A transition invariant, in contra st, is 
a predicate that may hold at the entry and exit points of an 
arbitrarycode block and is expressed in terms of two version s
ofaprogramvariableattheentryandexitpoints.Forexampl e,
in Figure 2, the predicate A.x == orig(A.x) + 10 is a 
transition invariantover the code block from Line 1 to Line 4 ,
where orig(A.x) is the value of variable A.x at Line 1 
(the original value) and A.x is the value of variable A.x at
Line 4.
Forreasoningaboutconcurrencyrelatedprogrambehaviors ,
we argue that it is often sufﬁcient to focus on these transiti on 
invariants. The reason is that they capture the no-thread- 
interference properties,i.e., whether the associated code block 
is atomic or whetherit should be made atomic .By atomic,we 
mean that the execution of the code block is not affected by 
the execution of other instructions from concurrently runn ing 
threads.
ConsidertheprograminFigure2.AbugcanoccurifThread 
2 sets the value of pto null when Thread 1 is executing 
Lines 2–6. If we generate invariants only from the non-buggy
runs,wewillobservethetransitioninvariant p == orig(p) 
for the code block from Line 2 to Line 6. Examining the 
buggy runs, we will see that this invariant no longer holds.
The differencein the invariantsgeneratedbetweenthe pass ing 
and failing runs shows the root cause of the bug: pis not
constant.
Throughout this paper, we will show how discrepancies 
betweentheinvariantsgeneratedfrompassingandfailingr uns,
like this example, can be leveragedto understandthe softwa re 
code and diagnose concurrency bugs.
We have implemented our new methods and conducted 
experiments on a set of open-source multithreaded C/C++ programs. Our results show that the invariants generated by
our new method are often signiﬁcantly higher in quality than
the previousstate-of-the-art Daikon .Overall,this papermakes 
the following contributions:
•We show, through experiments, that existing dynamic 
invariant generation tools such as Daikon do not work 
well on multithreaded programs, both in terms of the 
number of true invariants generated and in terms of the 
usefulness of these invariants.
•We propose a new method for improving the quality of 
dynamic invariantgeneration for multithreadedprograms 
byleveragingselectiveinterleavingexplorationstrateg ies.
•We show that transition invariants are the most relevant
invariants to help in reasoning about concurrency related 
behaviors.Theyareusefulinprogramcomprehensionand 
diagnosing concurrency errors.
•We implement the proposed method and demonstrate its 
efﬁciency and effectivenessthrough experimentson a set
of multithreaded C/C++ benchmarks.
The remainder of this paper is organized as follows. We 
present examples to illustrate our new methods in Section II .
We establish notation in Section III and then present our new
invariant generation algorithm in Section IV. We present bo th 
runtimeoptimizationsandmethodstoclarifyoutputtotheu ser 
in Section V. This is followed by our experimentalevaluatio n
in Section VI. We review related work in Section VII and 
ﬁnally give our conclusions in Section VIII.
II. M OTIVATING EXAMPLES 
In this section, we present examples to illustrate the prob-
lemsinexistingmethods,highlightourmaincontributions ,and 
demonstrate some potential use cases for our new method.
First, consider the program in Figure 3, which has a 
global variable named balance being accessed by functions 
getBalance() and setBalance() . The third function,
withdraw() , invokes the previous two functions to deduct
a certain amount from balance . Since the global variable 
balance is protected by a Lock() and Unlock() pair ev- 
erytimeitisaccessedthereisnodata-race.However,there can 
be atomicity violations. The function withdraw() is meant
to be executed atomically—without other threads interleav ed 
betweenthecallsto getBalance() and setBalance() —
but the atomicity is not enforced. For example, starting wit h
balance=400 , if two concurrentthreads run withdraw() 
at the same time, the result may be either 300 or 200 .
Existing invariant generation tools do not work well in this
case. For example, if we run the program with Daikon’s C
frontend, mostlikely we will get a false invariant.The reas on 
is thatDaikon relies on the native execution environment to 
determine the program’s thread schedule at run time, and 
in this example, since the code in each thread is small—
signiﬁcantly smaller than what can be executed in the Linux 
kernel’s time slice—all threads will have ample time to run t o
completion before encountering a context switch.
If the erroneous interleaving never occurs during its 
analysis, Daikon would report the following false transi- 
tion invariant for the withdraw() function:balance 
= orig(balance) - 100 , where orig(balance) de- 
notes the original value of balance at the entry point of 1int getBalance() { 
2int bal; 
3Lock(); 
4bal = balance; 
5Unlock(); 
6return bal; 
7}
8void setBalance( int bal) { 
9Lock(); 
10 balance = bal; 
11 Unlock(); 
12 }
13 void withdraw() { 
14 int bal = getBalance(); 
15 bal = bal -100; 
16 setBalance(bal); 
17 }
Figure 3. Concurrent bank account example.
withdraw() and balance denotes the value of balance 
at the exit point. This is not a true invariant, and reporting it
to the user may do more harm than good.That is, it can make 
the developer believe that withdraw() is atomic, thereby 
masking the concurrency bug.
Our new method, in contrast, controls the thread schedul- 
ing of the program in order to create a diverse and rep- 
resentative set of execution traces. Consequently, the in-
variant inference engine would produce the following cor- 
rect invariant for the withdraw() function:balance < 
orig(balance) . This is the correct result and is the best
onecaninferfromtheexecutionsofthisprogram(twothread s
running withdraw() concurrently). That is, the balance 
always decreases but not necessarily by 100 .
Another problem with existing tools is that they often 
report too many invariants. For example, running Daikon on 
the benchmark FibBench [4], which has 55 lines of code,
would generate 24 likely invariants. Among them, 15 are true
invariants(therestarefalse),butonlythreeofthemarere lated 
to the concurrency behaviors of the threads. The others are 
either speciﬁc to the particular program input used in the 
test runs or the sequential part of the computation. Since ou r
goal is to reason about concurrency related behaviors, our 
new method allows the user to focus only on the concurrency 
related invariants, known as transition invariants .
Therecanbemanyapplicationsfortransitioninvariantssu ch 
as balance = orig(balance) - 100 and balance 
< orig(balance) . Here, we give two examples: to help 
diagnose concurrency bugs and to infer atomic code regions.
During software testing, it is reasonable to expect the user
to provide a test oracle which separates failing test runs fr om 
passing test runs. We allow users of our new tool to specify 
correctness conditions using R_assert() which, from the 
user’s perspective, is identical to the standard C assert() 
function.Forourrunningexample,assumetheuserassertst hat
balance==200 must hold at the end of the execution. This 
is illustrated in Figure 4. For the buggy program in Figure 3,
if the function withdraw() is executed atomically by both 
threads, the assertion would pass; but, if the function is no t
executed atomically, the assertion would fail.
If we run our new invariant generation method on the 
passing traces only, it would report the transition invaria nt
balance = orig(balance) - 100 . In contrast, if we int main( void ) { 
thread_create(&t1,withdraw); 
thread_create(&t2,withdraw); 
thread_join(t1); 
thread_join(t1); 
R_assert(balance==200); // test oracle 
}
Figure 4. The main() function for the example in Figure 3.
void withdraw() { 
Lock(); 
int bal = getBalance(); 
bal = bal - 100; 
setBalance(bal); 
Unlock(); 
}
Figure 5. Enforcing atomicity in the withdraw() function in Figure 3.
run our new invariant generation method on the failing trace s
only, it would report the transition invariant balance < 
orig(balance) . The discrepancy between these two sets 
of results (from passing and failing runs) will help the user
diagnose the root cause of the concurrency failure.
Regarding atomic region inference, consider the same ex- 
ample in Figure 3. The transition invariant generated from 
the passing runs for the code block spanning Lines 13–17 
is balance = orig(balance) - 100 ,whichis consis- 
tent with the thread-localtransition relation of this code block 
when it is executed without interference from other threads .
In other words, the programmer’s design intent, as revealed
by all the passing test runs, is that withdraw() should 
be executed atomically. This suggests that to ﬁx the bug in 
withdraw() , we need to enforce atomicity around the calls 
to getBalance() and setBalance() as illustrated in 
Figure 5.
1typedef struct {int a, b, c;} Data; 
2Data *A[128] ; 
3Data *p = A[0]; 
4void thr1() { 
5Data *tmp = p; 
6if (tmp != NULL) { 
7p->a = 100; 
8p->b = 200; 
9p->c = 300; 
10 R_assert(tmp->a == 100 && tmp->b == 200 
11 && tmp->c == 300 ); 
12 }
13 }
14 void thr2() { 
15 p = A[rand() % 127]; 
16 }
Figure 6. Concurrent program where one thread updates the ﬁe lds of a 
structure while the other modiﬁes a global pointer.
Another limitation with Daikon , due to the location of 
code instrumentation, is that invariants are only generate d at
function entry and exit points. Although this design choice is 
suitable for sequential code, it is not suitable for concurr ent
programsbecauseconcurrencyconstructs,suchasatomicco de 
regions, rarely coincide with the procedural boundaries. O ur 
method,in contrast, has the capabilityof generatinginvar iants ======================================= 
..main.c_3_9():::ENTER 
::p != NULL 
======================================= 
..main.c_3_9():::EXIT 
::p == orig(::p) 
======================================= 
..main.c_5_9():::ENTER 
::p != NULL 
======================================= 
..main.c_5_9():::EXIT 
::p == orig(::p) 
======================================= 
Figure 7. Portion of the output generated by our tool for the e xample in 
Figure 6.
at the boundaryof code blocks of arbitrary size—the user can
set the block size as input to our tool as shown in Figure 1.
We illustrate this feature using the following example.
Consider the program in Figure 6. Within thr1() , the 
three ﬁelds are intended to be updated atomically; similar t o
the previous example, the programmer asserts the correctne ss 
condition using R_assert() . To infer the intended atomic 
region that spans from Line 6 to Line 12, the capability of 
generating invariants for arbitrary code blocks is crucial .
Figure 7 shows a section of our tool’s output regarding the 
transition invariant generated from passing runs. It ﬁrst s tarts 
with a code block size of two and then iteratively expands 
the block size. A block size of two means that our tool will
attemptto generateinvariantsoverany code regioncontain ing 
two consecutive accesses to a shared object. The partitioni ng 
of the source code into code regions was performed by our 
LLVM based instrumentation front end as shown in Figure 1.
In Figure 7, ..main.c_6_12() means that we consider 
the block from Line 6 to Line 12 in Figure 6, whereas 
..main.c_8_12() means that we consider the block from 
Line 8 to Line 12. With a block spanning Lines 6–12 we 
cover all the shared memory accesses in thr1() . In both 
cases, when analyzing the passing runs, we can generate the 
invariantp==orig(p) ,whichindicatesthatthe valueof pis 
neverchanged.Amongstallthefailingruns,thisinvariant does 
not hold. Again, the discrepancies in the invariants genera ted 
by the passing and failing runscorrectlysuggeststhat in or der 
forthetestrunstopass,thecodeblockfromLine6toLine12 
must be kept atomic.
III. P RELIMINARIES 
In this section, we present a formal model for concurrent
programs, and introduce the basics of the dynamic invariant
generation process.
A. Concurrent Programs 
A multithreaded program consists of a set of shared vari- 
ables, and a set of threads {T1,...,T n}where nis the 
number of threads in the program. Each thread is a sequential
program with a set of thread-local variables. Let st be an 
instruction. An execution instance of st is called an event ,
denoted e=/a\}bracketle{ttid,l,st,l ′/a\}bracketri}ht,where tid isthethreadID,and land 
l′are the thread program locations before and after executing
st . An event is said to be visible if it accesses a shared 
variable or a thread synchronization object (mutex lock or conditionvariable). Otherwise, the event accesses only th read- 
local variables and it is said to be invisible . During systematic 
interleaving exploration and execution trace logging, inv isible 
events will be ignored.
We model each thread Tias a state transition system Mi.
The transition system of the program, denoted M=M1×
M2×... ×Mn, is constructed using interleaving composition.
LetM=/a\}bracketle{tS,R,s 0/a\}bracketri}ht, where Sis the set of global states, R
is the set of transitions, and s0is the initial state. Each state 
s∈Sis a n-tuple of thread program states. Each transition 
e∈Ris an event from one of the nthreads. An execution 
trace of Mis a sequence ρ=s0e0−→s1... en−1−→sn, where 
s0e0−→s1correspondstoexecutingevent e1instate s0leading 
to state s1.
We use the special event haltto denote normal program 
termination, and the special event abortto denote faulty pro- 
gramtermination,whichcorrespondstoafailed R_assert() 
statement. An event from thread Timay have the following 
types:
•halt, which denotes the normal program termination;
•abort, which denotes the faulty program termination;
•fork(j) for creating child thread Tj, where j/\e}atio\slash=i;
•join(j) for joining back thread Tj, where j/\e}atio\slash=i.
•lock(lk) for acquiring lock lk ;
•unlock(lk) for releasing lock lk ;
•signal(cv) for setting signal on condition variable cv ;
•wait(cv) for receiving signal on condition variable cv ;
•read(v) for reading of shared variable v;
•write(v) for writing to shared variable v;
•mEntry(m) for entering a function call;
•mExit(m) for returning from a function call;
•bEntry(blk) for starting a code block;
•bExit(blk) for ending a code block.
Here,mEntry() and mExit() are also supported by existing 
invariant generation tools such as Daikon , whereas bEntry() 
and bExit() are the new additions in our method.
We model thread synchronization events in our method in 
order to controlthe executionorder during thread interlea ving 
exploration. Using this model, at each moment during a 
program’s execution, we know which threads are blocked 
(disabled ) and which threads are executing ( enabled )
An enabled thread becomes disabled if (i) it attempts to 
execute lock(lk) while lk is held by another thread; (ii) it
attemptsto execute wait(cv) while the signalon cv hasnotyet
been set; or (iii) it attempts to execute join(j) while the child 
thread Tjisstillrunning.Similarly,adisabledthreadbecomes 
enabled if (i) another thread releases the lock lk by executing 
unlock(lk) ,(ii) anotherthread sets the conditionvariable cv by 
executing signal(cv) ,or(iii)thechildthread Tjterminates.An 
accurate model of the sets of enabled and disabled threads at
runtimeisrequiredsinceattemptstoscheduledisabledthr eads 
while postponing the execution of enabled threads may lead 
to artiﬁcial deadlocks.
At runtime, our scheduler selects a given event from the 
set of enabled events. Which event to select is determined 
by the exploration strategy used by the scheduler. Similarl y,
the scheduler repeatedly executes the program, systematic ally 
exploring new interleavings, until the search strategy’s i nter- leaving coverage criteria is satisﬁed. We defer discussion s
of exploration strategies until Section IV. For an in-depth
discussion on systematic concurrent program testing refer
to [5], [6], [7].
B. Dynamic Invariant Generation 
Dynamically generated invariants are predicates that hold
over the execution traces produced by test runs. As such, the y
are not guaranteed to hold for all possible executions of the
program. Furthermore, the invariant inference engine ofte n
uses a statistical analysis and, in theory, is neither sound nor 
complete. However, in practice, dynamic invariant generat ion 
tools such as Daikon have shown to be useful in a wide range 
of applications. In general, the number of invariants that c an 
be generated, as well as the likelihood of them being true 
invariants, depends on the test suite.
Daikon [1], [2] is a highly successful invariant discovery 
tool that supports programming languages such as C, C++,
C#, Eiffel, F#, Java, Lisp, and Visual Basic. For each of 
these languages, Daikon provides a front end tool for code 
instrumentationtoaddloggingcapabilitytothetargetpro gram.
The frontend tools preparethe programto generate trace log s
in a commonformat,which are then fed to the same back end 
invariant inference engine.
We have developed a new instrumentation tool based on 
the popular LLVM compiler platform to replace Daikon’s 
previousfrontend.The main advantageof ournewinstrumen-
tation tool is to leverage the large number of static program
analysis procedures implemented in LLVM as well as to 
reduce the runtime overhead caused by instrumentation. We 
will show through experiments (Section VI) that our LLVM 
based instrumentation tool can indeed lead to faster dynami c
analysis compared to the default front end in Daikon due to 
its signiﬁcantly lower instrumentation overhead.
Since Daikon cannot diversify the thread schedules, it may 
generate many bogus invariants for a multithreaded program .
Furthermore, Daikon iseffectiveingeneratinglinearinvariants 
of the form ( ax +by < c ), but weak in generating more 
complex invariants such as polynomialinvariants ( c0+c1x1+
c2x2+... <0)andarrayinvariants.Forthelatestdevelopment
along this line, please refer to the recent work by Nguyen 
et al. [8], [9]. However, our focus is not on improving the 
expressiveness of the generated invariants, but on improvi ng 
their quality with respect to concurrency.The vast majorit y of 
invariants generated by existing tools such as Daikon capture 
the sequentialprogrambehavior. Our new method,in contras t,
focuses on invariants that capture the concurrency behavio rs.
IV. U DON : O UR NEW DYNAMIC INVARIANT GENERATION 
TOOL 
In this section, we present the three components of our 
new method: an LLVM based code instrumentation tool, a 
systematic interleaving exploration tool, and a customize d
inference engine for Daikon . The overall ﬂow of our tool,
called Udon , is illustrated in Algorithm 1, which takes the 
source code of a multithreaded C/C++ program as input and 
returns a set of likely invariants as output.Algorithm1 High-level algorithm for Udon 
inst output ←inspect pass (src code )
inst output ←daikon pass (inst output )
inst output ←spacer pass (inst output ,spacer size )
trace ﬁle ←gen traces (inst output )
thrd mod traces ←trace classiﬁer (trace ﬁle )
invariants ←inv inference (thrd mod traces )
A. LLVM Based Code Instrumentation 
We developed an LLVM based front end for instrumenting 
multithreaded C/C++ code. As shown in Algorithm 1, it
consists of three code transformation passes.
The ﬁrst pass, inspect_pass() , takes C/C++ code as 
inputandreturnsaninstrumentedversionofthecodeasoutp ut.
Inside this pass, we ﬁrst identify all the programpoints whe re 
a thread’s schedule needs to be controlled. These program 
pointsincludethecallstothreadsynchronizationroutine s,and 
the read and write operations on shared memory locations 
discussed in Section III. At each program point, we inject
new code before these visible operations to allow the control
of the thread at run time by the scheduler. We leverage the 
conservative static analysis techniques implemented in LL VM 
to identify these visible operations.
The second pass, daikon_pass() , takes the previously 
instrumented code as input and returns another instrumente d
version of the code as output. Inside this pass, we inject
new code to add event trace generation capabilities to the 
program. The event trace generated by the program conforms 
to the common ﬁle format as required by the back end 
invariant inference engine in Daikon [2]. By default, this pass 
instrumentsthecode onlyatthe functionentryandexitpoin ts,
whichiscomparabletotheoriginalC/++frontendfor Daikon .
The third pass, spacer_pass() , takes the previously 
instrumentedcode as inputand returns the ﬁnal version of th e
code as output. Inside this pass, we also inject new code to 
add logging capabilities not just at the procedural boundar ies,
but also at the boundary of arbitrary code blocks. This is 
accomplishedbyinsertinghookfunctioncallsto the entrya nd 
exit points of these code blocks, which in turn take care of 
the trace generation at run time.
Note that the events logged as a result of the second 
and third passes are kept in the same format. From the 
standpoint of the back end invariant inference engine, ther e
is no distinction between a pair of entry and exit points for a
function, and a pair of entry and exit points for an arbitrary
code block. Therefore, the back end inference engine does 
not have to be drastically altered in order to infer invarian tsat
the boundary of arbitrary code blocks. By varying the size of
the instrumented code blocks, we can dynamically change the
locations where state and transition invariants are genera ted.
This will help us to detect likely atomic regions in the code.
B. Systematic Interleaving Exploration 
The gen_traces() function in Algorithm 1 involves 
an exploration of the concurrent state space of the program.
Due to the well-known interleaving explosion, in general,
we cannot afford to naively enumerate all possible thread 
schedules while diversifying the execution traces for the b ack end inference engine. In this work, we build off a set of 
interleaving exploration strategies to produce a represen tative 
subset of thread interleavings.
Thebaselinesearchstrategyreliesonthetheoryofpartial or- 
derreduction(POR)[5].Itgroupsthepossibleinterleavin gsof 
aconcurrentprogramintoequivalenceclasses,andthensel ects 
one representative interleaving from each equivalence cla ss to 
explore. Equivalence classes are deﬁned using Mazurkiewic z
traces [10]. Two sequences of events are said to be in the 
same equivalence class if we can create one sequence from 
the other by successively permuting adjacent and independent 
events.Twoeventsare dependent iftheyarefromtwodifferent
threads, access the same memory location, and at least one of
them is a write or modifyoperation;otherwise,the two event s
are independent . It has been shown [11] that in the context
of verifying concurrent systems, exploring one representa tive 
interleaving from each equivalence class is sufﬁcient to ca tch 
all deadlocks and assertion violations.
One beneﬁt of POR based methods is that they are a sound 
reduction.Thereducedset ofinterleavingsstill cancaptu reall
possiblebehaviorsoftheconcurrentprogram.Therefore,u sing 
theexploredinterleavingsasinputforthesubsequentinva riant
inference will lead to the best possible result; the explore d
interleavings form a maximally diversiﬁed set of execution 
traces.
Thecurrentstate-of-the-artPORbasedalgorithmisdynami c
partial order reduction (DPOR) [3]. DPOR computes the 
dependency relation among events dynamically at run, as op-
posed to statically at compile time. As a result, DPOR proved
to be a practical step forward for POR algorithms. It allowed
for realistic programs written in full ﬂedged programming 
languages such as C/C++ to be veriﬁed.
However, due to its exhaustive exploration of the search 
space, even DPOR may incur a large runtime overhead. As a 
result, there is a large body of work dedicated to the devel- 
opment of more efﬁcient, yet unsound, exploration strategi es.
Two methods along these lines are preemptive context bound-
ing(PCB)[6],andhistory-awarepredecessorsets(HaPSet) [7].
In practice, even though they are unsound (i.e., they do not
guaranteeto ﬁndallconcurrencybugs),empiricalstudiesh ave 
shown that they still providedecentbug coveragein program s
far too complex for DPOR to handle.
With respect to invariant generation, we will address per- 
formance concerns in Section V. We will show that replacing 
the ideal, yet slow, exhaustive search of DPOR with faster 
selective exploration strategies such as PCB and HaPSet, we
can still maintain the quality of generated invariants whil e
signiﬁcantly reducing the execution time.
C. Customized Invariant Generation 
The ﬁnal two steps in Algorithm 1, denoted by 
trace_classifier() and inv_inference() , respec- 
tively,solvetwo issues in previousinvariantinferenceen gines.
First, the inference engine may produce confusing results 
when the event trace from both passing and failing execution s
are simultaneously used as input. To solve this problem, we 
developed a trace classiﬁcation module that separates the 
executiontracesintotwogroupsbasedonatestoracle.Spec if- 
ically,Udon providesafunctioncalled R_assert() through which the programmers can assert correctness conditions fo r
the program. During dynamic analysis, if the assertion fail s
the corresponding execution trace will be classiﬁed as fail ing.
Otherwise, the trace is passing. This behavior, from the use r
perspective,isidenticaltothestandardC assert() function.
While inferring likely invariants, the engine may choose to
consider only the bad traces, only the good traces, or all
traces. As a result, we can compareand contrastthe invarian ts 
generated in these three scenarios.
Second, we customized the invariant generation engine in 
Daikon to focus on two types of invariants in a multithreaded 
program: the state invariants and the transition invariant s.
Both transition and state invariants are expressed in terms of 
shared variables–variablesaccessedbymultiplethreadsinthe 
execution traces. A state invariant is a predicate expresse d in 
terms of the value of variablesat a single programlocation. A
transitioninvariantisapredicateexpressedintermsofva riable 
values at two different program locations of the same thread .
In essence, a transition invariant is capable of capturing t he 
non-interference impact of executing an arbitrary code block.
Moreformally,weconsideraprogram Pasastatetransition 
system P=/a\}bracketle{tS,R,I /a\}bracketri}ht, where Sis the set of states, I⊆Sis 
thesetofpossibleinitialstates,and R⊆S×Sisthetransition 
relation.Ingeneral,atransitioninvariant[12],denoted T,isan 
over-approximation R+ofthetransitiveclosureof Rrestricted 
to the reachable state space, i.e., R+∩(R∗(I)×R∗(I)) ⊆
T. Intuitively, a transition invariant summarizes the relat ion 
between the pre- and post-conditions of a consecutive set of
instructions (transitions) executed by a thread.
Transition invariants are particularly useful in software
veriﬁcation. To verify the concurrent behavior of a program ,
one typically assumes the sequential computation is correc t
but the thread interaction is potentially buggy. In this cas e,
transition invariants would conform to the transition rela tion 
ofasequentialcodeblockintheabsenceofunexpectedthrea d
interference, but would deviate from the transition relati on in 
the presence of thread interference. Therefore, observing that
a sequential transition invariant differs from the concurr ent
transition invariantfor the same code block is often indica tive 
of a bug caused by thread interference.
For example, consider a shared counter that is incremented 
by multiple threads. The sequential (or correct) invariant for 
the increment operation would be that the counter increases
its value by one at a time ( counter = orig(counter) 
+ 1 ). However, if the programmer fails to enforce atomicity 
in the increment operation, this invariant would no longer 
hold for all possible executions of the program. Clearly, th e
discrepancybetweenthesequential(orcorrect)andconcur rent
(orincorrect)transitioninvariantshintsatthe rootcaus e ofthe 
aforementioned bug.
Due to the help of both systematic interleaving exploration
techniques and customized invariant inference engines, Udon 
can more robustly generate high-quality invariants for mul ti- 
threaded applications than existing methods.
V. O PTIMIZATIONS 
The method presented in the previous section addresses 
the problem of dynamically generating high-quality invari ants 
from multithreaded programs. However, using DPOR may cause a performancebottleneck due to the exponentialgrowt h
in the number of interleavings with respect to program size.
Another problem of the new method is that the number of 
reported invariants can still be very large. Despite that ma ny 
of them are indeed invariants, reporting all of them without
ﬁltering can overwhelm the user. In this section, we present
our solutions to these problems.
A. Exploring Interleavings Selectively 
We address the performance problem by replacing the 
exhaustive DPOR exploration strategy with efﬁcient, but un -
sound, selective search strategies. In this context, our go al is 
to drastically reduce the runtime overhead while maintaini ng 
the diversity of the generated interleavings. DPOR is a soun d
reduction in that it can prune away redundant interleavings
withoutmissinganyconcurrencyrelatedprogrambehavior. To 
thisend,it groupsallpossiblethreadinterleavingsintoe quiva- 
lence classes and then tries to explore only one representat ive 
interleaving from each equivalence class. However, due to t he 
limited amount of program information available at run time ,
DPOR still may create many redundant equivalence classes 
for the purpose of generating invariants.
Consider the busy-waiting example in Figure 8, which 
has two threads T1,T 2communicating via the variable x
(x= 1 initially). Under DPOR, the systematic exploration 
wouldgenerateinﬁnitelymanyinterleavings.Eachinterle aving 
corresponds to a different execution of the loop by the ﬁrst
thread. Each of these interleavings belongs to a separate 
equivalenceclass(sincedependentmemorylocationsarebe ing 
updated)soeachmustbetested.Noticethat,exceptfortheﬁ rst
two interleavings, denoted c(ab )and (ab )c(ab ), respectively,
none of the other interleavings of the form (ab )kc(ab ), where 
k= 2 ,3,... , can offer new concurrency scenarios.
In this paper, we propose to avoid generating an excessive 
number of execution traces during interleaving exploratio n
by using a selective search , as opposed to an exhaustive 
search . The aim of a selective search is to cover a small
subset of high-risk concurrency scenarios, while avoiding the 
redundant ones as shown in the example in Figure 8. The 
rationale is that, in practice, programmersoften make impl icit
assumptionsregardingtheconcurrencycontrolofthreads, e.g.,
certain code blocks should be mutually exclusive, certain 
code blocks should be atomic, and a certain operation order 
should be obeyed. Concurrency bugs are frequently the resul t
of these implicit assumptions being broken, leading to data
races, atomicity violations, and orderviolations. The goa lof a 
selective search is to maximize the coverageof such scenari os 
while reducing the runtime overhead.
One popular selective search strategy proposed in the 
context of software testing is preemptive context bounding
(PCB) [6]. PCB explores interleavings with only a bounded 
number of involuntary context switches. The strategy can be
effective in concurrency testing because, in practice, man y
concurrency bugs can be exposed using a small number of 
context switches. We note that although PCB is effective 
in practice, the number of explored interleavings remains 
exponential with respect to the number of concurrent thread s.
Furthermore, it is not effective on the example in Figure 8,
where all interleavings have exactly one context switch.Thread T1
do {
a tmp = x;
b}while(tmp); Thread T2
cx= 0; Execution 
a1: R(x) 
c: W(x) 
a2: R(x) 
Figure 8. Using HaPSet reduction to prune interleavings ( x= 1 initially).
Another popular, and more scalable, selective search strat -
egy is the history-aware predecessor set (HaPSet) [7] based
reduction.ItcanbeviewedasanimprovementoverPCBsince 
the number of explored interleavings is no longer exponenti al
withrespecttothe numberofconcurrentoperationsorthrea ds.
This is accomplished by focusing on covering only the order-
ing combinationsof read/write instructions in the program , as 
opposed to the many instances of these instructions.
Formally, a program statement st is deﬁned as a tuple 
(ﬁle ,line ,thr ,ctx ),where ﬁle is theﬁle name, line istheline 
number,thr is the thread ID, and ctx is the bounded calling 
context. Given a set T={ρ1,...,ρ n}of interleavings and a 
statement st ∈Stmt thataccessesasharedobject,theHistory- 
aware Predecessor Set, or HaPSet [st ], is deﬁned as the set
{st 1,...,st k}of statements such that, for all i: 1 ≤i≤k,
an eventeproduced by st is immediately dependent upon an 
eventeiproduced by st iin some interleaving ρ∈ T .
Consider again the example in Figure 8. After exploring 
the ﬁrst two interleavings, denoted c(ab )and (ab )c(ab ), re- 
spectively, the HaPSets computed over these interleavings are 
as follows: HaPSet [a] = {c}and HaPSet [c] = {a}. Since a
and care the only two conﬂicting program statements in the 
program, all possible HaPSet combinations have already bee n
covered.Therefore,the interleavingexplorationstops si nce no 
other interleaving can lead to new HaPSet scenarios.
We shallshowthroughexperimentsinSectionVIthatfaster 
interleaving exploration algorithms such as PCB and HaPSet
areoftenasgoodasDPORintermsofgeneratinghigh-quality
invariants. At the same time, these algorithms can be orders -
of-magnitude faster, which makes Udon practically useful.
B. Focusing on Transition Invariants 
By default, the number of invariants generated by our 
method—as well as other similar tools such as Daikon —can 
be large. However, not all of these invariants are useful for
reasoning about the concurrency related program behaviors .
For example, among the 22 likely invariants generated for 
FibBench [4] by Daikon , onlythree are related to concurrency,
whereas the others are speciﬁc to the sequential part of the 
computation. Therefore, in this work, we propose to focus 
on only the transition invariants over shared variables. In the 
remainder of this section, we show why transition invariant s
are useful in helping the user understand the software code 
and diagnose concurrency bugs.
Let us assume that a multithreaded program has some 
assertions that would be satisﬁed in most cases, but may 
fail in some rare interleavings. Furthermore, regarding th e
concurrency control, there is no formal documentation othe r
than the source code that describes the programmer’s intent .
In this case, we classify the execution traces of the program
into two groups: the passing traces and the failing traces. T o1void thread() { 
2sum = sum + 1; 
3}
4int main(){ 
5// create, run, and join (NUM) threads... 
6R_assert (sum==NUM); 
7}
Figure 9. Concurrent program with multiple threads updatin g a counter.
help the user understand the root cause of the concurrency 
error manifested in the failing traces, we leveragethe two s ets 
of invariants generated by Udon from the set of passing and 
failing traces and identify the discrepancy between them.
Formally, let Ipand Ifbe the likely invariants generated 
from the passing and failing traces, respectively. As a resu lt,
Id=Ip\Ifconsists of all the invariants satisﬁed by the 
passing but not the failing traces. Our conjecture is that th e
discrepancyoftenprovidesinformationtohelpunderstand why 
the error occurs. In the following example, we show that Id
can indeed help a programmer understand the root cause.
ConsiderFigure9,whereaparameterizednumberofthreads 
share a global counter sum initialized to zero. NUM is the 
number of threads that execute the function thread() con- 
currently. These threads are created, run, and joined insid e
main() , before it checks the value of sum . The test oracle 
providedby the programmeris shown on Line 6, which states 
that the expected result is sum==NUM . The assertion passes 
in runs where each thread executes the function thread() 
atomically, but fails in runs where the threads interfere wi th 
each other. Speciﬁcally, depending on how they interfere wi th 
each other, the value for sum ranges from 1to NUM .
When given the passing traces, Udon will generate the tran- 
sition invariant sum == orig(sum) + 1 for the inc() 
function. However, when given the bad traces Udon will
generate the transition invariant sum > orig(sum) , which 
covers cases in which sum is increased by 2,3,..., NUM. By 
comparing the two sets of transition invariants, we can see 
the difference in behavior of the passing and failing runs. I n
the passing runs, sum is always incremented, whereas in the 
failing runs, it is not.
Another possible application of transition invariants is t o
help the user identify atomic regions. When the transition 
relationofacoderegionisconsistentwiththetransitioni nvari- 
ant generated for the same code region, we say that section 
has been executed atomically. Furthermore, if the transiti on 
invariant is generated from passing traces only—and they ar e
not satisﬁed by the bad traces—we can assume that the code 
region is intended to be atomic . For example, the function 
thread() in Figure 9 and the function withdraw() in 
Figure 3 are intended to be atomic, whereas only the ﬁxed 
version of withdraw() in Figure 5 is atomic. The atomic 
coderegionsinferredinthiswaycanhelptheusercomprehen d
the software code and diagnose failed execution traces.
VI. E XPERIMENTS 
We have implemented the proposed method in a software 
tool called Udon .Udon can handle unmodiﬁed C/C++ code 
written using POSIX threads to automatically generate dy- 
namic invariants. We used LLVM to create a new front end for Daikon [2] and a modiﬁed version of Inspect [13], [14] 
for systematic concurrent program exploration.
We evaluated Udon on 19 open source programs. The ﬁrst
set of programs come from the 2014 Software Veriﬁcation 
Competition [4]. These programs, while small in terms of 
lines of code, implement complex low-level synchronizatio n
algorithmssuchasPeterson’s[15]andDekker’s[16]soluti ons 
to the mutual exclusion problem. The second set of programs 
are real-world applications: pfscan is a parallel directory 
scanner and nbds [17] is a C implementation of several non- 
blocking concurrent data structures. All tests were run on a
machine with a 2.60 GHz Intel Core i5-3230M CPU and 8 
GB of RAM running a 64-bit Linux OS.
Our experimental evaluation was designed to answer the 
following research questions:
•Can the previous state-of-the-art method, Daikon [2],
generate correct invariants for concurrent programs? 
•Can our new method, Udon , robustly generate high- 
quality invariants for concurrent programs? 
•Can Udon scale to programs of realistic size and com- 
plexity? 
A. Results 
Table I shows the results of an experiment comparing the 
performance of Udon againstDaikon [2]. For each test pro- 
gram,both Daikon and Udon were used to generate invariants.
Bydesign, Udon needstore-runtheprogrammultipletimesin 
ordertoexploretheconcurrentbehaviorofaprogram,where as 
Daikon runs the program only once. In order to create a fair 
comparison,we also allowed Daikon to run each test program 
the same number of times as Udon . We refer to the multi- 
run Daikon strategy as Daikon* and the single-run strategy as 
Daikon in Table I.
Columns 1–4 of Table I show the program name, lines of 
code (LoC), number of program program points, and number 
of monitoredsharedvariables, for each test. Columns5–7 an d
8–10 show the total number and the number of incorrect in- 
variantsgeneratedby Daikon ,Daikon* ,and Udon respectively.
For experiment purposes only, we manually inspected the re-
sultsto verifyifthe invariantsweretrue.Finally,Column s11– 
13and14–16showthenumberofrunsandruntimeinseconds 
for each method.
First,theresultsshowthat Daikon generatesincorrectinvari- 
ants for every test program. The cause of this is clear: since
Daikon onlyexercisesa smallportionofthe concurrentbehav- 
ior of a program— even if it runsthe programmultiple times 
— it fails to observe many different states of the program and ,
asaresult,deducesincorrectinvariants.ComparingColum ns8 
and 9 of Table I shows that running Daikon multiple times on 
thesameprogramhaslittletonoeffectatreducingthenumbe r
of incorrect invariants. The likely cause of this is that sim ply 
re-running a concurrent program repeatedly explores only a
small portion of the entire interleaving space.
Second, Columns 7 and 10 show that Udon is capable 
of generating a large number of correct invariants for each 
test program. On average, Udon produces only one incorrect
invariant per test. Compared to Daikon and Daikon* ,Udon 
produces,on average, over an order of magnitudefewer incor -
rectinvariants.Theincorrectinvariantsgeneratedby Udon are Table I. Comparison of the invariants generated by Daikon (baseline), Daikon* (multi-run), and Udon (new). for the same set of program points.
Number of Invariants Incorrect Invariants Number of Runs Ru n Time (s) 
Name LoC Prog. Points Shared Vars. Daikon Daikon* Udon Daiko n Daikon* Udon Daikon Daikon* Udon Daikon Daikon* Udon 
Sync01 Safe 64 3 1 15 15 15 1 1 0 1 4 4 1.6 2.8 4.3 
FibBenchSafe 55 3 2 24 24 17 10 10 0 1 6 6 2.9 3.4 4.2 
Lazy01Safe 52 4 1 20 22 22 7 4 0 1 9 9 2.6 4.3 4.8 
Stateful01 Safe 55 3 2 21 21 21 6 6 3 1 4 4 1.4 2.7 3.9 
DekkerSafe 68 3 4 39 44 52 29 24 8 1 53 53 1.7 19.5 4.8 
LamportSafe 119 3 5 48 59 76 36 44 2 1 58 58 5.0 21.5 5.3 
PetersonSafe 55 3 4 39 39 57 29 29 0 1 46 46 1.7 17.3 4.7 
TimeVarMutex 55 3 3 27 27 24 9 9 0 1 3 3 1.6 2.2 3.7 
Szymanski 106 3 3 30 32 35 25 24 0 1 111 111 1.6 39.4 5.2 
IncTrue 55 2 2 15 19 30 3 3 0 1 19 19 2.2 7.7 3.9 
IncCas 60 3 1 14 19 38 3 3 0 1 9 9 2.7 3.7 3.6 
IncDec 68 5 6 95 122 205 57 53 0 1 39 39 3.4 15.7 6.3 
IncDecCas 89 4 3 49 49 73 38 38 1 1 8 8 2.9 4.3 4.8 
Reorder 63 3 4 44 54 95 8 6 0 1 29 29 2.5 10.6 7.4 
AccountBad 61 4 6 74 78 121 22 15 2 1 9 9 3.8 5.4 6.9 
Pfscan 932 24 15 670 798 840 9 9 0 1 20 20 3.0 13.8 8.9 
nbds-hashtable 3278 77 14 1123 1194 2064 2 2 0 1 74 74 5.4 119.7 3 9.0 
nbds-skiplist01 2362 54 24 1053 1055 1370 1 1 0 1 161 161 4.4 287 .6 26.2 
nbds-listidx01 2386 54 22 773 773 1143 1 1 0 1 132 132 4.6 235.2 17.0 
Average 220 234 332 16 15 1 1 42 42 2.8 42.9 8.6 
Table II. Breakdown of the invariants generated by Udon .
Name Regular Invs Transition Invs Total Invs % Trans. Invs 
Sync01 Safe 12 3 15 20.0 
FibBenchSafe 14 3 17 17.6 
Lazy01Safe 17 5 22 22.7 
Stateful01 Safe 15 6 21 28.6 
DekkerSafe 30 22 52 42.3 
LamportSafe 61 15 76 19.7 
PetersonSafe 44 13 57 22.8 
TimeVarMutex 15 9 24 37.5 
Szymanski 25 10 35 28.6 
IncTrue 22 8 30 26.7 
IncCas 33 5 38 13.2 
IncDec 167 38 205 18.5 
IncDecCas 58 15 73 20.5 
Reorder 83 12 95 12.6 
AccountBad 97 24 121 19.8 
Pfscan 619 221 840 26.3 
nbds-hashtable 1419 645 2064 31.3 
nbds-skiplist01 939 431 1370 31.5 
nbds-listidx01 812 331 1143 29.0 
duetothefactthatHaPSet[7],thedefaultconcurrentcover age 
metricusedby Udon ,canskipcertaininterleavingswherenew 
values of memory could have been explored. As a result, the 
invariants are generated based on an incomplete exploratio n
of the program.
Finally, we examine the scalability of our method.
Columns 15 and 16 of Table I show that, on average, using 
ournewLLVMbasedfrontendforinstrumentationresultsina
ﬁve times speedup over the previous, Daikon , front end. The 
reason is that Daikon ’s front end for C/C++ (called Kvasir) 
uses Valgrind [18] to dynamically instrument the executabl e
everytime itrunstheprogram.Whereas Udon instrumentsthe 
program only once at the compile time. As a result, using our 
newfrontend should providea speed up when analyzingboth 
sequential and multithreaded C/C++ programs.
Table II shows a breakdown of the invariants generated 
by Udon . We classiﬁed each invariant into one of two cat- 
egories: transition invariants over shared variables and a ll
other (regular)invariants.Transition invariantswere ge nerated 
with respect to the entry and exit of each function. Table II 
shows that by considering only transition invariants we can
present the user with a more manageable output compared to 
considering all invariants. As shown in the previous sectio ns,
these transition invariants present a concise summary of th e
concurrency behavior of a program.Figure 10 compares the scalability of three interleaving 
explorationstrategies implementedin Udon .HaPSet [7] is the 
default strategy, DPOR [3] is theoretically the ideal strat egy 
(since it will lead to the most precise results), and PCB [6],
which is a widely used strategy in the testing literature (we
used a context bound of two). In this experiment, we ran 
Udon on the Indexer benchmark from SVCOMP’14 varying 
thenumberofthreadsintheprogram.Here,the x-axisdenotes 
the number of threads, and the y-axis denotes the number of 
interleavings explored by each strategy. The result shows t hat
the number of interleavings quickly explodes under DPOR.
Under PCB, the increase in the number of interleavings is 
slower, since only the interleavings with a bounded number 
of preemptive context switches are explored;nevertheless , the 
growth is still (predictably) exponential with respect to t he 
number of threads. In contrast, the increase in the number of
interleavings is the smallest under HaPSet.
To quantify the effect of different interleaving explorati on 
strategies on the quality of the generated invariants, we ra n
Udon on the benchmarks using HaPSet [7], PCB [6], and 
DPOR [3], respectively. For PCB, we used a context bound 
of two. The results are summarized in Table III. Here, we 
compare the number of runs, time, and number of incorrect
invariants. An ✗in a column indicates the test took longer 
thantwohours.Column1showsthenameofthetestprogram,
Columns 2–4 and 5–7 show the number of runs and time of 
HaPSet,PCB, andDPOR,respectively.Finally,Columns8–10
showthenumberofincorrectinvariantsfoundbyeachmethod .
First, since DPOR provides a sound guarantee to explore 
all relevant thread schedules, it produces no incorrect inv ari- 
ants. However, DPOR suffers from an exponential increase 
in run time relative to the length and number of threads 
in a program. As a result, DPOR failed to ﬁnish analyzing 
nbds-hashtable while both HaPSet and PCB were able 
to ﬁnish in a reasonable time. However, if a user desires high
invariant accuracy at the cost of longer run time, Udon is 
capable of using DPOR instead of HaPSet.
Since both HaPSet and PCB skip interleavings where new 
memory values could be encountered, they both suffer from 
incorrect invariants being generated. However, on average ,
HaPSet performs signiﬁcantly better than PCB in terms of 2 3 4 5 6 7 8 9 10 0500 1000 1500 2000 2500 3000 3500 4000 4500 5000 
Thread Count Number of Iterations Ha PSET 
PCB 
DPOR 
Figure 10. Comparing thethread interleaving exploration s trategies in Udon .
TableIII. Detailed comparisonofinterleaving exploratio n strategies in Udon .
Number of Runs Run Time (s) Incorrect Invariants 
Name HaPSet PCB DPOR HaPSet PCB DPOR HaPSet PCB DPOR 
Sync01 Safe 4 14 7 4.3 4.3 4.7 0 0 0 
FibBenchSafe 6 33 17K 4.2 4.3 139.1 0 0 0 
Lazy01Safe 9 49 40 4.8 5.3 5.5 0 1 0 
Stateful01 Safe 4 13 12 3.9 4.2 4.2 3 1 0 
DekkerSafe 53 13 3896 4.8 4.4 37.6 8 16 0 
LamportSafe 58 19 392 5.3 4.6 9.4 2 9 0 
PetersonSafe 46 13 730 4.7 4.4 12.4 0 0 0 
TimeVarMutex 3 17 4 3.7 4.0 4.2 0 4 0 
Szymanski 111 21 5980 5.2 4.2 50.3 0 11 0 
IncTrue 19 17 212 3.9 3.8 6.1 0 2 0 
IncCas 9 19 33 3.6 4.4 5.0 0 0 0 
IncDec 39 52 484 6.3 6.7 12.0 0 0 0 
IncDecCas 8 22 30 4.8 5.0 5.6 1 8 0 
Reorder 29 506 19K 7.4 12.5 234.9 0 2 0 
AccountBad 9 53 40 6.9 7.4 7.8 2 19 0 
Pfscan 20 100 56K 8.9 11.8 2263 0 0 0 
nbds-hashtable 74 879 ✗39.0 86.1 ✗ 0 0 ✗
nbds-skiplist01 161 249 10K 26.2 20.6 217.0 0 0 0 
nbds-listidx01 132 85 1498 17.0 16.0 44.5 0 0 0 
Average: 42 115 6187 8.75 11.31 161.2 1 4 0 
the number of correct invariants created, the number of runs
explored, and time. For these reasons, we selected HaPSet as
thedefaultsearchstrategyin Udon :itprovidesa goodbalance 
between precision and scalability.
VII. R ELATED WORK 
Static Techniques. There is a large body of work on using 
static analysis [19], [20] for invariant generation, whose main 
advantage is that the reported invariants are true for every
reachable state of the program. Typically, invariants gene r- 
ated by these techniques are predicates expressed in some 
linear abstract domains, such as difference logic, octagon al,
or polyhedral. There are also methods based on constraint
solving [21], [22], [23], which can generate more complex 
invariantssuchaspolynomialandnon-linearinvariants.R ecent
development along this line includes the work by Furia et 
al. [24] on generating loop invariants from post-conditions,
and invariants related to integer arrays [25], [26], [27]. H ow- 
ever, due to the inherent limitations of static analysis, th ese 
methods tend to lack either in precision or in scalability. O ur 
method, in contrast, relies on dynamic analysis.
Dynamic Techniques. There is also a large body of work 
on dynamic invariant generation, including tools such as 
Daikon [1], [2], which havebeen highlysuccessful in practice.
The main advantage of dynamic invariant generation is scala -
bility: they have been applied to realistic applications wh ere 
static techniques fail to scale. Other dynamic invariant ge ner- 
ation tools include DIDUCE [28], DySy [29], Agitator [30],
and Iodine [31]. However, existing dynamic generation tool sdo not work well on multithreaded programs due to the 
nondeterminismin thread scheduling. Our contribution, Udon ,
ﬁlls the gap by solving the issue of nondeterminism with 
respect to dynamic invariant generation.
Hybrid Techniques. There are also hybrid techniques for in- 
variant generation, which leverage both static analysis an d dy- 
namicanalysisto improveperformance.For example,Nguyen
et al.[8], [9] proposed a method for generating invariants 
expressed as polynomials and linear relations over a limite d
number array variables. Such invariants have been difﬁcult to 
generatebyexistingmethods.Therearealsohybridtechniq ues 
basedonrandomtesting[32]andguess-and-check[33],whic h
ﬁrst generate a set of candidate invariants from concrete 
execution data and then verify them using SMT solvers.
Interleaving Exploration. There is a large body of work on 
using selective interleaving exploration techniques for t esting 
concurrent programs, including ConTest [34], CHESS [6],
[35], [36], [37], CTrigger [38], CalFuzzer [39], PENE- 
LOPE [40], and Maple [41], and property guided pruning 
techniques[42],[43]implementedinInspect.Recentempir ical
evaluationsofsuchtechniquescanbefoundin[44], [45],[4 6].
However,the focusof this paper is not on improvingsoftware
testing,butonleveragingthe relatedtechniquesforgener ating 
high-quality invariants. In this sense, our work is orthogo nal
to these existing methods.
Atomicity Inference. Various methods have been proposed 
for inferring atomicity and detecting concurrency bugs. Th ey 
may rely on static analysis [47], dynamic analysis [48], [49 ],
[50], [51], [40], [52], or symbolic analysis [53], [54], [55 ],
[56], [57], [55] techniques. However, their focus is primar ily 
on discovering the intended order of conﬂicting events from
different threads. The thread-local transition invariant s gener- 
ated by our new method is similar to the likely deterministic
speciﬁcations generated by the Determin tool [58], which 
has its own construct for speciﬁcation of invariants. The 
main difference is that Determin relies on a given set of 
thread schedules, whereasin our work, differentschedules are 
generated automatically.
VIII. C ONCLUSIONS 
We presented a new method for dynamically generating 
invariants from multithreaded programs. We used selective
interleaving exploration to simultaneously improve invar iant
quality while keeping runtime overhead low. We also pro- 
posed the use of thread-local transition invariants to help the 
user understand the code and diagnose concurrency errors.
We implemented our method and evaluated it on a set of 
multithreaded C/C++ programs. Our experiments show that,
whencomparedtothestate-of-the-art,suchas Daikon ,ournew 
method produces better invariants while remaining scalabl e.
IX. A CKNOWLEDGMENTS 
This research was primarily supported by the ONR under 
grant N00014-13-1-0527.Partial support was provided by th e
NSF under grants CCF-1149454, CCF-1405697, and CCF- 
1500024. Any opinions, ﬁndings, and conclusions expressed
in this material are those of the authorsand do not necessari ly 
reﬂect the views of the funding agencies.REFERENCES 
[1] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin, “D ynamically 
discovering likely program invariants to support program e volution,” in 
International Conference on Software Engineering , 1999, pp. 213–224.
[2] M. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant, C. Pache co, M. S.
Tschantz, and C. Xiao, “The daikon system for dynamic detect ion of 
likely invariants,” Sci. Comput. Program. , vol. 69, no. 1-3, pp. 35–45,
Dec. 2007.
[3] C. Flanagan and P. Godefroid, “Dynamic partial-order re duction for 
model checking software,” in ACM SIGPLAN-SIGACT Symposium on 
Principles of Programming Languages , 2005, pp. 110–121.
[4] “2014 software veriﬁcation competition. URL: http://s v-comp.sosy- 
lab.org/2014/.” 
[5] P. Godefroid, Partial-Order Methods for the Veriﬁcation of Concurrent 
Systems: An Approach to the State-Explosion Problem . Springer-Verlag 
New York, Inc., 1996.
[6] M.Musuvathi,S.Qadeer,T.Ball,G.Basler,P.A.Nainar, andI.Neamtiu,
“Finding and reproducing heisenbugs in concurrent program s,” in 
USENIX Symposium on Operating Systems Design and Implement ation ,
2008, pp. 267–280.
[7] C. Wang, M. Said, and A. Gupta, “Coverage guided systemat ic con- 
currency testing,” in International Conference on Software Engineering ,
2011, pp. 221–230.
[8] T. Nguyen, D. Kapur, W. Weimer, and S. Forrest, “Using dyn amic 
analysis to discover polynomial and array invariants,” in International 
Conference on Software Engineering , 2012, pp. 683–693.
[9] T. Nguyen, D. Kapur, W. Weimer, and S. Forrest, “Using dyn amic 
analysis to generate disjunctive invariants,” in International Conference 
on Software Engineering , 2014, pp. 608–619.
[10] A. Mazurkiewicz, “Trace theory,” in Advances in Petri Nets 1986, Part 
II on Petri Nets: Applications and Relationships to Other Mo dels of 
Concurrency , 1987, pp. 279–324.
[11] P. Godefroid, “Model checking for programming languag es using 
verisoft,” in ACM SIGPLAN-SIGACT Symposium on Principles of Pro- 
gramming Languages , 1997, pp. 174–186.
[12] A. Podelski and A. Rybalchenko, “Transition invariant s,” International 
Symposium on Logic in Computer Science , pp. 32–41, 2004.
[13] Y. Yang, X. Chen, and G. Gopalakrishnan, “Inspect: A run time model
checker for multithreaded C programs,” University of Utah, Tech. Rep.
UUCS-08-004, 2008.
[14] Y. Yang, X. Chen, G. Gopalakrishnan, and R. Kirby, “Efﬁc ient stateful
dynamic partial order reduction,” in International SPIN Workshop on 
Model Checking Software , 2008, pp. 288–305.
[15] G. Peterson, “Myths about the mutual exclusion problem ,” Information 
Processing Letters , vol. 12, no. 3, pp. 115–116, 1981.
[16] E.W.Dijkstra,“Theoriginofconcurrentprogramming. ” NewYork,NY,
USA:Springer-Verlag NewYork,Inc.,2002,ch.Cooperating Sequential
Processes, pp. 65–138.
[17] “Non-blocking data structures. URL: https://code.go ogle.com/p/nbds/.” 
[18] N. Nethercote and J. Seward, “Valgrind: A program super vision frame- 
work,” Electr. Notes Theor. Comput. Sci. ,vol.89,no.2,pp.44–66,2003.
[19] P. Cousot and R. Cousot, “Abstract interpretation: A un iﬁed lattice 
model for static analysis of programs by construction or app roximation 
of ﬁxpoints,” in ACM SIGACT-SIGPLAN Symposium on Principles of 
Programming Languages , 1977, pp. 238–252.
[20] P. Cousot and N. Halbwachs, “Automatic discovery of lin ear restraints 
among variables of a program,” in ACM SIGACT-SIGPLAN Symposium 
on Principles of Programming Languages , 1978, pp. 84–96.
[21] M. A. Col´ on, S. Sankaranarayanan, and H. B. Sipma, “Lin ear invariant
generation using non-linear constraint solving,” in In Computer Aided 
Veriﬁcation , 2003, pp. 420–432.
[22] S. Sankaranarayanan, H. B. Sipma, and Z. Manna, “Non-li near loop 
invariant generation using Gr¨ obner bases,” in ACM SIGPLAN-SIGACT 
Symposium on Principles of Programming Languages , 2004, pp. 318– 
329.
[23] E. Rodr´ ıguez-Carbonell and D. Kapur, “Generating all polynomial
invariants in simple loops,” J. Symb. Comput. , vol. 42, no. 4, pp. 443– 
476, Apr. 2007.
[24] C. A. Furia and B. Meyer, “Fields of logic and computatio n,” 2010, ch.
Inferring Loop Invariants Using Postconditions, pp. 277–3 00.
[25] M.Bozga,P.Habermehl,R.Iosif,F.Koneˇ cn´ y,andT.Vo jnar,“Automatic 
veriﬁcation of integer array programs,” in International Conference on 
Computer Aided Veriﬁcation , 2009, pp. 157–172.
[26] A. R. Bradley, Z. Manna, and H. B. Sipma, “What’s decidab le about
arrays?” in International Conference on Veriﬁcation, Model Checking,
and Abstract Interpretation , 2006, pp. 427–442.[27] T. A. Henzinger, T. Hottelier, L. Kov´ acs, and A. Voronk ov, “Invariant
andtypeinference formatrices,” in International Conference on Veriﬁca- 
tion, Model Checking, and Abstract Interpretation , 2010, pp. 163–179.
[28] S. Hangal and M. S. Lam, “Tracking down software bugs usi ng 
automatic anomaly detection,” in International Conference on Software 
Engineering , 2002, pp. 291–301.
[29] C. Csallner, N. Tillmann, and Y. Smaragdakis, “DySy: Dy namic sym- 
bolic execution for invariant inference,” in International Conference on 
Software Engineering , 2008, pp. 281–290.
[30] M. Boshernitsan, R. Doong, and A. Savoia, “From daikon t o agitator:
Lessons and challenges in building a commercial tool for dev eloper 
testing,” in International Symposium on Software Testing and Analysis ,
2006, pp. 169–180.
[31] S. Hangal, N. Chandra, S. Narayanan, and S. Chakravorty , “Iodine: a 
tool to automatically infer dynamic invariants for hardwar e designs,” in 
Design Automation Conference , 2005, pp. 775–778.
[32] M. Li, “A practical loop invariant generation approach based on random 
testing, constraint solving and veriﬁcation,” in International Conference 
on Formal Engineering Methods: Formal Methods and Software Engi- 
neering , 2012, pp. 447–461.
[33] R. Sharma, S. Gupta, B. Hariharan, A. Aiken, P. Liang, an d A. V. Nori,
“A data driven approach for algebraic loop invariants,” in European 
Symposium on Programming , 2013, pp. 574–592.
[34] O. Edelstein, E. Farchi, Y. Nir, G. Ratsaby, and S. Ur, “M ultithreaded 
java program test generation,” IBM Syst. J. , vol. 41, no. 1, pp. 111–125,
Jan. 2002.
[35] K. E. Coons, M. Musuvathi, and K. S. McKinley, “Bounded p artial- 
order reduction,” in ACM SIGPLAN Conference on Object Oriented 
Programming, Systems, Languages, and Applications , 2013, pp. 833– 
848.
[36] S. Bindal, S. Bansal, and A. Lal, “Variable and thread bo unding for sys- 
tematic testing of multithreaded programs,” in International Symposium 
on Software Testing and Analysis , 2013, pp. 145–155.
[37] M. Emmi, S. Qadeer, and Z. Rakamaric, “Delay-bounded sc heduling,” 
in ACM SIGACT-SIGPLAN Symposium on Principles of Programming
Languages , 2011, pp. 411–422.
[38] S. Park, S. Lu, and Y. Zhou, “Ctrigger: Exposing atomici ty violation 
bugs from their hiding places,” in International Conference on Archi- 
tectural Support for Programming Languages and Operating S ystems ,
2009, pp. 25–36.
[39] P. Joshi, M. Naik, C.-S. Park, and K. Sen, “CalFuzzer: An extensible 
active testing framework for concurrent programs,” in International 
Conference on Computer Aided Veriﬁcation , 2009, pp. 675–681.
[40] F. Sorrentino, A. Farzan, and P. Madhusudan, “PENELOPE : weaving 
threads to expose atomicity violations,” in ACM SIGSOFT Symposium 
on Foundations of Software Engineering , 2010, pp. 37–46.
[41] J.Yu,S.Narayanasamy, C.Pereira, and G.Pokam, “Maple : Acoverage- 
driven testing tool for multithreaded programs,” in ACM SIGPLAN 
Conference on Object Oriented Programming, Systems, Langu ages, and 
Applications , 2012, pp. 485–502.
[42] C. Wang, Y. Yang, A. Gupta, and G. Gopalakrishnan, “Dyna mic model
checking with property driven pruning to detect race condit ions,” in 
International Symposium on Automated Technology for Veriﬁ cation and 
Analysis , 2008, pp. 126–140.
[43] M. Kusano and C. Wang, “Assertion guided abstraction: a cooperative 
optimization fordynamic partial orderreduction,” in IEEE/ACMInterna- 
tional Conference On Automated Software Engineering , 2014, pp. 175– 
186.
[44] P. Thomson, A. F. Donaldson, and A. Betts, “Concurrency testing using 
schedule bounding: An empirical study,” in ACM SIGPLAN Symposium 
on Principles and Practice of Parallel Programming , 2014, pp. 15–28.
[45] S. Hong, M. Staats, J. Ahn, M. Kim, and G. Rothermel, “The impact of 
concurrent coverage metrics on testing effectiveness,” in IEEE Interna- 
tional Conference on Software Testing, Veriﬁcation and Val idation , 2013,
pp. 232–241.
[46] S.Hong,J.Ahn,S.Park,M.Kim,andM.J.Harrold,“Testi ngconcurrent
programs to achieve high synchronization coverage,” in International 
Symposium on Software Testing and Analysis , 2012, pp. 210–220.
[47] M. Xu, R. Bod´ ık, and M. D. Hill, “A serializability viol ation detector 
for shared-memory server programs,” in ACM SIGPLAN Conference on 
Programming Language Design and Implementation , 2005, pp. 1–14.
[48] S. Lu, J. Tucek, F. Qin, and Y. Zhou, “Avio: Detecting ato micity 
violations viaaccessinterleaving invariants,” in International Conference 
on Architectural Support for Programming Languages and Ope rating 
Systems , 2006, pp. 37–48.[49] L. Wang and S. D. Stoller, “Runtime analysis of atomicit y for mul- 
tithreaded programs,” IEEE Trans. Software Eng. , vol. 32, no. 2, pp.
93–110, 2006.
[50] F. Chen, T. Serbanuta, and G. Rosu, “jPredictor: a predi ctive runtime 
analysis tool for java,” in International Conference on Software Engi- 
neering , 2008, pp. 221–230.
[51] Y. Yang, X. Chen, G. Gopalakrishnan, and C. Wang, “Autom atic dis- 
covery of transition symmetryin multithreaded programs us ing dynamic 
analysis,” in International SPIN workshop on Model Checking Software ,
2009, pp. 279–295.
[52] A. Sinha, S. Malik, C. Wang, and A. Gupta, “Predictive an alysis 
for detecting serializability violations through trace se gmentation,” in 
International Conference on Formal Methods and Models for C odesign ,
2011, pp. 99–108.
[53] C. Wang, R. Limaye, M. Ganai, and A. Gupta, “Trace-based symbolic 
analysis for atomicity violations,” in International Conference on Tools 
and Algorithms for Construction and Analysis of Systems , 2010, pp.
328–342.[54] V. Kahlon and C. Wang, “Universal Causality Graphs: A pr ecise 
happens-before model for detecting bugs in concurrent prog rams,” in 
International Conference on Computer Aided Veriﬁcation , 2010, pp.
434–449.
[55] A. Sinha, S. Malik, C. Wang, and A. Gupta, “Predicting se rializability 
violations: SMT-based search vs. DPOR-based search,” in Haifa Veriﬁ- 
cation Conference , 2011, pp. 95–114.
[56] M. Said, C. Wang, Z. Yang, and K. Sakallah, “Generating d ata race 
witnesses by an SMT-based analysis,” in NASA Formal Methods , 2011,
pp. 313–327.
[57] N. Sinha and C. Wang, “On interference abstractions,” i nACM SIGACT- 
SIGPLAN Symposium on Principles of Programming Languages , 2011,
pp. 423–434.
[58] J. Burnim and K. Sen, “DETERMIN: inferring likely deter ministic 
speciﬁcations of multithreaded programs,” in International Conference 
on Software Engineering , 2010, pp. 415–424.