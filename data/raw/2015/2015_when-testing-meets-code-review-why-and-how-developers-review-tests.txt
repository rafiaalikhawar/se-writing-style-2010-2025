When Testing Meets Code Review:
Why and How Developers Review Tests
Davide Spadini
Delft University of Technology
Software Improvement Group
Delft, The Netherlands
d.spadini@sig.euMaurício Aniche
Delft University of Technology
Delft, The Netherlands
m.f.aniche@tudelft.nlMargaret-Anne Storey
University of Victoria
Victoria, BC, Canada
mstorey@uvic.ca
Magiel Bruntink
Software Improvement Group
Amsterdam, The Netherlands
m.bruntink@sig.euAlberto Bacchelli
University of Zurich
Zurich, Switzerland
bacchelli@ifi.uzh.ch
ABSTRACT
Automated testing is considered an essential process for ensuring
softwarequality.However,writingandmaintaininghigh-quality
test code is challenging and frequently considered of secondary
importance. For production code, many open source and industrial
software projects employ code review, a well-established software
qualitypractice,butthequestionremainswhetherandhowcode
reviewisalsousedforensuringthequalityoftestcode.Theaim
ofthisresearchistoanswerthisquestionandtoincreaseourun-
derstanding of what developers think and do when it comes to
reviewing test code. We conducted both quantitative and quali-
tative methods to analyze more than 300,000 code reviews, and
interviewed 12 developers about how they review test files. This
workresultedinanoverviewofcurrentcodereviewingpractices,a
set of identified obstacles limiting the review of test code, and a setofissuesthatdeveloperswouldliketoseeimprovedincodereview
tools. The study reveals that reviewing test files is very different
fromreviewingproductionfiles,andthatthenavigationwithinthe
review itself is one of the main issues developers currently face.Based on our findings, we propose a series of recommendations
and suggestions for the design of tools and future research.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
KEYWORDS
software testing, automated testing, code review, Gerrit
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthe firstpage.Copyrights forcomponentsof thisworkowned byothersthan the
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180192ACM Reference Format:
Davide Spadini, Maurício Aniche, Margaret-Anne Storey, Magiel Bruntink,
andAlbertoBacchelli.2018.WhenTestingMeetsCodeReview:Whyand
How Developers Review Tests. In Proceedings of ICSE ’18: 40th International
Conference on Software Engineering , Gothenburg, Sweden, May 27-June 3,
2018 (ICSE ’18), 11 pages.
https://doi.org/10.1145/3180155.3180192
1 INTRODUCTION
Automatedtestinghasbecomeanessentialprocessforimproving
thequalityofsoftwaresystems[ 15,31].Automatedtests(hereafter
referredtoasjust‘tests’)canhelpensurethatproductioncodeis
robust undermany usageconditions andthat codemeets perfor-
mance and security needs [ 15,16]. Nevertheless, writing effective
testsisaschallengingaswritinggoodproductioncode.Atesterhas
toensurethattestresultsareaccurate,thatallimportantexecutionpathsareconsidered,andthattheteststhemselvesdonotintroducebottlenecksinthedevelopmentpipeline[
15].Likeproductioncode,
test code must also be maintained and evolved [49].
Astestinghasbecomemorecommonplace,somehaveconsidered
that improving the quality of test code should help improve the
qualityoftheassociatedproductioncode[ 21,47].Unfortunately,
thereisevidencethattestcodeisnotalwaysofhighquality[ 11,49].
Vazhabzadeh et al.showed that about half of the projects they
studied had bugs in the test code [ 46]. Most of these bugs create
false alarms that can waste developer time, while other bugs cause
harmfuldefectsinproductioncodethatcanremainundetected.Wealsoseethattestcodetendstogrowovertime,leadingtobloatand
technical debt [49].
Ascodereviewhasbeenshowntoimprovethequalityofsource
code in general [ 12,38], one practice that is now common in many
development projects is to use Modern Code Review (MCR) to
improve the quality of test code. But how is test code reviewed? Is
it reviewed as rigorously as production code, or is it reviewed atall? Are there specific issues that reviewers look for in test files?
Doestestcodeposedifferentreviewingchallengescomparedtothereview of production code? Do some developers use techniques for
reviewing test code that could be helpful to other developers?
To address these questions and find insights about test code
review, we conducted a two-phase study to understand how test
6772018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden D. Spadini et al.
codeisreviewed,toidentifycurrentpracticesandrevealthechal-
lenges faced during reviews, and to uncover needs for tools and
featuresthatcansupportthereviewoftestcode.Tomotivatethe
importance of our work, we first investigated whether being a test
changesthechancesofafiletobeaffectedbydefects.Havingfound
no relationship between type of file and defects, then in the first
phase,weanalyzedmorethan300,000codereviewsrelatedtothree
opensourceprojects(Eclipse,OpenStackandQt)thatemployex-
tensive code review and automated testing. In the second phase,
we interviewed developers from these projects and from a variety
of other projects (from both open source and industry) to under-
standhowtheyreviewtestfilesandthechallengestheyface.These
investigations led to the following research contributions:
(1)To motivate our study, we first explored whether the type of
code(productionortest)isassociatedwithfuturedefects.Our
resultsshowthatthereisnoassociation,whichsuggeststhattest
files are no less likely to have defects in the future and should
benefit from code review.
(2)Weinvestigatedhowtestcodeisreviewedandfoundempirical
evidencethattestfilesarenotdiscussedasmuchasproduction
files during code reviews, especially when test code and pro-
duction code are bundled together in the same review. When
test files are discussed, the main concerns include test coverage,
mocking practices, code maintainability, and readability.
(3)Wediscoveredthatdevelopersfaceavarietyofchallengeswhen
reviewing test files, including dealing with a lack of testing con-
text, poor navigation support within the review, unrealistic time
constraints imposed by management, and poor knowledge of
goodreviewingandtestingpracticesbynovicedevelopers.We
discuss recommendations for practitioners and educators, and
implications for tool designers and researchers.
(4)We created GerritMiner, an open source tool that extracts
code reviews from projects that use Gerrit. When performing
a review, GerritMiner provides information regarding files,
comments,andreviewers.Wedesignedthistooltohelpuscol-
lectadatasetof654,570codereviewsfromthreepopularopen
source, industry-supported software systems. GerritMiner and
the dataset we studied are publicly available [7].
2 BACKGROUND AND MOTIVATION
Pastresearchhasshownthatbothtestcodeandproductioncode
suffer from quality issues [ 11,32,49]. We were inspired by the
study by Vahabzadeh et al.[46] who showed that around half of
all the projects they studied had bugs in the test code, and even
though the vast majority of these test bugs were false alarms, they
negatively affected the reliability of the entire test suite. They also
foundthatothertypesoftestbugs(silenthorrors )maycausetests
tomissimportantbugsinproductioncode,creatingafalsesense
of security. This study also highlighted how current bug detection
tools are not tailored to detect test bugs [ 46], thus making the role
of effective test code review even more critical.
SomeresearchershaveexaminedMCRpracticesandoutcomes
andshowedthatcodereviewcanimprovethequalityofsourcecode.
For example, Bacchelli et al.[12] interviewed Microsoft developers
and found that code reviews are important not only for finding
defectsorimprovingcode,butalsofortransferringknowledge,andfor creating a bigger and more transparent picture of the entire
system. McIntosh et al.[30] found that both code review coverage
andparticipationshareasignificantlinkwithsoftwarequality,pro-
ducing components with up to two and five additional post-release
defects, respectively. Thongtanunam et al.[43] evaluated the im-
pactthatcharacteristicsofMCRpracticeshaveonsoftwarequality,
studying MCR practices in defective and clean source code files.
DiBiaseetal.[22]analyzedtheChromiumsystemtounderstand
the impact of MCR on finding security issues, showing that the
majority of missed security flaws relate to language-specific issues.
However, these studies, as well as most of the current literature on
contemporary code review, either focus on production files only or
do not explicitly differentiate production from test code.
Nevertheless, pastliterature has shown that test code is substan-
tially different from production code. For instance, van Deursen et
al.[21]showedthatwhenrefactoringtestcode,thereisaunique
set ofcode smells—distinct from thatof production code—because
improving test code involves additional test-specific refactoring.Moreover, test files have their own libraries that lead to specific
codingpractices:forexample,Spadini etal.[41]studiedatestprac-
tice called mocking, revealing that the usage of mocks is highlydependent on the responsibility and the architectural concern of
the production class.
Furthermore, other literature shows that tests are constantly
evolvingtogetherwithproductioncode.Zaidman etal.[49]investi-
gated how test and production code co-evolve in both open source
and industrial projects, and how test code needs to be adapted,
cleaned and refactored together with production code.
Due to the substantial differences between test code and pro-
duction code, we hypothesize that how they should be reviewed
mayalsodiffer.However,eventhoughcodereviewisnowwidely
adopted in both open source and industrial projects, how it is con-
ductedontestfilesisunclear.Weaimtounderstandhowdevelopers
reviewtestfiles,whatdevelopersdiscussduringreviewsessions,
whattoolsorfeaturesdevelopersneedwhenreviewingtestfiles,
and what challenges they face.
3 SHOULD TEST FILES BE REVIEWED?
Eventhoughpreviousliteraturehasraisedawarenessonthepreva-lenceofbugsintestfiles,suchastheworkbyVahabzadeh etal.[
46],
it may well be that these type of bugs constitute such a negligible
number compared to defects found in production code that invest-
ingresourcesinreviewingthemwouldnotbeadvisable.Iftestcodetendstobelessaffectedbydefectsthanproductioncode,pragmaticdevelopersshouldfocustheirlimitedtimeonreviewingproduction
code, and research efforts should support this.
Tomotivateourresearchandtounderstandwhethertestcode
shouldbereviewedatall,weconductedapreliminaryinvestigationtoseewhethertestandproductioncodefilesareequallyassociated
with defects. To studythis, we useda research methodproposed
byMcIntosh etal.[30]where theyanalyzedwhethercodereview
coverage and participation had an influence on software quality.
They built a statistical model using the post-release defect count
as a dependentvariable and metrics highly correlatedwith defect-
pronenessasexplanatoryvariables.Theythenevaluatedtheeffects
of each variable by analyzing their importance in the model [17].
678
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. When Testing Meets Code Review: Why and How Developers Review Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Weappliedthesameideainourcasestudy;however,asouraim
was to understand whether test files are less likely to have bugs
thanproductionfiles,weaddedthetypeoffile(i.e.,eithertestor
production) as an additional explanatory variable. If a difference
existed,wewouldexpectthevariabletobesignificantforthemodel.
Ifthevariablewasnotrelevant,atestfileshouldneitherincrease
ordecreaseincreasethelikelihoodofdefects,indicatingthattest
files should be reviewed with the same care as production files
(assuming one also cares about defects with tests).
Similar to McIntosh et al.[30], we used the three most common
familiesofmetricsthatareknowntohavearelationshipwithdefect
pronenessascontrolvariables,namely productmetrics (size),process
metrics(priordefects,churn,cumulativechurn),and humanfactors
(total number of authors, minor authors, major authors, author
ownership). To determine whether a change fixed a defect (our
dependentvariable),wesearchedversion-controlcommitmessages
for co-occurrences of defect identifiers with keywords like ‘bug’,
‘fix’,‘defect’,or‘patch’.Thisapproachhasbeenusedextensivelyto
determine defect-fixing and defect-inducing changes [27, 28].
Weconsideredthesamesystemsthatweusedforthemainparts
ofourstudy(Qt,Eclipse,andOpenstack)astheyperformconsid-
erablesoftwaretestingandtheirrepositoriesareavailableonline.
Due to the size of their code repositories, we analyzed a sample
of sub-projects from each one of them. For QT and Openstack, we
chosethefivesub-projectsthatcontainedmorecodereviews: qt,
qt3d,qtbase,qtdeclarative, qtwebengine (Qt),and cinder,heat,neu-
tron,nova, and tempest(Openstack). Eclipse, on the other hand,
does not use identifiers in commit messages. Therefore, we used
the dataset provided by Lam et al.[29] for thePlatformsub-project.
ThisdatasetincludesallthebugsreportedintheEclipsebugtracker
tool, together with the corresponding commit hash, files changed,
and other useful information.
We measured dependent and independent variables during the
six-month period prior to a release date in a release branch. We
chose the release that gave us at least 18 months of information to
analyze.IncontrasttoMcIntosh etal.’s[30]work,wecalculated
metrics at the file level (not package level) to measure whether the
file being test code vs. production code had any effect.
We observed that the number of buggy commits was much
smallercomparedtothenumberofnon-buggycommits, i.e.,classes
were imbalanced, which would bias the statistical model. There-
fore, we applied SMOTE (Synthetic Minority Over-sampling TEch-
nique)[18]tomakebothclassesbalanced.AllRscriptsareavailable
in our online appendix [7].
Toranktheattributes,weusedWeka[ 6],asuiteofmachinelearn-
ingsoftwarewritteninJava.Wekaprovidesdifferentalgorithmsfor
identifyingthemostpredictiveattributesinadataset—wechose In-
formation Gain Attribute Evaluation (InfoGainAttributeEval) , which
has been extensively used in previous literature [ 9,26,40]. In-
foGainAttributeEval is a method that evaluates the worth of an
attribute by measuring the information gain with respect to the
class.Itproducesavaluefrom0to1,whereahighervalueindicates
a stronger influence.
The precision and recall of the resulting model were above 90%,
indicatingthatitisabletocorrectlyclassifywhethermostofthe
files contain defects, strengthening the reliability of the results.Table1:Rankingoftheattributes,bydecreasingimportance
Attribute Average merit Average Rank
Churn 0.753±0.010 1 ±0
Author ownership 0 .599±0.002 2 .2±0.4
Cumulative churn 0 .588±0.013 2 .8±0.4
Total authors 0 .521±0.001 4 ±0
Major authors 0 .506±0.001 5 ±0
Size 0.411±0.027 6 ±0
Prior defects 0 .293±0.001 7 ±0
Minor authors 0 .149±0.001 8 ±0
Is test 0.085±0.001 9 ±0
We ran the algorithm using 10-fold cross-validation. Table 1
showstheresultsoftheimportanceofeachvariableinthemodel
as evaluated by InfoGainAttributeEval. The variable is testwas
consistentlyrankedastheleastimportantattributeinthemodel,
whileChurn,Author ownership, and Cumulative churn were the
mostimportantattributes(inthatorder)forpredictingwhethera
filewilllikelycontainabug.Thisisinlinewithpreviousliterature.
From this preliminary analysis, we found that the decision to
review a file should not be based on whether the file contains
productionortestcode,asthishasnoassociationwithdefects.Mo-
tivated bythis result,we conductedour investigation ofpractices,discussions, and challenges when reviewing tests.
4 RESEARCH METHODOLOGY
Themain goalofourstudyistoincreaseourunderstandingofhow
test code is reviewed. To that end, we conducted mixed methods
research[19] to address the following research questions:
RQ1: Howrigorouslyistestcodereviewed? Previousliterature
hasshownthatcodechangesreviewedbymoredevelopersare
less prone to future defects [ 35,38], and that longer discussions
between reviewers help find more defects and lead to better
solutions[ 30,45].Basedonourpreliminarystudy(Section3)that
showedhowthetypeoffile(testvs.production)doesnotchange
its chances of being prone to future defects, and to investigate
the amount of effort developers expend reviewing test code, we
measuredhowoftendeveloperscommentontestfiles,thelength
ofthesediscussions,andhowmanyreviewerscheckatestfile
before merging it.
RQ2: Whatdoreviewersdiscussintestcodereviews? In line
withBacchelli& Bird[ 12],weaimed tounderstandtheconcerns
thatreviewersraisewheninspectingtestfiles.Bacchelli&Bird
notedthatdevelopersdiscusspossibledefectsorcodeimprove-
ments,andsharecommentsthathelponeunderstandthecode
orsimplyacknowledgewhatotherreviewershaveshared.Weinvestigated if similar or new categories of outcomes emerge
when reviewing test code compared with production code to
gatherevidenceonthekeyaspectsoftestfilereviewsandonthe
reviewers’ needs when working with this type of artifact.
RQ3: Whichpracticesdoreviewersfollowfortestfiles? Little
isknownaboutdeveloperpracticeswhenreviewingtestfiles.To
identifythem,weintervieweddevelopersfromthe3opensource
projectsanalyzedinthefirsttwoRQs,aswellasdevelopersfrom
679
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden D. Spadini et al.
otherprojects(includingclosedprojectsinindustry).Weasked
themhowtheyreviewtestfilesandiftheirpracticesaredifferent
tothosetheyusewhenreviewingproductionfiles.Thishelped
discover review patterns that may guide other reviewers and
triangulate reviewers’ needs.
RQ4: Whatproblemsandchallengesdodevelopersfacewhen
reviewing tests? We elicited insights from our interviews to
highlightimportantissuesthatbothresearchersandpractition-
ers can focus on to improve how test code is reviewed.
Inthefollowingsubsections,wediscussthethreedatacollection
methods used in this research. Section 4.1 describes the three open
source projects we studied and Section 4.2 explains how we ex-
tracted quantitative data related to the prevalence of code reviews
intestfiles.Section4.3discussesthemanualcontentanalysiswe
conductedon astatisticallysignificant datasampleof comments
pertainingtoreviewsoftestcode.Section4.4describestheinter-
viewprocedureusedtocollectdataaboutpractices,challenges,and
needs of practitioners when reviewing test files.
4.1 Project Selection
To investigate what the current practices in reviewing test files are,
weaimedatchoosingprojectsthat(1)testtheircode,(2)intensively
reviewtheircode,and(3)useGerrit,amoderncodereviewtoolthat
facilitatesatraceablecodereviewprocessforgit-basedprojects[ 30].
Thethreeprojectswestudiedinourpreliminaryanalysis(dis-
cussedinSection3),matchthesecriteria: Eclipse,Openstack and
Qtand we continue to study these projects to answer our research
questions. Moreover, these projects are commonly studied in code
review research [ 24,30,43]. Table 2 lists their descriptive statistics.
Table 2: Subject systems’ details after data pre-processing
#o f
prod. files#o f
test files#o fc od e
reviews#o f
reviewers#o f
comments
Eclipse 215,318 19,977 60,023 1,530 95,973Openstack 75,459 48,676 199,251 9,221 894,762Qt 159,894 8,871 114,797 1,992 19,675
Total 450,671 77,524 374,071 12,743 1,010,410
4.2 Data Extraction and Analysis
To investigate how developers review test files, we extracted code
review data from the Gerrit review databases of the systems under
study.Gerritexplicitlylinkscommits inaVersionControlSystem
(VCS) to their respective code review. We used this link to connect
commits to their relevant code review, obtaining information re-
gardingwhichfileshavebeenmodified,themodifiedlines,andthe
number of reviewers and comments in the review.
EachreviewinGerritisuniquelyidentifiedbyahashcodecalled
Change-ID. After a patch is accepted by all the reviewers, it isautomatically integrated into the VCS. For traceability purposes,
thecommitmessageoftheintegratedpatchcontainstheChange-
ID; we extracted this Change-ID from commit messages to link
patches in the VCS with the associated code review in Gerrit.
To obtain code review data, we created GerritMiner, a tool
that retrieves allthe code reviews from Gerrit for each project
using the Gerrit REST API [ 4]. The tool saves all review-relateddata (e.g., Change-ID, author, files, comments, and reviewers) ina MySQL database. Through GerritMiner, we retrieved a total
of654,570 codereviewspertainingto thethree systems.Since we
wereinterestedinjustproductionandtestfiles,weonlystoredcode
reviewsthatchanged sourcecode files(e.g.,wedidnotconsider‘.txt’
file, ‘README’, JSON files, configuration files). After this process,
we were left with 374,071 reviews. Table 2 presents the statistics.
ToanswerRQ 1,weselectedonlyreviewsthatcontainedlessthan
50filesandhadatleastonereviewerinvolvedwhowasdifferent
thantheauthor[ 8,34,37].Infact,asexplainedbyRigby etal.[36],a
codereviewshouldideallybeperformedonchangesthataresmall,
independent, and complete: a small change lets reviewers focus on
theentirechangeandmaintainanoverallpictureofhowitfitsintothesystem.Aswewereinterestedincodereviewswherereviewers
actually examined the code closely, we did not consider reviews
where theauthor wasthe onlyreviewer.We alsodid not consider
bots as reviewers (e.g., Jenkins and Sanity Bots). At the end, the
distribution of the number of reviewers per review (excluding the
author)isthefollowing:27%have1reviewer,26%have2,16%have
3, 10% have 4, and 20% have more than 4.
Tounderstandhowoftenandextensivelydiscussionsareheld
duringreviewsabouttestfiles,weconsideredthefollowingmetrics
asproxies,whichhavebeenvalidatedinpreviousliterature[ 42]:
number of comments in the file, number of files with comments,
numberofdifferentreviewers,andthelengthofthecomments.We
only considered code reviews that contained at least one comment
becausewewereinterestedinunderstandingwhethertherewas
anydifferencebetweenreviewdiscussionsoftestandproduction
files, and reviews that do not contain any discussion are not useful
for this investigation.
We used the production file metrics as a baseline and separately
analyzedthethreedifferentreviewscenariosbasedonwhatneededtobereviewed:(1)bothproductionandtestfiles,(2)onlyproduction
files, or (3) only test files.
4.3 Manual Content Analysis
To answer RQ 2, we focused on the previously extracted comments
(Section 4.2) by practitioners reviewing test files. To analyze the
contentofthesecomments,weperformedamanualanalysissimilar
to Bacchelli & Bird [ 12]. Due to the size of the total number of
comments(1,010,410),weanalyzedastatisticallysignificantrandomsample.Oursampleof600commentswascreatedwithaconfidence
level of 99% and error (E ) of 5% [44].
Themanualanalysiswasconductedbythefirsttwoauthorsof
this paper, using the following process: (1) Each researcher wasresponsible for coding 300 comments. (2) All the sampled com-
ments were listed in a shared spreadsheet. (3) The researchers
workedtogethertocategorizeandexplaincomments,usinga nego-
tiatedagreement technique[ 39]toachieveagreement.Asagreement
is negotiated on-the-fly, there is no inter-rater agreement value.
Agreement was found after 60 comments, at which point the work
continuedinparallel.(4)Asastartingpoint,researchersusedthe
same categories described by Bacchelli & Bird [ 12], including code
improvement, understanding, socialcommunication, defect,knowl-
edgetransfer, miscellaneous, testing,externalimpact,and reviewtool.
680
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. When Testing Meets Code Review: Why and How Developers Review Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Furthermore,researchersdidasecondpasstoretrievemorefine–
grainedinformationforeachcategory,obtainingmoredetailson
whatdevelopersdiscuss.(5)Incaseofdoubt, i.e.,thecategoryof
a specific comment was not clear to one of the researchers, the
category was then analyzed by both researchers together.
4.4 Interviews
To answer RQs 3 and 4, guided by the results of the previous RQs,
we designed an interview in which the goal was to understand
which practices developers apply when reviewing test files. The in-
terviews were conducted by the first author of this paper and were
semi-structured,aformofinterviewoftenusedinexploratoryin-
vestigations to understand phenomena and seek new insights [ 48].
Each interview started with general questions about code re-
views,withtheaimofunderstandingwhytheintervieweeperforms
code reviews, whether they consider it an important practice, and
how they perform them. Our interview protocol also contained
many questions derived from the results of previous research ques-
tions.Ourfullinterviewprotocolisavailableintheappendix[ 7].
We asked interviewees the following main questions:
(1) What is the importance of reviewing these files?
(2)How do you conduct reviews? Do you have specific prac-
tices?
(3)What are the differences between reviewing test files and
production files?
(4)Whatchallengesdoyoufacewhenreviewingtestfiles?What
are your needs related to this activity?
During each interview, the researcher summarized the answers,
and before finalizing the meeting, these summaries were presented
totheintervieweetovalidateourinterpretationoftheiropinions.
We conducted all interviews via Skype. With the participants’ con-
sent, the interviews were recorded and transcribed for analysis.
We analyzed the interviews by initially assigning codes [ 25]t o
all relevant pieces of information, and then grouped these codes
into higher-level categories. These categories formed the topics we
discuss in our results (Section 5).
We conducted 12 interviews (each lasting between 20 and 50
minutes)withdevelopersthatperformcodereviewsaspartoftheir
dailyactivities.Threeofthesedevelopersworkedontheprojects
westudiedinthepreviousRQs.Inaddition,wehadoneparticipant
from another open source project and 8 participants from industry.
Table 3 summarizes the interviewees’ demographics.
4.5 Threats to Validity and Limitations
We describe the threats to validity and limitations to the results of
our work, as posed by the research methodology that we applied.
Construct validity. When building ourmodel we assume that each
post-releasedefecthasthesameimportance,wheninrealitythis
could not be the case. We mitigate this issue analyzing only therelease branch of the systems, which are more controlled than a
developmentbranch,toensurethatonlytheappropriatechanges
will appear in the upcoming release [30].
Thecontentanalysiswasperformedmanually,thusgivingriseto
potentiallysubjectivejudgement.Tomitigatethisthreat,weemploy
the negotiated agreement technique [ 39] between the first and
second authors until agreement was reached (after 60 comments).Table3:Interviewees’experience(inyears)andworkingcon-
text (OSS project, or company)
IDYears as
developerYears as
reviewerWorking context
P1 5 5 OSS
P2 10 10 EclipseP3 10 10 Company AP4 20 6 QtP5 13 8 Company B
P6 5 5 Openstack
P7 7 5 Company C
P8 5 1 Company D
P9 11 3 Company E
P10 9 9 Company FP11 7 2.5 Company DP12 6 4.5 Company D
Internalvalidity–Credibility. Threatsto internalvalidity concern
factorswedid notconsiderthatcouldaffectthe variablesandthe
relations being investigated. In our study, we interview develop-
ers from the studied software to understand how they review testfiles.Everydeveloperhasaspecificwayofreviewing,whichmay
differ from the practices ofother practitioners. We try to mitigate
this issue by interviewing a range of developers from different
open-source and industry projects. In addition, their interviewees’
opinionsmayalsobeinfluencedbyotherfactors,suchascurrent
literature on MCR, which could may have led them to social desir-
ability bias[23], or bypractices in otherprojects that theypartici-
patein.Tomitigatethisissue,weconstantlyremindedinterviewees
that we were discussing the code review practices specifically of
their project. At the end of the interview, we asked them to freely
talk about their ideas on code reviews in general.
Generalizability – Transferability. Our sample contains three open-
source systems, which is small compared to the overall population
of software systems that make use of code reviews. We reduce this
issue by considering diverse systems and by collecting opinions
from other open-source projects as well as from industry.
5 RESULTS
Inthissection,wepresenttheresultstoourresearchquestionsthat
aimed to understand how rigorously developers review tests, what
developers discuss with each other in their reviews of test code,
andwhatpractices andchallengesdevelopersuseandexperience
while performing these code reviews.
RQ1. How rigorously is test code reviewed?
InTable 4,we showthe distributionof commentsin codereviews
forbothproductionandtestfileswhentheyareinthesamereview,
and for code reviews that only contain either type.
Discussionincodereviewsoftestfiles. Thenumberoftestfile
reviews that contain at least one comment ranges from 29% (in
reviews that combinetest and production files) to48% (in reviews
that only look at test files). The number of production files that
681
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden D. Spadini et al.
Table 4: The prevalence of reviews in test files vs production files (baseline)
Code review#o f
files# of files
w/ comments# of files
wo/ commentsodds ratio # of commentsAvg number
of commentsAvg #
of reviewersAvg length
of comments
Production 157,507 68,338 (43%) 89,169 (57%)1.90472,020 3.005.4919.09
Test 102,266 29,327 (29%) 72,939 (71%) 129,538 1.27 15.32
Only production 74,602 32,875 (44%) 41,725 (56%)0.86122,316 1.64 3.95 18.13
Only test 22,732 10,808 (48%) 11,924 (52%) 52,370 2.30 5.15 17.01
contain at least a single comment ranges from 43% (when together
withtestfiles)to44%(inreviewsthatonlylookatproductionfiles).
In a code review that contains both types of files, the odds of
aproductionfilereceivingacommentis1 .90[1.87−1.93]higher
thanwithtestfiles.Ontheotherhand,whenareviewonlycontains
onetypeoffile,theoddsofatestfilereceivingacommentishigher
than that of a production file: 1 .15[1.11−1.18].
Wealsoobservedalargenumberoffilesthatdidnotreceiveany
discussion. Thenumber ofcode filesthat did notreceive atleast a
single comment ranges from 52% (in reviews that only look at test
files) to 71% (in reviews that combine test and production files).
Discussion intensity in test files. Inthecodereviewsthatcon-
tainbothtypesoffiles,productionfilesreceivedmoreindividual
comments than test files (3 .00 comments per file for production,
1.27commentsfortests).Thedifferenceisstatisticallysignificant
butsmall(Wilcoxonp-value <2.2e−16,Cliff’sDelta=-0.1643);this
is due to the large number of files with no comments (median =
0inbothtestandproduction,3 rdquantile=1).Thedifferenceis
largerwhenweanalyzeonlyfileswithatleastasinglecomment
(Wilcoxon p-value <2.2e−16, Cliff’s Delta=-0.1385).
Again,numberschangewhenbothfilesarenotbundledinthe
same review. Code reviews on only production files contain fewer
individualcommentsonaveragethanreviewsononlytestfiles(1 .64
comments for production, 2 .30 comments for tests). The difference
is statistically significant but small (Wilcoxon p-value <2.2e−16,
Cliff’s Delta=0.0585).
Production files receive longer comments than test files on aver-
age, both when they are in the same review (an average of 19 .08
characters per comment in a production file against 15 .32 in a
test) and when they are not (18 .13 against 17 .01). The difference is
againstatisticallysignificantbutsmall(Wilcoxonp-value <2.2e−16,
Cliff’s Delta=0.0888).
Reviewers of test files. The number of reviewers involved in
reviews containing both files and only tests is slightly higher com-
pared to reviews containing production files. However, from the
Wilcoxon rank sum test and the effect size, we observe that the
overall difference is statistically significant but small (Wilcoxon
p-value<2.2e−16, Cliff’s Delta=−0 .1724).
Finding 1 . Test files are almost 2 times less likely to be
discussed during code review when reviewed together with
production files. Yet, the difference is small in terms of the
number and length of the comments, and the number of
reviewers involved.0% 10% 20% 30%0% 10% 20% 30%
Code improvement
Understanding
Social communication
Defect
Knowledge transfer
Misc
Figure 1: The outcomes of comments in code review of test
files,aftermanualanalysisin600comments(CL=99%,CI=5).
RQ2. What do reviewers discuss when reviewing
test code?
InFigure1,wereporttheresultsofourmanualclassificationof600
comments.WhencomparedtothestudybyBacchelli&Bird[ 12]
that classified production and test code together, we exclude the
‘Testing’ category as all our comments were related to test code.
In addition, we did not observe any comments related to ‘Exter-
nalimpact’and‘Reviewtool’.Interestingly,themagnitudeofthe
remaining outcomes is the same as found by Bacchelli & Bird [ 12].
Codeimprovements( 35%).Thisisthemostfrequentlydiscussed
topic by reviewers when inspecting test files. This category in-
cludessuggestionstousebettercodingpractices,fixtypos,write
better Java-docs, and improve code readability. Interestingly, thecode improvements that we found are also similar to the ones
foundbyBacchelli&Bird[ 12].Yetthereviewersmostlydiscussim-
provements focused on testing, as opposed to generic code quality
practices,suchasmaintainability,whicharethefocusofreviews
on production code [12, 14].
More specifically, 40% of the code improvement comments con-
cern improvements to testing practices, such as better mocking
usageand testcohesion. Inaddition,we foundthat in12%ofthe
cases,developerswerediscussingbetternamingfortestclasses,test
methods, and variables. Interestingly, our interviewees mentioned
that naming is important when reviewing test files, as P9 put it:
“MostofthetimeIcomplainofcodestyling.Sometimesit’sdifficult
to understand the name of the variables, so I often complain to
change the names.”
Ofthecodeimprovementcomments,14%areaboutuntestedand
tested paths. According to our interviewees, this is an important
concernwhenreviewingtestfiles.Someexamplesofsuchreview
682
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. When Testing Meets Code Review: Why and How Developers Review Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
discussion comments are “Where is the assert for the non synchro-
nized case?”, “Add a test for context path of /.”, and “I wouldn’t do
this test. That’s an implementation detail.”
Another6%ofthecommentsconcernwrongassertions.Aswe
discuss in the following RQs, developers often complain aboutreadability of the assertions, namely the assertion has to be asspecific as possible to let the developer better understand whythe test failed. As an example, a developer asked to change an
assertTrue(equals()) to anassertEqual() in one comment.
Finally, we observed that 12% of the comments concern unused
or unnecessary code, and 17% of them mention code styling. These
kinds of comments are in line with those found on reviews of
production code [12, 14].
Understanding( 32%).This category represents all the comments
where reviewers ask questions to better understand the code, in-
cluding posing questions asking for explanations, suggestions, and
motivating examples. In this category, we included comments such
as “why do you need this for?”, “what does this variable name
mean?” and “why is this class static?”. Interestingly, as opposed to
review comments related to code improvements, the comments in
this category did not reveal any differences from what we found
in the test and production files analyzed in our previous work,
i.e.,therewerenocommentsthatwerespecificallyrelatedtotesting
practices, suchas assertion and mocking.This provides additional
evidence on the importance of understanding when performing
code reviews [12], regardless of the types of files under review.
Defectfinding ( 9%).Within this category, we see discussion con-
cerningtestdefectssuchaswrongasserthandling,missingtests,
misuseoftestconventions,andincorrectuseofmocks.Weobserve
three different categories of defects: severe, not severe, and wrong
assertions.Morespecifically,43%ofthecommentsareaboutsevere
issues,i.e., tests that completely fail because of a wrong variable
initialization, a wrong file path, orincorrect use of mocks. On the
other hand, 41% of the comments are focused on less severe issues,
suchasmissingtestconfigurations.Finally,14%ofthecomments
concern wrong assertion handling, such as assertions of wrong
scenarios.Interestingly,as opposedtotheresults reportedbyBac-
chelli & Bird who found that “review comments about defects ...
mostly address ‘micro’ level and superficial concerns” [ 12], a large
portion of the defects discussed in test file code reviews concern
ratherhigh-levelandsevereissues.Ahypothesisforthisdifference
may be that a good part of the severe, high-level defects that affect
testfilesarelocalized(i.e.,visiblejustlookingatthechangedlines),
while production files are affected by more delocalized defects that
may be harder to detect by simply inspecting the changed lines.
Knowledge transfer( 4%).Thiscategory,whichalsoemergedin
previous work [ 12], represents all the comments where the review-
ersdirectthecommitterofthecodechangetoanexternalresource
(e.g.,internaldocumentationorwebsites).Weobservetwodifferent
typesofcommentsinthiscategory:commentsthatlinktoexternal
resourcesandthatcontainexamples.Morespecifically,55%ofthese
commentscontainlinkstoexternaldocumentation(e.g.,Mockito
website, python documentation), to other classes of the project
(e.g.,othertests),andtootherpatches.Therestofthecomments
areexampleswherethereviewershowedhowtotackletheissuewith an example, within the review comment itself, of how s/he
would do it.
Socialcommunication ( 11%).Finally,thiscategory,inlinewith
theworkbyBacchelli&Bird[ 12],includesallthecommentsthat
aresocialinnatureandnotaboutthecode,examplessuchas“Great
suggestion!” and “Thank you for your help”.
Finding 2 . Reviewers discuss better testing practices, tested
and untested paths, and assertions. Regarding defects, half
of the comments regard severe, high-level testing issues, as
opposedtoresultsreportedinpreviouswork,wheremostof
thecommentsonproductioncoderegardedlowlevelconcerns.
RQ3. Which practices do reviewers follow for
test files?
We analyze the answers obtained during our interviews with de-
velopers. We refer to individual interviewees using (P #).
Testdriven reviews. Regardlessofhavingtestfilesinthepatch,
allparticipantsagreed thatbeforedivingintothe source code, they
firstgetanideaofwhatthechangeisabout,byreadingthecommit
message or any documentation attached to the review request
[P1,2,6−10 ,12].P2added:“IlookatwhatitsaysandIthinktomyself
‘how would I implement that?’, just to give a sense of what are the
files that I would be expecting to be changed, what are the areas of
the system that I’m expecting that they touch.” If the files that are
attachedtothepatchlookmuchdifferentfromwhattheyexpected,
they immediately reject the code change [P 2,7,9,10].
Once they understood what the change is about, developers
start to look at the code. In this case, we identified two different
approaches:somedeveloperspreferto readtestfilesfirst followed
byproductionfiles[P 2,4,5],whileotherdeveloperspreferto read
production files first and then review tests [P 1,3,6−10].
Whenstartingfromtests,developerssaytheycanunderstand
what the feature should do even before looking atits implementa-
tion[P2,4,5].P5says:“Itissimilartoreadinterfacesbeforeimple-
mentations. I start from the test files because I want to understand
theAPIfirst.”Inthisapproach,developersfirstseewhatthecodeistested for, then check whether the production code does only what
is tested for or if it does more than what is necessary [P 2,4,5]: “If I
starttofindsomethinginproductionthatismuchdifferentofwhat
I am inferring from the tests, those are my first questions” [P 5].
Moredevelopersinsteadstartreviewingthechangeinitspro-
duction files first [P 1,3,6−10]. As P8explained: “I start first from
the production code, because I can have a sense of what should be
tested.”Theadvantageofsuchanapproachisthatdevelopersdo
not waste time validating whether the test covers every path fora change that is wrong in first place [P
1,3,6−10]. P7also said that
he would prefer to start reviewing the tests, but the poor quality of
thetestsmakesthisnotrealistic:“Iwouldprefertostartingwith
the tests, the problem is that tests are usually very bad. And the
reason is because they usually tend to test what they implemented
and not what they should have implemented. In TDD I would also
start to review tests first but, as you know, this is not the case.”
683
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden D. Spadini et al.
Finding 3 . Similarly to when writing new code, when re-
viewingsomedevelopersprefertostartfromtests,othersfrom
production. Reviewers who start inspecting tests use them to
determinewhattheproductioncodeshoulddoandwhether
itdoes only that.Reviewers whostart inspectingproduction
code prefer to understand the logic of production code before
validating whether its tests cover every path.
Reviewerslookfordifferentproblemswhenreviewingtests.
Accordingto the interviewees, reviewing testfiles require different
practicestothoseusedforreviewingproductionfiles;thisisinline
withthedifferenceswefoundinthecontentofthesubcategoriesof
commentsleftfortestfilesvs.productionfilesforRQ2.Interviewees
explain that it is especially different in terms of whatto look for.
P10saidthatwhenreviewingproductioncodetheydiscussmore
aboutthedesignandcleanlinessofthecode,whereasforreviewsof
teststheydiscussmoreontested/untestedpaths,testingpractices
like mocking and the complexity of the testing code.
A main concern for all the developers is understanding if all the
possible paths of the production code are tested, especially corner
cases[P 8−12].“Ifthemethodbeingtestedreceivestwovariables,and
withthesetwovariablesitcanhaveonaveragefivetotendifferent
returns, I’ll try to see whether they cover a sufficient number of
cases so that I can prove that that method won’t fail.”[P 11]
Intervieweesexplainedthattheyoftencomplainaboutmaintain-
ability and readability ofthe test (i.e., complex or duplicatedcode,
if the test can be simpler, or divided into two smaller tests), but
especially the name of the tests and the assertions [P 7−12]. “I often
complainontheassertions,someassertionsarereallydifficultto
understand, we should try to be as specific as possible.”[P 8]
Finding 4 . A main concern of reviewers is understanding
whether the test covers all the paths of the production code
and to ensure tests’ maintainability and readability.
Havingthecontextualinformationaboutthetest. Aswewill
discuss in the next RQ, a main concern for developers is the small
amountofinformationprovidedinthecodereview[P 1,6,8−10 ,12].
Indeed, within a code review, reviewers can only see files that
are changed and only the lines that have been modified, while
interviewees complain that they do not have the possibility to, for
example, automatically switch between production code and its
test code, or to check other related test cases that are not modified
in the patch [P 1,8,9].
For this reason, two developers explained that they check out
thecodeunderreviewandopenitwithanothertool,forexamplealocalIDE[P
9,12].Inthisway,theycanhavethefullpictureofwhat
ischanged andget fullsupport ofother tools: “Iparticularly like
openingthepullrequesttoknowwhathascomeinagain,which
classeshavebeenedited.I[open]GitHub,IaccessthePRandopenit[in]myIDE.SoIcanlookatthecodeasawhole,notjustthefiles
changed as it is in GitHub."[P 12] Another advantage of checking
outthecommitwiththepatchisthatdeveloperscanseethetests
running: “Most of the time I pull the pull request to see the feature
and tests running, so I can have a sense of what have changedandhow.”[P 9]Nevertheless,thispracticecanonlybeaccomplished
whenthecodebaseislimitedinsize andthecodingenvironment
can be easily pulled to the local machine of the reviewers.
Finding 5 . Due to the lack on test-specific information
withinthecodereviewtool,weobservedthatdeveloperscheck
out the code under review and open it in a local IDE: This
allows them to navigate through the dependencies, have a
fullpictureof thecode,andrunthetest code.Howeverthis
workaround is limited to small scale code bases.
RQ4. What problems and challenges do
developers face when reviewing tests?
Testcodeissubstantiallydifferentthanproductioncode. Ac-
cording to our interviewees, even if sometimes writing tests is sim-
plerthanwritingproductioncode[P 1,3,4,6−9 ,11],thischangeswhen
reviewing tests [P 3,4,6−9]: “Imagine you have to test a method that
doesanarithmeticsum.Theproductioncodeonlyaddstwonum-
bers,whilethetestwillhavetodoapositivetest,anegativeand
haveaboutfifteendifferentalternatives,forthesamefunction.”[P 11]
Accordingtoourinterviewees,reviewingtestfilesbecomescom-
plicatedduetolackofcontext[P 1,6,8−10 ,12].Whenreviewingatest,
code review tools do not allow developers to have production and
test files side-by-side [P 12]: “Having the complete context of the
test is difficult, we often use variables that are not initialized in
the reviewed method, so often I have to go around and understand
wherethevariableisinitialized.”[P 8]Anotherdevelopersaidthat
“It’shardtounderstandwhichtestisactuallytestingthismethod,
or this path in the function.”[P 1] and that when the test involves a
lot of other classes (it is highly coupled) s/he never knows whether
and how the other classes are tested [P 9].
Furthermore, one of the main challenges experienced by our
intervieweesisthatoftenreviewingatestmeansreviewingcode
additions, which is more complicated than reviewing just code
changes [P 2,11,12]. “Code changes make me think, why is this line
changingfromthegreaterthansidetoalessthanside?Whilecode
additions you have to think what’s going on in there, and tests are
almost all the time new additions.”[P 2] According to developers,
testcodeistheoreticallywrittenonceandifitiswrittencorrectlyit
will not change [P 2]. The reason is that while the implementation
ofthefeaturemaychange(e.g.,howitisimplemented),boththe
result and the tests will stay the same [P 11].
Finding 6 . Reviewingtestfilesrequiresdeveloperstohave
context about not only the test, but also the production file
under test.In addition,test filesare oftenlong andare oftennew additions, which makes the review harder to do.
Theaveragedeveloperbelievestestcodeislessimportant.
“I
willgetacallfrommymanagerifthereisabuginproductionwhile
I’m not going to get a call if there’s a bug in the test right?”[P 2]
Accordingtoourinterviewees,developerschoosesavingtimetothe
detrimentofquality.Thisisduetothefactthatthereisno immediate
value on having software well tested; as P 7explained “it is only
684
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. When Testing Meets Code Review: Why and How Developers Review Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
good for the long term.” For a product that is customer-driven, it is
moreimportanttoreleasethefeatureontimewithoutbugs,because
that is the code that will run on the client’s machine [P 2,4−7 ,9]. P7
said:“IfIwanttogetagoodbonusbytheendoftheyearIwillmake
sure that my features make it into production level code. If instead
we would start to get punished for bugs or bad code practices,
you will see a very very different approach, you would see way
morediscussionsaboutteststhanproductioncode.”Interviewees
affirmedthatthemainproblemisthedevelopersmindset[P 1,2,7,8]:
“It is the same reason as why people write bad tests, testing is
considered as secondary class task, it is considered not but it is
not.”[P7] Developers see test files as less important, because a bug
in a test is a developer’s problem while a bug in production code is
aclient’sproblem.AsexplainedbyP 7,developersarenotrewarded
for writing good code, but for delivering features the clients want.
Furthermore, according to our interviewees, during a review
sometimes they donot even lookat the testfiles, their presence is
enough [P 1,3−6]. As P6said “Sometimes you don’t look at the test
becauseyouseethereisthefile,youknowthatthecodeisdoing
what it has to do and you trust the developer who wrote it (maybe
we trust too much sometimes).”
Finding 7 . Developers have a limited amount of time to
spendonreviewingandaredrivenbymanagementpoliciesto
reviewproductioncodeinsteadoftestcodewhichisconsidered
less important.
Better education on software testing and reviewing. Most in-
tervieweesagreedontheneedtoconvincedevelopersandmanagers
that reviewing and testing are highly important for the software
systemoverallquality[P 1,2,4,6−8 ,10].Educatingdevelopersongood
and bad practices and the dangers of bad testing [P 2,7,10]. “I would
love tosee inuniversity peopleteaching goodpractice ontesting.
Furthermore,peoplecomingfromuniversitytheyhavenofreak-ing clue on how a code review is done. Educating on testing and
reviewing, how to write a good test and review it.”[P 7]
Furthermore,withthehelpofresearchers,developerscouldsolve
partoftheeducationproblem:oneintervieweesaidthatresearch
should focus more on developers’ needs, so that tool designerscan take advantage of these needs and improve their tools [P
6].
“I think it is important to give this feedback to the people who
write [code review] tools so they can provide the features that
the community wants. Having someone like you in the middle,
collectingthisfeedbackandsendingthemtotooldevelopers,this
could be very helpful.”[P 6]
Finding 8 . Novicedevelopersandmanagersarenotawareof
what is the impact of poor testing and reviewing on software
quality,educationsystemsshouldfixthis.Moreover,research
shouldfocusmoreondevelopers’needsandexposethemto
tool makers to have an impact.
Tool improvements. Partoftheinterviewwasfocusedonwhat
can be improved in the current code review tools.According to our interviewees, the navigation between the pro-
ductionandthetestfileswithinthereviewisdifficult[P 2,9,10,12].
“We don’t have a tool to easily switch between your tests and your
productioncode,wehavetogobackandforth,thenyouhavetolook
forthesamenameandtryingtomatchthem.”[P 2]Asmentioned
before,thecontextofthereviewislimitedtothefilesattachedto
the review itself, and this makes it difficult to have a big picture
of the change. For example, test files are usually highly coupled to
severalproductionclasses:however,developerscannotnavigate
to the dependencies of the test, or other test in general, without
openinganewwindow[P 2,9].“Ifwecouldclickonthedefinition
ofaclassandgotoitsimplementationwouldbeamazing.That’s
why Ipullthe PRevery-time andI losea lotof timedoing it.”[P9]
P12‘said:“It’sveryunproductivetoreviewinGitHub,becauseyou
firstvisualizeallthecodes,andthenattheendareallthetests,and
it ends up being more difficult having to keep going in the browser
several times.”
Inaddition,addingfine-grainedinformationaboutcodecoverage
during the review is considered helpful [P 1,7,11,12]. More specifi-
cally,whichtestscoveraspecificline[P 1,7,11,12],whatpathsare
alreadycoveredbythetestsuite[P 2,12],andwhethertestsexercise
exceptional cases [P 12]. Regarding the latter, P 12says: “I think it’s
harder to automate, but it is to ensure that not only the "happy"
pathsarecovered.Itistoensurethatthecoverisinthehappycase,
in case of errors and possible variations. A lot of people end up
covering very little or too much.”
Tool features that are not related to test also emerged during
our interviewees. For example, enabling developers to identify the
importance of each file within the code review [P 7,8,11,12] and
splitting the code review among different reviewers [P 7].
Finding 9 . Review tools should provide better navigation
between test and production files, as well as in-depth code
coverage information.
6 DISCUSSION
Wediscusshowourresultsleadtorecommendationsforpractition-
ers and educators, as well as implications for future research.
6.1 For Practitioners and Educators
Underlinetheimportanceofreviewingtestcode. The results
of both our quantitative and qualitative analysis indicate that most
reviewersdeemreviewingtestsaslessimportantthanreviewingproductioncode.Especiallywheninspectingproductionandtest
files that are bundled together, reviewers tend to focus more on
productioncodewiththeriskofmissingbugsinthetests.However,
previousresearchhasshownthatbugsintestfilescanlowerthequality of the corresponding production code, because a bug in
thetestcanleadtoseriousissuessuchas‘silenthorrors’or‘false
alarms’[46].Moreover,ouranalysisprovidedempiricalevidence
that being a test does not change the chances of a file to have
futuredefects(Section3).Forthisreason,practitionersshouldbe
instructed and keep in mind to put the same care when reviewing
test or production code.
685
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden D. Spadini et al.
Set aside sufficient time for reviewingtest files. Ourintervie-
weesagreedthatreviewingtestfilesisanon-trivialtask,because
changesinvolvingtests aremoreoftencode additions ratherthan
codemodifications and several test options must be analyzed. This
indicates that correctly reviewing test files would hardly take less
time than reviewing production code. A good team culture must
be developed, in which the time spent on reviewing test code is
consideredasimportantasthetimespentonreviewingproduction
codeandscheduledaccordingly.Infact,aspreviousworkalready
pointed out [ 12], good reviewing effectiveness is found mostly
withinteamsthatvaluethetimespentoncodereview;testsshould
not be treated differently.Educate developers on how to review test code.
Many books
andarticleshavebeenwrittenbypractitionersonbestpracticesfor
codereview[ 1,2,5]andresearcherscontinuetoconductstudies
to increase our empirical understanding of code review [ 30,42].
Nevertheless, bestpractices for reviewingtest code havenot been
discussednorproposed yet.Ourwork,as afirststep, collects cur-
rent bestpractices for reviewingof tests. Thesepractices show us
that developers should learn how to look for possible false alarms,to check that the tests will be easily understandable and maintain-
able, and to check whether all the possible paths of the production
code are tested. Novice reviewers may also consider the practice of
reviewing test code before production to (1) make sure to give it
enoughtimeand(2)tobetterunderstandthegoalsoftheproduction
code under test, since novices may not know them beforehand.
6.2 For Tool Designers and Researchers
Providingcontexttoaidinreviewingoftests. Thelackofcon-
text when reviewing test code is a concern for many developers.
Specifically,developersarguethatitisimportanttounderstandand
inspecttheclassesthatareundertestaswellaswhichdependencies
aresimulatedbytests(i.e.,mockobjects).However,knowingwhich
classesareexecutedbyatestnormallyrequiresdynamicallyexecut-ing the code during review, which is not always feasible, especially
when large code bases are involved [ 20]. This is an opportunity to
adapt and extend existing research that determines the coverage of
tests using static analysis [ 10]. Moreover, developers would like to
beabletoeasilynavigatethroughthetestandtestedclasses;future
researchstudiescouldinvestigatehowtoimprovefilenavigation
forcodereviewingeneral(anexistingopenresearchproblem[ 13])
but also to better support review of tests in particular.
Providing detailed code coverage information for tests. As
ourresultsshow,oneofthemostimportanttasksduringthereview
ofatestcodeistomakesurethetestcoversallthepossiblepaths
of the productioncode. Although external tools provide code cov-
eragesupportfordevelopers(e.g.,Codecov[ 3]),thisinformation
is usually not “per test method”, i.e., coverage reports focus on the
finalcoverageaftertheexecutionoftheentiretestsuite,andnot
forasingletestmethod.Therefore,newmethodsshouldbedevised
to not only provide general information on code coverage, but also
provide information thatis specific toeach test method. Aneffort
inthisdirection hasbeenpresentedby Oosterwaal etal.[33];our
analysis points to the need for further research in this area.
Understanding how to review test code and benefits of test
reviews. Our research highlights some of the current practicesusedbydeveloperswhenreviewingtestfiles,suchastestdriven
review (review tests before production code). Nevertheless, therealeffectofthesepracticesoncoderevieweffectivenessandonthe eventual test code quality is not known: some practices may
be beneficial, other may simply waste reviewers’ time. This calls
forin-depth,empiricalexperimentstodeterminewhichpractices
should be suggested for adoption by practitioners.
7 CONCLUSIONS
Automatedtestingisnowadaysconsideredtobeanessentialpro-
cess for improving the quality of software systems. Unfortunately,
past literature showed that test code, similarly to production code,
canoftenbeoflowqualityandmaybepronetocontaindefects[ 46].
Tomaintaina highcodequalitystandard, manysoftwareprojects
employcodereview,butistestcodetypicallyreviewedandifso,
how rigorously? In this paper we investigated whether and how
developers employ code review for test files. To that end, we stud-
iedthreeOSSprojects,analyzingmorethan300,000reviewsand
interviewing three of their developers. In addition, we interviewed
another 9 developers, both from OSS projects and industry, obtain-
ingmoreinsightsonhowcodereviewisconductedontests.Our
results provide new insights on what developers look for when re-
viewingtests,whatpracticestheyfollow,andthespecificchallenges
they face.
Inparticular,afterhavingverifiedthatacodefilethatisatest
does not make it less likely to have defects—thus little justification
forlowerqualityreviews—weshowthatdeveloperstendtodiscusstest files significantly less than production files. The main reportedcause is that reviewers see testing as a secondary task and they arenotawareoftheriskofpoortestingorbadreviewing.Wediscoveredthatwheninspectingtestfiles,reviewersoftendiscussbettertesting
practices, tested and untested paths, and assertions. Regarding
defects, often reviewers discuss severe, high-level testing issues, as
opposedtoresultsreportedinpreviouswork[ 12],wheremostof
the comments on production code regarded low level concerns.
Among the various review practices on tests, we found two ap-
proacheswhenareviewinvolvestestandproductioncodetogether:
some developers prefer to start from tests, others from production.
In the first case, developers use tests to determine what the pro-
duction code should do and whether it does only that, on the other
hand when starting from production they want to understand the
logic before validating whether its tests cover every path. As for
challenges, developers’ main problems are: understanding whether
the test covers all the paths of the production code, ensuring main-
tainability and readability of the test code, gaining context for the
test under review, and difficulty reviewing large code additions
involving test code.
Weproviderecommendationsforpractitionersandeducators,
as well as viable directions for impactful tools and future research.
Wehopethattheinsightswehavediscoveredwillleadtoimproved
toolsandvalidatedpracticeswhichinturnmayleadtohighercode
quality overall.
ACKNOWLEDGMENTS
Theauthorswouldliketothankallparticipantsoftheinterviews.A.
BacchelligratefullyacknowledgesthesupportoftheSwissNational
Science Foundation through the SNF Project No. PP00P2_170529.
686
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. When Testing Meets Code Review: Why and How Developers Review Tests ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1][n. d.]. Best Practices: Code Reviews. https://msdn.microsoft.com/en-us/library/
bb871031.aspx. ([n. d.]). [Online; accessed 25-August-2017].
[2][n. d.]. Best Practices for Code Reviews. https://smartbear.com/learn/
code-review/best-practices-for-peer-code-review/. ([n.d.]). [Online; accessed
25-August-2017].
[3][n. d.]. Codecov. https://codecov.io. ([n. d.]). [Online; accessed 25-August-2017].
[4][n. d.]. Gerrit REST APIs. https://gerrit-review.googlesource.com/
Documentation/rest-api.html. ([n. d.]). [Online; accessed 25-August-2017].
[5][n. d.]. Modern Code Review. https://www.slideshare.net/excellaco/
modern-code-review. ([n. d.]). [Online; accessed 25-August-2017].
[6][n.d.].WEKA.http://www.cs.waikato.ac.nz/ml/weka/.([n.d.]). [Online;accessed
25-August-2017].
[7] 2018. Appendix. https://doi.org/10.5281/zenodo.1172419. (2018).
[8]R T Fielding A. Mockus and J D Herbsleb. 2002. Two case studies of open source
softwaredevelopment: ApacheandMozilla. ACMTrans. Softw.Eng.Meth. 11,3
(2002), 309–346.
[9]Megha Aggarwaland Amrita. 2013. PerformanceAnalysis OfDifferent Feature
SelectionMethodsInIntrusionDetection. InternationalJournalOfScientific&
Technology Research 2, 6 (2013), 225–231.
[10]TiagoLAlvesandJoostVisser.2009. Staticestimationoftestcoverage.In Source
CodeAnalysisandManipulation,2009.SCAM’09.NinthIEEEInternationalWorking
Conference on. IEEE, 55–64.
[11]DimitriosAthanasiou,AriadiNugroho,JoostVisser,andAndyZaidman.2014.
TestCodeQualityandItsRelationtoIssueHandlingPerformance. IEEETrans.
Software Eng. 40, 11 (2014), 1100–1125. https://doi.org/10.1109/TSE.2014.2342227
[12]Alberto Bacchelli and Christian Bird. 2013. Expectations, outcomes, and chal-
lenges of modern code review. In Proceedings - International Conference on Soft-
ware Engineering. 712–721. https://doi.org/10.1109/ICSE.2013.6606617
[13]MikeBarnett,ChristianBird,JoãoBrunet,andShuvenduKLahiri.2015. Helpingdevelopershelpthemselves:Automaticdecompositionofcodereviewchangesets.
InProceedings of the 37th International Conference on Software Engineering. IEEE
Press, 134–144.
[14]Moritz Beller, Alberto Bacchelli, Andy Zaidman, and Elmar Juergens. 2014. Mod-
erncodereviewsinopen-sourceprojects:Whichproblemsdotheyfix?.In Pro-
ceedings of the 11th working conference on mining software repositories. ACM,
202–211.
[15]AntoniaBertolino.2007. Softwaretestingresearch:Achievements,challenges,
dreams. In 2007 Future of Software Engineering. IEEE Computer Society, 85–103.
[16]GeorgeCandea,StefanBucur,andCristianZamfir.2010. Automatedsoftware
testingasaservice.In Proceedingsofthe1stACMsymposiumonCloudcomputing.
ACM, 155–160.
[17]J.M. Chambers and T. Hastie. 1992. Statistical Models in S. Wadsworth &
Brooks/Cole Advanced Books & Software.
[18]NiteshV.Chawla,KevinW.Bowyer,LawrenceO.Hall,andW.PhilipKegelmeyer.
2002. SMOTE: Synthetic minority over-sampling technique. Journal of Arti-
ficial Intelligence Research 16 (2002), 321–357. https://doi.org/10.1613/jair.953
arXiv:1106.1813
[19]VickiLCreswellJW,ClarkP,JohnW.J.W.Creswell,V.L.VickiLPlanoClark,Vicki
L.P.PlanoClark,andV.L.VickiLPlanoClark.2007. DesigningandConducting
Mixed Methods Research. 275 pages. https://doi.org/10.1111/j.1753-6405.2007.
00096.x
[20]JacekCzerwonka,RajivDas,NachiappanNagappan,AlexTarvo,andAlexTeterev.
2011.Crane:Failureprediction,changeanalysisandtestprioritizationinpractice–
experiencesfromwindows.In SoftwareTesting,VerificationandValidation(ICST),
2011 IEEE Fourth International Conference on. IEEE, 357–366.
[21]ArieVanDeursen, Leon Moonen, AlexVanDenBergh,and GerardKok.2001.
RefactoringTestCode. Proceedingsofthe2ndInternationalConferenceonExtreme
ProgrammingandFlexibleProcessesXP2001 (2001),92–95. https://doi.org/10.1109/
ICSEA.2007.57
[22]Marco di Biase, Magiel Bruntink, and Alberto Bacchelli. 2016. A Security Per-
spectiveonCodeReview:TheCaseofChromium. 2016IEEE16thInternational
Working Conference on Source Code Analysis and Manipulation (SCAM) (2016),
21–30. https://doi.org/10.1109/SCAM.2016.30
[23]Adrian Furnham. 1986. Response bias, social desirability and dissimulation.
Personality and individual differences 7, 3 (1986), 385–400.
[24] KazukiHamasaki,RaulaGaikovinaKula,NorihiroYoshida,CamargoCruzAna
Erika, Kenji Fujiwara, and Hajimu Iida. 2013. Who does what during a Code
Review ? An extraction of an OSS Peer Review Repository. Proceedings of the
10th Working Conference on Mining Software Repositories (MSR’ 13) (2013), 49–52.
https://doi.org/10.1109/MSR.2013.6624003
[25]BruceHaningtonandBellaMartin.2012. Universalmethodsofdesign:100waysto
research complex problems, develop innovative ideas, and design effective solutions.
Rockport Publishers.
[26]TruongHo-Quang,MichelR.V.Chaudron,IngimarSamuelsson,JoelHjaltason,
Bilal Karasneh, and Hafeez Osman. 2014. Automatic Classification of UML ClassDiagramsfromImages. 201421stAsia-PacificSoftwareEngineeringConference
December (2014), 399–406. https://doi.org/10.1109/APSEC.2014.65
[27]Y.Kamei,E.Shihab,B.Adams,A.E.Hassan,A.Mockus,A.Sinha,andN.Ubayashi.
2013. A large-scale empirical study of just-in-time quality assurance. IEEE
Transactions on Software Engineering 39, 6 (June 2013), 757–773. https://doi.org/
10.1109/TSE.2012.70
[28]Sunghun Kim, E. James Whitehead, and Yi Zhang. 2008. Classifying software
changes: Clean or buggy? IEEE Transactions on Software Engineering 34, 2 (2008),
181–196. https://doi.org/10.1109/TSE.2007.70773
[29]AnNgocLam,AnhTuanNguyen,HoanAnhNguyen,andTienN.Nguyen.2015.CombiningDeepLearningwithInformationRetrievaltoLocalizeBuggyFilesfor
Bug Reports (N). In Automated Software Engineering (ASE), 2015 30th IEEE/ACM
International Conference on. 476–481. https://doi.org/10.1109/ASE.2015.73
[30]ShaneMcintosh,YasutakaKamei,BramAdams,andAhmedEHassan.2014. The
Impact of Code Review Coverageand Code Review Participationon SoftwareQuality Categories and Subject Descriptors. Proceedings of the 11th Working
ConferenceonMiningSoftwareRepositories (2014),192–201. https://doi.org/10.
1145/2597073.2597076
[31]Glenford Myers. 2004. The Art of Software Testing, Second edition. Vol. 15. 234
pages. https://doi.org/10.1002/stvr.322 arXiv:arXiv:gr-qc/9809069v1
[32]Helmut Neukirchen and Martin Bisanz. 2007. Utilising Code Smells to De-
tect Quality Problems in TTCN-3 Test Suites. Proceedings of the 19th IFIP
TC6/WG6.1 International Conference, and 7th International Conference on Testing
ofSoftwareandCommunicatingSystems (2007),228–243. https://doi.org/10.1007/
978-3-540-73066-8_16
[33]SebastiaanOosterwaal,ArievanDeursen,RobertaCoelho,AnandAshokSawant,
and Alberto Bacchelli. 2016. Visualizing code and coverage changes for code
review.In Proceedingsofthe201624thACMSIGSOFTInternationalSymposiumon
Foundations of Software Engineering. ACM, 1038–1041.
[34]D.L.ParnasandD.M.Weiss.1987.Activedesignreviews:Principlesandpractices.TheJournalofSystemsandSoftware 7,4(1987),259–265. https://doi.org/10.1016/
0164-1212(87)90025-2
[35] Eric S. Raymond. 1999. The cathedral and the bazaar. First Monday 12, 3 (1999),
23–49. https://doi.org/10.5210/fm.v3i2.578
[36]Peter Rigby, Brendan Cleary, Frederic Painchaud, Margaret-Anne Storey, and
Daniel German. 2012. Contemporary Peer Review in Action: Lessons from Open
SourceDevelopment. IEEESoftware 29,6(nov2012),56–61. https://doi.org/10.
1109/MS.2012.24
[37]Peter C Rigby. 2011. Understanding Open Source Software Peer Review: Review
Processes, Parameters and Statistical Models, and Underlying Behaviours and
Mechanisms. ProQuestDissertationsandTheses (2011),194. http://search.proquest.
com.proxy1.ncu.edu/docview/898609390?accountid=28180
[38]PeterC.Rigby,DanielM.German,andMargaret-AnneStorey.2008. Opensource
software peerreview practices. Proceedings ofthe 13thInternational Conference
on Software Engineering (2008), 541. https://doi.org/10.1145/1368088.1368162
[39] Johnny Saldaña. 2015. The coding manual for qualitative researchers. Sage.
[40]P M Soni, Varghese Paul, and M Sudheep Elayidom. 2016. Effectiveness of
Classifiers for the Credit Data Set : an Analysis. (2016), 78–83.
[41]Davide Spadini, Maurício Aniche, Magiel Bruntink, and Alberto Bacchelli. 2017.
ToMockorNotToMock?AnEmpiricalStudyonMockingPractices. Proceedings
of the 14th International Conference on Mining Software Repositories (2017), 11.
[42]Patanamon Thongtanunam, Shane McIntosh, Ahmed E. Hassan, and HajimuIida. 2017. Review participation in modern code review. Empirical Software
Engineering 22, 2 (Apr 2017), 768–817.
[43]Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Raula Gaikovina Kula,
Norihiro Yoshida, Hajimu Iida, and Ken Ichi Matsumoto. 2015. Who should
review my code? A file location-based code-reviewer recommendation approach
forModernCodeReview. 2015IEEE22ndInternationalConferenceonSoftware
Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings (2015), 141–150.
https://doi.org/10.1109/SANER.2015.7081824
[44] Mario Triola. 2006. Elementary Statistics (10th ed.). Addison-Wesley.
[45]Jason Tsay, Laura Dabbish, and James Herbsleb. 2014. Let’s talk about it: evaluat-
ing contributions through discussion in GitHub. Proceedings of the 22nd ACM
SIGSOFT International Symposium on Foundations of Software Engineering (2014),
144–154. https://doi.org/10.1145/2635868.2635882
[46]Arash Vahabzadeh and Ali Mesbah. 2015. An Empirical Study of Bugs in Test
Code. (2015), 101–110.
[47]Eva Van Emden and Leon Moonen. 2002. Java quality assurance by detectingcode smells. Proceedings - Working Conference on Reverse Engineering, WCRE
2002-Janua (2002), 97–106. https://doi.org/10.1109/WCRE.2002.1173068
[48]R.S. Weiss. 1995. Learning from strangers: The art and method of qualitative
interview studies. Simon and Schuster.
[49]a. Zaidman, B. Van Rompaey, S. Demeyer, and a. Van Deursen. 2008. Mining
SoftwareRepositoriestoStudyCo-EvolutionofProduction&TestCode. 20081st
International Conference on Software Testing, Verification, and Validation 3 (2008),
220–229. https://doi.org/10.1109/ICST.2008.47
687
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:36:21 UTC from IEEE Xplore.  Restrictions apply. 