Automatic Microbenchmark Generation to Prevent Dead
Code Elimination and Constant Folding
Marcelino
Rodriguez-Cancio
University of Rennes 1, France
marcelino.rodriguez-
cancio@irisa.frBenoit Combemale
University of Rennes 1/INRIA,
France
benoit.combemale@irisa.frBenoit Baudry
INRIA, France
benoit.baudry@inria.fr
ABSTRACT
Microbenchmarking evaluates, in isolation, the execution
time of small code segments that play a critical role in large
applications. The accuracy of a microbenchmark depends
on two critical tasks: wrap the code segment into a pay-
load that faithfully recreates the execution conditions of the
large application; build a scaold that runs the payload a
large number of times to get a statistical estimate of the
execution time. While recent frameworks such as the Java
Microbenchmark Harness (JMH) address the scaold chal-
lenge, developers have very limited support to build a correct
payload.
This work focuses on the automatic generation of pay-
loads, starting from a code segment selected in a large ap-
plication. Our generative technique prevents two of the most
common mistakes made in microbenchmarks: dead code
elimination and constant folding. A microbenchmark is such
a small program that can be \over-optimized" by the JIT
and result in distorted time measures, if not designed care-
fully. Our technique automatically extracts the segment into
a compilable payload and generates additional code to pre-
vent the risks of \over-optimization". The whole approach
is embedded in a tool called AutoJMH , which generates
payloads for JMH scaolds.
We validate the capabilities AutoJMH , showing that the
tool is able to process a large percentage of segments in real
programs. We also show that AutoJMH can match the
quality of payloads handwritten by performance experts and
outperform those written by professional Java developers
without experience in microbenchmarking.
CCS Concepts
Software and its engineering !Software perfor-
mance;Keywords
Performace evaluation; microbencharking; text tagging
1. INTRODUCTION
Microbenchmarks allow for the nest grain performance
testing (e.g., test the performance of a single loop). This
kind of test has been consistently used by developers in
highly dependable areas such as operating systems [30, 19],
virtual machines [9], data structures [32], databases [23],
and more recently in computer graphics [25] and high per-
formance computing[29]. However, the development of mi-
crobenchmarks is still very much a craft that only a few
experts master [9]. In particular, the lack of tool support
prevents a wider adoption of microbenchmarking.
Microbenchmarking consists in identifying a code segment
that is critical for performance, a.k.a segment under anal-
ysis (SUA in this paper), wrapping this segment in an in-
dependent program (the payload ) and having the segment
executed a large number of times by the scaold in order to
evaluate its execution time. The amount of technical knowl-
edge needed to design both the scaold and the payload hin-
der engineers from eectively exploiting microbenchmarks
[2, 3, 9]. Recent frameworks such as JMH [2, 20, 18] address
the generation of the scaold . Yet, the construction of the
payload is still an extremely challenging craft.
Engineers who design microbenchmark payloads very com-
monly make two mistakes: they forget to design the payload
in a way that prevents the JIT from performing dead code
elimination [9, 20, 7, 3] and Constant Folds/Propagations
(CF/CP) [1, 20]. Consequently, the payload runs under dif-
ferent optimizations than the original segment and the time
measured does not reect the time the SUA will take in the
larger application. For example, Click [9] found dead code
in the CaeineMark and SciMark benchmarks, resulting
in innite speed up of the test. Ponge also described [21]
how the design of a popular set of microbenchmarks that
compare JSON engines1was prone to \over-optimization"
through dead code elimination and CF/CP. In addition to
these common mistakes, there are other pitfalls for payload
design, such as choosing irrelevant initialization values or
reaching an unrealistic steady state.
In this work, we propose to automatically generate pay-
loads for Java microbenchmarks, starting from a specic seg-
ment inside a Java application. The generated payloads are
guaranteed to be free of dead code and prevent CF/CP.
1https://github.com/bura/json-benchmarks
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970346
132
Our technique statically slices the application to automati-
cally extract the SUA and all its dependencies in a compi-
lable payload. Second, we generate additional code to: (i)
prevent the JIT from \over-optimizing" the payload using
dead code elimination (DCE) and constand folding/constant
propagation(CF/CP), (ii) initialize payload's input with rel-
evant values and (iii) keep the payload in steady state. We
run a novel transformation, called sink maximization , to pre-
vent Dead Code Elimination. We turn some SUA's local
variables into elds in the payload, to mitigate (CF/CP).
Finally, we maintain the payload in stable state by smart
reseting variables to their initial value.
We have implemented the whole approach in a tool called
AutoJMH . Starting from code segment identied with a
specic annotation, it automatically generates a payload
for the Java Microbenchmark Harness (JMH). JMH is the
de-facto standard for microbenchmarking. It addresses the
common pitfalls when building scaolds such as Loop Hoist-
ing and Strength Reduction, optimizations that can make
the JIT reduce the number of times the payload is executed.
We run AutoJMH on the 6 028 loops present in 5 mature
Java projects, to assess its ability to generate payloads out
of large real-world programs. Our technique extracts 4705
SUA into microbenchmarks (74% of all loops) and correctly
generates complete payloads for 3462 (60% of the loops).
We also evaluate the quality of the generated microbench-
marks. We use AutoJMH to re-generate 23 microbench-
marks handwritten by performance experts. Automatically
generated microbenchmarks measure the same times as the
microbenchmarks written by the JMH experts. Finally, we
ask 6 professional Java engineers to build microbenchmarks.
All these benchmarks result in distorted measurements due
to naive decisions when designing benchmarks, while Auto-
JMH prevents all these mistakes by construction.
To sum up, the contributions of the paper are:
A static analysis to automatically extract a code seg-
ment and all its dependencies
Code generation strategies that prevent articial run-
time optimizations when running the microbenchmark
An empirical evaluation of the generated microbench-
marks
A publicly available tool and dataset to replicate all
our experiments2
In section 2 we discuss and illustrate the challenges for
microbenchmark design, which motivate our contribution.
In section 3 we introduce our technical contribution for the
automatic generation of microbenchmarks in Java. In sec-
tion 4 we present a qualitative and quantitative evaluation
of our tool and discuss the results. Section 5 outlines the
related work and section 6 concludes.
2. PAYLOAD CHALLENGES
In this section, we elaborate on some of the challenges
that software engineers face when designing payloads. These
challenges form the core motivation for our work. In this
work we use the Java Microbenchmark Harness (JMH) as
to generate scaolds. This allows us to focus on payload
generation and to reuse existing eorts from the community
in order to build an ecient scaold.
2https://github.com/autojmh2.1 Dead Code Elimination
Dead Code Elimination (DCE) is one of the most common
optimizations engineers fail to detect in their microbench-
marks [9, 21, 7, 20]. During the design of microbenchmarks,
engineers extract the segment they want to test, but usually
leave out the code consuming the segment's computations
(the sink), allowing the JIT to apply DCE. It is not always
easy to detect dead code and it has been found in popu-
lar benchmarks [9, 21]. For example, listing 1 displays a
microbenchmark where the call to Math.log is dead code,
while the call to m.put is not. The reason is that m.put
modies a public eld, but the results of the Math.log are
not consumed afterwards. Consequently the JIT will apply
DCE when running the microbenchmark, which will distort
the time measured.
Map < String , Double > m = MapUtils . buildRandomMap ();
@Benchmark
public void hiddenDCE () {
Math .log(m. put ("Ten", 10));
}
Listing 1: An example of dead code
A key feature of the technique we propose in this work is
to automatically analyze the mircrobenchmark in order to
generate code that will prevent the JIT from running DCE
on this kind of benchmark.
2.2 Constant Folding / Constant Propagation
In a microbenchmarks more variables has to be explic-
itly initialized than in the program. A quick, naive solu-
tion is to initialize these elds using constants, allowing the
compiler to use Constant Folding and Constant Propaga-
tion (CF/CP) to remove computations that can be inferred.
While mostly considered prejudicial for measurements, in
some punctual cases a clever engineer may want to actu-
ally pass a constant to a method in a microbenchmark to
see if CF/CP kicks in, since it is good for performance that
a method can be constant folded. However, when not ex-
pected the optimizations causes microbenchmarks to return
deceitfully good performance times.
Good examples of both DCE and CF/CP optimizations
and their impact on the measurements can be found in liter-
ature [20]. Concrete evidence can also be found in the JMH
examples repository3.
2.3 Non-representative Data
Another source of errors when designing payloads is to
run a microbenchmark with data not representing the actual
conditions in which the system being measured works.
For example, suppose a maintenance being done over an
old Java project and that dierent sort methods are being
compared to improve performance, one of them being the
Collections.sort method. Suppose that the system con-
sistently uses Vector<T> but the engineer fails to see this
and uses LinkedLis<T> in the benchmarks, concluding that
Collections.sort is faster when given as input an already
sorted list. However, as the system uses Vector lists, the
actual case in production is the opposite: sorted lists will
result in longer execution times, as shown in table 1, mak-
ing the conclusions drawn from the benchmark useless.
3http://hg.openjdk.java.net/code-tools/jmh/le/tip/
jmh-samples/src/main/java/org/openjdk/jmh/samples/
133Table 1: Execution times of Collections.sort
Using a sorted list Using an unsorted list
LinkedList 203 ns 453 ns
Vector 1639 ns 645 ns
2.4 Reaching Wrong Stable State
The microbenchmark scaold executes the payload many
times, warming up the code until it reaches a stable state
and is not optimized anymore. A usual pitfall is to build
microbenchmarks that reach stable state in conditions un-
expected by the engineer. For example, if we were to observe
the execution time of the Collection.sort while sorting a list,
one could build the following wrong microbenchmark:
LinkedList <Double > m = ListUtils . buildRandomList ();
@Benchmark
public void doSort () {
Collections . sort (m); }
Listing 2: Sorting a sorted list in each run
Unfortunately, after the rst execution the list gets sorted.
In consecutive executions, the list is already sorted and con-
sequently, we end up measuring the performance of sorting
an already sorted list, which is not the situation we initially
wanted to measure.
3. AUTOJMH
AutoJMH automatically extracts a code segment and
generates a complete payload with inputs that reect the be-
havior of the segment in the original application. The gener-
ation process not only wraps the segment in an independent
program, it also mitigates the risks of unexpected DCE and
CF/CP optimizations and ensures that it will reach stable
state in the same state executed by the SUA during the unit
tests.
Test suiteApplicationinstrument and executePayload0. check snippet1. extract segment and its dependencies2. sink maximization 3. CF / CP prevention4. smart reset5. generate test cases6. generate initialization valuesreg. test cases
Figure 1: Global process of AutoJMH for payload
generation.
Figure 1 illustrates the dierent steps of this process. If
the SUA satises a set of preconditions (detailed in sec-
tion 3.1), AutoJMH extracts the segment into a wrap-
per method. Then, the payload is rened to prevent dead
code elimination, constant folding and constant propagation
(steps 2, 3), as well as unintended stable state (step 4) when
the payload is executed many times. The last steps consistin running the test suite on the original program to produce
two additional elements: a set of data inputs to initialize
variables in the payload; a set of regression tests that ensure
that the segment has the same functional behavior in the
payload and in the original application.
In the rest of this section we go into the details of each
step. We illustrate the process through the creation of a
microbenchmark for the return statement inside the Enu-
meratedDistribution::value() method of Apache Com-
mon Math , shown in listing 3. The listing also illustrates
that a user identies a SUA by placing the Javadoc-like com-
ment @bench-this on top of it. This comment is specic to
AutoJMH and can be put on top of every statement. The
resulting payload is shown listing 4.
double value ( double x,double ... param ) throws
DimensionMismatchException , NullArgumentException {
validateParameters ( param );
/** @bench - this */
return Sigmoid . value (x, param [0] , param [1]) ;
}
Listing 3: An illustrating example: a SUA in
commons.math
In listing 4 we can see that AutoJMH has wrapped the
return statement into a method annotated with @Bench-
mark. This annotation is used to indicate the wrapper method
that is going to be executed many times by the JMH scaf-
fold. The private static method Sigmoid.value has been ex-
tracted also into the payload, since it is needed by the SUA.
AutoJMH has turned variables xand params into elds
and provides initialization code from them, loading values
from a le, which is part of our strategy to avoid CF/CP.
Finally, AutoJMH ensures that some value is returned in
the wrapper method to avoid DCE.
class MyBenchmark {
double [] params ; double x;
@Setup
void setup () {
Loader l = new Loader ("/ data / Sigmoid_160 .dat");
x = l. loaddouble ();
params = l. loaddoubleArray1 ();
}
double Sigmoid_value (
double x,double lo , double hi) {
return lo + (hi - lo) / (1 + FastMath .exp (-x));
}
@Benchmark
public double payloadWrapper () {
return Sigmoid_value (x, params [0] , params [1]) }
}
Listing 4: An illustrating example: the payload
generated by AutoJMH
3.1 Preconditions
The segment extraction is based on a static analysis and
focuses on SUAs that meet the following conditions. These
preconditions ensure that the payload can reproduce the
same conditions than those in which the SUA is executed
in the original program.
1. Initialized variables used by the SUA are of the follow-
ing types: primitive ( int, double, boolean ), their
class counterparts ( Integer, Double, Boolean ), String,
types implementing the Serializable interface, or, col-
lections and arrays of all the above. Non-initialized
variables used by the SUA can be of any public type.
134This condition ensures that AutoJMH can store the
values of all variables used by the SUA
2. None of the methods invoked inside the SUA can have
a target not supported in item 1. This ensures that
AutoJMH is able to extract all methods used by the
SUA.
3. All private or protected methods used by the SUA can
be resolved statically. Dynamically resolved methods
have a dierent performance behavior than statically
resolved ones [4]. Using dynamic slicing we could make
available to the microbenchmark a non-public dynamic
method, but we would distort its performance behav-
ior.
4. The call graph of all methods used by the SUA can-
not be more than a user-dened number of levels deep
before reaching a point in which all used methods are
public. This sets a stopping criterion for the explo-
ration of the call graph.
3.2 SUA extraction
AutoJMH starts by extracting the segment under analy-
sis (SUA) to create a compilable payload using a custom for-
ward slicing method over the Abstract Syntax Tree (AST) of
the large application, which includes the source code of the
SUA. The segment's location is marked with the @bench-
this Javadoc-like comment, introduced by AutoJMH to
select the segments to be benchmarked. If the SUA satis-
es the preconditions, AutoJMH statically slices the source
code of the SUA and its dependencies (methods, variables
and constants) from the original application into the pay-
load. Non-public eld declarations and method bodies used
by the SUA are copied to the payload, their modiers (static,
nal, volatile) are preserved.
Some transformations may be needed in order to achieve
a compilable payload. Non-public methods copied into the
payload are modied to receive their original target in the
SUA as the rst parameter (e.g., data.doSomething() be-
comes doSomething(data) ). Variable and method may be
renamed to avoid name collision and to avoid serializing
complex objects. For example, if a segment uses both vari-
able data and a eld myObject.data ,AutoJMH declares
two public elds: data and myObject_data . When method
renaming is required, AutoJMH uses the fully qualied
name.
At the end of the extraction phase, AutoJMH has sliced
the SUA code into the payload's wrapper method. This
relieves the developer from a very mechanical task and its
automation reduces the risks of errors when copying and
renaming pieces of code. Yet, the produced payload still
needs to be rened in order to prevent the JIT from \over-
optimizing" this small program.
Preserving the original performance conditions.
We aim at generating a payload that recreates the ex-
ecution conditions of the SUA in the original application.
Hence, we are conservative in our preconditions before slic-
ing. We also performed extensive testing to be sure that
the code modications explained above do not distort the
original performance of the SUA. These tests are publicly
available4. Then, all the additional code generated by Au-
toJMH to avoid DCE, initialize values, mitigate CF/CP
4https://github.com/autojmh/syntmodand keep stable state, is inserted before or after the wrapped
SUA.
3.3 Preventing DCE with Sink Maximization
During the extraction of the SUA, we may leave out the
code consuming its computations (the sink), giving the JIT
an opportunity for dead code elimination (DCE), which would
distort the time measurement. AutoJMH handles this po-
tential problem featuring a novel transformation that we
callSink maximization . The transformation appends code
to the payload, which consumes the computations. This is
done to maximize the number of computations consumed
while minimizing the performance impact in the resulting
payload.
There are three possible strategies to consume the results
inside the payload:
Make the payload wrapper method return a re-
sult. This is a safe and time ecient way of preventing
DCE, but not always applicable (e.g., when the SUA
returns void).
Store the result in a public eld . This is a time
ecient way of consuming a value, yet less safe than
the previous solution. For example, two consecutive
writes to the same eld can make the rst write to be
marked as dead code. It can also happen that the pay-
load will read from the public eld with a new value,
modifying its state.
JMH Black hole methods . This is the safest so-
lution, which does not modify the microbenchmark's
state. Black holes (BH) are methods provided by JMH
to make the JIT believe their parameters are used,
therefore preventing DCE. Yet, black holes have a small
impact on performance.
A naive solution is to consume all local variables live at
the end of the method with BHs. Yet, the accumulation of
BH method calls can be a considerable overhead when the
execution time of the payload is small. Therefore, we rst
use the return statement at the end of the method, taking
into consideration that values stored in elds are already
sinked and therefore do not need to be consumed. Then,
we look for the minimal set of variables covering the whole
sink of the payload to minimize the number of BH methods
needed.
Sink maximization performs the following steps to gener-
ate the sink code:
1. Determine if it is possible to use a return statement.
2. Determine the minimal set of variables Vmincovering
the sink of the SUA.
3. When the use of return is possible, consume one vari-
able from Vminusing one return and use BHs for the
rest. If no return is possible, use BHs to consume all
local variables in Vmin.
4. If a return is required to satisfy that all branches return
a value and there is no variables left in Vmin, return a
eld.
To determine the minimal set Vmin, the AutoJMH con-
verts the SUA code into static single assignment (SSA) form[34]
and builds a value dependency graph (VDG) [35]. In the
VDG, nodes represent variables and edges represent direct
value dependencies between variables. For example, if the
135value of variable Adirectly depends on B, there is an edge
fromBtoA. An edge going from one variable node to a phi
node merging two values of the same variable is a back-edge .
In this graph, sink-nodes are nodes without ingoing edges.
Initially, we put all nodes of the VDG in Vmin, except
those representing elds values. Then, we remove all vari-
ables that can be reached from sink-nodes fromVmin. After
doing this, if there are still variables in Vminother than the
ones represented by sink-nodes , we remove the back-edges
and repeat the process.
int d = 0; a = b + c;
i f( a > 0 ) {
d = a + h;
a = 0;
}
b = a;
Listing 5: A few lines of code to exemplify Sink
maximization
To exemplify the process of nding Vminwithin Sink Max-
imization let us consider listing 5. The resulting VDG graph
is represented in gure 2. Sink nodes are nodes dand b1,
which are represented as rounded nodes. The links go from
variables to their dependencies. For example, ddepends on
a0andh. Since it is not possible to arrive to all nodes from
a single sink dorb1, in the example Vmin=fd; b1g. Con-
sequently both dand bmust be consumed in the payload.
Figure 2: VDG of listing 5
3.4 CF/CP Mitigation
Since all SUA are part of a larger method, they most often
use variables dened upfront in the method. These variables
must be declared in the payload. Yet, naively declaring these
variables might let the JIT infer the value of the variables
at compile time and use constant folding to replace the vari-
ables with a constant. Meanwhile, if this was possible in
the original system, it should also be possible in the pay-
load. The challenge is then to detect when CF/CP must be
avoided and when it must be allowed to declare variables
and elds accordingly.
AutoJMH implements the following rules to declare and
initialize a variable in the payload:
Constants ( static final elds ) are initialized using
the same literal as in the original program.
Fields are declared as elds, keeping their modiers
(static, nal, volatile) and initialized in the @Setup
method of the microbenchmark. Their initial values
are probed through dynamic analysis and logged in a
le for reuse in the payload (cf. section 3.6 for details
about this probing process).
Local variables are declared as elds and initialized
in the same way, except when (a) they are declared
by assigning a constant in the original method and(b) all possible paths from the SUA to the beginning
of the parent method include the variable declaration
(i.e. the variable declaration dominates [34] the SUA)
, in which case their original declaration is copied into
the payload wrapper method. We determine whether
the declaration of the variable dominates the SUA by
analyzing the control ow graph of the parent method
of the SUA.
Listing 4 shows how the variables xandparams are turned
into elds and initialized in the @Setup method of the pay-
load. The @Setup method is executed before all the execu-
tions of the wrapper method and its computation time is
not measured by the scaold.
3.5 Keep Stable State with Smart Reset
In Section 2 we discussed the risk for the payload to reach
an unintended stable state. This happens when the pay-
load modies the data over which it operates. For example,
listing 6 shows that variable sumis auto-incremented. Even-
tually, sumwill be always bigger than randomValue and the
payload will stop to execute the return statement.
public T sample () {
final double randomValue = random . nextDouble ();
double sum = 0;
/** @bench - this */
for (int i = 0; i < probabilities . length ; i++) {
sum += probabilities [i];
i f( randomValue < sum ) return singletons .get(i);
}
return singletons .get( singletons . size () - 1);
}
Listing 6: Variable sumneeds to be reset to stay in
the same state
AutoJMH assumes that the computation performed in
the rst execution of the payload is the intended one. Hence,
it automatically generates code that resets the data to this
initial state for each run of the SUA. Yet, we implement
this feature of AutoJMH carefully to bring the reset code
overhead to a minimum. In particular, we reset only the
variables inuencing the control ow of the payload. In list-
ing 7 AutoJMH determined that summust be reset, and it
generates the code to do so.
@Benchmark
public double doBenchmark () {
sum = sum_reset ; // <- SMART RESET HERE !
for (int i = 0; i < probabilities . length ; i++) {
sum += probabilities [i];
i f( randomValue < sum ) return singletons .get(i);
}
return sum;
}
Listing 7: Variable sumis reset by code appended to
the microbenchmark
To determine which variables must be reset, AutoJMH
reuses the VDG built to determine the sinks in the Sink
maximization phase. We run Tarjan's Strongly Connected
Components algorithm to locate cycles in the VDG, and all
variables inside a cycle are considered as potential candi-
dates for reset. In a second step we build a Control Flow
Graph (CFG) and we traverse the VDG, trying to nd paths
from variables found in the branching nodes of the CFG to
those found in the cycles of the VDG. All of the variables
that we succesfully reach are marked for reset.
1363.6 Retrieving Inputs for the Payload
The last part of the microbenchmark generation process
consists in retrieving input values observed in the original
application's execution (steps 5 and 6 of gure 1). To re-
trieve these values, we instrument the original program to
log the variables just before and after the SUA. Then, we
run once the test cases that cover the SUA in order to get
actual values. The user may also congure the tool to use
any program executing the SUA.
To make the collected values available to the payload,
AutoJMH generates a specic JMH method marked with
the@Setup annotation (which executes only once before the
measurements), containing all the initialization code for the
extracted variables. Listing 4 shows an example where the
initial values of variables xand params are read from le.
@Test
public void testMicroBench () {
Loader l = new Loader ();
// Get values recorded before execution
l. openStream ("/ data / Sigmoid_160 . dat ");
MyBenchmark m = new MyBenchmark ();
m.x = l. readdouble ();
m. params = l. readdoubleArray1 ();
double mResult = m. payloadWrapper ();
// Check SUA 's output is equal to payload 's output
l. openStream ("/ data / Sigmoid_160_after .dat");
assertEquals (m.x, l. readdouble ());
assertArrDblEquals (m.params , l. readdoubleArray1 ());
assertEquals ( mResult , m. payloadWrapper ());
}
Listing 8: Generated unit test to ensure that the
microbenchmark has the same functional behavior
than the SUA
3.7 Verifying Functional Behavior
To check that the wrapper method has the same func-
tional behavior as the SUA in the original application (i.e.
produces the same output given the same input), Auto-
JMH generates a unit test for each microbenchmark, where
the outputs produced by the microbenchmark are required
to be equal to the output values recorded at the output of
the SUA. These tests serve to ensure that no optimization
applied on the benchmark interferes with the expected func-
tional behavior of the benchmarked code. In the test, the
benchmark method is executed twice to verify that the re-
sults are consistent within two executions of the benchmark
and signal any transient state. Listing 8 shows a unit test
generated for the microbenchmark of listing 4.
4. EVALUATION
We perform a set of experiments on large Java programs
to evaluate the eectiveness of our approach. The purpose
of the evaluation is twofold. First, a quantitative assessment
ofAutoJMH aims at evaluating the scope of our program
analysis, looking at how many situations AutoJMH is able
to handle for automatic microbenchmark generation. Sec-
ond, two qualitative assessments compare the quality of Au-
toJMH 's generated microbenchmarks with those written by
experts and with those built by expert Java developers who
have little experience in microbenchmarking. We investi-
gate these two aspects of AutoJMH through the following
research questions:
RQ1: How many loops can AutoJMH automati-
cally extract from programs into microbenchmarks?In addition to the generation of accurate microbenchmarks,
it is important to have a clear understanding of the reach
ofAutoJMH 's analysis capacities. Remember that Au-
toJMH can only handle those segments that meet certain
preconditions. Therefore, we need to quantify the impact of
these conditions when analyzing real-world code.
RQ2: How does the quality of AutoJMH's gener-
ated microbenchmarks compare with those written
by experts?
Our motivation is to embed expert knowledge into Auto-
JMH , to support Java developers who have little knowledge
about performance evaluation and who want to get accurate
microbenchmark. This research question aims at evaluating
whether our technique can indeed produce microbenchmarks
that are as good as the ones written by an expert.
RQ3: Does AutoJMH generate better microbench-
marks than those written by engineers without ex-
perience in microbenchmarking?
Here we want to understand to what extent AutoJMH
can assist Java developers wanting to use microbenchmarks.
4.1 RQ1: Automatic Segment Extraction
We automatically annotate all the 6 028 loops of 5 real
Java projects with the @bench-this annotation to nd out
to what extent the tool is able to automatically extract loops
and generate corresponding payloads. We focus on the gen-
eration of benchmarks for loops since they are often a per-
formance bottleneck and they stress AutoJMH 's capacities
to deal with transient states, although the only limitations
to the slicing procedure are the ones described in section 3.1.
We selected the following projects for our experiments, be-
cause their authors have a special interest in performance:
Apache Math is the Apache library for mathematics and
statistics; Vectorz is a vector and matrix library, based
around the concept of N-dimentional arrays. Apache Com-
mon Lang provides a set of utility methods to handle Java
core objects; Jsyn is a well known library for the genera-
tion of music software synthesizers. ImageLib2 is the core
library for the popular Java scientic image processing tool
ImageJ. Exact versions of these projects can be found in
AutoJMH 's repository5.
Table 2 sumarizes our ndings, one column for each project
and the last column shows totals. The row \Payloads gener-
ated" shows the number of loops that AutoJMH succesfully
analyzed and extracted in a payload code. The row \Pay-
loads Generated & Initialized" renes the previous number,
indicating those payloads for which AutoJMH was able to
generate code and initialization values (i.e. they were cov-
ered with at least one unit test). The row\Microbenchmarks
generated" further renes the previous numbers, indicating
the amount of loops for which AutoJMH was able to gen-
erate and initialize a payload that behaves functionally the
same as the SUA (i.e. equal inputs produce equal results).
The rows below detail the specic reason why some loops
could not be extracted. We distinguish between \Variables
unsupported" or \Invocations Unsupported". As we can see,
the main reason for rejection are unsupported variables. Fi-
nally, row \Test Failed" shows the number of microbench-
marks that failed to pass the generated regressions tests.
The percentages are overall percentages.
The key result here is that out of the 6 028 loops found in
all 5 projects, AutoJMH correctly analyzed, extracted and
5https://github.com/autojmh/autojmh-validation-data.git
137Table 2: Reach of AutoJMH
PROPERTY MATH % VECT % LANG % JSYN % Img2 % Total %
Total Loops 2851 1498 501 306 926 6082
Payloads generated 2086 73 1377 92 408 81 151 49 683 74 4705 77
Payloads generated & initialized 1856 65 940 63 347 69 88 29 254 27 3485 57
Microbenchmarks generated 1846 65 934 62 345 69 84 29 253 27 3462 57
Rejected: 765 26 121 8 93 19 155 50 243 26 1377 23
* Variables unsupported: 601 21 81 5 53 11 123 40 169 18 1027 17
+ Unsupported type collection 52 2 12 1 2 0,4 18 6 15 2 99 2
+ Type is not public 132 5 2 0,1 8 2 23 7 0 - 165 3
+ Type is not storable 417 15 67 5 43 9 82 27 154 17 763 13
* Invocations unsupported: 164 6 40 3 40 8 32 10 74 8 350 6
+ Target unssuported 150 5 34 3 37 7,39 28 9 74 8 323 5
+ Levels too deep 0 0 0 0 2 0,4 0 0 0 0 2 0.03
+ Private constructor 3 0,1 3 0,2 0 0 3 1 0 0 9 0.1
+ Protected abstract method 11 0,4 3 0,2 1 0,2 1 0,3 0 0 16 0.3
Test failed 10 0,4 6 0,4 2 0,4 4 1,3 1 0,1 23 0.4
wrapped 3 462 loops into valid microbenchmarks. These mi-
crobenchmarks resulted from 3 485 payloads for which Au-
toJMH was able to generate and nd initialization values
and who's regression test did not fail. In total, AutoJMH
generated the code for 4 705 payloads. The tool rejected
1377 loops because they did not meet the preconditions.
Looking into the details, we observe that Vectorz and
Apache Lang contain relatively more loops that satisfy
the preconditions. The main reason for this is that most
types and classes in Vectorz are primitives and serializables,
while Apache Lang extensively uses Strings and collec-
tions. Apache Math also extensively uses primitives. The
worst results are to JSyn : the reason for this seems to be
that the parameters to the synthesizers are objects instead
of numbers, as we initially expected.
The results vary with the quality of the test suite of the
original project. In all the Apache projects, almost all loops
that satisfy the precondition nally turn into a microbench-
mark, while only half of the loops of Vectorz andJSyn that
can be processed by AutoJMH are covered by one test case
at least. Consequently, many payloads cannot be initial-
ized by AutoJMH , because it cannot perform the dynamic
analysis that would provide valid initializations.
outer :
for (int i = 0; i < csLen ; i ++) {
final char ch = cs. charAt (i);
/** @bench - this */
for (int j = 0; j < searchLen ; j++) {
i f( searchChars [j] == ch) {
i f(i < csLast && j < searchLast && Character .
isHighSurrogate (ch)) {
i f( searchChars [j + 1] == cs. charAt (i + 1)) {
continue outer ; }
}else {continue outer ; }}}
Listing 9: The SUA depends on outer code to work
properly
Table 2 also shows that some microbenchmarks fail re-
gression tests. A good example is the inner loop of listing
9, extracted from Apache Common Lang . This loop de-
pends on the chvariable, obtained in its outer loop. In this
case, AutoJMH generates a payload that compiles and can
run, but that does not integrate the outer loop. So the pay-
load's behavior is dierent from the SUA and the regression
tests fails.It is worth mentioning that while AutoJMH failed to
generate the inner loop, it did generate a microbenchmark
for the outer one.
Answer to RQ1: AutoJMH was able to generate
3 485 microbenchmarks out of 6 028 loops found in real-
word Java programs, and only 23% of the analyzed loops
did not satisfy the tool's preconditions.
4.2 RQ2:Microbenchmarks Generated by Au-
toJMH vs Handwritten by Experts
To answer RQ2, we automatically re-generate mircrobench-
marks that were manually designed by expert performance
engineers. We assess the quality of the automatically gener-
ated microbenchmarks by checking that the times they mea-
sure are similar to the times measured by the handwritten
microbenchmarks.
4.2.1 Microbenchmarks Dataset
We re-generate 23 JMH microbenchmarks that were used
to nd 8 documented performance regression bugs in projects
by Oracle6and Sowatec AG [12]. We selected microbench-
marks from Oracle, since this company is in charge of the
development of Hotspot and JMH. The agship product of
Sowatec AG, Arregulo7, has reported great performance re-
sults using microbenchmarks. The microbenchmarks in our
dataset contained several elements of Java such as condi-
tionals, loops, method calls, elds. They where aimed at
variety of purposes and met the AutoJMH preconditions.
Follows a small description of each one of the 23 mi-
crobenchmarks (MB) in our dataset:
MB 1 and 2 : Measure the dierences between the two
methods ArrayList.add andArrayList.addAll when adding
multiple elements.
MB 3 to 5 : Compare dierent strategies of creating ob-
jects using reection, using as baseline the operator new.
MB 6 : Measure the time to retrieve elds using reection.
MB 7 to 9 : Compare strategies to retrieve data from
maps when the key is required to be a lower case string.
6http://bugs.java.com. Bugs ids: 8152910, 8050142,
8151481 and 8146071
7http://www.sowatec.com/en/solutions-services/arregulo/
138MB 10 and 11 : Compare the ConcurrentHashMap.get
method vs.theNonBlockingHashMapLong.get method.
MB 12 to 14 : See whether BigInteger.value can be
constant folded when given as input a number literal.
MB 15 and 16 : Contrasts the performance of Math.max
given two numbers vs.a greater than (a >b) comparison.
MB 17 : Evaluate the performance of the Matcher.reset
method.
MB 18 to 23 : Evaluate the performance of the String.format
method using several types of input ( double ,long,String ).
4.2.2 Statistical Tests
We use the statistical methodology for performance eval-
uation introduced by George [13] to determine the similarity
between the times measured by the automatically generated
microbenchmarks and the handwritten ones. This consists
in nding the condence interval for the series of execution
times of both programs and to check whether they overlap,
in which case there is no statistical reason to say they are dif-
ferent. We run the experiment following the recommended
methodology, considering 30 virtual machine invocations, 10
of which run for microbenchmarks and 10 warm up iterations
to reach steady state. We select a condence level of 0,05.
To determine whether AutoJMH actually dees the pit-
falls shown in section 2, we also generate three other sets of
23 microbenchmarks. Each set of microbenchmark is prone
to the following pitfall: DCE, CF/CP and wrong initial val-
ues. DCE was provoked by turning o sink maximization .
CF/CP was provoked by inverting the rules of variable dec-
laration where constants (static nal elds) are declared as
regular elds and initialized from le; elds are redeclared
as constants (static nal eld) and initialized using literals
(10, "zero", 3.14f ); local variables are always declared as
local variables and initialized using literals. In the third
set, we feed random data as input to observe dierences in
measurements caused by using dierent data. Using these
3 dierent sets of microbenchmarks, we performed the pair-
wise comparison again between them and the handwritten
microbenchmarks.
Table 3: Comparison of generated vs handwritten
benchmarks
# Set Successful tests
1Generated with AutoJMH 23 / 23
2 DCE 0 / 23
3 CF/CP 11 / 23
4 Bad initialization 3 / 23
Table 3 shows the results of this experiment. Column
\Successful tests" shows for how many of the 23 automati-
cally generated microbenchmarks measured the same times
as the ones written by experts. Row 1 shows the set gen-
erated with all features of AutoJMH . Rows 2, 3 and 4 the
ones generated with induced errors.
4.2.3 Analysis of the Results
The key result of this set of experiments is that all the 23
microbenchmarks that we re-generated using all the features
ofAutoJMH measure times that are statistically similar to
those measured by the ones handwritten by experts, while
microbenchmarks with induced errors consistently drift away
from this baseline. For us, this is an strong indication that
AutoJMH actually dees the pitfalls of section 2.Row 2 of table 3 shows the strong impact of DCE on the
accuracy of microbenchmarks: 100% of microbenchmarks
that we generate without sink maximization measure sig-
nicantly dierent times from the times of handwritten mi-
crobenchmarks. The inverted rules for CF/CP take a toll
on 12 microbenchmarks, for example the result of a com-
parison between two constants is also a constant ( MB 15 )
and therefore there is no need to perform the comparison.
Eleven microbenchmarks generated with wrong variable dec-
larations still measure similar times, because some SUA can-
not be constant folded (e.g., the Map.get method in in MB
7). Finally, line 5 shows that passing wrong initial values
produces dierent results, since adding 5 elements to a list
takes less time than adding 20 ( MB 1, 2 ) or converting
PI (3,14159265) to string is certainly slower than an integer
such as 4 ( MB 18 to 23 ). The three cases that measured
correct times occur when the elds initialized in the payload
are not used (as is the case in MB 5 ).
The code for all the microbenchmarks used in this ex-
periment, as well as the program and the unit test used to
rebuild them, can be found in the website of AutoJMH8.
Answer to RQ2: microbenchmarks automatically gen-
erated by AutoJMH systematically perform as good
as benchmarks built by a JMH experts with a con-
dence level of 0.05. The code generated to prevent DCE,
CF/CP and initialize the payload plays a signicant role
in the quality of the generated microbenchmarks.
4.3 RQ3: AutoJMH vs Engineers without Mi-
crobenchmarking Experience
For this research question, we consider 5 code segments,
all contained in a single class and we ask 6 professional Java
developers with little experience in performance evaluation
to build a microbenchmark for each segment. This simulates
the case of software engineers looking to evaluate the per-
formance of their code without specic experience in time
measurement. This is a realistic scenario, as many engi-
neers arrive to microbenchmarking due to an eventual need,
gathering the knowledge they require by themselves using
available resources as Internet tutorials and conferences.
We provided all participants a short tutorial about JMH.
All participants had full access to Internet during the exper-
iment and we individually answered all questions relative to
better microbenchmarking. Participants were also reminded
that code segments may have multiple performance behav-
iors and that otherwise noticed, they should microbench-
mark all behaviors they could nd.
4.3.1 Segments Under Analysis
Each of the 5 code segments is meant to test one dierent
feature of AutoJMH .
SUA 1 in listing 10: participants were requested to
evaluate the execution time of the forloop. Here we eval-
uate a segment which execution time depends on the dif-
ferent input's types. The parameter cofaddFunction is of
type MyFunction , which is inherited by two subclasses, both
overriding the calc method. The calculations performed
by both subclass are dierent, which required several mi-
crobenchmarks to evaluate all possibilities.
8https://github.com/autojmh
139SUA 2 and 3 in listing 11: participants were requested
to evaluate the time it takes to add one element into an array
list, and the time it takes to sort a list of 10 elements. Here
we wanted to test the participant's ability at using dierent
reset strategies to force the microbenchmark reach stable
state measuring the desired case. The payload for SUA 2
must constrain the list size , otherwise the JVM runs out of
memory. For SUA 3 it is necessary to reset the list into an
unordered state.
SUA 4 and 5 in listing 12: participants were requested
to estimate how long takes the expression to execute. The
segments consist of simple mathematical expressions meant
to investigate if participants are able to avoid DCE and con-
stant folding when transplanting a SUA into a payload.
All microbenchmarks used in this experiment are publicly
available in the github repository of AutoJMH
4.3.2 Resulting Microbenchmarks
Figure 3 shows the execution times measured by all mi-
crobenchmarks. The y-axis shows execution times in mil-
liseconds (log scale). On the x-axis we show 6 clusters:
MB1a and MB1b for the two performance behaviors of SUA
1 and MB2 to MB5 for all other segments. Each cluster
includes the time measured by the microbenchmarks de-
signed by the 6 Java developers. In each cluster, we add
two microbenchmarks: one generated by AutoJMH and
one designed manually by us and that has been reviewed
by the main developer of JMH. The latter microbenchmark
(for short: the expert ) is used as the baseline for comparison.
We use the similiraty of execution times for comparison: the
closest to the baseline, the better.
First, we observe that the times for the AutoJMH and
the baseline microbenchmarks are consistently very close to
each other. The main dierences we can see are located in
SUAs 2 and 3. This is because AutoJMH uses a generic
reset strategy consisting in clearing the list and adding the
values, which is robust and performs well in most cases.
However, the expert microbenchmarks and the one made by
Engineer 6 for SUA 3 featured specic reset strategies with
less overhead. The best strategy to reset in SUA 2 is to reset
only after several calls to the addmethod have been made,
distributing the reset overhead and reducing the estimation
error. In the expert benchmark for SUA 3, each element is
set to a constant value. A clever trick was used by engineer 6
in SUA 39: the sort method was called twice with two dif-
ferent comparison functions (with equivalent performance),
changing the correct order in every call. This removes the
need to reset the list, since every consecutive call to sort is
considered unordered.
Second, we observe that Java developers build microbench-
marks that measure times that are very dierent from the
baseline. In order to understand the root cause of these dif-
ferences, we manually review all the microbenchmark. Here
we observe that the participants did encounter the pitfalls
we expected for each kind of segment: 3 participants fail
to distinguish 2 performance behaviors in MB1; 3 partici-
pants made mistakes when initializing MB2 and MB3; we
found multiple issues in MB4 and MB5, where 3 engineers
did not realize that their microbenchmark was optimized by
9https://github.com/autojmh/autojmh-validation-data/
blob/master/eng6/src/main/java/fr/inria/diverse/
autojmh/validation/eng6/TransientStateListSortEng6.java
Figure 3: Execution times comparison between mi-
crobenchmarks generated by AutoJMH those man-
ually built by Java developers and one JMH expert
DCE, Engineer 6 allowed parts of its microbenchmark to be
constant folded and 3 participants bloated to some extend
their microbenchmark with overhead. An interesting fact
was that Engineer 6 was aware of constant folding, since
he asked about it, meaning that a trained eye is needed to
detect optimizations, even when one knows about them.
Answer to RQ3: microbenchmarks generated by Au-
toJMH prevent mistakes commonly made by Java de-
velopers without experience in microbenchmarking.
4.4 Threats to Validity
The rst threat is related to the generalizablity of obser-
vations. Our qualitative evaluation was performed only with
5 segments and 6 participants. Yet, segments were designed
to be as dierent as possible and to cover dierent kinds of
potential pitfalls. The quantitative experiment also allowed
us to test AutoJMH on a realistic code base, representative
of a large number of situations that can be encountered in
Java applications.
AutoJMH is a complex tool chain, which combines code
instrumentation, static and dynamic analysis and code gen-
eration. We did extensive testing of our the whole infrastruc-
ture and used it to generate a large number of microbench-
marks for a signicant number of dierent applications. How-
ever, as for any large scale experimental infrastructure, there
are surely bugs in this software. We hope that they only
change marginal quantitative things, and not the qualita-
tive essence of our ndings. Our infrastructure is publicly
available on Github.
5. RELATED WORK
We are not aware of any other tool that automatically
generates the payload of a microbenchmark. However, there
are works related to many aspect of AutoJMH .
Performance Analysis.
The proper evaluation of performance is the subject of a
large number of papers [13, 24, 2, 18, 9]. They all point
out non-determinism as the main barrier to obtain repeat-
able measurements. Sources of non-determinism arise in the
140addFunction ( MyFunction c) {
i f(c == null ) c = new FunA ();
// SUA #1:
for (int i = 0; i < 100; i ++)
sinSum += c. calc (i);}
Listing 10: SUA 1. Dierents
inputs in 'c' dene performanceappendSorted ( ArrayList < Integer > a,
int value ) {
// SUA #2:
a.add ( value );
// SUA #3:
a. sort ( new Comparator < Integer >() {
compare ( Integer o1 , Integer o2) {
return o1 - o2 ;}}) ;}
Listing 11: Segments 2 and 3// SUA #4
angle += Math .abs ( Math . sin (y)) /
PI;
// SUA #5
double c = x * y;
Listing 12: SUAs 4 and 5
data, the code [20], the compiler[24], the virtual machine[14]
the operating system [24] and even in the hardware[10].
Various tools and techniques aim at minimizing the eect
of non-determinism at each level of abstraction[24, 10, 14].
JMH stands at the frontier between code and the JVM by
carefully studying how code triggers JVM optimizations[1].
AutoJMH is at the top of the stack, automatically generat-
ing code for the JMH payload, avoiding unwanted optimiza-
tions that may skew the measurements.
Microbenchmarking determines with high precision the
execution time of a single point. This is complementary
to other techniques that use proling [31, 5] and trace anal-
ysis [16, 15] that cover larger portions of the program at the
cost of reducing the measurement precision. Symbolic exe-
cution is also used to analyze performance [8, 36] however,
symbolic execution alone cannot provide execution times.
Finally several existing tools are specic for one type of bug
[26, 27] or even for one given class of software, like the one
by Zhang [36] which generates load test for SQL Servers.
AutoJMH is a tool that sits between proling/trace anal-
ysis and microbenchmarking, providing execution times for
many individuals points of the program with high precision.
Performance testing in isolation.
Specially close to our work are the approaches of Hork y
[18, 17], Kuperberg [22] and Pradel [28].
Microbenchmarking, and therefore AutoJMH , evaluate
performance by executing one segment of code in isolation.
A simpler alternative favored by industry are performance
unit tests [10, 11], which consist in measuring the time a
unit test takes to run. Hork y et.al. proposes methodologies
and tools to improve the measurements that can be obtained
using performance unit tests uses, unlike AutoJMH , which
uses unit tests only to collect initialization data. Kuperberg
creates microbenchmarks for Java APIs using the compiled
bytecode. Finally, Pradel proposes a test generator tailored
for classes with high level of concurrency, while AutoJMH
uses the JMH built-in support for concurrency. All these
approaches warm-up the code and recognize the intrinsic
non-determinism of the executions.
The main distinctive feature of AutoJMH over these sim-
ilar approaches is its unique capability to measure at the
statement level . These other approaches generate test exe-
cution for whole methods at once. Baudry [6] shows that
some methods use code living as far as 13 levels deep in the
call stack, which gives us an idea of how coarse can be ex-
ecuting a whole test method. AutoJMH is able to measure
both complete methods and statements as atomic as a single
assignment. During the warm-up phase the generated JHM
payload wrapper method gets in-lined and therefore, the mi-
crobenchmark loop do actually execute statements. Another
important distinction if that AutoJMH uses data extractedfrom an expected usage of the code, (i.e. the unit tests).
Pradel uses randomly generated synthetic data, which may
produce unrealistic performance cases. For example, JIT
in-lining is a very common optimization that improves per-
formance in the usual case, while reducing it in less usual
cases. The performance improvement of this well known op-
timization is hard to detect assuming that all inputs have
the same probability of occurrence.
Program Slicing.
AutoJMH creates a compilable slice of a program which
can be executed, stays in stable state and is not aected un-
wanted optimizations. Program slicing is a well established
eld [33]. However, to the best of our knowledge, no other
tool creates compilable slices with the specic purpose of
microbenchmarking.
6. CONCLUSION AND FUTURE WORK
In this paper, we propose a combination of static and dy-
namic analysis, along with code generation to automatically
build JMH microbenchmarks. We present a set of code gen-
eration strategies to prevent runtime optimizations on the
payload, and instrumentation to record relevant input values
for the SUA. The main goal of this work is to support Java
developers who want to develop microbenchmarks. Our ex-
periments show that AutoJMH does generate microbench-
marks as accurate as those handwritten by performance en-
gineers and better than the ones built by professional Java
developers without experience in performance assessment.
We also show that AutoJMH is able to analyze and extract
thousands of loops present mature Java applications in order
to generate correct microbenchmarks.
Even when have addressed the most common pitfalls found
in the current microbenchmarks today, we are far from be-
ing able to handle all possible optimizations and situations
detrimental for microbenchmark design, therefore, our fu-
ture work will consist in further improve AutoJMH to ad-
dress these situations.
7. ACKNOWLEDGMENTS
We would like to thank our anonymous reviewers for their
feedback. Special thanks goes to Aleksey Shipilev for re-
viewing the `Expert' set of microbenchmarks and providing
valuable insights on the paper. This work is partially sup-
ported by the EU FP7-ICT-2011-9 No. 600654 DIVERSIFY
project. This work was also partially funded by the Clar-
ity project funded by the call "embedded systems and con-
nected objects" of the French future investment program.
Cf. http://www.clarity-se.org
1418. REFERENCES
[1] Aleksey Shipilev. Java Microbenchmarks Harness (the
lesser of two evils). http://shipilev.net/talks/
devoxx-Nov2013-benchmarking.pdf, 2013.
[2] Aleksey Shipilev. Java Benchmarking as easy as two
timestamps. http://shipilev.net/talks/
jvmls-July2014-benchmarking.pdf, July 2014.
[3] Aleksey Shipilev. Nanotrusting the nanotime.
http://shipilev.net/blog/2014/nanotrusting-nanotime,
Oct. 2014.
[4] Aleksey Shipilev. The Black Magic of (Java) Method
Dispatch. http://shipilev.net/blog/2015/
black-magic-method-dispatch/, 2015.
[5] E. Altman, M. Arnold, S. Fink, and N. Mitchell.
Performance Analysis of Idle Programs. In Proceedings
of the ACM International Conference on Object
Oriented Programming Systems Languages and
Applications , OOPSLA '10, pages 739{753, New York,
NY, USA, 2010. ACM.
[6] B. Baudry, S. Allier, M. Rodriguez-Cancio, and
M. Monperrus. Automatic Software Diversity in the
Light of Test Suites. arXiv:1509.00144 [cs] , Sept.
2015. arXiv: 1509.00144.
[7] Brian Goets. Java theory and practice: Anatomy of a
awed microbenchmark. http:
//www.ibm.com/developerworks/library/j-jtp02225/,
Feb. 2005.
[8] J. Burnim, S. Juvekar, and K. Sen. WISE: Automated
Test Generation for Worst-case Complexity. In
Proceedings of the 31st International Conference on
Software Engineering , ICSE '09, pages 463{473,
Washington, DC, USA, 2009. IEEE Computer Society.
[9] Cli Click. The Art of Java Benchmarking.
http://www.azulsystems.com/presentations/
art-of-java-benchmarking, June 2010.
[10] C. Curtsinger and E. D. Berger. STABILIZER:
Statistically Sound Performance Evaluation. In
Proceedings of the Eighteenth International Conference
on Architectural Support for Programming Languages
and Operating Systems , ASPLOS '13, pages 219{228,
New York, NY, USA, 2013. ACM.
[11] David Astels. Test-Driven Development: A Practical
Guide: A Practical Guide . Prentice Hall, Upper Saddle
River, N.J.; London, 1 edition edition, July 2003.
[12] Dmitry Vyazalenko. Using JMH in a real world
project. https://speakerdeck.com/vyazelenko/
using-jmh-in-a-real-world-project, Oct. 2015.
[13] A. Georges, D. Buytaert, and L. Eeckhout.
Statistically Rigorous Java Performance Evaluation.
InProceedings of the 22Nd Annual ACM SIGPLAN
Conference on Object-oriented Programming Systems
and Applications , OOPSLA '07, pages 57{76, New
York, NY, USA, 2007. ACM.
[14] A. Georges, L. Eeckhout, and D. Buytaert. Java
Performance Evaluation Through Rigorous Replay
Compilation. In Proceedings of the 23rd ACM
SIGPLAN Conference on Object-oriented
Programming Systems Languages and Applications ,
OOPSLA '08, pages 367{384, New York, NY, USA,
2008. ACM.
[15] M. Grechanik, C. Fu, and Q. Xie. Automatically
Finding Performance Problems withFeedback-directed Learning Software Testing. In
Proceedings of the 34th International Conference on
Software Engineering , ICSE '12, pages 156{166,
Piscataway, NJ, USA, 2012. IEEE Press.
[16] S. Han, Y. Dang, S. Ge, D. Zhang, and T. Xie.
Performance Debugging in the Large via Mining
Millions of Stack Traces. In Proceedings of the 34th
International Conference on Software Engineering ,
ICSE '12, pages 145{155, Piscataway, NJ, USA, 2012.
IEEE Press.
[17] V. Horky, P. Libic, L. Marek, A. Steinhauser, and
P. Truma. Utilizing Performance Unit Tests To
Increase Performance Awareness. In Proceedings of the
6th ACM/SPEC International Conference on
Performance Engineering , ICPE '15, pages 289{300,
New York, NY, USA, 2015. ACM.
[18] V. Horky, P. Libic, A. Steinhauser, and P. Truma.
DOs and DON'Ts of Conducting Performance
Measurements in Java. In Proceedings of the 6th
ACM/SPEC International Conference on
Performance Engineering , ICPE '15, pages 337{340,
New York, NY, USA, 2015. ACM.
[19] W. K. Josephson, L. A. Bongo, K. Li, and D. Flynn.
DFS: A File System for Virtualized Flash Storage.
Trans. Storage , 6(3):14:1{14:25, Sept. 2010.
[20] Julien Ponge. Avoiding Benchmarking Pitfalls on the
JVM. Oracle Java Magazine , Aug. 2014.
[21] Julien Ponge. Revisiting a (JSON) Benchmark. https:
//julien.ponge.org/blog/revisiting-a-json-benchmark/,
Sept. 2014.
[22] M. Kuperberg, F. Omri, and R. Reussner. Automated
Benchmarking of Java APIs .
[23] E. Liarou, R. Goncalves, and S. Idreos. Exploiting the
Power of Relational Databases for Ecient Stream
Processing. In Proceedings of the 12th International
Conference on Extending Database Technology:
Advances in Database Technology , EDBT '09, pages
323{334, New York, NY, USA, 2009. ACM.
[24] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F.
Sweeney. Producing Wrong Data Without Doing
Anything Obviously Wrong! In Proceedings of the 14th
International Conference on Architectural Support for
Programming Languages and Operating Systems ,
ASPLOS XIV, pages 265{276, New York, NY, USA,
2009. ACM.
[25] A. P. Navik, M. A. Zaveri, S. V. Murthy, and
M. Dawarwadikar. Microbenchmark Based
Performance Evaluation of GPU Rendering. In N. R.
Shetty, N. H. Prasad, and N. Nalini, editors, Emerging
Research in Computing, Information, Communication
and Applications , pages 407{415. Springer India, 2015.
DOI: 10.1007/978-81-322-2550-8 39.
[26] A. Nistor, P.-C. Chang, C. Radoi, and S. Lu. Caramel:
Detecting and Fixing Performance Problems That
Have Non-intrusive Fixes. In Proceedings of the 37th
International Conference on Software Engineering -
Volume 1 , ICSE '15, pages 902{912, Piscataway, NJ,
USA, 2015. IEEE Press.
[27] O. Olivo, I. Dillig, and C. Lin. Static Detection of
Asymptotic Performance Bugs in Collection
Traversals. In Proceedings of the 36th ACM SIGPLAN
Conference on Programming Language Design and
142Implementation , PLDI 2015, pages 369{378, New
York, NY, USA, 2015. ACM.
[28] M. Pradel, M. Huggler, and T. R. Gross. Performance
Regression Testing of Concurrent Classes. In
Proceedings of the 2014 International Symposium on
Software Testing and Analysis , ISSTA 2014, pages
13{25, New York, NY, USA, 2014. ACM.
[29] N. Rajovic, A. Rico, J. Vipond, I. Gelado, N. Puzovic,
and A. Ramirez. Experiences with Mobile Processors
for Energy Ecient HPC. In Proceedings of the
Conference on Design, Automation and Test in
Europe , DATE '13, pages 464{468, San Jose, CA,
USA, 2013. EDA Consortium.
[30] C. J. Rossbach, J. Currey, M. Silberstein, B. Ray, and
E. Witchel. PTask: Operating System Abstractions to
Manage GPUs As Compute Devices. In Proceedings of
the Twenty-Third ACM Symposium on Operating
Systems Principles , SOSP '11, pages 233{248, New
York, NY, USA, 2011. ACM.
[31] D. Shen, Q. Luo, D. Poshyvanyk, and M. Grechanik.
Automating Performance Bottleneck Detection Using
Search-based Application Proling. In Proceedings of
the 2015 International Symposium on Software
Testing and Analysis , ISSTA 2015, pages 270{281,
New York, NY, USA, 2015. ACM.
[32] M. J. Steindorfer and J. J. Vinju. OptimizingHash-array Mapped Tries for Fast and Lean
Immutable JVM Collections. In Proceedings of the
2015 ACM SIGPLAN International Conference on
Object-Oriented Programming, Systems, Languages,
and Applications , OOPSLA 2015, pages 783{800, New
York, NY, USA, 2015. ACM.
[33] F. Tip. A Survey of Program Slicing Techniques.
Technical report, CWI (Centre for Mathematics and
Computer Science), Amsterdam, The Netherlands,
The Netherlands, 1994.
[34] L. Torczon and K. Cooper. Engineering A Compiler .
Morgan Kaufmann Publishers Inc., San Francisco,
CA, USA, 2nd edition, 2011.
[35] D. Weise, R. F. Crew, M. Ernst, and B. Steensgaard.
Value Dependence Graphs: Representation Without
Taxation. In Proceedings of the 21st ACM
SIGPLAN-SIGACT Symposium on Principles of
Programming Languages , POPL '94, pages 297{310,
New York, NY, USA, 1994. ACM.
[36] P. Zhang, S. Elbaum, and M. B. Dwyer. Automatic
Generation of Load Tests. In Proceedings of the 2011
26th IEEE/ACM International Conference on
Automated Software Engineering , ASE '11, pages
43{52, Washington, DC, USA, 2011. IEEE Computer
Society.
143