Mimic: Computing Models for Opaque Code
Stefan Heule
Stanford University
Stanford, CA, USA
sheule@cs.stanford.eduManu Sridharan
Samsung Research America
Mountain View, CA, USA
m.sridharan@samsung.comSatish Chandra
Samsung Research America
Mountain View, CA, USA
schandra@acm.org
ABSTRACT
Opaque code, which is executable but whose source is unavail-
able or hard to process, can be problematic in a number of
scenarios, such as program analysis. Manual construction of
models is often used to handle opaque code, but this process
is tedious and error-prone. (In this paper, we use model to
mean a representation of a piece of code suitable for program
analysis.) We present a novel technique for automatic gener-
ation of models for opaque code, based on program synthesis.
The technique intercepts memory accesses from the opaque
code to client objects, and uses this information to construct
partial execution traces. Then, it performs a heuristic search
inspired by Markov Chain Monte Carlo techniques to dis-
cover an executable code model whose behavior matches
the opaque code. Native execution, parallelization, and a
carefully-designed tness function are leveraged to increase
the eectiveness of the search. We have implemented our
technique in a tool Mimic for discovering models of opaque
JavaScript functions, and used Mimic to synthesize correct
models for a variety of array-manipulating routines.
Categories and Subject Descriptors
I.2.2 [ Automatic Programming ]: Program synthesis
Keywords
Opaque code, program synthesis, MCMC, model generation,
JavaScript
1. INTRODUCTION
We consider the problem of computing models from opaque
code. By opaque , we mean code that is executable but
whose source is unavailable or otherwise dicult to process.
Consider the case of a JavaScript runtime, which provides
\native"implementation of a large number of utility functions,
such as array functions. Natively-implemented methods are
opaque to an analysis tool built to analyze JavaScript sources,
and are a hindrance to eective analysis because the tool
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
FSE ‚Äô15, Bergamo, Italy
Copyright 2015 ACM X-XXXXX-XX-X/XX/XX ...$15.00.must make either unsound or overly conservative assumptions
on what those native methods might do. This same situation
arises for many languages and runtimes.
Opacity also is a concern when a third-party library is dis-
tributed in deliberately obfuscated form that hinders static
analysis. In JavaScript, for example, an obfuscator might
replace access to elds by computed names, e.g., transform-
ing y = x.foo top = ["o","fo"]; y = x[p[1]+p[0]] . Such
changes can foil a static analyzer's ability to reason pre-
cise about data ow through the heap.
Models provide an alternate, easy to analyze representation
of the opaque code. Sometimes, models can be written by
hand; however, this is tedious and error prone, and requires
understanding of how the opaque code works. A need for
better, automated ways of creating models has been argued
in the literature [10, 19, 7, 11].
In this paper, we show a new, automatic way of creating
models for opaque functions, based on a novel search-based
program synthesis strategy. We observe that the behavior
of an opaque function can often be indirectly observed by
intercepting accesses to memory shared between the client
and the opaque code. (Here, \shared" is in the sense of an
object that is passed as a parameter to a called function
and has nothing to do with concurrency.) In most common
dynamic languages (e.g., JavaScript, Lua, Python, Ruby),
interception of accesses to shared objects can be achieved
using indirection via proxy objects (discussed in more detail
in Section 4). In this paper, we show that, surprisingly, it is
in fact possible to generate useful models of opaque functions
based solely on observation of these shared memory accesses .
Given a set of inputs for an opaque function, our technique
collects traces of (shared) memory accesses to those inputs
by running the function against those inputs and recording
the intercepted accesses. It then carries out a random search,
inspired by Markov Chain Monte Carlo (MCMC) sampling,
to synthesize from scratch a function whose execution re-
sults in the same sequence of reads and writes. Our strategy
is a \generate-and-test" strategy, leveraging ecient native
execution|simply running it|to test the quality of candi-
date models. Comparison of quality of models is done using
a carefully designed tness function that takes into account
the degree to which a model matches the available traces.
Native execution lets the technique run tens of thousands
of trials per minute, and it yields models that are in fact
concrete code. Thus, our models are agnostic to whatever
abstraction a program analysis wishes to employ. Figure 2
shows the model that our approach recovers for the Java-
Script Array.prototype.shift method. Notice that the model
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ESEC/FSE‚Äô15 , August 30 ‚Äì September 4, 2015, Bergamo, Italy
c2015 ACM. 978-1-4503-3675-8/15/08...$15.00
http://dx.doi.org/10.1145/2786805.2786875
710includes complex control-ow constructs in addition to the
elementary statements.
Related approaches The problem of generating an imple-
mentation from a trace of actions is not new, but to the
best of our knowledge, our technique is new. Closely re-
lated work [10, 6] in this area has used the concept of a
version-space algebra to search through a space of candidate
programs. In version-space algebra, a domain-specic data
structure encodes all programs that are consistent with the
traces seen so far. When a new trace is presented, the data
structure is adjusted to eliminate those programs that would
be inconsistent with this new trace. The nal space can be
ranked using domain-specic insights. The success of the
version-space technique depends on careful design both of
the space of domain-specic programs and of the data struc-
ture that represents them, in such a way that the knowledge
embedded in a new trace can be factored in eciently [6].
While the version-space algebra approach has been very
successful in specic domains such as spreadsheet manipu-
lation, its success has not been shown on general-purpose
programs. The work by Lau et al. [10] handles only the
situation in which the complete trace with the state of all
variables is given at each step, along with the knowledge of
the program counter; this simplies the search space con-
siderably. Given our execution traces, which contain only
shared memory accesses and no program counter information,
there is not clear way to represent all consistent programs in
a compact data structure that can be eciently updated for
new traces.
Other recent work uses an \oracle-guided" approach to
model generation, based on constraint generation and SMT
solvers [15, 8]. The generated constraints allow for a model
to be constructed from pre-dened \building blocks" or \com-
ponents," and they ensure the model is consistent with input-
output examples obtained from running the opaque code.
Here, the limitation is that the set of building blocks must
be chosen very carefully to make the constraint problem
tractable [15]|it is unclear if such an approach could scale
to generate models as complex as that of Figure 2 without
excessive hand-tuning.
The alternative, then, is to adopt a generate-and-test strat-
egy, and the important question is how to organize the gen-
eration of candidates so as to not have to evaluate enormous
spaces (even if bounded) of programs exhaustively. In this
regard, recent work [4] has shown the promise of genetic algo-
rithms to synthesize patches to x buggy programs. In that
work, the operations of mutation and crossover are applied
to an existing defective program that has both passing and
failing test inputs. Mutation is applied by grafting existing
code exercised in failing inputs, and this contains the search
space to manageable size. Since our problem is to synthesize
code from just the execution traces, we cannot readily adopt
this method: there is no place to graft from. Nevertheless,
this work has strongly inspired our own work in its use of
statistical techniques to explore an enormous search space of
code fragments.
For our setting, an approach inspired by MCMC sampling
seems to be an eective search strategy, and this is borne
out by our results.
Overview of Results We have implemented our approach
in a tool called Mimic for JavaScript. It collects traces
by wrapping function parameters with JavaScript proxy ob-
jects (see Section 4). We have found Mimic to be surpris-1function shift ( arg0 ) {
2 var n0 = arg0 . length
3 if (n0) {
4 var n1 = arg0 [0]
5 for ( var i = 0; i < (n0 -1); i += 1) {
6 var n2 = (i +1) in arg0
7 if (n2) {
8 var n3 = arg0 [i+1]
9 arg0 [i] = n3
10 } else {
11 delete arg0 [i]
12 }
13 }
14 delete arg0 [i]
15 arg0 . length = i
16 return n1
17 } else {
18 arg0 . length = 0
19 }
20}
Figure 2: Model of Array.prototype.shift generated by
our tool.
ingly eective in computing models of several of the array-
manipulating routines in JavaScript runtime. In fact, the
models generated by Mimic have occasionally been of higher
quality than the hand-written models available with WALA,
a production-quality program analysis system [20]. When it
succeeded, Mimic required, on average, roughly 60 seconds
per method, evaluating hundreds of thousands of candidates
in the process. The technique is easily parallelized, and in
our evaluation we leveraged a 28-core machine for improved
performance. Mimic was just as capable in extracting mod-
els from obfuscated versions of the same library methods
written in JavaScript. Our implementation is available at
https://github.com/Samsung/mimic .
As with any search-based technique, there are limitations
(see Section 5.3 for a detailed discussion). Our approach
is currently limited to discovering models of functions that
primarily perform data ow, with relatively simple logical
computation. We cannot generate a model for a complex
mathematical function (e.g., sine), since our trace of shared
memory accesses does not expose the complex calculations
within the function. Nevertheless, we believe our approach
can still be usefully applied in a variety of scenarios.
Contributions This paper makes the following contribu-
tions:
We introduce the problem of computing models for
opaque code, and present a novel search-based synthesis
algorithm to address this problem. To our knowledge,
no other technique can nd models for opaque code with
the minimal information that our algorithm requires.
We describe a prototype synthesizer Mimic for Java-
Script functions based on use of proxy objects, and show
that it can successfully synthesize accurate models for
a variety of array-manipulating routines.
2. OVERVIEW
In this section, we give an overview of our synthesis tech-
nique Mimic , using an example function from the JavaScript
standard library.
Consider JavaScript's built-in Array.prototype.shift func-
tion. For a non-empty array, shift removes its rst element e
711Ini$al''Inputs'
Input'Gen'All''Inputs'
Loop'Detect'Loop''Structure'
Categorizer'Input'Cat1'Input'Cat2'
Search'Model1'Model2'
Merge'Unknown''Condi$ons''Model'
Final''Model'
‚Ä¶"Input'Catn'Modeln'
Search'
Search'
Search''+'Cleanup'
Figure 1: Overview of Mimic phases. The green phases (with solid outlines) are deterministic, while the red
phases (with dashed outlines) incorporate randomness. We call the procedure corresponding to the whole
gure MimicCore.
(at index 0), shifts the remaining values down by one index
(so the value at index 1 is shifted to 0, 2 to 1, etc.), and
nally returns e, e.g.:
var arr = [ 'a','b','c','d'];
var x = arr. shift ();
// x is 'a', and arr is now [ 'b','c','d']
For an empty array, shift returns the value undefined .Mimic
is able to generate a model for shift , shown in Figure 2,
that captures the above behaviors based solely on collected
execution traces, with no access to a source representation.
Note that the method also works for so-called sparse arrays,
which may have certain elements missing.
In the remainder of this section, we outline the steps taken
byMimic to generate this model. These steps are illustrated
in Figure 1.
Generating inputs and traces. Our approach begins by
generating a set of execution traces for the function in ques-
tion, based on some initial inputs provided by the user.
An execution trace includes information about the memory
operations performed by the function on any objects reach-
able from its parameters, and also what value was returned.
Mimic uses proxy objects to collect such traces for JavaScript
functions (details in Section 4). For the shift function, given
the input ['a','b','c','d'], we obtain the following trace:
read field 'length 'of arg0 // 4
read field 0 of arg0 // 'a'
read field 1 of arg0 // 'b'
write 'b'to field 0 of arg0
read field 2 of arg0 // 'c'
write 'c'to field 1 of arg0
read field 3 of arg0 // 'd'
write 'd'to field 2 of arg0
delete field 3 of arg0
write 3 to field 'length 'of arg0
return 'a'
The trace contains reads from and writes to the array
parameter arg0, with writes showing what value was written
but not how the value was computed.
Given traces based on the initial inputs, Mimic generates
other potentially interesting inputs whose traces may clarify
the behavior of the function. In the above trace, we see
entry read field 1 of arg0 , which reads 'b'from the input
array, and then write 'b'to field 0 of arg0 , which writes
'b'. Based solely on these trace entries, one cannot tell if
'b'is being copied from the input array, or whether the
write obtains 'b'from some other computation. Hence,
Mimic generates an input with a dierent value in arg0[1] ,
to attempt to distinguish these two cases. Input generation is
based on heuristics, and is discussed more fully in Section 3.2.Loops. Given a completed set of traces, our technique next
tries to discover possible looping structures in the code. We
rst abstract each trace to a trace skeleton , which contains
the operations performed by a trace but elides specic values
and eld osets. A trace skeleton exposes the structure of the
computation in a manner less dependent on the particular
input values of a trace. For the above trace, the skeleton of
the rst four entries is read;read;read;write; .
Given a set of trace skeletons, Mimic then proposes loop
structures by searching for repeated patterns in the skeletons,
rating loop structures by how well they match (see Section 3.3
for details). Each loop structure is assigned a probability
proportional to its rating, and then Mimic randomly selects
a structure to use based on these probabilities. The full space
of loop structures supported by Mimic is covered by runs of
the tool with dierent random seeds.
For shift , the highest-rated loop structure is also the
correct one, and is as follows (shown over the skeleton in a
regular language-like syntax):
read ; read ;( has ;( read ; write ;| delete ;))* delete ;
write ;
The trace event hasis a check if a given index is part of the
array (which may return false for sparse arrays).
Categorization. After a loop structure is chosen, Mimic
groups the traces into distinct categories based on their
skeletons. For shift , two categories are created, one for
traces matching the loop structure above, and one for the
trace corresponding to the empty array (which does not
match the structure). Mimic synthesizes models for each
category separately, as described next, and then merges them
to discover the nal model which works for all inputs.
Search. To begin a random search for a model, we rst
generate a program to closely match one of the given input
traces. For the trace above and the given loop structure, we
generate the following initial program:
var n0 = arg0 . length
var n1 = arg0 [0]
for ( var i = 0; i < 0; i += 1) {
var n2 = 1 in arg0
if ( true ) {
var n3 = arg0 [1]
arg0 [0] = 'b'
} else {
delete arg0 [2]
}
}
delete arg0 [3]
arg0 . length = 3
return 'a'
712s::= v:=e[e]je[e] := ejv:=e(e, . . . , e)
j if ( e) {s} else { s}jv:=e
j for ( s;e;s) {s}js;s
e::= . . .j-1j0j1j. . .jtruejfalse
j e+eje-eje<eje==ej!e
j (v, . . . , v) => e
Figure 3: Syntax of a small object-oriented lan-
guage.
This program is clearly too specic to the given trace (it
uses specic values from its input array), and its loop and if
conditions are incorrect.1
A random mutation is then applied to this program. For
instance, the second line might be replaced with the state-
ment var n1 = arg0[n0+1] . The mutations allow the program
gradually be transformed towards a solution. To this end,
the mutated program is compared with the current program
in terms of how closely is matches the available traces. If it
\ranks" better|as per a tness function|the mutated pro-
gram becomes the current candidate. If not (like in this case,
where the mutation is worse), it still becomes the current
candidate with a certain probability. This is a random search
inspired by MCMC (see Section 3.4).
Our random search is able to gradually evolve this program
into the code in lines 4{16 of Figure 2, which works for all non-
empty arrays. Key to the ecacy of the search is a tness
function for programs that rewards incremental progress
toward a correct solution.
For the category corresponding to empty arrays, the search
discovers the following program:
var n0 = arg0 . length
arg0 . length = 0
// program implicitly returns undefined
Merging. After generating models for all categories, Mimic
merges the models using conditional statements to create a
single model. At rst, the branching conditions are unknown,
so some trivial condition such as true is used. Another phase
of random search discovers the correct conditions to make the
model work for all input traces. Finally, a last cleanup search
makes the program more readable by removing unnecessary
statements and simplifying the program. For shift , this nal
search yields the model in Figure 2.
3. APPROACH
In this section we detail the dierent phases of our approach
to synthesizing models of opaque code, shown in Figure 1.
As input, we require a function and one or more inputs
to that function. Furthermore, we assume that we have a
means of executing the function on an input of our choice and
recording a trace of this execution. We talk more about how
these assumptions are fullled for JavaScript in the setting
ofMimic in Section 4.
We explain our approach for a simple, dynamic, object-
oriented language with object allocation, eld reads and
writes, functions (\callable" objects), integers, arithmetic
operations, conditionals, and loops. For simplicity, all eld
1In our implementation, the initial program is actually
slightly more general and contains an auxiliary result variable
as well as other ways to break out of the loop; see Section 3.names are integers, and objects can be viewed as arrays,
where theith array element is stored in eld i 1. The
syntax for statements and expressions is given in Figure 3.
A program trace captures the execution of a function on a
particular input and is a sequence of trace events and can be
eld reads, writes and function invocations. Finally, the trace
contains a return value. For all the values in the trace (such
as the eld being read, or the receiver), the trace directly
holds that value if it is a primitive (i.e., an integer). If the
value is an object, then the trace holds a unique identier
for that object. For our simple language, this trace format is
sucient to capture all object accesses performed by opaque
code to objects passed as parameters. Note that the trace
does not contain information about where values originate.
3.1 Main Loop
Algorithm 1 Main loop for model synthesis
Require: opaque function f, initial inputs I, timeoutt
1:procedure FindModel (f;I;t )
2:m null
3: whilem=nulldo
4: InitRandomSeed ()
5:m MimicCore (f;I) run with timeout t
6: returnm
Algorithm 1 gives pseudocode for the main loop of our
technique. In each iteration, a global pseudo-random number
generator is initialized with a fresh random seed, and then
the synthesis procedure MimicCore (shown in Figure 1) is
invoked with some timeout t(nullis returned if tis exceeded).
Recall from Figure 1 that the loop detection and search
phases make use of randomness. Hence, by periodically
restarting the search with a fresh random seed, we ensure
that (1) dierent loop structures are explored and (2) the core
search explores dierent parts of the state space, breaking out
of local minima. Note that the loop parallelizes trivially, by
simply running multiple iterations concurrently and stopping
when any iteration succeeds. In our implementation, we
used an exponential backo strategy to vary the timeout (see
Section 5).
3.2 Input Generation
In order to generate an accurate opaque function model in
our approach, we require a set of inputs that covers all of the
possible behaviors the function can exhibit. While generating
such an input set is impossible in general, we have developed
heuristics that have worked well for the opaque functions we
have tested thus far. Given one or more representative inputs
provided by the user, we automatically generate additional
inputs, with three main goals:
1.Determine the ow of values through a method. As
illustrated with the example trace in Section 2, with
only a trace it is impossible to know if a method copies
a value from another location or computes it from
scratch.
2.Find dependencies on values of the inputs. For instance,
the method Array.prototype.indexOf in JavaScript re-
turns the index of a value in an array (or -1if it doesn't
exist in the array). By changing the values in the ar-
ray and what value is searched for, we can learn this
dependency on the input values.
3.Expose corner cases. For many functions it is possible
to increase the coverage by trying corner cases such
713as the empty array. For instance, this reveals that
Array.prototype.pop returns undefined for the empty
array.
Algorithm 2 Input generation
Require: initial inputs init
1:procedure InputGen (init)
2:R init;I init;I0 ;
3: whileI6=;do
4: fori2Ido
5: t GetTrace (i)
6: L ExtractReadLocs (t)
7: I0 I0[GenNewVals (i;L)
8:I I0 R;R R[I;I0 ;
9: returnR
Algorithm 2 gives pseudocode for input generation. In
each iteration of the while loop, new inputs I0are generated
from a set of inputs Ias follows. For every input iinI, the
opaque function is rst executed on ito record a trace t.
Fromt, we extract all memory locations Lthat were read,
where a memory location consists of a base object and eld
oset. We generate new inputs by calling GenNewVals ,
which heuristically replaces the values of input locations in
Lwith new values.
To decide what dierent values to generate for a location,
we use heuristics based on the type of the original value. For
integers, we randomly choose values, and include a few xed
values that are likely corner cases such as 0,-1, or 1. For
objects, we sometimes replace the object with a clone, to
distinguish cases involving aliased parameters. Furthermore,
we can remove some elds, add additional elds, or provide
an empty object.
At the outermost level, we repeat the steps above until no
new inputs can be generated. In our prototype, we termi-
nated input generation after two iterations of the while loop,
as we found that this generated a sucient set of inputs.
3.3 Loop Detection
After input generation, our technique next discovers pos-
sible control-ow structure for the function, i.e., loops and
conditionals (the \Loop Detection" and \Categorizer" phases
in Figure 1). By xing the control ow structure for the
model rst, the core random search can concentrate on nd-
ing the correct expressions to complete the program. Here
we describe loop detection in more detail; categorizing and
merging were described in Section 2.
Given a set of execution traces like ours, discovering arbi-
trary looping constructs (with nesting, complex conditionals
within loops, etc.) is a highly non-trivial problem. The
problem can be viewed as learning a regular language form
only positive examples, where the examples are the program
traces, and the regular language represents the control ow.
Gold [5] showed that the class of regular languages is not
learnable from only positive examples. However, we have had
success in discovering basic looping structures with a simple,
probabilistic technique based on detecting repeated patterns
that appear in multiple execution traces for a function, as
described below.
We restrict the methods to have a single loop, with po-
tentially one conditional statement in the body. To discover
loop-like patterns of this form in traces, we rst abstractAlgorithm 3 Loop detection
Require: set of traces T
1:procedure LoopDetect (T)
2:S GetSkeletons (T);C ;
3: fors2Sdo
4:C C[LoopCandidates (s)
5: forc2Cdo
6:score [c] Rank (c;S)
7:L Sort (C;score )
8: returnL[i] with probability loop(1 )i 1
the traces to trace skeletons , which only consist of the event
kinds in the trace. Such abstraction is useful since there is
often some variation in the events generated by a statement
in a loop (e.g., the index being accessed by an array read).
Given such a trace skeleton, we enumerate candidate control
ow structures in the skeleton by simply enumerating all
possible loop start points, as well as branch lengths inside
the loop.2Consider the following trace skeleton:
read ; read ; write ; call ; read ; write ; call ;
read ; read ; write ; call ; read ; write ; call ;
For this example, the following control ow candidates are
generated, among others (in a regular expression-like syntax):
( read ; | write ; call )*
( read ; ( write ; call ;|))*
read ; ( read ; ( write ; call ;|))*
Note that the candidates must match the full trace, otherwise
the candidate could not be the control ow that generated
the given trace. To avoid guessing unlikely loops, one can
restrict the search to loops that have at least some number
of iterations, say 3.
We repeat the procedure above across skeletons of all traces
to create a complete set of loop candidates. Depending on
the length and the number of traces, this set can be quite
large. Our next step is to rank the loop candidates based
on their likelihood. First, we can check how many traces
can be explained by a given loop structure by matching
it against all trace skeletons. Intuitively, the more traces
that can be explained by a possible loop, the more likely
that loop is correct. Secondly, if several loops match the
same number of traces, then we further rank them by the
number of statements in the control ow candidate, with
fewer statements ranking higher.
Once the loops are sorted according to their rank, we choose
one loop at random with which to continue the search, where
theith loop is picked with probability loop(1 )i 1
for some parameters ; loop2(0;1).loopcontrols the
probability that no loop candidate is chosen. In our setting,
we found= 0:7 andloop= 0:9 to work well.
3.4 Random Search
We now describe the random search procedure at the heart
of our technique (the \Search" boxes in Figure 1). We view
the problem of nding a suitable code model for a set of
inputs as an optimization problem, for a tness function that
2This algorithm runs in polynomial time, but could get slow
for very long traces, in which case optimizations might be
necessary. In the cases we have looked at, this has not been
a problem.
714Algorithm 4 Random search for a model
Require: set of inputs I, loop structure l
1:procedure Search (I;l)
2:To fi7!GetTrace (i)gfori2I
3:m InitialModel (To[i];l) for somei2I
4:c Fitness (m;T o)
5: whilec>0do
6:m0 Mutate (m)
7:c0 Fitness (m0;To)
8: ifc0<cthen
9: m m0
10: else
11: m m0with probability
12: min (1 ;exp ( (c0 c) ))
13: returnm
14:procedure Fitness (m;T o)
15:s 0
16: for(i;t)2Todo
17:t0 GetModelTrace (m;i)
18:s s+Compare (t;t0)
19: returns
evaluates a model by comparing its execution traces to those
of the underlying opaque function. Finding a model then
corresponds to nding a minimum for the tness function
in the highly irregular and high-dimensional space of all
models. To nd a model, we employ a technique inspired
by Markov Chain Monte Carlo (MCMC) sampling and the
Metropolis{Hastings algorithm [1], which has been recently
used successfully in the domain of superoptimization [16].
The advantage of this technique is that the sampling fre-
quency is proportional to the value of the \density" function
at that point, so the search is carried out more productively.
Algorithm 4 gives pseudocode for our random search. We
maintain a current candidate model m, and each iteration
creates a mutated model m0.m0is then evaluated using
the tness function, and replaces the current model if it
has a lower cost. Otherwise, the new model might still
be accepted, with a probability that is proportional to the
dierence between the two costs. This allows the search to
recover from local minima and makes it robust against the
irregular nature of the search space. More precisely, if the
new model has a higher cost c0compared to the current cost
c, then the it is accepted with probability
min 
1;exp 
 (c0 c) 
whereandare parameters that controls how often models
that don't improve the score are accepted.3Empirically, we
found a value of 8for bothandto work well in our
setting.
3.4.1 Initial Program
We generate an initial program by exactly matching one
execution trace t, respecting the given loop structure (if any).
So, the initial program uses the exact concrete values from t
in its statements as needed. If there is ambiguity because of
aliasing (e.g., the same object is passed as two parameters),
we can pick any of the choices. For statements inside a loop,
3In an approach more closely resembeling MCMC, would
be zero. However, we found a non-zero value for to reduce
convergence times.where values might change with every iteration, we simply
use values from the rst iteration. It is then the job of the
random search to generalize this very specic initial program
to other inputs, and generalize statements inside loops to
work for all iterations.
More precisely, a trace event read field f of o intis trans-
lated to the statement n := o[f]; for a fresh local variable
n. The trace event write v to field f of o is translated to
o[f] := v; and a call call f with a0 .. an yields the state-
ment f(a0, ..., an); . If there is a loop, then a loop header
for (i = 0; i < 0; i++) is generated, for a fresh local vari-
able i. Initially, the loop body is never executed, and it
is up to the random search to nd the correct termination
statement. To allow breaking out of the loop early, we add
an additional statement to the end of the loop body of the
form if (false) break; .4
Finally, we introduce a local variable result for the result
of the function, which gets returned at the end. To allow
incrementally building the result, we additionally add the
statement if (true) result = result; inside the loop body
as well as the ifstatement containing the break . Initially,
these are nops, but allow the search to accumulate the result
if necessary.
3.4.2 Fitness Function
Our tness function Fitness in Algorithm 4 computes
the tness of a candidate model by comparing its execution
traces on the inputs against To, the traces from the opaque
function. To do so, it uses a function Compare that takes as
input a trace tfrom the opaque function called the reference
trace , andt0from the current model called the candidate
trace, both generated from the same input. It then computes
ascore that measures how close the t0is tot. A score of zero
indicatest=t0, while any larger value indicates a dierence
in the traces.
For our approach to work eectively, it is crucial that the
random search can make incremental progress toward a good
model. This incremental progress is enabled by the tness
function reecting partial correctness of a model in the score.
To this end, Compare gives ne-grained partial credit, even
for partially-correct individual events. For instance, if the
reference trace contains the event read field 1 of #1 , and
the candidate trace contains read field 2 of #1 , then the
score is lower (better) than in the case where no read event
was present at all, or where the read event also read from
the wrong object.
Formally, Compare (t;t0) is dened as:
1
20
BBB@X
Event
kindkX
i<jevs(t;k)j
i<jevs(t0;k)jdist(evs(t;k)[i];evs(t0;k)[i])1
CCCA(1)
+0
B@X
Event
kindkjevs(t;k)j evs(t0;k)1
CA (2)
+1
return-value( t)6= return-value( t0)	
(3)
We use evs(t;k)to refer to all events in a trace tof kindk
(e.g., all eld reads) as a list, which can be indexed using
4Alternatively, we could allow more complicated termination
expression in the loop header.
715the notation `[i], andjjreturns the length of a list (or the
absolute value, depending on context). return-value ()refers
to the return value of a trace, and dist(;)measures the
similarity of two trace events (of the same kind). It is dened
as follows:
dist(e1;e2) =1
jvals(e1)jX
i<jvals(e1)j1fvals(e1)[i]6= vals(e2)[i]g
where vals()is a list of values for a given trace event. For
a eld read, this is the receiver and eld name; for a eld
write it additionally includes the value written; and, for a
function call it includes the function invoked as well as all
arguments. Note that dist(;)is scaled to return a value
between 0 and 1.
In the formula for Compare ,(1)calculates the dierence
for all the events that are present in both traces, while (2)
penalizes the model for any event that is missing or added
(compared to the reference trace). Because (1)is scaled by
1
2, the search gets partial credit for just having the right
number of events of a given kind in a trace, even if the
wrong elds are read/written. This is useful to determine
the correct loop termination conditions without necessarily
having a correct loop body already. Finally, (3)ensures that
the result is correct. Section 5 will show the advantage of
this ne-grained tness function over a more straightforward
approach.
3.4.3 Random Program Mutation
The control ow structure is xed at this point, and the
random search can concentrate on nding the correct sub-
expressions for all the statements. To this end, a statement
is selected at random, and modied randomly. For eld
reads, writes and function calls, a sub-expression is selected
at random and replaced with a random new expression (we
will explain shortly what kinds of expressions are generated).
Similarly, for local variable assignments, the right-hand side
is randomly replaced. For a forloop, the upper bound is
replaced with a new random expression. For an ifstatement,
the condition is replaced randomly. break statements are not
modied.
Generating random expression follows the grammar of the
programming language in Figure 3. To avoid creating very
large nested expressions, the probability of producing an
expression of depth ddecreases exponentially with d. The
set of local variables than can be generated is determined
by the set of variables that is dened at the given program
point. The set of constants is taken from all constants that
appear in any of the traces, as well as a few heuristically
useful constants (such as 0).
3.5 Cleanup
When the random search succeeds, the resulting program
typically contains redundant parts. For instance, if it is not
necessary to break out of a loop early, then the statement
if (false) break; can be removed. Similarly, there might
be unused local variables, or expressions that are more com-
plicated then necessary (e.g., 1+1). To clean up the resulting
model, another random search is performed, with a slightly
modied tness function as well as dierent random muta-
tions: In addition to the existing program transformations,
statements can now also be removed. Furthermore, we add
the number of AST nodes of the program to the cost. This
allows the search to improve the program by removing un-used statements and simplifying expressions like 1+1to2.
Furthermore, the cost will never reach zero, and thus the
search is stopped after a certain number of iterations have
been carried out. Cleanup is not strictly necessary, as the
models perform the same observable behavior, whether they
are cleaned up or not. However, cleanup can make programs
nicer to look at by humans.
4. IMPLEMENTATION
We have implemented the ideas presented in Section 3 in a
prototype implementation called Mimic for JavaScript. The
tool is open source and available online.5In this section we
highlight challenges and solutions particular to our setting,
and discuss some implementation choices.
Trace Recording using Proxies. We leverage the ECMA-
Script 6 proxy feature [13], which allows for objects with
programmer-dened behavior on interactions with that object
(such as eld reads and writes, enumeration, etc.). An object
ocan be virtualized with a handler has follows:
var p = new Proxy(o, h);
Any interaction with pwill ask the handler how to handle
that interaction (and default to accessing odirectly if the
handler does not specify any action). We leverage this feature
to record the traces required for our technique, by proxying
all non-primitive arguments to the opaque function. This
way, we can intercept any interaction the opaque function
has with the arguments and record it as an event in the
trace. Our handler responds to all interactions just like the
underlying object would (by forwarding all calls), with the
additional book-keeping to record the trace.
Newly Allocated Objects. One challenge with this
approach is that newly allocated objects will only be visible
to the recording infrastructure when they are returned (or
otherwise used, e.g., written to a eld). For instance, the
function Array.prototype.map takes an array and a function,
and applies the function to all arguments, returning a new
array with all the results. When we record the trace, we
see all the accesses to the array elements and the function
invocations, but the eld writes to the newly allocated array
are not visible.
To handle such functions, we generate the relevant object
allocation at the beginning of the initial model and assign
it to the result variable. We also add a number of guarded
eld writes to the newly allocated object at dierent loca-
tions in the model (e.g., before the loop or inside the loop
body): if (false) result[0] = 0; . The random search is
then able to identify which particular assignments are cor-
rect (by changing the guard) and nd the correct eld name
and value to be written.
Non-Terminating Models. It is easy for the random mu-
tations to generate programs that are non-terminating, or
take very long to terminate. This can cause an innite loop
when recording the traces as part of evaluating the tness
function. Luckily, given the reference trace, we know how
long the trace from the model should be, and we can abort
the execution of a model when the trace gets signicantly
longer than the reference trace. In that case, the tness
5https://github.com/Samsung/mimic
716function assigns a very large cost to the trace (note that
small increases in trace length are not excessively penalized).
JavaScript-SpeciÔ¨Åc Features. JavaScript contains various
features not in our language from Section 3. Many of these
are straightforward to support, e.g., deletion of elds, func-
tion receivers, and exceptions. JavaScript allows variadic
functions by providing the special variable arguments that can
be used to access any argument using argument[i] , as well
as the number of arguments passed with arguments.length .
For instance, the function Array.prototype.push adds zero
or more elements to an array. Mimic supports such func-
tions, by generating inputs of dierent lengths and generating
expressions of the form arguments[i] and arguments.length
during the random search. Essentially, we can view the func-
tion as just having a single argument that is an array, and
then use the usual input generation strategy described earlier.
The only dierence is that we do not get any information
in the trace about accesses to arguments (this special object
cannot be proxied).
Optimizations. It is important for the random search to be
able to evaluate many random proposals, which is why we
implemented the following optimizations to our approach.
Input Selection. Input generation can create thousands
of inputs (or more), and executing all of them during search
would be prohibitively slow. For this reason, we restrict the
inputs used during search to a much smaller number. We
found 20 inputs to work well in our experiments, if the inputs
are diverse enough. To get diverse inputs, we choose input
that generate traces of dierent lengths, as well as inputs
that have dierent scores on the initial program. If the search
succeeds, all inputs are used to validate the result. If in this
nal validation, the score for some input is non-zero, then
the search is considered to have failed, and a new run (with a
dierent) random seed as described earlier is necessary. We
found this to not be an issue.
Program Mutations. If the program mutations are
generated na vely, it is easy to generate nonsensical programs.
For instance, by just following the programming language
grammar, one might generate an expression that tries to
read a eld of an integer, or use an array object as a eld
oset. To avoid exploring such malformed programs, we lter
out expressions when we can statically decide that they are
invalid. Given the dynamic nature of JavaScript, this is not
always possible, but we found that our ltering eliminates
many common malformed expressions.
Parallelization Strategy. Our procedure FindModel
is embarrassingly parallel. However, one can further improve
performance by exploiting the fact that some of the successful
runs nish much more quickly than others. It is often better
to kill a search early and retry again, in the hope of nding
one of those very quick runs. To do this, we start with a
small initial timeout t0, and then exponentially increase it
by a factor f. We found that running 28 threads in parallel
with a timeout of t0= 3 seconds to work well, with a factor
off= 2.6All threads run with the same timeout, and if any
of them succeed, all others can be aborted and the model can
be returned. If none succeed, then the timeout is increased
6If run with fewer threads, Mimic will automatically use a
smaller factor, so that roughly the same number of short
runs are made.by a factor of f, and the process starts again.
Cleanup As noted earlier, cleanup is not necessary, strictly
speaking, as models exhibit the same observable behavior
with or without cleanup. For this reason, our prototype
implements a quick cleanup that only removes unnecessary
statements and is always applied. Then, a full cleanup as a
random search can be applied optionally, to make the models
more readable.
5. EVALUATION
Here, we present an experimental evaluation of Mimic
on a suite of JavaScript array-manipulating routines. We
evaluated Mimic according to the following ve research
questions:
(RQ1) Success Rate : How often was Mimic successful in
generating an accurate model for the tested routines?
(RQ2) Performance : When it succeeded, how long did
Mimic take to generate a model?
(RQ3) Usefulness : Were the models generated by Mimic
useful for a program analysis client?
(RQ4) Obfuscation : IsMimic robust to obfuscation?
(RQ5) Fitness Function : How important was the ne-
grained partial credit given by our tness function
(Section 3.4.2)?
5.1 Experimental Setup
Our primary subject programs were the built-in meth-
ods for arrays on Array.prototype provided by any standard
JavaScript runtime [12]. These methods exhibit a variety
of behaviors, including mutation of arrays, loops that iter-
ate forward and backward through the array, and methods
returning non-trivial computed results (e.g., reduce ). Fur-
thermore, many of these methods can operate on JavaScript's
sparse arrays (in which certain array indices may be missing),
necessitating additional conditional logic.
We ran our experiments on a Intel Xeon E5-2697 ma-
chine with 28 physical cores, running at 2.6 GHz and using
node v0.12.0 to execute Mimic (written in TypeScript and
Python). Our implementation parallelizes the search for a
model using dierent random seeds.
5.2 Results
Success Rate Mimic was able to generate models for
some of the Array.prototype functions, but not others. The
functions for which it succeeded are listed in Table 1; we
discuss the other functions below. The models we synthesized
can be found at https://github.com/Samsung/mimic/blob/
master/models/array.js .7We also included three other
functions over arrays ( max,min, and sum) that performed a
bit more computation than the built-in functions.8
Performance Experimental data addressing (RQ2) ap-
pears in Table 1. The performance of Mimic was quite
reasonable, with models taking an overall average of 60.6
seconds to generate using our exponential increasing timeout
strategy, and less than 5 minutes on average for all models.
This experiment was repeated 100 times per example, with-
out using the full cleanup (see Section 4). The additional
time it would require to perform a full cleanup is 3.74 seconds
7Some array methods can handle \Array-like" objects; we
have not used such inputs and focused only on actual arrays.
8Though code is available for these functions, Mimic made
no use of it, observing their behavior as if they were opaque.
717Table 1: Summary of all Array.prototype functions
that Mimic can handle, as well as some handwrit-
ten functions to compute the sum, maximum and
minimum of an array of numbers. We report the av-
erage time (over 100 runs) it takes to synthesize the
model (using the quick cleanup) on our hardware
as well as how high the correct loop was ranked (1
being the highest rank).
Function Time to synthesize Loop
(in seconds) rank
every 67.86 22.41 1
lter 43.05 16.13 1
forEach 4.59 1.52 1
indexOf 42.94 38.59 1
lastIndexOf 36.92 19.22 2
map 15.64 6.86 1
pop 2.35 0.74 loop-free
push 291.94 310.17 3
reduce 25.33 12.51 1
reduceRight 126.41 53.77 1
shift 117.54 52.11 1
some 6.74 3.16 1
max 56.61 107.86 2
min 50.69 121.95 2
sum 20.35 39.83 2
on average, and less than 8 seconds for all examples (for the
default of 1000 cleanup iterations).
In the same table we also show the loop rank assigned by
our ranking heuristic (1 being the highest ranked proposal).
The loop ranking chooses the correct control structure for 9
out of 15 examples with rank 1, and with our choices for 
andloop,MimicCore then picks this loop with probability
64%. For the ve examples where the correct loop is ranked
second, the probability is 18.9%, and for push with rank 3
it is 5.7%. Finally, popdoes not have a loop (and the loop
inference does not propose any loops).
Longer search times were due to several reasons: (1) loop
ranking, (2) complicated expressions (e.g., generating the
index n0-i-1 is lower probability than, say, i), and complex
dependencies between loop iterations ( reduce requires the
result of the one iteration to be passed in the next iteration).
Mimic currently cannot synthesize models for Array.prototype
methods not listed in Table 1, due to the following issues:
Multiple loops :concat ,sort,splice and unshift all
require multiple loops, for which we currently do not
have any heuristics.
Complex conditionals within loops: reverse re-
verses a list in place. The loop body takes two indices
from the front and back and exchanges them. To handle
sparse arrays, four dierent cases need to be handled
(depending on whether either element is present or not).
Lack of relevant mutations: join,toString and
toLocalString require computing a (possibly localized)
string representation of an arbitrary array element,
which our mutations do not propose at the moment.
Proxy-related bugs: We discovered some bugs in
the current proxy implementation in node.js . Unfor-
tunately, it crashes for slice . It also reports traces
that are not in accordance with the specication forconcat ,shift and reverse . For shift we used our own
implementation that follows the specication.
Usefulness To answer (RQ3) , we compared the models
generated by Mimic with those present in the T.J. Watson
Libraries for Analysis (WALA) [20], a production-quality
static analysis framework for Java and JavaScript. We found
that WALA did not yet have any model for functions reduce ,
every , and some. Since these functions perform callbacks
of user-provided functions, a lack of a model could lead to
incorrect call graphs for programs invoking the functions.
We added the Mimic -generated models for these functions to
WALA, and conrmed via small examples that their presence
improved call graph completeness. Additionally, we found
WALA's existing model of the frequently-used popfunction
was written incorrectly, such that it would always return
undefined rather than the popped value. Many of WALA's
models also do not handle sparse arrays, which Mimic models
do handle. These examples illustrate how writing models by
hand can be tedious and error-prone, and how tool support
can ease the process. Mimic -generated models for the above
functions were accepted for inclusion in WALA.9
Obfuscation To answer (RQ4) , we ran Mimic -generated
models through a well-known JavaScript obfuscator,10and
tested that Mimic generated the same model for the ob-
fuscated function. The obfuscator employed rewrites static
property accesses into dynamic ones (see Section 1), which
could signicantly degrade the quality of many static anal-
yses. We conrmed that for all functions, we obtained the
same model when using the obfuscated code as the baseline,
showing the promise of our technique for making such code
more analyzeable.
Fitness Function To answer (RQ5) , we ran Mimic with
a more na ve tness function that only gave credit to a
trace when an individual event matched exactly with the
reference trace, rather than giving partial credit for partial
event matches (see Section 3.4.2). In an experiment, we
found that our tness function led to decreased running
times compared to the na ve function, validating our use
of ne-grained partial credit. Specically, the running time
average increased by 39.7% on average for the na ve tness
function, and as much as 170% for some examples.
5.3 Limitations and Threats to Validity
Mimic currently cannot handle opaque functions with any
of the following properties:
Complex local computations , e.g., a sine function, as
our trace does not expose such computations.
Nested loops or complex conditionals inside loops , as
discussed above. Adding such support without exces-
sively increasing the number of loop candidates is future
work.
Reliance on non-parameter inputs , e.g., a variable in
the closure state initialized via some other sequence of
method calls. This could perhaps be handled by the
user providing a richer test harness for the function.
Global side eects , e.g., on the network or le system.
This is a limitation of the native execution approach.
Long running time , which will slow down our search,
again due to native execution.
9https://github.com/wala/WALA/pull/64
10http://www.javascriptobfuscator.com/
Javascript-Obfuscator.aspx
718The primary threat to validity of our evaluation is our
choice of benchmarks. We tested Mimic on all Array.prototype
methods, and discussed cases it could and could not handle.
But,Mimic 's handling of dierent array routines, or routines
on other data structures, could vary. Our results thus far are
quite encouraging, and we plan to further validate Mimic
for other types of functions in future work.
6. RELATED WORK
Summary Computation Computing summaries of proce-
dure calls has been a very active area of in program analysis,
and it is impossible to mention all the related work on the
topic; Sharir and Pneuli's seminal work [17] is a good overview
of the foundational approaches to this problem. We have
used the term \models" in this paper; summaries can be
thought of as models that suce for specic static analyses,
which use abstractions.
Most of the work on summary computation assumes the
availability of library code for analysis. However, recently,
several authors have tried to deal with unavailable libraries.
The work by Zhu et al. [21] deals with the problem of missing
library code by inferring the minimal number of \must not
ow" specications that would prevent undesirable ows in
the client application. Bastani et al. [2] have also presented an
approach that infers potential summaries for an unavailable
library, based on a particular static analysis problem being
solved. In both the above approaches, ultimately a human
auditor must judge the validity of the ndings of the tool.
Like these works, we also do not require the code to be
available in analyzable form, but (unlike these works) we
do assume the library code is available to execute. On the
other hand, while the summaries computed in the above
mostly capture tainted-ness and aliasing, the models that we
compute are considerably richer.
Madsen et al. [11] construct models of library functions
through static analysis of uses of values returned from native
JavaScript functions. However, their work does not infer ow
of values through the library functions as would be needed,
for instance, for alias analysis; rather, it only infers the type
structure of values returned from the libraries.
Summaries can be helpful in dynamic analysis settings
as well. Palepu et al. [14] compute summaries of library
functions with the intention of avoiding repeated execution
of instrumented library code. Their summary representation
is suitable for aliasing and taint-ow like properties. By
contrast, our technique requires only partial observation of
the execution of the library code, and our models are richer,
but they are more expensive to compute.
Trace-based Synthesis As mentioned in Section 1, there
has been considerable work related to the problem of con-
structing programs to t traces. One of the earlier papers
in this area is that of Biermann et al. [3]. Given a trace of
allinstructions, as well as conditionals, they show how to
construct a (looping) program that would explain the trace.
They present a backtracking procedure that tries to discover
the right program structure. However, the technique requires
all instructions, including conditionals, in the trace. By con-
trast, our technique requires observing only the shared reads
and writes.
Traces can be detailed traces of low-level instructions, or
they can be traces of high level domain-specic instructions,
e.g. deleting a character in an editing buer. Automatic con-
struction of \programs" to automate end-user tasks has beenan area of much work in the last decade [10, 6]. Repetitive
user actions correspond to example traces, and a tool syn-
thesizes a program representation that automates such user
actions. However, most current approaches to synthesis from
examples are limited to generating programs in carefully-
designed domain-specic languages, leveraging clever algo-
rithms and data structures for eciency (cf. version algebra
discussion in Section 1). Extending such approaches to gen-
erating programs with richer data and control structures,
as needed in our scenario, is non-trivial. When it comes to
general purpose programs, Lau et al.'s work [10] mentions the
increased diculty in synthesis when traces do not capture
all the steps of a program's execution; in fact, they report
experiments only on complete traces, as in Biermann's work.
Other Synthesis Approaches Jha et al. [8] presented a
technique to synthesize straight-line programs from exam-
ples. Their technique searches through a space of combina-
tions of program operations which could be either primitive
statements, or API calls. Like our work, they check the
correctness of their synthesized programs against a native
implementation on a set of inputs. Later, Qi et al. [15] ex-
tended the technique to handle loops and data structures
by extending the set of possible primitive statements. As
discussed in Section 1, the main limitation of this approach
is in scalability of the constraint solver as the number of
primitive statement types increases; our generate-and-test
approach leverages native execution and parallelization to
improve scalability.
Sketching [18] is another synthesis system based on con-
straint solving. The idea of sketching is to synthesize missing
expressions (a.k.a. \holes") from an otherwise complete pro-
gram. For certain domains, boolean satisability (SAT)
techniques can be used to eciently synthesize completions
of programs with holes. Kuncak et al. [9] have presented
automated synthesis procedures that work (based on formula
solving) for a limited setting of linear arithmetic and data
structures. In our setting, the entire program needs to be
synthesized, and hence the above constraint-based techniques
do not apply directly.
Other Uses of Search Our work is inspired by recent suc-
cesses of search-based techniques in the research community.
To pick two examples, there is the automatic patch genera-
tion work by Weimer at al. [4], which uses genetic algorithms
for search (and which we discussed in Section 1), and com-
piler optimization work by Schkufza et al. [16], which also
uses the Markov-Chain Monte Carlo technique.
7. CONCLUSIONS
We have presented a novel technique for synthesizing mod-
els for opaque code. We collect partial execution traces by
intercepting memory accesses to shared state, and then use
a random search technique to construct an executable code
model that matches the traces. We implemented our tech-
nique in a tool Mimic for JavaScript, and showed that it
could synthesize non-trivial models for a variety of array-
manipulating routines.
Along with the paper we have submitted a replication
package containing the full source code of our implementation
as well as all the experimental setup to reproduce the results
in this paper. It has been successfully evaluated by the
Replication Packages Evaluation Committee and found to
meet expectations.
7198. REFERENCES
[1]C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan.
An introduction to MCMC for machine learning.
Machine Learning , 50(1-2):5{43, 2003.
[2] O. Bastani, S. Anand, and A. Aiken. Specication
inference using context-free language reachability. In
Proceedings of the 42nd Annual ACM
SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, POPL 2015, Mumbai, India,
January 15-17, 2015 , pages 553{566, 2015.
[3] A. W. Biermann, R. I. Baum, and F. E. Petry.
Speeding up the synthesis of programs from traces.
IEEE Trans. Comput. , 24(2):122{136, Feb. 1975.
[4]S. Forrest, T. Nguyen, W. Weimer, and C. Le Goues. A
genetic programming approach to automated software
repair. In Proceedings of the 11th Annual Conference
on Genetic and Evolutionary Computation , GECCO
'09, pages 947{954, New York, NY, USA, 2009. ACM.
[5] E. M. Gold. Language identication in the limit.
Information and Control , 10, 1967.
[6] S. Gulwani, W. R. Harris, and R. Singh. Spreadsheet
data manipulation using examples. Commun. ACM ,
55(8):97{105, 2012.
[7]S. H. Jensen, M. Madsen, and A. Mller. Modeling the
HTML DOM and browser API in static analysis of
javascript web applications. In SIGSOFT/FSE'11 19th
ACM SIGSOFT Symposium on the Foundations of
Software Engineering (FSE-19) and ESEC'11: 13rd
European Software Engineering Conference (ESEC-13),
Szeged, Hungary, September 5-9, 2011 , pages 59{69,
2011.
[8] S. Jha, S. Gulwani, S. A. Seshia, and A. Tiwari.
Oracle-guided component-based program synthesis. In
Proceedings of the 32nd ACM/IEEE International
Conference on Software Engineering - Volume 1, ICSE
2010, Cape Town, South Africa, 1-8 May 2010 , pages
215{224, 2010.
[9] V. Kuncak, M. Mayer, R. Piskac, and P. Suter.
Complete functional synthesis. In Proceedings of the
2010 ACM SIGPLAN Conference on Programming
Language Design and Implementation , PLDI '10, pages
316{329, New York, NY, USA, 2010. ACM.
[10] T. A. Lau, P. Domingos, and D. S. Weld. Learning
programs from traces using version space algebra. In
Proceedings of the 2nd International Conference on
Knowledge Capture (K-CAP 2003), October 23-25,
2003, Sanibel Island, FL, USA , pages 36{43, 2003.
[11] M. Madsen, B. Livshits, and M. Fanning. Practical
static analysis of javascript applications in the presence
of frameworks and libraries. In Joint Meeting of the
European Software Engineering Conference and theACM SIGSOFT Symposium on the Foundations of
Software Engineering, ESEC/FSE'13, Saint Petersburg,
Russian Federation, August 18-26, 2013 , pages 499{509,
2013.
[12] M. D. Network. Array.prototype. https://developer.
mozilla.org/en-US/docs/Web/JavaScript/
Reference/Global_Objects/Array/prototype .
Accessed: 2015-03-15.
[13] M. D. Network. Proxy - JavaScript.
https://developer.mozilla.org/en-US/docs/Web/
JavaScript/Reference/Global_Objects/Proxy .
Accessed: 2015-03-16.
[14] V. K. Palepu, G. H. Xu, and J. A. Jones. Improving
eciency of dynamic analysis with dynamic
dependence summaries. In 2013 28th IEEE/ACM
International Conference on Automated Software
Engineering, ASE 2013, Silicon Valley, CA, USA,
November 11-15, 2013 , pages 59{69, 2013.
[15] D. Qi, W. N. Sumner, F. Qin, M. Zheng, X. Zhang,
and A. Roychoudhury. Modeling software execution
environment. In Working Conference on Reverse
Engineering , WCRE'12, pages 415{424, Washington,
DC, USA, 2012. IEEE Computer Society.
[16] E. Schkufza, R. Sharma, and A. Aiken. Stochastic
superoptimization. In V. Sarkar and R. Bod k, editors,
ASPLOS , pages 305{316. ACM, 2013.
[17] M. Sharir and A. Pnueli. Two approaches to
interprocedural data ow analysis. Program Flow
Analysis: Theory and Applications , 1981.
[18] A. Solar-Lezama, L. Tancau, R. Bodik, S. Seshia, and
V. Saraswat. Combinatorial sketching for nite
programs. In Proceedings of the 12th International
Conference on Architectural Support for Programming
Languages and Operating Systems , ASPLOS XII, pages
404{415, New York, NY, USA, 2006. ACM.
[19] M. Sridharan, S. Artzi, M. Pistoia, S. Guarnieri,
O. Tripp, and R. Berg. F4F: taint analysis of
framework-based web applications. In Proceedings of
the 26th Annual ACM SIGPLAN Conference on
Object-Oriented Programming, Systems, Languages,
and Applications, OOPSLA 2011, part of SPLASH
2011, Portland, OR, USA, October 22 - 27, 2011 , pages
1053{1068, 2011.
[20] T.J. Watson Libraries for Analysis (WALA).
http://wala.sourceforge.net .
[21]H. Zhu, T. Dillig, and I. Dillig. Automated inference of
library specications for source-sink property
verication. In Programming Languages and Systems -
11th Asian Symposium, APLAS 2013, Melbourne, VIC,
Australia, December 9-11, 2013. Proceedings , pages
290{306, 2013.
720