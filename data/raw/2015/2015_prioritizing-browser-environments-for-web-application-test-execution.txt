Prioritizing Browser Environments for Web
Application Test Execution
Jung-Hyun Kwon
School of Computing, KAIST
South Korea
junghyun.kwon@kaist.ac.krIn-Young Ko
School of Computing, KAIST
South Korea
iko@kaist.ac.krGregg Rothermel
University of Nebraska–Lincoln
USA
grother@cse.unl.edu
ABSTRACT
Whentestingclient-sidewebapplications,itisimportanttocon-
siderdifferentweb-browserenvironments.Differentpropertiesof
these environments such as web-browser types and underlying
platforms may cause a web application to exhibit different typesof failures. As web applications evolve, they must be regression
testedacrossthesedifferentenvironments.Becausetherearemanyenvironmentstoconsiderthisprocesscanbeexpensive,resultingindelayedfeedbackaboutfailuresinapplications.Inthiswork,wepro-
pose six techniques for providing a developer with faster feedback
on failureswhen regressiontesting webapplications acrossdiffer-
ent web-browser environments. Ourtechniques drawon methods
usedintestcaseprioritization;however,inourcaseweprioritize
web-browser environments, based on information on recent and
frequentfailures.Weevaluatedourapproachusingfournon-trivial
andpopularopen-sourcewebapplications.Ourresultsshowthat
ourtechniquesoutperformtwobaselinemethods,namely,noor-
deringandrandomordering,intermsofthecost-effectiveness.The
improvement rates ranged from -12.24% to 39.05% for no ordering,
and from -0.04% to 45.85% for random ordering.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;Empiricalsoftwarevalidation ;Maintainingsoftware ;
KEYWORDS
Web application testing, Regression testing, Browser environments
ACM Reference Format:
Jung-Hyun Kwon, In-Young Ko, and Gregg Rothermel. 2018. Prioritizing
Browser Environments for Web Application Test Execution. In ICSE ’18:
ICSE ’18: 40th International Conference on Software Engineering , May 27-
June 3, 2018, Gothenburg, Sweden. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3180155.3180244
1 INTRODUCTION
Modern, client-side web applications are becoming increasingly
more complex and fault prone [ 34,40]. Faults in these applications
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180244may manifest themselves differently in various combinations of
web browsers, their versions, and underlying operating systems.
Onereasonthisoccursisthatsomebrowsercombinationsmaynot
support certain features that a client-side web application requires.
To prevent such configuration-specific faults from affecting the
usageofawebapplication,appropriatetestingisneeded.Thisispar-
ticularlytrueasawebapplicationevolves,becauseneworchangedcodecancauseunexpectedfaultsinexisting,previously-testedfunc-
tionalities.Forthisreason,techniquesforregressiontestingweb
applicationshavebeenactivelystudied.Suchtechniquesinclude
approaches for automatically generating test cases or oracles from
previous versions of web applications [ 20,34–37,45,51], repairing
test cases that are invalid after web application updates [ 21,25],
selecting subsets of or prioritizing regression test suites [ 49,59],
augmentingtestsuites[ 33],amongothers.Noneofthiswork,how-
ever, has considered the additional problems that arise when re-
gressiontestingmustbeperformedacrossdifferentweb-browser
environments (henceforth referred to as browser environments),
such as environments in which different web browsers and operat-
ing systems are utilized.
Whenadevelopermodifiesawebapplicationthatismeantto
function in different browser environments, regression testing that
web application can require a large amount of time and computing
resources. To date, more than a hundred different web browsers
exist[57].Eachwebbrowsercanhavemanydifferentversionsin
thefieldsimultaneously,andeachcanberunonvarioustypesof
operating systems on a wide range of different computing devices.
Thiscreatesproblemsofscaleforregressiontestingofwebappli-
cations [44]. Extended regression testing times delay the feedback
that can be provided to developers on failing combinations of web
browsers, and therefore, delay the debugging of failures and the
release of new versions.
In this work, to improve the process of regression testing client-
side web applications, and in particular to reduce delays in provid-
ing feedback about failures, we propose six different techniquesforprioritizing browser environments.
1Our techniques make use
of information about the failure history associated with browser
environments in prior applications of regression testing. Two of
ourtechniquesutilizerecentfailureinformation,includingtypes
of web browsers, browser versions, and of operating systems, that
is stored in a cache; test execution environments are scheduled
according to the cached information. Browser environments with
recentfailureinformationaregivenhigherprioritythanothers,and
willbetestedearly.Anothertwotechniquesarebasedoncommonly
1Our techniques should not be confused with existing approaches for prioritizing test
cases;wearefocusinginsteadonbrowserenvironmentsundertheassumptionthat
existing test cases will be run on each environment on which they are applicable.
4682018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Jung-Hyun Kwon, In-Young Ko, and Gregg Rothermel
failing browser environments, and we propose a failure-frequency-
basedapproachthatgivesgreaterprioritytobrowserenvironments
underwhichregressiontestingfailedmanytimesinpriorbuilds,
andmachine-learning-basedapproachesthatautomaticallylearn
thepatternoffailuresandpredictsbrowserenvironmentsonwhich
a web application test is likely to fail. The last two techniques
consider information on both recent failing and commonly failing
environments.Thetechniquesfirstschedulebrowserenvironments
basedoncachedinformation,andforbrowserenvironmentswith
equalpriority,applyafailure-frequency-basedormachine-learning-
based approach to order them.
Our prioritization techniques have several advantages. First, our
techniquesareapplicabletomodernwebapplications.Modernweb
applicationsarewritteninamixtureoflanguagessuchasHTML,
JavaScript, and CSS; therefore, obtaining code coverage for test
cases,whichisusuallyrequiredbytraditionaltestcaseprioritization
methods,ischallengingandexpensive.Ourtechniques,however,
do not require code coverage information. Second, our techniques
are especially effective for supporting Continuous Integration (CI)
practices.InCIenvironments,thereisashorttimeintervalbetween
runs of regression tests. Developers frequently check their code
in to the mainline codebase, and regression tests relevant to thatcode need to be performed in applicable browser environments.
Techniques for calculating code coverage cannot keep up with the
pace of change that occurs in such processes.
We empirically studied our techniques on four non-trivial, pop-
ular open source web applications. Our results show that our tech-
niques can be cost-effective. Our approaches generally detect more
failures faster than two baseline approaches, in which browser en-
vironments are not prioritized or are randomly ordered. We also
compared our six prioritization techniques, and analyzed whichtechniques are more cost-effective than the other techniques foreach experiment object. This analysis can suggest which prior-itization techniques a developer should apply first in their web
application testing. In addition, we addressed some practical issues
about using our techniques in industry, such as prioritization time
overhead and parallel test execution.
The main contributions of this work are as follows:
•Thisisthefirstworktoinvestigatetheprocessofprioritizing
browser environments for web applications.
•Weproposefournoveltechniquesforprioritizingwebbrowser
environments for regression testing, that rely on test results
from prior regression testing sessions.
•We report the results of an empirical evaluation of our tech-
niques using non-trivial, real-world web applications; our
results demonstrate the cost-effectiveness of our techniques.
•Wemakeourimplementationsanddatapubliclyavailable[ 9].
2 MOTIVATION
Tofurthermotivatethiswork,wepresenttheresultsofaninitial
investigation into whether web-browser-environment scheduling
affects the cost-effectiveness of regression testing for web appli-
cations. Forthis analysis, we consideredMootools[ 38],a popular
open-sourcewebapplicationthatmanipulatesDocumentObject
Model(DOM)elementsandhandlesevents.Toobtainweb-browserFigure 1: The effect of prioritizing web-browser environ-ments for web applications
environments,andtomeasurethetimerequiredtotesttheappli-
cationineachenvironment,weretrievedpreviousbuildhistories
of Mootools from Travis CI [ 39], where test results for Mootools
are stored. The build histories we used began on March 16, 2014and ended on February 9, 2015. We excluded build histories that
were not directly relevant to this investigation. For instance, some
buildhistoriesdonotcontaintestresultsduetocompilationfail-
ures, and some build histories do not provide information on failed
web-browserenvironments.Afterexcludingsuchbuildhistories,
ten remained.
For each build, we counted the number of web-browser envi-
ronmentstested.Themedian,minimum,andmaximumnumbers
acrossthebuildswere23.5,22.0,and24.0,respectively.Eachweb-
browser environment consists of multiple properties; these include
browser type and version, operating system and version, and build
type(“default”and“nocompat”).2Therearefivebrowsertypes,but
by varying other properties of the web browser environments, the
number of environments is increased to 24.
Onecommonstrategyforreducingregressiontestingtimeacross
execution environments is to run test suites under each environ-ment concurrently. Popular cloud-based testing services such as
SauceLabs[ 50]andBrowserStack[ 10]supportsuchapproaches,
and Mootools uses Sauce Labs. Although test suites can be run
concurrently in such cases, the total testing time required may
vary with the number of environments to be tested. If the number
of testing environments is greater than the number of resources
available for parallelization, a substantial amount of time can still
berequiredtodetectfaultsthatcanbeuncoveredonlyinspecific
environments. Increasing the number of resources available can
address this problem, but at additional costs. If there are many
web applications to manage, it becomes more important to use
computing resources in an efficient manner by preventing each
web application from consuming excessive resources. For a similar
reason, existing studies on test case prioritization emphasize the
importanceofschedulingtestcasesinaconcurrentenvironment
to improve the cost-effectiveness of regression testing [17].
Wecomparedtwodifferentschedulesofweb-browserenviron-
mentsonMootools:optimalandoriginal,andmeasuredwhether
the optimal schedule improved the cost-effectiveness of regressiontesting. The original ordering involves using the test schedule that
2Theterms “default”and“nocompat” indicatewhetherthe applicationiscompatible
or not with older versions.
469
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Prioritizing Browser Environments for Web Application Test Execution ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
isrecordedinthebuildhistoryofMootools,whereastheoptimal
schedule places all web-browser environments that were observed
to fail ahead of those that were not observed to fail. We then ex-
tractedtheexecutiontimeforeachofthebrowserenvironments
(“jobtimes”inSauceLabs)fromthebuildhistory.Next,wecalcu-
latedthetimesatwhichweb-browserenvironmentsthatcontained
at least one failing test cases were reported. We then compared the
calculated times (“feedback times”) reported for the optimal and
original schedules.
Across all ten builds, the optimal schedule provided feedback
315.12 seconds faster on average than the original schedule. Fig-
ure 1 provides a line plot showing the feedback times obtained for
each schedule when browser environments used in build ID 200
werescheduled.Theoptimalscheduleispresentedasasolidline,
and the unordered schedule as a dashed line. Across 24 browser
environments,fivefailed.TheX-axisdenoteseachfailedbrowser
environment,andtheY-axisdenotesthefeedbacktimeassociated
witheachfailedbrowserenvironment.Theoptimalschedulepro-
vided feedback 153 to 472 seconds faster than the original schedule
(68% to 86% faster). This shows that techniques for scheduling web
browser environments do have the potential to improve feedback.
3 TECHNIQUES FOR PRIORITIZING
WEB-BROWSER ENVIRONMENTS
In this section, we present techniques for prioritizing web-browser
environments for use when regression testing web applications.
Wemodifytheformaldefinitionofthetestcaseprioritizationprob-
lem[46]toformallydefinetheproblemofprioritizingweb-browser
environments as follows:
Definition 1: Problem of prioritizing test-execution environments:
Given:E, a list of web-browser environments, PE, the set of
permutations of E, and f, a function from PEto the real numbers.
Problem: Find E/prime∈PEs.t. (∀E/prime/prime)(E/prime/prime∈PE )(E/prime/prime/nequalE/prime)
[f(E/prime)≥f(E/prime/prime)]
InDefinition1, PEreferstoallpossibleorderingsof E,and frepre-
sents an objective function used to quantify a goal of prioritization.Inthiswork,ourgoalistoincreasetherateoffaultdetectionofthe
testsuitesforawebapplicationunderacertainorderingofweb-
browser environments. Different objective functions are defined
and used for different prioritization techniques.
In this work we consider eight different techniques for ordering
web-browser environments overall. The first two techniques repre-
sent cases in which no heuristic is applied; these serve as baseline
orderings to compare results against, as follows:
M1:No prioritization. The web-browser environments are not
prioritized; they retain the ordering utilized by developers initially.
M2:Random prioritization. The web-browser environments are
ordered randomly.
Thesixothertechniqueswepresentcanbecategorizedintothree
classes: techniques that consider recently failing web-browser envi-
ronments,techniquesthatconsider frequentlyfailing environments,
and techniques that consider both. We describe these next.
M3:Exact matching-based prioritization. This technique con-
siders recently failed web-browser environments first. The tech-
nique is based on the assumption that recently failed web-browser
environments may fail again on future builds of a web application.Build type Browser name Browser version OS
production safari 5 OS X 10.6
ID Build type Browser nameBrowser 
versionOS Test result Cache hit
1 development Internet Explorer 10 Windows 7 Pass 0
2 development Safari 5 OS X 10.6 Fail 03 production Internet Explorer 10 Windows 7 Pass 0
4 production Safari 5 OS X 10.6 Fail 1Cache
Testing environments
Prioritization result : 4 12  3
Figure 2: Example of matching browser environments
Assuch,thetechniqueisbasedonpriorworkthatconsidersrecent
failure information [ 17,30]. To utilize recent failure information in
prioritizing web-browser environments we use a cache. A cache
lists the environments for which previous builds failed. If web-browser environments in the current build are contained in the
cache, a higher priority is assigned to those environments.
We formally define a cache-hit as follows:
TE={C1,C2, ...,Cn}
Build i⊂TE
Cache it⊂TE
Cm=(x1,x2, ...,xk)
Cache_hiti=Build i∩Cache it⎫⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭(1)
Here, TEis a set of web-browser environments that a developer is
considering for regression testing. Build irepresents a subset of TE
at build i.Cache itrefers to the subset of TEthat exhibited failures
whenmovingfrombuild i−1−ttobuild i−1.tisathresholdthat
specifies thebuild range, and abrowserenvironment inthe range
canbecached.If i−1−tislessthan1,thevalueissetto1.Each
browser environment ( Cm)i nTEis a vector containing properties
ofbrowserenvironments.Forexample,Figure2illustrateshowthe
cache-hitoccurs.Aweb-browserenvironmentinthefigureconsists
of four properties: Build type, Browser name, Browser version and
OS. Each property in the vector takes a value from a set of possible
values. Cache_hitirefers to the intersection of Build iandCache it.
InFigure2,asinglebrowserenvironmenthitsthecache.Thecache-
hit browser environment receives a higher priority than the other
environments. Thebrowser environmentsare scheduledbased on
priorityscoreinadescendingorder.Iftheprioritiesoftwobrowser
environmentsarethesame,theycanbescheduledintheoriginal
order or randomly. In Figure 2, the priority of the last browser
environment is1 andthe prioritiesof theother environmentsare
0; therefore, the scheduling result becomes “4, 1, 2, 3”.
An advantage of the cache-based approach is that it is computa-
tionallylight-weight,andthereforeissuitableforfastdevelopment
environmentssuchasCIenvironments.Adisadvantageofusing
the cache-based approach, however, is that the performance of the
approach could be poor if a web application does not satisfy our
assumption that a recently failed browser environment is likely to
failagaininnearfuture.Asecondpotentialdisadvantageisthattheapproachmaynotbecost-effectiveinsituationsinwhichtestingof
the previous build fails on most browser environments, but testing
in the next build fails on few of the environments. Because there is
no additional priority considered among cache-hit browser envi-
ronments, the cost-effectiveness of this approach may suffer if the
470
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Jung-Hyun Kwon, In-Young Ko, and Gregg Rothermel
failedenvironmentsarenotdetectedbeforemostofthecache-hit
browser environments have been tested. A third potential disad-
vantage is that it is possible that only some properties in a browser
environment are relevantto predicting failed environments in the
nextbuild,andthecache-basedapproachdoesnotconsidersuch
cases.Instead,itrequiresallpropertiesofabrowserenvironment
to be matched.
M4:Similaritymatching-basedprioritization. Oursecondtech-
niqueattemptstoaddressthesecondandthirddisadvantagesofthe
exact matching technique. Instead of assigning the same priorities
to cache-hit web-browser environments, this technique calculates
the similarity between each environment in the build under test
and each environment in the cache. The priority of a browser envi-
ronment inthebuild undertest isthe averagesimilarity, whichis
calculated by summing up the similarity values between that envi-
ronmentandeachenvironmentinthecache,dividedbythenumber
of cached browser environments. Then, the browser environments
arescheduledbasedontheirsimilarityscoresinadescendingorder.
To measure similarity in this context, we use the Jaccard similarity
coefficient [ 58]. This assigns a floating number between 0 and 1
to a browser environment. In comparison to the exact matching
technique,thisreducesthenumberofcasesinwhichbrowserenvi-
ronmentshavethesamepriority.Inaddition,thisassignsapositive
prioritytobrowserenvironmentsthatdonotexactlymatchthose
inthecache.Forexample,evenifabrowserversioninabrowser
environmentdoesnotexistinthecache,theenvironmentcanre-
ceive a positive priority if other features such as the browser name
and operating system are in the cache.
We use Equation 2 to measure the similarity between a browser
environmentinabuildundertestandanenvironmentinthecache.
The notations are the same as those used in Equation 1. Simij
represents the similarity score for the jth browser environment in
Build i.Inaddition,foreachvectorin Cache it,theJaccardsimilarity
coefficient is calculated by finding the norm of the intersectionbetweenavectorfrom
Cache it,andavectorfrom Build idivided
bythenormoftheir union.TheJaccardsimilaritycoefficientsare
then averaged.
Simij=1
|Cache it|/summationdisplay
Ck∈Cache it|Build ij∩Ck|
|Build ij∪Ck|⎫⎪⎬⎪⎭(2)
For the browser environments shown in Figure 2, using our
approach, the similarity scores calculated for the environments are
0/8=0,3/5=0.6,1/7=0.14,and4 /4=1,respectively.Notethat
whenusingtheexactmatchingmethod,browserenvironments2
and 3 have priority 0. When using the similarity matching method,
however,theyhavepriorityvalues0.6and0.14,respectively,and
therefore the prioritization result becomes “4, 2, 3, 1”. If this added
level of distinction is useful, it may render the similarity-matching
technique more cost-effective than the exact matching technique.
M5:Failure-frequency-basedprioritization. Onewayinwhich
toconsiderfrequentlyfailedweb-browserenvironmentsistocount
the number of previous failures for each browser environment.
Thismethodisbasedontheassumptionthatfrequentlyfailedweb-
browser environments are likely to fail again in future builds. If
testing against a web application fails more often in a certain web-
browser environment than in other environments, this method
can be effective for predicting environments that will fail in futurebuilds. Thus, this method gives higher priority to browser environ-
ments that have greater numbers of previous failures. To schedule
browserenvironmentsusingthismethod,weneedtohaveakey-
value data structure in which the key is a browser-environment
vector, Ci, and the value is the number of previous builds in which
the environment failed.
One issue regarding this technique is that using all the prop-
erties of a browser environment to count failure frequency may
decrease the cost effectiveness of the browser-environment sched-
uling. For example, when a browser environment consists of the
browser name, browser version, and OS name, using a subset of
thosepropertiessuchasthebrowsernameandOSnameasthekey
and the number of the failed browser environments containing the
keyasthevaluecanresultinhighercosteffectiveness.Itisdifficult,
however, for a developer to manually determine which subset ofthe properties might be effective to render scheduling more costeffective. As the number of properties in a browser environment
increases, this issue may become more important.
M6:Machine-learning-basedprioritization. Toaddresstheprob-
lems of the failure frequency method, a machine learning (ML)method may be useful. An ML method can automatically learn apatternoffailuresfromthepreviousbuildhistory,andprovidea
failure probability for browser environments in subsequent builds.
Here, we use Bernoulli Naive Bayes classifier, which calculates fail-
ure probabilities based on Bayes’ theorem [ 4]. The properties of
thebrowserenvironmentsinthisstudyarediscretedata,andthe
Bernoulli Naive Bayes classifier is suitable for such data.
A classifier is built by learning test history from previous builds
in which some but not all browser environments failed. The classi-
fierreturnstheprobabilityoftestfailuresunderagivenbrowser
environment.Theclassifieradaptivelyupdatesitslearningmodel
once the testing of a build is finished and has failed on some but
not all browser environments. The priority score for each browser
environmentisequaltotheoutputoftheMLmodel,theprobabilityoftestfailuresundertheenvironments.Thebrowserenvironments
are then sorted based on priority scores in descending order.
Equation3showshowtocalculatetheprobabilityoftestfailures
under a given browser environment. Drefers to a set of browser
environmentsand kreferstothenumberofpossiblebrowseren-
vironments. Tis a set of test results, which are either “Pass” or
“Fail”. P(Fail|Di)is the probability of test failures under a given
browser environment, Di. Using Bayes’ theorem, the posterior,
P(Fail|Di), can be calculated by a likelihood, P(Di|Fail ), times the
prior, P(Fail ),dividedbytheevidence, P(Di).P(Fail )orP(True )
can be obtained from either a uniform distribution or a domain
expert, P(Di|Fail )orP(Di|True )canbecalculatedfromthetestre-
sultsfromthepreviousbuilds,and P(Di)canbecalculatedfromthe
priorresultsandlikelihood.Notethatthistechniqueordersbrowser
environments based on the posterior probability, so a threshold of
the posterior probability is not needed.
D={D1,D2, ...,Dk}
T={Pass ,Fail}
P(Di)=/summationtext
j∈TP(Di|Tj)×P(Tj)
P(Fail|Di)=P(Di|Fail )×P(Fail )
P(Di)⎫⎪⎪⎪⎪⎪⎪⎪⎬⎪⎪⎪⎪⎪⎪⎪⎭(3)
471
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Prioritizing Browser Environments for Web Application Test Execution ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
One potential advantage of the ML-based method is that it pro-
videsmoresophisticatedschedulingthanthecache-basedorfailure-
frequency-basedmethods.Thus, eachpropertyofa browserenvi-
ronmentcanbeusedinanadaptivemannertopredictfailure-prone
environments. A second advantage is that the approach is suitable
forfastdevelopmentenvironments,becauseadeveloperdoesnot
need to create a new learning model whenever a new build is con-
ducted.Instead,theyneedonlyupdatetheexistingmodelbasedon
thenewtestingresultsthataregeneratedfromthecurrentbuild.ApotentialdisadvantageoftheML-basedapproachisthatitassumesthere exist frequently failed browser environments or properties; if
thisassumptionisnotsatisfied,theperformanceoftheapproach
may be compromised.
M7andM8:Hybrid prioritization. Techniques may perform dif-
ferentlyacrossapplications,soitcouldbedifficultforadevelopertodecide which technique to use. It is possible to combine techniquestoreducethesedifferences.Thehybridprioritizationtechniqueuses
two prioritization criteria, so browser environments are ordered
based on the first criterion, and if more than one environment has
thesamepriority,thehybridprioritizationusesthesecondcriterion
to order those environments.
Wechosetheexactmatching-basedprioritizationtechniqueas
the first criterion because the priority score of the exact matching-
based technique is either zero or one, so it is more likely for the
technique to have more browser environments with the same pri-
ority than the other techniques. The second criterion can be either
the failure-frequency-based or ML-based techniques. We define
M7as a prioritization technique that combines the exact matching-
basedtechniquewiththefailure-frequency-basedtechnique,and
we define M8as a prioritization technique that combines the ex-
actmatching-basedtechniquewiththeML-basedtechnique.One
advantageofthehybridprioritizationtechniquesisthattheycan
considerbothrecentlyfailedbrowserenvironmentsandfrequently
failed browser environments.
Table 1 shows the code, mnemonic and description of the priori-
tization techniques that we introduced in this section.
Table 1: Prioritization Techniques
Code Mnemonic Description
M1untreated no ordering
M2 random random ordering
M3exact-matching exact match-based
M4similar-matching similar match-based
M5 fail-freq failure frequency-based
M6 ML machine learning-based
M7 M3+M5exact-matching plus
fail-frequency
M8 M3+M6exact-matching plus ML
4 EVALUATION
Weconductedanempiricalstudytoinvestigatetheeffectivenessof
the foregoing techniques.
4.1 Objects of Study
Forthisstudy,weselectedfourwebapplications.Theseapplications
wereallobtainedfromthe“popularpackage”sectionofbower.io[ 8],Table 2: Objects of Study and Build Information
Lines
of CodeBuild
PeriodBuildsFailed
BuildsMedian
BEs
Backbone 121415/2/20-
16/10/273840 105 23
Bootstrap 238414/1/31-
16/11/2411219 58 12
Lodash 589313/11/05-
16/10/10520 227 99
Underscore 168315/2/20-
16/10/271546 50 23
which is a repository of client-side JavaScript packages. Table 2
provides details on the study objects as of October 26, 2016, includ-
ing lines of code, time period of collected build history, number
ofbuilds,numberoffailedbuilds,andthemediannumberofweb
browser environments (obtained via processes described below).
Backbone [1] is a JavaScript framework that supports the use of
Model-View-Controllerpatternsinwebapplicationdevelopment.
Bootstrap [5] is an HTML, CSS and JavaScript framework for de-
veloping responsive and mobile web applications. Lodash[28]i sa
JavaScriptutilitylibrarythatprovidesvariousfunctionsforimprov-
ing modularity and performance, among others. Underscore [56]
is a JavaScript utility library that provides help functions for func-
tional programming. We excluded “Mootools,” which was used for
ourinitialinvestigationdescribedinSection2,becauseithastoo
few builds available to support statistical analysis.
WemeasuredlinesofcodeusingCLOC[ 11],whichcountsthe
lines of code excluding blank and comment lines. We collectedall previous build information for each object from Travis CI, a
cloud-basedcontinuousintegrationservice[ 54].Foreachbuild,unit
testingisconductedinatestingenvironment.QUnit[ 43]istheunit
testing framework that is used for the objects. A cloud-based cross-
browser testing service, Sauce Labs, is used to conduct regression
testing of the objects in each testing environment. Sauce Labs
considerstestinginatestingenvironmenttobeajob,sothenumber
ofjobsisequaltothenumberofweb-browserenvironments.For
all objects except Lodash, the job IDs are recorded in build logs
obtainedfromTravisCI.GivenjobIDs,wewereabletoobtaintest
resultsforeachtestingenvironmentusingtheSauceLabsREST
API.In Lodash,passedjobIDsarenotrecordedinbuildlogs,but
property names of the web-browser environments and unit testing
resultsarerecordedinthelogs.InTable2,the“BuildPeriod”and
“Builds” columns showthe period and number of buildson which
regression testing is performed for each object.
Prioritizationtechniquesareappliedtobrowserenvironments
following the occurrence of a failed build. In this study, we exclude
builds that have passed in regression testing for all browser en-
vironments, and builds that have failed in regression testing for
allbrowserenvironments.The“FailedBuilds”columninTable2
indicates the number of builds that we actually utilized.
Thenumberofbrowserenvironmentsconsideredinagivenbuild
canchangeacrossbuilds,becausesomebrowserenvironmentsmay
be removed and some new ones (e.g., using new web browsersor browser versions) may be added. The “Median BE” column in
Table2indicatesthemediannumberofbrowserenvironmentsthat
472
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Jung-Hyun Kwon, In-Young Ko, and Gregg Rothermel
aretestedacrossallbuildsforourobjects. Lodashhasthelargest
numberofbrowserenvironmentsbecausethenumberofproperties
in its browser environments (five) is larger than that for the other
objects (all of which have three). Specifically, Lodashadds module
nameswithinformationonrelevantmodules,andabuildtypesuch
as “development” or “production”.
4.2 Variables and Measures
4.2.1 Independent Variable. Our independent variable involves
prioritization techniques. We evaluated eight techniques; these
correspond to the two baseline techniques ( M1andM2) and the
six heuristics ( M3–M8) described in Section 3. The ordering in
which browser environments were tested in practice for each web
applicationwasobtainedfromtheTravisCIlogdata;weregard
this ordering as the original schedule.
4.2.2 Dependent Variable. Tomeasuretheeffectivenessofpri-
oritization techniques at improving the rate of failure detection,
weused theAveragePercentageFaults Detected(APFD C)metric;
this metric is typically used to measure the cost-effectiveness ofprioritizing test cases [
15] when those test cases have different
costs (execution times). We used this metric because we can re-gard a browser environment as a test case; therefore, schedulingbrowser environments will produce effects similar to those seen
when prioritizing test cases.
The equation for APFD Cis as follows:
AP F D C=/summationtextm
i=1(fi×(/summationtextn
j=Citj−1
2tCi))
/summationtextn
i=1ti×/summationtextm
i=1fi
Here, nisthenumberofbrowserenvironmentsand misthenumber
of failures. Cirefers to the position, in the list of browser environ-
ments considered, of the browser environment that detects the ith
failure. tjdenotes the cost of performing regression testing on a
Cj, and firefers to the severity of ith failure. The more effective a
prioritization technique is, the closer the APFD Cvalue is to 100%.
Fromthelogdataforeachapplicationwewereabletodetermine
whenregressiontestingoneachbrowserenvironmentbeganand
ended;thisyieldsameasureof jobtime.Inthisstudy,forallapps
otherthan Lodash,For Lodash,jobtimesforbuildsarenotavailable,
so we set tjto 1, which implies treating all builds as being equally
costly.Wewereunabletoassesstheseverityoffailures,however,
and therefore we set fito 1 for each failure.
4.3 Study Procedure
We simulate regression testing on different browser environments
and with different orders of environments using the log data for
Travis CI and Sauce Labs. The procedure was as follows:
(1)Buildthecache,datastructureforfailurefrequency,andML
model based on the test results of the first-failed build.
(2)Prioritize the browser environments in the current build
based on the priority produced by each technique.
(a)Ifmorethanonebrowserenvironmenthasthesamepri-
ority, order them randomly.
(3)Measure the APFD Cvalue of the result of the regression
testing session on the current build.
(4) Repeat (2) and (3) 30 times; average the APFD Cvalues.(5)Updatethecache,datastructureforfailurefrequency,and
ML model based on the test result from the current build
when regression testing on that build fails on some but not
all browser environments.
(6) Repeat Steps (2) – (5) for the next failed build.
We performed the foregoing process for each of the eight priori-
tizationtechniquesconsidered,withtwoexceptions.Forrandom
ordering,Step(2)isperformedviasimplerandomizationusinga
differentorderingoneachofthe30runs,andfornoordering,Steps
(2)-(a) and (4) are skipped. We performed 30 runs for techniques
other than no ordering (Step (4)) because we used a random order
wheneverthereisatie(inStep(2)-(a)),whichrenderstechniques
non deterministic. We could have used no ordering instead of ran-
domordering;however,thereisnoevidencethattheoriginalorder
is meaningful.
We adaptively update the ML model online, so training data
consists of the properties of the failed browser environments in
previousbuildsandtheirtestresults,andtestdataistheproperties
of the browser environments under which tests are performed.
4.4 Threats to Validity
External validity: The web applications that we study are widely
used and popular open source projects, regression tested across
morethan10browserenvironments.Theircodesizesrangefrom
1214to5893lines,whichplacesthemassmallandmid-sizeappli-
cations. As such, the applications cannot represent all classes of
webapplications,especiallylarge-scalewebapplicationssuchas
on-line shopping and game applications. However, we were unable
tofindotherwebapplicationsthatpubliclyprovidetestfailuredata
relative to cross-browser testing.Internal validity:
We have implemented the recent-failure-based
methods and failure-frequency-based method by ourselves, and
therefore there is a possibility that our implementations do not
work as intended. To reduce this threat, we conducted a manual
verification process with a small number of builds to determine
whetherresultsinthosecaseswerecorrect.Anotherthreattoin-
ternalvalidityisthatweconductedourrunsonacloudserver,and
this may allow jobs to be interrupted, causing measures of time to
differ. However, Sauce Labs creates a new virtual machine having
the same computing resources for each job, and each job performs
regression testing on an application with each browser environ-
ment.Inthiscontext,aregressiontestruncannotbeinterrupted
by another regression test run within the cloud server.Construct validity:
We used APFD Cto measure prioritization
techniquecost-effectiveness.Weassumethattheseverityofeach
failureisthesame,butincasesinwhichitdifferscost-effectiveness
resultscoulddiffer.Weconsider everydetectionofafailurewhen
calculating APFD C, even when a failure may have previously been
detected;thisisnecessary,however,becauseitisdifficulttoaccu-
rately assess (in this experiment, or in practice) whether a given
failureis“thesame”asapreviouslyencounteredfailure.Wedonot
accountforpotentialchangesinAPFD Cvaluesthatmayoccurif
faultsarefixedduringatestingprocess.Weusejobtimetorepre-
sent the cost of regression testing within a testing environment.
Jobtimeincludesexecutiontimeoftestsuitesandtimeforgener-
atinglogsandmetadatawhichhelpadevelopertodebugfailures.
473
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Prioritizing Browser Environments for Web Application Test Execution ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Figure 3: APFD Cfor each web application and prioritization technique
We considerjobtimealoneas ourmeasurefortesting cost,butin
practice,testingcostmayincludeadditionalfactorssuchastime
spent by humans interpreting results.
4.5 Results and Analysis
Figure 3 provides data on the APFD Cvalues obtained using the
prioritizationtechniquesandbaselinetechniquesweconsidered,
per web application. The Y-axes indicate APFD Cvalues, and the
X-axes denote techniques using their abbreviated names. For each
application, for each technique, the box represents the distribu-tion of APFD
Cvalues obtained across all the runs involving that
technique and application.
Inspection of the boxplots suggests that for all four applications,
thefourbasicprioritizationtechniques outperformedthebaseline
approaches(intermsofmedianperformance)inalmostallcases.
TheexceptionsinvolveM3comparedwithM2onBootstrap,and
M3 compared to M1 on Lodash. Across all techniques and web
applications,themedianimprovementratesrangedfrom-12.24%
to 39.05% for no ordering, and from -0.04% to 45.85% for random
ordering.
The techniques that achieved the highest median APFD Cvalue
are the exact matching-based prioritization technique (M3) on
Backbone , and the ML-based technique (M6) on Bootstrap , and
the hybrid technique using the ML-based technique (M8) on theother applications. The lowest median APFD
Cvalues among the
heuristicsoccurredwhenusingtheML-basedtechnique(M6)for
Backbone ,the exactmatching-based prioritizationtechnique (M3)
forBootstrap andLodash, and the similarity matching-based pri-
oritizationtechnique(M4)for Underscore .Forboth Backbone and
Underscore ,alltheheuristicstendedtoachievehighAPFD Cval-
ues(morethan88%medianAPFD C).For Bootstrap andLodash,
the exact matching-based prioritization technique (M3) displays
the lowest median APFD Camong the heuristics, but after combin-
ingitwiththetechniquesthatconsiderfrequentlyfailedbrowser
environments (M7, M8), the median APFD Cvalues increased by
25.82% and 22.12% on average, respectively.
TodeterminewhethertheAPFD Cdifferencesweobservedare
statisticallysignificant,weperformedaone-wayANOVAtestusing
PythonSciPy[ 52].Ournullhypothesiswas: H0:alltheprioritiza-
tion techniques have the same APFD Cmeans, and because resultsTable 3: Pairwise Tests on Technique Pairs
Backbone Bootstrap
Grouping MeanTechnique Grouping MeanTechnique
A93.80 M3 A68.35 M7
A93.61 M8 A68.20 M6
A93.52 M7 A67.78 M8
A93.45 M4 A66.63 M5
B91.22 M5 A62.78 M3
C85.24 M6 A59.18 M4
D69.48 M1 B50.05 M2
E50.10 M2 B42.77 M1
Lodash Underscore
Grouping MeanTechnique Grouping MeanTechnique
A79.07 M8 A89.41 M8
A, B 76.15 M6 B88.48 M6
A, B 75.10 M7 B88.23 M7
B72.05 M4 C86.21 M3
C70.91 M5 C86.02 M4
D67.02 M3 C85.91 M5
D66.77 M1 D73.07 M1
E49.84 M2 E49.11 M2
vary somewhat widely across web applications, we applied the test
to the results on a per-application basis. The ANOVA test showsthat the p-value for each application was less than 0.001, so we
wereabletorejectthenullhypothesisforeachoftheexperiment
objects.TheresultsofthisANOVAtestindicatethatatleastone
of the prioritization techniques produced statistically significantly
different mean APFD Cvalues.
Todeterminewhichpairsoftechniquesdiffer,weappliedpair-
wisetests.BecausetherateofType1errors(incorrectlyrejecting
the null hypothesis) increases when conducting two-sample t-tests
multiple times, we corrected the significance level using Bonfer-roni correction [
14]. After the pairwise testing, we grouped the
techniquesthat arenotstatisticallysignificant intothesamegroup.
Table 3 shows the relationships between techniques obtained by
the foregoing process, per program. Techniques are listed in terms
of descending order of APFD Cmeans, and shared grouping letters
indicate cases in which techniques do not statistically significantly
474
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Jung-Hyun Kwon, In-Young Ko, and Gregg Rothermel
Figure 4: Distribution of ratio of failed environments to all
environments in a build
differ. For all objects, the grouping of random ordering (M2) dif-
fers from that of all prioritization heuristics. For all objects except
Lodash, the grouping of no ordering (M1) differs from that of all
prioritization heuristics. (The exact match-based technique (M3) in
Lodashbelongs to the same group as no ordering.)
Basedontheresultofthepairwisetesting,iftheseresultsgen-
eralize, we would suggest that techniques belonging to the first
grouping (A) be the first choices for use in practice; however, these
differ across web applications and practitioners would need to use
results from initial runs to select techniques to use longer term.
OnBackbone , techniques that consider recently failed browser en-
vironments(M3andM4)andhybridtechniques(M7andM8)are
such techniques.For Bootstrap , any prioritization heuristics can
beused.For Lodash,theML-based(M6)andhybridtechniquesare
suchtechniques,andfor Underscore ,thehybridapproachusing
theMLtechnique(M8)istheonlysuchtechnique.Thatsaid,acrossalltheobjects,thehybridapproachusingtheMLtechniquealways
belongs to the first group, so this technique might be the one toconsider first when a new web application must be tested across
many browser environments.
5 DISCUSSION
We now present the results of additional analyses of our results.We analyze the ratio of failed browser environments and details
about test cases and causes of failures. We also consider several
other issues such as parallel test execution and prioritization time.
Ratio of failed browser environments in a build. We ana-
lyzed the ratio of failed browser environments to all browser envi-
ronments tested for each build of each experiment object. Figure 4
shows the result. We found that the ratio of failed builds to total
buildswaslessthan20%inallcases(9.64%for Backbone ,13.41%for
Bootstrap , 19.01% for Lodashand 18.12% for Underscore ). This
resultshowsthatthere isroomforprioritizationofbrowserenvi-
ronments to provide faster feedback to developers.
Details on test cases and causes of failures. Toidentifythe
causes of test failures in browser environments and determine
whetherfailuresareenvironment-specific,weanalyzedthenumber
of failed unit test cases for each failed browser environment and
the overlap among failed test cases between environments. In thisanalysis,wefocusontestcasefailures,notfaults.Thisisbecauseit
is difficult and error prone in many cases to trace failures to faults,
inpartbecausedevelopersseldomexplicitlyreportenvironment
failures in issue reports.
From the test logs for Backbone andUnderscore , we extracted
the number of total and failed test cases, and the names of failedtest cases, for each browser environment. For
Bootstrap , failed
test cases are not described in the logs, but we were able to obtain
information about six builds from unit test reports in the Sauce
Labswebsite.For Lodash,thenumberoftotaltestcasesisnotin
the logs, so we extracted the number including other information
on 160 builds from the unit test reports.
Thenumberoffailedtestcasesissmallforallobjects.Theper-
centage of failed test cases to total test cases ranged from 0.10% to
1.14%.Theaveragenumberoftotalandfailedtestcaseswere399.67
and 2.07 for Backbone , 341.96 and 3.89 for Bootstrap , 3173.70 and
3.32 for Lodash, and 211.49 and 1.47 for Underscore.
To calculate overlap among failed test cases between environ-
ments, wecalculatedJaccardsimilaritycoefficients betweenpairs
ofbrowserenvironments,andaveragedthecoefficientsofallthe
pairs.Acoefficientiscalculatedbythesizeoftheintersectionof
thetwofailedtest-casenamesetsdividedbythesizeoftheirunion.
ForBackbone ,Bootstrap andUnderscore , the overlaps are low
(8%,0%and4%,respectively).Meanwhile,for Lodash,theoverlapis
high (90%). It is possible that faults in Lodashmanifest themselves
in more browser environments than faults in other objects.
We were able to find several environment-specific failure causes
by reading conversations among developers and commit messages.
Table4providesexamples.Fourcausesinvolvedimplementation;
in these cases browsers did not fully implement certain features.One cause involved a test case: even when an application does
not contain environment-specific failures, its test code can contain
anenvironment-specificfault.Twoothercausesinvolvedmobile
devices. Mobile browsers have different levels of feature support
thandesktopbrowsers,andtendtoperformworsethanbrowsers
on desktop devices. Finally, there were flaky failures.
Reducedfeedbacktimeusingtheproposedtechniques. To
determine why our prioritization techniques’ APFD Cvalues are
higher than those for the baseline techniques, we investigated how
feedbacktimeisactuallyreducedforeachfailure.Theboxploton
theleftsideofFigure5showsthereductioninfeedbacktimefor
failuresforeachoftheexperimentobjectsotherthan Lodash,when
usingtheML-basedtechnique.( Lodashisomittedbecausewedo
nothavetimeinformationforit).TheML-basedtechniqueprovidesfeedbackforeachfailurebetween0.86and1.19minutesfasterthan
whennoorderingisused,andbetween0.93and3.12minutesfaster
than when random ordering is used.
ForLodash, we estimated job time as the job time for accessible
builds(ninebuilds),andmeasuredreductioninfeedbacktimebased
on that. We estimated the job time for a browser environmentas the average job time for the browser environment with the
same module name and browser type. As a result, the reduction in
feedbacktimewasestimatedat10.77minutes.Giventhat Lodash’s
median number of browser environments is 99, we expect thatthe larger the number of browser environments, the greater the
reduction in feedback time that can be achieved.
475
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Prioritizing Browser Environments for Web Application Test Execution ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 4: Causes of Failures
Cause Cause detail
A feature is not
fully implementedIE11 does not implement string
description for Map, Setand
WeakMap objects [55]
A feature that is not
implemented is usedin test codeA test case usesArray.prototype.indexOffunction but the function is notimplemented in IE8 [2]
A feature isimplementeddifferently hasOwnProperty is a non-enumerable
property in IE8 while it is enumerablein other browsers [3]
A feature is notimplementedin a mobile browserTypedArray.prototype.subarray() is
supported from Safari on iOS 4.2 [26]
Device performanceis differentPerformance of Safari on iOS 8 isslower than desktop browser, so thebrowser cannot finish rendering anupdated page in a given time in test code [6]
Flaky failure iOS 7.1 flakiness [7]
Figure5:Left:Reducedfeedbacktimeforfailures.Right:anexample showing different feedback times in Underscore
The right side of Figure 5 presents an example showing how
faster feedback is achieved on failures for one particular buildof
Underscore . The ML-based technique detected all four failed
browserenvironmentsbytestingtheapplicationonthefirstfive
browser environments, whereas the use of no ordering did not
detect all failures.
Prioritization time analysis. Each prioritization technique
usescomputationtimetoprioritizebrowserenvironments.Recently-
failed-based techniques spend time comparing browser environ-
mentsinabuildwithcachedbrowserenvironments,andtheML-
based technique spends time learning its prediction model and
predicting the probability of failure on a browser environment. Ta-
ble5showsthetimerequiredtoapplyeachtechniqueotherthan
the hybrid techniques to browser environments. Each technique
wasabletoprioritizebrowserenvironmentsinlessthan0.03sec-
onds for each of the applications. Naturally, the more sophisticatedsimilarity-basedmatchingandML-basedtechniquesrequiredmore
timethanthelesssophisticatedexactmatching-basedandfailure
frequency-based techniques, but the times were still small. WhereTable 5: Runtime Costs of Prioritization Techniques
Backbone Bootstrap Lodash Underscore
M30.007 0.007 0.007 0.007
M40.023 0.023 0.023 0.023
M50.006 0.006 0.006 0.006
M60.012 0.012 0.012 0.012
hybrid techniques are concerned, the time required to prioritizebrowser environments can be estimated as the sum of the time
taken to run the two techniques the hybrid is composed of.
Parallel test execution. Test cases can be run in parallel in
different browser environments, and prioritizing browser environ-
ments is still necessary. In fact, Sauce Labs allows developers toset priorities for browser environments. When Sauce Labs runs
out of available virtual machines or a concurrency limit given to adeveloper is exceeded, browser environments with higher priority
take precedence over browser environments with lower priority.
To quantifythe benefitsof scheduling testenvironments in the
presence of parallel test execution, we modified the existing APFD
equation [16] as follows:
AP F D p=1−⌈C1
p⌉+⌈C2
p⌉+...+⌈Cm
p⌉
nm+1
2n
Thevariablesusedherearethesameasthoseusedearlier,withthe
exceptionof p.Here, prepresentsthenumberofparallelexecutions.
When p=1,themodifiedequationbecomesthesameastheoriginal
equation.Whenweassumethatthetestingtimeundereachbrowser
environment is the same, all the Cmvalues are reduced by ptimes
because psessionscanberuninparallel,andtestfailurescanbe
detected ptimes faster.
Wecompareoneofthehybridapproaches(M8)tonoordering
andrandomorderinginaparallelenvironment.Figure6showsthe
APFDpresults as the number of parallel executions increases. Each
greenlinewithsquaresindicatesourapproach,eachblueline(with
points denotedby stars)indicates no ordering,each redline (with
points denoted by plus signs) indicates random ordering, and each
green line (with points denoted by squares) indicates our hybrid
approach. For all the approaches, the APFD pvalues increase as the
degree of concurrency increases. In addition, the APFD pvalues for
both approaches convergeas the degree ofconcurrency increases.
Althoughthereisadifferenceindegree,ourapproachoutperforms
the baseline approach even in the presence of parallel executions.
Exact matching-based prioritization technique in Lodash.
M3 (Exact-matching) has an APFD Cvalue that is 12 percentage
pointslowerthanM1(noordering).Therearetwopossiblereasons
forthisresult.First,thenumberofpropertiesfor Lodashbrowser
environments is more than that for other applications, so it canbe more difficult to obtain exact matchings for
Lodash. Second,
forLodash, the browser version in the list of environments was
updated more often than in the other applications. According toLodash’s maintainer, their builds consider current and previous
versions of most browsers [
27]. Consequently, M3 often fails to
assign higher priority values to environments that partially match
previously failed environments. Ho wever,other techniques assign
higher priority scores regarding the partial matching.
476
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Jung-Hyun Kwon, In-Young Ko, and Gregg Rothermel
Figure 6: APFD paccording to concurrency
6 RELATED WORK
Detecting regression failures under multiple web-browser environ-
ments is closely related to detecting cross-browser incompatibility
(XBI) problems. Many researchers have attempted to automatically
detectcross-browserincompatibilityproblems[ 13,29,32,47,53]
or match compatible features across different web-browser envi-
ronments [ 48]. To date, however, this work has not specifically
considered regression testing. In addition, existing research consid-
ersXBIsattheDOMleveltoensureconsistentappearanceofHTML
pages, but does not consider XBIs at the unit-function level. It is
importanttocheckXBIsattheunit-functionlevelbecauseunittest-
ing of functionscan isolate root causesof errors more easilythan
other types of tests; moreover, for open source web applications,
functions are usually called by various other web applications.
Testing of web-browser environments is also similar to the test-
ing of configurable software. Configurable software is softwarethat can be customized by users through selections of options;
suchsoftwaremustbetestedonvariousconfigurations.Cohenet
al. [12] present a study in which a single version of a configurable
web browser (Firefox) is tested. This study quantifies the effects
of changing configurations on fault detection effectiveness andthe code coverage achieved by test suites. Qu et al. [
42] present
aregressiontestingtechniquethatprioritizesconfigurationsofa
configurable software system, and show that scheduling configura-
tionscanresultinearlierfaultdetection.Thesestudies,however,
donotconsiderfaultsinclient-sidewebapplications.Inaddition,
these approaches require code coverage information, which can be
expensive to obtain in a rapid development environment.
The issues fortesting of Software ProductLines (SPLs) are also
closelyrelatedtoissuesfortestingwebapplicationsacrossbrowser
environments. To reduce the costs of testing SPLs, researchers
haveconsideredtechniquesthatselectsubsetsofvariation-point
combinations.Oneapproachutilizescombinatorialtestdesignsthatselectonlypair-wisecombinations,therebyreducingthenumberof
combinations [ 41]. Asecond approachextends combinatorialtest
designs to apply a specific testing method to SPL models, such as afeaturemodelororthogonalvariability model[ 31].Similartoour
work,theseapproachesattempttoreducetestingcostsinsituations
where many combinations of variation points are present. Our
approach, however, focuses on scheduling browser environments.
Our prioritization techniques are based on considering recently
orfrequentlyfailingtestingenvironments.Thisideaiscloselyre-
latedtoadefectpredictionworkbyKimetal.[ 24]thatconsiders
recentfaults.Kimetal.utilizeacachetostorefault-pronesoftware
entities, locations of fixed faults, and other locations including the
locations of recently added or changed code. They show that when
10% of the source code files are in a cache, 73%–95% of faults occur
in these files, thus demonstrating the usefulness of cached infor-
mationinfaultfinding.Engstrometal.[ 18]alsostudytheuseof
cached information in regression testing. Their approach, however,
focuses on regression test selection rather than test-case priori-tization, and they do not consider the XBI problem. Some defect
predictiontechniquesusemachinelearningalgorithms;thesebuild
machine learning classifiers that automatically learn patterns in
buggyfilesorAPIs,andpredictdefectivefilesorAPIs[ 19,23].This
work does not, however, consider regression testing.
Several regression testing techniques have made use of previous
build history information [ 17,22,30]. These approaches are based
ontheideathatsometestsuitesaremorelikelytorevealfailures
than others. In particular, Elbaum et al. [ 17] use time windows,
whichare closelyrelatedto caches,totrack recenttestsuites that
revealedfailures.Inthatwork,timewindowsareusedfortestsuiteselectionandprioritization.Incontrast,ourworkconsidersrecently
and frequently failed web-browser environments.
7 CONCLUSION
Wehavepresentedcost-effectivetechniquesforprioritizingbrowser
environments when regression testing web applications, that func-
tionbyconsideringrecentlyandfrequentlyfailedbrowserenviron-ments.Wehaveempiricallycomparedourapproacheswithbaseline
approaches, including the original ordering and random orderings
of browser environments for several web applications. We show
thattherateoffaultdetection(APFD C)achievedbyourtechniques
isbetterthanthatofthebaselineapproaches,andtheimprovement
is statistically significant.
We intend to conduct additional studies on more web appli-
cations. In addition, if we can obtain information about failure
severities,wecanincorporatethisintotechniques.Todothiswe
canconvertfailureseveritytoaweightvalue,andmultipliedthe
priority score of each browser environment generated by each ap-
proach by that weight. We also plan to use other software artifacts
toschedulebrowserenvironments,suchasinformationonupdated
code.Bycombiningsuchdatawithbuildhistorydata,wehopeto
be able to create more cost-effective prioritization techniques.
ACKNOWLEDGMENTS
This researchwas supported bythe Next-Generation Information
Computing Development Program through the National Research
Foundation of Korea (NRF) funded by the Ministry of Science, ICT
(NRF-2017M3C4A7066210).
477
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. Prioritizing Browser Environments for Web Application Test Execution ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1] Backbone 2018. Backbone. https://github.com/jashkenas/backbone. (2018).
[2]Backbone 2018. Example of failure cause in Backbone.
https://github.com/jashkenas/backbone/pull/3998. (2018).
[3]Backbone 2018. Example of failure cause in Backbone.
https://github.com/jashkenas/backbone/pull/4000. (2018).
[4]Christopher MBishop. 2006. Pattern recognition. Machine Learning 128 (2006),
1–58.
[5] Bootstrap 2018. Bootstrap. https://github.com/twbs/bootstrap. (2018).
[6]Bootstrap 2018. Example of failure cause in Bootstrap.
https://github.com/twbs/bootstrap/issues/14851. (2018).
[7]Bootstrap 2018. Example of failure cause in Bootstrap.
https://github.com/twbs/bootstrap/pull/13423. (2018).
[8] Bower 2017. Bower. https://bower.io/search/. (2017).
[9]Browser environment prioritization 2018. Implementation and data.
http://webeng.kaist.ac.kr/webengpress/browser-env-prio/. (2018).
[10] BrowserStack 2018. BrowserStack. https://www.browserstack.com. (2018).
[11] CLOC 2018. CLOC. https://www.npmjs.com/package/cloc. (2018).
[12]Myra B Cohen, Joshua Snyder, and Gregg Rothermel. 2006. Testing across
configurations:implicationsforcombinatorialtesting. ACMSIGSOFTSoftware
Engineering Notes 31, 6 (2006), 1–9.
[13]Valentin Dallmeier,Martin Burger, TobiasOrth, and Andreas Zeller. 2012. Web-
mate:atoolfortestingweb2.0applications.In ProceedingsoftheWorkshopon
JavaScript Tools. ACM, 11–15.
[14]David M Diez, Christopher D Barr, and Mine Cetinkaya-Rundel. 2012. OpenIntro
statistics. Vol. 12. CreateSpace.
[15]SebastianElbaum,AlexeyMalishevsky,andGreggRothermel.2001.Incorporating
varyingtestcostsandfaultseveritiesintotestcaseprioritization.In Proceedingsof
the 23rd International Conference on Software Engineering (ICSE). IEEE Computer
Society, 329–338.
[16]Sebastian Elbaum, Alexey G Malishevsky, and Gregg Rothermel. 2002. Test
case prioritization: A family of empirical studies. IEEE Transactions on Software
Engineering 28, 2 (2002), 159–182.
[17]SebastianElbaum,GreggRothermel,andJohnPenix.2014. Techniquesforim-
proving regression testing in continuous integration development environments.
InProceedingsofthe22ndACMSIGSOFTInternationalSymposiumonFoundations
of Software Engineering (FSE). ACM, 235–245.
[18]EmelieEngström,PerRuneson,andGregerWikstrand.2010. Anempiricalevalu-
ationofregressiontestingbasedonfix-cacherecommendations.In Proceedingsof
the Third International Conference on Software Testing, Verification and Validation
(ICST). IEEE, 75–78.
[19]BaljinderGhotra,ShaneMcIntosh,andAhmedEHassan.2015. Revisitingthe
impact of classification techniques on the performance of defect prediction mod-
els.InProceedingsofthe37thInternationalConferenceonSoftwareEngineering
(ICSE). IEEE, 789–800.
[20]Sylvain Hallé, Nicolas Bergeron, Francis Guerin, and Gabriel Le Breton. 2015.
Testing Web Applications Through Layout Constraints. In Proceedings of the 8th
InternationalConferenceonSoftwareTesting,VerificationandValidation(ICST) .
IEEE, 1–8.
[21]MarkHarmanandNadiaAlshahwan.2008.Automatedsessiondatarepairforweb
applicationregressiontesting.In Proceedingsofthe1stInternationalConference
on Software Testing, Verification, and Validation (ICST). IEEE, 298–307.
[22]Jung-MinKimandAdamPorter.2002. Ahistory-basedtestprioritizationtech-
niqueforregressiontestinginresourceconstrainedenvironments.In Proceedings
ofthe24rdInternationalConferenceonSoftwareEngineering(ICSE).IEEE,119–129.
[23]Mijung Kim, Jaechang Nam, Jaehyuk Yeon, Soonhwang Choi, and Sunghun Kim.
2015. REMI: Defect prediction for efficient API testing. In Proceedings of the 2015
10th Joint Meeting on Foundations of Software Engineering (FSE). ACM, 990–993.
[24]Sunghun Kim, Thomas Zimmermann, E James Whitehead Jr, and Andreas Zeller.
2007. Predictingfaultsfromcachedhistory.In Proceedingsofthe29thInternational
Conference on Software Engineering (ICSE). IEEE Computer Society, 489–498.
[25]MaurizioLeotta,AndreaStocco,FilippoRicca,andPaoloTonella.2015. Using
multi-locators to increase the robustness of web test cases. In Proceedings of the
8thInternationalConferenceonSoftwareTesting,VerificationandValidation(ICST) .
IEEE, 1–10.
[26]Lodash 2018. Example of failure cause in Lodash.
https://github.com/lodash/lodash/commit/410969. (2018).
[27]Lodash2018.IssuereportinLodash.https://github.com/lodash/lodash/issues/2898.
(2018).
[28] Lodash 2018. Lodash. https://github.com/lodash/lodash. (2018).[29]
Sonal Mahajan and William GJ Halfond. 2014. Finding html presentation fail-
uresusingimagecomparisontechniques.In Proceedingsofthe29thACM/IEEE
International Conference on Automated Software Engineering (ASE) . ACM, 91–96.
[30]Dusica Marijan, Arnaud Gotlieb, and Sagar Sen. 2013. Test case prioritization for
continuousregressiontesting:Anindustrialcasestudy.In Proceedingsofthe29th
IEEE International Conference on Software Maintenance (ICSM). IEEE, 540–543.[31]JohnD.McGregor.2001. Testinga SoftwareProductLine. SoftwareEngineering
InstituteTechnicalReportCMU/SEI-2001-TR-022.CarnegieMellonUniversity,
Pittsburgh, PA.
[32]Ali Mesbah and Mukul R Prasad. 2011. Automated cross-browser compatibility
testing.In Proceedingsofthe33rdInternationalConferenceonSoftwareEngineering
(ICSE). ACM, 561–570.
[33]AminMilaniFard,MehdiMirzaaghaei,andAliMesbah.2014. Leveragingexisting
testsinautomatedtestgenerationforwebapplications.In Proceedingsofthe29th
ACM/IEEE International Conference on Automated Software Engineering (ASE).
ACM, 67–78.
[34]Shabnam Mirshokraie and Ali Mesbah. 2012. JSART: JavaScript assertion-based
regression testing. In Proceedings of the International Conference on Web Engi-
neering (ICWE). Springer, 238–252.
[35]Shabnam Mirshokraie, Ali Mesbah, and Karthik Pattabiraman. 2013. Pythia:
GeneratingTestCaseswithOraclesforJavaScriptApplications.In Proceedingsof
the ACM/IEEE International Conference on Automated Software Engineering (ASE),
New Ideas Track. IEEE, 610–615.
[36]ShabnamMirshokraie,AliMesbah,andKarthikPattabiraman.2015. JSeft:Au-
tomated JavaScript unit test generation. In Proceedings of the 8th International
Conference on Software Testing, Verification and Validation (ICST). IEEE, 1–10.
[37]Shabnam Mirshokraie, Ali Mesbah, and Karthik Pattabiraman. 2016. Atrina:
InferringUnitOraclesfromGUITestCases.In ProceedingsoftheInternational
ConferenceonSoftwareTesting,Verification,andValidation(ICST).IEEE,330–340.
[38] Mootools 2018. Mootools. http://mootools.net. (2018).
[39]Mootools-core in Travis CI 2018. Mootools-core in Travis CI. https://travis-
ci.org/mootools/mootools-core. (2018).
[40]Frolin Ocariza, Kartik Bajaj, Karthik Pattabiraman, and Ali Mesbah. 2013. An
empirical study of client-side JavaScript bugs. In Proceedings of the ACM/IEEE
International Symposium on Empirical Software Engineering and Measurement.
IEEE, 55–64.
[41]SebastianOster,FlorianMarkert,andPhilippRitter.2010. Automatedincremental
pairwise testing of software product lines. In Proceedings of the International
Conference on Software Product Lines. Springer, 196–210.
[42]XiaoQu,MyraBCohen,andGreggRothermel.2008. Configuration-awarere-
gressiontesting:anempiricalstudyofsamplingandprioritization.In Proceedings
of the 2008 International Symposium on Software Testing and Analysis (ISSTA).
ACM, 75–86.
[43] QUnit 2018. QUnit. https://qunitjs.com. (2018).
[44]JohnResig.2009. JavaScriptTestingDoesNotScale. (Mar2009). https://johnresig.
com/blog/javascript-testing-does-not-scale/
[45]DannyRoest,AliMesbah,andArieVanDeursen.2010. Regressiontestingajax
applications: Coping with dynamism. In Proceedings of the Third International
Conferenceon Software Testing,Verification andValidation(ICST) .IEEE, 127–136.
[46]Gregg Rothermel, Roland H. Untch, Chengyun Chu, and Mary Jean Harrold.
2001. Prioritizing test cases for regression testing. IEEE Transactions on Software
Engineering 27, 10 (2001), 929–948.
[47]ShauvikRoyChoudhary,MukulRPrasad,andAlessandroOrso.2013. X-PERT:
accurateidentificationofcross-browserissuesinwebapplications.In Proceedings
ofthe2013InternationalConferenceonSoftwareEngineering(ICSE).IEEEPress,
702–711.
[48]Shauvik Roy Choudhary, Mukul R Prasad, and Alessandro Orso. 2014. Cross-
platform featurematching forweb applications. In Proceedings ofthe 2014 Inter-
national Symposium on Software Testing and Analysis (ISSTA). ACM, 82–92.
[49]Sreedevi Sampath, Renee C Bryce, Gokulanand Viswanath, Vani Kandimalla,and A Gunes Koru. 2008. Prioritizing user-session-based test cases for web
applicationstesting.In Proceedingsofthe1stInternationalConferenceonSoftware
Testing, Verification, and Validation (ICST). IEEE, 141–150.
[50] Sauce Labs 2018. Sauce Labs. https://saucelabs.com. (2018).[51]
MatthiasSchur,AndreasRoth,andAndreasZeller.2013. Miningbehaviormodels
from enterprise web applications. In Proceedings of the 2013 9th Joint Meeting on
Foundations of Software Engineering (FSE). ACM, 422–432.
[52]Scipy. 2018. scipy.stats.f_oneway — SciPy v0.19.1 Reference Guide. (2018).Retrieved January 2, 2018 from https://docs.scipy.org/doc/scipy/reference/
generated/scipy.stats.f_oneway.html
[53]NataliiaSemenenko,MarlonDumas,andTõnisSaar.2013. Browserbite:Accurate
cross-browsertestingviamachinelearningoverimagefeatures.In Proceedings
of the 29th IEEE International Conference on Software Maintenance (ICSM). IEEE,
528–531.
[54] Travis CI 2018. Travis CI. https://travis-ci.org. (2018).
[55]Underscore 2018. Example of failure cause in Underscore.
https://github.com/jashkenas/underscore/pull/2464. (2018).
[56]Underscore 2018. Underscore. https://github.com/jashkenas/underscore. (2018).
[57]Wikipedia. 2016. List of web browsers — Wikipedia, The Free Encyclopedia.
(2016). https://en.wikipedia.org/w/index.php?title=List_of_web_browsers&
oldid=753422787 [Online; accessed 7-December-2016].
[58]Wikipedia.2017.Jaccardindex—Wikipedia,TheFreeEncyclopedia.(2017). https:
//en.wikipedia.org/w/index.php?title=Jaccard_index&oldid=788825557 [Online;
accessed 21-August-2017].
478
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Jung-Hyun Kwon, In-Young Ko, and Gregg Rothermel
[59]Yunxiao Zou, Zhenyu Chen, Yunhui Zheng, Xiangyu Zhang, and Zebao Gao.
2014. VirtualDOMcoverageforeffectivetestingofdynamicwebapplications.InProceedingsoftheInternationalSymposiumonSoftwareTestingandAnalysis
(ISSTA). ACM, 60–70.
479
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:53:28 UTC from IEEE Xplore.  Restrictions apply. 