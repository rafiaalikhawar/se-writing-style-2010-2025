A System Identiﬁcation based Oracle for
Control-CPS Software Fault Localization
Zhijian He1, Yao Chen1, Enyan Huang1, Qixin Wang1,∗,Y uP e i1,∗, Haidong Yuan2
1Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR, China
2Dept. of Mechanical and Automation Engineering, The Chinese Univ. of Hong Kong, Hong Kong SAR, China
∗Corresponding authors. Email: {csqwang, csypei }@comp.polyu.edu.hk
Abstract —Control-CPS software fault localization (SFL, aka
bug localization) is of critical importance as bugs may cause
major failures, even injuries/deaths. To locate the bugs in
control-CPSs, SFL tools often demand many labeled (“cor-
rect”/“incorrect”) source code execution traces as inputs. To label
the correctness of these traces, we must judge the corresponding
control-CPS physical trajectories’ correctness. However, unlike
discrete outputs, the boundaries between correct and incorrect
physical trajectories are often vague. The mechanism (aka oracle)
to judge the physical trajectories’ correctness thus becomes a
major challenge. So far, the ad hoc practice of “human oracles”
is still widely used, whose qualities heavily depend on the human
experts’ expertise and availability. This paper proposes an oracle
based on the well adopted autoregressive system identiﬁcation
(AR-SI). With proven success for controlling black-box physical
systems, AR-SI is adapted by us to identify the buggy control-
CPS as a black-box. We use this identiﬁcation result as an oracle
to judge the control-CPS’s behaviors, and propose a methodology
to prepare traces for control-CPS debugging. Comprehensive
evaluations on classic control-CPSs with injected real-life and
artiﬁcial bugs show that our proposed approach signiﬁcantly
outperforms the human oracle approach in SFL accuracy (recall)
and latency, and in oracle false positive/negative rates. Our
approach also helps discover a new real-life bug in a consumer-
grade control-CPS.
Keywords -Oracle; Cyber-Physical System; Debug; Testing.
I. I NTRODUCTION
Control Cyber-Physical Systems (CPSs), aka control-CPSs,
are growing rapidly due to the inevitable convergence of
computer (i.e. cyber) and physical systems [1][2][3]. Typical
control-CPSs include avionics, vehicles, robotics etc. Many
control-CPSs are life/mission critical, hence faults can cause
major failures, even injuries/deaths [4][5][6][7][8][9][10].
Over the years, faults caused by physical subsystems
noises/inaccuracies are well taken care of by modern control
theories [11][12]. On the other hand, modern control-CPSs’
cyber subsystems scale up rapidly. They no longer just in-
volve numerical computations of core control laws, but also
middleware, libraries (e.g. for computer vision) etc. and can
total millions of lines of source code. How to help removing
software faults (aka “ bugs ”) from these cyber subsystems is
becoming a bigger challenge, and is the focus of this paper.
A critical step for removing bugs (aka “debugging”) is to
locate bugs in the source code, aka software fault localization
(SFL). As software complexity scales up, manual SFL no
longer sufﬁces. Over the years, many automatic SFL toolsare developed. According to a recent survey [13], there are
over 331 papers on SFL. The corresponding tools can be
categorized into 8 families, respectively based on program
spectrum, statistics, machine learning, data mining, program
slice, program state, model, and others. Particularly, with the
recent rise of big data technology, many modern mainstream
SFL tools and over 38% of all the tools surveyed (particularly
those of the program spectrum, statistics, and machine learning
tool families) need a large number of labeled (as “correct” or
“incorrect”) source code execution traces (aka “ code traces ”)
as input. Manually labeling so many code traces is no longer
practical. This leads to a compelling need for an automatic
judgement mechanism, aka oracle , to do the labeling.
However, how to design an oracle and the corresponding
code trace preparation methodology is a well-known hard
problem: i.e. the so-called oracle problem [14][15][16][17].
Solutions to the oracle problem are highly application domain
dependent [14][18]. For some application domains, the oracle
problem is still open [14][19][20][21].
Control-CPS is one such domain, where the oracle problem
faces unique challenges. Unlike the clean-cut discrete output
of pure cyber systems, control-CPS outputs are continuous
physical trajectories . At the ﬁrst look, multiple physical tra-
jectories in an envelope can seem correct (see Fig. 1). To
exactly decide which one(s) is(are) correct, an ideal black-
box oracle approach would need a known a priori bug-free
physical trajectory, i.e. the expected physical trajectory .T o
predict the expected physical trajectory, however, the approach
needs an emulation of the control-CPS upon a bug-free cyber
subsystem and a correctly simulated physical subsystem1. This
contradicts the fact that our cyber subsystem is to be debugged.
(a) Is/are arc A →Bo rC→D
correct?(b) Is/are the dotted or the dashed
curve correct?
Fig. 1: Control-CPS oracle problem is hard, e.g. in quadcopter
autopilot, how to tell which physical trajectories are correct?
Replacing the prerequisite of the “bug-free” cyber subsys-
1Usually, the physical subsystem model is available and simple enough.
Hence, we assume that the physical subsystems can be correctly simulated.
1162019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 ©2019 IEEE
DOI 10.1109/ICSE.2019.00029
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. tem with a substitute cyber subsystem implementation (e.g.
those generated by Matlab/Simulink [22]) cannot solve the
problem. This is because a simpliﬁed substitute implementa-
tion (e.g. a Matlab/Simulink implementation of the core con-
trol laws) would not catch all the subtleties (those caused by
the large additional software modules, such as middleware and
libraries besides the core control law computation) needed by
debugging; while a comprehensive substitute implementation
costs too much human effort (hence contradicts our goal of
SFL automation), and may itself needs debugging.
Therefore, the ideal black-box oracle approach is unlikely to
work. Meanwhile, speciﬁcations inferred by program analysis
[23][24][25][26][27] can neither be used as oracles. This is
because the speciﬁcations inferred must comply with existing
source code and program behaviors, including the buggy
source code and behaviors. Thus speciﬁcations found before
debugging will regard buggy behaviors as normal, hence fail
as oracles.
Due to the above reality, human oracles are still widely
used in control-CPS SFL [28][14][29][30][31][32][33]. Typ-
ically, a group of human experts are convened to discuss
and manually design a set of assertions to judge physical
trajectory correctness with best-effort. These assertions are
the so-called “ human oracles ”. Apparently, the human oracle
approach is fundamentally ad-hoc: it heavily depends on the
human experts’ expertise and availability.
To improve, this paper aims to ﬁnd another approach to
deterministically and automatically generate better oracles for
a large category of control-CPSs’ SFL. To achieve this, we
notice a key domain feature :when noises are under control
(e.g. in simulated physical subsystems), many control-CPSs are
designed to move smoothly (see more rigorous deﬁnitions in
Section III-A) . For example, if an airplane bumps when there
is no turbulence nor passenger/cargo movement, something
might be wrong.
A mature tool that checks man-made control systems’
smoothness is the autoregressive system identiﬁcation (AR-SI).
Its validity is well evidenced by the great success of model
predictive control (MPC) [34], which is a widely adopted
online control technology [35][36][37][38][39][40][41]. Main-
stream MPCs often use AR-SI to identify the math models of
concerned control systems as black-boxes, assuming smooth-
ness; and use the identiﬁed models to predict future and make
control decisions. If a control system’s physical trajectory is
smooth, the AR-SI prediction shall succeed; otherwise, the
prediction shall fail. In other words, we can use AR-SI to
check if a control-CPS’s physical trajectory is smooth, which
in turn indicates if the trajectory is buggy.
Inspired by the above, we propose the following solution
heuristics. To prepare the many code traces for SFL, we
emulate the control-CPS with the real cyber subsystem and
the simulated physical subsystem (using the accurate physical
subsystem model, and put noises under control). During the
emulation, we use AR-SI to identify the control-CPS as a
black-box and predict the physical trajectory in the near
(emulation) future. We speculate that when all the easy-to-ﬁndbugs are removed (i.e. when automatic SFL tools are needed),
physical trajectories shall usually be smooth. A segment of
a physical trajectory that is not smooth can be detected by
AR-SI, and shall indicate a possibly buggy behavior. That is,
we can use the AR-SI prediction as an oracle to label the
smoothness, hence correctness, of physical trajectories; and
then label the corresponding code traces. Guided by the above
heuristics, this paper makes the following contributions.
1) We propose an AR-SI based oracle for control-CPS SFL.
2) We propose a corresponding code trace preparation method-
ology, which runs deterministically and automatically.
3) We compare the performance of our proposed approach
with the mainstream practice of human oracle approach.
Comprehensive evaluations on classic control-CPSs with in-
jected real-life and artiﬁcial bugs show that our proposed
approach signiﬁcantly outperforms the human oracle approach
in SFL accuracy (recall) and latency, and in oracle false
positive/negative rates. Our proposed approach also helps to
discover a new real-life bug in a consumer-grade control-CPS
(while the human oracle approach does not).
The rest of the paper is organized as follows. Section II gives
the background and assumptions. Section III proposes our
oracle and corresponding code trace preparation methodology.
Section IV evaluates our proposal. Section V discusses related
work. Section VI concludes the paper.
II. S YSTEM ARCHITECTURE AND ASSUMPTIONS
Fig. 2 illustrates a typical control-CPS architecture. In this
architecture, the control-CPS consists of a cyber subsystem
and a physical subsystem (see the gray area in Fig. 2). User
input at time t∈[0,+∞)to the control-CPS is U(t)∈Rq.
Depending on the cyber subsystem design, U(t)can be an
actuation signal, an intended target output etc.U(t)is sampled
before it enters the cyber subsystem. The hth (h=0 ,1,
···) sample happens at th=hT, whereTis the user input
sampling period . Correspondingly, we denote Uhdef=U(th).
The cyber subsystem zero-order holds the sample Uhfor the
duration[hT,(h+1 )T). That is, during [hT,(h+1 )T), the
cyber subsystem takes constant Uhas the user input.
U(t)is the user input to the control-CPS. X(t)andY(t)are respectively
the state and the output of the physical plant.
In case of emulation, the physical subsystem is replaced by its simulator, and
the human user is often replaced by a mimicking program, aka “monkey”.
Fig. 2: A typical control-CPS architecture
Inside of the physical subsystem lies the physical object, aka
plant , being controlled. The state and the output of the plant at
timet∈[0,+∞)are respectively denoted as X(t)∈Rnand
117
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. Y(t)∈Rm. As a closed-loop system, Y(t)is also sampled
by the cyber subsystem periodically. The ith (i=0,1,···)
sample happens at ti=iΔ, whereΔis the plant sampling
period andΔ=T/N , whereN∈Z>1is a preconﬁgured
constant. Correspondingly, we denote Yidef=Y(ti). The cyber
subsystem also zero-order holds the sample Yifor the duration
[iΔ,(i+1)Δ) .
In this paper , we only focus on control-CPSs complying with
the following assumptions :
Assumption 1 :Y(t)is continuous and differentiable, with a
frequency component upper bound Fmax<∞(Hz).
Control-CPSs complying Assumption 1 are common, e.g.
when the last stage of the physical subsystem is a low-pass
ﬁlter with maximal passing frequency of Fmax. Indeed this
is why airplane trajectories, temperature trajectories etc. are
usually continuous and differentiable, and cannot be a step
function over time (which contains ∞Hz frequency compo-
nent). Corresponding to Assumption 1 , control-CPS design
has the following empirical rule on how to set Δ[11][12]:
Assumption 2 :Δ/lessorequalslant1/(20Fmax).
Intuitively, this means the cyber subsystem must sample fast
enough so that the physical subsystem can regard it as if a
continuous signal processor. This is an empirical prerequisite
for reusing continuous domain control theories [11][12].
The above said, to generate the many labeled code traces
for SFL, the user shall run the control-CPS many times.
Usually, the human user is replaced by an automatic human-
user mimicking program, aka “ monkey ”, and the physical sub-
system is replaced by its simulator (see footnote 1). The cyber
subsystem is still the buggy real implementation. Thus, the
monkey, the buggy real cyber subsystem, and the trustworthy
physical subsystem simulator form an emulation platform (see
Fig. 2). We can run the emulated control-CPS millions of times
without fatiguing any human user or real physical hardware;
and we can run the emulations on parallel computers in a much
faster pace than in real-time.
Each run of the emulated (or real) control-CPS generates a
code trace and a corresponding physical trajectory. To label the
correctness of the code trace, we need to judge the correctness
of the physical trajectory. This leads to the control-CPS oracle
problem discussed in Section I.
III. S OLUTION
A. Mathematical Background and Heuristics
Section I proposes using AR-SI as an oracle for control-CPS
SFL. The following gives the math details of AR-SI.
AR-SI [42][43][34] regards a control system as a black-box,
and identiﬁes the relationship between the control system’s
sampled input and output online. The identiﬁcation result is
called the identiﬁed model . Speciﬁcally, if we regard the the
gray area of Fig. 2 as a black-box, whose input is Uh∈Rq
and output is Yi∈Rm, then AR-SI typically models therelationship between UhandYi(∀i=p,p+1,···)a s
Yi=(p/summationdisplay
j=1AjYi−j)+BU h+ξi,, (1)
whereh=⌊iΔ/T⌋=⌊i/N⌋;p∈{1,...,N−1}is a preset
constant to be discussed later; A1,A2,···,Ap∈Rm×mand
B∈Rm×qare the model parameters to be identiﬁed; and
ξi∈Rmis the SI error .
As mentioned in Section II, during runtime, in each user
input sampling period, the user input sample Uhholds constant
throughout [hT,(h+1 )T). In such a user input sampling
period, once i+1 (p/lessorequalslanti<N ) consecutive plant states
(without loss of generality, denote them as Y0,...,Yi−(p−1),
...,Yi−1,Yi) become available, AR-SI optimizes the values
ofA1,A2,...,ApandBin Exp. (1), so that the runtime
accumulated SI error energy is minimized (see the seminal
textbooks of [42][43][34] for details). Suppose the optimal
A1,A2,...,Ap, andBvalues are A(∗,i)
1,A(∗,i)
2,...,A(∗,i)
p,
andB(∗,i), then AR-SI predicts Yi+1withˆYi+1:
ˆYi+1def=(p/summationdisplay
j=1A(∗,i)
jYi+1−j)+B(∗,i)Uh. (2)
WhenYi+1becomes available, we know the
AR-SI prediction error :ei+1def=ˆYi+1−Yi+1. (3)
Heuristics 1 : When the actual maximal frequency component
of the continuous control-CPS output Y(t)isFactual
max<
∞(Hz), it is well known in control theory that the Y(t)in
a small enough moving time window (speciﬁcally, pΔ/lessorequalslant
1/(20Factual
max)(sec)) can be regarded as linearizable [11][12].
That is, the AR-SI prediction error magnitude (when pΔ/lessorequalslant
1/(20Factual
max)) should be small; a magnitude outlier indicates
something unusual (i.e. likely buggy) is happening. In this
sense, we can use the AR-SI prediction error as an oracle.
Now let us discuss the setting of p. We have the following
heuristics. When noise is under control, usually a control-CPS
is designed so that the normal behavior of Y(t)should not
reach its frequency component upper bound Fmax. Indeed, the
actual maximal frequency component Factual
max of a normal Y(t)
is usually order(s) of magnitude below Fmax. That is,
Heuristics 2 : Normally, a control-CPS’ output should have
Factual
max/lessorequalslantFmax
10, or equivalently10
Fmax/lessorequalslant1
Factual
max.
For example, though an engine can shake at 100Hz maximal,
a normal design shall not let it shake at such extreme (under
controlled noises). The design shall limit the shaking below
10Hz.
With Heuristics 2 and Assumption 2 ,w eh a v ei f p=1 0 ,
thenpΔ/lessorequalslant10/(20Fmax)/lessorequalslant1/(20Factual
max). Thus, the prereq-
uisite for Heuristics 1 holds. Hence, by setting pto10,w e
can use Heuristics 1 as the oracle2for control-CPS SFL. This
AR-SI based oracle has three advantages.
2A breach of Heuristics 2 or other faults may cause big AR-SI prediction
error. If the error is big enough to be identiﬁed as an “outlier”, Heuristics 1
ﬂags the output Y(t).
118
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. First, the method procedure is deterministic, and is readily
implementable as an automatic program.
Second, both the method procedure and the identiﬁed model
are simple, and are independent of the internal complexity of
the control-CPS. Hence it is easy to implement and use.
Third, though the method is simple, it can effectively
catch subtleties of control-CPS physical trajectories due to
Assumption 2 ,Heuristic 2 , and the setting of pto10.
B. Proposed Solution
Based on the heuristics of Section III-A, we propose the
oracle and code trace preparation methodology (referred to as
“our proposed approach ” in the following) as Fig. 3.
Step1 Sample a valid initial state X0of the plant.
Step2 Task1 : Emulate the control-CPS using the emula-
tion platform of Fig. 2 starting from X(0) :=X0.
Log the physical trajectory {Yi}, user input trace
{Uh}, and code trace θ.
Task2 : Run AR-SI in parallel with the control-
CPS emulation. For each new physical trajectory
sampleYi+1from the emulation, calculate the AR-
SI prediction error ei+1via Exp. (3) and log it.
Step3 When the emulation ends, check if the AR-SI
prediction error trace {ei}contains outlier(s)a.I f
so, label θas “incorrect”; otherwise label θas
“correct”.
Step4 If enough code traces are collected, terminate;
otherwise go to Step1 .
aNote in statistics, currently there is no consensus on the strict deﬁnition
of an “outlier” [44][45]. In this paper, unless otherwise noted, we regard a
data point outside of (mean ±6std) as an outlier. See Section IV-G-2.3 for
more discussions.
Fig. 3: Our proposed oracle and methodology to prepare code
traces (this oracle and methodology are referred to as “ our
proposed approach ”).
Compared to our proposed approach, the other approach is
the so-called human oracle approach depicted by Fig. 4.
Step1 Same as Fig. 3- Step1 .
Step2 Same as Fig. 3- Step2 -Task1 .
Step3 Use human oracle to judge the correctness of phys-
ical trajectory {Yi}, and label the corresponding
code trace θ.
Step4 Same as Fig. 3- Step4 .
Fig. 4: Human oracle and methodology to prepare code traces
(this oracle and methodology are referred to as the “ human
oracle approach ”).
IV . E V ALUATION
Next, we shall evaluate our proposed approach (see Fig. 3).A. Evaluation Metrics and Research Questions
Our proposed approach (see Fig. 3) mainly serve three main-
stream families of SFL tools: program spectrum, statistics, and
machine learning. An SFL tool of these families takes in a
large number of labeled code traces, and outputs a suspect
list. This list lists suspected buggy source code entities (such
as statements, methods, and classes) in descending order of
suspiciousness.
We have three metrics to measure the quality of the suspect
list. Suppose the suspect list is S=(s1,s2,···,sl), wheresi
(i=1,···,l)i st h eith suspected ( s1is the most suspected)
source code entity. Suppose Bis the set of truly buggy source
code entities. Then accuracy refers to |{si|si∈B}|/l, i.e.
coverage of true bugs by S;recall refers to|{si|si∈B}|/|B|,
i.e. coverage of true bugs in B;latency refers to min{i|si∈
B}, i.e. starting from s1, the index of the ﬁrst suspect in S
that is truly buggy. In case none of the items in Sbelong
toB, we deﬁne latency as the length of suspect list S, i.e.
l, as the programmer needs to investigate all litems in the
suspect list before s/he stops. Note when land|B|are given,
recall is just a linear scale-up of accuracy. In this context,
accuracy implies recall. Therefore, to save space, we omit the
corresponding recall evaluations in the following.
As the suspect list is the SFL result, the above suspect list
quality metrics are hence also the SFL quality metrics. Our
evaluation intends to answer the following research question.
Q1 How does our proposed approach impact SFL quality
(measured by accuracy (recall) and latency)?
In addition to SFL, we are also interested in the raw quality
of the oracle itself. Speciﬁcally, suppose we have collected a
set of code traces Θ; letPdef={θ∈Θ|θis labeled by the
oracle as buggy (i.e. “incorrect”) },PTdef={θ∈Θ|θtruly
includes buggy entity of the source code },Ndef=Θ−P, and
NTdef=Θ−P T; we can then deﬁne the false positive rate RFP
and the false negative rate RFNof the oracle as
RFPdef=|P ∩N T|/|N T|, andRFNdef=|N ∩P T|/|P T|. (4)
We intend to answer the following research question.
Q2 How does our proposed approach impact raw oracle
quality (measured by RFP,RFN)?
In the following, we shall carry out evaluations on two
control-CPS test-beds with various bugs from real-life and/or
artiﬁcial injection to answer the above research questions.
B. Control-CPS Test-beds
The ﬁrst test-bed is ArduPilot [46]. ArduPilot is a state-
of-the-art open platform to build consumer-grade unmanned
aerial vehicles (UA Vs) [47][48]. The cyber subsystem of
ArduPilot is mostly written in C++. It consists of over 1.4
million lines of source code from more than 9,600 ﬁles, with
a total size of over 380MB. To debug the entirety of ArduPilot
is too much work for one paper. In this paper, we focus on
debugging the application layer of ArduPilot, which already
119
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. consists of 27,000 lines of source code from 160 ﬁles, with a
total size of over 1.06MB.
ArduPilot also features a physical subsystem simulator: the
software in the loop (SITL) simulator. As the UA V physical
subsystem model is simple and well established, the SITL
simulator is relatively simple (only consists of about 6 thou-
sand lines of source code). Also, the SITL simulator is already
widely used by the community, even for the development of
consumer-products [47][48][49]. Based on these, we reason-
ably regard the SITL simulator as bug-free.
By connecting a (possibly buggy) ArduPilot cyber subsys-
tem with the SITL physical subsystem simulator, we build
the control-CPS emulation platform required by our pro-
posed approach and the human oracle approach (see Fig. 2,
Fig. 3- Step2 , and Fig. 4- Step2 ). The monkey (see Fig. 2)
we adopt is a program that steers the ArduPilot UA V from
randomly sampled initial states. The initial x, y, z loca-
tion and pitch, roll, yaw of the UA V are sampled3from
range[0,111.3km],[0,111.3km],[0.4km,0.5km],[−20◦,20◦],
[−20◦,20◦],[0◦,360◦)respectively. Each emulation spans an
emulated time duration of 12 seconds. In the emulated time,
the plant sampling period Δ = 100 ms, and the user input
sampling period T=2s.
The second control-CPS test-bed is one that combines
inverted pendulum and computer vision (IP+CV). The physical
subsystem is an inverted pendulum (IP), which is a classic
robotic test-bed for control theories [11][50][12]. An IP con-
sists of a cart moving along the x-axis and a metal rod (i.e.
the pendulum) with one end hinged on the cart. The other end
of the metal rod is free to rotate. The cyber subsystem has
two parts: an on-IP module and an off-IP module. The on-IP
module runs inner-loop control: i.e. the core control law that
moves the IP cart back and forth along the x-axis to keep
the metal rod standing up-right still. The off-IP module runs
outer-loop control: i.e. the complex computer logic involving
video monitoring and computer vision (CV) to decide the
target locations (i.e. reference points ) on the x-axis for the
IP to stabilize around.
To build the emulation platform of Fig. 2 for the IP+CV
test-bed, we need a physical subsystem simulator, which is
realized by the virtual reality software of Unity [51]. Due to
interfacing constraints, the cyber subsystem’s on-IP module
and the off-IP module’s video camera sub-module also run
upon the Unity server. The total source codes running on the
Unity server consist of 1464 lines. Due to the small scale,
these source codes can be manually thoroughly debugged. We
therefore regard them as bug-free. The remaining of the cyber
subsystem (mainly the CV part of the off-IP module) runs
outside of the Unity server, and has 4000 lines of source code
from 11 ﬁles, with a total size of 161.9KB.
3Note Fig. 3- Step1 leaves the sampling strategy open. Indepth discussion
on the optimal strategy is way beyond this paper’s coverage. Instead, we
adopt a mixed-clustered-uniform sampling strategy. Speciﬁcally, we start with
uniform sampling. Once we have an initial sample set, we quadruple the
number of samples by adding disturbances to the initial sample set. So the
ﬁnal set of samples are clustered (4 samples per cluster), and the clusters are
uniformly distributed.By connecting a (possibly buggy) IP+CV cyber subsystem
with the physical subsystem simulator, we can build the
control-CPS emulation platform required by our proposed
approach and the human oracle approach (see Fig. 2, Fig. 3-
Step2 , and Fig. 4- Step2 ). The monkey (see Fig. 2) we adopt
is a program that steers the IP from randomly sampled initial
states. The initial x-axis location of the IP cart is sampled
(see footnote 3 for the sampling strategy details) from range
[−0.32,0](m). Each emulation trial spans an emulated time
duration of 280 seconds. In the emulated time, the plant
sampling period Δ=4 0 ms, and the user input sampling
periodT=8s.
C. Evaluations with Injected Real-life Bugs
ArduPilot archives its release history in GitHub [52]. From
it we can ﬁnd real-life bugs appeared in earlier released
versions. Of these bugs, 8 belong to the application layer
source code that we focus on. We inject these bugs back into
our ArduPilot test-bed, as described by Table I.
TABLE I: Real-life bugs to be injected into ArduPilot
File-Function-Line# Bug Description
mode steering.cpp-update-56 wrong stop throttle calculation
mode steering.cpp-update-6wrong navigation throttle calculation when
reversing
mode auto.cpp-update-56wrong acceleration calculation when revers-
ing
mode auto.cpp- enter-12 wrong initialization in auto mode
mode rtl.cpp- enter-56 wrong initialization in rtl mode
mode guided.cpp-update-28 wrong mavlink signal message
mode guided.cpp- enter-6 wrong initialization in guided mode
mode.cpp-calc throttle-79 wrong braking throttle value calculation
Guided by Table I, we create 40 versions of buggy ArduPilot
cyber subsystems, each called a subject . Eight of the 40
subjects are respectively injected with one of the candidate
real-life bugs in Table I. The other 32 subjects are respectively
injected with 5 candidate real-life bugs selected from Table I.
For each buggy ArduPilot cyber subsystem subject, our
emulation platform runs 3 trials. In each trial, the emulation
platform generates 2665 code traces (and the corresponding
physical trajectories) by repeating Fig. 3- Step1 toStep2 -
Task1 2665 times4. These code traces are then labeled by
our AR-SI oracle as per Fig. 3- Step2 -Task2 and Step3 . The
2665 labeled code traces are then sent to three different SFL
tools to ﬁnd bugs: Tarantula (TA) [53], Crosstab (CR) [54],
and BP Neural Network (NN) [55]. These three tools are
respectively the widely recognized representatives from the
program spectrum, statistics, and machine learning SFL tool
families [13]. As mentioned before, such tools respectively
output a suspect list of litems (i.e. lpossibly buggy source
code entities) in descending order of suspiciousness. We
assume a human programmer’s effort is limited and can only
investigate the top 10 suspect list items. Hence, we set l=1 0 .
In comparison, we also carry out the human oracle approach
(see Fig. 4). In the human oracle approach evaluation, each
4In fact, as per our sampling strategy (see footnote 3), 7996 initial states
are sampled, but only the ﬁrst 2665 ×3 = 7995 samples are used.
120
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. buggy ArduPilot subject is again given 3 trials. In each trial,
the emulation platform generates 2665 code traces (and the
corresponding physical trajectories)5. These code traces are
then labeled by the human oracle, and then sent to TA, CR,
and NN for SFL.
Speciﬁcally, the human oracle for ArduPilot runs as follows.
A code trace is labeled “correct” iff its corresponding physical
trajectory obeys all the rules H1.1∼H1.3 listed below; and
labeled “incorrect” otherwise.
H1.1 The UA V velocity shall never exceed ±20m/s.
H1.2 The UA V velocity component along longitude, lat-
itude, and altitude shall never exceed ±31.5m/s,
±28.16m/s,±30m/s respectively.
H1.3 The UA V angular velocity along pitch and roll shall
never exceed ±3.5rad/s and ±3.6rad/s respectively.
H1.1∼H1.3 are given by a panel of ﬁve domain experts via
thorough discussions6. Three of the domain experts have PhD
degrees, and 9 to 15 years of experiences on control systems
and CPS research and development (R&D). Two experts hold
professorship in control theory, including one from a leading
mechanical engineering department in the world. Three experts
have over 2 years of ArduPilot R&D experiences.
The evaluation results are plotted in Fig. 5, and will be
discussed in Section IV-E.
Ideally, we would also like to carry out injected real-life bug
evaluation on the IP+CV control-CPS test-bed. Unfortunately,
this test-bed is a legacy of our lab. Its debugging history is not
well documented. Meanwhile, its smaller size (compared to
ArduPilot) and long usage history make it unlikely to contain
more bugs. Therefore, we do not include the IP+CV test-bed
in the injected real-life bug evaluation.
D. Evaluations with Injected Artiﬁcial Bugs
Besides the real-life bugs mentioned in Section IV-C, there
are other commonly seen bugs. Natella et al. conducted a
comprehensive survey, and publish a list of commonly seen
bugs-in-the-ﬁeld as shown in Table II [58]. In our evaluation,
we also inject such bugs (referred to as “ artiﬁcial bugs ”i n
the following) into our test-beds, and evaluate our proposed
approach in the corresponding SFL.
We run evaluations on both test-beds.
For the ArduPilot test-bed, guided by Table II, we inject
various artiﬁcial bugs into the cyber subsystem source code.
These candidate artiﬁcial bugs and injection locations are
listed in Table III. We create 61 versions (i.e. 61 subjects)
of buggy ArduPilot cyber subsystems. 17 of the 61 subjects
respectively contain one of the candidate artiﬁcial bugs in
5Note for both our proposed approach and the human oracle approach, the
emulation methods are the same (see Fig. 3- Step1 toStep2 -Task1 , Fig. 4-
Step1 toStep2 ). Therefore, the (unlabeled) code traces (and the corresponding
physical trajectories) generated for our proposed approach evaluation are
reused. Such reuse also applies to all other comparison evaluations between
our proposed approach and the human oracle approach in the paper.
6Note Lyapunov stability region [11][12] is not listed as an oracle, because
the ArduPilot UA V controller uses model-less PID control [11][12], whose
Lyapunov stability region is impractical to derive.Accuracy (the higher the better)
(a) 1-bug subjects’ trials (b) 5-bug subjects’ trials (c) all subjects’ trials
Latency (the shorter the better)
(d) 1-bug subjects’ trials (e) 5-bug subjects’ trials (f) all subjects’ trials
Oracle false positive rate ( RFP) and false negative rate ( RFN)
(g) 1-bug subjects’ trials (h) 5-bug subjects’ trials (i) all subjects’ trials
X-axis for (a) ∼(f): oracle approach + SFL tool. X-axis for (g) ∼(i): ora-
cle approach. PA: our proposed approach; HA: human oracle approach.
TA: Tarantula; CR: Crosstab; NN: BP Neural Network. FP: oracle false
positive rate; FN: oracle false negative rate.
Y-axis for (a) ∼(f): statistics of per trial SFL quality metrics. Y-axis for
(g)∼(i): statistics of per trial oracle false positive/negative rates.
In each boxplot [56], the thick bar in each box is the median of the
data, the box bottom/top are the 1st and 3rd quartile of the data (see
[56][57] for some subtle details on their deﬁnitions). In (a) ∼(f), the
“+”s outside the boxes are data outside of the 1st and 3rd quartiles (as
they have relatively discrete values, we plot them individually instead
of using whiskers, so as to provide more information).
For each given oracle approach and SFL tool, there are 8×3=2 4
one-bug subject trials and 32×3=9 6 ﬁve-bug subject trials, hence a
total of 120 trials. Each trial corresponds to 2665 code traces.
Fig. 5: Evaluation results: ArduPilot with real-life bugs
TABLE II: Common bugs-in-the-ﬁeld (quoted from [58])
Type Description
WPFV Wrong Variable used in Parameter of Function call
WV A V Wrong Value Assigned to Variable
MV AE Missing Variable Assignment using Expression
MFC Missing Function Call
MIA Missing IF construct Around statements
MVIV Missing Variable Initialization using a Value
MV A V Missing Variable Assignment using a Value
MIFS Missing IF construct plus Statements
MIEB Missing IF construct plus statements plus ELSE Before statements
MLC Missing a Logic Clause in branch condition
MLPA Missing small and Localized Part of the Algorithm
W AEP Wrong Arithmetic Expression in Parameter of function call
Table III. The other 44 subjects each contains 5 candidate
artiﬁcial bugs selected from Table III.
For each buggy ArduPilot cyber subsystem subject, our em-
ulation platform runs 3 trials. Each trial generates 2665 code
traces (and the corresponding physical trajectories). These
code traces are then labeled by our AR-SI oracle as per Fig. 3-
Step2 -Task2 and Step3 .
The 2665 labeled code traces are then sent to the TA, CR,
121
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. TABLE III: Artiﬁcial bugs to be injected into ArduPilot
Type Function-Line# Bug Description
WV A V pos torate xy-845 mistake “multiply” by “divide”
MV A V calc leash length-999 an assignment deleted
WV A V pos torate z-365 assigned value negated
MV AE pos torate z-360 an assignment deleted
MV AE pos torate xy-798 an assignment deleted
WPFV pos torate z-378 swapped parameter x,y
MLPA getstopping point z-287 wrong expression used
MLPA getstopping point z-289 wrong expression used
MV A Vadvance wp
target along track-610an assignment deleted
MV A Vcalculate wp
leash length-794an assignment deleted
MFCsetwporigin
and destination-487immediate function return
MVIVcalc slow down
distance-1232initialization deleted
MIEB pos toaccel z-410missing if construct plus statements
plus else before “! ﬂags.freeze ffz”
MIA pos torate xy-847 missing if construct
MIFS calc leash length-1000missing if construct plus “leash length”
statements
W AEP getstopping point z-290 wrong arithmetic expression
MLCsetalttarget from climb
rate ff-211a logic clause in branch condition
deleted
and NN SFL tools to ﬁnd bugs. Same as Section IV-C, the
SFL tools respectively output an l=1 0 item suspect list.
In comparison, we also carry out the human oracle approach
(see Fig. 4). Simply put, in the human oracle approach eval-
uation, each buggy ArduPilot subject is also given 3 trials. In
each trial, the emulation platform generates 2665 code traces
(and the corresponding physical trajectories). These code
traces are then labeled by the human oracle (see Section IV-C
H1.1∼H1.3 ), and then sent to TA, CR, and NN for SFL.
The evaluation results are plotted in Fig. 6, and will be
discussed in Section IV-E.
For the IP+CV test-bed, we do the same thing. Guided
by Table II, we inject various artiﬁcial bugs into the cyber
subsystem source code. These candidate artiﬁcial bugs and
injection locations are listed in Table IV. We create 12 versions
(i.e. 12 subjects) of buggy IP+CV cyber subsystems. 6 of the
12 subjects respectively contain one of the candidate artiﬁcial
bugs in Table IV. The other 6 subjects each contain 5 candidate
artiﬁcial bugs selected from Table IV.
TABLE IV: Artiﬁcial bugs to be injected into IP+CV
Type Function-Line# Bug Description
WV A V cvFindChessboardCorners-260 assigned value negated
MV AE cvFindChessboardCorners-516 an assignment deleted
WV A V cvCreateMatHeader-130 assigned value negated
MVIV cvCreateMatHeader-137 initialization deleted
WV A V cvInitImageHeader-2973 mistake “0” by “-1”
W AEP cvInitImageHeader-2980 wrong arithmetic operation
For each buggy IP+CV cyber subsystem subject, our emu-
lation platform runs 3 trials. Each trial generates 100 code
traces (and the corresponding physical trajectories). These
code traces are then labeled by our AR-SI oracle as per Fig. 3-
Step2 -Task2 and Step3 . The 100 labeled code traces are thenAccuracy (the higher the better)
(a) 1-bug subjects’ trials (b) 5-bug subjects’ trials (c) all subjects’ trials
Latency (the shorter the better)
(d) 1-bug subjects’ trials (e) 5-bug subjects’ trials (f) all subjects’ trials
Oracle false positive rate ( RFP) and false negative rate ( RFN)
(g) 1-bug subjects’ trials (h) 5-bug subjects’ trials (i) all subjects’ trials
See Fig. 5 annotations for the meanings of X-axis labels, Y-axis labels,
and legends in boxplots.
For each given oracle approach and SFL tool, there are 17×3=5 1
one-bug subject trials and 44×3 = 132 ﬁve-bug subjects trials, hence
a total of 183 trials. Each trial corresponds to 2665 code traces.
Fig. 6: Evaluation results: ArduPilot with artiﬁcial bugs
sent to the TA, CR, and NN SFL tools to ﬁnd bugs. Same
as Section IV-C, the SFL tools respectively output an l=1 0
item suspect list.
In comparison, we also carry out the human oracle approach
(see Fig. 4). Simply put, in the human oracle approach evalu-
ation, each buggy IP+CV subject is also given 3 trials. In each
trial, the emulation platform generates 100 code traces (and
the corresponding physical trajectories). These code traces are
then labeled by the human oracle, and then sent to TA, CR,
and NN for SFL.
Speciﬁcally, the human oracle for IP+CV runs as follows. A
code trace is labeled “correct” iff its corresponding physical
trajectory obeys all the rules H2.1∼H2.3 listed below; and
labeled “incorrect” otherwise.
H2.1 When the IP velocity is within ±0.01837 m/s, the IP
angular velocity shall never exceed ±0.0731 rad/s.
H2.2 The IP angular displacement shall never exceed
±0.09rad.
H2.3 The IP velocity shall never exceed ±0.4545 m/s.
H2.1∼H2.3 are given by a panel of ﬁve domain experts
via thorough discussions7. Three of the ﬁve domain experts
have PhD degrees and 9 to 15 years of experiences on control
systems (including IP) and CPS R&D. Two experts hold
professorship in control theory, including one from a leading
7Note Lyapunov stability region [11][12] is not listed as an oracle, because
it focuses on the core control law implementation (in the inner control
loop), which is quite simple for the IP+CV , and is thoroughly debugged.
As mentioned in Section I, this paper focuses on the additional modules (e.g.
CV libraries in the outer control loop) of the cyber subsystem, not the core
control law implementations.
122
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. mechanical engineering department in the world. Two experts
have over 3 years of CV R&D experiences.
The evaluation results are plotted in Fig. 7, and will be
discussed in Section IV-E.
Accuracy (the higher the better)
(a) 1-bug subjects’ trials (b) 5-bug subjects’ trials (c) all subjects’ trials
Latency (the shorter the better)
(d) 1-bug subjects’ trials (e) 5-bug subjects’ trials (f) all subjects’ trials
Oracle false positive rate ( RFP) and false negative rate ( RFN)
(g) 1-bug subjects’ trials (h) 5-bug subjects’ trials (i) all subjects’ trials
See Fig. 5 annotations for the meanings of X-axis labels, Y-axis labels,
and legends in boxplots.
For each given oracle approach and SFL tool, there are 6×3=1 8
one-bug subject trials and 6×3=1 8 ﬁve-bug subject trials, hence a
total of 32 trials. Each trial corresponds to 100 code traces.
Fig. 7: Evaluation results: IP+CV with artiﬁcial bugs
E. Discussions on Injected Bug Evaluation Results
Now we discuss the above evaluation results, aiming to
answer the research questions Q1and Q2(see Section IV-A).
Corresponding to Q1, Fig. 5(a) ∼(f), Fig. 6(a) ∼(f), and
Fig. 7(a)∼(f) all show our proposed approach outperforms
the human oracle approach in accuracy (recall) and latency
in the TA and CR SFL. The signiﬁcance and magnitude
of the improvement are respectively quantiﬁed by the p-
values [59][60] and effect size (ES) [61][62] values in Table V.
Note p-values estimate the probabilities that the comparison
differences are only by chance [60], hence the smaller the more
signiﬁcant are the comparison differences. ES values measure
the magnitudes of the differences. Our ES values are quantiﬁed
by Cohen’s d [62]: an absolute value of over 0.4 indicates the
difference magnitude is at least medium.
As shown in Table V, for accuracy (recall) and latency
comparisons related to the TA and CR SFL, the p-values
are mostly (except for a few light gray cells with slightly
over 5% p-values) below 5%, hence the improvement of
our proposed approach over the human oracle approach is
statistically signiﬁcant. The corresponding ES absolute values
are mostly above 0.4 (except for a few light gray cells with
slightly less than 0.4 values), and often exceed 0.8, implying
the magnitude of the improvement is medium to large.TABLE V: Quality of Fig. 5, 6, and 7 statistics
Fig. 5 PA vs HA Comparisons (on ArduPilot, with real-life bugs)
metric1-bug sbj 5-bug sbj all sbj
p-value ES p-value ES p-value ES
accuracyTA 2.9% 0.65 1.0% 0.37 0.3% 0.38
CR 0.7% 0.82 <1‰ 0.68 <1‰ 0.67
NN 15.5% - 55.1% - 38.5% -
latencyTA 4.5% -0.60 5.6% -0.28 2.2% -0.30
CR 0.7% -0.81 0.8% -0.39 0.2% -0.41
NN 17.9% - 25.2% - 32.0% -
RFP <1‰ -4.9 <1‰ -4.2 <1‰ -4.6
RFN <1‰ -2.8 <1‰ -3.4 <1‰ -3.0
Fig. 6 PA vs HA Comparisons (on ArduPilot, with artiﬁcial bugs)
metric1-bug sbj 5-bug sbj all sbj
p-value ES p-value ES p-value ES
accuracyTA 5.5% 0.38 <1‰ 0.93 <1‰ 0.69
CR 5.5% 0.38 <1‰ 0.71 <1‰ 0.56
NN 65.0% - 20.1% - 17.6% -
latencyTA 1.1% -0.51 <1‰ -0.62 <1‰ -0.53
CR 4.3% -0.41 <1‰ -0.46 <1‰ -0.41
NN 14.0% - 20.5% - 7.2% -
RFP <1‰ -7.2 <1‰ -4.5 <1‰ -5.4
RFN <1‰ -4.9 <1‰ -4.6 <1‰ -4.8
Fig. 7 PA vs HA Comparisons (on IP+CV , with artiﬁcial bugs)
metric1-bug sbj 5-bug sbj all sbj
p-value ES p-value ES p-value ES
accuracyTA 2.5% 0.78 2.1% 0.81 0.3% 0.72
CR 5.6% 0.66 1.8% 0.83 0.4% 0.70
NN 38.6% - 60.5% - 37.2% -
latencyTA 2.4% -0.79 0.8% -0.94 <1‰ -0.86
CR 1.0% -0.91 <1‰ -1.22 <1‰ -1.05
NN 8.5% - 38.7% - 7.0% -
RFP <1‰ -4.9 <1‰ -6.9 <1‰ -5.8
RFN <1‰ -5.7 <1‰ -4.7 <1‰ -5.1
For the NN SFL, neither our proposed approach nor the
human oracle approach works well. This is possibly because
as a neural network solution, NN needs really big data to work.
That is, millions to trillions of items in the training set. The
hundred or thousands of labeled code traces that we generate
for each trial are probably still insufﬁcient.
Correspondingly, for NN, the lack of difference between
our proposed approach and the human oracle approach is
reﬂected in the p-values in Table V (see the dark gray cells).
For accuracy and latency comparisons related to the NN SFL,
the p-values are big. This also means that it is meaningless to
evaluate the magnitude of improvement, i.e. the corresponding
ES values (which are hence omitted in the table).
Corresponding to Q2, Fig. 5(g) ∼(i), Fig. 6(g) ∼(i), and
Fig. 7(g)∼(i) all show our proposed approach outperforms the
human oracle approach in RFPandRFN. The signiﬁcance and
magnitude of the improvement are respectively quantiﬁed by
the p-values and ES values in Table V.
As shown in Table V, for RFPandRFNcomparisons, the p-
values are all much smaller than 5%, hence the improvement
of our proposed approach over the human oracle approach is
statistically signiﬁcant. The corresponding ES absolute values
are all over 0.8, indicating large improvement magnitudes.
In summary, our evaluations imply the following answers.
Answer to Q1 :Compared to the human oracle approach, our
proposed approach signiﬁcantly improves the quality of TA
123
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. and CR SFL (in terms of accuracy (recall) and latency). The
magnitude of improvement is medium to large.
Answer to Q2 :Compared to the human oracle approach,
our proposed approach signiﬁcantly improves the raw quality
of the oracle (in terms of false positive/negative rates). The
magnitude of improvement is large.
F . Discovery of New Real-life Bugs in ArduPilot
Besides the injected bug evaluations, we also apply our
approach to discover new real-life bugs. For the current version
of ArduPilot (GitHub version hash: a7fd353), we run an
extensive emulation trial generating 8000 code traces (and
the corresponding physical trajectories) by repeating Fig. 3-
Step1 toStep2 -Task1 8000 times. These code traces are then
labeled8by our AR-SI oracle as described by Fig. 3- Step2 -
Task2 and Step3 . The 8000 labeled code traces are then sent
to TA for SFL. We checked the top 30 suspected source code
blocks outputted by TA SFL, and the 7th is conﬁrmed by the
ArduPilot core developers as a bug [63][64]. Correspondingly,
we also carried out the human oracle approach for comparison,
but none of the found suspects is conﬁrmed. Particularly, the
new real-life bug discovered by our proposed approach is not
in the suspect list found by the human oracle approach.
G. Threats to V alidity
1. Construct Validity Threats :
1.1 Human Oracle V alidity. In a human oracle design panel,
an expert can be biased due to his/her knowledge and skill
set differences. To mitigate the bias introduced by individuals,
each panel has at least ﬁve experts with high qualiﬁcations
(e.g. three experts hold PhDs and over 9 to 15 years of R&D
experiences on CPS; two experts hold professorship in control
theory, including one from a leading mechanical engineering
department of the world) across various domains (embedded
systems, CV , control theory, mechanical engineering) to design
the human oracles via thorough discussions. Though our
panels can always be improved, their qualities are reasonably
good compared to real-world practices.
1.2 Golden Oracle V alidity. To judge the raw correctness of
oracles (in the oracle false positive/negative rates evaluations),
we need a “golden oracle” that tells if the control-CPS
behavior is truly buggy or not. Unfortunately, for control-
CPSs, what shall be the golden oracle is indeed also an open
problem. As shown by Fig. 1(b), a so-called “correct” physical
trajectory may not be unique. We cannot, for example, use the
behavior of a speciﬁc implementation (e.g. ArduPilot without
bug injection) as the golden oracle, and deny the correctness
of other behavioral possibilities.
Given this, we adopt a quasi-golden oracle. We conser-
vatively classify a code trace as truly buggy as long as it
covers a buggy source code entity. Such approximation is
acceptable considering that our ultimate goal is to locate the
buggy source code entity instead of detecting buggy behavior.
For this ultimate goal, it makes sense that a code trace covering
8Note instead of 6std, we used 7std as thresholds for outliers.buggy entities is always reported to SFL tools, even if it may
indeed cause no buggy behaviors.
Also, note the golden oracle is not used to label code
traces for SFL in our evaluations. Those code traces are
labeled by our AR-SI oracle and/or human oracle. Hence the
golden oracle has nothing to do with TA, CR, and NN SFL
evaluations.
1.3 SFL Tool Representativeness. To reduce this threat, we
used three widely recognized representatives from different
families of SFL tools [13] and compared their performance in
terms of accuracy (recall) and latency in the same evaluation
settings. We also plan to conduct extensive evaluations on
more SFL tools.
1.4 Other SI Methods. We choose AR-SI of Exp. (1)
because it is deterministic, automatic, simple, and
empirically well tested by broad adoption in practice
[35][36][37][38][39][40][41][34]. However, there are many
other SI methods [42]. Nevertheless, to our best knowledge,
we are the ﬁrst to use SI for control-CPS SFL oracle.
Whether there exists an even better SI oracle is still an open
problem for now. In this paper, we show AR-SI oracle alone
already outperforms the mainstream human oracle approach
signiﬁcantly. Next, we will conduct comprehensive survey of
other SI methods; and the optimal SI oracle to be found shall
only outperform the human oracle even more.
2. Internal Validity Threats :
2.1 Evaluation Platform Implementation Correctness. There
might be bugs in the implementation of our proposed approach
and the human oracle approach, and in the reimplementation of
existing SFL tools. To address the threat, we review our code
to ensure their correctness before conducting the evaluations.
2.2 Setting of p.We set the AR model order p(see Exp. (1),
(2)) to10due to Assumption 2 and Heuristics 2 . In case the
constants in Assumption 2 and/or Heuristics 2 change, this
setting shall change accordingly.
2.3 Setting of Outlier Bounds. We set the outlier bounds (see
Fig. 3 remark a) to (mean ±6std). This is an empirical
setting advised by domain experts. How to optimize this
settings is still an open problem. Fortunately, our evaluations
show that even without optimization, our proposed approach
already outperforms the human oracle approach (note the latter
approach does not use this outlier bound). With optimization,
our proposed approach shall outperform even more.
2.4 Bug Injection Locations and Combinations. The locations
of injected real-life bugs are determined by the public archives
(see the “File-Function-Line#” column of Table I). While
the locations of injected artiﬁcial bugs are randomly selected
(see the “Function-Line#” columns of Table III, IV). Due to
combinatorial explosion and feasibility constraints, we cannot
try every possible injection location/combination. But we are
trying our best to randomize these selections.
3. External Validity Threats :
3.1 V ariety of Test-beds and Injected Bugs. The limited types
of control-CPSs and injected bugs used in our evaluations
may threaten the external validity. This is also a well-known
124
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. problem facing control-CPS research. Control-CPS naturally
involves other domains, for which there lacks an open-source
movement. This results in much less open control-CPS plat-
forms for academia to explore compared to the pure computer
science community. To mitigate the problem, we deploy two
widely used control-CPSs as test-beds, and deploy real-life, as
well as artiﬁcial bugs reﬂecting commonly seen bugs in ﬁeld.
Such test-beds and bugs, however, may still under-represent
all control-CPSs and bugs. As future work, we plan to carry
out more evaluations on other test-beds and types of bugs.
V. R ELATED WORK
This paper focuses on cross-domain control-CPSs. Hence
the topic is orthogonal to general-purpose oracle (or SFL)
design for pure cyber systems [14][15][65][16][17][66]
[67][68][69][70][71][72].
SI methods that identify hybrid automata models [73][74]
from control-CPS behaviors may also provide control-CPS
oracles. But these SI methods still have open issues. For
example, [73] focuses on converging behaviors, which are
not always the case for generic control-CPS behaviors. [74]
needs known domain knowledge of the control-CPS (e.g.
semantics of feature vectors), hence is not entirely black-
box. Choi et al. [75] explored SI based oracles on detecting
malicious attacks to control CPSs. In comparison, this paper
focuses on evaluating oracle performance over typical bugs
unintentionally left by the programmers.
SI is closely related to speciﬁcation inference. Various
approaches including static analysis and dynamic analysis
techniques have emerged to infer speciﬁcations in the form of
behavioral models [76][77][78] [79] and/or invariants between
variables [23] [24][25][26][27] [80]. The inferred speciﬁca-
tions, however, comply with the existing programs and/or
training behaviors, including the incorrect ones. Hence they
are unﬁt for oracles, though they can help programmers
understand existing programs better.
There are works on exploiting static analysis to ﬁnd the most
relevant oracle variables [81]. But these works are not about
speciﬁcations inferred by static analysis; and how to apply
these works to control-CPS is still an open problem. Alippi
et al. [82] designed an oracle for control-CPS fault detection.
But their focus is on sensor faults instead of software faults.
Metamorphic testing [83][19][84][85] exploits necessary
conditions governing outputs’ interrelationships as imperfect
oracles . Chen et al. [84] propose a metamorphic testing
oracle for control-CPS with PID controller software. Modern
control-CPSs, however, are often more complex than PID
control. Recently, Goebel et al. [86] propose a set of hybrid
systems [87][88][89][90] stability theories for control-CPSs.
The stability rules can be used as metamorphic testing oracles.
However, Goebel et al.’s theories require the deﬁnition of a
“Lyapunov” function for the concerned hybrid system. For
a given generic control-CPS, the existence of a Lyapunov
function is not guaranteed, neither is there a generic routine
to decide its existence. Even if the Lyapunov function exists,
there is no generic mechanical routine to ﬁnd it. ThereforeGoebel et al.’s hybrid systems stability theories are mainly
for designing new control-CPSs, instead of for designing
imperfect oracles for existing and/or generic control-CPSs.
Nonetheless, the above endeavors inspire us to propose this
paper’s AR-SI based oracle, which is indeed an imperfect
oracle in a general sense.
There are other efforts to solve the oracle problem. Several
modeling/programming languages are developed to formally
describe oracles [78][91][92][93]; in addition, assertions and
contracts [94] are forms to describe oracles. But they do not
answer the oracle problem itself: what oracles to describe.
We can use N-version [95] or previous versions [96] to
generate trajectories for comparisons, but such methods are
orthogonal to the oracle problem solutions for debugging a
single version of a control-CPS.
There are a few works on test case generation for control-
CPSs [97][98][99]. Test case generation, however, is orthog-
onal to this paper’s topic. Meanwhile, a good oracle can
always beneﬁt testing, especially by spotting bugs whose
triggering mechanisms are not well known. Liu et al. [100]
propose a SFL tool specialized for control-CPS whose cyber
and physical models are known accurately. However, this
paper focuses on control-CPSs whose accurate cyber model
is unavailable. Deshmukh et al. [101] propose using SFL to
tune control parameters, this is not related to oracle designing.
The content of this paper is included in the ﬁrst author’s
PhD thesis.
VI. C ONCLUSION
In this paper, we proposed an AR-SI based oracle and corre-
sponding code trace preparation methodology for control-CPS
SFL. Our proposed approach can run deterministically and
automatically. We evaluated our proposal on classic control-
CPS test-beds with SFL of injected real-life and artiﬁcial bugs.
The evaluation results show that our proposed approach sig-
niﬁcantly outperforms the human oracle approach, achieving
medium to large improvements on SFL accuracy (recall) and
latency, as well as oracle false positive/negative rates. Using
our approach, we also discovered a new real-life bug in a
consumer-grade control-CPS.
ACKNOWLEDGMENT
We thank Mr. Ming Jin for the Unity program develop-
ment; and thank anonymous reviewers for their comments.
Yao Chen is currently with the Dept. of Computer Science,
Southwestern Univ. of Finance and Economics, Chengdu,
China. The research related to this paper is supported in
part by Hong Kong RGC GRF PolyU 152002/18E, PolyU
152164/14E, PolyU 152703/16E, RGC ECS PolyU 5328/12E,
RGC Germany/HK Joint Research Scheme G-PolyU503/16,
The Hong Kong Polytechnic Univ. fund G-YBMW, G-YBXW,
G-YN37, G-UA7L, 1-BBWC, 4-ZZHD, 1-ZVJ1, G-YBXU.
REFERENCES
[1] L. Sha, S. Gopalakrishnan, X. Liu, and Q. Wang, “Cyber-physical sys-
tems: A new frontier,” in Machine Learning in Cyber Trust . Springer,
2009, pp. 3–13.
125
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. [2] M. Anitha, R. Sanjai, W. Michael, and H. M. PE, “Design considera-
tions for modeling modes in cyber–physical systems,” IEEE Design &
Test, vol. 32, no. 5, pp. 66–73, 2015.
[3] J. Geismann, C. Gerking, and E. Bodden, “Towards ensuring security
by designing in cyber-physical systems engineering processes,” Proc.
of ICSSP , May 2018.
[4] J. L. Lions, Ariane 5 ﬂight 501 failure: report by the inquiry board ,
Paris, July 19 1996.
[5] “Longer video of ‘ariane 5’ rocket ﬁrst launch failure/explosion,” http:
//www.youtube.com, Sep. 2010, [Online: accessed Jan-2018].
[6] J. Ayre, “16,000 ﬁat 500e electric cars recalled for software issue,”
Clean Technica , June 6 2016.
[7] J. Hirsch and K. Bensinger, “Toyota settles acceleration lawsuit after
$3-million verdict,” Los Angeles Times , Oct 25 2013.
[8] K. Poulsen, “Software bug contributed to blackout,” Securityfocus.com,
Feb 11 2004, [Online; accessed Jan-2018].
[9] ——, “Tracking the blackout bug,” Securityfocus.com, April 7 2004,
[Online; accessed Jan-2018].
[10] “Therac-25,” https://en.wikipedia.org/wiki/Therac-25, Jan. 2018, [On-
line; accessed Jan-2018].
[11] G. F. Franklin, J. D. Powell, and A. Emami-Naeini, Feedback control
of dynamic systems (7th ed.) . Pearson, 2014.
[12] W. L. Brogan, Modern control theory (3rd ed.) . Pearson, 1990.
[13] W. E. Wong, R. Gao, Y . Li, R. Abreu, and F. Wotawa, “A survey on
software fault localization,” IEEE Trans. on Software Engineering , vol.
42(8):707-740, Aug. 2016.
[14] E. T. Barr, M. Harman, P. McMinn, M. Shahbaz, and S. Yoo, “The or-
acle problem in software testing: a survey,” IEEE TSE , vol. 41(5):507-
525, May 2015.
[15] G. Gay, M. Staats, M. Whalen, and M. P. Heimdahl, “Automated oracle
data selection support,” IEEE Trans. on Software Engineering , vol.
41(11), Nov. 2015.
[16] G. Fraser and A. Zeller, “Mutation-driven generation of unit tests and
oracles,” IEEE Trans. on Software Engineering , vol. 38(2):278-292,
Mar/Apr 2012.
[17] T. Xie, “Augmenting automatically generated unit-test suits with re-
gression oracle checking,” Proc. of the 20th European Conf. on Object-
Oriented Programming (ECOOP 2006) , pp. 380–403, Jul. 2006.
[18] W. Zheng, H. Ma, M. R. Lyu, T. Xie, and I. King, “Mining test oracles
of web search engines,” Proc. of the 26th IEEE/ACM Intl. Conf. on
Automated Software Engineering , pp. 408–411, Nov. 2011.
[19] T. Y . Chen, T. H. Tse, and Z. Q. Zhou, “Semi-proving: an integrated
method for program proving, testing, and debugging,” IEEE TSE , vol.
37(1):109-125, 2011.
[20] E. J. Weyuker, “On testing non-testable programs,” The Computer J. ,
vol. 25(4):465-470, 1982.
[21] W. E. Howden, “Introduction to the theory of testing,” Edward Miller
and William E. Howden, editors, Tutorial: Software Testing and V ali-
dation Techniques , pp. 16–19, 1978.
[22] E. Denney, G. Pai, and I. Whiteside, “Model-driven development of
safety architectures,” Proc. of ACM/IEEE 20th Intl. Conf. on Model
Driven Engineering Languages and Systems (MODELS) , pp. 156–166,
2017.
[23] P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Mine, D. Monniaux,
and X. Rival, “The astree analyzer,” Proc. of ESOP , 2005.
[24] T. Nguyen, D. Kapur, W. Weimer, and S. Forrest, “Using dynamic
analysis to generate disjunctive invariants,” Proc. of ICSE , pp. 608–
619, 2014.
[25] ——, “Using dynamic analysis to discover polynomial and array
invariants,” Proc. of ICSE , pp. 683–693, 2012.
[26] C. Csallner, N. Tillmann, and Y . Smaragdakis, “DySy: dynamic sym-
bolic execution for invariant inference,” Proc. of ICSE ,pp. 281–290,
2008.
[27] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin, “Dynamically
discovering likely program invariants to support program evolution,”
IEEE TSE , vol. 27(2):99-123, Feb. 2001.
[28] A. Kane, T. Fuhrman, and P. Koopman, “Monitor based oracles for
cyber-physical system testing,” Proc. of the 44th Annual IEEE/IFIP
Intl. Conf. on Dependable Systems and Networks (DSN) , pp. 148–155,
2014.
[29] P. Loyola, M. Staats, I.-Y . Ko, and G. Rothermel, “Dodona: automated
oracle data set selection,” Proc. of ISSTA’14 , pp. 193–203, 2014.
[30] F. Pastore, L. Mariani, and G. Fraser, “CrowdOracle: can the crowd
solve the oracle problem?” Proc. of Intl. Conf. on Software Testing,
V eriﬁcation, V alidation , pp. 342–351, 2013.[31] H. Malik, H. Hemmati, and A. E. Hassan, “Automatic detection of
performance deviations in the load testing of large scale systems,” Proc.
of ICSE’13 , pp. 1012–1021, 2013.
[32] S. Afshan, P. McMinn, and M. Stevenson, “Evolving readable string
test inputs using a natural language model to reduce human oracle cost,”
Proc. of the 6th IEEE Intl. Conf. on Software Testing, V eriﬁcation and
V alidation , pp. 352–361, 2013.
[33] P. McMinn, M. Stevenson, and M. Harman, “Reducing qualitative
human oracle costs associated with automatically generated test data,”
ACM ISSTA’10 , July 12-16 2010.
[34] E. F. Camacho and C. B. Alba, Model predictive control . Springer
Science & Business Media, 2013.
[35] U. Eren, A. Prach, B. Kocer, S. Rakovic, E. Kayacan, and B. Acikmese,
“Model predictive control in aerospace systems: Current state and
opportunities,” AIAA Journal of Guidance, Control, and Dynamics ,
2017.
[36] S. Kannan, S. A. S. Alamdari, J. Dentler, M. A. Olivares-Mendez, and
H. V oos, “Model predictive control for cooperative control of space
robots,” AIP Conf. Proc. , vol. 1798(1), Jan. 2017.
[37] J. Dentler, S. Kannan, M. A. O. Mendez, and H. V oos, “A tracking error
control approach for model predictive position control of a quadrotor
with time varying reference,” Proc. of the IEEE Intl. Conf. on Robotics
and Biomimetics , Dec. 3-7 2016.
[38] L. Sebeke, X. Luo, B. de Jager, W. P. M. H. Heemels, E. Heijman, and
H. Gruell, “Model predictive control algorithm for large-area regional
hyperthermia,” Journal of Therapeutic Ultrasound , vol. 4:A173, 2016.
[39] J. Yang, R. Grosu, S. A. Smolka, and A. Tiwari, “Love thy neighbor: V-
formation as a problem of model predictive control,” Proc. of the 27th
Intl. Conf. on Concurrency Theory (CONCUR) , vol. LIPICS 59:4:1-5,
Aug. 2016.
[40] A. Jain, E. Biyik, and A. Chakrabortty, “A model predictive control
design for selective modal damping in power systems,” Proc. of
American Control Conference , 2015.
[41] A. Weiss, M. Baldwin, R. S. Erwin, and I. Kolmanovsky, “Model
predictive control for spacecraft rendezvous and docking: strategies for
handling constraints and case studies,” IEEE Trans. on Control Systems
Tech. , vol. 23(4):1638-1647, 2015.
[42] A. K. Tangirala, Principles of System Identiﬁcation: Theory and
Practice . CRC Press, 2015.
[43] L. Ljung, System Identiﬁcation: Theory for the User (2nd Ed.) . Pren-
tice Hall, Jan. 1999.
[44] NIST/SEMATECH, “e-handbook of statistical methods (chapter
7.1.6),” http://www.itl.nist.gov/div898/handbook/, 2018, [Online; ac-
cessed april-2018].
[45] “Outlier,” https://en.wikipedia.org/wiki/Outlier, Jan. 2018, [Online; ac-
cessed 27-Jan-2018].
[46] “ARDUPILOT,” http://ardupilot.org/, 2018, [Online; accessed Jan-
2018].
[47] “3dr site scan,” https://3dr.com, 2017, [Online; accessed Jan-2018].
[48] “Aeromao,” http://www.aeromao.com, Jan. 2018, [Online: accessed
Jan-2018].
[49] D. Bozhinoski, D. D. Ruscio, I. Malavolta, P. Pelliccione, and
M. Tivoli, “FLY AQ: enabling non-expert users to specify and generate
missions of autonomous multicopters,” Proc. of the 30th IEEE/ACM
Intl. Conf. on Automated Software Engineering , 2015.
[50] J.-J. E. Slotine and W. Li, Applied nonlinear control .Pearson, 1991.
[51] “Unity - game engine,” https://unity3d.com, 2018, [Online; accessed
Jan-2018].
[52] “Ardupilot github repository,” https://github.com/ArduPilot/ardupilot,
Jan. 2018, [Online: accessed Jan-2018].
[53] J. A. Jones and M. J. Harrold, “Empirical evaluation of the tarantula
automatic fault-localization technique,” in Proceedings of the 20th
IEEE/ACM international Conference on Automated Software Engineer-
ing. ACM, 2005, pp. 273–282.
[54] E. Wong, T. Wei, Y . Qi, and L. Zhao, “A crosstab-based statistical
method for effective fault localization,” in Software Testing, V eriﬁca-
tion, and V alidation, 2008 1st International Conference on . IEEE,
2008, pp. 42–51.
[55] W. E. Wong and Y . Qi, “Bp neural network-based effective fault local-
ization,” International Journal of Software Engineering and Knowledge
Engineering , vol. 19, no. 04, pp. 573–597, 2009.
[56] “Box plot,” https://en.wikipedia.org/wiki/Box plot/, Jan. 2018, [Online;
accessed Jan-2018].
126
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. [57] “Quartile,” https://en.wikipedia.org/wiki/Quartile, Jan. 2018, [Online;
accessed Jan-2018].
[58] R. Natella, D. Cotroneo, J. A. Duraes, and H. S. Madeira, “On fault
representativeness of software fault injection,” IEEE Transactions on
Software Engineering , vol. 39, no. 1, pp. 80–96, 2013.
[59] “p-value,” https://en.wikipedia.org/wiki/P-value, Jan. 2018, [Online;
accessed 28-jan-2018].
[60] J. Cohen, Statistical Power Analysis for the Behavioral Sciences .
Routledge, 2013.
[61] “Effect size,” https://en.wikipedia.org/wiki/Effect size, 2018, [Online;
accessed Jan-2018].
[62] C. Wohlin, P. Runeson, M. Host, M. C. Ohlsson, B. Regnell, and
A. Wesslen, Experimentation in Software Engineering: An Introduction .
Kluwer Academic Publishers, 2000.
[63] “Ardupilot discuss forum,” https://discuss.ardupilot.org/t/
questions-about-cruise-speed-and-cruise-throttle/26359, Mar. 2018,
[Online; accessed April-2018].
[64] “Ardupilot discuss forum,” https://discuss.ardupilot.org/t/
about-cruise-speed-and-cruise-throttle/27782, Apr. 2018, [Online;
accessed April-2018].
[65] T. Yu, W. Srisa-an, and G. Rothermel, “An empirical comparison of
the fault-detection capabilities of internal oracles,” Proc. of Intl. Symp.
on Software Reliability Engineering (ISSRE) , pp. 11–20, 2013.
[66] L. K. Dillon and Q. Yu, “Oracles for checking temporal properties of
concurrent systems,” Proc. of FSE , 1994.
[67] T. Kuchta, T. Lutellier, E. Wong, L. Tan, and C. Cadar, “On the
correctness of electronic documents: Studying, ﬁnding, and localizing
inconsistency bugs in pdf readers and ﬁles,” Proc. of FSE-18/EMSE-18
(Journal First) , 2018.
[68] C. L. Goues, Y . Brun, S. Forrest, and W. Weimer, “Clariﬁcations on
the construction and use of the manybugs benchmark,” IEEE TSE , vol.
43(11):1089-1090, Nov. 2017.
[69] J. Cleland-Huang, O. C. Z. Gotel, J. H. Hayes, P. Mader, and A. Zis-
man, “Software traceability: trends and future directions,” Proc. of
FOSE , pp. 55–69, 2014.
[70] T. L. Nguyen, P. Schrammel, B. Fischer, S. L. Torre, and G. Parlato,
“Parallel bug-ﬁnding in concurrent programs via reduced interleaving
instances,” Proc. of ASE , 2017.
[71] P. S. Kochhar, F. Thung, and D. Lo, “Code coverage and test suit
effectiveness: Empirical study with real bugs in large systems,” Proc.
of IEEE SANER , 2015.
[72] M. Pradel and T. R. Gross, “leveraging test generation and speciﬁcation
mining for automated bug detection without false positives,” Proc. of
ICSE , Jun. 2012.
[73] A. Balkan, P. Tabuada, J. V . Deshmukh, X. Jin, and J. Kapinski, “Un-
derminer: a framework for automatically identifying non-converging
behaviors in black box system models,” Proc. of EMSOFT’16 , Oct.
2016.
[74] R. Medhat, S. Ramesh, B. Bonakdarpour, and S. Fischmeister, “A
framework for mining hybrid automata from input/output traces,” Proc.
of the 12th Intl. Conf. on Embedded Software (EMSOFT’15) , pp. 177–
186, 2015.
[75] H. Choi, W.-C. Lee, Y . Aafer, F. Fei, Z. Tu, X. Zhang, D. Xu,
and X. Deng, “Detecting attacks against robotic vehicles: A control
invariant approach,” Proc. of CCS’18 , 2018.
[76] M. H. Heule and S. Verwer, “Software model synthesis using satisﬁ-
ability solvers,” Empirical Software Engineering , vol. 13(4):825-856,
Aug. 2013.
[77] M. Merten, F. Howar, B. Steffen, P. Pellicione, and M. Tivoli, “Auto-
mated inference of models for black box systems based on interface
descriptions,” Proc. of the 5th Intl. Conf. Leveraging Appl. of F ormal
Methods, V eriﬁcation and V alidation: Tech. for Mastering Change , vol.
LNCS 7609:79-96, 2012.
[78] D. K. Peters and D. L. Parmas, “Using test oracles generated from
program documentation,” IEEE Trans. on Software Engineering , vol.
24(3):161-173, Mar. 1998.
[79] V . Dallmeier, C. Lindig, A. Wasylkowski, and A. Zeller, “Mining object
behavior with adabu,” pp. 17–24, 2006.[80] G. Fraser and A. Arcuri, “Evosuite: Automatic test suite generation
for object-oriented software,” Proceedings of the 19th ACM SIGSOFT
Symposium and the 13th European Conference on F oundations of
Software Engineering (ESEC/FSC’11) , pp. 416–419, 2011.
[81] J. Chen, Y . Bai, D. Hao, L. Zhang, L. Zhang, B. Xie, and H. Mei,
“Supporting oracle construction via static analysis,” Proc. of the 31st
IEEE/ACM Intl. Conf. on Automated Software Engineering (ASE) , pp.
178–189, 2016.
[82] C. Alippi, S. Ntalampiras, and M. Roveri, “Model-free fault detection
and isolation in large-scale cyber-physical systems,” IEEE Trans. on
Emerging Topics in Computational Intelligence , vol. 1(1):61-71, Feb.
2017.
[83] X. Xie, W. E. Wong, T. Y . Chen, and B. Xu, “Metamorphic slice:
an application in spectrum-based fault localization,” Info. & Software
Tech. , vol. 55:866-879, 2013.
[84] T. Y . Chen, F.-C. Kuo, and W. K. Tam, “Testing a software-based pid
controller using metamorphic testing,” Proc. of the 1st Intl. Conf. on
Pervasive and Embedded Computing and Communication Systems , pp.
387–396, 2011.
[85] T. Y . Chen, S. C. Cheung, and S. M. Yiu, “Metamorphic testing: a new
approach for generating next test cases,” The HKUST CS Tech. Report
HKUST-CS98-01 , 1998.
[86] R. Goebel, R. G. Sanfelice, and A. R. Teel, Hybrid dynamical systems:
modeling, stability, and robustness . Princeton University Press, 2012.
[87] P. S. Duggirala, L. Wang, S. Mitra, M. Viswanathan, and C. Munoz,
“Temporal precedence checking for switched models and its application
to a parallel landing protocol,” Proc. of F ormal Methods (FM) , vol.
LNCS 8442:215-229, May 2014.
[88] P. Tabuada, V eriﬁcation and Control of Hybrid Systems: A Symbolic
Approach . Springer, 2009.
[89] S. Bayraktar, G. E. Fainekos, and G. J. Pappas, “Hybrid modeling
and experimental cooperative control of multiple unmanned aerial
vehicles,” Tech. Rprt. MS-CIS-04-32, Dept. of CIS, U. of Penn. , Dec.
2004.
[90] O. Sokolsky and H. S. Hong, “Qualitative modeling of hybrid systems,”
Proc. of Workshop on F ormal Models in Software Development , Jun.
2001.
[91] R.-K. Doong and P. G. Frankl, “The ASTOOT approach to testing
object-oriented programs,” ACM TSEM , vol. 3(2):101-130, Apr. 1994.
[92] J. M. Spivey, Z notation - a reference manual (2nd ed.) . Westview,
2014.
[93] J. M. Wing, “A speciﬁer’s introduction to formal methods,” IEEE
Computer , vol. 23(9):8-24, Sep. 1990.
[94] B. Meyer, “Eiffel: a language and environment for software engineer-
ing,” J. of Systems and Software , vol. 8(3):199-246, Jun. 1988.
[95] M. R. Lyu, Software fault tolerance . Wiley, 1995.
[96] S. Yoo and M. Harman, “Regression testing minimization, selection
and prioritization: a survey,” J. of Software Testing, V eriﬁcation &
Reliability , vol. 22(2):67-120, Mar. 2012.
[97] C. S. Timperley, A. Afzal, D. Katz, J. M. Hernandez, and C. L. Goues,
“Crashing simulated planes is cheap: Can simulation detect robotics
bug early?” Proc. of the 11th Intl. Conf. on Software Testing, V alidation,
and V eriﬁcation (ICST) , 2018.
[98] R. Matinnejad, S. Nejati, L. C. Briand, and T. Bruckmann, “Automated
test suite generation for time-continuous simulink models,” Proc. of
ICSE’16 , pp. 595–606, May 14-22 2016.
[99] D. Giannakopoulou, N. Rungta, and M. Feary, “Automated test case
generation for an autopilot requirement prototype,” Proc. of IEEE SMC ,
2011.
[100] B. Liu, Lucia, S. Nejati, L. C. Briand, and T. Bruckmann, “Simulink
fault localization: an iterative statistical debugging approach,” Software
Testing, V eriﬁcation and Reliability , vol. 26:431-459, May 11 2016.
[101] J. Deshmukh, X. Jin, R. Majumdar, and V . Prabhu, “Parameter
optimization in control software using statistical fault localization
techniques,” Proc. of ACM/IEEE Intl. Conf. on Cyber-Physical Systems
(ICCPS) , pp. 220–231, Apr. 2018.
127
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:58:45 UTC from IEEE Xplore.  Restrictions apply. 