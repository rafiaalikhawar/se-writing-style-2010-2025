An Empirical Investigation into the Nature of Test Smells
Michele Tufano1, Fabio Palomba2, Gabriele Bavota3, Massimiliano Di Penta4
Rocco Oliveto5, Andrea De Lucia2, Denys Poshyvanyk1
1The College of William and Mary, USA —2University of Salerno, Italy —3Università della Svizzera
italiana (USI), Switzerland —4University of Sannio, Italy —5University of Molise, Italy
ABSTRACT
Test smells have been deﬁned as poorly designed tests and,
as reported by recent empirical studies, their presence may
negatively a↵ect comprehension and maintenance of test
suites. Despite this, there are no available automated tools
to support identiﬁcation and repair of test smells. In this
paper, we ﬁrstly investigate developers’ perception of test
smells in a study with 19 participants. The results show that
developers generally do not recognize (potentially harmful)
test smells, highlighting that automated tools for identify-
ing such smells are much needed. However, to build e↵ective
tools, deeper insights into the test smells phenomenon are
required. To this aim, we conducted a large-scale empir-
ical investigation aimed at analyzing (i) when test smells
occur in source code, (ii) what their survivability is, and
(iii) whether their presence is associated with the presence
of design problems in production code (code smells). The
results indicate that test smells are usually introduced when
the corresponding test code is committed in the repository
for the ﬁrst time, and they tend to remain in a system for
a long time. Moreover, we found various unexpected rela-
tionships between test and code smells. Finally, we show
how the results of this study can be used to build e↵ective
automated tools for test smell detection and refactoring.
CCS Concepts
•Software and its engineering !Software evolution;
Keywords
Test Smells, Mining Software Repositories, Software Evolu-
tion
1. INTRODUCTION
Testing represents a signiﬁcant part of the whole software
development e↵ort [9]. When evolving a software system,
developers evolve test suites as well by repairing them when
needed and by updating them to sync with the new versionof the system. To ease developers’ burden in writing, or-
ganizing, and executing test suites, nowadays appropriate
frameworks ( e.g., JUnit [9])—conceived for unit testing but
also used beyond unit testing—are widely adopted.
Concerning other code artifacts (in the following referred
to as “production code”) researchers have provided deﬁni-
tions of symptoms of poor design choices, known as “code
smells” [13] for which refactoring activities are desirable.
Subsequently, researchers developed automated tools to de-
tect them ( e.g., [6, 40]), and empirically studied the de-
velopers’ awareness of such smells [29] as well as the rela-
tionship between smells and negative phenomena such as
higher fault- and change-proneness [18]. At the same time,
researchers have also confuted some common wisdom about
“software aging”, showing that often the presence of code
smells is not necessarily due to repeated changes performed
on source code artifacts during their evolution, rather in
most cases smells are introduced when such artifacts are cre-
ated [41]. In conclusion, both researchers and practitioners
are carefully looking at the code smell identiﬁcation, and,
nowadays, smell detection is often included as part of con-
tinuous integration and delivery processes [5, 12, 39].
A quite related phenomenon to code smells can also oc-
cur in test suites, which can be a↵ected by test smells .T e s t
smells—deﬁned by van Deursen et al. [42]—are caused by
poor design choices (similarly to code smells) when develop-
ing test cases: the way test cases are documented or orga-
nized into test suites, the way test cases interact with each
other, with the production code and with external resources
are all indicators of possible test smells. For instance, Mys-
tery Guest occurs when a test case is using an external re-
source, such as a ﬁle or a database (thus, making the test
not self-contained), and Assertion Roulette when a test case
contains multiple assertions without properly documenting
all of them [42].
Empirical studies have shown that test smells can hinder
the understandability and maintainability of test suites [7],
and refactoring operations aimed at removing them have
been proposed [42]. Nevertheless, it is still not clear how
developers perceive test smells and whether they are aware
of them at all. Also, it is not known whether test smells are
introduced as such when test suites are created, or whether
test suites become “smelly” during software evolution, and
whether developers perform any refactoring operations to
remove test smells. Such information is of paramount im-
portance for designing smell detection rules and building
automated detection tools to be incorporated in the devel-
opment process, and especially in the continuous integra-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE’16 , September 3–7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970340
4
tion processes [12], where automated tools could identify test
smells and, because of that, make the build fail and notify
developers about the presence of the test smells. Highlight-
ing test smells in scenarios where it is known that develop-
ers do not want and need to maintain them— e.g., because
there is no better solution—would make automated smell
detection tools usable, avoiding recommendation overload
[27] and even build failures.
Paper contribution. This paper reports a thorough em-
pirical investigation into the perceived importance of test
smells and of their lifespan across software projects’ change
histories. First, we conducted a survey with 19 develop-
ers assessing whether developers could recognize instances
of test smells in software projects. Such a survey obtained a
clear negative result, indicating that, unlike what previously
found for code smells [29], there is basically no awareness
about test smells, highlighting the need for (semi-) auto-
matic support to aid in detecting these design issues. Thus,
we conducted a mining study over the change history of 152
software projects to gather the deeper knowledge needed to
design e↵ective test smells detectors. In the context of this
study we investigate (i) when test smells are introduced; (ii)
how long test smells survive (and whether developers try to
remove them); and (iii) whether test smells are related to
the presence of smells in production code, and, therefore,
there can be synergies in their detection. The achieved re-
sults indicate that (i) test smells mostly appear as the result
of bad design choices made during the creation of the test
classes, and not as the result of design quality degradation
over time, (ii) test smells stay in the system for a long time,
with a probability of 80% that a test smell would not be
ﬁxed after 1,000 days from its introduction, and (iii) com-
plex classes ( e.g., Blob classes) in the production code are
often tested by smelly test classes, thus, highlighting a rela-
tionship existing between code and test smells.
Paper structure. Section 2 describes the survey we per-
formed to investigate developers’ perception of test smells.
Section 3 details the mining study deﬁnition and planning,
while the results are reported in Section 4. Threats to the
studies’ validity are discussed in Section 5. After a discus-
sion of the related work (Section 6), Section 7 concludes the
paper and outlines directions for future work.
2. TEST SMELL PERCEPTION
In this section, we report the design and the results of a
survey we conducted with the aim of understanding whether
developers perceive test smells as design problems. Speciﬁ-
cally, we aim at answering the following research question:
•RQ 0:Are test smells perceived by developers as actual
design problems?
Our study focuses on ﬁve types of test smells from the
catalogue by van Deursen et al. [42]:
1.Assertion Roulette (AR) : As deﬁned by van Deursen
et al. this smell comes from having a number of asser-
tions in a test method that have no explanation [42].
Thus, if an assertion fails, the identiﬁcation of the as-
sert that failed can be di cult. Besides removing the
unneeded assertions, to remove this smell and make
the test more clear an operation of Add Assertion Ex-
planation can be applied [42].2.Eager Test (ET) :At e s ti sa ↵ e c t e db y Eager Test when
it checks more than one method of the class to be
tested [42], making the comprehension of the actual
test target di cult. The problem can be solved by
applying Extract Method refactoring, splitting the test
method in order to specialize its responsibilities [13].
3.General Fixture (GF) : A test class is a↵ected by this
smell when the setUp method is too generic and the
test methods only access part of it [42]. In other words,
the test ﬁxture is not only responsible for setting the
common environment for all the test methods. A suit-
able refactoring to remove the smell is the Extract
Method , which reorganizes the responsibilities of the
setUp method [13].
4.Mystery Guest (MG) : This smell arises when a test
uses external resources ( e.g., a ﬁle containing test
data), and thus it is not self-contained [42]. Tests con-
taining such a smell are di cult to comprehend and
maintain, due to the lack of information to understand
them. To remove a Mystery Guest aSetup External
Resource operation is needed [42].
5.Sensitive Equality (SE) : When an assertion contains
an equality check through the use of the toString
method, the test is a↵ected by a Sensitive Equality
smell. In this case, the failure of a test case can depend
on the details of the string used in the comparison, e.g.,
commas, quotes, spaces etc.[42]. A simple solution for
removing this smell is the application of an Introduce
Equality Method refactoring, in which the use of the
toString is replaced by a real equality check.
While several other smells exist in the literature [42], we
decided to limit our analysis to a subset of such smells in
order to focus the questions for survey’s participants on a
few smell types, allowing to collect more opinions for the
same smell. However, we take into account a fairly diverse
catalogue of test smells, which are related to di↵erent char-
acteristics of test code. As reported in a previous work [8],
our selection includes test smells having the greatest di↵u-
sion in both industrial and open source projects.
To answer RQ 0, we invited the original developers of ﬁve
projects from the Apache and Eclipse ecosystems, namely
Apache James Mime4j ,Apache JSPWiki ,Apache POI ,
Eclipse Mylyn , and Eclipse Platform UI . These projects rep-
resent a subset of those considered in our larger mining study
described in Section 3, and they were selected because they
contain all types of smells from the considered catalogue.
For this study, smells were manually identiﬁed by one of the
authors and double-checked by another author.
We chose to involve original developers rather than ex-
ternal developers ( i.e., developers with no experience with
the subject systems) since we wanted to collect the opin-
ions of developers that actually developed the systems un-
der analysis and, therefore, have a good knowledge about
the rationale behind the design choices applied during the
development of such systems. In total, we invited 298 de-
velopers receiving responses from 19 of them: three from
Apache James Mime4j , one from Apache JSPWiki , six from
Apache POI , one from Eclipse Mylyn , and eight from Eclipse
Platform UI . Note that even though the number of respon-
dents appears to be low (6.3% response rate), our results are
close to the suggested minimum response rate for the survey
studies, which is deﬁned around 10% [16].
52.1 Survey Questionnaire Design
The general idea behind the study design was to show, to
each developer, one test smell instance of each type. This is
done to avoid having a long questionnaire that might have
discouraged developers to take part in our study. For each
test smell instance, the study participants had to look at the
source code and answer the following questions:
1.In your opinion, does this class have any design prob-
lem? Please, rate your opinion from 1=strongly dis-
agree to 5=strongly agree.
2.Ifyou agreed or strongly agreed to the question
number 1, please explain what are, in your opinion,
the design problems of this class.
3.Ifyou agreed or strongly agreed to the question
number 1, please explain why the design problem has
been introduced.
4.Ifyou agreed or strongly agreed to the question
number 1, do you think that this class needs to be
refactored? Please, rate your opinion from 1=strongly
disagree to 5=strongly agree.
5.Ifyou agreed or strongly agreed to the question
number 4, how would you refactor this class?
The survey was designed to be completed within approxi-
mately 30 minutes.
To automatically collect the answers, the survey was
hosted using a Web application, eSurveyPro1. Developers
were given 20 days to respond to the survey. Note that the
Web application allowed developers to complete the ques-
tionnaire in multiple rounds, e.g., to answer the ﬁrst two
questions in one session and ﬁnish the rest later. At the
end of the response period, we collected developers’ answers
of the 19 complete questionnaires in a spreadsheet in order
to perform data analysis. Note that the developers of the
ﬁve systems were invited to evaluate only the test smells
detected in the system they contribute to.
2.2 Analysis Method
To answer RQ 0we computed:
1.The distribution of values assigned by developers when
evaluating whether the analyzed test classes had a de-
sign problem (question #1 of the survey).
2.The percentage of times the smell has been identi-
ﬁedby the participants. By identiﬁed we mean cases
where participants, besides perceiving the presence of
a smell, were also able to identify the exact smell af-
fecting the analyzed test code, by describing it when
answering question #2 of the survey. Note that we
consider a smell as identiﬁed only if the design prob-
lems described by the participant are clearly traceable
onto the deﬁnition of the test smell a↵ecting the code
component.
3.The distribution of values assigned by developers when
evaluating whether the test classes analyzed should be
refactored (question #4 of the survey).
4.The percentage of times the refactoring of the test
smell has been identiﬁed by the participants (ques-
tion #5 of the survey). In this case, by identiﬁed we
mean cases where participants correctly identiﬁed how
1http://www.esurveyspro.comTable 1: Answers for questions #1 and #4.
QuestionAnswer
12 3 4 5
#1 78 1484
#4 87 1 1 7 0
the design problems a↵ecting the test class should be
removed.
Moreover, we collected the answers to question #3 in order
to understand the reasons why test smells are introduced.
2.3 Analysis of the Results
Table 1 reports the distribution of values assigned by de-
velopers when answering the questions #1 and #4, respec-
tively. We can see that often developers do not perceive test
smells as actual problems. Indeed, only in 17 cases (out of
the 95 total test smells analyzed by the developers) a design
ﬂaw has been identiﬁed ( i.e., answers to question #1 with
value >1). These answers come from only ﬁve developers
(out of 19). In these cases, however, participants were of-
ten not able to correctly diagnose the test smell a↵ecting
the analyzed test code (only in 2% of the cases developers
correctly identiﬁed a test smell). Moreover, when analyzing
the answers to question #4 of the survey, we found that al-
most always (91% of the cases) participants did not feel that
the refactoring activity would be beneﬁcial to improve the
design of the considered test classes. For this reason, devel-
opers were not able to provide good suggestions for possible
refactoring operations (question #5).
The most important feedback we obtained from this study
is related to the answers provided when answering question
#3. As an example, analyzing an instance of Eager Test ,a
developer from Apache POI claimed that “probably the code
was written in a hurry and was never reviewed” . Also, a de-
veloper from Eclipse Platform UI claimed that “the code
analyzed was introduced in 2002 and hasn’t got much atten-
tion”. This feedback highlights that, even when perceiving
design ﬂaws, developers are not able to correctly identify
and explain the reasons behind test smell introduction. This
constitutes the need for automated tool support in order to
alert developers about the presence of test smells in the test
code that they produce.
3. TEST SMELL LIFECYCLE: DESIGN
Thegoal of the study is to analyze the change history of
software projects, with the purpose of investigating when
test smells are introduced by developers, what is their sur-
vivability, and whether they are associated with code smells
in production code. The context of the study consists of 152
open source projects belonging to two ecosystems (Apache
and Eclipse) for which we investigated the presence and evo-
lution of test smells.
3.1 Research Questions and Context
The study aims at answering the following RQs:
•RQ 1:When are test smells introduced? This research
question aims at assessing whether test smells are in-
troduced as a consequence of maintenance activities
performed on test classes or whether they are intro-
duced as soon as the corresponding test class is com-
mitted to the repository for the ﬁrst time. Results of
6Table 2: Ecosystems under analysis.
Ecosystem #Proj. #Classes KLOC #CommitsMean Story Min-Max
Length Story Length
Apache 164 4-5,052 1-1,031 207,997 6 1-15
Eclipse 26 142-16,700 26-2,610 264,119 10 1-13
Overall 190 -- 472,116 6 1-15
RQ 1will help understand the kind of automatic de-
tection tools developers need to identify test smells,
and the way such tools should be integrated in the
development process. Indeed, if test smells are intro-
duced as the result of continuous maintenance and evo-
lution activities, then detectors can be executed peri-
odically, or as a part of a continuous build process, as
it happens with approaches proposed in the literature
to catch code smells ( e.g., [25, 30]). Instead, if test
smells are introduced when the test code is written
in the ﬁrst place, then just-in-time refactoring tools
could be built; such tools should continuously monitor
the (test) code written in the IDE, alerting the devel-
oper when it is deviating from good design practices,
thus avoiding the introduction of the (test) smells in
the ﬁrst place.
•RQ 2:What is the longevity of test smells? This re-
search question aims at analyzing the lifetime of test
smells, with the goal of understanding to what extent
they remain in a software project from their introduc-
tion until their (possible) removal. Long living test
smells are likely to indicate design issues di cult to
catch for software developers, thus indicating the need
for (semi-)automated detection tools.
•RQ 3:Are test smells associated with particular code
smells a↵ecting the production code? In this research
question, we intend to assess whether test smells are
usually associated with design problems occurring in
production code. While test and code smells have a
di↵erent nature, uncovering relationships between the
two can highlight possible synergies in their detection
and refactoring recommendation.
Thecontext of the study consists of 190 software projects
belonging to two di↵erent ecosystems, i.e., those managed
by the Apache Software Foundation and by the Eclipse
Foundation. Table 2 reports for each ecosystem (i) the
number of projects analyzed, (ii) size ranges in terms of
the number of classes and KLOC, (iii) the overall number
of commits analyzed, and (iv) the average, minimum, and
maximum length of the projects’ history (in years) analyzed
in each ecosystem. All the analyzed projects are hosted in
Gitrepositories. The Apache ecosystem consists of 164 Java
systems randomly selected among those available2, while the
Eclipse ecosystem consists of 26 projects randomly mined
from the list of GitHub repositories managed by the Eclipse
Foundation3. The choice of the ecosystems to analyze is not
random, but guided by the will to consider projects hav-
ing (i) di↵erent scope, (ii) di↵erent sizes, and (iii) di↵erent
architectures, e.g., we have Apache libraries as well as plug-
in based architectures in Eclipse projects. From the orig-
inal dataset, we discarded projects having no test case in
their entire change history. This was the case for 36 Apache
2https://projects.apache.org/indexes/quick.html
3https://github.com/eclipseTable 3: Test cases found in the analyzed projects.
Ecosystem Min. 1st Qu. Median Mean 3rd Qu. Max.
Apache 1 9 35 78 96 553
Eclipse 1 15 32 102 106 809
projects and two Eclipse projects, thus leading to 152 to-
tal projects analyzed. Table 3 reports the distribution of
number of test cases identiﬁed in the 152 analyzed projects.
The complete list of projects and the number of test cases
identiﬁed is available in our (anonymized) online appendix
[1].
The study focuses on the ﬁve types of test smells also con-
sidered in the survey: Assertion Roulette ,Eager Test ,Gen-
eral Fixture ,Mystery Guest , and Sensitive Equality .A l s o ,
since in the context of RQ 3we assess the possible relation-
ship between test and code smells, in our study we consider
the following types of code smells:
1.Blob Class : a large class with di↵erent responsibilities
that monopolizes most of the system’s processing [10];
2.Class Data Should be Private : a class that exposes
its attributes, thus, violating the information hiding
principle [13];
3.Complex Class : a class having a high cyclomatic com-
plexity [10];
4.Functional Decomposition : a class where inheritance
and polymorphism are poorly used, declaring many
private ﬁelds and implementing few methods [10];
5.Spaghetti Code : a class without structure that declares
long methods without parameters [10].
This analysis is limited to a subset of the smells that ex-
ist in the literature [10, 13] due to the computational con-
straints. However, we preserve a mix of smells related to
complex/large code components ( e.g., Blob Class, Com-
plex Class) as well as smells related to the lack of adoption
of good Object-Oriented coding practices ( e.g., Class Data
Should be Private, Functional Decomposition).
3.2 Data Extraction
To answer our research questions, we ﬁrstly cloned the Git
repositories of the subject software systems. Then, we mined
the evolution history of each repository using a tool that we
developed, named HistoryMiner : it checks out each com-
mit of a repository in chronological order and identiﬁes the
code ﬁles added and modiﬁed in each speciﬁc commit. Note
that only java ﬁles are considered as code ﬁles, while the
remaining ﬁles are discarded ( e.g., building ﬁles, images).
3.2.1 Artifact Lifetime Log
For each commit HistoryMiner classiﬁes code ﬁles in two
sets: test classes andproduction classes . Speciﬁcally, a ﬁle
is classiﬁed as a test class if the corresponding Java Class
extends the class junit.framework.TestCase . Those code
ﬁles, which are not classiﬁed as test classes , are considered as
production classes . Finally, for each test class identiﬁed, our
tool analyzes the structural dependencies of the test code
to identify the list of associated production classes ( i.e.,the
classes exercised by the test class). All production classes
having structural dependencies ( e.g., method invocations)
with a test class Ctare considered as exercised by Ct.
The output of HistoryMiner is an artifact lifetime log for
each code ﬁle in a project repository. The tool generates two
7Table 4: Rules used to detect test smells [7].
Name Abbr. Description
Assertion Roulette AR JUnit classes containing at least one
method having more than one assertion
statement, and having at least one asser-
tion statement without explanation.
General Fixture GF JUnit classes having at least one method
not using the entire test ﬁxture deﬁned in
thesetUp() method
Eager Test EG JUnit classes having at least one method
that uses more than one method of the
tested class.
Mystery Guest MG JUnit classes that use an external resource
(e.g., a ﬁle or database).
Sensitive Equality SE JUnit classes having at least one assert
statement invoking a toString method.
di↵erent lifetime logs for Test Cases andProduction Classes .
For each test ftcin the repository, the tool outputs a logtc
which contains a row for each commit which modiﬁed ftc.
Thei-th row contains the following ﬁelds:
•ftc.name : the fully qualiﬁed name of ftc;
•ci.id: the commit hash of the i-th commit which mod-
iﬁed ftc;
•ci.time : the timestamp when the i-th commit has been
performed in the repository;
•TStc: the list of the test smells a↵ecting ftcatci.time
(if any);
•PC tc: the list of fully qualiﬁed names of production
classes having structural dependencies with ftc.W e
assume these classes to be the ones exercised by ftc.
Similarly, for each production class ﬁle fpcin the project
repository, the tool outputs a logpc, which contains a row
for each commit which modiﬁed fpc. The i-th row contains
the following ﬁelds:
•fpc.name : the fully qualiﬁed name of ftc;
•ci.id: the commit hash of the i-th commit, which mod-
iﬁed fpc;
•ci.time : the timestamp when the i-th commit has been
performed in the repository;
•CS pc: the list of code smells a↵ecting fpcatci.time
(if any).
With the collected information we are able to identify for
a given test ﬁle ftcthe list of code smells a↵ecting the pro-
duction classes it tests ( PC tc) at any moment in time (this
is needed in order to answer RQ 3). In particular, given a
commit cimodifying ftc, we retrieve the list of code smells
a↵ecting each class C2PC tcat time ci.time by:
•Retrieving, among all commits a↵ecting ( i.e.,adding/-
modifying) C, the one ( cj) having the greatest times-
tamp lower than ci.time (i.e.,the commit a↵ecting C
being the “closest” in terms of time to ciamong those
preceding ci).
•Consider the smells a↵ecting Cat time cj.time as the
ones a↵ecting it also at time ci.time , when the commit
modifying ftcwas performed.
3.2.2 Identiﬁcation of Smells
In the context of our analysis we had to identify test and
production code smells at each commit. Given the high
number of commits analyzed ( i.e., 472,116), a manual de-
tection is practically infeasible. For this reason, during theanalysis of each commit HistoryMiner used detection rules
which identify both test and code smells. Concerning the
test smell detection, HistoryMiner relied on the approach
proposed by Bavota et al. [7] to detect the ﬁve analyzed
test smells. Such an approach applies a heuristic metric-
based technique that overestimates the presence of test de-
sign ﬂaws with the goal of identifying all the test smell in-
stances ( i.e.,it targets 100% recall). Table 4 reports the set
of rules used by the tool in order to detect instances of test
smells. For example, it marks JUnit classes as a↵ected by
General Fixture those having at least one test method not
using the entire test ﬁxture deﬁned in the setUp() method.
This approach has been shown to have a precision higher
than 70% for all detected smells [7]. However, it is not able
to recommend refactoring operations to remove the identi-
ﬁed smells, thus only providing a very limited support to
software developers interested in removing test smells from
their system.
As for the code smells detection, we run on each commit
an implementation of the DECOR smell detector based on
the original rules deﬁned by Moha et al. [25]. DECOR iden-
tiﬁes smells using detection rules rooted in internal quality
metrics4. The choice of using DECOR is driven by the fact
that (i) it is a state-of-the-art smell detector having a high
accuracy in detecting smells [25]; and (ii) it applies simple
detection rules that make it very e cient.
3.3 Data Analysis
In this section we describe the data analysis performed to
answer each of the three formulated research questions.
3.3.1 RQ 1: When Are Test Smells Introduced?
We analyzed each lifetime log for a test ﬁle ftcto iden-
tify for each test smell TSka↵ecting it the following two
commits:
•ccreation : The ﬁrst commit creating ftc(i.e.,adding it
in the versioning system);
•csmell k: The ﬁrst commit where TSkis detected in ftc
(i.e.,the commit in which TSkhas been introduced).
Then, we analyze the distance between ccreation and
csmell kin terms of number of commits, considering only the
commits which involved ftc. We show the distribution of
such distances for each type of test smell as well as for all
test smell instances aggregated.
3.3.2 RQ 2: What Is the Longevity of Test Smells?
To address RQ 2, we need to determine when a smell has
been introduced and when a smell disappears from the sys-
tem. To this aim, given a test smell TSka↵ecting a test
ﬁleftc, we analyze logtcand formally deﬁne two types of
commits:
•Smell-introducing commit: the ﬁrst chronological com-
mitciwhere TSkhas been detected.
•Smell-removing commit: the ﬁrst chronological com-
mitcjfollowing the smell-introducing commit ciin
which the test smell TSkno longer a↵ects ftc.
Given a test smell TSk, the time interval between the
smell-introducing commit and the smell-removing commit
4An example of detection rule exploited to identify Blob
classes can be found at http://tinyurl.com/paf9gp6.
8is deﬁned as smelly interval , and determines the longevity
ofTSk. Given a smelly interval for a test smell a↵ecting the
ﬁleftcand bounded by the last-smell-introducing commit ci
and the smell-removing commit cj, we compute as proxies
for the smell longevity:
•#commits: the total number of commits between ci
andcj;
•#tcChanges: the number of commits between ciand
cjthat modiﬁed the test case ftc(a subset of the pre-
viously deﬁned set #commits);
•#days: the number of days between the introduction
of the smell ( ci.time ) and its ﬁx ( cj.time ).
Note that since we are analyzing a ﬁnite change history
for a given repository, it could happen that for a speciﬁc
ﬁle and a smell a↵ecting it we are able to detect the smell-
introducing commit but not the smell-removing commit, due
to the fact that the ﬁle is still a↵ected by the test smell in
the last commit. In other words, we can discriminate two
di↵erent types of smelly intervals in our dataset:
•Closed smelly intervals: intervals delimited by a smell-
introducing commit as well as by a smell-removing
commit;
•Censored smelly intervals: intervals delimited by a
smell-introducing commit and by the end of the change
history ( i.e.,the date of the last commit we analyzed).
The collected data was used to answer the following sub-
questions.
RQ 2.1How long does it take to ﬁx a test smell? We
analyze only the closed smelly intervals, which correspond to
the ﬁxed test smell instances. For each type of test smell we
show the distribution of #commits, #tcChanges and #days
related to the closed smelly intervals.
RQ 2.2What is the percentage of test smell in-
stances ﬁxed? We report, for each type of test smell, the
percentage of ﬁxed instances ( i.e., closed smelly intervals).
In doing this, we pay particular attention to the censored
intervals related to test smells introduced towards the end
of the observable change history of each project repository.
This is because for those instances developers might not have
had enough time to ﬁx them. Indeed, we compute the per-
centage of ﬁxed and not ﬁxed instances by progressively re-
moving instances introduced xdays before the date of the
last commit of the project repository we mined. To deﬁne
x, we use the results of the previous sub-research question
related to the number of days usually needed to ﬁx a test
smell. Therefore, we compute the percentages of ﬁxed and
not ﬁxed test smell instances using: (i) all the smelly inter-
vals; (ii) excluding censored intervals using x=1stquartile of
the #days to ﬁx a test smell; (iii) x=median value; and (iv)
x=3rdquartile. It is worth noting that, while in general any
test smell instance deﬁning a censored interval could poten-
tially be ﬁxed in the future, this is progressively less likely
to happen as the time goes by.
RQ 2.3What is the survivability of test smells? We
answer this subquestion by relying on survival analysis [36],
a statistical method that aims at analyzing and modeling
the time duration until one or more events happen. Such
time duration is modeled as a random variable and typically
it has been used to represent the time to the failure of a
physical component (mechanical or electrical) or the time
to the death of a biological unit (patient, animal, cell, etc.)[36]. The survival function S(t)= Pr(T> t ) indicates
the probability that a subject (in our case the code smell)
survives longer than some speciﬁed time t. The survival
function never increases as tincreases; also, it is assumed
S(0) = 1 at the beginning of the observation period, and, for
time t!1,S(1)!0. The goal of the survival analysis is
to estimate such a survival function from data and assess the
relationship of explanatory variables (covariates) to survival
time. Time duration data can be of two types:
1.Complete data : the value of each sample unit is ob-
served or known. For example, the time to the failure
of a mechanical component has been observed and re-
ported. In our case, the code smell disappearance has
been observed.
2.Censored data : The event of interest in the analysis
has not been observed yet (so it is considered as un-
known). For example, a patient cured with a particular
treatment has been alive till the end of the observation
window. In our case, the smell remains in the system
until the end of the observed project history. For this
sample, the time-to-death observed is a censored value,
because the event (death) has not occurred during the
observation.
Both complete and censored data can be used, if prop-
erly marked, to generate a survival model. The model can
be visualized as a survival curve that shows the survival
probability as a function of the time. In the context of our
analysis, the population is represented by the test smell in-
stances while the event of interest is its ﬁx. Therefore, the
“time-to-death”is represented by the observed time from the
introduction of the test smell instance, till its ﬁx (if observed
in the available change history). We refer to such a time pe-
riod as “the lifetime” of a test smell instance. Complete data
is represented by those instances for which the event (ﬁx)
has been observed, while censored data refers to those in-
stances which have not been ﬁxed in the observable window.
We generate survival models using both the # of days and
the # of commits in the smelly intervals as time variables.
We analyzed the survivability of test smells by ecosystem.
That is, for each ecosystem, we generated a survival model
for each type of test smell by using Rand the survival
package5. In particular, we used the Surv type to gener-
ate a survival object and the survfit function to compute
an estimate of a survival curve, which uses Kaplan-Meier
estimator [17] for censored data.
It is important to highlight that, while the survival anal-
ysis is designed to deal with censored data, we performed
a cleaning of our dataset aimed at reducing possible biases
caused by censored intervals before running the analysis. In
particular, test smell instances introduced too close to the
end of the observed change history can potentially inﬂuence
our results, since in these cases the period of time needed for
their removal is too small for being analyzed. Thus, as done
for the previous sub-question, we excluded from our sur-
vival analysis all censored intervals for which the last-smell-
introducing commit was “too close” to the last commit we
analyzed in the system’s change history ( i.e.,for which the
developers might not have had “enough time” to ﬁx them).
In the paper we report the survival analysis considering all
the closed and censored intervals (no instances removed).
5https://cran.r-project.org/package=survival
9Table 5: RQ 1: Number of commits between the cre-
ation of the test case and the introduction of a test
smell.
Ecosystem Smell Min. 1st Qu. Median Mean 3rd Qu. Max % Smelly 1st
AR 0 0 0 0.13 0 58 96
ET 0 0 0 0.12 0 245 97
Apache GF 0 0 0 0.27 0 23 93
MG 0 0 0 0.49 0 245 93
SE 0 0 0 0.59 0 62 90
AR 0 0 0 0.37 0 99 96
ET 0 0 0 0.13 0 26 98
Eclipse GF 0 0 0 0.41 0 99 94
MG 0 0 0 1.13 0 56 91
SE 0 0 0 1.79 0 102 88
The results of the survival analysis with di↵erent thresholds
to clean the dataset are available on our online appendix [1].
3.3.3 RQ 3: Are Test Smells Associated with Par-
ticular Code Smells Affecting the Production
Code?
For each test ﬁle ftc, we collect the set of test smells ( TStc)
a↵ecting it at the time of its creation as well as the set of code
smells ( CS PCtc) a↵ecting the production classes exercised
byftc(PC tc) at the same time. The goal is to identify
patterns of co-occurrence among items belonging to TStc
and items of CS PCtc. To do so, we generate a database
of transactions, which contains a transaction for each test
ﬁleftc. Each transaction contains the test and code smells
inTStcandCS PCtcas items. Note that if TStcis empty,
we insert the item cleanTC to represent a test ﬁle created
without any test smell. Similarly, if CS PCtcis empty, we
insert the item cleanPC .
We analyze such database of transactions using Associa-
tion Rule Mining [3] to identify patterns of co-occurrence of
test and code smells. In particular, we use the statistical
software Rand the package arules .
4. TEST SMELL LIFECYCLE: RESULTS
This section reports the results of our empirical study.
4.1 RQ 1: Introduction of Test Smells
Table 5 reports, for each ecosystem, the statistics of the
distribution of the number of commits needed by each test
smell type to manifest itself. Also, the last column of Ta-
ble 5 reports the percentage of test smell instances that have
been introduced when the a↵ected test class has been com-
mitted for the ﬁrst time. The results shown in Table 5 clearly
highlight one ﬁnding: test smells mostly appear as the result
of bad design choices made during the creation of the test
classes, and not as the result of design quality degradation
during maintenance and evolution activities . Indeed, for all
the considered test smell types the 3rdquartile of the distri-
butions equals zero ( i.e., the commit which introduces the
smell is the same which introduces the test class). Also,
for all test smells and in both ecosystems, at least 88% of
the smell instances are introduced when the artifact is cre-
ated and committed for the ﬁrst time in the repository (see
last column in Table 5). This result is in line with previous
ﬁndings on code smells in production code [41], and it con-
tradicts the common wisdom for which test smell instances
are the result of test code evolution [42]. For this reason, fu-
ture automatic identiﬁcation tools should take into account
the fact that the lifespan of test smells starts, in most of
the cases, with the creation of a test case. IDEs and au-Table 6: Percentage of ﬁxed instances in the observed
change history using di↵erent thresholds to remove
censored intervals.
Smell History Apache Eclipse Overall
All 0.97 2.03 1.15
Excluding 1st Q. 0.98 2.03 1.16
Excluding Median 1.00 2.03 1.18AR
Excluding 3rd Q. 1.06 2.05 1.25
ETAll 2.31 1.71 2.19
Excluding 1st Q. 2.39 1.71 2.24
Excluding Median 2.39 1.71 2.24
Excluding 3rd Q. 2.48 1.72 2.32
All 3.29 2.99 3.22
Excluding 1st Q. 3.32 2.99 3.24
Excluding Median 3.41 2.99 3.31GF
Excluding 3rd Q. 3.70 3.13 3.55
MGAll 2.23 5.39 2.71
Excluding 1st Q. 2.25 5.39 2.73
Excluding Median 2.27 5.39 2.74
Excluding 3rd Q. 2.35 5.39 2.82
All 3.50 7.38 4.21
Excluding 1st Q. 3.51 7.41 4.22
Excluding Median 3.53 7.53 4.31SE
Excluding 3rd Q. 3.82 7.56 4.52
AllAll 2.09 2.57 2.18
Excluding 1st Q. 2.11 2.57 2.20
Excluding Median 2.15 2.57 2.24
Excluding 3rd Q. 2.27 2.62 2.34
tomatic refactoring tools should pay particular attention to
when test classes are ﬁrstly created and committed to the
repository, since their quality can be compromised by the
presence of a design ﬂaw. Promptly suggesting an alterna-
tive design (or refactorings) for a newly created test class
could signiﬁcantly increase the chances of having a clean
test class.
4.2 RQ 2: Longevity of Test Smells
When evaluating the survivability of test smells, we take
into account several aspects related to this main research
question, as detailed in the following subsections.
4.2.1 How Long Does It Take to Fix a Test Smell?
Fig. 1 shows the distribution of the number of days be-
tween the introduction of a test smell and its removal. Re-
member that this data is only available for test smell in-
stances for which we observed both their introduction and
their removal over the analyzed change history. The box-
plots are organized by the ecosystem with the rightmost
one showing the overall results obtained by combining both
Apache and Eclipse projects. Overall, we can observe that
test smell instances are removed, on average, after 100 days
from their introduction6. We do not ﬁnd any signiﬁcant dif-
ference among the di↵erent test smell types. We also con-
sidered the distribution of the number of modiﬁcations a de-
veloper performs before ﬁxing a test smell (shown in Fig. 2).
As we can see, no more than ﬁve modiﬁcations involving the
a↵ected test class are performed before the test smell disap-
pears. This ﬁnding shows on one hand that test classes are
not often modiﬁed during the history of a software project
(on average, ﬁve changes in 100 days), and on the other hand
that a limited number of modiﬁcations is generally required
to remove a test smell.
6On median, 169 commits and, on average, 671 commits are
performed in this time period. We do not report these box-
plots due to space constraints. Complete data are available
in our online appendix [1].
10●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
AR ET GF MG SE1 5 10 50 100 500Number of Days (log scale)(a) Apache●●●●●●●●●●●●●●
AR ET GF MG SE1 5 10 50 100 500Number of Days (log scale)(b) Eclipse●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
AR ET GF MG SE1 5 10 50 100 500Number of Days (log scale)(c) Overall
Figure 1: Distribution of number of days a test smell remained in the system before being removed.
●●●●●
●●●●●●●●●●●●●●●●
AR ET GF MG SE1 2 5 10 20Number of tcChanges (log scale)(a) Apache●●●●●●●
●●●●●●●
●●
AR ET GF MG SE1 2 5 10 20Number of tcChanges (log scale)(b) Eclipse●●●●●●●
●●●●●●●●●●●
●●●●●●●●●●●●
AR ET GF MG SE1 2 5 10 20Number of tcChanges (log scale)(c) Overall
Figure 2: Distribution of number of modiﬁcations a smell remained in the system before being removed.
4.2.2 What Is the Percentage of Fixed Smell In-
stances?
Table 6 shows the percentage of test smell instances ﬁxed
in Apache, Eclipse, and in the complete dataset. In a con-
servative fashion, we show the percentage of ﬁxed instances
considering all the change history and progressively remov-
ing censored instances ( i.e., not ﬁxed yet) introduced too
close to the end of the mined change history. It is inter-
esting to notice that the percentage of ﬁxed instances is
deﬁnitely small. Indeed, only 2.09% and 2.59% of all test
smell instances respectively in Apache and Eclipse are ﬁxed
(2.18% overall). During this analysis, we noticed that in the
Eclipse ecosystem speciﬁc test smells appear to have higher
ﬁxing percentage (7.38% and 5.39% for Sensitive Equality
and Mystery Guest, respectively) with respect to the Apache
ecosystem. Even when conservatively discarding instances
too close to the end of the observable change history, the per-
centage of ﬁxed instances remains very small (2.34% overall).
This result highlights the poor attention devoted by devel-
opers to the removal of test smells that, however, have been
shown to hinder code comprehensibility and maintainability
[7]. Automated tools, promptly recommending developers
on how to refactor test smells, could help in drastically in-
creasing the percentage of ﬁxed instances.4.2.3 What Is the Survivability of Test Smells?
Fig. 3 shows the survival curves, in terms of number of
days, for the di↵erent test smell types grouped by ecosys-
tems. Overall, test smells have a very high survivability.
Indeed, after 1,000 days the probability that a test smell
survives ( i.e., has not been ﬁxed yet) is 80%. After 2,000
days the survival probability is still around 60%. Fig. 4
reports instead the survivability of test smells when consid-
ering the number of commits. Again, the survivability is
very high, with 50% of probability that a test class is still
a↵ected by a test smell after 2,000 commits from its intro-
duction. Having test smells lasting that long in the systems:
(i) further stresses the need for automatic detection tools,
and (ii) poses questions on the high maintenance costs the
a↵ected test classes could have. We plan to empirically in-
vestigate the latter point in our future work.
4.3 RQ 3: Test and Code Smells
Table 7 reports the results achieved when applying as-
sociation rules mining to identify patterns of co-occurrence
between test and code smells. We observed several interest-
ing associations relating design ﬂaws occurring in test and
production code. A clear case is the ﬁrst rule shown in Ta-
ble 7, associating clean test cases to clean production class.
This indicates that test classes do not a↵ected by test smells
(i.e., clean test classes) usually tests production classes do
110.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00 1000 2000 3000 40000.0 0.2 0.4 0.6 0.8 1.0Number of DaysSurvival ProbabilityARETGFMGSE
(a) Apache
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00 1000 2000 3000 4000 50000.0 0.2 0.4 0.6 0.8 1.0Number of DaysSurvival ProbabilityARETGFMGSE
(b) Eclipse
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00 1000 2000 3000 4000 50000.0 0.2 0.4 0.6 0.8 1.0Number of DaysSurvival ProbabilityARETGFMGSE
(c) Overall
Figure 3: Survival probability of test smells (with respect to the number of days).0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00 2000 4000 6000 8000 100000.0 0.2 0.4 0.6 0.8 1.0Number of CommitsSurvival ProbabilityARETGFMGSE
(a) Apache
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00 2000 4000 6000 8000 10000 120000.0 0.2 0.4 0.6 0.8 1.0Number of CommitsSurvival ProbabilityARETGFMGSE
(b) Eclipse
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.00 2000 4000 6000 8000 10000 120000.0 0.2 0.4 0.6 0.8 1.0Number of CommitsSurvival ProbabilityARETGFMGSE
(c) Overall
Figure 4: Survival probability of test smells (with respect to the number of commits).
Table 7: RQ 3: Relationships between test and code
smells.
id lhs rhs support conﬁdence lift
1cleanTC cleanPC 0.05 0.96 1.24
2 AR, SC BC 0.05 0.99 4.74
3CDSBP ET 0.06 0.93 1.17
4 SC ET 0.09 0.97 1.22
5BC ET 0.20 0.95 1.18
6 CDSBP, SC ET 0.02 0.98 1.22
7AR, CDSBP ET 0.07 0.94 1.17
8 CDSBP, GF BC 0.02 0.77 3.66
9CDSBP, MG AR 0.01 0.72 1.14
10 AR, CDSBP ET 0.04 0.94 1.17
11AR, SC CC 0.04 0.85 7.93
12 GF, SE AR 0.02 0.76 1.22
13AR, CC, ET, GF, SC BC 0.02 1.00 4.75
14 GF, SE ET 0.02 0.87 1.08
15MG, SE AR 0.01 0.72 1.15
16 BC, SE AR 0.02 0.79 1.25
17BC, SE ET 0.02 0.97 1.21
not a↵ected by code smells ( i.e.,clean production classes).
Eager Test (ET) in test classes is often associated with
code smells in production code (see rules #3, #4, and #5).
Speciﬁcally, in our dataset we found several cases in which
three code smells, namely Class Data Should Be Private
(CDSBP), Spaghetti Code (SC), and Blob Class (BC) co-
occur with the Eager Test smell. This result is quite rea-
sonable if we consider the deﬁnitions of these smells. Indeed,a CDSBP instance appears when a production class violates
the information hiding principle by exposing its attributes
[13], and often co-occur with code smells related to long or
complex code, such as SC and BC [45]. On the other hand,
anEager Test appears when a test method checks more than
a single method of the class to be tested [42]. Thus, it is rea-
sonable to think that when testing large and complex classes,
developers tend to create more complex test methods, ex-
ercising multiple methods of the tested class. Similar ob-
servations can explain the results achieved for the Assertion
Roulette (AR) test smell (rules #9 and #12). Indeed, this
smell appears when a test case has several assertions with
no explanation [42]. When testing complex code, such as
the one represented by a Blob Class , more assertions which
test the behavior of the production class might be needed.
The results of this analysis highlight some interesting ﬁnd-
ings that should be taken into account for building e↵ective
tools for detecting code and test smells. Indeed, on one side
the presence of an Eager Test could indicate complex and
long production code to test. On the other side, complex
code to test is likely to trigger design issues in the related
test code.
5. THREATS TO VALIDITY
Threats to construct validity concern the relationship be-
tween theory and observation and are mainly related to the
12measurements we performed. In particular, such threats are
related to the way we detect test smells and, for RQ 3, code
smells. As explained in Section 3.2, we rely on the imple-
mentation of previously proposed approaches to identify test
smells [7] and code smells [25], both exhibiting a reasonable
precision in smell detection. Clearly, we cannot ignore the
fact that some smells have not been detected by the tools
and, while for the survey study we have manually validated
the considered smells, we are aware that for the second study
the reported results could be a↵ected by tools’ imprecision.
Another threat is related to the observation of the smells’
lifetime. We consider a period of time since the ﬁrst observ-
able commit, which, however, might not correspond to when
a ﬁle has been created (and this threat can a↵ect the results
ofRQ 1) until the last observed snapshot (the latter can in-
troduce a threat in RQ 2because smells could be removed
in the future, however survival analysis properly deals with
censored data).
Threats to internal validity concern factors internal to our
study that we cannot control and that could have inﬂuenced
the results. Above all, the results observed in RQ 2about
smell survival might not only be due to the lack of aware-
ness (observed in the survey study), but also to the lack of
necessity to perform (risky) improvements to working test
suites.
Threats to external validity concern the generalization of
our ﬁndings. On one hand, the results of the survey are
clearly conﬁned to the speciﬁcity of our 19 respondents. Al-
though the results are quite consistent, it is possible that
other developers might exhibit di↵erent levels of awareness
about test smells. While large, the mining study surely
needs to be extended to other projects beyond the open
source ones that we considered (which belong to two ecosys-
tems). Also, it is desirable to extend the study to other test
smells beyond the ﬁve considered; however, the considered
test smells are the most di↵used ones [7].
6. RELATED WORK
As well as production code, test code should be designed
following good programming practices [37]. During the last
decade, the research community spent a lot of e↵ort to deﬁne
the methods and tools for detecting design ﬂaws in produc-
tion code [19, 20, 23, 25, 26, 28, 31, 30, 33, 40], as well as
empirical studies aimed at assessing their impact on main-
tainability [2, 4, 11, 18, 22, 29, 34, 35, 38, 45, 44, 21, 32].
However, design problems a↵ecting test code have been only
partially explored. The importance to have well designed
test code has been originally highlighted by Beck [9], while
van Deursen et al. [42] deﬁned a catalogue of 11 test smells,
i.e.,a set of a poor design solutions to write tests, together
with refactoring operations aimed at removing them. This
catalogue takes into account di↵erent types of bad design
choices made by developers during the implementation of
test ﬁxtures ( e.g., setUp() method too generic where test
methods only access a part of it), or of single test cases
(e.g., test methods checking several objects of the class to be
tested). Besides the test smells deﬁned by van Deursen et al.
[42], Meszaros deﬁned other smells a↵ecting test code [24].
Starting from these catalogues, Greiler et al. [14, 15] showed
that test smells related to ﬁxture set-up frequently occur in
industrial projects and, therefore, presented a static analy-
sis tool, namely TestHound , to identify ﬁxture related test
smells. van Rompaey et al. [43] proposed a heuristic struc-tural metric-based approach to identify General Fixture and
Eager Test instances. However, the results of an empirical
study demonstrated that structural metrics have lower ac-
curacy while detecting these test smells. Bavota et al. [8]
conducted an empirical investigation in order to study (i) the
di↵usion of test smells in 18 software projects, and (ii) their
e↵ects on software maintenance. The results of the study
demonstrated that 82% of JUnit classes in their dataset are
a↵ected by at least one test smell, and that the presence of
design ﬂaws has a strong negative impact on the maintain-
ability of the a↵ected classes.
7. CONCLUSION
This paper presented (i) a survey with 19 developers
aimed at investigating their perception of test smells
as design issues, and (ii) a large-scale empirical study
conducted over the commit history of 152 open source
projects and aimed at understanding when test smells
are introduced, what their longevity is, and whether they
have relationships with code smells a↵ecting the tested
production code classes. The achieved results provide
several valuable ﬁndings for the research community:
Lesson 1 .Test smells are not perceived by developers as
actual design problems. Our survey with 19 original devel-
opers of ﬁve systems showed that developers are not able to
identify the presence of test smells in their code. However,
recent studies empirically highlighted the negative e↵ect of
test smells on code comprehensibility and maintainability
[7]. This highlights the importance of investing e↵ort in the
development of tools to identify and refactor test smells.
Lesson 2 .In most cases test artifacts are a↵ected by bad
smells since their creation . This result contradicts the com-
mon wisdom that test smells are generally due to a negative
e↵ect of software evolution and it is inline with what ob-
served for code smells [41]. Also, this ﬁnding highlights that
the introduction of most smells can simply be avoided by
performing quality checks at commit time, or even while the
code is written in the IDE by recommending the developer
how to “stay away” from bad design practices ( i.e., just-
in-time refactoring ). Tools supporting these quality checks
could avoid or at least limit the introduction of test smells.
Lesson 3 .Test smells have a very high survivability . This
result further stresses the fact that developers are not catch-
ing such problems in the design of their test code. This
might be due to (i) the limited time dedicated to refactor-
ing activities, (ii) the unavailability of test smells refactoring
tools, or, as shown in our survey, (iii) the fact that devel-
opers are not perceiving test smells as bad design practices.
The reason behind this result must be further investigated
in the future work.
Lesson 4 .There exist relationships between smells in test
code and the ones in the tested production code . Given the
di↵erent nature of test and code smells, we found this result
to be quite surprising. Still, knowing the existence of these
relationships could deﬁnitively help in better managing both
types of smells, by using the presence of test smells as an
alarm bell for the possible presence of code smells in the
tested classes and vice versa .
These lessons learned represent the main input for our
future research agenda on the topic, mainly focused on de-
signing and developing a new generation of code quality-
checkers, such as those described in Lesson 2.
138. REFERENCES
[1]Online appendix.
https://sites.google.com/site/testsmells/, 2016.
[2]M. Abbes, F. Khomh, Y.-G. Gu´ eh´ eneuc, and
G. Antoniol. An empirical study of the impact of two
antipatterns, Blob and Spaghetti Code, on program
comprehension. In European Conf. on Software
Maintenance and Reengineering (CSMR) , pages
181–190. IEEE, 2011.
[3]R. Agrawal, T. Imieli´ nski, and A. Swami. Mining
association rules between sets of items in large
databases. SIGMOD Rec. , 22(2):207–216, June 1993.
[4]R. Arcoverde, A. Garcia, and E. Figueiredo.
Understanding the longevity of code smells:
preliminary results of an explanatory survey. In
Proceedings of the International Workshop on
Refactoring Tools , pages 33–36. ACM, 2011.
[5]T. Bakota, P. Heged ¨us, I. Siket, G. Lad´ anyi, and
R. Ferenc. Qualitygate sourceaudit: A tool for
assessing the technical quality of software. In 2014
Software Evolution Week - IEEE Conference on
Software Maintenance, Reengineering, and Reverse
Engineering, CSMR-WCRE 2014, Antwerp, Belgium,
February 3-6, 2014 , pages 440–445, 2014.
[6]G. Bavota, A. De Lucia, A. Marcus, and R. Oliveto.
Automating extract class refactoring: An improved
method and its evaluation. Empirical Softw. Engg. ,
19(6):1617–1664, Dec. 2014.
[7]G. Bavota, A. Qusef, R. Oliveto, A. De Lucia, and
D. Binkley. An empirical analysis of the distribution of
unit test smells and their impact on software
maintenance. In 28th IEEE International Conference
on Software Maintenance, ICSM 2012, Trento, Italy,
September 23-28, 2012 , pages 56–65, 2012.
[8]G. Bavota, A. Qusef, R. Oliveto, A. De Lucia, and
D. Binkley. Are test smells really harmful? an
empirical study. Empirical Software Engineering ,
20(4):1052–1094, 2015.
[9]Beck. Test Driven Development: By Example .
Addison-Wesley Longman Publishing Co., Inc.,
Boston, MA, USA, 2002.
[10]W. J. Brown, R. C. Malveau, W. H. Brown, H. W.
McCormick III, and T. J. Mowbray. Anti Patterns:
Refactoring Software, Architectures, and Projects in
Crisis . John Wiley and Sons, 1st edition, 1998.
[11]A. Chatzigeorgiou and A. Manakos. Investigating the
evolution of bad smells in object-oriented code. In Int’l
Conf. Quality of Information and Communications
Technology (QUATIC) , pages 106–115. IEEE, 2010.
[12]P. M. Duvall, S. Matyas, and A. Glover. Continuous
Integration: Improving Software Quality and Reducing
Risk. Addison-Wesley, 2007.
[13]M. Fowler. Refactoring: improving the design of
existing code . Addison-Wesley, 1999.
[14]M. Greiler, A. van Deursen, and M.-A. Storey.
Automated detection of test ﬁxture strategies and
smells. In Proceedings of the International Conference
on Software Testing, Veriﬁcation and Validation
(ICST) , pages 322–331, March 2013.
[15]M. Greiler, A. Zaidman, A. van Deursen, and M.-A.
Storey. Strategies for avoiding text ﬁxture smells
during software evolution. In Proceedings of the 10thWorking Conference on Mining Software Repositories
(MSR) , pages 387–396. IEEE, 2013.
[16]R. M. Groves. Survey Methodology, 2nd edition .W i l e y ,
2009.
[17]E. L. Kaplan and P. Meier. Nonparametric estimation
from incomplete observations. Journal of the American
Statistical Association ,5 3 ( 2 8 2 ) : 4 5 7 – 4 8 1 ,1 9 5 8 .
[18]F. Khomh, M. Di Penta, Y.-G. Gu´ eh´ eneuc, and
G. Antoniol. An exploratory study of the impact of
antipatterns on class change- and fault-proneness.
Empirical Software Engineering ,1 7 ( 3 ) : 2 4 3 – 2 7 5 ,2 0 1 2 .
[19]F. Khomh, S. Vaucher, Y.-G. Gu´ eh´ eneuc, and
H. Sahraoui. A bayesian approach for the detection of
code and design smells. In Proc. Int’l Conf. on Quality
Software (QSIC) , pages 305–314. IEEE, 2009.
[20]M. Lanza and R. Marinescu. Object-Oriented Metrics
in Practice: Using Software Metrics to Characterize,
Evaluate, and Improve the Design of Object-Oriented
Systems . Springer, 2006.
[21]M. Linares-V´ asquez, S. Klock, C. McMillan,
A. Saban´ e, D. Poshyvanyk, and Y.-G. Gu´ eh´ eneuc.
Domain matters: Bringing further evidence of the
relationships among anti-patterns, application
domains, and quality-related metrics in java mobile
apps. In Proceedings of the 22Nd International
Conference on Program Comprehension , ICPC 2014,
pages 232–243, New York, NY, USA, 2014. ACM.
[22]A. Lozano, M. Wermelinger, and B. Nuseibeh.
Assessing the impact of bad smells using historical
information. In Proc. of the Int’l workshop on
Principles of Software Evolution (IWPSE) , pages
31–34. ACM, 2007.
[23]R. Marinescu. Detection strategies: Metrics-based
rules for detecting design ﬂaws. In Proceedings of the
International Conference on Software Maintenance
(ICSM) ,p a g e s3 5 0 – 3 5 9 ,2 0 0 4 .
[24]G. Meszaros. xUnit Test Patterns: Refactoring Test
Code . Addison Wesley, 2007.
[25]N. Moha, Y.-G. Gu´ eh´ eneuc, L. Duchien, and A.-F. L.
Meur. DECOR: A method for the speciﬁcation and
detection of code and design smells. IEEE Trans. on
Software Engineering ,3 6 ( 1 ) : 2 0 – 3 6 ,2 0 1 0 .
[26]M. J. Munro. Product metrics for automatic
identiﬁcation of “bad smell” design problems in java
source-code. In Proc. Int’l Software Metrics
Symposium (METRICS) , page 15. IEEE, 2005.
[27]G. C. Murphy. Houston: We are in overload. In 23rd
IEEE International Conference on Software
Maintenance (ICSM 2007), October 2-5, 2007, Paris,
France , page 1, 2007.
[28]R. Oliveto, F. Khomh, G. Antoniol, and Y.-G.
Gu´ eh´ eneuc. Numerical signatures of antipatterns: An
approach based on B-splines. In Proceedings of the
European Conference on Software Maintenance and
Reengineering (CSMR) , pages 248–251. IEEE, 2010.
[29]F. Palomba, G. Bavota, M. Di Penta, R. Oliveto, and
A. De Lucia. Do they really smell bad? A study on
developers’ perception of bad code smells. In 30th
IEEE International Conference on Software
Maintenance and Evolution, Victoria, BC, Canada,
September 29 - October 3, 2014 , pages 101–110, 2014.
[30]F. Palomba, G. Bavota, M. Di Penta, R. Oliveto,
14D. Poshyvanyk, and A. De Lucia. Mining version
histories for detecting code smells. IEEE Trans. on
Software Engineering , 41(5):462–489, May 2015.
[31]F. Palomba, G. Bavota, M. D. Penta, R. Oliveto,
A. D. Lucia, and D. Poshyvanyk. Detecting bad smells
in source code using change history information. In
Automated Software Engineering (ASE), 2013
IEEE/ACM 28th International Conference on , pages
268–278, Nov 2013.
[32]F. Palomba, D. D. Nucci, M. Tufano, G. Bavota,
R. Oliveto, D. Poshyvanyk, and A. D. Lucia. Landﬁll:
An open dataset of code smells with public evaluation.
In2015 IEEE/ACM 12th Working Conference on
Mining Software Repositories , pages 482–485, May
2015.
[33]F. Palomba, A. Panichella, A. De Lucia, R. Oliveto,
and A. Zaidman. A textual-based technique for smell
detection. In Proceedings of the International
Conference on Program Comprehension (ICPC) , page
to appear. IEEE, 2016.
[34]R. Peters and A. Zaidman. Evaluating the lifespan of
code smells using software repository mining. In Proc.
of the European Conf. on Software Maintenance and
ReEngineering (CSMR) , pages 411–416. IEEE, 2012.
[35]D. Ratiu, S. Ducasse, T. Gˆ ırba, and R. Marinescu.
Using history information to improve design ﬂaws
detection. In European Conf. on Software
Maintenance and Reengineering (CSMR) , pages
223–232. IEEE, 2004.
[36]J. Rupert G. Miller. Survival Analysis, 2nd Edition .
John Wiley and Sons, 2011.
[37]A. Schneider. Junit best practices. Java World, 2000.
[38]D. I. K. Sjøberg, A. F. Yamashita, B. C. D. Anda,
A. Mockus, and T. Dyb˚ a. Quantifying the e↵ect of
code smells on maintenance e↵ort. IEEE Trans.Software Eng. ,3 9 ( 8 ) : 1 1 4 4 – 1 1 5 6 ,2 0 1 3 .
[39]G. Szoke, C. Nagy, L. J. F ¨ul¨op, R. Ferenc, and
T. Gyim´ othy. FaultBuster: An automatic code smell
refactoring toolset. In 15th IEEE International
Working Conference on Source Code Analysis and
Manipulation, SCAM 2015, Bremen, Germany,
September 27-28, 2015 ,p a g e s2 5 3 – 2 5 8 ,2 0 1 5 .
[40]N. Tsantalis and A. Chatzigeorgiou. Identiﬁcation of
move method refactoring opportunities. IEEE
Transactions on Software Engineering ,3 5 ( 3 ) : 3 4 7 – 3 6 7 ,
2009.
[41]M. Tufano, F. Palomba, G. Bavota, R. Oliveto,
M. Di Penta, A. De Lucia, and D. Poshyvanyk. When
and why your code starts to smell bad. In Int’l Conf.
on Softw. Engineering (ICSE) , pages 403–414. IEEE,
2015.
[42]A. van Deursen, L. Moonen, A. Bergh, and G. Kok.
Refactoring test code. In Proceedings of the 2nd
International Conference on Extreme Programming
and Flexible Processes in Software Engineering (XP) ,
pages 92–95, 2001.
[43]B. Van Rompaey, B. Du Bois, S. Demeyer, and
M. Rieger. On the detection of test smells: A
metrics-based approach for general ﬁxture and eager
test. IEEE Transactions on Software Engineering ,
33(12):800–817, Dec 2007.
[44]A. F. Yamashita and L. Moonen. Do developers care
about code smells? an exploratory survey. In
Proceedings of the Working Conference on Reverse
Engineering (WCRE) , pages 242–251. IEEE, 2013.
[45]A. F. Yamashita and L. Moonen. Exploring the
impact of inter-smell relations on software
maintainability: An empirical study. In 2013 35th
International Conference on Software Engineering
(ICSE) , pages 682–691, May 2013.
15