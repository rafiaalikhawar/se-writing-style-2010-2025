Predicting Semantically Linkable Knowledge in Developer
Online Forums via Convolutional Neural Network
Bowen Xu1, Deheng Y e2, Zhenchang Xing2, Xin Xia1y, Guibin Chen2, Shanping Li1
1College of Computer Science and Technology, Zhejiang University, China
2School of Computer Science and Engineering, Nanyang Technological University, Singapore
max_xbw@zju.edu.cn, ye0014ng@e.ntu.edu.sg, zcxing@ntu.edu.sg,
xxia@zju.edu.cn, gbchen@ntu.edu.sg, shan@zju.edu.cn
ABSTRACT
Consider a question and its answers in Stack Overow as a
knowledge unit. Knowledge units often contain semantically
relevant knowledge, and thus linkable for dierent purposes,
such as duplicate questions, directly linkable for problem
solving, indirectly linkable for related information. Recog-
nising dierent classes of linkable knowledge would support
more targeted information needs when users search or ex-
plore the knowledge base. Existing methods focus on bi-
nary relatedness (i.e., related or not), and are not robust
to recognize dierent classes of semantic relatedness when
linkable knowledge units share few words in common (i.e.,
have lexical gap). In this paper, we formulate the prob-
lem of predicting semantically linkable knowledge units as
a multiclass classication problem, and solve the problem
using deep learning techniques. To overcome the lexical gap
issue, we adopt neural language model (word embeddings)
and convolutional neural network (CNN) to capture word-
and document-level semantics of knowledge units. Instead of
using human-engineered classier features which are hard to
design for informal user-generated content, we exploit large
amounts of dierent types of user-created knowledge-unit
links to train the CNN to learn the most informative word-
level and document-level features for the multiclass classi-
cation task. Our evaluation shows that our deep-learning
based approach signicantly and consistently outperforms
traditional methods using traditional word representations
and human-engineered classier features.
CCS Concepts
‚Ä¢Software and its engineering !Software libraries and
repositories; ‚Ä¢Information systems !Social networking
sites;
‚àóJoint rst authors, contributed equally.
‚Ä†Corresponding author.Keywords
Link prediction, Semantic relatedness, Multiclass classica-
tion, Deep learning, Mining software repositories
1. INTRODUCTION
In Stack Overow, computer programming knowledge has
been shared through millions of questions and answers. We
consider a Stack Overow question with its entire set of
answers as a knowledge unit regarding some programming-
specic issues. The knowledge contained in one unit is likely
to be related to knowledge in other units. When asking a
question or providing an answer in Stack Overow, users
reference existing questions and answers that contain rele-
vant knowledge by URL sharing [46], which is strongly en-
couraged by Stack Overow [2]. Through URL sharing, a
network of linkable knowledge units has been formed over
time [46].
Unlike linked pages on Wikipedia that follows the under-
lying knowledge structure, questions and answers are spe-
cic to individual's programming issues, and URL sharing
in Q&As is opportunistic, because it is based on the com-
munity awareness of the presence of relevant questions and
answers. A recent study by Ye et al. [46] shows that the
structure of the knowledge network that URL sharing activ-
ities create is scale free. A scale free network follows a power
law degree distribution, which can be explained using pref-
erential attachment theory [4], i.e., \the rich get richer". On
one hand, this means that a small proportion of the knowl-
edge units is attracting a large proportion of users' attention.
On the other hand, this means that large amounts of knowl-
edge units in the long tail of the power law distribution are
mostly under linked.
To mitigate this issue, Stack Overow recommends re-
lated questions when users are viewing a question. Stack
Overow's recommendation of related questions is essen-
tially based on lexical similarity of word overlap between
questions [1]. However, linkable knowledge units often share
few words in common (i.e., lexical gap) due to two main rea-
sons. First, users could formulate the same question in many
dierent ways. For example, Table 1 shows two duplicate
questions from Stack Overow. The question (Id 510357)
has been recognized as a duplicate to another question (Id
19477465) by the Stack Overow users, because they can be
answered by the same answer. However, literally, we can
hardly see common words and phrases between these two
questions. Second, two questions could discuss dierent but
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE‚Äô16 , September 3‚Äì7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970357
51
Table 1: An Example of Duplicate Knowledge Units from Stack Overow
Question Id 510357 (marked as duplicate to 19477465) Question Id 19477465
Title: Python read a single character from the user Title: Get python program to end by pressing anykey
and not enter
Body : Is there a way of reading one single character from the Body: How can I get my Python program to end by pressing any
user input? For instance, they press one key at the terminal key without pressing enter. So if the user types "c", the program
and it is returned (sort of like getch()). I know there's a should automatically end without pressing enter. My code so far:
function in Windows for it, but I'd like something that is print("Hi everyone! This is just a quick sample code I made")
cross-platform. print("Press anykey to end the program.")
Table 2: An Example of Directly Linked Knowledge Units from Stack Overow
Question Id 4547310 Question Id 53513
Title: Iterating over a stack (reverse list), is there an isempty() method? Title: Best way to check if a list is empty
Body : What's the best way to iterate over a stack in Python? I couldn't nd Body : For example, if passed the following:
an isempty method, and checking the length each time seems wrong somehow. a = [] How do I check to see if a is empty?
Table 3: An Example of Indirectly Linked Knowledge Units from Stack Overow (both have an link to 53513)
Question Id 32831543 Question Id 1115313
Title: How to check if a nested list has a value Title: Cost of len() function
Body : I have a nested list and I want to check if an item has a value or not. Body : What is the cost of len() function for Python
Not really sure how to describe it, so basically, how do I get this to work? built-ins? (list/tuple/string/dictionary)
relevant knowledge. For example, Table 2 shows two directly
linkable questions, one asks \iterating over a stack (reverse
list)" while the other asks \best way to check if a list is
empty". Again, the two questions share few common words,
but they embody strongly relevant knowledge (stack itera-
tion and list empty checking). Furthermore, questions may
discuss relevant knowledge but not directly for solving the
questions, as the examples in Table 3 shows. As such ques-
tions discuss indirectly linkable knowledge, they are unlikely
to share many common words. Traditional word represen-
tations (e.g., BM25 [26], LDA [5]) cannot reliably recognize
many cases of potentially linkable knowledge units when lex-
ical gaps exist between them.
As shown in the above examples, knowledge units can be
linkable for dierent purposes. Ye et al.'s empirical study [46]
on the knowledge network in Stack Overow conrms this
phenomenon. Being able to classifying dierent classes of
linkable knowledge units would support more targeted in-
formation needs when users search or explore the linkable
knowledge. For example, duplicate questions allow users
to understand a problem from dierent aspects, directly-
linkable questions help explain concepts or sub-steps of a
complex problem, while indirectly-linkable questions provide
extended knowledge. However, such multiclass semantic re-
latedness in software engineering knowledge is not consid-
ered in existing work [50], [6], which focuses on only bi-
nary classication, i.e., related or not. Furthermore, ex-
isting work heavily relies on human-engineered word- and
document-level features to train the classier, which are
hard to design for informal user-generated content in Stack
Overow.
In this paper, we propose a novel approach for predict-
ingmulticlass semantically linkable knowledge units in Stack
Overow. Inspired by Ye et al. [46], we consider four classes
of semantic relatedness: duplicate, direct linkable, indirectly
linkable, and isolated. To overcome the lexical gap issue,
Our approach recognizes and quanties semantic relatedness
between knowledge units using word embeddings [21], [20]
and Convolutional Neural Network (CNN) [13], which are
the state-of-the-art deep learning techniques for capturing
word-level and document-level semantics for Natural Lan-
guage Processing (NLP) tasks. Furthermore, our approachdoes not rely on human-engineered features to classify se-
mantic relatedness. Instead, we collect large amounts of dif-
ferent types of user-created knowledge-unit links (duplicate,
direct-linked, indirect-linked) from Stack Overow to train
the CNN to automatically learn the most informative word-
and document-level features to classify semantic relatedness
between knowledge units.
We conduct extensive experiments to compare our deep-
learning based approach with the baseline methods using
traditional word representations (TF-IDF) and the human-
engineered features for determining semantic similarity. Our
results show that: 1) our approach signicantly outperforms
the baseline methods, and performs much more consistently
for dierent classes of semantic relatedness; 2) both word
embeddings and CNN help improve the performance of mul-
ticlass classication of linkable knowledge units. CNN plays
a more important role for predicting duplicate, directly link-
able, and isolated knowledge unit, due to its capability of
capturing document-level semantic features, while word em-
beddings are good at predicting indirectly linkable knowl-
edge units; 3) domain-specic training corpus help improve
the performance of deep-learning techniques for specic tasks.
Our contributions are as follows:
We formulate the research problem of predicting mul-
ticlass linkable knowledge units in Stack Overow;
We propose a deep learning based approach to tackle
the problem;
We evaluate the eectiveness of our approach against
the traditional methods for predicting relevant soft-
ware engineering knowledge [30].
2. RELATED WORK
Many studies have been carried out on Stack Overow
[35, 44, 36, 41]. Our work predicts semantically linkable
knowledge in developers' discussions in Stack Overow. It
is related to two lines of research: link prediction in complex
networks and semantic relatedness in software data.
2.1 Link Prediction in Complex Networks
52Link Prediction focuses on detecting potentially linkable
objects in an observed network that is complex and evolves
dynamically. Link prediction in complex networks has at-
tracted enormous attention from physics, biochemical, and
computer science communities [7, 18, 19, 9, 3, 8]. A signi-
cant proportion of the related research falls into link predic-
tion in social networks, many of which have been found to be
complex networks, e.g., the user-user network in online Q&A
forums [49], in Twitter [14]. These social network research
work aims to understand the network evolution patterns and
identify potential social relations between users [18, 9].
Ye et al. [46] report that the knowledge network formed by
developers' URL sharing activities in Stack Overow is also
a complex network, i.e., the network structure is scale-free.
They show that \the rich get richer" eect [4] inuences the
knowledge sharing activities and the growth of the knowl-
edge network in Stack Overow. As a result, a small portion
of questions and answers attracts a large portion of devel-
opers' attention, while large amounts of relevant questions
and answers in the long tail of the power law distribution are
under linked. This nding motivates our work to predict po-
tentially linkable knowledge units, which could enhance the
knowledge sharing and search in Stack Overow.
Unlike link prediction in social network which predicts
only whether the two persons are linkable or not (i.e., bi-
nary classication), our work predicts dierent types of re-
latedness (duplicate, direct link, or indirect link) between
potentially linkable knowledge units (i.e., multiclass classi-
cation). This milticlass link prediction is again inspired by
the study of Ye et al. [46], which reveals that users link
relevant questions and answers for dierent purposes. They
suggest that mixing dierent types of linkable knowledge
units together could hinder the knowledge search as users
often need dierent types of relevant information in dier-
ent situations. Therefore, we treat the linkable knowledge
prediction as a multiclass classication problem.
2.2 Semantic Relatedness in Software Data
Measuring the relatedness (or similarity) between two pie-
ces of textual contents has long been studied. The un-
derlying mathematic model of textual contents has evolved
from Vector-Space-Model (e.g., TF-IDF), n-gram language
model, topic modeling (e.g., LDA [5]), to the recent develop-
ment of neural language model (e.g., distributed word rep-
resentations (or word embeddings) [32, 21, 20]). The trend
is towards semantically richer similarity measures. In 2013,
Mikolov et al. [21, 20] propose two neural language models
(referred to as word2vec in the literature) (continuous bag-
of-words and continuous skip-gram) and ecient negative
sampling method for learning word embeddings from large
amounts of text. In this work, we adopt continuous skip-
gram model for learning domain-specic word embeddings
from Stack Overow text.
In the software engineering community, researchers uti-
lize the textual similarity between two software artifacts to
solve various software engineering tasks, such as duplicate
or similar bug report detection [27, 23, 31, 29, 30], relevant
software tweets detection [28], duplicate online question de-
tection [50]. For example, Sun et al. use a Support Vector
Machine (SVM) classier in [30] and an IR based similarity
measure (BM25F [26]) in [29], respectively, to detect dupli-
cate bug reports. Zhang et al. [50] detect duplicate Stack
Overow questions by comparing their titles and questiondescriptions using topic model (LDA). These existing work
are formulated as a binary prediction problem, e.g., dupli-
cate or non-duplicate.
In the NLP community, the development of CNN architec-
tures for sentence-level and document-level text processing
is under intensive research. Some recent work utilizes CNN
to learn the semantic relations between two pieces of texts.
For example, Kim [16] proposes a simple CNN trained on
top of Mikolov's word embeddings [20], and then apply the
CNN to sentence classication. Bogdanova et al. [6] apply
CNN with word embeddings to duplicate question detection
in online Q&A sites. These CNN-based methods outperform
traditional NLP methods for sentence classication and du-
plication question detection.
Comparing with these existing works on semantic relat-
edness between two pieces of text, we leverage CNN with
domain-specic word embeddings for the problem of multi-
class classication of potentially linkable knowledge units in
Stack Overow, beyond the binary prediction of duplicate
content or not.
3. THE APPROACH
This section formulates our research problem and describes
the technical details of our approach.
3.1 Problem Formulation
We consider a question together with its entire set of an-
swers in Stack Overow as a knowledge unit regarding some
programming-specic issues. If two knowledge units are se-
mantically related, we consider them as linkable . We further
predict the types of the relatedness between the two knowl-
edge units. Inspired by Ye et al.'s study [46] on the purposes
of URL sharing, we dene four types of the relatedness be-
tween the two knowledge units:
Duplicate: Two knowledge units discuss the same ques-
tion in dierent ways, and can be answered by the same
answer.
Direct link: One knowledge unit can help solve the
problem in the other knowledge unit, for example, by
explaining certain concepts, providing examples, or
covering a sub-step for solving a complex problem.
Indirect link: One knowledge unit provides related in-
formation, but it does not directly answer the question
in the other knowledge unit.
Isolated: The two knowledge units are not semantically
related.
We formulate our task as a multiclass classication prob-
lem. Specially, given two knowledge units, our task is to
predict whether the two knowledge units have one of the
above four types of relatedness.
3.2 Overview of Main Steps
In NLP tasks, words are usually represented as vectors. In
this work, we use word embeddings (distributed representa-
tions of words in a vector space) [32, 21] as word representa-
tions, because word embeddings require only large amounts
of unlabeled text to train, and have been shown to be able
to capture rich semantic and syntactic features of words.
We use continuous skip-gram model [20] (the Python im-
plementation in Gensim [25]) to learn domain-specic word
embeddings from large amounts of software engineering text
from Stack Overow questions and answers. The output is
a dictionary of word embeddings for each unique word.
53Data Extraction 
& Preprocessing
Knowledge 
Unit TextsWord2vecWord 
EmbeddingsLookupKU 
Vector 
Pairs
Prediction
PhaseLookupKnowledge -unit 1
Knowledge -unit 2KU Vector 1
KU Vector 2Trained 
CNN ModelÔÅ±Duplicate Link 
ÔÅ±Direct Link
ÔÅ±Indirect Link
ÔÅ±IsolatedUser -linked Knowledge -unit (KU) Pairs
Duplicate Link
Direct Link
Indirect Link
IsolatedTrainingPhaseInput LayerHidden Layer
Output LayerConvolutional Neural Network
Cosine
LossFigure 1: Overview of the Main Steps
Word embeddings encode word-level semantics. To pre-
dict semantic relatedness between knowledge units, we need
to capture semantics at sentence and knowledge unit level.
To this end, we resort to convolutional neural network[13].
Given two knowledge units, they are rst converted into
word vector representations by looking up the word embed-
dings dictionary, which are then fed into a CNN to produce
a feature vector for each knowledge unit. The similarity of
the two knowledge units are measured by the cosine simi-
larity of their feature vectors. Based on the similarity score,
the relatedness of the two knowledge units are classied as
one of the four classes dened in Section 3.1.
To train the CNN for the prediction task, we collect a
set of user-linked knowledge-unit pairs from Stack Overow
for each type of relatedness: duplicate, direct link, indirect
link, and isolated. This set of training data is fed into the
CNN and the training is guided by the cosine loss of the re-
latedness of the knowledge-unit pairs against the user-linked
relatedness. During the training process, the CNN automat-
ically learns the most informative word and sentence features
for predicting multiclass linkable knowledge units.
3.3 Learning Word Representations
Distributed word representations assume that words ap-
pear in similar context tend to have similar meanings [11].
Therefore, individual words are no longer treated as unique
symbols, but mapped to a dense real-valued low-dimensional
vector space. Each dimension represents a latent semantic or
syntactic feature of the word. Semantically similar words are
close in the embedding space. Figure 2 shows some exam-
ples of domain-specic word embeddings we learn from Stack
Overow java text, visualized in a two-dimensional space us-
ing the t-SNE dimensionality reduction technique [33]. Se-
mantically close words, such as JPanel ,JButton, JFrame
andJLabel which belong to GUI component are close in the
vector space.
Word embeddings are unsupervised word representations,
which requires only large amounts of unlabeled text to learn.
In this work, we collect body content of questions and an-
swers from Stack Overow as software engineering text. We
preprocess the body content by removing large code snip-
pets in<pre> HTML tag and cleaning HTML tags. Short
code elements in < code > HTML tag in natural language
sentences are kept. We use the software-specic tokenizer
JavaEEJUnitGUIBuildFigure 2: Example of Word Embedding
developed in [45, 47] to tokenize the sentences. This tok-
enizer preserves the integrity of code tokens. For example,
it tokenizes dataframe:apply () as a single token, instead
of a sequence of 5 tokens: dataframe . apply ( ) . This
supports more accurate domain-specic word embeddings
learning because code tokens will not be mixed with normal
words.
Word embeddings are typically learned using neural lan-
guage models. In this work, we use continuous skip-gram
model, a popular word to vector (word2vec) model proposed
by Mikolov et al. [21], [20]. As shown in Figure 3, continu-
ous skip-gram model learns the word embedding of a center
word (i.e., wi) that is good at predicting the surrounding
words in a context window of 2 k+ 1 words (k = 2 in this
example). The objective function of skip-gram model is to
maximize the sum of log probabilities of the surrounding
context words conditioned on the center word:
nX
i=1X
 kjk;j6=0logp(wi+jjwi)
wherewiandwi+jdenote the center word and the context
word in a context window of length 2 k+ 1.ndenotes the
length of the word sequence.
54Word Vector
of  ùê§ùê§ùê§ùê§ùê±ùê±Input LayerHidden Layer
CONV                    RELU                  POOL
Feature Vector
of ùêüùêüùêüùêüùê±ùê±
Feature Vector
ofùêüùêüùêüùêüùê≤ùê≤Output Layer
Word Vector
ofùê§ùê§ùê§ùê§ùê≤ùê≤1-gram
3-gram
5-gram
7-gram
9-gramRelatedness( kux,kuy)
=ùêüùêüùêüùêüùê±ùê±‚ãÖùêüùêüùêüùêüùê≤ùê≤
‚à•ùêüùêüùêüùêüùê±ùê±‚à•‚à•ùêüùêüùêüùêüùê≤ùê≤‚à•Figure 3: Architecture of the Proposed CNN
The logp(wi+jjwi) is the conditional probability dened
using the softmax function :
p(wi+jjwi) =exp(v0T
wi+jvwi)P
w2Wexp(v0Twvwi)
wherevwandv0
ware respectively the input and output vec-
tors of a word win the underlying neural model, and W
is the vocabulary of all words. Intuitively, p(wi+jjwi) esti-
mates the normalized probability of a word wi+jappearing
in the context of a center word wiover all words in the vo-
cabulary. Mikolov et al. [21] propose an ecient negative
sampling method to compute this probability.
The output of the model is a dictionary of words, each
of which is associated with a vector representation. Given
a knowledge unit, its title, description and answers con-
tent (after proper preprocessing) will be converted into a
knowledge-unit vector by looking up the dictionary of word
embeddings and concatenating the word embeddings of words
comprising the body content. Let wvi2Rdbe thed-
dimensional word vector corresponding to the i-th word in
a knowledge unit. The knowledge unit of nwords is repre-
sented as,knv=wv1wv2:::wvn, which is used as
input to the CNN.
3.4 The CNN Architecture
Figure 3 presents the architecture of our CNN. Our CNN
takes as input a knowledge-unit vector knv=wv1wv2
:::wvn, and captures the most informative word and sen-
tence features in the knowledge unit for determining seman-
tic relatedness. Treating the knowledge-unit vector as an
\image", we perform convolution on it via linear lters. Be-
cause each word is represented as a d-dimensional vector, we
use lters with widths kequal to the dimensionality of the
word vectors (i.e., k=d). Letwvi:i+h 1 refer to the con-
catenation vector of hadjacent words wi;wi+1;:::;w i+h 1 .
A convolution operation involves a lterw2Rhk(a vector
ofhkdimensions) and a bias termb2Rh, which is applied
tohwords to produce a new value oi2R:
oi=wTwvi:i+h 1 +b (1)
wherei= 1:::n h+1, andis the dot product between the
lter vector and the word vector. This lter is applied re-
peatedly to each possible window of hwords in the sentences
(zero padding where necessary) in the knowledge unit (i.e.,
wv1:h;wv2:h+1;:::;wv n h+1:n ) to produce an output sequence
o2Rn h+1, i.e.,o= [o 1;o2;:::;o n h+1 ]. We apply the non-
linear activation function ReLu [10] to each oito produce a
feature map c2Rn h+1whereci=ReLu (oi) =max(0;o i).
To capture features of phrases of dierent length, we vary
thewindow size hof the lter, i.e., the number of adja-cent words considered jointly. In our CNN, we use lters
of 5 dierent window size (1, 3, 5, 7, 9), which can capture
features of dierent n-grams respectively. For each window
size, we use 128 lters to learn complementary features from
the same word windows. The dimensionality of the feature
map generated by each lter will vary as a function of the
knowledge-unit length and the lter's window size. Thus,
a pooling function should be applied to each feature map
to induce a xed-length vector. In this work, we use 1-
max pooling , which extracts a scalar (i.e., a feature vector
of length 1) with the maximum value in the feature map
for each lter. Together, the outputs from each lter can
be concatenated into a top-level feature vector, denoted as
fvku. This feature vector would capture the most informa-
tive 1, 3, 5, 7 and 9-grams in the input knowledge unit.
Given two knowledge units kuxandkuy, the CNN outputs
their respective feature vectors fvxandfvy. For this pair of
feature vectors fvxandfvy, the CNN computes a similarity
score between fvxandfvyin the last layer, using the cosine
similarity, i.e.,
relatedness(ku x;kuy) =fvxfvy
kfvxkkfv yk
In the training phase, the computed similarity score will be
used to compute the mean square error against the ground-
truth similarity score for the given pair of knowledge units
(see Section 3.5). In the prediction phase, the computed
similar score will be used to determine the class of semantic
relatedness of the given two knowledge units. Because the
computed similarity score is a continuous value, we need to
bin the similarity score as four discrete classes of semantic
relatedness between the two knowledge units: [0; 0:25) as
isolated, [0:25; 0:5) as indirect link, [0 :5;0:75) as direct link,
and [0:75; 1] as duplicate.
3.5 Training the CNN
Training of the proposed CNN follows supervised learn-
ing paradigm. According to our task objective, the train-
ing data must consist of sucient examples of four types
of related knowledge-unit pairs, i.e., duplicate, direct link,
indirect link and isolated, so that the CNN can learn to cap-
ture informative word and sentence features for classifying
knowledge-unit relatedness.
Fortunately, URL sharing activities by Stack Overow
users create all types of the needed relatedness. User-created
links between knowledge units are stored in the post links ta-
ble. We parse the post links to generate the training dataset
as follows. First, we randomly select pairs of user-linked
knowledge-unit pairs. If the LinkTypeId is 3 (i.e., duplicate
posts), we mark the selected pair as an instance of duplicate
knowledge units. Otherwise, the selected pair is an instance
of direct-link. Second, we randomly select pairs of knowl-
edge units that are only transitively linked (i.e., the shortest
path between the knowledge units is at least 2). These pairs
of knowledge units are instances of indirect-link knowledge
units. Finally, we randomly select pairs of knowledge units
that do not have links in the post links table. These pairs
are instances of isolated knowledge units. We select equal
number of instances for dierent types of relatedness.
Let the tuple <ku x;kuy;simValue> be a pair of knowl-
edge units in the training dataset TD, andsimValue is the
ground-truth similarity score between kuxandkuy. In this
work, we set 0.125, 0.375, 0.625 and 0.875 as the ground-
55truth similarity score for the four classes: isolated, indirect
link, direct link and duplicate, respectively. The CNN with
the current parameter set computes a similarity score be-
tween the knowledge units, i.e., relatedness(ku x;kuy). The
training objective is to minimize the mean-squared error of
the computed similarity score and the ground-truth similar-
ity score (i.e., cosine loss ) with respect to :
7!1
jTDjX
TD(simValue relatedness(ku x;kuy))2
We add the l2 norm regularization loss to the mean-squared
error data loss. The l2 norm constraint penalizes large
weight parameters. It helps avoid model overtting, be-
cause no input dimension can have a very large inuence
on the predicted probabilities all by itself. The parameters
to be estimated include: the convolution lters wand the
bias termsb(see Eq. 1). We solve the optimization problem
using Adam update algorithm [17]. Once we learn the CNN
parameters, the CNN can be used to predict the semantic
relatedness of an unseen pair of knowledge units.
4. EXPERIMENT
We conduct a set of experiments to evaluate the eec-
tiveness of our approach, compare it against a well-designed
baseline, and investigate the impact of domain-specic word
embeddings. As our approach relies on deep learning tech-
niques, we also report the training cost of our approach.
4.1 Baseline Building
As our task is essentially to predict relatedness between
two pieces of text, we design two baselines, each of which is
a multiclass SVM classier with dierent textual features,
i.e., traditional Term Frequency (TF) and Inverse Document
Frequency (IDF) versus word embeddings.
4.1.1 Feature Extraction with TF and IDF
TF and IDF are widely used in predicting textual simi-
larity with cosine distance [30]. In this baseline, we dene
in total 36 features based on the TF and IDF values of the
words in a pair of knowledge units. That is, the pair of the
knowledge units is represented as a 36-dimensional feature
vector to be used as input to the SVM classier.
Given a knowledge unit KUx, we split its text into three
sub-documents, denoted as Ci
x(1i3), which corre-
spond to question title (i = 1), question body (i = 2), and
question title plus question body plus body of all answers
(i= 3). Let<KU x;KU y>be a pair of knowledge units.
For TF-based features, we rst obtain a vocabulary Vi;j
x;y
with all words in a combination of dierent sub-documents of
the two knowledge units, i.e., Vi;j
x;y=fwjw2Ci
xSCj
y(1
i;j3)g. The TF value of each word in the vocabulary
Vi;j
x;yis then computed. These word TF values are used to
convert the sub-document Ci
xof the knowledge unit KUx
into a vector representation vi
x. The dimensionality of the
vi
xis the size of the vocabulary, and each dimension is the TF
value of the word in the sub-document Ci
x, or 0 otherwise.
We compute the cosine similarity of the two vectors vi
xand
vj
yto measure the similarity of the sub-document Ci
xandCj
y
of the knowledge unit KUxandKUy, respectively. As each
knowledge unit has three sub-documents, i.e., 1 i;j3,
we obtain 33 (9) TF-based features for a pair of knowledge
units.For IDF-based features, we rst combine the respective
sub-documents Ci
xof all knowledge units in a dataset as
a corpusCi. The IDF value of a word win the corpus
Ciis then computed, denoted as idfi(w). We measure the
IDF-based similarity between the sub-documents Ci
xof the
knowledge unit KUxand the sub-document Cj
yof the knowl-
edge unitKUyas:
SimIDF k(Ci
x;Cj
y) =X
w2CixTCj
yidfk(w)
i.e., the sum of the IDF value of the common words of Ci
x
andCj
yin the corpus Ck. As 1i;j;k3, we obtain
333 (27) IDF-based features for a pair of knowledge
units.
4.1.2 Feature Extraction with Word Embeddings
The semantic information of a piece of text can be cap-
tured by taking the mean of the word embeddings of the
words comprising the text [15]. Following this treatment,
we compute the mean of the word embeddings of the words
in a knowledge unit KUx(including question title plus ques-
tion body plus body of all answers) as the word embedding
of the knowledge unit, i.e.,
wv(KU x) =1
nX
w2Bxwv(w)
whereBxis the bag of words of the KUxandnis the size
ofBx. A pair of knowledge units KUxandKUyis then
represented as the mean of the two knowledge-unit word
embeddings, i.e.,
wv(KU x;KU y) =1
2(wv(KU x) +wv(KU y))
which will be used as input to the SVM classier. In this
work, we use 200-dimensional word vector.
4.1.3 Multiclass SVM ClassiÔ¨Åer
We develop a multiclass SVM classier to predict the re-
latedness of the two knowledge units. Based on the feature
vector used (TF-IDF based or word embedding based as
described above), we have two baselines: Baseline1 (TF-
IDF+SVM) and Baseline2 (WordEmbed+SVM). The two
baseline SVM classiers are trained using the same dataset
as that for training our approach. As our task is a multi-
class classication problem, we use RBF kernel [34] in the
SVM model. We set the parameter of the SVM to 1 =k,k
denotes the dimensionality of the feature vector, i.e., 1 =36
for the TF-IDF+SVM baseline and 1 =200 for the WordEm-
bed+SVM baseline. We use grid search [12] to optimize the
SVM parameters.
4.2 Dataset
Our experimental data is from Stack Overow data dump
of March 20161.
Word Embedding Corpora. From the posts table,
we randomly select 100,000 knowledge units tagged with
\java"as the word embedding corpora. Note that our cor-
pora contains not only questions but also question answers.
Training and test linkable knowledge units. From
thepostlinks table, we randomly select in total 6,400 pairs of
knowledge units that are tagged with \java", i.e., 1,600 pairs
1https://archive.org/download/stackexchange
56for each type of relatedness dened in Section 3.1 (Duplicate,
Direct Link, Indirect Link and Isolated ). These 6,400 pairs
of knowledge units are used as training data. For test data,
we select in total 1,600 pairs of knowledge units that are
tagged with\java", i.e., 400 pairs for each type of relatedness.
User-created links are considered as ground truth label for
the semantic relatedness of the selected pairs of knowledge
units. The knowledge units in the test data does not overlap
with those in the training data.
4.3 Evaluation Metrics
We represent the multiclass classication results as a K
KmatrixA, whereKis the number of classes (in our work
K= 4). The rows represent the ground truth labels and
the columns represent the predicted labels. The value of the
Aijis the number of times that a pair of knowledge units
with the ground truth label Lithat is classied as the label
Lj. Therefore, the value of Aiiis the number of correct
classication for the label Li,P
1jKAijis the number of
the ground truth label Liin a dataset,P
1iKAijis the
number of predicted label Ljin a dataset, andP
iP
jAij
is the number of knowledge-unit pairs in a dataset.
Precision, recall, and F1-scores as the evaluation metrics
are standard and widely used to evaluate the eectiveness of
a prediction technique [37, 38, 39, 40, 42, 43, 51]. Thus, we
use them to compare our approach with the two baselines:
Accuracy is dened as the proportion of numbers of cor-
rect classication in a dataset, i.e.,
Accuracy =P
iAiiP
iP
jAij
Precision for a class jis the proportion of knowledge-
unit pairs correctly classied as the class jamong all pairs
classied as the class j. Precision for all classes is the mean
of the precision for each class.
Precision j=AjjP
1iKAij
Recall for a class iis the percentage of knowledge-unit
pairs correctly classied as the class icompared with the
number of ground truth label Liin the dataset. Recall for
all classes is the mean of the recall for each class.
Recall i=AiiP
1jKAij
F1-score for a classiis a harmonic mean of precision and
recall for that class. F1-score for all classes is the mean of
the F1-score for each class.
F1i=2Precision iRecall i
Precision i+Recall i
F1-score evaluates if an increase in precision (or recall)
outweighs a loss in recall (or precision). As there is often
a trade o between precision and recall, F1-score is usu-
ally used as the main evaluation metric in many software
engineering papers [24, 48]. In this paper, we also choose
F1-score as the main evaluation metric.
4.4 Research Questions and Findings
In our experiment, we are interested in the following two
research questions:Table 4: Accuracy Comparison
Accuracy
Baseline1 TF-IDF + SVM 0.625
Baseline2 Word Embedding + SVM 0.669
Our Approach Word Embedding + CNN 0.841
4.4.1 Overall Comparison
RQ1: How much improvement can our approach achi-
eve over the baseline approaches?
Motivation. Our approach uses deep learning techniques
(word embeddings and CNN-derived features) to quantify
semantic relatedness between two pieces of software engi-
neering text, which is very dierent from the baseline ap-
proaches using traditional TF-IDF word representations and
human-engineered features. Answer to this research ques-
tion will shed light on whether and to what extent deep
learning techniques can improve the results of the multiclass
classication task on software engineering text.
Approach. We apply our approach and the baseline ap-
proaches to the test data, i.e., 1,600 pairs of linkable knowl-
edge units. We compare the accuracy, precision, recall, and
F1-score metrics of dierent approaches.
Result. Table 4 and Table 5 present the results. We can see
that the accuracy of our approach outperforms the Baseline1
and Baseline2 by 34.6% and 25.7%, respectively. Overall
(see the last column of Table 5), our approach outperforms
theBaseline1 and Baseline2 in terms of F1-score by 36.5%
and 28.2%, respectively. Similar improvement on overall
precision and recall can be observed.
Overall, our approach achieves the best performance in all
the evaluated metrics by a substantial margin, compared
with the two baseline approaches.
4.4.2 Comparison by Different Classes
RQ2: How eective is our approach in predicting dif-
ferent classes of semantic relatedness, compared with
the baseline approaches?
Motivation. Dierent from existing work on binary text
classication, our approach is for multiclass text classica-
tion. Dierent classes of semantic relatedness may exhibit
dierent word and sentence features, which may aect the
eectiveness of dierent semantic similarity measures. We
would like to investigate the advantages and disadvantages
of our approach and the baseline approaches for predicting
dierent classes of semantic relatedness.
Approach. We compare the precision, recall and F1-score
for each class of relatedness (i.e., duplicate, direct link, indi-
rect link, and isolated) by our approach and the two baseline
approaches (see Table 5).
Result. The performance of our approach and the baseline
approaches do vary for dierent classes of relatedness. Our
approach performs the best for three of the four classes (i.e.,
duplicate, direct link and isolated) on all the evaluation met-
rics, while the Baseline2 performs the best for the indirect
link class only on recall and F1-score. In fact, the Baseline2
achieves the extremely high recall (0.980) but low precision
(lower than our approach) for the indirect link class. The
performance variance of our approach for dierent classes is
small (0.05-0.09 in terms of F1-score), compared with the
performance variance of the two baseline methods for dier-
ent classes (in terms of F1-score, 0.1-0.27 for the Baseline1
and 0.27-0.39 for the Baseline2).
57Table 5: Precision, Recall and F1-Score of Our Approach and the Baseline Approaches
DuplicateDirect
LinkIndirect
LinkIsolated Overall
PrecisionBaseline1 TF-IDF + SVM 0.585 0.611 0.542 0.767 0.626
Baseline2 Word Embedding + SVM 0.611 0.560 0.787 0.676 0.659
Our Approach Word Embedding + CNN 0.898 0.758 0.840 0.890 0.847
RecallBaseline1 TF-IDF + SVM 0.81 0.413 0.505 0.773 0.625
Baseline2 Word Embedding + SVM 0.725 0.433 0.980 0.538 0.669
Our Approach Word Embedding + CNN 0.898 0.903 0.773 0.793 0.842
F1-ScoreBaseline1 TF-IDF + SVM 0.679 0.493 0.523 0.770 0.616
Baseline2 Word Embedding + SVM 0.663 0.488 0.873 0.600 0.656
Our Approach Word Embedding + CNN 0.898 0.824 0.805 0.849 0.841
Our approach performs better on more classes of related-
ness than the baseline methods (3:1). Our approach also
performs more consistently for dierent classes of related-
ness. The Baseline2 is extremely good at recalling indirectly
linkable knowledge units with a sacrice of precision, but
its performance varies greatly for dierent classes of relat-
edness.
4.4.3 Effects of Word Embeddings and CNN
RQ3: What is the impact of word embeddings and
CNN on the performance improvement respectively?
Motivation. Word embeddings and CNN are two deep
learning techniques our approach relies on. They help cap-
ture semantics at word-level and sentence/document-level
respectively. Answer to this research question helps us un-
derstand the importance of dierent levels of semantics for
the multiclass text classication task.
Approach. To understand the importance of word embed-
ding, we compare the performance of the Baseline1 (i.e., TF-
IDF+SVM) and the Baseline2 (i.e., WordEmbedding+SVM).
To understand the importance of CNN, we compare the per-
formance of the Baseline2 (i.e., WordEmbedding+SVM) and
our approach (i.e., WordEmbedding+CNN).
Result. Overall, the Baseline2 outperforms the Baseline1
by a small margin (see Accuracy in Table 4 and the Overall
Column in Table 5). This suggests that word embeddings
can moderately improve the text classication performance
compared with the traditional TF-IDF word representation.
Overall, our approach outperforms the Baseline2 by a sub-
stantial margin. This suggests that CNN plays a more im-
portant role than word embeddings for improving the mul-
ticlass classication performance.
The performance of the Baseline2 is not always better
than that of the Baseline1 for dierence classes of relat-
edness. For some classes, the Baseline2 has better preci-
sion but worse recall than the Baseline1, and vice versa
for other classes. In terms of F1-score, the two baselines
are almost the same for duplicate and direct link classes,
while the Baseline2 is signicantly better than the Base-
line1 for indirect link class, but signicantly worse for iso-
lated classes. This suggests that word-level semantics en-
coded in word embeddings may not be appropriate for de-
termining all classes of relatedness. For isolated knowledge
units, traditional TF-IDF representation which is sensitive
to lexical gap performs better than word embeddings. Tak-
ing the mean of word embeddings as knowledge-unit rep-
resentations may blur the semantic distinction between the
knowledge units, which helps the Baseline2 recall indirectly
linkable knowledge units, but degrades the performance of
the Baseline2 for isolated knowledge units.
In contrast, our approach consistently outperforms the
Baseline2 on all the evaluation metrics for dierent classesof relatedness, except the recall and F1-score for indirectly
linkable knowledge units. This suggests that word-level se-
mantics are especially useful for determining semantic sim-
ilarity of indirectly linkable knowledge units. As indirectly
linkable knowledge units may not exhibit semantic similarity
at sentence/document level, considering sentence/document-
level semantics by the CNN could rule out false-negative
links, which degrades its recall.
Both word embeddings and CNN help improve the perfor-
mance of multiclass text classication, but CNN has a big-
ger impact than word embeddings. Word-level semantics are
especially useful for predicting indirectly linkable knowledge
units, while sentence/document-level semantics plays more
signicant role for predicting duplicate, directly linkable and
isolated knowledge units.
4.4.4 Impact of Domain-SpeciÔ¨Åc Word Embeddings
RQ4: How sensitive is our approach to word embed-
dings learned from dierent corpus?
Motivation. In this work, we predict linkable knowledge
in developers' online forum. Therefore, we learn word em-
beddings from Stack Overow text which is representative
of the ways people ask/answer questions and the vocabu-
lary people use. We would like to investigate whether and
to what extent word embeddings learned from a dierent
corpora aect the performance of our approach. This will
help us understand the importance of suitable corpus for a
software-specic machine learning task.
Approach. We collect a corpus of Wikipedia web pages
from the Wikipedia data dump2. The number of sentences
and the size of the vocabulary of the Wikepedia corpus is
comparable to that of the corpus of the 100,000 knowledge
units from Stack Overow. We learn word embeddings from
the Wikipedia corpus and use it to train the CNN subse-
quently. We compare the performance of the two CNNs,
one trained with Stack Overow word embeddings, and the
other trained with Wikipedia word embeddings.
Table 6: Performance of Our Approach with Word
Embeddings Learned from Dierent Corpus
AccuracyOverall
PrecisionOverall
RecallOverall
F1-Score
General Corpus
From Wikipedia0.770 0.790 0.776 0.777
Domain-specic Corpus
From Stack Overow0.841 0.847 0.842 0.841
Result. Table 6 presents the accuracy, and the overall pre-
cision, recall and F1-score of our approach with domain-
specic word embeddings (i.e., Stack Overow) versus gen-
eral word embeddings (i.e., Wikipedia). General word em-
beddings degrade the performance of our approach, com-
2https://dumps.wikimedia.org/enwiki/latest/
58pared with domain-specic word embeddings. However, the
degrade is moderate, in terms of accuracy, precision, recall
and F1-score by 7%, 5%, 7% and 7%, respectively. Further-
more, although the performance of our approach degrades
with general word embeddings, it still outperforms the two
baseline methods (at least 11% on all the evaluation met-
rics).
Our approach demonstrates certain level of reliability even
with word embeddings learned from a general corpus that is
completely dierent from Stack Overow discussions. How-
ever, suitable domain-specic word embeddings leads to bet-
ter performance.
4.4.5 Training Cost
RQ5: What is the time cost for training the underly-
ing deep learning models?
Motivation. The underlying word embeddings and CNN
models need to be trained before they can be used for pre-
diction task. Training of word embeddings and the CNN
model is done only once oine. After model training, using
the model to predict the relatedness of a pair of knowledge
units takes negligible time. Understanding the training time
cost helps us understand the practicality of our approach.
Approach. We record the start time and the end time of
the program execution to obtain the training time cost of the
word embeddings and the CNN model. The experimental
environment is an Intel(R) Core(TM) i7 2.5 GHz PC with
16GB RAM running Windows7 OS (64-bit).
Result. For learning word embeddings model, the 100,000
knowledge units contain 23,759,119 words (as bags of words),
and the vocabulary size (i.e., the number of unique words)
is 434,836. It takes about 15 minutes to analyze the text of
these 100,000 knowledge units to learn the word embeddings.
For training the CNN model, it takes about 14 hours for the
CNN model to achieve the loss convergence ( <e 3).
Training of the word embeddings model and the CNN model
can be done eciently oine and only need to be done once.
Our approach is practical for large dataset.
5. DISCUSSION
This section presents the qualitative analysis of some ex-
amples to illustrate the capability of our approach and dis-
cusses threads to validity of our experiments.
5.1 Qualitative Analysis
Table 7 presents one example for each class of linkable
knowledge unit and the classication results by our approach
and the two baselines. The rst example is a pair of dupli-
cate questions. However, the two questions do not have
many words in common. The two baseline methods classify
them as indirectly linkable knowledge units. In contrast, our
approach can capture semantic similarity between terms like
\standard input / output"and\System.out.println()"3. This
helps our approach classify the two questions as duplicate.
In the second example, the two knowledge units contain
directly relevant knowledge, i.e., the \GUI repaint() prob-
lem" is directly relevant to \text not displaying correctly"
problem. Unfortunately, the two knowledge units share few
words in common, which makes the baseline methods classify
them as isolated. Our approach can capture the semantic
3Note that we use software-specic tokenizer [45, 47] which
can preserve the integrity of code tokens.relatedness between the two technical problems, and thus
correctly classify the two knowledge units as direct link.
In the third example, the two indirectly linked questions
share some common words. Based on these overlap words,
the Baseline1 which essentially relies on lexical similarity
of overlapping words classies the two questions as dupli-
cate. The Baseline2 which considers word-level semantics
by word embeddings makes a better judgment, classifying
them as isolated. Our approach makes the most accurate
prediction by taking into account not only word-level but
also document-level semantics. Similarly, in the fourth ex-
ample, the Baseline1 makes the least appropriate prediction
based on overlapping words, the Baseline2 makes a better
judgment based on word-level semantics, and our approach
makes the most accurate prediction.
5.2 Error Analysis
We also analyze the cases in which our approach makes the
wrong prediction. We nd that many cases that fail our ap-
proach involve knowledge units whose essential information
is presented as a code snippet4or an image5. In this work,
we remove code snippets and images during preprocessing.
In the future, we could incorporate code snippets and images
into our approach. Incorporating image semantics into our
approach would be relatively easy because CNN is originally
invented for image classication. Incorporating code seman-
tics into our approach could be a challenging task. A recent
work by Mou et al. [22] proposes to use CNN to encode the
program ASTs for program analysis tasks. However, their
approach is not directly applicable to code snippets in Q&A
discussion, which is usually incomplete and cannot be com-
plied. We plan to tackle this challenge as our future work.
5.3 Threats to Validity
There are several threats that may potentially aect the
validity of our experiments. Threats to internal validity re-
late to errors in our experimental data and tool implemen-
tation. We have double checked our experimental data and
tool implementation. We have also manually checked the
selected knowledge units in our dataset to ensure that they
are really tagged with "java" and have the right types of
knowledge-units links. Threats to external validity relate to
the generalizability of our results. In this study, we use a
medium-size training and test dataset (100,000 knowledge
units for word embedding learning, 6,400 pairs of knowl-
edge units for CNN training, and 1,600 pairs of knowledge
units for testing). This allows us to perform some manual
analysis to understand the capability and limitations of our
approach. In the future, we will reduce this threat by ex-
tending our approach to larger word embeddings corpus and
more knowledge-unit pairs for training and testing.
6. CONCLUSION AND FUTURE WORK
In this paper, we propose a novel deep-learning based ap-
proach for predicting multiclass semantically linkable knowl-
edge units in Stack Overow. Our approach can predict four
types of semantic relatedness, duplicate link, direct link, in-
direct link and isolated . At word level, our approach adopts
4For example, http://stackoverow.com/questions/
5985912/simpledateformat-bug
5For example, http://stackoverow.com/questions/
4382178/android-sdk-installation-doesnt-nd-jdk
59Table 7: Examples of Dierent Classes of Semantically Linkable Knowledge Units
Knowledge Unit1 Knowledge Unit2 Prediction
DuplicateQuestion Id : 2169330 Question Id : 10561540 Our approach :
Duplicate Link
Baseline1:
Indirect Link
Baseline2 :
Indirect LinkTitle : Java, Junit - Capture the standard
input /Output for use in a unit testTitle : In JUnit testing is it possible to
check the method System.out.println()
Body : I'm writing integration tests using
JUnit to automate the testing of a console
based application. /.../
I only need to have the test execute and
verify the the output is what is expected
given the input./.../Body:Is it possible to check, through
JUnit testing, if the method
System.out.println("One, Two") ,
actually prints One, Two?
Direct
LinkQuestion Id : 1775858 Question Id : 369823 Our approach :
Direct Link
Baseline1 :
Isolated
Baseline2 :
IsolatedTitle:Text in Label notdisplaying correctly
withsetText methodTitle : Java GUI repaint() problem?
Body : I'm trying to set the text in a label
dynamically by calling the setText method
whenever a button is clicked. /.../ that are
passed to the setText method aren't visible
on the screen when the submit button is
clicked until I click on the window and drag
to resize it. /.../
On a PC, the strings are are visible but
obscured until I resize thewindow . /.../
On the Mac, the strings appear as intended,
however, they're obscured until the window
is resized. What am I missing?Body:I have a JFrame. This JFrame
contains a JButton . I click the
JButtonand 10JTextFields are created.
the problem: I cannot see them until
"I force a repaint() " by resizing the
window . Only then do I see the
JTextFields created. CODE:
/.../
Answer : Calling validate() on the Frame as
mentioned here (Link to Question Id: 369823)
solved the problem.Answer:Container.add API docs sayeth:
Note: If a component has been added to
a container that has been displayed,
validate must be called on that container
todisplay the new component . If
multiple components are being added,
you can improve eciency by calling
validate only once./.../
Indirect
LinkQuestion Id :465099 Question Id :6973820 Our approach :
Indirect Link
Baseline1 :
Duplicate Link
Baseline2 :
IsolatedTitle:Best way to build a Plugin
system with JavaTitle: plugin base pattern for web app
Body:How would you implement
aPlugin-system for your Java application?
Is it possible to have an easy to use (for the
developer) system which achieves the
following: Users put their plugins into a
subdirectory of the app.
ThePlugin can provide a conguration
screen If you use a framework, is the
license compatible with commercial
developement?Body:I would like in a web application
to be able to add some functionnality. Is
there any pattern for a web application
to be able to add plugin ? Every plugins
could be a jar le and the user could
select the plugin it want to use. Does
somebody have information about this
kind of action?
IsolatedQuestion Id :2438387 Question Id : 644814 Our approach :
Isolated
Baseline1 :
Direct Link
Baseline2 :
Indirect LinkTitle : Hibernate - How to store java.net.URL
into a database through HibernateTitle:Does Hibernate EntityManager
include Core?
Body : I have a eld URL countryURL; in a
Country class. I want to store its data into a
COUNTRY table in a database through
Hibernate.Which Hibernate type I should use
in the hibernate mapping le
/...code.../
It is not excepting string and text type.Body:I'm using Hibernate's
implementation of JPA. /.../
My question is, does Hibernate
EntityManager depend on or
use the Hibernate Core code? /.../
I want to know if EntityManager
uses Core and thus has this x.
the state-of-the-art distributed word representations (i.e.,
word embedding) to encode word semantics in dense low-
dimensional real-valued vectors. At document level, we train
a CNN to automatically learn the most informative word
and sentence features for classifying the semantic related-
ness between two knowledge units. The training of the
CNN is guided by sucient examples of dierent types of
semantically linkable knowledge units. Due to the adop-
tion of word-level and document-level semantics, our ap-
proach is robust to the lexical gap (i.e., sharing few words
in common) between linkable knowledge units. Our exper-
iments conrm the eectiveness and consistency of our ap-
proach for predicting multiclass semantically linkable knowl-edge units, compared with the well-designed baselines using
TF-IDF word representations and human-crafted word and
sentence/document features. In the future, we will enhance
our approach by incorporating image and code-snippet se-
mantics into our framework. We will also develop automated
tool to help developers search and explore dierent types of
linkable knowledge in Stack Overow.
Acknowledgments. This work was partially supported by
Singapore MOE AcRF Tier-1 grant M4011165.020 and the
NSFC Program (No.61572426), and National Key Technol-
ogy R&D Program of the Ministry of Science and Technol-
ogy of China under grant 2015BAH17F01.
607. REFERENCES
[1] How are related questions selected?,
http://meta.stackexchange.com/questions/20473.
[2] How to ask a good question?,
http://stackoverow.com/help/how-to-ask.
[3] M. Al Hasan and M. J. Zaki. A survey of link
prediction in social networks. In Social network data
analytics , pages 243{275. Springer, 2011.
[4] A.-L. Barab asi and R. Albert. Emergence of scaling in
random networks. Science , 286(5439):509{512, 1999.
[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. the Journal of machine Learning
research , 3:993{1022, 2003.
[6] D. Bogdanova, C. N. dos Santos, L. Barbosa, and
B. Zadrozny. Detecting semantically equivalent
questions in online user forums. In Proceedings of the
19th Conference on Computational Natural Language
Learning, CoNLL 2015, Beijing, China, July 30-31,
2015, pages 123{131, 2015.
[7] A. Clauset, C. Moore, and M. E. Newman.
Hierarchical structure and the prediction of missing
links in networks. Nature, 453(7191):98{101, 2008.
[8] D. M. Dunlavy, T. G. Kolda, and E. Acar. Temporal
link prediction using matrix and tensor factorizations.
ACM Transactions on Knowledge Discovery from
Data (TKDD) , 5(2):10, 2011.
[9] E. Gilbert and K. Karahalios. Predicting tie strength
with social media. In Proceedings of the SIGCHI
conference on human factors in computing systems ,
pages 211{220. ACM, 2009.
[10] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse
rectier neural networks. In International Conference
on Articial Intelligence and Statistics , pages 315{323,
2011.
[11] Z. S. Harris. Distributional structure. Word ,
10(2-3):146{162, 1954.
[12] C.-W. Hsu, C.-C. Chang, C.-J. Lin, et al. A practical
guide to support vector classication. 2003.
[13] D. H. Hubel and T. N. Wiesel. Receptive elds of
single neurones in the cat's striate cortex. The Journal
of physiology , 148(3):574{591, 1959.
[14] A. Java, X. Song, T. Finin, and B. Tseng. Why we
twitter: understanding microblogging usage and
communities. In Proceedings of the 9th WebKDD and
1st SNA-KDD 2007 workshop on Web mining and
social network analysis , pages 56{65. ACM, 2007.
[15] T. Kenter and M. de Rijke. Short text similarity with
word embeddings. In Proceedings of the 24th ACM
International on Conference on Information and
Knowledge Management, pages 1411{1420. ACM,
2015.
[16] Y. Kim. Convolutional neural networks for sentence
classication. arXiv preprint arXiv:1408.5882 , 2014.
[17] D. Kingma and J. Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 , 2014.
[18] D. Liben-Nowell and J. Kleinberg. The link-prediction
problem for social networks. Journal of the American
society for information science and technology ,
58(7):1019{1031, 2007.
[19] L. L u and T. Zhou. Link prediction in complex
networks: A survey. Physica A: Statistical Mechanics
and its Applications , 390(6):1150{1170, 2011.[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean.
Ecient estimation of word representations in vector
space. arXiv preprint arXiv:1301.3781 , 2013.
[21] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and
phrases and their compositionality. In Advances in
neural information processing systems , pages
3111{3119, 2013.
[22] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin.
Convolutional neural networks over tree structures for
programming language processing. In Proceedings of
the Thirtieth AAAI Conference on Articial
Intelligence, 2016.
[23] A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, D. Lo,
and C. Sun. Duplicate bug report detection with a
combination of information retrieval and topic
modeling. In Automated Software Engineering (ASE),
2012 Proceedings of the 27th IEEE/ACM
International Conference on, pages 70{79. IEEE, 2012.
[24] F. Rahman, D. Posnett, and P. Devanbu. Recalling
the imprecision of cross-project defect prediction. In
Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software
Engineering, page 61. ACM, 2012.
[25] R. Reh u rek and P. Sojka. Software Framework for
Topic Modelling with Large Corpora. In Proceedings of
the LREC 2010 Workshop on New Challenges for NLP
Frameworks , pages 45{50, Valletta, Malta, May 2010.
ELRA. http://is.muni.cz/publication/884893/en.
[26] S. Robertson and H. Zaragoza. The probabilistic
relevance framework: BM25 and beyond . Now
Publishers Inc, 2009.
[27] H. Rocha, M. Tulio Valente, H. Marques-Neto, and
G. C. Murphy. An empirical study on
recommendations of similar bugs. In SANER, 2016.
[28] A. Sharma, Y. Tian, and D. Lo. Nirmal: Automatic
identication of software relevant tweets leveraging
language model. In 2015 IEEE 22nd International
Conference on Software Analysis, Evolution, and
Reengineering (SANER) , pages 449{458, 2015.
[29] C. Sun, D. Lo, S.-C. Khoo, and J. Jiang. Towards
more accurate retrieval of duplicate bug reports. In
Proceedings of the 2011 26th IEEE/ACM International
Conference on Automated Software Engineering, pages
253{262. IEEE Computer Society, 2011.
[30] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo. A
discriminative model approach for accurate duplicate
bug report retrieval. In Proceedings of the 32nd
ACM/IEEE International Conference on Software
Engineering-Volume 1 , pages 45{54. ACM, 2010.
[31] Y. Tian, C. Sun, and D. Lo. Improved duplicate bug
report identication. In Software Maintenance and
Reengineering (CSMR), 2012 16th European
Conference on, pages 385{390. IEEE, 2012.
[32] J. Turian, L. Ratinov, and Y. Bengio. Word
representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
annual meeting of the association for computational
linguistics , pages 384{394. Association for
Computational Linguistics, 2010.
61[33] L. Van der Maaten and G. Hinton. Visualizing data
using t-sne. Journal of Machine Learning Research ,
9(2579-2605):85, 2008.
[34] J.-P. Vert, K. Tsuda, and B. Sch olkopf. A primer on
kernel methods. Kernel Methods in Computational
Biology , pages 35{70, 2004.
[35] X.-Y. Wang, X. Xia, and D. Lo. Tagcombine:
Recommending tags to contents in software
information sites. Journal of Computer Science and
Technology , 30(5):1017{1035, 2015.
[36] X. Xia, D. Lo, D. Correa, A. Sureka, and E. Shihab. It
takes two to tango: Deleted stack overow question
prediction with text and meta features. In The 40th
Annual International Computers, Software &
Applications Conference (COMPSAC) , 2016.
[37] X. Xia, D. Lo, S. J. Pan, N. Nagappan, and X. Wang.
Hydra: Massively compositional model for
cross-project defect prediction. IEEE Transactions on
Software Engineering (TSE) , 2016.
[38] X. Xia, D. Lo, E. Shihab, and X. Wang. Automated
bug report eld reassignment and renement
prediction. In In IEEE Transactions on Reliability .
IEEE, 2015.
[39] X. Xia, D. Lo, E. Shihab, X. Wang, and X. Yang.
Elblocker: Predicting blocking bugs with ensemble
imbalance learning. Information and Software
Technology , 61:93{106, 2015.
[40] X. Xia, D. Lo, E. Shihab, X. Wang, and B. Zhou.
Automatic, high accuracy prediction of reopened bugs.
Automated Software Engineering, 22(1):75{109, 2015.
[41] X. Xia, D. Lo, X. Wang, and B. Zhou. Tag
recommendation in software information sites. In
Proceedings of the 10th Working Conference on
Mining Software Repositories , pages 287{296. IEEE
Press, 2013.
[42] X. Xia, E. Shihab, Y. Kamei, D. Lo, and X. Wang.
Predicting crashing releases of mobile applications. In
10th ACM/IEEE International Symposium on
Empirical Software Engineering and Measurement
(ESEM) , 2016.
[43] B. Xu, D. Lo, X. Xia, A. Sureka, and S. Li.
Efspredictor: Predicting conguration bugs withensemble feature selection. In 2015 Asia-Pacic
Software Engineering Conference (APSEC), pages
206{213. IEEE, 2015.
[44] B. Xu, Z. Xing, X. Xia, D. Lo, Q. Wang, and S. Li.
Domain-specic cross-language relevant question
retrieval. In Proceedings of the 13th International
Workshop on Mining Software Repositories , pages
413{424. ACM, 2016.
[45] D. Ye, Z. Xing, C. Y. Foo, Z. Q. Ang, J. Li, and
N. Kapre. Software-specic named entity recognition
in software engineering social content. In The 23rd
IEEE International Conference on Software Analysis,
Evolution and Reengineering (SANER 2016) .
[46] D. Ye, Z. Xing, and N. Kapre. The structure and
dynamics of knowledge network in domain-specic
q&a sites: a case study of stack overow. Empirical
Software Engineering, 2016.
[47] D. Ye, Z. Xing, J. Li, and N. Kapre. Software-specic
part-of-speech tagging: An experimental study on
stack overow. In Proceedings of the 31st Annual
ACM Symposium on Applied Computing, SAC '16,
pages 1378{1385, New York, NY, USA, 2016. ACM.
[48] H. Zhang, L. Gong, and S. Versteeg. Predicting
bug-xing time: an empirical study of commercial
software projects. In Proceedings of the 2013
International Conference on Software Engineering ,
pages 1042{1051. IEEE Press, 2013.
[49] J. Zhang, M. S. Ackerman, and L. Adamic. Expertise
networks in online communities: structure and
algorithms. In Proceedings of the 16th international
conference on World Wide Web, pages 221{230. ACM,
2007.
[50] Y. Zhang, D. Lo, X. Xia, and J.-L. Sun. Multi-factor
duplicate question detection in stack overow. Journal
of Computer Science and Technology, 30(5):981{997,
2015.
[51] B. Zhou, X. Xia, D. Lo, C. Tian, and X. Wang.
Towards more accurate content categorization of api
discussions. In Proceedings of the 22nd International
Conference on Program Comprehension, pages 95{105.
ACM, 2014.
62