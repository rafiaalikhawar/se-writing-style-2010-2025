Speedoo : Prioritizing Performance Optimization Opportunities
Zhifei Chen
State Key Laboratory for Novel
Software Technology
Nanjing University, ChinaBihuan Chen
School of Computer Science,
Shanghai Key Laboratory of Data
Science, and Shanghai Institute of
Intelligent Electronics & Systems
Fudan University, ChinaLu Xiao
Xiao Wang
School of Systems and Enterprises
Stevens Institute of Technology
United States
Lin Chen∗
State Key Laboratory for Novel
Software Technology
Nanjing University, ChinaYang Liu
School of Computer Science and
Engineering
Nanyang Technological University
SingaporeBaowen Xu∗
State Key Laboratory for Novel
Software Technology
Nanjing University, China
ABSTRACT
Performance problems widely exist in modern software systems.
Existing performance optimization techniques, including profiling-
based and pattern-based techniques, usually fail to consider the
architectural impacts among methods that easily slow down the
overallsystemperformance.Thispapercontributesanewapproach,
namedSpeedoo,toidentifygroupsofmethodsthatshouldbetreated
together and deserve high priorities for performance optimization.
The uniqueness of Speedoo is to measure and rank the perfor-
mance optimization opportunities of a method based on 1) the
architectural impact and 2) the optimization potential. For each
highlyrankedmethod,welocatearespective OptimizationSpace
based on 5 performance patterns generalized from empirical ob-servations. The top ranked optimization spaces are suggested to
developers as potential optimization opportunities. Our evaluation
onthreereal-lifeprojectshasdemonstratedthat18.52%to42.86%
ofmethodsinthetoprankedoptimizationspacesindeedundertook
performance optimization in the projects. This outperforms oneof the state-of-the-art profiling tools YourKit by 2 to 3 times. An
important implication of this study is that developers should treat
methodsinanoptimizationspacetogetherasagroupratherthan
asindividualsinperformanceoptimization.Theproposedapproach
can provide guidelines and reduce developers’ manual effort.
CCS CONCEPTS
•Software and its engineering →Software performance;
KEYWORDS
Performance, Metrics, Architecture
∗Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180229ACM Reference Format:
ZhifeiChen,BihuanChen,LuXiao,XiaoWang,LinChen,YangLiu,andBaowen
Xu.2018.Speed oo:PrioritizingPerformanceOptimizationOpportunities.
InICSE ’18: 40th International Conference on Software Engineering, May
27-June3,2018,Gothenburg,Sweden. ACM,NewYork,NY,USA,11pages.
https://doi.org/10.1145/3180155.3180229
1 INTRODUCTION
Performanceproblems1widelyexistinmodernsoftwaresystems
due to the high complexity of source code [ 7,23,29,33,39,43,59].
They can slow down production runs, hurt user experience, or
even cause system failures. Therefore, the first task for software
performance optimization is to find performance optimization op-
portunities and then conduct performance refactoring.
Theremainly existtwotypesof techniquesforfindingperfor-
manceoptimizationopportunities.Profiling-basedtechniqueslever-ageinstrumentationtocollectprofilesorexecutiontraces.Thepur-
poseistolocatethecoderegions(or hotspots)thatconsumemostre-
sources(e.g.,memoryandtime)[
1,3],toidentifythefrequentlyexe-
cutedprogrampaths(or hotpaths)[ 6,17,27],ortofitaperformance
function[ 11,14,30,60].Suchtechniquesuseconsumedresourcesor
executionfrequenciesastheonlyperformanceindicator,anddonot
consider the performance impact due to architectural connections
amongsoftwareelements, e.g.modules,sourcefiles,methods,etc.
Thus,theyprovideanarrowviewforidentifyingperformanceprob-
lems,anddevelopershavetospendalargeamountofmanualeffort
to locate the root causes that are not even in the ranked list of
profilers[ 43,44].Moreover,themanifestationofthecodewithper-
formance problems heavilyrelies on the qualityof the test inputs.
Despite advances on producing performance test inputs [ 21,41],
important performance problems still remain unrevealed.
Anothertypeofperformanceproblemdetectiontechniquesis
pattern-basedtechniques.Suchtechniquesleveragestaticand/or
dynamic program analysis to identify code regions that match
specificpatterns; e.g., inefficientloops [ 16,34,44], inefficientcon-
tainers [28,53], inefficient synchronizations [ 36,57], or redundant
collection traversals [ 35]. While they are effective at identifying
specifictypesofperformanceproblems,theyfailtobeapplicable
1Performanceproblemsaresometimesalsocalledperformancebugsorperformance
issues. For consistency, we use performance problems throughout the paper.
8112018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Z. Chen et al.
to a wider range of performance problems. Moreover, they are not
designedandthuswillnotprovideanysuggestionsonthebenefited
code regions after the performance optimization.
Inthispaper,weproposeanewapproach,named Speedoo2,topri-
oritize performance optimization opportunities based on their im-
pact on the overall system performance as well as their local po-
tential of being optimized. Thus, the identified optimization op-
portunitieshaveahighchanceofbeingoptimized;andonceopti-
mized, they can benefit to the overall system performance. A main
insightisthattheinvestigationofperformanceoptimizationshould
consider the architectural connections among methods, instead of
treating each method in isolation, because the performance of amethod can affect or be affected by the behaviors of other meth-ods (e.g., the methods that it calls). Hence, each opportunity is
a set of architecturally related methods with respect to both ar-
chitecture and performance (e.g., the performance of a methodis improved by optimizing the methods it calls). Different from
profiling-based techniques, S peedoo furtherr elies on the architec-
tural knowledge to prioritize potential optimization opportunities
more accurately. Different from pattern-based techniques, Speedoo
is more generic, and considers the overall architecture of methods.
Our approachworks inthree stepsto identifythe optimization
opportunitiesthatshouldbegivenhighprioritiestoimprovethe
overallperformance.First,wecomputeasetofmetricsthatmeasurethearchitecturalimpactandoptimizationpotentialofeachmethod.
The metrics include architectural metrics (e.g. the number of meth-
ods called by a method; the larger the number, the more impact
theoptimizationhas),dynamicexecutionmetrics(e.g.,thetimea
method consumes; the longer the time, the more potentially it can
beoptimized),andstaticcomplexitymetrics(e.g.,thecomplexityof
the method; the more complex the code structure, the more poten-
tiallyitcanbeoptimized).Then,wecomputean optimizationprior-
ityscoreforeachmethodtoindicatethepriorityforoptimization
basedonthemetrics,andrankthemethods.Finally,foreachhighlyrankedmethod(i.e., candidate),welocatearespective Optimization
Spacecomposedofthemethodsthatcontributetothedynamicper-
formance of the candidate, which is based on five performance pat-
terns summarized from real performance optimization cases. Each
methodinoptimizationspacecouldbeoptimizedtoimprovethe
overall system performance. Such optimization space is suggested
todevelopersaspotentialoptimizationopportunities,whichcan
provide guidelines and reduce developers’ manual effort.
Wehaveimplementedtheproposedapproach,andconductedan
experimental study on three real-life Java projects to demonstrate
the effectiveness and performance of our approach. The resultsshow that 18.52% to 34.62% of methods in the top ranked opti-
mization spaces calculated by Speedoo indeed undertook perfor-
mance optimization during software evolution. We also compared
Speedoowith thestate-of-the-artprofilerYourKit[ 3]anddiscov-
ered Speedoo covers 3%-16% more refactored methods. The results
have demonstrated that Speedoo effectiv ely prioritizes potential
performance optimization opportunities, significantly outperforms
YourKit, and scales well to large-scale projects.
In summary, our work makes the following contributions.
2Thespeed optimization opportunities that we are pursuing with our approach.1 2 3 4 5 6 7 8 91 01 11 21 31 41 51 61 71 81 92 0
1 bnf.Node (1)
2i o . P i p e ( 2 )
3a s t . N o d e ( 3 )
4 Filter (4)
5 io.InputPipe Ext (5)
6i o . O u t p u t P i p e Ext (6)
7 io.WriterOutputPipe Impl (7)
8 io.MemoryOutputPipe Impl (8)
9 io.ReaderInputPipe Impl (9)
10 io.MemoryInputPipe Impl (10)
11 ast.TreeVisitor (11)
12 Interpreter Impl Impl (12)
13 parse.Parser Impl (13)
14 lex.Lexer Impl (14)
15 parse.Convert Impl (15)
16 ast.Variable Ext (16)
17 ast.Number Ext (17)
18 ast.OperExpr Ext (18)
19 ast.FuncExpr Ext (19)
20 ast.UnaryOperExpr Ext (20)
Figure 1: An Example of DRH
•Weproposedanewapproachtoprioritizingperformanceopti-
mization opportunities based on their architectural impact on
the overall performance and their potential of being optimized.
•We implementedtheproposedapproachandconductedexperi-
mentsonthreereal-lifeprojectstodemonstratetheeffectiveness
and performance of our approach.
2 PRELIMINARIES ON ARCHITECTURE
To understand the architectural roles of different elements in a sys-
tem,Design Rule Hierarchy (DRH) algorithm [ 10,47] was proposed
toclustersoftwareelementsintohierarchicallayers.Thisalgorithm
isbasedonthe designruletheory [5],whichindicatesthatamodular
structureiscomposedof designrules (i.e.,architecturallyimportant
elements) and modules(i.e., elements depending on and decoupled
by thedesign rule elements). The DRHcaptures the roles of soft-
ware elements as design rules andmodulesin hierarchical layers
based on the directed dependency graph of the software elements.
Eachlayercontainsasubsetofelements.Thelayersmanifesttwo
key features: (1) the elements in the lower layers depend on the
elementsintheupperlayers,but notviceversa;and(2)elements
inthesamelayerareclusteredintomutuallyindependentmodules.
Ingeneral,elementsintheupperlayersarearchitecturallymore
important than elements in the lower layers, because the latter de-
pendontheformer.Forexample,Fig.1illustratesa DRHcalculated
fromthe“extend"and“implement"dependenciesinamathcalcula-
torprogram.The DRHisdepictedasa DesignStructureMatrix,a
n×nmatrix. The rowsand columns represent software elements,
i.e. source files in this example. Each cell indicates the dependency
from the file on the row to the file on the column. The internalrectangles in the DSM represent layers and modules. This DRH
capturesthekeybaseclassesorinterfacesasthehigherleveldesign
rules, and captures the concrete classes as two lower layers. Files 1
to4formlayer1onthetop,whicharethekeyinterfacesandparent
classes. Files 5 to 6 form layer 2 in the middle, whose elements
“Extend” the elements in the top layer. Files 7 to 20 form layer 3 in
the bottom, which contains 5 mutually independent modules. Each
module “extend" or “implement" a design rule element in the upper
layers.Forexample,files16to20formamoduleastheyall“Extend”
the design rule: mij.ast.Node, which is a parent class.
To achieve different analysis goals, the elements could be at dif-
ferentgranularities,e.g.methodlevel.Inthispaper,weapplythe
DRHalgorithm at method level to capture the architectural impor-
tanceofeachmethodandtounderstandthepotentialbenefitsof
812
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Speedoo: Prioritizing Performance Optimization Opportunities ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Titan
Understand
YourKit Dynamic Execution MetricsStatic Complexity MetricsArchitectural Metrics
Normalizing 
MetricsComputing 
Priority 
ScoreRanking 
Methods
Detecting 
Performance 
PatternsComputing 
Optimization 
SpaceRanking 
Optimization 
Spaces
Performance Optimization OpportunitiesSource Code of Target System
Step 1: Metric 
Computation
Step 2: Method 
Prioritization
Step 3: Optimization 
Space Localization
Figure 2: Approach Ov erviewofSpeedoo
improvingitfortheoverallsystemperformance.Wewillintroduce
this in details in Section 3.1.
3 METHODOLOGY
Fig.2showstheoverviewofSpeedoo. Basically,ourapproachworks
inthreesteps:computingmetricsforeachmethodinthetargetsys-
tem(Section3.1), prioritizingmethodsbasedonmetric valuesto
indicate their optimization priorities (Section 3.2), and locating the
optimizationspaceforeachhighly-rankedmethod(Section3.3).The
prioritizedoptimizationspacesarereportedtodevelopersaspoten-
tial optimization opportunities; and every methodin an optimiza-
tionspacehasthepotentialofbeingoptimizedtoimprovetheover-
all system performance. We elaborate each step as follows.
3.1 Metric Computation
Inthefirststep,wecomputethevaluesofmetricsthatmeasureeach
method’sarchitecturalimpactandoptimizationpotential.Before
divingintothedetails,wefirstexplaintherationalityofmeasuring
architectural impact and optimization potential.Rationality.
Givenalistofhotspotmethods,developersusually
treat them equally by considering whether any optimization could
beappliedtoimprovethesystemperformance.However,theimpactofaperformanceoptimizationvariesdependingonthearchitecturerole of the optimized method in a system. For example, if a method
is invoked by many methods, the performance optimization of this
method will greatly improve the system performance. It is desired
thatsuchmethodsshouldbegivenhigherprioritiesforperformance
optimization. Thus we analyze the architectural impact of each
method to estimate the impact scope of performance optimization.
In addition, considering the functionality of each method, not
everymethodhasthesamepotentialtobeimprovedwithrespect
toitsperformance.Intuitively,ifamethodhashighercomplexityor
consumes more runtime, it has more potential to be optimized. For
example,ifamethodfrequentlyinvokesatime-consumingmethodthroughaloopstructure,wecaneitherreplacethetime-consuming
method or avoid unnecessary invocations. It is desired that such
methodsshouldobtainahigherpriorityforoptimization.Hence,
weanalyzetheoptimizationpotentialofeachmethodtoestimate
the potential space of performance optimization.Based on the previous understanding, we derive a set of metrics,
as shown in Table 1, to represent and estimate the architectural im-
pact and optimization potential of a method.
Architectural Impact. We propose an architectural-role-based
metricandthreecaller-callee-scope-basedmetrics,whosevalues
are computed from the output of Titan [48].
1. Role-based Metric: Layer. As briefly discussed in Section 2, we
apply the DRHalgorithm to the method level call graph of a sys-
temto capturethearchitectural importanceofeach method.Each
methodresidesinauniquelayeroftheDRH,andthelayernumber
reflects its architectural role. The top layers usually contain infras-
tructurallevelmethods,whicharecalledbymanyothermethodsin
the system; while the bottom layers usually contain main methods
that call many other methods. The methods in the middle layers
gradually transitfrom the infrastructuralroles to controlroles. In
addition, the naming conventions of the methods are generally
consistentwiththearchitecturalrolessuggestedbytheDRH Layer.
For example, methods in the top layers are likely to be named with
“util”and“commons”etc..Incomparison,methodsinthebottom
layers are likely to be named with “execute” and “main” etc..
Hence,weuseDRH Layerasametrictodescribethearchitectural
importance of a method. A method in an upper layer is architec-
turallymoreimportantthanamethodinalowerlayer,astheperfor-
mance problems with methods in top layers tend to produce larger
impactsonthewholesystem.ThehighertheDRH Layer,thesmaller
the metric value but the higher the architectural importance.
2. Scope-based Metrics: Size, Depth, and Width. We calculate two
scopes related to each method f: 1) the caller-scope containing
the methods directly or transitively calling f; and 2) the callee-
scopecontainingthemethodsdirectlyortransitivelycalledby f.
Iffhas a performance problem, the methods in the caller-scope
also perform poorly as their execution time contains f’s execution
time.Iffhasaperformanceproblem,e.g.,aninefficientloop,the
methods called by it may also consume more time due to frequent
invocations by f. The actual impact of fon other methods may
dependontherun-timeenvironment,butthetwoscopescontainall
the potentially impacted methods if fhas a performance problem.
Thecaller-scopeandthecallee-scopecanbecalculatedastwo
sub-treesbytransversingfrom fthroughstatic“call”and“called
by”relationsrespectively.Weextractthreemetricsfromthetwo
sub-trees of fto measure its importance:
(1)Size:thetotalnumberofmethodsinthecaller-scopeandthe
callee-scope.Thelargerthevalue,thelargertheimpactsof f,
and thus the more important is f.
(2)Depth:thesumofthedepthofthetwosub-trees.Thelargerthe
value, the deeper the impact of f, and thus the more important
isf.
(3)Width:thesumofthewidthofthetwosub-trees.Thelargerthe
value,thebroadertheimpactof f,andthusthemoreimportant
isf.
Optimization Potential. We measure both the static complex-
ityanddynamicexecutionofamethodtoestimatetheoptimization
potential. For the complexity of a method, we choose five met-
rics based on the internal structure of the method and the external
relationshipwithothermethods,asshowninthesixthtotenthrows
of Table 1. CCcaptures the Cyclomatic Complexity of a method.
813
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Z. Chen et al.
Table 1: Metrics Used for Measuring Architectural Impact and Optimization Potential of a Method
Category Metric Description
Architectural Metrics
(Architectural Impact)Layer The residing DRH layer number of a method
Size The total number of methods in the caller-callee-scope of a method
Depth The sum of the depth of the caller- and callee-scope of a method
Width The sum of the width of the caller- and callee-scope of a method
Static Complexity Metrics(Optimization Potential)CC Cyclomatic complexity, i.e., the number of linearly independent paths in a method
SLOC The source lines of code in a method
FanIn The number of methods directly calling a methodFanOut The number of methods directly called by a method.
Loop The number of loops in a method
Dynamic Execution Metrics(Optimization Potential)Time The CPU time consumed by a method
OwnTime The CPU time consumed by a method itself without subcalls
Counts The invocation counts of a method
SLOC,FanInandFanOutcharacterizethesizeandcomplexityof
thefunctionalresponsibilityofamethod.Thehigherthesemetrics,
the more opportunities to optimize the structure or refactor the
coupling relationships to improve performance. Loopis selected
becauseinefficientorredundantloopsarecommonperformance
problems [ 16,32,34,44]. If a method contains many loops, there
are potential optimization opportunities in it. Note that these met-
rics are all widely used in the literature as useful indicators for
quality problems [ 56]. Here we leverage them to reflect the poten-
tial of performance optimization. We use Understand [ 2] in our
implementation to compute these metrics.
Moreover,consideringthedynamicexecutionofamethod,we
useTime,OwnTime andCounts, as listed in the last three rows of
Table 1, to measure the performance level of a method during con-
creteexecutions.Ahighervalueofthesemetricsindicatesaseverer
performance problem of a method and thus a higher possibility
to improve its performance. These metrics are also widely used in
the existingprofilingtools [ 1,3] toidentify hot spot methods. In
ourcurrentimplementation,weobtainthesemetricsbyrunning
profiling tools, i.e., YourKit [3], against the test cases.
As will be discussed in Section 5, our approach can be extended
with additional metrics to prioritize optimization opportunities.
3.2 Method Prioritization
In the second step, based on the metric values, we compute an
optimizationpriorityscore foreachmethod f∈F(whereFisthe
set of all methods in a system) to indicate its optimization priority,
and rankall methods in Faccording totheir optimization priority
scores. This step is achieved in the following three sub-steps.Normalizing Metrics.
To allow a unified measurement of these
metricsindependentoftheirunitsandranges,wenormalizeeach
metricintoavaluebetween0and1bycomparingitwiththemaxi-mumandminimumvalueofthemetric,asformulatedinEq.1and2.
m/prime
f=(mf−min i∈Fmi)/(max i∈Fmi−min i∈Fmi)(1)
m/prime
f=(max i∈Fmi−mf)/(max i∈Fmi−min i∈Fmi)(2)
mfrepresentsthevalueofametric mofamethod f;andm/prime
fisthe
normalized value of mf. Except for Layer, all themetrics are posi-
tive metrics, i.e., the higher the metric value, the higher the opti-
mizationpriority.Thus, LayerisnormalizedbyEq.2,andtheother
metrics are normalized by Eq. 1.Computing PriorityScore. Basedonthenormalizedmetricval-
ues,wecomputeanoptimizationpriorityscore OPSforeachmethod
fto estimate the optimization priority, as formulated in Eq. 3.
OPS f=AIf×OPf (3)
The first term AIf(see Eq. 4) represents the architectural impact of
amethod,andthesecondterm OPf(seeEq.5)indicatestheopti-
mizationpotentialofamethod.Themultiplicationrepresentsthe
globalpotentialperformanceimprovementontheoverallsystem
by optimizing the method f.
AIf=(Layer/prime
f+Size/prime
f+Depth/prime
f+Width/prime
f)/4 (4)
OPf=(SCf+DEf)/2 (5)
Noticethat OPfconsidersboththestaticcomplexity(seeEq.6)and
the dynamic execution (see Eq. 7) of a method.
SCf=(CC/prime
f+SLOC/prime
f+FanIn/prime
f+FanOut/prime
f+Loop/prime
f)/5 (6)
DEf=(Time/prime
f+OwnTime/prime
f+Counts/prime
f)/3 (7)
Here we assign equal weights to the architectural metrics in
Eq4,tothestaticcomplexityanddynamicexecutioninEq.5,tothe
static complexity metrics in Eq. 6, and to the dynamic execution
metricsinEq.7,becauseweconsiderdifferentfactorstobeequally
important.Ranking Methods.
Once the priority scores of all the methods
are computed, we rank these methods in decreasing order of their
scores.Thus,thelargertheoptimizationpriorityscoreofamethod,thehigherpriorityitshouldbegivenforperformanceoptimization.
3.3 Optimization Space Localization
If a highly ranked method manifests performance problems, the
optimization opportunities reside not only in the method itself, butalsointhemethodsthatcontributetoitsdynamicperformance(e.g.,itscallees).However,suchcontributingmethodsmaynotrankhigh,
even if they are potentially important for solving the performance
problem.Hence,welocatearespective OptimizationSpace foreach
highlyrankedmethod(i.e., candidate )byfindingitscontributing
methods. Each method in the space could be an optimization op-
portunity to improve the performance of the candidate.
Based on our empirical analysis of performance problems fixed
bydevelopers,wefindthattheoptimizationspaceofacandidate
isoftendeterminedbytheperformancepatternsofthecandidate.
814
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Speedoo: Prioritizing Performance Optimization Opportunities ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 2: Summarized Performance Patterns from Four Apache Systems
Performance Pattern Symptom Description Optimization Strategies # Issues
Cyclic Invocationa slow or frequently executed method
is in an invocation cycleimprove any method in the cycle 1
change dependency relations to break the circle 2
Expensive Recursion a slow method calls itself repeatedlyimprove the method 2
avoid or reduce the calls in its callers 3
Frequent Invocation a method is frequently executedimprove the method 3
cache the returned values to reduce calls 10
add or update the call conditions in its callers 10
avoid unnecessary or duplicated calls in the callers 4
Inefficient Method a method is slow in executing its own codeimprove the method 15
reduce its calls in its callers 5
replace its calls with cheaper callees in its callers 6
Expensive Callee a method is slow in executing its callees’ codeimprove the expensive callees 15
reduce the calls to expensive callees 5
replace the calls to expensive callees with cheaper callees 6
Others all the issues not belonging to the above – 14
Commit: Ivy 6f8302fe
Candidate: XmlSettingsParser.startElement(String,String,String,Attributes)
private void includeStarted(Map attributes) throws IOException,  
ParseException {
    ...  new XmlSettingsParser(ivy).parse(configurator, setting sURL);  ...
}
private void parse(Configurator configurator, URL configuration ) throws 
IOException, ParseException {
    ...  doParse(configuration);  Ă
}
...
public void startElement(String uri, String localName, String q Name, 
Attributes att) throws SAXException {
    ...  includeStarted(attributes);  Ă
}
Optimization: improve includeStarted(Map) by removing unnecessary cast
Figure 3: A Case for Cyclic Invocation
Thus, we first present the performance patterns we summarized,
and then introduce the steps to locate optimization spaces.
Performance Patterns. We manually analyzed real performance
problemsfromfourApachesystems(i.e., CXF,Avro,IvyandPDFBox),
which are all mature and industrial scale software projects. We
searched the issue databases using a set of performance-related
keywords,aswassimilarlydoneintheliterature[ 23,39,59].Finally,
werandomlysampled75fixedperformanceissues,andanalyzed
how the problems were fixed (i.e., optimization strategies). Table 2
describesthesummarizedperformancepatternswiththeirsymp-
tomdescription,optimizationstrategiesandthenumberofcorre-spondingissues.Theseperformancepatternsappearnearlyinall
the systems. Notice that although performance patterns were also
summarized in the literature (e.g., [ 23,34]), their patterns focus on
root causes of performance problems and are local to a method,
andthusareoftenspecific.Ourpatternsemphasizetheoptimiza-
tion locations of performance problems from the perspective of
symptomsandinfluence,andarehenceglobalandgeneric.Inthe
following, we illustrate each performance pattern.
1. Cyclic Invocation. The invocation of a set of methods forms a
cycle.Anyinefficientorfrequentlyexecutedmethodinthecircle
would slow down the execution of all involved methods. For ex-
ample, inFig. 3, themethod startElement was reportedto be slow,
becauseit’sinaninvocationcirclecontainingaslowmethod, in-
cludeStarted. This issue was solved by improving includeStarted.
Note that the performance problem would worsen if the callingCommit: PDFBox 7929477d
Candidate: PDFStreamParser.parseNextToken()
private Object parseNextToken() throws IOException {
    ...
    while((nextToken = parseNextToken()) instanceof COSName ) {
        Object value = parseNextToken();  ...
    }
}
Optimization: improve this method by using per-image color conversion
Figure 4: A Case for Expensive Recursion
Commit: PDFBox 54037862
Candidate: COSDictionary.getDictionaryObject(COSName)
public Map<String, PDFont> getFonts() throws IOException {    …  COSBase font = fontsDictionary.getDictionaryObject(fontName);  ...
}
public Map<String, PDColorSpace> getColorSpaces() {    ...  COSDictionary csDictionary = (COSDictionary) 
resources.getDictionaryObject(COSName.COLORSPACE);  ...
}
Optimization: reduce the calls to getDictionaryObject(COSName) in getFonts() and 
getCo lorSp aces() thro ugh caching
Figure 5: A Case for Frequent Invocation
circleisrepeatedlyexecuted.Analternativesolutionistobreakthe
cycle evidenced by 2 real performance issues.
2. Expensive Recursion. Recursion allows a method to repeatedly
callitself.Ifthemethodisexpensive,theperformanceproblemwill
bedramaticallyamplified.Forexample,themethod parseNextToken
inFig.4repeatedlycallsitselfbyrecursion.Thereasonwasthatthe
PDF parser converted the color spaces of images for every pixel.
Finally, developers improved this method by performing color con-
versioninoneoperationforeachimage.Performanceproblemsdue
toexpensiverecursioncanbeoptimizedbyimprovingtherecursion
method itself or by reducing calling such methods.
3. Frequent Invocation. Frequently executed methods, especially
the most influential methods in low DRHlayers, can be optimized
to produce a significant improvement of system performance, even
with minor optimization. For example, in Fig. 5, the method getDic-
tionaryObject iscalledfrequentlyby PDResource forobtainingre-
sources.Incommit54037862, PDResources wasrefactoredtoreduce
theinvocationsof getDictionaryObject.Thisisthemostcommon
patterninthestudiedissues.Itcanalsobereducedbyupdatingthe
call conditions, or avoiding unnecessary/duplicated invocations.
4. Inefficient Method. A method manifests poor performance due
totheinefficiencyinitself.Fig.6showsanexamplein Avro.The
815
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Z. Chen et al.
Table 3: Detection Conditions and Optimization Spaces of Performance Patterns
Performance Pattern Detection Condition Optimization Space
Cyclic Invocation isCallCycle and isHigh(Time) and isHigh(Counts) OS(f) = all the methods in the invocation cycle
Expensive Recursion isRecursion and isHigh(Time) and isHigh(Counts) OS(f )=f+O S ( each method in getCallers(f) that satisfies isHigh(Time))
Frequent Invocation isHigh(Counts) OS(f)=f+O S ( each method in getCallers(f) that satisfies isHigh(Invocations))
Inefficient Method isHigh(Time) and isHigh(OwnTime) OS(f)=f+O S ( each method in getCallers(f) that satisfies isHigh(Time))
Expensive Callee isHigh(Time) and Not isHigh(OwnTime) OS(f )=f+O S ( each method in getCallees(f) that satisfies isHigh(Time))
Others all the conditions not belonging to the above OS(f) = f
Commit: Avro e0966a1e
Candidate: GenericData.deepCopy(Schema, T)
public <T> T deepCopy(Schema schema, T value) {
    ...
    case FLOAT:
        return (T)new Float((Float)value);
    case INT:
        return (T)new Integer((Integer)value);
    case LONG:
        return (T)new Long((Long)value);    ...
}
Optimization: improve this method by avoiding creating new instances
Figure 6: A Case for Inefficient Method
Commit: PDFBox 4746da78Candidate: PreflightParser.parseObjectDynamically(long, int, boolean)
protected COSBase parseObjectDynamically(long objNr, int objGen Nr, 
boolean requireExistingNotCompressedObj) throws IOException {
    ...  final Set<Long> refObjNrs = 
xrefTrailerResolver.getContainedObjectNumbers(objstmObjNr); ...
}
Optimization: replace xrefTrailerResolver.getContainedObjectNumbers(int) with 
xrefTrailerResolver.getXrefTable()
Figure 7: A Case for Expensive Callee
methoddeepCopy createdmanynewinstancesforprimitives,which
is unnecessary. This method was optimized in commit e0966a1e by
removingunnecessary instancecreations.Ifan inefficientmethod
canhardlybeoptimized,theoverallperformanceofasystemcan
still be improved by reducing the invocations of this method.
5.ExpensiveCallee. Theperformanceofamethodisalsodeter-
minedbytheperformanceofitscallees.Expensivecalleeisaperfor-
mancepatternwhereamethodisslowinexecutingitscallees’code.
Theoptimizationstrategiesincludeimprovingtheexpensivecallees,
and reducing or avoiding the invocations of expensive callees. The
example in Fig. 7 illustrates the performance optimization of the
methodparseObjectDynamically.Asreported,themethodtookquite
some time which was mostly spent in getContainedObjectNumbers.
Thereasonwasthatmanyfullscanswereperformedin getContaine-
dObjectNumbers.Developers fixedthis issueby replacingthe calls
togetContainedObjectNumbers with the calls to getXrefTable.
Notethattheoptimizationofaninefficientmethodcanbetreated
eitherasthe InefficientMethod pattern(i.e.,toimprovetheineffi-
cientmethod)orasthe ExpensiveCallee pattern(i.e.,toimprovethe
caller of the inefficient method). Thus, we counted the numberof corresponding issues equally for these two patterns. We sepa-
ratelydefinethesetwopatternsforlocatingindividualoptimization
opportunities for each target method. There were 14 issues thatexhibit no specific patterns, e.g., moving a generic method from
other classes to a target class, avoiding using type parameters, andpackage reorganization. Current patterns are defined among archi-
tecturallyconnectedmethods,whichcanbeextendedtosupport
high-level structures like packages and modules.DetectingPerformancePatterns. Asshownabove,theoptimiza-
tionopportunitiesofamethodresideinallthemethodsthatcon-
tribute to its performance, and the scope is determined by the per-
formancepatternsitbelongsto.Therefore,beforecomputingthe
optimizationspaceofeachcandidate,wefirstdetectitsperformance
patterns, using the dynamic execution metrics ( Time,OwnTime
andCounts) as well as its call relations with other methods.
Generally,themethodsindifferentDRHlayersareexpectedto
exhibit different level of performance. The dynamic execution met-
rics of methods are influenced by the architectural roles suggested
by the DRH layer. Intuitively, since the infrastructural methods
provide basic functions to support other methods, they usually
havelowermethodexecutiontimebuthigher invocationcounts.
Thecontrolmethodsoftenprovidehigherlevelfunctionsbycall-
ing manyother methods,thus theyusually havelower invocation
countsbuthigherexecutiontime.Inthatsense,theperformanceof
a method is only comparable to the methods within the same DRH
layer. Hence, our general idea of detecting performance patterns is
tocheckwhetherthedynamicperformanceofamethodactsasa
outlier among all the methods in the same DRH layer.
Specifically,theconditionsfordetectingperformancepatternsof
acandidate faredefinedinthesecondcolumnofTable3basedon
their symptoms in Table 2. Here isHighreturns whether the metric
valueoffisastatisticaloutlieramongthemethodsinthesameDRH
layer.Forcallrelations, isCallCycle returnswhether fisinvolvedin
acyclicinvocationand isRecursion returnswhether finvokesitself.
Following the detection conditions, a method can be matched in
morethanoneperformancepatterns.Forexample,amethodmatchesbothFrequentInvocation andExpensiveCallee,whichmeansthatthe
optimization opportunities reside in the two optimization spaces.ComputingOptimizationSpace.
Basedontheperformancepat-
ternsofacandidate f,wecomputetheoptimizationspace OS(f)
tolocatethepotentialoptimizationopportunities.Guidedbythe
optimizationstrategiesinTable2,welocatetheoptimizationspace
foreachpatterninthethirdcolumnofTable3. getCallers returns
the list of callers of f, andgetCallees returns the list of callees.
Ranking Optimization Spaces. The optimization space consists
ofallthecontributingmethodstoacertainperformanceproblem,
and should be investigated by the developers as a whole. Thus we
computetheoptimizationpriorityscoreofanoptimizationspace
as the average priority score of all the methods in the space to
represent its optimization priority. Finally, the optimization spaces
arerankedbasedontheirpriorityscores.Onlythosehighly-ranked
optimizationspacesarerecommendedtothedevelopersaspotential
optimization opportunities.
816
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Speedoo: Prioritizing Performance Optimization Opportunities ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 4: Subject Projects
ProjectAnalyzed Release Refactored Release
Ver. Date LOC (#) Meth. (#) Ver. Date
Avro1.3.0 2010-05-10 10,637 1,238 1.8.1 2016-05-14
Ivy2.0.0 2009-01-18 41,939 4,673 2.4.0 2014-12-13
PDFBox 1.8.4 2014-01-27 88,435 8,352 2.0.4 2016-12-12
4 EVALUATION
To evaluate the effectiveness and performance of our approach, we
conducted an experimental study on three real-life Java projects.
4.1 Evaluation Setup
SubjectProjects. Table4describesthesubjectprojectsusedinour
evaluation.Threewell-knownApacheprojectsareselected: Avrois
a data serialization system; Ivyis a popular dependency manager;
andPDFBoxis a tool for working with PDF documents. They are
selectedduetothefollowingreasons.First,wecancollectsufficient
performance problems from the formal issue reports documented
in JIRA and the entire commits history traced through GitHub (see
below).Second,theyarenon-trivialprojectsthatcontainthousands
of methods. Third, theseprojects belong to different domains, and
have high performance requirements. Notice that CXF,u s edi nS ec -
tion3.3,isnotusedintheevaluationaswefailedtorunitsoriginal
tests.Tworeleasesareselectedfromeachproject,namely analyzed
release(column2-5)and refactoredrelease (column6-7).Weapply
Speedooonthe analyzedrelease,andevaluatewhetherthemethods
in highly-ranked optimization spaces are actually optimized in the
refactored release. Generally, the studied time frame should be long
enoughtoallowplentyofperformanceproblemsreportedandfixed.
Consideringtheprojects’updatefrequencies,thestudiedinterval
is approximately five years in AvroandIvy, and three years in
PDFBox.ThentworeleasesinTable4wereselectedtocovermost
performance problems within this time frame.Ranked Methods to Optimize.
We apply our approach on the
analyzedreleaseofeachprojecttoidentifyhighly-rankedoptimiza-tionspaces.Themethodsintheseoptimizationspacesaresuggested
for optimization to developers. The suggested methods all suffer
from performance problems (running slowly or frequently called).
Table5reportsthedistributionofperformancepatternsaggre-
gating the identified methods. For each pattern, we list the num-
berofpatterninstancesdetectedintheprojectundercolumn Count
aswellastheaveragesize(i.e.numberofmethods)intheoptimiza-
tion spaces under column Avg. Size. We can observe that:
•FrequentInvocationandExpensiveCalleearethemostprevalent
performancepatterns,implyingdevelopersshouldpayspecific
attention to frequently executed and slow methods.
•TheimpactscopeofCyclicInvocationandFrequentInvocationis
larger than other performance patterns, with the average size of
optimization spaces ranging from 4 to 19. Therefore, it may take
more time for developers to solve these performance problems.
Actually OptimizedMethods. Wecollecttheperformanceprob-
lems fixed during the two selected releases as the ground truth.
Foreachproject,wefirstidentifiedalltheperformanceissuesdoc-
umented in JIRA by matching performance-related keywords (e.g.,
“performance” and “optimization”) in the issue descriptions, as was
similarlydoneintheliterature[ 23,39,59].Second,wescannedthecommitsbetweentheanalyzedandrefactoredreleasestoextractthe
matched commits that mention performance issue IDs in the com-
mitmessages.Further,toavoidmissingundocumentedperformance
issues, we also scanned those unmatched commits to extract the
commits thatmention performance-related keywordsin the com-
mit messages. Finally, the resulting performance issues and the
correspondinglinkedcommitsweremanuallyanalyzedtofilterout
non-performanceoptimization,andthemethodsparticipatingin
performance optimization were identified as the ground truth.ResearchQuestions.
Wedesignedtheexperimentstoanswerthe
following three research questions.
•RQ1:Howistheeffectivenessofourapproachinprioritizingper-
formance optimization opportunities?
•RQ2:Howisthesensitivityofthemetricsontheeffectivenessof
our approach?
•RQ3:How is the performance overhead of our approach?
4.2 Effectiveness Evaluation (RQ1)
To evaluate the effectiveness of the proposed approach, we further
breakdown the evaluation into 2 sub-questions below:•
RQ1-1:Howmanyoftherankedmethodstooptimizefromthe
analyzedreleaseareactuallyoptimizedintherefactoredrelease?
How does our approach compare to random search (assuming
developers have no knowledge what to do) and YourKit? This
question evaluates whether Speedoo can truly identify worth-
while optimization opportunities.
Toanswerthisquestion,weusethedensity,i.e.,thepercentage
of actually optimized methods in highly-ranked optimization
spacestoevaluatetheeffectiveness ofSpeedoo .Ascomparison,
wealsocalculatethedensityofactuallyoptimizedmethodsinthewholesystem,whichapproximatestheeffectivenessofarandom
search; and the density of actually optimized methods in the hot
spots reported by YourKit. We do not compar e Speedoo with
pattern-based techniques because each pattern-based technique
oftendetectsonlyonekindofperformanceproblemsandmost
of them are not publicly available.
•RQ1-2:Howthebefore/afteroptimizationpriorityoftheactually
optimized method and of the actually optimized optimization
space change? Presumably, after performance optimization, the
performanceofthemethodand/ormethodsinitsoptimization
spaceshouldimprove.Consequently,theoptimizationpriority
score offered by our approach should captur e such improvement
to show its effectiveness. Otherwise, it implies that the proposed
optimization priority score may not be effective (the another
possibility is that the optimization is not successful).
Toanswerthisquestion,weuseWilcoxontest[ 46]tocompare
theoptimizationpriorityscoresoftheactuallyoptimizedmeth-
ods and of the actually optimized optimization spaces in two
releases. Wilcoxon test is a non-parametric statistical hypothesistest,whichisusedtocomparetwogroupsofindependentsamples
to assess whether their population mean ranks differ. Without
assumingthatthedatahasanormaldistribution,wetestatthe
significancelevelof0.001toinvestigatewhethertheoptimization
817
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Z. Chen et al.
Table 5: Distribution of Detected Performance Patterns
ProjectCyclic Invocation Expensive Recursion Frequent Invocation Inefficient Method Expensive Callee Others
Count Avg. Size Count Avg. Size Count Avg. Size Count Avg. Size Count Avg. Size Count
Avro 5 4 2 2 76 4 3 2 22 2 555
Ivy 32 19 2 3 270 5 12 2 118 2 1759
PDFBox 7 7 2 3 460 16 68 1 40 2 3584
Table 6: Effectiveness of Optimization Space Ranks
ProjectWhole System Top 25 Spaces Top 50 Spaces Top 100 Spaces YourKit Hot Spots
Opt. Total Den. Opt. Total Den. Opt. Total Den. Opt. Total Den. Opt. Total Den.
Avro 43 1,238 3.47% 5 26 19.23% 7 60 11.67% 12 110 10.91% 4 110 3.63%
Ivy 61 4,673 1.31% 5 27 18.52% 8 72 11.11% 14 166 8.43% 9 166 5.42%
PDFBox 621 8,352 7.44% 12 28 42.86% 24 61 39.34% 48 126 38.10% 16 96 16.67%
Table 7: Change of Optimization Priority Scores after Performance Optimization
ProjectOptimized Methods Methods in Optimized Optimization Spaces
Total Higher Rank Lower Rank p-value dTotal Higher Rank Lower Rank p-value d
Avro 36 18 18 0.7462 - 48 14 34 0.0878 -
Ivy 40 8 32 0.0035 - 176 17 159 3.431e-21 0.1288
PDFBox 428 250 178 0.9999 - 539 169 370 1.940e-05 0.1031
05101520
0 50 100 150 200
# of Methods in Top Optimization Spaces# of Actually Optimized MethodsOur Approach
Our Approach w/o SC
Our Approach w/o DEOur Approach w/o AI
Our Approach w/o Gen. Tests
(a) Avro05101520
0 50 100 150 200
# of Methods in Top Optimization Spaces# of Actually Optimized MethodsOur Approach
Our Approach w/o SC
Our Approach w/o DEOur Approach w/o AI
Our Approach w/o Gen. Tests
(b)Ivy0204060
0 50 100 150 200
# of Methods in Top Optimization Spaces# of Actually Optimized MethodsOur Approach
Our Approach w/o SC
Our Approach w/o DEOur Approach w/o AI
Our Approach w/o Gen. Tests
(c) PDFBox
Figure 8: Metric Sensitivity in Subject Projects
priorityscoresofthemethodsandtheoptimizationspacessignif-
icantlybecomelowerafterperformanceoptimization.Further-
more, we use Cliff’s Delta effect size [ 13] to measure the magni-
tudeofthedifferenceifWilcoxontestindicatesasignificantdiffer-
ence.Cliff’sDelta(i.e., d)measureshowoftenthevaluesinadis-
tributionarelargerthanthevaluesinanotherdistribution.Theef-fect size is negligible for
|d|<0.147,smallfor 0.147≤|d|<0.33,
mediumfor 0.33≤|d|<0.474, andlargefor|d|≥0.474.
Answer RQ1-1. Table 6 compares five groups of methods: the
whole methods in a system (column 2-4), the methods in top 25
(column5-7),50(column8-10)and100(column11-13)optimization
spaces,andthehotspotsreportedbyYourKit(column14-16).For
eachgroupofmethods,wereportthenumberofactuallyoptimizedmethods(column Opt.),thetotalnumberofmethods(column Total),
and the density (i.e., Opt./Total ) in this group (column Den.).
The table shows 19% to 43% of methods in top 25 spaces are
actually optimized. When the investigated optimization spaces
adjustfromtop25totop100ones,thenumberofmethodsincreasesdrasticallyandthedensityofactuallyoptimizedmethodsdecreasesslightly.Itsuggestsdeveloperstofocusonthetopfewoptimization
spaces to narrow down the methods as candidates to optimize.
In comparison, the density of actually optimized methods in
thewholesystemrangesfrom1.31%to7.44%.Thedensitybyourapproachisseveralordersofmagnitudehigherthanrandomsearch
(when developers have no clue). YourKit reports a list of hot spots
andsuggestseachoneasapotentialoptimizationopportunity.In
ordertocompareSpeedoowithYourKit,we selectedthesamenum-
ber of hot spots (excluding library methods) with the number of
methodsintop100optimizationspaces.NotingthatYourKitonly
reported 96 hot spots in PDFBox. The density shown in column 13
and column 16 indicate that our approach outperforms YourKit by
2-3 times. We can conclude that our approach provides a signifi-
cantlymoreeffectiveprioritizationofoptimizationopportunities,
comparedwithrandomsearchandYourKit,andthusisworthwhile.
Answer RQ1-2. Table 7 reports the change of optimization pri-
ority scores in the actually optimized methods and the methodsin optimized optimization spaces. For each group of methods, itreports the total number of target methods (column Total), the
number of methods whose priority scores increase (column Higher
Rank), the number of methods whose priority scores decrease (col-umnLower Rank), the p-value of Wilcoxon test, and Cliff’s Delta d
if Wilcoxon test shows significant lower values (p -value<0.001).
The table reveals that the optimization priorities of actually
optimizedmethodsarenotsignificantlylowerafterperformance
optimization;incontrast,theoptimizationprioritiesofthemethods
inactuallyoptimizedoptimizationspacesaresignificantlylower.
818
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Speedoo: Prioritizing Performance Optimization Opportunities ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 8: Performance Ov erheadofSpeedoo
ProjectMetric ComputationMethod Prioritization Optimization Space Localization
Architectural Metrics Static Complexity Metrics Dynamic Execution Metrics
Avro 0.4 s 2.8 s 1.5 h + 2.5 h 0.3 s 3.0 s
Ivy 2.2 s 22.0 s 1.0 h + 4.5 h 3.1 s 79.7 s
PDFBox 3.7 s 30.6 s 3.0 h + 5.5 h 10.3 s 195.8 s
Wilcoxon test does not indicate a significant difference in Avro,be -
causethenumberoftargetmethodsissmall(i.e.,only48methods),
but most of the methods in actually optimized optimization spaces
(i.e.,34outof48methods)arerankedlowerintherefactoredrelease.
These results indicate that even if the performance of optimized
methodsdoesnotshowsignificantimprovements,theperformance
ofthemethodsintheirspaceswouldbenefitfromtheoptimization.
Thisalsodemonstratestherationalityofourprioritizationonthe
level of optimization spaces but not on the level of methods.
ThesmalleffectsizesinTable7indicatethattheperformanceim-
provements are limited in most cases. In fact, we find performance
optimizationperformedbydevelopersisusuallyminor,e.g.,remov-
ing an unnecessary type cast. We also find some highly-rankedmethods were optimized many times in history. It implies that
performance optimization is a long way in software evolution.Summary.
Speedoo is effective i n prioritizing potential perfor-
mance optimization opportunities, and outperforms a state-of-the-
art profiling tool YourKit; and the prioritization is advisable as the
priorities of the methods in actually optimized optimization spaces
become significantly lower after performance optimization.
4.3 Sensitivity Evaluation (RQ2)
Three groups of metrics are used in our approach (i.e., architec-
tural impact metrics, static complexity metrics and dynamic execu-
tionmetrics),thequestionis:whatisthesensitivityofthesemetricsontheeffectivenessofourapproach?Toanswerit,weremoveeach
group of metrics from our approach, and then estimate the metricsensitivity by comparing the effectivenessof different approaches.
AnswerRQ2. TheresultsarevisuallyillustratedinFig.8,wherethe
x-axisrepresentsthenumberofmethodsintopoptimizationspaces,
andthey-axisrepresentsthenumberofactuallyoptimizedmethods.
Generally, the curves of our approach without architectural impact
metrics (AI), static complexity metrics (SC) and dynamic execution
metrics (DE) are lower than the curve of our approach with all the
metrics,especiallysignificantfor AvroandIvy.Theresultsindicate
thateachgroupofmetricscontributestotheeffectivenessofour
approach, and thus a combination of them is reasonable.
Similarly, wealsoevaluatethesensitivity ofeachmetric.After
removing any metric in Table 1 from our approach, the density in
top25spacesdecreasesto14.81%-19.23%,10.34%-18.52%,and36.36%-
44.44% for three projects respectively. The density is lower than or
samewithSpeedoo, withonlyoneexceptioncomingfromremoving
CCforPDFBox.Itindicatesthateachusedmetric isvitaltoSpeedoo.
Besides,wetriedtoassigndifferentweightstothesemetricsand
found that, using the same weights performed best in general.
In order to cover more methods when collecting dynamic execu-
tionmetrics,werunboththeoriginaltestsinsystemsaswellasthetestsautomaticallygeneratedbytheunittestingtoolEvoSuite[
19].In comparison, we show the effectiveness of our approach with-out generated tests in Fig. 8 (Our Approach w/o Gen. Tests ). The
approach without generated tests turns out to be as effective as
the whole approach. This is because the execution behaviors in
automatically generated tests are different from the real behaviors
in manually written tests. This suggests that the effectiveness of
approachisalsoaffectedbywhetherthecollecteddynamicmetrics
can reflect real executions that manifest performance problems.
Summary. All the metrics used in this study have contribution to
theeffectivenessof Speedoo.Besides, dynamicexecutionmetrics
should be obtained through real execution behaviors.
4.4 Performance Evaluation (RQ3)
Thisresearchquestionaddresseshowtheperformanceoverhead
ofSpeedoois.Toanswerthisquestion,wecollectthetimespentineachstepoverthethreeprojects,consistingofmetriccomputation,
method prioritization, and optimization space localization.
AnswerRQ3. Table 8 presents the time consumed in each step of
theproposedapproach.Wecanseethatcomputingdynamicexecu-tionmetricstakesthemosttime.Notethatthisstepincludesthetime
for running original tests with YourKit, as well as the time for gen-
eratingtestsbyEvoSuiteandrunningthemwithYourKit,whicharerespectivelyreportedinTable8.AstheresultofRQ2indicatesthatourapproachwithoutgeneratedtestsisaseffectiveasourapproach
withgeneratedtests,theperformanceoverheadwouldbesignifi-
cantlyimprovedwhenapplyingSpeedoowithoutgeneratedtests.
Generally, our approach scales to large-scale project like PDFBox.
Summary. Speedooscales tolarge-scaleprojects;andcomputing
dynamic metrics takes most of the time as it needs to run tests.
5 DISCUSSION
ThreatstoValidity. First,thevaluesofdynamicexecutionmetrics
are affected by the tests that produce them. Basically ,Speedoocan
beimprovedthroughtheteststhatcovermoresuspectmethodsand
reflect real execution behaviors. Hence, in this study, the dynamic
executionmetricswereobtainedbyexecutingnotonlytheoriginal
testsbutalsotheautomaticallygeneratedtests,althoughitturns
outthatautomaticallygeneratedtestsarenotveryhelpfulasshown
inSection4.3.Inthefuture,weplantofurtherinvestigatehowto
generateadvancedtests[ 21,41],andintegrateitintoourapproach.
Second,thegroundtruthwascollectedfromthedocumentedper-
formanceissuesandcommitlogsofsubjectprojects.Insomecases,
developersdidnotdocumentoptimizationoperationsinissuere-
portswhencommittingchanges.Wemitigatedthisproblembyalso
matching performance keywords in commit messages in case of
lacking issue records. The second problem is the informal descrip-
tionofperformanceproblemsinissuesandcommitlogs.Tomitigateit,weadoptedalistofperformancekeywordsthatiswidelyusedin
literature, and manually checked the matched commits and issues.
819
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Z. Chen et al.
Analysis of Top Optimization Spaces. We manually checked
somereallyoptimizedmethodsintopoptimizationspaces.Wefind
that performance optimization tends to be conducted on the meth-
odsinhigherDRHlayers.Forexample, getDictionaryObject inFig.5
is in the highest DRH layer and its optimization space is ranked
2.Wealsofindthatthemethodswereoptimizedmostlybyminor
changes.Thisindicatesthattimeisnoteverythinginmotivating
performanceoptimization,andarchitecturalconsiderationsarealso
important. This also explains why Speedoooutp erforms YourKit.
We also checkedthose highly-ranked methodswhich were not
optimized. We can discover that, some methods were not opti-
mized, although users reported a performance issue, because it
wouldincurhighmaintenancecost(e.g.,issueAVRO-1809)orthe
code is kept remained for future extensions (e.g., issue AVRO-464);
and some methods have been optimized before the analyzed re-
lease, but the previous optimization only reduces the performance
problem and cannot totally fix it (e.g., issue AVRO-911).Extensions.
Speedoo provides a ranked list of optimization op-
portunitiestodevelopers.Onepotentialextensionistointegrate
pattern-basedtechniquesintoSpeedoo.Oncepattern-basedtech-
niques locate a pattern in the system, we can extend Speedoo to
determineoptimizationspacesofthemethodsinthepattern,which
indicates the impact of the pattern on other methods as well as the
potential improvement that can be achieved by performance opti-mization.Anotherpossibleextensionistointegratemoremetrics
intoSpeedootob etter reveal performance problems. For example,
we can give higher priorities to the methods with more expensive
API calls. Furthermore, this study assigns equal weights to the fac-
tors in Eq. 5, Eq. 6, and Eq. 7. If additional metrics are integrated
intoourapproach,thefactorscanbeassignedindividualweights
which are tuned with different combinations of weights.
6 RELATED WORK
PerformanceUnderstanding. Severalempiricalstudieshavebeen
conducted to understand the characteristics ofperformance prob-
lemsfrom differentperspectives [ 7,23,29,33,39,43,59].They in-
vestigatedrootcausesofperformanceproblemsaswellashowper-
formance problems are introduced, discovered, reported, and fixed,
which providesvaluable insights and guidances fordesigning per-
formanceprofilingandperformanceproblemdetectionapproaches.
Profiling-BasedPerformanceAnalysis. Profilingtools[ 1,3]are
widelyusedtolocatehotspotmethodsthatconsumemostresources
(e.g., memory and time). Besides, several path-sensitive profiling
techniques(e.g., [ 6,17,27])have been proposedtoanalyzeexecu-
tion traces of an instrumented program for predicting execution
frequencyofprogrampathsandidentifyinghotpaths.Further,there
has been some recent work on input-sensitive profiling techniques
(e.g.,[14,20,60]).Theyexecuteaninstrumentedprogramwithaset
ofdifferentinputsizes,measuretheperformanceofeachbasicblock,andfitaperformancefunctionwithrespecttoinputsizes.Moreover,
Mudduluru and Ramanathan [ 30] proposed an efficient flow profil-
ingtechniquefordetectingmemory-relatedperformanceproblems.Chenetal.[
11]appliedprobabilisticsymbolicexecutiontogenerate
performance distributions, while Brünink and Rosenblum [ 9] used
in-field data to extract performance specifications. These profiling
techniquesaremorehelpfultocomprehendperformancethantopinpointperformance problems, becausetheyuseresourcesor exe-
cution frequencies as the only performance indicator and do not
consider the performance impact among architecturally connected
code. Thus, developers have to waste many manual effort to locate
rootcauses. Differently,our approachtries torelieve suchburden
from developers by prioritizing optimization opportunities.
Besides,severaladvances[ 4,21,22,41,58]havebeenmadetofur-
ther analyze the profiles or execution traces for performance prob-
lem detection. Specifically, Ammons et al. [ 4] find expensive call
sequencesincall-treeprofiles,andthecallsequencesthataresignif-
icantly more expensive in one call-tree profile than in another call-
tree profile. Han et al. [ 22] and Yu et al. [ 58] mine performance
behavioralpatternsfromstacktracesandexecutiontraces.These
approachesheavilyrelyonthetestinputsthatgeneratetheprofiles;
i.e.,performanceproblemsmightstayunrevealedastheyarenot
manifested by those test inputs. Instead, our approach also con-siders architectural impact and static complexity to avoid solely
depending on dynamic executions.Pattern-BasedPerformanceProblemDetection.
Alargebody
ofworkhasbeendoneonthedetectionofspecificperformanceprob-
lems. One line of work focuses on loop-related performance prob-
lems;e.g.,inefficientloops[ 16,34],redundantloops[ 32,34]andre-
dundantcollectiontraversals[ 35].Anotherlineofworkfocuseson
memory-relatedperformanceproblems;e.g.,under-utilizedorover-
utilizedcontainers[ 25,28,40,53],inefficientusageoftemporary
structuresorobjects[ 18,51,52,55]andreusableorcacheabledata[ 8,
15,31,50,54].Besidesthesetwomainlinesofwork,researchershave
also investigated performance problems caused by inefficient or in-
correct synchronization [ 36,57], slow responsiveness of user inter-
faces [24,37], heavy input workloads [ 49], inefficient order of eval-
uatingsubexpressions[ 38]andanomalousexecutions[ 26].Inaddi-
tion,performanceanti-patterns[ 42]areusedtodetectperformance
problems satisfying the anti-patterns [ 12,45]. While they are effec-
tive at detecting specific types of performanceproblems, they fail
to be applicable to a wider range of performance problem types,
as these patterns focus on the root causes of specific performance
problems. Differently, thepatterns inthis studyconsider theopti-
mizationlocationsfromexecutionsymptoms,andaremoregeneric.
7 CONCLUSIONS
Weproposedandimplementedanovelapproach,named Speedoo,
to identify the optimization opportunities that should be givenhigh priorities to performance-critical methods to improve theoverall system performance. Our evaluation on real-life projectshas indicated that our approach effectively prioritizes potential
performanceoptimizationopportunitiestoreducethemanualeffort
ofdevelopers, andsignificantly outperformsrandomsearch andastate-of-the-art profiling tool YourKit.
ACKNOWLEDGMENTS
The work is supported by the National Key Basic Research and De-
velopmentProgramofChina(2014CB340702),theNationalNatural
ScienceFoundationofChina(61472175,61472178,61772263),the
NaturalScienceFoundationofJiangsuProvinceofChina(BK20140611),
and the program B for Outstanding PhD candidate of Nanjing Uni-
versity.
820
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. Speedoo: Prioritizing Performance Optimization Opportunities ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]2017. JPROFILER. https://www.ej-technologies.com/products/jprofiler/overview.
html. (2017).
[2] 2017. Understand. https://scitools.com/. (2017).
[3] 2017. YourKit. https://www.yourkit.com/. (2017).[4]
GlennAmmons,Jong-DeokChoi,ManishGupta,andNikhilSwamy.2004.Finding
and Removing Performance Bottlenecks in Large Systems. In ECOOP. 172–196.
[5]CarlissY.Baldwinand KimB.Clark.1999. DesignRules:ThePowerofModularity
Volume 1. MIT Press, Cambridge, MA, USA.
[6]ThomasBallandJamesR.Larus.1996. EfficientPathProfiling.In MICRO.46–57.
[7]S. Baltes, O. Moseler, F. Beck, and S. Diehl. 2015. Navigate, Understand, Commu-
nicate: How Developers Locate Performance Bugs. In ESEM. 1–10.
[8]SuparnaBhattacharya,MangalaGowriNanda,K.Gopinath,andManishGupta.
2011. Reuse, Recycle to De-bloat Software. In ECOOP. 408–432.
[9]MarcBrüninkandDavidS.Rosenblum.2016. MiningPerformanceSpecifications.
InFSE. 39–49.
[10]Yuanfang Cai and Kevin J. Sullivan. 2006. Modularity Analysis of Logical Design
Models. In ASE. 91–102.
[11]BihuanChen,YangLiu,andWeiLe.2016. GeneratingPerformanceDistributions
via Probabilistic Symbolic Execution. In ICSE. 49–60.
[12]Tse-Hsun Chen, Weiyi Shang, Zhen Ming Jiang, Ahmed E. Hassan, MohamedNasser, and Parminder Flora. 2014. Detecting Performance Anti-patterns for
Applications Developed Using Object-relational Mapping. In ICSE. 1001–1012.
[13]Norman Cliff. 1993. Dominance statistics: Ordinal analyses to answer ordinal
questions. Psychological Bulletin 114, 3 (1993), 494.
[14]Emilio Coppa, Camil Demetrescu, and Irene Finocchi. 2012. Input-sensitive
Profiling. In PLDI. 89–98.
[15]Luca Della Toffola, Michael Pradel, and Thomas R. Gross. 2015. Performance
ProblemsYouCanFix:ADynamicAnalysisofMemoizationOpportunities.In
OOPSLA. 607–622.
[16]Monika Dhok and Murali Krishna Ramanathan. 2016. Directed Test Generation
to Detect Loop Inefficiencies. In FSE. 895–907.
[17]Evelyn Duesterwald and Vasanth Bala. 2000. Software Profiling for Hot Path
Prediction: Less is More. In ASPLOS. 202–211.
[18]Bruno Dufour, Barbara G. Ryder, and Gary Sevitsky. 2008. A Scalable Tech-
nique for Characterizing the Usage of Temporaries in Framework-intensive Java
Applications. In FSE. 59–70.
[19]Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: Automatic Test Suite Genera-
tion for Object-Oriented Software. In ESEC/FSE. 416–419.
[20]SimonF.Goldsmith,AlexS.Aiken,andDanielS.Wilkerson.2007. Measuring
Empirical Computational Complexity. In ESEC-FSE. 395–404.
[21]Mark Grechanik, Chen Fu, and Qing Xie. 2012. Automatically Finding Perfor-mance Problems with Feedback-directed Learning Software Testing. In ICSE.
156–166.
[22]Shi Han, Yingnong Dang, Song Ge, Dongmei Zhang, and Tao Xie. 2012. Per-
formanceDebuggingintheLargeviaMiningMillionsofStackTraces.In ICSE.
145–155.
[23]Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and Shan Lu. 2012.
Understanding and Detecting Real-world Performance Bugs. In PLDI. 77–88.
[24]MilanJovic,AndreaAdamoli,andMatthiasHauswirth.2011. CatchMeifYou
Can: Performance Bug Detection in the Wild. In OOPSLA. 155–170.
[25]Changhee Jung, Silvius Rus, Brian P. Railing, Nathan Clark, and Santosh Pande.
2011. Brainy: Effective Selection of Data Structures. In PLDI. 86–97.
[26]Charles Killian, Karthik Nagaraj, Salman Pervez, Ryan Braud, James W. An-
derson, and Ranjit Jhala. 2010. Finding Latent Performance Bugs in Systems
Implementations. In FSE. 17–26.
[27] James R. Larus. 1999. Whole Program Paths. In PLDI. 259–269.
[28]LixiaLiuandSilviusRus.2009. Perflint:AContextSensitivePerformanceAdvisor
for C++ Programs. 265–274.
[29]Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing and Detect-
ing Performance Bugs for Smartphone Applications. In ICSE. 1013–1024.
[30]Rashmi Mudduluru and Murali Krishna Ramanathan. 2016. Efficient Flow Profil-
ing for Detecting Performance Bugs. In ISSTA. 413–424.
[31]KhanhNguyenandGuoqingXu.2013. Cachetor:DetectingCacheableDatato
Remove Bloat. In ESEC/FSE. 268–278.
[32]AdrianNistor,Po-ChunChang,CosminRadoi,andShanLu.2015. CARAMEL:
Detecting and Fixing Performance Problems That Have Non-Intrusive Fixes. InICSE. 902–912.
[33]Adrian Nistor, Tian Jiang, and Lin Tan. 2013. Discovering, Reporting, and Fixing
Performance Bugs. In MSR. 237–246.
[34]AdrianNistor,LinhaiSong,DarkoMarinov,andShanLu.2013.Toddler:Detecting
Performance Problems via Similar Memory-access Patterns. In ICSE. 562–571.
[35]Oswaldo Olivo, Isil Dillig, and Calvin Lin. 2015. Static Detection of Asymptotic
Performance Bugs in Collection Traversals. In PLDI. 369–378.
[36]Michael Pradel, Markus Huggler, and Thomas R. Gross. 2014. Performance
Regression Testing of Concurrent Classes. In ISSTA. 13–25.
[37]MichaelPradel,ParkerSchuh,GeorgeNecula,andKoushikSen.2014.EventBreak:
Analyzing the Responsiveness of User Interfaces Through Performance-guided
Test Generation. In OOPSLA. 33–47.
[38]Marija Selakovic, Thomas Glaser, and Michael Pradel. 2017. An Actionable
PerformanceProfilerforOptimizingtheOrderofEvaluations.In ISSTA.170–180.
[39]MarijaSelakovicandMichaelPradel.2016. PerformanceIssuesandOptimizations
in JavaScript: An Empirical Study. In ICSE. 61–72.
[40]Ohad Shacham, Martin Vechev, and Eran Yahav. 2009. Chameleon: Adaptive
Selection of Collections. In PLDI. 408–418.
[41]DuShen,QiLuo,DenysPoshyvanyk,andMarkGrechanik.2015. Automating
Performance Bottleneck Detection Using Search-based Application Profiling. In
ISSTA. 270–281.
[42]Connie Smith and Lloyd G. Williams. 2002. New Software Performance AntiPat-
terns: More Ways to Shoot Yourself in the Foot. In CMG. 667–674.
[43]LinhaiSongandShanLu.2014. StatisticalDebuggingforReal-worldPerformance
Problems. In OOPSLA. 561–578.
[44]Linhai Song and Shan Lu. 2017. Performance Diagnosis for Inefficient Loops. In
ICSE. 370–380.
[45]Alexander Wert, Jens Happe, and Lucia Happe. 2013. Supporting Swift Reaction:
Automatically Uncovering Performance Problems by Systematic Experiments. In
ICSE. 552–561.
[46]Frank Wilcoxon. 1992. Individual Comparisons by Ranking Methods", bookTi-
tle="Breakthroughs in Statistics: Methodology and Distribution. 196–202.
[47]Sunny Wong, Yuanfang Cai, Giuseppe Valetto, Georgi Simeonov, and Kan-
warpreet Sethi. 2009. Design Rule Hierarchies and Parallelism in Software
Development Tasks. In ASE. 197–208.
[48]Lu Xiao, Yuanfang Cai, and Rick Kazman. 2014. Titan: A Toolset That Connects
Software Architecture With Quality Analysis. In FSE. 763–766.
[49]XushengXiao,ShiHan,DongmeiZhang,andTaoXie.2013. Context-sensitive
DeltaInference forIdentifyingWorkload-dependentPerformanceBottlenecks.
InISSTA. 90–100.
[50] Guoqing Xu. 2012. Finding Reusable Data Structures. In OOPSLA. 1017–1034.
[51]GuoqingXu,MatthewArnold,NickMitchell,AtanasRountev,andGarySevitsky.
2009. GowiththeFlow:ProfilingCopiestoFindRuntimeBloat.In PLDI.419–430.
[52]Guoqing Xu, Nick Mitchell, Matthew Arnold, Atanas Rountev, Edith Schonberg,
and Gary Sevitsky. 2010. Finding Low-utility Data Structures. In PLDI. 174–186.
[53]Guoqing Xu and Atanas Rountev. 2010. Detecting Inefficiently-used Containers
to Avoid Bloat. In PLDI. 160–173.
[54]GuoqingXu,DacongYan,andAtanasRountev.2012. StaticDetectionofLoop-
invariant Data Structures. In ECOOP. 738–763.
[55]DacongYan,GuoqingXu,andAtanasRountev.2012. UncoveringPerformance
Problems in Java Applications with Reference Propagation Profiling. In ICSE.
134–144.
[56]Yibiao Yang, Mark Harman, Jens Krinke, Syed Islam, David Binkley, Yuming
Zhou,andBaowenXu.2016. AnEmpiricalStudyonDependenceClustersfor
Effort-Aware Fault-Proneness Prediction. In ASE. 296–307.
[57]Tingting Yu and Michael Pradel. 2016. SyncProf: Detecting, Localizing, and
Optimizing Synchronization Bottlenecks. In ISSTA. 389–400.
[58]Xiao Yu, Shi Han, Dongmei Zhang, and Tao Xie. 2014. Comprehending Per-
formance from Real-world Execution Traces: A Device-driver Case. In ASPLOS.
193–206.
[59] Shahed Zaman, Bram Adams, and Ahmed E. Hassan. 2012. A Qualitative Study
on Performance Bugs. In MSR. 199–208.
[60]DmitrijsZaparanuksandMatthiasHauswirth.2012. AlgorithmicProfiling.In
PLDI. 67–76.
821
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:37:39 UTC from IEEE Xplore.  Restrictions apply. 