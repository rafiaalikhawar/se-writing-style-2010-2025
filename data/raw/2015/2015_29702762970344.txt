Recommending Relevant Classes for Bug Reports using
Multi-objective Search
Raﬁ Almhana∗, Wiem Mkaouer∗, Marouane Kessentini∗, Ali Ouni‡
∗Computer and Information Science Department
University of Michigan, Dearborn, MI, USA‡Graduate School of Information Science and T echnology
Osaka University , Osaka, Japan
ﬁrstname@umich.edu ali@ist.osaka-u.ac.jp
ABSTRACT
Developers may follow a tedious process to ﬁnd the cause
of a bug based on code reviews and reproducing the ab-normal behavior. In this paper, we propose an automated
approach to ﬁnding and ranking potential classes with the
respect to the probability of containing a bug based on a bugreport description. Our approach ﬁnds a good balance be-tween minimizing the number of recommended classes and
maximizing the relevance of the proposed solution using a
multi-objectiveoptimizationalgorithm. Therelevanceoftherecommended classes (solution) is estimated based on theuse of the history of changes and bug-ﬁxing, and the lexicalsimilarity between the bug report description and the API
documentation. We evaluated our system on 6 open source
Java projects, using the version of the project before ﬁx-ing the bug of many bug reports. The experimental resultsshow that the search-based approach signiﬁcantly outper-
forms three state-of-the-art methods in recommending rele-
vant ﬁles for bug reports. In particular, our multi-objectiveapproach is able to successfully locate the true buggy meth-ods within the top 10 recommendations for over 87% of thebug reports.
CCS Concepts
•Software and its engineering →Search-based soft-
ware engineering;
Keywords
Search-basedsoftwareengineering; bugreports; multi-objective
optimization; software maintenance.
1. INTRODUCTION
A software bug is a coding error that may cause abnormal
behaviors and incorrect results when executing the system
[6]. After identifying an unexpected behavior of the soft-ware project, a user or developer will report it in a docu-ment, called a bug report [3]. Thus, a bug report should
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributedfor proﬁt or commercial advantage and that copies bear this notice and the full cita-tion on the ﬁrst page. Copyrights for components of this work owned by others thanACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior speciﬁc permissionand/or a fee. Request permissions from permissions@acm.org.
ASE ’16 Singapore, Singapore
c/circlecopyrt 2016 ACM. ISBN 123-4567-24-567/08/06 . . . $15.00
DOI:10.475/123 4provide useful information to identify and ﬁx the bug. The
number of these bug reports can be large. For example,MOZILLA had received more than 420,000 bug reports [4].
These reports are important for managers and developers
during their daily development and maintenance activities[11].
A developer always uses a bug report to reproduce the
abnormal behavior to ﬁnd the origin of the bug. However,
the poor quality of bug reports can make this process te-
dious and time-consuming due to missing information. Toﬁnd the cause of a bug, developers are not only using theirdomain knowledge to investigate the bug report, but inter-
act with peer developers to collect additional information.
An eﬃcient automated approach for locating and rankingimportant code fragments for a speciﬁc bug report may leadto improving the productivity of developers by reducing thetime to ﬁnd the cause of a bug [11]. Most of the existing
studies are mainly based on lexical matching scores between
the statements of bug reports and the name of code elementsin software systems [23]. However, there is a signiﬁcant dif-ference between the natural language used in bug reports
and the programming language which limits the eﬃciency
of existing approaches.
In this work, we start from the following observations.
First, API documentation of the classes and methods canbe more useful than the name of code elements or comments
to estimate the similarity between code fragments and bug
reports. Second, classes associated to previously ﬁxed bugreports may be relevant also to the current report if thesepreviously bug reports are similar to a current bug report.
Third, a code fragment that was ﬁxed recently is more likely
to still contain bugs than another class that was last ﬁxedlongtimeago. Fourth, aclassthathasbeenfrequentlyﬁxed,tend to be fault-prone and may cause more than one abnor-mal behavior in the future. Finally, the recommendation of
a large number of classes to inspect may make the process
of ﬁnding the cause of a bug time-consuming.
To consider the above observations, we propose a compre-
hensive approach for bugs localization based on bug reports
description. To this end, we propose, for the ﬁrst time, to
use a multi-objective optimization algorithm [7] to ﬁnd abalance between maximizing lexical and history-based simi-larity, and minimizing the number of recommended classes.The problem is formulated as a search for the best combina-
tionand sequenceofclassesfromalltheclassesofthesystem
that optimize as much as possible the above two conﬂictingobjectives.
We have executed an extensive empirical evaluation of 6
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
ASE’16 , September 3–7, 2016, Singapore, Singapore
c2016 ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970344
286
large open-source software projects with more than 22,000
bug reports in total based on an existing benchmark [26].
The results on the before-ﬁx versions show that our system
outperforms, on average, three state-of-the-art approachesnot based on search techniques [26, 16, 28]. In particular,our search-based approach is able to successfully locate thetrue buggy methods within the top 10 recommendations for
over 87% of the bug reports.
The primary contributions of this paper can be summa-
rized as follows:
•To the best of our knowledge and based on recent sur-
veys [14], the paper proposes the ﬁrst search-basedsoftware engineering approach to address the problem
of ﬁnding relevant code fragments for bug reports. Theapproach combines the use of lexical and history basedsimilarity measures to locate and rank relevant code
fragments for bug reports while minimizing the num-
b e ro fr e c o m m e n d e dc l a s s e s .
•The paper reports the results of an empirical study
withanimplementationofourmulti-objectiveapproach.The obtained results provide evidence to support theclaim that our proposal is more eﬃcient, on average,
than existing techniques [16, 26, 28] based on a bench-
mark of 6 open source systems. We also compared theresults of our multi-objective approach with a mono-objective formulation to make sure that our objectivesare conﬂicting.
The remainder of this paper is as follows: Section 2 is
dedicated for the related work. Section 3 describes the pro-
posed approach and the search algorithm. An evaluation ofthe approach and its results are explained in Section 4 whileSection 5 further discusses the obtained results. Section 6describes the threats to validity related to our experiments.
Finally, concluding remarks and future work are provided in
Section 7.
2. RELATED WORK
In this section, we survey diﬀerent studies related to the
areas of bug localization and search-based software engineer-
ing.
2.1 Bug Localization
Theproblemofbuglocalizationcanbeconsideredassearch-
ing the source of a bug given its description. To address thisproblem, the majority of existing studies is based on the use
Information-Retrieval (IR) techniques through the detection
of textual and semantic similarities between a newly givenreport and source code entities [23]. Several IR techniqueshave been investigated, namely the Latent Semantic Index-ing (LSI) [9], Latent Dirichlet Allocation (LDA) [5] and the
Vector Space Model (VSM) [21]. In addition, hybrid models
extracted from these IRs techniques to tackle the problemof bug localization were proposed [26].
We summarize, in the following, the diﬀerent tools and
approaches proposed in the literature based on the above
IR techniques. BugScout [16] is a topic-based approach us-ing LDA to analyze the bug related information (descrip-tion, comments, external links, etc.) to detect the source ofa bug and duplicated bug reports. The main limitation of
BugScout is the dependency of the results on the keywordsentered by the user. DebugAdvisor’s [2] is a bug investiga-
tion system that takes as input a bug report in terms of textqueries then uses them to mine existing ﬁxed bug repos-
itory and generate a graph of possible reports. However,
DebugAdvisor accuracy depends on the accuracy of the re-port’s description and its accuracy when describing the bugand its related code entities.
BugLocator [28] combines several similarity scores from
previous bug reports for bug localization. It generates a
VSM model to extract suspect source ﬁles for a given bugreport. Then, BugLocator mines previously ﬁxed bug re-ports along with related ﬁles involved to rank suspect code
fragments. The main issue raised in this work is the prone-
ness of the weight density to the noise in the large ﬁles. Toovercomethislimitation, [25]addedsegmentationandstack-traceanalysistoimprovetheperformanceoftheBugLocatorapproach. The limitation of this extension is that execution
traces are not necessarily available in bug repositories.
BLUiR [20] has been proposed also to compare a bug
report to the structure of source ﬁles. It decomposes re-ports into summaries and then uses the structural retrieval
to calculate similarities between these tokenized elements
and source code ones to rank source code ﬁles. Saha etal. [19] extended BLUiR to consider similar reports infor-mation, similarly to BugLocator as an additional similarityscore. DHbPd [18] incorporated code change information
for bug localization. The main idea is to consider recently
changed source code elements as potential candidates forhosting a bug.
Ye et al. [26] has modeled the similarity between bug re-
ports and source code through several characteristics that
are captured through the use of 6 similarity features thatdescribe the projects domain knowledge. The combinationofthesemeasuresisfedtoarankingheuristiccalledlearning-to-rank. Therankingmodelreturnsthetopcandidatesource
ﬁles to investigate for a given bug report. The main original-
ity of their work is the use of projects API description andauto-generated documentation as one of the features to uti-lize to reduce the lexical gap between the human description
and the source code.
In [27], Ye et al. extended their previous work by ex-
tending their ranking features utilized by learning-to-rankfrom 6 to 19. Besides the existing surface lexical similarity,API-based lexical similarity, collaborative ﬁltering, code ele-
ments naming similarity, ﬁxed bugs frequency, they included
other source code characteristics that can be extracted fromthe projects such as summaries, naming conventions, inter-class dependencies etc. Although taking these features into
account has given better results in terms of better ﬁles rank-
ing, suchinformationmaynotbeavailableinallprojectsandsometimes it may be outdated and that may deteriorate thelocalizations accuracy.
We propose, in this paper, a more comprehensive ap-
proach to address the problem of bugs localization from dif-
ferent perspectives as detailed in the next sections.
2.2 Search-Based Software Engineering
Search-Based Software Engineering (SBSE) uses a compu-
tational search approach to solve optimization problems in
software engineering [13]. Once a software engineering task
is framed as a search problem, by deﬁning it in terms of so-lution representation, ﬁtness function, and solution changeoperators, there is a multitude of search algorithms that can
287Finding relevant classes for bug 
reports using NSGA-II
Objective 1 : Maximize the relevance 
of recommended classes
Objective 2 : Minimize the number of 
recommended classes.Source code and API specifications 
of the program to be inspected
A list of previous bug reportsBest sequence of 
classes to inspectThe history of the applied changes 
in previous releasesThe description of the bug report(s)
Figure 1: Approach Overview.
be applied to solve that problem.
Many search-based software testing techniques have been
proposedfortestcasesgeneration[17], mutationtesting[15],regression testing [22] and testability transformation. How-ever, the problem of bugs localization was not addressed be-
fore using SBSE. The closest problem addressed using SBSE
techniques is the bugs prioritization problem [8]. A mono-objective genetic algorithm was proposed to ﬁnd the best se-quence of bugs resolution that maximizes the relevance and
importance of the bugs to ﬁx while minimizing the cost. The
main limitation of this work is the use of a mono-objectivetechnique that aggregates two conﬂicting objectives. In thenext section, we describe our formulation of bug localizationas a multi-objective problem.
3. MULTI-OBJECTIVE FORMULATION
We ﬁrst present an overview of our multi-objective ap-
proach to identify and prioritize relevant code fragments
(e.g. classes) for bug reports, and then we describe the de-tails of our multi-objective formulation.
3.1 Approach Overview
Our approach aims at exploring a large search space to
ﬁnd relevant classes, to inspect by developers, given a de-scription of a bug report. The search space is determinednotonlybythenumberofpossibleclasscombinationstorec-ommend, but also by the order in which they are proposed
to the programmer. In fact, bug reports may require the
inspection of more than one class to identify and ﬁx bugs.
A heuristic-based optimization method is proposed based
on two main conﬂicting objectives. The ﬁrst objective is the
correctness function that includes two sub-functions: 1.a)
maximizingtheLexicalsimilaritybetweenrecommendedclassesand the description of the bug report (including the API andname of code elements similarity), and 1.b) maximizing thehistory-based function score that includes the number of a
recommended classes that have been ﬁxed in the past, re-
cent changes introduced by the developers to these classesand similarities with previous bug reports. The second ob-jective is to minimize the number of classes to recommend.
It is clear that these two objectives are conﬂicting since
maximizing the relevance of recommended classes may leadsto a lower precision and thus increases the number of recom-mended classes. Thus, we consider, in this paper, the taskof localizing bugs as a multi-objective optimization problem
using the non-dominated sorting genetic algorithm (NSGA-
II) [7]. The proposed algorithm will explore a large searchspace of a combinatorial number of combinations of classesto recommend.
The general structure of our approach is sketched in Fig-
ure 1. It takes as input the source code of the program to beinspected, theAPIspeciﬁcationsoftheclassesofthesystem,the description of the bug report and a list of previous bugreports and the history of the applied changes in previous
releases. Our approach generates as output a near-optimalsequence of ranked classes that maximizes the relevance to
the bug report and minimizes the number of recommendedclasses. In the following, we describe an overview of NSGA-
II, the solution representation, a formal formulation of the
two objectives to optimize and the change operators.
3.2 NSGA-II
In this paper, we adapted one of the widely used multi-
objective algorithms called NSGA-II [7]. NSGA-II is a pow-
erful search method stimulated by natural selection that isinspired by the theory of Darwin. Hence, the basic ideaof NSGA-II is to make a population of candidate solutionsevolve toward the near-optimal solution in order to solve a
multi-objective optimization problem. NSGA-II is designed
to ﬁnd a set of optimal solutions, called non-dominated so-lutions, also Pareto set. A non-dominated solution is theone which provides a suitable compromise between all ob-
jectives without degrading any of them. As described in Al-
gorithm 1, the ﬁrst step in NSGA-II is to create randomly apopulation P
0of individuals encoded using a speciﬁc repre-
sentation (line 1). Then, a child population Q0is generated
from the population of parents P0using genetic operators
such as crossover and mutation (line 2). Both populations
are merged into an initial population R0of size N (line 5).
As a consequence, NSGA-II starts by generating an initialpopulation based on a speciﬁc representation that will be
discussed later, using the exhaustive list of classes from the
system to inspect given as input as mentioned in the pre-vious section. Thus, this population stands of a set solu-tions represented as sequences of classes to inspect, whichare randomly selected and ordered, for a speciﬁc bug report
description taken as input.
Algorithm 1 High level pseudo code for NSGA-II
1:Create an initial population P0
2:Create an oﬀspring population Q0
3:t=0
4:whilestopping criteria not reached do
5: Rt=Pt∪Qt
6:F = fast-non-dominated-sort( Rt)
7: Pt+1=∅and i=1
8: while|Pt+1|+|Fi|/lessorequalslantNdo
9:Apply crowding-distance-assignment( Fi)
10: Pt+1=Pt+1∪Fi
11: i=i+1
12: end while
13: Sort(Fi,≺n)
14: Pt+1=Pt+1∪Fi[N−|Pt+1|]
15: Qt+1= create-new-pop( Pt+1)
16:t=t + 1
17:end while
The whole population that contains N individuals (solu-
tions) is sorted using the dominance principle into several
fronts (line 6). The dominance level becomes the basis of
a selection of individual solutions for the next generation.Fronts are added successively until the parent populationPt+1 is ﬁlled with N solutions (line 8). When NSGA-IIhas to cut oﬀ a front Fi and select a subset of individual
solutions with the same dominance level, it relies on the
crowding distance to make the selection (line 9). This frontFi to be split, is sorted in descending order (line 13), and theﬁrst (N-|Pt+1|) elements of Fi are chosen (line 14). Then
a new population Qt+1 is created using selection, crossover
and mutation (line 15). This process will be repeated until
288reaching the last iteration according to stop criteria (line 4).
The following three subsections describe more precisely
our adaption of NSGA-II to the model change detection
problem.
3.3 Solution Approach
3.3.1 Solution representation
To represent a candidate solution (individual), we used a
vector representation. Each dimension of the vector repre-sents a class to recommend for a speciﬁc bug report. Thus,a solution is deﬁned as a sequence of classes to recommendfor inspection by the developer to locate the bug.
When created, the order of recommended classes corre-
spondstotheirpositionsinthevector. Theclassestorecom-mend are dependent since a bug can be located in diﬀerentclasses. In addition, the goal is to recommend a minimum
set of classes while maximizing the correctness objective.
StackRenderer CompositeRenderer TrimUtil
Figure 2: Simpliﬁed Example of a Solution Representation.
Bug ID: 378535
Summary: “Close All " and “ Close Others " menu
options available when right clicking on tab in 
PartStack when no part is closeable . 
Description :If I create a PartStack that contains
multiple parts but none of the parts arecloseable ,
when I right click on any of the tabs I get menu
options for “Close All“and “Close Others ".
Selection of either of the menu options doesn't cause
any tabs to be closed since none of the tabs can be
closed .
Reported: 2012-05-04 14:03
Figure 3: An Eclipse Bug Report Example1(ID 378535)
Figure 2 describes a simpliﬁed solution generated to ﬁnd
possible relevant classes for the bug report of Figure 3 that
shows an example of a bug report from the Eclipse project(ID 378535)
1. This bug report describes a defect about in-
correct menu options for parts that are not closeable. Thesolution consists of a sequence of three classes to inspect
extracted from the Eclipse project.
3.3.2 Fitness functions
Correctness objective: This objective is deﬁned as the av-
erage of two functions: lexical-based similarity (LS) and
history-based similarity (HS). Thus, we formally deﬁne this
function as:
f1=LS+HS
2(1)
The lexical-based similarity (LS) consists of an average
of two functions. The ﬁrst function is based on a cosinesimilarity [24] between the description of a bug report andthe source code. We used the whole content of a source code
ﬁle (the code and comments). The vocabulary was extracted
1https://bugs.eclipse.org/bugs/show bug.cgi?id=378535
Figure 4: A code fragment from the class StackRenderer.
from the names of variables, classes, methods, parameters,
types, etc. We used the Camel Case Splitter to perform theTokenization for preprocessing the identiﬁers [10].
During the tokenization process, we used a standard in-
formation retrieval stop words to eliminate irrelevant infor-
mation such as punctuation, numbers, etc. In addition, the
words are reduced to their stem based on a Porter stem-mer. This operation reduces the deviation between relatedwords such as designing and designer to the same stem de-
sign. Then, the cosine similarity measure is used to compare
between the description of a bug report and the source code.
Equation 2 calculates the cosine similarity between two
actors. Each actor is represented as an n dimensional vec-tor, whereeachdimensioncorrespondstoavocabularyterm.
The cosine of the angle between two vectors is considered as
an indicator of similarity. Using cosine similarity, the con-ceptual similarity between two actors: c
1andc2is deter-
mined as follows:
Sim(c1,c2)=Cos(−→c1,−→c2)=−→c1.−→c2
/bardbl−→c1/bardbl×/bardbl−→c2/bardbl
=n/summationtext
i=1(wi,1×wi,2)
/radicalBigg
n/summationtext
i=1(wi,1)2×n/summationtext
i=1(wi,2)2∈[0,1] (2)
where−→c1=(w1,1,...,w n,1) is the term vector correspond-
ing to actor c1and−→c2=(w1,2,...,w n,2) is the term vector
corresponding to c2. The weights wi,jis computed using in-
formation retrieval based techniques such as the Term Fre-
quency - Inverse Term Frequency (TF-IDF) method. Theﬁrst lexical simialrity function is then deﬁned as the sum ofthe of the cosine similarity scores between a description of a
bug report and the source code of each the suggested classes
divided by the total number of recommended classes.
As described in Figures 4 and 5, the description of the
bug report example includes several similar words with oneof the recommended classes to inspect, the class StackRen-
derer. Thus, the cosine similarity function applied between
the source code of that class and the description of the bugreport will detect such similarities. However, the only useof this similarity function may not be enough.
In fact, the text of a bug report is, in general, expressed
in a natural language however the large part of the content
289API Specification of MUIElement :
Description: A representation of the model object 'UI Label'. T his is a mix in that will be used 
for UI Elements that are capable of showing label information i n the GUI (e.g. Parts , Menus / 
Toolbars, Perspectives, ...). The following features are suppor ted: Label, Icon URI, Tooltip ...API Specifications:
Figure 5: API Speciﬁcation of the interface MUIElement.
of a source code is described in a programming language.
Thus, the similarity score between a bug report description
and a source code will be higher in case of an extensive use
of comments in the code or if the bug report clearly usesthe names of code elements. To address this challenge, wepropose to use an additional lexical similarity function.
The second lexical function is based on the use of cosine
similarity between the bug report description and the API
speciﬁcation of each method of a recommended buggy class.Thus, it is deﬁned as the sum of the maximum of the cosinesimilarity scores between a description of a bug report and
each of the methods composing the suggested class divided
by the total number of recommended classes.
As described in Figure 5, the class StackRenderer includes
a variable uiElement having as a type MUIElement. Figure5 shows the API speciﬁcation of the MUIElement interface
that includes diﬀerent terms such as parts and menus that
also exists in the bug report description of Figure 4. Thus,the lexical similarity between the API speciﬁcation and thedescription of a bug report may also help to better identify
relevant buggy classes.
The second component of the correctness objective is the
history-based similarity. This measure is an average of threefunctions. Theﬁrstfunctioncountsthenumberoftimesthata class was ﬁxed to eliminate bugs based on the history of
bug reports. In fact, a class that was ﬁxed several times has
a high probability of being a buggy class and includes newbugs. Formally, this function, normalized between [0,1] isdeﬁned as:
H
1=/summationtextSize (S)
i=1NbFixedBugs (report,c i)
Size(S)∗Max(NbFixedBugs (report,c))(3)
The second function checks if a recommended class was
recently changed or ﬁxed. In fact, a class that was modi-ﬁed recently has a higher probability of containing a bug.Thus, the function compares between the date of the bug
report and the last date where the recommended class was
modiﬁed. If a suggested class was modiﬁed on the same dayof the bug report then the value of this function is 1. Wedeﬁne this normalized function, normalized in the range of
[0, 1] as following:
H
2=/summationtextSize (S)
i=11
report.date −last (report,c i)+1
Size(S)(4)
The third function evaluates the consistency between the
recommended classes based on previous bug reports. Theclasses that are recommended together for similar previousbug reports have a high probability to include a bug evolv-
ing most of them. To this end, this function calculates ﬁrst
the cardinality, Cbr, of the largest intersection set of classesbetween the solution S and the sets of classes recommendedfor each of previous bug reports. Then, this measure is nor-malized as follows:
H
3=Cbr
Size(S)(5)3.3.3 Change operators
In a search algorithm, the variation operators play the
key role of moving within the search space with the aim ofdriving the search towards better solutions. We used theprinciple of the Roulette wheel [12] to select individuals for
mutation and crossover. The probability to select an indi-
vidual for crossover and mutation is directly proportional toits relative ﬁtness in the population. In each iteration, weselect half of the population in iteration i. These selected in-dividuals will give birth to another half of the population of
new individuals in iteration i+1 using a crossover operator.Therefore, two parent individuals are selected, and a few di-
mensions (recommended classes) picked on each one. Theone point crossover operator allows creating two oﬀspring
P
/prime
1andP/prime
2from the two selected parents P1andP2.I ti sd e -
ﬁnedasfollows: arandomposition, k, isselected. Theﬁrstk
classes of P1become the ﬁrst kelements of P/prime
1. Similarly, the
ﬁrstkoperations of P2become the ﬁrst koperations of P/prime
2.
Our crossover operator could create a child that contains
redundant recommended classes. In order to resolve this
problem, for each obtained child, we verify whether thereare redundant classes or not. In case of redundancy, we re-place the redundant classes by randomly chosen ones from
the system without causing another redundancy.
The mutation operator can be applied to pairs of dimen-
sions of the vector selected randomly. Given a selected so-lution, the mutation operator ﬁrst randomly selects one ormany pairs of dimensions of the vector. Then, for each se-
lected pair, the dimensions, which correspond to classes, are
deleted or replaced by new classes. We used the same repairoperator, described previously, to eliminate redundancy.
4. EV ALUATION
In order to evaluate our approach for recommending rel-
evant classes to inspect for bug reports, we conducted a set
of experiments based on diﬀerent versions of 6 open source
systems. Each experiment is repeated 30 times, and the ob-tained results are subsequently statistically analyzed withthe aim to compare our NSGA-II proposal with a variety
of existing approaches not based on heuristic search [26, 16,
28] and a mono-objective formulation. In this section, wepresent our research questions and then
4.1 Research Questions
In our study, we assess the performance of our approach
by ﬁnding out whether it could identify the most relevant
classes to inspect for bug reports. Our study aims at ad-
dressing the following research questions outlined below. Wealso explain how our experiments are designed to addressthese questions. The main question to answer is to what ex-tent the proposed approach can propose meaningful bug lo-
calization solutions based on the description of a bug report.
To this end, we deﬁned the following research questions:
•RQ1.(Eﬃciency) To what extent can the proposed
approachidentifyrelevantclassestolocalizebugsbasedon bug reports description?
•RQ2.(Comparison to search techniques) How does
theproposedmulti-objectiveapproachbasedonNSGA-II perform compared to random search and a mono-
objective approach?
290•RQ3.(Comparison to state-of-the-art) How does our
approach perform compared to existing bugs localiza-
tion techniques not based on heuristic search?
To answer RQ1, we validate the proposed multi-objective
technique on six medium to large-size open-source systems,as detailed in the next section, to evaluate the correctness
of the recommended classes to inspect for a bug report. To
this end, we used the following evaluation metrics:
•Precision@k denotes the number of correct recom-
mended ﬁles in the top k of recommended ﬁles by the
solution divided by the minimum number of ﬁles to in-spect, in the ranked recommendations list, to localizethe bug.
•Recall@k denotesthenumberofcorrectrecommended
ﬁles in the top k of recommended ﬁles by the solutiondivided by the total number of expected ﬁles to be
recommended that contain the bug.
•Accuracy@k measures the percentage of bug reports
forwhichatleastonecorrectrecommendationwaspro-
vided in the top k ranked classes.
To answer RQ2, we compared, using the above metrics,
theperformanceofNSGA-IIwithrandomsearchandamono-
objective genetic algorithm aggregating all the objectivesinto one objective with equal weight. If Random Searchoutperforms a guided search method thus, we can concludethat our problem formulation is not adequate. It is impor-
tant also to determine if our objectives are conﬂicting and
outperforms a mono-objective technique. The comparisonbetween a multi-objective technique with a mono-objectiveone is not straightforward. The ﬁrst one returns a set of
non-dominated solutions while the second one returns a sin-
gle optimal solution. To this end, for we choose the nearestsolution to the Knee point [7] (i.e., the vector composed ofthe best objective values among the population members) asa candidate solution to be compared with the single solution
returned by the mono-objective algorithm.
ToanswerRQ3, wecomparedourmulti-objectiveapproach
todiﬀerentexistingtechniquesnotbasedonheuristicsearch:1. BugScout [16] identiﬁes relevant classes based on the use
of Latent Dirichlet Allocation measure [5]; 2. BugLocator
[28] ranks classes using both textual and structural simi-larity.3. Learning-to-rank (LR) [26] technique ranks classesusing a machine learning technique to learn from the his-tory of previous bug reports. In addition, we compared our
work with two additional baselines. The ﬁrst one is based
on the only use of the lexical measure (LS) to rank classesand the second one is based on the only use of the historymeasure (HS). These two baselines may justify or not the
need of considering complementary information from both
the lexical and history similarities in our multi-objective for-mulation.
In the next section, we describe the diﬀerent projects and
the 10-fold cross-validation used in our experiments.
4.2 Software Projects and Experimental Set-
ting
As described in Table 1, we used a benchmark datasets
for six open-source systems [26].
•Eclipse UI is the user interface of the Eclipse devel-
opment framework.•Tomcat implements several Java EE speciﬁcations.
•AspectJ is an aspect-oriented programming (AOP)
extension created for the Java programming language.
•Birtprovides reporting and business intelligence ca-
pabilities.
•SWTis a graphical widget toolkit.
•JDTprovides a set of tool plug-ins for Eclipse.
Table 1 shows the diﬀerent statistics of the analyzed sys-
temsincludingthetimerangeofthebugreports, thenumber
ofbugreports, thesize, thenumberofAPIs, andthenumber
of ﬁxed classes per bug report.
The total number of collected bug reports and associated
classes is more than 22,000 bug reports for the six opensource systems. All these projects are using BugZilla track-
ing system and GIT as a version control system. To avoid
using a ﬁxed code revision, we associated a before-ﬁxed ver-sion of the system to each bug report. Therefore, for eachbug report, the version of the software package just before
the ﬁx was committed was used in our validation.
Based on the collected data, we created two sets: one for
the training data and the other for the test data. The bugreports for each system were sorted chronologically based onthe time dimension. The sorted bug reports are then split
into 10 folds with equal sizes, where fold
1contains the most
recentbugreportsandthelastfoldfold 10containstheoldest
ones. In addition, the oldest fold is split into 70% training(history of bug reports) and 30% validation. The approach
is trained on fold
i+1a n dt e s t e do nf o l d i, for all i from 1 to
9. The best recommended solution is then compared withexpected solution of classes that contain the bug.
4.3 Parameters Tuning and statistical tests
Since metaheuristic algorithms are stochastic optimizers,
they can provide diﬀerent results for the same problem in-
stance from one run to another. For this reason, our experi-mental study is performed based on 30 independent simula-tion runs for each problem instance and the obtained results
are statistically analyzed by using the Friedman test with
a 95% conﬁdence level ( α= 5%) [1]. The Friedman test
is a non-parametric statistical test useful for multiple pair-wise comparisons. The latter veriﬁes the null hypothesisH
0that the obtained results of the diﬀerent algorithms are
samples from continuous distributions with equal medians,
as against the alternative that they are not, H 1. The p-value
of the Friedman test corresponds to the probability of reject-ing the null hypothesis H
0while it is true (type I error). A
p-value that is less than or equal to α(≤0.05) means that
we accept H 1and we reject H 0. However, a p-value that
is strictly greater than α(>0.05) means the opposite. In
this way, we could decide whether the superior performanceof NSGA-II to one of each of the other algorithms (or the
opposite) is statistically signiﬁcant or just a random result.
The Friedman test allows verifying whether the results
are statistically diﬀerent or not. However, it does not giveany idea about the diﬀerence in magnitude. To this end,
we used the Vargha and Delaneys A statistics which is a
non-parametric eﬀect size measure. In our context, giventhe diﬀerent performance metrics (such as Precision and Re-call), the A statistics measures the probability that runningan algorithm B1 (NSGA-II) yields better performance than
291Table 1: Studied Projects
Project # Bug reports Time #A P I# ﬁles in the project
(average per version)# ﬁxed ﬁles/classes per bug report
(median)
Eclipse UI 6495 10/2001-01/2014 1314 3454 2
Birt 4178 06/2005-12/2013 957 6841 1
JDT 6274 10/2001-01/2014 1329 8184 2
AspectJ 593 03/2002-01/2014 54 4439 2
Tomcat 1056 07/2002-01/2014 389 1552 1
SWT 4151 02/2002-01/2014 161 2056 3
running another algorithm B2 (such as GA). If the two al-
gorithms are equivalent, then A = 0.5.
An often-omitted aspect in metaheuristic search is the
tuning of algorithm parameters. In fact, parameter set-
ting inﬂuences signiﬁcantly the performance of a search al-
gorithm on a particular problem. For this reason, for eachsearch algorithm and each system, we performed a set ofexperiments using several population sizes: 10, 20, 30, 40and 50. The stopping criterion was set to 100,000 ﬁtness
evaluations for all search algorithms in order to ensure fair-ness of comparison. We used a high number of evaluations
as a stopping criterion since our approach requires involvesmultiple objectives. Each algorithm was executed 30 times
with each conﬁguration and then the comparison between
the conﬁgurations was performed based on diﬀerent metricsdescribed previously using the Friedman test. The otherparameters values were ﬁxed by trial and error and are asfollows: (1) crossover probability = 0.4; mutation probabil-
ity = 0.2 where the probability of gene modiﬁcation is 0.1.
4.4 Results
4.4.1 Results for RQ1
The results of Table 2 and Figures 6 to 8 conﬁrm the eﬃ-
ciency of our multi-objective approach to identify the most
relevantclassesforbugreportsthatincludethebugsonthe6
open source systems. Table 2 shows the average precision@kresults of our multi-objective technique on the diﬀerent sixsystems, with k ranging from 5 to 20. For example, most
of the recommended classes to inspect in the top 5 (k=5)
are relevant with a precision of 89%. The lowest precisionis around 70% for k=20 which is still could be consideredacceptable since most of the bug reports do not have manyclasses to inspect. In terms of recall, Table 2 conﬁrms that
the majority of the expected classes to recommend are lo-
cated in the top 20 (k=20) with an average recall score of94%. An average of more than 72% of classes recommendedin the top5 cover the expected buggy classes.
The average accuracy@k results on the diﬀerent six sys-
tems are described in Table 2 showing that an average of68%, 86%, 94% and 97% are achieved for k = 5, 10, 15and 20 respectively. These results conﬁrm that if we recom-mend only 10 classes to programmers, we can make correct
recommendations for 86% of the thousands of collected bug
reports for every system.
Figures 6 to 8 summarize the results of the precision@10,
recall@10 and accuracy@10 for every of the studied systems.
The obtained results clearly show that most of the buggy
classes were recommended correctly by our multi-objectiveapproach in the top 10 with a minimum precision of 78% forAspectJ, a minimum recall of 79% for Eclipse and a mini-mum accuracy of 82% for Eclipse as well. Thus, we noticed
that our technique does not have a bias towards the eval-Table 2: Median Precision@k, Recall@k and Accuracy@k on
30 independent runs. The results were statistically signiﬁ-cant on 51 independent runs using the Friedman test with a95% conﬁdence level ( α<5%).
k Precision@k
NSGA-IIBug
ScoutBug
LocatorLR LS HS RS GA
5 89 76 78 81 69 71 34 71
10 82 71 74 76 61 64 29 61
15 74 63 69 72 57 58 33 55
20 68 48 51 58 48 51 24 53
k Recall@k
NSGA-IIBug
ScoutBug
LocatorLR LS HS RS GA
5 72 59 62 64 54 56 27 54
10 81 64 67 72 60 62 31 62
15 87 69 72 79 65 67 26 69
20 94 74 80 83 70 72 24 76
k Accuracy@k
NSGA-IIBug
ScoutBug
LocatorLR LS HS RS GA
5 68 41 44 49 37 34 29 38
10 86 62 69 71 56 59 24 59
15 94 74 78 82 68 72 31 79
20 97 79 82 86 74 77 33 77
uated system. As described in Figures 6-8, in all systems,
we had almost similar average scores of precision, recall and
accuracy. All these results based on the diﬀerent measureswere statistically signiﬁcant on 30 independent runs usingthe Friedman test with a 95% conﬁdence level ( α<5%).
To answer RQ1, the obtained results on the six open
source systems using the diﬀerent evaluation metrics of pre-cision, recall and accuracy clearly validate the hypothesesthat our multi-objective approach can recommend eﬃcientlyrelevant buggy classes to inspect for each bug report.
4.4.2 Results for RQ2
Concerning RQ2, Table 2 and Figures 6 to 10 conﬁrm that
NSGA-II is better than random search and the three mono-
objective formulations (LS, HS and GA) based on the threemetrics of precision, recall and accuracy on all the 6 systems.Three mono-objective formulations were implemented:
1. with an equal aggregation of both objectives (GA);
2. a mono-objective algorithm with the only objective of
lexical similarity (LS); and
3. a mono-objective algorithm with the only objective of
history similarity (HS).
The average accuracy, precision and recall values of ran-
dom search (RS) on the six systems are lower than 35% as
292Figure 6: Average Precision@k of NSGA-II, BugScout, Bu-
gLocator, LR, LS, HS, RS and GA on the diﬀerent systemsfor 30 independent runs. .
Figure 7: Average Recall@k of NSGA-II, BugScout, BugLo-cator, LR, LS, HS, RS and GA on the diﬀerent systems for30 independent runs.
Figure 8: Average Accuracy@k of NSGA-II, BugScout, Bu-
gLocator, LR, LS, HS, RS and GA on the diﬀerent systems
for 30 independent runs.described in Table 2. This can be explained by the huge
search space to explore to identify the best order of classes
to inspect for bugs localization. The performance of the
three mono-objective algorithms was much better than ran-dom search but lower than our multi-objective formulation.The aggregation of both objectives into one objective gener-ates better results on all the six systems than the two other
algorithms considering each objective separately. Thus, an
interesting observation is the clear complementary betweenthe history-based similarity function and the lexical-basedmeasure. In fact, we found that the buggy classes that are
not detected by one of the two algorithms were identiﬁed
by the other algorithm. The average precision, recall andaccuracy of each of the two algorithms (LH and HS) wasbetween 67% and 72% but the aggregation of both objec-tives into one in our multi-objective formulation improve a
lot the obtained results. In addition, since NSGA-II out-
performs the mono-objective GA then it is clear that thetwo objectives of correctness/relevance and the number ofrecommended classes are conﬂicting.
All these results were statistically signiﬁcant on 30 inde-
pendent runs using the Friedman test with a 95% conﬁdencelevel (α<5%). We have also found the following results
of the Vargha Delaney A
{12}statistic : a) On large and
medium scale systems (Birt, JDT, Eclipse UI and AspectJ )
NSGA-II is better than all the other algorithms based on allthe performance metrics with an A eﬀect size higher than0.93; b) On small scale systems (Tomcat, SWT), NSGA-IIis better than all the other algorithms with a an A eﬀect size
higher than 0.96.
We conclude that there is empirical evidence that our
multi-objectiveformulationsurpassestheperformanceofran-dom search and mono-objective approaches thus our formu-
lation is adequate (this answers RQ2).
4.4.3 Results for RQ3
Sinceitisnotsuﬃcienttocompareourapproachwithonly
search-based algorithms, we compared the performance ofNSGA-II with three diﬀerent bugs localization techniques
not based on heuristic search [26, 16, 28]. Table 2 and
Figures 6 to 8 present the precision@k, recall@k and accu-racy@k results for the 3 implemented methods, with k rang-ing from 5 to 20. NSGA- II achieves better results, on av-erage, than the other three methods on all six projects. For
example, our approach achieved, on average, Precision@k
of 90%, 84%, 73% and 69% are achieved for k= 5, 10, 15and 20 respectively as described in Table 2. In compari-son, BugLocator achieved an average Precision@k of 68%.
BugScout and LR achieved an average Precision@k of 66%
and 72%, respectively. Similar observations are also validfor the recall@k and accuracy@k.
Based on the results of Figures 6 to 8 Birt and Tomcat
are two projects where LR performs close to the NSGA-II
approach. For many bug reports in Birt, most of the buggy
classes are those that have been frequently ﬁxed in previousbug reports which explain the relatively high performanceobtained by LR and NSGA-II. Since the bug ﬁxing informa-
tion is exploited by both the NSGA-II approach and LR, it
is expected that they obtain the best performance results.
To answer RQ3, the obtained results on the six open
source system using the diﬀerent evaluation metrics of pre-cision, recall and accuracy clearly validate the hypotheses
that our multi-objective approach outperforms several bugs
293Figure 9: Impact of the data training size (folds) on the
three metrics based on the JDT project.
Figure 10: Average execution time (in minutes) of NSGA-II, on the diﬀerent systems for 30 independent runs on thediﬀerent systems.
localization techniques not based on heuristic search.
5. DISCUSSIONS
Impact of Data Size. To evaluate the impact of in-
creasing the size of the data used (history of previous bug
reports and changes), we executed a scenario on the JDTproject in which we increased the size of the dataset in-crementally fold by fold until we include all the 9 folds in
the dataset. It is clear from Figure 9 that for all the three
metrics of Precision@k, Recall@k and Accuracy@k that in-creasing the size of the previous bug reports do not improveall the three metrics. This can be explained by the fact thatrecent bug reports and history of changes are the most im-
portant part of the data. The obtained results conﬁrm also
that our multi-objective approach did not require a large setof data to generate good results in terms of ﬁnding possiblebuggy classes for bug reports.
Execution time. We executed our multi-objective algo-
rithm on a desktop computer with CPU Intel(R) Core(TM)i7 3.2 GHz and 20G RAM. Figure 10 presents the executiontime performance of our approach. The average executiontime on the diﬀerent systems was around 18 minutes. The
highest execution time was observed on the JDT system
with 23 minutes and the lowest one was around 11 minutesfor SWT. We believe that the execution is reasonable sincebugs localization is not a real-time problem. We also found
that the execution time is related to the number of ﬁles to
parse and the history of bug reports.6. THREATS TO V ALIDITY
We explore, in this section, the factors that can bias our
empirical study. These factors can be classiﬁed in three cat-egories: construct internal and external validity. Constructvalidityconcernstherelationbetweenthetheoryandtheob-servation. Internal validity concerns possible bias with the
results obtained by our proposal. Finally, external validity
is related to the generalization of observed results outsidethe sample instances used in the experiment.
In our experiments, construct validity threats are related
to the absence of similar work that uses search-based tech-niques for bug’s localization. For that reason, we compared
our proposal with diﬀerent mono-objective formulations tocheck the need for a multi-objective approach. A constructthreat can also be related to the corpus of manually local-
ized bugs for every bug report. A limitation related to our
experiments is the diﬃculty to set the thresholds for some ofthe parameters of Bug Locator. In fact, we used the defaultthresholds used by the authors that can have an impact on
the quality of the generated results.
We take into consideration the internal threats to valid-
ity in the use of stochastic algorithms since our experimentalstudy is performed based on 30 independent simulation runsfor each problem instance, and the obtained results are sta-
tistically analyzed by using the statistical test with a 95%
conﬁdence level ( α= 5%). The parameter tuning of the
diﬀerent optimization algorithms used in our experimentscreates another internal threat that we need to evaluate in
our future work by additional experiments to evaluate the
impact of the parameters on the quality of the results.
External validity refers to the generalization of our ﬁnd-
ings. In this study, we performed our experiments on sixdiﬀerent widely-used open-source systems belonging to dif-
ferent domains and with diﬀerent sizes. However, we cannot
assert that our results can be generalized to other applica-tions, other programming languages, and to other practi-tioners.
7. CONCLUSION AND FUTURE WORK
We propose, in this paper, an automated approach to lo-
calize and rank potential relevant classes for bug reports.Our approach ﬁnds a trade-oﬀ between minimizing the num-ber of recommended classes and maximizing the correctness
of the proposed solution using a multi-objective optimiza-
tion algorithm. The correctness of the recommended classesis estimated based on the use of the history of changes andbug-ﬁxing, and the lexical similarity between the bug report
description and the API documentation. We have executed
extensive empirical evaluations on 6 large open-source soft-ware projects with more than 22,000 bug reports in totalbased on an existing benchmark. The results on the before-ﬁx versions show that our system outperforms, on average,
three state-of-the-art approaches not based on search tech-
niques [16, 26, 28]. In particular, our search-based approachis able to successfully locate the true buggy methods withinthe top 10 recommendations for over 87% of the bug reports.
As part of our future work, we plan to evaluate our multi-
objective approach on further projects in other diﬀerent pro-gramming languages. In addition, we will extend our workto address the problem of the software bugs managementand prioritization using multi-objective search techniques.
2948. REFERENCES
[1] A. Arcuri and G. Fraser. Parameter tuning or default
values? an empirical investigation in search-based
[ 2 ]B .A s h o k ,J .J o y ,H .L i a n g ,S .K .R a j a m a n i ,
G. Srinivasa, and V. Vangala. Debugadvisor: arecommender system for debugging. In 7th joint
meeting of the European software engineeringconference and the ACM SIGSOFT symposium onThe foundations of software engineering ,p a g e s
373–382. ACM, 2009.
[3] N. Bettenburg, S. Just, A. Schr ¨o t e r ,C .W e i s s ,
R. Premraj, and T. Zimmermann. What makes a goodbug report? In 16th ACM SIGSOFT International
Symposium on Foundations of software engineering ,
pages 308–318. ACM, 2008.
[4] N. Bettenburg, R. Premraj, T. Zimmermann, and
S. Kim. Duplicate bug reports considered harmfulˆ a˘A
/arrowhooklefte
really? In International Conference on Software
maintenance (ICSM) , pages 337–345, 2008.
[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. Journal of machine Learning
research, 3(Jan):993–1022, 2003.
[6] B. Bruegge and A. H. Dutoit. Object-Oriented
Software Engineering Using UML, Patterns and
Java-(Required) . Prentice Hall, 2004.
[7] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. A
fast and elitist multiobjective genetic algorithm:Nsga-ii. IEEE transactions on evolutionary
computation , 6(2):182–197, 2002.
[ 8 ]D .D r e y t o n ,A .A .A r a ´ u j o ,A .D a n t a s , ´A. Freitas, and
J. Souza. Search-based bug report prioritization forkate editor bugs repository. In International
Symposium on Search Based Software Engineering ,
pages 295–300. Springer, 2015.
[9] S. T. Dumais. Latent semantic analysis. Annual review
of information science and technology , 38(1):188–230,
2004.
[10] E. Enslen, E. Hill, L. Pollock, and K. Vijay-Shanker.
Mining source code to automatically split identiﬁers
for software analysis. In International Working
Conference on Mining Software Repositories , pages
71–80. IEEE, 2009.
[11] M. Fischer, M. Pinzger, and H. Gall. Analyzing and
relating bug report data for feature tracking. In
WCRE, volume 3, page 90, 2003.
[12] D. E. Goldberg and K. Deb. A comparative analysis of
selection schemes used in genetic algorithms.
Foundations of genetic algorithms , 1:69–93, 1991.
[13] M. Harman and B. F. Jones. Search-based software
engineering. Information and software Technology ,
43(14):833–839, 2001.
[14] M. Harman, S. A. Mansouri, and Y. Zhang.
Search-based software engineering: Trends, techniques
and applications. ACM Computing Surveys (CSUR) ,
45(1):11, 2012.
[15] C. Henard, M. Papadakis, and Y. Le Traon.
Mutation-based generation of software product linetest conﬁgurations. In International Symposium on
Search Based Software Engineering , pages 92–106.
Springer, 2014.software engineering. Empirical Software Engineering ,
18(3):594–623, 2013.
[16] A. T. Nguyen, T. T. Nguyen, J. Al-Kofahi, H. V.
Nguyen, and T. N. Nguyen. A topic-based approachfor narrowing the search space of buggy ﬁles from abug report. In IEEE/ACM International Conference
on Automated Software Engineering (ASE) ,p a g e s
263–272, 2011.
[17] A. N´ u˜nez, M. G. Merayo, R. M. Hierons, and
M. N´u˜nez. Using genetic algorithms to generate test
sequences for complex timed systems. Soft Computing ,
17(2):301–315, 2013.
[18] S. Rao and A. Kak. Retrieval from software libraries
for bug localization: a comparative study of genericand composite text models. In 8th Working
Conference on Mining Software Repositories ,p a g e s
43–52, 2011.
[19] R. K. Saha, J. Lawall, S. Khurshid, and D. E. Perry.
On the eﬀectiveness of information retrieval based bug
localization for c programs. In ICSME, pages 161–170,
2014.
[20] R. K. Saha, M. Lease, S. Khurshid, and D. E. Perry.
Improving bug localization using structuredinformation retrieval. In IEEE/ACM International
Conference on Automated Software Engineering
(ASE), pages 345–355, 2013.
[21] G. Salton, A. Wong, and C.-S. Yang. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613–620, 1975.
[22] J. Shelburg, M. Kessentini, and D. R. Tauritz.
Regression testing for model transformations: A
multi-objective approach. In International Symposium
on Search Based Software Engineering , pages 209–223.
Springer, 2013.
[23] C. Sun, D. Lo, X. Wang, J. Jiang, and S.-C. Khoo. A
discriminative model approach for accurate duplicate
bug report retrieval. In 32nd ACM/IEEE
International Conference on SoftwareEngineering-Volume 1 , pages 45–54. ACM, 2010.
[24] P.-N. Tan et al. Introduction to data mining .P e a r s o n
Education India, 2006.
[25] C.-P. Wong, Y. Xiong, H. Zhang, D. Hao, L. Zhang,
and H. Mei. Boosting bug-report-oriented fault
localization with segmentation and stack-trace
analysis. In ICSME, pages 181–190. Citeseer, 2014.
[26] X. Ye, R. Bunescu, and C. Liu. Learning to rank
relevant ﬁles for bug reports using domain knowledge.In22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering , pages 689–699.
ACM, 2014.
[27] X. Ye, R. Bunescu, and C. Liu. Mapping bug reports
to relevant ﬁles: A ranking model, a ﬁne-grainedbenchmark, and feature evaluation. IEEE
Transactions on Software Engineering , 42(4):379–402,
2016.
[28] J. Zhou, H. Zhang, and D. Lo. Where should the bugs
be ﬁxed? more accurate information retrieval-based
bug localization based on bug reports. In 34th
International Conference on Software Engineering
(ICSE), pages 14–24. IEEE, 2012.
295