Online App Review Analysis for Identifying Emerging Issues
Cuiyun Gao, Jichuan Zeng, Michael R. Lyu, and Irwin King
Shenzhen Research Institute of The Chinese University of Hong Kong, China
The Chinese University of Hong Kong, China
{cygao,jczeng,lyu,king}@cse.cuhk.edu.hk
ABSTRACT
Detectingemergingissues( e.g.,newbugs)timelyandpreciselyis
crucialfordeveloperstoupdatetheirapps.Appreviewsprovidean
opportunity to proactively collect user complaints and promptly
improveapps’userexperience,intermsofbugfixingandfeature
refinement. However, the tremendous quantities of reviews and
noisewords( e.g.,misspelledwords)increasethedifficultiesinac-
curately identifying newly-appearing app issues. In this paper, we
propose a novel and automated framework IDEA, which aims to
IDentify Emerging App issues effectively based on online review
analysis.Weevaluate IDEAonsixpopularappsfrom Google Play
and Apple’s App Store, employing the official app changelogs as
our ground truth. Experiment results demonstrate the effective-ness of
IDEAin identifying emerging app issues. Feedback from
engineersandproductmanagersshowsthat88.9%ofthemthink
that the identified issues can facilitate app development in practice.
Moreover,wehavesuccessfullyapplied IDEAtoseveralproducts
of Tencent, which serve hundreds of millions of users.
CCS CONCEPTS
•Softwareanditsengineering →Dynamicanalysis ;•Informa-
tion systems →Web and social media search ;
KEYWORDS
App reviews, online analysis, emerging issues
ACM Reference format:
CuiyunGao,JichuanZeng,MichaelR.Lyu,andIrwinKing.2018.Online
App Review Analysis for Identifying Emerging Issues. In Proceedings of
ICSE’18:40thInternationalConferenceonSoftwareEngineering,Gothenburg,
Sweden, May 27-June 3, 2018 (ICSE ’18), 11 pages.
https://doi.org/10.1145/3180155.3180218
1 INTRODUCTION
Appdevelopersareeagertoknowwhatisgoingonwiththeirapps
afterpublished[38].Timelyandpreciselyidentifyingtheemerging
issues of apps is of great help for app developers to update their
apps, such as fixing bugs, refining existing features, and adding
new functions.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05 ...$15.00
https://doi.org/10.1145/3180155.3180218User reviews are direct feedback from the users who have expe-
riencedtheapps,andreflecttheinstantuserexperience[33].The
emergingissuesdetectedfromuserreviews,suchastheexisting
bugs (e.g., crashes) and unfavorable app features ( e.g., too many
ads)[13],canprovideinformativeevidenceforappdevelopersin
maintaining their apps and scheduling the app updates. For exam-
ple, Facebook Messenger received massive one-star ratings (the
lowest rating) in August, 2014, accounting for nearly 94% of all its
reviewsonApple’sAppStore1,andsufferedalargelossofusers[8],
since the version contained severe privacy issues ( e.g., accessing
the photosand contact numbersin users’ phones).However, such
issues had already been flushed out with complaints from over
12,600 user reviews on App Store one month ago. The situation
couldbeeffectivelyalleviatediftheemergingissuesweretimelydetected from user reviews. Therefore, user reviews provide an
effective and efficient way to identify the emerging issues of apps,
which would be a significant help to the developers.
Thecharacteristicsofuserreviewsmakeaccurateissuedetection
verychallenging.First,appreviewsaregeneratedeverydayinlarge
volume. Manual analysis is prohibitively time-consuming for apps
withlargenumbersofreviews( e.g.,Facebookreceivesmorethan
10,000 reviews in Google Play every day [2]). Second, app reviews
containnumerousnoisewords,suchasmisspelledwords,repetitive
words,andnon-Englishwords.Also,theyareoftenshorterinlength,
sincemostofthemarewrittenbyusersviamobileterminals.Third,
only 30% of the reviews provide informative user opinions for app
updates [6]. Furthermore, detailed and newly-appearing app issues
arehardtobepredefined,becausetheyarediversefordifferentapps
and versions. Previous research mainly focuses on reducing the
manualpowerinextractingsoftwareaspectsoruserpreferences,
such as establishing dictionaries for preprocessing reviews [42],filtering out non-informative reviews [6], or classifying reviews
to predefined topics [40]. However, effectively detecting emerging
issues from user reviews has rarely been studied.
Weproposeanovelandautomatedframework IDEAfordetecting
emergingissues/topics2basedononlinereviewanalysis. IDEAtakes
reviews of different versions as input. To track the topic variations
overversions,anovelmethodAOLDA(AdaptivelyOnlineLatent
DirichletAllocation)is employedforgeneratingversion-sensitive
topicdistributions.Theemergingtopicsarethenidentifiedbasedon
the typical anomaly detection method. To make the topics compre-
hensible, IDEAlabelseachtopicwiththemostrelevantphrasesand
sentencesbasedonaneffectiverankingschemeconsideringboth
semantic relevance and user sentiment. The prioritized topic labels
are the app issues identified. Finally, IDEAvisualizes the variations
ofappissuesalongwithversions,andhighlightstheemergingones
for better understanding.
1The App Store in this paper refers to Apple’s App Store.
2The topics and issues are semantically equal in this paper.
482018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. To verify the effectiveness of IDEA, we consider the official app
changelogs as ground truth, since they encompass the primary
changesofthereleasesandrepresenttheissuesconcernedbyde-
velopers.Ourexperimentsareconductedonsixpopularapps,with
twoof themfrom AppStore andtheothers fromGoogle Play.We
compare IDEAwith the method based on OLDA (Online Latent
DirichletAllocation)[1],oneclassicalmethodforemergingissue
detection. Results indicate that the average precision, recall, and
F-score of IDEAon the subject apps are 60.4%, 60.3%, and 58.5% re-
spectively, which increases the F-score of the OLDA-based method
by72.0%.WealsoconductausersurveyinTencent,indicatingthat
88.9% of respondents think that the identified issues of IDEAcan
facilitate app development in practice. Moreover, we apply IDEAto
four Tencent3products which serve hundreds of millions of users
worldwide, and confirm the effectiveness and efficiency of IDEAin
industrial practice.
The contributions of our paper are elaborated as below.
•We propose a framework called IDEAto automatically iden-
tifyemergingissuesfromappreviewseffectively.Also, IDEA
isanonlineanalysistoolandcanprocessnewappreviews
in a timely fashion.
•WeproposeanovelmethodcalledAOLDAforonlinereview
analysis,whichadaptivelycombinesthetopicsofprevious
versions to generate topic distributions of current versions.
•Wevisualizethevariationsofthecaptured(emerging)app
issues along with versions, with the emerging ones high-
lighted. We publish the code and review data on website4.
•Weverifytheeffectivenessof IDEAbasedontheappreviews
of six popular apps which are from different categories and
platforms.ThesurveyandapplicationinTencentalsovali-
date the performance of our framework in practice.
The remainder of the paper is organized as follows. Section 2
describes the motivation and the background of our work. Sec-
tion 3 outlines the overall picture and details each step involved in
theframework.Section4illustratesexperimentresults.Section5
presents the practical usage of our framework. Section 6 discusses
possible limitations, with related work introduced in Section 7.
Section 8 concludes the paper.
2 BACKGROUND AND MOTIVATION
2.1 Emerging App Issues
For an app issue to be considered an emerging issue, it must be
(heavily)discussedinthistimeslicebutnotpreviously[16].Figure1
(a) presents the issue distributions of Facebook Messenger in three
periods (March-April, May-June, and July-August), based on the
manually labelled 100 review samples from each period. Generally,
theissuedistributionsarenearlyconsistentalongwithperiods, e.g.,
from March-April to May-June in Figure 1 (a). However, emerging
issuescaninfluencetheissuedistributionofoneperiod,creating
significantdifferenceswiththoseofpreviousperiodsintermsof
proportion.Forexample,theproportionofthecrashissuepresentsa
hugeincreaseduringtheJuly-Augustperiod.Wefurtherinvestigate
the number of reviews containing the keyword “crash” along with
3The company has many popular products, such as WeChat, QQ, and Honor of Kings,
and serves billions of users worldwide.
4https://github.com/ReMine-Lab/IDEAtheir timing, and present the results in Figure 1 (b). The volume of
thecrashissueshowsasuddenincreasearoundJuly-August,which
signifiesthattheissuetendstobeanemergingissueduringthat
period.
Definition 2.1 (Emerging Issues in User Reviews). An issue in a
timesliceiscalledanemergingissueifitrarelyappearsinprevious
slicebutismentionedbyasignificantproportionofuserreviews
in current slice.
In Definition 2.1, the “time slice”, the degree of “rarely”, and
the“significantproportion”canbedefinedaccordingtodifferent
situations.Forexample,the“timeslice”inthispapercorrespondsto
the app version. Based on the detected emergingissues, developers
canlocatethebuggyfeaturesoftheirappsefficiently,updatethe
apps accordingly, and ultimately improve the user experience.
(a) Issue distribution in Facebook Messenger.
(b) The number of reviews containing the keyword “crash”.
Figure 1: Illustration of emerging issues.
2.2 Online Review Analysis
Online review analysis (ORA) is an automated way to acquire and
processuserreviewsinrealtimeasreviewsarearrivedcontinuously.
AsshowninFigure2,ORAtakesthereviewsofslice t(currentreview
slice)asinput,andoutputsanalysisresults,suchastrackinguser
preferenceanddetectingemergingissues.Inthisway,theurgent
user concernsincarnated byapp reviewscan becaptured byORA
inatimelymannerandfedbacktodevelopersforinstantbugfixing
orfeatureimprovement.Thus,ORAisacrucialcomponentinthe
closed cycle of app development.
User ReviewsOnline Review Analysis
App UpdateCurrent Review SliceSlice (t-3) Slice (t-2) Slice (t-1) Slice t
Review Stream…
 Emerging Issues
 Rating Changes Bug Tracking
…
Figure 2: Closed cycle for app development.
2
49
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. Currently,mostoftheappissuesminedfromuserreviewsare
manuallysettledordefined[24,40,42],suchasprivacyandGUI,
which are usually general categories. Although such definition
facilitates the process of task assignment to individuals, it is un-
favorablefordetectingnewly-presentedandmoredetailedissues
(e.g., notification center). Thus, for detecting emerging issues, ORA
isa practicalwaydueto itstimelinessandno needforpredefined
issues, which has rarely been studied previously.
2.3 App Changelogs
App changelogs describe the noticeable modifications of the latest
versions for attracting users to install and experience new releases.
Similar to user reviews. changelogs also correspond to specific ver-
sions.Generally,developerswriteintothechangelogswithinfor-
mationrelatedtowhethertheappsareaddingorremovingfeatures,
and whether the apps have made improvements with certain de-
vices or to specific bugs. Figure 3 illustrates a sample changelog of
NOAA Radar Pro, a weather alerts & forecast app in App Store.
What's New in Version 3.16
-Introducing weather reporting. The app now allows anyone to be 
a weather reporter. Confirm the w eather or report your weather 
conditions and take part in impr oving our data and forecasts.
- Performance improvements you won't necessarily notice but definitely enhancing your experience with the app.
Figure 3: Changelog of NOAA Radar Pro. The rectangles
highlighttwokeytermswhichrepresentthemajorchanges
of Version 3.16.
As Figure 3 indicates, the new version introduces new function-
ality(i.e.,weatherreporting)andrefinesperformanceissues.The
delivered changes exhibit the issues that are concerned by develop-
ers.Althoughthechangelogsmaynotcoverallthemodifications
to the releases, they represent a lower bound and the prominent
part of the changes. Hence, changelog is a reasonable ground truth
for verifying whether the extracted emerging issues are helpful for
developers.
3 METHODOLOGY
Figure 4: Framework of IDEA.
In this section, we first outline the overall framework of IDEA
in Figure 4 and then elaborate on the four components involved
in the framework. Each time, in the first stage (Part A in Figure 4),
IDEApreprocessesaversionofrawreviewsfromthereviewstream
for reducing noisy words and non-informative words, and extracts
phrasesforsubsequentanalysis(Section3.1).Inthesecondstage(PartBinFigure4),theproposedalgorithmAOLDAcapturesthe
topic distributions of each version by considering the topics in
previousversions,basedonwhichemergingtopicsareidentified
usinganomalydiscovery(Section3.2).Then,tointerpretthetopics
(Part C in Figure 4), IDEAemploys the meaningful phrases and sen-
tences as candidates to label each topic according to their semantic
relevance and user sentiment (Section 3.3). The topic labels are the
identifiedappissues.Finally(PartDinFigure4), IDEAvisualizes
the appissues along with thedifferent versions, andhighlight theemerging ones for better understanding (Section 3.4).
3.1 Preprocessing
Since app reviews are generally submitted via mobile terminals
andwrittenusinglimitedkeyboards,theycontainmassivenoisy
words, such as casual words, repetitive words, misspelled words,
andnon-informativewords( e.g.,thewordssimplydescribingusers’
feelings).Inthefollowing,weintroduceourrule-basedmethodsfor
formatting words, the phrase extraction process, and our filtering
method for reducing non-informative words.
3.1.1 Word Formatting. We first convert all the words in the
review collection into lowercase, and then stem each word intoits original form. We employ the preprocessing method in [26]
for lemmatization. We then replace all digits with “<digit>”. Since
newtermsandcasualwordswouldcontinuouslyincreaseinuser
reviews, we do not employ the dictionaries provided by [42] for
avoidingover correction.We adopttherule-based methodsbased
on[42, 26]torectify repetitivewords,misspelled words,andnon-
English words.
3.1.2 Phrase Extraction. Since phrases (mainly referring to two
consecutivewordsinourpaper)areemployedinPartCof IDEAfor
interpretingtopics,theyshouldbeextractedinthepreprocessing
step and trained along with all the other words in Part B. In this
way, we can capture the semantics of each phrase, based on which
wecanlabelthetopicswiththemostrelevantphrases.Sincethe
topiclabelsinphrasesshouldbemeaningfulandcomprehensible,
we use a typical phrase extraction method based on PMI (Point-
wise Mutual Information) [35], which is effective in identifying
meaningful phrases based on co-occurrence frequencies:
PMI(wi,wj) = logp(wiwj)
p(wi)p(wj), (1)
wherep(wiwj)indicatestheco-occurrenceprobabilityofthephrase
wiwjandp(wi)( o rp(wj)) represents the probability of the word
wi(orwj) in the whole review collection. Higher PMI values ex-
hibitthatthecombinationofthetwowordsismorelikelytobea
meaningful phrase. We extract the meaningful phrases by exper-
imentally set a threshold for PMI. The phrases with PMIs larger
than the threshold are extracted.
3.1.3 Filtering. Thefilteringstepaimstoreducethenon-informative
words,suchasemotionalwords( e.g.,“bad”and“nice”),abbrevia-
tions(e.g.,“asap”),anduselesswords( e.g.,someone).Non-informative
words are summarized by two researchers from 1,000 reviews,
which are also referred to as predefined stop words . The box be-
low lists 18 of the total 78 non-informative words due to space
limitations. The predefined stop words are filtered out together with
3
50
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. the stop words provided by NLTK [34]. We do not employ the
supervisedmethodin[6]forfiltering,sinceinthisworklabeling
massivenon-informativereviewsrequiresagreatdealofmanual
effort.Finally,alltheremainingwordsandextractedphrases(where
thewordsineachphraseareconnectedwith“_”)arefedintothe
next step for emerging topic detection.
Predefined Stop Words: cool, fine, hello, alright, poor, plz, pls, thank,
old, new, asap, someone, love, like, bit, annoying, beautiful, dear.
3.2 Emerging Topic Detection
In this section, we aim to detect the emerging topics of current
versions by considering the topics in previous versions. We first
introducetheproposedmethodAOLDAforadaptivelyonlinetopic
modeling, from which we capture the topic evolutionsalong with
versions. We then present how we discover the emerging topics
(e.g., anomaly topics).
3.2.1 AOLDA - Adaptively Online Latent Dirichlet Allocation.
Online Latent Dirichlet Allocation (OLDA) [1] is a classic method
for tracking the topic variations of text streams, which models the
topics of texts in one time slice based on the topics of the last slice.
However,appreviewsaretypicallyshortandcontainmassivenoise
words.Suchreviewfeaturescaninfluencethetopicdistributionsin
consecutive versions with OLDA, and thereby decrease the perfor-
manceofemergingtopicdetection.Toreducetheinfluenceofnoise
words and more accurately capture the topic evolution along with
versions, we propose an adaptively online topic modeling method,
AOLDA.TheproposedAOLDAimprovesOLDAbyadaptivelycom-
bining the topic distributions in previous versions. The details are
described below.
The preprocessed reviews are divided by version, denoted as
R={R1,R2, ...,Rt, ...}(wheretindicates the t-th version), and
inputintoAOLDAonebyone.InAOLDA,eachreviewistreated
as one document. The prior distributions over document-topic and
topic-word distributions are defined initially, represented as αand
βrespectively. βdetermines the topic distributions of the terms in
theinput.Thenumberofthetopicsisspecifiedas K.Forthek-th
topic,ϕt
kis the probability distribution vector over all the input
terms. We introduce the parameter - window size w, which defines
the number of previous versions to be considered for analyzing
the topic distributions of the current version. The overview of the
model AOLDA is depicted in Figure 5.
Different from OLDA, as Figure 5 shown, we adaptively inte-
grate the topic distributions of the previous wversions, denoted
as{ϕt−1, ...,ϕt−i, ...,ϕt−w}, for generating the prior βtof thet-th
version.The adaptiveintegrationreferstosummingupthetopic
distributions of different versions with different weights γt,i:
βt
k=w/summationdisplay
i=1γt,i
kϕt−i
k, (2)
whereidenotesthe i-thpreviousversion(1 ≤i≤w).Theweight
γt,i
kisdeterminedbythesimilarityofthe k-thtopicbetweenthe
(t−i)-th version and the ( t−1)-th version, which is calculated by
the softmax function [39]:
Figure5:OverviewofAOLDA.Theredrectanglewithdasheddots highlights the adaptive integration of the topics of the
wpreviousversionsforgeneratingtheprior βinthet-thver-
sion.R
tis the review corpus in the t-th version. The dotted
linesindicatethatwesimplifytheoriginalLDA[4]stepsforclearness.
γt,i
k=exp(ϕt−i
k·βt−1
k)
/summationtextw
j=1exp(ϕt−j
k·βt−1
k), (3)
wherethedotproduct( ϕt−i
k·βt−1
k)computesthesimilaritybetween
thetopicdistribution ϕt−i
kandthepriorofthe( t−1)-thversion βt−1
k.
Such adaptive integration can endow the topics of the previous
versionswithdifferentcontributionstothetopicdistributionsof
the current version.
3.2.2 AnomalyDiscovery. Basedonthecapturedtopicevolution
by AOLDA, we identify the anomaly topics which present obvious
differenceswiththoseofthepreviousversions.Theidentifiedanom-
alytopics are regarded asemergingtopics. To obtainthedifference
ofthek-thtopicsbetweentwoconsecutiveversions, e.g.,ϕt
kand
ϕt−1
k,weemploytheclassicJensen-Shannon(JS)divergence[19].
JS divergence measures the similarity between the two probability
distributions:
DJS(ϕt
k||ϕt−1
k)=1
2DKL(ϕt
k||M)+1
2DKL(ϕt−1
k||M),(4)
whereM=1
2(ϕt
k+ϕt−1
k). The Kullback-Leibler (KL) divergence
DKLis utilized to measure the discrimination from one probability
distribution Pto another Q, computed by:
DKL(P||Q)=/summationdisplay
iP(i)log(P(i)/Q(i)), (5)
whereP(i) is thei-th item in P. Higher JS divergence indicates that
the two topic distributions have a larger difference.
Basedonthecomputeddivergences DJSbetweenthetopicsof
consecutiveversions,wecaptureanomalytopicsbyleveraginga
typical outlier detection method [37]. The method assumes that
thedivergences followa Gaussiandistribution withthemean and
variance at μandσ2respectively. The anomaly topics are then
detectedbysettingathreshold δ.Forthet-thversion,thethreshold
δtis dynamically defined according to the following steps.
4
51
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. 1.Wecompute DJSoftheprevious wversionsforeach topic,
which generatesa w×Kmatrix (where Kis thenumber of
topics).
2.We compute the mean μand variance σ2of all the values in
the computed DJSmatrix.
3.Wesetthethreshold δtasδt=μ+1.25σ,wherethecoeffi-
cient 1 .255is experimentally set for accepting 10% of topics
as anomaly topics, as shown in Figure 6.
Figure 6: Gaussian distribution for anomaly discovery. The
shadedareameanstheintegraloftheGaussiandistribution,
whichequals90%.Thetopicswithdivergencelargerthan δt
are considered as emerging topics.
Forthet-thversion,thetopicswithdivergenceshigherthanthe
defined threshold δtare regarded as emerging topics.
3.3 Topic Interpretation
The topics based on AOLDA are represented as the probability dis-
tributions over all the input terms. One snapshot of the top five
terms to each topic is illustrated in Table 1. By only observing afew words, it would be non-trivial for developers to capture the
meaningofthetopics.Inthissection,weaimtointerpretthetopics
automatically.Tointerpreteachtopic,wecanutilizewords,phrases,
sentences, or entire reviews. However, single words may be am-
biguous in semantics and cannot display the complete meaningsof the topic. For example, we list the top five relevant words for
eachofthefourtopicsofYouTube,asshowninTable1,although
boththewords“video”and“work”aremostrelevanttoTopics2
and 4, these two topics may deliver different meanings, e.g., Topic
2isrelatedtothevideodescriptionsandTopic4isaboutloading
videos.Moreover,onereviewmaycomplainaboutseveralissues.
Forexample,oneInstagramusercomplainsaboutthevideosand
stories in one review: Videos don’t post. Videos don’t load. Stories
disappearallthetime .Therefore,topiclabelsinwordsorreviews
may not be helpful in accurately capturing the semantics of the
topics.Torenderthetopicscomprehensible,weemploythemost
relevant phrases and sentences to label each topic in this section.
3.3.1 CandidateExtraction. Weobtaincandidatephrasesand
sentences for labeling topics.
PhraseCandidate: Thecandidatesofthephraselabelsaregen-
eratedbasedontheextractedphrasesinSection3.1. Three rules
are employed to identify more meaningful phrases: 1) Length limit:
Thelengthofeachwordinthephraseshouldbenolessthanthree;
2)Stopwordlimit:Thephraseshouldnotcontainwordsthatarein
thestopwordlistofNLTK[34]; and3)Part-Of-S peechlimit:The
5Thecoefficientcanbeadjustedaccordingtothepercentageofanomalytopicstobe
discovered. We use 1.25here for accepting 10% of the total topics as anomalies.Table 1: Top five terms for each topic of YouTube.
Topic Topic 1 Topic 2 Topic 3 Topic 4
Termcomment link back load
say video also video
reply open button even
try work change work
error description go back take
phrase should include at least one noun or verb, and no adverbs
(e.g., “here”) or determiners ( e.g., “the”).
SentenceCandidate: We employthe reviews before thefilter-
ingstepinSection3.1,startingbychunkingthemintosentences
based on NLTK’s punkt tokenizer [36]. Then we retrieve sentences
withmorethanfourwords,duringwhichthenoisysentences(such
assofarsobad andgreatone)arefilteredout.Theremainingsen-
tences are regarded as our sentence candidates.
3.3.2 Topic Labeling. The topic labeling method is a ranking
method,whichconsiderstwoaspects:thesemanticsimilaritybe-
tweenthecandidatesandthetopics,andalsotheusersentimentof
the candidates.
SemanticScore: Goodtopiclabelsshouldcoverthelatentmean-
ing of the topic [30]. The semantic score measures the semanticsimilarity between the candidate and the topic. Moreover, the la-
bels of different topics should be discriminative and cover different
aspects of input reviews, instead of delivering overlapping infor-mation. Hence, the semantic score of one candidate involves the
semanticsimilaritytothetargettopicandalsothesemanticsimilar-
ities to all the other topics. A good topic label should be similar to
thetargettopicandalsodifferentfromtheothertopicsinsemantics.
Weemploythemethodin[30]tomeasurethesemanticsimilarity
betweenonephrasecandidate aandthetargettopic ϕt
k,definedas:
sim(a,ϕt
k)=−DKL(a||ϕt
k)
≈/summationdisplay
wp(w|ϕt
k)logp(a,w|C)
p(a|C)p(w|C),(6)
wherep(w|ϕt
k) is the probability of term win the topic distribution
ϕt
k.p(w|C)andp(a|C)denotethepercentagesoftheterms wandain
the whole review collection C, respectively. The p(a,w|C) indicates
the co-occurrence probability of the two terms aandwin the
collection C. For the sentence candidates s, we utilize Equation (7)
to calculate the similarity.
sim(s,ϕt
k)=−DKL(s||ϕt
k)
≈/summationdisplay
wp(w|ϕt
k)logp(w|s)/len(s)
p(w|ϕt
k),(7)
wherep(w|s)canbecalculatedbythetermfrequencyof winthesen-
tences.Thesemanticscoreisthendefinedbycombining sim(l,ϕt
k)
with the similarity scores to other topics/summationtext
j/negationslash=ksim(l,ϕt
j), which
means the label lshould be semantic close to the topic distribution
ϕt
kand discriminate from other topic distributions.
Scoresem(l,ϕt
k)=sim(l,ϕt
k)−μ
K−1/summationdisplay
j/negationslash=ksim(l,ϕt
j),(8)
5
52
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. wherelcan be a phrase candidate aor sentence candidate s, and
Kis the number of topics. The parameter μis utilized to adjust
the penalty for the semantic similarities to other topics. Larger
μsignifies that the candidates that are more different from other
topics.
Sentiment Score: The topic labels should reflect users’ con-
cerns. Generally, the reviews with low ratings tend to express poor
user experience and app issues [6], and the reviews with longerlengths are more likely to provide valuable information to devel-
opers. Therefore, we compute the sentiment score Scoresenof one
candidate lby combining the user ratings and review lengths:
Scoresen(l)=e x p (rl
log(hl)), (9)
wherelcanbeaphrasecandidateorsentencecandidate. randh
denote the average user rating and the average word length of the
reviews containing l, respectively.
OverallScore: Weprioritizethecandidatesforeachtopicbased
on their semantic scores and sentiment scores. The overall score
Score(l,ϕt
k) is defined as:
Score(l,ϕt
k)=Scoresem(l,ϕt
k)+λScoresen(l),(10)
wheretheweight λistobalancethetwoaspects.Inthismanner,all
the topics including the detected emerging topics are labeled with
the prioritized candidates. The topic labels are the identified app
issues.Foreachtopic,thereisatradeoffbetweenthenumberof
prioritizedlabelsandthecostofusercomprehension( e.g.,toomany
labelsusuallyspendusersmoretimeinunderstandingthemeaning
of the topic). According to the survey [43], three labels are the
moderate choice for users to comprehend the topics. Therefore, for
onetopic,wechoosethe threemostrelevantphrasesandsentences
respectively as labels for each topic.
3.4 Visualization
Inthis part,we visualizethe theevolutionof appissues ( i.e.,topic
labels)alongwithversionsforbetterunderstanding.Weemployan
issueriver todisplayissuevariations.Figure7presentsoneexample
ofYouTubeforiOS.Alltheappissuesconstituteoneriverandeach
branch of the river indicates one topic. By moving the mouse over
onetopic,onecantrackdetailedissuechangesalongwithversions,
where the emerging issues are highlighted as shown in Figure 7.
Theappissueswithwiderbranchesareofgreaterconcerntousers,
wherethe widthofthek-thbranchinthe t-thversionisdefinedas:
widtht
k=/summationdisplay
alogCount(a)×Scoresen(a), (11)
whereCount(a) is the count of the phrase label ain the review
collectionofthe t-thversion,and Scoresen(a)denotesthesentiment
score of the label a.
4 EXPERIMENTATION
We evaluate the performance of IDEAin identifying emerging app
issues based on case studies. In this section, we explain how we
selectthesubjectappsforexperiments,theperformancemetrics,
andfinallythecomparisonresultsofdifferentmethods.Wefocus
on answering the following three research questions.
Figure7:IssueRiverofYouTubeforiOS.Thenumberoftop-
icsKis set as 10, corresponding to 10 branches of the river.
The horizontal axis represents the app versions, and the
brancheswithlargerwidthsindicatethatthecorrespondingissues are more cared about by users at those versions.
RQ1:
What is the performance of IDEAin identifying emerging
app issues?
RQ2:Can IDEAachievebetter performance comparedwithother
methods?
RQ3:Howdodifferentparametersettingsimpacttheperformance
ofIDEA?
4.1 Dataset
Weselectthesubjectappsbasedonthefollowingfourcriteria:The
apps are i) popular apps in the app markets - indicating that the
developers would update their apps regularly and the user reviews
can be collected from several consecutive versions; ii) apps from
different categories and platforms - to ensure the generalizationof the proposed framework; iii) apps with enough user reviews -
whichnecessitatesanautomatedanalysis;andiv)appswithdetailed
changelogs for most versions - to facilitate our validation process.
To obtain apps that satisfy the first three criteria, we randomly
inspect the apps ranked in the top 100 on either App Store or
GooglePlayaccordingtoAppAnnie[2],anappanalyticsplatform.
Onlytheappswithmorethan2,000USreviews[6]areinspected
further,sincesignificanteffortisrequiredformanualanalysis.To
filterouttheappsthatdonotmeetthefourthcriterion,wecheck
the historical changelogs of these apps. We eliminate apps with
more thanfive successive sketchychangelogs, i.e., thechangelogs
provide no details related to what functionality had been changed
or how the user experience was being affected. One example of
sketchychangelogsis“Multiplebugfixesandimprovementsacross
the entire app”, where the bugs and improvement are not concrete
enough for verifying prioritized app issues. Finally, we select six
subject apps, with the details illustrated in Table 2.
In Table 2, we list the subject apps with the app name, category,
platform,thenumberofreviewscrawled,andthenumberofver-
sionsinthereviewcollection.Overall,weobtain164,026reviews
(from August 2016 to April 2017) for the six apps, from 89 versions
intotal.Theappsaredistributedindifferentcategories,withtwo
6
53
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. Table 2: Subject apps.
App Name Category Platform #Reviews #Versions
NOAA Radar Weather App Store 8,363 16
YouTube Multimedia App Store 37,718 33
Viber Communication Google Play 17,126 8
Clean Master Tools Google Play 44,327 7
Ebay Shopping Google Play 35,483 9
Swiftkey Productivity Google Play 21,009 16
of them from App Store and the others from Google Play. With
multiplecategoriesandplatforms,thegeneralizationof IDEAcan
be ensured.
4.2 Performance Metrics
The app changelogs, i.e., our ground truth, are collected from App
Annie. Since the prioritized issues of IDEAare in phrases and sen-
tences,wemanuallyextractkeytermsfromthesechangelogsfor
verification. One example is illustrated in Table 6, with the key
terms highlighted. For each key term in changelogs, we validate
whether the term is covered by the prioritized issues. Since the
word2vec model[31]canaccuratelycapturethesemanticmeanings
of input terms based on their vector representations, we obtain
the cosine similarities between each key term and the phrase-level
issuesbasedonthemodel.Thekeytermisconsideredcoveredifits
similarity to one of the issues is larger than 0.6 [18]. For sentence-
level issues, we split the sentences into terms (including phrases
and words) andverify whether the key term inchangelogs can be
coveredinasimilarway.Weemploysuchsemi-automaticevalu-
ationmethodtofacilitateparameteradjustmentandcomparison
with other methods.
Weemploythreeperformancemetrics6forverifyingtheeffec-
tiveness of IDEA. The first metric is for measuring the accuracy
in detecting emerging issues, defined as Precision E. The second
is to evaluate whether our prioritized app issues (including both
emerging and non-emerging issues) reflect the changes mentioned
inthechangelogs,definedas RecallL.Weintroducethethirdmetric
Fhybridto measure the balance between Precision EandRecallL.
Higher values of Fhybridindicate that changelogs are more pre-
ciselycoveredbydetectedemergingissuesandmorechangelogs
are reflected in the prioritized issues.
Precision E=I(E∩G)
I(E),RecallL=I(L∩G)
I(G),
Fhybrid=2×Precision E×RecallL
Precision E+RecalL.(12)
whereE,G, andLare three sets, containing the detected emerging
issues,thekeytermsinthechangelogs,andallappissues(including
both emerging and non-emerging issues), respectively. I(·) denotes
thenumberoftheissuesin ·.Duringevaluation,weexperimentally
set the parameters as w=3 ,K=1 0 ,λ=0.5,PMI= 5, andμ=0.75.
We also initialize αandβwith 0.1 and 0.01 respectively.
6We do not involve Recall Efor validation since changelogs possibly include partial
emergingissues.Also, Precision Lcannotbeconsideredbecausechangelogsmay
cover items other than user-concerned issues. Here, Precision EandRecall Lmea-
suretheprecisionoftheemergingissuesandcoveragerateofchangelogsbyallthe
extracted issues respectively, which are consistent with the standards and convincing
for this task.Table 3: Topic-word distributions based on AOLDA.
v11.07 v11.10 v11.11
Topic 1link open video
open video watch
video work fine go
work go want
description click change
Topic 2make <digit> back
want thing make
button get would
back interfacebutton
use want people
4.3 Result of RQ1: Case Study
In this part, we evaluate the performance of IDEAby employing a
case study on YouTube. We first present the results of the version-
sensitive topic distributions based on AOLDA, then exhibit the
prioritized labels to interpret the topics, and finally illustrate the
performance of the proposed framework on YouTube.
4.3.1 ResultofAOLDA. Table3depictstheexampletopic-word
distributions based on AOLDA, where the top five words are listed
for each topic. According to the table, the general meanings of the
topicsareconsistentalongwithversions.Forexample,Topic1isre-
latedtothevideoforallthethreeversions,andTopic2isconstantly
related to the user interface. Howev er, for one topic, the specific
meanings may be distinguished in the three versions. Take Topic 1
asanexample.Thetopicmaydiscussthevideodescription/linkfor
version11.07,whileittalksabout“click”-relatedthingsinversion
11.10. It would be very laborious for developers to comprehend
topics based on the top words. Therefore, we conduct automatic
topic interpretation in the next step.
4.3.2 ResultofTopicInterpretation. Table4illustratestheprior-
itizedphrasesforlabelingtopics,whereonlyoneofthethreelabels
arelistedforsavingspace.ThehighlightedlabelsinTable4arethe
emerging app issues detected by the anomaly discovery method in
Section 3.2.2. Topic 1 of version 11.07 is interpreted as “description
box”, which is consistent with the meaning of that topic in Table 3
intuitively. Table 5 illustrates the ranked sentence for labeling each
topic. Although phrase labels can be quickly understood, we dis-
cover that sentence labels can detail the information conveyed by
phrases and interpret the topics more comprehensively. For exam-
ple,thesentencelabelofTopic1forversion11.07( i.e.,“...clickalink
in the description...”) provides more details than the corresponding
phrase label (“description box”) in Table 4. With both issues in
phrasesandsentences,developerscanefficientlyspotandlocate
specificapp issues.Tohelpdevelopers gainbetter understanding,
wevisualizetheidentifiedissuesalongwithversionsinFigure7.
By moving the mouse over Topic 10 of version 11.15, we can ob-serve both phrase-level and sentence-level issues, among whichthe emerging ones are highlighted. Developers can readily trackthe changes of each topic and discover urgent issues in a timely
manner.
4.3.3 PerformanceEvaluation. Wecollectthegroundtruthof
YouTube based on the method in Section 4.2. Table 6 displays part
of the changelogs. We manually inspect whether the identified
app issues of one version can be reflected in the changelogs of the
next version. Accordingto Table 6,version 11.10improves theuser
7
54
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. Table 4: Topic labels in phrases for YouTube. The high-
lighted ones indicate detected emerging issues. The valueafter each label is the overall score of the label.
v11.07 v11.10 v11.11
Topic 1 descriptionbox:2.03 comment section: 1.48 notification center: 1.33
Topic 2 user interface: 1.25 splitscreen:1.23 split screen: 0.94
Topic 3 playback error: 1.44 batterydrain:0.99performanceimprovement: 1.41
Topic 4 certain spot: 1.81 cpu usage: 0.85 camera roll: 1.22
Topic 5 profile picture: 2.19 main page: 1.11 home screen: 1.18
Topic 6 say playback error: 1.54 long period: 0.92 forcequit:1.26
Topic 7 copyright issue: 1.11 bring back: 1.14 nothing happen: 1.53
Topic 8 take forever: 1.88 ten minute: 1.12 pure torture: 1.02
Topic 9 sound quality: 1.55 major issue: 1.45 buffer forever: 1.03
Topic 10 home button: 1.15 full screen: 1.07 home page: 1.29
Table 5: Topic labels in sentences for YouTube. The high-
lighted ones are the detected emerging issues.
v11.07 v11.10
Topic 1Imeanitworkbutwhydoyoutakeoff
whereyouwouldclickalinkinthe
descriptionanditdoesn’tevenletme
gothroughthevideo:-0.05It say error every time I try to reply
back to a comment: 0.52
Topic 2But right now the lack of multitasking
have actually make it a betterexperience to use YouTube in safari:
-0.79Addsplitviewandslideoverbutno
pictureinpicture:-1.36
Topic 3Please fix this app fix this bug and that
playback error: -0.80DearYouTube pleasereleaseafixfor
overheatissueonolderiPhoneandthe
batterydrainjusttooridiculous:-0.45
Table 6: Changelog of YouTube
Version Date Changelog
11.1022-Mar-16(1) Added slideover and splitview support
(2)Moved hometabsinto navigationbarforiPadin land-
scapemode
(3) Fixed bug that prevented URLs invideodescriptions
from opening
11.1129-Mar-16(1)Fixedbugwhereaccessibility VoiceOver loopedoverthe
same elements
(2) Fixed issue where the video couldn’t be exitedafter
completing
(3) Bug fixes and stability improvements
Views
0510152025303540
012345678Posts ViewsPosts
Figure 8: Count of posts and views related to the battery is-
sue in YouTube iOS forum.
interfacebyaddingthefunctionalityofmultitasking( i.e.,sliding
overandsplittingview[32])andfixesthebuginvideodescriptions.
ReferringtoTable4andTable5,wediscoverthatthetwoissuesaredetectedby
IDEAinTopic1andTopic2ofthepreviousversion11.07.
Thentostatisticallymeasuretheperformanceofourframework,
we employ the proposed three metrics in Section 4.2. Based on
the collected 33 versions for YouTube, IDEAachieves Precision E,
RecallL,andFhybridat0.628,0.666, 0.636insentence-levelissues
and 0.592, 0.472, and 0.523 in phrase-level issues, respectively.
Discussionoftheperformance: Sincethechangelogsmaynot
cover all the changes in releases, the metric Precision ErepresentsTable 7: Comparison result of different methods on six sub-
ject apps. The value under each app name indicates the av-erage number of reviews across the versions.
/g51/g85/g72/g70/g76/g86/g76/g82/g81 /g40/g53/g72/g70/g68/g79/g79 /g47 /g41/g75/g92/g69/g85/g76/g71/g51/g85/g72/g70/g76/g86/g76/g82/g81 /g40/g53/g72/g70/g68/g79/g79 /g47 /g41/g75/g92/g69/g85/g76/g71
/g50/g47/g39/g36 /g19/g17/g23/g25/g27 /g19/g17/g24/g21/g27 /g19/g17/g23/g26/g22 /g19/g17/g23/g27/g21 /g19/g17/g25/g21/g21 /g19/g17/g24/g22/g23
/g44/g39/g40/g36/g16 /g53 /g19/g17/g25/g19/g25 /g19/g17/g23/g25/g20 /g19/g17/g24/g21/g19 /g19/g17/g23/g26/g27 /g19/g17/g24/g26/g19 /g19/g17/g24/g19/g22
/g44/g39/g40/g36/g16/g54 /g19/g17/g21/g24/g19 /g19/g17/g24/g22/g19 /g19/g17/g22/g23/g19 /g19/g17/g23/g20/g26 /g19/g17/g24/g23/g26 /g19/g17/g23/g26/g22
/g44/g39/g40/g36/g14/g19/g17/g24/g26/g20 /g19/g17/g23/g28/g26 /g19/g17/g24/g22/g20 /g19/g17/g23/g26/g25 /g19/g17/g25/g22/g28 /g19/g17/g24/g23/g25
/g50/g47/g39/g36 /g19/g17/g23/g23/g20 /g19/g17/g23/g25/g21 /g19/g17/g23/g24/g20 /g19/g17/g24/g26/g27 /g19/g17/g25/g25/g23 /g19/g17/g24/g28/g26
/g44/g39/g40/g36/g16 /g53/g19/g17/g24/g19/g25 /g19/g17/g23/g21/g28 /g19/g17/g23/g24/g25 /g19/g17/g24/g24/g19 /g19/g17/g25/g24/g28 /g19/g17/g24/g27/g25
/g44/g39/g40/g36/g16/g54 /g19/g17/g24/g23/g27 /g19/g17/g23/g25/g25 /g19/g17/g24/g19/g21 /g19/g17/g23/g24/g25 /g19/g17/g25/g24/g25 /g19/g17/g24/g21/g21
/g44/g39/g40/g36/g14/g19/g17/g24/g28/g21 /g19/g17/g23/g26/g21 /g19/g17/g24/g21/g22 /g19/g17/g25/g21/g27 /g19/g17/g25/g25/g25 /g19/g17/g25/g22/g25
/g50/g47/g39/g36 /g19/g17/g20/g24/g26 /g19/g17/g22/g19/g24 /g19/g17/g20/g25/g25 /g19/g17/g22/g20/g22 /g19/g17/g24/g24/g19 /g19/g17/g22/g26/g24
/g44/g39/g40/g36/g16 /g53/g19/g17/g24/g23/g21 /g19/g17/g22/g21/g25 /g19/g17/g23/g19/g26 /g19/g17/g25/g21/g24 /g19/g17/g24/g26/g20 /g19/g17/g24/g28/g26
/g44/g39/g40/g36/g16/g54 /g19/g17/g24/g19/g19 /g19/g17/g22/g23/g21 /g19/g17/g23/g19/g25 /g19/g17/g24/g19/g19 /g19/g17/g24/g20/g27 /g19/g17/g24/g19/g28
/g44/g39/g40/g36/g14/g19/g17/g25/g21/g24 /g19/g17/g22/g23/g19 /g19/g17/g23/g23/g19 /g19/g17/g25/g21/g24 /g19/g17/g25/g24/g20 /g19/g17/g25/g22/g27
/g50/g47/g39/g36 /g19/g17/g22/g19/g19 /g19/g17/g21/g25/g28 /g19/g17/g20/g25/g19 /g19/g17/g21/g19/g19 /g19/g17/g23/g21/g20 /g19/g17/g20/g21/g28
/g44/g39/g40/g36/g16 /g53/g19/g17/g24/g19/g19 /g19/g17/g21/g20/g25 /g19/g17/g22/g19/g20 /g19/g17/g26/g24/g19 /g19/g17/g22/g26/g26 /g19/g17/g24/g19/g21
/g44/g39/g40/g36/g16/g54 /g19/g17/g19/g25/g26 /g19/g17/g21/g27/g28 /g19/g17/g22/g25/g25 /g19/g17/g24/g19/g19 /g19/g17/g22/g28/g27 /g19/g17/g23/g23/g22
/g44/g39/g40/g36/g14/g19/g17/g25/g25/g26 /g19/g17/g22/g20/g27 /g19/g17/g23/g22/g20 /g19/g17/g25/g25/g26 /g19/g17/g23/g22/g23 /g19/g17/g24/g21/g25
/g50/g47/g39/g36 /g19/g17/g20/g25/g26 /g19/g17/g21/g22/g27 /g19/g17/g20/g28/g25 /g19/g17/g24/g19/g19 /g19/g17/g23/g27/g27 /g19/g17/g23/g28/g23
/g44/g39/g40/g36/g16 /g53 /g19/g17/g21/g21/g28 /g19/g17/g21/g23/g22 /g19/g17/g21/g21/g19 /g19/g17/g25/g23/g25 /g19/g17/g23/g28/g25 /g19/g17/g24/g25/g20
/g44/g39/g40/g36/g16/g54 /g19/g17/g20/g21/g24 /g19/g17/g21/g27/g24 /g19/g17/g20/g22/g21 /g19/g17/g22/g24/g23 /g19/g17/g23/g26/g25 /g19/g17/g23/g19/g25
/g44/g39/g40/g36/g14/g19/g17/g21/g21/g28 /g19/g17/g21/g24/g20 /g19/g17/g21/g21/g26 /g19/g17/g25/g23/g25 /g19/g17/g24/g21/g26 /g19/g17/g24/g27/g19
/g50/g47/g39/g36 /g19/g17/g20/g19/g19 /g19/g17/g24/g25/g26 /g19/g17/g20/g23/g27 /g19/g17/g22/g25/g26 /g19/g17/g25/g20/g26 /g19/g17/g23/g24/g27
/g44/g39/g40/g36/g16 /g53/g19/g17/g22/g22/g22 /g19/g17/g25/g20/g20 /g19/g17/g22/g26/g25 /g19/g17/g23/g20/g26 /g19/g17/g26/g22/g22 /g19/g17/g24/g20/g24
/g44/g39/g40/g36/g16/g54 /g19/g17/g22/g22/g22 /g19/g17/g25/g21/g21 /g19/g17/g22/g26/g21 /g19/g17/g24/g19/g19 /g19/g17/g26/g20/g20 /g19/g17/g24/g27/g26
/g44/g39/g40/g36/g14/g19/g17/g24/g20/g26 /g19/g17/g25/g24/g22 /g19/g17/g24/g21/g22 /g19/g17/g24/g27/g22 /g19/g17/g26/g19/g19 /g19/g17/g24/g27/g26/g57/g76/g69/g72/g85
/g11/g21/g15/g20/g23/g20/g12
/g38/g79/g72/g68/g81/g3/g48/g68/g86/g87/g72/g85
/g11/g25/g15/g22/g22/g21/g12
/g40/g69/g68/g92
/g11/g22/g15/g28/g23/g22/g12
/g54/g90/g76/g73/g87/g46/g72/g92
/g11/g20/g15/g22/g20/g22/g12/g36/g83/g83/g3/g49/g68/g80/g72
/g11/g6/g68/g89/g74/g17/g3/g85/g72/g89/g76/g72/g90/g86/g12/g48/g72/g87/g75/g82/g71/g51/g75/g85/g68/g86/g72 /g54/g72/g81/g87/g72/g81/g70/g72
/g49/g50/g36/g36/g3/g53/g68/g71/g68/g85
/g11/g24/g21/g22/g12
/g60/g82/g88/g87/g88/g69/g72
/g11/g20/g15/g20/g23/g22/g12
a lower bound of the performance. For example, the highlighted
emerging issues, such as “split screen” and “battery drain” for ver-
sion 11.10 in Table 4, are not clearly embodied by the changelog of
version 11.11(shown inTable 6).Wethen inspectthe reasonwhy
thedetectedissues “fail”tobenoticed bydevelopers.Wediscover
that “split screen” is one new added feature of version 11.10 and
it is reasonable for a hot discussion about the drawbacks of thisfeature in the user reviews, which explains why “split view” is
identifiedas oneemergingissue.Then fortheissue “batterydrain”,
we dig into the official user forum of YouTube for iOS [41], and
observethenumberofpostsandviewsoftheissuebysearchingthe
phrase(illustratedinFigure8).Wefindthatthereexistsasudden
increase in the counts of posts and views around May 2016, which
also demonstrates that the battery issue was an emerging issue
for the version. Therefore, we summarize that changelogs may not
completely cover all emerging issues, and our performance metric
computesa lower boundof theperformance of IDEA.The compari-
sonwithothermethodscanvalidateourproposedframeworkmore
sufficiently.
4.4 Result of RQ2: Comparison Results with
Different Methods
For validatingthe performanceof AOLDAin IDEA, wechoose the
typical method for online topic modeling - OLDA [1]. For evalu-
ating theproposed topic labelingmethod in Section 3.3.2,we also
compare with the method only considering the sentiment score for
labeling (denoted as IDEA-R), and the method only considering the
semantic score for labeling (denoted as IDEA-S). For clarity, our
proposedframeworkisrepresentedas IDEA+.Table7illustratesthe
comparison resultson thesix subjectapps. We discussthe perfor-
mance of IDEAfrom three aspects in the following subsections.
4.4.1 Issues in Phrases v.s. Issues in Sentences. According to the
results of IDEA+in Table 7, issues in sentences can attain better
performance than those in phrases by 30.7%, 52.5%, and 43.2% in
Precision E,RecallL,andFhybridonaveragerespectively.Thismay
beattributedtothefactthatsentencescandelivermoredetailedand
completeinformationthanphrases(explainedinSection4.3.2),and
8
55
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. therebycovermorekeytermsinchangelogs.Focusingonthemetric
Fhybrid, employing sentence-level issues improves the properties
of using phrase-level issues by 2.7% ∼1.56 times. For Precision E,
the issues in sentences increase those in phrases by -16.7% ∼1.8
times.ThenegativeincreaseonlyoccurstotheappNOAARadar,
whichmaybebecausethesmalldatasetsoftheapp(512reviews
perversion)introduceinstabilityforourframework[23].Forthe
metricRecallL,IDEA+showsanincreaserangeof7.1% ∼1.1times.
Overall,sentence-levelissuescanbetterrepresent appissues,and
we employ such issue representations for comparing with different
methods in the following.
4.4.2 AOLDA v.s. OLDA. On average, IDEA+achieves 0.604,
0.603,and0.585for Precision E,RecallL,andFhybridrespectively,
while the OLDA-based method only obtains 0.407, 0.560, and 0.431
for the three metrics. Considering the metric Fhybrid, AOLDA en-
hancestheperformanceofOLDAby2.1% ∼3.08times,whereOLDA
presentsthepoorestperformance(0.129)ontheappwiththelargest
quantity of reviews ( e.g., Clean Master with 6,332 reviews per ver-
sion).Forthemetrics Precision EandRecallL,ourframeworkcan
improvetheperformanceby-1.1% ∼2.33timesand0.3% ∼18.4%re-
spectively. Although IDEA+exhibits a slightly lower Precision E
thantheOLDA-basedmethodfortheappNOAARadar,itshows
better performance in both FhybridandRecallL, which indicates
that our framework can well balance the precision and recall in
issue detection.
4.4.3 IDEAv.s.DifferentTopicLabelingMethods. Wediscover
thatIDEA+canachievebetterperformancethan IDEA-Randpresent
theincreaseratesat7.1%,7.3%,and7.7%onaverageforthethree
metricsrespectively.For Fhybrid,ourframeworkimproves IDEA-R
by3.4%∼14.0%.Whencomparedwith IDEA-S,ourframeworkin-
creasesby34.9%,10.4%,and20.7%onaveragein Precision E,RecallL,
andFhybrid, respectively.Therefore, both theuser sentimentand
semantic similarity should be considered for topic labeling.
4.5 RQ3: Effect of Different Parameter Settings
In this part, we demonstrate the impact of different parameter set-
tingsontheperformanceofourframework.Wefocusonanalyzing
two important parameters, including the window size wand the
numberoftopics K.Wealsoexplainhowwechoosetheparameters
in our experiments.
4.5.1 WindowSize. Figure9illustratestheresultsofdifferent
windowsizeson twoapps,includingYouTubeandEbay.For both
apps, the values of Fhybridpresent an inverted “U” shape for both
phrase-level and sentence-level issues. We attribute this to the
reasonthatthetopicdistributionsofthecurrentversionarestrongly
dependent on those of the previous versions. When the window
sizeissetrelativelysmall,thedetectedissuesofcurrentversions
maybemoredivergentandunstable .However, largerwindowsizes
mayweakenthedistinctionofappissuesamongversions,which
isunfavorablefordetectingtheemergingissues.Since w= 3can
achievethebestperformanceonourdatasets(indicatedinFigure9),
we set the window size as three in our experiments.
4.5.2 TheNumberofTopics. Generally,thetopicnumbershould
be defined according to the size of the review collection [3]. In
IDEA,a largertopicnumber canbringmoreprioritized appissues,0.30.40.50.60.70.8
12345Phrase Sentence0.050.150.250.350.450.55
12345Phrase Sentence
Window size Window sizeܨ௛௬௕௥௜ௗ
ܨ௛௬௕௥௜ௗ
(a) Youtube (b) Ebay
Figure 9: Impact of window size.
whichcancovermorechangelogs( i.e.,increasing RecallL).How-
ever, more app issues may be a double-edged sword, since the
metricPrecision Ecan be decreased. Figure 10 shows the results of
differenttopicnumbersontwoapps,includingNOAARadarandEbay.ForEbay(onaverage3,943reviewsperversion),thevalues
ofFhybriddisplay an ascending tendency in both phrase-level and
sentence-levelissues.ButforNOAARadar(onaverage523reviews
per version), a larger topic number will reduce the performance
whenusingphrase-levelissues.Tobetterbalancetheprecisionand
recall, we set the topic number as 10 during experiments.
0.10.20.30.40.50.60.7
6 7 8 9 10 11 12Phrase Sentence
Topic Numberܨ௛௬௕௥௜ௗ
(a) Noaa Radar00.10.20.30.40.50.60.7
6 7 8 9 10 11 12Phrase Sentence
Topic Numberܨ௛௬௕௥௜ௗ
(b) Ebay
Figure 10: Impact of topic number.
5 IDEA IN PRACTICE
In this section, we explore the performance of IDEAin practice.
First,weintroduceausersurveyconductedinTencent.Thenwe
describe the successful application of IDEAin Tencent’s products.
5.1 User Survey
To further demonstrate the significance and effectiveness of our
work, we conduct a user study among 45 staff in Tencent, with 29
developers (64.4%), five data analysts (11.1%), four product man-
agers (8.9%), three maintenance engineers (6.7%), one test engineer
(2.2%), and three from other positions (6.7%). The user study is
conductedthroughanonlinequestionnaire,whichconsistsofsix
questions:onequestiononparticipants’background,fourquestions
for experimental assessment, and one question for understanding
their attitude towards such automatic analysis.
5.1.1 Changelogs as Ground Truth. We interview the partici-
pants about their opinions of using changelogs as ground truth,
since changelogs may only include partial changes of the releases.
Thesurveyresultsindicatethat31(68.9%)oftheintervieweesagree
thatchangelogscanreflectmodifiedissuesofthenewreleases,and
10(22.2%)ofthemindicateastrongapproval.Moreover,88.9%of
participants think that changelogs embody the user concerns of
the previous releases,with 11.1% echoing strong agreement. Since
our framework aims to prioritize app issues based on user reviews,
using changelogs as ground truth is reasonable.
5.1.2 Effectiveness of Our Framework. During the survey, we
validate our framework in terms of three aspects: the presentation
9
56
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. styleof IDEA’sresults,theperformanceachievedbyourframework,
and the significance of such automatic analysis. The survey results
indicatethat75.6%ofparticipantsthinkthevisualizationwithan
issueriver iscomprehensibleforthem,whilethephrase-levelissues
(only with the approval rate at 11.1%) are considered more difficult
to understand than sentence-level issues (with an approval rate
of 37.8%). For inquiring about their opinions of the performanceof
IDEA, we present the example results of WizNote [44] with
Precision EandFhybridat50%∼60%.Accordingtothesurvey,88.9%
of the interviewees think that the performance is acceptable in
practicalusage,and31.1%stronglyapproveofsuchperformance.
In addition, all the participants think such automatic analysis of
detecting emerging issues is significant for app development, with
73.3% of them strongly agreeing with this sentiment. These results
provide strong evidence of the effectiveness of our framework.
5.2 Successful Story in Industrial Practice
TeamXofTencentaimstoprovidedeveloperswithabnormalevents
reportand operationstatisticsof20+ appsofTencent.Traditional
reviewanalysisinXrequireslotsofmanpower.Withtheincreasingquantitiesofappreviewsandtheonslaughtofspaminuserreviews,
X has been seeking a means of automatic analysis. We have suc-
cessfully applied IDEAinto X to maintain four apps with review
quantitiesat500 ∼5,000perday.Thefourappsservehundredsof
millionsofusersworldwide,andtheirqualityisveryimportantfor
thecompany. IDEAobtainsuserreviewsbythehourordaybased
on the review collection API provided by X. The collected reviews
aregroupedbyversionsandprocessedinrealtime.Thedetected
emerging issues are fed back to developers for further analysis.
In July of 2017, App Y encountered a serious problem when the
contentsearchservicewasnotavailableforaperiodoftime,and
receivedasuddenincreaseintheamountofuserfeedback.With
IDEA, the team X quickly identified the issue and reported it to the
development team. The team also confirmed this issue.
Moreover, IDEAcanefficientlyanalyzelargenumbersofreviews.
Wedeploy IDEAonaPCwithIntel(R)XeonE5-2620v2CPU(2.10
GHz,6cores)and16GBRAM.For36,000productreviewsperver-
sion, IDEAachieves a high throughput (nearly 160 reviews per
second),andonlyconsumes1.02GBofmemoryonaverage.Overall,
IDEAisprovedtobeeffectiveandefficientinquicklypinpointing
urgent app issues for developers in the industrial practice.
6 THREATS TO VALIDITY
First, we only select six subject apps for validating our framework
and the apps represent a tiny portion of all apps on app markets.Since we utilize user reviews for detecting emerging issues, our
methods can be easily applied to other apps, even those with other
languages. Also, we alleviate this threat by choosing the apps from
different categoriesand platforms. Second,the number ofuser re-
viewscanimpacttheperformanceof IDEA.However,sincesmall
datasetscanbe easilyanalyzedmanually,our frameworkaimsfor
automaticanalysisoflargereviewdatasets.Wealsomitigatethis
threatbyselectingappswithdifferentquantitiesofuserreviews(on
average523 ∼6,332reviewsperversion).Third,thetopicnumber
should bemanually defined, which caninfluence the performanceofourframework.Suchathreatstemsfromtheoriginaltopicmod-
elingmethod[4],whichisstillagreatchallengeinacademia[45].
Inthispaper,wealleviatethisthreatbytestingondifferenttopic
numbers(introducedinSection4.5.2).Inpractice,wecanemploy
heuristic approaches [45] to determine the optimal topic number.
7 RELATED WORK
7.1 App Review Mining
Some previous work [12, 20] focuses on identifying users’ major
concerns or preferences from app reviews [28]. Different from
these work, where the reviews are manually analyzed, there exists
someresearchwhichextractsappissuesautomatically.Forexam-
ple,[17,42]designframeworksforautomaticretrievalofmobile
app feature requests from reviews. Mcilroy et al.[29] contribute to
automaticallyassigningmultiplelabelstoeachreview.Although
thework[40,25,14]classifyappreviewsintodifferentcategories
for recommending software updates, they mainly analyze static
reviews and pay little attention to tracking issue changes. In [21,
9,15,27],theauthorsanalyzevariationsinappratings,prices,or
reviewsizesalongwithtime,buttheissuesareneitheridentified
automatically nor studied online. Similarly, online review analysis
isnotthefocusofGao etal.’swork[10,11].Therealsoexistssome
work [13] focusing on analyzing the parts of apps that are loved
byusers.Differentfrompreviousefforts,ourworkaimstodetect
the emerging issues automatically and dynamically. We employ
changelogsforverifyingeffectivenessofourframework.Moreover,
wepresentappissuesinaninteractiveandcomprehensiblemanner.
7.2 Emerging Topic Detection
There are research efforts focused on detecting emerging topics in
socialmedia,suchasTwitter[5]andMicroblog[7].Onlinetopic
models [1, 22] are the typical methods for discovering burst topics.
Wearethefirsttoapplyonlinetopicmodelingmethodsintoapp
reviews,andweimproveonpreviousworkbyproposinganovel
AOLDA. AOLDA can adaptively combine the topics in previous
app versions and greatly enhances the performance of OLDA [1].
8 CONCLUSION
Timely and effectively detecting app issues is crucial for app de-
velopers.Wepropose IDEA,anovelframeworkforautomatically
identifying emerging issues from user reviews. Our frameworkcan be easily applied to text-based online detection tasks and re-
portemergingissuestimely.Industrialpracticealsovalidatesthe
effectiveness of IDEA. In the future, we will refine IDEAto be capa-
ble of definingthe topic number automatically, andmake IDEAa
distributed algorithm for supporting ultra-large-scale datasets.
ACKNOWLEDGMENTS
WegreatlythankWeiwenQiuandJunOuyangofTencentforassist-
ingourindustrystudy.ThisworkwassupportedbytheKeyProject
of National Natural Science Foundation of China (No. 61332010
and No. 61472338), the Research Grants Council of the Hong Kong
SpecialAdministrativeRegion,China(No.CUHK14234416andNo.
14208815oftheGeneralResearchFund),andMicrosoftResearch
Asia via 2018 MSRA Collaborative Research Award.
10
57
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1]LoulwahAlSumait,DanielBarbará,andCarlottaDomeniconi.“On-lineLDA:
Adaptive Topic Models for Mining Text Streams with Applications to Topic
DetectionandTracking”.In: Proceedingsofthe8thIEEEInternationalConference
on Data Mining, ICDM 2008, December 15-19, 2008, Pisa, Italy . 2008, pp. 3–12.
[2]App Annie . https://www.appannie.com/en/.
[3]R. Arun et al. “On Finding the Natural Number of Topics with Latent Dirichlet
Allocation:SomeObservations”.In: AdvancesinKnowledgeDiscoveryandData
Mining,14thPacific-AsiaConference,PAKDD2010,Hyderabad,India,June21-24,
2010. Proceedings. Part I . 2010, pp. 391–402.
[4]DavidM.Blei,AndrewY.Ng,andMichaelI.Jordan.“LatentDirichletAlloca-
tion”. In: Journal of Machine Learning Research 3 (2003), pp. 993–1022.
[5]Mario Cataldi, Luigi Di Caro, and Claudio Schifanella. “Emerging topic detec-
tion on twitter based on temporal and social terms evaluation”. In: Proceedings
oftheTenthInternationalWorkshoponMultimediaDataMining(MDMKDD) .
ACM. 2010, p. 4.
[6]Ning Chenet al.“AR-miner: mininginformative reviews fordevelopers from
mobileappmarketplace”.In: 36thInternationalConferenceonSoftwareEngi-
neering, ICSE 2014, Hyderabad, India - May 31 - June 07, 2014 . 2014, pp. 767–
778.
[7]Yan Chen et al. “Emerging topic detection for organizations from microblogs”.
In:The36thInternationalACMSIGIRconferenceonresearchanddevelopment
in Information Retrieval, SIGIR 2013, Dublin, Ireland - July 28 - August 01, 2013 .
2013, pp. 43–52.
[8]Facebook Messenger is getting slammed by tons of negative reviews . http://
www.businessinsider.com/facebook-messenger-app-store-reviews-are-
humiliating-2014-8.
[9]Bin Fu et al. “Why people hate your app: making sense of user feedback in
a mobile app store”. In: The 19th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August
11-14, 2013 . 2013, pp. 1276–1284.
[10]Cuiyun Gao et al. “AR-Tracker: Track the Dynamics of Mobile Apps via User
Review Mining”. In: 2015 IEEE Symposium on Service-Oriented System Engi-
neering, SOSE 2015, San Francisco Bay, CA, USA, March 30 - April 3, 2015 . 2015,
pp. 284–290.
[11]Cuiyun Gao et al. “PAID: Prioritizing app issues for developers by tracking
userreviewsoverversions”.In: 26thIEEEInternationalSymposiumonSoftware
Reliability Engineering, ISSRE 2015, Gaithersbury, MD, USA, November 2-5, 2015 .
2015, pp. 35–45.
[12]JudithGebauer,YaTang,andChaiwatBaimai.“Userrequirementsofmobile
technology: results from a content analysis of user reviews”. In: Inf. Syst. E-
Business Management 6.4 (2008), pp. 361–384.
[13]Xiaodong Gu and Sunghun Kim. “"What Parts of Your Apps are Loved by
Users?"(T)”.In: 30thIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering,ASE2015,Lincoln,NE,USA,November9-13,2015 .2015,pp.760–
770.
[14] EmitzaGuzman andWalid Maalej. “HowDo UsersLike This Feature?A Fine
Grained Sentiment Analysis of App Reviews”. In: IEEE 22nd International
Requirements Engineering Conference, RE 2014, Karlskrona, Sweden, August
25-29, 2014 . 2014, pp. 153–162.
[15]LeonardHoon etal.“An analysisofthe mobileappreview landscape:trends
and implications”. In: Faculty of Information and Communication Technologies,
Swinburne University of Technology, Tech. Rep (2013).
[16]Jiajia Huang et al. “A probabilistic method for emerging topic tracking in
Microblog stream”. In: World Wide Web 20.2 (2017), pp. 325–350.
[17]Claudia Iacob and Rachel Harrison. “Retrieving and analyzing mobile appsfeature requests from online reviews”. In: Proceedings of the 10th Working
Conference on Mining Software Repositories, MSR 2013, San Francisco, CA, USA,
May 18-19, 2013 . 2013, pp. 41–44.
[18]Aminul Islam and Diana Zaiu Inkpen. “Semantic text similarity using corpus-
based word similarity and string similarity”. In: TKDD2.2 (2008), 10:1–10:25.
[19]Jensen Shannon divergence . https://en.wikipedia.org/wiki/Jensen-Shannon_
divergence.
[20]H Khalid et al. “What do mobile app users complain about? A study on free
iOS apps. 2014”. In: IEEE Software 10 (2015).
[21]Jieun Kim et al. “Trends and relationships of smartphone application services:
Analysis of apple app store using text mining-based network analysis”. In:
Proceedings of the 4th ISPIM Innovation Symposium . 2012.
[22]JeyHanLau,NigelCollier,andTimothyBaldwin.“On-lineTrendAnalysiswith
Topic Models: \#twitter Trends Detection Topic Model Online”. In: COLING2012,24thInternationalConferenceonComputationalLinguistics,Proceedings
oftheConference:TechnicalPapers,8-15December2012,Mumbai,India .2012,
pp. 1519–1534.
[23]LDA on small datasets . https://stats.stackexchange.com/questions/78926/at-
what-point-does-lda-latent-dirichlet-allocation-not-make-sense-to-use.
[24]Qingwei Lin et al. “iDice: problem identification for emerging issues”. In:
Proceedings ofthe 38th InternationalConference on SoftwareEngineering, ICSE
2016, Austin, TX, USA, May 14-22, 2016 . 2016, pp. 214–224.
[25]WalidMaalejandHadeerNabil.“Bugreport,featurerequest,orsimplypraise?
On automatically classifying app reviews”. In: 23rd IEEE International Require-
mentsEngineeringConference,RE2015,Ottawa,ON,Canada,August24-28,2015 .
2015, pp. 116–125.
[26]Yichuan Man et al. “Experience Report: Understanding Cross-Platform App
Issues from User Reviews”. In: 27th IEEE International Symposium on Software
ReliabilityEngineering,ISSRE2016,Ottawa,ON,Canada,October23-27,2016 .
2016, pp. 138–149.
[27]WilliamMartin,FedericaSarro,andMarkHarman.“Causalimpactanalysisfor app releases in google play”. In: Proceedings of the 24th ACM SIGSOFT
International Symposium on Foundations of Software Engineering, FSE 2016,
Seattle, WA, USA, November 13-18, 2016 . 2016, pp. 435–446.
[28]William Martin et al. “A Survey of App Store Analysis for Software Engineer-
ing”. In:IEEE Trans. Software Eng. 43.9 (2017), pp. 817–847.
[29]Stuart McIlroy et al. “Analyzing and automatically labelling the types of user
issuesthatareraisedinmobileappreviews”.In: EmpiricalSoftwareEngineering
21.3 (2016), pp. 1067–1106.
[30]Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. “Automatic labeling of
multinomialtopicmodels”.In: Proceedingsofthe13thACMSIGKDDInterna-
tionalConferenceonKnowledgeDiscoveryandDataMining,SanJose,California,
USA, August 12-15, 2007 . 2007, pp. 490–499.
[31]Tomas Mikolov et al. “Distributed Representations of Words and Phrases and
their Compositionality”. In: Advances in Neural Information Processing Systems
26: 27th Annual Conference on Neural Information Processing Systems 2013.
ProceedingsofameetingheldDecember5-8,2013,LakeTahoe,Nevada,United
States.2013, pp. 3111–3119.
[32]Multi-tasking in iOS . https://developer.apple.com/ios/human-interface-
guidelines/features/multitasking/.
[33] Thanh-Son Nguyen, Hady Wirawan Lauw, and Panayiotis Tsaparas. “Review
SynthesisforMicro-ReviewSummarization”.In: ProceedingsoftheEighthACM
InternationalConferenceonWebSearchandDataMining,WSDM2015,Shanghai,
China, February 2-6, 2015 . 2015, pp. 169–178.
[34]NLTK. http://www.nltk.org.
[35]PMI. https://en.wikipedia.org/wiki/Pointwise_mutual_information.
[36]Punkt tokenizer . http://www.nltk.org/modules/nltk/tokenize/punkt.html.
[37]PeterJ.RousseeuwandMiaHubert.“Robuststatisticsforoutlierdetection”.In:
Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery 1.1 (2011), pp. 73–
79.
[38]FedericaSarroetal.“Featurelifecyclesastheyspread,migrate,remain,anddie
inAppStores”.In: 23rdIEEEInternationalRequirementsEngineeringConference,
RE 2015, Ottawa, ON, Canada, August 24-28, 2015 . 2015, pp. 76–85.
[39]Softmax function . https://en.wikipedia.org/wiki/Softmax_function.
[40]Andrea Di Sorbo et al. “What would users change in my app? summarizing
appreviewsforrecommendingsoftwarechanges”.In: Proceedingsofthe24th
ACMSIGSOFTInternationalSymposiumonFoundationsofSoftwareEngineering,
FSE 2016, Seattle, WA, USA, November 13-18, 2016 . 2016, pp. 499–510.
[41]UserforumofYoutubeiOS .https://productforums.google.com/forum/#!forum/
youtube.
[42]Phong Minh Vu et al. “Mining User Opinions in Mobile App Reviews: A
Keyword-Based Approach (T)”. In: 30th IEEE/ACM International Conference on
AutomatedSoftwareEngineering,ASE2015,Lincoln,NE,USA,November9-13,
2015. 2015, pp. 749–759.
[43]XiaojunWanandTianmingWang.“AutomaticLabelingofTopicModelsUsing
Text Summaries”. In: Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany,
Volume 1: Long Papers . 2016.
[44]WizNote. https://www.wiz.cn/.
[45]Weizhong Zhao et al. “A heuristic approach to determine an appropriate num-
ber of topics in topic modeling”. In: BMC Bioinformatics 16.13 (2015), S8.
11
58
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 10:49:35 UTC from IEEE Xplore.  Restrictions apply. 