SCMiner: Localizing System-Level Concurrency
Faults from Large System Call Traces
Tarannum Shaila Zaman, Xue Han, Tingting Yu
Department of Computer Science
University of Kentucky
tarannum.zaman@uky.edu, xha225@g.uky.edu, tyu@cs.uky.edu
Abstract ‚ÄîLocalizing concurrency faults that occur in produc-
tion is hard because, (1) detailed Ô¨Åeld data, such as user input,Ô¨Åle content and interleaving schedule, may not be available to
developers to reproduce the failure; (2) it is often impractical to
assume the availability of multiple failing executions to localizethe faults using existing techniques; (3) it is challenging to searchfor buggy locations in an application given limited runtime data;and, (4) concurrency failures at the system level often involvemultiple processes or event handlers (e.g., software signals), which
cannot be handled by existing tools for diagnosing intra-process
(thread-level) failures. To address these problems, we presentSCMiner, a practical online bug diagnosis tool to help developers
understand how a system-level concurrency fault happens basedon the logs collected by the default system audit tools. SCMinerachieves online bug diagnosis to obviate the need for ofÔ¨Çine bugreproduction. SCMiner does not require code instrumentation
on the production system or rely on the assumption of the
availability of multiple failing executions. SpeciÔ¨Åcally, after thesystem call traces are collected, SCMiner uses data mining andstatistical anomaly detection techniques to identify the failure-inducing system call sequences. It then maps each abnormalsequence to speciÔ¨Åc application functions. We have conducted
an empirical study on 19 real-world benchmarks. The results
show that SCMiner is both effective and efÔ¨Åcient at localizingsystem-level concurrency faults.
I. I NTRODUCTION
Due to the worldwide spread of multi-core architecture, con-
current systems are becoming more pervasive. Debugging con-current programs is difÔ¨Åcult because of the non-deterministicbehavior and the speciÔ¨Åc sequences of interleaving in the
execution Ô¨Çow. It often takes a tremendous amount of time
and effort to reproduce and localize concurrency faults [1].
A concurrency fault may occur either during testing or
in the production environment. If the failure occurs in pro-duction, developers often have to diagnose it in a different(debugging) environment to identify the root cause. However,this is challenging primarily because a program can behavedifferently in a different environment for each execution,
especially for a concurrent system with non-deterministic
behaviors. In addition, customers may not be willing to sharetheir inputs for being used to reproduce failures due toprivacy concerns. Therefore, it is hard to apply existing ofÔ¨Çinedebugging tools [2]‚Äì[4] to diagnose concurrency failures that
cannot be reproduced outside the production environment.While previous research [2], [5]‚Äì[12] have been conducted
This research is supported in part by the NSF grant CCF-1652149.to help developers in debugging concurrency faults, it takes
advantage of Ô¨Åne-grained logging for deterministic record-and-replay, which is infeasible in the production environment dueto the unbearable performance overhead.
To relieve the burden of debugging, there has been some
research on analyzing the runtime information and automat-
ically localize faults [13]‚Äì[16]. For example, Falcon [14]collects both passing and failing execution traces by instru-menting each memory access of a concurrent program. Itthen uses statistical analysis to rank interleaving patternsinvolving the memory accesses. This approach is intended
to be used in the pre-deployment environment because of
the heavy-weighted instrumentation. Cooperative Concurrentbug Isolation (CCI) [13] leverages statistical debugging andviews interleavings as predicates, which are collected at theruntime and analyzed to Ô¨Ånd the location of the concurrency
fault. CCI induces less overhead than Falcon but still requires
code instrumentation on the predicates. Therefore, the twoapproaches can be impractical for being used in the produc-tion environment. In addition, both Falcon and CCI requiremultiple failed and passed runs to perform the statisticalanalysis. However, this assumption often does not hold in the
production environment ‚Äì it is difÔ¨Åcult to obtain multiple failed
runs because a concurrency fault often manifests itself underspeciÔ¨Åc interleavings and inputs [17].
In this work, we propose SCMiner, a practical online failure
diagnosis tool to help developers understand why a concur-
rency failure occurs in production and localizes the cause of
the failure in speciÔ¨Åc application functions. SCMiner focuseson inter-process concurrency faults, where multiple operating-system components (e.g., processes, software signals, andinterrupts) incorrectly share resources [18]. The main differ-ence between an inter-process (system-level) concurrency fault
and an intra-process (thread-level) concurrency fault is that a
system-level concurrency fault corrupts the persistent storageand the other system-wide resources, which can crash theentire system; whereas a thread-level concurrency fault oftencorrupts the volatile memory within a process [17]. Research
has shown that more than 73 %of the race conditions reported
in the Linux distributions were system-level races [11].
SCMiner works as follows. When an anomaly (failure) is
discovered by the user, SCMiner is triggered to perform onlinefault localization that outputs a list of abnormal system callsequences and their associated application functions ranked in
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXB SF&OHJOFFSJOH	"4&
¬•*&&&
%0*"4&
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. terms of their likelihood of causing the failure. To achieve
this goal, SCMiner analyzes a window of recent system calls.The rationale behind our approach is twofold. First, a systemcall trace can be easily collected via system audit tools [19]in production (e.g., cloud computing infrastructures) with lowoverhead (19 %). Second, system-level concurrency failures are
often caused by incorrect synchronizations of system calls on
shared resources between two application processes. There-fore, we can detect buggy locations by monitoring systemcalls. However, it is challenging to identify the abnormalsystem call sequences from a trace potentially containing
millions of system calls. Even if the sequences are identiÔ¨Åed,
a modern server system typically consists of tens of thousandsof functions ‚Äì mapping a sequence to speciÔ¨Åc functions in theprograms is a non-trivial task.
To address the above challenges, SCMiner is designed to
have two major phases. In the Ô¨Årst phase, SCMiner uses prin-
cipal component analysis (PCA) ‚Äì an unsupervised learning
approach [20] to identify abnormal system call sequences.Since the number of system calls in the trace can be enormous,SCMiner splits the trace into a list of execution segments andgenerates a feature vector representation for each segment,
where each element in the vector is a system call sequence.
The segments together with their feature vectors are used toperform PCA for identifying abnormal system call sequences.PCA is efÔ¨Åcient because its runtime is linear with the numberof vectors so the detection can scale to large traces.
In the second phase, SCMiner maps each abnormal se-
quence to speciÔ¨Åc application functions. Since SCMiner does
not assume the availability of source code, it is impossible touse static analysis to link system calls to application functions.Instead, SCMiner obtains multiple system call traces outsidethe production environment by using binary instrumentation
for building a map between system calls and function names.
However, Due to inconsistencies between production and non-production environment and the lack of inputs, an exact match-ing is almost impossible. To address this problem, SCMineruses frequent pattern mining to extract the frequently executedsystem calls from each function as a function signature . The
function signatures can serve as a high-level matching to detect
and rank a list of functions that potentially map to an abnormalsystem call sequence.
SCMiner has several distinguishing features, which make it
more advantageous over existing approaches. First, SCMiner
does not require developers to reproduce bugs on their side toachieve fault localization. Second, SCMiner uses the default
auditd [19] daemon in Linux and does not require heavy
program instrumentation, which makes the tool transparent andpractical for production use. Third, existing techniques oftenrequire multiple failing and passing executions to localize
faults [6], but it is hard to collect multiple failing execu-
tions especially for concurrent programs. Instead, SCMineronly assumes the existence of one failing system call tracegenerated by the auditd daemon. Finally, SCMiner can capture
the buggy functions for inter-process failures, whereas existingtechniques [6], [15], [21], [22] focus on intra-process failures.
47=rcvmsg, 59 = execve , 1 = write , 2 = open, 3= close.
Figure 1. A partial system call trace
To evaluate SCMiner, we conducted an empirical study on
19 applications with known real-world concurrency failures.
Our results show that SCMiner effectively identiÔ¨Åes the ab-
normal system call sequences and their associated applicationfunctions leading to the concurrency failures. We also foundthat the use of optimization and function signature techniquescan improve the effectiveness and efÔ¨Åciency of SCMiner.
Finally, we found that SCMiner was highly robust in handling
system call traces with different sizes. Overall, we considerthese results to be strong and they indicate that SCMiner couldbe a useful approach for helping developers to automaticallylocalize system-level concurrency failures in production givenan arbitrarily-sized system call trace.
In summary, this paper makes the following contributions:
‚Ä¢We propose SCMiner, the Ô¨Årst fully automated tool for
fault localization in multi-process applications.
‚Ä¢We implement SCMiner and conduct an empirical study
to demonstrate its effectiveness and efÔ¨Åciency on real-world Linux applications [23].
II. B
ACKGROUND AND MOTIV ATION
In this section, we Ô¨Årst deÔ¨Åne our problem and then show a
motivating example. We also discuss the Principle ComponentAnalysis (PCA) brieÔ¨Çy.
A. Problem Statement
We deÔ¨Åne the production-level fault localization in multi-
process applications as follows. Given the binaries of a setof Processes under Debugging (PuDs) and system call tracesgenerated by these PuDs from the system built-in auditd
daemon, we compute a short system call sequence Sleading
to the failure and the associated the application functions F
of the system calls in S.
We assume that a concurrent system consists of a set of
processes {P
1...Pm}and a set of software signals {S1...Sn}.
Each process may create multiple threads, but for ease ofpresentation, we focus only on the process-level concurrencyfailure in this work while assuming each process has onethread. A failing process P
Fis a process that generates the
failure (or anomaly).
A system call trace contains a sequence of system calls gen-
erated from all applications running on the system. Each entryin the trace includes system call number, process ID, process

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. name, parent process ID, resource name, inode number, and
execution command parameters. Each system call number ismapped to a speciÔ¨Åc system call name, which can be obtainedfrom the system call table [24].
System-level concurrency faults. A system-level concurrency
fault occurs when multiple processes, signals, or interruptsaccess a system-wide resource (e.g., Ô¨Åle, device, etc.) withoutproper synchronization [25]. Such resources are often accessedthrough system calls. Thus, handling system-level concurrencyfault requires the modeling of read/write effects and synchro-
nization operations involving system calls. For example, the
lstat system call on Ô¨Åle freads the metadata of f. The
clone system call creates a new process inode under the
/proc directory (write). Synchronization operations control
process interactions through kernel process scheduler. Com-
mon process-level synchronization primitives include fork ,
wait ,exit ,pipe , and signal .
B. A Motivating Example
Debian - 283702 [26] is a real-world bug in bash version
3.0-10. Bash is an intuitive and Ô¨Çexible standard GNU
shell for common users [27]. It keeps a history of executedcommands in a history Ô¨Åle so that users can easily viewthe commands that are recently executed. However, problem
occurs when multiple bash shells execute concurrently and
corrupts the shell history Ô¨Åle.
Figure 1 shows a piece of system call trace recorded by
Linux auditd [19]. The full trace contains around 3,000K
system calls from 31 processes recorded within 30 minuteswhile bash was actively running. To simplify presentation, we
show system call number, process ID, process name, resourcename, and inode number. The trace can grow quickly depend-ing on how users interact with the shell and the behaviors of
other programs running in the system.
When applying SCMiner to diagnosing the bug generated
bybash , the goal is to identify abnormal system call se-
quences leading to the concurrency failure and their associatedapplication functions. In Figure 1, the system call sequence(the grey area) <open(Ô¨Åle), write(Ô¨Åle), write(Ô¨Åle) >from two
different bash processes indicates the root cause of the failure.
When one bash process (pid #11589) opens bash-3.0/history
Ô¨Åle before writing to it, another bash process (pid #11587)opens this Ô¨Åle too and writes to it. The failed executionproduces only one history message, whereas two messages areexpected from the two processes. This is because the secondprocess overwrites the message generated by the Ô¨Årst process.
The abnormal system call sequence is then mapped to theapplication functions, where the root cause is stemming from
the function history_do_write in the bash application.
This function is responsible for reading and writing the bashhistory Ô¨Åle.
Challenges. In practice, it is difÔ¨Åcult to localize the root
cause of abnormal system call sequence and the associatedapplication from only system call traces. The Ô¨Årst challenge isto identify the processes involved in the erroneous execution.In the above example, only the bash processes are actually
relevant. Therefore, we need to quickly weed out irrelevantprocesses. Moreover, in some cases, the failing process mightnot be the process that contains the bug. A bug in one processmay propagate to another process (e.g., when a corrupted Ô¨Ålegenerated by bash is accessed by a cat process and it is the
cat process that reports the error). The second challenge is
that a system call trace can easily become massive. Identifyingthe abnormal system call sequences are difÔ¨Åcult especially inthe absence of multiple failed executions, where existing faultlocalization techniques [13], [14] cannot be applied. Third,
even if the abnormal system call sequences are identiÔ¨Åed,
searching the buggy functions associated with them among thelarge number functions in the target application is challenging.For example, the bash program contains 456 functions. Since
the Linux system has only 33 system calls and a sequencecould appear in many functions, an exact match between the
system call names and the abnormal system call sequences
could return a number of irrelevant functions.
C. Principal Component Analysis
Principal component analysis (PCA) analyzes a data matrix
(X) where each row is an observation and each column is afeature. The data points in X are described by several inter-correlated quantitative dependent variables (features) [28]. Ifwe have data with a large number of features, some might
be correlated. The correlation between features can cause
redundancies in the information. Therefore, in order to reducethe computational cost and complexity, we can use PCA totransform the original features into their independent, linearcombinations (PCs) [29]. For example, in Figure 2a, we
displayed a 3-dimensional variable space and plotted the
observations.
Applying PCA to Xyields a set of mprincipal components.
The Ô¨Årst principal component (e.g., PC
1in Figure 2a) captures
the variance of the data to the greatest degree possible on asingle axis. The next principal components (PC
2to PC m) then
each captures the maximum variance among the remaining
orthogonal directions. Variance measures how far a data set isspread out, which provides us a general idea of the spread ofthe data [30]. Each observation (a dot in Figure 2a) may nowbe projected onto the PCs to obtain a coordinate value along
with each PC-line. This new coordinate value is known as the
PCA score [31].
In this way, we can identify the Ô¨Årst (k‚àí1)principal
components and conclude that the k
thprincipal component
corresponds to the maximum variance of the residual. Thedifference between the original data and the data mapped ontothe Ô¨Årst (k‚àí1)principal axes is called the residual [32].
Here, kis the number of dimensions required to capture at
least n%of the variance in data [33], [34]. Therefore, in
the case of abnormal items detection, where these items areassumed to be rare, PCA can capture the dominant itemsand construct a (k‚àí1)-dimensional normal subspace S
d.
The remaining dimensions construct the abnormal subspace
Sa. The abnormal items can be identiÔ¨Åed by calculating the

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. (a) First two PCs in a plane
 (b) A sample data set T
 (c) Score plot of PC1 and PC2
 (d) Loading plot of PC1 and PC2
Figure 2. Principle Component Analysis
distance of each item form the normal subspace. The item with
the longest distance from Sdis marked as an abnormal item.
An example. Figure 2b shows an example of applying PCA
to an example of dataset T. The dataset has Ô¨Åve features and
10 observations, which represents food consumption habit ofpeople from different countries. Each data point in the table
represents the percentage of the population in a country, who
eat a speciÔ¨Åc kind of food. After applying PCA to T,w e
Ô¨Ånd that the Ô¨Årst two principal components (PCs) can explainalmost 85 %of the data variance. Hence, kis set to 2, which
divides the original data set into ( k‚àí1) normal sub-spaces
and the rest as abnormal sub-spaces.
When plotting the PCA score vector for the example of
Figure 2b, data points that are correlated are placed together.Figure 2c shows that countries from the same regions aregrouped together. This is because people in the same region eatthe same kind of food. On the other hand, the country inside
the red circle, which is far from the other countries, and hasthe longest distance from the Ô¨Årst PC, indicates that it has a
distinct food consumption criteria (i.e., an ‚Äúabnormal‚Äù item).Likewise, we can Ô¨Ånd the correlation between the features(e.g., rice, bread, etc.) by plotting the loading vector of theÔ¨Årst two PCs. The loading vector contains the data in a rotated
coordinate [35]. Features contributing similar information aregrouped together, which means they are correlated. In this
example, the feature, seafood separates the country Stika from
the other countries. This country is characterized as having ahigh consumption of seafood. Therefore, we can conclude thatStika , which is the farthest country from the normal subspace
in Figure 2c, has some rare kind of food habit. By observing
the loading plot 2d, we can identify that, the feature seafood
has the strongest impact to make Stika ‚Äôs food consumption
criteria rare.
III. SCM
INER APPROACH
Figure 3 provides an overview of SCMiner. It consists
of two major steps: 1) Identifying abnormal system callsequences; 2) Mapping abnormal sequences into a rankedlist of buggy functions. To carry out the Ô¨Årst step, SCMinerprocesses the system call trace into trace segments that aresuitable for PCA by using Ô¨Åltering and a set of optimizationtechniques. It uses PCA to identify a set of potential abnormalinter-process system call sequences. To map these sequencesinto the application‚Äôs functions, SCMiner performs dynamic
analysis outside the production environment to extract func-tion signatures, where each function signature indicates thefrequently executed system call sequences within that function.It then uses function signatures to match against the abnormalsystem call sequences to identify and rank a list of functions
that are likely to be the root cause of the system-levelconcurrency failure.
A. Identifying Abnormal System Call Sequences
Algorithm in Figure 4 shows the steps of identifying ab-
normal system call sequences. The input to the algorithmincludes a set of system call traces Tcollected by built-in
tools, such as linux auditd daemon [19]. The output is a set
of potential abnormal system call sequences Seq
a. SCMiner
Ô¨Årst merges the traces into a single trace according to theirtimestamps in ascending order. It then extracts informationthat is relevant to system-level concurrency faults from the raw
system call traces (Line 4). Next, it groups related system calls
from the extracted information to construct feature vectors,i.e., a data table, for PCA (Lines 5-9). SpeciÔ¨Åcally, SCMinersplits the trace into segments (Line 5), where the segments areexpected to contain similar program behaviors (i.e., handlingan HTTP request), so system calls grouped into each segmentare intrinsically determined by program logic. SCMiner thenencodes the feature vector by generating a set of system callsequences from each segment and counting their appearance(Lines 6-9). Next, we apply PCA to analyze the feature vectorsfor Ô¨Ånding the most uncommon segments (Line 10). Finally,from those selected uncommon segments, SCMiner identiÔ¨Åes
the unique system call sequences, which describe the data
points that deviate from the others.
1) Extracting Relevant System Calls: Since our target is
diagnosing system-level concurrency failures, we need to
identify system calls and their associated processes that canpotentially lead to system-level concurrency failures. As dis-cussed in Section II-A, system-level concurrency faults are dueto incorrectly shared resource accesses between processes, so
a system call sis considered ‚Äúrelevant‚Äù if its associated shared
resource (passed as a parameter) is accessed by at least oneother process that is different from the one associated with s.
In addition, the execve system call is always considered
to be relevant because it indicates the start of the execution

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. /g11/g30/g27/g18/g33/g17/g32/g22/g27/g26
/g11/g2
/g11/g3
/g37
/g15/g33/g18/g22/g32/g18
/g13/g35/g31/g32/g19/g25/g1/g6/g15/g24/g24/g1
/g14/g30/g15/g17/g19/g8/g22/g24/g32/g19/g30/g22/g26/g21/g1
/g15/g26/g18/g1
/g10/g28/g32/g22/g25/g22/g36/g15/g32/g22/g27/g26
/g11/g30/g22/g26/g17/g22/g28/g15/g24/g1
/g6/g27/g25/g28/g27/g26/g19/g26/g32/g1
/g4/g26/g15/g24/g35/g31/g22/g31/g7/g34/g32/g30/g15/g17/g32/g22/g26/g21
/g10/g20/g20/g24/g22/g26/g19/g1
/g8/g33/g26/g17/g32/g22/g27/g26
/g13/g22/g21/g26/g15/g32/g33/g30/g19/g31
/g9/g18/g19/g26/g32/g22/g20/g35/g22/g26/g21/g1
/g5/g33/g21/g21/g35/g1
/g8/g33/g26/g17/g32/g22/g27/g26/g31/g12/g15/g26/g23/g22/g26/g21
/g5/g33/g21/g21/g35/g1
/g8/g33/g26/g17/g32/g22/g27/g26/g31/g4/g16/g26/g27/g30/g25/g15/g24/g1
/g13/g35/g31/g32/g19/g25/g1/g6/g15/g24/g24/g1
/g13/g19/g29/g33/g19/g26/g17/g19/g31
Figure 3. The overview of SCMiner framework
Abnormal System Call IdentiÔ¨Åcation Algorithm
1:Inputs: T
2:Outputs: Seq a
3:begin
4: TR‚ÜêExtractTrace (T)
5: Seg‚ÜêCreateSegments (TR)
6: foreach segi‚ààSeg
7: List seqi‚ÜêGenerateSequences (segi)
8: P V ector .update (List seqi)
9: endfor
10: PC‚ÜêComputePCA (Seg ,P V ector )
11: Seq a‚ÜêIdentifyAbnormalSeq (PC)
12: return Seq a
Figure 4. Identifying abnormal system call sequences
of a program, which will be used to build feature vectors
for PCA. Therefore, SCMiner iterates through all system callentries in the traces and retain only relevant system calls. Ourobservation on 19 real-world Linux applications shows that,
on average, only 23 %(Column NOS
fof Table III) system
calls are relevant. In the example of Figure 1, the system callsrelated to bash are retained for further analysis.
2) PCA-Based Anomaly Detection: We use principal com-
ponent analysis (PCA) ‚Äì an unsupervised learning approachto identify abnormal system call sequences from the relevanttraces. We use unsupervised learning because it does notrequire manually labeling the data to build training sets, whichneeds extensive manual effort and the large training data issometimes difÔ¨Åcult to obtain in the production environment.The key idea of using PCA in our context is to discover the sta-
tistically dominant system call segments and thereby detect therare segments, as well as the abnormal system call sequences
(i.e., outliers ) inside rare segments. The insight behind usingPCA is that we observe low effective dimensionality in thedata table, where each row (i.e., dimension) is a system callsegment corresponding to a certain program behavior (e.g.,
an HTTP request) and each column is a candidate abnormal
system call sequence.
Representing system call traces. We need to convert the trace
containing relevant system calls to a numerical representationsuitable for applying PCA detector. The whole set of systemcalls in the trace can be represented by an M*N matrix
(Section II-C). In SCMiner, each row (i.e., observation) in
the matrix corresponds to the trace segments by splitting the
traces. Each segment contains a set of consecutive systemcalls describing a certain program behavior (e.g., processing
an HTTP request).
For each segment, SCMiner generates a set of system call
sequences with different lengths to create feature vectors,
where each index in the vector represents a system call
sequence and the corresponding value represents the numberof times the sequence appears in the segment.
Table I shows an example of the numerical representation
of a system call trace. Here, each row indicates a vectorrepresentation of a trace segment. Each item in the vector is asequence of system calls, where a system call sc
svnindicates
system call scaccesses a shared resource svfrom the process
IDn.
Identifying trace segments. SCMiner splits the extracted
system call traces into Ô¨Åne-grained segments of closely relatedsystem calls. To do this, for each trace, SCMiner divides it
into a set of segments based on the execution system call
execv , where each segment begins with execv . The execv
is called when a new process starts and the Ô¨Årst parameterofexecv is the execution command. The intuition is that
most segments go through similar program execution paths andprocess interleaving patterns. This results in high correlation
and thus low intrinsic dimensionality, which is suitable for
applying PCA. For example, each time when the user issuesa command in bash , it will cause the execution to start
from main for triggering the execv system call. On the
other hand, the minority components may contain sequences
with interleaved system calls, which are the root causes ofconcurrency fault. However, any bug can occur during thetransition from one process to another, which means theexecv system call may also present in the buggy system call
sequence. To make sure that this kind of system call sequenceis detected by our technique, we keep the last system call S
s
form the previous segment as the Ô¨Årst system call of the new
segment.
Generating vector representations. SCMiner generates a
feature vector representation for each trace segment. Each item(feature) in the vector is a system call sequence, which isa candidate of the abnormal sequence. To generate a list of
features Ffor each vector, SCMiner Ô¨Årst identiÔ¨Åes sequences
of semantically related system calls according to the sharedresources. The intuition behind this is that most system-level concurrency failures occur in the case of a particularinterleaving of system calls accessing a shared resource [36].

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. The output of this step is a set of system sequences ( Seq sv),
where system calls in each sequence access the same sharedresource.
Each sequence in Seq
svcan still be long and may not be
helpful in understanding the bug. For example, in the Apache
[37] server bug, the length of sequence with respect to a
shared resource is 983. To reduce the size of the sequenceencoded as a feature in the vector, SCMiner utilizes the A-priori candidate generation algorithm [38] to generate a setof shorter sequences for each S
sv. Basically, the A-priori
candidate generation algorithm uses a lattice structure toenumerate the list of all possible item-sets [39], resulting inan overly expensive computational cost O( 2
N), where Nis
the number of system calls in Seq sv.
To minimize the number of system call sequences and
reduce the computational cost, SCMiner employs three op-timization methods. First, the traditional A-priori algorithm
[38] exhaustively computes the short sequences regardless of
the orders of the system calls in each execution. However,we need to consider the program execution Ô¨Çow and thuskeep only system call sequences actually appeared in thetrace. Therefore, the computational cost is reduced to O( N
2).
For example, given a sequence {S1,S2,S3,S4}inSeq sv,
our modiÔ¨Åed candidate generation algorithm will output six
instead of 16 sequences: {S1,S2},{S1,S2,S3},{S1,S2,S3},
{S2,S3},{S2,S3,S4},{S3,S4}. Each sequence is encoded
as a feature in the feature vector.
Second, SCMiner removes the system call sequences that
are not relevant to system-level concurrency bugs. SpeciÔ¨Åcally,a sequence is removed if both of them involve read access.
Third, we propose a sequence abstraction method to min-
imize the size of Seq
svand thus reduce the number of
short sequences generated by the A-priori algorithm. The key
idea is to detect system call sequences that are frequentlyexecuted sequences in all Seq
svs and replace each frequent
sequence with a symbolic name. We use frequent patternmining algorithm [38] to obtain the frequent sequences. Forexample, given Seq
sv1={S1,S2,S3,S4}andSeq sv2={S2,
S3,S5}. Suppose {S1,S2}is a frequent pattern, it is replaced
with a symbolic name A. As a result, Seq sv1={S1,A ,S4}
andSeq sv1={A,S5}.
In this case, the cost of candidate generation algorithm can
be reduced to O( (N‚àíP)2), where Nis the number of system
calls and Pis the number of frequent patterns. At the end of
the Ô¨Årst phase, if an abnormal system call contains a symbolicname, it will be replaced with the real system calls.
Ultimately, a feature vector is generated for each trace
segment, in which each item (or observation) corresponds toa system call sequence extracted from the segment and thevalue of the item indicates the number of times the sequenceappears in the segment. The size of the vector is the uniquenumber of system call sequences from all trace segments.
Applying PCA detector . We create a Feature Matrix Dto
perform PCA, where each row corresponds to a feature vectorfrom a trace segment. In the example of Table I, each columnis a feature (i.e., candidate system call sequence) and eachTable I
AN NUMERICAL REPRESENTATION OF A SYSTEM CALL TRACE
<open f2, read f2> <write f2, open f2> <read f2, write f2, stat f2>.
1 0 2.
... ... ...
Figure 5. Variance plot and biplot of PCA on bash traces
row is an observation (i.e., trace segment). PCA Ô¨Ånds a low-
dimensional representation of the Matrix Dthat contains as
much as possible of the variation [40].
In our benchmark programs, even though there are 200
segments on average, we found that 85 %of the variance can
be captured by Ô¨Åve principal components on average whichshows the low effective dimensionality in the feature metrics
of our benchmarks. For our feature vector, each dimension
corresponds to a certain execution sequence in the program.As the execution sequences are determined by the programlogic, the sequences in a group are correlated. In the passingexecutions, it is natural that we Ô¨Ånd most of the sequences tobe highly correlated with each other. For example, Figure 5
(left one) shows the plot of the variances (y-axis) associatedwith the PCs (x-axis). This indicates that only 5 principle
components can capture 86 %variance [30] of the data of bash
program which have 184 segments and 924 unique system callsequences. The plot in the right side of the Ô¨Ågure 5 represents
both the PCA score and loading in the normal subspace S
dand
in the abnormal subspace Sa. This plot indicates that segment
‚Äú4‚Äù has a signiÔ¨Åcantly different score than the other segmentsand has the longest distance from the normal subspace. Thus,we can separate this segment from the other data points.We then calculate the distances of the features and select
the unique features of segment 4 and obtain 38 system call
sequences. Furthermore, these features are less co-related withthe other features and mostly co-related with each other.
3) Finalize Abnormal System Call Sequences: With the
help of PCA, we isolate the anomalous system call sequencesand after that, we prepare different sets of them. In order toprepare the sets, we identify the shortest unique system callsequences Ô¨Årst. Then, sort out the supersets of a system callsequence and group them all in the same set. In the same set,the smallest system call sequence will be placed in the top.If a system call sequence does not have any supersets, weconsider that sequence as a set.
For example, we have Ô¨Åve system call sequences <P
1P2P3,

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. P1P8,P1,P4,P4P5>. These Ô¨Åve sequences can be divided
into two different sets 1. <P1,P1P8,P1P2P3>and 2. <P4,
P4P5>. The top item of a set is the most frequent subset of
all system call sequences of that set.
B. Localizing Buggy Functions in Applications
Mapping an abnormal system call sequence to speciÔ¨Åc bug-
related application functions is challenging, especially when
the source code is not available. We propose to leverage
off-line proÔ¨Åling to associate application functions with theabnormal system call sequences. SpeciÔ¨Åcally, We obtain sys-tem call traces using dynamic binary instrumentation outsidethe production environment and match against the abnormalsequence.
One challenge is that an ofÔ¨Çine execution trace is unlikely
to be exactly the same as the production trace due to envi-ronmental inconsistencies, the unavailable inputs, or the non-
deterministic interleavings. To address this problem, SCMinerproposes an ofÔ¨Çine function signature mapping method that
creates a signature for each function outside the productionenvironment. The signature is obtained by a set of closed fre-
quent system call sequences for each function across multiple
executions. Therefore, we can map the abnormal sequenceback to each function signature to determine the suspectedbuggy function.
The beneÔ¨Åts of using a signature are that we do not require
the exactly same inputs, environment, or workload to localizethe buggy functions. Since the mapping table is obtained
ofÔ¨Çine outside the production environment, it does not induce
production runtime overhead.
1) Extracting OfÔ¨Çine Function Signatures: Given an abnor-
mal system call sequence, we obtain the processes containedin the sequence as the process under debugging (PuDs). Wethen use PIN [41], a dynamic binary instrumentation tool, toinstrument PuDs and execute them against a set of randomly
generated inputs multiple times. At the end of each execution,we obtain a function execution list , where each entry in the list
contains system call numbers, resource ID, and the functionname associated with them.
Once all executions are Ô¨Ånished, SCMiner groups all entries
for all function execution lists by the same function andthe same process name together. Next, SCMiner extracts the
function signature, which is the maximal frequent system call
sequence, from each group. The signature can characterize thebehavior of a function. For example, suppose there are threefunction execution lists for a function fin application Ais:
<open, write, close >,<stat, open, read >, and <lstat, open,
write, close >. The function signature of fwith respect to R
(the minimum support) is <open, write, close >because it is
the maximal frequent system call sequence [42].
2) Identifying and Ranking Buggy Functions.: Algorithm in
Figure 6 shows the steps of localizing bug-related functions.SCMiner takes as input the ofÔ¨Çine function signatures ( SIG
A)
for PuDs and one item set of the abnormal system callsequences SC
ab. It outputs a list of top Nranked applicationBuggy Function IdentiÔ¨Åcation Algorithm
1:Inputs: SCab,SIG A
2:Outputs: Fbug
3:begin
4: foreach scin ordered SCab
5: foreach scAinsc
6: Rm‚ÜêSIG A.match ( scA)
7: Fbug.add(scA,Rm)
8: endfor
9: Fbug‚ÜêFbug.rank()
10: endfor
11: return Fbug
Figure 6. Algorithm for locating the buggy sequence in the buggy function
functions Fbugthat are likely to contain bugs. Each function
inFbugis associated with a ranking score.
SpeciÔ¨Åcally, SCMiner iterates through SCab, beginning with
the top-ranked system abnormal call sequence, and for eachfound sequence sc, SCMiner extracts the system calls sharing
the same application name into an application-speciÔ¨Åc systemcall sequence sc
A. For example, in a sc=<write, read,
write >, suppose the two writes are from the same function f1
and the read is from function f2, then scf1=<write, write >
andscf2=<read>. SpeciÔ¨Åcally, SCMiner treats each scAin
application Afrom scas a query and searches scAagainst all
function signatures in A. The search problem is formulated
as the the longest common sub-string matching problem. Thematching score R
mis determined by the percentage of the
matched system calls in each function signature. For example,suppose there are three function signatures F
1:<stat, read,
write >,F2:<unlink, rename, read >, and F3:<read, write >.
The abnormal system sequence scAis<read, write >. When
matching scAagainst the three functions, the scores are 2/3,
1/3, and, 1. Therefore, F3is ranked at the top.
IV . E XPERIMENTS
We developed SCMiner as a software tool based on several
open-source platforms. SpeciÔ¨Åcally, we used a Linux built-
in audit daemon auditd [19] to collect system traces. Our
abnormal system call sequence identiÔ¨Åcation algorithm isimplemented by SPMF [43], an open source data mining tooland Principle Component Analysis (PCA) library deÔ¨Åned inR programming language [44]. The ofÔ¨Çine trace collection in
fault localization is implemented by PIN [41].
In order to evaluate SCMiner, we consider three research
questions:
RQ1: How effective is SCMiner at localizing abnormal system
call sequences and buggy functions?RQ2: How efÔ¨Åcient is SCMiner at localizing abnormal system
call sequences and buggy functions?RQ3: What are the roles of PCA optimization and signature
function matching in improving the effectiveness and the
efÔ¨Åciency of SCMiner?
A. Benchmarks and Evaluation Metrics
All our benchmarks are real Linux applications with known
concurrency failures due to incorrectly shared resources be-

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Table II
BENCHMARK ,DESCRIPTION AND RESOURCE INFORMATION OF THE FAILURES
Application NLOC NOF Bug ID Bug Description NOP NOR NOSR NOS
mv 7,002 77 Bugzilla-438076 another process terminates (‚ÄúÔ¨Åle is missing‚Äù) 96 148 8 413,345
rm 5525 76 Bugzilla-1211300 rm terminates (‚Äúdirectory not empty‚Äù) 234 165 6 762,104
mkdir 4,213 26 Debian-304556 Ô¨Åle permission mode is modiÔ¨Åed 68 144 6 576,210
mknod 3,840 26 Debian-304556 Ô¨Åle permission mode is modiÔ¨Åed 72 177 6 606,628
mkÔ¨Åfo 3,959 26 Debian-304556 Ô¨Åle permission mode is modiÔ¨Åed 64 139 6 534,983
ln 3,890 81 Debian-357140 ln terminates (‚ÄúÔ¨Åle does not exist‚Äù) 115 251 8 588,233
tail 4,492 104 Changelog output not updated after attached process exits 193 242 12 680,640
chmod 3,983 57 GNU-11108 Ô¨Åle permission mode is modiÔ¨Åed 56 79 6 277,520
pxz 370 5 Bugzilla-1182024 Ô¨Åle permission mode is modiÔ¨Åed 79 134 6 327,402
cp1 4,010 70 Changelog Ô¨Åle permission mode is modiÔ¨Åed 148 269 8 685,872
cp2 4,132 70 Changelog Directory creates fails(‚Äúdirectory exists‚Äù) 161 268 6 417,944
gzip 7,252 35 Debian-303927 Ô¨Åle permission mode is modiÔ¨Åed 112 232 8 831,420
bzip2 9,263 136 Debian-303300 Ô¨Åle permission mode is modiÔ¨Åed 84 110 8 457,884
bash 39,102 456 Debian-283702 corrupted history Ô¨Åle 287 424 31 3,059,987
Ô¨Åndutils 32538 271 Debian 67782 new database would be empty 292 342 26 916,842
lighttpd-1 37,919 883 Lighttpd-2217 http timeout 264 284 18 4,356,560
lighttpd-2 41,292 927 Lighttpd-2542 incorrect output 296 438 21 1,264,552
apache 195,005 5665 Apache -43696 server shutdown command is ignored 534 2529 29 3,629,040
locate 32,538 271 Debian 461585 File is missing 284 367 17 894,480
NLOC = the number of non-comment lines of code. NOF = the number of functions. NOP = the number of processes. NOR = the
number of unique system-wide resources accessed by the system calls in the log. NOSR = the number of unique system-wide shared
resources accessed by the system calls in the log. NOS = the number of system calls.
tween processes and/or signal handlers. These benchmarks are
identiÔ¨Åed by searches from open-source repositories such asGNU, Bugzilla, and Debian. There are 19 program versions
from 17 unique applications, among which 12 applications
were from Linux Coreutils. To minimize bias, searches fromthese open-source repositories are conducted by a student whois not involved in the SCMiner project. These benchmarkshave been used in other research [18], [45] for handlingprocess-level concurrency bugs. The total number of bench-
marks in this experiment is also comparable with prior work.
The student collected a system call trace for each benchmark
by running multiple test cases multiple times and at least one
execution can trigger the failure described in the bug report.
The ofÔ¨Çine traces are collected by running a set of black-box (or functional) test cases to mimic the production runsagainst different input scenarios. The black-box test cases areoften designed based on system parameters and knowledge of
functionality [46]. The student followed this approach, usingthe category-partition method [47], which employs a Test
SpeciÔ¨Åcation Language (TSL) to encode choices of parametersand environmental conditions that affect system operations andcombine them into test cases. However, we did not know theroot causes of these failures until we Ô¨Ånished running and
analyzing the results of SCMiner. Table II shows the statistics
for each benchmark. The last column indicates the size of thesystem call traces.
B. Evaluation Metrics
Identifying abnormal system call sequences. To evaluate the
effectiveness of abnormal system call sequence identiÔ¨Åcation,we use the measurement of precision [48]. Precision representsthe percentage of the ground truth (i.e., the actual abnormal)system call sequences from the system call sequence generatedby our technique. To determine the ground truth, we manually
examined the solution discussed in the corresponding issue
report and the patch used for Ô¨Åxing the issue.Localizing buggy functions. SCMiner reports top- Nfunc-
tions that are likely to be buggy and by default, N=20. In
order to assess the effectiveness of localizing buggy functions,we measure two metrics. The Ô¨Årst metric measures the ranknumber (position) of functions identiÔ¨Åed as bug-related. Again,the ground truths are determined by manually examining the
solution discussed in the corresponding issue report and thepatch used for Ô¨Åxing the issue.
For the second metric, we use Mean Average Precision
(MAP). MAP is a single-Ô¨Ågure measure of ranked retrievalresults independent of the size of the top list [49]. It is designedfor general ranked retrieval problems, where a query canhave multiple relevant documents (e.g., an abnormal system
call sequence may associate with more than one function),
we compute the average ranking. To compute MAP, it Ô¨Årstcalculates the average precision (AP) for each individual queryQ
i, and then calculates the mean of APs on the set of queries:
MAP =1
|Q|¬∑/summationdisplay
Qi‚ààQAP(Qi)
To illustrate the MAP calculation, suppose there are bug-
related functions f1andf2If Technique-I ranks the two
options at the 1stand2ndpositions among all 500 functions
and Technique-II ranks the two functions at the 1stand3rd
positions, then the MAP of Technique-I is (1/1 + 2/2) / 2 = 1
and the MAP of Technique-II is (1/1 + 2/3 )/2= 0.8.
C. Results and Analysis
Table III summarizes the results of applying SCMiner to the
benchmark programs. The results showed that 83 %of system
calls were removed after the Ô¨Åltering process. Column SCseq
shows the abnormal system call sequence, in the format of
SystemCall process . Column Func shows the function names
associated with the abnormal system call sequence.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Table III
RESULTS OF APPLYING SCM INER OVER BENCHMARK APPLICATIONS
Prog NOS f#Seg. #Ftr . Syscall seq. Func.Location Root Cause Time
Seq. prec. Rank MAP SCseq Func (sec)
mv 19260 189 223 1 100 1,2 1 unlink mv,open cat,rename mv mv: copy internal(), cat: main() 15.74
rm 23080 184 640 1 100 1,3,4 0.81 fstatat rm,symlink ln,opentat rm rm: fts open(), fts build(), ln: do link() 29.083
mkdir 7920 135 35 1 100 1,2 1 mkdir mkdir ,symlink ln,chmod mkdir mkdir: main(), ln: do link() 19.42
mknod 7209 148 55 1 100 1,2 1 mknod mknod ,symlink ln,chmod mknod mknod: main(), ln: do link() 23.89
mkÔ¨Åfo 7200 148 55 1 100 1,2 1 mknod mkfifo ,symlink ln,chmod mkfifo mkÔ¨Åfo: main(), ln: do link() 21.857
ln 5376 187 36 1 100 1,2 1 stat ln,unlink rmsymlink ln ln: do link(), rm: remove entry() 16.69
tail 21600 200 120 2 100 1,3,4 0.8 read tail,rename mv,fstat tail tail: tail forever inotify(), dump reminder(), mv: copy internal() 22.302
chmod 4278 179 21 1 100 1,2,3 1 stat chmod ,symlink ln,fchmodat chmod chmod: fts open(), main(), ln: do link() 9.86
pxz 30803 181 171 279.74 1,2 1 umask pxz,symlink ln,chmod pxz pxz: main(), ln: do link() 31.112
cp1 9446 190 74 2 100 2,3 0.58 mkdir cp,fchmod chmod ,stat cp cp1: copy internal() chmod: main() 20.28
cp2 16728 190 105 2 75 2,3 0.58 stat cp,mkdir mkdir ,mkdir cp cp2: copy internal(), mkdir: main() 27.78
gzip 10560 190 153 2 100 1,2 1 close gzip ,symlink ln,chmod gzip gzip: treat Ô¨Åle(), ln: do link() 19.97
bzip2 16665 200 190 1 100 1,2 1 close bzip2,symlink ln,chmod bzip2 bzip2: compressStream(), compress(), ln: do link() 29.76
bash 236096 174 924 2 100 1 1 open bash 1,write bash 2,write bash 1 bash: history dowrite() 208.999
Ô¨Åndutils 150967 180 843 1 100 1,2 1 unlink mv,opentat rm,rename mv mv: copy internal(), rm: rm() 61.52
lighttpd-1 731663 398 3240 478.28 1,4 0.75 exit cgi,rtsigreturn light ,wait light lighttpd: fdevent event del() 230.45
lighttpd-2 293367 292 1711 266.67 1,2,4 0.92 close light ,wait cgi,wait light lighttpd: fdevent unregister(), plugins call handle subrequest 178.32
apache 1661990 320 56953 3 100 2,3 0.58 rtsigpromask httpd ,rtsigaction bash apache: ap mpm run(), bash: set signal handler() 273.62
locate 144824 186 427 1 100 1,2 1 unlink mv,fchmodat chmod ,rename mv mv: copy internal(), chmod: fts open() 53.63
NOS f= the number of system calls after Ô¨Åltering. # Seg. = the number of segments. # Ftr . = the number of features (system call sequences). #Seq = the
number of abnormal sequence sets. Rank = the ranking position of the ground truth. MAP = the MAP score. SCseq = the abnormal system call sequence.
Func = the buggy functions. Time = the time spent on the analysis.
1) RQ1: Effectiveness of SCMiner: SCMiner is successful
inÔ¨Ånding abnormal system call sequences and bug-related
functions in all 19 programs. The number of abnormal systemcall sequences computed by SCMiner ranged from 1 to 4.The size of each system call sequence ranged from 2 to 4
across all applications. Given the total number of system callsin the trace (Column ‚ÄúNOS‚Äù in Table II), the results indicatethat developers only need to examine from 0.01 %to 0.04 %
system calls among all system calls in the trace, with anaverage of 0.02 %. The results also show that the identiÔ¨Åcation
of the buggy system call sequence is 66.67 %to 100 %precise
(Column ‚Äúprec.‚Äù in Table III), with an average of 93 %for
all benchmark applications. In addition, SCMiner successfullylocalized buggy functions in all 19 applications. The average
ranking position is 1.3 overall applications. The ‚Äúrank‚Äù columncontains multiple ranks because we have multiple ground-truth functions. The MAP score ranged from 0.58 to 1, withan average of 0.79. The MAP score indicates that all buggyfunctions identiÔ¨Åed SCMiner are ranked at the top-5. Giventhe total number of functions in a program (Column ‚ÄúNOF‚Äù inTable II), developers are required to examine at most 0.04 %to
20%of all functions across all applications, with an average
of 5%.
We conclude that SCMiner is effective at detecting abnormal
system call sequences and localizing buggy functions withrespect to system-level concurrency failures in production.
2) RQ2: EfÔ¨Åciency of SCMiner: The last column of Table
III reports the end-to-end total run time of SCMiner, includingÔ¨Åltering, PCA analysis, optimization, and buggy function
localization. The overhead of collecting system call traces by
the auditd daemon is almost negligible, ranging from zero to
2X, with an average of 0.31X overall applications.
For the binary instrumentation used to localize buggy
functions, the overhead ranged from 1.3X to 36X, with anaverage of 8.5X. These overheads are in the similar order of
magnitude as that of other proÔ¨Ålers [50], [51]. We considerthese overheads to be acceptable for out-of-production usage,which is the intended usage of collecting function signatures.
The above results indicate that SCMiner is efÔ¨Åcient and
practical for being used for localizing system-level concur-
rency faults.
3) RQ3: The Role of Optimization and Function Signature:
To evaluate the role of the optimization techniques used inÔ¨Ånding abnormal system calls (i.e., removing irrelevant sys-tem calls, sequence abstraction), we computed the total timeof SCMiner without optimization, denoted by SCMiner
nop.
Figure 7 shows the time spent by SCMiner and SCMiner nop,
respectively. Compared to SCMiner nop, SCMiner is 1.5 times
faster on average in terms of the end-to-end analysis timeacross all applications. The speedup is more signiÔ¨Åcant inlarger applications (e.g., bash ,apache ). This is primarily
because the optimization reduced the size of feature vectors
used for PCA, reduced the overall number of system call se-quences, and thus also reduced the time of searching frequent
system call sequences in the source code. Overall, these resultsindicate that the use of optimization techniques contributed to
enhancing the efÔ¨Åciency of SCMiner .
To evaluate whether the use of function signatures can
improve the effectiveness of identifying buggy functions, weuse a baseline version SCMiner
nfsto compare with SCMiner.
SCMiner nfs does not compute function signatures. Instead,
it collects a single trace outside the production environmentand then uses a simple exact string matching approach [52]to determine if an abnormal system call belongs to certain
functions. For example, a system call from the buggy sequence
is considered as a query and will be searched in the systemcall sequence of the single execution trace. Figure 8 shows theMAP scores of both SCMiner and SCMiner
nfs across the 19

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Figure 7. Comparing the time taken by SCMiner and SCMiner nop
Figure 8. Comparing the effectiveness of SCMiner and SCMiner nfs
applications. The results show that the use of function signa-
ture increased the effectiveness of SCMiner in 16 applications,ranging from 0.3 %to 100 %, with an average of 44 %. On the
bash program, we observed a 100 %improvement because
the buggy function was not ranked in the top 20 functions bySCMiner
nfs.
The above results indicate that the function signature tech-
nique is more effective in localizing buggy functions than asimple string matching approach.
V. L
IMITATIONS AND DICUSSION
A. Limitations
SCMiner assumes each logged system call contains suf-
Ô¨Åcient information on resources being accessed. SCMinermay not process logs, in which resource information is notavailable in each system call. Second, function signatures are
collected from the execution traces. Therefore, the accuracy
of the signatures largely depend on the quality of inputs.Existing automated test case generation techniques [47] can beleveraged to cover as many functions as possible for improvingthe quality of traces.
B. Discussion
Quality of traces. We investigated how the quality of system
call traces inÔ¨Çuence the effectiveness of SCMiner. We variedthe percentage of the passing and failing executions in thelog under analysis. As shown in Figure 9, the x-axis indicates
Figure 9. Precision changes with the content of traces.
Figure 10. Precision changes with the size of traces.
the ratio of the percentage of passing and failing executionsand the y-axis indicates the precision scores of SCMiner indetecting abnormal system call sequences. For example, the
ratio score 9 means, there are 90 %passing executions and 10 %
failing executions. The precision score is 0 %when the buggy
sequence cannot be captured by SCMiner and it happens whenthe failing executions occupy a large percentage than thepassing executions. These results show that there is a trendwhen the ratio between the passing and failing executions
increases, the precision increases. The precision score reachesits peak for all applications when the percentage of passing
executions is about 90 %.
The above results indicate that SCMiner is most useful when
the number of normal system call sequences is a dominantmajority in the trace and they appear frequently. This is due
to the PCA algorithm used in the approach.
Scalability. We further examined the effectiveness and efÔ¨Å-
ciency of SCMiner when handling system call traces withdifferent sizes. In addition to the original traces, we consider
two variations of the original traces generated from the 19
applications: 1) small-size trace and 2) large-size trace. Tocreate small-size traces, we removed 50 %of executions from
each original trace. To create large-size traces, we added anadditional 50 %of executions to each original trace.
Figure 10 plots the precision scores of SCMiner. The
results indicate that precision varied on all applications when
changing the size of the trace from ‚Äúsmall‚Äù to ‚Äúoriginal‚Äù.On all applications, the precision scores generally remainthe same when the trace size is increased from ‚Äúoriginal‚Äù to

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. Figure 11. Time changes with the size of traces.
‚Äúlarge‚Äù. Figure 11 plots the efÔ¨Åciency. The results indicate that
compared to the original traces, the time spent on analyzingsmall traces was 88% (7.30 seconds on average) less andthat on analyzing large traces was 53% (104.24 seconds onaverage) more.
The above results imply that SCMiner is able to handle
large-size traces with little extra cost.
VI. R
ELATED WORK
Fault localization for concurrent programs. There has
been a lot of work on fault localization for concurrent pro-grams [6], [16], [53]. For example, Park et al. [6] monitormemory-access patterns associated with a program‚Äôs pass/failresults. Wang et al. [15] identify shared memory access pairs
that behave distinctively in failed and successful runs, and
pinpoint root causes using different test procedures. Thistechnique targets order violations and does not rank concur-rency violation patterns. CCI [13] ranks only shared variableaccesses (predicates) and thus provides less contextual infor-mation. However, these techniques assume multiple failing
and passing executions, which are often hard to obtain in
practice. In addition, they require instrumenting memory ac-cesses and thus are intended to work outside of the productionenvironment. In contrast, SCMiner is a production-level faultlocalization tool that uses system-generated system call traceswith little overhead. In addition, SCMiner assumes that failing
executions happen more rarely than normal executions, whichis a practical assumption in the production environment.
Process-level concurrency failures. RacePro [11] leverages
the vector-clocks algorithm to detect a process-level race if
it happens during test execution. It tracks the accesses of
shared kernel resources via system calls and records exe-cutions of multiple processes. SimRacer [18] and RacePro[11] aim to detect process-level concurrency faults by testingfor different interleavings of system calls. Descry [17] can
reproduce system-level concurrency failures by combining
static and dynamic analysis techniques to generate test inputs.[11] However, all of these techniques have different goals fromSCMiner; none of them focus on detecting abnormal systemcalls from traces or localizing buggy functions.
Anomaly detection from runtime logs. There has been
some research on detecting anomalies [33], [54], [55] fromlogs. For example, Xu et al. [33], [54] mine console logs andidentify the abnormal log message patterns. Liu et al. [55]and Du et al. [56] analyze the characteristics of system logsto identify the abnormal behaviors of a system that are caused
by attacks. Lakhina et al. [32] use PCA anomaly detectionalgorithm to diagnose network-wide trafÔ¨Åc anomalies. Thismethod uses Principal Component Analysis to identify ananomalous subspace of the network trafÔ¨Åc which is noisier andcontains signiÔ¨Åcant trafÔ¨Åc spikes. In contrast, SCMiner focuseson Ô¨Ånding system call sequences for diagnosing system-levelconcurrency faults. Moreover, SCMiner can pinpoint the rootcauses of failures associated with abnormal system calls.
VII. C
ONCLUSIONS
We have presented SCMiner, the Ô¨Årst automated tool to
diagnose system-level concurrency failures in multi-processapplications. SCMiner can detect abnormal system call se-
quences from the traces generated by the default systemauditd daemon by using a combination of dynamic analysis,data mining, and statistical analysis techniques. SCMiner can
also localize buggy application functions associated with the
abnormal system call traces. We have evaluated SCMiner on19 real-world multi-process applications. The results showedthat SCMiner is both effective and efÔ¨Åcient in diagnosingsystem-level concurrency failures.
R
EFERENCES
[1] R. Capuano, ‚ÄúInteractive visualization of concurrents programs,‚Äù in 19th
IEEE International Conference on Automated Software Engineering(ASE) , 2004, pp. 418‚Äì421.
[2] G. Altekar and I. Stoica, ‚ÄúOdr: Output-deterministic replay for multicore
debugging,‚Äù in Proceedings of the ACM SIGOPS 22Nd Symposium on
Operating Systems Principles (SOSP) , 2009, pp. 193‚Äì206.
[3] H. Cleve and A. Zeller, ‚ÄúLocating causes of program failures,‚Äù in
International Conference on Software Engineering , 2005, pp. 342‚Äì351.
[4] J. Clause and A. Orso, ‚ÄúA technique for enabling and supporting
debugging of Ô¨Åeld failures,‚Äù in International Conference on Software
Engineering , 2007, pp. 261‚Äì270.
[5] A. V . Thakur, R. Sen, B. Liblit, and S. Lu, ‚ÄúCooperative crug isolation,‚Äù
inProceedings of the International Workshop on Dynamic Analysis:
held in conjunction with the ACM SIGSOFT International Symposiumon Software Testing and Analysis (ISSTA), (WODA) , 2009, pp. 35‚Äì41.
[6] S. Park, R. W. Vuduc, and M. J. Harrold, ‚ÄúFalcon: fault localization in
concurrent programs,‚Äù in Proceedings of the 32nd ACM/IEEE Interna-
tional Conference on Software Engineering - V olume 1, ICSE , 2010, pp.
245‚Äì254.
[7] J.-D. Choi and A. Zeller, ‚ÄúIsolating failure-inducing thread schedules,‚Äù
inProceedings of the 2002 ACM SIGSOFT International Symposium on
Software Testing and Analysis , ser. ISSTA ‚Äô02, 2002, pp. 210‚Äì220.
[8] H. Cleve and A. Zeller, ‚ÄúLocating causes of program failures,‚Äù in Pro-
ceedings of the 27th International Conference on Software Engineering,(ICSE) , 2005, pp. 342‚Äì351.
[9] N. Jalbert and K. Sen, ‚ÄúA trace simpliÔ¨Åcation technique for effective
debugging of concurrent programs,‚Äù in Proceedings of the Eighteenth
ACM SIGSOFT International Symposium on F oundations of Software
Engineering (FSE) , 2010, pp. 57‚Äì66.
[10] J. Huang, P. Liu, and C. Zhang, ‚ÄúLEAP: lightweight deterministic
multi-processor replay of concurrent java programs,‚Äù in Proceedings of
the 18th ACM SIGSOFT International Symposium on F oundations ofSoftware Engineering , 2010, pp. 207‚Äì216.
[11] O. Laadan, N. Viennot, C.-C. Tsai, C. Blinn, J. Yang, and J. Nieh, ‚ÄúPer-
vasive detection of process races in deployed systems,‚Äù in Proceedings
of the Twenty-Third ACM Symposium on Operating Systems Principles(SOSP) , 2011, pp. 353‚Äì367.
[12] C. ZamÔ¨År and G. Candea, ‚ÄúExecution synthesis: A technique for
automated software debugging,‚Äù in Proceedings of the 5th European
Conference on Computer Systems (EuroSys) , 2010, pp. 321‚Äì334.
[13] A. Thakur, R. Sen, B. Liblit, and S. Lu, ‚ÄúCooperative crug isolation,‚Äù in
Proceedings of the Seventh International Workshop on Dynamic Analysis(WODA) , 2009, pp. 35‚Äì41.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. [14] S. Park, R. W. Vuduc, and M. J. Harrold, ‚ÄúFalcon: Fault localization
in concurrent programs,‚Äù in International Conference on Software En-
gineering , 2010, pp. 245‚Äì254.
[15] W. Wang, C. Wu, P. Yew, X. Yuan, Z. Wang, J. Li, and X. Feng,
‚ÄúConcurrency bug localization using shared memory access pairs,‚Äù inACM SIGPLAN Symposium on Principles and Practice of ParallelProgramming, (PPoPP) , 2014, pp. 375‚Äì376.
[16] S. Park, R. W. Vuduc, and M. J. Harrold, ‚ÄúA uniÔ¨Åed approach for
localizing non-deadlock concurrency bugs,‚Äù in Fifth IEEE International
Conference on Software Testing, V eriÔ¨Åcation and V alidation, ICST , 2012,
pp. 51‚Äì60.
[17] T. Yu, T. S. Zaman, and C. Wang, ‚ÄúDESCRY: reproducing system-level
concurrency failures,‚Äù in Proceedings of the 2017 11th Joint Meeting on
F oundations of Software Engineering, ESEC/FSE , 2017, pp. 694‚Äì704.
[18] T. Yu, W. Srisa-an, and G. Rothermel, ‚ÄúSimracer: An automated frame-
work to support testing for process-level races,‚Äù in Proceedings of the
2013 International Symposium on Software Testing and Analysis , ser.
ISSTA 2013, 2013, pp. 167‚Äì177.
[19] auditd(8) - linux man page. [Online]. Available:
https://linux.die.net/man/8/auditd
[20] (2008) Principal component analysis (pca) procedure. [Online].
Available: https://onlinecourses.science.psu.edu/stat505/node/51/
[21] D. Engler and K. Ashcraft, ‚ÄúRacerx: Effective, static detection of race
conditions and deadlocks,‚Äù in Proceedings of the Nineteenth ACM
Symposium on Operating Systems Principles (SOSP) , 2003, pp. 237‚Äì
252.
[22] M. Naik, A. Aiken, and J. Whaley, ‚ÄúEffective static race detection
for java,‚Äù in Proceedings of the 27th ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI) , 2006, pp.
308‚Äì319.
[23] Scminer. [Online]. Available: https://github.com/Tarannum-
Zaman/Scminer
[24] F. Valsorda. Searchable linux syscall table for x86 and x86
64.
[Online]. Available: https://Ô¨Ålippo.io/linux-syscall-table/
[25] O. Laadan, N. Viennot, C.-C. Tsai, C. Blinn, J. Yang, and J. Nieh,
‚ÄúPervasive detection of process races in deployed systems,‚Äù in ACM
symposium on Operating Systems Principles , 2011, pp. 353‚Äì367.
[26] (2004) Debian bug report logs - #283702. [Online]. Available:
https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=283702
[27] Bash guide for beginners. [Online]. Avail-
able: https://www.tldp.org/LDP/Bash-Beginners-Guide/html/ sect
0101.html
[28] H. Abdi and L. J. Williams, ‚ÄúPrincipal component analysis,‚Äù Wiley
interdisciplinary reviews: computational statistics , vol. 2, no. 4, pp. 433‚Äì
459, 2010.
[29] J. Shlens, ‚ÄúA tutorial on principal component analysis,‚Äù in Systems
Neurobiology Laboratory, Salk Institute for Biological Studies , 2005.
[30] S. H. To. (2019) Variance: Simple deÔ¨Åni-
tion, step by step examples. [Online]. Avail-able: https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/variance/
[31] L. Eriksson. (2018) What is principal component analysis (pca) and
how it is used? [Online]. Available: https://blog.umetrics.com/what-is-principal-component-analysis-pca-and-how-it-is-used
[32] A. Lakhina, M. Crovella, and C. Diot, ‚ÄúDiagnosing network-wide trafÔ¨Åc
anomalies,‚Äù in Proceedings of the 2004 Conference on Applications,
Technologies, Architectures, and Protocols for Computer Communica-tions (SIGCOMM) , 2004, pp. 219‚Äì230.
[33] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan, ‚ÄúDetecting
large-scale system problems by mining console logs,‚Äù in Proceedings of
the ACM SIGOPS 22Nd Symposium on Operating Systems Principles(SOSP) , 2009, pp. 117‚Äì132.
[34] I. Jolliff, Principal Component Analysis , 2nd ed. Springer, 2002.
[35] H. Lohninger. (2012) Pca - loadings and scores. [Online]. Available:
http://www.statistics4u.com/ fundstat
eng/ccpcaloadscore .html[36] S. Lu, S. Park, E. Seo, and Y . Zhou, ‚ÄúLearning from mistakes: A
comprehensive study on real world concurrency bug characteristics,‚ÄùinInternational Conference on Architectural Support for Programming
Languages and Operating Systems , 2008, pp. 329‚Äì339.
[37] ‚ÄúApache Deadlock,‚Äù 2003, http://marc.info/?l=apache-httpd-
bugs&m=105967988713871.
[38] R. Agrawal and R. Srikant, ‚ÄúFast algorithms for mining association rules
in large databases,‚Äù in Proceedings of the 20th International Conference
on V ery Large Data Bases (VLDB) , 1994, pp. 487‚Äì499.
[39] P.-N. Tan, M. Steinbach, A. Karpatne, and V . Kumar, Introduction to
Data Mining (2Nd Edition) , 2nd ed. Pearson, 2018.
[40] S. M. Holland. (2008) Principal components analysis (pca). [Online].
Available: https://strata.uga.edu/software/pdf/pcaTutorial.pdf
[41] (2012) Pin - a dynamic binary instrumentation tool. [On-
line]. Available: https://software.intel.com/en-us/articles/pin-a-dynamic-binary-instrumentation-tool
[42] B. Ziani and Y . Ouinten, ‚ÄúMining maximal frequent itemsets: A java
implementation of fpmax algorithm,‚Äù in Proceedings of the 6th Interna-
tional Conference on Innovations in Information Technology (IIT) , 2009,
pp. 11‚Äì15.
[43] P. Fournier-Viger, A. Gomariz, T. Gueniche, A. Soltani, C.-W. Wu, and
V . S. Tseng, ‚ÄúSpmf: A java open-source pattern mining library,‚Äù J. Mach.
Learn. Res. , vol. 15, no. 1, pp. 3389‚Äì3393, Jan. 2014.
[44] (2014) Principal component analysis based un-
supervised anomaly detection. [Online]. Available:http://www.bistaumanga.com.np/blog/pccAnomalyProbe/
[45] 2016, http://cs.uky.edu/ tyu/research/descry.[46] A. Causevic, D. Sundmark, and S. Punnekkat, ‚ÄúAn industrial survey
on contemporary aspects of software testing,‚Äù in IEEE International
Conference on Software Testing, V eriÔ¨Åcation and V alidation , 2010, pp.
393‚Äì401.
[47] T. J. Ostrand and M. J. Balcer, ‚ÄúThe category-partition method for
specifying and generating fuctional tests,‚Äù Commun. ACM , pp. 676‚Äì686,
1988.
[48] W. Koehrsen, ‚ÄúBeyond accuracy: Precision and recall,‚Äù
https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c, March 3 2018.
[49] P. R. . H. S. Christopher D. Manning. (2008) Evaluation of
ranked retrieval results. [Online]. Available: https://nlp.stanford.edu/IR-
book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html
[50] L. Gong, M. Pradel, and K. Sen, ‚ÄúJITProf: Pinpointing JIT-unfriendly
JavaScript code,‚Äù in ACM SIGSOFT Symposium on F oundations of
Software Engineering , 2015, pp. 357‚Äì368.
[51] A. Nistor, L. Song, D. Marinov, and S. Lu, ‚ÄúToddler: Detecting perfor-
mance problems via similar memory-access patterns,‚Äù in International
Conference on Software Engineering , 2013, pp. 562‚Äì571.
[52] A. Reyes. (2015) Exact string matching algorithms. [Online]. Avail-
able: https://www.hackerearth.com/practice/notes/exact-string-matching-algorithms/
[53] J. Pei, J. Han, B. Mortazavi-Asl, J. Wang, H. Pinto, Q. Chen, U. Dayal,
and M. Hsu, ‚ÄúMining sequential patterns by pattern-growth: The pre-Ô¨Åxspan approach,‚Äù IEEE Trans. Knowl. Data Eng. , vol. 16, no. 11, pp.
1424‚Äì1440, 2004.
[54] W. Xu, L. Huang, A. Fox, D. Patterson, and M. Jordan, ‚ÄúOnline system
problem detection by mining patterns of console logs,‚Äù in Proceedings of
the 2009 Ninth IEEE International Conference on Data Mining (ICDM) ,
2009, pp. 588‚Äì597.
[55] Z. Liu, T. Qin, X. Guan, H. Jiang, and C. Wang, ‚ÄúAn integrated method
for anomaly detection from massive system logs,‚Äù IEEE Access , vol. 6,
pp. 30 602‚Äì30 611, 2018.
[56] M. Du, F. Li, G. Zheng, and V . Srikumar, ‚ÄúDeeplog: Anomaly detection
and diagnosis from system logs through deep learning,‚Äù in Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communica-tions Security (CCS) , 2017, pp. 1285‚Äì1298.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:25:32 UTC from IEEE Xplore.  Restrictions apply. 