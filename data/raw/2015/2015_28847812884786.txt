Program Synthesis using Natural Language
Aditya Desai
IIT Kanpur
adityapd@cse.iitk.ac.inSumit Gulwani
MSR Redmond
sumitg@microsoft.comVineet Hingorani Nidhi Jain
IIT Kanpur
viner,nidhij@cse.iitk.ac.in
Amey Karkare
IIT Kanpur
karkare@cse.iitk.ac.inMark Marron
MSR Redmond
marron@microsoft.comSailesh R Subhajit Roy
IIT Kanpur
sairaj,subhajit@cse.iitk.ac.in
ABSTRACT
Interacting with computers is a ubiquitous activity for millions of
people. Repetitive or specialized tasks often require creation of
small, often one-off, programs. End-users struggle with learning
and using the myriad of domain-speciÔ¨Åc languages (DSLs) to ef-
fectively accomplish these tasks.
We present a general framework for constructing program syn-
thesizers that take natural language (NL) inputs and produce ex-
pressions in a target DSL. The framework takes as input a DSL
deÔ¨Ånition and training data consisting of NL/DSL pairs. From
these it constructs a synthesizer by learning optimal weights and
classiÔ¨Åers (using NLP features) that rank the outputs of a keyword-
programming based translation. We applied our framework to three
domains: repetitive text editing, an intelligent tutoring system, and
Ô¨Çight information queries. On 1200+ English descriptions, the re-
spective synthesizers rank the desired program as the top-1 and top-
3 for 80% and 90% descriptions respectively.
1. INTRODUCTION
Program synthesis is the task of automatically synthesizing a
program in some underlying domain-speciÔ¨Åc language (DSL) from
a given speciÔ¨Åcation [12]. Traditional program synthesis, synthe-
sizing programs from complete speciÔ¨Åcations [18, 35, 51, 52], has
not yet seen wide adoption due to the difÔ¨Åcultly of writing such
speciÔ¨Åcations and verifying the synthesized program satisÔ¨Åes this
speciÔ¨Åcation.
Recent work has experimented with another class of (possibly in-
complete) speciÔ¨Åcations, namely examples [7, 13, 14, 17, 31]. Pro-
gramming by Example (PBE) systems have seen much wider adop-
tion, thanks to the ease of providing such a speciÔ¨Åcation. However,
they can suffer in cases where many examples are required to ac-
curately specify intent or where examples are difÔ¨Åcult to construct.
The classic L* algorithm [3], a PBE system for describing a regu-
lar language, has the well-known drawback of requiring too many
examples. A domain such as ATIS queries (Air Travel Information
System [8]) is a case where constructing an example input/output
pair is a non-trivial task for an end-user. However, tasks in these
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14-22, 2016, Austin, TX, USA
c2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2884781.2884786domains can easily be speciÔ¨Åed natural language as, as we show in
this work, can be used to reliably synthesize the desired program.
In this paper, we address the problem of synthesizing programs
in an underlying domain speciÔ¨Åc language (DSL) from natural lan-
guage (NL). NL is inherently imprecise; hence, it may not be pos-
sible to guarantee the correctness of the synthesized program. In-
stead, we aim to generate a ranked set of programs and let the
user possibly select one of those programs by either inspecting the
source code of the program, or the result of executing them on some
test inputs [37]. The synthesis algorithm in this paper is able to
consistently produce and rank the desired result program in the top
spot, over 80% of the time, or in the top 3 spots, over 90% of the
time in our benchmarks. To give users conÔ¨Ådence in the program
they choose, we show both the translation of the code into disam-
biguated English and/or run it to show the result as a preview.
Unlike most existing synthesis techniques that specialize to a
speciÔ¨Åc DSL, such as [13, 15, 50, 51], our approach can be ap-
plied to a variety of DSLs. Our approach requires two inputs from
the synthesis designer: (i) The DSL deÔ¨Ånition, which we assume
provides a set of operations that are similar to the concepts an end-
user might express in NL, (ii) Training data consisting of example
pairs of English sentences and corresponding intended programs in
the DSL. A training phase infers a dictionary relation over pairs
of English words and DSL terminals (in a semi-automated interac-
tive manner), and optimal weights/classiÔ¨Åers (in a completely au-
tomated manner) for use by the generic synthesis algorithm. Our
approach can be seen as a meta-synthesis framework for construct-
ing NL-to-DSL synthesizers.
The generic synthesis algorithm (Alg. 1) takes as input an En-
glish sentence and generates a ranked set of likely programs. First,
it uses a bag algorithm (Alg. 2) to efÔ¨Åciently compute the set of all
well-typed DSL programs whose terminals are related to the words
that occur in the sentence. For this, it uses a dictionary (learned
during the training phase) that is a relation over English words and
DSL terminals. Then, it ranks these programs based on a set of
scoring functions (¬ß4.2) inspired by our view of the Abstract Syn-
tax Tree (AST) of a program as involving two constituents: the set
of terminals in the program, and the tree structure between those
terminals. A weighted linear combination of 3 scores determines
the rank of each program: (i) a coverage score that captures the
intuition that results that ignore many words in the user input are
unlikely to be correct (ii) a mapping score that captures the intuition
that English words can have multiple meanings wrt. the DSL but
we prefer the more probable interpretations (iii) a structure score
that uses the insight that natural and programming languages have
common idiomatic structures [22], and prefer more natural results.
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   345
(a)
Grammar
S:=
Command | SEQ(Command, Command)
Command:=
ReplaceCmd | RemoveCmd | InsertCmd | PrintCmd
ReplaceCmd:=
Replace(SelectStr, NewString, IterScope)
RemoveCmd:=
Remove(SelectStr, IterScope)
InsertCmd:=
Insert(PString, Position, IterScope)
PrintCmd:=
Print(SelectStr, IterScope)
SelectStr:=
(Token, BCond, Occurrence)
IterScope:=
(Scope, BCond, Occurrence) | DOCUMENT
Token:=
PString | LINE | WORD | NUMBER | ...
BCond:=
AtomicCond |Not(AtomicCond)|And(AtomicCond,AtomicCond)|...
AtomicCond:=
StartsWith(Token) | Contains(Token) | CommonCond
CommonCond:=
Between(Token, Token)| After(Token)| ...
Occurrence:=
ALL | AtomicOccurrence | ...
AtomicOccurrence:=
IntSetByAnd | FirstFew(Integer) | ...
IntSetByAnd:=
Integer | INTSET(IntegerSet)
Scope:=
LINESCOPE | WORDSCOPE
Position:=
START | END | ...
NewString:=
BY(PString)(b)
Sample Benchmarks
1.Remo
ve the Ô¨Årst word of lines which start with number.
2.Replace ‚Äú&‚Äù with ‚Äú&&‚Äù unless it is inside ‚Äú[‚Äù and ‚Äú]‚Äù.
3.Add ‚Äú$‚Äù at the beginning of those lines that do not already start
with ‚Äú$‚Äù.
4.Add ‚Äú..???‚Äù at the last of every 2nd statement.
5.In every line, delete the text after ‚Äú//‚Äù.
6.Remove 1st ‚Äú&‚Äù from every line.
7.Add the sufÔ¨Åx ‚Äú_IDM‚Äù to the word right after ‚Äúidiom:‚Äù.
8.Delete all but the 1st occurrence of ‚ÄúCook‚Äù.
9.Delete the word ‚Äúthe‚Äù wherever it comes after ‚Äúall‚Äù.
(c)
Variations in NL for description of the same task.
1.
Prepend the line containing ‚ÄúP.O. BOX‚Äù with ‚Äú*‚Äù
2. Add a ‚Äú*‚Äù at the beginning of the line in which the string "P.O.
BOX" occurs
3. Put a ‚Äú*‚Äù before each line that has ‚ÄúP.O. BOX‚Äù in it
4. Put ‚Äú*‚Äù in front of lines with ‚ÄúP.O. Box‚Äù as a substring
5. Insert ‚Äú*‚Äù at the start of every line which has ‚ÄúP.O. BOX‚Äù word
Table 1: Grammar and sample benchmarks for the Text Editing domain.
The classiÔ¨Åers to compute these scores, as well as the weights for
combining the scores are learned during the training phase using
off-the-shelf machine learning algorithms. The novelty of of our
approach lies in the generation of training data for classiÔ¨Åer learn-
ing from the top-level training data (Alg. 3and4), and in smoothing
a discrete scoring metric into a continuous and differentiable loss
function for effective learning of weights (¬ß5.4).
This paper makes the following contributions:
We describe a meta-synthesis framework for constructing NL-
to-DSL synthesizers, consisting of a synthesis algorithm (¬ß4)
for translating English sentences into corresponding programs
in the underlying DSL, and a training phase for learning a dictio-
nary and weights that are used by the synthesis algorithm (¬ß5).
Our method can be applied to new DSLs, and requires only the
DSL deÔ¨Ånition along with translation pair training data.
We apply our generic framework to three different domains,
namely automating end-user data manipulation (¬ß2.1), generat-
ing problem descriptions in intelligent tutoring systems (¬ß2.2),
and database querying (¬ß2.3). In cases where comparisons can
be made with state-of-the-art NLP based approaches, the results
of the approach presented in this paper are competitive.
We gather an extensive corpus of data consisting of 1272 pairs of
English descriptions and corresponding programs. We use this
data for evaluation in this paper, and provide it as a resource [9]
for researchers in the community. Of these, 535 English de-
scriptions come from the Air Travel Information System (ATIS)
benchmark suite [8], 227 come from another corpus [36], while
510 English descriptions were collected by us from various on-
line sources (including help forums and course materials), text-
books, and user studies.
We evaluate the effectiveness of our approach on three differ-
ent DSLs (¬ß6). The synthesizers produced by our framework
run in 1 2 seconds on average per benchmark and produce a
ranked set of candidate programs with the correct result in the
top-1/top-3 choices for over 80%/90% benchmarks respectively.
2. MOTIV ATING SCENARIOS
We describe 3 different domains where a NL-to-DSL synthesizer
is useful: text editing (¬ß2.1), automata construction problems for
intelligent tutoring (¬ß2.2), and answering queries for an air travel
information systems (¬ß2.3).2.1 Text Editing (End-User Programming)
Through a study of help forums for OfÔ¨Åce suite applications like
Microsoft Excel and Word, we observed that users frequently re-
quest help with repetitive text editing operations such as insertion,
deletion, replacement, or extraction in text Ô¨Åles. These operations
(Table 1(b)) are more complicated than simple search-and-replace
of a constant string by another in two ways. First, the string be-
ing searched for is often not constant and instead requires regular
expression matching. Second, the editing is often conditional on
the surrounding context. Programming of even such relatively sim-
ple tasks requires the user to understand syntax and semantics of
regular expressions, conditionals, and loops, which are beyond the
ability of most end-users.
This inspired us to design a command language for text-editing
(a subset of the grammar is shown in Table 1(a)) that includes key
commands Insert, Remove, Print andReplace. Each of these
commands relies on an IterScope expression that speciÔ¨Åes the re-
gion (a set of lines, a set of words, or the entire Word document)
that the text editing operation is on. The SelectStr production
includes a Token, which allows for limited wild-card matching
(e.g., an entire WORD, NUMBER, or a pattern speciÔ¨Åed by PString),
a Boolean condition BCond that acts as an additional (local) Ô¨Ålter
on the matched value, and an Occurrence value that performs an
index based selection from the resultant matches. Use of the occur-
rence values like FirstFew(N) (from AtomicOccurrence) when
performing a Remove results in the removal of only the Ô¨Årst N items
(here N is a positive integer) that match the condition, while use of
ALL will instead result in all matches of the condition being re-
moved. The Boolean conditions BCond cover the standard range
of string matching predicates (Contains, StartsWith etc.) and
allow conjunction of conditions (And, Notetc.). The CommonCond
production speciÔ¨Åes the position relative to the string token(s) that
occurs after it (After), before it (Before), or around it (Between)
acts as another (global) Ô¨Ålter.
EXAMPLE 1.For text editing task 1 in Table 1(b), our system
produces the following translation:
Remove(SelectStr(WORD, ALWAYS, 1),
IterScope(LINESCOPE, StartsWith(NUMBER), ALL))
EXAMPLE 2.For text editing task 2 in Table 1(b), our system
produces the following translation:
Replace(SelectStr(‚Äú&‚Äù, Not(Between(‚Äú[‚Äù, ‚Äú]‚Äù)), ALL),
By(‚Äú&&‚Äù),
DOCUMENT)
3461.
Consider the set of all binary strings where the difference between
the number of ‚Äú0‚Äù and the number of ‚Äú1‚Äù is even.
2.
The set of strings of ‚Äú0‚Äù and ‚Äú1‚Äù such that at least one of the last 10
positions is a ‚Äú1‚Äù.
3.
the set of strings w such that the symbol at every odd position in w
is ‚Äúa‚Äù.
4.
Let L1 be the set of words w that contain an even number of ‚Äúa‚Äù, let
L2 be the set of words w that end with ‚Äúb‚Äù, let L3 = L1 intersect L2.
5.
The set of strings over alphabet 0 to 9 such that the Ô¨Ånal digit has
not appeared before.
Table 2: Sample benchmarks for the Automata domain.
1.
I would like the time of your earliest Ô¨Çight in the morning from
Philadelphia to Washington on American Airlines.
2.
I need information on a Ô¨Çight from San Francisco to Atlanta that
would stop in Fort Worth.
3.
What is the earliest Ô¨Çight from Washington to Atlanta leaving on
Wednesday September fourth.
4.
Okay we‚Äôre going from Washington to Denver Ô¨Årst class ticket I
would like to know the cost of a Ô¨Årst class ticket.
5.
What ground transportation is there from the airport in Atlanta to
downtown.
Table 3: Sample benchmarks for the ATIS domain.
Table 1(c) describes a sample of the variations that our system
can handle for description of a task that is expressible in our DSL.
Our belief is that once users are able to accomplish these types of
smaller conditional and repetitive tasks, they can accomplish com-
plex tasks by reducing them to a sequence of smaller tasks using
end-user programming environments [20, 37, 55].
2.2 Automata Theory (Intelligent Tutoring)
Results from formal methods research have been used in many
parts of intelligent tutoring systems [16] including problem gener-
ation, solution generation, and especially feedback generation for a
variety of subject domains including geometry [19] and automata
theory [1]. Each of these domains involves a specialized DSL that
is used by a problem generator tool to create new problems, a solu-
tion generation tool to produce solutions, and more signiÔ¨Åcantly, a
feedback generation tool to provide feedback on student solutions.
Consider the domain of automata constructions, where students
are asked to construct an automaton that accepts a language whose
description is provided in English (For some examples, see Ta-
ble 2). We designed a DSL based on the description provided by
Alur et.al. [1] on constructs required to formally specify such lan-
guages. This DSL contains predicates over strings, Boolean con-
nectives, functions that return positions of substrings, and univer-
sal/existential quantiÔ¨Åcation over string positions. As stated in [1],
such a language is used to generate feedback for students‚Äô incorrect
attempts in two ways: (i) it is used by a solution generation tool to
generate correct solutions against which a student‚Äôs attempts are
graded, (ii) it is also used to provide feedback and generate prob-
lem variations consistent with a student‚Äôs attempt. This feedback
generation tool has been deployed in the classroom and has been
able to assign grades and generate feedback in a meaningful way
while being both faster and more consistent than a human.
EXAMPLE 3.SpeciÔ¨Åcation 1 in Table 2 is translated as:
IsEven(Diff(OccurCount(0), OccurCount(1)))
EXAMPLE 4.SpeciÔ¨Åcation 2 in Table 2 is translated as:
Exists(LastK(10), StrEquals(SymbolAtPos(), 1))
2.3 Air Travel Information Systems (ATIS)
ATIS [8] is a standard benchmark for querying air travel infor-
mation, consisting of English queries and an associated databasecontaining Ô¨Çight information. It has long been used as a standard
benchmark in both natural language processing and speech pro-
cessing communities. Table 3 shows some sample queries from the
ATIS suite. For ATIS, we designed a DSL that is based around SQL
style row/column operations and provided support for predicates/-
expressions that correspond to important concepts in air-travel
queries, arrival/departure locations, times, dates, prices, etc.
EXAMPLE 5.The Ô¨Årst query in the ATIS examples, Table 3, is
translated into our DSL as:
ColSelect(DEP_TIME, RowMin(DEP_TIME,
RowPred(EqDepart(PHILADELPHIA, Time(MORNING)),
EqArrive(WASHINGTON, Time(ANY)),
EqAirline(AMERICAN))))
3. PROBLEM DEFINITION
We study the problem of synthesizing NL-to-DSL synthesizers,
given a DSL deÔ¨Ånition and training data. A DSL L= (G;TC)con-
sists of a context free grammar G(with terminal symbols denoted
byGTand production rules by GR), and a type/semantic checker
TCthat can check if a given program is well-typed. The training
data consists of a set of pairs (S;P), where Sis an English sentence
andPis the corresponding intended program from the DSL L. A
sentence is simply a sequence of words [w1;w2;:::; wn]. The goal
of the generated NL-to-DSL synthesizer is to translate an English
sentence to a ranked set of programs, [P1;P2;:::; Pk], inL.
4. NL TO DSL SYNTHESIS ALGORITHM
Our synthesis algorithm (Alg. 1) takes a natural language com-
mand from the user and creates a ranked list of candidate DSL pro-
grams. The Ô¨Årst step (loop on line 2) is to convert each of the words
in the user input into one or more terminals (function names or val-
ues) using the NL to program terminal Dictionary NLDict. This
loop ranges over the length of the input sentence and for each in-
dex looks up the set of terminals in the DSL that are associated with
the word at that index in NLDict. Fundamentally, NLDict encodes,
for each terminal, which English language words are likely to in-
dicate the presence of that terminal in the desired result program.
This map can be constructed in a semi-automated manner (¬ß5.3).
Once this association has been made for a terminal twe store a tu-
ple of the terminal and a singleton map, relating the index of the
word to a terminal that was produced, into the set R0(line 4).
For each natural language word, the dictionary NLDict associates
a set of terminals with it. The terminals may be constant values or
function applications with holes (2) as arguments. Thus, Alg. 1,
when applying NLDict on line 3, can create incomplete programs,
where some arguments to functions are missing. For example, Con-
sider the sentence ‚ÄúPrint all lines that do not contain 834‚Äù. Since the
grammar contains PrintCmd:= Print(SelectStr, IterScope)
as a production and the dictionary relates the word ‚Äúprint‚Äù to the
function Print, the partial program Print(2, 2)will be gener-
ated. These holes are later replaced by other programs that match
the argument types SelectStr and IterScope.
Once the base set of terminals has been constructed, the algo-
rithm uses the Bagalgorithm (Alg. 2) to generate the set of all con-
sistent programs, ResT, that can be constructed from it (line 5). The
Ô¨Ånal step is to rank (¬ß4.2) this set of programs, using a combination
of scores and weights, in the loop on line 8.
4.1 Synthesizing Consistent Programs
A program Pin the DSL is either an atomic value (i.e., a terminal
in G), or a function/operator applied to a list of arguments. By con-
vention we represent function application as s-expressions where a
function Fapplied to karguments is written (F;P1;:::; Pk).
347Algorithm 1: Top-Level NL to DSL Synthesis Algorithm
Input: NL sentence S, Word-to-Terminal dictionary NLDict
Output: Ranked set of programs
1R0 /0;
2fori2[0;S:Length 1]do
3 T NLDict( S[i]);
4 foreach t2TdoR0 R0[(t;SingletonMap( i;t));
5Res T Bag( S;R0);
6Res P fPj9 Ms.t.(P;M)2Res Tg;
7foreach program P2Res Pdoscore( P)  ¬•;
8foreach program (P;M)2Res Tdo
9 scov CoverageScore( P;S;M)wcov;
10 smap MappingScore( P;S;M)wmap;
11 sstr StructureScore( P;S;M)wstr;
12 score( P) max( score( P);scov+smap+sstr);
13return set of programs in Res Pordered by score
Consistent Programs and Witness Maps.
Given a DSL L= (G;TC), we say a program Pin language L
is consistent with a sentence Sif there exists a map Mthat maps
(some) word occurrences in Sto terminals in GTsuch that the range
ofMequals the set of terminals in the program P.1We refer to such
a map Mas awitness map, and use the notation WitnessMaps( P;S)
to denote the set of all such maps.
Usable and Used Words.
LetSbe an English sentence, Pbe a program consistent with
S, and Mbe any witness map. UsableWords( S)are those word
occurrences in Sthat are mapped to some grammar terminal and
hence might be useful in translation. UsedWords( S;M)is the set of
usable word occurrences in Sthat are used as part of the map M.
UsableWords( S) =fijS[i]2Domain( NLDict)g
UsedWords( S;M) =UsableWords( S)\Domain( M)
Partial Programs.
A partial program extends the notion of a program to also allow
for a hole (2) as an argument. A hole is a symbolic placeholder
where another complete program (program without any hole) can
be placed to form a larger program. To avoid verbosity, we often
refer to a partial program as simply a program.
Given a partial program P= (F;:::;2;::: )with a hole 2, we can
substitute a complete program P0to Ô¨Åll the hole:
P[2 P0] =(
(F;:::; P0;:::)ifTC((F;:::; P0;:::))
? otherwise
The validity check, TC, ensures that all synthesized programs are
well deÔ¨Åned in terms of the DSL grammar and type system (other-
wise we return the invalid program?).
Combination.
The combination operator SubAll generates the set of all valid
programs that can be obtained by substituting a complete program
P0in some hole of a partial program P. This is done by going over
all the arguments of Pand producing substitutions for argument
positions with holes. Given partial program P= (F;P1;:::; Pk)and
complete program P0, we have:
SubAll( P;P0) =fP[2i P0]jPi=2ig
1Since the same English word can occur at different positions in
S, having different meanings, any map Mmust take the position
information also as an argument. We ignore this in the paper.Algorithm 2: Bag Synthesis Sub-Algorithm
Input: NL sentence S, Initial Tuple Set B0
1result B0;
2repeat
3 oldResult result;
4 foreach (P1;M1);(P2;M2)2result do
5 okpc P1is partial^P2is complete ;
6 disjoint UsedWords( S;M1)\UsedWords( S;M2) =/0;
7 ifokpc^disjoint then
8 combs SubAll( P1;P2) f?g;
9 new f( Pr;M1[M2)jPr2combsg;
10 result result[new;
11until oldResult =result;
12return result;
Bag Algorithm.
TheBagalgorithm (Alg. 2) is based on computing the closure of
a set of programs by enumerating all possible well-typed combina-
tions of the programs in the set. The main loop (line 2) is a Ô¨Åxpoint
iteration on the result set of programs that have been constructed.
The requirement that P2is a complete program (line 5) when
applying the SubAll function ensures that the only holes in the re-
sult programs are holes that were originally in P1. We restrict the
initialization of B0to include only complete programs and partial
programs with holes at the top level only. Using this restriction
we can inductively show that at each step all partial programs only
have holes at the top-level. Thus, we can efÔ¨Åciently compute the
Ô¨Åxpoint of all possible programs in a bottom-up manner. The con-
dition UsedWords( S;M1)\UsedWords( S;M2) =/0(line 6) ensures
that the two programs do not use overlapping sets of words from the
user input. This ensures that the Ô¨Ånal program cannot create multi-
ple sub-programs with different meanings from the same part of the
user input. This also ensures that the set of possible combinations
has a Ô¨Ånite bound based on the number of words in the input. Line 8
constructs the set of all possible substitutions of P2into holes in P1
(ignoring any invalid results). For each of the possible substitutions
we add the result (and the union of the Mmaps) to the newprogram
set (line 9). Since the domains of the maps were disjoint the union
operation is well-deÔ¨Åned.
TheBagalgorithm has a high recall, but, in practice, it may gen-
erate spurious programs that arise as a result of arbitrary rearrange-
ment of the words in the English sentence. To Ô¨Åx this, the cor-
rect translation is reported by selecting the top-most rank program
based on features of the program and the parse tree of the sentence.
4.2 Ranking Consistent Programs
We view the abstract syntax tree of the synthesized program as
consisting of two important constituents: the set of terminals in the
program, and the tree structure between those terminals. We use
these constituents to compute the following three scores to deter-
mine the rank of a consistent program: (i) a coverage score that
reÔ¨Çects how many words in the English sentence were mapped to
some operation or value in the program, (ii) a mapping score that
reÔ¨Çects the likelihood that a word-to-terminal mapping is capturing
the user intent (iii) a structure score that captures the naturalness of
the tree program structure and the connections between parts of the
program and the parts of the sentence that generated them.
4.2.1 Coverage Score
For a given sentence S, a candidate translation P, and a witness
map M, the coverage score is deÔ¨Åned as,
CoverageScore( P;S;M) =jUsedWords( S;M)j
jUsableWords( S)j
348TheCoverageScore( P;S;M)denotes the fraction of available infor-
mation in Sthat is actually used to generate P. Intuitively we want
to prefer programs that make more use of the input information.
EXAMPLE 6.Consider possible translations for an input S:
S:find the cheapest flight from Washington
to Atlanta
P1:RowMin(FARE, RowPred(EqDepart(WASHINGTON),
EqArrive(ATLANTA)))
P2:RowPred(EqDepart(WASHINGTON), EqArrive(ATLANTA)))
The Ô¨Årst program P 1makes use of all parts of the user input, includ-
ing the desired cheapest fare, while the second program P 2ignores
this information. The Coverage score ranks P 1higher than P 2.
4.2.2 Mapping Score
For any word wthere may be multiple terminals (functions or
values) in the set NLDict( w)each of which corresponds to a dif-
ferent interpretation of w. We use machine learning techniques to
obtain a classiÔ¨Åer Cmapbased on the part-of-speech (POS) tag pro-
vided for the word by the Stanford NLP engine [26]. Cmap:Predict
function of the classiÔ¨Åer predicts the probability of each word-to-
terminal mapping being correct. We use predictions from Cmapto
compute the MappingScore, the likelihood that terminals in Pare
correct interpretation of corresponding words in S.
MappingScore( P;S;M) =
√ï
w2UsableWords( S)Cmap:Predict( w;POS( w;S);M(w))
A limitation of the MappingScore score is that it looks only at
the mapping of a word but not its relation to other words and how
they are are mapped by the translation. Thus, interchanging a pair
of terminals in a correct translation gives us an incorrect translation
which has the same score.
EXAMPLE 7.Consider an input S and 2 possible translations:
S:If ‚ÄúXYZ‚Äù is at the beginning of the line,
replace ‚ÄúXYZ‚Äù with ‚ÄúABC‚Äù
P1:Replace(SelectStr(‚ÄúXYZ‚Äù, ALWAYS, ALL),
By(‚ÄúABC‚Äù),
IterScope(LINESCOPE, StartsWith(‚ÄúXYZ‚Äù), ALL))
P2:Replace(SelectStr(‚ÄúXYZ‚Äù, ALWAYS, ALL)
By(‚ÄúABC‚Äù),
IterScope(LINESCOPE, Before(‚ÄúXYZ‚Äù), ALL))
Both the programs use same sets words, so they have the same
coverage score. The only difference is that the word ‚Äúbeginning‚Äù
is mapped to StartsWith (POS: Verb Phrase) in P 1, and to Before
(POS: Prepositional Phrase) in P 2. Mapping score helps in identi-
fying P 1as the correct choice.
4.2.3 Structure Score
Structure score captures the notion of naturalness in the place-
ment of sub-programs. We use connection features obtained from
the sentence S, the natural language parse tree for the sentence,
NLParse(S ), and the corresponding program Pto deÔ¨Åne the overall
structure score. These features are used to produce the classiÔ¨Åer
Cstrwhich computes the probability that each of the combinations
inPis correct.
DEFINITION 1 (C ONNECTION ).For a production R 2GRof
the form N!N1:::Ni:::Nj:::Nk, the tuple (R;i;j)where 1
i;jk;and i6=j is called a connection.DEFINITION 2 (C OMBINATION ).Consider the program P =
(P1;P2; :::; Pk)generated using the production R :N!N1N2:::Nk,
such that N igenerates P ifor1ik. We say the pair of sub-
programs (Pi;Pj)is combined via connection (R;i;j)and this com-
bination is denoted as Conn( Pi;Pj).
The overall StructureScore is obtained by taking the geometric
mean of the various connection probabilities of the scores for the
program P‚Äîthis normalizes the score to account for programs with
differing numbers of connections.
StructureScore( P;S;M)=GeometricMean( ConnProbs( P;S;M))
ConnProbs( P;S;M)=[
Conn( Pi;Pj)inPfCstr[Conn] :Predict( ~f;1)g
where ~fhfpos1;fpos2;flca1;flca2;forder;fover;fdisti
computed for PiandPjusing P,S, and M.
We obtain separate classiÔ¨Åer, Cstr[Conn], for each connection
Conn. The function Cstr[Conn].Predict asks the classiÔ¨Åer to pre-
dict the probability that f-vec belongs to class 1 (i.e., present in
correct translation). The other class is 0.
Given a program Pand input sentence Sthat are related by a wit-
ness map Mand the parse tree NLParse(S ), the following functions
deÔ¨Åne several useful relationships:
TreeCover( P;S;M) =minimal sub-tree Tsubof NLParse( S)
s.t.UsedWords( S;M)UsableWords( Tsub)
Root( P;S;M) =root node of TreeCover( P;S;M)
Span( P;M) = [ Min( Domain( M));Max( Domain( M))]
In the rest of the section, we assume that P1andP2denote two
sub-programs of P. The following features determine the natural-
ness of the connections between P,P1,P2, and S:
DEFINITION 3 (R OOT POS T AGS).Part-of-speech features
are the POS tags assigned by the NL Parser to the root nodes of
the sub-trees associated with P 1and P 2respectively:
fpos1POS( Root( P1;S;M)) fpos2POS( Root( P2;S;M))
The features fpos1 and fpos2 help to learn the phrases that are
commonly combined using a particular connection.
DEFINITION 4 (LCA D ISTANCES ).Let LCA be the least
common ancestor of Root( P1;S;M)and Root( P2;S;M). The LCA
distance features are the tree-distances from LCA to the root nodes
of the sub-trees associated with P 1and P 2respectively:
flca1TreeDistance( LCA; Root( P1;S;M))
flca2TreeDistance( LCA; Root( P2;S;M))
DEFINITION 5 (O RDER ).The order feature is determined by
the positions of the roots of the sub-tree roots associated with P 1
and P 2in the in-order traversal of NLParse(S).
forder=8
><
>:1if Root( P1;S;M)occurs before Root( P2;S;M)
in in-order traversal of NLParse(S)
 1otherwise
Features flca1;flca2and forder are used to learn the correspon-
dence between the parse tree structure and the program structure.
We use these to maintain the structure of translation close to the
structure of the parse tree.
349DEFINITION 6 (O VERLAP ).The overlap feature captures the
possibility that two programs are constructed from mixtures of two
subtrees in the NL Parse tree:
fover=8
><
>:1if Span( P1;M1):end<Span( P2;M2):start
 1if Span( P1;M1):start>Span( P2;M2):end
0otherwise
DEFINITION 7 (D ISTANCE ).Given two programs P 1and P 2
we deÔ¨Åne the distance feature for programs by looking at the dis-
tance between the word spans used in the programs:
fdist=8
><
>:Span( P2;M2):start Span( P1;M1):end if f over=1
Span( P1;M1):start Span( P2;M2):end if f over= 1
0 otherwise
The features foverandfdistcapture the proximity information of
words and are useful because related words often occur together in
the input sentence.
EXAMPLE 8.Consider possible translations for an input S:
S:Print all lines that do not contain ‚Äú834‚Äù
P1:Print(SelectStr(LINE, Not(Contains(‚Äú834‚Äù)),
ALL), DOCUMENT)
P2:Print(SelectStr(‚Äú834‚Äù, Not(Contains(LINE)),
ALL), DOCUMENT)
In the parse tree NLParse(S), ‚Äúprint‚Äù will have two arguments,
what to print (‚Äúlines‚Äù) and when to print(‚Äúnot contain 834‚Äù). We
observe the following for the candidate programs: (a) The word
‚Äúlines‚Äù is closer to ‚Äúprint‚Äù, while the word ‚Äú834‚Äù is farther in
NLParse(S). The same structure is observed for P 1, but not for P 2.
This is captured by LCA Distances. (b) The order of the words in
NLParse(S) matches the order in P 1better than in P 2. This is cap-
tured by the Order feature. (c) The phrase ‚Äúdo not contain 834‚Äù
is kept intact in P 1, but is split apart in P 2. Overlap and Distance
features will capture this splitting and reordering.
Both the programs use the same set of words, and the same word
to terminal mappings, resulting in the same coverage score and the
same mapping scores. However, the program P 1is correct and our
choice of features rank it higher.
4.3 Combined Score Example
To provide some intuition into the complementary strengths and
weaknesses of the various scores, we examine how they behave on
a subset of the programs generated by the Bagalgorithm for the fol-
lowing text editing task: Add a ‚Äú*‚Äù at the beginning of the
line in which the string ‚ÄúP.O. BOX‚Äù occurs. Table 4
shows some of the consistent programs generated by the Bag al-
gorithm. The Ô¨Årst program (P 1) is the intended translation. Let us
look at the performance of each of the component scores:
Coverage Score: Both P1andP4use the maximum number of
words from the sentence, and are tied on top score. P4is wrong as
it adds ‚ÄúP. O. BOX‚Äù at the beginning of the line containing ‚Äú*‚Äù.
Mapping Score: The classiÔ¨Åer learnt by our system maps the word
‚Äúbeginning‚Äù to the terminal StartsWith with a high probability but
to the terminal START with a lower probability. Further, it maps
‚Äúoccurs‚Äù to the terminal Contains with a still lower probability. P2
does not use the word ‚Äúoccur‚Äù, otherwise it has same mappings as
P1. As a result it has higher mapping score than P1, but suffers on
coverage. P3maps ‚Äúbeginning‚Äù to StartsWith, and does not use the
word ‚Äúoccurs‚Äù. As a result it has a mapping score lower than P2
but higher than P1. If we had used the mapping score alone, we
would not have been able to rank the desired program P1above the
incorrect programs P3andP4.Algorithm 3: Learning Mapping Score ClassiÔ¨Åer Cmap
Input: Training Data T
1 foreach training pair (S;P)2Tdo
2 ÀúM WitnessMaps( P;S);
3 M argmaxM02ÀúM(Likeability( P;S;M0));
4 foreach (w;t)2Mdo
5 Cmap.Train(w,POS(w,S ),t)
6 return Cmap;
Structure Score: Coverage score and mapping score look only at
the mapping of a word but not its relation to other mappings and
their placement with respect to the original sentence. Structure
score Ô¨Åxes this by considering structural information (parse tree,
ordering of words and distance among words) from the sentence.
P4has poor structure score because it swaps the sentence ordering
for strings ‚Äú*‚Äù and ‚ÄúP.O. BOX‚Äù. P3also suffers as it moves ‚Äúbegin-
ning‚Äù (mapped to StartsWith) away from ‚ÄúAdd‚Äù (mapped to Insert).
P1gets a high structure score as it maintains the parse tree structure
of the input text. Note that, P2andP5have high structure score
as well. This is because structure score does not take into account
the fraction of used words or word-to-terminal mappings. So, an
incomplete translation that uses very few words but maps them to
correct terminals and places them correctly, is likely to have a high
value.
The desired program, P1, is only top ranked by one of the scores
and even in that case, the score is tied with another incorrect result.
However, a combination of the scores with appropriate weights (¬ß5)
ranks P1as the clear winner!
5. TRAINING PHASE
This section describes the learning of classiÔ¨Åers, weights, and
the word-to-terminal mapping used by the synthesis algorithm de-
scribed in ¬ß4. The key aspects in this process are (i) deciding which
machine learning algorithm to use, and (ii) generation of (lower
level) training data for that machine learning algorithm from the
top level training data provided by the DSL designer.
5.1 Mapping Score ClassiÔ¨Åer ( Cmap)
The goal of the CmapclassiÔ¨Åer is to predict the likelihood of a
word wmapping to a terminal t2GTusing the POS tag of the
word w. The learning of this classiÔ¨Åer is performed using an off-
the-shelf implementation of a Naive Bayesian ClassiÔ¨Åer [6]. The
training data for this classiÔ¨Åer is generated as shown in Alg. 3.
The key idea is to Ô¨Årst construct the set ÀúMof all witness maps
that can yield program Pfrom natural language input S. We then
select the most likely map Mout of these witness maps based on
the partial lexicographic order given by the likeability score tuples.
Likeability( P;S;M) =( UsedWords( S;M);Disjointness( P;S;M))
Disjointness( P;S;M) = √•
P02SubProgs( P)s(P0)
where s((P1;:::; Pn))=1 if8Pi;Pj;Pi\Pj=/0;0 otherwise
The likeability tuples serve two purposes: First, via the UsedWords,
they guide the system to prefer mappings that use all parts of the
input sentence. Second, via the Disjointness, they guide the sys-
tem to prefer mappings that penalize the use of a single part of a
sentence to construct multiple different subprograms.
5.2 Structure Score ClassiÔ¨Åers ( Cstr)
We now describe how the classiÔ¨Åers used in structure score,
Cstr[Conn] for each connection Conn, are learned. The goal of each
350Pr
ogram Generated Co
verage
ScoreMapping
Scor
eStructur
e
ScoreFinal
Scor
e
P1Insert(‚Äú*‚Äù,
START, IterScope(LINESCOPE, Contains(‚ÄúP.O. BOX‚Äù), ALL)) 8.33 5.73 4.45 322.17
P2Insert(‚Äú*‚Äù,
START, IterScope(LINESCOPE, ALWAYS, ALL)) 5.00 8.40 4.45 248.17
P3Insert(‚Äú*‚Äù,
START, IterScope(LINESCOPE, StartsWith(‚ÄúP.O. BOX‚Äù), ALL)) 6.67 6.35 1.09 232.57
P4Insert(‚ÄúP
.O. BOX‚Äù, START, IterScope(LINESCOPE, Contains(‚Äú*‚Äù), ALL)) 8.33 5.73 1.00 272.43
P5Insert(‚Äú*‚Äù,
START, DOCUMENT) 3.33 4.74 6.84 216.33
Table 4: Ranking the set of consistent programs generated by the Bagalgorithm.
Algorithm 4: Learning Structure Score ClassiÔ¨Åers Cstr
Input: Training Data T
1 foreach training pair (S;P)2Tdo
2 AllOpts =SynthNoScore( S);
3 foreach program P02AllOpts do
4 foreach combination c that occurs in P0do
5 if c occurs in P then class 1;
6 else class 0;
7 Conn connection used by c;
8 ~f hfpos1;fpos2;flca1;flca2;forder;fover;fdisti;
9 Cstr[Conn].Train( ~f, class);
10 return Cstr
classiÔ¨Åer Cstr[Conn] is to predict the likelihood that a combination
cis an instance of connection Conn using the 7 features of cfrom
¬ß4.2. We use an off-the-shelf implementation of a Naive Bayesian
ClassiÔ¨Åer and generate the training data for it as shown in Alg. 4.
The key idea is to run the synthesis algorithm without the scoring
step, SynthNoScore, to construct the set of all possible programs,
AllOpts, from the English sentence S. Any combination present in
a program in P0inAllOpts but not present in Pis used as a negative
example, while that present in Pis used as a positive example.
5.3 Dictionary Construction
We construct the dictionary NLDict in a semi-automated man-
ner using the names of the terminals (functions and arguments) in
the DSL. If the name of an operation is a proper English word,
such as Insert, we use the WordNet [38] synonym list to gather
commonly used words which are associated with the action. Cases
where the name is not a simple word but instead concatenations
of (or abbreviations of) several words, such as StartsWith, are
handled by splitting the name and resolving the synonyms of each
sub-component word.
It is possible that the general purpose synonym sets provided by
WordNet contain English words that are not useful for the partic-
ular domain we are constructing the translator for. However, the
mapping score learning in ¬ß5.1 will simply assign these words low
scores. Once the learning algorithm for the mappings has Ô¨Ånished
assigning weights to each word/terminal we discard all mappings
below a certain threshold. Conversely, it is also possible that an
important domain speciÔ¨Åc synonym will not be provided by the
WordNet sets or that the names in the DSL are not well matched
with proper English words. Our system automatically detects these
cases as a result of being unable to Ô¨Ånd witness maps for programs
(in the training data) involving certain DSL terminals. In these
cases, it prompts the user to identify a word in an input sentence
that corresponds to an unmapped terminal in a program. These new
seed words are then further used to build a more extensive synonym
set using WordNet.
5.4 Learning Combination Weights
In the previous section, we deÔ¨Åned 3 component scores for a
translation. A standard mechanism for combining multiple scoresinto a single Ô¨Ånal score is to use a weighted sum of the component
scores. In this section we describe a novel method for learning the
required weights to use to maximize the following function.
Optimization Function: Number of benchmarks in the training set,
for which the correct translation is assigned rank 1.
In numerical optimization, maximization of an optimization func-
tion is a standard problem which can be solved using stochastic
gradient descent [5]. In order to use gradient descent to Ô¨Ånd the
weight values that maximize our optimization function we need to
deÔ¨Åne a continuous and differentiable loss function, Floss. This loss
function is used to guide the iterative search for a set of weights
that maximizes the value of the optimization function as follows:
~wn+1=~wn g~5Floss(~wn)n=0;1;2;::
where ~5denotes the gradient and gis a positive constant. At each
step,~wmoves in the direction in which the value of Flossdecreases
and the process is stopped when the change in the function value in
successive steps drops below a speciÔ¨Åed threshold e.
A common form for loss functions is a sigmoid. We can convert
our ill-behaved optimization function into a loss function that is
closer to what is needed to perform gradient descent by basing the
sigmoid on the ratio score given to the best incorrect result and the
score given to the desired rate via the following construction:
Floss(~w) = √•
8training Sf(~w;S)
f(~w;S) =1
1+e c(l 1)where l=vwrong
Score( Pdesired )^c>0
Pdesired =correct translation of S
vwrong =max(fScore( P)jP2Bag( S)^P6=Pdesiredg)
Although the above transformation results in a loss function
which is mostly well behaved, it saturates appropriately and is
piecewise continuous and differentiable, there are still points were
the function is not continuous. In particular the presence of the max
function in the deÔ¨Ånition of vwrong creates discontinuous points in
Floss. However, the following insight enables us to replace the dis-
continuous max operation with a continuous approximation:
max( a;b)log(eca+ecb)=cwhere c1 ifab_ba
Thus, we can replace the max operator with this function, extended
in the natural way to karguments, in the computation of vwrong to
produce a globally continuous and differentiable loss function. The
cases where there are several incorrect results which are given very
similar scores are minimized by the selection of a large value for c,
which ampliÔ¨Åes small differences. Additionally, in the worst case
where two scores are extremely close, the impact of the approxi-
mation is to drive the gradient descent to increase the ratio between
vwrong and Score( Pdesired ). Thus, the correctness of the gradient
descent algorithm is not impacted.
In addition to satisfying the basic requirements for performing
gradient descent, our loss function, Floss, saturates for large values
ofl. This implies that if an input S, hasvwrong
Score( Pdesired)1 it will
351not dominate the gradient descent causing it to improve the ranking
results for a single benchmark at the expense of rank quality on
a large number of other benchmarks. The saturation also implies
that the descent will not become stuck trying to Ô¨Ånd weights for an
input where there is no assignment to the weights that will improve
the ranking, i.e., there is an incorrect result program Piwhere every
component score has a higher value than the desired program Pd.
6. EXPERIMENTAL EV ALUATION
The (online) synthesis algorithm, consisting of the Bagalgorithm
and feature extraction (for ranking), was implemented in C# and
used the Stanford NLP Engine (Version 2.0.2) [53] for POS tag-
ging and extracting other NL features. The ofÔ¨Çine gradient decent
was implemented in C# while the classiÔ¨Åers used for training the
component features were built using MATLAB.
A major goal of this research is the production of a generic frame-
work for synthesizing programs in a given DSL from English sen-
tences. Thus, we selected 3 different categories of tasks, ques-
tion answering (ATIS), constraint based model construction (Au-
tomata Theory Tutoring), and command execution on unstructured
data (Repetitive Text Editing). These domains, described in de-
tail in ¬ß2, present a variety of structure in the underlying DSL, the
language idioms that are used, and the complexity of the English
sentences that are seen. For benchmarks, automata descriptions
are taken verbatim from textbooks and online assignments. Text
editing descriptions are taken verbatim from help forums and user
studies. ATIS descriptions are part of a standard suite. Tables 1(b),
1(c), 2, and 3describe a sample of the benchmarks. The details of
the benchmarks and their sources can be obtained from companion
website: https://sites.google.com/site/nl2pgm/ [9].
Air Travel Information System (ATIS).
We selected 535 queries at random from the full ATIS suite
(which consists of few thousand queries) and, by hand, constructed
the corresponding program in our DSL to realize the query. Each
task in ATIS domain is a query over Ô¨Çight related information.
Automata Theory Tutoring.
We collected 245 natural language speciÔ¨Åcations (accepting con-
ditions) of Ô¨Ånite state automata from books and online courses on
automata theory.
Repetitive Text Editing.
We collected a description of 21 text editing tasks from Excel
books and help forums. We collected 265 English descriptions for
these 21 tasks via a user study, which involved 25 participants (who
were Ô¨Årst and second year undergraduate students). The large num-
ber of participants ensured variety in the English descriptions (e.g.,
seeTable 1(c)). In order to remove any description bias, each of
these tasks was described not using English but using representa-
tive pairs of input and output examples. Additionally, we obtained
227 English descriptions for 227 text editing tasks (one for each
task) from an independent corpus [36].
6.1 Precision, Recall, and Computational Cost
In this study we used standard 10-fold cross-validation to eval-
uate the precision and recall of the translators on each of the do-
mains. Thus, we select 90% of the data at random to use for learn-
ing the classiÔ¨Åers/weights and then evaluate the system on the re-
maining 10% of data which was held back (and not seen during
training). In the ranking we handle ties in the scores assigned to
an element using a 1334 ranking scheme [46]. In 1334 ranking, in
Figure 1: Ranking precision of algorithm on all domains.
the case of tied scores, each element in the tied group is assigned
a rank corresponding to the lowest position in the ordered result
list (as opposed to the highest). This ensures that the reported re-
sults represent the worst case number of items that may appear in a
ranked list before the desired program is found.
Precision.
Fig. 1 shows the percentage of inputs for which the desired pro-
gram in the DSL is the top ranked result and the percentage of in-
puts where the desired result is in the Ô¨Årst three results. As shown
in the Ô¨Ågure, for every domain, on over 80% of the inputs the de-
sired program is unambiguously identiÔ¨Åed as the top ranked result.
Further, for the ATIS domain the desired result is the top ranked
result for 88: 4% of the natural language inputs. Given the size of
our sample from the full ATIS suite we can infer that the desired
program will be the top ranked result for 88: 44:2% of the nat-
ural language inputs at a 95% conÔ¨Ådence interval. These results
show that our novel program synthesis based translation approach
is competitive with the state-of-the-art natural language process-
ingsystems: 85% in [57], 84% in [42], and 83% in [27].
Recall.
In addition to consistently producing the desired program as the
top ranked result for most inputs the ranking algorithm places the
desired program in the top 3 results an additional 5%-12% of the
time. Thus, across all three of the domains, for over 90% of the
natural language inputs the desired program is one of the three top
ranked results. This leaves less than 10% of the inputs for any of
the domains, and only 5% in the case of the text editing domain,
where the synthesizer was unable to produce and place the desired
program in the top three spots.
Computational Cost.
Fig. 2 shows the distribution of the time required to run the syn-
thesis algorithm and perform the ranking on a 2.80 GHz Intel(R)
Core(TM) i7 CPU with 8 GB RAM. On average translation takes
0:68 seconds for Text Editing, 1: 72 seconds for Automata and 1: 38
seconds for the ATIS inputs. Further, the distribution of times is
heavily skewed with more than 85% of the inputs taking under 1
second and very few taking more than 3 seconds. The outliers tend
to be inputs in which the user has speciÔ¨Åed an action in an excep-
tionally redundant manner.
As with any program synthesis technique which fundamentally
involve search over exponential spaces, the cost of our technique is
also worst case exponential in the size of the DSL. However, the
key issue is doing this efÔ¨Åciently for practical cases. Our synthe-
sis works efÔ¨Åciently (usually under 1 second) for a range of useful
DSLs. The size of the dictionary has minimal impact on the run-
352Figure 2: Timing performance of algorithm on all domains.
1334 Top Rank
Domain CoverageScore MappingScore StructureScore
ATIS 5.6% 1.9% 20.2%
Automata 17.9% 31.0% 51.4%
Text Editing 8.1% 8.9% 34.4%
Table 5: Performance of individual component scores in ranking
the desired program as top result.
time as the translation only depends on the subset of the dictionary
corresponding to the words in the input sentence.
6.2 Individual Component Evaluation
In ¬ß4 we deÔ¨Åned various components for ranking and provided
intuition into their usefulness. To validate that these component
scores are important to achieving good results, we evaluated our
choices by using various subsets of the component scores, learning
the best weights for the subset, and re-ranking the programs.
Performance of Individual Scores.
The results of using each component in isolation are presented in
Table 5. This table shows that when identifying the top-ranked pro-
gram the best performance for using only CoverageScore is 17: 9%,
using MappingScore is 31: 0% and for StructureScore is 51: 4%.
This is far worse than the result obtained by using the combined
ranking which placed the desired program as the top result for
84:9% of the inputs. Thus, we conclude that the components are
not individually sufÔ¨Åcient.
Score Independence.
Although these results show that independently none of the com-
ponents are sufÔ¨Åcient for the program ranking it may be the case
that one of the components is, effectively, a combination of the
other two. Table 6 shows the results of ranking the programs when
dropping one of the components. Dropping StructureScore results
in the largest decrease, as high as 81: 86% in the worst case, and
even the best case has a decrease of 47: 75%. Dropping
CoverageScore also results in substantial degradation, although not
as high as for StructureScore. The impact of dropping
MappingScore is much smaller, between 2: 04% and 4: 67%. How-
ever, the consistent positive contribution of MappingScore shows
that it still provides useful information for the ranking. Thus, all of
the components provide distinct and useful information.
Dictionary Construction.
In practice the semi-automated approach makes dictionary con-
struction a task that, while usually requiring manual assistance,
does not require expertise in natural language processing or pro-
gram synthesis. On average the dictionaries for each domain con-
tained 144 English words averaging 4.51 words/terminal and 1.48
terminals/word. The user was prompted to provide 20.7% map-% change on dropping
Domain CoverageScore MappingScore StructureScore
ATIS -30.6% -4.7% -81.9%
Automata -22.4% -2.0% -47.7%
Text Editing -19.7% -4.7% -65.8%
Table 6: Impact of dropping individual component scores on top
rank percentage.
Domain Total Equal Wt. Rank Boost Gradient
Count Top Top-3 Top Top-3 Top Top-3
ATIS 535 73.2% 90.8 % 79.8% 89.9 % 88.4% 93.3 %
Automata 245 74.2% 91.4 % 73.1% 93.5 % 84.9% 91.2 %
Text Editing 492 74.0% 91.1 % 74.4% 91.8 % 82.3% 94.9 %
Table 7: Comparison of ranking using equal weights, gradient de-
scent, and RankBoost. Top (Top-3) shows the percentage of bench-
marks where the correct translation is ranked 1 (in top 3).
pings on average across the three DSLs. Although beyond the
scope of this work, as it requires a larger corpus of training data,
the amount of user intervention can be further reduced by using
statistical alignment to automatically extract the domain speciÔ¨Åc
synonyms from the training data.
Score Combination Weights.
We used gradient descent to learn how much to weight each score
in the computation of the Ô¨Ånal rank of a program. To evaluate the
quality of the weights identiÔ¨Åed via the gradient descent we com-
pared them with a naive selection of equal weights for all the com-
ponent scores and with the results of boosting. Boosting [10] is
a frequently technique which combines a set of weaker rankings,
such as the individual component scores, to produce a single strong
ranking. Table 7 shows the results of the rankings obtained with
the three approaches.
Using gradient descent has improved the number of top ranked
benchmarks signiÔ¨Åcantly over the naive weight selection (as large
as 15%). However, the improvement in the top 3 ranked bench-
marks is much smaller. Similarly, the gradient descent approach
produces substantially better results than RankBoost with an aver-
age difference of 9% in the top ranked benchmarks. Thus, we can
conclude that the use of gradient descent for learning the combina-
tion weights is important for the overall quality of the results.
Our choice of the ranking functions is critical to the quality of
results. As shown in Table 6, dropping any of the component func-
tions results in a substantial loss of precision. Also, using a simpler
method, such as equal weights or boosting [10], to compute the
combination weights results in a loss of 9-15% in precision when
compared to the use of gradient descent (Table 7).
In our system, most failures (i.e. the correct solution failing to
rank in the top-three solutions) arise because some key informa-
tion is left implicit in the English description, e.g. ‚ÄúI want to Ô¨Çy to
Chicago on August 15‚Äù. In this case, the departing city should
default to ‚ÄúCURRENT_CITY‚Äù , and the time should default to
‚ÄúANY‚Äù . Such issues might be Ô¨Åxed either by having orders of mag-
nitude larger training data or by building some specialized support
for handling implicit contextual information in various domains.
As part of learning the weights for the component scores we used
a shifted variant of the logistic function as our loss function (¬ß5).
Fig. 3 shows how the value of loss changes with iteration index
and the corresponding number of top ranked benchmarks. It can
be seen that as the loss value decreases, the number of top ranked
benchmarks increases and vice-a-versa. Thus, as these values are
negatively correlated as needed for optimal performance of the gra-
dient descent algorithm, and even though our loss function contains
353Figure 3: Behavior of Loss Function and Top Ranks.
% change on using weights learnt for
Domain ATIS Automata Text Editing
ATIS 0.0% -1.5% -5.0%
Automata -2.0% 0.0% -1.2%
Text Editing -0.6% -0.8% 0.0%
Table 8: Generalization of learning across domains.
the log-exponential approximation of the max operation it is well
behaved for the gradient descent algorithm.
Since the weights learned in ¬ß4.2 are general purpose, we expect
that weights learned from one domain are applicable to other do-
mains, eliminating the the time and effort required to re-learn these
values on each new domain. The results in Table 8show that the
weight vectors that are learned for one domain perform well when
used to rank the results for a new domain. The average decrease in
the number of top ranked programs is only 1: 9% (with a maximum
decrease of 5: 0%). For the number of top-3 ranked programs the
change is insigniÔ¨Åcant with a maximum decrease of less than 0: 5%
and thus we do not include them here. This result demonstrates
that the learning of the component weights is highly domain inde-
pendent and generalizes well, allowing it to be reused (or used as a
starting point) for new domains.
7. RELATED WORK
Programming by demonstration (PBD) systems, which use a
trace of a task performed by a user, and programming by example
(PBE) systems, which learn from a set of input-output examples,
have been used to enable end-user programming for a variety of
domains. For PBD these domains include text manipulation [28]
and table transformations [23] among others [7]. Recent work on
PBE by Gulwani et. al. has included domains for manipulating
strings [13, 48], numbers [49], and tables [21]. As mentioned ear-
lier, both PBD and PBE based techniques struggle when the desired
transformations involve conditional operations. In contrast the nat-
ural language based approach in this work performs well for both
simple and conditional operations.
Keyword programming refers to the process of translating a set
or sequence of keywords into function calls over some API. This
API may consist either of operations in an existing programming
language [34, 41, 54] or a DSL constructed for a speciÔ¨Åc class of
tasks [32, 33]. These techniques use various program synthesis ap-
proaches to build expression trees from the elements of the under-
lying API, similar to the Bagalgorithm in ¬ß4, and then use simple
heuristics, such as words used and keyword-to-terminal weights, to
rank the resulting expression trees. These systems have low pre-
cision when used on inputs with complex intents, 50%-60% accu-
racy in [34], and will frequently suggest incorrect programs. Con-
versely, due to the ranking methodology, the synthesizer in this pa-
per is able to maintain high accuracy even on complex domains
such as ATIS (where we achieve 88% accuracy).Semantic parsing [39] is a sophisticated means of constructing a
program from natural language using a specialized language parser.
Several approaches including, syntax directed [25], NLP parse
trees [11], SVM driven [24], combinatory categorical grammars [27,
56, 57], and dependency-based semantics [30, 42] have been pro-
posed. These systems have high precision, usually suggesting the
correct program, but recall rates below 85% [57] in ATIS. In con-
trast the technique in this paper achieves similar or higher levels of
precision while providing a recall rate near 92%.
A number of natural language programming systems have been
built around grammars, NLC [4], or templates, NaturalJava [45],
which impose various constraints on input expressions. Such sys-
tems are sensitive to grammatical errors or extraneous words. There
has been extensive research on developing natural language inter-
faces to databases (NLIDB) [2, 40]. While early systems were
based on pattern matching the user‚Äôs input to one of the known pat-
terns, PRECISE [43, 44] translates semantically tractable NL ques-
tions into corresponding SQL queries. These systems depend heav-
ily on the underlying data having a known schema which makes
them impractical when the underlying data structure is unknown
(or non-existent) as in the text-editing domain used in this work.
SmartSynth [29] is a system for synthesizing smartphone scripts
from NL. The synthesis technique in SmartSynth is highly spe-
cialized to the underlying smartphone domain and uses a simple
the ranking strategy for the programs that it produces. Similarly,
the NLyze [20] system synthesizes spreadsheet formulas from NL.
Again, NLyze is designed for a speciÔ¨Åc domain (spreadsheet for-
mula) and uses a relatively simple ranking system consisting of
only the equivalent of the coverage, mapping, and overlap features
presented in our paper. In contrast, our technique is agnostic to
the speciÔ¨Åcs of the target DSL, the ranking features are indepen-
dent of the underlying DSL, and we automatically learn appropri-
ate weights for the features. In addition, as the experimental results
inTable 6 demonstrate, the use of the ranking methodologies de-
scribed in SmartSynth or NLyze results in substantial reductions
in recall/precision. Thus, the ranking methodology in this paper
presents an improvement on those use in NLyze/SmartSynth and
an opportunity to further improve their performance by integrating
the advancements.
The work in [47] leverages natural language to enable composi-
tional PBE (programming by examples). It does not apply any nat-
ural language learning techniques used in PBNL approaches, but
only utilizes the natural language decomposition into phrases fol-
lowed by asking the user to provide example based interpretation
of those phrases.
8. CONCLUSION
This paper develops a meta approach for synthesizing programs
from natural language descriptions that can be instantiated for a
range of interesting DSL‚Äôs including text-processing, automata con-
struction, and information retrieval queries. Our approach takes
three inputs from the synthesis designer, a suitable DSL deÔ¨Ånition,
a basic training data set, and assistance in construction of a words-
to-token dictionary, and from these inputs constructs a correspond-
ing high-precision and high-recall NL-to-DSL synthesizer.
We aim to further generalize the framework to allow construction
of synthesizers for a wider variety of domains. Another area of
investigation is the addition of context-awareness when translating
single-line intents (like ours) to provide interactive programming
environment for programming in larger DSLs and accomplishing
more complex tasks.
354References
[1] R. Alur, L. D‚ÄôAntoni, S. Gulwani, D. Kini, and
M. Viswanathan. Automated grading of DFA constructions.
InIJCAI, 2013.
[2] I. Androutsopoulos, G. Ritchie, and P. Thanisch. Natural lan-
guage interfaces to databases - an introduction. CoRR, 1995.
[3] D. Angluin. Learning regular sets from queries and coun-
terexamples. Inf. Comput., 75(2):87‚Äì106, 1987.
[4] B. W. Ballard and A. W. Biermann. Programming in natu-
ral language: ‚ÄúNLC" as a prototype. In Annual conference,
ACM, 1979.
[5] D. P. Bertsekas. Nonlinear Programming: 2ndEdition.
Athena Press, 2004.
[6] C. M. Bishop and N. M. Nasrabadi. Pattern recognition and
machine learning, volume 1. springer New York, 2006.
[7] A. Cypher. Watch What I Do: Programming by Demonstra-
tion. MIT Press, 1993.
[8] D. A. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-
Smith, D. Pallett, C. Pao, A. Rudnicky, and E. Shriberg. Ex-
panding the scope of the atis task: The atis-3 corpus. In HLT,
1994.
[9] A. Desai, S. Gulwani, V . Hingorani, N. Jain, A. Karkare,
M. Marron, R. Sailesh, and S. Roy. Benchmarks for pro-
gram synthesis using natural language, January 2016. https:
//sites.google.com/site/nl2pgm/.
[10] Y . Freund, R. Iyer, R. E. Schapire, and Y . Singer. An efÔ¨Å-
cient boosting algorithm for combining preferences. J. Mach.
Learn. Res., 4, Dec. 2003.
[11] R. Ge and R. J. Mooney. A statistical semantic parser that
integrates syntax and semantics. In CoNLL, 2005.
[12] S. Gulwani. Dimensions in program synthesis. In PPDP,
2010.
[13] S. Gulwani. Automating string processing in spreadsheets us-
ing input-output examples. In POPL, 2011.
[14] S. Gulwani. Synthesis from examples: Interaction models and
algorithms. In SYNASC, 2012.
[15] S. Gulwani. Flash Fill: Excel 2013 Feature , 2013.
http://research.microsoft.com/en-us/um/people/sumitg/
Ô¨ÇashÔ¨Åll.html.
[16] S. Gulwani. Example-based learning in computer-aided stem
education. In CACM, 2014.
[17] S. Gulwani, W. Harris, and R. Singh. Spreadsheet data ma-
nipulation using examples. CACM, 2012.
[18] S. Gulwani, S. Jha, A. Tiwari, and R. Venkatesan. Synthesis
of loop-free programs. In PLDI, 2011.
[19] S. Gulwani, V . A. Korthikanti, and A. Tiwari. Synthesizing
geometry constructions. In PLDI, 2011.
[20] S. Gulwani and M. Marron. NLyze: Interactive programming
by natural language for spreadsheet data analysis and manip-
ulation. In SIGMOD, 2014.
[21] W. R. Harris and S. Gulwani. Spreadsheet table transforma-
tions from examples. In PLDI, 2011.
[22] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. On
the naturalness of software. In ICSE, 2012.
[23] S. Kandel, A. Paepcke, J. Hellerstein, and J. Heer. Wrangler:
Interactive visual speciÔ¨Åcation of data transformation scripts.
InCHI, 2011.
[24] R. J. Kate and R. J. Mooney. Using string-kernels for learning
semantic parsers. In ACL, 2006.[25] R. J. Kate, Y . W. Wong, and R. J. Mooney. Learning to trans-
form natural to formal languages. In AAAI, 2005.
[26] D. Klein and C. Manning. Accurate unlexicalized parsing. In
ACL, 2003.
[27] T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steed-
man. Lexical generalization in CCG grammar induction for
semantic parsing. In EMNLP, 2011.
[28] T. Lau, S. Wolfman, P. Domingos, and D. Weld. Program-
ming by demonstration using version space algebra. Machine
Learning, 53(1-2), 2003.
[29] V . Le, S. Gulwani, and Z. Su. Smartsynth: Synthesizing
smartphone automation scripts from natural language. In Mo-
biSys, 2013.
[30] P. Liang, M. I. Jordan, and D. Klein. Learning dependency-
based compositional semantics. In ACL, 2011.
[31] H. Lieberman. Your Wish Is My Command: Programming by
Example. Morgan Kaufmann, 2001.
[32] G. Little, T. A. Lau, A. Cypher, J. Lin, E. M. Haber, and
E. Kandogan. Koala: Capture, share, automate, personalize
business processes on the web. In CHI, 2007.
[33] G. Little and R. C. Miller. Translating keyword commands
into executable code. In UIST, 2006.
[34] G. Little and R. C. Miller. Keyword programming in Java.
Autom. Softw. Eng., 16(1):37‚Äì71, 2009.
[35] Z. Manna and R. J. Waldinger. A deductive approach to pro-
gram synthesis. ACM TOPLAS, 2(1), 1980.
[36] M. Manshadi, J. F. Allen, and M. D. Swift. A corpus of scope-
disambiguated english text. In ACL (Short Papers), 2011.
[37] M. Mayer, G. Soares, M. Grechkin, V . Le, M. Marron,
O. Polozov, R. Singh, B. G. Zorn, and S. Gulwani. User inter-
action models for disambiguation in programming by exam-
ple. In UIST, 2015.
[38] G. A. Miller. Wordnet: A lexical database for english. CACM,
2012.
[39] R. J. Mooney. Learning for semantic parsing. In CICLing,
2007.
[40] N. Nihalani, S. Silakari, and M. Motwani. Natural language
interfce for database: A brief review. IJCSI, 8(2), 2011.
[41] D. Perelman, S. Gulwani, T. Ball, and D. Grossman. Type-
directed completion of partial expressions. In PLDI, 2012.
[42] H. Poon. Grounded unsupervised semantic parsing. In ACL,
2013.
[43] A.-M. Popescu, A. Armanasu, O. Etzioni, D. Ko, and
A. Yates. Modern natural language interfaces to databases:
Composing statistical parsing with semantic tractability. In
COLING, 2004.
[44] A.-M. Popescu, O. Etzioni, and H. A. Kautz. Towards a the-
ory of natural language interfaces to databases. In IUI, 2003.
[45] D. Price, E. Riloff, J. L. Zachary, and B. Harvey. NaturalJava:
A natural language interface for programming in Java. In IUI,
2000.
[46] Ranking. ModiÔ¨Åed competition ranking ("1334" ranking).
http://en.wikipedia.org/wiki/Ranking, 2015.
[47] M. Raza, S. Gulwani, and N. Milic-Frayling. Compositional
program synthesis from natural language and examples. In
IJCAI, 2015.
[48] R. Singh and S. Gulwani. Learning semantic string transfor-
mations from examples. PVLDB, 5, 2012.
[49] R. Singh and S. Gulwani. Synthesizing number transforma-
tions from input-output examples. In CAV, 2012.
355[50] A. Solar-Lezama. Program synthesis by sketching, 2008.
[51] A. Solar-Lezama, L. Tancau, R. Bod√≠k, S. A. Seshia, and V . A.
Saraswat. Combinatorial sketching for Ô¨Ånite programs. In
ASPLOS, 2006.
[52] S. Srivastava, S. Gulwani, and J. Foster. From program veri-
Ô¨Åcation to program synthesis. In POPL, 2010.
[53] N. G. Stanford. Stanford parser - 2.0.2. http://nlp.stanford.
edu/software/lex-parser.shtml/, 2014.
[54] D. M. L. Xu, R. Bod√≠k, and D. Kimelman. Jungloid mining:
Helping to navigate the API jungle. In POPL, 2005.[55] K. Yessenov, S. Tulsiani, A. Menon, R. C. Miller, S. Gulwani,
B. Lampson, and A. Kalai. A colorful approach to text pro-
cessing by example. In UIST, 2013.
[56] L. Zettlemoyer and M. Collins. Learning to map sentences
to logical form: Structured classiÔ¨Åcation with probabilistic
categorial grammars. In UAI, 2005.
[57] L. S. Zettlemoyer and M. Collins. Online learning of re-
laxed CCG grammars for parsing to logical form. In EMNLP-
CoNLL, 2007.
356