Supporting the Statistical Analysis
of V ariability Models
Ruben Heradio
Universidad Nacional de
Educacion a Distancia
Madrid, Spain
rheradio@issi.uned.esDavid Fernandez-Amoros
Universidad Nacional de
Educacion a Distancia
Madrid, Spain
david@issi.uned.esChristoph Mayr-Dorn
Johannes Kepler University
Linz, Austria
christoph.mayr-dorn@jku.atAlexander Egyed
Johannes Kepler University
Linz, Austria
alexander.egyed@jku.at
Abstract â€”Variability models are broadly used to specify the
conï¬gurable features of highly customizable software. In practice,
they can be large, deï¬ning thousands of features with theirdependencies and conï¬‚icts. In such cases, visualization techniquesand automated analysis support are crucial for understanding themodels. This paper contributes to this line of research by pre-senting a novel, probabilistic foundation for statistical reasoningabout variability models. Our approach not only provides a newway to visualize, describe and interpret variability models, butit also supports the improvement of additional state-of-the-artmethods for software product lines; for instance, providing exactcomputations where only approximations were available before,and increasing the sensitivity of existing analysis operations forvariability models. We demonstrate the beneï¬ts of our approachusing real case studies with up to 17,365 features, and writtenin two different languages (KConï¬g and feature models).
Index T erms â€”Variability modeling, feature modeling, software
product lines, software visualization, binary decision diagrams.
I. I NTRODUCTION
A common challenge in software engineering is enabling
and coping with many variants of software products that are
customized for different market segments or contexts of use.
This is explored in paradigms such as Software Product Lines
(SPLs) [1] or Context-Aware Software [2]. An essential tool
to tackle this challenge are V ariability Models (VMs), which
specify the common and variable features available for thesoftware products, together with the inter-feature conï¬‚icts and
dependencies [3], [4].
Numerous visualization methods [5] and analysis operations
[6] support the reasoning on non-trivial VMs. Introduced in1990, feature diagrams [7] are the prevalent way to visualize
VMs as graphs whose nodes and edges depict features and
inter-feature relationships. Such representation works nicely
for small VMs, but it becomes ineffective for large models
because the resulting graphs are overly complicated. Many
analysis operations are excessively rigid. For instance, current
approaches for detecting dispensable features only identifythose that, due to conï¬‚icts/dependencies with the remaining
features, cannot be included in any product at all, overlooking
thus features with a reusability insigniï¬cantly above zero.
This paper proposes an alternative way to reason about
VMs. The basic idea is adopting a method that, in many otherknowledge domains, has proven to be successful for describingand interpreting variation in large samples/populations: statis-
tics. For that, it presents two algorithms that compute theprimary elements needed for the VM statistical analysis: (i)
the Feature Inclusion Probability (FIP) algorithm determines
the probability for a feature to be included in a valid product,
and (ii) the Product Distribution (PD) algorithm determines
the number of products having a given number of features.
SPL engineering typically distinguishes two roles: the do-
main engineer and the application engineer [8]. Whereas the
domain engineer undertakes the product line development (i.e.,she engineers forreuse), the application engineer obtains par-
ticular systems from the product line through a conï¬guration
process (i.e., she engineers with reuse). Our approach assists
both roles.
Regarding the domain engineer, our method supports repre-
senting the feature and product variation using general statis-
tical plots (e.g., histograms, box-plots, etc.), and summarizing
the variation through descriptive statistics (e.g., mean, standard
deviation, etc.). This way, the engineer receives informationabout the complexity of the software products, and the SPL
itself. Moreover, our approach supports augmenting the sensi-
tivity of binary analysis operations by redeï¬ning them intoprobabilistic terms, hence providing a continuous range of
values instead of a simplistic yes/no categorization. Engineers
may use this, for instance, to detect highly dispensable featureswhose reuse probability is close to zero, but not exactly zero.
Regarding the application engineer, our method provides
information about the implications of her decisions (i) interms of features (e.g., if feature ğ‘“is selected, which other
features become selected/excluded due to their dependencies/conï¬‚icts with ğ‘“?), and also (ii) in terms of the ï¬nal product
(e.g., if feature ğ‘“is selected, what size will the ï¬nal product
probably have?). Moreover, some procedures have been pro-posed to guide the engineer through the conï¬guration spaceby using the concept of feature probability [9], [10], [11],
[12]. However, as existing methods for computing feature
probabilities do not scale for large VMs [13], probabilities
are often roughly approximated from samples of historical
product conï¬gurations [12], [14] or set manually by theengineer according to her beliefs [15]. This paper contributes
to conï¬guration guidance procedures by supporting the exact
and scalable feature probability computation.
8432019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
1558-1225/19/$31.00 Â©2019 IEEE
DOI 10.1109/ICSE.2019.00091
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. Most existing methods for automated reasoning on VMs
convert the models into Boolean logic formulas for subsequent
processing with logic engines [16]. This translation of VMs
into Boolean logic is a well-studied problem, supported for
most VM notations, such as feature models [16], KConï¬g [17],
[18], or CDL [19]. Our algorithms work with practically every
VM notation as they build on the Binary Decision Diagram
(BDD) [20], [21] encoding of the VM Boolean formulas.
We demonstrate the feasibility and beneï¬t of our approach
with real VMs speciï¬ed in two distinct languages (KConï¬g
and feature models). The investigated VM examples differ in
the number of features (ranging from small to huge with up
to 17,365 features), and come from different application do-
mains (open source software projects, the automotive industry,
and web conï¬gurators). Among other issues, the experiments
reveal that some models have a surprisingly high number of
features with extremely low reusability.
The remainder of this paper is organized as follows: Section
II motivates the statistical analysis of variability models, illus-
trating its beneï¬ts with a real example. Section III describes
our algorithms in detail. Section IV reports the application
of the approach to distinct case studies. Section V discusses
related work. Finally, Section VI summarizes this paperâ€™s main
conclusions and outlines directions for future research.
II. M OTIV A TING THE STA TISTICAL ANALYSIS OF
V ARIABILITY MODELS
Most approaches for providing engineers with visualization
assistance to understand non-trivial VMs use graphs (or trees),
whose nodes and edges represent features and constraints,
respectively [5]. Feature models are the most widespread
graphical notation for VMs [22].
In practice, VMs can be huge [23] and for those cases,
their visual graph representation becomes ineffective. For ex-
ample, the EmbToolkit project (www.embtoolkit.org) eases the
application development and ï¬rmware generation for highly
customized embedded Linux products. Its VM is speciï¬ed in
a text-based language called KConï¬g, which is also used in
other popular open source projects, such as the Linux Kernel,
uClib, or axTLS. The KConï¬g speciï¬cation of EmbToolkit
encompasses 1,815 conï¬gurable features, together with 7,193
inter-feature constraints. Figure 1 shows the graph representa-
tion of the KConï¬g speciï¬cation of EmbToolkit 1.7.0. Such
visual representation offers little value even when zooming in
to make the node labels readable.
In contrast, we propose a statistical approach to describe and
interpret the variation of the features and products speciï¬ed
by a VM. In the following subsections, we outline how this
method assists both the domain and the application engineers.
A. Domain engineerâ€™s support
Our algorithms provide the fundamental information to
enable answering the following key questions:
1) How complex are the products?: The complexity of a
product can be roughly measured by its number of features
[24]. Our PD algorithm computes the productsâ€™ distribution
Fig. 1: Graph-representation of the EmbToolkit KConï¬g
regarding their number of features. This distribution is the
basis for distinct plots and descriptive statistics further char-
acterizing productsâ€™ complexity.
For instance, the density plot in Figure 2 and the descriptive
statistics in Table I summarize the product distribution for
the KConï¬g speciï¬cation of EmbToolkit 1.7.0. This way, the
engineer becomes aware that the most frequently occurring
number of features for a product is 773, that the smallest and
largest products have 19 and 1398 features, respectively, etc.
Fig. 2: EmbToolkit product distribution
Mean Standard Median Median Mode Min Max Range
deviation absolute
deviation
741.49 330.91 748 391.41 773 19 1398 1379
TABLE I: Product distribution descriptive statistics
2) How complex is the product line?: The complexity of a
SPL may be characterized by the following three core metrics:
the number of features the SPL manages, the number of valid
products that can be derived, and the resulting homogeneity
of those products [24] (i.e., how much does one product differ
from the others). The PD algorithm in combination with our
844
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. Feature Inclusion Probability (FIP) algorithm provides a clear
picture of the productsâ€™ homogeneity. Figure 3 presents two
extreme cases: one of extreme homogeneity (top), the other of
extreme heterogeneity (bottom):
âˆ™The top row describes a SPL where products are very
homogeneous because (i) most products contain a similar
number of features (i.e., its distribution has low variance - see
the plot on the left), and (ii) most features are nearly always
included (i.e., the feature probabilities are close to one and
have low variance - right plot).
âˆ™The bottom row describes a SPL where the products are
very heterogeneous because (i) some products may contain
only a few features while others may contain a high number
of features (i.e., the product distribution has high variance),
and (ii) most features are nearly never included in a product
(i.e., the feature probabilities are close to zero and have low
variance).
Fig. 3: Productsâ€™ homogeneity characterization
3) Should the SPL be refactored to simplify its mainte-
nance?: The histogram in Figure 4 depicts the feature prob-
ability distribution for EmbToolkit 1.7.0. Three zones have
been highlighted in the plot, whose detailed information is
summarized in Table II:
âˆ™The red shaded area (left) highlights the features with
probability less than or equal to 0.05 of being included
in a valid product. The extreme cases are those with zero
probability, which are commonly called dead [6], [25].
Interestingly, 6.23% of the EmbToolkit features are dead,
and thus they should be removed from the KConï¬g speciï¬ca-
tion as they are completely without value.
âˆ™The green shaded area (right) emphasizes the features that
are required by almost every valid product, being the extreme
cases those with probability one, which are usually called core
as they are present in all products.
âˆ™The yellow shaded zone (middle) identiï¬es low-constraint
features. In particular, those with probability 0.5 are typically
pure optional features whose selection is unconstrained.
Dead features Unconst. opt. features Core features
ğ‘=0 ğ‘â‰¤0.05 ğ‘=0.50.475â‰¤ğ‘â‰¤0.525 ğ‘=1 ğ‘â‰¥0.095
6.23% 11.9% 25.73% 38.95% 1.21% 5.29%
TABLE II: Dead, core, and (potentially) optional features
Fig. 4: EmbToolkit feature probability distribution
Our approach also provides assistance when historical data
about the actual feature inclusion are available; e.g., the De-
bian popularity contest gathers information about how many
times each Debian package has been installed (https://popcon.
debian.org/). In this case, the domain engineer compares the
VM statistics with the historical ones. If, for example, the
actual products tend to be much smaller than the product dis-
tribution mode obtained from the VM, then perhaps the SPL is
unnecessarily complex and could be simpliï¬ed. Understanding
the answers to these questions is thus of essential value for
SPL and product testing, evolution, and reuse.
B. Application engineerâ€™s support
Our approach supports the application engineerâ€™s decision
making by showing the impact that a decision has on:
1) The remaining features: For example, if the
engineer selects the ARM architecture for EmbToolkit
(EMBTK_ARCH_ARM ), then our FIP algorithm will
show that some other features will necessarily be
excluded from the product (e.g., the probability of
KEMBTK_UCLIBC_TARGET_mips becomes zero), and
that the selection of other features will become difï¬cult
(e.g., the probability of EMBTK_CLIB_GLIBC decreases to
7.41â‹…10âˆ’35). It is worth noting that our approach determines
feature exclusion beyond explicit constraints among two
features by considering the overall set of constraints and
currently selected features.
2) The product under conï¬guration: For instance, our FIP
and PD algorithms support providing plots such as the one in
Figure 5, which shows how the conï¬guration space shrinks
with each engineerâ€™s decision about selecting/excluding fea-
tures. Note that the product distribution variance decreases
progressively until it becomes zero at the end of the conï¬gu-
ration process.
Fig. 5: Visualizing the product derivation progress
845
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. Several heuristics have been proposed to speed up product
conï¬guration by taking advantage of the fact that, due to the
inter-feature constraints, some decisions can be automatically
derived from other decisions previously made. Some of those
heuristics are based on approximating feature probabilities
[10], [11]. Since our FIP algorithm computes those prob-
abilities, it provides better support for the aforementioned
heuristics.
III. C OMPUTING FEA TURE AND PRODUCT DISTRIBUTIONS
This section describes a new method to compute the feature
and product distributions from a VM. First, Section III-A in-
troduces some probability deï¬nitions and the BDD technology
our approach is built upon. Then, Sections III-B and III-C
explain our algorithms in detail.
A. Preliminaries
The KConï¬g ï¬le in Figure 6 will be used throughout
this section as running example. It is composed of several
conï¬gs that specify three features ğ´,ğµandğ¶, and their
interdependencies.
1config A
2 bool "A value?"
3 select C if !B
4config B
5 bool "B value?"
6 depends on A
7config C
8 bool
Fig. 6: Running
example: a
KConï¬g ï¬leAll features are Boolean (Lines 2,
5 and 8), meaning that they can be
either selected or deselected. Features
can acquire their value from the user
input, but also from other feature values.
For instance, Conï¬gs ğ´andğµspecify
aprompt to request the user about their
feature values (e.g., "A value?" ). In
contrast, ğ¶does not specify any prompt,
and its value is derived as follows: ğ¶is
selected whenever ğ´is selected, but not
ğµ(Line 3). Finally, feature ğµdepends
onğ´, i.e., to be selected in a product,
ğµrequires that ğ´is selected as well.
As a result, the conï¬guration space encompasses only
three valid products: {ğ´,ğµ,ğ¶},{ğ´,ğµ,ğ¶},{ğ´, ğµ, ğ¶}, where
ğ‘“orğ‘“represents that feature ğ‘“is selected or deselected,
respectively. Therefore:
âˆ™The product distribution, regarding the number of features
each product has, is: one product with zero features
({ğ´,ğµ,ğ¶}), zero products with one feature, two products
with two features ( {ğ´,ğµ,ğ¶}and{ğ´, ğµ, ğ¶}), and zero
products with three features.
âˆ™The probability of A, B and C to be selected in a valid
product is 2/3,1/3and1/3, respectively.
1) Boolean representation of variability models: Most ap-
proaches for automated reasoning on VMs are based on
converting the models into Boolean logic formulas, which are
then processed with logic engines.
The details of this translation can be found in [16] and [18]
for feature and KConï¬g models, respectively.
For instance, the VM in Figure 6 is equivalent to the formula
Î¦=( ( ğ´âˆ§ğµ)â†”ğ¶)âˆ§(ğµâ†’ğ´), whose truth table
is summarized in Table III (1 and 0 means true and false,
respectively).ABC Î¦
000 1
001 0
010 0
011 0
100 0
101 1
110 1
111 0
TABLE III: Running
example truth tableThe truth table contains all pos-
sible conï¬gurations. The valid and
invalid products are represented by
rows where Î¦is 1 and 0, respec-
tively. For each row, the inclu-
sion/exclusion of a feature is rep-
resented by 1/0 in its correspond-
ing column. For example, the sev-
enth row depicts the valid product
{ğ´, ğµ, ğ¶}.
The following probabilities are
deï¬ned from the truth table:
âˆ™ğ‘(Î¦) andğ‘(Î¦)are the probabilities of Î¦to be 1 and 0,
respectively; ğ‘(Î¦) andğ‘(Î¦)are calculated as the number
of rows where Î¦is 1 and 0, respectively, divided by
the total number of rows. In Table III, ğ‘(Î¦) = 3 /8and
ğ‘(Î¦) = 5 /8.
âˆ™ğ‘(ğ‘¥,Î¦)is the joint probability ofğ‘¥andÎ¦to be both 1; it
is computed as the number of rows where both ğ‘¥andÎ¦
are 1 divided by the total number of rows. For example,
ğ‘(ğ´,Î¦) = 2 /8. It is worth noting that joint probabilities
are symmetrical, i.e., ğ‘(ğ‘¥,Î¦) = ğ‘(Î¦,ğ‘¥). Obviously, other
joint probabilities can be deï¬ned negating ğ‘¥orÎ¦; e.g.,
ğ‘(ğ´,Î¦) = 1 /8,ğ‘(ğ´,Î¦ )=2 /8, etc.
âˆ™The conditional probability ğ‘(ğ‘¥âˆ£Î¦)is the probability that
ğ‘¥is 1 knowing beforehand that Î¦is 1. In other words, it is
the number of rows where both ğ‘¥andÎ¦are 1 divided by
the number of rows where Î¦is 1. For example, ğ‘(ğ´âˆ£Î¦) =
2/3,ğ‘(ğ´âˆ£Î¦) = 1 /3, etc.
In this paper, we are especially interested in getting the
probability each feature has to be included in a valid product,
i.e.,ğ‘(ğ‘¥âˆ£Î¦). Nevertheless, this computation will be built upon
other probabilities. In particular, by deï¬nition:
ğ‘(ğ‘¥âˆ£Î¦) =ğ‘(ğ‘¥,Î¦)
ğ‘(Î¦)â‡’ğ‘(ğ‘¥,Î¦) = ğ‘(ğ‘¥âˆ£Î¦)ğ‘(Î¦)
Likewise, ğ‘(Î¦âˆ£ğ‘¥)=ğ‘(Î¦,ğ‘¥)
ğ‘(ğ‘¥)â‡’ğ‘(Î¦,ğ‘¥)=ğ‘(Î¦âˆ£ğ‘¥)ğ‘(ğ‘¥).
As joint probabilities are symmetrical, then ğ‘(ğ‘¥,Î¦) =
ğ‘(Î¦,ğ‘¥)â‡’ğ‘(ğ‘¥âˆ£Î¦)ğ‘(Î¦) = ğ‘(Î¦âˆ£ğ‘¥)ğ‘(ğ‘¥)â‡’ğ‘(ğ‘¥âˆ£Î¦) =
ğ‘(Î¦âˆ£ğ‘¥)ğ‘(ğ‘¥)
ğ‘(Î¦). This last relation, which supports deriving ğ‘(ğ‘¥âˆ£Î¦)
from ğ‘(Î¦âˆ£ğ‘¥), is known as Bayesâ€™ rule , and it will be used in
Section III-B to get ğ‘(ğ‘¥âˆ£Î¦).
2) Binary decision diagrams: Truth tables are convenient
to understand the concepts we will handle to get the feature
probabilities and product distribution, but not to make the
computations because their size grows exponentially with the
number of variables (a table with ğ‘›variables has 2ğ‘›rows).
In contrast, BDDs, which can be thought as compressed truth
tables without redundancies, are by far more scalable [20],
[26], [21]. An example that illustrates their compression power
is reported in this paper experimental section: the KConï¬g
speciï¬cation of the uClibc library for developing embedded
Linux systems has 306 features and thus its truth table would
have 2306rows; nevertheless, its BDD encoding has only 3,085
nodes.
846
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. (a) Non-reduced
 (b) Reduced
Fig. 7: BDD encoding for the KConï¬g example in Figure 6
A BDD is a rooted directed acyclic graph where (i) all
terminal nodes are labeled with 0 or 1, and (ii) all non-terminal
nodes are labeled with a Boolean variable. Each non-terminal
node has exactly two edges from that node to others: one
labeled 0 and the other 1. They are called the low and high
edges, and are usually represented graphically with dashed and
solid lines, respectively. A BDD is ordered if the variables
always appear in the same order for all the paths from the
root to the terminal nodes. For instance, Figure 7a represents
a BDD with the ordering [ğ´, ğµ, ğ¶ ]for our running example.
It has eight nodes, two terminals ğ‘›0andğ‘›1, and six non-
terminals ğ‘›2,ğ‘›3,...,ğ‘›7.
Likewise rows in truth tables, paths in BDDs represent
variable assignments. In a path, ğ‘¥is assigned to 0 (or 1) if
it goes through the low (or high) outgoing edge of a node
labeled ğ‘¥, and the resulting evaluation is 0 (or 1) if the path
ends up in the terminal 0 (or 1). For example, the 6throw
in Table III ( ğ´,ğµ,ğ¶, Î¦) corresponds to the path ğ‘›7â†’ğ‘›6
/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadrightğ‘›4â†’ğ‘›1in Figure 7a.
To save memory, BDDs are usually reduced by (R1) re-
moving duplicated nodes (i.e., nodes that are the roots of
structurally identical subBDDs), and (R2) deleting nodes with
identical outgoing edges. In Figure 7a, R1 was performed but
not R2, as the shaded node ğ‘›2could be removed. Figure 7b
shows a completely reduced BDD without these redundant
nodes.
From here on, we will assume that BDDs are ordered and
totally reduced. Thus, the algorithms we present in the next
sections deal not only with the existing nodes in the BDD, but
also with those removed due to R2.
In Section II-B, we saw that, in order to assist the applica-
tion engineer to understand the impact of her decisions, it is
convenient to restrict the conï¬guration space according to a
given set of selected/excluded features. Fortunately, most BDD
libraries include a function called restrict that provides
exactly this functionality [27].
Finally, Algorithm 1 shows Bryantâ€™s method [20] to traverse
a BDD in a depth-ï¬rst fashion, which will be used by our
algorithms. Traverse is called at the top level with the BDD
root as argument, and with a Boolean mark for every node
being either all true or all false. Traverse visits all nodesby recursively visiting the low ğ‘›LO and high ğ‘›HIsubBDDs
rooted by ğ‘›. Whenever a node is visited, its mark value is
complemented. Comparing the marks of ğ‘›and its children,
it can be determined if they have already been visited. The
method ensures that each node is visited exactly once and that,
when traverse ï¬nishes, all node marks have the same value.
Algorithm 1. Bryantâ€™s method for BDD traversing
1Function traverse( n)
2mark( ğ‘›)â†mark( ğ‘›)
3 ifğ‘›is non-terminal then
4 ifmark( ğ‘›)âˆ•= mark( ğ‘›LO)then traverse( ğ‘›LO)
5 ifmark( ğ‘›)âˆ•= mark( ğ‘›HI)then traverse( ğ‘›HI)
6traverse( ROOT )
B. Computing feature probabilities
Algorithm 2 (FIP) obtains, for each feature, the proportion
of valid products that include it, i.e., ğ‘(ğ‘¥âˆ£Î¦).T od os o ,i t
applies Bayesâ€™ rule to ultimately derive ğ‘(ğ‘¥âˆ£Î¦)from ğ‘(Î¦âˆ£ğ‘›).
First, the deï¬nition of conditional probability is used in Line
37:ğ‘(ğ‘¥âˆ£Î¦) =ğ‘(ğ‘¥,Î¦)
ğ‘(Î¦); being ğ‘(Î¦) andğ‘(ğ‘¥,Î¦) computed by
the auxiliary Functions getNodePr andgetJointPr .
1) Computing node probabilities: In a BDD, let us deï¬ne
the probabilities ğ‘(ğ‘›)andğ‘(ğ‘›)for a node ğ‘›as the number of
paths that go from the root to the terminal nodes by traversing
ğ‘›through its high and low outgoing edges, respectively,
divided by the total number of paths. Let us start reasoning on
how to compute ğ‘(ğ‘›)when Reduction R2 has not been done
yet. For instance, in Figure 7a, ğ‘(ğ‘›6)=2 /8since there are
eight paths in total from root to terminals, and two of them
go through the high edge of ğ‘›6:ğ‘›7â†’ğ‘›6â†’ğ‘›3â†’ğ‘›0and
ğ‘›7â†’ğ‘›6â†’ğ‘›3/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadrightğ‘›1
By construction, in a truth table every variable ğ‘¥is 1 half
the rows, and it is 0 the other half. For instance, in Table III,
there are four rows where ğµis 1, and there are other four
rows where ğµis 0. This fact can be expressed as ğ‘(ğ‘¥)=
ğ‘(ğ‘¥)=1 /2. If R2 is not applied, ğ‘(ğ‘¥)=ğ‘(âˆª
ğ‘›labeled ğ‘¥ğ‘›);
being ğ‘(âˆª
ğ‘›labeled ğ‘¥ğ‘›)=âˆ‘
ğ‘›labeledğ‘(ğ‘›)because all BDD
paths are mutually exclusive as they represent independent
variable assignments. For example, in Figure 7a, ğ‘(ğµ)=
ğ‘(ğ‘›5)+ğ‘(ğ‘›6)=2 /8+2 /8=1 /2.
The ï¬rst variable in the BDD ordering is represented by
a single node: the root. So, ğ‘(ROOT) = ğ‘(ROOT) = 1 /2.
The next variable in the ordering is encoded with two nodes
ROOTHIandROOTLObecause every node has exactly two
outgoing edges. Hence, the variable probability is shared out
both nodes and thus ğ‘(ROOT HI)= ğ‘(ROOTHI)=1/2
2,
andğ‘(ROOT LO)= ğ‘(ROOTLO)=1/2
2. Proceeding this
way, the node probabilities will be subsequently divided by
two until the terminal nodes are reached. Finally, we need
to be aware that whereas a node always has two outgoing
edges, it may have any number greater than one of incoming
edges. Therefore, for a non-terminal node ğ‘›with parents
ğ‘¢1,ğ‘¢2,...ğ‘¢ ğ‘ , then ğ‘(ğ‘›)=âˆ‘ğ‘ 
ğ‘–=1ğ‘(ğ‘¢ğ‘–)
2; and for a terminal node,
847
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. Algorithm 2. Feature Inclusion Probability (FIP)
1Function getNodePr( n)
2mark( ğ‘›)â†mark( ğ‘›)
3 ifğ‘›is non-terminal then
// explore low
4 ifğ‘›LO is terminal then ğ‘(ğ‘›LO)â†ğ‘(ğ‘›LO)+ğ‘(ğ‘›)
5 else ğ‘(ğ‘›LO)â†ğ‘(ğ‘›LO)+ğ‘(ğ‘›)
2
6 ifmark( ğ‘›)âˆ•= mark( ğ‘›LO)then getNodePr( ğ‘›LO)
// explore high
7 ifğ‘›HI is terminal then ğ‘(ğ‘›HI)â†ğ‘(ğ‘›HI)+ğ‘(ğ‘›)
8 else ğ‘(ğ‘›HI)â†ğ‘(ğ‘›HI)+ğ‘(ğ‘›)
2
9 ifmark( ğ‘›)âˆ•= mark( ğ‘›HI)then getNodePr( ğ‘›HI)
10 Function getJointPr( n)
11mark( ğ‘›)â†mark( ğ‘›)
12 ifğ‘›is non-terminal then
// explore low
13 ifğ‘›LO=ğ‘›0then ğ‘(Î¦âˆ£ğ‘›)â†0
14 else if ğ‘›LO=ğ‘›1then ğ‘(Î¦âˆ£ğ‘›)â†1
15 else
16 ifmark( ğ‘›)âˆ•= mark( ğ‘›LO)then getJointPr( ğ‘›LO)
17 ğ‘(Î¦âˆ£ğ‘›)â†ğ‘(Î¦,ğ‘›LOâˆ¨ğ‘›LO)
2ğ‘(ğ‘›LO)
18 ğ‘(ğ‘›,Î¦)â†ğ‘(Î¦âˆ£ğ‘›)ğ‘(ğ‘›)
// explore high
19 ifğ‘›HI=ğ‘›0then ğ‘(Î¦âˆ£ğ‘›)â†0
20 else if ğ‘›HI=ğ‘›1then ğ‘(Î¦âˆ£ğ‘›)â†1
21 else
22 ifmark( ğ‘›)âˆ•= mark( ğ‘›HI)then getJointPr( ğ‘›HI)
23 ğ‘(Î¦âˆ£ğ‘›)â†ğ‘(Î¦,ğ‘›HIâˆ¨ğ‘›HI)
2ğ‘(ğ‘›HI)
24 ğ‘(ğ‘›,Î¦)â†ğ‘(Î¦âˆ£ğ‘›)ğ‘(ğ‘›)
// combine both low and high
25 ğ‘(Î¦,ğ‘›âˆ¨ğ‘›)â†ğ‘(Î¦,ğ‘›)+ğ‘(Î¦,ğ‘›)
26 ğ‘(var( ğ‘›),Î¦)â†ğ‘(var( ğ‘›))+ ğ‘(ğ‘›,Î¦)
// add joint probabilities of the removed nodes
27 foreach ğ‘¥ğ‘—betweenvar(ğ‘›)andvar(ğ‘›HI)do
28 ğ‘(ğ‘¥ğ‘—,Î¦)â†ğ‘(ğ‘¥ğ‘—,Î¦)+ğ‘(ğ‘›,Î¦)
2
29 foreach ğ‘¥ğ‘—betweenvar(ğ‘›)andvar(ğ‘›LO)do
30 ğ‘(ğ‘¥ğ‘—,Î¦)â†ğ‘(ğ‘¥ğ‘—,Î¦)+ğ‘(ğ‘›,Î¦)
2
31ğ‘(ROOT) â†1/2
32ğ‘(ğ‘›ğ‘–)â†0for all nodes ğ‘›ğ‘–except the BDD root
33getNodePr( ROOT )
34ğ‘(ğ‘¥ğ‘—,Î¦)â†0for all variables ğ‘¥ğ‘—
35getJointPr( ROOT )
36ğ‘(Î¦)â†ğ‘(ğ‘›1)
37 foreach ğ‘¥ğ‘—doğ‘(ğ‘¥ğ‘—âˆ£Î¦)â†ğ‘(ğ‘¥ğ‘—,Î¦)
ğ‘(Î¦)
ğ‘(ğ‘›)=âˆ‘ğ‘ 
ğ‘–=1ğ‘(ğ‘¢ğ‘–)(the parentsâ€™ probability is not divided as
the node has no outgoing edges).
Let us move now to realistic BDDs, where R2 is performed.
In this case, we need to take into account the removed nodes:
ğ‘(ğ‘¥)=ğ‘((âˆª
ğ‘›labeled ğ‘¥ğ‘›)
âˆª(âˆª
ğ‘›â€²labeled ğ‘¥
but removedğ‘›â€²))
=âˆ‘
ğ‘›ğ‘(ğ‘›)+âˆ‘
ğ‘›â€²ğ‘(ğ‘›â€²)
Let us see how to compute the number of redundant nodes
removed between any two nodes due to R2. If the variables
follow the ordering [ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘ ],l e tvar(ğ‘›)be the position
of the variable that labels the node ğ‘›in the ordering. For
example, in Figure 7b, var(ğ‘›4)=2 since ğ‘›4is labeled ğµ, and
ğµis in the second position of the ordering [ğ´, ğµ, ğ¶ ]. Finally,
letvar(ğ‘›0)=v a r ( ğ‘›1)=ğ‘ +1 . Then, var(ğ‘›LO)âˆ’var(ğ‘›)âˆ’1is the number of nodes that have been removed between ğ‘›
andğ‘›LO, and var(ğ‘›HI)âˆ’var(ğ‘›)âˆ’1is the number of nodes
that have been removed between ğ‘›andğ‘›HI. For example, as
var(ğ‘›0)âˆ’var(ğ‘›4)âˆ’1=4âˆ’2âˆ’1=1 , it can be deduced that
one node was removed in the high edge that goes from ğ‘›4to
ğ‘›0(i.e., the shaded node ğ‘›2in Figure 7a).
When a non-reduced BDD has a path ğ‘¢â†’ğ‘›1âˆ’ â†’/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadrightğ‘›2âˆ’ â†’/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadright...âˆ’ â†’/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadrightğ‘£, after applying R2 the path becomes ğ‘¢â†’ğ‘£
According to what was previously discussed above, ğ‘(ğ‘›1)=
ğ‘(ğ‘¢)/2. For the rest of the nodes ğ‘›2,ğ‘›3,...,ğ‘£ , the probability
is not divided again since both the high and low edges go to the
same node, e.g., ğ‘(ğ‘›2)=ğ‘(ğ‘›1HI)+ğ‘(ğ‘›1LO)
2=ğ‘(ğ‘¢)/2+ğ‘(ğ‘¢)/2
2=
ğ‘(ğ‘¢)/2. To sum up, (i) the probability of the reduced nodes
between any two nodes ğ‘¢andğ‘£isğ‘(ğ‘¢)/2, and (ii) the
probability of ğ‘£is not affected by the amount of reduced
nodes, being equal to ğ‘(ğ‘¢)/2as well.
Function getNodePr combines the ideas discussed above
with Bryantâ€™s traverse method. In Algorithm FIP , ğ‘(ROOT) is
set to 1/2, and ğ‘(ğ‘›)is initialized to 0 for the remaining nodes
(Lines 31-32). Then, getNodePr traverses the BDD in pre-
order to update ğ‘(ğ‘›). Finally, it is worth noting that ğ‘(Î¦) =
ğ‘(ğ‘›1)andğ‘(Î¦) = ğ‘(ğ‘›0), being ğ‘(Î¦) andğ‘(Î¦)the proportion
of valid and invalid products of the VM, respectively.
2) Computing joint probabilities: Following the same ar-
gumentation line than in the previous section:
ğ‘(ğ‘¥,Î¦) =âˆ‘
ğ‘›ğ‘(ğ‘›,Î¦) +âˆ‘
ğ‘›â€²ğ‘(ğ‘›â€²,Î¦)
Let us start ï¬rst with the non-reduced nodes. By deï¬nition,
ğ‘(ğ‘›,Î¦) = ğ‘(Î¦âˆ£ğ‘›)ğ‘(ğ‘›). As we rely on Bryantâ€™s recursive
method to perform the computations, let us deï¬ne ğ‘(Î¦âˆ£ğ‘›)in
function of ğ‘›high descendant (as the probability is condi-
tioned to ğ‘›=1 , in principle we only care about the high
descendant). Two cases need to be considered:
1) When ğ‘›HIis terminal, (a) if ğ‘›HI=ğ‘›0it means that the
path is evaluated to 0, i.e., Î¦is 0 for the variable assignment
the path represents and so ğ‘(Î¦âˆ£ğ‘›)=0 ; (b) otherwise as ğ‘›HI=
ğ‘›1thenğ‘(Î¦âˆ£ğ‘›)=1 .
2) When ğ‘›HIis non-terminal, ğ‘(Î¦âˆ£ğ‘›)is calculated as:
ğ‘(Î¦âˆ£ğ‘›)=ğ‘(Î¦âˆ£ğ‘›HIâˆ¨ğ‘›HI)=ğ‘(Î¦,ğ‘›HIâˆ¨ğ‘›HI)
ğ‘(ğ‘›HIâˆ¨ğ‘›HI)
=ğ‘(Î¦,ğ‘›HI)+ğ‘(Î¦,ğ‘›HI)
ğ‘(ğ‘›HI)+ğ‘(ğ‘›HI)=ğ‘(Î¦,ğ‘›HI)+ğ‘(Î¦,ğ‘›HI)
2ğ‘(ğ‘›HI)
Equation 1 summarizes the cases above to compute ğ‘(Î¦âˆ£ğ‘›).
As it needs knowing ğ‘(Î¦,ğ‘›HI), Equation 2 is used (which is
indeed the symmetrical case of Equation 1).
ğ‘(Î¦âˆ£ğ‘›)=â§
ï£´â¨
ï£´â©0 ifğ‘›HI=ğ‘›0
1 ifğ‘›HI=ğ‘›1
ğ‘(Î¦,ğ‘›HI)+ğ‘(Î¦,ğ‘›HI)
2ğ‘(ğ‘›HI)otherwise(1)
ğ‘(Î¦âˆ£ğ‘›)=â§
ï£´â¨
ï£´â©0 ifğ‘›LO=ğ‘›0
1 ifğ‘›LO=ğ‘›1
ğ‘(Î¦,ğ‘›LO)+ğ‘(Î¦,ğ‘›LO)
2ğ‘(ğ‘›LO)otherwise(2)
848
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. Function getJointPr in Algorithm FIP uses both Equa-
tions 1 and 2 to get the joint probability ğ‘(ğ‘¥,Î¦) for non-
removed nodes (Lines 13-26). Then, Equation 3 is applied to
obtain ğ‘(ğ‘›â€²,Î¦)for the removed nodes ğ‘›â€²(Lines 27-30). It is
worth noting that such equation follows the same reasoning
presented in Section III-B1 to obtain ğ‘(ğ‘›â€²).
ğ‘(ğ‘›â€²,Î¦) ={ğ‘(ğ‘›,Î¦)
2ifğ‘›â€²was between ğ‘›andğ‘›HI
ğ‘(ğ‘›,Î¦)
2ifğ‘›â€²was between ğ‘›andğ‘›LO(3)
C. Computing product distribution
Algorithm 3 (PD) sketches the computation of the prod-
uct distribution, accounting for how many products have no
features, one feature, two features, ..., all features. It uses
Bryantâ€™s method to traverse the BDD in post-order by calling
the auxiliary Function getProdDist with the BDD root as
argument. From the terminals to the root, it progressively ob-
tains the partial distributions that correspond to the subBDDs
rooted by each node, being the ï¬nal distribution placed at the
root.
Algorithm 3. Product Distribution (PD)
1Function getProdDist( n)
2mark( ğ‘›)â†mark( ğ‘›)
3 ifğ‘›is non-terminal then
// traverse
4 ifmark( ğ‘›)âˆ•= mark( ğ‘›LO)then getProdDist( ğ‘›LO)
// compute lowDist to account for the removed
nodes through low
5 removedNodes â†var(ğ‘›LO)âˆ’var(ğ‘›)âˆ’1
6 letlowDist b eav e c t o rw i t h removedNodes + length of
dist( ğ‘›LO)zeros
7 for (ğ‘–=0 ;ğ‘–â‰¤removedNodes ;ğ‘–++) do
8 for (ğ‘—=0 ;ğ‘—< length ofdist( ğ‘›LO);ğ‘—++) do
9 lowDist[ ğ‘–+ğ‘—]â†
lowDist[ ğ‘–+ğ‘—]+dist( ğ‘›LO)[ğ‘—]â‹…(removedNodes
ğ‘–)
// traverse
10 ifmark( ğ‘›)âˆ•= mark( ğ‘›HI)then getProdDist( ğ‘›HI)
// compute highDist to account for the removed
nodes through high
11 removedNodes â†var(ğ‘›HI)âˆ’var(ğ‘›)âˆ’1
12 lethighDist be a vector with removedNodes + length of
dist( ğ‘›HI)zeros
13 for (ğ‘–=0 ;ğ‘–â‰¤removedNodes ;ğ‘–++) do
14 for (ğ‘—=0 ;ğ‘—< length ofdist( ğ‘›HI);ğ‘—++) do
15 highDist[ ğ‘–+ğ‘—]â†
highDist[ ğ‘–+ğ‘—]+dist( ğ‘›HI)[ğ‘—]â‹…(removedNodes
ğ‘–)
// combine low and high distributions
16 iflowDist is longer than highDist then
17 distLength â† length ofdist( ğ‘›LO)
18 elsedistLength â† length ofdist( ğ‘›HI)+1
19 letdist( ğ‘›)be a vector of length distLength ï¬lled with zeros
20 for (ğ‘–=0 ;ğ‘—< length oflowDist ;ğ‘–++) do
21 dist( ğ‘›)[ğ‘–]â†lowDist[ ğ‘–]
22 for (ğ‘–=0 ;ğ‘—< length ofhighDist ;ğ‘–++) do
23 dist( ğ‘›)[ğ‘–+1]â†dist( ğ‘›)[ğ‘–+1]+highDist[ ğ‘–]
24dist( ğ‘›0)â†[]// no products
25dist( ğ‘›1)â†[1]// one product with no features
26getProdDist( ROOT )
27 return dist(ROOT )
Figure 8 shows each nodeâ€™s distribution for our running
example, which is stored in different vectors dist. Starting from
0, the position ğ‘–in a dist vector accounts for the number ofproducts that have ğ‘–features; e.g., dist (ğ‘›3)=[ 0 ,1]because
the subBDD with nodes ğ‘›0,ğ‘›1,andğ‘›3represents no products
with zero features, and one product with one feature (i.e.,
product {ğ¶}).
Fig. 8: dist vectorsgetProdDist â€™s recursive base
cases are node ğ‘›0, representing no
products at all, and node ğ‘›1, rep-
resenting a single product with no
features. Accordingly, dist (ğ‘›0)=[]
and dist (ğ‘›1) = [1] (Lines 24-25).
To understand the more advanced
recursive cases, three observations
need to be done:
1) Including new features into all
products is achieved by shifting the
dist vector to the right (O1): Let
us imagine that dist =[ 1,0,4], i.e.,
there is 1 product with 0 features,
0 products with 1 feature, and 4
products with 2 features.
If no new features are added, dist remains the same. If one
feature is added to all products, dist becomes [0,1,0,4], i.e.,
there are no products without features because all of them have
at least the new feature, the product that had zero features now
have 1 feature, and the 4 products that had 2 features now
have 3 features. If two features are added to all products, dist
becomes [0,0,1,0,4], and so on.
In general, the addition of ğ‘ features to all products means
shifting dist ğ‘ positions to the right.
2) Combining dist vectors is accomplished by adding
them (O2): Let us think about how to get dist(ğ‘›)from
dist(ğ‘›LO)anddist(ğ‘›HI). First, let us suppose that no nodes
were removed between ğ‘›and its descendants. Imagine that
dist(ğ‘›LO)=[ 2 ,0,3]anddist(ğ‘›HI)=[ 1 ,2]. According to
O1,dist(ğ‘›HI)needs to be shifted one position to account for
the additional feature that labels ğ‘›. Then, both descendants
distributions are combined by just adding them: dist(ğ‘›)=
[2,0,3] + [0 ,1,2] = [1 ,1,3].
3) Removed nodes require taking into account both ob-
servations O1 and O2, and blending them by means of
combinatorial numbers (O3): If a non-reduced BDD had a
path ğ‘¢/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadrightğ‘›1âˆ’ â†’/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadrightğ‘›2âˆ’ â†’/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadright...ğ‘›ğ‘ âˆ’ â†’/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadrightğ‘£, R2 would remove
theğ‘ redundant nodes, and thus the path would become ğ‘¢
/amsdasharrowaxis/amsdasharrowaxis/amsarrowheadrightğ‘£. Hence, dist (ğ‘¢LO)should be adjusted as any of the
removed nodes could be set to 1, and so one new feature would
be added to all products. Furthermore, any pair of redundant
nodes(ğ‘ 
2)
could also be set to 1, any combination of three
nodes(ğ‘ 
3)
,..., and ï¬nally the combination of ğ‘ nodes(ğ‘ 
ğ‘ )
.
original dist (ğ‘¢LO) 1 0 4
adding(2
1)
features 0 2 0 8
adding(2
2)
features 0 0 1 0 4
adjusted ï¬nal dist (ğ‘¢LO) 1 2 5 8 4
TABLE IV: Distribution
adjustmentLet us see how the
adjustment should work
with an example: imagine
that dist (ğ‘›LO)=[ 1 ,0,4]and
two nodes where removed
between ğ‘›andğ‘›LO. Table IV
summarizes the computations.
The ï¬rst and last rows
849
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. represent the initial and
adjusted distributions, respectively. The two intermediate
rows represent the required adjustments.
First, adding one feature to all products implies shifting dist
one position to the right (O1). As there are(2
1)
=2!
1!(2âˆ’1)!=
2different combinations of one feature, two shifted vectors
should be added (O2). As a result, [1,0,4]becomes[
0,1â‹…(2
1)
,0â‹…(2
1)
,4â‹…(2
1)]
=[ 0,2,0,8].
Second, there is only one possibility(2
2)
=1 to add two
features to all products. So, [1,0,4]becomes [0,0,1,0,4].
Finally, all distributions are combined by adding them (O2):
[1,0,4] + [0 ,2,0,8] + [0 ,0,1,0,4] = [1 ,2,5,8,4].
Lines 5-9 and 11-15 of Algorithm PD adjust the low and
high distributions of the non-terminal nodes to account for
the removed nodes. Then, Lines 16-23 combine both adjusted
distributions.
D. Computational complexity
Both Algorithms FIP and PD traverse the whole BDD, and
thus their complexity depends linearly on the number ğ‘of
BDD nodes. Visiting each node requires (i) one loop on the
number ğ‘‰of variables for FIP , and (ii) two nested loops on
the variables for PD. As a result, the time complexities are
ğ‘‚(ğ‘ğ‘‰)andğ‘‚(ğ‘ğ‘‰2)for FIP and PD, respectively.
IV . E XPERIMENTAL ANALYSIS OF VM S
This section reports the analysis of seven VMs gathered
from open source projects and academic repositories with
the aim of illustrating the usefulness and generality of our
approach. All the material described in this section (implemen-
tation of the FIP and PD algorithms, VM benchmark, BDD-
encoding of the VMs, and results of the analysis) is available
at the following public repository:
https://github.com/rheradio/VMStatAnal
A. Experimental setup
Our algorithms have been implemented as an extension of
the library CUDD 3.0 for BDDs (https://github.com/vscosta/
cudd). The benchmark is composed of VMs coming from dif-
ferent application domains and speciï¬ed in distinct languages:
(i)axTLS ,Fiasco ,uClibc ,Busybox , and EmbToolkit are open
source projects to enable the creation of highly customizable
products, whose variability models are written in KConï¬g; (ii)
the Dell feature model speciï¬es a laptop conï¬gurator reverse-
engineered from the DELL homepage; and (iii) Automotive is
a feature model coming from the automotive industry. Table
V summarizes (i) the models, (ii) the size of the BDDs that
encode them, (iii) and our algorithmsâ€™ running times on an
HP ProLiant DL360 G9 with an Intel Xeon E5-2660v3.
B. Results
Our approach enables reasoning on VMs under two per-
spectives:
âˆ™The productsâ€™ view . Table VI provides descriptive statis-
tics for the VMsâ€™ product distribution regarding theirVM VM Reference #Features #Clauses BDD Running time
name notation #nodes FIP PD
axTLS
1.5.3KConï¬g http://axtls.
sourceforge.net/64 119 108 0.018s 0.019s
Dell Feature [11] 118 2,304 1,876 0.055s 0.052s
Laptops Model
Fiasco
2014092821KConï¬g https://os.inf.tu-
dresden.de/ï¬asco/125 4,717 1,235 0.020s 0.033s
uClibc
20150420KConï¬g https://www.
uclibc.org/306 903 4,862 0.362s 0.315s
Busybox
1.23.2KConï¬g https://busybox.
net/677 572 1,036 0.213s 0.323s
EmbToolkit
1.7.0KConï¬g https://www.
embtoolkit.org/1,815 7,193 263,636 12.863s 14.716s
Automotive
02Feature
model[28] 17,365 321,933 30,432 1m 54.321s 1m 2.922s
TABLE V: VM benchmarkVM name
Mean
SD
Min
Max
ğ‘=0
ğ‘â‰¤0.05
ğ‘=0.5
0.475â‰¤ğ‘â‰¤0.525
ğ‘=1
ğ‘â‰¥0.095
axTLS 25.46 10.46 3 46 0 9.38 6.25 37.50 0 3.12
Dell 17.50 2.24 14 21 0 47.46 0 2.54 0.85 0.85
Fiasco 24.84 9.70 4 44 31.20 46.40 15.20 24.80 0 1.60
uClibc 106.49 46.13 8 200 2.61 23.86 25.49 35.29 0 2.94
Busybox 324.44 149.05 5 635 2.95 3.55 37.81 53.91 0.44 3.10
EmbToolkit 741.49 330.91 19 1,398 6.23 11.9 25.73 38.95 1.21 5.29
Automotive 4,048.48 778.7 2,562 5,472 0.03 57.31 13.92 18.66 9.71 10.39
TABLE VI: Descriptive statistics for product distribution, and
percentage of dead, core, and unconstrained optional features
number of features, and Figure 9 visualizes that distri-
bution.
âˆ™The featuresâ€™ view . Figure 10 shows the feature probabil-
ity distribution, and colored columns in Table VI detail
the number of features in the zones dead ,unconstrained
optional , and core .
The product distribution graphs (Fig. 9) and feature prob-
ability distribution graphs (Fig. 10) (respectively Table VI)
highlight the existence of two rough VM groups. In the ï¬rst
group, axTLS, uClibc, Busybox, and EmbToolkit represent
families of loosely constraint products. V alid products may
range from consisting of only a few features (as low as three
features for axTLS), to close to all features (e.g., over 90% of
all features in the case of Busybox). Hence, also the feature
probability distribution graphs for these models show more
features in the range 0.475â‰¤ğ‘â‰¤0.525 compared to the
range ğ‘â‰¤0.05. In contrast, the second group consisting
of Dell Laptops, Fiasco, and Automotive, represents SPLs
with rather restricted products. V alid products may contain
at a maximum 18%, 35%, and 32%, respectively, of available
features compared to the ï¬rst group with 72%, 65%, 94%,
and 77% respectively. SPLs in the second group also tend to
come with highly rare features. Between 46% and 57% of all
features have a reusing probability less or equal than 0.05. A
detailed list of all feature probabilities for every VM in the
benchmark is published at our repository. This list will help
domain engineers to polish their VMs, especially for Fiasco,
which has a surprisingly high percentage of dead features:
31.2%. For Dell Laptops and due to the sensitivity augment
that our FIP algorithm provides, some low reusable features
are discovered where current approaches do not detect any
problem at all: although there are no dead features, 17.8% of
850
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. Dellâ€™s features are allowed in at most 0.001% of the valid
products.
Fig. 9: Product distribution
V. R ELA TED WORK
The seminal work by Kang et al. [7] established what has
been the mainstream for visually representing VMs from 1990
to nowadays: graphs whose nodes depict features, and whose
edges represent inter-feature constraints. The most popular
notation is feature modeling [5], which puts the emphasis
on those constraints that enable arranging the features hierar-
chically as a tree [3]. There are also other graph notations,
e.g., decision diagrams [29], the OVM language [30], etc.
Nevertheless, the differences among notations are minor, and
so most approaches can be considered equivalent [4].
As in practice variability models can include thousands of
features [23], some efforts have been made to clarify the
Fig. 10: Feature probability distribution
visualization of large graph VM representations: applying 3D
techniques to visualize the graphs in the space instead of the
plane [31], supporting zooming on different graph areas [32],
focusing the visualization on a selected feature [33], decom-
posing the graphs [34], etc. Our work complements existing
research by introducing an alternative way to look at VMs
through statistics, supporting thus the use of centrality/spread
measures, plots, etc.
Sections V-A and V-B discuss related work that aims to
assist domain and application engineering, respectively.
A. Domain engineerâ€™s assistance
A literature review by Benavides et al. [6] reports thirty
analysis operations on VMs, most of them oriented to domain
engineering. This paper supports augmenting the sensitivity of
851
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. some of those operations. For instance, a feature is typically
considered dead if it cannot appear in any product at all. The
main reason why most approaches stick to this deï¬nition for
detecting dispensable features is due to the current limitations
of the technology they are built upon, as they detect whether a
feature ğ‘“in a VM Î¦is dead by checking with a SA T solver if
ğ‘“âˆ§Î¦is unsatisï¬able [28]. In contrast, our algorithms support
a more ï¬‚exible deï¬nition, detecting features with an extremely
low probability of being selected.
Beek et al. [15], [35], [36] point out the convenience of
providing the domain engineer with information about the
product distribution regarding distinct quantitative attributes
(e.g., number of features, product cost, failure probability,
etc.). Their approach requires (i) that the domain engineer sets
manually the feature probabilities, or (ii) that the feature prob-
abilities are derived from historical data. Then, the product dis-
tribution is estimated by generating multiple samples through
a simulation process. Compared to Beek et al.â€™s method, our
procedure provides the exact product distribution instead of an
approximation. Nevertheless, Algorithm PD currently supports
only one quantitative attribute, the number of features, and
could be extended to consider domain speciï¬c properties.
B. Application engineerâ€™s assistance
There are several approaches to guide the application
engineer through product conï¬guration. Some of them are
built upon historical data about previous conï¬gurations. For
instance, Pereira et al. [12], [14] proposes a recommender
system that limits the engineerâ€™s decision space towards con-
ï¬gurations included in historical data. In addition, Martinez
et al. [37] provide the engineer with feedback on the impact
of her decisions by estimating the feature probabilities from
historical data. These approaches have several shortcomings:
ï¬rst, the historical data may not be a representative sample of
the product population, especially if the sample size is small
and its variance is high; and most important, feature selectivity
cannot be strictly constrained to a sample. For example, if a
non-dead feature is not included in any conï¬guration of the
historical data, then the system could conclude erroneously
that the engineer should never select such feature.
Other approaches, instead of relying on previous conï¬gura-
tions, work directly with the VM. For example, Czarnecki et
al. [9] suggest the application of the entropy measure to guide
the VM conï¬guration process, which is calculated from the
feature probabilities. In addition, N Â¨ohrer et al. [10], [11] pro-
pose another heuristic also based on the feature probabilities.
However, none of those works scale to large VMs.
Fernandez-Amoros et al. [38] provide an algorithm to com-
pute the feature probabilities from a feature model. However,
the algorithm is speciï¬c for feature models and it does not
scale when many constraints cross the tree structure of the
feature model.
To the extent of our knowledge, Algorithm FIP is the most
scalable and general approach to compute the feature proba-
bilities from a VM. This way, our work not only supports the
conï¬guration heuristics that rely on the feature probabilitiesobtained from the VM, but also the ones based on historical
data. In the latter case, our algorithms can be used to overcome
the limitations of reasoning exclusively on the basis of a
single product sample by applying Bayesian inference [39]
to combine both the prior probabilities coming from the VM
with the posterior probabilities coming from historical data.
VI. C ONCLUSIONS AND FUTURE WORK
In this paper, the algorithmic foundation for analyzing VMs
from an innovative perspective has been presented, where the
featuresâ€™ and productsâ€™ variation is visualized and described
using statistics. We have justiï¬ed why this approach beneï¬ts
both, the domain and the application engineer, exemplifying
such beneï¬ts on real models gathered from open source
projects and academic repositories. We have shown that our
approach not only enables new ways to reason about VMs, but
also supports the improvement of current VM-related methods:
increasing the sensitivity of existing analysis operations on
VMs, and providing exact computations for approaches that
currently work with approximations.
We believe that our work opens a range of additional
opportunities for future research. Applied to other product line
related activities, such as testing, our work enables checking
whether current methods for SPL testing are able to generate
suites covering the whole product distribution range, and thus
avoid missing any rare boundary cases. Also, our approach
may be of assistance during maintenance of projects for highly
customizable software; e.g., we have reported that the VMs of
some relevant open source projects have an alarming amount
of dead features. The causes of those useless features need in-
vestigation. A longitudinal study would provide insights under
which circumstances these projects exhibit these problems, and
whether they are corrected or stay in the successive versions of
the VMs. Finally, our algorithms rely on the BDD encoding of
VMs. It is well-known that a BDDâ€™s size is extremely sensitive
to its variable ordering, and that ï¬nding an optimal ordering is
an NP-complete problem. Therefore, our approachâ€™s scalability
greatly depends on the performance of existing heuristics
for variable ordering. Hence, future research might look for
adapting our algorithms to other alternative logic technologies
that also support model counting, such as #SAT solvers [40]
orSentential Decision Diagrams (SDDs) [41].
ACKNOWLEDGMENT
We thank Armin Biere and Tom van Dijk for their insight
and helpful comments about the strengths and weaknesses of
BDDs, and other logic related technologies in the earlier stages
of this work, which has been supported by (i) the Spanish
Ministry of Education and V ocational Training under the
projects with reference DPI2016-77677-P and CAS17/00022,
(ii) the Austrian Science Fund (FWF): P29415-NBL funded by
the Government of Upper Austria; and (iii) the FFG, Contract
No. 854184.
852
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] R. Heradio, H. Perez-Morago, D. Fernandez-Amoros, F. J. Cabrerizo,
and E. Herrera-Viedma, â€œA bibliometric analysis of 20 years of re-
search on software product lines,â€ Information and Software Technology ,
vol. 72, pp. 1 â€“ 15, 2016.
[2] W. Cazzola and A. Shaqiri, â€œContext-aware software variability through
adaptable interpreters,â€ IEEE Software , vol. 34, no. 6, pp. 83â€“88,
November 2017.
[3] P . Heymans, P . . Schobbens, J. . Trigaux, Y . Bontemps, R. Matulevi-
cius, and A. Classen, â€œEvaluating formal properties of feature diagram
languages,â€ IET Software , vol. 2, no. 3, pp. 281â€“302, June 2008.
[4] K. Czarnecki, P . Gr Â¨unbacher, R. Rabiser, K. Schmid, and A. Wasowski,
â€œCool features and tough decisions: a comparison of variability modeling
approaches,â€ in 6th Int. Workshop on V ariability Modelling of Software-
Intensive Systems (V aMoS) , Leipzig, Germany, 2012, pp. 173â€“182.
[5] R. E. Lopez-Herrejon, S. Illescas, and A. Egyed, â€œA systematic mapping
study of information visualization for software product line engineering,â€
Journal of software: evolution and process , vol. 30, no. 2, pp. 1â€“18,
2018.
[6] D. Benavides, S. Segura, and A. Ruiz-Cort Â´es, â€œAutomated analysis of
feature models 20 years later: A literature review,â€ Information Systems ,
vol. 35, no. 6, pp. 615 â€“ 636, 2010.
[7] K. C. Kang, S. G. Cohen, J. A. Hess, W. E. Novak, and A. S. Pe-
terson, â€œFeature-Oriented Domain Analysis (FODA) Feasibility Study,â€
Carnegie Mellon University/Software Engineering Institute, Tech. Rep.
CMU/SEI-90-TR-21, November 1990.
[8] K. Czarnecki and U. Eisenecker, Generative Programming: Methods,
Tools, and Applications . Addison-Wesley Professional, 2000.
[9] K. Czarnecki, S. She, and A. Wasowski, â€œSample spaces and feature
models: There and back again,â€ in 12th Int. Software Product Line
Conference (SPLC) , Limerick, Ireland, Sept 2008, pp. 22â€“31.
[10] A. N Â¨ohrer and A. Egyed, â€œOptimizing user guidance during decision-
making,â€ in 15th Int. Software Product Line Conference (SPLC) , Mu-
nich, Germany, Aug 2011, pp. 25â€“34.
[11] A. N Â¨ohrer and A. Egyed, â€œC2O conï¬gurator: a tool for guided decision-
making,â€ Automated Software Engineering , vol. 20, no. 2, pp. 265â€“296,
Jun 2013.
[12] J. A. Pereira, P . Matuszyk, S. Krieter, M. Spiliopoulou, and G. Saake,
â€œA feature-based personalized recommender system for product-line
conï¬guration,â€ in ACM SIGPLAN Int. Conference on Generative Pro-
gramming: Concepts and Experiences (GPCE) , New Y ork, NY , USA,
2016, pp. 120â€“131.
[13] R. Heradio, D. Fernandez-Amoros, J. A. Cerrada, and I. Abad, â€œA
literature review on feature diagram product counting and its usage
in software product line economic models,â€ International Journal of
Software Engineering and Knowledge Engineering , vol. 23, no. 8, pp.
1177â€“1204, 2013.
[14] J. A. Pereira, J. Martinez, H. K. Gurudu, S. Krieter, and G. Saake,
â€œVisual guidance for product line conï¬guration using recommendations
and non-functional properties,â€ in 33rd Annual ACM Symposium on
Applied Computing (SAC) , New Y ork, NY , USA, 2018, pp. 2058â€“2065.
[15] M. H. ter Beek, A. Legay, A. Lluch-Lafuente, and A. V andin, â€œQuan-
titative analysis of probabilistic models of software product lines with
statistical model checking,â€ in 6th Workshop on F ormal Methods and
Analysis in SPL Engineering (FMSPLE@ETAPS) , London, UK, Apr.
2015, pp. 56â€“70.
[16] D. S. Batory, â€œFeature Models, Grammars, and Propositional Formulas,â€
in9th Software Product Lines Conference (SPLC) , Rennes, France, Sep.
2005, pp. 7â€“20.
[17] T. Berger and S. She, â€œFormal Semantics of the CDL Language,â€
University of Leipzig, Tech. Rep., 2010.
[18] R. Tartler, â€œMastering V ariability Challenges in Linux and Related
Highly-Conï¬gurable System Software,â€ Ph.D. dissertation, Friedrich-
Alexander-Universit Â¨at Erlangen-N Â¨urnber g, 2013.
[19] S. She and T. Berger, â€œFormal semantics of the kconï¬g language,â€
University of Waterloo, Tech. Rep., 2010.
[20] R. E. Bryant, â€œGraph-based algorithms for boolean function manipula-
tion,â€ IEEE Transactions on Computers , vol. C-35, no. 8, pp. 677â€“691,
Aug 1986.
[21] T. van Dijk and J. van de Pol, â€œSylvan: multi-core framework for deci-
sion diagrams,â€ International Journal on Software Tools for Technology
Transfer , vol. 19, no. 6, pp. 675â€“696, Nov 2017.[22] S. Apel, D. Batory, and C. Kastner, Feature-Oriented Software Product
Lines: Concepts and Implementation . Springer, 2013.
[23] T. Berger, S. She, R. Lotufo, A. Wasowski, and K. Czarnecki, â€œA study
of variability models and languages in the systems software domain,â€
IEEE Transactions on Software Engineering , vol. 39, no. 12, pp. 1611â€“
1640, Dec 2013.
[24] P . C. Clements, J. D. McGregor, and S. G. Cohen, â€œThe Structured
Intuitive Model for Product Line Economics (SIMPLE),â€ Carnegie
Mellon University/Software Engineering Institute, Tech. Rep. CMU/SEI-
2005-TR-003, 2005.
[25] H. Perez-Morago, R. Heradio, D. Fernandez-Amoros, R. Bean, and
C. Cerrada, â€œEfï¬cient identiï¬cation of core and dead features in vari-
ability models,â€ IEEE Access , vol. 3, pp. 2333â€“2340, 2015.
[26] C. Meinel and T. Theobald, Algorithms and Data Structures in VLSI
Design: OBDD - F oundations and Applications . Springer, 1998.
[27] M. Huth and M. Ryan, Logic in Computer Science: Modelling and
Reasoning about Systems . Cambridge University Press, 2004.
[28] S. Krieter, T. Th Â¨um, S. Schulze, R. Schr Â¨oter, and G. Saake, â€œPropagating
conï¬guration decisions with modal implication graphs,â€ in 40th Int.
Conference on Software Engineering (ICSE) , New Y ork, NY , USA,
2018, pp. 898â€“909.
[29] K. Schmid, R. Rabiser, and P . Gr Â¨unbacher, â€œA comparison of decision
modeling approaches in product lines,â€ in 5th Workshop on V ariability
Modeling of Software-Intensive Systems (V aMoS) , New Y ork, NY , USA,
2011, pp. 119â€“126.
[30] K. Pohl, F. V . D. Linden, and G. Bockle, Software Product Line
Engineering: F oundations, Principles, and Techniques , Springer, Ed.
Springer, 2005.
[31] P . Trinidad, A. R. Cort Â´es, D. Benavides, and S. Segura, â€œThree-
dimensional feature diagrams visualization,â€ in 12th Int. Software Prod-
uct Lines Conference (SPLC) , Limerick, Ireland, Sep. 2008, pp. 295â€“
302.
[32] M. Stengel, M. Frisch, S. Apel, J. Feigenspan, C. Kastner, and
R. Dachselt, â€œView inï¬nity: a zoomable interface for feature-oriented
software development,â€ in 33rd International Conference on Software
Engineering (ICSE) , Honolulu, HI, USA, May 2011, pp. 1031â€“1033.
[33] M. Garba, A. Noureddine, and R. Bashroush, â€œMusa: A scalable multi-
touch and multi-perspective variability management tool,â€ in 13th Work-
ing IEEE/IFIP Conference on Software Architecture (WICSA) , V enice,
Italy, April 2016, pp. 299â€“302.
[34] S. Urli, A. Bergel, M. Blay-Fornarino, P . Collet, and S. Mosser, â€œA visual
support for decomposing complex feature models,â€ in IEEE 3rd Working
Conference on Software Visualization (VISSOFT) , Bremen, Germany,
Sept 2015, pp. 76â€“85.
[35] M. H. ter Beek, A. Legay, A. Lluch-Lafuente, and A. V andin, â€œStatistical
model checking for product lines,â€ in 7th Int. Symposium on Leveraging
Applications of F ormal Methods, V eriï¬cation and V alidation (ISoLA) ,
Corfu, Greece, Oct. 2016, pp. 114â€“133.
[36] M. T. Beek, A. Legay, A. L. Lafuente, and A. V andin, â€œA framework for
quantitative modeling and analysis of highly (re)conï¬gurable systems,â€
IEEE Transactions on Software Engineering , p. (Early Access), 2018.
[37] J. Martinez, T. Ziadi, R. Mazo, T. F. Bissyand, J. Klein, and Y . L.
Traon, â€œFeature relations graphs: A visualisation paradigm for feature
constraints in software product lines,â€ in 2nd IEEE Working Conference
on Software Visualization (VISSOFT) ,Victoria, BC, Canada, Sept 2014,
pp. 50â€“59.
[38] D. Fernandez-Amoros, R. Heradio, J. A. Cerrada, and C. Cerrada,
â€œA scalable approach to exact model and commonality counting for
extended feature models,â€ IEEE Transactions on Software Engineering ,
vol. 40, no. 9, pp. 895â€“910, Sept 2014.
[39] J. K. Kruschke, Doing Bayesian Data Analysis, 2nd Edition: a Tutorial
with R, JAGS, and Stan . Academic Press/Elsevier, 2015.
[40] C. P . Gomes, A. Sabharwal, and B. Selman, Handbook of Satisï¬ability .
IOS Press, 2009, ch. Model Counting, pp. 633â€“654.
[41] A. Darwiche, â€œSDD: A New Canonical Representation of Propositional
Knowledge Bases,â€ in 22nd Int. Joint Conference on Artiï¬cial Intelli-
gence (IJCAI) , 2011, pp. 819â€“826.
853
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 09:39:03 UTC from IEEE Xplore.  Restrictions apply. 