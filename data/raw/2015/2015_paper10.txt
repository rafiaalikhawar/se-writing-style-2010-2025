Can Testedness be Effectively Measured?
Iftekhar Ahmed
Oregon State University
ahmedi@oregonstate.eduRahul Gopinath
Oregon State University
gopinatr@oregonstate.eduCaius Brindescu
Oregon State University
brindesc@oregonstate.edu
Alex Groce
Oregon State University
agroce@gmail.comCarlos Jensen
Oregon State University
cjensen@oregonstate.edu
ABSTRACT
Among the major questions that a practicing tester faces are
deciding where to focus additional testing eort, and decid-
ing when to stop testing. Test the least-tested code, and stop
when all code is well-tested, is a reasonable answer. Many
measures of \testedness" have been proposed; unfortunately,
we do not know whether these are truly eective.
In this paper we propose a novel evaluation of two of the
most important and widely-used measures of test suite qual-
ity. The rst measure is statement coverage, the simplest
and best-known code coverage measure. The second mea-
sure is mutation score, a supposedly more powerful, though
expensive, measure.
We evaluate these measures using the actual criteria of
interest: if a program element is (by these measures) well
tested at a given point in time, it should require fewer fu-
ture bug-xes than a \poorly tested" element. If not, then
it seems likely that we are not eectively measuring tested-
ness. Using a large number of open source Java programs
from Github and Apache, we show that both statement cov-
erage and mutation score have only a weak negative corre-
lation with bug-xes. Despite the lack of strong correlation,
there arestatistically and practically signicant dierences
between program elements for various binary criteria. Pro-
gram elements (other than classes) covered by any test case
see about half as many bug-xes as those not covered, and
a similar line can be drawn for mutation score thresholds.
Our results have important implications for both software
engineering practice and research evaluation.
CCS Concepts
‚Ä¢Software and its engineering !Empirical software val-
idation;
Keywords
test suite evaluation, coverage criteria, mutation testing, sta-
tistical analysis
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
¬© 2016 ACM. ISBN 978-1-4503-2138-9.
DOI: 10.1145/12351. INTRODUCTION
The quality of software artifacts is one of the key concerns
for software practitioners and is typically measured through
eective testing. While it is widely held that \you cannot
test quality into a product," you canuse testing to detect
that the Software Under Test (SUT) has faults, and to esti-
mate its likely overall quality. Moreover, while testing itself
does not produce quality, it leads to the discovery of faults.
When these faults are corrected, software quality improves.
Testing software poses questions. First, how much testing
is needed? Has \enough" testing been done? Second, where
should future test eorts be applied in a partially tested pro-
gram? The typical approach to answering these questions is
to measure the quality of the test suite, not the SUT. Nu-
merous measures, primarily focused on code coverage [20{22]
have been proposed, and numerous organizations set testing
requirements in terms of coverage levels [37]. Both code
coverage and mutation score measure the \testedness" of an
SUT, using the dynamic results of executing a test suite1.
However, it is not established that using such testing re-
quirements, or suite quality measures in general, translates
into eective an practice for producing better software.
While test driven development, in particular, has pushed
testing to new prominence, practicing software programmers
often balk at having to satisfy what they see as arbitrary
coverage requirements. Some go so far as to suggest that
\code coverage is a useless target measure"2. Some studies
seem to support this conclusion, at least in part [25]. The
utility of testing itself has even come under attack3.
Our aim in this paper is to place the evaluation of test
suites (and thus decision-making in testing) on a rmer foot-
ing, using a measure that translates directly into practical
terms. There is, of course, no end to studies measuring
the eectiveness of test suite evaluation techniques. How-
ever, these studies tend to either cover only a few subject
programs or faults, not use real faults, not measure what
1Most studies consider coverage as measuring the test-
edness of the entire SUT for a given suite, but it is also
obviously reasonable to project this concept onto individual
program elements and measure how well tested each is, and
most practical applications of coverage assume this usage as
well.
2For examples, see http://martinfowler.com/bliki/
TestCoverage.html and http://blog.ploeh.dk/2015/11/16/
code-coverage-is-a-useless-target-measure/.
3For examples, see https://pragprog.com/
magazines/2012-10/its-not-about-the-unit-tests and
http://www.rbcs-us.com/documents/Why-Most
unit-Testing-is-Waste.pdfdevelopers directly care about, or assume the validity of mu-
tation testing, which is itself a relatively unproven method
for evaluating a test suite. The methodology of such studies
also often involves using tool-generated or randomly-chosen
subset-based test suites to measure correlation. Such suites
may not resemble real-world test suites, and thus be of little
relevance to most developer practice.
We propose a simpler and more direct method of evalua-
tion that eliminates some of the concerns mentioned above.
It can be argued that a correlation between measure of suite
quality and fault detection is meaningless if the faults de-
tected are unimportant, trivial, or articial. As a recent
paper evaluating the ability of automated unit test gener-
ators to detect real faults phrased it \Just because a test
suite... [is eective] does not necessarily mean that it will
nd the faults that matter " [38]. It is very hard to argue
that a bug that has been discovered and xed did not mat-
ter. However, xing bugs is dicult and resource-intensive,
requiring developers (and possibly testers) to devote time to
implementing a correction (and, hopefully, validating it). In
many cases, the bug was detected because it caused prob-
lems for users. There is evidence that problems in code,
such as those identied by code smells, that do not have sig-
nicant immediate consequences are often never corrected,
even at the price of design degradation [1]. Bug-xes usually
indicate important defects as developers thought these prob-
lems worth addressing. The most practical goal of testing
therefore, is to prevent future bug-xes , by detecting faults
before release, avoiding impact on users, and usually lower-
ing development cost.
We should then evaluate test suite quality measures by a
simple process: does a higher measure of testedness for a
program element (in our work, a statement, block, method
or class) correlate with a smaller number of future bug-xes?
By avoiding whole-project measures and focusing on individ-
ual program elements, we avoid the confounding eects of
test suite size [25]. While a large test suite can produce
higher coverage and detect more faults, even if coverage and
fault detection are not themselves directly related, it can-
not (at least in any way we can imagine) cause statements
that are covered to have fewer faults than those that are
not covered, unless coverage itself is meaningful . Similarly,
using individual program entities as the basis of analysis mit-
igates some of the possible eects of, e.g., test suites with
good assertions also having better coverage. This can cause
a test suite with high coverage to perform better than a test
suite without high coverage, on average, but it cannot, so
far as we can see, plausibly result in covered entities having
fewer bug-xes than entities not covered, unless coverage it-
selfmatters.
The core argument for our analysis is as follows: if a par-
ticular program element is fully tested to conform to its spec-
ication, then that program element should have no bug-
xes applied (until the element ceases to exist as a result
of a non-bug-x modication, e.g. adding new functionality
potentially invalidating the old specication). Similarly, a
program element that is not tested at all should on aver-
age have a higher chance of seeing future bug-xes applied
for the simple reason that a fault had no chance of being
caught through testing. This, on its own, should result in
a strong negative correlation between testedness and future
bug-xes, if our fundamental assumption about the utility of
testing is correct and our measure of testedness is eective.In general, a well-tested program element should require
fewer bug xes than a less well-tested program element.
In this paper we assume (and later, based on empirical
data, argue) that testing itself is benecial. We therefore
primarily aim to evaluate the measurement of test suite qual-
ity/testedness. We focus on two important, widely studied,
measures of suite quality. First, statement coverage is the
simplest, most widely used, and easiest to understand cover-
age measure, and has some support as an eective measure
of test suite quality in recent work [21]. Second, mutation
score is not only commonly advocated as the best method
for evaluating suite quality, but is essential to most other
studies of coverage method eectiveness [22,27].
We evaluate these test suite evaluation methods empiri-
cally using a large representative set of real world programs,
real world test suites, and bug-xes, and nd that while
there is a small (but statistically signicant) negative corre-
lation between our testedness measures and future bug-xes
for program elements, the eect is so small as to be prac-
tically insignicant. There is very little useful continuous
relationship between measures of testedness and actual ten-
dency to not have bugs detected and xed, and while it is
reasonable to bet that a more-tested element will have fewer
faults, the size of the eect is very small.
However, we do nd that there isa consistent and prac-
tically (as well as statistically) signicant dierence in the
mean number of bug-xes for code, if we apply selected bi-
nary measures of \well-testedness" based on coverage or mu-
tation score. For example, program elements with at least a
75% mutation score see, on average, only about half as many
future big-xes (normalized4) as program elements with a
lower mutation score .
One intuitively appealing explanation for the low correla-
tion of testedness to bug-xes is that, even if\poorly"tested,
unimportant pieces of code are likely to see few future bug
xes. If very few users execute a program element, or if its
eects have very limited impact, then the bug will likely not
be xed (even if reported). However, the problem of varying
program element importance is unlikely to be the root cause
for the lack of a useful continuous correlation for suite eval-
uation measures. If it were, we would expect the eects of
importance to also prevent binary testedness criteria based
on mere coverage from predicting future bug-xes (since no
one will bother to test program elements that are unlikely
to ever exhibit important bugs). However, like program el-
ements with <75% mutation score, program elements that
are not covered are also likely to see nearly twice as many
future bug-xes5.
Nonetheless, perhaps a suite quality measure should re-
ect the importance of program elements. However, forcing
developers to annotate code by its importance is impracti-
cal; we need a static measure of importance. One approach
is to say that complex elements are more likely to be impor-
tant, since developing complex code with many operators
4By normalized bug-xes, we mean bug-xes per state-
ment/line for elements larger than a single statement or line;
unless we indicate otherwise, we always normalize bug-xes
when required.
5We only demonstrate this result for statements and
methods; there were too few classes that were not covered
by any tests in our data to show a signicant relationship.and conditionals, but low importance, is an unwise use of
development resources. In this case, in addition to its other
advantages, mutation testing may help take importance of
code into account, in that complex program elements pro-
duce more mutants than simple elements (e.g., a simple log-
ging statement will seldom perform any calculations, and
so often only produce a single statement-deletion mutant).
We therefore also measure whether the number of mutants
(as a measure of code complexity) predicts the number of
bug-xes applied to a program element, and whether the
number of mutants predicts the mutation score for an ele-
ment. Both eects are signicant but small. Surprisingly,
more complex code sees slightly fewer bug-xes than simple
code. As might be expected if complexity is associated with
importance, more complex code is also slightly more tested,
according to mutation score. Both eects are too weak to
be of much practical value, however.
Our ndings with respect to correlation of testedness mea-
sures and bug-xes are potentially troubling for the research
community. Software testing researchers often use a dier-
ence of a few percentage points in mutation score or a cov-
erage measure as a means to assert that one test generation
or selection technique is superior to another. However, our
data shows that relying on a few percentage points is dan-
gerous, as such small dierences may not indicate real im-
pact in terms of defects that are worth xing. On the other
hand, our data seems to support the types of \arbitrary" ad-
equacy criteria often imposed by managers or governments
(if not the precise values used). Indeed, our data suggests
that while a continuous ranking of testedness for program
elements is not currently possible, using various empirically-
validated \strata" of testedness (not covered, covered but
with poor mutation score, covered with high mutation score)
may provide a simple, practical way to direct testing eorts.
The contributions of this paper are:
A novel approach to examining the utility of test suite
quality measures that is based on direct practical con-
sequences of testing.
Analysis of relationships between bug-xes, test suite
quality (testedness) measures, and code complexity
and importance metrics for 49 sampled projects from
Github and Apache.
Evidence that there is small negative correlation be-
tween the number of mutants (normalized) and the
number of future bug-xes to a program element, indi-
cating that complexity alone does not predict impor-
tance well; in fact, more complex program elements
seem to be changed less often than simple ones. How-
ever, this may partly be due to the fact that more
complex elements are also somewhat more well-tested
(in terms of mutation score).
Evidence that the negative correlation between tested-
ness (by our measures) and number of future normal-
ized bug-xes is statistically signicant, but far too
small to have much practical impact.
Evidence that well-chosen adequacy criteria (e.g.,is the
mutation score above 75%) can be used to predict fu-
ture normalized bug-xes in a practical way (leading to
dierences of a factor of two in expected future bugs),
and can serve to distinguish untested, poorly tested,
and well-tested elements of an SUT.2. RELATED WORK
Ours is not the rst study to attempt to evaluate mea-
sures of testedness [22]. Researchers have often attempted to
prove that mutation score is well correlated with real world
faults. DeMillo et al. [13] empirically investigated the rep-
resentativeness of mutations as proxies for real faults. They
examined the 296 errors in T EX and found that 21% were
simple faults, while the rest were complex errors. Daran et
al. [11] investigated the representativeness of mutations em-
pirically using error traces. They studied the 12 real faults
found in a program developed by a student, and 24 rst-
order mutants. They found that 85% of the mutants were
similar to real faults.
Another important study by Andrews et al. [2], investi-
gated the ease of detecting a fault (both real faults and
hand seeded faults), and compared it to the ease of detect-
ing faults introduced by mutation operators. The ease was
calculated as the percentage of test cases that killed each
mutant. Their conclusion was that the ease of detection of
mutants was similar to that of real faults. However, they
based this conclusion on the result from a single program,
which makes it unconvincing. Further, their entire test set
was eight C programs, which makes the statistical inference
drawn liable to type I errors. We also note that the programs
and seeded faults were originally from Hutchins et al. [24]
who chose programs that were subject to certain specica-
tions of understandability, and the seeded faults were se-
lected such that they were neither too easy nor too dicult
to detect. In fact, the study eliminated 168 faults for be-
ing either too easy or too hard to detect, ending up with
just 130 faults. This is clearly not an unbiased selection and
cannot really tell us anything about the ease of detection
of hand seeded faults in general (because the criteria of se-
lection itself is confounding). A follow up study [3] using a
large number of test suites from a single program, space.c,
found that the mutation detection ratio and fault detection
ratio are related linearly, with similar results for other cover-
age criteria (0.83 to 0.9). Linear regression on the mutation
kill ratio and fault detection ratio showed a high correlation
(0.9).
The problems with some of these studies were highlighted
in the work of Namin et al. [34] who used the same set of
C programs as Andrews et al. [2], but combined them with
analysis of four more Java classes from the JDK. This study
used a dierent set of mutation operators and fault seed-
ing by student programmers for the Java programs. Their
analysis concluded that we have to be careful when using
mutation analysis as a stand-in for real faults. The pro-
gramming language, the kind of mutation operators used,
and even the test suite size all have an impact on the re-
lation between mutations introduced by mutation analysis
and real faults. In fact, using a dierent mutation operator
set, they found that there is only a weak correlation be-
tween real faults and mutations. However, their study was
constrained by the paucity of real faults, which were only
available for a single C program (also used in Andrews et
al. [2]). Thus, they were unable to judge the ease of detec-
tion of real faults in Java programs. Moreover, the students
who seeded the faults had knowledge of mutation analysis
which may have biased the seeded faults (thus resulting in
high correlation between seeded faults and mutants). Fi-
nally, the manually seeded faults in C programs, originally
introduced by Hutchins et al. [24], were again confounded bya selection criteria which eliminated the majority of faults
as being either too easy or too hard to detect.
Just et al. [27], using 357 real faults from 5 projects,
showed that 1) adding more fault-detecting tests to a test
suite led to the mutation score increasing more often (73%)
than either branch (50%) or statement coverage (30%) and
2) mutation score was more positively correlated with fault
detection than either of the other measures. Multiple studies
provide evidence that mutation analysis subsumes dierent
coverage measures [8,30,35], and it is on this basis that mu-
tation score is often regarded as the \gold standard" for test
suite quality measures.
One metric that is commonly used to measure the ade-
quacy of testing is code coverage, that is, a measure of the
set of program elements or code paths that are executed by a
set of tests. A large body of work considers the relationship
between coverage criteria and fault detection. Mockus et al.
[31] found that increased coverage leads to a reduction in
the number of post-release defects but increases the amount
of test eort. Gligoric et al. [19,20] used the same statistical
approach as our paper, measuring both Kendall andR2
to examine correlations, for realistically non-adequate suites.
Gligoric et al. found that branch coverage does the best job,
overall, of predicting the best suite for a given SUT, but that
acyclic intra-procedural path coverage is highly competitive
and may better address the issue of ties, which is important
in their research/method comparison context. Inozemtseva
et al. [25] investigated the relationship of various coverage
measures and mutation score for dierent random subsets of
test suites. They found that when test suite size is controlled
for, only low to moderate correlation is present between cov-
erage and eectiveness. This conclusion held for all the cov-
erage measures used. Frankl and Weiss [16] performed a
comparison of branch coverage and def-use coverage, show-
ing that def-use is more eective than branch coverage for
fault detection and there is stronger correlation to fault de-
tection for def-use than branch coverage. Namin and An-
drews [33] showed that fault detection ratio (non-linearly)
correlated well with block coverage, decision coverage, and
two dierent data-ow criteria. Their research suggested
that test suite size was a signicant factor in the model. Wei
et al. [43] examined branch coverage as a quality measure for
suites for 14 Eiel classes, showing that for randomly gener-
ated suites, branch coverage behavior was consistent across
many runs, while fault detection varied widely. In their ex-
periments, early in random testing, when branch coverage
rose rapidly, current branch coverage had high correlation
to fault detection, but branch coverage eventually saturated
while fault detection continued to increase; the correlation
at this point became very weak.
Out et al. [35] showed that mutation coverage subsumes
many other coverage criteria, including the basic six pro-
posed by Myers [32]. Gupta et al. [23] compared the eec-
tiveness and eciency of block coverage, branch coverage,
and condition coverage, with mutation kill of adequate test
suites as their evaluation metric. They found that branch
coverage adequacy was more eective (killed more mutants)
than block coverage in all cases, and condition coverage was
better than branch coverage for methods having compos-
ite conditional statements. The reverse, however, was true
when considering the eciency of suites (average number of
test cases required to detect a fault). Li et al. [29] com-
pared four dierent criteria (mutation, edge pair, all uses,and prime path), and showed that mutation adequate test-
ing was able to detect the most hand seeded faults (85%),
while other criteria performed similarly to each other (in
the range of 65% detection). Similarly, mutation coverage
required the fewest test cases to satisfy the adequacy crite-
ria, while prime path coverage required the most. Therefore,
while there are no compellingly large-scale studies of many
SUTs selected in a non-biased way to support the eective-
ness of mutation testing, it is at least highly plausible as a
better standard than other criteria.
Cai et al. [9] investigated correlations between coverage
criteria under dierent testing proles: whole test set, func-
tional test, random test, normal test, and exceptional test.
They investigated block coverage, decision coverage, C-use
and P-use criteria. Curiously, they found that the relation-
ship between block coverage and mutant kills was not always
positive. Block coverage and mutant kills had a correlation
ofR2= 0:781 when considering the whole test suite, but as
low as 0.045 for normal testing and as high as 0.944 for ex-
ceptional testing. The correlation between decision coverage
and mutation kills was higher than statement coverage, for
the whole test suite (0.832), ranging from normal test (0.368)
to exceptional test (0.952). Frankl et al. [17] compared the
eectiveness of mutation testing with all-uses coverage, and
found that at the highest coverage levels, mutation test-
ing was more eective. Kakarla [28] and Inozemtseva [26]
demonstrated a linear relationship between mutation detec-
tion ratio and coverage for individual programs. Inozemt-
seva's study used machine learning techniques to come up
with a regression relation, and found that eectiveness is
dependent on the number of methods in a test suite, with
a correlation coecient in the range 0 :81r0:93. The
study also found a moderate to high correlation, in the range
0:610:81, between eectiveness and block coverage
when test suite size was ignored, which reduced when test
suite size was accounted for. Kakarla found that statement
coverage was correlated to mutation coverage in the range
of 0:73r0:99 and 0:570:94:Gopinath et al. [21]
found that statement, out of branch, and path coverages,
best correlated with mutation score, and hence may best
predict defect density, in a study that compared suites and
mutation scores across projects, rather than using multiple
suites for the same project.
The study by Tengeri et al. [41] provided a simple (es-
sentially non-statistical) assessment of how statement cov-
erage, mutation score, and reducibility predicted project de-
fect densities for four open source projects, using a limited
set of mutation operators.
None of these studies, to our knowledge, adopted the
method used in this paper, where rather than investigate
faults and their detection, we look at whether being \well
tested" has predictive power with respect to future defects6.
Most also consider a smaller, less representative (at least
of open source projects) set of programs, and the majority
are based on programs chosen opportunistically, rather than
by our more principled sampling approach. The programs
used are often small but well-studied benchmarks such as the
Siemens/SIR [40] suite, partly for purposes of comparison to
earlier papers, and partly due to the lack of easily available
6It is remotely possible that Tengeri et al. [41] are using a
similar method, but this is not clear from their description,
and the reasoning behind our approach is not elaborated in
their work.‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
101000
100 1000 10000
cloctestsmuscore
‚óè
‚óè
‚óè
‚óè0.00
0.25
0.50
0.75
110100commitsFigure 1: CLOC vs. tests for our projects.
realistic projects with test suites and defects, before the ad-
vent of very large open source repositories. Unfortunately,
as noted by Arcuri and Briand, not at least attempting to
randomize selection of programs to study can greatly reduce
the generalizability of results [6].
3. METHODOLOGY
Our goal was to evaluate various approaches to assessing
the testedness of a program or program element using test
suite quality measures.
3.1 Collecting the Subjects
For our empirical evaluation, we tried to ensure that the
programs chosen oered a reasonably unbiased representa-
tion of modern software. We also tried to reduce the number
of variables that can contribute to random noise during eval-
uation. With these goals in mind, we chose a sample7of Java
projects from Github [18] and the Apache Software Foun-
dation [4]. All projects selected used the popular maven [5]
build system. We randomly selected 1 ;800 projects. From
these, we eliminated aggregate projects that were dicult to
analyze, leaving 1 ;321 projects, of which only 796 had test
suites. Out of these, 326 remained after eliminating projects
that did not compile (for reasons such as unavailable depen-
dencies, or compilation errors due to syntax or bad cong-
urations). Next, the projects that did not pass their own
test suites were eliminated as mutation analysis requires a
passing test suite. Finally, we eliminated projects our AST
walker could not handle. This resulted in 49 projects se-
lected. The distribution of project size vs. test suite size,
and the corresponding mutation score is given in Figure 1.
3.2 Mutant Generation
In the next phase of our analysis, we used PIT [10] for our
mutation analysis. PIT has been used in multiple previous
studies [12,21,25,39]. We extended PIT to provide the full
matrix of test failures over mutants and tests. Mutants can
basically be divided into three groups based on their runtime
behavior: not covered, killed, and live mutants. We used this
basic categorization in our analysis.
7Github allows us to access only a subset of projects using
their search API. We believe that the results returned by
Github search would not be dependent on their test suites,
and hence should not confound our results.3.3 Tracking Program Elements
We started our investigation from an arbitrarily deter-
mined recent, but not too recent, point in time deemed the
\epoch" | December 1, 2014. This was done to provide a
point from which testedness (mutation score and statement
coverage) could be calculated, and with respect to which
bug-xes could be considered to be \in the future". For
the source code and test suite at epoch, we computed muta-
tion score and statement coverage for each statement, block,
method, and class in each project.
In order to determine when a program element (statement,
block, method, or class) was changed, and track its history,
we used the GumTree Dierencing Algorithm [14]. For each
element of interest, we considered it changed if the corre-
sponding AST node was changed, or had any children that
were added, deleted or modied. The algorithm maps the
correspondence between nodes in two dierent trees, which
allowed us to accurately track the history of the program
elements.
Using AST dierencing gives us three advantages over
simple line based dierencing. The rst is that the algo-
rithm ignores any whitespace changes. Second, we are able
to track a node even if its position in the le changes (e.g.
because lines have been added or deleted before our node of
interest). Third, we are able to track nodes across refactor-
ings, as long as the node stays in the same le. For example,
we can track a node that has been moved because of an ex-
tract method refactoring.
When considering which statements to track, we used the
version of the source code at epoch to determine which AST
node resided at that particular line. We ltered only the
commits that touch the le of interest. We then tracked
that AST node forward in time, taking note of the commits
that changed that particular node. For Java, it is possible
for multiple statements to be in the same line (for example, a
local variable declaration statement inside an if statement).
In this case, we considered the innermost statement, as this
gives the most precise results.
3.4 Classifying Commits
In order to answer our research questions, we needed to
categorize the code commits. For each program element, we
computed the number of commits that touched that element
starting from the epoch. For our purpose, code commits can
be broadly grouped into one of two categories: (1) bug-xes
and improvements (modifying existing code), and (2) Other
| commits that introduced new features or functionality
(adding new code) or commits that were related to docu-
mentation, test code, or other concerns. Two key problems
are that it is not always trivial to determine which cate-
gory a commit falls under, and that larger projects see a
huge amount of activity. Manual classication of all com-
mits was therefore not an option, and we decided to use ma-
chine learning techniques for this purpose, rather than limit
the statistical power of our study (especially as arbitrarily
dropping the most active subjects would clearly potentially
introduce a large bias into our results).
3.4.1 Manual ClassiÔ¨Åcation of Fix-inducing Changes
In order to build a classier for bug-xing commits, we
randomly sampled commits and manually labeled x-inducing
commits. Some keywords indicating bug-xes were Fix,Bug,
and Resolves , along with their derivatives. We should men-1248
0.00 0.25 0.50 0.75 1.00
Mutation ScoreBug‚àífix CommitsFigure 2: Mutation score vs. bug-x commits for
covered lines.
0.250.501.002.004.008.00
0.00 0.25 0.50 0.75 1.00
Mutation ScoreBug‚àífix CommitsFigure 3: Mutation score vs. bug-x commits for
covered blocks.
Precision Recall f1.score Support
Bug-x 0.63 0.43 0.51 75.00
Other 0.74 0.86 0.80 140.00
Table 1: Naive Bayes classier details
tion that not all bug-xing commit messages include the
words bugorx; indeed, commit messages are written by the
initial contributor of a patch, and there are few guidelines as
to their contents. A similar observation was made by Bird
et al. [7], who performed an empirical study showing that
bias could be introduced due to missing linkages between
commits and bugs. Improvements were manually identied
based on the following keywords: Cleanup ,Optimize , and
Simplify or their derivatives. Commits were placed into the
Other category if they had the keywords Add orIntroduce .
The number of lines modied was also compared with the
lines added. Those commits with more lines added than
modied were considered more likely to be associated with
new features and were placed in the Other category. Any-
thing that did not t into this pattern was also marked as
Other . We manually classied a set of 1,500 commits.
3.4.2 Training the Commit ClassiÔ¨Åer
We used the set of manually classied commits as the
training data for the machine learning classiers. Two eval-
uators worked independently to classify the commits. Their
datasets had a 33% overlap, which we used to calculate the
inter-rater reliability. This gave us a Cohen's Kappa of 0.90.
In our training dataset the portion of bug-xes was 46.30%,
with 53.70% of the commits assigned to the Other category.
We trained a Naive-Bayes (NB) classier and a Support
Vector Machine (SVM) for automatically classifying the com-
mits, using the scikit [36] platform. We applied the classi-
ers to the training data with 12-fold cross-validation. Our
goal was to achieve high precision and recall, so we used the
F1-score to measure and compare the performance of the
models. The F1-score considers precision and recall by tak-
ing their harmonic mean. The NB classier outperformed
the SVM. Tian et al. [42] suggested that for keyword based
classication the F1score is usually around 0.55 which also
happened in our case. We used the classication identi-
ed by the NB classier to classify 11566 commits. Table 1
has the quality indicator characteristics of the NB classier.
While our classier is far from perfect, it is comparable to
\good" classiers for this purpose in the literature (over a
larger set of projects), and we believe it is likely that any bi-
ases do not have confounding interactions with the goals ofour project. That is, while we may only analyze about 43%
of bug-xes, it would be surprising if the missed bug-xes
relate in some systematic way to the dynamic testedness
measures of program elements, given that the classier only
sees code commits. Since our analysis only relies on rela-
tive counts of bug-xes for elements, so long as we do not
systematically undercount bug-xes for only some elements,
our results should be valid.
The bug-xes associated with each program element in
our analysis are based on the classier results in a simple
way. For each element, we count commits that aect that
element that are classied as Bug-x up to the rst commit
that is classied as Other. This is because once an element
has had a change that is not a bug-x, it is often no longer
valid to assume tests at the epoch apply to that element, or
that it even still exists with the same functionality. However,
so long as only bug-xes are applied, we assume the tests
still apply to the program element, so all bug-xes count as
missed by the tests at epoch. Note that our classier for
Other commits is highly eective.
4. ANALYSIS
We analyze the impact of testedness on program element
bug-xes using two measurements: mutation score and state-
ment coverage. For mutation score, we analyze the score of
each statement, block, method, and class, in increasing lex-
ical scope. Since statement coverage is already at the state-
ment level, we investigate the statement coverage of each
block, method, and class in increasing lexical scope.
4.1 Correlation Results
We answer this question in increasing scope from state-
ment, smallest block, method, and then class. In each scope,
we evaluate how the degree of adequacy in both mutation
score and statement coverage aects the total number of
bug-x commits.
4.1.1 Mutation Score ( )
The correlation between number of bug-xes per state-
ment and mutation score is given in Table 2.
For statements and methods, there is a statistically sig-
nicant small negative linear correlation between number of
bug-xes per statement and mutation score. A similar eect
is observed with Kendall bcorrelation, where a small but
statistically signicant negative correlation is observed for
statements, blocks, and methods but not for classes, where
the correlation was, surprisingly, a very weak, but signi-0.250.501.002.004.008.00
0.00 0.25 0.50 0.75 1.00
Mutation ScoreBug‚àífix CommitsFigure 4: Mutation score vs. bug-x commits for
covered methods.
0.250.501.002.004.008.00
0.00 0.25 0.50 0.75 1.00
Mutation ScoreBug‚àífix CommitsFigure 5: Mutation score vs. bug-x commits for
covered classes.
(a)R2
Mean Low High p
Statements -0.12 -0.13 -0.11 0.00
Blocks -0.14 -0.15 -0.12 0.00
Methods -0.16 -0.18 -0.14 0.00
Classes -0.13 -0.18 -0.08 0.00(b) Kendall b
Mean p
-0.13 0.00
-0.19 0.00
-0.14 0.00
-0.10 0.00
Table 2: Correlation between total number of bug-xes per
line and mutation score
(a)R2
Mean Low High p
Statements -0.11 -0.12 -0.10 0.00
Blocks -0.13 -0.14 -0.12 0.00
Methods -0.14 -0.16 -0.12 0.00
Classes 0.09 0.04 0.13 0.00(b) Kendall b
Mean p
-0.13 0.00
-0.21 0.00
-0.13 0.00
-0.04 0.07
Table 3: Correlation between total number of bug-xes per
line and statement coverage
cant, positive correlation.
The plot of mutation score vs. normalized bug-xes for
statements is given in Figure 2, for blocks in Figure 3, for
methods in Figure 4, and for classes in Figure 5.
4.1.2 Statement Coverage ( )
The correlation between number of bug-xes per state-
ment and statement coverage is given in Table 3.
For statements, blocks, methods, and classes, there was
a small but statistically signicant negative linear correla-
tion between coverage and bug-xing commits. A similar
small but statistically signicant negative correlation is also
observed using Kendall b.
These correlations (for mutant score and for statement
coverage) are much lower than those seen in recent stud-
ies showing good correlation between coverage metrics and
mutation scores [19,21] (these studies are measuring a dier-
ent property, but in some sense aiming for similarly strong
correlations). These correlations are not so small as to be
completely devoid of value, but they do make the use of these
measures dubious when comparing program elements or test
suites with only small testedness dierences. Unfortunately,
this is a common practice in the evaluation of software test-
ing experiments. Worse still, these results might be thought
to suggest that testedness cannot be eectively measured,
leaving the practicing tester without useful guidance.Covered Uncovered p
Statement 0.68 1.20 0.00
Block 0.42 0.83 0.00
Method 0.40 0.87 0.00
Class 0.45 0.32 0.10
Table 4: Dierence in bug-xes between covered and
uncovered program elements
4.2 Binary Testedness: Is It Covered?
However, using testedness as a continuous valuation, where
we expect slightly more tested program elements to have
fewer bug-xes, is not the only way to make use of test-
edness. Instead of trying to separate very similarly tested
elements, we could simply draw a line between tested and
not-tested program elements. For example, common sense
suggests that if testing is useful at all, then code that is
not covered should probably have more bug-xes that code
that has at least some test covering it. This rationale is
the intuition behind ideas like \getting to 80% code cover-
age," though it does not justify any particular target value.
Code that isn't executed in tests is surely less tested than
code that executes in even very poor tests (since even very
badly designed tests with a weak oracle may catch crashes,
uncaught exceptions, and innite loops, for example).
We compared the mean number of bug-xes for covered vs.
uncovered program elements using a t-test. The results are
shown in Table 4. By covered element we mean a program
element which has at least a single statement exercised by
some test case. While this is a reasonable binary distinction
up the method level, a class with only a single statement
covered may not be much more tested than a class that
does not have any statements covered. This may account
for the dierence seen for classes in Table 4. We also note
that there is insucient data for statistical signicance in
classes (most classes are covered by at least some test).
4.3 Binary Testedness: Mutation Score and
Coverage Thresholds
While measuring testedness based on mutation score or
statement coverage as a continuous value of limited value, we
can do much better than just drawing a meaningful dividing
line between covered and not-covered program elements.
We can instead evaluate whether the mean number of bug-
xes diers signicantly when the tests reach a given ade-
quacy threshold. Table 5 and Table 6 tabulate the mean
number of normalized bug-x commits per line for both(a) 0.25
0:25<0:25 p
Statements 0.60 1.20 0.00
Blocks 0.39 0.81 0.00
Methods 0.32 0.87 0.00
Classes 0.11 0.55 0.00(b) 0.5
0:5<0:5 p
0.60 1.19 0.00
0.39 0.79 0.00
0.33 0.85 0.00
0.12 0.51 0.00(c) 0.75
0:75<0:75 p
0.58 1.16 0.00
0.39 0.71 0.00
0.34 0.81 0.00
0.13 0.46 0.00(d) 1.0
1<1 p
0.58 1.14 0.00
0.39 0.67 0.00
0.41 0.75 0.00
0.20 0.40 0.00
Table 5: Mutation score thresholds
(a) 0.25
0:25<0:25 p
Statements 0.68 1.20 0.00
Blocks 0.42 0.83 0.00
Methods 0.40 0.87 0.00
Classes 0.48 0.31 0.04(b) 0.5
0:5<0:5 p
0.68 1.20 0.00
0.42 0.83 0.00
0.41 0.86 0.00
0.51 0.30 0.01(c) 0.75
0:75<0:75 p
0.68 1.20 0.00
0.42 0.82 0.00
0.42 0.84 0.00
0.59 0.28 0.00(d) 1.0
1<1 p
0.68 1.20 0.00
0.42 0.82 0.00
0.46 0.80 0.00
0.90 0.24 0.00
Table 6: Statement coverage score thresholds
above and below the thresholds =f0:25;0:5;0:75;1:0gand
=f0:25;0:5;0:75;1:0g. We nd that there is a statistically
and practically signicant dierence between the mean num-
ber of bug-xes for both measures at all thresholds selected
(though with classes perfect statement coverage strangely
becomes a predictor of more faults). Note that for individ-
ual statements, all thresholds based on statement coverage
are equivalent (coverage is always 0 or 1).
Table 7 shows mutant threshold results if we rst remove
all program entities that are not covered. This has little
impact on the ability of thresholds to predict bug-xes.
4.4 Complexity and Change
We also compare the number of mutants, normalized by
the size of the program element (dividing by the number
of lines), to the number of post-epoch bug-xes for that
element.
Statements: Comparing the number of bug-xes to the
number of mutants per statement, we nd that the 95%
condence interval is given by f 0:004697, 0.013204gatp>
0:01.
Methods: Comparing the number of bug-xes to the num-
ber of mutants per method, we nd that the 95% condence
interval is given by f 0:087117, 0:048715gatp<0:01.
Classes: Comparing the number of bug-xes to the num-
ber of mutants per class, we nd that the 95% condence
interval is given by f 0:096285, 0:000863gatp>0:01.
Summary: Most of the results are statistically signicant.
We also observe that there is a weak correlation between
the number of mutants (normalized) and the number of
bug-xes. More \complex" code as measured by number of
mutations has slightly fewer bug-xes, but the correlation
is even weaker than between testedness measures and bug-
xes. However, the dierence in correlation is not very large,
so another way to interpret this is that as a continuous mea-
sure, simple number of mutants, normalized, is only slightly
worse as a predictor of bug-xes than\testedness." However,
unlike testedness measures, the number of mutants does not
provide a useful binary predictor for bug-xes. Binary splits
based on a threshold using the mean number of normalized
mutants (2.79) do not produce signicantly dierent popu-
lations. Setting a threshold of 5 or more normalized mutants
does produce signicant dierences ( p-value<0.0001), but
the means are very similar, e.g., 1.1 bug-xes for less com-plex statements vs. 0.95 bug-xes for statements with more
mutants.
4.5 Complexity and Testedness
Statements: Comparing the normalized number of mu-
tants to the mutation score per statement, we nd that the
95% condence interval is given by f0.008016, 0.025912 gat
p<0:01.
Methods: Comparing the normalized number of mutants
to the mutation score per method, we nd that the 95%
condence interval is given by f0.005755, 0.044311 gatp>
0:01.
Classes: Comparing the normalized number of mutants to
themutation score per class, we nd that the 95% condence
interval is given by f 0:049426, 0.046223gatp>0:01.
Summary: We found that at the statement level (only)
there is a statistically signicant but very weak correlation
between the number of mutants (normalized) and the mu-
tation score. More complex statements are (very slightly)
more well-tested.
5. DISCUSSION
This paper presents a novel approach to determining if
what we call testedness measures actually help predict how
many defects that escaped testing will be found (and xed)
in parts of a program. Our empirical results have some
potentially important consequences for testing research and
practice.
5.1 The Danger of Relying on Small Tested-
ness Differences
First, there is only a weak correlation between either state-
ment coverage or mutation score and future bug-xes. This
indirectly suggests that research eorts using coverage or
mutants to evaluate test suite selection, generation, or re-
duction algorithms may draw unwarranted conclusions from
small, signicant dierences in these measures. In particu-
lar, it may suggest that using mutation to evaluate testing
experiments can potentially fail to reect the ability of sys-
tems to detect the types of faults that are detected by prac-
titioners and worth correcting in real-life. Given that the
literature supporting the value of code coverage as a predic-
tor of fault detection mostly relies on the ability of muta-(a) 0.25
0:25<0:25 p
Statements 0.60 1.16 0.00
Blocks 0.39 0.72 0.00
Methods 0.32 0.90 0.00
Classes 0.11 1.66 0.00(b) 0.5
0:5<0:5 p
0.60 1.11 0.00
0.39 0.64 0.00
0.33 0.71 0.00
0.12 1.13 0.00(c) 0.75
0:75<0:75 p
0.58 0.95 0.00
0.39 0.50 0.00
0.34 0.53 0.00
0.13 0.75 0.00(d) 1.0
1<1 p
0.58 0.89 0.00
0.39 0.47 0.00
0.41 0.39 0.68
0.20 0.50 0.00
Table 7: Mutation score thresholds with uncovered program elements ltered out
tion testing to reect real fault detection, and that mutation
testing's eectiveness is validated by only a small number of
studies, none of which present overwhelming evidence over a
large number of programs, we strongly suggest that testing
experiments, whenever possible, should rely on the use of
some real faults in addition to coverage or mutation-score
based evaluations. In some contexts, where detecting all
possible faults is the goal (e.g., safety critical systems) and
the oracle for correctness is known to be extremely good,
mutation-based analyses may be justied, but even in those
cases data based on real faults would be ideal.
5.2 Practical Application of Thresholds
On the other hand, our results show that numerous simple
percentage thresholds for statement coverage and mutation
score can, in a statistically signicant way, predict the num-
ber of bug-xes (with mean dierences between populations
of about 2x). This suggests a simple method for prioritizing
testing targets in a program. The entities with the highest
bug-x counts were, unsurprisingly, those not even covered
by any tests. As a rst priority, covering uncovered program
elements is likely to be the most rewarding way to improve
testedness, since these elements can be expected to have the
most potential undetected bugs that will be revealed in the
near future. Surviving mutants of entities with low mutation
scores can then be used to guide further testing. One obvi-
ous question is, which threshold should be used, since many
thresholds seem eective? Our data shows that it really
does not matter much | the signicance and even average
bug-xes are not radically dierent for dierent thresholds.
The simplest answer is to start with low thresholds, keep
improving testing until there are no remaining interesting
elements below the current threshold, then move on to a
higher threshold. Setting a particular threshold for project-
level testing is not supported by our data, however, as there
is no clearly \best" dividing line, only a number of ways
to dene \less tested" and \more tested" elements, most of
which equate to more bug-xes for less tested elements.
5.3 Complexity, Bug-Fixes, and Testedness
There does not seem to be any very strong or interest-
ing relationship between complexity (as measured by num-
ber of mutants) and bug-xes, or between complexity and
testedness. More complex code is (very slightly) less xed,
perhaps because it is (very slightly) more tested. The main
take-away from the complexity analysis is that the number
of mutants is almost as good a predictor of lack of bug-xes
as testedness, if used as a simple correlation, but it does not
support useful binary distinctions in likely bug-xes.
5.4 Testing is Likely Effective
One nal point to note is that our data provides fairly
strong support for the idea that testing is eective in forcingquality improvements in code . Our measures of testedness
are, essentially, based purely on the dynamic properties of a
test suite, not on static properties of program elements (the
number of mutants for an entity depends on static proper-
ties, but all statements with any mutants can achieve or fail
to achieve a score of any particular threshold). This means
that, without using the static properties of code, the degree
to which code is exercised in a test suite can often be used
to predict which of two entities will turn out to require more
bug-xes. As far as we can determine, there are only a few
potential causes for this ability to use the dynamics of a test
suite to predict bug-xes:
1. Some unknown property not related to code quality
results in both a tendency to write tests that cover
code and in fewer bug-xes for that code.
2. A known property results in both a tendency to write
tests that cover code and in fewer bug-xes for that
code: namely, good developers write tests for their
already more correct code. Testing itself is more a
sign of good code than a cause of good code.
3. Tests covering code often detect bugs, and developers
x the bugs, so the code has fewer bugs to x.
The rst possibility is, in our opinion, unlikely | it is
dicult to imagine such an unknown factor. Some obvious
candidate factors do not really bear up on examination. For
example, perhaps code with many bug-xes is new code, and
so has not yet had tests written for it. If the act of writing
tests for the new code makes it less buggy, however, then
testing is in fact eective. Moreover, the predictive power
of mutation score being over a threshold is present even if
we restrict our domain to entities that have at least one
covering test. New code might be expected to be completely
untested, removing most truly new (no tests) code from this
population.
The second possibility is more plausible, and may well be
true to some extent. The third possibility seems most plau-
sible, and we believe is likely to be the main cause of the
observed eects. However, even if we assume that the sec-
ond explanation is the primary cause for the relationships we
observed, notice the peculiar consequences of this claim: de-
velopers who believe testing is worthwhile, and devote more
time to it, are \wrong" in that testing itself is useless, but
on the whole produce statistically better code than those
who do not value testing. This may not be an appealing
argument to those dubious about testing's value.
6. THREATS TO VALIDITY
While we have taken care to ensure that our results are
unbiased, and have tried to eliminate the eects of randomnoise, we cannot guarantee that our results are valid. In
particular, our results are subject to the following threats.
Threats Due to Sampling Bias: To ensure representa-
tiveness of our samples, we opted to use search results from
the Github repository of Java projects that use the Maven
build system. We picked allprojects that we could retrieve
given the Github API, and selected from these only based
on necessary constraints (e.g., the project must build, and
tests at epoch must pass). However, our sample of pro-
grams could be biased by skew in the projects returned by
Github. Github 's selection mechanisms favoring projects
based on some unknown criteria may be another source of
error. We also handpicked some projects from Apache, such
as commons-lang. As our samples only come from Github
and Apache, this may be a source of bias, and our ndings
may be limited to open source programs. However, we be-
lieve that the large number of projects more than adequately
addresses this concern.
Bias Due to Tool Used: For our study, we relied on PIT.
We have done our best to extend PIT to provide a reasonably
sucient set of mutation operators, ensuring also that the
mutation operators were non-redundant (and have checked
for redundancy in past work using PIT).
Secondly, we used the Gumtree algorithm discussed earlier
for tracking program elements across commits. However, the
algorithm used is unable to track program elements across
renames or movement to another folder. Further, refactoring
that involves modication of scope, such as moving the code
out of the current compilation unit also causes the algorithm
to lose track of the program element after refactoring.
Bias Due to Mutant Distribution: There is still a pos-
sibility that the kind of mutants produced may be skewed,
which may impact our analysis.
Bias Due to Equivalent Mutants: In this study we did
not apply a systematic method for the detection of equiv-
alent mutants and also did not remove equivalent mutants.
This might have impacted the mutation score of some projects
where a large portion of the mutants were equivalent and
were not killed.
Bias Due to Commit Classication: Our determination
of commits as bug-xes or not and of commits that \end the
history" of a program element both depend on a learned
classier. While our results do not require those results to
be anywhere near perfect, it may be that some unknown bias
in the failures unduly inuences our results, or gives rise to
the weakness of observed correlations.
Bias Due to Lack of High Coverage: Some researchers
have found that a strong relationship between coverage and
eectiveness does not show up until very high coverage levels
are achieved [15,17,24]. Since the coverage for most projects
rarely reached very high values, it is possible that we missed
the existence of such a dependent strong relationship.
7. CONCLUSION
This paper uses a novel method to evaluate the eective-
ness of test suite quality measurements, which, we suggest
essentially aim to capture the \testedness" of a program or
program elements. Much of previous research attempting to
evaluate such measures operates by a procedure that, at a
suitably high level of abstraction, can be described as rst
collecting a large set of tuples of the form (testedness mea-
sure for suite, # faults found by suite) , then apply-
ing some kind of statistical analysis. Details vary, in thatsuites may all be for one SUT, or for multiple SUTs (though
seldom for more than 5-10 SUTs), and in most cases \ac-
tual faults" are either hand-seeded or \faults" produced by
mutation testing (which is assumed to measure real fault
detection on a largely recently established and still limited
empirical basis [27]). These studies have produced a variety
of results, sometimes almost contradictory [22]. Is coverage
useful? Is mutation score (more) useful?
We propose a dierent approach. Measuring fault detec-
tion for a suite can be extremely labor-intensive; worse, de-
pending on the denition of faults, we may give too much
credit for detecting faults that are of little interest to most
developers. Instead, our evaluation chooses a point in time,
collects testedness measures (statement coverage and muta-
tion score) for a passing test suite from that date, and then
examines whether these measures predict actual future bug-
xes for program elements. If \well tested" elements of a
program require no less eort to correct, then either we are
not measuring testedness eectively, or testing itself is not
eective.
We assume that testing is eective. Under this assump-
tion, we show that there isthe expected negative correlation
between testedness and number of future bug-xes. How-
ever, this correlation is so weak that it makes using it to
compare testedness values in the continuous fashion, where
slightly more tested code is assumed to be slightly better, or
slightly higher scoring test suites are assumed to be better
than slightly lower scoring test suite, a dubious enterprise.
This suggests that the evaluation method in many software
testing publications may be of questionable value. On the
other hand, when we use testedness measures to split pro-
gram elements into simple \more tested" and \less tested"
groups, the population dierences are typically signicant
and the mean bug-xes are suciently dierent (usually
about a factor of 2x) to provide practical guidance in testing.
So, is (statement) coverage useful? Is mutation score rel-
evant? Is mutation score (more) useful? The answers, we
believe, may be that it depends on what you expect to achieve
using these methods. Testing is an inherently noisy and id-
iosyncratic process, and whether a suite detects a fault de-
pends on a large number of complex variables. It would,
given this complexity of process, be very surprising if any
simple dynamic measure computable without human eort
for any test suite produced strong correlations like those of-
ten shown between code coverage and mutation score (0.6-
0.9). The correlations between these measures are often high
because both result from regular, even-handed, automated
analysis of the dynamics of a test suite. Actual faults are ap-
parently (unsurprisingly) produced and detected by a much
more complex and irregular process. However, when used to
draw the line between less tested and more tested program
elements, testedness measures can provide a simple auto-
mated way to prioritize testing eort, and recognize when all
the elements of an SUT have passed beyond a high threshold
of testedness, and are thus likely to have fewer future faults.
In short, while we cannot (at present) measure testedness
as precisely as we (software engineering researchers) would
like, we can measure testedness in such a way as to provide
some practical assistance to the humble working tester.
Our data is available for inspection and further analysis
at http://eecs.osuosl.org/rahul/fse2016/.8. REFERENCES
[1] I. Ahmed, U. A. Mannan, R. Gopinath, and C. Jensen.
An empirical study of design degradation: How
software projects get worse over time. In ACM/IEEE
International Symposium on Empirical Software
Engineering and Measurement , pages 31{40, 2015.
[2] J. H. Andrews, L. C. Briand, and Y. Labiche. Is
mutation an appropriate tool for testing experiments?
InInternational Conference on Software Engineering ,
pages 402{411. IEEE, 2005.
[3] J. H. Andrews, L. C. Briand, Y. Labiche, and A. S.
Namin. Using mutation analysis for assessing and
comparing testing coverage criteria. IEEE
Transactions on Software Engineering , 32(8), 2006.
[4] Apache Software Foundation. Apache commons.
http://commons.apache.org/.
[5] Apache Software Foundation. Apache maven project.
http://maven.apache.org.
[6] A. Arcuri and L. C. Briand. A hitchhiker's guide to
statistical tests for assessing randomized algorithms in
software engineering. Softw. Test., Verif. Reliab. ,
24(3):219{250, 2014.
[7] C. Bird, A. Bachmann, E. Aune, J. Duy,
A. Bernstein, V. Filkov, and P. Devanbu. Fair and
balanced?: bias in bug-x datasets. In Proceedings of
the the 7th joint meeting of the European software
engineering conference and the ACM SIGSOFT
symposium on The foundations of software
engineering , pages 121{130. ACM, 2009.
[8] T. A. Budd. Mutation Analysis of Program Test Data .
PhD thesis, Yale University, New Haven, CT, USA,
1980.
[9] X. Cai and M. R. Lyu. The eect of code coverage on
fault detection under dierent testing proles. In ACM
SIGSOFT Software Engineering Notes , volume 30,
pages 1{7. ACM, 2005.
[10] H. Coles. Pit mutation testing. http://pitest.org/.
[11] M. Daran and P. Th evenod-Fosse. Software error
analysis: A real case study involving real faults and
mutations. In ACM SIGSOFT International
Symposium on Software Testing and Analysis , pages
158{171. ACM, 1996.
[12] M. Delahaye and L. Bousquet. Selecting a software
engineering tool: lessons learnt from mutation
analysis. Software: Practice and Experience , 2015.
[13] R. A. DeMillo and A. P. Mathur. On the use of
software artifacts to evaluate the eectiveness of
mutation analysis for detecting errors in production
software. Technical Report SERC-TR92-P, Software
Engineering Research Center, Purdue University, West
Lafayette, IN"1991.
[14] J.-R. Falleri, F. Morandat, X. Blanc, M. Martinez,
and M. Monperrus. Fine-grained and accurate source
code dierencing. In Proceedings of the 29th
ACM/IEEE International Conference on Automated
Software Engineering , ASE '14, pages 313{324, New
York, NY, USA, 2014. ACM.
[15] P. G. Frankl and O. Iakounenko. Further empirical
studies of test eectiveness. In ACM SIGSOFT
Software Engineering Notes , volume 23, pages
153{162. ACM, 1998.
[16] P. G. Frankl and S. N. Weiss. An experimentalcomparison of the eectiveness of branch testing and
data ow testing. IEEE Transactions on Software
Engineering , 19:774{787, 1993.
[17] P. G. Frankl, S. N. Weiss, and C. Hu. All-uses vs
mutation testing: an experimental comparison of
eectiveness. Journal of Systems and Software ,
38(3):235{253, 1997.
[18] GitHub Inc. Software repository.
http://www.github.com.
[19] M. Gligoric, A. Groce, C. Zhang, R. Sharma,
A. Alipour, and D. Marinov. Guidelines for
coverage-based comparisons of non-adequate test
suites. ACM Transactions on Software Engineering
and Methodology , 24(4):4{37, 2014.
[20] M. Gligoric, A. Groce, C. Zhang, R. Sharma, M. A.
Alipour, and D. Marinov. Comparing non-adequate
test suites using coverage criteria. In ACM SIGSOFT
International Symposium on Software Testing and
Analysis . ACM, 2013.
[21] R. Gopinath, C. Jensen, and A. Groce. Code coverage
for suite evaluation by developers. In International
Conference on Software Engineering . IEEE, 2014.
[22] A. Groce, M. A. Alipour, and R. Gopinath. Coverage
and its discontents. In Proceedings of the 2014 ACM
International Symposium on New Ideas, New
Paradigms, and Reections on Programming &
Software , Onward! 2014, pages 255{268, New York,
NY, USA, 2014. ACM.
[23] A. Gupta and P. Jalote. An approach for
experimentally evaluating eectiveness and eciency
of coverage criteria for software testing. International
Journal on Software Tools for Technology Transfer ,
10(2):145{160, 2008.
[24] M. Hutchins, H. Foster, T. Goradia, and T. Ostrand.
Experiments of the eectiveness of dataow-and
controlow-based test adequacy criteria. In
International Conference on Software Engineering ,
pages 191{200. IEEE Computer Society Press, 1994.
[25] L. Inozemtseva and R. Holmes. Coverage Is Not
Strongly Correlated With Test Suite Eectiveness. In
International Conference on Software Engineering ,
2014.
[26] L. M. M. Inozemtseva. Predicting test suite
eectiveness for java programs. Master's thesis,
University of Waterloo, 2012.
[27] R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst,
R. Holmes, and G. Fraser. Are mutants a valid
substitute for real faults in software testing? In ACM
SIGSOFT Symposium on The Foundations of
Software Engineering , pages 654{665, Hong Kong,
China, 2014. ACM.
[28] S. Kakarla. An analysis of parameters inuencing test
suite eectiveness. Master's thesis, Texas Tech
University, 2010.
[29] N. Li, U. Praphamontripong, and J. Outt. An
experimental comparison of four unit test criteria:
Mutation, edge-pair, all-uses and prime path coverage.
InInternational Conference on Software Testing,
Verication and Validation Workshops , pages 220{229.
IEEE, 2009.
[30] A. P. Mathur and W. E. Wong. An empirical
comparison of data ow and mutation-based testadequacy criteria. Software Testing, Verication and
Reliability , 4(1):9{31, 1994.
[31] A. Mockus, N. Nagappan, and T. T. Dinh-Trong. Test
coverage and post-verication defects: A multiple case
study. In Empirical Software Engineering and
Measurement, 2009. ESEM 2009. 3rd International
Symposium on , pages 291{301. IEEE, 2009.
[32] G. J. Myers. The art of software testing. A
Willy-Interscience Pub , 1979.
[33] A. S. Namin and J. H. Andrews. The inuence of size
and coverage on test suite eectiveness. In ACM
SIGSOFT International Symposium on Software
Testing and Analysis , pages 57{68. ACM, 2009.
[34] A. S. Namin and S. Kakarla. The use of mutation in
testing experiments and its sensitivity to external
threats. In ACM SIGSOFT International Symposium
on Software Testing and Analysis , pages 342{352, New
York, NY, USA, 2011. ACM.
[35] A. J. Outt and J. M. Voas. Subsumption of condition
coverage techniques by mutation testing. Technical
report, Technical Report ISSE-TR-96-01, Information
and Software Systems Engineering, George Mason
University, 1996.
[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, et al. Scikit-learn: Machine
learning in python. The Journal of Machine Learning
Research , 12:2825{2830, 2011.
[37] RTCA Special Committee 167. Software
considerations in airborne systems and equipment
certication. Technical Report DO-1789B, RTCA,
Inc., 1992.
[38] S. Shamshiri, R. Just, J. M. Rojas, G. Fraser,
P. McMinn, and A. Arcuri. Do automatically
generated unit tests nd real faults? an empirical
study of eectiveness and challenges. In IEEE/ACM
International Conference on Automated Software
Engineering , pages 201{211, 2015.
[39] A. Shi, A. Gyori, M. Gligoric, A. Zaytsev, and
D. Marinov. Balancing trade-os in test-suite
reduction. In Proceedings of the 22Nd ACM SIGSOFT
International Symposium on Foundations of Software
Engineering , FSE 2014, pages 246{256, New York,
NY, USA, 2014. ACM.
[40] SIR: Software-artifact Infrastructure Repository. Sir
usage information , accessed at mar 8, 2016.
http://sir.unl.edu/portal/usage.php.
[41] D. Tengeri, L. Vidacs, A. Beszedes, J. Jasz, G. Balogh,
B. Vancsics, and T. Gyim othy. Relating code coverage,
mutation score and test suite reducibility to defect
density,accepted paper. In mutationworkshop , 2016.
[42] Y. Tian, J. Lawall, and D. Lo. Identifying linux bug
xing patches. In Software Engineering (ICSE), 2012
34th International Conference on , pages 386{396.
IEEE, 2012.
[43] Y. Wei, B. Meyer, and M. Oriol. Empirical Software
Engineering and Verication , chapter Is branch
coverage a good measure of testing eectiveness?,
pages 194{212. Springer-Verlag, Berlin, Heidelberg,
2012.