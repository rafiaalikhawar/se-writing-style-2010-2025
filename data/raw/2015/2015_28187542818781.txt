Lightweight Adaptive Filtering for EfÔ¨Åcient
Learning and Updating of Probabilistic Models
Antonio Filieri
University of Stuttgart
Stuttgart, GermanyLars Grunske
University of Stuttgart
Stuttgart, GermanyAlberto Leva
Politecnico di Milano
Milan, Italy
Abstract ‚ÄîAdaptive software systems are designed to cope with
unpredictable and evolving usage behaviors and environmental
conditions. For these systems reasoning mechanisms are needed
to drive evolution, which are usually based on models capturing
relevant aspects of the running software. The continuous update
of these models in evolving environments requires efÔ¨Åcient learn-
ing procedures, having low overhead and being robust to changes.
Most of the available approaches achieve one of these goals at the
price of the other. In this paper we propose a lightweight adaptive
Ô¨Ålter to accurately learn time-varying transition probabilities
of discrete time Markov models, which provides robustness to
noise and fast adaptation to changes with a very low overhead.
A formal stability, unbiasedness and consistency assessment of
the learning approach is provided, as well as an experimental
comparison with state-of-the-art alternatives.
I. I NTRODUCTION
Non-functional properties such as reliability, performance,
or energy consumption are a central factor in the design of
software systems, moving from the niche of critical systems to
everyday software. Probabilistic quantitative properties, which
are able to characterize the uncertainty and unpredictability of
external phenomena affecting software behavior from the in-
teraction with the users to the contention on accessing physical
resources. For this reason, signiÔ¨Åcant research effort has been
conducted in recent years about speciÔ¨Åcation and veriÔ¨Åcation
of probabilistic quantitative properties [1‚Äì7]. These veriÔ¨Å-
cation approaches commonly build upon convenient formal
models able to capture the probabilistic nature of the described
phenomena (e.g., Markov models or queuing networks).
However, most of these models are constructed at design
time based on initial assumptions about the software and its ex-
ecution environment. These assumptions might be invalidated
by unforeseen changes the software may undergo during its
execution [8, 9]. To handle this issue, probabilistic models
need to be continually updated during runtime [10‚Äì13] to
provide a current view on the running systems, supporting
also the runtime veriÔ¨Åcation of the desired properties.
In general, designing time efÔ¨Åcient and accurate algorithms
to keep a probabilistic model continuously updated during
runtime is an open problem, deeply investigated by the Soft-
ware Engineering community [12‚Äì15]. An early example is
the Kami approach [12] that uses a Bayesian estimator to
learn transition probabilities of Discrete-Time Markov Chains
(DTMCs). However, the longer a Kami estimator runs the
higher the effect of the historical data is on the estimation.Thus KAMI is producing inaccurate results once the probabili-
ties change. The authors of the initial Kami approach have also
noticed this and have extended their approach with a change
point detection algorithm [16], which resets the estimation
once the observed transition probabilities have signiÔ¨Åcantly
changed. Adding a change point detection method to Kami
signiÔ¨Åcantly increases the robustness towards change, however
it comes at the cost of an increased runtime overhead. The
Cove approach [17] enhances Kami‚Äôs Bayesian estimator by
adding an aging mechanism to forget old information. Cove
results are thus more robust to changes, however the intrinsic
noise Ô¨Åltering capability of the original Bayesian estimator
is weakened by the aging mechanism, leading to more noisy
estimates. Cove has been extended with a procedure to au-
tomatically set an optimal aging factor [14]. In the area of
performance tracking, Kalman Ô¨Ålters are conÔ¨Ågured and used
to estimate performance measures and to keep them updated at
runtime [15]. Kalman Ô¨Ålters are well known for their ability to
reduce input noise and to provide smooth estimates. However,
this comes at the price of slower responses to abrupt changes.
A good trade-off between these two aspects usually requires
a non-trivial tuning of the algorithm‚Äôs parameters.
Trading off noise-rejection, prompt reaction to changes,
and computational overhead remains an open problem. In
this paper we propose a novel lightweight adaptive Ô¨Ålter to
learn and continuously update the transition probabilities of a
DTMC that:
is speciÔ¨Åcally designed to improve the trade-off between
smooth estimation and prompt reaction to changes
is equipped with an online auto-tuning procedure to
robustly discriminate between actual changes and outliers
isprovably stable, unbiased, and consistent, with a for-
mal quantiÔ¨Åcation of its convergence time and its noise
Ô¨Åltering strength
requires a negligible runtime computational overhead.
We implemented our approach in Python [18] and formally
proved (cp. Section IV) that the algorithm satisÔ¨Åes the desired
properties. We further performed a preliminary experimental
evaluation (cp. Section VI) with common input data patterns
to highlight the strengths and weaknesses. Additionally, we
applied the algorithm to learn the operational proÔ¨Åle of a large
case study [19] to underpin these results (cp. Section VII).
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.41200
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.41200
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.41200
2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
978-1-4799-1934-5/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICSE.2015.41200
ICSE 2015, Florence, Italy
II. B ACKGROUND
This section brieÔ¨Çy recalls essential background concepts
for our approach. In Section II-A a formal deÔ¨Ånition of
Discrete-Time Markov Chains (DTMCs) is provided. In Sec-
tion II-B we will introduce basic deÔ¨Ånitions and assumptions
about statistical inference for DTMCs.
A. Discrete Time Markov Models
A Discrete-Time Markov Chain is a state-transition sys-
tem where the choices among successor states are governed
by a probability distribution. Formally, a DTMC is a tuple
(S;s 0;P;L;AP )[20], where Sis a (Ô¨Ånite) set of states,
s02Sis the initial state, P:SS![0;1]is a stochastic
matrix,APis a set of atomic propositions, and L:S!2AP
is a labeling function that associates to each state the set of
atomic propositions that are true in that state. An element pij
of the Matrix Prepresents the transition probability from state
sito statesj, i.e. the probability of going from state sito state
sjin exactly one step.
The probability of moving from sitosjin exactly two steps
can be computed asP
sx2Spixpxj, that is the sum of the
probabilities of all the paths originating in si, ending insj, and
having exactly one intermediate state. The previous sum is, by
deÔ¨Ånition, the entry (i;j)of the power matrix P2. Similarly,
the probability of reaching sjfromsiin exactlyksteps is
the entry (i;j)of matrixPk. As a natural generalization, the
matrixP0Irepresents the probability of moving from state
sito statesjin zero steps, i.e., 1ifsi=sj,0otherwise.
SincePis a stochastic matrix, the sum of the elements
for each of its rows has to be 1. Formally, each row iofP
identiÔ¨Åes a categorical distribution [21]. Furthermore, thanks
to the Markov property [22], these categorical distributions are
pairwise probabilistically independent, since the choice of the
next state only depends on the current one. This property will
be exploited in the next section to support the deÔ¨Ånition of a
localized learning approach of the transition matrix P.
B. Statistical Learning for DTMCs
The identiÔ¨Åcation of DTMC models from the observation of
a running system is a well-known statistical problem [23, 24],
with relevant applications in many disciplines [22], including
software engineering [25‚Äì32].
In this paper we focus on learning the transition probability
matrixPof a DTMC, assuming its structure does not change,
i.e., only transition probabilities may be unknown or subject
to change [29, 33, 34]. Thanks to the Markov property, this
problem can be reduced to the independent learning of n
independent categorical distribution, where nis the number
of states composing the DTMC. This simpliÔ¨Åes both the
monitoring and the learning tasks.
Several approaches have been proposed in literature, in-
cluding maximum likelihood estimators [23] and Bayesian
estimators [13, 35]. The latter have recently gained more
relevance for online learning thanks to the (usually) faster
convergence and the ability to embed expert knowledge inthe form of an assumed prior next state distribution [12‚Äì
14, 17]. Despite their ability to estimate the actual transition
probabilities, even in presence of noisy observations, for time-
invariant processes, most statistical approaches fail to promptly
react to changes in the transition probability. This leads to
slow convergence time after a change and, consequently, poor
accuracy and reliability of the estimates.
III. L EARNING THROUGH FILTERS
In this section we will introduce our online learning ap-
proach for DTMCs based on Ô¨Åltering. The input to our systems
is a sequence of measures representing the average transition
frequency from a state sito each state sjover a period of
observation. Calling kthek-th period of observation (also
referred to as time step), the average transition frequencies
pm
ij(k)at time step kare deÔ¨Åned as nij(k)=P
xnix(k), where
nij(k)is the number of transitions from sitosjoccurred at
time stepk. Since those counts are obtained by monitoring the
system for a limited time, we assume the observed frequencies
to include an additive zero-mean noise component, accounting
for both the uncertainty of the sampling procedure and possible
issues with the monitoring infrastructure (e.g., communication
delays). The values of the noise for each time step are assumed
to be independent among one another and, approximately,
normally distributed, with unknown variance [36, 37].
For the considerations stated in Section II-B, we will
instantiate a Ô¨Ålter for each state, aiming at learning its next
state distribution. A similar approach is followed by most
state-of-the-art approaches for learning DTMCs [12‚Äì14].
To describe our approach, let us Ô¨Årst focus in Section III-A
on the estimation of a scalar parameter not subject to any
constraint. In Section III-B, we will extend the approach to
cope with multiple dependent variables, whose sum has to be
equal to a given value. This extension is needed to handle the
structural dependencies among transitions of a DTMC origi-
nating from the same state. Finally, in Section III-C, we will
introduce an online auto-tuning procedure to automatically
adapt the change point detection mechanism of the Ô¨Ålter to
cope with changing and unpredictable operation scenarios.
A. Learning a Scalar Measure
The goal of our learning procedure is to estimate an un-
known, time-varying probability vector p(k)from the (noisy)
measurements pm(k). The output of the Ô¨Ålter will be an
estimatebp(k)ofp(k). The simplest viable Ô¨Ålter for our
purpose is a unity-gain, Ô¨Årst-order discrete-time Ô¨Ålter [38],
whose dynamics is described in Equation (1):
bp(k) =abp(k 1) + (1 a)pm(k 1); 0<a< 1(1)
For this Ô¨Ålter high values of a(i.e., close to 1) provide good
noise Ô¨Åltering and smoothing, which is desirable to estimate
a stationary probability from noisy observation. However, the
tracking of abrupt (e.g., stepwise) variations of p(k)would
be very slow. On the other hand, small values of a(i.e.,
close to 0) would promptly follow abrupt variations of p(k)
but at the price of poor noise Ô¨Åltering. An example of such
behavior is shown in Figure 1. Ideally, we would like to have
201
201
201
201
ICSE 2015, Florence, Italy0.10.20.30.40.5
0 10000(a)a= 0:98
0.10.20.30.40.5
0 10000 (b)a= 0:02
Fig. 1. Noise Ô¨Åltering versus fast change tracking for the Ô¨Ålter of Equation (1).
aclose to 1when thep(k)is stationary, in order to obtain a
smooth estimation, while aclose to 0whenever a change in
p(k)occurs. Our goal in the remainder of this section is to
introduce our strategy for the dynamic adaptation of a, based
on the the behavior of p(k), as we can observe it through the
measurement pm(k).
The strategy we propose is to control ain the range between
two asymptotic bounds aloandahi. For a stable and non-
oscillatory behavior of the estimation process, we require
0<alo<ahi<1(we will come back to stability later). We
want our adaptation mechanism to drive atowardahiwhen the
process is stationary, and toward aloin presence of a change.
To distinguish these two situations, we propose to consider the
observed probability distribution of the input measurements
pm(k). Indeed, being produced by monitoring the process for
a limited amount of time (i.e., a time step), these measurements
usually show a certain degree of dispersion around the actual
valuep(k). Let us assume for now we can quantify such
dispersion, and use it to deÔ¨Åne a threshold ethr0such
that an input measure distant more than ethrfrom the current
estimatebp(k)can be ‚Äúlikely‚Äù assumed to be the bad smell of
a change point1.
For smoothness reasons, let us deÔ¨Åne the value of a(k)
(the introduction of the adaptation mechanism made it time-
dependent) as:
a(k) =a0+ afa(e(k) ethr) (2)
wherea0= 0:5(ahi+alo),a> 0tunes the adaptation speed
(the larger the faster; default 1), and e(k) =jpm(k) bp(k 
1)jandfa()<7! ( 0:5; 0:5) is a continuous, differentiable,
strictly monotonically decreasing function such that
lim
x! 1fa(x) = 0:5; f a(0) = 0;lim
x!1fa(x) = 0:5: (3)
To ensure that alo<a(k)<ahi8emag(k)we choose:
fa(x) = arctan(x)
(4)
whereis a design parameter determining the gradient of
fa()around the origin, and, in turn, the ‚Äúspeed‚Äù of transition
between the two asymptotic values 0:5. Notice that the
deÔ¨Ånition of fa()in terms of the arctan ()function satisÔ¨Åes
all the requirements stated above. Furthermore, the selection
ofarctan ()is typical [39], when arbitrarily steep transitions
between two values are required to be obtained through a con-
tinuous and continuously differentiable function. Alternative
deÔ¨Ånitions of fa()are possible, but their analysis is beyond
1This intuition is used in several statistical tests about the mean of a
population and for the identiÔ¨Åcation of outliers [21, 35].the scope of this paper. Higher-order Ô¨Ålters can also provide
for a Ô¨Åner speciÔ¨Åcation of the transition function, though with
an increase computational complexity and, in general, less
provable stability result. Our solution aims at achieving the
simplest strategy suitable for solving our problem, and with
the lowest possible computational overhead.
Combining Equations (1) and (2), under the assumption
that we have properly quantiÔ¨Åed ethr, results in the nonlinear
discrete-time dynamic equations of our learning Ô¨Ålter:8
<
:e(k) =jpm(k) bp(k 1)j
a(k) =a0+ afa(e(k) ethr)
bp(k) =a(k)bp(k 1) + (1 a(k))pm(k 1)(5)
where in a mathematical sense pmis the input, and bpboth the
state and the output of the dynamic system [37].
The Ô¨Ålter in (5) is the core of our learning approach. In
the next section we will extend it to estimate categorical
distributions instead of scalar values, while in Section III-C
we will formally describe the online adaptation mechanism
that allows for automatically adjusting the value of ethr, and,
consequently, of a(k).
B. Learning Categorical Distributions
The Ô¨Ålter deÔ¨Åned in the previous section can be used to
estimate each single probability pijindividually. However, the
obtained estimates for each row of Pwould most likely not
constitute correct categorical distributions (sum is not 1).
In order to ensure the estimation of correct categorical dis-
tribution of each state si, we will Ô¨Årst estimate each probability
pijindividually and then applying a convenient ‚Äúcorrection‚Äù
procedure. This procedure minimizes the Euclidean norm of
the distance between the vector bpof the uncorrected estimates
and the vector of the corrected one, subject to a unity sum
constraint for the latter (and only positive probabilities by con-
struction). Formally, let bpandcpcbebp(k) = [bp1(k):::bpn(k)]0
andcpc(k) = [bpc1(k):::bpcn(k)]0Our correction procedure
requires to solve the following optimization problem:(
min
bpc(k)(bpc(k) bp(k))0(bpc(k) bp(k))
subject toPn
i=1bpci(k) = 1(6)
Using the Lagrange multipliers method to solve this optimiza-
tion problem [40], the Lagrangian of the problem is
L(k) =nX
i=1 bpci(k) bpi(k)2+ nX
i=1bpci(k) 1!
(7)
and solving the corresponding Karush, Kuhn, and Tucker
(KKT) equations [41] leads to the (afÔ¨Åne) correction formula
cpc(k) =Fcbp(k) +Hc (8)
whereFc=In 1nnn; Hc=1n1
n; (9)
and the symbol 1pc denoting a pcmatrix with unity
elements, and Inbeing the identity matrix of order n.
In the remainder of this paper, we will always refer to the
corrected estimator for the transition probabilities of a DTMC
(i.e.bpij,bpcij), unless otherwise speciÔ¨Åed.
202
202
202
202
ICSE 2015, Florence, ItalyC. Online Filter Adaptation
The core element of the online adaptation mechanism is
the dynamic correction of the parameter ethr. This parameter
has to capture the dispersion of an input measurement pm(k)
around the actual value it is measuring p(k).
An effective and sequentially computable index of the
dispersion of a probability distribution is its variance. An
efÔ¨Åcient and numerically stable algorithm for the sequential
(i.e., sample by sample) estimation of the variance has been
proposed by Knuth [42] and reported in Algorithm 1.
n=0; mean=0; m2 = 0;
forx in data do
n = n+1;
delta = x - mean;
mean = mean + delta/n;
m2 = m2 + delta*(x - mean);
end
variance = m2/(n-1);
Algorithm 1: Knuth‚Äôs algorithm for seq. variance estimation.
We adapt Knuth‚Äôs algorithm by executing the body of the
loop for each incoming measurement pm(k), and updating
the input variance (2). This way we can efÔ¨Åciently keep our
dispersion index updated after every new measure is gathered.
In order to compute ethr, we assume the input measure-
ments to have Gaussian distribution centered around the actual
measure and with variance 2[36, 37]. We then want to
decide for each incoming measurement if it can be reasonably
explained under this assumption or not. To do so, we operate
similarly to a statistical hypothesis test on the mean of the
input distribution. Our hypothesis is that the actual mean of
the input distribution is bp(k 1), i.e., the latest estimate from
our Ô¨Ålter, and try to decide whether the measurement pm(k)is
far enough from bp(k 1)to contradict our hypothesis. Since
the variance of the input distribution has been estimated, we
should refer in this case to a t-test [21]. However, assuming
enough measurements have been retrieved (as a rule of thumb,
at least 30), we can safely use a z-test [21]. After deciding a
conÔ¨Ådence value , we can decide whether the last measure-
ment retrieved should be considered ‚Äúexplainable‚Äù with the
current estimation or the outcome of a change in the process
(it may also be the case of an outlier, but we will discuss how
to handle this situation later on). In particular, we assume a
change point occurred if
jpm(k) bp(k 1)j
z1 
2(10)
where=p
2is the standard deviation of the input
distribution. Equation (10) can be straightforwardly used to
adjust the value of ethrafter a new sample has been gathered:
ethr(k) =(k)z1 
2(k) (11)
The decision about constitutes a trade-off between a quick
response to smooth changes in the process and robustness to
measurement outliers. Particularly relevant for this decision
is the (average) number of transitions observed in each timestep: a small number will likely lead to noisy measurements
and increases the likelihood of outliers; in such case a very
small value of is recommended (see also Section VI).
D. On the Filter ConÔ¨Åguration
The Ô¨Ålter introduced in Section III-A has four conÔ¨Åguration
parameters: ,alo,ahi, and. While the role of has been
already discussed in the previous section, we will brieÔ¨Çy (and
informally) discuss here how the values of the others affect the
performance of the Ô¨Ålter. Some addition formal considerations
will be provided in Section IV .
Intuitively, there are two extreme operating conditions for
a Ô¨Ålter: the probability to be estimated is stationary, or it
is undergoing an abrupt change. In the Ô¨Årst case, we aim
at obtaining a ‚Äúclean‚Äù and smooth estimate. This means we
would like ato be close to 1. If the input measurements
come from a stationary distribution they will most likely be
recognized as ‚Äúcompatible‚Äù with the current estimate (see
Section III-C). This will drive the value of a(asymptotically)
toahi. Thus, setting ahiclose to 1will provide smooth
estimates of a stationary probability. Analogous considerations
can be stated for the case of abrupt changes, when the value
ofawill be moved toward its lower bound alo.
However, while it is quite safe to set ahivery close to
1, bringing aloclose to 0reduces the Ô¨Ålter‚Äôs robustness to
outliers. Indeed, allows to set a threshold to decide if
an input measurement is not compatible with the current
estimate. However, there is always the chance that an extreme
measurement is over the threshold and brings atowardsalo.
Thus, a more conservative choice of alois recommended if
the input measures are known to have a high variance. A rule
of thumb to decide such value is that 1 alocan be considered
as the maximum degree of trust on a new input measurement.
However, setting alo>:3may produce a sensible slow down
in change point tracking, and should be done carefully.
The value of determines how ‚Äúfast‚Äù to move from alo
toahiand vice versa. High values of are recommended
(1000) to obtain prompt switches when a change occurs.
Finally, a hidden parameter to consider is the duration of
a time step. Indeed, longer time steps may allow to collect
more events within the period, which in turn improves the
quality of the input measurements (which are the transition
frequencies computed over each time step). However, since
the Ô¨Ålter updates its estimates every time step, extending their
duration may slow down the Ô¨Ålter reaction to changes.
IV. F ORMAL ASSESSMENT
The proposed adaptive Ô¨Ålter requires a constant number of
operations each time step, as easily observable from Equa-
tion (5) and the Ô¨Ålter adaptation procedure in Algorithm 1.
Furthermore, the operations are mostly elementary Ô¨Çoating
point operations, and only a minimal amount of memory is
required, making our approach suitable to be executed even
on low-power devices (e.g., embedded systems). An empirical
estimation of the actual execution time on a general purpose
computer will be shown in Section VI.
203
203
203
203
ICSE 2015, Florence, ItalyHowever, the low computational overhead was only one of
our goals. In this section we will formally prove several other
critical properties of our approach. The proofs will provide
a theoretical guarantee about its applicability and the quality
of its results. An empirical assessment based on selected case
studies and the comparison with state of the art alternatives
will also be provided in Section VI.
In Section IV-A, we will prove the stability of our system,
i.e., its ability to converge to a steady state equilibrium for
every constant value of the input. In Section IV-B we will
prove that our Ô¨Ålter is an unbiased and consistent estimator of
the transition probabilities we aim at learning. In Section IV-C,
we will precisely quantify the ability of our Ô¨Ålter to reduce the
noise in the input measurements (i.e., their variance, under the
weak assumption of Gaussian distribution) as a function of
the Ô¨Ålter‚Äôs conÔ¨Åguration parameters. Finally, in Section IV-D
we will assess the settling time required for our estimates to
converge after a change as occurred.
A. Stability
A dynamic system is asymptotically stable if there exists an
equilibrium point to which the system tends; i.e., for any given
constant input, the output converges to a speciÔ¨Åc value (within
a convenient accuracy) regardless of the initial state [37]. As
time tends to inÔ¨Ånity, the distance to the equilibrium point has
to tend to zero.
Our Ô¨Ålter is formally deÔ¨Åned by the dynamic system of
Equation (5). In the following we will prove that the Ô¨Ålter
always converges to an equilibrium value, regardless its initial
conditions and for all the (valid) values of the input measures.
Let (5) be subject to the constant input pm(k) =pm. The
corresponding equilibrium value bpcan be obtained by com-
puting the Ô¨Åxed point solutions of the dynamic system [37]:
bp=abp+ (1 a)pm(12)
whereais the equilibrium value for a(k). This yields the
unique solution bp=pm(13)
since the case a= 1 is excluded by construction (Equa-
tion (1)). Also, since at the computed equilibrium e= 0
(because of Equation (13)), we can compute the value of a
as follows:a=a0+ afa( ethr) (14)
Hence, for any pmthere is one equilibrium with bp=pm, and
agiven by (14). To prove the stability of all equilibria (i.e.,
the stability of the Ô¨Ålter in every operating condition), we can
analyze the response of the system to a deviation from such
equilibrium [37]. We deÔ¨Åne the system output variation with
respect to the equilibrium value as
bp(k),bp(k) bp (15)
Combining Equation (15) with the last equation in (5):
bp(k) =a(k)bp(k 1) + (1 a(k))(pm(k 1) bp)(16)
DeÔ¨Åning now the input variation with respect to the equilib-
rium value as pm(k),pm(k) pm(17)
we have
bp(k) =a(k)bp(k 1) + (1 a(k))pm(k 1): (18)Furthermore, exploiting again the equilibrium,
e(k) =jpm(k)+pm (bp(k 1)+bp)j=jpm(k) bp(k 1)j
(19)
and the estimator can be rewritten in input/output variational
form [37] as(
a(k) =a0+ afa(jpm(k) bp(k 1)j ethr)
bp(k) =a(k)bp(k 1) + (1 a(k))pm(k 1)
(20)
For the purpose of the equilibrium stability, it is required to
study the motion of (20) under the constant input correspond-
ing to the equilibrium, i.e., with pm(k) = 0. This means
analyzing the systema(k) =a0+ afa(jbp(k 1)j ethr)
bp(k) =a(k)bp(k 1)(21)
Given the bounds on a(k)inherent to fa(),br(k)in (21)
eventually converges to zero irrespectively of its initial value.
Thus, all the equilibria of the dynamic system in Equation (5)
are globally asymptotically stable.
The stability proof guarantees the applicability of our learn-
ing approach under any possible (valid) input measurements,
in particular to learn the transition probability of any DTMC.
B. Unbiasedness and Consistency
Denoting with pothe true (constant) value of the measure to
estimate, from Equation (13) and the stability proof provided
in the previous section it follows that
lim
k!1bp(k) =po8bp(0) (22)
Hence, we can state
lim
k!1E[bp(k)] =poand lim
k!1E[(bp(k) po)2] = 0 (23)
Thus the estimator is (asymptotically) unbiased and consistent.
C. Variance of the Estimate
Consider the case where the input measurements provided to
our Ô¨Ålter are a realization of a white Gaussian noise input, i.e.,
a Gaussian distribution with mean 0and variance 2
w(i.e. the
simplest distribution allowing to arbitrarily set its variance).
For the sake of simplicity, assume ato be a constant value.
The ratio of the output variance over the input variance 2
w
is theH2norm of the estimator [37], i.e., of the linear, time-
invariant dynamic system with input pm(k)and outputbp(k).
The Z-Transform of the Ô¨Ålter‚Äôs transfer function [37] is:
G(z) =1 a
z a(24)
ItsH2norm can be expressed as
jjGjj 2=vuuttr 1X
k=0g(k)gT(k)!
(25)
whereg(k)is the system‚Äôs impulse response, i.e., the inverse
Ztransform of its transfer function [37]. In our case, then
g(k) =Z 11 a
z a
= (1 a)a k(26)
which leads to
jjGjj 2=vuut1X
k=0(1 a)2a 2k=r
(1 a)2
1 a2(27)
204
204
204
204
ICSE 2015, Florence, ItalyNote thatjjGjj 2tends to 1 and0+whenatends to 0+and
1 , respectively. This means that the output variance is never
greater than that of the input, and is reduced by higher values
ofa. This is in line with the intuitive considerations on the
impact of large and small values of aprovided in Section III-A.
D. Convergence Time
For an intuitive analysis, assume pm(k)undergoes a step
from zero to one, and that the convergence time kcis taken
as the number of steps required to drive the estimation error
magnitude down to the same threshold ethrused for switching
from the ‚Äúfast tracking‚Äù to the ‚Äúsharp Ô¨Åltering‚Äù modes of the
system, by moving atowardsaloandahi, respectively. This
immediately leads to determine kcas the minimum value of
k, s.t.ak
lo<ethr,
kc=&
logethr
logalo'
(28)
V. R ELATED WORK
Markov model learning and inferring of transition probabil-
ities of a DTMC has been widely used in different domains
[23, 24]. Two additional requirements for Software Engineer-
ing applications are the possibility of embedding experts or
domain knowledge and the ability of performing the estimation
online, by continuously improving on the prior knowledge
initially assumed.
One of the Ô¨Årst approaches providing these features is
Kami [12, 13]. This approach implements an established
Bayesian estimator to learn the transition probabilities of a
DTMC online. It requires a low computational overhead and
provides high accuracy and noise Ô¨Åltering for the estimation
of stationary processes. However, it provides slow responses
in presence of changes [13]. The same authors [16] faced
the problem of change point detection, again following a
Bayesian approach. The resulting technique is designed to
operate ofÔ¨Çine on recorded execution traces. It is quite accurate
in identifying change points, however it involves the use of a
Gibbs sampling techniques to compute the posterior change
point distribution [43]. Such randomized method requires a
large number of operations for each change point probe. This
requires a relatively high computational power, which might
be too expensive to be deployed on many embedded system.
The execution time is orders of magnitudes higher than the
original approach.
In [15], the authors propose an approach for the continuous
tracking of time-varying parameters of performance models.
The approach is based on (Extended) Kalman Ô¨Ålters [37] and is
able to estimate also correlated parameters, to take into account
nonlinear constraints among their values, and to embed a
prior knowledge about the distribution to estimate. However,
as reported by the authors, Kalman Ô¨Ålters provide their op-
timal performance when the model describing the temporal
evolution and the dependencies among parameters is linear.
Despite having been proposed in the domain of performance,
the approach of [15] can be easily adapted to also learn
the (time-varying) transition probabilities of a DTMC. Thesimplest way is to use a Kalman Ô¨Ålter to estimate each single
transition probability and then to apply the correction strategy
we introduced in Section III-B for the transitions originating
from the same state. The conÔ¨Åguration of the Kalman Ô¨Ålters
requires specifying two parameters: the measurement error co-
varianceRand the disturbance error covariance Q[15]. If we
estimate each single transition probability independently, the
two matrices reduce to two scalar values randq, representing
the variance of the measurement error and the variance of the
disturbance error. Informally, a high value of rmeans a poor
information from measures, while a high value of qmeans
high drift expected for the parameters‚Äô estimates. By tuning
these two parameters it is possible to deÔ¨Åne a tradeoff between
noise Ô¨Åltering (thus smoother estimates) and quick reaction to
changes in the estimated probabilities.
Another approach based on Bayesian estimation that aims
at overcoming the limitations of Kami in presence of changing
transition probabilities is the Cove approach [10, 14, 17]. The
basic intuition is to scale of each input measurement with
an aging factor that gives more relevance to recent observa-
tions [17]. In presence of a change, this input aging allows
to quickly discard old information and to give more relevance
to the new ones. The conÔ¨Åguration of this approach requires
the speciÔ¨Åcation of a prior knowledge for the distribution to
estimate, the conÔ¨Ådence c0>0on such an initial prior, and the
value of a parameter cwhich determines the aging factors: a
input measurement which has been observed ttime steps ago
it will be discounted by a factor in the order of  t
c. Cove
has been extended with a procedure to automatically adjust
the values of c0andc[14]. In the special case for the aging
factor (c= 1) Cove reduces to Kami.
VI. E XPERIMENTAL EVALUATION
In this section we report on the experimental evaluation of
our lightweight adaptive Ô¨Åltering approach to learn transition
probabilities of a DTMC. We will benchmark it against related
approaches with respect to (i) the estimation accuracy and
(ii) the time required for the estimations. Following from the
discussion in Section V , we selected for comparison the two
algorithms by Calinescu et al. [14] and Zheng et al. [15]. They
will be referred to as Cove andKalman respectively.
The three approaches will be compared in this section on six
different change patterns. On the Ô¨Årst part of the comparison
we will consider a selected case of each pattern for which
visualizing the behavior of the estimates and assessing their
accuracy. The scope of the comparison will be then extended to
a set of 7;000execution traces composed by both randomized
realizations of each pattern and combination thereof. Finally,
we will report on the computation time required by the three
approaches and discuss possible threats to validity.
Accuracy metrics. As accuracy metrics the Mean Average
Relative Error (MARE) [44] (similar to [15, 45]) will be used:
MARE =1nnX
i=1^p(i) p(i)
p(i): (29)
205
205
205
205
ICSE 2015, Florence, Italywhere ^p(i)represents the estimate at time i,p(i)the actual
value to estimate, and nis the number of points.
Experimental settings. We implemented all the approaches
in Python (v2.7) and executed the experiments on a quad-
core Intel(R) Xeon(R) CPU E31220 @3.10GHz with 32Gb
of memory and running an Ubuntu Server 12.04.4 64bit. The
memory consumption of the three approaches was negligible.
Our implementation is available at [18].
The three algorithms are compared on their performance on
estimating the next state distribution of a state of a DTMC
(i.e., a row of its transition matrix). The input traces are
composed by sequences of 30;000 events. Each event is
an integer number identifying the destination state of the
taken transition. This destination state is randomly selected
according to known (time-varying) transition probabilities. For
the sake of readability of the experiments, the number of
reachable states has been set to 3. This choice does not affect
the assessment of the accuracy of the estimates for none of the
approaches, since the accuracy of the estimates is not directly
affected by the number of transitions. The comparison of the
execution times of the three approaches is also not effected by
this choice, since such time grows linearly with the number of
reachable states for all the approaches. Thus the comparison
is straightforwardly generalizable to larger problems.
Explorative Study. Figures 3a to 3f report how the three
approaches perform with respect to six change patterns. Before
going into details of these Ô¨Ågure, we brieÔ¨Çy discuss the
conÔ¨Åguration settings for each of the three approaches.
Tools conÔ¨Åguration. Concerning Cove, we tried both the
static parametrization proposed [17] and the adaptive one
introduced in [14]. The latter provides for an online tuning
to trade accuracy for change reaction time. However, since
the inter-arrival time of the events in these settings is constant,
the online tuning method does not provide signiÔ¨Åcant improve-
ment on the optimal static conÔ¨Åguration proposed in [17]. For
this reason we report here the result of the superior static
parameter settings with c0= 1 andc= 1:01.
ForKalman, we set the parameter qto the square of the
maximum change to be captured in a single time step, as rec-
ommended in [15], which in our case yields q= 0:22(notably,
Kalman Ô¨Ålters are in general robust to bad initial values of
qby continuously tuning their behavior on incoming data).
Concerning the value of rjust general recommendations have
been proposed in [15]. Coherently with them, we set r=q 1.
This is also common practice in the area of control [38].
The Kalman approach [15] has been developed for tracking
performance measures and is not directly applicable to the
constraints of categorical probability distributions for DTMCs.
Consequently, we used it to estimate each single transition
probability and then applied the same correction procedure
we introduced for LAF in Section III-B.
Concerning LAF, we conÔ¨Ågured it to have prompt reactions
to changes ( = 10000), to consider as outlier a measure
having maximum probability to occur = 10 4, and to
provide relatively good Ô¨Åltering capabilities, both while theestimated probability is stationary (a hi= 0:9) and during
its changes (a lo= 0:3). Notice that higher values of ahi
would improve the noise reduction capabilities of LAF, but
slow down its convergence after a change. On the other hand,
lower values of alowould make the reaction to changes more
drastic, which has the counterpart of increasing the risk of
erroneously following possible outliers.
Finally, while Cove can update its estimates after each new
event, Kalman andLAF update theirs every time step. For this
reason we deÔ¨Åned a time step to occur every 75events. This
value is relatively small. However, a large time step may slow
down the reaction to changes (see Section III-D).
Experimental results. We executed and compared the
three approaches over six input data patterns that are
commonly used to evaluated the related approaches [14,
15]: Noisy, Step, Ramp, Square wave, Triangle wave and
Outlier. By covering these input data patterns we are
stressing different aspects of the learning problem.
TABLE I
SIX-PATTERNS BENCHMARK .
LAF Kalman Cove Noisy 4.41% 4.54% 11.70%
Step 4.19% 8.51% 8.65%
Ramp 4.28% 7.04% 8.27%
Square 6.54% 21.47% 8.50%
Triangle 7.79% 12.84% 8.16%
Outlier 3.68% 3.78% 8.56%The accuracy results
(MARE) for a single
run of all the input
patterns are reported in
Table I. For readability,
we report in Figures 3a
to 3f only the estimates
of the probability of
moving toward one target
state. For all the plots in
Figures 3a to 3f, a dashed grey line represents the actual
transition probability to estimate, while the continuous black
line represents its estimate. In the following we discuss each
of the six transition patterns and the performance of the three
approaches.
Noisy. For this case (Figure 3a), we do not sample from the
actual stationary transition probabilities but we add them a
white noise with standard deviation :01. For LAF andKalman
this white noise adds to the unavoidable measurement noise.
After an initial transitory, both LAF and Kalman roughly
converge to the actual mean value of the estimated probability
(0.2) and provide similar values for the MARE index. In this
situation, LAF has not perceived any signiÔ¨Åcant change in the
measures and is thus operation as a low pass Ô¨Ålter with pole
inahi. Hence, increasing ahiwould lead to a slower initial
convergence, but smoother estimate, as it is for Kalman, whose
optimality properties in this scenario are well studied [38] (the
worst accuracy of Kalman is mostly due to the initial slow
convergence). Cove converges almost immediately, but with a
poor Ô¨Åltering of the input noise.
Step. In the step change pattern the estimated probability
suddenly changes from 0:2 to0:4. Kalman provides the
smoothest estimates, though at the price of a slower reaction
to the change. On the other hand, LAF promptly reacts to
the change. Notice on Figure 3b the exponential convergence
toward the new estimate, as expected from the stability proof
and the settling time assessment of Section IV . The settling
206
206
206
206
ICSE 2015, Florence, Italytime can be improved by reducing the value of alo. However,
too small values of it may lead to overshooting due to an
overreaction. While Cove reacts immediately to the change,
its estimates keep being noisy. Notably, in this case LAF is
about two times more accurate than the others.
Ramp. The estimate transition probability moves here lin-
early from .2 to .4 in 10;000time points. This situation is par-
ticularly stressful for the change-point detection mechanism of
LAF, whose ethrgets continuously updated until the variance
estimator converges to the new steady value. The accumulation
of the errors of the internal variance estimator and the main
LAF Ô¨Ålter might lead to false positives during or right after
the ramp (as in Figure 3c around time 26;000). In this cases,
a not too small value of alo(as a rule of thumb between :2
and:35) may reduce the deviation after the false detection
and allow for a faster recovery, as evident from the Ô¨Ågure. As
expected from [15, 38, 45] Kalman can cope reasonably well
with smooth changes, however it is slower than LAF, which
leverages change reaction to perform step-shaped cuts of the
estimate error (see Figure 3c around time 15;000and19;000).
Cove reacted immediately to the change and has been able to
follow the ramp, though with the usual noise. Also in this case
LAF is about two times more accurate than the others.
Square wave. The square wave ampliÔ¨Åes the issues relate to
the step change, by allowing for a shorter learning time before
each change. While Kalman suffers a slow convergence rate,
LAF andCove follow the changes, again with Cove producing
a more noisy output. Under this scenario, the accuracy of LAF
is about three times higher than Kalman, while producing a
smoother estimate than Cove.
Triangle wave. In this case, smooth changes between two
probability values alternates periodically. With respect to the
case of ramp, the slopes are steeper, requiring a faster con-
vergence to the estimators. Cove can follow the changes as
quickly as for the ramp case. Kalman provides smooth esti-
mates, but its convergence time is too long and fails to follow
the repeated changes. On the other hand, LAF copes with
the continuous changes by combining a shorter convergence
rate of the adaptive low-pass Ô¨Ålter with occasional step-shaped
error cuts triggered by the change point detection mechanism
(as already observed for the ramp). However, as for the case
of the ramp, the continuously changing distribution may lead
to the accumulation of internal estimation errors that increase
the chance of false positives (see time 20;000of Figure 3e),
whose effects are recovered by alo=:3.
Outlier. In the last case we artiÔ¨Åcially introduced an outlier
with the duration of 25 events. LAF and Kalman show a
negligible reaction to the outlier. This is both due to the
Ô¨Åltering actions of the two and to the fact that, operating on
a time window, the impact of the outlier is already reduced
by the preliminary computation the window‚Äôs transition fre-
quencies. Despite the triggering of a false change detection,
alo=:3keeps the Ô¨Ålter robust to outliers, making it achieve
an accuracy slightly higher then Kalman, whose effective in
Ô¨Åltering outliers is known [38]. Notice that, as for the caseofNoisy, the main loss of accuracy of Kalman is due to the
initial convergence. The very fast reaction to change of Cove
made it quickly follow the outlier, though recovering to the
correct estimate right after.
General comments. As Ô¨Ånal remarks, we noticed that Cove
provides a very fast reaction to changes, which make their
presence almost irrelevant as for the impact on the MARE.
This comes however at the price of having noisy estimates.
To obtain a similar behavior with LAF, bothaloandahihave
to be set to very low values: this way LAF will approximate
a low pass Ô¨Ålter with a very small pole, which, looking at
the equations, would behave similarly to Cove. A well-known
problem of statistical estimation for probability values is the
difÔ¨Åculty of catching rare events, i.e. with probability close to
0 (or close to 1, since this implies another transition probability
has to be very small). This issue is present for the three
approaches. The stability prove of the two Ô¨Ålters LAF and
Kalman guarantees that they will eventually converge to the
estimated probability, however, for such extreme cases the
convergence time might be longer.
Randomized pattern instances. To further investigate the
accuracy of our Ô¨Ålter, we generated for each pattern a set
of1;000 random instances and analyzed the performance
of the three approaches on this broader set of problems.
Concerning the generation of the random instances: Noisy and
Outlier require to generate a baseline stationary distribution
and, respectively, the standard deviation of the noise (sampled
between:001 and:1) and the duration of the outlier (we
take as amplitude half of the maximum gap allowed by
the baseline distribution); all the other patterns require to
deÔ¨Åne two distributions and, for the square and triangle wave
patterns, the period of the wave (30; 000=n withn2[2;15]).
The MAREs of the three approaches are reported in the Ô¨Årst six
boxplots in Figure 3g. Notably, the results for all the patterns
resemble the accuracies reported in Table I for the exploratory
study, thus the behavior of the three approaches in each of
the six change patterns does not depend, on average, on the
characteristics of the speciÔ¨Åc instance of such pattern.
Finally, the last box of Figure 3g (VarMix ) shows the accu-
racy of the three approaches on long traces (from 50;000 to
500;000events) obtained by sequentially combining multiple
random instances of the six patterns. The duration and of each
instance and their oerder are randomized as well. The results
ofVarMix conÔ¨Årm the earlier results of the single patterns. In
particular, Kalman suffers from the presence of fast changes,
while the MARE of Cove is not signiÔ¨Åcantly affected by these
changes, but by the high noise of its estimates.
Runtime overhead. On average over 6;000 runs, LAF,
Kalman, and Cove required 50, 80, and 126 ms to process
30;000events. Consequently, LAF reduces the runtime over-
head of Kalman, and Cove. Notice that LAF and Kalman
update their estimates every time window, while Cove updates
every new measure.
Threats to validity. A threat to external validity is the
use of predeÔ¨Åned input data patterns for the comparison of
207
207
207
207
ICSE 2015, Florence, Italythe approaches and the ability to generalize these results to
common traces of realistic software systems. We have selectedthese inputs inline with common theory, common practice incontrol theory to stress the response of dynamic systems (Step,Ramp), and of Ô¨Ålters in particular (Noisy, Periodic, Outlier),and the related approaches [14, 15, 37] and could observesimilar patterns also in QoS data sets of web services [46] andweb systems (cp. next Section VII). Consequently, we arguethat a good performance for these basic input data types willalso results in a good general performance.
The threats to internal validity include obviously the selec-
tion of the parameters c
0,Œ±c,qandrfor the related approaches
and the number of events per time step. We have speciÔ¨Åcallyselected these parameters with defaults that are also deÔ¨Ånedin the original papers. Furthermore, parameter sweeps otherthe range of these parameters conÔ¨Årmed that they where goodchoices for our experiment. Another threat to internal validityis the implementation of the related approaches and measure-ment environment. As can be seen from the experimentalsetup we tried to avoid systematic measurement errors andfor the implementation we did follow the instructions for thealgorithms provided in the original literature.
VII. A
PPLICATION TO A REALISTIC PROBLEM
A common application of DTMC learning is to learn users
behavior for a software system (e.g., [47]). This problem canbe reÔ¨Åned to the scenario of estimating the probability of auser browsing from one webpage to another capturing onlinelog events. To evaluate suitability of LAF in this scenario we
took the open logs of the World Cup 98 website [19].
The logs spread over a period of about 3 months. The
website is composed of a total of over 32000 pages. Wemapped every webpage to a unique integer identiÔ¨Åer. Eachline of the log includes a unique client id. To identify aclient session we set a timeout of 30 minutes after the lastoccurrence of the client id to consider the corresponding userdisconnected. A session is thus described by the sequence ofpairs [time,pageId] visited by a client. During a session, theclient is expected to move from one page to another followingnavigation links. For demonstration purpose, we show inFigure 2 the online estimation of the transition probabilityfrom the page ‚Äú/english/teams/teambio160.htm‚Äù to the page‚Äú/english/competition/statistics.htm‚Äù.
The monitored webpage has been active for about 33 days
for a total of 2835186 recorded transitions. We reduced thegranularity of the observations by applying a sliding windowof 500 seconds. Excluding the windows where no eventsoccurred, 14852 windows have been processed.
The gray line represents the transition frequency observed
during the corresponding window reported on the x-axis. Theblack thicker line represents the value of the LAF estimate.
Since we do not know the real value to be estimated, it
is hard to evaluate the ability of LAF to capture the average
transition probability other than by visual inspection (indeed,a proper computation of MARE would require to know the
Fig. 2. Estimate of a transition probability from the Worldcup98 case study
(x-axis in time windows).
actual transition probability of whom the observed transition
frequencies are realizations). However, it is easy to recognizein Figure 2 the occurrence of several patterns we analyzedpreviously in this section for which a deep quantitative inves-tigation has been provided. The execution time (over 5 runs)to estimate the transition probabilities from the observed state(5 destination states) have been 1703, 2177, and 15953 ms forLAF, Kalman, and Cove respectively. Since Cove operates per
transition, its execution time is higher than LAF and Kalman,
which instead updates their estimates every 75 transitions andperformed inline with the results on the benchmark patterns.Overall, with the new algorithm LAF we have been able to
process the data for this realistic case and we could conÔ¨Årmthe results of the experimental evaluation.
VIII. C
ONCLUSIONS
In this work we presented a lightweight adaptive Ô¨Ålter
for online learning of the transition matrix of a DTMC. Weproved it is stable and provides an unbiased and consistentestimate of the transition probabilities. We also quantiÔ¨Åed itsability to reduce the variance of noisy input measurementsand its convergence time after a change has occurred. TheÔ¨Ålter introduces a minimal computational overhead, beingable to process 30,000 events in about 0.05 seconds on
a general purpose computer. Its memory demand does notdepend on the number of events to be processed, and it is fairlynegligible. The experimental results show an high accuracy ofthe obtained estimates.
We plan to extend this work along several directions. First,
we will expand the developed algorithm to other probabilisticquality evaluation models, including queueing networks forperformance analysis, and integrate it into a general frameworkfor continual veriÔ¨Åcation [10]. Second, we plan to increase theorder of the Ô¨Ålter to further improve its ability of trading offreaction to changes versus robustness to outliers. Finally, weplan to investigate the combination of LAF with forecastingtechniques [46, 48] for proactive problem detection.
A
CKNOWLEDGEMENTS
This work has been partially supported by the DFG (German
Research Foundation) under the Priority Programme SPP1593:Design For Future - Managed Software Evolution.
208
208
208
208
ICSE 2015, Florence, Italy0.150.200.250.300.35
0 10000 20000 30000
0.150.200.250.300.35
0 10000 20000 30000
0.150.200.250.300.35
0 10000 20000 30000(a) Stationary signal with white noise ( =:001)
0.20.30.40.5
0 10000 20000 30000
0.20.30.40.5
0 10000 20000 30000
0.10.20.30.40.5
0 10000 20000 30000
(b) Step (gap=:2)
0.20.30.40.5
0 10000 20000 30000
0.20.30.40.5
0 10000 20000 30000
0.10.20.30.40.5
0 10000 20000 30000
(c) Ramp (gap=:2, duration=10000 steps)
0.20.30.40.5
0 10000 20000 30000
0.20.30.40.5
0 10000 20000 30000
0.10.20.30.40.5
0 10000 20000 30000
(d) Square wave (gap=:2, period=10000 steps)
0.20.30.40.5
0 10000 20000 30000
0.20.30.40.5
0 10000 20000 30000
0.10.20.30.40.5
0 10000 20000 30000
(e) Triangle wave (gap=:2, period=10000 steps)
0.20.30.40.5
0 10000 20000 30000
0.20.30.40.5
0 10000 20000 30000
0.10.20.30.40.5
0 10000 20000 30000
(f) Outlier (gap=:4, duration=25 steps)
0.11
0.05 0.060.088
0.0540.0990.082
00.20.4
Noisy Outl Ramp Squar Step Trian VarMix
0.11
0.0460.0720.18
0.0720.13 0.12
00.20.4
Noisy Outl Ramp Squar Step Trian VarMix
0.14
0.097 0.099 0.11 0.1 0.0920.1
00.20.4
Noisy Outl Ramp Squar Step Trian VarMix
(g) Boxplots of the relative error over 1000 random instances of the six change patterns and combinations thereof.
Fig. 3. First six rows: estimates of the probability of moving toward the Ô¨Årst state obtained by LAF (left column), Kalman (central column), and Cove (right
column). Last row: IQR boxplots of the relative errors obtained over 1000 random instances of the six patterns and combinations thereof.
209
209
209
209
ICSE 2015, Florence, ItalyREFERENCES
[1] A. Aziz, K. Sanwal, V . Singhal, and R. Brayton, ‚ÄúModel-checking
continuous-time markov chains,‚Äù ACM Trans. Comput. Log., vol. 1,
no. 1, pp. 162‚Äì170, Jul 2000 DOI:10.1145/343369.343402
[2] C. Baier, J.-P. Katoen, and H. Hermanns, ‚ÄúApproximate symbolic model
checking of continuous-time markov chains,‚Äù in Proceedings of the 10th
International Conference on Concurrency Theory (CONCUR 1999), vol.
1664. Springer, 1999, pp. 146‚Äì161 DOI:10.1007/3-540-48320-9 12
[3] C. Baier, B. R. Haverkort, H. Hermanns, and J.-P. Katoen,
‚ÄúModel-checking algorithms for continuous-time markov chains,‚Äù
IEEE Trans. Softw. Eng. , vol. 29, no. 6, pp. 524‚Äì541, 2003
DOI:10.1109/TSE.2003.1205180
[4] A. Bianco and L. de Alfaro, ‚ÄúModel checking of probabilistic and nonde-
terministic systems,‚Äù in Proceedings of the 15th Conference on Founda-
tions of Software Technology and Theoretical Comp. Science (FSTTCS),
vol. 1026. Springer, 1995, pp. 499‚Äì513 DOI:10.1109/MC.2009.326
[5] H. Hermanns, J.-P. Katoen, J. Meyer-Kayser, and M. Siegle, ‚ÄúA tool
for model-checking Markov chains,‚Äù International Journal on Software
Tools for Technology Transfer (STTT), vol. 4, no. 2, pp. 153‚Äì172, Feb
2003 DOI:10.1007/s100090100072
[6] M. Z. Kwiatkowska, G. Norman, and D. Parker, ‚ÄúProbabilistic symbolic
model checking with prism: a hybrid approach,‚Äù Int. Journal on Software
Tools for Technology Transfer (STTT) , vol. 6, no. 2, pp. 128‚Äì142, Aug
2004 DOI:10.1007/s10009-004-0140-2
[7] L. Grunske, ‚ÄúSpeciÔ¨Åcation patterns for probabilistic quality properties,‚Äù
in30th International Conference on Software Engineering (ICSE 2008).
ACM, 2008, pp. 31‚Äì40 DOI:10.1145/1368088.1368094
[8] L. Baresi, E. D. Nitto, and C. Ghezzi, ‚ÄúTowards open-world soft-
ware: Issues and challenges,‚Äù ser. SEW. IEEE, 2006, pp. 249‚Äì252
DOI:10.1109/MC.2006.362
[9] B. H. C. Cheng, R. de Lemos, H. Giese, P. Inverardi, J. Magee,
J. Andersson, B. Becker, N. Bencomo, Y . Brun, B. Cukic, G. D. M.
Serugendo, S. Dustdar, A. Finkelstein, C. Gacek, K. Geihs, V . Grassi,
G. Karsai, H. M. Kienle, J. Kramer, M. Litoiu, S. Malek, R. Mirandola,
H. A. M ¬®uller, S. Park, M. Shaw, M. Tichy, M. Tivoli, D. Weyns,
and J. Whittle, ‚ÄúSoftware engineering for self-adaptive systems: A
research roadmap,‚Äù in Software Engineering for Self-Adaptive Systems,
ser. LNCS, vol. 5525. Springer, 2009, pp. 1‚Äì26.
[10] R. Calinescu, K. Johnson, Y . RaÔ¨Åq, S. Gerasimou, G. C. Silva, and
S. N. Pehlivanov, ‚ÄúContinual veriÔ¨Åcation of non-functional properties
in cloud-based systems,‚Äù in Proceedings of the 5th International Work-
shop Non-functional Properties in Modeling: Analysis, Languages and
Processes, Miami, USA, September 29, 2013, vol. 1074, 2013, pp. 1‚Äì5.
[11] G. S. Blair, N. Bencomo, and R. B. France, ‚ÄúModels@ run.time,‚Äù IEEE
Computer, vol. 42, no. 10, pp. 22‚Äì27, 2009 DOI:10.1109/MC.2009.326
[12] I. Epifani, C. Ghezzi, R. Mirandola, and G. Tamburrelli, ‚ÄúModel
evolution by run-time parameter adaptation,‚Äù in Proceedings of the 31st
International Conference on Software Engineering, (ICSE 2009). IEEE,
2009, pp. 111‚Äì121 DOI:10.1109/ICSE.2009.5070513
[13] A. Filieri, C. Ghezzi, and G. Tamburrelli, ‚ÄúA formal approach to adaptive
software: continuous assurance of non-functional requirements,‚Äù Formal
Aspects of Computing, vol. 24, pp. 163‚Äì186, 2012 DOI:10.1007/s00165-
011-0207-2
[14] R. Calinescu, Y . RaÔ¨Åq, K. Johnson, and M. E. Bakir, ‚ÄúAdaptive
model learning for continual veriÔ¨Åcation of non-functional properties,‚Äù
inACM/SPEC International Conference on Performance Engineering,
ICPE‚Äô14, Dublin, Ireland, March 22-26, 2014. ACM, 2014, pp. 87‚Äì98
DOI:10.1145/2568088.2568094
[15] T. Zheng, C. M. Woodside, and M. Litoiu, ‚ÄúPerformance model estima-
tion and tracking using optimal Ô¨Ålters,‚Äù IEEE Trans. Softw. Eng., vol. 34,
no. 3, pp. 391‚Äì406, 2008 DOI:10.1109/TSE.2008.30
[16] I. Epifani, C. Ghezzi, and G. Tamburrelli, ‚ÄúChange-point detection
for black-box services,‚Äù in Proceedings of the 18th ACM SIGSOFT
International Symposium on Foundations of Software Engineering (FSE
2010). ACM, 2010, pp. 227‚Äì236 DOI:10.1145/1882291.1882326
[17] R. Calinescu, K. Johnson, and Y . RaÔ¨Åq, ‚ÄúUsing observation ageing to
improve markovian model learning in qos engineering,‚Äù in Second Joint
WOSP/SIPEW International Conference on Performance Engineering
(ICPE‚Äô11). ACM, 2011, pp. 505‚Äì510 DOI:10.1145/1958746.1958823[18] A. Filieri, L. Grunske, and A. Leva, ‚ÄúLightweight adaptive Ô¨Åltering for
dtmc learning: supplementary material,‚Äù http://www.antonio.Ô¨Ålieri.name/
publications/preprints/2015-icse/.
[19] Hewlett-Packard Labs, ‚ÄúWorldcup98 web logs,‚Äù http://ita.ee.lbl.gov/
html/contrib/WorldCup.html.
[20] C. Baier and J.-P. Katoen, Principles of Model Checking. MIT Press,
2008.
[21] W. Pestman, Mathematical Statistics, ser. De Gruyter Textbook. De
Gruyter, 2009.
[22] S. Ross, Stochastic processes, ser. Wiley series in probability and
statistics: Probability and statistics. Wiley, 1996.
[23] T. W. Anderson and L. A. Goodman, ‚ÄúStatistical inference about markov
chains,‚Äù The Annals of Mathematical Statistics, vol. 28, no. 1, pp. 89‚Äì
110, 1957.
[24] C. ChatÔ¨Åeld, ‚ÄúStatistical inference regarding markov chain models,‚Äù J.
R. Stat. Soc., vol. 22, no. 1, pp. 7‚Äì20, 1973.
[25] A. Immonen and E. Niemel, ‚ÄúSurvey of reliability and availability
prediction methods from the viewpoint of software architecture,‚Äù SoSyM,
vol. 7, no. 1, pp. 49‚Äì65, 2008 DOI:10.1007/s10270-006-0040-x
[26] R. C. Cheung, ‚ÄúA user-oriented software reliability model,‚Äù
IEEE Trans. Softw. Eng., vol. 6, no. 2, pp. 118‚Äî125, 1980
DOI:10.1109/TSE.1980.234477
[27] A. Filieri, C. Ghezzi, V . Grassi, and R. Mirandola, ‚ÄúReliability
analysis of component-based systems with multiple failure modes,‚Äù
Component-Based Software Engineering (CBSE 2010), pp. 1‚Äì20, 2010
DOI:10.1007/978-3-642-13238-41
[28] M. Kwiatkowska, ‚ÄúQuantitative veriÔ¨Åcation: models techniques and
tools,‚Äù in Proceedings of the the 6th joint meeting of the European
software engineering conference and the ACM SIGSOFT symposium
on The foundations of software engineering (ESEC/FSE 2007). ACM,
2007, pp. 449‚Äî458 DOI:10.1145/1287624.1287688
[29] A. Filieri, C. Ghezzi, and G. Tamburrelli, ‚ÄúRun-time efÔ¨Åcient prob-
abilistic model checking,‚Äù in Proceedings of the 33rd International
Conference on Software Engineering (ICSE 2011) . ACM, 2011, pp.
341‚Äì350 DOI:10.1145/1985793.1985840
[30] R. Calinescu, L. Grunske, M. Z. Kwiatkowska, R. Mirandola, and
G. Tamburrelli, ‚ÄúDynamic qos management and optimization in service-
based systems,‚Äù IEEE Trans. Softw. Eng., vol. 37, no. 3, pp. 387‚Äì409,
2011 DOI:10.1109/TSE.2010.92
[31] A. Filieri, C. Ghezzi, A. Leva, and M. Maggio, ‚ÄúSelf-adaptive software
meets control theory: A preliminary approach supporting reliability
requirements,‚Äù in Proceedings of the 26th IEEE/ACM International
Conference on Automated Software Engineering (ASE 2011) . IEEE
CS, 2011, pp. 283‚Äì292 DOI:10.1109/ASE.2011.6100064
[32] ‚Äî‚Äî, ‚ÄúReliability-driven dynamic binding via feedback control,‚Äù
inSoftware Engineering for Adaptive and Self-Managing Sys-
tems (SEAMS), 2012 ICSE Workshop on, june 2012, pp. 43‚Äî52
DOI:10.1109/SEAMS.2012.6224390
[33] A. Filieri and C. Ghezzi, ‚ÄúFurther steps towards efÔ¨Åcient runtime veriÔ¨Å-
cation: Handling probabilistic cost models,‚Äù in Proceedings of the First
International Workshop on Formal Methods in Software Engineering:
Rigorous and Agile Approaches, ser. FormSERA ‚Äô12. IEEE Press,
2012, pp. 2‚Äì8.
[34] I. Meedeniya and L. Grunske, ‚ÄúAn efÔ¨Åcient method for architecture-
based reliability evaluation for evolving systems with changing pa-
rameters,‚Äù in IEEE 21st International Symposium on Software Reli-
ability Engineering, (ISSRE 2010). IEEE CS, 2010, pp. 229‚Äì238
DOI:10.1109/ISSRE.2010.19
[35] C. Robert, The Bayesian Choice: From Decision-Theoretic Foundations
to Computational Implementation, ser. Springer Texts in Statistics.
Springer, 2007.
[36] A. Law, Simulation Modeling and Analysis, ser. McGraw-Hill series
in industrial engineering and management science. McGraw-Hill
Education, 2014.
[37] K. ÀöAstr¬®om and B. Wittenmark, Computer-Controlled Systems: Theory
and Design. Dover Publications, 2013.
[38] W. Levine, The Control Handbook, ser. Electrical Engineering Hand-
book. Taylor & Francis, 2010.
210
210
210
210
ICSE 2015, Florence, Italy[39] F. Cellier and E. Kofman, Continuous System Simulation. Springer,
2006.
[40] K. Ito and K. Kunisch, Lagrange Multiplier Approach to Variational
Problems and Applications, ser. Advances in Design and Control.
Society for Industrial and Applied Mathematics, 2008.
[41] H. W. Kuhn and A. W. Tucker, ‚ÄúNonlinear programming,‚Äù in Proc.
2ndBerkeley Symposium on Mathematical Statistics and Probabilistics.
Berkeley: University of California Press, 1951, pp. 481‚Äì492.
[42] D. Knuth, The Art of Computer Programming: Seminumerical algo-
rithms, ser. Addison-Wesley series in computer science and information
processing. Addison-Wesley, 1981.
[43] C. Robert and G. Casella, Monte Carlo Statistical Methods, ser. Springer
Texts in Statistics. Springer, 2010.
[44] C. ChatÔ¨Åeld, The Analysis of Time Series: An Introduction, ser. Chapman
& Hall/CRC Texts in Statistical Science. Taylor & Francis, 2013.
[45] A. Filieri, H. Hoffmann, and M. Maggio, ‚ÄúAutomated design of
self-adaptive software with control-theoretical formal guarantees,‚Äù inProceedings of the 36th International Conference on Software En-
gineering (ICSE 2014), ser. ICSE. ACM, 2014, pp. 299‚Äì310
DOI:10.1145/2568225.2568272
[46] A. Amin, L. Grunske, and A. Colman, ‚ÄúAn automated approach to
forecasting qos attributes based on linear and non-linear time series
modeling,‚Äù in Proceedings of the 27th IEEE/ACM International Con-
ference on Automated Software Engineering (ASE 2012). IEEE, Sept
2012, pp. 130‚Äì139 DOI:10.1145/2351676.2351695
[47] C. Ghezzi, M. Pezz `e, M. Sama, and G. Tamburrelli, ‚ÄúMining behavior
models from user-intensive web applications,‚Äù in Proceedings of the 36th
International Conference on Software Engineering (ICSE 2014). ACM,
2014, pp. 277‚Äì287 DOI:10.1145/2568225.2568234
[48] A. Amin, A. Colman, and L. Grunske, ‚ÄúAn approach to forecasting
qos attributes of web services based on arima and garch models,‚Äù in
Proceedings of the IEEE 19th International Conference on Web Services
(ICWS 2012). IEEE, June 2012, pp. 74‚Äì81 DOI:10.1109/ICWS.2012.37
211
211
211
211
ICSE 2015, Florence, Italy