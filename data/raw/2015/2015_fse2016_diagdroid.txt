DiagDroid: Android Performance Diagnosis via
Anatomizing Asynchronous Executions
Yu KangyzY angfan ZhouxHui XuyzMichael R. Lyuyz
School of Computer Science, Fudan University, China
yShenzhen Research Institute, The Chinese University of Hong Kong, China
zDepartment of Computer Science and Engineering, The Chinese University of Hong Kong, China
xEngineering Research Center of Cyber Security Auditing and Monitoring, Ministry of Education, China
ABSTRACT
Rapid UI responsiveness is a key consideration to Android
app developers. However, the complicated concurrency mod-
el of Android makes it hard for developers to understand and
further diagnose the UI performance. This paper presents
DiagDroid , a tool specically designed for Android UI per-
formance diagnosis. The key notion of DiagDroid is that
UI-triggered asynchronous executions contribute to the UI
performance, and hence their performance and their run-
time dependency should be properly captured to facilitate
performance diagnosis. However, there are tremendous ways
to start asynchronous executions, posing a great challenge to
proling such executions and their runtime dependency. To
this end, we properly abstract ve categories of asynchron-
ous executions as the building basis. As a result, they can be
tracked and proled based on the specics of each category
with a dynamic instrumentation approach carefully tailored
for Android. DiagDroid can then accordingly prole the
asynchronous executions in a task granularity, equipping it
with low-overhead and high compatibility merits. The tool is
successfully applied in diagnosing 33 real-world open-source
apps, and we nd 14 of them contain 27 performance issues.
It shows the eectiveness of our tool in Android UI perfor-
mance diagnosis. The tool is open-source released online.
CCS Concepts
Software and its engineering !Software perfor-
mance;
Keywords
Android; Performance Diagnosis; UI Responsiveness
1. INTRODUCTION
As daily-use personal devices, smartphones are required
to provide quick response to the user interface (UI). UI per-
formance of a smartphone app is a critical factor to its user
experience, and hence becomes a major concern to develop-
ers [38, 39]. Many recent research eorts have therefore beenput on addressing the performance issues of Android apps
(e.g., Asynchronizer [36], Panappticon [72]). However, poor
UI performance of Android apps remains a widely-complaint
type of issues among users [38, 39]. App developers are still
lacking a handy tool to help combat performance issues.
Android provides a non-blocking paradigm to process UI
events ( i.e., user inputs) for its apps. The UI main thread
dispatches valid UI events to their corresponding UI event
procedures ( i.e., the UI event-handling logic). A UI event
procedure generally runs in an asynchronous manner, so that
the main thread can handle other UI events simultaneously.
After the asynchronous part is done, the UI can be updat-
ed with a call-back mechanism. This paradigm will lead to
complicated concurrent executions. The asynchronous exe-
cution processes may bear implicit dependency during their
runtime. For example, two may be scheduled to run in the
same thread by Android, and one may consequently wait for
the other to complete. Such unexpected waiting may result
in longer delay for a UI procedure, leading to UI perfor-
mance issues. However, it is hard to predict such runtime
dependency during the coding phase due to the complica-
tions of Android's asynchronous execution mechanisms [53].
Performance issues are hence inevitable.
Concurrency is a notorious source of bugs [41]. Current
tools for diagnosing Android UI performance issues generally
consider either the synchronous part of the UI event proce-
dure [63], or the execution process of one UI event procedure
per se [72]. They do not focus on the dependency of multi-
ple asynchronous execution processes. Hence, they are still
not enough to cope with the UI performance issues largely
caused by such runtime dependency.
Long-term testing is a well-known, viable means to trigger
bugs caused by concurrency [34]. Unfortunately, we lack an
automatic mechanism to verify whether there exists a per-
formance issue in the long-term testing. Manual inspection
of the tremendous traces produced by current method trac-
ing tools ( e.g., Traceview [54]) is extremely labor-intensive,
if not infeasible, not to mention their huge overhead.
We nd that unlike general concurrent programs [13, 14,
28, 32], an Android UI event procedure can be anatomized
into a set of trackable tasks , which can then be properly
proled so as to facilitate the detection and localization of
performance issues. Specically, although Android supports
tremendous ways to schedule asynchronous executions, we
conclude that they can actually be abstracted as ve cat-
egories. Executions of each category can be tracked and
proled in task granularity according to their specics. UI
performance can hence be modeled by the performance of
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
FSEâ€™16 , November 13â€“18, 2016, Seattle, WA, USA
c2016 ACM. 978-1-4503-4218-6/16/11...$15.00
http://dx.doi.org/10.1145/2950290.2950316
410File - D:\svn\perf_test\paper\fse2016_perftest\fig\Original_Asy ncTask.java
Page 1 of 1publicÂ classÂ MyActivity Â extends Â Activity Â {
private Â classÂ RetrieveDataTask Â extends Â AsyncTask<String, Â Void,Â String> Â {
...
//Â doInBackground Â willÂ beÂ executed Â asynchronously Â inÂ aÂ workerÂ thread
protected Â StringÂ doInBackground(String... Â urls)Â {
...Â Â Â //Â Retrieve Â content Â fromÂ Internet
returnÂ content;
}
//Â onPostExecute Â willÂ beÂ invoked Â inÂ theÂ mainÂ threadÂ afterÂ doInBackground
//Â completes, Â whichÂ showsÂ theÂ downloaded Â content Â inÂ theÂ UI.
protected Â voidÂ onPostExecute(String Â content){
Â Â Â Â Â Â Â Â Â Â Â Â  this .textView.setText(content);
}
}
...private Â classÂ MyOnClickListener Â implements Â OnClickListener{
protected Â voidÂ onClick(View Â v){
RetrieveDataTask Â task1Â =Â newÂ RetrieveDataTask(textView1);
//Â callÂ execute Â methodÂ according Â toÂ theÂ example Â ofÂ official Â document
task1.execute(url1);
}
}
}Figure 1: An AsyncTask example
the tasks. We further tackle the complication of runtime de-
pendency via examining the dependency of tasks, which can
be solved by checking whether the tasks request the same
execution unit ( e.g., a thread pool). Via modeling task per-
formance by not only its execution time, but the time when
it waits for execution ( i.e., the time between when it is sched-
uled and when it starts execution), we can model how a task
is inuenced by the others. Thus, performance issues due to
asynchronous executions can be properly captured.
Hence, this paper proposes DiagDroid (Performance Di-
agnosis for An droid ), a novel tool to exercise, prole, and
analyze the UI performance of an Android app without mod-
ifying its codes. First, via a light-weight static analysis of the
target app, DiagDroid obtains the necessary information for
proling. Then it employs a plugin testing approach ( e.g.,
Monkey [67]) to exercise the original app. The required run-
time data are then captured during the testing run via its
proler. The data are then processed oine to generate a
human readable report. The report can unveil potential per-
formance bugs to developers and direct them to suspicious
locations in the source codes. Human eorts can greatly be
reduced in diagnosing UI performance issues. Finally, Diag-
Droid solves the compatibility and eciency challenges gen-
erally faced by the dynamic analysis tools by slightly instru-
menting only the general Android framework invocations
with a dynamic instrumentation approach. Hence, it can be
applied to most o-the-shelf smartphone models and apps.
We have implemented and released DiagDroid [19]. We
show it is easy to apply it to real-world apps with light con-
gurations. In the 33 open-source real-world apps we study,
27 performance defects in 14 apps are found, and we receive
positive feedbacks from their developers. These defects are
caused by the complicated dependency of asynchronous exe-
cutions, which can hardly be located with current diagnosis
practice. This shows the eectiveness of DiagDroid .
2. ANDROID APPLICATION SPECIFICS
2.1 UI Event Processing
Designed mainly for user-centric usage patterns, Android
apps are typically UI oriented: An app will iteratively pro-
cess user inputs, and accordingly update the display to show
the intended contents. The main thread of an app is the sole
thread that handles the UI-related operations [53], such as
processing user inputs and displaying UI components ( e.g.,
buttons and images). When a valid user input ( i.e., a UI
event) comes, the main thread can invoke its corresponding
UI event procedure ,i.e., the codes that handle the UI event.
Some UI event procedures may be time-consuming, e.g.,
File - D:\svn\perf_test\AsyncHook\app\src\main\java\com\cudroid\xposed\activityhook\various_async_exec.java
Page 1 of 1package Â com.cudroid.xposed.activityhook;
importÂ android.app.DownloadManager;
importÂ android.app.IntentService;
importÂ android.content.Context;
importÂ android.content.Intent;
importÂ android.net.Uri;
importÂ android.os.AsyncTask;
importÂ android.os.HandlerThread;
importÂ java.util.concurrent.ExecutorService;
importÂ java.util.concurrent.Executors;
/**
*Â Created Â byÂ curidÂ onÂ 2016/3/7.
*/
publicÂ classÂ AsyncTasks Â extends Â Context Â {
private Â classÂ DownloadRunnable Â implements Â Runnable{
DownloadRunnable(String Â url){}
@Override
publicÂ voidÂ run()Â {
}
}private Â StringÂ urlÂ =Â "";
publicÂ classÂ DownloadService Â extends Â IntentService {
publicÂ staticÂ StringÂ URLKEYÂ =Â "";
publicÂ DownloadService
(String Â name) {
super (name);
}publicÂ DownloadService(){ super ("");}
@Override
protected Â voidÂ onHandleIntent(Intent Â intent) {
}
}
publicÂ classÂ DownloadTask Â extends Â AsyncTask{
publicÂ DownloadTask(String Â url){}
@Override
protected Â ObjectÂ doInBackground(Object[] Â objects) Â {
returnÂ null ;
}
}
publicÂ voidÂ asyncTasks{
//Create Â aÂ newÂ thread Â andÂ download Â inÂ that Â thread
ThreadÂ threadÂ =Â newÂ Thread (newÂ DownloadRunnable 
(url));
thread.start();
//Download Â inÂ one Â thread Â ofÂ aÂ thread Â poolÂ withÂ capacity Â 10
ExecutorService Â threadPool Â =Â Executors.newFixedThreadPool( 10);
threadPool.execute( newÂ DownloadRunnable(url));
//Download Â withÂ AysncTask
AsyncTask Â asyncTask Â =Â newÂ DownloadTask(url);
asyncTask.execute();//Download Â inÂ aÂ HandlerThread Â byÂ posting Â aÂ task Â onÂ the Â attached Â handler
HandlerThread Â handlerThread Â =Â newÂ HandlerThread ("DownloadHanderThread" );
handlerThread
.start();
//Download Â inÂ aÂ user â€defined Â Service
IntentÂ downloadIntent Â =Â newÂ Intent(this ,Â DownloadService. class );
downloadIntent.putExtra(DownloadService.URLKEY, Â url);
//UseÂ standard Â DownloadManager Â Service, Â utilizing Â ThreadPoolExecutor Â implictly
DownloadManager Â dmÂ =Â (DownloadManager) Â getSystemService Â (DOWNLOAD_SERVICE);
DownloadManager.Request Â reqÂ =Â newÂ DownloadManager.Request(Uri.parse(url));
dm.enqueue(req);
}
}Thread
ThreadPool
Executor
Handler
Intent 
Service
Download  
ManagerAsyncTaskFigure 2: Asynchronous execution examples
one to download a le from the Internet. To avoid block-
ing the main thread, UI event procedures conduct heavy-
weighted work in an asynchronous manner so that the main
thread can handle other UI inputs simultaneously [53]. Af-
ter such asynchronous executions are done, the UI can be
updated in the main thread with a call-back mechanism.
Figure 1 shows the codes of an Activity (i.e., a window
container for the UI components to be displayed). It re-
trieves data from the Internet and displays the data in a
TextView (i.e., a UI component to display text) after a but-
ton is touched. The Internet access is done asynchronously
in another thread while the TextView update is done in the
main thread. More specically, the RetrieveDataTask ex-
tends the AsyncTask class. It overrides the doInBackground
method to allow accessing the Internet asynchronously in
a worker thread. Its onPostExecute method is a call-back
mechanism to allow the corresponding TextView object up-
date in the main thread. These codes are abstracted from
RestC [66] project which shows a common coding practice.
2.2 Asynchronous Executions
Android provides high exibility to implement asynchron-
ous executions. There are tremendous ways for an app
to start asynchronous executions ( e.g., using AsyncTask ,
ThreadPoolExecutor , and IntentService ). Actually we
nd hundreds of classes or methods in the Android frame-
work that can start asynchronous executions.The implicit
ways to start asynchronous executions include, for example,
those via the customized classes that override the Android
framework classes such as AsyncTask orHandlerThread .
Figure 2 shows various ways of conducting asynchronous
executions. For a simple task to download Internet contents,
we could name at least 6 ways (including examples shown
in Figure 1 and Figure 2). Choosing which way generally
depends on the developer's own preference.
No matter how an asynchronous execution starts, it is
executed by the operating system (OS) via the thread mech-
anism so as to implement concurrency. However, Android
may start a new thread or reuse a running thread for the
asynchronous execution. As a result, dierent asynchronous
executions may share the same thread and run sequentially.
In other words, they may compete for the same execution
unit. Unfortunately, it is hard for the developer to be aware
of such dependency of asynchronous executions: She may
not know exactly how the asynchronous executions run.
The complex ways of starting asynchronous executions,
together with their complicated runtime dependency, make
it dicult for developers to comprehend the performance of
the UI event procedures they write. Performance issues are
hence hard to be eliminated without a proper tool. Next,
we will show a representative performance issue.
4113. A MOTIV ATING EXAMPLE
Synchronous part of a UI event procedure may cause laggy
UI, if it contains time-consuming codes [36, 63]. However,
addressing performance problems solely in the synchronous
part is far from enough. When a user is suering from lag-
gy UI, she is actually experiencing a long period of time
between her UI operation and its corresponding display up-
date. Even if asynchronous executions are introduced to
make the synchronous part completing quickly, she still ex-
periences laggy if the asynchronous executions are slow, and
consequently cause the slow intended display update.
As Android allows complex ways to start asynchronous ex-
ecutions, it may introduce various tricky performance issues.
The issues may lie in simple, widely adopted and seemingly-
correct codes. Next, we show a subtle performance issue
caused by unexpected sequentialized AsyncTask s.
Suppose an event procedure will show Internet contents
in three TextView s in an Activity . As the contents are in-
dependent, developers expect to download them in parallel.
They may instantiate three RetrieveDataTask objects (de-
ned in Figure 1) and invoke their execute methods which is
given as a usage example in the ocial guide [7], as follows.
(newÂ RetrieveInfoTask()).execute(url1); Â 
(newÂ RetrieveInfoTask()).execute(url2); Â 
(newÂ RetrieveInfoTask()).execute(url3); Â 
Â Â 
Â 
Â 
They may believe every RetrieveDataTask s will be exe-
cuted in separated threads, and hence calling their execute
methods makes them run in parallel. However, the codes
contain a subtle potential performance issue. In the recent
versions of Android, the execute method dened in the su-
perAsyncTask class will insert the corresponding tasks into
a global thread pool with capacity one. Thus all Retrieve-
DataTask s will be executed in sequence in one thread instead
of in parallel in multiple threads. This incurs more time to
complete the download tasks and to accordingly update the
UI. As a result, the user may experience a laggy UI.
Such a code defect can be resolved by calling execu-
teOnExecutor to customize a larger thread pool instead. It
is also worth noting that such sequential execution mecha-
nism is introduced in Android 3.0 or above. In the earlier
versions, the codes will run in parallel as expected. It is easy
for developers to neglect such changes and introduce poten-
tial performance issues. Such defects are common. Our ex-
perimental study nds eight such cases in real-world apps.
The above performance issue actually can hardly be tack-
led by current tools. StrictMode [63] and Asynchronizer [36]
consider only the synchronous part of a UI event procedure,
which cannot locate the issues caused by the asynchron-
ous executions. Other tools like Panappticon [72], Method
Tracing [54] can track such executions. But they largely
do not focus on the runtime dependency of asynchronous
executions. It is hard to nd out such dependency via exam-
ining the tremendous traces produced by these tools. Hence,
they are still not enough to cope with UI performance issues.
Fixing this gap is one major aim of DiagDroid .
4. UI PERFORMANCE DIAGNOSIS
We notice the key to pinpoint the above performance is-
sue is to know not only the execution time of an AsyncTask ,
but the time between when it is scheduled and when it s-
tarts to execute ( i.e., the queuing time), as well as the other
AsyncTask s that are in the same thread pool. Specically, an
unexpected long queuing time of an AsyncTask indicates a
AppÂ BinaryÂ 
Runtime DataTestExecute
Dependency â€awareÂ 
Performance Â 
DiagnosisDefects
Asynchronous
Execution â€based
ProfilingStatic
Analysis
Cases
AppÂ Info
Figure 3: DiagDroid Overview
performance issue. We can know there are too many Async-
Tasks in the pool. By examining which AsyncTask s are in
the pool and the pool capacity, we can locate the cause.
The above notion can also be applied to other mechanisms
that start asynchronous executions. This is the basis of the
DiagDroid design, which we overview in Figure 3. Diag-
Droid anatomizes the Android UI event procedures into a
set of tasks and then quantize them so that data analysis can
be conducted towards automating performance diagnosis.
Specically, as shown in Figure 3, DiagDroid rst per-
forms a light static analysis of the target app and obtains
some required information to assist runtime proling. It
then exercises the original app via a plugin testing approach
which can involve random test cases ( e.g., Monkey [67]) or
user-dened ones ( e.g., Robotium [59], UIAutomator [65]).
During the testing run, the proler can track asynchronous
executions, so as to anatomize the UI event procedures into
a set of tasks. The performance of the tasks, together with
their runtime dependency, can then be captured. Based on
the proling data, DiagDroid detects performance issues and
analyzes their causes. A report can nally be generated with
an aim to direct the debugging process.
To this end, we need to address several critical considera-
tions. Next, we will discuss the proling granularity of Di-
agDroid , and the required runtime data for modeling asyn-
chronous executions and their runtime dependency (in Sec-
tion 4.1). Then, we illustrate how such data can facilitate
UI performance diagnosis (in Section 4.2).
4.1 Modeling Tasks and Their Dependencies
As shown in Section 3, the subtle runtime dependency of
asynchronous tasks can result in tricky performance issues.
Analyzing such dependency is a key concern to DiagDroid .
We analyze the app runtime in tasklevel, dened as follows.
Denition 1. Anasynchronous task (or task) is a segment
of codes that run in sequence in a single thread. It denes
a developer-intended asynchronous execution process.
DiagDroid proles the app runtime in task granularity.
The reasons are as follows. First, it is good enough for
performance diagnosis to prole in such a granularity. A task
is a short segment of codes that can also be well understood
by its developer. If the developer can know which task is
anomalous, she can instantly reason its cause by inspecting
the task-related codes. Second, such a granularity will not
incur too much proling overhead, compared with the ner
granularity ( e.g., in method level or in line level). Most
importantly, proling app runtime in task granularity can
well capture the runtime dependency of two asynchronous
tasks. As a result, UI performance issues caused by such
dependency can be easily detected and located.
The task performance naturally reects the performance
of the entire UI event procedure: A slow task may result in a
slow UI event procedure, leading to a laggy UI. Next, we dis-
cuss modeling task performance. We are aware that a task
is possibly queued in an execution unit before it is executed.
As a result, the execution time as well as the queuing time
412main threadAsyncTask capacity: 1
Task 1Task 2Task 3
Task queuing
Task executing
Task scheduling
Task completiondefault
thread poolFigure 4: Asynchronous tasks runtime
in the unit should be considered in the performance model.
Denition 2. The queuing time of a task is the interval
between when the task is scheduled ( e.g., when execute is
called to start an AsyncTask ) and when it starts to execute.
We propose using both the queuing time and the execu-
tion time to model the task performance. Note that this is
generally dierent from the current diagnosis practice with
tools like TraceView and dumpsys [54]. It is hard, if not
infeasible, for these tools to obtain such data as they focus
only on the execution time of individual methods.
The queuing time of a task is inuenced by the other tasks
that may compete for the same execution unit. We formally
dene task runtime dependency as follows.
Denition 3. Two tasks bear execution dependency if 1)
they run in the same execution unit; and 2) one task is
scheduled in during the other task's queuing time.
As shown in Section 3, task runtime dependency is a crit-
ical factor that inuences the UI performance. We propose
employing three queue-related features, the queuing time ,
thepool capacity , and the queuing length of a task, to model
the task execution dependency.
Denition 4. The pool capacity of an execution unit is the
maximum tasks that the unit can simultaneously execute.
For example, for a thread the pool capacity is 1; while for
a thread pool the pool capacity is its size. Pool capacity
is usually set once and remains unchanged during runtime.
Supposektasks bear runtime dependency in an execution
unit with capacity Nandk>N .k-Ntasks have to wait for
execution in the unit. Then, when one task completes its
execution, one of the waiting tasks can be executed.
Denition 5. The queuing length of a task is the total
number of tasks waiting for execution in the execution unit
after it is scheduled.
A queuing length Lindicates that the task should wait for
the completion of other Ltasks before it can be executed.
Figure 4 illustrates how the three tasks discussed in Sec-
tion 3 are executed. The thread pool capacity of AsyncTask
is 1. When Task 2 is scheduled, it has to wait until Task 1
nishes its execution. So the queuing length of Task 2 is 1.
Similarly, the queuing length of Task 3 is 2.
The queuing time of a task reects how other tasks inu-
ence its performance. The queuing length and pool capacity
indicate the cause of a bad-performance task. Such informa-
tion can greatly help performance diagnosis. However, exist-
ing tools for performance diagnosis ( e.g., Panappticon [72],
Method Tracing [54]) cannot provide such information. As
a result, it is dicult for them to diagnose the subtle per-
formance issues caused by execution dependency.
We will elaborate how DiagDroid collects these runtime
data in Section 5. Next, we will rst discuss how DiagDroid
conducts performance diagnosis with the collected data.
4.2 Dependency-Aware Diagnosis
A laggy UI indicates that a UI event procedure requires
longer time to complete. As discussed, this can be rooted
in either the synchronous or the asynchronous executions.
Although many tools ( e.g., [36, 63]) have addressed the for-mer case, DiagDroid moves a step further by focusing on the
latter case, a far more dicult task in addressing the subtle
performance issues caused by the asynchronous executions.
If the asynchronous part is laggy, it means at least one
of the asynchronous tasks requires more time to complete.
Consequently, DiagDroid should detect performance anoma-
ly by checking whether there are any anomalous asynchron-
ous tasks. Human inspection of all the involved tasks is
prohibitively labor-intensive. DiagDroid requires an auto-
matic way to detect anomalous tasks. A possible approach
is to group the tasks in such a way that we can assume the
tasks in the same group have similar performance. Then we
can perform anomaly detection in a group basis.
But easy as it looks, how to group the tasks is challenging.
An instant way is to consider the method call-stack when a
task is scheduled. We name such call-stack the execution
context of the task. The execution context actually links
to the source codes that dene the task and how the codes
are reached. Two tasks with the same execution context
mean that they are corresponding to the same specic source
code segment and execution sequence. Hence, they should
naturally be grouped together.
But this simple consideration will result in extensive de-
bugging eorts. A code defect may manifest in similar tasks
with slightly-dierent execution contexts. Reporting all such
tasks one by one based on their execution contexts is very
tedious, and even makes the diagnosis dicult with such te-
dious information. For example, two UI event procedures
of two buttons may invoke the same buggy asynchronous
task (in the source codes). In these two cases, the two task
invocations have dierent execution contexts since they are
invoked by dierent event procedures. But we should group
them together to reduce human eorts for code inspection.
DiagDroid addresses the challenge by putting similar tasks
into a group with properly dened task similarity . By con-
sidering each method call as a symbol, an execution context
can be encoded into a vector. Then the dierence of two
tasks is the edit distance of their execution contexts. We
adopt such an edit distance as a similarity measure due to
following considerations. First, it is suitable to model the
dierences of two tasks. Consider the above example, if two
UI event procedures of two buttons invoke the same buggy
asynchronous task, the edit distance of execution contexts
of the two task invocations will be close. As a result, they
can be grouped together. Second, it considers the invoca-
tion order information, where such order is important to de-
scribe the app runtime. Consequently, two close execution
contexts indicate that the corresponding tasks are similar
during runtime. With such a similarity measure, DiagDroid
conducts the single-linkage clustering, a widely-adopted se-
quence clustering method, to form groups [24].
DiagDroid examines whether performance anomaly mani-
fests in each group of tasks with the execution context, queu-
ing time and execution time. DiagDroid considers both the
maximum value of queuing time and the execution time of
all tasks within each group A. The values are denoted by
Mq(A) =max(Q(a)) andMe(A) =max(E(a)) (8a2A),
Q(a) andE(a) are queuing time and execution time for task
arespectively. Mq(A) andMe(A) are the performance met-
rics of group A, since either a long execution time or a long
queuing time can result in anomalous performance.
DiagDroid considers a group is anomalous if one of its two
performance metrics is larger than a threshold . It ranks
413Table 1: Categories of asynchronous tasks
CategoryÂ  TypeÂ  Representative Â classes
ReusingÂ existingÂ 
threadsÂ LooperÂ &Â HandlerÂ HandlerThread Â 
IntentService Â 
Poolâ€basedÂ 
executorÂ ThreadPoolExecutor Â 
AsyncTask Â 
CreatingÂ newÂ threadsÂ ThreadÂ  ThreadÂ 
Â 
the anomalous groups according to their performance met-
rics as well as their execution contexts. Since each group is
corresponding to a specic source code segment, the rank
can direct the manual debugging eorts towards a suspi-
cious code segment that may cause the performance anoma-
ly. Moreover, a key consideration of DiagDroid is that the
runtime dependency of tasks may also cause performance is-
sues. In other words, the anomalous task per se is not always
the root cause of its poor performance. Especially, when the
queuing time of the task is too long, it is usually caused by
other tasks that bear runtime dependency. Therefore, Di-
agDroid also employs the performance data (the queuing
length and performance) of such tasks to locate the root
cause. We will show in our experimental study that the
localization approach can greatly save human eorts.
We consider maximum values instead of average values
as performance metrics. A large average means that many
tasks in the group perform poorly, hence is a good indicator
of performance issues. However, a small average for cases
that only a small portion of the tasks perform poorly, can
still be an important symptom of performance issues [20].
Since for the above two situations the maximum remains
large, we consider the maximum as performance metrics.
We have considered a performance threshold as an in-
dicator of poor-performance tasks. is selected empirically
based on the developers' consideration on laggy UI. Previous
work ( e.g., [47, 50, 60]) has suggested user-tolerable waiting
time in web browsing, mobile web browsing and mobile ser-
vices, which ranges from two to several seconds. One second
is considered as the limit for the user's ow of thought to
stay uninterrupted [49]. We regard that mobile app users
are more sensitive to UI response time. Thus, we use 500 ms
as the value of . We will show in our experimental study
in Section 6 that such a value is an eective choice.
5. PROFILING ASYNCHRONOUS TASKS
DiagDroid requires to prole the queuing time, the exe-
cution time, and the queuing length of a task, as well as the
pool capacity of an execution unit. Hence, DiagDroid must
rstly track the life-cycle of a task, i.e., when it is scheduled,
when it is executed, and when it completes.
However, there is no sole entry/exit points for hundreds of
ways to implicitly/explicitly start asynchronous tasks (men-
tioned in Section 2). It is dicult, if not infeasible, to design
specic proling mechanism for each. We attack this chal-
lenge with a separation-of-concerns approach. We rst clas-
sify the tremendous ways to start tasks into ve categories
(Section 5.1). Then we can specically track and prole the
necessary runtime data for each category (Section 5.2).
5.1 Categorizing Asynchronous Tasks
We notice that the underlying mechanisms for Android to
execute a task can be narrowed down into two approaches:
1) reusing existing threads created beforehand, and 2) cre-
ating a new thread. The former case can be further dividedinto two types: One directly schedules a task ( Pool-based
Executor mechanism) and the other requests the schedul-
ing of a task by a delegate via sending a message ( Looper
&Handler mechanism). We list them in Table 1, together
with their representative classes in Android.
Both HandlerThread and IntentService depend on the
Looper & Handler mechanism to start tasks. They create a
worker thread and wait for new tasks to the looper associated
with the thread. The request of scheduling a task is sent via
ahandler attached to the looper. The requested task will
then wait to be processed in the worker thread. Since there
is only one worker thread, it can process one message at a
time. Other requests should wait in a queue.
ThreadPoolExecutor and AsyncTask both use the Pool-
based Executor mechanism. They maintain a pool of worker
threads with its size not exceeding a preset capacity. A new
coming task will be executed in one thread in the pool if
there are available threads ( i.e., the number of busy threads
is smaller than the capacity). Otherwise, the task has to
wait for an available thread.
Thread mechanism starts a task immediately in a new
thread. Its building basis, i.e., the Thread class in Android,
is the same as the traditional Java one.
The underlying mechanisms of numerous ways to implicit-
ly or explicitly start asynchronous tasks are based on these
ve representative classes. For example, the AsyncQuery-
Handler class, which conveniently queries data from a con-
tent provider, is based on HandlerThread . The CursorLoad-
erclass, which acquires data from the database, is based on
AsyncTask . Moreover, the DownloadManager mentioned in
Section 2 employs the ThreadPoolExecutor .
Each of the ve classes for conducting asynchronous tasks
has its pros and cons. The Thread class is exible which
enables fully control on the threading mechanism, while
more management eorts are required. Moreover, creat-
ing a new thread per task consumes system resources. The
HandlerThread class requires many development eorts to
customize both the background threads and the handler for
the tasks. The ThreadPoolExecutor class, as a tradition-
al JAVA class for multi-threading, is widely used to man-
age a pool of worker threads. However, comparing to the
AsyncTask class, it is unsuitable for tasks updating UI since
Android prohibits UI updating in worker threads. The In-
tentService class can start a background service indepen-
dent of the activity life cycle. It is a relatively heavier con-
tainer which requires more system resources on execution.
Choosing which way depends on the specic programming
requirements, as well as the developer's preference.
5.2 Proï¬ling Asynchronous Tasks
DiagDroid tracks tasks with a dynamic instrumentation
mechanism on Android framework methods. It requires no
changes to the target app, or recompiling the underlying
OS and the Android framework. This can guarantee the
compatibility of DiagDroid with diverse Android versions
and smartphone models. Moreover, it requires little human
eorts in installing and applying the tool.
Specically, unlike general Linux processes, Android pro-
cesses of its apps are all created by duplicating a system
process called Zygote [18]. Android framework functionali-
ties have already been loaded in Zygote before such dupli-
cation. Therefore, we can instrument the Zygote process
and \hijack" the Android framework methods of interest be-
414forehand. Then when an app runs by forking Zygote , the
method invocations are inherently hijacked by DiagDroid .
Hence, we can easily track methods. We adopt such a mech-
anism implemented in the tool named Xposed [70], usually
used for modifying UI [69]. We program our own codes to
hijack the methods of our interest. Next we introduce how
DiagDroid tracks tasks in each category.
1)Thread : An asynchronous task that implements as a
thread always starts with the start method. Hence, we
can instantly obtain the time when it is scheduled by track-
ing the start method. However, the task is executed in the
overridden runmethod of a Runnable object, which is an ab-
stract method that cannot be instrumented directly. Hence,
we resort to static analysis to nd the implementations of
the abstract runmethod and instrument them instead.
The static analysis is performed via the tool apktool [6].
It decompiles the binary into well-structured Dalvik byte-
code. They can be parsed to obtain the implementations
of the abstract runmethod, which can direct our dynam-
ic instrumentation approach to obtain the execution time.
Note that DiagDroid only decompiles and discovers these
methods, instead of modifying and recompiling the app.
2)HandlerThread :HandlerThread is a thread that pro-
vides a Looper object attached to it. A Handler is associ-
ated with the Looper object and handles messages for the
Looper . Hence, we can obtain the request time of a task
by tracking the time when a Message object is sent to the
Handler . Since eventually sendMessageAtTime orsendMes-
sageAtFrontOfQueue must be invoked to send a Message , we
record the invocation time of these two framework methods
as the time when a task is scheduled. Handler performs the
task by processing its corresponding Message , we track task
execution by instrumenting its dispatchMessage method.
3)IntentService : An IntentService task always starts
by invoking the startService method of the framework
class ContextImpl . Hence, the invocation time of this me-
thod is the task scheduling time. IntentService actual-
ly relies on the ServiceHandler class, which inherits from
Hanlder , to process the task. Hence, we track task execution
by tracking the dispatchMessage method of Handler .
4)ThreadPoolExecutor :ThreadPoolExecutor is a pool-
based execution mechanism which has an elegant pattern. A
task is always requested via invoking the execute method.
Moreover, the task always starts immediately after the be-
foreExecute method, and is followed by the afterExecute
method. Hence, we track tasks via these methods.
5)AsyncTask : A task can base its implementation on the
complicated Java class inheritance of the basis AsyncTask
class. However, it is always scheduled by the execute or
executeOnExecutor methods eventually, regardless of class
inheritance layers. Hence, the task scheduling time is the
invocation time of the two methods. For both cases, Async-
Task actually relies on the ThreadPoolExecutor . Hence, we
track task execution similar to that of ThreadPoolExecutor .
For the tasks in categories 2-5, they are put in a queue be-
fore executed. To model their task runtime dependency, we
use the hash code of the execution unit ( e.g.,ThreadPoolEx-
ecutor object) as the queue identier . Such a hash code is
easy to obtain during runtime according to Java specics.
Two tasks with the same queue identier may bear run-
time dependency. Finally, the pool capacity is obtained via
checking some internal elds of the queue object ( e.g., the
maximumPoolSize eld of a ThreadPoolExecutor object).Table 2: Representative performance issues found
(Rank: ranking of the buggy issue / total # issues reported)
Category Issue description Class@App Rank
Not awaring AsyncTask.execute() method
results in undesired seqential executionLawListFragment@OpenLaw 1/7
Loading tens of icons in sequence AppListArrayAdapter@AFWall+ 1/3
Improper cancelation of asynchronous tasks GetRouteFareTask@Bart RunnerAndroid 1/4
Not canceling obsolete queries when new
query arrivesAsyncQueryTripsTask@Liberario 2/2
Failed to set optimal size of the thread pool ZLAndroidImageLoad er@FBReader 1/2
Use the same pool for loading app list and
app iconsMainActivty@AFWall+ 2/3
Posting various types of tasks (e.g., update
progress, store book) to the samebackgroundHandlerReadingFragment@PageTurner 3/9
Executing Filter method of AutoComplete-
TextView occupies the Handler of a publicmessage queueLocationAdapter@Liberario 1/2
Not canceling the tasks implemented by
third-party library, Android asynchronoushttp client - loopjHeadlineComposerAdapter@OpenLaw 7/7
Use the deprecated findall method of
WebView class which causes blockingMainActivity@Lucid Browser 1/5Misusing
third-partylibrarySequential
execution
Forgetting
cancelingexecution
Improper
threadpool
Overloading
messagequeue
6. EXPERIMENTAL STUDY
We have implemented DiagDroid and released the project
online [19]. In our experimental study, we target on open-
source apps since we need the source codes to verify the
eectiveness. To this end, we download such apps from F-
Droid, an app market that hosts only free and open-source
Android apps [21]. It is also a popular app source for the
research community [42]. We employ Monkey [67], the o-
cial random testing tool, to exercise our target apps. It is
also known as the most ecient tool in terms of code cov-
erage [17]. Among the apps we download, we exclude those
that require a login account for convenience consideration
(so that we do not bother to register new accounts). Note
that DiagDroid can easily handle such apps by applying a
login script, which is trivial and will not inuence the eec-
tiveness. We thus get 33 target apps covering diverse cate-
gories including Reading ,Multimedia ,Science &Education ,
Navigation ,Security and Internet .
We verify the compatibility of DiagDroid on four smart-
phone models covering a wide range of device capacities:
Samsung GT-I9105P (Android 4.1.2), Huawei G610-T11 (An-
droid 4.2.2), Huawei U9508 (Android 4.2.2), and Lenovo
K50-T5 (Android 5.1). Experiments are conducted on the
four devices simultaneously to save time. We also conduct
stress tests by injecting loads on CPU, memory, sdcard IO
and network respectively with customized Android back-
ground services. We implement ve background services
occupying 80% CPU, ve background services occupying
80% memory, ve background services consuming 2 Inter-
net downloading threads each, two background services each
reading 1 le and writing 1 le on sdcard in separate threads.
The parameters are chosen by common practice. Develop-
ers could congure with their own preferences. We design
a system app to guarantee that these services would run
persistantly (i.e., they will not be terminated by LowMem-
oryKiller [40]). Four devices with ve congurations each
(four with load injections and one without load injection)
come up to 20 test congurations, each conguration is un-
der Monkey testing for 30 minutes. We run 19 ;800-minute
test in total for the 33 apps. DiagDroid reports overall 48
performance issues marked as highly suspicious for 14 apps,
on average 3 :4 issues per app. The reports are published on-
line. Via inspecting the related source codes in the reports
415 
1.Â Â asynchronous Â tasksÂ withÂ contextÂ c1Â Â 
2.Â Â  Â  maxÂ queuingÂ time:Â 1650msÂ 
3.Â Â  Â  poolÂ capacity:Â 1Â 
4.Â Â  Â  casesÂ withÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 1.00Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 1666.00ms Â 
7.Â Â  Â  runtimeÂ dependency: Â c2Â 
 
 
 
    ContextÂ c
1Â 
ClassÂ name:Â  de.jdsoft.law.data.LawSectionList
Callâ€stack:Â  android.os.AsyncTask.executeOnExecutor Â (NativeÂ Method)
Â  android.os.AsyncTask.execute Â (AsyncTask.java:535)
Â  de.jdsoft.law.LawListFragment.onCreate Â (LawListFragment.java:87)
Â  â€¦Â 
ContextÂ c2Â 
ClassÂ name:Â  de.jdsoft.law.data. Â UpdateLawList
Callâ€stack:Â  android.os.AsyncTask.executeOnExecutor Â (NativeÂ Method)
Â  android.os.AsyncTask.execute Â (AsyncTask.java:535)
Â  de.jdsoft.law.LawListFragment.onCreate Â (LawListFragment.java:91)
Â  â€¦Â Â 
publicÂ voidÂ onCreate(Bundle Â savedInstanceState) Â {Â 
Â 
//Â LoadÂ actualÂ listÂ 
finalÂ LawSectionList Â sectionDB Â =Â newÂ LawSectionList(LawSectionList.TYPE_ALL); Â 
Â 
sectionDB.execute(adapter); Â 
//Â AndÂ parallelÂ updateÂ theÂ listÂ fromÂ networkÂ 
UpdateLawList Â updaterÂ =Â newÂ UpdateLawList(); Â 
updater.execute(adapter); Â 
Â Â }Â 
Â â€¦Â 
â€¦Â 
â€¦Â 
 
1.Â Â asynchronous Â tasksÂ withÂ contextÂ c1Â Â 
2.Â Â  Â  maxÂ queuingÂ time:Â 1650msÂ 
3.Â Â  Â  poolÂ capacity:Â 1Â 
4.Â Â  Â  casesÂ withÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 1.00Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 1666.00ms Â 
7.Â Â  Â  runtimeÂ dependency: Â c2Â 
 
 
 
    ContextÂ c
1Â 
ClassÂ name:Â  de.jdsoft.law.data. Â UpdateLawList
Callâ€stack:Â  android.os.AsyncTask.executeOnExecutor Â (NativeÂ Method)
Â  android.os.AsyncTask.execute Â (AsyncTask.java:535)
Â  de.jdsoft.law.LawListFragment.onCreate Â (LawListFragment.java:91)
Â  â€¦Â 
ContextÂ c2Â 
ClassÂ name:Â  de.jdsoft.law.data.LawSectionList Â 
Callâ€stack:Â  android.os.AsyncTask.executeOnExecutor Â (NativeÂ Method) Â 
Â  android.os.AsyncTask.execute Â (AsyncTask.java:535) Â 
Â  de.jdsoft.law.LawListFragment.onCreate Â (LawListFragment.java:87) Â 
Â  â€¦Â Â Â 
publicÂ voidÂ onCreate(Bundle Â savedInstanceState) Â {Â 
Â 
//Â LoadÂ actualÂ listÂ 
finalÂ LawSectionList Â sectionDB Â =Â newÂ LawSectionList(LawSectionList.TYPE_ALL); Â 
Â 
sectionDB.execute(adapter); Â 
//Â AndÂ parallelÂ updateÂ theÂ listÂ fromÂ networkÂ 
UpdateLawList Â updaterÂ =Â newÂ UpdateLawList(); Â 
updater.execute(adapter); Â 
Â Â }Â 
Â â€¦Â 
â€¦Â 
â€¦Â Report
Â 
 
1.Â Â asynchronous Â tasksÂ withÂ contextÂ c1Â Â 
2.Â Â  Â  maxÂ queuingÂ time:Â 1650msÂ 
3.Â Â  Â  poolÂ capacity:Â 1Â 
4.Â Â  Â  casesÂ withÂ queuingÂ timeÂ <Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 0.00Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 0.00msÂ 
7.Â Â  Â  casesÂ withÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
8.Â Â  Â  Â  avg.Â queueÂ length:Â 1.00Â 
9.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 1666.00ms Â 
10.Â Â Â  runtimeÂ execution Â dependency: Â c2Â 
 
 
 
    ContextÂ c
1Â 
ClassÂ name:Â  de.jdsoft.law.data.LawSectionList
Callâ€stack:Â  android.os.AsyncTask.executeOnExecutor Â (NativeÂ Method)
Â  android.os.AsyncTask.execute Â (AsyncTask.java:535)
Â  de.jdsoft.law.LawListFragment.onCreate Â (LawListFragment.java:87)
Â  â€¦Â 
ContextÂ c2Â 
ClassÂ name:Â  de.jdsoft.law.data. Â UpdateLawList
Callâ€stack:Â  android.os.AsyncTask.executeOnExecutor Â (NativeÂ Method)
Â  android.os.AsyncTask.execute Â (AsyncTask.java:535)
Â  de.jdsoft.law.LawListFragment.onCreate Â (LawListFragment.java:91)
Â  â€¦Â Â 
publicÂ voidÂ onCreate(Bundle Â savedInstanceState) Â {Â 
Â 
//Â LoadÂ actualÂ listÂ 
finalÂ LawSectionList Â sectionDB Â =Â newÂ LawSectionList(LawSectionList.TYPE_ALL); Â 
Â 
sectionDB.execute(adapter); Â 
//Â AndÂ parallelÂ updateÂ theÂ listÂ fromÂ networkÂ 
UpdateLawList Â updaterÂ =Â newÂ UpdateLawList(); Â 
updater.execute(adapter); Â 
Â Â }Â 
Â â€¦Â 
â€¦Â 
â€¦Â 
Codes
Â 
Figure 5: Report and code segments of case 1
for several minutes per case and understanding the original
project, we surprisingly nd 14 of the target apps contain 27
performance issues. The bug cases are ranked highly in the
report, (with an average rank of 1.7). Although unfamiliar
with the target app design, we nd it very convenient for us
to pinpoint the root causes of the issues.
We categorize 27 detected issues into 5 categories. Ten
representative issues are shown in Table 2, with their caus-
es, defect locations and rankings in the reports. We have
reported the issues to app developers, many of which have
been conrmed and corrected accordingly. We have got pos-
itive feedbacks like \for faster search results", \I've modied
and I see the performance improvements." after developers
x the issues. Next, we elaborate our experiences of perfor-
mance diagnosis via ve representative cases.
6.1 Case Studies
Case 1: Unwanted Sequential Executions
We provide our experiences on diagnosing OpenLaw [52],
which provides access to over 6,000 laws and regulations.
DiagDroid reports 7 highly suspicious issues. After half an
hour inspecting of related source codes according to the re-
port, we summarize 2 performance issues and localize the
causes. One case is that the queuing time of the task group
with context c1is longer than the threshold 500 ms. This
case is found in 14 test congurations, mostly under those
with heavy CPU or sdcard IO load, which indicates the case
may be related to some IO intensive operations. We demon-
strate the content of DiagDroid report in Figure 5 (Line
numbers are added for discussion convenience).
We instantly nd that such a long queuing time is because
the task should wait in queue till the completion of other
long executing tasks based on Lines 4-7. We know on average
onetask (Line 5) bears runtime execution dependency with
an anomalous task, of which the context is c2(c2task for
short) (Line 7). In other words, anomalous c1tasks are due
1.Â Â asynchronous Â tasksÂ withÂ contextÂ c1:Â 
2.Â Â  Â  maxÂ queuingÂ time:Â 31885msÂ 
3.Â Â  Â  poolÂ capacity:Â 1Â 
4.Â Â  Â  casesÂ ofÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 19.50Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 1240.60ms Â 
7.Â Â  Â  runtimeÂ dependency: Â c1,Â c2Â 
8.Â Â Â Â  execution Â timeÂ ofÂ c1Â Â max:Â 19337.00ms Â Â avg.Â 1419.95ms Â 
9.Â Â Â Â  execution Â timeÂ ofÂ c2Â Â max:Â 3446.00ms Â Â Â avg.Â 786.69ms Â 
 
 
  
 
protected Â StringÂ doInBackground(Params... Â paramsArray) Â {Â 
ParamsÂ paramsÂ =Â paramsArray[0]; Â 
Â Â Â ifÂ (!isCancelled()) Â {Â 
Â Â Â Â Â  returnÂ getFareFromNetwork(params, Â 0);Â 
Â Â }Â elseÂ {Â 
Â Â Â Â Â Â  returnÂ null;Â 
Â Â Â }Â 
}Â 
 Report
Â 
1.Â Â asynchronous Â tasksÂ withÂ contextÂ c1:Â 
2.Â Â  Â  maxÂ queuingÂ time:Â 31885msÂ 
3.Â Â  Â  poolÂ capacity:Â 1Â 
4.Â Â  Â  casesÂ ofÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 19.50Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 1240.60ms Â 
7.Â Â  Â  runtimeÂ dependency: Â c1,Â c2 
 
  
 
protected Â StringÂ doInBackground(Params... Â paramsArray) Â {Â 
ParamsÂ paramsÂ =Â paramsArray[0]; Â 
Â Â Â ifÂ (!isCancelled()) Â {Â 
Â Â Â Â Â  returnÂ getFareFromNetwork(params, Â 0);Â 
Â Â }Â elseÂ {Â 
Â Â Â Â Â Â  returnÂ null;Â 
Â Â Â }Â 
}Â 
 
Codes
Â 
Figure 6: Report and code segments of case 2
to the heavy-weighted c2tasks. The pool capacity is only 1
(Line 3), which indicates the tasks have to run in sequence.
Waiting for the completion of another heavy-weighted task
should generally be avoided via proper scheduling.
Thec1andc2contexts are included in the report (Fig-
ure 5). We can conveniently nd the related source codes
and how they are scheduled. The comment\parallel update"
indicates developers intend to execute the two tasks in paral-
lel. But we notice this is the same mistake in Section 3. The
x is to call executeOnExecutor with a larger pool instead.
Usually, developers wrongly assume the availability of the
execution unit during task scheduling. Even worse, such
sequential tasks may be dened and scheduled across sev-
eral source les, making it harder to capture their depen-
dency manually. In the example, OpenLaw handles tens of
UI events. It is hence dicult to manually test and detect
performance issues in all UI event procedures. Even if devel-
opers notice the laggy UI procedure ( e.g., loading LawList-
Activity ), they can hardly pinpoint the defect by inspecting
nearly 300 hundred lines of codes in several les, which even
involves complicated third-party library invocations.
Existing tools [54, 72] focus on the execution time of meth-
ods. Generally, they lack the capability to model the queu-
ing time of tasks and to identify the execution dependency.
Therefore developers can hardly detect the subtle symptoms
and reason the defect caused by task dependency. Note that
the method tracing-based tools will generate a trace of thou-
sands of methods for a UI event procedure. The performance
diagnosis based on such data is like nding a needle in the
hay stack, given the fact that Method Tracing will produce
about 36GB of data for our 600-minute testing run per app.
In contrast, DiagDroid properly models the task execu-
tion dependency, and provides tidy but helpful information
to guide the diagnosis process. We show that it can great-
ly reduce the human eorts by directing the developer to
several lines of codes that cause the performance issue.
Case 2: Not Canceling Obsolete Tasks
The cancelation of a time-consuming task is necessary
when the task is no longer required. For example, when a
user performs activity-switching by a sliding operation, the
previous content downloading task becomes obsolete since
its associated activity is invisible. Obsolete tasks occupy re-
sources ( e.g., Internet bandwidth), and therefore deteriorate
the UI performance. Sometimes, they even block other tasks
by occupying the thread pool. However, canceling obsolete
tasks is not obligatory and hence often neglected by develop-
4161.Â Â asynchronous Â tasksÂ withÂ contextÂ c1:Â 
2.Â Â  maxÂ queuingÂ time:Â 514msÂ 
3.Â Â  Â  poolÂ capacity:Â 3Â 
4.Â Â  Â  casesÂ ofÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 1.50Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 659.29ms Â 
7.Â Â  Â  runtimeÂ dependency: Â c1Â 
 
  
privateÂ staticÂ finalÂ intÂ IMAGE_LOADING_THREADS_NUMBER Â =Â 3;//TODO: Â howÂ manyÂ threads?Â 
privateÂ finalÂ ExecutorService Â myPoolÂ =Â Executors.newFixedThreadPool( Â 
IMAGE_LOADING_THREADS_NUMBER, Â newÂ MinPriorityThreadFactory()); Â 
Â voidÂ startImageLoading( Â  ){Â 
finalÂ ExecutorService Â poolÂ =Â Â 
image.sourceType() Â ==Â ZLImageProxy.SourceType.FILE Â ?Â mySinglePool Â :Â myPool;Â 
pool.execute( Â Â  );Â 
}Â 
 â€¦â€¦Â â€¦Â Report
Â 
1.Â Â asynchronous Â tasksÂ withÂ contextÂ c1:Â 
2.Â Â  maxÂ queuingÂ time:Â 514msÂ 
3.Â Â  Â  poolÂ capacity:Â 3Â 
4.Â Â  Â  casesÂ ofÂ queuingÂ timeÂ <Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 0Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 0.00msÂ 
7.Â Â  Â  casesÂ ofÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
8.Â Â  Â  Â  avg.Â queueÂ length:Â 1.50Â 
9.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 659.29ms Â 
10.Â Â Â  runtimeÂ execution Â dependency: Â c1Â 
 
  
privateÂ staticÂ finalÂ intÂ IMAGE_LOADING_THREADS_NUMBER Â =Â 3;//TODO: Â howÂ manyÂ threads?Â 
privateÂ finalÂ ExecutorService Â myPoolÂ =Â Executors.newFixedThreadPool( Â 
IMAGE_LOADING_THREADS_NUMBER, Â newÂ MinPriorityThreadFactory()); Â 
Â voidÂ startImageLoading( Â  ){Â 
finalÂ ExecutorService Â poolÂ =Â Â 
image.sourceType() Â ==Â ZLImageProxy.SourceType.FILE Â ?Â mySinglePool Â :Â myPool;Â 
pool.execute( Â Â  );Â 
}Â 
 â€¦â€¦Â â€¦Â 
Codes
Â 
Figure 7: Report and code segments of case 3
ers. we reveal that many popular apps contain performance
issues caused by not canceling obsolete tasks.
DiagDroid nds 5 performance issues for BartRunnerAn-
droid , a public transport app. We can conveniently locate 5
bugs in the source codes with the report. Specically, we de-
tect the anomalous c1tasks under all 20 test congurations.
The corresponding content of the report is demonstrated in
Figure 6. Similar to Case 1, we can instantly nd the queu-
ing eect caused by execution dependency (Lines 4 to 7).
We can simply apply a x similar to Case 1 here, i.e., by
allowing tasks to run in separate execution units. However,
we notice both c1andc2tasks can be executed for a long
period (Lines 8-9), and they often block each other. By in-
specting the codes, we nd developers have already intended
to cancel the obsolete tasks. c1tasks inherit from GetRoute-
FareTask , whose source codes for executing are shown in
Figure 6. The cancelation checking is done before the task
begins, thus the task will not be canceled during execution
when it is obsolete. Similar analysis could be applied to
c2. In other words, the developers fail to conduct proper
cancelation steps via cancelation checking.
The correct cancelation involves 2 steps: 1) invoke cancel
method in onStop method of the container Activity , and 2)
periodically check isCancelled method in doInBackground
method, release the resource when true. Note that releasing
the resource in onCancel method is also a common mistake.
Case 3: Improper Thread Pool Size
Improper thread pool size is a common cause of long task
queuing since the pool is often busy. We show how to diag-
nose such defects on FBReader [22], a popular e-book reader.
DiagDroid reports only one issue for this app (Figure 7).
We detect this case under all 20 test congurations. We
pinpoint the issue in source codes in 20 minutes. Both the
execution and queuing time of c1tasks are anomalous. We
can quickly conrm that the image loading tasks reasonably
execute for long. However, it is undesired that a c1task
has to wait long for other c1tasks to complete (Lines 4-7).
Unlike Case 2, c1tasks cannot be canceled since images are
loading simultaneously. Hence, a quick x is to set a larger
pool size (5 may be good according to the report).
Properly setting a pool size is often hard during the coding
phase. We nd four such cases in our experiment. Devel-
opers are hard to predict the possible number of concurrent
tasks in the same pool. For example, in this case study, de-
velopers are not sure about the proper pool size, and hence
put down a to-do comment in the source (Figure 7).
Finally, note that the existing approaches [54, 72] can not
1.Â Â asynchronous Â tasksÂ withÂ contextÂ c1:Â 
2.Â Â  Â  maxÂ queuingÂ time:Â 6138msÂ 
3.Â Â  Â  poolÂ capacity:Â 1Â 
4.Â Â  Â  casesÂ ofÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 1.38Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 3274.31ms Â 
7.Â Â  Â  runtimeÂ dependency: Â c1Â 
 
  
//Â ThisÂ methodÂ couldÂ beÂ optimized Â aÂ lot,Â butÂ heyÂ processors Â areÂ fastÂ nowadays Â 
protected Â FilterResults Â performFiltering( Â  )Â {Â 
Â resultListÂ =Â autocomplete.execute(). get().getLocations(); Â 
Â Â }Â â€¦Â 
â€¦Â â€¦Report
Â 
Figure 8: Report of case 4
1.Â Â asynchronous Â tasksÂ withÂ contextÂ c1:Â 
2.Â Â  Â  maxÂ queuingÂ time:Â 664msÂ 
3.Â Â  Â  poolÂ capacity:Â 1Â 
4.Â Â  Â  casesÂ ofÂ queuingÂ timeÂ â‰¥Â 500ms:Â 
5.Â Â  Â  Â  avg.Â queueÂ length:Â 3.00Â 
6.Â Â  Â  Â  avg.Â execution Â timeÂ ofÂ theÂ inâ€queueÂ tasks:Â 323.00ms Â 
7.Â Â  Â  runtimeÂ dependency: Â c1Â 
 
 
publicÂ voidÂ onTextChanged( Â Â Â Â  )Â {Â 
WV.findAll(s.toString()); Â 
}Â 
Â â€¦Â 
Report
Â 
Figure 9: Report of case 5
identify issues caused by defects in Cases 2 and 3, since
these approaches focus only on the execution time. Even
if developers notice the bug symptoms, existing approaches
lack a way to automatically analyze the runtime dependency
of tasks. As a result, it requires daunting manual eorts to
nd that a task sometimes has to wait for other tasks by the
inspection of tremendous runtime traces.
Case 4: Overloading Message Queue
Message handler is also a queue-based execution unit with
only one thread. If messages come from separate UI oper-
ations ( i.e., continuous text inputs) too quickly, a message
should wait for processing previous messages. This type of
issues can also be easily pinpointed with DiagDroid . We nd
two such cases and take the Transportr (a public transport
app) case as an example. DiagDroid reports two highly sus-
picious issues, from which we nd two code defects in less
than an hour. Developers x the cases accordingly.
We can see from the report that c1tasks are with anoma-
lous queuing time and execution time (Figure 8). This prob-
lem is detected under all 20 test congurations. A c1task
has to wait long for other c1tasks to complete (Lines 4-7),
while the long execution time indicates that a c1task may
long occupy the handler. Via inspecting the source codes of
c1, we can easily nd the reason. The c1task is a Handler for
ltering input text, which invokes performFiltering direct-
ly. The method involves a time-consuming Internet query
on requesting a list of suggested locations with an incom-
plete input. Consequently, the upcoming messages will have
to queue in the message queue for previous inputs. Actually
the old queries are no longer useful, and hence should be
canceled when processing new messages.
Note we can know the long execution time with existing
tool [54]. But we may falsely accept the time-consuming In-
ternet query. Moreover, Panappticon [72], an event tracing
tool to identify critical execution paths in user transactions,
is unaware of the dependency between tasks invoked in dif-
ferent UI operations. In this case, tasks bearing execution
dependency are invoked by independent text inputs in the
AutoCompleteTextView . It is hard to know how a long exe-
cution time inuences other tasks, without a tool like Diag-
Droid to model the dependency of the concurrent tasks.
Case 5: Misusing Third-Party Library
Misusing third-party libraries is also a source of perfor-
mance issues. Without knowing the implementation details,
developers misunderstand the usage of the third-party li-
brary and introduce performance issues. For example, un-
aware of the asynchronous tasks in a third-party library, the
417 Context c 1 Context c 2 Context c 3
1. de.jdsoft.law.data.UpdateLawList de.jdsoft.law.data.UpdateLa wList de.jdsoft.law.data.UpdateLawList 
2. android.os.AsyncTask.executeOnEx
ecutor(Native Method) android.os.AsyncTask.executeOn
Executor(Native Method) android.os.AsyncTask.executeOn
Executor(<Xposed>) 
3. android.os.AsyncTask.execute(Asyn
cTask.java:534) android.os.AsyncTask.execute(As
yncTask.java:534) android.os.AsyncTask.execute(As
yncTask.java:539) 
4. de.jdsoft.law.LawListFragment.onCr
eate(LawListFragment.java:91) de.jdsoft.law.LawListFragment.on
Create(LawListFragment.java:91)de.jdsoft.law.LawListFragment.on
Create(LawListFragment.java:91) 
 â€¦  â€¦  â€¦  
9. android.support.v4.app.FragmentAct
ivity.onCreateView(FragmentActivit
y.java:285) android.view.LayoutInflater.creat
eViewFromTag(LayoutInflater.jav
a:676) android.view.LayoutInflater.creat
eViewFromTag(LayoutInflater.jav
a:727) 
 â€¦  â€¦  â€¦  
 Figure 10: Similar contexts without clustering
developer will neglect to cancel obsolete tasks. DiagDroid
can also save the eorts in troubleshooting such defects. We
detect three such cases and show the defect in Lucid Brows-
er, a web browser app, as an example. We infer this issue
from the 3 suspicious issues reported by DiagDroid .
We can know this is an unintended sequential execution
case similar to Case 1 (Lines 3-7, Figure 9). Checking the
source codes related to c1, we can locate the invocation of
thefindAll method in the third-party library WebView . It
nds the occurrences of a specic text in a webpage when
the text is changed. Revisiting findAll , we can nd that it
is deprecated, and should be replaced by findAllAsync .
Note that the case is only detected on 3 devices other than
Lenovo K50-T5 (Android 5.1). findAll does not introduce
performance issues in Android versions above 4.4 because
theChromium -based WebView replace the Webkit -based Web-
View. Panappticon [72] requires to recompile the kernel, can
work only on a small set of devices can not cope with such
defects that do not persist in all Android versions [12].
6.2 Why Clustering
As mentioned in Section 4.2, to reduce the number of re-
porting suspicious cases, we cluster the execution contexts
(i.e., call-stacks) belonging to the same asynchronous task
triggering by similar running sequences. First we extract two
features via scanning through massive contexts: 1) similar
call-stacks are similar in line level, and 2) similarity of call-
stacks is transitive. Then we perform clustering accordingly.
In this section, we show that the clustering is necessary and
eective with a randomly selected example of app OpenLaw .
ForOpenLaw , there are totally 1462 distinct contexts found
under all 20 test congurations. As a result, 226 suspicious
performance cases are reported. However, we nd many
of the reported cases are with similar contexts. We select
three similar contexts as examples in Figure 10. Actually,
the three contexts refer to the same asynchronous task Up-
dateLawList presented as context c1in Case study 1. They
are the same until the 9th line of the call-stacks; more specif-
ically, they have slight dierence in low-level VM processing
sequences. There is only one performance issue instead of
three in developers' viewpoint. To lighten the workload of
developers, the three contexts should be grouped into one.
Considering the aforementioned features, we cluster con-
texts with a customized edit distance (feature 1) plus single-
linkage strategy (feature 2). After the clustering, we success-
fully reduce the amount of total contexts from 1462 to 75
(groups). Moreover, only 7 performance cases are reported
without losing meaningful cases. This result indicates the
eectiveness of our clustering mechanism.
6.3 Performance Enhancement
DiagDroid is able to present to developers with the per-
formance enhancement after xing performance issues. Di-
Figure 11: Message handler blocking delays before
(left) and after (right) x of Transportr
Figure 12: Queuing delay of showing apps before
(left) and after (right) x of AFWall+
agDroid oers the distribution of the queuing & execution
delays of asynchronous executions . Besides conrming the
disappearing of the related case in the report of the xed ver-
sion, developers can ensure the performance gain via double-
checking the delay distributions of related asynchronous ex-
ecutions before and after xing the issue. Next, we illustrate
how to visualize the performance enhancement via demon-
strating two ocial xes by developers. Notice the exam-
ple distributions are simplied (yet good enough) to demon-
strate the enhancement. Developers could tune the param-
eter to obtain ner distributions.
Developers of Transportr x an issue of message queue
overloading with our report. They modify 11 les with 364
additions and 274 deletions. Since the message processing
is network related, we show the performance enhancement
on the Huawei G610-T11 with network load injected. The
result is depicted in Figure 11. Similar patterns can be found
in other test congurations. It could be seen that there is
no more blocking problem for the new app version.
With our report, developers of AFWall+ x the sequential
loading issue on displaying the app list. They modify the
source from (new GetAppList()).setContext(this).exe-
cute() to(new GetAppList()).setContext(this).execu-
teOnExecutor(AsyncTask.THREAD_POOL_EXECUTOR) . We no-
tice that the queuing eect is more obvious with CPU load
injected on a poorer device. We illustrate in Figure 12 the
distribution of queuing delay of the AsyncTask with test-
ing conguration of Samsung GT-I9105P with CPU load
injected. Similar patterns can be found in other test con-
gurations. It could be seen that there is no more queuing
problem in the new app version.
6.4 Discussions
Next, we discuss the threats to the validity of our experi-
ments, and the measures we take to address them. First is
the overhead of DiagDroid ,i.e., how it aects the test e-
ciency. We employ the time command to obtain the CPU
time for conducting 10 ;000 Monkey operations (200 msin-
terval between two consecutive operations) with DiagDroid
on and o. The 0 :8% overhead shows that DiagDroid does
not have a considerable impact on testing eciency.
Instead of relying solely on Monkey , test executor is a plug-
in in DiagDroid . Incorporating script-based testing tools
likeUIAutomator [65] and Monkey runner [45] is allowed.
We test 30 minutes for each of 20 congurations per app to
show the capability of DiagDroid in such a short-term test.
The settings can be changed to explore the specic app more
thoroughly. Note that DiagDroid also carefully addresses
418its compatibility issues. Parallel testing in multiple device
models is feasible as already shown.
Finally, DiagDroid resorts to dynamic analysis as we focus
on the performance issues caused by complicated runtime
dependency of asynchronous executions. It is hard for the
static analysis approaches to deal with such issues. For ex-
ample, determining the proper pool size based only on the
source codes is hard, since it is impossible to predict the
possible number of concurrent tasks. Moreover, it is hard
to know when to cancel a task beforehand. The aim of Di-
agDroid is to save inevitable human eorts, rather than ap-
proaching the task of automatic code correction. As shown
in our case studies, such eorts are light. DiagDroid is able
to provide a small set of possible issues. The issues that real-
ly contain bugs are with high rankings. As shown in Table 2,
the buggy cases rank 1.7 averagely. This indicates we can
easily be directed to where the code defect lies. The debug-
ging time of each case is generally less than an hour, even
for us who are unfamiliar with the app implementations.
7. RELATED WORK
Performance diagnosis has long been studied in many sys-
tems. Much work is conducted on predicting performance
of congurable systems [61, 73]. CARAMEL [51] detects
unnecessary loop executions with static analysis. Yu et
al.[71] propose a performance analysis approach based on
real-world traces. But such traces are lacking in our prob-
lem. PPD [68] resorts to dynamic instrumentation for goal-
oriented known performance issues search. Lag Hunting [31]
aims at nding performance bugs in JAVA GUI applications
in the wild without addressing concurrency issues. Exist-
ing work [2, 30] also considers \thread waiting time" ( i.e.,
the time when a thread waiting for other threads during
its execution) as a metric to nd performance bottlenecks.
In contrast, we focus on queuing time, i.e., the time when
a task waiting before its execution, to model task depen-
dency. Performance issues of Javascript programs are also
studied [62]. SAHAND [1] visualizes a behavioral model of
full-stack JavaScript apps' execution. But it does not take
the contentions of asynchronous tasks into consideration.
Performance is critical to mobile apps [4, 58]. Blocking op-
erations in the main thread are widely known as a cause of
many performance issues [38]. StrictMode [63] aims at nd-
ing such operations. Asynchronizer [36] and AsyncDroid [35]
provide a way to refactor specic blocking operations into s-
tandard AsyncTask andIntentService further to eliminate
the memory leakage problems. CLAPP [23] nds potential
performance optimizations via loop analysis. However, such
static analysis-based tools cannot capture runtime execution
dependency. Banerjee et al. [10, 11] design static analysis
driven testing for performance issues caused by anomalous
cache behaviors. Tango [26], Outatime [33] and Cedos [46]
optimize WiFi ooading mechanism to keep low-latency of
app. SmartIO [48] reduces the app delay via reordering IO
operations. These approaches solve specic performance is-
sues. DiagDroid in contrast aims at solving general UI per-
formance issues caused by runtime task dependency.
UI performance diagnosis captures much research atten-
tion. Method tracing [54] is an ocial tool used to diagnose
known performance issues due to its high overhead. QoE
Doctor [15] bases its diagnosis on Android Activity Testing
API [8] which can only handle pre-dened operations. Ap-
pinsight [57] is a tracing-based diagnosis tool for Windowsphone apps. It traces all asynchronous executions from a UI
event to its corresponding UI update, and identies the crit-
ical paths that inuence the performance. Panappticon [72]
adopts a similar approach on Android. But these approach-
es generally neglect the runtime dependency between tasks,
especially tasks executed for dierent UI operations. More-
over, it suers from low compatibility since they largely re-
quire to recompile the Android framework and OS kernel.
Performance diagnosis often requires to exercise the target
app automatically. Script-based testing is widely used ( e.g.,
UIAutomator [65], Monkey runner [45] and Robotium [59]).
MobiPlay [55], Reran [25] and SPAG-C [37] are record-and-
replay approaches which record the event sequence during
the manual exercising, and generate replayable scripts. Com-
plementary to these semi-automatic testing, fuzz testing ap-
proaches, for example, Monkey [67], Dynodroid [42] and
VanarSena [56], generate random input sequences to exer-
cise Android apps. Symbolic execution-base testings ( e.g.,
Mirzaei et al. [44], ACTEve [5], Jensen et al. [29], Evo-
Droid [43] and A3E[9]) aim at exploring the app function-
s systematically. Model-based testings ( e.g., Android Rip-
per [3] and SwiftHand [16]) aim at generating a nite state
machine model and event sequences to traverse the mod-
el. Test case selection techniques ( e.g., [27, 64]) can also be
adopted to exercise the apps. App exercising mechanisms
work as plugin modules of DiagDroid , enabling the devel-
opers to exploit their merits under dierent circumstances.
8. CONCLUSION
In this paper, we design DiagDroid for diagnosing An-
droid performance issues. To make DiagDroid a practical,
handy tool, we carefully consider the system design require-
ments like compatibility ,usability ,exibility and low over-
head. Specically, DiagDroid relies solely on the general
features of Android. Hence it works for most mainstream
Android devices, depending on no manufacturer specics.
Moreover, DiagDroid is convenient to use via a simple in-
stallation. It requires no eorts to recompile the OS kernel
and the Android framework. With a plugin mechanism, Di-
agDroid provides the exibility in selecting the test executor
to exercise a target app. Finally, DiagDroid keeps low over-
head by instrumenting slightly on the framework.
To conclude, this paper focuses on an important type of
Android performance issues caused by task execution de-
pendency. We carefully model the performance of the asyn-
chronous tasks and their dependency. DiagDroid is imple-
mented accordingly for task-level performance diagnosis. It
is equipped with a set of sophisticated task proling ap-
proaches based on the Android multithreading mechanisms.
We show DiagDroid can eectively reduce human eorts in
detecting and locating performance issues by applying the
tool successfully in nding bugs in tens of real-world apps.
9. ACKNOWLEDGEMENTS
The work described in this paper was supported by the
National Basic Research Program of China (973 Prj. No.
2014CB347701), the National Natural Science Foundation
of China (Prj. No. 61332010), the Research Grants Coun-
cil of the Hong Kong Special Administrative Region, China
(No. CUHK 14205214 of the General Research Fund), and
2015 Microsoft Research Asia Collaborative Research Pro-
gram (Prj. No. FY16-RES-THEME-005). The main work
was conducted when Yu Kang was a visiting student of Fu-
dan University. Yangfan Zhou is the corresponding author.
41910. REFERENCES
[1] S. Alimadadi, A. Mesbah, and K. Pattabiraman.
Understanding asynchronous interactions in full-stack
JavaScript. In Proc. of ICSE '16 , 2016.
[2] E. Altman, M. Arnold, S. Fink, and N. Mitchell.
Performance analysis of idle programs. In Proc. of
OOPSLA '10 , pages 739{753, 2010.
[3] D. Amaltano, A. R. Fasolino, P. Tramontana,
S. De Carmine, and A. M. Memon. Using gui ripping
for automated testing of android applications. In Proc.
of ASE '12 , pages 258{261, 2012.
[4] C. Amrutkar, M. Hiltunen, T. Jim, K. Joshi,
O. Spatscheck, P. Traynor, and S. Venkataraman.
Why is my smartphone slow? on the y diagnosis of
underperformance on the mobile internet. In Proc. of
DSN '13 , pages 1{8, 2013.
[5] S. Anand, M. Naik, M. J. Harrold, and H. Yang.
Automated concolic testing of smartphone apps. In
Proc. of FSE '12 , pages 1{11, 2012.
[6] Apktool: A tool for reverse engineering Android apk
les. http://ibotpeaches.github.io/Apktool/.
[7] AsyncTask. http://developer.android.com/reference/
android/os/AsyncTask.html.
[8] Automating User Interface Tests. http://developer.
android.com/training/testing/ui-testing/index.html.
[9] T. Azim and I. Neamtiu. Targeted and depth-rst
exploration for systematic testing of android apps. In
Proc. of OOPLSA '13 , pages 641{660, 2013.
[10] A. Banerjee. Static analysis driven performance and
energy testing. In Proc. of FSE '14 , pages 791{794,
2014.
[11] A. Banerjee, S. Chattopadhyay, and A. Roychoudhury.
Static analysis driven cache performance testing. In
Proc. of RTSS '13 , pages 319{329, 2013.
[12] G. Bavota, M. Linares-V asquez, C. E.
Bernal-C ardenas, M. Di Penta, R. Oliveto, and
D. Poshyvanyk. The impact of api change- and
fault-proneness on the user ratings of android apps.
Software Engineering, IEEE Transactions on ,
41(4):384{407, Apr 2015.
[13] J. Burnim and K. Sen. Asserting and checking
determinism for multithreaded programs. In Proc. of
FSE '09 , pages 3{12, 2009.
[14] Y. Cai and L. Cao. Eective and precise dynamic
detection of hidden races for java programs. In Proc.
of FSE '15 , pages 450{461, 2015.
[15] Q. A. Chen, H. Luo, S. Rosen, Z. M. Mao, K. Iyer,
J. Hui, K. Sontineni, and K. Lau. Qoe doctor:
Diagnosing mobile app qoe with automated ui control
and cross-layer analysis. In Proc. of IMC '14 , pages
151{164, 2014.
[16] W. Choi, G. Necula, and K. Sen. Guided GUI testing
of android apps with minimal restart and approximate
learning. In Proc. of OOPSLA '13 , pages 623{640,
2013.
[17] S. R. Choudhary, A. Gorla, and A. Orso. Automated
test input generation for android: Are we there yet?
InProc. of ASE '15 , pages 429{440, 2015.
[18] Conguring ART. https://source.android.com/
devices/tech/dalvik/congure.html.
[19] DiagDroid - Android Performance Diagnosis viaAnatomizing Asynchronous Executions.
http://www.cudroid.com/DiagDroid.
[20] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and
B. Chelf. Bugs as deviant behavior: A general
approach to inferring errors in systems code. In Proc.
of SOSP '01 , pages 57{72, 2001.
[21] F-Droid. https://f-droid.org/.
[22] FBReader - Favorite Book Reader.
https://fbreader.org/.
[23] Y. Fratantonio, A. Machiry, A. Bianchi, C. Kruegel,
and G. Vigna. Clapp: Characterizing loops in android
applications. In Proc. of FSE '15 , pages 687{697, 2015.
[24] G. Gan, C. Ma, and J. Wu. Data Clustering: Theory,
Algorithms, and Applications (ASA-SIAM Series on
Statistics and Applied Probability) . 2007.
[25] L. Gomez, I. Neamtiu, T. Azim, and T. Millstein.
Reran: Timing- and touch-sensitive record and replay
for android. In Proc. of ICSE '13 , pages 72{81, 2013.
[26] M. S. Gordon, D. K. Hong, P. M. Chen, J. Flinn,
S. Mahlke, and Z. M. Mao. Accelerating mobile
applications through ip-op replication. In Proc. of
MobiSys '15 , pages 137{150, 2015.
[27] K. Herzig, M. Greiler, J. Czerwonka, and B. Murphy.
The art of testing less without sacricing quality. In
Proc. of ICSE '15 , pages 483{493, 2015.
[28] J. Huang and L. Rauchwerger. Finding
schedule-sensitive branches. In Proc. of FSE '15 , pages
439{449, 2015.
[29] C. S. Jensen, M. R. Prasad, and A. Mller.
Automated testing with targeted event sequence
generation. In Proc. of ISSTA '13 , pages 67{77, 2013.
[30] M. Ji, E. W. Felten, and K. Li. Performance
measurements for multithreaded programs.
SIGMETRICS Perform. Eval. Rev. , 26(1):161{170,
June 1998.
[31] M. Jovic, A. Adamoli, and M. Hauswirth. Catch me if
you can: Performance bug detection in the wild. In
Proc. of OOPLSA '11 , pages 155{170, 2011.
[32] V. Kahlon, N. Sinha, E. Kruus, and Y. Zhang. Static
data race detection for concurrent programs with
asynchronous calls. In Proc. of FSE '09 , pages 13{22,
2009.
[33] K. Lee, D. Chu, E. Cuervo, J. Kopf, Y. Degtyarev,
S. Grizan, A. Wolman, and J. Flinn. Outatime: Using
speculation to enable low-latency continuous
interaction for mobile cloud gaming. In Proc. of
MobiSys '15 , pages 151{165, 2015.
[34] Y. Lei and R. H. Carver. Reachability testing of
concurrent programs. IEEE Trans. Softw. Eng. ,
32(6):382{403, June 2006.
[35] Y. Lin, S. Okur, and D. Dig. Study and refactoring of
android asynchronous programming. In Proc. of ASE
'15, pages 224{235, 2015.
[36] Y. Lin, C. Radoi, and D. Dig. Retrotting
concurrency for android applications through
refactoring. In Proc. of FSE '14 , pages 341{352, 2014.
[37] Y.-D. Lin, J. Rojas, E.-H. Chu, and Y.-C. Lai. On the
accuracy, eciency, and reusability of automated test
oracles for android devices. Software Engineering,
IEEE Transactions on , 40(10):957{970, Oct 2014.
[38] Y. Liu, C. Xu, and S.-C. Cheung. Characterizing and
420detecting performance bugs for smartphone
applications. In Proc. of ICSE '14 , pages 1013{1024,
2014.
[39] Y. Liu, C. Xu, and S.-C. Cheung. Diagnosing energy
eciency and performance for mobile internetware
applications. Software, IEEE , 32(1):67{75, Jan 2015.
[40] Low RAM Conguration. https://source.android.com/
devices/tech/cong/low-ram.html.
[41] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from
mistakes: A comprehensive study on real world
concurrency bug characteristics. In Proc. of ASPLOS
'08, pages 329{339, 2008.
[42] A. Machiry, R. Tahiliani, and M. Naik. Dynodroid:
An input generation system for android apps. In Proc.
of FSE '13 , pages 224{234, 2013.
[43] R. Mahmood, N. Mirzaei, and S. Malek. Evodroid:
Segmented evolutionary testing of android apps. In
Proc. of FSE '14 , pages 599{609, 2014.
[44] N. Mirzaei, S. Malek, C. S. P as areanu, N. Esfahani,
and R. Mahmood. Testing android apps through
symbolic execution. ACM SIGSOFT Software
Engineering Notes , 37(6):1{5, Nov. 2012.
[45] Monkeyrunner. http://developer.android.com/tools/
help/monkeyrunner concepts.html.
[46] Y. Moon, D. Kim, Y. Go, Y. Kim, Y. Yi, S. Chong,
and K. Park. Practicalizing delay-tolerant mobile apps
with cedos. In Proc. of MobiSys '15 , pages 419{433,
2015.
[47] F. F.-H. Nah. A study on tolerable waiting time: how
long are web users willing to wait? Behaviour &
Information Technology , 23(3):153{163, 2004.
[48] D. T. Nguyen, G. Zhou, G. Xing, X. Qi, Z. Hao,
G. Peng, and Q. Yang. Reducing smartphone
application delay through read/write isolation. In
Proc. of Mobisys '15 , pages 287{300, 2015.
[49] J. Nielsen. Usability Engineering . Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 1993.
[50] S. Niida, S. Uemura, and H. Nakamura. Mobile
services. IEEE Vehicular Technology Magazine ,
5(3):61{67, Sep 2010.
[51] A. Nistor, P. C. Chang, C. Radoi, and S. Lu. Caramel:
Detecting and xing performance problems that have
non-intrusive xes. In Proc. of ICSE '15 , 2015.
[52] OpenLaw - Die Gesetze App.
https://openlaw.jdsoft.de/.
[53] Processes and Threads. http://developer.android.
com/guide/components/processes-and-threads.html.
[54] Proling with Traceview and dmtracedump.
http://developer.android.com/tools/debugging/
debugging-tracing.html.
[55] Z. Qin, Y. Tang, E. Novak, and Q. Li. Mobiplay: A
remote execution based record-and-replay tool for
mobile applications. In Proc. of ICSE '16 , 2016.
[56] L. Ravindranath, S. Nath, J. Padhye, andH. Balakrishnan. Automatic and scalable fault
detection for mobile applications. In Proc. of Mobisys
'14, pages 190{203, 2014.
[57] L. Ravindranath, J. Padhye, S. Agarwal, R. Mahajan,
I. Obermiller, and S. Shayandeh. Appinsight: Mobile
app performance monitoring in the wild. In Proc. of
OSDI '12 , pages 107{120, 2012.
[58] L. Ravindranath, J. Padhye, R. Mahajan, and
H. Balakrishnan. Timecard: Controlling user-perceived
delays in server-based mobile applications. In Proc. of
SOSP '13 , pages 85{100, 2013.
[59] RobotiumTech. Robotium: User scenario testing for
Android. http://www.robotium.org.
[60] V. Roto and A. Oulasvirta. Need for non-visual
feedback with long response times in mobile hci. In
Proc. of WWW '05 , pages 775{781, 2005.
[61] A. Sarkar, J. Guo, N. Siegmund, S. Apel, and
K. Czarnecki. Cost-ecient sampling for performance
prediction of congurable systems. In Proc. of ASE
'15, pages 342{352, 2015.
[62] M. Selakovic and M. Pradel. Performance issues and
optimizations in JavaScript: An empirical study. In
Proc. of ICSE '16 , 2016.
[63] StrictMode. http://developer.android.com/reference/
android/os/StrictMode.html.
[64] V. Terragni, S.-C. Cheung, and C. Zhang. Recontest:
Eective regression testing of concurrent programs. In
Proc. of ICSE '15 , pages 246{256, 2015.
[65] Testing Support Library - UI Automator.
https://developer.android.com/tools/
testing-support-library/index.html#UIAutomator.
[66] R. A. to search for countries based on several
parameters. https://github.com/abhi2rai/RestC.
[67] UI/Application Exerciser Monkey. http:
//developer.android.com/tools/help/monkey.html.
[68] A. Wert, J. Happe, and L. Happe. Supporting swift
reaction: Automatically uncovering performance
problems by systematic experiments. In Proc. of ICSE
'13, pages 552{561, 2013.
[69] Xposed Module overview.
http://repo.xposed.info/module-overview.
[70] Xposed Module Repository. http://repo.xposed.info.
[71] X. Yu, S. Han, D. Zhang, and T. Xie. Comprehending
performance from real-world execution traces: A
device-driver case. SIGPLAN Not. , 49(4):193{206,
Feb. 2014.
[72] L. Zhang, D. R. Bild, R. P. Dick, Z. M. Mao, and
P. Dinda. Panappticon: Event-based tracing to
measure mobile application and platform performance.
InProc. of CODES+ISSS '13 , pages 1{10, 2013.
[73] Y. Zhang, J. Guo, E. Blais, and K. Czarnecki.
Performance prediction of congurable software
systems by fourier learning. In Proc. of ASE '15 ,
pages 365{373, 2015.
421