 
 Effective and Precise Dynamic  
Detection of Hidden Races  for Java Programs  
Yan Caiâ€  
State Key Laboratory of Computer Science  
Institute of Software  
Chinese Academy of Sciences, Beijing, China  
ycai.mail@gmail.com  Lingwei Cao  
State Key Laboratory of Computer Science  
Institute of Software  
Chinese Academy of Sciences, Beijing, China  
lingweicao@gmail.com  
 
ABSTRACT  
Happens -before relation is widely used to detect data races  dynam-
ically . However, it could easily hide many data races as it is inter-
leaving sensitive. Existing techniques based on randomized sched-
uling are  ineffective on dete cting these hidden races . In this paper, 
we propose DrFinder , an effective  and precise dynamic  technique 
to detect hidden races . Given an execution, DrFinder  firstly ana-
lyzes the lock acquisitions in it and collect s a set of "may-trigger " 
relations. Each m ay-trigger relation consists of  a method  and a type 
of a Java object. It indicates that, during execution, the method  may 
directly or indirectly acquire a lock of the type. In the subsequent 
executions of the same program, DrFinder  actively schedules the 
execution according to the set of collected may -trigger relations. It 
aims to reverse the  set of  happens -before relation that may exist in 
the previous executions so as to expose those hidden races. To ef-
fective ly detect hidden race s in each execution, DrFinder  also col-
lects a new set of may -trigger relation during its scheduling, which 
is used in its next scheduling . Our experiment on a suite of  real-
world Java multithreaded programs shows  that DrFinder  is effec-
tive to detect  89 new data races  in 10 runs . Many of these races 
could not be detected by existing techniques  (i.e., FastTrack , Con-
Test, and PCT ) even in 100 runs .  
Categories and Subject Descriptors  
D.2.4 [Software Engineering] : Program Verification;  D.2.5 
[Software Engineering ]: Testing and Debugging, testing tools; 
D.4.1  [Operating Systems ]: Processing Management â€“
synchronizations, threads.  
General Terms  
Reliability, verification . 
Keywords   
Data race, thread scheduling , hidden race,  synchronization order  
1. INTRODUCTION  
A data race  (or race for short)  [17] occurs when  two or more 
threads access a same  memory location  concurrently , and at least 
one of these accesses is a write [17]. Data race occurrences  often 
indicate  other concurrency bugs  in the same program  [34]. Many 
dynamic d ata race detectors  are based on the locking discipline  [40] or the happens -before relations  (HBR for short)  [25]. The locking 
discipline requires every  two concurrent access es (one of them is a 
write) to a shared memory location  to be  protected by a common 
set of lock s. But, such lockset -based detectors are  imprecise  [18]. 
HBR -based detectors  (or HB detectors  for short)  precisely [17] re-
port a data race  only if they observe the two accesses involving in 
a race  not ordered by any HBR  in an execution/trace . (In this paper, 
we use the two terms execution  and trace  interchangeably.)  
Some races in a program  can be easily exposed in many traces  and 
HB detectors  [17][35][37] can effectively  detect them.  There are  
other races that are difficult to be detected due to reasons like con-
ditional variables and ad -hoc synchronizations . They can be de-
tected by two most recent technique s RVPredict  [20] through data 
flow analysis  offline  and Racageddon  [16] through  generating spe-
cific test input s for each predicted race .  
HB detector s are interleaving -sensitive  [49]. They  may miss to  de-
tect a race  if the two accesses of this race are ordered by HBR s in 
an execution;  but the same race can be detected in another execu-
tion with a different thread interleavi ng such that no HBR orders 
the two accesses . That is, such a race is hidden by HB edges  in some 
executions  [43]. Some of these races, even on repeated executions , 
can still be hard to detect [43]. For ease of reference, we refer to 
such a race as a hidden race  (also  known as a "hard" race  [43]). 
This paper focuses on the detection of hidden races.  
Existing online  techniques (e.g., [8][15][41][50]) are ineffective to 
detect hidden races. Randomized scheduling technique s (e.g., PCT  
[8] and ConTest  [15]) only randomly  identify changed point s [8] or 
insert random  time delays [15] to modify thread priorit ies. They are 
ineffective in exposing races whose accesses are separated by con-
secutive sequence s of locking order s [25] among threads.  Active 
testing  (e.g.,  [41][50], Racageddon  and RVPredict ) techniques are  
built on top of  random  (or native ) scheduling and/or concurrency 
bug patterns  to produce a predictive trace for potential race analy-
sis. It fails to be successfully applied if no  hidden race can be pre-
dicted  in the predictive run. Besides, they need many runs to deem 
a potential race as a  false positive  with confidence . Coverage -
driven testing techniques  [50] or using adequacy criteria [53] de-
mand either patterns of problematic memory accesses as well as 
synchronization operations  or applicable adequacy criteria  as in-
puts. To the best of our knowledge, there is no effective patt ern or 
adequacy criterion for hidden races discovered yet.   
As such , a key challenge for dynamic race detectors is to generate 
execution s that effective ly expose hidden races .  
Offline  techniques [20][43] to may infer hidden races . The  Caus-
ally-Precedes ( CP) detector  [43] can interestingly predict hidde n 
races in a given trace  under limited scenarios . CP is however inap-
plicable if a hidden race is separated by HBR (i.e., HB edges ) hav-
ing conflicting data accesses [43]. Also, a nalyzing large traces (e.g., â€  Corresponding author.  
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciï¬c permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ESEC/FSEâ€™15 , August 30 â€“ September 4, 2015, Bergamo, Italy
ACM. 978-1-4503-3675-8/15/08...$15.00
http://dx.doi.org/10.1145/2786805.2786839
450 
 running Eclipse for 1 hour of code development) by CP is still im-
practical ; besides , no online CP detector has been invented  yet [43]. 
RVPredict  [20] confirm s each predicted data race via constraint 
solving . Like other predictive technique s [41], it needs to solve the 
scheduling constraint s for each predicted data race , which may fail .  
Our work  exploit s two observation s: (1) many races hidden in one 
execution can be detected  by reversing the direction of one or more 
consecutive HBRs  [25] in another  execution , and (2) in real -world 
programs, only a small proportion of methods generate s lock ac-
quisition events, and these methods usually generate events on des-
ignated (instead of arbitrary) lock objects .  
In this paper, we propose DrFinder  (Data races Finder ), a dynamic  
technique to detect hidden races by reversing possible HBRs. 
DrFinder  is based on may-trigger  relation . This relation relates a 
method to a type of lock  object  in Java programs . It represents that 
the method may directly or indirectly  (by calling several other 
methods)  trigger  a lock acquisition on a lock object  of that type.   
DrFinder  consists of two phases.  Figure 1 shows an overview of 
DrFinder . In Phase I, DrFinder  analyzes each lock acquisition event 
in a given trace to relate each selected method to the type of the 
lock object of the event  and takes them as  a set of may-trigger  re-
lations (i.e., MTR in Figure 1). In Phase II,  it generates a trace for 
hidden race detection  based on the collected set of may-trigger  re-
lation . Specifically, if a thread  generates  a lock acquisition event e1 
on a lock of type c and some other thread may -trigger a lock acqui-
sition event e2 on a lock of the same type c, DrFinder  postpones the 
execution  of e1 until an expected  event  e2 occurs . As such, the lock-
ing order  on these two events  that they may form in the trace ana-
lyzed in Phase I  is reversed . In this way, the races originally hidden 
by such HB Rs are exposed  in the later trace .  
DrFinder  also collects a set of new may -trigger relations in each 
scheduled execution  which is used in the next scheduled execution. 
This feedback mechanism makes DrFinder  effective to detect new 
hidden races in each its scheduled execution.  
We have implemented DrFinder  in the Jikes RVM [3], and evalu-
ated it on the Dacapo benchmarks [6]. The experi mental result 
shows that DrFinder  finds races that cannot be effectively detected 
by both native runs , ConTest , and PCT  configured with FastTrack . 
In total,  DrFinder  detects  89 new data races on 5 Dacapo  bench-
marks within  10 runs each. Besides, many of these new races could 
not be detected by existing techniques in 100 runs.  
In summary, the main contribution s of this paper are:  
ï‚· This paper propose s a may-trigger  relation and  a novel hidden 
data race detector  DrFinder . DrFinder  predicts locking orders , 
and makes decisions on the reversal s of locking orders at the 
may-trigger  relation level. It profile s no memory accesses in 
Phase I , and only  carries forward the may-trigger  relations  be-
tween phases.   
ï‚· It reports the  feasib ility of DrFinder  by implement ing it as a  
prototype tool in the Jikes RVM . It presents an experiment to 
evaluate DrFinder . In the experiment, DrFinder  detect s 89 new 
races  and is promising in exposing hidden races.  DrFinder  is 
scalable to large -scale Java programs (e.g., Eclipse) .  
In the rest of this paper, Section 2 reviews preliminaries  followed 
by a motivating exampl e in Section 3. Section s 4 and 5 present the 
design rationales and the details of DrFinder , respectively.  Section 
6 reports the evaluation of DrFinder . In Sections 7 and 8, we discuss 
the related work and conclude this paper , respectively .  2. PRELIMINARIES  
A multithreaded Java program ğ‘ contains  a set of c lasses denoted 
as C. Each class ğ‘ in C consists of  a set of fields and a set of meth-
ods. We denote the set of all methods of ğ‘ as M. An object ğ‘œ ïƒ O 
is an instance  of a class ğ‘, and the type of ğ‘œ, denoted as type(o), is 
ğ‘. Each lock is an instance of a class.   
Each thread ğ‘¡ in program ğ‘ executes a nested sequence of methods. 
Each  method may execute  a set of operations OP = { ğ‘Ÿğ‘‘, ğ‘¤ğ‘Ÿ, ğ‘ğ‘ğ‘, 
ğ‘Ÿğ‘’ğ‘™, ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ , ğ‘Ÿğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›  }, where ğ‘Ÿğ‘‘ and ğ‘¤ğ‘Ÿ mean read and write to a  
field of a class  instance , respectively;  ğ‘ğ‘ğ‘ and ğ‘Ÿğ‘’ğ‘™ mean acquisi-
tion and release  of a lock , respectively ; and ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ  and ğ‘Ÿğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›  
mean a call  and a return to and from a method , respectively .  
An event  ğ‘’=ïƒ¡ğ‘¡,ğ‘œğ‘,ğ‘œïƒ± mean s that a thread ğ‘¡ performs an operation 
ğ‘œğ‘ ïƒ OP on an object ğ‘œ ïƒ O ïƒˆ M. We denote ğ‘œ in ğ‘’ by ğ’ğ’ƒğ’‹ğ’†ğ’„ğ’•(ğ‘’). 
A trace  ğœ is a sequence of events.   
The Happens -before relation  (â†£, HBR ) [25] in a trace is defined 
by three rules: (1) if two events ï¡ and ï¢ are performed by the same 
thread, and ï¡ appeared before ï¢, then ï¡ â†£ ï¢. (2) if two events Î±=
ïƒ¡ğ‘¡ğ›¼,ğ‘Ÿğ‘’ğ‘™,ğ‘šïƒ± and Î²=ïƒ¡ğ‘¡ğ›½,ğ‘ğ‘ğ‘,ğ‘šïƒ±  are performed by two different  
threads, and ï¡ appeared before ï¢, then ï¡ â†£ ï¢. (3) if ï¡ â†£ ï¢ and ï¢ â†£ 
ï§, then ï¡ â†£ ï§.  
Two memory  events ğ‘’1=ïƒ¡ğ‘¡1,ğ‘œğ‘1,ğ‘£1ïƒ± and ğ‘’2=ïƒ¡ğ‘¡2,ğ‘œğ‘2,ğ‘£2ïƒ± form 
a race on ğ‘£1 if (1) ğ‘¡1â‰ ğ‘¡2âˆ§ğ‘£1=ğ‘£2, (2) {ğ‘¤ğ‘Ÿ}âˆ©{ğ‘œğ‘1,ğ‘œğ‘2}â‰ âˆ…, 
and (3) neither ğ‘’1â†£ğ‘’2 nor ğ‘’2â†£ğ‘’1.  
The relation ğ‘’1â†£ğ‘’2 is called a n HB edge  if ğ‘’1=ïƒ¡ğ‘¡1,ğ‘Ÿğ‘’ğ‘™,ğ‘™ïƒ±, ğ‘’2=
ïƒ¡ğ‘¡2,ğ‘ğ‘ğ‘,ğ‘™ïƒ±, and ğ‘¡1â‰ ğ‘¡2 [43]. For instance,  the two arrow s in Figure 
2 depict  two HB edge s ğ‘’1â†£ğ‘’9 and ğ‘’3â†£ğ‘’6, where "sync(o){â€¦}"  
denotes a pair of events " ğ‘ğ‘ğ‘(ğ‘œ) â€¦ ğ‘Ÿğ‘’ğ‘™(ğ‘œ) "; and we will use this 
short form in Section 3. Given two traces ğœ and ğœâ€² and a pair of 
events ğ‘’1 and ğ‘’2, if ğ‘’1 and ğ‘’2 form a race in ğœ but  does not form 
any race in ğœâ€², then the race is called  a Hidden Race  in ğœâ€².  
In a Java program, each thread starts its execution from its method 
run(). 
3. MOTIVATING EXAMPLE  
Figure 2 shows our motivating example , where,  each threa d (i.e., 
ğ‘¡1, ğ‘¡2, or ğ‘¡3) executes a sequence of events from top to bottom. 
These events  are memory accesses ( i.e., ğ‘š1 to ğ‘š4) to the locations 
ğ‘¥ and ğ‘¦, and lock acquisition /release events ( i.e., ğ‘’1 to ğ‘’9) on seven  
lock objects ğ‘˜, ğ‘›, and ğ‘œ1 to ğ‘œ1. The two HB edges ğ‘’1â†£ğ‘’9 and 
ğ‘’3â†£ğ‘’6 on the two lock objects ğ‘› and ğ‘˜, respectively, are denoted  
as arrows. We denote the trace as ğœ1=ïƒ¡â€¦,ğ‘’1â€¦,ğ‘’9â€¦, 
ğ‘’3â€¦,ğ‘’6â€¦ïƒ±. The two  pairs of accesses to locations x and y (i.e., ğ‘š1 
May-Trigger
Collector
Data RacesHB Race DetectorDrFinder
Scheduleracquire , release
enter , return
acquire , release
enter , returnPhase I
MTR: {mt(f, c)}
Trace ğœ2Trace ğœ 
Phase IIA depth value d
A data race 
is hidden 
by a HB edge.
A data race 
is exposed
by DrFinder .acquire , release
read, write 
Figure 1. An overview of DrFinder . 
451 
 and ğ‘š4, and ğ‘š2 and ğ‘š3) are ordered by the HB edges ğ‘’1â†£ğ‘’9 and 
ğ‘’3â†£ğ‘’6, respectively.  Hence, no HB detector can detect any race 
in the trace  ğœ1.  
Next, we reverse the direction of each HB edge shown in Figure 2 
to sketch Figure 3, which represents the trace ğœ2=ïƒ¡â€¦,ğ‘’9â€¦, 
ğ‘’1â€¦,ğ‘’6â€¦,ğ‘’3â€¦ïƒ± of the same program. A n HB detector can now 
report the two (hidden) races on x and y in ğœ1.  
If a native  schedule exhibits the trace ğœ1 as shown in Figure 2, the 
probability to observe the trace ğœ2 shown in Figure 3 is very low 
[43]. Hence, the two races may not be easily detected.  
NaÃ¯ ve strategy:  A naive strategy is to suspend every lock acquisi-
tion event observed in a trace  before executing the event. For trace 
ğœ2, it suspends  the three threads from executing ğ‘’1, ğ‘’4, and ğ‘’7, re-
spectively , producing an  occurrence of the suspension of all the 
threads of the trace (known as thrashing  [41]). Thrashing is typi-
cally resolved by randomly resuming one of the suspended threads . 
There is no thoughtful  design to ensure ğ‘’9â†£ğ‘’1 in order to expose 
the hidden race on x effectively.  
Offline techniques : CP [43] can infer these two races from ğœ1, 
providing that no  conflicting memory accesses  exist inside  the syn-
chronization bod ies of ğ‘’1 and ğ‘’9. That is , if ğ‘§ is a new location, 
where ğ‘’1 protects a write access to  ğ‘§ (i.e., ğ‘’1=ğ‘ ğ‘¦ğ‘›ğ‘(ğ‘˜){â€¦,ğ‘¤ğ‘Ÿ(ğ‘§) 
â€¦}) and ğ‘’9 protects a  read access to z (i.e., ğ‘’9=ğ‘ ğ‘¦ğ‘›ğ‘(ğ‘˜){â€¦,ğ‘Ÿğ‘‘(ğ‘§) 
â€¦}). Then, the race on x cannot be detected by CP (and the case on 
y is similar). This is restrictive. In their experiment [43], on only 2 
out of 11 programs,  can CP detect 2 and 7 more races than 
FastTrack  [17] (an online  HB d etector ).  
Online randomized schedulers : ConTest  [15] inserts a small 
amount  of random time delays on some lock acquisitions.  Suppose 
that ğ‘’1 is generated but not  executed by ğ‘¡1 yet. A small time delay 
in between the generation and execution of ğ‘’1 may not be long 
enough for ğ‘¡3 to have generated and executed ğ‘’9, which  depends on 
both the underlying (native  or randomized)  scheduler and the se-
quence of operations performed by ğ‘¡3 in between the current exe-
cution point and ğ‘’9. The design of ConTest  is insensitive to both 
factors.  PCT  [8] provides a theoretical guarantee to find a concur-
rency bug , but this guarantee is very low even for the illustrating 
example . Its guarantee d probability  (i.e.,  Ã·(ğ‘›Ã—ğ‘˜ğµğ‘¢ğ‘”ğ·ğ‘’ğ‘ğ‘¡ â„âˆ’1)) [8], where ğ‘› is the number of threads and ğ‘˜ is the number of in-
structions executed ) also decrease s exponentially  as ğµğ‘¢ğ‘”ğ·ğ‘’ğ‘ğ‘¡ â„ in-
creases . From our first -hand experience, many bugs can be d etected 
using 1 as ğµğ‘¢ğ‘”ğ·ğ‘’ğ‘ğ‘¡ â„, and yet a significant amount of races still can-
not be detected using much deeper depths with 100 runs.  
Our technique:  DrFinder  can effectively reverse the two HB edges 
ğ‘’1â†£ğ‘’9 and ğ‘’3â†£ğ‘’6 observed  in the trace ğœ1 when generating the 
trace ğœ2. When  observing ğ‘’1, it effectively foresee s the execution 
of the event ğ‘’9; and similarly, when observing ğ‘’3, it effectively 
foresee s the execution of the event ğ‘’6. DrFinder  achieves this pre-
diction via a novel  strategy.   
We denote the types of the lock objects ğ‘˜, ğ‘›, ğ‘œ1 to ğ‘œ5 as ğ‘ğ‘˜, ğ‘ğ‘›, ğ‘1 
to ğ‘5, respectively, and the methods that contain above events as ğ‘“1 
to ğ‘8 as shown in Figure 2, where an upper method  in a column 
invokes the method  immediately below it  (e.g., ğ‘“1 invokes ğ‘“2). 
Specifically, in Phase I, DrFinder  constructs every may -trigger re-
lation mt(f, c) (see Section  4.3 for definition ) between a method f 
and a lock object type c observed in ğœ1. Each may-trigger  relation 
mt(f, c) means that f may trigger a lock acquisition event on a lock 
object of type c. Table 1 shows the set of may -trigger relations con-
structed  from ğœ1 in Figure 2. (Note that may -trigger relation also 
considers program call stack, not a single function. )  
In Phase II, firstly, s uppose that ğ‘¡2 is executing some event s in the 
method ğ‘“3 and ğ‘¡3 is execu ting some event s in the method ğ‘“6. When  
DrFinder  observes  the event ğ‘’1 produced by ğ‘¡1, it checks the may-
trigger  relations involving  ğ‘“3 and ğ‘“6 (i.e., the two methods being 
executed by the other two threads ğ‘¡2 and ğ‘¡3, respectively),  and 
finds a may-trigger  relation mt(ğ‘“6, ğ‘ğ‘˜), meaning that ğ‘“6 may-trigger 
a lock acquisition event o n an object of  type ğ‘ğ‘˜, which is the same 
type as that of ğ‘’1. DrFinder  thus suspends ğ‘¡1 (depicted as a solid 
rectangle in Figure 3), and sets ğ‘¡1 to wait for an event of th is object 
type ğ‘ğ‘˜. It further escort s ğ‘¡3 to execute  all its events until ğ‘¡3 exe-
cutes the event ğ‘’9, which is the first encountered event on a lock 
object having the type ğ‘ğ‘˜. DrFinder  then resumes ğ‘¡1 to execute ğ‘’1 
to form the targeted HB edge ğ‘’9â†£ğ‘’1. A further execution of the 
two threads will execute memory accesses on x, which expose the 
hidden race on x.  
Next, it is feasible for ğ‘¡1 to execute ğ‘’2 or for ğ‘¡2 to execute ğ‘’4. Be-
cause neither mt(ğ‘“4, type(ğ‘œ1)) nor mt(ğ‘“2, type(ğ‘œ2)) matches any  
may-trigger relation, DrFinder  resolves the tie randomly: (1) Sup-
pose that ğ‘¡1 is selected . When ğ‘¡1 generates  ğ‘’3, DrFinder  finds that 
mt(ğ‘“4, type(n)) is a may -trigger relation. Thus, it suspends ğ‘¡1, and 
escorts ğ‘¡2 to execute until ğ‘¡2 has executed ğ‘’6. After that, ğ‘¡1 exe-
cutes ğ‘’3, and the HB edge ğ‘’6â†£ğ‘’3 is formed. When ğ‘¡2 executes 
ğ‘š3, the race on y is detected. (2) Suppose that ğ‘¡2 executes ğ‘’4 first. Table 1. The May-Trigger  relation for  methods  and lock object types 
of program shown in Figure 2. 
May-Trigger  Relation  May-Trigger  Relation  
mt(ğ‘“1, ğ‘ğ‘˜), mt(ğ‘“1, ğ‘1), mt(ğ‘“1, ğ‘ğ‘›) mt(ğ‘“5, ğ‘ğ‘›) 
mt(ğ‘“2, ğ‘1), mt(ğ‘“2, ğ‘ğ‘›) mt(ğ‘“6, ğ‘4), mt(ğ‘“6, ğ‘5), mt(ğ‘“6, ğ‘ğ‘˜) 
mt(ğ‘“3, ğ‘2), mt(ğ‘“3, ğ‘3), mt(ğ‘“3, ğ‘ğ‘›) mt(ğ‘“7, ğ‘4), mt(ğ‘“7, ğ‘5), mt(ğ‘“7, ğ‘ğ‘˜) 
mt(ğ‘“4, ğ‘2), mt(ğ‘“4, ğ‘3), mt(ğ‘“4, ğ‘ğ‘›) mt(ğ‘“8, ğ‘ğ‘˜)   
ğ‘¡1
â‹®
  :  ( )
ğ’† :   ğ’„ ( ){â€¦}
ğ‘’2:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ1){â€¦}
  :  ( )
ğ’† :   ğ’„ ( ){â€¦}ğ‘¡2
â‹®
â‹®
ğ‘’4:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ2){â€¦}
ğ‘’5:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ3){â€¦}
â‹®
â‹®
ğ’† :   ğ’„ ( ){â€¦}
  :  ( )ğ‘¡3
â‹®
â‹®
â‹®
ğ‘’7:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ4){â€¦}
â‹®
ğ‘’8:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ5){â€¦}
ğ’† :   ğ’„ ( ){â€¦}
  :  ( )ğ‘“1
ğ‘“2ğ‘“3
ğ‘“4
ğ‘“5ğ‘“6
ğ‘“7
ğ‘“8 
Figure 3. A trace ğˆ  generated by DrFinder , exposed two races on x 
and y. 
ğ‘¡1
â‹®
  :  ( )
ğ’† :   ğ’„ ( ){â€¦}
ğ‘’2:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ1){â€¦}
  :  ( )
ğ’† :   ğ’„ ( ){â€¦}ğ‘¡2
â‹®
â‹®
ğ‘’4:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ2){â€¦}
ğ‘’5:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ3){â€¦}
â‹®
â‹®
ğ’† :   ğ’„ ( ){â€¦}
  :  ( )ğ‘¡3
â‹®
â‹®
â‹®
ğ‘’7:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ4){â€¦}
â‹®
ğ‘’8:ğ‘ ğ‘¦ğ‘›ğ‘ (ğ‘œ5){â€¦}
ğ’† :   ğ’„ ( ){â€¦}
  :  ( )ğ‘“1
ğ‘“2ğ‘“3
ğ‘“4
ğ‘“5ğ‘“6
ğ‘“7
ğ‘“8 
Figure 2. A trace ğˆ  hiding two races on x and y as two HB edges ğ’† â†£
ğ’†  and ğ’† â†£ğ’†  order the two accesses of each race, respectively.  
452 
 When ğ‘¡2 further generates  ğ‘’5, DrFinder  finds that mt(ğ‘“2, type(ğ‘œ3)) 
does not match any may-trigger relation. Thus, both  threads ğ‘¡1 and 
ğ‘¡2 may proceed further . So, DrFinder  resolves the tie randomly. If 
ğ‘¡1 is selected to execute first, a race on y is detected. Otherwise, no 
race is reported because the HB edge between ğ‘’3 and ğ‘’6 is still  
ğ‘’3â†£ğ‘’6.  
4. DESIGN RATIONALES  
In this section, we present the design  rationales of DrFinder , with 
the help of  two traces ğœ1 and ğœ2, and the ir corresponding set of HB 
edges are ï„1 and ï„2, respectively. Besides, t here are two threads ğ‘¡1 
and ğ‘¡2 in two traces and they produce two events ğ‘’1 and ğ‘’2, respec-
tively, in trace ğœ1; however, the two events may not be produced by 
two threads in trace ğœ2.  
4.1 Basic Requirements   
We recall that a hidden race is difficult to expose in a trace gener-
ated by a native  scheduler or a pure randomized  scheduler. That is, 
although the two accesses involving in a hidden race may appear in 
a trace, yet the pair of accesses may be separated by non -trivial 
numbers of HB edges  (e.g., the two accesses ğ‘š1 and ğ‘š4 in Figure 
2). For ease of reference, we refer to such a native or pure random-
ized scheduler as a default scheduler.  
A strategy modeled after the above i ntuition is as follows: In Phase 
I, a technique observes the set of HB edges ï„1 in the trace ğœ1 pro-
duced by a default  scheduler.  Then, in Phase II, it aims to reverse 
the directions of some HB edges in ï„1 on generating  the trace ğœ2. 
That is, if the two events  ğ‘’1â†£ğ‘’2âˆˆ ï„1, it aims to produce ğ‘’2â†£ğ‘’1 
ïƒ ï„2 if possible  as shown in Figure 4 (a) and (b), respectively .  
As such, a dynamic hidden race detector should  aim to : 
Phase I ) keep a (sub)set of HB edges (i.e., ï„1) in trace ğœ1, 
Phase II ) and schedule a subsequent execution  (generating 
trace ğœ2) to reverse HB edges in ï„1 to expose race s hid-
den in trace ğœ1.  
However, DrFinder  does not target to keep any HB edges as which 
usually  incur s high runtime overhead  [12][21][41]. It tries to pre-
dict HB edges dynamically via the type of locks. In the next two 
subsections, we present how DrFinder  achieves this aim to reverse 
HB edges .  
4.2 Reversing Happens -before Edges  
Suppose that ğ‘’1â†£ğ‘’2 is an HB edge in ï„1 as depicted in Figure 
4(a), there is a good chance that, using the default scheduler, ğ‘’1â†£
ğ‘’2 may also exist in ï„2. That is,  most of HB  edges in ï„1 cannot be 
easily reversed in ï„2. Therefore,  our target is to actively produce 
ğ‘’2â†£ğ‘’1 as depicted in Figure 4(b).  
Suppose that during the execution  (to generate  the trace ğœ2), both 
events ğ‘’1 and ğ‘’2 exist. In theory, a precise but hypothetical  strategy 
can be formulated as follows:  
 
However, implementing such a strategy is challenging: if there is 
no such an event ğ‘’2 in trace ğœ2, then the  HB edge ğ‘’2â†£ğ‘’1 will not 
exist, and no HB edge needs to be reversed. Hence, above strategy 
will suspend the thread ğ‘¡1 until the thread ğ‘¡2 terminates. If all 
events (or at least lock acquisition events)  in trace ğœ1 are logged to 
check the existence of an event ğ‘’2, it is necessary to compute an 
object abstraction  [13][21][41] (e.g., a unique id) for each event. However,  before the occurrence of ğ‘’2, there is no way to compute 
an object abstraction for  ğ‘’2.  
Therefore, an effective technique must  address a problem : Given 
an event ğ‘’1 to be executed  by a thread ğ‘¡1, how to determine whether 
some other  thread ğ‘¡2 will execute an event ğ‘’2 such that  ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘’2) 
=ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘’1) without computing a n object  abstraction for each 
event ?  
4.3 Stack and Type Based Events Predictions  
Let us refine the problem further as the two events ğ‘’1 and ğ‘’2 should 
be ca usally related; otherwise, there is no need to consider them to 
form a n HB edge . Suppose  that when ğ‘¡1 is about to execute  ğ‘’1, 
thread ğ‘¡2 is executing an event ğ‘’âˆ— within the body of a method ğ‘“ğ‘˜ 
as shown in Figure 5(a). To ease our explanation, we refer to the 
current call stack of thread ğ‘¡2 as stack s. If the event ğ‘’2 will occur 
in future in the execution of ğ‘¡2, there will be another call stack  frag-
ment: ğ‘ 1 in the below  Backward Case or ğ‘ 2 in the below F orward 
Case:  
ï‚· Backward Case shown in Figure 5(b): after thread ğ‘¡2 returns 
from method ğ‘“ğ‘˜ recursively to a method ğ‘“1, and then calls 
some other methods , an event ğ‘’2 from a method ğ‘“ğ‘‘â€² is exe-
cuted. We refer to the two call stack fragments ïƒ¡ğ‘“1 â€¦, ğ‘“ğ‘˜ïƒ± and 
ïƒ¡ğ‘“1 â€¦, ğ‘“ğ‘‘â€²ïƒ± as ğ‘ â€² and ğ‘ 1, respectively , as depicted .  
ï‚· Forward Case  shown in Figure 5(c): before thread  t2 returns 
from its execution in method ğ‘“ğ‘˜, it further calls some meth-
ods and then an event ğ‘’2 from a method ğ‘“ğ‘‘â€²â€² is executed. We 
refer to the call stack fragments ïƒ¡ğ‘“ğ‘˜ â€¦, ğ‘“ğ‘‘â€²â€²ïƒ± as ğ‘ 2, as de-
picted.  
Therefore, to predict the existence of event ğ‘’2, thread ğ‘¡2 should be 
aware of the method ğ‘“ğ‘‘â€² (in Backward Case) or the method ğ‘“ğ‘‘â€²â€² (in 
Forward Case) that execute s an event ğ‘’2. To do so, it is necessary 
to record  the events  that a given method will execute directly or 
indirectly (i.e., via calling other methods). With such information, 
given an event ğ‘’âˆ— from a method ğ‘“ğ‘˜, it becomes easy to know 
whether there will be an event ğ‘’2. 
However, directly implementing above idea to detect hidden race s 
is ineffective or even does not work. It is b ecause each method, 
once called, directly (for event within this method) or indirectly (for 
events out of  this method) executes all later events. For example, 
the method run() executes all events. Therefore, to make the pre-
diction of an event effective for detection of hidden races via re-
versing HB edges, the used stack should be limited. In other words, 
only some methods in a stack should  be used to do prediction, but 
not all.  
Let's further review the two cases. For Backward Case, we can ob-
serve from Figure 5(b) that the effective  call stack to predict the 
event ğ‘’2 (when thread ğ‘¡2 is executing an event ğ‘’âˆ— in method ğ‘“ğ‘˜) is 
only the stack fragments ğ‘ â€² and ğ‘ 1. Similarly, for Forward Case, the 
effective stack is the stack fragment ğ‘ 2.  
Therefore, we only use the stack fragments ğ‘ â€² and ğ‘ 1, or ğ‘ 2 to pre-
dict events. In theory, the size of the stack ğ‘ â€² can range from 1 to 
infinite. In this paper, we aim to prese nt the basic model of To reverse an HB edge from ğ‘’1â†£ğ‘’2 to ğ‘’2â†£ğ‘’1, the thread ğ‘¡1 
should be suspended when it generates but does not execute the 
event ğ‘’1 until the thread ğ‘¡2 has executed the event ğ‘’2.  
ğ‘¡2ğ‘¡ 
ğ‘’1
ğ‘’2
(a) Observed trace ğœ1 (b) Targeted trace ğœ2ğ‘’1
ğ‘’2ğ‘¡2ğ‘¡  
Figure 4. Reversing an HB edge.  
453 
 DrFinder . Hence, we choose  the size of  ğ‘ â€² to be  1, which is a mini-
mal setting . In this case, we have ğ‘“ğ‘˜=ğ‘“1. Therefore, the two cases 
(i.e., Backward Case and Forward Case) are actually the same one. 
And the prediction of event ğ‘’2 is based on one stack fragment (i.e., 
ğ‘ 1 or ğ‘ 2) to be formed. Thus, we propose our stack based events 
prediction model M to predict the event ğ‘’2 as follows:  
 
We refer to the size of the above stack s1 plus the method ğ‘“ğ‘‘â€² as the 
depth  d of model M. Model  M looks forward to see whether there 
will be a sequence of at most d methods (i.e., ğ‘ 1) with the last 
method containing an above discussed event ğ‘’2.  
However, for above prediction model, w e still need to address a 
new problem : given an event ğ‘’1 by ğ‘¡1 and a stack s of ğ‘¡2, does there 
exist a method ğ‘“ğ‘‘â€² in the stack contains an event ğ‘’2?  
We propose the May-Trigger Relation  to further predict whether a 
given method will trigger a certain event. Suppose that there is an 
event e from a method  f such that ğ‘’=ğ‘’2. Then,  we must have 
ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘’)=ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘’2) , and ğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘’))=
ğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘œğ‘ğ‘—ğ‘’ğ‘ğ‘¡(ğ‘’2)). Our insight is that in real -world Java programs, 
most methods only acquire specific (instead of arbitrary) lock ob-
jects, and their method instances often follow the same locking pat-
terns.  We propose to use the type of a lock object in a lock acquisi-
tion event to predict the possible occurrence of the event e2 to 
achieve the reversal of the HB edges from ğ‘’1â†£ğ‘’2 in ï„1 to ğ‘’2â†£
ğ‘’1 in ï„2.  
Formally, May -Trigger Relation  is defined as follows: Given a 
method ğ‘“, a type ğ‘, and an execution trace ğœ. If a method ğ‘“â€² is 
reachable from ğ‘“ during the generation of  a trace  ğœ by a sequence 
of at most  ğ‘‘ methods , and ğ‘“â€² produce s an event ğ‘’=ïƒ¡ğ‘¡,ğ‘ğ‘ğ‘,ğ‘œïƒ± 
such that  ğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘œ)=ğ‘, then we sa y ğ‘“ and ğ‘ forms a May-Trigger 
Relation  (MTR  for short), denoted as mt(ğ‘“, ğ‘).  
DrFinder  is developed on top of M using  MTR to predict occur-
rence s of event s like  ğ‘’2 to schedule  a program to detect hidden 
races. It uses two pieces of information for its prediction: (1) an event ğ‘’1 from a thread ğ‘¡1, and (2) a method  ğ‘“1 from  a second thread  
ğ‘¡2 and a depth d. It interestingly predict s the presence of an event 
ğ‘’2 by check ing whether mt(ğ‘“1, ğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘’1)) is a MTR identified from 
ğœ1.  
4.4 Effective Scheduling via Feedbacks  
From above discussion,  DrFinder  execute s a program once to col-
lect a set of MTR  and then schedule s the program execution based 
on the relation set. However, races in a program cannot be dynam-
ically detected in merely one run. Therefore, it is necessary for 
DrFinder  to execute  a program multiple times to detect more hidden 
races.  
On the other hand, if a same set of MTR  relation is used in each 
execution by DrFinder , the increment  of new races  detected  is mar-
ginal. Actually, after the first scheduling execution  by DrFinder , the 
probability to detect new hidden races for DrFinder  at its other sub-
sequent executions is the same as the existing dynamic techniques 
(e.g., FastTrack ) and may be  even lower than existing active sched-
ulers ( e.g., PCT ). It is because, the subsequent executions are sim-
ilar to the first scheduled  execution as they are scheduled by 
DrFinder  based on a same set of MTR .  
Therefore, we design DrFinder , at each of its executions , to both 
schedule the execution and collect a new set of MTR  from the ex-
ecution  being scheduled . The newly collected set of MTR  is re-
garded as a feedback to be used in the next execution by DrFinder . 
As such, DrFinder  is able to effectively schedule each execution 
based on a set of MTR  exactly from the previous execution , to de-
tect new hidden races . This feedback mechanism is also depicted in 
Figure 1.  
5. DRFINDER IN DETAILS  
5.1 Phase I: May -Trigger Relation Collector  
The MTCollector algorithm is  responsible to collect may -trigger re-
lations,  shown in Algorithm 1 . Given a program ğ‘ and a depth ğ‘‘, 
MTCollector  executes the program , and collects  a set of may-trig-
ger relations  (i.e., MTR in Algorithm 1)  from the observed trace.  
Algorithm 1  first assigns the set of all threads in ğ‘ to the set Ena-
bled , null  to MTR, and assigns Stack (t) for each thread ğ‘¡ in En-
abled  to empty at line s 3â€“4. It uses t he data structure Stack (ğ‘¡) to 
keep track of the call stack fragment. It then uses randomized 
scheduling to execute the program ğ‘ by selecting the next event ğ‘’ During the generation of a trace ğœ2, if a thread ğ‘¡2 is executing 
an event ğ‘’âˆ— from a meth od ğ‘“ğ‘˜, and there exists a method ğ‘“ğ‘‘â€², 
such that the method ğ‘“ğ‘‘â€²: 
(1) contains an event ğ‘’2 and  
(2) is reachable by thread ğ‘¡2 via a sequence methods ğ‘ 1 after 
executing the event ğ‘’âˆ—, 
then thread ğ‘¡2 will execute the event ğ‘’2. Algorithm 1: DrFinder .MTCollector  
1.   
2.   
3.   
4.   
5.   
6.   
7.   
8.   
9.   
10.   
11.   
12.   
13.   
14.   
15.   
16.   
17.   
18.   
19.   Input : ğ‘ â€“ a given program  
Output : MTR â€“ may-trigger  relations  
Enabled  â‰” all threads in ğ‘; MTR â‰”âˆ… 
Stack (ğ‘¡) â‰”âˆ…, for each thread ğ‘¡âˆˆ Enabled  //method stack  
while  Enabled  â‰ âˆ… do 
â”‚  let ğ‘¡ be a random thread from Enabled . 
â”‚  let ğ‘’â‰”ïƒ¡ğ‘¡,ğ‘œğ‘,ğ‘œïƒ± be the next event of ğ‘¡. 
â”‚  if ğ‘œğ‘=ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ  then  
â”‚  â”‚  push  ğ‘œ into Stack (ğ‘¡) //o is a method  
â”‚  else if ğ‘œğ‘=ğ‘Ÿğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘›  then  
â”‚  â”‚  pop out from  Stack (ğ‘¡) 
â”‚  else if ğ‘œğ‘=ğ‘ğ‘ğ‘ then   
â”‚  â”‚  for ğ‘– = 1 to min(d, Stack (ğ‘¡).size()) do 
â”‚  â”‚  â”‚  let ğ‘“â‰” Stack (ğ‘¡).get(ğ‘–) //collect lock type  
â”‚  â”‚  â”‚  MTR â‰” MTR âˆª { mt(ğ‘“, Type (ğ‘œ)) } 
â”‚  â”‚  end for 
â”‚  end if 
â”‚  execute (ğ‘’) 
end while  
 
ğ‘¡2ğ‘¡1
ğ‘’1
ğ‘’2
(a) Current execution of two threadsğ‘’âˆ—
ğ‘“1
ğ‘“ğ‘˜ğ‘“ğ‘‘â€²
ğ‘’2
(b) Backward Caseğ‘“1
ğ‘“ğ‘˜
ğ‘“ğ‘‘â€²
sâ‹®
run
s' ğ‘ 1
ğ‘“ğ‘˜
ğ‘“ğ‘‘â€²â€²ğ‘’âˆ—
ğ‘’2
(c) Forward Casesrun
ğ‘ 2
ğ‘’âˆ—Stack s  
Figure 5. Call stack based prediction of event  ğ’† . 
454 
 (i.e., ïƒ¡ğ‘¡,ğ‘œğ‘,ğ‘œïƒ±) from a random thread (lines 6â€“7) and check s the op-
eration ğ‘œğ‘ of the event ğ‘’:  
ï‚· If ğ‘œğ‘ is either ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ  or ğ‘Ÿğ‘’ğ‘¡ğ‘¢ğ‘Ÿğ‘› , Algorithm 1  pushes the 
method o into the Stack (ğ‘¡) or pops out the topmost method 
from the Stack (ğ‘¡), respectively (lines 8â€“11).  
ï‚· If ğ‘œğ‘ is ğ‘ğ‘ğ‘, Algorithm 1  updates MTR â‰” MTRï€ âˆª{mt(ğ‘“, 
ğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘œ))}, for each method ğ‘“ in the top ğ‘‘ methods in the 
Stack (ğ‘¡) (lines 1 3â€“16) to maintain may-trigger  relations . 
Then, the algorithm executes  the event  at line 18.  
5.2 Part A of Phase II: DrFinder  Agent  
The Agent  (Algorithm 2) is responsible to execute each event . For 
the acq events , their executions depend on Scheduler .  
Algorithm 2 accepts the given program ğ‘, the set MTR (i.e., may-
trigger  relations ) from Phase I , and a n HB race detector RD. It re-
turns a set of new may -trigger relations (i.e., MTR' ). Algorithm 2  
firstly assigns the set of all threads of p to the set Enabled , which 
is shared by both Agent  and Scheduler , and set MTR'  to be empty 
(line 5).  It then starts Scheduler  (i.e., Algorithm 3  to be presented  in 
Section 5.3) through a fork() call (line 6). Next, it takes a next event 
ğ‘’=ïƒ¡ğ‘¡,ğ‘œğ‘,ğ‘œïƒ± from a random thread ğ‘¡ (lines 8â€“9), and checks the 
operation  ğ‘œğ‘ of the event ğ‘’. If ğ‘œğ‘ is ğ‘ğ‘ğ‘, Agent  asks Scheduler  
whether ğ‘¡ is allowed to execute ğ‘’ through a function  call to  re-
questALock () of Scheduler  (line 1 1). If the function returns a false 
value, Agent  simply keeps ğ‘’ from execution. (Note:  the thread t has 
been removed from Enabled  by Scheduler  at line 10 in Algorithm 
3). If requestALock () does not return  a false  value (line 1 2), 
Agent  will both execute and pass ğ‘’ to the race detector RD (lines 
16â€“17). Finally, if ğ‘’ is an ğ‘ğ‘ğ‘ event,  Agent  also informs Scheduler  
that the thread ğ‘¡ has acquired the lock object specified in ğ‘’ via 
function lockAcquired () (lines 18â€“20). It also collects a new set 
of MTR' at line 21 (which is based on Algorithm 1) as the input 
MTR of the next scheduling execution.   
5.3 Part B of Phase II: DrFinder  Scheduler  
Scheduler  (Algorithm 3) maintain s four data structures: ATHs , 
RTHs , allowedTH , and allowedLK  (lines 1 â€“4), to make  sched-
uling decision:   
ï‚· ATHs  is a set of all the threads in the program p.  
ï‚· RTHs  is a set of pairs of a thread  ğ‘¡ and a lock  object  ğ‘œ, each of which representing that ğ‘¡ is requesting to acquire the object 
ğ‘œ, but Scheduler  suspends this acquisition.  Thus, all the threads 
in this set are waiting to be scheduled by  Scheduler . For ease 
of our presentation , we use RTHs .get(ğ‘¡) to denote the lock  ğ‘œ 
paired with the thread ğ‘¡.  
ï‚· allowedTH  keep s a particular thread ğ‘¡ that both (i) is "es-
corted " by DrFinder  with a top priority to execute its lock ac-
quisition events and (ii) is the thread expected by DrFinder  to 
acquire a lock object defined by allowedLK . 
ï‚· allowedLK  keeps a lock object ğ‘œ. DrFinder  expects the 
thread defined  by allowedTH  to acquire a lock object having 
the same type as this lock object.   
Scheduler  consists of four functions : requestALock (), lock-
Acquired (), mayTrigger (), and schedule (). In Section 5.2, 
we have presented that  Agent  (Algorithm 2 ) invoke s the first two 
functions. We first ly present them  followed by presenting sched-
ule() which is the core part of DrFinder .  
The function requestALock () is called by Agent  on determining 
whether to execut e the event ğ‘’ (i.e., the lock acquisition on ğ‘œ by ğ‘¡). 
It checks whether the given thread ğ‘¡ is a chosen thread to execute 
any event (i.e., allowedTH ) at line 6. (As such, a targeted HB edge 
may be formed as soon as it can. ) If a true value is returned , it in-
dicates  that the event ğ‘’ is allowed to execute (line 7) ; otherwise, the 
thread ğ‘¡ is added to the set RTHs  and is also removed from the set 
Enabled  (lines 9 â€“10) so that Agent  will not pick any event of it 
for execution (line 8 in Algorithm 2 ). Next , the function notifies  
Scheduler  (that there is a thread to be scheduled, see line 30, to be 
explained below) . On the other hand, if it  return s a false  value at 
line 12 , it indicates that the event  ğ‘’ is not allowed  to be executed .  
The function lockAcquired () is called by Agent  right after a lock 
acquisition event  ğ‘’ is executed (lines 17â€“19 in Algorithm 2). It 
checks whether the executed lock acquisition event e is an event 
expected by Scheduler  (line 16). An event ğ‘’=ïƒ¡ğ‘¡,ğ‘œğ‘,ğ‘œïƒ± is an ex-
pected event if (1) ğ‘¡ is the thread defined by allowedTH  (i.e., ğ‘¡= 
allowedTH ) and  (2) ğ‘’ operates on a  lock object defined by al-
lowedLK  (i.e., ğ‘¡ğ‘¦ğ‘ğ‘’(ğ‘œ) = ğ‘¡ğ‘¦ğ‘ğ‘’ (allowedLK )) at line 16 . If so, an 
expected event occurs and Scheduler  resets  both allowedTH  and 
allowedLK  to null  (line 17 ). It then notifies function sched-
ule()that there is no thread marked  as "allowedTH " (line 18).   
We are going to explain the function schedule (). Scheduler  is 
started by Agent  (at line 5 in Algorithm 2). If no thread is requesting 
any lock  object  (i.e., RTHs .size() = 0) or there is no event expected 
by Scheduler  (i.e., allowedTH  â‰ âˆ…) at line 3 0, the function 
schedule () just waits for notifyScheduler () to notify it at ei-
ther line 11 or line 18.  
Once  schedule () is notified, the same function  selects a random 
thread t from the set ATHs. It then checks whether there is any 
thread whose current ly executing method ğ‘“ and the type of the 
given object ğ‘œ match any  may-trigger  relation kept in MTR via 
mayTrigger () (lines 35â€“39). All the threads  that match this con-
dition  are collected  as the set CT (line 3 7), in which a thread ğ‘¡â€² is 
randomly selected and escorted by DrFinder  to execute all its events 
before ğ‘¡ is allowed to continue its execution (line 4 1). The function 
then assigns the thread t' and the lock object ğ‘œ to allowedTH  and 
allowedLK , respectively (lines 4 2â€“43). If the thread ğ‘¡â€² is also in 
the set  RTHs , ğ‘¡â€² will be removed from RTHs  and added to Ena-
bled  (lines 4 4â€“46). Scheduler  then waits until the thread ğ‘¡â€² being 
escorted by DrFinder  has acquired a lock object having  the type Algorithm 2: DrFinder .Agent  
1.  
2.   
3.  
4.   
5.   
6.   
7.  
8.   
9.   
10.   
11.   
12.   
13.   
14.  
15.   
16.   
17.   
18.   
19.   
20.   
21.   
22.   Input : ğ‘ â€“ a given program.  
Input : MTR â€“ may-trigger  relations returned by MTCollector  
Input : RD â€“ an HB race detector (e.g., FastTrack ) 
Output : MTR' â€“ a set of new may-trigger  relations // feedbacks  
Enabled  â‰” all threads in ğ‘, MTR' â‰”âˆ… 
fork(schedule ()) //start scheduler (in Algorithm 3)  
while  Enabled  â‰ âˆ… do 
â”‚  let ğ‘¡ be a random thread from Enabled . 
â”‚  let ğ‘’â‰”ïƒ¡ğ‘¡,ğ‘œğ‘,ğ‘œïƒ±  be the next event of ğ‘¡. 
â”‚  if ğ‘œğ‘=ğ‘ğ‘ğ‘ then  
â”‚  â”‚  allow  â‰” DrFinder .Scheduler. requestALock (ğ‘¡, ğ‘œ) 
â”‚  â”‚  â”‚  if allow  = false  then 
â”‚  â”‚  â”‚   continue  //while loop  
â”‚  â”‚  end if 
â”‚  end if 
â”‚  execute (ğ‘’) 
â”‚  RD.onEvent (ğ‘’)  //for detection of data races  
â”‚  if ğ‘œğ‘=ğ‘ğ‘ğ‘ then  
â”‚  â”‚  DrFinder .Scheduler. lockAcquired (ğ‘¡, ğ‘œ) 
â”‚  end if 
â”‚  update  MTR'  according to Algorithm 1  
end while  
 
455 
 type(allowedLK ) (see the function lockAcquired ()) (lines 
48â€“50). Otherwise, if CT is empty ( line 4 0), the thread ğ‘¡ is allowed 
to execut e (lines 5 2â€“55).  
The function mayTrigger () accepts two threads ğ‘¡ and ğ‘¡â€² as its 
parameters.  This function firstly gets the lock object o being re-
quested by ğ‘¡ (i.e., paired with ğ‘¡ in RTHs ) and the current executing  
method ğ‘“ of ğ‘¡â€² (line 22). It then checks whether the tuple (ğ‘“, 
type(o)) is a valid May-trigger  relation (i.e., in the set  MTR ) and 
returns a true-or-false  result  accordingl y (lines 2 3â€“26).  
5.4 Discussion  
DrFinder  is an active scheduler. It suffers from thrashing [21][41], 
and may lead the execution to form deadlocks  [12][41]. Similar to 
existing techniques [9][11][12][21][41], when a thrashing occurs, 
DrFinder  randomly selects a suspended  thread to execute and when a deadlock occurs, the whole execution is restarted  (see Section 
6.1).  
DrFinder  actively sc hedules an execution to produce  HB edges that 
cannot be easily formed in normal executions to expose races. It 
drives a happens -before based detector (e.g., FastTrack ) to detect 
races precisely  and report all races. Therefore,  like other HB 
detectors,  DrFinder  is also precise .   
6. EXPERIMENT  
This section present s our evaluation on DrFinder  and its compari-
son with the state -of-the-art HB race detector FastTrack , a random 
delay scheduler ConTest , and a state -of-the-art randomized sched-
uler PCT . All these techniques are reviewed in Sections 1 and 3.  
6.1 Implementation  and Benchmarks  
Implementation . We implemented  DrFinder , FastTrack , ConTest , 
and PCT  in Jikes RVM  [2][3]. Jikes RVM is a Java virtual ma-
chine, developed almost in Java language, and could be run on 
Linux and Mac  OSX systems.  These tools report a race at the Java 
class field level [17]. Our tool  uses the shadow mechanism  [18] to 
track the state of an execution  and adds a shadow lock  to each object 
instance to keep  the vector clock data and type information . For 
each memory location  (i.e., an instance of a field of a Java class ), it 
allocate s a shadow memory to track the read s and write s to this 
memory  location . For each thread, it adds a member in the 
RVMThread class [2] to keep the Java thread data.   
To generate memory and synchronization events in runtime , our 
tool instruments each class wh en it is loaded , except those Jikes 
RVM classes and Java standard library classes . It uses a static  es-
cape analysis [5] to identify access es to provably thread loc al 
memory locatio n. It also fully tracks happens -before relation s on 
other program semantics (e.g., accesses to volatile field s [17]).  
Our implementation periodically monitors the state s of all threads 
by tracking various synchronizations events and scheduling of 
DrFinder  as well as other functions calls (e.g., sleep ()). Such 
monitor ing i s helpful to identify deadlock ocucrrences and 
thrashing occurrences.  
Benchmarks . We used the Dapaco benchmark suite  [6] to evaluate 
DrFinder . We selected two multithreaded programs from Dacapo 
2006 -10-M1 (xalan 06 and eclipse 06) and five multithreaded pro-
grams  from Dacapo 2009  (xlan09, pmd09, sunflow 09, luindex 09, 
and lusearch 09). Dacapo 2009 includes other multithreaded 
benchmarks; however, they  cannot be run on the latest Jikes  RVM 
3.1.3  even without our tool . In total , we selected 7 multithreaded  
benchmarks , including a large -scale real -word program Eclipse 
(eclipse 06).  
Table 2 shows the descripti ve statistics  of the benchmarks . The first 
two column s show the benchmark name  and size . The third column  Table 2. Descriptive and execution statistics of benchmarks . 
Benchmark  Jar Files  
 Size (KB)  # of locks / 
threads  # of methods  
(with sync)  # of HB  
 edges  
xalan 06 81.23  19,565  /   9 1,731 (1.7%)  2,607,853  
eclipse 06 41,821.53  118,803  / 26 7,581 (4.9%)  22,879,127  
xalan 09 4,826.81  10,522  /   5 1,869 (1.4%)  3,864,084  
pmd 09 2,996.30  230 /   5 2,289 (0.2%)  2,288  
sunflow 09 1,016.91  22 /   9 698 (1.6%)  778 
luindex 09 878.37  2,612  /   2 804 (14.8%)  217,343  
lusearch 09 883.02  94,668  /   5 484 (4.6%)  1,371,744  
Total  52,504.16  246,422  / 61 15,456 (3.8%)  30,943,217  
 Algorithm 3: DrFinder .Scheduler   
1.  
2.   
3.   
4.   
5.   
6.   
7.   
8.  
9.   
10.   
11.   
12.   
13.   
14.   
15.  
16.   
17.   
18.   
19.   
20.   
21.   
22.  
23.   
24.  
25.   
26.   
27.  
28.   
29.   
30.   
31.   
32.   
33.   
34.  
35.   
36.   
37.   
38.   
39.   
40.   
41.  
42.   
43.   
44.   
45.   
46.   
47.   
48.  
49.   
50.   
51.   
52.   
53.   
54.   
55.   
56.   
57.   ATHs   â‰” all threads in p 
RTHs   â‰” ïƒ† //a set of pairs of threads and locks  
allowedTH    â‰”âˆ… //a thread that is expected to acquire a lock  
allowedLK   â‰”âˆ… //the lock expected to be acquired  
Function  requestALock (ğ‘¡, ğ‘œ) 
â”‚  if allowedTH  = ğ‘¡ then  
â”‚  â”‚  return  true 
â”‚  else 
â”‚  â”‚  RTHs .add(ğ‘¡, ğ‘œ) 
â”‚  â”‚  Enabled  â‰” Enabled  \ {ğ‘¡} 
â”‚  â”‚  notifyScheduler () 
â”‚  â”‚  return false  
â”‚  end if 
end Function  
Function lockAcquired (ğ‘¡, ğ‘œ) 
â”‚  if allowedTH  = ğ‘¡ and type(allowedLK ) = type(ğ‘œ) then  
â”‚  â”‚  allowedTH  â‰”âˆ…, allowedLK â‰”âˆ… 
â”‚  â”‚  notifyScheduler () 
â”‚  end if 
end Function   
Function mayTrigger (ğ‘¡,ğ‘¡â€²) 
â”‚  let o â‰” RTHs .get(ğ‘¡), f â‰” getCurrentMethod (ğ‘¡â€²) 
â”‚  if âˆƒ mt(ğ‘“, ğ‘) âˆˆ MTR such that ğ‘ = type(ğ‘œ) then  
â”‚  â”‚  return  true 
â”‚  end if 
â”‚  return  false 
end Function  
Function schedule () 
â”‚  while  Agent  does not exit do 
â”‚  â”‚  while  RTHs .size() = 0 or allowedTH  â‰ âˆ… do 
â”‚  â”‚  â”‚  wait() 
â”‚  â”‚  end while  
â”‚  â”‚  ğ‘¡â‰” a random thread from ATHs  
â”‚  â”‚  CandidateThread CT â‰”âˆ… 
â”‚  â”‚  for each  thread ğ‘¡â€²âˆˆ ATHs  do 
â”‚  â”‚  â”‚  if ğ‘¡â‰ ğ‘¡â€² and mayTrigger (ğ‘¡, ğ‘¡â€²) then  
â”‚  â”‚  â”‚  â”‚  CT â‰” CT âˆª{ğ‘¡â€²} 
â”‚  â”‚  â”‚  end if 
â”‚  â”‚  end for       
â”‚  â”‚  if CT â‰  âˆ… then  
â”‚  â”‚  â”‚  ğ‘¡â€²â‰” a random thread from CT 
â”‚  â”‚  â”‚  allowedTH  â‰”ğ‘¡â€² 
â”‚  â”‚   â”‚  allowedLK  â‰” RTHs .get(ğ‘¡â€²) 
â”‚  â”‚  â”‚  if ğ‘¡â€²ïƒ RTHs  then  
â”‚  â”‚  â”‚  â”‚  Enabled  â‰” Enabled  âˆª{ğ‘¡â€²} 
â”‚  â”‚  â”‚  â”‚  RTHs .remove (ğ‘¡â€²) 
â”‚  â”‚  â”‚  end if 
â”‚  â”‚  â”‚  while  allowedTH  ï‚¹ ïƒ† do 
â”‚  â”‚  â”‚  â”‚  wait() 
â”‚  â”‚  â”‚  end while  
â”‚  â”‚  end if 
â”‚  â”‚  Enabled  â‰” Enabled  âˆª{ğ‘¡} 
â”‚  â”‚  RTHs .remove (ğ‘¡â€²) 
â”‚  â”‚  allowedTH  â‰”ğ‘¡ //let t to acquire the lock RTHs .get(t) 
â”‚  â”‚  allowedLK  â‰” RTHs .get(ğ‘¡) 
â”‚  end while  
end Function  
 
456 
 show s the number s of locks and threads in the benchmarks . The last 
two columns show the number s of methods (all  and those contain-
ing lock acquisitions ) and the mean  number of HB edges in each 
benchmark  over 10 runs . All dynamic data are collected under na-
tive schedul ing.  
6.2 Experiment al Setup  
Our experiment was performed on an Apple Mac Pro with 2.6GHz 
Intel Core i5 and 8GB memory ru nning OS X 10.9.2. We compiled 
Jikes RVM with GNU Class -path 0.98 [1]. We configured 
FastTrack  with the native (OS) scheduler,  with ConTest , with PCT , 
and with DrFinder , which are  refer red to as FT, CT, PCT, and DR, 
respectively . We follow ed the previous experiments  [43] and ran 
each technique on each subject for 10 times .   
6.3 Experiment al Results  
Table 3 summaries the experimental results. The first column 
shows the benchmark name. The second major column shows the 
number of distinct data races reported  by each technique  in 10 runs . 
The third major column shows the number  of new data races 
detected by DR but not detected by FT, CT, and PCT . The fourth 
major column shows that the total number of distinct races detected 
by all four techniques. The fifth major column shows the number 
of races not detected  by each technique compared to the total 
number of distinct races ( i.e., the data in the fourth major  column). 
The sixth major column  shows the mean  time in second s for each 
technique to run each benchmark . It also show s the time of  native 
run (i.e., without any  testing  tool) and the time needed by both  
phases of DR (as P-I and P-II, respectively ). The overhead of 
each technique is also calculated . The last column shows the 
thrashing rate ("Thrash. rate ")  of DR. The last row shows 
either the total ( "Sum") or the mean  value ( "Avg") of each column.   
6.3.1 Summary of Results  
Effectiveness . From Table 3 (the second and third major columns) , 
DR detect s more races on 5 out of 7 benchmarks by 11.1% to 
240.0%. On the remaining two benchmarks, all four techniques de-
tect the same set of data races. In total, DR detects 89 more  new 
races  from  all benchmark s. We find that FT, CT, and PCT detect 
almost the same set of  data races  (where the difference is at most  
10). This is consistent with an intuition that the random sleep strat-
egy used by CT is not quite effective  and randomized scheduling 
strategy used by PCT  is also not quite effective without a larger 
number of runs.  
Performance . From the column  on time , we observe that FT has 
the best performance, which is expected. The overhea d of CT on 
top of  FT ranges  from 0.1 1x to 5.28x except on eclipse 06. For 
PCT , it incurs about 2.2x higher overhead than DR on average .1 
                                                                 
1 Note that, there is a parallel version of PCT  known as PPCT  [33] that has the same effectiveness but runs faster  than PCT . Hence , the time overhead of PCT  in Table 3 is for 
reference only and we do not discuss the overhead of PCT  in the next paragraph.  We believe our DrFinder  could also be implemented in parallel and we leave it as a future work.   The overhead of the Phase I of DR is only 0.39 x. The overhead of 
DR (Phase II) on top of FT ranges  from 1.1 x to 4.0 x. On average, 
DR only incurs 2.65x on top of FT. 
On eclipse 06, CT incurs  a heavy overhead, which is 19.5x on top 
of FT; but, DR only incurs 1.1x on top of  FT. Although eclipse 06 
include d 26 threads, in most of the execution time,  there are only 
two active thread s. We find that  DR is able to suspend these two 
threads according to their locking orders  most of the time . But, CT 
has to delay each lock acquisition by a random period . As a result , 
the total time overhead of CT is much heav ier than that of DR on 
this benchmark.   
Thrashing Rate . From the last column of Table 3, DR is able to 
make successful thread suspension decision s in nearly  60% of all 
cases. In the remaining cases  (40.73%) , thrashing occurred. We 
have in spected these thrashing occurrences and found  that about 
half of them were caused due to the type of the unique and global 
instance of Class Loader  class used by the Dacapo t est harness to 
load each class instead of the program under test. (This harness 
strictly speaking is not a part of each benchmark.) In our 
experiment, we have not seen any deadlock occurrence.  
6.3.2 Comparison on Not Detected  Races  
Table 3 also show s the total number of distinct races detected by all 
four techniques in the fourth major column . It also shows  the num-
ber of races that are not detected  by each technique but detected by 
other three techniques in the fifth major column. Overall  speaking , 
among all 4 74 detected races  detected by all four techniques  on all 
the benchmarks, DR only miss es 26 races; however, FT, CT, and 
PCT  misse s 96, 102, and 89 races, respecti vely.  
From above analyses, we find that DR is effective in exposing  hid-
den races; but it may be unable to expose some  races  that can be 
detected by HB detectors with random ized or native schedul ing. 
We argue that  this is not a major issue. It is because , in practice, 
one may run a program with random or native scheduling to detect 
these  races (e.g., configured in Phase I of DR) followed  by detect-
ing the hidden races in Phase II of DR. We have checked the races 
not detected  by DR and found that almost all 26 races have been  
detected by FT in each of 10 runs , and the remaining ones can be 
detected by FT in at least one run .  
6.3.3 Comparison on Races Detected in 10 Runs 
Figure 6 shows the cumulative  number of races detected in the ex-
periment  by the four techniques on each benchmark except on 
luindex 09 (on which, all three techniques detected exactly one 
race in each run). The x -axis shows these 10 runs  and y -axis shows 
the cumulative  number of races  detected .  Table 3. Summary of results on 10 runs for FastTrack  (FT) , ConTest  (CT) , PCT , and DrFinder (DR) (depth d = 12). 
Bench - 
mark  Total races  by New races  
by DR (%) Total  
races  Missed races  Time in seconds (slowdown factor)  Thrash . 
 rate FT CT PCT  DR FT CT PCT  DR Native  FT CT PCT  DR (P-I) DR (P-II) 
xalan 06 16 16 18 26 13 (+72.2%) 31 15 15 13 5 5.62 30.7 (5.5)  60.3 (10.7)  63.4 (11.3)  11.8 (2.1)  52.3 (9.3)  45.1% 
eclipse 06 313 308 318 351 54 (+17.0%) 372 59 64 54 21 45.63  119 (2.6)  1,007.9 (22.1)  178.2 (3.9)  56.4 (1.2)  167.2 (3.7)  79.2% 
xalan 09 12 12 12 20 8 (+66.7%)  20 8 8 8 0 5.43 24.6 (4.5)  35.4 (6.5)  61.8 (11.4)  9.2 (1.7)  41.0 (7.6)  10.9% 
pmd 09 18 17 18 20 2 (+11.1%)  20 2 3 2 0 3.58 6.9 (1.9)  7.3 (2.0)  15.6 (4.4)  4 (1.1)  12.7 (3.5)  35.1% 
sunflow 09 5 5 5 17 12 (+240.0%)  17 12 12 12 0 10.36  63.0 (6.1)  84.1 (8.1)  136.7 (13.2)  12.7 (1.2)  99.7 (9.6)  68.7%  
luindex 09 1 1 1 1 0 (+0.0%)  1 0 0 0 0 2.41 8.4 (3.5)  9.8 (4.1)  21.7 (9.0)  3.2 (1.3)  11.9 (4.9)  33.3%  
lusearch 09 13 13 13 13 0 (+0.0%)  13 0 0 0 0 6.18 17.2 (2.8)  21.1 (3.4)  49.1 (7.9)  6.8 (1.1)  42.1 (6.8)  12.8% 
Sum: 378 372 385 448 89 (+23.1%)  474 96 102 89 26 Avg:  3.84 8.14 8.7 1.39 6.49 40.73%  
 
457 
 Figure 6 shows that FT, CT, and PCT  almost always detect the 
same number s of races except on eclipse 06. This indicate s that 
they have similar race detection ability  among all 10 runs  (where 
the detected races are almost the same ones) . Whereas, DR has an 
increasing trend on the number of detected races . Apparently, DR 
may detec t fewer races in some runs  (e.g., the first five runs on xa-
lan09). But, we have shown in  Table 3 that DR actually expose s 
more races.   
To measure the ability  of DR on the detection of new races with 
increasing number of runs, we further analyze  the cumulative  
number of 89 new data races detected by DR in the first "i" (where 
i is from 1 to 10) runs. We normalize this cumulative  number  by 
the total number of new races detect ed by DR on the correspond ing 
benchmark. The result  is shown in Figure 7. Note that  we do not 
show the result  on luindex 09 and lusearch 09 in Figure 7 as no 
new races is detected on them .  
Figure 7 shows that on each benchmark except eclipse 06, all the 
new races detected by  DR were  detected in the first 7 runs. On 
eclipse 06, almost on each run, more new races were detected by DR; and moreover , more than 71.43 % new races were detected in 
the first 4 runs. Therefore, we tend to believe that DR is able to 
effectively detect hidden races , even on large -scale multithreaded 
programs (e.g., eclipse 06), which cannot be detected by FT, CT, 
and PCT in 10 runs ( or even  up to  100 runs , see Section 6.3.5 ).  
6.3.4 DrFinder  with Different Depths 
In our main experiment, we have set the depth to 12. To evaluate 
the ability of DR on its detection of hidden races with different 
depths, we repeated the main  experiment for DR but set the depth 
from 2 to 20 with step 2  in turn, where each configuration was also 
conducted for 10 runs. The results are summarized in  Table 4. In 
each data cell, the format is "x (y)" where  the "x" is the total number 
of races detected by DR with corresponding depth and the "y" is the 
number of new races that cannot be  detected by FT, CT, and PCT  
in their10 runs. On each benchmark, if DR is able to detect mo st 
new races  among all its depths , we mark the corresponding  cell 
with gray color. Similarly, we do not mark cells corresponding to 
luindex 09 and lusearch 09 (as the data in the either entire row 
shows that there  is no new race detected ).  
From Table 4, we observe that with different depths, DR is gener-
ally able to detect more races  than that detected by FT, CT, and 
PCT . DR also detects  new races  in almost all  these depths , where 
the exception is on sunflow 09.  
Another observation  from Table 4 is that when depth values are 
within 4 and 12, DR is likely to detect a significant amount of  new 
races  on top of FT, CT, and PCT . And these depths values also 
lead to a larger amounts of total races. We also highlight these cells 
in gray color in the last row of Table 4.  
In future , we will study both how the depth  values affect the ability 
of DR and non -parametric strategies  to determine the depth .  
6.3.5 Further Evaluation  on New Races  
In the 10 runs by all techniques, DR detect s 89 new races. We 
further repeatedly ran other three techniques more times  until either  
Figure 6. Number of distinct races detected in 10 runs by FT, CT, 
PCT, and DR. 
5101520253035
12345678910# of data racesFT CT PCT DR
050100150200250300350400
12345678910# of data races
05101520
12345678910# of data races
14161820
12345678910# of data races
48121620
12345678910# of data races
1011121314
12345678910# of data races(a)xalan06 (b)eclipse06
(c)xalan09 (d)pmd09
(e)sunflow09 (f)lusearch09 
Figure 7. Cumulative  number of new races detected by DR in each 
run but missed in all the 10 runs by FT, CT, and PCT. 
0%20%40%60%80%100%
1 2 3 4 5 6 7 8 9 10Percentage of accumulated 
number of new data races
10 runsxalan06 eclipse06 xalan09 pmd09 sunflow09
Table 4. The total number of data races detected by DR with depths from 2 to  20 (with Step 2)  in 10 runs.  
Bench - 
mark  DR with a different Depth (total number of races and number of new races)  
2 4 6 8 10 12 14 16 18 20 
xalan 06 28 (16)  31 (15)  30 (15)  27 (15)  27 (15)  26 (1 3) 31 (15)  26 (15) 27 (15)  27 (15)  
eclipse 06 306 (13)  337 (43)  336 (42)  343 (46)  336 (41)  351 (54) 316 (21)  301 (18)  284 (15)  312 (15)  
xalan 09 18 (7)  18 (7)  18 (7)  18 (6)  18 (7)  20 (8) 17 (6)  12 (6)  11 (5)  13 (1)  
pmd 09 20 (2)  18 (1)  19 (2)  20 (2)  20 (2)  20 (2)  20 (2)  20 (2)  20 (2)  20 (2) 
sunflow 09 5 (0)  5 (0)  6 (0)  5 (0)  14 (9)  17 (12)  6 (1)  6 (0) 5 (0)  5 (0)  
luindex 09 1 (0)  1 (0)  1 (0)  1 (0)  1 (0)  1 (0)  1 (0)  1 (0)  1 (0)  1 (0)  
lusearch 09 13 (0)  13 (0)  13 (0)  13 (0)  12 (0)  13 (0)  13 (0)  13 (0)  13 (0)  13 (0)  
Sum:  391 (38)  423 (66)  423 (66)  427 (69)  428 (74)  448 (89 ) 404 (45)  379 (41)  361 (37)  391 (33)  
 
458 
 (1) they detect the same number of distinct races as that detected by 
DR or (2) the number of runs is up to 100 on each benchmark except 
on luindex 09 and lusearch 09. On these two benchmarks, other 
three techniques  already detect  the same set of races as DR in 10 
runs.  
We found  that on eclipse 06, xalan 06, xalan 09, and pmd09, other 
three techniques  could not detect  as many races even exhausting  all 
100 runs  as what DR detects in the 10 runs . On sunflow 09, all three 
techniques need more than 25 runs to catch up with DR.  
Because eclipse 06 is the largest one in our benchmarks, w e fur-
ther analyze d the ability of FT, CT, and PCT  on detect ing the 56 
new races from  eclipse 06 that are only detected by DR but missed 
by all the other three  in their 10 runs  in the main experiment . The 
result is shown in Figure 8, where we also list the 56 races for com-
parison . It shows that FT, CT, and PCT  were able to detect only 
35, 13, and 19 races  out of the 56 races in all 100 runs . (Note that, 
in a run, FT detect s 17 races. Excluding thes e 17 races, it detect s 
less races than that by PCT  in all  100 runs. ) This experiment, o nce 
more, illustrates that DR is effec tive on detecting hidden races.   
7.  RELATED WORK  
Many techniques on data race detection  have been proposed . They  
mainly fall into two groups: static techniques [22] [32][36] [46] and 
dynamic techniques [17][35][40][43][49]. Static techniques like 
RELAY  [46] and LockSmith  [36] rely on statica lly but imprecisely 
identifying memory -accessing statements that may concurrently 
visit same  memory location s without the protection of the same 
locksets. Chord  [32] reduces the number of false warnings by using 
several stages of refinement on the entire data race warning set. But, 
it loses the soundness guarantee of reporting all data races in a pro-
gram.   
Many dynamic detectors use the locking discipline  [40][42] to pre-
dict races . However, this discipline is not necessarily to be  obeyed 
even for data -race-free programs so that many false positives may 
be generated using such a  strategy . HB based dynamic ones  [17] 
[35] can precisely report  data race s. However, they are  sensitive to 
particular thread interleaving (even with improvement [45][47]) 
which provides less coverage than those  using the lockset strategy .  
RaceMob  [23] statically detect s data race warnings and distribut es 
them to a large number of user processes to validate real races. 
However RaceMob  only works on limited scenarios where a distrib-
uted user site computation is available. Active testing techniques 
[41][37] need  runs for confirmation after an imprecise race detec-
tion phase . In such a  run, the schedules are guided by the set of data 
race warnings to trigger real data races. This kind of approach is 
able to confirm real races but cannot eliminate false positives.   DrFinder takes another approach by using a precise data race detec-
tor, i.e., FastTrack  [17], in the first place. With the inherent limita-
tion of the sensitivity  on thread interleaving, a n effective thread 
scheduling technique such as DrFinder is a desirable  complement 
with HB detectors  (if used in our Phase I) to provide precise data 
race reports with high coverage.   
Thread Scheduling  techniques are more promising to detect races  
than pure stress testing . Systematic scheduling techniques such as 
model checking [48][30], are in theory able to exhaustively execute 
every schedule. However, due to the state explosion problem, enu-
merating  each schedule is not practical for real -world programs. 
Chess [30] sets a heuristic bound on the number of pre-emption s to 
explore the schedules. Also,  although systematic approaches avoid 
executing  previously explored schedules  and are more scalable  than 
pure model  check ing techniques [14], they usually incur large over-
head s and fail to scale up to handle long running  programs. Alt-
hough improvement for Chess  exists [4], finding the positions for 
such bounded exhaustive exploration from a large trace to effec-
tively expose hidden races is challenging [24]. 
Another type of scheduling technique  is based on some coverage 
criteri a of concurrent programs [7][19]. For example, Maple  [50] 
relies on patterns  (i.e., iRoots  [50]) to mine certain coverage to ex-
pose concurrency bugs. However, Maple  is insensitive to detect 
races requiring reversing more than one HB edge . Existing experi-
ments (e.g., [10]) have shown that on a large benchmark like Chro-
mium , there may be 16 million or more HB edges in a trace. It is 
challenging to select an effective subset of all such HB edges to 
confirm  given  patterns  as Maple  is designed to confirm one pattern 
per confirmation run . Besides,  the relation between the coverage of 
a specific metric and targeted concurrency  bugs cannot be verified 
in theory. A previous empirical study [26] has shown that different 
criteri a have different effectiveness on different testing techniques. 
This increases the difficulty of choosing a suitable criterion.  
DrFinder  is specially designed to detect hidden races  based on our 
may-trigger relation . Unlike above reviewed systematic scheduling 
techniques or coverage based techniques that have to restrict their 
scheduling bound s [30][50], DrFinder  is able to scale up to large -
scale programs (i.e., Eclipse in our ben chmark) and does not require 
any bug patterns.  
8. CONCLUSION  
This paper  present s a dynamic technique  DrFinder  to detect hidden 
races in multithreaded Java programs . It tries to revers e possible 
HB edges  based on a type based May -trigger Relation . The experi-
ment shows that DrFinder  is promising in detecting hidden races  
and detected 89 news races that were missed by existing techniques 
FastTrack , ConTest , and PCT . Many new races detected by 
DrFinder  in 10 runs cannot be detected by other techniques even in 
100 runs. DrFinder  is also efficient as it incurs less overhead than 
other active scheduling techniques CT and PCT . In future, we will 
extend our basic model of DrFinder  proposed in this paper to fur-
ther validate its ability on detection of hidden data races.  
9. ACKNOWLEDGEMENTS  
We thank anonymous reviewers for their invaluable comments and 
suggestions on improving this work. We also thank Dr. W.K. Chan 
and Mr. Chunbai Yang at City University of Hong Kong for their 
help on the preliminary version of this work. This work is partially 
supported by the National Natural Science Foundation of China 
(NSFC) under grant No. 91418206.    
Figure 8. Cumulative  effectiveness of FT, CT, and PCT in 100 runs on  
the 56 races that can only be detected by DR in 10 runs . 
56
35
1319
08162432404856
0 10 20 30 40 50 60 70 80 90 10056 races on eclipse06DR FT CT PCT
459 
 10. REFERENCE S 
[1] GNU Classpath, version 0.98, https://www.gnu.org/soft-
ware/classpa th/.  
[2] Jikes RVM 3.1.3. http://jikesrvm.org/ .  
[3] B. Alpern, C.R. Attanasio, A. Cocchi, D. Lieber, S. Smith, T. 
Ngo, J.J. Barton, S.F. Hummel, J.C. Sheperd, and M. Mer-
gen. Implementing jalapeÃ± o in Java. In Proc.  OOPSLA , 314 â€“
324, 1999.  
[4] S. Bindal, S. Bansal, A. Lal. Variable and thread bounding 
for systematic testing of multithreaded programs. In Proc . IS-
STA, 145 â€“155, 2013.  
[5] M.D. Bond, K. E. Coons and K. S. Mckinley. PACER: Pro-
portional detection of data races. In Proc . PLDI , 255 â€“268, 
2010 . 
[6] S.M. Blackburn, R. Garner, C. Hoffmann, A.M. Khang, K.S. 
McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, 
S.Z. Guyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, J. Eliot 
B. Moss, A. Phansalkar, D. StefanoviÄ‡, T. VanDrunen, D. 
von Dincklage, and B. Wiedermann. The Dacapo bench-
marks: Java benchmarking development and analysis. In 
Proc . OOPSLA , 169 â€“190, 2006.  
[7] A. Bron, E. Farchi, Y. Magid, Y. Nir, and S. Ur.  Applications 
of synchronization coverage. In Proc . PPoPP , 206â€“212, 
2005.  
[8] S. Burckhardt, P. Kotha ri, M. Musuvathi, and S. Nagarakatte. 
A randomized scheduler with probabilistic guarantees of 
finding bugs. In Proc . ASPLOS , 167 â€“178, 2010.  
[9] Y. Cai, and W.K. Chan. MagicFuzzer: scalable deadlock de-
tection for large -scale applications. In Proc . ICSE , 606 â€“616, 
2012.  
[10] Y. Cai and W.K. Chan. Magiclock: scalable detection of po-
tential deadlocks in large -scale multithreaded programs. 
IEEE Transactions on Software Engineering  (TSE), 40(3), 
266â€“281, 2014.  
[11] Y. Cai, C.J. Jia, S.R. Wu, K. Zhai, and W.K. Chan. ASN: a 
dynamic barrier -based approach to confirmation of dead-
locks from warnings for large-scale multithreaded programs.  
IEEE Transactions on Parallel and Distributed Systems  
(TPDS ), 26(1) , 13â€“25, 2015.  
[12] Y. Cai, S.R. Wu, and W.K. Chan. ConLock: A constraint -
based approach to dynamic checking on deadlocks in multi-
threaded programs. In Proc . ICSE , 491â€“502, 2014 .  
[13] Y. Cai, K. Zhai, S.R. Wu, and W.K. Chan. TeamWork: syn-
chronizing threads globally to detect real  deadlocks for mul-
tithreaded programs . In Proc . PPoPP , 311 â€“ 312, 2013.  
[14] E.M. Clarke, E.A. Emerson, and A.P. Sistla. Automatic veri-
fication of finite -state concurrent systems using temporal 
logic specifications. ACM Transactions on Programming 
Languages and  Systems  (TOPLAS ), 8(2), 244â€“263, 1986.  
[15] O. Edelstein, E. Farchi, Y. Nir, G. Ratsaby, and S. Ur. Mul-
tithreaded java program test generation. In IBM Systems 
Journal , 111 â€“125, 2002.  
[16] M. Eslamimehr and J. Palsberg. Race directed scheduling of 
concurrent program s. In Proc . PPoPP , 301 â€“314, 2014.  
[17] C. Flanagan and S. N. Freund. FastTrack: efficient and pre-
cise dynamic race detection. In Proc . PLDI , 121 â€“133, 2009.  
[18] C. Flanagan and S. N. Freund. The RoadRunner Dynamic 
analysis framework for concurrent programs. In Proc . 
PASTE , 1â€“8, 2010.  [19] S. Hong, J. Ahn, S. Park, M. Kim, and M.J. Harrold. Testing 
concurrent programs to achieve high synchronization cover-
age. In Proc . ISSTA , 210 â€“220, 2012.  
[20] J. Huang, P.O. Meredith, and G. Rosu. Maximal sound pre-
dictive race detection with control flow abstraction. In Proc . 
PLDI , 337 â€“348, 2014.  
[21] P. Joshi, C.S. Park, K. Sen, amd M. Naik. A randomized dy-
namic program analysis technique for detecting real dead -
locks. In Proc . PLDI , 110 â€“120, 2009.  
[22] V. Kahlon, Y. Yang, S. Sankaranarayanan, and A. Gupta. 
Fast and accurate static data -race detection for concurrent 
programs. In  Proc . CAV , 226 â€“239, 2007 . 
[23] B. Kasikci, C. Zamfir, and G. Candea. RaceMob: 
Crowdsourced data rac e detection. In Proc . SOSP , 406 â€“422, 
2013.  
[24] M. Kusano and C. Wang. Assertion guided abstraction: a co-
operative optimization for dynamic partial order reduction. In 
Proc . ASE, 2014. To Appear.  
[25] L. Lamport. Time, clocks, and the ordering of events. Com-
munications of the ACM 21(7):558 â€“565, 1978.  
[26] Z. Letko, T. Vojnar, and B. KË‡rena. Coverage metrics for sat-
uration -based and search -based testing of concurrent soft-
ware. In Proc. RV , 177â€“192, 2011.  
[27] N.G. Leveson and C. S. Turner. An investigation of the 
Therac-25 accidents. Computer , 26(7), 18 â€“41, 1993.  
[28] S. Lu, S. Park, E. Seo, and Y.Y. Zhou, Learning from mis-
takes: A comprehensive study on real world concurrency bug 
characteristics. In Proc . ASPLOS , 329 â€“339, 2008.  
[29] D. Marino, M. Musuvathi, and S. Narayanasamy. LiteRace: 
effective sampling for lightweight data -race detection. In 
Proc . PLDI , 134 â€“143, 2009.  
[30] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, 
and I. Neamtiu. Finding and reproducing heisenbugs in c on-
current pro grams. In Proc. OSDI , 267â€“280 2008.  
[31] W.N. Sumner and X. Zhang. Memory indexing: canonicaliz-
ing addresses across executions. In Proc . FSE, 217 â€“226, 
2010.  
[32] M. Naik, A. Aiken, and J. Whaley. Effective static race de-
tection for Java. In  Proc . PLDI , 308â€“319, 2006.  
[33] S. Nagarakatte, S. Burckhardt, M. M.K. Martin, and M. 
Musuvathi. Multicore acceleration of priority -based sched-
ulers for concurrency bug detection. In Proc . PLDI , 2012, 
543â€“554, 2012.  
[34] S. Narayanasamy, Z. Wang, J. Tigani,  A. Edwards, and B. 
Calder. Automatically classifying benign and harmful data 
races using replay analysis. In Proc . PLDI , 22â€“31, 2007.  
[35] E. Pozniansky and A. Schuster. Efficient on -the-fly data race 
detection in multithreaded C++ programs. In Proc . PPoPP , 
179â€“190, 2003.  
[36] P. Prati kakis, J.S. Foster, and M. Hicks. LOCKSMITH: con-
text-sensitive correlation analysis for race detection. In  Proc . 
PLDI , 320â€“331, 2006 .  
[37] C.S. Park, K. Sen, P. Hargrove, and C. Iancu. Efficient data 
race detection for distributed memory parallel programs. In  
Proc . SC, 2011.  
[38] K. Poulsen. Software bug contributed to blackout. 
http://www.securityfocus.com/news/8016, Feb. 2004  
460 
 [39] N. Rungta, E.G. Mercer, W. Visser. Efficient testing of con-
current programs with abstraction -guided symbolic execu-
tion. In Proc. SPIN , 174 â€“191, 2009.  
[40] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro and T. An-
derson. Eraser: a dynamic data race detector for multi-
threaded programs. ACM TOCS , 15(4) , 391â€“411, 1997 . 
[41] K. Sen. Race Directed Random Testing of Concurrent Pro-
grams. In Proc . PLDI , 11â€“21, 2 008. 
[42] K. Serebryany and T. Iskhodzhanov. ThreadSanitizer: data 
race detection in practice. In  Proc. WBIA , 62â€“71, 2009 .  
[43] Y. Smaragdakis, J. Evans, C. Sadowski, J. Yi, and C. Flana-
gan. Sound predictive race detection in polynomial time. In 
Proc . POPL , 387â€“400, 2012.   
[44] F. Sorrentino, A. Farzan, and P. Madhusudan. PENELOPE: 
weaving threads to expose atomicity violations. In Proc . FSE, 
37â€“46, 2010.  
[45] K. Vineet and C. Wang. Universal causality graphs: a precise 
happens -before model for detecting bugs in concur rent pro-
grams. In Proc . CAV , 434 â€“449, 2010.  
[46] J.W. Voung, R. Jhala, and S. Lerner. RELAY: static race de-
tection on millions of lines of code. In  Proc . FSE, 205â€“214, 
2007 . [47] C. Wang, K. Hoang. Precisely Deciding Control State Reach-
ability in Concurrent Traces with Limited Observability. In 
Proc . VMCAI , 376 â€“394, 2014.  
[48] C. Wang, M. Said, and A. Gupta. Coverage guided system-
atic concurrency testing. In  Proc . ICSE , 221â€“230, 2011 . 
[49] X.W. Xie and J.L. Xue. A cculock : Accurate and Efficient de-
tection of data races. In Proc . CGO , 201 â€“212, 2011.  
[50] J. Yu, S. Narayanasamy, C. Pereira, and G. Pokam. Maple: a 
coverage -driven testing tool for multithreaded programs. In 
Proc . OOPSLA , 485 â€“502, 2012.  
[51] Y. Yu, T. Rodeheffer, and W. Chen. RaceTrack: efficient de-
tection of data race conditions via adaptive tracking. In Proc . 
SOSP , 221 â€“234, 2005.  
[52] K. Zhai, B.N. Xu, W.K. Chan, and T.H. Tse. CARISMA: a 
context -sensitive approach to race -condition sample -instance 
selection  for multithreaded applications. In Proc . ISSTA , 
221â€“231, 2012.  
[53] H. Zhu. A formal analysis of the subsume relation between 
software test adequacy criteria. IEEE Transactions on Soft-
ware Engineering  (TSE), 22(4), 248 â€“255, 1996.  
 
461