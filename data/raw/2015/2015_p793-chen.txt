Guided Differential Testing of CertiÔ¨Åcate Validation in
SSL/TLS Implementations
Yuting Chen
Department of Computer Science and Engineering
Shanghai Jiao Tong University, China
chenyt@cs.sjtu.edu.cnZhendong Su
Department of Computer Science
University of California, Davis, USA
su@cs.ucdavis.edu
ABSTRACT
CertiÔ¨Åcate validation in SSL/TLS implementations is critical for
Internet security. There is recent strong effort, namely frankencert ,
in automatically synthesizing certiÔ¨Åcates for stress-testing certiÔ¨Åcate
validation. Despite its early promise, it remains a signiÔ¨Åcant chal-
lenge to generate effective test certiÔ¨Åcates as they are structurally
complex with intricate syntactic and semantic constraints.
This paper tackles this challenge by introducing mucert , a novel,
guided technique to much more effectively test real-world certiÔ¨Åcate
validation code. Our core insight is to (1) leverage easily accessible
Internet certiÔ¨Åcates as seed certiÔ¨Åcates, and (2) diversify them by
adapting Markov Chain Monte Carlo (MCMC) sampling. The diver-
siÔ¨Åed certiÔ¨Åcates are then used to reveal discrepancies, thus potential
Ô¨Çaws, among different certiÔ¨Åcate validation implementations.
We have implemented mucert and extensively evaluated it against
frankencert. Our experimental results show that mucert is signiÔ¨Å-
cantly more cost-effective than frankencert. Indeed, 1Kmucerts
(i.e., mucert-mutated certiÔ¨Åcates) yield three times as many distinct
discrepancies as 8Mfrankencerts (i.e., frankencert-synthesized cer-
tiÔ¨Åcates), and 200mucerts can achieve higher code coverage than
100;000frankencerts. This improvement is signiÔ¨Åcant as it incurs
much cost to test each generated certiÔ¨Åcate. We have analyzed and
reported 20+ latent discrepancies (presumably missed by franken-
cert), and reported an additional 357 discrepancy-triggering certiÔ¨Å-
cates to SSL/TLS developers, who have already conÔ¨Årmed some of
our reported issues and are investigating causes of all the reported
discrepancies. In particular, our reports have led to bug Ô¨Åxes, active
discussions in the community, and proposed changes to relevant
IETF‚Äôs RFCs. We believe that mucert is practical and effective for
helping improve the robustness of SSL/TLS implementations.
More information on mucert and our results can be found at
http://stap.sjtu.edu.cn/~chenyt/mucert.html .
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging‚Äî Testing
tools (e.g., data generators, coverage testing)General Terms
Algorithms
Keywords
Differential testing, mutation, certiÔ¨Åcate validation
1. INTRODUCTION
Secure Sockets Layer (SSL) [27] and Transport Layer Secu-
rity (TLS) [24] are cryptographic protocols for security protec-
tion over the Internet. Various implementations and libraries ( e.g.,
OpenSSL [1] and NSS [2]) exist to support the protocols; they fa-
cilitate the incorporation of SSL/TLS in user applications. All web
browsers also support users in establishing SSL/TLS connections.
X.509 certiÔ¨Åcates provide the principal medium for websites
and Internet users to authenticate each other and establish secure
SSL/TLS connections. For example, when a web browser requests
anhttps connection to a website, it will retrieve the site‚Äôs X.509
certiÔ¨Åcate and validate it. If the certiÔ¨Åcate fails in validation, the
browser will display a warning message to the user, who may then
refuse this connection. But similar to any real-world software,
SSL/TLS implementations or libraries may contain defects, and
in particular may not validate X.509 certiÔ¨Åcates correctly, making
certiÔ¨Åcation validation the most dangerous code in the world [28].
Indeed, certiÔ¨Åcate validation has been completely broken in many
security-critical applications and libraries [28, 31]; defects can be
embedded into certiÔ¨Åcate validation code, making SSL/TLS connec-
tions completely vulnerable or insecure.
CertiÔ¨Åcate validation mainly checks, given a server certiÔ¨Åcate,
whether it is well formed, whether it has not expired, and whether
it is issued by a trusted certiÔ¨Åcate authority (CA). However, all
SSL/TLS implementations validate X.509 certiÔ¨Åcates by following
a complicated, ad-hoc process described in several RFCs (including
RFC 2246, 2527, 2818, 4346, 5246, 5280, 6101, 6125) [19, 21 ‚Äì24,
27, 40, 41]. Developers must deÔ¨Åne their respective validation poli-
cies for handling ambiguous descriptions ( e.g., ‚Äúthe serial number
MUST be a positive integer assigned by the CA to each certiÔ¨Åcate
... non-conforming CAs may issue certiÔ¨Åcates with serial numbers
that are negative or zero. CertiÔ¨Åcate users SHOULD be prepared to
gracefully handle such certiÔ¨Åcates ‚Äù [21]). Developers can also make
minor mistakes, such as misunderstanding the SSL/TLS application
program interfaces (APIs), using insecure middleware or libraries,
and breaking/disabling certiÔ¨Åcate validation [28].
Furthermore, existing SSL/TLS implementations are not ade-
quately tested before being released, as test certiÔ¨Åcates have to be
designed elaborately and mainly manually. One main reason is that
X.509 certiÔ¨Åcates are themselves structurally complex data: each
certiÔ¨Åcate is composed of several Ô¨Åelds for identifying itself and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission and/or a
fee. Request permissions from Permissions@acm.org.
ESEC/FSE‚Äô15 , August 30 ‚Äì September 4, 2015, Bergamo, Italy
c2015 ACM. 978-1-4503-3675-8/15/08...$15.00
http://dx.doi.org/10.1145/2786805.2786835
793a sequence of extensions, each Ô¨Åeld can encompass semantic and
syntactic constraints, and certiÔ¨Åcates must be carefully organized
into certiÔ¨Åcate chains. Thus, any test certiÔ¨Åcate must be constructed
to conform to or deliberately violate the constraints, which makes
manual testing inevitably inadequate.
Brubaker et al. [13] propose the Ô¨Årst automated technique, called
frankencert , to randomly combine parts of real certiÔ¨Åcates for differ-
entially testing various SSL/TLS implementations. This is a strong
effort, but the ‚Äúblind‚Äù nature of frankencert makes it cost-ineffective:
an enormous number of frankencerts are generated and tested, so it
is very resource-intensive, but most of the frankencerts do not trigger
any discrepancies. In particular, 8,127,600 frankencerts could only
yield 208 discrepancies, which further reduce to only 9 distinct ones,
among the many SSL/TLS implementations [13].
Inspired by Brubaker et al. ‚Äôs work and recognizing its limitation,
we aim to generate effective test certiÔ¨Åcates, and in particular our
goal is to generate ‚Äúdiverse‚Äù certiÔ¨Åcates for testing. By diverse, we
mean, for example, that some certiÔ¨Åcates should pass validation and
some should not, the certiÔ¨Åcates take different control-Ô¨Çow paths,
and they enforce various validation policies or lead to different types
of exceptions. We cast effective test certiÔ¨Åcate generation as an
optimization problem: Given certiÔ¨Åcates Cert =fcert 0;cert 1;
:::; cert ng, construct Cert0=fcert0
0; cert0
1; :::;cert0
ngwhose
certiÔ¨Åcates are as diverse as possible .
To this end, we introduce mucert , a novel, guided approach to
differential testing of certiÔ¨Åcate validation. In particular, mucert
adapts Markov Chain Monte Carlo (MCMC) sampling to diversify
certiÔ¨Åcates. MCMC methods are a class of algorithms for sampling
from a probability distribution by constructing a Markov chain that
converges to the desired distribution [18, 36]. For many intractable
problems without exact optimization algorithms, MCMC sampling
provides a general solution [42]. Section 2 discusses detailed design
and technical challenges that we tackle to realize mucert, such as
choosing the acceptance condition and mutation operations.
This paper makes the following main contributions:
Problem formulation . We cast the difÔ¨Åcult problem of cer-
tiÔ¨Åcate generation as an optimization problem, which allows
us to leverage the many easily accessable Internet certiÔ¨Åcates
and transform them to effectively test certiÔ¨Åcate validation
logic. This high-level view is general and may be applicable
in other settings with structurally complex test inputs.
MCMC-guided certiÔ¨Åcate mutation . We adapt MCMC sam-
pling to effectively diversify certiÔ¨Åcates. In particular, we
use code coverage to guide the sampling process to accept
and retain representative certiÔ¨Åcates in the test suite. To our
knowledge, this work is the Ô¨Årst to utilize MCMC sampling
for generating diverse test inputs in differential testing.
Implementation and evaluation . We have implemented mucert
and compared it against frankencert and two other mutation
algorithms on 9 real-world SSL/TLS implementations. Our
results show that mucert signiÔ¨Åcantly outperforms frankencert
and the other techniques. Most notably, 1Kmucerts lead
to 3as many distinct discrepancies as 8Mfrankencerts,
demonstrating that mucert effectively diversiÔ¨Åes certiÔ¨Åcates.
Community feedback and impact . We have also reported 20+
issues and an additional 357discrepancy-triggering certiÔ¨Å-
cates, and have already received conÔ¨Årmations and positive
feedback from the SSL/TLS developers. For example, as a re-
sult of our reports, ARM mbed TLS-1.3.10 (formerly known
as PolarSSL) started to forbid repeated extensions in X.509
certiÔ¨Åcates, and active discussions on the reported certiÔ¨Åcates
have led to proposed changes to IETF‚Äôs relevant RFCs.The rest of the paper is structured as follows. Section 2 presents
the details of our guided technique, including basic background
on certiÔ¨Åcate validation, MCMC-guided certiÔ¨Åcate diversiÔ¨Åcation,
and the differential testing process. We next describe our extensive
evaluation of mucert against frankencert and two other certiÔ¨Åcate
mutation algorithms to demonstrate mucert‚Äôs effectiveness (Sec-
tion 3). Section 4 surveys related work, and Section 5 concludes.
2. APPROACH
This section presents the technical details of our approach. We
discuss necessary background on certiÔ¨Åcation validation, introduce
our MCMC-guided certiÔ¨Åcate mutation algorithm, and describe our
differential testing process.
2.1 Background: X.509 CertiÔ¨Åcate Validation
An input to certiÔ¨Åcate validation is a chain of X.509 certiÔ¨Åcates.
Each certiÔ¨Åcate consists of a sequence of three required Ô¨Åelds [21]:
CertiÔ¨Åcate , which contains a subject and an issuer, a public
key associated with the subject, a validity period, and other
information. An X.509 v3 certiÔ¨Åcate also has extensions that
can convey such data as additional subject identiÔ¨Åcation infor-
mation, policy information, and certiÔ¨Åcation path constraints;
CertiÔ¨Åcate Signature Algorithm , the identiÔ¨Åer for the signature
algorithm used by a certiÔ¨Åcate authority (CA) to sign this
certiÔ¨Åcate; and
CertiÔ¨Åcate Signature , a digital signature for the certiÔ¨Åcate.
In a public key infrastructure (PKI), a certiÔ¨Åcate does not exist in
isolation, but is recursively organized, together with its issuers, into a
certiÔ¨Åcate chain . A certiÔ¨Åcate chain usually starts with an end-entity
certiÔ¨Åcate followed by a list of certiÔ¨Åcates and the CA certiÔ¨Åcates.
Figure 1 illustrates a typical certiÔ¨Åcate chain, where the issuer of
each certiÔ¨Åcate is the subject of the next certiÔ¨Åcate, each certiÔ¨Åcate is
signed by the next certiÔ¨Åcate, and the last certiÔ¨Åcate is a self-signed
trust anchor. Given a certiÔ¨Åcate, an SSL/TLS implementation mainly
validates whether it can be chained to a ‚Äútrusted root‚Äù certiÔ¨Åcate and
whether each certiÔ¨Åcate on the chain is valid at the current time [21].
2.2 Guided CertiÔ¨Åcate Optimization
Intuitively, one may collect X.509 certiÔ¨Åcates from the Internet
to test SSL/TLS implementations, although these real certiÔ¨Åcates
unlikely expose Ô¨Çaws in validation code. On the other extreme is
frankencert [13], which randomly combines pieces of real certiÔ¨Å-
cates to construct fake ones, most of which are invalid and useless
for testing. Instead, we strive to Ô¨Ånd a sweet spot between the two
extremes by systematically and continuously mutating a set of real
certiÔ¨Åcates to make them diverse ( i.e., following different program
paths, triggering different validation policies, or triggering different
error handlers) for testing certiÔ¨Åcate validation logic.
2.2.1 MCMC Sampling and Fitness Function
For our purpose, we adapt MCMC sampling to optimize a test
suite with a Ô¨Åxed number of test certiÔ¨Åcates. We utilize code cover-
age as the Ô¨Åtness function to balance our optimization goal and the
easiness of measurement. Another reason for using code coverage
as the Ô¨Åtness function is that code coverage is shown to strongly cor-
relate with the output uniqueness of a test suite, one manifestation
of its diversity [10].
LetCov(Cert )be deÔ¨Åned for computing the code coverage
of a test suite Cert w.r.t. a given SSL/TLS implementation ( e.g.,
OpenSSL):
Cov(Cert ) =Cov(cert 0)Cov(certn);
794/g18/g286/g396/g410 /g1004
/g68/g286/g410/g396/g381/g393/g381/g367/g349/g400/g3
/g272/g346/g381/g349/g272/g286 /g373/g437/g410/g258/g410/g286
/g18/g286/g396/g410 /g1004/g918/g18/g286/g396/g410 /g1005
/g373/g437/g410/g258/g410/g286
/g18/g286/g396/g410 /g1005/g918/g68/g286/g410/g396/g381/g393/g381/g367/g349/g400/g3
/g272/g346/g381/g349/g272/g286/g18/g286/g396/g410 /g1006
/g856/g856/g856/g18/g286/g396/g410 /g349
/g373/g437/g410/g258/g410/g286
/g18/g286/g396/g410 /g349/g918/g68/g286/g410/g396/g381/g393/g381/g367/g349/g400/g3
/g272/g346/g381/g349/g272/g286/g856/g856/g856/g18/g286/g396/g410 /g374/g882/g1005
/g373/g437/g410/g258/g410/g286
/g18/g286/g396/g410 /g374/g882/g1005 /g918/g68/g286/g410/g396/g381/g393/g381/g367/g349/g400/g3
/g272/g346/g381/g349/g272/g286/g18/g286/g396/g410 /g374
/g17/g437/g396/g374/g882/g349/g374/g3/g393/g346/g258/g400/g286 /g94/g258/g373/g393/g367/g349/g374/g336/g3/g393/g346/g258/g400/g286Figure 2: MCMC-guided certiÔ¨Åcate optimization.
Certificate Certificate Certificate
Version Version Version
Serial Number Serial Number Serial Number
Algorithm ID Algorithm ID Algorithm ID
Issuer Issuer Issuer
Validity (Not Before/After) Validity (Not Before/Aft er) Validity (Not Before/After)
Subject Subject Subject
Subject Public Key Info Subject Public Key Info Subje ct Public Key Info
Public Key Algorithm Public Key Algorithm Public Key Algorithm
Subject Public Key Subject Public Key Subject Public Key
Issuer Unique Identifier Issuer Unique Identifier Iss uer Unique Identifier
Subject Unique Identifier Subject Unique Identifier S ubject Unique Identifier
Extensions Extensions Extensions
... ... ...
Certificate Signature
AlgorithmCertificate Signature
AlgorithmCertificate Signature
Algorithm
Certificate Signature Certificate Signature Certifica te Signature
Leaf cert Enterprise CA cert Root CA cert
Figure 1: A typical certiÔ¨Åcate chain.
whereCov(cert)denotes the coverage achieved by an individual
certiÔ¨Åcatecert, andallows the coverage to be computed cumula-
tively.
In our setting, code coverage helps make the test suite accept
‚Äúfresh‚Äù test certiÔ¨Åcates. Let cert inCert be mutated to cert0in
Cert0(see Section 2.2.3). Let Cov(Cert )< Cov (Cert0). The
certiÔ¨Åcatecert0is obviously distinct from cert or any other certiÔ¨Å-
cates inCert0, as it exploits some new validation code (or branches).
Therefore, code coverage provides the Ô¨Årst means to approach the
optimization goal, that is, given the test suite Cert , how can it be
mutated continuously to OptimizedCert such that
Cov(Cert )Cov(OptimizedCert )>;
whereCov(Cert )Cov(OptimizedCert )denotes that the cov-
erage should be increased as much as possible, and >is an upper
bound of coverage that can be achieved by any test suite.
MCMC sampling provides another opportunity, even if the cover-
age of a test suite cannot get increased, to stochastically diversify the
test certiÔ¨Åcates inside. MCMC advocates the idea of sampling from
a probability distribution by constructing a Markov chain which
converges to the desired distribution. When applied to optimization,
MCMC sampling can work as an intelligent hill climbing method,
and thus creates sufÔ¨Åcient samples most of which will be taken from
the optimal values of the adopted Ô¨Åtness function. In our setting,
each sample corresponds to a test suite.
2.2.2 Sampling ProcessAlgorithm 1 MCMC-guided algorithm to optimize certiÔ¨Åcates
Input: certiÔ¨Åcate corpus CertStore ,n,k
Output: test suite ofncertiÔ¨Åcates
1:Selectnrandom certiÔ¨Åcates from CertStore and add to Cert
2:highest _cov Cov(Cert )
3:OptimizedCerts  fCertg
4:repeat
5:cert random:choice (Cert )
6:mutator random:choice (Mutator )
7:cert0 apply (mutator;cert )
8:Cert0 (Certnfcertg)[fcert0g
9: ifhighest _cov<Cov (Cert0)then
10:highest _cov Cov(Cert0)
11:OptimizedCerts  fCert0g
12: else ifhighest _cov==Cov(Cert0)then
13:OptimizedCerts  OptimizedCerts[fCert0g
14: Accept Certs0according to A(Cert!Cert0)
15: ifaccepted then
16:Cert Cert0
17:untilhighest _covhas not been increased for ksteps
18:OptimizedCert random:choice (OptimizedCerts )
19:returnOptimizedCert
For MCMC sampling, mucert at Ô¨Årst transforms the Ô¨Åtness func-
tion into a probability density function, by following a commonly
used approach [42]:
P(Cert ) =1
Zexp( (Cov(Cert ) ?));
whereis a constant, Za partition function that normalizes the
distribution, and?a lower bound of coverage that can be achieved
by a test suite.
Mucert then adopts the Metropolis-Hastings algorithm [36] for
generating Markov chains. The Metropolis-Hastings algorithm is an
MCMC method for obtaining random samples from a probability
distribution. It works by generating a sequence of samples whose
distribution closely approximates the desired distribution; samples
are produced iteratively, with the distribution of the next sample (say
s0) being dependent only on the current one (say s). As Figure 2
shows, in the burn-in phase, the coverage of the samples increases
rapidly, while in the sampling phase, all samples hold high coverage
values, but their diversities stochastically vary. We use Metropolis
choiceA(s!s0)for sampling acceptance or rejection:
A(s!s0) =min(1;P(s0)
P(s)g(s0!s)
g(s!s0))
795+
+ +
+ +
Cov(cert 4) Cov(cert 5)
Cov(cert 1) Cov(cert 2) Cov(cert 3) Cov(cert 1')Figure 3: An example of coverage information tree. When one
leaf node (Cov(cert 1)) is updated, the tree is updated in a bot-
tom up style. All updated nodes are colored in brown.
whereg(s!s0)is the proposal distribution describing the condi-
tional probability of proposing a new sample s0givens.
The proposal distribution in our setting is symmetric, thus the
acceptance probability is reduced to
A(Cert!Cert0) =min(1;P(Cert0)
P(Cert ))
=min(1;exp((cov1 cov2)));
wherecov1=Cov(Cert )andcov2=Cov(Cert0). The ac-
ceptance probability can be directly computed from the coverage
functionCov().
Letbe a negative constant in ( 1:0;0). The importance of
A(Cert!Cert0)is: ifCov(Cert )Cov(Cert0), the proposal
is always accepted; otherwise the proposal is accepted with a certain
(small) probability (namely exp( (Cov(Cert0) Cov(Cert )))).
Further, the smaller Cov(Cert0) Cov(Cert )is, the less the
acceptance probability will be. For example, let be 0:033,
Cov(Cert )be7200 (SLOC); let Cov(Cert 1)andCov(Cert 2)
be7180 (SLOC) and 7170 (SLOC), respectively. Cert 1is easier to
accept thanCert 2because
A(Cert!Certs 1) = 0:517;and
A(Cert!Certs 2) = 0:371:
Algorithm 1 shows the mucert algorithm for certiÔ¨Åcate optimiza-
tion. It Ô¨Årst selects a test suite of ncertiÔ¨Åcates (certiÔ¨Åcate chains
more rigorously). It then performs a number of iterations. During
each iteration, exactly one certiÔ¨Åcate is chosen and mutated. The al-
gorithm chooses one sample with the highest coverage as an optimal
solution (theoretically any sample can be chosen for testing). Notice
that mucert mutates an X.509 certiÔ¨Åcate (or a certiÔ¨Åcate chain) by
rewriting the certiÔ¨Åcate (or rewriting one certiÔ¨Åcate in the chain),
which we will explain in Section 2.2.3.
2.2.3 CertiÔ¨Åcate Mutation
We deÔ¨Åne 37 mutators (mutation operations) for supporting cer-
tiÔ¨Åcate mutation. Mucert randomly picks a certiÔ¨Åcate (or a chain)
and mutates it, expecting that the mutant can exploit some new
validation policies in the validation code. As Table 1 shows, these
mutators are classiÔ¨Åed into two categories:
1.Chain mutator. A chain mutator is used to update a certiÔ¨Åcate
chain, e.g., inserting one certiÔ¨Åcate into the chain or deleting
one from the chain. A chain mutator is usually performed
together with an updating of the issuers of the certiÔ¨Åcates on
the chain, so that each certiÔ¨Åcate is issued by the subsequent
one.
2.CertiÔ¨Åcate mutator. A certiÔ¨Åcate mutator is used to update a
single certiÔ¨Åcate, e.g., rewriting the expiration date or addingTable 1: Sample mutation operations used.
Category Operations
(1) Insert a certiÔ¨Åcate at a given position of a chain
Chain (2) Append a certiÔ¨Åcate to a chain
mutator (3) Delete a certiÔ¨Åcate from a chain
(4) Replace one certiÔ¨Åcate in a chain with another
(5) Rewrite a certiÔ¨Åcate Ô¨Åeld ( e.g., notAfter, notBe-
fore, serial number, subject, extensions)
(6) Rewrite a subject Ô¨Åeld of a certiÔ¨Åcate ( e.g.,
countryname, stateOrProvinceName, stateOrProvin-
ceName, localityName, organizationname, organi-
zationalUnitName, commonName, emailAddress,
name, title)
CertiÔ¨Åcate (7) Append a set of extensions to the certiÔ¨Åcate
mutator (8) Append one extension to the certiÔ¨Åcate
(9) Rewrite the criticality of one extension
(10) Rewrite one extension
(11) Delete a certiÔ¨Åcate Ô¨Åeld or a subject Ô¨Åled
an extension to the certiÔ¨Åcate. Nevertheless, when the subject
of a certiÔ¨Åcate is updated, the issuer of the preceding certiÔ¨Å-
cate may be updated. When its issuer is updated, the subject
of the subsequent certiÔ¨Åcate may be updated. Further, we can
mutate a certiÔ¨Åcate by deleting one of its Ô¨Åelds in order to
check whether the mutant can trigger some parsing errors or
validation problems.
We prefer a grafting strategy when rewriting a certiÔ¨Åcate or its
Ô¨Åeld. Given a certiÔ¨Åcate chain, we can replace one certiÔ¨Åcate with
an ‚Äúinvader‚Äù certiÔ¨Åcate that is randomly chosen from the certiÔ¨Åcate
corpus. Similarly, we can insert the invader into the chain, or use
its Ô¨Åeld to update the corresponding Ô¨Åeld of a certiÔ¨Åcate in the
chain. We can also choose one or more extensions of the invader
certiÔ¨Åcate and add them into a certiÔ¨Åcate. Such a strategy helps
produce syntactically correct mutants that otherwise might have
been rejected early during validation due to trivial parsing errors.
2.3 Differential Testing
We have implemented a testing framework in Python to realize
and utilize mucert. Figure 4 illustrates the framework, which con-
tains two key components: (1) test certiÔ¨Åcate optimization, and (2)
differential testing:
CertiÔ¨Åcate optimization. Mucert selects a set of ncertiÔ¨Åcates
at random and then performs an MCMC sampling process.
One sample with the highest coverage is chosen for testing.
Differential testing. Differential testing is a mature testing
technology for large software systems [30, 35]: a test case
is randomly generated, and output is compared for a variety
of systems. In our work, we employ the generated mucerts
to test commonly used SSL/TLS implementations (including
OpenSSL [1], PolarSSL [3], Gnutls [4], NSS [2], CyaSSL [5],
and MatrixSSL [6]) and web browsers (including Google‚Äôs
Chrome [7], Mozilla‚Äôs Firefox [8], and Microsoft‚Äôs Internet
Explorer [9]) and then compare the validation results. Any
behavior discrepancies among these implementations become
oracles for Ô¨Ånding Ô¨Çaws in their certiÔ¨Åcate validation code.
2.3.1 CertiÔ¨Åcate Validation in Testing
An SSL/TLS implementation can validate an X.509 certiÔ¨Åcate
(or a certiÔ¨Åcate chain) in either a Ô¨Åle mode or a client-server (C-S)
mode, or both. The Ô¨Åle mode provides a rather simple validation
796Step 1: Certificate Mutation
Step 2: Differential Testing
OpenSSL 
PolarSSL
CyaSSL
Chrome
...cert 1
cert 2
cert 3
...
...cert 2
cert 3...Certificate 
validation code
Certificate 
mutantsValidation 
results
Certs with 
disprecencies
Defect reportcert 1
cert 2
cert 3
...Cert
ChooseMutatecert 1
cert 2
cert 3
...Cert‚Äô
covCompute coveragecov‚ÄôCompute 
coverage
Accept or 
reject?Reject
Accept Cert: = Cert /g255/g255/g255/g255Accept
Complete
TestingCert DBFigure 4: Guided differential testing of certiÔ¨Åcate validation.
style, allowing the implementation to load and validate a PEM
Ô¨Åle containing one or more certiÔ¨Åcates. The C-S mode provides a
typical, but slightly more complicated validation solution, requiring
a client to retrieve a server certiÔ¨Åcate and then validate it. Table 2
shows the supportability of the SSL/TLS tools and browsers to the
two validation modes.
Note that CyaSSL and MatrixSSL do not provide released utilities
for validating certiÔ¨Åcate Ô¨Åles. Although the C-S validation mode is
supported by all of the implementations, it is still difÔ¨Åcult to validate
a large number of certiÔ¨Åcates since each website is usually secured
by only one certiÔ¨Åcate. A browser mainly validates a certiÔ¨Åcate
when it connects to a server, while up to now, we do not have any
tools that can forge a web browser when it connects to a spoofed do-
main name (matching to the domain name appearing in a certiÔ¨Åcate).
Thus mucert adopts both modes to test SSL/TLS implementations:
1. Test OpenSSL, PolarSSL, Gnutls, and NSS in Ô¨Åle mode;
2.Test Cyassl and MatrixSSL in C-S mode. We use the server in
Brubaker et al. [13] to warp and send the mutated certiÔ¨Åcates,
and use the clients released in CyaSSL/MatrixSSL to retrieve
and validate the certiÔ¨Åcates. Each client takes three arguments
(host, port, path to the Ô¨Åle with trusted root certiÔ¨Åcates) and
makes an SSL 3.0 connection to the host/port. The client
records the validation results, including error codes if any;
3.Import the certiÔ¨Åcates into the certiÔ¨Åcate databases used by
the web browsers (Chrome, Firefox, and Internet Explorer).
We assume that a validation is performed when a certiÔ¨Åcate is
imported1. We also use the browsers to connect a localhost
1In fact, a certiÔ¨Åcate manager does not validate the trusted certiÔ¨Å-
cates (see Section 3.4).Table 2: Supported validation modes by SSL/TLS implementa-
tions.
SSL/TLS tools File mode (with standalone validation C-S
or libraries utility or certiÔ¨Åcate manager) mode
OpenSSL 1.0.1j Y (openssl) Y
PolarSSL 1.3.9 Y (cert_app) Y
Gnutls 3.3.10 Y (certtool) Y
NSS 3.17.3 Y (certutil) Y
CyaSSL 3.3.0 / Y
MatrixSSL 3.7-1 / Y
Chrome 39.0.2171.95 Y (an OS-level certiÔ¨Åcate manager) Y
Mozilla‚Äôs Firefox 34.0 Y (PSM/NSS) Y
Internet Explorer 11.0.14 Y (Microsoft Management Console) Y
OpenSSL PolarSSL Gnutls NSS Cyassl MatrixSSL Chrome
(Ubuntu)Firefox
(Ubuntu)IE
(Windows)
cert0 1 1 0 X X 1 0 1
0: rejected 1: accepted X: skipped
Figure 5: Result encoding example.
server on which fake certiÔ¨Åcates with the common name
‚Äúlocalhost ‚Äù are deployed and check whether the browsers
can be forged in SSL/TLS connections.
2.3.2 Discrepancy Representation
The validation results among the SSL/TLS implementations can
be discrepant, indicating that a certiÔ¨Åcate can be accepted by some
implementations, but rejected by the others. In this work, we encode
the validation results for facilitating computation of the diversity of
a test suite and identiÔ¨Åcation of the subtle discrepancies and their
root causes. Let each validation result be simpliÔ¨Åed to ‚Äúrejected‚Äù
(0) or ‚Äúaccepted‚Äù (1). As Figure 5 illustrates, the validation results
for a certiÔ¨Åcate can be encoded into a sequence of bits, representing
that the certiÔ¨Åcate is accepted by PolarSSL/GNUtls/Chrome/IE, but
rejected by OpenSSL/NSS/Firefox. In cases where the validation
is skipped ( e.g., due to a server connection error), we mark the
corresponding bit as ‚ÄúX‚Äù. Therefore, a behavior discrepancy can
appear if a sequence is not all zeros or all ones (‚ÄúX‚Äù is omitted); two
discrepancies can be classiÔ¨Åed into one category if their encoded
results are equal (again ‚ÄúX‚Äù is omitted). Theoretically, a sequence
ofkbits has at most 2kpossible values. In our setting, the results
can be reduced to at most 510 (= 29 2)distinct discrepancies.
3. EMPIRICAL EV ALUATION
We have conducted an extensive evaluation to compare mucert
with frankencert and two other mutation algorithms. Our results
show that mucert is signiÔ¨Åcantly more cost-effective than the other
algorithms. In particular, 200 mucerts can achieve higher code
coverage than 100;000frankencerts. More importantly, mucert di-
versiÔ¨Åes a test suite even if its coverage is not increased, making 1K
mucerts yield 2.2 times as many distinct discrepancies as 100;000
frankencerts in the experiment and 3 times as many as those reported
by Brubaker et al. [13].
We have reported 20+ latent discrepancies and an additional 357
discrepancy-triggering certiÔ¨Åcates to SSL/TLS developers, who
have been investigating causes of the reported discrepancies and
identifying Ô¨Çaws in their implementations. Our reported certiÔ¨Åcates
have also led to active discussions in the community and even pro-
posed changes to IETF‚Äôs RFCs. The rest of the section presents our
detailed results and analysis.
7973.1 Setup
Our empirical evaluation of mucert is designed to answer the
following research questions:
Coverage : What coverage can be achieved by mucerts when
used for testing of SSL/TLS implementations?
Precision : How precise are the mucerts for uncovering dis-
crepancies among certiÔ¨Åcate validation code?
Diversity : Are the mucerts diverse?
Flaws : Can the discrepancies pinpoint any real Ô¨Çaws in cer-
tiÔ¨Åcate validation code?
3.1.1 Preparation
Mucert optimizes a set of certiÔ¨Åcates iteratively, guided by achieved
code coverage w.r.t. a speciÔ¨Åc SSL/TLS implementation. For the
initial seed certifates, we use a collection of 1,006 certiÔ¨Åcates pro-
vided by frankencert [13]. These certiÔ¨Åcates were gathered using
ZMap [25] by scanning the Internet and attempting SSL connections
to hosts listening on port 443. If a connection was successful, the
certiÔ¨Åcate presented by the server was added to the collection.
As for the reference SSL/TLS implementation, we use OpenSSL-
1.0.1j, and the objective is to optimize mucerts to cover the source
code of OpenSSL as much as possible. We consider both statement-
andbranch- coverage optimized search heuristics (which we call
SOSH and BOSH repsectively) to direct certiÔ¨Åcate mutations. We
use the mature, widely adopted coverage tool GCOV +LCOV to
collect coverage statistics.
Our evaluation was conducted on a 64-bit Ubuntu 14.04 LTS
desktop (with an Intel Core i7-4770 CPU and 16GB RAM). We
performed differential testing on nine SSL tools and browsers, in-
cluding OpenSSL 1.0.1j, PolarSSL 1.3.9, Gnutls 3.3.10, NSS 3.17.3,
CyaSSL-3.3.0, MatrixSSL 3.7-1, Google‚Äôs Chrome 39.0.2171.95,
Mozilla‚Äôs Firefox 34.0, and Microsoft‚Äôs Internet Explorer 11.0.14.
Except Internet Explorer, all these tools and browsers were tested
on the 64bit Ubuntu 14.04 LTS machine. Internet Explorer was
tested on a 32-bit Windows 7 Enterprise desktop (with an Intel Core
i5-2430M CPU and 4GB RAM).
3.1.2 Evaluated Methods and Metrics
We evaluated mucert against three other methods:
frankencert : Frankencert produces a number of fake certiÔ¨Å-
cates that are randomly synthesized from parts of real cer-
tiÔ¨Åcates. Thus frankencerts include unusual combinations of
extensions and constraints;
randmut : It is a random mutation algorithm that we designed
to compare against mucert. It performs random mutation
operations on the certiÔ¨Åcates in a test suite; and
greedymut : It is a greedy mutation algorithm that we designed
also for the purpose of demonstrating mucert‚Äôs capability. It is
similar to mucert, and computes the coverage w.r.t. a sample.
It differs from mucert in that it will accept a proposed sam-
ple if the sample leads to increased coverage, and otherwise
rejects the sample.
We record several metrics during the evaluation. We report the
covered statements and branches by the test suites. The more pro-
gram statements/branches are covered, the more validation policies
can be triggered by the corresponding test suites. We also normalize
the coverage Cov using the following formula
NormCov =Cov ?
> ?100%;
where?and>are respectively the lower and the upper bounds of
the coverage values. We do not use the absolute coverage rates, asOpenSSL supports a rich set of functions, while many ( e.g., gen-
eration of self-signed certiÔ¨Åcates and private keys) do not concern
certiÔ¨Åcate validation. For the same reason, we can only approximate
the lower and the upper bounds, but not their actual values, which
sufÔ¨Åce for guiding certiÔ¨Åcate mutation.
We record any discovered validation discrepancies and compute
theprecision of a test suite Cert as follows
Precision =jDCertj
jCertj,
wherejCertjdenotes the number of certiÔ¨Åcates in the test suite, and
DCert a subset of the certiÔ¨Åcates that trigger discrepancies. The
more discrepancies discovered by a test suite with a Ô¨Åxed number
of certiÔ¨Åcates, the more precise the test suite is.
We compute the diversity of a test suite using the formula
Diversity =jDDCertj+
jCertj100%;
wherejDDCertjdenotes the number of distinct discrepancies,
and the numerator jDDCertj+denotes the number of distinct
encoded results ( 2f0;1;2gto count for the all zeros and all
ones if they exist). The more distinct discrepancies found, the more
diverse are the certiÔ¨Åcates in the test suite.
In practice, neither discrepancies nor diversity can be conveniently
computed during certiÔ¨Åcate generation, due to the very different
validation styles of SSL/TLS implementations. In our evaluation,
we will show that MCMC sampling does indeed help diversify a test
suite, besides increasing coverage (see Section 3.3).
3.2 Results on CertiÔ¨Åcate Generation
Mucert, randmut, and greedymut all require an initial set of ncer-
tiÔ¨Åcates, but take their respective strategies to optimize the set. In the
evaluation, we include in the initial set all 1;006certiÔ¨Åcate chains in
the corpus (n= 1;006). When using randmut, we perform 5;000
mutation operations, while for greedymut and mucert, we continue
to mutate the certiÔ¨Åcates until the coverage does not increase for k
(k= 500=8;000) iterations. For a straightforward, direct compari-
son of various methods, we produce 100;000frankencerts.
Table 3 and Figure 6 show the code coverage of the test suites on
OpenSSL, which has 77,264 SLOC and 58,897 branches in total.
We use the minimal values, 5,461 SLOC and 2,364 branches, in
Table 3 to approximate the lower bound ?, and the maximum values
7,714 SLOC and 3,783 branches, for the upper bound >. The last
column shows the normalized coverage achieved.
Finding 1 :1Kmucerts achieve up to 25% higher normalized cov-
erage than 1Kfrankencerts; 200 mucerts achieve higher coverage
than100Kfrankencerts.
Both mucert and greedymut achieve high coverage ‚Äî 7,701-
7,714 lines and 3,762-3,783 branches ‚Äî regardless of the values
ofkand. In contrast, frankencert and randmut achieve as low
coverage as the initial set,2indicating that the validation code is in-
adequately tested by their certiÔ¨Åcates. In particular, 200 mucerts can
achieve higher coverage than 100Kfrankencerts, demonstrating
that mucerts are more effective in testing than frankencerts. Al-
though mucerts and greedymut certiÔ¨Åcates achieve similar coverage
values, mucerts are more diverse than the greedymut certiÔ¨Åcates, as
the coverage of the former increases more rapidly than the latter.
2The certiÔ¨Åcates in the initial corpus cannot pass validation be-
cause they are validated against a special CA certiÔ¨Åcate, but not the
certiÔ¨Åcates of their issuers.
798Table 3: Coverage achieved by the test suites ( w.r.t. OpenSSL). Greedymut-1/2 and mucert-1/2 use SOSH, while greedymut-3/4 and
mucert-3/4 use BOSH. The arguments for greedymut and mucert are: (1) greedymut-1/3: k= 500 ; (2) greedymut-2/4: k= 8;000;
(3) mucert-1: = 0:03,k= 500 ; (4) mucert-2: = 0:03,k= 8;000; (5) mucert-3: = 0:3,k= 500 ; (6) mucert-4: = 0:3,
k= 8;000.
CertiÔ¨Åcation generation jCertj NormCov
approaches Cov. 1 200 400 600 800 1006 10000 100000 (jCertj= 1006100000)
initial test suite stmt. 5862 7041 7050 7078 7114 7117 / 0.735
branch 2629 3356 3368 3382 3405 3408 / 0.736
frankencert stmt. 5518 7085 7107 7120 7146 7149 7164 7208 0.7490.775
branch 2409 3419 3440 3451 3467 3471 3487 3520 0.7800.815
randmut stmt. 5513 7099 7103 7114 7114 7122 / 0.737
branch 2405 3414 3419 3431 3432 3438 / 0.756
greedymut-1 stmt. 5858 7184 7639 7670 7684 7701 / 0.994
branch 2627 3442 3725 3740 3748 3762 / 0.985
Stmt. greedymut-2 stmt. 5858 7199 7204 7700 7713 7714 ( >) / 1
Cov. branch 2627 3454 3464 3766 3772 3774 / 0.993
Optimized mucert-1 stmt. 5862 7627 7641 7669 7678 7704 / 0.995
Search branch 2629 3713 3732 3747 3755 3774 / 0.993
(SOSH) mucert-2 stmt. 6803 7663 7666 7694 7694 7702 / 0.994
branch 3172 3736 3743 3763 3763 3770 / 0.990
greedymut-3 stmt. 5507 7607 7610 7638 7699 7707 / 0.996
branch 2402 3704 3714 3729 3767 3776 / 0.995
Branch greedymut-4 stmt. 5503 7595 7645 7699 7710 7712 / 0.999
Cov. branch 2392 3687 3737 3771 3779 3783 ( >) / 1
Optimized mucert-3 stmt. 5461 (?) 7657 7674 7703 7707 7710 / 0.998
Search branch 2364 (?) 3736 3751 3770 3771 3776 / 0.995
(BOSH) mucert-4 stmt. 5465 7624 7645 7696 7698 7702 / 0.994
branch 2372 3698 3719 3760 3763 3767 / 0.988
5.866.26.46.66.877.27.47.67.8
50
100150200250300350400450500550600650700750800850900950
1,006
10,000
100,000/g2419/g2419/g2419/g2419
initial set
frankencert
randmut
greedymut-1
mucert-1
Test certificates executedKLOC
(a) Statement coverage.
2.752.953.153.353.553.75
50
100150200250300350400450500550600650700750800850900950
1,006
10,000
100,000initial set
frankencert
randmut
greedymut-1
mucert-1
Test certificates executed/g104/g104/g104/g1041000 Branches (b) Branch coverage.
Figure 6: Coverage achieved by the test suites. The X-axis
shows the numbers of executed certiÔ¨Åcates. For brevity, we
omit greedymut-2/3/4 and mucert-2/3/4.
Finding 2 :Compared with the simpler SOSH, BOSH does not
lead to improved coverage.
Mucert-1/2 and Mucert-3/4 achieve similar statement and branch
coverage values, which shows that even the simpler SOSH can help
achieve high code coverage when the test suite has been sufÔ¨Åciently
mutated and diversiÔ¨Åed.
Finding 3 :Greedymut/mucert generate certiÔ¨Åcates more slowly.
Table 4 compares the time spent by different approaches on gen-
erating certiÔ¨Åcates. As the table shows, frankencert generates cer-
tiÔ¨Åcates quickly, since it adopts the simple synthesis strategy for
test certiÔ¨Åcate generation. Greedymut and mucert spend much moreTable 4: Time spent on generating certiÔ¨Åcates, and iterations.
The time budget is 4 days ( i.e., 345,600 seconds).
#iterations time (seconds)
frankencert 100,000 369
randmut 5,300 823
greedymut-1 1,263 12,846
greedymut-2 9,236 96,247
greedymut-3 2,297 98,780
greedymut-4 11,792 timeout
mucert-1 1,179 7,327
mucert-2 8,883 105,950
mucert-3 2,299 81,088
mucert-4 9,116 timeout
time on generating certiÔ¨Åcates, since they need to compute the cover-
age of each sample for guiding the acceptance/rejection of the next
sample. Greedymut-3/4 and mucert-3/4 spend on average 3:16
more time than greedymut-1/2 and mucert-1/2 at each iteration, be-
cause when BOSH is employed, LCOV needs to generate and merge
large tracing Ô¨Åles containing branch coverage information.
Both greedymut and mucert reach a steady state (when the cov-
erage does not increase) after 679~3,792 ( i.e.,#iterations k)
iterations. When using SOSH, greedymut and mucert reach their
steady states in up to 3.58 (=9;236 8;000
9;23696;247
3;600) hours, while
when using BOSH, they reach their steady states in more than 17.6
(=2;299 500
2;29981;088
3;600) hours.
3.3 Results on Discrepancy Analysis
Table 5 shows the discrepancies found in testing. The last two
columns list the precisions and diversities of different approaches.
799Table 5: Discrepancies discovered by the respective test certiÔ¨Åcates.
200 400 600 800 1,006 10K 100K #all_accepted #all_rejected jDDCertjPrecision Diversity
(111111111) (000000000) (%) (%)
initial set 0 0 0 0 0 / / 0 1,006 0 0 0.10
frankencert 4 7 11 19 20 264 2,747 0 986 97,253 513 1.92.7 0.010.60
randmut 53 97 140 190 235 / / 0 766 4 23.4 0.50
greedymut-1 0 1 3 5 5 / / 0 1,001 4 0.5 0.50
greedymut-2 1 1 3 5 5 / / 0 1,001 4 0.5 0.50
greedymut-3 1 1 2 4 5 / / 0 1,002 3 0.5 0.40
greedymut-4 1 4 4 5 5 / / 0 1,002 3 0.5 0.40
mucert-1 26 59 94 119 153 / / 0 853 27 15.2 2.78
mucert-2 50 115 180 234 289 / / 0 717 26 28.7 2.68
mucert-3 42 96 151 206 252 / / 1 753 28 25.0 2.98
mucert-4 26 50 69 92 108 / / 0 898 17 10.7 1.79
/g1004/g1009/g1004/g1005/g1004/g1004/g1005/g1009/g1004/g1006/g1004/g1004/g1006/g1009/g1004/g1007/g1004/g1004/g1007/g1009/g1004
/g1006/g1004/g1004 /g1008/g1004/g1004 /g1010/g1004/g1004 /g1012/g1004/g1004 /g1005/g1004/g1004/g1010
/g100/g286/g400/g410/g3/g272/g286/g396/g410/g349/g296/g349/g272/g258/g410/g286/g400/g3/g286/g454/g286/g272/g437/g410/g286/g282/g349/g374/g349/g410/g349/g258/g367/g3/g400/g286/g410
/g296/g396/g258/g374/g364/g286/g374/g272/g286/g396/g410
/g396/g258/g374/g282/g373/g437/g410
/g336/g396/g286/g286/g282/g455/g373/g437/g410/g882/g1005
/g336/g396/g286/g286/g282/g455/g373/g437/g410/g882/g1006
/g336/g396/g286/g286/g282/g455/g373/g437/g410/g882/g1007
/g336/g396/g286/g286/g282/g455/g373/g437/g410/g882/g1008
/g373/g437/g272/g286/g396/g410/g882/g1005
/g373/g437/g272/g286/g396/g410/g882/g1006
/g373/g437/g272/g286/g396/g410/g882/g1007
/g373/g437/g272/g286/g396/g410/g882/g1008/g878/g24/g18/g286/g396/g410/g878
(a) Discrepancies.
/g1004/g1009/g1008 /g1008 /g1008/g1007 /g1007/g1006/g1011/g1006/g1010/g1006/g1012
/g1005/g1011
/g1004/g1009/g1005/g1004/g1005/g1009/g1006/g1004/g1006/g1009/g1007/g1004/g878/g24/g24/g18/g286/g396/g410/g878
/g1005/g1007 (b) Distinct discrepancies.
/g1004 /g1005/g856/g1013/g1013/g1006/g1007/g856/g1007/g1010
/g1004/g856/g1009 /g1004/g856/g1009 /g1004/g856/g1009 /g1004/g856/g1009/g1005/g1009/g856/g1006/g1005/g1006/g1012/g856/g1011/g1007
/g1006/g1009/g856/g1004/g1009
/g1005/g1004/g856/g1011/g1008
/g1004/g1009/g1005/g1004/g1005/g1009/g1006/g1004/g1006/g1009/g1007/g1004/g1007/g1009/g1081
/g1006/g856/g1011/g1008 (c) Precision.
/g1004/g856/g1005/g1004/g856/g1010/g1004/g856/g1009 /g1004/g856/g1009 /g1004/g856/g1009/g1004/g856/g1008 /g1004/g856/g1008/g1006/g856/g1011/g1012/g1006/g856/g1010/g1012/g1006/g856/g1013/g1012
/g1005/g856/g1011/g1013
/g1004/g1004/g856/g1009/g1005/g1005/g856/g1009/g1006/g1006/g856/g1009/g1007/g1007/g856/g1009/g1081 (d) Diversity.
Figure 7: Discrepancy analysis. The columns in orange hold the values for frankencerts when jCertj= 100;000.
/g1011/g1005/g1004/g1004/g1011/g1006/g1004/g1004/g1011/g1007/g1004/g1004/g1011/g1008/g1004/g1004/g1011/g1009/g1004/g1004/g1011/g1010/g1004/g1004/g1011/g1011/g1004/g1004
/g1005
/g1008/g1011/g1008/g1013/g1008/g1011
/g1005/g1008/g1006/g1004/g1005/g1012/g1013/g1007/g1006/g1007/g1010/g1010/g1006/g1012/g1007/g1013/g1007/g1007/g1005/g1006/g1007/g1011/g1012/g1009/g1008/g1006/g1009/g1012/g1008/g1011/g1007/g1005/g1009/g1006/g1004/g1008/g1009/g1010/g1011/g1011/g1010/g1005/g1009/g1004/g1010/g1010/g1006/g1007/g1011/g1004/g1013/g1010/g1011/g1009/g1010/g1013/g1012/g1004/g1008/g1006/g1012/g1009/g1005/g1009/g1012/g1013/g1012/g1012/g1013/g1008/g1010/g1005/g13099/g13099/g13099/g13099/g2119/g2119/g2119/g2119/g1007
/g13099/g13099/g13099/g13099/g2119/g2119/g2119/g2119/g1009/g373/g437/g272/g286/g396/g410/g882/g1006
/g373/g437/g272/g286/g396/g410/g882/g1008
/g349/g410/g286/g396/g258/g410/g349/g381/g374/g400/g62/g75/g18
(a) Coverage
/g1004/g1009/g1004/g1005/g1004/g1004/g1005/g1009/g1004/g1006/g1004/g1004/g1006/g1009/g1004/g1007/g1004/g1004
/g1009
/g1008/g1006/g1004/g1012/g1007/g1009
/g1005/g1006/g1009/g1004/g1005/g1010/g1010/g1009/g1006/g1004/g1012/g1004/g1006/g1008/g1013/g1009/g1006/g1013/g1005/g1004/g1007/g1007/g1006/g1009/g1007/g1011/g1008/g1004/g1008/g1005/g1009/g1009/g1008/g1009/g1011/g1004/g1008/g1013/g1012/g1009/g1009/g1008/g1004/g1004/g1009/g1012/g1005/g1009/g1010/g1006/g1007/g1004/g1010/g1010/g1008/g1009/g1011/g1004/g1010/g1004/g1011/g1008/g1011/g1009/g1011/g1012/g1013/g1004/g1012/g1007/g1004/g1009/g1012/g1011/g1006/g1004/g13099/g13099/g13099/g13099/g2119/g2119/g2119/g2119/g1007
/g13099/g13099/g13099/g13099/g2119/g2119/g2119/g2119/g1009/g373/g437/g272/g286/g396/g410/g882/g1006
/g373/g437/g272/g286/g396/g410/g882/g1008
/g349/g410/g286/g396/g258/g410/g349/g381/g374/g400/g951/g24/g18/g286/g396/g410|DCert| (b)Discrepancy
/g1004/g1005/g1006/g1007/g1008/g1009/g1010/g1011/g1012/g1013/g1005/g1004
/g1009
/g1008/g1006/g1004/g1012/g1007/g1009
/g1005/g1006/g1009/g1004/g1005/g1010/g1010/g1009/g1006/g1004/g1012/g1004/g1006/g1008/g1013/g1009/g1006/g1013/g1005/g1004/g1007/g1007/g1006/g1009/g1007/g1011/g1008/g1004/g1008/g1005/g1009/g1009/g1008/g1009/g1011/g1004/g1008/g1013/g1012/g1009/g1009/g1008/g1004/g1004/g1009/g1012/g1005/g1009/g1010/g1006/g1007/g1004/g1010/g1010/g1008/g1009/g1011/g1004/g1010/g1004/g1011/g1008/g1011/g1009/g1011/g1012/g1013/g1004/g1012/g1007/g1004/g1009/g1012/g1011/g1006/g1004/g13099/g13099/g13099/g13099/g2119/g2119/g2119/g2119/g1007
/g13099/g13099/g13099/g13099/g2119/g2119/g2119/g2119/g1009/g373/g437/g272/g286/g396/g410/g882/g1006
/g373/g437/g272/g286/g396/g410/g882/g1008
/g349/g410/g286/g396/g258/g410/g349/g381/g374/g400/g951/g24/g24/g18/g286/g396/g410|DDCert| (c) Distinct Discrepancies
Figure 8: Correlation between coverage and discrepancies. The X-axis stands for the numbers of iterations.
Table 6: Validation results of the 357 mucerts. A certiÔ¨Åcate is accepted by Firefox, Chrome, or IE if it can be imported into the
certiÔ¨Åcate manager without any warning messages. Any parsing error reported by browsers is taken as a rejection.
OpenSSL PolarSSL Gnutls NSS CyaSSL MatrixSSL Firefox Chrome IE
Accepted 120 344 10 9 262 299 308 2 109
Not parsable / 12 12 38 / 23 / / /
Rejected 237 1 335 310 95 35 49 355 248
800Figures 7(a) and (b) compare the discrepancies and distinct ones
found by the different approaches, respectively; (c) and (d) compare
their precisions and diversities, respectively.
Finding 4 :1Kmucerts trigger up to 14.5 as many discrepan-
cies and 5.6as many distinct discrepancies as 1Kfrankencerts;
1Kmucerts trigger up to 2.2 as many distinct discrepancies as
100Kfrankencerts.
Randmut and mucert trigger the most number of discrepancies
whenjCertj= 1;006. On average, 20% of mucerts can trigger
discrepancies, and each mucert-mutated test suite can help identify
about 25 distinct discrepancies. In comparison, 2.8% of 100K
frankencerts can only trigger 13 distinct discrepancies, as most
frankencerts are rejected due to trivial parsing errors. In particular,
1Kmucert-2 certiÔ¨Åcates can trigger 14.5 as many discrepancies
as1Kfrankencerts; 1Kmucert-3 certiÔ¨Åcates can trigger 5.6 and
2.2as many distinct discrepancies as 1Kand100Kfrankencerts,
respectively. Many discrepancies have also been discovered by
the randmut certiÔ¨Åcates, but they are reduced to only four distinct
discrepancies. This shows that most discrepancies found by randmut
certiÔ¨Åcates are redundant.
Finding 5 :Mucerts achieve 28.7% precision and 3.0% diver-
sity; all distinct discrepancies found by frankencerts and rand-
mut/greedymut certiÔ¨Åcates are also found by mucerts.
Up to 28.7% of mucerts reveal discrepancies, so do 2.7% franken-
certs, 23.4% randmut certiÔ¨Åcates and 0.5% greedymut certiÔ¨Åcates.
Mxucerts achieve up to 3.0% diversity, while the others achieve
only less than 0.6%. These results show that mucert is much more
cost-effective than frankencert and greedymut. In addition, we have
observed that each distinct discrepancy found by frankencerts and
randmut/greedymut certiÔ¨Åcates is also discovered by mucerts.
Finding 6 :Mucert can stochastically diversify a test suite even
when it does not further increase test coverage.
We have investigated the samples generated by mucert-2/4, and
observed how the coverage and discrepancies vary w.r.t. the number
of iterations. To make the analysis feasible, we selected one sample
every Ô¨Åve iterations, and only ran these test suites on OpenSSL,
PolarSSL, Gnutls, and NSS and identiÔ¨Åed validation discrepancies.
Figure 8(a) shows that a test suite can approach a peak coverage
value after about 400 iterations. Nevertheless, MCMC sampling
continues to diversify the test suite: Figures 8(b) and (c) show that
more (distinct) discrepancies can be triggered by the samples even
if their coverage does not increase (or even decrease).
In addition, we observe that most discrepancies (and distinct ones)
are discovered by the samples between 1,200 and 2,500 iterations,
but the numbers decrease slowly in subsequent iterations. One main
reason for this is that the proposed distribution of MCMC sampling
is in fact not symmetric: mucert needs to parse a certiÔ¨Åcate and
then mutate it, so a valid certiÔ¨Åcate can be mutated to a mutant with
parsing errors, but the opposite direction is usually infeasible. For
example, 1Kmucert-2 certiÔ¨Åcates can contain 159 (15.8%) certiÔ¨Å-
cates with parsing errors ( w.r.t. OpenSSL) after 8,000 iterations.
3.4 Bug Reports and Developer Feedback
We have reported 20+ distinct discrepancies and an additional 357
discrepancy-triggering mucerts to SSL/TLS developers/maintainers.
These certiÔ¨Åcates illustrate prevalent validation discrepancies among
the different implementations. As Table 6 shows, OpenSSL does
not report any parsing errors during validation, as the certiÔ¨Åcates
were created by OpenSSL. PolarSSL tends to accept more certiÔ¨Åcate
chains having self-signed certiÔ¨Åcates than OpenSSL, Gnutls, andNSS. 234 certiÔ¨Åcates are accepted by both MatrixSSL and CyaSSL,
and 30 rejected by both, while the remaining 93 are accepted by one
and rejected by the other. Browsers also behave differently when
certiÔ¨Åcates are imported, 49 (14%) and 248 (69%) certiÔ¨Åcates can
be imported into Firefox and IE‚Äôs certiÔ¨Åcate managers without trig-
gering any warning messages, respectively, while Chrome accepts
nearly all certiÔ¨Åcates except one expired and another failed to parse.
Finding 7 :CertiÔ¨Åcate validation discrepancies are prevalent.
However, this does not necessarily indicate that SSL/TLS con-
nections are insecure, because the discrepancies more likely exhibit
compatibility issues among SSL/TLS implementations. For exam-
ple, certiÔ¨Åcates in the test suite are issued by an issuer with a valid
private key, and thus the certiÔ¨Åcate chains can pass validation if
they are well-formed and have not expired. But some SSL/TLS
implementations also validate malformed certiÔ¨Åcate chains. To date,
we have yet to Ô¨Ånd any real exploits that maliciously use the discrep-
ancies we have found, although it has been reported that parsing
differences between CA software and browser certiÔ¨Åcate validation
code can be exploited as man-in-the-middle attacks [32].
We have also made some fake certiÔ¨Åcates issued by a valid CA
certiÔ¨Åcate with a fake private key, and checked whether they could
trigger any discrepancies. They are similar to some real man-in-the-
middle attacks.
Finding 8 :Developers do have many doubts and concerns when
implementing certiÔ¨Åcate validation code.
OpenSSL security team admitted that there are a lot of certiÔ¨Åcates
that can violate RFCs, but they cannot reject all of them unless they
are certainly security related.
PolarSSL developers have conÔ¨Årmed a parsing error: one certiÔ¨Å-
cate has RelativeDistinguishedName that contains more
than one AttributeTypeAndValue . They explained that they
were unsure if such structured names were actually used, and thus
they were hesitant to increase code complexity in order to cover that
case. They have Ô¨Åxed the naming problem in their internal develop-
ment branch and the patch will be included in the next release.
OpenSSL, GNUTLS, PolarSSL, and MatrixSSL behave differ-
ently when a certiÔ¨Åcate has two instances of the SubjectKey-
Identifier . This Ô¨Ånding has triggered an open question on
IETF PKIX,3which further requires the IETF PKIX/TLS working
groups to investigate the necessity of matching AKI/SKI certiÔ¨Åcate
extensions during certiÔ¨Åcate validation and inconsistencies among
RFCs 4158, 5280 and 6125. ARM mbed TLS developers also started
to forbid repeated extensions in X.509 certiÔ¨Åcates4because RFC
5280 states that ‚Äú a certiÔ¨Åcate MUST NOT include more than one
instance of a particular extension .‚Äù Gnutls developers replied that
although the certiÔ¨Åcate could be accepted in certiÔ¨Åcate veriÔ¨Åcation,
a user would receive warning messages when printing it out.
PolarSSL accepts certiÔ¨Åcate chains of the format [ cert 0,cert 1,
cert 2] even ifcert 1has the same subject as cert 2, but the others
(e.g., OpenSSL/Gnutls/CyaSSL) reject such chains and issue dif-
ferent warning messages ( e.g., unable to get local issuer certiÔ¨Åcate,
ASN signature error, or the certiÔ¨Åcate issuer is unknown). PolarSSL
developers have explained that PolarSSL accepts these certiÔ¨Åcate
chains since they have complete and valid key chains; they may
reject these chains in future development.
3‚ÄúSuggested replacement text for RFC5280, section 4.2.1.1,‚Äù
http://www.ietf.org/mail-archive/web/pkix/
current/msg33248.html
4https://tls.mbed.org/tech-updates/releases/
mbedtls-1.3.10-released
801OpenSSL, PolarSSL, and Gnutls accept a certiÔ¨Åcate chain if
the Ô¨Årst certiÔ¨Åcate is a trusted self-signed CA certiÔ¨Åcate, but omit
verifying the subsequent certiÔ¨Åcates. It does not strictly conform
to the validation policy ‚Äú when the trust anchor is provided in the
form of a self-signed certiÔ¨Åcate, this self-signed certiÔ¨Åcate is not
included as part of the prospective certiÔ¨Åcation path ‚Äù [21]. The
developers have explained that it is useful to accept it because
many misinformed users include the trust anchor in the chain they
provide, even though the RFCs recommend not to do it. More
importantly, accepting such chains allows users to trust self-signed
non-CA certiÔ¨Åcates if they choose to.
All the SSL/TLS implementations and browsers but NSS strictly
reject the invalid certiÔ¨Åcates issued by a valid CA with a fake private
key. We have reported to the NSS developers that certutil
incidentally skips signature checking when it validates a certiÔ¨Åcate
in a certiÔ¨Åcate database. The developers have responded to us that
in PKI, existence as trusted is sufÔ¨Åcient for validity. On the other
hand, other developers have claimed that this behavior confuses
many users and even security engineers.
The Chrome security team has explained that importing an invalid
certiÔ¨Åcate into the certiÔ¨Åcate manager is intentional and confers trust.
When an end user attempts to manually import an untrusted certiÔ¨Å-
cate into the certiÔ¨Åcate manager, certiÔ¨Åcate validation is skipped.
4. RELATED WORK
We discuss four strands of related work: (1) testing certiÔ¨Åcate
validation code, (2) random and symbolic execution, (3) feedback-
directed test generation, and (4) MCMC sampling for testing.
Testing CertiÔ¨Åcate Validation Code SSL/TLS tools and libraries
are vulnerable. Marlinspike [33, 34] has shown several vulnerabil-
ities in certiÔ¨Åcate validation code in browsers and SSL libraries.
Kaminsky et al. [32] have shown that a CA can issue a certiÔ¨Åcate
that can be used for man-in-the-middle attacks due to parsing differ-
ences between CA software and browser certiÔ¨Åcate validation code.
Georgiev et al. have used both white- and black-box techniques to
uncover vulnerabilities in validation logic [28], and have identiÔ¨Åed
several vulnerabilities. In contrast, mucert does not require deep
security knowledge, but rather employs a set of test certiÔ¨Åcates to
test the certiÔ¨Åcate validation code and Ô¨Ånd Ô¨Çaws inside. Thus the
mucert approach can be easily adapted in practice for verifying
SSL/TLS tools and libraries.
Bates et al. [12] present CertShim, a mechanism that improves
SSL security by interposing on SSL APIs and retroÔ¨Åtting legacy
software to support SSL trust enhancements. They also show how
to poll results of several veriÔ¨Åcation methods to improve security.
Mucert also leverages results of multiple certiÔ¨Åcate veriÔ¨Åcation han-
dlers, but focus on generating test certiÔ¨Åcates to trigger discrepancies
among these handlers.
The most closely related work to ours is frankencert [13], the
Ô¨Årst systematic technique for synthesizing test certiÔ¨Åcates for testing
SSL/TLS implementations. Due to its completely random and thus
‚Äúblind‚Äù nature, most of frankencerts are invalid or redundant, and
do not trigger new validation policies. Compared with frankencert,
mucert allows the tester to much more effectively explore the input
space and select diverse certiÔ¨Åcates for testing.
Random and Symbolic Execution Random testing has been a
very common testing technique, which requires testers to select tests
from an input domain at random. Random testing is usually not
cost effective, especially for settings with structurally complex input
domains. Adaptive Random Testing (ART) is a controversial idea
that tries to spread out the selected values over the input domain [15 ‚Äì
17, 20]. Many ART algorithms mainly select tests based on thelocations of successful tests, and use distances to measure whether
the next test case is sufÔ¨Åciently far away from all successful tests.
Meanwhile, ART is not applicable in our adversarial testing, as
the input space of X.509 certiÔ¨Åcate is high dimensional, while it
has been reported that ART often does not work well even in one-
dimension domains [11].
Symbolic execution has been used for generating test cases for
complex programs. KLEE [14] automatically generates test cases
by running programs symbolically and generating path constraints,
attempting to hit every line of executable code and detect danger-
ous operations. Dynamic symbolic execution techniques, such as
DART [29], CUTE [43], and Pex [44], improve the effectiveness
of symbolic execution and random testing by dynamically analyz-
ing the program behaviors under random testing and automatically
generating new test inputs to direct the execution along alternative
program paths. However, so far these symbolic execution tools have
not been used for generating certiÔ¨Åcates, as the modern constraint
solvers are not able to solve the intricate syntactical and semantic
constraints on the certiÔ¨Åcates. In contrast, mucert proposes coverage
optimized search heuristics to mutate certiÔ¨Åcates, but does not rely
on any constraint solver to generate certiÔ¨Åcates.
Feedback-directed Test Generation Another emerging direction
is to employ the previously constructed tests to design new tests.
Pacheco et al. present a feedback-directed random test generation
technique [37 ‚Äì39]. The feedback-directed technique builds inputs
incrementally by randomly selecting a method call to apply and
Ô¨Ånding arguments from among previously constructed inputs. Once
an input is built, it is executed, and the result determines whether
the input should be rejected or accepted for generating more inputs.
Inspired by the idea of feedback-directed testing, we leverage the
code coverage to measure and optimize a test suite.
Fraser and Arcuri introduce a technique called whole test suite
generation that can evolve all the test cases in a test suite simulta-
neously [26]. The technique starts with an initial set of randomly
generated test suites, and then uses a genetic algorithm to optimize
toward satisfying a chosen coverage criterion, while keeping the
total size of the test suite as small as possible. Mucert also em-
ploys coverage to measure a test suite, but uses MCMC sampling to
stochastically diversify a test suite, as the diversities of test suites
cannot be rapidly obtained during the sampling phase.
MCMC Sampling for Testing Zhou et al. propose a Markov chain
Monte Carlo Random Testing (MCMCRT) approach to random test-
ing [45]. On the basis of the Bayes approach to parametric models
for software testing, MCMCRT can utilize the prior knowledge and
the information on preceding test outcomes for their parameter esti-
mation. The MCMC sampler is also used to enhance performance
of random testing [47] and test case prioritization [46]. However,
MCMCRT mainly generates test inputs for numerical programs. To
our knowledge, our work is the Ô¨Årst that effectively uses MCMC
sampling for creating structured test inputs.
5. CONCLUSION
We have introduced mucert, a guided differential testing of cer-
tiÔ¨Åcate validation, by adopting MCMC sampling to mutate X.509
certiÔ¨Åcates and produce diverse certiÔ¨Åcates for testing SSL/TLS im-
plementations. Our experimental results have clearly demonstrated
mucert‚Äôs strengths over frankencert ‚Äî more detected discrepancies
and improved code coverage with orders of magnitude fewer test
certiÔ¨Åcates. We believe that developers can use mucert routinely to
identify latent Ô¨Çaws in SSL/TLS implementations to improve the
security and robustness of the Internet infrastructure.
8026. REPLICATION PACKAGE
The mucert package has been successfully evaluated by the Repli-
cation Packages Evaluation Committee and found to meet expecta-
tions. The mucert package is composed of three key components:
1.CertiÔ¨Åcate mutation engine that consumes a set of Internet
certiÔ¨Åcates and produces another set of diversiÔ¨Åed test certiÔ¨Å-
cates. Five certiÔ¨Åcate optimization algorithms are supported
in the mutation engine: the MCMC-guided certiÔ¨Åcate muta-
tion algorithm proposed in this paper, and two random and
two greedy mutation algorithms that we designed to compare
against mucert;
2.Scripts for supporting differential testing and analysis; and
3.Seeding certiÔ¨Åcates and a sample CA certiÔ¨Åcate ; for compari-
son reasons, we used the same seeding certiÔ¨Åcates provided
by frankencert [13].
A typical certiÔ¨Åcate generation process includes two phases:
1.Preparation : Mucert generates a coverage trace Ô¨Åle for each
certiÔ¨Åcate ( w.r.t. OpenSSL) and constructs a coverage infor-
mation tree for the corpus of seeding certiÔ¨Åcates; and
2.Mutation : Mucert takes one optimization algorithm to mutate
the seeding certiÔ¨Åcates. The mutants can be either accepted
for further mutations or discarded, and the coverage informa-
tion tree is updated along with the mutations.
After a number of iterations, the resulting certiÔ¨Åcates are used
to differentially test the SSL/TLS implementations. Most of the
state-of-the-art SSL/TLS implementations and web browsers can be
tested in C-S mode ( e.g., OpenSSL, PolarSSL/mbed TLS, GnuTLS,
CyaSSL, NSS, MatrixSSL, Google‚Äôs Chrome, and Mozilla‚Äôs Fire-
fox). Some SSL/TLS implementations ( e.g., OpenSSL, PolarSSL,
and GnuTLS) can also validate these certiÔ¨Åcate Ô¨Åles via the com-
mand line. Some basic commands for validating certiÔ¨Åcates are
shown next, where $ca_file is a CA certiÔ¨Åcate and $cert a
mutated certiÔ¨Åcate to be validated:
OpenSSL: openssl verify -CAfile $ca_file
$cert
PolarSSL/mbed TLS: cert_app mode=‚Äôfile‚Äô
filename=$cert ca_file=$ca_file
GnuTLS: certtool --verify
--load-ca-certificate=$ca_file < $cert
The validation results are recorded and compared within a spread-
sheet. Each cell records the validation result (accept or reject) of a
certiÔ¨Åcate w.r.t. an SSL/TLS implementation. When a certiÔ¨Åcate is
accepted by one SSL/TLS implementation but rejected by another,
a discrepancy is found and reported.
A corpus of 357 discrepancy-triggering certiÔ¨Åcates are also con-
tained in the replication package such that other researchers and
developers can use them to further investigate the causes of discrep-
ancies among SSL/TLS implementations.
7. ACKNOWLEDGEMENTS
We thank the anonymous reviewers and the Replication Packages
Evaluation Committee members for their constructive feedback. We
also thank Baishakhi Ray for her invaluable guidance and sugges-
tions on certiÔ¨Åcate validation. This research was sponsored in part by
973 Program in China (Grant No. 2015CB352203), the National Na-
ture Science Foundation of China (Grant No. 91118004, 61272102,
61472242, and 61100051), and United States NSF Grants (Grant
No. 1117603, 1319187, and 1349528). Yuting Chen is also partially
supported by the China Scholarship Council (CSC).8. REFERENCES
[1]https://www.openssl.org/ .
[2]https://developer.mozilla.org/en-US/
docs/Mozilla/Projects/NSS .
[3]https://polarssl.org/ .
[4]http://www.gnutls.org/ .
[5]http://www.yassl.com/yaSSL/
Products-cyassl.html .
[6]http://www.matrixssl.org/ .
[7]http://www.google.com/chrome/ .
[8]https://www.mozilla.org/en-US/firefox .
[9]http://windows.microsoft.com/en-us/
internet-explorer .
[10] N. Alshahwan and M. Harman. Coverage and fault detection
of the output-uniqueness test selection criteria. In
International Symposium on Software Testing and Analysis
(ISSTA) , pages 181‚Äì192, 2014.
[11] A. Arcuri and L. C. Briand. Adaptive random testing: an
illusion of effectiveness? In International Symposium on
Software Testing and Analysis (ISSTA) , pages 265‚Äì275, 2011.
[12] A. M. Bates, J. Pletcher, T. Nichols, B. Hollembaek, D. Tian,
K. R. B. Butler, and A. AlkhelaiÔ¨Å. Securing SSL certiÔ¨Åcate
veriÔ¨Åcation through dynamic linking. In ACM SIGSAC
Conference on Computer and Communications Security
(CCS) , pages 394‚Äì405, 2014.
[13] C. Brubaker, S. Jana, B. Ray, S. Khurshid, and V . Shmatikov.
Using frankencerts for automated adversarial testing of
certiÔ¨Åcate validation in SSL/TLS implementations. In IEEE
Symposium on Security and Privacy , pages 114‚Äì129, 2014.
[14] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: unassisted and
automatic generation of high-coverage tests for complex
systems programs. In Symposium on Operating Systems
Design and Implementation (OSDI) , pages 209‚Äì224, 2008.
[15] K. P. Chan, T. Y . Chen, and D. Towey. Restricted random
testing: Adaptive random testing by exclusion. International
Journal of Software Engineering and Knowledge Engineering ,
16(4):553‚Äì584, 2006.
[16] T. Y . Chen. Adaptive random testing. In International
Conference on Quality Software (QSIC) , page 443, 2008.
[17] T. Y . Chen, D. Huang, F. Kuo, R. G. Merkel, and J. Mayer.
Enhanced lattice-based adaptive random testing. In ACM
Symposium on Applied Computing , pages 422‚Äì429, 2009.
[18] S. Chib and E. Greenberg. Understanding the
Metropolis-Hastings algorithm. The American Statistician ,
49(4):327‚Äì335, Nov. 1995.
[19] S. Chokhani and W. Ford. Internet X.509 public key
infrastructure certiÔ¨Åcate policy and certiÔ¨Åcation practices
framework.
http://www.ietf.org/rfc/rfc2527.txt .
[20] I. Ciupa, A. Leitner, M. Oriol, and B. Meyer. ARTOO:
adaptive random testing for object-oriented software. In
International Conference on Software Engineering (ICSE) ,
pages 71‚Äì80, 2008.
[21] D. Cooper, S. Santesson, S. Farrell, S. Boeyen, R. Housley,
and W. Polk. Internet X.509 Public Key Infrastructure
CertiÔ¨Åcate and CertiÔ¨Åcate Revocation List (CRL) ProÔ¨Åle. RFC
5280 (Proposed Standard), May 2008.
803[22] T. Dierks and C. Allen. The TLS Protocol Version 1.0,
January 1999. Available from
http://www.ietf.org/rfc/rfc2246 .
[23] T. Dierks and E. Rescorla. The TLS Protocol Version 1.1,
April 2006. Available from
http://www.ietf.org/rfc/rfc4346 .
[24] T. Dierks and E. Rescorla. The TLS Protocol Version 1.2,
August 2008. Available from
http://www.ietf.org/rfc/rfc5246 .
[25] Z. Durumeric, E. Wustrow, and J. A. Halderman. ZMap: Fast
Internet-wide scanning and its security applications. In
USENIX Security Symposium , pages 605‚Äì620, 2013.
[26] G. Fraser and A. Arcuri. Whole test suite generation. IEEE
Trans. Software Eng. , 39(2):276‚Äì291, 2013.
[27] A. Freier, P. Karlton, and P. Kocher. The Secure Sockets Layer
(SSL) Protocol Version 3.0, August 2011. Available from
http://www.ietf.org/rfc/rfc6101 .
[28] M. Georgiev, S. Iyengar, S. Jana, R. Anubhai, D. Boneh, and
V . Shmatikov. The most dangerous code in the world:
validating SSL certiÔ¨Åcates in non-browser software. In ACM
Conference on Computer and Communications Security
(CCS) , pages 38‚Äì49, 2012.
[29] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed
automated random testing. In ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI) ,
pages 213‚Äì223, 2005.
[30] A. Groce, G. J. Holzmann, and R. Joshi. Randomized
differential testing as a prelude to formal veriÔ¨Åcation. In
International Conference on Software Engineering (ICSE) ,
pages 621‚Äì631, 2007.
[31] N. Gruschka, L. L. Iacono, and C. Sorge. Analysis of the
current state in website certiÔ¨Åcate validation. Security and
Communication Networks , 7(5):865‚Äì877, 2014.
[32] D. Kaminsky, M. L. Patterson, and L. Sassaman. PKI layer
cake: New collision attacks against the global X.509
infrastructure. In International Conference on Financial
Cryptography and Data Security (FC) , pages 289‚Äì303, 2010.
[33] M. Marlinspike. IE SSL vulnerability, 2002. Available from
http:
//www.thoughtcrime.org/ie-ssl-chain.txt .
[34] M. Marlinspike. Null preÔ¨Åx attacks against SSL/TLS
certiÔ¨Åcates, 2009. Available from
http://www.thoughtcrime.org/papers/
null-prefix-attacks.pdf .
[35] W. M. McKeeman. Differential testing for software. Digital
Technical Journal , 10(1):100‚Äì107, 1998.[36] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H.
Teller, and E. Teller. Equation of state calculations by fast
computing machines. Journal of Chemical Physics ,
21:1087‚Äì1092, 1953.
[37] C. Pacheco and M. D. Ernst. Randoop: feedback-directed
random testing for Java. In Companion to the Annual ACM
SIGPLAN Conference on Object-Oriented Programming,
Systems, Languages, and Applications (OOPSLA) , pages
815‚Äì816, 2007.
[38] C. Pacheco, S. K. Lahiri, and T. Ball. Finding errors in .NET
with feedback-directed random testing. In ACM/SIGSOFT
International Symposium on Software Testing and Analysis
(ISSTA) , pages 87‚Äì96, 2008.
[39] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball.
Feedback-directed random test generation. In International
Conference on Software Engineering (ICSE) , pages 75‚Äì84,
2007.
[40] E. Rescola. HTTP over TLS, May 2000. Available from
http://www.ietf.org/rfc/rfc2818 .
[41] P. Saint-Andre and J. Hodges. Representation and VeriÔ¨Åcation
of Domain-Based Application Service Identity within Internet
Public Key Infrastructure Using X.509 (PKIX) CertiÔ¨Åcates in
the Context of Transport Layer Security (TLS), March 2011.
Available from
https://tools.ietf.org/html/rfc6125 .
[42] E. Schkufza, R. Sharma, and A. Aiken. Stochastic
superoptimization. In Architectural Support for Programming
Languages and Operating Systems (ASPLOS) , pages 305‚Äì316,
2013.
[43] K. Sen and G. Agha. CUTE and jCUTE: Concolic unit testing
and explicit path model-checking tools. In International
Conference on Computer Aided VeriÔ¨Åcation (CAV) , pages
419‚Äì423, 2006.
[44] N. Tillmann and J. de Halleux. Pex: White box test generation
for .NET. In International Conference on Tests and Proofs
(TAP) , pages 134‚Äì153, 2008.
[45] B. Zhou, H. Okamura, and T. Dohi. Markov Chain Monte
Carlo random testing. In Advances in Computer Science and
Information Technology , pages 447‚Äì456, 2010.
[46] B. Zhou, H. Okamura, and T. Dohi. Application of Markov
Chain Monte Carlo random testing to test case prioritization
in regression testing. IEICE Transactions ,
95-D(9):2219‚Äì2226, 2012.
[47] B. Zhou, H. Okamura, and T. Dohi. Enhancing performance
of random testing through Markov Chain Monte Carlo
methods. IEEE Trans. Computers , 62(1):186‚Äì192, 2013.
804