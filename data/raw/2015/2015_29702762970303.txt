An Empirical Evaluation of Two User Interfaces of an
Interactive Program Veriﬁer
Martin Hentschel
TU Darmstadt
Dept. of Computer Science
Darmstadt, Germany
hentschel@cs.tu-
darmstadt.deReiner Hähnle
TU Darmstadt
Dept. of Computer Science
Darmstadt, Germany
haehnle@cs.tu-
darmstadt.deRichard Bubel
TU Darmstadt
Dept. of Computer Science
Darmstadt, Germany
bubel@cs.tu-
darmstadt.de
ABSTRACT
Theorem provers have highly complex interfaces, but there
are not many systematic studies of their usability and ef-
fectiveness. Speciﬁcally, for interactive theorem provers the
ability to quickly comprehend intermediate proof situations
is of pivotal importance. In this paper we present the (as
far as we know) ﬁrst empirical study that systematically
compares the eﬀectiveness of diﬀerent user interfaces of an
interactive theorem prover. We juxtapose two diﬀerent user
interfaces of the interactive veriﬁer KeY: the traditional one
which focuses on proof objects and a more recent one that
provides a view akin to an interactive debugger. We care-
fullydesignedacontrolledexperimentwhereusersweregiven
variousproofunderstandingtasksthathadtobesolvedwith
alternating interfaces. We provide statistical evidence that
the conjectured higher eﬀectivity of the debugger-like inter-
face is not just a hunch.
CCS Concepts
•Software and its engineering →Software veriﬁca-
tion; Formal software veriﬁcation; Software testing and
debugging;
Keywords
Veriﬁcation, Proof Understanding, Empirical Evaluation
1. INTRODUCTION
One of the most time-consuming and challenging steps in
deductive, semi-automatic program veriﬁcation is to under-
stand why a proof attempt fails. Reasons for failure include,
of course, buggy programs or speciﬁcations. It is also possi-
blethattheautomateddeductioncomponentisnotpowerful
enough to ﬁnd a proof. In either case the user needs to de-
velop suﬃcient understanding of the current proof situationto be able to ﬁx the program/speciﬁcation or to perform ad-
equate interactive proof steps. This involves navigating in
large proof trees and inspection of open proof obligations,
which may consist of dozens of large formulas.
To improve the eﬃciency of understanding intermediate
proof situations, therefore, promises considerable gains in
the overall human user time that needs to be invested into
formalveriﬁcation. Toachievethisgoalwedecidedtodesign
acompletelynewGUIforthestate-of-artveriﬁcationsystem
KeY [1] based on a software productivity tool all program-
mers are familiar with and which, according to anecdotal
evidence, they spend most of their time with—a debugger.
Speciﬁcally, we extended the Symbolic Execution Debug-
ger (SED) [8] by adding support for performing veriﬁcation
proofs [7]. The SED1is an Eclipse extension for interac-
tive symbolic execution into which any symbolic execution
engine can be integrated.
The enhanced SED is able to process the information con-
tained in any KeY proof, to summarize it, and to present
that summary in a succinct manner to the user in a num-
ber of diﬀerent views. More precisely, the SED visualizes
the partial program behavior explored in a proof attempt
in form of a symbolic execution tree. Each tree node repre-
sents an execution step made by the program and allows the
user to inspect the corresponding symbolic state, thus help-
ing to comprehend program behavior. In addition, nodes
are marked as not veriﬁed if a proof obligation could not be
shown. An important feature of the SED is that for each
subformula of a proof obligation it can be traced whether
it has already been shown to hold. This permits to easily
identify those parts of a proof obligation and of a related
speciﬁcation that have notbeen shown to hold.
All that sounds good, plus there was anecdotal evidence
from the user community that KeY’s new user interface in-
deed tends to improve the eﬃciency of veriﬁcation. But be-
fore investing more eﬀort in the new GUI we wanted to con-
ﬁrm whether this perceived eﬃciency increase is more than
a hunch. To do so, we decided to use an empirical approach
that is standard in experimental software engineering [14],
but (as far as we know) constitutes the ﬁrst study to em-
pirically compare the eﬀectiveness and eﬃciency of diﬀerent
user interfaces (UI) of an interactive theorem prover . We
compare the new UI based on the SED with the KeY’s stan-
dard UI. Both interfaces let the user inspect a proof attempt
to verify that a Java program complies with its speciﬁca-
1Available at www.key-project.org/eclipse/SED
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ASE’16 , September 3–7, 2016, Singapore, Singapore
ACM. 978-1-4503-3845-5/16/09...$15.00
http://dx.doi.org/10.1145/2970276.2970303
403
tion. The speciﬁcation style follows the design-by-contract
paradigm and the Java Modeling Language (JML) [12] is
used as the speciﬁcation language.
The goal of our work is to gain experimental evidence of
whether there is a signiﬁcant diﬀerence in eﬀectiveness and
eﬃciency between both UIs. Participants of the experiment
were shown several proof attempts and were asked questions
to measure how well they understand them. Each proof at-
temptwasabouttheveriﬁcationofasmallexample, inspired
by an interesting problem or a real-world case study. The
experiment was announced publicly on the KeY website and
KeYmailinglisttargetingallKeYusersandKeYdevelopers.
We summarize the scope of the experiment:
Analyze proof inspection in KeY and SED
for the purpose of evaluating eﬀectiveness and eﬃciency
from the point of view of the researcher
in the context of all KeY users .
Thepaperisorganizedasfollows: Section2discussesrelated
work. Then we brieﬂy summarize the main characteristics
of the prover UIs (Section 3), followed by a description of
the experiment’s planning and setup. The measured vari-
ables are determined in Section 4, the hypotheses to test in
Section 5. Section 6 lays out the design of the experiment,
Section 7 presents the instrumentation. We conclude exper-
iment planning by discussing threats to validity in Section 8.
The execution of the experiment is presented in Section 9.
Collected data are analyzed in Section 10 before discussing
the results in Section 11. We conclude the paper with Sec-
tion 12.
2. RELATED WORK
The authors of [4] evaluate the usability of KeY [1] and
Isabelle [13] using focus groups. Their goal is to improve
the usability of these tools, but they do not perform a direct
comparisonbetweenthem. Theauthorsof[10]presenta(co-
operative) evaluation of a speciﬁc feature, called interactive
proof critics . Similar to our experiment, participants had to
inspect (and correct) failed proof attempts, however, user
interfaces were not evaluated. In [11] a questionnaire to
identify desired features of user interfaces of proof systems
is presented. Most of the identiﬁed points are realized by
the SED. In [3] the use of HOL is analyzed and a three layer
model for interaction with a theorem prover is presented.
The SED covers all levels with its diﬀerent views. In [5] a
survey toexplorepossiblenewfeaturesoftheNuprltheorem
prover is presented. In summary, usability investigations
were made for several interactive theorem provers. But to
the best of our knowledge, no experiment was performed to
actually evaluate whether proposed features really improve
productivity or to compare diﬀerent user interfaces.
3. UI CHARACTERISTICS
The KeY veriﬁcation system is based on a deductive for-
mulation of symbolic execution in a program logic, where
programs are annotated with loop invariants and method
contracts. KeY’s standard UI focuses on proof objects while
the one based on SED provides a view akin to an interac-
tive debugger. To get an impression of these fundamentally
diﬀerent approaches consider Figures 1–2. In Figure 1 the
SED is used to inspect the proof attempt of class MyInteger
(in editor view on top left). The Symbolic Execution Treeview (top right) shows that only one execution path is feasi-
ble. This is, because the precondition non_null of the JML
method contract excludes that a NullPointerException is
thrown. In addition, the red crossed-out icon of the se-
lected symbolic execution tree node <end>indicates that the
method contract does not hold here. To identify the reason,
consider the Properties view (bottom left). The only open
goal (Node 133 ) shows on the right side of the sequent arrow
(==>) the formula to be proven in the current state (in curly
braces). That formula is a translation of the JML speciﬁca-
tion into logic and—with a little experience—easy to read.
The view colors subformulas to indicate whether they were
evaluated to true, false or that their status is still unknown.
It was evaluated to false (and-operators are colored in red)
indicating that something is wrong. We know that the class
invariant is preserved and that no exception is thrown (that
would violate JML’s normal_behavior ), because the related
formulas hold (are colored in green). Only the postcondi-
tion and the assignable clause remain open (colored in or-
ange). The left side of the sequent arrow says under which
assumption the right side is evaluated. The method param-
eter summand and the thisreference (called self) point to
the same object which is suspicious. This state is also visu-
alized in the symbolic object diagram shown in the callout
(top right). Alternatively, view Variables shows the symbolic
state as a tree structure. It is obvious that the precondition
does not hold in this state and the defect is found.
In Figure 2 the same proof attempt as in Figure 1 is in-
spected using KeY’s standard UI. The proof tree is shown
in the bottom left tab Proof. The proof obligation of the
selected proof tree node is shown in the view on the right
(Current Goal ). During proof attempt inspection it is rec-
ommended to focus on one goal. The tab Goalsallows the
user to navigate between the open goals. The ﬁrst step is
usually to identify the currently taken symbolic execution
path. This is achieved by hiding all non-branching proof
steps(seethe calloutinFigure2). Here, objects m,_summand
and m_1are not null. In addition, summand = self and
self.value != 0 hold. Reading this path condition is chal-
lenging, because it contains objects and expressions not part
of the veriﬁed source code and its speciﬁcation. The next
step is to inspect the proof obligation of the current goal. If
this does not reveal the defect, the only way to increase un-
derstanding is to look at each proof step. Alternatively, the
proof could be continued interactively which here is hope-
less, because of the present defect. To summarize, the full
proof including all performed steps can be inspected using
KeY whereas the SED hides many proof steps by oﬀering
various summary views.
4. VARIABLE SELECTION
First we need to determine and classify the variables of
the experiment. We distinguish two kinds of variables: inde-
pendent variables anddependent variables . The independent
variables are those which can be varied or at least controlled
by us and whose inﬂuence and eﬀect on the outcome of the
experiment we intend to study. Dependent variables are
the ones measured during the course of the experiment and
which we want to study. A value of an independent variable
that was changed during the experiment is called treatment .
Table 1 lists the variables of our experiment. The inde-
pendent variables which can be varied by us are M(with
treatments KeY and SED) and P(with the four proof at-
404Figure 1: Screenshot of the SED in which the proof attempt of class MyInteger is inspected
temptstoinspect). Thesubsetofindependentvariablesthat
are merely controlled are calledEJava|JML|KeY|SED. Their
values reﬂect the participants’ experience level. The separa-
tion between less and more than two years is made to sepa-
rate beginners from experienced users assuming that this is
roughly the time needed to master Java, JML and KeY well
enough for this evaluation. As the SED is rather new, it is
assumed that participants do not have a lot of experience
with it. For this reason, the separation between beginners
and experienced users is set to one year.
The dependent variables are used to quantify eﬃciency
andeﬀectiveness of the diﬀerent tools. Eﬃciency is mea-
sured by the time Ttmspend to answer the questions using
tooltm∈M. Eﬀectiveness is measured in the number of
correctly answered questions and the conﬁdence in the given
answers. Questions are single or multiple choice questions to
enable an automatic analysis. Each question lists a number
of correct and wrong answers from which the participant has
to choose. For a given tool tm, a multiple choice question
is answered correctly (measured by Qtm) if all and only the
correct answers are selected.
A correctness score is used to give credit for partially cor-
rect answers. The correctness score QStm=/summationtext
q∈tmqs(q)
is the sum of the scores over all questions of treatment tm.
For a single question qthe scoreqs(q)is deﬁned as:
qs(q) =/braceleftBigg#corSelAnsw (q)−#wrgSelAnsw (q)
#corSelAnswif#corSelAnsw (q)
>#wrgSelAnsw (q)
#corSelAnsw (q)−#wrgSelAnsw (q)
#wrgSelAnswif#corSelAnsw (q)
≤#wrgSelAnsw (q)
The question score qs(q)is the diﬀerence between the num-
berofselectedcorrectanswers #corSelAnsw (q)andthenum-
berofselectedwronganswers #corSelAnsw (q)ofquestion q,
normalized by the total number of correct/wrong answers.After each question we asked the participants about their
conﬁdence in the answer. Conﬁdence levels are sure (My
answer is correct!) ,educated guess (As far as I understood
the content, my answer should be correct.) andunsure (I
tried my best, but I don’t believe that my answer is correct.) .
For each question q(each question score qs(q)) a conﬁ-
dence rating c(q)(cs(q)) is computed according to Table 2.
A participant who is sure the answer is correct when it is
actually correct (the conﬁdence score is positive) obtains
maximal points. If the answer is wrong, but the participant
was sure that it is correct (the conﬁdence score is positive),
he or she gets the lowest possible rating. If the answer is
based on an educated guess, which is weaker than certainty,
the participant gets less (or loses less) points. If the par-
ticipant is unsure and thinks the answer is wrong, and it is
actually wrong (the conﬁdence score is not positive), then
still one score point is assigned, because the intuition was
correct. If the participant thinks the answer is wrong but it
is right (the conﬁdence score is positive), he or she loses one
scorepointforthesamereason. Finally, theconﬁdencescore
Ctm=/summationtext
q∈tmc(q)is the sum of the conﬁdence ratings over
all questions answered for treatment tm. The conﬁdence
score based on question scores CStm=/summationtext
q∈tmcs(q)·qs(q)
takes partially correct answers into account.
5. HYPOTHESIS FORMULATION
From our experiment we want to gain statistical evidence
that using the SED is more eﬃcient and eﬀective than using
KeY’s standard user interface. To this extent we formulate
for each dependent variable (see Table 1) an alternative hy-
pothesisH1Q–H1T(see Table 3). As usual [14] the claims
405Figure 2: Screenshot of KeY in which the proof attempt of class MyInteger is inspected
Table 1: Variables
Name Values Description
Independent
VariableM{KeY, SED} Compared tools
P{Calendar ,Account,
ArrayUtil ,MyInteger }Inspected proof attempts and related questions: Each proof attempt
veriﬁes a method contract of one of the listed classes
Controlled
VariableEJava{none,<2y.,≥2y.} Experience with Java
EJML{none,<2y.,≥2y.} Experience with JML
EKeY{none,<2y.,≥2y.} Experience with KeY
ESED{none,<1y.,≥1y.} Experience with SED
Dependent
VariableQtmInteger Number of correctly answered questions per treatment tmofM
QStmReal Correctness score per treatment tmofM
CtmInteger Conﬁdencescorepertreatment tmofMbasedonQ
CStmReal Conﬁdencescorepertreatment tmofMbasedon QS
TtmInteger Time needed to answer questions of a treatment tmofMin seconds
Table 2: Conﬁdence ratings c(q),cs(q)of question q
Correct answer Wrong answer
ofqorqs(q)>0ofqorqs(q)≤0
Sure 2 -2
Educated Guess 1 -1
Unsure -1 1
of these hypotheses are conﬁrmed by ruling out each corre-
sponding null hypothesis H0Q–H0T.
6. CHOICE OF DESIGN TYPE
An important design decision of the experiment is to en-
sure that participants beneﬁt from their participation. To
achieve this, each participant uses both tools resulting in a
paired comparison design (see Table 4). In case participants
are unfamiliar with KeY or the SED, this allows them to try
out both tools and to decide which one is of greater beneﬁt
for their work.We applied the general design principles randomization ,
blocking andbalancing [14] to avoid biases, to block out
eﬀects in which we are not interested in, and to simplify and
strengthen hypothesis testing. We randomized the order of
proof scenarios presented to the participants to avoid that,
forinstance, diﬀerencesinthelevelofdiﬃcultycaninﬂuence
the result of the experiment.
The ﬁrst two proof attempts are always to be understood
with help of the same tool and the next two proof attempts
with the other tool (recall that we have four proof under-
standing tasks). The decision which tool is used for the
ﬁrst two proof attempts is random. This avoids multiple
switches between tools which could confuse the participant.
Additionally, a participant who is not familiar with a tool
has more experience in the second task. The server used to
collect evaluation results guarantees that all possible per-
mutations of proof orders will be evaluated equally often as
well as all other constraints.
The performance of the participants may depend on their
experience with KeY and SED. As the SED is relatively
new and likely unknown to most participants, prior experi-
406Table 3: Hypotheses
Name of Null Hypothesis Deﬁnition of µVtmfor dependent variable V, treatment tmName of Alt. Hypothesis
H0QµQSED =µQKeYwithµQtm=Qtm
#questionsOfTmnt∈{x∈Q|0≤x≤1} H1QµQSED> µ QKeY
H0QSµQSSED=µQSKeYwithµQStm=QStm
#questionsOfTmnt∈{x∈Q|0≤x≤1} H1QSµQSSED> µ QSKeY
H0CµCSED =µCKeY withµCtm=Ctm
#questionsOfTmnt∈{x∈Q|−2≤x≤2}H1CµCSED> µ CKeY
H0CSµCSSED =µCSKeYwithµCStm=CStm
#questionsOfTmnt∈{x∈Q|−2≤x≤2}H1CSµCSSED> µ CSKeY
H0TµTSED =µTKeY withµTtm=Ttm
timeOfAllTmnts∈{x∈Q|0≤x≤1} H1TµTSED< µ TKeY
Table 4: Paired comparison design
Proof Proof Proof Proof
Attempt 1 Attempt 2 Attempt 3 Attempt 4
SED SubjnSubjnSubjn+1Subjn+1
KeY Subjn+1Subjn+1SubjnSubjn
ence with KeY is used for blocking. Grouping the partici-
pants according to their experience level with KeY allows us
to interpret the results for the diﬀerent groups separately.
Balancing is automatically achieved by the chosen design,
because each participant uses both tools and inspects all
four proof attempts. Thus the number of participants is the
same for each treatment.
7. INSTRUMENTATION
We did not want to limit the group of participants to peo-
ple familiar with formal methods, JML or the tools KeY and
SED. The only hard requirement the participants needed to
fulﬁll was basic knowledge of Java (or a similar language).
To accommodate this decision, the evaluation had to be self-
explanatory. We achieved this by showing three instruc-
tional videos: an introduction to the evaluation itself and
one to each tool. A brief textual introduction was given on
how to read and write JML speciﬁcations (we do not use
advanced concepts).
During the evaluation, a participant inspects proof at-
tempts with KeY and the SED. As both tools are available
within Eclipse, the evaluation itself is implemented as an
Eclipse wizard which is opened in an additional window so
that Eclipse itself remains fully functional.
The evaluation setup consists of two phases during which
information is collected and sent to the server. The ﬁrst
phase collects background knowledge on the participant and
determines the order of proof attempts and the tool assign-
ment. The actual evaluation is performed in the second
phase. A participant who cancels the evaluation during the
second phase is asked to send partial results to the server.
When that participant opens the evaluation wizard the next
time, heorsheisoﬀeredtorecoverthepreviousstatetocon-
tinue the already started evaluation. The evaluation work-
ﬂow is in detail:
1. Initialization Phase
(a)Terms of Use : Terms of use need to be accepted.
(b)Background Knowledge : Experience with Java,
JML, KeY, SED.
(c)Sending Data : Data is sent and order of proof
attempts is received.2. Evaluation Phase
(a)Evaluation Instructions : A video explaining how
to answer questions.
(b)JML: A textual documentation introducing the
features of JML necessary for the evaluation.
(c)SED/KeY Instruction : Avideoexplainingneeded
features and best practices to review a proof at-
tempt with SED/KeY (depending on order).
(d)Proof Attempts 1 and 2 : The ﬁrst and second
proof attempt and the questions that test the un-
derstanding.
(e)The complementary SED/KeY Instruction : The
remaining video.
(f)Proof Attempts 3 and 4 : As above
(g)Feedback about Tools and Evaluation : The partic-
ipant is asked to rate the usefulness of SED and
KeY features (mentioned in the videos).
(h)Sending Data and Acknowledgment : Data is sent
and the successful completion is acknowledged.
For each proof attempt the JML-annotated Java source
code2is shown to the user inside the Eclipse IDE. The in-
spectedproofattemptitselfisshowninsidethetoolassigned
to the current task. We summarize the four proof attempts
to be inspected and the defects the participants were sup-
posed to identify:
Account is a simpliﬁed problem used in a graduate course
on software veriﬁcation with KeY. To ﬁnd out why the
proofisstillopen, onehastodiscoverthatthecontract
of an invoked method is too weak (except the type, no
information about the returned value was speciﬁed).
Calendar is a simpliﬁed version of a program taken from
[2].3The participant must realize that the speciﬁed
class invariant renders one branch of an if-statement
unreachable. Inaddition, theclassinvariantisnotpre-
servedandan ArrayStoreException mightbethrown.
ArrayUtil is a simpliﬁed problem from an undergraduate
course taught at TU Darmstadt. The proof remains
openfortworeasons: thespeciﬁedloopinvariantisnot
preserved and on one execution path the value stored
atanarrayindexisreturnedinsteadoftheindexitself.
2www.key-project.org/eclipse/SED/
UnderstandingProofAttemptsEvaluation/proofs
3Available at www.key-project.org/fmco06
407MyInteger is from a talk “JML Editing in Eclipse and KeY-
IDE” given by one of the authors at the workshop
“JML: Advancing Speciﬁcation Language Methodolo-
gies”.4The challenge is to realize that the postcon-
dition does not hold in the case when two inputs are
aliased objects.
Wedesignedquestionsandanswersforeachproofattempt
following the same schema: The participant is ﬁrst asked
whether the proof is closed and which statements have been
symbolically executed. In case a participant answers that
the proof is still open, all applicable reasons why the proof
could not be closed from a predeﬁned list of potential causes
had to be chosen. The list is composed of (i) options gen-
erated for each present JML construct as a potential cause,
(ii) the possibility that automated reasoning power was in-
suﬃcient and interactive proof steps are required and (iii) a
free text form.
When a JML construct is selected as cause, the parti-
cipant is asked in which execution paths that construct is
invalid, preserved, etc. (Execution paths are identiﬁed by
comments in the source code.) In case a normal_behavior
contract is violated due to a thrown exception, the partici-
pant has to specify the kind of the uncaught exception in-
stead of selecting an execution path. All questions oﬀer also
the opportunity to give up after ten minutes.
8. VALIDITY EVALUATION
We discuss threats to the validity of our experiments and
the drawn conclusions. For each threat we provide a miti-
gation strategy.
“Conclusion validity concerns the statistical analysis of
results and the composition of subjects.” [14, p. 185] The
hypotheses of this experiment are tested with well known
statistical techniques. Threats to conclusion validity are the
low number of samples and the validity of the quality of an-
swers. Subjects may fake answers to compromise the exper-
iment. However, several participants are people we know
(colleagues, project partners, students, etc.), in addition,
some were monitored during the evaluation. We consider
the motivation of subjects to compromise the experiment to
be very low.
“Internal validity concerns matters that may aﬀect the in-
dependent variable with respect to causality, without the re-
searcher’s knowledge.” [14, p. 185] Inspecting a proof is time
intensiveandtheestimatedparticipationtimeis60minutes;
hence, participants may get tired or bored. There is also a
risk that participants lack motivation and thus answer ques-
tions not seriously. However, participation is voluntary and
can be done at any time convenient for the subject.
Maturation is a threat to internal validity as each tool
is applied to two proof attempts and participants may learn
how to use it, which is desired. We consider this non-critical
as randomization is applied to the order of proof attempts
and assigned tools. Some participants may have experience
with KeY, but most likely not with SED, as it is relatively
new. This is not critical as the instrumentation introduces
all relevant features of both tools. There might be a threat
that some participants are not willing to learn how to use
the SED. However, SED is designed to support the veriﬁ-
cation with KeY and not as a competitor. A potential bias
4www.lorentzcenter.nl/lc/web/2015/677/
info.php3?wsid=677about the experience with these tools would only contribute
against our claim that SED improves upon eﬃciency and ef-
fectiveness and not in its favor. Other threats are considered
to be uncritical.
“Construct validity concerns generalisation of the exper-
iment result to concept or theory behind the experiment.”
[14, p. 185] A threat to construct validity is that the chosen
proofattemptsmightnotberepresentative. Tomitigatethis
issue the proof attempts were taken from diﬀerent case stud-
ies and teaching materials that reﬂect typical usage scenar-
ios of KeY. In addition we took care that the proof attempts
cover all major JML features like method contracts as well
as class and loop invariants. Other threats to construct va-
lidity are considered uncritical. Even though a participant
might guess the expected outcome from the general motiva-
tion of the experiment (a comparison between KeY and the
SED) and that SED is the newer tool, we consider it un-
critical as the exact hypotheses and related measurements
are unknown to them. In addition, the participants do not
have any advantage or disadvantage from the outcome of
the experiment.
“External validity concerns generalisation of the experi-
ment result to other environments than the one in which
the study is conducted.” [14, p. 185] A threat to external
validity is that the source code related to proof attempts is
kepttoaminimum—inmanycasesonlyonemethod. Thisis
required to reduce the time participants need to understand
the veriﬁed source code and its speciﬁcation. Real Java code
is much more complex. On the other hand, contract-based
veriﬁcation is modular, only a small part of the source code
(one method at a time) is considered per proof. The partici-
pantsareselectedrandomlyandtheirexperiencevariesfrom
none to expert. Consequently, the selection of participants
is not a threat to external validity.
We state that there are threats to the validity of the ex-
periment, and hence, the drawn conclusions are valid within
the limitations of the threats.
9. EXECUTION
The experiment started in June 2015 with the staﬀ of the
Software Engineering group at TU Darmstadt. It included
students, PhD students and postdocs. Each participant was
monitored during the evaluation to improve the instructions
and answers. Questions and answers remained stable after
the ﬁrst participant. For this reason, the results of the ﬁrst
participant were excluded, but the others were kept. Ini-
tially, all instructions were given as textual descriptions. It
turned out that the participants have little motivation to
read them and would prefer videos instead. This was real-
ized after the ﬁrst few iterations. The content of the videos
is identical to the initial textual descriptions, so the results
of the previous participants are still valid.
The evaluation went public in July 2015. It was an-
nounced on the KeY website, KeY mailing list and during
the 14th KeY Symposium. The evaluation is available as a
preconﬁgured Eclipse product. Installation instructions and
download links are available on the KeY website.5Main
steps are to download and run the Eclipse product and to
perform the evaluation. An installation is not required,
the participants’ system is unaﬀected. Until mid Novem-
5www.key-project.org/eclipse/SED/
UnderstandingProofAttempts.html
408None< xyears≥xyears024681012141618
knowledge#participantsJava (x= 2)
JML (x= 2)
KeY (x= 2)
SED (x= 1)
Figure 3: Knowledge of participants
Table 5: KeY vs SED experience
SED
None<1 year≥1 year
None 4 2 0
KeY<2 years 7 1 0
≥2 years 4 1 2
ber 2015, 32 participants started the evaluation, but only
21 completed it. Twelve of the participants were monitored
during the evaluation (with their approval).
Figure 3 shows the distribution of background knowledge
of the participants. All participants had experience with
Java, only four were unfamiliar with JML. Knowledge about
KeY is evenly distributed between the experience levels. Six
participants had surprisingly experience with the SED.
The relation between the KeY and the SED experience
is shown in Table 5. It shows that for each level of KeY
experience (None, <2 year, and≥2 year), at least some
participants have also experience with the SED.
10. ANALYSIS
We visualize the collected data to get a ﬁrst impression
about their distribution and to identify possible outliers,
before we test our hypotheses. Interpretation and discussion
of the results is done in the following section.
To visualize data we use boxplots (Figures 4–8). The
middle vertical bar in the rectangle of a boxplot indicates
the median of the data. The left border represents the
lower quartile lqand the right border represents the up-
per quartile uq. The left and right whisker indicate the
theoretical bounds of the data assuming a normal distribu-
tion. Data points outside the whiskers are outliers. The left
whisker is deﬁned as lq−1.5 (uq−lq)and the right whisker
asuq+1.5 (uq−lq). Additionally, whiskers are truncated to
the nearest existing value within the bounds to avoid mean-
ingless values. The constant 1.5is chosen following [6].
The boxplots in Figure 4 show the distribution of the cor-
rectly answered questions with the lower bound 0meaning
that no question was answered correct and the upper bound
1attained when all answers were correct. The boxplots in
Figure 4a show the distribution for all participants for the
treatments KeY and SED, whereas Figures 4b–4d show the
distribution of correct answers broken down to diﬀerent lev-
els of KeY experience. Except for participants with ≥2
years of KeY experience, the results are better when using
SED. Only the group with ≥2years of KeY experience0 0.2 0.4 0.6 0.8 1SEDKeY
(a) All Participants
0 0.2 0.4 0.6 0.8 1SEDKeY
(b) No KeY experience only
0 0.2 0.4 0.6 0.8 1SEDKeY
(c)<2 years of KeY experience
0 0.2 0.4 0.6 0.8 1SEDKeY
(d)≥2years of KeY experience
Figure 4: Correct Answers
achieved better results using KeY. However, this class has
an outlier who answered everything correct using the SED.
Thedistributionofthemeasuredcorrectnessscores(which
take also partially correct answers into account, see Sec-
tion 4) are shown in Figure 5. In each case the correctness
score is equal to or higher than the correct answers (Fig-
ure 4). Except for the class of KeY users with ≥2years of
experience the achieved correctness scores are better when
using the SED as compared to KeY.
0 0.2 0.4 0.6 0.8 1SEDKeY
(a) All Participants
0 0.2 0.4 0.6 0.8 1SEDKeY
(b) No KeY experience only
0 0.2 0.4 0.6 0.8 1SEDKeY
(c)<2 years of KeY experience
0 0.2 0.4 0.6 0.8 1SEDKeY
(d)≥2years of KeY experience
Figure 5: Correctness Score
409The distributions of the conﬁdence scores are similar to
thecorrectnessscoredistributions. AsFigure6andFigure7
show, the conﬁdence is higher with SED except for the class
of KeY users with ≥2years of experience. In each case the
conﬁdence score increases (the boxplot “moves right”) when
taking partially correct answers into account.
−2−1 0 1 2SEDKeY
(a) All Participants
−2−1 0 1 2SEDKeY
(b) No KeY experience only
−2−1 0 1 2SEDKeY
(c)<2 years of KeY experience
−2−1 0 1 2SEDKeY
(d)≥2years of KeY experience
Figure 6: Conﬁdence Score
−2−1 0 1 2SEDKeY
(a) All Participants
−2−1 0 1 2SEDKeY
(b) No KeY experience only
−2−1 0 1 2SEDKeY
(c)<2 years of KeY experience
−2−1 0 1 2SEDKeY
(d)≥2years of KeY experience
Figure 7: Conﬁdence Score of Partially Correct An-
swersThe measured time6is shown in Figure 8. A value of 1(0)
means that a participant spent 100% (0%) of the time using
one tool. The participants with KeY experience spent less
time when using KeY than when using the SED. For partic-
ipants without KeY experience it is the other way round.
0 0.2 0.4 0.6 0.8 1SEDKeY
(a) All Participants
0 0.2 0.4 0.6 0.8 1SEDKeY
(b) No KeY experience only
0 0.2 0.4 0.6 0.8 1SEDKeY
(c)<2 years of KeY experience
0 0.2 0.4 0.6 0.8 1SEDKeY
(d)≥2years of KeY experience
Figure 8: Time
The null hypotheses of Table 3 can be rejected without
assuming a normal distribution using a one sided Wilcoxon
Signed Rank Test or a one sided Sign Test , see [14]. As
basis for the tests we used the results of allparticipants and
did not test each experience level separately, because there
are too few participants in each to apply any test method.
The signiﬁcance level is set to 0.05meaning that there is a
5% chance at which a hypotheses is wrongly rejected. The
results of the tests are shown in Table 6.
All tests reject the correctness-related hypotheses H0Q
andH0QSat the given signiﬁcance level, but not hypothe-
sesH0CS(for the conﬁdence taking partially correct answers
into account). As we can reject the latter hypothesis with
a signiﬁcance level of 0.1, we expect to be able to reject
hypotheses H0CSandH0Conce more people participate in
the experiment. However, the tendency of the data Figure 8
hints at only a low chance to reject hypothesis H0T.
11. INTERPRETATION
In Section 11.1 we analyze the proof situations in which
the participants performed better using the SED before we
summarize the feedback of the participants about speciﬁc
SED features in Section 11.2.
11.1 Correctness of Answers
Analysis of our experiment permits to conclude that par-
ticipants performed signiﬁcantly better in ﬁnding the actual
6Thetimesmeasuredforfouroftheparticipantswereinvalid
and, therefore, excluded.
410Table 6: One Sided Test Results with α= 0.05HypothesisWilcoxon Signed Rank Test Sign TestW-value
p-value
rejected
p-value
rejected
H0Q163,5 0,0479 true 0,0392 true
H0QS 172 0,0251 true 0,0392 true
H0C 149 0,1286 false 0,1917 false
H0CS 160 0,064 false 0,0946 false
H0T 79 0,4633 false 0,4018 false
reason for a failed proof attempt when they used the SED.
Concerning the conﬁdence participants put into the correct-
ness of their answers, we could not reject the corresponding
null hypothesis but there is clear tendency pointing towards
a higher conﬁdence of the participants in their answers to
those proof attempts for which they used the SED.
To answer the question whether the SED performs uni-
versally better or only in a speciﬁc proof situation, we look
at how often an expected correct answer was given using
each of the tools. Table 7 shows only the expected correct
answers. A selected answer does not mean that the question
is correctly answered, only that the participant selected it in
addition to possibly wrong answers. For each class of KeY
experience and both tools, the percentage how often an an-
swer is selected is given. If a correct answer was more often
selected using one of the tools, the value is colored in blue.
Almost all participants gave a correct answer to the ques-
tion whether a proof was closed. Some participants with
≥2years of KeY experience overlooked nodes marked as
not veriﬁed in the symbolic execution tree for ArrayUtil .
Consequently, they did not answer the questions about why
the proof is open. This indicates that usability of the SED
can be improved by better highlighting of nodes that are not
veriﬁed.
The correct reason why a proof remains open could of-
ten be better identiﬁed using SED. The only case where
this observation does not hold is where a proof could not
be closed, because a given loop invariant is not preserved.
In this case the results were better with KeY. A possible
explanation is that participants overlooked the “not veri-
ﬁed” marker attached to the loop body termination node in
the symbolic execution tree. Again, better highlighting of
unveriﬁed nodes might help. Another explanation is that
experienced KeY users understand loop invariants better in
general and hence they did better when using KeY.
The question about which execution paths do not comply
with the given speciﬁcation is answered more often correctly
using the SED. This is probably due to the visualization of
the symbolic execution tree which only distills the program
behavior which is hard to ﬁgure out from the standard proof
tree view of KeY. The SED clearly labels execution paths
that end with an uncaught exception which explains that
participants using the SED perform better when answering
questions about thrown exceptions.
When asked which contracts were applied during proof
search, an applied contract was more often correctly iden-
tiﬁed when using the SED. This is surprising, because KeY
has a feature for listing all applied contracts which was also
shown in the introductory video.Thequestionwhichstatementswereexecutedisoftenbet-
ter answered when using the SED, which highlights reached
code members directly in the source code. However, the
monitored participants often overlooked this feature and an-
swered this question by looking at the symbolic execution
tree. Interestingly, KeY users with ≥2years of experience
often failed to identify dead code in the Calendar proof at-
tempt when using KeY, but succeeded with SED.
The time spent using SED is always higher compared to
KeY. A possible reason is that most participants never used
the SED before. In the monitored evaluations, the partici-
pants needed some time to learn how to use the SED. Some
participants spent “non-productive” time to discover fea-
tures of the SED before actually answering the questions.
This behavior could not be observed when KeY was used,
perhaps because the participants already used KeY before.
We conclude that the SED helps in all proof attempts to
understand why the proof is still open. Participants without
or<2years of KeY experience achieved signiﬁcantly better
results using the SED. Participants with ≥2years of KeY
experience performed slightly better when using KeY.
11.2 Perceived Usefulness of Features
In the following we summarize how the participants clas-
siﬁed the helpfulness of selected features.7
For KeY almost all participants considered the proof tree
and the possibility to ﬁlter out intermediate proof steps as
mostly helpful. The goals tab that allows to navigate di-
rectly to open proof goals is considered helpful by 75% of
the participants, the others mostly never used it. The pos-
sibility to list applied contracts was used by less than half
of the participants. The feedback on the proof sequent view
(showing the proof obligation of the selected node in the
proof tree) is interesting: it is the only way in KeY to in-
spectanopengoalandtounderstandwhichpartoftheproof
obligation is not yet proven. Most participants without KeY
experience consider it as not helpful or only somewhat help-
ful; participants with some KeY experience considered it as
somewhat helpful and KeY user with ≥2years of experi-
ence considered it as very helpful. This suggests that years
of KeY experience are required to understand the cause for a
failed proof attempt when inspecting a failed proof attempt
with KeY.
For the SED, most participants considered the symbolic
execution tree, the highlighting of source code reached dur-
ing symbolic execution, and tracking the veriﬁcation status
of subformulas as very helpful. The variable view used to
show the symbolic state of a selected symbolic execution
tree node are mostly considered as somewhat helpful and
sometimes as not helpful. This view becomes important for
programs with complex heap modiﬁcations. This was not
the case in any of the four problems which explains the low
rating. Aliasing could occur only in one problem, hence
SED’s capability to visualize all (non-isomorphic) memory
layouts was used infrequently.
Some participants provided constructive suggestions for
improvements: ﬁrst, KeY and SED both translate JML ex-
pressions into logic terms and formulas. Mapping formu-
las in proofs back to corresponding JML constructs was
requested. Second, we highlight formulas with red, green
7Detailed results are presented at
www.key-project.org/eclipse/SED/
UnderstandingProofAttemptsEvaluation/results
411Table 7: Comparison of the Given Expected AnswersProof
AttemptKeY experience (%)
All None <2 y.≥2 y.
Question Answer KeY SED KeY SED KeY SED KeY SEDCalendarIs Veriﬁed No 100 100 100 100 100 100 100 100
Why not veriﬁedInvariant is not preserved 33 50 50 25 25 50 33 75
Exception is thrown 89 100 100 100 100 100 67 100
When not preserved After Else 33 33 50 25 25 50 33 25
What is thrown ArrayStoreException 89 100 100 100 100 100 67 100
What is executedLine 14 89 100 100 100 75 100 100 100
Line 32/33 67 100 100 100 50 100 67 100AccountIs Veriﬁed No 90 100 100 100 75 100 100 100
Why not veriﬁed Postcondition does not hold 40 100 0 100 25 100 100 100
When does not hold Termination 2 40 91 0 100 25 75 100 100
What is executedLine 11 100 100 100 100 100 100 100 100
Line 12 70 100 67 100 50 100 100 100
Line 13 80 100 67 100 75 100 100 100
Line 16 50 100 33 100 25 100 100 100
What is appliedMethod contract of withdraw 70 82 33 33 75 100 100 100
Method contract of canWithdraw 90 100 67 100 100 100 100 100ArrayUtilIs Veriﬁed No 100 91 100 100 100 100 100 75
Why not veriﬁedPostcondition does not hold 40 64 0 67 50 50 67 75
Loop invariant is not preserved 80 73 100 67 75 100 67 50
When does not hold Termination 2 40 64 0 33 50 75 67 75
When not preserved Loop Body Termination 1 60 45 67 0 50 75 67 50
What is executedLine 8/17/25 90 100 100 100 75 100 100 100
Line 9/10 90 91 100 67 75 100 100 100
Line 13 70 100 67 100 50 100 100 100
Line 14 70 91 67 100 50 100 100 75
Line 26/27 80 100 67 100 75 100 100 100
Line 34 50 91 33 100 25 100 100 75
Line 39 60 91 33 100 50 100 100 75My
IntegerIs Veriﬁed No 92 100 100 100 75 100 100 100
Why not veriﬁed Postcondition does not hold 69 88 75 100 75 100 60 50
What is executed Line 9 92 100 75 100 100 100 100 100
Winning Tool Count 3 32 6 16 0 30 8 7
and yellow to track their veriﬁcation status. Participants
with red-green blindness couldn’t distinguish these colors.
Finally, it was suggested to extend the SED in such a man-
ner that interactive proof steps can be performed without
leaving the tool. This was recently implemented. It is now
possible to use the SED as a full-ﬂedged replacement for the
standard KeY UI [9].
Participants were also asked which tool they prefer to in-
spect proof attempts. Both tools have diﬀerent strengths, so
it is not surprising that many let their preference depend on
the proof situation or task. Overall, participants without or
<2 years of KeY experience tended to prefer the SED and
experienced KeY users lean towards the standard KeY user
interface. This conﬁrms our conjecture that the SED lowers
the barrier to use the KeY system. To satisfy the needs of
KeY experts, features such as inspection of sequents and in-
teraction with the theorem prover are yet missing from the
SED.
12. CONCLUSION
We described an experiment comparing the eﬀectiveness
andeﬃciencyofunderstandingproofattemptsusingtwodif-ferent UIs of an interactive theorem prover: one with a focus
on proof objects and the other providing a view akin to an
interactive debugger. The result provides statistically sig-
niﬁcant evidence for increased eﬀectiveness when presenting
proofs in a debugger-like fashion. Especially inexperienced
users performed better with it. We can hence state that the
SED approach to user interface design lowers the barrier
to formal veriﬁcation. This is crucial when using theorem
provers in teaching and industrial contexts.
Despite the considerable eﬀort required to perform user
case studies like the one reported here, we strongly encour-
age system builders to do them: the investment is relatively
modest compared to the implementation eﬀort for major
user interface extensions, however, the result provides solid
support for the decision which development directions and
which features will give the largest return.
Acknowledgment:.
We thank all participants of the evaluation for their valu-
able time and feedback. Part of this paper was written dur-
ing a research sabbatical of the second author at University
of Torino whose support is gratefully ackowledged.
412References
[1] W. Ahrendt, B. Beckert, D. Bruns, R. Bubel, C. Glad-
isch, S. Grebing, R. Hähnle, M. Hentschel, M. Herda,
V. Klebanov, W. Mostowski, C. Scheben, P. Schmitt,
and M. Ulbrich. The KeY Platform for Veriﬁcation and
Analysis of Java Programs. In D. Giannakopoulou and
D.Kroening,editors, Veriﬁed Software: Theories, Tools
and Experiments , volume 8471 of LNCS, pages 55–71.
Springer, 2014.
[2] W. Ahrendt, B. Beckert, R. Hähnle, P. Rümmer, and
P. H. Schmitt. Verifying object-oriented programs with
KeY: a tutorial. In F. de Boer, M. M. Bonsangue,
S.Graf,andW.deRoever,editors, PostConf. Proc.5th
International Symposium on Formal Methods for Com-
ponents and Objects (FMCO) , volume 4709 of LNCS,
pages 70–101. Springer-Verlag, 2007.
[3] J. Aitken, P. Gray, T. Melham, and M. Thomas. In-
teractive theorem proving: An empirical study of user
activity. Journal of Symbolic Computation , 25(2):263 –
284, 1998.
[4] B. Beckert, S. Grebing, and F. Böhl. A Usability
Evaluation of Interactive Theorem Provers Using Focus
Groups. In Software Engineering and Formal Methods -
SEFM 2014 Collocated Workshops: HOFM, SAFOME,
OpenCert, MoKMaSD, WS-FMDS, Grenoble, France,
September 1-2, 2014, Revised Selected Papers , pages 3–
19, 2014.
[5] J. Cheney. Project Report: Theorem Prover Usability.
Technical report, Report of project COMM 641, 2001.
[6] M. Frigge, D. C. Hoaglin, and B. Iglewicz. Some Imple-
mentations of the Boxplot. The American Statistician ,
43(1):50–54, 1989.[7] M. Hentschel. Integrating Symbolic Execution, Debug-
ging and Veriﬁcation . PhD thesis, Technische Uni-
versität Darmstadt, Jan. 2016. http://tuprints.ulb.
tu-darmstadt.de/5399/.
[8] M. Hentschel, R. Bubel, and R. Hähnle. Symbolic Exe-
cution Debugger (SED). In B. Bonakdarpour and S. A.
Smolka, editors, Runtime Veriﬁcation, 14th Interna-
tional Conference, RV, Toronto, Canada , volume 8734
ofLNCS, pages 255–262. Springer, 2014.
[9] M.Hentschel, R.Hähnle, andR.Bubel. TheInteractive
Veriﬁcation Debugger: Eﬀective Understanding of In-
teractive Proof Attempts. In S. Apel and S. Khurshid,
editors,Proc. 31st IEEE/ACM International Confer-
ence on Automated Software Engineering (ASE), Sin-
gapore. ACM Press, Sept. 2016.
[10] A. Ireland, M. Jackson, and G. Reid. Interactive proof
critics.Formal Aspects of Computing , 11(3):302–325,
1999.
[11] G. Kadoda, R. Stone, and D. Diaper. Desirable fea-
tures of educational theorem provers - a Cognitive Di-
mensions viewpoint. In Proc. 11th Annual Workshop of
the Psychology of Programming Interest Group , 1996.
[12] G. T. Leavens, E. Poll, C. Clifton, Y. Cheon, C. Ruby,
D. Cok, P. Müller, J. Kiniry, P. Chalin, D. M. Zimmer-
man, and W. Dietl. JML Reference Manual , May 31,
2013. Draft Revision 2344.
[13] T. Nipkow, L. C. Paulson, and M. Wenzel. Is-
abelle/HOL: A Proof Assistant for Higher-Order Logic .
Springer-Verlag, Berlin, Heidelberg, 2002.
[14] C. Wohlin, P. Runeson, M. Höst, M. C. Ohlsson, and
B. Regnell. Experimentation in Software Engineering .
Springer, 2012.
413