Directed Test Generation to Detect Loop Inefﬁciencies
Monika Dhok
Indian Institute of Science, Bangalore, India
monika.dhok@csa.iisc.ernet.inMurali Krishna Ramanathan
Indian Institute of Science, Bangalore, India
muralikrishna@csa.iisc.ernet.in
ABSTRACT
Redundant traversal of loops in the context of other loops
has been recently identiﬁed as a source of performance bugs
in many Java libraries. This has resulted in the design of
static and dynamic analysis techniques to detect these per-
formance bugs automatically. However, while the eﬀective-
ness of dynamic analyses is dependenton the analyzed input
tests, static analyses are less eﬀective in automatically vali-
dating the presence of these problems, validating the ﬁxes
and avoiding regressions in future versions. This necessi-
tates the design of an approach to automatically generate
tests for exposing redundant traversal of loops.
In this paper, we design a novel, scalable and automatic
approach that addresses this goal. Our approach takes a
library and an initial set of coverage-driven randomly gene r-
ated tests as input and generates tests which enable detec-
tionofredundanttraversalofloops. Ourapproachisbroadl y
composed of three phases – analysis of the execution of ran-
dom tests to generate method summaries, identiﬁcation of
methods with potential nested loops along with the appro-
priate context to expose the problem, and test generation
to invoke the identiﬁed methods with the appropriate pa-
rameters. The generated tests can be analyzed by existing
dynamic tools to detect possible performance issues.
We have implemented our approach on top of the
SOOT bytecode analysis framework and validated it on
many open-source Java libraries. Our experiments re-
veal the eﬀectiveness of our approach in generating 224
tests that reveal 46 bugs across seven libraries, includ-
ing 34 previously unknown bugs. The tests generated us-
ing our approach signiﬁcantly outperform the randomly
generated tests in their ability to expose the ineﬃcien-
cies, demonstrating the usefulness of our design. The im-
plementation of our tool, named Glider, is available at
http://drona.csa.iisc.ac.in/~sss/tools/glider .
CCS Concepts
•Software and its engineering →Software performance;
Software testing and debugging;Keywords
redundant traversal bugs, performance, testing
1. INTRODUCTION
Performance is of signiﬁcant importance for any software
application. Unfortunately, underlying performance issu es
are hard to detect in-house during testing and usually mani-
fest in the ﬁeld [ 28]. Not surprisingly, these issues are found
even in well tested commercial products [ 1,2]. Since these
bugsdegradetheapplication responsiveness, techniquest hat
improve the possibility of detecting these problems before
deployment are helpful.
Many eﬀective techniques are developed to detect perfor-
mancebugsautomatically. These techniquesaddress variou s
kinds of performance related issues including repetitive c om-
putations [ 30,31], redundant loops [ 28], object bloat [ 27], la-
tentperformance bugs[ 21], andperformance issues inclouds
and smart phones [ 10,29]. The performance problems due
to repetitive and similar computations across iterations h ave
been found in many mature codebases [ 19,39]. This has
resulted in the design of eﬃcient static [ 31] and dynamic
analysis [ 30,28] techniques to detect these problems.
--------------------------------------------------- -----------
1. public class A {
2. public boolean containsAny(Collection c1, Collection c 2) {
3. Iterator itr = c1.iterator();
4. while(itr.hasNext())
5. if(c2.contains(itr.next()))
6. return true;
7. return false;
8. }
9.}
--------------------------------------------------- -----------
Figure 1 Example
Figure1illustrates the problem of redundant traversal.
Here, class AhascontainsAny method which accepts col-
lections, c1andc2, as input. The method iterates over
c1to check whether one of its elements is present in c2.
This innocuous looking code can result in poor performance.
IfcontainsAny is invoked with a non-empty hashset and
arraylist as parameters respectively, repeated invocatio n of
contains method (line 5) can result in a slowdown. This is
because the implementation of contains inArrayList has
linearcomplexity as it traverses the list to check the pres-
ence of the element. For each iteration of the outer loop
(line 4), the elements in c2areunnecessarily traversed. This
redundant traversal [ 31] can be addressed using memoiza-
tion [12], for example, by ensuring that c2is hashset in this
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
FSE’16 , November 13–18, 2016, Seattle, WA, USA
c2016 ACM. 978-1-4503-4218-6/16/11...$15.00
http://dx.doi.org/10.1145/2950290.2950360
Artifact evaluated by FSE✓
895
case. However, the core problem is to identify their exis-
tence.
Toddler [30] employs dynamic analysis to detect loops
which perform repetitive memory accesses. It takes a set
of tests as input and monitors their execution to detect
repetitive accesses across loop iterations. While unit tes ts
provided by developers can be used for this purpose, these
tests can be less eﬀective in revealing the entire gamut of de -
fects due to redundant traversals. This is because the unit
tests are usually directed towards ensuring functional cor -
rectness [ 13] and not necessarily to expose redundant traver-
sals. Therefore, if the test does not cover the loops per-
forming repetitive memory accesses, analyzing its executi on
becomes less useful. Manually writing the tests to help Tod-
dlerexpose the ineﬃciencies can be an arduous task.
In order to overcome the dependence on tests, Clar-
ity[31] employs static analysis to detect ineﬃcient loops.
Unfortunately, the defects reported by static analyses nee d
to be triaged by a programmer to ensure the validity of the
reported defects [ 24,5]. In comparison, if tests that expose
the defects are available, they can serve multiple purposes
– (a) help conﬁrm the validity of the bug based on the exe-
cution time, (b) the bug ﬁx can be automatically validated
by comparing execution times of the two program versions,
and (c) can be integrated into the testsuite to prevent any
regressions in future revisions. Therefore, to realize the se
beneﬁts without investing manual eﬀort, techniques for di-
rected generation of tests to expose loop ineﬃciencies are
desirable.
There are various challenges involved in generating tests
to detect these defects. Firstly, due to virtual call resolution,
a method invocation can be resolved to various methods de-
pending upon the type of the receiver. Generating tests
for all the possible resolutions of all the invocations is no t
scalable. Secondly , the realization of the defects can be de-
pendent on conditions that can aﬀect the reachability of the
problematic loop. Therefore, it is essential that the gener -
ated test has the appropriate context to ensure reachabilit y.
Finally, the problem can manifest only when the data struc-
ture being traversed has large number of elements arranged
in aspeciﬁcorder. For example, if the elements are ordered
in a manner that the loop is not traversed completely, then
the redundant traversal cannot be detected.
We elaborate these challenges using the example in Fig-
ure1. To expose the underlying problem, the generated test
must have the following characteristics:
•invokethemethod containsAny withnon-emptyinputcol-
lections so that both the loops execute suﬃcient number
of times,
•pass an object of typeArrayList as second parameter, and
•usedistinctelements in the input collections so that the
loop does not break at line 6.
Tests that satisfy these conditions will perform redundant
traversal of the list exposing the underlying problem. Our
goal is to automatically generate these tests.
In this paper, we address the goal of automatically gener-
ating useful tests to detect ineﬃcient loops in libraries. O ur
approach takes a library and a randomly generated [ 14,32]
testsuite as inputand generates atestsuite that helps expo se
the ineﬃcient loops in the library. Our approach is com-
posed of three phases – summary generation phase which
generates method summaries, method detection phase whichidentiﬁes methods whose invocation causes the ineﬃciencie s,
andtest generation phase which generates tests that invoke
the identiﬁed methods with appropriate parameters under
a suitable context. These tests can be analyzed by Tod-
dler[30] to expose the underlying defects.
More elaborately, in the summary generation phase, we
generate a random test suite for the library using existing
test generation tools ( Evosuite [14],Randoop [32]). Our
analysis instrumentsthesetests andanalyzes theirexecut ion
to derive method summaries. A method summary is com-
posed of information pertaining to the presence of loops, th e
object traversed in each loop, and methods (and the param-
eters) invoked. For example, the summary of containsAny
method in the Figure 1will represent the loop iterating on
c1and the invocation of the contains method on c2. We
leverage the method summaries to directthe test generation
process so that tests exposing loop ineﬃciencies alone are
generated. In other words, this phase prunes the large state
space of method sequences that can be invoked.
In themethod detection phase , our analysis traverses the
callgraph andidentiﬁesmethods thatmay executeineﬃcient
loops when invoked. For example, while traversing the call-
graph for the class corresponding to the example in Fig-
ure1, this phase detects that there exist a nested loop if
the method is invoked with c2of type ArrayList. It also
keeps track of the addition of objects to collection/array
ﬁelds by diﬀerent methods and identiﬁes these methods as
populator methods. In the test generation phase, we gener-
ate the test to invoke the identiﬁed methods in the previous
phase. We create the required objects (receiver and parame-
ters)andensurethatthemethodis invokedwithappropriate
types. Moreover, we set the context based on the concrete
execution from the initial set of tests so that the ineﬃcient
loops are reachable. Test generator also populates the coll ec-
tion/array ﬁelds (on which the loop traverses) with distinct
patterns and sizes using the populator methods detected in
the previous phase. The generated test will highly likely
expose the redundant traversal problem in the method.
We have implemented our approach on the sootbytecode
analysis framework [ 41]. We perform elaborate experimen-
tation and analyze seven popular Java libraries, including
Apache-collection andGuava. Our implementation gener-
ates 224 tests that enables detection of 46 bugs, including
34 previously unknown bugs. A few bugs reported by us are
conﬁrmed as real bugs by the developers of these libraries.
We ﬁxed the bugs (in-house) and executed the generated
tests on the original and ﬁxed versions. We observed per-
formance gains of 20 to 60% even with just 100 elements in
the collection objects on which the redundant traversal oc-
curs. Our experimental results also demonstrate that more
than 95% of the generated tests help reveal a defect. The
implementation of our tool, named Glider, is available at
http://drona.csa.iisc.ac.in/~sss/tools/glider .
The paper makes the following technical contributions:
•Wepresentanovelandeﬀectiveapproachtogeneratetests
for detecting ineﬃcient loops in Java libraries.
•Our analysis proposes a novel dynamic analysis to gen-
erate method summaries which are subsequently used to
identify methods with redundant traversals. These iden-
tiﬁed methods are invoked, with appropriate parameters,
as part of the tests generated by our approach.
896•We implement the proposed approach on the sootbyte-
code analysis framework and present the implementation
details necessary for our approach to be practical.
•We validate our approach on seven open-source Java li-
braries and generate 224 tests that enable the detection
of 46 bugs, including 34 previously unknown bugs.
2. MOTIV ATION
In this section, we motivate the need for our approach
by using a real example from the latest version (1.0.19) of
JFreeChart library, a free Java chart library that helps
developers generate professional quality charts.
CategoryPlot.java
--------------------------------------------------- ---------
1. public class CategoryPlot{
2. private Map<Integer, CategoryDataset> datasets;
3. public CategoryPlot() { ... }
4. public void setDataset(int index, CategoryDataset set) {
...
5. this.datasets.put(index, set);
...
6. }
7. public int indexOf(CategoryDataset dataset) {
8. for (Entry entry: this.datasets.entrySet())
9. if (entry.getValue() == dataset)
10. return entry.getKey();
11. return -1;
12. }
13. private List<CategoryDataset> datasetsMapped(int id x){
14. for (CategoryDataset set:this.datasets.values()){
...
15. int i = indexOf(set);
...
16. }
17. }
18. public List getCategoriesForAxis(CategoryAxis axis) {
20. int axisIndex = getDomainAxisIndex(axis);
21. datasetsMapped(axisIndex))
...
22. }
24. }
Figure 2 Motivating example.
Figure2presents a simpliﬁed implementation of Catego-
ryPlotclass. This implementation can cause an execution
slowdown when used in other applications because of the re-
dundantcomputationsinthemethod datasetsMapped (lines
13-17). This method traverses over datasets (line 14). Dur-
ing traversal, this method invokes another method indexOf
which also traverses over the same data structure (line 8).
This results in O( n2) complexity, where nis the number
of elements present in datasets . Ifdatasets is populated
with large number of objects, we can observe a signiﬁcant
slowdown. To detect this problem, a test should invoke
datasetsMapped method while ensuring that datasets con-
tains large number of elements. Apart from these require-
ments, there are intricate challenges in designing the test :
•The programmer needs to ﬁnd appropriate callsite to in-
vokedatasetsMapped which is a private method.
•The programmer should have detailed knowledge of the
class hierarchy. For instance, in the generated test, ob-
jects of type CategoryPlot ,CategoryAxis andCategory-
Dataset need to be instantiated.
•To add large number of elements to datasets , the pro-
grammer needs to identify the appropriate method.
•The programmer needs to update datasets with appro-
priate parameters. Updating a map by adding many ele-
ments at the same index will not be useful.TestCategoryPlot.java
--------------------------------------------------- -
1.public class TestCategoryPlot{
2. public static void main(String[] args) {
3. CategoryPlot categoryPlot = new CategoryPlot();
4. for(int i = 0; i < args[0]; i++) {
5. DefaultCategoryDataset defaultData;
6. defaultData = new DefaultCategoryDataset();;
7. SlidingCategoryDataset slidingData;
8.
9. slidingData = new SlidingCategoryDataset(defaultData ,i,i);
10. categoryPlot.setDataset(i, slidingData);
11. }
12. CategoryAxis axis = new CategoryAxis("abc");
13. categoryPlot.getCategoriesForAxis(axis);
14. }
15.}
Figure 3 Sample Test for defect in Figure 2.
Our implementation overcomes these challenges and auto-
matically generates the relevant test. To start with, the ap -
proach generates random set of tests for CategoryPlot class.
In thesummary generation phase, we monitor the execution
of these tests and derive method summaries. Further, in the
method detection phase , we identify methods that executes
nested loops (which includes the datasetsMapped method)
and the populator method that updates the collections (for
e.g.,datasets ) of the class. Subsequently, using static anal-
ysis we identify methods to create objects that can be used
to invoke the ineﬃcient method. Finally, we use this infor-
mation to synthesize the test. Figure 3shows a sample test
that exposes the aforementioned problem.
This test invokes getCategoriesForAxis method (line
13) which calls datasetsMapped . We create the receiver
of typeCategoryPlot for this method invocation at line 3.
The method setDataset accepts objects of type Category-
Dataset as input. Since categoryDataset is an interface,
required objects are created using appropriate constructo rs
of subclasses. We invoke setDataset method multiple times
to populate the datasets ﬁeld (line 10). Manually designing
this test is a non-trivial task. During execution, the test i t-
erates over datasets with numerous invocations of indexOf,
thereby, exposing the ineﬃcient loop in datasetsMapped
method. We ﬁxed the bug by merging the loops and ver-
iﬁed the ﬁx by executing the generated test, which resulted
in a 128×speedup between the new and old versions (when
the number of elements in datasets is 100K).1
3. DESIGN
Figure4presents the overall architecture of our approach.
Our approach takes a Java library as input and outputs
the set of the tests that enable the detection of redundant
traversals. The generated tests execute the potentially in -
eﬃcient loops with appropriate parameters. Our approach
is broadly composed of three phases – summary generation ,
method detection andtest generation . Initially, we generate
a set of random tests using coverage driven test generation
tools [14,32]. These tests and the input library are input
to thesummary generation phase. This phase generates
method summaries that are input to the method detection
phase. This phase outputs set of methods that will exe-
cute potentially redundant traversals. We also identify th e
1Our bug report available at
https://sourceforge.net/p/jfreechart/bugs/1147/ has
been acknowledged by developers and our suggested ﬁx is
incorporated in the version 1.0.20.
897(PHASE −1)random testsEvosuiteRandoop /libraryJava
summary generation
method detection
(PHASE −2)method summaries
inefficient methods populator methods
test generation
(PHASE −3)
generated tests
Figure 4 Architecture diagram.
populator methods that can be used to populate the collec-
tion/array ﬁelds on which the traversal occurs. Finally, test
generation phase generates tests that invoke the methods
identiﬁed in the previous phase using relevant parameters.
3.1 Summary Generation
In this phase, we generate method summaries which are
used in later phases to identify the methods with poten-
tial ineﬃciencies related to redundant traversal. A method
summary contains details about the loop traversals in the
method, objects on which the loops are traversed, and the
set of other methods invoked. We execute the randomly
generated tests and analyze the execution trace to derive
this information. We employ dynamic analysis here to gen-
erate precise summaries and depend on the test generation
tools [14,32] to provide an initial set of tests with good
coverage.
For this purpose, we will need to track the various method
invocations and the diﬀerent loops that are traversed. We
identifyeach loop inthemethodusingasequenceofsymbols.
The sequence length represents the nesting depthof the loop
in the method. The symbols provide theconnection between
the object on which the loop is traversed and the parameters
that are input to the method containing the loop. We also
track the method invocations along with parameters and use
symbols to connect the parameters in the invoking (current)
method and the parameters in the invoked method. More
speciﬁcally, we use the following data structures to encode
this information.
loop:< S1,S2,...,S k>
methodsInvoked :method→params
summary :loop→methodsInvoked
methodSummaries :method→summary
Eachloopis distinctly identiﬁed by a sequence of sym-
bols. The length of the sequence represents the nesting
depth. The symbols are based on the parameters input to
the method containing the loop. If the object on which
loop iteration happens at some nesting level is local (and
not reachable via the parameter passed to the method), we
use⊥to represent the symbol at that nesting level in loop.
methodsInvoked is a map from method toparams which
is the list of symbols corresponding to the objects passed
tomethod. The receiver of the method is encoded as the
ﬁrst parameter to the method invocation. If the parameter
passed to the invoked method is created locally, then werepresent it by ⊥. This is because the client of the invok-
ing method cannot inﬂuence the behavior relevant to that
parameter in the invoked method. summary is a map from
looptomethodsInvoked . This binding enables us to main-
tain the association between the loops and the methods in-
voked within the loop context. We represent the methods in-
voked outside a loop by mapping ⊥(representing absence of
loops) to methodsInvoked .methodSummaries integrates
summary of all the methods invoked from the tests.
1public void foo(A a, B b, C c) {
2HashSet set = new HashSet();
3for (E e : a) { // a is a collection of elements of type E
4 set.add(e);
5 for(E f: b) { c.remove(f); } // b -- collection of type E
6 a.baz(c);
7}
8a.update(10);
9}
Figure 5 Example for method summary generation
More elaborately, the methodsInvoked structure for the
simple example given in Figure 5is:
[(add4→<⊥,Sa>), (remove 5→< Sc,Sb>),
(baz6→< Sa,Sc>), (update 8→< Sa,⊥>)]
Here, the element (add4→<⊥,Sa>)means that method
addis invoked at line 4, the ﬁrst parameter (receiver) is
local and is therefore represented by a ⊥, and the second
parameter is given by symbol Sa, where Sais associated
with the symbol passed as the parameter to foo. The other
elements in the map can be constructed accordingly. The
distinct loops in fooare<Sa>,<Sa,Sb>representing the
loops from lines 3–7 and 5–5 respectively. The summary of
methodfoois shown in Table 1.
Table 1 summary for method foo
Loop methodsInvoked
< Sa>[(add4→<⊥,Sa>), (baz6→< Sa,Sc>)],
< Sa,Sb> [(remove 5→< Sc,Sb>)],
<⊥> [(update 8→< Sa,⊥>)]
Algorithm 1presents the procedure for generating the
method summaries for the input library. It takes a ran-
dom set of tests as input and outputs the methodSummaries
objectαcontaining the summary of each method. In this
procedure, we execute each test from the set of random tests
(line 2). During the execution of these tests, we monitor
three types of instructions – (a) loop start, (b) method in-
vocations, and (c) loop ﬁnish.
Algorithm 1 GenMethodSummary
Input: A random set of tests T
Output: methodSummaries α
1:foreach test tinTdo
2:execute( t);
3:while((I←nextInstruction( t))/∈{exception,halt})do
4: switch Ido
5: case(loop start) :
6: o←getLoopTarget() ;
7: if(isLocal( o))thenAppend⊥toloop;
8: elseAppend symbol(o) toloop;
9: case(loop ﬁnish):
10: Remove last symbol from loop
11: case(invocation of method x) :
12: m←getCurrentMethod()
13: MI←getMethodsInvoked( α[m],loop)
14: MI[x]←getSymbols( x);
15: α[m][loop]←MI;
16:returnα
898If the current instruction is a loop start, we use the auxil-
iary function getLoopTarget toobtain the object oon which
the loop traversal happens (line 6). If this object is create d
locally, modifying the object directly is not feasible from
the client invoking the method. Therefore, we append ⊥
toloop(line 7). On the other hand, if ois not local, then
we add the associated symbol (line 8). On a loop ﬁnish,
we simply remove the last element from loopto reﬂect the
nesting level appropriately. When a method xis invoked,
we get the methodsInvoked (inMI) in the corresponding
loopfrom the method summary of m, wheremcontains the
invocation of x(line 13). We update the method summaries
to appropriately represent the invocation of x(lines 14-15).
Information in the loopis intraprocedural. For ease of
presentation, we do not show the stack related operations
needed to maintain the intraprocedural data. For example,
ifxis invoked from mat a nesting depth of two, the invoca-
tions of other methods in x(assuming the absence of loops
inx) is considered to happen outside the context of a loop.
Wemergetheintra-proceduralinformationinthemethodde-
tection phase (see Section 3.2). At the end of the procedure
in Algorithm 1, we return the updated methodSummaries
object,α, containing the summary of each method.
3.2 Method Detection
In this section, we elaborately discuss the details of two
components in method detection . The ﬁrst component iden-
tiﬁes ineﬃciently implemented methods that execute nested
loops. The identiﬁed methods are invoked with appropri-
ate parameters during test generation. The second compo-
nent derives populator methods, which adds elements to the
collections or arrays and can be used to setup the context.
For instance, these methods are used to populate collection s
with large numberof elements, in speciﬁc patterns, toensur e
that repetitive memory accesses are performed across itera -
tions. Executing the populator methods is essential before
invoking the ineﬃcient methods to expose the problem.
3.2.1 Inefﬁcient Methods
Withthehelpofsummariesderivedforeachmethodinthe
previous phase, we determine if the method can potentially
execute nested loops. For this purpose, we traverse the call -
graph in a topologically reverse order and mergesummaries
at each node. Finally, we have a mapping from methods to
the list of possible nested loops. For any method min this
map, whenitisinvokedwithappropriateobjects correspond -
ing to the symbols present in the nested loops, the execution
will entail nested loop iterations. Exposing the magnitude
of the ineﬃciency will be handled by the populator methods
which is discussed in the next subsection.
The procedure to identify the ineﬃcient methods is pre-
sentedin Algorithm 2. Algorithm 2accepts themethodsum-
mariesαand a class θas input and outputs the candidates
map, which contains the map from methods to list of poten-
tial nested loops.
Initially, Algorithm 2builds the callgraph for the input
classθ(line 1), where each public method in the class is
considered an entry point. Vis the list of methods obtained
after performing a reverse topological sorting on G(line 2).
Webreakcyclesinthecallgraphrandomly. Foreachmethod
inV, if a method is the leaf node in the callgraph, then
other method invocations are not feasible. Hence, the only
possible loops are the loops that exist in m. Therefore, weAlgorithm 2 Identifying ineﬃcient methods
Input: method summaries α, Classθ
Output: candidates :method→loop
1:GraphG←getCallgraph( θ)
2:V←reverseTopologicalSort( G);
3:for(method minV)do
4:if(mis leafNode in G)then
5: candidates [m]←getLoops( α[m]);
6: goto label 3
7:for(calleenofm)do
8: loop←getLoop( α[m],n);
9: candidates [m]←loop⊗candidates [n]
10:remove all the private methods from candidates
11:remove any loop from candidates with length <2
12:returncandidates ;
extract all loops (using getLoops ) fromα[m] and add it to
candidates [m] (line 5).
For the non-leaf methods, we consider each callee nof
method m. This is to explore all possible nesting behavior.
Therefore, we recursively mergethe summary of methods m
andnusing the ⊗operator. This operator accepts a loop
corresponding to callee ninα[m] (line 8) and all possible
loops present in candidates [n] and generates a list of merged
loops (line 9). We deﬁne the ⊗operator using patterns:
/angbracketleftS1,...,Si/angbracketright⊗/angbracketleft⊥/angbracketright=/angbracketleftS1,...,Si/angbracketright (1)
/angbracketleft⊥/angbracketright⊗/angbracketleftS1,...,Si/angbracketright=/angbracketleftS′
1,...,S′
i/angbracketright (2)
/angbracketleftS1,...,Si/angbracketright⊗/angbracketleftSl,Sm/angbracketright=/angbracketleftS1,...,Si,S′
l,S′
m/angbracketright(3)
/angbracketleftS1,...,Si/angbracketright⊗/angbracketleft/angbracketleftSk/angbracketright,/angbracketleftSl,Sm/angbracketright/angbracketright=/angbracketleft/angbracketleftS1,...,Si/angbracketright⊗/angbracketleftSk/angbracketright,
/angbracketleftS1,...,Si/angbracketright⊗/angbracketleftSl,Sm/angbracketright/angbracketright(4)
Equation 1and2imply that the merge operation on ⊥is
identity,albeitwithmodiﬁcationstothesymbolsinequati on
2(representedbya′)tolocalize thesymbols in thecontextof
the caller. Equation 3represents the merging of two nested
loop sequences /angbracketleftS1,...,S i/angbracketrightand/angbracketleftSl,Sm/angbracketrightrespectively to out-
put a single nested loop sequence. Equation 4presents a
scenario where merging is done with two loops in the callee
(/angbracketleftSk/angbracketrightand/angbracketleftSl,Sm/angbracketright). It corresponds to independently merg-
ing the two loops as shown in the equation.
After updating candidates for each method in Algo-
rithm2(lines 1-9), we remove all the private methods from
candidates . This is because we can not invoke private meth-
ods directly. We also remove any loopwith length less than
two because these cannot expose any non-linear ineﬃcien-
cies. Finally, the candidates map contains a list of methods
and the possible nested loops within these methods. These
methods can be invoked with the appropriate parameters to
expose potential ineﬃciencies.
         
M1
M2
M6M7M5M3
M4
Figure 6 Example for deriving ineﬃcient methods
We now illustrate the working of Algorithm 2using the
callgraph given in Figure 6. The graph has seven meth-
ods named M1,...,M7. The shaded node ( M4) represents a
899private method. We assume an implementation where the
method summaries (including loops, methods invoked, their
parameters) are as shown in Table 2. For ease of presenta-
tion, we use the notation Sij, which represents the symbol
corresponding to parameter jof method Mi.
Table 2 Summaries for example in Figure 6
method mloop method params candidates[ m]
M4/an}bracketle{tS41,S42/an}bracketri}ht⊥ -/an}bracketle{tS41,S42/an}bracketri}ht
M2⊥ M4/an}bracketle{tS23,S22/an}bracketri}ht/an}bracketle{tS23,S22/an}bracketri}ht
M6 - - - -
M7/an}bracketle{tS71/an}bracketri}ht⊥ -/an}bracketle{tS71/an}bracketri}ht
M5⊥M6 - -
M7/an}bracketle{tS51/an}bracketri}ht/an}bracketle{tS51/an}bracketri}ht
M3/an}bracketle{tS32/an}bracketri}htM5/an}bracketle{tS31/an}bracketri}ht/an}bracketle{tS32,S31/an}bracketri}ht
M1⊥M2/an}bracketle{t⊥,⊥,S11/an}bracketri}ht/an}bracketle{tS11,⊥/an}bracketri}ht
M3/an}bracketle{t⊥,S11/an}bracketri}ht/an}bracketle{tS11,⊥/an}bracketri}ht
Algorithm 2traverses this graph in a reverse topological
order and starts from method M4. According to the sum-
mary in Table 2,M4does not invoke anymethod. Hence, we
add the existing loopto thecandidates map. Subsequently,
we consider M2which invokes M4withparams /angbracketleftS23,S22/angbracketright
outside a loop (based on a ⊥forloop). We merge /angbracketleft⊥/angbracketrightand
candidate[ M4] given by /angbracketleftS41,S42/angbracketrightto obtain /angbracketleftS23,S22/angbracketright. This
is because the loop in M4corresponds tothe ﬁrst two param-
eters ofM4andoursummaryconnectsthemtothethirdand
second parameters of M2respectively. Similarly, while cal-
culating the candidate[ M1], we observe that M2is invoked
outside a loop. The candidate[ M2] speciﬁes the presence of
a nested loop associated with traversing /angbracketleftS23,S22/angbracketright. Since,
this corresponds to /angbracketleftS11,⊥/angbracketrightbased on the parameter sum-
mary for M2when invoked in M1, we obtain the relevant
candidate[ M1]. The remaining candidates shown in the ta-
ble are obtained in a similar manner.
Finally, we can remove M4which is a private method and
anyloopwith depth <2 (e.g., in M7,M1andM5). The
ﬁnal candidate mappings are given below:
[M2→ /angbracketleftS23,S22/angbracketright,M3→ /angbracketleftS32,S31/angbracketright]
If we invoke M2andM3with the appropriate parameters,
we can potentially expose the ineﬃciencies due to the imple-
mentation of M4(a private method) and the nesting that
transcends method boundaries ( M3andM7).
3.2.2 Populator Methods
In this section, we discuss the procedure to derive popula-
tor methods. Given an ineﬃcient method that needs to be
invoked along with symbolic information on parameters, we
need a mechanism to populate the collections corresponding
to those symbols to enable traversal of the loops.
In order to derive the methods that can help populate the
collection objects, we monitor the execution of methods tha t
operate on the objects of that type. If there is an increase
in the total count of the elements in the collection object
after an invocation of a method, the method is identiﬁed as
a possible populator method. Since there can be multiple
methods which can aﬀect the overall count, we rank the
identiﬁed methods based on the behavior of the method in
diﬀerent contexts and select the highest ranking method.
More elaborately, we consider each test from the suite of
random tests. For any method minvoked from a test, we
statically extract the set of class ﬁelds (of collection typ es),
sayW, updated in the method. We execute the test and
count the number of elements present in each ﬁeld w∈W,
just before invoking the method m. After method exit, we
recount the number of elements in the ﬁeld wfor presence of
additional elements. If additional elements are present, w econsiderthemethodasapossiblepopulator. Afterrepeatin g
this procedure for each test from the set of random tests,
we rank the overall populator methods for any given type
and select the highest ranking method. Also, we do not
consider constructorsas possible populatormethodsbecau se
they cannot be repeatedly invoked to increase the size and
will deliver independent objects.
Class B
----------------------------
1.public class B {
2. HashSet set;
3. public B(){ set = new HashSet();}
4. public void add(A a) {
5. set.add(a);}
6. public void initialize() {
7. set = new HashSet(new A()); }
8. }Test
--------------------------
1.B b = new B();
2.b.initialize();
3.b.add(new A());
4.b.add(new A());
Figure 7 Example class Band corresponding test
We use the example in Figure 7to illustrate our approach.
For the class Bunder consideration, the constructor initial-
izes the ﬁeld set. Method addadds input object atoset
(line 5) and method initialize assigns a new HashSet with
one element (line 7). While monitoring the methods invoked
from the test (shown on the RHS of the ﬁgure), we ignore
the invocation of the constructor at line 1. Before invoking
initialize (at line 2), we count the number of elements in
setand obtain 0. After executing initialize , we observe
an increase in the number of elements by one and is consid-
ered a possible populator. Similarly, we observe addition o f
elements by addand include it as a populator. Since add
increases the overall count under diﬀerent contexts, we ran k
it higher and select it as a populator method for setinB.
3.3 Test Generation
In this section, we discuss our approach to generate tests
that execute nested loops in the library with the appropri-
ate objects. Algorithm 3takes the methods identiﬁed as
ineﬃcient and populator, from the previous phase, as input.
We represent the set of methods using IandPrespectively.
The goal then is to generate tests that invoke methods in I
and ensure that the objects corresponding to the parameters
of the method have large number of elements and cover the
code region that contain the ineﬃciencies.
Algorithm 3 genperftests
Input: Class (θ), Ineﬃcient methods ( I), Populator methods ( P)
1:for(eachminI)do
2:for(eachloopinI[m])do
3: receiver =createObject (θ)
4: for(each parameter iinm)do
5: if(iis inloop)then
6: type←getType( i)
7: µi←createObject (type)
8: mp←P[type]
9: for(j = 0; j <COUNT; j++) do
10: for(each parameter ptomp)do
11: cj=createObject (getType( p))
12: Invokemponµiwith params ( c1,c2,...,cn)
13: else
14: µi←Reuse parameter from test invoking m
15: Invokemwithreceiver on params ( µ1,µ2,...,µ n)
Algorithm 3traverses all methods in Imap (line 1). For
each method m, we iterate over each problematic nesting
behavior (line 2). We instantiate the method musingcre-
ateobject , an auxiliary function, which takes the type of
the object as input.
900We perform a simple static analysis to enable instantia-
tion of the object. If the class is public, we extract the set
of public constructors. If the class has only private constr uc-
tors, we derive the appropriate callsites within the class a nd
use them for instantiation. If the class is private, abstrac t
or an interface, we traverse the class hierarchy graph [25]
and check all the public subclasses and identify public con-
structors. If the constructor requires further objects, we
recursively perform a similar procedure. After collecting all
the required constructors, we instantiate the objects appr o-
priately to obtain the receiver for invoking the method m.
After creating the receiver object, we need to create pa-
rameters to invoke the method m. Moreover, we need to en-
sure that parameter objects on which loop traversals happen
in the implementation of mare suitably populated. There-
fore, if the parameter corresponds to an object on which a
loop traversal can happen, it will be present in the sequence
of symbols represented by the loop (line 5). Otherwise, we
simply reuse the parameter from the original random test
invoking m(line 14). This is because if the ineﬃcient region
under consideration is dependent on the value of the param-
eter, there will be minimal changes and the possibility of th e
generated test reaching the ineﬃcient regions is higher.
If the parameter is part of the loop sequence, then we
create the object of the appropriate type after obtaining it s
type(lines 6-7). Further, wepopulatetheconstructedobje ct
(lines 8-12). We obtain the populator method mpbased on
the type of the object (line 8). Subsequently, we invoke the
method mpbased on a parameterized number of times (
COUNT). To obtain parameter objects for the invocation, we
employcreateobject (line 11) to instantiate the objects.
We now illustrate the working of Algorithm 3using an
example. Consider the input IandPas follows:
I:[(A.foo<Bb1, Bb2, bool ﬂag >→<S1,S2>)]
P:[B, update <C>]
Based on the data in I, we need to generate a test that
invokesfoowith appropriate objects for ﬁrst and second pa-
rameters to foo. The populator method for object of type B
isupdatewhich takesobjects oftype C. Figure8presentsthe
test generated. It invokes method foothat will help expose
the redundant traversal defect. For the ﬁrst two parameter
objects on which the loop traversal happens, the objects are
created (at line 4) and populated subsequently using the in-
formation present in P. Because the third parameter to foo
need not be modiﬁed, we simply obtain the value used in
the concrete run using getRuntime method.
--------------------------------------------------- -----------
1. public class Test {
2. public static void main(String[] args) {
3. A a = new A();
4. B b1 = new B(); B b2 = new B();
5. for(int i=0; i < args[0]; i++){
6. C c = new C(random(Int));
7. b1.update(c);
8. }
9. for(int i=0; i < args[0]; i++){
10. C c = new C(random(Int));
11. b2.update(c);
12. }
13. a.foo(b1, b2, getRuntime(flag));
14. }
15.}
--------------------------------------------------- -----------
Figure 8 Generated test4. IMPLEMENTATION
We have implemented the algorithms discussed earlier as
part of the sootbytecode analysis framework [ 41]. Our
implementation generates tests for Java libraries. We now
discuss the tradeoﬀs considered to make our implementation
practical.
4.1 Populating the Collections with Patterns
In Algorithm 3, we discussed the procedure to instantiate
the objects that can be used as parameters for the various
method invocations. We now discuss how the parameter ob-
jects can be populated. While a straight-forward approach
to populate collection objects is to provide randomobjects,
it may not always be useful in exposing potential problems.
For instance, if there is a search of an element in a traversal
and the element being searched and the ﬁrst element in the
traversal coincidentally match, then the underlying probl em
may not beexposed. This is also necessitated as our analysis
is light-weight and does not track path constraints. There-
fore, to broaden the possibility of identifying the ineﬃcie n-
cies, we use the patterns based on size, similarity among the
elements across collections and the type of elements.
1.Size: To handle multiple scenarios where the imple-
mentation makes choices dependent on the size of the
collection objects on which the traversal happens, we
use two variants for all pairs of nested loops – (a) col-
lection sizes are equal, and (b) size of ﬁrst collection is
less than the second and vice versa.
2.Similarity : Afewiterationsare dependentonthesim-
ilarity of elements across the collection objects under
consideration. Therefore, we use three variants to pop-
ulate the collections with elements – (a) all elements
are distinct, (b) all elements are the same, and (c) ﬁrst
collection is a subset of the second collection and vice
versa.
3.Type of elements : If the collection has a speciﬁc
type, and is extended by multiple other types, we en-
sure that the original collection can take all possible
types. This is to ensure that any path condition that
is dependent on the type of the collection is satisﬁed,
thereby, exploring the code covered by the condition.
Because of the three diﬀerent patterns, the number of pos-
sible combinations among them (e.g., same size – diﬀerent
elements – same type, same size – same elements – same
type, etc.) can be signiﬁcant. Our current prototype han-
dles a limited combination of these patterns.
4.2 Handling Multiple Method Summaries
The input random tests can invoke the same method mul-
tiple times under diﬀerent contexts. Therefore, we need to
choose a summary among the possible summaries in the
process of generating candidate methods. We prioritize the
summaries based on the nesting depth of the loop, methods
invoked from the loop, and number of methods invoked.
More speciﬁcally, for two summaries α1andα2, we prior-
itize them as follows. If α1has nesting loop depth greater
than that of α2, we use the former. If both have the same
nestingloop depth, thenwe prioritize thesummary that con-
tains more method invocations within a loop context. If this
value is also the same, we choose the summary that contains
more method invocations (outside loops).
901Table 3 Benchmarks
Benchmark IDVersionKLoC#Classes
analysed#Methods#Static nested
loops#Randomly
generated testsAnalysis
time (S)
Apache collection B14.4.1117 10 364 45 621 753
PDFBox B21.8.10 219 2 80 8 124 275
Groovy B32.4.6197 2 69 10 108 59
Guava B4182517 4 585 21 89 241
JFreeChart B51.0.19 233 2 280 42 62 214
Ant B61.8.4187 3 73 9 105 141
Lucene B75.2.1320 2 168 42 60 85
4.3 Parameterizing Random Test Generation
Our approach accepts a randomly generated set of tests
as input. We use EvoSuite [14] andRandoop [32] for this
purpose. A key goal is to invoke more number of methods so
that inter-procedural summary information can be updated
suitably. Therefore, we guide the random test generation
tools to invoke more methods by setting the relevant param-
eters in these tools.
4.4 Optimizing Virtual Call Resolution
We statically analyze types of all the ﬁelds in the class
and track the type bindings performed in each constructor.
This is to optimize the virtual call resolution and reduce th e
overall possible set of methods that can be invoked.
1. public class A {
2. public B order;
3. public A() { order = new C(); }
4. public void foo(){
5. ...
6. order.baz();
7. ...
8. }
We illustrate this using a simple example as shown above.
Acontains a ﬁeld orderof typeB(line 2). The constructor
(at line 3) restricts the type to ﬁeld C(under the assumption
thatBis a superclass of C). In the invocation of bazfrom
foo, we use this information to restrict the possible types
on which bazis invoked.
5. EXPERIMENTAL EV ALUATION
In this section, we report the evaluation of our implemen-
tation and demonstrate the eﬀectiveness of our approach.
We have applied our approach to many popular Java li-
braries. Our selection of benchmarks was guided by earlier
bug reports on these benchmarks [ 30,31]. We performed
our experiments on an Ubuntu-14.04 desktop machine with
a 3.5Ghz Intel Core i7 processor with 16GB of RAM.
Table3providesthedetailsofthebenchmarksusedforour
experiments. Apache collection provides many powerful
datastructuresthatare usedtobuildJavaapplications; PDF-
Boxis a Java tool for working with pdf documents; Groovyis
aoptionallytypeddynamiclanguage thathasstatic compila -
tion capabilities; Guavais the Google Core Libraries for Java;
JFreeChart is a Java chart library to display professional
quality charts; Antis a Java library and command-line tool
to help build software; and Luceneis a high-performance,
full-featured text search engine library. For brevity, as i ndi-
cated in the table, we refer to our benchmarks as B1through
B7.
The table presents the version of the diﬀerent benchmarks
used for our experiments. The lines of code varies from
117Kl for Apache collection to 2.5MC for Guavalibraries.
We select the classes in these benchmarks on which bugs are
reported in other papers [ 30,31]. The cumulative numberof methods in all these classes varies from 69 for Groovyto
585 forGuava. We also count the static nested loops present
in the analyzed code which ranges from 8 to 45. The total
method and loop count indicate, without even considering
the parameters to method invocations and the complexities
duetovirtual calls, that analyzingevenfew classes manual ly
is a nontrivial task.
We use EvoSuite [14] andRandoop [32] to generate
the initial tests of random tests that is used as input to
our implementation. We restrict the number of tests gener-
ated by these tools to 200 for each class under consideration .
The overall time to generate the random tests ranges from a
minute to 12 minutes. Essentially, these tests invoke the va r-
ious methods in the class with random objects. We designed
our experiments to answer the following research questions :
1.RQ1: Is our implementation eﬀective in generating bug-
revealing tests?
2.RQ2: Is our approach useful for practical adoption?
3.RQ3: Are randomly generated tests suﬃcient to expose
the redundant traversal problem?
4.RQ4: How many elements in the collection object will
be necessary to expose the underlying performance issue?
5.1 RQ1: Effectiveness of Test Generation
Table 4 Information on generated tests, detected
bugs and analysis time.
ID#Generated
tests#Bugs#New bugs#False
positivesAnalysis
time (Min)
B1 80 16 9 1 45
B2 30 6 6 0 12
B3 20 5 4 0 7
B4 50 9 10 1 28
B5 15 3 1 4 24
B6 24 6 3 1 15
B7 5 1 1 0 16
Total 224 46 34 8 147
Table4presents the information on the tests generated us-
ing our approach when the initial test suite consists of test s
from various test generators [ 32,14]. The number of gener-
ated tests varies from 1 for B7to 16 for B1. This is signiﬁcant
reductionfrom thetotal numberofrandomlygeneratedtests
that is input to our implementation. Ideally, these tests wi ll
be input to Toddler [30] to detect bugs. However, since
the implementation of Toddler is unavailable2, we manu-
ally analyzed the generated tests. This helped us reveal 46
bugs in these benchmarks, including 34 previously unknown
performance issues .
The number of bugs detected also depends upon the gen-
erated tests. We observe that 36 bugs are detected when
the set of patterns proposed in the Section 4.1are disabled.
Other bugs are missed because the test with default pat-
tern either generates objects of inappropriate types or do
not meet the conditions to execute the loop.
2Personal communication – Adrian Nistor
902Our approach is able to generate useful tests by analyzing
random tests. The generated tests detect performance
problems even in well-tested open-source Java libraries.
5.2 RQ2: Practicality of Our Approach
Table4also gives information on the practicality of our
approach. There are two key issues involved w.r.t practical -
ity of program analysis tools – false positives and analysis
time [5]. A few tests generated by our approach do not re-
veal any defects. Our approach generates eight tests that
do not reveal any defect.3Based on industry standards [ 24],
the false positive rate of less than 5% is negligible.
On closer examination, the false positive tests are mainly
due to two reasons – generated tests take a diﬀerent path
compared to the original test, and absence of redundancy
during nested loop traversal. For example, the implementa-
tion of the library in Apache-collection compares the size
of objects present in the two input collections and follow
a path based on the result. Since, our generated test did
not use this constraint (recall that our current prototype e x-
plores a limited combination of patterns for parameters), w e
generate atestwhere adiﬀerentpathis taken. Inafewother
cases (e.g., JFreeChart ,ant), there is no redundancy in the
traversal (e.g., 2-dimensional table, checking for duplic ate
elements in list). This set of false-positive can be elimina ted
when their executions are analyzed using Toddler .
The overall time taken to generate the tests for all the
benchmarks is around 2.5 hours. On average, this corre-
sponds to 6 minutes per analyzed class. This time depends
on the number of tests considered in the beginning and also
the length of method sequences in those tests.
More than 95% of the tests generated by our approach
help in bug detection and the time taken is less than ﬁve
minutes per class. These numbers indicate the potential
for seamless integration of our implementation in the soft-
ware development process.
5.3 RQ3: Comparison with Randomly Gen-
erated Tests
 0 20 40 60 80 100
B1 B2 B3 B4 B5 B6 B7Percentage performance improvement
BenchmarksRandom tests
Generated tests21.11s
3.49s
4.94s
3.51s 3.92s
18s
3.09s2.59s
0.27s
0.24s
0.96s
0.25s
0.68s
3.06s
Figure 9 Comparison with randomly generated
tests.
We now discuss the usefulness of the tests generated by
our approach as compared to the random tests. The ﬁrst
author created new versions of the libraries by ﬁxing the
3Many defects are detected by multiple generated tests.bugs appropriately by removing redundant traversals. The
generated testsuite (with 10K elements populated in the col -
lection objects) and the (input) random testsuite were ex-
ecuted on the original and ﬁxed versions of the libraries.
Figure9presents the percentage performance improvement
of the ﬁxed version over the original version for the two test -
suites across all the benchmarks. The time taken to execute
the original version is shown on top of the bar (e.g., 21.11
seconds to execute randomly generated tests on B1).
The ﬁgure clearly demonstrates the huge performance
gains observed on the generated tests as compared to the
random tests. This is because the generated tests are di-
rected towards exposing the problems that are addressed by
our ﬁxes. Moreover, the time taken to execute the generated
tests on the original version is signiﬁcantly less than that of
the random tests across all benchmarks. This suggests that
the generated tests can be used as part of a regression test-
suite. Moreover, themagnitudeofperformancegains implie s
that the use of existing dynamic tools like Toddler will be
more successful with the generated tests.
The generated tests take less time toexecute and are more
suitable to expose the magnitude of the underlying perfor-
mance problem.
5.4 RQ4: Size of the Collection Objects
In order to use our implementation for practical purposes,
we wanted to ﬁnd the number of elements in the collections
to help expose the ineﬃciencies with the loop. Therefore,
we consider the original and ﬁxed versions of the bench-
mark. Further, we modiﬁed the number of elements that are
populated in the collection (based on the COUNTparameter)
from 100 to 100K. Then we execute the generated tests on
the original and ﬁxed versions and compare the percentage
performance improvement in time between the two versions.
Figure10presents the corresponding results.
 0 20 40 60 80 100 0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000Percentage performance improvement
Input sizesB1
B2
B3
B4
B5
B6
B7
Figure 10 Percentage performance improvement.
Accordingtotheﬁgure, theperformance improvementsat-
urates after 10K elements. In other words, populating the
collection objects with 10K elements will yield suﬃcient re p-
etition of memory accesses leading to the detection of the
underlying problem. Populating the collection with fewer e l-
ements (e.g., less than 1K elements) may not always demon-
strate a substantial diﬀerence.
Populating the collection objects with 10K elements will
enable detection of performance issues.
9035.5 Threats to Validity
Our approach is sensitive to the initial set of random tests
used. If these tests do not cover problematic code regions,
our approach will not be able to generate the necessary tests ,
a drawback that is shared with other dynamic analyses. Our
ability to identify populator methods can also be hampered
due to this reason. Also, as we do not explicitly track the
path conditions, it is possible that the generated tests may
cover a diﬀerent path, which can reduce the eﬀectiveness
of our approach. Although, benchmarks used in our exper-
imental results are representative of most codebases, it is
possible to have custom codebases where our analysis may
be less eﬀective.
6. RELATED WORK
Detection of redundant traversal bugs.
Elegant techniques to detect redundant traversal bugs
have been designed recently [ 30,31].Toddler [30] is a
dynamic analysis technique that analyzes execution of in-
put tests to detect repetitive memory accesses. The eﬀec-
tiveness of the technique is dependent on the ability of the
input tests to expose the problem. Manually writing these
tests is nontrivial andrandom test generation is not eﬀecti ve.
Our approach is complementary to Toddler , as we gener-
ate the tests that can be used by it. Clarity [31] analyzes
the source code of the library to detect redundant nested
loop traversals. However, as we discuss earlier, it is diﬃcu lt
to automatically conﬁrm the bug, validate any ﬁx and use
the information to avoid future regressions. In contrast, o ur
approach generates tests to serve these purposes.
Detection of performance bugs.
Many useful techniques to detect a variety of performance
bugs have been designed [ 27,21,10,43,12]. Dynamic anal-
yses have been proposed to detect problems pertaining to
object bloat in Java programs [ 38,44,45,46,26]. Muddu-
luruet al[26] propose an approach to eﬃciently instrument
object ﬂow proﬁles and use them to detect object bloats.
Yanet al[46] propose a framework for building runtime
graphs that can be used to detect this problem. Resurrec-
tor[43] uses dynamic analysis to detect problematic code
regions, which can be ﬁxed to reduce GC pressure. Bhat-
tacharya et al[6] propose a hybrid approach to detect object
bloat due to unnecessary creation of temporary container
and string objects. Techniques that propose static identi-
ﬁcation of locations where dead objects can be reclaimed
are also proposed [ 17]. Other proposed techniques include
identifying wasteful use of temporary objects [ 38], incorrect
use of data structures [ 37], latent performance bugs [ 21] and
performance issues in clouds and smartphones [ 10]. Event-
Break [33] uses a random testsuite and identiﬁes event han-
dlers whose execution time may gradually increase while us-
ing applications. This approach generates more tests based
on its execution. Our approach diﬀers by doing white-box
analysis and by focusing on unit-level tests. All these tech -
niques are geared towards detecting other kinds of perfor-
mance problems. Our approach generates tests to expose
redundant traversal bugs.
Automated test generation.
Automatic test-generation tools [ 32,14] for Java utilize
feedback from generated tests. We use the tests generatedby these random test generators as input to bootstrap the
process of generating tests necessary for exposing loop ine ﬃ-
ciencies. Seeker [40] combines static and dynamic analyses
to synthesize method sequences that are necessary for high
coverage testing. Concolic testing that integrates symbol ic
execution with concrete trace information enhancing path
coverage has been eﬀective in detecting bugs [ 16,15,8,9].
Our approach is inspired by these techniques and combines
concrete traces with symbols to generate the necessary test s.
Our approach will beneﬁt from the tests generated by this
approach as it increases coverage, thereby, the possibilit y of
exploring the problematic code regions containing the loop s.
Test generation for detecting concurrency bugs.
Automatically generating multithreaded tests to detect
data races [ 36], deadlocks [ 34], and atomicity violations [ 35]
have been found to be eﬀective in detecting rare bugs in
well-tested and thread-safe Java libraries. These techniq ues
analyze the execution of a random set of sequential tests
to generate the required tests. Our current approach is in-
spired by the successes of the test generators in the context
of detecting concurrency bugs. Our approach also operates
by analyzing a random set of tests and uses this concrete
information to generate relevant tests.
Proﬁling.
Proﬁling is a common technique to detect performance
problems in programs. Ball and Larus [ 4] propose a num-
bering scheme to get statistics on the control ﬂow paths
traversed in an execution with minimum overhead. Many
extensions to this technique have been proposed [ 42,11,22].
In [11], the imprecision of path proﬁling due to loop iter-
ations is addressed. Improving garbage collection by pro-
ﬁling data due to dynamically created objects is also pro-
posed [7,20,23,3,18]. Our work is orthogonal to these
approaches as we enable detection of unnecessary traversal s
of loops to improve the performance of libraries.
7. CONCLUSIONS
Redundant traversal of loops under nested conditions can
aﬀect the overall performance of Java libraries. The useful -
ness of existing techniques to detect these problems can be
signiﬁcantly enhanced in the presence of a directed test gen -
erator that generates tests to help expose these ineﬃcienci es.
In this paper, we designed a novel, scalable and automatic
dynamic analysis technique that analyzes the execution of
randomly generated tests to construct targeted tests, whic h
can serve as input to existing dynamic analyses. The evalu-
ation of our implementation on many Java libraries demon-
stratetheeﬃcacyofourdesign. Ourtoolgenerated224tests
that enabled detection of 46 bugs, including 34 previously
unknown bugs.
8. ACKNOWLEDGMENTS
We thank the anonymous reviewers for their useful feed-
back. We are grateful to Sridhar Gopinath for help in set-
ting up the initial infrastructure for experimental evalua tion.
The ﬁrst author is supported by a Google travel grant and
the second author is supported by a Google faculty research
award.
9049. ARTIFACT DESCRIPTION
9.1 Introduction
The artifact is a 4GB tar.gz ﬁle, and is available at
http://drona.csa.iisc.ac.in/~sss/tools/glider . The
uncompressed folder consists of a VM image, and a
README ﬁle which describes the instructions along with
snapshots for clear understanding of the artifact. The vir-
tual image provided has the required execution environment
set up to run our analysis. Packages like maven, ant and
gnuplot are preinstalled for building benchmarks and gener -
ating graphs. We have installed both Java7 and Java8 in the
virtual machine. This is necessary since random test gener-
ators like Evosuite work with Java8 and soot works with
Java7.
9.2 Hardware Dependencies
1) Virtual box with version 5.0.18
https://www.virtualbox.org/wiki/Downloads
2) System with at least 12GB of main memory.
3) System with at least 8 cores (preferably).
9.3 Installation
To execute virtual machine :
1) Download and uncompress the artifact (around 10GB).
2) Open VirtualBox.
3) Create a new VM by clicking machine – >new.
4) Give a name to the new VM.
5) Select Linux type and Ubuntu (64bit) version.
6) Click Next, select 4GB of RAM and select Next again.
7) In the hard drive selection screen, select the option to
”use an existing hard drive ﬁle”, then select artifact.vdi ﬁ le
containing the VM image that you just downloaded. Go
to settings- >system->processor and assign 4 cores to VM.
Start the VM with Username:artifactPassword:123
9.4 Description
The tool is present in the folder named artifact on the
desktop which is organized as:
•versions : original and modiﬁed versions of benchmarks.
•randoop, evosuite : contains scripts to generate and parse
randomly generated tests.
•sootAnalysis : static and dynamic analysis.
•perfTests : tests generated to detect loop ineﬃciencies.
•expected-perfTests : expected generated tests to detect
loop ineﬃciencies.
•ﬁgures: displays generated ﬁgures along the dimensions
of ﬁgure9 and ﬁgure10 in the paper.
•benchmark : contains the program under analysis.
•bugs: a ﬁle with detailed description of the bugs detected.
The artifact folder contains a copy of the paper named
paper.pdf.
9.5 Experimental Setup
We describe three steps that might be followed as part of
evaluation. We have used the environment variable $fsehome
in the subsequent discussion which points to the artifact
folder on the desktop. The ﬁrst step shows the working on a
simple example. This stepdescribes thesummarygenerated,
and how it is leveraged further for synthesizing tests. In th esecond step, we show a demonstration on a class. The third
step runs all the benchmarks together and generates data
for Figure 9 and Figure 10 as shown in the paper. [You can
skip STEP 1 and 2 to directly run benchmarks].
9.5.1 STEP1 : Analyzing a Simple Example
We have added examples in $fsehome/sootAnalysis/test
folder so that one can get familiar with the tool.
1) To run these examples, cd$fsehome/sootAnalysis
2) Execute ./sampleTest.sh ; outputs the summaries for each
method in Type.java and the generated tests. [Run other
examples using instructions at the header of sampleTest.sh ]
9.5.2 STEP2 : Analyzing a Class from Benchmarks
Consider the class CollectionBag from collections which is
placed in the package org.apache.commons.collections4.bag
1) Change the working directory to evosuite
cd$fsehome/evosuite
2) Remove contents of benchmark and perfTests
rm-rf$fsehome/benchmark/*
rm-rf$fsehome/perfTests/*
3) Copy class ﬁles of collections to benchmark
cp-r$fsehome/versions/collections/bin/* $fsehome/benchmark/
4)./Puﬀer.sh -R <packageName > <className >for our analysis
For this example, the command is :
./Puﬀer.sh -Rorg.apache.commons.collections4.bag CollectionBag.
This step takes around 10 minutes depending upon the ini-
tial number of tests generated and the performance tests are
stored in $fsehome/perfTests/ folder. It outputs 5 tests wi th
diﬀerent patterns for one redundant loop traversal. One of
which fails during compilation because of invalid type (Sec -
tion 4.1). This step may not generate any tests if the ran-
domly generated input tests do not execute ineﬃcient loop.
9.5.3 STEP3
Running all the benchmarks together to generate data for
Figure 9 and Figure 10 :
1) Change the working directory to $fsehome. cd$fsehome
2) Execute ./getData.sh.
This script performs the following operations.
i) Deletes pregenerated graphs for ﬁgure 9 and 10.
ii) Execute ./run.sh reuseSummary to analyse all the
benchmarks using pregerated summaries
iii) Execute ./generateFigure9.sh to generate data for
Figure9.
iv) Generate ﬁgure9 and ﬁgure10 in $fsehome/ﬁgures.
This script takes around 1.5 to 2 hours to ﬁnish execution.
During execution of the scripts, you may observe some test
failures. You can safely ignore them as those errors corre-
spond to inconsistency of java versions.
The generated tests are stored in $fsehome/perfTests
folder and methods with bugs are listed in the ﬁle Test-
sOutput.log. The generated ﬁgures (ﬁg9.eps and ﬁg10.eps)
are saved in $fsehome/ﬁgures folder. Please note that, the
results obtained will be roughly proportional to the perfor -
mance ratios mentioned in the Figure9 and Figure10 of the
paper. Figure 9 shows that the percentage performance im-
provement achieved using directed tests is more than ran-
dom tests, i.e., grey bars should rise above black bars. Fig-
ure 10 shows that percentage performance improvement in-
creases with size of the input collections.
90510. REFERENCES
[1] https://answers.acrobatusers.com/Performance-
Issues-Acrobat-Reader-11-0-0-2-secure-mode-enabled-
q91247.aspx.
[2] https://bugs.eclipse.org/bugs/show bug.cgi?id=394078.
[3] O. Agesen andA. Garthwaite. Eﬃcient object sampling
via weak references. In Proceedings of the 2Nd Inter-
national Symposium on Memory Management , ISMM
2000.
[4] T. Ball and J. R. Larus. Eﬃcient path proﬁling. In Pro-
ceedings of the 29th Annual ACM/IEEE International
Symposium on Microarchitecture , MICRO 29. IEEE
Computer Society, 1996.
[5] A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton,
S. Hallem, C. Henri-Gros, A. Kamsky, S. McPeak, and
D. Engler. A few billion lines of code later: Using static
analysis to ﬁnd bugs in the real world. Commun. ACM ,
53(2).
[6] S. Bhattacharya, M. G. Nanda, K. Gopinath, and
M. Gupta. Reuse, recycle to de-bloat software. In
Proceedings of the 25th European Conference on Object-
oriented Programming , ECOOP 2011.
[7] S. M. Blackburn, S. Singhai, M. Hertz, K. S. McKinely,
and J. E. B. Moss. Pretenuring for java. In Proceed-
ings of the 16th ACM SIGPLAN Conference on Object-
oriented Programming, Systems, Languages, and Appli-
cations, OOPSLA 2001.
[8] C. Cadar, D. Dunbar, and D. Engler. Klee: Unas-
sisted and automatic generation of high-coverage tests
for complex systems programs. In Proceedings of the
8th USENIX Conference on Operating Systems Design
and Implementation , OSDI 2008.
[9] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill, and
D. R. Engler. Exe: Automatically generating inputs of
death. In Proceedings of the 13th ACM Conference on
Computer and Communications Security , CCS 2006.
[10] D. J. Dean, H. Nguyen, X. Gu, H. Zhang, J. Rhee,
N. Arora, and G. Jiang. Perfscope: Practical online
server performance bug inference in production cloud
computing infrastructures. In Proceedings of the ACM
Symposium on Cloud Computing , SOCC 2014.
[11] D. C. D’Elia and C. Demetrescu. Ball-larus path pro-
ﬁling across multiple loop iterations. In Proceedings
of the 2013 ACM SIGPLAN International Conference
on Object Oriented Programming Systems Languages &
Applications , OOPSLA 2013.
[12] L. Della Toﬀola, M. Pradel, and T. R. Gross. Perfor-
mance problems you can ﬁx: A dynamic analysis of
memoization opportunities. In Proceedings of the 2015
ACM SIGPLAN International Conference on Object-
Oriented Programming, Systems, Languages, and Ap-
plications , OOPSLA 2015.
[13] D. D. Dunlop and V. R. Basili. A comparative analysis
of functional correctness. ACM Comput. Surv. , 14(2),
June 1982.[14] G. FraserandA.Arcuri. Evosuite: Automatictest suite
generation for object-oriented software. In Proceedings
of the 19th ACM SIGSOFT Symposium and the 13th
European Conference on Foundations of Software Engi-
neering, ESEC/FSE 2011.
[15] P. Godefroid. Compositional dynamic test generation.
InProceedings of the 34th Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming
Languages , POPL 2007.
[16] P. Godefroid, N. Klarlund, and K. Sen. Dart: Directed
automated random testing. In Proceedings of the 2005
ACM SIGPLAN Conference on Programming Language
Design and Implementation , PLDI 2005.
[17] S.Z.Guyer,K.S.McKinley, andD.Frampton. Free-me:
A static analysis for automatic individual object recla-
mation. In Proceedings of the 27th ACM SIGPLAN
Conference on Programming Language Design and Im-
plementation , PLDI 2006.
[18] W. huang, W. Srisa-an, and J. M. Chang. Dynamic pre-
tenuringschemes for generational garbage collection. In
Proceedings of the 2004 IEEE International Symposium
on Performance Analysis of Systems and Software , IS-
PASS 2004.
[19] G. Jin, L. Song, X. Shi, J. Scherpelz, and S. Lu. Un-
derstanding and detecting real-world performance bugs.
InProceedings of the 33rd ACM SIGPLAN Conference
on Programming Language Design and Implementation ,
PLDI 2012.
[20] M. Jump, S. M. Blackburn, and K. S. McKinley. Dy-
namic object sampling for pretenuring. In Proceedings
of the 4th International Symposium on Memory Man-
agement, ISMM 2004.
[21] C. Killian, K. Nagaraj, S. Pervez, R. Braud, J. W.
Anderson, and R. Jhala. Finding latent performance
bugs in systems implementations. In Proceedings of the
Eighteenth ACM SIGSOFT International Symposium
on Foundations of Software Engineering , FSE 2010.
[22] J. R. Larus. Whole program paths. In Proceedings of
the ACM SIGPLAN 1999 Conference on Programming
Language Design and Implementation , PLDI 1999.
[23] H. Lieberman and C. Hewitt. A real-time garbage col-
lector based on thelifetimes of objects. Commun. ACM ,
26(6), June 1983.
[24] S. McPeak, C.-H. Gros, and M. K. Ramanathan. Scal-
able and incremental software bug detection. In Pro-
ceedings of the 2013 9th Joint Meeting on Foundations
of Software Engineering , ESEC/FSE 2013.
[25] S. S. Muchnick. Advanced Compiler Design and Im-
plementation . Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 1997.
[26] R. Mudduluru and M. K. Ramanathan. Eﬃcient ﬂow
proﬁling for detecting performance bugs. In Proceed-
ings of the 2014 International Symposium on Software
Testing and Analysis , ISSTA 2016.
906[27] K. Nguyen and G. Xu. Cachetor: Detecting cacheable
data to remove bloat. In Proceedings of the 2013 9th
Joint Meeting on Foundations of Software Engineering ,
ESEC/FSE 2013.
[28] A. Nistor, P. C. Chang, C. Radoi, and S. Lu. Caramel:
Detecting and ﬁxing performance problems that have
non-intrusive ﬁxes. In Proceedings of the 37th Interna-
tional Conference on Software Engineering - Volume 1 ,
ICSE 2015.
[29] A. Nistor and L. Ravindranath. Suncat: Helping de-
velopers understand and predict performance problems
in smartphone applications. In Proceedings of the 2014
International Symposium on Software Testing and Anal-
ysis, ISSTA 2014.
[30] A. Nistor, L. Song, D. Marinov, and S. Lu. Toddler:
Detecting performance problems via similar memory-
access patterns. In Proceedings of the 2013 Interna-
tional Conference on Software Engineering , ICSE 2013.
[31] O. Olivo, I. Dillig, and C. Lin. Static detection of
asymptotic performance bugs in collection traversals.
InProceedings of the 36th ACM SIGPLAN Conference
on Programming Language Design and Implementation ,
PLDI 2015.
[32] C. Pacheco and M. D. Ernst. Randoop: Feedback-
directed random testing for java. In Companion to the
22Nd ACM SIGPLAN Conference on Object-oriented
Programming Systems and Applications Companion ,
OOPSLA 2007.
[33] M. Pradel, P. Schuh, G. Necula, and K. Sen. Event-
break: Analyzing the responsiveness of user interfaces
through performance-guided test generation. In Pro-
ceedings of the 2014 ACM International Conference
on Object Oriented Programming Systems Languages &
Applications , OOPSLA 2014.
[34] M. Samak and M. K. Ramanathan. Multithreaded
test synthesis for deadlock detection. In Proceedings
of the 2014 ACM International Conference on Object
Oriented Programming Systems Languages and Appli-
cations, OOPSLA 2014.
[35] M. Samak and M. K. Ramanathan. Synthesizing tests
for detecting atomicity violations. In Proceedings of the
2015 10th Joint Meeting on Foundations of Software
Engineering , ESEC/FSE 2015.
[36] M. Samak, M. K. Ramanathan, and S. Jagannathan.
Synthesizingracytests. In Proceedings of the 36th ACM
SIGPLAN Conference on Programming Language De-
sign and Implementation , PLDI 2015.[37] O. Shacham, M. Vechev, and E. Yahav. Chameleon:
Adaptive selection of collections. In Proceedings of the
30th ACM SIGPLAN Conference on Programming Lan-
guage Design and Implementation , PLDI 2009.
[38] A.Shankar,M.Arnold, andR.Bodik. Jolt: Lightweight
dynamic analysis and removal of object churn. In Pro-
ceedings of the 23rd ACM SIGPLAN Conference on
Object-oriented Programming Systems Languages and
Applications , OOPSLA 2008.
[39] L. Song and S. Lu. Statistical debugging for real-
world performance problems. In Proceedings of the 2014
ACM International Conference on Object Oriented Pro-
gramming Systems Languages & Applications , OOP-
SLA 2014.
[40] S. Thummalapenta, T. Xie, N. Tillmann, J. de Halleux,
and Z. Su. Synthesizing method sequences for high-
coverage testing. In Proceedings of the 2011 ACM Inter-
national Conference on Object Oriented Programming
Systems Languages and Applications , OOPSLA 2011.
[41] R. Vallee-Rai, E. Gagnon, L. Hendren, P. Lam, P. Pom-
inville, and V. Sundaresan. Optimizing java bytecode
using the soot framework: Is it feasible? In In Inter-
national Conference on Compiler Construction, LNCS
1781, 2000.
[42] K. Vaswani, A. V. Nori, and T. M. Chilimbi. Prefer-
ential path proﬁling: Compactly numbering interest-
ing paths. In Proceedings of the 34th Annual ACM
SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages , POPL 2007.
[43] G. Xu. Resurrector: A tunable object lifetime proﬁl-
ing technique for optimizing real-world programs. In
Proceedings of the 2013 ACM SIGPLAN International
Conference on Object Oriented Programming Systems
Languages & Applications , OOPSLA 2013.
[44] G. Xu, M. Arnold, N. Mitchell, A. Rountev, and G. Se-
vitsky. Go with the ﬂow: Proﬁling copies to ﬁnd run-
time bloat. In Proceedings of the 2009 ACM SIGPLAN
Conference on Programming Language Design and Im-
plementation , PLDI 2009.
[45] G. Xu, M. D. Bond, F. Qin, and A. Rountev.
Leakchaser: Helping programmers narrow down causes
of memory leaks. In Proceedings of the 32Nd ACM SIG-
PLAN Conference on Programming Language Design
and Implementation , PLDI 2011.
[46] D. Yan, G. Xu, and A. Rountev. Uncovering per-
formance problems in java applications with reference
propagation proﬁling. In Proceedings of the 34th In-
ternational Conference on Software Engineering , ICSE
2012.
907