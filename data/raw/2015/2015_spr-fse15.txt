Staged Program Repair with Condition Synthesis
Fan Long and Martin Rinard
MIT EECS & CSAIL, USA
{fanl, rinard}@csail.mit.edu
ABSTRACT
We present SPR, a new program repair system that com-
bines staged program repair and condition synthesis . These
techniques enable SPR to work productively with a set of
parameterized transformation schemas to generate and ef-
ciently search a rich space of program repairs. Together
these techniques enable SPR to generate correct repairs for
over ve times as many defects as previous systems evalu-
ated on the same benchmark set.
Categories and Subject Descriptors
D.2.5 [ SOFTWARE ENGINEERING ]: Testing and De-
bugging
Keywords
Program repair, Staged repair, Condition synthesis
1. INTRODUCTION
Despite decades of eort, defect triage and correction re-
mains a central concern in software engineering. Indeed,
modern software projects contain so many defects, and the
cost of correcting defects remains so large, that projects typ-
ically ship with a long list of known but uncorrected defects.
Consequences of this unfortunate situation include pervasive
security vulnerabilities and the diversion of resources that
would be better devoted to other, more productive, activi-
ties.
Automatic program repair holds out the promise of sig-
nicantly reducing the time and eort required to deal with
software defects. In the last decade researchers have devel-
oped automatic techniques that have been demonstrated to
successfully correct targeted but important classes of defects
such as out of bounds accesses [33, 30, 34], integer over-
ow errors [34], null pointer dereferences [11, 23], innite
loops [18], memory leaks [27], and data structure corruption
errors [9, 10, 8]. But impoverished search spaces and ine-
cient search algorithms have crippled the ability of previous
systems to generate correct patches for more general classes
of defects [20, 37, 32].1.1 Staged Program Repair (SPR)
We present SPR, a new program repair system that uses
a novel staged program repair strategy to eciently search a
rich search space of candidate repairs. Three key techniques
work synergistically together to enable SPR to generate suc-
cessful repairs for a range of software defects. Together,
these techniques enable SPR to generate correct repairs for
over ve times as many defects as previous systems evalu-
ated on the same benchmark set:
Parameterized Transformation Schemas: SPR
deploys a set of general transformation schemas, each
of which implements a strategy designed to generate
repairs that correct an identied class of defects. Be-
cause each schema is parameterized, it represents a
large class of program transformations. Together, these
schemas enable SPR to work with a rich search space
that contains many successful repairs for common de-
fects.
Target Value Search: Given a parameterized trans-
formation schema, SPR uses target value search to
quickly determine if there is anyparameter value that
will enable the schema to produce a successful repair.
If not, SPR rejects the schema and bypasses all of the
many repairs that the schema can generate.
Condition Synthesis: Many of the SPR transforma-
tion schemas take a logical condition as a parameter.
The SPR condition synthesis algorithm rst uses tar-
get value search to obtain constraints that any such
logical condition should satisfy. It then works with
these constraints to synthesize logical expressions that
enable the transformation schema to generate a suc-
cessful repair.
A key insight is that staging the repair process as transfor-
mation selection, target value search, and condition synthe-
sis enables SPR to immediately bypass the overwhelming
majority of candidate repairs and focus the search on the
most promising regions of the space. Our results highlight
the eectiveness of this strategy | staged repair can deliver
two orders of magnitude reduction in the size of the space
that SPR explores. Staged program repair is therefore crit-
ical in enabling SPR to work productively with large repair
search spaces that contain many useful repairs.
1.2 Experimental Results
We evaluate SPR on 69 real world defects and 36 func-
tionality changes from the repositories of eight real world
applications. This is the same benchmark set used to evalu-
ate several previous automatic patch generation systems [20,
This is the authorâ€™s version of the work. It is posted here for your personal use. Not for
redistribution. The deï¬nitive version was published in the following publication:
ESEC/FSEâ€™15 , August 30 â€“ September 4, 2015, Bergamo, Italy
c2015 ACM. 978-1-4503-3675-8/15/08...
http://dx.doi.org/10.1145/2786805.2786811
16637, 32].1The SPR search space contains transformations
that correctly repair 19 of the 69 defects (and 1 of the 36
functionality changes). For 11 of these 19 defects, SPR gen-
erates the correct repair as the rst repair to validate (i.e,
produce correct outputs for all test cases in the test suite).
These correct repairs semantically match corresponding re-
pairs provided by human developers. For comparison, pre-
vious systems generate correct patches for only one [20] or
two [37, 32] of the 69 defects in this benchmark set.
A repair is plausible if it produces correct outputs for all
of the test cases in the test suite (a plausible repair may be
incorrect | it may produce incorrect outputs for test cases
not in the test suite). SPR generates plausible repairs for 38
of the 69 defects (and 3 of the 36 functionality changes). For
comparison, previous systems generate plausible patches for
only 16 [20] or 25 [37, 32] of the 69 defects in this benchmark
set.2
These results highlight the success of SPR's staged ap-
proach and the synergistic relationship between its three
novel techniques. Parameterized transformation schemas
produce a rich repair space that contains many useful re-
pairs. Target value search enables the ecient search algo-
rithm required to make this rich search space viably search-
able in practice. And condition synthesis delivers the logical
expressions required to successfully instantiate the transfor-
mation schemas to obtain successful repairs.
1.3 Repair Prioritization and Prophet
SPR uses a set of heuristics to prioritize the order in which
it validates repairs (see Section 3.4). The goal is to obtain
a sequence of plausible repairs in which the correct repair
appears as early as possible (and ideally the rst repair in
the sequence to validate). The SPR heuristics are reasonably
eective at satisfying this goal | for 11 of the 19 relevant
defects the correct repair is the rst to validate. But there
is room for improvement.
Prophet [21] searches the same repair space as SPR, but
works with a large corpus of correct repairs from human de-
velopers. It processes this corpus to learn a probabilistic
model that assigns a probability to each candidate repair
in the search space. This probability indicates the likeli-
hood that the repair is correct. It then uses this model
to prioritize its search of the repair space. The results show
that Prophet's learned repair correctness model outperforms
SPR's heuristics | for 15 of the 19 defects, Prophet nds the
correct repair as the rst repair to validate. This result high-
lights how leveraging information available in existing large
software development projects can signicantly improve our
ability to automatically manipulate large software systems.
1Papers for these previous systems report that this bench-
mark set contains 105 defects [20, 37]. Our analysis of the
commit logs and applications indicates that 36 of these de-
fects correspond to deliberate functionality changes. That is,
for 36 of these defects, there is no actual defect to repair. We
evaluate SPR on all 105 defects/changes, but report results
separately for the actual defects and functionality changes.
2Because of errors in the patch evaluation scripts, previous
papers report incorrect results [20, 37, 32]. Specically, pre-
vious papers report patches for 55 [20] and 54 [37] of the 105
defects/changes. But for 37 of these 55 defects/changes [20,
32] and 27 of these 54 defects/changes [37, 32] none of the
reported patches produces correct outputs for the test cases
in the test suite used to validate the patches [20, 37, 32].1.4 Contributions
This paper makes the following contributions:
Staged Program Repair: It introduces staged pro-
gram repair a new technique for automatically gener-
ating and eciently searching rich repair spaces that
contain many useful repairs.
Search Space: It presents a set of transformation
schemas that 1) generate a search space with many
useful repairs and 2) synergistically enable the devel-
opment of a staged repair system that uses condition
synthesis to eciently search the generated space.
Condition Synthesis: It presents a novel condition
synthesis algorithm. This algorithm rst uses target
value search to obtain constraints that would enable
the repaired program to produce correct outputs for
all test cases in the test suite. It then generates logi-
cal conditions that successfully approximate the set of
branch directions. This condition synthesis algorithm
enables SPR to eciently search the space of condi-
tions in the SPR search space.
Experimental Results: It presents experimental re-
sults that characterize the eectiveness of SPR in auto-
matically nding correct repairs for 11 out of 69 bench-
mark defects and plausible repairs for 37 of these de-
fects. The results also show that the SPR search space
contains correct repairs for 19 of the 69 defects. We
discuss several extensions to the SPR search space and
identify the correct repairs that each extension would
bring into the search space.
2. EXAMPLE
We next present an example that illustrates how SPR re-
pairs a defect in the PHP interpreter. The PHP interpreter
before 5.3.7 (or svn version before 309580) contains a defect
(PHP bug #54283) in its implementation of the DatePeriod
object constructor [3]. If a PHP program calls the DatePe-
riod constructor with a single NULL value as the parameter
(e.g., DatePeriod(NULL) ), the PHP interpreter dereferences
an uninitialized pointer and crashes.
Figure 1 presents simplied source code (from the source
leext/date/php_date.c ) that contains this defect. The
code in Figure 1 presents the C function inside the PHP in-
terpreter that implements the DatePeriod constructor. The
PHP interpreter calls this function to handle DatePeriod
constructor calls in PHP programs.
A PHP program can invoke the DatePeriod constructor
with either three parameters or a single string parameter.
If the constructor is invoked with three parameters, one of
the two calls to zend_parse_parameter_ex() on lines 9-15
will succeed and set interval to point to a DateInterval
PHP object. These two calls leave isostr_len and isostr
unchanged. If the constructor is invoked with a single string
parameter, the third call to zend_parse_parameter_ex() on
lines 17-18 parses the parameters and sets isostr to point
to a PHP string object. isostr_len is the string length.
The then clause in lines 35-38 is designed to process calls
with one parameter. The defect is that the programmer
assumed (incorrectly) that all one parameter calls will set
isostr_len to a non-zero value. But if the constructor is
called with a null string, isostr_len will be zero. The if
condition at line 34 misclassies the call as a three param-
eter call and executes the else clause in lines 40-44. In this
1671 // Creates new DatePeriod object.
2 PHP_METHOD(DatePeriod, __construct) {
3 php_period_obj *dpobj;
4 char *isostr = NULL;
5 int isostr_len = 0;
6 zval *interval;
7 ...
8 // Parse (DateTime,DateInterval,int)
9 if (zend_parse_parameters_ex(..., &start, date_ce_date,
10 &interval, date_ce_interval, &recurrences,
11 &options)==FAILURE) {
12 // Parse (DateTime, DateInterval, DateTime)
13 if (zend_parse_parameters_ex(..., &start, date_ce_date,
14 &interval, date_ce_interval, &end, date_ce_date,
15 &options)==FAILURE) {
16 // Parse (string)
17 if (zend_parse_parameters_ex(...,&isostr,
18 &isostr_len, &options)==FAILURE) {
19 php_error_docref(...,"This constructor accepts"
20 " either (DateTime, DateInterval, int) OR"
21 " (DateTime, DateTimeInterval, DateTime)"
22 " OR (string) as arguments.");
23 ...
24 return;
25 } } }
26 dpobj = ...;
27 dpobj->current = NULL;
28 // repair transformation schema
29 /* if (isostr_len || abstract_cond() ) */
30 // instantiated repair. abstract_cond() -> (isostr != 0)
31 /* if (isostr_len || (isostr != 0)) */
32 // developer patch
33 /* if (isostr)*/
34 if (isostr_len) {
35 // Handle (string) case
36 date_period_initialize(&(dpobj->start), &(dpobj->end),
37 &(dpobj->interval), &recurrences, isostr, isostr_len);
38 ...
39 } else {
40 // Handle (DateTime,...) cases
41 /* pass uninitialized `interval' */
42 intobj = (php_interval_obj *)
43 zend_object_store_get_object(interval);
44 ...
45 }
46 ...
47 }
Figure 1: Simplied Code for PHP bug #54283
case interval is uninitialized and the program will crash
when the invoked zend_object_store_get_object() func-
tion dereferences interval .
We apply SPR to automatically generate a repair for this
defect. Specically, we give SPR:
Program to Repair: Version 309579 of the PHP
source code (this version contains the defect).
Negative Test Cases: Test cases that expose the
defect | i.e., test cases that PHP version 30979 does
not pass but the repaired version should pass. In this
example there is a single negative test case.
Positive Test Cases: Test cases that prevent regres-
sion | i.e, test cases that the version 30979 already
passes and that the patched code should still pass. In
this example there are 6974 positive test cases.
Defect Localization: SPR compiles the PHP interpreter
with additional proling instrumentation to produce execu-
tion traces. It then executes this proling version of PHP
on both the negative and positive test cases. SPR observes
that the negative test case always executes the statement at
lines 42-43 in Figure 1 while the positive test cases rarely ex-
ecute this statement. SPR therefore identies the enclosing
if statement (line 34) as a high priority repair target.First Stage: Select Transformation Schema: SPR se-
lects transformation schemas to apply to the repair target.
One of these schemas is a Condition Renement schema that
loosens the if condition by disjoining an abstract condition
abstract_cond() to the if condition.
Second Stage: Condition Synthesis: SPR uses con-
dition synthesis to instantiate the abstract condition ab-
stract_cond() in the selected transformation schema.
Target Condition Value Search: SPR replaces the
target if statement on line 34 with the transforma-
tion schema on line 29. The schema takes an abstrac-
tion condition abstract_cond() as a parameter. SPR
links PHP against a SPR library that implements ab-
stract_cond() . Note that if abstract_cond() always
returns 0, the semantics of PHP does not change.
SPR searches for a sequence of return values from ab-
stract_cond() that causes PHP to produce the cor-
rect result for the negative test case. SPR repeatedly
executes PHP on the test case, generating a dierent
sequence of 0/1 return values from abstract_cond()
on each execution. In the example, ipping the return
value of the last invocation of abstract_cond() from
0 to 1 produces the correct output.
Instrumented Reexecutions: SPR instruments
the code to record, for each invocation of ab-
stract_cond() , a mapping from the values of lo-
cal variables, accessed global variables, and values
accessed via pointers in the surrounding context to
theabstract_cond() return value. SPR reexecutes
the negative test case with the sequence of ab-
stract_cond() return values that produces the correct
output. It also reexecutes the positive test cases with
an all 0 sequence of abstract_cond() return values
(so that these reexecutions preserve the correct out-
puts for the positive test cases).
Condition Generation: SPR uses the recorded
mappings to generate a symbolic condition that ap-
proximates the mappings. In the example, isostr is
never 0 in the negative test case execution. In the pos-
itive test case executions, isostr is always zero when
abstract_cond() is invoked (note that jjis a short cir-
cuit operator). SPR therefore generates the symbolic
condition (isostr != 0) as the parameter.
Condition Validation: SPR reexecutes the PHP
interpreter with abstract_cond() replaced with
(isostr != 0) . PHP passes all test cases and SPR
has found a successful repair (lines 30-31 in Figure 1).
The ocial patch from the PHP developer in version 309580
replaces isostr_len with isostr (line 33 in Figure 1). At
this program point, isostr_len is zero whenever isostr is
zero. The SPR repair is therefore functionally equivalent to
the ocial patch from the PHP developer.
3. DESIGN
SPR starts with a program with a defect and a test suite.
The suite contains positive test cases , for which the program
already produces correct outputs, and negative test cases ,
which expose the defect that causes the program to pro-
duce incorrect outputs. The goal is to generate a repair
that enables the program to pass the supplied test suite (i.e,
produce correct outputs for all test cases in the test suite).
168c:= 1j0jc1&&c2jc1||c2j!c1j(c1)jv==const
prts := print vjprint const
simps :=v=v1opv2jv=const jv= read jprts
ifs := if ( c)`1`2
absts := if ( c&& !abstc) `1`2jif (c|| abstc) `1`2
jprint abstval
s := skip jstopjsimps jifsjabsts
v; v 1; v22Variable const 2Int `1; `22Label
c; c 1; c22CondExpr s2Stmt ifs2IfStmt
prts2PrintStmt simps 2SimpleStmt
absts 2AbstStmt
Figure 2: The language statement syntax
SPR rst uses fault localization to identify target state-
ments to transform [39, 15, 5]. It then stages the search for
a successful repair as follows. It rst selects a transforma-
tion schema to apply to a target statement. The result is a
candidate repair template, which may contain an abstract
expression as the template parameter. SPR then uses tar-
get value search to determine if there is any parameter value
that would instantiate the candidate repair template to de-
liver a successful repair. If so, SPR synthesizes candidate
parameter values and attempts to validate each resulting
repair in turn.
3.1 Core Language
Language Syntax: Figure 2 presents the syntax of the
core language that we use to present our algorithm. The cur-
rent implementation of SPR works with applications written
in the C programming language. See Section 3.4 for the ex-
tension of our algorithm to handle C.
A program is a pair hp;ni, wherep: Label!Statement
maps each label to the corresponding statement and n:
Label!Label maps each label to the label of the next
statement to execute. `0is the label of the rst statement
in the program.
The language in Figure 2 contains arithmetic statements
and if statements. An if statement of the form \ if (c)`1
`2" transfers the execution to `1ifcis 1 and`2ifcis 0.
The language uses if statements to encode loops. A state-
ment of the form \ v= read " reads an integer value from
the input and stores the value to the variable v. A state-
ment of the form \ printv" prints the value of the variable
vto the output. Statements that contain an abstract ex-
pression (i.e., AbstStmt ) are temporary statements that the
algorithm may introduce into a program during the repair
algorithm. Such statements do not appear in the original or
repaired programs.
Program State: A program state h`;;I;O;D;R;Sicon-
tains the current program point (a label `), the current
environment that maps each variable to its value ( :
Variable!Int), the remaining input ( I), and the gener-
ated output ( O).Iis a sequence of integer values (i.e.,
Sequence(Int)). Ois a sequence of integer and abstract val-
ues (i.e., Sequence(Int [fabstvalg)).
To support the extension to programs with abstract
expressions, the program state also contains a sequence
of future abstract condition values ( D), a sequence of
recorded abstract condition values ( R), and a sequence of
recorded environments for each abstract expression execu-
tion (S).DandRare sequences of zero or one values
(i.e., Sequence(0j1)).Sis a sequence of environments (i.e.,
Sequence(Variable !Int)).The rst three rules in Figure 3 present the operational
semantics of input read, if, and print statements. \ " in Fig-
ure 3 is the sequence concatenation operator. The notation
\`c)x" indicates that the condition cevaluates to xun-
der the environment . Note that for programs that do not
contain abstract expressions, D,R, andSare unchanged.
See our technical report [22] for the rules for other kinds of
statements.
3.2 Transformation Schemas
Figure 4 presents our program transformation function
M. It takes a program hp;niand produces a set of candi-
date modied programs after transformation schema appli-
cation. TL(hp;ni) is the set of target statement labels to
transform. Our error localizer (Section 3.4) identies this
set of statements. SimpleS (p) denotes all simple statements
(i.e. SimpleStmt ) inp.Vars (p) and Vars (s) denote all vari-
ables in the program pand in the statement s, respectively.
Consts (p) denotes all constants in p. RepS(p;s) is an utility
function that returns the set of statements generated by re-
placing a variable or a constant in swith other variables or
constants in p. Specically, SPR works with the following
transformation schemas:
Condition Renement: Given a target if statement,
SPR transforms the condition of the if statement by
conjoining or disjoining an abstract condition to the
original if condition (M Tighten and M Loosen ).
Condition Introduction: Given a target statement,
SPR transforms the program so that the statement
executes only if an abstract condition is true (M Guard ).
Conditional Control Flow Introduction: SPR in-
serts a new control ow statement (return, break, or
goto an existing label) that executes only if an abstract
condition is true (M Control ).
Insert Initialization: For each identied statement,
SPR generates repairs that insert a memory initializa-
tion statement before the identied statement (M Init).
Value Replacement: For each identied statement,
SPR generates repairs that replace either 1) one vari-
able with another, 2) an invoked function with another,
or 3) a constant with another constant (M Rep).
Copy and Replace: For each identied statement,
SPR generates repairs that copy an existing statement
to the program point before the identied statement
and then apply a Value Replacement transformation
(MCpRep ).
Note that the transformation schemas M Tighten , MLoosen ,
MGuard , and M Control introduce an abstract condition into
the generated candidate programs. The transformation
schemas M Repand M CpRep may introduce a print statement
with an abstract expression. These abstract conditions and
expressions will be handled by the repair algorithm later.
3.3 Staged Repair with Condition Synthesis
Figure 6 presents our main staged repair algorithm with
condition synthesis. Given a program hp;ni, a set of positive
test cases PosT , and a set of negative test cases NegT , the
algorithm produces a repaired program hp0;n0ithat passes
all test cases. Exec( hp;ni;I;D ) at lines 6, 13, 21, and 37
produces the results of running the program hp;nion the
inputIgiven the future abstract condition value sequence
D. Test(hp;ni;NegT;PosT ) at lines 31, 40, and 42 produces
169p(`) =v= readI=xI0
h`;;I;O;D;R;Si=Jhp;niK)hn(`);[v7!x];I0;O;D;R;Sip(`) =if(c)`1`2`c)1
h`;;I;O;D;R;Si=Jhp;niK)h`1;;I;O;D;R;Si
p(`) =printv O0=O(v)
h`;;I;O;D;R;Si=Jhp;niK)hn(`);;I;O0;D;R;Sip(`) =print abstval O0=Oabstval
h`;;I;O;D;R;Si=Jhp;niK)hn(`);;I;O0;D;R;Si
p(`) =if(c&& !abstc )`1`2`c)0
h`;;I;O;D;R;Si=Jhp;niK)h`2;;I;O;D;R;Sip(`) =if(c&& !abstc )`1`2`c)1
h`;;I;O;;R;Si=Jhp;niK)h`1;;I;O;;R0;Si
p(`) =if(c&& !abstc )`1`2`c)1D= 0D0
h`;;I;O;D;R;Si=Jhp;niK)h`1;;I;O;D0;R0;Sip(`) =if(c&& !abstc )`1`2`c)1D= 1D0
h`;;I;O;D;R;Si=Jhp;niK)h`2;;I;O;D0;R1;Si
Figure 3: Small step operational semantics
M(hp; ni) = M IfStmt (hp; ni)[MStmt(hp; ni)
MIfStmt (hp; ni) = [`2TL(hp;ni);p(`)2IfStmt (MTighten (hp; ni; `)[MLoosen (hp; ni; `))
MStmt(hp; ni) = [`2TL(hp;ni)(MControl (hp; ni; `)[MInit(hp; ni; `)[MGuard (hp; ni; `)[MRep(hp; ni; `)[MCpRep (hp; ni; `))
MTighten (hp; ni; `) = fhp[`7!if (c&& !abstc )`1`2]; nig, where p(`) = if ( c)`1`2
MLoosen (hp; ni; `) = fhp[`7!if (c|| abstc )`1`2]; nig, where p(`) = if ( c)`1`2
MControl (hp; ni; `) = fhp[`0 7!p(`)][`00 7! stop][`7!if (0 || abstc )`00n(`)]; n[`0 7!n(`)][`7!`0][`00 7!`0]ig
MGuard (hp; ni; `) = fhp[`0 7!p(`)][`7!if(1 && !abstc )`0n(`)]; n[`0 7!n(`)]ig
MInit(hp; ni; `) = fhp[`0 7!p(`)][`7!v=0]; n[`0 7!n(`)][`7!`0]i j 8v2Vars (p(`))g
MRep(hp; ni; `) = fhp[`7!s]; ni js2RepS( p; p(`))g
MCpRep (hp; ni; `) = fhp[`0 7!p(`)][`7!s]; n[`0 7!n(`)][`7!`0]i;
hp[`0 7!p(`)][`7!s0)]; n[`0 7!n(`)][`7!`0]i j 8s2SimpleS (hp; ni);8s0 2RepS(p ;s)g
RepS( p; v =v1opv2) = fv0=v1opv2; v=v0opv2; v=v1opv0 j 8v0 2Vars (p)g
RepS( p; v =const ) = fv0=const ; v=const0 j 8v0 2Vars (p);8const0 2Consts (p)g
RepS( p; v = read ) = fv0= read j 8v0 2Vars (p)g
RepS( p;prts) = fprint abstval g;where prts2PrintStmt
RepS( p; s) = ;;where s =2SimpleStmt
Figure 4: The program transformation function M. Note that `0and`00are fresh labels.
a boolean to indicate whether the program hp;nipasses all
test cases. See Figure 5 for the denitions of Exec and Test.
The algorithm enumerates all transformed candidate pro-
grams in the search space derived from our transformation
function M(hp;ni) (line 1). If the candidate program does
not contain an abstract expression, the algorithm simply
uses Test to validate the program (lines 42-43). If the candi-
date program contains an abstract condition, the algorithm
applies condition synthesis (lines 3-33) in two stages, target
condition value search and condition generation.
Target Condition Value Search: We augment the op-
erational semantics of the core language to handle state-
ments with an abstract condition. The last four rules in
Figure 3 present the semantics of \ if (c&& abstc) `1`2".
The rules for \ if (c|| abstc) `1`2" are similar [22].
The fth rule in Figure 3 species the case where the re-
sult of the condition does not depend on the abstract condi-
tion (the semantics implements short-circuit conditionals).
In this case the execution is transfered to the corresponding
labels with D,R, andSunchanged. The sixth rule speci-
es the case where there are no more future abstract condi-
tion values in Dfor the abstract condition abstc . This rule
uses the semantics-preserving value for the abstract condi-
tion abstc , withRandSappropriately updated. The last
two rules specify the case where Dis not empty. In this case
the execution continues as if the abstract condition returns
the next value in the sequence D, withRandSupdated
accordingly.
For each negative test case, the algorithm in Figure 6
searches a sequence of abstract condition values with the
goal of nding a sequence of values that generates the cor-
rect output for the test case (lines 5-19). Flip is an utility
function that ips the last non-zero value in a given valuesequence (see Figure 5 for the formal denition). The al-
gorithm (line 13) executes the program with dierent fu-
ture abstract condition value sequences Dto search for a
sequence that passes each negative test case. If the algo-
rithm cannot nd such a sequence, it moves on to the next
candidate program (line 16).
Note that the program may execute an abstract condition
multiple times. SPR tries a congurable number (in our
current implementation, 11) of dierent abstract condition
value sequences for each negative test case in the loop (lines
8-14). At each iteration (except the last) of the loop, the
algorithm ips the last non-zero value in the previous se-
quence. In the last iteration SPR ips all abstract condition
values to one (line 10 in Figure 6).
The rationale is that, in practice, if a negative test case
exposes an error at an if statement, either the last few exe-
cutions of the if statement or all of the executions take the
wrong branch direction. This empirical property holds for
all defects in our benchmark set.
If a future abstract condition value sequence can be found
for every negative test case, the algorithm concatenates the
found sequences R0and the corresponding recorded envi-
ronments to S0(lines 18-19). It then executes the candidate
program with the positive test cases and concatenates the
sequences and the recorded environments as well (lines 22-
23). Note that for positive cases the algorithm simply re-
turns zero for all abstract conditions, so that the candidate
program has the same execution as the original program.
Condition Generation: The algorithm enumerates all
conditions in the search space and evaluates each condition
against the recorded condition values (R 0) and environments
(S0). It counts the number of recorded condition values that
the condition matches. Our current condition space is the
170Exec(hp;ni;I;D ) =8
<
:hO;R;Si 9I0;O;R;S; such thath`0;0;I;;D;;i=Jhp;niK)
hnil;;I0;O;D0;R;Si
? otherwiseTest(hp;ni;NegT;PosT ) =8
<
:False9hI;Oi2(NegT[PosT );such that
Exec(hp;ni;I;) =hO0;R;Si; O6=O0
TrueotherwiseF(;;c ) = 0
`c)x
F(xR;S;c) =F(R;S;c ) + 1`c)(1 x)
F(xR;S;c) =F(R;S;c )Flip() =R=R00
Flip(R) =R01R=R01
Flip(R) = Flip(R0)
V(;;S;C ) =CO06=O O0=orO=
V(O0;O;S;C ) =;S=S0O0=xO00O=yO0x=y
V(O0;O;S;C ) =V(O00;O0;S0;C)
O0=xO00O=yO0x6=y x6=abstval
V(O0;O;S;C ) =;S=S0O0=abstvalO00O=yO0C0=V(O00;O0;S0;C)
V(O0;O;S;C ) =fvj(v) =y;v2C0g[fyjy2C0g
Figure 5: Denitions of Exec,Test,Flip,F, and V
Input : original program hp;ni
Input : positive and negative test cases NegT andPosT , each
is a set of pairshI;OiwhereIis the test input and O
is the expected output.
Output : the repaired program hp0;n0i, or;if failed
1forhp0;n0iinM(hp;ni)do
2 ifp0contains abstc then
3R0  
4S0  
5 forhI;OiinNegT do
6hO0;R;Si   Exec(hp0;n0i;I;)
7 cnt  0
8 whileO06=Oand cnt10do
9 ifcnt= 10 then
10 D  1111
11 else
12 D  Flip(R)
13 hO0;R;Si   Exec(hp0;n0i;I;D )
14 cnt  cnt+ 1
15 ifO6=O0then
16 skip to the next candidate hp0;n0i
17 else
18 R0  R0R
19 S0  S0S
20 forhI;OiinPosT do
21hO0;R;Si   Exec(hp0;n0i;I;)
22 R0  R0R
23 S0  S0S
24 C  fg
25 forinS0do
26 C  C[f(v==const );!(v==const )j
8v8const;such that(v) =constg
27 cnt  0
28 whileC6=;and cnt<20do
29 letc2CmaximizesF(R0;S0;c)
30 C  C=fcg
31 ifTest(hp0[c=abstc ];n0i;NegT;PosT )then
32 returnhp0[c=abstc ];n0i
33 cnt  cnt+ 1
34 else ifp0contains abstval then
35 C   Variable[Int
36 forhI;OiinNegT do
37hO0;;Si   Exec(hp0;n0i;I;)
38 C  V(O0;O;S;C )
39 forvalinCdo
40 ifTest(hp0[val=abstval ];n0i;NegT;PosT )then
41 returnhp0[val=abstval ];n0i
42 else if Test(hp0;n0i;NegT;PosT )then
43 returnhp0;n0i
44return;
Figure 6: Repair generation algorithm with con-
dition synthesisset of all conditions of the form (v==const )or!(v==
const )such that92S0:(v) = const . It is straightfor-
ward to extend this space to include comparison operators
(<;;;>) and a larger set of logical expressions. For our
benchmark set of defects, the current SPR condition space
contains a remarkable number of correct repairs, with exten-
sions to this space delivering relatively few additional correct
repairs (see Section 4.4).
F(R0;S0;c) in Figure 6 is an utility function that counts
the number of branch directions for the condition cthat
match the recorded abstract condition values R0given the
recorded environments S0. See Figure 5 for the formal de-
nition. The algorithm enumerates a congurable number (in
our current implementation, 20) of the top conditions that
maximizeF(R0;S0;c) (lines 27-33). The algorithm then val-
idates the transformed candidate program with the abstract
condition replaced by the generated condition c(lines 31-32).
p[c=abstc ] denotes the result of replacing every occurrence
ofabstc inpwith the condition c.
Enumerating all conditions in the space is feasible because
the overwhelming majority of the candidate transformed
programs do not pass the target condition value search stage.
SPR will therefore perform the condition generation stage
very infrequently and only when there is some evidence that
transforming the target condition may deliver a correct re-
pair. In our experiments, SPR performs the condition gen-
eration stage for less than 1% of the candidate programs
that contain an abstract condition (see Section 4.3).
Staged Repair for Print Statements: We augment
the semantics to support print statements with abstract ex-
pressions as shown as the fourth rule in Figure 3. When
such a print statement is executed, a special token abst-
valis appended to the output sequence Oto represent an
undetermined value.
At the rst stage, the algorithm executes the program
with such abstract print statements on each of the negative
test cases (lines 35-38). The algorithm compares the result
output sequence O0with the expected output sequence O.
This comparison uses the utility function V(O0;O;S;C ) to
compute a set of concrete values that, after replacing the
abstract expression, enable the repaired program to pass the
test case. See Figure 5 for the formal denition of V. If the
rst stage succeeds, the second stage replaces the abstract
expression with the computed concrete values and validates
the resulting repair (lines 39-41).
1713.4 Extensions for C
C Program Support: We have implemented SPR in
C++ using the clang compiler front-end [1]. SPR applies the
transformation function separately to each function in a C
program. When SPR performs variable replacement or con-
dition synthesis, it considers all variables (including local,
global, and heap variables) that appear in the transformed
function. During condition generation, SPR also searches
existing conditions cthat occur in the same enclosing com-
pound statement (in addition to conditions of the form (v
==const )or!(v==const )described in Section 3.3).
When SPR inserts control statements, SPR generates re-
pairs that include break ,return , and goto statements.
When inserting return statements, SPR generates a repair
to return each constant value in the returned type that ap-
peared in the enclosing function. When inserting goto state-
ments, SPR generates a repair to jump to each already de-
ned label in the enclosing function. When SPR inserts ini-
tialization statements, SPR generates repairs that call mem-
set() to initialize memory blocks. When SPR copies state-
ments, SPR generates repairs that copy compound state-
ments in addition to simple statements, as long as the copied
code can t into the new context. SPR also extends its
staged repair algorithm to constant string literals in C print
statements (e.g., printf() ). See our technical report [22]
for implementation details.
Error Localizer: The SPR error localizer recompiles the
given application with additional instrumentation. It inserts
a call back before each statement in the source code to record
a positive counter value as the timestamp of the statement
execution. SPR then invokes the recompiled application on
all test cases and produces a prioritized list that contains tar-
get statements to modify based on the recorded timestamp
values. SPR prioritizes statements that 1) are executed with
more negative test cases, 2) are executed with fewer positive
test cases, and 3) are executed later during executions with
negative test cases. Our technical report [22] presents the
error localization algorithm.
Repair Test Order: SPR validates the generated candi-
date repairs one by one (line 1 in Figure 6). SPR prioritizes
the generated patches as follows:
1. SPR rst tests repairs that change only a branch con-
dition (e.g., tighten and loosen a condition).
2. SPR tests repairs that insert an if-statement before a
statements, wheresis the rst statement of a com-
pound statement (i.e., C code block).
3. SPR tests repairs that insert an if-guard around a
statements.
4. SPR tests repairs with abstract print statements.
5. SPR tests repairs that insert a memory initialization.
6. SPR tests repairs that insert an if-statement before a
statements, wheresis not the rst statement of a
compound statement.
7. SPR tests repairs a) that replace a statement or b) that
insert a non-if statement (i.e., generated by M CpRep )
before a statement swheresis the rst statement of
a compound statement.
8. SPR nally tests the remaining repairs.
If two repairs have the same tier in the previous list, their
validation order is determined by the rank of the two corre-
sponding original statements (which the two repairs modify)
in the list returned by the error localizer.4. EXPERIMENTAL RESULTS
We evaluate SPR on a benchmark set containing 69 de-
fects and 36 functionality changes drawn from eight large
open source applications, libti, lighttpd, the PHP inter-
preter, gmp, gzip, python, wireshark, and fbc [2, 20]. We
address the following questions:
Repair Generation: How many correct/plausible re-
pairs can SPR generate for this benchmark set?
Design Decisions: How do the various SPR design
decisions aect the ability of SPR to generate repairs?
Previous Systems: How does SPR compare with
previous systems on this benchmark set?
4.1 Methodology
Reproduce the Defects/Changes: For each of the
eight applications, we collected the defects/changes, test
harnesses, test scripts, and test cases used in a previous
study [2]. We modied the test scripts and test harnesses
to eliminate various errors [32]. For libti we implemented
only partially automated patch validation, manually lter-
ing the nal generated repairs to report only plausible re-
pairs [32]. We then reproduced each defect/change (except
the fbc defects/changes) in our experimental environment,
Amazon EC2 Intel Xeon 2.6GHz Machines running Ubuntu-
64bit server 14.04. fbc runs only in 32-bit environments, so
we use a virtual machine with Intel Core 2.7Ghz running
Ubuntu-32bit 14.04 for the fbc experiments.
Apply SPR: For each defect/change, we ran SPR with
a time limit of 12 hours. We terminate SPR when either
1) SPR successfully nds a repair that passes all of the test
cases or 2) the time limit of 12 hours expires. To facilitate
the comparison of SPR with previous systems, we run SPR
twice for each defect: once without specifying a source code
le to repair, then again specifying the same source code le
to repair as previous systems [2, 20, 37].3
Inspect Repair Correctness: For each defect/change,
we manually inspect all of the repairs that SPR generates.
We consider a generated repair correct if 1) the repair com-
pletely eliminates the defect exposed by the negative test
cases so that no test case will be able to trigger the defect,
and 2) the repair does not introduce any new defects.
We also analyze the developer patch (when available) for
each of the defects/changes for which SPR generated plau-
sible repairs. Our analysis indicates that the developer
patches are consistent with our correctness analysis: 1) if
our analysis indicates that the SPR repair is correct, then
the repair has the same semantics as the developer patch and
2) if our analysis indicates that the SPR repair is not correct,
then the repair has dierent semantics from the patch.
We acknowledge that, in general, determining whether a
specic repair corrects a specic defect can be dicult (or in
some cases not even well dened). We emphasize that this
is not the case for the repairs and defects that we consider
in this paper. The correct behavior for all of the defects is
clear, as is repair correctness and incorrectness.
3Previous systems require the user of the system to identify
a source code le to patch [2, 20, 37]. This requirement re-
duces the size of the search space but eliminates the ability
of these systems to operate automatically without user in-
put. SPR imposes no such restriction | it can operate fully
automatically across the entire source code base. If desired,
it can also work with a specied source code le to repair.
172Table 1: Overview of SPR Repair Generation Results
App LoC TestsDefects/
ChangesPlausible CorrectInit
TimeSPR
TimeSPR
WSF
TimeSPRSPR GenAE SPRSPR GenAEWSF Prog WSF Prog
libti 77k 78 8/16 5/0 5/0 3/0 5/0 1/0 1/0 0/0 0/0 2.4m 10.8m 18.0m
lighttpd 62k 295 7/2 3/1 4/2 4/1 3/1 0/0 0/0 0/0 0/0 7.2m 111.0m 175.0m
php 1046k 8471 31/13 16/1 19/1 5/0 7/0 9/0 9/0 1/0 2/0 13.7m 119.5m 141.3m
gmp 145k 146 2/0 2/0 2/0 1/0 1/0 1/0 1/0 0/0 0/0 7.5m 94.0m 70.5m
gzip 491k 12 4/1 2/0 2/0 1/0 2/0 0/0 1/0 0/0 0/0 4.2m 10.5m 17.5m
python 407k 35 9/2 5/1 3/1 0/1 2/1 0/0 0/1 0/1 0/1 31.1m 137.0m 284.8m
wireshark 2814k 63 6/1 4/0 4/0 1/0 4/0 0/0 0/0 0/0 0/0 58.8m 23.5m 24.3m
fbc 97k 773 2/1 1/0 1/0 1/0 1/0 0/0 0/0 0/0 0/0 8.0m 49m 32m
Total 69/36 38/3 40/4 16/2 25/2 11/0 12/1 1/1 2/1
4.2 Summary of Experimental Results
Table 1 summarizes our benchmark set and our experi-
mental results. Column 1 (App) presents the name of the
benchmark application. Column 2 (LoC) presents the size
of the benchmark application measured in the number of
source code lines. Column 3 (Tests) presents the number of
test cases. Column 4 (Defects/Changes) presents the num-
ber of defects/changes we considered in our experiments.
Each entry is of the form X/Y, where X is the number of
defects and Y is the number of changes.
Each entry in Column 5 (Plausible SPR) is of the form
X/Y, where X is the number of defects and Y is the num-
ber of changes for which SPR generates a plausible repair.
Column 6 (Plausible SPR WSF) presents the corresponding
numbers for SPR running with a specied source code le
to repair. For comparison, Columns 7-8 present the corre-
sponding results for GenProg [20] and AE [37].4Columns
9-12 present the corresponding results for correct repairs.
Even with no specied source code le, SPR generates
plausible repairs for at least 13 more defects than GenProg
and AE (38 for SPR vs. 16 for GenProg and 25 for AE; Gen-
Prog and AE require the user to provide this information).
The GenProg result tar le [2] reports results from 10 dif-
ferent GenProg executions with dierent random seeds. We
count the defect as patched correctly by GenProg if any of
the patches for that defect is correct. Our results show that
SPR generates correct repairs for at least nine more defects
than GenProg and AE (11 for SPR vs. one for GenProg and
two for AE), even when the target source le is not specied.
Column 13 (Init Time) in Table 1 presents the average
time SPR spent to initialize the repair process, which in-
cludes compiling the application and running the error local-
izer. Column 14 (SPR Time) presents the average execution
time of SPR on all defects/changes for which SPR generates
repairs. Column 15 (SPR WSF Time) presents the average
execution time for the runs where we specify a source code
le to repair. When SPR generates a repair, it does so in
less than two hours on average.
4.3 Correct Repair Analysis
When the target source le is not specied, the SPR repair
search space contains correct repairs for 20 defects/changes.
Table 2 classies these 20 correct repairs. The rst 11 of
these 20 are the rst plausible repair that SPR encounters
4Due to errors in the repair evaluation scripts, at least half
of the originally reported patches from the GenProg and AE
papers do not produce correct results for the test cases in
the test suite used to validate the patches [32]. See previ-
ous work on the analysis of GenProg and AE patches for
details [32].Table 2: SPR Repair Type and Condition Synthesis
Results
Defect/
ChangeRepair TypeCondition
Value Search
On O
php-307562-307561 Replacey 0/139 6.3X
php-307846-307853 Add Inity 0/126 3.2X
php-307914-307915 Replace Printyz 0/188 67.5X
php-308734-308761 Guarded Controlyz 6/257 4.5X
php-309516-309535 Add Inity 0/133 5.8X
php-309579-309580 Change Condition yz 1/64 34.0X
php-309892-309910 Delete 1/144 25.5X
php-310991-310999 Change Condition y4/101 68.9X
php-311346-311348 Redirect Branchy 1/89 42.4X
libti-ee2ce5-b5691a Add Controlyz 3/294 94.1X
gmp-13420-13421 Replaceyz 0/515 3.7X
php-308262-308315 Add Guardyz N/A 6.9X
php-309111-309159 Copyz N/A 2.9X
php-309688-309716 Change Condition yz N/A 4.0X
php-310011-310050 Copy and Replace yz N/A 4.5X
libti-d13be-ccadf Change Condition y N/A 121.4X
libti-5b021-3dfb3 Replacey N/A 5.5X
gzip-a1d3d4-f17cbd Copy and Replace yz N/A 5.0X
python-69783-69784 Delete N/A 40.6X
fbc-5458-5459 Change Condition yz N/A 18.8X
during the search. The classication highlights the chal-
lenges that SPR must overcome to generate these correct
repairs. Column 1 (Defect/Change) contains entries of the
form X-Y-Z, where X is the name of the application that
contains the defect/change, Y is the defective version, and
Z is the reference repaired version.
Modication Operators: Column 2 (Repair Type)
presents the repair type of the correct repair for each de-
fect. \Add Control" indicates that the repair inserts a con-
trol statement with no condition. \Guarded Control" indi-
cates that the repair inserts a guarded control statement
with a meaningful condition. \Replace" indicates that the
repair modies an existing non-print statement using value
replacement to replace an atom inside it. \Replace Print"
indicates that the repair replaces an existing print state-
ment via staged repair. \Copy and Replace" indicates that
the repair copies a statement from somewhere else in the
application using value replacement to replace an atom in
the statement. \Add Init" indicates that the repair inserts
an initialization statement. \Delete" indicates that the re-
pair removes statements (this is a special case of the Con-
dition Introduction in which the guard condition is set to
false). \Redirect Branch" indicates that the repair removes
one branch of an if statement and redirects all executions to
the other branch (by setting the condition of the if statement
to true or false). \Change Condition" indicates that the re-
pair changes a branch condition in a non-trivial way (unlike
173\Delete" and \Redirect Branch"). \Add Guard" indicates
that the repair conditionally executes an existing statement
by adding an if statement to enclose the statement.
A \y" in Column 2 indicates that the SPR repair for this
defect is outside the search space of GenProg and AE (17
out of the 20 defects/changes). A\ z"in the column indicates
that the SPR repair for this defect is outside the search space
of PAR with the eight templates from the PAR paper [17] (11
out of the 20 defects/changes). See our technical report [22]
for the analysis details.
Condition Synthesis: Each entry in Column 3 (Condi-
tion Value Search On) is of the form X/Y. Here Y is the
total number of repair schema applications that contain an
abstract target condition. X is the number of these schema
applications for which SPR discovers a sequence of abstract
condition values that generate correct outputs for all test
cases. SPR performs the condition generation search for
only these X schema applications. These results highlight
the eectiveness of SPR's staged condition synthesis algo-
rithm | over 99.2% of the schema applications are discarded
before SPR even attempts to nd a condition that will re-
pair the program. For all defects except php-310991-310999,
SPR's condition generation algorithm is able to nd an exact
match for the recorded abstract condition values. For php-
310991-310999, the correct generated condition matches all
except one of the recorded values. We attribute the dis-
crepancy to the ability of the program to generate a correct
result for both branch directions [35].
Column 4 (Condition Value Search O) presents how
many times more candidate repairs SPR would need to con-
sider if SPR turned o condition value search and performed
condition synthesis by simply enumerating and testing all
conditions in the search space. These results show that
SPR's staged condition synthesis algorithm signicantly re-
duces the number of candidate repairs that SPR needs to
validate, in some cases by a factor of over two orders of
magnitude.
4.4 Search Space Extensions
The current SPR repair space contains repairs for 19 of
the 69 defects. Increasing the threshold of the error local-
ization ranked list from 200 to 2000 would bring a repair
for an additional defect into the search space. Extending
the SPR condition space to include comparison operations
(<;;;>) would bring repairs for an additional two de-
fects into the repair space. Extending the repair space to
include repairs that apply two transformation schemas (in-
stead of only one as in the current SPR implementation)
would bring repairs for another three defects into the space.
Extending the Copy and Replace schema instantiation space
to include more sophisticated replacement expressions would
bring repairs for four more defects into the search space.
Combining all three of these extensions would bring an ad-
ditional six more defects into the search space. Repairs for
the remaining 34 defects require changes to or insertions of
at least three statements.
All of these extensions come with potential costs. The
most obvious cost is the diculty of searching a larger space.
A more subtle cost is that increasing the search space may
increase the number of plausible but incorrect repairs and
make it harder to nd the correct repair. It is straightfor-
ward to extend SPR to include comparison operators. The
feasibility of supporting the other extensions is less clear.5. LIMITATIONS
The data set considered in this paper was selected not by
us, but by the GenProg developers in an attempt to obtain
a large, unbiased, and realistic benchmark set [20]. Never-
theless, one potential threat to validity is that our results
may not generalize to other defects and test suites.
SPR applies one transformation at each time it gener-
ates a candidate repair. It is unclear how to combine multi-
ple transformations and still eciently explore the enlarged
space. Note that previous tools [20, 31] that apply multiple
transformations produce only semantically simple patches.
The overwhelming majority of the patches are incorrect and
equivalent to simply deleting functionality [32].
6. RELATED WORK
CodePhage: Horizontal code transfer automatically lo-
cates correct code in one application, then transfers that
code into another application [34]. This technique has been
applied to eliminate otherwise fatal integer overow, buer
overow, and divide by zero errors and shows enormous po-
tential for leveraging the combined talents and labor of soft-
ware development eorts worldwide.
ClearView: ClearView is a generate-and-validate system
that observes normal executions to learn invariants that
characterize safe behavior [30]. It deploys monitors that
detect crashes, illegal control transfers and out of bounds
write defects. In response, it selects a nearby invariant that
the input that triggered the defect violates, and generates
patches that take a repair action to enforce the invariant.
SPR diers from ClearView in both its goal and its tech-
nique. SPR targets software defects that can be exposed
by supplied negative test cases, which are not limited to
just vulnerabilities. SPR operates on a search space derived
from its transformation schemas to generate repairs, while
ClearView generates patches to enforce violated invariants.
GenProg, RSRepair, AE, and Kali: GenProg [38, 20]
uses a genetic programming algorithm to search a space of
patches, with the goal of enabling the application to pass all
considered test cases. RSRepair [31] changes the GenProg
algorithm to use random search instead. AE [37] uses a
deterministic search algorithm and uses program equivalence
relations to prune equivalent patches during testing.
Previous work shows that, contrary to the design principle
of GenProg, RSRepair, and AE, the majority of the reported
patches of these three systems are implausible due to errors
in the patch validation [32]. Further semantic analysis on the
remaining plausible patches reveals that despite the surface
complexity of these patches, an overwhelming majority of
these patches are equivalent to functionality elimination [32].
The Kali patch generation system, which only eliminates
functionality, can do as well [32].
Unlike GenProg [20], RSRepair [31], and AE [37], which
only copy statements from elsewhere in the program, SPR
denes a set of novel modication operators that enables
SPR to operate on a search space which contains meaningful
and useful repairs. SPR then uses its condition synthesis
technique to eciently explore the search space. Our results
show that SPR signicantly outperforms GenProg and AE
in the same benchmark set. The majority of the correct
repairs SPR generates in our experiments are outside the
search space of GenProg, RSRepair, and AE.
174PAR: PAR [17] is another prominent automatic patch gen-
eration system. PAR is based on a set of predened human-
provided patch templates. We are unable to directly com-
pare PAR with SPR because, despite repeated requests to
the authors of the PAR paper over the course of 11 months,
the authors never provided us with the patches that PAR
was reported to have generated [17]. Monperrus found that
PAR xes the majority of its benchmark defects with only
two templates (\Null Pointer Checker" and \Condition Ex-
pression Adder/Remover/Replacer") [25].
In general, PAR avoids the search space explosion prob-
lem because its human supplied templates restrict its search
space. However, the PAR search space (with the eight tem-
plates in the PAR paper [17]) is in fact a subset of the SPR
search space. Moreover, the dierence is meaningful | the
SPR correct repairs for at least 11 of our benchmark defects
are outside the PAR search space (see Section 4.3). This
result illustrates the fragility and unpredictability of using
xed patch templates.
SemFix and MintHint: SemFix [26] and MintHint [16]
replace the faulty expression with a symbolic value and use
symbolic execution techniques [4] to nd a replacement ex-
pression that enables the program to pass all test cases.
SemFix and MintHint are evaluated only on applications
with less than 10000 lines of code. In addition, these tech-
niques cannot generate xes for statements with side eects.
Repair Model and Repair Shape: Martinez and Mon-
perrus [24] propose to stage the program repair in three
steps, error localization, selecting a repair shape, and repair
synthesis. They also mine the past human repairs to obtain
a probabilistic distribution of dierent repair shapes. The
SPR transformation schemas, in contrast, enable SPR to
preliminarily validate the result candidate patch templates
and signicantly reduce the number of candidate patches
SPR needs to consider.
Debroy and Wong: Debroy and Wong [6] present a
transformation-based patch generation technique. This
technique either replaces an arithmetic operator with an-
other operator or negates a condition. In contrast, SPR
uses more sophisticated and eective transformations and
search algorithms. None of the correct repairs in SPR's
search space for the 19 defects are within the Debroy and
Wong search space.
NOPOL: NOPOL [7, 12] is an automatic repair tool fo-
cusing on branch conditions. It identies branch statement
directions that can pass negative test cases and then uses
SMT solvers to generate repairs for the branch condition. A
key dierence between SPR and NOPOL is that SPR intro-
duces abstract condition semantics and uses target condition
value search to determine the value sequence of an abstract
condition, while NOPOL simply assumes that the modied
branch statement will always take the same direction dur-
ing an execution. In fact, this assumption is often not true
when the branch condition is executed multiple times for a
test case (e.g., php-308734-308761 and php-310991-310999).
In this case NOPOL will fail to generate a correct patch.
AutoFixE: AutoFix-E [36, 29] operates with a set of x
schemas to repair Eiel programs with human-supplied spec-
ications called contracts. SPR diers from AutoFixE in
that it requires no specication and uses the staged program
repair strategy to eciently explore the search space.
Deductive Program Repair: Deductive Program Re-
pair formalizes the program repair problem as a programsynthesis problem, using the original defective program as a
hint [19]. It replaces the expression to repair with a synthesis
hole and uses a counterexample-driven synthesis algorithm
to nd a patch that satises a formal specication. SPR,
in contrast, works with large real world applications, where
formal specications are typically not available.
Domain Specic Repair Generation: Other pro-
gram repair systems include VEJOVIS [28] and Gopinath
et al. [14], which applies domain specic techniques to re-
pair DOM-related faults in JavaScript and selection state-
ments in database programs respectively. SPR diers from
all of this previous research in that it focuses on generat-
ing xes for general purpose applications without human-
supplied specications.
Failure-Oblivious Computing: Failure-oblivious com-
puting [33] checks for out of bounds reads and writes. It dis-
cards out of bounds writes and manufactures values for out
of bounds reads. This eliminates data corruption from out
of bounds writes, eliminates crashes from out of bounds ac-
cesses, and enables the program to continue execution along
its normal execution path.
RCV: RCV [23] enables applications to survive null deref-
erence and divide by zero errors. It discards writes via null
references, returns zero for reads via null references, and
returns zero as the result of divides by zero. Execution con-
tinues along the normal execution path.
Bolt: Bolt [18] attaches to a running application, deter-
mines if the application is in an innite loop, and, if so, ex-
its the loop. A user can also use Bolt to exit a long-running
loop. In both cases the goal is to enable the application to
continue useful execution.
Cyclic Memory Allocation: Cyclic memory allocation
eliminates memory leaks by cyclically allocating objects out
of a xed-size buer [27].
Data Structure Repair: Data structure repair enables
applications to recover from data structure corruption er-
rors [9, 10]. It enforces a data structure consistency speci-
cation. This specication can be provided by a developer or
automatically inferred from correct program executions [8].
Self-Stabilizing Java: Self-Stabilizing Java uses a type
system to ensure that the impact of any errors are eventually
ushed from the system, returning the system back to a
consistent state [13].
7. CONCLUSION
The diculty of generating a search space rich enough to
correct defects while still supporting an acceptably ecient
search algorithm has signicantly limited the ability of previ-
ous automatic patch generation systems to generate success-
ful patches [20, 37, 32]. SPR's novel combination of staged
program repair, parameterized transformation schemas, tar-
get value search, and condition synthesis highlight how a rich
program repair search space coupled with an ecient search
algorithm can enable successful automatic program repair.
8. ACKNOWLEDGEMENTS
We would like to thank Zichao Qi and Sara Anchor for
their valuable help on the experiments. We also thank the
anonymous reviewers for their insightful comments. This
research was supported by DARPA (Grant FA8650-11-C-
7192).
175Table 3: SPR Results for Each Generated Plausible Repair Obtained via the Replication Package
Defect/
ChangeSPR SPR(With Specied File Name)Gen
ProgAESPR
TimeSPR
Search Gen CorrectResultSearch Gen CorrectResult(WSF)
Space At At Space At At Time
libti-ee2ce-b5691 67202 283 283 Correct 228157 1012 1012 Correct No Gen 9m 26m
libti-d13be-ccadf 71632 7 7 Gen 207237 116 116 Gen Gen Gen 13m 15m
libti-90d13-4c666 69955 296 - Gen 232963 1029 - Gen No Gen 9m 27m
libti-5b021-3dfb3 156766 40 17875 Gen 190979 24 19092 Gen Gen Gen 4m 4m
libti-08603-1ba75 74108 54 - Gen 209076 70 - Gen Gen Gen 19m 18m
lighttpd-1806-1807 19895 - - No 9662 30 - Gen Gen Gen - 676m
lighttpd-1913-1914 31120 8 - Gen 58347 33 - Gen Gen No 175m 174m
lighttpd-1948-1949 25037 152 - Gen 87815 2 - Gen No No 73m 51m
lighttpd-2661-2662 22608 951 - Gen 26544 5 81 Gen Gen Gen 150m 36m
php-307562-307561 22851 1672 1672 Correct 10825 959 959 Correct No No 192m 28m
php-307846-307853 17585 1852 1852 Correct 49643 4470 4470 Correct No No 51m 108m
php-307914-307915 230811 198 198 Correct 3300 140 140 Correct No No 34m 30m
php-307931-307934 57714 - - No 30234 8 - Gen Gen Gen - 174m
php-308262-308315 70577 - - No 14725 39 2071 Gen No No - 186m
php-308323-308327 33295 - - No 3965 3 - Gen No No - 34m
php-308525-308529 33933 121 - Gen 9798 73 - Gen No Gen 236m 362m
php-308734-308761 10390 2179 2179 Correct 5425 464 464 Correct No No 237m 174m
php-309111-309159 40641 74 12312 Gen 25794 52 9235 Gen No Correct 70m 61m
php-309516-309535 21016 1189 1189 Correct 51074 3683 3683 Correct No No 39m 100m
php-309579-309580 40106 4 4 Correct 8736 18 18 Correct No No 45m 37m
php-309688-309716 45498 361 4206 Gen 5779 253 501 Gen No No 29m 15m
php-309892-309910 27030 15 15 Correct 13925 6 6 Correct Correct Correct 62m 50m
php-309986-310009 45339 27 - Gen 9934 5 - Gen Gen Gen 409m 82m
php-310011-310050 52498 12 14611 Gen 2242 210 1382 Gen Gen Gen 301m 232m
php-310370-310389 44436 50 - Gen 3640 2 - Gen No No 103m 89m
php-310673-310681 21746 483 - Gen 25704 1685 - Gen Gen Gen 26m 157m
php-310991-310999 69387 12 12 Correct 454797 351 351 Correct No No 101m 219m
php-311346-311348 5099 21 21 Correct 7847 29 29 Correct No No 36m 68m
gmp-13420-13421 41681 6278 6278 Correct 26995 4024 4024 Correct No No 173m 129m
gmp-14166-14167 25539 4 - Gen 6336 2 - Gen Gen Gen 15m 12m
gzip-a1d3d4-f17cbd 33426 573 7898 Gen 75715 1089 1089 Correct No Gen 5m 19m
gzip-3fe0ca-39a362 70059 60 - Gen 79556 100 - Gen Gen Gen 16m 16m
python-69223-69224 44745 38 - Gen 32949 1640 - Gen No Gen 218m 526m
python-69368-69372 57051 69 - Gen 20401 - - No No No 30m -
python-69709-69710 47738 88 - Gen 395 - - No No No 37m -
python-70019-70023 17209 2646 - Gen 86032 4194 - Gen No No 420m 476m
python-70098-70101 17809 1485 - Gen 42666 30 - Gen No Gen 51m 83m
wshark-37112-37111 23874 2 - Gen 22794 12 - Gen Gen Gen 22m 32m
wshark-37172-37171 53801 307 - Gen 112514 506 - Gen No Gen 26m 22m
wshark-37172-37173 53801 307 - Gen 112514 506 - Gen No Gen 22m 21m
wshark-37284-37285 57946 315 - Gen 124408 537 - Gen No Gen 24m 22m
fbc-5458-5459 5847 13 54 Gen 716 3 9 Gen Gen Gen 49m 35m
lighttpd-1794-1795 21068 - - No 114801 74 - Gen Gen Gen - 70m
lighttpd-2330-2331 39200 29 - Gen 46194 67 - Gen Gen Gen 46m 43m
php-311323-311300 915018 16 - Gen 75550 4 - Gen No No 61m 620m
python-69783-69784 38144 48 52 Gen 21209 50 50 Correct Correct Correct 66m 54m
9. REPLICATION PACKAGE
We provide a full replication package for SPR at http:
//groups.csail.mit.edu/pac/spr/ . The package contains
a public AMI image for reproducing all of the experiments
except fbc and a VMWare image for reproducing the fbc
experiments. To reproduce the SPR experiment on a defect,
start an EC2 instance or a VM and follow the step-by-step
instructions inside the package.
Results: Table 3 summarizes the results available via
the replication package for the 46 defects/changes for which
SPR generates a plausible repair.5The rst 42 rows present
results for defects, the last 4 rows present results for func-
tionality changes. Column 1 (Defect/Change) presents the
defect/change. Columns 2-5 present results from SPR run-
ning without a specied source le to repair. Columns 6-9
present results from SPR running with a specied source le
to repair.
5This set is a superset of the set of defects/changes for which
GenProg/AE generate plausible patches.Columns 2 and 6 (Search Space) present the total number
of candidate repairs in the SPR search space. A number X in
Columns 3 and 7 (Gen At) indicates that the rst generated
plausible patch is the Xth candidate patch in SPR's search
space. Columns 4 and 8 (Correct At) present the rank of the
rst correct repair in the SPR search space (if any). Note
that even if the correct repair is within the SPR search space,
SPR may not generate this correct repair | the SPR search
may time out before SPR encounters the correct repair, or
SPR may encounter a plausible but incorrect repair before
it encounters the correct repair.
Comparison With GenProg and AE: Columns 5 and
9 (Result) present for each defect whether the SPR repair
is correct or not. Columns 10 and 11 present the status
of the GenProg and AE patches for each defect. \Correct"
in the columns indicates that the tool generated a correct
patch. \Gen" indicates that the tool generated a plausible
but incorrect patch. \No" indicates that the tool does not
generate a plausible patch for the corresponding defect.
17610. REFERENCES
[1] clang: a C language family frontend for LLVM.
http://clang.llvm.org/ .
[2] GenProgjEvolutionary Program Repair - Univeristy
of Virginia.
http://dijkstra.cs.virginia.edu/genprog/ .
[3] PHP: DatePeriod:: construct - Manual. http:
//php.net/manual/en/dateperiod.construct.php .
[4] C. Cadar, D. Dunbar, and D. Engler. Klee: Unassisted
and automatic generation of high-coverage tests for
complex systems programs. In Proceedings of the 8th
USENIX Conference on Operating Systems Design
and Implementation , OSDI'08, pages 209{224,
Berkeley, CA, USA, 2008. USENIX Association.
[5] S. Chandra, E. Torlak, S. Barman, and R. Bodik.
Angelic debugging. In Proceedings of the 33rd
International Conference on Software Engineering ,
ICSE '11', pages 121{130, New York, NY, USA, 2011.
ACM.
[6] V. Debroy and W. E. Wong. Using mutation to
automatically suggest xes for faulty programs. In
Software Testing, Verication and Validation (ICST),
2010 Third International Conference on , pages 65{74.
IEEE, 2010.
[7] F. DeMarco, J. Xuan, D. Le Berre, and M. Monperrus.
Automatic repair of buggy if conditions and missing
preconditions with smt. In Proceedings of the 6th
International Workshop on Constraints in Software
Testing, Verication, and Analysis , CSTVA 2014,
pages 30{39, New York, NY, USA, 2014. ACM.
[8] B. Demsky, M. D. Ernst, P. J. Guo, S. McCamant,
J. H. Perkins, and M. C. Rinard. Inference and
enforcement of data structure consistency
specications. In Proceedings of the ACM/SIGSOFT
International Symposium on Software Testing and
Analysis, ISSTA 2006, Portland, Maine, USA, July
17-20, 2006 , pages 233{244, 2006.
[9] B. Demsky and M. C. Rinard. Automatic detection
and repair of errors in data structures. In OOPSLA ,
pages 78{95, 2003.
[10] B. Demsky and M. C. Rinard. Goal-directed reasoning
for specication-based data structure repair. IEEE
Trans. Software Eng. , 32(12):931{951, 2006.
[11] K. Dobolyi and W. Weimer. Changing java's
semantics for handling null pointer exceptions. In 19th
International Symposium on Software Reliability
Engineering (ISSRE 2008), 11-14 November 2008,
Seattle/Redmond, WA, USA , pages 47{56, 2008.
[12] T. Durieux, M. Martinez, M. Monperrus,
R. Sommerard, and J. Xuan. Automatic repair of real
bugs: An experience report on the defects4j dataset.
CoRR , abs/1505.07002, 2015.
[13] Y. H. Eom and B. Demsky. Self-stabilizing java. In
ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI '12,
Beijing, China - June 11 - 16, 2012 , pages 287{298,
2012.
[14] D. Gopinath, S. Khurshid, D. Saha, and S. Chandra.
Data-guided repair of selection statements. In
Proceedings of the 36th International Conference on
Software Engineering , ICSE 2014, pages 243{253, New
York, NY, USA, 2014. ACM.[15] M. Jose and R. Majumdar. Cause clue clauses: Error
localization using maximum satisability. In
Proceedings of the 32nd ACM SIGPLAN Conference
on Programming Language Design and
Implementation , PLDI '11', pages 437{446, New York,
NY, USA, 2011. ACM.
[16] S. Kaleeswaran, V. Tulsian, A. Kanade, and A. Orso.
Minthint: Automated synthesis of repair hints. In
Proceedings of the 36th International Conference on
Software Engineering , ICSE 2014, pages 266{276, New
York, NY, USA, 2014. ACM.
[17] D. Kim, J. Nam, J. Song, and S. Kim. Automatic
patch generation learned from human-written patches.
InProceedings of the 2013 International Conference
on Software Engineering , ICSE '13', pages 802{811.
IEEE Press, 2013.
[18] M. Kling, S. Misailovic, M. Carbin, and M. Rinard.
Bolt: on-demand innite loop escape in unmodied
binaries. In Proceedings of the ACM international
conference on Object oriented programming systems
languages and applications , OOPSLA '12', pages
431{450. ACM, 2012.
[19] E. Kneuss, M. Koukoutos, and V. Kuncak. Deductive
program repair. In Computer-Aided Verication
(CAV) , 2015.
[20] C. Le Goues, M. Dewey-Vogt, S. Forrest, and
W. Weimer. A systematic study of automated
program repair: Fixing 55 out of 105 bugs for $8 each.
InProceedings of the 2012 International Conference
on Software Engineering , ICSE 2012, pages 3{13.
IEEE Press, 2012.
[21] F. Long and M. Rinard. Prophet: Automatic patch
generation via learning from successful patches.
Technical Report MIT-CSAIL-TR-2015-027, 2015.
[22] F. Long and M. Rinard. Staged Program Repair in
SPR. Technical Report MIT-CSAIL-TR-2015-008,
2015.
[23] F. Long, S. Sidiroglou-Douskos, and M. Rinard.
Automatic runtime error repair and containment via
recovery shepherding. In Proceedings of the 35th ACM
SIGPLAN Conference on Programming Language
Design and Implementation , PLDI '14', pages
227{238, New York, NY, USA, 2014. ACM.
[24] M. Martinez and M. Monperrus. Mining software
repair models for reasoning on the search space of
automated program xing. Empirical Software
Engineering , 20(1):176{205, 2015.
[25] M. Monperrus. A critical review of "automatic patch
generation learned from human-written patches":
Essay on the problem statement and the evaluation of
automatic software repair. In Proceedings of the 36th
International Conference on Software Engineering ,
ICSE 2014, pages 234{242, New York, NY, USA,
2014. ACM.
[26] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and
S. Chandra. Semx: Program repair via semantic
analysis. In Proceedings of the 2013 International
Conference on Software Engineering , ICSE '13', pages
772{781, Piscataway, NJ, USA, 2013. IEEE Press.
[27] H. H. Nguyen and M. C. Rinard. Detecting and
eliminating memory leaks using cyclic memory
allocation. In Proceedings of the 6th International
177Symposium on Memory Management, ISMM 2007,
Montreal, Quebec, Canada, October 21-22, 2007 , pages
15{30, 2007.
[28] F. S. Ocariza, Jr., K. Pattabiraman, and A. Mesbah.
Vejovis: Suggesting xes for javascript faults. In
Proceedings of the 36th International Conference on
Software Engineering , ICSE 2014, pages 837{847, New
York, NY, USA, 2014. ACM.
[29] Y. Pei, C. A. Furia, M. Nordio, Y. Wei, B. Meyer, and
A. Zeller. Automated xing of programs with
contracts. IEEE Trans. Software Eng. , 40(5):427{449,
2014.
[30] J. H. Perkins, S. Kim, S. Larsen, S. Amarasinghe,
J. Bachrach, M. Carbin, C. Pacheco, F. Sherwood,
S. Sidiroglou, G. Sullivan, W.-F. Wong, Y. Zibin,
M. D. Ernst, and M. Rinard. Automatically patching
errors in deployed software. In Proceedings of the
ACM SIGOPS 22nd symposium on Operating systems
principles , SOSP '09, pages 87{102. ACM, 2009.
[31] Y. Qi, X. Mao, Y. Lei, Z. Dai, and C. Wang. The
strength of random search on automated program
repair. In Proceedings of the 36th International
Conference on Software Engineering , ICSE 2014,
pages 254{265, New York, NY, USA, 2014. ACM.
[32] Z. Qi, F. Long, S. Achour, and M. Rinard. An analysis
of patch plausibility and correctness for
generate-and-validate patch generation systems. In
Proceedings of the ACM/SIGSOFT International
Symposium on Software Testing and Analysis, ISSTA
2015, 2015.
[33] M. Rinard, C. Cadar, D. Dumitran, D. M. Roy,
T. Leu, and W. S. Beebee. Enhancing serveravailability and security through failure-oblivious
computing. In OSDI , pages 303{316, 2004.
[34] S. Sidiroglou-Douskos, E. Lahtinen, F. Long, and
M. Rinard. Automatic error elimination by horizontal
code transfer across multiple applications. In
Proceedings of the 36th ACM SIGPLAN Conference
on Programming Language Design and
Implementation, Portland, OR, USA, June 15-17,
2015, pages 43{54, 2015.
[35] N. Wang, M. Fertig, and S. Patel. Y-branches: When
you come to a fork in the road, take it. In Proceedings
of the 12th International Conference on Parallel
Architectures and Compilation Techniques , PACT '03',
pages 56{, Washington, DC, USA, 2003. IEEE
Computer Society.
[36] Y. Wei, Y. Pei, C. A. Furia, L. S. Silva, S. Buchholz,
B. Meyer, and A. Zeller. Automated xing of
programs with contracts. In Proceedings of the 19th
International Symposium on Software Testing and
Analysis , ISSTA '10', pages 61{72, New York, NY,
USA, 2010. ACM.
[37] W. Weimer, Z. P. Fry, and S. Forrest. Leveraging
program equivalence for adaptive program repair:
Models and rst results. In ASE'13 , pages 356{366,
2013.
[38] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest.
Automatically nding patches using genetic
programming. In Proceedings of the 31st International
Conference on Software Engineering , ICSE '09', pages
364{374. IEEE Computer Society, 2009.
[39] A. Zeller and R. Hildebrandt. Simplifying and
isolating failure-inducing input. IEEE Trans. Softw.
Eng., 28(2):183{200, Feb. 2002.
178