iDice: Problem IdentiÔ¨Åcation for Emerging Issues
Qingwei Lin Jian-Guang Lou Hongyu Zhang Dongmei Zhang
Microsoft Research, Beijing 100080, China
{qlin, jlou, honzhang, dongmeiz}@microsoft.com
ABSTRACT
One challenge for maintaining a large-scale software system,
especially an online service system, is to quickly respond
to customer issues. The issue reports typically have many
categorical attributes that reect the characteristics of the
issues. For a commercial system, most of the time the vol-
ume of reported issues is relatively constant. Sometimes,
there are emerging issues that lead to signicant volume in-
crease. It is important for support engineers to eciently
and eectively identify and resolve such emerging issues,
since they have impacted a large number of customers. Cur-
rently, problem identication for an emerging issue is a te-
dious and error-prone process, because it requires support
engineers to manually identify a particular attribute combi-
nation that characterizes the emerging issue among a large
number of attribute combinations. We call such an attribute
combination eective combination, which is important for is-
sue isolation and diagnosis. In this paper, we propose iDice,
an approach that can identify the eective combination for
an emerging issue with high quality and performance. We
evaluate the eectiveness and eciency of iDice through ex-
periments. We have also successfully applied iDice to several
Microsoft online service systems in production. The results
conrm that iDice can help identify emerging issues and re-
duce maintenance eort.
Categories and Subject Descriptors
D.2.5 [ Software Engineering ]: Testing and Debugging -
diagnostics; D.2.8 [ Software Engineering ]: Management
- software quality assurance
Keywords
Emerging issues, problem identication, eective combina-
tion, problem diagnostic, issue reports
1. INTRODUCTION
Despite immense eorts spent on software quality assur-
ance, various types of failures often occur in software systems
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14-22, 2016, Austin, TX, USA
c2016 ACM. ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI:http://dx.doi.org/10.1145/2884781.2884795in actual operations. Whenever customers encounter a prob-
lem, they can report the issue to the technical support team
of the software system. For a widely-used, large-scale soft-
ware system, the support team could receive a large number
of issue reports from customers over a period of time. Fur-
thermore, in order to enable quick response to changing mar-
ket requirements, today's software systems evolve quickly
- developers constantly update existing features, add new
features, and release new versions within a relatively short
time. Therefore, support engineers often encounter new is-
sues reported by customers. The maintenance task could be
even more challenging for large-scale online service systems
(such as Microsoft Oce 365, Windows Azure, and Visual
Studio Online) [20, 7], which are attracting growing number
of customers and are updated more frequently.
A typical customer issue report consists of many cate-
gorical attributes such as product version, the problematic
product feature, product conguration, client OS, service
package, country, etc. Each attribute has a set of distinct
values. An issue report also includes a time stamp recording
the time at which the report was received. Therefore, the
issue report data can be treated as multi-dimensional, time
series data.
Oftentimes, it is observed that the volume of issue reports
under a certain attribute combination could suddenly in-
crease signicantly at a certain point of time. Such a \burst"
may be due to major feature changes, conguration mis-
takes, environmental incidents, or software bugs. We call
these issues emerging issues . For example, in June 2013, a
bug in a new update of a Microsoft online service caused
a sudden increases of issue reports related to a certain at-
tribute combination. This upward trend of issue reports
continued for several days and incurred signicant increase
of support cost. The emerging issues have negative impact
on a large number of customers, therefore, they should be
assigned a high priority for xing. Otherwise, more cus-
tomers could be aected and more issue reports could be
received. In order to detect emerging issues, the support en-
gineers need to identify a particular attribute combination
that can characterize the issues. The attribute combina-
tion helps isolate the problem, nd the root cause of the
issue, and bring the system back to normal. We call such an
attribute combination eective combination since it charac-
terizes an emerging issue.
In current practice, among the Microsoft support engi-
neers we have talked with, the identication of eective com-
binations is largely performed manually, e.g., they use Excel
Pivot Table to look for the candidate attribute combina-
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   214
tions that may indicate the signicant upward trend in the
number of issue reports. Clearly, when the number of at-
tributes and attribute values are many, support engineers
need to search through a large number of possible attribute
combinations in order to identify the eective ones reect-
ing emerging issues. Detecting emerging issues could be very
dicult, because the burst of an emerging issue can be eas-
ily lost within the background noisy issue reports and no
clear burst can be observed in the overall trend of all is-
sues. Detection of such hidden emerging issues is a rather
labor intensive and time-consuming process. We will elab-
orate more about this problem using a motivating example
in Section 2.
In this paper, we propose iDice, an automated algorithm
that helps support engineers identify the eective combina-
tions that are associated with emerging issues. We formu-
late the problem of identifying emerging issues as a pattern
mining problem: given a volume of customer issue reports
over a period of time, the goal is to search for an attribute
combination that isolates the entire multi-dimensional time
series dataset into two partitions: one showing a signicant
increase of issue volume, and the other not showing such an
increase. As the number of attribute combinations could be
huge, we design several pruning techniques to signicantly
reduce the search space.
We have performed experiments to evaluate the eective-
ness and eciency of iDice, on both real-world and synthetic
datasets. The F-measure values achieved by iDice are all
above 0.85. We have also successfully applied iDice to the
maintenance of Microsoft online services, and conrmed the
usefulness of iDice in industrial practice.
The contributions of this paper are as follows:
We propose iDice, an automated approach that helps
support engineers eectively and eciently identify the
eective combinations that isolate the emerging issues.
We evaluate iDice through in-house experiments. In
addition, we also apply iDice to Microsoft online ser-
vice systems in production. The results conrm the
eectiveness and eciency of iDice.
The rest of this paper is organized as follows. In Section
2, we introduce the motivation of this work. We formulate
the problem of emerging issue identication and introduce
iDice in detail in Sections 3 and 4, respectively. In Section
5, we evaluate iDice using experiments. In Section 6, we
describe the case studies where iDice is applied to production
online services. We present the related work in Section 7 and
conclude this paper in Section 8.
2. MOTIVATION
2.1 An Example
The research described in this paper is motivated by the
real-world requirements arisen from the maintenance of a
Microsoft online service. Every day, the support team re-
ceives a lot of issue reports from customers around the world.
Table I shows a sample of issue reports. In the table, each
row represents one issue report. Each issue report has a
set of associated attributes, such as TenantType, Product-
Feature, ProductVersion, SubscriptionPackage, DataCenter,
Country, UserAgent, ClientOS, and so on. Each attribute
has multiple possible values and depicts a certain aspectof the context about the issue. For example, the attribute
ProductFeature species the product feature with which
customers encountered problems. Besides the attributes,
each issue report also has a time stamp that records the
time at which the issue was reported.
Most of the time, the support team receives a relatively
stable number of issue reports every day. The issue reports
contain many combinations of attributes, such as:
Country=\India"; TenantType=\Home"; DataCenter=\DC1"
Country=\India"; TenantType=\Edu"; DataCenter=\DC6"
Country=\UK"; TenantType=\Edu"; DataCenter=\DC3"
...
As mentioned in Section 1, sometimes the number of is-
sue reports associated with a certain attribute combination
could suddenly increase, i.e. an emerging issue could occur.
This could be due to a server-side fault (e.g., a conguration
error, a software bug, a compatibility issue, or other software
problems), or a hardware-related fault (e.g., hard disk crash,
network device failure, etc.). For example, Figure 1 shows
a real emerging issue detected in 2013. The number of is-
sue reports containing the following attribute combination
signicantly increased on December 8:
Country=\India"; TenantType=\Edu"; DataCenter=\DC6"
Further investigation showed that before December 8, 2013,
this attribute combination was associated with an average
of 70 issue reports per day. Starting from December 8, the
number of issue reports associated with this attribute com-
bination rose to over 300 per day. This particular attribute
combination characterized the emerging issue and provided
useful information for problem investigation. We call such
an attribution combination Eective Combination.
Having identied the emerging issue, support engineers
quickly found out that these issue reports were related to a
software conguration error, which failed to create accounts
for customers in India, who were subscribed to the service
through TenantType EDU. Therefore, many customers con-
tacted the Microsoft support team asking for help. The ef-
fective combination associated with the emerging issue pro-
vided useful information for support engineers to isolate the
problem and locate the root cause.
 050100150200250300350400
27-Nov
28-Nov
29-Nov
30-Nov
1-Dec
2-Dec
3-Dec
4-Dec
5-Dec
6-Dec
7-Dec
8-Dec
9-Dec
10-Dec# Issue Reports
# DateCountry = India , 
TenantType = Edu,
DataCenter = DC6.
Figure 1: An example of emerging issue
As demonstrated by this example, it is important to eec-
tively and eciently detect such emerging issues and their
associated attribute combinations. This not only helps re-
solve customer issues and increases customer satisfaction,
215Table 1: Sample Issue Reports for a Microsoft Online Service System
Time TenantType ProductFeature ProductVersion Package DataCenter Country UserAgent ClientOS ...
5-Dec 11:01 Home Admin V15 RTM Basic DC1 India IE7.0 Win8.1 ...
6-Dec 15:01 Home Modify V15 RTM Basic DC1 USA IE6.0 WinXP ...
7-Dec 10:22 Edu Edit V16 SP1 Lite DC3 UK Firefox30.0 Win8.1 ...
8-Dec 05:33 Edu Share V16 SP2 Pro DC6 India IE7.0 WinXP ...
8-Dec 15:04 Enterprise Share V15 RTM Lite DC1 Australia IE11.0 Win8.1 ...
8-Dec 15:16 Edu Admin V16 SP1 Ultimate DC6 India IE11.0 Win8.1 ...
8-Dec 15:26 Edu Edit V16 SP2 Pro DC6 India Firefox31.0 Win7 ...
but also helps reduce the support cost as more incoming
support requests related to the same issue could be avoided.
2.2 Challenge
To detect emerging issues and identify the associated at-
tribute combinations, in current practice, most of the time
the support engineers manually explore dierent attribute
combinations using a pivot table1, and look for candidate
combinations that are associated with signicant upward
trend in the issue reports. For example, to detect the emerg-
ing patterns shown in Figure 1, the support engineers may
rst aggregate the data by date. They then look for poten-
tial emerging issues by selecting an attribute combination
iteratively and summarizing the data table by that attribute
combination.
However, manual identication of emerging issues could
be very dicult when support engineers cannot observe sig-
nicant changes in the overall number of issues. Some emerg-
ing issues can only be observed under certain attribute com-
binations. They may not always cause signicant changes
to the overall number of issue reports for the entire system.
For example, in the motivating example, the number of issue
reports associated with the eective combination fCountry
= \India " ;TenantType = \Edu ";DataCenter = \DC6"g
experienced a \burst". However, because the support engi-
neers actively resolved other issues, the overall quality of the
system was improved. Therefore, the total number of issues
for the entire system did not show any noticeable burst. In
this scenario, the support engineers need to examine various
attribute combinations one by one to detect emerging issues.
Clearly, the manual identication approach has the fol-
lowing problems:
Inecient : If there are many attributes and each at-
tribute has many dierent values, there will be an
explosive number of possible attribute combinations,
which makes manual identication expensive or even
impossible. For example, if each issue report has 8 at-
tributes and each attribute has more than 10 values,
then there are up to 108possible attribute combina-
tions. In order to locate the emerging issue as shown
in Figure 1, support engineers might need to examine
a huge number of candidate combinations. Obviously,
the manual identication approach does not scale.
Ineective : The manual approach is often an ad-hoc
exploration of dierent attribute combinations. The
eective attribute combinations may be missed. The
identied combinations may also contain redundancy
and overlap.
To automate the identication of emerging issues, sim-
ple frequent itemset mining approaches [10] alone are not
1http://en.wikipedia.org/wiki/Pivot tablesucient. From Table 1, we can see that the dataset of
issue reports has two characteristics. On one hand, it is
time series data with temporal order; on the other hand, it
is also multi-dimensional data since each record has many
attributes. Therefore, the dataset of issue reports can be
treated as an combination of multi-dimensional data and
time series data. Most of the closed itemset approaches
only handle multi-dimensionality without considering the
temporal property. They cannot detect the change point
at which the volume of issue reports signicantly increases.
Furthermore, the attribute combination associated with an
emerging issue may not be the frequent patterns across the
entire dataset. This is because the number of data records
associated with an eective combination may only occupy a
relatively small portion of the entire data space. For exam-
ple, the total number of reports associated with the eective
combinationfCountry = \India " ;TenantType = \Edu ";
DataCenter = \DC6"gmay be lower than the number of
reports associated with the attribute combination fCoun-
try= \USA" ;TenantType = \Home "gin the same month.
However, the number of reports associated with the eective
combination experienced a \burst", while the others did not.
Emerging pattern mining [8, 16] can be used to detect at-
tribute combinations whose frequencies change signicantly
from one dataset to the other. Emerging pattern mining
methods mainly target at dataset without temporal order,
while the issue report data is time series data. Therefore,
they cannot be directly applied to detect changes and isolate
issues in issue reports.
In summary, the problems of manual identication reect
a great demand on an automated tool for timely detection of
emerging issues. However, there is no existing approach that
can be directly used to solve this problem. This motivates
us to design a new approach to eectively and eciently
identify the emerging issues in issue reports.
3. PROBLEM FORMULATION
3.1 Effective Combinations
Motivated by the great demand from the maintenance of
real-world software systems, we design an approach to au-
tomatically identify the attribute combinations that charac-
terize the emerging issues (i.e., the eective combinations).
Because issue report data could contain a large number of
attribute combinations, the challenge is to eectively iden-
tify the eective combinations from all the possible combi-
nations.
The entire set of attribute combinations form a lattice
structure[10] through their subset-superset relationships, as
illustrated in Figure 2. Each node denotes an attribute com-
bination, each edge denotes a subset-superset relationship.
216For two attribute combinations XandY, if the data con-
tainingXis also included in the data containing Y, we call
Xa subset of YandYa superset of X. For example, ABC
is a subset of AB and AC.
Figure 2: Eective combination
In Figure 2, the node lled with black denotes an eec-
tive combination. It can isolate the nodes with signicant
increases from other nodes that do not exhibit signicant
increases. If an eective combination Xis the symptom
of an emerging issue, and it is related to the burst of is-
sue report volume, then all subsets of Xshould also be af-
fected by the same issue and exhibit certain degree of sig-
nicant volume increases at the same time, because other
attributes are orthogonal to this emerging issue. For exam-
ple, iffCountry=Japan, ProductVersion=V16SP2, Clien-
tOS=Win8.1gis an eective combination indicating an emerg-
ing issue, then all its subsets, such as fCountry=Japan,
ProductVersion=V16SP2, ClientOS=Win8.1, TenantType=
EdugandfCountry=Japan, ProductVersion= V16SP2, Clien-
tOS=Win8.1, UserAgent=IE7.0 g, should also experience sig-
nicant increases in issue volume. On the other hand, the
supersets of an eective combination may or may not ex-
hibit an increase in volume for the same emerging issue. For
example, we may not see a noticeable burst from all reports
aboutfClientOS =Win8:1g.
3.2 Requirements
The eective combinations should be able to provide infor-
mation to help support engineers identify and isolate prob-
lems. We have identied the following requirements for an
eective combination:
Impactful : Support team needs to allocate limited re-
source to help as many customers as possible through
resolving the most impactful emerging issues. There-
fore, an eective combination should associate with an
emerging issue that corresponds to a relatively large
number of issue reports.
Reecting changes : The identied attribute combina-
tions should be able to separate themselves from the
other attribute combinations. That is, the volume of
issue reports associated with the identied attribute
combination should exhibit a signicant burst after the
change point, while the volume of reports associated
with other attribute combinations does not exhibit the
same burst along time.Less redundant : The identied attribute combinations
should be compact and concise because redundant re-
sults decrease detection accuracy and waste investiga-
tion eorts.
4. THE PROPOSED APPROACH
We propose iDice, an approach to automatic identication
of eective combinations characterizing the emerging issues.
The key challenges for iDice is that the possible attribute
combinations form a huge search space, which makes it dif-
cult or even impossible to check all the combinations one by
one. Therefore, the core algorithm of iDice is to eectively
reduce the search space without missing the eective combi-
nations. In order to achieve this goal, we have designed the
following pruning strategies according to the requirements
described in Section 3.2:
Impact based Pruning. Each eective combination
should be related to a large volume of issue reports,
which means that it has impacted a large number of
customers. We adopt an impact-based strategy to re-
move the attribute combinations associated with small
numbers of issue reports.
Change Detection based Pruning. Eective com-
binations should be able to reect signicant volume
increases of issue reports. Therefore, in the search pro-
cess, we prune o the attribute combinations that ex-
hibit small or no changes in issue volume.
Isolation Power based Pruning . We use this strat-
egy to remove the possible redundancy in the identied
eective combinations, and to make the results concise
and compact.
Figure 3 shows an overview of the iDice approach. iDice
takes as input the issue report data, which is multi dimen-
sional, time series data such as the one shown in Table
1. Each dimension denotes a categorical attribute of is-
sues. After preprocessing, iDice performs the following three
steps to eciently search for eective combinations: impact-
based pruning, change detection based pruning, and isola-
tion power based pruning. Finally, the obtained results are
ranked and returned to users. In this section, we rst intro-
duce each of the major steps and then present the complete
algorithm.2
4.1 Impact based Pruning
In order to eectively reduce the search space, we perform
pruning based on the impact of attribute combinations. We
measure the impact of the attribute combinations as their
corresponding issue volume. We only consider the attribute
sets associated with large volume of issue reports, and prune
o those without high enough volume.
To achieve so, iDice utilizes a mining algorithm to iden-
tify all the equivalent closed itemsets whose support value
2Note that our approach works in batch mode. This is be-
cause for the service products we worked with, the issue
data is provided in a batch scenario - the data is rstly
collected from dierent channels (including phone call, on-
line form, and onsite-support) and then uploaded to a cen-
tralized server for further analysis on regular basis (after
post-processings such as data cleansing). Therefore, we are
unable to get data in a real-time manner.
217Effective 
Combinations
Change 
Detection 
based PruningImpact based 
PruningIssue Reports
Isolation Power 
based pruningResult
 Ranking
Attribute
CombinationsFigure 3: An overview of iDice
is larger than a user-dened support threshold. There are
many closed itemset mining algorithms [10] available and
any one of them can be applied. In our implementation,
we use the BFS (Breadth-rst Search) based closed itemset
mining approach [10]. We ignore the time stamp information
of the issue reports, and apply the BFS-based closed item-
set mining algorithm on the entire dataset D. Clearly, the
resultant closed itemsets Xcontain all the attribute com-
binations leading to the emerging issues. It is straightfor-
ward to prove that Xobtained from Dcontain all the closed
itemsets obtained from any supersets of D. As a property
of eective combination, if one attribute combination has
no sucient volume, it is obvious that all its subsets do not
have enough volume either (refer to Section 3). Thus we can
directly remove this attribute combination together with all
its subsets.
As an example, considering the following two attribute
combinations:
X=f Country=India; TenantType=Edu; DataCenter=DC6 g
Y=f Country=USA; TenantType=Edu; DataCenter=DC1 g
If the occurrence of Yis low (i.e., lower than the support
threshold) and the occurrence of Xis high (i.e., higher than
the support threshold), Yand all its subsets will be pruned
o andXwill remain.
4.2 Change Detection based Pruning
In addition to being impactful, the attribute combinations
we look for should also be related to emerging issues. In
other words, we need to nd out the attribute combinations
that correspond to signicant increases in issue report vol-
ume (i.e., the bursts).
For each closed itemset obtained after impact-based prun-
ing, we check whether the corresponding volume of reports
has a signicant increase. To achieve so, we consider the
time stamp information of the data and build time series
data for the closed itemsets. Each data point in the time
series denotes the volume of reports that contains the cor-
responding closed itemsets at a particular time stamp. We
then apply change detection algorithm to detect the change
points (i.e. the points where issue bursts occur) in the timeseries data.
In our implementation, we adopt GLR (Generalized Like-
lihood Ratio) [3] as the change detection algorithm. Change
detection can be formulated as a hypothesis-testing prob-
lem. Suppose the values of a time series t a distribution
0, if there is a change, the values during the change region
conform to another distribution 1. Here, the hypothesis H0
corresponds to \no change" , andH1corresponds to \change" .
GLR maintains a threshold. Given a few continuous data
points, if the sum of their logarithm-likelihood-ratio is greater
than the threshold, these continuous data points are de-
tected as a change region. The rst point of the continuous
data points is deemed as the change point [3]. For example,
in Figure 1, the points from \Dec 8" to \Dec 10" constitute
a change region, and the point of \Dec 8" is a change point.
For the time series data without any change points, the cor-
responding attribute combinations will be pruned.
As an example, considering the following two attribute
combinations:
X=f Country=India; TenantType=Edu; DataCenter=DC6 g
Y=f Country=UK; TenantType=Home; DataCenter=DC1 g
Their corresponding time series are SXandSY, respec-
tively. If we detect that the occurrence of SXhas a signi-
cant change (e.g., from 100 to 300) starting from Dec 8 (i.e.
change point), while SYdoes not have a signicant change
over time, then Ywill be pruned, and the change point of
SXwill be used in follow-up isolation power based pruning.
4.3 Isolation Power based Pruning
As discussed in Section 3, an eective combination should
be able to isolate the attribute combinations that exhibit
changes from the other combinations that do not. This prop-
erty can help us further remove the possible redundancy in
the obtained itemsets. To achieve so, we propose the notion
ofIsolation Power :
IP(X) = 1

a+
b
Xaln1
P(ajX)+Xbln1
P(bjX)
+(
a Xa) ln1
P(ajX)+ (
b Xb) ln1
P(bjX)
(1)
LetSXbe the time series data corresponding to the attribute
combination X,Xadenotes the volume of time series data
inSXduring the change region of X, andXbdenotes the
volume of time series data in SXbefore the change point of
X. 
adenotes the entire volume during the change region
ofX, and 
 bdenotes the entire volume before the change
point ofX. Alldenote the mean value of the corresponding
time series. Also:
P(ajX) =Xa
Xb+Xa,P(bjX) =Xb
Xb+Xa,
P(ajX) =
a Xa

a+
b Xb Xa,
P(bjX) =
b Xb

a+
b Xb Xa.
The proposed Isolation Power is based on the idea of Infor-
mation Entropy [1]. As discussed in Section 3 and illustrated
by Figure 2, the entire set of attribute combinations form a
lattice, and every node in the lattice can split the dataset
into two parts: the issue reports that contain the attributes,
and the reports that do not contain the attributes. If an at-
tribute combination is an eective combination, all its subset
218nodes in the lattice exhibit signicant increases in the same
change region, but its sibling nodes do not. Therefore, an
eective combination is the node that can exactly split the
entire dataset into two parts: with and without a signicant
increase. According to the information theory [1], the overall
entropy of two datasets (e.g., AandB) where each dataset
(AorB) contains samples with an identical property (e.g.,
all of them exhibit increases, or all of them do not exhibit
increases) is much smaller than the entropy of two datasets
where samples with dierent properties are mixed together.
Based on this concept, we calculate Isolation Power to mimic
the calculation of entropy.
During the search process, if the current set has a higher
isolation power than its direct supersets and subsets, then
the current set is an eective attribute combination, satisfy-
ing the requirements for eective combinations described in
Section 3. In this case, all its subsets will not be searched.
In this way, we can reduce search space using the Isolation
Power measure. Considering a simple example with three
attribute combinations:
X=f Country=Indiag
Y=f Country=India; TenantType=Edu; DataCenter=DC6 g
Z=fCountry=India; TenantType=Edu; DataCenter=DC6;
Package=Liteg
IfYhas a higher isolation power than its subset Zand su-
persetX,Ywill be considered and ZandXwill be removed
from the search space.
4.4 Result Ranking
In real-world scenarios, we may obtain a set of eective
combinations from the data. We rank the eective combi-
nations according to their relative signicance. We adopt a
score similar to Fisher distance [10] for the ranking:
R=palnpa
pb(2)
In the above equation, pdenotes the ratio: p=VXt
Vt,
whereVXtdenotes the volume of current eective combina-
tion during a time period tandVtdenotes the total volume
duringt.pa,pbare the ratios during the detected change
region (i.e. the time period of a change) and before the
detected change point (i.e. the starting point of a change
region) respectively.
We can see from Equation 2 that the score R: (1) considers
the global impact of the combination. If two combinations
have the same change ratio (which means that they have the
same trending signicance), we will rank the one with larger
volume higher; (2) reects the change ratio throughpa
pb.
The attribute combination with a very low Rscore is
considered less signicant and can be pruned away. In our
implementation, we prune away the attribute combinations
whose R score is lower than a cuto threshold (which is em-
pirically set to 1.0 in our implementation). Finally, we rank
the remaining attribute combinations and output them as
theeective combinations.
4.5 The Overall Algorithm
The pseudo code for identifying eective combinations is
shown in Algorithm 1. The algorithm takes the issue re-
port data (which is multi-dimensional, time-series data) as
input, and searches for eective combinations that are as-
sociated with emerging issues. The preprocessing at LineAlgorithm 1: iDice(D )
Input :D, issue reports (multi-dimensional time series
data.)
Output :P, the eective combinations.
1PreProcessing dataset D;
2 while (true) do
3 //BFS-based Closed Itemset Mining on D
pi=BFSClosedItemset ():getNext () ;
4 if (pi==NULL )then
5 Break;
6 Impact Evaluation ( pi);
7 if impact ofpi<minimum support then
8 Remove the current set pi;
9 Skip all subsets of pi;
10 Continue;
11si Mappiinto time series form;
12 Perform Change Detection on si;
13 ifsihas no change point then
14 Continue;
15 Evaluate isolation power of pi;
16 if isolation power of pi>isolation power of pi's
direct subsets then
17 Addpito the candidate list P;
18 Skip all subsets of pi;
19 Continue;
20/*Result ranking*/
21 foreach pinPdo
22 Calculate the ranking score Rp;
23 ifRp<cuto threshold then
24 RemovepfromP;
25 Continue;
26 Rankpby its signicance Rp;
27ReturnP;
1 includes data cleaning, which is to lter out the obvious
noise attributes in the dataset, e.g., all the \null" values.
For each closed itemset pireturned by the BFS (Breadth
First Search) based closed itemset mining process, iDice per-
forms Impact-based pruning (lines 6-10), Change Detection
based pruning (lines 11-14), and Isolation Power based prun-
ing (lines 15-19). These steps prune away attribute com-
binations and reduce the search space, making it possible
to identify eective combinations from a large number of
attribute combinations. Lines 21 to 26 denote the result
ranking part of iDice.
5. EVALUATION
In this section, we describe our experimental evaluation
of iDice. We aim to answer the following questions:
RQ1: How eective is iDice in detecting emerging issues?
RQ2: How ecient is iDice in detecting emerging issues?
RQ3: How does iDice perform under dierent congura-
tions?
5.1 Setup
Datasets . We use two datasets to evaluate the eective-
ness of iDice.
219Microsoft Service X : Service X is a geographically dis-
tributed, external-facing, web-based online service, serving
hundreds of millions of end users. During a certain pe-
riod of time, the support team encountered a \burst" in
the number of issue reports. We collected the actual issue
report data, which contains more than ten thousand issue
reports collected in early 2015. Each report has two at-
tributes Application and DataCenter (apart from the Time
and ID attributes). The Application attribute has 5 values
(A1-A5). The DataCenter attributes has 10 values (DC1-
DC10). There are total 92 emerging issues in this dataset,
which are veried by the domain experts in the product
team.3Due to sensitivity reasons, we omit and anonymize
the detail information about the dataset.
Synthetic : To further evaluate iDice, we also design a sim-
ulation dataset. We rst randomly generate a 60-day dataset
with 100 issue reports on each day. Each issue report has
8 attributes, and each attribute has 4 distinct values. A
randomly generated combination of attribute values is then
assigned to each report. In this way, we create a synthetic
dataset with 60*100 data points. We then simulate the oc-
currence of a single emerging issue as follows. First, we
randomly select an attribute combination feto represent an
error pattern and a day dcas the change point, i.e. the day
whenfeoccurs. Second, we increase the number of issue
reports containing feby a random value, e.g., between 15
and 20. Third, we perform such increment for feeach day
after the change point dcuntil the 60thday. In this way, we
get the resultant dataset seeded with one emerging issue.
Similarly, we generate a dataset to simulate the occurrence
of two emerging issues. During the evaluation, we randomly
seed the emerging issue(s), perform the detection, and cal-
culate the evaluation measures. We repeat such process N
times and compute the average results.
Measures . We use F-measure ,Recall , andPrecision
metrics to evaluate the eectiveness of iDice. The F-measure
is dened as follows:
F-measure =2PrecisionRecall
Precision +Recall(3)
wherePrecision =TP
TP+FPandRecall =TP
TP+FN. Here,
TP (true positive) is the number of actual eective combi-
nations correctly reported by iDice. FP (false positive) is
the number of wrongly reported eective combinations. TN
(true negative) is the number of non-eective combinations
that are correctly reported by iDice. FN (false negative) is
the number of eective combinations that are not reported
by iDice. The higher the metric values, the better the de-
tection performance. In addition, we measure the execution
time (in seconds) to evaluate the eciency of iDice.
Hardware. The experiments were run on a PC with
(Intel(R) Core(TM) i7-3770 CPU @ 3.34GHz with 16GB
RAM).
Comparison algorithm . We compare iDice with DP-
Miner [16], which is one of state-of-the-art algorithms for
mining emerging patterns. DPMiner detects emerging pat-
3We choose this dataset because it has quality labels veried
by domain experts. Although we do have other real datasets
that have a larger number of attributes, the quality of their
labels may be insucient due to the diculties of manual
identication eort as described in Section 2. Furthermore,
such real issue reports cannot be labeled by non-domain ex-
perts due to lacking of domain knowledge.Table 2: Evaluation results on Microsoft Service X
dataset
Metrics iDice DPMiner
Recall 0.84 0.83
Precision 0.88 0.37
F-meaure 0.86 0.51
Table 3: Evaluation results on synthetic dataset
Single Issue Two Issues
Metrics R P F R P F
iDice 0.98 0.95 0.96 0.84 0.90 0.86
DPMiner 1.00 0.16 0.27 1.00 0.14 0.25
(R: Recall, P: Precision, F: F-measure)
terns, which are itemsets whose support rates increase sig-
nicantly from one dataset to the other. As described in
Section 2, emerging pattern mining techniques such as DP-
Miner is not originally designed for mining eective com-
binations because it does not support change detection in
time series data. Also, there are no pruning strategies based
on isolation power. In our experiment, to enable compari-
son, we manually set the data before the change point dcas
one database, and the remaining data as the other database.
We then apply DPMiner and compare its results with those
produced by iDice.
5.2 Experimental Results
5.2.1 RQ1: The effectiveness of iDice
According to the setup described in Section 5.1, we con-
duct experiments to evaluate iDice using both Microsoft Ser-
vice X data and the synthetic data. The impact threshold
is set to 1%.
For Microsoft Service X data, iDice is able to achieve
good results, as shown in Table 2. The Recall, Precision,
and F-measure values are 0.84, 0.88, and 0.86, respectively.
Furthermore, iDice signicantly outperforms the DPMiner
approach. The improvement on F-measure is 69%.
For the synthetic data, we run iDice with single and two
seeded emerging issues. Each experiment is performed 50
times. The average results are shown in Table 3. For the
single-issue and two-issue experiments, the F-measure achieved
by iDice is 0.96 and 0.86, respectively. These results are
considered satisfactory. iDice also outperforms DPMiner in
terms of F-measure. Although DPMiner is able to detect
all eective attributes (Recall is 100%), it produces many
redundant and irrelevant results, leading to low Precision
values.
The reason why DPMiner performs worse than iDice is
that it does not utilize the Isolation Power pruning strategy
as designed by iDice. As a result, there are many redundant
and noisy itemsets in the results.
5.2.2 RQ2: The efÔ¨Åciency of iDice
To evaluate the eciency of iDice, we measure the exe-
cution time of iDice on the two datasets (Microsoft Service
X and synthetic). For each dataset, we select an increasing
percentage of records and run both iDice and DPMiner, until
all records are selected. We then compare the performance
of iDice and DPMiner under dierent data sizes.
The evaluation results are shown in Figure 4 and Figure 5.
On both datasets, the execution time of DPMiner increases
220sharply when the data size increases, while the execution
time of iDice stays relatively steady. The results conrm
the usefulness of the pruning strategies adopted by iDice.
Overall, our experiments show that iDice is eective and
ecient in identifying eective combinations for emerging
issues. iDice also performs better (in terms of both eective-
ness and eciency) than the related approach (DPMiner).
Figure 4: The performance of iDice on Microsoft
Service X dataset
Figure 5: The performance of iDice on the synthetic
dataset
5.2.3 RQ3: iDice under different conÔ¨Ågurations
As described in Section 4.1, iDice uses a parameter, the
impact threshold (i.e. the minimum support), for impact-
based pruning, because we are only interested in the at-
tribute combinations that are associated with a large volume
of issue reports (i.e. above the threshold). Currently, we al-
low users to set this parameter empirically based on their
domain knowledge. Dierent values of the parameter could
lead to dierent results. For example, if the impact thresh-
old is too high, some useful information may be removed
during the search process. On the other hand, if the thresh-
old is too low, the results could contain some redundant and
noisy information.
To understand the impact of dierent threshold settings
on the results, we evaluate the eectiveness of iDice un-der dierent minimum support rates varying from 0.1% to
5% of the total data. The results for the synthetic dataset
is shown in Figure 6. Initially, when the minimum sup-
port rate is 0.1%, the F-measure is about 0.86. When the
minimum support rate increases from 0.5% to 2%, the F-
measures achieved by iDice are around 0.9 and remain rel-
atively constant. When the minimum support continues in-
creasing, the F-measure starts to drop. This is because the
number of data points exceeding 2% minimum support is
rare. To summarize, the results show that, when minimum
support is properly set, iDice is able to achieve consistently
good results.
Figure 6: Eectiveness of iDice with dierent mini-
mum support values
As described in Section 4.2, iDice selects only attribute
combinations that are associated with signicant volume
changes, and uses the GLR method to detect the changes.
In our experiments on the synthetic dataset, we simulate
an emerging issue by increasing its occurrence by a certain
amount (between 15 and 20). Here, we study the impact of
dierent degrees of volume changes (i.e., dierent degrees of
burst) on the results.
To do so, for the seeded attribute combination, after the
change point, we insert the number of issue reports randomly
drawn from the following intervals: [3, 5], [5, 10], [10, 15],
[15, 20], [20, 25], and [25, 30]. In this way, we create 6 dif-
ferent synthetic datasets. We then evaluate the eectiveness
of iDice on each dataset. The results are shown in Figure 7.
We can see that when the number of seeded issues increases
from [3, 5] to [25, 30], the eectiveness of iDice increases
too. The results indicate that iDice performs better if the
degree of burst is high.
5.3 Threats to Validity
We have identied some threats to validity that should be
taken into consideration when using iDice.
Large number of issue reports: iDice is designed to
help support engineers quickly detect emerging issues in a
large number of issue reports based on data mining and sta-
tistical analysis. It is thus suitable for large-scale, software-
intensive systems that are experiencing a long period of evo-
lution and have received a large number of issue reports over
time. For a small or short-lived system, the number of issue
reports is often small, thus making the statistical analysis
inappropriate.
Lack of ground truth for validation: In practice, the
amount of issue reports for a large-scale system is huge and
221Figure 7: The eectiveness of iDice under dierent
degree of burst
keeps increasing. Furthermore, the issue reports could con-
tain a large number of attribute combinations. It is very
time-consuming for support engineers to manually identify
and verify each and every emerging issue as well as the asso-
ciated eective combinations. Furthermore, such real issue
reports cannot be labeled by non-domain experts due to the
lack of domain knowledge. Therefore, it is very dicult for
us to obtain a large-scale real dataset with high-quality la-
bels. To overcome this threat, in our experiments we used
a relatively smaller dataset from Microsoft Service X, which
has quality labels veried by domain experts in the prod-
uct team. We have also designed a synthetic dataset and
manually seeded issues into the dataset. Furthermore, we
have applied iDice to real-world online service systems and
conrmed its eectiveness in practice.
6. SUCCESSFUL STORIES IN INDUSTRIAL
PRACTICE
We have successfully applied iDice to the maintenance
of Microsoft Service Y, which is a large-scale, widely-used
online service system. The dataset of Service Y contains
11 attributes (including Feature, Country, Topic, Package,
QuantitySize, DataCenter, AppVersion, OSVersion, etc.),
and 5:21012attribute combinations. Using a brute-force
method, it may take 108seconds to analyze the combina-
tions, which is inapplicable in practice. iDice helped the
support team of Service Y identify and resolve emerging is-
sues. In this section, we present two successful stories of
iDice. We omit and anonymize some details for conden-
tiality reasons.
6.1 The Mobile Client Issue
In June 2013, the Service Y team experienced a sudden
increase of support requests. This upward trend of issue
reports continued for several days, and the team was not able
to locate the problem manually during those days. Although
Service Y team eventually found the problem and xed it,
the maintenance cost signicantly increased due to the large
number of support requests received during this period.
In order to reduce the support cost, Service Y team looked
for techniques to help them quickly and eectively detect the
emerging issues. As requested, we ran iDice over the issue
data of Service Y and successfully detected the signicant
upward trend in the support request volume starting in June
2013. As suggested by the identied eective combination,we found that this emerging issue was related to a version
update of Service Y's mobile client. A software bug in the
new mobile client caused an UI problem, which triggered the
high volume of issue reports asking for help from the support
team. Had iDice been used immediately after this incident
occurred, a large amount of support cost (in millions dollars)
could have been reduced.
6.2 The Double-Billing Issue
iDice helped Service Y team detect more emerging is-
sues after it was used in production. One example is the
double-billing issue as follows. In November 2013, iDice
detected a signicant upward trend in the support request
volume (Figure 8). The corresponding eective combination
is:fIssueCode =DoubleBilled, Country =UnitedStates,
Feature =CreditRequestg. As indicated by the eective
combination, this emerging issue occurred in the US market
and was related to credit request and double billing. Using
this information, the support team quickly diagnosed the
problem and located the root cause, which was related to
a vendor F. For the customers who used the charging ser-
vice provided by F, a software bug in the service caused the
double-billing problem. iDice enabled the support team to
quickly diagnose and x this emerging issue, thus reducing
the cost for handling support requests and improving cus-
tomer satisfaction.
050100150200250300350400450500# Issue Reports
November 2013ChangePoint IssueCode = Double Billed = 
Country = United States
Feature = Credit Request
Figure 8: An emerging issue of double billing for
Microsoft Service Y
7. RELATED WORK
7.1 The Analysis of Issue Reports
Many customer issues could be reported against a large
and complex software system. The issues could be caused by
program bugs, miscongurations, operation errors, or envi-
ronments. Over the years, there have been many empirical
studies on the characteristics of issues of software systems.
Various methods have also been proposed to detect such
issues. For example, Li et al. [18] and Chou et al. [5] man-
ually collected bugs from large-scale open source projects
(such as Mozilla and Apache Web Server) and analyzed
the bug characteristics. They classied the bugs into dif-
ferent categories (such as root causes, impacts and software
components) and studied the correlation between categories.
Zhang and Kim [28] studied the changes in bug report vol-
ume over time. They identied six common quality evolu-
tion patterns such as Impulse (short, dramatic increase of
bug volume) and Hills (long-lasting high volume of bugs).
222Their research suggests that the quality of an evolving soft-
ware system is constantly changing. Kenmei et al. [14] ap-
plied ARIMA time series to model and forecast the trend of
issue requests for open source projects. Li et al. [17] studied
18 software usage characteristics and investigated how they
are related to eld quality and how they dier between pre-
and post-release. They identied the ve most important us-
age characteristics through general linear regression. Kast-
ner et al. [12, 13] analyzed complete conguration spaces of a
highly congurable software system to detect some classes of
issues. Menzies et al. [21] proposed machine-learning based
methods for learning from bug datasets succinct rules that
explain quality issues. Our work on iDice integrates closed
itemset mining, change detection, and pruning techniques
to detect emerging issues in multi-dimensional, time-series
issue report data.
Many bug prediction techniques (e.g, [6, 22, 24, 27]) select
a small set of important features that can best predict the
defect-proneness of a software module. Our work focuses on
the selection of features (attributes) that characterizes the
emerging issues.
Kim et al. [15] found that only 10 to 20 crashes account for
the large majority of crash reports for FireFox. If these top
crashes are not xed, more crash reports are expected. They
trained a machine learner on the features of top crashes of
past releases, and predict the top crashes before a new re-
lease. Bird et al. [4] found that the reliability of a software
system depends on the environment it operates in. They
performed an empirical study of more than 200,000 Win-
dows users, and identied many factors that aect system
reliability. They also applied association rule mining to de-
tect the inuence of factor combinations on reliability. Our
work detects a set of attributes (features) that are associated
with a signicant change of issue reports.
Epifani et al. [9] targeted at the problem of identifying
change points concerning the reliability and performance of
a software service. Their approach is based on executions
trace produced by client invocations, and only tries to detect
change points. Nguyen et al. [23] detected performance re-
gressions by analyzing a large number of performance coun-
ters using control charts. iDice works on customer issue re-
ports. It identies not only the change points, but also the
eective attribute combinations that are associated with the
changes.
7.2 Frequent and Emerging Pattern Mining
Frequent pattern mining has been widely used in software
engineering research for problems such as bug detection [19,
26] and API usage mining [30, 25]. Our work on iDice is
related to emerging pattern mining [8], which is not well
explored by SE community. An emerging pattern is dened
as an itemset whose support rate increases signicantly from
one dataset to the other[8]. Zhang et al. [29] proposed an im-
proved algorithm for emerging pattern mining named Con-
sEPMiner, which utilizes two types of constraints, External
constraints and Inherent constraints, to prune the search
space eectively. Although these constraints are the basis of
many posterior ltering methods, the algorithm could delete
some important patterns, which adversely aects the detec-
tion accuracy.
Bailey et al. [2] introduced the rst tree-based approach
for mining Jump Emerging Patterns, which are patterns not
occurring in the background data. Unlike the traditionalFP-Trees [11], in their approach, the count of each item is
split into two values: count of an item in the rst dataset,
and count of an item in the second dataset. Li et al. in-
troduced DPMiner [16], which is an improved algorithm of
[2]. In our work, DPMiner is used as a baseline algorithm
for comparison.
Dierent from aforementioned methods for mining emerg-
ing patterns, iDice has the following characteristics: 1) The
data characteristic is dierent: the related emerging pattern
mining methods mainly target at dataset without temporal
order, while iDice supports time series data. 2) The goal is
also dierent: iDice targets at mining the attribute combi-
nation with the most isolation power in one dataset, while
the related methods focus on mining all itemsets whose sup-
port values are dierent from one dataset to the other. 3)
Regarding the pruning criteria, the related methods mainly
dene impact thresholds for pruning, while iDice performs
pruning based on impact, change detection, and isolation
power. Therefore, the results of iDice have not only isola-
tion power, but also less redundancy. These characteristics
make iDice more suitable for issue diagnosis, where both
eectiveness and eciency are desired.
8. CONCLUSION
Problem identication for emerging issues is important for
the maintenance of large-scale software systems, especially
online service systems. In this paper, we aim to identify
an eective combination that is associated with a signicant
volume increase of issue reports. We have proposed iDice, an
approach for identifying eective combinations for emerging
issues. Our experiments demonstrate the eectiveness and
eciency of our approach. We have also applied iDice to
the maintenance of several Microsoft online services, and
the results conrm the usefulness of iDice in practice.
In the future, we will make iDice a distributed algorithm
that is capable of supporting ultra-large-scale datasets. Fur-
thermore, we will also apply iDice to solve other software
engineering problems, such as the identication of miscon-
gurations for a large and complex software system.
Acknowledgment
We thank the intern students Chenhui Wang, Chen Luo,
Hongzhi Chen, Hongbo Zhao, Kelu Diao and Yudong Xiao
for their helpful discussions and the initial implementation
of iDice. We thank partners at Microsoft product teams
for their collaboration and suggestions on the application of
iDice in practice.
9. REFERENCES
[1] C. Arndt. Information Measures: Information and its
Description in Science and Engineering . Springer,
2004.
[2] J. Bailey, T. Manoukian, and K. Ramamohanarao.
Fast algorithms for mining emerging patterns. In
Principles of Data Mining and Knowledge Discovery ,
pages 39{50. Springer, 2002.
[3] M. Basseville, I. V. Nikiforov, et al. Detection of
abrupt changes: theory and application , volume 104.
Prentice Hall Englewood Clis, 1993.
[4] C. Bird, V.-P. Ranganath, T. Zimmermann,
N. Nagappan, and A. Zeller. Extrinsic inuence
factors in software reliability: A study of 200,000
windows machines. In Companion Proceedings of the
22336th International Conference on Software
Engineering, ICSE'14, pages 205{214, 2014.
[5] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler.
An empirical study of operating systems errors. In
Proceedings of the 8th ACM Symposium on Operating
Systems Principles , SOSP '01, pages 73{88, 2001.
[6] M. D'Ambros, M. Lanza, and R. Robbes. Evaluating
defect prediction approaches: A benchmark and an
extensive comparison. Empirical Softw. Engg. ,
17(4-5):531{577, Aug. 2012.
[7] R. Ding, Q. Fu, J. G. Lou, Q. Lin, D. Zhang, and
T. Xie. Mining historical issue repositories to heal
large-scale online service systems. In Proc. the 44th
IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN) , pages 311{322, 2014.
[8] G. Dong and J. Li. Ecient mining of emerging
patterns: Discovering trends and dierences. In
Proceedings of the fth ACM SIGKDD international
conference on Knowledge discovery and data mining ,
pages 43{52. ACM, 1999.
[9] I. Epifani, C. Ghezzi, and G. Tamburrelli.
Change-point detection for black-box services. In
Proceedings of the Eighteenth ACM SIGSOFT
International Symposium on Foundations of Software
Engineering, FSE '10, pages 227{236. ACM, 2010.
[10] J. Han and M. Kamber. Data Mining: Concepts and
Techniques . Morgan kaufmann, 2006.
[11] J. Han, J. Pei, and Y. Yin. Mining frequent patterns
without candidate generation. In ACM SIGMOD
Record , volume 29, pages 1{12. ACM, 2000.
[12] C. Kastner, P. G. Giarrusso, T. Rendel, S. Erdweg,
K. Ostermann, and T. Berger. Variability-aware
parsing in the presence of lexical macros and
conditional compilation. In Proc. International
Conference on Object-Oriented Programming,
Systems, Languages and Applications (OOPSLA'11) ,
pages 805{824. ACM Press, 2011.
[13] C. Kastner, K. Ostermann, and S. Erdweg. A
variability-aware module system. In Proc.
International Conference on Object-Oriented
Programming, Systems, Languages and Applications
(OOPSLA'12) , pages 773{792. ACM, 2012.
[14] B. Kenmei, G. Antoniol, and M. Di Penta. Trend
analysis and issue prediction in large-scale open source
systems. In Proc. the 12th European Conference on
Software Maintenance and Reengineering (CSMR
2008) , pages 73{82, April 2008.
[15] D. Kim, X. Wang, S. Kim, A. Zeller, S. Cheung, and
S. Park. Which crashes should i x rst?: Predicting
top crashes at an early stage to prioritize debugging
eorts. IEEE Transactions on Software Engineering ,
37(3):430{447, 2011.
[16] J. Li, G. Liu, and L. Wong. Mining statistically
important equivalence classes and delta-discriminative
emerging patterns. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 430{439, 2007.
[17] P. L. Li, R. Kivett, Z. Zhan, S.-e. Jeon, N. Nagappan,
B. Murphy, and A. J. Ko. Characterizing the
dierences between pre- and post- release versions of
software. In Proceedings of the 33rd International
Conference on Software Engineering, ICSE '11, pages716{725, 2011.
[18] Z. Li, L. Tan, X. Wang, S. Lu, Y. Zhou, and C. Zhai.
Have things changed now?: an empirical study of bug
characteristics in modern open source software. In
Proceedings of the 1st workshop on Architectural and
system support for improving software dependability ,
pages 25{33. ACM, 2006.
[19] B. Livshits and T. Zimmermann. Dynamine: Finding
common error patterns by mining software revision
histories. In Proceedings ESEC/FSE-13 , pages
296{305, New York, NY, USA, 2005. ACM.
[20] J.-G. Lou, Q. Lin, R. Ding, Q. Fu, D. Zhang, and
T. Xie. Software analytics for incident management of
online services: An experience report. In Proc. the
IEEE/ACM 28th International Conference on
Automated Software Engineering (ASE'13) , pages
475{485, 2013.
[21] T. Menzies, A. Butcher, D. Cok, A. Marcus,
L. Layman, F. Shull, B. Turhan, and T. Zimmermann.
Local versus global lessons for defect prediction and
eort estimation. Software Engineering, IEEE
Transactions on , 39(6):822{834, June 2013.
[22] T. Menzies, Z. Milton, B. Turhan, B. Cukic, Y. Jiang,
and A. Bener. Defect prediction from static code
features: Current results, limitations, new approaches.
Automated Software Engg. , 17(4):375{407, Dec. 2010.
[23] T. H. Nguyen, B. Adams, Z. M. Jiang, A. E. Hassan,
M. Nasser, and P. Flora. Automated detection of
performance regressions using statistical process
control techniques. In Proceedings of the 3rd
ACM/SPEC International Conference on Performance
Engineering, ICPE '12, pages 299{310, 2012.
[24] S. Shivaji, E. Whitehead, R. Akella, and S. Kim.
Reducing features to improve code change-based bug
prediction. Software Engineering, IEEE Transactions
on, 39(4):552{569, April 2013.
[25] J. Wang, Y. Dang, H. Zhang, K. Chen, T. Xie, and
D. Zhang. Mining succinct and high-coverage API
usage patterns from source code. In Proceedings of the
10th Working Conference on Mining Software
Repositories , MSR '13, pages 319{328, 2013.
[26] A. Wasylkowski, A. Zeller, and C. Lindig. Detecting
object usage anomalies. In Proceedings ESEC-FSE
'07, pages 35{44, New York, NY, USA, 2007. ACM.
[27] H. Zhang. An investigation of the relationships
between lines of code and defects. In Proc. IEEE
International Conference on Software Maintenance
(ICSM'09) , pages 274{283, Sept 2009.
[28] H. Zhang and S. Kim. Monitoring software quality
evolution for defects. IEEE Software , 27(4):58{64,
2010.
[29] X. Zhang, G. Dong, and R. Kotagiri. Exploring
constraints to eciently mine emerging patterns from
large high-dimensional datasets. In Proc. of the 6th
ACM SIGKDD international conference on Knowledge
discovery and data mining , pages 310{314, 2000.
[30] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. Mei.
MAPO: Mining and recommending API usage
patterns. In Proc. the 23rd European Conference on
Object-Oriented Programming (ECOOP 2009) , Genoa,
pages 318{343. Springer-Verlag, 2009.
224