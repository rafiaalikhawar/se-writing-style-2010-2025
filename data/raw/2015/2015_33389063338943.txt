On UsingMachine Learningto Identify Knowledgein API
Reference Documentation
Davide Fucci
Universityof Hamburg
Hamburg, Germany
fucci@informatik.uni-hamburg.deAlireza
Mollaalizadehbahnemiri
Universityof Hamburg
Hamburg, Germany
alirezam.alizadeh@gmail.comWalidMaalej
Universityof Hamburg
Hamburg, Germany
maalej@informatik.uni-hamburg.de
ABSTRACT
UsingAPIreferencedocumentationlikeJavaDocisanintegralpart
ofsoftwaredevelopment.Previousresearchintroducedagrounded
taxonomythatorganizesAPIdocumentationknowledgein12types,
including knowledge about the Functionality ,Structure, andQual-
ityof an API. We study how well modern text classification ap-
proachescanautomaticallyidentifydocumentationcontainingspe-
cific knowledge types.Wecompared conventionalmachine learn-
ing (k-NN and SVM) with deep learning approaches trained on
manually-annotated Java and .NET API documentation ( n= 5,574).
Whenclassifyingtheknowledgetypesindividually(i.e.,multiple
binaryclassifiers)thebestAUPRCwasupto87%.Thedeeplearning
andSVMclassifiersseemcomplementary.Forfourknowledgetypes
(Concept,Control,Pattern,andNon-Information ),SVMclearly out-
performs deep learning which, on the other hand, is more accurate
for identifying the remaining types. When considering multiple
knowledge types at once (i.e., multi-label classification) deep learn-
ingoutperformsnaïvebaselinesandtraditionalmachinelearning
achieving a MacroAUC up to 79%. We also compared classifiers
usingembeddingspre-trainedongenerictextcorporaandStack-
Overflow but did not observe significant improvements. Finally,
toassessthegeneralizabilityoftheclassifiers,were-testedthem
ona different,unseen Pythondocumentationdataset. Classifiers
forFunctionality ,Concept,Purpose,Pattern,andDirective seemto
generalizefromJavaand.NETtoPythondocumentation.Wedis-
cuss our results and how they inform the development of tools for
supportingdevelopers sharingandaccessingAPI knowledge.
CCS CONCEPTS
·Software andits engineering →Documentation .
KEYWORDS
API documentation, information needs,machine learning
Permissionto make digitalor hard copies of allorpart ofthis work for personalor
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’19, August 26ś30,2019, Tallinn,Estonia
©2019 Associationfor Computing Machinery.
ACM ISBN 978-1-4503-5572-8/19/08...$15.00
https://doi.org/10.1145/3338906.3338943ACMReference Format:
Davide Fucci, Alireza Mollaalizadehbahnemiri, and Walid Maalej. 2019. On
UsingMachineLearningtoIdentifyKnowledgeinAPIReferenceDocumen-
tation. In Proceedings of the 27th ACM Joint European Software Engineer-
ingConferenceandSymposiumontheFoundationsofSoftwareEngineering
(ESEC/FSE ’19), August 26ś30, 2019, Tallinn, Estonia. ACM, New York, NY,
USA,11pages.https://doi.org/10.1145/3338906.3338943
1 INTRODUCTION
SoftwaredevelopersreuselibrariesandframeworksthroughAppli-
cationProgrammingInterfaces(APIs).Theyoftenrelyonreference
documentation to identify which API elements are relevant for
the task at hand or how the API can be instantiated, configured,
andcombined[ 1].Comparedto otherknowledgesources,suchas
tutorialsandQ&Aportals,referencedocumentationlikeJavaDoc
and PyDoc are considered the official API technical documenta-
tion. They provide detailed and fundamental information about
API elements, components,operations, andstructures [ 2,3].
As API documentation can be thousands of pages long [ 4,5],
accessing specific knowledge therein can be tedious and time-
consuming[ 1].Theinformationnecessarytoaccomplishataskcan
be scatteredacross thedocumentationpages ofmultipleelements,
such asclasses, methods,and properties. Thus,developers try to
useothersourcestofulfilltheirinformationneeds[ 6].Forexam-
ple,althoughtheJavaDevelopment Kit(JDK)APIdocumentation
contains more than 7,000 pages, as of early 2019, there are more
than3million StackOverflowposts taggedas java.
Overthelastdecade,softwareengineeringresearchersstudied
whatinformationdevelopersneedwhenconsultingAPIdocumenta-
tion[3,7,8].Onelineofresearchfocusesonmatchinginformation
needs with the types of knowledge available in the documentation.
MaalejandRobillard[ 3]tookafirststepinthisdirectionbydevelop-
inganempirically-validatedtaxonomyof12knowledgetypesfound
within API reference documentation. A single documentation page
canincludeseveralknowledgetypes(Figure 1).Functionality and
Directive are particular types of knowledge needed to accomplish a
developmenttask,whereasthe Non-information typecontainsonly
uninformative boilerplate text [ 3]. Maalej and Robillard argue that
their knowledge categorization allows for a) understanding and
improving the documentation quality and b) satisfying developers’
information needs[ 3].
TheresearchcommunitystudiedspecificknowledgetypesinAPI
referencedocumentation.Forexample,Montperrusetal.[ 9]and
Seid etal. [ 10] studied Directive to prevent the violation ofAPI us-
age constraints. Robillard and Chhetri [ 5] filtered Non-information
whenrecommendingAPIstodevelopers.Automatedapproaches
109
ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia DavideFucci,Alireza Mollaalizadehbahnemiri, andWalidMaalej
Directive
Figure 1: A reference documentation page in the JDK API
annotated with theknowledgetypesitcontains.
suggested so far are based either on linguistic features engineer-
ing[5]oronsyntactic patterns [ 9].
Thisworkinvestigateshowwellmoderntextclassificationap-
proachescanautomaticallyidentifytheknowledgetypessuggested
by Maalej and Robillard in API documentation. Based on a dataset
of 5,574 labelled Java and .NET documentation, we trained, tested,
andcomparedconventionalmachinelearningapproachesÐi.e., k-
NearestNeighbors( k-NN)andSupportVectorMachines(SVM)Ðas
well as deep learning approachesÐi.e., recurrent neural network
(RNN) with a Long Short-Term Memory (LSTM) layer. The RNN
learns features from a semantic representation of general purpose
text(i.e.,embeddings).Hence,westudiedhowourresultsareim-
pactedbytrainingthenetworkusingsoftwaredevelopment-specific
corporafromStackOverflowasopposedtoageneralpurposeone.
Finally,westudiedthegeneralizabilityoftheclassifierstoanunseen
dataset obtainedfrom the Pythonstandardlibrary.
Thispapermakesthreecontributions.First,wepresentadetailed
classification benchmark for API documentation. The settings in-
clude different machine learning approaches and configurations,
different word embeddings, different datasets for different APIs, as
well as various evaluation metrics. Second, as we share the code
anddataofthisstudy,1severaltop-performingclassifiers (e.g.,
AUPRC≥80%)alreadyhavepracticalrelevance.Third,ourfindings
and discussion of related work provide insights to researchers, tool
vendors, and practitioners on how machine learning can help
better organize, access, and share API knowledge, e.g. through tag-
ging the documentation pages with the knowledge types. While
currentkeywords-basedsearchescanhelpdevelopers,e.g.,findthe
documentationpageforamethod,theycannotanswerquestions
relatedtospecificinformationneeds,e.g.inFigure 1,łWhathap-
pensifIpassa nullvaluetothe addPropertyChangeListener ?.A
developerisrequiredtoreadtheentirepage,whichmayormaynot
contain the answer to these questions. Machine learning classifiers
canhelpnavigatingthedocumentationnotonlythroughkeywords
1https://doi.org/10.5281/zenodo.3265783orAPIelementsbutalsothroughknowledgetypes.Finally,APIcre-
ators can use the classifiers results to identify gaps of information
needsintheirdocumentation.
Therestofthepaperisorganizedasfollows.Section 2describes
ourresearchsettings,Section 3presentstheconfigurationsofthe
classifiers, and Section 4reports their performance. We discuss
related work in Section 5and the implications and limitations of
our results inSection 6.Finally,Section 7concludes the paper.
2 RESEARCH SETTINGS
This section introduces the research questions,method,anddata.
2.1 Research QuestionsandMethod
Maalej and Robillard [ 3] proposed an empirically-validated taxon-
omy of 12 knowledge types based on grounded theory and sys-
tematic content analysis (17 experienced coders, 279 person-hours
effort). Table 1lists these knowledge types which represent the
basisforthiswork.Ourprimarygoalistostudyhowwellsimple
machine learning for text classification, without additional feature
engineeringoradvancednaturallanguageprocessing(NLP)tech-
niques, can identify these knowledge types. That is, our classifiers
label adocument withone ormore knowledge types.
Table 1: Twelve knowledge types included in reference doc-
umentation (adapted fromMaalej and Robillard [ 3]).
Knowledgetype Brief description
Functionality Describes the capabilities of the API, and
whathappenswhenitisused.
Concept Explains terms used to describe the API be-
havior or theAPI implementation.
Directive Describe what the user is allowed (not al-
lowed) todowith theAPI.
Purpose ExplainstherationaleforprovidingtheAPI
or for a design decision.
Quality Describesnon-functionalattributesofthe
API, including itsimplementation.
Control DescribeshowtheAPImanagesthecontrol-
flow and sequence ofcalls.
Structure DescribestheinternalorganizationofAPI
elementsincluding theirrelationships.
Pattern Explains how to get specific results using
theAPI.
Example Providesexamples abouttheAPI usage.
Environment Describes theAPI usageenvironment.
Reference Pointerstoexternal documents.
Non-information Uninformative,boilerplate text.
Westudytwomaintextclassificationapproachesinthispaper.
Traditional approaches usually learn the classes from the occur-
rencesofcertainkeywordsorphrasesinthetrainingset.Compu-
tational intensive approaches, often referred to as deep learning,
usethesemanticsofthekeywordsÐi.e.,thecontextofthekeyword
occurrences [ 11].
For traditional approaches, we study two algorithms frequently
used for text classification: k-NN and SVM. For deep learning,
weusedRNNwithanLSTMlayer,whichisparticularlyeffective
for text categorization problems [ 12]. This architecture is recom-
mendedover,forexample,ConvolutionalNeuralNetwork(CNN).
While the latter is more suited for image recognition [ 13], RNN
110On UsingMachineLearning to IdentifyKnowledgein APIReference Documentation ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
with LSTM handles more efficiently the dependencies between
features [ 12]. We alsocompare theseclassifiers to naïve baselines.
Thetasktackledinthisstudyistoassignknowledgetypestoan
APIdocument.Asthedocumentcancontainmorethanoneknowl-
edgetype,thistaskis modelled asamultiplebinary classification
problem consisting of independently train one binary classifier for
each knowledge type. Another approach is to train a multi-label
classifierÐi.e., a classifier that outputs a set of knowledge types
ratherthanasingleone.Weanalyzeandreporttheresultsforboth
approacheswhen answeringthe following researchquestion.
RQ1. How well can text-based classifiers identify knowledge
typesinAPIreferencedocumentation?Inparticular,candeep
learningimprove over traditional approaches?
For text classification tasks, the input layer of an RNN usually
consistsofembeddingstrainedonlargeunlabeled textual corpora
necessary to capture rich semanticfeatures [ 14]. Pre-trainedem-
beddings are available and caneasilybe łpluggedž in the network
withoutfurthereffort.However,whiletheseembeddingssavecom-
putationaltimeandwellrepresentcommonlanguagetasks,they
can miss software engineering or API-specific semantics. This mo-
tivates our secondresearchquestion.
RQ2. Do software development-specific text embeddings im-
proveclassificationresultscomparedtogeneralpurposeones?
Finally, a common question for machine learning evaluation is
whetheramodeltrainedonacertaindatasetgeneralizestoother
data.Theoriginaldataset includesdocumentationofthestandard
Javaand.NETlibraries[ 3].Sinceweaimtoassessthegeneralizabil-
ity of our approach to API reference documentation written in a
differentstyle,wemanuallyannotatedanewdatasetsampledfrom
thedocumentationofPythonStandardLibrary.Weusedthisdataset
as an additionaltest setto report our classifiers performance.
RQ3. Can documentation classification based on knowledge
types be generalizedacrossAPIs?
Weassessmodelsbasedon10-foldcross-validationusing10%of
thedatasetastestset.Whencomparingindividualknowledgetypes
classifiers, we report Area Under Precision-Recall Curve (AUPRC).
Precision-Recallcurvesareacommonmetrictoevaluatebinaryclas-
sificationandareobtainedbyplottingprecisionandrecallvaluesat
differentprobabilitiesthresholds[ 15].Inparticular,theyareused
to evaluate machine learning model trained on imbalanced data
sets[15].Therefore,AUPRCisasummarymeasureofperformance
irrespective ofaparticularthreshold.
Whencomparingclassifiersformultipleknowledgetypes,were-
portperformanceaccordingtotwotypesofmetrics,item-basedand
label-based.Theitem-basedmetricsarea) HammingLoss ,namely
theratioofwronglyclassifiedlabelstothetotalnumberoflabels(its
bestvalueiszero)andb) SubsetAccuracy ,namelythepercentage
ofexact matches between the predictedandthe actual labelset.The label-based metrics are precision, recall, F1-measure (For-
mula1), andArea UnderReceivingOperator Curve (AUC).
F1=2×TruePositives
2×TruePositives+FalsePositives+FalseNeдatives(1)
The Receiving Operator Curve is created by plotting recall against
false positive rate (FPR, Equation 2), at different probability thresh-
olds. Accordingly, AUC does not depend on a particular thresh-
old[16].TocalculatethevalueofTruePositive,FalsePositives,and
FalseNegatives we used0.5 as probability threshold [ 11].
FPR=1−TrueNeдatives
TrueNeдatives +FalsePositives(2)
Thelabel-basedmetricsaremacro-averaged.Macro-averaging
applies the metric to the binary partition of each predicted label
andthenaveragestheresultsÐi.e.,labelshaveequalcontributionin
the final result. In contrast, micro-averaging first aggregates the in-
dividualmetriccomponents(i.e.,truepositives,falsepositives,true
negatives,andfalsenegatives)ofeachlabelandthenaveragesthem.
Therefore,micro-averagingisbiasedtowardthemajorityclasses
andshould be avoidedwhen evaluating unbalanceddatasets [ 17].
Wecomparetheresultsoftheclassifierstonaïvebaselines, MF1,
MF2,andRAND.Thefirsttwoalwaysassignthefirst(respectively
oneofthefirsttwo)most-frequentlabelstoeachdocument,whereas
the latter assignsarandom label.
Table 2:Overview ofthe CaDOdataset.
#documents Wordsmax. Wordsmean Vocab.size
.NET 2,782 2,874 89 10,630
JDK 2,792 2,099 86 10,763
Total 5,574 2,874 87 17,758
2.2 Research Data
We use the CaDOdatasetcreated by Maalej and Robillard [ 3]
as the result of their content analysis of the JDK 6 and .NET 4.0
APIreferencedocumentation. CaDOcontains5,574observations.
The columns include the name of the API element (e.g., a class,
a method, or a property), its documentation text, and 12 binary
values indicating the presence (or absence) of the corresponding
knowledge type. Table 2summarizes the dataset textual properties.
Themostfrequentknowledgetypesare Functionality andNon-
information , whereas QualityandEnvironment are the least fre-
quent.WedidnotmergesomeoftheknowledgetypesasMaalejand
Robillardreportednosignificantevidenceoftheirco-occurence[ 3].
The majority of the documents (90.5%) contains one to five of
the 12 knowledge types. We use the SCUMBLE [18] score (∈[0,
1])toreportthelevelofunbalancedness.Foragivenlabel,ahigh
SCUMBLE score represents a large difference between the frequen-
ciesofalltheotherco-occurringlabels.Ingeneral,datasetswith
highscoresareproblematicforclassificationtasks[ 19].However,
for datasets characterized by low SCUMBLE score, resampling can
reduce unbalancedness [ 19].CaDOmeanSCUMBLE score is0.11.
We applied random under- and over-sampling to 90% of the
dataset (i.e., the training set). We did not resample the test set (10%
of the dataset) to avoid sampling bias. For resampling, we removed
111ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia DavideFucci,Alireza Mollaalizadehbahnemiri, andWalidMaalej
30%ofthedocumentscontaining Functionality andNon-Information
intheirlabelsetandduplicated50%ofthedocumentscontaining
Environment andQuality.Thethresholdswereobtainedempirically
basedonthe SCUMBLE score.Afterresampling,thetrainingand
testsetscontain3,876and430observationsrespectively.Figure 2
presents the label frequencies in the dataset we used to train the
models after re-sampling.
We prepareda new Python datasetconsistingof 100APIdoc-
umentation pages (i.e., modules, types, attributes, and methods)
from the Python 2.7 standard library.2We selected the Python
standard library since its code is organized differently than Java
or.NETasitmakesextensiveuseofmodulesinwhichfunctions,
classes, and variables are defined. The Python programmingpara-
digmis more functionalthan Javaand .NET which instead follow
an object-oriented paradigm.Python is dynamically typed, and its
referencedocumentationtendstofocusonfunctions,whereastypes
documentationisembeddableinthesourcecode(e.g.,throughDoc-
strings). Finally, its development and documentation are driven
by an open source, non-profit community (the Python Software
Foundation)whereas Java and.NETare ownedbycorporations.
We followed the sampling strategy suggested by Maalej and Ro-
billard [3]Ði.e., stratified random sampling. We first created strata
for each of the base modules and then randomly sampled API doc-
umentation from each stratum proportionally to their frequencies.
TwoPh.D.studentsinsoftwareengineering,accustomedtowork
withPython,manuallylabelledtheknowledgetypesineachdoc-
ument.Forthistask,weprovidedthemthesameguidelinesfrom
Maalej and Robillard3with small adaptations, such as providing
examplesusingthePythonprogramminglanguage.Theagreement
on the label set was 14%Ði.e., 14 out of the 100 examples were
labeled with the exact same set of knowledge types. The overall
agreement was 75%Ði.e., of the 1200 labels (100 examples ×12
labels), 300 were conflicting. Two of the authors addressed the con-
flicts and created the final dataset. Figure 3shows the distribution
of knowledge types in the Pythondataset.Functionality is the
majority label. Pythonrepresents an additional test set (i.e., no
2https://docs.python.org/2.7/library/
3https://cado.informatik.uni-hamburg.de
Figure 2:Knowledgetypesin CaDOafter resampling.
Figure 3:Knowledgetypesinthe Python dataset.
examples from this dataset are used to train the classifiers), which
we didnot resample to avoid biasedresults.
For both datasets, we performed several, simple operations to
clean and prepare the textual data. We lower-cased, tokenized,
and applied stop-words removal to the API documentation text.
Then,we transformedterms in an order-preserving one-hot vec-
tors.Forthedeeplearningclassifiersinourbenchmark,wetrain
GloVe[20]embeddingsbasedonfourlargecorpora,summarized
in Table3. The Common Crawl ( CC) is a pre-trained embedding
downloadedinMarch2018.4Itincludes840Btokensandavocab-
ulary of 2.2M words. The corpus contains high-quality, general-
purpose text crawled from the Internet. However, the CCcorpus is
missingdomain-specifictermspresentinthe CaDOdataset.Accord-
ingly,inthe CCotfembeddings,themissingwordsfromthe CC
corpusaretrainedon-the-fly[ 21].Finally,weobtainedacompletely
domain-specificrepresentationoftheinputbytrainingembeddings
on two additional corpora, StackOverflow ( SO) and StackOverflow
API (SOapi). The former includes 20 million posts, while the latter
includes 4million posts taggedas javaor.net.
3 CLASSIFIERSCONFIGURATION
Thissectionreportsinformationabouttheconfigurationofboth
machine learninganddeep learningclassifiers usedinthis study.
3.1 Traditional Machine Learning
Themachinelearningapproachesweselectedforourclassification
task are SVM [ 22] andk-NN [23] as well as their adaptations to
multi-label problems, namely One-vs-Rest SVM (OvRSVM) and
Multi Label k-NN (ML- kNN). We use unigrams and bigrams ex-
tracted from the CaDOdataset as their input features as n-gram
languagemodelsareeasytocomputeanduse.Moreover,theyhave
been used in studies where machine learning and natural language
processing are appliedto software engineeringcontexts[ 24,25].
SVM is one of the most investigated approaches for statistical
documentclassificationanditisconsideredstate-of-the-art[ 22,26].
Moreover, it showed good results in software engineering-specific
text classification problems (e.g., [ 27,28]). SVM finds the hyper-
planemaximizingthemarginbetweentwoclassesinthefeature
4https://commoncrawl.org
112On UsingMachineLearning to IdentifyKnowledgein APIReference Documentation ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
Table 3:Summary ofthecorporaused to train theGloVe embedding.
ID Name Corpusdescription #docs #vocabulary
CC Common Crawl General purpose,high-qualitytextcrawledfrom Internet pages 2.2M∼220.000
CCotf Common Crawlon-the-fly Common Crawlwhere missingwordsare learnedfrom CaDO 2.2M∼220.000
SO StackOverflow StackOverflowquestionsandanswers 20M∼400.000
SOapi StackOverflowJava and.NETposts StackOverflowquestionsandanswers taggedas javaor.net 4M∼100.000
space, and it can learn and generalize high-dimensional features
typical for text classification tasks [ 22,26]. When taking into ac-
count multipleknowledgetypesat once,wetrainedandreported
theresultsofaSVMmodeladaptedtosuchproblemÐi.e.,OvRSVM
usingbinary relevance [ 29]. OvRSVM considersadditional parame-
tersandconstraintsnecessarytosolvetheoptimizationproblem
withseveralclassesandto handletheseparationofseveralhyper-
planes [29]. For the SVM classifiers, we report the best model after
hyper-parameterstuningusing GridSearch[ 30].
k-NN is a widely-used approach in machine learning [ 31]. It
determines the knearest neighbors of a document using Euclidean
distance.Thenitassignsthedocumentalabelbasedonthedocu-
mentneighborsusingBayesdecisionrules[ 31].Forthemulti-label
classification,weuseML- kNN,whichoutperformswell-established
multi-label classifiers [ 32].
3.2 RNNwith LSTMLayer
Deeplearninghasrecentlybroughtsubstantialimprovementsinthe
fieldofmachinevisionandnaturallanguageprocessing(NLP)[ 11].
TheLSTMlayerextends RNNcapabilitiesbyutilizingseveralgates
andamemorycellintherecurrentmoduletoalleviatethevanishing
gradient problem and to handle more efficiently the long-term
dependencies between features [ 12].
tanh ReLUReLUsigmoidInput LSTM Dense Dense Output
Figure 4: Architecture of the RNN used for classification of
theknowledgetypes.
Figure4shows the architecture we used in this work. The
network is composed of a single LSTMlayer, two dense layers,
and an output layer. The number of units in the LSTMlayer is
proportionaltodimensionsofthevectorsusedtorepresenteach
word. The dense layers contain 128 and 64 unitsrespectively. The
number of units in the output layer is the number of knowledge
types (i.e., 12 units). The core component of the LSTMlayer is a
memorycellwhichstorestheinformationrelatedtotheprevious
analysis steps within the network. At each step of the training,
thenetworkpredictstheoutputbasedona)thenewinput,b)the
previousstateoftheotherhiddenlayersofthe RNN,andc)andthe
current state of the memory cell. The gates learn how to modify
the memory cell to enhance prediction accuracy(see Figure 5).Theforget gate (ft) processes the information from the previous
hidden state layer ( ht−1) and the current input ( Xt)Ði.e., a rep-
resentation of the API documentation text. It then decides what
information should be discarded or kept from the previous state
of the memory cell ( Ct−1). Theinput gate (it) is responsible for
selectingnewinformationfromtheinput( Xt)thatshouldbestored
inthecellstate.Thethirdgateiscalled outputgate (ot)anddecides
which part of the available information in the memory cell should
be usedto produce the final output( ht).
Figure5:AsingleLSTMrecurrentmodulecontaininginput
(it), output( ot), andforgets gates ( ft).
The role of the forget cell in our network is to optimally discard
informationrelatedtopreviousknowledgetypeswhentheychange
inthenewinput.Forinstance,whentheknowledgetypeinthenew
inputisDirective,theforgetgateremovesthepiecesofinformation
associatedwithotherknowledgetypes.Consequently,theforget
gatereducestheambiguityofthememorycellwhenlearningin-
dividual types. The features associated with the knowledge type
in the currentinput document are moved intothe memory cell. In
thememorycell,theinputgatedecideswhatinformationshould
be stored. For example, when the current input contains Directive,
itsfeatureswillbeextractedandstoredinthememorycellusing
the input gate.Finally,the outputgate selects the most significant
features associatedwiththe Directive type.
Theinputlayer fortheRNNconsistsofwordembeddingvec-
torstrainedusingGloVe[ 20].GLoVereliesontheglobaloccurrences
of a word in a corpus by defining a word-to-word co-occurrence
matrix.Eachvalueofthematrixcontainstheprobability Pofword
jappearinginthecontextofword i,asreportedinEquation 3.In
particular, Xijdenotes the number of times word joccurs in the
contextofword iandXidenotesthenumberoftimesthatanyword
kappearsinthecontextofword i.Namely,GLoVedefinesalearning
functionthatestimatestheprobabilityratiooftheco-occurrence
113ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia DavideFucci,Alireza Mollaalizadehbahnemiri, andWalidMaalej
oftwotarget words, iandj,given acontextword k[20].
Pij=P(j|i)=Xij/Xi,Xi=/summationdisplay.1
kXik. (3)
Asinputvectorswithidenticallengthspeeduptheprocessof
buildingtheembeddinglayerof theRNN[ 33],weconsidered300
as the maximum vector length and padded shorter vectors with
zeroes. Therefore, our input layer is a 2D matrix where each row is
a300-dimensionalunit(artificialneuron).Thenumberofunitsin
the inputlayer depends onthe vocabulary size ofthe corpusused
totraintheembeddings.Whena documentis fed totheRNN,the
unitsassociatedwiththe document terms willbe activated.
The firsthidden layer is aLSTMunit which learns textual fea-
turesofaninputdocument.Itissuitedtolearnlong-termdependen-
cies,such asinthecaseofthelargeAPI referencedocumentation
text.Toprevent over-fitting, we applied a dropouttechnique to the
weightsmatrixandto the biasvector [ 34].
Theoutputofthe LSTMlayer(i.e.,asetoffeaturesthatcanbeas-
sociated with a knowledge type) goes through two fully-connected
dense layers. The dense layers provide deep representations of the
features extracted by the LSTMlayer and enable the network to
learn their hierarchical and compositional characteristics. To al-
leviate feature loss due to the projection of the features from a
high-dimensionalspacetothelow-dimensionalspaceoftheoutput
layer,themodelsmoothlyreducesthenumberofunitsinthedense
layers from 128 to 64 [ 11]. We use the ReLU activation function for
the denselayers to preventover-fitting [ 11].
Theoutputlayer providesthepredictedknowledgetypesusing
a sigmoid activation function. Hence, the number of units in the
outputlayeristhenumberoflabelsthatthemodellearns.Asthe
outputof thesigmoid isa probability valuebetween 0 and 1, each
neuron in this layer learns to estimate the probability of observing
one of the labels. To binarize the predicted probabilities we used
differentthresholds according to the differentmetrics.
We tuned the following network parameters. Epochis a com-
plete pass(back andforward)of everysample throughtheneural
network.Ascustomary,werun100epochs[ 11].Batchsize isthe
numberofsamplespassedthroughthenetworkatonce.Ascustom-
ary,weusedabatchsizeof32[ 11].Optimizer istheoptimization
methodminimizingtheprediction error.We useAdam, astate-of-
the-artalgorithmfortrainingRNN[ 35].LossFunction isthemeasure
of the network prediction error. We use sigmoidal cross-entropy as
it is efficient for text classification [ 36].Learning Rate controls the
adjustmentstotheweightswithrespecttothepredictionerror.We
usedacustomary learningrateof.001[ 11].
4 RESULTS
In this section, we compare the performance of RNN and tradi-
tional machine learning approaches for the classification of API
reference documentation. We contrast the performance of RNN
classifiers trained using different embeddings. Moreover, we assess
the classifiers generalizabilityto anothertest set.
4.1 Knowledge TypesIdentification
Individual knowledge types. We trained two RNN-based clas-
sifiers using a general-purpose corpus to create the embeddings
for theinput layerÐ RNNCCandRNNCCotf. Table4reports the
evaluation of our classifiers. RNN and traditional machine learningapproaches improveover thenaïvebaselines for alltheindividual
knowledge type classification by up to 74% (41% on average). SVM
alwaysperformedbetterthan k-NN.
Deeplearningclassifies Functionality ,Example,andEnvironment
with high precision and high recall at different probability thresh-
olds(AUPRC ≥80%)outperformingtraditionalmachinelearning
approaches. The RNNs yields subpar results for Directive,Purpose,
Reference,Concept, andControl(AUPRC<50%). However, the best
SVM outperforms the bestRNNonly for the latter twotypes.
The best classifiers for Quality,Structure,Patterns, andNon-
information yield an AUPRC between 59% and 78%. Also in this
case,thebestmachinelearningapproach(i.e.,SVM)outperforms
thebestRNNsclassifiersonlyforthelattertwotypes.Compared
to machine learning,RNNsclassifybettereightknowledge types.
Multipleknowledgetypes. Thesecondstepistoconsiderour
task as a multi-label classification problem rather than building in-
dividual classifiers for each knowledge type. We compare the RNN
classifierstothemulti-labeladaptationofthesametwomachine
learning models and two naïve baselines (see Table 5). ML-kNN
and OvRSVM perform worse than the baselines for the item-based
metrics,whereastheRNNsshowsthebestperformance.TheRNNs
outperformML- kNN,OvRSVM,andthebaselinesforlabel-based
metrics. There is an 11% improvement regarding the most strict
metric(i.e., SubsetAccuracy)betweenthebestRNNandmachine
learningclassifiers.RegardingMacroPrecision,MacroRecall,and
MacroF1, there is an improvement between 25% and 28% for the
RNN. MF1 performs better than traditional machine learning re-
garding MacroAUC,whichRNNsimproves by17%.
Answer to RQ1. One-third of the knowledge types can be
automatically identified with good results (i.e., AUPRC ≥
80%).RNNcanmoreaccurately(>10%)identifyeightofthe
12 knowledge type compared to traditional machine learn-
ingapproaches.Whenconsideringmulti-labelclassification,
RNNoutperformstraditionalmachinelearningapproaches
for item- andlabel-basedmetrics.
4.2 SoftwareDevelopment-Specific Corpus
Individualknowledgetypes. OneRNNusesfreely-available,pre-
trained embeddings based on a general purpose textual corpus (i.e.,
RNNCC), whereas RNNCCotfuses the same corpus but learns
missing words on-the-fly from the CaDOdataset. The assumption
behind text statistical representations such as GloVe is that the
meaningofadocumentisdeterminedbythemeaningofthewords
that appear in it [ 37]. Accordingly, RNNSOandRNNSOapiuse
corporainadomaincloser to the one of API documentation.
As shown in Table 4, the best among these RNNs performs sim-
ilarly to their general domain counterparts ( ∆AUPRC = 4%). For
Functionality ,Purpose,Control,andStructure thedifferencesaremin-
imal(1-2%).However,for QualityandEnvironment thereisasub-
stantialdecreaseinperformancewhenusingsoftwaredevelopment-
specificembeddings(10%and14%,respectively).Overall,theim-
provement is rather limited given the overhead in obtaining the
corpusandcomputing the embeddings.
114On UsingMachineLearning to IdentifyKnowledgein APIReference Documentation ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
Table4:Comparisonbetweendeeplearningclassifiers(trainedwithembeddingsfromgeneralpurposeandsoftwaredevelop-
ment corpora), traditional machine learning, and naïve approaches for classifying individual knowledge types in the CaDO
dataset. Values report theArea UnderPrecision-RecallCurve (AUPRC).
Naïvebaselines Traditionalapproaches Deeplearning (General Purpose) Deeplearning (Softwaredev.)
KnowledgeTypeMF1 MF2 RAND k-NN SVM RNNCC RNNCCotf RNNSO RNNSOapi
Functionality 0.69 0.73 0.72 0.76 0.39 0.86 0.84 0.87 0.87
Concept 0.110.140.120.13 0.57 0.25 0.28 0.28 0.28
Directive 0.26 0.16 0.17 0.22 0.04 0.40 0.41 0.41 0.45
Purpose 0.220.210.170.17 0.09 0.36 0.40 0.40 0.41
Quality 0.04 0.04 0.05 0.12 0.13 0.78 0.69 0.68 0.54
Control 0.080.120.090.08 0.81 0.28 0.32 0.30 0.30
Structure 0.37 0.37 0.35 0.38 0.42 0.61 0.56 0.63 0.60
Pattern 0.140.170.140.21 0.59 0.46 0.46 0.48 0.51
Example 0.24 0.23 0.20 0.25 0.60 0.90 0.85 0.90 0.90
Environment 0.040.030.060.16 0.43 0.68 0.80 0.66 0.51
Reference 0.11 0.14 0.16 0.13 0.15 0.35 0.35 0.41 0.30
Non-information 0.290.310.280.33 0.71 0.57 0.58 0.62 0.55
Table5:Comparisonbetweendeeplearningclassifiers(trainedwithembeddingsfromgeneralpurposeandsoftwaredevelop-
ment corpora),traditionalmachinelearning,andnaïve approachesforclassifying multiple knowledgetypesin CaDO.
Naïvebaselines Traditionalapproaches Deeplearning (General Purpose) Deeplearning (Softwaredev.)
MetricMF1 MF2 ML- kNN OvRSVM RNN CC RNNCCotf RNNSO RNNSOapi
HammingLoss 0.17 0.20 0.18 0.30 0.16 0.14 0.14 0.14
SubsetAccuracy 0.000.13 0.11 0.02 0.20 0.22 0.19 0.21
MacroPrecision 0.05 0.08 0.41 0.21 0.56 0.66 0.61 0.63
MacroRecall 0.160.16 0.24 0.27 0.55 0.39 0.30 0.33
MacroF1 0.10 0.10 0.27 0.24 0.55 0.44 0.40 0.43
MacroAUC 0.620.50 0.55 0.61 0.73 0.74 0.78 0.79
Multiple knowledge types. Table5shows that RNNCCand
RNNCCotfoutperform RNNSOandRNNSOapiwhenconsider-
inglabel-basedmetrics(exceptforMacroAUC)andperformsimi-
larly when considering item-basedmetrics. The RNN trained using
Javaand.NETStackOverflowpostshasthebestMacroAUC(79%).
Answer to RQ2. RNN using software development-specific
embeddings show slight to no improvement over RNN using
generalpurposeembeddingsforclassificationofindividual
knowledge types. When considering multi-label learning, ex-
ceptforMacroAUC,usinggeneralpurposeembeddingsyields
betterresults acrossitem- andlabel-basedmetrics.
4.3 ClassifiersGeneralizability
Individualknowledgetypes. Table6reportstheperformanceof
theindividualRNN-basedclassifiers on the Pythontest set. Also
inthissetting,nonaïvebaselineperformsbetterthantraditional
or deep learning approaches. The RNNs are the best classifiers for
seven knowledge types, whereas SVM shows the best results for
the remaining five. Consistently with the CaDOsetting, SVM is
the best classifier for Concept,Pattern, andNon-information . Classi-
fiersforFunctionality ,Concept,andPurposeshowsomeimproved
performance comparedto the CaDOsettings (∆AUPRC=8.3%).
Thereisalargeabsolutedifference( ∆AUPRC=33%)betweenthe
twosettingswhenconsidering Directive,Quality,Control,Structure,
Example, andEnvironment , suggesting that these knowledge typesare dependent on the settings. On average, the performance on the
Pythondataset decreaseby ∼16%over the 12 knowledge types.
Multipleknowledgetypes. Table7presenttheresultsofthe
multi-labelclassificationtask.Regardingitem-basedmetrics,our
classifiers performworse oron parwith respect to the naïve base-
lines. The classifiers show low precision (40% for the best clas-
sifier, SVM) and recall (26% for the best classifiers, RNNCCotf
andRNNSOapi). SVMalso achievesthe best F1(30%). RNNSOapi
showsthe bestperformance for MacroAUC(64%).
Answerto RQ3. Classifiersfor Functionality ,Concept,Pur-
pose,Pattern,andDirective seem to generalizefrom Javaand
.NET to Python documentation. The generalization for multi-
ple knowledge types classifiers islimited.
5 RELATED WORK
To the best of our knowledge, this is the first study, addressing the
automated identification of severalknowledge types within API
referencedocumentation.Inthissection,wereportrelatedwork
investigatingsomeoftheknowledgetypesindividually.Wepresent
studiescomparing traditionalmachine learningand deep learning
approachesfor textclassification insoftware engineering.
5.1 Knowledge Typesin API Documentation
Identifyingadocumentbasedontheknowledgetypesitcontains
cansupportdocumentationqualityassessmentandimprovement.
115ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia DavideFucci,Alireza Mollaalizadehbahnemiri, andWalidMaalej
Table 6: Comparison between deep learning classifiers (trained with embeddings from general purpose and domain specific
corpora), traditional machine learning, and naïve approaches for classifying API documents based on individual knowledge
type inthe Python dataset. Values report theArea UnderPrecision-RecallCurve (AUPRC).
Naïvebaselines Traditionalapproaches Deeplearning (General Purpose) Deeplearning (Softwaredev.)
KnowledgeTypeMF1 MF2 RAND k-NN SVM RNN CC RNNCCotf RNNSO RNNSOapi
Functionality 0.89 0.89 0.92 0.85 0.94 0.90 0.89 0.95 0.94
Concept 0.290.280.310.26 0.64 0.40 0.33 0.49 0.41
Directive 0.41 0.41 0.49 0.42 0.71 0.49 0.44 0.55 0.63
Purpose 0.280.280.250.30 0.13 0.46 0.40 0.51 0.39
Quality 0.17 0.17 0.19 0.17 0.27 0.20 0.17 0.20 0.32
Control 0.270.270.320.24 0.33 0.43 0.46 0.39 0.35
Structure 0.24 0.24 0.24 0.32 0.11 0.26 0.24 0.30 0.32
Pattern 0.220.220.240.29 0.61 0.50 0.30 0.41 0.43
Example 0.36 0.36 0.38 0.43 0.44 0.48 0.49 0.51 0.48
Environment 0.160.160.170.16 0.37 0.15 0.15 0.18 0.17
Reference 0.12 0.12 0.17 0.11 0.22 0.16 0.19 0.24 0.25
Non-information 0.230.230.240.27 0.61 0.30 0.39 0.30 0.28
Table7:Comparisonbetweendeeplearningclassifiers(trainedwithembeddingsfromgeneralpurposeandsoftwaredevelop-
ment corpora),traditionalmachinelearning,andnaïve approachesforclassifying multiple knowledgetypesin Python.
Naïve Traditionalapproaches Deeplearning (General Purpose) Deeplearning (Softwaredev.)
MetricMF1 MF2 ML kNN OvRSVM RNN CC RNNCCotf RNNSO RNNSOapi
HammingLoss 0.23 0.25 0.28 0.35 0.27 0.30 0.26 0.27
SubsetAccuracy 0.050.05 0.02 0.01 0.02 0.03 0.04 0.05
MacroPrecision 0.07 0.10 0.33 0.40 0.36 0.31 0.31 0.31
MacroRecall 0.080.16 0.24 0.24 0.24 0.26 0.21 0.26
MacroF1 0.07 0.13 0.28 0.30 0.29 0.28 0.25 0.28
MacroAUC 0.500.50 0.53 0.54 0.60 0.57 0.62 0.64
For example, Ding et al. [ 38] systematic review of 60 primary stud-
iesinvestigatesdocumentationqualityattributes.Theauthorsfocus
onknowledge-basedapproachesusedtoaddressqualityissuesof
API documentation. Although retrievability is reported as an es-
sential quality attribute, the authors show a lack of advanced ways
toretrievespecificinformationfromAPIdocumentation.Onthe
onehand,ourworkrepresentsafirststeptowardsdevelopingre-
trievalmechanismsfordocumentscontainingasetofknowledge
types from the Java and .NET API reference documentation. On
the other hand, the individual classifiers showinga performance
(e.g.,Functionality ,Control,Example,andEnvironment )canbeused
to retrieve documents containing a specific knowledge type. More-
over,ourclassifierscanbeusedtoretrievedocumentscontaining
Functionality from the Pythonstandardlibrary documentation.
Previousresearchtriedtoautomaticallyretrieveparticularknowl-
edge from API documentation. Robillard and Chhetri [ 5] presented
an approach to identify API-related information that developers
shouldnotignoreaswellasnon-criticalinformation.TheirapproachÐ
based on natural language analysis (i.e., part-of-speech tagging,
word patterns)Ðshows 90% precision and 69% recall when applied
to 1000 Java documentation units. However, the authors needed
to manually assess, on top of the sensible knowledge items, also
obvious,unsurprising,andpredictabledocumentationÐi.e.,what
we consider Non-information [5]. Our SVM classifier, trained using
simplefeatures, identifies Non-information with71%accuracy.Montperrus et al. [ 9] studied a particular knowledge type found
in API reference documentation, Directive. They analyzed more
than 4000 API documentation from open source libraries. To deter-
minethedocumentscontaining Directive,theydevelopedasetof
syntacticpatternsassociatedwithconcernsreportedinthedocu-
mentation.Finally,theymanuallycreatedataxonomyof23direc-
tives. Pandita et al. [ 39] proposed an NLP-based approach to verify
the legal usage of API methods against its description extracted au-
tomatically from the documentation. Their approach uses features
derivedfrompart-of-speechtaggingandchunkingtechniquesto
semantically analyze text. Moreover, using a domain dictionary,
the authors extracted methods specifications as first-order logic
expressionsto verifytheir legal usage inclientcode.
Conversely,inthiswork,weattemptedasimpleapproachbased
onlyonfeatureswhichcanbeautomaticallyextractedfromtheraw
text.Ourgoalwastocreateabenchmarkwhichcanbeimproved
by including, for example, natural language patterns specific for
eachknowledgetypesanddomain-specificmodels.Weshowthat
someclassifiers have already practical relevance.
5.2 Deep Learning in SoftwareEngineering
Xu et al. [ 40] use CNN to semantically link together knowledge
units from StackOverflow. Their approach focuses on predicting
severalclassesofrelatedness(e.g.,duplicate,relatedinformation).
The network input is the word2vec representation of 100,000 Java-
related posts from StackOverflow, whereas the dataset includes
116On UsingMachineLearning to IdentifyKnowledgein APIReference Documentation ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
8,000knowledgeunitsbalancedamongrelatednesstypes.TheCNN
outperformed machine learning baselinesÐi.e., SVM trained using
tf-idfandword2vec.However,FuandMenzies[ 28]replicatedXu
et al. study comparing their results to the same SVM baselines
optimized using hyper-parameter tuning. The authors showed im-
provedresultsforthebaselineswhichperformclosely(ifnotbetter)
to the CNN, although the latter required 84x more time to train. In
thiswork, wealsoused adeeplearningapproachwith asemantic
representationoftheinputbasedonStackOverflow.Wefoundthat,
forourtask,thereareonlyafewsmallimprovementsduetothe
software development-specific corpus, which may not be worth
whenconsidering theextraeffortrequiredtoobtainandtrainthe
embeddings.Wecomparedthedeeplearningapproachto(among
others) SVM models trained in line with the suggestions of Fu and
Menzies [ 28]. We showed that the approaches are complementary
as theirperformance depends onthe specific knowledge types.
Fakhoury et al. [ 41] applied deep learning and traditional ma-
chinelearningtothedetectionoflanguageanti-patternsinsoftware
artifacts (e.g., poor naming conventions) using a dataset of 1,700
elements collected from 13 large Java system. The authors showed
thatusingBayesianoptimizationandmodelselection,traditional
machinelearningmodelscanoutperformdeeplearningnotonly
in accuracybut also regarding the use of computationalresources.
Theyadviseresearchersandpractitionerstoexploretraditionalma-
chinelearningmodelswithhyper-parametertuningbeforeturning
to deep learning approaches. Our results for individual knowledge
typespartlysupportthisconclusion.However,whentacklingmulti-
label problems, our work shows that deep learning performs better
thantraditional machine learningfor allthe reportedmetrics.
6 DISCUSSION
In this section, we discuss the implications for practitioners and
researchers.Then, we present the limitationsofthis study.
6.1 Implications
Building automated knowledge extraction tools. Classifiers
showing good performance (AUPRC ≥80%) can already be used
in practice to tag documents containing crucial information for
developers. Moreover, these classifiers are trained using either tra-
ditionalmachinelearningalgorithms withsimple text features or
usingdeeplearningbutwithreadilyavailableembeddings.Adocu-
ment containing Functionality can answer developers’ information
needsregardingwhattheAPIdoes,whereas ControlandExample
addresshow toaccomplishataskusing theAPI.Theclassifier for
Functionality can be applied also to Python documentation. The
Environment classifierscanbeusedtogetinformationregarding
an API usage context. Classifiers for QualityandNon-information
showed encouraging results (AUPRC ≥70%). The former is rele-
vanttounderstandAPIperformance,whereasthelatterisuseful
for suggesting information that a developer can ignore. Moreover,
theNon-information classifier showed promising results generaliz-
ing to the Python documentation. Given its particular use case, we
suggestresearchtofocusonmaximizingrecalltoensurethat allun-
informative documents can be tagged appropriately. For theother
knowledgetypes,wesuggestmaximizingprecisiontoguarantee
that fundamental information iscorrectlytagged.Theresultsforotherknowledgetypescanbeimprovedbyadding
NLP-basedfeatures.Forexample, Structure usuallycontainsrefer-
encesto otherAPIelementsthat canbeidentifiedusingaspecific
named-entitytagger(e.g.,[ 42]).ConceptandPatternarestrongly
characterized by explanations of specific terms and sequence of
steps.Thesecanbeidentifiedthroughspecializedfeaturesbasedon
linguisticinquiry[ 43],suchas drives(e.g.,łdothistoachievethatž)
andtimeorientation (e.g.,łdothis,thendothatž).Astheseclassifiers
showed similar results when applied to the Python documentation,
theirimprovement can alsoincreasetheir generalizability.
The classifiers showed the worst results (AUPRC <50%) forDi-
rective,Purpose,andReference knowledgetypes.Thefirsttwocanbe
thesubjectoffurtherresearch.Inparticular,featuresfora Directive
classifiercanbeextractedfromMaalejandRobillardwork[ 3]as
well as from the specific taxonomy developed by Montperrus et
al.[9].Furthermore,previousworkonrationaleminingforother
softwareengineeringtasks(e.g.,[ 44,45])canbeadaptedtoimprove
the results for the Purposeknowledge type. The Reference classifier
showedsomeoftheweakestperformancebutitcanbeimproved
withsimplesyntactical featuresÐe.g.,the presenceof links.
Thereisavariationinperformancebetweentheclassifiercon-
figurations (e.g., traditional machine learning vs. deep learning)
and between the individual knowledge types. We hypothesize that
some knowledge typescan be sensitive tospecific keywords,such
as łcallback,ž łevent,ž and łtriggerž in the case of Control. On the
other hand, knowledge types such as Environment andExample
are characterized by a change in the language context. The former
tendstointerpolatetextwithnumbers(asitincludesinformation
such as version and copyright year), while the latter contains se-
quences that do not occur in natural language (i.e., source code).
We postulate that the RNN can capture this change of context.
However, the explanations for some classifiers results are more
subtle. For example, Non-information implies expressing in natu-
ral language informationalready provided by amethod signature.
This implies a mapping between source code tokens and natural
language ones which need to be further investigated. Similarly, the
Purposeknowledge type containsinformationÐi.e., the answerto a
łwhyž questionÐwhich can be difficult to identify, from a semantic
perspective,usingthesimpleconfigurationsofourclassifiers.Ar-
guably,theintrinsicdifficultytoidentifyaknowledgetype,even
for a human expert, can explain some of the poor results. For in-
stance,MaalejandRobillardreportlowagreementfor PurposeÐa
knowledge type showing subparresults (AUPRC=41%).
Another explanation for the different performancebetweentra-
ditional machine learning and deep learning can lay in the parame-
tersusedtotunethelatter.Asuggestedimprovementistocreate
12 binary RNNs (one for each knowledge type) and select differ-
ent parameters for a) the activation function of the output layer
(e.g., SoftMax [ 46]), b) the loss function (e.g., Categorical Cross-
Entropy [ 47]),andc)the optimizer(e.g.,RMSProporADA [ 48]).
Usingknowledgetypeswhendevelopingsoftware. Oneof
themainapplicationsfortheclassifierspresentedinthisstudyis
documentation filtering. API websites can offer their users the pos-
sibilitytosearchdocumentationbasedonspecificknowledgetypes
in addition to current options (e.g., by package or class). For exam-
ple,adeveloperfixingaspecificperformancebug(e.g.,relatedto
wireless connectivity)cansearchthe network APIdocumentation
117ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia DavideFucci,Alireza Mollaalizadehbahnemiri, andWalidMaalej
containingthe Qualityknowledgetype.Inthiscase,theclassifier
can be optimized for precisionÐi.e., the developer would consult a
smallnumberofdocumentswhicharelikelytocontaintheinfor-
mationneeded.Ontheotherhand,developersexploringpossible
usageofanewsetofAPIscanfilterthemaccordingto Functionality
which describes their capabilities. In this case, the classifier can
be optimized for recallÐi.e., the developer consults a substantial
amount of documents which give a complete overview of the API
functionality,evenif somemay beirrelevant.Ourbenchmark isa
startingpointforselectingtheclassifiertooptimizeaccordingto
specific scenarios. Classifiers with AUPRC ≥80% can be utilized
in the scenarios above, while the ones with AUPRC ≥50% need
furtheroptimization.Theproposedbenchmarkisalsoastepping
stone to support software developers filtering API documentation
based on multipleknowledge types of interest. Given the complex-
ity of such a task, our best classifier ( RNNSOapi) showed good
results (MacroAUC = 79%). However, when disregarding recallÐ
based on the assumption that a developer will not read a large
number of documentsÐthe classifier with the highest precision
(66%) is RNNCCotf. Conversely, when a developer can tolerate
noisyyet comprehensiveresults,werecommendusing RNNCCÐ
i.e.,theclassifierwiththebestrecall.Inbothcases,theclassifiers
relyonłaffordabležembeddings.
Using knowledge types when authoring documentation.
API documentation providers can leverage the results of this work
tomonitortheirproduct.Forexample,theycanusesimplemachine
learning models (e.g., SVM) to find documents containing Non-
information andremoveirrelevanttextthatincreasethedevelopers’
cognitiveeffort(e.g.,therepetitionofamethodsignatureintextual
form).Furthermore,theycanmonitorthepresenceofknowledge
types containing crucial information for software developers, such
asFunctionality ,Control,andExample.APIdocumentationprovider
canalsomonitorthedecreaseofimportantknowledgetypes(e.g.,
Functionality ) or increase of harmful ones (i.e., Non-information )
before releasing new version of an API and its documentation.
API defects can be diagnosed by identifying (and subsequently
improving)documentation containing Directive andQuality.
Furtherresearchoutlets. Researchersinvestigatingdocumen-
tationqualitycanbenefitfromtheresultsofourwork.Forexample,
qualitymodelscanbedevisedbasedonthepresence(orabsence)of
specific types. A firststepistheidentification of knowledgetypes
in a set of documents. In our benchmark, the RNNSOapimodel
showed good results (MacroAUC = 0.79). The classifier correctly
identifies documents containing a set of knowledge types with 60%
false positives rate when maximizing recall. Researchers should
also consider the trade-off between using a pre-trained embedding
while losing some performance (5%) in terms of MacroAUC. Given
theresultsobtainedonthePythonstandardlibrary,werecommend
researchers to be careful when applying our multi-label models to
differentAPIdocumentation.Researchershaveshowninterestin
studyinghowtheusageofparticularelementsinaframeworkis
documented (e.g., [ 5,9,10]). This line of research can benefit from
anapproachtoautomaticallyretrieveAPIreferencedocumentation
containing the Functionality knowledge type using the RNNSO
model,as itshowedgoodperformance ondifferenttest sets.6.2 Threatsto Validity
The API reference documentation used to train our classifiers is
basedontwolibraries,JDKand.NET.Whilethelanguageparadigms
aresimilar,theirdocumentationstylesare different [ 3].Moreover,
wedirectlyaddressedathreattogeneralizabilitybyinvestigating
the less structured documentation of the Python programming lan-
guage API [ 49]. We acknowledge that our results may not hold for
APIreferencedocumentationinotherdomains(e.g.,foraspecific
framework) orfor adifferentprogramming paradigm(e.g.,declar-
ativeprogramming).AlthoughMaalejandRobillardtaxonomyis
general enough [ 3], other knowledge types may exist. The labeling
ofournewtestsetcanintroduceathreattointernalvalidity.Tomit-
igate such threat, two raters independently labeled the documents
using validated guidelines [ 3]. We reconciled the disagreements
(approximately 50% were clear mistakes) by discussing borderline
cases and reaching consensus among the authors. Our benchmark
only includes two traditional machine learning algorithms, one
specific deep learning architecture, and four representations for
the RNN input layer. Nevertheless, there may be other algorithms,
embeddings,andconfigurationsworthofinvestigation.Theresults
can be biased due to the unbalancedness of the dataset. To reduce
thisthreat,weappliedcommonresamplingtechniquestothetrain-
ing set and reported the performance according to appropriate
metrics. We did not observe a correlation between the classifiers
performance andthe distributionof the labels.
7 CONCLUSION AND FUTUREWORK
In this paper, we built several classifiers, using traditional machine
learninganddeeplearningapproaches,toautomaticallyidentifythe
12 knowledge types in API documentation asproposed by Maalej
and Robillard [ 3]. We used Java and .NET manually-annotated API
documentation pages (n =5,574) as a dataset for training and test-
ing the classifiers. We showed good results (i.e., AUPRC ≥80%) for
one-third of the knowledge types. RNN identifies eight types more
accurately than traditional machine learning. When considering
multipleknowledgetypesatonce(i.e.,multi-labelclassification),
RNN outperforms traditional machine learning approaches. When
wordembeddings(i.e.,theRNNinputlayer)arecreatedfromStack-
Overflow posts, rather than from general-purpose text, there is
slighttonoimprovementinperformance( ∆AUPRC=4%).When
considering multiple labels, software development-specific embed-
dings yield better results for MacroAUC (79% vs. 74%). We applied
the classifiers to a new test set (n = 100) obtained from the Python
API documentation. Classifiers for Functionality ,Concept,Purpose,
Pattern,andDirective generalizetoPython.However,thegeneral-
izationofmulti-labelclassifiersislimited.Someoftheclassifiers
presented in this work can be already used by practitioners (e.g.,
developers, API providers) in different application scenarios. We
propose possible improvements to the classifiers based on features
specificforaknowledgetype.Basedonourbenchmark,weplanto
implementatool(e.g.,abrowserplugin)tofilterAPIdocumentation
basedonknowledge types andevaluate its usefulness.
ACKNOWLEDGMENTS
ThisresearchispartiallyfundedbytheGermanScienceFoundation
DFG(MA-6149/1-3).WearegratefultotheannotatorsofthePython
datasetandtoNicoleNovielliforthefeedbackonanearlyversion.
118On UsingMachineLearning to IdentifyKnowledgein APIReference Documentation ESEC/FSE ’19, August 26–30, 2019,Tallinn,Estonia
REFERENCES
[1]M.P.RobillardandR.DeLine,łAfieldstudyofAPIlearningobstacles,ž Empirical
SoftwareEngineering , vol. 16,no. 6,pp. 703ś732, 2010.
[2]U.DekelandJ.D.Herbsleb,łImprovingapidocumentationusabilitywithknowl-
edge pushing,ž in Proceedings of the 31st International Conference on Software
Engineering . IEEE Computer Society, 2009,pp. 320ś330.
[3]W.MaalejandM.P.Robillard,łPatternsofKnowledgeinAPIReferenceDocumen-
tation,žIEEETransactionson SoftwareEngineering , vol.39,no. 9,pp.1264ś1282,
2013.
[4]G. Petrosyan, M. P. Robillard, and R. De Mori, łDiscovering information explain-
ing api types using text classification,ž in Proceedings of the 37th International
Conference onSoftwareEngineering-Volume 1 . IEEE Press,2015,pp. 869ś879.
[5]M.P.RobillardandY.B.Chhetri,łRecommendingreferenceAPIdocumentation,ž
EmpiricalSoftwareEngineering , vol. 20,no. 6,pp. 1558ś1586,Jul. 2014.
[6]W. Maalej, R. Tiarks, T. Roehm, and R. Koschke, łOn the comprehension
of program comprehension,ž ACM Transactions on Software Engineering and
Methodology , vol. 23, no. 4, pp. 31:1ś31:37, Sep. 2014. [Online]. Available:
http://doi.acm.org/10.1145/2622669
[7]J. Stylos, B. Graf, D. K.Busse, C. Ziegler, R. Ehret, and J. Karstens,łA case study
of api redesign for improved usability,ž in Visual Languages and Human-Centric
Computing, 2008. VL/HCC2008. IEEE Symposiumon . IEEE,2008,pp. 189ś192.
[8]J.StylosandB.A.Myers,łTheimplicationsofmethodplacementonapilearn-
ability,ž in Proceedings of the 16th ACM SIGSOFT International Symposium on
Foundationsofsoftwareengineering . ACM,2008,pp. 105ś112.
[9]M.Monperrus,M.Eichberg,E.Tekes,andM.Mezini,łWhatshoulddevelopersbe
awareof?AnempiricalstudyonthedirectivesofAPIdocumentation,ž Empirical
SoftwareEngineering , vol. 17,no. 6,pp. 703ś737, 2011.
[10]M.A.Saied,H.Sahraoui,andB.Dufour,łAnobservationalstudyonapiusage
constraints and their documentation,ž in Software Analysis, Evolution and Reengi-
neering (SANER), 2015 IEEE 22nd International Conference on . IEEE, 2015, pp.
33ś42.
[11] L. Deng and D. Yu, łDeep learning: methods and applications,ž Foundations and
Trends®inSignal Processing , vol. 7,no. 3ś4, pp. 197ś387, 2014.
[12]S.HochreiterandJ.Schmidhuber,łLongshort-termmemory,ž Neuralcomputation ,
vol. 9,no. 8,pp. 1735ś1780,1997.
[13]Y. LeCun, Y. Bengio, and G. Hinton, łDeep learning,ž Nature, vol. 521, no. 7553, p.
436, 2015.
[14]T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, łDistributed rep-
resentations of words and phrases and their compositionality,ž in Advances in
neuralinformation processingsystems , 2013,pp. 3111ś3119.
[15]K. Boyd, K. H. Eng, and C. D. Page, łArea under the precision-recall curve: Point
estimates and confidence intervals,ž in Joint European Conference on Machine
Learning and KnowledgeDiscoveryinDatabases . Springer, 2013,pp. 451ś466.
[16]J. Huang and C. X. Ling, łUsing auc and accuracy in evaluating learning algo-
rithms,žIEEE Transactions onknowledgeand Data Engineering , vol. 17,no. 3,pp.
299ś310, 2005.
[17]M.SokolovaandG.Lapalme,łAsystematicanalysisofperformancemeasures
for classification tasks,ž Information Processing & Management , vol. 45, no. 4, pp.
427ś437, 2009.
[18]F. Charte, A. Rivera, M. J. del Jesus, and F. Herrera, łConcurrence among imbal-
ancedlabelsanditsinfluenceonmultilabelresamplingalgorithms,žin Interna-
tionalConferenceonHybridArtificialIntelligenceSystems . Springer,2014,pp.
110ś121.
[19]F. Herrera, F. Charte,A. J.Rivera, and M. J. DelJesus, łMultilabel classification,ž
inMultilabelClassification . Springer, 2016,pp. 17ś31.
[20]J. Pennington, R. Socher, and C. D. Manning, łGlove: Global vectors for word
representation.žin EMNLP, vol. 14,2014,pp. 1532ś1543.
[21]G.Halawi,G.Dror,E.Gabrilovich,andY.Koren,łLarge-scalelearningofword
relatedness with constraints,ž in KDD. New York, NY, USA: ACM, 2012, pp.
1406ś1414.[Online].Available: http://doi.acm.org/10.1145/2339530.2339751
[22]T.Joachims,łTextcategorization withsupportvector machines:Learningwith
many relevant features,ž in European conference on machine learning . Springer,
1998,pp. 137ś142.
[23]K.Beyer,J.Goldstein,R.Ramakrishnan,andU.Shaft,łWhenisłnearestneighborž
meaningful?ž in International conference on database theory . Springer, 1999, pp.
217ś235.
[24]C. A. Cois and R. Kazman, łNatural language processing to quantify security
effort in the softwaredevelopmentlifecycle.žin SEKE, 2015,pp. 716ś721.
[25]A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, łOn the naturalness of
software,žin 201234thInternationalConferenceonSoftwareEngineering(ICSE) .IEEE,2012,pp. 837ś847.
[26]T.Joachims, łTraining linear svms in linear time,ž in Proceedings ofthe 12th ACM
SIGKDDinternationalconferenceonKnowledgediscoveryanddatamining . ACM,
2006,pp. 217ś226.
[27]F.Calefato,F.Lanubile,F.Maiorano,andN.Novielli,łSentimentpolaritydetection
forsoftwaredevelopment,ž EmpiricalSoftwareEngineering ,vol.23,no.3,pp.1352ś
1382,2018.
[28]W. Fu and T. Menzies, łEasy over hard: A case study on deep learning,ž in
Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering .
ACM,2017,pp. 49ś60.
[29]Y. Liu and Y. F. Zheng, łOne-against-all multi-class svm classification using
reliability measures,žin Proceedings.2005IEEEInternationalJointConference on
Neural Networks,2005. , vol. 2. IEEE,2005,pp. 849ś854.
[30]J. Bergstra and Y. Bengio, łRandom search for hyper-parameter optimization,ž
Journal ofMachineLearning Research , vol. 13,no. Feb, pp. 281ś305, 2012.
[31]T. M. Cover, P. E. Hart et al., łNearest neighbor pattern classification,ž IEEE
transactions oninformation theory , vol. 13,no. 1,pp. 21ś27, 1967.
[32]M.-L.ZhangandZ.-H.Zhou,łMl-knn:Alazylearningapproachtomulti-label
learning,ž Patternrecognition , vol. 40,no. 7,pp. 2038ś2048,2007.
[33]J. PattersonandA.Gibson, DeepLearning: APractitioner’s Approach . O’Reilly
Media,2017.
[34]N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
łDropout: a simple way to prevent neural networks from overfitting.ž Journal
of Machine Learning Research , vol. 15, no. 1, pp. 1929ś1958, 2014. [Online].
Available: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
[35]S. Ruder, łAn overview of gradient descent optimization algorithms,ž arXiv
preprint arXiv:1609.04747 , 2016.
[36]J.Nam,J.Kim,E.L.Mencía,I.Gurevych,andJ.Fürnkranz,łLarge-scalemulti-
label text classificationÐrevisiting neural networks,ž in Joint european conference
onmachinelearningandknowledgediscoveryindatabases . Springer,2014,pp.
437ś452.
[37]G. A. Miller and W. G. Charles, łContextual correlates of semantic similarity,ž
Language and cognitiveprocesses , vol. 6,no. 1,pp. 1ś28,1991.
[38]W. Ding, P. Liang, A. Tang, and H. Van Vliet, łKnowledge-based approaches
in software documentation: A systematic literature review,ž Information and
SoftwareTechnology , vol. 56,no. 6,pp. 545ś567, 2014.
[39]R.Pandita,X.Xiao,H.Zhong,T.Xie,S.Oney,andA.Paradkar,łInferringmethod
specifications from natural language api descriptions,ž in Proceedings of the 34th
InternationalConferenceonSoftwareEngineering . IEEEPress,2012,pp.815ś825.
[40]B.Xu,D.Ye,Z.Xing,X.Xia,G.Chen,andS.Li,łPredictingsemanticallylinkable
knowledge in developer online forums via convolutional neural network,ž in
Proceedingsofthe31stIEEE/ACMInternationalConferenceonAutomatedSoftware
Engineering . ACM,2016,pp. 51ś62.
[41]S. Fakhoury, V. Arnaoudova, C. Noiseux, F. Khomh, and G. Antoniol, łKeep
it simple: Is deep learning good for linguistic smell detection?ž in 2018 IEEE
25thInternationalConferenceonSoftwareAnalysis,EvolutionandReengineering
(SANER). IEEE,2018,pp. 602ś611.
[42]M.Mäntylä,F.Calefato,andM.Claes,łNaturallanguageornot(nlon)-apackage
for software engineering text analysis pipeline,ž in 2018 IEEE/ACM 15th Inter-
national Conference on Mining Software Repositories (MSR) . IEEE, 2018, pp.
387ś391.
[43]Y. R. Tausczik and J. W. Pennebaker, łThe psychological meaning of words:
Liwc and computerized text analysis methods,ž Journal of language and social
psychology , vol. 29,no. 1,pp. 24ś54, 2010.
[44]Z. Kurtanović and W. Maalej, łOn user rationale in software engineering,ž Re-
quirements Engineering , vol. 23,no. 3,pp. 357ś379, 2018.
[45]B.Rogers,J.Gung,Y.Qiao,andJ.E.Burge,łExploringtechniquesforrationale
extraction from existing documents,ž in 2012 34th international conference on
softwareengineering (ICSE) . IEEE,2012,pp. 1313ś1316.
[46]G.E.HintonandR.R.Salakhutdinov,łReplicatedsoftmax:anundirectedtopic
model,žin Advancesinneuralinformationprocessingsystems ,2009,pp.1607ś1614.
[47]Z. Zhang and M. Sabuncu, łGeneralized cross entropy loss for training deep
neural networks with noisy labels,ž in Advances in Neural Information Processing
Systems, 2018,pp. 8792ś8802.
[48]M. C. Mukkamala and M. Hein, łVariants of rmsprop and adagrad with logarith-
micregretbounds,žin Proceedingsofthe34thInternationalConferenceonMachine
Learning-Volume 70 . JMLR.org, 2017,pp. 2545ś2553.
[49]B.DagenaisandM.P.Robillard,łCreatingandevolvingdeveloperdocumentation:
understandingthedecisionsofopensourcecontributors,žin Proceedingsofthe
eighteenth ACM SIGSOFT international symposium on Foundations of software
engineering . ACM,2010,pp. 127ś136.
119