Compiler Bug Isolation via E￿ective Witness Test Program
Generation∗
Junjie Chen†
College of Intelligence and
Computing, Tianjin
University, China
junjiechen9208@gmail.comJiaqi Han
Peiyi Sun
HCST (Peking University),
MoE, China
{hanjiaqi,spy}@pku.edu.cnLingming Zhang
University of Texas at
Dallas, USA
zhanglm10@gmail.comDan Hao‡
Lu Zhang
HCST (Peking University),
MoE, China
{haodan,zhanglucs}@pku.edu.cn
ABSTRACT
Compiler bugs are extremely harmful, but are notoriously di￿cult
to debug because compiler bugs usually produce few debugging
information. Given a bug-triggering test program for a compiler,
hundreds of compiler ￿les are usually involved during compila-
tion, and thus are suspect buggy ￿les. Although there are lots of
automated bug isolation techniques, they are not applicable to com-
pilers due to the scalability or e￿ectiveness problem. To solve this
problem, in this paper, we transform the compiler bug isolation
problem into a search problem, i.e., searching for a set of e￿ective
witness test programs that are able to eliminate innocent compiler
￿les from suspects. Based on this intuition, we propose an auto-
mated compiler bug isolation technique, DiWi, which (1) proposes
a heuristic-based search strategy to generate such a set of e￿ective
witness test programs via applying our designed witnessing muta-
tion rules to the given failing test program, and (2) compares their
coverage to isolate bugs following the practice of spectrum-based
bug isolation. The experimental results on 90 real bugs from popu-
lar GCC and LLVM compilers show that DiWi e￿ectively isolates
66.67%/78.89% bugs within Top-10/Top-20 compiler ￿les, signi￿-
cantly outperforming state-of-the-art bug isolation techniques.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging .
KEYWORDS
Compiler Debugging, Bug Isolation, Test Program Generation
∗This work is partially supported by the National Key Research and Development
Program of China under Grant No. 2017YFB1001803, the National Natural Science
Foundation of China under Grant Nos. 61872008, 61672047, and 61861130363; it is also
partially supported by NSF grants CCF-1566589, CCF-1763906, and Amazon.
†This work was done when he was in Peking University.
‡Corresponding author. HCST is short for Key Lab of High Con￿dence Software
Technologies.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro￿t or commercial advantage and that copies bear this notice and the full citation
on the ￿rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speci￿c permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
©2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-5572-8/19/08. . . $15.00
https://doi.org/10.1145/3338906.3338957ACM Reference Format:
Junjie Chen, Jiaqi Han, Peiyi Sun, Lingming Zhang, Dan Hao, and Lu Zhang.
2019. Compiler Bug Isolation via E￿ective Witness Test Program Genera-
tion. In Proceedings of the 27th ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ES-
EC/FSE ’19), August 26–30, 2019, Tallinn, Estonia. ACM, New York, NY, USA,
12pages. https://doi.org/10.1145/3338906.3338957
1 INTRODUCTION
Software bugs in modern software systems can incur huge cost. For
example, Tricentis.com studied software bugs in 363 companies
all over the world, and reported that these bugs incurred almost
$1.1 Trillion cost, and a￿ected over 4.4 Billion customers in 2016.
Among software bugs, compiler bugs are especially critical, since
almost all software systems are compiled via compilers and a buggy
compiler can potentially a￿ect all the software systems built on it.
Therefore, it is crucial to detect, isolate, and ￿x compiler bugs.
Although researchers have devoted dedicated e￿orts to com-
piler testing [ 15,24,29,43,53,58,82], compiler bug isolation and
￿xing are still a tedious and time-consuming process, since com-
pilers are very complex and developers have to understand the
root cause of a compiler bug and then determine the ￿xing strat-
egy. In the literature, lots of automated bug isolation techniques
(also known as fault localization, e.g., spectrum-based techniques
or SBFL [ 22,38,52,72,77], slicing-based techniques [ 81], mutation-
based techniques [ 35,51,60,66,67,87]) have been proposed. How-
ever, these techniques can hardly be applicable to compilers due
to their scalability or e￿ectiveness problem. First, compilers like
GCC are complex and large, making it extremely expensive to per-
form advanced static/dynamic analysis on a compiler. Therefore,
program-analysis based techniques like slicing-based techniques
and mutation-based techniques can hardly be used in compiler bug
isolation. For example, we found that it can take over a month to
execute only a limited set of GCC compiler mutants. Second, due to
the complexity of compilers, the execution traces between passing
and failing test programs tend to di￿er signi￿cantly. Therefore, it is
hard for SBFL, which isolates compiler bugs by comparing coverage
between passing and failing test programs, to isolate compiler bugs
e￿ectively, which is also demonstrated in Sections 2and5. In other
words, automated compiler bug isolation is still challenging due to
its inherent di￿culty.
Given a bug-triggering test program (also called failing test pro-
gram) for a compiler, developers need to manually isolate buggy
compiler ￿les (i.e., source ￿les of compilers) among all the touched
compiler ￿les when compiling the given failing test program. How-
ever, there are usually at least hundreds of compiler ￿les involved in
223
ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Junjie Chen, Jiaqi Han, Peiyi Sun, Lingming Zhang, Dan Hao, and Lu Zhang
the compilation. All these ￿les are suspects. SBFL has demonstrated
that passing tests can be regarded as witnesses to help reduce the
suspicion of innocent program elements [ 7]. In this paper, we call
passing test programs that are able to eliminate innocent compiler
￿les from suspects witness test programs . However, as demonstrated
in Sections 2and5, the widely-used developer-provided passing
test programs are not e￿ective witnesses, since they can hardly
isolate compiler bugs e￿ectively. Therefore, in this paper, we aim
to ￿nd a set of e￿ective witness test programs. With the set of
e￿ective witness test programs and the given failing test program,
developers can automatically isolate buggy compiler ￿les precisely.
With this intention, we transform the problem of automated com-
piler bug isolation to a search problem, i.e., ￿nding a set of e￿ective
witness test programs to help isolate compiler bugs precisely.
However, it is challenging to ￿nd such a set of e￿ective witness
test programs. On the one hand, each witness test program in the
set is required to have a large witness capability, i.e, it can elimi-
nate innocent compiler ￿les as many as possible from suspects. On
the other hand, the witness capabilities of di￿erent witness test
programs in the set are required to be diverse, i.e., each of them
can eliminate di￿erent innocent compiler ￿les from suspects, so
that grouping them can be helpful to isolate the buggy compiler
￿les precisely. To solve this problem, in this paper, we propose
the ￿rst compiler bug isolation technique via searching for diver-
si￿ed witnesses, called DiWi (Diversi￿ed Witnesses). The main
contribution of DiWi lies in how to ￿nd the set of e￿ective witness
test programs. To address the ￿rst challenge, we design a series
of mutation rules for DiWi to generate witness test programs via
slightly mutating the given failing test program. The key insight
is that such minor changes are more likely to make the generated
witness test program share a close compiler execution trace with
the failing test program, so that the generated witness test program
can eliminate more innocent compiler ￿les from suspects. To ad-
dress the second challenge, DiWi utilizes some heuristics to guide
the construction of the set of e￿ective witness test programs. More
speci￿cally, during the generation of each witness test program,
DiWi considers the diversity of compiler execution traces between
it and the already generated witness test programs, so that these
generated witness test programs can eliminate di￿erent innocent
compiler ￿les from suspects. Finally, DiWi ranks the ￿les executed
by the failing test program based on coverage comparison between
the set of e￿ective witness test programs and the given failing test
program like SBFL [ 74].
To evaluate the e￿ectiveness of DiWi, we constructed an exten-
sive dataset including 90 real-world reproducible bugs from the
popular GCC [ 2] and LLVM [ 5] compilers. The experimental re-
sults on all the 90 studied bugs show that DiWi e￿ectively isolates
10/37/60/71 bugs (out of 90) within Top-1/5/10/20 buggy ￿les, sig-
ni￿cantly outperforming traditional SBFL. As the core of DiWi, we
investigated the contribution of our search-based witness test pro-
grams compared with the developer-provided test suite shipping
with the buggy compiler (which includes test programs passing
on this compiler) and the randomly generated passing test pro-
grams via Csmith [ 82] (the most widely-used random C program
generation tool). The results show that our search-based witness
test programs signi￿cantly outperform the latter two, e.g., isolating
150.00% and 66.67% more bugs than the developer-provided passingstruct S1
{
intf0;
intf1;
intf2;
} a;
struct S1 b;
intc = 1;
intfn1 () {
if(!c)
return 0;
b = a;
return 0;
}
intmain () {
struct S1 d = {0,1,0};
a = d;
a.f0 = d.f2;
fn1 ();
a = d;
if(b.f1 != 1)
__builtin_abort ();
return 0;
}
(a) Failingstruct S1
{
intf0;
intf1;
intf2;
} a;
struct S1 b;
intc = 1;
intfn1 () {
if(!c)
return 0;
b = a;
return 0;
}
intmain () {
struct S1 d = {0,1,0};
a = d;
a.f0 = d.f2;
fn1 ();
a= a ;
if(b.f1 != 1)
__builtin_abort ();
return 0;
}
(b) Passing 1struct S1
{
intf0;
intf1;
intf2;
} a;
struct S1 b;
intc = 1;
intfn1 () {
if(!c)
return 0;
b = a;
return 0;
}
intmain () {
struct S1 d = {0,1,0};
a = d;
a.f0 = d.f2;
fn1 ();
a = d;
if( a.f1 != 1)
__builtin_abort ();
return 0;
}
(c) Passing 2
Figure 1: LLVM Bug 24482
test programs and randomly generated passing test programs at
the Top-1 position. We also investigated the contribution of our
heuristic-based search strategy compared with the random search
strategy during generation, and explored the synthesis of DiWi and
developer-provided test suites. In summary, this paper makes the
following contributions:
•Idea. An automated compiler bug isolation technique that
transforms the problem of bug isolation to the problem of
guided test program generation via search&mutation.
•Implementation. A practical open-source tool implement-
ing the proposed compiler bug isolation technique based on
the LLVM Clang infrastructure.
•Benchmark. An open-source dataset containing 90 repro-
ducible real bugs from GCC and LLVM compilers for future
research on compiler bug detection, isolation, and ￿xing.
•Study. An extensive study on all 90 bugs from our dataset
demonstrating that the proposed technique is able to sig-
ni￿cantly outperform existing techniques; our search-based
witness test programs (the core of DiWi) have higher quality
than the developer-provided test suite and the randomly
generated passing test programs for compiler bug isolation;
lastly, components from developer-provided test suites can
further boost DiWi e￿ectiveness.
2 MOTIVATION
Here, we use an example to illustrate the motivation of this paper.
Figure 1shows an example from LLVM bug ID 24482, where the left
is the reported bug-triggering test program. When the buggy com-
piler (revision 245195 in LLVM trunk) compiles the failing program,
it produces di￿erent outputs under the compilation options “ -O1”
and “ -Os”. The bug occurs at the ￿le “ DeadStoreElimination.cpp ”
due to dead store elimination across basic blocks.
It is tedious for developers to manually ￿nd the buggy ￿le since
this revision contains a large number of ￿les (i.e., 3,581). Although
traditional SBFL is reported to be e￿ective in other software sys-
tems [ 68], it may not be e￿ective in compilers due to their character-
istics. We applied Ochiai [ 7] (one of the most e￿ective formulae in
SBFL) based on the failing test program and the developer-provided
224Compiler Bug Isolation via E￿ective Witness Test Program Generation ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
test suite, and found the buggy ￿le is ranked at the 659 thposition
out of 3,581 ￿les. That is, with traditional SBFL developers need to
examine 658 innocent compiler ￿les before ￿nding the buggy one,
indicating the inferior e￿ectiveness of traditional SBFL in compilers.
To isolate compiler bugs e￿ectively following SBFL, instead of
the developer-provided passing test programs, a set of e￿ective wit-
ness test programs are desirable. Intuitively, witness test programs
sharing similar execution traces (except the buggy ￿le) with the
given failing test program are more helpful to eliminate innocent
￿les from suspects. With this intuition, we generated 20 witness
test programs by randomly introducing various minor changes into
the failing test program, making them share similar execution trace
with the failing one. Then we compared their coverage to isolate the
bug like SBFL [ 74]. In this way, the buggy ￿le is ranked at the 5 th
position, demonstrating that these generated witness test programs
by slightly changing the failing test program can help isolate the
bug e￿ectively.
We further analyzed the isolation result using each witness test
program and the given failing test program (namely a pair of test
programs) to learn the performance of each individual witness
test program. Among the 20 pairs, 1 pair achieves the optimal
result (i.e., Top-1), 13 pairs rank the buggy ￿le within the ￿rst 10
positions, and 7 pairs rank the buggy ￿le after the 30 thpositions.
Overall, there are many e￿ective pairs and a few low-quality pairs,
by changing the failing test program minorly. For example, Figure 1b
shows the optimal witness test program that ranks the buggy ￿le
at the 1 stposition, which is generated by changing one variable
of the failing test program in Figure 1a; while Figure 1cshows a
low-quality witness test program that ranks the buggy ￿le at the
33rdposition, which is generated by replacing “ b.f1” with “ a.f1”.
Since we use coverage comparison between test programs to isolate
compiler bugs, a witness test program eliminating more innocent
￿les from suspects tends to mean that it shares a more similar
compiler execution trace with the failing test program, and also
means that it has higher quality.
Based on the above analysis, we have the following observations:
First, a set of high-quality witness test programs can help isolate
compiler bugs with the given failing test program; Second, even if
we change the failing test program minorly, the obtained witness
test programs are of various quality.
3 RELATED WORK
We discuss the most closely related work to compiler bug isolation.
Automated Debugging. In the literature, there are a huge amount
of work on automated debugging, e.g., fault localization [ 27,45–
47,55,56,80,85] and program repair [ 21,28,48,59,62,65], where
our work targets the former. As presented in Section 1, the existing
fault localization techniques cannot work well on compilers.
In the literature there also exists mutation-based fault localiza-
tion [ 50,51,60,66,87], which aims to mutate source programs to
check the impact of each code element on the test outcomes. How-
ever, our work is to introduce the idea of mutation to slightly change
test programs so as to generate a set of e￿ective witness test pro-
grams. Besides, some work focuses on improving fault localization
via test generation [ 8,10,11,54,71] or test selection [ 25,26,61,70]
for ordinary programs. However, compiler inputs are programs and
compilers are extremely complex and huge, making none of theseexisting test-generation and test-selection techniques for better
debugging directly applicable here.
In automated debugging, our work is mostly related to com-
piler debugging . Most compiler debugging work focused on pro-
viding debugging messages/visualization [ 13,31,41,64,73]. Some
work focused on reducing bug-triggering tests to facilitate debug-
ging [ 12,32,69,76,84]. In our work, the provided bug-triggering
tests are already the reduced ones, as required by compiler devel-
opers. Besides, Chen et al. [ 19] proposed a technique to rank test
programs triggering bugs such that test programs triggering distinct
bugs are early in the list. Holmes and Groce [ 34] further proposed a
new metric to determine similarity of failing test programs to facil-
itate debugging. In contrast, our work aims to automatically isolate
compiler bugs via e￿ective witness test program generation. In par-
ticular, Zeller [ 83] proposed to produce an entire cause-e￿ect chain
in GCC from input to result for facilitating compiler debugging.
Actually, cause-e￿ect chain and our technique are complementary:
1) the former produces fault-diagnosis information at the program-
state level while the latter does this at the source-code level, 2) the
former manipulates in memory and may not handle external states,
3) the latter is more lightweight.
Mutation Testing. Mutation testing is one of the most e￿ective
methods to measure test-suite quality [ 36,39,86]. It deliberately
seeds bugs into the original source program to simulate the bugs
that developers often make in practice. Di￿erent from it, our work
aims to conduct test program mutation to generate a set of e￿ective
witness test programs for facilitating compiler bug isolation. Here,
we both apply existing mutation rules from mutation testing and
also design new mutation rules for compiler bug isolation.
Compiler Testing. Compiler testing usually happens before com-
piler bug isolation (the target of our work). Most research on
compiler testing focuses on test program generation [ 20,82], test
oracle construction [ 43,57], and test execution acceleration [ 14–
16,18]. The general idea of mutation is also applied to compiler
testing [ 20,33,42,44], which aims to generate di￿erent test pro-
grams as much as possible by faster diverging from the given seed
program fordetecting deep compiler bugs . Di￿erent from them, our
work aims to utilize mutation for compiler bug isolation , and the
design goal of our mutation is to ￿ip the compiler execution results
(i.e., from failing topassing ) to generate a set of witness test
programs close to a given failing test program .
To sum up, the existing fault localization techniques cannot work
well for large-scale compiler systems, while the existing applica-
tions of mutation cannot be directly used to solve the problem of
compiler bug isolation. Therefore, this work makes the ￿rst attempt
to isolate compiler bugs via search-based mutation.
4 APPROACH
Following the observations in Section 2, we propose a novel tech-
nique, named DiWi (Diversi￿ed Witnesses), to isolate compiler
bugs. In DiWi, we ￿rst deliberately generate a set of e￿ective wit-
ness test programs, and then compare them with the given failing
test program like SBFL to isolate compiler bugs [ 74]. From Section 2,
we know that it is hard to ensure each generated witness test pro-
gram to be e￿ective, and thus we use an aggregation mechanism to
minimize the impact of low-quality witness test programs in DiWi.
Therefore, there are two key issues to the success of DiWi. First,
225ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Junjie Chen, Jiaqi Han, Peiyi Sun, Lingming Zhang, Dan Hao, and Lu Zhang
a::=x|n|opua|a1opaa2
b::=true |false |notb|
b1oplb2|a1opra2
S::=x:=a|S1;S2|
while (b)doS|
if(b)then S1elseS2
Figure 2: Syntax rules for the WHILE language
we need to carefully construct each witness test program to avoid
introducing many low-quality witness test programs; otherwise,
we have too much noise. Second, the witness test programs should
evenly eliminate buggy suspects; otherwise, the aggregation pro-
cess is prone to biases. To help construct such a set of e￿ective
witness test programs, we de￿ne the following two criteria.
C1:Each test program in the set of e￿ective witness test pro-
grams should share a similar compiler execution trace with
the failing test program.
C2:Test programs in the set of e￿ective witness test programs
should have great diversity in their compiler execution traces.
However, since the space of witness test programs is extremely
huge (in fact in￿nite to be precise), e￿ciently generating such a
set of e￿ective witness test programs satisfying the two criteria
is challenging. To satisfy the ￿rst criterion, we design a series
of mutation rules for DiWi to generate witness test programs by
slightly mutating the failing test program. Intuitively, such minor
changes are likely to make the generated witness test program
share a close compiler execution trace with the failing test program.
Here, we call our mutation witnessing mutation , aiming to generate
witness test programs with a large witness capability. To satisfy
the second criterion, DiWi utilizes some heuristics to guide the
construction of the set of e￿ective witness test programs. That
is, during the generation of a new witness test program, DiWi
considers the diversity of compiler execution traces between the
new one and the already generated witness test programs, aiming
to make their witness capabilities diversi￿ed .
In the following, we introduce the designed witnessing mutation
in Section 4.1, heuristic-based witness test program construction
in Section 4.2, and the aggregation mechanism for compiler bug
isolation in Section 4.3.
4.1 Witnessing Mutation
As presented in Section 3, the existing applications of mutation
cannot be directly used for compiler bug isolation. In our context,
the goal of test program mutation is to introduce slightly di￿erent
control- and data-￿ow information to ￿ip the compiler execution
results (i.e., from failing topassing ) to generate witness test
programs. Therefore, we design a series of mutation rules speci￿c
to our mutation goal.
Mutation Rules. To achieve the mutation goal of ￿ipping the fail-
ing test program, we manually analyzed historical compiler bugs
to investigate why these test programs are able to trigger bugs
and which characteristics of test programs can sensitively impact
compiler execution results. Also, to make the changes minor, we
design our mutation, based on the learned knowledge, at the level of
￿ne-grained program elements, including variables ,operators , and
constants . Therefore, we propose three mutation categories: variable
mutation ,operator mutation , and constant mutation . For variablemutation , each program variable can potentially be changed into
another compatible variable or type, since program variables are
the core of data dependencies and variable types can impact many
compiler optimizations. For operator mutation , each program opera-
tor (i.e., arithmetic, logical, relational, and unary) can potentially be
changed into another compatible operator, since program operators
can signi￿cantly impact data- and control-dependencies. For con-
stant mutation , each program constant can potentially be changed
into another value, since many compiler bugs are triggered under
some speci￿c values. Here, we both apply existing mutation rules
from traditional mutation testing [ 39] and design new mutation
rules for compiler bug isolation (e.g., variable type mutation).
More formally, following the presentation of prior work [ 89]w e
view a test program as a syntactic skeletal structure Pwith “holes”
that can be con￿gured with various variables (denoted as hole ~ ),
operators (denoted as hole ~o), and constant values (denoted as
hole ~c). In this way, we can ￿ll each hole with mutated content
to derive a new mutated program. For the ease of presentation, let
us consider a WHILE-style language that has been widely used in
the program analysis research [ 63,89]. The program syntax rules
for the WHILE-style language are shown in Figure 2. In the rules,
non-terminals a,b, and Sdenote arithmetic expressions, boolean ex-
pressions, and program statements, respectively; terminals opa,opl,
opr, and opudenote arithmetic, logical, relational, and unary opera-
tors, respectively; terminals xandndenote program variables and
constants. Note that we use the WHILE-style language for the ease
of presentation, and our technique applies to a full-￿edged language
such as C. To obtain program mutations, we recursively apply a mu-
tation transformation to the WHILE syntax rules. Figure 3presents
the syntax rules for all our three mutation categories. We denote
all the three categories of transformations and program holes as JK
and~, respectively, i.e., JK=JK [JKo[JKcand~=~ [~o[~c. In
this way, we formally de￿ne our mutation process.
D￿￿￿￿￿￿￿￿￿ 1 (M￿￿￿￿￿￿￿ S￿￿￿￿￿￿￿). Given any test program
P, we say Pis amutation skeleton ofPi￿ the abstract syntax tree
(denoted as TP) ofPis the same as the transformed abstract syntax
tree (denoted as JTPK) ofP, i.e., TP=JTPK.
D￿￿￿￿￿￿￿￿￿ 2 (F￿￿￿￿￿￿￿￿￿￿ M￿￿￿￿￿￿￿). Given a test program P
and its mutation skeleton P, for all the nholes (e.g., { ~1,~2,...,~n})
within P, ￿rst-order mutation ￿lls each hole with the same content as
Pexcept ￿lling one hole with a mutated content.
D￿￿￿￿￿￿￿￿￿ 3 (H￿￿￿￿￿￿￿￿￿ M￿￿￿￿￿￿￿). Given a test program P
and its mutation skeleton P, for all the nholes (e.g., { ~1,~2,...,~n})
within P, high-order mutation ￿lls i(2in) holes with mutated
contents, while ￿lling the rest holes with the same content as P.
Since we need diverse execution traces representing di￿erent
ways to ￿ip the failing test program for e￿ective compiler bug isola-
tion, for each mutation category we design a plurality of mutation
rules. The detailed mutation rules for each mutation category are
shown in Table 1. Based on the three types of holes, we design 132
speci￿c mutation rules in total and the full list can be found in the
project webpage. Here we regard each speci￿c mutation operation
on one type of program holes as an individual mutation rule, e.g,
replacing “ x=0” with “ y=0” and replacing “ x=0” with “ x=1” are two
di￿erent rules, while replacing “ x=0with “ y=0” and replacing “ x=0”
with “ z=0” belong to the same rule.
226Compiler Bug Isolation via E￿ective Witness Test Program Generation ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
JaK ::=~ |n|opuJaK |Ja1K opaJa2K 
JbK ::=true |false |notb|
Jb1K oplJb2K |Ja1K oprJa2K 
JSK ::=~ :=JaK |JS1K ;JS2K |
while (JbK )doJSK |
if(JbK )then JS1K elseJS2K 
(a) Syntax rules for variable mutationsJaKo::=x|n|~oJaKo|Ja1K ~oJa2Ko
JbKo::=true |false |notb|
Jb1Ko~oJb2Ko|Ja1Ko~oJa2Ko
JSKo::=x:=JaKo|JS1Ko;JS2Ko|
while (JbKo)doJSKo|
if(JbKo)then JS1KoelseJS2Ko
(b) Syntax rules for operator mutationsJaKc::=x|~c|opuJaKc|Ja1KcopaJa2Kc
JbKc::=true |false |notb|
Jb1KcoplJb2Kc|Ja1KcoprJa2Kc
JSKc::=x:=JaKc|JS1Kc;JS2Kc|
while (JbKc)doJSKc|
if(JbKc)then JS1KcelseJS2Kc
(c) Syntax rules for constant mutations
Figure 3: Skeletal program structures for the WHILE-style language
Table 1: Summary of mutation rules
“Holes” Mutation Rules
~ Inserting/removing a quali￿er, i.e., volatile ,const , and
restrict ;
Inserting/removing/replacing a modi￿er, i.e., long,short ,
signed ,unsigned ;
Replacing a variable by another variable within the feasible
scope;
~oReplacing a binary operator by another binary operator
within the same category, e.g., arithmetic, relational, and
logical operators;
Replacing/removing a unary operator, i.e., pre￿x increment,
post￿x increment, pre￿x decrement, post￿x decrement, and
logical negation;
~cChanging the value of an integer constant via a typical
operation, i.e., value+1 ,value-1 ,value*0 , and value*(-1) ;
Example. We use an example to illustrate our witnessing mutation
shown in Figure 4. Figure 4ashows an original program P. Figure 4b
shows the mutation skeleton PofP. Figures 4cand4dshow two
example mutated programs by ￿lling holes in P, where the former
is a￿rst-order mutation M1and the latter is a high-order mutation
M2. The mutated holes have been highlighted within boxes.
Mutation Outcomes. After generating a test program via muta-
tion, it is essential to judge whether it is passing or not. There are
two types of compiler bugs: crash andwrong-code bugs [ 75,82]. The
former denotes that the compiler crashes when compiling a test
program under some compilation options; while the latter mainly
denotes that the compiler miscompiles a program, causing it to pro-
duce inconsistent execution results without any failure messages
under di￿erent compilation options. Di￿erent types of bugs require
di￿erent test oracles, which determine whether a test program is
passing or not. If the given program triggers a crash bug, the used
oracle is whether the compiler crashes again under the same compi-
lation options. If the given program triggers a wrong-code bug, the
used oracle is whether a generated program still produces incon-
sistent results across prior inconsistent compilation options. DiWi
does not apply mutations on the code used as oracles (e.g., printf
statements in C), since it may cause the fake passing program prob-
lem, i.e., the constructed passing program via such mutations may
not be really passing but the used oracle simply cannot reveal the
bug. More discussion about it is presented in Section 6.
4.2 Heuristic-Based Test Program Generation
Due to huge search space and limited computational resources, we
cannot generate all witness test programs via mutation and then
select a subset of e￿ective witness test programs from them. One
of the most cost-e￿ective ways is that, during each generation, we
generate a witness test program that di￿ers from existing ones as
much as possible. In DiWi, given a failing test program, di￿erentintmain() {
inta = 1;
const int b = 2;
if(a > 0) {
intc = 3;
a = b + c;
}
return 0;
}
(a) original Pintmain() {
~ =~c;
~ =~c;
if(~ ~o~c){
~ =~c;
~ =~ ~o~ ;
}
return ~c;
}
(b) skeleton Pintmain() {
inta = 1;
int b = 2;
if(a > 0) {
intc = 3;
a = b + c;
}
return 0;
}
(c)mutation M1intmain() {
inta = 1;
int b = 2;
if(b > 0) {
intc = -3 ;
a = b + c;
}
return 0;
}
(d) mutation M2
Figure 4: Example of witnessing mutation
mutation rules are not equally e￿ective. The mutation rules that
more frequently generate diverse witness test programs should be
selected with higher probability for further mutations. Based on this
insight, we propose our heuristic-based test program generation in
the following subsections. In particular, we use coverage distance to
measure the diversity between test programs:
D￿￿￿￿￿￿￿￿￿ 4 (C￿￿￿￿￿￿￿ D￿￿￿￿￿￿￿). The distance Dist between
two test programs P 1and P 2is the Jaccard distance between their
statement coverage (where Stmt P1and Stmt P2represent the set of
covered statements by P 1and P 2respectively):
Dist (P1,P2)=1 Stmt P1\Stmt P2
Stmt P1[Stmt P2(1)
During the process of constructing a set of e￿ective witness
test programs, DiWi ￿rst selects a seed test program to mutate
(described in Section 4.2.1 ), and then selects a mutation rule to
apply (described in Section 4.2.2 ) in each iteration.
4.2.1 Seed Program Selection. The initial seed test program is the
given failing test program. All witness test programs are derived
from this initial seed test program, by conducting ￿rst-order or
high-order mutations on it. Actually, nth-order mutations can be
regarded as conducting ￿rst-order mutations on the programs gen-
erated by (n-1)th-order mutations on the initial seed test program.
That is, DiWi also treats the generated test programs via mutation
as seed test programs for following iterations. Here we can call the
initial seed test program the 0th-order mutation.
To reduce the risk of introducing failing test programs that are
due to other bugs, DiWi ￿rst selects (n 1)th-order (n  1) failing
test programs as seed test programs. If it cannot construct any
witness test program under the given terminating condition, DiWi
then selects nth-order (n  1) failing test programs. The reason is
that higher order failing test programs are more likely to incur other
bugs. Moreover, lower order failing test programs help control the
trace similarity between the newly generated witness test program
and the given failing test program. Please note that DiWi rejects any
newly generated witness test program without increasing diversity.
4.2.2 Mutation Rule Selection. Based on a selected seed program,
DiWi selects a mutation rule to mutate it. However, these mutation
rules are not equally e￿ective to generate diverse witness test pro-
grams for a given failing test program. Also, the same mutation rule
227ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Junjie Chen, Jiaqi Han, Peiyi Sun, Lingming Zhang, Dan Hao, and Lu Zhang
performs di￿erently for di￿erent initial failing test programs. There-
fore, we carefully design an adaptive procedure to select mutation
rules for constructing a set of e￿ective witness test programs. Intu-
itively, if a mutation rule can more frequently generate witness test
programs that have greater diversity with the existing ones, the mu-
tation rule should be selected with higher probability for further mu-
tations. Based on the intuition, we compute a priority score for each
mutation rule MRasScore (MR)=⇣
1
mÕm
i=1Dist(P,Pi)⌘
⇤Rate s,
where: 1 mis the number of existing witness test programs in
the set; 2 Pis the new witness test program generated by using
MR;3 Dist(,) is the coverage distance, computed by Formula 1;
4 Rate sis the success rate of generating accepted witness test
programs, i.e., the ratio of the number of times the witness test
program generated by MRis accepted into the witness set to the
number of times MRis selected for mutations.
Mutation rules can be ranked according to the descending order
of the computed priority scores. However, we cannot directly select
the mutation rule ranked at the 1 stposition for next mutation,
since the ranking is based on historical results of these mutation
rules and can hardly perfectly predict future results. Therefore, each
mutation rule should have some probability to be selected for next
mutation, and a mutation rule ranked higher in the ranking list
should have a larger probability to be selected. That is, the problem
of mutation rule selection in DiWi can be regarded as the sampling
problem from a probability distribution .
Here, since which mutation rule to be selected depends on the
most recent behavior of each mutation rule, it is actually a typical
Markov Chain (MC). Therefore, to solve the sampling problem from
a probability distribution, DiWi adopts the Metropolis-Hastings
(MH) algorithm [ 40], the most popular Markov Chain Monte Carlo
method, as heuristic by assuming the desired distribution to be
equilibrium distribution [ 23]. Here, MH obtains random samples
from a probability distribution. In our context, it samples the next
mutation rule (denoted as MRb) based on the current mutation rule
(denoted as MRa) according to a probability distribution. If MRb
is better than MRa, i.e., the priority score of MRbis larger than
that of MRa,MRbis de￿nitely accepted; If not, MRbstill has some
probability to be accepted. Following the existing work [ 20], we set
the probability distribution to be the geometric distribution, which
is the probability distribution of the number Xof Bernoulli trials
needed to obtain one success. If the success probability on each
trial is p, the probability the kthtrial being the ￿rst success can be
computed as Ps(X=k)=(1 p)k 1p.
During the process, mutation rules are selected randomly, and
thus the proposal distribution is symmetric. Therefore, the probabil-
ity of accepting MRbgiven MRais computed as Pa(MRb|MRa)=
Ps(MR b)
Ps(MR a)=(1 p)kb ka, where kaandkbare the positions of MRa
andMRbin the ranking list of mutation rules according to the de-
scending order of their priority scores. Please note that, when MRb
is better than MRa(i.e., Ps(MRb)>Ps(MRa)),Pa(MRb|MRa)=1.
After acquiring the result of MRb(i.e., accept or reject), DiWi up-
dates its score and re-ranks these mutation rules for next iteration.
4.2.3 Overall Algorithm. We formally present the generation pro-
cess of DiWi in Algorithm 1. The initial set of seed programs con-
tains only the given failing test program Pf, and the priority score
of each mutation rule is 0. In this algorithm, Line 1 randomly selectsAlgorithm 1: Heuristic-based Test Program Generation
Input : S: Seed test-program set { Pf}
MR : A list of mutation rules [ MRi|i21. . . 132]
Output: P: A set of witness test programs
1MRa MRi random (1... 132)
2S0 []
3while not termination do
4 P select (S)/*select a program from S */
5 ka position (MRa)/*getMRa’s position in MR */
6 do
7 MRb MRi random (1... 132)
8 kb position (MRb)
9 f random (0, 1)
10 while f (1 p)kb ka;
11 P0 mutate (P,MRb)
12 ifP0is passing ^8Pi2P,D(P0,Pi),0then
13 P P[{ P0}
14 end
15 ifP0is failing then
16 S0 S0[{P0}
17 end
18 updateScore (MRb,P0,P)
19 MR  sort(MR )
20 MRa MRb
21end
22ifSize(P)>0then
23 return P
24end
25else
26 S S0
27 Repeat from Line 2
28end
a mutation rule as the current one MRa. Lines 3-21 construct a set of
witness test programs until achieving a terminating condition. Line
4 selects a seed program Pto mutate. Line 5 gets the position of MRa
in the ranking list of mutation rules. Lines 6-10 acquire the next
mutation rule MRb. Line 11 applies MRbto mutate Pto generate
a new program P0. Lines 12-17 determine whether P0is accepted
byPandS0based on its execution results and coverage distances
with the existing witness programs in P. Lines 18-20 update the
score of MRband re-rank these mutation rules for next iteration.
IfP0is accepted into P, the score is updated by changing the two
items in the formula Score ; Otherwise, the score is updated by just
changing the latter item in the formula Score , since the witness set
and the coverage distances are not changed. Lines 22-28 determine
whether terminating the construction process. If there is no witness
program constructed via lower order mutation, the construction
process (Lines 2-21) is repeated by using higher order mutation (i.e.,
using higher order failing programs as seed programs).
4.3 Aggregation-Based Compiler Bug Isolation
After constructing a set of witness test programs, DiWi isolates
compiler bugs by analyzing the set of witness test programs and
the given failing test program. Following SBFL [ 7], DiWi computes
the suspicious value for each statement within the touched code
when compiling the given failing test program. Here DiWi adopts
Ochiai [ 7], one of the most e￿ective formulae in SBFL, to com-
pute the suspicious value for each touched statement. The formula
issus(s)=efsp
(efs+nfs)(efs+eps), where efsandnfsrepresent the
number of failing test programs that execute and do not execute
statement s, and epsrepresent the number of passing test programs
executing s. Here, we have only one given failing test program and
just consider the statements touched by the failing test program,
and thus efsis 1 and nfsis 0. Therefore, sus(s)=1p1+epsin DiWi.
228Compiler Bug Isolation via E￿ective Witness Test Program Generation ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
Then, similar to method-level aggregation [ 74], DiWi computes
the suspicious value of each ￿le by aggregating the suspicious
values of all the touched statements in the ￿le. When a failing test
program is mutated to a set of witness test programs, the coverage
for the buggy ￿le would be changed more than that for the bug-free
￿les on the whole. Therefore, to compute the suspicious value of
each ￿le, we use the formula SUS(f) =Õnf
i=1sus(si)
nf, where nfis
the number of touched statements when compiling the failing test
program in the ￿le f.
5 EVALUATION
In this study, we address the following research questions:
•RQ1: How does DiWi perform on compiler bug isolation?
•RQ2: Do our search-based witness test programs outper-
form the developer-provided test suite and the randomly
generated passing test programs?
•RQ3: Does our heuristic-based search strategy outperform
the random search strategy during mutation?
•RQ4: Can DiWi take advantage of the developer-provided
test suite (which is an exploration to further boost DiWi)?
5.1 Benchmark
We used GCC and LLVM as subjects, which cover almost all popular
C compilers used in the existing work [ 15,17,43,44,82]. To inves-
tigate the e￿ectiveness of DiWi, from the bug repositories [ 3,6]
of GCC and LLVM, we manually collected 90 bugs in total, each
of which is required to satisfy the following conditions: 1) the bug
has the equipped failing test program and the compilation options
triggering it in the bug report; 2) the bug has been ￿xed; 3) the bug
can be reproduced in our experimental environment. We manu-
ally collected bugs reported after 2013 following these conditions
until we had 45 bugs for each compiler since the manual process
is costly. Also, we manually identi￿ed the buggy locations (i.e.,
buggy ￿les) for each bug, which serve as the ground truth to eval-
uate the e￿ectiveness of bug isolation in our study. On average, a
GCC buggy version has 1,588 ￿les with 1,414K source lines of code
(SLOC), while a LLVM buggy version has 3,507 ￿les with 1,431K
SLOC1. We release the benchmark to facilitate the future research
on compiler bug detection, isolation, and ￿xing, and welcome more
researchers to contribute to this benchmark. In our benchmark,
each bug has: 1 buggy compiler version ;2 failing test program ;3 
compilation options for reproducing the bug ;4 buggy location ;5 
￿xed version . Our benchmark and code are available at the project
webpage: https://github.com/JunjieChen/DiWi .
5.2 Implementation and Con￿guration
DiWi utilizes Clang Libtooling library [ 1] to parse a test program to
an abstract syntax tree (AST) and then mutates it at the AST. DiWi
utilizes Gcov [ 4] to collect compiler test coverage. Here we set the
success probability of each Bernoulli trial pin Algorithm 1 to be
0.023 by satisfying the following conditions: 1 0.95Õ132
k=1(P(X=
k)) 1;2 p 1
132;3 (1 p)132 1p< , where  is a quite small
deviation (e.g., 0.001). We set the terminating condition of DiWi
to be one hour limit in our study. For any technique involving
1Since GCC and LLVM are implemented using C and C++ respectively, we consider
all the C ￿les for the former and C++ ￿les for the latter.randomness, we repeated it 5 times and use the median results.
Our study is conducted on a workstation with four-core CPU, 120G
memory, and Ubuntu 14.04 operating system.
5.3 Measurements
To evaluate the e￿ectiveness of bug isolation, we measure the posi-
tion of each buggy ￿le in the ranking list produced by a bug isolation
technique. If more than two ￿les have the same suspiciousness, we
use the worst ranking following the existing work [ 37,51,68]. We
compute the following widely used metrics [ 9,51,60,74]:
Top-n refers to the number of successfully isolated bugs within the
Top-n position (i.e., n2{1,5,10,20}in our study) in the ranking
list. Larger is better.
Mean First Rank (MFR) refers to the mean of the ￿rst buggy ￿le
rank for each bug. This metric emphasizes fast isolation of the ￿rst
buggy element to ease debugging. Smaller is better.
Mean Average Rank (MAR) refers to the mean of the average
rank of all buggy ￿les for each bug. Di￿erent from MFR, MAR
emphasizes precise isolation for all buggy elements.
5.4 Compared Techniques
Spectrum-based bug isolation (SBFL) [ 80] is the most widely-studied
bug isolation technique among traditional bug isolation techniques.
It is interesting to evaluate the e￿ectiveness of traditional SBFL on
compilers. It ￿rst records the coverage status of each program ele-
ment (i.e., each compiler ￿le in the study) during each test execution
and the test outcomes (i.e., passing or failing). Then SBFL computes
a suspicious value for each program element using some formula,
and ￿nally ranks the program elements based on their suspicious
values. Researchers have made dedicated e￿orts to design various
formulae on suspiciousness computation. We evaluated eight pop-
ular formulae following the existing work [ 68,81,88], including
SBI,Ochiai ,Tarantula ,Jaccard ,Ochiai2 ,Kulczynski2 ,OP, and D2,
and found they achieved extremely similar results for compiler bug
isolation in our study. Due to space limitation, we use the most
e￿ective Ochiai as the representative. For each compiler bug, we
use the given failing test program as the failing test program, and
use the developer-provided test suite as the passing test programs.
As the core of DiWi is to generate e￿ective witness test pro-
grams, it is interesting to investigate the impact of generated witness
test programs on bug isolation . To achieve this goal, we replace the
set of our search-based witness test programs with the developer-
provided test suite, and then use the aggregation-based SBFL to
isolate bugs. We call this technique SBFLde a. Besides, randomly
generated test programs via test-program generation tools like
Csmith [ 82] have been demonstrated to be quite e￿ective for de-
tecting compiler bugs [ 15,82]. Therefore, we also replace the set
of our search-based witness test programs with a set of randomly
generated passing test programs via Csmith [ 82], and then use
the aggregation-based SBFL to isolate bugs. Similarly, we call this
technique SBFLranda .
Besides, to investigate the impact of our heuristic-based search
strategy on bug isolation , we replace this strategy with the random
search strategy. That is, we do not have any guidance for construct-
ing witness test programs via mutation. We call this variant of DiWi
DiWirand(random search). Note that we used the same terminat-
ing condition for all compared techniques for fair comparison.
229ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Junjie Chen, Jiaqi Han, Peiyi Sun, Lingming Zhang, Dan Hao, and Lu Zhang
5.5 Threats to Validity
First, the ￿ndings in this work may not generalize to other compiler
bugs. To reduce this threat, we tried our best to construct a new
dataset including 90 real-world compiler bugs. Note that the process
was extremely time consuming and tedious; to our knowledge this is
the largest dataset for reproducible compiler bugs. Second, besides
the used two kinds of passing test programs, there are other kinds
of test programs, e.g., programs generated via swarm testing [ 29].
In the future, we will use more kinds of passing programs for
comparison. Third, the settings (e.g., parameters in our search-
based strategy and terminating condition of DiWi) may impact our
study. In the future, we will explore their impacts.
5.6 Results and Analysis
5.6.1 Overall e￿ectiveness of DiWi. Rows “DiWi” in Table 2present
the e￿ectiveness of DiWi. Overall, DiWi successfully isolates 10/37/60/71
bugs (out of 90 bugs) within Top-1/5/10/20 buggy ￿les, demonstrat-
ing its e￿ectiveness on compiler bug isolation. That is, about 66.67%
and 78.89% bugs are e￿ectively isolated within 10 and 20 compiler
￿les, respectively. We further analyzed the e￿ectiveness of DiWi
for di￿erent compiler systems. Intuitively, LLVM has a much larger
number of ￿les (shown in Section 5.1), and should be harder to per-
form bug isolation. However, interestingly, shown in Table 2, DiWi
achieves quite similar e￿ectiveness on GCC and LLVM, and DiWi
even isolates 3 more bugs within Top-5 on LLVM than GCC. More-
over, we ￿nd that other studied techniques indeed perform worse
on LLVM than GCC. Therefore, that demonstrates the scalability of
DiWi — DiWi’s e￿ectiveness does not decrease dramatically when
facing larger compiler systems.
We also analyzed the comparison e￿ectiveness between DiWi
and traditional SBFL shown in Rows “SBFL” in Table 2. We ￿nd
that traditional SBFL performs poorly on the studied compilers. For
example, SBFL ranks the GCC buggy ￿les as the 276.22 thposition,
while ranking the LLVM buggy ￿les as the 619.09 thposition on
average. To our knowledge, this is the ￿rst study demonstrating that
the intensively studied SBFL cannot scale to real-world compiler
systems. On the contrary, although equally simple and lightweight,
our DiWi is able to signi￿cantly outperform SBFL. On average, DiWi
localizes compiler buggy ￿les within 14.27(MFR) and 14.76(MAR),
outperforming SBFL by 96.81%(MFR) and 96.70%(MAR).
Qualitative Analysis. We further conducted qualitative analysis
using two examples. Figure 5shows an example LLVM bug, where
the left is the failing test program and the right is one of witness
test programs generated by DiWi. The test program (in Figure 5a)
is miscompiled by the LLVM trunk (revision 229830) at -O1and
above. The bug occurs at the ￿le “ ScalarEvolution.cpp ”, which
incorrectly promotes the 16-bit addinto a 32-bit add. After just one
mutation shown in Figure 5b, the mutated program does not trigger
the bug, demonstrating the power of our designed mutation rules.
We computed the coverage distance between the two programs, and
its value is only 0.013, which con￿rms our assumption that minor
changes are likely to make them share close compiler execution
traces. In particular, DiWi ranks the buggy ￿le at the 6thof all ￿les.
Figure 6shows an example for a GCC bug. The failing test pro-
gram in Figure 6ais miscompiled by the GCC trunk (revision 206472)
at-Osand above. The bug occurs at the ￿le “ tree-ssa-sink.c ”,
because it does not well handle the case where a second eliminatedunsigned short a = 1;
intb = 65536;
intc;
intmain () {
for(c = 0; c < 1; c = 1) {
for(;;) {
b &=   a;
break ;
}
}
if(b)
__builtin_abort ();
return 0;
}
(a) Failing test programunsigned short a = 1;
intb = 65536;
intc;
intmain () {
for(c = 0; c < 1; c = 1) {
for(;;) {
b &= a++ ;
break ;
}
}
if(b)
__builtin_abort ();
return 0;
}
(b) Passing mutation
Figure 5: LLVM Bug 22641
intprintf ( const char ∗, ...);
inta[6], b, c = 1, d;
short e;
void fn1 ( intp) {
b = a[p];
}
intmain () {
a[0] = 1;
if(c)
e  ;
d = e;
long long f = e;
fn1 ((f >> 56) & 1);
printf ( "%d\n", b);
return 0;
}
(a) Failing test programintprintf ( const char ∗, ...);
inta[6], b, c = 1, d;
volatile short e;
void fn1 ( intp) {
b = a[p];
}
intmain () {
a[0] = 1;
if(c)
e  ;
d = e;
long long f = e;
fn1 ((f >> 56) & 1);
printf ( "%d\n", b);
return 0;
}(b) Passing mutation
Figure 6: GCC Bug 59747
extension requires widening a copy created for elimination of a
prior extension. After adding “ volatile ” as shown in Figure 6b, the
mutated program does not trigger the bug anymore. We computed
their coverage distance, and its value is 0.019, which is also very
small. In particular, DiWi ranks the buggy ￿le at the 1stof all ￿les.
Furthermore, DiWi also brings extra bene￿ts. That is, these gen-
erated witness test programs via mutation provide some useful
hints for the developers to facilitate bug diagnosis/￿xing. For ex-
ample, from Figure 5, the bug is not triggered again by replacing
“- -” with “++”, which means that “-1” or “+1” has an impact on the
bug. That is true, because incorrect promotion from 16-bit addto
32-bit addleads to incorrect widening from “-1” to “65535”.
5.6.2 DiWi v.s. SBFLde av.s. SBFLranda.We compared DiWi with
SBFLde aand SBFLranda toinvestigate the impact of our search-
based witness test programs . From Rows “DiWi”, “SBFLde a”, and
“SBFLranda ” in Table 2, DiWi outperforms them in terms of all the
used metrics. For example, DiWi isolates 150.00%/68.18%/33.33%/
12.70% more bugs than SBFLde aand 66.67%/54.17%/36.36%/18.33%
more bugs than SBFLranda , within Top-1/5/10/20 for all the studied
bugs. That demonstrates that DiWi achieves much more precise
isolation results than SBFLde aand SBFLranda. In particular, we
conducted the Wilcoxon Signed-Rank Test [ 79] for their isolation
ranks at the signi￿cance level of 0.05 to determine whether there
are signi￿cant di￿erences between them on all the studied bugs.
The p-value is 0.0016 and 0.0021 respectively, demonstrating that
DiWi does signi￿cantly outperform them. These results indicate
that our search-based witness test programs are more powerful
than both the developer-provided and randomly generated (via
Csmith) witness test programs on isolating compiler bugs.
230Compiler Bug Isolation via E￿ective Witness Test Program Generation ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
Table 2: Compiler bug isolation e￿ectiveness comparison
Subject Technique Top-1 *Top  1 Top-5 *Top  5 Top-10 *Top  10 Top-20 *Top  20 MFR *MFR MAR *MAR
GCCDiWi 5– 17 – 30 – 36 – 13.93 – 14.76 –
SBFL 0 1 0 1 0 1 0 1 276.22 94.96 276.22 94.66
SBFLde a 3 66.67 12 41.67 25 20.00 30 20.00 17.29 19.43 18.07 18.32
SBFLranda 1 400.00 12 41.67 22 36.36 29 24.14 17.66 21.12 19.62 24.77
DiWirand3 66.67 14 21.43 24 25.00 33 9.09 16.91 17.62 17.57 16.51
LLVMDiWi 5– 20 – 30 – 35 – 14.60 – 14.76 –
SBFL 1 400.00 3 566.67 3 900.00 3 1,066.67 619.09 97.64 619.09 97.62
SBFLde a 1 400.00 10 100.00 20 50.00 33 6.06 26.44 44.78 26.61 44.53
SBFLranda 5– 12 66.67 22 36.36 31 12.90 24.02 39.22 24.21 39.03
DiWirand3 66.67 15 33.33 24 25.00 33 6.06 20.82 29.88 21.04 29.85
ALLDiWi 10 – 37 – 60 – 71 – 14.27 – 14.76 –
SBFL 1 900.00 3 1,133.33 3 1,900.00 3 2,266.67 447.66 96.81 447.66 96.70
SBFLde a 4 150.00 22 68.18 45 33.33 63 12.70 21.87 34.75 22.34 33.93
SBFLranda 6 66.67 24 54.17 44 36.36 60 18.33 20.84 31.53 21.92 32.66
DiWirand6 66.67 29 27.59 48 25.00 66 7.58 18.87 24.38 19.31 23.56
*Columns “ *⇤” present the improvement rates of DiWi over a compared technique in terms of various measurements.
●●●●
●●●●●●●●●●
0.00.20.40.6
DiWiDevRandCoverage distance(a) GCC●●0.00.10.20.30.40.5
DiWiDevRandCoverage distance(b) LLVM
Figure 7: Coverage distance between generated witness test
programs and the given failing test program
To further investigate why our search-based witness test pro-
grams can signi￿cantly outperform the others, we further quantita-
tively evaluated one basic assumption of DiWi, i.e., minor mutation
changes are likely to make them share close compiler traces. We
computed the mean coverage distance between the given failing test
program and all witness test programs for each bug. Figure 7shows
the coverage distance comparison among DiWi (our search-based
witness program generation), SBFLde a(the developer-provided
test suite, denoted as Dev in this ￿gure), and SBFLranda (the Csmith
random program generation, denoted as Rand ). In this ￿gure, the
violin plots show the density of coverage distances at di￿erent val-
ues, and the box plots show the median and interquartile ranges.
From this ￿gure, we ￿nd that our search-based witness programs
have much smaller coverage distances with the given failing test
program than the developer-provided witness programs and the
randomly generated witness programs via Csmith. That validates
our assumption. Interestingly, in GCC there are some cases where
the coverage distances are obviously larger than other cases. We
looked into the code and found that all these bugs are crash bugs,
and various code regions not executed by the failing test program
(due to crashes) can be executed by the witness test programs.
5.6.3 DiWi vs. DiWirand.We also compared DiWi and DiWirand
toinvestigate the impact of our heuristic-based search strategy shown
in Rows “DiWirand” in Table 2. From this table, DiWi outperforms
DiWirandfor both GCC and LLVM in terms of all the used metrics.DiWi isolates 66.67%/27.59%/25.00%/7.58% more bugs within Top-
1/5/10/20 than DiWirandfor all the studied bugs. DiWi performs
24.38% and 23.56% better than DiWirandin terms of MFR and MAR.
That demonstrates that DiWi performs more precise than DiWirand
for isolating both GCC and LLVM bugs. We also conducted the
Wilcoxon Signed-Rank Test for the isolation ranks of DiWi and
DiWirand. The p-value is 1.345e-06, demonstrating the superiority
of DiWi over DiWirand. The results indicate that our heuristic-
based search strategy outperforms random search.
To investigate why the heuristic-based search strategy outper-
forms the random search strategy, we quantitatively evaluated
another basic assumption of DiWi, i.e., witness test programs gen-
erated via our heuristic-based search strategy should have great
diversity. We computed the coverage diversity among witness test
programs. Here we ￿rst computed the minimum distance for each
witness test program with others, and then computed the mean
of all minimum distances for all witness test programs. We ￿nd
that the mean coverage diversity of our heuristic-based strategy is
about 214.72X and 14.10X greater than that of the random search
strategy for GCC and LLVM, respectively. That is, DiWi indeed has
greater diversity than DiWirand, validating our assumption.
5.6.4 Exploring DiWi with Developer Tests. Our search-based wit-
ness test programs have been demonstrated to outperform the other
two. We further analyzed the cases where each type of witness
programs performs well. Here we chose the bugs isolated within
Top-5 to analyze. We found although DiWi isolates 37 bugs within
Top-5, SBFLde aand SBFLranda can also isolate 11 additional bugs
in total within Top-5. That is, the developer-provided programs
and randomly generated programs via Csmith can complement our
search-based witness programs to some degree. If we can e￿ectively
synthesize them, the compiler bug isolation e￿ectiveness may be
improved. We analyzed why randomly generated programs can
isolate additional bugs, and found the reason to be that the tool
Csmith cannot cover all C language features, causing some ￿les al-
ways uncovered. When the bug occurs at these ￿les, due to such an
occasional factor, the ￿les are easy to isolate using Csmith generated
passing programs. Getting rid of this occasional factor, we made
the ￿rst attempt in the direction by synthesizing our search-based
witness programs and the developer-provided witness programs.
231ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Junjie Chen, Jiaqi Han, Peiyi Sun, Lingming Zhang, Dan Hao, and Lu Zhang
a =add(a,10);intadd(inta,intb) {…}randomly selectcandidatepooldependent materialsblendintadd(inta,intb) {…}…intnum = 1;a=add(a,10);……intnum = 1; …DiWigenerated program:intadd(inta,intb) {…}…intnum = 1;num=add(num,10);…renamea blended program
Figure 8: Test program synthesis
For each search-based witness test program by DiWi, the synthe-
sis technique produces a blended witness program automatically
as follows: (1) it treats all basic blocks/statements in all developer-
provided programs as a candidate pool; (2) it randomly selects a can-
didate block/statement from the pool and collects its all dependent
materials (e.g., method declaration and header ￿le); (3) it randomly
inserts the candidate block/statement and its dependent materials
to the search-based witness program, and conducts refactoring for
new variables in the block/statement to make the blended program
valid, i.e., renaming them to the variables occurred in the original
program with compatible types. Figure 8shows the process of gen-
erating a blended program. We used Clang Libtooling library [ 1]
to conduct such synthesis at the AST, and the whole process is
fully automated. We repeated the above steps for each search-based
witness program until a blended witness program is produced. In
this way, we get a set of blended witness programs. We then used
the aggregation mechanism to isolate compiler bugs based on the
given failing program and the blended witness programs.
We evaluated whether the synthesis can further improve DiWi.
From the results, among 90 compiler bugs, the synthesis improves
the isolation e￿ectiveness for 50% bugs (45 out of 90), and reduce
the e￿ectiveness for only 10 bugs. Also, such synthesis e￿ectively
isolates 30.00% and 24.32% more bugs within Top-1 and Top-5 ￿les
than DiWi, respectively. Its improvement rates of MFR and MAR
are 11.75% and 9.15%, respectively, compared with DiWi. This is
because the synthesis provides more possibilities for DiWi to ￿nd
e￿ective witness test programs by augmenting the mutation space,
demonstrating a promising future to further explore e￿ective ways
for blending test programs from di￿erent sources.
6 DISCUSSION
What developers want. To investigate the practicability of DiWi,
we conducted a survey by communicating with 7 compiler develop-
ers (sending out 10 requests in total) from 4 international companies
building their own compilers (including the LLVM team). 6 devel-
opers con￿rmed that their compiler bug debugging process starts
from buggy ￿les identi￿cation and this step is time-consuming,
indicating the necessity of compiler bug isolation at the ￿le level.
Moreover, 6 developers think the e￿ectiveness of DiWi (shown in
our study) is practical and show strong desire for DiWi by using
the words “can’t wait to see” during the communications. Even the
developer who does not ￿rst identify buggy ￿les when debugging,
also expresses his/her willing to improve the debugging process by
using DiWi. In the future, we will improve DiWi at ￿ner granularity
such as the method level.intprintf ( const char ∗,
...);
inta[1] = { 1 };
intb = 1;
intc;
intmain () {
for(; c < 1; c++) {
if(a[0]) {
a[0] &= 1;
b = 0;
}
}
printf ( "%d\n", b);
return 0;
}
(a) Failingintprintf ( const char ∗,
...);
inta[1] = { 1 };
intb = 1;
intc;
intmain () {
for(; c < 1; c++) {
if(a[0]) {
a[0] &= 1;
b = 0;
}
}
printf ( "%d\n",c );
return 0;
}
(b) Fake pass-1intprintf ( const char ∗,
...);
inta[1] = { 1 };
intb = 1;
intc;
intmain () {
for(; c < 1; c++) {
if(a[0]) {
a[0] &= 1;
b= 1 ;
}
}
printf ( "%d\n", b);
return 0;
}
(c) Fake pass-2
Figure 9: Example of test oracle challenge (GCC Bug 61140)
Test oracle challenge. We present the used test oracles to deter-
mine whether a mutated program is passing in Section 4.1. However,
such oracles are not absolutely precise, especially for wrong-code
bugs. DiWi treats the mutated program producing consistent results
as a passing program, but it may still trigger the bug. For example,
Figure 9ashows a failing program, where the outputs (of b) under
-O0and-O1are di￿erent. The other two ￿gures are two mutated
passing programs under the used oracle. However, both of them
arefake passing programs. In Figure 9b, the variable binprintf
statement is mutated to be c. It still triggers the bug, but it is re-
garded as a passing program since cprints the same results under
-O0and-O1. In Figure 9c, the value of b(i.e., 0) is mutated to be 1,
which is equal to the initial value of b. Such mutations make the
output always the same (i.e., 1), causing the used oracle to miss the
bug. Fake passing programs may impact the isolation e￿ectiveness.
However, it is challenging to solve this oracle problem. To reduce its
impact, DiWi avoids the mutations that directly change the oracle
(i.e., print statements), but cannot deal well with other cases such
as Figure 9c. In the future, we will introduce advanced data- and
control-￿ow analysis [ 63] to address this challenge.
Unde￿ned behavior challenge. Another challenge lies in unde-
￿ned behaviors for compilers, which mean the semantics of cer-
tain operations are unde￿ned in the programming-languages stan-
dards [ 30]. If a program contains unde￿ned behaviors, compilers
may produce varied results. Identifying unde￿ned behaviors is a
di￿cult challenge in compiler research [ 49,78]. It is also a threat
in our work, since our mutation may introduce unde￿ned behav-
iors. However, unde￿ned behaviors tend to impact bug detection,
since di￿erent results of a “failing” program may be caused by real
bugs or unde￿ned behaviors. In DiWi, we only kept the passing pro-
grams with the same results to isolate bugs, and thus the threat may
be not serious. We will relieve this problem by adopting existing
light-weight methods [ 49] for identifying unde￿ned behaviors.
7 CONCLUSION
In this paper, we propose a novel compiler bug isolation technique,
DiWi, which proposes a heuristic-based search strategy to carefully
generate a set of e￿ective witness programs by performing our
designed witnessing mutation rules on the given failing program.
The results on 90 real bugs for GCC and LLVM show, DiWi isolates
78.89% of the studied bugs within 20 compiler ￿les, signi￿cantly out-
performing state-of-the-art SBFL. DiWi is general and not limited to
compilers, we plan to apply it to other systems taking structurally
complex test inputs, e.g., operating systems and browsers.
232Compiler Bug Isolation via E￿ective Witness Test Program Generation ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia
REFERENCES
[1]Accessed: 2019. Clang Libtooling library. http://clang.llvm.org/docs/LibTooling.
html .
[2]Accessed: 2019. GCC. https://gcc.gnu.org .
[3]Accessed: 2019. GCC bug repository. https://gcc.gnu.org/bugzilla/ .
[4]Accessed: 2019. Gcov. https://gcc.gnu.org/onlinedocs/gcc/Gcov.html .
[5]Accessed: 2019. LLVM. https://llvm.org .
[6]Accessed: 2019. LLVM bug repository. https://bugs.llvm.org .
[7]Rui Abreu, Peter Zoeteweij, and Arjan JC Van Gemund. 2007. On the accuracy
of spectrum-based fault localization. In TAICPART-MUTATION 2007 . 89–98.
[8]Shay Artzi, Julian Dolby, Frank Tip, and Marco Pistoia. 2010. Directed test
generation for e￿ective fault localization. In ISSTA . 49–60.
[9]Tien-Duy B Le, David Lo, Claire Le Goues, and Lars Grunske. 2016. A learning-to-
rank based fault localization approach using likely invariants. In ISSTA . 177–188.
[10] Benoit Baudry, Franck Fleurey, and Yves Le Traon. 2006. Improving test suites
for e￿cient fault localization. In ICSE . 82–91.
[11] José Campos, Rui Abreu, Gordon Fraser, and Marcelo d’Amorim. 2013. Entropy-
based test generation for improved fault localization. In ASE. 257–267.
[12] Jacqueline M. Caron and Peter A. Darnell. 1990. Bug￿nd: A Tool for Debugging
Optimizing Compilers. SIGPLAN Notices 25, 1 (1990), 17–22.
[13] Bor-Yuh Evan Chang, Adam Chlipala, George C. Necula, and Robert R. Schneck.
2005. Type-based veri￿cation of assembly language for compiler debugging. In
TLDI . 91–102.
[14] Junjie Chen. 2018. Learning to accelerate compiler testing. In ICSE: Companion
Proceeedings . 472–475.
[15] Junjie Chen, Yanwei Bai, Dan Hao, Yingfei Xiong, Hongyu Zhang, and Bing Xie.
2017. Learning to prioritize test programs for compiler testing. In ICSE . 700–711.
[16] Junjie Chen, Yanwei Bai, Dan Hao, Yingfei Xiong, Hongyu Zhang, Lu Zhang,
and Bing Xie. 2016. Test case prioritization for compilers: A text-vector based
approach. In ICST . 266–277.
[17] Junjie Chen, Wenxiang Hu, Dan Hao, Yingfei Xiong, Hongyu Zhang, Lu Zhang,
and Bing Xie. 2016. An empirical comparison of compiler testing techniques. In
ICSE . 180–190.
[18] Junjie Chen, Guancheng Wang, Dan Hao, Yingfei Xiong, Hongyu Zhang, Lu
Zhang, and XIE Bing. 2018. Coverage Prediction for Accelerating Compiler
Testing. TSE (2018). to appear.
[19] Yang Chen, Alex Groce, Chaoqiang Zhang, Weng-Keen Wong, Xiaoli Fern, Eric
Eide, and John Regehr. 2013. Taming compiler fuzzers. In PLDI , Vol. 48. 197–208.
[20] Yuting Chen, Ting Su, Chengnian Sun, Zhendong Su, and Jianjun Zhao. 2016.
Coverage-directed Di￿erential Testing of JVM Implementations. In PLDI . 85–99.
[21] Vidroha Debroy and W Eric Wong. 2010. Using mutation to automatically suggest
￿xes for faulty programs. In ICST . 65–74.
[22] Nicholas DiGiuseppe and James A Jones. 2011. On the in￿uence of multiple faults
on coverage-based fault localization. In ISSTA . 210–220.
[23] Yadolah Dodge. 2006. The Oxford dictionary of statistical terms . Oxford University
Press.
[24] Alastair F. Donaldson, Hugues Evrard, Andrei Lascu, and Paul Thomson. 2017.
Automated Testing of Graphics Shader Compilers. Proc. ACM Program. Lang. 1,
OOPSLA (2017), 93:1–93:29.
[25] Robert Feldt, Simon Poulding, David Clark, and Shin Yoo. 2016. Test set diameter:
Quantifying the diversity of sets of test cases. In ICST . 223–233.
[26] Robert Feldt, Richard Torkar, Tony Gorschek, and Wasif Afzal. 2008. Searching
for cognitively diverse tests: Towards universal test diversity metrics. In ICSTW .
178–186.
[27] Zachary P Fry and Westley Weimer. 2010. A human study of fault localization
accuracy. In ICSM . 1–10.
[28] Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical Program
Repair via Bytecode Mutation. In ISSTA . to appear.
[29] Alex Groce, Chaoqiang Zhang, Eric Eide, Yang Chen, and John Regehr. 2012.
Swarm testing. In ISSTA . 78–88.
[30] Chris Hathhorn, Chucky Ellison, and Grigore Roşu. 2015. De￿ning the Unde-
￿nedness of C. In PLDI . 336–345.
[31] K. Scott Hemmert, Justin L. Tripp, Brad L. Hutchings, and Preston A. Jackson.
2003. Source Level Debugger for the Sea Cucumber Synthesizing Compiler. In
FCCM . 228.
[32] Satia Herfert, Jibesh Patra, and Michael Pradel. 2017. Automatically Reducing
Tree-structured Test Inputs. In ASE. 861–871.
[33] Christian Holler, Kim Herzig, and Andreas Zeller. 2012. Fuzzing with code
fragments. In USENIX Security . 445–458.
[34] Josie Holmes and Alex Groce. 2018. Causal distance-metric-based assistance for
debugging after compiler fuzzing. In ISSRE . 166–177.
[35] Shin Hong, Byeongcheol Lee, Taehoon Kwak, Yiru Jeon, Bongsuk Ko, Yunho
Kim, and Moonzoo Kim. 2015. Mutation-based fault localization for real-world
multilingual programs. In ASE. 464–475.
[36] Reyhaneh Jabbarvand and Sam Malek. 2017. µDroid: an energy-aware mutation
testing framework for Android. In FSE. 208–219.
[37] Dennis Je￿rey, Neelam Gupta, and Rajiv Gupta. 2008. Fault Localization Using
Value Replacement. In ISSTA . 167–178.[38] James A Jones and Mary Jean Harrold. 2005. Empirical evaluation of the tarantula
automatic fault-localization technique. In ASE. 273–282.
[39] René Just. 2014. The Major mutation framework: E￿cient and scalable mutation
analysis for Java. In ISSTA . 433–436.
[40] Robert E. Kass, Bradley P. Carlin, Andrew Gelman, and Radford M. Neal. 1998.
Markov Chain Monte Carlo in Practice: A Roundtable Discussion. American
Statistician 52, 2 (1998), 93–100.
[41] Nico Krebs and Lothar Schmitz. 2014. Jaccie: A Java-based compiler–compiler for
generating, visualizing and debugging compiler components. Science of Computer
Programming 79 (2014), 101–115.
[42] Stephen Kyle, Hugh Leather, Björn Franke, Dave Butcher, and Stuart Monteith.
2015. Application of Domain-aware Binary Fuzzing to Aid Android Virtual
Machine Testing. In VEE. 121–132.
[43] Vu Le, Mehrdad Afshari, and Zhendong Su. 2014. Compiler validation via equiv-
alence modulo inputs. In PLDI . 216–226.
[44] Vu Le, Chengnian Sun, and Zhendong Su. 2015. Finding Deep Compiler Bugs via
Guided Stochastic Program Mutation. In OOPSLA . 386–399.
[45] Wei Le and Mary Lou So￿a. 2010. Path-based fault correlations. In FSE. 307–316.
[46] Wei Le and Mary Lou So￿a. 2011. Generating analyses for detecting faults in
path segments. In ISSTA . 320–330.
[47] Wei Le and Mary Lou So￿a. 2013. Marple: Detecting faults in path segments
using automatically generated analyses. TOSEM 22, 3 (2013), 18.
[48] Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, and Westley Weimer.
2012. A systematic study of automated program repair: Fixing 55 out of 105 bugs
for $8 each. In ICSE . 3–13.
[49] Juneyoung Lee, Yoonseung Kim, Youngju Song, Chung-Kil Hur, Sanjoy Das,
David Majnemer, John Regehr, and Nuno P Lopes. 2017. Taming unde￿ned
behavior in LLVM. In PLDI . 633–647.
[50] Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. 2019. DeepFL: Integrating
Multiple Fault Diagnosis Dimensions for Deep Fault Localization. In ISSTA . to
appear.
[51] Xia Li and Lingming Zhang. 2017. Transforming Programs and Tests in Tandem
for Fault Localization. In OOPSLA . 92:1–92:30.
[52] Ben Liblit, Mayur Naik, Alice X. Zheng, Alex Aiken, and Michael I. Jordan. 2005.
Scalable Statistical Bug Isolation. In PLDI . 15–26.
[53] Christopher Lidbury, Andrei Lascu, Nathan Chong, and Alastair F. Donaldson.
2015. Many-core Compiler Fuzzing. In PLDI . 65–76.
[54] Bing Liu, Shiva Nejati, Lionel C Briand, et al .2017. Improving fault localization
for Simulink models using search-based testing and prediction models. In SANER .
359–370.
[55] Wes Masri. 2015. Automated Fault Localization: Advances and Challenges. In
Advances in Computers . Vol. 99. 103–156.
[56] Wes Masri and Rawad Abou Assi. 2010. Cleansing test suites from coincidental
correctness to enhance fault-localization. In ICST . 165–174.
[57] William M McKeeman. 1998. Di￿erential testing for software. Digital Technical
Journal 10, 1 (1998), 100–107.
[58] Hong Mei and Lu Zhang. 2018. Can big data bring a breakthrough for software au-
tomation? SCIENCE CHINA Information Sciences 61, 5 (2018), 056101:1–056101:3.
[59] Martin Monperrus. 2018. Automatic software repair: a bibliography. CSUR 51, 1
(2018), 17:1–17:24.
[60] Seokhyeon Moon, Yunho Kim, Moonzoo Kim, and Shin Yoo. 2014. Ask the
mutants: Mutating faulty programs for fault localization. In ICST . 153–162.
[61] Francisco Gomes de Oliveira Neto, Robert Feldt, Linda Erlenhov, and José Benardi
de Souza Nunes. 2018. Visualizing test diversity to support test optimisation.
arXiv preprint arXiv:1807.05593 (2018).
[62] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chan-
dra. 2013. Sem￿x: Program repair via semantic analysis. In ICSE . 772–781.
[63] Flemming Nielson, Hanne R. Nielson, and Chris Hankin. 1999. Principles of
program analysis. Springer Verlag Berlin (1999).
[64] Kazunori Ogata, Tamiya Onodera, Kiyokuni Kawachiya, Hideaki Komatsu, and
Toshio Nakatani. 2006. Replay compilation: improving debuggability of a just-in-
time compiler. In OOPSLA . 241–252.
[65] Kai Pan, Sunghun Kim, and E James Whitehead. 2009. Toward an understanding
of bug ￿x patterns. EMSE 14, 3 (2009), 286–315.
[66] Mike Papadakis and Yves Le Traon. 2012. Using mutants to locate" unknown"
faults. In ICST . 691–700.
[67] Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL: Mutation-based Fault
Localization. STVR 25, 5-7 (2015), 605–628.
[68] Spencer Pearson, José Campos, René Just, Gordon Fraser, Rui Abreu, Michael D.
Ernst, Deric Pang, and Benjamin Keller. 2017. Evaluating and Improving Fault
Localization. In ICSE . 609–620.
[69] John Regehr, Yang Chen, Pascal Cuoq, Eric Eide, Chucky Ellison, and Xuejun
Yang. 2012. Test-case reduction for C compiler bugs. In PLDI , Vol. 47. 335–346.
[70] Manos Renieres and Steven P Reiss. 2003. Fault localization with nearest neighbor
queries. In ASE. 30–39.
[71] Jeremias Rö  ler, Gordon Fraser, Andreas Zeller, and Alessandro Orso. 2012.
Isolating failure causes through test case generation. In ISSTA . 309–319.
233ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Junjie Chen, Jiaqi Han, Peiyi Sun, Lingming Zhang, Dan Hao, and Lu Zhang
[72] Raul Santelices, James A Jones, Yanbing Yu, and Mary Jean Harrold. 2009. Light-
weight fault-localization using multiple coverage types. In ICSE . 56–66.
[73] Anthony M. Sloane. 1999. Debugging Eli-Generated Compilers With Noosa. In
CC. 17–31.
[74] Jeongju Sohn and Shin Yoo. 2017. FLUCCS: using code and change metrics to
improve fault localization. In ISSTA . 273–283.
[75] Chengnian Sun, Vu Le, Qirun Zhang, and Zhendong Su. 2016. Toward Under-
standing Compiler Bugs in GCC and LLVM. In ISSTA . 294–305.
[76] Chengnian Sun, Yuanbo Li, Qirun Zhang, Tianxiao Gu, and Zhendong Su. 2018.
Perses: Syntax-guided program reduction. In ICSE . 361–371.
[77] Shaowei Wang, David Lo, Lingxiao Jiang, Hoong Chuin Lau, et al .2011. Search-
based fault localization. In ASE. 556–559.
[78] Xi Wang, Nickolai Zeldovich, M Frans Kaashoek, and Armando Solar-Lezama.
2013. Towards optimization-safe systems: Analyzing the impact of unde￿ned
behavior. In SOSP . 260–275.
[79] Frank Wilcoxon, SK Katti, and Roberta A Wilcox. 1970. Critical values and
probability levels for the Wilcoxon rank sum test and the Wilcoxon signed rank
test. Selected tables in mathematical statistics 1 (1970), 171–259.
[80] W. Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A
Survey on Software Fault Localization. TSE 42, 8 (2016), 707–740.[81] Jifeng Xuan and Martin Monperrus. 2014. Test case puri￿cation for improving
fault localization. In FSE. 52–63.
[82] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and under-
standing bugs in C compilers. In PLDI . 283–294.
[83] Andreas Zeller. 2002. Isolating cause-e￿ect chains from computer programs. In
FSE. 1–10.
[84] Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure-
inducing input. TSE 28, 2 (2002), 183–200.
[85] Lingming Zhang, Miryung Kim, and Sarfraz Khurshid. 2011. Localizing failure-
inducing program edits based on spectrum information. In ICSM . 23–32.
[86] Lingming Zhang, Tao Xie, Lu Zhang, Nikolai Tillmann, Jonathan De Halleux, and
Hong Mei. 2010. Test generation via dynamic symbolic execution for mutation
testing. In ICSM . 1–10.
[87] Lingming Zhang, Lu Zhang, and Sarfraz Khurshid. 2013. Injecting mechanical
faults to localize developer faults for evolving software. In OOPSLA . 765–784.
[88] Mengshi Zhang, Xia Li, Lingming Zhang, and Sarfraz Khurshid. 2017. Boosting
Spectrum-based Fault Localization Using PageRank. In ISSTA . 261–272.
[89] Qirun Zhang, Chengnian Sun, and Zhendong Su. 2017. Skeletal Program Enu-
meration for Rigorous Compiler Testing. In PLDI . 347–361.
234