Tackling Combinatorial Explosion:
A Study of Industrial Needs and Practices
for Analyzing Highly Configurable Systems
Mukelabai Mukelabai
Chalmers|University of Gothenburg
SwedenDamir Nešić
KTH Stockholm
SwedenSalome Maro
Chalmers|University of Gothenburg
Sweden
Thorsten Berger
Chalmers|University of Gothenburg
SwedenJan-Philipp Steghöfer
Chalmers|University of Gothenburg
Sweden
ABSTRACT
Highly configurable systems are complex pieces of software. To
tackle thiscomplexity, hundredsof dedicated analysistechniques
have been conceived, many of which able to analyze system prop-
erties for all possible system configurations, as opposed to tradi-
tional,single-systemanalyses.Unfortunately,itislargelyunknown
whether these techniques are adopted in practice, whether they
addressactualneeds,orwhatstrategiespractitionersactuallyapply
toanalyzehighlyconfigurablesystems.Wepresentastudyofanal-
ysispracticesandneedsinindustry.Itreliedonasurveywith27
practitioners engineering highly configurable systems and follow-
up interviews with 15 of them, covering 18 different companiesfrom eight countries. We confirm that typical properties consid-
ered in the literature (e.g., reliability) are relevant, that consistency
between variability models and artifacts is critical, but that the ma-
jority of analyses for specifications of configuration options (a.k.a.,
variabilitymodelanalysis)isnotperceivedasneeded.Weidentified
ratherpragmaticanalysisstrategies,includingpracticestoavoid
theneedforanalysis.For instance,testingwithexperience-based
sampling is the most commonly applied strategy, while systematic
sampling is rarely applicable. We discuss analyses that are missing
and synthesize our insights into suggestions for future research.
CCS CONCEPTS
•Software and its engineering →Software product lines ;•
General and reference →Empirical studies ;
KEYWORDS
Highly configurable systems, software product lines, analysis
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ASE ’18, September 3–7, 2018, Montpellier, France
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5937-5/18/09...$15.00
https://doi.org/10.1145/3238147.3238201ACM Reference Format:
MukelabaiMukelabai,DamirNešić,SalomeMaro,ThorstenBerger,andJan-
PhilippSteghöfer.2018.TacklingCombinatorialExplosion:AStudyofIndus-trialNeedsandPracticesforAnalyzingHighlyConfigurableSystems.In Pro-
ceedingsofthe201833rdACM/IEEEInternationalConferenceonAutomated
Software Engineering (ASE ’18), September 3–7, 2018, Montpellier, France.
ACM,NewYork,NY,USA, 12pages.https://doi.org/10.1145/3238147.3238201
1 INTRODUCTION
Engineering a highly configurable system allows addressing vary-
ingcustomerneeds,reducingtime-to-marketofnewsystemvari-
ants, and experimenting with new ideas. Large configurable sys-
tems can easily exhibit thousands of configuration options, lead-
ing to almost infinite configuration spaces. Software product lines
(SPLs)[1,20], software ecosystems[ 12,15], and personalization-
capablesystems—especiallyintheautomotive,avionics,telecom-
municationorpower-electronicsdomain—arecommonexamplesof
highlyconfigurablesystems.TheLinuxkernel[ 14]boastsaround
15,000ofconfigurationoptions,supportingdifferenthardwarear-
chitectures, software features or runtime environments ranging
from Android phones to large supercomputer clusters.
Engineering highly configurable systems is challenging due
tovariability—thenumberofconfigurationsandsystemvariants
grows exponentially with the number of configuration options.
ConsiderthepopularanalogybyKruegeretal.[ 35],whereasys-
tem with 320Boolean, non-constrained configuration optionshas
more configurations than estimated atoms in the universe. Over
void __init
init_IRQ(void)
{
#ifdef CONFIG_ACPI
acpi_boot_ini();
#endif
ia64_register_ipi();
register_percpu_
irq(...);
consistency properties (Sec. 4.5)code properties 
(Sec. 4.6)Variability model 
properties (Sec. 4.4)mapping
IA64
ACPI
 PCI
 PM
ACPI oPCI PMgeneral system properties (Sec. 4.3)
variability model
(option specification)codebase
Figure 1: Architecture of a highly configurable system and
categories of typical properties to assure or analyze
155
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Mukelabai, Nešić, Maro, Berger, and Steghöfer
thelast decades,manydevelopment techniques forhighlyconfig-
urable systems have been conceived, mainly in the field of product
lineengineering.Whileitsdevelopmentconceptshavewellbeen
adopted in industrial practice—consider the product line hall of
fame (splc.net/hall-of-fame) and case studies[ 61,69]—this is much
less clear for product-line analysistechniques.
Figure1shows a typical architecture of highly configurable sys-
tems. It consists of source code with configuration mechanisms
(here,preprocessordirectives),aspecificationofconfigurationop-
tions (a.k.a., features[10]) and their dependencies in a variability
model (here, a feature model[ 14,30]), and a mapping between
both (typically the build system). Properties to assure relate to: the
whole system with all configurations ( general system properties,
e.g., reliability or performance), the codebase ( code properties, e.g.,
optionscatteringornesting),theoptionspecification(variability
model properties, e.g., absence of dead options), and the mapping
(consistency properties, e.g., absence of dead source code).
Analysis techniques for highly configurable systems differ from
traditionalanalyses.Eventhough,thelattercanbeusedonindivid-
ual variants(e.g., for optimization)when the systemis configured
by the system vendor, using traditional analyses is not sufficient to
detecterrorsthatpertaintoallpossiblevariants,especiallywhen
customersconfigureit.Yet,evenwhenthevendorisincontrolof
theconfigurations,applyingtraditionalanalyses,suchastesting,
still requires effective configuration-sampling strategies[21, 53].
Creating analysis techniques for highly configurable systems by
lifting single-system analyses (e.g., model checking or type check-
ing)[47,66],hasreceivedsubstantialattentionoverthelastdecade.
Suchvariability-awareanalyses [66]elicitacertainsystemproperty
for all possible configurations . A recent survey[ 66] identified 123
analyses from the literature, including lifted type-checking, static-
analysis,andmodel-checkingtechniques.Aloneforfeaturemodels,
aneightyearoldsurvey[ 8]identified—in53publications—30dif-
ferent analyses, realized upon a multitude of solvers with different
logical representations (e.g., propositional logic).
Althoughavastvarietyofsuchanalyseshasbeenproposedin
the literature[ 6–8,66], it is largely unknown whether and how
these are used. Specifically, we lack empirical data whether the
proposedanalyses for highlyconfigurablesystemsareadoptedin
practice, whether they address actual needs or find errors, or what
analysis strategies are actually applied.
We present a study on the needs and practices for analyzing
highly configurable systems in industry. We combined a survey
with27employees—ofcompaniesrangingfromlessthantentoover
200 employees working on highly configurable systems ranging
fromlessthan25,000linesofcodetooveronemillionlinesofcode,
containing between ten to over 10,000 configuration options—with
in-depth interviews of 15 survey participants. Our study design
reliedoncategorizinganalysistechniquesfromtheliteratureand
identifying properties analyzed by them; we used these to elicit
theneedforandtheseverityofanalyzingtheproperties.Wealso
elicitedindustrialpractices.Sinceitisintrinsicallydifficulttoob-
jectivelyunderstandtherealpracticesandmapthemtothestateof
research,wetriangulatedresultsfromthesurveyandinterviews,
steering the latter based on the survey responses, and carefully
analyzing the results iteratively.
Our research questions are:•RQ1: What are important properties of highly configurable
systemsthatshouldbeanalyzed? Weelicitedtheperceived
severity and reasons for analyzing the properties we identi-
fied from the literature (in the categories shown in Figure 1)
and those expressed by the practitioners.
•RQ2: What are industrial analysis practices? We asked our
participants about established (textbook) analysis tools and
techniques,andadditionalpracticestheyapply.Wealsodug
deeper into specific ones to understand them qualitatively.
We contribute: (i) empirical data on the needs and state-of-
practiceofanalyzingconfigurablesystems,(ii)synthesizedinsights
organized in categories inspired by the architecture of highly con-
figurable systems and a classification of existing analyses from
the literature, (iii) a discussion of our study results and their im-
plicationsforresearchersandpractitioners,and(iv)areplication
package with further study details in an online appendix[65].
We proceed by discussing the background and related work on
theanalysisofhighlyconfigurablesystemsinSection 2,followed
by the design and data analysis procedure for our survey and in-
terviewsinSection 3.WereportresultsinSection 4,organizedas
insights covering the perceived relevance of various analyses and
reported challenges. We summarize the impact of our study andpropose possible directions for research in Section 5. We discuss
threats to validity in Section 6and conclude in Section 7.
2 BACKGROUND AND RELATED WORK
We introduce highly configurable systems and software product
lines, to discuss analyses that have been proposed in the literature,
as well as works related to our study.
2.1 Highly Configurable Systems
Highlyconfigurablesystemsofferconfigurationoptions(a.k.a.,fea-
tures[10,18,36] or calibration parameters) that can be of different
types (e.g., Boolean or integer). Options are used in implementa-
tion artifacts (e.g., source code, test cases, requirements), either to
parameterizefunctionalityorcontrolvariationpoints.Thelatter
canberealizedusingdifferentmechanisms,includingpreprocessordirectivesforconditionalcompilation(e.g., #IFDEFs),option/feature
toggles (e.g., simple IFs), build systems or component frameworks.
Theavailableoptionsandtheirdependenciesneedtobedeclared,
either in a dedicated variability model (explained shortly), a tex-
tual configuration/properties file or a database; or informally, such
as in a spreadsheet. Models can be input to a configurator toolthat guides users to a valid configuration, which binds variationpointsorparameterizesthesystem.Wegenerallyrefertooption
specifications as variability models.
Softwareproductlines—portfoliosofsystemvariantsinaspecific
domain—aretypicallyrealizedasahighlyconfigurablesystem.The
options (called featuresordecisions) are described in a dedicated
variabilitymodel,suchasafeature[ 11,14,30]oradecision[ 23,60]
model.Notethatweusetheterms optionandfeaturesynonymously.
Popular, feature-model based, configurator tools are pure::variants,
Gears, FeatureIDE or the Linux kernel configurator[ 5,14]. The
research field of product-line engineering has conceived a large
number of analysis techniques.
156
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Tackling Combinatorial Explosion: A Study of Industrial Needs... ASE ’18, September 3–7, 2018, Montpellier, France
2.2 Analysis Techniques and Tools
Analysis techniques for highly configurable systems have been
studied for almost two decades by now.
DynamicAnalyses .Aswewillshow,oneofthemostpopulardy-
namicanalysisforhighlyconfigurablesystemsinindustrialpractice
is testing[ 25,39,51]. Several testing techniques for configurable
systems are summarized by Engstrom et al.[ 25], with rather few
that have been evaluated through experiments or industrial case
studies,includingtechniquesforintegrationandsystemtesting[ 56]
as well as performance testing [55].
Testing is always specific to one configuration, Since it is of-
ten impossible to test all configurations, configuration sampling
strategies have been proposed[ 21,28,45,53], including random
sampling or more systematic sampling strategies that try to enable
each option or certain option combinations (e.g., n-wise feature
interaction sampling)[53].
Static Analyses . In order to effectively analyze all possible con-
figurations, a class of static analyses, called variability-aware anal-
yses,hasbeendevelopedoverthelastdecade,typicallybylifting
single-system analyses, such as dataflow analysis[ 42,58], model
checking[ 37] or deductive verification[ 68]. Also defect prediction
usingmachinelearningwasproposed[ 54].AccordingtoThümet
al’ssurvey[ 66],theseanalysescancheckpropertiesofthewhole
system (family-based analysis) or of individual options in isolation
(feature-based analysis).Examplesofpropertiesaretype-safety[ 31],
performance[ 27],temporalproperties[ 19],orabsenceofunwanted
featureinteractions[ 2,3,16,29].UnlikeThümetal.’ssurvey[ 66],
which identifies the state-of-the art, our study aims at present-
ing thestate-of-practice—offering insightsabout theadoption and
potentialchallengeswhenusingexistingvariability-awarestatic
analyses.
VariabilityModelAnalyses .Avastnumberofanalyseshasbeen
proposed for variability models (especially feature models) as well
as analyses to check consistencies between variability models and
implementationartifacts.RecallBenavidesetal.’ssurvey[ 8].Among
the 30 different analyses on feature models are, for instance, check-
ingmodelsatisfiability(i.e.,atleastoneconfigurationexists),check-
ing validity of configurations, counting or enumerating configura-
tions,andfindingdeadfeatures.Themajorityoftheseanalyseshas
beenevaluatedonrathersmall,artificialmodels[ 14,33].Ourstudy
iscomplementarytoBenavidesetal.’ssurvey[ 8],sincewematch
theseanalysestoindustrialneeds.Finally,therearealsoanalyses
that reason about feature-model edits[ 67], as well as analyses that
elicit quality properties on models using various model metrics[ 9],
such as the maintainability of a model[4].
ConsistencyandCodeAnalyses .Furthermore,to preventincon-
sistenciesbetweenvariabilitymodelsandimplementationsartifacts,
consistency-related analyses have been proposed[ 38,48,50,57,63,
64,70], many relying on SAT solvers, for instance, to check that
codeconstraintsareconsistentwithvariability-modelconstraints
(e.g.,tofinddeadcode[ 63,64]),buttherearealsoanalysesusing
theoremproving[ 59]ormodelchecking[ 26].Someofthesehave
beenappliedinpractice[ 34].Furthermore,assuringstructuralcode
properties is also relevant. Studies on such properties focus mainly
on the structure of variation points[ 40,41,71], but do not elicit
needs or practices related to source-code analyses.Initial survey Final Interview
guide 
5. Update survey 
Final survey 
questionnaire4. Test survey on four respondents + interview them 7. Invite 81 respondents6. Update interview guidefinal interview 
guide8. Invite survey respondents to interview 15 
accepted9. Conduct interviews
	1. Design initial interview guide 
2. Conduct test interview
3. Create Survey to familiarize interviewees with terminology and concepts 
g
intervie

	
3
f
w
g
Figure 2: Study design
Analysis Tools . As shown in a recent survey[ 46], many of the
analysesabovearetool-supported,butallareresearchprotoypes.
Some industry-scale evaluations of specific tools have been con-
ducted on industrial systems, such as consistency checking[ 70].
Otherevaluationsrelyonopen-sourcesystems,suchastheLinux
kernel or other systems software[ 21,32,43]. Even though these
evaluationsshowtheeffectivenessofatechnique,theydonotin-
vestigate its practical importance.
3 METHODOLOGY
Ourstudydesignaimedatobtaininginsightsaboutanalysesthat
areappliedinpracticeandabouttheneedsofpractitioners.Thereby,
we also strive to obtain insights about the relevance of analyses
that have been proposed in the literature. Recall that most of these
proposed analyses are automated, yet, we do not narrow our scope
totheseonly,butalsoconsidermanualanalyses.Tothisend,we
studiedliteraturesurveys,specificallyChasteketal.[ 17],Benavides
et al.[8], and Thüm et al.[ 66]. We designed a structure, both for
the survey questionnaire and the interview guide, based on a clas-
sification of analysis techniques for product lines, largely inspired
byChasteketal.[ 17]andthearchitectureofhighlyconfigurable
systems, as we illustrated in Figure 1. This structure comprised:
(i)analysisofgeneralsystemproperties,(ii)testingpracticesand
challenges,(iii)analysisofthevariabilitymodel,(iv)consistency
analysis between variability model and implementation artifacts
(e.g., code); (v) analysis of implementation artifacts. The typical
properties,analyses,tools,andchallengesidentifiedinthelitera-
ture were distributed in these categories. Specifically, instead ofasking about particular analysis techniques, which are likely un-
knownto theparticipants, ourfocus wasonproperties regarding
thefivecategories.Forinstance,weextractedeight(inouropinion)
most relevant properties of variability models (explained shortly inSection4.4),forwhichanalyseshadbeenproposed,fromBenavides
et al.’s literature survey[8].
Fromexperience,weexpectedmanytermsfromtheliterature
to be unknown to industrial participants, so we developed the
questionnaireandinterviewguideiterativelywithfiveindustrial
test participants. Figure 2shows our methodology. First, we de-
signedaninterviewguidebasedonthecategoriesandconducted
a pilot interview. The experiences led to the first version of the
survey questionnaire, which we then tested with four industrial
157
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Mukelabai, Nešić, Maro, Berger, and Steghöfer
participantsfromtheautomotive,powerelectronics,andcamera
systems domain. We then continued refining the terminology and
slightly adjusting the scope. For instance, we replaced product-line
terminologywithmorecommonterms(e.g., productline became
highly configurable system, variability model became configuration
specification). Furthermore, we added testing (and configuration
sampling), as the most frequently expressed analysis technique by
thepilotparticipants.Weconductedfurtherpilotinterviewswith
these participants, learning that a survey questionnaire before the
interviewhelpstofamiliarizeintervieweeswiththeterminology
and structure, and so allowed going deeper into certain aspects,
reducing unnecessary clarifications.
3.1 Survey Questionnaire
Design. We designed the survey questionnaire to take no more
than25minutestocomplete.Itfirstelicitedbriefrespondentethno-
graphics(e.g.,professionalbackgroundorexperience),thengeneralcharacteristicsoftheconfigurablesubjectsystem(e.g.,size,domain,
and architecture), and then covered the five analysis categories
in detail. Finally, we asked about general challenges. The whole
questionnaire is available in our online appendix[65].
All questions were optional to allow respondents to leave out
thosequestionstheydidnothavetherighttechnicalbackground
in or did not know sufficient details about. Most questions were
closed-ended;forsomeweaskedforadditionalelaborations.The
questions about the importance of analyzing specific properties
usedafour-pointscaleaskingparticipantstoratetheimportanceof
analyzing the property: not necessary ;nice to have, but not critical ;
critical; anddon’t know. This was followed by questions on which
property is assured and how.
Respondents .Tradingthenumberofresponsesforanincreased
confidenceinthem,wesent81invitationstoourindustrialpartners
(manufacturers,consultingcompanies,toolvendors),alsoasking
them to kindly forward the invite to colleagues or collaborators.
We received 54 responses (response rate: 66.7%). 43% were com-
plete responses and 57% were partial. From the latter, we disquali-
fied 50% for the lack of responses to any of the analysis sections
(even whenethnographics, system characteristics,and challenges
were filled in), so only 27 (50%) of the 54 respondents were con-
sidered. This indicates that many developers, who work on highly
configurablesystems,arenotfamiliarwithrespectiveanalysesat
all. Excluding these enhances the confidence in our results. In the
remainder, we refer to individual respondents as S1, S2, and so on.
3.2 Interviews
Design. The follow-up, semi-structured interviews relied on an
interviewguidelargelyfollowingthequestionnairestructure.We
first verified basic ethnographics with the interviewee and char-
acteristics of the subject system, which helped getting into theinterview. For the main part, our strategy was to first briefly an-alyze the survey responses for each section and then going into
depthaboutcertainpropertiesperceivedimportantorotheraspects
we foundrelevant. The sectionsand our preliminaryanalysis pro-
vided a structure, but we let the interview flow freely. This way,
we obtained additional insights into analysis needs, practices, andchallenges. For instance, we asked why a certain property was per-
ceivedascritical,whyitwasanalyzedornot,andwhatwouldbe
needed to do so (e.g., presentation or scalability requirements). We
also engaged the interviewee to discuss the feasibility of applying
existing analyses she might not be aware of.
Interviewees .Weinvited23surveyrespondentswhocompleted
allquestionnairesectionsforinterviews,amongwhich15accepted.
Theinterviewswereconductedinpersonorviaphone,typically
lasting around one hour or slighly longer, totaling to 17 hours
altogether.Theinterviewswererecorded,storedsecurely,andtran-
scribed.Intheremainder,werefertoindividualintervieweesusing
I1, I2, and so on.
3.3 Data Analysis and Interpretation
We analyzed the survey quantitatively by creating diagrams and
manually aggregating responses to questions, and qualitatively by
inspectingresponsestotheopen-endedquestions.Forobtaining
trendsabouttheimportanceofcertainproperties,wecreatedviolin
plots(note,thediagramsintheremainderomitthe“don’tknow”
responses)byassumingacontinuousscale,carefullyinterpreting
thesetrends,triangulatingwithqualitativedata.Violinplotsindi-
catewheremostanswersofthefour-pointscaleare;forinstance,
if most answers regarding the relevance of a particular property is
towardscritical,theplotwillbethickatthatpoint.Theblackdot
indicates the median. To analyze the interviews, we used open cod-ing[
62](amethodfromgroundedtheory)[ 22]:threeauthorscoded
the interviews iteratively, continuously discussing and refining the
codes,butalsohavingacodingworkshopwithallauthors,untilthe
codeswerefinalized.Basedonthecodes,wetriangulatedresults
from the survey and interviews; for instance, crossing information
that a certain system property is perceived as critical, with the
details from the respective interview. We report the results by first
providing details about the systems, survey respondents, and inter-
viewees, and then by using a narrative style for the main findings.
Whiletherelativelylownumberofsurveyresponsesandinterviews
didnot allowstatisticalhypothesistesting, wecross-checkedfor
supportinthesurveydatawheninterestingconjecturesemerged
during analysis.
4 RESULTS
We present our results by first describing our survey respondents,
interviewees,andtheirconfigurablesystems.Tables 1and2sum-
marize the latter two. We then provide findings related to our five
main analysis categories. We limit descriptive results and exactstatistics, which can be found in our appendix[
65], but instead
highlight and discuss our main findings. Table 3summarizes the
needs and practices observed.
4.1 Survey Respondents and Interviewees
Themajorityofsurveyrespondentscompriseddevelopers(70%),
followedbysoftwarearchitects(48%)andteamleaders(44%).Al-
most half had over ten years of experience working with highly
configurablesystems,athirdbetweenfiveandtenyears.Theethno-
graphics of our interviewees largely resembled those of the survey
respondents (only the average experience deviated slightly).
158
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Tackling Combinatorial Explosion: A Study of Industrial Needs... ASE ’18, September 3–7, 2018, Montpellier, France
Table1:Systemcharacteristicsstatedbysurveyrespondents
domain system size (LOC)
automotive 27 % < 25,000 4 %
industrial automation 20 % 25,000–50,000 4 %
aerospace&defense 12 % 50,001–100,000 12 %
consumer electronics 8 % 100,001–500,000 19 %
telecommunication 8 % 500,001–1,000,000 27 %
ofﬁce 8 % > 1,000,000 35 %
other 20 %
team size number of conf. options
< 10 27 % 10–50 15 %
10–50 39 % 51–100 31 %
51–100 19 % 101–500 12 %
101–200 4 % 501–1,000 8 %
> 200 12 % 1,001–10,000 19 %
> 10,000 15 %
configurable artifacts option specification
source code 89 % conﬁguration ﬁle 44 %
models 70 % variability model 41 %
test cases 52 % in source code 41 %
runtime conﬁguration ﬁle 52 % in conﬁgurator tool 33 %
requirements 48 % database 22 %
other 19 % other 11 %
options with dependencies variability mechanism
none 20 % option/feature toggles 67 %
1–25 % 36 % conﬁgurable build system 67 %
26–50 % 20 % conditional compilation 48 %
51–75 % 12 % other 30 %
76–100 % 12 % component/service framework 26 %
4.2 System Characteristics
Table1summarizesoursubjects’highlyconfigurablesystems,which
coverawiderangeofdomains,mainlyautomotive,industrialau-
tomation, and aerospace and defense, among others. The largest
systems(15%ofthem)exhibitover10,000configurationoptions.
As expected, the most frequent configurable artifact is source code
(89%), followed very frequently by models (70%), but half of the
surveyrespondentsalsomentionedtestcases,requirements,aswell
as runtime configuration files.
Since 70% of the survey respondents stated that they configure
models,weaskedourintervieweesaboutthekindsofmodels.These
weredomain-specificmodels,suchasaircraftsimulatormodels(I4)
intheaerospaceanddefensedomain,andSimulinkmodelsinthe
automotive domain (I15).
Closetoaquarteroftherespondentsdoesnotdeclaredependen-
ciesbetweenconfigurationoptions,whilethemajorityofthosewho
declare them only does so for 26–50% of the options. For instance,
interviewee I2 stated that only few dependencies, mostly optional,
aremodeledtoreducecomplexity.Thesedependenciesaredomain,
software, and hardware constraints (I1). For the systems where
dependencies are partly or not at all modeled, our interviewees
explained that this is because: (i) undeclared dependencies do not
cause problems due to the small number of variants (I3, I6, I13),
(ii)relationshipsforsomeoptionsaredeterminedatruntime(I4),
(iii) there is lack of time and money, since setting up rules and
calculating possibilities or consequences for certain rules takes too
much time (I4), or (iv) there are too many implicit dependencies in
the source code (I5) that cannot be expressed.
4.3 Analysis of General System Properties
Reliability (67%), performance (65%), absence of feature in-Table 2: Interviewee ethnographics
role exp.1domain size2
I1 Team leader, Domain expert,
System architect5-10 industrial automa-
tion100K-
500K
I2 Department lead > 10 industrial automa-
tion>1 M
I3 Developer, Team leader, Do-
main expert, Software architect> 10 aerospace & defense 100K-
500K
I4 Developer, Conﬁguration Man-
ager> 10 aerospace & defense > 1M
I5 Team leader, Software architect 5-10 automotive > 1M
I6 Developer, Domain expert,
Software architect> 10 telecommunication 2.5M
I7 Project manager, Product man-
ager5-10 banking/insurance 500K-1M
I8 Developer 5-10 automotive 50K-100K
I9 Developer, Modeler, System
architect5-10 automotive
I10 Developer, Software architect 5-10 automotive 500K
I11 Developer 5-10 heat pumps
I12 Developer > 10 automotive
I13 Developer, Team leader > 10 automotive 1.2K
I14 Developer, Domain expert,
System owner, System architect> 10 network cameras 50M
I15 System owner 5-10 automotive > 1M
1experience with the system in years2system size in lines of code
teraction(65%),behavioralcorrectness(62%),andsafety(44%)
are perceived as the most important system properties .A s
shown in Figure 3, assuring these properties for each configura-
tion is primarily perceived as critical, unlike properties such as
security or maintainability. While the differences are small and
likelynotstatisticallysignificant,thereportedpercentagesconfirm
theexpectedrelevanceoftheseproperties,whichhavebeencon-
sidered in the literature. Furthermore, for example in the case of
reliability,twosurveyrespondents(S1,S6)andtwointerviewees
(I2, I5) perceived reliability as encompassing other properties such
as safety and security. For instance, a system that crashes due to
some “unforeseen combinatorial path through the code” is deemed
unreliable, insecure, and not meeting safety standards (S1).
Weobservedstrongperformancerequirements,whicharecon-
firmed to be challenging to assure for all configurations. Their
violation affects safetyand can have serious consequences(I5, I12,
S6).I12:“Wehavetime-criticalperformance[requirements],deviation
of 2ms is a fail.” S6:“Our system is used to test telecom equipment
understress.Itiscriticalthattheperformancereportfromoursystem
isaccurategivenanyconfigurationorcombinationofconfigurations.”
With the exception of cost constraints, all other system proper-
ties(showninFigure 3)weaskedaboutareassuredby,atleast,29%
of the survey respondents, who confirmed assuring one or more
properties.Asexpected,reliabilityisassuredbythemajority(79%),
followed by behavioral correctness (71%), performance (50%), and
safety constraints (43%). Safety was largely related to legislation,
especiallyintheindustrialautomationandautomotive(e.g.,carbon
emission or vehicle safety) domain (I2, I10).
None of our survey respondents or interviewees assured cost
constraints,butsomehaveexpressedthelackofandtheneedfor
tools that would help in cost analysis, for instance, tools to analyzethecostofaddingorremovingfeatures(I11,S17),oranalyzefeature
resource-consumption (I1, I14).
Testing and manual reviews are the most prominent analy-
sis practices . All survey respondents in fact do testing (100%) for
assuringrelevantsystemproperties,followedbymanualreviews
159
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Mukelabai, Nešić, Maro, Berger, and Steghöfer
Table3:Overviewofreportedneedsandassociatedpractices
reported analysis needs reported analysis practices1
analysis of general system properties (Section 4.3)
analyze reliability, performance, be-
havioral correctness, safety, unwantedfeature interactionstesting (100 %), manual reviews (67 %),formal methods (20 %)
conﬁguration sampling for testing experience-based sampling (67 %), system-
atic sampling (13 %), random sampling
(7 %)
alleviate need for complex analyses modularization of features to enable feature-
based testing using unit tests or to enablefeature-interaction testing using integration
tests; modularization limits test runs
targeted test case selection –
2
traceability between failed test cases
and conﬁguration optionsexploit recorded database snapshots of con-ﬁguration option values
variability model analyses (Section 4.4)
analyze conﬁguration validity, modelsatisﬁability, number of conﬁgura-
tions, validity of editsmanual review (64 %), scripts (55 %), testing
(18 %), conﬁgurator tool (18 %)
analyze impact of quick ﬁxes and
small changes on variability model–2
reduce test effort analyse possible conﬁgurations
identify technical debt, lack of codingdiscipline or missing knowledgeanalyse anomalies of variability model (re-dundant constraints or dead options)
consistency analyses (Section 4.5)
assure consistency of the variabilitymodelmanual review (80 %), automated techniques
(40 %), testing (30 %)
fast, ﬂexible automated consistency
checking embedded in continous
deployment toolchain–2
integration of tools handling differentartifactsmanual work (e.g., snapshots and manualimport/export)
source-code analyses (Section 4.6)
variability-aware static analysis tools non-variability-aware static analysis tools
(80 %), manual reviews (70 %), proprietary
scripts (40 %)
reduce need for complex analyses adhere to coding standards (90 %), adhere to
company-speciﬁc style guides (80 %)
avoid deeply nested ifs and #ifdefs code reviews
feature interaction analyses (Section 4.7)
show absence of feature interactions manual reviews, extensive testing
analyze feature interactions in loosely
coupled architectures–2
1percentage of survey respondents stating the practice (where applicable)
2no practice reported
(67%). Only 20% use formal methods, such as model checking. The
manual review process usually entails manual inspection of one
ormoreartifactsbythedomainexperts,forinstance,I12reports
that absence of feature interaction is assured by manually compar-
ingfeaturespecifications,andtryingtocomeupwithpotentially
unwanted interactions.
70%ofoursurveyrespondentsuseregressiontesting.Apractice
hereistoperformregressiontestingonlyforafewselectedmain
configurations(I2),usually3–4,becauseregressiontestscanbetime-
intensive(twodaysinthecaseofI2)andtheresultsareanalyzed
manually by developers. On the other hand, I6 stated that theirregression tests are run on the stable release of the main branchand are intended to test that “all the features in the stable branch
still work reliably.”
Additionally, unit and integration testing are also used (I1, I6,
I11).ForI6,unittestsareperformedbyfunctiondevelopersandde-
signersto verifyuser-visible functionalrequirements, afterwhich● ● ●
● ● ●
● ●Correctness Reliability Security.
Cost Safety. Liveness
No unwanted feature interactions Performance
123 123
Legend: 1−not necessary, 2−nice to have, 3−critical
Figure 3: Importance of assuring general system properties
integration testing is performed for components to include test-
ing of “non-functional requirements.” Similarly for I11, test scripts
are written for features, when activated and when disabled, for
individualmodules,andforintegratedmodulesI11: “Wehavetest
scripts for both the small modules and also all modules together inthe main module [...] But when we implement a new feature that
isconfigurable,thenweshoulddoatestscriptforthefeaturethatit
works as it should and we should also do a test script that doesn’t do
anything when it’s not configured.”
Experience-basedconfigurationsamplingisdominant,while
systematic sampling is usually not applicable .87%ofsurvey
respondentssampleconfigurationsbyconsideringknownfeature
interactions, largely based on developer experience (67%). Only
13% use systematic sampling (e.g., sample all combinations of two
options), and 7% use random sampling.
Thereasonswhysystematicsamplingisnotusedare:(i)there
are too many meaningless combinations to test (I5) and (ii) impor-
tant configurations are usually known (I1, I2, I3, I6). Since most
companiesrelyontesting(seeabove),experienceisusedtosample
relevantconfigurations.Whiledeclaringdependenciesinavariabil-
itymodeltoruleoutmeaninglesscombinationscouldreducethe
configurationspaceamongwhichtosystematicallysample,even
this would leavetoo many meaningless configurations.Moreover,
declaring such dependencies is already perceived as challenging
owing to the high effort required to do so. I5: “In general, that’s one
of the concerns when the modeling is being done, the effort to foresee
all the possible constraints and dependencies sometimes is too high to
have really any effect.”
We dug deeper into the kind of experience used for sampling.
Accordingtoourinterviewees,itcomesfrom:typicalusecasesora
representativesetofconfigurationsthatcustomersuse(I1,I2,I3,I6),
standardorcriticalconfigurationsforacompany(I1,I2),knowledgeoffailuresthatoccurintheafter-market(I2),andrelevanttestcases
frequently executed based on developer knowledge of what might
go wrong (I5). For instance, S3: “Our regression test are based upon
theautomatedversionofourqualificationtests.Somepartshavebeen
made specifically to catch some typical issues we have seen.”
Insome cases,combinations ofthese experiencesareused. S22:
“[...] includes both configuration options (V8 engine, long-haulage
truck,etc.)andtestingconditions(hotclimate,mountainousterrain,
etc.). [...] the testing department would usually define some config-
urations/testsbasedonexperiencetoincreasetestingcoverage.” Itis
alsopossibletocombinesystematicandexperience-basedsampling
due to a very large configuration space. S6: “We try to test all com-
160
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Tackling Combinatorial Explosion: A Study of Industrial Needs... ASE ’18, September 3–7, 2018, Montpellier, France
binations(automatedtests),butusuallysomecombinationsarenot
possible due to large variability, therefore, we often prio some tests to
automate, and that is usually based on experience.”
Feature- and family-based analyses are used, but in a very
limited way . Different strategies are employed to avoid sophis-
ticated analyses. Modularization of features was stated by 70%of survey respondents as one such strategy. Three interviewees
(I1, I6, I11) use unit testing for individual modules, and integration
testingfor integrated modules. I1: “We have two approaches. One
is [...] to cover feature by feature that we make sure that it works[...] as intended. And then we have what we call confident testing
where we more look at the applications where you have bundles of
featuresthatareworkingtogether.[...]notnecessarilycoveringall
the requirements of every feature but covering [...] the interplay
between the features [...]” Based on the classification of analysis
strategiesbyThümetal.[ 66],wefoundthesetobepragmaticcases
of feature-based analysis, where configuration options are ana-
lyzed in isolation, and family-based analysis, where interactions of
configuration options are analyzed together. However, among our
participants,thisonlyworksforcoarse-grainedfeaturesthatare
modularized.Wefoundnoevidenceofanyparticipantbeingable
to do this for cross-cutting or more fine-grained features.
Effectivetestingrequirestargetedtestcaseselectionandtrace-
ability.Theneedswithrespecttotestingcanberoughlycatego-
rizedintotestcaseselectionandtraceability.Theformerisseparate
from sampling strategies (where configurations are selected), since
it pertains to the selection of the relevant test cases targeting dif-
ferent code qualities such as criticality or safety (I1), as well as
selecting test cases that target specific features (I5). This is often
challengingbecauseteststhemselvesarerarelyconfigurable(I2).
Traceability is relevant for understanding which configuration op-
tions were selected when a concrete test case failed (I3, I5). Thiscan be addressed by using a database snapshot of configurationsand their values to identify which configurations are affected by
bugs reported under a specific customer configuration (I5).
4.4 Variability Model Analyses
Satisfiability of a variability model and validity of a config-uration are perceived as the most critical properties
. Recall
thatweelicitedtheeightmostrelevantvariability-modelproper-
ties(cf.Section 3)forwhichanalyseshavebeenconceivedinthe
literature[ 8]. As Figure 4shows, only two properties are perceived
as critical, by the median of respondents: validity of a configura-
tion—an existing configuration adheres to the variability model
(85%)—and satisfiability of the model —at least one configuration
exists for the model (56%).
We asked interviewees why these two properties are deemed
more critical despite them being available in many configuratortools.Theystated thatthese twoproperties are mostcritical dueto quick fixes or one-off changes made to the variability model
for some customers (I2). Such changes have the potential to easily
invalidate the model, resulting in the system not working at all
(I5). Consequently, expensive and time-consuming (and reportedly
sometimes financially risky) rework has to be done to ensure that
the configurable system works (I5, I7).The properties of the variability model are mostly assured
through manual reviews . Thesurvey responsesshow thatonly
four of the properties from Figure 4, excluding those related to
edits, are assured: (i) validity of a configuration and (ii)satisfiability
of a specification, both assured by 85% of the respondents, (iii)
number of possible configurations by 33%, and (iv) list of all possible
configurations by 17%. The validity of variability-model edits is
assured by 33% of the respondents. The majority (64%) assures
thesepropertiesthrough manualreviews,55%use scripts(e.g.,to
collect statistics on configuration options and their references),
18% use testing(e.g., regression testing), and only 18% use the
capabilitiesof configuratortools.Notably,I7usesmodelchecking
to assure the validity of configurations. Since only 28% of thesurvey respondents stated to specify configuration options in a
configuratortool,andthatover85%specifythemdirectlyinsource
code or use a textual configuration file, it is not surprising that
these properties are mostly assured manually or through tests.
Change-impactanalysisofmodelchangesisimportant,but
shouldnotbelimitedtothemodel .Whileallintervieweesoften
makeeditstotheirvariabilitymodels,e.g.,addinganewconfigu-
ration option or changing dependencies, surprisingly, analyzing if
sucheditsleadtomoreconfigurations(generalization),fewerconfig-
urations (specialization) or maintain the number of configurations
(refactoring)—typical analysis reported in the literature [ 8,67]—
is not perceived as important. A minority of survey respondents
perceives thisanalysis ascritical (20%for generalization,17% for
refactoring,and16%forspecialization),whilethemajorityperceive
itasnicetohave(58%forrefactoring,and48%forspecialization
andgeneralization)orunnecessary(28%forspecialization,24%for
generalization,and17%forrefactoring).IntervieweeI5explained
thatsuchanalysisisnotcriticalaslongasrelevantconfiguration
options get activated and the system operates as expected. Two
interviewees, however, perceived this analysis as critical, since it isnecessarytogaugetheimpactofthechange(I10)ortoexhaustively
test a new addition (I15).
Fromsuchexplanations,welearnedthatanalyzingtheimpact
ofeditstovariabilitymodelsmeansmorethanknowinghowthe
numberofconfigurationsisaffected.Itismoreimportanttoanalyze
the impact on other artifacts or the whole system. Indeed, fiveinterviewees requested specific analyses: (i) assess how a model-
change impacts existing configurations (I10, I15), implementation
artifacts,andsystemcomplexity(I1,I14),(ii)assessimpactofoption
name change without changing its structure and vice-versa (I1),
(iii)assess impactof splittingan option intotwo ormore features
(I1), or (iv) easily understand the impact of switching options on
and of (S16). None of these analyses currently exists.
Finally,oneintervieweeexplainedtheneedforenhancedchange-
impact analysis as a tradeoff of adopting a highly configurable
system (instead of using clone&own[ 24] for realizing variants. I1:
“We do not clone, but the complexity moves from doing so to the
[variability model] [...] where a need for a change potentially affects
allprevious(aswellasfuture)configurations,anditcanbehardto
know the impact and to ‘fix’ previous configurations.”
Knowingthelistandnumberofpossibleconfigurationshelps
reducingtesteffort .Althoughnotconsideredcriticallyimportant
161
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Mukelabai, Nešić, Maro, Berger, and Steghöfer
●
●
●
●●
●
●
●●
●
●
Edit is a generalization Edit is a refactoringAbsence of false−optional options Explanation for problems Edit is a specializationList of all possible configurations Absence of redund. dependencie s Absence of dead optionsVar. model is satisfiable Configuration is valid No. of configuration options
123 123123
Legend: 1−not necessary, 2−nice to have, 3−critical
Figure 4: Importance of assuring various properties of the
variability model
bythemajorityofsurveyrespondents,the listofpossibleconfigura-
tionsandnumber of possible configurations were considered critical
by23%and15%,respectively.Aswelearned,theyhelpreducing
test effort. I12: “Every new combination of parameters that we do
resultsinabout threemonth’stimeoftesting.So, everythingwecan
do to keep down the amount of configurations is gold for us. So, I
would say that, that is not only nice to have, that is critical.”
Anomalies of a variability model (redundant options, false
optional options, and dead options) indicate a lack of disci-
plineinadherencetocodingguidelinesandalackofknowl-edgeofasystem
.Sinceitisperceivedcriticallyimportantby21%
ofoursurveyrespondents,weaskedtherespectiveinterviewees
whytheyperceivedtheanalysisofanomaliesofavariabilitymodel
as critical. Two interviewees stated that redundant constraints and
dead options indicate: a lack of discipline in adhering to codingguidelines(I6),missingknowledgeofone’sownsystem(I1),and
that false options indicate a bad smell or technical debt (I1).
Business priorities and the combinatorial explosion deter-
mine the criticality of addressing anomalies of a variabilitymodel
.Whileanomaliesofavariabilitymodelwereperceivedas
criticalbysomeinterviewees,othersdidnotsharethisview.Two
(I2 and I12) stated that they do not consider anomalies, such as
dead options, because: (i) the top management’s business priorities
mightpreferinvestingtimeandmoneyintoaddingnewfeatures
overaddressingdeadoptions,forinstance,and(ii)duetoalarge
configurationspacecausedbythecombinatorialexplosion,address-
inganomaliesofthevariabilitymodelmightintroducenewbugs
that might take considerable effort to fix.
● ● ●
● ●Var. model to code Dependency decl. to code Var. model to archtecture
Var. model to requirements Var. model to test scripts
123 123
Legend: 1−not necessary, 2−nice to have, 3−critical
Figure5:Importanceofassuringconsistenciesbetweenvari-ability model (especially constraints) and other artifacts4.5 Consistency Analyses
As Figure 5shows, allelicitedconsistency propertieswere merely
perceivedascriticalbythesurveyrespondents: variabilitymodelto
sourcecode (76%),dependencydeclarationstosourcecode (58%),vari-
abilitymodeltorequirements (56%),variabilitymodeltoarchitecture
specification (48%), and variability model to test scripts (47%).
Consistency between variability model and source code isseen as most critical
. Six interviewees explained that such in-
consistenciesleadtovariantsthatdonotimplementtheselected
configurationormorethantheconfigurationspecifies.Whilere-
search on assuring such consistency exists[ 38,49,52,57,64], such
approachestypicallyneedsubstantialadaptationandcalibration
effort. Instead, according to I14, effective solutions should: be moreinformativeormuchfasterthantesting,beflexibleandallowincon-
sistencies during intermediary stages of development, and allow
continuous delivery by differentiating between which inconsisten-
cies would or would not impact the next product release.
Manual review is the primary means of analyzing consis-
tency.80%ofsurveyrespondentsmanuallyinspectdifferentarti-
facts inorder toassure consistency. 40% useautomated techniques
and(30% use testing, that is, if a product behaves as expected, then
itisassumedthatthedependenciesbetweenimplementationarti-
facts are consistent. Examples of reported automated techniques
include:anautomatedbuildprocessthatchecksfordependencies
(S3), a code generator that checks for inconsistencies between the
variability model and source code (S15), and a tool that checks for
architectural violations given a variability model (S22).
Assuring consistency is also a tool-integration problem . Six
interviewees pointed out that they use automated consistency-assurance techniques. For instance, I6 uses a script to check if
optionsusedinthesourcecodearedefinedinthevariabilitymodel;
I5 checks consistency between the variability model and test cases
bymappingtestcasestofunctionalitiesunderwhichtheyaretested
foreachconfiguration(thismappingispartlytool-supported);I6
usesfunctionalteststocheckconsistencybetweenthevariability
modelandarequirementsdatabase.Connectingvariabilitymod-
eling tooling to, mainly, requirements management tools (I6), and
tools for deriving variants (I4), is primarily a tool-integration prob-
lem.AnexampleisdescribedbyI4whoraisestheneedforatool
that would track each configuration-option selection across the
complete tool chain, when deriving a variant.
4.6 Source-Code Analyses
Asshownin Figure 6,onlytwocodeproperties wereperceivedas
critical in the survey: absence of deep nesting of #IFDEFs and #IFs
(40%) and low scattering degree of configuration options (44%). The
remaining properties were all perceived as nice to have. We asked
theintervieweestoelaboratetheiranswersregardingthecriticality
of different code properties.
Manualreviewsandnon-variability-awarestaticanalysistools
are the primary means of analyzing source code . 70% of the
survey respondentsassure codeproperties usingmanual reviews,
followedby40%whousein-housescripts.Eventhough,80%as-
sure code properties using static analysis tools (e.g., Coverty, Code
Sonar or PC-Lint), none is variability-aware.
162
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Tackling Combinatorial Explosion: A Study of Industrial Needs... ASE ’18, September 3–7, 2018, Montpellier, France
●
●
●●
●
●●
●
Scattering degrees Tangling degreesLayout/style guides Disciplined use of IFDEFs Deep nesting of IFDEFs/IFsDead code Liveness of variables Coding standards
123 123123
Legend: 1−not necessary, 2−nice to have, 3−critical
Figure 6: Importance of assuring code properties
Adherence to coding standards can alleviate the need for
codeanalysis . The survey asked about codingguidelines, specifi-
cally about the adherence to coding standards such as MISRA, and
company-specific code style guides. Although both were judged
as nice to have, the former is assured by 90% and the latter by80% of the survey participants. Considering that MISRA is a de
factostandard, the high percentage assuring compliance is not sur-
prising. However, the reason for enforcing adherence to company-
specific code style guides varies. Interviewee I1 stated that such
rulesarecrucialforenablingthecollaborativedevelopmentamong
geographically distant teams. Other participants create rules to
facilitate script-based static code-analysis, for instance, I10: “There
are sometimes [...] static variables that are used in one C-function
andthentheyareusedagaininanotherC-function,whichmakesit
really difficult for our parsing to see what is going on.”
Deep nesting of IFs and #IFDEFs is avoided . Although per-
ceivedascritical,avoidingdeepnestingisinmostcasesonlyassured
throughcodereviewsbasedonaforementionedcodingstandards
(I1, I5, I2, I10). Another practice seen is to create explicit rules
limiting thenesting tothree levels.Other intervieweesstated that
decisionsaremadeonacase-by-casebasis,eventhough,theyadmit
thattheconsequencesofdeeplynestedstructurescanbeserious
(I2, I1, I10). I10: “the cyclomatic complexity in the tool that we use
goestothemaximumbecausethere[are]alotofwaysthroughthe
code. So the potential of something going wrong is very high.”
4.7 Feature Interaction
Althoughfeatureinteractionwasnotaseparatecategoryinthesur-
vey,interviewsregularlyincludeddiscussionsabouttheimportance
andthedifficultiesthathindersuchanalysis.Themainobservation
from the survey is that 65% of the respondents perceive absence
offeatureinteraction asacriticalproperty—inlinewith the74%of respondents stating that sometimes specific combinations of
options cause problems.
Absenceoffeatureinteractionsisalsoprimarilyanalyzedby
testingormanualreviews . In cases when the absence of feature
interactionisassured,itisusuallydonethroughmanualreviews(I9,
I12) or extensive testing (I10, I11). Still, recall the limited sampling
observed, disregarding more systematic coverage criteria. Four in-
terviewees stated that assuring the absence of feature interactions
wouldbenice,butnota gamechanger (I3).Reasonsfornotperceiv-
ing feature interaction as a big problem are the manageable size of
embeddedsoftware, organizationalculturethat emphasizesfeature
modularization, and mandatory regression testing.Analyzingfeatureinteractionsinlarge,looselycoupledsys-
tems is challenging .Aspecificchallengeforfeatureinteraction
in large systems that have loosely coupled modules was explained
by I14:“if you look into larger systems, [...] different processes [...]
communicate over IPC (Inter Process Communication) and then, you
cannot do this analysis with a compiler.” This can be extended to
distributedembeddedsystemswhereafeatureimplementationis
distributed across several computational units.
5 SUMMARY AND IMPACT
We now summarize our main results and discuss their implications
for practitioners and researchers.
Our subject systems can be seen as very typical and substan-
tial cases of industrial highly configurable systems from diverse
domainsandofvaryingscales.Whiletheirmaincharacteristics(Sec-tion4.3),includingtheconfigurationmechanismsandtechnologies
they use, largely resemble those of systems used in empirical stud-
ies or evaluations of analysis techniques (e.g., open-source systems
software),weobservedamismatchbetweentypicalassumptions
madeintheliteratureandtheactualpractitioners’needs.Certainde-
velopment structures and system characteristics—often abstracted
awaywhenproposingnewanalysistechnologies—appeartohinder
many of the more sophisticated analysis techniques.
Identified Needs (RQ1) . We both confirm and refute common
assumptions.First,theseveritythatourpractitionersexpressfor
thecommonpropertiessuggestedintheliteratureconfirmstheir
relevance for highly configurable systems. However, most of the
variability-model-related analysis properties are not seen as impor-
tantbyourpractitioners.Theproposedchange-impactanalysesare
notseenassufficient,becausetheyareconfinedtothemodelandits
configuration space,not providing holisticinsights on impactson
implementation artifacts. Assuring consistencies between artifacts
(especially variability model and source code) is considered highly
critical, as well as identifying unwanted feature interactions.
IdentifiedPractices (RQ2) . We observed (as expected) testing as
the dominant practice. Interestingly, the configuration sampling
criteria that are necessary for testing primarily rely on experience.
Hardlyanysystematicsamplingorrandomsamplingisused.Our
results also suggest that the latter are not even applicable given
the configuration spaces that would still leave too many irrelevant
variants.Furthermore,hardlyanyformalmethodisused(apartfrom
limitedmodelchecking).Besidestesting,manualwork,suchascode
reviews, is exercised, because often the variability models required
formoresophisticatedanalysesdonotexistorarenotexpressedina
formthatcanbeusedasaninput.Thelackofintegratedtoolchains
is also a factor, since artifacts required for performing analyses
are managed in different tools. Interestingly, the experience of the
developersandrules,suchascodingstandards,butalsoengineering
practicessuchasmodularizationofcode,oftenalleviatetheneed
for sophisticated analyses of the highly configurable system.
ResearchDirections .Ourresultssuggesttorefocussomeresearch
efforts towards the actual needs our study identified. General re-
search directions could be the following.
Improve engineering methods to alleviate the need for analyses.
Since most analyses need substantial investment and adaptation
towardsthespecifichighlyconfigurablesystemanditsengineering
163
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Mukelabai, Nešić, Maro, Berger, and Steghöfer
environment, investigating ways to avoid them in the first place is
a worthwhile research avenue.
Conceive lightweight analyses that account for the diversity of
artifacts and tool chains, or that are even independent of these.
At least, authors of new analysis techniques should clearly state
assumptions on how variability is expected to be modeled and
manifested in the software artifacts.
Unifyvariabilitymanagementandmodelingconcepts. Weagain[ 11,
13]observedavarietyofvariabilityandconfigurationmechanisms.
Addressing this tool-integration problem appears to require unify-
ing variabilityconcepts withconcepts fromcommonly usedtools,
especially version-control systems[44].
Conceivehybridanalyses thatcombinemanualreviewingwith
variability-awareanalyses.Recallthatmanual,case-by-casedeci-
sionsonactionstobedoneuponanalysisresultscannotbeavoided
and that state-of-the-art analysis tooling requires substantial setup.
Furthermore, variability-aware analysis are static by nature and,therefore, typically produce many false positives. Thus, hybrid
analysis techniques could be efficient.
Withrespecttoconcreteanalysistechniques,ourresultssuggest
the following main research directions.
Improve testing techniques for highly configurable systems, es-
peciallyautomatedtest-casegeneration,test-caseselectionbased
on quality properties (e.g., safety), and traceability management
between test cases and configurations that failed tests.
Investigate experience-based configuration sampling, especially
identify the sources of experience and map these sources to the
effectivenessoffindingbugs.Thisrequiresexperimentsandfield
studies.Furthermore,combiningexperience-basedsamplingand
systematicsamplingseemstobeaworthwhileresearchdirection
based on our results.
Conceivechange-impactanalysesthatarenotconfinedtothevari-
ability model,but offerinsights onthe impactof changesto other
artifacts when done to the variability model or to the implementa-
tion artifacts. Concisely and effectively presenting change impacts
to engineers is another challenge.
Realize quicker and more flexible consistency checking, for in-
stance, to support continuous integration and deployment. The
feedback loop (from making changes to being alerted about incon-
sistencies) needs to be much shorter.
Conceivefeature-interactionanalyses forlargeandlooselycou-
pled systems. These analyses should take more diverse artifactrelationships into account and should go beyond static analyses
currently offered by compilers.
Finally, our study again emphasized that regular feedback loops
with practitioners are crucial, steering research efforts away from
analysistechniquesthatmightbelow-hangingfruits,butarenot
perceived as needed in the real world.
6 THREATS TO VALIDITY
Construct Validity . Our survey and the interviews used the con-
ceptofhighlyconfigurablesystemtoensurethatallpractitioners
could describe their practices without the need to adopt a specific
terminology. We used terms such as configuration option to refer to
theconceptoffeature, configurationspecification torefertovariabil-
itymodel,andprovidedshortexplanationsfornon-trivialquestionstomitigatepotentialmisinterpretations.Weiterativelydeveloped
our questionnaire and our interview guide using pilot runs already
withindustrialparticipants.Toensurecompleteness,participants
could provide additional information. Finally, we used violin plots
to visualizetrends inthe responsesabout theseverity ofdifferent
analyses.So,weinterpretedthefour-pointscaleascontinuouswith
equal distances between points, but did not use this interpretation
as a foundation for statistical analysis.
Internal Validity . We selected the participants based on their in-
dustrialand technicalexperience.This experiencepairedwith the
combinationofsurveyandinterviewsprovidedbothgeneralper-
spectives on analysis techniques and on assured properties, as well
as specific insights with respect to how analyses are performed
and what the needs are. Since all subjects were interested in ex-
ploringnewanalysistechniquesandwereawarethattheirinput
mightshapefutureresearch,theywereveryopenaboutthecurrent
limitations and had no incentive to present their current practices
in a better light. Finally, even though, the interviews were con-
ducted by different researchers, the recordings were exchanged for
transcription and for coding to avoid potential biases.
External Validity . All our study participants work with highly
configurablesystemsofvaryingsizesandmaturity,coveringawide
rangeofdomains.Theneedsweelicitedandtheinsightswederived
canbeappliedtohighlyconfigurablesystemsinsimilardomains.
Some needs and practices reported are dependent on a concretesystem, but we identified these and marked them accordingly if
they were mentioned.
Conclusion Validity . Our qualitative analysis depends on our in-
terpretation. However, we mitigated bias by collaboratively coding
the interviews using open coding, cross-checking the codes, refin-
ingthecodes,andconductingacodingworkshopbyallauthors.We
usedtriangulationandcarefullyformulatedandverifiedinsights
and conclusions to enhance our study’s validity.
7 CONCLUSION
Wepresentedastudyontheneedsandpracticesofanalyzinghighly
configurablesystems.Westudiedsubstantialindustrialcasescov-
ering a wide range of domains, development scales, and system
complexities. Mapping existing research results to industrial needs
andpracticesisintrinsicallydifficult,giventhedifferentcultures,
terminologies, and system architectures in practice. Our focus was
to deeply understand each case using expert interviews, while also
going into a reasonable breadth (still focusing on response quality)
with our survey. We found rather pragmatic practices and a sur-
prisingly low adoption (and awareness) of academic analyses, even
though,mostofthestudiedcompanieshaveresearchcollaborations.
Asfuturework,weintendtomapneedstothestateoftheartfrom
the literature and conceive a research agenda.
ACKNOWLEDGMENT
We thank all of our study participants, Vinnova Sweden (2016-
02804) funding the EU ITEA project REVAMP2, and the Swedish
Research Council (257822902).
164
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. Tackling Combinatorial Explosion: A Study of Industrial Needs... ASE ’18, September 3–7, 2018, Montpellier, France
REFERENCES
[1]Sven Apel, Don Batory, Christian Kästner, and Gunter Saake. 2013. Feature-
Oriented Software Product Lines. Springer.
[2]Sven Apel, Hendrik Speidel, Philipp Wendler, Alexander von Rhein, and Dirk
Beyer. 2011. Detection of Feature Interactions Using Feature-aware Verification.
InASE.
[3]Sven Apel, Alexander von Rhein, Thomas Thüm, and Christian Kästner. 2013.
Feature-interactiondetectionbasedonfeature-basedspecifications. Computer
Networks 57, 12 (2013), 2399–2409.
[4]Ebrahim Bagheri and Dragan Gasevic. 2011. Assessing the maintainability of
software product line feature models using structural metrics. Software Quality
Journal19, 3 (2011), 579–612.
[5]RabihBashroush,MuhammadGarba,RickRabiser,IrisGroher,andGoetzBotter-
weck. 2017. CASE Tool Support for Variability Management in Software Product
Lines.Comput. Surveys 50, 1 (March 2017), 14:1–14:45.
[6]Don S. Batory, David Benavides, and Antonio Ruiz Cortés. 2006. Automated
analysisoffeaturemodels:challengesahead. Commun.ACM 49,12(2006),45–47.
[7]David Benavides, Antonio Ruiz-Cortés, Pablo Trinidad, and Sergio Segura. 2006.
A Survey on the Automated Analyses of Feature Models. In JISBD.
[8]David Benavides, Sergio Segura, and Antonio Ruiz-Cortés. 2010. Automated
analysisoffeaturemodels20yearslater:Aliteraturereview. InformationSystems
35, 6 (2010), 615–636.
[9]ThorstenBergerandJianmeiGuo.2014.TowardsSystemAnalysiswithVariability
Model Metrics. In VAMOS.
[10]ThorstenBerger,DanielaLettner,JuliaRubin,PaulGrünbacher,AdelineSilva,
Martin Becker, Marsha Chechik, and Krzysztof Czarnecki. 2015. What is a
Feature? A Qualitative Study of Features in Industrial Software Product Lines. In
SPLC.
[11]ThorstenBerger,DivyaNair,RalfRublack,JoanneM.Atlee,KrzysztofCzarnecki,
andAndrzejWasowski.2014. ThreeCasesofFeature-BasedVariabilityModeling
in Industry. In MODELS.
[12]Thorsten Berger, Rolf-Helge Pfeiffer, Reinhard Tartler, Steffen Dienst, Krzysztof
Czarnecki, Andrzej Wasowski, and Steven She. 2014. Variability Mechanisms
in Software Ecosystems. Information and Software Technology 56, 11 (2014),
1520–1535.
[13]Thorsten Berger, Ralf Rublack, Divya Nair, Joanne M. Atlee, Martin Becker,
Krzysztof Czarnecki, and Andrzej Wasowski. 2013. A Survey of Variability
Modeling in Industrial Practice. In VAMOS.
[14]ThorstenBerger,StevenShe,RafaelLotufo,AndrzejWasowski,andKrzysztof
Czarnecki.2013. AStudyofVariabilityModelsandLanguagesintheSystems
Software Domain. IEEE Transactions on Software Engineering 39, 12 (2013), 1611–
1640.
[15]Jan Bosch. 2009. From Software Product Lines to Software Ecosystems. In SPLC.
[16]Muffy Calder, Mario Kolberg, Evan H Magill, and Stephan Reiff-Marganiec. 2003.
Featureinteraction:acriticalreviewandconsideredforecast. ComputerNetworks
41, 1 (2003), 115–141.
[17]GaryChastek,PatrickDonohoe,KyoChulKang,andSteffenThiel.2001. Product
line analysis: a practical introduction. Technical Report CMU/SEI-2001-TR-001.
[18]Andreas Classen, Patrick Heymans, and Pierre-Yves Schobbens. 2008. What’s in
a Feature: A Requirements Engineering Perspective. In FASE.
[19]AndreasClassen,PatrickHeymans,Pierre-YvesSchobbens,AxelLegay,andJean-
François Raskin. 2010. Model Checking Lots of Systems: Efficient Verification of
Temporal Properties in Software Product Lines. In ICSE.
[20]PaulClementsandLindaNorthrop.2001. SoftwareProductLines:Practicesand
Patterns. Addison-Wesley.
[21]Myra B Cohen, Matthew B Dwyer, and Jiangfan Shi. 2007. Interaction testing of
highly-configurable systems in the presence of constraints. In ISSTA.
[22]JulietMCorbinandAnselmStrauss.1990. Groundedtheoryresearch:Procedures,
canons, and evaluative criteria. Qualitative sociology 13, 1 (1990), 3–21.
[23]Krzysztof Czarnecki, Paul Grünbacher, Rick Rabiser, Klaus Schmid, and Andrzej
Wąsowski.2012. CoolFeaturesandToughDecisions:AComparisonofVariability
Modeling Approaches. In VAMOS.
[24]YaelDubinsky,JuliaRubin,ThorstenBerger,SlawomirDuszynski,MartinBecker,
andKrzysztofCzarnecki.2013. AnExploratoryStudyofCloninginIndustrial
Software Product Lines. In CSMR.
[25]EmelieEngströmandPerRuneson.2011. Softwareproductlinetesting—asys-
tematic mapping study. Information and Software Technology 53, 1 (2011), 2–13.
[26]Joel Greenyer, Amir Molzam Sharifloo, Maxime Cordy, and Patrick Heymans.
2013. Features meet scenarios: modeling and consistency-checking scenario-
basedproductlinespecifications. RequirementsEngineering 18,2(2013),175–198.
[27]Jianmei Guo, Dingyu Yang, Norbert Siegmund, Sven Apel, Atrisha Sarkar, Pavel
Valov, Krzysztof Czarnecki, Andrzej Wasowski, and Huiqun Yu. 2018. Data-
efficient performance learning for configurable systems. Empirical Software
Engineering 23, 3 (2018), 1826–1867.
[28]AxelHalin,AlexandreNuttinck,MathieuAcher,XavierDevroey,GillesPerrouin,andBenoitBaudry.2017. Testthemall,isitworthit?Agroundtruthcomparison
of configuration sampling strategies. arXiv preprint arXiv:1710.07980 (2017).[29]Praveen Jayaraman, Jon Whittle, Ahmed M Elkhodary, and Hassan Gomaa. 2007.
Model Composition in Product Lines and Feature Interaction Detection using
critical pair analysis. In MODELS.
[30]KyoKang,SholomCohen,JamesHess,WilliamNowak,andSpencerPeterson.
1990.Feature-Oriented Domain Analysis (FODA) Feasibility Study. Technical
Report CMU/SEI-90-TR-21.
[31]Christian Kästner, Sven Apel, Thomas Thüm, and Gunter Saake. 2012. Type
Checking Annotation-based Product Lines. ACM Transactions on Software Engi-
neering and Methodology 21, 3 (July 2012), 14:1–14:39.
[32]ChristianKästner,PaoloG.Giarrusso,TillmannRendel,SebastianErdweg,KlausOstermann,andThorstenBerger.2011. Variability-AwareParsinginthePresence
of Lexical Macros and Conditional Compilation. In OOPSLA.
[33]Alexander Knüppel,Thomas Thüm,Stephan Mennicke,Jens Meinicke,and Ina
Schaefer. 2017. Is There a Mismatch Between Real-world Feature Models and
Product-line Research?. In ESEC/FSE.
[34]ChristianKröher,SaschaEl-Sharkawy,andKlausSchmid.2018. KernelHaven:
An Experimentation Workbench for Analyzing Software Product Lines. In ICSE.
[35]Charles W. Krueger. 2006. New Methods in Software Product Line Practice.
Commun. ACM 49, 12 (Dec. 2006), 37–40.
[36]Jacob Krüger, Wanzi Gu, Hui Shen, Mukelabai Mukelabai, Regina Hebig, and
Thorsten Berger. 2018. Towards a Better Understanding of Software Features
and Their Characteristics: A Case Study of Marlin. In VaMoS.
[37]Kim Lauenroth, Klaus Pohl, and Simon Toehning. 2009. Model Checking of
Domain Artifacts in Product Line Engineering. In ASE.
[38]Duc Minh Le, Hyesun Lee, Kyo Chul Kang, and Lee Keun. 2013. Validating
Consistency Between a Feature Model and its Implementation. In ICSR.
[39]Jihyun Lee, Sungwon Kang, and Danhyung Lee. 2012. A Survey on Software
Product Line Testing. In SPLC.
[40]Jorg Liebig, Sven Apel, Christian Lengauer, Christian Kästner, and Michael
Schulze.2010. AnAnalysisoftheVariabilityinFortyPreprocessor-BasedSoft-
ware Product Lines. In ICSE.
[41]Jörg Liebig, Christian Kästner, and Sven Apel. 2011. Analyzing the Discipline of
Preprocessor Annotations in 30 Million Lines of C Code. In AOSD.
[42]Jörg Liebig, Alexander von Rhein, Christian Kästner, Sven Apel, Jens Dorre,
and Christian Lengauer. 2012. Large-Scale Variability-Aware Type Checking and
Dataflow Analysis. Technical Report MIP-1212.
[43]Jörg Liebig, Alexander von Rhein, Christian Kästner, Sven Apel, Jens Dörre, and
Christian Lengauer. 2013. Scalable Analysis of Variable Software. In ESE/FSE.
[44]LukasLinsbauer,ThorstenBerger,andPaulGrünbacher.2017. AClassification
of Variation Control Systems. In GPCE.
[45]Flávio Medeiros, Christian Kästner, Márcio Ribeiro, Rohit Gheyi, and Sven Apel.
2016. A Comparison of 10 Sampling Algorithms for Configurable Systems. In
ICSE.
[46]Jens Meinicke, Thomas Thüm, Reimar Schröter, Fabian Benduhn, and Gunter
Saake. 2014. An Overview on Analysis Tools for Software Product Lines. In
SPLat.
[47]JanMidtgaard,ClausBrabrand,andAndrzejWasowski.2014. SystematicDeriva-
tion of Static Analyses for Software Product Lines. In MODULARITY.
[48]Sarah Nadi,Thorsten Berger,Christian Kästner, andKrzysztof Czarnecki.2014.
Mining Configuration Constraints: Static Analyses and Empirical Results. In
ICSE.
[49]Sarah Nadi,Thorsten Berger,Christian Kästner, andKrzysztof Czarnecki.2014.
Mining Configuration Constraints: Static Analyses and Empirical Results. In
ICSE.
[50]Sarah Nadi,Thorsten Berger,Christian Kästner, andKrzysztof Czarnecki.2015.
WheredoConfigurationConstraintsStemFrom?AnExtractionApproachandan
EmpiricalStudy. IEEETransactionsonSoftwareEngineering 41,8(2015),820–841.
[51]PauloAnselmodaMotaSilveiraNeto,IvandoCarmoMachado,JohnDMcGregor,
Eduardo Santana De Almeida, and Silvio Romero de Lemos Meira. 2011. A
systematic mapping study of software product lines testing. Information and
Software Technology 53, 5 (2011), 407–423.
[52]Leonardo Passos, Jianmei Guo, Leopoldo Teixeira, Krzysztof Czarnecki, Andrzej
Wąsowski,andPauloBorba.2013. CoevolutionofVariabilityModelsandRelated
Artifacts: A Case Study from the Linux Kernel. In SPLC.
[53]Gilles Perrouin, Sagar Sen, Jacques Klein, Benoit Baudry, and Yves Le Traon.
2010. Automated and scalable t-wise test case generation strategies for software
product lines. In ICST.
[54]Rodrigo Queiroz, Thorsten Berger, and Krzysztof Czarnecki. 2016. Towards
Predicting Feature Defects in Software Product Lines. In FOSD.
[55]Sacha Reis, Andreas Metzger, and Klaus Pohl. 2006. A reuse technique for
performance testing of software product lines. In SPLiT.
[56]Andreas Reuys, Sacha Reis, Erik Kamsties, and Klaus Pohl. 2006. The scented
methodfortestingsoftwareproductlines. In SoftwareProductLines.Springer,
479–520.
[57]Alcemir Rodrigues Santos, Raphael Pereira de Oliveira, and Eduardo Santana de
Almeida. 2015. Strategies for Consistency Checking on Software Product Lines:
A Mapping Study. In EASE.
[58]FlorianSattler,AlexandervonRhein,ThorstenBerger,NiklasSchalckJohansson,
Mikael Mark Hardø, and Sven Apel. 2018. Lifting Inter-App Data-Flow Analysis
165
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. ASE ’18, September 3–7, 2018, Montpellier, France Mukelabai, Nešić, Maro, Berger, and Steghöfer
to Large App Sets. Automated Software Engineering 25 (jun 2018), 315–346. Issue
2.
[59]Tonny KurniadiSatyananda, DanhyungLee, andSungwon Kang.2007. Formal
VerificationofConsistencyBetweenFeatureModelandSoftwareArchitecture
in Software Product Line. In ICSEA.
[60]KlausSchmid,RickRabiser,andPaulGrünbacher.2011. Acomparisonofdecision
modeling approaches in product lines. In VaMoS.
[61]Mirjam Steger, Christian Tischer, Birgit Boss, Andreas Müller, Oliver Pertler,
Wolfgang Stolz, and Stefan Ferber. 2004. Introducing PLA at Bosch Gasoline
Systems: Experiences and Practices. In SPLC.
[62]Anselm Strauss and Juliet Corbin. 1990. Open Coding. Basics of Qualitative
Research: Grounded Theory Procedures and Techniques 2 (1990), 101–121.
[63]Reinhard Tartler, Daniel Lohmann, Julio Sincero, and Wolfgang Schröder-
Preikschat. 2011. Feature Consistency in Compile-Time-Configurable System
Software: Facing the Linux 10,000 Feature Problem. In EuroSys.
[64]ReinhardTartler,JulioSincero,ChristianDietrich,WolfgangSchröder-Preikschat,
andDanielLohmann.2012. Revealingandrepairingconfigurationinconsisten-
ciesinlarge-scalesystemsoftware. InternationalJournalonSoftwareToolsforTechnology Transfer 14, 5 (2012), 531–551.
[65]The Authors. 2018. Online Appendix. https://sites.google.com/view/planalysis/ .
[66]Thomas Thüm, Sven Apel, Christian Kästner, Ina Schaefer, and Gunter Saake.
2014. Aclassificationandsurveyofanalysisstrategiesforsoftwareproductlines.
Comput. Surveys 47, 1 (June 2014), 6:1–6:45.
[67]Thomas Thum, Don Batory, and Christian Kastner. 2009. Reasoning about edits
to feature models. In ICSE.
[68]Thomas Thüm, Ina Schaefer, Sven Apel, and Martin Hentschel. 2012. Family-
based Deductive Verification of Software Product Lines. In GPCE.
[69]FrankJ.vanderLinden,KlausSchmid,andEelcoRommes.2007. SoftwareProduct
LinesinAction:TheBestIndustrialPracticeinProductLineEngineering. Springer.
[70]MichaelVierhauser,PaulGrünbacher,WolfgangHeider,GeraldHoll,andDaniela
Lettner. 2012. Applying a Consistency Checking Framework for Heterogeneous
Models and Artifacts in Industrial Product Lines. In MODELS.
[71]Bo Zhang, Martin Becker, Thomas Patzke, Krzysztof Sierszecki, and Juha Erik
Savolainen. 2013. Variability Evolution and Erosion in Industrial Product Lines:
A Case Study. In SPLC.
166
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 13:47:06 UTC from IEEE Xplore.  Restrictions apply. 