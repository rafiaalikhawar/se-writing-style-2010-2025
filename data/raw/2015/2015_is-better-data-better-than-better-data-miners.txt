Is “Better Data” Better Than “Better Data Miners”?
On the Benefits of Tuning SMOTE for Defect Prediction
Amritanshu Agrawal
Department of Computer Science
North Carolina State University
Raleigh, NC, USA
aagrawa8@ncsu.eduTim Menzies
Department of Computer Science
North Carolina State University
Raleigh, NC, USA
tim@menzies.us
ABSTRACT
We report and fix an important systematic error in prior studies
that ranked classifiers for software analytics. Those studies did not
(a) assess classifiers on multiple criteria and they did not (b) study
how variations in the data affect the results. Hence, this paper
applies(a)multi-performancecriteriawhile(b)fixingtheweaker
regions ofthe training data (usingSMOTUNED, whichis an auto-
tuning version of SMOTE). This approach leads to dramatically
large increases in software defect predictions when applied in a
5*5cross-validationstudyfor3,681JAVAclasses(containingover
a million lines of code) from open source systems, SMOTUNED
increasedAUCandrecallby60%and20%respectively.Theseim-
provements are independent of the classifier used to predict for
defects. Same kind of pattern (improvement) was observed when a
comparativeanalysisofSMOTEandSMOTUNEDwasdoneagainst
the most recent class imbalance technique.
Inconclusion, forsoftware analytictasks likedefect prediction,
(1)datapre-processingcanbemoreimportantthanclassifierchoice,
(2) ranking studies are incomplete without such pre-processing,
and (3) SMOTUNED is a promising candidate for pre-processing.
KEYWORDS
Search based SE, defect prediction, classification, data analytics for
software engineering, SMOTE, imbalanced data, preprocessing
ACM Reference Format:
Amritanshu Agrawal and Tim Menzies. 2018. Is “Better Data” Better Than
“Better Data Miners”?: On the Benefits of Tuning SMOTE for Defect Pre-
diction . In ICSE ’18: ICSE ’18: 40th International Conference on Software
Engineering , May 27-June 3, 2018, Gothenburg, Sweden. ACM, New York,
NY, USA, 12 pages. https://doi.org/10.1145/3180155.3180197
1 INTRODUCTION
Softwarequalitymethodscostmoneyandbetterqualitycostsexpo-
nentially more money [ 16,66]. Given finite budgets, quality assur-
anceresourcesareusuallyskewedtowardsareasknowntobemost
safety critical or mission critical [ 34]. This leaves “blind spots”: re-
gions of the system that may contain defects which may be missed.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180197Therefore, in addition to rigorously assessing critical areas, a paral-
lel activity should be to sample the blind spots [37].
To sample those blind spots, many researchers use static code
defectpredictors.Sourcecodeisdividedintosectionsandresearchers
annotatethecodewiththenumberofissuesknownforeachsection.
Classification algorithms are then applied to learn what static code
attributesdistinguishbetweensectionswithfew/manyissues.Such
static code measures can be automatically extracted from the code
base with little effort even for very large software systems [44].
One perennial problem is what classifier should be used to build
predictors? Many papers report ranking studies where a quality
measure is collected from classifiers when they are applied to data
sets[13,15–18,21,25–27,29,32,33,35,40,53,62,67].Theseranking
studies report which classifiers generate best predictors.
Research of this paper began with the question would the use of
datapre-processorchangetherankingsofclassifiers? SEdatasetsare
oftenimbalanced,i.e.,thedatainthetargetclassisoverwhelmedbyanover-abundanceofinformationabouteverythingelseexceptthe
target[36].Asshownintheliteraturereviewofthispaper,inthe
overwhelming majority of papers (85%), SE research uses SMOTE
to fix data imbalance [ 7] but SMOTE is controlled by numerous
parameters which usually are tuned using engineering expertiseor left at their default values. This paper proposes SMOTUNED,an automatic method for setting those parameters which whenassessed on defect data from 3,681 classes (over a million lines
ofcode)takenfromopensourceJAVAsystems,SMOTUNEDout-
performed both the original SMOTE [ 7] as well as state-of-the-art
method [4].
To assess, we ask four questions:
•RQ1:Are the default “off-the-shelf” parameters for SMOTEap-
propriate for all data sets?
Result 1
SMOTUNED learneddifferentparametersforeachdataset,allof
which were very different from default SMOTE.
•RQ2:Is there any benefit in tuning the default parameters of
SMOTEfor each new data set?
Result 2
Performance improvements using SMOTUNED are dramatically
large, e.g., improvements in AUC up to 60% against SMOTE.
In those results, we see that while no learner was best across all
data sets and performance criteria, SMOTUNED was most often
seen in the best results. That is, creating better training data might
be more important than the subsequent choice of classifiers.
10502018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Amritanshu Agrawal and Tim Menzies
•RQ3:In terms of runtimes, is the cost of running SMOTUNED
worth the performance improvement?
Result 3
SMOTUNED terminates in under two minutes, i.e., fast enough to
recommend its widespread use.
•RQ4:How does SMOTUNED perform against the recent class
imbalance technique?
Result 4
SMOTUNED performsbetterthanaveryrecentimbalancehandling
technique proposed by Bennin et al. [4].
In summary, the contributions of this paper are:
•Thediscoveryofanimportantsystematicerrorinmanyprior
ranking studies, i.e., all of [ 13,15–18,21,25–27,29,32,33,35,
40, 53, 62, 67].
•A novel application of search-based SE (SMOTUNED) to handle
class imbalance that out-performs the prior state-of-the-art.
•Dramatically large improvements in defect predictors.
•Potentially, for any other software analytics task that uses clas-
sifiers, a way to improve those learners as well.
•Amethodologyforassessingthevalueofpre-processingdata
sets in software analytics.
•A reproduction package to reproduce our results then (perhaps)
to improve or refute our results (Available to download from
http://tiny.cc/smotuned).
The rest of this paper is structured as follows: Section 2.1 gives an
overview on software defect prediction. Section 2.2 talks about all
the performance criteria used in this paper. Section 2.3 explains
the problem of class imbalance in defect prediction. Assessment
ofthepreviousrankingstudiesisdoneinSection2.4.Section2.5
introduces SMOTE and discusses how SMOTE has been used in lit-
erature.Section2.6providesthedefinitionofSMOTUNED.Section3describestheexperimentalsetupofthispaperandaboveresearch
questions are answered in Section 4. Lastly, we discuss the validity
of our results and a section describing our conclusions.
Note that the experiments of this paper only make conclusions
aboutsoftwareanalyticsfordefectprediction.Thatsaid,manyother
software analytics tasks use the same classifiers explored here: for
non-parametricsensitivityanalysis[ 41],asapre-processortobuild
the tree used to infer quality improvement plans [ 31], to predict
Githubissueclosetime[ 55],andmanymore.Thatis,potentially,
SMOTUNED is a sub-routine that could improve many software
analyticstasks.Thiscouldbeahighlyfruitfuldirectionforfuture
research.
2 BACKGROUND AND MOTIVATION
2.1 Defect Prediction
Softwareprogrammersare intelligent,butbusypeople. Suchbusy
peopleoftenintroducedefectsintothecodetheywrite[ 20].Testing
software for defects is expensive and most software assessment
budgets arefinite. Meanwhile, assessmenteffectiveness increases
exponentiallywithassessmenteffort[ 16].Suchexponentialcosts
exhaustfiniteresourcessosoftwaredevelopersmustcarefullyde-
cide what parts of their code need most testing.Avarietyofapproacheshavebeenproposedtorecognizedefect-
prone software components using code metrics (lines of code, com-
plexity)[10,38,40,45,58]orprocess metrics(numberofchanges,
recent activity) [ 22]. Other work, such as that of Bird et al. [ 5],
indicatedthatitispossibletopredictwhichcomponents(fore.g.,
modules)arelikelylocationsofdefectoccurrenceusingacompo-
nent’sdevelopmenthistoryanddependencystructure.Prediction
models based on the topological properties of components within
them have also proven to be accurate [71].
Thelessonofalltheaboveisthattheprobablelocationoffuture
defects can be guessed using logs of past defects [ 6,21]. These logs
mightsummarizesoftwarecomponentsusingstaticcodemetrics
such as McCabes cyclomatic complexity, Briands coupling metrics,
dependencies between binaries, or the CK metrics [ 8] (which is
described in Table 1). Oneadvantage with CK metrics is that they
aresimpletocomputeandhence,theyarewidelyused.Radjenović
et al. [53] reported that in the static code defect prediction, the CK
metrics are used twice as much (49%) as more traditional source
code metrics such as McCabes (27%) or process metrics (24%). The
staticcodemeasuresthatcanbeextractedfromasoftwareisshown
in Table 1. Note that such attributes can be automatically collected,
even for very large systems [ 44]. Other methods, like manual code
reviews, are far slower and far more labor intensive.
Static code defect predictors are remarkably fast and effective.
Giventhecurrentgenerationofdataminingtools,itcanbeamatter
ofjustafewsecondstolearnadefectpredictor(seetheruntimes
in Table 9 of reference [ 16]). Further, in a recent study by Rahman
et al. [54], found no significant differences in the cost-effectiveness
of (a) static code analysis tools FindBugs and Jlint, and (b) static
code defect predictors. This is an interesting result since it is much
slower to adapt static code analyzers to new languages than defect
predictors(sincethelatterjustrequireshackingtogethersomenew
static code metrics extractors).
2.2 Performance Criteria
Formally, defectpredictionis abinary classificationproblem.The
performanceof adefect predictorcan beassessed viaa confusion
matrixlikeTable 2wherea“positive”outputis thedefectiveclass
under study and a “negative” output is the non-defective one.
Table 2: Results Matrix
Actual
Prediction falsetrue
defect-free TNFN
defective FPTPFurther, “false” means the
learner got it wrong and
“true” means the learner cor-
rectlyidentifiedafaultornon-
fault module. Hence, Table 2
hasfourquadrantscontaining,
e.g.,FPwhich denotes “false positive”.
From this matrix, we can define performance measures like:
•Recall=pd=TP/(TP+FN)
•Precision =prec=TP/(TP+FP)
•False Alarm =pf=FP/(FP+TN)
•Area Under Curve (AUC) , which is the area covered by an
ROCcurve[ 11,60]inwhichtheX-axisrepresents,falsepositive
rate and the Y-axis represents true positive rate.
As shown in Figure 1, a typical predictor must “trade-off” be-
tweenfalse alarmandrecall.This isbecausethe moresensitivethe
detector, the more often it triggers and the higher its recall. If a
1051
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Is “Better Data” Better Than “Better Data Miners”? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Table 1: OO CK code metrics used for all studies in this paper. The last line shown, denotes the dependent variable.
amc average method complexity e.g., number of JAVA byte codes
avg, cc average McCabe average McCabe’s cyclomatic complexity seen in class
ca afferent couplings how many other classes use the specific class.
cam cohesion amongst classes summationofnumberofdifferenttypesofmethodparametersineverymethoddividedbyamultiplicationof
number of different method parameter types in whole class and number of methods.
cbm coupling between methods total number of new/redefined methods to which all the inherited methods are coupled
cbo coupling between objects increased when the methods of one class access services of another.
ce efferent couplings how many other classes is used by the specific class.
dam data access ratio of the number of private (protected) attributes to the total number of attributes
dit depth of inheritance tree
ic inheritance coupling number of parent classes to which a given class is coupled
lcom lack of cohesion in methods number of pairs of methods that do not share a reference to an case variable.
locm3 another lack of cohesion measure ifm,aarethenumberof methods ,attributes inaclassnumberand μ(a)isthenumberofmethodsaccessing
an attribute, then lcom3=((1
a/summationtext,jaμ(a,j))−m)/(1−m).
loc lines of code
max, cc maximum McCabe maximum McCabe’s cyclomatic complexity seen in class
mfa functional abstraction no. of methods inherited by a class plus no. of methods accessible by member methods of the class
moa aggregation count of the number of data declarations (class fields) whose types are user defined classes
noc number of children
npm number of public methods
rfc response for a class number of methods invoked in response to a message to the object.
wmc weighted methods per class
nDefects raw defect counts numeric: number of defects found in post-release bug-tracking systems.
defects present? boolean ifnDefects >0thentrueelsefalse
detectortriggersmoreoften,italsoraisesmorefalsealarms.Hence,
when increasing recall, we should expect the false alarm rate to
increase (ideally, not by very much).
Figure 1: Trade-offs false alarm vs
recall (probability of detection).There are many
more ways to eval-uate defect predic-
torsbesidesthefour
listed above. Previ-
ously,Menziesetal.
catalogued dozensof them (see Table23.2 of [
39]) and
even several novel
ones were proposed
(balance,G-measure[ 38]).
Butnoevaluationcriteriais“best”sincedifferentcriteriaareappro-
priate in different business contexts. For e.g., as shown in Figure 1,
whendealingwithsafety-criticalapplications,managementmaybe
“risk adverse” and hence many elect to maximize recall, regardless
ofthetimewastedexploringfalsealarm.Similarly,whenrushing
somenon-safetycriticalapplicationtomarket,managementmay
be“costadverse”andelectto minimizefalsealarm sincethisavoids
distractions to the developers.
In summary, there are numerous evaluation criteria and numer-
ousbusinesscontextswheredifferentcriteriamightbepreferred
bydifferentlocalbusinessusers.Inresponsetothecornucopiaof
evaluation criteria, we make the following recommendations: a)
doevaluatelearnersonmorethanonecriteria,b)donotevaluate
learners on all criteria (there are too many), and instead, apply the
criteria widely seen in the literature. Applying this advice, this pa-
perevaluatesthedefectpredictorsusingthefourcriteriamentioned
above (since these are widely reported in the literature [ 16,17]))
butnotothercriteriathathaveyettogainawideacceptance(i.e.,
balance and G-measure).
2.3 Defect Prediction and Class Imbalance
Class imbalance is concerned with the situation in where someclasses of data are highly under-represented compared to otherclasses[
23].Byconvention,theunder-representedclassiscalled
theminority class, and correspondingly the class which is over-
representediscalledthe majorityclass.Inthispaper,wesaythat
classimbalanceis worsewhentheratioofminorityclasstomajority
increases,thatis, class-imbalanceof5:95 isworsethan 20:80.Menzies
et al. [36] reported SE data sets often contain class imbalance. In
their examples, they showed static code defect prediction data sets
with class imbalances of 1:7; 1:9; 1:10; 1:13; 1:16; 1:249.
The problem of class imbalance is sometimes discussed in the
software analytics community. Hall et al. [ 21] found that models
basedonC4.5under-performiftheyhaveimbalanceddatawhile
NaiveBayesandLogisticregressionperformrelativelybetter.Their
general recommendation is to not use imbalanced data. Some re-
searchersofferpreliminaryexplorationsintomethodsthatmight
mitigate for class imbalance. Wang et al. [ 67] and Yu et al. [ 69]
validatedtheHalletal.resultsandconcludedthattheperformance
ofC4.5isunstableonimbalanceddatasetswhileRandomForest
and Naive Bayes are more stable. Yan et al. [ 68] performed fuzzy
logic and rules to overcome the imbalance problem, but they only
explored one kind of learner (Support Vector Machines). Pelayo et
al.[49]studiedtheeffectsofthepercentageofoversamplingand
undersamplingdone.Theyfoundoutthatdifferentpercentageof
eachhelpsimprovetheaccuraciesofdecisiontreelearnerfordefect
prediction using CK metrics. Menzies et al. [ 42] undersampled the
non-defectclasstobalancetrainingdataandreportedhowlittlein-formationwasrequiredtolearnadefectpredictor.Theyfoundthat
throwing away data does not degrade the performance of Naive
BayesandC4.5decisiontrees.Otherpapers[ 49,50,57]haveshown
the usefulness of resampling based on different learners.
Wenotethatmanyresearchersinthisarea[ 19,67,69]referto
the SMOTE method explored in this paper, but only in the context
of future work. One rare exception to this general pattern is the
recentpaperbyBenninetal.[ 4],whichweexploredaspartofRQ4.
2.4 Ranking Studies
A constant problem in defect prediction is what classifier should
beappliedtobuildthedefectpredictors?Toaddressthisproblem,
many researchers run ranking studies where performance scores
1052
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Amritanshu Agrawal and Tim Menzies
Table 3: Classifiers used in this study. Rankings from Ghotra et al. [17].
RANK LEARNER NOTES
1 “best” RF= random forest Random forest of entropy-based decision trees.
LR=Logistic regression A generalized linear regression model.
2 KNN= K-means Classify a new instance by finding “k” examples of similar instances. Ghortra et al. suggested K=8.
NB= Naive Bayes Classify a new instance by (a) collecting mean and standard deviations of attributes in old instances of different
classes; (b) return the class whose attributes are statistically most similar to the new instance.
3 DT= decision trees Recursively divide data by selecting attribute splits that reduce the entropy of the class distribution.
4 “worst” SVM= support vector machines Map the raw data into a higher-dimensional space where it is easier to distinguish the examples.
Table 4: 22 highly cited Software Defect prediction studies.
RefYearCitationsRanked
Classifiers?Evaluated
using
multiple
criteria?Considered
Data
Imbalance?
[38]2007 855  2 
[32]2008 607  1 
[13]2008 298  2 
[40]2010 178  3 
[18]2008 159  1 
[30]2011 153  2 
[53]2013 150  1 
[25]2008 133  1 
[67]2013 115  1 
[35]2009 92  1 
[33]2012 79  2 
[28]2007 73  2 
[49]2007 66  1 
[27]2009 62  3 
[29]2010 60  1 
[17]2015 53  1 
[26]2008 41  1 
[62]2016 31  1 
[61]2015 27  2 
[50]2012 23  1 
[16]2016 15  1 
[4]2017 0  3 
are collected from many classifiers executed on many software
defectdatasets[ 13,16–18,21,25–27,29,32,33,35,40,53,62,67].
This section assesses those ranking studies. We will say a ranking
study is “good” if it compares multiple learners using multiple data
sets and multiple evaluation criteria while at the same time doing
something to address the data imbalance problem.
Figure 2: Summary of Table 4.InJuly2017,wesearched
scholar.google.com for
the conjunction of “soft-
ware”and“defectpredic-
tion”and“OO”and“CK”
published in the lastdecade. This returned231 results. We only se-
lected OO and CK key-
wordssinceCKmetrics
are more popular and
better than process met-
rics for software defect prediction [ 53]. From that list, we selected
“highly-cited” papers, which we defined as having more than 10
citations per year. This reduced our population of papers down
to107.Afterreadingthetitlesandabstractsofthosepapers,andskimming the contents of the potentially interesting papers, we
found22papersofTable4thateitherperformedrankingstudies
(asdefinedabove)orstudiedtheeffectsofclassimbalanceondefect
prediction.Inthecolumn“evaluatedusingmultiplecriteria”,papersscoredmorethan“1”iftheyusedmultipleperformancescoresof
the kind listed at the end of Section 2.2.
Wefindthat,inthose22papersfromTable4,numerousclassi-
fiershaveusedAUCasthemeasuretoevaluatethesoftwaredefect
predictor studies. We also found that majority of papers (from last
column of Table 4, 6/7=85%) in SE community has used SMOTE to
fix the data imbalance [ 4,28,49,50,61,67]. This also made us to
propose SMOTUNED. As noted in [ 17,32], no single classification
techniquealwaysdominates. Thatsaid,TableIX ofarecentstudy
by Ghotra et al. [ 17] ranks numerous classifiers using data similar
to what we use here (i.e., OO JAVA systems described using CK
metrics).Usingtheirwork,wecanselectarangeofclassifiersfor
this study ranking from “best” to “worst’: see Table 3.
The key observation to be made from this survey is that, as
showninFigure2,theoverwhelmingmajorityofpriorpapersin
oursample donotsatisfy ourdefinitionofa“good”project(thesole
exception is the recent Bennin et al. [ 4] which we explore in RQ4).
Accordingly, the rest of this paper defines and executes a “good”
ranking study, with an additional unique feature of an auto-tuning
version of SMOTE.
2.5 Handling Data Imbalance with SMOTE
SMOTE handles class imbalance by changing the frequency ofdifferent classes of the training data [
7]. The algorithm’s name
isshortfor“syntheticminorityover-samplingtechnique”.When
appliedtodata,SMOTEsub-samplesthemajorityclass(i.e.,deletes
someexamples)whilesuper-samplingtheminorityclassuntilall
classeshavethesamefrequency.Inthecaseofsoftwaredefectdata,
the minority class is usually the defective class.
defSMOTE(k=2, m=50%,r=2):# defaults
whileMajority > mdo
deleteanymajority item # random
whileMinority < mdo
add something_like(any minority item)
defsomething_like(X0):
relevant = emptySet
k 1=0
while(k1++ < 20 andsize(found) < k){
all= k1 nearest neighbors
relevant += items in"all"of X0class}
Z=anyof found
Y = interpolate (X0, Z)
returnY
defminkowski_distance(a,b,r):
return(Σiabs(ai−bi)r)1/r
Figure 3: Pseudocode of SMOTEFigure 3 shows
howSMOTEworks.
Duringsuper-sampling,
a member of the mi-
nority class finds k
nearestneighbors.It
builds an artificial
member of the mi-
nority class at some
pointin-betweenit-
self and one of its
randomnearestneigh-
bors. During thatprocess, some dis-
tancefunctionisre-
quired which is the
minkowski_distance function.
SMOTE’s control parameters are (a) kthat selects how many
neighborstouse(defaultsto k=5),(b) mishowmanyexamplesof
1053
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Is “Better Data” Better Than “Better Data Miners”? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
each class which need to be generated (defaults to m=50% of the
totaltrainingsamples),and(3) rwhichselectsthedistancefunction
(default is r=2, i.e., use Euclidean distance).
In the software analytics literature, there are contradictory find-
ingsonthevalueofapplyingSMOTEforsoftwaredefectprediction.
Van et al. [ 64], Pears et al. [ 47] and Tan et al. [ 61] found SMOTE to
be advantageous, while others, such as Pelayo et al. [49] did not.
Further,someresearchersreportthatsomelearnersrespondbet-
terthanotherstoSMOTE.Kameietal.[ 28]evaluatedtheeffectsof
SMOTEappliedtofourfault-pronenessmodels(lineardiscriminant
analysis, logistic regression, neural network, and decision tree) by
using two module sets of industry legacy software. They reported
thatSMOTEimprovedthepredictionperformanceofthelinearand
logisticmodels,butnotneuralnetworkanddecisiontreemodels.
Similar results, that the value of SMOTE was dependent on the
learner, was also reported by Van et al. [64].
Recently, Bennin et al. [ 4] proposed a new method based on the
chromosomal theory of inheritance. Their MAHAKIL algorithm
interpretstwodistinctsub-classesasparentsandgeneratesanew
synthetic instance that inherits different traits from each parent
and contributesto thediversity withinthe datadistribution. They
report that MAHAKIL usually performs as well as SMOTE, but
doesmuchbetterthanallotherclassbalancingtechniquesinterms
of recall. Please note, that work did not consider the impact of
parameter tuning of a preprocessor so in our RQ4 we will compare
SMOTUNED to MAHAKIL.
2.6 SMOTUNED = auto-tuning SMOTE
One possibleexplanation for the variabilityin the SMOTE results
is that the default parameters of this algorithm are not suited to
all data sets. To test this, we designed SMOTUNED, which is an
auto-tuning version of SMOTE. SMOTUNED uses different control
parameters for different data sets.
SMOTUNED uses DE (differential evolution [ 59]) to explore the
parameter space of Table 5. DE is an optimizer useful for functions
that may not be smooth or linear. Vesterstrom et al. [ 65] find DE’s
optimizations to be competitive with other optimizers like particle
swarm optimization or genetic algorithms. DEs have been usedbefore for parameter tuning [
2,9,14,16,46]) but this paper is
thefirstattempttodoDE-basedclassre-balancingforSEdataby
studying multiple learners for multiple evaluation criteria.
In Figure 4, DE evolves a frontierof candidates from an ini-
tialpopulationwhichisdrivenbyagoal(likemaximizingrecall)evaluated using a fitness function (shown in line 17). In the caseof SMOTUNED, each candidate is a randomly selected value for
SMOTE’s k,mandrparameters.Toevolvethefrontier,withineach
generation,DEcompareseachitemtoa newcandidategenerated
bycombining threeotherfrontier items(andbetter newcandidates
replace older items). To compare them, the betterfunction (line
17)calls SMOT Efunction(fromFigure3)usingtheproposed new
parameter settings. This pre-processed training data is then fed
into a classifier to find a particular measure (like recall). When our
DE terminates, it returns the best candidate ever seen in the entire
run.
Table6providesimportanttermsofSMOTUNEDwhenexplor-
ing SMOTE’s parameter ranges, shown in Table 5. To define theparameters, we found the range of used settings for SMOTE and1 defDE(n=10,cf=0.3,f=0.7):# default settings
2 frontier = sets of guesses (n=10)
3 best =frontier.1 # any value at all
4 lives = 1
5 while(lives−− > 0):
6 tmp = empty
7 fori=1t o|frontier|:# size of frontier
8 old =frontier i
9 x,y,z =anythreefromfrontier, picked at random
10 new= copy(old)
11 forj=1t o|new|:# for all attributes
12 ifrand() <cf# at probability cf...
13 new.j =x.j+f∗(z.j−y.j)# ...change item j
14 # end for
15 n e w=n e wif better(new,old) elseold
16 tmp i=n e w
17 ifbetter(new,best) then
18 be s t=n e w
19 lives++# enable one more generation
20 end
21 # end for
22 frontier = tmp
23 # end while
24 returnbest
Figure 4: SMOTUNED uses DE (differential evolution).
Table 5: SMOTE parameters
ParaDefaults
used by
SMOTETuning Range
(Explored by
(SMOTUNED)Description
k 5 [1,20]Number of neighbors
m 50%{50, 100, 200, 400} Number of synthetic examples to
create.Expressedasapercentof
final training data.
r 2 [0.1,5]Power parameter for the
Minkowski distance metric.
Table 6: Important Terms of SMOTUNED Algorithm
Keywords Description
Differential weight (f=0.7)Mutation power
Crossover probability (cf=0.3)Survival of the candidate
Population Size (n=10)Frontier size in a generation
LivesNumber of generations
Fitness Function (better)Driving factor of DE
Rand() function Returns between 0 to 1
Best (or Output) Optimal configuration for SMOTE
distance functions in the SE and machine learning literature. Toavoidintroducingnoisebyoverpopulatingtheminoritysamples
we are not using mas percentage rather than number of examples
to create. Aggarawal et al. [ 1] argue that with data being highly
dimensional, rshould shrink to some fraction less than one (hence
the bound of r=0.1 in Table 5).
3 EXPERIMENTAL DESIGN
This experiment reports the effects on defect prediction after using
MAHAKIL or SMOTUNED or SMOTE. Using some data Di∈D,
performance measure Mi∈M, and classifier Ci∈C, this experi-
mentconductsthe5*5cross-validationstudy,definedbelow.Our
datasets DareshowninTable7.TheseareallopensourceJAVA
OO systems described in terms of the CK metrics. Since, we are
comparingtheseresultsforimbalancedclass,onlyimbalancedclass
data sets were selected from SEACRAFT (http://tiny.cc/seacraft).
Our performance measures Mwere introduced in Section 2.2
which includes AUC, precision, recall, and the false alarm. Our
1054
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Amritanshu Agrawal and Tim Menzies
classifiers Ccomefromarecentstudy[ 17]andwerelistedinTable3.
For implementations of these learners, we used the open source
tool Scikit-Learn [ 48]. Our cross-validation study [ 56] is defined as
follows:
(1)We randomized the order of the data set Difive times. This
reduces the probability that some random ordering of examples
in the data will conflate our results.
(2) Each time, we divided the data Diinto five bins;
(3)For each bin (the test), we trained on four bins (the rest) and
then tested on the test bin as follows.
(a)The training set is pre-filtered using either No-SMOTE (i.e.,
do nothing) or SMOTE or SMOTUNED.
(b)When using SMOTUNED, we further divide those four bins
oftrainingdata.3binsareusedfortrainingthemodel,and
1 bin is used for validation in DE. DE is run to improve
the performance measure Miseen when the classifier Ci
was applied to the training data. Important point: we only
usedSMOTEon the training data, leaving the testing data
unchanged.
(c) After pre-filtering, a classifier Cilearns a predictor.
(d)Themodelisappliedtothetest datatocollectperformance
measure Mi.
(e)We print the relative performance delta between this Miand
another Migeneratedfromapplying Citotherawdata Di
(i.e., compare the learner without any filtering). We finally
report median on the 25 repeats.
Note that the above rig tunes SMOTE, but not the control pa-
rametersoftheclassifiers.Wedothissince,inthispaper,weaimto
documentthebenefitsoftuningSMOTEsinceasshownbelow,they
are very large indeed. Also, it would be very useful if we can show
thatasinglealgorithm(SMOTUNED)improvestheperformanceof
defectprediction.Thiswouldallowsubsequentworktofocuson
the task of optimizing SMOTUNED (which would be a far easier
task than optimizing the tuning of a wide-range of classifiers).
3.1 Within- vs Cross-Measure Assessment
We call the above rig as the within-measure assessment rig since it
isbiasedin itsevaluationmeasures. Specifically,inthis rig,when
SMOTUNED is optimized for (e.g.,) AUC, we do not explore theeffects on (e.g.,) the false alarm. This is less than ideal since it is
Table7:Datasetstatistics.Datasetsaresortedfromlowper-
centageofdefectiveclasstohighdefectiveclass.Datacomesfrom the SEACRAFT repository: http://tiny.cc/seacraft
.Version Dataset Name Defect % No. of classes lines of code
4.3 jEdit 2 492 202,363
1.0 Camel 4 339 33,721
6.0.3 Tomcat 9 858 300,674
2.0 Ivy 11 352 87,769
1.0 Arcilook 11.5 234 31,342
1.0 Redaktor 15 176 59,280
1.7Apache Ant 22 745 208,653
1.2 Synapse 33.5 256 53,500
1.6.1 Velocity 34 229 57,012
total: 3,681 1,034,314known that our performance measures are inter-connected viathe Zhang’s equation [
70]. Hence, increasing (e.g.,) recall might
potentially have the adverse effect of driving up (e.g) the false
alarmrate.Toavoidthisproblem,wealsoapplythefollowing cross-
measure assessment rig. At the conclusion of the within-measure
assessment rig, we will observe that the AUC performance measure
will show the largest improv ements. Using that best performer, we
will re-apply steps 1,2,3 abcde (listed above) but this time:
•In step 3b, we will tell SMOTUNED to optimize for AUC;
•Instep 3d,3ewe willcollect theperformancedelta onAUCas
well as precision, recall, and false alarm.
Inthisapproach,steps3dand3ecollecttheinformationrequiredto
checkifsucceedingaccordingtooneperformancecriteriaresults
indamagetoanother.Wewouldalsowanttomakesurethatour
model is not over-fitted based on one evaluation measure. And
sinceSMOTUNEDisatimeexpensivetask,wedonotwanttotune
for each measure which will quadruple the time. The results of
within- vs cross-measure assessment is shown in Section 4.
3.2 Statistical Analysis
WhencomparingtheresultsofSMOTUNEDtoothertreatments,we
useastatisticalsignificancetestandaneffectsizetest.Significance
test are useful for detecting if two populations differ merely byrandom noise. Also, effect sizes are useful for checking that two
populations differ by more than just a trivial amount.
For the significance test, we used the Scott-Knott procedure [ 17,
43]. This technique recursively bi-clusters a sorted set of numbers.
Ifanytwoclustersarestatisticallyindistinguishable,Scott-Knott
reports them both as one group. Scott-Knott first looks for a break
inthesequencethatmaximizestheexpectedvaluesinthedifference
in the means before and after the break. More specifically, it splits
lvaluesintosub-lists mandninordertomaximizetheexpected
value of differences in the observed performances before and after
divisions. For e.g., lists l,mandnof size ls,msandnswhere l=
m∪n,Scott-Knottdividesthesequenceatthebreakthatmaximizes:
E(Δ)=ms/ls∗abs(m.μ−l.μ)2+ns/ls∗abs(n.μ−l.μ)2
Scott-Knottthenappliessomestatisticalhypothesistest Htocheck
ifmandnare significantly different. If so, Scott-Knott then re-
cursesoneachdivision.Forthisstudy,ourhypothesistest Hwasa
conjunctionoftheA12effectsizetest(endorsedby[ 3])andnon-
parametric bootstrap sampling [ 12], i.e., our Scott-Knott divided
the data if bothbootstrapping and an effect size test agreed that
the division was statistically significant (99% confidence) and not a
“small” effect ( A12≥0.6).
4 RESULTS
RQ1:Arethedefault“off-the-shelf”parametersforSMOTEappropriate for all data sets?
As discussed above, the default parameters for SMOTE, k,m
andrare 5 ,50% and 2. Figure 5 shows the range of parameters
found by SMOTUNED across nine data sets for the 25 repeatsof our cross-validation procedure. All the results in this figure
arewithin-measureassessment results,i.e.,here,weSMOTUNED
on a particular performance measure and then we only collect
performance for that performance measure on the test set.
1055
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Is “Better Data” Better Than “Better Data Miners”? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Figure 5a: Tuned values for k
(default: k=5).
Figure 5b: Tuned values for m
(default: m=50%).
Figure 5c: Tuned values for r
(default: r=2).
Figure 5: Data sets vs Parameter Variation when optimized for recall and results reported on recall. “Median” denotes 50th
percentile values seen in the 5*5 cross-validations and “IQR” shows the intra-quartile range, i.e., (75-25)th percentiles.
Figure 6: SMOTUNED improvements over SMOTE. Within -Measure assessment (i.e., for eachof these charts, optimize for per-
formance measure Mi, then test for performance measure Mi). For most charts, largervalues are better, but for false alarm,
smallervalues are better. Note that the corresponding percentage of minority class (in this case, defective class) is written
beside each data set.
InFigure5,the medianisthe50thpercentilevalueand IQRisthe
(75-25)th percentile (variance). As can be seen in Figure 5, most of
thelearnedparametersarefarfromthedefaultvalues:1)Median k
isneverlessthan11;2)Median mdiffersaccordingtoeachdataset
andquitefarfromtheactual;3)The rusedinthedistancefunction
was never 2, rather, it was usually 3. Hence, our answer to RQ1is
“no”: the use of off-the-shelf SMOTE should be deprecated.
We note that many of the settings in Figure 5 are very simi-
lar; for e.g., median values of k=13 and r=3 seems to be a
commonresultirrespectiveofdataimbalancepercentageamong
the datasets. Nevertheless, we do notrecommend replacing the
defaults of SMOTE with the findings of Figure 5. Also, IQR bars
are very large. Clearly, SMOTUNED’s decisions vary dramatically
depending on what data is being processed. Hence, we strongly
recommend that SMOTUNED be applied to each new data set.
RQ2: Is there any benefit in tuning the default parameters
of SMOTE for each new data set?Figure 6 shows the performance delta of the within-measure as-
sessmentrig.PleaserecallthatwhenthisrigappliesSMOTUNED,itoptimizesforperformancemeasure,
Mi∈{recall ,precision ,fa l s e
alarm ,AU C}after which it uses the sameperformance measure
Miwhenevaluatingthetestdata.InFigure6,eachsubfigureshows
that DE is optimized for each M_iand results are reported against
thesame M_i.Fromthefigure6,itisobservedthatSMOTUNED
achieves large AUC (about 60%) and recall (about 20%) improve-
mentsrelativelywithoutdamagingprecisionandwithonlyminimal
changestofalsealarm.Anotherkeyobservationherethatcanbe
made is thatimprovements in AUC with SMOTUNED is constant
whether imbalance is of 34% or 2%. Another note should be taken
oftheAUCimprovements,thatthesearethelargestimprovements
we have yet seen, for any prior treatment of defect prediction data.
Also, for the raw AUC values, please see http://tiny.cc/raw_auc.
Figure 7 offers a statistical analysis of different results achieved
after applying our three data pre-filtering methods: 1) NO=d o
nothing, 2) S1= use default SMOTE, and 3) S2= use SMOTUNED.
Forany learner, there arethreesuchtreatmentsand darkerthecell,
1056
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Amritanshu Agrawal and Tim Menzies
Figure7:ScottKnottanalysisofNo-SMOTE,SMOTEandSMOTUNED.ThecolumnheadersaredenotedasNoforNo-SMOTE,
S1 for SMOTE and S2 for SMOTUNED. (∗)Mark represents the best learner combined with its techniques.
betterthe performance. In that figure, cells with the same color are
eithernotstatisticallysignificantlydifferentoraredifferentonly
via asmall effect (as judged by the statistical methods described in
Section 3.2).
As to what combination of pre-filter+learner works better for
anydataset,thatismarkedbya‘*’.Sincewehavethreepre-filtering
methods and six learners providing us with in-total 18 treatments,
and‘*’representsthebestlearnerpickedwithhighestmedianvalue.
In the AUC and recall results, the best “*” cell always appears in
theS2=SMOTUNEDcolumn,i.e.,SMOTUNEDisalwaysusedby
the best combination of pre-filter+learner .
As to precision results, at first glance, the results in Figure 7
lookbadforSMOTUNEDsince,lessthanhalfthetimes,thebest
“*” happens in S2=SMOTUNED column. But recall from Figure 6
thattheabsolutesizeoftheprecisiondeltasisverysmall.Hence,
even though SMOTUNED “losses” in this statistical analysis, thepragmatic impact of that result is negligible. But if we can get
feedbackfromdomain/expert,wecanchangebetweenSMOTEand
SMOTUNED dynamically based on the measures and data miners.
AstothefalsealarmresultsfromFigure7,asdiscussedabove
in Section 2.2, the cost of increased recall is to also increase the
false alarm rate. For e.g., the greatest increasein the recall was 0.58
seen in the jEditresults. This increase comes at a cost of increasing
thefalsealarmrateby0.20.Apartfromthisonelargeoutlier,the
overall pattern is that the recall improvements range from +0.18 to
+0.42 (median to max) and these come at the cost of much smaller
false alarm increaseof 0.07 to 0.16 (median to max).
Insummary,theanswerto RQ2isthatourAUCandrecallresults
stronglyendorsetheuseofSMOTUNEDwhiletheprecisionand
false alarm rates show there is little harm in using SMOTUNED.
Before moving to the next research question, we note that these
results offer an interesting insight on prior ranking studies. Based
on the Ghotra et al. results of Table 3, our expectation was that
Random Forests (RF) would yield the best results across this defect
data.Figure7reportsthat,aspredictedbyGhotraetal.,RFearns
more“stars”thananyotherlearner,i.e.,itisseentobe“best”more
often than anything else. That said, RF was only “best” in 11/36 of
those results, i.e., even our “best” learner (RF) fails over half the
time.
It is significant to note that SMOTUNED was consistently used
by whatever learner was found to be “best” (in recall and AUC).Hence, we conclude prior ranking study results (that only assessed
differentlearners)havemissedamuchmoregeneraleffect;i.e.it
canbemoreusefultoreflectondatapre-processorsthanalgorithm
selection. To say that another way, at least for defect prediction,
“better data” might be better than “better data miners”.
RQ3:Intermsofruntimes,isthecostofrunningSMOTUNED
worth the performance improvement?
Figure 8 shows the mean runtimes for running a 5*5 cross-
validation study forsix learners for each data set.These runtimes
werecollectedfromonemachinerunningCENTOS7,with16cores.
Notethattheydonotincreasemonotonicallywiththesizeofthe
datasets–aresultwecanexplainwithrespecttotheinternalstruc-
ture of the data. Our version of SMOTE uses ball trees to optimize
the nearest neighbor calculations. Hence, the runtime of that algo-
rithm is dominated by the internal topology of the data sets rather
thanthenumberofclasses.Also,asshowninFigure3,SMOTUNED
explores the local space until it finds kneighbors of the same class.
This can take a variable amount of time to terminate.
Figure 8: Data sets vs Runtimes. Note that the numbersshown here are the mean times seen across 25 repeats of a5*5 cross-validation study.
As expected, SMOTUNED is an order of magnitude slower than
SMOTEsinceithastorunSMOTEmanytimestoassessdifferentpa-rametersettings.Thatsaid,thoseruntimesarenotexcessivelyslow.
SMOTUNEDusuallyterminatesinundertwominutesandnever
more than half an hour. Hence, in our opinion, we answer RQ3
1057
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Is “Better Data” Better Than “Better Data Miners”? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Figure 9: SMOTUNED improvements over MAHAKIL [4]. Within -Measure assessment (i.e., for each of these charts, optimize
for performance measure Mi, then test for performance measure Mi). Same format as Figure 6.
as “yes” since the performance increment seen in Figure 6 is more
than to compensate for the extra CPU required for SMOTUNED.
RQ4: How does SMOTUNED perform against more recent
class imbalance technique?
All the above work is based on tuning the original 2002 SMOTE
paper [7]. While that version of SMOTE is widely used in the SE
literature, it is prudent to compare SMOTUNED with more recent
work.OurreadingoftheliteratureisthattheMAHAKILalgorithm
ofBenninetal.[ 4]representsthemostrecentworkinSEonhan-
dling class imbalance. At the time of writing of this paper (early
August2017),therewasnoreproductionpackageavailableforMA-
HAKILsowewroteourownversionbasedonthedescriptioninthat paper (Available on http://tiny.cc/mahakil). We verified our
implementationontheirdatasets,andachievedclosetotheirvalues
±0.1. The difference could be due to different random seed.
Figure9comparesresultsfromMAHAKILwiththosefromSMO-
TUNED.Theseresultsweregeneratedusingthesameexperimental
methods as used for Figure 6 (those methods were described in
Section3.1).Thefollowingtablerepeatsthestatisticalanalysisof
Figure7toreporthow oftenSMOTE,SMOTUNED,orMAHAKIL
achieves best results across nine data sets. Note that, in this follow-
ing table, largervalues are better:
number of wins
Treatments AUCRecall Precision False Alarm
MAHAKIL 1/90/96/9 9/9
SMOTE 0/91/90/9 0/9
SMOTUNED 8/98/9 3/9 0/9
These statistical tests tell us that the differences seen in Figure 9
are large enough to be significant. Looking at Figure 9, there are9datasetsonx-axis,andthedifferencesinprecisionaresosmallin 7 out of those 9 data sets that the pragmatic impact of those
differencesissmall.AstoAUCandrecall,weseethatSMOTUNED
generatedlarger andbetter resultsthanMAHAKIL (especiallyfor
recall).SMOTUNEDgeneratesslightlylargerfalsealarmsbut,in
7/9 data sets, the increase in the false alarm rate is very small.Accordingtoitsauthors[ 4],MAHAKILwasdevelopedtoreduce
the false alarm rates on SMOTE and on that criteria it succeeds (as
seeninFigure9,sinceSMOTUNEDdoesleadtoslightlyhigherfalsealarmrates).But,asdiscussedaboveinsection2.2,thedownsideonminimizingfalsealarmsisalsominimizingourabilitytofinddefects
whichismeasuredintermsofAUCandrecall,SMOTUNEDdoes
best. Hence, if this paper was acomparative assessment ofSMO-
TUNED vs MAHAKIL, we would conclude that by recommending
SMOTUNED.
However, thegoal of this paperis to defend theclaim that “bet-
ter data” could be better than “better data miners”, i.e., data pre-
processingismoreeffective thanswitchingtoanotherdataminer.
In this regard, there is something insightful to conclude if we com-
bine the results of bothMAHAKIL and SMOTUNED. In the MA-
HAKILexperiments,theresearchersspentsometimeontuningthelearner’sparameters.Thatis,Figure9isreallyacomparisonoftwo
treatments: tuned data miners+adjust data against just using SMO-
TUNED to adjust the data. Note that SMOTUNED still achieves
better results even though the MAHAKIL treatment adjusted both
dataanddataminers.SinceSMOTUNEDperformedsowellwithout
tuningthedataminers,wecanconcludefromtheconjunctionof
these experiments that “better data” is better than using “better
data miners”.
Of course, there needs to be further studies done in other SE
applications to make the above claim. There is also one more treat-
mentnotdiscussedinthepaper:tuning boththedatapre-processor
andthedataminers.Thisisavery,verylargesearchspacesowhile
we have experiments running to explorethis task, atthis time we
have not definitive conclusions to report.
5 THREATS TO VALIDITY
As with any empirical study, biases can affect the final results.
Therefore,anyconclusionsmadefromthisworkmustconsiderthe
following issues in mind.
Orderbias :Witheachdatasethowdatasamplesaredistributed
in training and testing set is completely random. Though there
1058
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Amritanshu Agrawal and Tim Menzies
Figure10:SMOTUNEDimprovementsoverSMOTE.Cross -Measureassessment(i.e.,foreachofthesecharts,optimizeforAUC ,
then test for performance measure Mi). Same format as Figure 6.
could be times when all good samples are binned into training and
testingset.Tomitigatethisorderbias,weruntheexperiment25
times by randomly changing the order of the data samples each
time.
Samplingbias threatensanyclassificationexperiment,i.e.,what
matterstheremaynotbetruehere.Fore.g.,thedatasetsusedhere
comesfromtheSEACRAFTrepositoryandweresuppliedbyone
individual. These data sets have used in various case studies by
various researchers [ 24,51,52,63], i.e., our results are not more
biasedthanmanyotherstudiesinthisarena.Thatsaid,ournine
open-source data sets are mostly from Apache. Hence it is an open
issueifourresultsholdforproprietaryprojectsandopensource
projects from other sources.
Evaluation bias : In terms of evaluation bias, our study is far
less biased than many other ranking studies. As shown by our
sampleof22rankingstudiesinTable4,19/22ofthosepriorstudies
usedfewerevaluation criteria than the four reported here (AUC,
recall, precision and false alarm).
TheanalysisdoneinRQ4couldbeaffectedbysomeothersettings
whichwemightnothaveconsideredsincethereproductionpackage
was not available from the original paper [ 4]. That said, there is
anothermoresubtleevaluationbiasarisesintheFigure6.Thefour
plots of that figure are four differentruns of our within-measure
assessment rig (defined in Section 3.1). Hence, it is reasonable to
check what happens when (a) one evaluation criteria is used to
control SMOTUNED, and (b) the results are assessed using all four
evaluation criteria. Figure 10 shows the results of such a cross-
measureassessmentrig whereAUCwasusedtocontrolSMOTUNED.
WenotethattheresultsinthisfigureareverysimilartoFigure6,
e.g.,theprecisiondeltasaverusuallytiny,andfalsealarmincreases
areusuallysmallerthantheassociatedrecallimprovements.But
there are some larger improvements in Figure 6 than Figure 10.Hence, we recommend cross-measure assessment only if CPU iscritically restricted. Otherwise, we think SMOTUNED should becontrolledbywhateveristhedownstreamevaluationcriteria(as
done in the within-measure assessment rig of Figure 6.)
6 CONCLUSION
Prior work on ranking studies tried to improve software analytics
by selecting better learners. Our results show that there may bemorebenefits in exploring data pre-processors like SMOTUNED
because we found that no learner was usually “best” across all data
sets and all evaluation criteria. On one hand, across the same data
sets, SMOTUNEDwasconsistently usedby whateverlearner was
found to be “best” in the AUC/recall results. On the other hand,
fortheprecisionandfalsealarmresults,therewaslittleevidence
against the use of SMOTUNED. That is, creating better training
data (usingtechniques like SMOTUNED) maybe more important
than the subsequent choice of a classifier. To say that another way,
at least for defect prediction, “better data” is better than “better
data miners”.
Asto specificrecommendations, we suggestthat anyprior rank-
ing study which did not study the effects of data pre-processing
needs to be analyzed again. Any future such ranking study should
includeaSMOTE-likepre-processor.SMOTEshouldnotbeused
with its default parameters. For each new data set, SMOTE should
be used with some automatic parameter tuning tool in order tofind the best parameters for that data set. SMOTUNED is one of
the examplesof parametertuning. Ideally,SMOTUNED should be
tunedusingtheevaluationcriteriausedtoassessthefinalpredic-
tors. However, if there is not enough CPU to run SMOTUNED for
eachnewevaluationcriteria,SMOTUNEDcanbetunedusingAUC.
REFERENCES
[1]Charu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. 2001. On the
surprisingbehaviorofdistancemetricsinhighdimensionalspace.In International
Conference on Database Theory. Springer, 420–434.
[2]Amritanshu Agrawal, Wei Fu, and Tim Menzies. 2016. What is wrong with
topic modeling?(and how to fix it using search-based se). arXiv preprint
1059
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. Is “Better Data” Better Than “Better Data Miners”? ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
arXiv:1608.08176 (2016).
[3]AndreaArcuriandLionelBriand.2011. Apracticalguideforusingstatisticaltests
toassessrandomizedalgorithmsinsoftwareengineering.In SoftwareEngineering
(ICSE), 2011 33rd International Conference on. IEEE, 1–10.
[4]KwabenaEboBennin,JackyKeung,PassakornPhannachitta,AkitoMonden,and
Solomon Mensah. 2017. MAHAKIL: Diversity based Oversampling Approach
to Alleviate the Class Imbalance Issue in Software Defect Prediction. IEEE
Transactions on Software Engineering (2017).
[5]Christian Bird, Nachiappan Nagappan, Harald Gall, Brendan Murphy, and
Premkumar Devanbu. 2009. Putting it all together: Using socio-technical net-
works to predict failures. In 2009 20th ISSRE. IEEE, 109–119.
[6]Cagatay Catal and Banu Diri. 2009. A systematic review of software fault predic-
tion studies. Expert systems with applications 36, 4 (2009), 7346–7354.
[7]NiteshV.Chawla,KevinW.Bowyer,LawrenceO.Hall,andW.PhilipKegelmeyer.
2002. SMOTE: synthetic minority over-sampling technique. Journal of artificial
intelligence research 16 (2002), 321–357.
[8]Shyam R Chidamber and Chris F Kemerer. 1994. A metrics suite for object
oriented design. IEEE Transactions on software engineering 20, 6 (1994), 476–493.
[9]I. Chiha, J. Ghabi, and N. Liouane. 2012. Tuning PID controller with multi-
objective differential evolution. In ISCCSP ’12. IEEE, 1–4.
[10]MarcoD’Ambros,MicheleLanza,andRomainRobbes.2010. Anextensivecom-
parison of bug prediction approaches. In 2010 7th IEEE MSR). IEEE, 31–41.
[11]Richard O Duda, Peter E Hart, and David G Stork. 2012. Pattern classification.
John Wiley & Sons.
[12]Bradley Efron and Robert J Tibshirani. 1994. An introduction to the bootstrap.
Chapman and Hall, London.
[13]Karim O Elish and Mahmoud O Elish. 2008. Predicting defect-prone software
modules using support vector machines. JSS81, 5 (2008), 649–660.
[14]WeiFuandTimMenzies.2017. EasyoverHard:ACaseStudyonDeepLearning.
arXiv preprint arXiv:1703.00133 (2017).
[15]Wei Fu and Tim Menzies. 2017. Revisiting unsupervised learning for defect
prediction.In Proceedingsofthe201711thJointMeetingonFoundationsofSoftware
Engineering. ACM, 72–83.
[16]WeiFu,TimMenzies,andXipengShen.2016. Tuningforsoftwareanalytics:Isit
really necessary? IST76 (2016), 135–146.
[17]BaljinderGhotra,ShaneMcIntosh,andAhmedEHassan.2015. Revisitingtheim-
pact of classification techniques on the performance of defect prediction models.
In37th ICSE-Volume 1. IEEE Press, 789–800.
[18]Iker Gondra. 2008. Applying machine learning to software fault-proneness
prediction. Journal of Systems and Software 81, 2 (2008), 186–195.
[19]David Gray, David Bowes, Neil Davey, Yi Sun, and Bruce Christianson. 2009.
Using the support vector machine as a classification method for software defect
predictionwithstaticcodemetrics.In InternationalConferenceonEngineering
Applications of Neural Networks. Springer, 223–234.
[20]PhilipJGuo,ThomasZimmermann,NachiappanNagappan,andBrendanMurphy.
2011. Not my bug! and other reasons for software bug report reassignments. In
Proceedings of the ACM 2011 conference on Computer supported cooperative work.
ACM, 395–404.
[21]Tracy Hall, Sarah Beecham, David Bowes, David Gray, and Steve Counsell. 2012.
A systematic literature review on fault prediction performance in software engi-
neering.IEEE TSE 38, 6 (2012), 1276–1304.
[22]Ahmed EHassan. 2009. Predicting faults usingthe complexity ofcode changes.
In31st ICSE. IEEE Computer Society, 78–88.
[23]Haibo He and Edwardo A Garcia. 2009. Learning from imbalanced data. IEEE
Transactions on knowledge and data engineering 21, 9 (2009), 1263–1284.
[24]Zhimin He, Fengdi Shu, Ye Yang, Mingshu Li, and Qing Wang. 2012. An investi-
gationonthefeasibilityofcross-projectdefectprediction. AutomatedSoftware
Engineering 19, 2 (2012), 167–199.
[25]Yue Jiang, Bojan Cukic, and Yan Ma. 2008. Techniques for evaluating fault
prediction models. Empirical Software Engineering 13, 5 (2008), 561–595.
[26]YueJiang,BojanCukic,andTimMenzies.2008. Candatatransformationhelp
in the detection of fault-prone modules?. In Proceedings of the 2008 workshop on
Defects in large software systems. ACM, 16–20.
[27]YueJiang,JieLin,BojanCukic,andTimMenzies.2009. Varianceanalysisinsoft-
ware fault prediction models. In Software Reliability Engineering, 2009. ISSRE’09.
20th International Symposium on. IEEE, 99–108.
[28]YasutakaKamei,AkitoMonden,ShinsukeMatsumoto,TakeshiKakimoto,and
Ken-ichiMatsumoto.2007. Theeffectsofoverandundersamplingonfault-prone
module detection. In ESEM 2007. IEEE, 196–204.
[29]Taghi M Khoshgoftaar, Kehan Gao, and Naeem Seliya. 2010. Attribute selection
and imbalanced data: Problems in software defect prediction. In Tools with Artifi-
cial Intelligence (ICTAI), 2010 22nd IEEE International Conference on, Vol. 1. IEEE,
137–144.
[30]SunghunKim,HongyuZhang,RongxinWu,andLiangGong.2011. Dealingwith
noise in defect prediction. In Software Engineering (ICSE), 2011 33rd International
Conference on. IEEE, 481–490.
[31]Rahul Krishna, Tim Menzies, and Lucas Layman. 2017. Less is More: MinimizingCodeReorganizationusingXTREE. InformationandSoftwareTechnology (2017).[32]Stefan Lessmann, Bart Baesens, Christophe Mues, and Swantje Pietsch. 2008.
Benchmarking classification models for software defect prediction: A proposed
framework and novel findings. IEEE TSE 34, 4 (2008), 485–496.
[33]Ming Li, Hongyu Zhang, Rongxin Wu, and Zhi-Hua Zhou. 2012. Sample-based
software defect prediction with active and semi-supervised learning. Automated
Software Engineering 19, 2 (2012), 201–230.
[34]MichaelLowry,MarkBoyd,andDeepakKulkami.1998. Towardsatheoryforintegration of mathematical verification and empirical testing. In Automated
Software Engineering, 1998. Proceedings. 13th IEEE International Conference on.
IEEE, 322–331.
[35]Thilo Mende and Rainer Koschke. 2009. Revisiting the evaluation of defect
prediction models. In Proceedings of the 5th International Conference on Predictor
Models in Software Engineering. ACM, 7.
[36]Tim Menzies, Alex Dekhtyar, Justin Distefano, and Jeremy Greenwald. 2007.
Problems with Precision: A Response to" commentson’datamining static code
attributes to learn defect predictors’". IEEE TSE 33, 9 (2007).
[37]Tim Menzies and Justin S. Di Stefano. 2004. How Good is Your Blind Spot
SamplingPolicy.In ProceedingsoftheEighthIEEEInternationalConferenceonHigh
Assurance Systems Engineering (HASE’04). IEEE Computer Society, Washington,
DC, USA, 129–138. http://dl.acm.org/citation.cfm?id=1890580.1890593
[38]Tim Menzies, JeremyGreenwald, and ArtFrank. 2007. Data miningstatic code
attributes to learn defect predictors. IEEE TSE 33, 1 (2007), 2–13.
[39]Tim Menzies, Ekrem Kocaguneli, Burak Turhan, Leandro Minku, and Fayola
Peters.2014. Sharingdataandmodelsinsoftwareengineering. MorganKaufmann.
[40]TimMenzies,ZachMilton,BurakTurhan,BojanCukic,YueJiang,andAyşeBener.
2010. Defect prediction from static code features: current results, limitations,
new approaches. Automated Software Engineering 17, 4 (2010), 375–407.
[41]Tim Menzies and Erik Sinsel. 2000. Practical large scale what-if queries: Case
studies with software risk assessment. In Automated Software Engineering, 2000.
Proceedings ASE 2000. The Fifteenth IEEE International Conference on. IEEE, 165–
173.
[42]Tim Menzies, Burak Turhan, Ayse Bener, Gregory Gay, Bojan Cukic, and Yue
Jiang.2008. Implicationsofceiling effectsindefect predictors.In Proceedingsof
the 4th international workshop on Predictor models in software engineering. ACM,
47–54.
[43]NikolaosMittasandLefterisAngelis.2013. Rankingandclusteringsoftwarecost
estimation models through a multiple comparisons algorithm. IEEE Transactions
on software engineering 39, 4 (2013), 537–551.
[44]Nachiappan Nagappan and Thomas Ball. 2005. Static analysis tools as early
indicatorsofpre-releasedefectdensity.In Proceedingsofthe27thinternational
conference on Software engineering. ACM, 580–586.
[45]NachiappanNagappan, ThomasBall, andAndreasZeller. 2006. Miningmetrics
to predict component failures. In Proceedings of the 28th international conference
on Software engineering. ACM, 452–461.
[46]M.Omran,A.P.Engelbrecht,andA.Salman.2005. Differentialevolutionmethods
for unsupervised image classification. In IEEE Congress on Evolutionary Compu-
tation ’05, Vol. 2. 966–973.
[47]Russel Pears, Jacqui Finlay, and Andy M Connor. 2014. Synthetic Minority over-
sampling technique (SMOTE) for predicting software build outcomes. arXiv
preprint arXiv:1407.2330 (2014).
[48]Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
BertrandThirion,OlivierGrisel,MathieuBlondel,PeterPrettenhofer,RonWeiss,
Vincent Dubourg, and others. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research 12, Oct (2011), 2825–2830.
[49]LourdesPelayoandScottDick.2007. Applyingnovelresamplingstrategiesto
software defect prediction. In NAFIPS 2007-2007 Annual Meeting of the North
American Fuzzy Information Processing Society. IEEE, 69–72.
[50]Lourdes Pelayo and Scott Dick. 2012. Evaluating stratification alternatives to
improve software defect prediction. IEEE Transactions on Reliability 61, 2 (2012),
516–525.
[51]FayolaPeters,TimMenzies,LiangGong,andHongyuZhang.2013. Balancing
privacy and utility in cross-company defect prediction. IEEE Transactions on
Software Engineering 39, 8 (2013), 1054–1068.
[52]FayolaPeters,TimMenzies,andAndrianMarcus.2013. Bettercrosscompany
defect prediction. In Mining Software Repositories (MSR), 2013 10th IEEE Working
Conference on. IEEE, 409–418.
[53]Danijel Radjenović, Marjan Heričko, Richard Torkar, and Aleš Živkovič. 2013.
Software fault prediction metrics: A systematic literature review. Information
and Software Technology 55, 8 (2013), 1397–1418.
[54]Foyzur Rahman, Sameer Khatri, Earl T. Barr, and Premkumar Devanbu. 2014.
Comparing Static Bug Finders and Statistical Prediction (ICSE). ACM, New York,
NY, USA, 424–434. DOI:http://dx.doi.org/10.1145/2568225.2568269
[55]MitchRees-Jones,MatthewMartin,andTimMenzies.2017. BetterPredictorsfor
Issue Lifetime. CoRRabs/1702.07735 (2017). http://arxiv.org/abs/1702.07735
[56]Payam Refaeilzadeh, Lei Tang, and Huan Liu. 2009. Cross-validation. In Encyclo-
pedia of database systems. Springer, 532–538.
[57]JCRiquelme,RRuiz,DRodríguez,andJMoreno.2008. Findingdefectivemodules
fromhighlyunbalanceddatasets. ActasdelosTalleresdelasJornadasdeIngeniería
1060
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Amritanshu Agrawal and Tim Menzies
del Software y Bases de Datos 2, 1 (2008), 67–74.
[58]Martin Shepperd, David Bowes, and Tracy Hall. 2014. Researcher bias: The use
of machine learning in software defect prediction. IEEE Transactions on Software
Engineering 40, 6 (2014), 603–616.
[59]Rainer Storn and Kenneth Price. 1997. Differential evolution–a simple and
efficient heuristic for global optimization over continuous spaces. Journal of
global optimization 11, 4 (1997), 341–359.
[60]John A Swets. 1988. Measuring the accuracy of diagnostic systems. Science240,
4857 (1988), 1285.
[61]Ming Tan, Lin Tan, Sashank Dara, and Caleb Mayeux. 2015. Online defect
prediction for imbalanced data. In ICSE-Volume 2. IEEE Press, 99–108.
[62]Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, and Kenichi
Matsumoto.2016. Automatedparameteroptimizationofclassificationtechniques
for defect prediction models. In ICSE 2016. ACM, 321–332.
[63]BurakTurhan,AyşeTosunMısırlı,andAyşeBener.2013. Empiricalevaluation
of the effects of mixed project data on learning defect predictors. Information
and Software Technology 55, 6 (2013), 1101–1118.
[64]Jason Van Hulse, Taghi M Khoshgoftaar, and Amri Napolitano. 2007. Experi-
mental perspectives on learning from imbalanced data. In Proceedings of the 24th
international conference on Machine learning. ACM, 935–942.[65]Jakob Vesterstrom and Rene Thomsen. 2004. A comparative study of differential
evolution, particle swarm optimization, and evolutionary algorithms on numeri-
cal benchmark problems. In Evolutionary Computation, 2004. CEC2004. Congress
on, Vol. 2. IEEE, 1980–1987.
[66]Jeffrey M. Voas and Keith W Miller. 1995. Software testability: The new verifica-
tion.IEEE software 12, 3 (1995), 17–28.
[67]ShuoWangandXinYao.2013. Usingclassimbalancelearningforsoftwaredefect
prediction. IEEE Transactions on Reliability 62, 2 (2013), 434–443.
[68]Zhen Yan, Xinyu Chen, and Ping Guo. 2010. Software defect prediction using
fuzzy support vector regression. In International Symposium on Neural Networks.
Springer, 17–24.
[69]Qiao Yu, Shujuan Jiang, and Yanmei Zhang. 2017. The Performance Stability
of Defect Prediction Models with Class Imbalance: An Empirical Study. IEICE
TRANSACTIONS on Information and Systems 100, 2 (2017), 265–272.
[70]HongyuZhangandXiuzhenZhang.2007. CommentsonDataMiningStaticCode
Attributes to Learn Defect Predictors. IEEE Transactions on Software Engineering
33, 9 (2007), 635–637.
[71]ThomasZimmermannandNachiappanNagappan.2008. Predictingdefectsusing
network analysis on dependency graphs. In ICSE. ACM, 531–540.
1061
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:56:19 UTC from IEEE Xplore.  Restrictions apply. 