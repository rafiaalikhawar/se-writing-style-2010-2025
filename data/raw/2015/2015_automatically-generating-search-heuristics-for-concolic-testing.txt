Automatically Generating Search Heuristics for Concolic Testing
Sooyoung Cha
Korea University
sooyoungcha@korea.ac.krSeongjoon Hong
Korea University
seongjoon@korea.ac.krJunhee Lee
Korea University
junhee_lee@korea.ac.krHakjoo Oh∗
Korea University
hakjoo_oh@korea.ac.kr
ABSTRACT
We present a technique to automatically generate search heuristics
forconcolictesting.Akeychallengeinconcolictestingishowto
effectivelyexplore theprogram’s executionpathsto achievehigh
code coverage in a limited time budget. Concolic testing employs a
searchheuristictoaddressthischallenge,whichfavorsexploring
particular types of paths that are most likely to maximize the final
coverage. However, manually designing a good search heuristic
isnontrivialandtypicallyendsupwithsuboptimalandunstable
outcomes.Thegoalofthispaperistoovercomethisshortcomingof
concolic testing by automatically generating search heuristics. We
defineaclassofsearchheuristics,namelyaparameterizedheuristic,
and present an algorithm that efficiently finds an optimal heuristic
foreachsubjectprogram.Experimentalresultswithopen-sourceC
programsshowthatourtechniquesuccessfullygeneratessearch
heuristicsthatsignificantlyoutperformexistingmanually-crafted
heuristics in terms of branch coverage and bug-finding.
CCS CONCEPTS
•Software and its engineering →Software testing and de-
bugging;
ACM Reference Format:
SooyoungCha,SeongjoonHong,JunheeLee,andHakjooOh.2018.Auto-
matically Generating Search Heuristics for Concolic Testing. In ICSE ’18:
ICSE ’18: 40th International Conference on Software Engineering , May 27-
June 3, 2018, Gothenburg, Sweden. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3180155.3180166
1 INTRODUCTION
Concolic testing [ 15,28] has emerged as an effective software-
testing method with diverse applications [ 1,7,21,30,33]. The idea
of concolic testing is to symbolically execute a program alongside
the concrete execution, where the main job of the symbolic execu-
tion is to collect path conditions. Initially, the program is executed
witharandominput.Aftertheprogramfinishes,abranchofthe
current path is selected and negated to find an input that drives
thenextprogramexecutiontofollowapreviouslyunexploredpath.
This way concolic testing systematically explores the execution
paths of the program, greatly improving random testing.
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
on the first page. Copyrights for components of this work owned by others than ACMmustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee. Request permissions from permissions@acm.org.
ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
©2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5638-1/18/05...$15.00
https://doi.org/10.1145/3180155.3180166A key component of concolic testing is the so-called search
heuristic. Because of the path-explosion problem, exploring all exe-
cution paths of a nontrivial program is simply impossible. Instead,
concolic testing relies on a search heuristicto maximize code cov-
erage in a limited time budget. A search heuristic has a criterion
andsteersconcolictestingbychoosingthebestbranchtonegate
according to the criterion. For example, the CFDS (Control-Flow
DirectedSearch)heuristic[ 3]picksthebranchthatisclosesttothe
uncoveredregionsoftheprogramandtheCGS(Context-Guided
Search) heuristic [ 29] selects a branch only if it is in a new context.
It is well-known thatthe effectiveness of concolic testing depends
heavily on the choice of the search heuristic [3, 21, 27, 29].
However,manuallydesigningsuchaheuristicischallenging.Itis
notonly nontrivialbut alsolikely todeliver sub-optimal andunsta-
bleresults.Aswedemonstrateinthispaper,nomanually-designedexistingheuristicsconsistentlyachievegoodcodecoverageinprac-
tice.Forexample,theCGSheuristicisarguablyastate-of-the-art
andoutperformsexistingapproachesforanumberofprograms[ 29].
However,wefoundthatCGSissometimesbrittleandinferioreventoarandomheuristic.Furthermore,existingsearchheuristicscame
from a huge amount of engineering effort and domain expertise.
The difficulty of manually coming up with a good search heuristic
is a major remaining challenge in concolic testing.
To addressthis challenge,this paper presentsa newapproach
that automatically generates search heuristics for concolic testing.
To this end, we use two key ideas. First, we define a parameterized
search heuristic, which creates a large class of search heuristics.
The parameterized heuristic reduces the problem of designing a
goodsearchheuristicintoaproblemoffindingagoodparameter
value.Second,wepresentasearchalgorithmspecializedtoconcolic
testing. The search space that the parameterized heuristic poses is
intractably large. Our algorithm effectively guides the search byiteratively refining the search space based on the feedback from
previous runs of concolic testing.
Experimental results show that automatically-generated heuris-
tics by our approach outperform existing manually-crafted heuris-
ticsforarangeofCprograms.Wehaveimplementedourtechnique
in CREST [ 3] and evaluated it on 10 C programs (0.5–150KLoC).
For every benchmark program, our technique has successfully gen-
erated a search heuristic that achieves considerably higher branch
coverage than the existing state-of-the-art techniques. We also
demonstrate that the increased coverage by our technique leads to
more effective finding of real bugs.
This paper makes the following contributions:
•We present a new approach for automatically generating
search heuristics for concolic testing. Our work represents a
significantdeparturefrompriorwork;whileexistingwork
(e.g.[3,21,27,29])focusesonmanuallydevelopingaparticu-
larsearchheuristic,ourgoalistoautomatetheveryprocess
of generating such a heuristic.
12442018 ACM/IEEE 40th International Conference on Software Engineering
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Sooyoung Cha, Seongjoon Hong, Junhee Lee, and Hakjoo Oh
•Wepresentaparameterizedsearchheuristicandanefficient
algorithm for finding good parameter values.
•We extensively evaluate our approach with C programs. We
makeourtool,calledParaDySE,anddatapubliclyavailable.1
2 PRELIMINARIES
InSection2.1,wedefineagenericconcolictestingalgorithm.Sec-
tion 2.2 discusses existing search heuristics and their limitations.
2.1 Concolic Testing
Concolictestingisahybridsoftwaretestingtechniquethatcom-
binessymbolic[ 24]andconcreteexecutionstosystematicallyex-
plore the program’s execution paths.
Concolic testing begins with executing the subject program P
with an initial input v0. During the concrete execution, concolic
testingmaintainsa symbolicmemorystate Sandapathcondition
Φ. The symbolic memory is a mapping from program variables
to symbolic values. It is used to evaluate the symbolic values of
expressions. For instance, when Sis[x/mapsto→α,y/mapsto→β+1](variables
xandyaremappedtosymbolicexpressions αandβ+1where α
andβaresymbols),thestatement z:=x+ytransfersthesymbolic
memory into [x/mapsto→α,y/mapsto→β+1,z/mapsto→α+β+1]. The path
condition represents the sequence of branches taken during the
currentexecutionoftheprogram.Itisupdatedwheneveranassume
statement assume(e)is encountered. For instance, when S=[x/mapsto→
α]ande=x<1,thepathcondition Φgetsupdatedby Φ∧(α<1).
LetΦ=ϕ1∧ϕ2∧···∧ ϕnbe the path condition that results
from the initial execution. To obtain the next input value, concolic
testing chooses a branch condition ϕiand generates the new path
condition Φ/primeas follows: Φ/prime=/logicalandtext.1
j<iϕj∧¬ϕi. That is, the new
condition Φ/primehas the same prefix as Φup to the i-th branch with
ϕinegated,sothatinputvaluesthatsatisfy Φ/primedrivetheprogram
execution to follow the opposite branch of ϕi. Such concrete input
valuescanbeobtainedfromanSMTsolver.Thisprocessisrepeated
until a fixed testing budget runs out.
Algorithm1presentstheconcolictestingalgorithm.Thealgo-
rithm takes a program P, an initial input vector v0, and a testing
budget N(i.e., the number of executions of the program). The algo-
rithm maintains the execution tree Tof the program, which is the
list of previously explored path conditions. The execution tree T
andinputvector vareinitiallyemptyandtheinitialinputvector,
respectively (lines 1 and 2). At line 4, the program Pis executed
with the input v, resulting in the current execution path Φmex-
plored.Thepathconditionisappendedto T(line5).Inlines6–8,
the algorithm chooses a branch to negate. The function Choose
first chooses apath condition Φfrom T, thenselects abranch,i.e.,
ϕi,f r o m Φ. Once a branch ϕiis chosen, the algorithm generates
the new path condition Φ/prime=/logicalandtext.1
j<iϕj∧¬ϕi.I fΦ/primeis satisfiable,
the next input vector is computed (line 9), where SAT(Φ)returns
true iff Φis satisfiable andmodel(Φ) finds an input vector vwhich
is a model of Φ, i.e., v|=Φ. Otherwise, if Φ/primeis unsatisfiable, the
algorithm repeatedly triesto negate another branch untila satisfi-
ablepathconditionisfound.Thisprocedurerepeatsforthegiven
budget Nand the final number of covered branches |Branches( T)|
is returned.
1Parametric Dy namic Symbolic E xecution: https://github.com/kupl/ParaDySEAlgorithm 1: Concolic Testing
Input :Program P, initial input vector v0, budget N
Output:The number of branches covered
1:T←/angbracketleft /angbracketright
2:v←v0
3:form=1toNdo
4:Φm←RunProgram( P,v)
5: T←T·Φm
6:repeat
7:(Φ,ϕi)←Choose( T)( Φ=ϕ1∧···∧ϕ n)
8:until SAT(/logicalandtext.1
j<iϕj∧¬ϕi)
9: v←model(/logicalandtext.1
j<iϕj∧¬ϕi)
10:end for
11:return|Branches( T)|
TheperformanceofAlgorithm1variesdependingonthechoice
of the function Choose, namely a search heuristic. Since the num-
berofexecutionpathsinaprogramisusuallyexponentialinthe
numberofbranches,exploringallpossibleexecutionpathsisinfea-
sible. To address this problem, concolic testing relies on the search
heuristic that steers concolic testing in a way to maximize code
coverageinagivenlimitedtimebudget[ 6].Thegoalofthispaperis
toautomaticallygenerateaneffectiveheuristicforagivenprogram.
2.2 Existing Search Heuristics
Beforepresentingourtechnique,wedescribetwonotablesearch
heuristics.Theseheuristicsareknowntoperformcomparatively
better than other heuristics [3, 29].
Control-FlowDirectedSearch(CFDS)[3]. CFDSisbasedon
the natural intuition that uncovered branches near the current
executionpathwouldbeeasiertobeexercisedinthenextexecution.
This heuristic first picks the last path condition Φm, then selects a
branchwhoseoppositebranchisthenearestfromanyoftheunseen
branches. Thedistance betweentwo branchesis calculatedby the
number of branches on the path from the source to the destination.
To calculate the distance, CFDS uses control flow graph of the
program, which is statically constructed before the testing.
Context-GuidedSearch (CGS)[29]. CGSbasicallyperforms
the breath-first search (BFS) on the execution tree, while reducing
the search space by excluding branches whose “contexts” are al-
readyexplored.Givenanexecutionpath,thecontextofabranch
in the path is defined as a sequence of preceding branches. The
searchgatherscandidatebranchesatdepth dfromtheexecution
tree, picks a branch from the candidates, and the context of the
branch is calculated. If the context has been already considered,CGS skips that branch and continues to pick the next one. Oth-erwise, the branch is negated and the context is recorded. When
all the candidate branches at depth
dare considered, the search
proceedstothedepth d+1oftheexecutiontreeandrepeatsthe
process explained above.
Limitations. Existingsearchheuristicshaveakeylimitation;
they rely on a fixed heuristic and fail to consistently perform well
on a wide range of target programs. Our experience with these
1245
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. Automatically Generating Search Heuristics for Concolic Testing ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
heuristicsisthattheyareunstableandtheireffectivenesssignifi-
cantly varies depending on the target programs. For example, CGS
outperforms other existing heuristics for several benchmarks: e.g.,
expat-2.1.0 andgrep-2.2 (Figure1).However,wefoundthatthe
CGSheuristicissometimesinferioreventotherandomheuristic
(e.g., tree-1.6.0 ). That is,the key feature, contexts, of CGSis not
appropriate for some programs.
Besidestheirsub-optimality,anotherkeylimitationofexisting
approaches is that developing a good search heuristic requires a
huge amount of engineering effort and expertise. Given that theeffectiveness of concolic testing depends heavily on the searchheuristic,ordinarydeveloperscannotfullybenefitfromconcolictesting. These observations motivated us to develop a technique
that automatically generates search heuristics.
3 OUR TECHNIQUE
In this section, we present our technique for automatically gen-erating search heuristics for concolic testing. We define a family
ofsearchheuristics,namelyparameterizedsearchheuristics(Sec-
tion3.1),andpresentanalgorithmtochoosethebestheuristicfrom
the family for a given subject program (Section 3.2).
3.1 Parameterized Search Heuristic
LetP∈Programbe a subject program under test. Recall that a
search heuristic, the Choosefunction in Algorithm 1, is a function
from execution trees to pairs of a path condition and a branch:
Choose∈SearchHeuristic =ExecutionTree →PathCond ×Branch
where ExecutionTree isthesetofallexecutiontreesoftheprogram,
PathCond the set of all path conditions in the trees, Branchthe set
of all branches in P.
Wedefineafamily H⊆ SearchHeuristic ofsearchheuristicsasa
parameterizedheuristic Chooseθ,where θistheparameterwhichis
ak-dimensional vector of real numbers: H={Chooseθ|θ∈Rk}.
Given an execution tree T=/angbracketleftΦ1Φ2···Φm/angbracketright, our parameterized
search heuristic is defined as follows:
Chooseθ(/angbracketleftΦ1···Φm/angbracketright)=(Φm,argmax
ϕj∈Φmscoreθ(ϕj))
Intuitively,theheuristicfirstchoosesthelastpathcondition Φm
fromtheexecutiontree T,thenselectsabranch ϕjfromΦmthat
getsthehighest scoreamongallbranchesin thatpath.Exceptfor
the CGS heuristic, all existing search heuristics choose a branch
from the last path condition. In this work, we follow this common
strategybutourmethodcanbegeneralizedtoconsidertheentire
execution tree as well. We explain how we score each branch ϕin
Φmwith respect to a given parameter θ:
(1)Werepresentthebranchbyafeaturevector.Wedesigned40
boolean features describing properties of branches in con-
colictesting.Afeature πiisabooleanpredicateonbranches:
πi:Branch→{ 0,1}.Forinstance,oneofthefeatureschecks
whether the branch is located in the main function or not.
Given a set of kfeatures π={π1,..., πk}, where kis the
length of the parameter θ, a branch ϕis represented by a
boolean vector as follows:
π(ϕ)=/angbracketleftπ1(ϕ),π2(ϕ),..., πk(ϕ)/angbracketright.Table 1: Branch features for concolic testing. Features 1–12
are static, and Features 13–40 are dynamic.
#Description
1branch in the main function
2true branch of a loop
3false branch of a loop
4nested branch
5branch containing external function calls
6branch containing integer expressions
7branch containing constant strings
8branch containing pointer expressions
9branch containing local variables
10branch inside a loop body
11true branch of a case statement
12false branch of a case statement
13first 10% branches of a path
14last 10% branches of a path
15branch appearing most frequently in a path
16branch appearing least frequently in a path
17branch newly covered in the previous execution
18branch located right after the just-negated branch
19branch whose context ( k=1) is already visited
20branch whose context ( k=2) is already visited
21branch whose context ( k=3) is already visited
22branch whose context ( k=4) is already visited
23branch whose context ( k=5) is already visited
24branch negated more than 10 times
25branch negated more than 20 times
26branch negated more than 30 times
27branch near the just-negated branch
28branch failed to be negated more than 10 times
29the opposite branch failed to be negated more than 10 times
30the opposite branch is uncovered (depth 0)
31the opposite branch is uncovered (depth 1)
32branch negated in the last 10 executions
33branch negated in the last 20 executions
34branch negated in the last 30 executions
35branchinthefunctionthathasthelargestnumberofuncov-
ered branches
36theopposite branchbelongs tounreachedfunctions (top10%
of the largest func.)
37theopposite branchbelongs tounreachedfunctions (top20%
of the largest func.)
38theopposite branchbelongs tounreachedfunctions (top30%
of the largest func.)
39the opposite branch belongs to unreached functions (# of
branches >10)
40branch inside the most recently reached function
(2)Next we compute the score of the branch. In our method,
the dimension kof the parameter θequals to the number
of branch features. We use the simple linear combination of
thefeaturevectorandtheparametertocalculatethebranch:
scoreθ(ϕ)=π(ϕ)·θ.
(3)Finally,wechoosethebranchwiththehighestscore.Thatis,
amongthebranches ϕ1,..., ϕninΦm,wechoosethebranch
ϕjsuch that scoreθ(ϕj)≥scoreθ(ϕk)for all k.
1246
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Sooyoung Cha, Seongjoon Hong, Junhee Lee, and Hakjoo Oh
Branch Features. We have designed 40 features to describe
useful properties of branches in concolic testing. Table 1 shows the
features,whichareclassifiedinto12staticand28dynamicfeatures.
Astaticfeaturedescribesabranchpropertythatcanbeextracted
without executing the program. A dynamic feature requires to
execute the program and is extracted during concolic testing.
The static features 1-12 describe the syntactic properties of each
branchintheexecutionpath,whichcanbegeneratedbyanalyzing
the program text. For instance, feature 8 indicates whether the
branchhasapointerexpressioninitsconditionalexpression.We
designed these features to see how much such simple features help
toimprovebranchcoverage,asthereisnoexistingheuristicthat
extensively considersthe syntactic properties ofbranches. At first
glancefeatures2and3seemredundant,butnotso.Thetrueand
false branches of loops have different roles; by giving a high score
to a true branch we can explicitly steer concolic testing away from
the loop (i.e. negating the true branch) while giving a high score to
a false branch leads to getting into the loop.
On the other hands, we designed dynamic features (13-40) to
capturethedynamicsofconcolictesting.Forinstance,feature24checkswhetherthebranchhasbeennegatedmorethan10times
duringconcolictesting.Thatis,duringtheexecutionoftheprogram,thebooleanvalueofeachdynamicfeatureforthesamebranchmay
change while the static feature values of the branch do not.
We also incorporated the key insights of the existing search
heuristicsintothefeatures.Forexample,dynamicfeatures19-23
were designed based on the notion of contexts used in the CGS
heuristic[ 29]whilefeatures30-31arebasedontheideaoftheCFDS
heuristic [3] that calculates the distance to uncovered branches.
3.2 Parameter Optimization Algorithm
Nowwedescribeouralgorithmforfindingagoodparametervalue
of the parameterized search heuristic. We formally define the opti-
mization problem, and then present our algorithm.
OptimizationProblem. Inourapproach,findingagoodsearch
heuristiccorrespondstosolvinganoptimizationproblem.Wemodel
the concolic testing procedure in Algorithm 1 by the function:
C:Program×SearchHeuristic →N
whichtakesaprogramandasearchheuristic,andreturnsthenum-
ber of covered branches. Given a program Pand a search heuristic
Choose,C(P,Choose)performsconcolic testing(Algorithm1) us-
ing the heuristic for a fixed number of executions (i.e. N). We
assume thatthe initialinput ( v0) andthe number ofexecutions ( N)
are fixed for the program.
Given a program Pto test, our goal is to find a parameter θthat
maximizes the performance of the concolic testing algorithm with
respectto P. Formally, our objective is to find θ∗such that
θ∗=argmax
θ∈RkC(P,Chooseθ). (1)
That is, we aim to find a parameter θ∗that causes the concolic
testingalgorithm Cwiththesearchheuristic Chooseθtomaximize
the number of covered branches in P.
Optimization Algorithm. Weproposeanalgorithmthateffi-
cientlysolvestheoptimizationproblemin(1).Asimplisticapproachto solve the problem would be random sampling, which randomly
samplesparametervaluesandreturnsthebestparameterfoundforagiventimebudget.However,wefoundthatthisnaivealgorithmis
extremely inefficient and leads to a failure when it is used for find-
ing a good search heuristic of concolic testing (Section 4.3). This is
mainlybecauseoftworeasons.First,thesearchspaceisintractably
large and therefore blindly searching for good parameters without
anyguidanceishopeless.Second,asingleevaluationofaparameter
value is generally unreliable and does not represent the average
performance in concolic testing. This performance variation arises
from the inherent nondeterminism in concolic testing (e.g. branch
prediction failure) [15].
Inresponse,wedesignedanoptimizationalgorithm(Algorithm2)
specializedtoefficientlyfindinggoodparametervaluesofsearch
heuristics.Thekeyideabehindthisalgorithmistoiterativelyre-
fine the sample space based on the feedback from previous runs
of concolic testing. The main loop of the algorithm consists of the
threephases: Find,Check,and Refine.Thesethreestepsarerepeated
until the average performance converges.
Atline2,thealgorithminitializesthesamplespaces.Itmaintains
ksample spaces, Ri(i∈[1,k]), where kis the dimension of the
parameters(i.e.,thenumberofbranchfeaturesinourparameterizedheuristic). In our algorithm, the
i-th components of the parameters
aresampledfrom Ri,independentlyfromothercomponents.For
alli,Riis initialized to the space [−1,1].
Inthefirstphase(Find),werandomlysample nparametervalues:
θ1,θ2,..., θnfromthecurrentsamplespace R1×R2×···× Rk(line
7), and their performance numbers (i.e., the number of branches
covered)areevaluated(lines9–11).Inexperiments,weset nto1,000
(300 for vim). Among the 1,000 parameters, we choose the top K
parametersaccordingtotheirbranchcoverage.Inourexperiments,
Kis set to 10 because we observed that parameters with good
qualities are usually found in the top 10 parameters. This first step
of executing a program 1,000 times can be run in parallel.
Inthenextphase(Check),wechoosethetop2parametersthat
showthebestaverageperformance.Atlines16–17,the Kparam-
eters chosen from the first phase are evaluated again to obtain
theaveragecodecoverage over10trials,where B∗
irepresentsthe
average performance of parameter θ/prime
i. At line 19, we choose two
parameters θt1(top 1) and θt2(top 2) with the best average perfor-
mance.Thisstep(Check)isneededtoruleoutunreliableparameters.
Because of the nondeterminism of concolic testing, the quality of a
search heuristic must be evaluated over multiple executions.
Inthethirdstep(Refine),werefinethesamplespaces R1,...,Rk
based on θt1andθt2. Each Riis refined based on thevalues of the
i-th components ( θi
t1andθi
t2)o fθt1andθt2. When both θi
t1andθi
t2
are positive, we modify Riby[min( θi
t1,θi
t2),1]. When both θi
t1and
θi
t2are negative, Riis refined by [−1,max( θi
t1,θi
t2)]. Otherwise, Ri
remains the same. Then, our algorithm goes back to the first phase
(Find)andrandomlysamples nparametervaluesfromtherefined
space.
Finally,ouralgorithmterminateswhenthebestaveragecoverage
(B∗
t1)obtainedinthecurrentiterationislessthanthecoverage( max)
fromthepreviousiteration(lines30–31).Thisway,weiterativelyrefineeachsamplespace
Riandguidethesearchtocontinuously
find and climb the hills toward top in the parameter space.
1247
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. Automatically Generating Search Heuristics for Concolic Testing ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Algorithm 2: Our Parameter Optimization algorithm
Input :Program P
Output:Optimal parameter θ∈RkforP
1:/*k: the dimension of θ*/
2:initialize the sample spaces Ri=[−1,1]fori∈[1,k]
3:/angbracketleftmax ,converge /angbracketright←/angbracketleft 0,false/angbracketright
4:repeat
5:/* Step 1: Find */
6:/* sample nparameters: θ1,... ,θn(e.g., n=1,000) */
7:{θi}n
i=1←sample from R1×R2×···× Rk
8:/* evaluate the sampled parameters */
9:fori=1tondo
10: /*Bi: branch coverage achieved with θi*/
11: Bi←C ( P,Choose θi)
12:end for
13:pick top Kparameters {θ/prime
i}K
i=1from{θi}n
i=1with highest Bi
14:
15:/* Step 2: Check */
16:for all Kparameters θ/prime
ido
17: B∗
i←1
10/summationtext.110
j=1C(P,Chooseθ/prime
i)
18:end for
19:pick top 2 parameters θt1,θt2with highest B∗
i
20:
21:/* Step 3: Refine */
22:fori=1tokdo
23: ifθi
t1>0andθi
t2>0then
24: Ri=[min(θi
t1,θi
t2), 1]
25: else ifθi
t1<0andθi
t2<0then
26: Ri= [-1, max(θi
t1,θi
t2)]
27: end if
28:end for
29:
30:/* Check Convergence */
31:ifB∗
t1<maxthen
32: converge ←true
33:else
34: /angbracketleftmax ,θmax/angbracketright←/angbracketleft B∗
t1,θt1/angbracketright
35:end if
36:untilconverge
37:returnθmax
4 EXPERIMENTS
In this section, we experimentally evaluate our approach that auto-
matically generates search heuristics of concolic testing. We imple-
mentedourapproachinatool,ParaDySE,ontopofCREST[ 9],a
concolic testing tool widely used for C programs [ 3,12,23,29]. We
conductedexperimentstoanswerthefollowingresearchquestions:
•Effectivenessofgeneratedheuristics :Doesourapproach
generateeffectivesearchheuristics?Howdotheyperform
compared to the existing state-of-the-art heuristics?
•Time for obtaining the heuristics : How long does our
approachtaketogeneratethesearchheuristics?Isourap-
proach useful even considering the training effort?
•Efficacy of optimization algorithm : How does our opti-
mizationalgorithmperformcomparedtothenaivealgorithm
by random sampling?
•Important features : What are the important features to
generate effective search heuristics for concolic testing?Table 2: 10 benchmark programs
Program # Total branches LOC Source
vim-5.7 35,464 165K [3]
gawk-3.0.3 8,038 30K ours
expat-2.1.0 8,500 49K [29]
grep-2.2 3,836 15K [3]
sed-1.17 2,656 9K [22]
tree-1.6.0 1,438 4K ours
cdaudio 358 3K [29]
floppy 268 2K [29]
kbfiltr 204 1K [29]
replace 196 0.5K [3]
Evaluation Setting. We have compared our approach with
fiveexistingheuristics:CGS(Context-GuidedSearch)[ 29],CFDS
(Control-FlowDirectedSearch)[ 3],Randombranchsearch[ 3],DFS
(Depth-FirstSearch)[ 15],andGenerationalsearch[ 16].Wechose
these heuristics for comparison because they have been commonly
usedinpriorwork[ 3,10,15,16,29].Inparticular,CGSandCFDSare
arguablythestate-of-the-artsearchheuristicsthatoftenperform
the best in practice [ 3,29]. The implementation of CFDS, Random,
andDFSheuristicsareavailableinCREST.Theimplementationsof
CGS and Generational search came from the prior work [29].2
We used 10 open-source benchmark programs (Table 2).3The
benchmarks are divided into the large and small programs. The
large benchmarks include vim,expat,grep,sed,gawk, and tree.
Thefirstfourarestandardbenchmarkprogramsinconcolictesting
forC,whichhavebeenusedmultipletimesinpriorwork[ 2,3,5,
22,29].Thelasttwoprograms( gawkandtree)werepreparedby
ourselves, which are available with our tool. Our benchmark set
alsoincludes4smallones: cdaudio,floppy,kbfiltr,and replace,
which were used in [3, 22, 29].
Weconductedallexperimentsunderthesameevaluationsetting;
theinitialinput(i.e. v0inAlgorithm1)wasfixedforeachbench-
markprogramandasinglerunofconcolictestingusedthesame
testing budget (4000 executions, i.e., N=4000in Algorithm 1).
Note that the performance of concolic testing generally dependson the initial input. We found that in our benchmark programs,except for
grepandexpat, different choices of initial input did
not much affect the final performance, so we generated random
inputsforthoseprograms.For grepandexpat,theperformanceof
concolic testing varied significantly depending on the initial input.
For instance, with some initial inputs, CFDS and Random covered
150lessbranchesin grepthanwithotherinputs.We avoidedthis
exceptional case when selecting the input for grepandexpat. For
expat, we chose the same input used in prior work [ 29]. For grep,
weselected aninputon whichtherandom heuristicwaseffective.
The initial inputs we used are available with our tool.
Theperformanceofeachsearchheuristicwasaveragedovermul-
tipletrials.Evenwiththesameinitialinput,thesearchheuristicshavecoveragevariationsforseveralreasons:searchinitialization
inconcolictesting[ 15],therandomnessofsearchheuristics,and
soon.Werepeatedtheexperiments100timesforallbenchmarks
2We obtained the implementation from authors via personal communication.
3Henceforth, the version numbers will be omitted when there is no confusion.
1248
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Sooyoung Cha, Seongjoon Hong, Junhee Lee, and Hakjoo Oh
Table 3: Average branch coverage on 4 small benchmarks
OURS CFDSCGSRandom Gen DFS
cdaudio 250 250 250 242250236
floppy 205 205 205 170205168
replace 181177181 174 176 171
kbfiltr 149 149 149 149 149 134
Table4:Effectivenessintermsofmaximumbranchcoverage
OURS CFDS CGS Random Gen DFS
vim8,7888,585 6,488 8,143 5,161 2,646
expat1,4221,060 1,337 965 1,348 1,027
gawk2,6842,532 2,449 2,035 2,443 1,025
grep1,8071,726 1,751 1,598 1,640 1,456
sed 830780 781 690 698 568
tree 797702 599 704 600 360
Table 5: Effectiveness in terms of finding bugs
OURS CFDS CGS Random Gen DFS
gawk-3.0.3 100/100 0/100 0/100 0/100 0/100 0/100
grep-2.2 47/100 0/100 5/100 0/100 0/100 0/100
exceptfor vimforwhichweaveragedover50trialsasitsexecution
takes much longer time. The experiments were done on a linux
machine with two Intel Xeon Processor E5-2630 and 192GB RAM.
4.1 Effectiveness of Generated Heuristics
Foreachbenchmarkprogram,weranouralgorithm(Algorithm2)to
generateoursearchheuristic(ours),andcompareditsperformance
withthatoftheexistingheuristics.Weevaluatetheeffectiveness
with two measures: branch coverage and capability to find bugs.
Branch Coverage. Forbranchcoverage,wemeasuredtheav-
erage and maximum coverages. The average branch coverage isobtained by averaging the results over the 100 trials (50 for
vim).
The maximum coverage refers to the highest coverage achieved
during the 100 trials (50 for vim). The former indicates the average
performancewhilethelatterthebestperformanceachievableby
each heuristic.
Figure 1 compares the average branch coverage achieved by
differentsearchheuristicson6largebenchmarks.Theresultsshowthatthesearchheuristicsgeneratedbyourapproach(ours)achieve
thebestcoverageonallprograms.Inparticular,ourssignificantly
increased the branch coverage on two largest benchmarks: vim
andgawk.For vim,ourscovered8,297branchesin4,000executions
while the CFDS heuristic, which took the second place for vim,
covered 7,990 branches. Note that CFDS is already highly tuned
andthereforeoutperformstheotherheuristicsfor vim(forinstance,
CGS covered 6,166 branches only). For gawk, ours covered 2,684
brancheswhile theCGSheuristic, thesecondbestone,managedto cover 2,321 branches. For
expat,sed, and tree, our approach
improved the existing heuristics considerably. For example, ours
covered 1,327 branches for expat, increasing the branch coverageofCGSby50.For grep,oursalsoperformedthebestfollowedby
CGS and CFDS. On small benchmarks, we obtained similar results;
ours(togetherwithCGS)consistentlyachievedthehighestaverage
coverage (Table 3). In the rest of the paper, we focus only on the 6
large benchmarks, where existing manually-crafted heuristics fail
to perform well.
OnallbenchmarksinFigure1,OURSexclusivelycoveredbranches
that were not covered by other heuristics. For example, in vim,a
total of 504 branches were exclusively covered by our heuristic.
For other programs, the numbers are: expat(14), gawk(7),grep(23),
sed(21), tree(96).
These results are statistically significant: on all benchmark pro-
grams in Figure 1, the pvalue was less than 0.01according to
Wilcoxon signed-rank test. In Figure 1, the standard deviations
for each heuristic are as follows: (1) OURS: vim(258), expat(42),
gawk(0),grep(51), sed(22), tree(7); (2)CFDS: vim(252), expat(44),
gawk(120), grep(33), sed(24), tree(13);(3)CGS: vim(200), expat(24),
gawk(57), grep(29), sed(27), tree(15).Othersearchheuristicsalso
have similar standard deviations.
InFigure1,wecomparedtheeffectivenessofsearchheuristics
over iterations (# of executions)4, but our approach was also su-
periortoothersoverexecutiontime.Forexample,giventhesame
time budget (1,000 sec), ours and Random (the second best) cov-
ered 8,947 and 8,272 branches, respectively, for vim(Figure 2). The
results were averaged over 50 trials.
Table4comparestheheuristicsintermsofthemaximumbranch
coverage on 6 large benchmarks. The results show that our ap-
proachinthiscasealsoachievesthebestperformanceonallpro-
grams.Forinstance,in vim,weconsiderablyincreasedthecoverage
of CFDS, the second best strategy; ours covered 8,788 branches
while CFDS managed to cover 8,585. For expat, ours and CGS (the
second best) have covered 1,422 and 1,337 branches, respectively.
Note that there is no clear winner among the existing search
heuristics.Exceptforours,CFDStookthefirstplacefor vimandsed
in terms of average branch coverage. For gawk,expat, and grep,
the CGS heuristic was the best. For tree, the Random heuristic
wasbetterthanCFDSandCGS.Intermsofthemaximumbranch
coverage, CFDS was better than the others for vimandgawkwhile
CGSwasfor grepandsed.TheGenerationalandRandomheuristics
surpassedCFDSand CGSin expatandtree,respectively.On the
otherhand,ourapproachisabletoconsistentlyproducethebest
search heuristics in terms of both coverage metrics.
Bug Finding. We found that the increased branch coverage
by our approach leads to more effective finding of real bugs (not
seeded ones). Table 5 reports the number of trials that successfully
generate test-cases, which trigger the known performance bugs in
gawkandgrep[13,18].Duringthe100trials(whereasingletrial
consistsof4,000executions),ourheuristicalwaysfoundthebugin
gawkwhile all the other heuristics completely failed to find it.
Ingrep,ourssucceededtofindthebug47timesoutof100trials,
which is much better than CGS does (5 times). Other heuristics
were not able to trigger the bug at all.
4Evaluatingtheperformanceofsearchheuristicsoveriterationsisacommonprac-
tice[3,29],astheexecutiontimeofaprogrammayvaryconsiderablydependingon
the input.
1249
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. Automatically Generating Search Heuristics for Concolic Testing ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Figure 1: Average branch coverage achieved by each search heuristic on 6 large benchmarks
Ourheuristics aregoodatfinding bugsbecausetheyaremuch
better than other heuristics in exercising diverse program paths.
WeobservedthatotherheuristicssuchasCGS,CFDS,andGenalso
covered the branches where the bugs originate. However, the bugs
arecausedonlybysomespecificpathconditionsandtheexisting
heuristics could not generate inputs that satisfy the conditions.
We remark that we did not specially tune our approach towards
findingthosebugs.Infact,wewerenotawareofthepresenceof
those bugs at the early stage of this work. The bugs in gawkand
grep[13,18] cause performance problems; for example, grep-2.2requires exponentialtime andmemory on particularinput strings
thatinvolveback-references[ 18].Duringconcolictesting,wemon-
itored the program executions and restarted the testing procedure
when the subject program ran out of memory or time. Those bugs
weredetectedunexpectedlybyacombinationofthismechanism
and our search heuristic.
4.2 Time for Obtaining the Heuristics
Table6reportstherunningtimeofouralgorithmtogeneratethe
search heuristics evaluated in Section 4.1. To obtain our heuristics,
1250
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Sooyoung Cha, Seongjoon Hong, Junhee Lee, and Hakjoo Oh
Figure 2: Average branch coverage over execution time
Table 6: Time for generating the heuristics
Benchmarks # Sample # Iteration Total times
vim-5.7 300 5 24h 17min
expat-2.1.0 1,000 6 10h 25min
gawk-3.0.3 1,000 4 6h 28mingrep-2.2 1,000 5 5h 26minsed-1.17 1,000 4 8h 55mintree-1.6.0 1,000 4 3h 17min
Table 7: Effectiveness in the training phase
OURS CFDS CGS Random Gen DFS
vim14,003 13,706 7,934 13,835 7,290 2,646
expat 2,455 2,339 2,157 1,325 2,116 2,036
gawk 3,473 3,382 3,261 3,367 3,302 1,905
grep 2,167 2,024 2,016 2,066 1,965 1,478
sed 1,019 1,041 1,042 1,007 979 937
tree 808 800 737 796 730 665
werantheoptimizationalgorithm(Algorithm2)inparallelusing
20 cores. Specifically, in the first phase (‘Find’) of the algorithm,
we sampled 1,000 parameters, where each core is responsible for
evaluating 50 parameters. For vim, we set the sample size to 300 as
executing vimis expensive. The results show that our algorithm
convergeswithin4–6iterationsoftheouterloopofAlgorithm2,
taking 3–24 hours depending on the size of the subject program.
Ourapproachrequirestrainingeffortbutitisrewardingbecause
1) our approach enables effective concolic testing even in the train-
ingphase;and2)thelearnedheuristiccanbereusedmultipletimes
as the subject program evolves.
Effectiveness in the training phase. Note that running Al-
gorithm 2 is essentially running concolic testing on the subjectprogram. We compared the number of branches covered during
this training phase with the branches covered by other search
heuristicsgiven thesametimebudget reportedinTable 6.Table7
comparestheresults:exceptfor sed,runningAlgorithm2achieves
greaterbranchcoveragethanothers.Toobtaintheresultsforotherheuristics, we ran concolic testing (with N=4,000) repeatedly
using the same number of cores and amount of time. For instance,
in 24 hours, Algorithm 2 covered 14,003 branches of vimwhile
concolic testing with the CFDS and CGS heuristics covered 13,706
and 7,934 branches, respectively.
Reusabilityoverprogramevolution. Moreinterestingly,the
learned heuristic can be reused over multiple subsequent program
variations.Tovalidatethishypothesis,wetrainedasearchheuristic
ongawk-3.0.3 andappliedthelearnedheuristictothesubsequent
versionsuntil gawk-3.1.0 .Wealsotrainedaheuristicon sed-1.17
and applied it to later versions. Figure 4 shows that the learned
heuristics manage to achieve the highest branch coverage over
theevolutionoftheprograms.Forexample,ourscoveredatleast
90 more branches than the second best heuristic (CFDS) in all
variationsbetween gawk-3.0.3 andgawk-3.1.0 .Theeffectiveness
lasted for at least 4 years for gawkand 1 year for sed.
4.3 Efficacy of Optimization Algorithm
Wecomparedtheperformanceofouroptimizationalgorithm(Algo-
rithm2)withanaiveapproachbasedonrandomsampling.Because
both approaches involve randomness, we statistically compare the
qualities of parameters found by our algorithm and the random
sampling method.
Figure4showsthedistributionsoffinalcoveragesachievedby
those two algorithms on grep-2.2 andsed-1.17 . In the exper-
iments, our algorithm required a total of 1,100 trials of concolic
testingtocompleteasinglerefinementtask:100trialsfortheCheck
phase to select top 2 parameters and the rest for the Find phasetoevaluatetheparametersgeneratedfromtherefinedspace.We
compared the distributions throughout each iteration ( I1,I2,...,IN)
where 1,100 trials were given as budget for finding parameters.
The firstrefinementtask of ouralgorithm begins withthe initial
samples in the first iteration I1, which are prepared by random
sampling method.
Theresultshowsthatouralgorithmismuchsuperiortorandom
samplingmethod:themedianofthesamplesincreaseswhilethe
variance decreases, as the refinement task in our algorithm goes
on. The median value (the band inside a box) of the samples found
by our algorithm increases as the refinement task continues, while
randomsamplinghasnonoticeablechanges.Theincreaseofmedian
indicates that the probability to find a good parameter grows as
the tasks repeat. In addition, the variance (the height of the box, in
simple) in our algorithm decreases gradually, which implies that
the mix of CheckandRefinetasks was effective.
Weremarkthatuseofouroptimizationalgorithmwascritical;
the heuristics generated by random sampling failed to surpass the
existing heuristics. For instance, for grep, our algorithm (Algo-
rithm2)succeededingeneratingaheuristicwhichcovered1,701
branchesonaverage.However,thebestonebyrandomsampling
covered 1,600 branches only, lagging behind CGS (the second best)
by 83 branches.
4.4 Important Features
Winning Features. We discussthe relativeimportance of fea-
turesbyanalyzingthelearnedparameters θforeachbenchmark
program. Intuitively, when the i-th component θihas a negative
1251
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. Automatically Generating Search Heuristics for Concolic Testing ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
Figure 3: Average coverage of each search heuristic on multiple subsequent program variants
Figure 4: Comparison between our algorithm and random sampling method
Table 8: Top 10 positive features
RankBenchmarks
vim gawk expat grep sed tree
1# 1 5 # 10(⋆)#2 7 #1 4 # 13(+) #3 6
2# 1 8 # 13(+) # 30(+) #4 0 #2 #1 5
3# 35(⋆)#1 2 #2 3 #2 4 #2 9 #5
4# 4 0 # 38(⋆) # 31(+) #1 #3 # 25(⋆)
5# 31(+) #1 4 #4 # 30(+) #8 #4 0
6 # 7# 9# 9# 38( ⋆) # 30(+) #9
7# 13(+) # 35(⋆)#8 #3 2 # 35(⋆) # 13(+)
8# 3 # 31(+) #1 5 #1 7 #6 #3 9
9# 1 2 # 4 # 25(⋆) # 31(+) #2 1 # 30(+)
10# 10(⋆)#3 3 #7 #2 9 #1 6 #2 2
number in θ, it indicates that the branch having i-th component
should not be selected to be negated. Thus, both strong negative
and positive features are equally important for our approach to
improve the branch coverage. Table 8 and Table 9 show the top 10
positive and negative features for each benchmark, respectively.Table 9: Top 10 negative features
RankBenchmarks
vim gawk expat grep sed tree
1# 1 7 # 26(-) #3 9 #2 0 # 11(-) # 10(⋆)
2# 11(-) #8 # 35(⋆)#3 9 #3 2 # 35(⋆)
3 # 34 # 16 # 33 # 22(-) #1 9 #6
4 # 33 # 29 # 37 # 25(⋆)#4 0 #2 4
5# 22(-) #3 # 38(⋆) # 26(-) # 38( ⋆)#7
6 #2 1 #6 #2 #1 9 #1 8 #1 27# 26(-) # 22(-) #2 4 #2 7 #5 #2 3
8# 25(⋆) # 11(-) # 22(-) #2 1 #2 0 #2
9 # 37 # 19 # 10(⋆)#3 3 #3 4 #2 7
10 # 20 # 28 # 32 # 37 # 26(-) # 11(-)
The results show that there is no winning feature which always
belongs to the top 10 positive or negative features. Nevertheless,
the features 13 (front parts of a path) and 30-31 (distances of un-
coveredbranches)arecomparativelyconsistentpositiveones.For
4 benchmarks, the feature 11 (case statement), 22 (context) and 26
(frequently negated branch) are included in the top 10 negative
1252
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden Sooyoung Cha, Seongjoon Hong, Junhee Lee, and Hakjoo Oh
features. For designing effective search heuristics, the key ideas
of CFDS heuristic (#30-31) and CGS (#19-20, #22) heuristics are
generally used as good positive and negative features, respectively.
Notethatthefeatures10,25,35and38appearinbothTable8
and Table 9. That is, depending on the program under test, the role
of each feature changes from positive to negative (or vice versa).For instance, the feature 10 is used as the most positive featurein
gawkwhile it is the most negative one for tree. This finding
supportsourclaimthatnosinglesearchheuristiccanperformwell
for all benchmarks, and therefore it should be adaptively tuned for
each target program.
Impact of Combining Static and Dynamic Features. The
combined use of static and dynamic features was important. Weassessed the performance of our approach with different feature
sets in two ways: 1) with static features only; and 2) with dynamic
featuresonly.Withoutdynamicfeatures,generatinggoodheuristic
wasfeasibleonlyfor grep.Withoutstaticfeatures,ourapproach
succeeded in generating good heuristics for grepandtreebut
failed to do so for the remaining programs.
4.5 Threats to Validity
(1) We collected eight benchmarks from prior work [ 2,3,5,22,
29]and createdtwonew benchmarks( gawkandtree). However,
these 10 benchmarks may not be representative and not enough
to evaluate the performance of the search heuristics in general. (2)
We chose 4,000 executions as the testing budget because it is the
same criterion that was used for evaluating the existing heuristics
(CGS, CFDS) in prior work [ 3,29]. However, this might not be a
best setting in practice. (3) The performance of search heuristics
may vary when using different SMT solvers. We used Yices, the
default SMT solver in CREST.
5 RELATED WORK
Wediscussexistingworksonimprovingtheperformanceofcon-
colictesting. We classifyexisting techniquesintothe fourclasses:
(1)improvingsearchheuristics;(2)hybridapproaches;(3)reducing
search space; and (4) solving complex path conditions.
Search Heuristics. All existing works on improving search
heuristics focus on manually-designing a new strategy [ 3,4,21,
27,29,32].InSection2.2,wealreadydiscussedtheCFDS[ 3]and
CGS [29] heuristics. Another successful heuristic is generational
search[16],whichdrivesconcolictestingtowardsthehighestin-
crementalcoveragegaintomaximizecodecoverage.Foreachex-
ecution path, all branches are negated and executed. Then, next
generationbranchisselectedaccordingtothecoveragegainofeach
single execution. Xie et al. [ 32] designed a heuristic that guides
the search based on the fitness values that measure the distance of
branchesintheexecutionpathtothetargetbranch.TheCarFast
heuristic[ 27]guidesconcolictestingbasedonthenumberofun-
covered statements. In [ 4], several concolic search heuristics are
used in a round robin fashion. Our work is different from these
works as we automate the heuristic-designing process itself.
Hybrid Approaches. Our approach is orthogonal to the exist-
ing techniques that combine concolic testing with other testing
techniques. In [ 12,26], techniques such as random testing are firstused and they switch to concolic testing when the performance
gains saturate. In [ 19], concolic testing is combined with evolution-
ary testing to be effective for object-oriented programs.
Reducing SearchSpace. Ourwork isalsoorthogonal totech-
niquesthatreducethesearchspaceofconcolictesting[ 2,10,14,17,
20].Theread-writesetanalysis[ 2]identifiesandprunesprogram
pathsthathavethesamesideeffects.Jaffaretal.[ 20]introducedan
interpolation methodthat subsumespaths guaranteednot tohit a
bug.Goderfroidetal.[ 14,17]proposedtousefunctionsummarizes
to identify equivalence classes of function inputs. It ensures that
the concrete executions in the same class have the same side effect.
Abstraction-driven concolic testing [ 10] also reduces search space
forconcolictestingbyusingfeedbackfromamodelchecker.Our
work can be combined with these techniques to boost concolic
testing further.
Solving Complex Path Conditions. Our technique can also
be improved by incorporating existing techniques for solving com-
plexpathconditions.ConventionalSMTsolversarenoteffectivein
handling constraints that involve non-linear arithmetic or external
function calls, which often causes concolic testing to have poor
coverage. In [ 11], an algorithm was introduced that can solve hard
arithmeticconstraintsinpathconditions.Theideaistogenerate
geometricstructuresthathelpsolvenon-linearconstraintswithex-
isting heuristics [ 8]. In [31], a technique to solve string constraints
wasproposedbasedonantcolonyoptimization.Thereareattempts
tosolvethisproblembymachinelearning[ 25].Itencodesnotonly
thesimplelinearpathconditions,butalsocomplexpathconditions
(e.g., function calls of library methods) into the symbolic path con-
ditions. The objective function is defined by dissatisfaction degree.
By iteratively generating sample solutions and getting feedback
from the objective function, it learns how to generate solution for
complex path condition containing even black-box function which
cannot be solved by current solver.
6 CONCLUSION
Thedifficultyofmanuallycraftinggoodsearchheuristicshasbeena
majoropenchallengeinconcolictesting.Inthispaper,weaddressed
thisproblemwithanovelapproachforautomaticallygenerating
searchheuristics.Givenaprogramundertest,ourtechniquegen-
eratesan optimalsearch heuristicfor thesubjectprogram. Sucha
“machine-tuned” heuristic has been shown to outperform existing
hand-tuned heuristics. To achieve this, we developed a parame-
terizedsearchheuristicforconcolictestingwithanoptimization
algorithm to efficiently search for good parameter values. We hope
thatourtechniquecansupplantthelaboriousandlessrewarding
task of manually tuning search heuristics of concolic testing.
ACKNOWLEDGMENTS
ThisworkwassupportedbyInstituteforInformation&commu-
nications Technology Promotion(IITP)grant funded by theKorea
government(MSIT) (No.2015-0-00565, Development of Vulnerabil-
ityDiscoveryTechnologiesforIoTSoftwareSecurity).Thiswork
was supported by Samsung Research Funding & Incubation Center
of Samsung Electronics under Project Number SRFC-IT1701-09.
1253
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. Automatically Generating Search Heuristics for Concolic Testing ICSE ’18, May 27-June 3, 2018, Gothenburg, Sweden
REFERENCES
[1]Thanassis Avgerinos, Alexandre Rebert, Sang Kil Cha, and David Brumley. 2014.
Enhancing Symbolic Execution with Veritesting. In Proceedings of the 36th Inter-
national Conference on Software Engineering (ICSE ’14). 1083–1094.
[2]Peter Boonstoppel, Cristian Cadar, and Dawson Engler. 2008. RWset: Attacking
pathexplosioninconstraint-basedtestgeneration.In InternationalConference
on Tools and Algorithms for the Construction and Analysis of Systems (TACAS ’08) .
351–366.
[3]Jacob Burnim and Koushik Sen. 2008. Heuristics for Scalable Dynamic Test Gen-
eration. In Proceedings of 23rd IEEE/ACM International Conference on Automated
Software Engineering (ASE ’08). 443–446.
[4]Cristian Cadar, Daniel Dunbar, and Dawson Engler. 2008. KLEE: Unassisted and
AutomaticGenerationofHigh-coverageTestsforComplexSystemsPrograms.
InProceedings of the 8th USENIX Conference on Operating Systems Design and
Implementation (OSDI ’08). 209–224.
[5]Cristian Cadar, Vijay Ganesh, Peter M. Pawlowski, David L. Dill, and Dawson R.
Engler.2008. EXE:AutomaticallyGeneratingInputsofDeath. ACMTrans.Inf.
Syst. Secur. 12, 2 (2008), 10:1–10:38.
[6]Cristian Cadar and Koushik Sen. 2013. Symbolic Execution for Software Testing:
Three Decades Later. Commun. ACM 56, 2 (2013), 82–90.
[7]Maria Christakis, Peter Müller, and Valentin Wüstholz. 2016. Guiding Dynamic
SymbolicExecutionTowardUnverifiedProgramExecutions.In Proceedingsof
the 38th International Conference on Software Engineering (ICSE ’16). 144–155.
[8]PhilippeCodognetandDanielDiaz.2001. Yetanotherlocalsearchmethodfor
constraint solving. In International Symposium on Stochastic Algorithms. 73–90.
[9]CREST. A concolic test generation tool for C. 2008. https://github.com/jburnim/
crest. (2008).
[10]PrzemysławDaca,AshutoshGupta,andThomasA.Henzinger.2016. Abstraction-
driven Concolic Testing. In Proceedings of the 17th International Conference on
Verification,ModelChecking,andAbstractInterpretation-Volume9583 (VMCAI
’16). 328–347.
[11]PeterDingesandGulAgha.2014. SolvingComplexPathConditionsThrough
Heuristic Search on Induced Polytopes. In Proceedings of the 22nd ACM SIGSOFT
International SymposiumonFoundations ofSoftware Engineering (FSE’14).425–
436.
[12]PranavGarg,FranjoIvancic,GogulBalakrishnan,NaotoMaeda,andAartiGupta.
2013. Feedback-directed Unit Test Generation for C/C++ Using Concolic Execu-
tion. InProceedings of the 2013 International Conference on Software Engineering
(ICSE ’13). 132–141.
[13]Gnu Bug Report (gawk). 2005. http://gnu.utils.bug.narkive.com/Udtl5IZR/
gawk-bug. (2005).
[14]PatriceGodefroid.2007. CompositionalDynamicTestGeneration.In Proceedings
of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Program-
ming Languages (POPL ’07). 47–54.
[15]Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-
mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI ’05). 213–223.
[16]PatriceGodefroid,MichaelYLevin,andDavidAMolnar.2008. AutomatedWhite-
boxFuzzTesting..In ProceedingsoftheSymposiumonNetworkandDistributed
System Security (NDSS ’08). 151–166.
[17]Patrice Godefroid, Aditya V. Nori, Sriram K. Rajamani, and Sai Deep Tetali.
2010. Compositional May-must Program Analysis: Unleashing the Power of
Alternation.In Proceedingsofthe37thAnnualACMSIGPLAN-SIGACTSymposium
on Principles of Programming Languages (POPL ’10). 43–56.
[18]GNUBugReport(grep).2014. https://www.gnu.org/software/grep/manual/html_
node/Reporting-Bugs.html. (2014).
[19]Kobi Inkumsah and Tao Xie. 2008. Improving Structural Testing of Object-
OrientedProgramsviaIntegratingEvolutionaryTestingandSymbolicExecution.
InProceedings of the 23rd IEEE/ACM International Conference on Automated Soft-
ware Engineering (ASE ’08). 297–306.
[20]JoxanJaffar,VijayaraghavanMurali,andJorgeA.Navas.2013. BoostingConcolic
Testing viaInterpolation. In Proceedings ofthe 9thJoint Meetingon Foundations
of Software Engineering (ESEC/FSE ’13). 48–58.
[21]Su Yong Kim, Sangho Lee, Insu Yun, Wen Xu, Byoungyoung Lee, Youngtae Yun,
and Taesoo Kim. 2017. CAB-Fuzz: Practical Concolic Testing Techniques for
COTS Operating Systems. In 2017 USENIX Annual Technical Conference (USENIX
ATC ’17). 689–701.
[22]YunhoKimandMoonzooKim.2011. SCORE:AScalableConcolic TestingTool
forReliableEmbeddedSoftware.In Proceedingsofthe19thACMSIGSOFTSympo-
siumandthe13thEuropeanConferenceonFoundationsofSoftwareEngineering
(ESEC/FSE ’11). 420–423.
[23]YunhoKim,MoonzooKim,YoungJooKim,andYoonkyuJang.2012. Industrial
Application of Concolic Testing Approach: A Case Study on Libexif by Using
CREST-BV and KLEE. In Proceedings of the 34th International Conference on
Software Engineering (ICSE ’12). 1143–1152.
[24]James C. King. 1976. Symbolic Execution and Program Testing. Commun. ACM
19, 7 (1976), 385–394.[25]XinLi,YongjuanLiang,HongQian,Yi-QiHu,LeiBu,YangYu,XinChen,and
Xuandong Li. 2016. Symbolic Execution of Complex Program Driven by Ma-
chineLearningBasedConstraintSolving.In Proceedingsofthe31stIEEE/ACM
International Conference on Automated Software Engineering (ASE ’16). 554–559.
[26]RupakMajumdarandKoushikSen.2007. HybridConcolicTesting.In Proceedings
of the 29th International Conference on Software Engineering (ICSE ’07). 416–426.
[27]Sangmin Park, B. M. Mainul Hossain, Ishtiaque Hussain, Christoph Csallner,
MarkGrechanik,KunalTaneja,ChenFu,andQingXie.2012. CarFast:Achieving
Higher Statement Coverage Faster. In Proceedings of the ACM SIGSOFT 20th
International Symposium on the Foundations of Software Engineering (FSE ’12).
35:1–35:11.
[28]KoushikSen,DarkoMarinov,andGulAgha.2005. CUTE:AConcolicUnitTesting
EngineforC.In Proceedingsofthe10thEuropeanSoftwareEngineeringConference
Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of
Software Engineering (ESEC/FSE ’05) . 263–272.
[29]HyunminSeoandSunghunKim.2014. HowWeGetThere:AContext-guided
Search Strategy in Concolic Testing. In Proceedings of the 22nd ACM SIGSOFT
International SymposiumonFoundations ofSoftware Engineering (FSE’14) .413–
424.
[30]NickStephens,JohnGrosen,ChristopherSalls,AndrewDutcher,RuoyuWang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In
Proceedings of the Symposium on Network and Distributed System Security (NDSS
’16). 1–16.
[31]Julian Thomé, Lwin Khin Shar, Domenico Bianculli, and Lionel Briand. 2017.Search-driven String Constraint Solving for Vulnerability Detection. In Pro-
ceedingsofthe39thInternationalConferenceonSoftwareEngineering (ICSE’17).
198–208.
[32]Tao Xie, Nikolai Tillmann, Jonathan de Halleux, and Wolfram Schulte. 2009.
Fitness-guidedpathexplorationindynamicsymbolicexecution.In 2009IEEE/IFIP
International Conference on Dependable Systems Networks. 359–368.
[33]Yufeng Zhang, Zhenbang Clien, Ji Wang, Wei Dong, and Zhiming Liu. 2015.
RegularPropertyGuidedDynamicSymbolicExecution.In Proceedingsofthe37th
International Conference on Software Engineering - Volume 1 (ICSE ’15) . 643–653.
1254
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 11:58:37 UTC from IEEE Xplore.  Restrictions apply. 