Belief & Evidence in Empirical Software Engineering
Prem Devanbu
Dept of Computer Science
UC Davis
Davis, California, 95616, USA
ptdevanbu@ucdavis.eduThomas Zimmermann, Christian Bird
Microsoft Research
Redmond, Washington
USA
{tzimmer, cbird}@microsoft.com
ABSTRACT
Empirical software engineering has produced a steady stream of
evidence-based results concerning the factors that affect important
outcomes such as cost, quality, and interval. However, program-
mers often also have strongly-held a priori opinions about these
issues. These opinions are important, since developers are highly-
trained professionals whose beliefs would doubtless affect their
practice. As in evidence-based medicine, disseminating empirical
Ô¨Åndings to developers is a key step in ensuring that the Ô¨Åndings im-
pact practice. In this paper, we describe a case study, on the prior
beliefs of developers at Microsoft, and the relationship of these be-
liefs to actual empirical data on the projects in which these develop-
ers work. Our Ô¨Åndings are that a) programmers do indeed have very
strong beliefs on certain topics b) their beliefs are primarily formed
based on personal experience, rather than on Ô¨Åndings in empirical
research and c) beliefs can vary with each project, but do not neces-
sarily correspond with actual evidence in that project. Our Ô¨Åndings
suggest that more effort should be taken to disseminate empirical
Ô¨Åndings to developers and that more in-depth study the interplay of
belief and evidence in software practice is needed.
1. INTRODUCTION
We all learn from experience; however, what we learn is pro-
foundly inÔ¨Çuenced by our prior beliefs. If a new experience roundly
contradicts strongly-held prior beliefs, we often tend to cling these
beliefs, unwilling to let go until our pet theories are repeatedly
and resoundingly refuted. On the other hand, if a new experience
is mostly consistent with, (but somewhat differentiated from) our
prior beliefs, we are more willing to accept it, as long as we don‚Äôt
have to revise our beliefs too much. Sticking to prior beliefs is not
just always just mindless stubbornness: in fact, it is often sensible.
Prior beliefs are either themselves learned from experience, or are
Devanbu‚Äôs primary appointment is listed; however, his contribu-
tions to this paper were made mostly during his visit to the other
authors at Microsoft Research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation
on the Ô¨Årst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
ICSE ‚Äô16, May 14 - 22, 2016, Austin, TX, USA
c2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-3900-1/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2884781.2884812innate (in our genes); in either case, it would be imprudent and per-
haps dangerous to abandon them too quickly. We all are, therefore,
naturally suspicious of new phenomena that contradict our beliefs.
Science (and Engineering practice) are all about learning from
experience. Not surprisingly, the effects of prior beliefs in Sci-
ence are complex, even paradoxical; however, these effects are vi-
tally important to the continued vibrancy and societal impact of
experimental disciplines. On the one hand, when a new experiment
reports surprising or unexpected results, we demand that the ex-
periment be very convincing. How the experimental subjects were
chosen, we ask. How was the data collected? How was it analyzed?
What was the effect size? Questions and debates thicken and inten-
sify for more surprising results; getting the community to accept
these results can be a challenge! This resistance to new ideas is a se-
rious issue in Medicine, where is vital that Physicians embrace, and
adopt new practices that are supported by evidence and Ô¨Åndings;
but they won‚Äôt do this if they are not convinced! Chaloner et al [11]
have argued, from a Bayesian perspective, that rigorous, demand-
ing experimental design constraints are needed (or even, morally
obligated) when the Ô¨Åndings might contradict strongly-held prior
beliefs and practices, and might actually save lives.
On the other hand, paradoxically, students of the sociology of
science have noted that surprising results are disproportionately re-
warded by the ScientiÔ¨Åc Community. Prestigious journals such as
Science andNature favor surprising results, which are more likely
to attract mainstream media attention. The same is arguably true
in computer science; surprising results tend to be received more fa-
vorably. However, a Bayesian analysis on this [21, 55] yields the
rather pessimistic view that surprising results are more often wrong
(Section 2.2). This leads to a distressing situation: surprising (and
therefore perhaps wrong) results get lots of media attention, and
thus are actually more likely to be noticed by politicians, inÔ¨Çuence
policy, etc.
This paper, to our knowledge, is the Ô¨Årst to empirically assess de-
veloper belief and project evidence in empirical software engineer-
ing. The cost, pervasiveness, and socio-economic impact of soft-
ware are well-known, and provide a durable and formidable imper-
ative for evidence-based improvements in the practice of software
engineering [27], just as in medicine. And so, just as in medicine,
we argue that taking the prior beliefs of practitioners into account
can strengthen the Ô¨Åeld in several ways: Ô¨Årst, we could have a
stronger impact on practice, more carefully and systematically dis-
seminating our work; second, by deploying more robust and rig-
orous experimental techniques when Ô¨Åndings may contradict pro-
grammers‚Äô beliefs (and thus encounter resistance); and, Ô¨Ånally, by
being inÔ¨Çuenced by prior beliefs, we can more systematically ame-
liorate the risk of making false discoveries ourselves (specially in
settings where large sample sizes are difÔ¨Åcult to obtain).
2016 IEEE/ACM 38th IEEE International Conference on Software Engineering
   108
We make the following contributions:
1. We surveyed developers in several large Microsoft projects
as to the strength (and disposition) of their beliefs regarding
several consequential claims about empirical software engi-
neering. The survey results suggest that developers do hold
strong, and diverse opinions, and that, some results inspire
more passion and dissension than others. We also Ô¨Ånd that
their beliefs don‚Äôt always correspond with known results in
empirical software engineering.
2. Our survey results indicate that developers‚Äô beliefs are pri-
marily based on ‚Äúpersonal experience‚Äù and far less so on re-
search results; this suggests that empirical software engineer-
ing researchers need to make more efforts to disseminate our
Ô¨Åndings.
3. Finally, we investigated the relationship between Microsoft
developer beliefs concerning the quality effects of distributed
development, and the actual phenomena, as observable from
project data. While developers in two different projects ex-
pressed differing opinions, we found that the project data
consistently indicated very little quality effect of distribution
(as did previous Ô¨Åndings at Microsoft).
Our work suggests that a) more, and more systematic effort is
required to disseminate empirical Ô¨Åndings and b) further study on
the sources of developer belief, and how it might be changed, are
needed.
The rest of the paper is structured as follows: we begin in Sec-
tion 2 with a review of the Bayesian and Frequentist approaches to
evidence and belief, and prior related work in Medicine. We re-
late this work to software engineering research in Section 3. We
then present our approach to surveying developers‚Äô beliefs, and
the results of these surveys in Section 4; Section 5 presents ac-
tual quantitative evidence from two projects relating speciÔ¨Åcally to
the quality effects of distributed software development, and the ap-
parent inconsistency of this evidence with developers‚Äô beliefs as
found in the surveys. In this study, we did not speciÔ¨Åcally study
how developers incorporate empirical evidence with prior beliefs,
and whether this conforms with the Bayesian model; we hope to do
so in future research.
2. BACKGROUND
The project of adapting belief to evidence is as old as science
itself: the goal is to design a repeatable procedure to gather data
relevant to a hypothesis under study, and then use this data to shed
light on the hypothesis. Certainly, of course, experimenters come
with with some sort of belief concerning the hypothesis, before they
gather any data. Once the data is gathered, however, it must be an-
alyzed to make an inference regarding the hypothesis; at this point,
there is a systematic process by which one‚Äôs prior beliefs are inte-
grated with the data; this is the realm of statistical inference. Two
competing statistical inference methods are available: Frequentist
and Bayesian.
2.1 Two ConÔ¨Çicting Views
Frequentist analysis of experimental data arguably has its roots
in the monumental work of R. A. Fisher. The decades-old views
of Fisher (and his collaborators Neyman and Pearson), to this day,
dominate the statistical analysis of experimental data. Frequentists
assert that the probability of correctness of a theory should be in-
directly (but exclusively) inferred from the frequency with which
the consequents of the theory are observed in experiments, whenviewed in light of assumed sampling distributions of the measure-
ments of concern. This view is empirical, and entirely grounded on
observation, and data, leaving no room for prior belief. It doesn‚Äôt
matter what you believe, before or after the experiment: the data
is the data, and it speaks the observer via the p-value‚Äîas long as
the experiment is powerful and well-designed. This way, of ignor-
ing prior belief, and taking a neutral belief stance, is epistemically
symmetric: all beliefs are considered equi-probable, viz., as sides
of a fair dice‚Äîwith all possible related hypotheses being consid-
ered equally likely before the experiment begins; and after the ex-
periment is done, and the data is gathered, you can calculate the
probability (p-value) of the observation, under each (a priori equi-
probable) hypothesis, and reject the null hypothesis if it renders the
data observation sufÔ¨Åciently improbable.
Bayesians assert that probability and statistics should reÔ¨Çect a
state of subjective belief: if I were to say that the probably of an
eventxis 0.3, I‚Äôm essentially stating that I would bet 30 cents on
a possible pay off of $1, should the event occur. In the Bayesian
world-view, the prior belief regarding the hypothesis must be con-
sidered. In an experimental setting, we probe the world, gather
data, and interpret the data in the context of our prior belief, and
then combine the prior belief with experimental data to yield an
updated posterior belief. Bayesians argue that is a more realistic
view of people actually react to evidence: we are more doubtful
of outlandish claims, and ask for stronger evidence: thus, a report
from NASA that an asteroid is sterile is likely to attract less skep-
ticism than a claimed discovery of evidence of intelligent alien life
thereon. In other words, given two experimental results with the
same p-value, the one that is less dissonant with our preconcep-
tions (viz., less surprising) will convince us more.
The use of the frequentist p-value has recently come under at-
tack, most prominently by Ioannidis et al [21], with the publica-
tion of their alarmingly titled paper ‚ÄúWhy most published scientiÔ¨Åc
results are wrong‚Äù. The paper notes that quite a number of very
prominently reported scientiÔ¨Åc results turn out to be wrong, and
are subsequently retracted.
2.2 Ascertainment, Publication, and Media Bi-
ases
Critiques and alternative perspectives on frequentist p-values are
having a strong impact in Ô¨Åelds such as medicine; these critiques
have not yet made a strong impact in empirical software engineer-
ing. We present an overview below.
At the outset, we note that many of the arguments below are ap-
plicable in settings where experimental power (viz, the unlikelihood
of a false negative Ô¨Ånding) is limited by practical considerations,
such as cost, effort, or the need for human subjects. Thus, these ar-
guments are applicable more in research settings where controlled
experiments are done with human subjects. In the (currently more
popular) ‚Äúbig data‚Äù settings, which are based on historical data
mined from software repositories, the following arguments are less
of a concern. These are essentially observational studies where
large sample sizes, and available variances in variables of interest,
enable sophisticated multiple regression analysis. In these cases,
sample sizes in the thousands (or even more; in this paper, we have
sample sizes in the hundreds of thousands) afford formidable ex-
perimental power (even for effects that are statistically small) and
also yield very low p-values (false positive rates). This power al-
lows experimental p-values to take a dominant role, and attenuate
the effect of prior beliefs.
However, the following are still applicable in experimental set-
tings in software engineering, where human subjects may be used,
and samples are small; and so are presented for completeness.
1092.2.1 The problem with p-values
Wacholder [55] and later, Ioannidis [21] have argued that fre-
quentist p-values are only one component of a rational approach to
scientiÔ¨Åc knowledge. Their critiques have two main thrusts. First
is a purely Bayesian argument: if one assumes that the prior prob-
abilities of a hypothesis being true, , or false, (1 ) are not
equal, and that the hypothesis is actually false, then simple experi-
mental error (false positive rate, or alpha , as well as false negative
ratebeta) leads to a higher rate of erroneous alternative hypothesis
conÔ¨Årmation ("false discovery"), as given by the formula
(1 )
(1 ) + (1 )(1)
More concretely, if you give a hypothesis a 25% prior chance of
being true ( = 0:25), and even your false negative rate is vanish-
ingly small, then with a p value of 0.05, there is an almost 13%
risk of false discovery. This can be viewed as a Bayesian account
of the popular adage "extraordinary claims require extra-ordinary
evidence"; the more extra-ordinary the claim, the lower the prior
belief, i.e., the lower the value of .
The second argument is that the inherent variation in p-values
due to sampling error has an unfortunate interaction with the career
incentives of scientists. Because of the desire to publish, there is
often a tendency to meander towards a favorable conclusion, and
stop there; thus, eg., one might (often unconsciously) tend to re-
design the experiment a few times when it appears that conclusions
are not what is expected, and stop the redesign when the conclusion
(i.e., the p-value) is in the expected range for a signiÔ¨Åcant Ô¨Ånding in
the desired direction. Another complication, as pointed out in Ion-
nides [20], ethical imperatives that govern medical research, might
require the studies be halted when the treatment appears effective,
and the treatment be provided to all, including the control group.
Unfortunately, Ô¨Årst-principle sampling probabilities indicates that
the initial effect in the sample being observed in such early-stop
studies could by chance tend to be higher than the effect in the gen-
eral population; and thus the effect sizes observed upon replication
would tend to be smaller.
The third and Ô¨Ånal point here is that the media focus on surpris-
ing results often emphasizes Ô¨Åndings that might be false discover-
ies. Findings are "surprising" precisely when the prior subjective
belief in them is low (viz., is small). A purely Bayesian analy-
sis would lead one conclude that the risk of false discovery is high
in this setting; combining this with the career incentives mentioned
above, leads to an unfortunate mix of incentives and false discovery
risk. Worst of all, media coverage leads to greater public awareness
and political inÔ¨Çuence!
As mentioned earlier, this issue is more of a concern for cases
where experimental power and sample sizes are limited; so re-
searchers doing human studies subjects, for example, should care-
fully consider the admonishments of Ioannidis and Wacholder.
2.2.2 Bayesian Experimental Design
A more constructive ("Positivist") Bayesian analysis of prior be-
liefs can be found in the work of Chaloner et al. Chaloner [11]
and her colleagues argue that practitioner belief matters. Even if an
experiment has a true and therapeutically important Ô¨Ånding, unless
medical practitioners buy into it, they won‚Äôt change their practice,
and the work will have no pragmatic effect. Thus, an experimental
design (concerning a health outcome of great public value) that ig-
nores practitioners‚Äô prior belief could be criticized as being uneth-
ical (even if it has adequate experimental power, large effect size,
and low p-values) if it fails to gather evidence to overcome practi-
tioners‚Äô prior beliefs (if these are known a-priori).In Bayesian experimental design, one decides ahead of time a de-
sired level of utility to be gained from an experiment; typically this
the information gain with respect to the subjective distribution of
an outcome of interest (say duration over which a patient remains
symptom-free after treatment). This gain corresponds to the net
reduction of uncertainty (increased knowledge) as a result of the
experiment. Next, the prior belief of practitioners is gathered and
aggregated (usually using a survey methodology). Based on the in-
formation gain goals, and the prior beliefs, an optimization process
chooses experimental parameters (sample sizes, treatment (dosage)
levels, etc.). If the practitioners are skeptical, their prior belief is
generally clustered around the belief that the treatment is ineffec-
tive; then a high information gain (viz., high experimental power,
large effect sizes, and low p-values) are required to overcome their
skepticism.
Thus when designing human-studies experiments in software en-
gineering on topics that inspire a great deal of passion among de-
velopers, such as the role of programming languages in software
quality (see below), it would be quite sensible to design experi-
ments with large sample sizes and high power, so as to provide
evidence that would ‚Äúmove the needle‚Äù on practitioner belief.
2.3 From Evidence to Belief
For society to reap maximum beneÔ¨Åt, scientiÔ¨Åc evidence must
translate into practitioner belief. This imperative to transmit re-
search Ô¨Åndings to practitioners has long been recognized in Med-
icine. However, busy practitioners have trouble keeping up with
research. A great deal of effort [12] has been made to digest and
disseminate Ô¨Åndings to practitioners in a brief, digestible form. On-
line, curated, indexed, catalogued collections of scientiÔ¨Åc results,
such as the Cochrane Collaboration1or the American College of
Physicians2provide practitioners a convenient, reliable, up-to-date,
and centralized means to access research Ô¨Åndings on relevant top-
ics.
Kitchenham et al have been strong advocates of similar efforts in
software engineering: practitioners should be made more aware of
the latest empirical Ô¨Åndings! Their pioneering paper [27] was pub-
lished in 2004. Since then, and rather unexpectedly, empirical work
in SE has accelerated, gaining momentum from the Ô¨Çood of data in
open-source repositories. Hundreds of papers on empirical Ô¨Ånd-
ings have been published, in Ô¨Çagship conferences like ICSE, FSE,
ASE, PLDI, POPL, OOPSLA, etc, as well as in more specialized
conferences on repository mining, empirical work, software main-
tenance and re-engineering, and others. A broad set of aspects of
software product and process has come under study. Secondarily,
as advocated by evidence-based practice pioneers [24, 26] system-
atic literature reviews are starting to be published.
What has been the impact of all this activity? Have empirical
Ô¨Åndings translated into practitioner belief?
The enduring inÔ¨Çuence of Kitchenham et al, and the tremendous
rate of research results coming forth in empirical software engi-
neering, makes this an opportune moment to study, in the trenches,
as it were, what developers actually believe, and how this relates to
the actual evidence. This is the central goal of this paper.
What do programmers believe, and how do these beliefs relate
to the actual empirical evidence?
3. A RESEARCH PROGRAM
1http://www.cochrane.org
2http://www.acponline.org
110Question Score Variance
Code quality (defect occurrence) depends on which programming language is used [46] 3.17 1.16
Fixing defects is riskier (more likely to cause future defects) than adding new features [34, 48] 2.63 1.08
Geographically distributed teams produce code whose quality (defect occurrence) is just as good as
teams that are not geographically distributed [29, 6]2.86 1.07
When it comes to producing code with fewer defects speciÔ¨Åc experience in the project matters more
than overall general experience in programming [39]3.5 1.06
Well commented code has fewer defects [52] 3.4 1.05
Code written in a language with static typing (e.g., C#) tends to have fewer bugs than code written in a
language with dynamic typing (e.g., Python) [46, 15]3.75 1.02
Stronger code ownership (i.e, fewer people owning a module or Ô¨Åle) leads to better software quality [7, 57, 15] 3.75 1.02
Merge commits are buggier than other commits. 3.4 0.97
Components with more unit tests have fewer customer-found defects [22]. 3.85 0.95
More experienced programmers produce code with fewer defects. [34, 39] 3.86 0.94
More defects are found in more complex code. [25] 4.0 0.93
Factors affecting code quality (defect occurrence) vary from project to project. [59, 42] 3.8 0.92
Using asserts improves code quality (reduces defect occurrence) [4, 3] 3.78 0.89
The use of static analysis tools improves end user quality (fewer defects are found by users) [53, 58] 3.77 0.87
Coding standards help improve software quality [8] 4.18 0.79
Code reviews improve software quality (reduces defect occurrence) [38] 4.48 0.64
Table 1: The four most controversial, viz, inciting the most disparity in agreement, (top above the Ô¨Årst double line) and least controversial (below the 2nd
double line) statements in our survey. The Score is numerical (1-Strongly Disgree to 5 Strongly Agree). Answer score of 3 indicates neutrality. The Variance is
a measure of disagreement between respondents. The citations indicate relevant work, due to space reasons, we preferred to cite only the most closely related
works. The item on merge commits was added opportunistically, to help future research.
Like Medicine, software engineering is highly consequential to
personal health and safety, social well-being, and the economy. As
in Medicine, outcomes of interest (e.g., cost, quality, and interval)
arise from the interaction of technical factors (programming lan-
guages, architectures, designs, software tools) with human and so-
cial factors. Thus outcomes arising from any given factor (e.g.,
sound static typing) may be quite different in practice than in the-
ory, and must be evaluated empirically, using controlled or natural
experiments. Researchers in ESE now produce a steady stream of
evidence-based Ô¨Åndings regarding the factors affecting cost, qual-
ity and interval in software development. However, like medicine,
SE is knowledge-intensive; software developers are skilled, trained,
practitioners who can be expected to practice their craft based on
a set of beliefs gleaned, thoughtfully, not only from their training,
but also from their own work experience.
Prima facie, there are good reasons to believe that many of the
arguments quoted above (which arise in the Biological sciences and
Medicine) apply equally well to software. Articles in trade maga-
zines, and in the blogosphere, suggest that developers do often hold
passionate opinions on such matters as static vs. dynamic typing,
compiled vs. scripting languages etc. These opinions may be based
actual experience, or may be held for ideological reasons (or self-
interest), even in the face of evidence to the contrary. Regardless of
the origins of prior beliefs, there is good reason to believe that the
way these practitioners respond to new evidence will be inÔ¨Çuenced
by their prior beliefs.
Given that developers are highly-trained, quantitatively oriented
professionals, one can expect them to have strong opinions that are
both informed by, and inform, their practice. The above discussion
suggests a wide range of questions: What opinions do program-
mers hold? How do they come by this evidence? Do beliefs vary?
Do they change? Why? Do beliefs and evidence contradict? or Re-inforce? How are we to combine the two in formulating research
results? If developers‚Äô beliefs are dissonant with evidence, why is
that? How can we change developers‚Äô beliefs? There many such
questions; we believe that the answers to these questions will play
a vital role in ensuring that Ô¨Åndings from empirical software engi-
neering actually have an impact in the practice of software engi-
neering.
Note that this argument applies equally well to beliefs and Ô¨Ånd-
ings that relate to artifacts and artifact performance (language fea-
tures, static analysis tools, veriÔ¨Åcation algorithms etc.) as well as
beliefs and Ô¨Åndings that relate to process aspects (distributed de-
velopment, agile methods, team organization etc.). The nature and
trajectory of the interactions between belief and evidence in soft-
ware engineering practice is a complex, important, and impactful
phenomenon, worthy of sustained study.
Prior work in this area of beliefs and evidence has been primar-
ily qualitative in nature. Rainer et al ‚Äôs focus group studies Ô¨Ånd
that when programmers form views about software process im-
provement, they ‚Äúfavour local opinion over independent empirical
evidence‚Äù [43]. Passos [37] using a qualitiative interview-based
approach, that organizational and project contexts inÔ¨Çuence belief
formation. Dyb√§ et al‚Äôs practice prescriptions for evidence-based
software engineering require (See Step 4, [13]) that the presenta-
tion and incorporation of evidence is always contextualized within
prior belief and experience. J√∏rgensen et al [23], in a study of effort
estimation practice, found that manager‚Äôs prior beliefs inÔ¨Çuenced
how they interpreted neutral (randomly-generated) data; they also
found, more disturbingly, evidence suggesting that Ô¨Åndings in re-
search papers could also have been affected by prior biases. Our
work here is complementary to the above research; we show, quan-
titatively, that practitioner beliefs can be inconsistent with project
evidence, which suggests the need for careful reconciliation.
1114. RESEARCH QUESTION AND
METHODOLOGY
The central question in our initial foray (into the project de-
scribed above) is recapitulated below:
What do programmers believe, and how do these beliefs re-
late to the actual empirical evidence?
Our initial focus was to address these questions in the speciÔ¨Åc
context of one large industrial organization (Microsoft) and gather
data on both programmers beliefs, and secondly, to perform a de-
tailed case study to judge the relationship of these beliefs to actual
evidence in some speciÔ¨Åc projects.
Survey Design The core of the survey started with a series of em-
pirically falsiable claims, mostly drawn from software engineering
research. We chose a number of claims, for inclusion in our survey,
based on the following set of criteria
The consequences of the claim being true or false are ‚Äúac-
tionable", viz, consequential for software practice.
We believed that our target population (Microsoft develop-
ers) would have opinions on these claims.
We believed that this population would have had experience
with the tools or processes in question, and would have been
able to form not just an opinion, but an informed one.
We believed that regardless of expressed opinion, we would
be able to gather evidence strongly relevant to the claims.
Opportunistically, we also added a few claims that we found in-
teresting, but about which, as of yet, we weren‚Äôt aware of well-
established results. We added these as possible avenues of future
study, to take advantage of this survey instance to gather some more
useful data on developer beliefs. The full list of claims is in Table 1.
For each of these claims, we asked developers to respond on a
5-point Likert scale (Strongly Disagree, Disagree, Neutral, Agree,
Strongly Agree). Finally, in all cases, we scripted the survey to
choose questions on which developers expressed more polarized
opinions, and asked them to explain the origins of their view (more
details below, Section 4.1, under ‚ÄúOpinion Formation‚Äù). In addi-
tion, they were also asked to provide a rationale, in form of ‚Äúrea-
sons for your answer". This rationale was used as a way for us to
understand the answers.
In addition we collected demographic evidence, similar to Lo et
al[31]. The following information was gathered:
Demographics: Age, gender, years at Microsoft, years as a
developer, highest level of schooling.
Employment : Primary division, years at current job, job title,
whether they are managing anyone.
Geographic: Primary work location.
Target Audience Our target audience were people primarily in a
software engineering discipline at Microsoft: this included devel-
opers, testers, program managers, and their immediate supervisors.
These people we felt, would have the opportunity to form informed
opinions about the claims that were offered to them. We identiÔ¨Åed
about 2500 professionals, from various locations around the world,in various projects, and sent an email with a link to the survey and
solicited a response.
No identifying information was required or gathered from re-
spondents; they could, separately, and without connection to their
answers, volunteer to offer themselves for a follow-up interview.
Distinct from the survey, the could also enter their email addresses
to be entered into a rafÔ¨Çe to win a gift card.
4.1 Survey Results
We now present our main Ô¨Åndings from the survey.
Overall impressions We received a total of 564 responses, a re-
sponse rate around 22%. Survey respondents varied in age, gender,
location, etc. The mean age was 32.5 ( = 8). The respondents
skewed male (497 male, 53 female, 7 other, and 7 didn‚Äôt state).
They were largely college educated. Of those that stated, 267 had
Bachelor‚Äôs degrees, 211 had Master‚Äôs, and 29 had a PhD; the rest
didn‚Äôt respond. Respondents are from all locations of Microsoft; of
the ones who stated location, the largest cohort was from the US
(386); the rest were from India (48), China(39), Europe (66), and
other locations (25). While these demographics suggest capture
of a broad cross-section of the overall population of developers, it
should be noted that all respondents to one degree or another are
inÔ¨Çuenced by the business context of Microsoft, and to a large ex-
tent, the (dominantly male) North American software engineering
culture.
We scored the Likert scale from 1 for ‚ÄúStrongly Disagree" to
5 for "Strongly Agree". In Table 1 we present the average score
and standard deviation for all the claims in our survey. The higher
the average, the more the agreement with the claim; the higher the
variance, the more the disagreement within respondents as to the
claim. We have listed the claims in Table 1 in descreasing order of
variance; thus, we interpret the ordering as going from most con-
troversial claim to least controversial.
Overall, the claim that people disagreed with most (2.63) con-
cerned the riskiness of Ô¨Åxing defects (as compared with new fea-
tures) and the claim that most people agreed with concerned the
beneÔ¨Åts of code reviews (4.48).
4.1.1 Controversial Claims
We turn now to the claims that incited the most disparity in the
answers; we take these to be controversial claims. The most con-
troversial claim of all relates to the effect of programming language
choice on code quality. Most of the developers who disagreed with
this statement focused on the notion that programmer‚Äôs skill mat-
ters more than the language (e.g., ‚ÄúEvery trade has its master. It
depends on who write the code‚Äù). Some focused on application
logic (not programming language) as the main determinant of qual-
ity (‚ÄúMost defects stem from the application logic rather than par-
ticular platform/tools‚Äù). Developers who agreed with this claim
focused on language features, such as static typing or memory man-
agement (e.g.,‚ÄúBecause statically typed languages make mistakes
more difÔ¨Åcult‚Äù, ‚ÄúManaged code is designed to be less prone to de-
fects" ). Interestingly as it turns out, this is a topic on which there is
limited literature, with results just recently starting to emerge [46];
this certainly has long been a controversial topic, and one mightn‚Äôt
expect developer opinion on this much-debated topic to be moved
towards consensus by just one publication.
Moving through the most-controversial list, the next claim re-
lates to defect repair commits being riskier than new feature addi-
tions. Developers who disagree felt that bug Ô¨Åxes involve small-
scope changes, which are less risky: (e.g., ‚Äúdefects are generally
small and localized; features are broader‚Äù); other felt that all types
112of changes are risky. People in agreement felt that defect repair
entails greater risk of regression failure and code decay. Previ-
ous work [34, 48] provides pretty strong evidence that defect repair
changes are quite risky, but this turns out to the claim that attracts
the most disagreement from our survey respondents (lowest aver-
age agreement score, 2.63).
The next opinion relates to the effects of geographical distri-
bution on software quality. Respondents who disagreed (with the
proposition that geographically distributed development doesn‚Äôt af-
fect software quality) focused heavily on communication issues
with distant team members (e.g., ‚ÄúLack of easy communication is
the reason. Time zone differences are a big hindrance for easy com-
munication." ) and the absence of face-to-face communication, the
inability to meet in the hallways, talk over a whiteboard, etc. Peo-
ple who agreed, thought that electronic collaboration tools (email,
shared repositories, distributed review tools) made distributed de-
velopment workable (e.g., ‚Äúwe can communicated by code review
and online meeting" ).
Interestingly, two studies, both done at Microsoft, on different
projects examined this very issue: one found that distributed de-
velopment had no effect on software quality, and the other found
a very small, barely discernible effect. We examine this speciÔ¨Åc
issue in much greater depth below (Section 6).
The next claim states: speciÔ¨Åc experience matters. Developers
who disagreed stated several reasons: Ô¨Årst, they believed that skill,
once developed, was portable: ‚ÄúExcellent engineering and coding
practices are a skill that transfers between projects. ‚Äù. Developers
who agreed tended to focus on the importance of domain experi-
ence ‚ÄúThe domain experience helps alleviate common pattern of is-
sues. General experience in programming allows us to have strong
grasp on design patterns. However how that pattern translates to
the problem at hand is a function of the speciÔ¨Åc project experience‚Äù
There is a strong evidence that speciÔ¨Åc experience matters more
than general experience [39], and also that minor contributors, with
little speciÔ¨Åc experience, are strongly implicated in errors [7].
Overall, in the survey results, considering the most controversial
questions, rather unexpectedly, we Ô¨Ånd some supported by strong,
consistent empirical evidence3. These are also questions which
most would consider highly consequential and actionable; indeed
these are precisely the sorts of questions where one might most
wish to Ô¨Ånd consistency between research Ô¨Åndings and practitioner
belief, and also between practitioners.
4.1.2 Un-controversial Claims
We now turn to questions which incited the least disparity in re-
sponder agreement. We note that the least controversial claim, the
one that most people agreed with (4.48, variance of 0.64), was that
Code reviews improve software quality. It is interesting that this is
perhaps one of the most well-supported Ô¨Åndings in empirical soft-
ware engineering, with numerous conÔ¨Årming their beneÔ¨Åts, going
back to Fagan‚Äôs work at IBM in the 1970s. It is heartening to note
that these established Ô¨Åndings from our Ô¨Åeld have taken hold in
developers‚Äô belief systems, at least in this case.
The situation is more complex on the other beliefs. Thus, pro-
grammers are strongly congruent on the belief (4.18, variance of
0.78) that coding standards help improve software quality. The ra-
tionale comments from the programmers who answered this ques-
tion, clearly indicate that developers who agreed with this claim
believed that coding standards reduced defect occurrence. How-
3While the geographic distribution question has had some contra-
dictory results, the results within Microsoft data are largely con-
sistently supporting the claim that Geographic distribution doesn‚Äôt
affect software quality.ever, actual empirical evidence on this is quite limited; we could
Ô¨Ånd only one published result, which examined a coding standard
in the context of one industrial case study, which found scant ev-
idence that the coding standard was beneÔ¨Åcial. Comments from
programmers, however, clearly indicate that they believe coding
standards make code easier to maintain, (e.g. ‚ÄúCoding standards
can help to make the code written by different developers easier to
read and maintain‚Äù and‚ÄúIf everyone in the team follows coding
standards, it will be very easy to review others code and requires
less time. " ). This relates coding standards to readability of code;
recent results on natural coding conventions [2] and coding reviews
in pull requests [17] that predictable, regular coding styles are pre-
ferred. This Ô¨Ånding suggests that more research in this area would
be well-warranted.
Our survey respondents also widely believed that static analysis
tools improve software quality. This is another area where the re-
sults in the literature are not as consistent nor as strong as developer
belief: Zheng et al report that static analysis tools are only about a
third as effective as testing, and no cheaper than inspections; how-
ever they are helpful, specially in identifying certain kinds of cod-
ing errors, relating to error-checking, or assignments; while Rah-
manet al [41] report that while static bug Ô¨Ånders are helpful, they
are not much better that statistical defect prediction, which only de-
pends on process measures; and Wagner et al [56] report that static
bug Ô¨Ånders can Ô¨Ånd different errors, but suffer from very high false
positive rates. The developers‚Äô overall strong belief in static analy-
sis tools, however, suggests that more study of their beneÔ¨Åts would
be valuable.
Developers also believe that asserts are beneÔ¨Åcial. In their ra-
tionale, they offer two distinct beneÔ¨Åts: the documentary function
(e.g., ‚ÄúWell placed asserts make people aware of the assumptions
they make when coding." ), where asserts held understand code, and
the more common diagnostic function (e.g., ‚ÄúUsing asserts usu-
ally helps to detect errors at the earliest point." ), where asserts
can help quickly detect violations of coding assumptions, or in-
terface pre-conditions. This is also an area where there is not a
lot research into understanding what the precise role by the two
functions (documentary and diagnostic) of asserts, and the relative
beneÔ¨Åts of each.
Finally, we note another claim ‚Äúmore defects are found in more
complex code‚Äù which has a high level of average agreement (4.0)
with some controversy (0.93). Developers repeatedly observe (in
the rationale) that complex code is more difÔ¨Åcult to maintain ("har-
der to understand, debug and test", "Complex code is harder to
reason", "It hidden (hides) the purpose of the code, and confuse
reviewers or maintainers" ). These beliefs suggest that metrics that
can identify complex code could would be useful in predicting de-
fects. Thus far, attempts to deÔ¨Åne complexity metrics have been
stymied by the fact that they are all strongly correlated with size [14]:
so complexity metrics simply end up predicting that bigger Ô¨Åles
contain more bugs. Indeed, recent work casts doubt on useful-
ness of any kind of product metric (property of source code) to
usefully predict defects [40]. However, programmers‚Äô strong be-
lief that complex code is buggy code suggests that further study,
perhaps to develop metrics that better isolate complexity, somehow
de-correlated from size, would be a helpful way to predict defective
code.
4.1.3 Opinion Formation
Our survey did solicit (as described earlier) developers‚Äô state-
ments on how they formed their opinion. In this section, we present
those results.
113Figure
1:Factors stated as most inÔ¨Çuential on forming opinions
of survey respondents.
For the two beliefs on which each developer expressed the strongest
opinions (either agreeing or disagreeing) we asked them to rank
possible factors in his/her opinion formation4. Thus if the devel-
oper said they "Strongly Disagreed" with the claim that program-
ming language choice affects defect occurrence, they would be pre-
sented with this answer, and asked:
What factors played a role in your previous answer?
Please choose the relevant factors from the list below,
and rank them
They were given a choice of ‚ÄúPersonal experience", ‚ÄúWhat I hear
from my peers", ‚ÄúWhat I hear from my mentors/managers" ,‚ÄúArti-
cles in industry magazines", ‚ÄúResearch papers‚Äù. and "Other". We
then gathered the ranks given for each of the above factor. Thus,
we had for each of the answers (agreement/disagreement with the
claims), and each of possible factors, a collection of ranks. If a
particular factor is ranked more frequently, more highly, we would
get a higher range of values for that factor. Thus for each factor, we
gathered a collection of ranks assigned to that factor. For clarity (so
that higher ranks appear higher in the plot, we inverted the rank, so
on the plot higher values correspond to higher ranks. The results
are in Figure 1.
The highest ranked factor inÔ¨Çuencing respondents‚Äô opinions on
the given claims, is Personal experience, which was chosen for
ranking 1033 times. A look at the box plot in Figure 1 shows that
it was almost always ranked at the top, and a handful times at other
positions. Next was What I hear from my peers, which was chosen
for ranking 674 times, with a median second rank. Next was What
I hear from my mentors/managers, chosen 499 times for ranking,
and with a median third rank. The lowest ranked was Other, chosen
148 times. Just above that, in Ô¨Åfth position, was Research papers,
chosen 257 times for ranking. We take a couple of conclusions
from this.
First, the factors affecting opinion are consistent with earlier
work in social science‚Äîthe strength of inÔ¨Çuence on opinion for-
mation decays with strength of social connection‚Äîthings we hear
4In
case they expressed strong (or equally strong) opinions on more
than 2 claims, we chose 2 at random.from people closer to us matter more than others. Brown & Rein-
gen [9] for example, found that stronger social ties are more in-
Ô¨Çuential in opinion formation than weaker ones. This provides a
reasonable framework to understand why developers give strongest
weight to personal experience, and then to peers, and then to man-
agers. Furthermore, developers appear to be inÔ¨Çuenced by trade
journals rather than research papers; this may also be because they
view trade journals as closer to their situation than research papers;
this requires further study.
Secondly, while this type of opinion formation might be accept-
able in ordinary society, this is hardly an ideal way for profession-
als to form opinions. Personal experience is highly isolated, and
doesn‚Äôt provide a broad sample of experience. A particular devel-
oper‚Äôs experience may be based on recollections of his or her own
work experience, with his/her code, team, project etc, and have lit-
tle to do with overall, large-scale trends. Furthermore, what we
remember can also be highly inÔ¨Çuenced by salience rather than fre-
quency. We tend to remember emotionally-laden experiences [10]
and don‚Äôt necessarily recall more mundane occurrences quite so
well. Thus one developer might vividly recall having a very dif-
Ô¨Åcult time repairing another person‚Äôs low-quality bug Ô¨Åxes, at one
time, and then conclude all defect-repairs are risky; she might not
recall, or even notice, that most bug-Ô¨Åxes were entirely adequate,
and never cause any trouble. She might then strongly argue this
view, repeatedly, that repairing bugs is risky; however, if a statisti-
cal analysis were done, at large scale, in her project, of the entire
population of bug Ô¨Åxes, there might be a very different conclusion
to be drawn.
In Medicine, the need to have therapeutic practice based on ev-
idence is well-recognized, and there is a concerted, well-funded,
systematic effort to disseminate scientiÔ¨Åc evidence from research
Ô¨Åndings to physicians. Indeed, most modern physicians, if asked,
would profess to be strongly evidence- and research-based in their
practice.
Consider how you would react if your physician said that his
or her medical decisions are most strongly based on ‚Äúpersonal ex-
perience" and much less so on ‚Äúresearch"? The current state of
evidence-based practice, in Medicine, offers an inspiring model for
more a more organized and effective dissemination of research re-
sults in Software Engineering.
5. EVIDENCE
Of the top 3 most controversial statements, we chose one of
them, ‚ÄúGeographically distributed teams produce code whose qual-
ity, viz., defect occurrence, is just as good as teams that are not
geographically distributed‚Äù to examine in more detail: we decided
to compare the beliefs of programmers, with evidence from actual
project data drawn from two large projects, which we denote as
Pr-A andPr-B . They are both quite large: Pr-A is an operat-
ing system, and consists of about 400,000 Ô¨Åles, with over 150M
SLOC and Pr-B is a web service, and consists of about 430,000
Ô¨Åles, with about 85 Million SLOC. Our choice to delve into this
particular question, with these two particular projects, was based on
a curious phenomenon: statistical analysis revealed Pr-A members
tended to be ones who largely disagreed with the above statement,
while Pr-B members tended to largely agree with the statement
(p < 0:001, using Pearson‚Äôs Chi-squared test). This difference
in belief particularly striking, given that projects practice widely
distrib
uted development: both Pr-A andPr-B have around 8000
developers located in over 100 different buildings, in dozens of
cities in about a dozen different countries around the world. In
both projects, a non-trivial number of Ô¨Åles received commit activ-
ity from multiple buildings, cities, regions, and countries (see Table
114Project Commits Commits Commits Commits
(1 Building) (1 City) (1 Region) (1 Nation)
Pr-A 56% 90% 91% 92%
Pr-B 76% 80% 83% 85%
Table 2: Proportion of Ô¨Åles in projects with majority (75%) com-
mits from one building, one city, one region, and one nation.
Clearly, while Pr-A ‚Äúlives‚Äù in more buildings, Pr-B has substan-
tively more activity outside of a single city, region, and country
2).
Thus, developers in both countries could be expected to have
personal experience with distributed development, and thus would
have had the opportunity to form their beliefs based on their ex-
periences. The statistically signiÔ¨Åcant differences that emerge in
the survey, therefore, could be presumed to be based on intrinsic,
observable differences in the two projects.
Perhaps Pr-A had encountered more quality difÔ¨Åculties in dis-
tributed development, and Pr-B had none. Findings on this topic,
while by no means uniform [29, 6, 18, 45, 44], tends to lean on
the side of agreement with the statement above, particularly with
respect to teams at Microsoft [29, 6]. To examine these issues
in more detail, we gathered data on development histories (who
changed what Ô¨Åle, how much, and when), defect repair (commits
that marked as defective in the logs, using techniques popularized
by Mockus et al [33], and Sliwerski et al [50]), as well as developer
locations at the time changes were made (using internal Microsoft
databases). Using this gathered data, we used known techniques
for studying the effects of geographical distribution, based on mea-
sures used in earlier work [29, 6].
Our data was gathered on a per Ô¨Åle basis; Pr-A andPr-B both
had around 400,000 Ô¨Åles, and the data comprised millions of changes,
performed starting in 2012. We gathered several metrics on these
Ô¨Åles, described below. Since the goal of this study to gather quanti-
tative, project-speciÔ¨Åc evidence on software quality, the main phe-
nomenon we were interested reÔ¨Çected the number of bug-Ô¨Åx com-
mits. This, then, is our primary response variable:
nÔ¨Åx Number of defect repairs associated to the Ô¨Åle. This data was
gathered using project-speciÔ¨Åc conventions on identifying defect
repair changes. One project used the convention that all bug-Ô¨Åx
logs began with "BUG: :::". The other project had a range of con-
ventions for bug-Ô¨Åx logs. These conventions were known to the
authors from prior investigations, and informants in the respective
development communities.
Next, since our goal is to attempt to isolate and measure the ef-
fect of geographic distribution, it is important to control for known
confounds that might affect the nÔ¨Åx outcome. We use 4 different
control measures, all chosen from prior work on the determinants
of software quality.
meansize Average size of the Ô¨Åle, in lines of code. This is a control
variable; in general size is expected to be strongly correlated with
number of Ô¨Åxes.
chgcnt Number of commits made to the Ô¨Åle. Prior work has es-
tablished that change (churn) is strongly correlated with defects;
the more Ô¨Åles change, the more likely it is that defects are intro-
duced [35].todc Total number of distinct developers commiting to the Ô¨Åle.
Prior research as indicated that the number of developers involved
in a Ô¨Åle inÔ¨Çuences quality [6, 39, 32], and can be a confounding
factor in studies of the effect of distributed development on qual-
ity [6].
otop Ownership: percentage of commits made by the most fre-
quent committer to this Ô¨Åle. Strong, dominant ownership, i.e.,the
proportion of commits made by the majority contributor to a Ô¨Åle,
can inÔ¨Çuence software quality [39], so we include this as a control.
After controlling for the known confounds, we include the key
experimental variables relating to degree of distribution. Follow-
ing [6, 29], we consider several distinct levels of distributed devel-
opment.
Binary indicators of localization level of Ô¨Åles We used 4 binary
variables, indicating whether more than 75% of the commits were
made within one building (in1b), in one city (in1c), region (in1r),
and nation (in1n) respectively. This modeling approach and the
threshold levels used were based on prior work by Bird et al [6],
and Kocaguneli et al [29]. As Kocaguneli et al justify, these vari-
ables ‚Äúindicate the smallest geographical entity‚Äù within which ‚Äúde-
velopers account for 75% of the edits to a Ô¨Åle". The variables cap-
ture different degrees to which a Ô¨Åle can be distributed: for example
distances within a building are walkable; within a city, some trans-
portation, or a phone call may be involved; outside of the same
city personal contact is even harder, and and once outside a coun-
try time-zones complicate personal live communications. As with
Kocaguneli et al we performed sensitivity analysis, and got similar
results for thresholds ranging from 65% to 85%
Table ??shows the level of distributed development activity in
both Pr-A andPr-B , using the binary indicator variables described
above. Thus, for Pr-B , 76% of Ô¨Åles have 75% or more of their
commits from a single city, and 80% of the Ô¨Åles have 75% or more
of their commits from a single building. As can be seen, there are a
non-trivial number of Ô¨Åles in both projects that have signiÔ¨Åcant dis-
tributed development activity. Thus these projects are both reason-
able settings to study the quality effects of distributed devlopment,
as well as being settings were developers can be expected to have
had reasonable experience with the practice and consequences of
distributed development.
We began our data analysis by modeling the nÔ¨Åx variable as a
response, against just the controls (meansize, chgcnt, todc and
otop). All our models are linear regression models; model diag-
nostics were performed using recommended criteria: in all cases,
the residual distributions (from the linear models) were inspected
using the qqnorm plots to ensure acceptable normality; variance
inÔ¨Çation was found to be in recommended ranges, and outliers were
removed to avoid high-leverage points. The data was reasonably
balanced between zeros, and non-zeros, and there was no indica-
tion of zero inÔ¨Çation. In addition, since the data were slighly over-
dispersed, we compared the linear regression models with count
(quasi-poisson) models, and got essentially the same results; for
simplicity, we just present the results of linear regression. The
model for Pr-A (Model 2) and Pr-B (Model 3) are shown below.
The models show the direction and signiÔ¨Åcance of the effect (T-
value magnitude shows signiÔ¨Åcance, the sign shows the direction
of the effect); the p-values are all highly signiÔ¨Åcant (and calculable
from the T-distribution).
We infer from the models above is that a) all the controls are
signiÔ¨Åcant, and b) the effects are in the directions that we might
115Model 1 Effect on number of repairs, at each level of (non) distribution. We show effect size measured using Cohen‚Äôs f2. All effects much
lower than the small effect threshold, which is 0:02. All effects, however are statistically signiÔ¨Åcant (p < 0.001), except for the ‚Äúsame city‚Äù
effect in Pr-B , thanks to large sample sizes. Linear regression diagnostics (normality of residuals, VIF, heteroskedasticity, etc) are well
controlled.
Project Same Building Same City Same Region Same Country Model
Cohen‚Äôs f2and (T value) Cohen‚Äôs f2and (T value) Cohen‚Äôs f2and (T value) Cohen‚Äôs f2and (T value) F SigniÔ¨Åcance
Pr-A f2= 0:0015 (-20.9) f2< 0.001 (11.3) f2= 0.0030 (15.2) f2< 0.001 (7.9) (All p<0.001)
Pr-B f2= 0:0035 (-30.0) f2<0:001 (-2.17, p= 0.03) f2= 0:0017 (21.9) f2= 0:001 (16.9) (All p< 0.001,
unless noted)
Model 2 Pr-A data, controls only. F Statistic = 2.5e+5, p <:001,
R2= 0.65; linear regression diagnostics (normality of residuals,
VIF, heteroskedasticity, etc) are well controlled and/or within ac-
cepted limits
Variable T value SigniÔ¨Åcance
(Intercept) 49.8 p < :001
meansize 4.7 p < :001
chgcnt 548.3 p < :001
todc 59.3 p < :001
otop -17.2 p < :001
Model 3 Pr-B data, controls only. F Statistic = 6.4e+4, p < :001,
R2= 0.34; linear regression diagnostics (normality of residuals,
VIF, heteroskedasticity, etc) are well controlled and/or within ac-
ceptable limites
Variable T value SigniÔ¨Åcance
(Intercept) 72.2 p < :001
meansize 13.6 p < :001
chgcnt 241.6 p < :001
todc 88.2 p < :001
otop -40.0 p < :001
expect, viz.defects increase signiÔ¨Åcantly with size of Ô¨Åles, churn,
total number of developers committing to a Ô¨Åle; defects decrease
with ownership level.
To gauge the effect of the experimental variables, we add the
indicator variables for each distribution level as described above,
in turn to the above models. In other words, we built 4 successive
models for each of the two projects, adding (in turn) the variables
in1b, in1c etc. In total, we have 8 different models, each consisting
of the four control variables and one indicator variable. Each model
would thus give us an indication of the effect on Ô¨Åles which were
(mostly) changed within the one geographic location indicated by
the operative indicator variable. The results are shown in Model 1.
For each indicator variable, we calculated the percentage differ-
ence in annual defect-repair activity corresponding to that level of
localization, when controlling for Ô¨Åle size, change count, number
of developers, and Ô¨Åle ownership. All levels of localization had
a statistically signiÔ¨Åcant impact on software quality. All indica-
tor variables showed statistically signiÔ¨Åcant effects in the model.
Given the large sample sizes (several hundred thousand Ô¨Åles), we
can expect to able to measure even small effects.
However, the inclusion of the localization variables didn‚Äôt change
the explanatory power of the models in any instance: in all cases,
the proportionate change in R2value was less than 510 3(0.5%).
We used the Cohen‚Äôs f2measure to gauge the effect size of these
indicator variables. Cohen‚Äôs f2values are computed asR2
AB R2
A
1 R2
ABwhere the subscript indicates the regressors included in the model;R2
ABmeasures the multiple R2while including regressors Bas
well as regressors A, whileR2
Ais the value with just regressors A.
Thisf2is a measure of the additional variance explained by the
addition of regressor Binto the model. In our cases Ais just the
controls used in Models 1 and 2 above, Bis the binary indicator
variable for each level of localization.
A threshold value of 0:02 (2%) forf2is suggested5as a mini-
mum value to determine that an effect size is small ; in all cases, our
computedf2values were a lot smaller than even that, leading us
to the conclusion: the effect of localization on software quality at
all levels, in both Pr-A andPr-B , when controlling for confound-
ing factors, is minimal. These Ô¨Åndings are consistent with earlier
Ô¨Åndings within the Microsoft setting by Bird et al and Kocaguneli
et al. Another noteworthy aspect of the effect of distribution is
thatit is not always in the same direction!. Thus for both Pr-A
andPr-B , it appears clear that it is (very slightly) better to be in
the same building (negative t-values, -20.9, and-30.0 respectively),
and for Pr-B it is possibly very slightly better (barely signÔ¨Åicant,
t = -2.17,p= 0.03, negligible f2) to be in the same city. For all
other cases, it appears consistently, and statistically signiÔ¨Åcantly,
very slightly better to be distributed! Although the f2for the ef-
fect in the positive direction in these cases is small, the t-values are
all quite strongly positive, indicating that Ô¨Åles mostly committed
in the same geographic area (city, region, nation) are actually very
slightly more defect-prone!!!
Thus, the respondents from Pr-B had formed beliefs that
were consistent with the actual evidence from that project,
whereas the respondents from Pr-A had formed beliefs that
were inconsistent with the actual data from that project.
This case study, in conjunction with the responses to our survey,
suggests that developers form opinions subjectively, and anecdo-
tally, based on personal experience. One possible explanation for
the difference in beliefs might be conÔ¨Årmation bias6.Pr-A (see
Table 2) is less distributed geographically, than Pr-B .Pr-A started
earlier, with its beginnings in Puget Sound, the long-time headquar-
ters of Microsoft. As a project that was initially developed in just
one location, developers from Pr-A may be more familiar, com-
fortable, and trusting with non-distributed development, whereas
Pr-B members may have had richer experience with distributed de-
velopment, and might have a more realistic view.
Indeed, in the absence of controls, as prior studies have noted, it
would appear that distributed Ô¨Åles are indeed more defective: Bird
5(See https://en.wikipedia.org/wiki/Effect_size), and the pwr
package inR.
6ConÔ¨Årmation bias is a kind of bias that ‚Äúconnotes the seeking or
interpreting of evidence in ways that are partial to existing beliefs,
expectations, or a hypothesis in hand" [36]
116et al [6] report that if one didn‚Äôt control for the number of devel-
opers, it appeared that binaries which weren‚Äôt primarily developed
in one building had 15% more defects (p< 0:0005) than ones that
were! However, once developer count was introduced as a control,
this effect vanished. People do sometimes ignore confounding vari-
ables, and jump to false conclusions in observational studies [54].
While software developers are, in general, quantitatively sophisti-
cated, and do engage in reÔ¨Çective practice, it‚Äôs unlikely that they
would have taken care to engage in the kind of careful multiple-
regression analysis that is required to tease out the insigniÔ¨Åcance
(or in our projects, the very small effect) of distribution. Absent
such careful analysis, developers soldier on with their biases and
misconceptions, without the beneÔ¨Åt of actual evidence. Again we
see that by comparison with medicine, which has been strongly
evidence-based for decades, the impact of evidence on software
engineering practice has quite some ground to cover.
Threats to Validity Our formulation of distribution levels is based
on prior analysis of the same data; however, our quantization of
distribution might fail to capture more subtle effects, such as those
relating to personal relationships. In addition we interpret software
quality directly as relating to defects; other aspects of quality (such
as maintainability, readability, etc) are not modeled.
6. RELATED WORK
There have been several prior reports of surveys of developers
in several settings. Surveys have been done to explore developer
attitudes, such as work habits [30], or motivation [19]. Others have
explored what developers do [49, 28]. Most related to our work
are studies of the epistemic attitudes of devleopers, viz, what they
know, believe, or would like to know about. Begel & Zimmer-
mann [5] used a survey methodology to Ô¨Ånd questions that are of
most interest to developers. More recently, Lo et al [31] surveyed
developers as to their views on the relevance of various research
results from software engineering. That survey asked for devel-
oper views on the relevance of the research, not on the correctness
thereof. To our knowledge, we are the Ô¨Årst to survey developers ex-
plicitly as to their agreement with claims from empirical software
engineering, and then attempt to relate some of the survey results
with statistical analysis of data drawn directly from the project in
which the respondents work.
There has been a considerable body of work on the factors that
drive methodology adoption. Hardgrave et al [16] Ô¨Ånd that devel-
oper‚Äôs opinions play a strong role in choice of software develop-
ment methodology than other factors, including social and organi-
zational pressure. Sultan & Chan [51] study the inÔ¨Çuence of in-
dividual attributes on the adoption OO technologies. Others have
studied how beliefs affect the adoption of C by COBOL program-
mers [1]. Roberts et al study the attributes of software development
methods that inÔ¨Çuence adoption [47].
There has been considerable interest in the Ô¨Åeld of medicine on
the interaction of belief and evidence, which we discussed in the
background section at the beginning of this paper.
The results of our study are very much in support of Kitchenham
et al‚Äôs project [27, 13] to promote systematic, prompt, and wide
dissemination of empirical study results.
7. CONCLUSIONS
Our goal in this paper was to explore the relationship between
quantitative evidence and practitioner belief in software engineer-
ing settings. We conducted a survey of developer beliefs with re-
spect to several claims of practical importance. We found that some
claims attracted more agreement, and dissension than others. Sur-prisingly, the level of agreement didn‚Äôt always correspond very well
with the strength of evidence in regards to the claim.
Indeed, we found that programmers give ‚Äúpersonal experience‚Äù
as the strongest inÔ¨Çuence in forming their opinions. Interestingly,
‚ÄúResearch papers‚Äù were ranked the second lowest, just above ‚ÄúOther‚Äù.
We also selected a speciÔ¨Åc question, regarding the quality effects of
Geographic distribution, where respondents from one team tended
to believe that Geographic distribution was bad for software qual-
ity, and from a different team tended to believe it had no bad effect.
Based on a quantitative analysis of the project repositories of both,
we found that geographic distribution had a barely measurable ef-
fect on quality; it was statistically signiÔ¨Åcant, but only because of
very large sample sizes (in the hundreds of thousands). Further-
more, the effect was not always in the expected direction; some-
times the effect was good, and sometimes bad. Thus, we found that
one team‚Äôs beliefs were consistent with the evidence, and another
team‚Äôs wasn‚Äôt. This Ô¨Ånding illustrates the risks that programmers
might face, by relying too much on their personal experience; sub-
jective, personal recollection is notoriously error-prone.
We draw two reccommendations from our Ô¨Åndings:
Dissemination Our Ô¨Åndings reinforce those of Kitchenham et al
in regards to evidence-based software engineering. Given
the volume of Ô¨Åndings and publications in empirical software
engineering, greater efforts should be made to set up system-
atic ways to collect, organize, disseminate our research to
practitioners, so that they (as do Medical Doctors) come to
rely on veriÔ¨Åed evidence, rather than personal observation,
which can be biased, error-prone and spotty. Initiatives like
the Cochrane Collaboration offer a practical model.
Research Directions However, prior experience in Medicine, no-
tably the work of Ioannidis, Chaloner and colleagues, sug-
gests that practitioner belief should be given due attention.
Specially in areas where research results are few and prelim-
inary, or where large sample-sizes are hard-won, we would
be well-advised to take practitioner belief into account both
in developing hypotheses for study, as well as designing ex-
perimental methods. In our study, for example, developers
strongly endorse Coding Standards, and the use Static anal-
ysis tools; there is limited empirical understanding of the ef-
fects of these practices, and further study could well be war-
ranted.
8. ACKNOWLEDGMENTS
We thank all survey participants for responding to our survey.
9. REFERENCES
[1] R. Agarwal and J. Prasad. A Ô¨Åeld study of the adoptxion of
software process innovations by information systems
professionals. Engineering Management, IEEE Transactions
on, 47(3):295‚Äì308, 2000.
[2] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton. Learning
natural coding conventions. In Proceedings of the 22nd ACM
SIGSOFT International Symposium on Foundations of
Software Engineering, pages 281‚Äì293. ACM, 2014.
[3] M. F. Aniche, G. Oliva, M. Gerosa, et al. What do the asserts
in a unit test tell us about code quality? a study on open
source and industrial projects. In Software Maintenance and
Reengineering (CSMR), 2013 17th European Conference on,
pages 111‚Äì120. IEEE, 2013.
[4] B. Baudry, Y . L. Traon, and J.-M. J√©z√©quel. Robustness and
diagnosability of oo systems designed by contracts. In
117Software Metrics Symposium, 2001. METRICS 2001.
Proceedings. Seventh International, pages 272‚Äì284. IEEE,
2001.
[5] A. Begel and T. Zimmermann. Analyze this! 145 questions
for data scientists in software engineering. In Proceedings of
the 36th International Conference on Software Engineering,
pages 12‚Äì23. ACM, 2014.
[6] C. Bird, N. Nagappan, P. Devanbu, H. Gall, and B. Murphy.
Does distributed development affect software quality?: an
empirical case study of windows vista. Communications of
the ACM, 52(8):85‚Äì93, 2009.
[7] C. Bird, N. Nagappan, B. Murphy, H. Gall, and P. Devanbu.
Don‚Äôt touch my code!: examining the effects of ownership
on software quality. In Proceedings of the 19th ACM
SIGSOFT symposium and the 13th European conference on
Foundations of software engineering, pages 4‚Äì14. ACM,
2011.
[8] C. Boogerd and L. Moonen. Evaluating the relation between
coding standard violations and faultswithin and across
software versions. In Mining Software Repositories, 2009.
MSR‚Äô09. 6th IEEE International Working Conference on,
pages 41‚Äì50. IEEE, 2009.
[9] J. J. Brown and P. H. Reingen. Social ties and word-of-mouth
referral behavior. Journal of Consumer research, pages
350‚Äì362, 1987.
[10] A. Burke, F. Heuer, and D. Reisberg. Remembering
emotional events. Memory & cognition, 20(3):277‚Äì290,
1992.
[11] K. Chaloner and I. Verdinelli. Bayesian experimental design:
A review. Statistical Science, pages 273‚Äì304, 1995.
[12] F. Davidoff, B. Haynes, D. Sackett, and R. Smith. Evidence
based medicine. BMJ: British Medical Journal,
310(6987):1085, 1995.
[13] T. Dyb√§, B. A. Kitchenham, and M. Jorgensen.
Evidence-based software engineering for practitioners.
Software, IEEE, 22(1):58‚Äì65, 2005.
[14] K. El Emam, S. Benlarbi, N. Goel, and S. N. Rai. The
confounding effect of class size on the validity of
object-oriented metrics. Software Engineering, IEEE
Transactions on, 27(7):630‚Äì650, 2001.
[15] S. Hanenberg. An experiment about static and dynamic type
systems: Doubts about the positive impact of static type
systems on development time. In ACM Sigplan Notices,
volume 45, pages 22‚Äì35. ACM, 2010.
[16] B. C. Hardgrave, F. D. Davis, and C. K. Riemenschneider.
Investigating determinants of software developers‚Äô intentions
to follow methodologies. Journal of Management
Information Systems, 20(1):123‚Äì151, 2003.
[17] V . J. Hellendoorn, P. T. Devanbu, and A. Bacchelli. Will they
like this? evaluating code contributions with language
models. In Mining Software Repositories (MSR), 2015
IEEE/ACM 12th Working Conference on, 2015.
[18] J. D. Herbsleb and A. Mockus. An empirical study of speed
and communication in globally distributed software
development. Software Engineering, IEEE Transactions on,
29(6):481‚Äì494, 2003.
[19] G. Hertel, S. Niedner, and S. Herrmann. Motivation of
software developers in open source projects: an
internet-based survey of contributors to the linux kernel.
Research policy, 32(7):1159‚Äì1177, 2003.
[20] J. P. Ioannidis. Effect of the statistical signiÔ¨Åcance of resultson the time to completion and publication of randomized
efÔ¨Åcacy trials. Jama, 279(4):281‚Äì286, 1998.
[21] J. P. Ioannidis. Why most published research Ô¨Åndings are
false. Chance, 18(4):40‚Äì47, 2005.
[22] D. S. Janzen. Software architecture improvement through
test-driven development. In Companion to the 20th annual
ACM SIGPLAN conference on Object-oriented
programming, systems, languages, and applications, pages
240‚Äì241. ACM, 2005.
[23] M. J√∏rgensen and E. Papatheocharous. Believing is seeing:
ConÔ¨Årmation bias studies in software engineering. In
Software Engineering and Advanced Applications (SEAA),
2015 41st Euromicro Conference on, pages 92‚Äì95. IEEE,
2015.
[24] S. Keele. Guidelines for performing systematic literature
reviews in software engineering. In Technical report, Ver. 2.3
EBSE Technical Report. EBSE. 2007.
[25] C. F. Kemerer. Software complexity and software
maintenance: A survey of empirical research. Annals of
Software Engineering, 1(1):1‚Äì22, 1995.
[26] B. Kitchenham, O. P. Brereton, D. Budgen, M. Turner,
J. Bailey, and S. Linkman. Systematic literature reviews in
software engineering‚Äìa systematic literature review.
Information and software technology, 51(1):7‚Äì15, 2009.
[27] B. A. Kitchenham, T. Dyba, and M. Jorgensen.
Evidence-based software engineering. In Proceedings of the
26th international conference on software engineering,
pages 273‚Äì281. IEEE Computer Society, 2004.
[28] A. J. Ko, R. DeLine, and G. Venolia. Information needs in
collocated software development teams. In Proceedings of
the 29th international conference on Software Engineering,
pages 344‚Äì353. IEEE Computer Society, 2007.
[29] E. Kocaguneli, T. Zimmermann, C. Bird, N. Nagappan, and
T. Menzies. Distributed development considered harmful? In
Software Engineering (ICSE), 2013 35th International
Conference on, pages 882‚Äì890. IEEE, 2013.
[30] T. D. LaToza, G. Venolia, and R. DeLine. Maintaining
mental models: a study of developer work habits. In
Proceedings of the 28th international conference on
Software engineering, pages 492‚Äì501. ACM, 2006.
[31] D. Lo, N. Nagappan, and T. Zimmermann. How practitioners
perceive the relevance of software engineering research. In
ESEC-FSE, 2015.
[32] A. Meneely and L. Williams. Secure open source
collaboration: an empirical study of linus‚Äô law. In
Proceedings of the 16th ACM conference on Computer and
communications security, pages 453‚Äì462. ACM, 2009.
[33] A. Mockus and L. G. V otta. Identifying reasons for software
changes using historic databases. In Software Maintenance,
2000. Proceedings. International Conference on, pages
120‚Äì130. IEEE, 2000.
[34] A. Mockus and D. M. Weiss. Predicting risk of software
changes. Bell Labs Technical Journal, 5(2):169‚Äì180, 2000.
[35] N. Nagappan and T. Ball. Use of relative code churn
measures to predict system defect density. In Software
Engineering, 2005. ICSE 2005. Proceedings. 27th
International Conference on, pages 284‚Äì292. IEEE, 2005.
[36] R. S. Nickerson. ConÔ¨Årmation bias: A ubiquitous
phenomenon in many guises. Review of general psychology,
2(2):175, 1998.
[37] C. Passos, D. S. Cruzes, A. Hayne, and M. Mendonca.
118Recommendations to the adoption of new software practices:
A case study of team intention and behavior in three software
companies. In Empirical Software Engineering and
Measurement, 2013 ACM/IEEE International Symposium on,
pages 313‚Äì322. IEEE, 2013.
[38] A. Porter, H. P. Siy, C. Toman, L. G. V otta, et al. An
experiment to assess the cost-beneÔ¨Åts of code inspections in
large scale software development. Software Engineering,
IEEE Transactions on, 23(6):329‚Äì346, 1997.
[39] F. Rahman and P. Devanbu. Ownership, experience and
defects: a Ô¨Åne-grained study of authorship. In Proceedings of
the 33rd International Conference on Software Engineering,
pages 491‚Äì500. ACM, 2011.
[40] F. Rahman and P. Devanbu. How, and why, process metrics
are better. In Proceedings of the 2013 International
Conference on Software Engineering, pages 432‚Äì441. IEEE
Press, 2013.
[41] F. Rahman, S. Khatri, E. T. Barr, and P. Devanbu. Comparing
static bug Ô¨Ånders and statistical prediction. In Proceedings of
the 36th International Conference on Software Engineering,
pages 424‚Äì434. ACM, 2014.
[42] F. Rahman, D. Posnett, and P. Devanbu. Recalling the
imprecision of cross-project defect prediction. In
Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering,
page 61. ACM, 2012.
[43] A. Rainer, T. Hall, and N. Baddoo. Persuading developers to
‚Äúbuy into" software process improvement: a local opinion
and empirical evidence. In Empirical Software Engineering,
2003. ISESE 2003. Proceedings. 2003 International
Symposium on, pages 326‚Äì335. IEEE, 2003.
[44] N. Ramasubbu and R. K. Balan. Globally distributed
software development project performance: an empirical
analysis. In Proceedings of the the 6th joint meeting of the
European software engineering conference and the ACM
SIGSOFT symposium on The foundations of software
engineering, pages 125‚Äì134. ACM, 2007.
[45] N. Ramasubbu, M. Cataldo, R. K. Balan, and J. D. Herbsleb.
ConÔ¨Åguring global software teams: a multi-company
analysis of project productivity, quality, and proÔ¨Åts. In ICSE,
pages 261‚Äì270. ACM, 2011.
[46] B. Ray, D. Posnett, V . Filkov, and P. Devanbu. A large scale
study of programming languages and code quality in github.
InProceedings of the 22nd ACM SIGSOFT International
Symposium on Foundations of Software Engineering, pages
155‚Äì165. ACM, 2014.
[47] T. L. Roberts Jr, M. L. Gibson, K. T. Fields, and R. K.
Rainer Jr. Factors that impact implementing a system
development methodology. Software Engineering, IEEE
Transactions on, 24(8):640‚Äì649, 1998.[48] E. Shihab, A. E. Hassan, B. Adams, and Z. M. Jiang. An
industrial study on the risk of software changes. In
Proceedings of the ACM SIGSOFT 20th International
Symposium on the Foundations of Software Engineering,
page 62. ACM, 2012.
[49] S. E. Sim, C. L. Clarke, and R. C. Holt. Archetypal source
code searches: A survey of software developers and
maintainers. In Program Comprehension, 1998. IWPC‚Äô98.
Proceedings., 6th International Workshop on, pages
180‚Äì187. IEEE, 1998.
[50] J. ¬¥Sliwerski, T. Zimmermann, and A. Zeller. When do
changes induce Ô¨Åxes? In ACM sigsoft software engineering
notes, volume 30, pages 1‚Äì5. ACM, 2005.
[51] F. Sultan and L. Chan. The adoption of new technology: the
case of object-oriented computing in software companies.
Engineering Management, IEEE Transactions on,
47(1):106‚Äì126, 2000.
[52] T. Tenny. Program readability: Procedures versus comments.
Software Engineering, IEEE Transactions on,
14(9):1271‚Äì1279, 1988.
[53] F. Thung, D. Lo, L. Jiang, F. Rahman, P. T. Devanbu, et al. To
what extent could we detect Ô¨Åeld defects? an empirical study
of false negatives in static bug Ô¨Ånding tools. In Proceedings
of the 27th IEEE/ACM International Conference on
Automated Software Engineering, pages 50‚Äì59. ACM, 2012.
[54] J. Utts. What educated citizens should know about statistics
and probability. The American Statistician, 57(2):74‚Äì79,
2003.
[55] S. Wacholder, S. Chanock, M. Garcia-Closas, N. Rothman,
et al. Assessing the probability that a positive report is false:
an approach for molecular epidemiology studies. Journal of
the National Cancer Institute, 96(6):434‚Äì442, 2004.
[56] S. Wagner, J. J√ºrjens, C. Koller, and P. Trischberger.
Comparing bug Ô¨Ånding tools with reviews and tests. Lecture
Notes in Computer Science, 3502:40‚Äì55, 2005.
[57] E. J. Weyuker, T. J. Ostrand, and R. M. Bell. Do too many
cooks spoil the broth? using the number of developers to
enhance defect prediction models. Empirical Software
Engineering, 13(5):539‚Äì559, 2008.
[58] J. Zheng, L. Williams, N. Nagappan, W. Snipes, J. P.
Hudepohl, M. V ouk, et al. On the value of static analysis for
fault detection in software. Software Engineering, IEEE
Transactions on, 32(4):240‚Äì253, 2006.
[59] T. Zimmermann, N. Nagappan, H. Gall, E. Giger, and
B. Murphy. Cross-project defect prediction: a large scale
experiment on data vs. domain vs. process. In Proceedings of
the the 7th joint meeting of the European software
engineering conference and the ACM SIGSOFT symposium
on The foundations of software engineering, pages 91‚Äì100.
ACM, 2009.
119