Learning from Examples to Find Fully QualiÔ¨Åed
Names of API Elements in Code Snippets
C M Khaled Saifullah Muhammad Asaduzzaman‚Ä† Chanchal K. Roy
Department of Computer Science, University of Saskatchewan, Canada
‚Ä†School of Computing, Queen's University, Canada
{khaled.saifullah, chanchal.roy }@usask.ca ‚Ä†muhammad.asaduzzaman@queensu.ca
Abstract ‚ÄîDevelopers often reuse code snippets from online
forums, such as Stack OverÔ¨Çow, to learn API usages of softwareframeworks or libraries. These code snippets often contain am-
biguous undeclared external references. Such external references
make it difÔ¨Åcult to learn and use those APIs correctly. Inparticular, reusing code snippets containing such ambiguousundeclared external references requires signiÔ¨Åcant manual effortsand expertise to resolve them. Manually resolving fully qualiÔ¨Åednames (FQN) of API elements is a non-trivial task. In this paper,
we propose a novel context-sensitive technique, called COSTER,
to resolve FQNs of API elements in such code snippets. The pro-posed technique collects locally speciÔ¨Åc source code elements aswell as globally related tokens as the context of FQNs, calculateslikelihood scores, and builds an occurrence likelihood dictionary(OLD). Given an API element as a query, COSTER captures thecontext of the query API element, matches that with the FQNs of
API elements stored in the OLD, and rank those matched FQNs
leveraging three different scores: likelihood, context similarity,and name similarity scores. Evaluation with more than 600Kcode examples collected from GitHub and two different StackOverÔ¨Çow datasets shows that our proposed technique improvesprecision by 4-6% and recall by 3-22% compared to state-of-
the-art techniques. The proposed technique signiÔ¨Åcantly reduces
the training time compared to the StatType, a state-of-the-arttechnique, without sacriÔ¨Åcing accuracy. Extensive analyses onresults demonstrate the robustness of the proposed technique.
Index T erms ‚ÄîAPI usages, Context sensitive technique, Rec-
ommendation system, Fully QualiÔ¨Åed Name
I. I NTRODUCTION
Developers extensively reuse Application Programming In-
terfaces (APIs) of software frameworks and libraries to save
both development time and effort. This requires learning newAPIs during software development. However, inadequate andoutdated documentation of APIs hinder the learning process[1], [2]. As a result, developers favor code examples over
documentation [3]. To understand APIs with code examples,
developers explore online forums, such as Stack OverÔ¨Çow(SO)
1, GitHub Issues, GitHub Gists2and so on. These online
forums provide a good amount of resources regarding APIusages [4]. However, such usage examples can suffer fromexternal reference and declaration ambiguities when one at-tempts to compile them [2], [5]. External reference ambiguityoccurs due to missing external references, whereas declaration
ambiguity is caused by missing declaration statements. As a
result of these ambiguities, code snippets from online forums
1https://stackoverÔ¨Çow.com
2https://gist.github.com/discoverare difÔ¨Åcult to compile and run. According to Horton andParnin [6] only 1% of the Java and C# code examples includedin the Stack OverÔ¨Çow posts are compilable. Yang et al. [7]also report that less than 25% of Python code snippets inGitHub Gist are runnable. Resolving FQNs of API elements
can help to identify missing external references or declaration
statements.
Prior studies link API elements in forum discussions to their
documentation using Partial Program Analysis (PPA) [8], textanalysis [2], [9], and iterative deductive analysis [5]. All thesetechniques except Baker [5], need adequate documentation or
discussion in online forums. However, 47% of the APIs donot have any documentation [10] and such APIs cannot beresolved by those techniques. Baker [5] depends on scope rules
and relationship analysis to deduce FQNs of API elements.However, the technique fails to leverage the code context andcannot infer 15-31% of code snippets due to inadequate infor-
mation within the scope [11]. Recently, Statistical Machine
Translation (SMT) is used to determine FQNs of APIs inStatType [11]. However, the technique requires a large numberof code examples to train and it performs poorly for APIshaving fewer examples. The training time of StatType is also
considerably higher than other techniques.
In this paper, we propose a context-sensitive type solver,
called COSTER. The proposed technique collects locally spe-ciÔ¨Åc source code elements as well as globally related tokensas the context of FQNs of API elements. We calculate thelikelihood of appearing context tokens and the FQN of eachAPI element. The collected usage contexts and likelihoodscores are indexed based on the FQNs of API elements inthe occurrence likelihood dictionary (OLD). Given an APIelement as a query, COSTER Ô¨Årst collects the context of thequery API element. It then matches the query context with
that of the FQNs of API elements stored in the OLD, and then
rank those FQNs leveraging three different scores: likelihood,context similarity, and name similarity scores.
We compare COSTER against two state-of-the-art tech-
niques for resolving FQNs of API elements, called Baker [5]and StatType [11], using more than 600K code snippets from
GitHub [12] and two different Stack OverÔ¨Çow (SO) datasets.
We not only reuse the SO dataset prepared by Phan et al.[11] but also build another dataset of 500 SO posts. Resultsfrom our evaluation show that COSTER improves precisionby 4-6% and recall by 3-22% compared to state-of-the-art
UI*&&&"$.*OUFSOBUJPOBM$POGFSFODFPO"VUPNBUFE4PGUXB SF&OHJOFFSJOH	"4&
¬•*&&&
%0*"4&
Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. techniques. COSTER also needs ten times less training time
and one third less memory than StatType that is consideredas a state-of-the-art technique for this problem. We alsoinvestigate why the proposed technique outperforms othersthrough extensive analyses on i) sensitivity, ii) number oflibraries, iii) API popularity, iv) receiver expression types, andv) multiple mapping cardinality. Thus, the contributions of thispaper are as follows:
1) A technique that leverages a context-sensitive approach
to resolve the FQN of an API element.
2) Evaluation of the proposed technique against two state-
of-the-art techniques.
3) Extensive analyses on the results of all competing tech-
niques to answer why the proposed technique outper-forms others.
The rest of the paper is organized as follows. Section II
presents a motivating example and explains the challenges inresolving FQNs of API elements. We describe our proposedtechnique in Section III. Section IV introduces datasets and ex-plains the evaluation procedure. Section V evaluates COSTERagainst two other state-of-the-art techniques and Section VIprovides further insights on the performance of our proposedtechnique. We discuss threats to the validity of our work insection VII and Section VIII presents prior studies related toour work. Finally, Section IX concludes the paper.
II. M
OTIV ATING EXAMPLE
Let us consider a code snippet collected from a SO post3
as shown in Fig. 1. The post describes a situation where a
developer wants to use the Element and Document classes,
but (s)he does not know which libraries or APIs need to beimported.
1private void writeFile(){
2 dFact = DocumentBuilderFactory .newInstance();
3 build = dFact.newDocumentBuilder();
4 doc = build.newDocument();
5
6 Element root = doc.createElement( "outPutResult" );
7 doc.appendChild(root);
8
9 for(Result r:resultList){
10 Element title = doc.createElement( "Title" );
11 title.appendChild(doc.createTextNode(r.getTitle));
12 root.appendChild(title);
13
14 Element add = doc.createElement( "Address" );
15 add.appendChild(doc.createTextNode(r.getAddress));
16 root.appendChild(address);
17 }
18}//End of Write function
Fig. 1. A Stack OverÔ¨Çow post3regarding how to use the Element class
What are the challenges in resolving the FQNs of this
code snippet? First, the code in online forums is not alwayscompilable or runnable. For example, in Fig. 1, the codesnippet is incomplete, having not been enclosed by a class.Thus, we cannot compile or run the code directly as morechanges are required.
3https://stackoverÔ¨Çow.com/questions/20157996/Second, the code snippets often contain identiÔ¨Åers without
declarations. In Fig. 1, identiÔ¨Åers dFact ,build , and doc at
line 2, 3, and 4, respectively are not declared within thecode snippet. While completing the declaration statements ofthese identiÔ¨Åers, declaration ambiguity will occur because ofmissing type information.
Third, API elements used in a code snippet require spe-
ciÔ¨Åc external references. For example, the classes Document-
BuilderFactory ,Result and Element at lines 2, 6 and 9 require
external references.
Last but not least, API elements can have name ambiguity.
For example, there are Ô¨Åve Element classes in JDK 8
4and it
is not clear which Element class we should import to compilethe code.
To tackle the above challenges, existing techniques either
use rules or heuristics [2], [5], [13], or statistical machinetranslation [11]. Rule-based systems (such as RecoDoc [2] andACE [13]) search documentation or discussion to resolve thetypes. However, they have three limitations: documentationis rarely available [10], discussions are usually informal [5]and using Partial Program Analysis [8] results in partiallyqualiÔ¨Åed names [11]. Baker [5] resolves a type by deducingthe candidate FQNs based on the tokens within the scope ofthat type. The declaration of an API element can be locatedoutside the current scope and Baker fails to resolve the FQN ofthat API element. For example, the undeclared variables build
and dFact at lines 2 and 3 caused insufÔ¨Åcient information for
Baker [5]. Moreover, increasing the number of libraries alsoincreases the likelihood of mapping the same token name tomultiple APIs with a similar name in the oracle. That creates
name ambiguities and Baker has too little information to tackle
such ambiguities. To overcome the limitations of rule-basedsystems, StatType [11] used locally speciÔ¨Åc resolved codeelements to Ô¨Ånd the regularity of co-occurring tokens. How-ever, StatType requires a large number of training examplesto perform well. Moreover, the technique also requires a long
training time. These motivate us to investigate the problem
further.
Key Idea:
Instead of relying only on the locally speciÔ¨Åc code elements
(i.e., local context), COSTER also considers globally relatedtoken (i.e., global context) of an API element. Such combina-
tion is found effective in other research areas [14]‚Äì[16]. The
deÔ¨Ånitions of the local and global contexts are as follows:
DeÔ¨Ånition I Local Context : The local context of an API
element consists of method calls, type names, Java keywords
and operators that appear within the top four and bottom four
lines including the line in which the API element appears. Forexample, local context of root.appendChild(title) at line 12 of
Fig. 1 is{for,Result ,Element ,=,createElement ,appendChild ,
createTextNode , getTitle , Element , =, createElement ,
appendChild ,createTextNode ,getAddress ,appendChild }.
4https://docs.oracle.com/javase/8/

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. DeÔ¨Ånition II Global Context : The global context of an
API element consists of any methods that are called on thereceiver variable, or use either the receiver variable or theAPI element as a method parameter, and located outside thetop and bottom four lines of that API element. The globalcontext of root.appendChild(title) at line 12 of Fig. 1 is
{appenChild }since the appendChild method at line 7 uses
the receiver variable root of the API element as a parameter.
Local context captures the naturalness [17] and local-
ness [18] properties of the code. On the other hand, global
context tries to capture the long term dependency of the APIelement. The motivation behind choosing global context ismainly because they enrich the context of an API elementby adding the tokens that are related to the element but do notclosely located. For example, in Fig. 1, the local context of
doc,root,title and add have the method name appendChild .
Therefore, co-occurrence based on local context will suggestthat all four are the object of the Element class. However,
when we add the global context, doc will have other methods,
such as createTextNode , and the other three will have the
appendChild only. Thus, the global context differentiates doc
from the other three and helps to infer more accurate FQN.
III. P
ROPOSED TECHNIQUE
This section describes our proposed technique for Ô¨Ånding
FQNs of API elements, called COSTER ( Context Sensitive
Type Solv er). Fig. 2 shows an overview of the proposed tech-
nique. Our example-based context-sensitive technique works
in two steps as follows:
‚Ä¢Build Occurrence Likelihood Dictionary (OLD) .W e
collect two different forms of contexts: local context as
per DeÔ¨Ånition I and global context as per DeÔ¨Ånition II(see Section II, Key Idea) for each API element; i.e.,a method call, a Ô¨Åeld call or a type variable. Next,we combine them based on the position in the sourcecode to form the usage context. Finally, we calculate the
likelihood of appearing usages context tokens and the
FQN of each API element. Collected usage contexts andlikelihood scores are indexed based on the FQNs of APIelements in the OLD.
‚Ä¢Infer FQN of an API element . This involves searching
for any FQN in OLD whose usage context matches with
that of the target API element. COSTER collects onlythose FQNs whose usage contexts share a minimum num-ber of tokens with the target API element. We called thisthe candidate list. Next, we synthesize FQNs from thecandidate list leveraging a) likelihood scores of contexts
in the candidate list, b) cosine similarity score between
the usage contexts in the candidate list and the usagecontext of the target API element, and c) name similarityscore between the candidate FQNs and the name of thetarget API element using the Levenshtein distance. Acombined similarity score is calculated and the technique
sorts FQNs in the candidate list in descending order ofOccurance
Likelihood
DictonaryCollect API elements with
local and global contexts and
FQNsCalculate likelihood
score of FQNs given
usage context's tokensCodebase
A Query
API ElementCollect local and
global contextsFetch the
Candidate
ListSort list using
likelihood, context
similarity, and name
simialrity scoresRecommend
FQNsBuilding Occurrence Likelihood Dictionary
Infer FQN of an API element
Fig. 2. Overview of COSTER‚Äôs entire process of building OLD and
recommending FQN of a query API element
their combined similarity score. We recommend the top-k
FQNs after removing any duplicates.
We describe each of these steps in detail as follows.
A. Building Occurrence Likelihood Dictionary (OLD)
In this step, we build a dictionary of usage context of
API elements that will be used to infer the FQN of queryAPI element. To do that, COSTER uses Eclipse JDT
5to
parse source code examples and collects usage context of
API elements (e.g., method calls, class names, and Ô¨Åeld calls)
including their FQNs. The usage context of an API elementconsists of two different contexts: local and global contextsdeÔ¨Åned previously.
The FQN of each API element and the corresponding usage
context together constitute a transaction. We then calculate the
likelihood of appearing a context token and the FQN of thecorresponding API element leveraging the trigger pair conceptof Rosenfeld [19]. If a token tis signiÔ¨Åcantly correlated with
the FQN f
pof an API element p, thentcan be considered as
a trigger for fp. However, instead of using maximum entropy
as was used by Rosenfeld [19], we estimate the likelihood of
the FQN fpgiven the token tappeared in the usages context
by considering the ratio of transactions that contain both t
andfp(N(t,fp)) over the number of transactions that contain
thet(N(t)) as shown below. Thus, the ratio represents the
likelihood score ( ls(fp|t)) between a token and the FQN of
the corresponding API element.
ls(fp|t)=N(t,fp)+1
N(t)+1(1)
To include the distance into consideration between pandt
(i.e., the more pandtare closely located, the higher will be
the likelihood score between them), we update the likelihoodscore ( ls(p,t) ) calculation as follows:
ls(p,t)=ls(fp|t)√ówweight
distance (p,t)+k(2)
Here,wweight represents the weight of the token and kis a
small positive number. We set the value of kto 0.0001 for our
experiments to avoid division by zero. If the token is located in
the local context, we set the weight value to 1; otherwise, theweight value is set to 0.5. The distance between the token and
5https://www.eclipse.org/jdt/

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. TABLE I
EXAMPLE CODE SNIPPET WITH CONTEXT ,LIKELIHOOD ,CONTEXT SIMILARITY ,NAME SIMILARITY SCORES AND POSSIBLE FQN CANDIDATES
1private static int countFlips( String stack) {
2 Set<String > visited = new HashSet <>();
3 Queue <State > bfsQueue = new LinkedList <>();
4 visited.add(stack);
5 bfsQueue.add( new State(0, stack));
6 while (!bfsQueue.isEmpty() && !isSolved(bfsQueue.peek().pancakes)) {
7 State state = bfsQueue.poll();
8 for (int i = 1; i <= state.pancakes.length(); ++i) {
9 String flipped = flip(state.pancakes, i);
10 if(!visited.contains(flipped)) {
11 bfsQueue.add( new State (state.flips + 1, flipped));
12 visited.add(flipped);
13 }
14 }
15 }
16 return bfsQueue.poll().flips;
17}
API Element :bfsQueue.add
Local Context :{State, =, poll, for , int, =, <=, length, ++, String, =, if, !, contains, State, +, add }
Global Context :{add, isEmpty, peek, poll }
Combined Context :{add, isEmpty, peek, State, =, poll, for , int, =, <=, length, ++, String, =, if, !, contains, State, +, add, poll }
Candidate Candidate Context Likelihood Score Context Similarity Name Similarity Candidate FQN
c1 ..., add, isEmpty,..., peek,....., for , poll 0.51 0.47 0.33 java.util.Queue
c2 for , int, ..., =, String, ....., if, ... 0.31 0.27 0.00 java.lang.String
c3 ....., if, contains, ..., isEmpty,...., String 0.26 0.24 0.07 java.util.List
c4 poll,..., for , ..., peek, add,..., isEmpty 0.21 0.36 0.33 java.util.Queue
c5 .., add, ..., poll, ...., for , .., peek 0.17 0.21 0.10 java.util.LinkedList
... ... ... ... ... ...
the API element, referred to distance (p,t), is calculated by
considering the number of tokens between pandt. The closer
the token tto the API element p, the smaller would be the
distance. Given a set of tokens (i.e., T={t1,t2,t3,...,t n})
as the usages context, we calculate the likelihood score of theFQNf
pof the corresponding API element pby summing the
scores for all pairs of {p, t i}, as shown in Eq. 3.
log(ls(fp|T))/similarequallog(ls(p,T))
/similarequallog(ls(p,t1)) + log( ls(p,t2)) +....+ log(ls(p,t n))(3)
We note that to avoid the underÔ¨Çow, we use the logarithmic
form. The collected usage contexts and likelihood scores areindexed based on the FQNs of API elements in the OLD.
B. Inferring FQN of an API element
This section discusses the steps we follow to determine the
FQN of an API element.
1) Context Collection: Given an API element for which
FQN needs to be determined, COSTER collects both localand global contexts of the API element. Let us consider theAPI element bfsQueue.add as shown in Table I. We follow
the same approach as described in the previous subsection tocollect both local and global contexts of API elements. The
global context for the above-mentioned example consists of
the following four method calls: add,isEmpty ,peek , and poll.
Next, we combine tokens of local and global contexts to form acombined context that preserves the order of tokens. CombinedContext at Table I shows the context for our example. From
now on we refer to the combined context as the query context,API element of the query context as the query API element,
and the FQN of the query API element as the query FQN.
2) Candidate list generation: Our next step is to select
FQNs from OLD along with their contexts and likelihoodscores where each context matches with the query context.We select only those FQNs whose combined context sharesat least 25% of the tokens with that of the query context.The choice of the threshold value of 0.25 (25%) is made byrunning the inference step for different values and getting themost stable performance for 0.25. Our query context in Table Ihas 17 unique tokens in it. Therefore, if any contexts in theOLD has a minimum of 17√ó0.25‚âà4shared tokens, we
include that in the candidate list.
3) Context similarity calculation: We now have a list of
candidate contexts along with their FQNs, and we need to
calculate how similar are they to the query context. The goalof this step is to Ô¨Ånd similar contexts that not only contain
similar tokens but also those tokens that appear in the sameorder. Thus, we calculate the cosine similarity [20] score andmultiply that with the fraction of matched tokens that are inthe same order of query context to obtain the context similarity
score as follows:
Sim context (Tq,Tci)=Norder
Nmatched√óTq¬∑Tci
||Tq||||Tci||(4)
In Eq. 4,TqandTciare the numerical vector representations
of the set of tokens of the query context and each candidatecontext, respectively, N
order is the number of tokens in order
andNmatched is the number of tokens matched. In the case
of our example at Table I, the column Context similarity

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. shows the similarity score between the query context and each
candidate context.
4) Name similarity calculation: During our manual investi-
gation of FQNs of API elements, we observe that the names ofthe API elements that share similarity with the FQN are mostlikely the desired output. To leverage such similarity in our
ranking, we calculate the Levenshtein distance [21] between
the name of the query API element ( p
q) and the candidate
FQNs (fci). The distance simply calculates the number of
edits required to attain a particular FQN from the query APIelement. The smaller the number of required edits, the higherwould be the similarity between the name of the query APIelement and a candidate FQN. Thus, we calculate the namesimilarity score using the following equation:
Sim name (pq,fci)=/braceleftBigg
1‚àílev(pq,fci)
len(fci)if len>lev
0 otherwise(5)
In Eq. 5, lev(pq,fci)andlen(fci)refer to the Levenshtein
distance between the query API element and each candidateFQN, and the length of the candidate FQN, respectively. Col-
umn name similarity in Table I represents the name similarity
scores between the API element bfsQueue and the candidate
FQNs. We found that java.util.Queue has the highest name
similarity score having the number of edits required as 10.Therefore, the name similarity score becomes: (1‚àí
10
15)=0.33.
java.lang.String requires 16 edits which is the same as the
length of FQN. Thus, the name similarity score becomes zero.
5) Recommending top-k FQNs: The candidate FQNs are
sorted in descending order of the similarity scores calculatedusing Eq. 6.
candidate score (fci)=Œ±√óls(fci|Tci)
+Œ≤√óSim context (Tq,Tci)
+Œ≥√óSim name (pq,fci)(6)
Here,ls(fci|Tci))is the likelihood score of the candidate
FQNfcigiven the set of tokens in the candidate context Tci.
Moreover, Œ±,Œ≤, andŒ≥are the coefÔ¨Åcients of likelihood, con-
text similarity, and name similarity scores, respectively. Thevalues of these variables are determined using Hill ClimbingAdaptive Learning algorithm [22] over the training data. Forour example in Table I, the Ô¨Ånal score for java.util.Queue ,
java.lang.String ,java.util.List and java.util.LinkedList based
on Eq. 6 are 0.68, 0.38, 0.34 and 0.27, respectively. Ourtechnique recommends the top-k FQNs of API elements afterremoving any duplicates and the value of kcan be adjusted.
IV . E
V ALUATION
This section compares COSTER with two state-of-the-art
techniques, Baker [5] and StatType [11]. To evaluate COSTER,we answer the following three research questions:
‚Ä¢RQ1: Intrinsic Accuracy. How accurate is COSTER in
identifying FQNs of API elements in Java source code
snippets collected from Github dataset [12]?TABLE II
DATASET OVERVIEW
GitHub Dataset Stack OverÔ¨Çow Dataset
Info Number Info Number
No. of Projects 50,000 Noof Posts 500
No. Of Files 602,173 LOC 3,182
No. of Libraries 100 No. of Libraries 11
No. of Classes 19,259 No. of Classes 203
No. of Methods 99,473 No. of Methods 1,375
No. of Fields 21,739 No. of Fields 624
‚Ä¢RQ2: Extrinsic Accuracy. How accurate is COSTER in
identifying FQNs of API elements in Java code snippetscollected from Stack OverÔ¨Çow posts?
‚Ä¢RQ3: Timing and memory performance. Does
COSTER improve the timing and memory performancecompared with Baker and StatType?
All experiments were performed on a machine with an Intel
Xeon processor having a processing speed of 2.10 GHz, 16GB of memory, and running on Ubuntu 16.04 LTS operatingsystem.
A. Dataset Overview
We collected datasets from two different sources for evalu-
ating our technique and for comparing with the state-of-the-arttechniques. A brief overview of the datasets is shown in TableII.
GitHub Dataset: We consider a collection of 50K Java
projects collected from GitHub, called 50K-C projects [12].We use the term GitHub Dataset to refer to the dataset asshown in Table II. The dataset consists of 19K unique class-es/types, 99K unique methods, and 21K Ô¨Åelds. Our selection
of this dataset is based on the fact that all these projects are
compilable and include all required dependencies in the formof jars to resolve FQNs of all APIs. We select the top frequent100 libraries used by these projects. Then we use EclipseJDT
5to parse the source code and to resolve FQNs of all
API elements for those libraries.
Stack OverÔ¨Çow Datasets: We leverage two different Stack
OverÔ¨Çow (SO) datasets to conduct the extrinsic experiment.
First, we consider the SO dataset used in the study of Stat-Type [11]. We use the term StatType-SO to refer to this dataset.We also built another dataset by collecting code snippets fromSO posts considering eleven popular libraries, referred to as
COSTER-SO. Out of eleven libraries, ten are selected as the
top frequent libraries of GitHub dataset and the remaining oneis JDK8. We downloaded the latest SO data dump to collectcode snippets. For each selected library, we searched theclass, method and Ô¨Åeld names in the code snippet to identifylibrary posts. Similar to StatType [11], we collected code
snippets from both questions and answers. We then randomly
collected 500 code snippets with an equal number of codesnippets selected for each library of interest. Code snippets inSO often do not contain required import statements, variabledeclarations, class names or method bodies. To resolve FQNs,
we need to convert those code snippets to compilable Java

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. source Ô¨Åles by incorporating those missing information. Five
annotators, all are computer science graduate students, madethose code snippets to compilable code snippets by manuallyincorporating the missing information. The dataset consists ofAPI elements from 203 unique classes, 1,375 unique methods,and 624 unique Ô¨Åelds, as shown in table II.
B. Evaluation Procedure
In the case of intrinsic evaluation, we apply the 10-fold
cross-validation technique to measure the performance of each
technique for resolving FQNs of API elements. We dividethe dataset into ten different folds, each containing an equal
number of API elements. Nine of those folds are used to trainand the remaining fold is used to test the performance of thecompeting techniques. We use precision, recall, and F
1score
to measure the performance of each of the techniques. For eachAPI element in the test dataset, we present the code example
to each technique to recommend the FQN of the selected
API element. If the target FQN is within the top-k positionsin the list of recommendations, we consider it relevant. Theprecision, recall, and F
1are deÔ¨Åned as follows.
Precision =recommendations made ‚à©relevant
recommendations made(7)
Recall =recommendations made ‚à©relevant
recommendations requested(8)
F1=2¬∑Precision ¬∑Recall
Precision +Recall(9)
Here, recommendations requested is the number of API
elements in the test set. The recommendation made is the
number of cases the technique can recommend FQNs. We usetwo-tailed Wilcoxon signed-rank test [23] for our study. For
each evaluation metric (i.e., precision, recall and F
1score),
we collect the result of COSTER for each fold as one datapoint and compare ten data points we obtain from ten differentfolds with that of Baker and StatType. Since we are performing
two comparisons (i.e., comparing COSTER with StatType and
Baker), our result can be affected by the type I error innull hypothesis testing. To minimize the error, we restrict thefalse discovery rate (FDR) by adjusting the p-values usingBonferroni correction [24]. If adjusted p-values are less than
the signiÔ¨Åcance level then we reject the null hypothesis (i.e.,
statistically, the results of COSTER are signiÔ¨Åcantly differentthan Baker and StatType).
For the extrinsic evaluation, we evaluate the effectiveness of
all the competing techniques in recommending FQNs of APIelements in SO code snippets. We train each technique usingcode examples of GitHub dataset and then test the techniqueusing two different SO datasets. We then compute precision,recall and F
1scores for each dataset separately. For both of
the state-of-the-art techniques (i.e., Baker and StatType), weuse the settings used in their prior studies.
V. E
XPERIMENTAL RESULT AND ANALYSIS
This section presents the evaluation results and answers
research questions described in Section IV.A. RQ1: Intrinsic Accuracy. How accurate is COSTER in
identifying FQNs of API elements in Java source code snippetscollected from Github dataset [12]?
Table III shows the evaluation results for all three can-
didate techniques on the GitHub dataset. We determine theperformance of candidate techniques for top-1, top-3 and top-5
recommendations. Table III only shows the top-1 recommen-dation for Baker since the technique only recommends the bestmatch.
TABLE III
PRECISION (PREC.), R ECALL (REC.)AND F1SCORE (F1)OF ALL
COMPETING FOR GITHUB DATASET [12]
Techniques Recc. Prec. Rec. F1
BakerTop-1 83.63 68.19 75.12
Top-3 - - -
Top-5 - - -
StatT ypeTop-1 85.91 86.74 86.32
Top-3 89.34 90.76 90.04
Top-5 91.74 92.47 92.10
COSTERTop-1 89.48 90.04 89.76
Top-3 92.11 93.26 92.68
Top-5 95.43 95.84 94.63
Results from our evaluation show that Baker gives compar-
atively lower precision and recall. While the precision for thetop-1 recommendation is 83.63%, the recall drops to 68.19%.Compared with Baker, StatType improves both precision andrecall by 2.28% and 18.55%, respectively. Among the threecompared techniques, COSTER obtains the best precisionand recall. While the precision is 89.48%, the recall reachesto 90.04% for the top-1 recommendation. Thus, COSTERachieves 5.85% higher precision and 21.85% higher recall incomparison with Baker and 3.57% higher precision and 3.30%
higher recall than StatType. These indicate the effectiveness
of the context that COSTER collects to capture the usages ofFQNs of API elements. Performance improves as we increasethe number of recommendations. For example, the precisionand recall for the top-5 recommendations are 95.43% and95.84%, respectively for COSTER. Statistically, the precision,
recall and F
1scores of COSTER are signiÔ¨Åcantly different
than the compared techniques for top-1, top-3, and top-5recommendations.
B. RQ2: Extrinsic Accuracy. How accurate is COSTER in
identifying FQNs of API elements in Java code snippetscollected from Stack OverÔ¨Çow posts?
Table IV shows the evaluation results for the StatType-SO
dataset considering the topmost recommendation. The dataset
consists of API elements from six different libraries. SinceStatType performed better than Baker for this dataset in theirexperiment [11], we only report results for StatType andCOSTER.
Interestingly, as we see from Table IV for the Hibernate
library, COSTER obtains 3.9% and 8.9% higher precision andrecall compared to that of StatType. For the remaining Ô¨Åvelibraries, the differences between the evaluation results of Stat-
Type and COSTER are very small. StatType has marginally

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. TABLE IV
PRECISION (PREC.), R ECALL (REC.)AND F1SCORE (F1)COMPARISON
BETWEEN STATTYPE AND COSTER FOR STATTYPE-SO
LibrariesTotal
APIsStatT ype COSTER
Prec. Rec. F1 Prec. Rec. F1
Android 1,022 98.7 97.9 98.3 98.4 98.1 98.2
Joda Time 652 98.3 98.0 98.1 97.6 98.4 98.0
XStream 463 99.8 99.6 99.7 99.3 99.6 99.4
GWT 1,243 96.6 95.9 96.2 97.1 96.5 96.8
Hibernate 840 89.8 86.3 88.0 93.7 95.2 94.4
JDK 2,934 98.9 99.1 99.0 97.6 98.8 98.2
better precision for four libraries but COSTER obtains a
slightly better recall. While the Precision of COSTER rangesbetween 93.7% and 98.4%, the recall ranges between 95.2%and 99.6%.
TABLE V
PRECISION (PREC.), R ECALL (REC.)AND F1SCORE (F1)COMPARISON
BETWEEN ALL COMPETING TECHNIQUES FOR COSTER-SO DATASET
Techniques Recc. Prec Rec. F1
BakerTop-1 87.34 75.92 81.23
Top-3 - - -
Top-5 - - -
StatT ypeTop-1 90.65 91.66 91.15
Top-3 93.76 94.86 94.31
Top-5 95.73 97.05 96.39
COSTERTop-1 92.17 93.27 92.72
Top-3 96.65 97.09 96.87
Top-5 98.27 98.95 98.61
Next, we compare all three techniques for COSTER-SO
dataset and the results are shown in Table V. Similar to the
intrinsic experiment, Baker recommends only top-1 with com-paratively poor performance. The technique obtains 87.34%and 75.92% precision and recall, respectively. StatType obtains3.31% and 15.74% higher precision and recall compared tothat of Baker. COSTER outperforms both Baker and StatType
for top-1 recommendation by obtaining 92.17% precision and
93.27% recall. For top-3 and 5 recommendations, COSTERachieves 1-3% more precision and recall than StatType. Sim-ilar to intrinsic experiment, statistical test after adjusting p-values shows that the results of COSTER are signiÔ¨Åcantly
different than Baker and StatType.
C. RQ3: Timing and memory performance. Does COSTER
improve the timing and memory performance compared withBaker and StatType?
This section compares the time and memory performances
that include training and testing times, and the sizes ofvocabulary, language model and dictionary. The sum of timesrequired to parse source code to identify API elements and todetermine their FQNs is reported as the code extraction time inTable VI. Training time includes the creation of OLD, trainingany machine learning model and so on. Inference time refersto the time needed to detect the FQN of a query API element.
V ocabulary, language model, and dictionary sizes refer to the
number of words in the vocabulary, size of the language model(if any), and the size of the dictionary (if any), respectively.
To have a fair comparison, all these techniques were run onthe same machine for GitHub dataset.
TABLE VI
TIMING AND MEMORY PERFORMANCE FOR ALL THREE COMPETING
TECHNIQUES
Bak er StatT ype COSTER
Code Extraction Time (hrs) 7.9 9.1 8.2
Training Time (hrs) - 109 11
Inference Time (ms) 6.2 4.3 5.2
Vocabulary Size (n words) 1.7M 7.9M 2.8M
Language Model Size (GB) - 6.9 -
Dictionary Size (GB) 1.63 - 2.3
Baker requires the least amount of time for parsing source
code whereas COSTER takes 30 more minutes to collect theusage context of all API elements in the Github dataset. Stat-Type, on the other hand, requires more time, possibly because
of generating source and target languages, and to check the
alignment between them. Baker does not require any trainingtime since it simply stores the APIs in the dictionary withoutcalculating any scores. COSTER calculates the likelihood ofthe FQN of each API element given usage context tokens
in the training code examples (i.e., likelihood scores) and
builds the OLD. It takes around 11 hours to complete theseoperations. StatType requires signiÔ¨Åcantly higher training time.It takes more than 100 hours to train. One can argue thattraining is a one-time operation. However, we would like topoint to the fact that supporting a new library would require
training the technique. Such a long training time can increase
the cost signiÔ¨Åcantly if a user leverages any web services formodel training. For example, on Amazon EC2
6, StatType will
cost more than 200 USD to train the technique only oncewhereas COSTER will cost between 18-20 USD. In the caseof inference, COSTER requires 0.9 milliseconds more than
StatType to determine FQN of a query API element. The
difference is negligible and can be ignored.
Baker has the least memory requirement, having 1.7 million
tokens in the vocabulary that requires 1.63 gigabytes of mem-ory. Having just 0.9 million more tokens and 700 megabytes
more memory, COSTER performs signiÔ¨Åcantly better than
Baker. StatType requires about three times the number oftokens and memory required by COSTER. In short, the resultsin Table VI show that our proposed technique is capable toexhibit the best performance (reported in Table III), requiringone-tenth training time and one-third memory than StatType.
Thus, our proposed technique can be considered as efÔ¨Åcient,
not only in terms of accuracy but also in terms of timing andmemory requirements.
VI. D
ISCUSSION
The evaluation results in the previous section provide a
ranking of competing techniques in terms of their performance.However, it does not answer why COSTER performs betterthan other techniques. We hypothesize that this is because of
6https://aws.amazon.com/ec2/pricing/on-demand/

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. COSTER‚Äôs ability to capture the fuller context of the FQNs of
API elements. To provide further insights into this hypothesis,we conduct a set of studies and present their results in thissection. All the experiments and analyses in this section areperformed on GitHub Dataset.
A. Sensitivity Analysis: Impact of decision
This section validates our design decisions for building the
model. Our local context consists of the top and the bottom
four lines, including the line in which the API element is
located. We select four lines because we observe that theprecision becomes steady after considering more than fourlines whereas the execution time increases exponentially. Weconduct a set of studies to understand how the selection oftokens, different contexts, and similarity scores affect the
performance of the technique. Our initial context C
0contains
tokens from the top four lines only. Next, we add the tokensof the bottom four lines with C
0to create the context C1
(i.e., local context). To understand the importance of using
the global context, Ô¨Årst, we incorporate those methods of
the global context that are called on the receiver variable
to create C2, and then add the methods that use either the
receiver variable or the API element as a method parameterto create C
3. Therefore, the context-wise categories are:
C0: Context containing tokens from the top four lines.
C1:C0+ Tokens from the bottom four lines.
C2:C1+ Methods of global context that are called on the
receiver variable.
C3:C2+ Methods of global context that use either the
receiver variable or the API element as a method parameter.
COSTER considers three different similarity scores: likeli-
hood score, context similarity score, and name similarity score.
To understand the effect of those similarity scores, we trainand test COSTER using different context settings (i.e., C
0,C1,
C2, andC3) using only the likelihood score. Next, we train
and test COSTER by including the context similarity scoreand the name similarity score, one at a time. We record theprecision, recall, and F
1score after each run, as shown in
Table VII.
Considering only top four lines of the local context, pre-
cision and recall values reach to 45.72% and 46.27% for the
top-1 recommendation. Adding the bottom four lines of the
local context also helps to improve the result, precision andrecall values are increased by 5.95% and 1.94%, respectively.We also observe that the inclusion of the global context alsohas a positive impact on the performance. The precision and
recall values reach to 71.38% and 72.94%, respectively for
the top-1 recommendation. Context similarity score plays amore important role than the name similarity score. Adding thecontext similarity score increases precision and recall values to85.67% and 86.19%, respectively. Finally, when we considerall the contexts and similarity scores we obtain the best result.
The precision and recall values reach to 89.48% and 90.04%
for the top-1 recommendation. We also observe similar effectswhen we consider top-3 and top-5 recommendations.TABLE VII
PRECISION (PREC.), R ECALL (REC.)AND F1SCORE (F1)OFCOSTER
FOR CONSIDERING DIFFERENT CONTEXTS AND SIMILARITY SCORES
Models Description Recc. Prec. Rec. F1
M0C0+
Likelihood ScoreTop-1 45.72 46.27 45.99
Top-3 52.94 51.73 52.33
Top-5 54.28 53.61 53.94
M1C1+
Likelihood ScoreTop-1 51.67 48.21 49.88
Top-3 54.34 53.71 54.02
Top-5 55.07 54.83 54.95
M2C2+
Likelihood ScoreTop-1 62.76 65.17 63.94
Top-3 71.83 74.33 73.06
Top-5 75.28 77.92 76.58
M3C3+
Likelihood ScoreTop-1 71.38 72.94 72.15
Top-3 79.17 82.94 81.01
Top-5 83.77 85.67 84.71
M4M3+
Conte xt SimilarityTop-1 85.67 86.19 85.93
Top-3 90.82 92.08 91.45
Top-5 94.33 95.17 94.75
M5M4+
Name SimilarityTop-1 89.48 90.04 89.76
Top-3 92.11 93.26 92.68
Top-5 95.43 95.84 95.63
B. Effect of increasing the number of libraries
Increasing the number of libraries can have the following
two effects. First, with the increase of libraries, the number
of infrequent APIs also increases. Second, the likelihood of
mapping the same API name to multiple FQNs in the trainingexamples also increases. We were interested in examining howthese affect the performance. Baker and COSTER can easilybe adapted to an iterative experiment setting where we increasethe number of libraries by adding one library at a time and
record the performance at each step. However, we could not
do so for StatType because the technique takes a considerableamount of time for training. Thus, we conduct the experimentby considering seven different number of libraries and recordthe performance of all three competing techniques for the
top-1 recommendation at each number. Note that we apply
the same 10-fold cross-validation technique to measure theperformance.
Fig. 3(a) shows the F
1score of Baker, StatType and
COSTER for different number of libraries. Among the threecompeting techniques, Baker performs relatively poorly whereit has around 90% F
1score for Ô¨Åve libraries and the per-
formance drops as we increase the number of libraries. Theprimary reason for such declination is that the more we
increase the number of libraries, the more the API names are
mapped to multiple FQNs. Thus, Baker fails to reduce thesize of the candidate set into one for those multiple mappingcases. StatType and COSTER have similar F
1score when the
number of libraries is Ô¨Åve. However, increasing the numberof libraries affects the performance of StatType more thanthat of COSTER. Increasing the number of libraries alsoincreases the number of APIs and many of those APIs lacka large number of examples. This affects the performance of
StatType. However, the performance of COSTER affects the
least. This is possible because the technique considers differentinformation sources to recommend FQNs of APIs and does notrequire large training examples to capture their usage patterns

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. Fig. 3. The effect of increasing the number of libraries on the (a) performance
( i.e., F1score) and (b) Code extraction + Training time of Baker, StatType
and COSTER
(discussed more in the next analysis).
With respect to timing, shown in Fig. 3(b), StatType has
the worst outcome. With the increase of libraries, the num-
ber of examples also increases and the training time grows
exponentially for StatType. Baker has the best performancesince it does not require any training time. On the other hand,COSTER consumes twice more time than Baker and ten timesless than StatType, and manages to maintain the highest F
1
score.
C. Impact on API popularity
This section investigates the relationship between the per-
formance of recommending FQNs of APIs and popularityof those APIs. The popularity of an API is deÔ¨Åned as thenumber of times that API is used in source code examples.We categorize APIs into Ô¨Åve different groups based on theirpopularity or frequency of usages. The Ô¨Årst group consists ofAPIs whose usage frequency is no more than 5% of all usages
of APIs. We refer to this group as the very unpopular APIs
(VU). The usage frequency of the second group of APIs rangesbetween 6-25%, referred to as the unpopular APIs (UP). Theusage frequency of the next two groups ranges between 26-50% and 51-75%, and are called the popular (P) APIs and verypopular (VP) APIs, respectively. Finally, APIs whose usagefrequency is more than 75% of all API usages are referred toas the extremely popular (EP). We calculate the precision andrecall for all Ô¨Åve groups of APIs using the GitHub dataset.
From Fig. 4, we see that the performance of StatType
and COSTER are very close for the extremely popular APIs.The difference is no more than 2% for both precision and
recall. However, the performance difference becomes moresigniÔ¨Åcant as the popularity of APIs decreases. For example,for the popular APIs COSTER achieves 4-7% higher precisionand 3-16% higher recall than the other two techniques. Thedifference becomes the highest for the very unpopular APIs,where COSTER is about 6-28% more accurate in terms of
precision and recall compared with the other two techniques.Thus, among the three techniques we compared, API popular-ity affects the performance of COSTER the least. Moreover,
for unpopular and very unpopular API categories, StatType
obtains the worst precision values. For these APIs, StatTypecould not Ô¨Ånd enough examples in the training dataset andthat affects the performance of the technique. We collected30 examples of very unpopular APIs where StatType failedto produce the correct result and manually investigated them.We found that StatType returned FQNs in 16 cases which arenowhere close to the actual FQNs. This indicates that StatTypecannot perform well in detecting FQNs of those APIs thatare either unpopular or very unpopular. However, COSTERconsiders a rich set of information to form a context of an
API and does not require a large number of examples for
training. Statistically, the precision and recall of COSTER aresigniÔ¨Åcantly different than those of the compared techniquesfor API popularity analysis.
D. Effect of receiver expression types
We categorized receiver expressions of API method or Ô¨Åeld
calls based on their AST node types. We were interested inlearning whether the performance of Baker, StatType, andCOSTER vary across different receiver expression types.
Table VIII shows the performance of all three techniques
across different receiver expression types. The second columnof the table shows the percentage of test cases for each receiverexpression type.
TABLE VIII
PRECISION (PREC.)AND RECALL (REC.)OFBAKER ,STATTYPE AND
COSTER FOR DIFFERENT RECEIVER EXPRESSION TYPES .
Expr . Type Data(%)Bak er StatT ype COSTER
Prec. Rec. Prec. Rec. Prec. Rec.
Class Inst. Creat. 0.27 0 0 84.13 85.11 89.43 91.53
Array Access 0.28 76.14 78.34 85.43 87.76 90.17 91.27
Type Literal 0.34 66.34 72.43 86.73 87.11 89.73 90.17
String Literal 1.20 67.14 72.14 98.34 99.47 98.34 99.71
Simple Name 72.21 83.14 76.17 85.17 86.20 90.43 91.83
QualiÔ¨Åed Name 16.21 80.73 78.59 86.74 89.43 91.74 92.68
Method Invoc. 6.93 18.24 11.49 84.21 85.27 84.91 86.72
Field Access 1.11 64.14 75.18 87.34 87.66 88.17 89.17
The simple name is the most popular expression type,
followed by the qualiÔ¨Åed name and the method invocation.Around 95% of test cases belong to these three expression
types. The difference in performance between COSTER and
StatType for these expression types are small compared toother expression types. The lack of code examples contributesto the difference between StatType and COSTER for otherexpression types (discussed in Section VI-C). For the threemost frequent receiver expression types, the precision andrecall of StatType range between 84-86% and 85-89%, respec-tively. In the case of COSTER, the precision and recall range
between 85-91% and 86-92%, respectively. We investigated 50
incorrect predictions made by StatType which were correctlyinferred by COSTER, and found that global context played theprimary role for such difference. Due to the presence of global

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. (a)
 (b)
Fig. 4. Comparing precision and recall of Baker, StatType and COSTER for API groups of different popularity.
context, COSTER was able to Ô¨Ånd similar contexts from OLD
and determined the correct FQNs. StatType considers only thelast four tokens and was not able to determine FQNs of thosecases. Baker performs very poorly compared to StatType andCOSTER. Finally, results from two-tailed Wilcoxon signed-rank test [23] after adjusting p-values show that, statisticallythe precision and recall of COSTER are signiÔ¨Åcantly differentthan Baker and StatType for all receiver expression types.
E. Multiple Mapping Cardinality Analysis
Name ambiguity poses a threat for resolving FQNs of
APIs, as indicated by prior studies [2], [5], [11], [13]. Name
ambiguity occurs when multiple classes, methods, or Ô¨Åelds
with the same name exist in different libraries or differ-ent packages of the same library. This section investigatesCOSTER‚Äôs ability in resolving the name ambiguity for API
elements with one or multiple FQN mapping candidates and
compared the result with that of Baker and StatType. Weuse the term cardinality to refer to the number of FQNmapping candidates and the test cases are categorized based on
different cardinality values. Next, we calculate the precision
of Baker, StatType, and COSTER for those categories for thetop-1 recommendations. We only consider precision becausewe cannot determine cardinality for missing cases. The Ô¨Årst
column of Table IX shows different cardinality values. Thesecond column shows the percentage of test cases for each
cardinality value. The remaining three columns show theprecision of Baker, StatType, and COSTER.
Table IX shows that 46.7% of total test APIs have only
one mapping candidate. Therefore, for around half of the testcases, the techniques do not need to deal with ambiguities.According to Table IX, COSTER can solve all single mappingcases successfully similar to Baker and StatType. With the
increase of cardinality, the precision decreases to 19.7% for
Baker. In the case of StatType, the precision drops from100% to 83.5% as we increase the cardinality. However, theperformance of COSTER affects the least among all three
competing techniques. The precision of COSTER drops from
100% to 88.17% when cardinality value changes from 1 toTABLE IX
PRECISION (FOR TOP -1RECOMMENDATION )OFBAKER ,STATTYPE AND
COSTER FOR MULTIPLE MAPPING CARDINALITY ANALYSIS
Cardinality Data (%) Bak er StatT ype COSTER
1 46.72 100 100 100
<3 16.54 91.73 92.61 96.73
<10 11.46 84.36 90.43 92.47
<20 7.30 78.98 88.81 91.26
<50 6.34 68.51 86.72 90.72
<100 4.50 62.43 85.12 89.43
<500 3.68 54.72 84.73 89.02
<1K 2.92 43.57 84.28 88.43
1K+ 0.53 19.76 83.52 88.17
1K+. Statistically, the precision of COSTER is signiÔ¨Åcantly
different than both Baker and StatType.
F . Limitation
Despite having the best results for all of the experiments,
COSTER has some limitations that are discussed in this
section.
First, if an API element contains multiple method
calls, COSTER often fails to resolve the FQN ofthe last method call. For example, consider thefollowing method call statement: ‚Äú DownloadMan-
ager .getInstance().getDownloadsListFiltered().toString() ‚Äù,
COSTER was able to detect the FQN of the Ô¨Årst two methodcalls but failed for the last method call (i.e., toString).However, such cases are very rare (0.0004%).
Second, Stack OverÔ¨Çow code fragments can be very short,
which can even contain only one line. In such cases, COSTER
Ô¨Ånds very few to no context at all and fails to resolve FQNs
of API elements. However, we investigated 20 such cases andfound that 16 of them can be solved by reading the posts. Thatgives us a future research direction of resolving FQNs of API
elements leveraging textual content of SO posts. That can be
combined with the current implementation of COSTER.
Finally, similar to StatType, out-of-vocabulary issue also
affects the recall of our technique. However, our proposedtechnique received a high accuracy by considering code ex-amples collected from open-source software repositories.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. VII. T HREATS TO VALIDITY
This section discusses threats to the validity of this study.
First, we considered 100 most frequently used libraries of
the GitHub dataset in this study. One can argue that the resultmay not generalize to other frameworks or libraries. However,all these libraries are popular and have been actively used bydevelopers. we also observed that increasing the number oflibraries affected the performance of COSTER the least (seeSection VI-B). Thus, our results should largely carry forward.
Second, the accuracy of our proposed technique can be
affected by the ability to correctly Ô¨Ånd API usages in Stack
OverÔ¨Çow code snippets. To mitigate this issue, each code
snippet was analyzed by two different annotators. When therewere ambiguities, they talked to each other to resolve the issue.However, such cases were very rare.
Third, the process of making Stack OverÔ¨Çow code compil-
able by the annotators can be erroneous by importing incorrectimport statements for code compilation. However, the Ô¨Årst twoauthors of the paper manually validated the random selection
of those compilable Java source Ô¨Åles, and they did not Ô¨Ånd
any such errors.
Finally, we consider the likelihood score, cosine [20] based
context similarity score and Levenshtein distance [21] basedname similarity score. Other similarity measures could give usdifferent results. However, those similarity measures that weselected are widely used and we are conÔ¨Ådent with the results.
VIII. R
ELATED WORK
One closely related work to ours is that of Baker [5]. For
each API element name, the technique builds a candidate listand shorten after each iteration based on the scoping rules
and a set of relationships. The process continues until allelements are resolved or the technique cannot shorten those
lists further. Our work is also closely related to StatType [11].The technique uses the type and resolution context of APIelements and statistical machine translation to infer FQNs of
API elements. However, we capture long-distance relations
of an API Element through global context along with localcontext and reduces search space step by step. Thus, COSTERperforms better than both Baker and StatType with lessertraining time than StatType (see Section V-C).
Another related work is Partial program analysis (PPA) [8].
The technique leverages a set of heuristics to identify the
declared type of expressions. PPA can be an inclusion of
a technique rather than being standalone for resolving APInames. For example, RecoDoc [2] uses PPA [8] to linkbetween code elements and their documentation. However,47% of libraries in the Maven repository do not contain anydocumentation [10]. Therefore, RecoDoc cannot be applied to
those libraries. ACE [13] is another technique that works on
SO posts, analyzes texts around the code and links them. ACEinvolves text to code linking rather than code to code linking,and thus not suited for evaluation.
Techniques have been developed that focus on type res-
olution for dynamically typed languages, such as JavaScript(JS) and Python [5], [25]‚Äì[28]. JSNice [25] uses conditionalrandom Ô¨Åelds to perform a joint prediction of type annotation
for JavaScript variables. DeepTyper [26] leverages a neuralmachine translation to provide type suggestions for JS codewhereas NLP2Type [28] uses a deep neural network to inferthe function and its parameter from docstring. Tran et al. [27]recognize the variable name from miniÔ¨Åed JS, and the workof Xu et al. [29] resolves Python‚Äôs variable by applying prob-abilistic method on multiple sources of information. However,the primary challenge and application of these techniquesare different than ours. An interesting research direction canbe combining any of these techniques with our solution andexamine the effect for dynamically typed languages.
A number of studies in the literature trace the links between
source code and documentation using various approaches.These include but are not limited to topic modelling [30], [31],
Latent Semantic Indexing [32], [33], text mining [9], feature
location [34], Vector Space Model [35], [36], classiÔ¨Åer [37],Structure-oriented Information Retrieval [38], [39], rankingbased learning [40] and deep learning [41]. However, thesetechniques primarily focus on documentation, bug reports,
and email content whereas we focus on linking between code
elements.
IX. C
ONCLUSION
This paper explores an important and non-trivial problem
of Ô¨Ånding FQNs of API elements in the code snippets.We propose a context-sensitive technique, called COSTER.
COSTER collects locally speciÔ¨Åc source code elements
(i.e., local context) and globally related tokens (i.e., globalcontext) for each API element. We calculate the likelihoodof appearing those tokens and the FQN of each API element.The collected usage contexts, likelihood scores and FQNs
of API elements are stored in the occurrence likelihood
dictionary (OLD). Using the likelihood score along withcontext and name similarity scores, the proposed techniqueresolves FQNs of API elements. Comparing COSTER withtwo other state-of-the-art techniques for both intrinsic andextrinsic settings show that our proposed technique is more
robust and time-efÔ¨Åcient. The technique improves precision
by 4-6% and recall by 3-22% along with an improvementof training time by a factor of ten in comparison withexisting state-of-the-art technique. Experiments on the effectthe number of libraries, API popularity, receiver expression
types, multiple mapping cardinality, and sensitivity analysis
elaborates why COSTER performs better than Baker andStatType. Future studies can combine our solution with thosetechniques developed for dynamically typed programminglanguages.
Acknowledgments: We would like to thank the authors of
StatType for providing us the implementation (both StatType
and Baker) and the dataset (StatType-SO). This research issupported by the Natural Sciences and Engineering Research
Council of Canada (NSERC).

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] B. Dagenais and M. P. Robillard, ‚ÄúCreating and evolving developer doc-
umentation: understanding the decisions of open source contributors,‚Äù
inProceedings of the 18th International Symposium on F oundations of
Software Engineering , 2010, pp. 127‚Äì136.
[2] ‚Äî‚Äî, ‚ÄúRecovering traceability links between an api and its learning
resources,‚Äù in Proceedings of the 34th International Conference on
Software Engineering , 2012, pp. 47‚Äì57.
[3] J. Singer, ‚ÄúPractices of software maintenance,‚Äù in Proceedings of the
15th International Conference on Software Maintenance , 1998, pp. 139‚Äì
145.
[4] C. Parnin and C. Treude, ‚ÄúMeasuring api documentation on the web,‚Äù in
Proceedings of the 2nd international workshop on Web 2.0 for softwareengineering , 2011, pp. 25‚Äì30.
[5] S. Subramanian, L. Inozemtseva, and R. Holmes, ‚ÄúLive api documenta-
tion,‚Äù in Proceedings of the 36th International Conference on Software
Engineering , 2014, pp. 643‚Äì652.
[6] D. Yang, A. Hussain, and C. V . Lopes, ‚ÄúFrom query to usable code: an
analysis of stack overÔ¨Çow code snippets,‚Äù in Proceedings of the 13th
International Conference on Mining Software Repositories , 2016, pp.
391‚Äì402.
[7] E. Horton and C. Parnin, ‚ÄúGistable: Evaluating the executability of
code snippets on the web,‚Äù in Proceedings of the 34th International
Conference on Software Maintenance and Evolution , 2018, pp. 217‚Äì
227.
[8] B. Dagenais and L. Hendren, ‚ÄúEnabling static analysis for partial
java programs,‚Äù in Proceedings of the 23rd International Conference
on Object-oriented Programming Systems Languages and Applications ,
vol. 43, no. 10, 2008, pp. 313‚Äì328.
[9] A. Bacchelli, M. Lanza, and R. Robbes, ‚ÄúLinking e-mails and source
code artifacts,‚Äù in Proceedings of the 32nd International Conference on
Software Engineering , 2010, pp. 375‚Äì384.
[10] S. Raemaekers, A. van Deursen, and J. Visser, ‚ÄúThe maven repository
dataset of metrics, changes, and dependencies,‚Äù in Proceedings of the
10th International Conference on Mining Software Repositories , 2013,
pp. 221‚Äì224.
[11] H. Phan, H. Nguyen, N. Tran, L. Truong, A. Nguyen, and T. Nguyen,
‚ÄúStatistical learning of api fully qualiÔ¨Åed names in code snippets of
online forums,‚Äù in Proceedings of the 40th International Conference on
Software Engineering , 2018, pp. 632‚Äì642.
[12] P. Martins, R. Achar, and C. V . Lopes, ‚Äú50k-c: A dataset of compilable,
and compiled, java projects,‚Äù in Proceedings of the 15th International
Conference on Mining Software Repositories , 2018, pp. 1‚Äì5.
[13] P. C. Rigby and M. P. Robillard, ‚ÄúDiscovering essential code elements
in informal documentation,‚Äù in Proceedings of the 35th International
Conference on Software Engineering , 2013, pp. 832‚Äì841.
[14] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, ‚ÄúSuggesting accurate
method and class names,‚Äù in Proceedings of the 10th Joint Meeting on
F oundations of Software Engineering , 2015, pp. 38‚Äì49.
[15] M. Asaduzzaman, C. K. Roy, K. A. Schneider, and D. Hou, ‚ÄúCscc:
Simple, efÔ¨Åcient, context sensitive code completion,‚Äù in Proceedings
of the 30th International Conference on Software Maintenance and
Evolution , 2014, pp. 71‚Äì80.
[16] A. T. Nguyen, M. Hilton, M. Codoban, H. A. Nguyen, L. Mast,
E. Rademacher, T. N. Nguyen, and D. Dig, ‚ÄúApi code recommendationusing statistical learning from Ô¨Åne-grained changes,‚Äù in Proceedings of
the 24th International Symposium on F oundations of Software Engineer-ing, 2016, pp. 511‚Äì522.
[17] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, ‚ÄúOn the
naturalness of software,‚Äù in Proceedings of the 34th International
Conference on Software Engineering , 2012, pp. 837‚Äì847.
[18] Z. Tu, Z. Su, and P. Devanbu, ‚ÄúOn the localness of software,‚Äù in
Proceedings of the 22nd Joint Meeting on the F oundations of SoftwareEngineering , 2014, pp. 269‚Äì280.
[19] R. Lau, R. Rosenfeld, and S. Roukos, ‚ÄúTrigger-based language models:
A maximum entropy approach,‚Äù in Proceedings of the 19th International
Conference on Acoustics, Speech, and Signal Processing , vol. 2, 1993,
pp. 45‚Äì48.
[20]
R. Mihalcea, C. Corley, C. Strapparava et al. , ‚ÄúCorpus-based and
knowledge-based measures of text semantic similarity,‚Äù in AAAI , vol. 6,
2006, pp. 775‚Äì780.[21] V . I. Levenshtein, ‚ÄúBinary codes capable of correcting deletions, inser-
tions, and reversals,‚Äù in Soviet physics doklady , vol. 10, no. 8, 1966, pp.
707‚Äì710.
[22] C. Sun, D. Lo, S.-C. Khoo, and J. Jiang, ‚ÄúTowards more accurate re-
trieval of duplicate bug reports,‚Äù in Proceedings of the 26th International
Conference on Automated Software Engineering , 2011, pp. 253‚Äì262.
[23] F. Wilcoxon, ‚ÄúIndividual comparisons by ranking methods,‚Äù Biometrics
Bulletin , vol. 1, no. 6, pp. 80‚Äì83, 1945.
[24] M. Aickin and H. Gensler, ‚ÄúAdjusting for multiple testing when reporting
research results: the bonferroni vs holm methods.‚Äù American journal of
public health , vol. 86, no. 5, pp. 726‚Äì728, 1996.
[25] V . Raychev, M. Vechev, and A. Krause, ‚ÄúPredicting program properties
from big code,‚Äù in Proceedings of the 42nd Annual Symposium on
Principles of Programming Languages , vol. 50, no. 1, 2015, pp. 111‚Äì
124.
[26] V . J. Hellendoorn, C. Bird, E. T. Barr, and M. Allamanis, ‚ÄúDeep
learning type inference,‚Äù in Proceedings of the 26th Joint Meeting on
the F oundations of Software Engineering , 2018, pp. 152‚Äì162.
[27] H. Tran, N. Tran, S. Nguyen, H. Nguyen, and T. Nguyen, ‚ÄúRecovering
variable names for miniÔ¨Åed code with usage contexts,‚Äù in Proceedings
of the 41st International Conference on Software Engineering , 2019, pp.
1‚Äì11.
[28] R. S. Malik, J. Patra, and M. Pradel, ‚ÄúNl2type: inferring javascript
function types from natural language information,‚Äù in Proceedings of
the 41st International Conference on Software Engineering , 2019, pp.
304‚Äì315.
[29] Z. Xu, X. Zhang, L. Chen, K. Pei, and B. Xu, ‚ÄúPython probabilistic
type inference with natural language support,‚Äù in Proceedings of the
24th Joint Meeting on the F oundations of Software Engineering , 2016,
pp. 607‚Äì618.
[30] H. U. Asuncion and R. N. Asuncion, Arthur U.and Taylor, ‚ÄúSoftware
traceability with topic modeling,‚Äù in Proceedings of the 32nd Interna-
tional Conference on Software Engineering , 2010, pp. 95‚Äì104.
[31] A. T. Nguyen, T. T. Nguyen, and H. V . N. T. N. Al-Kofahi, J.and Nguyen,
‚ÄúA topic-based approach for narrowing the search space of buggy Ô¨Ålesfrom a bug report,‚Äù in Proceedings of the 26th International Conference
on Automated Software Engineering , 2011, pp. 263‚Äì272.
[32] A. Marcus and J. I. Maletic, ‚ÄúRecovering documentation-to-source-code
traceability links using latent semantic indexing,‚Äù in Proceedings of the
25th International Conference on Software Engineering , 2003, pp. 125‚Äì
135.
[33] A. De Lucia, R. Oliveto, and G. Tortora, ‚ÄúAdams re-trace,‚Äù in Pro-
ceedings of the 30th International Conference on Software Engineering ,
2008, pp. 839‚Äì842.
[34] D. Liu, A. Marcus, D. Poshyvanyk, and V . Rajlich, ‚ÄúFeature location via
information retrieval based Ô¨Åltering of a single scenario execution trace,‚Äù
inProceedings of the 22nd International Conference on Automated
Software Engineering , 2007, pp. 234‚Äì243.
[35] L. D. Wang, S. and J. Lawall, ‚ÄúCompositional vector space models
for improved bug localization,‚Äù in Proceedings of the 30th International
Conference on Software Maintenance and Evolution , 2014, pp. 171‚Äì180.
[36] J. Zhou, H. Zhang, and D. Lo, ‚ÄúWhere should the bugs be Ô¨Åxed?
more accurate information retrieval-based bug localization based onbug reports,‚Äù in Proceedings of the 34th International Conference on
Software Engineering , 2012, pp. 14‚Äì24.
[37] D. Kim, Y . Tao, S. Kim, and A. Zeller, ‚ÄúWhere should we Ô¨Åx this bug?
a two-phase recommendation model,‚Äù IEEE Transactions on Software
Engineering , vol. 39, no. 11, pp. 1597‚Äì1610, 2013.
[38] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, and C. Fu,
‚ÄúPortfolio: Ô¨Ånding relevant functions and their usage,‚Äù in Proceedings of
the 33rd International Conference on Software Engineering , 2011, pp.
111‚Äì120.
[39] M. Saha, R. K.and Lease, S. Khurshid, and D. E. Perry, ‚ÄúImproving bug
localization using structured information retrieval,‚Äù in Proceedings of
the 28th International Conference on Automated Software Engineering ,
2013, pp. 345‚Äì355.
[40] X. Ye, R. Bunescu, and C. Liu, ‚ÄúLearning to rank relevant Ô¨Åles for
bug reports using domain knowledge,‚Äù in Proceedings of the 22nd
International Symposium on F oundations of Software Engineering , 2014,
pp. 689‚Äì699.
[41] A. N. Lam, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, ‚ÄúCombining
deep learning with information retrieval to localize buggy Ô¨Åles for bugreports (n),‚Äù in Proceedings of the 30th International Conference on
Automated Software Engineering , 2015, pp. 476‚Äì481.

Authorized licensed use limited to: LAHORE UNIV OF MANAGEMENT SCIENCES. Downloaded on August 07,2025 at 12:24:30 UTC from IEEE Xplore.  Restrictions apply. 